"Speaker Name","Start Time","End Time","Transcript"
"Speaker 1","00;00;12;14","00;00;52;09","Hello and welcome, everyone. This is the Active Insurance Lab. It is active inference. Live live stream number 26.0 on August six, 2021. We're going to be discussing the paper. Bayesian Mechanics for Stationary Processes. Welcome to Active Inference Live, everyone. We are a participatory online lab that is communicating, learning and practicing applied active inference. You can find us at the links on this slide This is a recorded and an archived live stream, so please provide us with feedback so that we can improve on our work All backgrounds and perspectives are welcome here as well."
"Speaker 1","00;00;53;21","00;01;14;11","Today, as you can find out at this short link, we're going to be having the context video for the discussions on August 10th and 17th in number 26.1 and 26.2, which are going to be group discussions with some of the authors joining. So if you want to join for any of these talks or discussions, then please just let us know."
"Speaker 1","00;01;14;24","00;01;46;04","And also we have a few weeks where we don't have the paper decided so if you have a suggestion and want to be in those discussions, let us know. All right. Today in item stream number 26.0, the goal is going to be to learn and discuss this paper, Bayesian Mechanics for Stationary Processes by de Costa, first in Haines and probably Otis, which was posted in June 2020 on this video, just like all the dot zeros is just an introduction to some of the ideas and a walk through the paper."
"Speaker 1","00;01;46;04","00;02;08;13","One way, it's not a review or a final word. We're going to give some overview to the paper, then address a few of the key words from a big picture perspective. Then go through the figures in the formalism I'm Daniel and I'm postdoc researcher in Davis, California."
"Speaker 1","00;02;10;28","00;02;47;29","So in this paper, the aims in the claims are laid out as follows They wrote in this paper, we considered the consequences of a boundary mediating interactions between states, internal and external, to a system such as about interfaces and interactions. That's the first point. Unpacking this notion we found that the states internal to a Markov blanket look as if they perform variational Bayesian inference, optimizing beliefs about their external counterparts so when the boundaries set up in a certain way for which kinds of systems, that's what we'll be asking."
"Speaker 1","00;02;49;22","00;03;13;08","How do those systems look from the outside or what does it look as if they're doing? And then three, when subdividing the blanket into sensory and active states, we found that autonomous states perform active inference and various forms of stochastic control, i.e. generalizations of PID control. So a lot of these terms like Variational, Bayesian, Markov, blanket, PID control we're going to be talking about more."
"Speaker 1","00;03;13;08","00;03;42;21","But at the overview level, that's the aims and claims as the authors write them the abstract states that the paper develops a Bayesian mechanics for adaptive systems. And then there's only these three claims. Again, which are again related to the blanket and adaptiveness of systems. And then this partitioning of the interface into sense and action. So first we model the interface between a system and its environment with a marker and blanket."
"Speaker 1","00;03;43;10","00;04;11;03","This force conditions under which states internals of the blanket encode information about external states. Second, we introduce dynamics and represent adaptive systems as Markov blankets at steady state. This allows us to identify a wide class of systems whose internal states appear to infer external states consistent with variation inference in Bayesian statistics and theoretical neuroscience. Finally, oops, we partitioned a blanket into sensory and active states."
"Speaker 1","00;04;11;26","00;04;51;09","It follows that active states can be seen as performing active inference and well known forms of stochastic control, such as PID control, which are prominent formulations of adaptive behavior in theoretical biology and engineering. So this is going to be relating to math and physics with Markov blankets and the far from equilibrium thermodynamics component and information theory. And then it's also going to be developed into some domain specific cases or shown to be a generalization of domain specific cases like theoretical biology and engineering these are the sections of the paper."
"Speaker 1","00;04;51;19","00;05;16;20","So first there's an introduction which has some of the general concepts. Then there's a section on Mars called Blankets which is going to be a key framework and a key formalism to track because it's kind of what the paper's about. So first technical sections about the Markov blankets, followed by a discussion of how that relates to Bayesian systems or certain types of Bayesian systems."
"Speaker 1","00;05;17;13","00;05;53;14","And then this general Bayesian framework is then sort of specified towards an active inference model, specifically with an eye towards stochastic control. And then there's a discussion and conclusion, and then there's three supplemental sections related to the synchronization map, which is about the synchronization of internal and external states and how that's related to some math results. There's the Helmholtz decomposition which is related to vector fields and splitting them into different components."
"Speaker 1","00;05;53;22","00;06;33;08","And then there's the free energy computation and some more details related to what exactly that formalism is used us here the key words of the paper were variational, Bayesian inference, non equilibrium, steady state, Markov, blanket free energy principle predictive processing and active inference. So first 94 were non equilibrium, steady state. Here's a figure from this paper stable, unstable and metastable states of equilibrium definitions and applications to human movement."
"Speaker 1","00;06;33;16","00;06;57;02","So there's going to be some mathematical definitions and uses of non equilibrium steady state, but this is a domain specific usage that gets some of the nuances. There's not just one single way to frame equilibrium, especially for dynamical systems. And this is show us with an example of a person in movement. So there might be a total thermodynamic equilibrium like death."
"Speaker 1","00;06;57;12","00;07;22;05","But even within the non death regime, there are still many types of equilibrium. There's lying down there sitting in a chair, there's a walking steady state, there's this metastable state, and then there's unstable states at equilibrium. So if you just questions would be like, what does it mean to have equilibrium of a measurement or a system at a given level?"
"Speaker 1","00;07;22;05","00;07;49;22","And what's the difference between that static or just something on the ground versus a dynamic equilibrium, like something that maintains its position and gives us energy to do that? So in those cases, where does the energy come from? How does this relate to dissipated systems? So something where there's energy to be expended like a battery or a tornado, and how does that relate to adaptive systems and predictive systems?"
"Speaker 1","00;07;50;19","00;08;19;25","Which ones are related to active inference? One or both? And what about when there are multilevel systems of equilibrium? So some that are higher frequency or shorter time scale or smaller spatial scale and others that are broader? And also what if we want to have action oriented models of those systems? So we want to have the description that's optimal for some other set of requirements other than just accurate measurement."
"Speaker 1","00;08;20;21","00;08;29;26","And that's where this paper is going to be kind of headed, is thinking about dynamical systems in general and looking at equations that relate to solving them."
"Speaker 1","00;08;31;26","00;09;02;02","Another keyword was Variational Bayesian inference, and there's a lot of good literature on this topic from a lot of different perspectives. But this article had a good way of phrasing it, which is that variational inference methods consist in finding the best approximation of a distribution among a Parametrized family. And in contrary to sampling approaches, a model is assumed, which is the parameter ized family, implying a bias but also a lower variance."
"Speaker 1","00;09;02;13","00;09;36;20","In general, variational inference methods are less accurate than sampling based like M.C. M.C. ones, but produce results much faster these methods are better adapted to big scale, very statistical problems. So again, to just kind of give a qualitative example, it's like the real world is giving you this data, which is the blurry image of a cat. One way to get at what was the generative underlying cat that generated this blurry image would be to sample and find the most likely cat generating this image or do various other sampling schemes."
"Speaker 1","00;09;37;07","00;10;14;16","And if it's set up properly, it can fit arbitrary distributions. However, it can be computationally expensive and really hard to know if you're solving the problem at a functional enough rate. In contrast, the variational inference approach is related to fitting the data based upon a pre specified family of distributions which can still be really general and it can be fast, but it can sometimes lead to fitting the optimal support for a given distribution, but having it just be categorically wrong on some other dimension like estimated to a deep time or something like that."
"Speaker 1","00;10;15;23","00;10;56;20","So Variational Bayesian inference is this lower cat where there's a known cat and then you stretch and distort the cat. For example, re parameter is the cat to find an underlying likely cat another keyword was Markov Blanket and there was good discussion of this with Mel Andrews in active stream 14 and we kind of had this continuum from Markov on the left here which was studying systems from a mathematical perspective didn't have computer so it was looking at matrices and which elements of matrices were connected to each other or not."
"Speaker 1","00;10;57;16","00;11;55;27","And then that was developed later by Perl and others in this broader Bayesian statistical framework. That's where the Markov blanket terminology starts to come from. And it's also oriented towards empirical data and discovering dependencies in data, and it's very computational. And then part of the literature that we're discussing is where version and others have built on or used this Perl Blanket Slash Martha Blanket theme and done a few things like separating this the blanket states into sensory and action incoming and outgoing nodes, introducing the cybernetic imperative to be a goal, seeking or a multi-skilled, good regulator to maintain that non equilibrium steady state which we'll talk about or and we did."
"Speaker 1","00;11;57;11","00;12;43;06","And then also what's being explored from a formalism perspective in the paper we're discussing is how is it or under what conditions do internal states act as if they're a model of their non local dependencies. So causal dependencies in the world would be reflected by how the brain or some other computer system works. So that entails this generative model of the action in the niche and also brings in the element of including the blanket in internal external states as part of this broader inactive embodied in culture slash pragmatic term also in active upstream."
"Speaker 1","00;12;43;06","00;13;24;20","14, we had this sort of progression emerging from the structuring of the paper where there's a sort of vector field meets diffusion thermodynamics branch on the bottom with the Felker Planck and Helmholtz, and that's related to non equilibrium steady state densities and then the free energy principle subsuming all these sort of pieces like the mark of blanket being maintained through time necessitates the generative model of action is what the internal states have to be inferring."
"Speaker 1","00;13;24;20","00;13;55;16","So the useful components of the external world even under a free energy principle, there's all these sort of related areas like active inference is an implementation of systems under the free energy principle. How is this related to predictive processing Bayesian brain? So we talked more about that, but today we're going to look at how it's used more by the authors in this paper."
"Speaker 1","00;13;57;14","00;14;47;15","So the first thing is that the code is available on Connor's GitHub. And we'll also be talking with Connor, hopefully. So let's maybe work through it or if anyone has the chance to run it. Tell us about how it went or what they learned from it. But I didn't get to run it so we can start with figure one, which is laying out the big graphical intuition of the whole paper and everything is going to be specifying constraints or some details or implications of a way of interpreting this intervention of the blanket states between internal and external states."
"Speaker 1","00;14;48;10","00;15;26;02","So we have view the internal states be the blanket states and ETA, which are looks like a curly end. And this is going to allow us to separate any set of nodes that we're specifying and ask what are the blanket states that insulate from some statistical codependency sense, which will be exploring, insulates these two partitions and the caption says the mark."
"Speaker 1","00;15;26;05","00;15;55;26","All blanket is depicted graphically as an undirected graphical model, also known as a mark of random field. The circles represent random variables. The lines represent conditional dependencies between random variables. The mark all blanket condition means that there is no line between view internal states and after the external states. This means that MU and ETA are conditionally dependent given B so conditioned on the blanket, these internal and external states are separated."
"Speaker 1","00;15;56;08","00;16;40;20","In other words, knowing the internal state, MEU does not afford additional information about the external state, ETA when the blanket state B is known. Thus, blanket states act as an informational boundary between internal and external states. So that's what the blanket states do. And this paper is going to explore some of the consequences of that. And again, anyone who has more expertize in this area or some piece of knowledge that I totally missed or got wrong, my apologies, but I just want to try to represented as I saw it, because a lot of the details I was just wanting to learn more about and go over with the authors."
"Speaker 1","00;16;41;21","00;17;22;22","So if anyone has comments in the chat, they can also just like mention it. OK, so let's jump into how they present some of the key pieces of the formalism, because that's the details of free energy principles slash some of the grounding of active inference. So even if this new formalism for you, then also hopefully it's also interesting so keeping that picture in mind to figure one another way of writing out that graphical model with formalism is this that's a then upside down T so that's the external states."
"Speaker 1","00;17;23;20","00;17;53;02","And then you conditioned on B so that is blanket states are remarkable blanket separating internal external states. So in the top third of the slide, these are like writing out in prose. The same thing is this graphical model. And then we can take that tuple, that set of those three kinds of notes so we have measurements from all these different things or generative models of each of these different categories of nodes."
"Speaker 1","00;17;53;28","00;18;23;24","And then that set so all the measurements slash generative processes that X is existing in a bigger state space. So the little X which consists of like the real measurements are part of the bigger X, which is kind of like the capital versions of each of these and those spaces that the E.v and I are taking to be Euclidean spaces for simplicity."
"Speaker 1","00;18;23;24","00;18;59;16","So I don't know if it holds for other kinds of spaces. So that's just defining the total space of the possible states of each of these nodes. And then also the ones that you actually observe they start with this case of a Gaussian distribution P encoding the distribution of states of the world. So P is a random variable that has drawn from N, that's like a normal distribution symbol with a mean and a variance."
"Speaker 1","00;18;59;16","00;19;35;15","And the mean is zero. And the variance is this PI to the -1 and this has an associated precision covariance matrix. So it means zero. That's the convention and means the normal distribution. And so it's zero mean. That's the variance type and the precision matrix is PI. So then the variance is this PI inverse and sometimes the equations are formulated in terms of precision, other times they're discussed in terms of variance."
"Speaker 1","00;19;35;25","00;20;01;15","But those are kind of like two sides of the same coin because the same it's just like describing it. Whether it's how uncertain you are is like looking from the mean out and then your precision might be kind of like is like focusing down unpacking in terms of Gaussian densities. So we find that a Markhor blanket is equivalent to Spar City and the precision matrix."
"Speaker 1","00;20;02;14","00;20;39;28","So the marginal blanket condition is entailing that the covariance, the pie between external states and internal states, that's the first one here is equivalent to between external states and internal states, the median ETA and both of those equal zero. So there's no covariance and they're the same as zero. So they're conditionally independent. That's the Bayesian covariance matrix framing of the Markov blanket."
"Speaker 1","00;20;39;28","00;21;41;25","Sparsely Blanket States act as an information boundary between external and internal states. And this allows the breakdown of instead of the all by all consideration of correlations for data fitting or generative modeling, you can reduce the the model space to a much better formulated subset of equations which are related to inferring distributions relating to like external states, conditioning on a blanket states and internal states, conditioning all the blanket states this enables us to associate to any blanket state its corresponding expected, external and expected internal states so then this equation, said Drew, is a set of equivalencies which were that the internal states as a function of blanket states."
"Speaker 1","00;21;41;25","00;22;58;22","Some would be were equal to the expectation fancy of mu conditioned on b so that's the expectation of external states conditioned upon both internal states internal states conditioned on blanket. I think that's corrected is right then three that that expectation is the same as the distribution. That's being calculated involving the objective distribution. P and that also that that's related to a the variance in covariance like this sigma variable of internal and blanket states MU and B mapping to the space of all the possible internal states, which was I from here and then they wrote, pursuing the example of the nervous system, each sensory impression on the retina and ocular motor orientation, blanket state is associated"
"Speaker 1","00;22;58;22","00;23;54;07","with an expected scene that caused sensory input, which is the expected external state and does an expected pattern and an expected pattern of neural activity in the visual cortex expected internal state. So it's a way that that objective distribution of P gets integrated into the internal model's output given blanket states or maybe there's another interpretation too. So I was just I don't know, one way to look at it this question about the relationship between the internal and external states after you've not D noise them, but D correlated them in a specific way."
"Speaker 1","00;23;56;19","00;24;20;23","Is this topic of the synchronization map. So they go from just pointing out that, OK, we've do correlated internal and external states with these blanket states. Now we're going to ask how these internal states might encode information about expected external states. So after we've d correlated systems how do they have a memory or a model of one another?"
"Speaker 1","00;24;22;07","00;24;53;01","For this, we would need to characterize a synchronization function sigma mapping the expected internal state to the expected external state given the blanket state. So sigma mapping function of internal states, of blanket states is going to relate to the external states as a condition on blanket states. So there's still the dependance of both internal and external states on the blanket states."
"Speaker 1","00;24;53;02","00;25;41;08","B And now there's a sigma mapping that is going to connect internal and external states and Lemma 2.1 goes through some details of that specification. So the first is proposing that there is a function that maps internal and external states for any blanket state I don't know how generally it holds, but or this would be cool to talk to the authors about, but it's sort of proposing this sigma function and maybe in certain cases it's a zero line, maybe in other cases there's some meaningful relationship."
"Speaker 1","00;25;41;08","00;26;24;02","But for biological systems, it does seem like there's a meaningful relationship so those are the systems that we want to explore modeling they then write in how to construct this synchronization map. The key idea is to map an expected internal state of B to an expected external state at a B, and to do this, the two points they provide are one, to find a blanket state that maps to this expected internal state, i.e. by inverting you and two from this blanket state, find the corresponding expected external state i.e. by applying error, we now proceed to solving this problem."
"Speaker 1","00;26;24;12","00;27;45;09","Given an internal state view, we study the blanket state b such that the internal states of B equals U they provide some details maybe that is meant to be external states. Not sure here they're framing that inference that MU is doing the internal states in terms of covariance matrices, which is in this equation 2.4 then frames that matrix of unknowns as a system of linear equations or those are kind of similar representation patterns and then solve for it in the vector space using this equation 2.5 they use this matrix negative function, which is the more Penrose pseudo inverse it would be helpful to have someone explain it a little more detail, what the matrix inverse"
"Speaker 1","00;27;45;09","00;28;09;15","is or what degrees of freedom there are in choosing different kinds of matrix inverses. But I just found it interesting from Wikipedia that a common use of the pseudo inverse is to compute a best fit or at least square solution to a system of linear equation that locks a solution. Another use is to find the minimum norm solution to a system of linear equations with multiple solutions."
"Speaker 1","00;28;09;29","00;28;53;22","So this means that there might be multiple solutions that are equally well fit, and this just allows you to pick one fast. Maybe and then they write that the key for the existence of a function sigma mapping expected internal states to expected external states. Given blanket states is that for any two blanket states associated with the same expected internal state, these be associated with the same expected external state this non digit degeneracy means that the internal states, e.g. patterns of activity in the visual cortex, have enough capacity to represent all possible expected external states, e.g. three d scenes of the environment that's formalized in this lemma."
"Speaker 1","00;28;54;13","00;29;33;23","So first was the point discussed before that we're interested in positing the existence of this Sigma function 11.2. So these are all points that are going to be equivalent to each other. Point two is that for any two blanket states, 12 that is the two external states are equal. I'm sorry if the two internal states are equal, the use of those two B's then it's going to map on to equal points in external what?"
"Speaker 1","00;29;34;14","00;29;45;21","So the blanket is a is a map that conserves certain properties of the relationship between internal and external states."
"Speaker 1","00;29;50;17","00;30;33;25","Then points three and four of the Lemma 2.1 use this kernel in image notation. So here was an image that was kind of interesting about how spaces were mapped to each other. So someone could definitely help explain what some of these mappings mean or what the implications are. And then this sideways u shape is the proper subset symbol so that's meaning that every element of the first one is in the second one, but B is a bigger subset."
"Speaker 1","00;30;34;15","00;31;34;25","So certain mappings between the internal and blanket covariance and the external and blanket covariance are then similar for the pie FIG. two provides a visualization of some simulations drawn that support just from a simulation perspective. The existence of a useful synchronization map function. So figure two on the left side shows the external states given blanket states. So those are the orange external states up."
"Speaker 1","00;31;36;03","00;32;23;24","These are the flipped sorry about that. So the blue are the internal states the sigma mapping corresponding to the internal states conditioned on the external states. And they're doing a very accurate job of mapping onto the external states in Orange and then in B is a so-called non example and I'd be curious what changed or what was different about this second example or is there more than one way to break the model?"
"Speaker 1","00;32;23;24","00;32;55;04","What are the cases where it does or doesn't have a mapping then in Section three, they go from that sort of static estimate of variables that are just having a specific relationship to one another to Bayesian mechanics operating through time. So they write in order to study the time evolution of systems with Markov Blanket, we introduced dynamics into the external blanket and internal states."
"Speaker 1","00;32;55;18","00;33;26;27","Henceforth we assume a synchronization map under the conditions of the Lemma 2.1. So that was what was discussed up here and now in contrast with FIG. one, which was just the variables by themself now were putting the T subscript and looking at how those variables are changing through time we use a bacillus to depict an intuitive example of a miracle blanket that persists over time."
"Speaker 1","00;33;27;21","00;33;39;00","Here's a blanket state between the membrane and actin filaments of the cytoskeleton which mediate all interactions between the internal states and the external medial external states."
"Speaker 1","00;33;41;11","00;34;19;06","In example, 31 they write There are many examples of stochastic processes out of Gaussians Steady State P, to name a few stationary diffusion processes with initial conditions. X zero tilde p. The time evolution is given by an Ito stochastic differential equation. Appendix B Next slide that will be on the next slide. And then also example 31. More generally, any Markov process at steady state such as the zigzag process, any mean zero Gaussian process of steady state and any random dynamical system at steady state."
"Speaker 1","00;34;19;06","00;34;52;18","P So questions for people who no more or the authors would be what? What kinds of systems fall under this category of stochastic processes? And what kind of real world systems would be modeled well or not by this formalism? And then going a little deeper into that first point in the examples, Appendix B points to this Helmholtz decomposition, and that is conditioned on this IDO stochastic differential equation."
"Speaker 1","00;34;53;14","00;35;30;10","And again, just questions for someone who knows more or for the authors, what does this Ido calculation entail and what does or doesn't fit under this IDO framework that would or would work using some other definition just not sure about that. But in this Helmholtz supplement, there is a very nice figure ten that is looking at in this kind of hill climbing perspective."
"Speaker 1","00;35;30;26","00;36;02;19","So in an algorithm that's looking in its local neighborhood and doing a calculation to sort of make local decision making and hill climb that action of a stochastic but hill climbing algorithm up this gradient or down depending on which way you set it up is going to have two components to its motion this straight up the mountain component like putting a ruler on the mountain and the maximum slope."
"Speaker 1","00;36;03;05","00;36;50;17","And then another component that's at the same likelihood, like the same altitude, same energy that's going at an ISO contour just like you can always do for a mountain. So with that idea in mind, it's possible to take the fall dynamic of a stochastic arc trace on the top left. This is like this particle that is going different colors through time that just like converging to the top of this mountain, diffusing light to the bottom of the ball, like a marble going to the bottom of a ball, and then that trajectory can be separated into this spiral or this oval, the time irreversible component, and then a time reversible component that has a much more"
"Speaker 1","00;36;50;17","00;37;18;13","direct trajectory and that was in the section with Remark 31, which is again related to the IDO stochastic differential equation, which I just am not too sure about but they write when the dynamics are given by this idea of stochastic differential equation, a Markov blanket of the steady state density does not necessarily imply that internal and external states cannot influence each other."
"Speaker 1","00;37;19;14","00;37;57;03","So that's, I think, kind of the interesting tension in question is how do certain mathematical definitions of conditional independence of blanket states separating internal external states, how does that partition sometimes result in subsets or systems that don't influence each other versus systems that do so? In the case of systems that don't influence each other, I was thinking of like an empty bottle of air in a room of air, like they're separated systems, but they interact through the bottle so they could transfer heat."
"Speaker 1","00;37;57;03","00;38;27;28","For example, a hot bottle could transfer heat to the room, but the systems would only interact conditioned on the temperature of the interface versus a person in a room could be planning or thinking about the external states. So just how do we think about what kinds of systems are still having the influence or long term implication for each other, even after conditioned on the interfacing a certain way?"
"Speaker 1","00;38;28;14","00;39;04;28","And then also, just how does this example provided kind of show that that'd be good to walk through and the absence of reciprocal influences between two states in the drift sometimes but not always implies conditional independence OK, going back to the main action figures in Figure four, there's now looking at this synchronization map and transition probabilities for processes at the Gaussians Steady State."
"Speaker 1","00;39;05;25","00;39;49;04","So on the left is we plot the synchronization map as in FIG. two only here the samples are drawn from trajectories of a diffusion process with a mark of a blanket. So now it's a temporal process on the right panel, there are the transition probabilities of that same diffusion process for the blanket stay at two different types, and this is called a joint distribution because there's two distributions that are being looked at jointly and if you kind of went to one side and looked through the corn field one way, you would see this Gaussian P of piece of S."
"Speaker 1","00;39;49;27","00;40;16;19","And if you looked at it the other way, you'd see P and B through T and then they write, which is kind of interesting. This shows that in general processes at a Gaussian steady state are not Gaussian processes. So I was just like very curious about what that meant. In fact, the Ornstein or back process is the only stationary diffusion process that is a Gaussian process."
"Speaker 1","00;40;17;01","00;40;54;15","So the transition probabilities of nonlinear diffusion processes are never multivariate. Gaussians I'm not sure what that means, but it's very kind of interesting. All right. So then from this sort of mapping between internal and external states through time, the synchronization mapping sigma there right in the section on predictive processing, we can go further and associate each internal state view a probability distribution over external states such that each internal state encodes beliefs about external states."
"Speaker 1","00;40;55;14","00;41;27;21","So now, rather than just framing these as two dynamical processes that track one another, like, oh, there's this synchronization map between this random variable internally and this random variable externally, they're going to frame this Q distribution. So Q is going to be a probability distribution inside of you. That's over external states such that each of those states encode beliefs about external states."
"Speaker 1","00;41;27;21","00;41;59;04","So Q of internal states is about ETA, and that's going to be a normal distribution with a mean of the actual external state. It's like mean and variance. So it's better than sigma of MU, the mapping function comma variance. So here these two are both what it's being is being inferred is the the external state and the mapping."
"Speaker 1","00;42;00;04","00;42;50;10","And then there's the variance of the precision on external states. And then 3.4 is just giving a little bit more details about how that Q is like this internal function where the states of it are reflecting beliefs about external states and then there's a relationship to how the internal states are conditioned on blanket states. This was a interesting remark that would be cool to have people discuss would be when they wrote note of potential connection with epistemic accounts of quantum mechanics, namely a world governed by classical mechanics, sigma condition at zero or I don't know if there's another way to read that's in all the three lines in 3.2."
"Speaker 1","00;42;50;15","00;43;41;03","I'm not sure because in 3.2, I'm not sure if there was a sigma in which each agent encodes Gaussian beliefs about external states. The could appear to the agents as reproducing many features of quantum mechanics. So this was just cool. Like, what is the relationship between the Sigma internal external mapping function and then classical versus quantum mechanics? So maybe classical systems are like the ones where there's a clean separation between the internal and the external states, quantum states being ones that are more with residual influence between internal and external states so returning to the predictive processing, they write that under that 34."
"Speaker 1","00;43;41;23","00;44;25;05","So under this implication of the Q internal distribution expected, internal states are the unique minimization of a callback. Labor divergence so that you through time, internal states through time are the minimization. The argument over the view states, internal states of the detail, the scale, divergence of the internal belief distribution. Q About external states. Q of ETA line divergence between the objective distribution of external states conditioned on blanket states."
"Speaker 1","00;44;25;26","00;45;28;08","So if that Q distribution can converge to the P, then that is being minimized effectively. This measures the discrepancy between beliefs about the external world you some you of error and the posterior distribution over external variables, computing scale divergence. They're going to obtain this 3.5 and that's orange mean. So it's minimizing the differential between Sigmas prediction so that sigma of mu minus the actual external states, three time sigma mapping function of internal states minus the actual state multiplied by so pi of the precision matrix of ETA relating to how wrong that differential is."
"Speaker 1","00;45;30;01","00;46;05;06","So that's kind of an interesting framing. And then they write that the right hand side of 3.5, that equation is commonly known as the squared precision weighted prediction error, which is the discrepancy between the prediction and the expected state of the environment. And it's weighted with a precision matrix that derives from the steady state density. This equation is formally similar to that found in predictive coding formulations of biological function, which stipulated that organisms minimize prediction errors and in doing so optimize their beliefs to match the distribution of external states."
"Speaker 1","00;46;07;00","00;46;37;12","So that's pretty cool and we can talk more about it, but it kind of reminds me of this uncertainty reduction imperative rather than a reward maximization imperative. It's kind of like saying if the organisms are minimizing their prediction errors, not maximizing the reward they will optimize their beliefs to match the distribution of external states. So maybe that's related to that variance."
"Speaker 1","00;46;37;12","00;47;25;07","Reduction impairs so we had the bridge of processing section and then there's one more layer that gets added on. So from the Marco Blanket to making it a Bayesian Markov blanket happening through time to introducing this Q distribution, that's a special type of internal state there. Right in the next section, we can go further and associate expected internal states to the solution to the classical variational inference problem from statistical machine learning and theoretical neurobiology."
"Speaker 1","00;47;26;02","00;47;54;13","So it's going to be that Q function is going to be a very specific kind of Q function. So now all systems are going to be doing this subtype of variational inference, or maybe they are that is something we could discuss, but we can finesse that mapping function between the internal and external states and what exactly the internal states are."
"Speaker 1","00;47;54;27","00;48;18;23","Because if you can co-design both the internal states and the mapping then you have a lot of degrees of freedom in mapping on to external states even as they change. So now we're going to do really some special phrasing, maybe it's a more general one, but just one specific phrasing of how the internal states could map onto the external states."
"Speaker 1","00;48;19;05","00;49;08;06","And that's going to be as expected, internal states as the unique minimizes of a free energy functional IEEE in evidence bound. So now there's an F function over blanket and internal states through time, and that F function is going to consist of two elements, which is like a divergence term that we saw before with a divergence between Q's internal estimate of external states divergence and the actual external states, conditional blanket states minus the the evidence term and then that can be equivalent to this energy minus entropy term."
"Speaker 1","00;49;09;16","00;49;37;04","So that last line expresses the free energy as a difference between energy and entropy. Pretty cool energy or accuracy measures. To what extent predicted external states are close to the true external states while entropy penalizes beliefs that are overly precise. So that's cool, like energy is locking in on the good solution. That's like maybe the component that is going up the hill."
"Speaker 1","00;49;37;24","00;50;15;10","And then entropy is keeping things away from just going up the hill and keeping this other component in. But if entropy overwhelms entropy, the energy, then the solution will not kind of fit the situation. So at first sight, variational inference and predictive processing are solely useful to characterize the average internal state. Given blanket states at steady state it is then surprising to see that the free energy says a great deal about a system's expected trajectories as it relaxes to steady state."
"Speaker 1","00;50;16;10","00;51;08;29","Pretty interesting so here's how they kind of show that Figure five is using that free energy minimizing framework to look at variational inference and predictive processing so the highlighted process, the figure illustrates the system's behavior after experiencing a surprising blanket state. So kind of like a perturbation this is a multi dimensional Ornstein or back or you process with two external blanket and internal variables initialized at the steady state density conditioned on an improbable blanketed state which was given at the initial time actual distribution of X and B at times zero and on the left top left quadrant is sort of this red to blue gradient of free energy."
"Speaker 1","00;51;08;29","00;51;55;03","So this kind of high elevation to low elevation and through time the blanket states just converge down to the bottom of a bowl. So that's the free energy minimization in action. The upper right is the is the free energy over time averaged over multiple trajectories. So it starts out really high and then it kind of rolls down and it converges to this low level at 40 time points the lower left is showing this predictive processing framing of the Q distribution versus the actual distribution and said steady state from timestamp about 100."
"Speaker 1","00;51;55;14","00;52;38;16","The predictions become accurate. So that's showing how the estimate of the parameter converges to the accurate external states under their simulation because you walk through and then on the bottom right is looking at the error prediction errors, the covariance of so the two co errors, the evolution of precision weighted prediction errors over time these are normally distributed with zero mean steady state figure six is a maybe I didn't change."
"Speaker 1","00;52;38;24","00;53;14;17","I'm not sure exactly. It's a similar figure, has a little bit of differences. I again showing the predictive processing lock in on a free energy minimizing process, showing the average free energy initially having a slight increase maybe, and then just decreasing through time. And then also just showing that the the relation ship of the covariance was raised up from zero and then started converging back to zero."
"Speaker 1","00;53;15;02","00;53;42;00","So just cool figures but cool to see what other people would think about them in figure four or I'm sorry, section four, we get two active inference and stochastic control. So they write in order to model agents that interact with their environment, we now partition blanket states into sensory and active states. So for activity, everything that we've talked about before is like an undirected Bayesian graph."
"Speaker 1","00;53;43;24","00;54;10;26","It's not using that first and blanket separation. So that's where we're going to be bringing in a lot of other cool formalisms and ideas. So let's go from that sort of blanket one type of fiber in the blanket to to now in order to model agents to interact with their environment, we now partition blanket states into sensory and active states."
"Speaker 1","00;54;11;23","00;54;43;01","So the blanket state B of T blanket three C is now two subsets of sensory and action states through time. So now that whole tuple, the whole set that you need to infer the four kinds of states are external sense, action and internal. That s a yeah. And now that previous example of the bacillus, so here we had just one, we had external and internal states mediated by one kind of node."
"Speaker 1","00;54;43;20","00;55;37;07","And now the external states are coupled to the sensory states that's SFO and sensory states are coupled to internal states. Internal states are coupled to action states and actions can act back to external states. Also sense an action are connected to each other, but I wonder if that part is needed or I'm always wondering what topologies of connectedness different functionalities arise from and Miguel Aguilera's paper and recent guest stream speak to that so we take that dynamic Markov blanket model from the previous sections and now split the blanket states into sense and action that was this piece here."
"Speaker 1","00;55;38;03","00;56;09;09","Now then they write intuitively in agents actions and internal states depend upon its sensations. Therefore, we are interested in characterizing autonomous, i.e. active and internal. So we can't control our senses directly, but we can control internal states and active states. So those two states, we're going to think about that pair of notes given sensory states. So that is the following free energy equation that was brought up earlier."
"Speaker 1","00;56;09;14","00;56;43;19","So taking that free energy, minimizing perspective on the distribution of new internal states and blanket states as a whole be before we introduce the split here, then do a similar free energy equation with three variables on the sensory action and internal states, which is just another way of saying the blanket states and the internal states view and then et cetera, et cetera."
"Speaker 1","00;56;44;11","00;57;11;07","More details that people can explain if they know better. But the results that comes out relates to action. So be cool to learn more about what this equation means. But then they write This is known as active inference because expected autonomous states minimize free energy. Crucially, active inference is a well known framework to describe and generate adaptive behavior in neuroscience."
"Speaker 1","00;57;11;07","00;57;53;21","Machine learning and robotics. See Figure eight. So FIG. eight is a similar diagram to we had before, but now it's an active inference agent rather than just the free energy minimize year of the previous framing. And then they write, the figure illustrates the system's behavior after experiencing a surprising sensory state averaging internal variables for any blanket state. This is an ornstein or back process with two external, one sensory, one active, and two internal variables initialized at steady state density, conditioned upon the improbable sensory state."
"Speaker 1","00;57;55;21","00;58;31;28","And also one pattern that they found was that the free energy minimization here, it's relatively monotonic. It kind of doesn't ever bump back up. It just goes down or stays the same. But then it was kind of interesting. They said we thought the free energy of the expected internal state averaged over multiple trajectories so this is not just one aberrant run in this example, or maybe this is just one example, but they wrote the average free energy does not decrease monotonically."
"Speaker 1","00;58;32;03","00;59;09;13","See Figure five for an explanation and then maybe we can learn more about that. But that was kind of cool. OK, they continue on and write that any means stationary means zero stationary calcium process with exponentially decaying other covariance function so it resembles itself less and less through time is an ornstein or back process, which is kind of cool because in phylogenetic we sometimes would use this for, let's just say the average femur length of some mammal."
"Speaker 1","00;59;10;00","00;59;31;19","And then you would be inferring a model that had an average value. So it was kind of like a peg and then like a tether, so like a diffusion term and then a mean. So just a mean and a variance that drift through time is like the Ornstein or back or the o u process, sometimes known as Doom's Theorem."
"Speaker 1","00;59;32;11","01;00;08;25","So those kinds of processes which are apparently general enough to be really good model systems for studying math, I guess thus if C equals a finite sum of exponentially decaying functions, we can express as of five states sensory states as a linear function of several nested oew processes, i.e. an integrator chain from control theory. So the integrator chain was kind of a cool idea and we'll talk about maybe applications of integrator chain in a few slides."
"Speaker 1","01;00;09;17","01;00;41;26","But just to continue with the math that we have. So there's this s of T states or time sensor states is going to be this F function. And then there's these D these, these kind of similar D equations. So it says in this example, F and F Survi are suitably chosen linear functions not sure the pronunciation is, but the squiggle are matrices and W are standard Brownian motion."
"Speaker 1","01;00;43;01","01;01;16;08","Thus we can see sensory states through time as T as the output of a continuous time of hidden Markov model whose hidden states s of t superscript. I encode various orders of motion, position, velocity, jerk, et cetera. These are known as generalized coordinates of motion in the Bayesian filtering literature which will look at graphically in Fig. nine. And more generally, the state process as of T and a function F need not be linear, which enables to realize nonlinear non Gaussian processes as a t."
"Speaker 1","01;01;16;24","01;01;41;04","Technically, this follows as arresting all back processes are the only stationary calcium marker of processes. Yet we emphasize that stochastic realization is not as well developed in the general case, so this is in a way saying that we're going to have like the position of the ball is the sensory data that's like the first pass of where the ball is."
"Speaker 1","01;01;41;04","01;02;10;00","That's the measurement, that's the sensory data. Then there's the variance, and then there's the second derivative of that statistical moment. And so if the variance is equal through time, then you can just integrate away in the higher levels or zero, and you've kind of locked on to the pattern after just one or two levels. But there's other distributions where as you go to higher and higher distribution derivatives, higher moments there's just more or different information."
"Speaker 1","01;02;10;23","01;02;46;03","So there's this concept of the integrator chain where these relationships of equations are all integrals or derivatives of each other, which are kind of like that going up or down in this rows of equations. And then this helps use Gaussian type mathematics like the o you stochastic process where a lot of the math is clean. It allows you to apply these systems of equations of well behaved types o you to model highly nonlinear system."
"Speaker 1","01;02;46;03","01;03;18;28","So this was related to some interesting discussions I will look forward to hearing what you have to say about these so-called generalized coordinates of motion and that relationship between control theory and then looking at nonlinear functions as relationships of integrals and derivatives leads to this discussion of the PID control. So proportional integral derivative control P ID is a well known control method in engineering."
"Speaker 1","01;03;19;13","01;03;52;06","More than 90% of controllers in engineered systems implement either PID or PI. No derivatives like just proportional integral control. The goal of PID control is to control a signal as sub t, it's integral as sub t with a zero instead of a one, and its derivative s sub two of the two close to a pre specified target value since like I want 95% battery and I want it to be a rectangle at 95% and its derivatives to be such and such during use."
"Speaker 1","01;03;53;04","01;04;28;23","This turns out to be exactly what happens when we take stochastic control of an integrator chain with with three orders of motion. It's not just a three level model in this sort of infinite rows of the moments of the distribution, the three levels are just what are described here. The signal, the integral and the derivative when f is linear expected autonomous states control to what extent integral proportional derivative processes as of yet 01 and two levels of derivative are from their target value of zero."
"Speaker 1","01;04;28;29","01;04;59;12","So it's like if it's a linear response function, then the first derivative is going to be f a constant value. As in the second derivative you get rid of it. Furthermore, from F and K alone, one can derive integral proportional derivative gains which penalize deviations of these three moments respectively from a target value of zero. So these are highly optimized for problems and computable crucially, these control gains are simple byproducts of the steady state density and the stochastic realization problems."
"Speaker 1","01;05;00;10","01;05;26;06","This is the core question Why restrict ourselves to control when stochastic control of integrator chains is available? It turns out that one sensory states s of T are expressed as a function of an integrator chain one to get away with controlling an approximation of the true sensory process obtained by truncating high orders of motion as these have less effects on the dynamics though, knowing when this is warranted is a problem in approximation theory."
"Speaker 1","01;05;26;10","01;05;59;01","So how? Let's just say we open the door to doing infinite derivatives of statistical distributions, infinite variances upon variances. How would we know when to stop? That's the problem in approximation theory or approximation science. This may explain why integral feedback control, which is just like battery, good pi control and equals one is in the control with two extra layers of the derivatives or the most ubiquitous control methods in engineering applications."
"Speaker 1","01;05;59;14","01;06;38;15","However, when simulating biological or biological control, usually with highly nonlinear systems, it is not uncommon to consider generalized motion to fourth or sixth order. So that's pretty cool. Like how many levels of recursion and depth in modeling are needed to have a people. So calling back to here, when we defined that initial sense function as part of this integrator chain setting off the integrator chain, here's the initial sense function as part of a broader set of dependencies."
"Speaker 1","01;06;39;03","01;07;14;10","So here is that sense function at time three, six and nine the figure is depicting in a graphical format. The Bayesian network of those equations flipped the encircled variables are random variables and the processes are index an arbitrary, so an arbitrary, arbitrary sequence of subsequent times. The arrows represent relationships of causality. So the sensory states are not causing each other through time."
"Speaker 1","01;07;14;26","01;07;50;28","They're getting measured through time, but they are being emitted through time by this s at the zero s level like the just the base generative model of this s a zero level function. So this is the generative process then that can have higher derivatives of change. And each of those are being framed as well behaved statistically. So in the hidden mark, our model and these white nodes are hidden states, but they're inferred by statistics."
"Speaker 1","01;07;52;01","01;08;25;05","The Hidden Integrator state, hidden state processes as a team is given by an integrator chain, i.e. nested stochastic differential equations. These processes can be seen as encoding the position, velocity jerk, et cetera. So higher derivatives of the distribution so this into integrator chain is kind of a cool idea. So here was one paper stabilization of integrator chains in the presence of magnitude and rate saturations, again, scheduling approach."
"Speaker 1","01;08;25;26","01;08;51;06","So this paper wrote and the conclusions in this paper, we have given a time varying controller. So kind of like a Harris stick or an algorithm for control, for stabilizing a chain of integrators in the presence of magnitude and rate saturations. It is proved that the controller gives a convergent closed loop system, i.e. for any bounded state, initial or initial condition, the state will converge to the origin."
"Speaker 1","01;08;51;23","01;09;20;09","The main strength of the controller is that it gives provable convergence without being overly conservative. This is verified in a simulation study. So it's kind of like we have the frame of the airplane at steady state and then which is flying through the air. So how are we going to re equilibrate all the sensors so that if we get knocked off to the side, what's the path by which we're going to get those sensors back to their desired level?"
"Speaker 1","01;09;22;00","01;09;48;10","And it's just a question. This is kind of what it looks like. One of the figures from the paper. So these are these derivative functions. They get like bumped and then they come back to the level that they're desired almost in this signal processing like way so what are integrator chains? How are they used? Maybe someone has used them before that before."
"Speaker 1","01;09;49;06","01;10;17;08","And then one kind of random thought was there's a lot of cases where we frame a process as mean zero because it allows thinking about something in relationship to a defined axis, whether it's the number line or whether it's some other variable, it kind of splits half the things negative and positive, or there's various other times, but there's actually a bunch of places and maybe more."
"Speaker 1","01;10;17;08","01;10;39;20","Or maybe these are not all the same. So it's OK if somebody knows more and helps us all out. But there's actually a few cases where a distribution that's hard to estimate is replaced with a zero mean. Like sometimes there's replacing elements of a list with a Z score, like their standard deviation from the mean or the mode."
"Speaker 1","01;10;40;26","01;11;06;09","But then it's like how far above half or below. So that's like replacing something with a zero center Z score or T distribution. Sometimes there are error distributions which are extracted from a signal. It's like the signal is the underlying line. And the goal of the signal extraction is to leave your error distribution in mean zero and then equally distributed in both directions."
"Speaker 1","01;11;06;09","01;11;44;05","So sometimes we're pulling out an error distribution sometimes we're trying to find that mean zero. We get to a level of derivative of a signal where the mean is zero, like where the rate of change of some high derivative is zero. Which implies that it's not changing at that level. Then sometimes we're trying to get like in predictive processing study the differentiable predictive processing with trying to get a zero centered variance on the sensory input."
"Speaker 1","01;11;44;28","01;12;21;12","But more about how we can study the differential between some controllable distribution and some other distribution that we can't control. And then how to make how to zero center that to determine which solutions are better or worse than each other. And there's a lot of different methods there and then there was this other paper time optimal regulation of a chain of integrators with saturated input and internal planning variables and application to trajectory planning so again, just kind of a open question."
"Speaker 1","01;12;21;18","01;12;45;15","What is the relationship between integrator chains and this idea that we talk about a lot with action planning as inference then the conclusion this paper outlines some of the key relationships between stationary processes, inference and control. These relationships rest upon partitioning the world into those things that are internal or external to a statistical boundary known as a Merkel blanket."
"Speaker 1","01;12;46;00","01;13;08;16","When equipped with dynamics, the expected internal states appear to engage in variation or inference, while the expected active states appear to be performing active inference in various forms of stochastic control, the rationale behind these findings is rather simple If a Markov blanket derives from a steady state density, the states the system will look as if they are responding adaptively to external perturbations."
"Speaker 1","01;13;08;29","01;14;06;25","In order to recover the steady state. Conversely, well-known methods used to build adaptive systems implement the same kind of dynamics implicitly so the system maintains a steady state with its environment. So maybe there's isn't totally accurate classification but there's some algorithms that are being connected on one side to each other. So we have learning and optimization learning as inference all of machine learning, et cetera, learning and optimal optimization, and then variational inference being used as an optimization technique with gradient descent by free energy in machine learning and maybe in evolutionary systems then there's this question about estimating latent causes in the world and connecting that to higher derivatives in the integrator chains or others."
"Speaker 1","01;14;06;25","01;14;48;03","Variables under consideration. That's that mapping of external states which aren't directly seen then maybe this one goes on this side, but there's this whole other set of algorithms related to adaptive systems, and these are framed variously by adaptive systems, cybernetic systems anticipatory systems, holism, homeostatic slash active learning, all the static agents, all these sort of biologically inspired. Yes, drawing on far from equilibrium thermodynamics."
"Speaker 1","01;14;48;12","01;15;45;09","But here was like the flame dissipating like it's about something that just sort of gets to its free energy minimization, just like gets there. But with these active systems, the question becomes how do we take this ongoing multi scale inference side and connect it to this multi scale action in the loop niche modifying also action planning biology section one friend looked at these slides and then made a comment and they wrote, I think it would be good to expand on the differences of the paradigms in Section three, which were lettered as they were in the paper a priori, a posteriori estimate estimation afterwards estimation, predictive processing and variational base."
"Speaker 1","01;15;45;28","01;16;12;28","I get the differences formally, but still not at a super intuitive level. So this might be good as a pedagogical exercise for ourselves. So that'll be cool. Maybe everyone can think about that and come to their own explanation for what the differences between these three and we'll talk about it in the dark one and not two then just a few of the implications and bigger questions that are raised by the paper."
"Speaker 1","01;16;13;18","01;16;52;27","So in the section on interacting Markov blankets, they write the sort of inference we have described could be nuanced by partitioning the external state space into several systems that are themselves Markov blankets, such as Markov blankets nested at several different scales from the perspective of internal states. This leads to a more interesting inference problem with a more complex generative model it may be the distinction between the sorts of systems we generally think of as engaging in cognitive inferential dynamics, and simpler systems rest upon the level of structure in the generative models, i.e. the steady state densities that describe their inferential dynamics."
"Speaker 1","01;16;53;26","01;17;21;15","So that's pretty cool about maybe this framework or at least work in this level of analysis, mapping just directly to nested Markov blankets. And it brings up this whole discussion about what kinds of systems can we model with active inference. So no one's saying that the stock market is an auto regressive model, but people use auto regressive models."
"Speaker 1","01;17;21;21","01;17;51;01","So what kinds of systems will we usefully be able to model with active inference without really just getting into a debate about whether it is active inference in the system? Just how are we going to use Variational Bayes methods with modern twists and developments related to cybernetics and motor skills systems? So that's like the instrumental ISM side and then the realism side is what kind of systems are doing active inference."
"Speaker 1","01;17;51;22","01;18;26;00","So there's maybe even a realism of philosophy branch and then even maybe on the applied side, like it's a philosophical debate, is the cell doing active inference, doing free energy principle? Is it realizing certain constraints of nature, or how would we know what an ant is doing? Free energy minimization in its brain and then on the engineering side, it could be argued that if a robot is implementing active inference code, that it that it is actually doing active inference."
"Speaker 1","01;18;26;07","01;18;50;20","So this question about designing systems within the active inference framework. So instead of saying I think all Internet networks are active inference systems, it's definitely a claim somebody could make interesting claim, but it's quite another thing to design a computer network that is within an ontology of active inference. Then it would be fair to say that that network is for doing active inference."
"Speaker 1","01;18;51;01","01;19;16;06","So that's a realism and maybe for some it's more of an engineering realism, others it's a philosophical realism another implication, a bigger question that arises in the paper is about temporally deep inference. So they right. This distinction may speak to a straightforward extension of the treatment on offer from simply inferring an external state to inferring the trajectories of external states."
"Speaker 1","01;19;17;06","01;20;00;19","So again, moving from not just the point estimate, current estimate of external state, that was what was presented earlier to inferring the trajectories of external states, which is to say decomposing their higher orders of movement. This may be achieved by representing the external process in terms of its higher orders of motion by solving the stochastic realization problem. By repeating the analysis above, internal states may be seen as inferring the position, velocity jerk, et cetera, of the external process consistently with temporally deep inference in the sense of a Bayesian filter, a special case of which is the extended common boozy filter so these are cool things to talk about."
"Speaker 1","01;20;00;19","01;20;25;29","We could talk about what is temporally deep active inference. What are the ways that the model deals with internal and external categories and discrete variables? How do we deal with the inference on external states and how does that relate to our action selection? And then what happens when there's a feedback or some other type of interaction with a niche informational niche?"
"Speaker 1","01;20;25;29","01;20;58;00","The social needs the stigma object niche. So or other questions people can write well, that was a sort of quick run through paper and I'm not sure if I got each section 100% correct. So it will be good to have the author's perspective on a lot of pieces, as well as a lot of other experts who I'm sure could have said a ton more and better about some of the math."
"Speaker 1","01;20;58;16","01;21;26;05","But we can offer up the same questions that we have for any paper, like what might a good understanding enable what are the unique predictions and implications and the unique developments of this paper? What are some of the next steps for free energy principle and active inference research? What are the goals of this research? And then also to the participants and the authors, what are they still curious about?"
"Speaker 1","01;21;26;28","01;21;48;06","Learning so thanks for participating. It was a fun 26 and we hope that you'll join for 26 .1.2 if you can, or comment on the videos or whatever and thanks talk to you later. Bye."

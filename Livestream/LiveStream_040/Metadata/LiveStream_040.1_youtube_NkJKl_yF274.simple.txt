SPEAKER_09:
All right, hello and welcome everyone.

It's March 17th, 2022.

We're here in ACT-INF Lab live stream number 40.1 discussing a free energy principle for generic quantum systems.

Welcome to the ACT-INF Lab.

We're a participatory online lab that is communicating, learning, and practicing applied active inference.

This is recorded in an archived live stream, so please provide us with feedback so we can improve our work.

All backgrounds and perspectives are welcome here, all QRF.

And video etiquette for live stream will be hopefully following.

To learn more about Active Lab, head over to activeinference.org.

And for the rest of Active Stream 40.1, Blue, thanks for being the facilitator.


SPEAKER_04:
Sure.

So in this stream today, our goal is to learn and discuss this paper, A Free Energy Principle for Generic Quantum Systems by Chris Fields, Carl Friston, James Blaisbrook, and Michael Levin.

And we have both Chris and Carl on the stream with us today.

So we maybe want to just start off briefly with doing some introductions.

So my name is Blue Knight.

I'm an independent researcher in New Mexico, and I will pass it off to the first author, Chris.


SPEAKER_00:
Hi, good morning, and thanks for attending this discussion.

This is a paper about active inference and how to place the active inference framework in a quantum information theoretic setting.

So what I thought I should do to introduce it is just give a little bit of the motivation for doing that.

Quantum information theory is an outgrowth of quantum theory that started in the 70s, or even you could take it back to the 60s if you wanted to.

And its goal was to consider physical interactions as information exchange processes

and to reformulate quantum theory in this way.

And so it has a very natural kind of overlap with what Carl has been doing with the free energy principle in terms of reformulating physics, essentially all of physics and information.

in decision theoretic terms.

So why quantum theory?

Why go to this theory that's often thought of as a theory only of very tiny objects that are moving very fast?

And the reason is that it's scale-free.

And conceptually, quantum theory is very simple.

And it has one

core hypothesis, which is the principle of unitarity.

And the principle of unitarity is simply the idea that total information is conserved.

So quantum theory is based on a conservation law, a proposed conservation law, conservation of information.

So what going to quantum theory really forces us to do

is to take seriously the project that Carl spent a lot of time on in his particular physics paper in 2019, which is to understand what we mean when we say that we've observed a system multiple times.

What we mean when we say we've interacted with the same object

multiple times and it forces us to or at least it encourages us to take that question seriously and it provides a number of tools for talking about that question and we'll talk about that as we get into the paper but and that's in a sense our motivations to take this

very nice, quite abstract, but in a sense, very fundamental toolkit that's based around the idea of conservation of information and apply it to this question of active inference.

So I'll let Carl pick it up from there.


SPEAKER_01:
Thank you.

It's a great pleasure to be here.

I have to say I was hoping to learn more than provide a background.

So this is a great opportunity from my point of view to hear a conversation and participate in terms of

how it's held under control, scale-free, background-free, and that's what I think is one of the most attractive, parsimonious benefits of taking a quantum perspective on this.

It reminds me of the Quinean desert landscape, a minimal assumption, which

i'm always drawn to and apply this to some fundamental questions which have actually been beautifully listed for us at the top here about things and crucially um how things coupled to other things in terms of measurement and observation and inference so i'm personally

delighted and hungry to learn about how simple questions about the nature of things we observe and measure and make inference about can be cast from this fundamental perspective of quantum information theory.


SPEAKER_02:
Very nice.

Thank you.

James, thanks for joining us.

Would you like to introduce yourself and maybe say a few words about the paper or pick it up from there?


SPEAKER_06:
Oh, hello.

You may call me Jim.

Thank you.


SPEAKER_07:
You want a potted biography, do you?

Anyway, I'm a retired academic emeritus professor at Eastern Illinois University and adjunct associate professor at the University of Illinois at Urbana-Champaign.

My background is global analytic science.

geometry and some related areas.

But in the last 16 or 17 years I've been applying mathematical techniques to cognitive neuroscience and foundations of quantum biology and more recently quantum information as it relates to the free energy principle.


SPEAKER_03:
Great, thank you.


SPEAKER_04:
Stephen, do you want to maybe introduce yourself and give a thought about the paper, something you liked or remembered or were curious about today?


SPEAKER_08:
Yeah, thank you.

Hi, I'm Stephen Sillit.

I'm based in Toronto.

I'm just doing a practice-based PhD into social topographies and community development.

And I have got a background many years ago in chemistry.

And I'm really curious about...

The way that this idea of something that's scale-free also ties to something which seems to be very specific on scale, i.e.

where something that's highly contextual and highly bounded by degrees of freedom at a certain scale, and then how something could be inferred that's across all sorts of scales.

So that's really something that I'm curious about on this particular piece of work.


SPEAKER_02:
Thanks.

Daniel, do you have any other particular questions that you want to address?


SPEAKER_09:
just looking forward to seeing the uh discussion and anyone who's watching live can ask a question so i'm curious about how also like stephen said the specificity of a given experiment interfaces with general aspects of the formalism and uh many other things but we'll go go ahead blue


SPEAKER_04:
Well, do we want to maybe start there?

Do you guys want to speak to that scale deterministic aspect of the quantum FEP?


SPEAKER_00:
Well, I can say something here.

Any information theory will start with some state space, so some set of degrees of freedom that have some set of values.

And in the paper we restrict ourselves to finite state spaces because we're really interested in systems that only have access to finite energy, and so finite computational resources.

So one starts with a state space, and in a sense that state space, when viewed from kind of the external perspective of the theorist who's defining it, sets the scale of the problem.

Because in defining the state space, you're saying what the values are of the degrees of freedom,

And when you say, what are the possible values, you are implicitly making some assumptions about resolution and making some assumptions about measurement.

So for example, if we think of a canonical quantum system, which is a qubit, just one, a particle with a spin, and we say the spin can be up or down,

then we made an assumption about how we can measure the spin.

And the most natural mathematical framework is a three-dimensional ball of values or a sphere of values if you normalize them.

And that's making an assumption that we can measure the spin in any one of three directions.

And so here we have imposed some possible values on this set of degrees of freedom.

And so in a sense, we've set a scale for the problem.

Similarly, if we're talking about interactions between human beings,

Well, clearly there are lots and lots of degrees of freedom that we can think of, but we typically think about only a tiny handful of degrees of freedom that have to do, for example, with verbal communication or visual communication, olfactory communication or some other kind of communication.

And we always impose resolution restrictions.

We can say this difference is detectable and these other differences aren't detectable.

And whenever we make a decision like that, we're setting a scale.

And when we characterize the information theory as scale-free, we're just saying that the formalism doesn't change when the scale changes.

So the formalism, the mathematical description, remains exactly the same.

regardless of what scales we pick by choosing the possible values of the degrees of freedom that we're talking about.

We don't change anything else about the way the theory works.

So that's all that scale-free means.

It doesn't mean that we've somehow abolished the notion of scale.

That's part of our experience.

So it's not something we're gonna make go away.

It's that we haven't changed the way the math works depending on the notion of scale.

And the most notorious example of that thing that we're not doing is the historical division of the world into kind of a quantum domain, a classical domain, and a cosmological domain where you use three entirely different theories.

like quantum theory, classical physics, and general relativity to talk about what's going on.

So that's not scale-free.

There, you're radically altering the mathematical description to deal with different kinds of problems.

So that's what we're not doing, trying to use the same mathematical description to deal with all problems.


SPEAKER_04:
I think that's a really helpful description, Chris, because so many times when we think about quantum, we think about the very, very, very small, like subatomic particles.

And so it's pretty amazing to see math that's interchangeable with my level of existence, like not just tiny little pieces of me being used from quantum theory.

What...

Is there any other math that you know of or any other examples of math that's been interchangeable with classical and cosmological and quantum scales ever before?

Or is this maybe the first time that's ever been done?


SPEAKER_00:
Well, no.

If you look back to classical physics in the era of Newton,

Or even I think the better representative is classical physics is reformulated by Laplace.

In that era, electromagnetism wasn't really understood yet.

So people were thinking about classical physics in terms of mechanical interactions in gravity.

But the idea at that point was that forces act instantaneously.

and forces act with one over R squared dependence.

That was the only force that was really understood, the force of gravity.

Forces were symmetrical, right?

Newton's second law, action and reaction.

And this theory was meant to be universal

And so if you look at the structure of that theory, as various people have pointed out, Nicholas Geisen has written a couple of papers about this.

It's a non-local theory.

And even in Laplace's era, they realized that there was a deep issue with this theory that if a rock rolls downhill on the moon,

then that immediately affects the state of everything on Earth.

In fact, immediately affects the state of everything in the universe.

So that starts to look a lot like entanglement, that states are not locally independent.

And that's the fundamental thing that Einstein attacked about classical physics in formulating

the theory of relativity was to insist on non-instantaneous forces, which just means forces transmitted by some carrier, which we call light.

But light is really just a carrier of information.

It could have been anything.

It could have been neutrinos or something like that.

so what einstein did that was most revolutionary was to impose locality on physics and that in a sense divided the world into realms that from a formal perspective not just a practical perspective could be treated independently

And so it's no surprise that Einstein hated the idea of entanglement, because entanglement was undoing precisely what he had done to, in his mind, repair physics.

So yeah, I mean, classical physics of the time of Laplace was formally a scale-free theory.


SPEAKER_04:
That's awesome.

Thank you for the clarification there.

So Einstein broke physics.

Stephen, did you have a question?


SPEAKER_08:
Yeah, just one question sort of following from that is

As someone sort of thinking about how this might play out in other ways, is it useful to think about things all the way up from the smallest and systems all the way up down from the biggest and classical as being where we're trying to name things as systems?

Because often I hear people talking systems, systems, systems only, but the use of things seems to be important.


SPEAKER_00:
as well and i'm wondering if we should have more thinness in our way of looking at the world uh shall i comment on that yeah okay um yeah this i think you're raising here a really interesting question and uh when we talk about

things, as Carl so beautifully emphasized in his 2019 paper, we are implicitly talking about something that has an identity and that maintains that identity through time.

And that's a deep

philosophical assumption that's nearly always implicit.

I mean, people just don't talk about that assumption very much.

And when we use the word system, we're, I think, often trying to speak a lot more loosely, but in fact, our language doesn't let us speak more loosely.

If I talk about a system,

then implicitly I'm saying I've got a particular state space.

That's what a system is.

And if I talk about a different state space, then I'm talking about a different system.

So let's apply that to something interesting like embryology.

I pick a set of degrees of freedom that are meant to be the degrees of freedom of a zygote.

And I then, you know, a few days later, have a set of degrees of freedom that are the degrees of freedom of some kind of early stage embryo.

But I want to say this is the same thing as the zygote.

Well, I'm now really abusing my notation, right?

In a really serious way by saying that these two are the same thing, even though they have different state spaces.

So this is a, I think this is a fundamental question for

any science that bases itself on the notion of a state space, how to talk about moving between different degrees of freedom and associating those things with some kind of evolution of a kind of meta thing that we regard as a single entity.

And this is one reason that at the beginning of this paper, we assume that we have some single system that we divide in two.

We draw a decompositional boundary.

And that decompositional boundary is basically arbitrary, or it's completely arbitrary.

But when we're doing something like embryology,

we're looking at a decompositional boundary between the thing we're interested in, the embryo, and the rest of the universe, including us, that's getting bigger and bigger and bigger.

And so we need a way to talk about that sort of thing.

And we just, we allied that problem in this paper.

We talk about a fixed decomposition of,

that divides the entire universe into two parts that we can characterize the interaction between.

But yes, underlying all of this is this much larger question of how we think about decompositional boundaries that are expanding with respect to some parameter that we call time.

And of course, that time is not anyone's observable time.

because we're trying to talk about it at the level of the universe as a whole.

So we can very quickly get into extremely deep questions.


SPEAKER_04:
So two things that came up there really for me is like okay yeah you're thinking about the embryo suddenly that's like a system that's separate from the world but even like within the embryo like where do you draw the line between the placenta and the embryo and when?

So like there's even some fuzzy blurring at the developmental scale and also really the notion of time and time irreversibility

and memory and encoding was just something I found absolutely fascinating about this paper.

So you mentioned that the quantum, the free energy principle for generic quantum systems does not imply like a space-time background, like it doesn't need a space-time background to operate, but then like brought time back in, in terms of memory and encoding.

And I just wonder if anybody maybe wants to speak a little bit more about that or help us understand how and why you don't need space or time for it to work.


SPEAKER_00:
Well, I could talk about that a little bit if no one else wants to pick it up.

Let me go back to the first thing you said, though, Blue, which was about boundaries and how hard it is, for example, to draw boundaries in something like embryology.

There's another deep, deep assumption that we make that the boundaries that we draw in the world are somehow ontological, objective, and observer-independent.

And quantum theory and many other sort of theoretical approaches remind us constantly that that's a bad assumption.

That drawing, thinking of these boundaries as ontic causes all kinds of formal problems.

So for example, the associativity of the state space multiplication operator becomes invalid.

So you have to throw out the whole vector space formalism.

So I mean, real problems.

So boundaries are observer dependent in any formalism like this.

So what do we mean by this universal time?

it's a construct.

It's, it's a theoretical convenience and we map it onto our observable time, which is time measured by our clock.

And since we're, we're the theorists, we can say, okay, we're going to privilege our clock.

And, uh,

we're going to assume that our clock is somehow universal, even though we know that that doesn't make any sense.

And when we think of space-time, I mean, the term background-free comes from cosmology or quantum cosmology.

And it's the idea that

if we want to understand gravity, or if we want to understand space-time as an emergent phenomenon, then we need a theoretical approach that doesn't build space-time into it.

But if we think of biology, how many organisms, what fraction of organisms have anything like our experience of space?

To what extent

is space part of the world of organisms that we're interested in?

And to what extent is space as we see it a construct of our ability to make certain kinds of measurements?

And in a strict quantum theoretic formalism, time is not an observable

but distance is so it's an observable like any other so in a in a very strict sense quantum theory is not a mechanical theory because it's not really about things going on in an independent space-time did you have a question


SPEAKER_08:
Yeah, I'd just like to comment on, I say thanks for referencing state spaces for systems.

I think that's really helpful because I've been wrestling with this dynamical part of systems, which is really helpful to thinking dynamics.

But most or a lot of people, when they talk about systems, they just think about relationships between things and system dynamics.

It just flows as if it's like things being passed around rather than dynamical things.

like processes.

So I think it's helpful to think about state space in that way to frame systems.

And I'm just, yeah, I'm curious about how we have to be careful about people who use systems in those other ways and don't have a dynamic or

concept of something which is non-linear being taken into account, because that seems to be something that this approach really gives the ability to do.

So I wonder what your thoughts are on that.


SPEAKER_00:
Carl, would you like to say something to that?


SPEAKER_01:
yeah i'm not sure it's going to be deeply informed um so i'm i'm as i'm listening i'm sort of trying to translate all these beautiful ideas into a classical framework that i'm much more comfortable with so the the last point is um from the classical perspective absolutely fundamental i mean you start with a launch valve or a random dynamical system

that is just cast in terms of flow operators or movement operators that you can connect with time if you want to.

But just to reinforce Chris's point, time and space are both constructs and they're just explanations

for these flow operators.

So if these flow operators have a dynamics, then implicitly you've got a rate of change with respect to something which is time, but that's quite a big move.

So putting things in terms of mappings, I think is absolutely fundamental.

So, Steve, if your point was when you need to think about

um systemic processes or systems from the point of view of mappings usually associated some time i get the impression from um working with chris and jim that um you know there is an ordinal or a sequential aspect so just coming back to the question is where did memory get into the game you know there has to be um there has to you know there can still be sequences or um you know

sets of things that constitute a memory, but it doesn't have to have the attribute of time in the sense that we would understand a flow from the point of view of a differential equation.

So I'd like to hear Chris and Jim's comments on that.

While I'm talking though, just this importance of

time as a construct.

It does strike me that that takes, there's a lot of heavy lifting if we just abandon the notion of some universal or clock time in relation to what I would

from a classical perspective, read the scale-free aspect of it.

I'm thinking here of the renormalization group.

So I'd be interested to know if there is a renormalization group or the apparatus of the renormalization group in quantum physics.

But from my perspective, that notion of having a scale-free approach is exactly

what is the aspiration of any, in particular, the free energy principle approach would have when trying to account for the progressive

increase in the scale of a system, but preserving exactly the same dynamics, the same Lagrangians, the same, again, from a classical perspective, the same functional forms for the things that are conserved.

So to reiterate Chris's fundamental point earlier on, that we're not talking about a commitment to a particular scale, be it very, very small or very, very big.

What we're talking about is a commitment to the same functional forms and the apparatus and the mathematics that underwrite the dynamics, if you like, at any given scale.

And then that calls then into question

how do you get from one scale to the other?

And one thing that sort of jumps out at you when you look at the renormalization group in relation to summarizing dynamics of random differential systems,

is that you get this time dilation or you move from one time scale to another time scale.

So I'm coming back to the point that when you talk about time, what time are you talking about?

evolutionary time?

Are you talking about the time that the fetus is enjoying in its mother's womb?

Are you talking about the time scale appropriate for the fluxes of ions over a cell membrane?

So the deeper question here is that how does one time relate to another?

um the people that i know who are interested in this issue from the point of view of the perception of time or time as a construct um usually fall back on information rates they usually fall back on the the rate of belief updating or the amount to which you have moved in some belief space technically a statistical manifold from a classical point of view but as soon as you say rate

With respect to what?

And then, of course, you're back with some time.

Which time?

And where you end up is, well, OK, I can't talk about any absolute information rate, but I can certainly talk about the number of moves I make at one scale.

per number of moves I make at another scale.

Even we could be doing that in our heads.

When we, very high scales, say from the point of view of a generative model in our heads, dealing with things that unfold,

very slowly in relation to fast-moving updating much closer, say, to our sensory organs or actuators, that it does make sense to actually talk about how far have I moved in terms of an information length.

um in my belief updating in um at a higher level of a hierarchy or your higher scale in relation to the to the number of moons or how far i have moved okay

We've got so many chatting away.

All good.


SPEAKER_04:
Jim, do you want to mute if you have background noise, please?


SPEAKER_09:
Okay, or you can just... It'll be muted on live stream.

Continue.


SPEAKER_04:
Sorry, please continue.


SPEAKER_01:
That little response was very rambling because there are so many interesting questions that have come up over the past 10-20 minutes.

I'd like to pass back to Chris and Gin now.

to get their take on how comfortable they would be by taking the notions of say the renormalization group as applied to path integral formulations of random diagonal systems and whether that has any homologue or any currency in a truly background-free quantum formulation.


SPEAKER_05:
Well, I don't know.


SPEAKER_07:
Basically, I think what Carl is proposing concerning the renormalization group and van der Waals theories is something to be investigated, but I'm in consonant with Carl concerning

um the aspect of um memory from the informational point of view and um may i speak about the specifics of our construction namely the cone cone diagram which um yeah i'll be there hello yep


SPEAKER_09:
Continue, Jim.

Okay, you could mute or just do Yeah.

That's all good.

We're going to be looking at the cone cocoon.

Maybe just Chris or Carl, you can give a first pass.

Like what is this being shown?

And also how does it relate to active inference?


SPEAKER_06:
Oh, hello.


SPEAKER_09:
Okay.

Welcome back.


SPEAKER_07:
Continue with the local distraction.

Yes.

Okay.

Well, you see, at the beginning of the paper, when we discussed the holographic screen, which underlies the Markov blanket, you have these preparation measurement operations, which

feed into the diagram 14, of which the lower part emanating from D' is a memory write system.

So as the top part is picking up information from

the measurement and preparation operators, there is a memory rights system being implemented from the opposite direction.

One thing I would like to say about the individual classifiers that

we haven't sort of fully mentioned in the paper that an individual classifier can be the result of an algebraic operation on two other classifiers.

These algebraic operations can be of the type such as concurrency,

or orthocurrents, for example, when you have a system of trains going through a system of stations, the classifiers in question can be combined to that extent, and also by choice.

So I think the scope for

What is happening informationally in diagram 14 is quite significant.

Originally, two spaces were applied to such areas as concurrency and high dimensional automata.

And also,

we can look at the possible uh possibilistic uh case and there are ways of interpreting diagram 14 in terms of the graph network too so um

I know this doesn't exactly answer Carl's questions, but I want to say that there is the mechanism there for the memory, right?

So this is, of course, influential when you see that active inference leads to minimizing the

variational free energy principle.

If you have any questions about those diagrams, please ask me.

Perhaps Chris can elaborate on what I'm saying.


SPEAKER_00:
Yeah, let me make a background comment here.

which in a sense goes to one reason that we're using this quantum theoretic formalism.

If we think of everything in terms of a quantum theoretic formalism instead of a classical formalism, then

The only place that there's any classical information in the system is actually on the boundary between the two interacting components.

So in that original,

I don't know whether it's figure one or the very original idea in the paper is that we have some closed system, which we can think of as the universe as a whole or the universe of interest as a whole.

And it's in some quantum state.

And if we divide it into two and we make the assumption that

the joint state after the division is separable, that we've divided it in a place where the interaction of the system with itself is weak enough that we can think of the two halves independently or the two components independently.

Then we have this natural sense of classical information written on that boundary.

And that's the sense of classical information that one encounters in the holographic principle in cosmology.

And it's in that sense of classical encoding that this boundary serves as a Markov blanket.

But since the boundary is the only locus anywhere of classical information in this formal description, then any classical memory has to be resident on that boundary.

And

This actually brings to mind a picture which was not included in this paper, but that I'm using in a subsequent paper with Mike Levin, which really is just thinking of this boundary using a pie chart, and as we pointed out in this paper,

Another kind of classical information is the classical free energy flow that powers information processing.

So this actually isn't the right picture.

And so one can immediately carve off a large chunk of the boundary and say, okay, this is just thermodynamic exchange.

These bits aren't useful to anybody.

They're

They're the bits that are being used, burned as power.

Yeah, this is a better picture for that.

And then there's a chunk of the pie chart that we can think of as perception and another chunk of the pie chart that we can think of as action.

But then some other piece of the pie chart has to be allocated to memory or there's no internal time.

Having an internal clock and having a memory are the same concept.

If I don't have any memory at all, then I have no awareness of the passing of time and I have no internal clock.

So when Jim talks about, in that previous cone-cocoon diagram, a memory-write operation, it's a memory-write operation on the boundary.

And of course, the opposite partner in the interaction, my memory segment of the boundary, from my point of view, might be my partner's perception segment of the boundary, or might overlap with my partner's perception of the segment of the boundary.

I have no idea how my interaction partner cuts up the boundary from their point of view.

So,

This is, yeah, that's what this picture sort of illustrates.

So this question of what is the memory also becomes observer independent, which gets back to this question of time and the relationship of time to a local clock and the relationship of a local clock to memories.

So whenever we talk about time as observed by someone, we're making reference to a local clock and a memory resource, and of course, an energy flow that powers that memory resource and prevents it from decaying entropically.

So we keep circling back to this abstraction of the universal time that parameterizes the universal dynamics.

And this formalism, like any such formalism, requires that concept as a theoretical construct.

But at least it doesn't confuse it with the experienced time of any particular observer, which has to be represented in this very different, in this very distinct way.


SPEAKER_04:
Awesome.

Thanks.

So I think that is a perfect segue into a question that was asked by Franz Kuchling in the live chat.

And also Mike Levin has joined us.

I don't know if he's able to talk.

He says he's having some internet issues.

Mike, did you want to say hello before I ask Franz's question?

If you can.

Okay, I'll ask the question and then we'll see if Mike figures out how to unmute and say hi.

So Franz asks, how does the definition of a thing that maintains its identity over time scale with time scales or over time scale with time scales?

There you go.

How does the definition of a thing that maintains its identity over time scale with time scales?

What?

I still didn't get it.

For example, I would call something a thing if it persisted for some time with respect to my observational timescale.

So in other words, can quantum theory say something about how to set temporal observer limited boundaries of defining a thing?

There's a lot in there.

So how does the definition of a thing?


SPEAKER_09:
Can I give one first off, Lou?


SPEAKER_04:
Yeah, yeah, yeah.


SPEAKER_09:
We see many ways, some that are actually implicitly scale friendly or scale free, like the hierarchical parametric models in SPM, statistical parametric modeling.

That's separating like in a multi-scale experimental design from an experimentalist's perspective into signal and noise, including the combinatoric interactions

in a classical parametric state space.

And there's also the Bayesian identification of parameters, which also is just as was brought up, scale implies choosing what the state space is.

And so what...

statistical variance gets partitioned into the noise term, or the kind of ripple term, and what gets partitioned into the wave term, into the signal, into the regression slope, rather than the variance term.

These kinds of variance partitioning questions

can be approached in a instantaneous sense, like descriptive statistics, even if it's about time series.

And I think a theme that we're hearing brought up is that taking repeated measurements seriously brings in a whole host of essentially cognitive features, like as well as information, theoretic,

perspective, dependence, all of these features that are tucked under the rug or down projected onto in a special case.

If we just assume a synchronous universe or a synchronous regression, it's a totally different scenario.

But considering this quantum statistical background helps us

deal with the cognitive modeling, and also take the consequences of action into account because action is implied in the measurement, preparation, recording, experimental, active inference cycle.

It was just so subtle in the paper where he said all inference in this scheme is active inference, but the paper was about quantum and the FEP.


SPEAKER_05:
Thanks.


SPEAKER_08:
Could I bounce off what Daniel just said very quickly?

Sure.

Because I was also thinking about the nature of action.

So I thought what Daniel just mentioned there ties in with if memory happens via there being some action and then the action defines the memory scale, so I think that makes sense.

that starts to define a scale.

So I'm curious if there's something different between the temporality of a thing maintaining its thingness and the sort of temporality of actions by systems of things.

Because I think things can only act to maintain their thingness, but they don't, if they start doing things with others,

they become a system.

So I suppose you've got where that action still starts to come into the equation.

Or am I mistaken there?


SPEAKER_00:
Can I address that briefly?

Yeah, I think the relevant fable here is the fable of the two drunks who are walking home from the bar leaning on each other.

And if they're lucky, they manage to keep each other upright.

If we think of an observer in an environment, and so we draw a boundary between the observer and the environment,

and we think about the observer's interaction with its total environment, then we've divided the boundary up, as we were talking about earlier, into a pie chart where a large fraction of the bits are fuel, and so they're uninformative, and there's a perception space and an action space and a memory space and so on.

Now let's think about what happens when the observer identifies a thing embedded in the environment.

So the observer has now segmented or sectored the perception part of the space into a part that includes bits that represent the state of this thing.

and the rest of the perception space, which are bits that represent the background that the thing is embedded in.

So the thing's environment.

And so the question becomes, what does it mean to say that that thing remains, retains its thingness, its identity over time for that observer?

Well, that means that that observer is able to maintain this distinction between bits that communicate information about the thing and bits that communicate information about the thing's environment.

So this becomes a cognitive question about the observer.

So it's

The question, how does a thing maintain its identity, is replaced in this way by the question, how does an observer of a thing maintain the thing's identity for himself or for itself?

How does the observer maintain the ability to lock in on this thing as distinct from the environment?

And so let's go to an example, right?

Think about motor babbling in an infant.

So here's this infant.

She's lying on her back.

She's waving her arms and her legs around.

And what she's doing in this motor babbling is figuring out the relationship between the motor system and the visual system.

So among other things, she's figuring out that something that she sees, some visual stimulus, is actually part of her body.

It's a hand.

And that at a certain moment, she begins to be able to control what this hand is doing.

using visual and motor feedback.

So here's a living example of an observer fixing the identity of an object.

And what we're trying to do here in a sense is use this as to insist on thinking about

Beinghood always in this observer-specific way.

So I noticed that Carl just raised his hand.

So let me turn it over completely to him to carry on with that or contest it or give a different perspective.


SPEAKER_01:
No, I don't want to contest it.

I want to fully endorse that.

And if you just...

look at the free energy principle as classically formulated, it is exactly along the lines of the thingness being an attribute of the observer.

So if you just start off with the notion of a Markov blanket that stipulatively defines thingness, how is the Markov blanket defined?

Well, it's defined by the conditional independence between the dynamics on the inside versus the outside.

But of course,

those dynamics are not observable once they're on the inside from the outside by definition.

So you're immediately imputing some magical observer that could actually recognize that independence before you've even defined what a thing is.

So I just wanted to, in an abstract and possibly even philosophical way,

reinforce this notion that thingness is an attribute of something that could possibly observe it, even from the classical perspective of the free energy principle predicated on the notion of a Markov blanket that is in turn defined in terms of conditional independences or disentanglement.

That example of motor babbling, I think, is again a lovely illustration of observation underwriting thingness, and in this instance, a highly anthropomorphised thingness, namely selfhood, to develop the hypothesis or the explanation

for my holographic screen or all the information on my Markov blanket, that a plausible parsimonious explanation is that I am a thing, I am a self, I am me.

It actually takes months, if not years, to fully actually arrive at as you're an infant, slowly discovering that I am different from mum, and perhaps mum is different from her background, and ultimately, perhaps I am something like mum.

So the emergence of thingness, I think it is quintessentially observer-dependent.


SPEAKER_04:
both mathematically and in terms of developmental psychology great thank you oh did you want to add up sorry let's try and lower my hand it does that by itself i think oh how lucky the technology

So maybe I will put this back to Carl.

Maybe you were talking earlier, before Jim started talking, about the rate of information flow and the rate of belief updating.

And so I just wonder, I've been curious about this actually,

just in terms of human information, like are we really bombarded with more information now than we used to be?

And is this like influencing our rate of cultural change, cultural evolution?

Because as a culture, like we update our beliefs more often.

Do you think that these two things are proportional, the rate of information flow and the rate of belief updating?

Something I've been thinking about.


SPEAKER_01:
Yes, no, I'm sure that's absolutely right.

And right, I think, at every scale of analysis.

So you could look at it from a point of view of culture and the availability of information on social media and globalisation at a cultural scale.

But you could also, I think,

bring us back to motor babbling, you know, the degree of bleep updating an infant has to do is clearly going to be much greater than somebody of my age.

So, you know, time will, a year in the life of an infant will not be the same as a year in the life, in my life, you know.

And I think that that is a direct reflection of the degree of belief updating.

Interestingly, that question touches upon the separation of time scales, which I think underwrites everything that we're talking about here.

So just to stand back again and go back to some of Chris's early observations about the remit of the paper in question,

It was really at any given but not specified scale where we could assume that we didn't have to worry about the separation of time scales.

From the point of view of a cognitive neuroscientist, for example, what we're talking about is inference, where inference in this instance would be read as a kind of reading and writing.

onto this holographic screen, or if you want a classical perspective, the updating of active and sensory states on the Markov blanket.

But under the assumption that

other things that change at a slower timescale are not changing.

And those other things might be, for example, at a slower timescale, they might be learning.

So you've got this notion that the formalism at hand, the reading and writing at hand from the quantum information theoretic perspective,

could be a very good metaphor for inference over a short time period where things are not changing very quickly from the point of view of the things to be learned.

So within a few seconds or minutes of the life of the zygote.

acknowledging that you could apply the same apparatus and the same mechanics at a slower time scale that would account for the transition from a zygote to a neonate as it learns.

So the difference between inference and learning, I think, is a nice, common, sensible example of this separation of time scales.

So when you ask about cultural information available, I think you're probably talking about a

a scale of learning um and absolutely you know when we learn we update our beliefs there it will be from a classical perspective a statistical manifold that um holds those beliefs and you're literally moving over that um and with respect to an another scale um then the degree of belief updating the degree of asking questions the degree of rewriting um will certainly be you know

be a function of the amount of reading and writing that's going on.

And I can imagine that somebody who has access to Wikipedia or news or social media is doing a lot more read writing than I was, say, in the 1960s.


SPEAKER_09:
Okay, you're muted, but to me, Blue?

Yes, to you.

So, Carl, you made a fascinating point about connecting the motor babbling and taking it into the informational and the belief updating domain into the adult, where we're kind of motor babbling on social media in a way that has different dynamics than before.

And people often give the example of like, well, a year feels longer for a seven-year-old because it's a seventh of their life.

But then for the 80-year-old, it's an 80th of their life.

So that piece of the pie is smaller versus bigger.

And that is implicitly the fallacy of the observer independent time.

as if there was a linear chart that was being linearly divided by some static observer whereas another way to look at that or an alternative model is like the rate of development or experiences or change or novelty or continue to explore and don't worry i'll mute james oh he's muted on the live stream so it's all this um that uh rate of belief updating

could set that internal kairos, that biological time, like that biological information geometry.

And so it connects the cognitive sciences and also the statistical perspectives that have been discussed to this time question, which as also brought up earlier, does so much work in so-called dynamical modeling.

And it just makes me think about how there was a movement from the observer independent synchronous

into the relativistic which prepared and almost like prepared the um partition to exist so that all these other domains like the holographic principle could come into play so my question for any of the authors is where does the qrf come into play and how does this relate to discussions of pluralism and polycentrality


SPEAKER_02:
Awesome question.

Jim, did you want to answer?


SPEAKER_07:
I wonder if you can bring up a diagram there in the paper.

Let's see.

Yes, it's diagram 34.

Well, there you have configuration that relates the QRF to the Cocoa Cone diagrams.

And we say here that, let's see,

Well, here we focus on co-deployable observers when this diagram commutes.

It's interesting, of course, when the diagram doesn't commute and then you have a kind of intrinsic context, reality.

So,

I think what this is saying is that there are certain elements of information represented by the logic of the distributed system in question that are

kind of ambiguous, or rather they're manifest in classifiers and infomorphisms that aren't defined, or the co-limit, the bold C at the top there, does not exist.

So this is a very useful criteria, I think, to distinguish between sort of empirical models, which the best part are based on code deployable variables.

and what happens when quantum contextuality occurs.

That is diagram 34 doesn't commute.

So there's some sort of discrepancy there in the mechanism of the quantum reference frame relative to the dynamics of

the Markov blanket or the holographic screen underlying it.

So there is some kind of distortion in meaning that results in this sort of intrinsic contextuality.

I don't know if that's a satisfactory answer at all, but


SPEAKER_02:
When this breaks down.

I haven't seen that before.

Sounds good.


SPEAKER_07:
I'm sorry.

There's another local district.

It's all good.

Go ahead.


SPEAKER_04:
Chris, did you want to add on to that or make another comment?


SPEAKER_00:
Well, I wanted to go back a little bit to the basics of what a quantum reference frame is because this is a concept that is both very familiar and I think a little bit unfamiliar.

And the term was only coined back in the 1990s when the idea of a quantum measurement was associated with the idea of

the units in which the measurement is made and what are the requirements for a measurement being meaningful and we're going to get right back to the same issues we've been talking about with respect to time and thingness but a reference frame is just a way of giving meaning to an outcome

And it's what allows outcomes to be compared.

So if I have an apparatus like a meter stick, I can go around and measure lengths.

And as long as I believe that my meter stick maintains its identity, as long as I believe that my meter stick remains the same thing, then I can compare the lengths that I've measured.

But if I don't believe my meter stick remains the same thing, then these length comparisons are meaningless.

So the notion of a reference frame is intimately tied up with this notion of meaning, which in this free energy principle context is really a notion of actionability.

Can I use what something means to guide my actions and my decision-making?

So this notion of a QRF becomes a notion of giving an outcome value that I obtain and write in memory comparability with other outcome values that I maintain and write in memory.

And we're very used to, as human beings, we have this incredibly complicated sensory and inferential apparatus that involves many, many degrees of freedom and many, many values that are distinguishable.

So I think if we want to think about something like the role of reference frames in perception, it's

It's better to think in terms of something like E. coli.

So think of a much simpler kind of system that can measure fewer things.

E. coli can measure concentrations of salt and sugars of various kinds.

And those concentrations are meaningful to it.

those concentrations allow it to make decisions because its internal biochemistry assigns them an actionable meaning.

So chemotaxis in E. coli works because the level of phosphorylation of a particular chemical

remains reasonably constant.

That's the reference frame.

And concentration levels can be compared to each other as long as that phosphorylation state remains reasonably constant.

That reference frame is meaningful.

And so chemotaxis makes sense because E. coli has, if you will, a slow clock.

The phosphorylation state of QiY.

that assigns, that confers meanings upon its outcomes.

So now we can spool back to what Jim was saying about code deployable reference frames.

Reference frames that I can use at the same time that I can obtain information from simultaneously have to commute.

This is what Heisenberg's uncertainty principle is about.

And if I can't use them at the same time, it's because they don't commute.

If I use them in different orders, I get different answers.

And what that means is that by switching reference frames, I'm effectively changing the calibration.

I'm effectively changing the meaning that I've assigned.

And I think...

A very beautiful place to see this is a paper that we referenced here from Cervantes and Zafiroff called the Snow Queen Experiment.

The paper's called Snow Queen is Evil and Beautiful.

It came out a few years ago.

And basically this paper is demonstrating quantum contextuality in human decision-making.

And it's doing it by exploiting differences, context-dependent differences in the meanings of words.

So it's perfectly pointing out this relationship between meaning and co-deployability and hence context-dependence in human cognition.

So that's a paper I'd very much recommend reading.


SPEAKER_04:
Thank you so much.

Carl, did you want to comment on that also?


SPEAKER_01:
I'm aware that Stephen also has a question, but just to follow up on that beautiful deconstruction, this is more of a question, but I'm going to state it as an observation.

This notion of the QRF as being fundamentally getting measurements right or metrics right,

would be understood from the point of view of inference, technically, as getting an internally consistent metric correct.

And the metric we're talking about is a Fisher information metric.

So if you look at all reading and writing or action and sensation from a classical perspective,

as just trying to minimize some divergence or some prediction error.

What you are saying is that you are trying to find an internally consistent metric in an information space that makes sense of your active sampling, querying the world.

So it just sounded to me as if what Chris was saying was exactly getting the measuring stick

at least behaving consistently.

You'll never know whether it's changing its length or not, but at least if it behaves consistently in relation to what you can see of the world, the way that you can query that world actively, then that's good enough.

And that, of course, is one way of simply stating that the name of the game in Active Imprints is just to minimize your prediction error, where you are now reading prediction error

as a KL divergence.

And of course, the information length or the amount, the measurement of movement, the metric is just a path integral of infinitesimal changes in KL divergence.

So it's been from a mathematical perspective on the perspective of information geometries.

There's a beautiful link here.

between aligning your QRFs and getting your generative models right to make the most sense in the sense of minimizing or making as internally consistent as possible all your information metrics.


SPEAKER_04:
Awesome.

Thank you.

Before we go to Stephen's question, we have a question or two from Amber Aguirre in the live chat.

He asks, does the existence of a holographic screen slash Markov blanket automatically yield a statistical manifold for the belief state of the agent?

And is there a canonical renormalization procedure for a quantum mechanical system?

If there is one, how would the core screening translate into the statistical manifold?


SPEAKER_00:
I can take one whack at that, which is if I'm coarse-graining something, if we think of this in terms of QRFs, then I'm deploying a different reference frame with lower resolutions.

So in a sense, what's being renormalized is my representation of the units that I use to distinguish states of the world.

So if I have a meter stick that's made of stainless steel,

that has millimeters indicated on it, then I can go around and make measurements in millimeters.

But if I coarse grain to a meter stick that only has half centimeters indicated on it, then I've lost a factor of five in resolution.

I've coarse grained by a factor of five.

And so I've in effect rounded off all my outcome values.

And now my function of comparing outcome values is differently defined because it has to work at this lower resolution.

And if I want to compare my previous values to my later values,

I have to incorporate this explicit roundoff function into my comparison of values because I've changed my QRF.

So, coarse graining, which of course couples to scale changing from the very beginning of our conversation, can be thought of as a transition from

a QRF with some grain size to a QRF for a similar degree of freedom with a different grain size.

And that has ripple effects through everything, including the free energy cost of memory.

If I'm writing two-bit numbers into memory, then that's a lot less expensive than writing five-bit numbers into memory.

So this raises another aspect of this question of renormalization, which is how does it influence, how can I use it

to solve the trade-off problems that arise if I'm an organism with a limited fuel supply, which of course I am, going about the world trying to gather, preferentially gather the information that's going to be most useful to me at the lowest possible cost.

So again, we get back to this

he issue in all of active inference of balancing the risk of exploration against the reward of new information.

And coarse graining is intimate to that.

And it involves the choice I make of the tools that I'm going to use to make measurements.

and how that choice impacts my need to allocate computational resources and energy resources to what I'm doing too.

To the complex task of getting new information while at the same time surviving.


SPEAKER_04:
Awesome.

Thanks.

Daniel, did you, oh, Carl, do you want to speak to that first?


SPEAKER_09:
perhaps stephen should ask but then i can come back to to what just said yeah i'll um make a note and then carl would be great to have your response then steven's your question so to transpose it into statistical parametric mapping the true classics there's a mapping from the measured voxels

which are being defined by the resolution of the EEG or the fMRI machine, that's the holograph.

That's like what the experimental apparatus is providing is the measurements.

Those are the O in our partially observable Markov decision process.

It's the predicted value in the regression.

And that space, if it's 128 channels in the EEG and they're getting this sampling rate, that is the state space that's being discussed.

And that's being transposed or mapped or connected to a different statistical space with different features.

And rather than the voxels,

there's like the mapping onto resels.

So Carl, that's why I'd appreciate if this is true or not, but those are the resolution elements and that it can be like a coarse graining.

And in that stretch meter stick,

and different space there's a lot more statistical inference options available that are just not accessible at all from the um voxel coarse graining which is noisier and there's a signal enrichment in the coarse graining to the resolves that's appropriate and then that gets um

juxtaposed with the contrasts, which is the experimental design of the scientist.

Like you couldn't differentiate A versus B category if you only tested A. So the experimental design, and that's where like the statistics actually comes into play.

And then just as Chris said, there's the organism with limited food supply.

And then there's like the scientist with limited time, attention, computer storage, processor time, funding,

maybe seven billion people like it just there's plausibility and so that's where this operational code deployment connects with experimental design and so many of the patterns and analogies exist in the purely classical domain of the parametric statistics like spm in the classical physics and the flow and the information flow area that's coming into play and also these more recent in quantum areas so carl would love to hear your thoughts on that


SPEAKER_01:
Yes, of course, I'm mindful that Stephen's still got his hand up.

Perhaps I'll speak to that and then Stephen can ask a question.

But yes, I think these questions and Daniel, your responses are touching and Chris is touching on something really fundamental.

So just to answer the question from the point of view of Markov blanket, yes, it does automatically yield a statistical manifold.

or at least it looks as if there is a statistical manifold if you can observe the behaviour of the thing that you are observing.

The second thing, question, which we've been focusing on, the core screening, I think that's a really fundamental thing.

I would argue, though, not to conflate too much the notion of renormalisation, which would be, I think,

more a question of how you get from one scale to another scale with the notion of coarse graining, which could be read just simply as the degree of quantization or choosing the right resolution element, the right coarse graining of a given QRF at any given scale.

You could argue that you could contextualize it, but I think there's a more fundamental argument here.

And that's the argument that there will be

from a statistical perspective, an optimal coarse graining for any given holographic screen or exchange over a Markov blanket.

And that's the one that statistically maximizes the marginal likelihood

which you could, if you like, relate to this sort of getting the measurements as internally consistent as possible, maximizing the predictability of the next measurement you make.

That goes hand in hand with a maximization of the marginal likelihood or a minimization of the variation of free energy.

That is important to remember because the marginal likelihood can always be expressed as accuracy minus complexity, where complexity scores the degrees of freedom.

you're burning up, if you remember about the bits on the holographic screen, you need energy to, from a classical perspective, represent.

What that translates to in a very simple way in terms of probability distributions and functionals is that in maximizing the marginal likelihood or the evidence for your

aligned or model of the world you are to maintain a certain degree of accuracy trying to minimize complexity so you're using as few degrees of freedom your core screening to the extent you can get away with

while maintaining an accurate account of, say, the sensory states or the perceptual space on your holographic screen.

If you now, from the point of view of active inference, just think about, well, what would that sort of accuracy and complexity look like in terms of the consequences of action, you get exactly what Chris was talking about, which was the expected complexity

being the risk usually, and I mean risk in the sense of an economist or Bayesian decision theorist.

It is literally mathematically the same thing.

The expected accuracy now becomes, you know, a sort of,

expected utility or the expected log evidence, the likelihood, the expected likelihood that you would get if you committed to this kind of course grading.

So this course grading is absolutely fundamental.

It's really interesting to go back to

all the sources where the coarse graining starts to make a difference.

And I'm thinking here, coming back to Daniel's notion of resolution elements, you know, you find this in sort of Wiener filtering, you find it in the matched filtering theorem, there is an optimal level of smoothing or blurring

for any given kind of data about which you want to make an inference or you want to assimilate.

You see this in formulations of universal computation that themselves are based upon Solomoff induction that get you right back to Kolmogorov complexity and compressibility.

So you see it in the drive that underwrites all formulations of universal computation.

to maximize compression by minimizing the complexity of your message passing, getting it as efficient and as simple as you can.

And of course, the best way to make your account as simple as possible is to coarse-grain it.

And I think you also see it in the sort of the different levels of cognition in psychology and in neurophysiology in the sense that the higher level, more efficient ways in which we align our QRFs or optimize our generative models always lead you to a very coarse grade quantization of the world.

So, for example,

I represent, I am sure in my brain and some neural populations, the fact that I am in my study.

I do not represent that in terms of very fine grained X, Y, Z coordinates down to the millimeter.

I represented, no, I'm in my study.

I carved the world.

at its joints, and those joints basically demarcate the kind of core spreading I bring to the table.

There are all sorts of arguments you can pursue here to vindicate this, the very existence of receptive fields.

Tiling a sensorium in a way that is covered by discrete quantized receptive fields tells you immediately

that the way that the brain represents stuff on the inside, if you can look at it through brain imaging, is in this quantized fashion, this coarse-grained fashion.

And that coarse-grainedness and the size and the deployment and the organization of those receptive fields has been optimized at many different temporal scales.

So I'll just give you a few takes on coarse grading as something absolutely fundamental to the structure of a generative model.

or to the way that we align our QRFs to make sense of the stuff on the holographic screen, or at least when we are reading from that and sense-making in that sense.


SPEAKER_04:
Great.

Thank you.

Stephen?

Does anyone want to ask a question?


SPEAKER_08:
Yeah, thanks.

No, this is really cool.

I was going to come back a little bit to now and the past and the idea that you mentioned about the way that we interface or the assumptions of the interface then changes how we resolve the thing and the background sort of situation, situatedness of that thing.

So it's like... So that could then also...

and this is the question in a way, is how can that also, because if the way that that thing and the assumptions are made sets up the meaningful influence, the action policy selection, and the way things are thought about, how can that itself change the way the brain or the way that organisms look at what things are?

For instance, in the modern world, we do see everything as things.

We carve things up.

We see things as separate, like an individual is separate.

Whereas if you go back to sort of indigenous approaches, time for them was cyclical or followed the river going down a hill.

So there was a complexity focus in a way.

It was almost like the mountain car problem of finding the landscape of meaning

rather than defining the thing and what it is and carving it.

So I'm curious around is this, how much is the way that the quantum reference field, you know, translated into thinking about what that thing is and how that, you know, compares to say our Western cognitive frames, which kind of,

We measure everything because we can, because basically we can control our environment so heavily.

We can make things very accurate.

Whereas in the past, it might have been the Kalahari Bushmen or the people in the middle of the Amazon that somehow go with the landscape of complexity.

So the way that that is understood is going to be different.

So I'm just curious about how you think about that ways of knowing.


SPEAKER_04:
Great question.

I wonder also if that ties into the individuality of temporal scales, like you mentioned the cyclical time and back to Franz's question maybe earlier about how

a thing over time that stays a thing over time scales with the scale of the time or with the cyclical nature of the time.

Or it reminds me back of when we had Chris and Shana here talking about like flattening time.

Maybe that was just Shana's talk, but yeah.

Anyway, I don't know.

Do you guys, anybody have any temporal comments?


SPEAKER_09:
Just that there's an analogy to the synchronization of clocks, which is a huge part of computer science and of quantum and other areas.

That question of synchronizing clocks seems to have some relationship to cognitive model, not naive congruence in terms of synchronization, but in terms of time zone coordination.

So if we're in the same time zone, then there's a comparability, but for cognitive states.

And so I think that will be something awesome to explore in

next week's session and of course in the long run about what that kind of semantic and rhetorical and narrative alignment that isn't congruence but reflects a broader frame, which actually the differences within the frame are the search through that field.


SPEAKER_04:
Thanks.

And in the interest of temporal comments, I think maybe does everyone just want to go around and give a final thought and maybe an idea of what they would like to see in next week's discussion?

And we will start with Stephen.


SPEAKER_08:
Yeah, thanks.

Lots of interesting things.

I suppose I'm curious about the way the collapsing of the quantum reference frame might scale differently, or might the assumptions that lead to the collapse could inform different scales of interactions and mixing and blending of data.

behavior and Yeah, digesting a lot of stuff that's come up.


SPEAKER_03:
So yeah, thank you for for the opportunity to talk to you Awesome Daniel anything you want to see happened next week?


SPEAKER_09:
We always think of the dot zero, it's kind of like dropping into the skate park or the bathtub.

And so that was like a microcosm of all the threads that get brought together.

And then in dot one, I feel like we opened up so many

areas of possible connection.

And so I'll just look forward to the second repeated measurement of this semantic or regime of attention space and see how the continued interaction and engagement updates our generative model past the dot to even.


SPEAKER_03:
Awesome.

Carl or Chris, do you want to give a thought about what you'd like to unpack in the next question?


SPEAKER_01:
Should I go first?

Well, Chris thinks of something very clever to say.

So following on from Daniel, to a certain extent, Steven's question as well.

I'd like to hear about the principle of unitarity, entanglement, and its relationship to this notion of synchronization and the mutual alignment of QRFs between things that are coupled.

So coming to

the regimes of attention.

I think there could be a very transparent link there in the sense that

Contextualizing your QRF, I think for a cognitive neuroscientist would be getting your attentional set right and possibly implicit coarse graining that comes along with a particular attentional set.

Taking that notion to tending to others and synchronizing and communicating and living in a universe that is also composed of things like me.

I think brings some fundamental questions about classically generalized synchronization,

quantum theoretically entanglement and enabling that entanglement in the right kind of way via aligning your QRS and getting the coarse graining right in sympathy with your partner.

So I'd like to hear about the principle of unitality, entanglement, and how it relates to classical notions of communication, mutual inference, and generalized synchrony.


SPEAKER_04:
Well, thanks for stealing what I was going to say.

I was also going to say how there's one like the final concluding part of the paper talks about how the FEP is asymptotically the principle of unitarity.

And I really would like to dive into that more and maybe any potential implications of that.

And now Chris gets the final and cleverest word.


SPEAKER_00:
Okay, this won't be terribly clever.

What I was going to do was to recommend a lovely paper by Alexi Grinbaum that we refer to in this paper.

And the closing sentence of that paper is that physics is really about languages.

And I think this ties in nicely to some of the issues that Stephen was raising in his last question, which I found very interesting, about how do conceptual schemes evolve at the cultural level?

And in hearing about...

the whole issue of reference frame synchronization or reference frame alignment and how that relates to entanglement.

This really is a question about language.

And that paper by Alexi Grinbaum, I think makes this point very, very beautifully.

So that would be a nice thing to look at at some point.

Let's see, this is, that's probably a newer paper by him.

The one I was referring to is titled something like how device-free, how device-independent methods change the nature of physical theory or something like that.

It was published in Studies in History and Philosophy of Science 2017 or something like that.

Okay, anyway, thank you very much for this.


SPEAKER_09:
Yeah, this was very interesting and engaging and really appreciated everyone's participation and Blue's facilitation.

So we'll look forward to seeing anyone just drop in or out when works for you next week at the same time in number 40.2.

So till the next measurement.


SPEAKER_08:
Thank you.

Take care.
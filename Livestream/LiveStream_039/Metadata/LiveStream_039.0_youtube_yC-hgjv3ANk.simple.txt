SPEAKER_00:
All right.

Hello and welcome everyone to ACT-INF Lab live stream number 39.0.

It's February 25th, 2022.

Welcome to the ACT-INF Lab.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links here on the slide.

This is a recorded and an archived live stream, so please provide us with feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll be following good video etiquette for live streams.

If you want to learn more about .coms, which is the organizational unit that puts on all these live streams and many other products or any of the other activities in the Actinf lab, go to actininference.org.

All right.

Well.

Today, in Act-Inf Stream number 39.0, the goal is to learn and discuss this really awesome paper called Morphogenesis as Bayesian Inference.

A Variational Approach to Pattern Formation and Control in Complex Biological Systems.

It's by Franz Kuchling, Karl Fristen, Georgi Georgiev, and Michael Levin from 2020.

And just like all the .Zero videos, and indeed all our videos, it's just an introduction to some of these ideas.

It's not a review or a final word.

That being said, we're going to go over the aims, claims, abstract, roadmap, keyword, etc.

of the paper.

And then with a focus on the first parts of the paper and less so on the formalism and more on the big ideas, we're going to go over the paper and the figures.

that will put us in a good position cognitively or morphologically we might even say for the discussions in the coming weeks 39.1 and 39.2 when we'll get to unpack this so if you would like to it'd be awesome to have you participate live or if it's past 39.2 you can still ask questions all right we'll start with introductions and warm-ups we can each

say hi, and maybe one thing that was exciting or interesting about the paper, which I'll reserve for after you, but I'm Daniel, I'm a researcher in California, and Dean, thanks a ton for joining.


SPEAKER_01:
thanks Natalie I'm Dean I'm here in Calgary and what really got me excited about this was the idea that they talked a little bit later closer to the end we may not even talk about it today is the role that epigenetics might play in terms of how cells behave and and what sorts of things we might be able to turn in terms of

simply what we see in biological form into some sort of static statistical prediction device so i'm really excited about that awesome yeah it's a complex paper about complex systems so we'll see what we can get done today but


SPEAKER_00:
I'm really excited to bring active inference into this domain, at least as far as our discussions go into the morphological and the morphogenetic areas.

And it's just bringing like another lens, another way that we can apply active inference as a filter, another set of phenomena that we're bringing into this active inference realm.

so we'll just go right into the big question so i'll read this and then feel free to give a thought the big question or at least one way to put one of the questions is how can we find an integrative path between math and morphogenesis morphogenesis means form like physical form morpho and genesis meaning how it arises so how does physical form arise and how is that related to math

And to kind of unpack that, here's some quotes.

The molecular mechanisms, which are like signaling pathways of all different kinds, underlying development and regeneration in biological organisms have undergone a tremendous amount of knowledge gain in the last several years due to genomics and biotechnology advances.

So that's kind of the one side with the biological.

And then from the mathematical and the formal, the emerging field of the free energy principle in pattern formation, not always morphogenetic patterns, but could be cognitive patterns, rhythmic patterns, provides an essential quantitative formalism for understanding cellular decision-making in the context of embryogenesis, regeneration, and cancer suppression.

So if that's the if,

Then how might the mathematical formalism reveal new understandings about developmental change in evolution and for designing new interventions in regenerative medicine settings?

So what did you think about that?


SPEAKER_01:
Well, I just love any time that two things that seem disparate or distant are brought together, because I think all sorts of good things can happen.

And I'm going to do a little shout out.

That's kind of what I would like to see if we can organize in August with a couple of other papers that don't start out necessarily appearing to be close or proximal, but by bringing them together, all kinds of interesting things can happen.


SPEAKER_00:
Awesome.

Okay, to the main aims and claims of the paper.

So the paper is in Physics of Life Reviews 2020, and we already mentioned the title and the authors.

Just to select a few aims that the authors represent, and then a couple of key claims that are relevant for the paper.

So as far as aims go, one of their aims is to introduce an overarching concept that can predict the emergence of form and the robust maintenance of complex anatomy.

Another aim is to

derive the mathematics behind Bayesian inference and use simulations to show, simulations that are consistent and within the Bayesian framework, to show that the formalism can reproduce experimental top-down manipulations of complex morphogenesis.

And then just a few of their claims, more of which we'll be unpacking in the coming minutes.

Classic, i.e.

dynamical systems and analytical mechanics,

Approaches such as last least action principles are difficult to use when characterizing open, far from equilibrium systems that predominate in biology.

So that's kind of a big challenge that's come up again and again, which is we might have really great equations for a pendulum, maybe even a pendulum with friction, but how about an active pendulum that's doing strategy and trying to run away from you?

A bit harder to put those same equations on.

The Bayesian inference framework treats cells as information processing agents, where the driving force behind morphogenesis is the maximization of a cell's model evidence, which is to say it's reduction of uncertainty relative to a generative model, which is what we're gonna be unpacking.

And we had previously read this about how they're applying free energy principle to morphogenesis

when it goes well, like during embryogenesis and results in a zygote becoming like an adult life form, as well as all of the ways that it's maintained during that stationarity of healthy life and including cases where it might be relevant to even think about making interventions like medical settings.

So those are a few of the aims and claims.

Anything to add on it?


SPEAKER_01:
Yeah, just real quick.

It's really interesting that they said top-down, and then we're going to see some slides where there's a top-top and a down-down.

So stick around, because there's actually going to be something kind of interesting happening here.


SPEAKER_00:
Okay.

How about you read the first half of the top half of this abstract slide?


SPEAKER_01:
Yeah, sure.

Recent advances in molecular biology, such as gene editing, bioelectric recording and manipulation, and live cell microscopy, using fluorescent reporters, especially with the advent of light-controlled protein activation through optogenetics, have provided the tools to measure and manipulate molecular signaling pathways.

with unprecedented spatial temporal precision.

This has produced ever increasing detail about the molecular mechanisms underlying development and regeneration in biological organisms.

However, an overarching concept that can predict the emergence of form and the robust maintenance of complex anatomy is largely missing in the field.

Classic, i.e.

dynamic systems and analytical mechanics, approaches such as least action principles are difficult to use when characterizing open, far from equilibrium systems that predominate in biology.

Similar issues arise in neuroscience when trying to understand neuronal dynamics from first principles.

So they're basically covering their basis here by making sure that we understand this is open systems.


SPEAKER_00:
Yes.

Increases in tools in biological systems like genomics, microscopy, optogenetics are helping us reduce our uncertainty about the what, but without a bit more information about the how and the why, etc.

That's the overarching concept that's largely missing.

So we can grind up the cells and sequence the genome.

We can extract the RNA and we can even tell what the ratio of different RNA molecules is.

But it turns out that that type of information alone doesn't help us understand why two related species or the same species in two different contexts or the same genome in two different tissues or the same tissue in two different environments has such different morphology.

Okay, and then the second half.

In this neurobiology setting, a variational free energy principle has emerged based upon a formulation of self-organization in terms of active Bayesian inference.

The free energy principle has recently been applied to biological self-organization beyond the neurosciences.

For biological processes that underwrite development or regeneration, the Bayesian inference framework treats cells as information processing agents, where the driving force behind morphogenesis is the maximization of a cell's model evidence.

This is realized by the appropriate expression of receptors and other signals that correspond to the cell's internal, i.e.

generative, model of what type of receptors and other signals it should express.

emerging field of the free energy principle in pattern formation provides an essential quantitative formalism for understanding cellular decision making in the context of embryogenesis regeneration and cancer suppression okay and then if you read the first um two parts here on the second slide yeah no problem in this paper we derive the mathematics behind bayesian inference as understood in this framework and use simulations to show that formalism can reproduce


SPEAKER_01:
Interesting.

Experimental top-down manipulations of complex morphogenesis.

First, we illustrate the first principle approach to morphogenesis through simulated alterations of anterior-posterior axial polarity.

The induction of two heads or two tails, ah, spoiler alert, as a planarian regeneration.


SPEAKER_00:
Don't you hate it when the abstract spoils the paper?

Then we consider aberrant signaling and functional behavior of a single cell within a cellular ensemble as a first step in carcinogenesis, the formation of cancer, as false beliefs about what a cell should sense and do.

Inference, perception, action, inference.

we further show that simple modifications of the inference process can cause and rescue mis-patterning of developmental and regenerative events without changing the implicit generative model of a cell as specified, for example, by its DNA.

This formalism offers a new roadmap for understanding developmental change in evolution and for designing new interventions in regenerative medicine settings.

Speaking of the new roadmap, let's look at the roadmap.

Kind of cool that they actually say that it is providing a roadmap.

So we've talked about guides.

We've talked about roadmaps.

Where do you see all of that before we look at the section titles?


SPEAKER_01:
Well, where a guide, I think, basically gives sort of gives you

The parameters, I think a roadmap is a lot more specific in terms of the sequencing, but that's just my interpretation of it.

I think other people can interpret roadmap differently in terms of sort of the relational aspect of how the different parts come together.

And we've talked about that in other live streams, but I don't want to get bogged down in this part right now because I think we got a lot of stuff to cover here.


SPEAKER_00:
Cool.

Yeah.

So the paper begins with an introduction to Bayesian inference.

Section two is mathematical foundations that are building on Bayesian inference.

So going from kind of Bayes' theorem to several dynamical systems representations, some of which we've discussed before, others which will push it into a little bit of a new area.

Then the key

contribution and the focus of the paper is modeling morphogenesis and so instead of just presenting bays and then variational bays and then free energy minimization on a bacterium like we saw axel constant do in live stream number 34 we're going to take that momentum and move it towards

application of modeling in morphogenesis and that's where we'll talk about like what model they construct and how it relates to some of the formalisms that were brought up earlier and then there's a discussion and conclusion section which we won't go too much into today in the dot zero but it's going to be awesome to go into it in the dot one and dot two because there's a ton of awesome content and material

So here's the keywords that were listed and we won't go into most of them just to say that they're gonna come up in this discussion.

We have free energy principle, Bayesian inference, morphogenesis and developmental biology regeneration.

Kind of got some mathematical stuff we've seen before.

We have a triad of developmental biology, which broadly understood includes embryogenesis, as well as aging and cancer and all these other developmental things that happen during biology and top-down modeling.

And maybe we'll see what that means coming up.

Okay, well, to get right into it,

Section one is an introduction to Bayesian inference.

So here's a nice picture by Sasha and here are the opening lines of the paper.

They wrote, and then feel free to give a thought.

Evolutionary change results from mutations in DNA and selection acting on functional bodies.

Thus, it is essential to understand how the hardware encoded by the genome enables the behavioral plasticity of cells that can cooperate to build and repair complex anatomies.

Indeed, most problems of biomedicine, repair of birth defects, regeneration of traumatic injury, tumor reprogramming, et cetera, could be addressed if prediction and control could be gained over the processes by which cells implement dynamic pattern homeostasis.

fundamental knowledge gap and opportunity of the next decades in the biosciences is to complement bottom-up molecular understanding of mechanisms with a top-down computational theory of cellular decision making and info taxes movement towards information so dean what do you think what should we study and why and how is this related to active and fep


SPEAKER_01:
Okay, well, my background in genetics is near zero.

But what I did come away from with that last bullet in particular was one of the things that I beat like a dead animal, which is that you need a minimum of two.

And I think that the bottom-up sort of building blocks aspect of it with the top down, how do we get a better grip on what the signals are that are passing between all of those bricks was really interesting.

And so carrying on through,

I kind of wanted to open that up and see what that marriage looked like.


SPEAKER_00:
Awesome.

So speaking as someone who started in the science journey with genetics, pretty much,

It always was so interesting how genetics would frame itself as if the bottom-up information flow could just simply replace theory.

We'll sequence so many genomes, we won't need to speculate about evolution, or we'll have your genome, so we won't even need to ask about what medicine is best.

And that's kind of this bottom-up molecular understanding, which is vital, it's essential, it's important, it's changing a lot, but it's this bottom-up

subunit description, the what and the where of the subunits and how they touch and all of that.

And that is always met implicitly and increasingly explicitly with a top-down theory of cellular decision-making and infotaxis.

And so we're always kind of meeting in the middle with empirical wet lab biology, revealing these mechanisms and these findings.

and we're meeting that as researchers doing model-based science with what are being called here top-down modeling, top-down computational theories.

Okay, that's just how they open up and also set a challenge for the coming decades.

It's their expectation of the next few decades.

Then they provide a math roadmap, which is an awesome paragraph.

So they write, we lay out the mathematical foundations of a type of Bayesian modeling that they're going to use to simulate pattern formation.

So red, Bayesian modeling.

Orange, we start by identifying a Lyapunov function that can be used to analyze and solve any dynamic system using the fundamental theorem of vector calculus, i.e.

the Helmholtz decomposition.

So we're using Bayesian modeling and layering in what we'll talk about soon, the Lyapunov function.

green.

We use it to characterize the generalized flow of systemic states of the whole system in terms of conversions to a non-equilibrium steady state, a NES.

Next layer, blue.

We then introduce the notion of a Markov blanket that separates the external and internal states of the system.

And finally, purple.

We can then replace the Lyapunov function

with a variational free energy function to solve for the evolution of internal and active states, that's of the particle, and thereby characterize self-organization in far from equilibrium systems that can be partitioned into a cell and an external milieu.

That's the math that they go through.

That's the math rainbow.

Then sections after that apply this formalism to illustrate morphogenesis and neoplasia, which is the growth of new forms, neoplasia, using simulations.

And what's so cool is this trajectory from 400 to 700 nanometers, from the base,

through the vector calculus and flow states to the finding or the figuring out of a Markov blanket and then the usage of the Markov partitioning legitimated by the orange and the green steps to find a heuristic way to go about solving what was

needing to be addressed right in the beginning.

We've seen some variants of this trajectory multiple times.

And then in this paper, they study morphogenesis.

And so then they model morphogenesis.

But save for a couple of words, a different paper could have said, we do the Bayes thing, the Lyapunov thing, the Helmholtz, the Ness, the Markov, and then we're back to doing far from equilibrium strategy in online teams.

And then we applied it to online teams.

And so let's keep in mind, like the paper can be long.

There are a lot of equations, but where is this broader trajectory that's helping us fine tune our regime of attention and update our generative model about what we are doing here and how it's different than other approaches.

And then where is the part that's actually studying morphogenesis?

Just things to keep in mind.

Okay.

Anything to add on this?


SPEAKER_01:
Well, correct me if I'm wrong on this, but I think what we're doing is essentially taking, as you said, we can look at a variety of things.

We can look at the physical nature of ants.

We can look at cells.

We can look at online teams.

Whatever we're looking at, what they basically said is there is a parallel way of looking at the formalisms of

that exist in those contexts with that type of content.

That's essentially what this said to me.

There's a way of bumping it up into that sort of statistical mathematical realm.

And now we're going to walk you through what that is.


SPEAKER_00:
Great.

Okay.

So into the Bay, the Bay's area where I grew up.

Down by the Bay.

sitting by the dock of the bays all right oh i thought it's gonna say where the water well where the watermelons grow carry on base theorem rests on three basic axioms of probability theory and is used to relate the conditional probability of an unobservable event a okay so the vertical bar means conditioned upon or given and so what we really want to know is like the probability of something that we cannot observe

conditioned on what we did observe B. So you're never observing the lightning strike, you're observing photons on your receptor, and then you want to know about whether there was lightning in that cloud over a few miles away.

So we want to know the conditional probability of an unobservable event A, that's this left-hand side.

We want to relate that to an observable quantity B, that's what the data are,

And that's going to be calculated using the Bayes theorem as the probability of the data given the unobservable state, which is called a prior.

So that's like had to do with cats and meowing or in a previous example, or it has to do with like, if there were lightning, yes or no, what would be the likelihood of me observing photons?

multiplied by the probability of those unobservable states happening, which is of course a prior about them, but it could be set with empirical data, divided by how likely the data are, which is called P of B, the marginal likelihood or evidence.

Okay, so P of A given B is called the posterior because it's like after you've done your inference, you want the correct posterior, which is like updated beliefs about how likely the thing that you care about A is given what has come in.

Priors are like prior to this round of inference.

What are your beliefs about how likely A is?

And then the evidence coming in is B.

Okay, and the likelihood is how likely the data are under the prior distribution.

So that's Bayes' theorem, and we've talked about it multiple times, and I'm sure there's other awesome videos and courses that can explain it even better.

How are they using it here?

describe the dynamics of an ensemble of information processing agent like cells as a process of bayesian belief updating we need to relate the stochastic differential equations governing newtonian motion and biochemical activity to the probabilistic quantities above long sentence but

it's kind of saying we need to connect these variables, which are informational, they're probabilistic, they're statistical, to movement in space.

Movement in a Newtonian coordinate system, because we need these variables to measure spatial movement so we can study morphogenesis.

This is fairly straightforward to do if we...

associate biophysical states with the parameters of a probability density and ensure their dynamics perform a gradient flow on a quantity called variational free energy.

Variational free energy is a quantity in Bayesian statistics that, when minimized, ensures the parameterized density

converges to the posterior belief as we will see below so yes this is our first slide on bayes theorem and we did just kind of go from the total description of the variables and what is bayes theorem at a first pass to this introduction of the variational free energy so

It can be accordioned out and unpacked a lot more.

But what variational free energy is doing is helping us tractably converge to the appropriate posterior belief.

And that's its role here.

Okay.

Anything to add on this one, Dean?


SPEAKER_01:
Well, I'll ask you a question.

So if you were to talk to this to somebody for the first time, not people who've probably listened to other of our live streams, would you say that it would be dangerous to say that we're taking something that's kind of ratio-based and now using that to try to build a model of what our predictions are going to be based on?

Is that fair to say?


SPEAKER_00:
I believe so.

It's like one way to approach that.

I hope this would be accessible.

The ratio based part is on the right hand side.

And one could say, depending on like how much data you're getting and what you're trying to estimate, it could be very hard to go from observations to inference on hidden states.

pretty fair conclusion.

So what we could do would be construct a smooth approximatable model so that when we slide to the bottom of our model, just like the ball sliding down the hill, we would actually be doing the same thing as fitting this effectively.

So instead of like number crunching and putting everything into this equation and running the calculation, we're going to construct a bowl

which is the variational free energy that we can do gradient descent on that is going to converge to a really good solution.


SPEAKER_01:
Do we construct it or do they kind of tell us that we should assume it if we want to make it tractable?

Assume the bowl.


SPEAKER_00:
When we pick up this instrument, we're making variational free energy bowls.

Yeah, okay, good.

And we've seen this before.

In live streams 26, 32, and 34, we did some variance on moving from Bayes theorem and exact Bayes to variational inference, which uses this variational free energy value.

So those are a few times that we've seen this discussed before.

In neuroscience, the minimization of variational free energy is referred to as active inference.

We will see that when the basic condition for an inference type description of a system

namely the existence of a Markov blanket separating external and internal states, is satisfied, agents such as biological cells form into organized conglomerations based on their generative model of how their blanket states influence and are influenced by external states in the external milieu.

So the connection between Bayes' theorem and Markov blankets is the unobservables are external states.

The observables are blanket states.

It's a pretty fair model, whether it also provides like strong guarantees on optimal perception or optional action.

That's another discussion, but at the very least, it's a fair framing that external states are unobserved.

If you could observe them, they'd be observables.

Then they'd be more like B than A. So we're talking about a situation where we have some A's and we have some B's, and we're going to think about that in terms of a blanket formalism.

The dynamics of a system with a Markov blanket that self-organizes to non-equilibrium steady states can be described as a gradient flow on this computable variational free energy bound.

So we can set up the blanket and it turns out that we don't have to do the ratio number crunch.

We can do the bowl gliding down.

That's the beginning part on just Bayes that takes us from Bayes theorem to introducing variational free energy and doing gradient descent on variational free energy so that one can effectively essentially perform Bayesian inference.

Cool?

Perfect.

Okay.

Section two, mathematical foundations.

All right.

So section 2.1, stability and convergence in coupled dynamical systems.

And subsection 1, the Helmholtz decomposition.

So they write, the Helmholtz decomposition states that any sufficiently smooth, i.e.

possessing continuous derivatives, vector field F can be decomposed into an irrotational curl-free, so kind of straight up and down, putting the ruler and finding the slope of a hill, and

solenoidal divergence free vector field the divergence free solenoidal is like putting the ruler at ground level and then being able to make an iso contour around a hill because an irrotational vector field has only a scalar potential and the solenoidal vector field has only a vector potential we can express the vector field as

insert formalism two.

And just so people know, the upside-down triangle is called del or nabla, which means harp due to the shape.

So del phi is the irrotational vector field.

Del A is the solenoidal field.

We talked about this mostly in...

number 26 and 32, where we talked about the partition or the decomposition into a solenoidal and a gradient, as well as this housekeeping term, which was everyone's favorite housekeeping term.

So we're not having a housekeeping term here, but we're keeping in mind that it's not implausible that it's still there.

It's just that there was a whole paper coming out after this morphogenesis paper that talked about the housekeeping, et cetera.

So again, it wouldn't be surprising if a future morphogenesis paper or project

did have this housekeeping term, but 32 was actually a later paper.

So that might be a more advanced place or updated place to check out how the partition, I'm sorry, the Helmholtz decomposition works.

And here was a image from 26, just showing how you could have like a kind of hill

And in physics, you could have hill climbing and trying to get to the top, or sometimes it's easier to think about that as like the flip and the ball rolling to the bottom of the hill.

So whether it's the optimistic biologists talking about going up to the fitness peak, or whether it's the pessimistic physicists talking about going to the bottom of the energy well, it's the same idea that there's a particle on a landscape

And there's been a ton of work on passive particles.

That's not to say immobile particles, but particles that roll and get trapped.

And then once they're trapped, they stay, or they jiggle stochastically.

We're studying active particles.

And so that's where action selection comes into play.

Landscapes of energy with balls rolling down them, but they're kind of like jumping beans where they also have a few tricks that they can play.

Okay, anything to add here?


SPEAKER_01:
Just a reminder to people that that's a density that we're looking at, not a vacuum.

I know most people, like if I was to talk to younger people and they started imagining a ball rolling down a hill, they wouldn't necessarily confirm that it's an actual density that we're looking at.

So just reinforce that.


SPEAKER_00:
yeah like the bottom right here it's kind of like you're looking down at an ink drop and that's where it's densest is where it's darkest and then that is like a landscape where where it's darkest is highest right okay

Section 212, Lyopinib functions.

Okay, so Lyopinib functions have been used extensively in dynamical systems theory and engineering to characterize the stability of fixed points of a dynamical system.

Lyopinib functions are generally defined for smooth systems through the following conditions.

So L is going to be the Lyopinib function and X is going to be a point.

So a fixed point is going to be x star and a non-fixed point is going to be just regular x. So x star, the special points where if you are there, you're perfectly balanced.

So this is saying that at fixed points, the Lyapunov function evaluates as zero and it's not zero if it's not a fixed point.

And

the Lyapunov function change in L with respect to time.

It's below zero for all x. Equation 3a requires the Lyapunov function to be minimal for fixed points x star, representing local minima,

and b denotes conversions to these fixed points over time so what does it mean to have a lyopin value of zero above zero or below zero so if you're at zero just like it said with the fixed points that means that it's not moving at all a lyopin of exponent below zero means that things are converging towards

Lyopinib bigger than zero means diverging away.

Specifically, what it means is two points that are really, really close to each other.

Do they stay exactly the same distance away?

That's zero.

A negative lyopinib means that two points close to each other will converge.

So that's a convergence attractor.

And then positive lyopinib exponents, lyopinib values, mean that two points that are placed very close to each other will diverge away from each other.

I hope I have not oversimplified or made unforced errors about Lyapunov functions because it's obviously a really technical area.

and they write following 16 we can generalize this local lyapunov function of stability so zero equal fixed point positive equals divergence negative equals conversions to a global lyapunov function that plays the role of a potential function of any dynamical system and we won't go into it but it is the citation to this real she

paper from 2013, and they do some pretty interesting math.

So if somebody is more familiar with this, it would be awesome to hear about.

But what this is doing is it's taking the same dynamical systems perspective that we've been thinking about, about things changing through time, and then overlaying on that landscape

a stability landscape.

And so if we think about this hill on the top right, again, hopefully not being wrong or misleading here, at the very top of the hill,

it's like a fixed point it's an unstable fixed point but if one were perfectly balanced there we could just say um it could get knocked off by stochastic things which will come back later but it's perfectly balanced it's like a fixed point two points that were very close to each other on a hill two balls placed right next to each other would start to diverge as they rolled down the hill so that would be like a slight divergence

And so that is what the Lyapunov function as potential function is allowing.

It's allowing us to take the landscape of ball rolling, where gravity is the potential function.

The higher up, the more potential you have, and then it rolls down because of the dissipation of potential energy.

And they're going to...

show that any dynamics with lyapunov function so any smooth differentiable etc landscape has a corresponding physical realization a friction force a lorentz force and a potential function so we can think about this lyapunov function being related to potential functions like something that draws the system back that's where the negative lyapunov comes into play okay dean


SPEAKER_01:
Just a quick thing, Daniel.

When we were setting up the slides, between those two bullets, they talked about something called a saddle point.

Did you understand what a saddle point was previous to reading this?

Or do you know what a saddle point is now?


SPEAKER_00:
Yes.

I'm just pulling up the paper in the window.

Okay.

So people can see it.

Okay.

So yeah, the stability.

Okay.

So it's the following sentence.

So we can generalize the Lyapunov function to a global function that plays the role of a potential function of any system.

This follows by generalizing condition A to allow for saddle points.

So a saddle point is when the landscape is shaped like a saddle and there's like, so if it were on top of a hill, the curvature is negative with respect to all directions.

In a saddle point, the curvature is negative with respect to one of the dimensions, but it's positive with respect to another dimension.


SPEAKER_01:
And so it's like... Just like a saddle.


SPEAKER_00:
Yeah.


SPEAKER_01:
Okay.

Why do they call it that?


SPEAKER_00:
And so...

The ball on the top of the hill is like an infinite multifurcator.

Like a little jiggle, it could go any direction because it's unstable in every direction.

Saddle points induce a symmetry where that ball is never going to roll up the hill.

It's going to fall to one side of the saddle or the other side of the saddle.

And one of those could be like a certain attractor and the other one could be like another elevation basically.

Though I don't know the details of how the saddle point is related to some of this Lyapunov stuff.

Okay.

All right.

So following 16, so this is, they're showing that the Lyapunov is equivalent to a potential function.

In physics, a potential function, psi, can be constructed to describe the flow of or forces acting on a particle through a potential energy gradient.

So the potential energy function, F, is del psi.

So here's just one way to state the big idea of equations five through nine, which are not discussed here.

So the idea is create a potential function.

And when you think of potential function, think it's like kinetic energy based upon elevation, but it's not for gravity and elevation.

It's like balls falling down tall buildings, but for not gravity, for something that's related to dynamical conversions.

So if you were like at the bottom of the bowl, you'd only have a little bit of potential energy.

the potential function would be low.

If you are really far up the wall, you'd have a high potential function.

You'd have a long way to fall.

That is going to be reflected by a balance of forces, namely in a tractor force, which is the combination of the Lorenz force and the potential energy induced force.

And then there's the dissipative force, and that's the frictional force due to dissipative random fluctuations.

So that's like the Velcro ball on the Velcro bowl.

the friction is so high that it still is being kept on the side or it's rolling down very slowly or something like that.

Combining these definitions, we can express the total balance, total force as a balance of the forces defined above resulting in.

And so just like if it was two colliding vectors and then we were going to express the outcome resultant as like the total forces being applied on something, it's like that, but it's not about the physical forces and the gravity.

It's like more about

these forces abstractly.

And so equation 10 is expressing this balance of forces on x using a gradient landscape.

So there's that del psi, and here's the combination of forces being expressed as del psi.

Dean?


SPEAKER_01:
So they use the word tensor here, and when I was reading this, I didn't see it.

I mean, okay, so if we're having a tug-of-war on a rope, it might appear to the outsider, the outside observer, that there's a balance.

But really what I read into this, and maybe I'm wrong, was that there's a tension between these two forces, between the attractor force and the dissipative force.

Am I misinterpreting that?


SPEAKER_00:
Someone has a different answer.

It would be awesome to hear it.

A tensor is like a generalization of a matrix.

It's like a matrix through time.

I don't think that it is directly related to tensile or tensegrity.

Okay.

That being said, you're right that there is

a tension between these forces or there can be and then they like tension can reflect like a compromised position like if you have a rope with tension on it then the ending of the rope is going to be like some compromise between the various forces and so we're thinking about like the various forces

And then we're expressing it as like this matrix through time.

That's what makes it a tensor, not the fact that the forces are in tension or opposition with each other.

Yeah.

I hope that's accurate.

Okay.

And then they follow 16 again and transform that expression in 10 into a more standard form using the diffusion tensor gamma and a tensor Q.

This is like a rewriting that helps them get to this format where f of x is describing the flow of states.

So that's the change in states through time.

And it's like a vector field.

And it is going to be q minus gamma.

So tensor minus diffusion.

Both of that, so that multiplied by that landscape.

So that's kind of like the decomposition.

But if someone could explain this more, let's hear it.


SPEAKER_01:
Yeah.

And my simple words, this is where I, I keep bringing up the idea that maybe when we're looking at these things through an active inference lens, we're not just looking through a frame.

we're also looking through and they talk about filters later on.

So, so like, how do we, how do we make sure that we're including both and the, and sort of the tension between both of those views working together and being in tension with one another.

So, yeah, this is very, this is very formal, but I think it kind of reinforces that there there's what's inside the, the,

the partitions and then what's going on within them as well.


SPEAKER_00:
Yeah.

So it's describing the flow of states resulting from these conservative and dissipative forces.

So if Q and gamma were the same, then the flow would be zero.

If one is larger than the other, then it's going to be dominated by that component with respect to being multiplied by this landscape.

So we'll return to it many times.

Okay.

Variational free energy.

What did you want to add on this slide?


SPEAKER_01:
Well, I don't know that there's any... I mean, most people already know what we're talking about in terms of variation of free energy.

What they're doing is introducing it in terms of, so now where does the model of a biological entity, where is that sourced from?

And it's this sort of idea of these...

these things like posterior beliefs, the foundation or the idea that we don't walk into a situation completely blind.

We have some sort of prior and we're modeling situations where internal states can be interpreted and framed as a generative model.

So it's not, they're focusing on the model aspect of this as opposed to the generative processing side.

They're going to talk about what the external milieu is and where the generative process is here shortly


SPEAKER_00:
Great.

So good point to pause the video and read this because we're not going to read it out here.

But you're absolutely right.

They're focusing on the generative model of the agent of the cell, not

doing that analysis on the niche, for example.

Section 2.2.1.

Okay, so this is a fun part.

We can describe dynamics in generalized coordinates of motion denoted with a tilde.

where X tilde is defined as X tilde equals X. That's like the absolute state of X. And then X with a dot is the first derivative of X. That's called velocity.

X with two dots is, or a two double prime, depending on the notation, is the acceleration.

what people don't always know is that there are higher derivatives and they also have names.

So the third derivative is the jerk, and then the fourth, fifth, and sixth are the snap, crackle, and pop.

So that's just kind of funny, but the generalized coordinates of motion, which came up in number 26, have to do with doing prediction on not just, you know,

how high or low something is, or like on the number line left and right.

That would be like if X were just one number.

But even if it's just one number that we're tracking, like body temperature, for example, we might be interested in kind of unpacking that into this generalized coordinate where we wanna know body temperature and how fast it's changing and how fast that is changing.

And so one can imagine that once a zero has been achieved,

the higher derivatives are also zero.

If it's zero through time, the higher derivatives are also zero.

If it's zero instantaneously, that's not always the case.

But if it's zero through time, then it's kind of like you've cashed out the derivatives.

The derivative of zero just stays at zero.

And so it's like, if the position is unchanging, the velocity is zero, end.

If the position is five miles an hour going in the same direction,

velocity is constant but the acceleration is zero so keep on carrying that out to how fast things are changing how fast they change etc etc um and then they're going to take that x tilde how x tilde changes with time x tilde dot equals the flow on tilde plus

a variance so a noise or fluctuation a random fluctuation term this is the form of the langfine equation generalized coordinates of motion plus a random fluctuation term um any of the mathematical colleagues would be awesome to hear how this is related to the wiener assumptions etc but we're going to continue so

Using the Helmholtz decomposition, we can now express steady state flow in terms of a divergence-free component and a curl-free descent on a scalar Lyapunov function, L of x tilde, to obtain F of x tilde equals, there's the Q minus gamma that we saw a few slides ago, multiplied by now the L. So here we had F of x, not generalized coordinates of motion,

Q minus gamma del psi of X. That was the potential function.

Now, because the potential function and Lyapunov are related to each other, we're looking at the generalized coordinates of motion using the same Q minus gamma Helmholtz decomposition, but multiplying it now by Lyapunov exponent, or I'm sorry, the Lyapunov function on X tilde.

It's the solution at non-equilibrium steady state and is exactly the same solution for the flow of particles in the classical treatment above.

Crucially, we can now see the Lopunov function is the negative log probability of finding the system in any generalized state.

The Lyapunov of X tilde is the negative natural log of how surprised one should be, P of X tilde.

This is known as self-information of a state in information theory, surprisal or surprise.

In Bayesian statistics, it is the negative log evidence.

So the change in the generalized coordinates of motion, F of X tilde, breathe, is an irrotational solenoidal decomposition,

that's the left part of the right-hand side, multiplied by a Lyapunov function, which also has the interpretation of surprisal.

So if you were exactly resting, so to speak, you'd be totally unsurprised.

In summary, any weakly mixing dynamical system at non-equilibrium steady state will evince, give us evidence for, or look like it is, a flow that can be decomposed into a gradient flow on surprise and an accompanying solenoidal flow.

Because we can associate the Lyapunov function in 18 with a free energy, the system is effectively minimizing the

free energy in its conversions to a set of attracting states which have a high probability of being occupied so it's a little circular it's like we're finding ourselves where we expect to find ourself

And so if we just said, well, how surprised are we?

And let's gradient descend towards not being surprised.

Just at a first approximation.

We would find ourselves in regions of low surprise.

And we would be surprised to find ourselves in regions of high surprise.

Hmm.


SPEAKER_01:
Is it circular or is it kind of the byproduct of a recursive process?

Again, I'm asking, I'm not questioning.


SPEAKER_00:
It's a great question.

In like a really meta way, all of math is circular once the axioms have been stated because like the equal sign is there.

So we're just, you could add one to both sides and it would still be equal.

So is that circular?


SPEAKER_01:
Right.


SPEAKER_00:
um it's useful and that's what matters for this paper okay yeah um we're not going to talk about it too much but in a dot one we absolutely will and that's this least action principles in two two two um and they even give the example we love to see for example in colonies ants find the path of least action to harvest food and bring it to the colony

citation needed slash let's see that.

But conceptually, no problem.

This example considers their paths as flow channels or trajectories, finding the least average action for each instance of foraging given available resources.

So what does least action principle mean?

does least action mean and what is it necessary or sufficient for because certainly foraging is not the least biochemical expenditure of energy that's like the sort of naive interpretation of least action meaning just stay where you are but we're thinking about a usage of least action

that impels this ant to forage in an adaptive way, such that even when its foraging trip is hard and the seed is heavy and it gets like lost or something, that is still a realization of a least action on something.

It's still a ball rolling down a hill as efficiently as it can in a space.

So it's a nuanced topic that it will be great to discuss because they also write, minimization of action in an open system leads to structure formation.

So that'd be a good thing to unpack.

Okay, carrying on.

In dissipative random dynamical systems, action is not minimized for each element of the system, but on average over an ensemble of elements or repeated trajectories of the same element.

since self-organizing open systems are not conservative their structured flow is quintessentially dissipative good to think about that and hear many perspectives and questions on it classically for a conservative system the lagrangian is defined as l

This is L, not the operative.

Now it's Lagrangian.

Equals T minus V. So T is the kinetic energy of the particles and V is the potential energy.

And so that's how the trajectory of system states could be solved for a conservative system where the total energy is going to be conserved.

But there...

opening up into this dissipative and open scenario where this is not going to cut it um a little bit more detail on least action which we're not going to go into okay continuing on two two three markov blanket okay

A robust literature is developing about around the ability of cells and other a neural non neural systems measuring aspects of their environment via specific sensors.

They introduce here the Markov partition as this general case of separation of states into four categories.

E, external states, sensory states S, active states A, internal states I. And so the little E is like, it's a realization of big E. That's the bigger space.

And then X tilde comes back and the generalized coordinates are going to be a realization of values of those partitioned states.

So we've talked about the Markov blanket before, but now we're interested in the generalized flow on the blanket partition states.

So here's what the Markov blanket looks like.

And

We'll talk more about it in the dot one and dot two.

What do the arrows mean and why are sense and action connected?

What part of the cell are we looking at?

Is it a part of the cell?

Is it a model of a cell?

All these things that have come up before, but just to say that's their figure one and table one gives the math formalisms.

Okay, anything to add on Markov before we continue?


SPEAKER_01:
No, because we'll unpack.


SPEAKER_00:
Yeah.

So this is where they apply the Markov blanket.

So M is describing the Markov partition that defines the underlying random dynamical system such the cell.

So that is like the model.

It's the partition, Markov partition of the cell.

So inserting C into A and B is going to give

There's A is actions and I is internal states.

So the top equation is basically saying the flow on action is a function of sense action and internal, and then it has a right-hand side.

So the flow on action, the performance of action is, and then there's that gamma minus Q,

but now they have a subscript A because it's those about the action landscape, multiplied by del triangle on action tilde, generalized action, and then a self-surprisal, looking like the self-surprise of sense, action, and internal conditioned on M partition.

And the bottom equation in 30B is basically the same, but it's the flow on internal states.

f sub i on generalized states of sense action and i and that same gamma minus q del self-surprise condition on model so we're seeing some patterns come up and even if um it's like confusing and we all definitely are confused by it to some extent we're starting to see like some patterns why does it matter to focus so much on a and i the action

and the internal states, because those are like the ones we control.

We know we don't control external states directly.

Maybe we can intervene so that they change differently, but we know we don't control them.

We also don't directly control sense states.

We're getting dealt this hand every second by the niche

generative process and so we can take action so that we can expect different sensory outcomes but even then we wouldn't be controlling sensory outcomes and so a lot of active inference comes down to doing inference on internal model internal states generative model learning and action action selection flows on action um

This bounds the surprise on the particular states, which is the internal and the blanket states, through control of the autonomous states, which is just the action and internal states, the states we control.

Because of the sparsity of the blanket, not every node is connected to every node.

there might be a factorized and tractable form to bound our surprise about the particular states in general it would be very difficult problem to solve however we can replace the lagrangian that's the one here that was like going to be used for conservative systems but it didn't really apply

with a variational free energy functional or of a probabilistic model of how a system thinks it should behave or how we think the system should think it should behave.

So it's hard to solve these generally, but in practice, there are heuristics such that these are approximatable.

Section 2.4 goes like one layer deeper into KL divergence in the VFE, but we're not going to go into it.

Okay, we're just going to keep on plowing through.

Two, three is Bayesian filtering and self-organization.

We're not going to talk about it now, but the big questions here are like, how does Bayesian statistics relate to identity?

And what is self-evidencing?

And there's probably a lot of other good questions we could ask.

Like, why does Bayesian filtering and self-organization come here?

All right.

Modeling morphogenesis.

So now we actually get to the contribution of the paper, which is the modeling of morphogenesis.

So they're going to illustrate self-organization to Ness using the variational principles above.

by trying to explain the behavior of a model of pattern regulation by consideration of information processing and error minimization with respect to a specific target morphology.

In this setting, the game changes subtly but profoundly.

It's a Dean line if I ever read one.

Above, which is what we've reviewed, the dynamics of any random dynamical system equipped with a Markov blanket can be formulated in terms of a gradient flow on variational free energy.

That was just here, the flow on action and the flow on internal states.

Here, we turn this formulation on its head by specifying a generative model and implicit VFE function and simulate self-organization by solving the equations of motion in equation 34.

So they're going to specify the form of the attracting set, V, the generative model, and then they're going to let it ride.

So we have to specify-

They specify the external dynamics as the generative process, that's the niche, and the generative model of that process, which is being described by the flow of internal states.

Okay, so we're gonna look at the figures and then we're not going to even like go into the details.

So let's just look at the figures and see what they're doing.

So here's some empirical biology happening in the lab of Mike Levin and others.

So in A,

It turns out that when you dissect out the center of a flatworm, a planarian, those cells will remodel into a new worm.

So cutting out the middle and it reforms into this new flatworm and red is the head and then like blue is the tail.

So there's a self-organization of this target morphology, even from an initially clumpy cell.

what they're going to do in B is showing like the final fixed point of these different cells.

So here we have like four different cell types.

There's like the red neural or head cells, then yellow cells, green cells, and blue cell.

And then basically like this, like three, two, one, one, and then the bottom one, it's this one.

Maybe they changed the green and the blue

Like I think these two should be green and the bottom should be blue, but minor point, but it's an attractor on this target morphology.

And then C is describing a little bit about how it happens, which is that cells are constantly comparing their sense signal concentration, like of a gradients of some morphogen to expectations by minimizing their free energy functional.

so it's like what kind of a cell type am i and what should i be expecting right flip side what should i be surprised by because to say what i expect is the other side of the coin of what one is surprised by okay so they're modeling this empirical scenario where a flatworm can regenerate its form from just a clump of cells here and that

if we think about the location of these cells as being like a steady state attractor, like we want to have this body form last, then if all the cells just get in line like that, everything's going to work out.

All right.

How do we take this first jump that we made from the empirical biology to the state space framing,

Here it is in XY state space.

And now take the next jump, one more mathematical.

Here on the right side is that XY positioning of each cell.

this is reflected by the position in x and y of the cells so like negative nine and zero so that's like this top red one zero is the midline so it's like y and x and here like negative five it's a little lower and then like negative four and four it's like one is on they're on the same y elevation

but then one is four to the left and one is four to the right.

And so this E star of X is the position.

So it's a matrix of position because there's like eight cells and two location variables per cell.

So it's like a two by eight matrix and it changes through time.

So it's a tensor.

The E star C is the external signals.

And so in this steady state attractor, all three red cells are getting like signal one, here it's a binary signal.

There are four signals, but they only have an on or an off state.

In reality, there's many, many more signals than four, and there's a lot more nuance than just on or off, but it's a toy example.

All three red cells are getting the exact same signaling milieu of 1100.

So factor A and B are diffusing near me, but not C and D. The yellow cells are experiencing

those two diffuses and one other one the blue cells are experiencing the first and the fourth and then the green one is another and so this is like the attractor state for a stable location and signaling expression

So converging to this spatially makes your body look like that.

But the way that you get a beach ready planarian body like that is actually by reducing your surprise on signal expression.

Okay.

So here's them running it through time.

So it's a time-lapse movie montage of simulations of morphogenesis.

And so here it is converging towards on the right side, the morphology that's been discussed here.

And so the cells start out with undifferentiated.

They know what they're sensing.

They don't know what kind of cell they are and they don't know where they are.

And then they sharpen their expectations about what kind of cell they are.

while also moving into a different spatial niche, but they're not tracking their location.

Like I'm at three comma two, where are you?

It's like, this is what I'm biochemically sensing.

And so it's a relational morphology that doesn't need the blueprint in the nucleus.

So it's a lot like an ant colony.

There's not the nest architecture blueprint in the brain.

There's the process for stigmergy.

And then they show that with some other changes in the generative process, positive squared gradient in the generative process, they get double head formation, which some modulations have been shown to empirically result in in the lab.

And they can also make it so it gets double tail formation.

So it's just recapitulating this basic example and then showing that like modifying the external field changes the morphology that gets attracted to.

So those are the key pieces.

That's modeling morphogenesis as Bayesian inference, reducing surprise with a variational free energy flow on action and internal states.

Action states...

internal states.

Okay.

We're not gonna go into it here, definitely for the dot one and dot two.

So 3.1 is the construction of the model.

They then unpack that signal matrix, the combination of the signals.

And then

they give even more information about modeling signals.

And they also imbue it here with like this stemminess, which helps the cells start out with, I believe, a weaker prior about what kind of cell they are, which translates to where they should be.

In figure five, they do a targeted intervention in their studying of anomalous cell behavior

And so basically they start off with these cells that have initially unspecified state and they converge in A, the panels on A, by 32 time points, they're converged to the target morphology.

But in B, they show that if one of the cells that gets hit with a white arrow here has a perturbed signaling response, it fails to correctly infer its place in the ensemble.

So B could be like a genetic mutation or a targeted modification of the signaling.

And it's like that third red cell never forms.

And then in C, the same aberrant cell from B initially is rescued by an increased signaling sensitivity of other cells, leading another cell, green arrow, to switch position with the aberrant cell, pink arrow.


SPEAKER_01:
pretty cool okay yeah I wanted to come up with an analogy that would actually work that it would explain this like if you're out geocaching one afternoon and then all of a sudden some something was able to turn all the lights off and then the person that was geocaching beside you change their behavior but I couldn't even come up with a valid analogy to try to explain this how about um we're playing American football


SPEAKER_00:
And so there's a plan and everyone's trying to reduce their uncertainty about how the plan is playing out from their perspective.

And so if all goes to plan, we're all going to deploy into the exact right positions.

But then somebody doesn't move.

If each nestmate on the football team has low sensitivity, then they're going to continue in their own expected trajectory as if nothing had happened.

That's kind of like what we see in B. It's like a partial rollout.

Whereas this is changing the environment, mixing our metaphors, et cetera, so that a high sensitivity teammate

fills in for that critical position so that the attractor state of the strategy can still be morphologically realized, even though there still is that one individual who's not moving.

But they're not interfering with the strategy's attraction.

But we'll come up with probably some other and better ways to talk about it.

okay and then they um just mention that this is something that can be modeled and it's in spm and also on franz kuchling's um github so maybe we will um look at the code and then just to say that we're not going to go into it at all today but all of section 4 is super interesting they review some of the mathematical assumptions and limitations of the model

there's a really fascinating discussion about variational principles and open systems.

And so just to read, we have shown that the variational free energy minimization in active inference is related to the variational principle of least action.

It is worth pointing out where these two approaches diverge.

Two instruments in a wood diverged.

then it's all about that divergence and the contrasting and so i think there's going to be a lot to unpack there and then they have some closing remarks on the applicability in biological systems and some of the predictive capacities of the simulations um in terms of like you could make a simulation of a healthy functioning tissue and then have predictive capacity

about asking counterfactuals.

Well, what if this happened?

What might I expect?

So that is a lot of info.

So thanks to everybody who's been watching and again, hope that especially with some of these technical parts that we were able to represent it with high integrity.

So you won't have the last word, but what would you say in closing as we move from the dot zero into the one, two and beyond?


SPEAKER_01:
Well, first of all, you went through a heck of a lot of stuff in a very short period of time.

So that's quite the gradient flow right there.

So that's impressive.

I don't think much in terms of, I mean, this is a classic example of where you have to unpack

as opposed to lots of the other papers that are maybe more philosophically based where you provision, like you pack a bunch of stuff and then you kind of go off on a bit of a journey.

And I think in the point one and the point two, we may still be doing a little bit more unpacking just because of the

the density of this kind of information, but that doesn't, I mean, you want to be able to do both.

So I think being able to cover this and hopefully we got it right, like I did my best to try to understand it and I think you did a good job of explaining it.

So we'll see who shows up in the point one and two, whether the point one and two are more unpacking or maybe

starting to actually think of ways that we can employ this you already mentioned that we could we can apply it to digital teams but whether or not people have the confidence to be able to take this and apply it like they've done to morphogenesis maybe that's part of that conversation cool yeah i'll be looking forward to


SPEAKER_00:
going through with multiple concurrent regimes of attention some of the formalisms that we either um disgraced or skipped in this discussion and then keeping it open to think about morphogenesis like where has morphogenesis been in numbers one through 38.

why haven't we been talking about morphogenesis and if not morphogenesis then what else have we been focusing on um how does it relate to embodiment and to spatial and physical aspects of cognition there's so many interesting angles and uh i'm sure we'll have a lot to discuss so dean thanks a ton for all the help on the slides and for this discussion um hope to see you in the coming weeks


SPEAKER_01:
All right.

Thanks, Daniel.

Take care.

Peace.

See you later.
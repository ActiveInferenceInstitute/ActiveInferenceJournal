SPEAKER_00:
Hello, everyone.

I think we are live.

This is Active Inference Livestream 13.0.

It is January 3rd, 2021.

Welcome to the Active Inference Lab, everyone, new and old listeners.

We are an experiment in online team communication, learning, and practice related to active inference.

You can find us at our website, on Twitter, our email address, YouTube page, or Keybase public team and username.

This is a recorded and an archived livestream, so please provide us with feedback so that we can improve on our work in the live chat or in comments.

And all backgrounds and perspectives are welcome here.

Video etiquette for livestream, mute if there's noise in the background, raise your hand so we can hear from everyone, and use respectful speech behavior.

So happy 2021, everyone.

For the scheduled Active Inference live streams in 2021, we're going to do every single Tuesday from 7 to 9 a.m.

Pacific time, which will include daylight savings time changes.

And for schedule and participation instructions, just go to this shortened link here.

also we're going to do several special sessions for example coding sessions or discussions with external people from the other communities and just if you have an idea for an event that you think could be interesting or you would like to moderate or just suggest you can let us know or you can send a tweet and tag that person if you know them and then we can set it up but the regular active inference live streams the group plan discussions

are at again that shortened link and it has instructions for how to let us know you want to participate in any of these events and the first two weeks of january so these next two weeks we are going to be in uh reading this paper by adam saffron and colin de young and then you can see whatever papers are most updated for the next several months at that link

Today in Active Inference Livestream 13.0, the goal is really to set context for 13.1 and 13.2, so kind of like a read-along for the paper if you want to participate in the discussion or if you want more context on it.

And the paper is Integrating Cybernetic Big Five Theory with a Free Energy Principle, A New Strategy for Modeling Personalities as Complex Systems by Saffron and DeYoung on October 16th, 2020 at this SciArchive link.

And the video, just like all the point zeros are, is just an introduction to the ideas and the context.

It's not a review or a final work.

The punchline of the paper is that we can combine psychology with the cybernetic theories of CB5T, the Cybernetic Big Five Theory, which we'll discuss, and Free Energy Principle Active Inference to have a better understanding of psychobehavioral dynamics in humans and other systems.

The sections of 13 will be as follows.

First will be the aims and claims of the paper, then the keywords abstract and roadmap,

then go through the figures and the table, and then conclude with several questions that people can ask themselves, whatever level of familiarity they have with the ideas.

So in 13.1 and 13.2, we're going to be discussing this paper as a group, so save and submit your questions and get in touch if you want to participate.

Okay, the paper is as said on the previous slide.

The aims and claims of the paper in the author's words are, from the conclusion,

We have begun to outline how artificial agents based upon free energy principle and active inference might be used to test cybernetic Big Five theories, theoretical propositions regarding the functional significance of the dimensions in the Big Five trait hierarchy, which is from psychology.

Here are the implications and why it matters from the author's conclusion as well.

If deep correspondences can be found between CB5T and FEP AI, as we suggest is likely to be the case due to their common cybernetic foundations, then this integration may allow personality modelers to obtain new understanding with respect to functions and then to parlay that understanding of function into new hypotheses about mechanisms in the brain.

And, finally, identifying the configurations of personality mechanisms that correspond to effective cybernetic function may yield insights into how personality imbalances can lead to mental disorders, thereby furthering the aims of computational psychiatry and other attempts at developing theories of human mental health and well-being.

So that is what they have set out to do, and by the conclusion, that's where they want to have gotten.

And these .zero videos are meant to be a sort of bi-directional on and off ramp.

For those in the Active Inference community, it's a chance to see the fields that Active Inference is adjacent to.

And for those who might be learning about Active Inference for even the first time,

it could still possibly be a way to learn about this broad area from wherever you are as far as familiarity with these keywords, which include active inference and the free energy principle, but also generative models, cybernetics and the cybernetic big five theory.

And then there's a few related keywords as well.

so first active inference and i uh got some interesting feedback and just thought about it over the new year and i think it's a great exercise to always start with just what active inference is just like how evolutionary biology which is in the area i'm in often talks will begin with the principles of natural selection because they're then going to be demonstrated in the subsequent sections so even just having a high level summarization allows an individual to personalize their approach to

portraying what active inference is in the context of a specific system or question, just like an evolutionary biologist doesn't need to re-justify all of natural selection, they just draw upon those principles and outlines and then apply them.

I'm going to start with active inference in the words of the author whose paper we're reading.

And they wrote, work within the free energy principle paradigm has yielded a normative model of behavior called active inference.

The active inference framework describes processes by which free energy is minimized and provides close ties to other frameworks such as expected utility theory and risk sensitive control.

Further, advances in deep reinforcement learning appear to be converging on the kinds of solutions that are predicted to be necessary for bounded optimal decision-making and learning in the free energy principle and active inference framework.

and another quotation that they have about active inference is the development of the active inference framework has made connections between free energy principle and cybernetics more obvious as it highlights that organisms are motivated not only to predict their perceptions accurately but also to influence states of the world in order to ensure that perceptions correspond to goal attainment so that's two really nice quotes on active inference from the authors and here's just a way that i kind of laid it out here maybe we can even try

a new one every single time, or something similar.

Active inference is a framework, which means that it's a cluster of scientific theories, which are ideas that make specific testable predictions, hypotheses about the world, that's a theory, and tools.

So that includes software toolkits and vocabularies and shared narratives.

So it's a framework.

It's beyond just a single hypothesis, but there are important hypotheses that are more or less central to what it says in specific cases.

It's a framework that integrates action and perception.

So action is whatever the behavior of the system of interest is.

So if we're talking about a certain bacterial possibility and affordance that a bacteria can undertake, then we're talking about action being our measurement of something that's, for example, moving or being secreted in the world or emitted from a hidden state.

And perception is sort of the obverse of action in some way, and it relates to the sensory capacity of the system of interest.

and as we've talked about in other discussions there's something like a interface being defined with the organism and the environment so that's how we're integrating internalism and externalism reductionism and holism all these things are coming together because by defining the interface that one is studying the blanket of some type you're at the same time delimiting the system from the external part so you're not assuming that there's another level of boundedness like a closed system beyond the refrigerator and the kitchen

you can now just model it as your inference on a certain boundedness given the measurements and the structure of the system little digression it integrates action and perception downstream of the free energy principle free energy principle is a statistical physics approach to modeling complex multi-scale systems so it uses several different threads of physics and

For those who are just learning about it, there are a lot better resources than here to hear specifically about the mathematical details in this discussion, but we hope to be able to provide easy entry points for this

aspect of the discussion as well and it's about how far from equilibrium systems work so things that are passive like let's just say a cloud of gas accreting in some galaxy somewhere it can appeal to just gravitational forces to describe the dynamics through time however there are systems whose dynamics through time like

organisms, their dynamics through time do not resemble that of passive bodies.

And so there's these active systems.

And what is it that these active systems, these negantropic systems, organization in the world, how is this happening?

What is life?

That's where that element comes in.

But those are the systems that we want to understand, design, predict, and control.

So active inference is a framework that integrates action and perception downstream of the free energy principle to explain how far from equilibrium systems work.

What about generative models?

Generative models can be contrasted with descriptive models.

Descriptive models go from data, which are observations about the world, or results of an experiment, and go from data to then the modeling of parameters about the world.

So given the height of these 50 children, what is the mean and the variance of height in the classroom?

And so statistics can be seen as a specific kind of dimensional reduction about measurements in the world.

For example, you have 50 measurements.

That's a 50 dimensional vector.

And then it's being reduced onto a mean and a variance component.

So two numbers.

It's like a bottleneck that only two numbers get to pass through.

So it's a dimensional reduction.

And then you hope that those dimensions capture the average variance.

uh value of the trait or whatever and the variability so that's going from data to the model dimensional reduction generative models go the other direction which is from the model's parameters

for example, the mean and the variance, to the expected data.

Given a mean and a variance estimate for height, the two numbers, can we generate sets of predicted measurements that are consistent?

So generate a draw of 50 children.

And so even if it's 51, it's going to be just changing it a little bit, and the average is going to recapitulate those two numbers, but it's going to be possible to generate hundreds or thousands or counterfactual worlds given this generative model.

Generative models are useful for statisticians for a lot of the same, but also some of the different reasons that they're different or that they're useful for organisms to have generative models.

So here's some of the similarities is that in both the statistician and the organismal case,

it allows the system to use a lot of its previous knowledge on sparse data sets, because generative models might only need a slight cue to tip their likelihood from something being very unlikely to totally likely.

For example, hearing the sound of an ambulance outside your window means that it's extremely likely that there is an ambulance there, even though it's also possible that there's one at other times.

It makes it extremely likely.

And one kind of model that's used that's a generative model in a way just to understand without the control theory, without the cybernetics, how a generative model is useful statistically is what's called expectation maximization.

And this cycle, expectation maximization,

The data or initial estimates are input into what can be thought of as one layer of the model, which is just the estimator of the observed data parameters.

And parameter is just, in the computational sense, any single variable that's going to be stored.

So these are the data.

This is the actual 50 children and their heights.

And then there's a maximization step, which asks what is the maximum likelihood of those underlying hidden parameters, which can be a different dimension than this one.

And then there's a generation step, which is the expectation step, which data are expected or expectable, I guess, I don't think that's a word, but that's kind of it, given the hidden parameters that are in the world.

And so this can continue until it converges.

And this could be about static things in the world, or it could be about dynamic things in the world.

And the expectation maximization could also be on causal things in the world.

And when that inference is going to be about action, that's where action enters into the loop, because instead of just having a data set and trying to converge on the best descriptor, it's trying to converge on a policy that could involve hidden aspects of the world.

So let's go a little bit more into detail in this generative models and processes, generative models and generative processes and active inference idea, because I don't think this person's blog has been on here, Solopchuk, tutorial on active inference, some really nice images.

So here on the left is this idea that a generative process which has a true hidden state s is emitting observations which are modeled by some p function but its observations are being emitted from the world and the niche is really the generative process that leads to those observations like the sky emits the sun and the moon or the ecology emits this rainfall or something.

Then on the other side of the blanket, we have the organism, which is embodying and enacting a generative model of those observations.

So that's like your brain predicting what the retina would see.

Well, how do we know that the brain is predicting what we would see?

There's no blind spot.

It has usual, under healthy conditions, equal clarity everywhere and equal color vision everywhere.

And that's just not concordant with the actual distribution of cells on the retina.

So really, it's like upgraded resolution at your periphery that's an illusion based upon the generation of the assumption that things are actually equally clear outside of your perception.

So it doesn't have the same actual experienced...

characteristics as what you'd expect in the retina.

Just, you know, interesting things, and you can imagine that for other senses too.

So the organism's enacting this generative model of those observations, and then the surprise or the residual of those predictions is kind of getting passed upwards into these models.

And that's changing how the model of the organism in this Bayesian framework here

is modeling these observations.

So the organism has a generative model or is a generative model, depending on how you think about it, of the niche, which is what facilitates action.

If your model of the circadian rhythm is 18 hours and it's 24 hours for the real thing to happen, then your niche, your temporal modeling is off and it's going to be pathological and maladaptive, or potentially not.

That's where selection comes in.

So here's another representation from that same blog that shows how now from that inference process described here, where the organism has a generative model of inference on those observations in the world.

So I'm predicting what observations on Bitcoin price I'm going to see emitted by the true generative model.

Now, I don't know what the generative model is, but I can see the observations.

The organism has preferences for certain observations.

So here the darker circle is representing preferable observations.

So the observations in this case are interoceptive, which is really nice.

And the organism has a preference for, let's just say in this categorical example, one outcome of the observations over the other, the gray one.

And now we can make a 2x2 matrix with the two possible observations of the world with a preference over them and the state of the world which can't be directly perceived, which is whether the stomach is empty or full.

This isn't a physiological debate, it's just that's the simple case here.

And that relies on this A matrix, which is the probability of observations of the world

which is the data given the states which can't be directly experienced, which is the stomach.

That's the A matrix.

Then the B matrix maps a policy and the prior states to the way in which the hidden states are expected to change through time, and you can evaluate it for different Bs.

So here there's two possible U's, which are policies, eat,

or do nothing.

So under each policy, eating or doing nothing, which are represented by two different matrices, B1 and B2, the state at time t is like the current moment, and the state at time t plus one is the state at the next moment.

So there can be mappings between the present and the future

conditioned on policy that's the probability basically of the state uh right now given the state in the past and a policy so that's phrased from like past to now and this is about now to the next step

And here is the graphical representation through time.

So t minus one is the past, t is the present moment, and t plus one is the future.

So the actual state of the world is evolving under a certain dynamic, and it's emitting observations, and then under multiple possible policy trajectories, policy parameters or different policies that can be enacted and embodied,

what is being estimated is this relationship from how previous states of the policy influenced or were associated with this true state of the role of the current moment in this layout and how that observation then influences the updating of the policies in relative comparison for the current moment moving forward under a generative model of action in the niche by the organism.

All right, big five, psychology.

So the big five are dimensions of variation that are statistical and usually from self-reported surveys that are based upon basically personality differences in the population.

So it's a dimension of variability in populations, kind of like the trait of heritability from quantitative genetics.

So it's a measure of variance explained on a certain population sample.

It's not a causal model of traits, nor is it a fraction due to genetic causes, even though that fallacy is made throughout the literature.

So the Big Five is based upon the use of statistical methods, basically correlation matrices and principal component analyses, on survey data that finds patterns across questions and participants.

And the acronym for Big Five is OCEAN.

So here's an example of how the big five are calculated and then what the oceans stand for.

So what will happen will be that a bunch of participants are asked a survey or are scored through some way.

And this survey may have a hundred questions or a certain different number of questions.

And that corresponds to a hundred dimensions of data.

It's an a hundred dimensional vector per person.

So the total survey study design can be just the columns are the people and the rows are the questions and how they answered.

And so that's a simple matrix that can be given all the data of that entire survey.

And some statistical techniques are used on that matrix.

and like a principal component analysis and it according to the psychology literature falls out into these five large axes of variation so when we do the pca the algorithm is not told specifically which questions are expected to be correlated with or negatively or positively with other questions

But because there are actual statistical regularities in certain pairs of questions or sets of questions in the real world in terms of how people answer things, for example, a question and its negation should have a negative correlation, as well as other relationships in the world, it's possible to find the structure of some of these dimensions that map very well on these dimensions of variability in the data.

And so the five big axes of variation are openness to experience,

the axis between inventive and curious versus consistent cautiousness conscientiousness extroversion agreeableness and neuroticism or ocean and uh i uh hesitate to go too much into the adjective side because those are always very english language dependent and really it's about the structure of the data in the real observations and here's just a few other issues with uh the

topic of big five which is that there's often population and survey specific covariance structures though some structures are quite well correlated across different surveys there's often a normative or a culturally narrow understanding of what personality is or how it manifests and how it represents itself and also there's just this fundamental difficulty in attaching the natural language words to these differences in the manifold

One way though that we can kind of get at what these dimensions really are is by looking at the questions that are very highly correlated with each of these axes.

So for openness, a few questions, I drew three questions from each of the letters.

Is I'm quick to understand things.

I like to try new things.

I have a good imagination.

c conscientiousness is i'm always prepared i pay attention to details i like order extraversion e is i am the life of the party i feel comfortable around people i start conversations a agreeableness is i take time out for others i feel others emotions i make people feel at ease neuroticism n is i get upset easily i have frequent mood swings and i worry about things such as in my life and

One interesting note on this paper and definitely a little fun fact is that

one can do a vector decomposition on the big five to reveal these additional aspects of personality beyond those five principal structurings.

That's a coarse graining to a very high degree, and this is going to go one layer chiseling down into the statistical variability structure.

And this paper, I was searching in my library for it because I thought it was an interesting one and I remembered it, and it turned out that it was actually the author of this paper.

um from 13 14 years ago with jordan peterson yes that jordan peterson and it draws somewhat on this idea of vector decomposition but statistical vector decomposition and just as refresher uh two vectors can be added kind of head to tail and they end up at the same place no matter which way you add them just like regular numbers like two plus three equals five and

3 plus 2 equals 5.

So they took the five big dimensions and broke them down.

The blue and the red arrows, the up and the down, they don't represent anything about the trait characteristic.

It's just using the same example of vector addition.

It's breaking down a statistical dimension into another axis of variability, which is a pretty cool technique.

And it's going to relate in the paper that we're going to discuss to the ways to get at very fine grained aspects of the high dimensional space that represents action propensities.

Cybernetics is, it's not the first time we've had cybernetics as a keyword on the ACT-IMP stream, but each time we do want to kind of see it a little bit differently.

So the image on the left is a brain with a bunch of wires and kind of represents this crossover mind machine mapping.

Is it an analogy?

Is it an interface?

Is it a vision?

Is it a prophecy?

All these different things that mind and machine can be is kind of captured in cybernetics culturally.

But also on the right side is a more formal systems diagram approach to cybernetics, which describes kind of first and second order cybernetic systems.

And here is from this CyberSoft post down on the bottom.

um they wrote the basic idea of cybernetics is that complex systems such as living organisms societies and brains are self-regulated by the feedback of information by systematically analyzing the feedback mechanisms which regulate complex systems cybernetics hopes to discover the means of controlling these systems technologically and to develop the capability of synthesizing artificial systems with similar capacities

There's a few other parts about cybernetics that are interesting because it has had quite its own history and it's related to complex system study, but there's some interesting history.

So here's a few more quotes from this article.

They write, the word cybernetics was coined by the MIT mathematician Norbert Wiener in the summer of 1947 to refer to the new science of command and control in animals and machines, which he helped to establish and develop.

The word was derived from the Greek kubernetes, meaning steersman or ship pilot.

Unknown to Wiener at the time, Plato had used the adjective kubernetes

in the Gorgias to refer to the science of piloting.

And the French physicist Marie Ampere had derived the French word cybernetique directly from the Greek to refer to the science of government in his classification of sciences.

And so the word cuber leads us straight to governor, the regulator, guide, and pilot of steam engines and states.

It thus lies at a powerful metaphorical intersection of political economy and technology, social and material order, and the regulation of animals and machines.

And then another quotation that was pretty interesting from the end of this article was, When Donna Haraway challenged feminists and leftists to embrace the language and metaphysics of cybernetics, what was she aiming at?

Was the cyborg just another metaphor for hybridity?

If it was, then why was she so insistent on adopting not only the terminology of cybernetics systems and information, but its whole ontology?

No, this was not mere rhetoric.

This was...

This move was political, and it was savvy, wise, and insightful.

For Haraway shows us it is not just the world, the environment, that is cybernetic.

We are cybernetic.

We are cyborgs, cybernetic organisms.

We are hybrids made up of organic beings and mechanical parts.

And the only thing that ties us all together, you and me, you and other parts of you, is information.

Fundamentally, cybernetics is a new ontology.

Andy Pickering has called it an ontology of becoming.

So pretty cool quote.

Cybernetic Big Five Theory, CB5T, is a line of research pursued by one of the co-authors of this paper, primarily Colin DeYoung, and it uses cybernetics, as the name might suggest, to integrate the dynamics of personality and get at potential mechanisms amongst other things.

And the highlights from the 2015 paper, which is cited in the 2020 paper,

They write, cybernetic Big Five theory is a new integrative theory of personality.

Personality traits reflect parameters of evolved cybernetic mechanisms.

Characteristic adaptations, which is a term from cybernetics, are goals, interpretations, and strategies in updatable memory.

Specific mechanisms are identified for traits in the Big Five hierarchy.

Implications for pathophysiology, psychophysiology, and well-being are discussed.

And this was a figure from the DeYoung 2015 paper, but it's actually drawn from Jordan Peterson's 1999 book, Maps of Meaning.

And here's how it is in the book slash paper, where at any given state, there's the current state of the world and strategies are being implemented to attain desired states or goals.

Then anomalous information is always entering the picture from chaos, entropy, the unknown.

At some point, the current state of the world, given the strategies and the desired goals, can no longer keep the integration process maintained.

The center cannot hodl, and there's a disintegration process.

That leads to a state of chaos and entropy in the unknown.

followed by a reintegration, because you can only have a reintegration from disorder to order, you can only reintegrate.

Even if it's a very similar state, because it's still an accessible one, even if it's a similar one, it still is a reintegration.

And if the world state has changed, that means the niche is different, which means the reintegration must be different.

So I thought it was kind of interesting to map this onto a few other ideas from Act-Inf and other things that we've talked about in the streams.

So first, there's the dividing line between order and disorder, which correlates to basically the blanket, the interface, the whole thing we're talking about with the self-organizational systems.

Because that boundary between the agent and the niche, again, is not just an informational one, it's also an info-thermodynamic one.

So the agent is where organization is happening.

For example, the person in their apartment is eating food and they're organizing biological matter, radiating heat.

They could be getting more organized, making more organized tissue of any kind, depending on their hormonal state.

And then depending on that, the room is going to be warming up and there's going to be heat released in the supply chain.

So that's actually like a thermodynamic boundary as well as an information one.

And we can think about these preferences over different observed outcomes of the world as being akin to goals.

Peterson's framework here is still very reward oriented rather than precision oriented.

That does enter the picture here.

We'll get to that in a second.

The strategy is like policies, which truly are strategies, and the current state of the world and the interpretation are like hidden state estimates, which could be like, is my career stable or will things be okay with this thing of the world?

So given current understandings of the world, which can be accurate or not, helpful or not, two by two, and given strategies being implemented, which is like the field of affordances and active inference, are organisms attaining their preferred states, their desired states of the world?

and in the framework of peterson as well as an active inference anomalous information is continually not just occasionally entering into the picture so it's always a question of resiliency and of self-organization around anomalous information being integrated because no model is going to be perfectly predicting it's about incorporating the variability into a healthy and resilient model and here it's like surprise being passed from the environment through the interface to the organism

and the organism can disintegrate and enter into potentially a pathological region or a strange attractor state that could reintegrate or re-stabilize in a new or a different niche into another different state policy preference combination and this encounter

with chaos which could be the dark night of the soul or the hero's journey or alchemy or phase transitions of the self that's an interesting thing to think about with the self-organization and the reorganization reintegration of societies cities teams relationships individuals

Another thing that is mentioned in the paper, but it wasn't a keyword, was SPM, which is Statistical Parametric Mapping, SPM.

So here's from the SPM website, whose URL is on the bottom.

It says, SPM 12, first released 1st of October, 2014, but previous versions of SPM have been released for many years, and last updated on the 13th of January, 2020, so almost one year old at this point.

is a major update to the SPM software containing substantial theoretical algorithmic structural and interface enhancements over previous versions.

Cool.

What is SPM?

SPM is a software package that has been designed for the analysis of basically brain imaging data sequences, but it turns out that that's a quite general question for a lot of reasons, which we'll discuss a few of right now.

So what SPM does is it starts with a series of images that are from various types of experimental designs.

It could be from similar or different cohorts, or it could be different kinds of time series from one subject.

um and the current release is designed for the integrated analysis of fmri pet spect eeg and meg so the most common interfaces for neuro imaging are incorporated uh despite their usually extremely different error profile and actually uh representing quite different aspects of the tissue being imaged and over different time scales with different accuracies so

It's quite a general question to integrate different kinds of time series into underlying generative models of extremely specific types.

What is the statistical parametric map?

It is the construction and the assessment of a spatially extended statistical process.

So a statistical process that happens through time and space.

that is used to test hypotheses about functional imaging data.

So there's like the map and the territory, that's the mapping, is kind of that two layer, the data to the underlying model, that's the mapping.

And then it's a statistical parametric mapping.

So like a p-value comes from a z-score or a t-score or some other type of variable, that's a statistical parameter.

And so it's going to be like an underlying statistical test, and it's related to a lot of different kinds of data.

And this SPM textbook from 2007 is quite a mathematical read, maybe it or a later Wikipedia version of the SPM toolkit.

Could be interesting to have a math set of discussions, but we have this one at home and it's a classic.

It's a true, true classic.

Here's just a summary of what SPM does so that the key aspects can be seen.

And this is what, for years before becoming more popular in the machine learning and theory of everything and philosophy of everything,

universe uh many years before that he was and still is the most cited neuroscientist and the huge reason for that is the many many papers that have gone into this type of development and then also its use as a toolkit broadly by many communities so the way that it begins with the spm is taking in the data which represents slices of

different underlying objects.

So it can be measured in various different ways, whether it's all the measurements simultaneously, or whether the slices take time as they slice through a system.

And that is first done, there's a first processing step that's basically a realignment to a standardized brain atlas or a person-specific brain atlas, for example.

So you can take a static image that has much higher resolution and align dynamical images, or not,

to this more static high resolution image, which can help a huge amount.

That is then processed through smoothing, which is like this thing called a kernel, which is like, you have a fabric, which is the data, and there's gonna be like a finger and it's gonna move across the data and reweight the data

according to some smoothing technique and this represents a gaussian kernel so this is just blurring the image to ease out a little bit over the noise at a certain spatial and temporal scale that can also be related to specific instrumentation related biases that are known like non-linearities in the response

And that is this process where the data are processed into a cleanly comparable format, spatially and temporally and statistically renormalized.

Then there is the specification of a design matrix, which there's so many of these in the SPM textbook.

It's really awesome and they look really beautiful.

And the design matrices are...

very broadly types of things that you want to consider in your analysis, whether it's something you know about the patients, like their age or their pathological diagnosis, whether it's something you know about them and you want to include it in the model or not.

uh as in as something you're predicting or including in your prediction of something else or whether the design matrix includes other more esoteric components that are often discussed in this spm textbook like these time kernels that design matrix and the smooth brain image sequences are

put together into a general linear model and it's funny because you hear a lot of uh disdain for linear models but this is linear models in a extremely rigorous and non-linear way that can lead to

these parameter estimates because it's like a linear model on a space that's been warped by this prior and updates in a multi-scale way.

So that's really the interesting part is it takes renormalization to the next level and then applies general linear models in an extremely specific way, though I don't understand all the details.

That relates to what is called a statistical parametric map, which is here like a heat map of the score of that voxel.

So that's like the Z score.

So like a Z score of two or two standard deviations above the mean.

or one or whatever it happens to be.

It's like a z-score, positive and negative.

And then there's a variety of approaches that can be used to then, on this statistical parametric map,

draw p-value distinctions on this and it turns out that that process of setting a p-value threshold and saying whether 80 of the brain is used in a task or one percent or correct for one person's structure but you're trying to detect the structure that area is an extremely nuanced field

pun intended and it draws on this random field theory and there's several chapters in the spm textbook that do random field theory and it was just like quite an interesting approach to statistics that is uh really being implemented here all right the abstract of the paper is that cybernetics is a study of goal-directed systems that self-regulate via feedback a category that includes human beings

Cybernetic Big Five Theory attempts to explain personality in cybernetic terms, conceptualizing personality traits as manifestations of variation in parameters of the neural mechanisms that evolve to facilitate cybernetic control.

The Free Energy Principle and Active Inference Framework, FEPAI, is an overarching approach for understanding how it is that complex systems manage to persist in a world governed by the second law of thermodynamics, the inevitable tendency towards entropy.

So the authors have framed the conversation as being firstly about cybernetics, first word.

That's about action selection in complex systems that self-regulate and use feedback, big field.

then within the context of the cybernetic big five theory which this is a psychological theory that draws heavily on cybernetics we are going to consider the neural mechanisms that facilitate and have evolved to facilitate this kind of cybernetic control then the free energy principle and active inference framework are introduced to explain a similar set of topics behavior as well as this resistance of dissolution far from equilibrium systems

Although these two cybernetic theories were developed independently, they overlap in their theoretical foundations and implications and are complementary in their approaches to understanding persons.

FEP AI contributes a potentially valuable formal modeling framework for CB5T, while CB5T provides details about the science and structure of personality.

In this chapter, we explore how CB5T and FEP AI may begin to be integrated into a unified approach to modeling persons.

We'll need another acronym, won't we?

So, the roadmap.

The first section is a cybernetic approach to personality and has the first figure, which shows some of the causal processes that are related to the functioning of personality of the individual.

The second section is the free energy principle and CB5T.

Third is predictive processing and personality neuroscience, active inference, generative models, and personality modeling.

There's two figures in that section which draw upon the graphical models, some of which we've seen some of the structure before, and also the SPM demo.

Then in section six talks about personality traits from the perspectives of CB5T and FEPAI.

And there's a representation of the personality trait hierarchy.

And then the final three sections are the meta traits, the big five personality traits, then a conclusion on the levels of analysis in personality modeling.

Table one then has some of the analogies between or mappings between CB5T and FEP AI.

so figure one in the paper is a drawn from another paper and it relates to the causal processes that are involved in the functioning of personality within an individual and so it's a pretty uh loose

chart saying that both genes and environment play into these relatively stable parameters of cybernetic mechanisms so that's saying somebody who has uh these are uh cybernetic underlying features like given a certain attractor state some people may spiral towards it or spiral away from it or stay in a certain stable orbit to it and that is the cybernetics of biological systems

that maps onto personality trait architectures statistically in matrices because there's this underlying statistical regularity in the world.

And that leads to a feedback system, with Stigma G perhaps, of characteristic adaptations and life outcomes.

And that's why there's correlations between life outcomes and personality traits.

And it's hard to determine cause and effect and all these other aspects of it.

Figure two is pretty nice.

It's the author's redrawing, I believe, of the graphical model structure that we've seen a lot of.

But I wanted to read a quotation from page 21 because they laid it out very nicely.

They wrote,

Generative modeling in FEP AI can be complex, but the basic setup involves specifying agents and their situations as systems of interacting variables and parameters.

These variables take the form of mutually referencing matrices, whose entries specify probabilistic mappings between different portions of the generative model.

Nice.

as we previously described a single objective function of expected free energy is used that's g to determine which policies are selected according to whichever actions are in to determine which policies are selected that's pi

according to whichever actions are anticipated to minimize overall prediction error.

An example FEP AI generative model may include the following matrices where each mapping is conditional upon the probabilistic mappings from preceding matrices.

A matrix is the likelihood mapping

from latent world states to sensory observations, or how sensations are interpreted via perception and imagination.

From the agent's perspective, this would correspond to answering the question, what is likely to have caused my sensations?

The B matrix is the transition probabilities, or how dynamics are expected to evolve from one moment to the next.

From the agent's perspective, this would correspond to answering the question, how do I believe things are likely to change through time?

Three, C matrix.

The C matrix is the prior beliefs over policies or outcomes or which states an agent predicts itself occupying.

Via mechanisms of prediction error minimization, these prior expectations constitute the attracting states of the system and function as preferences.

From the agent's perspective, this would correspond to answering the question, what do I want to happen?

And thereby doing slash enacting it through active inference in the ways described above.

From a CB5T, these preferences are an agent's high-level goals.

Four, the D matrix.

The dematrix is an agent's belief regarding its initial position within the decision environment.

From the agent's perspective, this would correspond to answering the question, where am I likely to be in this kind of situation?

Or where am I likely to have started?

And five, the E-matrix.

The E-matrix is the baseline prior over policies governing policy selection in a habitual or a heuristic fashion.

From the agent's perspective, this would correspond to answering the question, what am I likely to be doing right now?

And thereby doing slash enacting through active inference, but with actions deployed in a fashion that ignores uncertainty with respect to the likelihood of goal attainment in a particular situation.

From a CB5C perspective, these habit priors would constitute a major class of characteristic adaptations.

Pretty nice.

Then you have the states and observations, and then the beta and the gamma, which is the precision estimators, which is the confidence and the temperature parameter over that confidence.

So very nicely laid out by the authors and always cool to hear it a different way and framed as a catechism, which is actually like, what does each matrix represent the question?

What is the question that reflects how each matrix is answering something in the model, doing something in the model?

Figure 3 is an illustrative figure accompanying a generative modeling demo that is prepackaged with SPM 12.

This particular simulation involved modeling curiosity with respect to a task with unknown rules, where agents attempted to maximize valued outcomes while also needing to discover the latent task structure.

And the generative patterns of the data that are part of this integrated generative model in SPM include the inferred rules of the world,

the top left the patterns of the task choices here and the neuronal activity anticipated to occur based upon mechanistic process theories associated with fep ai bottom left and all these three so unit firing firing rates local field potentials and dopamine responses as a function of these things happening under this rule and choice

So it's policy, behavior, and neural or underlying responses, which can be of various categories, from the neurometabolic, like dopamine, to the firing, the local field potentials.

So it's quite a broad modeling framework.

Figure four is the personality trait hierarchy.

And they write, note that the structure indicated here is the between person covariance structure of traits in the population, not a within person causal structure.

So in other words, these connections reflect ways that you can partition axes of variability, not like there's some sort of downward cause.

So that's sort of the challenge with this naming of these things is that they're all kind of like adjectives.

They all seem like they're equal adjectives.

I mean, some might have a more negative or positive connotation or preference for some people, but they're all just adjectives.

And yet, really, it's a specific statistical structure where the connections don't really reflect relationships.

They're subcomponents that are...

distinct under another subcomponent of a population variability.

So it's really better just to think about what the real question is and what this statistical structure is doing because the adjective part, I just don't really know how good of an approach that is.

And then also the piece of the niche is that eco, evo, devo, so ecology, evolution, and development are what structure those stability and plasticity metatrates.

Because the ants that we see today, for example, they've been on a lineage that's been successful since the beginning of ants and back even further.

So it has had to maintain some parameter values which have kept pace with changing local context.

So that's what structures these really specific top level traits because it's actually the process that structures the entire landscape that all these things are playing out upon.

Table one shows some mappings between the key concepts in CB5T and their definitions and the free energy principle or active inference terms or related topics.

So cybernetics itself is about governing systems that self-regulate via feedback, and that's related to many of the cybernetic underpinnings and also information theory and entropic aspects of FEP.

Psychological goals are representations in cybernetics that get enacted through action.

And FEP, they would say that that is a Bayesian belief about the expectations of how prediction errors are gonna be minimized.

So that's preferences via policy selection.

Psychopathology is a persistent or maybe a self-diagnosed or not failure to move towards one's psychological goals due to failure to generate effective new goals over different timescales, let's say.

And that could map in the FEP to a persistent inability to minimize prediction error.

And there's a lot of interesting work on that.

In the second part of table one, personality are the reasonably enduring psychological individual differences, although also it would have to do with the ecological niche because it can't be defined abstract from the relationships and the experiments.

And then the FEP active inference correspondence would be these enduring attractors,

that defines systems as weakly ergodic processes.

So states that some systems revisit, but others don't.

So some people return to a YouTube channel or a TV channel or whatever, and some people don't.

And some people never enters into their orbit.

And so that's why it's always conditional on what is being received from the niche.

Those personality traits are those probabilistic descriptions of stable patterns within personality.

And their correspondence suggested for the FEP is the minimal message length

or the maximally informative descriptions of agents as generative models.

So this is like the parsimonious description of the agents representing their niche.

The characteristic adaptations are the stable goals, interpretations, and strategies specified in relation to basically the niche, and that relates to this whole policy and adaptive component of FEP AI.

So that's basically it for this video.

And a few questions to return to at the end, questions that are often asked.

What would we really get at with a good framework here?

So what would a good framework enable?

what are the unique predictions and experiments of this model so what is our knowledge gain what are we getting out of this specific integration paper and how is it going to influence not just how we think about the world but how we act in the world in terms of which experiments are now optimally informative even if those experiments the data already exists maybe it's just an analysis but that's still an experiment

Another question is what's the next step for active inference?

Maybe we could bring active inference models to bear using some of these new tutorials that are becoming available on large psychological databases and see if there's an active inference or a cybernetic descriptor behavior that explains more variants more parsimoniously than the big five structure does.

Another question is what is the goals of this research?

What do different people want to see happen and

What are the goals of the research?

And also, very importantly, what are we still curious about?

Because there's going to be 13.1 and 13.2 where we, as a group, discuss this paper.

And then we'll have more papers to come, so it's okay to be curious about it.

Any level, anytime you're listening to this, live or in replay.

But yeah, just...

leave a comment on the video or join us in the discussion if you listen to this at the right time and it's good to hear about what you're curious about because in this video I really just tried to run through the figures and give some background or some definitions so maybe some parts were a bit too much or too little for some people so as always it's just throwing it out there and let me know

can improve so thanks for participating even listening is participating we provide follow-up forums to live participants and we request your feedback suggestions or questions and just stay in communication and hopefully we will see you another time so thanks for listening and i'm going to stop the live stream
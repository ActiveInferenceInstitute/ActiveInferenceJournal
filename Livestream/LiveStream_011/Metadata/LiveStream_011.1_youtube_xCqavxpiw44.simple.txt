SPEAKER_00:
hello and welcome everyone to the active inference live stream it is active inference live stream 11.1 on 1 2 2 2 2 0 2 0 december 22nd 2020 this is going to be a really fun discussion and hopefully we'll see if anyone else joins us but other than that we got a nice small group and i'm really looking forward to talking to you all about this paper

Welcome to the Active Inference Lab, everyone.

The Active Inference Lab is an experiment in online team communication, learning, and practice related to Active Inference.

You can find us at our website, activeinference.org, on Twitter, at email, our YouTube channel, or our public Keybase team and username.

This is a recorded and an archived live stream, so please provide us with feedback so that we can improve on our work.

All backgrounds and perspectives are welcome here.

And as far as video etiquette for live streams, mute if there's noise in your background, raise your hand so that we can hear from everybody who wants to speak, and we'll use respectful speech, behavior, et cetera.

So just a couple points of process.

Today, on December 22nd, we are in 11.1.

And next week will be the last act imp stream for 2020 on December 29th, 2020.

And that will be at 7.30 to 9 a.m.

PST on the same paper that we're talking about today.

And everyone is welcome to join.

Then we are very happy to announce the active inference stream schedule for 2021, which can be found at this rb.gy link.

Going to this link, you'll be able to find out which papers we'll be discussing which week and how to participate if you are so interested.

And also all these sessions come and join for however much you can.

So if you have to join late or leave early, it's all good.

We just want to have participation and your perspective.

This is what the calendar looks like.

So looking ahead to some of the first papers we're going to discuss.

The first weeks of January, we're going to be discussing in 13, a paper by Adam Saffron and Colin DeYoung.

In the second weeks of January, we're going to be talking about Mel Andrews' paper, The Math is Not the Territory.

So everybody should be finding exciting papers and letting us know when we can read them for ActimStream.

Also, just one last note, if you go to ActiveInference.org, you'll see a call for a collaboration for the Active Inference Lab 2021.

So check that out if you'd like to get involved in any way.

OK, thanks for all that.

Here we are in Active Inference Stream 11.1.

And today, first, we are going to have introductions and warm up questions.

And then we're going to turn to the sections of 11.1.

The paper we're discussing is Sophisticated Affective Inference, Simulating Anticipatory Affective Dynamics of Imagining Future Events by Kasper Hesp et al.

in 2020 from the IY Proceedings.

We're going to talk about the aims and claims, the abstract and the roadmap, and then we'll go through the figures and maybe even some of the citations that are referenced in the paper.

Next week, we're going to be discussing the same paper.

So save and submit your questions and get in touch if you'd like to participate.

Okay, here we are in the intros and warmups.

So let's introduce ourselves.

We can each give a short introduction or check-in and then pass to somebody who hasn't spoken.

So I'm Daniel, I'm in California and I'll pass it to Sasha.


SPEAKER_02:
Hi, I'm Sasha.

I'm also in California and I will pass it to Blue.


SPEAKER_03:
Hi, I'm Blue.

I'm based out of New Mexico.

I'm an independent research consultant looking at bioinformatics a lot and a lot of agent-based modeling mostly.

And I'll pass it to Stephen.


SPEAKER_01:
Hello, I'm Steven.

I'm based in Canada, in Toronto.

And I'm interested in the ways Active Inference can help inform the ways groups and communities understand their place in the world.

And I will pass it over to Ivan.


SPEAKER_00:
Let's go to Alex, actually.


SPEAKER_04:
Oh, sorry.

Yeah, hello.

My name is Alex.

I am in Moscow, Russia.

I'm a researcher in systems management school, and I'm trying to find interconnections and possible ways to connect active inference with systems engineering and system thinking frameworks.

Awesome.


SPEAKER_00:
And have we a new fellow jitzer?

It could be Ryan.

So any time that Ryan or anyone joins, we'll just hop in and go from there.

Okay, I just want to check these messages.

All right.

Thanks.

Okay, let's go to the warm-up questions.

So for the warm-up questions, we can each just raise our hand, give some thoughts, and we'll have plenty of time in today's relatively small discussion to get through everyone's thoughts on the paper and on the associated areas.

So first, just an introductory question is what gets you excited to read and discuss this paper?

So thanks, everyone, for showing up and for those who are listening live and in replay.

And I'll tie that to the second question as well.

How can we learn and communicate the technical aspects of active inference in an accessible fashion?

so it's kind of like intention setting as we head into this relatively technical paper and indeed relatively technical area of active inference that draws on mathematics and computer science and a few other areas so again anyone can raise their hand but i'll give a first answer just think about what's something that gets you excited uh in terms of the content and then from a process-based perspective what is a way that we can learn communicate excessively

So let's go with Stephen first.


SPEAKER_01:
Well, this is interesting because I think this discussion came up on one of our conversations on our chats.

I think finding different ways in will give you different ways to communicate to the different people that you're talking to.

So I think active inference

needs to be connected to some process that people are familiar with in terms of how they make meaning in the world.

Otherwise, it's a big, big jump to build it all from first principles.


SPEAKER_00:
yes so definitely it's exciting to hear about things that we've heard about in a new perspective so i agree when we see keywords in an active inference paper and we know that it's going to be like familiar territory but approach from a different angle that's exciting any other thoughts there alex


SPEAKER_04:
Yeah, thanks.

For me it was interesting how we provide an example and the case for defining agent and generative model for this agent.

And so as a result to have simulation as possible way to... In a practical way when we usually discuss in papers here is like exact case for simulation.

Thanks.


SPEAKER_00:
Okay.


SPEAKER_04:
Blue.


SPEAKER_03:
So I think a lot of things were exciting to me, but like computational psychiatry, that's a field like, what is that about?

Um, but I mean, it really just kind of hones in on like, maybe are we like a sum of input output that gives our effective state, like just direct results of our input.

And if so, like,

i don't know that it feels a little bit like brave new world-ish like are we gonna move toward you know control theory and optimization of that affective state it's kind of um just like crazy and a little bit scary to me yep from implicitly to explicitly steering others behavior and valence and affect sasha


SPEAKER_02:
Yeah, I just remember reading this paper when it came out earlier this year and thinking, like, what does affect have to do with agents?

And why is that important to model or to include mathematically?

And so that's been something

I'm still thinking about and yet very interested in the aspects of computational psychiatry.

And yet, I guess just on a deeper level reminds us that affect and mood are really important aspects of the human experience and not something to be ignored in favor of like bigger decision aspects that it really does matter.


SPEAKER_00:
Yeah, one thought on that is that in active inference, we're always talking about how everything is prediction.

And then we're always talking about how prediction isn't just a point estimate of what will happen.

It's actually a state estimate and an uncertainty about that estimate.

so you know i predict the giants are going to win the world series and i'm this certain about it or i'm not certain about it so it's always the state estimation and the uncertainty and so that uncertainty is the risk or the ambiguity depending on the context and then the state itself good versus bad is the valence

And so it's sort of like the affect, which is multidimensional, contains something about how good or bad it is.

And then orthogonally or independently, whether it's good or bad is whether you're confident or not.

And so you could be very confident about being happy or very confident about being sad.

Ryan, we see you joining.

Can you hear us?

Not sure while Ryan is figuring out his setup, but thanks Sasha for bringing up the computational psychiatry angle.

And I hope that Ryan will be able to fill us in soon.

Steven.


SPEAKER_01:
Yeah, I think this is a really important point because when we think about how things happen over time in this process, how that gets integrated, how we make sense, not just in some sort of snapshot about a situation, i.e.

we think this is a problem situation and then suddenly there's this emotional churn that's somehow a byproduct of that, but actually...

it's not a snapshot this ongoing process which seems incredibly impossibly complex is made all of it's made sense of because of affect and emotion it's not some freudian sort of repression that's sitting there in the background as a as a mistake but it's actually like the big game in town which is pretty revolutionary really


SPEAKER_00:
Interesting.

And also yet another transformation or sort of paradigm shift associated with thinking from a free energy perspective is rather than prioritizing the valence, the plus versus minus, and then having risk or ambiguity as a secondary layer, we can actually think about the ambiguity reduction as the primary drive, and then the plus and minus as falling out of the quest to reduce uncertainty, rather than uncertainty being an auxiliary factor in the quest to improve outcomes.

So it just shows how there's these two related dimensions of prediction, which is the state and the uncertainty.

And then we can think about how prioritizing one or the other is going to lead to different outcomes.

Ryan, we can see you raise your hand.

Can you talk?

Okay.

How about you can raise your hand and lower it in Morse code and we'll communicate with you that way.

But Ryan, thanks for trying.

And if it's not working with your current browser setup for whatever reason today, then we'll make sure to work it out with you in the interim and for next week and for following discussions.

We'll have you on unabated.

All right.

And here's our last warm-up question, which is something that you are wondering about or would like to have resolved by the end of today's discussion.

Blue?


SPEAKER_03:
So I don't really expect this to be resolved, but I wonder instead of modeling, you know, to predict like positive or negative valence, like if there is a way to model like equanimity in this sense, like so neutral valence, like is that, I mean, what would drive that force?

And so I'm just kind of wondering as we dive deeper into the model, if that's going to become clear or not.


SPEAKER_00:
Can you explain what you mean by that and what other types of models might exist to think about that?


SPEAKER_03:
So just like how, you know, the results here were geared obviously toward, you know, pushing assets towards either a positive state or driving away from a negative state.

I mean, that's the, I assume the idea.

So,

What about driving toward a neutral state?

Like, is this something that's been considered, right?

Like, so, you know, overthinking tends to lead, like overthinking, like supposing about the future

leads to like, you know, these supposed negative thoughts.

And so then it's anxiety producing because you're worried about these negative thoughts.

Like, so that's overthinking or thinking like worst case scenario, perhaps, right?

So people do like a lot of thought redirection in this cognitive behavioral therapy.

Like people do thought redirection so that maybe you're thinking in not such a negative state, but in a more positive way, like, well, it could also be great, right?

Like, so there's that.

But what about like the non-thinking?

So if you non-think, could you lead to like a valence neutral, like valence equals zero to just have this kind of equanimity baseline that's like something maybe meditation achieves, right?

So like, can you work towards like non-think, maybe non-thinking is that valence neutral, right?

Like, so I don't know, how can that factor into computational psychiatry?

And has that been thought of before?

I don't know.


SPEAKER_00:
cool great question and so it's kind of like if we were going to be doing modeling of a car there's probably a lot of different ways that you could set up car crashes or cars that drive off the rails and we just want to know about what does it look like to have a computational psychiatry model that isn't just capturing a negative spiral

as overthinking progressively sort of accelerates but rather something that returns to a neutral attractor state or something that's related to mindfulness or attention to detail or abstraction but not hyper abstraction rumination but not hyper rumination so could the strange attractor be a neutral or even a healthy one not a delusionally positive or negative stephen


SPEAKER_01:
I suppose you could say that there's thousands and thousands of dimensions which are kind of sitting there as neutral, and you really notice it when you're not there and you've got to re-establish it.

So that's what they talk about in sense-making is when you're immersed in something, you don't do sense-making, but once something jolts you out of it, certain consciously, you might be doing unconscious sense-making, but at more of a conscious level, it's when you're surprised, basically, when things aren't,

going the way that your habitual understanding of them are.

And then suddenly, your cognition has got to kick in and say, Okay, how do I work out what is the sense of what I should be doing at the moment.

And that's when everything becomes transparent.

So it's, it's quite interesting to actually then see this now.

That again, taking that that thinking and

and that sort of abstract and bringing it right into the body and the and and the emotions as the kind of like the way of really knowing i mean in some ways is your mind checking in with your body was your body checking in with your mind cool very interesting about the mind and the body and always an important take um any other thoughts here or can we continue sasha


SPEAKER_02:
Another aspect of this paper I really liked is discussing the future clinical applications of understanding the mechanism of how certain existing therapies work, because there's a lot of different kinds of therapies out there.

Namely, I'm thinking of art therapy, but of course all other forms included, to start to understand why they work and by what mechanism they might be tapping into this kind of affective and predictive coding.

So I think that's really exciting.

And I also wonder who is fit to address those kinds of questions.

Is it medical professionals?

Is it psychologists?

Who is best suited for that role?

Because it's a very challenging subject to talk about without giving people perhaps incorrect medical advice or false hope.

And so that's just something that I'm curious about.

But

Don't want to step on any toes.


SPEAKER_00:
Cool.

Let's keep that in mind.

And as Ryan joins and rejoins, but no need to send the Jitsi messages because we can hear it very loudly.

So let's just hope that Ryan can figure it out.

Otherwise, we'll take care of it for next week.

Cool.

Here we are in the paper for today, which is Sophisticated Affective Inference Simulating Anticipatory Affective Dynamics of Imagining Future Events.

And this was from October 2020 in the first International Workshop on Active Inference, or IY.

And I believe that more recently, like in the last couple of days, this has been codified into a book.

So we're using the ResearchGate version.

The aims and the claims of the paper were a bit rehearsed in 11.0.

So go and check that one out if you want a deeper dive into what the aims and the claims were.

But the big point of this paper was to take two bodies of previous work,

which was the sophisticated inference, which is sophisticated meaning deep through time, and then affective inference, affect being related to emotion and valence, and fusing these two threads into sophisticated affective inference, so deep through time and also involving affect.

And the big question from a non-active inference jargon perspective is how to model affective and anticipatory agents

course we're going to be thinking about that within the active inference framework and then we're going to be asking what features do these simulated agents display and then how does this carry out or connect to areas that are like psychiatry or cybernetics control theory all these fun things that we've been talking about in previous active streams how are we going to connect what we're learning about today something that's a recent development in only the last couple of months

this is the current developments.

So let's have fun.

Let's treat it as just an initial inroad and a connection between these two different areas.

Here's the abstract.

And again, it was read through in 11.0 and we're gonna just go more discussion oriented in the panels that we have.

But fundamentally in the abstract, they describe what I just said, that they combine those two different types of active inference models, sophisticated and deep parametric models.

And then they use a augmented Markov decision process.

Please know Jitsi chat if possible.

Thank you.

Ryan, we'll see you soon.

And then they give a minimal simulation at the end of a ruminating agent.

And that is where Blue was talking about the negative agent that sort of spirals off into negative affect.

Okay.

um the roadmap of the paper there's not that many stops not that many gas stations along the way it's also a short paper and i have it in this other tab if we want to go through it as it is as a paper because it's so clear and concise but what they do is introduce the ideas then show a couple figures that just summarize the model and show a table with what the formalisms were

At this point, does anyone have any thoughts or where do we want to go?

Like what was something that somebody thought structurally or from a topical perspective that they wanted to bring up?

Or it's okay if not.

All right, let's look at the figures and then use the figures as our scaffold for kind of understanding what they are up to in this paper and then understand why they did some of the things that they did and then what some of the next steps could be.

So here we are in figure one.

And figure one, does anyone want to point something out that they see first?

So feel free to just raise your hand any time.

In Figure 1, I've put red boxes around two pieces.

These are the two pillars that things are being built upon.

Here is the affective inference part, which we'll get to in a second.

And then in the middle is the task-specific sophisticated inference.

So that's deep inference here.

And then, as with a few other figures we've seen, time is moving from left to right.

Now, in this model, there are several boxes that are shaded, and that's because there's this recursive embedding of the models.

So we start with one single time point.

That's this up here on the top level of the model.

This is like the outer layer of the model.

It's kind of like the hour hand of the model you can think of.

And what's happening at each time point of the model itself is that the agent is doing a forecast through time.

So it's kind of like in the year 2000, you're doing a forecast for the next 100 years.

And then it's a recursive string because at 2000, you do an 100 year prediction.

And then 2001, you do an 100 year prediction.

So it's like at each time point, each big hour tick of the big model

we're going to be doing this recursive rolling out of predictions through time.

So that's the biggest structure of the model.

It's top level is the actual time.

That's the real what's happening in the world, we can think.

But then the agent is involved in this recursive anticipatory simulation.

And that's the deep or the sophisticated inference.

Now there's a few other elements to highlight.

And again, anyone can raise their hand at any time.

The first thing is that within this prediction through time, we've seen a couple elements here.

We've seen S, the states, A, how the hidden state estimates are mapped to observations.

We've also seen B, which is how the states change through time or are predicted to change through time.

Again, this is like happening within an instance of the agent.

we've also seen u which is how actions how policy maps on to the way that states change remember that policy enters the b matrix so states are being inferred and then we're inferring the b matrix which is basically how our policies affect how states influence uh how states change through time

Another notable feature here and slightly outside of the blue box, but in the gray box is this gamma or a precision parameter.

So as we do this rollout at a given instant and think ahead through time how things are gonna be, we're not just estimating the state rollout through time, we're also estimating our actions through time and how that influences states and how they change.

And a layer above that is this gamma, which is how sure am I about that?

That's this layer of precision in the estimate.

And that's coming through this G, which is related to the estimated free energy.

So again, for each moment of the rollout, the agent is estimating a bunch of things inside this model.

And then Steven, I'll get to you after I finish this last little bit.

And then in the moment, in the actual timeline of the agent, it is having affect.

And so these are the ways that these two models are combined.

And we can look at some more unpackings in other slides, but I just wanted to start with the actual figure used in the paper itself before going to another citation.

And the sophisticated inference is capturing this deep rollout at an instance with a precision estimator.

And the affective inference is capturing this affective state that carries forward in time.

And so combining affective and sophisticated inference is quite literally structurally combining the patterns of affective inference from this Hesp et al.

citation with the topological model structures of sophisticated inference.

We'll stay on here and continue thinking about it.

Stephen?


SPEAKER_01:
So would I be right in saying that the affective

inference over time is is kind of looking at the shape of how actions look like they're going to turn out over time and and sort of putting a feeling to that in a simplistic term it's like there's it's not so you've got all these fairly discrete moments being looked at or assessed over time and the sort of the shape of that

is what the feeling is about, if that makes sense.

It's like, is it going, is it like, am I predicting this to be a roller coaster ride of uncertainty over the next few minutes?

Or am I expecting it to be smooth?

You know, because that would be the kind of the shape of the observations and the expectations as I stretch it out over time.

And that itself becomes something which is,

you know, has a generative model tracking it.

Would that be one way of saying it?


SPEAKER_00:
Great.

And thanks for phrasing it in the wording as it appears to you.

So let's start with that shape of the prediction.

That shape is actually the shape of a probability distribution.

It's the shape of the probability distribution that is a generative model.

and the shape of that it's kind of like a cloud of points or the bubble that encloses the cloud of points it's a shape that is existing let's just think initially in two dimensions so like a piece of paper and on the y-axis is going to be how rewarding is it lots of reward or negative reward and then the x-axis is precision from low precision to high precision

And so you could imagine that the shape could be all the way clustered up on the top right corner.

So that would be high expected reward, but high precision.

That's the best state.

The lowest state would be like the bottom left where it's like low expected reward and low precision.

So again, the reward prioritizing mindset says the agent's state should correspond to their state estimate.

The active inference perspective says, no, no, no.

The agent's affect is going to be related to their uncertainty estimate more so because if you have a small positive state estimate, but you're very precise, I'm going to give you $5 plus or minus one cent.

You say, great.

And if you say, I'm going to give you $5 plus or minus $25, you say, wait a minute, you're going to maybe take $20 from me?

Even if it's a very small chance, that isn't as good as getting 5 plus or minus 1.

And so this is where the state estimate and the precision estimator are orthogonal.

And so that's what delineates the shape of the distribution that is the generative model of the future.

in a traditional computational psychology model again agent affect would be tracked by the state estimate if it's above zero expected then you're good if it's below zero it's bad whereas in the active inference framework good affect is going to be associated with precise estimates and anxiety negative affect is going to be associated with high uncertainty

And so as this agent runs simulations going forward and rolling them out, we can look at what that looks like.

Even if the expected reward is positive, if the ambiguity is high, the uncertainty is high, it can be anxiety producing.

So that is what we're really tracking is there's the structure of this model, which is like the topology or the connectedness of how different variables influence each other in the model.

But then the outcome of all of this is the shape of the distribution of the prediction of the agent.

And that's respectively just like all predictions are about state as well as uncertainty.

So estimate about the world states, the hidden states in the world, and then a metacognitive estimate of our precision.

And so it's like, this is one click through time.

And then the agent acts, but it's simulating its future actions at this moment.

And then the actual model clicks over.

And then it's in the next time step for the agent, and it starts simulating from that moment going forward.

And so that's one of the features that is something that's a hallmark of this model is it's like the game that's being played has a net positive return.

And let's just assume that we had a degree of agency.

We could probably choose small winning outcomes within this game that we're about to show.

However, by spiraling and by ruminating deep through time,

the agent starts thinking about potential roads that are highly rewarding, as well as a small fraction of paths that are very negative.

And then the uncertainty in that distribution, the spread on the x-axis or the higher uncertainty on the x-axis is actually leading to it being anxiety producing, even if the state estimate is positive.

Stephen?


SPEAKER_01:
Yeah, I think this supports some of the challenges we also find with

you know, if you see the numbers for the stock market, you can get this sense of full sense of certainty that we all know what's going on.

And it gives us for some people that can make them quite relaxed and confident.

And here I go, you know, bullish, as we might say.

And this would sort of support that sense of like,

um if you're given this high level of supposedly precision and um supposed um accuracy and reliability you could get a full sense of of that and and that could be where you know at the same time people challenge that and look at like we need to blow that apart because you know if you if if the confidence in that suddenly disappears and you see it as not being as precise as you thought

the anxiety rises greatly and other ways of thinking might need to be thought, come into play.


SPEAKER_00:
cool good connection there with decision making in markets which are very related to not just estimates of value but estimates of volatility and uncertainty estimates about value and about volatility and so then you have people trading vix you know trading the volatility index or leveraging that and so these are like higher level ambiguity about ambiguity or risk about risk

and then all of that is associated with valence is it a good trade or a bad trade and so that's like coming back to sasha's question about valence and affect it's sort of like well there's so many layers and ways that you could leverage trading but then in the end it's like right but am i going to do action a or not is it going to be a or b is it a good trade or not and that implicitly entails

preferences, like I want to see my number of Bitcoin go up, not down.

But if you didn't have that preference, which is part of the C matrix, which is not shown here as part of the G, if you didn't have a preference, then you wouldn't know which policy to partake in.

Like if you don't know where you're going, every step is a losing step.

But then with a preference vector C, you can actually ask, is this trade a good one or a bad one?

And that is not just is the expected value above zero.

Again, that's rewardlandia.

That's reward first, uncertainty as a satellite.

We're taking uncertainty first, and we still want to partake in preference-guided policy selection.

So instead of doing preferences to reward maximization, and then we'll deal with the uncertainty later, it's like, let's go from preference to policy selection with ambiguity as the heart of what we're doing.

and reward will fall out as a function of reducing uncertainty about optimal action in the world given our preferences and affordances.

So this is sort of the simple rearranging of ideas and reconfiguring that leads to some of these innovative insights in Active Inference.

Steven?


SPEAKER_01:
just adding something into that is it also shows it can work the other way.

I know someone who does trading and he has to monitor his heart rate.

They do a whole thing where he does some relaxing and measures the heart rate and that before doing the work so that the state of the body doesn't

mask what's actually going on you know so this also shows that you know if your physical state and your you know emotional state is is is out of whack in any way your your ability to to do this deep inference might be skewed or even you know uh

lost because the top level in the diagram is not necessarily tapping into the real data it's just tapping into the the the values about context arousal valence etc that is just in the whole system for the person


SPEAKER_00:
Correct.

And there's a lot of behavioral research not done in the active inference framework that shows like when people are in a pessimistic framework, they might be more willing to satisfy with like a smaller gain in the short term.

Whereas when somebody is feeling like abundance or optimistic or confident about their success, they might be able to delay gratification and

partake in larger gains in the future.

And so if you have somebody who's carrying over from the other tab, quite literally, negative affect or adversarial affect about the world, and they flip over their trading account, they're going to be making different kinds of decisions based upon their affective state.

That's going to carry into their estimates of precision, which is related to anxiety, higher precision, lower anxiety.

Those two are negatively correlated.

And then again, in this caption, the imagined action model, precision actions and states and outcomes, everything inside of the gray and the blue box is inside the mind of an agent at a moment.

And so this is the rollout inside of the mind of an agent at a moment.

Let's look at figure two and talk about this game that they actually simulate out.

So this is four-state game.

There's a lot of games that you could simulate, but this is just one game.

And each of the edges reflect transitions that are possible.

So you start on square one, and then you can either move to square two or square three with some probability.

and you start out with a gain of zero it's a neutral state you can go to a safe small gain so if you're just wanting to play it safe you could hang out and go between one and two however also you can go to this high gain three

Now three, you're getting twice as much money.

However, there's a one in three chance that you're going to, or depending on how these edges are parameterized, I should say, there's a chance that you can move into this absorbing state that's painful.

And so the two adjectives there are painful, so it has a negative reward of negative two.

and it's absorbing because you see that it can also uh swallow your trajectory like once you end up in four it's sort of like a negative state and so there are absorbing states in the world like

Death, let's say.

Once you hit this state, you're staying in that state.

And so the agent is starting here.

And so it's like, which square should I walk on to get the most money or something like that?

And so if it just walks around, it's going to end up here really quickly and get into this painful absorbing state.

And so it's thinking, okay, there's some paths I can imagine where I just walk back and forth between one and two.

There's some other paths where I walk around on one, two, three.

I'm just getting money or neutral in every step.

And so what it's doing here is time 1.

It's saying, well, on my next move, I could go to 1, 2, or 3.

And I could go to 4, but actually this edge isn't there, so it's not likely.

So the first step, it's literally all good.

Your expectation is either 1 or 2.

So your kind of estimate is a mean of 1 and 1 half with a small variance.

Okay, but now here, if you stay on one and then continue on one, now you're getting this path at time point three would give you zero.

Here you'd have $1 total, zero, zero, one.

Here you'd have $3 total.

Here you'd have plus one, zero.

So one, two, four.

Here you'd have two plus two is four.

So you're getting all these estimates.

Now you have some fours, but you also have some negative twos and some negative fours.

So as you're rolling out this simulation,

At each time point, actually the expectation is getting higher because if you think about it, you can get negative two or plus two, that cancels out, and then plus one.

So each game you play, each step, the expectation is going to strictly grow.

However, the variance is also going to strictly grow.

And so this is related to a few other probability games where it's like,

will you play this game where we gamble?

And then it's like double or nothing, following double or nothing.

It's like the St.

Petersburg gamble.

It has another name as well.

And the reason why people don't pay infinite money to play these games with an infinite expected value is because the variance is so high.

And so what's happening is as the agent is rolling out in its mind,

these possible trajectories and this could be like chess trajectories or it could be like action selection policies or trades as it's rolling these out the variance is increasing and so that's making it anxious

That's sort of the heart of what's happening.

And then as we trace out the time steps of the model, this is again, just one trace of the simulation.

They're not exploring every parameter, but this is very illustrative.

What happens is that in the first couple of hundred time points of the model,

the valence is good the agent is feeling good it's feeling good about ending up in large reward states and it has a fairly precise model on action and a small fraction of an of the imagined events are negative 0.04 but then what starts happening is because that negative state is absorbing

you get more and more trajectories lining up on this right side where it's like four as an absorbing state and so that is making that uh making the uncertainty strictly rise so around 500 we have this phase transition where the precision on the agent's estimate drops rapidly it's like yeah i'm not sure what's going to happen after 500 time points because there's some paths that are just super bad there's some that are super good

So when it was one time point, I was like pretty sure I was going to get $1 or $2 and I felt great.

But now that you're asking me to predict 500 time points out, I don't know if I'm going to have $100,000 or be $100,000 in debt.

And that's actually getting me anxious that I don't know the precision.

And so again, if someone said, hey, don't worry about the future because the expected value is gonna be positive $1,000.

That's not $1,000 in my pocket.

That's a precise $1,000.

If somebody says the expected value is $1,000, but it has a huge variance, it's gonna be anxiety producing.

And so even though the threat perception is inching up from 0.04 to 0.1, so it still is not more than a 10% barely chance of these highly threatening events happening.

it turns out that the uncertainty is what drives the valence, not the percentage of negative events.

And then also, interestingly, and this could be over-reading into one trace of the model, but the phase transition and the precision and valence happen at exactly the same time, around 500, right before 500.

Interestingly, the threat perception is going down overall, leading up to 500, and then it starts kicking up again.

That's sort of like, okay, I drank all the coffee.

I'm feeling great.

It's like, oh no, I'm starting to get a little tired.

That coffee's not gonna work.

That quantitative easing is not gonna work.

And so even though the threat perception actually isn't as high as it has been,

the precision being so low leads to the valence being negative because the valence is being derived again from the anxiety, from the uncertainty, not from the expected value.

This is like a point we can always return to because it's a great example of the complementarity of traditional reward centric understandings of reward learning and this more precision guided understanding of action selection in the niche.

Okay.

Any thoughts on those figures?

That's sort of the contribution of the paper in many ways is just to make this combination model that features the affective inference of the agent in their timeline.

And then at each moment, this imagined rollout.

And yeah, it's a discrete time model.

And yeah, any thoughts or questions or there's more stuff we can continue to look through.

Cool, so just to put it up there and anyone can raise your hand.

Table one has some of the mathematical details.

So hopefully with Ryan and any other authors next week, we'll be able to talk about these details.

And here is that gamma, which is one over the precision.

So sometimes you'll hear about it as,

precision maximizing.

Other times you'll hear about it as uncertainty minimizing.

And those two, the min and the max, are related by this one over feature.

Just one point.

So it's kind of like on networks,

there's one type of problem called the max flow.

Like you want to have flow through a network, internet or water or resources.

And so you might want to know what the cut is that's going to be most damaging or the least damaging to the network.

And so sometimes the most and the least are related by this one over feature.

Um, okay.

Any thoughts?

Because there's a lot of formalisms we can continue to walk through and we have a good amount of time, but like maybe just pausing here, what would be something you would say to somebody who had never heard of active inference and just what is something that it captures that other models haven't captured or something that you'd be excited to see it head in the direction towards Steven.

Thanks.


SPEAKER_01:
Well, I think the counterintuitive nature and the ability for things to be unexpectedly stable and unexpectedly transition is something that I don't know of in a way that traditional models of the world

would show you, you know, because, you know, you could be like, oh, people would normally either just take one of those moments and then that gets extrapolated for everything.

But actually you need to follow it over time because it matters a lot.


SPEAKER_00:
It does, and it needs to be kind of through time in two ways.

The actual agent needs to be progressing through time, but at each time point, the agent needs to be simulating time points of the past, present, and future.

And so that's this like double timeline that's happening with the model's topology here.

is you could have another pink box right next to this one and that would be like the next time point of the agents but it's like right now it's 10 a.m and i'm looking ahead to 10 11 12 and then it's 11 and then i'm thinking ahead again but i'm also looking back and all of that is being spun out of a generative model of a trajectory stephen


SPEAKER_01:
yeah that's and i thought it was interesting i haven't seen a positive valence and a negative valence i haven't seen sort of two dynamics mapped alongside each other simultaneously normally there's just the one so i thought that's something that's kind of uh the good and bad as two you know it's not just about good

it's about the blending of good and bad i thought that's good so you i mean theoretically you could have loads of variants of that you know you could have good the bad the ugly yep yep valence just like uh an electron or a proton you know valence of plus or minus


SPEAKER_00:
that's plus to minus positive negative valence but affect is a multi-dimensional landscape and so that is something that could be explored in a higher dimensional way but again let's think about that trading example there's so many ways oh it's an elegant trade it's an ugly trade it was a clumsy one it was a uh one with very much foresight or it was a retributive trade all these adjectives but then what does it boil down to given my preferences and my affordances was it a good trade and

And that, if your preferences are, hey, I have too much money, I'm trying to sponsor a couple people who have less crypto than me, then a quote good trade given that preference vector might not look like a preference or a policy selected by somebody with a different preference.

So that is where agent-specific preferences come into play, is you're never going to get away from that.

The agent is not doing this neutral calculation followed by a secondary step of policy selection.

The preferences for policy and for outcomes are entrenched within the kernel of this model at every single time step.

Stephen?


SPEAKER_01:
And that gives a plausible Bayesian route to your gut feeling.

I mean, some of those...

emotions like you're saying the an elegant trade might be kind of a cognitive process and uh and uh some other type of part of it might be kicking your deep you know there's certain times when you're thinking about stuff and then suddenly you get a sinking feeling in your stomach you know and it's like well that that um that who knows where that's coming from but this gives a you only get one overall

sort of gut feeling you only get one overall sort of route to actively infer how to take action in the world and all these different things they only become available to you when you maybe reflect on it later and you have to describe what why you had that gut feeling like you just mentioned

There could be hundreds of different things all being Bayesianly integrated.


SPEAKER_00:
Exactly.

And that's this go, no go.

Hey, I'm just not feeling good about this, this whole vacation, or I'm just feeling really great.

I don't know what's behind the door number four, but I'm feeling great about it.

That valence aspect of prediction for the future is like the highest level control summary statistic.

It's like, are we going to accept this person into the college or not?

so many other dimensions but it gets distilled into the choke point through the markov blanket of action it gets distilled into go or no go on that specific person or that specific policy or that specific outcome and so that's actually the justification for the use of valence as a simple positive to negative even though there's so many other dimensions to affect overall and

it does get distilled into this gut feeling which you can go into and and uh you can cognize you can add adjectives on top but it's sort of interesting it's like is the gut feeling just positive versus negative and when you introspect do you get higher and higher levels of abstracted polysyllabic emotions or do you get deeper and deeper into your body

and into increasingly summary statistics like go, no go, freeze, fight.

These are things like go and no go.

Let's look through a couple.

Oh, Sasha, go ahead.


SPEAKER_02:
That's kind of what I was thinking is

with this model of valence, would a human participant, let's say, be able to self-report in the similar way about their valence during an experience?

Or is this kind of like the...

hidden by the Markov blanket and we can have these models to help explain what was happening at the time, but those states might be unaccessible to people actually experiencing this decision-making process.

And yeah, it just reminds me of, I guess, how people spend money and thinking about economic decisions that people make that are either logical or not logical, and that it really has little to do with the

money that we have and then the mindset that we have about the money and like how we ought to be spending it and um you know who we want to be supporting or not with our uh investment decision and yeah it just really uh i don't know it it's at such a higher level than um just thinking about someone's salary it's how they interact with the economic system and um

what role they might play in it.

And it just, especially in the holidays, like all I can think is we're so deeply influenced by times and like season and tradition with how we choose to how and where we choose to spend money.


SPEAKER_00:
Right.

You send money.

You literally give money away to people on their birthday, on their biorhythm or, you know, in the holiday season.

And then also what you just said, Sasha, reminded me of like coarse graining estimates and why it's so important.

If your economic estimate that you're making is will we have enough or not?

you might be very confident that you'll have enough.

Say, I don't know the details, but I'm confident that we'll have enough.

Or even, I'm confident we're not going to have enough and we're going to have to do XYZ policy.

Okay, we can work with that.

But if you're trying to do an estimate down to the cent, you might have a very noisy estimate.

And then,

Apparently people can get quite anxious about relatively small amounts of money that are way up there in the big salary range.

Is it going to be this number or a slightly higher or a slightly smaller number?

So there's this jockeying and uncertainty up there at the high level because they're doing a nine figure precision estimate with a large variance.

Whereas there's something to be said for coarse graining and simplifying and functionalizing it.

and actually not just reducing the cognitive load, but increasing the precision of our estimate, even if the state estimate is inferior.

So there's so much there to continue exploring.

Let's just walk through some more of these formalisms because most of this paper, we can just scroll through it just to show you, but it's 10 pages long, including references.

And basically they introduce, give that figure one that we just looked at, give figure two that we just looked at, give the table we looked at,

show the results for figure three and there's the whole paper so short very very short so let's go into these two citations underlying the paper centrally and then just find out what they might show us and we're going to be talking about deeply felt affect in april i believe so again sophisticated affective inference is drawing the affect from this left side and the temporal depth from the right side

Three areas we could imagine this area of research heading towards would be more advanced simulations.

So agents that are simulating in continuous time or with higher dimensional action policies, just like we've seen from ACT-INF 8 with scaling active inference or towards the computational psychiatry angle, something that Ryan Smith and others have written review papers about and a lot of empirical work as well.

So we'll hear more about that next time.

And then there's this angle of the models for robotics,

which is like, what if the drone had a top level summary statistic that's just like, is this mission going well or not?

And if not, then that might engage some policies like head back home or pause or land or get to safety.

But that's a top level summary statistic that even robots might be able to utilize.

And I know that some people are doing that kind of stuff.

So in sophisticated inference, and this is to bridge the gap between the formalisms and the actual sentences that are driving us, we're gonna kind of write it in a little bit of a hybrid script, like a rebus.

So the big question that agents are trying to answer in the world and in sophisticated inference is how can we have the best policy over a given time horizon?

So not what's the best chess move now,

It's what's the best chess move over the time horizon that I'm considering.

And tied up in that question about evaluating the best policy over a given time horizon is the question of what are the consequences of policies through time and how do we estimate that?

What is our estimate?

And what that is related to, how we benchmark, how we compare policies and their consequences is specifically related to how we understand observations being mapped to hidden states and our policy, which is how our actions are influencing the way that hidden states change between each other.

That's that B matrix.

And the generalized free energy, which is this Parr and Friston citation, so another citation link away from the IY paper, is that the generalized free energy basically consists of one model that conditions on policies and then asks about how state transitions occur.

So conditioning on me exercising every day, what kind of state transitions can I expect to see in the world?

And then the second part of the free energy is conditioning on states

What policies should be selected?

So this is the two stroke engine.

Condition on what I know about the world, world states and causal model of the world, generative model of the world.

What is my policy selection?

That's the control theory.

That's the cybernetic side.

That's the action side, action looking side.

then this first part is again conditioning on my policies in the world what are the state transitions and how do those change and there's more nuance because of course we're just describing it in natural language but this is sort of the broad outline one half of free energy conditions on policy the other part um selects policy and basically vice versa with state transitions

And so here's related to this Q and P, which we saw in tail of two densities.

We talked about it again in integrating internalism and externalism.

Q conditions on policy.

That's a vertical line P. You're estimating O and S through T, through time, as conditioned on P. And then P is conditioned on states.

So again, we're conditioning on two different parts, and that's also how we end up using this two-stroke engine to do inference in complex settings.

We also talked about amortized inference, but don't wanna go into that here.

Blue, this is the slide that always makes me think about you with where's the information gain and could the pure information gaining agent get reward or does the pure reward seeker gain information?

And like, where is this interplay between the intrinsic values

or the salience, and then the intrinsic value in the novelty.

So the way that these two and all these different features and the different representations, I mean, there's so much to say about and think here.

But Stephen, and then anyone else.


SPEAKER_01:
yeah could you just go back to that the one where you put those two terms between action the generative model and the that one yeah see that's kind of interesting that generative model says state transitions policies and policies final states so policies final states is probably a bit more action and like least energy route and the other one maybe is a bit more generative model is a bit more

processing sensory data is a bit more cognitive sort of feely cognition if that makes sense I'd like that if that if that's correct but I is it like you've got like a perception action part to those two in a way one's a bit more perception heavy one's a bit more action heavy or let's return to that in a second blue


SPEAKER_03:
So I just wanted to comment while we're here about what you had said in the introduction video related to why are we always trying to estimate the states when really we should be trying to estimate the best policy, right?

That's what we should be predicting.

We're not trying to predict the states.

Bring three jackets instead of one jacket, and then you'll always be at the right temperature.

If we prepare versus trying to predict the state of what's going to be, I think that that just was a neat thought that occurred to me.


SPEAKER_00:
Exactly.

When there's too much emphasis on precise estimates of world states,

it's implicitly like we're focusing so much on precise estimates of world states that we'll just know what to do when we have that precise estimate it's like that's not true it turns out that maybe again a one digit of precision or even uh to the nearest 10 of the temperature from a policy perspective you'll be fine if it's um 20 celsius plus or minus 10 you might be fine within a broad range of world state predictions from an action selection policy

because your action selection could be, cool, I'll throw a jacket in my backpack and then I'll be fine, whatever it is.

And so that is this two-stroke engine, this handoff or handshake between co-estimating states and their transitions, like weather in the world and the transitions between different weather states in the world, given what you know about weather.

We're not on Mars.

It's not gonna do a acid rainstorm at 400 degrees.

So it's like, given what we know about the distributions of the niche, the statistical regularities of the niche,

We want to be co-estimating states and our action, action and perception.

And we want to be doing that in a way that suffices, that's adequate to remain within our preferred areas, our preferred strange attractors, where we're able to basically have reduced uncertainty about the things that we care about, the physiological variables that matter, like our temperature.

And okay, so it is very cool.

And again, just for those who haven't seen these kinds of equations that much, the vertical line means conditions on, and then whatever follows it, you can think of is like fixed.

So it's like you have a spreadsheet and let's just say that the rows are policies and the columns are like state transitions.

And so when you fix on one, it's like you pick out, you fix a column, and then you're asking about the distribution of that vector fixed on that column.

And similarly, you could fix a row, select one row, condition on one row, and then ask about the distribution in that line going across.

so that's the full matrix and also that's related to what's called marginal likelihood in bayesian statistics the margin is literally the margin of the page when they used to do it with physical pages so it's like rows by columns and the marginal likelihood is related to the margin of the page which is just so fun it's kind of like a floppy disk icon but for statistics um

Here's another cool piece of the sophisticated inference paper.

And I think there's a lot I'd like to learn and ask here to Casper and others, but this formulation of policy P, PI selection, people can see my red thing, right?

pi is related to the argument minimization of free energy of policy so this is saying the policy that gets selected pi is going to be like a free energy minimization of g as a function of all the policies that could be done so of my affordances we're going to do a free energy minimization and that's how i'm going to select my policy okay well what is g of pi

g of pi is related to the expectation of what?

Risk minus ambiguity.

So high risk is going to be a higher number, and that's worse.

In this case, it's like golf.

We want to minimize the free energy.

And so we're jointly trading off and there's some natural logs and some negative natural logs.

So it's sometimes a little bit hard to see if you're going like up or downhill, which is why it's so important to have it phrased like this.

But basically risk is bad and ambiguity is bad, but it turns out with the natural logs that you can just think of them as being combined.

So it's like, if we seek to maximize our precision, minimize risk, minimize ambiguity,

what we'll be doing is actually a strict bound on the intrinsic value and the extrinsic value so this formulation of search of information gain related to risk and ambiguity turns out to be strictly bounded by this value-driven process so

worldview related to reward you see that agents are reward seekers and uncertainty is something that they just simply deal with or they figure out or they thrive despite in the active inference perspective agents are uncertainty reducers who obtain rewards and if they don't they die that's evolution and so we're putting risk and uncertainty first and so instead of saying like well the agent predicts value and that ends up reducing its uncertainty about the world

we're actually saying, no, it's a way more tractable calculation for the agent to be reducing its uncertainty about action.

Again, not just about states of the world, but the agent is reducing its uncertainty about action through action.

It's performing optimal foraging experimentation in order to reduce its uncertainty about best actions.

And it turns out that that is going to have a bound.

It's always going to be converging towards this perfect trade-off between intrinsic and extrinsic value.

And special cases fall out of this where certain variables are basically fixed to zero.

So if you have no prior at the beginning and you just let it start

uh kind of draw from your first observations you take those and you use that as your first hyper prior that's like the parametric empirical base

And in this case, you end up recapitulating several things from Bayesian statistics like optimal Bayesian design or just surprise.

And then similarly, if you set the ambiguity to zero, so like the case in active inference eight, where the states were perfectly observable, like the pendulum and the mountain car, it wasn't like it was getting a noisy estimate of where it was on the mountain.

It knew exactly where it was on a mountain.

And in the case of no ambiguity about states, again, ambiguity is the P distribution of observations given states.

So that's like, given that it's daytime, am I seeing photons?

So if you have no ambiguity about that, then this term is zero, right?

Which makes risk minimization the imperative.

And that's why when you get no ambiguity, you find that this whole framework collapses into risk control.

Whereas if you had risk collapse to zero, like it's a no risk situation, the only imperative would be reduction of ambiguity.

So now let's think about that in reward land.

If you didn't have to seek value, it could be purely novelty based search.

Whereas if the only imperative were to obtain short-term value, you would not expect that kind of agent to be engaged in a creative search.

So it's sort of like each of these are phrasings, whether risk minus ambiguity or intrinsic minus extrinsic value, you can see how either one of them could be like,

zeroed out and what that would do for the other side or you could imagine in realistic situations where they're sort of like tied up in a specific way so again you could imagine a no risk scenario a no ambiguity scenario no intrinsic or no extrinsic any of these combinations we'll do stephen and then sasha


SPEAKER_01:
Yeah, this is really good the way you're putting this together here and explaining it.

I think also obviously these things, sometimes they become really clear when it's an extreme case and then you've got this blending that's going on.

It's not fully any one of these.

But I think that no ambiguity piece is a useful one to think about what happens sometimes when

well, you could say, I don't know if you'd say Trump, but in the politics, when something is, if people say these people are bad,

and you know you often see that even at work and it's a no-brainer things are said to be a no-brainer then suddenly everyone the the the intrinsic value of trying to explore other options is can be collapsed in people and they just go with the extrinsic value of gain because there's no there's supposedly no ambiguity and i think that that could

be interested in social.


SPEAKER_00:
Yep.

Yep.

Like we know exactly how dangerous this virus is.

There's no discussion about what the risk minimizing policy is.

Oh, wait a minute.

Is there Sasha?


SPEAKER_02:
Okay.

Very leading questions.

In hearing this description, just really thinking about using this to define play.

And really seeing play as the combination of factors that has very low risk, but like high value of exploration.

And so setting up things that have low risk, but high opportunities for curiosity, that would be, I guess, for me fit into the category of play.

So that's a good way to think about it.

Yeah.


SPEAKER_00:
another fun part there is like when value is the imperative it's difficult to be exploratory and creative it's like maslow's hierarchy of needs but then when there can be the space where you can actually engage in a novelty or a curiosity guided search kind of the prerequisite the enabling feature of that is sufficiency at a material level

So that isn't simply related to reward.

We don't just give people reward, kind of dial up their dopamine, and then they'll be acting creatively.

It's actually like we need to scaffold the agent in the environment in a way where it can reduce its risk so that it can tolerate higher ambiguity or reduce ambiguity so that we can engage with higher risk scenarios.

So sometimes these combinations of features, even though risk and ambiguity, they're just English words.

So really it is about the formalism, but it helps us reimagine and repartition and fluidly move across partitionings for how to think about these nuanced control problems.

Sasha.


SPEAKER_02:
Also, it makes me think about how this is commonly studied, like in psychology lab spaces and

Yeah, really thinking about how using this framework to figure out what would make the participants, what state that would put them in, as well as how

they feel about the risk and ambiguity of the situation that they're in, because depending on how you feel about participating in research, that would change what state of exploration or curiosity you might be in.

And that, I don't know, it just really makes me rethink a lot of research in this field because it's ignoring, or I guess, excluding one of the key variables of this process.


SPEAKER_00:
Yep.

And it makes me think about safe spaces, but we could also say, how risky is this space?

That's sort of related to safety.

Those are like one over each other, you know, risk equals one over safety, but also how ambiguous is this space?

We want this space to be safe, so not risky, but we also want this space to be not ambiguous.

We want to be clear and communicating about what this space is.

However, we don't want to make it so unambiguous there's no room to play.

So we want to have the optimal level of ambiguity and risk.

That's controlled novelty.

That's Pareto optimality.

That's free energy minimization.

That's living on the frontier between performance and innovation.

So that's where all these models can take us.

And that's why it re-imagines the bottom up versus top down, the explore versus exploit.

the maximization in the long-term versus the short-term, those are all coming from the reward mentality.

And when we think about the situated agent reducing uncertainty about action through action, we have a whole new set of ways to talk about these questions.

Stephen, you're then blue.


SPEAKER_01:
And bringing it back as well with this paper and the way it talks about the temple depth and is it can also combine with this give a way to think about what they call stages of readiness.

Because people sort of think, oh, they could read this and like, okay, we've made it safe.

I've said it's safe.

I've told everyone it's safe.

Well, it might be you've told them it's safe and you've demonstrated it's safe and the environment's been safe, but it might be five minutes, it might be three weeks of training before people start to

bring that safety into the way that they balance these different parameters.

And it could be that it crashes quite rapidly, positive or negatively, as we saw earlier with the way that the dynamics work.

And that shows the importance of facilitation and the importance of the way the space is set up, the safety between participants and all that sort of stuff.


SPEAKER_00:
Cool.

Blue?


SPEAKER_03:
So I know that you said we're not going to unpack the formalism of the intrinsic value and the extrinsic value, or at least you said that on the intro video.

But those two things, it makes me think the special cases really.

They talked about no intrinsic value, but what about no extrinsic value?

It just always reminds me of what about just exploring for info gain or just out of curiosity to satisfy that for intrinsic value?


SPEAKER_00:
i did note that i was like waiting for i was like okay no risk no ambiguity um then no intrinsic it's like the next one and uh i wondered if that actually was related to maximum entropy so it's like given a distribution if you have no preferences for value then you're just sampling your preference is to sample the distribution unadulterated and so that is like max info game max ent

sampling neutrally without preference across a distribution.

And so that's very related to statistical sampling techniques because those computer algorithms have been designed actually without a preference vector in mind just to sample distributions like Monte Carlo Markov chain methods.

And so now we're taking a sampling agent, you're sampling temperatures, but you don't want just all temperatures for that distribution.

You want to sample repeatedly from body temperature.

However, there might be another distribution where you do want to have a very, very exploratory sampling pattern.

just see if there's any other interesting stuff so here is from sophisticated inference uh another kind of rollout and then here we can see observations and then there's different actions that can be taken and each of those actions would lead to different observations and so on and so it gets exponentially large and this is just like early chess algorithms they used to do if this then that if this then that and they would do these rollouts and then they'd prune this branch oh that doesn't look that good so i'm not going to explore that way

And since those sort of tree pruning chess algorithms, there's been many developments in chess and other algorithms for this kind of decision-making.

And so here's the structure of the model and here's that rollout.

And then one big question is how can we look down the branches that are most informative, not just fantasize and dream about the one in a billion branches that's the best,

not negatively ruminate and get dwelling on the one branch that's the worst.

But how can we sample information across this branching tree pattern in a way that leads to effective action in the world?

And then here's another integrative plot that they made, which is showing that basically there's a few ways to think about active inference, what it's doing.

Here, it's minimizing risk and minimizing ambiguity, okay?

So that's related to this phrasing on the top.

Minimize risk, minimize ambiguity, okay?

Make it safe, make it clear.

Positive way of saying it, the negative way is minimize risk, minimize ambiguity.

So policies, states, observations.

States are the hidden part of the world.

Observations, you know, photons on my retina, policies, where my eyes move.

And so the risk framing is you should be minimizing free energy in a way that reduces risk and ambiguity about this chain that connects policy states and outcomes.

Another branch of active inference is actually sort of like a yin and yang.

This is like a push-pull.

It's saying minimize the entropy of the distribution of outcomes.

So achieve...

predictable outcomes, reduce big reduction on there and expands here with the information gain.

And so instead of, hey, do something that's like creative, but FYI, don't be risky and don't be ambiguous.

Here is like, be as clear as you can and predictable as you can the outcomes while being as info gain and creative as you possibly can.

And so I like this because it's sort of like instead of two things that you wanna run away from, and then hopefully you become creative in the gap, this is like something to run away from and something to run towards.

So it rephrases that risk and ambiguity minimization as actually a push-pull of a minimization of surprise about observations and a maximization of the information gain given the policies that can be carried out.

So I kind of like that graphic.

And then, just in this last couple minutes, Casper and several other authors will be on in early April, hopefully before then as well, for Act Inf 19, and that will be pretty cool.

But here's where they walk step by step.

M1, at each time step, the agent is inferring how hidden states are related to outcomes.

then we see that S, A, O mapping, and now we have this, and D is the initial states.

Now we're going to take this single time point model and use B to ask how states change through time.

Then we're going to take M2, that state's changing through time, and put it within a bigger model of M3, which is action.

So policy is influencing how states change their time, the B matrix.

Policy is being selected by what?

Minimization of free energy, G. How do we calculate minimized free energy?

Including a preference vector, C.

And then we have a prior over policies.

This is related to the affordances.

So now we're not just doing state prediction at a moment, but through time, M1 and M2, and we're going to be doing policy selection on that right now.

And then where we got to in sophisticated affective inference was this M4 layer, which is state observation mappings through time or at a given moment and through time with the A and the B and choosing policy using our C preference vector and G, the minimizing of free energy.

And about that free energy, we're gonna have a precision.

We're gonna have a prior on our precision.

This is our metacognition.

And here's where valence enters the picture, which is anxiety bad.

Anxiety comes from high uncertainty.

So if you have high precision on free energy, you're going to feel better.

If you have low uncertainty or low certainty about free energy, it's going to lead to anxiety.

Not because the expected value of the outcomes is negative.

So not within that reward first mindset, but in the precision first mindset, even a positive expected value with a high variance can be anxiety producing.

So we kind of return to the paper as being the summation of these two citations building before, just like so many other papers are building on citations.

It's always just like tips of icebergs all the way down, Markov citation blankets all the way down.

and we can ask our kind of usual sets of questions and people can provide their closing thoughts or questions or what would they like addressed in 11.2 next week?

Just what's something fun that anyone or each of you thought about today?

then also to kind of maybe even close on a note about how we can have accessible formalisms for active inference and include people in the discussion who speak math or not who speak english or not so how do we communicate it make it exciting build those bridges to other areas any closing thoughts on that stephen


SPEAKER_01:
Lots of really good stuff.

I mean, I suppose there's some elements of this where I'd love to find a way.

I'm trying to find a way to take people into an activity where...

you know, well, I suppose some of us do that, you know, with play or with an exploration, different types of exploration activities that maybe illustrate some of these and then they get unpacked after because it may be just some of this stuff is in its nature pretty involved, but that's still good.

So I like the way you've been describing it, but maybe there's, yeah, maybe there's something then that this now bridges with something else that's experiential.


SPEAKER_00:
Anyone else have a thought on that?

Yeah, I think that the questions I would like to have the authors or anyone familiar with the formalism help us on would be, how can we communicate and understand the differences between partitioning risk and ambiguity this way or that way?

How do we know what to apply in what scenario?

How does it change how we act?

How does it change how we model systems?

How would we infer it in a system?

How do we disconnect the qualia, the experience of surprise, uncertainty, ambiguity, risk, valence,

How do we disconnect from the qualia so that we can apply it to systems that are on the other side of our Markov blanket, systems that we are not experiencing, but are acting in a way, in an intentional stance way, they're acting as if they had these kinds of attributes.

Well, if there's any other thoughts, we can definitely hear it, but otherwise, very excellent discussion.

Wait, Stephen, do you have another point?

Oh, so everyone, thanks for the awesome discussion.

There's a feedback form in the calendar invite if you'd like to take a look and fill that out.

Otherwise, people who are watching it live and in replay, thanks so much.

It means a lot to us.

And we hope to hear your questions, hear your thoughts so that we can address them in 11.2.

And thanks so much for a great 2020.

We are almost there.

This is the penultimate active stream.

So everyone, thanks so much.

And we'll talk later.
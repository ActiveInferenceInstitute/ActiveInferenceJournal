[
  {
    "start": 6.459,
    "end": 8.02,
    "text": " Hello and welcome everyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 8.24,
    "end": 10.321,
    "text": "This is the Active Inference Livestream.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 10.521,
    "end": 15.064,
    "text": "It is ACT-INF Livestream 8.2 and it is November 17th, 2020.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 16.485,
    "end": 19.587,
    "text": "So welcome everyone, listeners and participants.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 20.646,
    "end": 22.247,
    "text": " Welcome to Team Calm, everyone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 22.767,
    "end": 28.709,
    "text": "We are an experiment in online team communication, learning, and practice related to active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 29.349,
    "end": 36.632,
    "text": "You can find us at our website, our Twitter, our email, on our public Keybase team, or on YouTube.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 37.432,
    "end": 43.455,
    "text": "This is a recorded and an archived livestream, so please do provide us with feedback so that we can improve on our work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 44.6,
    "end": 48.963,
    "text": " All backgrounds and perspectives are welcome here in learning about these questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 49.643,
    "end": 57.647,
    "text": "And as far as video etiquette for live streams goes, just remember to mute if there's noise in the background and raise your hands so we can hear from everyone on the stack.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 58.288,
    "end": 60.169,
    "text": "Use respectful speech, behavior, et cetera.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 61.465,
    "end": 67.449,
    "text": " Today in exciting Act ImpStream 8.2, we are going to have introductions and warmups.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 68.089,
    "end": 71.512,
    "text": "And then we will, at whatever rate makes sense, welcome Shannon.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 72.112,
    "end": 73.633,
    "text": "We will walk through the sections of 8.2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 74.994,
    "end": 81.638,
    "text": "As with 8.0 and 8.1, we are going to be discussing the paper Scaling Active Inference by Shantz et al.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 82.379,
    "end": 81.638,
    "text": "2019."
  },
  {
    "start": 82.559,
    "end": 85.501,
    "text": "And Alec, thanks so much for coming on to the show today.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 86.222,
    "end": 90.805,
    "text": "We're going to talk about the goals and the roadmap of the paper, hopefully from the author's perspective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 91.525,
    "end": 97.446,
    "text": " We'll then have a little pause just for an open Q&A because there's a lot of different directions this paper can take one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 97.666,
    "end": 106.168,
    "text": "So we'll just pause there and then we'll continue through the figures, just two of them, and just address a few different domains of follow up questions about the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 106.729,
    "end": 112.43,
    "text": "And then if we want to slash still have time, we can go through some of the notation and math again.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 113.558,
    "end": 118.06,
    "text": " For the rest of 2020, we are going to be moving into Papers 9, 10, and 11.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 118.761,
    "end": 120.081,
    "text": "So check it out on Twitter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 120.442,
    "end": 123.303,
    "text": "Paper 9 is going to be about consciousness.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 123.823,
    "end": 126.885,
    "text": "Paper 10 is about scripts and about social interactions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 126.945,
    "end": 132.348,
    "text": "And Paper 11 is about modeling under the sophisticated effective inference framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 133.971,
    "end": 147.382,
    "text": " for the intros and warmups, if everyone could just introduce their name and their location, whatever else they'd like to state, especially our first time guests, and then just pass to somebody who has not spoken.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 147.542,
    "end": 152.467,
    "text": "So I'll start, I'm Daniel, I'm in California, and I will pass it to Alec.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 155.429,
    "end": 156.27,
    "text": "Hey, everyone.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 156.51,
    "end": 157.751,
    "text": "Yeah, I'm Alec Chance.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 158.231,
    "end": 159.452,
    "text": "I'm based in Brighton.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 160.489,
    "end": 162.99,
    "text": " final year PhD student at Sussex.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 163.87,
    "end": 165.15,
    "text": "Thanks for having me on the stream.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 168.471,
    "end": 170.392,
    "text": "We will go to Sasha.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 174.953,
    "end": 176.494,
    "text": "Hi, my name is Sasha.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 176.694,
    "end": 180.915,
    "text": "I'm also a graduate student and I'm based out of Davis, California.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 181.615,
    "end": 183.376,
    "text": "I will pass it to Stephen.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 190.31,
    "end": 194.233,
    "text": " Stephen, you're unmuted, but it doesn't have any sound.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 194.313,
    "end": 197.175,
    "text": "Maybe just reload and temporarily let's just go to Shannon.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 197.195,
    "end": 200.938,
    "text": "Hi, guys.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 201.218,
    "end": 201.878,
    "text": "I'm Shannon.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 202.219,
    "end": 208.263,
    "text": "I'm based at the University of California in Merced, but currently in South Dakota.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 210.504,
    "end": 211.245,
    "text": "I'll pass it to Blu.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 215.015,
    "end": 216.116,
    "text": " Hi, I'm Blue Knight.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 216.417,
    "end": 221.422,
    "text": "I am an independent research consultant and I am based out of New Mexico.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 222.163,
    "end": 224.025,
    "text": "I will pass it to Alex.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 228.009,
    "end": 229.731,
    "text": "Hi, my name is Alex.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 230.352,
    "end": 235.898,
    "text": "I'm in Moscow, Russia, and I'm a researcher in systems management school and I pass it to Ivan.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 239.175,
    "end": 240.376,
    "text": " Hi, my name is Ivan.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 240.396,
    "end": 246.579,
    "text": "I'm in Russia, Moscow, and I pass it to Stephen.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 247.04,
    "end": 247.82,
    "text": "Stephen, are you here?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 248.24,
    "end": 249.181,
    "text": "I'm going to try again.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 249.221,
    "end": 249.741,
    "text": "Does that work?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 249.801,
    "end": 250.041,
    "text": "Yes.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 250.101,
    "end": 250.301,
    "text": "Yep.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 251.082,
    "end": 251.322,
    "text": "Okay.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 251.362,
    "end": 252.182,
    "text": "Hi, I'm Stephen.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 252.383,
    "end": 253.543,
    "text": "I'm based in Toronto.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 253.583,
    "end": 261.628,
    "text": "I'm actually studying a practice-based PhD through Canterbury Christ Church University in the UK, and I'm pleased to be here.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 263.229,
    "end": 263.509,
    "text": "And now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 265.592,
    "end": 272.261,
    "text": " I am Mel Andrews and I'm in Cincinnati doing a PhD.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 273.502,
    "end": 273.823,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 273.843,
    "end": 276.526,
    "text": "A lot of different stages and areas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 276.586,
    "end": 280.872,
    "text": "So it will be a great discussion to bring to the technical side with this paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 282.042,
    "end": 286.925,
    "text": " For the warmup questions, people can just raise their hand as they'd like to speak.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 287.666,
    "end": 290.408,
    "text": "And I will just put up the first two questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 290.868,
    "end": 295.051,
    "text": "They are, for today's discussion, what is something that you are excited about?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 295.691,
    "end": 299.854,
    "text": "And then the second question is, what is a question you're wondering about?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 300.294,
    "end": 308.84,
    "text": "So while people are thinking about it or raising their hand, one question I was wondering about was how do we trade off between explore and exploit?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 309.48,
    "end": 321.285,
    "text": " In the two figures we saw examples of explore or exploit, but what is required to enact something that is able to mediate between the two of them or find a compromise between those two strategies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 324.106,
    "end": 326.867,
    "text": "Anyone have any thoughts or want to raise their hand.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 330.149,
    "end": 330.369,
    "text": "Stephen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 331.974,
    "end": 345.301,
    "text": " Yeah, I'm also excited about the explore and the exploit side as well, to be honest, because I think that opens up a big shift in the way that we socially interact, in the way that we organize ourselves.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 345.942,
    "end": 353.566,
    "text": "So trying to look at a real foundational way to sort of approach that is going to be quite helpful, I think, in a lot of fields of practice.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 355.047,
    "end": 355.307,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 355.507,
    "end": 357.468,
    "text": "Alec, and then anyone else who raises their hand.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 358.973,
    "end": 361.155,
    "text": " Yeah, am I unmuted or muted?",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 361.195,
    "end": 362.917,
    "text": "No, you sound good, sound good.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 363.717,
    "end": 368.281,
    "text": "I just wanted to comment on your comment, because I'm also very interested in that.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 368.722,
    "end": 379.571,
    "text": "But I guess I'll go on the record saying that I'm not convinced that Active Inference provides, sometimes it's said to provide a solution for the explore, exploit dilemma.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 379.811,
    "end": 381.513,
    "text": "I don't think it does.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 382.494,
    "end": 386.117,
    "text": "I think it recasts it, so now it's not a question of balancing",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 388.01,
    "end": 393.075,
    "text": " exploration exploitation, but they're both zoomed into this different objective function.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 393.095,
    "end": 401.102,
    "text": "But in practice, there isn't a sort of magic balancing act that you get out of this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 401.122,
    "end": 406.907,
    "text": "So I think it's a good way to recast the exploration exploitation dilemma, but certainly not a solution to the problem.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 407.748,
    "end": 408.068,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 408.449,
    "end": 408.629,
    "text": "Luke?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 410.973,
    "end": 413.534,
    "text": " So I kind of have like a tangential question.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 413.554,
    "end": 432.419,
    "text": "And thank you, Alec, for your comments, because it just plays right into, I think what's missing, it's very clear that if the environment is homogenous, there's no real need for exploration because the agent is not going to gain any, you know, any additional progress by exploring the environment.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 432.459,
    "end": 432.579,
    "text": "But",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 433.259,
    "end": 435.881,
    "text": " I just wonder where autonomy factors in, right?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 435.921,
    "end": 453.049,
    "text": "Like, so, you know, some agents have more or less control over their environment and there's no like real room for this or not that I've seen anyway in the active inference framework, like where the differing levels of autonomy of the agent play into the need to explore versus exploit.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 455.67,
    "end": 455.951,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 456.851,
    "end": 458.432,
    "text": "Good question, Stephen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 460.626,
    "end": 475.526,
    "text": " Yeah, I think that that point about how to look at the agent starts to bring in the inactive component a lot more, I think, and the generative model that's kind of sitting inside the person and existential kind of sense making there.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 475.726,
    "end": 498.58,
    "text": " so i think that's that also yeah it's like pushing this back from a real world perspective and saying okay maybe we need to work back the other way from complex multi-sensory experiences towards active inference as well as using active inference to do modeling of maybe kind of very clean sort of low dimensional",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 499.52,
    "end": 522.866,
    "text": " approaches it's like should we also be coming back from the other direction um and i did i've got an activity called conceptual action sociometry an activity where people move around space to understand how their orientation is during a moment of encounter and i've asked people to unpack it in terms of the three different areas of active influence the sort of pragmatic gain",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 523.786,
    "end": 528.833,
    "text": " epistemic foraging or risk mitigation or salience.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 529.514,
    "end": 535.022,
    "text": "And it's actually really interesting when they break it down that way and start to think about what was I trying to do?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 535.062,
    "end": 538.767,
    "text": "But from that kind of high dimensional kind of",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 539.674,
    "end": 554.592,
    "text": " synthesis of being in a really immersive moment and they actually people seem to who don't know anything about active influence actually were quite intuitively seeing quite a lot of shifts in those three different areas of focus depending on the type of um",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 555.372,
    "end": 572.504,
    "text": " event or moment that they were reflecting on because they were going back to an event and thinking about the focus of their attention at that particular moment in time and how much were they sort of bringing these three areas in and then i suppose between that you could infer something about the exploration",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 574.281,
    "end": 579.388,
    "text": " sort of desire to learn or the exploitation gain.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 579.849,
    "end": 580.59,
    "text": "But I agree.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 580.65,
    "end": 583.454,
    "text": "I think it does depend on the moment that's in question.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 583.694,
    "end": 589.182,
    "text": "So there's a few things that kind of this is sort of tapping into, which I'm really interested in.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 590.162,
    "end": 590.442,
    "text": " Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 590.763,
    "end": 597.629,
    "text": "And to tie that just together with a recast idea from Alec, it's kind of like nature, nurture, explore, exploit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 598.149,
    "end": 599.69,
    "text": "People don't know about these binaries.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 599.71,
    "end": 601.552,
    "text": "They're not natural kinds until they're taught them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 602.032,
    "end": 610.18,
    "text": "And then the resolution to these kinds of dichotomies or binaries, false or not, is not some parameter that varies from zero to one that trades off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 610.64,
    "end": 616.008,
    "text": " For example, when Evelyn Fox Keller was moving beyond nature and nurture, it's the mirage of a space between.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 616.108,
    "end": 621.555,
    "text": "It's about reconsidering the situation so that there doesn't have to be something like a bipartition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 621.916,
    "end": 625.901,
    "text": "So that's definitely the direction that we will try to take it because it's a really cool idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 626.502,
    "end": 636.493,
    "text": " The second or I guess the last warm up question is and on a related note, what would be an interesting control system or any system to model with active inference?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 636.773,
    "end": 642.839,
    "text": "So we're going to go into like a trolley car and a hopper, I guess, and a pendulum.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 643.3,
    "end": 644.962,
    "text": "And I think it will be fun to hear about those.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 645.062,
    "end": 648.766,
    "text": "But we heard about the social and the spatial.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 649.366,
    "end": 657.432,
    "text": " There's questions that we've raised about biology, communication, anything else that somebody wants to bring up or whether it's, yeah, Mel, please.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 658.232,
    "end": 658.833,
    "text": "And then Sasha.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 660.194,
    "end": 660.994,
    "text": "Yeah.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 661.855,
    "end": 668.1,
    "text": "So what I'm excited about, I'm really excited about this kind of work.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 670.762,
    "end": 677.927,
    "text": "I'm a philosopher, but with this sort of stuff, the active inference, FEP stuff, I'm like, please stop theorizing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 677.967,
    "end": 678.307,
    "text": "Just like,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 679.712,
    "end": 680.413,
    "text": " Let's play with this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 680.633,
    "end": 683.855,
    "text": "So I'm really excited about this kind of work.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 684.776,
    "end": 693.463,
    "text": "And the question that's motivating me is really sort of figuring out what the boundaries of these various models are.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 693.563,
    "end": 705.391,
    "text": "So what boundaries of active inference are relative to the free energy principle and then relative to EG predictive coding models and that sort of thing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 707.073,
    "end": 708.634,
    "text": "Someone brought up recently",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 709.659,
    "end": 712.52,
    "text": " on Twitter, I guess.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 712.64,
    "end": 731.705,
    "text": "There's a biologist at my university who's doing really kind of groundbreaking work in like perception cognition, perceptual cognition and stuff in, I guess, butterflies, but mostly jumping spiders, saltiste.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 733.766,
    "end": 737.347,
    "text": "The Morehouse lab, Nate Morehouse is doing, for example,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 738.223,
    "end": 760.749,
    "text": " Saltice Day has like very elaborate like mating dances and things and they're signaling to each other and they're doing stuff like like the evolution of color perception in in these jumping spiders it's really fascinating work and they mentioned recently or Nate mentioned recently that um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 761.974,
    "end": 772.256,
    "text": " the lab is doing some like simulation work on how we evolve additional color per sets.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 772.597,
    "end": 791.601,
    "text": "So you can, right, you can, if you have certain like genes and proteins for pursuing certain colors, you can kind of like shift the probability distribution, the range of color that those pick up, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 792.511,
    "end": 796.655,
    "text": " That's something that evolution can kind of modulate.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 796.795,
    "end": 805.982,
    "text": "But the evolution of an entirely different, an entirely new color percept is something different.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 806.662,
    "end": 812.006,
    "text": "And so they're doing some like in silico work on that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 812.247,
    "end": 816.97,
    "text": "And I thought, yeah, that's what active inference, that's exactly what active inference should be doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 816.99,
    "end": 818.291,
    "text": "That's what we should be doing, great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 820.033,
    "end": 821.614,
    "text": "So I think that's what we should be striving for.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 822.455,
    "end": 826.458,
    "text": " Thanks for that very interesting summary, Sasha, and then anyone else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 828.099,
    "end": 830.742,
    "text": "Thank you, Mel, for bringing up the jumping spiders.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 831.462,
    "end": 832.743,
    "text": "They have incredible eyes.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 833.003,
    "end": 833.784,
    "text": "You should all look it up.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 834.805,
    "end": 851.398,
    "text": "But I'm interested in this from a developmental perspective and what active inference can contribute to how systems develop and communicate and specifically humans.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 852.488,
    "end": 874.918,
    "text": " how our in utero environment and the activity of that process shapes the development of the human and kind of on a more broad scale how this can play into teaching and how active inference can be applied in the classroom at different educational levels but in a way that",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 876.314,
    "end": 886.319,
    "text": " sets up the environment such that agents are encouraged to explore instead of being dragged through the material as students often are.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 887.82,
    "end": 889.661,
    "text": "So that's my curiosity in this.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 890.822,
    "end": 891.562,
    "text": "And Vygotsky.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 892.963,
    "end": 893.283,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 893.443,
    "end": 900.847,
    "text": "Yes, a lot of Vygotsky fans slash colleagues through time on this discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 901.307,
    "end": 902.308,
    "text": "Any other comments?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 903.612,
    "end": 907.315,
    "text": " but really great points there about like the evolution and the development of perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 908.055,
    "end": 914.12,
    "text": "And then at the level that we can at least experience or enact agency, can we choose to see a color as different?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 914.16,
    "end": 917.983,
    "text": "Can you look at that ambiguous pattern and choose to see it one way or another?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 918.703,
    "end": 928.427,
    "text": " Can we understand many of these perceptive questions as being related to maybe not simply explore, exploit, maybe having an element of multi-scale agency?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 928.447,
    "end": 936.31,
    "text": "I think there's a lot of ways that that could play out in people's life while also recognizing that we're not talking about just metaphors here or narratives.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 936.61,
    "end": 941.052,
    "text": "We're also talking about this specific framework with the paper that we're going to discuss now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 941.292,
    "end": 944.594,
    "text": "So let's talk about those details, but then maybe we can dip back",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 945.154,
    "end": 955.758,
    "text": " to the other systems, but let's think about how this paper and also our special time with Alec will help us understand these broader questions, at least where we want to go after this short discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 956.419,
    "end": 960.24,
    "text": "So the paper, Scaling Active Inference, we read the goal last time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 961.241,
    "end": 969.704,
    "text": "Alec, maybe if you could just however you want to or however long, just what were you setting out to do or how did the collaboration come to be?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 969.924,
    "end": 971.245,
    "text": "What were you asking before?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 971.625,
    "end": 973.486,
    "text": "How did you get to this paper and this goal?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 975.914,
    "end": 988.182,
    "text": " Um, so I guess historically the sort of historic, uh, lead up to the paper was just, uh, very interested in reinforcement learning, machine learning, and also very interested in active inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 988.502,
    "end": 1000.19,
    "text": "So it was a, those sort of, uh, natural overlap, but in terms of the motivation, I think the two communities don't really speak to each other.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1001.095,
    "end": 1002.536,
    "text": " as much as they should.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1002.916,
    "end": 1015.381,
    "text": "And so much of the work that's happening in reinforcement learning, especially model-based reinforcement learning, has direct analogies to the work that's being developed in the active inference community.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1017.142,
    "end": 1026.806,
    "text": " So stuff like the use of variational methods, variational autoencoders, the use of dynamics models, the use of trajectory-based planning, or just planning in general.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1026.826,
    "end": 1030.308,
    "text": "And these algorithms have a lot of similarities between the two.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1030.828,
    "end": 1035.91,
    "text": "The use of intrinsic objectives, these information gain terms is widespread in both.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1036.07,
    "end": 1037.371,
    "text": "The use of belief-based planning.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1037.991,
    "end": 1040.052,
    "text": "So all the things that we sort of touched on in the paper.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1041.129,
    "end": 1052.395,
    "text": " In my mind when I started out, the aim was really just to construct an agent using the machinery that would be common and relatable to the machine learning crowd.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1052.956,
    "end": 1055.117,
    "text": "But that was consistent with the active influence framework.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1056.278,
    "end": 1067.824,
    "text": "And I guess a secondary goal was also to put together an initial attempt to look at whether what needs to be put in to get some of these ideas to scale.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1069.823,
    "end": 1088.201,
    "text": " With a particular focus on, you know, this wasn't a biologically plausible suggestion, but looking at some of the objective functions, the expected free energy at scale, I think is of interest rather than, you know, where it's commonly looked at in these small discrete teammates.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1091.561,
    "end": 1101.748,
    "text": " Cool, really helpful and a lot of terms that we've heard, but to link together the ideas of the variational inference with the belief and the trajectory based planning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1102.108,
    "end": 1103.829,
    "text": "Those are all really cool things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1104.29,
    "end": 1106.431,
    "text": "I just have a question about the at scale part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1106.831,
    "end": 1112.295,
    "text": "So one aspect was that you introduced the continuous state space as opposed to the discrete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1112.675,
    "end": 1118.039,
    "text": "But is there anything else that's meant by at scale or what does it mean to scale in this context?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1120.168,
    "end": 1141.305,
    "text": " Well, I mean, so I guess the scale just means it's almost defined in relation to what is currently common in the active inference literature, which is where your entire world is often designed by four or five states in those transitions.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1141.545,
    "end": 1148.25,
    "text": "So this paper presents a framework that can be applied to observations of",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1150.929,
    "end": 1151.409,
    "text": " you know, 10,000, 100,000.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1151.529,
    "end": 1155.971,
    "text": "And so your state space or your observation space is much larger.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1158.312,
    "end": 1166.555,
    "text": "So that's what I meant by scale is putting forward a framework that could be applied to high dimensional tasks than the ones that we currently consider.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1166.575,
    "end": 1172.237,
    "text": "And there's also a second thing which is not really captured in the word scale, but also the complexity of the dynamics.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1174.042,
    "end": 1186.115,
    "text": " So some of these tasks actually have quite a small state space, you know, four or five, but the complexity of their dynamics requires large models of a number of parameters to learn.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1187.356,
    "end": 1193.403,
    "text": "So there's kind of a scaling aspect and a scaling in the space of complexity of the dynamics.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1195.043,
    "end": 1197.624,
    "text": " Cool, very interesting stuff.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1197.784,
    "end": 1203.405,
    "text": "And at any point people can just raise their hand, but I think we can keep on walking through these areas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1204.185,
    "end": 1223.829,
    "text": "And last time we did walk through the different areas in the roadmap, just how we introduce active inference and the current state, and then walk through the specifics of the model, show some proof of concept experiments related to previous work, and then discuss and conclude with some of these sentiments related to what Alec",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1224.269,
    "end": 1224.669,
    "text": " is saying.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1225.21,
    "end": 1226.531,
    "text": "And I also found it interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1226.571,
    "end": 1228.152,
    "text": "There's the scaling dimensionality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1228.792,
    "end": 1232.034,
    "text": "Then there's the introduction of continuous state spaces, not just discrete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1232.394,
    "end": 1239.919,
    "text": "And then often as machine learning, people might use the word scaling to mean it's about the number of observations, the size of the data set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1241.06,
    "end": 1244.002,
    "text": "And that relates to the compute scaling relationship.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1244.122,
    "end": 1248.605,
    "text": "As you add, when you double the data set input, is it the same amount of training time?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1248.705,
    "end": 1249.606,
    "text": "Is it twice as long?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1249.646,
    "end": 1250.747,
    "text": "Is it four times as long?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1250.807,
    "end": 1251.447,
    "text": "Is it 4,000 times as long?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1252.888,
    "end": 1258.912,
    "text": " And so really interesting because there's also the scaling in terms of the understanding in the world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1259.072,
    "end": 1263.395,
    "text": "And people talk about scaling and innovation and entrepreneurship and how do you scale a solution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1263.415,
    "end": 1271.881,
    "text": "So there's a lot of parallel meanings and a fun and concise title because you know that the finding isn't going to be in the title.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1272.201,
    "end": 1276.064,
    "text": "There isn't just a simple way that says, oh, this protein does this in this species.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1276.344,
    "end": 1282.368,
    "text": "We're talking about developments in a modeling framework, but we also want to be specific about what those advances are",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1282.828,
    "end": 1284.368,
    "text": " and how they relate to systems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1284.669,
    "end": 1291.651,
    "text": "So maybe just on the roadmap, Alec or anyone else, just anything to say about it or why was it arranged this way?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1292.191,
    "end": 1295.292,
    "text": "Personally, I was just kind of wondering why the previous work is at the end.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1295.472,
    "end": 1300.913,
    "text": "Is that a difference in our academic conventions or is that just because it was more results oriented?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1302.033,
    "end": 1306.975,
    "text": "Yeah, I mean, you don't get a lot of leeway in machine learning style papers.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1307.095,
    "end": 1311.316,
    "text": "I mean, previous work might go after the introduction or at the end.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1312.211,
    "end": 1323.337,
    "text": " But it's always, you know, introduction, some methods of presenting a model, normally quite short in tests, and the experiments in the previous work section in a very short discussion.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1323.357,
    "end": 1326.759,
    "text": "That's kind of standard across the whole field.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1326.819,
    "end": 1338.445,
    "text": "But the reason I think I chose previous work at the end was just because I discussed active inference, which naturally leads in from the introduction, and that introduces a little bit of maths, which then naturally flows into the model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1338.505,
    "end": 1340.946,
    "text": "I feel like a previous work might have broken up that flow.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1342.54,
    "end": 1344.681,
    "text": " but it was more or less obvious.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1345.401,
    "end": 1345.721,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1346.741,
    "end": 1347.402,
    "text": "Makes sense.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1348.122,
    "end": 1351.663,
    "text": "I think we can walk through the next sections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1352.443,
    "end": 1356.525,
    "text": "So right now we're in the just open pause for questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1357.847,
    "end": 1378.204,
    "text": " I have a few written down just as far as just big questions because we're trying to be a bridge in this discussion and through our conversation between those who might be hearing about active inference for the first time, but they're coming from machine learning, thinking about how cool methods, perhaps with philosophical implications, might be utilized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1378.705,
    "end": 1384.35,
    "text": "And there's also a lot of people who are in the active inference side of things or the inactivism side or the philosophy side",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1384.73,
    "end": 1388.354,
    "text": " where the details of the machine learning might be their first time hearing about it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1388.434,
    "end": 1396.623,
    "text": "So how will we just come from where we are and think about what questions and what kinds of fun things to talk about will make that bridge?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1396.963,
    "end": 1398.805,
    "text": "Stephen first, and then anyone who wants to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1400.783,
    "end": 1410.774,
    "text": " Yeah, I'm just curious because I know that the scaling question has been maybe looked at more from a philosophical perspective in the kind of active inference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1410.814,
    "end": 1417.781,
    "text": "So what this paper really helped me do or helped me was it like broke that fear of questioning",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1418.582,
    "end": 1444.429,
    "text": " the kind of these models because the like people like yourself and Anil and that are saying look there's a challenge here in the scaling like this stuff is being kind of used at these low dimensional spaces and basically it showed like a gap in the literature in terms of how to action that which kind of helped me because it sort of gave me a bit of permission to say okay it's okay there's a gap there",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1445.069,
    "end": 1447.271,
    "text": " And it's not that I just don't understand everything.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1447.391,
    "end": 1449.052,
    "text": "It's just not easy to scale.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1449.173,
    "end": 1459.061,
    "text": "And I think this paper was the first one to explicitly say that because all the other papers tend to talk about, yes, we know how to scale, but in a philosophical way.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1459.521,
    "end": 1463.645,
    "text": "But this is like, OK, well, this is the problem from a practice perspective.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 1465.674,
    "end": 1467.895,
    "text": " Cool, just one note on that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1467.975,
    "end": 1474.957,
    "text": "If anyone else wants to raise their hand, I really love this idea where there's the gap in the literature and there's a billion gaps on literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1475.578,
    "end": 1478.639,
    "text": "It's about which ones are salient and fundable and relevant.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1478.899,
    "end": 1480.259,
    "text": "So there's many gaps in literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1480.399,
    "end": 1484.281,
    "text": "And then when we find that gap, which is often very niche, like in a PhD,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1484.881,
    "end": 1508.714,
    "text": " and i thought wow this data set has been collected from this ant species but not this other ant species there's a gap in literature we don't have this data set on this ant species and then seeing that as a opportunity and something that's okay even if it's not your traditional field of study it's okay because you're at the edge like at the gap with us talking to the author and talking through the equations but this is what it looks like to look",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1509.534,
    "end": 1529.177,
    "text": " across the gap and try to connect because somebody else has identified it as a research question or maybe you have alec yeah i just wanted to comment on that as well and uh entirely agree with what you're saying um to come to maybe the defense of uh the existing literature on to influence i think it's",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1530.512,
    "end": 1549.124,
    "text": " viewed a little bit or this maybe it's just me interpreting what they're saying but um that maybe that the problems that they're um testing their agents in uh for instance something like the teammates are actually viewed uh as quite complex in a different dimension so a lot of these",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1550.071,
    "end": 1574.931,
    "text": " reinforcement learning tasks stuff like atari uh most of atari most of you know stuff where you're really pushing to get state-of-the-art results with huge amount of compute and deep mind of computing into there they're often you can often get by with just this model free reinforcement learning that doesn't require any notion of beliefs or any notion of proper epistemics you can get away with under its expression so what i think uh",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1575.916,
    "end": 1590.729,
    "text": " the active influence community has tried to do is look at simple tasks such as the teammates that actually, well, you can solve them without beliefs, but are far, far more amenable to being solved with a belief-based scheme and with this directed exploration and uncertainty reduction.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1592.53,
    "end": 1596.812,
    "text": " to show the kind of complexity that the framework can come up with.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1597.232,
    "end": 1616.48,
    "text": "But then you could also respond by saying that this is also thought about a lot in reinforcement learning and there is the question obviously of whether the kind of process theories they put forward will apply when you've really got the scale that reinforcement learning tends to be.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1617.884,
    "end": 1620.887,
    "text": " Yeah, let me add on to that because it's a really great point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1621.047,
    "end": 1636.721,
    "text": "So a lot of times what's perceived as cutting edge or modern or advanced research in machine learning, I mean, go check the lay media and it's going to be about this many graphical processors or this size of data set or this accuracy or this level of skill using massive data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1637.421,
    "end": 1646.792,
    "text": " And it's very much on the performance end and on the eking out a little bit more performance from increasingly large data sets with increasingly large types of models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1648.093,
    "end": 1651.437,
    "text": "But it's actually in some sense of local exploration.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1651.957,
    "end": 1657.664,
    "text": "It's locally exploring certain frameworks and ways of doing machine learning, which is just computer statistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1658.304,
    "end": 1662.288,
    "text": " And so what this is like with this paper is a return to simplicity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1662.708,
    "end": 1667.512,
    "text": "And it's a return to a slightly different way of conceptualizing some of the parameters and how they're related.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1668.272,
    "end": 1673.136,
    "text": "And the big data, just train it bigger, train it better is kind of like thinking that we can do",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1674.017,
    "end": 1675.098,
    "text": " X without a belief.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1675.218,
    "end": 1679.741,
    "text": "We can train on Go just by watching Go, and we can train the laws of physics just by watching physics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1679.781,
    "end": 1682.843,
    "text": "We can learn language by just watching regularities in human language.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1683.383,
    "end": 1685.725,
    "text": "These approaches, there's merit to them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1685.885,
    "end": 1688.346,
    "text": "This isn't just about one way being better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1688.806,
    "end": 1690.788,
    "text": "It's just that what is being done in this paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1691.448,
    "end": 1713.84,
    "text": " is to take the free energy principle and active inference which previously had made these kinds of philosophically at the very least tantalizing claims like the relevance of a belief guided trajectory based optimization and search taking these very fascinating ideas a little bit out of the sandbox into the next level of the playground where now we can actually start to compete",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1714.46,
    "end": 1720.385,
    "text": " or at least compare and contrast directly with the kinds of benchmarking algorithms that are used.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1720.825,
    "end": 1741.361,
    "text": "So maybe one day, you know, Active Inference Go or Active Inference Chess, but today we have the simple control theory parameters, which is one level closer to these kinds of use cases that are happening today, one level closer than the T-Maze or the three-state decision, is it a mouse or a hawk or a cat, that type of stuff.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1743.097,
    "end": 1754.86,
    "text": " Cool, just the questions that I put up, just thinking about how can we continue to deepen our understanding, which we'll move on from, but just like, what's something you wondered about, just always stick with that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1754.98,
    "end": 1756.1,
    "text": "And anyone can raise your hand as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1756.48,
    "end": 1766.202,
    "text": "What's something you learned about while studying the paper, whether it was something that they wrote specifically, or whether it was something that you kind of went down a rabbit hole and started studying about, definitely that happened for me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1766.582,
    "end": 1770.843,
    "text": "And then lastly, like what's something you're motivated to do more or learn about now",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1771.403,
    "end": 1773.424,
    "text": " that you've read this paper and had this discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1773.444,
    "end": 1774.485,
    "text": "Yeah, Mel, thanks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1774.565,
    "end": 1775.465,
    "text": "And then anyone else?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1778.187,
    "end": 1793.355,
    "text": "Yeah, I guess I just wanted to jump on that and say that I'm used to seeing people sort of try to compare reinforcement learning with accident inference or related approaches.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1793.716,
    "end": 1797.778,
    "text": "And it's nice to see something more that's more of a synthesis there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1800.489,
    "end": 1800.769,
    "text": " Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1801.289,
    "end": 1801.549,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1802.13,
    "end": 1823.359,
    "text": "So I think that if there's any other thoughts, feel free, but we'll look again over the experiments and then we'll sort of work kind of in and out about broader topics, but let's try to understand what was really done because this also is the point of contact with people who might be very familiar with machine learning and optimization control theory, but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1824.119,
    "end": 1843.091,
    "text": " for them it might be the first time hearing active inference so where's the common train stop where we're on board with the machine learning community they're interested in these kinds of tasks they frame it as explore exploit and as we've been talking about maybe there's like a bit of a re-imagination or a re-conceptualization of this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1844.062,
    "end": 1846.203,
    "text": " relationship, explore and exploit, or what are these variables?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1846.363,
    "end": 1848.323,
    "text": "That's what we want to get to at the end.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1848.383,
    "end": 1849.724,
    "text": "So let's for sure remember that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1850.304,
    "end": 1857.426,
    "text": "But for now, let's think about being in common with benchmarking different data sets and different machine learning algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1857.946,
    "end": 1869.51,
    "text": "So maybe we could talk about explore and exploit, how these communities think about these, and maybe your observations about how they're different, Alec, in terms of first, the example that you chose to highlight as explore.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1869.87,
    "end": 1871.452,
    "text": " So maybe tell us about the mountain car.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1871.492,
    "end": 1873.716,
    "text": "And then when you tell me, I'll go to the figure one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1874.136,
    "end": 1875.158,
    "text": "But like, what is this?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1875.358,
    "end": 1876.64,
    "text": "What does it have to do with exploring?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1877.08,
    "end": 1879.063,
    "text": "What does the machine learning community think about it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1879.264,
    "end": 1880.545,
    "text": "How does active inference apply?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1880.626,
    "end": 1881.507,
    "text": "Like what is happening here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1883.516,
    "end": 1885.337,
    "text": " Yeah, so mountain car is quite an interesting one.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1886.377,
    "end": 1887.878,
    "text": "It seems so simple.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1888.118,
    "end": 1892.12,
    "text": "When you do it in a fully observed environment, there's two states, one action.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1892.2,
    "end": 1894.481,
    "text": "This is the continuous version, so it's continuous actions.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1894.981,
    "end": 1900.284,
    "text": "So, you know, it's like the simplest task that machine learning people present in papers.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1901.004,
    "end": 1902.064,
    "text": "But it's actually one of the hardest.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1902.084,
    "end": 1907.767,
    "text": "So, you know, DQN, vanilla DQN doesn't stand a chance of solving this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1908.027,
    "end": 1908.727,
    "text": "What is DQN?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1909.448,
    "end": 1909.628,
    "text": "Sorry.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1909.648,
    "end": 1909.708,
    "text": "DQN.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1912.779,
    "end": 1914.56,
    "text": " Q-networks, I have to remember that.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1914.58,
    "end": 1922.923,
    "text": "So that's the taking like the old idea of Q-learning, which is kind of was the biggest idea of reinforcement learning for this whole deep phase.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1923.524,
    "end": 1925.445,
    "text": "I'm just mending the deep neural networks.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1925.525,
    "end": 1930.127,
    "text": "It's the one that the Atari,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1931.418,
    "end": 1931.839,
    "text": " Uh, yeah.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1932.219,
    "end": 1938.186,
    "text": "The first like deep mind nature, David, I can't remember the details, but it was, it was kind of the birth of the deep reinforcement learning.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1938.446,
    "end": 1938.687,
    "text": "Great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1938.787,
    "end": 1941.77,
    "text": "So that one has challenges on here and then just continue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1941.91,
    "end": 1948.958,
    "text": "I mean, basically the reason is because you only get a reward when you reach that, um, flag up at the top.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1949.158,
    "end": 1949.839,
    "text": "And before that,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1952.201,
    "end": 1961.57,
    "text": " that takes about 180 steps or 180 actions that you have to string together before you get any notion of rewards.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1961.67,
    "end": 1965.353,
    "text": "So just pure reward-based schemes really struggle with this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1965.393,
    "end": 1971.619,
    "text": "They essentially have to do more or less random actions until you get out of the hill.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1972.9,
    "end": 1974.461,
    "text": "So yeah, it's a struggle.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1974.601,
    "end": 1977.324,
    "text": "And that's why it's kind of exemplified exploration",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1978.342,
    "end": 1989.148,
    "text": " You need to care about learning about the dynamics or some other intrinsic quantity if you want a chance at finding out how to solve this path.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 1991.549,
    "end": 1991.909,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1992.25,
    "end": 1993.811,
    "text": "Very interesting, Blue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1993.891,
    "end": 1997.853,
    "text": "This is just really reminding me of a lot of what we've talked about with Explore.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1997.973,
    "end": 2005.937,
    "text": "Explore a lot of the perspectives you've brought about how exploring how and when and where does it matter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2005.957,
    "end": 2005.997,
    "text": "So...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2007.478,
    "end": 2010.121,
    "text": " Pretty interesting to hear, Alec, just how you phrased that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2010.541,
    "end": 2014.545,
    "text": "And also just like to hear that it's 180 sequences of actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2015.206,
    "end": 2017.909,
    "text": "So this is not like control theory, like Connect Four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2018.009,
    "end": 2025.316,
    "text": "This is like walking, you know, who knows how many degrees of freedom there are, but this is many tasks that have to get strung together in long sequences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2025.697,
    "end": 2029.381,
    "text": "And then you said something about how it has to learn something intrinsic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2029.801,
    "end": 2041.848,
    "text": " for example, the relationship between its velocity, position and policy, a little bit more nuanced nexus of action rather than just learning like simply whether, you know, go means fast and stop means slow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2042.268,
    "end": 2043.569,
    "text": "Stephen, and then anyone else?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2044.75,
    "end": 2049.853,
    "text": "Yeah, can I just ask the question about that when you say stringing together actions?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2049.873,
    "end": 2053.195,
    "text": "Because I kind of had this feeling that it was kind of a case of like,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2053.991,
    "end": 2078.841,
    "text": " you keep it by there being a desire to explore in itself through free energy that the car realized that it could go up the other side of the hill further because it could just have a reason for going there because of trying to minimize the desire to free energy around exploration and in doing so it gets that extra kind of momentum to get up the other side of the hill",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2079.341,
    "end": 2103.84,
    "text": " but i haven't really got into a detail so just wondering how this kind of sequence of steps and maybe just finding yourself far enough far enough up the other hill to get enough potential energy just to go down and up and make it to the flag i wonder how those two sort of relate great question alec maybe if you have a thought on that yeah yeah so i think your intuition's mostly correct so",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2104.892,
    "end": 2110.215,
    "text": " how it plays out is that this agent is evaluating different sequences of actions.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2110.435,
    "end": 2114.177,
    "text": "Some of those actions are gonna keep it where it's already been, which is at the bottom of this hill.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2115.377,
    "end": 2120.74,
    "text": "And then some of the actions are gonna take it into a place where it's uncertain about the outcome.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2122.961,
    "end": 2127.763,
    "text": "And that uncertainty is essentially valued by the agent that wants to resolve it.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2128.323,
    "end": 2133.206,
    "text": "So it's gonna say, okay, what happens if I do a little, cause it has to go left up the hill",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2134.386,
    "end": 2135.187,
    "text": " I should have mentioned that before.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2135.207,
    "end": 2136.448,
    "text": "It has to go left up the hill.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2136.568,
    "end": 2140.511,
    "text": "Actually, it has to go away from the flag to gain momentum to go around.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2140.591,
    "end": 2143.293,
    "text": "So it's never gone left up the hill and then accelerated.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2143.313,
    "end": 2146.036,
    "text": "It doesn't know what's going to happen when it does that.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2146.056,
    "end": 2148.378,
    "text": "That's valuable to it in terms of epistemics.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2148.638,
    "end": 2151.68,
    "text": "And then it realizes, oh, wow, up the hill.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2153.174,
    "end": 2157.917,
    "text": " Yeah, let me link that to the question, the sequence of actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2158.257,
    "end": 2163.399,
    "text": "So imagine if you were gonna do a control theory optimization on shooting a bow and arrow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2163.619,
    "end": 2168.221,
    "text": "So shooting a bow is a complex motor movement, uses probably many joints.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2168.261,
    "end": 2170.622,
    "text": "There's a lot of, you could do different speeds, different ratios.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2170.663,
    "end": 2174.464,
    "text": "It's gonna be dependent on the bow, just like this is gonna be dependent on the slope and the car.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2174.784,
    "end": 2179.687,
    "text": "So it's an enacted affordance that you're trying to develop an action sequence for.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2180.387,
    "end": 2183.889,
    "text": " And your action sequence in a game is often just a one stepper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2184.13,
    "end": 2185.991,
    "text": "Drop the Connect Four token here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2186.491,
    "end": 2188.492,
    "text": "I'll reevaluate after I see what they do.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2188.993,
    "end": 2195.257,
    "text": "But if you're going to be evaluating prospectively an action sequence in the world, it often has depth.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2195.737,
    "end": 2198.64,
    "text": " So it's like getting out of my chair, shooting the bow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2199.001,
    "end": 2202.945,
    "text": "There's state spaces are very, there's multi-joint.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2203.245,
    "end": 2205.688,
    "text": "So it's multi-dimensional and it's continuous.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2206.028,
    "end": 2208.531,
    "text": "So that's why the multi-dimensional and the continuous are so important.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2209.031,
    "end": 2217.741,
    "text": "And then the depth through time is so important because you can't just be doing a get out of my chair or a shoot the bow and arrow short-term one-step optimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2218.161,
    "end": 2240.803,
    "text": " there's no opponent for you to then take a look at their move like in a chess checkers go paradigm or even a video game paradigm to some extent this is something about planning in an enormous state space and thinking really constructively about these intrinsic relationships so in the bow example you might be attuning to the tension between the proprioception in your shoulder and how tense the bow is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2241.383,
    "end": 2258.987,
    "text": " to know if you're at the end of your range of movement, or the bow is, and if you trained on one that was half size, and then you go to a larger one, or if your arm is hurting that day, or all these differences, differences in our abilities, these things all become enacted in the relationships that we're learning about the intrinsic variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2259.447,
    "end": 2269.67,
    "text": "So in the depth through time and intrinsic variables, if you learn that, hey, if I actually reverse and start accelerating downhill, a hundred time steps later, it's just better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2270.29,
    "end": 2273.673,
    "text": " Like if I go to sleep at this time, a hundred times steps later, it's just better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2274.433,
    "end": 2290.386,
    "text": "And so these allow for very non-linear policies to be selected because there can be temporal depth that's learnt as a function of the actual physics, basically, of the setting, not just like counterfactual, you know, if 13 moves down the row, this person does this with my rook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2290.867,
    "end": 2293.549,
    "text": "That still is in a very if-then context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2293.949,
    "end": 2299.714,
    "text": "And this is taking it into a totally different domain with a proactive action selection policies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2302.048,
    "end": 2302.228,
    "text": " Yep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2302.629,
    "end": 2305.631,
    "text": "So that's why this is such an interesting paper and approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2305.671,
    "end": 2319.083,
    "text": "And like, again, a branching off point, that's why it's foundational in the machine learning, because just like Alec described, like if deep Q learning cannot accomplish this task, then yeah, it's all great to be a human at chess or go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2319.643,
    "end": 2324.968,
    "text": "But if that algorithm or even that architecture can't defeat this challenge, then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2325.208,
    "end": 2328.45,
    "text": " then we're developing something that's incredibly specific, which is great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2328.87,
    "end": 2331.632,
    "text": "We should have good map routing algorithms and things like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2332.152,
    "end": 2337.796,
    "text": "But we're definitely going hyperlocal down a rabbit hole if it can't solve this, but it can solve Go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2337.936,
    "end": 2339.197,
    "text": "I don't know if it is that case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2340.037,
    "end": 2341.738,
    "text": "A machine learning person would be really welcome to help",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2342.719,
    "end": 2349.101,
    "text": " fill us in on some of these details, but that's the kind of stuff that is interesting and broached in this topic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2349.522,
    "end": 2363.286,
    "text": "So just to see what that looks like empirically, like the first hundred epochs are plotted in terms of the state space coverage, which here is just the position on the X axis and then the velocity on the Y and the flag is at 0.5, right Alec?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2365.137,
    "end": 2367.639,
    "text": " like the flag represents being at position of 0.5.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2367.679,
    "end": 2368.74,
    "text": "But yeah, continue from here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2368.8,
    "end": 2371.823,
    "text": "Just anything you want to add on figure one or what does this mean?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2372.484,
    "end": 2373.124,
    "text": "What is happening here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2373.905,
    "end": 2375.487,
    "text": "I guess now your description is great.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2375.507,
    "end": 2376.247,
    "text": "The flag's not 0.5.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 2377.168,
    "end": 2377.629,
    "text": "The flag's...",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2380.1,
    "end": 2401.776,
    "text": " is it oh yeah 0.5 yeah uh but i just kind of want to um self-deprecate myself these results kind of they're not i mean they're not bad but um we had a follow-up paper where we got this all working properly um called reinforcement learning for active inference um which is very similar but we use a slightly",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2403.524,
    "end": 2408.812,
    "text": " I can discuss the differences, I think they're interesting, if you want, but just that the results are so much more impressive.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2408.832,
    "end": 2411.656,
    "text": "We can still, it's kind of both,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2413.168,
    "end": 2418.673,
    "text": " Hitting on this and also trying to advertise that work.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2418.973,
    "end": 2421.776,
    "text": "So in that work, oh yeah, if you can actually just show the figure in that.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2422.136,
    "end": 2427.261,
    "text": "So there it can solve the mountain car in a single trial.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2428.042,
    "end": 2430.784,
    "text": "And I have also the sort of state space spots that look a bit better.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2430.805,
    "end": 2433.887,
    "text": "So here's the... So just that left one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2434.188,
    "end": 2434.348,
    "text": "Yeah.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2435.345,
    "end": 2438.386,
    "text": " So if you kind of go, yeah, we should have the video.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2438.546,
    "end": 2442.407,
    "text": "The first time it goes into the ring, it just solves it straight away, which is much nicer.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2442.827,
    "end": 2452.709,
    "text": "So the other one in the paper that we're looking at today, so here it's being solved every time that it does it, and it just constantly gets outwards.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2452.789,
    "end": 2455.79,
    "text": "And we tried some harder tasks in this, like the half-cheater and the unmade.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2456.27,
    "end": 2460.971,
    "text": "Ant maze coverage, gotta love it.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2461.551,
    "end": 2463.792,
    "text": "Yeah, I guess that was just a caveat that I wanted to highlight there.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2465.245,
    "end": 2466.346,
    "text": " it can do a lot better than this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2467.247,
    "end": 2471.31,
    "text": "But I'll be happy to discuss the differences because it's just one part of the architecture.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2471.99,
    "end": 2472.19,
    "text": "Sure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2472.21,
    "end": 2472.691,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2472.751,
    "end": 2478.355,
    "text": "What takes us from here to what we just sort of peeked into in this paper?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2478.375,
    "end": 2480.957,
    "text": "Like what was the one thing that you added or changed?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2481.832,
    "end": 2509.587,
    "text": " yeah so when i wrote the original one to um so to get this kind of what what carl calls parameter uh expiration or parameter information again it's essentially trying to reduce uncertainty not about what's out there in the world but about your own model yeah beliefs about um your model uh and this kind of in a sense is you know what you don't know and to get that you have to have a distribution over your model",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2510.547,
    "end": 2513.91,
    "text": " And in this case, our model is a neural network.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2514.431,
    "end": 2516.573,
    "text": "So you need distribution over your neural network.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2516.633,
    "end": 2518.815,
    "text": "There's two ways in the literature to do this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2519.336,
    "end": 2523.38,
    "text": "In the first paper, Scaling Active Inference, we tried Bayesian neural networks.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2524.561,
    "end": 2526.503,
    "text": "There may be more principles.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2526.583,
    "end": 2532.489,
    "text": "You know, you're actually using variation inference to estimate this distribution over each of your parameters.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2532.689,
    "end": 2534.571,
    "text": "But in practice,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2536.041,
    "end": 2543.004,
    "text": " they don't work nearly as well as what we tried in the next paper, which is called the ensemble approach.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2543.524,
    "end": 2551.207,
    "text": "And the idea there is you just take, in this case, 25 dynamics models and train them on different batches of the data.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2551.307,
    "end": 2559.17,
    "text": "And that's kind of like a proxy for a non-parametric Bayesian posterior over the dynamics model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2559.89,
    "end": 2560.771,
    "text": "You can do a little,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2561.874,
    "end": 2570.561,
    "text": " people are kind of working out how close it is to actually proper Bayesian beliefs, but you definitely get a notion of uncertainty there.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2570.661,
    "end": 2581.029,
    "text": "And there's lots of interesting reasons as to why, principle reasons as to why ensemble models would work better than Bayesian models.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2581.249,
    "end": 2587.033,
    "text": "It's also easier in practice to estimate stuff like information gain,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2589.457,
    "end": 2590.9,
    "text": " So that just turned out to be a hundred times better.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2590.96,
    "end": 2602.298,
    "text": "And in the machine learning community, people generally find more success with deep ensembles for both uncertainty calibration and directed exploration.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2603.919,
    "end": 2605.14,
    "text": " That's very cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2605.42,
    "end": 2607.102,
    "text": "Thanks for the great explanation there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2607.302,
    "end": 2620.454,
    "text": "And so just to sort of rehash that or say it a little differently, the approach that was taken in this scaling active inference paper to estimating some of these essential parameters was done with a Bayesian neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2620.754,
    "end": 2623.637,
    "text": " which Alec just described, it's like one way to do it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2623.717,
    "end": 2631.225,
    "text": "It might be a way that interfaces pretty cleanly with a lot of other software packages or approaches, but it's the skeleton of the model and that's the diagram.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2631.285,
    "end": 2632.807,
    "text": "And then we think about how can we improve it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2633.187,
    "end": 2637.712,
    "text": "And so One Direction is more analytical or more principled as you described it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2638.092,
    "end": 2642.953,
    "text": " And that's like the fully Bayesian sort of specifying every little hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2643.413,
    "end": 2659.177,
    "text": "And that approach can provide some interesting avenues at times, but an approach that is also relatively easy to implement from a programmatic interface level, and also makes a relative minimum of assumptions about the specific mechanics of the system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2659.757,
    "end": 2662.058,
    "text": " is this ensemble or deep ensemble approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2662.118,
    "end": 2670.061,
    "text": "So the deep adjective is just meaning you're going to have like a multi-scale or, I mean, a multi-level neural network with hidden layers, or it's just going to be deep and modern.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2670.401,
    "end": 2672.162,
    "text": "But it's kind of like just an adjective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2672.422,
    "end": 2679.805,
    "text": "The ensemble approach is describing having a bunch of different models that are going to be trained up on the same or different parts of the data set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2680.305,
    "end": 2686.548,
    "text": "And then you could look at the average of the ensemble, or you could look at some other weighted combination of the ensemble's performance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2687.188,
    "end": 2702.236,
    "text": " And so it's almost like bridging the gap from individual testing, that's like the single model testing, to now the ensemble approach is like, we're gonna have a classroom and then the best answer from our nine group, all working on it independently, the best one will push forward or will average.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2702.776,
    "end": 2715.663,
    "text": "And then even another level beyond that is like the truly emergence, which is actually what the ants are doing, where the ensemble works as an ensemble in a way that is itself shaped by development and learning and evolution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2716.323,
    "end": 2722.485,
    "text": " So right now the ensemble modeling is still like sort of, well, if you just split it up into many parts, you can cover more state space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2722.745,
    "end": 2724.646,
    "text": "You might be able to train the model in parallel.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2724.926,
    "end": 2727.567,
    "text": "You might make sure that no single model overgeneralizes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2727.807,
    "end": 2734.87,
    "text": "There's so many benefits that come simply from batching and ensemble modeling that go beyond just saying, well, what's the best single model?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2735.51,
    "end": 2746.856,
    "text": " or what's the best parameter range in this type of model, the ensemble can consist of a single type with a different parameters between the different ensemble mates, or they can be heterogeneous in some aspect.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2747.236,
    "end": 2748.317,
    "text": "So just really cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2748.817,
    "end": 2759.742,
    "text": "You went with what was implementable and a common point of departure with machine learning community and the Bayesian neural network kind of peaks in one direction towards a more analytical, more principled, fully Bayesian approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2760.142,
    "end": 2765.025,
    "text": "And then on the other side to modern approaches in machine learning, like deep ensemble learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2767.438,
    "end": 2767.678,
    "text": " Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2767.898,
    "end": 2771.32,
    "text": "Let's look at figure two and what is conveyed here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2771.4,
    "end": 2773.08,
    "text": "So this is the hopper task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2773.18,
    "end": 2774.361,
    "text": "Oh, we'll go blue first.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2774.881,
    "end": 2775.181,
    "text": "Go ahead.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2775.201,
    "end": 2775.802,
    "text": "Sorry, I didn't see that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2777.122,
    "end": 2778.443,
    "text": "Can you back up to the last figure?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2778.503,
    "end": 2780.023,
    "text": "Let's do blue and then mouth.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2780.283,
    "end": 2781.684,
    "text": "And then anyone else?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2782.765,
    "end": 2790.188,
    "text": "So I just wanted to ask, and this is kind of related to the next figure, but I just wanted to ask that...",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2791.445,
    "end": 2801.089,
    "text": " When you did the hopper task and the pendulum, so I know that that was only based on the extrinsic value part of the free energy equation.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2801.129,
    "end": 2807.231,
    "text": "So you left off the exploration because exploration probably had no value in that problem.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2807.311,
    "end": 2808.411,
    "text": "I mean, I'm assuming there.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2808.972,
    "end": 2812.593,
    "text": "But I was wondering here on the exploration problem.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2812.713,
    "end": 2814.074,
    "text": "Sorry, did I say exploration before?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2814.874,
    "end": 2817.035,
    "text": "Anyway, so here with the exploration problem,",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2818.797,
    "end": 2827.299,
    "text": " Did you ever think about leaving off the extrinsic part of the equation and just using the information gain as a reward?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2827.879,
    "end": 2828.879,
    "text": "Can you structure it that way?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2828.899,
    "end": 2829.799,
    "text": "Would it be different?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2830.159,
    "end": 2842.082,
    "text": "If the object of the agent was just to gain more information about the environment, do you think that it would have succeeded in climbing the hill because it would get to a new part of the environment?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2842.102,
    "end": 2843.182,
    "text": "Or how do you think that would work?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2845.074,
    "end": 2847.997,
    "text": " Yeah, not in this paper, but it does.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2848.398,
    "end": 2851.862,
    "text": "So that other paper I mentioned, the reinforcement learning through active inference,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2852.449,
    "end": 2854.03,
    "text": " because it solves it in the very first task.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2854.11,
    "end": 2856.651,
    "text": "It literally can't be because of reward.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2856.811,
    "end": 2859.533,
    "text": "We've tested it without reward as well to confirm this.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2860.073,
    "end": 2863.735,
    "text": "But it hasn't ever experienced a reward and it still solves the task.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2863.775,
    "end": 2865.776,
    "text": "So it's exactly what you're specifying.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2865.816,
    "end": 2871.259,
    "text": "It's just purely trying to reach all the parts of the state space.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2872.18,
    "end": 2879.824,
    "text": "If you just left it with explosion, what you tend to find is that it will solve it for, say, the first 10 trials while it's still the peaks of the...",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2881.264,
    "end": 2886.265,
    "text": " hill are the most kind of interesting because they have the most extreme dynamics with the most variants.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2887.126,
    "end": 2893.047,
    "text": "But then after a while, the top of the hill is no more interesting than the bottom of the hill because it's kind of stored up there.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2893.127,
    "end": 2902.05,
    "text": "Whereas if you've got the rewards and the exploration, it kind of slowly transfers from solving it because of exploration to solving it because it knows how to get rewards.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 2904.62,
    "end": 2905.121,
    "text": " Awesome, thank you.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2905.161,
    "end": 2908.683,
    "text": "I didn't see that follow-up paper, and I'm going to go read it right after this.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2909.044,
    "end": 2915.069,
    "text": "I know, we need to do an after-party, active after-party with the next paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2915.689,
    "end": 2917.451,
    "text": "But Mel, and then anyone else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2921.014,
    "end": 2932.363,
    "text": "Yeah, just what you were describing with stringing a bow, something like this, I think that was what really got me excited about",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2933.624,
    "end": 2944.248,
    "text": " active inference in the 300 principle in the first place is the idea that with something like a Towers of Hanoi style problem, are people familiar with that?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2944.528,
    "end": 2953.752,
    "text": "It's like you've got three sticks and you've got discs of various sizes on the sticks and you've got to get them in a ascending order, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2954.492,
    "end": 2962.815,
    "text": "And the idea is that the puzzle requires that you go backwards before you can go forwards.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2963.741,
    "end": 2983.244,
    "text": " And if you're just minimizing or maximizing some function on a single level, if you've got a 1D optimization, you're going to not be able to solve a problem that requires that you backtrack in order to make progress.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2984.885,
    "end": 2992.886,
    "text": "And something like the free energy principle where we've got temporal depth, we've got hierarchical depth of the model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2995.22,
    "end": 3002.545,
    "text": " is really equipped to solve these kinds of problems in a way that a lot of traditional problem solving approaches are not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3003.406,
    "end": 3004.126,
    "text": "I totally agree.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3004.326,
    "end": 3006.228,
    "text": "And I think there's a quantitative and a qualitative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3006.328,
    "end": 3014.573,
    "text": "So at the quantitative level, there's control policies that we can't have computers look beyond some locally non-favorable states to get around.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3014.994,
    "end": 3016.615,
    "text": "So we want to do these quantitative policies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3016.635,
    "end": 3017.616,
    "text": "That's what this paper is about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3017.956,
    "end": 3022.679,
    "text": "But at the qualitative and really the philosophical level, how do we come to grasp that?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3022.979,
    "end": 3033.126,
    "text": " with processes where, yeah, we're not always strictly walking a staircase directly to the top of the mountain, or it might not seem like we can do it at all initially if we just look.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3033.146,
    "end": 3037.489,
    "text": "And there's so much there with how we think about challenges and about exploring.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3037.949,
    "end": 3044.113,
    "text": "One actual link there, because Blue asked about what would happen if you just let it go wild on exploring.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3044.734,
    "end": 3052.379,
    "text": "And Alec, what you said is that the learning gets you to the top pretty quickly, because in that sense, it's similar to this model that also prioritizes reward.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3053.119,
    "end": 3061.783,
    "text": " But then after getting to the top, you spend a lot of your time learning on the most extremely variant areas of parameter space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3062.403,
    "end": 3078.251,
    "text": "So it's so much like curiosity-driven learning where a lot of times curiosity-driven learning with no reward or scaffolding, it ends up learning, learning, learning a ton, shocking amounts, but then also spending a lot of time in the most extreme ranges of space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3078.911,
    "end": 3098.622,
    "text": " and that doesn't always just play out like in the kind of online gutter but even in the literature we see a lot of the attention being spent on the extreme hyperboles and then the middle ground where it's like yeah it's kind of balanced and we can work on it together that is not as extreme of a viewpoint and so people spend less learning and attentional regimes on these kinds of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3099.082,
    "end": 3126.239,
    "text": " projects so there's a lot of like parallels that we can quantitatively model but also help us think about how we can say no you're just trying to make me explore the top of the hill i get it but the flag's on the other side so i've heard about the other side of the hill how are we going to enact a policy to get us to the flag together that's a little bit better than your hill is worse than my hill for example how could these kinds of things be ported onto like human decision making is a cool area",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3128.638,
    "end": 3129.699,
    "text": " Any other questions on one?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3129.739,
    "end": 3131.36,
    "text": "Otherwise, let's talk about two a little bit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3131.74,
    "end": 3148.794,
    "text": "Well, just one quick piece, just picking up on what Blair said there, I thought it was quite interesting, is if there's this idea of exploring the environment in different ways, but say there's exploring or information gain in terms of personal preference.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3149.274,
    "end": 3149.815,
    "text": "So for instance,",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3150.996,
    "end": 3165.809,
    "text": " maybe it was a bit more of a con i like to always feel what it's like to go slightly up a slope and turn to the right that feels cool right so then you have this kind of like it's not necessarily it's like an information gain about having fun",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3167.458,
    "end": 3175.424,
    "text": " So would it, right, say the car was like, hey, I really like to do this type of things because cars like me like doing that.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3175.844,
    "end": 3177.806,
    "text": "I know, you know, if it was a living animal.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3178.306,
    "end": 3188.573,
    "text": "So it kind of ties into what sort of exploration of the world and then what exploration of just being an entity that likes to do certain things.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3188.813,
    "end": 3194.337,
    "text": "And that could end up taking you there in another way or together they help.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3195.518,
    "end": 3195.799,
    "text": "Great.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3195.999,
    "end": 3196.179,
    "text": "Alec?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3197.628,
    "end": 3208.294,
    "text": " Yeah, I just wanted to comment on that, because this is, I totally agree, and I think it's one of the most promising kind of directions or perspectives that active inference gives.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3209.115,
    "end": 3220.622,
    "text": "And I wrote about this in another paper that I can link afterwards, of this notion of, I don't really know what to call it, but goal-directed exploration that you derive from something like expected fringe.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3220.642,
    "end": 3225.525,
    "text": "And that means you don't necessarily, or it's just too inefficient just to have",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3227.273,
    "end": 3229.235,
    "text": " everlasting exploration with no constraints.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3229.255,
    "end": 3233.74,
    "text": "And it's also too inefficient just to do have exploitation.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3233.78,
    "end": 3246.233,
    "text": "What you need is some objective function like expected free energy or model evidence that contains these two things so that you're not selecting actions to just explore or just to exploit, that each action is kind of shaded with both of these things.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3246.954,
    "end": 3255.879,
    "text": " And that way, you're getting an exploration that's geared towards your goals, and it greatly constrains the type of exploration you'll be doing.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3256.079,
    "end": 3261.322,
    "text": "And I think from an engineering, you know, that also fits with our kind of experience, maybe, of daily life, or maybe not.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3261.362,
    "end": 3269.306,
    "text": "But I think from an engineering perspective, that's crucial, because in real-world tasks, there's just too much to explore, and you need to prioritize that exploration.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3269.686,
    "end": 3271.447,
    "text": "And I think that's something that's important to focus on.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3272.563,
    "end": 3292.438,
    "text": " yep that's so beautiful and even another layer is the ensemble of mountain cars so now imagine we all see different things and we don't know the way to get to the top of the mountain we don't know the policy we don't even know what the end point is we don't know if the one that we can see close by is the best one or if there's a way better one way further away and then everybody",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3293.672,
    "end": 3296.214,
    "text": " is who they are, and they all have their own landscapes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3296.735,
    "end": 3306.182,
    "text": "And then through the ensembles modeling collectively, with or without information sharing of whatever kind, the ensemble, as we're seeing, gets better performance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3306.422,
    "end": 3308.844,
    "text": "There isn't just one best policy of mountain car.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3309.165,
    "end": 3317.691,
    "text": "There are so many different ways to ascend the mountain, and then you open it up with what the objective functions and the policies, the goals could be, and it really is a great space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3317.872,
    "end": 3319.453,
    "text": "Shannon, and then anyone else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3321.495,
    "end": 3321.915,
    "text": "Hi, thanks.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3322.155,
    "end": 3322.575,
    "text": "I was just...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3324.073,
    "end": 3333.798,
    "text": " thinking about your ensemble of cars here and comparing this to like flocking behaviors in birds or even people who are foraging together.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3333.818,
    "end": 3351.748,
    "text": "And maybe I was wondering how, so maybe this entire flock is minimizing its free energy by getting closer to reward, which is food, but any individual bird in the flock is just following",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3352.455,
    "end": 3365.184,
    "text": " local rules about like how close to fly to other agents or when to follow them or when to break off and find a new, like go away from the rest of the flock or drag the flock along with them.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3365.204,
    "end": 3380.315,
    "text": "And I wonder if every single bird is having their own little mountain car model, or if there's just one gross model for the entire flock as a mountain car finding, finding its food.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3381.536,
    "end": 3381.796,
    "text": " Yep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3382.276,
    "end": 3393.019,
    "text": "Well, one angle on the forging and affordances is imagine that ensemble of human forgers, but they're not exactly the same size or they don't have exactly the same preference or they have slightly different vision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3393.099,
    "end": 3396.7,
    "text": "Some people see closer or further, they see color or they don't see color.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3397.2,
    "end": 3403.182,
    "text": "And so all these differences are what allow the ensemble to explore and exploit, especially with information sharing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3403.402,
    "end": 3424.466,
    "text": " hey i was over under this tree and i found this but somebody else wouldn't have found it so we can all use the ensemble to take advantage of our um you know differences of the nest mates or the flock or cognitive diversity and then in the question that you raised about is there a single uh layer being enacted by the flock or is there a little mountain car as you said by each bird",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3425.006,
    "end": 3436.792,
    "text": " So from a modeler's perspective, if it turns out that we can explain variance about the bird's trajectory by putting a linear regression on it, it doesn't say it's a linear regression that the bird is doing, just that it helped us explain variance in the world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3437.212,
    "end": 3448.497,
    "text": "So similarly, the low bar here is that we can use this kind of model just like we could use another type of control theory model or action policy selection model to explain variance about the real world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3448.938,
    "end": 3453.22,
    "text": "And especially because we see organisms succeeding, in fact, it's really the only ones that we do see,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3453.84,
    "end": 3460.085,
    "text": " This helps us explain those successful systems as opposed to like funny little gifs of, you know, a robot flailing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3460.466,
    "end": 3471.174,
    "text": "And then whether there's like one layer that's being enacted and then the group is purely epiphenomenal and there's no downward causation or there's no influence of the group states on the lower level states, there might be some systems like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3471.655,
    "end": 3477.139,
    "text": "There might be other systems where there's an analytical solution, like a well-defined solution at the bird and the flock level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3477.56,
    "end": 3482.724,
    "text": "There might be another one where it's well-defined at one level, but then because of interactions and emergence,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3483.064,
    "end": 3488.489,
    "text": " it isn't as defined or isn't defined by the same class of model at a higher or a more coarse-grained level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3492.304,
    "end": 3492.784,
    "text": " Any thoughts?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3492.804,
    "end": 3493.764,
    "text": "Yeah, that's all great.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3493.784,
    "end": 3493.964,
    "text": "Thanks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3494.145,
    "end": 3494.385,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3494.785,
    "end": 3496.485,
    "text": "Let's look at figure two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3496.685,
    "end": 3512.29,
    "text": "So similarly, as you did for that mountain car, which was like such a helpful direction to take, what can you tell us about the Hopper V2 or the inverted pendulum, which I don't have here, but those were the two tasks, maybe more on the Hopper or the pendulum, whichever one, just what are they about?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3512.71,
    "end": 3516.311,
    "text": "What will machine learning people recognize or know these models as signify?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3518.732,
    "end": 3519.612,
    "text": "So Hopper,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3520.908,
    "end": 3523.69,
    "text": " isn't particularly exploration-based.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3523.75,
    "end": 3525.511,
    "text": "It's quite a dense reward that you get.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3525.531,
    "end": 3528.773,
    "text": "So you kind of get rewarded or dis-rewarded for each of your actions.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3529.534,
    "end": 3532.375,
    "text": "But it's slightly more high-dimensional.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3534.917,
    "end": 3537.379,
    "text": "It says two-dimensional there, but that's just the liquid.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3537.799,
    "end": 3541.541,
    "text": "When I say dimensions, I mean how many observations it receives.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3543.015,
    "end": 3544.015,
    "text": " It's more than two.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3544.035,
    "end": 3545.696,
    "text": "I think it might be like 16.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3545.756,
    "end": 3546.196,
    "text": "I'm not sure.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3546.857,
    "end": 3549.298,
    "text": "And it's just generally the dynamics that you've got to learn.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3549.318,
    "end": 3554.42,
    "text": "You've got maybe four actions, which are real value numbers between minus four and four.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3554.981,
    "end": 3561.584,
    "text": "You've got to learn how that changes 16 variables over the course of however long you're planning.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3561.664,
    "end": 3563.645,
    "text": "So it's a much harder task to learn.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3566.445,
    "end": 3575.588,
    "text": " So I guess that's why it was included in this, just that it's generally regarded as a bit of a highly tasked in the machine learning community.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3576.189,
    "end": 3576.449,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3576.689,
    "end": 3578.81,
    "text": "And so what is happening in figure two?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3579.17,
    "end": 3580.05,
    "text": "What can we say?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3580.21,
    "end": 3582.471,
    "text": "How is it different than DDPG?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3582.611,
    "end": 3585.312,
    "text": "And what actually is DDPG for reference?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3586.889,
    "end": 3593.236,
    "text": " DDPG is, there's so many acronyms, deep deterministic policy gradients.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3594.277,
    "end": 3595.198,
    "text": "So what is that one doing?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3595.278,
    "end": 3599.963,
    "text": "And then what is active inference doing that's different that maybe enables it to have such better performance?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3602.386,
    "end": 3604.929,
    "text": "So what is DDPG doing?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3604.949,
    "end": 3606.951,
    "text": "So it's just, it's a model three reinforcement model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3607.64,
    "end": 3635.588,
    "text": " learning algorithm so you have a policy that maps from states to actions and you essentially do a lot of maths from something like the bellman equation to get to an update for your policy parameters the reason that it's doing so much better and the reason that model-based reinforcement learning in general does so much better is because it's essentially it's a planning algorithm so",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3639.497,
    "end": 3640.88,
    "text": " What's the best way to describe this?",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3640.921,
    "end": 3641.241,
    "text": "Yeah.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3645.086,
    "end": 3651.79,
    "text": " Due to the fact that it's model-based, it can learn from every single bit of data that it receives.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3651.91,
    "end": 3656.633,
    "text": "So that is in terms of each state transition and as well as the reward signals.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3658.414,
    "end": 3670.521,
    "text": "Whereas this model-free reinforcement learning is just simply learning from that single bit of information that it gets at each time step, which is the corresponding or whatever your rewards are.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3673.76,
    "end": 3681.947,
    "text": " So, I mean, eventually DDBG after enough epochs would probably asymptote around or higher than model-based.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3681.987,
    "end": 3687.552,
    "text": "That's the kind of general pattern you get with model-based and model-free reinforcement learning.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3688.192,
    "end": 3699.202,
    "text": "Model-based is far more sample efficient, but model-free takes a hell of a lot more samples, but is asymptotes gets levels out at a higher reward.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3700.663,
    "end": 3708.012,
    "text": " Let me also add a layer there with a few other aspects from machine learning and the idea of the ruggedness of the landscape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3708.452,
    "end": 3714.98,
    "text": "So if you're doing this task, which was described as a dense task, another way of thinking about these is you're trying to keep something upright.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3715.24,
    "end": 3719.165,
    "text": "So it's like very obvious if you're succeeding or failing and in the big landscape,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3719.485,
    "end": 3722.527,
    "text": " There's really easy to tell differences between succeeding and failing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3722.767,
    "end": 3731.831,
    "text": "That being said, policy planning, especially when you have four variables of control that are projecting out to like 16 potentially non-linear connected outcomes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3732.692,
    "end": 3735.473,
    "text": "When you're trying to do that, there may be many strategic mappings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3735.513,
    "end": 3738.154,
    "text": "There might be many policies that help you keep the pendulum up.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3738.354,
    "end": 3741.316,
    "text": "Like for example, going a little bit back and forth with a certain speed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3741.636,
    "end": 3743.678,
    "text": " Or you can go a little further with a different rhythm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3744.138,
    "end": 3754.166,
    "text": "So there might be many, many different ways, even for a single joint of control, many policy sequences through time, many learned relationships that help you stay in that yes-no area.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3754.686,
    "end": 3760.17,
    "text": "Now, if you have a model-free reinforcement learner, it means it's learning basically the raw connection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3760.531,
    "end": 3764.694,
    "text": "It's model-free, so-called, between the reward and the policy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3765.154,
    "end": 3770.777,
    "text": " And so it may spend a lot of its time exploring locally a policy because it's like in the spotlight, it's working.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3770.918,
    "end": 3772.358,
    "text": "And then it goes to a slightly different area.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3772.418,
    "end": 3772.939,
    "text": "It's not working.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3773.419,
    "end": 3774.94,
    "text": "Where do we go from outside of the spotlight?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3775.08,
    "end": 3775.56,
    "text": "It's all dark.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3775.881,
    "end": 3776.281,
    "text": "No idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3776.441,
    "end": 3776.861,
    "text": "Model free.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3777.562,
    "end": 3780.924,
    "text": "With a deep generative model, it's not like this spotlight in or out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3781.644,
    "end": 3796.825,
    "text": " reinforcement it's like we're learning these intrinsic relationships which actually helps us get a grasp of the maps that are going to guide us to search on the territory a little bit more exhaustively but then that part at the end which was so interesting that the model free sometimes gets to higher",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3797.526,
    "end": 3824.423,
    "text": " final performance in some context that can be because a truly model-free search can result in some wacky combination that wouldn't have necessarily been approached that it just uniquely potentially not in a resilient way but it uniquely allows some performance on a task and so that reminds me of like the evolutionary computation where its goal will be to travel distance and then some will actually go down the walking road and those start slow but they can walk forever and then other ones just like fall",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3825.143,
    "end": 3835.233,
    "text": " And so it's kind of like hacking to get over a barrier without necessarily a deeper understanding of the ecology because it's so blindly pursuing just performance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3836.034,
    "end": 3839.177,
    "text": "So there's a lot of stuff there, but this is like really an illustrative example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3839.657,
    "end": 3846.384,
    "text": "And it does highlight a lot of the differences between the model free performance in the state of the art in that as well as what potentially active inference could bring.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3848.601,
    "end": 3852.265,
    "text": " Any thoughts on two, or now we'll just have some general areas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3852.846,
    "end": 3855.829,
    "text": "Basically, the first area was implications.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3856.029,
    "end": 3866.921,
    "text": "I know we've probably talked about some of them, but three areas that I thought about were like robotics, resources and allocation, and then rugged landscapes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3867.542,
    "end": 3869.044,
    "text": "This one probably in the Southwest.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3870.265,
    "end": 3872.066,
    "text": " So any thoughts on these?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3872.126,
    "end": 3886.694,
    "text": "I think we've almost touched on related ideas, but if anyone wants to speak to one of these areas of possible implication, whether they do or they don't see what an implication could be, or what would another domain of implication be?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3888.055,
    "end": 3889.316,
    "text": "Yeah, Alec, go ahead.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3890.956,
    "end": 3897.84,
    "text": "So I also am becoming increasingly interested in some other people in",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3899.73,
    "end": 3917.625,
    "text": " whether some, so I'm very invested in this idea of kind of a Bayesian brain and as active inference kind of as fast as, but whether some of these ideas from Bayesian machine learning that I used in this paper, so stuff like amortization might also be employed in nervous systems.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3919.249,
    "end": 3932.538,
    "text": " So, you know, we've had a few very, very big proposals for how the brain might implement Bayesian inference stuff like appropriation coding, predictive coding, the process theory that's most commonly associated with active inference in terms of message parsing.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3932.578,
    "end": 3941.805,
    "text": "But it might also be, you know, this amortization is also another real possibility of how the brain implements some type of inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3941.825,
    "end": 3944.487,
    "text": "So I think it could have implications for understanding neuroscience.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 3945.987,
    "end": 3963.128,
    "text": " Wow, did not expect that message passing and predictive processing, predictive coding type models can be understood alongside amortization models as an alternate implementation or mechanism of Bayesian brain as a specific testable hypothesis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3963.829,
    "end": 3973.313,
    "text": " So this is where the rubber hits the road with the modeling and with showing the deep mathematical isomorphisms between these different kinds of relationships.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3973.753,
    "end": 3978.975,
    "text": "Like there might be some paper, I'm sure there is, where like message passing is equivalent to Bayesian networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3979.575,
    "end": 3983.059,
    "text": " And then that allows us to bridge to big areas of literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3983.399,
    "end": 3992.629,
    "text": "And so if we do a lot of investigation in Bayesian brain, and then we find out, wait, there's actually multiple ways that could be implemented in different systems, whether that's through message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3992.769,
    "end": 3994.791,
    "text": "So maybe that's more applicable to a computer network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3995.251,
    "end": 3999.494,
    "text": " Maybe it's also going to be implemented by policy planning ensembles.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3999.895,
    "end": 4004.458,
    "text": "So then, whoa, what else is doing this kind of Bayesian-like processing?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4004.918,
    "end": 4007.38,
    "text": "So there's so many cool directions there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4007.82,
    "end": 4013.945,
    "text": "Great topic because also robotics, or at least the question of implementation of action and selection of policy,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4014.665,
    "end": 4016.266,
    "text": " and from sensors and with actuators.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4016.767,
    "end": 4020.289,
    "text": "Resources in terms of informational, attentional, whatever they may be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4020.689,
    "end": 4023.251,
    "text": "And then rugged landscapes is just, that's everything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4023.551,
    "end": 4028.855,
    "text": "That is policy selection under uncertainty for any kind of system that wants to stay alive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4029.756,
    "end": 4032.057,
    "text": "Let's go, oh, sorry, Sasha or Blue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4032.638,
    "end": 4033.378,
    "text": "Oh, see you later, Blue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4033.398,
    "end": 4035.54,
    "text": "Oh, wait, Sasha out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4035.74,
    "end": 4036.941,
    "text": "Blue, please, thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4038.722,
    "end": 4039.923,
    "text": "Wait, muted, but yeah, go ahead.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4041.171,
    "end": 4062.867,
    "text": " So I just I had the same reaction that you did like Bayesian brain amortization like wow I was just was wondering Alec if you could maybe just unpack that like in a very Simple way or elaborate more on on that Yeah, sure so The best way so I mean amortization",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 4064.341,
    "end": 4068.042,
    "text": " as it's realized in stuff like various normal encoders, but also other schemes.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4069.062,
    "end": 4079.524,
    "text": "We can maybe discuss a bit more together what defines amortization or just an instance, a way of learning a generative model and well, more precisely doing inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4079.984,
    "end": 4091.686,
    "text": "The key defining features for me, although I don't think this has ever been properly defined in the literature is this notion of having an encoder, how that would play out in the brain",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4092.936,
    "end": 4109.864,
    "text": " how it could play out in the brain is just kind of a feed forward mapping, a feed forward part of say the visual cortex that maps from some data or a lower part in the hierarchy to some posterior parameters higher up in the hierarchy.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4110.024,
    "end": 4117.467,
    "text": "And when you have stuff like population coding or predictive coding, there's not this notion of quickly",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4118.126,
    "end": 4131.432,
    "text": " in a feed-forward manner, mapping from data to parameters, you've got this kind of iterative procedure that slowly and sequentially updates the beliefs, those posterior beliefs, based on the data, based on some learning.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4131.853,
    "end": 4135.794,
    "text": "So it's another route towards Bayesian inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4135.975,
    "end": 4140.817,
    "text": "It's far more fast and doesn't require a current processing, and that kind of",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4141.696,
    "end": 4144.598,
    "text": " speaks well with some things we know about the brain.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4144.779,
    "end": 4145.479,
    "text": "It's not all the brain.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4145.779,
    "end": 4151.004,
    "text": "The brain definitely does have a processing, but a lot can be done in a kind of very quickly.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4153.566,
    "end": 4164.733,
    "text": " And the second thing is that the parameters that you're optimizing of the encoder are optimized over the entire data set, whereas in something like predictive coding, it's optimized individually.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4165.173,
    "end": 4170.876,
    "text": "The posterior parameters of your beliefs are optimized based on the current data point.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4171.957,
    "end": 4178.341,
    "text": "And I think that has some, that might speak to some of the generalization capabilities of Bayesian inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4179.721,
    "end": 4182.663,
    "text": " And that's something that we're looking at at the moment.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4182.763,
    "end": 4184.945,
    "text": "So I hope that kind of answers your question.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4184.985,
    "end": 4185.626,
    "text": "No, definitely.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4186.206,
    "end": 4187.267,
    "text": "Thank you so much.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4187.547,
    "end": 4194.753,
    "text": "Totally epic to connect that feed forward encoder model to the machine learning side.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4195.313,
    "end": 4202.759,
    "text": "So instead of doing like a back and forth expectation maximization where you're just updating back and forth or especially a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4204.42,
    "end": 4210.204,
    "text": " like a back propagation type thing where there's very, very complex interactions about how parameters are trained.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4210.625,
    "end": 4214.487,
    "text": "This allows us to kind of train on the fly and just learn as we go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4214.507,
    "end": 4219.171,
    "text": "Now to the second point, which is the usage of the entire data set versus point by point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4219.871,
    "end": 4231.039,
    "text": "This is like we're learning on the fly, but we're learning on the fly in the state space that we want to be learning about with respect to the entire data set that we have access to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4231.9,
    "end": 4236.121,
    "text": " which is a little bit different than like you're kind of trailing your finger along a time series.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4236.601,
    "end": 4242.743,
    "text": "And then you're updating based upon that data point and what you believe and what you've recently seen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4243.223,
    "end": 4252.946,
    "text": "That's like a kind of point by point way to use a whole data set to update parameters versus this variational approach, which actually enables like almost a simultaneous utilization",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4253.826,
    "end": 4279.426,
    "text": " of the entire state space i'm not sure if that's totally correct but those are just two parts that hit me about the two sides that you mentioned alec stephen and then anyone else so i've not heard of this amortization so it's quite interesting it's it's almost like a reverse then is it it's like you you have a a big complex space of",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4280.732,
    "end": 4288.58,
    "text": " knowing and you quickly discount stuff and the process is about discounting away rather than building up the model.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4289.501,
    "end": 4291.283,
    "text": "Is that kind of the idea?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4294.031,
    "end": 4303.238,
    "text": " I'm not sure if I've understood exactly what you're looking at, but for my, I'm not sure if it is, but maybe we could get into that a bit more.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4303.498,
    "end": 4307.221,
    "text": "Well, how would you define, Alec, how would you define amortized inference?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4307.301,
    "end": 4310.263,
    "text": "Like we see it here in the paper, but how would you define it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4310.323,
    "end": 4316.348,
    "text": "You mentioned some benefits, we've talked about them a little bit, but like in this kind of new way of thinking about it, what does it mean?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4316.408,
    "end": 4317.769,
    "text": "Or what would it mean for the brain to do it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4317.929,
    "end": 4319.45,
    "text": "Or what happened programmatically?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4320.14,
    "end": 4322.261,
    "text": " I mean, so this is what I was getting at earlier.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4322.341,
    "end": 4324.322,
    "text": "It doesn't really have a clear definition.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4325.643,
    "end": 4327.144,
    "text": "I try and define it in two ways.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4327.304,
    "end": 4329.425,
    "text": "One is how it's generally used.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4329.605,
    "end": 4338.77,
    "text": "If you see amortized inference in machine learning, you're generally, you should think of a encoder network.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4338.79,
    "end": 4341.492,
    "text": "So you've got a neural network that takes in your data",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4342.518,
    "end": 4352.604,
    "text": " and it will output literal values for your parameters that would normally, and those parameters are the ones that would normally be optimized in traditional variational methods.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4353.184,
    "end": 4356.045,
    "text": "Here it's just, they're just spat out by the network.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4356.886,
    "end": 4364.41,
    "text": "And then you have some other learning method which updates that encoder for you and also your generative model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4364.55,
    "end": 4371.074,
    "text": "But in terms of what amortization actually means, going back to the word, I think the word's in economics,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4372.432,
    "end": 4385.99,
    "text": " I think it's this idea that you, coming to the second point that I made where you're sharing the parameters of that encoder across all of your data, so you're amortizing the cost of inference is how I think about it.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4386.07,
    "end": 4388.112,
    "text": "I could be very wrong.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4389.154,
    "end": 4390.716,
    "text": "So that just means you kind of don't,",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4392.443,
    "end": 4401.028,
    "text": " reset with each new observation, or you're not trying to optimize with respect to each new observation, and then kind of discarding information.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4401.048,
    "end": 4408.793,
    "text": "You're just sharing that encoder, the encoder, which is the relationship between the data and the parameters across your entire lifetime.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4408.893,
    "end": 4412.455,
    "text": "So you're amortizing the cost of inference, is how I think about it.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4412.915,
    "end": 4413.896,
    "text": "So if I find a bad quote.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4415.997,
    "end": 4421.641,
    "text": "So is it like, so as opposed to just discounting data, it's discounting ways of",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4422.717,
    "end": 4450.657,
    "text": " chunking up the data or ways of analyzing it if that makes sense like you could have lots of different models and they say okay i'm going to apply them in a more efficient way over time as you start to infer it's almost inferring what models should be deployed is that the kind of idea and you start to take away the models which are less useful in a way i guess you could another kind of people sometimes refer to it as learning to infer",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4451.798,
    "end": 4459.364,
    "text": " so in which might be what you're getting at um so in normal inference you're just doing inference but here you're learning",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4460.478,
    "end": 4461.438,
    "text": " how to do inference.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4461.718,
    "end": 4463.299,
    "text": "So the learning, yeah.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4463.859,
    "end": 4467.18,
    "text": "So maybe that was what you were going at with your kind of model selection.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4468.16,
    "end": 4468.8,
    "text": "Cool, thanks.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4468.82,
    "end": 4470.521,
    "text": "Yeah, that makes sense.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4471.061,
    "end": 4473.722,
    "text": "I don't know, but I'm trying to find my way through.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4474.722,
    "end": 4477.443,
    "text": "There's a few dimensions, so it's really helpful to unpack it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4477.483,
    "end": 4481.684,
    "text": "There's the learning how to learn, there's that meta learning elements, but also this is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4482.324,
    "end": 4487.266,
    "text": " really important and in the paper is the number of parameters remains constant with respect to the size of the data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4487.546,
    "end": 4495.189,
    "text": "So if I'm trying to estimate the mean and the variance of a normal distribution, I know that I want to squeeze down whatever data set I have into two numbers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4495.81,
    "end": 4499.451,
    "text": "And so if I have three numbers coming in the pipeline, I still want a mean and a variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4499.851,
    "end": 4504.073,
    "text": "If I want a billion numbers to come in the pipeline, I still I want mean variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4504.493,
    "end": 4511.076,
    "text": "And so it turns out that by knowing and by specifying ahead of time that you know exactly the size of the outcome,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4511.676,
    "end": 4521.67,
    "text": " it allows you to scale a lot better because you can know that no matter what you're going to put in on the inside, it's going to be able to be distilled down very rapidly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4522.05,
    "end": 4528.999,
    "text": "Whereas another method that results, okay, well, we put in a data set and then we find out how many principal components explain it best.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4529.6,
    "end": 4551.897,
    "text": " that might computationally scale in a way that is very um disadvantageous with respect to the size of the data whereas this method can um do that and then as a sort of corollary there it's happening through a single forward pass of a network which can be ab initio sort of like de novo setting it up from the beginning and or updated in a learning fashion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4552.637,
    "end": 4554.178,
    "text": " And so there's a few dimensions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4554.238,
    "end": 4564.545,
    "text": "And Alec, I'd really look forward to seeing what you continue to do with the amortized inference, seeing as that concept becomes a bit more formalized because this seems like really powerful.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4565.005,
    "end": 4582.357,
    "text": "And then if it also turns out that it's an implementation of Bayesian statistics or Bayesian brain, then it just taps into the entire message passing compute graph Bayesian net framework, which already has been the most helpful for machine learning and a lot of other areas too.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4585.258,
    "end": 4586.258,
    "text": " Very, very cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4586.558,
    "end": 4597.722,
    "text": "So I guess in the next steps, which is really, I guess the last thing that we'll talk about for the last few minutes, just thought of a few areas to go into for next steps.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4597.742,
    "end": 4603.424,
    "text": "So Alec, definitely you take the first pass, but the areas could be like computationally, what does that mean?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4603.484,
    "end": 4609.886,
    "text": "Hardware, software, from a math perspective, what could be analytically shown or what relationships would be good to know?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4610.807,
    "end": 4615.136,
    "text": " From an applications perspective, what robots need this kind of software update?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4615.537,
    "end": 4620.989,
    "text": "And then just educationally, how do we come to control our attention so that we understand these concepts as well?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4628.539,
    "end": 4630.82,
    "text": " or what did you start working on after this paper?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4630.86,
    "end": 4631.76,
    "text": "I guess it was- Oh, sorry.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4631.84,
    "end": 4632.8,
    "text": "Oh yeah, no worries, no worries.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4633.501,
    "end": 4634.721,
    "text": "I wasn't, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4634.821,
    "end": 4636.581,
    "text": "I guess you did this one, or where are you at now?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4636.621,
    "end": 4657.448,
    "text": "What the next steps are, I guess on this line of thinking, this line of work, the stuff that I'm still thinking about is what was mentioned right at the beginning, which is the balance of exploitation and exploration and whether the active inference perspective buys you anything in practice.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4659.008,
    "end": 4674.493,
    "text": " because, you know, we've got this nice objective, functional, expected free energy, and we can optimize actions with respect to it, and it does contain exploration, exploitation, and it motivates exploration and exploitation from first days and principles.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4674.593,
    "end": 4683.516,
    "text": "But in practice, you're gonna be, in any model, you're going to be fine-tuning those, the balance between those.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4683.996,
    "end": 4686.136,
    "text": "You've got a bit more of leeway to do it, because now you can",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4688.237,
    "end": 4706.862,
    "text": " change the shape of your prior beliefs for instance uh and change a few other parameters that are gonna lead to that playoff rather than just having some uh scale or weight that defines them so some of the stuff that i've been looking at is yeah learning prior beliefs uh in order to facilitate exploration exploitation um",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4708.319,
    "end": 4726.185,
    "text": " Because I think, to summarize that, the balance between exploration and exploitation is probably one of the big unsolved questions in control, reinforcement learning, neuroscience, et cetera, and for making these machines work as well as we'd like them to.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4727.606,
    "end": 4732.948,
    "text": "And active inference offers a new route to try and solve that, but I don't think it has been solved.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4734.989,
    "end": 4735.309,
    "text": " Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4735.589,
    "end": 4736.829,
    "text": "That's really exciting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4737.169,
    "end": 4752.833,
    "text": "There's just so many aspects to the control theory question when there's a lot of solutions that could work and maybe it's even unclear what it would look like to work, especially as we think about some of these systems that go beyond the merely just keep a pendulum standing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4752.913,
    "end": 4754.773,
    "text": "But what if it's give someone a massage?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4755.233,
    "end": 4757.434,
    "text": "Well, that's a little bit of a different question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4757.494,
    "end": 4761.835,
    "text": "It's relational or how are you going to exercise in a way that's comfortable for you?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4762.375,
    "end": 4766.016,
    "text": " These are control theory questions that are literally about action sequences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4766.656,
    "end": 4780.901,
    "text": "And so when we start thinking about how we're going to apply it to different systems, having a framework that is at least moving in this direction of what you said is like with the reconsidering the explore exploit by adding things like reconsidering the shape of your priors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4781.681,
    "end": 4795.195,
    "text": " So how can you step into a framework where the explore exploit is a dimension that could be calculated or described or kind of summarized, but isn't simply the underlying framework of the model?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4795.555,
    "end": 4797.197,
    "text": "We're going to get exploratory behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4797.217,
    "end": 4800.661,
    "text": "We're going to get so-called exploitative or narrowly searching behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4801.221,
    "end": 4803.422,
    "text": " That is not being ruled out by what we're discussing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4803.842,
    "end": 4817.068,
    "text": "We're talking about another way that optimization could proceed that doesn't use simply the explore versus exploit or the coefficient weighting of these two through time or statically to outline its whole learning approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4817.508,
    "end": 4824.371,
    "text": "So Steven first and then anyone else if they want to give any like last thoughts, but this has been a great conversation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4824.491,
    "end": 4825.792,
    "text": "So Steven and then anyone else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4827.779,
    "end": 4833.083,
    "text": " I mean, also, if you go into the real world environment, you've got this explore, exploit situation.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4833.163,
    "end": 4841.089,
    "text": "But in high stakes situations, you also got like they often talk about risk mitigation versus gain optimization.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4841.71,
    "end": 4851.437,
    "text": "If you're in a kind of a conflict or, you know, a legal situation, because it may be that it's a question of, you know, which is in a way.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4852.538,
    "end": 4874.775,
    "text": " game optimization is a bit like an exploit but it depends which way you're going it could be an information game but whatever you're you're trying to do you those two two aspects sort of come into play depending on the state of the of the situation you know so you know explore exploit is less relevant when you're standing on the edge of a cliff",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 4875.535,
    "end": 4900.807,
    "text": " playing a game you know because it's like there's a risk mitigation issue so anyway i thought that might be interesting i just thought about you know doing at learning action policies but if you're learning an action policy where you could die if you're rock climbing or something it's going to change how you learn and so when you're trying to minimize risk versus failure attempt or minimize the number of opportunities you have to see it successfully performed or you can only infer it by observing it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4901.627,
    "end": 4908.471,
    "text": " These things may contain a ton of information in terms of how real systems learn, where failure is not an option.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4908.811,
    "end": 4911.533,
    "text": "Mel, and then anyone else who wants to close it out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4914.455,
    "end": 4928.583,
    "text": "Yeah, sorry if this is, I guess, a sort of an ignorant question, but I thought some of the beauty of these active inference and FEP approaches is that they're sort of doing a kind of like multi-level dynamic outcomes analysis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4929.085,
    "end": 4929.926,
    "text": " razor type thing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4929.966,
    "end": 4937.75,
    "text": "So they're maximizing predictability while also kind of minimizing the complexity of what they're doing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4937.79,
    "end": 4945.635,
    "text": "And so I guess what I don't quite understand is how the amortization of inference process works.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4945.695,
    "end": 4957.742,
    "text": "But if you're fixing the param, doesn't that kind of constrain the ability of these approaches to do that kind of Occam's razor type thing?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4961.983,
    "end": 4968.791,
    "text": " Not really, because the parameters of your encoder aren't the parameters that you're up to.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4969.012,
    "end": 4970.634,
    "text": "They're not the free energy parameters.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4970.854,
    "end": 4972.696,
    "text": "They're mapping to the free energy parameters.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4973.557,
    "end": 4974.778,
    "text": "So you could map...",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4975.865,
    "end": 4985.51,
    "text": " you know, to, you know, just says that you've got to maximize the accuracy of the likelihood while minimizing the complexity.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4985.53,
    "end": 4994.155,
    "text": "So as long as your encoder maps to the part of belief space that is maximally accurate and also minimally complex, then you're still minimizing",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4995.606,
    "end": 4997.167,
    "text": " complexity while maximizing accuracy.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 4998.207,
    "end": 5008.811,
    "text": "And then obviously there's, what we haven't discussed is the learning schemes for, so the way you learn your encoder parameters is so that they output something that does conform to minimal variation of range.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 5009.872,
    "end": 5010.152,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5010.412,
    "end": 5018.395,
    "text": "Okay, so you're not actually constraining, you're not actually placing like additional constraints on the parameters of the actual.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5018.435,
    "end": 5021.597,
    "text": "Not on the parameters of the encoder, on the output of the encoder.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 5022.237,
    "end": 5022.377,
    "text": "Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5024.423,
    "end": 5025.284,
    "text": " I guess that's a good point.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 5025.624,
    "end": 5033.646,
    "text": "So the encode, I guess a problem with amortization is that the encode is kind of not part, you shouldn't think of it as part of your generative model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 5033.706,
    "end": 5044.329,
    "text": "It's just almost like a tool that can map to the belief space of your generative model.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 5044.429,
    "end": 5044.789,
    "text": "So yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5046.195,
    "end": 5054.001,
    "text": " Just on a closing note there, that really returns us to this dual instrumentalism with rappers and rappers and is it what the system is doing or is it how we're looking at it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5054.081,
    "end": 5058.344,
    "text": "These questions are very rich and so it's a great conversation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5058.825,
    "end": 5061.527,
    "text": "Really, thanks everyone for participating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5061.907,
    "end": 5070.213,
    "text": "This was such a helpful discussion and I think definitely while re-listening, we'll all pick out some questions for ourselves to follow up on and some...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5071.394,
    "end": 5094.996,
    "text": " curious things to learn about because there were so many good ideas brought up so people who are on live if they check their calendar event they will see a feedback form which would be helpful and anyone else who's listening or watching please provide us with feedback suggestions or questions but other than that just um stay in touch and everyone awesome work for this helpful discussion and we'll see you soon",
    "speaker": "SPEAKER_00"
  }
]
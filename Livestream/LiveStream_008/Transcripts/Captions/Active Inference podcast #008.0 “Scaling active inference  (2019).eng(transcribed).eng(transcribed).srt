1
00:00:08,000 --> 00:00:08,880
all right

2
00:00:08,880 --> 00:00:11,759
hello and welcome everyone to the active

3
00:00:11,759 --> 00:00:13,200
inference live stream

4
00:00:13,200 --> 00:00:15,120
this is active inference live stream

5
00:00:15,120 --> 00:00:17,279
number 8.0

6
00:00:17,279 --> 00:00:20,640
it is november 4th 2020

7
00:00:20,640 --> 00:00:23,920
and i am daniel friedman i'll be doing a

8
00:00:23,920 --> 00:00:28,000
solo contextualizing discussion today

9
00:00:28,000 --> 00:00:30,960
welcome to teamcom everyone we are an

10
00:00:30,960 --> 00:00:33,280
experiment in online team communication

11
00:00:33,280 --> 00:00:35,200
learning and practice related to active

12
00:00:35,200 --> 00:00:37,200
inference you can find us

13
00:00:37,200 --> 00:00:38,960
on our twitter account at

14
00:00:38,960 --> 00:00:40,640
inferenceactive at

15
00:00:40,640 --> 00:00:43,360
activeinference gmail.com at our public

16
00:00:43,360 --> 00:00:45,120
key base team

17
00:00:45,120 --> 00:00:48,160
or at our youtube channel this is a

18
00:00:48,160 --> 00:00:49,120
recorded

19
00:00:49,120 --> 00:00:51,360
and an archived live stream so please

20
00:00:51,360 --> 00:00:53,120
provide us with feedback so that we can

21
00:00:53,120 --> 00:00:54,719
improve on our work

22
00:00:54,719 --> 00:00:56,559
all backgrounds and perspectives are

23
00:00:56,559 --> 00:00:58,559
welcome here and as far as

24
00:00:58,559 --> 00:01:00,719
video etiquette for live streams mute if

25
00:01:00,719 --> 00:01:02,160
there's noise in your background

26
00:01:02,160 --> 00:01:03,440
raise your hand so we can hear from

27
00:01:03,440 --> 00:01:05,680
everyone use respectful speech behavior

28
00:01:05,680 --> 00:01:06,880
etc

29
00:01:06,880 --> 00:01:10,159
so first the announcement is that

30
00:01:10,159 --> 00:01:12,400
we have chosen the papers and the dates

31
00:01:12,400 --> 00:01:13,200
and the time

32
00:01:13,200 --> 00:01:15,200
for the rest of the actin streams for

33
00:01:15,200 --> 00:01:17,119
2020 all meetings

34
00:01:17,119 --> 00:01:19,759
for the rest of 2020 will be from 7 30

35
00:01:19,759 --> 00:01:22,159
to 9 a.m pst

36
00:01:22,159 --> 00:01:24,240
and the papers will be reading number

37
00:01:24,240 --> 00:01:26,159
eight is scaling active inference that's

38
00:01:26,159 --> 00:01:27,439
what this

39
00:01:27,439 --> 00:01:30,400
8.0 is going to be about and that's

40
00:01:30,400 --> 00:01:33,119
going to be on november 10th and 17th

41
00:01:33,119 --> 00:01:34,960
paper 9 will be the projective

42
00:01:34,960 --> 00:01:36,560
consciousness model and phenomenal

43
00:01:36,560 --> 00:01:39,200
selfhood a 2018 paper

44
00:01:39,200 --> 00:01:41,759
paper 10 is going to be a variational

45
00:01:41,759 --> 00:01:42,880
approach to scripts

46
00:01:42,880 --> 00:01:45,200
paper 11 is sophisticated active

47
00:01:45,200 --> 00:01:47,119
inference effective information sorry

48
00:01:47,119 --> 00:01:48,880
simulating anticipatory effective

49
00:01:48,880 --> 00:01:51,840
dynamics of imagining future events

50
00:01:51,840 --> 00:01:53,280
and you can see the dates that all these

51
00:01:53,280 --> 00:01:55,119
events are so

52
00:01:55,119 --> 00:01:57,360
set aside at times if it's possible for

53
00:01:57,360 --> 00:01:58,799
you to participate

54
00:01:58,799 --> 00:02:00,719
and if you have a time zone or kind of

55
00:02:00,719 --> 00:02:02,159
event that you want to do that's not

56
00:02:02,159 --> 00:02:04,240
reflected here just let us know

57
00:02:04,240 --> 00:02:06,479
and there's our twitter address all

58
00:02:06,479 --> 00:02:07,680
right

59
00:02:07,680 --> 00:02:10,959
so here's what's going to happen in

60
00:02:10,959 --> 00:02:14,800
active stream 8.0 this one

61
00:02:14,800 --> 00:02:17,120
the goal of this talk is going to be to

62
00:02:17,120 --> 00:02:20,000
set the context for 8.1 and 8.2 which

63
00:02:20,000 --> 00:02:21,040
are going to be on this

64
00:02:21,040 --> 00:02:23,360
same paper scaling active inference this

65
00:02:23,360 --> 00:02:24,560
is a paper by

66
00:02:24,560 --> 00:02:27,760
alex chance baltierri seth and buckley

67
00:02:27,760 --> 00:02:29,200
from 2019

68
00:02:29,200 --> 00:02:35,040
with the archive 1911.10601

69
00:02:35,040 --> 00:02:36,800
the video is an introduction to the

70
00:02:36,800 --> 00:02:38,879
context of some of these ideas it's not

71
00:02:38,879 --> 00:02:39,599
a review

72
00:02:39,599 --> 00:02:42,000
or a final word definitely i learned a

73
00:02:42,000 --> 00:02:43,920
lot just reading through the paper and

74
00:02:43,920 --> 00:02:47,440
researching for this presentation

75
00:02:47,440 --> 00:02:50,080
so the idea is that this video will

76
00:02:50,080 --> 00:02:52,080
contextualize some of the ideas

77
00:02:52,080 --> 00:02:55,040
math and notation and vocabulary of the

78
00:02:55,040 --> 00:02:56,640
shantz paper

79
00:02:56,640 --> 00:02:58,480
and the video should be accessible

80
00:02:58,480 --> 00:03:00,480
though this is also hopefully cool

81
00:03:00,480 --> 00:03:03,680
and cutting edge research

82
00:03:03,680 --> 00:03:06,159
and the punch line and don't worry if it

83
00:03:06,159 --> 00:03:07,680
doesn't make sense yet or the

84
00:03:07,680 --> 00:03:08,879
implications aren't clear yet

85
00:03:08,879 --> 00:03:11,599
is that active inference is scalable and

86
00:03:11,599 --> 00:03:12,720
it's homologous too

87
00:03:12,720 --> 00:03:14,640
so it's similar to and potentially

88
00:03:14,640 --> 00:03:15,920
preferable to other

89
00:03:15,920 --> 00:03:18,640
common algorithms in a similar space

90
00:03:18,640 --> 00:03:21,440
like control theory or machine learning

91
00:03:21,440 --> 00:03:25,120
so in 8.0 the first section

92
00:03:25,120 --> 00:03:28,879
is going to be some background on all

93
00:03:28,879 --> 00:03:30,560
the keywords that they used

94
00:03:30,560 --> 00:03:33,040
in this paper that they provided and

95
00:03:33,040 --> 00:03:35,599
then talk about the goals the abstract

96
00:03:35,599 --> 00:03:37,680
and the roadmap

97
00:03:37,680 --> 00:03:40,959
then the second part of 8.0 will be the

98
00:03:40,959 --> 00:03:42,879
key equations annotation

99
00:03:42,879 --> 00:03:44,879
and quotations walkthrough and we're

100
00:03:44,879 --> 00:03:46,000
going to do like the 80

101
00:03:46,000 --> 00:03:48,959
20. so most of the notation most of the

102
00:03:48,959 --> 00:03:49,840
meaning but

103
00:03:49,840 --> 00:03:52,959
not all the sections not all the symbols

104
00:03:52,959 --> 00:03:55,920
and then talk about figure 1 and figure

105
00:03:55,920 --> 00:03:56,400
2

106
00:03:56,400 --> 00:03:58,400
and what they represent and how that

107
00:03:58,400 --> 00:04:00,799
supports the conclusions of the paper

108
00:04:00,799 --> 00:04:04,640
and then in 8.1 and 8.2 we will all come

109
00:04:04,640 --> 00:04:06,720
together to discuss the same paper

110
00:04:06,720 --> 00:04:08,560
so save and submit your questions or put

111
00:04:08,560 --> 00:04:10,319
them as a comment and then get in touch

112
00:04:10,319 --> 00:04:12,560
if you want to participate in this one

113
00:04:12,560 --> 00:04:15,920
okay so let's start with the keywords

114
00:04:15,920 --> 00:04:17,680
so these were just the keywords that the

115
00:04:17,680 --> 00:04:19,839
paper provided so i would think of them

116
00:04:19,839 --> 00:04:21,199
these are the topics that

117
00:04:21,199 --> 00:04:23,040
this research is going to be on the

118
00:04:23,040 --> 00:04:25,280
cutting edge with respect to that field

119
00:04:25,280 --> 00:04:28,080
hopefully so they are artificial

120
00:04:28,080 --> 00:04:28,800
intelligence

121
00:04:28,800 --> 00:04:31,759
machine learning and they mention also

122
00:04:31,759 --> 00:04:33,040
reinforcement learning

123
00:04:33,040 --> 00:04:36,000
and model based reinforcement learning

124
00:04:36,000 --> 00:04:36,560
and then

125
00:04:36,560 --> 00:04:38,320
systems and control and information

126
00:04:38,320 --> 00:04:40,240
theory and then a keyword that wasn't on

127
00:04:40,240 --> 00:04:40,479
the

128
00:04:40,479 --> 00:04:42,479
paper but we can just add here is of

129
00:04:42,479 --> 00:04:43,840
course active inference and the free

130
00:04:43,840 --> 00:04:45,280
energy principle

131
00:04:45,280 --> 00:04:48,639
so um each of these keywords definitely

132
00:04:48,639 --> 00:04:49,680
you could have a

133
00:04:49,680 --> 00:04:52,720
course or a phd on so

134
00:04:52,720 --> 00:04:54,240
each one of them is going to get about

135
00:04:54,240 --> 00:04:56,160
one slide so

136
00:04:56,160 --> 00:04:58,960
of course go deeper into the resources

137
00:04:58,960 --> 00:04:59,600
mentioned

138
00:04:59,600 --> 00:05:02,560
or find other courses that teach these

139
00:05:02,560 --> 00:05:04,000
techniques because there's a lot to them

140
00:05:04,000 --> 00:05:05,280
i'm just going to kind of

141
00:05:05,280 --> 00:05:07,120
go through them in a way that builds

142
00:05:07,120 --> 00:05:08,560
towards where we're going

143
00:05:08,560 --> 00:05:10,479
with the math and with this research

144
00:05:10,479 --> 00:05:12,240
paper and just know that there's other

145
00:05:12,240 --> 00:05:14,080
people's perspectives on these topics

146
00:05:14,080 --> 00:05:15,120
too

147
00:05:15,120 --> 00:05:18,560
so first artificial intelligence

148
00:05:18,560 --> 00:05:21,840
wikipedia when we last checked it

149
00:05:21,840 --> 00:05:24,160
was saying that artificial intelligence

150
00:05:24,160 --> 00:05:25,120
or ai

151
00:05:25,120 --> 00:05:27,280
is intelligence that is demonstrated by

152
00:05:27,280 --> 00:05:29,199
machines unlike the

153
00:05:29,199 --> 00:05:31,759
natural intelligence displayed by humans

154
00:05:31,759 --> 00:05:33,280
and animals

155
00:05:33,280 --> 00:05:36,240
so even using this definition or perhaps

156
00:05:36,240 --> 00:05:37,680
an alternate one which i'll get to in a

157
00:05:37,680 --> 00:05:38,479
second

158
00:05:38,479 --> 00:05:41,280
we can say that what ai results in today

159
00:05:41,280 --> 00:05:42,240
2020

160
00:05:42,240 --> 00:05:45,039
is it results in maps policies

161
00:05:45,039 --> 00:05:47,039
individual and collective policies

162
00:05:47,039 --> 00:05:49,120
recommendation engines it results in

163
00:05:49,120 --> 00:05:51,280
encryption techniques and security

164
00:05:51,280 --> 00:05:53,440
it relates to classification algorithms

165
00:05:53,440 --> 00:05:55,039
and things that influence us

166
00:05:55,039 --> 00:05:58,800
us all now where i would come at this

167
00:05:58,800 --> 00:06:01,600
is by saying well ai is made and used by

168
00:06:01,600 --> 00:06:02,960
humans as part of their

169
00:06:02,960 --> 00:06:06,000
natural world and everything is natural

170
00:06:06,000 --> 00:06:09,520
in that it exists in the world and so by

171
00:06:09,520 --> 00:06:11,759
positing that there's an artificial type

172
00:06:11,759 --> 00:06:13,120
of cognition

173
00:06:13,120 --> 00:06:15,759
that is somehow uh distinct rather than

174
00:06:15,759 --> 00:06:17,199
extended or embedded

175
00:06:17,199 --> 00:06:20,400
enacted by humans does a few things

176
00:06:20,400 --> 00:06:21,840
that aren't very helpful and there's

177
00:06:21,840 --> 00:06:24,000
been some excellent

178
00:06:24,000 --> 00:06:26,720
other discussions this topic just to

179
00:06:26,720 --> 00:06:28,319
move it in a positive direction

180
00:06:28,319 --> 00:06:30,000
i would just say that often other

181
00:06:30,000 --> 00:06:31,919
phrasing can be more helpful

182
00:06:31,919 --> 00:06:35,360
than calling it ai so here's one example

183
00:06:35,360 --> 00:06:36,720
what we're going to talk about a lot

184
00:06:36,720 --> 00:06:38,720
today is computer statistics

185
00:06:38,720 --> 00:06:41,600
so using computers to do statistics but

186
00:06:41,600 --> 00:06:43,520
basically things that pen and paper

187
00:06:43,520 --> 00:06:45,759
could do slowly so things that are about

188
00:06:45,759 --> 00:06:47,440
matrix calculations are about

189
00:06:47,440 --> 00:06:51,680
statistics with computers on big data

190
00:06:51,680 --> 00:06:54,560
another way that ai can be meant today

191
00:06:54,560 --> 00:06:55,280
is like

192
00:06:55,280 --> 00:06:57,919
human in the loop ai so human in the

193
00:06:57,919 --> 00:06:59,520
loop ai could describe

194
00:06:59,520 --> 00:07:02,160
a map or a recommendation engine to

195
00:07:02,160 --> 00:07:04,400
emphasize that it's human agency all the

196
00:07:04,400 --> 00:07:05,039
way through

197
00:07:05,039 --> 00:07:07,120
in the use affordances as well as

198
00:07:07,120 --> 00:07:08,720
hopefully in what the designers

199
00:07:08,720 --> 00:07:10,479
considered

200
00:07:10,479 --> 00:07:13,039
another phrasing of ai that can be

201
00:07:13,039 --> 00:07:13,599
helpful

202
00:07:13,599 --> 00:07:16,400
and also center the role of the human in

203
00:07:16,400 --> 00:07:17,199
deciding

204
00:07:17,199 --> 00:07:19,039
how to use it and how to design it is

205
00:07:19,039 --> 00:07:20,560
intelligence augmentation which is

206
00:07:20,560 --> 00:07:22,400
clarifying that as us who are

207
00:07:22,400 --> 00:07:24,400
augmented or even a more general

208
00:07:24,400 --> 00:07:26,080
phrasing than ai would just be the human

209
00:07:26,080 --> 00:07:27,280
technological niche

210
00:07:27,280 --> 00:07:28,639
which would also include physical

211
00:07:28,639 --> 00:07:30,720
aspects of our niche for example what we

212
00:07:30,720 --> 00:07:32,400
talked about in 7.2

213
00:07:32,400 --> 00:07:34,240
but that would include effects that

214
00:07:34,240 --> 00:07:35,680
aren't um just

215
00:07:35,680 --> 00:07:37,759
simply in the sort of silicon chips and

216
00:07:37,759 --> 00:07:39,280
their interactions that ai

217
00:07:39,280 --> 00:07:42,960
captures all right machine learning

218
00:07:42,960 --> 00:07:46,000
so machine learning there's a lot of

219
00:07:46,000 --> 00:07:47,840
things to say about it it's a big area

220
00:07:47,840 --> 00:07:49,039
but the main

221
00:07:49,039 --> 00:07:51,440
machine learning topics that come into

222
00:07:51,440 --> 00:07:52,800
play in this paper

223
00:07:52,800 --> 00:07:55,199
are reinforcement learning and model

224
00:07:55,199 --> 00:07:57,199
based reinforcement learning

225
00:07:57,199 --> 00:08:01,039
so the simple reinforcement learner

226
00:08:01,039 --> 00:08:04,560
is an agent in an environment and the

227
00:08:04,560 --> 00:08:06,000
agent

228
00:08:06,000 --> 00:08:08,800
takes actions a that act on the

229
00:08:08,800 --> 00:08:09,520
environment

230
00:08:09,520 --> 00:08:12,240
and then the environment sends back

231
00:08:12,240 --> 00:08:14,000
states and rewards

232
00:08:14,000 --> 00:08:15,759
and so sometimes it sends back just

233
00:08:15,759 --> 00:08:17,199
states directly

234
00:08:17,199 --> 00:08:20,080
and then the reward is um understood

235
00:08:20,080 --> 00:08:21,039
through a layer

236
00:08:21,039 --> 00:08:23,280
of perception by the agent other times

237
00:08:23,280 --> 00:08:24,960
the environment directly sends reward

238
00:08:24,960 --> 00:08:27,599
back

239
00:08:27,599 --> 00:08:29,520
where model based reinforcement learning

240
00:08:29,520 --> 00:08:31,840
builds on that is by the introduction

241
00:08:31,840 --> 00:08:34,080
of this model which is outlined in red

242
00:08:34,080 --> 00:08:35,039
here

243
00:08:35,039 --> 00:08:38,320
and in these types of model based

244
00:08:38,320 --> 00:08:39,440
reinforcement learning

245
00:08:39,440 --> 00:08:41,599
rather than learning these the simple

246
00:08:41,599 --> 00:08:42,880
raw connection

247
00:08:42,880 --> 00:08:46,640
between successful behavior and

248
00:08:46,640 --> 00:08:49,440
reward and learning that as a behavioral

249
00:08:49,440 --> 00:08:50,640
correlation

250
00:08:50,640 --> 00:08:53,040
the model based reinforcement learner is

251
00:08:53,040 --> 00:08:54,800
able to have a model of

252
00:08:54,800 --> 00:08:57,279
oh for example running is painful now

253
00:08:57,279 --> 00:08:58,640
but then i'll feel better

254
00:08:58,640 --> 00:09:01,360
later and so then it allows one to

255
00:09:01,360 --> 00:09:03,600
pursue goals that are transiently

256
00:09:03,600 --> 00:09:06,640
uh unpleasant in order to get at

257
00:09:06,640 --> 00:09:08,800
larger term goals so think about in a

258
00:09:08,800 --> 00:09:10,560
little maze you know trying to get

259
00:09:10,560 --> 00:09:13,680
not just greedily towards the exit but

260
00:09:13,680 --> 00:09:15,040
trying to take a step back

261
00:09:15,040 --> 00:09:18,080
to take two steps forward so the

262
00:09:18,080 --> 00:09:20,160
similarities are that agents are acting

263
00:09:20,160 --> 00:09:22,000
on or within the environment and that

264
00:09:22,000 --> 00:09:24,000
their actions arise from policies which

265
00:09:24,000 --> 00:09:25,760
are models of action

266
00:09:25,760 --> 00:09:28,080
so what is and isn't likely and those

267
00:09:28,080 --> 00:09:29,200
can be

268
00:09:29,200 --> 00:09:32,000
um concurrent with what the organism

269
00:09:32,000 --> 00:09:33,120
actually can or can't do

270
00:09:33,120 --> 00:09:35,519
quote but at the very least it's what

271
00:09:35,519 --> 00:09:37,440
ends up getting implemented by this

272
00:09:37,440 --> 00:09:40,160
organism the agents modify their action

273
00:09:40,160 --> 00:09:41,440
policies based upon

274
00:09:41,440 --> 00:09:43,040
their reward from the environment either

275
00:09:43,040 --> 00:09:45,760
in a direct way or in a symbolic way

276
00:09:45,760 --> 00:09:48,800
or in a model based way they learn

277
00:09:48,800 --> 00:09:51,680
or to use the bayesian phrasing they

278
00:09:51,680 --> 00:09:52,800
update their model

279
00:09:52,800 --> 00:09:54,080
so in any case it's going to be

280
00:09:54,080 --> 00:09:55,600
computational because we're going to be

281
00:09:55,600 --> 00:09:57,279
using computers and

282
00:09:57,279 --> 00:10:00,080
math to describe it but then bayesians

283
00:10:00,080 --> 00:10:00,800
would say like

284
00:10:00,800 --> 00:10:03,440
the priors updated with evidence to

285
00:10:03,440 --> 00:10:05,279
generate the posterior

286
00:10:05,279 --> 00:10:06,560
and there's various ways that this

287
00:10:06,560 --> 00:10:08,320
learning or update can be done

288
00:10:08,320 --> 00:10:09,519
another similarity is that the

289
00:10:09,519 --> 00:10:11,519
environment sends rewards or signals to

290
00:10:11,519 --> 00:10:13,200
the agent

291
00:10:13,200 --> 00:10:16,399
and one area of debate or question to

292
00:10:16,399 --> 00:10:16,880
think about

293
00:10:16,880 --> 00:10:19,200
is is the observation the reward or how

294
00:10:19,200 --> 00:10:20,800
is it the reward

295
00:10:20,800 --> 00:10:22,320
is the learning itself the reward

296
00:10:22,320 --> 00:10:23,920
there's machine learning algorithms that

297
00:10:23,920 --> 00:10:25,920
have that kind of approach as well

298
00:10:25,920 --> 00:10:28,320
and then does the observation symbolize

299
00:10:28,320 --> 00:10:29,120
or signal

300
00:10:29,120 --> 00:10:30,959
the reward or how do we think about that

301
00:10:30,959 --> 00:10:32,399
those are kind of the ways that

302
00:10:32,399 --> 00:10:34,160
reinforcement learning and behavioral

303
00:10:34,160 --> 00:10:35,040
algorithms

304
00:10:35,040 --> 00:10:38,079
can be applied to both abstract data

305
00:10:38,079 --> 00:10:40,000
classification

306
00:10:40,000 --> 00:10:41,440
approaches clustering and things like

307
00:10:41,440 --> 00:10:43,839
that as well as direct plans for action

308
00:10:43,839 --> 00:10:45,279
and it turns out that active inference

309
00:10:45,279 --> 00:10:46,640
is going to relate to reinforcement

310
00:10:46,640 --> 00:10:48,240
learning an interesting way that's why

311
00:10:48,240 --> 00:10:50,560
the authors made it a keyword

312
00:10:50,560 --> 00:10:53,760
all right now in a modern context this

313
00:10:53,760 --> 00:10:55,200
reinforcement learning

314
00:10:55,200 --> 00:10:58,720
is going to be implemented with a

315
00:10:58,720 --> 00:11:00,720
for example neural network that in this

316
00:11:00,720 --> 00:11:02,720
case we have the environment

317
00:11:02,720 --> 00:11:05,839
sending the state to state s on the

318
00:11:05,839 --> 00:11:07,680
agent and that's going to go through

319
00:11:07,680 --> 00:11:10,000
some sort of deep neural network dnn and

320
00:11:10,000 --> 00:11:13,440
result in some policy selection

321
00:11:13,440 --> 00:11:15,200
so neural networks are inside the agent

322
00:11:15,200 --> 00:11:16,800
that's a way that we can implement it

323
00:11:16,800 --> 00:11:19,839
with basically software that's available

324
00:11:19,839 --> 00:11:21,440
and that allows for some types of

325
00:11:21,440 --> 00:11:24,320
scalable in the sense that we know how

326
00:11:24,320 --> 00:11:26,240
there's uh going to be a relationship

327
00:11:26,240 --> 00:11:28,240
between how many models our parameters

328
00:11:28,240 --> 00:11:31,360
have how many how big our model is and

329
00:11:31,360 --> 00:11:33,279
how effective it's going to be and then

330
00:11:33,279 --> 00:11:35,120
it also allows for non-linear

331
00:11:35,120 --> 00:11:38,160
inference not just a simple linear

332
00:11:38,160 --> 00:11:40,480
regressor let's say

333
00:11:40,480 --> 00:11:42,800
and sometimes the little and sometimes

334
00:11:42,800 --> 00:11:44,480
the big theta is used for parameters but

335
00:11:44,480 --> 00:11:46,000
we'll get to that later

336
00:11:46,000 --> 00:11:48,320
and the neural network can be model free

337
00:11:48,320 --> 00:11:49,519
so it can just be

338
00:11:49,519 --> 00:11:52,240
sort of let loose to find the patterns

339
00:11:52,240 --> 00:11:53,440
in the data

340
00:11:53,440 --> 00:11:56,880
or it can be using a underlying model

341
00:11:56,880 --> 00:11:58,720
so if a certain type of event is a

342
00:11:58,720 --> 00:12:00,399
priority can be much more likely

343
00:12:00,399 --> 00:12:02,480
those kinds of observations can be used

344
00:12:02,480 --> 00:12:06,480
to in a more weighted way

345
00:12:06,560 --> 00:12:09,760
one example of another

346
00:12:09,760 --> 00:12:11,680
modern reinforcement learning is this q

347
00:12:11,680 --> 00:12:13,519
learning which was used for some games

348
00:12:13,519 --> 00:12:15,040
and some types of things like that

349
00:12:15,040 --> 00:12:16,000
recently

350
00:12:16,000 --> 00:12:18,240
and the insight is just another way of

351
00:12:18,240 --> 00:12:19,120
doing it

352
00:12:19,120 --> 00:12:22,560
is by mapping the cue table

353
00:12:22,560 --> 00:12:25,360
which is the mapping of state action and

354
00:12:25,360 --> 00:12:26,399
reward

355
00:12:26,399 --> 00:12:29,040
so that would be mapping your current

356
00:12:29,040 --> 00:12:30,639
state and your behavior and that say

357
00:12:30,639 --> 00:12:32,000
like oh i'm tired but i'm going to keep

358
00:12:32,000 --> 00:12:33,680
running because it's rewarding

359
00:12:33,680 --> 00:12:36,959
let's say but in some mathematical way

360
00:12:36,959 --> 00:12:39,040
and then the deep q learning also

361
00:12:39,040 --> 00:12:40,240
represented here is

362
00:12:40,240 --> 00:12:42,320
when again it's not just a table but

363
00:12:42,320 --> 00:12:43,920
it's a neural network so kind of the

364
00:12:43,920 --> 00:12:46,240
modern inside just replace any other

365
00:12:46,240 --> 00:12:49,120
statistical module with a deep learning

366
00:12:49,120 --> 00:12:50,639
something

367
00:12:50,639 --> 00:12:52,959
and uh that allows for apparently some

368
00:12:52,959 --> 00:12:55,360
more nuanced policies to arise

369
00:12:55,360 --> 00:12:56,880
and then just one slide just to throw up

370
00:12:56,880 --> 00:12:58,480
there you can pause if you want to look

371
00:12:58,480 --> 00:13:00,000
at it but it's from a paper called deep

372
00:13:00,000 --> 00:13:01,120
reinforcement learning

373
00:13:01,120 --> 00:13:02,880
and it shows just the depth of how many

374
00:13:02,880 --> 00:13:04,959
different areas this is kind of related

375
00:13:04,959 --> 00:13:05,519
to

376
00:13:05,519 --> 00:13:07,360
so these are the kind of structure of

377
00:13:07,360 --> 00:13:09,360
the problems and the areas that

378
00:13:09,360 --> 00:13:12,160
this paper is getting at with these

379
00:13:12,160 --> 00:13:13,519
keywords

380
00:13:13,519 --> 00:13:16,560
okay so let's start from this model

381
00:13:16,560 --> 00:13:18,240
based reinforcement learning framework

382
00:13:18,240 --> 00:13:21,040
and then talk about systems and control

383
00:13:21,040 --> 00:13:24,480
so in this framing we can look at the

384
00:13:24,480 --> 00:13:26,560
model based reinforcement learning

385
00:13:26,560 --> 00:13:28,880
and the green green and the blue model

386
00:13:28,880 --> 00:13:30,639
free reinforcement learning so model

387
00:13:30,639 --> 00:13:31,200
free again

388
00:13:31,200 --> 00:13:32,880
is the direct mapping from experience

389
00:13:32,880 --> 00:13:34,560
onto policy

390
00:13:34,560 --> 00:13:36,959
and the increasingly abstract model

391
00:13:36,959 --> 00:13:38,720
based reinforcement learning is based

392
00:13:38,720 --> 00:13:40,240
upon the supervised learning of

393
00:13:40,240 --> 00:13:41,360
experience

394
00:13:41,360 --> 00:13:43,600
into updates for the transition model

395
00:13:43,600 --> 00:13:44,880
generative model of the world

396
00:13:44,880 --> 00:13:46,959
and then using that to implement some

397
00:13:46,959 --> 00:13:48,480
sort of planning process

398
00:13:48,480 --> 00:13:52,079
resulting in some policy selection

399
00:13:52,079 --> 00:13:54,880
so control systems theory is about

400
00:13:54,880 --> 00:13:57,279
planning for action amidst uncertainty

401
00:13:57,279 --> 00:13:58,880
because you have to plan for action and

402
00:13:58,880 --> 00:14:00,639
there's going to be uncertainty

403
00:14:00,639 --> 00:14:02,480
and there's various kinds of uncertainty

404
00:14:02,480 --> 00:14:04,480
sometimes they're metaphors for each

405
00:14:04,480 --> 00:14:05,839
other other times the methods are

406
00:14:05,839 --> 00:14:07,120
transferable

407
00:14:07,120 --> 00:14:09,360
other times one system will suffer from

408
00:14:09,360 --> 00:14:11,519
one type of these challenges

409
00:14:11,519 --> 00:14:13,839
versus another but it includes systems

410
00:14:13,839 --> 00:14:15,920
that are chaotic like a double pendulum

411
00:14:15,920 --> 00:14:18,000
stochastic multi-scale partially

412
00:14:18,000 --> 00:14:19,040
observed noisy

413
00:14:19,040 --> 00:14:22,160
etc so if you can't act you can't

414
00:14:22,160 --> 00:14:23,920
control you can only watch and that's

415
00:14:23,920 --> 00:14:25,279
really what differentiates

416
00:14:25,279 --> 00:14:28,720
control theory from just statistics

417
00:14:28,720 --> 00:14:30,880
because statistics is descriptive

418
00:14:30,880 --> 00:14:34,000
but systems and control approaches are

419
00:14:34,000 --> 00:14:35,519
more oriented towards

420
00:14:35,519 --> 00:14:37,600
understanding the dynamics of the

421
00:14:37,600 --> 00:14:39,279
systems in a way that can be intervened

422
00:14:39,279 --> 00:14:39,680
in

423
00:14:39,680 --> 00:14:41,519
so modeling this planning process and

424
00:14:41,519 --> 00:14:43,519
the policy explicitly

425
00:14:43,519 --> 00:14:46,720
and then also of course the caveat that

426
00:14:46,720 --> 00:14:48,800
not present in many of these models is

427
00:14:48,800 --> 00:14:50,320
the strategic

428
00:14:50,320 --> 00:14:53,680
axiom that non-action can be a form of

429
00:14:53,680 --> 00:14:55,040
action

430
00:14:55,040 --> 00:14:56,959
so the summary here is that we want to

431
00:14:56,959 --> 00:14:58,720
have a mathematical

432
00:14:58,720 --> 00:15:02,560
model a way to talk about how

433
00:15:02,560 --> 00:15:05,279
a system has to supervise or model their

434
00:15:05,279 --> 00:15:07,040
self their action affordances and their

435
00:15:07,040 --> 00:15:08,480
system and their world well

436
00:15:08,480 --> 00:15:11,040
that's the good regulator and also have

437
00:15:11,040 --> 00:15:11,680
this

438
00:15:11,680 --> 00:15:14,000
action policy plan so supervised

439
00:15:14,000 --> 00:15:14,880
learning

440
00:15:14,880 --> 00:15:17,760
is about supervising your input and

441
00:15:17,760 --> 00:15:19,279
appropriately updating your generative

442
00:15:19,279 --> 00:15:21,360
model of the world

443
00:15:21,360 --> 00:15:22,720
and then what is your generative model

444
00:15:22,720 --> 00:15:26,000
the world for it's gonna be about this

445
00:15:26,000 --> 00:15:29,040
policy selection and here's a more

446
00:15:29,040 --> 00:15:32,160
classic control theory diagram with the

447
00:15:32,160 --> 00:15:33,040
way that the

448
00:15:33,040 --> 00:15:35,199
system measuring element controller and

449
00:15:35,199 --> 00:15:37,279
effector are related

450
00:15:37,279 --> 00:15:40,480
all right information theory

451
00:15:40,480 --> 00:15:42,560
another keyword so there's probably

452
00:15:42,560 --> 00:15:43,600
another

453
00:15:43,600 --> 00:15:47,199
500 hours of youtube to watch on info

454
00:15:47,199 --> 00:15:49,040
theory and a lot you could say about a

455
00:15:49,040 --> 00:15:51,360
lot of different areas

456
00:15:51,360 --> 00:15:54,480
um a simple way to phrase it is that

457
00:15:54,480 --> 00:15:55,360
information

458
00:15:55,360 --> 00:15:57,360
is the reduction of uncertainty whether

459
00:15:57,360 --> 00:15:58,880
it's actionable or not

460
00:15:58,880 --> 00:16:00,160
so it doesn't have to be connected to

461
00:16:00,160 --> 00:16:02,560
control theory it can just be purely

462
00:16:02,560 --> 00:16:04,560
descriptive like taking the

463
00:16:04,560 --> 00:16:07,839
shannon entropy of some dna strand

464
00:16:07,839 --> 00:16:09,839
information is always contextual it's

465
00:16:09,839 --> 00:16:11,680
relational it's model dependent there's

466
00:16:11,680 --> 00:16:12,800
a lot of critiques of this

467
00:16:12,800 --> 00:16:16,079
sort of naive shannon interpretation

468
00:16:16,079 --> 00:16:18,800
of information theory sometimes and the

469
00:16:18,800 --> 00:16:20,480
measurement of or the comparison of

470
00:16:20,480 --> 00:16:22,560
information is challenging like if you

471
00:16:22,560 --> 00:16:24,240
calculate the entropy of the

472
00:16:24,240 --> 00:16:26,000
biodiversity of insects

473
00:16:26,000 --> 00:16:28,160
and plants on the same island they're

474
00:16:28,160 --> 00:16:29,680
not necessarily on the same scale et

475
00:16:29,680 --> 00:16:31,279
cetera et cetera

476
00:16:31,279 --> 00:16:34,320
and uh there's a lot of areas areas that

477
00:16:34,320 --> 00:16:36,959
info theory touches on like semiotics

478
00:16:36,959 --> 00:16:39,120
and semantics measure theory

479
00:16:39,120 --> 00:16:43,360
dynamical systems and signal processing

480
00:16:43,360 --> 00:16:45,519
reduction of uncertainty is also not

481
00:16:45,519 --> 00:16:46,800
truth because of course

482
00:16:46,800 --> 00:16:49,839
wrong and precise estimates do exist and

483
00:16:49,839 --> 00:16:51,600
precise estimates of external states

484
00:16:51,600 --> 00:16:52,959
don't really facilitate

485
00:16:52,959 --> 00:16:56,079
effective action in many cases a lot of

486
00:16:56,079 --> 00:16:58,160
other related areas are the

487
00:16:58,160 --> 00:16:59,519
quantification

488
00:16:59,519 --> 00:17:03,600
storage and communication of information

489
00:17:03,600 --> 00:17:05,280
the measurement comparison mentioned

490
00:17:05,280 --> 00:17:07,199
before how do you store

491
00:17:07,199 --> 00:17:08,959
and transmit information through time

492
00:17:08,959 --> 00:17:10,240
that's like memory

493
00:17:10,240 --> 00:17:12,400
how do you communicate that's about

494
00:17:12,400 --> 00:17:14,079
transmitting information through time

495
00:17:14,079 --> 00:17:14,720
and space

496
00:17:14,720 --> 00:17:19,199
protocols for communication

497
00:17:19,199 --> 00:17:23,679
and when information gleaned by a system

498
00:17:23,679 --> 00:17:25,839
reduces its uncertainty about world

499
00:17:25,839 --> 00:17:28,559
states or causes or relationships

500
00:17:28,559 --> 00:17:30,400
it can facilitate the updating of

501
00:17:30,400 --> 00:17:32,480
effective policy models and therefore

502
00:17:32,480 --> 00:17:32,960
action

503
00:17:32,960 --> 00:17:35,679
in some cases but not all cases and when

504
00:17:35,679 --> 00:17:37,039
the action is effective

505
00:17:37,039 --> 00:17:40,320
it can be rewarding or sustainable or

506
00:17:40,320 --> 00:17:42,000
high fitness there's various ways that

507
00:17:42,000 --> 00:17:43,760
you can think about that kind of a

508
00:17:43,760 --> 00:17:45,600
success depending on the mechanism

509
00:17:45,600 --> 00:17:48,000
scaled system

510
00:17:48,000 --> 00:17:50,480
and then the parameters are updated by

511
00:17:50,480 --> 00:17:52,000
this rewarding

512
00:17:52,000 --> 00:17:54,000
so whether it's which genotypes of the

513
00:17:54,000 --> 00:17:55,440
plant

514
00:17:55,440 --> 00:17:58,880
are successful in a certain niche or

515
00:17:58,880 --> 00:18:00,720
whether it's which computer virus is

516
00:18:00,720 --> 00:18:02,000
mutating the best

517
00:18:02,000 --> 00:18:03,679
it's this sort of learning slash

518
00:18:03,679 --> 00:18:05,919
development evolution process that

519
00:18:05,919 --> 00:18:07,120
ultimately we want to kind of move

520
00:18:07,120 --> 00:18:09,600
towards an integration of

521
00:18:09,600 --> 00:18:12,240
all right just to talk a little bit more

522
00:18:12,240 --> 00:18:13,919
about information theory and also

523
00:18:13,919 --> 00:18:16,160
introduced this conditional notation

524
00:18:16,160 --> 00:18:17,679
just for those who may or may not have

525
00:18:17,679 --> 00:18:19,280
seen it before

526
00:18:19,280 --> 00:18:21,039
why are we talking about information

527
00:18:21,039 --> 00:18:22,640
theory here because

528
00:18:22,640 --> 00:18:25,520
again when we reduce our uncertainty

529
00:18:25,520 --> 00:18:25,919
about

530
00:18:25,919 --> 00:18:27,919
causal relationships and the statistical

531
00:18:27,919 --> 00:18:29,520
regularities in the world

532
00:18:29,520 --> 00:18:32,080
we can enact better policies so let's

533
00:18:32,080 --> 00:18:33,120
get a grip on

534
00:18:33,120 --> 00:18:35,120
some basic information theory concepts

535
00:18:35,120 --> 00:18:36,720
but also introduce this notion of a

536
00:18:36,720 --> 00:18:37,600
conditional

537
00:18:37,600 --> 00:18:40,880
so the notation a vertical line b

538
00:18:40,880 --> 00:18:43,440
means a given b so the part after the

539
00:18:43,440 --> 00:18:44,559
vertical line

540
00:18:44,559 --> 00:18:47,840
is the part you're conditioning on h of

541
00:18:47,840 --> 00:18:48,400
a

542
00:18:48,400 --> 00:18:50,080
is the information content how

543
00:18:50,080 --> 00:18:51,919
surprising

544
00:18:51,919 --> 00:18:55,440
is the random variable a in units or in

545
00:18:55,440 --> 00:18:56,640
bits

546
00:18:56,640 --> 00:18:59,200
and just to kind of combine these two

547
00:18:59,200 --> 00:19:00,480
bullet points

548
00:19:00,480 --> 00:19:03,200
h of a given b is the information

549
00:19:03,200 --> 00:19:04,080
content of

550
00:19:04,080 --> 00:19:06,720
a conditioned on b sorry there's a typo

551
00:19:06,720 --> 00:19:08,880
on there

552
00:19:08,880 --> 00:19:12,320
information of of a semicolon

553
00:19:12,320 --> 00:19:14,400
b is the mutual information between a

554
00:19:14,400 --> 00:19:15,600
and b so that's

555
00:19:15,600 --> 00:19:17,919
shown in these sort of single overlap

556
00:19:17,919 --> 00:19:20,080
spaces on the venn diagram below

557
00:19:20,080 --> 00:19:23,200
and then also there's a triple overlap

558
00:19:23,200 --> 00:19:25,280
and you can also have overlap and

559
00:19:25,280 --> 00:19:26,320
conditionals

560
00:19:26,320 --> 00:19:28,160
so the way that we're going to think

561
00:19:28,160 --> 00:19:30,160
about mathematics is just sort of like

562
00:19:30,160 --> 00:19:30,720
nested

563
00:19:30,720 --> 00:19:34,000
operations that hold different kinds of

564
00:19:34,000 --> 00:19:35,840
transformations and potential

565
00:19:35,840 --> 00:19:37,679
and there's a lot of cool areas of

566
00:19:37,679 --> 00:19:39,120
information theory

567
00:19:39,120 --> 00:19:42,480
james gleek's a great writer and

568
00:19:42,480 --> 00:19:45,679
this book by stone in the middle is a

569
00:19:45,679 --> 00:19:49,120
good introduction and then this book

570
00:19:49,120 --> 00:19:50,320
buys a neil at all

571
00:19:50,320 --> 00:19:52,640
algorithmic information dynamics is also

572
00:19:52,640 --> 00:19:53,440
a cool

573
00:19:53,440 --> 00:19:56,960
application and theory so let's return

574
00:19:56,960 --> 00:19:58,720
to a little variant of a slide that

575
00:19:58,720 --> 00:20:00,640
we've looked at kind of before

576
00:20:00,640 --> 00:20:05,200
which is this inactive versus bayesian

577
00:20:05,200 --> 00:20:08,080
dichotomy divide that ultimately active

578
00:20:08,080 --> 00:20:09,840
is going to try to integrate

579
00:20:09,840 --> 00:20:11,919
so in the inactive worldview we have the

580
00:20:11,919 --> 00:20:14,159
agent in the world and they're enacting

581
00:20:14,159 --> 00:20:17,440
in this exit nexus of action and

582
00:20:17,440 --> 00:20:18,799
perception

583
00:20:18,799 --> 00:20:21,200
and the outcome of this interaction is

584
00:20:21,200 --> 00:20:23,679
the embodied and ecological action

585
00:20:23,679 --> 00:20:26,480
sequence or policy from an embedded

586
00:20:26,480 --> 00:20:27,280
agent who

587
00:20:27,280 --> 00:20:31,039
makes slash is their behavior and

588
00:20:31,039 --> 00:20:33,360
just to go one more level now from its

589
00:20:33,360 --> 00:20:35,039
control theory perspective

590
00:20:35,039 --> 00:20:38,240
into the agent we can see that the agent

591
00:20:38,240 --> 00:20:40,720
has the sensory states on the in

592
00:20:40,720 --> 00:20:43,280
input side then they have their causal

593
00:20:43,280 --> 00:20:44,480
model of the world

594
00:20:44,480 --> 00:20:46,960
and then their policy model in some

595
00:20:46,960 --> 00:20:48,480
sense these could be like

596
00:20:48,480 --> 00:20:50,480
one integrated unit or not we'll get

597
00:20:50,480 --> 00:20:52,640
there later

598
00:20:52,640 --> 00:20:56,080
and then we can line that up with the

599
00:20:56,080 --> 00:20:56,640
bayesian

600
00:20:56,640 --> 00:20:59,520
structural representationalist view so

601
00:20:59,520 --> 00:21:00,400
in this

602
00:21:00,400 --> 00:21:02,960
view you have the data the observation

603
00:21:02,960 --> 00:21:04,720
which are sort of the base level

604
00:21:04,720 --> 00:21:07,120
observations or parameters in the model

605
00:21:07,120 --> 00:21:09,679
and then it through a recognition model

606
00:21:09,679 --> 00:21:11,919
hyper parameters are updated which

607
00:21:11,919 --> 00:21:12,720
result in

608
00:21:12,720 --> 00:21:15,600
a generative model of the data and this

609
00:21:15,600 --> 00:21:16,240
kind of

610
00:21:16,240 --> 00:21:20,080
a expectation maximization scheme

611
00:21:20,080 --> 00:21:21,919
results in the statistical convergence

612
00:21:21,919 --> 00:21:23,360
of a multi-level model

613
00:21:23,360 --> 00:21:25,600
that's representing hence the

614
00:21:25,600 --> 00:21:26,799
representationalist

615
00:21:26,799 --> 00:21:29,039
structures of the world hence the

616
00:21:29,039 --> 00:21:30,320
structural part

617
00:21:30,320 --> 00:21:33,679
maybe and then just to line it up we can

618
00:21:33,679 --> 00:21:37,520
think about that model that's a bayesian

619
00:21:37,520 --> 00:21:40,720
control agent let's say

620
00:21:40,720 --> 00:21:42,720
it's going to have sensory states what

621
00:21:42,720 --> 00:21:43,919
is my

622
00:21:43,919 --> 00:21:46,559
robot sensor detecting what is the

623
00:21:46,559 --> 00:21:49,600
causal model for what that means for the

624
00:21:49,600 --> 00:21:51,679
pose of the robot and then the policy

625
00:21:51,679 --> 00:21:53,200
selection how are we going to

626
00:21:53,200 --> 00:21:58,000
correct so these uh both can be

627
00:21:58,000 --> 00:22:00,320
these two sides of the dashed line can

628
00:22:00,320 --> 00:22:01,919
kind of be

629
00:22:01,919 --> 00:22:05,200
internally consistent and we want to

630
00:22:05,200 --> 00:22:06,960
return to this question which is about

631
00:22:06,960 --> 00:22:07,600
how

632
00:22:07,600 --> 00:22:09,120
the free energy principle and active

633
00:22:09,120 --> 00:22:10,320
inference is going to say something

634
00:22:10,320 --> 00:22:10,880
about

635
00:22:10,880 --> 00:22:12,640
the relationship between the world and

636
00:22:12,640 --> 00:22:14,799
the agents

637
00:22:14,799 --> 00:22:16,960
so we have action and perception and we

638
00:22:16,960 --> 00:22:18,640
want them to be symmetrical in this kind

639
00:22:18,640 --> 00:22:19,760
of

640
00:22:19,760 --> 00:22:22,480
niche concept way we've talked about we

641
00:22:22,480 --> 00:22:23,120
want to have

642
00:22:23,120 --> 00:22:25,520
the free energy principle be an

643
00:22:25,520 --> 00:22:26,640
axiomatic

644
00:22:26,640 --> 00:22:29,360
set we want to build on top with a

645
00:22:29,360 --> 00:22:31,280
multi-scale active inference

646
00:22:31,280 --> 00:22:35,039
perspective this is sort of where we

647
00:22:35,039 --> 00:22:37,919
want to be moving towards how has active

648
00:22:37,919 --> 00:22:38,799
inference

649
00:22:38,799 --> 00:22:41,360
stepped into this gap and built up

650
00:22:41,360 --> 00:22:43,039
towards this goal

651
00:22:43,039 --> 00:22:46,480
so active inference 1.0 and

652
00:22:46,480 --> 00:22:49,120
open to people building on slash

653
00:22:49,120 --> 00:22:50,480
correcting me on any of these parts here

654
00:22:50,480 --> 00:22:51,440
this is just very

655
00:22:51,440 --> 00:22:55,360
rough epoch level active inference 1.0

656
00:22:55,360 --> 00:22:57,760
is a implementation of a machine

657
00:22:57,760 --> 00:23:00,240
learning type algorithm that builds on

658
00:23:00,240 --> 00:23:04,159
free energy principle as well as the

659
00:23:04,159 --> 00:23:06,960
inactivist approach and it relates the

660
00:23:06,960 --> 00:23:08,720
attunement of local sensory states and

661
00:23:08,720 --> 00:23:09,440
policies

662
00:23:09,440 --> 00:23:11,440
so in that way it's kind of like the

663
00:23:11,440 --> 00:23:12,480
model free

664
00:23:12,480 --> 00:23:15,360
reinforcement learner and it focused on

665
00:23:15,360 --> 00:23:16,960
very discrete cases

666
00:23:16,960 --> 00:23:20,080
so just like on a grid for example

667
00:23:20,080 --> 00:23:23,120
and it used pretty simple expectation

668
00:23:23,120 --> 00:23:25,200
maximization algorithms and

669
00:23:25,200 --> 00:23:27,440
had relatively short range spatial and

670
00:23:27,440 --> 00:23:28,240
or temporal

671
00:23:28,240 --> 00:23:31,440
reach or depth it was um more of a

672
00:23:31,440 --> 00:23:34,799
uh let me see if my

673
00:23:34,799 --> 00:23:38,559
video not sure what happened here

674
00:23:38,960 --> 00:23:41,200
um

675
00:23:42,799 --> 00:23:45,840
sorry about that

676
00:23:46,960 --> 00:23:50,159
so i'm gonna just

677
00:23:50,159 --> 00:23:53,520
re-add my camera

678
00:24:02,840 --> 00:24:05,840
sorry

679
00:24:07,360 --> 00:24:11,360
all right well c'est la vie

680
00:24:11,360 --> 00:24:15,039
anyways active 2.0 added

681
00:24:15,039 --> 00:24:17,760
temporal depth and niche interactions

682
00:24:17,760 --> 00:24:20,240
into the equations with a longer term

683
00:24:20,240 --> 00:24:23,440
planning phase and also added in this

684
00:24:23,440 --> 00:24:25,679
aspect of agent embeddedness and

685
00:24:25,679 --> 00:24:28,000
introduced the idea of valence and

686
00:24:28,000 --> 00:24:29,200
learning into

687
00:24:29,200 --> 00:24:32,159
the formulation and this is kind of

688
00:24:32,159 --> 00:24:34,159
where active inference 3.0

689
00:24:34,159 --> 00:24:37,600
and beyond go so one direction they've

690
00:24:37,600 --> 00:24:38,240
been going

691
00:24:38,240 --> 00:24:40,880
is towards learning and affect and the

692
00:24:40,880 --> 00:24:43,520
sophisticated or counterfactual aspects

693
00:24:43,520 --> 00:24:43,919
of

694
00:24:43,919 --> 00:24:46,960
action here's a citation

695
00:24:46,960 --> 00:24:49,039
and then another direction that it was

696
00:24:49,039 --> 00:24:50,080
going towards is

697
00:24:50,080 --> 00:24:52,080
reflected by this scaling active

698
00:24:52,080 --> 00:24:53,200
inference paper

699
00:24:53,200 --> 00:24:55,200
which is going to introduce the idea of

700
00:24:55,200 --> 00:24:56,400
the high dimensional data

701
00:24:56,400 --> 00:24:59,200
the continuous variables explore exploit

702
00:24:59,200 --> 00:25:00,000
behavior and

703
00:25:00,000 --> 00:25:02,559
also homology to reinforcement learning

704
00:25:02,559 --> 00:25:04,240
and working at scale

705
00:25:04,240 --> 00:25:07,919
so for me it was like the hero's journey

706
00:25:07,919 --> 00:25:10,000
reading this paper because this is just

707
00:25:10,000 --> 00:25:11,919
a framework for a completion

708
00:25:11,919 --> 00:25:13,919
story so this is the scaling active

709
00:25:13,919 --> 00:25:16,000
inference

710
00:25:16,000 --> 00:25:18,799
journey so it starts off with a call to

711
00:25:18,799 --> 00:25:19,760
adventure

712
00:25:19,760 --> 00:25:22,799
which is the question how can we model

713
00:25:22,799 --> 00:25:25,200
complex control tasks with active

714
00:25:25,200 --> 00:25:26,640
inference

715
00:25:26,640 --> 00:25:27,840
here's the beginning of the

716
00:25:27,840 --> 00:25:30,240
transformation the challenge is in the

717
00:25:30,240 --> 00:25:31,679
temptation

718
00:25:31,679 --> 00:25:34,240
should should one go to sleep or should

719
00:25:34,240 --> 00:25:35,120
they read

720
00:25:35,120 --> 00:25:38,320
fristen's bibliography

721
00:25:38,320 --> 00:25:40,960
the helper comes into the picture here's

722
00:25:40,960 --> 00:25:42,799
alec

723
00:25:42,799 --> 00:25:45,120
we have a transformative moment when we

724
00:25:45,120 --> 00:25:46,720
read and when we understand

725
00:25:46,720 --> 00:25:49,919
the paper scaling active inference then

726
00:25:49,919 --> 00:25:50,880
we have our

727
00:25:50,880 --> 00:25:52,880
movement from dark to light from

728
00:25:52,880 --> 00:25:54,640
reinforcement learning to active

729
00:25:54,640 --> 00:25:55,440
inference

730
00:25:55,440 --> 00:25:59,200
from failure to control along the way we

731
00:25:59,200 --> 00:26:02,000
receive the gift of meeting new friends

732
00:26:02,000 --> 00:26:04,159
discussing cool ideas

733
00:26:04,159 --> 00:26:06,880
having impact on systems and making

734
00:26:06,880 --> 00:26:07,840
first and memes

735
00:26:07,840 --> 00:26:11,440
of course so let's get to the paper

736
00:26:11,440 --> 00:26:14,240
so scaling active inference is the paper

737
00:26:14,240 --> 00:26:15,360
the goal of the paper

738
00:26:15,360 --> 00:26:18,400
is presented as we present a model

739
00:26:18,400 --> 00:26:20,400
of active inference that is applicable

740
00:26:20,400 --> 00:26:22,320
in high dimensional control tasks with

741
00:26:22,320 --> 00:26:24,799
both continuous states and actions

742
00:26:24,799 --> 00:26:27,200
our model builds upon previous attempts

743
00:26:27,200 --> 00:26:28,880
to scale active inference by including

744
00:26:28,880 --> 00:26:30,640
an efficient planning algorithm as well

745
00:26:30,640 --> 00:26:32,000
as the quantification and active

746
00:26:32,000 --> 00:26:34,159
resolution of model uncertainty

747
00:26:34,159 --> 00:26:35,440
our model makes two primary

748
00:26:35,440 --> 00:26:37,600
contributions first we showed

749
00:26:37,600 --> 00:26:39,360
that the full active inference construct

750
00:26:39,360 --> 00:26:41,279
can be scaled to the kinds of tasks

751
00:26:41,279 --> 00:26:43,440
considered in the rl literature

752
00:26:43,440 --> 00:26:45,279
this involved extending previous models

753
00:26:45,279 --> 00:26:46,960
of deep active inference to include

754
00:26:46,960 --> 00:26:49,600
model uncertainty and expected info gain

755
00:26:49,600 --> 00:26:51,600
second we highlight the overlap between

756
00:26:51,600 --> 00:26:53,120
active inference state-of-the-art

757
00:26:53,120 --> 00:26:53,840
approach

758
00:26:53,840 --> 00:26:56,880
to model-based reinforcement learning so

759
00:26:56,880 --> 00:26:58,960
the non-active inference phrasing here

760
00:26:58,960 --> 00:27:01,840
is how can we apply active inference to

761
00:27:01,840 --> 00:27:03,760
challenging control tasks

762
00:27:03,760 --> 00:27:05,520
and connect it formally to reinforcement

763
00:27:05,520 --> 00:27:08,480
learning so the abstract was

764
00:27:08,480 --> 00:27:11,760
in reinforcement learning agents often

765
00:27:11,760 --> 00:27:13,600
operate in partially observed and

766
00:27:13,600 --> 00:27:15,120
uncertain environments

767
00:27:15,120 --> 00:27:17,360
model based reinforcement learning

768
00:27:17,360 --> 00:27:19,520
suggests that this is best achieved

769
00:27:19,520 --> 00:27:21,440
by learning and exploiting a

770
00:27:21,440 --> 00:27:24,720
probabilistic model of the world

771
00:27:24,720 --> 00:27:26,240
active inference is an emerging

772
00:27:26,240 --> 00:27:28,080
normative framework in cognitive and

773
00:27:28,080 --> 00:27:29,760
computational neuroscience

774
00:27:29,760 --> 00:27:31,919
that offers a unifying account of how

775
00:27:31,919 --> 00:27:34,480
biological agents achieve this

776
00:27:34,480 --> 00:27:37,039
on this framework inference learning and

777
00:27:37,039 --> 00:27:37,600
action

778
00:27:37,600 --> 00:27:39,360
emerge from a single imperative to

779
00:27:39,360 --> 00:27:41,039
maximize the bayesian evidence for a

780
00:27:41,039 --> 00:27:43,919
shared niche of the world

781
00:27:43,919 --> 00:27:46,480
however implementations of this process

782
00:27:46,480 --> 00:27:48,080
have thus far been restricted to low

783
00:27:48,080 --> 00:27:50,559
dimensional and idealized situations

784
00:27:50,559 --> 00:27:53,600
here we present a working implementation

785
00:27:53,600 --> 00:27:55,679
of active inference that applies to high

786
00:27:55,679 --> 00:27:57,279
dimensional tasks with proof of

787
00:27:57,279 --> 00:27:58,320
principle results

788
00:27:58,320 --> 00:28:00,960
demonstrating efficient exploration and

789
00:28:00,960 --> 00:28:02,080
an order of magnitude

790
00:28:02,080 --> 00:28:04,320
decrease in sample efficiency over

791
00:28:04,320 --> 00:28:06,799
strong model-free baselines

792
00:28:06,799 --> 00:28:08,880
our results demonstrate the feasibility

793
00:28:08,880 --> 00:28:11,360
of applying active inference at scale

794
00:28:11,360 --> 00:28:13,360
and highlight the operational homologies

795
00:28:13,360 --> 00:28:14,640
between active inference

796
00:28:14,640 --> 00:28:16,640
and current model based approaches to

797
00:28:16,640 --> 00:28:18,240
reinforcement learning

798
00:28:18,240 --> 00:28:21,600
okay so for some of you

799
00:28:21,600 --> 00:28:25,039
that might be a lot of new ideas others

800
00:28:25,039 --> 00:28:28,320
may recognize a lot in there so

801
00:28:28,320 --> 00:28:30,480
let's just lay out the road map for how

802
00:28:30,480 --> 00:28:31,679
we're gonna get from

803
00:28:31,679 --> 00:28:34,960
a to z and then hopefully

804
00:28:34,960 --> 00:28:37,600
it will be approachable from all these

805
00:28:37,600 --> 00:28:38,240
different

806
00:28:38,240 --> 00:28:41,600
angles section one

807
00:28:41,600 --> 00:28:44,000
is an introduction and two is about

808
00:28:44,000 --> 00:28:45,279
active inference

809
00:28:45,279 --> 00:28:47,919
and what work has been done in the area

810
00:28:47,919 --> 00:28:48,399
three

811
00:28:48,399 --> 00:28:49,760
is the model which is what we're going

812
00:28:49,760 --> 00:28:51,440
to focus on in this

813
00:28:51,440 --> 00:28:55,279
discussion and 3.1 is the generative

814
00:28:55,279 --> 00:28:56,840
model and the recognition

815
00:28:56,840 --> 00:28:59,279
distribution those are the p and q

816
00:28:59,279 --> 00:29:00,880
models

817
00:29:00,880 --> 00:29:02,480
then there's a section on learning and

818
00:29:02,480 --> 00:29:04,240
inference policy selection

819
00:29:04,240 --> 00:29:06,080
trajectory sampling and expected free

820
00:29:06,080 --> 00:29:08,000
energy

821
00:29:08,000 --> 00:29:09,760
as well as a section on the fully

822
00:29:09,760 --> 00:29:11,600
observed model those are the sections we

823
00:29:11,600 --> 00:29:12,240
won't go

824
00:29:12,240 --> 00:29:15,840
that much into today then there are the

825
00:29:15,840 --> 00:29:17,039
experiments

826
00:29:17,039 --> 00:29:20,799
related to exploration and exploitation

827
00:29:20,799 --> 00:29:22,880
and the two figures related to the

828
00:29:22,880 --> 00:29:25,039
comparison of exploration strategies and

829
00:29:25,039 --> 00:29:26,880
the comparison of performance onto

830
00:29:26,880 --> 00:29:28,720
continuous control tasks

831
00:29:28,720 --> 00:29:31,200
then the relationship with previous work

832
00:29:31,200 --> 00:29:32,240
specifically deep

833
00:29:32,240 --> 00:29:33,760
active inference model based

834
00:29:33,760 --> 00:29:35,520
reinforcement learning and information

835
00:29:35,520 --> 00:29:35,919
gain

836
00:29:35,919 --> 00:29:38,840
theory then there's a discussion and

837
00:29:38,840 --> 00:29:40,559
conclusion

838
00:29:40,559 --> 00:29:42,879
all right

839
00:29:44,000 --> 00:29:47,360
this quote which you can pause

840
00:29:47,360 --> 00:29:51,440
to read in full i'm just going to pick

841
00:29:51,440 --> 00:29:52,799
up on the

842
00:29:52,799 --> 00:29:55,279
end of the top paragraph while this

843
00:29:55,279 --> 00:29:57,200
approach the previous work provides an

844
00:29:57,200 --> 00:29:59,279
elegant framework for evaluating

845
00:29:59,279 --> 00:30:01,039
expected free energy it can only be

846
00:30:01,039 --> 00:30:02,960
applied in discrete state and action

847
00:30:02,960 --> 00:30:03,919
spaces

848
00:30:03,919 --> 00:30:06,000
meaning it is not directly applicable to

849
00:30:06,000 --> 00:30:07,919
the high dimension states and continuous

850
00:30:07,919 --> 00:30:09,520
actions considered in reinforcement

851
00:30:09,520 --> 00:30:10,880
learning benchmarks

852
00:30:10,880 --> 00:30:12,720
so this is one reason why active

853
00:30:12,720 --> 00:30:13,919
inference discrete

854
00:30:13,919 --> 00:30:16,000
states based learners have not been

855
00:30:16,000 --> 00:30:17,120
compared

856
00:30:17,120 --> 00:30:20,320
in the author's phrasing is that

857
00:30:20,320 --> 00:30:22,240
the reinforcement learning benchmarks

858
00:30:22,240 --> 00:30:23,760
that are used to test

859
00:30:23,760 --> 00:30:25,600
the adequateness or the efficacy of

860
00:30:25,600 --> 00:30:27,840
different reinforcement learning

861
00:30:27,840 --> 00:30:31,360
control theory algorithms are

862
00:30:31,360 --> 00:30:33,279
are continuous tasks like the kind that

863
00:30:33,279 --> 00:30:35,039
we'll talk about in the figures

864
00:30:35,039 --> 00:30:37,679
and so what this paper is going to do is

865
00:30:37,679 --> 00:30:39,520
employ an approach called

866
00:30:39,520 --> 00:30:41,440
amortized inference which we'll talk

867
00:30:41,440 --> 00:30:44,080
about more in a little bit

868
00:30:44,080 --> 00:30:46,720
which utilizes functional approximators

869
00:30:46,720 --> 00:30:47,200
i.e

870
00:30:47,200 --> 00:30:48,960
neural networks to parameterize

871
00:30:48,960 --> 00:30:50,240
distributions

872
00:30:50,240 --> 00:30:51,919
free energy is then minimized with

873
00:30:51,919 --> 00:30:53,520
respect to the parameters of the

874
00:30:53,520 --> 00:30:55,039
function approximators not the

875
00:30:55,039 --> 00:30:57,120
variational parameters themselves

876
00:30:57,120 --> 00:30:59,200
and then they restate what each of the

877
00:30:59,200 --> 00:31:01,679
sections and three are

878
00:31:01,679 --> 00:31:04,240
here is a notation reference sheet it's

879
00:31:04,240 --> 00:31:06,159
not every single

880
00:31:06,159 --> 00:31:09,440
parameter especially the statistical

881
00:31:09,440 --> 00:31:12,720
parameters that are distributional and

882
00:31:12,720 --> 00:31:15,440
a lot of the later sections too but this

883
00:31:15,440 --> 00:31:15,760
is

884
00:31:15,760 --> 00:31:17,760
the sort of parameters that we're going

885
00:31:17,760 --> 00:31:18,960
to see a bunch

886
00:31:18,960 --> 00:31:22,960
in the coming slides and

887
00:31:22,960 --> 00:31:26,080
we can really

888
00:31:26,080 --> 00:31:28,799
just already look at what a lot of the

889
00:31:28,799 --> 00:31:30,480
topics that the model is going to cover

890
00:31:30,480 --> 00:31:33,120
are so for example the policy of the

891
00:31:33,120 --> 00:31:34,000
agent

892
00:31:34,000 --> 00:31:36,000
the states that it's trying to estimate

893
00:31:36,000 --> 00:31:37,600
and the true states with the hats it's

894
00:31:37,600 --> 00:31:38,799
going to be through time it's going to

895
00:31:38,799 --> 00:31:40,399
involve observations

896
00:31:40,399 --> 00:31:41,519
we're kind of seeing a lot of the

897
00:31:41,519 --> 00:31:44,159
ingredients of the reinforcement

898
00:31:44,159 --> 00:31:45,279
learning algorithms

899
00:31:45,279 --> 00:31:48,320
that we were talking about earlier so

900
00:31:48,320 --> 00:31:48,880
let's

901
00:31:48,880 --> 00:31:53,360
talk about this amortized inference

902
00:31:53,360 --> 00:31:55,360
here's what they write the author's

903
00:31:55,360 --> 00:31:56,480
chance at all

904
00:31:56,480 --> 00:31:58,240
amortizing the inference procedure

905
00:31:58,240 --> 00:31:59,760
offers several benefits

906
00:31:59,760 --> 00:32:01,600
for ex for instance the number of

907
00:32:01,600 --> 00:32:03,519
parameters remains constant with respect

908
00:32:03,519 --> 00:32:04,960
to the size

909
00:32:04,960 --> 00:32:06,320
of the data and inference can be

910
00:32:06,320 --> 00:32:08,080
achieved through a single forward pass

911
00:32:08,080 --> 00:32:09,840
of the network rather than let's say

912
00:32:09,840 --> 00:32:12,080
back propagation training

913
00:32:12,080 --> 00:32:14,240
moreover while the amount of information

914
00:32:14,240 --> 00:32:16,320
encoded about variables is fixed

915
00:32:16,320 --> 00:32:17,840
the conditional relationship between

916
00:32:17,840 --> 00:32:20,080
variables can be arbitrarily complex

917
00:32:20,080 --> 00:32:21,840
in equation three which we'll get to the

918
00:32:21,840 --> 00:32:23,200
parameters of the transition

919
00:32:23,200 --> 00:32:26,640
distribution data are themselves

920
00:32:26,640 --> 00:32:28,799
random variables in the current context

921
00:32:28,799 --> 00:32:30,159
these parameters are the weights of a

922
00:32:30,159 --> 00:32:32,159
neural network f

923
00:32:32,159 --> 00:32:34,000
this approach allows the uncertainty

924
00:32:34,000 --> 00:32:35,679
about these parameters to be quantified

925
00:32:35,679 --> 00:32:37,200
and cast learning as a process of

926
00:32:37,200 --> 00:32:38,559
variational inference

927
00:32:38,559 --> 00:32:40,240
the prior probability of theta is given

928
00:32:40,240 --> 00:32:41,919
by a standard gaussian which acts like a

929
00:32:41,919 --> 00:32:44,080
regularizer during learning

930
00:32:44,080 --> 00:32:47,120
um and this just talks about how free

931
00:32:47,120 --> 00:32:48,960
energy is going to be used to select the

932
00:32:48,960 --> 00:32:49,519
action

933
00:32:49,519 --> 00:32:52,240
policy so what is this amortized

934
00:32:52,240 --> 00:32:54,640
inference why is it being used

935
00:32:54,640 --> 00:32:56,320
these papers on the bottom have a lot

936
00:32:56,320 --> 00:32:58,480
more information but in amortized

937
00:32:58,480 --> 00:33:01,039
inference the number of parameters used

938
00:33:01,039 --> 00:33:03,039
is flexible you can have a small or a

939
00:33:03,039 --> 00:33:04,320
large model

940
00:33:04,320 --> 00:33:07,440
but what's good about it is that you can

941
00:33:07,440 --> 00:33:07,919
also

942
00:33:07,919 --> 00:33:11,200
set the size of the model to not change

943
00:33:11,200 --> 00:33:13,519
even as more data is input so this

944
00:33:13,519 --> 00:33:15,519
allows you to train on devices that you

945
00:33:15,519 --> 00:33:16,080
know are going to

946
00:33:16,080 --> 00:33:18,799
have limited computational power or use

947
00:33:18,799 --> 00:33:20,480
a very large but

948
00:33:20,480 --> 00:33:24,320
defined model size to implement on

949
00:33:24,320 --> 00:33:27,760
larger scale networks let's say

950
00:33:27,760 --> 00:33:29,120
again the model can be trained with a

951
00:33:29,120 --> 00:33:30,799
single forward pass rather than more

952
00:33:30,799 --> 00:33:32,000
complex games

953
00:33:32,000 --> 00:33:35,440
and then also there's all this degree of

954
00:33:35,440 --> 00:33:37,279
freedom when you can design the neural

955
00:33:37,279 --> 00:33:38,559
network because there's a lot of

956
00:33:38,559 --> 00:33:40,320
approaches that can be implemented with

957
00:33:40,320 --> 00:33:42,799
neural networks

958
00:33:42,799 --> 00:33:45,519
the general framing that they're going

959
00:33:45,519 --> 00:33:49,279
to be thinking about for this whole

960
00:33:49,519 --> 00:33:52,000
mathematical journey that we're about to

961
00:33:52,000 --> 00:33:52,799
describe

962
00:33:52,799 --> 00:33:55,919
is a partially observed markov decision

963
00:33:55,919 --> 00:33:59,279
process so these are the key variables

964
00:33:59,279 --> 00:34:01,399
that are going to be defining this

965
00:34:01,399 --> 00:34:03,840
organism-centric approach let's say

966
00:34:03,840 --> 00:34:05,600
at each time step t so that one's easy

967
00:34:05,600 --> 00:34:07,360
to remember the true state of the

968
00:34:07,360 --> 00:34:08,480
environment which is

969
00:34:08,480 --> 00:34:10,480
s with a hat hat means that it's the

970
00:34:10,480 --> 00:34:11,760
real state and

971
00:34:11,760 --> 00:34:13,359
the states are what are the important

972
00:34:13,359 --> 00:34:14,879
things those are

973
00:34:14,879 --> 00:34:17,119
like is the bus gonna hit me or not

974
00:34:17,119 --> 00:34:18,560
gonna hit me let's say

975
00:34:18,560 --> 00:34:20,320
but they could be various other things

976
00:34:20,320 --> 00:34:21,918
so we're keeping it general

977
00:34:21,918 --> 00:34:24,800
and then that s hat sub t so the real

978
00:34:24,800 --> 00:34:26,079
states through time

979
00:34:26,079 --> 00:34:28,239
exists in a state space with a certain

980
00:34:28,239 --> 00:34:29,760
dimensionality

981
00:34:29,760 --> 00:34:33,280
and there's also a real transition

982
00:34:33,280 --> 00:34:35,760
dynamics of the real states that's

983
00:34:35,760 --> 00:34:37,119
related to

984
00:34:37,119 --> 00:34:40,480
um this p s with a hat t given

985
00:34:40,480 --> 00:34:42,719
the previous times and the action so

986
00:34:42,719 --> 00:34:44,079
that's saying that

987
00:34:44,079 --> 00:34:46,560
the time evolution of the system the

988
00:34:46,560 --> 00:34:48,719
real one with a hat s of t

989
00:34:48,719 --> 00:34:50,399
is related let me see if i can actually

990
00:34:50,399 --> 00:34:53,598
get the laser

991
00:34:54,000 --> 00:34:57,440
yep the s of t with a hat

992
00:34:57,440 --> 00:35:00,560
is related to the current time

993
00:35:00,560 --> 00:35:02,800
and the previous time and the actions

994
00:35:02,800 --> 00:35:05,839
and that has also a dimensionality

995
00:35:05,839 --> 00:35:07,839
and agents do not have access to the

996
00:35:07,839 --> 00:35:09,359
true state of the environment but

997
00:35:09,359 --> 00:35:11,920
might instead receive observations so oh

998
00:35:11,920 --> 00:35:12,800
observations

999
00:35:12,800 --> 00:35:14,880
it's a good easy one and those have a

1000
00:35:14,880 --> 00:35:16,240
certain dimensionality which are

1001
00:35:16,240 --> 00:35:17,520
generated to an actual

1002
00:35:17,520 --> 00:35:20,160
observation distribution t which is

1003
00:35:20,160 --> 00:35:20,960
related to

1004
00:35:20,960 --> 00:35:23,440
how observations are conditioned on real

1005
00:35:23,440 --> 00:35:24,720
states of the world so if it's really

1006
00:35:24,720 --> 00:35:25,920
sunny outside

1007
00:35:25,920 --> 00:35:27,520
then you're really going to get photons

1008
00:35:27,520 --> 00:35:29,440
if you look at the sun

1009
00:35:29,440 --> 00:35:32,800
as such um again this is because agents

1010
00:35:32,800 --> 00:35:34,320
only get observations and not the

1011
00:35:34,320 --> 00:35:35,920
absolute truth from the world

1012
00:35:35,920 --> 00:35:37,680
agents must operate on their beliefs

1013
00:35:37,680 --> 00:35:38,960
about states so that's

1014
00:35:38,960 --> 00:35:42,079
s without a hat through time those have

1015
00:35:42,079 --> 00:35:43,440
a certain dimensionality

1016
00:35:43,440 --> 00:35:45,599
about the true environmental states as

1017
00:35:45,599 --> 00:35:46,800
with a hat

1018
00:35:46,800 --> 00:35:49,280
and then there's a difference between

1019
00:35:49,280 --> 00:35:51,280
the true dynamics without

1020
00:35:51,280 --> 00:35:53,920
italics and the model of the dynamics

1021
00:35:53,920 --> 00:35:56,720
which are always with the italics

1022
00:35:56,720 --> 00:35:58,560
and here's how those equations are

1023
00:35:58,560 --> 00:36:00,480
linked here's where active inference

1024
00:36:00,480 --> 00:36:01,839
comes into play

1025
00:36:01,839 --> 00:36:04,400
active inference proposes that agents

1026
00:36:04,400 --> 00:36:06,880
implement an update a generative model

1027
00:36:06,880 --> 00:36:10,320
of their world so italics

1028
00:36:10,320 --> 00:36:12,640
p that means the model of the world of

1029
00:36:12,640 --> 00:36:14,079
observations

1030
00:36:14,079 --> 00:36:17,200
states policies and parameters of the

1031
00:36:17,200 --> 00:36:18,000
world

1032
00:36:18,000 --> 00:36:20,000
where the tilde is the sequence of

1033
00:36:20,000 --> 00:36:21,280
variables through time

1034
00:36:21,280 --> 00:36:23,040
so those are the observations and states

1035
00:36:23,040 --> 00:36:25,119
through time and pi is the policy

1036
00:36:25,119 --> 00:36:26,880
like if i have the policy of running

1037
00:36:26,880 --> 00:36:28,960
every day then my observations through

1038
00:36:28,960 --> 00:36:30,560
time are going to look like this

1039
00:36:30,560 --> 00:36:34,240
you can imagine that one and theta

1040
00:36:34,240 --> 00:36:38,240
um with italics is belonging to a larger

1041
00:36:38,240 --> 00:36:40,640
version of theta capital theta and that

1042
00:36:40,640 --> 00:36:42,079
denotes parameters of the generative

1043
00:36:42,079 --> 00:36:43,520
model which are themselves random

1044
00:36:43,520 --> 00:36:44,560
variables

1045
00:36:44,560 --> 00:36:46,800
and also agents maintain a recognition

1046
00:36:46,800 --> 00:36:48,560
distribution this is like p and q

1047
00:36:48,560 --> 00:36:51,760
so mind the p's and q's of the states

1048
00:36:51,760 --> 00:36:52,560
through time

1049
00:36:52,560 --> 00:36:55,920
and the policies and the theta

1050
00:36:55,920 --> 00:36:58,400
parameters of the world representing the

1051
00:36:58,400 --> 00:36:59,359
agent's

1052
00:36:59,359 --> 00:37:02,000
beliefs over their states policies and

1053
00:37:02,000 --> 00:37:04,800
model parameters

1054
00:37:04,960 --> 00:37:08,000
all right so in this

1055
00:37:08,000 --> 00:37:10,880
part we're going to look at the p model

1056
00:37:10,880 --> 00:37:13,200
remember it's the ps and the qs

1057
00:37:13,200 --> 00:37:17,359
so the p is a agent implementation

1058
00:37:17,359 --> 00:37:19,359
that's why it's in italics

1059
00:37:19,359 --> 00:37:22,640
of a model of the observations through

1060
00:37:22,640 --> 00:37:23,200
time

1061
00:37:23,200 --> 00:37:25,280
o with the tilde states through time s

1062
00:37:25,280 --> 00:37:26,320
with a tilde

1063
00:37:26,320 --> 00:37:29,920
policy and model parameters so that's

1064
00:37:29,920 --> 00:37:30,400
what

1065
00:37:30,400 --> 00:37:33,520
this model is about

1066
00:37:33,520 --> 00:37:35,280
and it's implemented by the agent that

1067
00:37:35,280 --> 00:37:37,280
we care about

1068
00:37:37,280 --> 00:37:39,839
here's what each of these lines says so

1069
00:37:39,839 --> 00:37:40,640
the first line

1070
00:37:40,640 --> 00:37:43,040
on the left side of the equation says p

1071
00:37:43,040 --> 00:37:43,839
which is

1072
00:37:43,839 --> 00:37:46,400
that function that we have in big on the

1073
00:37:46,400 --> 00:37:47,680
top

1074
00:37:47,680 --> 00:37:51,440
is equivalent to the

1075
00:37:51,440 --> 00:37:54,640
probability model of the

1076
00:37:54,640 --> 00:37:57,760
parameters theta take those out

1077
00:37:57,760 --> 00:38:00,000
then the probability of the policy is so

1078
00:38:00,000 --> 00:38:01,599
if it's not possible for me to jump

1079
00:38:01,599 --> 00:38:03,440
a thousand feet then it's not going to

1080
00:38:03,440 --> 00:38:05,680
be part of my probability distribution

1081
00:38:05,680 --> 00:38:07,359
if it is part of my power to jump a

1082
00:38:07,359 --> 00:38:09,280
thousand feet then i'll consider it in

1083
00:38:09,280 --> 00:38:09,839
my

1084
00:38:09,839 --> 00:38:13,280
policy estimations and then the big

1085
00:38:13,280 --> 00:38:16,079
pi it's kind of like how sigma adds

1086
00:38:16,079 --> 00:38:16,400
stuff

1087
00:38:16,400 --> 00:38:20,000
up pi multiplies stuff up and so this is

1088
00:38:20,000 --> 00:38:20,400
saying

1089
00:38:20,400 --> 00:38:22,320
multiply it all out through time

1090
00:38:22,320 --> 00:38:23,680
starting from the beginning t

1091
00:38:23,680 --> 00:38:26,960
equals one and look at the p

1092
00:38:26,960 --> 00:38:30,320
the agent model of o

1093
00:38:30,320 --> 00:38:31,760
through time observations through time

1094
00:38:31,760 --> 00:38:33,760
given states through time so the first

1095
00:38:33,760 --> 00:38:35,359
part is just

1096
00:38:35,359 --> 00:38:38,720
what's the probability of given that i'm

1097
00:38:38,720 --> 00:38:41,760
um healthy that i'm seeing this

1098
00:38:41,760 --> 00:38:44,800
state and then that's multiplied

1099
00:38:44,800 --> 00:38:48,160
at each time point by the

1100
00:38:48,160 --> 00:38:52,320
ways in which states s t are

1101
00:38:52,320 --> 00:38:55,520
conditioned on the prior states and the

1102
00:38:55,520 --> 00:38:56,480
prior policies

1103
00:38:56,480 --> 00:38:59,359
and the parameters of the model so how

1104
00:38:59,359 --> 00:39:00,000
all those things

1105
00:39:00,000 --> 00:39:02,880
are linked multiplied out through time

1106
00:39:02,880 --> 00:39:03,440
is

1107
00:39:03,440 --> 00:39:06,880
what at a first pass describes this big

1108
00:39:06,880 --> 00:39:08,880
equation on the top the total model of

1109
00:39:08,880 --> 00:39:10,079
os pi

1110
00:39:10,079 --> 00:39:13,280
theta then

1111
00:39:13,280 --> 00:39:16,320
to dive into this p of observations

1112
00:39:16,320 --> 00:39:17,200
given states

1113
00:39:17,200 --> 00:39:18,960
through time model that the agent

1114
00:39:18,960 --> 00:39:20,240
implements n

1115
00:39:20,240 --> 00:39:22,880
big n notation means normal and so this

1116
00:39:22,880 --> 00:39:23,359
is saying

1117
00:39:23,359 --> 00:39:25,599
the model of observations given the

1118
00:39:25,599 --> 00:39:26,560
state estimates

1119
00:39:26,560 --> 00:39:30,240
is a normal distribution again

1120
00:39:30,240 --> 00:39:33,200
with observations through time with a

1121
00:39:33,200 --> 00:39:34,000
certain

1122
00:39:34,000 --> 00:39:37,040
mean and variance mu and sigma with a

1123
00:39:37,040 --> 00:39:39,920
certain parameter this lambda and

1124
00:39:39,920 --> 00:39:40,880
together

1125
00:39:40,880 --> 00:39:44,160
these two this kind of like dupl

1126
00:39:44,160 --> 00:39:47,440
tuple of variables

1127
00:39:47,440 --> 00:39:51,359
constitute this function lambda

1128
00:39:51,359 --> 00:39:53,599
so the way that we're going to estimate

1129
00:39:53,599 --> 00:39:54,640
these mean

1130
00:39:54,640 --> 00:39:56,960
variants may or may not be a least

1131
00:39:56,960 --> 00:39:57,920
square it might be

1132
00:39:57,920 --> 00:40:01,040
not not an l2 norm we could

1133
00:40:01,040 --> 00:40:02,880
use a functional approach here that's

1134
00:40:02,880 --> 00:40:06,720
the insight of amortization

1135
00:40:07,119 --> 00:40:09,920
the probability of states that

1136
00:40:09,920 --> 00:40:10,880
probability

1137
00:40:10,880 --> 00:40:12,800
transition model of how the agent

1138
00:40:12,800 --> 00:40:14,480
believes states through time

1139
00:40:14,480 --> 00:40:17,520
to be linked to conditional two um

1140
00:40:17,520 --> 00:40:19,760
states at prior time and policy at prior

1141
00:40:19,760 --> 00:40:20,880
time and then the parameters of the

1142
00:40:20,880 --> 00:40:21,839
world

1143
00:40:21,839 --> 00:40:24,000
is related to another normal

1144
00:40:24,000 --> 00:40:25,280
distribution

1145
00:40:25,280 --> 00:40:27,520
of states through time with another mu

1146
00:40:27,520 --> 00:40:29,839
and sigma with a different

1147
00:40:29,839 --> 00:40:33,200
subscript and for

1148
00:40:33,200 --> 00:40:36,640
those mu and sigma we have f um

1149
00:40:36,640 --> 00:40:40,240
with the sub theta of how the uh

1150
00:40:40,240 --> 00:40:43,760
states and the policies are related

1151
00:40:43,760 --> 00:40:47,119
so this is another just sort of way to

1152
00:40:47,119 --> 00:40:48,000
estimate

1153
00:40:48,000 --> 00:40:50,640
the variance of different kinds of

1154
00:40:50,640 --> 00:40:51,359
events

1155
00:40:51,359 --> 00:40:53,280
happening therefore weight sensory

1156
00:40:53,280 --> 00:40:54,400
inputs

1157
00:40:54,400 --> 00:40:56,240
together the state estimations

1158
00:40:56,240 --> 00:40:57,680
distribution

1159
00:40:57,680 --> 00:41:01,839
are again yeah estimated by a function

1160
00:41:01,839 --> 00:41:06,480
p of theta is given

1161
00:41:06,480 --> 00:41:09,680
as a normal distribution with a mean of

1162
00:41:09,680 --> 00:41:11,200
zero and this

1163
00:41:11,200 --> 00:41:14,480
identity matrix of covariance i believe

1164
00:41:14,480 --> 00:41:17,839
and uh i think this is just reflecting

1165
00:41:17,839 --> 00:41:20,720
a neutral initiating approach or

1166
00:41:20,720 --> 00:41:21,920
regularization but

1167
00:41:21,920 --> 00:41:25,920
i i'm this is not 100 sure about that

1168
00:41:25,920 --> 00:41:28,400
and this is reflecting that the pr the

1169
00:41:28,400 --> 00:41:29,839
probability model of

1170
00:41:29,839 --> 00:41:33,119
policies is related to this

1171
00:41:33,119 --> 00:41:35,520
max function so it's a sigma but it's

1172
00:41:35,520 --> 00:41:36,319
not the same

1173
00:41:36,319 --> 00:41:39,359
as sigma squared in a statistical model

1174
00:41:39,359 --> 00:41:42,079
and it's the soft max function of the

1175
00:41:42,079 --> 00:41:42,720
negative

1176
00:41:42,720 --> 00:41:46,160
expected free energy of the policy

1177
00:41:46,160 --> 00:41:48,960
which we'll talk about in a later

1178
00:41:48,960 --> 00:41:50,800
section

1179
00:41:50,800 --> 00:41:54,800
so that was our p let's look at q

1180
00:41:54,800 --> 00:41:57,839
q is this agent recognition distribution

1181
00:41:57,839 --> 00:41:59,040
the q with the italic

1182
00:41:59,040 --> 00:42:01,359
of states through time policy and model

1183
00:42:01,359 --> 00:42:03,359
parameters

1184
00:42:03,359 --> 00:42:06,720
here's how that one is defined

1185
00:42:06,720 --> 00:42:08,720
so q is the probability of the

1186
00:42:08,720 --> 00:42:10,240
parameters theta

1187
00:42:10,240 --> 00:42:11,920
multiplied by the probability of the

1188
00:42:11,920 --> 00:42:14,240
policies pi and then that same

1189
00:42:14,240 --> 00:42:17,359
multiplication through time for all time

1190
00:42:17,359 --> 00:42:20,480
of q which is the model of states given

1191
00:42:20,480 --> 00:42:22,880
observations

1192
00:42:22,880 --> 00:42:26,480
so previously

1193
00:42:26,640 --> 00:42:30,319
we had observed p-o-s the second line

1194
00:42:30,319 --> 00:42:30,800
here

1195
00:42:30,800 --> 00:42:34,880
probability of observations given states

1196
00:42:38,240 --> 00:42:41,359
now we have um q s o

1197
00:42:41,359 --> 00:42:44,880
so this is kind of going the other way

1198
00:42:44,880 --> 00:42:48,000
two of theta

1199
00:42:48,240 --> 00:42:50,720
is a normal distribution kind of as we

1200
00:42:50,720 --> 00:42:51,599
saw before

1201
00:42:51,599 --> 00:42:54,079
with its mean and variance parameters

1202
00:42:54,079 --> 00:42:56,319
same as the policy

1203
00:42:56,319 --> 00:42:59,520
and same as the uh the q

1204
00:42:59,520 --> 00:43:01,280
model of the states given the

1205
00:43:01,280 --> 00:43:04,000
observations

1206
00:43:04,400 --> 00:43:08,720
and it turns out that those mu and sigma

1207
00:43:08,720 --> 00:43:10,960
are gonna also have a functional that's

1208
00:43:10,960 --> 00:43:14,160
going to approximate them

1209
00:43:14,720 --> 00:43:16,960
so to make active inference applicable

1210
00:43:16,960 --> 00:43:18,240
to the kinds of tasks that are

1211
00:43:18,240 --> 00:43:19,839
considered in reinforcement learning

1212
00:43:19,839 --> 00:43:22,400
the authors treated reward signals

1213
00:43:22,400 --> 00:43:23,200
observations

1214
00:43:23,200 --> 00:43:25,200
as observations in a separate modality

1215
00:43:25,200 --> 00:43:27,520
so that extends the generative model to

1216
00:43:27,520 --> 00:43:28,160
include

1217
00:43:28,160 --> 00:43:31,200
a additional scalar gaussian so that's

1218
00:43:31,200 --> 00:43:32,800
another

1219
00:43:32,800 --> 00:43:34,800
reward value of scalars like a single

1220
00:43:34,800 --> 00:43:36,960
number over reward observations with a

1221
00:43:36,960 --> 00:43:38,800
unit variance in mean

1222
00:43:38,800 --> 00:43:41,119
and that allows them to wrap this neural

1223
00:43:41,119 --> 00:43:42,400
network

1224
00:43:42,400 --> 00:43:45,119
which is uh f sub alpha of the states

1225
00:43:45,119 --> 00:43:46,480
which is like this reward

1226
00:43:46,480 --> 00:43:47,920
fully connected neural network with the

1227
00:43:47,920 --> 00:43:49,119
parameters that they're then going to

1228
00:43:49,119 --> 00:43:50,079
train

1229
00:43:50,079 --> 00:43:52,160
so that's really where the formal

1230
00:43:52,160 --> 00:43:54,640
homology arises from the model based

1231
00:43:54,640 --> 00:43:56,079
reinforcement learning

1232
00:43:56,079 --> 00:43:58,720
is that now we have pretty much the same

1233
00:43:58,720 --> 00:44:01,440
architecture that was specified earlier

1234
00:44:01,440 --> 00:44:05,599
in this discussion where the agent state

1235
00:44:05,599 --> 00:44:09,040
is then of sensory input is run through

1236
00:44:09,040 --> 00:44:10,000
this dnn

1237
00:44:10,000 --> 00:44:12,319
and that results in this output of the

1238
00:44:12,319 --> 00:44:13,599
policy

1239
00:44:13,599 --> 00:44:16,560
or the reward but it's a little bit

1240
00:44:16,560 --> 00:44:17,839
different but that's

1241
00:44:17,839 --> 00:44:20,960
how we see a lot of the same homologies

1242
00:44:20,960 --> 00:44:24,319
all right so what do the

1243
00:44:24,319 --> 00:44:27,040
learners do as new observations are

1244
00:44:27,040 --> 00:44:27,839
sampled

1245
00:44:27,839 --> 00:44:29,520
agents update the parameters of the

1246
00:44:29,520 --> 00:44:31,200
recognition distribution

1247
00:44:31,200 --> 00:44:34,480
to minimize variational free energy or f

1248
00:44:34,480 --> 00:44:38,160
so the f of the observation is

1249
00:44:38,160 --> 00:44:42,319
the expectation um okay well

1250
00:44:42,319 --> 00:44:44,480
let me finish reading first this makes

1251
00:44:44,480 --> 00:44:45,440
the

1252
00:44:45,440 --> 00:44:47,520
what this f is going to do is make the

1253
00:44:47,520 --> 00:44:48,800
recognition distribution

1254
00:44:48,800 --> 00:44:51,440
q converge towards an approximation of

1255
00:44:51,440 --> 00:44:53,920
the intractable posterior distribution

1256
00:44:53,920 --> 00:44:56,720
p thereby implementing a tractable form

1257
00:44:56,720 --> 00:44:58,800
of approximate bayesian inference

1258
00:44:58,800 --> 00:45:02,079
alright so f of

1259
00:45:02,079 --> 00:45:04,240
o through time with a tilde is the

1260
00:45:04,240 --> 00:45:05,119
variational

1261
00:45:05,119 --> 00:45:07,040
free energy of the observations through

1262
00:45:07,040 --> 00:45:08,319
time

1263
00:45:08,319 --> 00:45:12,000
so what is this magical free energy

1264
00:45:12,000 --> 00:45:13,920
not magical it's just this equation in

1265
00:45:13,920 --> 00:45:15,200
this case

1266
00:45:15,200 --> 00:45:18,000
it is an expectation that is conditioned

1267
00:45:18,000 --> 00:45:19,839
on a specific recognition model

1268
00:45:19,839 --> 00:45:22,720
which is this expectation of q that's

1269
00:45:22,720 --> 00:45:23,280
going to be

1270
00:45:23,280 --> 00:45:26,960
related to this um s

1271
00:45:26,960 --> 00:45:30,160
pi policy and theta parameters

1272
00:45:30,160 --> 00:45:34,560
uh variables what is this expectation

1273
00:45:34,560 --> 00:45:35,760
gonna be of

1274
00:45:35,760 --> 00:45:37,520
that's everything that's about to be in

1275
00:45:37,520 --> 00:45:38,960
square brackets

1276
00:45:38,960 --> 00:45:41,040
this expectation is going to be the

1277
00:45:41,040 --> 00:45:43,119
natural log ln

1278
00:45:43,119 --> 00:45:46,319
which is a way to um turn a

1279
00:45:46,319 --> 00:45:48,640
question about maximizing the goodness

1280
00:45:48,640 --> 00:45:50,000
of a model's fit

1281
00:45:50,000 --> 00:45:52,319
into this minimizing the negative

1282
00:45:52,319 --> 00:45:53,440
sometimes there's a few

1283
00:45:53,440 --> 00:45:55,920
ways that the natural law can help the

1284
00:45:55,920 --> 00:45:56,720
expectation

1285
00:45:56,720 --> 00:45:58,880
is going to be the natural log of the

1286
00:45:58,880 --> 00:46:00,880
recognition model

1287
00:46:00,880 --> 00:46:03,599
minus the natural log of the generative

1288
00:46:03,599 --> 00:46:04,240
model

1289
00:46:04,240 --> 00:46:06,720
so again that's the part that we cue

1290
00:46:06,720 --> 00:46:07,760
that we can learn on

1291
00:46:07,760 --> 00:46:09,280
and then the part p that is

1292
00:46:09,280 --> 00:46:10,880
characterized as intractable

1293
00:46:10,880 --> 00:46:13,359
it turns out that this expectation is

1294
00:46:13,359 --> 00:46:15,440
always bigger than or equal to the

1295
00:46:15,440 --> 00:46:18,720
natural log of p of the observations

1296
00:46:18,720 --> 00:46:21,599
so that's the just the p model of

1297
00:46:21,599 --> 00:46:24,000
observations likelihood

1298
00:46:24,000 --> 00:46:26,079
by minimizing the free energy of the

1299
00:46:26,079 --> 00:46:27,760
observations through time

1300
00:46:27,760 --> 00:46:29,839
the agent converges heuristically

1301
00:46:29,839 --> 00:46:31,760
towards this intractable p

1302
00:46:31,760 --> 00:46:34,960
distribution given that

1303
00:46:34,960 --> 00:46:38,079
q is only over the state's

1304
00:46:38,079 --> 00:46:40,960
policies and parameters and that p is

1305
00:46:40,960 --> 00:46:42,640
really including and focusing on the

1306
00:46:42,640 --> 00:46:44,079
observations

1307
00:46:44,079 --> 00:46:46,640
what this allows for the agent to do is

1308
00:46:46,640 --> 00:46:48,319
to focus on modeling

1309
00:46:48,319 --> 00:46:50,079
the states and the policies and the

1310
00:46:50,079 --> 00:46:52,319
parameters that are relevant via queue

1311
00:46:52,319 --> 00:46:55,520
and then potentially use math slash

1312
00:46:55,520 --> 00:46:58,960
inactive behavior in this model to

1313
00:46:58,960 --> 00:47:01,119
attractively include condition upon or

1314
00:47:01,119 --> 00:47:02,800
learn from its observations

1315
00:47:02,800 --> 00:47:04,800
so we could bring previous knowledge to

1316
00:47:04,800 --> 00:47:06,400
the table just like bayesian models

1317
00:47:06,400 --> 00:47:07,359
allow for

1318
00:47:07,359 --> 00:47:10,240
but also have a way to deal with sparse

1319
00:47:10,240 --> 00:47:11,359
or dense information

1320
00:47:11,359 --> 00:47:14,240
as it is

1321
00:47:14,480 --> 00:47:18,000
okay crucially active inference also

1322
00:47:18,000 --> 00:47:19,599
proposes that an agent's goals and

1323
00:47:19,599 --> 00:47:21,440
desires are encoded in the generative

1324
00:47:21,440 --> 00:47:23,520
model as prior preferences for favorable

1325
00:47:23,520 --> 00:47:24,640
observations

1326
00:47:24,640 --> 00:47:28,000
i.e blood temperature at 37 celsius

1327
00:47:28,000 --> 00:47:29,839
free energy then provides a proxy for

1328
00:47:29,839 --> 00:47:31,920
how surprising ie unlikely some

1329
00:47:31,920 --> 00:47:34,160
observations are under the agent's model

1330
00:47:34,160 --> 00:47:36,319
while minimizing equation 1 provides an

1331
00:47:36,319 --> 00:47:38,000
estimate for how surprising some of our

1332
00:47:38,000 --> 00:47:39,760
observations are it cannot reduce the

1333
00:47:39,760 --> 00:47:41,040
quantity directly

1334
00:47:41,040 --> 00:47:44,240
so here equation one we defined what f

1335
00:47:44,240 --> 00:47:46,720
was but it's just a definition and a

1336
00:47:46,720 --> 00:47:47,839
bounding

1337
00:47:47,839 --> 00:47:51,359
of what f is it cannot reduce this

1338
00:47:51,359 --> 00:47:53,280
quantity directly so it's not really

1339
00:47:53,280 --> 00:47:54,960
related to

1340
00:47:54,960 --> 00:47:58,720
policy as directly approaches

1341
00:47:58,720 --> 00:48:01,200
to achieve this agents must change their

1342
00:48:01,200 --> 00:48:03,280
observations through action

1343
00:48:03,280 --> 00:48:05,280
acting to minimize variational free

1344
00:48:05,280 --> 00:48:07,040
energy ensures the minimization of

1345
00:48:07,040 --> 00:48:07,599
surprise

1346
00:48:07,599 --> 00:48:10,160
surprisal negative natural log of the

1347
00:48:10,160 --> 00:48:12,880
probability model of observations

1348
00:48:12,880 --> 00:48:14,720
or the maximization of the bayesian

1349
00:48:14,720 --> 00:48:16,240
model evidence

1350
00:48:16,240 --> 00:48:19,440
p of observations through time since

1351
00:48:19,440 --> 00:48:20,079
free energy

1352
00:48:20,079 --> 00:48:21,920
provides an upper bound on surprisal

1353
00:48:21,920 --> 00:48:23,599
that was equation one

1354
00:48:23,599 --> 00:48:26,160
active inference therefore proposes that

1355
00:48:26,160 --> 00:48:27,680
agents select policy

1356
00:48:27,680 --> 00:48:29,520
in order to minimize expected free

1357
00:48:29,520 --> 00:48:30,960
energy

1358
00:48:30,960 --> 00:48:34,000
fancy g where the expected free energy

1359
00:48:34,000 --> 00:48:36,160
for a given policy pi at some future

1360
00:48:36,160 --> 00:48:38,240
time is

1361
00:48:38,240 --> 00:48:41,760
going to be defined this way so

1362
00:48:41,760 --> 00:48:44,880
we want to do more than just update

1363
00:48:44,880 --> 00:48:47,839
our internal model to bound our surprise

1364
00:48:47,839 --> 00:48:49,359
on our observations we don't just want

1365
00:48:49,359 --> 00:48:51,119
to fit the statistical descriptor model

1366
00:48:51,119 --> 00:48:52,160
of the world

1367
00:48:52,160 --> 00:48:53,920
we want to reduce the uncertainty in the

1368
00:48:53,920 --> 00:48:55,920
future which is a future time point t

1369
00:48:55,920 --> 00:48:59,440
tau through the policy pi that we take

1370
00:48:59,440 --> 00:49:00,960
now

1371
00:49:00,960 --> 00:49:02,960
this minimizable expected free energy

1372
00:49:02,960 --> 00:49:04,880
function fancy g

1373
00:49:04,880 --> 00:49:07,599
is going to take in the arguments of the

1374
00:49:07,599 --> 00:49:08,800
policy selection

1375
00:49:08,800 --> 00:49:11,680
and the future time step so fancy g

1376
00:49:11,680 --> 00:49:12,640
fancy pi

1377
00:49:12,640 --> 00:49:15,760
fancy t that's the free energy function

1378
00:49:15,760 --> 00:49:17,920
in this case

1379
00:49:17,920 --> 00:49:21,680
and what that value

1380
00:49:21,680 --> 00:49:24,319
which optimization on it is going to

1381
00:49:24,319 --> 00:49:25,599
allow for effective action

1382
00:49:25,599 --> 00:49:27,599
is going to be defined as is the

1383
00:49:27,599 --> 00:49:28,720
expectation

1384
00:49:28,720 --> 00:49:31,760
that's going to be related to q model

1385
00:49:31,760 --> 00:49:34,720
which is about this observation states

1386
00:49:34,720 --> 00:49:37,119
and parameters given policy models so

1387
00:49:37,119 --> 00:49:39,359
given that my policy is this

1388
00:49:39,359 --> 00:49:42,720
then how will the states and transitions

1389
00:49:42,720 --> 00:49:43,599
of the world

1390
00:49:43,599 --> 00:49:46,960
appear and it's going to be the

1391
00:49:46,960 --> 00:49:50,400
expectation of this quantity that's in

1392
00:49:50,400 --> 00:49:51,280
brackets

1393
00:49:51,280 --> 00:49:53,760
it's going to be the log of states and

1394
00:49:53,760 --> 00:49:54,640
parameters

1395
00:49:54,640 --> 00:49:58,079
so that's the q of the s sub t

1396
00:49:58,079 --> 00:50:01,359
and parameters given the policy

1397
00:50:01,359 --> 00:50:04,480
so that's what states will i be in and

1398
00:50:04,480 --> 00:50:06,960
what will i think about

1399
00:50:06,960 --> 00:50:10,000
um along the way potentially these are

1400
00:50:10,000 --> 00:50:10,400
just

1401
00:50:10,400 --> 00:50:12,960
first pass verbal explanations just look

1402
00:50:12,960 --> 00:50:15,119
at the math but

1403
00:50:15,119 --> 00:50:18,810
what states and parameters am i in now

1404
00:50:18,810 --> 00:50:20,000
[Music]

1405
00:50:20,000 --> 00:50:23,359
at a future time given my policy so if i

1406
00:50:23,359 --> 00:50:25,599
go out on a walk every day how will it

1407
00:50:25,599 --> 00:50:27,119
influence my

1408
00:50:27,119 --> 00:50:29,200
states what does my model have to be at

1409
00:50:29,200 --> 00:50:30,319
that time

1410
00:50:30,319 --> 00:50:32,880
and then that quantity minus natural log

1411
00:50:32,880 --> 00:50:34,000
of p

1412
00:50:34,000 --> 00:50:36,400
the observations states in the model

1413
00:50:36,400 --> 00:50:38,000
given policy so that's the part that we

1414
00:50:38,000 --> 00:50:40,000
want is actually the observations to be

1415
00:50:40,000 --> 00:50:43,520
the correct values but what we can do

1416
00:50:43,520 --> 00:50:46,480
is um just condition the states on

1417
00:50:46,480 --> 00:50:48,960
policy

1418
00:50:50,160 --> 00:50:53,760
and that g value is always greater than

1419
00:50:53,760 --> 00:50:54,240
the

1420
00:50:54,240 --> 00:50:55,760
negative that's this last part of

1421
00:50:55,760 --> 00:50:57,599
equation two then the negative

1422
00:50:57,599 --> 00:50:59,040
expectation

1423
00:50:59,040 --> 00:51:01,599
of this q which is the observations

1424
00:51:01,599 --> 00:51:03,599
conditioned on policy so that's like

1425
00:51:03,599 --> 00:51:08,319
how things will actually turn out um

1426
00:51:08,319 --> 00:51:10,880
of this log part of the generative model

1427
00:51:10,880 --> 00:51:11,920
that is

1428
00:51:11,920 --> 00:51:14,319
the observations given the policy that's

1429
00:51:14,319 --> 00:51:15,760
how things will go

1430
00:51:15,760 --> 00:51:18,160
so maybe there's other interpretations

1431
00:51:18,160 --> 00:51:18,800
or other

1432
00:51:18,800 --> 00:51:20,240
parts that are really critical but i

1433
00:51:20,240 --> 00:51:22,000
think this is one of the key

1434
00:51:22,000 --> 00:51:25,119
formulations which is that there's a

1435
00:51:25,119 --> 00:51:29,119
policy related optimization problem

1436
00:51:29,119 --> 00:51:32,240
that can be phrased in a variational way

1437
00:51:32,240 --> 00:51:35,520
that is going to be bounded through a

1438
00:51:35,520 --> 00:51:36,960
free energy like

1439
00:51:36,960 --> 00:51:40,000
strategy to

1440
00:51:40,000 --> 00:51:43,440
estimate tractably some

1441
00:51:43,440 --> 00:51:46,720
other function q something related to

1442
00:51:46,720 --> 00:51:47,760
that

1443
00:51:47,760 --> 00:51:50,319
of basically what we would want to know

1444
00:51:50,319 --> 00:51:51,440
which is like

1445
00:51:51,440 --> 00:51:54,559
okay the observation that i look into my

1446
00:51:54,559 --> 00:51:58,160
bitcoin wallet and there's 15 bitcoin

1447
00:51:58,160 --> 00:52:01,200
how what policy do i have to take to

1448
00:52:01,200 --> 00:52:03,680
see that observation or for that counter

1449
00:52:03,680 --> 00:52:06,160
factual to exist

1450
00:52:06,160 --> 00:52:08,880
if only one knew so that's why there's

1451
00:52:08,880 --> 00:52:09,839
this tractable

1452
00:52:09,839 --> 00:52:12,800
boundable equation potentially i i i

1453
00:52:12,800 --> 00:52:13,839
think there's a lot to

1454
00:52:13,839 --> 00:52:16,960
say here and alec and anyone else i'd

1455
00:52:16,960 --> 00:52:18,000
appreciate any other

1456
00:52:18,000 --> 00:52:20,800
input but that was just one read on it

1457
00:52:20,800 --> 00:52:21,680
okay

1458
00:52:21,680 --> 00:52:23,920
so that was most of the part that i

1459
00:52:23,920 --> 00:52:25,440
wanted to go through at that level of

1460
00:52:25,440 --> 00:52:27,280
depth but then just to convey

1461
00:52:27,280 --> 00:52:29,680
the other sections and what they do for

1462
00:52:29,680 --> 00:52:30,400
the paper

1463
00:52:30,400 --> 00:52:32,160
but not go into every equation as much

1464
00:52:32,160 --> 00:52:33,599
and then talk about the figures

1465
00:52:33,599 --> 00:52:36,960
so 3.2 is the learning and inference

1466
00:52:36,960 --> 00:52:37,760
section

1467
00:52:37,760 --> 00:52:41,119
and in order to actually implement

1468
00:52:41,119 --> 00:52:43,520
this optimization scheme as it's laid

1469
00:52:43,520 --> 00:52:44,160
out

1470
00:52:44,160 --> 00:52:46,079
it turns out there has to be an update

1471
00:52:46,079 --> 00:52:47,599
process so

1472
00:52:47,599 --> 00:52:49,760
is it going to be a sort of uh stick

1473
00:52:49,760 --> 00:52:51,359
with what has worked or is it going to

1474
00:52:51,359 --> 00:52:51,760
be

1475
00:52:51,760 --> 00:52:53,760
novelty search those are kinds of things

1476
00:52:53,760 --> 00:52:54,880
that optimization

1477
00:52:54,880 --> 00:52:56,720
algorithms have to navigate the

1478
00:52:56,720 --> 00:52:59,040
trade-offs that are implicit uh

1479
00:52:59,040 --> 00:53:00,880
with different strategies depending on

1480
00:53:00,880 --> 00:53:02,079
different problems

1481
00:53:02,079 --> 00:53:05,599
and so this section defines how this

1482
00:53:05,599 --> 00:53:09,280
f uh variational free energy is defined

1483
00:53:09,280 --> 00:53:11,119
through time

1484
00:53:11,119 --> 00:53:12,720
and i'm not going to cover the

1485
00:53:12,720 --> 00:53:14,559
implementation details but alex

1486
00:53:14,559 --> 00:53:17,520
alec would be cool if you um you know

1487
00:53:17,520 --> 00:53:19,119
showed us a simulation or something like

1488
00:53:19,119 --> 00:53:20,880
that

1489
00:53:20,880 --> 00:53:23,920
uh 3.3 is about policy selection here's

1490
00:53:23,920 --> 00:53:25,280
what they write

1491
00:53:25,280 --> 00:53:27,440
under active inference policy selection

1492
00:53:27,440 --> 00:53:29,040
is achieved by updating q

1493
00:53:29,040 --> 00:53:31,040
of policy in order to minimize free

1494
00:53:31,040 --> 00:53:33,200
energy at fancy f

1495
00:53:33,200 --> 00:53:35,440
given the prior belief that policies

1496
00:53:35,440 --> 00:53:37,280
minimize expected free energy

1497
00:53:37,280 --> 00:53:41,040
i.e the p of policy is related to

1498
00:53:41,040 --> 00:53:44,240
this choice soft max related to the

1499
00:53:44,240 --> 00:53:47,920
um negative g negative uh

1500
00:53:47,920 --> 00:53:51,280
expected free energy of policy spaces

1501
00:53:51,280 --> 00:53:53,359
with short

1502
00:53:53,359 --> 00:53:55,920
as specified in equation three free

1503
00:53:55,920 --> 00:53:57,359
energy is minimized

1504
00:53:57,359 --> 00:54:02,079
when the uh q distribution of policy

1505
00:54:02,079 --> 00:54:05,599
is the same as the optimal free energy

1506
00:54:05,599 --> 00:54:08,160
minimizing uh expectation of policies

1507
00:54:08,160 --> 00:54:10,000
across policies like just like the free

1508
00:54:10,000 --> 00:54:13,200
energy optimal policy selection

1509
00:54:13,200 --> 00:54:14,800
for discrete action spaces with short

1510
00:54:14,800 --> 00:54:16,240
temporal horizons

1511
00:54:16,240 --> 00:54:19,200
basically g of policy can be evaluated

1512
00:54:19,200 --> 00:54:20,720
in full by considering each possible

1513
00:54:20,720 --> 00:54:22,720
policy so that's like connect four

1514
00:54:22,720 --> 00:54:25,520
you could just run out or tic-tac-toe

1515
00:54:25,520 --> 00:54:26,160
you can run out

1516
00:54:26,160 --> 00:54:27,839
every single option for the games at

1517
00:54:27,839 --> 00:54:29,599
that branch point

1518
00:54:29,599 --> 00:54:31,680
compute the absolute value of every

1519
00:54:31,680 --> 00:54:33,520
single possible outcome

1520
00:54:33,520 --> 00:54:35,920
and then make your choice however in

1521
00:54:35,920 --> 00:54:37,760
continuous action spaces there are

1522
00:54:37,760 --> 00:54:39,839
infinite policies meaning an alternative

1523
00:54:39,839 --> 00:54:41,040
approach is required

1524
00:54:41,040 --> 00:54:42,960
that's a really cool sentence because it

1525
00:54:42,960 --> 00:54:45,200
really conveys well

1526
00:54:45,200 --> 00:54:47,599
that actually the continuous action

1527
00:54:47,599 --> 00:54:48,240
space

1528
00:54:48,240 --> 00:54:50,720
even for a single variable is a truly

1529
00:54:50,720 --> 00:54:51,680
different domain

1530
00:54:51,680 --> 00:54:54,400
than a let's just say prisoner's dilemma

1531
00:54:54,400 --> 00:54:55,119
game

1532
00:54:55,119 --> 00:54:58,160
there's sometimes uh almost no overlap

1533
00:54:58,160 --> 00:55:00,160
in the algorithms that's why this paper

1534
00:55:00,160 --> 00:55:02,880
actually reflects quite in advance and

1535
00:55:02,880 --> 00:55:06,079
there's more details in that section

1536
00:55:06,079 --> 00:55:09,200
3.4 trajectory sampling so now how do

1537
00:55:09,200 --> 00:55:10,559
you go from just having this

1538
00:55:10,559 --> 00:55:12,400
distributional relationship between

1539
00:55:12,400 --> 00:55:15,200
policies and other uh variables like

1540
00:55:15,200 --> 00:55:16,720
related to generative models of the

1541
00:55:16,720 --> 00:55:17,920
world

1542
00:55:17,920 --> 00:55:21,119
and observation models how are they

1543
00:55:21,119 --> 00:55:21,520
going to

1544
00:55:21,520 --> 00:55:23,520
get there so to evaluate from the

1545
00:55:23,520 --> 00:55:25,599
distribution to the specific policy

1546
00:55:25,599 --> 00:55:27,839
so to evaluate the expected free energy

1547
00:55:27,839 --> 00:55:29,680
for any specific policy

1548
00:55:29,680 --> 00:55:31,599
first they have to evaluate the expected

1549
00:55:31,599 --> 00:55:33,680
future beliefs condition on that policy

1550
00:55:33,680 --> 00:55:37,440
so uh one can't really undertake

1551
00:55:37,440 --> 00:55:40,559
uh in this sort of normative framing

1552
00:55:40,559 --> 00:55:42,480
it's a secondary debate which we can

1553
00:55:42,480 --> 00:55:44,480
definitely have about to what extent

1554
00:55:44,480 --> 00:55:45,119
these are

1555
00:55:45,119 --> 00:55:46,880
experienced cognitively by humans but

1556
00:55:46,880 --> 00:55:48,160
we're just going to kind of

1557
00:55:48,160 --> 00:55:50,160
go with it with the intentional stance

1558
00:55:50,160 --> 00:55:51,760
and the sort of machine learning control

1559
00:55:51,760 --> 00:55:52,160
theory

1560
00:55:52,160 --> 00:55:55,359
take but and so beliefs it's not

1561
00:55:55,359 --> 00:55:57,200
necessarily cognitive

1562
00:55:57,200 --> 00:55:58,400
we're just thinking about what the agent

1563
00:55:58,400 --> 00:56:00,319
wants to do to succeed

1564
00:56:00,319 --> 00:56:02,240
but basically if the agent doesn't have

1565
00:56:02,240 --> 00:56:03,920
an expected future

1566
00:56:03,920 --> 00:56:06,400
belief sequence for that policy it will

1567
00:56:06,400 --> 00:56:08,160
be extremely difficult for them to

1568
00:56:08,160 --> 00:56:09,599
undertake it

1569
00:56:09,599 --> 00:56:12,559
and the trans um the fact that the

1570
00:56:12,559 --> 00:56:13,359
transition

1571
00:56:13,359 --> 00:56:14,880
model is probabilistic and the

1572
00:56:14,880 --> 00:56:16,480
parameters of the transition model or

1573
00:56:16,480 --> 00:56:17,599
random variables

1574
00:56:17,599 --> 00:56:19,440
induces a distribution over future

1575
00:56:19,440 --> 00:56:21,040
trajectories so

1576
00:56:21,040 --> 00:56:23,520
rather than just playing every possible

1577
00:56:23,520 --> 00:56:24,640
chess move

1578
00:56:24,640 --> 00:56:27,280
we're thinking about how we are fitting

1579
00:56:27,280 --> 00:56:28,559
a

1580
00:56:28,559 --> 00:56:31,200
more advanced kind of model well chess

1581
00:56:31,200 --> 00:56:32,480
is still discrete so

1582
00:56:32,480 --> 00:56:35,040
let's choose we'll see a specific

1583
00:56:35,040 --> 00:56:35,760
example

1584
00:56:35,760 --> 00:56:38,319
but this is just continuous optimization

1585
00:56:38,319 --> 00:56:38,880
later

1586
00:56:38,880 --> 00:56:42,799
in this figures but um this is like

1587
00:56:42,799 --> 00:56:45,680
having a distribution over future branch

1588
00:56:45,680 --> 00:56:46,880
points in a chess game

1589
00:56:46,880 --> 00:56:49,280
so whether it's discrete or continuous

1590
00:56:49,280 --> 00:56:50,880
you can still have this element of

1591
00:56:50,880 --> 00:56:51,839
uncertainty

1592
00:56:51,839 --> 00:56:54,559
of a distributional range over future

1593
00:56:54,559 --> 00:56:55,440
trajectories

1594
00:56:55,440 --> 00:56:59,040
in state spaces so several approaches

1595
00:56:59,040 --> 00:57:01,119
exist to approximate the propagation of

1596
00:57:01,119 --> 00:57:02,559
uncertain trajectories

1597
00:57:02,559 --> 00:57:04,480
for example one can ignore uncertainty

1598
00:57:04,480 --> 00:57:06,160
entirely and propagate the mean of the

1599
00:57:06,160 --> 00:57:07,280
distribution

1600
00:57:07,280 --> 00:57:09,440
or one can explicitly propagate the full

1601
00:57:09,440 --> 00:57:11,440
statistics of the distribution

1602
00:57:11,440 --> 00:57:13,680
so those are kind of two extreme

1603
00:57:13,680 --> 00:57:15,839
approaches one of them is just say yep

1604
00:57:15,839 --> 00:57:17,680
ensemble modeling is tracking the mean

1605
00:57:17,680 --> 00:57:19,200
and so i'm going to keep that moving

1606
00:57:19,200 --> 00:57:19,920
forward

1607
00:57:19,920 --> 00:57:22,160
and then the alternative is basically to

1608
00:57:22,160 --> 00:57:23,359
keep a summarization

1609
00:57:23,359 --> 00:57:25,359
of the entire distribution and use that

1610
00:57:25,359 --> 00:57:26,880
as what is being learned by the

1611
00:57:26,880 --> 00:57:29,520
population

1612
00:57:29,520 --> 00:57:32,400
in the current work it's utilized a

1613
00:57:32,400 --> 00:57:33,359
particle approach

1614
00:57:33,359 --> 00:57:35,839
whereby a monte carlo so sampling based

1615
00:57:35,839 --> 00:57:36,960
scheme

1616
00:57:36,960 --> 00:57:39,680
samples are propagated in particular we

1617
00:57:39,680 --> 00:57:41,119
consider

1618
00:57:41,119 --> 00:57:43,680
this big b samples from the parameter

1619
00:57:43,680 --> 00:57:44,480
distribution

1620
00:57:44,480 --> 00:57:48,799
data which is drawn from cube data

1621
00:57:49,280 --> 00:57:52,960
these sections convey how information on

1622
00:57:52,960 --> 00:57:54,480
policy selection

1623
00:57:54,480 --> 00:57:56,480
is accomplished in the model and how the

1624
00:57:56,480 --> 00:57:58,400
policy fitness landscape is explored in

1625
00:57:58,400 --> 00:57:59,040
order to

1626
00:57:59,040 --> 00:58:00,319
implement predictive control on

1627
00:58:00,319 --> 00:58:02,240
trajectories so the

1628
00:58:02,240 --> 00:58:05,440
policy through time implement it must

1629
00:58:05,440 --> 00:58:06,720
involve some element

1630
00:58:06,720 --> 00:58:10,480
of predictive control all right

1631
00:58:10,480 --> 00:58:14,079
then there's section um 3.5 3.6

1632
00:58:14,079 --> 00:58:17,200
so 3.5 has details on computing this

1633
00:58:17,200 --> 00:58:18,960
expected free energy

1634
00:58:18,960 --> 00:58:22,480
value as the title might suggest

1635
00:58:22,480 --> 00:58:24,480
in this section they describe how to

1636
00:58:24,480 --> 00:58:25,599
evaluate negative g

1637
00:58:25,599 --> 00:58:29,520
for a policy where they have used this

1638
00:58:29,520 --> 00:58:31,760
notational convenience to talk about

1639
00:58:31,760 --> 00:58:33,200
policies moving forward

1640
00:58:33,200 --> 00:58:36,720
so kind of just made it like a naked pie

1641
00:58:36,720 --> 00:58:40,240
rather than this upper superscript that

1642
00:58:40,240 --> 00:58:43,440
has to do with the time states um and

1643
00:58:43,440 --> 00:58:44,559
uncertainty

1644
00:58:44,559 --> 00:58:47,599
and the negative expected free energy

1645
00:58:47,599 --> 00:58:48,400
which is what we

1646
00:58:48,400 --> 00:58:49,839
are trying to get out here with the

1647
00:58:49,839 --> 00:58:51,760
negative g

1648
00:58:51,760 --> 00:58:54,880
is equal to the sum of this negative

1649
00:58:54,880 --> 00:58:56,480
expected free energies through

1650
00:58:56,480 --> 00:58:59,520
time and then that is where

1651
00:58:59,520 --> 00:59:03,599
a there's a decomposition that

1652
00:59:03,599 --> 00:59:06,720
not my area but just i would

1653
00:59:06,720 --> 00:59:09,440
think what it means is that there's a

1654
00:59:09,440 --> 00:59:10,480
decomposition

1655
00:59:10,480 --> 00:59:13,760
into this state information game

1656
00:59:13,760 --> 00:59:16,640
gain and an extrinsic value component so

1657
00:59:16,640 --> 00:59:18,240
this kind of breaks down into an explore

1658
00:59:18,240 --> 00:59:19,200
and exploit

1659
00:59:19,200 --> 00:59:21,920
components or some other trade-off we

1660
00:59:21,920 --> 00:59:22,960
see this a lot

1661
00:59:22,960 --> 00:59:25,520
with this negative g i would love it if

1662
00:59:25,520 --> 00:59:26,799
somebody could uh

1663
00:59:26,799 --> 00:59:29,040
talk a little bit why this is or if it

1664
00:59:29,040 --> 00:59:30,480
always is the case but

1665
00:59:30,480 --> 00:59:34,079
it always uh seems to be presented as

1666
00:59:34,079 --> 00:59:36,480
model accuracy penalized by model

1667
00:59:36,480 --> 00:59:37,359
complexity or

1668
00:59:37,359 --> 00:59:40,640
this plus this is it always two

1669
00:59:40,640 --> 00:59:42,640
parts in the equation is it ever just

1670
00:59:42,640 --> 00:59:44,960
one term can it be three terms

1671
00:59:44,960 --> 00:59:48,000
here it's three terms but what um

1672
00:59:48,000 --> 00:59:52,000
or four so i'm not exactly sure

1673
00:59:52,000 --> 00:59:55,280
why sometimes it has more or less terms

1674
00:59:55,280 --> 00:59:58,319
but that's what i'm curious about

1675
00:59:58,319 --> 01:00:01,119
then in 3.6 they write the model

1676
01:00:01,119 --> 01:00:01,760
presented

1677
01:00:01,760 --> 01:00:04,000
in the preceding sections serves as the

1678
01:00:04,000 --> 01:00:06,079
most general formulation applicable in

1679
01:00:06,079 --> 01:00:07,760
both partially observed and fully

1680
01:00:07,760 --> 01:00:09,599
observed environments

1681
01:00:09,599 --> 01:00:11,200
in what follows we describe an

1682
01:00:11,200 --> 01:00:13,119
implementation full of fur

1683
01:00:13,119 --> 01:00:15,040
implementation for the fully observed

1684
01:00:15,040 --> 01:00:16,880
case leaving an analysis of the

1685
01:00:16,880 --> 01:00:19,119
partially observed case for future work

1686
01:00:19,119 --> 01:00:21,680
so it's just another wrapper around the

1687
01:00:21,680 --> 01:00:23,040
fully observed case

1688
01:00:23,040 --> 01:00:26,160
to have the partial observations and

1689
01:00:26,160 --> 01:00:28,160
so they decided to use some benchmarks

1690
01:00:28,160 --> 01:00:29,599
that facilitated them

1691
01:00:29,599 --> 01:00:32,480
to basically have total observation and

1692
01:00:32,480 --> 01:00:33,200
this isn't like

1693
01:00:33,200 --> 01:00:35,599
cheating the examples they're gonna

1694
01:00:35,599 --> 01:00:36,240
describe

1695
01:00:36,240 --> 01:00:38,240
are kind of like balancing a dinner

1696
01:00:38,240 --> 01:00:39,440
plate on a stick

1697
01:00:39,440 --> 01:00:40,880
so in that case you do have total

1698
01:00:40,880 --> 01:00:42,640
observation of the model

1699
01:00:42,640 --> 01:00:45,440
so just because you can't control um uh

1700
01:00:45,440 --> 01:00:46,079
well

1701
01:00:46,079 --> 01:00:47,760
it may or may not be possible given a

1702
01:00:47,760 --> 01:00:49,119
certain stick and uh

1703
01:00:49,119 --> 01:00:52,079
motor reflex time but at the very least

1704
01:00:52,079 --> 01:00:53,520
you can observe everything so there's

1705
01:00:53,520 --> 01:00:55,200
also unobservable control theory

1706
01:00:55,200 --> 01:00:56,960
problems and then there's ones where the

1707
01:00:56,960 --> 01:00:58,160
system is actually

1708
01:00:58,160 --> 01:01:00,160
understandable and controllable like the

1709
01:01:00,160 --> 01:01:01,599
dinner plate but

1710
01:01:01,599 --> 01:01:03,760
you only get distorted or malicious

1711
01:01:03,760 --> 01:01:04,640
information

1712
01:01:04,640 --> 01:01:06,319
there's other ones where you get perfect

1713
01:01:06,319 --> 01:01:08,000
information but only at time points and

1714
01:01:08,000 --> 01:01:09,599
there's a game going on so

1715
01:01:09,599 --> 01:01:10,960
a lot of different ways that that can

1716
01:01:10,960 --> 01:01:12,640
play out but that's

1717
01:01:12,640 --> 01:01:15,680
sort of the uh the parts of the model

1718
01:01:15,680 --> 01:01:17,520
that can get switched out

1719
01:01:17,520 --> 01:01:19,200
and that's what also i think it will be

1720
01:01:19,200 --> 01:01:20,799
interesting to hear from any of the

1721
01:01:20,799 --> 01:01:22,880
authors on

1722
01:01:22,880 --> 01:01:26,240
so in section 4 they

1723
01:01:26,240 --> 01:01:28,799
go through some experiments and this is

1724
01:01:28,799 --> 01:01:29,760
kind of the

1725
01:01:29,760 --> 01:01:32,720
results phase of their paper and they

1726
01:01:32,720 --> 01:01:34,400
describe how they investigate

1727
01:01:34,400 --> 01:01:37,280
whether one whether the proposed active

1728
01:01:37,280 --> 01:01:39,040
inference model can successfully promote

1729
01:01:39,040 --> 01:01:41,200
exploration in the absence of reward

1730
01:01:41,200 --> 01:01:43,440
observations i.e exploration so that's

1731
01:01:43,440 --> 01:01:45,119
what's really difficult for the model

1732
01:01:45,119 --> 01:01:45,760
free

1733
01:01:45,760 --> 01:01:47,520
reinforcement learners to do is when

1734
01:01:47,520 --> 01:01:48,799
it's not getting the reward

1735
01:01:48,799 --> 01:01:50,720
it doesn't do very well can't explore

1736
01:01:50,720 --> 01:01:52,000
very well

1737
01:01:52,000 --> 01:01:54,960
because it's not winning and two whether

1738
01:01:54,960 --> 01:01:56,799
the model can achieve good performance

1739
01:01:56,799 --> 01:01:58,319
and high sample efficiency on

1740
01:01:58,319 --> 01:02:00,559
challenging continuous control tasks i.e

1741
01:02:00,559 --> 01:02:02,400
exploitation so that's what requires

1742
01:02:02,400 --> 01:02:04,240
tracking on something

1743
01:02:04,240 --> 01:02:06,079
we evaluate these two aspects of the

1744
01:02:06,079 --> 01:02:07,760
model separately leaving analysis of

1745
01:02:07,760 --> 01:02:08,400
their joint

1746
01:02:08,400 --> 01:02:10,400
performance i.e the exploration

1747
01:02:10,400 --> 01:02:12,000
exploitation dilemma

1748
01:02:12,000 --> 01:02:14,319
to future work so again show it

1749
01:02:14,319 --> 01:02:15,039
separately

1750
01:02:15,039 --> 01:02:17,440
in the kind of pure use case show that

1751
01:02:17,440 --> 01:02:18,480
it can do either one

1752
01:02:18,480 --> 01:02:20,079
and then it will be something like a

1753
01:02:20,079 --> 01:02:21,760
trade-off between the two of them or a

1754
01:02:21,760 --> 01:02:23,680
wrapper layer that will end up mediating

1755
01:02:23,680 --> 01:02:25,839
this trade-off

1756
01:02:25,839 --> 01:02:27,599
all right the explore task that they're

1757
01:02:27,599 --> 01:02:29,839
gonna do is called mountain car

1758
01:02:29,839 --> 01:02:33,039
and you're a little car on the bottom of

1759
01:02:33,039 --> 01:02:34,079
this valley

1760
01:02:34,079 --> 01:02:37,200
and you can push forward with

1761
01:02:37,200 --> 01:02:40,480
um the forward pedal or you can

1762
01:02:40,480 --> 01:02:43,039
push reverse with a reverse pedal and

1763
01:02:43,039 --> 01:02:44,880
you don't have a strong enough engine

1764
01:02:44,880 --> 01:02:47,760
to get up to the yellow flag with just

1765
01:02:47,760 --> 01:02:48,000
one

1766
01:02:48,000 --> 01:02:50,160
go it's like you're at the bottom of the

1767
01:02:50,160 --> 01:02:51,280
hill and you're in the

1768
01:02:51,280 --> 01:02:53,599
gear three seven on your bike but you

1769
01:02:53,599 --> 01:02:55,280
can go up a little bit and back a little

1770
01:02:55,280 --> 01:02:56,880
bit and so here you have to kind of

1771
01:02:56,880 --> 01:02:58,640
explore and figure out a strategy that

1772
01:02:58,640 --> 01:03:00,480
helps you explore more and more

1773
01:03:00,480 --> 01:03:02,640
in order to get enough speed by going up

1774
01:03:02,640 --> 01:03:04,079
this left hill in order to get

1775
01:03:04,079 --> 01:03:07,359
yellow flat and then the exploit tasks

1776
01:03:07,359 --> 01:03:10,079
that they're going to be talking about

1777
01:03:10,079 --> 01:03:11,119
are two-fold

1778
01:03:11,119 --> 01:03:12,960
there's the inverted pendulum task so

1779
01:03:12,960 --> 01:03:14,480
here we're still a little cart

1780
01:03:14,480 --> 01:03:17,119
or something like that and we can move

1781
01:03:17,119 --> 01:03:18,799
forward or backwards on a track

1782
01:03:18,799 --> 01:03:21,200
and then there's the pendulum that we're

1783
01:03:21,200 --> 01:03:21,920
trying to keep

1784
01:03:21,920 --> 01:03:23,599
upright and so that's like the dinner

1785
01:03:23,599 --> 01:03:25,520
plate on a stick

1786
01:03:25,520 --> 01:03:27,920
then there's the hopper task which is a

1787
01:03:27,920 --> 01:03:29,119
jumping

1788
01:03:29,119 --> 01:03:32,400
motor coordination task so let's look at

1789
01:03:32,400 --> 01:03:33,920
what they actually did for each of these

1790
01:03:33,920 --> 01:03:34,240
two

1791
01:03:34,240 --> 01:03:37,200
experiments and then see how the active

1792
01:03:37,200 --> 01:03:39,520
inference learner stacked up against

1793
01:03:39,520 --> 01:03:43,039
other algorithms so the mountain car

1794
01:03:43,039 --> 01:03:45,440
example is a one-dimensional track

1795
01:03:45,440 --> 01:03:47,039
positioned between two mountains the

1796
01:03:47,039 --> 01:03:48,640
goal is to drive up the mountain on the

1797
01:03:48,640 --> 01:03:50,160
right however the car's engine is not

1798
01:03:50,160 --> 01:03:51,520
strong enough to scale the mountain in a

1799
01:03:51,520 --> 01:03:52,720
single pass

1800
01:03:52,720 --> 01:03:55,359
similar to what set the only way to

1801
01:03:55,359 --> 01:03:56,880
succeed is to drive back and forth to

1802
01:03:56,880 --> 01:03:58,799
build up momentum so what was cool about

1803
01:03:58,799 --> 01:03:59,200
this

1804
01:03:59,200 --> 01:04:02,319
i thought was that the

1805
01:04:02,319 --> 01:04:05,359
code was uh on the openai site and on

1806
01:04:05,359 --> 01:04:07,280
their github.com

1807
01:04:07,280 --> 01:04:10,880
link is here and so it could be really

1808
01:04:10,880 --> 01:04:13,520
seen and played with and used as a

1809
01:04:13,520 --> 01:04:14,160
standard

1810
01:04:14,160 --> 01:04:16,240
so it seems pretty cool didn't know

1811
01:04:16,240 --> 01:04:17,440
about this resource

1812
01:04:17,440 --> 01:04:20,559
for standardizing different learning

1813
01:04:20,559 --> 01:04:21,760
algorithms

1814
01:04:21,760 --> 01:04:25,039
so how did the active inference agent

1815
01:04:25,039 --> 01:04:28,240
do well it did well in this test

1816
01:04:28,240 --> 01:04:29,839
otherwise they wouldn't have

1817
01:04:29,839 --> 01:04:32,640
made the paper and the way to read these

1818
01:04:32,640 --> 01:04:33,280
graphs

1819
01:04:33,280 --> 01:04:35,119
um a b and c are going to have the same

1820
01:04:35,119 --> 01:04:36,880
x and y axis

1821
01:04:36,880 --> 01:04:40,480
so i put the map

1822
01:04:40,480 --> 01:04:43,520
with the engine on the top

1823
01:04:43,520 --> 01:04:46,480
left so it kind of maps on to the bottom

1824
01:04:46,480 --> 01:04:46,960
left

1825
01:04:46,960 --> 01:04:50,319
panel a and so the position is related

1826
01:04:50,319 --> 01:04:51,599
to the x-axis

1827
01:04:51,599 --> 01:04:53,760
and so that's how far to the left and

1828
01:04:53,760 --> 01:04:55,280
how far to the right

1829
01:04:55,280 --> 01:04:57,839
that little train car goes and then the

1830
01:04:57,839 --> 01:04:58,720
velocity

1831
01:04:58,720 --> 01:05:01,359
is whether it's at rest zero whether

1832
01:05:01,359 --> 01:05:03,200
it's moving positively to the right or

1833
01:05:03,200 --> 01:05:04,640
whether it's moving negatively to the

1834
01:05:04,640 --> 01:05:05,280
left

1835
01:05:05,280 --> 01:05:08,240
so the reward agent who just sort of

1836
01:05:08,240 --> 01:05:09,680
pursues momentum

1837
01:05:09,680 --> 01:05:13,680
whichever way i guess um only ends up

1838
01:05:13,680 --> 01:05:16,400
getting a max speed of around 0.02 in

1839
01:05:16,400 --> 01:05:17,039
either way

1840
01:05:17,039 --> 01:05:20,079
and then only moves like .7 or something

1841
01:05:20,079 --> 01:05:22,960
in either direction the epsilon

1842
01:05:22,960 --> 01:05:25,599
greedy agent does manage to learn a

1843
01:05:25,599 --> 01:05:26,880
short range policy

1844
01:05:26,880 --> 01:05:29,760
that we can see brings it a little bit

1845
01:05:29,760 --> 01:05:31,760
further on one side

1846
01:05:31,760 --> 01:05:34,400
and it does take some rare excursions

1847
01:05:34,400 --> 01:05:34,960
into

1848
01:05:34,960 --> 01:05:37,760
slightly higher velocity regimes but in

1849
01:05:37,760 --> 01:05:38,880
the end it uh

1850
01:05:38,880 --> 01:05:41,440
doesn't really get too far like it

1851
01:05:41,440 --> 01:05:42,000
learns

1852
01:05:42,000 --> 01:05:44,079
how to go uphill when it's going uphill

1853
01:05:44,079 --> 01:05:46,400
but that only helps at a limited extent

1854
01:05:46,400 --> 01:05:48,799
whereas the active inference model as

1855
01:05:48,799 --> 01:05:49,599
implemented

1856
01:05:49,599 --> 01:05:52,480
has not just a more broad sampling

1857
01:05:52,480 --> 01:05:53,280
within the

1858
01:05:53,280 --> 01:05:55,760
um state-space conversions after 100

1859
01:05:55,760 --> 01:05:56,960
epochs

1860
01:05:56,960 --> 01:06:00,960
but their position

1861
01:06:00,960 --> 01:06:03,760
extends to much higher range in the x

1862
01:06:03,760 --> 01:06:04,000
and

1863
01:06:04,000 --> 01:06:06,079
also a higher velocity distribution

1864
01:06:06,079 --> 01:06:07,599
range in the y

1865
01:06:07,599 --> 01:06:10,640
so it was more able to get out of this

1866
01:06:10,640 --> 01:06:12,640
little ditch

1867
01:06:12,640 --> 01:06:14,559
by being an active inference learner

1868
01:06:14,559 --> 01:06:15,839
than by being

1869
01:06:15,839 --> 01:06:19,119
epsilon greedy or just sort of model

1870
01:06:19,119 --> 01:06:21,280
free reward based

1871
01:06:21,280 --> 01:06:25,520
all right the second optimization

1872
01:06:25,520 --> 01:06:27,599
example presented in the paper is this

1873
01:06:27,599 --> 01:06:29,440
hopper v2 again didn't know about this

1874
01:06:29,440 --> 01:06:30,799
resource

1875
01:06:30,799 --> 01:06:34,160
so this one is interesting it is

1876
01:06:34,160 --> 01:06:36,799
described as making a two-dimensional

1877
01:06:36,799 --> 01:06:37,839
one-legged robot

1878
01:06:37,839 --> 01:06:40,400
hop forward as fast as possible and the

1879
01:06:40,400 --> 01:06:42,079
code is also there i'm not going to put

1880
01:06:42,079 --> 01:06:43,520
the code up but

1881
01:06:43,520 --> 01:06:45,599
it kind of reminded me of this idea of

1882
01:06:45,599 --> 01:06:47,359
evolutionary computation

1883
01:06:47,359 --> 01:06:50,960
as control and so sometimes

1884
01:06:50,960 --> 01:06:52,799
there's so many angles to this first off

1885
01:06:52,799 --> 01:06:54,000
just this image

1886
01:06:54,000 --> 01:06:57,039
with a checkerboard um

1887
01:06:57,039 --> 01:07:00,079
floorboard it reminded me of some

1888
01:07:00,079 --> 01:07:03,200
simulators of evolutionary computation

1889
01:07:03,200 --> 01:07:05,039
like for linux and stuff like that where

1890
01:07:05,039 --> 01:07:06,799
you could have these blocky aliens that

1891
01:07:06,799 --> 01:07:08,000
would replicate and

1892
01:07:08,000 --> 01:07:10,160
take up a ton of computational resources

1893
01:07:10,160 --> 01:07:11,200
and just

1894
01:07:11,200 --> 01:07:12,720
clash into each other and sometimes it

1895
01:07:12,720 --> 01:07:14,079
was just the outline other times it was

1896
01:07:14,079 --> 01:07:16,000
3d blocks with this kind of a background

1897
01:07:16,000 --> 01:07:18,079
and a floor plan

1898
01:07:18,079 --> 01:07:20,640
but here and what we're evolving is

1899
01:07:20,640 --> 01:07:22,240
control strategies not

1900
01:07:22,240 --> 01:07:24,960
chromosomes representing genotypes or

1901
01:07:24,960 --> 01:07:25,920
let's just say

1902
01:07:25,920 --> 01:07:28,000
but actually a similarity is this idea

1903
01:07:28,000 --> 01:07:30,000
of the genetic algorithm and the genetic

1904
01:07:30,000 --> 01:07:30,640
algorithm

1905
01:07:30,640 --> 01:07:33,119
stochastic search because sometimes the

1906
01:07:33,119 --> 01:07:34,559
way that these control policies are

1907
01:07:34,559 --> 01:07:35,520
optimized

1908
01:07:35,520 --> 01:07:38,000
is actually through steps that involve

1909
01:07:38,000 --> 01:07:39,839
things like genetic search algorithms

1910
01:07:39,839 --> 01:07:41,440
that switch over large sets of their

1911
01:07:41,440 --> 01:07:42,319
parameters

1912
01:07:42,319 --> 01:07:43,680
so there's some just interesting

1913
01:07:43,680 --> 01:07:45,839
parallels and this i just thought was a

1914
01:07:45,839 --> 01:07:47,039
cool example

1915
01:07:47,039 --> 01:07:49,599
and at the intersection of motor

1916
01:07:49,599 --> 01:07:50,240
behavior

1917
01:07:50,240 --> 01:07:53,280
control but also complex state state

1918
01:07:53,280 --> 01:07:53,920
space

1919
01:07:53,920 --> 01:07:56,000
estimation even the beginnings of an

1920
01:07:56,000 --> 01:07:57,200
inactive

1921
01:07:57,200 --> 01:07:59,119
uh and maybe even embodied approach

1922
01:07:59,119 --> 01:08:00,960
because at some level

1923
01:08:00,960 --> 01:08:03,760
of movement complexity it's going to be

1924
01:08:03,760 --> 01:08:05,680
implicit in the system

1925
01:08:05,680 --> 01:08:07,680
and distributed in it just like other

1926
01:08:07,680 --> 01:08:09,920
systems that it has some notion of its

1927
01:08:09,920 --> 01:08:11,039
own affordances

1928
01:08:11,039 --> 01:08:13,280
if it had 500 muscles and it was a human

1929
01:08:13,280 --> 01:08:15,839
leg it would have some sort of uh

1930
01:08:15,839 --> 01:08:17,520
tensegrity structure that kept it

1931
01:08:17,520 --> 01:08:19,198
balanced in a way that facilitated

1932
01:08:19,198 --> 01:08:21,439
some types of actions but not others so

1933
01:08:21,439 --> 01:08:22,719
very interesting

1934
01:08:22,719 --> 01:08:25,839
choice i'd like to hear from

1935
01:08:25,839 --> 01:08:28,238
any authors or from other people like

1936
01:08:28,238 --> 01:08:29,600
what are other

1937
01:08:29,600 --> 01:08:32,158
benchmarks that are cool what other

1938
01:08:32,158 --> 01:08:32,960
control

1939
01:08:32,960 --> 01:08:35,198
systems problems are interesting or

1940
01:08:35,198 --> 01:08:36,640
applicable

1941
01:08:36,640 --> 01:08:39,520
what about social versions or networked

1942
01:08:39,520 --> 01:08:41,920
or communication based versions

1943
01:08:41,920 --> 01:08:44,880
so what were the results

1944
01:08:45,198 --> 01:08:47,679
okay so here is how to look at these

1945
01:08:47,679 --> 01:08:48,238
graphs

1946
01:08:48,238 --> 01:08:50,479
the x-axis is the epoch so that's just

1947
01:08:50,479 --> 01:08:52,238
the sampling through time that's how

1948
01:08:52,238 --> 01:08:52,719
many

1949
01:08:52,719 --> 01:08:55,439
generations of parameters are tried and

1950
01:08:55,439 --> 01:08:56,640
here the red lines

1951
01:08:56,640 --> 01:08:59,920
indicate the same interval of time so

1952
01:08:59,920 --> 01:09:03,359
c and d are related to epochs 1 through

1953
01:09:03,359 --> 01:09:04,000
100

1954
01:09:04,000 --> 01:09:05,359
and that's actually compressed up

1955
01:09:05,359 --> 01:09:07,520
against um a very small

1956
01:09:07,520 --> 01:09:10,640
uh unit on a and b and a

1957
01:09:10,640 --> 01:09:12,479
and c respectively are for this inverted

1958
01:09:12,479 --> 01:09:13,839
pendulum and um b

1959
01:09:13,839 --> 01:09:16,880
and d are for the hopper task so we can

1960
01:09:16,880 --> 01:09:18,640
talk about the same phenomena looking at

1961
01:09:18,640 --> 01:09:20,960
a and c in reference to the pendulum

1962
01:09:20,960 --> 01:09:24,238
and b and d in the hopper and in both of

1963
01:09:24,238 --> 01:09:25,600
these cases

1964
01:09:25,600 --> 01:09:28,399
zoomed in or zoomed out what is pretty

1965
01:09:28,399 --> 01:09:29,040
apparent

1966
01:09:29,040 --> 01:09:32,080
is that the act and the y-axis is reward

1967
01:09:32,080 --> 01:09:33,120
so that's just like

1968
01:09:33,120 --> 01:09:34,640
in the hopper task is how far you're

1969
01:09:34,640 --> 01:09:36,238
getting and then in the inverted

1970
01:09:36,238 --> 01:09:37,439
pendulum task

1971
01:09:37,439 --> 01:09:40,560
it's about um your time maintained given

1972
01:09:40,560 --> 01:09:42,479
a certain parameter combination for

1973
01:09:42,479 --> 01:09:46,158
policy and in both cases

1974
01:09:46,158 --> 01:09:49,359
the reward sharply increases

1975
01:09:49,359 --> 01:09:52,799
for the active inference agent and

1976
01:09:52,799 --> 01:09:56,320
also just immediately uh starts climbing

1977
01:09:56,320 --> 01:09:57,520
after some

1978
01:09:57,520 --> 01:10:00,480
sampling within the first 100 or even

1979
01:10:00,480 --> 01:10:01,679
the first uh

1980
01:10:01,679 --> 01:10:04,159
several epochs probably because of the

1981
01:10:04,159 --> 01:10:05,760
lower state space

1982
01:10:05,760 --> 01:10:09,360
of the uh cart because all the cart can

1983
01:10:09,360 --> 01:10:09,760
do

1984
01:10:09,760 --> 01:10:11,040
is move forward and backwards have a

1985
01:10:11,040 --> 01:10:12,800
policy on that like gas and break and

1986
01:10:12,800 --> 01:10:13,520
stuff like that

1987
01:10:13,520 --> 01:10:15,679
that's it's all cooked into the physics

1988
01:10:15,679 --> 01:10:16,880
of the pendulum

1989
01:10:16,880 --> 01:10:18,239
i wonder if there's a dump inverted

1990
01:10:18,239 --> 01:10:20,000
double pendulum task by the way that'd

1991
01:10:20,000 --> 01:10:21,600
be kind of interesting

1992
01:10:21,600 --> 01:10:23,440
and um in the hopper task there's

1993
01:10:23,440 --> 01:10:25,440
several more parameters at play

1994
01:10:25,440 --> 01:10:28,400
i didn't copy out the exact number but

1995
01:10:28,400 --> 01:10:29,679
there's several more so

1996
01:10:29,679 --> 01:10:32,800
it takes a couple more epochs reflected

1997
01:10:32,800 --> 01:10:33,679
by the zero

1998
01:10:33,679 --> 01:10:36,880
through um maybe 40 where the

1999
01:10:36,880 --> 01:10:38,719
the average is like not really getting

2000
01:10:38,719 --> 01:10:40,560
off the ground and this is being

2001
01:10:40,560 --> 01:10:42,719
contrasted against the ddpg which you

2002
01:10:42,719 --> 01:10:44,400
can read more about i'm just not going

2003
01:10:44,400 --> 01:10:45,440
to go into what this

2004
01:10:45,440 --> 01:10:48,080
algorithm is i just trusted all right

2005
01:10:48,080 --> 01:10:49,600
the authors used something that makes

2006
01:10:49,600 --> 01:10:50,480
sense

2007
01:10:50,480 --> 01:10:53,679
let's hear from anyone about what other

2008
01:10:53,679 --> 01:10:56,000
alternatives could be tested against

2009
01:10:56,000 --> 01:10:57,280
so those would all be things where i'm

2010
01:10:57,280 --> 01:10:59,280
just not as

2011
01:10:59,280 --> 01:11:02,560
up on literature but definitely would

2012
01:11:02,560 --> 01:11:03,280
welcome

2013
01:11:03,280 --> 01:11:05,040
the comments of someone from the machine

2014
01:11:05,040 --> 01:11:06,560
learning field and

2015
01:11:06,560 --> 01:11:08,400
what would be an impressive or useful

2016
01:11:08,400 --> 01:11:10,400
demonstration of active inference or

2017
01:11:10,400 --> 01:11:11,280
what would it mean

2018
01:11:11,280 --> 01:11:15,280
to do a control theory simulation

2019
01:11:15,280 --> 01:11:18,320
in this sort of a framework what else

2020
01:11:18,320 --> 01:11:20,000
was used

2021
01:11:20,000 --> 01:11:21,600
what what else is the current state of

2022
01:11:21,600 --> 01:11:24,000
the art for these inverted pendulum and

2023
01:11:24,000 --> 01:11:24,560
hopper

2024
01:11:24,560 --> 01:11:27,920
tasks just i i don't know

2025
01:11:27,920 --> 01:11:32,159
but it'd be great if anyone does know so

2026
01:11:32,159 --> 01:11:34,480
just to close out with the relationship

2027
01:11:34,480 --> 01:11:36,400
to the previous work

2028
01:11:36,400 --> 01:11:39,199
okay this is from their closing section

2029
01:11:39,199 --> 01:11:40,960
which maybe it's a

2030
01:11:40,960 --> 01:11:44,480
disciplinary thing but often

2031
01:11:44,480 --> 01:11:46,800
it's like a discussion section but if it

2032
01:11:46,800 --> 01:11:48,320
was relationship with previous work i

2033
01:11:48,320 --> 01:11:49,520
would have expected it to be a little

2034
01:11:49,520 --> 01:11:51,280
bit more contextualizing

2035
01:11:51,280 --> 01:11:55,280
and at the front of the paper so

2036
01:11:55,280 --> 01:11:57,520
but it read more like a discussion

2037
01:11:57,520 --> 01:11:58,320
related to

2038
01:11:58,320 --> 01:12:00,400
deep active inference which is like this

2039
01:12:00,400 --> 01:12:02,800
through timeness of active inference

2040
01:12:02,800 --> 01:12:05,040
they write our work builds upon these

2041
01:12:05,040 --> 01:12:06,800
previous models by incorporating model

2042
01:12:06,800 --> 01:12:09,199
uncertainty in its active resolution

2043
01:12:09,199 --> 01:12:10,800
we extend the previous point estimate

2044
01:12:10,800 --> 01:12:12,560
models to include full distributions

2045
01:12:12,560 --> 01:12:14,480
over parameters and update the expected

2046
01:12:14,480 --> 01:12:15,760
free energy functional

2047
01:12:15,760 --> 01:12:17,679
such that the uncertainty in these

2048
01:12:17,679 --> 01:12:19,840
distributions is actively minimized

2049
01:12:19,840 --> 01:12:21,600
this brings our implementation in line

2050
01:12:21,600 --> 01:12:23,199
with the canonical models of active

2051
01:12:23,199 --> 01:12:24,320
inference from the cognitive and

2052
01:12:24,320 --> 01:12:26,400
computational neuroscience literature

2053
01:12:26,400 --> 01:12:29,120
so this is almost like saying yeah one

2054
01:12:29,120 --> 01:12:29,679
more

2055
01:12:29,679 --> 01:12:32,320
moreover it enables us to evaluate the

2056
01:12:32,320 --> 01:12:34,640
feasibility of active exploration

2057
01:12:34,640 --> 01:12:36,239
under the scaled active inference

2058
01:12:36,239 --> 01:12:38,320
framework apply the model to more

2059
01:12:38,320 --> 01:12:40,320
complex control tasks

2060
01:12:40,320 --> 01:12:42,400
and obtain increased sample efficiency

2061
01:12:42,400 --> 01:12:43,920
relative to previous models

2062
01:12:43,920 --> 01:12:45,840
so it's kind of like saying we've made

2063
01:12:45,840 --> 01:12:47,840
this active inference model closer to

2064
01:12:47,840 --> 01:12:50,000
what it has been speculated to be

2065
01:12:50,000 --> 01:12:53,280
in the qualitative neurobehavioral realm

2066
01:12:53,280 --> 01:12:55,360
which i think is a good reason to be

2067
01:12:55,360 --> 01:12:56,800
reading it right now

2068
01:12:56,800 --> 01:12:58,960
because it does fit in nicely with our

2069
01:12:58,960 --> 01:13:01,280
discussions of behavior

2070
01:13:01,280 --> 01:13:04,320
and of um

2071
01:13:04,320 --> 01:13:07,679
neurobiology in the variational ecology

2072
01:13:07,679 --> 01:13:10,560
discussions so this is on the simulation

2073
01:13:10,560 --> 01:13:13,280
side how it comes to be this way

2074
01:13:13,280 --> 01:13:15,280
so this is kind of cool because deep

2075
01:13:15,280 --> 01:13:16,560
active inference is now being

2076
01:13:16,560 --> 01:13:17,440
implementable

2077
01:13:17,440 --> 01:13:19,199
through these kinds of models that are

2078
01:13:19,199 --> 01:13:20,880
presented in this paper

2079
01:13:20,880 --> 01:13:23,120
and previously was like oh yeah i guess

2080
01:13:23,120 --> 01:13:25,199
if it were deep active inference then it

2081
01:13:25,199 --> 01:13:27,040
would accommodate for narratives

2082
01:13:27,040 --> 01:13:28,640
and now this is the kind of framework

2083
01:13:28,640 --> 01:13:30,400
where oh yeah okay we could talk about

2084
01:13:30,400 --> 01:13:31,600
narrative through time

2085
01:13:31,600 --> 01:13:35,520
in this framework could we the second

2086
01:13:35,520 --> 01:13:38,000
area they address again another key word

2087
01:13:38,000 --> 01:13:39,199
for the paper

2088
01:13:39,199 --> 01:13:42,000
is model based reinforcement learning in

2089
01:13:42,000 --> 01:13:42,960
the current work we

2090
01:13:42,960 --> 01:13:45,120
opted for bayesian neural networks to

2091
01:13:45,120 --> 01:13:47,199
ensure consistency with the variational

2092
01:13:47,199 --> 01:13:48,719
principles espoused by the active

2093
01:13:48,719 --> 01:13:49,679
inference framework

2094
01:13:49,679 --> 01:13:51,679
but note that ensembles can be made

2095
01:13:51,679 --> 01:13:53,360
explicitly bayesian with minor

2096
01:13:53,360 --> 01:13:54,480
modifications

2097
01:13:54,480 --> 01:13:57,920
so they talked about several homologies

2098
01:13:57,920 --> 01:13:59,679
to reinforcement learning

2099
01:13:59,679 --> 01:14:01,840
and specifically amortized inference

2100
01:14:01,840 --> 01:14:03,040
there's actually one quote

2101
01:14:03,040 --> 01:14:06,159
that uh i pulled out from one of the

2102
01:14:06,159 --> 01:14:07,520
papers that are on the slide and that

2103
01:14:07,520 --> 01:14:09,280
talked about how

2104
01:14:09,280 --> 01:14:10,480
you might get the sense that this is

2105
01:14:10,480 --> 01:14:13,040
like a win-for-free situation with just

2106
01:14:13,040 --> 01:14:14,159
wrapping a functional

2107
01:14:14,159 --> 01:14:15,679
around the estimation of these

2108
01:14:15,679 --> 01:14:17,600
parameters because it's like oh it's a

2109
01:14:17,600 --> 01:14:19,120
neural network i'm implementing a neural

2110
01:14:19,120 --> 01:14:20,560
network so i have an additional degree

2111
01:14:20,560 --> 01:14:22,800
of freedom to learn non-linear policy

2112
01:14:22,800 --> 01:14:26,000
but then the paper argued that

2113
01:14:26,000 --> 01:14:28,800
actually you're constrained because the

2114
01:14:28,800 --> 01:14:30,400
functional is estimating

2115
01:14:30,400 --> 01:14:32,320
the mean and the variance on this

2116
01:14:32,320 --> 01:14:34,080
gaussian outcome

2117
01:14:34,080 --> 01:14:37,520
and so it doesn't actually liberate a

2118
01:14:37,520 --> 01:14:41,920
degree of uh of output

2119
01:14:41,920 --> 01:14:44,960
expressivity but rather

2120
01:14:44,960 --> 01:14:48,080
a definable and a scalable way to

2121
01:14:48,080 --> 01:14:48,640
implement

2122
01:14:48,640 --> 01:14:51,120
on current computational hardware a way

2123
01:14:51,120 --> 01:14:52,560
to implement this

2124
01:14:52,560 --> 01:14:54,719
massive pencil and paper problem of

2125
01:14:54,719 --> 01:14:56,159
estimating the gaussian mean and

2126
01:14:56,159 --> 01:14:58,400
variance of super super complex

2127
01:14:58,400 --> 01:15:00,640
high dimensional state spaces but that

2128
01:15:00,640 --> 01:15:02,239
still is what it's doing

2129
01:15:02,239 --> 01:15:05,440
so super interesting stuff and

2130
01:15:05,440 --> 01:15:09,520
um also the note here that the ensembles

2131
01:15:09,520 --> 01:15:11,679
could be explicitly bayesian so with

2132
01:15:11,679 --> 01:15:14,320
all the kinds of variants and flavors

2133
01:15:14,320 --> 01:15:16,320
that that introduces so

2134
01:15:16,320 --> 01:15:19,360
anyone who's a enthusiast or

2135
01:15:19,360 --> 01:15:22,159
expert in these areas i would appreciate

2136
01:15:22,159 --> 01:15:23,199
their perspective

2137
01:15:23,199 --> 01:15:26,080
on what that means or why this is

2138
01:15:26,080 --> 01:15:26,800
important or

2139
01:15:26,800 --> 01:15:30,640
interesting all right then info gain

2140
01:15:30,640 --> 01:15:34,400
and this was one of the key words too so

2141
01:15:34,400 --> 01:15:36,960
identifying scalable and efficient

2142
01:15:36,960 --> 01:15:38,719
exploration strategies remains one of

2143
01:15:38,719 --> 01:15:40,320
the key open questions in reinforcement

2144
01:15:40,320 --> 01:15:41,280
learning

2145
01:15:41,280 --> 01:15:43,440
model-free methods such as the greedy or

2146
01:15:43,440 --> 01:15:45,440
boltzmann choice rules

2147
01:15:45,440 --> 01:15:48,800
paging dr proton sean utilize

2148
01:15:48,800 --> 01:15:51,520
noise in the action selection process or

2149
01:15:51,520 --> 01:15:52,960
uncertainty in the reward

2150
01:15:52,960 --> 01:15:55,840
statistics a more powerful approach is

2151
01:15:55,840 --> 01:15:57,280
to construct a model of the world

2152
01:15:57,280 --> 01:15:58,800
allowing the agent to evaluate which

2153
01:15:58,800 --> 01:16:00,400
parts of the state space it has and has

2154
01:16:00,400 --> 01:16:01,440
not visited

2155
01:16:01,440 --> 01:16:02,960
this allows for measures such as the

2156
01:16:02,960 --> 01:16:04,400
amount of prediction error or prediction

2157
01:16:04,400 --> 01:16:04,640
or

2158
01:16:04,640 --> 01:16:05,920
improvement to be utilized for

2159
01:16:05,920 --> 01:16:08,159
exploration okay cool

2160
01:16:08,159 --> 01:16:10,640
if the learn model implicitly or

2161
01:16:10,640 --> 01:16:12,320
explicitly captures probabilistic

2162
01:16:12,320 --> 01:16:12,960
features

2163
01:16:12,960 --> 01:16:14,880
so that's the statistical regularities

2164
01:16:14,880 --> 01:16:16,239
of a niche then

2165
01:16:16,239 --> 01:16:17,760
information theoretic measures can be

2166
01:16:17,760 --> 01:16:19,600
used to guide exploration

2167
01:16:19,600 --> 01:16:22,560
so there's so much to say here and it's

2168
01:16:22,560 --> 01:16:24,960
very interesting

2169
01:16:24,960 --> 01:16:27,360
to bring in the information gain at the

2170
01:16:27,360 --> 01:16:28,239
end

2171
01:16:28,239 --> 01:16:30,880
really what it's saying is this learned

2172
01:16:30,880 --> 01:16:31,840
model

2173
01:16:31,840 --> 01:16:34,560
of active inference and the way that the

2174
01:16:34,560 --> 01:16:36,719
exploration is tied into the generative

2175
01:16:36,719 --> 01:16:38,400
model of the world such that action is

2176
01:16:38,400 --> 01:16:40,560
facilitated not just towards the

2177
01:16:40,560 --> 01:16:42,480
straightforwardly rewarding that's sort

2178
01:16:42,480 --> 01:16:44,719
of the model free rl

2179
01:16:44,719 --> 01:16:48,719
nor merely the um deep model

2180
01:16:48,719 --> 01:16:51,520
rewarding that's like i love running

2181
01:16:51,520 --> 01:16:53,679
marathons because it's healthy i'm not

2182
01:16:53,679 --> 01:16:55,840
you know saying it's not healthy just

2183
01:16:55,840 --> 01:16:57,920
it's it's the mindset that helps one

2184
01:16:57,920 --> 01:17:03,600
get there for sure and then um

2185
01:17:03,600 --> 01:17:05,040
this is saying we can actually go

2186
01:17:05,040 --> 01:17:06,640
another level beyond

2187
01:17:06,640 --> 01:17:10,159
that we can say with a deep generative

2188
01:17:10,159 --> 01:17:11,040
model

2189
01:17:11,040 --> 01:17:13,679
of what one is likely to be doing we can

2190
01:17:13,679 --> 01:17:15,040
maximize precision

2191
01:17:15,040 --> 01:17:16,800
which may be something you hear amongst

2192
01:17:16,800 --> 01:17:19,199
the you know ultra ultra

2193
01:17:19,199 --> 01:17:22,800
um committed of an area something like

2194
01:17:22,800 --> 01:17:26,080
this is just how and who i am so

2195
01:17:26,080 --> 01:17:28,480
in that sense it would be going beyond

2196
01:17:28,480 --> 01:17:30,000
this whole oh well this many

2197
01:17:30,000 --> 01:17:31,920
miles per day is healthy for me which is

2198
01:17:31,920 --> 01:17:33,840
a great motivation it might be

2199
01:17:33,840 --> 01:17:36,480
might be a multi-scale thing um just one

2200
01:17:36,480 --> 01:17:37,280
example

2201
01:17:37,280 --> 01:17:39,280
and definitely i know you know even for

2202
01:17:39,280 --> 01:17:40,560
me it's just changed like

2203
01:17:40,560 --> 01:17:42,880
through life so it just shows how it's

2204
01:17:42,880 --> 01:17:45,040
related to your niche your context your

2205
01:17:45,040 --> 01:17:46,719
social relationships

2206
01:17:46,719 --> 01:17:49,840
so many other areas and

2207
01:17:49,840 --> 01:17:51,679
just to kind of close on one thought on

2208
01:17:51,679 --> 01:17:54,400
this information again

2209
01:17:54,400 --> 01:17:57,360
um information gain surprised me at

2210
01:17:57,360 --> 01:17:58,560
first because i was like trying to

2211
01:17:58,560 --> 01:18:01,040
connect all this to information theory

2212
01:18:01,040 --> 01:18:04,000
and i realized that what it's about is

2213
01:18:04,000 --> 01:18:05,760
reducing uncertainty on target

2214
01:18:05,760 --> 01:18:06,480
parameters

2215
01:18:06,480 --> 01:18:07,920
information is about reducing your

2216
01:18:07,920 --> 01:18:09,840
uncertainty about things

2217
01:18:09,840 --> 01:18:11,679
and it's just about reducing our

2218
01:18:11,679 --> 01:18:13,600
uncertainty about something

2219
01:18:13,600 --> 01:18:16,400
else or about more specifically

2220
01:18:16,400 --> 01:18:18,480
something else being a multi-scale model

2221
01:18:18,480 --> 01:18:20,400
that includes some attributes that we're

2222
01:18:20,400 --> 01:18:22,080
probably pretty familiar with like the

2223
01:18:22,080 --> 01:18:23,600
observation model

2224
01:18:23,600 --> 01:18:26,880
some aspects that are homologous to

2225
01:18:26,880 --> 01:18:30,880
a reward model like sort of a preference

2226
01:18:30,880 --> 01:18:33,280
model let's just say but then some

2227
01:18:33,280 --> 01:18:34,239
aspects that are

2228
01:18:34,239 --> 01:18:37,280
mainly maybe new and that might be the

2229
01:18:37,280 --> 01:18:39,040
part about the free energy

2230
01:18:39,040 --> 01:18:42,800
and then that area as i understand it

2231
01:18:42,800 --> 01:18:46,080
is related to another type of

2232
01:18:46,080 --> 01:18:48,640
information theory and model selection

2233
01:18:48,640 --> 01:18:51,280
and the variational framework so if that

2234
01:18:51,280 --> 01:18:52,159
is an area

2235
01:18:52,159 --> 01:18:55,360
that anyone has insight into

2236
01:18:55,360 --> 01:18:57,120
i think it's pretty cool maybe we could

2237
01:18:57,120 --> 01:18:58,640
try to unpack that i'm not

2238
01:18:58,640 --> 01:19:01,280
sure what it looks like but uh

2239
01:19:01,280 --> 01:19:02,400
definitely

2240
01:19:02,400 --> 01:19:04,960
something i wondered about and minding

2241
01:19:04,960 --> 01:19:06,560
the p's and q's and

2242
01:19:06,560 --> 01:19:08,159
thanks alec for a little bit of the

2243
01:19:08,159 --> 01:19:10,480
email discourse about clarifying a few

2244
01:19:10,480 --> 01:19:11,520
aspects of the paper

2245
01:19:11,520 --> 01:19:15,199
because um it was pretty interesting

2246
01:19:15,199 --> 01:19:16,159
and i think there's a lot of

2247
01:19:16,159 --> 01:19:18,719
implications and what systems we can

2248
01:19:18,719 --> 01:19:21,520
possibly apply this to proximally or in

2249
01:19:21,520 --> 01:19:22,480
the medium term

2250
01:19:22,480 --> 01:19:26,080
so tons of interesting stuff we'll be

2251
01:19:26,080 --> 01:19:29,440
talking about this on november 10th 2020

2252
01:19:29,440 --> 01:19:29,840
and

2253
01:19:29,840 --> 01:19:34,239
the 17th at 7 30 am pst

2254
01:19:34,239 --> 01:19:38,560
and i think that is it and

2255
01:19:38,560 --> 01:19:43,600
let's see yeah one one closing thought

2256
01:19:43,600 --> 01:19:45,280
all we can have is a deep generative

2257
01:19:45,280 --> 01:19:46,960
model of the space intertwined with

2258
01:19:46,960 --> 01:19:47,679
action

2259
01:19:47,679 --> 01:19:49,440
so that the parameters we explore are

2260
01:19:49,440 --> 01:19:50,880
the ones that are at the trade-off

2261
01:19:50,880 --> 01:19:53,679
of successful and optimally informative

2262
01:19:53,679 --> 01:19:54,960
sometimes we fear

2263
01:19:54,960 --> 01:19:57,440
we veer towards successful which can be

2264
01:19:57,440 --> 01:19:59,040
not optimally informative

2265
01:19:59,040 --> 01:20:00,400
other times we are getting informed

2266
01:20:00,400 --> 01:20:02,000
greatly in a way that may not be

2267
01:20:02,000 --> 01:20:03,040
successful

2268
01:20:03,040 --> 01:20:04,880
but overall we manage that trade-off

2269
01:20:04,880 --> 01:20:07,520
well it's how we've gotten this far

2270
01:20:07,520 --> 01:20:09,760
so great work everyone thanks a lot for

2271
01:20:09,760 --> 01:20:10,960
listening

2272
01:20:10,960 --> 01:20:14,880
keep it up thank you for participating

2273
01:20:14,880 --> 01:20:17,280
we do provide follow-up forms to live

2274
01:20:17,280 --> 01:20:18,800
participants

2275
01:20:18,800 --> 01:20:21,199
and would be awesome if anyone had

2276
01:20:21,199 --> 01:20:21,840
feedback

2277
01:20:21,840 --> 01:20:25,120
or suggestions or questions

2278
01:20:25,120 --> 01:20:28,480
you can stay in communication with us

2279
01:20:28,480 --> 01:20:31,679
and up one more slide on that

2280
01:20:31,679 --> 01:20:34,159
parameters reparameterization trick and

2281
01:20:34,159 --> 01:20:36,080
the slides from that paper but

2282
01:20:36,080 --> 01:20:39,600
other than that thanks for listening

2283
01:20:39,600 --> 01:20:43,040
and for bearing with the uh anomalous

2284
01:20:43,040 --> 01:20:46,080
camera error but

2285
01:20:46,080 --> 01:20:48,520
yeah i'm looking forward to the end of

2286
01:20:48,520 --> 01:20:49,679
2020

2287
01:20:49,679 --> 01:20:51,440
coming in with all these cool

2288
01:20:51,440 --> 01:20:53,280
discussions we'll be having

2289
01:20:53,280 --> 01:20:55,280
we'd love to have some new participants

2290
01:20:55,280 --> 01:20:56,560
come online

2291
01:20:56,560 --> 01:20:59,040
or bring in some new perspectives that

2292
01:20:59,040 --> 01:21:00,239
we haven't considered

2293
01:21:00,239 --> 01:21:02,960
or talk about it from a beginner's

2294
01:21:02,960 --> 01:21:03,920
perspective

2295
01:21:03,920 --> 01:21:06,639
from any number of fields as a starting

2296
01:21:06,639 --> 01:21:07,120
point

2297
01:21:07,120 --> 01:21:09,040
just let us know that's the kind of

2298
01:21:09,040 --> 01:21:10,560
stuff we

2299
01:21:10,560 --> 01:21:13,679
just always appreciate hearing about so

2300
01:21:13,679 --> 01:21:17,679
have a good november and end of 2020

2301
01:21:17,679 --> 01:21:21,120
season and we look forward to

2302
01:21:21,120 --> 01:21:26,880
talking with you alright bye


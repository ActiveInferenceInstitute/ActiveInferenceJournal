1
00:00:06,399 --> 00:00:07,600
hello and welcome

2
00:00:07,600 --> 00:00:09,760
everyone this is the active inference

3
00:00:09,760 --> 00:00:11,360
live stream it is act

4
00:00:11,360 --> 00:00:14,480
imp live stream 8.2 and it is november

5
00:00:14,480 --> 00:00:16,320
17 2020

6
00:00:16,320 --> 00:00:18,960
so welcome everyone listeners and

7
00:00:18,960 --> 00:00:20,480
participants

8
00:00:20,480 --> 00:00:23,279
welcome to team com everyone we are an

9
00:00:23,279 --> 00:00:24,960
experiment in online team

10
00:00:24,960 --> 00:00:27,119
communication learning and practice

11
00:00:27,119 --> 00:00:29,199
related to active inference

12
00:00:29,199 --> 00:00:32,000
you can find us at our website our

13
00:00:32,000 --> 00:00:32,800
twitter

14
00:00:32,800 --> 00:00:36,000
our email on our public key base team or

15
00:00:36,000 --> 00:00:38,960
on youtube this is a recorded and an

16
00:00:38,960 --> 00:00:40,719
archived live stream so please do

17
00:00:40,719 --> 00:00:42,640
provide us with feedback so that we can

18
00:00:42,640 --> 00:00:45,520
improve on our work all backgrounds and

19
00:00:45,520 --> 00:00:46,559
perspectives are

20
00:00:46,559 --> 00:00:48,480
welcome here in learning about these

21
00:00:48,480 --> 00:00:50,960
questions and as far as video etiquette

22
00:00:50,960 --> 00:00:52,399
for live streams goes

23
00:00:52,399 --> 00:00:54,000
just remember to mute if there's noise

24
00:00:54,000 --> 00:00:55,520
in the background and raise your hands

25
00:00:55,520 --> 00:00:57,199
so we can hear from everyone on the

26
00:00:57,199 --> 00:00:58,079
stack

27
00:00:58,079 --> 00:01:01,280
use respectful speech behavior etc

28
00:01:01,280 --> 00:01:05,040
today in exciting act imp stream 8.2

29
00:01:05,040 --> 00:01:06,960
we are going to have introductions and

30
00:01:06,960 --> 00:01:08,479
warm-ups and then

31
00:01:08,479 --> 00:01:10,640
we will at whatever rate make sense

32
00:01:10,640 --> 00:01:13,040
welcome shannon we will walk through the

33
00:01:13,040 --> 00:01:14,720
sections of 8.2

34
00:01:14,720 --> 00:01:18,080
as with 8.0 and 8.1 we are going to be

35
00:01:18,080 --> 00:01:20,080
discussing the paper scaling active

36
00:01:20,080 --> 00:01:22,400
inference by chance at all 2019

37
00:01:22,400 --> 00:01:24,640
and alec thanks so much for coming on to

38
00:01:24,640 --> 00:01:26,000
the show today

39
00:01:26,000 --> 00:01:27,520
we're going to talk about the goals in

40
00:01:27,520 --> 00:01:29,439
the roadmap of the paper hopefully

41
00:01:29,439 --> 00:01:31,840
from the author's perspective we'll then

42
00:01:31,840 --> 00:01:34,000
have a little pause just for an open

43
00:01:34,000 --> 00:01:35,680
q a because there's a lot of different

44
00:01:35,680 --> 00:01:37,600
directions this paper can take one so

45
00:01:37,600 --> 00:01:38,960
we'll just pause there

46
00:01:38,960 --> 00:01:40,720
and then we'll continue through the

47
00:01:40,720 --> 00:01:42,320
figures just two of them

48
00:01:42,320 --> 00:01:44,479
and just address a few different domains

49
00:01:44,479 --> 00:01:46,560
of follow-up questions about the paper

50
00:01:46,560 --> 00:01:49,040
and then if we want to slash still have

51
00:01:49,040 --> 00:01:49,680
time

52
00:01:49,680 --> 00:01:51,439
we can go through some of the notation

53
00:01:51,439 --> 00:01:53,360
and math again

54
00:01:53,360 --> 00:01:55,600
for the rest of 2020 we are going to be

55
00:01:55,600 --> 00:01:56,479
moving into

56
00:01:56,479 --> 00:01:59,600
papers 9 10 and 11. so check it out on

57
00:01:59,600 --> 00:02:00,320
twitter

58
00:02:00,320 --> 00:02:02,560
paper 9 is going to be about

59
00:02:02,560 --> 00:02:03,680
consciousness

60
00:02:03,680 --> 00:02:05,840
paper 10 is about scripts and about

61
00:02:05,840 --> 00:02:08,959
social interactions in paper 11 is about

62
00:02:08,959 --> 00:02:10,878
modeling under the sophisticated

63
00:02:10,878 --> 00:02:13,840
effective inference framework

64
00:02:13,840 --> 00:02:16,959
for the intros and warm-ups if everyone

65
00:02:16,959 --> 00:02:18,239
could just introduce

66
00:02:18,239 --> 00:02:20,800
their name and their location whatever

67
00:02:20,800 --> 00:02:22,720
else they'd like to state especially

68
00:02:22,720 --> 00:02:25,680
our first time guests and then just pass

69
00:02:25,680 --> 00:02:27,440
to somebody who has not spoken

70
00:02:27,440 --> 00:02:29,520
so i'll start i'm daniel i'm in

71
00:02:29,520 --> 00:02:30,720
california

72
00:02:30,720 --> 00:02:34,400
and i will pass it to alec

73
00:02:35,200 --> 00:02:38,080
hey everyone uh yeah i'm alex chance uh

74
00:02:38,080 --> 00:02:38,720
i'm based

75
00:02:38,720 --> 00:02:42,080
in brighton um final year phd student

76
00:02:42,080 --> 00:02:44,720
at sussex and thanks for having me on

77
00:02:44,720 --> 00:02:47,120
the stream

78
00:02:48,319 --> 00:02:52,319
we will go to um sasha

79
00:02:54,800 --> 00:02:57,680
hi my name is sasha i'm also a graduate

80
00:02:57,680 --> 00:02:58,239
student

81
00:02:58,239 --> 00:03:01,360
and i'm based out of davis california

82
00:03:01,360 --> 00:03:05,840
i will pass it to stephen

83
00:03:10,080 --> 00:03:13,440
stephen you're unmuted but um it doesn't

84
00:03:13,440 --> 00:03:15,599
have any sound maybe just reload and

85
00:03:15,599 --> 00:03:19,839
temporarily let's just go to shannon

86
00:03:20,319 --> 00:03:24,000
hi guys i'm shannon i'm

87
00:03:24,000 --> 00:03:25,599
based at the university california in

88
00:03:25,599 --> 00:03:30,239
merced but currently in south dakota

89
00:03:30,239 --> 00:03:33,840
i'll pass it to blue

90
00:03:34,720 --> 00:03:37,680
hi i'm blue knight i am an independent

91
00:03:37,680 --> 00:03:39,120
research consultant

92
00:03:39,120 --> 00:03:42,480
and i am based out of new mexico i will

93
00:03:42,480 --> 00:03:44,840
pass it to

94
00:03:44,840 --> 00:03:47,840
alex

95
00:03:47,920 --> 00:03:51,519
hi my name is alex i'm in moscow russia

96
00:03:51,519 --> 00:03:53,599
and i'm a researcher in systems

97
00:03:53,599 --> 00:03:54,560
management school

98
00:03:54,560 --> 00:03:57,920
and i pass it to v1

99
00:03:59,040 --> 00:04:03,680
hi my name is iran i'm in russia moscow

100
00:04:03,680 --> 00:04:06,879
and i pass it to stephen

101
00:04:06,879 --> 00:04:09,040
steven are you here i'm gonna try again

102
00:04:09,040 --> 00:04:10,000
does that work yes

103
00:04:10,000 --> 00:04:12,879
yep okay hi i'm steven i'm based in

104
00:04:12,879 --> 00:04:15,599
toronto i'm actually studying a

105
00:04:15,599 --> 00:04:17,600
practice-based phd through canterbury

106
00:04:17,600 --> 00:04:20,560
christchurch university in the uk

107
00:04:20,560 --> 00:04:25,360
and i'm pleased to be here and now

108
00:04:25,360 --> 00:04:28,639
hi i'm now andrews um and

109
00:04:28,639 --> 00:04:33,360
i'm in cincinnati doing a phd

110
00:04:33,360 --> 00:04:36,400
cool a lot of different stages and areas

111
00:04:36,400 --> 00:04:37,600
so it will be a great

112
00:04:37,600 --> 00:04:39,759
discussion to bring to the technical

113
00:04:39,759 --> 00:04:42,400
side with this paper for the

114
00:04:42,400 --> 00:04:44,800
warm-up questions people can just raise

115
00:04:44,800 --> 00:04:47,360
their hand as they'd like to speak

116
00:04:47,360 --> 00:04:49,840
and i will just put up the first two

117
00:04:49,840 --> 00:04:50,720
questions

118
00:04:50,720 --> 00:04:53,360
they are for today's discussion what is

119
00:04:53,360 --> 00:04:55,440
something that you are excited about

120
00:04:55,440 --> 00:04:58,320
and then the second question is what is

121
00:04:58,320 --> 00:04:59,040
a question

122
00:04:59,040 --> 00:05:00,960
you're wondering about so while people

123
00:05:00,960 --> 00:05:02,479
are thinking about it or

124
00:05:02,479 --> 00:05:04,960
raising their hand one question i was

125
00:05:04,960 --> 00:05:06,560
wondering about was how do we

126
00:05:06,560 --> 00:05:09,440
trade off between explore and exploit in

127
00:05:09,440 --> 00:05:10,479
the two figures

128
00:05:10,479 --> 00:05:13,039
we saw examples of explore or exploit

129
00:05:13,039 --> 00:05:13,759
but what

130
00:05:13,759 --> 00:05:16,320
is required to enact something that is

131
00:05:16,320 --> 00:05:17,280
able to

132
00:05:17,280 --> 00:05:19,120
mediate between the two of them or find

133
00:05:19,120 --> 00:05:22,479
a compromise between those two

134
00:05:22,840 --> 00:05:24,560
strategies anyone

135
00:05:24,560 --> 00:05:26,479
have any thoughts or want to raise their

136
00:05:26,479 --> 00:05:28,800
hand

137
00:05:30,000 --> 00:05:33,360
steven yeah i'm also excited about the

138
00:05:33,360 --> 00:05:34,400
explore

139
00:05:34,400 --> 00:05:36,960
um and the exploit side as well to be

140
00:05:36,960 --> 00:05:38,400
honest because i think

141
00:05:38,400 --> 00:05:42,160
that opens up a big shift in the way

142
00:05:42,160 --> 00:05:43,840
that we socially interact in the way

143
00:05:43,840 --> 00:05:45,759
that we organize ourselves

144
00:05:45,759 --> 00:05:48,240
so trying to look at a real foundational

145
00:05:48,240 --> 00:05:50,320
way to sort of approach that

146
00:05:50,320 --> 00:05:52,160
is going to be quite helpful i think in

147
00:05:52,160 --> 00:05:54,880
a lot of fields of practice

148
00:05:54,880 --> 00:05:56,800
cool alec and then anyone else who

149
00:05:56,800 --> 00:05:58,800
raises their hand

150
00:05:58,800 --> 00:06:01,199
yeah am i i'm neither don't need to know

151
00:06:01,199 --> 00:06:03,199
you sound good sound good

152
00:06:03,199 --> 00:06:05,360
uh i just wanted to comment on your

153
00:06:05,360 --> 00:06:06,319
comment

154
00:06:06,319 --> 00:06:08,560
because i'm also very interested in that

155
00:06:08,560 --> 00:06:10,800
but i guess i'll go on the

156
00:06:10,800 --> 00:06:13,520
record saying that i'm not not convinced

157
00:06:13,520 --> 00:06:15,120
that active inference

158
00:06:15,120 --> 00:06:17,039
provides sometimes it's said to provide

159
00:06:17,039 --> 00:06:19,039
the solution for the explore exploit

160
00:06:19,039 --> 00:06:20,560
that and i i don't

161
00:06:20,560 --> 00:06:22,960
think it does uh i think it really

162
00:06:22,960 --> 00:06:24,240
recasts it so

163
00:06:24,240 --> 00:06:25,520
you know now it's not a question of

164
00:06:25,520 --> 00:06:27,840
bouncing

165
00:06:27,840 --> 00:06:30,240
exploration exploitation but uh they're

166
00:06:30,240 --> 00:06:31,840
both zoomed into this

167
00:06:31,840 --> 00:06:33,280
different objective function but in

168
00:06:33,280 --> 00:06:36,960
practice uh

169
00:06:37,199 --> 00:06:39,520
there isn't a sort of magic balancing

170
00:06:39,520 --> 00:06:40,240
act that you

171
00:06:40,240 --> 00:06:42,400
you get out of this um so i think it's a

172
00:06:42,400 --> 00:06:44,000
good way to recast the exploration

173
00:06:44,000 --> 00:06:44,960
expectations

174
00:06:44,960 --> 00:06:46,560
but certainly not a solution to the

175
00:06:46,560 --> 00:06:48,240
problem awesome

176
00:06:48,240 --> 00:06:52,960
blue so i kind of have like a tangential

177
00:06:52,960 --> 00:06:55,199
question i and thank you alex for your

178
00:06:55,199 --> 00:06:56,080
comments

179
00:06:56,080 --> 00:06:58,479
because it just plays right into i think

180
00:06:58,479 --> 00:06:59,440
what's missing

181
00:06:59,440 --> 00:07:01,680
it's very clear that um if the

182
00:07:01,680 --> 00:07:03,360
environment is homogeneous

183
00:07:03,360 --> 00:07:06,400
there's no real need for exploration

184
00:07:06,400 --> 00:07:07,039
because

185
00:07:07,039 --> 00:07:09,199
the agent is not going to gain any you

186
00:07:09,199 --> 00:07:11,360
know any additional progress by

187
00:07:11,360 --> 00:07:12,880
exploring the environment but

188
00:07:12,880 --> 00:07:15,520
i just wonder where autonomy factors in

189
00:07:15,520 --> 00:07:16,000
right like

190
00:07:16,000 --> 00:07:18,240
so you know some agents have more or

191
00:07:18,240 --> 00:07:20,800
less control over their environment

192
00:07:20,800 --> 00:07:22,639
and there's no like real room for this

193
00:07:22,639 --> 00:07:24,639
or not that i've seen anyway in

194
00:07:24,639 --> 00:07:27,120
the um active inference framework like

195
00:07:27,120 --> 00:07:28,080
where the

196
00:07:28,080 --> 00:07:29,599
differing levels of autonomy of the

197
00:07:29,599 --> 00:07:31,120
agent play into

198
00:07:31,120 --> 00:07:34,960
the need to explore versus exploit

199
00:07:35,520 --> 00:07:40,400
cool good question stephen

200
00:07:40,479 --> 00:07:43,120
yeah i think that that point about how

201
00:07:43,120 --> 00:07:44,160
to

202
00:07:44,160 --> 00:07:46,720
look at the agent starts to bring in the

203
00:07:46,720 --> 00:07:47,919
inactive

204
00:07:47,919 --> 00:07:49,520
component a lot more i think and the

205
00:07:49,520 --> 00:07:51,280
generative model that's kind of sitting

206
00:07:51,280 --> 00:07:52,319
inside

207
00:07:52,319 --> 00:07:54,400
the person and existential kind of

208
00:07:54,400 --> 00:07:55,520
sense-making

209
00:07:55,520 --> 00:07:58,000
so i think that's that also yeah it's

210
00:07:58,000 --> 00:07:59,840
like pushing this back

211
00:07:59,840 --> 00:08:02,240
from a real world perspective and saying

212
00:08:02,240 --> 00:08:02,960
okay

213
00:08:02,960 --> 00:08:05,280
maybe we need to work back the other way

214
00:08:05,280 --> 00:08:06,879
from complex

215
00:08:06,879 --> 00:08:09,759
multi-sensory experiences towards active

216
00:08:09,759 --> 00:08:11,120
inference as well as

217
00:08:11,120 --> 00:08:14,080
using active inference to do modeling of

218
00:08:14,080 --> 00:08:16,080
maybe kind of very

219
00:08:16,080 --> 00:08:19,360
clean sort of low dimensional

220
00:08:19,360 --> 00:08:22,000
approaches it's like should we also be

221
00:08:22,000 --> 00:08:24,080
coming back from the other direction

222
00:08:24,080 --> 00:08:26,720
um and i did i've got an activity called

223
00:08:26,720 --> 00:08:28,879
conceptual action sociology an activity

224
00:08:28,879 --> 00:08:30,560
where people move around space to

225
00:08:30,560 --> 00:08:31,919
understand

226
00:08:31,919 --> 00:08:34,559
how their orientation is during a moment

227
00:08:34,559 --> 00:08:36,000
of encounter

228
00:08:36,000 --> 00:08:38,399
and i've asked people to unpack it in

229
00:08:38,399 --> 00:08:40,399
terms of the three different areas of

230
00:08:40,399 --> 00:08:42,399
active inference the sort of pragmatic

231
00:08:42,399 --> 00:08:43,599
gain

232
00:08:43,599 --> 00:08:47,120
epistemic foraging or or

233
00:08:47,120 --> 00:08:49,680
risk mitigation or salience and it's

234
00:08:49,680 --> 00:08:51,040
actually really interesting

235
00:08:51,040 --> 00:08:52,640
when they break it down that way and

236
00:08:52,640 --> 00:08:54,560
start to think about what was i trying

237
00:08:54,560 --> 00:08:55,920
to do

238
00:08:55,920 --> 00:08:58,399
but from that kind of high dimensional

239
00:08:58,399 --> 00:08:59,600
kind of

240
00:08:59,600 --> 00:09:02,080
synthesis of being in a really immersive

241
00:09:02,080 --> 00:09:02,880
moment

242
00:09:02,880 --> 00:09:04,800
and they actually people seem to who

243
00:09:04,800 --> 00:09:05,839
don't know anything about active

244
00:09:05,839 --> 00:09:06,720
inference

245
00:09:06,720 --> 00:09:08,640
actually we're quite intuitively seeing

246
00:09:08,640 --> 00:09:10,959
quite a lot of shifts in those three

247
00:09:10,959 --> 00:09:12,880
different areas of focus depending on

248
00:09:12,880 --> 00:09:14,160
the type of

249
00:09:14,160 --> 00:09:16,880
um event or moment that they were

250
00:09:16,880 --> 00:09:17,839
reflecting on

251
00:09:17,839 --> 00:09:19,760
because they were going back to an event

252
00:09:19,760 --> 00:09:21,360
and thinking about the focus of their

253
00:09:21,360 --> 00:09:22,560
attention at that

254
00:09:22,560 --> 00:09:24,480
particular moment in time and how much

255
00:09:24,480 --> 00:09:26,480
were they sort of bringing these three

256
00:09:26,480 --> 00:09:27,600
areas in

257
00:09:27,600 --> 00:09:29,200
and then i suppose between that you

258
00:09:29,200 --> 00:09:31,200
could infer something about

259
00:09:31,200 --> 00:09:34,800
the exploration sort of

260
00:09:34,800 --> 00:09:38,800
desire to learn or the exploitation

261
00:09:38,800 --> 00:09:41,519
gain but i i agree i think it does

262
00:09:41,519 --> 00:09:43,519
depend on the moment that's in question

263
00:09:43,519 --> 00:09:46,480
so yeah there's a few things that kind

264
00:09:46,480 --> 00:09:47,519
of this is sort of

265
00:09:47,519 --> 00:09:49,040
tapping into which i'm really interested

266
00:09:49,040 --> 00:09:50,560
in cool

267
00:09:50,560 --> 00:09:52,320
and to tie that just together with a

268
00:09:52,320 --> 00:09:54,800
recast idea from alec

269
00:09:54,800 --> 00:09:56,640
it's um kind of like nature nurture

270
00:09:56,640 --> 00:09:58,000
explore exploit

271
00:09:58,000 --> 00:09:59,600
people don't know about these binaries

272
00:09:59,600 --> 00:10:01,120
they're not natural kinds until they're

273
00:10:01,120 --> 00:10:01,839
taught them

274
00:10:01,839 --> 00:10:03,760
and then the resolution to these kinds

275
00:10:03,760 --> 00:10:06,800
of dichotomies or binaries false or not

276
00:10:06,800 --> 00:10:08,880
is not some parameter that varies from

277
00:10:08,880 --> 00:10:10,480
zero to one that trades off

278
00:10:10,480 --> 00:10:12,880
for example when evelyn fox keller was

279
00:10:12,880 --> 00:10:14,480
moving beyond nature and nurture it's

280
00:10:14,480 --> 00:10:16,399
the mirage of a space between it's about

281
00:10:16,399 --> 00:10:18,399
reconsidering the situation

282
00:10:18,399 --> 00:10:20,160
so that there doesn't have to be

283
00:10:20,160 --> 00:10:21,760
something like a bipartition

284
00:10:21,760 --> 00:10:23,519
so that's definitely the direction that

285
00:10:23,519 --> 00:10:25,200
we will try to take it because it's a

286
00:10:25,200 --> 00:10:26,320
really cool idea

287
00:10:26,320 --> 00:10:28,399
the second or i guess the last warm-up

288
00:10:28,399 --> 00:10:29,519
question is

289
00:10:29,519 --> 00:10:31,920
and on a related note what would be an

290
00:10:31,920 --> 00:10:32,720
interesting

291
00:10:32,720 --> 00:10:35,360
control system or any system to model

292
00:10:35,360 --> 00:10:36,640
with active inference

293
00:10:36,640 --> 00:10:38,720
so we're going to go into like a trolley

294
00:10:38,720 --> 00:10:40,000
car and a

295
00:10:40,000 --> 00:10:43,120
um hopper i guess and a pendulum

296
00:10:43,120 --> 00:10:44,480
and i think it'll be fun to hear about

297
00:10:44,480 --> 00:10:46,399
those but um we heard

298
00:10:46,399 --> 00:10:49,200
about this the social and the spatial

299
00:10:49,200 --> 00:10:50,640
there's questions that we've raised

300
00:10:50,640 --> 00:10:51,680
about biology

301
00:10:51,680 --> 00:10:53,680
communication anything else that

302
00:10:53,680 --> 00:10:54,720
somebody wants to

303
00:10:54,720 --> 00:10:58,000
bring up or whether it's yeah mel please

304
00:10:58,000 --> 00:11:01,760
and then sasha yeah um

305
00:11:01,760 --> 00:11:04,079
so i'm what i'm excited about i'm really

306
00:11:04,079 --> 00:11:05,680
excited about

307
00:11:05,680 --> 00:11:09,440
uh this kind of work um

308
00:11:09,440 --> 00:11:13,279
as i'm a philosopher but um

309
00:11:13,279 --> 00:11:15,360
with this sort of stuff the accident

310
00:11:15,360 --> 00:11:17,279
prince fep stuff i'm like please stop

311
00:11:17,279 --> 00:11:19,519
theorizing just like

312
00:11:19,519 --> 00:11:22,480
let's play with this um so i'm really

313
00:11:22,480 --> 00:11:24,480
excited about this kind of work

314
00:11:24,480 --> 00:11:27,519
and um the question that's motivating me

315
00:11:27,519 --> 00:11:30,160
is really sort of figuring out um what

316
00:11:30,160 --> 00:11:31,120
the boundaries

317
00:11:31,120 --> 00:11:33,760
of these various models are so what

318
00:11:33,760 --> 00:11:36,480
boundaries of active instruments

319
00:11:36,480 --> 00:11:38,160
are relative to the free energy

320
00:11:38,160 --> 00:11:40,160
principle and then relative to

321
00:11:40,160 --> 00:11:43,920
eg predictive coding models and that

322
00:11:43,920 --> 00:11:44,880
sort of thing

323
00:11:44,880 --> 00:11:48,000
um someone brought

324
00:11:48,000 --> 00:11:51,839
recently on twitter i guess

325
00:11:51,839 --> 00:11:54,880
there's a there's a biologist at my

326
00:11:54,880 --> 00:11:57,519
university um this thing really kind of

327
00:11:57,519 --> 00:11:59,120
groundbreaking work

328
00:11:59,120 --> 00:12:02,959
in in like perception cognition um

329
00:12:02,959 --> 00:12:05,940
perceptual cognition and stuff um in

330
00:12:05,940 --> 00:12:07,120
[Music]

331
00:12:07,120 --> 00:12:10,000
i guess butterflies but mostly uh

332
00:12:10,000 --> 00:12:11,120
jumping spiders so

333
00:12:11,120 --> 00:12:15,200
to say um the the morehouse lab nate

334
00:12:15,200 --> 00:12:16,240
moore house

335
00:12:16,240 --> 00:12:18,880
is doing for example um soltista has

336
00:12:18,880 --> 00:12:19,600
like very

337
00:12:19,600 --> 00:12:22,800
uh elaborate like um

338
00:12:22,800 --> 00:12:24,959
mating dances and things and they're

339
00:12:24,959 --> 00:12:27,040
signaling to each other um

340
00:12:27,040 --> 00:12:29,440
and they're doing stuff like like the

341
00:12:29,440 --> 00:12:31,200
evolution of color perception

342
00:12:31,200 --> 00:12:33,920
in in these jumping spiders um it's

343
00:12:33,920 --> 00:12:35,360
really fascinating work

344
00:12:35,360 --> 00:12:38,880
and they mentioned recently um

345
00:12:38,880 --> 00:12:42,160
or nate mentioned recently that um the

346
00:12:42,160 --> 00:12:42,800
lab is

347
00:12:42,800 --> 00:12:46,480
doing some like simulation work on

348
00:12:46,480 --> 00:12:49,360
on how we evolve additional color

349
00:12:49,360 --> 00:12:50,839
percepts

350
00:12:50,839 --> 00:12:54,160
um so so you can right you can

351
00:12:54,160 --> 00:12:57,519
you can if you have uh

352
00:12:57,519 --> 00:13:00,639
certain like genes and proteins for for

353
00:13:00,639 --> 00:13:01,040
um

354
00:13:01,040 --> 00:13:02,959
pursuing certain colors you can you can

355
00:13:02,959 --> 00:13:04,839
kind of like shift

356
00:13:04,839 --> 00:13:07,839
the probability distribution the

357
00:13:07,839 --> 00:13:11,279
the range of of color that those pick up

358
00:13:11,279 --> 00:13:13,360
right um that's something that that

359
00:13:13,360 --> 00:13:16,000
evolution can kind of

360
00:13:16,000 --> 00:13:18,959
modulate but um the evolution of an

361
00:13:18,959 --> 00:13:20,399
entirely different

362
00:13:20,399 --> 00:13:23,440
um an entirely new color percept is is

363
00:13:23,440 --> 00:13:24,880
something

364
00:13:24,880 --> 00:13:28,639
different um and so

365
00:13:28,639 --> 00:13:30,399
they're doing some some like insilico

366
00:13:30,399 --> 00:13:32,079
work on that

367
00:13:32,079 --> 00:13:34,320
and i i thought yeah that's what that's

368
00:13:34,320 --> 00:13:35,680
what active influence that's exactly

369
00:13:35,680 --> 00:13:36,959
what active influence should we do

370
00:13:36,959 --> 00:13:38,000
that's what we should be doing it's

371
00:13:38,000 --> 00:13:39,839
great

372
00:13:39,839 --> 00:13:41,040
so i think that's what we should be

373
00:13:41,040 --> 00:13:43,360
striving for thanks for that

374
00:13:43,360 --> 00:13:45,920
very interesting summary sasha and then

375
00:13:45,920 --> 00:13:47,440
anyone else

376
00:13:47,440 --> 00:13:49,839
um thank you mel for bringing up the

377
00:13:49,839 --> 00:13:50,800
jumping spiders

378
00:13:50,800 --> 00:13:53,120
um they have incredible eyes you should

379
00:13:53,120 --> 00:13:54,079
all look it up

380
00:13:54,079 --> 00:13:58,160
um but i i'm interested in this

381
00:13:58,160 --> 00:14:00,720
from a developmental perspective and

382
00:14:00,720 --> 00:14:02,000
what active inference

383
00:14:02,000 --> 00:14:05,760
can contribute to um

384
00:14:05,760 --> 00:14:08,800
how systems develop and communicate

385
00:14:08,800 --> 00:14:11,920
and specifically uh humans

386
00:14:11,920 --> 00:14:15,040
um how our in utero environment and the

387
00:14:15,040 --> 00:14:16,720
activity

388
00:14:16,720 --> 00:14:20,560
of that process shapes the development

389
00:14:20,560 --> 00:14:21,360
of

390
00:14:21,360 --> 00:14:25,120
the human um and kind of on a more broad

391
00:14:25,120 --> 00:14:28,480
scale how this can play into teaching

392
00:14:28,480 --> 00:14:30,880
and how active inference can be applied

393
00:14:30,880 --> 00:14:32,000
in the classroom

394
00:14:32,000 --> 00:14:34,320
at different educational levels but in a

395
00:14:34,320 --> 00:14:36,160
way that um

396
00:14:36,160 --> 00:14:38,959
sets up the environment such that agents

397
00:14:38,959 --> 00:14:40,079
are

398
00:14:40,079 --> 00:14:42,720
encouraged to explore instead of being

399
00:14:42,720 --> 00:14:44,240
dragged through the material

400
00:14:44,240 --> 00:14:47,680
as uh students often are

401
00:14:47,680 --> 00:14:50,560
so that's that's my curiosity in this

402
00:14:50,560 --> 00:14:52,800
kind of you got this game

403
00:14:52,800 --> 00:14:56,839
cool yes a lot of vygotsky fans

404
00:14:56,839 --> 00:14:59,600
slash colleagues through time

405
00:14:59,600 --> 00:15:03,440
on this discussion any other comments

406
00:15:03,440 --> 00:15:04,880
but really great points there about like

407
00:15:04,880 --> 00:15:06,639
the evolution and the development of

408
00:15:06,639 --> 00:15:07,839
perception

409
00:15:07,839 --> 00:15:10,000
and then at the level that we can at

410
00:15:10,000 --> 00:15:11,040
least experience

411
00:15:11,040 --> 00:15:13,279
or enact agency can we choose to see a

412
00:15:13,279 --> 00:15:15,040
color is different can you look at that

413
00:15:15,040 --> 00:15:17,040
ambiguous pattern and choose to see it

414
00:15:17,040 --> 00:15:18,480
one way or another

415
00:15:18,480 --> 00:15:20,720
can we understand uh many of these

416
00:15:20,720 --> 00:15:22,079
perceptive questions

417
00:15:22,079 --> 00:15:24,639
as being related to maybe not simply

418
00:15:24,639 --> 00:15:26,880
explore exploit maybe having an element

419
00:15:26,880 --> 00:15:27,839
of multi-scale

420
00:15:27,839 --> 00:15:29,680
agency i think there's a lot of ways

421
00:15:29,680 --> 00:15:31,519
that that could play out in people's

422
00:15:31,519 --> 00:15:31,920
life

423
00:15:31,920 --> 00:15:33,680
while also recognizing that we're not

424
00:15:33,680 --> 00:15:35,759
talking about just metaphors here or

425
00:15:35,759 --> 00:15:36,399
narratives

426
00:15:36,399 --> 00:15:38,560
we're also talking about this specific

427
00:15:38,560 --> 00:15:40,000
framework with the paper that we're

428
00:15:40,000 --> 00:15:40,800
going to discuss

429
00:15:40,800 --> 00:15:42,560
now so let's talk about those details

430
00:15:42,560 --> 00:15:44,959
but then maybe you know we can dip back

431
00:15:44,959 --> 00:15:46,639
to the other systems but let's think

432
00:15:46,639 --> 00:15:48,480
about how this paper

433
00:15:48,480 --> 00:15:50,639
and also our special time with alec will

434
00:15:50,639 --> 00:15:52,160
help us understand these broader

435
00:15:52,160 --> 00:15:53,279
questions at least

436
00:15:53,279 --> 00:15:55,279
where we want to go after this short

437
00:15:55,279 --> 00:15:57,120
discussion so the paper

438
00:15:57,120 --> 00:15:59,199
scaling active inference we read the

439
00:15:59,199 --> 00:16:00,560
goal last time

440
00:16:00,560 --> 00:16:04,079
um alec maybe if you could just however

441
00:16:04,079 --> 00:16:06,320
you want to or however long just what

442
00:16:06,320 --> 00:16:08,000
what were you setting out to do or how

443
00:16:08,000 --> 00:16:09,759
did the collaboration come to be

444
00:16:09,759 --> 00:16:11,839
what were you asking before how did you

445
00:16:11,839 --> 00:16:15,440
get to this paper and this goal

446
00:16:15,600 --> 00:16:19,120
um so i guess historically the sort of

447
00:16:19,120 --> 00:16:19,839
historic

448
00:16:19,839 --> 00:16:23,040
uh lead up to the paper was just i'm

449
00:16:23,040 --> 00:16:24,480
very interested in reinforcement

450
00:16:24,480 --> 00:16:26,399
learning machine learning and also very

451
00:16:26,399 --> 00:16:29,120
interested in active inference so it was

452
00:16:29,120 --> 00:16:29,680
a

453
00:16:29,680 --> 00:16:33,600
those sort of uh natural overlap but in

454
00:16:33,600 --> 00:16:34,399
terms of

455
00:16:34,399 --> 00:16:37,839
the motivation i think

456
00:16:37,920 --> 00:16:39,519
the two communities don't really speak

457
00:16:39,519 --> 00:16:40,959
to each other

458
00:16:40,959 --> 00:16:44,320
as much as they should and so much of

459
00:16:44,320 --> 00:16:45,440
the work that's happening

460
00:16:45,440 --> 00:16:49,040
in reinforcement learning um

461
00:16:49,040 --> 00:16:50,480
especially model-based reinforcement

462
00:16:50,480 --> 00:16:52,959
learning has direct analogies to the

463
00:16:52,959 --> 00:16:54,160
work that's being developed in the

464
00:16:54,160 --> 00:16:55,519
active inference community

465
00:16:55,519 --> 00:16:59,600
um so stuff like the use of variational

466
00:16:59,600 --> 00:17:01,519
methods variational auto encoders the

467
00:17:01,519 --> 00:17:02,079
use of

468
00:17:02,079 --> 00:17:03,440
dynamics models the use of

469
00:17:03,440 --> 00:17:05,359
trajectory-based planning

470
00:17:05,359 --> 00:17:07,760
uh or just planning in general these

471
00:17:07,760 --> 00:17:08,480
algorithms

472
00:17:08,480 --> 00:17:10,000
have a lot of similarities between the

473
00:17:10,000 --> 00:17:12,480
two um the use of intrinsic

474
00:17:12,480 --> 00:17:14,640
objectives these information game terms

475
00:17:14,640 --> 00:17:16,319
is widespread in both the use of

476
00:17:16,319 --> 00:17:17,760
belief-based planning

477
00:17:17,760 --> 00:17:18,880
so all the things that we sort of

478
00:17:18,880 --> 00:17:20,959
touched on on the paper so

479
00:17:20,959 --> 00:17:22,640
in my mind when i started that out that

480
00:17:22,640 --> 00:17:24,480
the aim was really just to

481
00:17:24,480 --> 00:17:27,599
construct an agent using the machinery

482
00:17:27,599 --> 00:17:28,720
that would be

483
00:17:28,720 --> 00:17:31,679
common and um relatable to the machine

484
00:17:31,679 --> 00:17:32,000
learning

485
00:17:32,000 --> 00:17:34,080
crowd but that was consistent with the

486
00:17:34,080 --> 00:17:35,840
active influence very much

487
00:17:35,840 --> 00:17:37,600
um and i guess the secondary goal was

488
00:17:37,600 --> 00:17:39,918
also

489
00:17:40,080 --> 00:17:42,559
to put together an initial attempt to

490
00:17:42,559 --> 00:17:43,280
look at

491
00:17:43,280 --> 00:17:46,160
whether what needs to be put in to get

492
00:17:46,160 --> 00:17:47,360
some of these ideas to

493
00:17:47,360 --> 00:17:50,960
scale um uh with a particular focus

494
00:17:50,960 --> 00:17:52,480
on you know this wasn't a biologically

495
00:17:52,480 --> 00:17:54,080
plausible

496
00:17:54,080 --> 00:17:56,480
suggestion but looking at some of the

497
00:17:56,480 --> 00:17:58,240
objective functions the expected free

498
00:17:58,240 --> 00:17:59,200
energy

499
00:17:59,200 --> 00:18:03,280
at scale um i think is of interest

500
00:18:03,280 --> 00:18:05,440
rather than you know where it's commonly

501
00:18:05,440 --> 00:18:06,320
looked at in these

502
00:18:06,320 --> 00:18:08,880
uh small discrete screen uh teammates

503
00:18:08,880 --> 00:18:11,360
style environments

504
00:18:11,360 --> 00:18:14,559
cool really helpful in a lot of

505
00:18:14,559 --> 00:18:15,919
terms that we've heard but to link

506
00:18:15,919 --> 00:18:18,160
together the ideas of the variational

507
00:18:18,160 --> 00:18:19,840
inference with the belief and the

508
00:18:19,840 --> 00:18:21,039
trajectory based

509
00:18:21,039 --> 00:18:23,440
planning those are all really cool

510
00:18:23,440 --> 00:18:24,080
things

511
00:18:24,080 --> 00:18:25,520
i just have a question about the at

512
00:18:25,520 --> 00:18:27,280
scale part so one

513
00:18:27,280 --> 00:18:28,960
aspect was that you introduced the

514
00:18:28,960 --> 00:18:30,880
continuous state space

515
00:18:30,880 --> 00:18:33,039
as opposed to the discrete but is there

516
00:18:33,039 --> 00:18:34,480
anything else that's meant by

517
00:18:34,480 --> 00:18:36,720
at scale or what does it mean to scale

518
00:18:36,720 --> 00:18:40,000
in this context

519
00:18:40,080 --> 00:18:43,120
well i mean so

520
00:18:43,120 --> 00:18:47,520
i guess the

521
00:18:47,679 --> 00:18:50,799
scale just means is almost defined in

522
00:18:50,799 --> 00:18:52,559
relation to what is currently

523
00:18:52,559 --> 00:18:54,080
uh common in the active influence

524
00:18:54,080 --> 00:18:55,520
literature which is where your entire

525
00:18:55,520 --> 00:18:56,799
world is

526
00:18:56,799 --> 00:19:00,240
often designed by four or five states um

527
00:19:00,240 --> 00:19:01,760
and there's transitions for you so this

528
00:19:01,760 --> 00:19:04,400
paper um

529
00:19:04,400 --> 00:19:06,080
presents a framework that can be applied

530
00:19:06,080 --> 00:19:08,840
to you know observations of

531
00:19:08,840 --> 00:19:11,760
um you know ten thousand a hundred

532
00:19:11,760 --> 00:19:12,720
thousand

533
00:19:12,720 --> 00:19:14,320
uh and so your state space or your

534
00:19:14,320 --> 00:19:16,640
observation space is much larger

535
00:19:16,640 --> 00:19:20,320
um so that's what i meant by scale is

536
00:19:20,320 --> 00:19:22,480
it's putting forward a framework that

537
00:19:22,480 --> 00:19:24,640
could be applied to a higher dimensional

538
00:19:24,640 --> 00:19:25,840
task than the ones that you

539
00:19:25,840 --> 00:19:27,280
can't consider and there's also a second

540
00:19:27,280 --> 00:19:29,200
thing which is not really captured in

541
00:19:29,200 --> 00:19:31,200
the word scale but also the complexity

542
00:19:31,200 --> 00:19:34,400
of the dynamics um so some of these

543
00:19:34,400 --> 00:19:36,320
tasks actually have quite a small state

544
00:19:36,320 --> 00:19:37,280
space you know

545
00:19:37,280 --> 00:19:39,280
four or five but the complexity of their

546
00:19:39,280 --> 00:19:41,679
dynamics requires

547
00:19:41,679 --> 00:19:44,480
um large models of a number of

548
00:19:44,480 --> 00:19:45,760
parameters to them

549
00:19:45,760 --> 00:19:48,880
um there's kind of a scaling aspect and

550
00:19:48,880 --> 00:19:50,160
there's scaling in

551
00:19:50,160 --> 00:19:54,960
the space of complexity of your diamonds

552
00:19:54,960 --> 00:19:58,080
cool very interesting stuff and at any

553
00:19:58,080 --> 00:19:59,360
point people can just

554
00:19:59,360 --> 00:20:01,360
raise their hand but i think we can keep

555
00:20:01,360 --> 00:20:02,720
on walking through

556
00:20:02,720 --> 00:20:06,000
these areas and last time we

557
00:20:06,000 --> 00:20:08,799
did walk through the different areas in

558
00:20:08,799 --> 00:20:09,919
the road map just

559
00:20:09,919 --> 00:20:11,919
how we introduce active inference and

560
00:20:11,919 --> 00:20:13,200
the current state

561
00:20:13,200 --> 00:20:14,960
and then walk through the specifics of

562
00:20:14,960 --> 00:20:16,400
the model show some

563
00:20:16,400 --> 00:20:18,400
proof of concept experiments relate it

564
00:20:18,400 --> 00:20:19,679
to previous work

565
00:20:19,679 --> 00:20:21,840
and then discuss and conclude with some

566
00:20:21,840 --> 00:20:24,159
of these sentiments related to what alec

567
00:20:24,159 --> 00:20:26,000
is saying and i also found it

568
00:20:26,000 --> 00:20:27,280
interesting there's the scaling

569
00:20:27,280 --> 00:20:28,640
dimensionality

570
00:20:28,640 --> 00:20:29,760
then there's the introduction of

571
00:20:29,760 --> 00:20:31,520
continuous state spaces not just

572
00:20:31,520 --> 00:20:32,240
discrete

573
00:20:32,240 --> 00:20:34,400
and then often as machine learning

574
00:20:34,400 --> 00:20:35,360
people might

575
00:20:35,360 --> 00:20:37,360
use the word scaling to mean it's about

576
00:20:37,360 --> 00:20:39,200
the number of observations the size of

577
00:20:39,200 --> 00:20:40,320
the data set

578
00:20:40,320 --> 00:20:42,720
um and that relates to like the compute

579
00:20:42,720 --> 00:20:43,919
scaling relationship

580
00:20:43,919 --> 00:20:46,000
as you add when you double the data set

581
00:20:46,000 --> 00:20:48,159
input is it the same amount of training

582
00:20:48,159 --> 00:20:49,919
time is it twice as long is it four

583
00:20:49,919 --> 00:20:51,679
times as long is it four thousand times

584
00:20:51,679 --> 00:20:52,640
as long

585
00:20:52,640 --> 00:20:55,120
and um so really interesting because

586
00:20:55,120 --> 00:20:57,280
there's also the scaling in terms of

587
00:20:57,280 --> 00:20:59,280
the understanding in the world and

588
00:20:59,280 --> 00:21:00,960
people talk about scaling and innovation

589
00:21:00,960 --> 00:21:02,320
and entrepreneurship and how do you

590
00:21:02,320 --> 00:21:03,679
scale a solution

591
00:21:03,679 --> 00:21:05,760
so there's a lot of parallel meanings

592
00:21:05,760 --> 00:21:06,880
and a fun

593
00:21:06,880 --> 00:21:09,840
and concise title because uh you know

594
00:21:09,840 --> 00:21:11,360
that the finding isn't going to be in

595
00:21:11,360 --> 00:21:12,000
the title

596
00:21:12,000 --> 00:21:13,919
there isn't just a simple way that says

597
00:21:13,919 --> 00:21:15,520
oh this protein does this in this

598
00:21:15,520 --> 00:21:16,159
species

599
00:21:16,159 --> 00:21:18,000
we're talking about developments in a

600
00:21:18,000 --> 00:21:19,200
modeling framework

601
00:21:19,200 --> 00:21:21,039
but we also want to be specific about

602
00:21:21,039 --> 00:21:22,640
what those advances are

603
00:21:22,640 --> 00:21:24,799
and how they relate to systems so maybe

604
00:21:24,799 --> 00:21:26,080
just on the roadmap

605
00:21:26,080 --> 00:21:29,200
alec or anyone else just anything to say

606
00:21:29,200 --> 00:21:30,159
about it or

607
00:21:30,159 --> 00:21:32,480
why was it arranged this way personally

608
00:21:32,480 --> 00:21:33,840
i was just kind of wondering why the

609
00:21:33,840 --> 00:21:35,679
previous work is at the end is that a

610
00:21:35,679 --> 00:21:36,960
difference in our um

611
00:21:36,960 --> 00:21:39,120
academic conventions or is that just

612
00:21:39,120 --> 00:21:41,919
because it was more results oriented

613
00:21:41,919 --> 00:21:43,120
yeah i mean you don't get a lot of

614
00:21:43,120 --> 00:21:45,679
leeway in machine learning style papers

615
00:21:45,679 --> 00:21:48,320
um i mean previous work and i'd go after

616
00:21:48,320 --> 00:21:50,720
the introduction or

617
00:21:50,720 --> 00:21:54,000
at the end but it's always you know

618
00:21:54,000 --> 00:21:56,960
introduction some methods or presenting

619
00:21:56,960 --> 00:21:57,679
a model

620
00:21:57,679 --> 00:21:59,600
normally quite short and tests from the

621
00:21:59,600 --> 00:22:01,120
experiments thing previously

622
00:22:01,120 --> 00:22:01,679
exceptionally

623
00:22:01,679 --> 00:22:04,799
very short discussion that's kind of

624
00:22:04,799 --> 00:22:07,679
stunned across the field but um the

625
00:22:07,679 --> 00:22:09,120
reason i think i chose previously at the

626
00:22:09,120 --> 00:22:10,400
end was just because

627
00:22:10,400 --> 00:22:12,080
i discussed active inference which

628
00:22:12,080 --> 00:22:13,600
naturally leads him from the

629
00:22:13,600 --> 00:22:14,320
introduction

630
00:22:14,320 --> 00:22:16,240
and that introduces a little bit of mass

631
00:22:16,240 --> 00:22:18,080
which then naturally flows into the

632
00:22:18,080 --> 00:22:18,480
model

633
00:22:18,480 --> 00:22:19,919
for like a previous work might have

634
00:22:19,919 --> 00:22:21,919
broken up that flow

635
00:22:21,919 --> 00:22:25,280
uh a little bit but it was more or less

636
00:22:25,280 --> 00:22:29,039
cool um makes sense um i think we can

637
00:22:29,039 --> 00:22:32,320
walk through the next sections

638
00:22:32,320 --> 00:22:35,360
so right now we're in the just open

639
00:22:35,360 --> 00:22:38,880
pause for questions i have a few

640
00:22:38,880 --> 00:22:41,120
written down just as far as just big

641
00:22:41,120 --> 00:22:42,640
questions because

642
00:22:42,640 --> 00:22:45,200
we're trying to be a bridge in this

643
00:22:45,200 --> 00:22:46,000
discussion

644
00:22:46,000 --> 00:22:48,400
and through our conversation between

645
00:22:48,400 --> 00:22:50,000
those who might be hearing about active

646
00:22:50,000 --> 00:22:51,520
inference for the first time but they're

647
00:22:51,520 --> 00:22:52,880
coming from machine learning

648
00:22:52,880 --> 00:22:54,960
thinking about how cool methods perhaps

649
00:22:54,960 --> 00:22:56,799
with philosophical implications

650
00:22:56,799 --> 00:22:59,440
might be utilized and there's also a lot

651
00:22:59,440 --> 00:23:00,640
of people who are in the active

652
00:23:00,640 --> 00:23:01,200
inference

653
00:23:01,200 --> 00:23:03,360
side of things or the inactivism side or

654
00:23:03,360 --> 00:23:04,559
the philosophy side

655
00:23:04,559 --> 00:23:06,640
where the details the machine learning

656
00:23:06,640 --> 00:23:08,159
might be their first time hearing about

657
00:23:08,159 --> 00:23:08,799
it so

658
00:23:08,799 --> 00:23:11,280
how will we just come from where we are

659
00:23:11,280 --> 00:23:13,280
and think about what questions and

660
00:23:13,280 --> 00:23:15,360
what kinds of fun things to talk about

661
00:23:15,360 --> 00:23:16,799
we'll make that rich

662
00:23:16,799 --> 00:23:18,640
steven first and then anyone who wants

663
00:23:18,640 --> 00:23:20,480
to

664
00:23:20,480 --> 00:23:22,559
yeah i'm just curious because i know

665
00:23:22,559 --> 00:23:24,720
that the scaling question

666
00:23:24,720 --> 00:23:27,360
has been maybe looked at more from a

667
00:23:27,360 --> 00:23:28,640
philosophical

668
00:23:28,640 --> 00:23:30,240
perspective in the kind of active

669
00:23:30,240 --> 00:23:32,559
inference so what this paper really

670
00:23:32,559 --> 00:23:33,120
helped me

671
00:23:33,120 --> 00:23:36,400
do or helped me was it like broke that

672
00:23:36,400 --> 00:23:37,120
fear of

673
00:23:37,120 --> 00:23:40,480
questioning the kind of these models

674
00:23:40,480 --> 00:23:42,480
because the like people like yourself

675
00:23:42,480 --> 00:23:43,600
and um

676
00:23:43,600 --> 00:23:45,679
anil and that are saying look there's a

677
00:23:45,679 --> 00:23:47,600
challenge here in the scaling

678
00:23:47,600 --> 00:23:50,000
like this stuff is being kind of used at

679
00:23:50,000 --> 00:23:51,840
these low dimensional

680
00:23:51,840 --> 00:23:55,039
spaces and basically it showed like a

681
00:23:55,039 --> 00:23:55,600
gap

682
00:23:55,600 --> 00:23:57,440
in the literature in terms of how to

683
00:23:57,440 --> 00:23:59,520
action that which kind of

684
00:23:59,520 --> 00:24:01,120
helped me because it sort of gave me a

685
00:24:01,120 --> 00:24:02,960
bit of permission to say okay

686
00:24:02,960 --> 00:24:05,200
it's okay there's a gap there and it's

687
00:24:05,200 --> 00:24:06,799
not that i just don't understand

688
00:24:06,799 --> 00:24:07,279
everything

689
00:24:07,279 --> 00:24:09,520
it's just not easy to scale and i think

690
00:24:09,520 --> 00:24:10,880
this paper was

691
00:24:10,880 --> 00:24:12,720
the first one to explicitly say that

692
00:24:12,720 --> 00:24:14,720
because all the other papers tend to

693
00:24:14,720 --> 00:24:17,440
talk about yes we know how to scale but

694
00:24:17,440 --> 00:24:19,360
in a philosophical way

695
00:24:19,360 --> 00:24:20,880
but this is like okay well this is the

696
00:24:20,880 --> 00:24:22,400
problem from a

697
00:24:22,400 --> 00:24:25,520
practice perspective

698
00:24:25,520 --> 00:24:28,159
cool just one note on that if anyone

699
00:24:28,159 --> 00:24:29,360
else wants to raise their hand

700
00:24:29,360 --> 00:24:31,120
i really love this idea where there's

701
00:24:31,120 --> 00:24:32,559
the gap in the literature

702
00:24:32,559 --> 00:24:34,400
and there's there's a billion gaps on

703
00:24:34,400 --> 00:24:36,320
literature um it's about which ones are

704
00:24:36,320 --> 00:24:37,840
salient and fundable

705
00:24:37,840 --> 00:24:39,760
and relevant so there's many gaps in

706
00:24:39,760 --> 00:24:41,440
literature and then when we find that

707
00:24:41,440 --> 00:24:41,840
gap

708
00:24:41,840 --> 00:24:44,720
which is often very niche like in a phd

709
00:24:44,720 --> 00:24:46,320
and i thought wow this data set has been

710
00:24:46,320 --> 00:24:48,080
collected from this ant species but not

711
00:24:48,080 --> 00:24:50,000
this other ant species there's a gap in

712
00:24:50,000 --> 00:24:51,279
literature we don't have this data set

713
00:24:51,279 --> 00:24:52,880
on this ant species

714
00:24:52,880 --> 00:24:55,679
and then seeing that as a opportunity

715
00:24:55,679 --> 00:24:56,799
and something that's okay

716
00:24:56,799 --> 00:24:59,120
even if it's not your traditional field

717
00:24:59,120 --> 00:25:00,000
of study

718
00:25:00,000 --> 00:25:02,240
it's okay because you're at the edge

719
00:25:02,240 --> 00:25:04,400
like at the gap with us

720
00:25:04,400 --> 00:25:05,919
talking to the author and talking

721
00:25:05,919 --> 00:25:07,600
through the equations but this is what

722
00:25:07,600 --> 00:25:08,400
it looks like to

723
00:25:08,400 --> 00:25:11,360
look across the gap and try to connect

724
00:25:11,360 --> 00:25:12,960
because somebody else has identified it

725
00:25:12,960 --> 00:25:14,880
as a research question or maybe you have

726
00:25:14,880 --> 00:25:18,240
alec yeah

727
00:25:18,240 --> 00:25:19,440
so i just wanted to comment on that as

728
00:25:19,440 --> 00:25:21,679
well and uh entirely agree

729
00:25:21,679 --> 00:25:23,679
with what you're saying um to come to

730
00:25:23,679 --> 00:25:24,720
maybe the defense

731
00:25:24,720 --> 00:25:28,240
of uh the existing literature on active

732
00:25:28,240 --> 00:25:30,320
inference i think it's

733
00:25:30,320 --> 00:25:32,000
viewed a little bit or this maybe it's

734
00:25:32,000 --> 00:25:33,600
just me interpreting

735
00:25:33,600 --> 00:25:36,240
what they're saying but um that may be

736
00:25:36,240 --> 00:25:38,400
the the problems that they're

737
00:25:38,400 --> 00:25:41,039
um testing their agents in for instance

738
00:25:41,039 --> 00:25:42,559
some of the teammates

739
00:25:42,559 --> 00:25:45,679
are actually viewed uh as quite complex

740
00:25:45,679 --> 00:25:46,320
in a different

741
00:25:46,320 --> 00:25:49,919
dimension so a lot of these

742
00:25:49,919 --> 00:25:51,679
reinforcement learning tasks stuff like

743
00:25:51,679 --> 00:25:52,960
atari uh

744
00:25:52,960 --> 00:25:54,880
most of atari most of you know the stuff

745
00:25:54,880 --> 00:25:56,159
where you're really pushing to get

746
00:25:56,159 --> 00:25:57,520
state-of-the-art results with

747
00:25:57,520 --> 00:25:59,760
a huge amount of compute and deep mind

748
00:25:59,760 --> 00:26:01,120
competing against there

749
00:26:01,120 --> 00:26:03,440
they're often you can often get by with

750
00:26:03,440 --> 00:26:05,120
just this model-free reinforcement

751
00:26:05,120 --> 00:26:06,880
learning that doesn't require any notion

752
00:26:06,880 --> 00:26:09,360
of beliefs or any notion of proper

753
00:26:09,360 --> 00:26:10,720
epistemics you can get

754
00:26:10,720 --> 00:26:13,440
away with conduct explosions so what i

755
00:26:13,440 --> 00:26:15,600
think

756
00:26:15,760 --> 00:26:17,360
the active inference community has tried

757
00:26:17,360 --> 00:26:19,279
to do is look at simple tasks such as

758
00:26:19,279 --> 00:26:20,480
the teammates that

759
00:26:20,480 --> 00:26:22,400
actually well you can solve them without

760
00:26:22,400 --> 00:26:24,159
beliefs but are far far more

761
00:26:24,159 --> 00:26:25,600
amenable to being sold with a

762
00:26:25,600 --> 00:26:27,200
belief-based scheme and with this

763
00:26:27,200 --> 00:26:28,400
directed

764
00:26:28,400 --> 00:26:31,440
uh exploration and uncertainty reduction

765
00:26:31,440 --> 00:26:35,120
um to show the kind of complexity that

766
00:26:35,120 --> 00:26:37,120
the framework can come up

767
00:26:37,120 --> 00:26:40,320
but then you could also respond by

768
00:26:40,320 --> 00:26:42,480
saying that this is also thought about a

769
00:26:42,480 --> 00:26:45,279
lot in reinforcement learning and

770
00:26:45,279 --> 00:26:47,440
there is the question obviously whether

771
00:26:47,440 --> 00:26:48,559
the kind of

772
00:26:48,559 --> 00:26:50,320
process theories they put forward will

773
00:26:50,320 --> 00:26:51,600
apply

774
00:26:51,600 --> 00:26:54,080
when you've got uh when you've really

775
00:26:54,080 --> 00:26:55,600
got the scale that reinforcement

776
00:26:55,600 --> 00:26:57,760
learning

777
00:26:57,760 --> 00:26:59,679
yeah let me add on to that because it's

778
00:26:59,679 --> 00:27:01,200
a really great point so

779
00:27:01,200 --> 00:27:02,720
a lot of times what's perceived as

780
00:27:02,720 --> 00:27:04,640
cutting edge or modern or advanced

781
00:27:04,640 --> 00:27:06,080
research in machine learning

782
00:27:06,080 --> 00:27:08,960
i mean go to go check the lay media and

783
00:27:08,960 --> 00:27:10,400
it's going to be about this many

784
00:27:10,400 --> 00:27:12,400
graphical processors or this size of

785
00:27:12,400 --> 00:27:14,559
data set or this accuracy or

786
00:27:14,559 --> 00:27:17,200
this level of skill using massive data

787
00:27:17,200 --> 00:27:18,320
and it's very much

788
00:27:18,320 --> 00:27:21,039
on the performance end and on the eeking

789
00:27:21,039 --> 00:27:22,960
out a little bit more performance from

790
00:27:22,960 --> 00:27:24,480
increasingly large data sets with

791
00:27:24,480 --> 00:27:25,600
increasingly

792
00:27:25,600 --> 00:27:28,640
large types of models but

793
00:27:28,640 --> 00:27:30,640
it's actually in some sense a local

794
00:27:30,640 --> 00:27:31,760
exploration

795
00:27:31,760 --> 00:27:33,600
it's locally exploring certain

796
00:27:33,600 --> 00:27:35,520
frameworks and ways of doing machine

797
00:27:35,520 --> 00:27:36,000
learning

798
00:27:36,000 --> 00:27:38,799
which is just computer statistics and so

799
00:27:38,799 --> 00:27:39,760
what this is like

800
00:27:39,760 --> 00:27:41,600
with this paper is a return to

801
00:27:41,600 --> 00:27:43,520
simplicity and it's a return to a

802
00:27:43,520 --> 00:27:44,559
slightly different way of

803
00:27:44,559 --> 00:27:46,640
conceptualizing some of the parameters

804
00:27:46,640 --> 00:27:48,000
and how they're related

805
00:27:48,000 --> 00:27:50,159
and the big data just train it bigger

806
00:27:50,159 --> 00:27:51,600
train it uh better

807
00:27:51,600 --> 00:27:53,760
is kind of like thinking that we can do

808
00:27:53,760 --> 00:27:56,159
x without a belief we can train on go

809
00:27:56,159 --> 00:27:57,919
just by watching go and we can train the

810
00:27:57,919 --> 00:27:59,600
laws of physics just by watching physics

811
00:27:59,600 --> 00:28:01,039
we can learn language by just

812
00:28:01,039 --> 00:28:03,200
watching regularities in human language

813
00:28:03,200 --> 00:28:04,320
these approaches

814
00:28:04,320 --> 00:28:06,640
um there's merit to them this isn't just

815
00:28:06,640 --> 00:28:08,559
about one way being better

816
00:28:08,559 --> 00:28:10,240
it's just that what is being done in

817
00:28:10,240 --> 00:28:11,919
this paper is to

818
00:28:11,919 --> 00:28:13,520
take the free energy principle and

819
00:28:13,520 --> 00:28:15,520
active inference which previously

820
00:28:15,520 --> 00:28:17,840
had made these kinds of philosophically

821
00:28:17,840 --> 00:28:18,720
at the very least

822
00:28:18,720 --> 00:28:21,520
tantalizing claims like the relevance of

823
00:28:21,520 --> 00:28:23,840
a belief guided trajectory based

824
00:28:23,840 --> 00:28:26,399
optimization and search taking these

825
00:28:26,399 --> 00:28:27,919
very fasting ideas

826
00:28:27,919 --> 00:28:29,760
a little bit out of the sandbox into the

827
00:28:29,760 --> 00:28:31,600
next level of the playground

828
00:28:31,600 --> 00:28:33,279
where now we can actually start to

829
00:28:33,279 --> 00:28:36,080
compete or at least compare and contrast

830
00:28:36,080 --> 00:28:37,120
directly

831
00:28:37,120 --> 00:28:38,960
with the kinds of benchmarking

832
00:28:38,960 --> 00:28:40,640
algorithms that are used

833
00:28:40,640 --> 00:28:42,720
so maybe one day you know active

834
00:28:42,720 --> 00:28:44,880
inference go or active inference chess

835
00:28:44,880 --> 00:28:47,600
but today we have the simple control

836
00:28:47,600 --> 00:28:48,720
theory parameters

837
00:28:48,720 --> 00:28:51,360
which is one level closer to these kinds

838
00:28:51,360 --> 00:28:52,399
of use cases that are

839
00:28:52,399 --> 00:28:54,640
happening today one level closer than

840
00:28:54,640 --> 00:28:56,080
the teammates or the

841
00:28:56,080 --> 00:28:58,720
the three-state uh decision is it a

842
00:28:58,720 --> 00:29:00,559
mouse or a hawk or a cat

843
00:29:00,559 --> 00:29:03,919
that type of stuff cool just the

844
00:29:03,919 --> 00:29:05,440
questions that

845
00:29:05,440 --> 00:29:07,919
i put up just thinking about how can we

846
00:29:07,919 --> 00:29:09,039
continue to

847
00:29:09,039 --> 00:29:10,480
deepen our understanding which we'll

848
00:29:10,480 --> 00:29:12,240
move on from but just like

849
00:29:12,240 --> 00:29:13,679
what's something you wondered about just

850
00:29:13,679 --> 00:29:15,360
always stick with that and anyone can

851
00:29:15,360 --> 00:29:16,320
raise your hands well

852
00:29:16,320 --> 00:29:17,600
what's something you learned about while

853
00:29:17,600 --> 00:29:19,120
studying the paper whether it was

854
00:29:19,120 --> 00:29:20,720
something that they wrote specifically

855
00:29:20,720 --> 00:29:21,919
or whether it was something that you

856
00:29:21,919 --> 00:29:23,200
kind of went down a rabbit hole and

857
00:29:23,200 --> 00:29:25,039
started studying about definitely that

858
00:29:25,039 --> 00:29:25,760
happened

859
00:29:25,760 --> 00:29:27,279
for me and then lastly like what's

860
00:29:27,279 --> 00:29:29,200
something you're motivated to do more

861
00:29:29,200 --> 00:29:32,000
or um learn about now that you've read

862
00:29:32,000 --> 00:29:33,520
this paper and had this discussion

863
00:29:33,520 --> 00:29:37,440
yeah mel thanks and then anyone else

864
00:29:38,080 --> 00:29:40,159
yeah uh i guess i just wanted to jump on

865
00:29:40,159 --> 00:29:42,399
that and say that um

866
00:29:42,399 --> 00:29:46,399
i'm i'm used to seeing people sort of

867
00:29:46,399 --> 00:29:49,520
try to compare um reinforcement learning

868
00:29:49,520 --> 00:29:51,120
with with accent prints or

869
00:29:51,120 --> 00:29:54,159
or related approaches and it's nice to

870
00:29:54,159 --> 00:29:55,600
see something more that's more of a

871
00:29:55,600 --> 00:29:59,678
synthesis there

872
00:30:00,320 --> 00:30:03,520
cool yes so i think that

873
00:30:03,520 --> 00:30:05,200
if there's any other thoughts feel free

874
00:30:05,200 --> 00:30:07,120
but we'll look again over the

875
00:30:07,120 --> 00:30:08,399
experiments

876
00:30:08,399 --> 00:30:11,360
and then we'll sort of work kind of in

877
00:30:11,360 --> 00:30:12,240
and out

878
00:30:12,240 --> 00:30:14,080
about broader topics but let's try to

879
00:30:14,080 --> 00:30:15,360
understand what was really done

880
00:30:15,360 --> 00:30:17,120
because this also is the point of

881
00:30:17,120 --> 00:30:19,520
contact with people who might be very

882
00:30:19,520 --> 00:30:21,039
familiar with machine learning and

883
00:30:21,039 --> 00:30:22,000
optimization

884
00:30:22,000 --> 00:30:25,200
control theory but for them it might be

885
00:30:25,200 --> 00:30:26,640
the first time hearing active inference

886
00:30:26,640 --> 00:30:29,760
so where's the common train stop where

887
00:30:29,760 --> 00:30:31,360
we're on board with the machine learning

888
00:30:31,360 --> 00:30:33,200
community they're interested in these

889
00:30:33,200 --> 00:30:34,480
kinds of tasks

890
00:30:34,480 --> 00:30:37,200
they frame it as explore exploit and as

891
00:30:37,200 --> 00:30:38,799
we've been talking about maybe there's

892
00:30:38,799 --> 00:30:41,640
like a bit of a re-imagination or a

893
00:30:41,640 --> 00:30:43,919
re-conceptualization of this

894
00:30:43,919 --> 00:30:45,600
relationship explore exploit or what are

895
00:30:45,600 --> 00:30:47,440
these variables that's what we want to

896
00:30:47,440 --> 00:30:49,039
get to at the end so let's for sure

897
00:30:49,039 --> 00:30:50,159
remember that

898
00:30:50,159 --> 00:30:52,640
but for now let's think about being in

899
00:30:52,640 --> 00:30:53,200
common

900
00:30:53,200 --> 00:30:55,360
with benchmarking different data sets

901
00:30:55,360 --> 00:30:56,880
and different machine learning

902
00:30:56,880 --> 00:30:57,760
algorithms

903
00:30:57,760 --> 00:30:59,440
so maybe we could talk about explore and

904
00:30:59,440 --> 00:31:01,360
exploit how these communities think

905
00:31:01,360 --> 00:31:02,159
about these

906
00:31:02,159 --> 00:31:03,679
and maybe your observations about how

907
00:31:03,679 --> 00:31:05,360
they're different alec

908
00:31:05,360 --> 00:31:07,679
in terms of first the example that you

909
00:31:07,679 --> 00:31:08,559
chose to highlight

910
00:31:08,559 --> 00:31:10,799
as explore so maybe tell us about the

911
00:31:10,799 --> 00:31:12,240
mountain car and then when you tell me

912
00:31:12,240 --> 00:31:14,000
i'll go to the figure one

913
00:31:14,000 --> 00:31:15,679
but like what is this what does it have

914
00:31:15,679 --> 00:31:17,360
to do with exploring what does the

915
00:31:17,360 --> 00:31:18,880
machine learning community think about

916
00:31:18,880 --> 00:31:20,640
it how does active inference apply like

917
00:31:20,640 --> 00:31:23,360
what is happening here

918
00:31:23,360 --> 00:31:25,120
yeah so mankind's quite an interesting

919
00:31:25,120 --> 00:31:27,919
one um it seems so simple it's to

920
00:31:27,919 --> 00:31:29,200
when you do it in the fully observed

921
00:31:29,200 --> 00:31:30,840
environment there's two states one

922
00:31:30,840 --> 00:31:33,279
action uh this is the continuous version

923
00:31:33,279 --> 00:31:34,799
so it's continuous actions

924
00:31:34,799 --> 00:31:36,640
so you know it's like the simplest task

925
00:31:36,640 --> 00:31:38,960
that machine learning people

926
00:31:38,960 --> 00:31:41,519
present new papers but it's actually one

927
00:31:41,519 --> 00:31:42,480
of the hardest so

928
00:31:42,480 --> 00:31:45,919
um you know dqn vanilla dqn

929
00:31:45,919 --> 00:31:47,840
doesn't stand the chance of solving this

930
00:31:47,840 --> 00:31:49,279
what is dqm

931
00:31:49,279 --> 00:31:52,559
sorry uh deep

932
00:31:52,559 --> 00:31:54,799
q networks let's remember that so that's

933
00:31:54,799 --> 00:31:56,159
the taking like the old

934
00:31:56,159 --> 00:31:58,640
um the old idea of q learning which is

935
00:31:58,640 --> 00:31:59,279
kind of

936
00:31:59,279 --> 00:32:01,519
was the biggest idea of reinforcement

937
00:32:01,519 --> 00:32:03,360
running before this whole deep phase

938
00:32:03,360 --> 00:32:05,039
and just uh amending the deep neural

939
00:32:05,039 --> 00:32:07,679
networks it's the one that um

940
00:32:07,679 --> 00:32:11,039
be atari

941
00:32:11,039 --> 00:32:13,360
uh yeah the first like deep mind nature

942
00:32:13,360 --> 00:32:14,640
paper i can't remember the details

943
00:32:14,640 --> 00:32:16,559
but it was it was kind of the the birth

944
00:32:16,559 --> 00:32:18,320
of the deep reinforcement moment

945
00:32:18,320 --> 00:32:20,240
great so that one has challenges on here

946
00:32:20,240 --> 00:32:21,760
and then just continue yeah

947
00:32:21,760 --> 00:32:24,240
i mean basically the reason is because

948
00:32:24,240 --> 00:32:24,799
you only

949
00:32:24,799 --> 00:32:27,840
get a reward when you reach that

950
00:32:27,840 --> 00:32:31,039
flag up at the top and before that your

951
00:32:31,039 --> 00:32:34,159
um you know that takes about 180 steps

952
00:32:34,159 --> 00:32:36,880
so 180 actions that you have to

953
00:32:36,880 --> 00:32:38,799
string together before you get any

954
00:32:38,799 --> 00:32:40,880
notion of

955
00:32:40,880 --> 00:32:42,880
a reward so just pure reward-based

956
00:32:42,880 --> 00:32:44,080
schemes

957
00:32:44,080 --> 00:32:45,440
uh really struggle with this they

958
00:32:45,440 --> 00:32:47,279
essentially have to do more or less

959
00:32:47,279 --> 00:32:48,240
random actions

960
00:32:48,240 --> 00:32:52,399
until you get um out of the hill

961
00:32:52,399 --> 00:32:54,720
um so yeah it's a struggle and that's

962
00:32:54,720 --> 00:32:57,279
why it's kind of exemplifies exploration

963
00:32:57,279 --> 00:32:58,159
because

964
00:32:58,159 --> 00:33:00,399
you need to care about learning about

965
00:33:00,399 --> 00:33:01,760
the dynamics

966
00:33:01,760 --> 00:33:05,440
um or some other intrinsic quantity

967
00:33:05,440 --> 00:33:08,399
if you want a chance at finding out how

968
00:33:08,399 --> 00:33:11,440
to solve this class

969
00:33:11,440 --> 00:33:14,320
cool very interesting blue this is just

970
00:33:14,320 --> 00:33:16,159
really reminding me of a lot of what

971
00:33:16,159 --> 00:33:17,840
we've talked about with explore

972
00:33:17,840 --> 00:33:19,760
exploit a lot of the perspectives you've

973
00:33:19,760 --> 00:33:21,600
brought about

974
00:33:21,600 --> 00:33:25,200
how exploring how and when and where

975
00:33:25,200 --> 00:33:28,159
does it matter so pretty interesting to

976
00:33:28,159 --> 00:33:28,640
hear

977
00:33:28,640 --> 00:33:30,559
alex just how you phrase that and and

978
00:33:30,559 --> 00:33:33,120
also just like to hear that it's 180

979
00:33:33,120 --> 00:33:36,240
sequences of actions so this is not like

980
00:33:36,240 --> 00:33:38,559
control theory like connect four this is

981
00:33:38,559 --> 00:33:39,919
like walking you know

982
00:33:39,919 --> 00:33:41,360
who knows how many degrees of freedom

983
00:33:41,360 --> 00:33:43,519
there are but this is many tasks that

984
00:33:43,519 --> 00:33:44,799
have to get strung together in long

985
00:33:44,799 --> 00:33:45,519
sequences

986
00:33:45,519 --> 00:33:47,840
and then you said something about how um

987
00:33:47,840 --> 00:33:49,600
it has to learn something intrinsic

988
00:33:49,600 --> 00:33:51,760
for example the relationship between its

989
00:33:51,760 --> 00:33:53,919
velocity position and policy a little

990
00:33:53,919 --> 00:33:55,760
bit more nuanced nexus

991
00:33:55,760 --> 00:33:58,880
of action rather than just learning like

992
00:33:58,880 --> 00:34:00,799
simply whether you know go means fast

993
00:34:00,799 --> 00:34:02,080
and stop means slow

994
00:34:02,080 --> 00:34:04,960
steven and then anyone else yeah can i

995
00:34:04,960 --> 00:34:06,320
just ask a question about that when you

996
00:34:06,320 --> 00:34:06,640
say

997
00:34:06,640 --> 00:34:10,239
stringing together um actions because i

998
00:34:10,239 --> 00:34:12,159
i kind of had this feeling that it was

999
00:34:12,159 --> 00:34:13,839
kind of a case of like

1000
00:34:13,839 --> 00:34:16,639
you keep the by by there being a desire

1001
00:34:16,639 --> 00:34:17,679
to explore

1002
00:34:17,679 --> 00:34:20,079
in itself through free energy that the

1003
00:34:20,079 --> 00:34:21,679
car realized

1004
00:34:21,679 --> 00:34:22,879
that it could go up the other side of

1005
00:34:22,879 --> 00:34:25,359
the hill further because it could just

1006
00:34:25,359 --> 00:34:28,000
have a reason for going there because of

1007
00:34:28,000 --> 00:34:29,199
trying to

1008
00:34:29,199 --> 00:34:31,679
minimize the desire to ex free energy

1009
00:34:31,679 --> 00:34:33,199
around exploration

1010
00:34:33,199 --> 00:34:36,000
and in doing so it gets that extra kind

1011
00:34:36,000 --> 00:34:36,719
of

1012
00:34:36,719 --> 00:34:38,480
momentum to get up the other side of the

1013
00:34:38,480 --> 00:34:40,079
hill but i haven't really gone into a

1014
00:34:40,079 --> 00:34:41,679
detail so just wondering how

1015
00:34:41,679 --> 00:34:44,560
this kind of sequence of steps and maybe

1016
00:34:44,560 --> 00:34:45,199
just

1017
00:34:45,199 --> 00:34:47,839
finding yourself firing up far enough up

1018
00:34:47,839 --> 00:34:48,960
the other hill

1019
00:34:48,960 --> 00:34:51,520
to get enough potential energy just to

1020
00:34:51,520 --> 00:34:52,480
go down and up

1021
00:34:52,480 --> 00:34:54,560
and make it to the flag i wonder how

1022
00:34:54,560 --> 00:34:56,879
those two sort of relate

1023
00:34:56,879 --> 00:34:59,200
great question alec maybe if you have a

1024
00:34:59,200 --> 00:35:00,320
thought on that

1025
00:35:00,320 --> 00:35:02,240
yeah yeah so i think your intuition's

1026
00:35:02,240 --> 00:35:04,720
mostly correct so

1027
00:35:04,720 --> 00:35:07,599
how it plays out is that this agent is

1028
00:35:07,599 --> 00:35:08,560
evaluating

1029
00:35:08,560 --> 00:35:10,480
different sequences of actions some of

1030
00:35:10,480 --> 00:35:12,160
those actions are going to keep it where

1031
00:35:12,160 --> 00:35:13,599
it's already been which is at the bottom

1032
00:35:13,599 --> 00:35:14,880
of this hill

1033
00:35:14,880 --> 00:35:16,720
uh and then some of the actions are

1034
00:35:16,720 --> 00:35:17,920
going to

1035
00:35:17,920 --> 00:35:19,920
take into a place where it's uncertain

1036
00:35:19,920 --> 00:35:21,680
about the outcome

1037
00:35:21,680 --> 00:35:24,640
um and that onset and that uncertainty

1038
00:35:24,640 --> 00:35:25,839
is essentially

1039
00:35:25,839 --> 00:35:27,599
valued by the agent it wants to resolve

1040
00:35:27,599 --> 00:35:29,440
it so it's going to say okay what

1041
00:35:29,440 --> 00:35:30,800
happens if i

1042
00:35:30,800 --> 00:35:32,800
do a little because it has to go left up

1043
00:35:32,800 --> 00:35:34,079
the hill

1044
00:35:34,079 --> 00:35:35,200
i should have mentioned that before it

1045
00:35:35,200 --> 00:35:36,960
has to go left up there actually i just

1046
00:35:36,960 --> 00:35:38,480
go away from the flag

1047
00:35:38,480 --> 00:35:40,320
uh to gain momentum to go around so it's

1048
00:35:40,320 --> 00:35:41,839
gonna you know it's never gone left up

1049
00:35:41,839 --> 00:35:44,480
the hill and then accelerated um

1050
00:35:44,480 --> 00:35:45,520
doesn't know what's going to happen it

1051
00:35:45,520 --> 00:35:47,440
does that that's valuable to it in terms

1052
00:35:47,440 --> 00:35:47,680
of

1053
00:35:47,680 --> 00:35:52,560
epistemics and then it realizes oh wow

1054
00:35:53,839 --> 00:35:56,000
yeah let me let me link that um to the

1055
00:35:56,000 --> 00:35:58,079
question the sequence of actions

1056
00:35:58,079 --> 00:35:59,359
so imagine if you were going to do a

1057
00:35:59,359 --> 00:36:01,280
control theory optimization on

1058
00:36:01,280 --> 00:36:04,160
shooting a a bow and arrow so shooting a

1059
00:36:04,160 --> 00:36:04,960
bow

1060
00:36:04,960 --> 00:36:07,200
um is a complex motor movement uses

1061
00:36:07,200 --> 00:36:08,720
probably many joints there's a lot of

1062
00:36:08,720 --> 00:36:10,000
you could do different speeds different

1063
00:36:10,000 --> 00:36:11,520
ratios it's going to be dependent on the

1064
00:36:11,520 --> 00:36:12,720
bow just like this is going to be

1065
00:36:12,720 --> 00:36:13,920
dependent on the slope

1066
00:36:13,920 --> 00:36:15,920
and the car so it's an enacted

1067
00:36:15,920 --> 00:36:17,760
affordance that you're trying to

1068
00:36:17,760 --> 00:36:20,640
develop in action sequence for and your

1069
00:36:20,640 --> 00:36:21,760
action sequence in

1070
00:36:21,760 --> 00:36:24,320
a game is often just a one stepper drop

1071
00:36:24,320 --> 00:36:25,599
the connect 4 token

1072
00:36:25,599 --> 00:36:28,000
here i'll reevaluate after i see what

1073
00:36:28,000 --> 00:36:28,800
they do

1074
00:36:28,800 --> 00:36:30,400
but if you're going to be evaluating

1075
00:36:30,400 --> 00:36:32,000
prospectively an

1076
00:36:32,000 --> 00:36:34,480
action sequence in the world it often

1077
00:36:34,480 --> 00:36:35,520
has depth

1078
00:36:35,520 --> 00:36:37,760
so it's like getting out of my chair

1079
00:36:37,760 --> 00:36:38,880
shooting the bow

1080
00:36:38,880 --> 00:36:41,040
there's state spaces are very there's

1081
00:36:41,040 --> 00:36:42,560
multi um

1082
00:36:42,560 --> 00:36:44,960
joint so it's multi-dimensional and it's

1083
00:36:44,960 --> 00:36:45,839
continuous

1084
00:36:45,839 --> 00:36:47,200
so that's why the multi-dimensional and

1085
00:36:47,200 --> 00:36:49,040
the continuous are so important and then

1086
00:36:49,040 --> 00:36:50,160
the depth through time

1087
00:36:50,160 --> 00:36:51,680
is so important because you can't just

1088
00:36:51,680 --> 00:36:53,200
be doing a get out of my chair

1089
00:36:53,200 --> 00:36:56,240
or a shoot the bow and arrow short-term

1090
00:36:56,240 --> 00:36:58,400
one-step optimization there's no

1091
00:36:58,400 --> 00:37:00,480
opponent for you to then take a look at

1092
00:37:00,480 --> 00:37:02,480
their move like in a chess checkers go

1093
00:37:02,480 --> 00:37:04,320
paradigm or even a video game paradigm

1094
00:37:04,320 --> 00:37:06,240
to some extent this is something about

1095
00:37:06,240 --> 00:37:08,079
planning in an enormous state

1096
00:37:08,079 --> 00:37:11,280
space and thinking really constructively

1097
00:37:11,280 --> 00:37:13,760
about these intrinsic relationships so

1098
00:37:13,760 --> 00:37:14,960
in the bow example

1099
00:37:14,960 --> 00:37:17,119
you might be attuning to the tension

1100
00:37:17,119 --> 00:37:18,880
between the proprioception in your

1101
00:37:18,880 --> 00:37:20,560
shoulder and how tense the bow

1102
00:37:20,560 --> 00:37:22,800
is to know if you're at the end of your

1103
00:37:22,800 --> 00:37:24,000
range of movement

1104
00:37:24,000 --> 00:37:25,839
or the bow is and if you train on one

1105
00:37:25,839 --> 00:37:27,119
that was half size and

1106
00:37:27,119 --> 00:37:29,119
then you go to a larger one or if your

1107
00:37:29,119 --> 00:37:30,960
arm is hurting that day or

1108
00:37:30,960 --> 00:37:32,560
all these differences differences in our

1109
00:37:32,560 --> 00:37:34,880
abilities these things all become

1110
00:37:34,880 --> 00:37:35,760
enacted

1111
00:37:35,760 --> 00:37:37,440
in the relationships that we're learning

1112
00:37:37,440 --> 00:37:39,280
about the intrinsic variables

1113
00:37:39,280 --> 00:37:40,960
so in the depth through time and

1114
00:37:40,960 --> 00:37:43,119
intrinsic variables if you learn

1115
00:37:43,119 --> 00:37:45,599
that hey if i actually reverse and start

1116
00:37:45,599 --> 00:37:47,440
accelerating downhill

1117
00:37:47,440 --> 00:37:49,359
a hundred time steps later it's just

1118
00:37:49,359 --> 00:37:51,359
better like if i go to sleep at this

1119
00:37:51,359 --> 00:37:52,880
time a hundred time steps later

1120
00:37:52,880 --> 00:37:55,520
it's just better and so these allow for

1121
00:37:55,520 --> 00:37:58,160
very non-linear policies to be selected

1122
00:37:58,160 --> 00:38:00,000
because there can be temporal depth

1123
00:38:00,000 --> 00:38:02,000
that's learnt as a function of the

1124
00:38:02,000 --> 00:38:05,200
actual physics basically of the setting

1125
00:38:05,200 --> 00:38:07,280
not just like counter factual you know

1126
00:38:07,280 --> 00:38:09,440
if 13 moves down the row this person

1127
00:38:09,440 --> 00:38:10,720
does this with my rook

1128
00:38:10,720 --> 00:38:13,760
that still is in a very if then context

1129
00:38:13,760 --> 00:38:15,520
and this is taking it into a totally

1130
00:38:15,520 --> 00:38:17,119
different domain with a

1131
00:38:17,119 --> 00:38:21,680
proactive action selection policies

1132
00:38:21,920 --> 00:38:23,760
yep so that's why this is such an

1133
00:38:23,760 --> 00:38:26,400
interesting paper and approach and like

1134
00:38:26,400 --> 00:38:28,480
again a branching off point that's why

1135
00:38:28,480 --> 00:38:29,839
it's foundational in the machine

1136
00:38:29,839 --> 00:38:31,040
learning because just

1137
00:38:31,040 --> 00:38:34,160
like alec described like if dq learning

1138
00:38:34,160 --> 00:38:36,880
cannot accomplish this task then yeah

1139
00:38:36,880 --> 00:38:38,720
it's all great to be a human at chess or

1140
00:38:38,720 --> 00:38:39,440
go

1141
00:38:39,440 --> 00:38:41,359
but if that algorithm or even that

1142
00:38:41,359 --> 00:38:42,640
architecture

1143
00:38:42,640 --> 00:38:45,520
can't defeat this challenge then we're

1144
00:38:45,520 --> 00:38:46,000
developing

1145
00:38:46,000 --> 00:38:47,760
something that's incredibly specific

1146
00:38:47,760 --> 00:38:49,760
which is great we should have good

1147
00:38:49,760 --> 00:38:51,359
map routing algorithms and things like

1148
00:38:51,359 --> 00:38:53,520
that but we're definitely going hyper

1149
00:38:53,520 --> 00:38:55,280
local down a rabbit hole

1150
00:38:55,280 --> 00:38:57,520
if it can't solve this but it can solve

1151
00:38:57,520 --> 00:38:59,359
go i don't know if it is that case

1152
00:38:59,359 --> 00:39:00,880
um a machine learning person would be

1153
00:39:00,880 --> 00:39:02,560
really welcome to help

1154
00:39:02,560 --> 00:39:04,160
fill us in on some of these details but

1155
00:39:04,160 --> 00:39:05,760
that's the kind of stuff that

1156
00:39:05,760 --> 00:39:08,640
is um interesting and broached in this

1157
00:39:08,640 --> 00:39:09,359
topic

1158
00:39:09,359 --> 00:39:10,800
so just to see what that looks like

1159
00:39:10,800 --> 00:39:12,880
empirically like the first hundred

1160
00:39:12,880 --> 00:39:13,599
epochs

1161
00:39:13,599 --> 00:39:16,079
um are plotted in terms of the state

1162
00:39:16,079 --> 00:39:16,640
space

1163
00:39:16,640 --> 00:39:19,200
coverage which here is just the position

1164
00:39:19,200 --> 00:39:21,119
on the x-axis and then the velocity on

1165
00:39:21,119 --> 00:39:21,599
the y

1166
00:39:21,599 --> 00:39:24,960
and the flag is at 0.5 right alec

1167
00:39:24,960 --> 00:39:26,640
like the flag represents being at

1168
00:39:26,640 --> 00:39:28,480
position of 0.5 but yeah continue from

1169
00:39:28,480 --> 00:39:29,920
here just anything you want to add on

1170
00:39:29,920 --> 00:39:32,320
figure one or what what does this mean

1171
00:39:32,320 --> 00:39:34,560
what is happening here i guess no your

1172
00:39:34,560 --> 00:39:35,359
description was great

1173
00:39:35,359 --> 00:39:39,760
the flag's not 0.5 the flags

1174
00:39:39,760 --> 00:39:42,079
what is it oh yeah 0.5 yeah uh but i

1175
00:39:42,079 --> 00:39:42,960
just kind of wanna

1176
00:39:42,960 --> 00:39:46,079
um self-deprecate myself and say

1177
00:39:46,079 --> 00:39:48,800
that uh these results are kinda they're

1178
00:39:48,800 --> 00:39:49,200
not

1179
00:39:49,200 --> 00:39:52,160
i mean they're not bad uh but um we had

1180
00:39:52,160 --> 00:39:53,119
a follow-up

1181
00:39:53,119 --> 00:39:54,839
paper where we got this all working

1182
00:39:54,839 --> 00:39:56,400
properly um

1183
00:39:56,400 --> 00:39:58,000
called reinforcement learning for active

1184
00:39:58,000 --> 00:39:59,920
inference um which is uh

1185
00:39:59,920 --> 00:40:03,280
very similar but we use a slightly

1186
00:40:03,280 --> 00:40:04,800
i can discuss the differences i think

1187
00:40:04,800 --> 00:40:06,560
they're interesting if we want but uh

1188
00:40:06,560 --> 00:40:08,240
just that the the results are so much

1189
00:40:08,240 --> 00:40:10,880
more impressive we can still uh

1190
00:40:10,880 --> 00:40:13,599
it's kind of that's both uh hitting on

1191
00:40:13,599 --> 00:40:14,000
this

1192
00:40:14,000 --> 00:40:17,200
and also um here's the paper

1193
00:40:17,200 --> 00:40:19,680
advertise that that work so in that work

1194
00:40:19,680 --> 00:40:21,040
oh yeah if you can actually just show

1195
00:40:21,040 --> 00:40:22,640
the figure and that we can check so

1196
00:40:22,640 --> 00:40:25,280
there it can solve the mountain car in a

1197
00:40:25,280 --> 00:40:26,800
single

1198
00:40:26,800 --> 00:40:29,200
trial and i have also the sort of state

1199
00:40:29,200 --> 00:40:31,119
space spots that look a bit better so

1200
00:40:31,119 --> 00:40:34,079
here's the so just like the left one

1201
00:40:34,079 --> 00:40:35,200
yeah

1202
00:40:35,200 --> 00:40:37,520
so if you kind of go i yeah we shot the

1203
00:40:37,520 --> 00:40:38,400
video can you just

1204
00:40:38,400 --> 00:40:39,839
the first time it goes into the ring it

1205
00:40:39,839 --> 00:40:41,680
just uh solves it straight away which is

1206
00:40:41,680 --> 00:40:43,040
much nicer uh so

1207
00:40:43,040 --> 00:40:44,480
the other the other one in the paper

1208
00:40:44,480 --> 00:40:46,160
that we're looking at today

1209
00:40:46,160 --> 00:40:48,160
um so the hero is being solved every

1210
00:40:48,160 --> 00:40:50,160
time that it does it and it responds to

1211
00:40:50,160 --> 00:40:52,960
this outweighs the

1212
00:40:57,280 --> 00:40:59,920
gotta love it

1213
00:41:00,480 --> 00:41:02,960
um yeah i guess that was just a caveat i

1214
00:41:02,960 --> 00:41:05,040
wanted to highlight that um

1215
00:41:05,040 --> 00:41:07,200
it can do a lot better business um but

1216
00:41:07,200 --> 00:41:08,560
i'll be happy to discuss the differences

1217
00:41:08,560 --> 00:41:09,839
because it's just one part of the

1218
00:41:09,839 --> 00:41:11,839
architecture

1219
00:41:11,839 --> 00:41:14,960
sure yeah what takes us from here to

1220
00:41:14,960 --> 00:41:18,000
what we just sort of peeked into in this

1221
00:41:18,000 --> 00:41:19,920
paper like what was the one thing that

1222
00:41:19,920 --> 00:41:21,680
you added or changed

1223
00:41:21,680 --> 00:41:25,520
yeah so when i wrote the original one to

1224
00:41:25,520 --> 00:41:28,720
um so to get this kind of what what

1225
00:41:28,720 --> 00:41:32,400
carl calls parameter expiration or

1226
00:41:32,400 --> 00:41:33,839
parameter information again it's

1227
00:41:33,839 --> 00:41:35,040
essentially trying to reduce

1228
00:41:35,040 --> 00:41:36,640
uncertainty not about what's out there

1229
00:41:36,640 --> 00:41:38,560
in the world but about your own model

1230
00:41:38,560 --> 00:41:42,000
if you have beliefs about um your model

1231
00:41:42,000 --> 00:41:44,480
uh and it's kind of in a sense you know

1232
00:41:44,480 --> 00:41:45,680
what you don't know

1233
00:41:45,680 --> 00:41:47,200
and to get that you have to have a

1234
00:41:47,200 --> 00:41:48,800
distribution

1235
00:41:48,800 --> 00:41:51,839
over your model and in this case our

1236
00:41:51,839 --> 00:41:54,640
model is a neural network so you need a

1237
00:41:54,640 --> 00:41:55,440
distribution

1238
00:41:55,440 --> 00:41:56,960
overview on your network there's two

1239
00:41:56,960 --> 00:41:59,119
ways in the literature to do this

1240
00:41:59,119 --> 00:42:00,880
in the first paper scaling active

1241
00:42:00,880 --> 00:42:02,880
inference we tried bayesian neural

1242
00:42:02,880 --> 00:42:04,400
networks

1243
00:42:04,400 --> 00:42:06,640
there may be more principles you know

1244
00:42:06,640 --> 00:42:07,920
you're actually using variational

1245
00:42:07,920 --> 00:42:10,560
inference to estimate this

1246
00:42:10,560 --> 00:42:11,839
distribution over each of your

1247
00:42:11,839 --> 00:42:13,760
parameters but

1248
00:42:13,760 --> 00:42:17,760
in practice they don't work

1249
00:42:17,760 --> 00:42:19,920
nearly as well as what we tried in the

1250
00:42:19,920 --> 00:42:21,040
next paper

1251
00:42:21,040 --> 00:42:23,520
which is called the ensemble approach

1252
00:42:23,520 --> 00:42:24,960
and the idea there

1253
00:42:24,960 --> 00:42:28,079
is you just take in this case 25

1254
00:42:28,079 --> 00:42:29,599
dynamics models and training them on

1255
00:42:29,599 --> 00:42:31,200
different batches of the data

1256
00:42:31,200 --> 00:42:33,920
and that's kind of like a proxy for a

1257
00:42:33,920 --> 00:42:36,960
non-parametric bayesian posterior over

1258
00:42:36,960 --> 00:42:40,319
the um dynamics model um you can

1259
00:42:40,319 --> 00:42:42,640
do a little people are kind of working

1260
00:42:42,640 --> 00:42:44,560
out how close it is to actually proper

1261
00:42:44,560 --> 00:42:46,079
baiting beliefs but

1262
00:42:46,079 --> 00:42:49,680
um you definitely get a notion of

1263
00:42:49,680 --> 00:42:50,800
uncertainty there so

1264
00:42:50,800 --> 00:42:54,000
here um and there's lots of interesting

1265
00:42:54,000 --> 00:42:56,160
reasons as to why

1266
00:42:56,160 --> 00:42:58,000
uh principled reasons as to why ensemble

1267
00:42:58,000 --> 00:42:59,520
models would work better than

1268
00:42:59,520 --> 00:43:03,680
um bayesian models it's also easier

1269
00:43:03,680 --> 00:43:06,240
in practice to estimate stuff like um

1270
00:43:06,240 --> 00:43:07,440
information game

1271
00:43:07,440 --> 00:43:10,319
um so that just turned out to be a

1272
00:43:10,319 --> 00:43:10,800
hundred times

1273
00:43:10,800 --> 00:43:12,240
and in the machine learning community

1274
00:43:12,240 --> 00:43:14,400
people generally find more success

1275
00:43:14,400 --> 00:43:18,000
more success with uh deep ensembles for

1276
00:43:18,000 --> 00:43:19,280
both

1277
00:43:19,280 --> 00:43:21,680
uncertainty calibration and directed

1278
00:43:21,680 --> 00:43:23,760
exploration

1279
00:43:23,760 --> 00:43:26,240
that's very cool thanks for the great

1280
00:43:26,240 --> 00:43:28,960
explanation there and so just to sort of

1281
00:43:28,960 --> 00:43:30,400
re-hash that or say it a little

1282
00:43:30,400 --> 00:43:32,880
differently the approach that was taken

1283
00:43:32,880 --> 00:43:35,119
in this scaling active inference paper

1284
00:43:35,119 --> 00:43:36,319
to estimating

1285
00:43:36,319 --> 00:43:38,640
some of these essential parameters was

1286
00:43:38,640 --> 00:43:39,680
done with the bayesian

1287
00:43:39,680 --> 00:43:42,240
neural network which alec just described

1288
00:43:42,240 --> 00:43:44,000
it's like one way to do it it might be a

1289
00:43:44,000 --> 00:43:45,680
way that interfaces pretty cleanly with

1290
00:43:45,680 --> 00:43:46,319
a lot of other

1291
00:43:46,319 --> 00:43:48,400
software packages or approaches but it's

1292
00:43:48,400 --> 00:43:50,640
the skeleton of the model and that's the

1293
00:43:50,640 --> 00:43:52,240
diagram and then we think about how can

1294
00:43:52,240 --> 00:43:54,319
we improve it and so one direction

1295
00:43:54,319 --> 00:43:56,960
is more analytical or more principled as

1296
00:43:56,960 --> 00:43:57,920
you described it

1297
00:43:57,920 --> 00:44:00,079
and that's like the fully bayesian sort

1298
00:44:00,079 --> 00:44:01,839
of specifying every little

1299
00:44:01,839 --> 00:44:04,800
uh hidden state and that approach can

1300
00:44:04,800 --> 00:44:06,560
provide some interesting avenues at

1301
00:44:06,560 --> 00:44:07,280
times

1302
00:44:07,280 --> 00:44:10,079
but an approach that is also relatively

1303
00:44:10,079 --> 00:44:11,280
easy to implement

1304
00:44:11,280 --> 00:44:13,599
from a programmatic interface level and

1305
00:44:13,599 --> 00:44:15,040
also makes a relative

1306
00:44:15,040 --> 00:44:17,440
minimum of assumptions about the

1307
00:44:17,440 --> 00:44:19,599
specific mechanics of the system

1308
00:44:19,599 --> 00:44:21,599
is this ensemble or deep ensemble

1309
00:44:21,599 --> 00:44:23,359
approach so the deep adjective

1310
00:44:23,359 --> 00:44:24,800
is just meaning you're going to have

1311
00:44:24,800 --> 00:44:26,400
like a multi-scale or i mean a

1312
00:44:26,400 --> 00:44:28,000
multi-level neural network with hidden

1313
00:44:28,000 --> 00:44:29,599
layers or it's just going to be deep and

1314
00:44:29,599 --> 00:44:30,240
modern

1315
00:44:30,240 --> 00:44:32,240
but it's kind of like just an adjective

1316
00:44:32,240 --> 00:44:33,760
the ensemble approach

1317
00:44:33,760 --> 00:44:35,599
is describing having a bunch of

1318
00:44:35,599 --> 00:44:37,280
different models that are going to be

1319
00:44:37,280 --> 00:44:38,800
trained up on the same or different

1320
00:44:38,800 --> 00:44:40,160
parts of the data set

1321
00:44:40,160 --> 00:44:41,920
and then you could look at the average

1322
00:44:41,920 --> 00:44:43,680
of the ensemble or you could look at

1323
00:44:43,680 --> 00:44:45,359
some other weighted combination of the

1324
00:44:45,359 --> 00:44:47,040
ensemble's performance

1325
00:44:47,040 --> 00:44:48,960
and so it's almost like bridging the gap

1326
00:44:48,960 --> 00:44:50,079
from individual

1327
00:44:50,079 --> 00:44:52,560
testing that's like the single model

1328
00:44:52,560 --> 00:44:53,359
testing

1329
00:44:53,359 --> 00:44:54,960
to now the ensemble approach is like

1330
00:44:54,960 --> 00:44:56,240
we're gonna have a classroom and then

1331
00:44:56,240 --> 00:44:58,240
the best answer from r9

1332
00:44:58,240 --> 00:45:00,319
group all working on it independently

1333
00:45:00,319 --> 00:45:01,760
the best one will push forward or we'll

1334
00:45:01,760 --> 00:45:02,560
average

1335
00:45:02,560 --> 00:45:04,880
and then even another level beyond that

1336
00:45:04,880 --> 00:45:06,560
is like the truly emergence

1337
00:45:06,560 --> 00:45:07,920
which is actually what the ants are

1338
00:45:07,920 --> 00:45:10,079
doing where the ensemble

1339
00:45:10,079 --> 00:45:12,319
works as an ensemble in a way that is

1340
00:45:12,319 --> 00:45:13,200
itself

1341
00:45:13,200 --> 00:45:15,119
shaped by development and learning and

1342
00:45:15,119 --> 00:45:17,440
evolution so right now the ensemble

1343
00:45:17,440 --> 00:45:19,040
modeling is still like sort of

1344
00:45:19,040 --> 00:45:20,480
well if you just split it up into many

1345
00:45:20,480 --> 00:45:22,560
parts you can cover more state space

1346
00:45:22,560 --> 00:45:24,000
you might be able to train the model in

1347
00:45:24,000 --> 00:45:25,760
parallel you might make sure that no

1348
00:45:25,760 --> 00:45:27,680
single model over generalizes

1349
00:45:27,680 --> 00:45:29,119
there's so many benefits that come

1350
00:45:29,119 --> 00:45:31,119
simply from batching and ensemble

1351
00:45:31,119 --> 00:45:33,359
modeling that go beyond just saying

1352
00:45:33,359 --> 00:45:35,760
what's the best single model or what's

1353
00:45:35,760 --> 00:45:36,480
the best

1354
00:45:36,480 --> 00:45:38,960
parameter range in this type of model

1355
00:45:38,960 --> 00:45:40,800
the ensemble can consist of a single

1356
00:45:40,800 --> 00:45:42,480
type with the different parameters

1357
00:45:42,480 --> 00:45:44,240
between the different ensemble mates or

1358
00:45:44,240 --> 00:45:46,319
they can be heterogeneous in some

1359
00:45:46,319 --> 00:45:49,200
aspect so just really cool you went with

1360
00:45:49,200 --> 00:45:50,720
what was implementable

1361
00:45:50,720 --> 00:45:52,480
and a common point of departure with

1362
00:45:52,480 --> 00:45:53,839
machine learning community

1363
00:45:53,839 --> 00:45:55,680
and the bayesian neural network kind of

1364
00:45:55,680 --> 00:45:57,200
peaks in one direction towards a more

1365
00:45:57,200 --> 00:45:58,880
analytical more principled fully

1366
00:45:58,880 --> 00:46:00,720
bayesian approach and then on the other

1367
00:46:00,720 --> 00:46:01,440
side to

1368
00:46:01,440 --> 00:46:03,440
modern approaches in machine learning

1369
00:46:03,440 --> 00:46:07,040
like deep ensemble learning

1370
00:46:07,280 --> 00:46:10,480
cool let's look at figure two and what

1371
00:46:10,480 --> 00:46:12,079
is conveyed here so this is the

1372
00:46:12,079 --> 00:46:14,880
hopper task oh we'll go blue first go

1373
00:46:14,880 --> 00:46:16,960
ahead sorry i didn't see that

1374
00:46:16,960 --> 00:46:18,560
can you back up to the last figure let's

1375
00:46:18,560 --> 00:46:20,079
do blue and then math

1376
00:46:20,079 --> 00:46:23,440
and then so um

1377
00:46:23,440 --> 00:46:25,440
i just wanted to ask and this is kind of

1378
00:46:25,440 --> 00:46:27,599
related to the next

1379
00:46:27,599 --> 00:46:30,160
figure but um i just wanted to ask that

1380
00:46:30,160 --> 00:46:31,280
uh

1381
00:46:31,280 --> 00:46:34,560
when you did the hopper task and the um

1382
00:46:34,560 --> 00:46:37,119
pendulum so i know that that was only

1383
00:46:37,119 --> 00:46:39,119
based on the extrinsic value

1384
00:46:39,119 --> 00:46:41,119
like part of the free energy equation so

1385
00:46:41,119 --> 00:46:42,880
you like you left off the exploration

1386
00:46:42,880 --> 00:46:43,599
because

1387
00:46:43,599 --> 00:46:46,079
like exploration probably had no value

1388
00:46:46,079 --> 00:46:48,160
in in that problem i mean i'm assuming

1389
00:46:48,160 --> 00:46:48,800
there

1390
00:46:48,800 --> 00:46:51,280
but i i was wondering here like on the

1391
00:46:51,280 --> 00:46:53,200
exploration problem sorry did i say

1392
00:46:53,200 --> 00:46:54,079
exploration before

1393
00:46:54,079 --> 00:46:56,000
exploit anyway so here with the

1394
00:46:56,000 --> 00:46:57,440
exploration problem

1395
00:46:57,440 --> 00:47:01,200
um did you ever think about leaving off

1396
00:47:01,200 --> 00:47:04,240
uh the extrinsic part of the equation

1397
00:47:04,240 --> 00:47:06,160
and just using like the information

1398
00:47:06,160 --> 00:47:08,400
gain as a reward like can you structure

1399
00:47:08,400 --> 00:47:10,000
it that way would it be different

1400
00:47:10,000 --> 00:47:12,560
like if the object of the agent was just

1401
00:47:12,560 --> 00:47:13,119
to gain

1402
00:47:13,119 --> 00:47:14,480
more information about the environment

1403
00:47:14,480 --> 00:47:15,839
do you think that it would have

1404
00:47:15,839 --> 00:47:16,720
succeeded

1405
00:47:16,720 --> 00:47:19,680
in um you know climbing the hill because

1406
00:47:19,680 --> 00:47:21,200
it would get to a new part of the

1407
00:47:21,200 --> 00:47:22,640
of the environment or how do you think

1408
00:47:22,640 --> 00:47:24,960
that would work

1409
00:47:24,960 --> 00:47:27,040
yeah yeah uh not in this paper but it

1410
00:47:27,040 --> 00:47:28,240
does um

1411
00:47:28,240 --> 00:47:30,079
so that other paper i mentioned the

1412
00:47:30,079 --> 00:47:32,240
reinforcement through active influence

1413
00:47:32,240 --> 00:47:33,599
because it solves it in the very first

1414
00:47:33,599 --> 00:47:35,520
task it literally can't

1415
00:47:35,520 --> 00:47:37,440
be because of reward we tested it

1416
00:47:37,440 --> 00:47:38,960
without reward as well to

1417
00:47:38,960 --> 00:47:41,359
confirm this but you know it hasn't ever

1418
00:47:41,359 --> 00:47:43,280
experienced a reward and it still solves

1419
00:47:43,280 --> 00:47:45,119
the task so it's exactly what you're

1420
00:47:45,119 --> 00:47:47,839
specifying it's just purely trying to

1421
00:47:47,839 --> 00:47:49,119
reach all the parts of

1422
00:47:49,119 --> 00:47:52,800
the uh state space uh if you just left

1423
00:47:52,800 --> 00:47:54,079
it with expiration what you

1424
00:47:54,079 --> 00:47:55,520
tend to find is that it will solve it

1425
00:47:55,520 --> 00:47:57,680
for say the first 10 trials while it's

1426
00:47:57,680 --> 00:47:58,480
still

1427
00:47:58,480 --> 00:48:01,520
the peaks of the uh hill

1428
00:48:01,520 --> 00:48:03,040
are the most kind of interesting so they

1429
00:48:03,040 --> 00:48:04,559
have the most extreme dynamics with the

1430
00:48:04,559 --> 00:48:06,960
most variants um

1431
00:48:06,960 --> 00:48:09,599
but then after a while uh the top of the

1432
00:48:09,599 --> 00:48:10,480
hill is no more

1433
00:48:10,480 --> 00:48:11,839
interesting than the bottom of the hill

1434
00:48:11,839 --> 00:48:12,960
because it's kind of explored but

1435
00:48:12,960 --> 00:48:14,240
whereas if you've got the rewards

1436
00:48:14,240 --> 00:48:16,319
and the exploration kind of slowly

1437
00:48:16,319 --> 00:48:17,839
transfers from solving it because of

1438
00:48:17,839 --> 00:48:19,359
expiration to solving it because it

1439
00:48:19,359 --> 00:48:21,200
knows

1440
00:48:21,200 --> 00:48:24,000
how to get rewards

1441
00:48:24,400 --> 00:48:25,920
awesome thank you i didn't see that

1442
00:48:25,920 --> 00:48:27,599
follow-up paper and i like i'm going to

1443
00:48:27,599 --> 00:48:28,800
go read it right after this

1444
00:48:28,800 --> 00:48:32,960
i know we need to do a uh after party

1445
00:48:32,960 --> 00:48:35,520
active after parties with next paper um

1446
00:48:35,520 --> 00:48:39,359
but uh mel and then anyone else

1447
00:48:40,800 --> 00:48:43,920
yeah just um what you're describing

1448
00:48:43,920 --> 00:48:47,680
with uh like stringing a bow

1449
00:48:47,680 --> 00:48:50,640
something like this um i think that was

1450
00:48:50,640 --> 00:48:53,359
what really got me excited about

1451
00:48:53,359 --> 00:48:54,720
active influence in the free entrance

1452
00:48:54,720 --> 00:48:56,720
from the first place is is the idea that

1453
00:48:56,720 --> 00:49:00,240
um with something like like a

1454
00:49:00,240 --> 00:49:03,440
towers of hanoi style problem are people

1455
00:49:03,440 --> 00:49:04,880
familiar with that uh-huh it's like

1456
00:49:04,880 --> 00:49:05,760
you've got you've got

1457
00:49:05,760 --> 00:49:08,400
three sticks and and you've got disks of

1458
00:49:08,400 --> 00:49:09,760
various sizes on the stick so you've got

1459
00:49:09,760 --> 00:49:10,480
to get them in

1460
00:49:10,480 --> 00:49:13,280
in a in a like a ascending order right

1461
00:49:13,280 --> 00:49:14,240
um

1462
00:49:14,240 --> 00:49:18,160
and the idea is that uh

1463
00:49:18,160 --> 00:49:21,280
the puzzle requires that you you go

1464
00:49:21,280 --> 00:49:23,520
backwards before you can go forward

1465
00:49:23,520 --> 00:49:27,040
and if you're just

1466
00:49:27,040 --> 00:49:29,520
minimizing or maximizing some function

1467
00:49:29,520 --> 00:49:31,359
on a single level

1468
00:49:31,359 --> 00:49:35,359
um if you've got a like a 1d

1469
00:49:35,359 --> 00:49:38,720
optimization you're you're going to

1470
00:49:38,720 --> 00:49:40,319
not be able to solve a problem that

1471
00:49:40,319 --> 00:49:42,079
requires that you backtrack

1472
00:49:42,079 --> 00:49:45,040
in order to make progress um and

1473
00:49:45,040 --> 00:49:46,319
something like

1474
00:49:46,319 --> 00:49:47,839
academics the free energy principle

1475
00:49:47,839 --> 00:49:49,599
where we've got

1476
00:49:49,599 --> 00:49:51,599
temporal depth we've got hierarchical

1477
00:49:51,599 --> 00:49:53,359
depth of the model

1478
00:49:53,359 --> 00:49:56,480
um is really equipped to solve these

1479
00:49:56,480 --> 00:49:58,240
kinds of problems in a way that a lot of

1480
00:49:58,240 --> 00:49:59,599
traditional

1481
00:49:59,599 --> 00:50:03,200
um problem-solving approaches are not

1482
00:50:03,200 --> 00:50:04,880
i totally agree and i think there's a

1483
00:50:04,880 --> 00:50:06,480
quantitative and a qualitative so at the

1484
00:50:06,480 --> 00:50:07,680
quantitative level

1485
00:50:07,680 --> 00:50:09,680
there's control policies that we can't

1486
00:50:09,680 --> 00:50:12,240
have fit computers look beyond some

1487
00:50:12,240 --> 00:50:14,160
locally non-favorable states to get

1488
00:50:14,160 --> 00:50:15,599
around so we want to do these

1489
00:50:15,599 --> 00:50:16,880
quantitative policies that's what this

1490
00:50:16,880 --> 00:50:17,760
paper is about

1491
00:50:17,760 --> 00:50:19,520
but at the qualitative and really the

1492
00:50:19,520 --> 00:50:20,880
philosophical level

1493
00:50:20,880 --> 00:50:23,839
how do we come to grasp with processes

1494
00:50:23,839 --> 00:50:25,680
where yeah we're not always strictly

1495
00:50:25,680 --> 00:50:27,599
walking a staircase directly

1496
00:50:27,599 --> 00:50:29,680
to the top of the mountain or it might

1497
00:50:29,680 --> 00:50:31,839
not seem like we can do it at all

1498
00:50:31,839 --> 00:50:33,520
initially if we just look and there's so

1499
00:50:33,520 --> 00:50:34,480
much there

1500
00:50:34,480 --> 00:50:36,559
with how we think about challenges and

1501
00:50:36,559 --> 00:50:37,760
about exploring

1502
00:50:37,760 --> 00:50:40,880
one actual uh link there because blue

1503
00:50:40,880 --> 00:50:42,160
asked about what would happen if you

1504
00:50:42,160 --> 00:50:44,480
just let it go wild on exploring

1505
00:50:44,480 --> 00:50:46,160
and alec what you said is that the

1506
00:50:46,160 --> 00:50:47,520
learning gets you to the top

1507
00:50:47,520 --> 00:50:49,520
pretty quickly because in that sense

1508
00:50:49,520 --> 00:50:50,720
it's similar to this

1509
00:50:50,720 --> 00:50:53,119
model that also prioritizes reward but

1510
00:50:53,119 --> 00:50:55,359
then after getting to the top

1511
00:50:55,359 --> 00:50:58,160
you spend a lot of your time learning on

1512
00:50:58,160 --> 00:50:58,800
the most

1513
00:50:58,800 --> 00:51:01,359
extremely variant areas of parameter

1514
00:51:01,359 --> 00:51:02,240
space

1515
00:51:02,240 --> 00:51:04,720
so it's so much like curiosity driven

1516
00:51:04,720 --> 00:51:05,599
learning

1517
00:51:05,599 --> 00:51:07,760
where a lot of times curiosity driven

1518
00:51:07,760 --> 00:51:08,880
learning with no

1519
00:51:08,880 --> 00:51:11,920
reward or scaffolding it ends up

1520
00:51:11,920 --> 00:51:13,599
learning learning learning a ton

1521
00:51:13,599 --> 00:51:15,520
shocking amounts but then also spending

1522
00:51:15,520 --> 00:51:17,680
a lot of time in the most extreme ranges

1523
00:51:17,680 --> 00:51:20,000
of space and that doesn't always just

1524
00:51:20,000 --> 00:51:21,680
play out like in the kind of online

1525
00:51:21,680 --> 00:51:23,920
gutter but even in the literature we see

1526
00:51:23,920 --> 00:51:24,960
a lot of the attention

1527
00:51:24,960 --> 00:51:27,920
being spent on the extreme hyperboles

1528
00:51:27,920 --> 00:51:29,440
and then the middle ground where it's

1529
00:51:29,440 --> 00:51:31,040
like yeah it's kind of balanced and we

1530
00:51:31,040 --> 00:51:32,559
can work on it together

1531
00:51:32,559 --> 00:51:34,800
that is not as extreme of a viewpoint

1532
00:51:34,800 --> 00:51:36,400
and so people spend less learning and

1533
00:51:36,400 --> 00:51:37,839
attentional regimes on

1534
00:51:37,839 --> 00:51:40,480
these kinds of projects so there's a lot

1535
00:51:40,480 --> 00:51:41,040
of like

1536
00:51:41,040 --> 00:51:42,720
parallels that we can quantitatively

1537
00:51:42,720 --> 00:51:45,040
model but also help us think about

1538
00:51:45,040 --> 00:51:47,280
how we can say no you're just trying to

1539
00:51:47,280 --> 00:51:48,720
make me explore the top of the hill

1540
00:51:48,720 --> 00:51:50,480
i get it but the flag's on the other

1541
00:51:50,480 --> 00:51:52,319
side so i've heard about the other side

1542
00:51:52,319 --> 00:51:52,960
of the hill

1543
00:51:52,960 --> 00:51:54,960
how are we going to enact a policy to

1544
00:51:54,960 --> 00:51:56,880
get us to the flag together

1545
00:51:56,880 --> 00:51:58,240
that's a little bit better than your

1546
00:51:58,240 --> 00:52:00,480
hill is worse than my hill for example

1547
00:52:00,480 --> 00:52:02,000
how could these kinds of things be

1548
00:52:02,000 --> 00:52:04,800
ported onto like human decision making

1549
00:52:04,800 --> 00:52:08,240
is a cool area

1550
00:52:08,400 --> 00:52:10,079
any other questions on one otherwise

1551
00:52:10,079 --> 00:52:11,599
let's talk about two a little bit

1552
00:52:11,599 --> 00:52:13,440
well just just one quick piece just

1553
00:52:13,440 --> 00:52:15,119
picking up on what blur

1554
00:52:15,119 --> 00:52:16,400
said there i thought it's quite

1555
00:52:16,400 --> 00:52:18,800
interesting is if

1556
00:52:18,800 --> 00:52:21,760
there's this idea of exploring the

1557
00:52:21,760 --> 00:52:22,640
environment

1558
00:52:22,640 --> 00:52:24,319
in different ways but say there's

1559
00:52:24,319 --> 00:52:26,480
exploring or information going

1560
00:52:26,480 --> 00:52:29,359
in terms of personal preference so for

1561
00:52:29,359 --> 00:52:30,800
instance

1562
00:52:30,800 --> 00:52:33,040
maybe it was a bit more of a con i like

1563
00:52:33,040 --> 00:52:34,480
to always

1564
00:52:34,480 --> 00:52:36,480
feel what it's like to go slightly up a

1565
00:52:36,480 --> 00:52:38,160
slope and turn to the right

1566
00:52:38,160 --> 00:52:40,160
that feels cool all right so then you

1567
00:52:40,160 --> 00:52:41,760
have this kind of like

1568
00:52:41,760 --> 00:52:44,079
it's not necessarily it's like an

1569
00:52:44,079 --> 00:52:47,280
information gain about having funds

1570
00:52:47,280 --> 00:52:49,520
so would it like say the car was like

1571
00:52:49,520 --> 00:52:51,040
hey i really like to do

1572
00:52:51,040 --> 00:52:54,480
this type of things because cars like me

1573
00:52:54,480 --> 00:52:56,960
like doing that i know you know if it

1574
00:52:56,960 --> 00:52:58,160
was a living animal

1575
00:52:58,160 --> 00:53:00,640
so it kind of ties into what sort of

1576
00:53:00,640 --> 00:53:02,000
exploration of

1577
00:53:02,000 --> 00:53:04,240
the world and then what exploration of

1578
00:53:04,240 --> 00:53:06,000
just being

1579
00:53:06,000 --> 00:53:08,079
an entity that likes to do certain

1580
00:53:08,079 --> 00:53:10,559
things and that could end up

1581
00:53:10,559 --> 00:53:12,400
taking you there in another in another

1582
00:53:12,400 --> 00:53:14,000
way or together they

1583
00:53:14,000 --> 00:53:17,520
help great alec

1584
00:53:17,520 --> 00:53:18,960
yeah i just wanted to point on that

1585
00:53:18,960 --> 00:53:21,680
because this is uh i totally agree and i

1586
00:53:21,680 --> 00:53:22,400
think it's

1587
00:53:22,400 --> 00:53:25,839
one of the most uh promising kind of

1588
00:53:25,839 --> 00:53:27,599
directions or perspectives that active

1589
00:53:27,599 --> 00:53:28,800
inference gives

1590
00:53:28,800 --> 00:53:31,359
and i wrote about this in a in another

1591
00:53:31,359 --> 00:53:33,119
paper that i can link afterwards

1592
00:53:33,119 --> 00:53:35,760
uh of this notion of i don't really know

1593
00:53:35,760 --> 00:53:36,640
what to call it

1594
00:53:36,640 --> 00:53:38,480
but goal-directed exploration that you

1595
00:53:38,480 --> 00:53:40,319
derive from something like expected free

1596
00:53:40,319 --> 00:53:42,079
energy and that means

1597
00:53:42,079 --> 00:53:44,240
you don't necessarily or it's just too

1598
00:53:44,240 --> 00:53:47,040
inefficient just to have

1599
00:53:47,040 --> 00:53:48,640
everlasting expiration with no

1600
00:53:48,640 --> 00:53:50,960
constraints you know

1601
00:53:50,960 --> 00:53:52,800
and it's also too inefficient just to do

1602
00:53:52,800 --> 00:53:54,240
have exploitation what you need

1603
00:53:54,240 --> 00:53:56,000
is some objective function like expected

1604
00:53:56,000 --> 00:53:58,640
free energy or model evidence that

1605
00:53:58,640 --> 00:54:00,160
contains these two things so that you're

1606
00:54:00,160 --> 00:54:02,240
not selecting actions to just explore or

1607
00:54:02,240 --> 00:54:03,200
just to exploit

1608
00:54:03,200 --> 00:54:05,280
that each action is kind of shaded with

1609
00:54:05,280 --> 00:54:06,720
both of these things

1610
00:54:06,720 --> 00:54:09,119
and that way you're getting a

1611
00:54:09,119 --> 00:54:10,319
exploration that's

1612
00:54:10,319 --> 00:54:12,880
um geared towards your goals and it

1613
00:54:12,880 --> 00:54:14,640
greatly constrains the type of

1614
00:54:14,640 --> 00:54:16,880
exploration and i think from the engine

1615
00:54:16,880 --> 00:54:18,960
you know that also fits with our kind of

1616
00:54:18,960 --> 00:54:19,839
experience maybe

1617
00:54:19,839 --> 00:54:21,680
of daily life or maybe not but i think

1618
00:54:21,680 --> 00:54:23,040
from an engineering perspective

1619
00:54:23,040 --> 00:54:25,280
that's crucial because in real world

1620
00:54:25,280 --> 00:54:26,880
tasks there's just too much to explore

1621
00:54:26,880 --> 00:54:27,680
and you need

1622
00:54:27,680 --> 00:54:29,920
to prioritize that exploration i think

1623
00:54:29,920 --> 00:54:32,400
there's something excellent

1624
00:54:32,400 --> 00:54:35,520
yep that's so beautiful and even another

1625
00:54:35,520 --> 00:54:36,000
layer

1626
00:54:36,000 --> 00:54:38,480
is the ensemble of mountain cars so now

1627
00:54:38,480 --> 00:54:39,760
imagine

1628
00:54:39,760 --> 00:54:41,599
we all see different things and we don't

1629
00:54:41,599 --> 00:54:43,200
know the way to get to the top of the

1630
00:54:43,200 --> 00:54:43,839
mountain

1631
00:54:43,839 --> 00:54:45,200
we don't know the policy we don't even

1632
00:54:45,200 --> 00:54:46,640
know what the end point is we don't know

1633
00:54:46,640 --> 00:54:47,680
if the one that we can see

1634
00:54:47,680 --> 00:54:49,599
close by is the best one or if there's a

1635
00:54:49,599 --> 00:54:51,599
way better one way further away

1636
00:54:51,599 --> 00:54:54,640
and then everybody is who they are and

1637
00:54:54,640 --> 00:54:56,559
they all have their own landscapes

1638
00:54:56,559 --> 00:54:59,280
and then through the ensembles modeling

1639
00:54:59,280 --> 00:55:00,319
collectively

1640
00:55:00,319 --> 00:55:01,920
with or without information sharing of

1641
00:55:01,920 --> 00:55:03,920
whatever kind the ensemble

1642
00:55:03,920 --> 00:55:06,240
as we're seeing gets better performance

1643
00:55:06,240 --> 00:55:08,160
there isn't just one best policy of

1644
00:55:08,160 --> 00:55:08,960
mountain car

1645
00:55:08,960 --> 00:55:10,240
there are so many different ways to

1646
00:55:10,240 --> 00:55:11,680
ascend the mountain and then you open it

1647
00:55:11,680 --> 00:55:13,280
up with what the objective functions and

1648
00:55:13,280 --> 00:55:15,040
the policies the goals could be

1649
00:55:15,040 --> 00:55:18,240
and it really is a a great space shannon

1650
00:55:18,240 --> 00:55:21,280
and then anyone else

1651
00:55:21,280 --> 00:55:24,480
hi thanks i was just um thinking about

1652
00:55:24,480 --> 00:55:26,799
your ensemble of cars here

1653
00:55:26,799 --> 00:55:29,599
and comparing this to like flocking

1654
00:55:29,599 --> 00:55:31,680
behaviors in birds or even

1655
00:55:31,680 --> 00:55:36,079
um people who are foraging together and

1656
00:55:36,079 --> 00:55:39,520
maybe i was wondering how so maybe this

1657
00:55:39,520 --> 00:55:42,240
entire flock is minimizing its

1658
00:55:42,240 --> 00:55:45,280
free energy by getting closer to reward

1659
00:55:45,280 --> 00:55:45,760
which is

1660
00:55:45,760 --> 00:55:48,799
food but any individual bird in the

1661
00:55:48,799 --> 00:55:49,920
flock

1662
00:55:49,920 --> 00:55:53,119
is just following local

1663
00:55:53,119 --> 00:55:55,440
rules about like how close to fly to

1664
00:55:55,440 --> 00:55:57,280
other agents or when to follow them or

1665
00:55:57,280 --> 00:55:58,160
when

1666
00:55:58,160 --> 00:56:01,280
to break off and find a new

1667
00:56:01,280 --> 00:56:03,200
like go away from the rest of the flock

1668
00:56:03,200 --> 00:56:04,640
or drag the flock along

1669
00:56:04,640 --> 00:56:08,000
with them and i wonder if every single

1670
00:56:08,000 --> 00:56:08,880
bird is

1671
00:56:08,880 --> 00:56:12,000
having their own little mountain car

1672
00:56:12,000 --> 00:56:15,200
model or if there's just one gross model

1673
00:56:15,200 --> 00:56:17,599
for the entire flock as a mountain car

1674
00:56:17,599 --> 00:56:19,440
finding

1675
00:56:19,440 --> 00:56:22,480
finding its food yep well

1676
00:56:22,480 --> 00:56:24,799
one angle on the forging and affordances

1677
00:56:24,799 --> 00:56:25,760
is imagine that

1678
00:56:25,760 --> 00:56:27,760
ensemble of human foragers but they're

1679
00:56:27,760 --> 00:56:29,440
not exactly the same

1680
00:56:29,440 --> 00:56:31,119
size or they don't have exactly the same

1681
00:56:31,119 --> 00:56:32,160
preference or they have slightly

1682
00:56:32,160 --> 00:56:32,960
different vision

1683
00:56:32,960 --> 00:56:35,280
some people see closer or further they

1684
00:56:35,280 --> 00:56:37,040
see color or they don't see color

1685
00:56:37,040 --> 00:56:39,359
and so all these differences are what

1686
00:56:39,359 --> 00:56:41,200
allow the ensemble to explore and

1687
00:56:41,200 --> 00:56:42,799
exploit especially with information

1688
00:56:42,799 --> 00:56:43,760
sharing oh hey i

1689
00:56:43,760 --> 00:56:45,440
was over under this tree and i found

1690
00:56:45,440 --> 00:56:46,559
this but somebody else wouldn't have

1691
00:56:46,559 --> 00:56:47,119
found it

1692
00:56:47,119 --> 00:56:49,599
so we can all use the ensemble to take

1693
00:56:49,599 --> 00:56:50,480
advantage of our

1694
00:56:50,480 --> 00:56:52,319
um you know differences of the nest

1695
00:56:52,319 --> 00:56:54,240
mates or the flock or cognitive

1696
00:56:54,240 --> 00:56:55,280
diversity

1697
00:56:55,280 --> 00:56:57,680
and then in the question that you raised

1698
00:56:57,680 --> 00:56:58,319
about

1699
00:56:58,319 --> 00:57:00,640
is there a single uh layer being enacted

1700
00:57:00,640 --> 00:57:01,520
by the flock

1701
00:57:01,520 --> 00:57:03,520
or is there a little mountain car as you

1702
00:57:03,520 --> 00:57:04,799
said by each bird

1703
00:57:04,799 --> 00:57:06,960
so from a modeler's perspective if it

1704
00:57:06,960 --> 00:57:08,319
turns out that we can explain

1705
00:57:08,319 --> 00:57:10,880
variance about the bird's trajectory by

1706
00:57:10,880 --> 00:57:12,079
putting a linear regression

1707
00:57:12,079 --> 00:57:13,440
on it it doesn't say it's linear

1708
00:57:13,440 --> 00:57:14,880
aggression that the bird is doing just

1709
00:57:14,880 --> 00:57:16,400
that it helped us explain variance in

1710
00:57:16,400 --> 00:57:17,040
the world

1711
00:57:17,040 --> 00:57:19,839
so similarly the low bar here is that we

1712
00:57:19,839 --> 00:57:21,599
can use this kind of model

1713
00:57:21,599 --> 00:57:23,280
just like we could use another type of

1714
00:57:23,280 --> 00:57:24,880
control theory model or

1715
00:57:24,880 --> 00:57:27,040
action policy selection model to explain

1716
00:57:27,040 --> 00:57:28,720
variants about the real world

1717
00:57:28,720 --> 00:57:30,559
and especially because we see organisms

1718
00:57:30,559 --> 00:57:32,240
succeeding in fact it's really the only

1719
00:57:32,240 --> 00:57:33,680
ones that we do see

1720
00:57:33,680 --> 00:57:35,760
this helps us explain those successful

1721
00:57:35,760 --> 00:57:38,240
systems as opposed to like funny little

1722
00:57:38,240 --> 00:57:40,400
gifs of you know a robot flailing and

1723
00:57:40,400 --> 00:57:42,000
then whether there's like one layer

1724
00:57:42,000 --> 00:57:43,280
that's being enacted and then

1725
00:57:43,280 --> 00:57:45,599
the group is purely epi phenomenal and

1726
00:57:45,599 --> 00:57:47,040
there's no downward causation

1727
00:57:47,040 --> 00:57:48,559
there's no influence of the group states

1728
00:57:48,559 --> 00:57:50,240
on the lower level states there might be

1729
00:57:50,240 --> 00:57:51,440
some systems like that

1730
00:57:51,440 --> 00:57:52,559
there might be other systems where

1731
00:57:52,559 --> 00:57:54,480
there's an analytical solution like a

1732
00:57:54,480 --> 00:57:55,599
well-defined solution

1733
00:57:55,599 --> 00:57:57,520
at the bird and the flock level there

1734
00:57:57,520 --> 00:57:58,720
might be another one where it's well

1735
00:57:58,720 --> 00:58:00,000
defined at one level

1736
00:58:00,000 --> 00:58:02,160
but then because of interactions and

1737
00:58:02,160 --> 00:58:04,319
emergence it isn't as defined or isn't

1738
00:58:04,319 --> 00:58:06,240
defined by the same class of model

1739
00:58:06,240 --> 00:58:08,160
at a higher or a more coarse-grained

1740
00:58:08,160 --> 00:58:10,480
level

1741
00:58:12,000 --> 00:58:14,640
anything yeah that's great yep cool

1742
00:58:14,640 --> 00:58:15,760
let's look at

1743
00:58:15,760 --> 00:58:18,240
figure two so similarly as you did for

1744
00:58:18,240 --> 00:58:19,920
that on mountain car which was like such

1745
00:58:19,920 --> 00:58:20,960
a helpful

1746
00:58:20,960 --> 00:58:23,280
direction to take what can you tell us

1747
00:58:23,280 --> 00:58:24,319
about the hopper

1748
00:58:24,319 --> 00:58:27,040
v2 or the inverted pendulum which i

1749
00:58:27,040 --> 00:58:28,079
don't have here but those were the two

1750
00:58:28,079 --> 00:58:30,160
tasks maybe more on the hopper

1751
00:58:30,160 --> 00:58:31,680
or the pendulum whichever one just what

1752
00:58:31,680 --> 00:58:33,119
are they about what will machine

1753
00:58:33,119 --> 00:58:34,960
learning people recognize or know these

1754
00:58:34,960 --> 00:58:37,920
models as signifying

1755
00:58:37,920 --> 00:58:41,280
um so hopper isn't

1756
00:58:41,280 --> 00:58:43,760
um particularly expiration based it's

1757
00:58:43,760 --> 00:58:45,760
quite a dense reward that you get so

1758
00:58:45,760 --> 00:58:47,760
you kind of get rewarded or disregarded

1759
00:58:47,760 --> 00:58:49,359
for each of the action

1760
00:58:49,359 --> 00:58:51,520
but uh it's slightly more high

1761
00:58:51,520 --> 00:58:52,880
dimensional

1762
00:58:52,880 --> 00:58:55,839
i can't it says two dimensional there

1763
00:58:55,839 --> 00:58:56,799
but that's just the lip

1764
00:58:56,799 --> 00:58:59,359
that's not when i speak say dimensions i

1765
00:58:59,359 --> 00:59:00,720
mean how many observations

1766
00:59:00,720 --> 00:59:04,480
uh it receives um it's more than two

1767
00:59:04,480 --> 00:59:06,799
it might be like 16 i'm not sure uh and

1768
00:59:06,799 --> 00:59:08,480
it's just generally the dynamics you've

1769
00:59:08,480 --> 00:59:09,760
got to learn you know you've got maybe

1770
00:59:09,760 --> 00:59:11,440
four actions which are real value

1771
00:59:11,440 --> 00:59:13,280
numbers between

1772
00:59:13,280 --> 00:59:15,520
minus four and four you've got to learn

1773
00:59:15,520 --> 00:59:17,440
how that changes

1774
00:59:17,440 --> 00:59:20,319
16 variables over the course of your

1775
00:59:20,319 --> 00:59:21,680
however long you're planning so

1776
00:59:21,680 --> 00:59:26,078
it's a much harder task to learn um

1777
00:59:26,319 --> 00:59:30,000
so i guess that's why it was included

1778
00:59:30,000 --> 00:59:32,400
in in this just that it's generally

1779
00:59:32,400 --> 00:59:33,680
regarded as a bit of a

1780
00:59:33,680 --> 00:59:35,119
harder task in the machine learning

1781
00:59:35,119 --> 00:59:37,760
community cool and so what is happening

1782
00:59:37,760 --> 00:59:38,480
in figure

1783
00:59:38,480 --> 00:59:41,359
2 what can we say how is it different

1784
00:59:41,359 --> 00:59:42,480
than ddpg

1785
00:59:42,480 --> 00:59:46,720
and what actually is ddpg for reference

1786
00:59:46,720 --> 00:59:49,920
ddpg is uh of descending acronyms and

1787
00:59:49,920 --> 00:59:52,559
reinforcing deep deterministic policy

1788
00:59:52,559 --> 00:59:55,119
gradients um so what is that one doing

1789
00:59:55,119 --> 00:59:56,720
and then what is active inference doing

1790
00:59:56,720 --> 00:59:57,520
that's different

1791
00:59:57,520 --> 00:59:59,119
that maybe enables it to have such

1792
00:59:59,119 --> 01:00:00,640
better performance

1793
01:00:00,640 --> 01:00:05,280
um so what is uh dvpd doing so it's just

1794
01:00:05,280 --> 01:00:07,520
a it's a model three reinforcement

1795
01:00:07,520 --> 01:00:09,359
learning program so you have a policy

1796
01:00:09,359 --> 01:00:11,119
that maps from states to actions

1797
01:00:11,119 --> 01:00:13,839
and you essentially do a lot of maths

1798
01:00:13,839 --> 01:00:14,720
from

1799
01:00:14,720 --> 01:00:16,160
something like the bellman equation to

1800
01:00:16,160 --> 01:00:19,118
get to um

1801
01:00:19,520 --> 01:00:23,520
update for your um policy parameters um

1802
01:00:23,520 --> 01:00:25,040
the reason that it's doing so much

1803
01:00:25,040 --> 01:00:26,880
better and the reason that model-based

1804
01:00:26,880 --> 01:00:28,400
reinforcement learning

1805
01:00:28,400 --> 01:00:31,839
in general does so much better um is

1806
01:00:31,839 --> 01:00:34,720
because it's essentially it's a planning

1807
01:00:34,720 --> 01:00:36,559
algorithm so

1808
01:00:36,559 --> 01:00:40,799
um what's the best way to describe this

1809
01:00:40,799 --> 01:00:43,839
yeah um

1810
01:00:44,880 --> 01:00:46,640
due to the fact it's model based it can

1811
01:00:46,640 --> 01:00:49,280
learn from every single

1812
01:00:49,280 --> 01:00:52,079
um bit of data that i've received so

1813
01:00:52,079 --> 01:00:52,960
that is

1814
01:00:52,960 --> 01:00:55,440
in terms of each state transition and as

1815
01:00:55,440 --> 01:00:57,280
well as the reward signals

1816
01:00:57,280 --> 01:00:59,839
um whereas this model free reinforcement

1817
01:00:59,839 --> 01:01:00,640
learning is

1818
01:01:00,640 --> 01:01:02,880
is just simply landing from that single

1819
01:01:02,880 --> 01:01:04,559
bit of information that gets each time

1820
01:01:04,559 --> 01:01:05,440
step which is

1821
01:01:05,440 --> 01:01:10,160
uh um the corresponding authority so

1822
01:01:10,160 --> 01:01:13,599
however your rewards are thinking um

1823
01:01:13,599 --> 01:01:16,240
so i mean eventually ddbg after enough

1824
01:01:16,240 --> 01:01:17,760
epochs would probably uh

1825
01:01:17,760 --> 01:01:21,359
asymptote around or higher than um

1826
01:01:21,359 --> 01:01:24,880
model based that's a kind of general

1827
01:01:24,880 --> 01:01:26,160
pattern you get with model based on

1828
01:01:26,160 --> 01:01:28,000
model 3 reinforced on learning

1829
01:01:28,000 --> 01:01:30,559
model base is far more sample efficient

1830
01:01:30,559 --> 01:01:31,520
uh

1831
01:01:31,520 --> 01:01:33,680
but model 3 takes a hell of a lot more

1832
01:01:33,680 --> 01:01:35,280
samples but is

1833
01:01:35,280 --> 01:01:38,480
asymptotes gets levels out a

1834
01:01:38,480 --> 01:01:42,079
higher reward let me also add a layer

1835
01:01:42,079 --> 01:01:42,559
there

1836
01:01:42,559 --> 01:01:45,520
um with a few other aspects from machine

1837
01:01:45,520 --> 01:01:47,280
learning and the idea of the ruggedness

1838
01:01:47,280 --> 01:01:48,319
of the landscape

1839
01:01:48,319 --> 01:01:50,559
so if you're doing this task which was

1840
01:01:50,559 --> 01:01:52,240
described as a dense task

1841
01:01:52,240 --> 01:01:53,440
another way of thinking about these is

1842
01:01:53,440 --> 01:01:55,039
you're trying to keep something upright

1843
01:01:55,039 --> 01:01:56,640
so it's like very obvious if you're

1844
01:01:56,640 --> 01:01:58,559
succeeding or failing and in the big

1845
01:01:58,559 --> 01:02:00,240
landscape there's really

1846
01:02:00,240 --> 01:02:01,599
easy to tell differences between

1847
01:02:01,599 --> 01:02:04,079
succeeding and failing that being said

1848
01:02:04,079 --> 01:02:06,079
policy planning especially when you have

1849
01:02:06,079 --> 01:02:07,760
four variables of control that are

1850
01:02:07,760 --> 01:02:10,160
projecting out to like 16 potentially

1851
01:02:10,160 --> 01:02:10,880
non-linear

1852
01:02:10,880 --> 01:02:13,119
connected outcomes when you're trying to

1853
01:02:13,119 --> 01:02:14,960
do that there may be many strategic

1854
01:02:14,960 --> 01:02:16,640
mappings there might be many policies

1855
01:02:16,640 --> 01:02:18,319
that help you keep the pendulum up like

1856
01:02:18,319 --> 01:02:19,760
for example going a little bit back and

1857
01:02:19,760 --> 01:02:20,319
forth

1858
01:02:20,319 --> 01:02:22,000
with a certain speed or you can go a

1859
01:02:22,000 --> 01:02:23,920
little further with a different rhythm

1860
01:02:23,920 --> 01:02:25,359
so there might be many many different

1861
01:02:25,359 --> 01:02:28,240
ways even for a single joint of control

1862
01:02:28,240 --> 01:02:30,559
many policy sequences through time many

1863
01:02:30,559 --> 01:02:31,680
learned relationships

1864
01:02:31,680 --> 01:02:34,480
that help you stay in that yes no area

1865
01:02:34,480 --> 01:02:36,000
now if you have a model free

1866
01:02:36,000 --> 01:02:37,359
reinforcement learner

1867
01:02:37,359 --> 01:02:39,520
it means it's learning basically the raw

1868
01:02:39,520 --> 01:02:41,599
connection it's model 3

1869
01:02:41,599 --> 01:02:44,160
so-called between the reward and the

1870
01:02:44,160 --> 01:02:44,960
policy

1871
01:02:44,960 --> 01:02:47,280
and so it may spend a lot of its time

1872
01:02:47,280 --> 01:02:49,200
exploring locally a policy because it's

1873
01:02:49,200 --> 01:02:50,160
like in the spotlight

1874
01:02:50,160 --> 01:02:51,520
it's working and then it goes to a

1875
01:02:51,520 --> 01:02:53,200
slightly different area it's not working

1876
01:02:53,200 --> 01:02:54,480
where do we go from outside of the

1877
01:02:54,480 --> 01:02:56,559
spotlight it's all dark no idea model

1878
01:02:56,559 --> 01:02:57,359
free

1879
01:02:57,359 --> 01:02:59,520
with a deep generative model it's not

1880
01:02:59,520 --> 01:03:01,440
like this spotlight in or out

1881
01:03:01,440 --> 01:03:03,039
reinforcement it's like we're learning

1882
01:03:03,039 --> 01:03:04,559
these intrinsic relationships

1883
01:03:04,559 --> 01:03:06,720
which actually helps us get a grasp of

1884
01:03:06,720 --> 01:03:08,559
the maps that are going to guide us to

1885
01:03:08,559 --> 01:03:09,839
search on the territory

1886
01:03:09,839 --> 01:03:11,680
a little bit more exhaustively but then

1887
01:03:11,680 --> 01:03:13,039
that part at the end which was so

1888
01:03:13,039 --> 01:03:13,920
interesting that

1889
01:03:13,920 --> 01:03:17,359
the model free sometimes gets to higher

1890
01:03:17,359 --> 01:03:19,520
final performance in some context that

1891
01:03:19,520 --> 01:03:20,480
can be because a

1892
01:03:20,480 --> 01:03:23,119
truly model free search can result in

1893
01:03:23,119 --> 01:03:25,119
some wacky combination that wouldn't

1894
01:03:25,119 --> 01:03:25,520
have

1895
01:03:25,520 --> 01:03:28,319
necessarily been approached that it just

1896
01:03:28,319 --> 01:03:30,160
uniquely potentially not in a resilient

1897
01:03:30,160 --> 01:03:32,079
way but it uniquely allows some

1898
01:03:32,079 --> 01:03:34,000
performance on a task and so that

1899
01:03:34,000 --> 01:03:35,440
reminds me of like the evolutionary

1900
01:03:35,440 --> 01:03:36,480
computation where

1901
01:03:36,480 --> 01:03:38,640
its goal will be to travel distance and

1902
01:03:38,640 --> 01:03:39,839
then some will actually go down the

1903
01:03:39,839 --> 01:03:40,799
walking road

1904
01:03:40,799 --> 01:03:42,559
and those start slow but they can walk

1905
01:03:42,559 --> 01:03:44,000
forever and then other ones just like

1906
01:03:44,000 --> 01:03:46,400
fall and so it's kind of like hacking to

1907
01:03:46,400 --> 01:03:47,520
get over

1908
01:03:47,520 --> 01:03:50,960
um a barrier without necessarily uh

1909
01:03:50,960 --> 01:03:52,640
a deeper understanding of the ecology

1910
01:03:52,640 --> 01:03:54,559
because it's so blindly pursuing just

1911
01:03:54,559 --> 01:03:55,760
performance

1912
01:03:55,760 --> 01:03:57,280
so there's a lot of stuff there but this

1913
01:03:57,280 --> 01:03:59,440
is like really an illustrative example

1914
01:03:59,440 --> 01:04:00,640
and it does highlight a lot of the

1915
01:04:00,640 --> 01:04:02,400
differences between the model free

1916
01:04:02,400 --> 01:04:04,000
performance and the state of the art in

1917
01:04:04,000 --> 01:04:05,680
that as well as what potentially active

1918
01:04:05,680 --> 01:04:08,400
inference can bring

1919
01:04:08,400 --> 01:04:10,000
any thoughts on too or now we'll just

1920
01:04:10,000 --> 01:04:11,760
have some general

1921
01:04:11,760 --> 01:04:15,039
areas basically the first area was

1922
01:04:15,039 --> 01:04:16,559
implications i know we've probably

1923
01:04:16,559 --> 01:04:18,319
talked about some of them

1924
01:04:18,319 --> 01:04:21,359
but three areas that i thought about

1925
01:04:21,359 --> 01:04:22,960
were like robotics

1926
01:04:22,960 --> 01:04:26,240
resources and allocation and then rugged

1927
01:04:26,240 --> 01:04:28,400
landscapes this one probably in the

1928
01:04:28,400 --> 01:04:30,079
southwest um

1929
01:04:30,079 --> 01:04:32,400
so any thoughts on these i think we've

1930
01:04:32,400 --> 01:04:34,559
almost touched on

1931
01:04:34,559 --> 01:04:36,880
related ideas but if anyone wants to

1932
01:04:36,880 --> 01:04:39,039
like speak to one of these areas

1933
01:04:39,039 --> 01:04:41,760
of possible implication whether they do

1934
01:04:41,760 --> 01:04:43,200
they don't see what an implication could

1935
01:04:43,200 --> 01:04:43,599
be

1936
01:04:43,599 --> 01:04:45,680
or what would another domain of

1937
01:04:45,680 --> 01:04:47,440
implication be

1938
01:04:47,440 --> 01:04:50,799
um yeah alec go ahead

1939
01:04:50,799 --> 01:04:54,559
so um i also

1940
01:04:54,559 --> 01:04:56,400
i'm becoming increasingly interested in

1941
01:04:56,400 --> 01:04:57,920
so are some other people

1942
01:04:57,920 --> 01:05:01,599
in um whether some so i'm very invested

1943
01:05:01,599 --> 01:05:04,000
in this idea of kind of a bayesian brain

1944
01:05:04,000 --> 01:05:06,720
and um as actual infants kind

1945
01:05:06,720 --> 01:05:08,559
infrastructure as fast as uh but whether

1946
01:05:08,559 --> 01:05:09,680
some of these ideas from bayes and

1947
01:05:09,680 --> 01:05:11,280
machine learning that are used in this

1948
01:05:11,280 --> 01:05:13,760
paper so stuff like amortization

1949
01:05:13,760 --> 01:05:17,119
um might also be employed in nervous

1950
01:05:17,119 --> 01:05:17,839
systems

1951
01:05:17,839 --> 01:05:21,039
um so you know we've had a few

1952
01:05:21,039 --> 01:05:24,079
very very uh big proposals for how the

1953
01:05:24,079 --> 01:05:25,280
brain might implement bayesian infant

1954
01:05:25,280 --> 01:05:26,880
stuff like population coding predictive

1955
01:05:26,880 --> 01:05:27,839
coding

1956
01:05:27,839 --> 01:05:29,599
uh the process theory that's most

1957
01:05:29,599 --> 01:05:30,799
commonly associated with vector

1958
01:05:30,799 --> 01:05:32,079
inference in this terms of message

1959
01:05:32,079 --> 01:05:34,400
passing but it might also be

1960
01:05:34,400 --> 01:05:37,119
you know this amortization is also um

1961
01:05:37,119 --> 01:05:38,559
another real possibility of how the

1962
01:05:38,559 --> 01:05:41,359
brain implements some type of um

1963
01:05:41,359 --> 01:05:42,480
influence so i think it could have

1964
01:05:42,480 --> 01:05:43,839
implications for understanding

1965
01:05:43,839 --> 01:05:45,839
neuroscience

1966
01:05:45,839 --> 01:05:49,039
wow did not expect that message passing

1967
01:05:49,039 --> 01:05:50,960
and predictive processing predictive

1968
01:05:50,960 --> 01:05:52,079
coding

1969
01:05:52,079 --> 01:05:54,640
type models can be understood alongside

1970
01:05:54,640 --> 01:05:56,240
amortization models

1971
01:05:56,240 --> 01:05:58,319
as an alternate implementation or

1972
01:05:58,319 --> 01:06:00,160
mechanism of bayesian

1973
01:06:00,160 --> 01:06:03,680
brain as a specific testable hypothesis

1974
01:06:03,680 --> 01:06:06,160
so this is where the rubber hits the

1975
01:06:06,160 --> 01:06:06,799
road

1976
01:06:06,799 --> 01:06:09,520
with the modeling and with showing the

1977
01:06:09,520 --> 01:06:11,920
deep mathematical isomorphisms between

1978
01:06:11,920 --> 01:06:13,520
these different kinds of relationships

1979
01:06:13,520 --> 01:06:15,039
like there might be some paper i'm sure

1980
01:06:15,039 --> 01:06:17,440
there is where like message passing is

1981
01:06:17,440 --> 01:06:19,760
equivalent to bayesian networks and then

1982
01:06:19,760 --> 01:06:21,039
that allows us to

1983
01:06:21,039 --> 01:06:23,599
bridge to big areas of literature and so

1984
01:06:23,599 --> 01:06:24,079
if we

1985
01:06:24,079 --> 01:06:25,839
do a lot of investigation in bayesian

1986
01:06:25,839 --> 01:06:27,839
brain and then we find out wait there's

1987
01:06:27,839 --> 01:06:29,280
actually multiple ways that could be

1988
01:06:29,280 --> 01:06:30,960
implemented in different systems

1989
01:06:30,960 --> 01:06:32,559
whether that's through message passing

1990
01:06:32,559 --> 01:06:34,000
so maybe that's more applicable to a

1991
01:06:34,000 --> 01:06:35,119
computer network

1992
01:06:35,119 --> 01:06:37,119
maybe it's also going to be implemented

1993
01:06:37,119 --> 01:06:38,799
by policy planning

1994
01:06:38,799 --> 01:06:41,680
ensembles so then whoa what else is

1995
01:06:41,680 --> 01:06:43,280
doing this kind of bayesian

1996
01:06:43,280 --> 01:06:46,400
like processing so there's so many cool

1997
01:06:46,400 --> 01:06:48,799
directions there great topic because

1998
01:06:48,799 --> 01:06:49,680
also

1999
01:06:49,680 --> 01:06:51,359
robotics or at least the question of

2000
01:06:51,359 --> 01:06:53,280
implementation of action and selection

2001
01:06:53,280 --> 01:06:54,480
of policy

2002
01:06:54,480 --> 01:06:56,640
and from sensors and with actuators

2003
01:06:56,640 --> 01:06:58,880
resources in terms of informational

2004
01:06:58,880 --> 01:07:00,640
attentional whatever they may be and

2005
01:07:00,640 --> 01:07:02,480
then rugged landscapes is just

2006
01:07:02,480 --> 01:07:04,240
that's everything that is policy

2007
01:07:04,240 --> 01:07:05,920
selection under uncertainty

2008
01:07:05,920 --> 01:07:08,000
for any kind of system that wants to

2009
01:07:08,000 --> 01:07:09,599
stay alive

2010
01:07:09,599 --> 01:07:12,720
let's go oh sorry sasha or blue oh

2011
01:07:12,720 --> 01:07:15,599
see you later blue oh wait sasha out

2012
01:07:15,599 --> 01:07:16,160
blue

2013
01:07:16,160 --> 01:07:19,760
please thank you wait muted but yeah go

2014
01:07:19,760 --> 01:07:20,640
ahead

2015
01:07:20,640 --> 01:07:22,559
uh so i just i had the same reaction

2016
01:07:22,559 --> 01:07:24,160
that you did like bayesian brain

2017
01:07:24,160 --> 01:07:26,799
amortization like wow i just was

2018
01:07:26,799 --> 01:07:27,359
wondering

2019
01:07:27,359 --> 01:07:29,359
uh alec if you could maybe just unpack

2020
01:07:29,359 --> 01:07:31,200
that like in a very

2021
01:07:31,200 --> 01:07:35,598
simple way or elaborate more on on that

2022
01:07:36,319 --> 01:07:40,960
yeah uh sure so um

2023
01:07:40,960 --> 01:07:44,160
the best way so i mean amortization um

2024
01:07:44,160 --> 01:07:46,000
as it's realized in stuff like various

2025
01:07:46,000 --> 01:07:47,280
norton codes but also

2026
01:07:47,280 --> 01:07:50,240
other schemes um we can like maybe

2027
01:07:50,240 --> 01:07:52,160
discuss a bit more together what defines

2028
01:07:52,160 --> 01:07:53,280
amortization

2029
01:07:53,280 --> 01:07:56,240
are just an instance a way of learning a

2030
01:07:56,240 --> 01:07:57,119
german model

2031
01:07:57,119 --> 01:07:59,839
and well more precisely doing inference

2032
01:07:59,839 --> 01:08:00,400
the key

2033
01:08:00,400 --> 01:08:02,880
defining features for me although i

2034
01:08:02,880 --> 01:08:04,559
don't think this has ever been

2035
01:08:04,559 --> 01:08:07,280
properly defined in the literature is

2036
01:08:07,280 --> 01:08:08,079
this notion

2037
01:08:08,079 --> 01:08:10,799
of having an encoder um how that would

2038
01:08:10,799 --> 01:08:12,640
play out in the brain

2039
01:08:12,640 --> 01:08:14,400
um how it could play out in the brain

2040
01:08:14,400 --> 01:08:16,799
it's just kind of a

2041
01:08:16,799 --> 01:08:19,920
feed-forward mapping um a few forward

2042
01:08:19,920 --> 01:08:20,399
part of

2043
01:08:20,399 --> 01:08:23,120
say the visual cortex that maps from

2044
01:08:23,120 --> 01:08:24,799
some data or a lower part in the

2045
01:08:24,799 --> 01:08:25,839
hierarchy to

2046
01:08:25,839 --> 01:08:28,960
some posterior parameters higher up

2047
01:08:28,960 --> 01:08:30,640
in the hierarchy and when you have stuff

2048
01:08:30,640 --> 01:08:32,319
like um

2049
01:08:32,319 --> 01:08:35,279
population coding or predictive coding

2050
01:08:35,279 --> 01:08:36,960
there's not this notion of

2051
01:08:36,960 --> 01:08:39,359
quickly in a fee-forward manner mapping

2052
01:08:39,359 --> 01:08:41,279
from data to parameters you've got this

2053
01:08:41,279 --> 01:08:43,520
kind of iterative procedure that

2054
01:08:43,520 --> 01:08:47,759
slowly um and sequentially updates the

2055
01:08:47,759 --> 01:08:49,679
beliefs those posterior beliefs based on

2056
01:08:49,679 --> 01:08:50,319
the data

2057
01:08:50,319 --> 01:08:52,640
based on some learning so it's another

2058
01:08:52,640 --> 01:08:54,319
um

2059
01:08:54,319 --> 01:08:56,319
root towards bayesian inference it's far

2060
01:08:56,319 --> 01:08:57,600
far more fast

2061
01:08:57,600 --> 01:08:59,600
and doesn't require recurrent processing

2062
01:08:59,600 --> 01:09:01,520
and that kind of

2063
01:09:01,520 --> 01:09:03,359
speaks well with some things you know

2064
01:09:03,359 --> 01:09:05,600
about the brain it's not all the brain

2065
01:09:05,600 --> 01:09:06,880
the brain definitely does have required

2066
01:09:06,880 --> 01:09:09,198
processing but a lot can be done in

2067
01:09:09,198 --> 01:09:10,640
a kind of feedforward manner very

2068
01:09:10,640 --> 01:09:13,040
quickly uh

2069
01:09:13,040 --> 01:09:14,799
and and the second thing is that um the

2070
01:09:14,799 --> 01:09:16,479
parameters that you're optimizing of the

2071
01:09:16,479 --> 01:09:17,120
encoder

2072
01:09:17,120 --> 01:09:20,080
are optimized um over the entire data

2073
01:09:20,080 --> 01:09:20,799
set whereas

2074
01:09:20,799 --> 01:09:23,439
in something like predictive coding it's

2075
01:09:23,439 --> 01:09:25,040
optimized individually

2076
01:09:25,040 --> 01:09:28,158
the posterior parameters of your beliefs

2077
01:09:28,158 --> 01:09:29,359
are optimized

2078
01:09:29,359 --> 01:09:32,238
based on the current data point uh and i

2079
01:09:32,238 --> 01:09:34,319
think that has some

2080
01:09:34,319 --> 01:09:35,920
that might uh speak to some of the

2081
01:09:35,920 --> 01:09:37,759
generalization capabilities of those

2082
01:09:37,759 --> 01:09:38,399
influence

2083
01:09:38,399 --> 01:09:40,238
um and that's something that we're

2084
01:09:40,238 --> 01:09:41,520
looking at

2085
01:09:41,520 --> 01:09:44,080
so more obviously i hope that kind of

2086
01:09:44,080 --> 01:09:45,839
answers your question

2087
01:09:45,839 --> 01:09:48,080
no definitely thank you so much totally

2088
01:09:48,080 --> 01:09:48,960
epic

2089
01:09:48,960 --> 01:09:51,759
to connect that feed forward encoder

2090
01:09:51,759 --> 01:09:52,399
model

2091
01:09:52,399 --> 01:09:55,520
to the machine learning um side so

2092
01:09:55,520 --> 01:09:57,520
instead of doing like a back and forth

2093
01:09:57,520 --> 01:09:59,760
expectation maximization where you're um

2094
01:09:59,760 --> 01:10:01,440
just updating back and forth

2095
01:10:01,440 --> 01:10:04,800
or especially a uh like a back

2096
01:10:04,800 --> 01:10:06,320
propagation type thing

2097
01:10:06,320 --> 01:10:08,000
where there's very very complex

2098
01:10:08,000 --> 01:10:09,760
interactions about how parameters are

2099
01:10:09,760 --> 01:10:10,480
trained

2100
01:10:10,480 --> 01:10:12,560
this allows us to kind of train on the

2101
01:10:12,560 --> 01:10:14,880
fly and just learn as we go

2102
01:10:14,880 --> 01:10:16,719
now to the second point which is the

2103
01:10:16,719 --> 01:10:18,320
usage of the entire data set versus

2104
01:10:18,320 --> 01:10:19,760
point by point

2105
01:10:19,760 --> 01:10:21,920
this is like we're learning on the fly

2106
01:10:21,920 --> 01:10:22,880
but um

2107
01:10:22,880 --> 01:10:25,280
we're learning on the fly in the state

2108
01:10:25,280 --> 01:10:27,679
space that we want to be learning about

2109
01:10:27,679 --> 01:10:29,920
with respect to the entire data set that

2110
01:10:29,920 --> 01:10:31,679
we have access to

2111
01:10:31,679 --> 01:10:33,199
which is a little bit different than

2112
01:10:33,199 --> 01:10:34,880
like you're kind of trailing your finger

2113
01:10:34,880 --> 01:10:35,679
along a time

2114
01:10:35,679 --> 01:10:38,239
series and then you're updating based

2115
01:10:38,239 --> 01:10:38,880
upon

2116
01:10:38,880 --> 01:10:41,600
that data point and what you believe and

2117
01:10:41,600 --> 01:10:43,040
what you've recently seen

2118
01:10:43,040 --> 01:10:44,719
that's like a kind of point-by-point way

2119
01:10:44,719 --> 01:10:46,400
to use a whole data set

2120
01:10:46,400 --> 01:10:48,640
to update parameters versus this

2121
01:10:48,640 --> 01:10:50,080
variational approach which actually

2122
01:10:50,080 --> 01:10:52,159
enables like almost a simultaneous

2123
01:10:52,159 --> 01:10:53,600
utilization

2124
01:10:53,600 --> 01:10:56,400
of the entire state space i'm not sure

2125
01:10:56,400 --> 01:10:57,280
if that's totally

2126
01:10:57,280 --> 01:10:59,199
correct but those are just two parts

2127
01:10:59,199 --> 01:11:00,719
that hit me about the two

2128
01:11:00,719 --> 01:11:04,000
sides that you mentioned alec

2129
01:11:04,800 --> 01:11:08,480
stephen and then anyone else

2130
01:11:08,640 --> 01:11:10,960
so i've not heard of this amortization

2131
01:11:10,960 --> 01:11:11,920
so it's quite interesting

2132
01:11:11,920 --> 01:11:14,480
is it's almost like in reverse then is

2133
01:11:14,480 --> 01:11:15,840
it it's like you

2134
01:11:15,840 --> 01:11:19,199
you'd have a a big complex place

2135
01:11:19,199 --> 01:11:23,440
of knowing and you quickly discount

2136
01:11:23,440 --> 01:11:25,600
stuff and the process is about discount

2137
01:11:25,600 --> 01:11:26,880
in a way

2138
01:11:26,880 --> 01:11:29,440
rather than building up the model is

2139
01:11:29,440 --> 01:11:30,320
that is that kind of

2140
01:11:30,320 --> 01:11:33,840
the the idea uh

2141
01:11:33,840 --> 01:11:36,480
i'm not sure if i've understand

2142
01:11:36,480 --> 01:11:37,760
understood exactly what you're gonna

2143
01:11:37,760 --> 01:11:38,640
hear but for my

2144
01:11:38,640 --> 01:11:41,679
from i'm not sure if it if it is but

2145
01:11:41,679 --> 01:11:43,520
maybe we could get into that a bit more

2146
01:11:43,520 --> 01:11:45,679
how would you define alec how would you

2147
01:11:45,679 --> 01:11:47,600
define amortized inference like we see

2148
01:11:47,600 --> 01:11:49,040
it here in the paper but

2149
01:11:49,040 --> 01:11:50,640
how would you define it you mentioned

2150
01:11:50,640 --> 01:11:52,080
some benefits we've talked about them a

2151
01:11:52,080 --> 01:11:53,280
little bit but like

2152
01:11:53,280 --> 01:11:54,880
in this kind of new way of thinking

2153
01:11:54,880 --> 01:11:56,560
about it what what does it mean or what

2154
01:11:56,560 --> 01:11:57,920
would it mean for the brain to do it or

2155
01:11:57,920 --> 01:11:58,239
what

2156
01:11:58,239 --> 01:12:01,360
happened programmatically i mean so this

2157
01:12:01,360 --> 01:12:02,320
is what i was getting earlier that

2158
01:12:02,320 --> 01:12:03,440
doesn't really have a

2159
01:12:03,440 --> 01:12:06,640
clear definition um i i trying to fight

2160
01:12:06,640 --> 01:12:07,520
in two ways one

2161
01:12:07,520 --> 01:12:09,920
is how it's generally used if you see

2162
01:12:09,920 --> 01:12:11,040
amortized inference

2163
01:12:11,040 --> 01:12:14,320
in um machine learning uh

2164
01:12:14,320 --> 01:12:17,520
you're generally you should think of a

2165
01:12:17,520 --> 01:12:19,440
encoder network so you've got a neural

2166
01:12:19,440 --> 01:12:22,320
network it takes in your data

2167
01:12:22,320 --> 01:12:24,800
and it will output literal values for

2168
01:12:24,800 --> 01:12:25,679
your parameters

2169
01:12:25,679 --> 01:12:27,360
that would normally and those parameters

2170
01:12:27,360 --> 01:12:29,440
are the ones that would normally be

2171
01:12:29,440 --> 01:12:32,159
optimized in traditional uh variational

2172
01:12:32,159 --> 01:12:33,040
methods

2173
01:12:33,040 --> 01:12:35,199
here it's just um they're just spat out

2174
01:12:35,199 --> 01:12:36,560
by the network

2175
01:12:36,560 --> 01:12:39,199
um and then you have some other um

2176
01:12:39,199 --> 01:12:40,080
learning

2177
01:12:40,080 --> 01:12:42,159
method which which updates that encode

2178
01:12:42,159 --> 01:12:44,080
if you uh and also your your genitive

2179
01:12:44,080 --> 01:12:46,080
model but um

2180
01:12:46,080 --> 01:12:47,760
in terms of what amortization actually

2181
01:12:47,760 --> 01:12:49,120
means going back to the

2182
01:12:49,120 --> 01:12:52,159
word i think derives from economics um

2183
01:12:52,159 --> 01:12:55,040
i think it's this idea that you come

2184
01:12:55,040 --> 01:12:56,719
into the second point that i made where

2185
01:12:56,719 --> 01:12:58,719
uh you're sharing the parameters of that

2186
01:12:58,719 --> 01:13:00,159
encoder

2187
01:13:00,159 --> 01:13:01,360
across all of your data so you're

2188
01:13:01,360 --> 01:13:04,880
amortizing the cost of inference um

2189
01:13:04,880 --> 01:13:06,480
is how i think about it i could be i

2190
01:13:06,480 --> 01:13:08,960
could be very wrong um

2191
01:13:08,960 --> 01:13:12,320
so that just means you you kind of don't

2192
01:13:12,320 --> 01:13:15,360
reset with um each new observation or

2193
01:13:15,360 --> 01:13:16,960
you're not trying to optimize with

2194
01:13:16,960 --> 01:13:19,440
respect to um each new observation and

2195
01:13:19,440 --> 01:13:20,880
then kind of discarding information

2196
01:13:20,880 --> 01:13:21,199
you're

2197
01:13:21,199 --> 01:13:24,560
just sharing that encoder um the

2198
01:13:24,560 --> 01:13:25,760
encoder which is the relationship

2199
01:13:25,760 --> 01:13:26,960
between the data and the parameters

2200
01:13:26,960 --> 01:13:29,360
across uh your entire lifetime so you're

2201
01:13:29,360 --> 01:13:31,199
amortizing the cost of influence

2202
01:13:31,199 --> 01:13:33,040
that's how i think about it so you might

2203
01:13:33,040 --> 01:13:35,679
find a better quote

2204
01:13:35,679 --> 01:13:37,520
so is it like is it like you so as

2205
01:13:37,520 --> 01:13:39,840
opposed to just discounting data it's

2206
01:13:39,840 --> 01:13:43,520
discounting ways of chunking up the data

2207
01:13:43,520 --> 01:13:44,880
or ways of

2208
01:13:44,880 --> 01:13:46,640
analyzing it and if that makes sense

2209
01:13:46,640 --> 01:13:47,840
like you could have lots of different

2210
01:13:47,840 --> 01:13:50,080
models and they say okay

2211
01:13:50,080 --> 01:13:53,040
i'm going to apply them in a more

2212
01:13:53,040 --> 01:13:54,400
efficient way over time

2213
01:13:54,400 --> 01:13:56,719
as you start to infer it's almost

2214
01:13:56,719 --> 01:13:58,400
inferring what models

2215
01:13:58,400 --> 01:14:00,000
should be deployed is that the kind of

2216
01:14:00,000 --> 01:14:01,440
idea and you start to take away the

2217
01:14:01,440 --> 01:14:03,600
models which are less useful

2218
01:14:03,600 --> 01:14:06,560
in a way i guess you could another uh

2219
01:14:06,560 --> 01:14:08,320
kind of

2220
01:14:08,320 --> 01:14:10,080
people sometimes refer to it as learning

2221
01:14:10,080 --> 01:14:11,600
to infer

2222
01:14:11,600 --> 01:14:14,159
so in which might be what you're getting

2223
01:14:14,159 --> 01:14:14,719
at um

2224
01:14:14,719 --> 01:14:17,199
so in normal inference you're just doing

2225
01:14:17,199 --> 01:14:18,080
inference

2226
01:14:18,080 --> 01:14:20,880
but here you're learning how to do

2227
01:14:20,880 --> 01:14:21,520
inference

2228
01:14:21,520 --> 01:14:24,239
so the learning um yeah so maybe that

2229
01:14:24,239 --> 01:14:24,880
was what

2230
01:14:24,880 --> 01:14:26,320
um you're going out with your kind of

2231
01:14:26,320 --> 01:14:28,080
model selection

2232
01:14:28,080 --> 01:14:31,040
cool thanks yeah that that makes sense i

2233
01:14:31,040 --> 01:14:32,400
don't know but i'm

2234
01:14:32,400 --> 01:14:34,880
trying to find my way through there's a

2235
01:14:34,880 --> 01:14:36,800
few dimensions so it's really helpful to

2236
01:14:36,800 --> 01:14:38,320
unpack it there's the learning how to

2237
01:14:38,320 --> 01:14:38,719
learn

2238
01:14:38,719 --> 01:14:40,880
there's that meta learning element but

2239
01:14:40,880 --> 01:14:42,159
also this is

2240
01:14:42,159 --> 01:14:44,480
really important in the paper is the

2241
01:14:44,480 --> 01:14:46,000
number of parameters remains constant

2242
01:14:46,000 --> 01:14:47,360
with respect to the size of the data

2243
01:14:47,360 --> 01:14:49,120
so if i'm trying to estimate the mean

2244
01:14:49,120 --> 01:14:50,400
and the variance of a normal

2245
01:14:50,400 --> 01:14:51,199
distribution

2246
01:14:51,199 --> 01:14:52,640
i know that i want to squeeze down

2247
01:14:52,640 --> 01:14:54,719
whatever data set i have into two

2248
01:14:54,719 --> 01:14:55,600
numbers

2249
01:14:55,600 --> 01:14:57,520
and so if i have three numbers coming in

2250
01:14:57,520 --> 01:14:58,960
the pipeline i still want a mean and a

2251
01:14:58,960 --> 01:14:59,679
variance

2252
01:14:59,679 --> 01:15:01,440
if i want a billion numbers to come in

2253
01:15:01,440 --> 01:15:03,280
the pipeline i still i want

2254
01:15:03,280 --> 01:15:05,280
mean variance and so it turns out that

2255
01:15:05,280 --> 01:15:07,520
by knowing and by specifying ahead of

2256
01:15:07,520 --> 01:15:08,239
time

2257
01:15:08,239 --> 01:15:10,640
that you know exactly the size of the

2258
01:15:10,640 --> 01:15:11,520
outcome

2259
01:15:11,520 --> 01:15:15,199
it allows you to uh scale a lot better

2260
01:15:15,199 --> 01:15:17,679
because you can know that no matter what

2261
01:15:17,679 --> 01:15:19,199
you're going to put in on the inside

2262
01:15:19,199 --> 01:15:20,480
it's going to be able to be distilled

2263
01:15:20,480 --> 01:15:22,880
down very rapidly whereas another

2264
01:15:22,880 --> 01:15:24,960
method that results okay well we put in

2265
01:15:24,960 --> 01:15:26,640
a data set and then we find out

2266
01:15:26,640 --> 01:15:28,640
how many principal components explain it

2267
01:15:28,640 --> 01:15:31,280
best that might computationally

2268
01:15:31,280 --> 01:15:33,600
scale in a way that is very um

2269
01:15:33,600 --> 01:15:35,280
disadvantageous with respect to the size

2270
01:15:35,280 --> 01:15:36,239
of the data

2271
01:15:36,239 --> 01:15:39,040
whereas this method can um do that and

2272
01:15:39,040 --> 01:15:40,320
another then as a

2273
01:15:40,320 --> 01:15:42,400
sort of corollary there it's happening

2274
01:15:42,400 --> 01:15:44,159
through a single forward pass of a

2275
01:15:44,159 --> 01:15:46,000
network which can be ab initio

2276
01:15:46,000 --> 01:15:47,760
sort of like de novo setting it up from

2277
01:15:47,760 --> 01:15:49,760
the beginning and or

2278
01:15:49,760 --> 01:15:52,719
updated in a learning fashion and so

2279
01:15:52,719 --> 01:15:54,480
there's there's a few dimensions and

2280
01:15:54,480 --> 01:15:56,480
alec i'd really look forward to you know

2281
01:15:56,480 --> 01:15:58,159
seeing what you continue to do with the

2282
01:15:58,159 --> 01:15:59,280
amortized inference

2283
01:15:59,280 --> 01:16:01,520
seeing as that concept becomes a bit

2284
01:16:01,520 --> 01:16:02,719
more formalized

2285
01:16:02,719 --> 01:16:04,800
because this seems like really powerful

2286
01:16:04,800 --> 01:16:06,560
and then if it also turns out that it's

2287
01:16:06,560 --> 01:16:07,840
an implementation

2288
01:16:07,840 --> 01:16:11,120
of bayesian statistics or bayesian brain

2289
01:16:11,120 --> 01:16:12,880
then it just taps into the entire

2290
01:16:12,880 --> 01:16:14,239
message passing

2291
01:16:14,239 --> 01:16:17,440
compute graph um bayesian net framework

2292
01:16:17,440 --> 01:16:19,360
which already has been the

2293
01:16:19,360 --> 01:16:21,280
um most helpful for machine learning in

2294
01:16:21,280 --> 01:16:24,320
a lot of other areas too

2295
01:16:25,120 --> 01:16:29,040
very very cool so i guess

2296
01:16:29,040 --> 01:16:30,960
in the next steps which is really i

2297
01:16:30,960 --> 01:16:32,239
guess the last thing that we'll talk

2298
01:16:32,239 --> 01:16:34,080
about for the last few minutes i just

2299
01:16:34,080 --> 01:16:35,120
thought of a few

2300
01:16:35,120 --> 01:16:38,080
areas to go into for next step so alec

2301
01:16:38,080 --> 01:16:38,719
definitely

2302
01:16:38,719 --> 01:16:40,560
you take the first pass but the areas

2303
01:16:40,560 --> 01:16:42,640
could be like computationally

2304
01:16:42,640 --> 01:16:44,480
what does that mean hardware software

2305
01:16:44,480 --> 01:16:46,239
from a math perspective

2306
01:16:46,239 --> 01:16:48,400
what could be analytically shown or what

2307
01:16:48,400 --> 01:16:50,400
relationships would be good to know

2308
01:16:50,400 --> 01:16:52,719
um from an applications perspective what

2309
01:16:52,719 --> 01:16:53,440
robots

2310
01:16:53,440 --> 01:16:55,440
need this kind of software update and

2311
01:16:55,440 --> 01:16:57,360
then just educationally how do we

2312
01:16:57,360 --> 01:16:59,199
come to you know control our attention

2313
01:16:59,199 --> 01:17:00,840
so that we understand these concepts as

2314
01:17:00,840 --> 01:17:03,840
well

2315
01:17:08,320 --> 01:17:10,239
or what did you start working on after

2316
01:17:10,239 --> 01:17:11,920
this paper i guess it was nice oh yeah

2317
01:17:11,920 --> 01:17:14,640
no worries no worries

2318
01:17:14,640 --> 01:17:16,000
i guess you did this one or where are

2319
01:17:16,000 --> 01:17:18,080
you at steps are

2320
01:17:18,080 --> 01:17:21,520
um i guess on this line of thinking that

2321
01:17:21,520 --> 01:17:22,719
the

2322
01:17:22,719 --> 01:17:24,560
um this line of work the stuff that i'm

2323
01:17:24,560 --> 01:17:26,080
still thinking about is

2324
01:17:26,080 --> 01:17:27,040
what was mentioned right at the

2325
01:17:27,040 --> 01:17:29,520
beginning which is the balance of uh

2326
01:17:29,520 --> 01:17:32,480
exploitation and exploration and whether

2327
01:17:32,480 --> 01:17:34,640
the active influence perspective

2328
01:17:34,640 --> 01:17:38,320
buys you um anything in practice

2329
01:17:38,320 --> 01:17:41,199
um because you know we've got this nice

2330
01:17:41,199 --> 01:17:42,000
objective

2331
01:17:42,000 --> 01:17:44,400
functional expected for energy and we

2332
01:17:44,400 --> 01:17:46,640
can optimize actions with respect to it

2333
01:17:46,640 --> 01:17:48,960
and it does contain exploration

2334
01:17:48,960 --> 01:17:50,640
exploitation and it motivates

2335
01:17:50,640 --> 01:17:51,440
exploration

2336
01:17:51,440 --> 01:17:53,920
and exploitation from first phase and

2337
01:17:53,920 --> 01:17:56,080
principles but in practice

2338
01:17:56,080 --> 01:17:58,719
you're gonna be in any model you're

2339
01:17:58,719 --> 01:17:59,360
going to be

2340
01:17:59,360 --> 01:18:02,800
uh fine-tuning those um the balance

2341
01:18:02,800 --> 01:18:03,600
between those

2342
01:18:03,600 --> 01:18:05,280
um you've got a bit more of leeway to do

2343
01:18:05,280 --> 01:18:06,880
it because now you can

2344
01:18:06,880 --> 01:18:09,199
um change the shape of your prior

2345
01:18:09,199 --> 01:18:12,000
beliefs for instance

2346
01:18:12,000 --> 01:18:14,880
a few other parameters that are gonna

2347
01:18:14,880 --> 01:18:16,159
lead to that play off rather than just

2348
01:18:16,159 --> 01:18:17,040
having some uh

2349
01:18:17,040 --> 01:18:19,120
scalar weight that defines them so some

2350
01:18:19,120 --> 01:18:20,560
of the stuff that i've

2351
01:18:20,560 --> 01:18:22,239
been looking at is yeah learning prior

2352
01:18:22,239 --> 01:18:24,960
beliefs uh in order to facilitate

2353
01:18:24,960 --> 01:18:28,159
exploration exploitation um

2354
01:18:28,159 --> 01:18:30,560
because i think to summarize that yeah

2355
01:18:30,560 --> 01:18:31,920
the balance between exploration

2356
01:18:31,920 --> 01:18:32,719
expectations

2357
01:18:32,719 --> 01:18:34,719
is probably one of the big unsolved

2358
01:18:34,719 --> 01:18:36,320
questions in

2359
01:18:36,320 --> 01:18:39,120
um control reinforcement learning

2360
01:18:39,120 --> 01:18:40,960
neuroscience etc

2361
01:18:40,960 --> 01:18:44,960
um and for making these machines work um

2362
01:18:44,960 --> 01:18:48,159
as well as we'd like them to and active

2363
01:18:48,159 --> 01:18:49,600
inference

2364
01:18:49,600 --> 01:18:51,600
offers a new route to try and solve that

2365
01:18:51,600 --> 01:18:54,800
i don't think it has been solved

2366
01:18:54,800 --> 01:18:57,280
awesome that's really exciting there's

2367
01:18:57,280 --> 01:18:58,800
just so many aspects to the

2368
01:18:58,800 --> 01:19:01,440
control theory question when there's a

2369
01:19:01,440 --> 01:19:03,360
lot of solutions that could work and

2370
01:19:03,360 --> 01:19:05,199
maybe it's even unclear what it would

2371
01:19:05,199 --> 01:19:06,480
look like to work

2372
01:19:06,480 --> 01:19:08,320
especially as we think about some of

2373
01:19:08,320 --> 01:19:10,000
these systems that go beyond

2374
01:19:10,000 --> 01:19:12,719
the merely just keep a pendulum standing

2375
01:19:12,719 --> 01:19:13,600
but what if it's

2376
01:19:13,600 --> 01:19:16,159
give someone a massage well that's a

2377
01:19:16,159 --> 01:19:17,520
little bit of a different question it's

2378
01:19:17,520 --> 01:19:18,480
relational

2379
01:19:18,480 --> 01:19:20,560
or how are you going to exercise in a

2380
01:19:20,560 --> 01:19:22,159
way that's comfortable for you

2381
01:19:22,159 --> 01:19:23,840
these are control theory questions that

2382
01:19:23,840 --> 01:19:26,480
are literally about action sequences

2383
01:19:26,480 --> 01:19:28,960
and so when we start thinking about how

2384
01:19:28,960 --> 01:19:30,159
we're going to apply it to different

2385
01:19:30,159 --> 01:19:31,199
systems

2386
01:19:31,199 --> 01:19:33,040
having a framework that is at least

2387
01:19:33,040 --> 01:19:35,040
moving in this direction of

2388
01:19:35,040 --> 01:19:36,239
what you said as like with the

2389
01:19:36,239 --> 01:19:38,320
reconsidering the explore exploit

2390
01:19:38,320 --> 01:19:39,920
by adding things like reconsidering the

2391
01:19:39,920 --> 01:19:41,520
shape of your priors

2392
01:19:41,520 --> 01:19:44,400
so how can you step into a framework

2393
01:19:44,400 --> 01:19:45,840
where the explore exploit

2394
01:19:45,840 --> 01:19:49,040
is a dimension that could be calculated

2395
01:19:49,040 --> 01:19:50,239
or described

2396
01:19:50,239 --> 01:19:53,280
or kind of summarized but isn't simply

2397
01:19:53,280 --> 01:19:55,360
the underlying framework of the model

2398
01:19:55,360 --> 01:19:57,040
we're going to get exploratory behavior

2399
01:19:57,040 --> 01:19:58,239
we're going to get so-called

2400
01:19:58,239 --> 01:19:59,360
exploitative or

2401
01:19:59,360 --> 01:20:01,600
narrowly searching behavior that is not

2402
01:20:01,600 --> 01:20:03,679
being ruled out by what we're discussing

2403
01:20:03,679 --> 01:20:05,600
we're talking about another way that

2404
01:20:05,600 --> 01:20:07,280
optimization could proceed that doesn't

2405
01:20:07,280 --> 01:20:07,920
use

2406
01:20:07,920 --> 01:20:10,880
simply the explore versus exploit or the

2407
01:20:10,880 --> 01:20:12,639
coefficient weighting of these two

2408
01:20:12,639 --> 01:20:15,600
through time or statically to um outline

2409
01:20:15,600 --> 01:20:16,159
its whole

2410
01:20:16,159 --> 01:20:18,639
learning approach so stephen first and

2411
01:20:18,639 --> 01:20:19,360
then

2412
01:20:19,360 --> 01:20:21,360
anyone else if they want to give any

2413
01:20:21,360 --> 01:20:23,120
like last thoughts but this has been a

2414
01:20:23,120 --> 01:20:23,679
great

2415
01:20:23,679 --> 01:20:25,600
conversation so stephen and then anyone

2416
01:20:25,600 --> 01:20:27,600
else

2417
01:20:27,600 --> 01:20:29,360
i mean also if you're going to the real

2418
01:20:29,360 --> 01:20:31,600
world environment you've got this

2419
01:20:31,600 --> 01:20:34,480
exploit situation but in high stakes

2420
01:20:34,480 --> 01:20:36,400
situations you've also got

2421
01:20:36,400 --> 01:20:38,560
um like they often talk about risk

2422
01:20:38,560 --> 01:20:41,520
mitigation versus gain optimization

2423
01:20:41,520 --> 01:20:44,960
if you're in a kind of um a conflict or

2424
01:20:44,960 --> 01:20:47,679
you know a legal situation because it

2425
01:20:47,679 --> 01:20:48,239
may be

2426
01:20:48,239 --> 01:20:50,719
that it's a question of you know which

2427
01:20:50,719 --> 01:20:52,400
is in a way

2428
01:20:52,400 --> 01:20:54,320
gain optimization is a bit like an

2429
01:20:54,320 --> 01:20:56,320
explorer but it depends which way you're

2430
01:20:56,320 --> 01:20:57,679
going it could be an information

2431
01:20:57,679 --> 01:21:00,719
game but whatever you're you're trying

2432
01:21:00,719 --> 01:21:01,840
to do

2433
01:21:01,840 --> 01:21:04,239
those two two aspects sort of come into

2434
01:21:04,239 --> 01:21:05,760
play depending on

2435
01:21:05,760 --> 01:21:09,600
the state of the of the situation

2436
01:21:09,600 --> 01:21:12,239
you know so you know explore exploit is

2437
01:21:12,239 --> 01:21:13,920
less relevant when you're standing on

2438
01:21:13,920 --> 01:21:15,440
the edge of a cliff

2439
01:21:15,440 --> 01:21:17,600
playing a game you know because it's

2440
01:21:17,600 --> 01:21:19,440
like there's a risk mitigation issue so

2441
01:21:19,440 --> 01:21:20,639
anyway i thought that might be

2442
01:21:20,639 --> 01:21:22,560
interesting i just thought about you

2443
01:21:22,560 --> 01:21:23,360
know doing

2444
01:21:23,360 --> 01:21:25,199
learning action policies but if you're

2445
01:21:25,199 --> 01:21:26,560
learning an action policy where you

2446
01:21:26,560 --> 01:21:27,120
could die

2447
01:21:27,120 --> 01:21:29,040
if you're rock climbing or something

2448
01:21:29,040 --> 01:21:30,880
it's going to change how you learn

2449
01:21:30,880 --> 01:21:32,560
and so when you're trying to minimize

2450
01:21:32,560 --> 01:21:34,159
risk versus failure

2451
01:21:34,159 --> 01:21:36,000
attempt or minimize the number of

2452
01:21:36,000 --> 01:21:37,440
opportunities you have to see

2453
01:21:37,440 --> 01:21:39,040
it successfully performed or you can

2454
01:21:39,040 --> 01:21:41,440
only infer it by observing it

2455
01:21:41,440 --> 01:21:43,520
these things may contain a ton of

2456
01:21:43,520 --> 01:21:44,639
information

2457
01:21:44,639 --> 01:21:47,199
in terms of how real systems learn where

2458
01:21:47,199 --> 01:21:48,639
failure is not an option

2459
01:21:48,639 --> 01:21:50,960
mel and then anyone else who wants to

2460
01:21:50,960 --> 01:21:53,440
close it out

2461
01:21:54,320 --> 01:21:56,719
yeah sorry this is i guess a sort of an

2462
01:21:56,719 --> 01:21:57,760
ignorant question but i

2463
01:21:57,760 --> 01:21:59,760
i thought some of like the beauty of

2464
01:21:59,760 --> 01:22:02,639
these acting prince in itp approaches

2465
01:22:02,639 --> 01:22:05,600
is is that they're sort of doing a kind

2466
01:22:05,600 --> 01:22:07,920
of like multi-level dynamic

2467
01:22:07,920 --> 01:22:10,480
outcomes razer type thing so they're

2468
01:22:10,480 --> 01:22:11,920
maximizing

2469
01:22:11,920 --> 01:22:14,560
predictability while well also kind of

2470
01:22:14,560 --> 01:22:16,560
minimizing the complexity

2471
01:22:16,560 --> 01:22:19,679
of what they're doing and

2472
01:22:19,679 --> 01:22:21,920
so i guess what i don't quite understand

2473
01:22:21,920 --> 01:22:23,440
is how the amortization

2474
01:22:23,440 --> 01:22:26,000
of inference process works but if you're

2475
01:22:26,000 --> 01:22:26,960
fixing a

2476
01:22:26,960 --> 01:22:31,440
param doesn't that kind of uh

2477
01:22:31,440 --> 01:22:33,360
constrain the ability of of these

2478
01:22:33,360 --> 01:22:36,080
approaches to to do that kind of

2479
01:22:36,080 --> 01:22:38,960
outcomes razor

2480
01:22:40,800 --> 01:22:44,480
um not really because the

2481
01:22:44,480 --> 01:22:47,600
parameters of your encoder aren't the

2482
01:22:47,600 --> 01:22:48,800
parameters that you're

2483
01:22:48,800 --> 01:22:50,719
they're not the free energy parameters

2484
01:22:50,719 --> 01:22:52,080
they're mapping to the free energy

2485
01:22:52,080 --> 01:22:53,360
parameters

2486
01:22:53,360 --> 01:22:56,639
so you could map you know to

2487
01:22:56,639 --> 01:22:58,639
uh you know free energy just says that

2488
01:22:58,639 --> 01:23:00,480
you've got to maximize uh

2489
01:23:00,480 --> 01:23:02,560
yeah the accuracy of the likelihood

2490
01:23:02,560 --> 01:23:04,639
while minimizing

2491
01:23:04,639 --> 01:23:06,080
the complexity so as long as your

2492
01:23:06,080 --> 01:23:08,000
encoder maps to the part of police

2493
01:23:08,000 --> 01:23:09,040
states that is

2494
01:23:09,040 --> 01:23:12,000
maximally uh accurate and also minimally

2495
01:23:12,000 --> 01:23:13,440
complex then you're still

2496
01:23:13,440 --> 01:23:16,080
minimizing on complexity while

2497
01:23:16,080 --> 01:23:17,679
maximizing accuracy

2498
01:23:17,679 --> 01:23:19,360
uh and then obviously there's what we

2499
01:23:19,360 --> 01:23:20,400
haven't discussed is that learning

2500
01:23:20,400 --> 01:23:21,760
schemes for

2501
01:23:21,760 --> 01:23:23,040
so the way you learn your encoded

2502
01:23:23,040 --> 01:23:24,639
parameters is so that they output

2503
01:23:24,639 --> 01:23:25,440
something that

2504
01:23:25,440 --> 01:23:28,480
does conform to minimal variation of

2505
01:23:28,480 --> 01:23:31,280
energy yes okay so you're not actually

2506
01:23:31,280 --> 01:23:33,040
constraining

2507
01:23:33,040 --> 01:23:34,080
you're not actually placing like

2508
01:23:34,080 --> 01:23:36,880
additional constraints on on the

2509
01:23:36,880 --> 01:23:38,800
params of the actual not on the

2510
01:23:38,800 --> 01:23:40,080
parameters of the encoder

2511
01:23:40,080 --> 01:23:43,199
on the output of the encoder yeah

2512
01:23:43,199 --> 01:23:46,000
um i guess that's a good point uh so the

2513
01:23:46,000 --> 01:23:46,880
encode

2514
01:23:46,880 --> 01:23:48,560
i guess a problem with amortization is

2515
01:23:48,560 --> 01:23:51,360
that um the encode is kind of not part

2516
01:23:51,360 --> 01:23:52,719
you shouldn't think of it as part of

2517
01:23:52,719 --> 01:23:54,239
your generative model it's just almost

2518
01:23:54,239 --> 01:23:55,440
like a

2519
01:23:55,440 --> 01:23:58,800
a tool that can um map

2520
01:23:58,800 --> 01:24:01,679
to the belief space of the genesis

2521
01:24:01,679 --> 01:24:02,080
number

2522
01:24:02,080 --> 01:24:04,320
um

2523
01:24:06,000 --> 01:24:08,000
just on a closing note there that really

2524
01:24:08,000 --> 01:24:10,080
returns us this dual instrumentalism

2525
01:24:10,080 --> 01:24:12,000
with rappers and rappers and is it what

2526
01:24:12,000 --> 01:24:13,360
the system is doing or is it how we're

2527
01:24:13,360 --> 01:24:15,040
looking at it these questions are

2528
01:24:15,040 --> 01:24:17,520
very rich and so it's a great

2529
01:24:17,520 --> 01:24:18,639
conversation

2530
01:24:18,639 --> 01:24:21,679
really thanks everyone for participating

2531
01:24:21,679 --> 01:24:23,679
this was such a helpful discussion and i

2532
01:24:23,679 --> 01:24:25,679
think definitely while re-listening

2533
01:24:25,679 --> 01:24:28,159
we'll all pick out some questions for

2534
01:24:28,159 --> 01:24:31,199
ourselves to follow up on and some

2535
01:24:31,199 --> 01:24:32,960
curious things to learn about because

2536
01:24:32,960 --> 01:24:34,800
there are so many good ideas brought up

2537
01:24:34,800 --> 01:24:36,639
so if people who are on live if they

2538
01:24:36,639 --> 01:24:38,719
check their calendar event they will see

2539
01:24:38,719 --> 01:24:41,360
a feedback form which would be helpful

2540
01:24:41,360 --> 01:24:42,639
and anyone else who's listening or

2541
01:24:42,639 --> 01:24:43,440
watching

2542
01:24:43,440 --> 01:24:44,719
please provide us with feedback

2543
01:24:44,719 --> 01:24:46,639
suggestions or questions

2544
01:24:46,639 --> 01:24:49,440
but other than that just um stay in

2545
01:24:49,440 --> 01:24:50,159
touch

2546
01:24:50,159 --> 01:24:52,800
and everyone awesome work for this

2547
01:24:52,800 --> 01:24:53,840
helpful discussion

2548
01:24:53,840 --> 01:24:59,040
and we'll see you soon

2549
01:24:59,040 --> 01:25:01,840
oh


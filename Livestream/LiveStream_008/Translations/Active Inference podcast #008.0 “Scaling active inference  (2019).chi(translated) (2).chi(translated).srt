1
00:00:08,000 --> 00:00:08,880
好的，

2
00:00:08,880 --> 00:00:11,759
你好，歡迎大家觀看主動

3
00:00:11,759 --> 00:00:13,200
推理直播

4
00:00:13,200 --> 00:00:15,120
這是主動推理直播，

5
00:00:15,120 --> 00:00:17,279
編號 8.0，

6
00:00:17,279 --> 00:00:20,640
現在是 2020 年 11 月 4 日

7
00:00:20,640 --> 00:00:23,920
，我是丹尼爾·弗里德曼，我今天將進行

8
00:00:23,920 --> 00:00:28,000
單獨的情境化討論，

9
00:00:28,000 --> 00:00:30,960
歡迎來到 teamcom，大家我們

10
00:00:30,960 --> 00:00:33,280
是在線實驗

11
00:00:33,280 --> 00:00:35,200
與主動推理相關的團隊交流學習和實踐

12
00:00:35,200 --> 00:00:37,200
您可以

13
00:00:37,200 --> 00:00:38,960
在我們的推特帳戶

14
00:00:38,960 --> 00:00:40,640


15
00:00:40,640 --> 00:00:43,360
inferenceactive 在我們的

16
00:00:43,360 --> 00:00:45,120
公鑰基礎團隊

17
00:00:45,120 --> 00:00:48,160
或我們的 youtube 頻道上找到

18
00:00:48,160 --> 00:00:49,120


19
00:00:49,120 --> 00:00:51,360


20
00:00:51,360 --> 00:00:53,120
我們 這樣我們就可以

21
00:00:53,120 --> 00:00:54,719
改進我們的工作

22
00:00:54,719 --> 00:00:56,559
，歡迎所有背景和觀點在

23
00:00:56,559 --> 00:00:58,559
這裡，

24
00:00:58,559 --> 00:01:00,719
就直播的視頻禮儀而言，如果

25
00:01:00,719 --> 00:01:02,160
您的背景中有噪音，

26
00:01:02,160 --> 00:01:03,440
請舉手，這樣我們就可以聽到

27
00:01:03,440 --> 00:01:05,680
每個人使用尊重的言語行為

28
00:01:05,680 --> 00:01:06,880
等，

29
00:01:06,880 --> 00:01:10,159
所以首先宣布的是

30
00:01:10,159 --> 00:01:12,400
我們已經為 2020 年所有會議選擇了其餘肌動蛋白流的論文以及日期

31
00:01:12,400 --> 00:01:13,200
和時間

32
00:01:13,200 --> 00:01:15,200


33
00:01:15,200 --> 00:01:17,119


34
00:01:17,119 --> 00:01:19,759
2020 年剩餘時間的 gs 將從

35
00:01:19,759 --> 00:01:22,159
太平洋標準時間上午 7 點 30 分到上午 9 點進行

36
00:01:22,159 --> 00:01:24,240
，論文將閱讀第

37
00:01:24,240 --> 00:01:26,159
8 篇正在擴展主動推理，

38
00:01:26,159 --> 00:01:27,439
這

39
00:01:27,439 --> 00:01:30,400
就是 8.0 的內容，這

40
00:01:30,400 --> 00:01:33,119
將在 11 月 10 日和 17 日

41
00:01:33,119 --> 00:01:34,960
發表，論文 9 將是 投射

42
00:01:34,960 --> 00:01:36,560
意識模型和現象

43
00:01:36,560 --> 00:01:39,200
自我 2018 年論文

44
00:01:39,200 --> 00:01:41,759
論文 10 將成為腳本的變體

45
00:01:41,759 --> 00:01:42,880
方法

46
00:01:42,880 --> 00:01:45,200
論文 11 是複雜的主動

47
00:01:45,200 --> 00:01:47,119
推理有效信息抱歉

48
00:01:47,119 --> 00:01:48,880
模擬

49
00:01:48,880 --> 00:01:51,840
想像未來事件的預期有效動態

50
00:01:51,840 --> 00:01:53,280
，你可以看到所有這些事件的日期

51
00:01:53,280 --> 00:01:55,119


52
00:01:55,119 --> 00:01:57,360
如果您有可能

53
00:01:57,360 --> 00:01:58,799
參加，

54
00:01:58,799 --> 00:02:00,719
並且如果您有一個時區或

55
00:02:00,719 --> 00:02:02,159
您想要參加的活動類型，請不時留出

56
00:02:02,159 --> 00:02:04,240
此處未反映的情況，只需讓我們知道

57
00:02:04,240 --> 00:02:06,479
，我們的推特地址

58
00:02:06,479 --> 00:02:07,680
就可以了，

59
00:02:07,680 --> 00:02:10,959
所以這就是活動中將要發生的事情

60
00:02:10,959 --> 00:02:14,800
流 8.0 這個

61
00:02:14,800 --> 00:02:17,120
本次演講的目標是

62
00:02:17,120 --> 00:02:20,000
為 8.1 和 8.2 設置上下文，

63
00:02:20,000 --> 00:02:21,040
這將在

64
00:02:21,040 --> 00:02:23,360
同一篇論文中擴展主動推理 這

65
00:02:23,360 --> 00:02:24,560
是

66
00:02:24,560 --> 00:02:27,760
alex chance baltierri seth 和 buckley

67
00:02:27,760 --> 00:02:29,200
於 2019

68
00:02:29,200 --> 00:02:35,040
年撰寫的論文，存檔 1911.10601

69
00:02:35,040 --> 00:02:36,800
該視頻是

70
00:02:36,800 --> 00:02:38,879
對其中一些想法的背景的介紹，它

71
00:02:38,879 --> 00:02:39,599
不是評論

72
00:02:39,599 --> 00:02:42,000
或最終結論，我絕對

73
00:02:42,000 --> 00:02:43,920
只是通過閱讀論文和研究學到了很多東西

74
00:02:43,920 --> 00:02:47,440
對於這個

75
00:02:47,440 --> 00:02:50,080
演示，我的想法是，這個視頻將把

76
00:02:50,080 --> 00:02:52,080
shantz 論文的

77
00:02:52,080 --> 00:02:55,040
數學、符號和詞彙的一些想法背景化，

78
00:02:55,040 --> 00:02:56,640


79
00:02:56,640 --> 00:02:58,480
並且視頻應該可以訪問，

80
00:02:58,480 --> 00:03:00,480
儘管這也希望是很酷

81
00:03:00,480 --> 00:03:03,680
的前沿研究

82
00:03:03,680 --> 00:03:06,159
和妙語，不用擔心 如果它還

83
00:03:06,159 --> 00:03:07,680
沒有意義或含義

84
00:03:07,680 --> 00:03:08,879


85
00:03:08,879 --> 00:03:11,599
尚不清楚，那麼主動推理是可擴展的並且

86
00:03:11,599 --> 00:03:12,720
它也是同源的，

87
00:03:12,720 --> 00:03:14,640
因此它

88
00:03:14,640 --> 00:03:15,920


89
00:03:15,920 --> 00:03:18,640


90
00:03:18,640 --> 00:03:21,440
類似於控制理論或機器學習等類似空間

91
00:03:21,440 --> 00:03:25,120
中的其他常見算法，並且可能比 8.0 中的其他常見算法更可取 第一

92
00:03:25,120 --> 00:03:28,879
部分將介紹

93
00:03:28,879 --> 00:03:30,560
他們在本文中使用的所有關鍵字的背景

94
00:03:30,560 --> 00:03:33,040
，

95
00:03:33,040 --> 00:03:35,599
然後討論他們提供的目標 tract

96
00:03:35,599 --> 00:03:37,680
和 roadmap

97
00:03:37,680 --> 00:03:40,959
然後 8.0 的第二部分將是

98
00:03:40,959 --> 00:03:42,879
關鍵方程註釋

99
00:03:42,879 --> 00:03:44,879
和引文演練，我們

100
00:03:44,879 --> 00:03:46,000
將像 80 20 那樣做。

101
00:03:46,000 --> 00:03:48,959
所以大多數符號大部分

102
00:03:48,959 --> 00:03:49,840
含義但

103
00:03:49,840 --> 00:03:52,959
不是所有部分不是所有符號

104
00:03:52,959 --> 00:03:55,920
和 然後討論圖 1 和圖

105
00:03:55,920 --> 00:03:56,400
2

106
00:03:56,400 --> 00:03:58,400
以及它們代表什麼以及它們如何

107
00:03:58,400 --> 00:04:00,799
支持論文的結論

108
00:04:00,799 --> 00:04:04,640
，然後在 8.1 和 8.2 中，我們將聚

109
00:04:04,640 --> 00:04:06,720
在一起討論同一篇論文，

110
00:04:06,720 --> 00:04:08,560
所以保存並提交您的問題或將

111
00:04:08,560 --> 00:04:10,319
它們作為評論和

112
00:04:10,319 --> 00:04:12,560
如果你想參與這個

113
00:04:12,560 --> 00:04:15,920
，那就聯繫吧，讓我們從關鍵詞開始

114
00:04:15,920 --> 00:04:17,680


115
00:04:17,680 --> 00:04:19,839


116
00:04:19,839 --> 00:04:21,199


117
00:04:21,199 --> 00:04:23,040


118
00:04:23,040 --> 00:04:25,280
關於那個領域，

119
00:04:25,280 --> 00:04:28,080
希望它們是人工智能

120
00:04:28,080 --> 00:04:28,800


121
00:04:28,800 --> 00:04:31,759
機器學習，他們還提到了

122
00:04:31,759 --> 00:04:33,040
強化學習

123
00:04:33,040 --> 00:04:36,000
和基於模型的強化學習

124
00:04:36,000 --> 00:04:36,560
，然後是

125
00:04:36,560 --> 00:04:38,320
系統、控制和

126
00:04:38,320 --> 00:04:40,240
信息 理論，然後是一個不在

127
00:04:40,240 --> 00:04:40,479


128
00:04:40,479 --> 00:04:42,479
紙上但我們可以在這裡添加的關鍵字，

129
00:04:42,479 --> 00:04:43,840
當然是主動推理和自由

130
00:04:43,840 --> 00:04:45,280
能原理，

131
00:04:45,280 --> 00:04:48,639
所以這些關鍵字中的每一個肯定

132
00:04:48,639 --> 00:04:49,680
你可以有一個

133
00:04:49,680 --> 00:04:52,720
課程或一個博士學位，所以

134
00:04:52,720 --> 00:04:54,240
它們中的每一個都是 將獲得

135
00:04:54,240 --> 00:04:56,160
一張幻燈片，

136
00:04:56,160 --> 00:04:58,960
因此當然要更深入地研究提到的資源

137
00:04:58,960 --> 00:04:59,600


138
00:04:59,600 --> 00:05:02,560
或找到其他教授這些

139
00:05:02,560 --> 00:05:04,000
技術的課程，因為它們有很多

140
00:05:04,000 --> 00:05:05,280


141
00:05:05,280 --> 00:05:07,120


142
00:05:07,120 --> 00:05:08,560
內容

143
00:05:08,560 --> 00:05:10,479
使用數學和這篇研究

144
00:05:10,479 --> 00:05:12,240
論文，只知道其他

145
00:05:12,240 --> 00:05:14,080
人對這些主題的

146
00:05:14,080 --> 00:05:15,120


147
00:05:15,120 --> 00:05:18,560


148
00:05:18,560 --> 00:05:21,840


149
00:05:21,840 --> 00:05:24,160


150
00:05:24,160 --> 00:05:25,120


151
00:05:25,120 --> 00:05:27,280


152
00:05:27,280 --> 00:05:29,199


153
00:05:29,199 --> 00:05:31,759
看法也是如此 人類

154
00:05:31,759 --> 00:05:33,280
和動物，

155
00:05:33,280 --> 00:05:36,240
所以即使使用這個定義，或者

156
00:05:36,240 --> 00:05:37,680
我將在一秒鐘內得到的替代定義，

157
00:05:37,680 --> 00:05:38,479


158
00:05:38,479 --> 00:05:41,280
我們可以說人工智能在今天

159
00:05:41,280 --> 00:05:42,240
2020 年的結果

160
00:05:42,240 --> 00:05:45,039
是什麼 產生地圖政策

161
00:05:45,039 --> 00:05:47,039
個人和集體政策

162
00:05:47,039 --> 00:05:49,120
推薦引擎 它產生

163
00:05:49,120 --> 00:05:51,280
加密技術和安全性

164
00:05:51,280 --> 00:05:53,440
它與分類算法

165
00:05:53,440 --> 00:05:55,039
和影響我們

166
00:05:55,039 --> 00:05:58,800


167
00:05:58,800 --> 00:06:01,600
所有人的事物

168
00:06:01,600 --> 00:06:02,960
有關 他們的

169
00:06:02,960 --> 00:06:06,000
自然世界和一切都是自然

170
00:06:06,000 --> 00:06:09,520
的，因為它存在於世界中，因此通過

171
00:06:09,520 --> 00:06:11,759
假設存在一種人工類型

172
00:06:11,759 --> 00:06:13,120
的認知

173
00:06:13,120 --> 00:06:15,759
，這種認知在某種程度上是獨特的，而不是

174
00:06:15,759 --> 00:06:17,199


175
00:06:17,199 --> 00:06:20,400
由人類製定的擴展或嵌入，這會做

176
00:06:20,400 --> 00:06:21,840
一些不是很有幫助的事情，而且

177
00:06:21,840 --> 00:06:24,000
其他一些很好的

178
00:06:24,000 --> 00:06:26,720
討論這個話題只是為了

179
00:06:26,720 --> 00:06:28,319
讓它朝著積極的方向發展

180
00:06:28,319 --> 00:06:30,000


181
00:06:30,000 --> 00:06:31,919


182
00:06:31,919 --> 00:06:35,360


183
00:06:35,360 --> 00:06:36,720


184
00:06:36,720 --> 00:06:38,720


185
00:06:38,720 --> 00:06:41,600
使用計算機進行統計，但

186
00:06:41,600 --> 00:06:43,520
基本上是筆和紙

187
00:06:43,520 --> 00:06:45,759
可以慢慢完成的事情，所以關於

188
00:06:45,759 --> 00:06:47,440
矩陣計算的事情 e 關於

189
00:06:47,440 --> 00:06:51,680
用計算機對大數據進行統計的

190
00:06:51,680 --> 00:06:54,560
另一種方式，今天的人工智能

191
00:06:54,560 --> 00:06:55,280
就像

192
00:06:55,280 --> 00:06:57,919
人類在循環中人工智能，所以人類在

193
00:06:57,919 --> 00:06:59,520
循環中人工智能可以

194
00:06:59,520 --> 00:07:02,160
描述地圖或推薦引擎，以

195
00:07:02,160 --> 00:07:04,400
強調它在使用過程中一直是人類代理

196
00:07:04,400 --> 00:07:05,039


197
00:07:05,039 --> 00:07:07,120
可供性以及

198
00:07:07,120 --> 00:07:08,720
希望在設計者

199
00:07:08,720 --> 00:07:10,479
認為的

200
00:07:10,479 --> 00:07:13,039
另一種人工智能措辭中可能會

201
00:07:13,039 --> 00:07:13,599
有所幫助，

202
00:07:13,599 --> 00:07:16,400
並且也將人類在

203
00:07:16,400 --> 00:07:17,199
決定

204
00:07:17,199 --> 00:07:19,039
如何使用它以及如何設計它方面的作用居中，這是

205
00:07:19,039 --> 00:07:20,560
智能增強，它

206
00:07:20,560 --> 00:07:22,400
澄清了作為我們誰被

207
00:07:22,400 --> 00:07:24,400
增強或 甚至比人工智能更籠統的

208
00:07:24,400 --> 00:07:26,080
措辭只是人類

209
00:07:26,080 --> 00:07:27,280
技術利基

210
00:07:27,280 --> 00:07:28,639
，它還包括

211
00:07:28,639 --> 00:07:30,720
我們利基的物理方面，例如我們

212
00:07:30,720 --> 00:07:32,400
在 7.2 中討論的內容，

213
00:07:32,400 --> 00:07:34,240
但這將

214
00:07:34,240 --> 00:07:35,680
包括不

215
00:07:35,680 --> 00:07:37,759
只是在矽芯片和

216
00:07:37,759 --> 00:07:39,280
他們的交互，人工智能

217
00:07:39,280 --> 00:07:42,960
捕獲了所有正確的機器學習，

218
00:07:42,960 --> 00:07:46,000
所以機器學習

219
00:07:46,000 --> 00:07:47,840
有很多話要說，這是一個很大的領域，

220
00:07:47,840 --> 00:07:49,039
但主要的

221
00:07:49,039 --> 00:07:51,440
機器學習頂部

222
00:07:51,440 --> 00:07:52,800
在本文中發揮作用的 ics

223
00:07:52,800 --> 00:07:55,199
是強化學習和

224
00:07:55,199 --> 00:07:57,199
基於模型的強化學習，

225
00:07:57,199 --> 00:08:01,039
因此簡單的強化學習器

226
00:08:01,039 --> 00:08:04,560
是環境中的

227
00:08:04,560 --> 00:08:06,000
代理，代理

228
00:08:06,000 --> 00:08:08,800
採取行動，作用於

229
00:08:08,800 --> 00:08:09,520
環境

230
00:08:09,520 --> 00:08:12,240
，然後環境發回

231
00:08:12,240 --> 00:08:14,000
狀態和獎勵

232
00:08:14,000 --> 00:08:15,759
，所以有時 它直接發回

233
00:08:15,759 --> 00:08:17,199
狀態

234
00:08:17,199 --> 00:08:20,080
，然後

235
00:08:20,080 --> 00:08:21,039


236
00:08:21,039 --> 00:08:23,280
通過代理的感知層理解獎勵，其他

237
00:08:23,280 --> 00:08:24,960
時候環境直接將獎勵發

238
00:08:24,960 --> 00:08:27,599


239
00:08:27,599 --> 00:08:29,520
回基於模型的強化學習

240
00:08:29,520 --> 00:08:31,840
建立的地方，這是通過

241
00:08:31,840 --> 00:08:34,080
引入這個模型，這裡用紅色勾勒出來

242
00:08:34,080 --> 00:08:35,039


243
00:08:35,039 --> 00:08:38,320
在這些類型的基於模型的

244
00:08:38,320 --> 00:08:39,440
強化學習中，

245
00:08:39,440 --> 00:08:41,599
而不是學習這些

246
00:08:41,599 --> 00:08:42,880


247
00:08:42,880 --> 00:08:46,640
成功行為和

248
00:08:46,640 --> 00:08:49,440
獎勵之間的簡單原始聯繫以及學習作為行為

249
00:08:49,440 --> 00:08:50,640
相關性

250
00:08:50,640 --> 00:08:53,040
，基於模型的強化學習者

251
00:08:53,040 --> 00:08:54,800
能夠擁有一個模型，

252
00:08:54,800 --> 00:08:57,279
例如，跑步現在很痛苦，

253
00:08:57,279 --> 00:08:58,640
但後來 我以後會感覺好些

254
00:08:58,640 --> 00:09:01,360
，所以它允許一個人

255
00:09:01,360 --> 00:09:03,600
追求目標

256
00:09:03,600 --> 00:09:06,640
為了達到

257
00:09:06,640 --> 00:09:08,800
更大的長期目標，帽子會暫時呃不愉快，所以想想在一個

258
00:09:08,800 --> 00:09:10,560
小迷宮中你知道

259
00:09:10,560 --> 00:09:13,680
不僅要貪婪地走向出口，而且

260
00:09:13,680 --> 00:09:15,040
要後退一步

261
00:09:15,040 --> 00:09:18,080
向前邁出兩步，所以

262
00:09:18,080 --> 00:09:20,160
相似之處在於代理人是

263
00:09:20,160 --> 00:09:22,000
在環境中或在環境中採取行動，並且

264
00:09:22,000 --> 00:09:24,000
他們的行動源於

265
00:09:24,000 --> 00:09:25,760
作為行動模型的政策，

266
00:09:25,760 --> 00:09:28,080
所以什麼是可能的，什麼是不可能的，這些可能

267
00:09:28,080 --> 00:09:29,200


268
00:09:29,200 --> 00:09:32,000
與有機體

269
00:09:32,000 --> 00:09:33,120
實際可以或不能做的事情

270
00:09:33,120 --> 00:09:35,519
同時發生，但至少它是

271
00:09:35,519 --> 00:09:37,440
最終被這個

272
00:09:37,440 --> 00:09:40,160
有機體實現的東西 代理

273
00:09:40,160 --> 00:09:41,440
根據

274
00:09:41,440 --> 00:09:43,040
他們從環境

275
00:09:43,040 --> 00:09:45,760
中得到的獎勵以直接的方式或以像徵性的方式

276
00:09:45,760 --> 00:09:48,800
或以他們學習的基於模型的方式修改他們的行動策略，

277
00:09:48,800 --> 00:09:51,680
或者使用貝葉斯措辭他們

278
00:09:51,680 --> 00:09:52,800
更新他們的模型，

279
00:09:52,800 --> 00:09:54,080
所以 無論如何，它將是

280
00:09:54,080 --> 00:09:55,600
計算的，因為我們將

281
00:09:55,600 --> 00:09:57,279
使用計算機和

282
00:09:57,279 --> 00:10:00,080
數學來描述它，但是貝葉斯主義者

283
00:10:00,080 --> 00:10:00,800
會說，

284
00:10:00,800 --> 00:10:03,440
就像先驗更新了證據以

285
00:10:03,440 --> 00:10:05,279
生成 p  osterior

286
00:10:05,279 --> 00:10:06,560
並且有多種方法

287
00:10:06,560 --> 00:10:08,320
可以完成這種學習或更新

288
00:10:08,320 --> 00:10:09,519
另一個相似之處是

289
00:10:09,519 --> 00:10:11,519
環境向代理髮送獎勵或信號，

290
00:10:11,519 --> 00:10:13,200


291
00:10:13,200 --> 00:10:16,399
而要考慮的一個辯論或問題領域

292
00:10:16,399 --> 00:10:16,880


293
00:10:16,880 --> 00:10:19,200
是觀察獎勵或獎勵

294
00:10:19,200 --> 00:10:20,800


295
00:10:20,800 --> 00:10:22,320
如何 自我學習獎勵

296
00:10:22,320 --> 00:10:23,920
有機器學習算法也

297
00:10:23,920 --> 00:10:25,920
有這種方法

298
00:10:25,920 --> 00:10:28,320
，然後觀察是否象徵

299
00:10:28,320 --> 00:10:29,120
或

300
00:10:29,120 --> 00:10:30,959
表明獎勵或者我們

301
00:10:30,959 --> 00:10:32,399
如何認為這些是

302
00:10:32,399 --> 00:10:34,160
強化學習和行為

303
00:10:34,160 --> 00:10:35,040
算法

304
00:10:35,040 --> 00:10:38,079
可以應用於兩者的方式 抽像數據

305
00:10:38,079 --> 00:10:40,000
分類

306
00:10:40,000 --> 00:10:41,440
方法聚類和類似的

307
00:10:41,440 --> 00:10:43,839
事情以及直接的行動計劃

308
00:10:43,839 --> 00:10:45,279
，事實證明，主動

309
00:10:45,279 --> 00:10:46,640
推理將與強化學習相關，

310
00:10:46,640 --> 00:10:48,240
這是一種有趣的方式，這就是

311
00:10:48,240 --> 00:10:50,560
為什麼作者

312
00:10:50,560 --> 00:10:53,760
現在在現代背景下將它作為關鍵字

313
00:10:53,760 --> 00:10:55,200


314
00:10:55,200 --> 00:10:58,720
學習將使用

315
00:10:58,720 --> 00:11:00,720
例如神經網絡來實現，在這種

316
00:11:00,720 --> 00:11:02,720
情況下，我們有 環境

317
00:11:02,720 --> 00:11:05,839
將狀態發送到代理上的狀態 s

318
00:11:05,839 --> 00:11:07,680
，這將通過

319
00:11:07,680 --> 00:11:10,000
某種深度神經網絡 dnn 並

320
00:11:10,000 --> 00:11:13,440
導致一些策略選擇，

321
00:11:13,440 --> 00:11:15,200
因此神經網絡位於代理內部，

322
00:11:15,200 --> 00:11:16,800
這是我們可以

323
00:11:16,800 --> 00:11:19,839
使用可用的基本軟件實現它的一種方式

324
00:11:19,839 --> 00:11:21,440
這允許某些類型的

325
00:11:21,440 --> 00:11:24,320
可擴展性，因為我們知道

326
00:11:24,320 --> 00:11:26,240


327
00:11:26,240 --> 00:11:28,240
我們的參數有多少模型，我們的

328
00:11:28,240 --> 00:11:31,360
模型有多少，

329
00:11:31,360 --> 00:11:33,279
以及它的有效性如何之間存在關係，然後

330
00:11:33,279 --> 00:11:35,120
它還允許非 - 線性

331
00:11:35,120 --> 00:11:38,160
推理不僅僅是一個簡單的線性

332
00:11:38,160 --> 00:11:40,480
回歸器

333
00:11:40,480 --> 00:11:42,800
，有時是小的，有時

334
00:11:42,800 --> 00:11:44,480
是大的 theta 用於參數，但

335
00:11:44,480 --> 00:11:46,000
我們稍後會談到

336
00:11:46,000 --> 00:11:48,320
，神經網絡可以是無模型的，

337
00:11:48,320 --> 00:11:49,519
所以它可以

338
00:11:49,519 --> 00:11:52,240
放開 找到

339
00:11:52,240 --> 00:11:53,440
數據中的模式，

340
00:11:53,440 --> 00:11:56,880
或者它可以使用基礎模型，

341
00:11:56,880 --> 00:11:58,720
因此如果某種類型的事件是

342
00:11:58,720 --> 00:12:00,399
優先事項，則更有

343
00:12:00,399 --> 00:12:02,480
可能使用這些觀察

344
00:12:02,480 --> 00:12:06,480
來衡量 另一種

345
00:12:06,560 --> 00:12:09,760


346
00:12:09,760 --> 00:12:11,680
現代強化學習的一個例子是這個q

347
00:12:11,680 --> 00:12:13,519
學習，它最近被用於一些遊戲

348
00:12:13,519 --> 00:12:15,040
和一些類似的事情，

349
00:12:15,040 --> 00:12:16,000


350
00:12:16,000 --> 00:12:18,240
洞察力只是另一種方式，

351
00:12:18,240 --> 00:12:19,120
它

352
00:12:19,120 --> 00:12:22,560
是通過映射提示表

353
00:12:22,560 --> 00:12:25,360
，它是狀態動作的映射 和

354
00:12:25,360 --> 00:12:26,399
獎勵，

355
00:12:26,399 --> 00:12:29,040
這樣就可以映射你當前的

356
00:12:29,040 --> 00:12:30,639
狀態和你的行為，

357
00:12:30,639 --> 00:12:32,000
就像哦，我累了，但我會繼續

358
00:12:32,000 --> 00:12:33,680
跑步，因為它是有回報的，

359
00:12:33,680 --> 00:12:36,959
讓我們說，但是以某種數學方式

360
00:12:36,959 --> 00:12:39,040
，然後這裡也代表了深度 q 學習

361
00:12:39,040 --> 00:12:40,240


362
00:12:40,240 --> 00:12:42,320
再一次，它不僅僅是一張桌子，

363
00:12:42,320 --> 00:12:43,920
而是一個神經網絡，如此

364
00:12:43,920 --> 00:12:46,240
現代的內部只是

365
00:12:46,240 --> 00:12:49,120
用深度學習的東西替換任何其他統計模塊

366
00:12:49,120 --> 00:12:50,639


367
00:12:50,639 --> 00:12:52,959
，呃，這顯然允許出現一些

368
00:12:52,959 --> 00:12:55,360
更細微的政策

369
00:12:55,360 --> 00:12:56,880
，然後只是一張幻燈片就扔了

370
00:12:56,880 --> 00:12:58,480
如果你想看的話，你可以暫停

371
00:12:58,480 --> 00:13:00,000
一下，但它來自一篇名為深度強化學習的論文

372
00:13:00,000 --> 00:13:01,120


373
00:13:01,120 --> 00:13:02,880
，它顯示了與多少

374
00:13:02,880 --> 00:13:04,959
不同領域相關的深度

375
00:13:04,959 --> 00:13:05,519


376
00:13:05,519 --> 00:13:07,360
所以這些是問題的結構

377
00:13:07,360 --> 00:13:09,360
以及

378
00:13:09,360 --> 00:13:12,160
本文使用這些關鍵字所涉及的領域

379
00:13:12,160 --> 00:13:13,519


380
00:13:13,519 --> 00:13:16,560
好吧，讓我們從這個

381
00:13:16,560 --> 00:13:18,240
基於模型的強化學習框架開始

382
00:13:18,240 --> 00:13:21,040
，然後討論系統和控制，

383
00:13:21,040 --> 00:13:24,480
所以在這個框架中，我們可以看看

384
00:13:24,480 --> 00:13:26,560
基於模型的強化學習

385
00:13:26,560 --> 00:13:28,880
和綠色綠色和藍色無模型

386
00:13:28,880 --> 00:13:30,639
強化學習，因此無模型

387
00:13:30,639 --> 00:13:31,200
再次

388
00:13:31,200 --> 00:13:32,880
是從經驗到策略的直接映射，

389
00:13:32,880 --> 00:13:34,560


390
00:13:34,560 --> 00:13:36,959
並且越來越抽象的

391
00:13:36,959 --> 00:13:38,720
基於模型的強化學習是

392
00:13:38,720 --> 00:13:40,240
基於經驗的監督學習

393
00:13:40,240 --> 00:13:41,360


394
00:13:41,360 --> 00:13:43,600
到過渡模型的更新

395
00:13:43,600 --> 00:13:44,880
世界的生成模型

396
00:13:44,880 --> 00:13:46,959
，然後使用它來實施

397
00:13:46,959 --> 00:13:48,480
某種規劃過程，

398
00:13:48,480 --> 00:13:52,079
從而產生一些政策選擇，

399
00:13:52,079 --> 00:13:54,880
因此控制系統理論是關於

400
00:13:54,880 --> 00:13:57,279
在不確定性中規劃行動，

401
00:13:57,279 --> 00:13:58,880
因為你必須為行動制定計劃，並且

402
00:13:58,880 --> 00:14:00,639
會有各種不確定性

403
00:14:00,639 --> 00:14:02,480
不確定性

404
00:14:02,480 --> 00:14:04,480
有時它們是彼此的隱喻

405
00:14:04,480 --> 00:14:05,839
這些方法在其他時候是可

406
00:14:05,839 --> 00:14:07,120
轉移的

407
00:14:07,120 --> 00:14:09,360
，一個系統將面臨

408
00:14:09,360 --> 00:14:11,519
一種類型的

409
00:14:11,519 --> 00:14:13,839
挑戰，但它

410
00:14:13,839 --> 00:14:15,920
包括混亂的系統，如雙擺

411
00:14:15,920 --> 00:14:18,000
隨機多尺度部分

412
00:14:18,000 --> 00:14:19,040
觀察到的噪聲

413
00:14:19,040 --> 00:14:22,160
等，所以如果你不能採取行動，你就無法

414
00:14:22,160 --> 00:14:23,920
控制你 只能觀察，這

415
00:14:23,920 --> 00:14:25,279
就是

416
00:14:25,279 --> 00:14:28,720
控制理論與統計

417
00:14:28,720 --> 00:14:30,880
數據的真正區別，因為統計數據是描述性的，

418
00:14:30,880 --> 00:14:34,000
但係統和控制方法

419
00:14:34,000 --> 00:14:35,519
更傾向於

420
00:14:35,519 --> 00:14:37,600


421
00:14:37,600 --> 00:14:39,279
以一種可以乾預的方式理解系統的動態，

422
00:14:39,279 --> 00:14:39,680


423
00:14:39,680 --> 00:14:41,519
因此明確地建模這個規劃過程

424
00:14:41,519 --> 00:14:43,519
和政策

425
00:14:43,519 --> 00:14:46,720
然後當然還有

426
00:14:46,720 --> 00:14:48,800
很多這些模型中不存在的警告

427
00:14:48,800 --> 00:14:50,320
是戰略

428
00:14:50,320 --> 00:14:53,680
公理，即不行動可以是行動的一種形式，

429
00:14:53,680 --> 00:14:55,040


430
00:14:55,040 --> 00:14:56,959
所以這裡的總結是我們希望

431
00:14:56,959 --> 00:14:58,720
有一個數學

432
00:14:58,720 --> 00:15:02,560
模型來討論

433
00:15:02,560 --> 00:15:05,279
系統如何具有 監督或塑造他們的

434
00:15:05,279 --> 00:15:07,040
自我，他們的行動可供性，他們的

435
00:15:07,040 --> 00:15:08,480
系統和他們的世界

436
00:15:08,480 --> 00:15:11,040
，這是很好的監管者，也有

437
00:15:11,040 --> 00:15:11,680
這個

438
00:15:11,680 --> 00:15:14,000
行動政策計劃如此有監督的

439
00:15:14,000 --> 00:15:14,880
學習

440
00:15:14,880 --> 00:15:17,760
是關於監督你的輸入並

441
00:15:17,760 --> 00:15:19,279
適當地更新你

442
00:15:19,279 --> 00:15:21,360
的世界生成模型

443
00:15:21,360 --> 00:15:22,720
，然後你

444
00:15:22,720 --> 00:15:26,000
的世界生成模型是什麼，它將是關於這個

445
00:15:26,000 --> 00:15:29,040
政策選擇的，這裡有一個更

446
00:15:29,040 --> 00:15:32,160
經典的控制理論圖，它的

447
00:15:32,160 --> 00:15:33,040
方式是

448
00:15:33,040 --> 00:15:35,199
系統測量元件控制器和

449
00:15:35,199 --> 00:15:37,279
效應器是相關的，

450
00:15:37,279 --> 00:15:40,480
好吧，信息論是

451
00:15:40,480 --> 00:15:42,560
另一個關鍵字，所以可能

452
00:15:42,560 --> 00:15:43,600
還有

453
00:15:43,600 --> 00:15:47,199
500 小時的 youtube 觀看

454
00:15:47,199 --> 00:15:49,040
信息論，關於

455
00:15:49,040 --> 00:15:51,360
很多不同的領域，你可以說很多，

456
00:15:51,360 --> 00:15:54,480
嗯，一種簡單的表達方式就是

457
00:15:54,480 --> 00:15:55,360
信息

458
00:15:55,360 --> 00:15:57,360
是減少不確定性

459
00:15:57,360 --> 00:15:58,880
是否可行，

460
00:15:58,880 --> 00:16:00,160
因此它不必與

461
00:16:00,160 --> 00:16:02,560
控制理論聯繫起來它可以只是純粹的

462
00:16:02,560 --> 00:16:04,560
描述性，就像獲取

463
00:16:04,560 --> 00:16:07,839
一些 dna 鏈信息的香農熵

464
00:16:07,839 --> 00:16:09,839
總是上下文相關的它是

465
00:16:09,839 --> 00:16:11,680
相關的它依賴於模型

466
00:16:11,680 --> 00:16:12,800
有很多 對

467
00:16:12,800 --> 00:16:16,079
這種對信息論的幼稚香農解釋

468
00:16:16,079 --> 00:16:18,800
的批評有時是 d 信息的

469
00:16:18,800 --> 00:16:20,480
測量或比較

470
00:16:20,480 --> 00:16:22,560
具有挑戰性，例如，如果您

471
00:16:22,560 --> 00:16:24,240
計算

472
00:16:24,240 --> 00:16:26,000


473
00:16:26,000 --> 00:16:28,160
同一個島上昆蟲和植物的生物多樣性的熵，它們

474
00:16:28,160 --> 00:16:29,680
不一定具有相同的規模

475
00:16:29,680 --> 00:16:31,279
等等等等

476
00:16:31,279 --> 00:16:34,320
，呃，有很多地區

477
00:16:34,320 --> 00:16:36,959
信息理論涉及符號學

478
00:16:36,959 --> 00:16:39,120
和語義測量理論

479
00:16:39,120 --> 00:16:43,360
動態系統和信號處理

480
00:16:43,360 --> 00:16:45,519
減少不確定性也不是

481
00:16:45,519 --> 00:16:46,800
真理，因為當然

482
00:16:46,800 --> 00:16:49,839
存在錯誤和精確的

483
00:16:49,839 --> 00:16:51,600
估計，並且在許多情況下，對外部狀態的精確估計

484
00:16:51,600 --> 00:16:52,959
並不能真正促進

485
00:16:52,959 --> 00:16:56,079
有效行動

486
00:16:56,079 --> 00:16:58,160
其他相關領域是信息的

487
00:16:58,160 --> 00:16:59,519
量化

488
00:16:59,519 --> 00:17:03,600
存儲和通信

489
00:17:03,600 --> 00:17:05,280
前面提到的測量比較

490
00:17:05,280 --> 00:17:07,199
你如何

491
00:17:07,199 --> 00:17:08,959
通過時間存儲和傳輸信息

492
00:17:08,959 --> 00:17:10,240
，就像記憶

493
00:17:10,240 --> 00:17:12,400
一樣你如何通信是關於

494
00:17:12,400 --> 00:17:14,079
通過時間

495
00:17:14,079 --> 00:17:14,720
和空間

496
00:17:14,720 --> 00:17:19,199
協議傳輸信息進行通信

497
00:17:19,199 --> 00:17:23,679
以及系統收集信息的時間

498
00:17:23,679 --> 00:17:25,839
減少了它對世界的不確定性

499
00:17:25,839 --> 00:17:28,559
狀態或原因或關係

500
00:17:28,559 --> 00:17:30,400
它可以促進

501
00:17:30,400 --> 00:17:32,480
有效政策模型的更新，因此

502
00:17:32,480 --> 00:17:32,960


503
00:17:32,960 --> 00:17:35,679
在某些情況下（但不是所有情況）

504
00:17:35,679 --> 00:17:37,039
採取行動，當行動有效時，

505
00:17:37,039 --> 00:17:40,320
它可能是有益的、可持續的或

506
00:17:40,320 --> 00:17:42,000
高度適合的，

507
00:17:42,000 --> 00:17:43,760
您可以通過多種方式考慮這種情況

508
00:17:43,760 --> 00:17:45,600
成功取決於機制

509
00:17:45,600 --> 00:17:48,000
規模系統

510
00:17:48,000 --> 00:17:50,480
，然後通過這種獎勵更新參數，

511
00:17:50,480 --> 00:17:52,000


512
00:17:52,000 --> 00:17:54,000
因此無論是植物的哪些基因型

513
00:17:54,000 --> 00:17:55,440


514
00:17:55,440 --> 00:17:58,880
在某個利基市場中成功，

515
00:17:58,880 --> 00:18:00,720
還是哪種計算機病毒

516
00:18:00,720 --> 00:18:02,000
變異得最好

517
00:18:02,000 --> 00:18:03,679
，這是一種學習斜線

518
00:18:03,679 --> 00:18:05,919
發展進化過程

519
00:18:05,919 --> 00:18:07,120
最終我們想要

520
00:18:07,120 --> 00:18:09,600
朝著整合的方向

521
00:18:09,600 --> 00:18:12,240


522
00:18:12,240 --> 00:18:13,919


523
00:18:13,919 --> 00:18:16,160


524
00:18:16,160 --> 00:18:17,679


525
00:18:17,679 --> 00:18:19,280


526
00:18:19,280 --> 00:18:21,039


527
00:18:21,039 --> 00:18:22,640
發展 因為

528
00:18:22,640 --> 00:18:25,520
當我們再次減少

529
00:18:25,520 --> 00:18:25,919
對

530
00:18:25,919 --> 00:18:27,919
因果關係和統計規則的不

531
00:18:27,919 --> 00:18:29,520
確定性時

532
00:18:29,520 --> 00:18:32,080
我們可以製定更好的政策，所以讓

533
00:18:32,080 --> 00:18:33,120
我們掌握

534
00:18:33,120 --> 00:18:35,120
一些基本的信息論概念

535
00:18:35,120 --> 00:18:36,720
，同時引入條件的概念，

536
00:18:36,720 --> 00:18:37,600


537
00:18:37,600 --> 00:18:40,880
所以符號 a 垂直線 b

538
00:18:40,880 --> 00:18:43,440
表示給定的 b 所以

539
00:18:43,440 --> 00:18:44,559
垂直

540
00:18:44,559 --> 00:18:47,840
線之後的部分是你的部分 '以 a 的 h 為條件

541
00:18:47,840 --> 00:18:48,400


542
00:18:48,400 --> 00:18:50,080
是信息內容

543
00:18:50,080 --> 00:18:51,919


544
00:18:51,919 --> 00:18:55,440
，隨機變量 a 以單位或位為單位有多令人驚訝

545
00:18:55,440 --> 00:18:56,640


546
00:18:56,640 --> 00:18:59,200
，只是將這兩個

547
00:18:59,200 --> 00:19:00,480
項目符號點結合起來

548
00:19:00,480 --> 00:19:03,200
，給定 b 的 h 是以 b 為條件的 a 的信息

549
00:19:03,200 --> 00:19:04,080
內容，

550
00:19:04,080 --> 00:19:06,720
對不起，有 a

551
00:19:06,720 --> 00:19:08,880


552
00:19:08,880 --> 00:19:12,320
分號 b 的信息上的拼寫錯誤

553
00:19:12,320 --> 00:19:14,400
是 a 和 b 之間的互信息，

554
00:19:14,400 --> 00:19:15,600
因此

555
00:19:15,600 --> 00:19:17,919
如下

556
00:19:17,919 --> 00:19:20,080
文維恩圖上的這些單一重疊空間所示

557
00:19:20,080 --> 00:19:23,200
，然後還有三重重疊

558
00:19:23,200 --> 00:19:25,280
，您還可以有重疊和

559
00:19:25,280 --> 00:19:26,320
條件，

560
00:19:26,320 --> 00:19:28,160
這樣的方式 我們將要考慮

561
00:19:28,160 --> 00:19:30,160
的數學有點像

562
00:19:30,160 --> 00:19:30,720
嵌套

563
00:19:30,720 --> 00:19:34,000
運算，它包含不同類型的

564
00:19:34,000 --> 00:19:35,840
轉換和潛力，

565
00:19:35,840 --> 00:19:37,679
並且有很多很酷的

566
00:19:37,679 --> 00:19:39,120
信息領域 heory

567
00:19:39,120 --> 00:19:42,480
james gleek 是一位偉大的作家，

568
00:19:42,480 --> 00:19:45,679
這本書中間的石頭是一個

569
00:19:45,679 --> 00:19:49,120
很好的介紹，然後這本書

570
00:19:49,120 --> 00:19:50,320
買了一個尼爾，

571
00:19:50,320 --> 00:19:52,640
算法信息動力學也是

572
00:19:52,640 --> 00:19:53,440
一個很酷的

573
00:19:53,440 --> 00:19:56,960
應用程序和理論，所以讓我們

574
00:19:56,960 --> 00:19:58,720
回到幻燈片的一個小變體。

575
00:19:58,720 --> 00:20:00,640
我們已經研究過

576
00:20:00,640 --> 00:20:05,200
這種不活躍與貝葉斯

577
00:20:05,200 --> 00:20:08,080
二分法的區別，最終活躍

578
00:20:08,080 --> 00:20:09,840
的將試圖

579
00:20:09,840 --> 00:20:11,919
整合到不活躍的世界觀中，我們

580
00:20:11,919 --> 00:20:14,159
在世界上有代理人，他們正在

581
00:20:14,159 --> 00:20:17,440
行動和感知的退出聯繫中製定

582
00:20:17,440 --> 00:20:18,799


583
00:20:18,799 --> 00:20:21,200
這種交互的結果

584
00:20:21,200 --> 00:20:23,679
是來自嵌入代理的具身的和生態的行動

585
00:20:23,679 --> 00:20:26,480
序列或策略，

586
00:20:26,480 --> 00:20:27,280
他們

587
00:20:27,280 --> 00:20:31,039
使斜線是他們的行為，

588
00:20:31,039 --> 00:20:33,360
現在從

589
00:20:33,360 --> 00:20:35,039
控制理論的角度再上一層

590
00:20:35,039 --> 00:20:38,240
到代理，我們可以看到

591
00:20:38,240 --> 00:20:40,720
代理的感覺狀態在 在

592
00:20:40,720 --> 00:20:43,280
輸入方面，他們有他們

593
00:20:43,280 --> 00:20:44,480
的世界因果模型

594
00:20:44,480 --> 00:20:46,960
，然後他們的政策模型在

595
00:20:46,960 --> 00:20:48,480
某種意義上可能像

596
00:20:48,480 --> 00:20:50,480
一個集成單元，或者我們不會得到

597
00:20:50,480 --> 00:20:52,640


598
00:20:52,640 --> 00:20:56,080
稍後我們可以將其與

599
00:20:56,080 --> 00:20:56,640
貝葉斯

600
00:20:56,640 --> 00:20:59,520
結構表示主義視圖對齊，因此

601
00:20:59,520 --> 00:21:00,400
在此

602
00:21:00,400 --> 00:21:02,960
視圖中，您擁有數據觀察

603
00:21:02,960 --> 00:21:04,720
，這些觀察是模型中的基本級別

604
00:21:04,720 --> 00:21:07,120
觀察或參數

605
00:21:07,120 --> 00:21:09,679
，然後通過識別模型

606
00:21:09,679 --> 00:21:11,919
更新超參數

607
00:21:11,919 --> 00:21:12,720
導致

608
00:21:12,720 --> 00:21:15,600
數據的生成模型，

609
00:21:15,600 --> 00:21:16,240


610
00:21:16,240 --> 00:21:20,080
這種期望最大化方案

611
00:21:20,080 --> 00:21:21,919
導致

612
00:21:21,919 --> 00:21:23,360
多層次模型的統計收斂，

613
00:21:23,360 --> 00:21:25,600
因此

614
00:21:25,600 --> 00:21:26,799
代表

615
00:21:26,799 --> 00:21:29,039
世界的代表性結構，因此可能是

616
00:21:29,039 --> 00:21:30,320
結構部分

617
00:21:30,320 --> 00:21:33,679
，然後將其排列起來，我們可以

618
00:21:33,679 --> 00:21:37,520
想想那個模型，它是一個貝葉斯

619
00:21:37,520 --> 00:21:40,720
控制代理，假設

620
00:21:40,720 --> 00:21:42,720
它會有感覺狀態什麼

621
00:21:42,720 --> 00:21:43,919
是我的

622
00:21:43,919 --> 00:21:46,559
機器人傳感器檢測什麼是

623
00:21:46,559 --> 00:21:49,600
因果模型，這對機器人的姿勢意味著什麼

624
00:21:49,600 --> 00:21:51,679
，然後是策略

625
00:21:51,679 --> 00:21:53,200
選擇我們將如何

626
00:21:53,200 --> 00:21:58,000
糾正所以 這些呃兩者都可以

627
00:21:58,000 --> 00:22:00,320
是虛線的這兩個邊

628
00:22:00,320 --> 00:22:01,919
可以是

629
00:22:01,919 --> 00:22:05,200
內部一致的，我們想要

630
00:22:05,200 --> 00:22:06,960
retu 回答這個問題，即

631
00:22:06,960 --> 00:22:07,600


632
00:22:07,600 --> 00:22:09,120
自由能原理和主動

633
00:22:09,120 --> 00:22:10,320
推理將如何說明

634
00:22:10,320 --> 00:22:10,880


635
00:22:10,880 --> 00:22:12,640
世界與主體之間的關係，

636
00:22:12,640 --> 00:22:14,799


637
00:22:14,799 --> 00:22:16,960
因此我們有行動和感知，我們

638
00:22:16,960 --> 00:22:18,640
希望它們在這種利基概念中是對稱的，

639
00:22:18,640 --> 00:22:19,760


640
00:22:19,760 --> 00:22:22,480
我們 '''''''''''''''''''''''0H 談過我們

641
00:22:22,480 --> 00:22:23,120
希望

642
00:22:23,120 --> 00:22:25,520
讓自由能原理成為

643
00:22:25,520 --> 00:22:26,640


644
00:22:26,640 --> 00:22:29,360
我們希望以

645
00:22:29,360 --> 00:22:31,280
多尺度主動推理

646
00:22:31,280 --> 00:22:35,039


647
00:22:35,039 --> 00:22:37,919


648
00:22:37,919 --> 00:22:38,799


649
00:22:38,799 --> 00:22:41,360
視角建立在其之上的公理集的 為

650
00:22:41,360 --> 00:22:43,039
實現這一目標而構建，

651
00:22:43,039 --> 00:22:46,480
因此主動推理 1.0 並向

652
00:22:46,480 --> 00:22:49,120
在斜線上

653
00:22:49,120 --> 00:22:50,480
糾正我的任何這些部分的人們開放，

654
00:22:50,480 --> 00:22:51,440
這只是非常

655
00:22:51,440 --> 00:22:55,360
粗略的時代級別主動推理 1.0

656
00:22:55,360 --> 00:22:57,760
是

657
00:22:57,760 --> 00:23:00,240
一種基於

658
00:23:00,240 --> 00:23:04,159
自由能原理的機器學習類型算法的實現 以及

659
00:23:04,159 --> 00:23:06,960
不活躍的方法，它與

660
00:23:06,960 --> 00:23:08,720
當地感官狀態和政策的協調相關，

661
00:23:08,720 --> 00:23:09,440


662
00:23:09,440 --> 00:23:11,440
所以它有點像

663
00:23:11,440 --> 00:23:12,480
無模型

664
00:23:12,480 --> 00:23:15,360
強化 nt 學習者，它專注於

665
00:23:15,360 --> 00:23:16,960
非常離散的案例，

666
00:23:16,960 --> 00:23:20,080
所以就像在網格上一樣

667
00:23:20,080 --> 00:23:23,120
，它使用了非常簡單的期望

668
00:23:23,120 --> 00:23:25,200
最大化算法，並且

669
00:23:25,200 --> 00:23:27,440
具有相對較短的空間和/

670
00:23:27,440 --> 00:23:28,240
或時間

671
00:23:28,240 --> 00:23:31,440
範圍或深度，它更像是一個

672
00:23:31,440 --> 00:23:34,799
呃讓我看看我的

673
00:23:34,799 --> 00:23:38,559
視頻 不知道這裡發生了什麼，

674
00:23:38,960 --> 00:23:41,200


675
00:23:42,799 --> 00:23:45,840
對不起，

676
00:23:46,960 --> 00:23:50,159
所以我要

677
00:23:50,159 --> 00:23:53,520
重新添加我的相機，

678
00:24:02,840 --> 00:24:05,840
對不起，

679
00:24:07,360 --> 00:24:11,360
好吧，無論如何，c'est la vie 是

680
00:24:11,360 --> 00:24:15,039
活躍的 2.0 將

681
00:24:15,039 --> 00:24:17,760
時間深度和利基交互添加

682
00:24:17,760 --> 00:24:20,240
到具有長期規劃階段的方程中，

683
00:24:20,240 --> 00:24:23,440
並且還添加了 在

684
00:24:23,440 --> 00:24:25,679
代理嵌入的這方面，並將

685
00:24:25,679 --> 00:24:28,000
效價和學習的概念引入

686
00:24:28,000 --> 00:24:29,200


687
00:24:29,200 --> 00:24:32,159
到公式中，這

688
00:24:32,159 --> 00:24:34,159
就是主動推理 3.0

689
00:24:34,159 --> 00:24:37,600
及更高版本的發展方向，因此他們

690
00:24:37,600 --> 00:24:38,240
一直在

691
00:24:38,240 --> 00:24:40,880
朝著學習和影響以及行動的

692
00:24:40,880 --> 00:24:43,520
複雜或反事實

693
00:24:43,520 --> 00:24:43,919
方面發展

694
00:24:43,919 --> 00:24:46,960
這是一個引文

695
00:24:46,960 --> 00:24:49,039
，然後它的另一個方向

696
00:24:49,039 --> 00:24:50,080


697
00:24:50,080 --> 00:24:52,080
反映在這個縮放主動

698
00:24:52,080 --> 00:24:53,200
推理論文中

699
00:24:53,200 --> 00:24:55,200
，它將介紹

700
00:24:55,200 --> 00:24:56,400
高維

701
00:24:56,400 --> 00:24:59,200
數據 連續變量探索利用

702
00:24:59,200 --> 00:25:00,000
行為

703
00:25:00,000 --> 00:25:02,559
以及與強化學習

704
00:25:02,559 --> 00:25:04,240
和大規模工作的同源性，

705
00:25:04,240 --> 00:25:07,919
所以對我來說，閱讀這篇論文就像英雄的旅程，

706
00:25:07,919 --> 00:25:10,000
因為這只是

707
00:25:10,000 --> 00:25:11,919
一個完成故事的框架，

708
00:25:11,919 --> 00:25:13,919
所以這就是擴展主動

709
00:25:13,919 --> 00:25:16,000
推理

710
00:25:16,000 --> 00:25:18,799
之旅 所以它開始於對冒險的呼籲，

711
00:25:18,799 --> 00:25:19,760


712
00:25:19,760 --> 00:25:22,799
這是一個問題，我們如何

713
00:25:22,799 --> 00:25:25,200
通過主動推理來模擬複雜的控制任務，

714
00:25:25,200 --> 00:25:26,640


715
00:25:26,640 --> 00:25:27,840
這是轉型的開始，

716
00:25:27,840 --> 00:25:30,240
挑戰在於

717
00:25:30,240 --> 00:25:31,679
誘惑

718
00:25:31,679 --> 00:25:34,240
應該去睡覺還是

719
00:25:34,240 --> 00:25:35,120
應該閱讀

720
00:25:35,120 --> 00:25:38,320
弗里斯滕的

721
00:25:38,320 --> 00:25:40,960
參考書目助手來了 進入圖片這裡是

722
00:25:40,960 --> 00:25:42,799
亞歷克，

723
00:25:42,799 --> 00:25:45,120
當我們閱讀時，我們有一個變革性的時刻

724
00:25:45,120 --> 00:25:46,720
，當我們

725
00:25:46,720 --> 00:25:49,919
理解論文擴展主動推理時，

726
00:25:49,919 --> 00:25:50,880
我們就會

727
00:25:50,880 --> 00:25:52,880
從黑暗到光明，從

728
00:25:52,880 --> 00:25:54,640
強化學習到主動

729
00:25:54,640 --> 00:25:55,440
推理，

730
00:25:55,440 --> 00:25:59,200
從失敗到控制，一路走來，我們

731
00:25:59,200 --> 00:26:02,000
收到了會議的禮物 新朋友

732
00:26:02,000 --> 00:26:04,159
討論

733
00:26:04,159 --> 00:26:06,880
對系統有影響的酷想法並製作

734
00:26:06,880 --> 00:26:07,840
冷杉 st 和

735
00:26:07,840 --> 00:26:11,440
meme 當然讓我們來看看論文，

736
00:26:11,440 --> 00:26:14,240
因此擴展主動推理是論文提出本文

737
00:26:14,240 --> 00:26:15,360
的目標

738
00:26:15,360 --> 00:26:18,400
是因為我們提出了一個

739
00:26:18,400 --> 00:26:20,400
主動推理模型，該模型適用

740
00:26:20,400 --> 00:26:22,320
於我們模型構建的

741
00:26:22,320 --> 00:26:24,799
具有連續狀態和動作的高維控制任務

742
00:26:24,799 --> 00:26:27,200
在之前嘗試

743
00:26:27,200 --> 00:26:28,880
通過包含

744
00:26:28,880 --> 00:26:30,640
有效的規划算法

745
00:26:30,640 --> 00:26:32,000
以及模型不確定性的量化和主動

746
00:26:32,000 --> 00:26:34,159
解決來擴展主動推理的過程中，

747
00:26:34,159 --> 00:26:35,440
我們的模型做出了兩個主要

748
00:26:35,440 --> 00:26:37,600
貢獻，首先我們

749
00:26:37,600 --> 00:26:39,360
證明了完整的主動

750
00:26:39,360 --> 00:26:41,279
推理構造可以擴展

751
00:26:41,279 --> 00:26:43,440
到 rl 文獻

752
00:26:43,440 --> 00:26:45,279
這涉及擴展先前

753
00:26:45,279 --> 00:26:46,960
的深度主動推理模型以包括

754
00:26:46,960 --> 00:26:49,600
模型不確定性和預期信息增益

755
00:26:49,600 --> 00:26:51,600
第二我們強調

756
00:26:51,600 --> 00:26:53,120
主動推理最先進的

757
00:26:53,120 --> 00:26:53,840
方法

758
00:26:53,840 --> 00:26:56,880
與基於模型的強化學習之間的重疊，因此

759
00:26:56,880 --> 00:26:58,960
這裡的非主動推理措辭

760
00:26:58,960 --> 00:27:01,840
是 我們如何將主動推理應用於

761
00:27:01,840 --> 00:27:03,760
具有挑戰性的控制任務

762
00:27:03,760 --> 00:27:05,520
並將其連接起來 正式到強化

763
00:27:05,520 --> 00:27:08,480
學習，所以摘要是

764
00:27:08,480 --> 00:27:11,760
在強化學習中 代理經常

765
00:27:11,760 --> 00:27:13,600
在部分觀察和

766
00:27:13,600 --> 00:27:15,120
不確定的環境中運行

767
00:27:15,120 --> 00:27:17,360
基於模型的強化學習

768
00:27:17,360 --> 00:27:19,520
表明，這最好

769
00:27:19,520 --> 00:27:21,440
通過學習和

770
00:27:21,440 --> 00:27:24,720
利用世界的概率模型來實現

771
00:27:24,720 --> 00:27:26,240
主動推理

772
00:27:26,240 --> 00:27:28,080
是認知和認知領域新興的規範框架

773
00:27:28,080 --> 00:27:29,760
計算神經科學

774
00:27:29,760 --> 00:27:31,919
提供了

775
00:27:31,919 --> 00:27:34,480
生物代理如何

776
00:27:34,480 --> 00:27:37,039
在這個框架上實現這一目標的統一解釋，推理學習和

777
00:27:37,039 --> 00:27:37,600
行動

778
00:27:37,600 --> 00:27:39,360
源於一個單一的命令，以

779
00:27:39,360 --> 00:27:41,039
最大化世界共享利基的貝葉斯證據，

780
00:27:41,039 --> 00:27:43,919


781
00:27:43,919 --> 00:27:46,480
但是迄今為止，該過程的實現

782
00:27:46,480 --> 00:27:48,080
僅限於低

783
00:27:48,080 --> 00:27:50,559
維 和理想化的情況

784
00:27:50,559 --> 00:27:53,600
，我們提出了

785
00:27:53,600 --> 00:27:55,679
一種適用於高維任務的主動推理的工作實現，其

786
00:27:55,679 --> 00:27:57,279


787
00:27:57,279 --> 00:27:58,320
原理結果

788
00:27:58,320 --> 00:28:00,960
證明證明了有效的探索和

789
00:28:00,960 --> 00:28:02,080


790
00:28:02,080 --> 00:28:04,320
样本效率在

791
00:28:04,320 --> 00:28:06,799
強大的無模型基線上下降

792
00:28:06,799 --> 00:28:08,880
了一個數量級 結果證明了

793
00:28:08,880 --> 00:28:11,360
大規模應用主動推理的可行性，

794
00:28:11,360 --> 00:28:13,360
並突出了

795
00:28:13,360 --> 00:28:14,640
主動推理

796
00:28:14,640 --> 00:28:16,640
與當前基於模型的

797
00:28:16,640 --> 00:28:18,240
強化學習方法之間的操作

798
00:28:18,240 --> 00:28:21,600
同源性，所以對於你們中的一些人來說，

799
00:28:21,600 --> 00:28:25,039
這可能是很多新想法，其他人

800
00:28:25,039 --> 00:28:28,320
可能會在那裡認識到很多，所以

801
00:28:28,320 --> 00:28:30,480
讓我們只是 為

802
00:28:30,480 --> 00:28:31,679
我們如何從

803
00:28:31,679 --> 00:28:34,960
a 到 z 制定路線圖，然後希望

804
00:28:34,960 --> 00:28:37,600
它可以從所有這些不同的角度變得平易近人，第一

805
00:28:37,600 --> 00:28:38,240


806
00:28:38,240 --> 00:28:41,600
部分

807
00:28:41,600 --> 00:28:44,000
是介紹，第二部分是關於

808
00:28:44,000 --> 00:28:45,279
主動推理

809
00:28:45,279 --> 00:28:47,919
，在第三領域所做的工作

810
00:28:47,919 --> 00:28:48,399


811
00:28:48,399 --> 00:28:49,760
是 我們將

812
00:28:49,760 --> 00:28:51,440
在本次

813
00:28:51,440 --> 00:28:55,279
討論中關注的模型和 3.1 是生成

814
00:28:55,279 --> 00:28:56,840
模型和識別

815
00:28:56,840 --> 00:28:59,279
分佈，它們是 p 和 q

816
00:28:59,279 --> 00:29:00,880
模型，

817
00:29:00,880 --> 00:29:02,480
然後是學習和

818
00:29:02,480 --> 00:29:04,240
推理策略選擇

819
00:29:04,240 --> 00:29:06,080
軌跡採樣和預期自由

820
00:29:06,080 --> 00:29:08,000
能的部分

821
00:29:08,000 --> 00:29:09,760
作為完全

822
00:29:09,760 --> 00:29:11,600
觀察模型的一部分，這些部分是我們

823
00:29:11,600 --> 00:29:12,240


824
00:29:12,240 --> 00:29:15,840
今天不會深入討論的部分，然後是相關的

825
00:29:15,840 --> 00:29:17,039


826
00:29:17,039 --> 00:29:20,799
實驗 涉及到探索和開發

827
00:29:20,799 --> 00:29:22,880
以及與

828
00:29:22,880 --> 00:29:25,039
探索

829
00:29:25,039 --> 00:29:26,880
策略比較和連續控制任務上的性能比較相關的兩個數字，

830
00:29:26,880 --> 00:29:28,720


831
00:29:28,720 --> 00:29:31,200
然後是與以前工作的關係，

832
00:29:31,200 --> 00:29:32,240
特別是

833
00:29:32,240 --> 00:29:33,760
基於深度主動推理模型的

834
00:29:33,760 --> 00:29:35,520
強化學習和信息

835
00:29:35,520 --> 00:29:35,919
增益

836
00:29:35,919 --> 00:29:38,840
理論，然後有一個討論和

837
00:29:38,840 --> 00:29:40,559


838
00:29:40,559 --> 00:29:42,879
結論 正確的

839
00:29:44,000 --> 00:29:47,360
這句話，你可以

840
00:29:47,360 --> 00:29:51,440
停下來閱讀全文

841
00:29:51,440 --> 00:29:52,799


842
00:29:52,799 --> 00:29:55,279


843
00:29:55,279 --> 00:29:57,200


844
00:29:57,200 --> 00:29:59,279


845
00:29:59,279 --> 00:30:01,039


846
00:30:01,039 --> 00:30:02,960
動作

847
00:30:02,960 --> 00:30:03,919
空間

848
00:30:03,919 --> 00:30:06,000
意味著它不能直接

849
00:30:06,000 --> 00:30:07,919


850
00:30:07,919 --> 00:30:09,520
應用於強化學習基準中考慮的高維狀態和連續動作，

851
00:30:09,520 --> 00:30:10,880


852
00:30:10,880 --> 00:30:12,720
因此這就是為什麼在作者的措辭中沒有比較基於主動

853
00:30:12,720 --> 00:30:13,919
推理離散

854
00:30:13,919 --> 00:30:16,000
狀態的學習器的原因之一

855
00:30:16,000 --> 00:30:17,120


856
00:30:17,120 --> 00:30:20,320
是使用

857
00:30:20,320 --> 00:30:22,240
的強化學習基準

858
00:30:22,240 --> 00:30:23,760


859
00:30:23,760 --> 00:30:25,600
測試適當的

860
00:30:25,600 --> 00:30:27,840
不同強化學習

861
00:30:27,840 --> 00:30:31,360
控制理論算法的有效性或有效性

862
00:30:31,360 --> 00:30:33,279
是連續任務，就像

863
00:30:33,279 --> 00:30:35,039
我們將在圖中討論的那種

864
00:30:35,039 --> 00:30:37,679
，因此本文要做的是

865
00:30:37,679 --> 00:30:39,520
採用一種稱為

866
00:30:39,520 --> 00:30:41,440
攤銷推理的方法，我們將

867
00:30:41,440 --> 00:30:44,080
對此進行更多討論

868
00:30:44,080 --> 00:30:46,720
稍微利用函數逼近器，

869
00:30:46,720 --> 00:30:47,200
即

870
00:30:47,200 --> 00:30:48,960
神經網絡來參數化

871
00:30:48,960 --> 00:30:50,240
分佈，

872
00:30:50,240 --> 00:30:51,919
然後將自由能

873
00:30:51,919 --> 00:30:53,520
相對於函數逼近器的參數而

874
00:30:53,520 --> 00:30:55,039
不是

875
00:30:55,039 --> 00:30:57,120
變分參數本身最小化

876
00:30:57,120 --> 00:30:59,200
，然後它們重申每個

877
00:30:59,200 --> 00:31:01,679
部分和三個

878
00:31:01,679 --> 00:31:04,240
是符號參考 表

879
00:31:04,240 --> 00:31:06,159
不是每個單獨的

880
00:31:06,159 --> 00:31:09,440
參數，尤其

881
00:31:09,440 --> 00:31:12,720
是分佈的統計參數

882
00:31:12,720 --> 00:31:15,440
和很多後面的部分，但這

883
00:31:15,440 --> 00:31:15,760


884
00:31:15,760 --> 00:31:17,760
是我們將

885
00:31:17,760 --> 00:31:18,960


886
00:31:18,960 --> 00:31:22,960
在接下來的幻燈片中看到的參數類型，

887
00:31:22,960 --> 00:31:26,080
我們真的

888
00:31:26,080 --> 00:31:28,799
可以看看什麼

889
00:31:28,799 --> 00:31:30,480
該模型將涵蓋的許多主題

890
00:31:30,480 --> 00:31:33,120
都是如此，例如時代政策

891
00:31:33,120 --> 00:31:34,000
nt

892
00:31:34,000 --> 00:31:36,000
它試圖估計

893
00:31:36,000 --> 00:31:37,600
的狀態和真正的狀態隨著

894
00:31:37,600 --> 00:31:38,799
時間的推移它會涉及到

895
00:31:38,799 --> 00:31:40,399
觀察

896
00:31:40,399 --> 00:31:41,519
我們有點

897
00:31:41,519 --> 00:31:44,159


898
00:31:44,159 --> 00:31:45,279


899
00:31:45,279 --> 00:31:48,320
看到我們之前討論過的強化學習算法的很多成分所以

900
00:31:48,320 --> 00:31:48,880
讓我們

901
00:31:48,880 --> 00:31:53,360
談論這個攤銷的推理

902
00:31:53,360 --> 00:31:55,360
這裡是他們寫的作者的

903
00:31:55,360 --> 00:31:56,480
機會

904
00:31:56,480 --> 00:31:58,240
攤銷推理過程

905
00:31:58,240 --> 00:31:59,760
提供了幾個好處

906
00:31:59,760 --> 00:32:01,600
例如參數的數量

907
00:32:01,600 --> 00:32:03,519
相對於

908
00:32:03,519 --> 00:32:04,960


909
00:32:04,960 --> 00:32:06,320
數據的大小保持不變並且可以

910
00:32:06,320 --> 00:32:08,080
通過單次前向傳遞

911
00:32:08,080 --> 00:32:09,840
來實現推理 網絡而不是讓我們說

912
00:32:09,840 --> 00:32:12,080
反向傳播訓練

913
00:32:12,080 --> 00:32:14,240
此外，雖然

914
00:32:14,240 --> 00:32:16,320
關於變量的編碼信息量是固定

915
00:32:16,320 --> 00:32:17,840
的，但變量之間的條件關係

916
00:32:17,840 --> 00:32:20,080
可以

917
00:32:20,080 --> 00:32:21,840
在等式三中任意複雜，我們將得到轉換

918
00:32:21,840 --> 00:32:23,200


919
00:32:23,200 --> 00:32:26,640
分佈數據的參數本身就是

920
00:32:26,640 --> 00:32:28,799
隨機變量 當前上下文

921
00:32:28,799 --> 00:32:30,159
這些參數是

922
00:32:30,159 --> 00:32:32,159


923
00:32:32,159 --> 00:32:34,000
這種方法的神經網絡

924
00:32:34,000 --> 00:32:35,679
允許量化這些參數的不確定性

925
00:32:35,679 --> 00:32:37,200
，並將學習作為

926
00:32:37,200 --> 00:32:38,559
變分推理

927
00:32:38,559 --> 00:32:40,240
的過程，theta 的先驗概率

928
00:32:40,240 --> 00:32:41,919
由標準高斯給出，它

929
00:32:41,919 --> 00:32:44,080
在學習過程中就像一個正則化器

930
00:32:44,080 --> 00:32:47,120
，這只是談論自由

931
00:32:47,120 --> 00:32:48,960
能如何 將用於選擇

932
00:32:48,960 --> 00:32:49,519
操作

933
00:32:49,519 --> 00:32:52,240
策略，所以這個攤銷

934
00:32:52,240 --> 00:32:54,640
推理是什麼，為什麼要使用它

935
00:32:54,640 --> 00:32:56,320
底部的這些論文有

936
00:32:56,320 --> 00:32:58,480
更多的信息，但是在攤銷

937
00:32:58,480 --> 00:33:01,039
推理中，使用的參數數量

938
00:33:01,039 --> 00:33:03,039
是靈活的，你可以有一個小的或

939
00:33:03,039 --> 00:33:04,320
大的 模型，

940
00:33:04,320 --> 00:33:07,440
但它的好處是您還可以

941
00:33:07,440 --> 00:33:07,919


942
00:33:07,919 --> 00:33:11,200
將模型的大小設置為

943
00:33:11,200 --> 00:33:13,519
即使輸入更多數據也不會改變，

944
00:33:13,519 --> 00:33:15,519
這樣您就可以在您知道

945
00:33:15,519 --> 00:33:16,080


946
00:33:16,080 --> 00:33:18,799
計算能力有限或

947
00:33:18,799 --> 00:33:20,480
使用非常大但

948
00:33:20,480 --> 00:33:24,320
定義模型大小以在

949
00:33:24,320 --> 00:33:27,760
更大規模的網絡上實現讓我們再說

950
00:33:27,760 --> 00:33:29,120
一遍，模型可以通過

951
00:33:29,120 --> 00:33:30,799
單次前向傳遞而不是更

952
00:33:30,799 --> 00:33:32,000
複雜的遊戲進行訓練，

953
00:33:32,000 --> 00:33:35,440
並且

954
00:33:35,440 --> 00:33:37,279
當你可以設計神經網絡時，還有所有這些自由度，

955
00:33:37,279 --> 00:33:38,559
因為有很多

956
00:33:38,559 --> 00:33:40,320
方法可以用

957
00:33:40,320 --> 00:33:42,799
神經網絡來

958
00:33:42,799 --> 00:33:45,519


959
00:33:45,519 --> 00:33:49,279


960
00:33:49,519 --> 00:33:52,000
實現

961
00:33:52,000 --> 00:33:52,799
描述的

962
00:33:52,799 --> 00:33:55,919
是一個部分觀察到的馬爾可夫決策

963
00:33:55,919 --> 00:33:59,279
過程，所以這些

964
00:33:59,279 --> 00:34:01,399
是定義這種以

965
00:34:01,399 --> 00:34:03,840
有機體為中心的方法的關鍵變量，讓我們

966
00:34:03,840 --> 00:34:05,600
在每個時間步 t 說，這樣人們很

967
00:34:05,600 --> 00:34:07,360
容易記住環境的真實狀態

968
00:34:07,360 --> 00:34:08,480
，即

969
00:34:08,480 --> 00:34:10,480
s 帽子意味著它是

970
00:34:10,480 --> 00:34:11,760
真實的狀態，而

971
00:34:11,760 --> 00:34:13,359
這些狀態是重要的

972
00:34:13,359 --> 00:34:14,879
事情，

973
00:34:14,879 --> 00:34:17,119
就像公共汽車會撞到我還是

974
00:34:17,119 --> 00:34:18,560
不會撞到我可以說，

975
00:34:18,560 --> 00:34:20,320
但它們可能是其他各種東西，

976
00:34:20,320 --> 00:34:21,918
所以我們保持一般性

977
00:34:21,918 --> 00:34:24,800
，然後 s hat sub t 所以真實

978
00:34:24,800 --> 00:34:26,079
狀態通過時間

979
00:34:26,079 --> 00:34:28,239
存在於具有一定維度的狀態空間中，

980
00:34:28,239 --> 00:34:29,760


981
00:34:29,760 --> 00:34:33,280
並且還有與 um this p

982
00:34:33,280 --> 00:34:35,760
s 相關的真實狀態的真實轉換動態

983
00:34:35,760 --> 00:34:37,119


984
00:34:37,119 --> 00:34:40,480
戴著帽子 t 給出

985
00:34:40,480 --> 00:34:42,719
了以前的時間和動作，所以

986
00:34:42,719 --> 00:34:44,079
這就是說

987
00:34:44,079 --> 00:34:46,560
系統的時間演變與

988
00:34:46,560 --> 00:34:48,719
真正的戴著帽子 t 的人

989
00:34:48,719 --> 00:34:50,399
是相關的讓我看看我是否真的

990
00:34:50,399 --> 00:34:53,598
可以得到激光

991
00:34:54,000 --> 00:34:57,440
是的

992
00:34:57,440 --> 00:35:00,560
與當前時間

993
00:35:00,560 --> 00:35:02,800
和上一個時間以及動作有關

994
00:35:02,800 --> 00:35:05,839
，並且也具有維度，

995
00:35:05,839 --> 00:35:07,839
並且代理無法訪問

996
00:35:07,839 --> 00:35:09,359
環境的真實狀態，但

997
00:35:09,359 --> 00:35:11,920
可能會收到觀察結果，所以哦

998
00:35:11,920 --> 00:35:12,800
觀察，

999
00:35:12,800 --> 00:35:14,880
這是一個很好的簡單觀察，並且那些具有

1000
00:35:14,880 --> 00:35:16,240
一定的

1001
00:35:16,240 --> 00:35:17,520
生成到實際

1002
00:35:17,520 --> 00:35:20,160
觀察分佈 t 的維度，這

1003
00:35:20,160 --> 00:35:20,960
與

1004
00:35:20,960 --> 00:35:23,440
觀察如何

1005
00:35:23,440 --> 00:35:24,720
以世界的真實狀態為條件有關，所以如果外面真的很

1006
00:35:24,720 --> 00:35:25,920
陽光，

1007
00:35:25,920 --> 00:35:27,520
那麼

1008
00:35:27,520 --> 00:35:29,440
如果你這樣看

1009
00:35:29,440 --> 00:35:32,800
太陽，你真的會得到光子 是因為代理

1010
00:35:32,800 --> 00:35:34,320
只能從世界中獲得觀察結果，而不是

1011
00:35:34,320 --> 00:35:35,920
絕對真理

1012
00:35:35,920 --> 00:35:37,680
代理必鬚根據他們對狀態的信念進行操作，

1013
00:35:37,680 --> 00:35:38,960
所以

1014
00:35:38,960 --> 00:35:42,079
隨著時間的推移，那些具有一定維度的人沒有帽子

1015
00:35:42,079 --> 00:35:43,440


1016
00:35:43,440 --> 00:35:45,599
關於真實環境狀態的真實性，就像

1017
00:35:45,599 --> 00:35:46,800
戴帽子一樣

1018
00:35:46,800 --> 00:35:49,280
，然後在

1019
00:35:49,280 --> 00:35:51,280
沒有斜體的真實動力學

1020
00:35:51,280 --> 00:35:53,920
和

1021
00:35:53,920 --> 00:35:56,720
總是用斜體的動力學模型之間存在差異，這是

1022
00:35:56,720 --> 00:35:58,560
這些方程如何

1023
00:35:58,560 --> 00:36:00,480
联系在這裡是主動

1024
00:36:00,480 --> 00:36:01,839
推理髮揮作用的地方

1025
00:36:01,839 --> 00:36:04,400
主動推理提出 代理

1026
00:36:04,400 --> 00:36:06,880
實施更新

1027
00:36:06,880 --> 00:36:10,320
他們世界的生成模型，所以斜體

1028
00:36:10,320 --> 00:36:12,640
p 表示觀察世界的模型狀態世界的

1029
00:36:12,640 --> 00:36:14,079


1030
00:36:14,079 --> 00:36:17,200
政策和參數，

1031
00:36:17,200 --> 00:36:18,000


1032
00:36:18,000 --> 00:36:20,000
其中波浪號是隨

1033
00:36:20,000 --> 00:36:21,280
時間變化的變量序列，

1034
00:36:21,280 --> 00:36:23,040
因此這些是隨時間變化的觀察和

1035
00:36:23,040 --> 00:36:25,119
狀態，  pi 是

1036
00:36:25,119 --> 00:36:26,880
策略，如果我有每天跑步的策略，

1037
00:36:26,880 --> 00:36:28,960
那麼我對

1038
00:36:28,960 --> 00:36:30,560
時間的觀察將看起來像這樣

1039
00:36:30,560 --> 00:36:34,240
你可以想像一個和

1040
00:36:34,240 --> 00:36:38,240
帶有斜體的 theta um 屬於更大

1041
00:36:38,240 --> 00:36:40,640
版本的 theta 大寫 theta 並且

1042
00:36:40,640 --> 00:36:42,079
表示參數 生成

1043
00:36:42,079 --> 00:36:43,520
模型本身是隨機

1044
00:36:43,520 --> 00:36:44,560
變量

1045
00:36:44,560 --> 00:36:46,800
，並且代理保持識別

1046
00:36:46,800 --> 00:36:48,560
分佈 這就像 p 和 q，

1047
00:36:48,560 --> 00:36:51,760
所以請注意狀態的 p 和 q，

1048
00:36:51,760 --> 00:36:52,560


1049
00:36:52,560 --> 00:36:55,920
以及世界的政策和 theta

1050
00:36:55,920 --> 00:36:58,400
參數，它們代表

1051
00:36:58,400 --> 00:36:59,359
代理

1052
00:36:59,359 --> 00:37:02,000
對其狀態政策和

1053
00:37:02,000 --> 00:37:04,800
模型參數

1054
00:37:04,960 --> 00:37:08,000
的信念，所以在這一

1055
00:37:08,000 --> 00:37:10,880
部分中，我們將看看 在 p 模型中

1056
00:37:10,880 --> 00:37:13,200
記住它是 ps 和 qs

1057
00:37:13,200 --> 00:37:17,359
所以 p 是一個代理實現

1058
00:37:17,359 --> 00:37:19,359
，這就是為什麼它是

1059
00:37:19,359 --> 00:37:22,640
斜體的觀察模型，通過

1060
00:37:22,640 --> 00:37:23,200
時間

1061
00:37:23,200 --> 00:37:25,280
o 與波浪號狀態通過時間 s

1062
00:37:25,280 --> 00:37:26,320
與波浪號

1063
00:37:26,320 --> 00:37:29,920
策略和模型參數所以這

1064
00:37:29,920 --> 00:37:30,400
就是

1065
00:37:30,400 --> 00:37:33,520
這個 模型是

1066
00:37:33,520 --> 00:37:35,280
關於它的，它是由我們關心的代理實現的，這就是

1067
00:37:35,280 --> 00:37:37,280


1068
00:37:37,280 --> 00:37:39,839
這些行中的每一行所說的，所以等式

1069
00:37:39,839 --> 00:37:40,640


1070
00:37:40,640 --> 00:37:43,040
左側的第一行表示 p，這

1071
00:37:43,040 --> 00:37:43,839
是我

1072
00:37:43,839 --> 00:37:46,400
們在頂部大的函數

1073
00:37:46,400 --> 00:37:47,680


1074
00:37:47,680 --> 00:37:51,440
等於概率

1075
00:37:51,440 --> 00:37:54,640
參數模型

1076
00:37:54,640 --> 00:37:57,760
theta 把這些

1077
00:37:57,760 --> 00:38:00,000
去掉，那麼策略的概率就是，

1078
00:38:00,000 --> 00:38:01,599
如果我不可能

1079
00:38:01,599 --> 00:38:03,440
跳一千英尺，那麼它就

1080
00:38:03,440 --> 00:38:05,680
不會成為我的概率分佈的一部分，

1081
00:38:05,680 --> 00:38:07,359
如果它我 這是我跳一千英尺的能力的一部分，

1082
00:38:07,359 --> 00:38:09,280
然後我會在

1083
00:38:09,280 --> 00:38:09,839
我的

1084
00:38:09,839 --> 00:38:13,280
政策估計中考慮它，然後是大

1085
00:38:13,280 --> 00:38:16,079
pi，這有點像 sigma 如何將

1086
00:38:16,079 --> 00:38:16,400


1087
00:38:16,400 --> 00:38:20,000
東西相加 pi 將東西相乘，所以這就是說

1088
00:38:20,000 --> 00:38:20,400


1089
00:38:20,400 --> 00:38:22,320
隨著時間的推移將它全部

1090
00:38:22,320 --> 00:38:23,680
相乘 從一開始 t

1091
00:38:23,680 --> 00:38:26,960
等於 1 並通過時間觀察

1092
00:38:26,960 --> 00:38:30,320
o 的代理模型 p

1093
00:38:30,320 --> 00:38:31,760
通過時間觀察

1094
00:38:31,760 --> 00:38:33,760
給定的狀態通過時間所以第一

1095
00:38:33,760 --> 00:38:35,359
部分就是

1096
00:38:35,359 --> 00:38:38,720


1097
00:38:38,720 --> 00:38:41,760
假設我很健康我看到這個

1098
00:38:41,760 --> 00:38:44,800
狀態的概率是多少 然後

1099
00:38:44,800 --> 00:38:48,160
在每個時間點乘以

1100
00:38:48,160 --> 00:38:52,320
狀態 s t

1101
00:38:52,320 --> 00:38:55,520
以先前狀態和

1102
00:38:55,520 --> 00:38:56,480
先前策略

1103
00:38:56,480 --> 00:38:59,359
以及模型的參數為條件的方式，因此

1104
00:38:59,359 --> 00:39:00,000
所有這些事物

1105
00:39:00,000 --> 00:39:02,880
如何通過時間相乘乘以

1106
00:39:02,880 --> 00:39:03,440


1107
00:39:03,440 --> 00:39:06,880
第一次通過描述這麼大

1108
00:39:06,880 --> 00:39:08,880
頂部的方程是 os pi theta 的總模型，

1109
00:39:08,880 --> 00:39:10,079


1110
00:39:10,079 --> 00:39:13,280
然後

1111
00:39:13,280 --> 00:39:16,320
通過時間模型深入研究這個 p 的觀察值

1112
00:39:16,320 --> 00:39:17,200


1113
00:39:17,200 --> 00:39:18,960


1114
00:39:18,960 --> 00:39:20,240


1115
00:39:20,240 --> 00:39:22,880


1116
00:39:22,880 --> 00:39:23,359


1117
00:39:23,359 --> 00:39:25,599
給定狀態估計的觀測值 el

1118
00:39:25,599 --> 00:39:26,560


1119
00:39:26,560 --> 00:39:30,240
又是一個正態分佈，

1120
00:39:30,240 --> 00:39:33,200
隨著時間的推移，觀測值具有

1121
00:39:33,200 --> 00:39:34,000
一定的

1122
00:39:34,000 --> 00:39:37,040
均值和方差 mu 和 sigma 具有

1123
00:39:37,040 --> 00:39:39,920
一定的參數 this lambda 和

1124
00:39:39,920 --> 00:39:40,880


1125
00:39:40,880 --> 00:39:44,160
這兩個類似

1126
00:39:44,160 --> 00:39:47,440
的變量的雙重元組一起

1127
00:39:47,440 --> 00:39:51,359
構成了這個函數 lambda

1128
00:39:51,359 --> 00:39:53,599
所以方式 我們將估計

1129
00:39:53,599 --> 00:39:54,640
這些平均

1130
00:39:54,640 --> 00:39:56,960
變體可能是最小二乘也可能不是

1131
00:39:56,960 --> 00:39:57,920
最小二乘 它可能

1132
00:39:57,920 --> 00:40:01,040
不是 l2 範數 我們可以

1133
00:40:01,040 --> 00:40:02,880
在這裡使用函數方法 這是

1134
00:40:02,880 --> 00:40:06,720
對攤銷的洞察力

1135
00:40:07,119 --> 00:40:09,920
狀態概率

1136
00:40:09,920 --> 00:40:10,880


1137
00:40:10,880 --> 00:40:12,800
代理如何的概率轉換模型

1138
00:40:12,800 --> 00:40:14,480
認為隨著時間的

1139
00:40:14,480 --> 00:40:17,520
推移狀態與先前時間的有條件的兩個 um

1140
00:40:17,520 --> 00:40:19,760
狀態和先前時間的政策相關聯

1141
00:40:19,760 --> 00:40:20,880
，然後世界的參數

1142
00:40:20,880 --> 00:40:21,839


1143
00:40:21,839 --> 00:40:24,000


1144
00:40:24,000 --> 00:40:25,280


1145
00:40:25,280 --> 00:40:27,520


1146
00:40:27,520 --> 00:40:29,839
與具有不同

1147
00:40:29,839 --> 00:40:33,200
下標的另一個 mu 和 sigma 的另一個狀態正態分佈相關，對於

1148
00:40:33,200 --> 00:40:36,640
那些 mu 和 sigma，我們

1149
00:40:36,640 --> 00:40:40,240
對 uh 州和政策如何相關的 sub theta 很感興趣，

1150
00:40:40,240 --> 00:40:43,760


1151
00:40:43,760 --> 00:40:47,119
所以這是另一種公正的分類 估計發生

1152
00:40:47,119 --> 00:40:48,000


1153
00:40:48,000 --> 00:40:50,640
的不同類型事件的方差的方法，

1154
00:40:50,640 --> 00:40:51,359


1155
00:40:51,359 --> 00:40:53,280
因此將感官輸入加權

1156
00:40:53,280 --> 00:40:54,400


1157
00:40:54,400 --> 00:40:56,240
在一起 狀態估計

1158
00:40:56,240 --> 00:40:57,680


1159
00:40:57,680 --> 00:41:01,839
分佈再次是由函數

1160
00:41:01,839 --> 00:41:06,480
p 估計的 theta 給出

1161
00:41:06,480 --> 00:41:09,680
為均值為零的正態分佈，

1162
00:41:09,680 --> 00:41:11,200


1163
00:41:11,200 --> 00:41:14,480
我相信這個協方差單位矩陣

1164
00:41:14,480 --> 00:41:17,839
呃，我認為這只是反映

1165
00:41:17,839 --> 00:41:20,720
了一種中性的啟動方法或

1166
00:41:20,720 --> 00:41:21,920
正則化，但

1167
00:41:21,920 --> 00:41:25,920
我對此不是 100 確定

1168
00:41:25,920 --> 00:41:28,400
，這反映了策略的

1169
00:41:28,400 --> 00:41:29,839
概率模型

1170
00:41:29,839 --> 00:41:33,119
與這個

1171
00:41:33,119 --> 00:41:35,520
最大函數相關，所以它是一個 sigma，但它是

1172
00:41:35,520 --> 00:41:36,319
與

1173
00:41:36,319 --> 00:41:39,359
統計模型中的 sigma 平方不同

1174
00:41:39,359 --> 00:41:42,079
，它是我們將在後面部分討論的策略的負預期自由能的軟最大函數，

1175
00:41:42,079 --> 00:41:42,720


1176
00:41:42,720 --> 00:41:46,160


1177
00:41:46,160 --> 00:41:48,960


1178
00:41:48,960 --> 00:41:50,800


1179
00:41:50,800 --> 00:41:54,800
所以這是我們的 p 讓我們看看 q

1180
00:41:54,800 --> 00:41:57,839
q 是這個代理識別

1181
00:41:57,839 --> 00:41:59,040
分佈 q

1182
00:41:59,040 --> 00:42:01,359
通過時間策略和模型參數以斜體表示狀態

1183
00:42:01,359 --> 00:42:03,359


1184
00:42:03,359 --> 00:42:06,720
這是如何定義的，

1185
00:42:06,720 --> 00:42:08,720
所以 q 是

1186
00:42:08,720 --> 00:42:10,240
參數 theta

1187
00:42:10,240 --> 00:42:11,920
multi 的概率 乘以策略 pi 的概率

1188
00:42:11,920 --> 00:42:14,240
，然後在 q

1189
00:42:14,240 --> 00:42:17,359
的所有時間中與時間相乘

1190
00:42:17,359 --> 00:42:20,480
，這是給定觀察的狀態模型，

1191
00:42:20,480 --> 00:42:22,880


1192
00:42:22,880 --> 00:42:26,480
所以以前

1193
00:42:26,640 --> 00:42:30,319
我們觀察到 p-o-s 第二行

1194
00:42:30,319 --> 00:42:30,800
這裡

1195
00:42:30,800 --> 00:42:34,880
是給定狀態的觀察概率，

1196
00:42:38,240 --> 00:42:41,359
現在我們有 um q s o

1197
00:42:41,359 --> 00:42:44,880
所以這個 是另一種方式

1198
00:42:44,880 --> 00:42:48,000
，theta

1199
00:42:48,240 --> 00:42:50,720
中的兩個是正態分佈，就像我們

1200
00:42:50,720 --> 00:42:51,599
之前看到的那樣

1201
00:42:51,599 --> 00:42:54,079
，其均值和方差參數

1202
00:42:54,079 --> 00:42:56,319
與

1203
00:42:56,319 --> 00:42:59,520
策略相同，並且與給定觀察值的狀態的 q 模型相同

1204
00:42:59,520 --> 00:43:01,280


1205
00:43:01,280 --> 00:43:04,000


1206
00:43:04,400 --> 00:43:08,720
，結果證明那些 mu 和

1207
00:43:08,720 --> 00:43:10,960
sigma 也將有一個函數

1208
00:43:10,960 --> 00:43:14,160
來近似它們，

1209
00:43:14,720 --> 00:43:16,960
以便使主動推理適用

1210
00:43:16,960 --> 00:43:18,240
於強化學習中考慮的任務類型，

1211
00:43:18,240 --> 00:43:19,839


1212
00:43:19,839 --> 00:43:22,400
作者將獎勵信號

1213
00:43:22,400 --> 00:43:23,200


1214
00:43:23,200 --> 00:43:25,200
觀察視為單獨模態中的觀察，

1215
00:43:25,200 --> 00:43:27,520
以便將生成模型擴展到

1216
00:43:27,520 --> 00:43:28,160
包括

1217
00:43:28,160 --> 00:43:31,200
一個額外的標量高斯，這是標量的

1218
00:43:31,200 --> 00:43:32,800
另一個

1219
00:43:32,800 --> 00:43:34,800
獎勵值，例如

1220
00:43:34,800 --> 00:43:36,960
獎勵觀察的單個數字 s

1221
00:43:36,960 --> 00:43:38,800
在均值上有一個單位方差

1222
00:43:38,800 --> 00:43:41,119
，這允許他們包裝這個神經

1223
00:43:41,119 --> 00:43:42,400
網絡

1224
00:43:42,400 --> 00:43:45,119
，它是狀態的 uh f sub alpha，

1225
00:43:45,119 --> 00:43:46,480
就像這個獎勵

1226
00:43:46,480 --> 00:43:47,920
完全連接的神經網絡與

1227
00:43:47,920 --> 00:43:49,119
他們將要訓練的參數，

1228
00:43:49,119 --> 00:43:50,079


1229
00:43:50,079 --> 00:43:52,160
所以這真的是 形式

1230
00:43:52,160 --> 00:43:54,640
同源性源於基於模型的

1231
00:43:54,640 --> 00:43:56,079
強化

1232
00:43:56,079 --> 00:43:58,720
學習，現在我們擁有與本討論

1233
00:43:58,720 --> 00:44:01,440
前面指定的幾乎相同的架構，

1234
00:44:01,440 --> 00:44:05,599
其中代理狀態

1235
00:44:05,599 --> 00:44:09,040
是感官輸入，然後通過此 dnn 運行，

1236
00:44:09,040 --> 00:44:10,000


1237
00:44:10,000 --> 00:44:12,319
並導致

1238
00:44:12,319 --> 00:44:13,599
策略

1239
00:44:13,599 --> 00:44:16,560
或 獎勵，但它有點

1240
00:44:16,560 --> 00:44:17,839
不同，但這就是

1241
00:44:17,839 --> 00:44:20,960
我們如何看到許多相同的同調，

1242
00:44:20,960 --> 00:44:24,319
所以

1243
00:44:24,319 --> 00:44:27,040
當新的觀察被採樣時，學習者會做什麼

1244
00:44:27,040 --> 00:44:27,839


1245
00:44:27,839 --> 00:44:29,520
代理更新識別分佈的參數

1246
00:44:29,520 --> 00:44:31,200


1247
00:44:31,200 --> 00:44:34,480
以最小化變分自由能或 f

1248
00:44:34,480 --> 00:44:38,160
所以 f 觀察

1249
00:44:38,160 --> 00:44:42,319
是期望 嗯好吧好吧

1250
00:44:42,319 --> 00:44:44,480
讓我先讀完這使得

1251
00:44:44,480 --> 00:44:45,440


1252
00:44:45,440 --> 00:44:47,520
這個 f 要做的是讓

1253
00:44:47,520 --> 00:44:48,800
識別分佈 函數

1254
00:44:48,800 --> 00:44:51,440
q 收斂於

1255
00:44:51,440 --> 00:44:53,920
難以處理的後驗分佈

1256
00:44:53,920 --> 00:44:56,720
p 的近似值，從而實現了一種易於處理

1257
00:44:56,720 --> 00:44:58,800
的近似貝葉斯推理

1258
00:44:58,800 --> 00:45:02,079
形式，所以 f 的

1259
00:45:02,079 --> 00:45:04,240
o 隨時間與波浪號是隨時間

1260
00:45:04,240 --> 00:45:05,119
變化

1261
00:45:05,119 --> 00:45:07,040
的觀察自由能，

1262
00:45:07,040 --> 00:45:08,319


1263
00:45:08,319 --> 00:45:12,000
所以這種神奇的自由能是什麼？

1264
00:45:12,000 --> 00:45:13,920
在

1265
00:45:13,920 --> 00:45:15,200
這種情況下，

1266
00:45:15,200 --> 00:45:18,000
它只是這個方程，它是

1267
00:45:18,000 --> 00:45:19,839
一個以特定識別模型為條件的

1268
00:45:19,839 --> 00:45:22,720
期望，這是對 q 的期望，這

1269
00:45:22,720 --> 00:45:23,280
將

1270
00:45:23,280 --> 00:45:26,960
與這個 um s

1271
00:45:26,960 --> 00:45:30,160
pi 策略和 theta 參數

1272
00:45:30,160 --> 00:45:34,560
uh 變量有關，這個期望

1273
00:45:34,560 --> 00:45:35,760
是什麼，

1274
00:45:35,760 --> 00:45:37,520
這就是一切 這將在

1275
00:45:37,520 --> 00:45:38,960
方括號中

1276
00:45:38,960 --> 00:45:41,040
這個期望將是

1277
00:45:41,040 --> 00:45:43,119
自然對數

1278
00:45:43,119 --> 00:45:46,319
ln 這是一種將

1279
00:45:46,319 --> 00:45:48,640
關於最大化模型擬合優度的問題

1280
00:45:48,640 --> 00:45:50,000


1281
00:45:50,000 --> 00:45:52,319
轉化為最小化負數的方法

1282
00:45:52,319 --> 00:45:53,440
有時

1283
00:45:53,440 --> 00:45:55,920
自然法則可以提供一些幫助

1284
00:45:55,920 --> 00:45:56,720


1285
00:45:56,720 --> 00:45:58,880
期望將是

1286
00:45:58,880 --> 00:46:00,880
識別模型

1287
00:46:00,880 --> 00:46:03,599
的自然對數減去基因的自然對數 rative

1288
00:46:03,599 --> 00:46:04,240


1289
00:46:04,240 --> 00:46:06,720
模型同樣是我們

1290
00:46:06,720 --> 00:46:07,760
提示我們可以學習

1291
00:46:07,760 --> 00:46:09,280
的部分，然後是被

1292
00:46:09,280 --> 00:46:10,880
描述為難以處理的部分 p

1293
00:46:10,880 --> 00:46:13,359
結果證明這個期望

1294
00:46:13,359 --> 00:46:15,440
總是大於或等於

1295
00:46:15,440 --> 00:46:18,720
觀察的 p 的自然對數，

1296
00:46:18,720 --> 00:46:21,599
所以這就是 p

1297
00:46:21,599 --> 00:46:24,000
觀察似然模型

1298
00:46:24,000 --> 00:46:26,079
通過隨著時間的推移最小化觀察的自由能

1299
00:46:26,079 --> 00:46:27,760


1300
00:46:27,760 --> 00:46:29,839
代理啟發式

1301
00:46:29,839 --> 00:46:31,760
地收斂於這個棘手的 p

1302
00:46:31,760 --> 00:46:34,960
分佈，因為

1303
00:46:34,960 --> 00:46:38,079
q 僅超過國家的

1304
00:46:38,079 --> 00:46:40,960
政策和參數，並且 p

1305
00:46:40,960 --> 00:46:42,640
確實包括並關注

1306
00:46:42,640 --> 00:46:44,079
觀察結果

1307
00:46:44,079 --> 00:46:46,640
這允許 代理要做的

1308
00:46:46,640 --> 00:46:48,319
是專注於通過隊列對相關

1309
00:46:48,319 --> 00:46:50,079
的狀態、策略和

1310
00:46:50,079 --> 00:46:52,319
參數進行建模

1311
00:46:52,319 --> 00:46:55,520
，然後可能

1312
00:46:55,520 --> 00:46:58,960
在該模型中使用數學斜線非活動行為來

1313
00:46:58,960 --> 00:47:01,119
有吸引力地包含條件或

1314
00:47:01,119 --> 00:47:02,800
從其觀察中學習，

1315
00:47:02,800 --> 00:47:04,800
這樣我們就可以帶來以前的知識

1316
00:47:04,800 --> 00:47:06,400
就像貝葉斯模型

1317
00:47:06,400 --> 00:47:07,359
允許的一樣，

1318
00:47:07,359 --> 00:47:10,240
但也有辦法處理稀疏

1319
00:47:10,240 --> 00:47:11,359
或密集 信息，

1320
00:47:11,359 --> 00:47:14,240
因為它是

1321
00:47:14,480 --> 00:47:18,000
好的，關鍵的主動推理還

1322
00:47:18,000 --> 00:47:19,599
提出，代理的目標和

1323
00:47:19,599 --> 00:47:21,440
慾望在生成

1324
00:47:21,440 --> 00:47:23,520
模型中被編碼為有利觀察的先驗偏好，

1325
00:47:23,520 --> 00:47:24,640


1326
00:47:24,640 --> 00:47:28,000
即 37 攝氏度自由能下的血液溫度，

1327
00:47:28,000 --> 00:47:29,839
然後提供了一個代理，

1328
00:47:29,839 --> 00:47:31,920
說明一些觀察是多麼令人驚訝，即不太可能

1329
00:47:31,920 --> 00:47:34,160
在 代理模型

1330
00:47:34,160 --> 00:47:36,319
在最小化方程 1 的同時提供了一個

1331
00:47:36,319 --> 00:47:38,000
估計，說明我們的一些觀察結果是多麼令人驚訝，

1332
00:47:38,000 --> 00:47:39,760
它不能

1333
00:47:39,760 --> 00:47:41,040
直接減少數量，

1334
00:47:41,040 --> 00:47:44,240
所以在這裡方程 1 我們定義了 f

1335
00:47:44,240 --> 00:47:46,720
是什麼，但它只是 f 是什麼的定義和

1336
00:47:46,720 --> 00:47:47,839
界限

1337
00:47:47,839 --> 00:47:51,359
它不能直接減少這個

1338
00:47:51,359 --> 00:47:53,280
數量 所以它與政策沒有真正的

1339
00:47:53,280 --> 00:47:54,960
關係，

1340
00:47:54,960 --> 00:47:58,720
因為

1341
00:47:58,720 --> 00:48:01,200
實現這一目標的直接方法必須通過行動來改變他們的

1342
00:48:01,200 --> 00:48:03,280
觀察，

1343
00:48:03,280 --> 00:48:05,280
以最小化變分自由

1344
00:48:05,280 --> 00:48:07,040
能，確保最小

1345
00:48:07,040 --> 00:48:07,599


1346
00:48:07,599 --> 00:48:10,160


1347
00:48:10,160 --> 00:48:12,880
化觀察概率模型的意外負自然對數

1348
00:48:12,880 --> 00:48:14,720
或最大化貝葉斯

1349
00:48:14,720 --> 00:48:16,240
模型證據

1350
00:48:16,240 --> 00:48:19,440
p 通過時間的觀察 由於

1351
00:48:19,440 --> 00:48:20,079
自由能

1352
00:48:20,079 --> 00:48:21,920
提供了意外的上限，

1353
00:48:21,920 --> 00:48:23,599
即等式一個

1354
00:48:23,599 --> 00:48:26,160
主動推理因此建議

1355
00:48:26,160 --> 00:48:27,680
代理選擇策略

1356
00:48:27,680 --> 00:48:29,520
以最小化預期的

1357
00:48:29,520 --> 00:48:30,960


1358
00:48:30,960 --> 00:48:34,000
自由能幻想 g 其中

1359
00:48:34,000 --> 00:48:36,160
給定策略 pi 在未來某個

1360
00:48:36,160 --> 00:48:38,240
時間

1361
00:48:38,240 --> 00:48:41,760
的預期自由能將被定義為 所以

1362
00:48:41,760 --> 00:48:44,880
我們想做的不僅僅是更新

1363
00:48:44,880 --> 00:48:47,839
我們的內部模型來限制我們

1364
00:48:47,839 --> 00:48:49,359
對觀察的驚訝我們不只是想

1365
00:48:49,359 --> 00:48:51,119
適應世界的統計描述模型

1366
00:48:51,119 --> 00:48:52,160


1367
00:48:52,160 --> 00:48:53,920
我們想要減少未來的不確定性，

1368
00:48:53,920 --> 00:48:55,920
這是一個未來的時間點 t

1369
00:48:55,920 --> 00:48:59,440
tau 通過我們現在採用的策略 pi

1370
00:48:59,440 --> 00:49:00,960


1371
00:49:00,960 --> 00:49:02,960
這個可最小化的預期自由能

1372
00:49:02,960 --> 00:49:04,880
函數 fancy

1373
00:49:04,880 --> 00:49:07,599
g 將接受

1374
00:49:07,599 --> 00:49:08,800
策略選擇

1375
00:49:08,800 --> 00:49:11,680
和未來時間步長的參數所以 fancy g

1376
00:49:11,680 --> 00:49:12,640
fancy pi

1377
00:49:12,640 --> 00:49:15,760
fancy t 這就是這種情況下的自由能函數

1378
00:49:15,760 --> 00:49:17,920


1379
00:49:17,920 --> 00:49:21,680
和

1380
00:49:21,680 --> 00:49:24,319
優化將

1381
00:49:24,319 --> 00:49:25,599
允許有效行動的

1382
00:49:25,599 --> 00:49:27,599
值將被定義為

1383
00:49:27,599 --> 00:49:28,720


1384
00:49:28,720 --> 00:49:31,760
與 q 模型相關的期望值

1385
00:49:31,760 --> 00:49:34,720
這是關於

1386
00:49:34,720 --> 00:49:37,119
給定政策模型的觀察狀態和參數，所以

1387
00:49:37,119 --> 00:49:39,359
鑑於我的政策是這樣的，

1388
00:49:39,359 --> 00:49:42,720
那麼世界的狀態和轉變將如何

1389
00:49:42,720 --> 00:49:43,599


1390
00:49:43,599 --> 00:49:46,960
出現，這將

1391
00:49:46,960 --> 00:49:50,400
是括號中這個數量的期望，

1392
00:49:50,400 --> 00:49:51,280


1393
00:49:51,280 --> 00:49:53,760
它將成為狀態的對數 和

1394
00:49:53,760 --> 00:49:54,640
參數，

1395
00:49:54,640 --> 00:49:58,079
所以這是 s sub t 的 q

1396
00:49:58,079 --> 00:50:01,359
和給定策略的參數，

1397
00:50:01,359 --> 00:50:04,480
所以這就是我將處於什麼狀態以及

1398
00:50:04,480 --> 00:50:06,960
我會怎麼想

1399
00:50:06,960 --> 00:50:10,000
嗯，可能這些只是

1400
00:50:10,000 --> 00:50:10,400


1401
00:50:10,400 --> 00:50:12,960
首先通過口頭解釋，

1402
00:50:12,960 --> 00:50:15,119
只看數學，但

1403
00:50:15,119 --> 00:50:18,810
什麼狀態

1404
00:50:18,810 --> 00:50:20,000


1405
00:50:20,000 --> 00:50:23,359
考慮到我的政策，我現在在[音樂]中的參數，所以如果我

1406
00:50:23,359 --> 00:50:25,599
每天出去散步，它將如何

1407
00:50:25,599 --> 00:50:27,119
影響我的

1408
00:50:27,119 --> 00:50:29,200
狀態我的模型當時必須是什麼

1409
00:50:29,200 --> 00:50:30,319


1410
00:50:30,319 --> 00:50:32,880
，然後那個數量減去自然對

1411
00:50:32,880 --> 00:50:34,000
數 p

1412
00:50:34,000 --> 00:50:36,400
模型中給定策略的觀察狀態，

1413
00:50:36,400 --> 00:50:38,000
所以我們

1414
00:50:38,000 --> 00:50:40,000
想要的部分實際上是觀察

1415
00:50:40,000 --> 00:50:43,520
值是正確的值，但我們能做的

1416
00:50:43,520 --> 00:50:46,480
就是嗯，只是根據

1417
00:50:46,480 --> 00:50:48,960
策略

1418
00:50:50,160 --> 00:50:53,760
和 g 值來調節狀態 總是大於

1419
00:50:53,760 --> 00:50:54,240


1420
00:50:54,240 --> 00:50:55,760
負數，即等式二的最後一部分，

1421
00:50:55,760 --> 00:50:57,599
然後

1422
00:50:57,599 --> 00:50:59,040


1423
00:50:59,040 --> 00:51:01,599
是這個 q 的負期望，這是以政策為條件的觀察結果

1424
00:51:01,599 --> 00:51:03,599
，這

1425
00:51:03,599 --> 00:51:08,319
就像事情實際上會變成

1426
00:51:08,319 --> 00:51:10,880
生成模型的這個對數部分的 um，

1427
00:51:10,880 --> 00:51:11,920
即

1428
00:51:11,920 --> 00:51:14,319
給出的觀察結果 政策

1429
00:51:14,319 --> 00:51:15,760
就是事情的發展方向，

1430
00:51:15,760 --> 00:51:18,160
所以也許還有其他解釋

1431
00:51:18,160 --> 00:51:18,800
或其他

1432
00:51:18,800 --> 00:51:20,240
部分非常關鍵，但我

1433
00:51:20,240 --> 00:51:22,000
認為這是關鍵的

1434
00:51:22,000 --> 00:51:25,119
表述之一，即存在一個

1435
00:51:25,119 --> 00:51:29,119
與政策相關的優化問題

1436
00:51:29,119 --> 00:51:32,240
，可以用變分的方式

1437
00:51:32,240 --> 00:51:35,520
表達 受到

1438
00:51:35,520 --> 00:51:36,960
類似自由能的

1439
00:51:36,960 --> 00:51:40,000
策略的限制，可以

1440
00:51:40,000 --> 00:51:43,440
輕鬆地估計一些

1441
00:51:43,440 --> 00:51:46,720
其他函數 q 與基本上我們想知道的相關的東西

1442
00:51:46,720 --> 00:51:47,760
，

1443
00:51:47,760 --> 00:51:50,319


1444
00:51:50,319 --> 00:51:51,440
這就像

1445
00:51:51,440 --> 00:51:54,559
好的觀察，我查看我的

1446
00:51:54,559 --> 00:51:58,160
比特幣錢包並且有 15 個比特幣

1447
00:51:58,160 --> 00:52:01,200
我有什麼政策 如果只有一個人知道，就可以

1448
00:52:01,200 --> 00:52:03,680
看到這種觀察或反

1449
00:52:03,680 --> 00:52:06,160
事實存在

1450
00:52:06,160 --> 00:52:08,880
，這就是為什麼會有

1451
00:52:08,880 --> 00:52:09,839
這種易處理的

1452
00:52:09,839 --> 00:52:12,800
有界方程可能我我

1453
00:52:12,800 --> 00:52:13,839
認為

1454
00:52:13,839 --> 00:52:16,960
這裡有很多話要說，亞歷克和其他任何人我都會很

1455
00:52:16,960 --> 00:52:18,000
感激任何其他

1456
00:52:18,000 --> 00:52:20,800
輸入，但這只是一個閱讀它的

1457
00:52:20,800 --> 00:52:21,680
好，

1458
00:52:21,680 --> 00:52:23,920
所以這是我

1459
00:52:23,920 --> 00:52:25,440
想在那個深度水平上經歷的大部分部分

1460
00:52:25,440 --> 00:52:27,280
但隨後只是

1461
00:52:27,280 --> 00:52:29,680
傳達其他部分以及他們為論文所做的工作

1462
00:52:29,680 --> 00:52:30,400
，

1463
00:52:30,400 --> 00:52:32,160
而不是深入每個方程

1464
00:52:32,160 --> 00:52:33,599
，然後討論數字，

1465
00:52:33,599 --> 00:52:36,960
所以 3.2 是學習和推理

1466
00:52:36,960 --> 00:52:37,760
部分

1467
00:52:37,760 --> 00:52:41,119
，為了實際實施

1468
00:52:41,119 --> 00:52:43,520
這個優化方案，因為它被佈置

1469
00:52:43,520 --> 00:52:44,160


1470
00:52:44,160 --> 00:52:46,079
事實證明，必須有一個更新

1471
00:52:46,079 --> 00:52:47,599
過程，

1472
00:52:47,599 --> 00:52:49,760
所以它會是一種呃堅持

1473
00:52:49,760 --> 00:52:51,359
已經奏效的東西，還是會

1474
00:52:51,359 --> 00:52:51,760
成為

1475
00:52:51,760 --> 00:52:53,760
新奇搜索那些

1476
00:52:53,760 --> 00:52:54,880
是優化

1477
00:52:54,880 --> 00:52:56,720
算法必須導航

1478
00:52:56,720 --> 00:52:59,040
權衡的東西 隱含的 uh

1479
00:52:59,040 --> 00:53:00,880
根據不同的問題採用不同的策略

1480
00:53:00,880 --> 00:53:02,079


1481
00:53:02,079 --> 00:53:05,599
，因此本節定義瞭如何隨時間定義這個

1482
00:53:05,599 --> 00:53:09,280
f uh 變分自由能

1483
00:53:09,280 --> 00:53:11,119


1484
00:53:11,119 --> 00:53:12,720
，我不打算介紹

1485
00:53:12,720 --> 00:53:14,559
實現細節 b  ut alex

1486
00:53:14,559 --> 00:53:17,520
alec 如果你知道

1487
00:53:17,520 --> 00:53:19,119
向我們展示了一個模擬或類似的東西會很酷

1488
00:53:19,119 --> 00:53:20,880
，

1489
00:53:20,880 --> 00:53:23,920
呃 3.3 是關於策略選擇的，

1490
00:53:23,920 --> 00:53:25,280
這是他們

1491
00:53:25,280 --> 00:53:27,440
在主動推理下寫的內容 策略選擇

1492
00:53:27,440 --> 00:53:29,040
是通過更新策略的 q 來實現的，

1493
00:53:29,040 --> 00:53:31,040
以最大限度地減少

1494
00:53:31,040 --> 00:53:33,200
花式 f 的自由能

1495
00:53:33,200 --> 00:53:35,440
考慮到政策

1496
00:53:35,440 --> 00:53:37,280
最小化預期自由能的先驗信念，

1497
00:53:37,280 --> 00:53:41,040
即政策的 p 與

1498
00:53:41,040 --> 00:53:44,240
此選擇相關 soft max 與

1499
00:53:44,240 --> 00:53:47,920
um 負 g 負 uh

1500
00:53:47,920 --> 00:53:51,280
政策空間的預期自由能

1501
00:53:51,280 --> 00:53:53,359
與

1502
00:53:53,359 --> 00:53:55,920
等式 3 中指定的短自由

1503
00:53:55,920 --> 00:53:57,359
能最小化

1504
00:53:57,359 --> 00:54:02,079
時 uh  q 策略的分佈

1505
00:54:02,079 --> 00:54:05,599
與最優自由能相同

1506
00:54:05,599 --> 00:54:08,160


1507
00:54:08,160 --> 00:54:10,000


1508
00:54:10,000 --> 00:54:13,200


1509
00:54:13,200 --> 00:54:14,800


1510
00:54:14,800 --> 00:54:16,240


1511
00:54:16,240 --> 00:54:19,200


1512
00:54:19,200 --> 00:54:20,720


1513
00:54:20,720 --> 00:54:22,720
所以這就像連接四個

1514
00:54:22,720 --> 00:54:25,520
你可以用完或井字遊戲

1515
00:54:25,520 --> 00:54:26,160
你可以用完

1516
00:54:26,160 --> 00:54:27,839
每一個選項 在

1517
00:54:27,839 --> 00:54:29,599
該分支點的遊戲

1518
00:54:29,599 --> 00:54:31,680
計算每個可能結果的絕對值

1519
00:54:31,680 --> 00:54:33,520


1520
00:54:33,520 --> 00:54:35,920
，然後做出選擇，但是在

1521
00:54:35,920 --> 00:54:37,760
連續動作空間中存在

1522
00:54:37,760 --> 00:54:39,839
無限策略，這意味著需要一種替代

1523
00:54:39,839 --> 00:54:41,040
方法

1524
00:54:41,040 --> 00:54:42,960
，這是一個非常酷的句子，因為它

1525
00:54:42,960 --> 00:54:45,200
確實很好

1526
00:54:45,200 --> 00:54:47,599
地傳達了實際上連續動作

1527
00:54:47,599 --> 00:54:48,240


1528
00:54:48,240 --> 00:54:50,720
即使對於單個變量來說，空間也是一個真正

1529
00:54:50,720 --> 00:54:51,680
不同的領域，而

1530
00:54:51,680 --> 00:54:54,400
不是讓我們說囚徒困境

1531
00:54:54,400 --> 00:54:55,119


1532
00:54:55,119 --> 00:54:58,160
游戲有時，呃

1533
00:54:58,160 --> 00:55:00,160
，算法中幾乎沒有重疊，這就是為什麼本文

1534
00:55:00,160 --> 00:55:02,880
實際上提前反映了這一點，並且

1535
00:55:02,880 --> 00:55:06,079
在第 3.4 節軌跡採樣中有更多細節，

1536
00:55:06,079 --> 00:55:09,200
所以現在如何

1537
00:55:09,200 --> 00:55:10,559
你是從

1538
00:55:10,559 --> 00:55:12,400


1539
00:55:12,400 --> 00:55:15,200
政策和其他變量之間的

1540
00:55:15,200 --> 00:55:16,720


1541
00:55:16,720 --> 00:55:17,920


1542
00:55:17,920 --> 00:55:21,119


1543
00:55:21,119 --> 00:55:21,520


1544
00:55:21,520 --> 00:55:23,520


1545
00:55:23,520 --> 00:55:25,599
分佈

1546
00:55:25,599 --> 00:55:27,839
關係開始的嗎

1547
00:55:27,839 --> 00:55:29,680
對於任何具體的政策，他們

1548
00:55:29,680 --> 00:55:31,599
必須首先評估 預期的

1549
00:55:31,599 --> 00:55:33,680
未來信念取決於該政策，

1550
00:55:33,680 --> 00:55:37,440
所以呃，

1551
00:55:37,440 --> 00:55:40,559
在這種規範框架中，

1552
00:55:40,559 --> 00:55:42,480
你不能真正承擔，這是一個次要辯論，我們

1553
00:55:42,480 --> 00:55:44,480
肯定可以討論人類在多大程度上

1554
00:55:44,480 --> 00:55:45,119


1555
00:55:45,119 --> 00:55:46,880
認知這些經驗，但

1556
00:55:46,880 --> 00:55:48,160
我們只是要

1557
00:55:48,160 --> 00:55:50,160
有點用故意的立場

1558
00:55:50,160 --> 00:55:51,760
和機器學習控制理論採取的那種，

1559
00:55:51,760 --> 00:55:52,160


1560
00:55:52,160 --> 00:55:55,359
但因此相信它

1561
00:55:55,359 --> 00:55:57,200
不一定是認知的，

1562
00:55:57,200 --> 00:55:58,400
我們只是在考慮代理

1563
00:55:58,400 --> 00:56:00,319
想要做什麼來成功，

1564
00:56:00,319 --> 00:56:02,240
但基本上如果代理沒有

1565
00:56:02,240 --> 00:56:03,920


1566
00:56:03,920 --> 00:56:06,400
該政策的預期未來信念序列，

1567
00:56:06,400 --> 00:56:08,160
他們將非常難以

1568
00:56:08,160 --> 00:56:09,599
承擔它，

1569
00:56:09,599 --> 00:56:12,559
並且轉換事實是過渡

1570
00:56:12,559 --> 00:56:13,359


1571
00:56:13,359 --> 00:56:14,880
模型是概率性的

1572
00:56:14,880 --> 00:56:16,480
，並且過渡模型或

1573
00:56:16,480 --> 00:56:17,599
隨機變量的參數會

1574
00:56:17,599 --> 00:56:19,440
導致未來軌跡上的分佈，

1575
00:56:19,440 --> 00:56:21,040


1576
00:56:21,040 --> 00:56:23,520
而不是僅僅玩 我們正在考慮的每一個可能的

1577
00:56:23,520 --> 00:56:24,640
國際象棋走法，

1578
00:56:24,640 --> 00:56:27,280
我們如何擬合

1579
00:56:27,280 --> 00:56:28,559


1580
00:56:28,559 --> 00:56:31,200
一種更高級的模型以及國際象棋

1581
00:56:31,200 --> 00:56:32,480
仍然是離散的，所以

1582
00:56:32,480 --> 00:56:35,040
l  et's choose 我們將看到一個具體的

1583
00:56:35,040 --> 00:56:35,760
例子，

1584
00:56:35,760 --> 00:56:38,319
但這只是

1585
00:56:38,319 --> 00:56:38,880


1586
00:56:38,880 --> 00:56:42,799
這個數字後面的連續優化，但這就像

1587
00:56:42,799 --> 00:56:45,680


1588
00:56:45,680 --> 00:56:46,880
國際象棋遊戲中未來分支點的分佈，

1589
00:56:46,880 --> 00:56:49,280
所以無論它是離散的還是連續的，

1590
00:56:49,280 --> 00:56:50,880
你仍然可以有這個

1591
00:56:50,880 --> 00:56:51,839
不

1592
00:56:51,839 --> 00:56:54,559
確定性元素 狀態空間中未來軌蹟的分佈範圍，

1593
00:56:54,559 --> 00:56:55,440


1594
00:56:55,440 --> 00:56:59,040
因此存在幾種方法

1595
00:56:59,040 --> 00:57:01,119
來近似

1596
00:57:01,119 --> 00:57:02,559
不確定軌蹟

1597
00:57:02,559 --> 00:57:04,480
的傳播，例如，一種可以完全忽略不確定性

1598
00:57:04,480 --> 00:57:06,160
並傳播分佈的均值，

1599
00:57:06,160 --> 00:57:07,280


1600
00:57:07,280 --> 00:57:09,440
或者一種可以顯式傳播分佈的完整

1601
00:57:09,440 --> 00:57:11,440
統計數據，

1602
00:57:11,440 --> 00:57:13,680
因此這些是一種 兩種極端

1603
00:57:13,680 --> 00:57:15,839
方法其中之一就是說是的，

1604
00:57:15,839 --> 00:57:17,680
集成建模正在跟踪平均值

1605
00:57:17,680 --> 00:57:19,200
，所以我將繼續前進

1606
00:57:19,200 --> 00:57:19,920


1607
00:57:19,920 --> 00:57:22,160
，然後另一種方法基本上是

1608
00:57:22,160 --> 00:57:23,359
保持

1609
00:57:23,359 --> 00:57:25,359
對整個分佈的總結，並將其

1610
00:57:25,359 --> 00:57:26,880
用作正在學習的內容

1611
00:57:26,880 --> 00:57:29,520


1612
00:57:29,520 --> 00:57:32,400
在當前的工作中，它使用了一種

1613
00:57:32,400 --> 00:57:33,359
粒子方法

1614
00:57:33,359 --> 00:57:35,839
，即蒙特卡洛 基於採樣的

1615
00:57:35,839 --> 00:57:36,960
方案

1616
00:57:36,960 --> 00:57:39,680
樣本被特別傳播，我們

1617
00:57:39,680 --> 00:57:41,119
考慮

1618
00:57:41,119 --> 00:57:43,680


1619
00:57:43,680 --> 00:57:44,480


1620
00:57:44,480 --> 00:57:48,799
從立方體數據中提取的參數分佈數據中的這個大 b 樣本，

1621
00:57:49,280 --> 00:57:52,960
這些部分傳達瞭如何

1622
00:57:52,960 --> 00:57:54,480


1623
00:57:54,480 --> 00:57:56,480
在模型中完成策略選擇的信息，以及如何

1624
00:57:56,480 --> 00:57:58,400
探索策略適應度環境以

1625
00:57:58,400 --> 00:57:59,040


1626
00:57:59,040 --> 00:58:00,319
實現 對軌蹟的預測控制，

1627
00:58:00,319 --> 00:58:02,240
因此

1628
00:58:02,240 --> 00:58:05,440
通過時間實施的政策必須

1629
00:58:05,440 --> 00:58:06,720
涉及一些

1630
00:58:06,720 --> 00:58:10,480
預測控制元素，好吧，

1631
00:58:10,480 --> 00:58:14,079
然後是第 3.5 節 3.6 節，

1632
00:58:14,079 --> 00:58:17,200
所以第 3.5 節有計算這個

1633
00:58:17,200 --> 00:58:18,960
預期自由能

1634
00:58:18,960 --> 00:58:22,480
值的詳細信息，正如標題可能

1635
00:58:22,480 --> 00:58:24,480
在本節中暗示的那樣，他們描述瞭如何

1636
00:58:24,480 --> 00:58:25,599
評估 負 g

1637
00:58:25,599 --> 00:58:29,520
表示他們使用這種

1638
00:58:29,520 --> 00:58:31,760
符號方便來談論

1639
00:58:31,760 --> 00:58:33,200
向前發展的政策的政策，

1640
00:58:33,200 --> 00:58:36,720
所以就像一個裸餡餅，

1641
00:58:36,720 --> 00:58:40,240


1642
00:58:40,240 --> 00:58:43,440
而不是與時間狀態 um 和

1643
00:58:43,440 --> 00:58:44,559
不確定性

1644
00:58:44,559 --> 00:58:47,599
以及負預期自由能有關的上標

1645
00:58:47,599 --> 00:58:48,400
這就是

1646
00:58:48,400 --> 00:58:49,839
我們試圖在這裡得到的

1647
00:58:49,839 --> 00:58:51,760
負g

1648
00:58:51,760 --> 00:58:54,880
是e 等於隨著時間的推移這種負

1649
00:58:54,880 --> 00:58:56,480
預期自由能

1650
00:58:56,480 --> 00:58:59,520
的總和，然後

1651
00:58:59,520 --> 00:59:03,599
這就是分解，

1652
00:59:03,599 --> 00:59:06,720
不是我的領域，但我

1653
00:59:06,720 --> 00:59:09,440
認為這意味著

1654
00:59:09,440 --> 00:59:10,480
分解

1655
00:59:10,480 --> 00:59:13,760
成這個狀態信息博弈

1656
00:59:13,760 --> 00:59:16,640
增益和外在價值分量，所以

1657
00:59:16,640 --> 00:59:18,240
這種分解為探索

1658
00:59:18,240 --> 00:59:19,200
和利用

1659
00:59:19,200 --> 00:59:21,920
組件或其他一些

1660
00:59:21,920 --> 00:59:22,960


1661
00:59:22,960 --> 00:59:25,520


1662
00:59:25,520 --> 00:59:26,799


1663
00:59:26,799 --> 00:59:29,040


1664
00:59:29,040 --> 00:59:30,480


1665
00:59:30,480 --> 00:59:34,079
折衷方案 呃似乎表現為

1666
00:59:34,079 --> 00:59:36,480
模型精度受到模型

1667
00:59:36,480 --> 00:59:37,359
複雜性的懲罰，或者

1668
00:59:37,359 --> 00:59:40,640
這個加上這總是

1669
00:59:40,640 --> 00:59:42,640
等式中的兩個部分是不是只有

1670
00:59:42,640 --> 00:59:44,960
一個項可以是三個項

1671
00:59:44,960 --> 00:59:48,000
在這裡它是三個項但是嗯

1672
00:59:48,000 --> 00:59:52,000
或四個所以我不完全是 確定

1673
00:59:52,000 --> 00:59:55,280
為什麼有時它有或多或少的術語，

1674
00:59:55,280 --> 00:59:58,319
但這就是我很好奇的原因，

1675
00:59:58,319 --> 01:00:01,119
然後在 3.6 中，他們編寫

1676
01:00:01,119 --> 01:00:01,760


1677
01:00:01,760 --> 01:00:04,000
了前面部分中介紹的模型作為

1678
01:00:04,000 --> 01:00:06,079
最通用的公式，適用

1679
01:00:06,079 --> 01:00:07,760
於部分 o 在接下來的觀察和完全

1680
01:00:07,760 --> 01:00:09,599
觀察的環境

1681
01:00:09,599 --> 01:00:11,200
中，我們為完全觀察的情況描述了一個

1682
01:00:11,200 --> 01:00:13,119
充滿毛皮的

1683
01:00:13,119 --> 01:00:15,040
實現，

1684
01:00:15,040 --> 01:00:16,880


1685
01:00:16,880 --> 01:00:19,119
為未來的工作留下了對部分觀察的情況的分析，

1686
01:00:19,119 --> 01:00:21,680
所以它只是

1687
01:00:21,680 --> 01:00:23,040
完全觀察的情況

1688
01:00:23,040 --> 01:00:26,160
的另一個包裝，有部分觀察，

1689
01:00:26,160 --> 01:00:28,160
所以他們 決定使用一些基準

1690
01:00:28,160 --> 01:00:29,599
來幫助

1691
01:00:29,599 --> 01:00:32,480
他們基本上進行全面觀察，

1692
01:00:32,480 --> 01:00:33,200
這不像

1693
01:00:33,200 --> 01:00:35,599
作弊他們將要

1694
01:00:35,599 --> 01:00:36,240
描述

1695
01:00:36,240 --> 01:00:38,240
的示例有點像

1696
01:00:38,240 --> 01:00:39,440
在棍子上平衡餐盤，

1697
01:00:39,440 --> 01:00:40,880
所以在這種情況下，您確實可以全面

1698
01:00:40,880 --> 01:00:42,640
觀察模型

1699
01:00:42,640 --> 01:00:45,440
所以僅僅因為你不能很好地控制嗯嗯

1700
01:00:45,440 --> 01:00:46,079


1701
01:00:46,079 --> 01:00:47,760
它可能會或可能不會給定

1702
01:00:47,760 --> 01:00:49,119
一定的棒和

1703
01:00:49,119 --> 01:00:52,079
運動反射時間，但至少

1704
01:00:52,079 --> 01:00:53,520
你可以觀察到一切，所以

1705
01:00:53,520 --> 01:00:55,200
還有無法觀察到的控制理論

1706
01:00:55,200 --> 01:00:56,960
問題，然後是

1707
01:00:56,960 --> 01:00:58,160
系統的問題 實際上

1708
01:00:58,160 --> 01:01:00,160
就像餐盤一樣可以理解和控制，

1709
01:01:00,160 --> 01:01:01,599
但

1710
01:01:01,599 --> 01:01:03,760
你只會得到扭曲或惡意的

1711
01:01:03,760 --> 01:01:04,640


1712
01:01:04,640 --> 01:01:06,319
信息 其他那些您可以獲得完美

1713
01:01:06,319 --> 01:01:08,000
信息但僅在時間點

1714
01:01:08,000 --> 01:01:09,599
上的遊戲，並且

1715
01:01:09,599 --> 01:01:10,960
有很多不同的方式可以

1716
01:01:10,960 --> 01:01:12,640
發揮作用，但這就是

1717
01:01:12,640 --> 01:01:15,680
模型中可以切換的部分

1718
01:01:15,680 --> 01:01:17,520


1719
01:01:17,520 --> 01:01:19,200
，這也是我的 認為聽到任何作者的意見會很

1720
01:01:19,200 --> 01:01:20,799
有趣，

1721
01:01:20,799 --> 01:01:22,880


1722
01:01:22,880 --> 01:01:26,240
所以在第 4 節中，他們

1723
01:01:26,240 --> 01:01:28,799
進行了一些實驗，這

1724
01:01:28,799 --> 01:01:29,760


1725
01:01:29,760 --> 01:01:32,720
是他們論文的結果階段，他們

1726
01:01:32,720 --> 01:01:34,400
描述了他們如何調查

1727
01:01:34,400 --> 01:01:37,280
提出的主動

1728
01:01:37,280 --> 01:01:39,040
推理模型是否可以成功推廣

1729
01:01:39,040 --> 01:01:41,200
在沒有獎勵觀察的情況下進行

1730
01:01:41,200 --> 01:01:43,440
探索，即探索，所以

1731
01:01:43,440 --> 01:01:45,119
對於無模型

1732
01:01:45,119 --> 01:01:45,760


1733
01:01:45,760 --> 01:01:47,520
強化學習者來說，真正困難的是，當

1734
01:01:47,520 --> 01:01:48,799
它沒有得到獎勵時，

1735
01:01:48,799 --> 01:01:50,720
它做得不好，不能很好地探索，

1736
01:01:50,720 --> 01:01:52,000


1737
01:01:52,000 --> 01:01:54,960
因為它沒有獲勝，並且兩個

1738
01:01:54,960 --> 01:01:56,799
是否 模型可以

1739
01:01:56,799 --> 01:01:58,319
在

1740
01:01:58,319 --> 01:02:00,559
具有挑戰性的連續控制任務（即

1741
01:02:00,559 --> 01:02:02,400
開發）上實現良好的性能和高樣本效率，因此這就是需要

1742
01:02:02,400 --> 01:02:04,240
跟踪的東西

1743
01:02:04,240 --> 01:02:06,079
我們分別評估模型的這兩個方面，將

1744
01:02:06,079 --> 01:02:07,760


1745
01:02:07,760 --> 01:02:08,400
它們的聯合

1746
01:02:08,400 --> 01:02:10,400
性能分析，即探索

1747
01:02:10,400 --> 01:02:12,000
利用困境

1748
01:02:12,000 --> 01:02:14,319
留給未來的工作，所以再次

1749
01:02:14,319 --> 01:02:15,039


1750
01:02:15,039 --> 01:02:17,440
在純用例中分別展示

1751
01:02:17,440 --> 01:02:18,480
它，表明它可以做任何一個

1752
01:02:18,480 --> 01:02:20,079
，然後它會像

1753
01:02:20,079 --> 01:02:21,760
他們兩個之間的權衡或

1754
01:02:21,760 --> 01:02:23,680
最終將調解這種權衡的包裝層

1755
01:02:23,680 --> 01:02:25,839


1756
01:02:25,839 --> 01:02:27,599
他們要做的探索任務

1757
01:02:27,599 --> 01:02:29,839
被稱為山地車

1758
01:02:29,839 --> 01:02:33,039
，你是這個山谷底部的一輛小車

1759
01:02:33,039 --> 01:02:34,079


1760
01:02:34,079 --> 01:02:37,200
你可以用前進踏板向前推，

1761
01:02:37,200 --> 01:02:40,480
或者你可以

1762
01:02:40,480 --> 01:02:43,039
用倒車踏板推動倒車，但

1763
01:02:43,039 --> 01:02:44,880
你沒有足夠強大的引擎

1764
01:02:44,880 --> 01:02:47,760
來一次就到達黃旗，

1765
01:02:47,760 --> 01:02:48,000


1766
01:02:48,000 --> 01:02:50,160
就像你在底部

1767
01:02:50,160 --> 01:02:51,280
山上，

1768
01:02:51,280 --> 01:02:53,599
你的自行車處於三七檔，但你

1769
01:02:53,599 --> 01:02:55,280
可以稍微上去一點，然後回來

1770
01:02:55,280 --> 01:02:56,880
一點，所以在這裡你必須

1771
01:02:56,880 --> 01:02:58,640
探索並找出一種策略來

1772
01:02:58,640 --> 01:03:00,480
幫助你越來越多地探索

1773
01:03:00,480 --> 01:03:02,640
，以便 goi 獲得足夠的速度

1774
01:03:02,640 --> 01:03:04,079
爬上左邊的小山，以便獲得

1775
01:03:04,079 --> 01:03:07,359
黃色的平坦，然後他們將要談論的漏洞利用任務

1776
01:03:07,359 --> 01:03:10,079


1777
01:03:10,079 --> 01:03:11,119
是雙重的，

1778
01:03:11,119 --> 01:03:12,960
還有倒立擺任務，所以

1779
01:03:12,960 --> 01:03:14,480
這裡我們仍然是一輛小推車

1780
01:03:14,480 --> 01:03:17,119
或類似的東西，我們可以

1781
01:03:17,119 --> 01:03:18,799
在軌道上向前或向後移動

1782
01:03:18,799 --> 01:03:21,200
，然後是我們

1783
01:03:21,200 --> 01:03:21,920
試圖保持

1784
01:03:21,920 --> 01:03:23,599
直立的鐘擺，這就像

1785
01:03:23,599 --> 01:03:25,520
一根棍子上的餐盤，

1786
01:03:25,520 --> 01:03:27,920
然後是漏斗任務，這是一個

1787
01:03:27,920 --> 01:03:29,119
跳躍

1788
01:03:29,119 --> 01:03:32,400
運動協調任務，所以讓我們

1789
01:03:32,400 --> 01:03:33,920
看看他們實際上做了什麼 對於這

1790
01:03:33,920 --> 01:03:34,240
兩個

1791
01:03:34,240 --> 01:03:37,200
實驗中的每一個，然後查看主動

1792
01:03:37,200 --> 01:03:39,520
推理學習器如何與

1793
01:03:39,520 --> 01:03:43,039
其他算法疊加，因此山地車

1794
01:03:43,039 --> 01:03:45,440
示例是

1795
01:03:45,440 --> 01:03:47,039
位於兩座山之間的一維軌道，

1796
01:03:47,039 --> 01:03:48,640
目標是在右側開車上山，

1797
01:03:48,640 --> 01:03:50,160
但汽車的引擎是

1798
01:03:50,160 --> 01:03:51,520
不夠強壯，無法一次攀登這座山，

1799
01:03:51,520 --> 01:03:52,720


1800
01:03:52,720 --> 01:03:55,359
類似於設定成功的唯一方法

1801
01:03:55,359 --> 01:03:56,880
是來回行駛以

1802
01:03:56,880 --> 01:03:58,799
建立動力，所以

1803
01:03:58,799 --> 01:03:59,200


1804
01:03:59,200 --> 01:04:02,319
我認為這很酷

1805
01:04:02,319 --> 01:04:05,359
代碼在 openai 網站上，在

1806
01:04:05,359 --> 01:04:07,280
他們的 github.com 上

1807
01:04:07,280 --> 01:04:10,880
鍊接在這裡，所以它可以真正

1808
01:04:10,880 --> 01:04:13,520
被看到和使用並用作

1809
01:04:13,520 --> 01:04:14,160
標準，

1810
01:04:14,160 --> 01:04:16,240
所以看起來很酷不

1811
01:04:16,240 --> 01:04:17,440
知道這個資源

1812
01:04:17,440 --> 01:04:20,559
用於標準化不同的學習

1813
01:04:20,559 --> 01:04:21,760
算法

1814
01:04:21,760 --> 01:04:25,039
所以 主動推理代理

1815
01:04:25,039 --> 01:04:28,240
如何做得好它在這個測試中做得很好，

1816
01:04:28,240 --> 01:04:29,839
否則他們不會

1817
01:04:29,839 --> 01:04:32,640
寫論文和閱讀這些圖的方式

1818
01:04:32,640 --> 01:04:33,280


1819
01:04:33,280 --> 01:04:35,119
um a b 和 c 將具有相同的

1820
01:04:35,119 --> 01:04:36,880
x 和 y 軸，

1821
01:04:36,880 --> 01:04:40,480
所以我把地圖

1822
01:04:40,480 --> 01:04:43,520
與 左上角的引擎，

1823
01:04:43,520 --> 01:04:46,480
所以它映射到

1824
01:04:46,480 --> 01:04:46,960
左下角的

1825
01:04:46,960 --> 01:04:50,319
面板a上，所以位置

1826
01:04:50,319 --> 01:04:51,599
與x軸有關，

1827
01:04:51,599 --> 01:04:53,760
這就是小火車向左

1828
01:04:53,760 --> 01:04:55,280
多遠和向右多遠

1829
01:04:55,280 --> 01:04:57,839
，然後 速度

1830
01:04:57,839 --> 01:04:58,720


1831
01:04:58,720 --> 01:05:01,359
是它是否靜止為零，無論

1832
01:05:01,359 --> 01:05:03,200
它是向右

1833
01:05:03,200 --> 01:05:04,640
移動還是向左負移動，

1834
01:05:04,640 --> 01:05:05,280


1835
01:05:05,280 --> 01:05:08,240
所以

1836
01:05:08,240 --> 01:05:09,680


1837
01:05:09,680 --> 01:05:13,680
無論我猜是哪種方式都追求動量的獎勵代理最終都會

1838
01:05:13,680 --> 01:05:16,400
獲得大約 0.02 的最大速度

1839
01:05:16,400 --> 01:05:17,039
方式

1840
01:05:17,039 --> 01:05:20,079
和th  en 僅像 0.7 或

1841
01:05:20,079 --> 01:05:22,960
任何方向移動一樣，epsilon

1842
01:05:22,960 --> 01:05:25,599
貪婪代理確實設法學習了一個

1843
01:05:25,599 --> 01:05:26,880
短程策略

1844
01:05:26,880 --> 01:05:29,760
，我們可以看到它

1845
01:05:29,760 --> 01:05:31,760
在一側更進一步

1846
01:05:31,760 --> 01:05:34,400
，它確實需要一些罕見的偏移

1847
01:05:34,400 --> 01:05:34,960
進入

1848
01:05:34,960 --> 01:05:37,760
稍高的速度狀態，但在

1849
01:05:37,760 --> 01:05:38,880
最後，它

1850
01:05:38,880 --> 01:05:41,440
並沒有真正走得太遠，就像它

1851
01:05:41,440 --> 01:05:42,000


1852
01:05:42,000 --> 01:05:44,079
在上坡時學會瞭如何上坡，

1853
01:05:44,079 --> 01:05:46,400
但這僅在有限程度上有所幫助，

1854
01:05:46,400 --> 01:05:48,799
而實施的主動推理模型

1855
01:05:48,799 --> 01:05:49,599


1856
01:05:49,599 --> 01:05:52,480
不僅在 um 狀態空間內具有更廣泛的採樣

1857
01:05:52,480 --> 01:05:53,280


1858
01:05:53,280 --> 01:05:55,760
在 100 個 epoch 之後進行轉換，

1859
01:05:55,760 --> 01:05:56,960


1860
01:05:56,960 --> 01:06:00,960
但它們的位置

1861
01:06:00,960 --> 01:06:03,760
擴展到 x 中

1862
01:06:03,760 --> 01:06:04,000


1863
01:06:04,000 --> 01:06:06,079
更高的範圍以及 y 中更高的速度分佈

1864
01:06:06,079 --> 01:06:07,599
範圍，

1865
01:06:07,599 --> 01:06:10,640
因此

1866
01:06:10,640 --> 01:06:12,640


1867
01:06:12,640 --> 01:06:14,559
通過成為積極的推理學習者而

1868
01:06:14,559 --> 01:06:15,839
不是通過

1869
01:06:15,839 --> 01:06:19,119
epsilon 貪婪或只是 一種基於模型的

1870
01:06:19,119 --> 01:06:21,280
免費獎勵

1871
01:06:21,280 --> 01:06:25,520


1872
01:06:25,520 --> 01:06:27,599
論文中提出的第二個優化示例是這個

1873
01:06:27,599 --> 01:06:29,440
hopper v2 再次不知道這個

1874
01:06:29,440 --> 01:06:30,799
資源

1875
01:06:30,799 --> 01:06:34,160
所以這個很有趣

1876
01:06:34,160 --> 01:06:36,799
描述為使

1877
01:06:36,799 --> 01:06:37,839
二維單腿機器人

1878
01:06:37,839 --> 01:06:40,400
盡可能快地向前跳躍並且

1879
01:06:40,400 --> 01:06:42,079
代碼也在那裡

1880
01:06:42,079 --> 01:06:43,520


1881
01:06:43,520 --> 01:06:45,599


1882
01:06:45,599 --> 01:06:47,359


1883
01:06:47,359 --> 01:06:50,960


1884
01:06:50,960 --> 01:06:52,799
首先這張圖片有很多角度，上面

1885
01:06:52,799 --> 01:06:54,000


1886
01:06:54,000 --> 01:06:57,039
有一個棋盤，嗯，

1887
01:06:57,039 --> 01:07:00,079
它讓我想起了一些

1888
01:07:00,079 --> 01:07:03,200
進化計算的模擬器，

1889
01:07:03,200 --> 01:07:05,039
比如 linux 和類似的東西，

1890
01:07:05,039 --> 01:07:06,799
你可以擁有這些塊狀的外星人，這些外星人

1891
01:07:06,799 --> 01:07:08,000
會復制並

1892
01:07:08,000 --> 01:07:10,160
佔用大量的計算資源

1893
01:07:10,160 --> 01:07:11,200
只是

1894
01:07:11,200 --> 01:07:12,720
相互衝突，有時

1895
01:07:12,720 --> 01:07:14,079
只是輪廓，有時是

1896
01:07:14,079 --> 01:07:16,000
具有這種背景和平面圖的 3d 塊，

1897
01:07:16,000 --> 01:07:18,079


1898
01:07:18,079 --> 01:07:20,640
但在這裡，我們正在發展的是

1899
01:07:20,640 --> 01:07:22,240
控制策略，而不是

1900
01:07:22,240 --> 01:07:24,960
代表基因型的染色體，或者

1901
01:07:24,960 --> 01:07:25,920
我們只是說

1902
01:07:25,920 --> 01:07:28,000
但實際上 相似之處

1903
01:07:28,000 --> 01:07:30,000
在於遺傳算法和遺傳

1904
01:07:30,000 --> 01:07:30,640
算法

1905
01:07:30,640 --> 01:07:33,119
隨機搜索的這種思想，因為

1906
01:07:33,119 --> 01:07:34,559
有時這些控制策略的

1907
01:07:34,559 --> 01:07:35,520
優化方式

1908
01:07:35,520 --> 01:07:38,000
ed 實際上是通過涉及

1909
01:07:38,000 --> 01:07:39,839
諸如遺傳搜索算法之類的步驟

1910
01:07:39,839 --> 01:07:41,440
來切換它們的大量

1911
01:07:41,440 --> 01:07:42,319
參數，

1912
01:07:42,319 --> 01:07:43,680
所以有一些有趣的

1913
01:07:43,680 --> 01:07:45,839
相似之處，我只是認為這是一個

1914
01:07:45,839 --> 01:07:47,039
很酷的例子

1915
01:07:47,039 --> 01:07:49,599
，它是運動行為控制的交叉點，

1916
01:07:49,599 --> 01:07:50,240


1917
01:07:50,240 --> 01:07:53,280
也是複雜的狀態狀態

1918
01:07:53,280 --> 01:07:53,920
空間

1919
01:07:53,920 --> 01:07:56,000
估計 甚至是一個

1920
01:07:56,000 --> 01:07:57,200
不活躍的

1921
01:07:57,200 --> 01:07:59,119
uh 甚至可能體現的方法的開始，

1922
01:07:59,119 --> 01:08:00,960
因為在某種程度

1923
01:08:00,960 --> 01:08:03,760
的運動複雜性中，它將

1924
01:08:03,760 --> 01:08:05,680
隱含在系統中

1925
01:08:05,680 --> 01:08:07,680
並分佈在

1926
01:08:07,680 --> 01:08:09,920
系統中，就像其他系統一樣，

1927
01:08:09,920 --> 01:08:11,039


1928
01:08:11,039 --> 01:08:13,280
如果它有 500 塊肌肉和 這是一條人的

1929
01:08:13,280 --> 01:08:15,839
腿，它會有某種呃張

1930
01:08:15,839 --> 01:08:17,520
拉整體結構，

1931
01:08:17,520 --> 01:08:19,198
以某種方式保持平衡，促進

1932
01:08:19,198 --> 01:08:21,439
某些類型的行動，但不是其他類型的行動，所以

1933
01:08:21,439 --> 01:08:22,719
非常有趣的

1934
01:08:22,719 --> 01:08:25,839
選擇我想從

1935
01:08:25,839 --> 01:08:28,238
任何作者或其他人那裡聽到

1936
01:08:28,238 --> 01:08:29,600
什麼是 其他

1937
01:08:29,600 --> 01:08:32,158
很酷的基準測試其他

1938
01:08:32,158 --> 01:08:32,960
控制

1939
01:08:32,960 --> 01:08:35,198
系統問題有趣或

1940
01:08:35,198 --> 01:08:36,640


1941
01:08:36,640 --> 01:08:39,520
適用的社交版本或網絡 工作

1942
01:08:39,520 --> 01:08:41,920
或基於通信的版本，

1943
01:08:41,920 --> 01:08:44,880
所以結果

1944
01:08:45,198 --> 01:08:47,679
如何，所以這裡是如何查看這些

1945
01:08:47,679 --> 01:08:48,238


1946
01:08:48,238 --> 01:08:50,479
圖表 x 軸是時代，所以這只是

1947
01:08:50,479 --> 01:08:52,238
通過時間的採樣，即嘗試了多少

1948
01:08:52,238 --> 01:08:52,719


1949
01:08:52,719 --> 01:08:55,439
代參數，

1950
01:08:55,439 --> 01:08:56,640
這里紅線

1951
01:08:56,640 --> 01:08:59,920
表示相同 時間間隔，因此

1952
01:08:59,920 --> 01:09:03,359
c 和 d 與 epoch 1 到

1953
01:09:03,359 --> 01:09:04,000
100 相關

1954
01:09:04,000 --> 01:09:05,359
，這實際上

1955
01:09:05,359 --> 01:09:07,520


1956
01:09:07,520 --> 01:09:10,640


1957
01:09:10,640 --> 01:09:12,479


1958
01:09:12,479 --> 01:09:13,839


1959
01:09:13,839 --> 01:09:16,880
是針對 um 壓縮的 所以我們可以

1960
01:09:16,880 --> 01:09:18,640
討論相同的現象

1961
01:09:18,640 --> 01:09:20,960
，參考鐘擺觀察 a 和 c

1962
01:09:20,960 --> 01:09:24,238
以及料斗中的 b 和 d 並且在這

1963
01:09:24,238 --> 01:09:25,600
兩種情況下

1964
01:09:25,600 --> 01:09:28,399
放大或縮小很

1965
01:09:28,399 --> 01:09:29,040
明顯

1966
01:09:29,040 --> 01:09:32,080
的是動作和 y 軸是

1967
01:09:32,080 --> 01:09:33,120
獎勵，就像

1968
01:09:33,120 --> 01:09:34,640
在料斗任務中你能走多遠

1969
01:09:34,640 --> 01:09:36,238
，然後在倒立

1970
01:09:36,238 --> 01:09:37,439
擺任務

1971
01:09:37,439 --> 01:09:40,560
中，在給定策略

1972
01:09:40,560 --> 01:09:42,479
的特定參數組合的

1973
01:09:42,479 --> 01:09:46,158
情況下，你的時間大約保持不變，在這兩種情況

1974
01:09:46,158 --> 01:09:49,359
下，獎勵都會急劇增加

1975
01:09:49,359 --> 01:09:52,799
對於主動推理代理，並且

1976
01:09:52,799 --> 01:09:56,320


1977
01:09:56,320 --> 01:09:57,520


1978
01:09:57,520 --> 01:10:00,480
在前 100 個

1979
01:10:00,480 --> 01:10:01,679
甚至前

1980
01:10:01,679 --> 01:10:04,159
幾個 epoch 內進行一些採樣後立即開始爬升，這可能是因為

1981
01:10:04,159 --> 01:10:05,760


1982
01:10:05,760 --> 01:10:09,360
uh 小車的狀態空間較低，因為小車所

1983
01:10:09,360 --> 01:10:09,760
能做的

1984
01:10:09,760 --> 01:10:11,040
就是向前和向後移動

1985
01:10:11,040 --> 01:10:12,800
像gas和break之類的政策

1986
01:10:12,800 --> 01:10:13,520


1987
01:10:13,520 --> 01:10:15,679
，這都是擺錘的物理原理

1988
01:10:15,679 --> 01:10:16,880


1989
01:10:16,880 --> 01:10:18,239


1990
01:10:18,239 --> 01:10:20,000


1991
01:10:20,000 --> 01:10:21,600


1992
01:10:21,600 --> 01:10:23,440


1993
01:10:23,440 --> 01:10:25,440
在玩的參數

1994
01:10:25,440 --> 01:10:28,400
我沒有復制出確切的數字，但是

1995
01:10:28,400 --> 01:10:29,679
還有更多，所以

1996
01:10:29,679 --> 01:10:32,800
它需要更多的時間

1997
01:10:32,800 --> 01:10:33,679
，從零

1998
01:10:33,679 --> 01:10:36,880
到 um 可能是 40，

1999
01:10:36,880 --> 01:10:38,719
其中平均值就像沒有真正

2000
01:10:38,719 --> 01:10:40,560
起飛，這

2001
01:10:40,560 --> 01:10:42,719
與 ddpg 你

2002
01:10:42,719 --> 01:10:44,400
可以閱讀更多關於我只是不

2003
01:10:44,400 --> 01:10:45,440
打算進入這個

2004
01:10:45,440 --> 01:10:48,080
算法是什麼我只是相信

2005
01:10:48,080 --> 01:10:49,600
好吧作者使用了一些有意義

2006
01:10:49,600 --> 01:10:50,480


2007
01:10:50,480 --> 01:10:53,679
的東西讓我們聽聽任何人 e 關於

2008
01:10:53,679 --> 01:10:56,000
可以測試哪些其他替代方案，

2009
01:10:56,000 --> 01:10:57,280
所以這些都是我

2010
01:10:57,280 --> 01:10:59,280


2011
01:10:59,280 --> 01:11:02,560
不喜歡文學但肯定會

2012
01:11:02,560 --> 01:11:03,280
歡迎

2013
01:11:03,280 --> 01:11:05,040
機器學習領域某人的評論的

2014
01:11:05,040 --> 01:11:06,560


2015
01:11:06,560 --> 01:11:08,400
事情，以及主動推理的令人印象深刻或有用的

2016
01:11:08,400 --> 01:11:10,400
演示 或者

2017
01:11:10,400 --> 01:11:11,280


2018
01:11:11,280 --> 01:11:15,280


2019
01:11:15,280 --> 01:11:18,320
在這種框架中進行控制理論模擬意味著什麼

2020
01:11:18,320 --> 01:11:20,000
還使用

2021
01:11:20,000 --> 01:11:21,600
了哪些

2022
01:11:21,600 --> 01:11:24,000
這些倒立擺和

2023
01:11:24,000 --> 01:11:24,560
料斗

2024
01:11:24,560 --> 01:11:27,920
任務的當前最新技術我不知道，

2025
01:11:27,920 --> 01:11:32,159
但它會很棒 如果有人確實知道，

2026
01:11:32,159 --> 01:11:34,480
只是為了結束

2027
01:11:34,480 --> 01:11:36,400
與以前工作的關係，

2028
01:11:36,400 --> 01:11:39,199
好吧，這是來自他們的結束部分

2029
01:11:39,199 --> 01:11:40,960
，這可能是一個

2030
01:11:40,960 --> 01:11:44,480
紀律問題，但通常

2031
01:11:44,480 --> 01:11:46,800
它就像一個討論部分，但如果它

2032
01:11:46,800 --> 01:11:48,320
是與以前工作的關係，我

2033
01:11:48,320 --> 01:11:49,520
會期望它

2034
01:11:49,520 --> 01:11:51,280
多一點語境化

2035
01:11:51,280 --> 01:11:55,280
，放在論文的前面

2036
01:11:55,280 --> 01:11:57,520


2037
01:11:57,520 --> 01:11:58,320


2038
01:11:58,320 --> 01:12:00,400


2039
01:12:00,400 --> 01:12:02,800


2040
01:12:02,800 --> 01:12:05,040
他們編寫了我們的工作以這些

2041
01:12:05,040 --> 01:12:06,800
先前的模型為基礎，通過將模型

2042
01:12:06,800 --> 01:12:09,199
不確定性納入其主動分辨率中，

2043
01:12:09,199 --> 01:12:10,800
我們擴展了先前的點估計

2044
01:12:10,800 --> 01:12:12,560
模型以包括

2045
01:12:12,560 --> 01:12:14,480
參數的完整分佈並更新預期的

2046
01:12:14,480 --> 01:12:15,760
自由能函數

2047
01:12:15,760 --> 01:12:17,679
，從而主動最小化這些分佈中的不確定性

2048
01:12:17,679 --> 01:12:19,840


2049
01:12:19,840 --> 01:12:21,600
這使我們的實現

2050
01:12:21,600 --> 01:12:23,199
與認知和計算神經科學文獻中主動推理的規範模型保持一致，

2051
01:12:23,199 --> 01:12:24,320


2052
01:12:24,320 --> 01:12:26,400


2053
01:12:26,400 --> 01:12:29,120
所以這幾乎就像說是的，

2054
01:12:29,120 --> 01:12:29,679


2055
01:12:29,679 --> 01:12:32,320
而且它使我們能夠評估

2056
01:12:32,320 --> 01:12:34,640


2057
01:12:34,640 --> 01:12:36,239
在縮放的主動推理

2058
01:12:36,239 --> 01:12:38,320
框架下進行主動探索的可行性，將模型應用於 與以前的模型相比，

2059
01:12:38,320 --> 01:12:40,320
控制任務更複雜

2060
01:12:40,320 --> 01:12:42,400
並獲得更高的樣本效率

2061
01:12:42,400 --> 01:12:43,920
，

2062
01:12:43,920 --> 01:12:45,840
所以這有點像說我們已經使

2063
01:12:45,840 --> 01:12:47,840
這個主動推理模型更

2064
01:12:47,840 --> 01:12:50,000
接近它

2065
01:12:50,000 --> 01:12:53,280
在定性神經行為領域中的推測

2066
01:12:53,280 --> 01:12:55,360
，我認為這是一個很好的理由

2067
01:12:55,360 --> 01:12:56,800
正確閱讀不 w

2068
01:12:56,800 --> 01:12:58,960
因為它確實非常適合我們在變分生態學

2069
01:12:58,960 --> 01:13:01,280
討論中對行為

2070
01:13:01,280 --> 01:13:04,320
和嗯

2071
01:13:04,320 --> 01:13:07,679
神經生物學的

2072
01:13:07,679 --> 01:13:10,560
討論，所以這是在模擬

2073
01:13:10,560 --> 01:13:13,280
方面它是如何變成這種方式的，

2074
01:13:13,280 --> 01:13:15,280
所以這很酷，因為深度

2075
01:13:15,280 --> 01:13:16,560
主動推理現在可以

2076
01:13:16,560 --> 01:13:17,440


2077
01:13:17,440 --> 01:13:19,199
通過 本文中介紹的這些類型的模型

2078
01:13:19,199 --> 01:13:20,880


2079
01:13:20,880 --> 01:13:23,120
，以前就像哦，是的，我想

2080
01:13:23,120 --> 01:13:25,199
如果它是深度主動推理，那麼

2081
01:13:25,199 --> 01:13:27,040
它將適應敘述

2082
01:13:27,040 --> 01:13:28,640
，現在這是一種框架

2083
01:13:28,640 --> 01:13:30,400
，哦，是的，我們可以

2084
01:13:30,400 --> 01:13:31,600
通過時間談論敘述

2085
01:13:31,600 --> 01:13:35,520
這個框架可以我們

2086
01:13:35,520 --> 01:13:38,000
他們再次解決的第二個領域論文的另一個關鍵詞

2087
01:13:38,000 --> 01:13:39,199


2088
01:13:39,199 --> 01:13:42,000
是基於模型的強化學習

2089
01:13:42,000 --> 01:13:42,960
在當前工作中我們

2090
01:13:42,960 --> 01:13:45,120
選擇貝葉斯神經網絡以

2091
01:13:45,120 --> 01:13:47,199
確保與主動推理框架所支持的變分原理的一致性，

2092
01:13:47,199 --> 01:13:48,719


2093
01:13:48,719 --> 01:13:49,679


2094
01:13:49,679 --> 01:13:51,679
但請注意集成可以是 做

2095
01:13:51,679 --> 01:13:53,360
了一些小的修改後明確地貝葉斯，

2096
01:13:53,360 --> 01:13:54,480


2097
01:13:54,480 --> 01:13:57,920
所以他們討論了幾個

2098
01:13:57,920 --> 01:13:59,679
與強化者的同源性 t learning

2099
01:13:59,679 --> 01:14:01,840
and specific amortized inference

2100
01:14:01,840 --> 01:14:03,040
實際上有一個引用

2101
01:14:03,040 --> 01:14:06,159
是我從

2102
01:14:06,159 --> 01:14:07,520
幻燈片上的一篇論文中摘錄的，它

2103
01:14:07,520 --> 01:14:09,280
談到了

2104
01:14:09,280 --> 01:14:10,480
你如何能感覺到這

2105
01:14:10,480 --> 01:14:13,040
就像一個免費的情況，只需

2106
01:14:13,040 --> 01:14:14,159
包裝一個 函數

2107
01:14:14,159 --> 01:14:15,679
圍繞這些參數的估計，

2108
01:14:15,679 --> 01:14:17,600
因為它就像是一個

2109
01:14:17,600 --> 01:14:19,120
神經網絡，我正在實現一個神經

2110
01:14:19,120 --> 01:14:20,560
網絡，所以我有額外

2111
01:14:20,560 --> 01:14:22,800
的自由度來學習非線性策略，

2112
01:14:22,800 --> 01:14:26,000
但是論文認為

2113
01:14:26,000 --> 01:14:28,800
實際上你受到了限制，因為

2114
01:14:28,800 --> 01:14:30,400
函數 正在估計

2115
01:14:30,400 --> 01:14:32,320
這個高斯結果的均值和方差

2116
01:14:32,320 --> 01:14:34,080


2117
01:14:34,080 --> 01:14:37,520
，因此它實際上並沒有釋放一定

2118
01:14:37,520 --> 01:14:41,920
程度的輸出

2119
01:14:41,920 --> 01:14:44,960
表達能力，而是

2120
01:14:44,960 --> 01:14:48,080
一種

2121
01:14:48,080 --> 01:14:48,640


2122
01:14:48,640 --> 01:14:51,120
在當前計算硬件上實現的可定義和可擴展的方式

2123
01:14:51,120 --> 01:14:52,560
來實現這個

2124
01:14:52,560 --> 01:14:54,719
大規模的鉛筆和紙問題 估計

2125
01:14:54,719 --> 01:14:56,159


2126
01:14:56,159 --> 01:14:58,400
超複雜

2127
01:14:58,400 --> 01:15:00,640
高維狀態空間的高斯均值和方差，但這

2128
01:15:00,640 --> 01:15:02,239
仍然是它正在做的事情，

2129
01:15:02,239 --> 01:15:05,440
所以超級感興趣 ting 的東西，

2130
01:15:05,440 --> 01:15:09,520
嗯，還有這裡的註釋，合奏

2131
01:15:09,520 --> 01:15:11,679
可以是明確的貝葉斯，所以有

2132
01:15:11,679 --> 01:15:14,320
各種各樣的變體和口味

2133
01:15:14,320 --> 01:15:16,320
，所以

2134
01:15:16,320 --> 01:15:19,360
任何在這些領域的愛好者或

2135
01:15:19,360 --> 01:15:22,159
專家我都會欣賞

2136
01:15:22,159 --> 01:15:23,199
他們

2137
01:15:23,199 --> 01:15:26,080
對這意味著什麼或為什麼會這樣的看法

2138
01:15:26,080 --> 01:15:26,800
重要或

2139
01:15:26,800 --> 01:15:30,640
有趣，然後信息增益

2140
01:15:30,640 --> 01:15:34,400
，這也是關鍵詞之一，因此

2141
01:15:34,400 --> 01:15:36,960
確定可擴展和有效的

2142
01:15:36,960 --> 01:15:38,719
探索策略仍然是

2143
01:15:38,719 --> 01:15:40,320
強化

2144
01:15:40,320 --> 01:15:41,280
學習

2145
01:15:41,280 --> 01:15:43,440
無模型方法（例如貪婪或

2146
01:15:43,440 --> 01:15:45,440
博爾茲曼選擇規則

2147
01:15:45,440 --> 01:15:48,800
分頁質子肖恩博士）中的關鍵開放問題之一

2148
01:15:48,800 --> 01:15:51,520
在動作選擇過程中利用噪聲或

2149
01:15:51,520 --> 01:15:52,960
在獎勵統計中利用不確定性，

2150
01:15:52,960 --> 01:15:55,840
更有效的方法

2151
01:15:55,840 --> 01:15:57,280
是構建一個世界模型，

2152
01:15:57,280 --> 01:15:58,800
允許代理評估

2153
01:15:58,800 --> 01:16:00,400
它訪問和未訪問過的狀態空間的哪些部分，

2154
01:16:00,400 --> 01:16:01,440


2155
01:16:01,440 --> 01:16:02,960
這允許測量諸如

2156
01:16:02,960 --> 01:16:04,400
數量 用於探索的預測錯誤或預測

2157
01:16:04,400 --> 01:16:04,640
或

2158
01:16:04,640 --> 01:16:05,920
改進的數量

2159
01:16:05,920 --> 01:16:08,159
還可以，

2160
01:16:08,159 --> 01:16:10,640
如果學習模型影響不大 合法地或

2161
01:16:10,640 --> 01:16:12,320
明確地捕獲概率

2162
01:16:12,320 --> 01:16:12,960


2163
01:16:12,960 --> 01:16:14,880
特徵，這就是利基的統計

2164
01:16:14,880 --> 01:16:16,239
規律，然後

2165
01:16:16,239 --> 01:16:17,760
可以使用信息論測量

2166
01:16:17,760 --> 01:16:19,600
來指導探索

2167
01:16:19,600 --> 01:16:22,560
，所以這裡有很多話要說，最後

2168
01:16:22,560 --> 01:16:24,960


2169
01:16:24,960 --> 01:16:27,360
引入信息增益真的很有趣，

2170
01:16:27,360 --> 01:16:28,239


2171
01:16:28,239 --> 01:16:30,880
這就是它所說的

2172
01:16:30,880 --> 01:16:31,840


2173
01:16:31,840 --> 01:16:34,560
主動推理的學習模型以及

2174
01:16:34,560 --> 01:16:36,719
探索與世界生成模型相關聯的方式，

2175
01:16:36,719 --> 01:16:38,400
從而

2176
01:16:38,400 --> 01:16:40,560
促進行動不僅朝著

2177
01:16:40,560 --> 01:16:42,480
直接獎勵的那種

2178
01:16:42,480 --> 01:16:44,719
無模型 rl

2179
01:16:44,719 --> 01:16:48,719
也不僅僅是

2180
01:16:48,719 --> 01:16:51,520
像我喜歡跑步的嗯深度模型獎勵

2181
01:16:51,520 --> 01:16:53,679
馬拉松，因為它是健康的，我不

2182
01:16:53,679 --> 01:16:55,840
知道你說它不健康，只是

2183
01:16:55,840 --> 01:16:57,920
它的心態可以幫助一個人

2184
01:16:57,920 --> 01:17:03,600
肯定到達那裡，然後嗯，

2185
01:17:03,600 --> 01:17:05,040
這就是說我們實際上

2186
01:17:05,040 --> 01:17:06,640
可以更進一步

2187
01:17:06,640 --> 01:17:10,159
，我們可以用一個深度生成

2188
01:17:10,159 --> 01:17:11,040
模型

2189
01:17:11,040 --> 01:17:13,679
來說明什麼 一個可能正在做的我們可以

2190
01:17:13,679 --> 01:17:15,040
最大限度地提高精度

2191
01:17:15,040 --> 01:17:16,800
，這可能是你在

2192
01:17:16,800 --> 01:17:19,199
你知道的超 ultr 中聽到的 一個

2193
01:17:19,199 --> 01:17:22,800
um 致力於一個類似這樣的領域，

2194
01:17:22,800 --> 01:17:26,080
這就是我的方式和身份，

2195
01:17:26,080 --> 01:17:28,480
從這個意義上說，它會超越

2196
01:17:28,480 --> 01:17:30,000
這個整體哦，好吧

2197
01:17:30,000 --> 01:17:31,920
，每天這麼多英里對我來說是健康的，這是

2198
01:17:31,920 --> 01:17:33,840
一個很大的動力，它

2199
01:17:33,840 --> 01:17:36,480
可能是一個多 -規模的事情，嗯，只是一個

2200
01:17:36,480 --> 01:17:37,280
例子

2201
01:17:37,280 --> 01:17:39,280
，我絕對知道你知道，即使對

2202
01:17:39,280 --> 01:17:40,560
我來說，它只是在生活中發生了變化，

2203
01:17:40,560 --> 01:17:42,880
所以它只是展示了它

2204
01:17:42,880 --> 01:17:45,040
與你的利基你的背景你的

2205
01:17:45,040 --> 01:17:46,719
社會關係

2206
01:17:46,719 --> 01:17:49,840
這麼多其他領域的關係，

2207
01:17:49,840 --> 01:17:51,679
只是為了接近一個想法 關於

2208
01:17:51,679 --> 01:17:54,400
這個信息，

2209
01:17:54,400 --> 01:17:57,360
嗯，信息增益起初讓我感到驚訝，

2210
01:17:57,360 --> 01:17:58,560
因為我想嘗試

2211
01:17:58,560 --> 01:18:01,040
將所有這些與信息論聯繫起來

2212
01:18:01,040 --> 01:18:04,000
，我意識到它是

2213
01:18:04,000 --> 01:18:05,760
關於減少目標參數的不確定性

2214
01:18:05,760 --> 01:18:06,480


2215
01:18:06,480 --> 01:18:07,920
信息是關於減少你

2216
01:18:07,920 --> 01:18:09,840
對事物的不確定性

2217
01:18:09,840 --> 01:18:11,679
，它只是為了減少我們的

2218
01:18:11,679 --> 01:18:13,600
不確定其他

2219
01:18:13,600 --> 01:18:16,400
事物或更具體的

2220
01:18:16,400 --> 01:18:18,480
事物是多尺度模型

2221
01:18:18,480 --> 01:18:20,400
，其中包含一些我們

2222
01:18:20,400 --> 01:18:22,080
可能非常熟悉的屬性 h 喜歡

2223
01:18:22,080 --> 01:18:23,600
觀察模型

2224
01:18:23,600 --> 01:18:26,880
一些與獎勵模型同源的方面，

2225
01:18:26,880 --> 01:18:30,880
比如偏好

2226
01:18:30,880 --> 01:18:33,280
模型，讓我們說，但是一些

2227
01:18:33,280 --> 01:18:34,239
方面

2228
01:18:34,239 --> 01:18:37,280
可能主要是新的，可能是

2229
01:18:37,280 --> 01:18:39,040
關於自由能的部分

2230
01:18:39,040 --> 01:18:42,800
，然後是我理解的那個領域

2231
01:18:42,800 --> 01:18:46,080
與另一種類型的

2232
01:18:46,080 --> 01:18:48,640
信息論和模型選擇

2233
01:18:48,640 --> 01:18:51,280
以及變分框架有關，所以如果這

2234
01:18:51,280 --> 01:18:52,159
是一個

2235
01:18:52,159 --> 01:18:55,360
任何人都可以深入了解的領域，

2236
01:18:55,360 --> 01:18:57,120
我認為這很酷，也許我們可以

2237
01:18:57,120 --> 01:18:58,640
嘗試解開我

2238
01:18:58,640 --> 01:19:01,280
不確定它是什麼樣子的，但嗯

2239
01:19:01,280 --> 01:19:02,400
肯定

2240
01:19:02,400 --> 01:19:04,960
我想知道並

2241
01:19:04,960 --> 01:19:06,560
註意 p 和 q 的一些事情，

2242
01:19:06,560 --> 01:19:08,159
感謝 alec 的一些

2243
01:19:08,159 --> 01:19:10,480
電子郵件討論，以澄清論文的一些

2244
01:19:10,480 --> 01:19:11,520
方面，

2245
01:19:11,520 --> 01:19:15,199
因為嗯，這很有趣

2246
01:19:15,199 --> 01:19:16,159
，我認為有很多

2247
01:19:16,159 --> 01:19:18,719
含義以及我們可以應用哪些系統

2248
01:19:18,719 --> 01:19:21,520
這是近期

2249
01:19:21,520 --> 01:19:22,480
或中期

2250
01:19:22,480 --> 01:19:26,080
如此大量有趣的東西，我們將在

2251
01:19:26,080 --> 01:19:29,440
2020 年 11 月 10 日

2252
01:19:29,440 --> 01:19:29,840


2253
01:19:29,840 --> 01:19:34,239
和 17 日太平洋標準時間上午 7 點 30 分討論這個問題

2254
01:19:34,239 --> 01:19:38,560
，我認為就是這樣

2255
01:19:38,560 --> 01:19:43,600
讓我們看看是的，一個結束的

2256
01:19:43,600 --> 01:19:45,280
想法我們所能擁有的只是一個

2257
01:19:45,280 --> 01:19:46,960
與行動交織在一起的空間的深度生成模型，

2258
01:19:46,960 --> 01:19:47,679


2259
01:19:47,679 --> 01:19:49,440
因此我們探索的參數

2260
01:19:49,440 --> 01:19:50,880
是在

2261
01:19:50,880 --> 01:19:53,679
成功和最佳信息之間進行權衡的參數，

2262
01:19:53,679 --> 01:19:54,960
有時我們擔心

2263
01:19:54,960 --> 01:19:57,440
我們會轉向成功 這可能

2264
01:19:57,440 --> 01:19:59,040
不是最佳信息，

2265
01:19:59,040 --> 01:20:00,400
其他時候我們

2266
01:20:00,400 --> 01:20:02,000
以一種可能不成功的方式獲得大量信息，

2267
01:20:02,000 --> 01:20:03,040


2268
01:20:03,040 --> 01:20:04,880
但總體而言，我們很好地管理了這種權衡，

2269
01:20:04,880 --> 01:20:07,520
這就是我們如何做到這一點，

2270
01:20:07,520 --> 01:20:09,760
如此出色的工作，每個人都非常感謝

2271
01:20:09,760 --> 01:20:10,960
傾聽，

2272
01:20:10,960 --> 01:20:14,880
繼續努力 感謝您的參與，

2273
01:20:14,880 --> 01:20:17,280
我們確實為現場參與者提供了後續表格

2274
01:20:17,280 --> 01:20:18,800


2275
01:20:18,800 --> 01:20:21,199
，如果有人有反饋

2276
01:20:21,199 --> 01:20:21,840


2277
01:20:21,840 --> 01:20:25,120
、建議或問題，

2278
01:20:25,120 --> 01:20:28,480
您可以與我們保持聯繫，

2279
01:20:28,480 --> 01:20:31,679
並在該

2280
01:20:31,679 --> 01:20:34,159
參數重新參數化技巧和

2281
01:20:34,159 --> 01:20:36,080
該論文中的幻燈片上再上一張幻燈片，那就太好了

2282
01:20:36,080 --> 01:20:39,600
除此之外，感謝您的聆聽

2283
01:20:39,600 --> 01:20:43,040
和忍受呃異常

2284
01:20:43,040 --> 01:20:46,080
相機錯誤，但是

2285
01:20:46,080 --> 01:20:48,520
是的，我期待著 2020 年底

2286
01:20:48,520 --> 01:20:49,679


2287
01:20:49,679 --> 01:20:51,440
到來 通過所有這些很酷的

2288
01:20:51,440 --> 01:20:53,280
討論，

2289
01:20:53,280 --> 01:20:55,280
我們希望有一些新的參與者上

2290
01:20:55,280 --> 01:20:56,560
線，

2291
01:20:56,560 --> 01:20:59,040
或者帶來一些

2292
01:20:59,040 --> 01:21:00,239
我們沒有考慮過的新觀點，

2293
01:21:00,239 --> 01:21:02,960
或者從初學者的

2294
01:21:02,960 --> 01:21:03,920
角度

2295
01:21:03,920 --> 01:21:06,639
從任意數量的領域作為

2296
01:21:06,639 --> 01:21:07,120
起點來討論它

2297
01:21:07,120 --> 01:21:09,040
讓我們知道這是

2298
01:21:09,040 --> 01:21:10,560


2299
01:21:10,560 --> 01:21:13,679
我們一直很高興聽到的那種東西，

2300
01:21:13,679 --> 01:21:17,679
所以祝 11 月和 2020 賽季結束愉快

2301
01:21:17,679 --> 01:21:21,120
，我們期待

2302
01:21:21,120 --> 01:21:26,880
與您交談，再見


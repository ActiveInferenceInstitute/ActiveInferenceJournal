[
  {
    "start": 10.529,
    "end": 35.915,
    "text": " all right hello and welcome everyone it is october 24th 2023 and we're an active live stream number 55.0 on realizing synthetic active inference agents okay welcome to the active inference institute we're a participatory online institute that is communicating learning and practicing applied active inference you can find us at some of the links on this page",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 36.857,
    "end": 42.062,
    "text": " This is a recorded and an archived livestream, so please provide feedback so we can improve our work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 43.003,
    "end": 47.848,
    "text": "All backgrounds and perspectives are welcome, and we'll be following video etiquette for livestreams.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 49.89,
    "end": 57.058,
    "text": "Head over to ActiveInference.org if you want to learn more about participating in livestreams or other activities.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 58.473,
    "end": 77.86,
    "text": " All right, well, we're in Livestream 55 series with a goal to learn and discuss these two very interesting papers on realizing synthetic active inference agents, part one on epistemic objectives and graphical specification, part two on the variational message updates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 78.981,
    "end": 84.263,
    "text": "As with all videos, it's an introduction for some of the ideas, not a review or a final word.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 85.336,
    "end": 102.502,
    "text": " We're going to introduce ourselves, then jump into a fairly lengthy background section that will prepare us to ask the questions and get to a place where the paper's contributions can be figured out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 103.513,
    "end": 114.719,
    "text": " So let us begin with introducing ourselves and saying hi and saying maybe something that was exciting to us or made us want to participate in this series.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 115.219,
    "end": 115.799,
    "text": "So I'm Daniel.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 115.979,
    "end": 123.763,
    "text": "I'm a researcher in California, and I was interested to go a little deeper on message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 123.983,
    "end": 131.387,
    "text": "It's something that's brought up a lot in the textbook and implicitly in other papers, but this was an opportunity to tackle it head on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 131.727,
    "end": 133.188,
    "text": "And I'll pass to Bert.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 135.14,
    "end": 147.184,
    "text": " So I'm Bert, I study civil engineering in the Netherlands and I struggle with the math of active inference, but recently I picked up reinforcement learning and together with this paper, I think it really helps.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 149.425,
    "end": 152.386,
    "text": "Hi, I'm Jakob.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 152.506,
    "end": 156.467,
    "text": "I'm also a researcher in California and I'm",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 157.182,
    "end": 161.543,
    "text": " really excited about this paper from a number of different angles.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 161.683,
    "end": 178.487,
    "text": "I guess the graphical notation and the notation that the paper introduces, I think, can have really profound impact on the field from both a computational and a theoretical viewpoint.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 179.347,
    "end": 185.369,
    "text": "And I'm interested to learn more about the implications of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 187.942,
    "end": 202.47,
    "text": " the new notation for further research yep new notation just dropped okay there's a pair of papers as mentioned and the information is here so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 204.011,
    "end": 228.431,
    "text": " of us can phrase the big question that brought us to the paper but i wrote it this way which is right there in the title there's at least a triple play a triple pun tendre and there's a dyad of papers so what is this realizing in the context of the title well in one sense we're realizing something",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 229.864,
    "end": 249.924,
    "text": " terms of implementing it or manifesting it they're deploying something that is being realized so there's a accomplishment sense of realizing also the work calls attention to our own realizing process our relevance realization how we come to appreciate and interact with synthetic intelligence ours",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 251.198,
    "end": 260.24,
    "text": " And then we're also talking about building agents that do some kind of realizing in themselves, like realizing agents in that sense.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 260.78,
    "end": 264.021,
    "text": "So what kind of ending starts off with a triple play?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 265.661,
    "end": 266.062,
    "text": "I don't know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 267.862,
    "end": 274.503,
    "text": "Bert or Jakob, what big questions brought you to the paper?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 274.604,
    "end": 276.424,
    "text": "What do you think the paper takes on?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 280.674,
    "end": 296.302,
    "text": " I think in terms of creating scalable active inference models that are reproducible across a variety of settings is quite exciting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 296.362,
    "end": 306.408,
    "text": "So maybe realizing in that sense across a number of different domains is part of the meaning here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 308.607,
    "end": 334.285,
    "text": " And of course, the triple play that we were exploring with our own work from going from just a simple graphical representation to a mathematical description of the generative model and the algorithm for message passing and updating the generative model through time to then a code implementation that can be deployed in various dynamic setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 337.856,
    "end": 347.923,
    "text": " That is also what I think is an important part of this paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 347.943,
    "end": 353.106,
    "text": "Oh, yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 353.607,
    "end": 365.995,
    "text": "And for me, especially as not an expert on active inference, but someone who is more interested in applying it, their work on working towards a PyTorch of active inference is really valuable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 366.972,
    "end": 391.313,
    "text": " and i think their way of deconstructing the mechanisms of expected and variational energy and to combine them into one function makes it simplifies it a lot i think cool so we're gonna look deeper at the papers of course soon but just a few of the aims of the paper",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 393.137,
    "end": 402.343,
    "text": " Part one, they construct a purely synthetic approach to the active inference framework, motivated from the point of view of engineering rather than neurobiology.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 402.363,
    "end": 404.585,
    "text": "And we're going to talk about what synthetic is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 405.405,
    "end": 420.095,
    "text": "And in part two, they use a variety of technical phenomena, variational calculus, message passing, and reactive programming to simulate a perception action cycle on the teammates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 421.916,
    "end": 422.056,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 422.076,
    "end": 422.156,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 424.289,
    "end": 450.688,
    "text": " one of you would you like to read the first abstract and then the other can read the other abstract whoever wants to go first um I can read the the first one so the free energy principle FEP is a theoretical framework for describing how intelligent systems self-organize into coherent stable structures by minimizing a free energy functional",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 451.423,
    "end": 458.062,
    "text": " Active inference is a corollary of the FEP that specifically details how systems that are able to plan for the future",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 459.517,
    "end": 464.219,
    "text": " function by minimizing particular free energy functionals that incorporate information seeking components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 465.099,
    "end": 472.242,
    "text": "This paper is the first in a series of two where we derive a synthetic version of active inference on free form factor graphs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 473.002,
    "end": 478.424,
    "text": "The present paper focuses on deriving a local version of the free energy functionals used for active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 479.204,
    "end": 487.247,
    "text": "This enables us to construct a version of active inference which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 488.168,
    "end": 490.671,
    "text": " The resulting messages are derived in our companion paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 491.211,
    "end": 495.115,
    "text": "We also identify a gap in the graphical notation used for factor graphs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 495.836,
    "end": 503.503,
    "text": "While factor graphs are great at expressing a generative model, they have so far been unable to specify the full optimization problem, including constraints.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 504.184,
    "end": 507.507,
    "text": "To solve this problem, we developed constrained Forney-style factor graph.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 508.507,
    "end": 537.503,
    "text": " cffg notation which permits a fully graphical description of variational inference objectives we then proceed to show how cffgs can be used to reconstruct prior algorithms for active inference as well as derive new ones the latter is demonstrated by deriving an algorithm that permits direct policy inference for active inference agents circumventing a long-standing scaling issue that has so far hindered the application of active inference in industrial setting",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 538.359,
    "end": 546.291,
    "text": " We demonstrate our algorithm on the classic team-based task, ensure that it reproduces the information-seeking behavior that is a hallmark feature of active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 548.694,
    "end": 549.055,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 549.836,
    "end": 550.597,
    "text": "And the second paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 553.331,
    "end": 554.512,
    "text": " Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 554.552,
    "end": 562.216,
    "text": "The free energy principle describes biological agents as minimizing variation of free energy with respect to a generative model of the environment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 563.336,
    "end": 572.241,
    "text": "Active inference is a corollary of the FEP that describes how agents explore and exploit the environment by minimizing an expected free energy objective.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 573.194,
    "end": 578.538,
    "text": " In two related papers, we describe a scalable epistemic approach to synthetic active inference agents.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 579.179,
    "end": 592.329,
    "text": "By message passing on free-form Forney-style factor graphs, FFGs, a companion paper, part 1, introduces a constrained FFG notation that visually represents free energy objectives for active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 592.83,
    "end": 601.757,
    "text": "The current paper derives message passing algorithms that minimize generalized free energy objectives on a constrained Forney factor graph by variational calculus.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 603.248,
    "end": 614.137,
    "text": " A comparison between simulated BAT and generalized free energy agents illustrates that synthetic active inference influences the behavior on a team-based navigation task.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 615.598,
    "end": 626.466,
    "text": "With full message-passing accounts of synthetic active inference agents, it becomes possible to derive and reuse message updates across models and move closer to industrial applications of synthetic active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 628.828,
    "end": 629.589,
    "text": "Great, thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 630.916,
    "end": 635.898,
    "text": " Here's the roadmap section titles of the first paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 639.16,
    "end": 643.301,
    "text": "Here are the section headers of the second paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 645.462,
    "end": 649.044,
    "text": "So before we jump into the background and then the papers,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 650.256,
    "end": 676.265,
    "text": " let's just have one last train stop what is synthetic and what's synthetic about active inference so as it being 2023 we went to the language model and we asked for some adjectives that are in the semantic neighborhood of synthetic and you can see them here artificial lab created synthetic counterfeit phony deceptive inauthentic unnatural",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 677.662,
    "end": 682.109,
    "text": " A lot of these are somewhat negative associations semantically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 682.871,
    "end": 690.163,
    "text": "So how does synthetic come to be meaning something negative or deceitful?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 691.368,
    "end": 702.955,
    "text": " And then in what ways is that a similar or a different sense of synthetic than referring to the unified compositionality of something that's blended?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 703.796,
    "end": 706.858,
    "text": "And similarly, we could talk about artificial, like artificial intelligence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 706.938,
    "end": 709.099,
    "text": "Is that artifice, like crafted?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 709.8,
    "end": 713.242,
    "text": "Or is it artificial, like phony, bogus, false?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 714.343,
    "end": 714.443,
    "text": "So...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 715.393,
    "end": 740.764,
    "text": " the context of the paper there's probably a bunch of different ways that we can draw with the authors and a little bit here too what is being synthesized if you have any thoughts on this but what else might be synthetic it might refer to something that's not just a hypothetical like a real synthesis a realized synthesis it might refer to information risk and synthetic intelligence",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 742.458,
    "end": 755.937,
    "text": " might refer to synthetic judgments those whose predicates are wholly distinct from their subjects and then recently at the topos institute there was a great talk by jonathan sterling synthetic domains in the 21st century",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 757.312,
    "end": 763.218,
    "text": " So there's a lot of very interesting references and ways that synthetic is used.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 764.479,
    "end": 769.684,
    "text": "Do you have any preliminary thoughts on what you think about these papers?",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 770.065,
    "end": 770.946,
    "text": "I think",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 782.014,
    "end": 803.404,
    "text": " I guess building on what you already said, part of the synthesis could be decreasing the gap between the mathematical description of a model and its graphical representation and its subsequent code implementation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 803.964,
    "end": 809.046,
    "text": "So the focus on graphical models, on constrained Forney style factor graphs,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 812.904,
    "end": 828.181,
    "text": " combines the aspects of generalized free energy or betty free energy which is motivated by a forney style graph structure with the actual implementation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 829.261,
    "end": 844.368,
    "text": " of a synthetic agent or an artificial agent in code or in synthetic settings, which themselves are trying to model some part of reality.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 849.005,
    "end": 873.139,
    "text": " cool we'll explore okay so to Bert for background part one and thank you for all the work leading up to this dot zero Bert so take it away with the background just let me know when to switch slides yeah all right uh so for the background of part one uh it will delve into what are base crafts and how do you do an inference over these crafts",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 874.586,
    "end": 892.736,
    "text": " and then the authors use make extensive use of arcangeln to optimize over the graph at a foreign factor graph and then some miscellaneous stuff that will be relevant to the play smaller parts in the paper all right next slide",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 895.669,
    "end": 904.813,
    "text": " So first off, base graphs, working up eventually to 4D graphs, but starting with the simplest one, which is just a Bayesian network.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 905.654,
    "end": 911.676,
    "text": "And then the classical example is the earthquake network.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 912.977,
    "end": 920.08,
    "text": "So each circle is a random variable, and they represent a particular state.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 920.3,
    "end": 923.762,
    "text": "So whether, for example, the alarm goes off, yes or no.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 926.631,
    "end": 955.176,
    "text": " and these variables can be continuous or discrete and so the total generative model is the joint probability of all these random variables and you can factorize them so they only relate to one or two or several others at least a limited set of other variables and then moving on from that is the hidden Markov model where you add time so you have multiple time steps so in this case x1",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 956.503,
    "end": 957.584,
    "text": " X0, X1, X2.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 958.665,
    "end": 964.61,
    "text": "And this works under the Markov assumption, which is that all the relevant information of the past is in the current time step.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 965.351,
    "end": 967.152,
    "text": "So you only need to consider the current.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 968.553,
    "end": 975.519,
    "text": "And on top of that, you cannot observe hidden states directly, so you need an observation error.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 976.059,
    "end": 976.76,
    "text": "So that's the E.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 978.911,
    "end": 985.837,
    "text": " And then the next step, building on top of the hidden Markov model, is the partially observable Markov decision process, which is a lot of words.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 987.498,
    "end": 994.344,
    "text": "But really it's the same, where you have a hidden state, S, with an observable state, and you transition from states.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 996.125,
    "end": 1005.193,
    "text": "But now you add an action, and the action acts the policy, and that acts on the B, the transition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1006.937,
    "end": 1014.562,
    "text": " So an example would be I expect S1 to have a certain value given my action, and this observation O2 should be in line with that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1016.883,
    "end": 1019.585,
    "text": "And the policy is selected using the expected free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1021.566,
    "end": 1022.526,
    "text": "All right, next slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1026.849,
    "end": 1030.811,
    "text": "And then how do you actually figure out the values of a base graph?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1031.071,
    "end": 1032.192,
    "text": "And that requires inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1033.775,
    "end": 1034.695,
    "text": " And there are multiple ways.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1034.775,
    "end": 1046.978,
    "text": "So the first one is exact inference, which is what you would do if you calculated by pen and paper, which is really only practical for small discrete models, like the example model on the last slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1048.318,
    "end": 1052.759,
    "text": "And if you want to go to bigger models, more complex models, you need approximate inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1054.319,
    "end": 1059.94,
    "text": "And within that Monte Carlo sampling is a key pillar, and there are a bunch of algorithms that apply it, that use it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1062.265,
    "end": 1084.357,
    "text": " but the core idea is the same you sample many times from an idealist distribution to approximate another distribution of interest another method is method which you may be familiar with this variation of the energy which yeah active inference uses a lot and then you indirectly minimize the difference between two distributions using the KL divergent",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1085.853,
    "end": 1089.616,
    "text": " You try to minimize the KL divergence, but you cannot do that directly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1090.237,
    "end": 1093.539,
    "text": "So instead, you maximize the elbow.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1098.063,
    "end": 1100.204,
    "text": "And this works with a family of distributions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1101.005,
    "end": 1104.968,
    "text": "And a family is, for example, gaussians or exponentials, but you have to pick one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1106.983,
    "end": 1110.107,
    "text": " An embedded free energy works on this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1110.167,
    "end": 1115.174,
    "text": "It's also a free energy, but it has the assumption that only local interactions matter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1115.834,
    "end": 1122.403,
    "text": "So instead of going over the entire generative model, you only look at one factorized part at a time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1125.14,
    "end": 1129.941,
    "text": " And optimization, which is also used in the paper to find some parameters.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1130.862,
    "end": 1136.523,
    "text": "And it's Newton's method, but you have a function and you want to know where this function goes through the x-axis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1137.343,
    "end": 1147.666,
    "text": "So you can take the slope and iteratively take more slopes until you get at approximately the point where it goes through the x-axis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1149.927,
    "end": 1150.267,
    "text": "Next slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1154.631,
    "end": 1157.033,
    "text": " And then I'll just use a paper.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1157.393,
    "end": 1163.379,
    "text": "And so to quickly introduce this, it is a method to describe an objective function with constraints.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1165.4,
    "end": 1171.445,
    "text": "So in the first derivative of the Lagrangian, if you set it equal to zero, those are the optimal points.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1172.927,
    "end": 1176.93,
    "text": "So that would be where the blue and red curve intersect in this picture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1179.565,
    "end": 1186.388,
    "text": " And the Lagrangian multiplier lambda is the rate of change the objective function will change if you relax a constraint.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1187.968,
    "end": 1194.651,
    "text": "So in this example, it will be the same as changing the constant b of the red function.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1197.773,
    "end": 1201.134,
    "text": "So concretely, if the lambda was positive here,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1202.105,
    "end": 1206.088,
    "text": " then increasing B would result in objective function also increasing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1206.588,
    "end": 1209.571,
    "text": "So you have some leeway to work to improve the function even further.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1210.531,
    "end": 1211.572,
    "text": "So you can optimize more.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1211.712,
    "end": 1212.293,
    "text": "That's what it's saying.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1214.294,
    "end": 1215.195,
    "text": "Alrighty, next slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1217.617,
    "end": 1218.758,
    "text": "And that's some miscellaneous stuff.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1220.679,
    "end": 1225.182,
    "text": "So the term entropy is used a lot, especially in terms of epistemic foraging.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1226.363,
    "end": 1229.926,
    "text": "But for now, it's just a measure of how uncertain a distribution is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1232.088,
    "end": 1237.049,
    "text": " And in general, sampling an uncertain distribution gives you more information than sampling a certain one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1237.79,
    "end": 1242.851,
    "text": "For example, if it rains every day, seeing rain again doesn't tell you much because it rains every day anyway.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1245.192,
    "end": 1250.813,
    "text": "A direct delta function is a probability distribution that is one for a single point and zero elsewhere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1253.414,
    "end": 1254.935,
    "text": "And then a statistical moment",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1259.92,
    "end": 1265.824,
    "text": " In mathematics, the moments of a function are certain quantitative measures related to the shape of a function's graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1267.145,
    "end": 1279.695,
    "text": "If a function is a probability distribution, then the first moment is the expected value, so the average, the second is the central moment of variance, the standard deviation, the third is the skewness, and the fourth is the kurtosis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1281.914,
    "end": 1297.839,
    "text": " And finally, the Kullback-Leibnag control, which is a term from control theory, which basically attempts to make a system converge to a set prior and adjust it by minimizing the KL divergence.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1299.839,
    "end": 1300.019,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1302.62,
    "end": 1303.04,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1303.38,
    "end": 1304.801,
    "text": "Great background.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1305.781,
    "end": 1311.043,
    "text": "So that kind of speed runs Bayesian stats first layer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1312.168,
    "end": 1320.012,
    "text": " Now we're going to head into a second layer of background that's going to bring us into the Forney graph space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1320.633,
    "end": 1326.976,
    "text": "And after the second background part, we'll be in a position to understand two papers a little better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1328.097,
    "end": 1331.799,
    "text": "So here's the big picture on background part two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1333.334,
    "end": 1344.406,
    "text": " Even with all of background part one in hand, there's also some more modern, important act-inf as well as non-act-inf related advances that are going to get us up to speed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1345.487,
    "end": 1351.813,
    "text": "So for a lot of the concepts here, we're just overviewing it first, and then we'll be talking more to the authors about them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1352.85,
    "end": 1359.217,
    "text": " And so I'll take a more narrative overview of recent updates and then we'll steer towards the technical.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1360.338,
    "end": 1364.743,
    "text": "So first, graphical models, active inference, and belief propagation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1365.7,
    "end": 1368.742,
    "text": " Over the last years, Carl Friston et al.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1369.002,
    "end": 1372.585,
    "text": "have been collaborating with the lab and collaborators of Bert de Vrij.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1373.385,
    "end": 1378.429,
    "text": "And around 2017, they released several very important papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1378.889,
    "end": 1385.613,
    "text": "One was a factor graph description of deep temporal active inference, and another was the graphical brain belief propagation and active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1386.474,
    "end": 1402.863,
    "text": " So as Bert alluded to earlier, many of the operations or formalisms of active inference can be understood as manipulations of parameters or variables that might reflect data, observations, priors, precisions, different kinds of variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1403.603,
    "end": 1409.707,
    "text": "And that is given exact form in the graphical model, specifically the Bayesian graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1410.347,
    "end": 1416.509,
    "text": " And here we see the familiar figure 4.3 from the 2022 textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1417.41,
    "end": 1424.672,
    "text": "Now, the nodes on that graphical model are variables, and edges are their causal or their informational relationship.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1425.332,
    "end": 1427.633,
    "text": "And so especially after Pearl et al.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1428.033,
    "end": 1430.894,
    "text": "and the causal modeling developments in the 1980s and 90s,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1433.024,
    "end": 1444.087,
    "text": " This was adapted into a probabilistic or cybernetic setting where certain nodes on this graph were associated with action, with perception, with cognitive phenomena.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1444.468,
    "end": 1448.289,
    "text": "And so this is a causal model, a Bayesian graphical model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1448.349,
    "end": 1451.77,
    "text": "It's the kind of thing that we see in very many active inference papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1453.168,
    "end": 1454.951,
    "text": " Now, the Bayesian graph is awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1455.091,
    "end": 1461.52,
    "text": "It allows us to tackle some really challenging compute problems, get a lot of semantic interpretability, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1462.161,
    "end": 1465.847,
    "text": "However, there's also a few limitations or challenges in execution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1466.528,
    "end": 1468.31,
    "text": "So first, just epistemically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1468.891,
    "end": 1476.735,
    "text": " there's some side information that is needed to provide in order to get to a fully reproducible Bayesian graphical simulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1477.315,
    "end": 1483.398,
    "text": "And Livestream 54 shows how that information is provided formally by category theory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1484.039,
    "end": 1498.266,
    "text": "But more to the pragmatic point for this discussion, in actual computational execution, whether you're running that Bayes graph on a single core, single thread, or whether you're using multiple computing elements,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1499.56,
    "end": 1506.003,
    "text": " a given or a particular generative model must be implemented in a certain way or order.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1506.584,
    "end": 1514.748,
    "text": "If we were doing a single agent or a multi-agent simulation, we might wonder, so should we update the perception of everybody and then let everybody make action?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1514.808,
    "end": 1518.269,
    "text": "Or should we do action and perception of number one, then number two, then number three?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1518.289,
    "end": 1519.99,
    "text": "But then isn't that unfair?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1520.551,
    "end": 1524.112,
    "text": "Because then isn't one of them always getting to see and act first and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1524.573,
    "end": 1526.914,
    "text": "So just drawing the graph",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1527.932,
    "end": 1533.278,
    "text": " doesn't give you the actual order of operations to carry it out reproducibly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1533.799,
    "end": 1542.268,
    "text": "So that underspecification of the Bayesian graphical logistics or scheduling leads to a lot of degrees and freedom in model deployment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1542.829,
    "end": 1546.132,
    "text": "Now this might be an issue or not be an issue for any given task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1547.125,
    "end": 1559.472,
    "text": " However, whether you use an off-the-shelf or a custom solution to address this challenge, these different approaches might take time, they might not transfer, they might not be general, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1560.72,
    "end": 1570.707,
    "text": " Also, there might be different simulation outcomes or different computational costs that happen when you implement the same graphical model differently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1571.227,
    "end": 1574.529,
    "text": "So that limits high reliability use and also transfer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1575.07,
    "end": 1582.715,
    "text": "And then lastly, kind of related to computational complexity, estimations are difficult to understand just by looking at the base graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1582.735,
    "end": 1589.879,
    "text": "It'd be really nice if there was a way to have the model and then know how much it's going to cost or take to run.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1591.54,
    "end": 1613.889,
    "text": " so enter the forney factor graphs in 2001 forney wrote a technical paper called codes on graphs normal realizations just like normal 2001 stuff and this introduced some fundamental components let's ask the authors and find out what exactly those fundamental moves were",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1615.049,
    "end": 1622.931,
    "text": " But what was introduced led to the development of what is now called a Forney factor graph, FFG, or a normal graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1623.571,
    "end": 1624.911,
    "text": "Again, another interesting question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1624.951,
    "end": 1626.012,
    "text": "How is normal being used here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1627.752,
    "end": 1634.794,
    "text": "The Forney factor graph representation is dual or informationally equivalent to any given Bayesian graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1635.629,
    "end": 1638.531,
    "text": " Importantly, though, FFGs enable message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1639.291,
    "end": 1648.537,
    "text": "Message passing provides node, local, and whole graph scheduling logistics for messages in a way that allows for more tractability in execution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1649.838,
    "end": 1654.621,
    "text": "Importantly, these messages are being passed amongst variables at a very granular level.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1655.482,
    "end": 1661.646,
    "text": "This is not referring to two bounded agents sending each other a postcard simply.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1664.502,
    "end": 1668.224,
    "text": " The network or the graph, it's a very general data structure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1668.985,
    "end": 1677.25,
    "text": "So it's not surprising that we're going to be encountering multiple kinds of graphs, but it is relevant to know which kinds of graphs we're talking about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1678.18,
    "end": 1698.829,
    "text": " And because this FFG representation can be applied to anywhere a Bayesian graph is used, people have applied both Bayesian graphs and, to a lesser or more recent extent, 40-factor graphs to a bunch of different statistical problems like filtering, smoothing, prediction, MDP, POMDP, and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1699.889,
    "end": 1702.831,
    "text": "So to kind of lock this in...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1703.991,
    "end": 1729.521,
    "text": " here we see on the right figure 7.3 from par pazulo and friston's textbook so this is the standard discrete time partially observable markov decision process and on the left here is a figure from the graphical brain paper of 2017. so across the top these two bayesian graphs are actually identical they're the same exact type of graph and they're both the same base graph",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1730.418,
    "end": 1740.903,
    "text": " And then the 2017 figure shows that that base graph has this one-to-one relationship with a Forney factor graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1742.884,
    "end": 1745.566,
    "text": "What is the difference with a Forney factor graph?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1745.646,
    "end": 1753.87,
    "text": "Why do we need to have a different format to describe what might at first glance appear to be the same information?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1755.291,
    "end": 1756.851,
    "text": "Let's go to Graphical Brain 2017.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1759.912,
    "end": 1764.954,
    "text": " We use graphical representations to characterize message passing under deep generative models that might be used by the brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1765.895,
    "end": 1769.476,
    "text": "We use three sorts of graphs to emphasize the form of generative models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1770.917,
    "end": 1773.078,
    "text": "One, the nature of Bayesian belief updating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1773.278,
    "end": 1776.059,
    "text": "Two, and how this might be accomplished in neuronal circuits.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1776.579,
    "end": 1776.779,
    "text": "Three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1777.579,
    "end": 1780.901,
    "text": "So here's the three sorts of graphs that they're going to talk about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1782.962,
    "end": 1785.563,
    "text": "First, Bayesian networks or dependency graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1786.786,
    "end": 1788.387,
    "text": " That's the one that we see the most commonly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1789.147,
    "end": 1791.188,
    "text": "Nodes correspond to unknown variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1792.108,
    "end": 1794.609,
    "text": "Edges denote dependencies amongst variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1794.629,
    "end": 1810.515,
    "text": "40-factor graphs have nodes that represent local functions or factors of a probability distribution over random variables, while edges come to represent variables per se, or more exactly, a probability distribution over those variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1811.462,
    "end": 1826.846,
    "text": " And finally, neural networks have nodes that are constituted by the sufficient statistics of unknown variables and other auxiliary variables like prediction errors, while the edges in those graphs denote an exchange of the functions of sufficient statistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1827.906,
    "end": 1839.649,
    "text": "Crucially, these graphical representations are formally equivalent in the sense that any Bayesian network can be expressed as a factor graph, and any message passing on a factor graph can be depicted as a neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1841.069,
    "end": 1847.814,
    "text": " However, as we will see later, the various graphical formulations offer different perspectives on belief updating or propagation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1848.955,
    "end": 1852.158,
    "text": "So this is a very rich topic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1854.28,
    "end": 1863.687,
    "text": "This background section was to say that Forney graphs have a different style or format than Bayesian graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1864.639,
    "end": 1868.28,
    "text": " However, they exist in this dual relationship.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1868.48,
    "end": 1882.183,
    "text": "So there's things like algorithms and procedures that you can do on FFG that you can't do on Bayes graphs, but it's all good because you can go from the Bayes graph to the FFG representation and then implement some procedure on the FFG.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1883.223,
    "end": 1885.744,
    "text": "So let's get to the papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1887.523,
    "end": 1913.615,
    "text": " The background one and background two sections have got us to perhaps the point where we're very excited to see how the authors implemented fully synthetic active inference agents using message passing, logistical and operational scheduling algorithms on 40-factor graphs representing Bayesian graphical models crafted to calculate a constrained beta-free energy",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1914.762,
    "end": 1926.347,
    "text": " as a generalized or unified free energy imperative, unifying the variational and the expected free energy, for sense-making and decision-making in active inference agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1927.588,
    "end": 1932.009,
    "text": "So now we're heading into another double movement of the coming two papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1933.15,
    "end": 1942.094,
    "text": "The first paper is itself going to be like a background and a notational paper, but also one that adds to the literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1943.294,
    "end": 1949.038,
    "text": " And then the second paper will make the addition on top of the advance of the first paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1950.159,
    "end": 1952.381,
    "text": "So a lot of material.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1953.021,
    "end": 1955.623,
    "text": "Let's go into the first paper, Bert.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1958.425,
    "end": 1962.868,
    "text": "Yes, so for the first paper, we'll be going over it chapter by chapter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1963.849,
    "end": 1968.932,
    "text": "To start with, setting up the Lagrangian, then defining",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1971.13,
    "end": 1975.191,
    "text": " the epistemic objective and how to include it in a synthetic agent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1976.151,
    "end": 1981.453,
    "text": "Then in chapter 4 they lay out what Lagrangian active inference means.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1982.753,
    "end": 1989.175,
    "text": "In chapter 5 they describe the notation for the constraint for an effector graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1991.112,
    "end": 2008.678,
    "text": " In six, they highlight why the original generalized classical active inference and the original generalized free energy algorithm are special cases of the Lagrangian active inference, so of that definition of the four-unit factor graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2010.079,
    "end": 2013.56,
    "text": "And then in seven, they do an experiment for policy inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2018.106,
    "end": 2019.647,
    "text": " Yeah.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2019.927,
    "end": 2022.788,
    "text": "And for chapter two, the Lagrangian approach to message passing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2024.388,
    "end": 2032.031,
    "text": "So as Daniel described, phony vector graphs represent a factorized function over variables, which you can see in equation one and figure one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2034.812,
    "end": 2040.354,
    "text": "So on the left, you see equation one that shows per node where V is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2042.395,
    "end": 2044.956,
    "text": "So A is a node in the set of all nodes V.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2047.446,
    "end": 2055.66,
    "text": " And so on the height, you can see the image which says that each node is a factor of all the variables and the variables are on edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2056.06,
    "end": 2056.962,
    "text": "And that's really important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2058.888,
    "end": 2063.412,
    "text": " And as mentioned in the background, approximate inference is used, in this case variational inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2064.233,
    "end": 2072.762,
    "text": "And so we start with a free energy function, an equation 3 on the left, which is used to approximate variational distribution Q star.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2072.922,
    "end": 2076.805,
    "text": "And Q star is the optimal distribution that we want to get to.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2077.226,
    "end": 2079.348,
    "text": "And we achieved it by minimizing it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2082.909,
    "end": 2088.853,
    "text": " And so Beth free energy is distinct from variation of free energy by factorizing the calculation of free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2089.674,
    "end": 2099.22,
    "text": "So instead of computing it all at once, you compute it per node and edge, and then aggregate the results, which you can see on the right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2102.863,
    "end": 2106.305,
    "text": "Each local free energy will include entropy terms from all connected edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2108.317,
    "end": 2113.204,
    "text": " And since edges can be connected to two nodes, the entropy of these variables would be counted twice.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2113.845,
    "end": 2118.592,
    "text": "So one edge is a variable, and if you connect it to two nodes, you would be counting the same variable twice.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2119.433,
    "end": 2124.2,
    "text": "And so to cancel that out, they add the one minus degree of the edge.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2126.198,
    "end": 2134.441,
    "text": " And at the bottom, I've added that normally an edge has always a degree of two, which might sound weird because how can an edge have a degree?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2134.842,
    "end": 2136.202,
    "text": "Because normally nodes have degrees.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2138.163,
    "end": 2145.706,
    "text": "But in chapter five, we will show that it's possible to have dangling edges that are not connected, that are not factorized on one end.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2146.386,
    "end": 2148.407,
    "text": "So it is possible to have edges with degree one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2150.313,
    "end": 2158.116,
    "text": " And last, three constraints ensure that the sum of probabilities for each node and edge equals one, and it's normalization.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2159.737,
    "end": 2168.861,
    "text": "And also that it is possible to retrieve edge and node probabilities from the joint distribution, so that the edge says something about the nodes, and the nodes say something about the edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2172.922,
    "end": 2173.523,
    "text": "Yes, next slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2173.543,
    "end": 2173.743,
    "text": "Cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2174.723,
    "end": 2175.964,
    "text": "Yeah, I've just got something to say.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2176.857,
    "end": 2205.954,
    "text": " Yeah, just that we have the variables on the edges of this graph, and the Fs are like functions or operations or little factories where variables can come in, potentially multiple variables can come in and some activity happens, but we're putting the variable that would be in the node of the base graph, and we're having it on an edge connecting functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2207.148,
    "end": 2223.782,
    "text": " So again, the information is identical or congruent, but everything that we're going to see now is flipped into this space where the edges are the variables, and that's what is going to enable the node local computations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2229.045,
    "end": 2238.89,
    "text": " And then in chapter 3, defining epistemic objectives, the authors write that ages interact with the world they inhabit and entail a generative model of the environment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2239.911,
    "end": 2242.933,
    "text": "Achieving future goals can be cast as free energy minimization.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2244.253,
    "end": 2248.556,
    "text": "And then they ask the question, what should this free energy functional look like and why?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2250.417,
    "end": 2252.798,
    "text": "And that sets up the rest of the paper, basically.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2255.164,
    "end": 2264.992,
    "text": " Only optimizing bad free energy or variation of free energy does not lead to exploration, but KL control, which means that only prior values are satisfied.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2266.513,
    "end": 2276.4,
    "text": "A hallmark of active inference is alternative functionals specifically made for inferring policies like expected free energy, which includes information collection.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2277.281,
    "end": 2283.185,
    "text": "And so epistemics arise from the optimization of approximate mutual information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2285.029,
    "end": 2299.801,
    "text": " mutual information uh on the bottom height is a metric like the r squared if you make a linear and that it tells you it tells you how much one variable relates to another one",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2302.478,
    "end": 2307.482,
    "text": " And to go back, mutual information between X and Z in equation 10 on the left.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2309.263,
    "end": 2320.411,
    "text": "To work on it, the uncertainty entropy of X. Yeah, that's in the height equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2321.191,
    "end": 2324.353,
    "text": "So it's how much you know about the one given that you know the other.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2326.375,
    "end": 2329.257,
    "text": "And so say X is an observation and Z is an eternal state.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2330.748,
    "end": 2342.394,
    "text": " then an agent can choose X to see information about set.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2342.474,
    "end": 2343.374,
    "text": "are related to each other.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2346.916,
    "end": 2351.318,
    "text": "And then how do you, how to define a functional that combines battery energy and .",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2351.738,
    "end": 2352.139,
    "text": "So to have .",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 2359.618,
    "end": 2375.969,
    "text": " the energy and add negative mutual information and minimizing the free energy functional the elbow with a negative mutual information means maximizing the mutual information so you're trying to get as much information about internal and observation states",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2379.591,
    "end": 2382.592,
    "text": " Yeah, just one comment there, Bert, and then Jakob, feel free to add anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2383.072,
    "end": 2387.153,
    "text": "That's a great comparison you made with the r-squared for linear regression.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2387.753,
    "end": 2397.076,
    "text": "So in a linear regression, r-squared summarizes how much the two axes, the x and the y-axis, resemble each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2397.096,
    "end": 2400.977,
    "text": "r-squared of 1 means they're exactly collinear.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2401.357,
    "end": 2402.037,
    "text": "They're on a manifold.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2402.995,
    "end": 2409.502,
    "text": " r-squared of 0 is there's no linear relationship, and r-squared of negative 1 would be a negative association.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2409.522,
    "end": 2414.228,
    "text": "Mutual information is kind of like that, but two characteristics that are importantly different.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2414.348,
    "end": 2418.813,
    "text": "First off, you can't have information on something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2419.4,
    "end": 2424.421,
    "text": " The worst thing that two things could be, or the least that they could be, is just noise to each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2424.441,
    "end": 2431.223,
    "text": "So you can take the negative of the mutual information, but you can't have negative information on something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2431.863,
    "end": 2434.804,
    "text": "And then secondly, it's not a linear relationship.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2435.504,
    "end": 2447.628,
    "text": "You could have something that is an inverted U, and the linear regression might find that that had a low R squared because the best regression line might go through the middle of the U.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2448.53,
    "end": 2454.651,
    "text": " But then if you just think about that a little bit more generally, of course one of those variables has information on the other variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2455.631,
    "end": 2468.974,
    "text": "And so it's kind of like moving a lot of our intuition and also some of the epistemic status of linear regressions into more of a pure information geometric space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2469.834,
    "end": 2473.055,
    "text": "And it's a really important point too that the VFE doesn't",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2474.284,
    "end": 2477.866,
    "text": " endogenously have an epistemic drive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2478.866,
    "end": 2483.449,
    "text": "It's just about the real-time surprise level of beliefs and incoming data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2484.529,
    "end": 2494.434,
    "text": "And so a hallmark of active inference is the construction of these functionals such as expected free energy or free energy of the expected future or generalized free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2498.276,
    "end": 2498.456,
    "text": "Okay."
  },
  {
    "start": 2501.909,
    "end": 2509.798,
    "text": " And so expected free energy only works for future time steps, whereas variational and bad free energy work for past and the current time step.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2511.1,
    "end": 2520.17,
    "text": "And instead, the authors posted generalized free energy works for both at the same time, and it includes a part that tracks",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2521.104,
    "end": 2523.005,
    "text": " time in equation 14.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2523.105,
    "end": 2528.707,
    "text": "But that's on the end, you have P with a curvy stripe, xk.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2529.487,
    "end": 2533.628,
    "text": "And that basically keeps count of which time step we have at this moment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2534.769,
    "end": 2540.371,
    "text": "And also note that the policy or control is fixed.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2540.591,
    "end": 2542.251,
    "text": "So that's you with a roof on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2544.792,
    "end": 2549.234,
    "text": "So past time steps have observed data and are locked using the direct delta function.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2550.097,
    "end": 2559.931,
    "text": " which was the function with a probability of 1 at a specific value and 0 at the rest, whereas future timesteps are still open to be optimized using generalized free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2563.045,
    "end": 2573.153,
    "text": " And so below that, you can see this free energy, how you lock some values and keep them open at other moments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2574.034,
    "end": 2585.023,
    "text": "And so for a sneak peek on the figure in the right from paper two, you can see that as time goes on and time steps are observed, data constraints are added.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2585.884,
    "end": 2590.728,
    "text": "And that's the little black circle with a delta in it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2593.857,
    "end": 2605.081,
    "text": " And committing to a full mode of specification up your limits what a generalize that you think you can do instead of making it fully local is synthetic active inference as the artist right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2610.323,
    "end": 2610.523,
    "text": "Okay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2614.602,
    "end": 2620.884,
    "text": " And then in chapter four, they discussed the Lagrangian active inference, which is the key of the paper.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2621.904,
    "end": 2627.905,
    "text": "And so with node-local generalized free energy, we can construct a Lagrangian for active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2628.485,
    "end": 2637.028,
    "text": "And the goal is to have a distributed inference procedure solving for each node individually and constraining the solution as shown in equation 13.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2638.288,
    "end": 2638.768,
    "text": "So on the right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2640.475,
    "end": 2644.537,
    "text": " And we need to assume that actual and approximate probabilities are about equal to each other.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2646.979,
    "end": 2647.979,
    "text": "We had to make that assumption.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2648.26,
    "end": 2649.781,
    "text": "So I guess we can ask that next week.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2650.721,
    "end": 2656.945,
    "text": "And now with this, messages can be derived as done in part two, which is the next paper.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2657.985,
    "end": 2663.909,
    "text": "However, in this paper, they attempt another way to arrive at the node-local generalized free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2664.149,
    "end": 2665.21,
    "text": "And this is done in two steps.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2666.35,
    "end": 2669.192,
    "text": "So first, they apply a mean field factorization.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2672.136,
    "end": 2685.582,
    "text": " that ensures edges are independent for each node, which you can see on the right, where you have the factor of the node, sr, has a product for all edges, so they are independent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2686.322,
    "end": 2695.447,
    "text": "And then the second move they make is that you can partition the connecting variables edges into two sets, one that is adjusted and the other that is left alone.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2696.327,
    "end": 2700.929,
    "text": "And to the adjusted set, they apply p substitution, which is very important.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2702.774,
    "end": 2707.657,
    "text": " And this basically replaces a local free energy with a local generalized free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2709.378,
    "end": 2717.663,
    "text": "And on the bottom is the formula for applying p-substitution, which I also don't fully understand.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2718.384,
    "end": 2720.165,
    "text": "So maybe you guys know a little more about that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2724.229,
    "end": 2725.771,
    "text": " Definitely will ask.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2726.672,
    "end": 2732.619,
    "text": "I'll just note here that the slash is used to mean except for.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2732.639,
    "end": 2734.922,
    "text": "I hope this is correct.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2735.783,
    "end": 2744.453,
    "text": "So here we're able to take something that contains all and we're able to kind of pull out one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2746.627,
    "end": 2758.358,
    "text": " with this more intractable integral and then still do the simpler log but we'll ask",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2764.916,
    "end": 2776.298,
    "text": " The second paper will show how p-substituted nodes are gradually removed from the Forney factor graph over time, as these nodes are now in the past and do not need to consider the future anymore.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2777.098,
    "end": 2784.98,
    "text": "Again, for a sneak peek, see on the right, everywhere you add the delta in the black circle, you remove the square.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2786.38,
    "end": 2789.781,
    "text": "And the square is the notation for p-substitutions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2793.365,
    "end": 2801.852,
    "text": " And then to construct the active inference Lagrangian, it is important that p-substituted nodes work with different messages, generalized free energy instead of path-free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2802.753,
    "end": 2804.054,
    "text": "And in this way, add epistemics.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2805.215,
    "end": 2816.004,
    "text": "The optimal points of the Lagrangian, where the first derivative is 0, are the stationary point of the message passing process, which means that nodes do not change anymore if you keep updating.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2817.225,
    "end": 2819.387,
    "text": "And so there's a very long formula for the Lagrangian.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2821.986,
    "end": 2828.549,
    "text": " And as I said, you split the nodes into two, one set that you p-substitute and the other that you don't.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2830.47,
    "end": 2835.192,
    "text": "And on top of that, you add three constraints, which have a lambda in front of them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2835.872,
    "end": 2840.534,
    "text": "And that is the marginalization, the normalization of both nodes and edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2843.956,
    "end": 2850.725,
    "text": " That's very important about the stationary point of message passing being kind of, like, settled.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2851.766,
    "end": 2855.351,
    "text": "Where everybody can pass all the messages they want and nothing is changing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2856.592,
    "end": 2859.095,
    "text": "And that's gonna be, um...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2860.39,
    "end": 2882.547,
    "text": " probably leveraged in the reactive message passing programming environment, where rather than needing to send all messages once to the calculate and send all messages again, it opens up the ability for different regions of the graph to be sending and receiving messages at different frequencies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2883.107,
    "end": 2887.551,
    "text": "So if one sensor was sampling a thousand times a second, one was sampling one time per second,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2888.271,
    "end": 2899.216,
    "text": " No longer do you have to decide, should we coarse-grain to one second, or should we waste 999 cycles on the slower cycle sensor?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2899.876,
    "end": 2909.64,
    "text": "Now, with reactive message passing, because there's no local descriptions, it's possible to open up that implementational space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2915.649,
    "end": 2920.154,
    "text": " And now we'll get into defining the notation for the constrained Forney factor graphs.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2921.014,
    "end": 2928.642,
    "text": "And so Forney factor graphs are useful for describing generative models, but it is important to know the exact functional to be minimized.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2929.483,
    "end": 2935.429,
    "text": "And the authors develop a new notation for writing constraints directly as part of the Forney factor graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2937.125,
    "end": 2939.767,
    "text": " because normal phony vector graphs do not show that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2941.748,
    "end": 2948.653,
    "text": "Figure 4 on the right builds upon representing vectors with squares, nodes, and edges as variables that can be vectorized.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2949.793,
    "end": 2957.739,
    "text": "They add circular beads, which indicate constraints that define our family Q. That has to do with the variation of distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2958.679,
    "end": 2965.824,
    "text": "A bead on an edge denotes that of the edge QSI, while a bead in the center of a node is QSA.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2968.346,
    "end": 2975.392,
    "text": " And then they build, they introduce four constraints that are needed as notation for the Forney factor graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2977.293,
    "end": 2983.818,
    "text": "So the first one is a factorization constraint, and it can either be a naive mean field or a structured mean field.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2984.539,
    "end": 2994.467,
    "text": "When a naive mean field is a stronger constraint, which means that every factor is independent, which is figure five, it's in the middle on the right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2996.537,
    "end": 3010.608,
    "text": " and structured is less strong because you can factorize multiple variables together and i added some example formats many edges like needed for",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3012.311,
    "end": 3019.519,
    "text": " If you want to have many edges, many variables into a factor, they introduce the notation with dots in between.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3020.24,
    "end": 3026.547,
    "text": "And this is useful when you have, for example, a Gaussian mixture model, which has a bunch of components.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3030.975,
    "end": 3042.638,
    "text": " We'll see if this is accurate, but let's imagine these four variables are coming in and meeting in the factory, four prerequisite components of the car assembly process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3043.778,
    "end": 3050.78,
    "text": "The fully factorized way to represent that is shown here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3052.008,
    "end": 3056.25,
    "text": " That's the mean field assumption that we can just treat those components separately.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3056.871,
    "end": 3063.574,
    "text": "Like if what we were doing was just multiplying, or I mean, adding the numbers together, maybe we could treat them separately.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3064.454,
    "end": 3074.079,
    "text": "But then also there are these intermediate structural factorizations where you can take four joint coming in",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3075.24,
    "end": 3100.287,
    "text": " and then you could separate it into these two are separate and these two are connected or two pairs so it gives you the expressivity to do the continuum from fully joint to fully factorized mean field and everything between all the combinations in between in a per node fashion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3101.425,
    "end": 3107.649,
    "text": " rather than at the whole graph level, whereas that is often how it's discussed elsewhere.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3108.269,
    "end": 3113.753,
    "text": "Somebody constructs a big generative model and says, and then we took a mean field approach to the graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3114.553,
    "end": 3118.896,
    "text": "So here we have that kind of expressivity in a given node.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3130.876,
    "end": 3134.237,
    "text": " unmute, then continue, Bert.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 3134.257,
    "end": 3134.617,
    "text": "Good call.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3135.397,
    "end": 3144.699,
    "text": "So the second is a form constraint, which enforces a particular function or distribution, really, on a local marginal of an edge or a node.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3146.14,
    "end": 3150.441,
    "text": "Figure 8 shows what enforcing a Gaussian constraint on an edge looks like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3151.801,
    "end": 3155.744,
    "text": " And figure nine shows two different constraints on the nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3156.765,
    "end": 3164.17,
    "text": "And the authors explicitly note that a constraint on an edge is independent of the nodes and that it factorizes into and vice versa.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3164.971,
    "end": 3167.492,
    "text": "So the messages pass.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3167.893,
    "end": 3172.656,
    "text": "And then after everything is done, it's like squeezing clay into a box.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3173.256,
    "end": 3173.937,
    "text": "It just has to fit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3174.357,
    "end": 3175.438,
    "text": "But it's only after the fact.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3179.048,
    "end": 3183.471,
    "text": " And then they note dangling edges, which do not terminate on a node on one side.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3186.033,
    "end": 3190.696,
    "text": "So they wouldn't require a bead, but you need it to be able to do a form constraint.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3191.337,
    "end": 3192.318,
    "text": "So they still hide them in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3195.14,
    "end": 3200.423,
    "text": "And that's why they add a sort of dummy factor node on one end.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3203.105,
    "end": 3205.247,
    "text": "And then the third constraint is a delta constraint.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3208.507,
    "end": 3212.53,
    "text": " The delta constraints and data points allow us to incorporate measurements into a model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3214.132,
    "end": 3216.714,
    "text": "Equation 30 and figure 10 show this for data constraints.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3216.874,
    "end": 3220.297,
    "text": "These are special since they denote observations and block the flow of messages.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3220.617,
    "end": 3222.579,
    "text": "So these points are now fixed.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3223.039,
    "end": 3225.101,
    "text": "Nothing goes through it because it is already set.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3226.642,
    "end": 3232.767,
    "text": "And then another option is that when you don't know which value to fix it to, so the S I had,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3234.423,
    "end": 3238.366,
    "text": " Then you can optimize the value, and then it is called a delta constraint.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3239.146,
    "end": 3242.888,
    "text": "And the optimization of this is don't use an expectation maximization.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3245.79,
    "end": 3251.674,
    "text": "What I saw in this section was, let's just say that we were drawing height observations from",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3253.456,
    "end": 3282.527,
    "text": " forest so one way to think about that is the the gaussian distribution that we're drawing from okay but now you get the data point and you could think about that data point as just an entry and a value you could also think of it as a dirac delta distribution parameterized exactly by the value of the data point and so thinking about data as being a dirac constrained",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3286.027,
    "end": 3303.147,
    "text": " variable where here the variables are on edges brings unification between empirical uh data coming in from the outside or passed around internally and broader distributional perspectives",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3309.366,
    "end": 3317.453,
    "text": " And then the fourth constraint is moment matching, which replaces the hard marginalization constraint used to include the entropy of edges.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3318.574,
    "end": 3322.237,
    "text": "And so this loosens the marginalization as now only the moments need to align.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3323.958,
    "end": 3329.703,
    "text": "And equation eight contains both node and edge terms since the moment matching constraint applies to both at the same time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3331.545,
    "end": 3335.048,
    "text": "And they apply to both at the same time because of the top formula.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3337.899,
    "end": 3352.452,
    "text": " And the middle one, the middle formula shows the sufficient statistics described by T, capital T. And so basically what this means is that you pick a distribution and you only pick the moments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3352.772,
    "end": 3358.257,
    "text": "So the first one would be the mean of the distribution and the second would be the standard deviation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3359.098,
    "end": 3360.359,
    "text": "And so only those need to fit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3360.519,
    "end": 3362.121,
    "text": "You don't care about the rest.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3367.734,
    "end": 3373.502,
    "text": " And P-substitution is the final piece needed to represent the active inference Lagrangian on a C-constraint for any vector graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3374.463,
    "end": 3385.378,
    "text": "Constructing the local generalized free energy using mean field vectorization and P-substitution was only done to represent the Lagrangian active inference on a constraint for any vector graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3387.256,
    "end": 3393.038,
    "text": " and recall that p-substitution involves substituting part of the model p for q in the expectation only.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3393.919,
    "end": 3396.019,
    "text": "And that is the formula on the bottom.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3396.74,
    "end": 3399.501,
    "text": "And it's represented with a square in the Forney factor graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3402.842,
    "end": 3410.145,
    "text": "And it shows the substitution on figure 13, replacing the local variation of free energy with a local generalized free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3414.707,
    "end": 3433.462,
    "text": " rule replacing Q on the data or on Y with P that conditions on X and Z. This red piece is being called attention to, and we'll unpack that more with the authors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3438.086,
    "end": 3444.231,
    "text": "And on top of that, lastly, the authors compressed the constraint for an effect to make it easier to read.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3445.063,
    "end": 3448.905,
    "text": " And only deviations relative to the default bed-free energy are shown.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3449.705,
    "end": 3456.508,
    "text": "And the bead chain is introduced as a series of beads connected by edges and is summarized if it contains no extra information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3457.548,
    "end": 3464.491,
    "text": "And the others go really into depth, but I will just keep it to this, where they go from the left image to the right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3465.032,
    "end": 3467.233,
    "text": "So the right is the cleaned up phony factor graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3468.273,
    "end": 3472.815,
    "text": "And then you can more easily compress it and make it smaller for very large diagrams.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3475.714,
    "end": 3487.721,
    "text": " Yeah, allow us to kind of compress or skip through the uninformative parts and just call attention to the new information that's being intersected.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3492.756,
    "end": 3501.921,
    "text": " And then in chapter six, they highlight how classical active inference and original generalized free energy algorithm are special cases of the developed Lagrangian active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3503.261,
    "end": 3514.147,
    "text": "So an integral part of message-passing algorithms is the choice of a schedule, which Daniel already explained, which is the order of messages in which they are passed.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3514.887,
    "end": 3522.451,
    "text": "Iterative methods are sensitive to the order, and Lagrangian active inference is an iterative method and might be sensitive to the choice of schedule.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3523.451,
    "end": 3529.056,
    "text": " Choosing the schedule carefully allows us to recover classical active inference planning algorithms as a special case.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3531.358,
    "end": 3543.348,
    "text": "Figure 20 on the left shows the constraint for an effect graph of a composite node for Lagrangian active inference on discrete state spaces.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3544.509,
    "end": 3548.392,
    "text": "The equations on the right show the corresponding factors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3549.275,
    "end": 3555.577,
    "text": " where HA is defined as a function of the transition matrix required for the order equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3555.637,
    "end": 3557.518,
    "text": "Figure 21 shows the message updates.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3559.839,
    "end": 3571.623,
    "text": "Some cannot be solved in closed form and instead require multi-color estimates.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3571.763,
    "end": 3578.905,
    "text": "U is the average free energy of the composite node and corresponds exactly to expected free energy issues in standard active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3581.312,
    "end": 3586.255,
    "text": " where the composite notes the block with a striped block around it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3587.516,
    "end": 3590.417,
    "text": "M of Z and M of A are solved differently than the rest.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3591.358,
    "end": 3597.121,
    "text": "They require Z-bar results using Newton's method, as it tends to fluctuate between multiple extremes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3598.201,
    "end": 3600.303,
    "text": "And M of A is estimated using a sampling procedure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3601.283,
    "end": 3608.147,
    "text": "And so below shows the generative model of a discrete prompt, and the equations",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3610.215,
    "end": 3612.357,
    "text": " to below that show the factors.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3613.217,
    "end": 3615.639,
    "text": "And note again the hat on U, so the x's are fixed.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3621.884,
    "end": 3635.434,
    "text": "And calculating the generalized free energy is done using a forward sweep, which you can see with the arrows on the bottom right going from left to right, and summing the free energy terms over all the substituted composite nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3637.159,
    "end": 3640.661,
    "text": " And these should then be equal to the result from the shown equation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3646.986,
    "end": 3647.346,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3647.646,
    "end": 3652.189,
    "text": "These images are just so interesting, even without knowing what they are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3657.705,
    "end": 3663.008,
    "text": " And reconstructing original generalized free energy method is then a matter of including past observations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3663.829,
    "end": 3665.99,
    "text": "For past time steps, adding data constraints.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3667.071,
    "end": 3669.833,
    "text": "While for future time steps, adding piece substitution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3671.093,
    "end": 3673.435,
    "text": "And a naive min field is applied to all the nodes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3674.115,
    "end": 3676.457,
    "text": "The schedule, the order of messages is shown below.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3677.817,
    "end": 3683.461,
    "text": "The update equations of the generalized free energy using this model are the same as of the expected free energy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3692.028,
    "end": 3697.45,
    "text": " And then the authors work on an example, which is the classic T-Maze task.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3700.491,
    "end": 3703.392,
    "text": "So the tools in this paper are not limited to restating prior work.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3704.472,
    "end": 3718.097,
    "text": "It offers more advantages, one of which is the ability to directly infer a policy instead of relying on post hoc selection, choosing actions which have the best energy terms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3719.921,
    "end": 3721.081,
    "text": " Instead, you can do it immediately.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3721.922,
    "end": 3735.447,
    "text": "So the optimal action to take is therefore to visit... Well, the optimal action that he makes is to first visit 4 because you can get information about 2 and 3, rather than immediately try to go to 2 and 3.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3736.367,
    "end": 3739.448,
    "text": "And that is the hallmark of an active inference agent.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3741.778,
    "end": 3744.22,
    "text": " Yeah, a lot to explore here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3744.8,
    "end": 3765.314,
    "text": "But instead of relying on the post-hoc comparison of energy terms, this is in reference to the way that usually expected free energy is used by taking in habit, the policy prior, and then iterating over every element in pi, the policy prior, and sharpening or updating them",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3766.315,
    "end": 3771.037,
    "text": " according to the expected free energy with a pragmatic value in the epistemic value component.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3771.057,
    "end": 3782.28,
    "text": "So you get some policy posterior, and then you might select simply the best one, or you might take some temperature-guided sampling across the policy posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3782.821,
    "end": 3787.902,
    "text": "So that's a post hoc comparison of expected free energy terms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3788.883,
    "end": 3789.843,
    "text": "And with the...",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 3795.526,
    "end": 3817.926,
    "text": " that the generative models now have the ability to directly infer policy potentially without explicit consideration of counterfactuals or at the very least without this kind of a post hoc comparison.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3826.042,
    "end": 3833.046,
    "text": " And so they build on that and create another full model.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3834.607,
    "end": 3844.332,
    "text": "And note that in equation 47, they use a mixture model of candidate transition matrices indexed by UK.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3845.132,
    "end": 3847.714,
    "text": "So each action has a different transition matrix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3851.096,
    "end": 3853.317,
    "text": "And yeah, a bunch of illustrations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3857.58,
    "end": 3873.273,
    "text": " And figure 28 shows an agent that initially prefers the epistemic action, and so moves down to state 4, and subsequently exhibits a preference for either of the potentially rewarding arms, so of course left or right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3873.973,
    "end": 3881.8,
    "text": "And this shows that the Lagrangian active inference is able to infer the optimal policy, and that approach can reproduce prior results of the teammates.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3886.977,
    "end": 3892.281,
    "text": " And then the authors do it again, but now by adding a data constraint.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3893.081,
    "end": 3902.488,
    "text": "And this is similar to using a maximum a posteriori estimate, which basically means that the action is yes or no.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3902.868,
    "end": 3903.428,
    "text": "So it's 100% or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3910.222,
    "end": 3924.116,
    "text": " Yeah, by pushing through the point mass constraint, kind of like that Dirac delta constraint, all that can be shared is the first moment, which is just the data points value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3924.176,
    "end": 3929.682,
    "text": "So here it's like, you might have a 70% chance of doing thing 1, 20%, 2%, 1%, but then...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3933.806,
    "end": 3954.803,
    "text": " the action selected is decisive and so you have this decisive passing of action and the the decisive realization of location even if also there's like a location distribution that includes support over the entire maze or a policy distribution that includes support overall policies",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3959.076,
    "end": 3967.606,
    "text": " And then to conclude, in this paper, we have proposed a novel approach to active inference based on Lagrangian optimization, which we have named Lagrangian active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3968.487,
    "end": 3976.597,
    "text": "We demonstrated Lagrangian active inference on a classic benchmark problem for the literature and found that it enhanced epistemic drive to this hallmark feature.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3977.958,
    "end": 3980.94,
    "text": " It presents three main advantages over previous algorithms.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3981.841,
    "end": 3994.869,
    "text": "First, an advantage is the computational efficiency afforded by being able to pass backwards messages instead of needing to perform forward rollouts for every policy, like a tree search, so it scales linearly over time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3996.863,
    "end": 4005.486,
    "text": " A second advantage is that it allows for directly inferring posteriors over control signals instead of relying on model comparison, like Daniel just explained.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4006.647,
    "end": 4017.551,
    "text": "And thirdly, it is inherently modular and consequently works for freely definable constraints for any factor crash, while prior work has focused mostly on specific generative models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4018.857,
    "end": 4024.263,
    "text": " And they have also introduced a notation for hiding down constraints and piece substitutions on a Forney graph.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4025.344,
    "end": 4031.03,
    "text": "The constraint factor Forney graphs are useful not only for active inference, but for specifying free energy functionals in general.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4032.171,
    "end": 4041.461,
    "text": "And the authors hope that this can become a standard tool similar to Forney factor graphs when it is desirable not just to write the model, but also a family of distributions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4042.853,
    "end": 4051.64,
    "text": " In the future, we plan to extend the work to more node constructions to further open the scope that can be attacked with active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4054.562,
    "end": 4055.462,
    "text": "Awesome work, Bert.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4055.662,
    "end": 4056.883,
    "text": "Great job preparing it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4058.104,
    "end": 4059.966,
    "text": "Jakob, anything you want to add on part one?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4064.509,
    "end": 4064.969,
    "text": "No, at the moment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4066.09,
    "end": 4066.39,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4067.151,
    "end": 4070.894,
    "text": "Scaling linearly in time is really fascinating.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4072.168,
    "end": 4079.91,
    "text": " Like, if you planned 10 time steps, should the 11th step be a 10% bump in difficulty?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4080.03,
    "end": 4087.552,
    "text": "Or should it be another combinatoric explosion every little kilometer deeper into the future?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4089.373,
    "end": 4090.773,
    "text": "So, second paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4092.222,
    "end": 4098.992,
    "text": " Okay, first just going in deeply to the first sentences before accelerating across.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4099.853,
    "end": 4108.065,
    "text": "So they begin the paper saying, free energy principle postulates that the behavior of biological agents can be modeled as minimizing a variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4108.991,
    "end": 4117.9,
    "text": " And this is commonly brought up, that we can model biological agents as or through or with minimizing variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4118.56,
    "end": 4126.728,
    "text": "Just like saying we can model this regression by fitting an L2 norm, we can model this biological agent by fitting the ELBO or the VFE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4127.822,
    "end": 4145.054,
    "text": " Active inference, which is AIF in this paper, is a corollary of the FEP that describes how agents propose effective agents by minimizing an expected free energy objective that internalizes a generative model, GM, of the agent's environment and prior beliefs about desired outcomes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4145.934,
    "end": 4146.035,
    "text": "So,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4146.894,
    "end": 4151.275,
    "text": " VFE is the real-time behavior minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4151.915,
    "end": 4161.417,
    "text": "EFE brings it into the prospective setting by introducing epistemic imperative and also talking about observations that haven't happened yet.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4162.818,
    "end": 4168.939,
    "text": "Variational objectives for active inference can be minimized by message passing on a Forney style factor graph representation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4169.946,
    "end": 4175.55,
    "text": " So everything we know about Bayesian graphical models transposes into the FFG space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4176.491,
    "end": 4178.292,
    "text": "The FFG is not a kind of Bayes graph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4178.892,
    "end": 4186.998,
    "text": "It is a dual, a different kind of graph conveying the same information in a restructured format that we can then optimize or implement differently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4188.919,
    "end": 4192.502,
    "text": "Several authors have attempted to scale active inference under message passing framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4193.002,
    "end": 4196.645,
    "text": "However, agents based on these approaches lack crucial epistemic characteristics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4197.796,
    "end": 4200.137,
    "text": " So, one question, why was that?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4200.817,
    "end": 4204.479,
    "text": "Was it that previous authors only modeled VFE but not EFE?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4205.459,
    "end": 4209.681,
    "text": "Or was there some failure of the EFE loading on epistemic value?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4210.261,
    "end": 4211.462,
    "text": "I think the answer is the first one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4214.423,
    "end": 4224.047,
    "text": "In part one, they identified this hiatus in the specification space, and they introduced the CFFG, and then...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4225.242,
    "end": 4239.534,
    "text": " In part two, which is the current paper, we use the CFFG notation as introduced in part one to define locally constrained variational objectives and derive variational message updates for GFE, generalized free energy, base control using variational calculus.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4240.995,
    "end": 4245.479,
    "text": "The resulting control algorithms introduce epistemic behavior in synthetic active inference agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4246.8,
    "end": 4252.084,
    "text": "We reason purely from an engineering point of view and do not concern ourselves with biological plausibility.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4253.459,
    "end": 4254.199,
    "text": " What a sentence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4255.64,
    "end": 4257.721,
    "text": "In this paper, our contributions are threefold.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4258.701,
    "end": 4264.043,
    "text": "They use variational calculus to derive message update expressions for GFE control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4264.923,
    "end": 4269.085,
    "text": "They derive specialized messages for discrete variable model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4270.385,
    "end": 4276.308,
    "text": "And they implement the results in a reactive programming framework simulating a perception action cycle on the T maze.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4277.571,
    "end": 4295.222,
    "text": " With a full message passing account and reactive implementation of GFE optimization, it becomes possible to derive and reuse custom message updates across models and get a step closer to realizing scalable synthetic active inference agents for industrial applications.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4296.022,
    "end": 4307.129,
    "text": "So what is it about their contributions that makes the industrial and the engineering work more transferable or scalable?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4308.993,
    "end": 4310.756,
    "text": " Then they summarize the coming sections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4311.637,
    "end": 4313.64,
    "text": "Section 2 reviews variational base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4314.522,
    "end": 4320.551,
    "text": "Section 3 reviews active inference, perception, learning, and control in terms of message passing on the CFFG.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4322.256,
    "end": 4333.899,
    "text": " Section 4 focuses on the constraint definitions around a submodel of two facing nodes, hashtag Alice and Bob, and derives stationary solutions and messages for GFE-based control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4334.9,
    "end": 4343.102,
    "text": "Section 5 applies the results to a specific discrete variable goal observation submodel that is often used in AIF practice.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4344.531,
    "end": 4351.334,
    "text": " They then work towards implementation in a simulated setting and describe a perception action cycle in terms of time-dependent constraints, section six.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4352.254,
    "end": 4357.896,
    "text": "The team A's task is described in section seven and simulated in a reactive programming framework in section eight.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4358.877,
    "end": 4362.378,
    "text": "Section nine has a summary of related work and the conclusions are in section 10.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4363.638,
    "end": 4367.48,
    "text": "It's also useful to put table one and table two up here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4368.56,
    "end": 4372.242,
    "text": "Table one is an overview of notational conventions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4373.792,
    "end": 4378.277,
    "text": " nice list of letters would be awesome to connect to the active inference ontology.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4379.118,
    "end": 4382.081,
    "text": "And then table two has a bunch of acronyms",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4385.979,
    "end": 4390.061,
    "text": " Okay, section two, review of variational message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4390.781,
    "end": 4397.764,
    "text": "So this section briefly reviews variational message passing as a distributed approach to minimizing variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4398.384,
    "end": 4402.325,
    "text": "So first, they review variational base, 2.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4403.026,
    "end": 4408.548,
    "text": "Then 2.2, they review Forney-style factor graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4408.568,
    "end": 4411.609,
    "text": "2.3, they move to the Beth-Lagrangian optimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4413.439,
    "end": 4425.267,
    "text": " Using Lagrange multipliers, we can convert the optimization on Q to a free-form optimization problem of the Lagrangian, where the Lagrangian multipliers enforce local constraints.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4426.588,
    "end": 4430.591,
    "text": "The fully local optimization then becomes this expression.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4434.053,
    "end": 4440.558,
    "text": "Again, more to say, they're using the Lagrangian constraint framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4441.863,
    "end": 4455.348,
    "text": " and the Bethe flavor of free energy, which is already node-local, to provide a very constrained, which is to say, possibly more solvable and tractable node-local computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4456.809,
    "end": 4460.83,
    "text": "And now they get to the constrained Forney-style factor graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4462.111,
    "end": 4468.273,
    "text": "As we heard in part one, an FFG alone does not unambiguously define a constrained VFE objective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4470.1,
    "end": 4470.82,
    "text": " Interesting question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4471.201,
    "end": 4475.303,
    "text": "What is needed for that unambiguous identification of the VFE?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4476.163,
    "end": 4479.905,
    "text": "And so here they review their CFFG notation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4480.785,
    "end": 4482.286,
    "text": "Here's figure one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4483.267,
    "end": 4497.394,
    "text": "And we can again see that they go from the initial factor graph on the top left with an explicit CFFG on the top right,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4498.817,
    "end": 4502.398,
    "text": " Then they compress it down to the bottom left.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4504.959,
    "end": 4513.182,
    "text": "And then that uniquely identifies a logistical or an operational schedule.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4517.239,
    "end": 4520.262,
    "text": " Section 3, review of active inference by variational message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4520.802,
    "end": 4524.425,
    "text": "In this section, we work towards a message passing formulation of synthetic active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4525.166,
    "end": 4531.411,
    "text": "We start by reviewing active inference and the CFFG representation for a GFE objective for control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4532.092,
    "end": 4535.054,
    "text": "So 3.1, they define active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4535.074,
    "end": 4539.638,
    "text": "3.2, they define generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4542.64,
    "end": 4544.242,
    "text": "Now they go to message passing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4545.839,
    "end": 4552.781,
    "text": " In this section, we formulate a synthetic active inference as a message passing procedure on a model of past and future states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4553.081,
    "end": 4560.823,
    "text": "Inference on a model of past states relates to perception and learning, while inference on a model of future states relates to control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4563.104,
    "end": 4566.885,
    "text": "What is control, if not just perception and learning we haven't had yet?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4568.68,
    "end": 4569.14,
    "text": " Figure 3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4569.821,
    "end": 4578.808,
    "text": "Constraint 40 style factor graph representations for variational objectives on models for past, left, and future states right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4581.53,
    "end": 4587.855,
    "text": "I don't know if one has to cross their eyes or do some other magic visual experience here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4588.556,
    "end": 4593.48,
    "text": "There are some small differences, like here there's a dashed box that's not here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4594.04,
    "end": 4597.944,
    "text": "There's a data constraint clamp dangling edge here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4599.161,
    "end": 4602.742,
    "text": " whereas that constraint is open here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4604.403,
    "end": 4625.452,
    "text": "However, the rest of the figure looks pretty similar, and that tantalizingly points to some similarities between learning and memory in the past where we can clamp down part of the model to be data",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4627.009,
    "end": 4644.22,
    "text": " and consideration of the future which is to say action and control where the observations are distributional beyond the dirac and they haven't happened yet and we have agency over them happening any thoughts on that either of you",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4650.325,
    "end": 4657.569,
    "text": " It's kind of cool, and it's kind of forsaged in some of the other graphical models, but we'll ask the authors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4657.809,
    "end": 4664.453,
    "text": "331 talks about past states, and then 332 goes into the model of future states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4665.634,
    "end": 4670.057,
    "text": "So, a lot to read in the paper, but that key idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4670.477,
    "end": 4677.321,
    "text": "Because future outcomes are by definition unobserved, we include goal priors on the future observation variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4677.961,
    "end": 4678.942,
    "text": "Those are preferences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4679.977,
    "end": 4696.24,
    "text": " We expect there to be observations, and we know that they will become constraints to a Dirac-like form, but in anticipation, we take a distributional approach to observations that haven't happened.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4696.88,
    "end": 4706.982,
    "text": "Now, what if we had uncertainties about the past, and we were doing kind of a policy-like search in our fuzzy memory?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4708.843,
    "end": 4709.823,
    "text": "Then where would time be?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4713.522,
    "end": 4739.555,
    "text": " section four they get to general gfe based message updates in the model for future states the goal prior and observation model impose simultaneous constraints on the observation variable in the corresponding cffg this configuration is modeled by two facing nodes so i i believe those nodes might be the two in the dash box tier",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4742.352,
    "end": 4745.994,
    "text": " In this section, we derive the general GFE-based message updates for pair-facing notes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4746.414,
    "end": 4749.215,
    "text": "We express the local optimization problem as Lagrangian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4750.076,
    "end": 4758.52,
    "text": "Using variational calculus, we then derive local stationary solutions from which we obtain general update expressions for GFE-based messages.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4760.101,
    "end": 4764.043,
    "text": "So they describe the goal and the observation model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4767.584,
    "end": 4768.825,
    "text": "4.2, local Lagrangian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4770.367,
    "end": 4780.035,
    "text": " after the substitution of the factorization and applying P substitution to the local variational free energy objective, they get the local GFE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4781.576,
    "end": 4791.765,
    "text": "The GFE is all set up to construct this Lagrangian and to do a variational optimization on that Lagrangian to get Q star.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4796.028,
    "end": 4797.79,
    "text": "4.3, local stationary solutions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4798.663,
    "end": 4802.085,
    "text": " We are now prepared to derive the stationary points of the node-local Lagrangian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4802.825,
    "end": 4809.408,
    "text": "We start by considering the node-local Lagrangian as a functional of the variational factor q sub x. Lemma 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4811.629,
    "end": 4818.492,
    "text": "Stationary points of L as a functional of q. What does lemma 1 do or show?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4820.733,
    "end": 4821.253,
    "text": "Lemma 2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4823.174,
    "end": 4826.716,
    "text": "We derive stationary points of equation 4 as a functional of q sub z.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4828.41,
    "end": 4831.192,
    "text": " Note that by symmetry, a similar result applies to Q sub theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4832.253,
    "end": 4833.854,
    "text": "What does lemma 2 do or show?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4839.758,
    "end": 4847.463,
    "text": "In this section, we show that the stationary solutions of section 4.3 correspond to the fixed points of a fixed point iteration scheme.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4848.543,
    "end": 4849.884,
    "text": "Theorem 1, theorem 2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4850.725,
    "end": 4852.626,
    "text": "What do these theorems do and show?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4856.289,
    "end": 4858.07,
    "text": "4.5, convergence considerations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4860.191,
    "end": 4887.823,
    "text": " some further considerations and a corollary one what does it do or show now we get to five application to a discrete variable model in this section we apply the general message update rules of section 4-4 to a specific discrete variable model that is often used in aif practice using the general results we derive messages on this specific model and so here we see figure five with",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4889.798,
    "end": 4896.601,
    "text": " It's been taken all the way to the point of the logistics of the messages for those two facing nodes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4898.721,
    "end": 4901.202,
    "text": "Five three, data-constrained message updates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4901.862,
    "end": 4917.888,
    "text": "The message updates for a data-constrained VFE objective, figure six on the left, reduced to standard variational message passing updates as derived by citation 28 and appendix A. So here we have those two nodes with a DROC",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4919.608,
    "end": 4920.148,
    "text": " intervening.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4921.329,
    "end": 4924.45,
    "text": "Here we had X intervening in figure five.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4925.691,
    "end": 4928.332,
    "text": "So we had a distributional bandwidth.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4929.733,
    "end": 4938.657,
    "text": "Now we're dealing with a Dirac distributional bandwidth or channel, which is to say a data point being passed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4942.419,
    "end": 4944.34,
    "text": "Now we get to section six, perception action cycle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4944.54,
    "end": 4948.602,
    "text": "In this section, we formulate a perception action cycle that extends upon the GFE formulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4949.864,
    "end": 4954.306,
    "text": " Specifically, we illustrate how CFFG notation allows us to be explicit about local constraints.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4954.966,
    "end": 4960.649,
    "text": "As a result, the perception action cycle can now be visualized as a process that modifies constraints over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4961.309,
    "end": 4961.829,
    "text": "Pretty cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4963.71,
    "end": 4971.313,
    "text": "At the initial time, t equals one, no observations are available, and we initialize the perception action cycle with the CFFG of figure seven on the top.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4972.114,
    "end": 4973.334,
    "text": "We got a sneak peek earlier.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4973.735,
    "end": 4974.695,
    "text": "Now it's real figure seven.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4976.514,
    "end": 4984.479,
    "text": " As actions are executed and observations become available, data constraints replace the P substitution on the observation variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4985.6,
    "end": 4993.344,
    "text": "So it's almost like the P substitution prepared us to flip it out with data in some way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4994.385,
    "end": 5004.331,
    "text": "Like data constraints are a secondary or a further constraint that is enabled through the P substitution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5005.711,
    "end": 5015.594,
    "text": " When the time horizon is reached and all observations are available on the bottom, data constraints replace the p-substitution on the observation variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5016.574,
    "end": 5022.736,
    "text": "So it's kind of like we'll know when we'll know, and then it'll all be inferenced like memory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5023.597,
    "end": 5028.598,
    "text": "But before we know what we later found out, it's more like control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5029.899,
    "end": 5033.16,
    "text": "So whether we're in a retrospective memory setting,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5034.119,
    "end": 5052.069,
    "text": " a real-time sense-making setting or a prospective control-theoretic setting, it would be pretty cool to have like a mega-unified imperative and a tractable local procedure to do it, which in a sense is what they do.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5053.17,
    "end": 5062.716,
    "text": "So the perception action cycle with time-dependent constraints thus unifies the tasks of perception, control, and learning under a single generative model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5063.986,
    "end": 5065.006,
    "text": " and schedule.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5065.906,
    "end": 5067.587,
    "text": "This is the huge piece.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5068.267,
    "end": 5086.811,
    "text": "It's only through the FFG and then the constrained FFG can we get to not just making a base graph of perception, cognition, and action, cool enough, but bringing in an explicit node-local scheduling and logistics approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5089.491,
    "end": 5089.731,
    "text": "Wow.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5091.051,
    "end": 5093.492,
    "text": "The experimental protocol is summarized in algorithm one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5095.339,
    "end": 5100.348,
    "text": " All it requires is a generative model and a variational distribution with associated constraints.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5102.611,
    "end": 5103.773,
    "text": "And then it's kind of like a dance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5104.134,
    "end": 5107.039,
    "text": "Do, infer, act, execute, observe, slide.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5109.912,
    "end": 5112.734,
    "text": " We then get to section 7 with the experimental setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5113.494,
    "end": 5119.298,
    "text": "In this section, we describe a team-based task that serves as a classical setting for investigating epistemic behavior, citation 10.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5120.178,
    "end": 5122.039,
    "text": "The setup closely follows the definition in 30.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5122.9,
    "end": 5128.323,
    "text": "Citation 10 is Friston et al., 2015, Active Inference and Epistemic Value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5129.203,
    "end": 5134.607,
    "text": "And citation 30 is from Fis Vandelaar et al., same authors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5135.586,
    "end": 5139.147,
    "text": " in 2022, active inference and epistemic value in graphical models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5139.947,
    "end": 5146.009,
    "text": "So here we have the T maze that was shown earlier with a smiley face.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5146.669,
    "end": 5147.569,
    "text": "No smiley face here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5148.629,
    "end": 5154.891,
    "text": "And it's laid out and labeled starting position O, Q, C. That's where the epistemic value is.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5155.351,
    "end": 5157.231,
    "text": "And then L and R, the two reward arms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5157.812,
    "end": 5161.873,
    "text": "And just for those seeking continuity, here's figure 7.4.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5164.13,
    "end": 5164.671,
    "text": " from the 2022 textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5164.711,
    "end": 5168.298,
    "text": "Here we have the 2022 active inference mouse in a TMAs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5173.681,
    "end": 5184.83,
    "text": " And, from 2021, here's our active InferAnts simulation, where it's actually appropriate, the nestmate is, like, much smaller.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5185.13,
    "end": 5195.398,
    "text": "So, for the nestmate, they can wander around a lot inside the teammates that, for the mouse, it only has one location it can be in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5195.998,
    "end": 5196.118,
    "text": "So...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5197.119,
    "end": 5206.546,
    "text": " It's not as closely allied as 7.4 is with figure 8 here, but it's like the same... You could do it in the same lap.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5209.928,
    "end": 5217.013,
    "text": "They further specify the teammates and the probabilities of different things happening.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5219.015,
    "end": 5226.18,
    "text": "They further specify the teammates and the probabilities of things happening, and then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5227.208,
    "end": 5233.47,
    "text": " In Figure 9, they get to a CFFG describing the TMAs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5235.031,
    "end": 5238.432,
    "text": "Let's go through it with the authors and annotate what it means.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5241.974,
    "end": 5249.317,
    "text": "We execute the same experimental protocol as before and plot the minimal free energies in Figure 10, top right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5250.817,
    "end": 5255.099,
    "text": "The BFE-based reference agent fails to identify epistemic modes of behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5257.011,
    "end": 5264.514,
    "text": " The specific choice of prior for the observation matrix prevents any extrinsic information, at least initially, from influencing policy selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5265.975,
    "end": 5275.559,
    "text": "By the lack of an epistemic drive, the BFE-based agent sticks to policies that confirm its prior belief without exploring possibilities to exploit available information in the TMA's environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5278.74,
    "end": 5284.843,
    "text": "A histogram of the number of wins per run is plotted in Figure 11 on the left.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5288.459,
    "end": 5293.488,
    "text": " The histogram suggests a bimodal distribution with a large mass group to the right and a smaller mass in the middle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5294.409,
    "end": 5299.658,
    "text": "For reference, dashed curves indicate ideal performance for agents that already know A from the start.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5300.475,
    "end": 5305.277,
    "text": " For agents that first must learn A, deviations from ideal performance are expected.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5305.777,
    "end": 5313.66,
    "text": "The smaller middle mass then indicates that GFE optimization offers no silver bullet for simulating fully successful epistemic agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5314.321,
    "end": 5319.783,
    "text": "Namely, for some choices of initialization, the GFE agent may still become stuck in local optima.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5324.716,
    "end": 5325.316,
    "text": " Section 9.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5325.836,
    "end": 5326.917,
    "text": "I won't read it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5327.397,
    "end": 5340.522,
    "text": "They describe a wide swath of related work, ranging from early work on the introduction of fornigraphs on through expected free energy and active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5343.163,
    "end": 5366.282,
    "text": " and in section 10 they conclude they took a constraint-centric approach to synthetic active inference they simulated a perception action cycle through message passing derived from a single generalized free energy objective and dot dot in this paper we have adopted a purely engineering point of view and we have not concerned ourselves with biological plausibility",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5367.256,
    "end": 5372.077,
    "text": " Specifically, the derived message updates come with considerations about stability and non-standard expressions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5372.917,
    "end": 5380.939,
    "text": "Although we have engineered solutions to overcome these complications, it seems unlikely to us that the brain resorts to such strategies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5382.98,
    "end": 5387.141,
    "text": "Interesting mic drop, but very humble.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5390.722,
    "end": 5395.343,
    "text": "And so now, as we give our last thoughts...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5397.512,
    "end": 5402.333,
    "text": " We've always held up the side-by-side at the Institute with a minimum of two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5402.353,
    "end": 5409.595,
    "text": "And we've talked about that before in the textbook figure 4.3 setting with continuous and discrete time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5410.355,
    "end": 5415.417,
    "text": "But there have been many other Min 2 experiences that we've all shared together.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5416.437,
    "end": 5422.639,
    "text": "And not just with this dyad of papers, but with this new intelligibility",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5423.692,
    "end": 5434.759,
    "text": " of the relationship between Bayes graphs and different styles and constrained forms of Forney graphs were in yet another min-2 setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5435.479,
    "end": 5441.943,
    "text": "So that's what I would like to say in closing as we conclude the .0.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 5453.15,
    "end": 5456.592,
    "text": " Yeah, I look forward to the point one and point two.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5457.732,
    "end": 5479.983,
    "text": "I have a bunch of questions about, for example, what piece substitution really means and for example, the time window, how many time steps you need to give your model already or whether it can unfold automatically and just in general to hear what they have to say because it is a very interesting topic.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5485.259,
    "end": 5503.126,
    "text": " yeah um same here also interested to um hear what the authors have just have to say and um i think yeah whenever there's um um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5505.071,
    "end": 5526.786,
    "text": " Whenever there's a generalization that, at least in the semantic space, affords a wider connection to different methods, and especially in the context of models represented on the graph, I'd be interested to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5529.676,
    "end": 5545.181,
    "text": " learn more about how we can represent agents in different kinds of graphs or how these constrained Forney style factor graphs afford different types of optimization methods to be performed on them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5548.135,
    "end": 5562.547,
    "text": " and how this work can help Active Inference to interface with other domains which try to solve similar problems or perhaps different problems from a different angle.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5566.11,
    "end": 5566.45,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5567.051,
    "end": 5571.895,
    "text": "My last thought is like, we planned to learn",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5573.317,
    "end": 5574.878,
    "text": " at least to try here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5575.638,
    "end": 5579.079,
    "text": "So I commend you both for that policy selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5579.979,
    "end": 5584.721,
    "text": "And now looking back, it's like data or a memory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5584.981,
    "end": 5588.402,
    "text": "And then in science, it's like, oh, it's 1994.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5589.143,
    "end": 5593.564,
    "text": "This new genome is available, or like this new thing is exciting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5593.604,
    "end": 5594.725,
    "text": "And here's the next step.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5594.885,
    "end": 5595.505,
    "text": "And it's like,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5598.374,
    "end": 5607.57,
    "text": " one must be as excited as can be in making the right measured decision then which later becomes an observation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5608.809,
    "end": 5611.451,
    "text": " at a different time point, yeah, and a different perspective.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5612.451,
    "end": 5631.183,
    "text": "So there are probably many exciting and important syntheses that this work is going to build our skills and our expressivity around, and the computational element is just excellent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5632.644,
    "end": 5635.566,
    "text": "So thank you again, Bert and Jakob.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5636.326,
    "end": 5637.727,
    "text": "See you fellows in the dot one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5641.73,
    "end": 5642.085,
    "text": " All right.",
    "speaker": "SPEAKER_04"
  }
]
UNKNOWN:
Thank you.


SPEAKER_08:
Hello and welcome to the Active Inference Livestream.

This is the Active Inference Lab and we are here in Active Inference Livestream 18.2 on March 30th, 2021.

Welcome everyone to the Active Inference Lab.

This is a recorded and an archived livestream, so please provide us with feedback so that we can improve on our work.

Sorry, the video just changed a little bit.

There we go.

All backgrounds and perspectives are welcome here and we'll all be following good video etiquette for live streams.

Today we're here in session 18.2 on March 30th having the follow-up discussion with the authors and other participants on this Predictive Global Neuronal Workspace paper.

And today the goal is really to follow up on the last week's discussion, continue discussing and learning about this very cool paper and

why the authors did what they did and what are the implications for different areas so today we're just going to go through some introductions and warm-up questions and then we'll be able to walk through the paper and the slides and ask a few questions to the authors and get a few different perspectives

So for the introductions, we'll just go around and introduce ourselves and then pass it to somebody who hasn't spoken yet.

I'm Daniel.

I'm a postdoc in California, and I'll pass it to Alex.


SPEAKER_00:
Thanks.

Hi, everyone.

I'm Alex Vyatkin.

I'm a researcher in Systems Management School in Moscow, Russia.

Also, I'm a co-organizer for Active Inference Lab.

And I pass it to Stephen.


SPEAKER_01:
Hello, I'm Stephen.

I'm based in Toronto.

And I do a lot of work with social topographies and landscapes for social drama.

And I will pass it over to Dean.


SPEAKER_02:
Hi, I'm Dean.

I am retired.

I'm hanging out in the loft of my cabin, which is not very well lit.

I'll pass it to Ryan.


SPEAKER_04:
Yeah.

So I'm Ryan Smith.

I'm a investigator at the Laureate Institute for Brain Research in Tulsa, Oklahoma.

Yeah, I mainly work on computational modeling of

empirical behavior in different psychiatric disorders, as well as some theoretical work on emotions and emotional awareness and interception.

I guess I'll send it to Chris.


SPEAKER_07:
Hi, I'm Christopher White.

I'm a PhD student at the University of Cambridge in England, and I mostly work on active inference or computation using active inference with computational models of cognitive control.

But a lot of my previous work is kind of focused on visual awareness.

So I'll pass it to Adam.


SPEAKER_06:
Hi, I'm Adam.

I'm a postdoc at

Johns Hopkins School of Medicine Center for Psychedelic and Consciousness Research.

I'm also a research fellow at the Kinsey Institute, and I'm interested in consciousness.

If you will, too.


SPEAKER_08:
Is that everyone?

Cool.

Does anyone want to raise their hand and give a thought on one of these warm-up questions, which are just what are something they're excited or curious about today?

something that they liked or remembered about the paper or last week's discussion to sort of jump start where we we were at um or maybe really quickly at the beginning christopher wanted to clear up something and then we'll kind of go into questions so maybe everyone could write down a question and then christopher go for it yeah show us i said two things that like


SPEAKER_07:
just false um last time why i just made mistakes i kind of want to correct them so the first one i was specifically referenced someone asked a question about the thalamus and its involvement in consciousness and i kind of referenced a whole body of work from matthew larkham's group um and i think i said the medial dorsal nucleus and it's not it's central lateral nucleus and thalamus um i think like 99 people wouldn't care but anyone who knows anything about the thalamus would be like pained by that um

And the second thing was I was talking about King and DeHaan's Bayesian model of conscious access.

I think I described the model as univariate Gaussian.

That's also not correct.

It's a multivariate Gaussian, although you can extend the same model to a univariate case.

Anyway, it doesn't matter.

But I just wanted to kind of correct those two things.


SPEAKER_08:
Thanks for the correction.

So if everyone could raise their hand and first, um, we'll go with Adam and then Steven.


SPEAKER_06:
Um, I was, uh, curious about, uh, what Chris just said.

So like medial dorsal, that would be, um, what Thelma is for interior cingulate in the frontal lobes.

Kind of like the gateway to, um, like amygdala at all.

Would that be right?

And then what, so the central lateral, like who's that looping with?

I forget.


SPEAKER_07:
I think central lateral would be looping.

I was specifically thinking about like a loop between layer five pyramidal neurons and central lateral thalamus.


SPEAKER_08:
So if I could ask a question there, just why are we identifying brain regions or what would it mean to be identifying brain regions one way or the other in humans or some other species?


SPEAKER_07:
So this particular body of work, not to go too far on a tangent, but there's this really impressive body of work that basically has identified both level of consciousness and content of consciousness with burst mode firing in layer five of cortex.

I think most of this evidence is from a mouse model, so interpret that as you will.


SPEAKER_08:
Yeah, Adam, or you can use the actual Jitsi.

Raise your hand.


SPEAKER_06:
But yeah, go on.

I mean, for me, it would partially be what systems might contribute to wakefulness or different aspects of consciousness, depending on how we want to carve the joints on it in different respects.

So let's say...

stimulating the medial dorsal was more important.

Well, then that might implicate the specific relays, its ability to form phalamocortical loops or phalamostriatocortical, like the whole what it helps to bind together functionally and other functions.

It would tell you what processes might be involved.

So maybe action or salience, that might be important.

Or maybe it's like

frontal lobes as hub, but like which part of the frontal lobes?

Is it the parts that are like a general topologically central hub?

Or does it look kind of like a motor bits?

Or does it look like something else?

And so it'd be trying to look for like fish around for clues about what's important in what ways, but it might mean, but it's easy to tell stories.


SPEAKER_08:
Okay, cool.

So Steven, and then anyone else who has a question?


SPEAKER_01:
So yeah, I'm just really excited about the... When I listened last week, you talked about temperature being used in the models, and you had different places where you used different amounts of temperature.

I think it's in the SoftMac function to show...

different types of model scenarios so i'll just be curious i'm just excited about maybe um just the general way that you um use your thinking to build the models to inform how you start to look at consciousness and the way the brain works as much as the way that you find the results from the models so that i'd be interested in that those sort of two sides


SPEAKER_04:
So, so I mean, one, one thing I just want to make sure this is clear is that, you know, like computational neuroscience borrows or has been historically ended up borrowing a lot from terminology and physics and engineering and things like that.

You know, so there's some of these sorts of terms that can be a little funny, right?

So like temperature, like a temperature parameter or typically an inverse temperature parameter is something that's borrowed from physics that just kind of has to do within the physics sense, kind of how much, you know, with how much energy particles are kind of bouncing around against each other.

Um, and, um, you know, in a very oversimplified way of putting it, um, but, but so it just means essentially there's more kind of randomness right in the, in the system.

So in the context of, um, of this, um, you're just applying that same equation to describe essentially the amount of noise in decision-making.

So, uh, uh, higher temperature in this case, just is just a parameter in an equation that.

controls how deterministic versus stochastic decision-making processes are.

So I just want to make sure that it has... I just want to make sure it's clear to anyone listening that it has nothing to do with actual physical temperature, like being hot or cold.

It's just a parameter that kind of modulates how random decision-making is.

So I just wanted to make that clear.

So in this case, it's just controlling... So for example, if

if the temperature parameter is really low or the inverse temperature has a really high value, then that just means that say, even if the model was only like 51% versus 49% confident in one thing over another, it would always choose the action associated with the 51%.

Um, whereas if the temperature value temperature parameter value was high or as low, um, or as high, or the inverse temperature parameter is low.

then um that means that the like for instance that with 51 of the time it would choose the um the action associated with 51 proper with 0.51 probability it would choose that one less often than it would if the probability was a 0.7 um you know and less probability than that than if it was if the probability was 0.9

So essentially just having a low inverse temperature parameter value just allows the actual frequency of choice to kind of scale in some sense with the actual probabilistic beliefs over the different states in the model.


SPEAKER_08:
Thank you, Ryan.

We'll have Christopher, then Adam, and then anyone else who raises their hand.


SPEAKER_07:
So just to kind of contextualize that a little bit.

So a softmax function is exactly the same function as a Boltzmann distribution from statistical physics.

So often if you have a, you'll just use the same terminology because it has, and in the physics case, it has like a literal physical interpretation.

In this case, it doesn't.

So that's kind of why there's that overlapping terminology.

In terms of deciding why or on those parameters or actually building them in, it was really just kind of came from thinking about what attention does.

So what we're trying to modulate there was attention and signal strength.

And both of those things

in some way modulate the strength of feed forward input.

So when you think about that, you then kind of think, okay, what part of the model corresponds to that?

Well, it's the likelihood matrix that maps, that decides the precision of the mapping between external observations and first level hidden states.

And so both the idea is that both

attention and kind of external signal strength, or it might be something like contrast, whatever, will jointly determine the precision of that mapping.

And then there's lots of different ways where you can kind of cash that out if you actually want to get into kind of the neurobiology of it, like the normalization model of attention or bias competition or something like that.

But kind of extracting away from that detail,

Basically, it just modulates the A matrix, the computational level of analysis.

So that was kind of the thinking that went into it.


SPEAKER_08:
Very interesting.

Adam, then Stephen, then anyone else.

Okay, Adam, can you be ready to speak?

But go for it.


SPEAKER_06:
Yeah, sorry about that.

Oh, it's kind of off the current topic, but from before.

So some quick Googling turned out the central lateral seems to be looping largely with parietal and temporal association areas.

So that's potentially interesting in ways that indirectly could like speak to debates on the physical substrates of consciousness, like the brouhaha between IIT and GMWT said more like the front of the brain or the back of the brain.

And in which senses do they mean consciousness?

But it's interesting.

I actually am having trouble finding an image of like a diagram of the thalamus actually shows the central lateral nucleus.

It's all just like, I see central medial, but not central lateral.

It seems like they might be kind of like tucking it in with the pulvinar, whatever that means.

Stephen?


SPEAKER_01:
Yeah, I just say thanks for that feedback.

That was really useful.

And I think also, because I've got a background in chemistry, I keep thinking about the Gibbs free energy where it talks about temperature times change in entropy in that term.

So it's like that the temperature is multiplied into the amount of entropy change there is to make it more significant.

So that would...

kind of mirror why it's a bit like you're putting in noise or entropy.

So that's quite useful.

So yeah, thanks.


SPEAKER_08:
Yeah, welcome, Blue.

You or anyone else can raise their hand, but one chemistry metaphor.

And again, Ryan, as you brought up, it's not always like a literal meaning, but thermodynamic versus the kinetic alternative for a reaction.

So like the thermodynamic...

outcome for the candle is to burn because it has all this pent up energy but it doesn't spontaneously combust because the kinetic barrier the activation barrier is too high and then when there's like an enzyme or a spark of energy then in the right situation the product can go all the way down

the big delta G to the kind of ultimate thermodynamic least energy versus the more kinetically accessible.

And so that's kind of like information foraging.

There's sort of the perfect citation you would find if you had the right surge and it was that low temperature or something analogous.

But then if it's too kinetically constrained, then the attention is drawn away.

I'm not sure if that will map directly in this whole active inference scenario.

mode, but it's just kind of another chemical informational distinction.


SPEAKER_04:
Well, I mean, it depends.

I mean, it depends how you're using it.

Like in this, in this case, there's not really much of a connection because we're just essentially, we're just using a temperature parameter and a softmax function to control the precision of a mapping.

Well, in a few different places for attention and for stimulus strength, we're just.

using it to change the precision of the mapping between states and observations at the first level.

So it just controls essentially how strong a sensory signal ends up updating beliefs, posterior reserve states at the first level, and then modulated by a second temperature parameter that does the same thing, but that accounts for attention.

So these sorts of things jointly control

essentially how much evidence sensory input provides for posterior giver states at the first level.

You know, so we're using it for that kind of thing.

And then we also use it for controlling deterministicness or choice at the second level of the model in terms of action selection.

So in all those cases, I don't personally see a direct relationship between that and the kind of thermodynamic

thermodynamic aspect of temperature of getting over, you know, energy barriers and things like that.

But I mean, I could be, I could be, I could be just not thinking through it all the way.


SPEAKER_08:
Stephen, then anyone else?


SPEAKER_01:
Yeah, I think that makes a lot of sense, what you're saying.

And I suppose the nice thing with the modeling is that you've got this potential to, I know you're using it, like you say, as this way to bring in noise or softmax, but it gives you that way to increase and decrease it, which in real life, when you do empirical work, it's hard to do that.

So it's one of the strengths of the modeling that you can change that parameter and see, it seems to be one way that you really get insights into the dynamics of the system.


SPEAKER_04:
Yeah, I mean, in terms of just being able to modulate, or mimic the influences of attention on modulating the background perception of the signal.


SPEAKER_08:
Yes, Steven, or Dean, and then Steven.


SPEAKER_02:
Yeah, I just had a quick question here.

And maybe this is far too oversimplified, but when you guys were doing this work, I started also looking at the,

the effect paper that I think the two together gave me a better sense of some of the work that I was doing around helping people walk into these novel situations pre-reflectively.

And there is a tendency to want to commit, to want to pay attention, to want to give over whatever you have in terms of processing power to figuring out what's going on.

But we had to spend quite a bit of time on helping people

get the idea that there's this channel there's this kind of sweet spot and when you were talking about degrees it brought it up for me again the idea of how to help a person know when they're over committing when they're over correcting and when they're over compensating so when they're trying to do this estimation how do they know that they're not seeing the moon walking bare because they're so committed to counting up the number of

basketball passes.

I wonder if you guys had spent a little bit of time looking at the other end of the of the channel when people actually readjust back in.

And I know that's that's probably where the degrees part comes in.

But I was just wondering about that.


SPEAKER_04:
So I mean, are you imagining are you imagining in some kind of like clinical scenario where a person like biased attention away from something that could be, say, aversive if they were to focus on it?


SPEAKER_02:
Yeah, exactly.


SPEAKER_04:
Yeah, so I mean, there's probably a mechanistic relation in a sense, just in the broad sense of how we think attention is implemented, right?

So for example, if you think that a person is avoiding paying attention to something because it would be aversive,

then you can end up with this kind of implicit reinforcement of attentional policy selection, which we don't directly have in this model.

The agent is choosing what the precision is associated with attention in this model, but that's something that can be added and it's something that we have actually added in some more recent work that we're doing right now.

But the idea there, or one way to think about that would be that whenever a person essentially starts

to pay attention to the thing that would be aversive, then it starts to feel aversive, right?

So then if they pay attention away again, then the choice of paying attention away actually ends up being negatively reinforcing because the bad feeling goes away, right?

So then what that ends up doing, at least according to this kind of reinforcement learning account, is that that ends up assigning higher value to the policy of attending away.

Over time, the probability of attending away just ends up getting higher and higher because it makes you feel better every time you do.

Then this avoidant attention can become really habitized.

That's one way of thinking about it.

In the clinical setting, you want to help people potentially start to

you know, gradually, you know, expose themselves to, to, you know, pay attention to this thing that's aversive in hopes that if they're able to kind of do that in a sustained way, then over the long run, the aversiveness will go down as they kind of learn to better deal with the thing that they've been avoiding attentionally.

You know, so, so that's, that's, you know, fairly tangential to, right, the kind of thing we're trying to do here.

But, you know, but there's a,

there's a connection in this very kind of broad sense in that if you can control attention and choices of what to pay attention to can be reinforced, then that's one way you can kind of end up with this, this kind of thing where a person might not even know, right.

That they've developed this strong habit, you know, not to pay attention to something.

It just becomes very automated.

Um, you know, the same way, the same way any other behavior can be habitized via repeated reinforcement.

Um,

but here just it would be a kind of a cognitive action as opposed to a more overt behavioral action.

Okay.


SPEAKER_08:
Very interesting.

Thanks for that question and response.

Adam, and then anyone else who raises their hand.


SPEAKER_06:
I guess to come out of left field, in what you were just describing, it seems like that could have some relation to maybe

grounding like psychodynamic like concepts in terms of if these patterns of attentional approach and avoidance can move from your C matrix to your E matrix you can end up finding yourself in these sort of like garden paths of attending and coming to inferences for reasons that you might not even understand just on the way your policy selections of attention have been shaped for conditions that are might be too numerous and varied for you to track and so


SPEAKER_04:
Yeah.

Yeah.

I mean, there's, I mean, we've written, you know, we've written a few papers on this, right.

Like, or I have with some colleagues, I mean, again, you know, my focus, um, you know, is, is largely in, you know, in terms of emotion and psychiatric conditions.

Right.

And I wouldn't, um, I would say it's a way of, of, um, recasting the sorts of behaviors that psychodynamic approaches are trying to capture.

but to do it in a way that doesn't actually posit the existence of these sorts of repressive, right?

Like, you know, like unconscious, like there's no unconscious agent, right?

There's no id, right?

That's being posited, that's sitting there keeping these things out of awareness, but somehow they're still present unconsciously, right?

There's nothing like that, right?

It's just a very simple story of having reinforced patterns of attention that, you know, so it,

It's a way of capturing the phenomena that I think psychodynamic approaches are often trying to capture, but it doesn't posit a lot of the ontological, a lot of the entities that are posited in psychodynamic approaches.


SPEAKER_08:
Cool.

Stephen, then anyone else?

Adam, go for it.

Yeah.


SPEAKER_06:
Not saying you're Freudian, but you're actually finding the baby in the bathwater, rescuing it, cleaning it up, contextualizing it.

Why did people have these intuitions to begin with?

What was the grains of truth?

I'm wondering if through this, this could lead to this kind of patterns of attending, some of which you're not tracking.

This could be an account of what would lead to potentially mismatch or discrepancies between things like stated and revealed preferences.

or a thing like your actual affective disposition with respect to thing and then what you think is going on and then those could potentially go across purposes in ways that could be difficult to explicitly model and potentially have like problematic sequelae or not.

But I'm wondering if like this could lead to like a mismatch or across purposes of attracting states at different levels.


SPEAKER_04:
Um, I mean, I'd probably need more concrete examples to try to figure that out.

I mean, that's probably something I'd have to think about more.


SPEAKER_06:
I guess one quickly would be like, um, implicit affect tests or something like that.

Like you say, like, um, uh, I'm not racist, but then it turns out, no, no, you are.

And like, if you actually look at the reaction or something like that, or I am really happy in this relationship, but then like I measure your heart rate variability and it's crap or things like that.

Um,

Just this, your actual affective state might end up being different than what you think it is.

And there can be like a lack of self-awareness, like a difficulty coming to coherent modeling of yourself because of these like shaped paths of attention.

Like you might like miss the mark about you.

Cause it was like difficult to see something like that.


SPEAKER_04:
I mean, I would, I would think that, you know, I mean,

perhaps there are sort of some instances of the sort of thing you're talking about that could be captured by the kind of mechanisms that, you know, we're talking about here or the kind of tangential extension of them that we're talking about here.

But I'm not confident that all of them could.

I imagine there's a number of mechanisms that you might appeal to to describe some of those things like, you know, implicit, yeah, like, I mean, if you're talking about like implicit associations on like an IAT,

right or something or something like that that's probably my intuition is that's probably different from you know it's more kind of like a like a like a right which i know chris has actually been working on models of true tasks um but um right which is which is probably pretty different from um um some of the other examples that you were giving but but yeah i mean like i said i'd have to

Taking a particular example, I'd probably have to think it through a little more to think about what kind of generative model would be able to reproduce the kind of behavior you're talking about.

Or most likely, there would be multiple generative models that could reproduce it, and then you'd have to do some kind of experiment to do model comparison and see which one actually best accounts for it.

But again, I wouldn't feel really confident saying what that might look like just in the very kind of

abstract general description we're talking about right now.


SPEAKER_08:
Nice.

Nice.

Thank you for the response.

Steven?


SPEAKER_01:
Yeah, just bringing this point with this model that's now maybe getting into the modeling part is I think that's interesting with the awareness base.

There's a lot of work with awareness-based systems change and looking at awareness.

And I'd be interested in your thoughts on how we can move beyond being trapped in a psychology of the individual

and you know pathology of the individual and it moves into the intersubjective multi-scale so like you talk about tasks but you it also could be used in taskscapes you know organizational contexts or groups of people so i just think that there's um because the the it's not nailed down to like a psychological model of the person's thoughts and feelings but more a general model of the attentional structure

And it has an ability to move between scales and context.

So I just think I've put that out there as something that seems quite interesting.


SPEAKER_04:
I mean, there's more to you.

Oh, go ahead, Scott.


SPEAKER_07:
So I think the utility of models like these is to identify a very specific phenomenon.

and that namely having conscious access to some content and not others um so i have conscious access to what's in front of me my visual field at the moment if i direct my attention around my body i can bring up all sorts of like i can suddenly become aware of different kind of smarter sensory um bits of information and all of these have very specific kind of neural signatures as it were

What we're trying to do is really propose a model at a computational level of analysis and algorithmic level of analysis that explains these.

And outside of that, I'm not sure to what extent actually talking about awareness, intersubjective awareness, is it all the same thing?

as talking about awareness as someone who works in neuroscience of consciousness, for example.

They might be, but I don't see any evidence that they are.


SPEAKER_08:
Pretty interesting, because it's in the title of the paper.

So many people might think of it differently.

I mean, it's just an interesting question, right?

Sorry, what's in the title of the paper?

Just the idea that the visual consciousness is involved.


SPEAKER_04:
Well, but I mean, but again, I mean, this is a model of a particular visual consciousness past, right?

I mean, so it's meant to account for things that are seen in the visual consciousness literature.

um you know there is the structure is abstract enough that you know we would and we say this in the discussion that it ought to apply to cases of having access to one bit of information versus another um in other domains right like in the auditory domain or you know something that i've been working on you know with a bunch of colleagues at um you know at library where i am um you know is trying to see if the same sort of thing

It looks like it's present in the interoceptive domain, for example.

So I think it's right to say that we would hope anyway that it's abstract enough that it can account for any piece of information that you either gain access to or not when it's being represented somewhere.

whether it's, you know, right now, as I'm talking to you, I'm not necessarily aware of what my heart's doing, but now I choose to pay attention to it.

And all of a sudden, okay, I can kind of feel that it's beating at a certain rate.

I believe, I believe, I believe that I, that I feel, I wasn't conscious of that before, despite the fact that my brain was unconsciously tracking, you know, what my heart was doing that whole time.

Right.

So it could apply to being aware or not aware of any stimulus.

So, I mean, to the, so only to the extent that when we say something about inner subjective awareness, only to the extent that that means something about a piece of information that.

know before i was attending to it i didn't know you know was there but now that i'm paying attention to it that i do so for example like if i happen to um notice that a person's smiling or something like that um even though they were smiling at me the whole time you know maybe my brain had picked up on the fact that they were smiling but um i wasn't aware of it until i focused my attention on it or something like that right or even maybe more abstract than that

know when my brain sort of had unconsciously detected the smile there might even have been some further inference that that means something about them being happy right so so there might even be a case where um i either do or do not gain access to this unconscious inference or this posterior belief that the person is more likely happy than unhappy or something like that right but uh but i mean these are sorts of cases they would have to be

tested in their own right.

My broad point is just that it's an abstract structure where all it is meant to account for is when your brain is representing something and you either become aware of it or not.


SPEAKER_07:
I mean, if I could just kind of build on that slightly.

So I think

One, it's very tempting when we've got computational models that are relative, when we've got lots of equations sitting in front of us to kind of think that, I think it's easy to get an illusion of rigor where there is none.

The thing that makes these models meaningful is that we can, we're tracking some real world phenomenon.

In terms of the abstract model structure, it may be the case that say you have group dynamics where there's some group, some aspect of that, maybe the individual behavior of individuals within a group evolves at a really quick time scale.

And then the group decision-making evolves at a slower time scale.

You may be able, you could, in principle, fit a two level partial observer Markov decision process to this as a model of that behaviour.

But that would be very, very difficult.

I think even in that case, even though we're fitting the same model, I don't think that those are at all the same phenomenon, although there may be similarities across them.

I would be extremely resistant to calling one visual awareness or that a group has visual awareness.


SPEAKER_08:
Interesting.

And of course, a parallel debate in the ant world.

Is there a colony awareness or a single six-legged ant awareness?

Or how should we think about distributed systems?

And aren't we all kind of just distributed systems all the way down?

So Adam, and then Steven.


SPEAKER_06:
So this would be related to, I guess, different types of interest subjective or

intrasubjective awareness that you might believe is a model for.

I recently uploaded a preprint after some conversations with people from the Brain Institute at Chapman University, basically trying to do the beginnings of an active inference account of LeBay phenomena and readiness potentials that are a little bit less deflationary with respect to volition and conscious causation.

And so in essence, I was

thinking of the actual moving your hand being an accumulation of model evidence with respect to appropriate receptive pose.

And it might be some sort of conjunction of also whether or not your will or feeling of urge was part of it.

You could think of that as inference over your affective state.

And that if these things are in what kind of alignments could help to explain whether you're like aware of readiness potential activity or whether this awareness is actually feeding back and contributing to it.

And so I feel like your model could potentially like provide like a very precise contextualization of this really subtle what's going on and I would like to collaborate.


SPEAKER_04:
So, I mean, just again, Brad, just to make it really,

maybe that could be the case.

I have no idea, but it could only potentially be the case if what you're talking about, right?

Like what you're, you know, motivated to do or what you're, you know, in the process of about, you know, being about to do or, you know, whatever aspect of, um, you know, action or action tendency or, you know, anything like that.

Um, the only way this could apply is if you think that that information about what your,

you know, what you're motivated to do or something like that, you know, whatever it is.

Only if it's the case that you think that is a represented piece of information that you can attend to or not.

And then you could sort of gate into the higher kind of deeper temporal level of the model that would inform verbal reporting and sort of other goal directed uses.

So it would only be the case, I guess my point is this, is if the representation associated with action that you're talking about is, in some sense, formally equivalent to a representation of something visual that you can attend to.


SPEAKER_06:
That makes sense.

So it's, so the preprint is like, it's like a three, I think like a three page, more of a promissory note.

But if I could like pick your brain later about like,

if you think there's like a way we could actually get some like empirical traction and like nailing down the specifics along the lines you're saying.


SPEAKER_04:
Well, if you have a, I mean, if you have a task and that task involves people paying attention to the aspect of action that you're talking about, then in principle, you could have people, you could have people, you know, in some sense cue them or prime them or something like that, that engages that sort of action bias or action motivation.


SPEAKER_06:
and again they can either attend to it or not or report on it or not in that case yes but otherwise probably not it's a little tricky I think in terms of like the action that would be attended to would be of a couple varieties but none of them few of them would be directly observable so it's like you could say like it's the action of the neural activity in terms of like the ramping activity like do they have access to that but the I was thinking it could also be like the action of like mental acts and like

rehearsing like imagining in a kind of sophisticated affective inference sense like am i going to move my hand or not what would that be like and then that being contributing to that signal um but and where like actual awareness of this like fictive meant this fictive action could be part of what would contribute to

whether or not you feel ownership of it.

It's interesting.

Some people don't show readiness potentials.

They just act.

Other people do.

Like schizophrenic patients, they tend not to show readiness potentials before they spontaneously raise their hand.

But it would be hard to get this... The action is internal.

And so I don't know what you can do with that.


SPEAKER_04:
Yeah, it just seems like you have to have some kind of behavioral readout that can be fit to...

to a model of this to be able to say that this explains the ability to consciously report on it or not, right?


SPEAKER_06:
So I guess could you use, like, right before leading to, like, the action, there tends to be, like, it starts out, like, more bilateral, and then the ramp activity tends to go lateral, and then the hand moves.

Could you use, like, that sort of signal as the readout?


SPEAKER_04:
Like the ensemble activity?

So in principle, you can fit models to neural activity.

I mean, you can fit models to anything that the generative model generates.

But I'm not necessarily all that confident that you'd be able to justify the claim that because the model in one case generates a readiness potential signal,

that that has anything to do with having conscious access to something.


SPEAKER_06:
And you have the other problem, like if you're using like the ensemble activity, it's still the interpretation, like awareness of what, what are we even talking about with the awareness of the whole point is to try to like contextualize what those signals mean.

And so, yeah.


SPEAKER_04:
Yeah.

I mean, the point is, I mean, in the majority of cases, you know, either directly or indirectly, you know, like verbal report, right.

It's still,

kind of gold standard for whether someone has access to something, right?

Even in no report paradigms, you're still later, right?

Asking them to report what they were experiencing at the time.

Um, um, so it's, I mean, in the, yeah, the issue is that in some sense, all kind of supportable or accepted sort of evidence for being consciously aware of something, um,

is, is always ultimately, and Chris, you can correct me if you think this is wrong, but, but I mean, I think that it always ultimately comes down to verbal report or some reliable correlate of verbal report, whether at the time or later.


SPEAKER_07:
I agree with that.

So I think I'm not an expert in no report paradigms, but to my knowledge, all of them, the reason we trust no report paradigms is because our measure, whether that is a particular mode of eye tracking or it is just like incidental memory afterwards, all of those things correlate really, really well with report.

And that's the thing that kind of like makes us trust them as measures of conscious access independent of report.

Yeah, I think it's always reporter foundational, unless we have really, really good evidence.

Otherwise, I think we have to take them kind of at face value.


SPEAKER_08:
Cool.

We'll return to the stack.

So Blue, and then Stephen, and then anyone else who raises their hand.


SPEAKER_03:
So Adam always brings me into this metaphysical thought space whenever he talks.

And I just was wondering, in terms of this visual model, you hear the expression that people see what they want to see.

And so have you thought about maybe testing

like there's the the idea of paying attention to something and so we thought about testing like whether you know people can deceive themselves in this visual process right like some self-deception or even like the you hear like the power of manifestation like can people bring into their awareness something because they simply want it to be there i mean have you thought about this or testing the model in this way yeah i mean if you go to a figure um maybe scroll down is it


SPEAKER_07:
which one so we have a we have a actually I'm not sure uh so that's the erps yeah that attacks the extended taxonomy so we start with this kind of four-way taxonomy between which to Han proposed um along with some other of his colleagues of the kind of factors underlying whether you'll have conscious access to a stimulus and you kind of broke it down into whether

attention was weak or absent, present or absent, or whether kind of this how strong the signal strength was weak or strong.

And then we extended this by adding in prior expectations.

So the extent to which you expect a stimulus to appear.

And there is now like one of the big motivations this work is that there is an enormous body of work now.

showing that expectation does play a fairly fundamental role, I think, in determining the contents of visual awareness.

So as for whether you can voluntarily bring something into awareness, the answer is I think that's more to do with attention, I suspect.

But say, for example, if you are looking at a bistable image,

you can attend to one part of that image and then have the, if it's a, I think it's called a NECA cube, you can have the cube flip versus if you attend to another part, it'll flip back.

And you can do kind of similar deliberate moves of attention with a lot of bi-stable images.

So also I think there's some really cool, there is some interesting kind of back to the,

of expectation thing and expecting seeing what you expect to see a lot of that depends upon how strong your bottom-up evidence is if you are much more likely so there are paradigms that kind of distract people but basically condition them into expecting a stimulus to appear um and

you can get kind of accidental hallucinate, or you can basically like induce people to blur what is essentially a hallucination in those cases, or mistakenly report something that we know as a fact, the experimentalists as experimentalists, they weren't there.

I think there are two examples that what I have in mind at the moment is a really clever set of studies by Jean Arou, but I know Ryan, and maybe you can talk about this.

There's also a lot of work on conditioned hallucinations from a group at Yale who I know you collaborate with.


SPEAKER_04:
yeah so yeah and that was that was yeah something i was actually going to bring up is and this is this is not yeah because i mean i agree with you chris in most cases i think it's going to have to do with some kind of like motivated selective attention um that kind of like biases say different possible uh interpretations that you have posteriors over states for the posteriors into the first level

um but um but yeah definitely you can have these expectation effects although i don't necessarily think of those as being um voluntary right i'm not sure that um like like selective attention is definitely a kind of controllable cognitive action whereas shifting uh prior expectations i'm less confident that that can be cast that way um but but um priors definitely have a

and influence right on what your posteriors of our states are given some stimulus, especially like Chris said, when the stimulus strength is fairly low or noisy.

Um, you know, so, so some of the work that, um, yeah, Al Powers and his group at Yale, and then, um, that I've been, we've been collaborating to build active inference models for this stuff, um, is, uh, yeah, these sorts of conditioned hallucination paradigms in people with psychosis.

So the way these paradigms work is that you start out always showing people a light and that is always coincident with a tone that appears in some white noise.

And so on the first several trials, there's a fairly strong tone that plays in the white noise every time they hear a light or every time they see a light.

Um, and it's thresholded so that they would perceive that tone, uh, 75% of the time.

Um, and then, and then after several of those trials, um, there starts to be cases where, um, the, the tone is still present, but it's weaker.

So they only would detect it 50 or 25% of the time.

And then there are trials where the light comes on, but no tone is played and there's just white noise.

Um, and what, um, what they have found.

is that people with psychosis have a higher probability of reporting hearing tones when the light shows up, even when there was no tone.

So they build up this expectation to hear a tone whenever they see the light, and somehow those prior expectations that the tone will be there have a stronger influence over whether they hallucinate the tone or not.

That influence is stronger in people that have hallucinations.

than in people that don't have hallucinations, which has kind of led to this idea that what's going, you know, part of the computational issue in people with psychosis that are more likely to have hallucinations,

is that prior expectations are in some way really precise and therefore gonna dominate perception.

And that could be either because the prior expectations themselves are really precise or because beliefs about the actual sensory signal are

Those are believed to be very imprecise.

But in either case, the relative precisions are supposed to, are thoughts that kind of favor prior expectations in psychosis to a greater degree than people without psychosis.


SPEAKER_08:
Thank you.


SPEAKER_03:
Can I just follow up really quick?


SPEAKER_08:
Yep.


SPEAKER_04:
Yeah.


SPEAKER_03:
So it's interesting that you brought the auditory thing into this, but the auditory hallucinations.

And I wonder if it's a visual cortex thing or if it's involved more with the auditory cortex, because even when Chris was talking, you know, you think about like,

people have seen like when you have two things played simultaneously auditory tracks that like it's like listen for the phone number and then you hear the phone number but then if you're not listening for the phone number you're you're hearing something else so it's interesting that that the auditory system like the the selective attention

in the auditory track really is is much more apparent like where you see the blue dress versus the white dress or the gold dress or whatever color it was like the two dresses is not so selective it's like look for the blue dress or look for the white dress if you set up that expectation in the visual system it's not as um prevalent i think yeah i mean i will agree that yeah it's definitely harder to switch the uh interpretation from the blue to the yellow when i've tried that


SPEAKER_04:
I can, I can get myself to do it, but it's way more effort for you.

Right.

Um, and, uh, yeah, I mean, I don't know, Chris, if you have any ideas about why exactly that would, that would be, I mean, I know cases of like, you know, attention and binocular rivalry and things like that.

In those cases, I think there are cases where I think selective attention can play a bigger role, like the Necker cube, for example, um, that's maybe a little more analogous to the kind of like, uh, multi, uh, channel auditory input.

Yeah.


SPEAKER_07:
I think it just depends on how you set the task up.

I'm not sure if the comparison is really a fair one, I suppose.

I think you definitely can set up a lot of... Expectations play a huge role in the visual system, I think is one thing to say.

Whether you have voluntary control, whether those things come under the purview of voluntary attention is another thing.

i still think there are lots of cases depending on how you set up the problems i think binocular rivalry is a good one or even something like actually literally like where you move your eyes i know that sounds trivial but um whether your eyes are moving around a page or not kind of determine determines uh some different types of visual illusions so if you think about um maybe illusion isn't even the right word

But what's the, sorry, we're working on a project about this right now.

What's it called again?

Thomas Parr's paper on this, Ryan.

Do you remember?

It was Troxel fading.


SPEAKER_04:
Yeah.

Yeah.


SPEAKER_07:
If you Google Troxel fading, I think that's a really powerful example of where just kind of selective attention has a really major role on whether you perceive something as present or absent.


SPEAKER_08:
Yep.

Very interesting.

It's almost like there's ambiguous and non-ambiguous stimuli in different domains and maybe even interpersonal variation.

One person being really good at picking up on something subtle in one domain or not.

So we'll do Steven, then Dean, then Adam.


SPEAKER_01:
I think this ties in actually to what I was mentioning earlier about when you scale things up.

And I agree with what you're saying.

When you scale things up, it may be useful to think about how someone facilitates the attention of people in a group and how you take them and how you think about taking their attention.

But it doesn't mean that whatever they consciously have intersubjectively together

is now read by the model.

So its usefulness may be in how people's attention is taken.

And I think it ties to this hypnosis thing as well, because I think your model speaks to top-down and bottom-up happen at different scales.

Because often we think top-down, we think executive function.

And executive function is telling me what I think.

But like you say, you can have a subconscious level

of trying to think what something is, even if it's in the subconscious.

And hypnosis is doing that.

Hypnosis is playing the game with someone.

Or with these optical illusions, you say to someone, you're looking at this cube from above or below, and it flips.

But it's not that you're being told it's this type of cube.

and your executive functions pushing it it's it's still top down but it's it's happening at a um a more subconscious what we might call a subconscious level and i think that's quite interesting so if i could just speak to that i think there are a couple of things going on there so i would talk about executive function well i mean i think it


SPEAKER_07:
one thing that this is a bit of a tri-point but top-down is a very ambiguous term um i think what where so you can mean kind of that can mean anything from basically like recurrent loops from higher quarters from cortices that are higher up the cortical hierarchy um to something like top-down intent like a voluntary control of attention via executive like frontal parietal networks or something like that um so those are just different different phenomena um

I think in our model control of attention would presumably be at, if, if we were to build a model of this would be at this, you, you would put the dorsal attention network at the second slower time scale for model.

I think maybe Ryan, you can, we can speak to this as well.

I haven't really thought about this much.


SPEAKER_04:
Um, I would have been a little more, uh, like executive control network.

sort of thing I guess would have done a little bit more what I would have thought about just kind of off the top but yeah you're probably right about that I mean visual attention right I mean obviously right the dorsal attention network is you know kind of defined or originally yeah like went specifically to yeah like visual attention and things like that or kind of quote unquote top down visual attention but in terms of the kind of directing

right, visual attention in a goal-directed way, then I think that would be more executive control network.

That's how I would thought of it.


SPEAKER_07:
I mean, on the hallucination point, I did my undergrad at Macquarie University and they have a whole hallucination group there within the cognitive science department.

And so I just feel like acutely aware of the fact that I don't know anything about it.

and it's also an extremely complicated phenomenon you can use hallucination to like as a model of various delusions like you can uh induce is it called like mirror basically like mirror agnosia where you don't no longer recognize yourself mirror self-agnosia or something where you no longer recognize yourself in a mirror like it's a really powerful tool but i have absolutely no idea how it works and so i'd be very kind of i think it's very tempting as kind of

modelers and theorists to kind of go into various experimentalist domains.

Like, hey, here's how my overarching framework explains your thing.

And I kind of want to resist that.

I know a little bit about visual consciousness and how expectation, all that stuff works there, but hallucination is such a hairy topic.

I just don't know.


SPEAKER_04:
Yeah, I mean, I would also say, I mean, in the comments, there's a few different things that have been brought up here, right?

But, I mean, in terms of

It's not clear, for example, but the kind of auditory hallucination story that I was talking about earlier in terms of having relatively more precise prior expectations, creating a stronger tendency for auditory hallucinations.

it's not it's not clear at all that that kind of story mechanistically is generalizes to other domains so i mean actually in the individual domain there's this whole other body of work where it looks like people that have delusions um again as part of the sort of as part of psychosis right um actually um show the opposite weighting in the visual system right so where they actually um assign um

too much precision to low-level bottom-up visual signals.

So they end up, there's these interesting cases where they end up, if you look at eye tracking, they end up doing, people with delusions end up actually, or psychosis, schizophrenia, end up doing better at certain tasks than healthy people, but worse at others.

So like one example of this is,

if you have people just kind of try to follow, just visual tracking, follow a kind of moving target, then if that target switches directions really quickly, then the people with psychosis end up actually following it better than healthy people.

Because healthy people kind of continue to follow it for a little bit after the turn in the kind of predicted direction.

um right so prior expectations are kind of playing a stronger role um so they actually do a little better whereas if instead you have something where the um where the target temporarily kind of moves behind some kind of barrier um then healthy people do better whereas people with schizophrenia kind of you know lose it because prior expectations don't do enough um to keep them kind of following what the trajectory would have been right as it goes behind the visual occlusion and there's there's lots of other cases

um for instance in like the proprioceptive domain where people have talked about the same kind of thing where people with um schizophrenia actually do um better like things like force matching illusions um and stuff or yeah um force matching tasks um um and i mean there's more to the the story there um but but um but people have tried to link that to like um delusions about agency um and things like that so but so just general point being that um

that the way that a computational mechanism looks like it happens in one, say, sensory domain can still operate very differently in the other sensory domain, even in the same disorder, in the context of the same psychosis.

So these things don't necessarily, they shouldn't be expected to generalize across domains.

is um is one one point um the other the other point was um you know in relation to you know things like hypnosis which um i don't know a lot about um as part of a textbook chapter um i was uh asked to write a paragraph or two on hypnosis so i kind of looked into that literature a little bit um and it's actually pretty messy they're very different there are multiple different theories about hypnosis work

Um, and I mean, one of them, you know, can relate to this idea that, um, that, um, you know, if, if the hypnotist repeatedly like give suggestions that are set up such that, um, they will be confirmed.

Right.

So it's like, hold out your, the hypnotist says, hold out your arms.

And then he says, um, your arms are going to start to feel heavy.

And then they'll start to fall.

Right.

Obviously anybody who holds out their arms, they'll eventually start to feel heavy and they're heavier and they'll start to drop just as a result of your muscles, your shoulder muscles getting tired.

Right.

So if you do a number of those things and the kind of, uh, Things the hypnotist, the predictions made by the hypnotist are confirmed, then, you know, it could be that you're just coming to assign a really high reliability.

to whatever the hypnotist says, right?

So in that case, if the hypnotist just comes to be assigned really high precision, then you can just adjust prior expectations in a way that just kind of dominate or can end up having a much stronger influence on what you do.

And that's kind of one idea.

But again, there's several other theories of hypnosis that just have to do with kind of conforming

um kind of social roles and things like that that you're told to um that you're kind of told to do and i mean anyway so i wouldn't necessarily feel that confident um saying much about about how this relates to hypnosis other than that in a broad sense it can involve manipulating um sort of tricky ways of manipulating prior expectations and other in other people but again i mean even that is pretty speculative


SPEAKER_07:
mean so if i could just quickly throw something in another complicating factor so even within visual the visual system having a strong prior in one domain or doesn't generalize to others there's this really great paper that just came that came out i think last year in cognition where they looked at representational momentum mooney images and um goodness i think there was one other i think it was illusory contours and they had one other

top-down expectation kind of paradigm.

And there were large individual differences between all of them.

And there were also large individual differences in kind of the effect of prior expectation across those different tasks.

So I think there's just kind of a lot of complicating facts, like moving parts and a lot of this stuff that we have to deal with when we talk about these things.


SPEAKER_08:
Okay, we're going to return to the stack and go to Dean, and then a question from the chat, and then Adam.


SPEAKER_02:
so i'll go back and be ambiguous um so when i was looking at your diagrams and one of the things that really i wanted to ask this question but we ran out of time last week um do you think we can say that if a person says i see this or i do not see that see this the or yes or no um can we can we apply an idea that there's a certain amount of acceptance

and a certain amount of letting go.

And I, I kind of bundled that under we reconcile, or is it something completely different?

I mean, how do you, how do you look at this now when a person's actually looking at something and going, okay, I do not see the box.

Is it just, is it that simple?

That's a simple yes, no.

Or do you think there's something more built into it?


SPEAKER_07:
Um, so,

There is a whole cottage industry.

I feel like cottage industry might even be an understatement on how to collect verbal reports and the proper ways of doing it.

So there's like a perceptual awareness scale where you in some sense have a description of the content.

So you have, I didn't see anything.

I maybe saw something, but there was no specific content.

Like you have a vague feeling.

You saw something, but it was a bit blurry.

And then you had like a full vivid experience.

That's one way.

You can have people place bets.

Um, that's another way you can have people do confidence ratings.

Um, lately there's been some incredibly clever work basically looking at, so just give a shout out to people who I, whose work I really admire, like Hakuan Lau and, um, Megan Peters have a really great paper in eLife where they basically look, show that people seem to have optimal in the sense of like Bayes optimal introspective access to their own performance.

So basically when they are

they will, people never behave correctly without kind of knowing that they behaved correctly as it were.

If you do these like very tightly controlled experimental situations.

So yeah, I think maybe I was a bit flippant before when I said you have to take reports as foundational.

I stand by that statement, but there is like years and years and years of like careful psychophysics have gone into being like, yeah, these things are reliable.

And basically it comes down to, I wouldn't believe if someone said, yeah, I see something, but they were getting like 50% on a forced choice task about the orientation of a Gabor patch, but they were claiming they would see it, I just wouldn't believe them.

Generally speaking, you would expect if you see something, your performance should also go up.

That is not to say, however, that you can't have

Um, for increased performance without awareness, there is some convincing evidence showing that those two things dissociate and that's kind of what we were trying to show here.

Right.

Right.

So hopefully that answers the question.


SPEAKER_08:
Yeah, it does.

Nice.

Yeah, I noticed on this extended taxonomy, the bottom right, it's like consistent prior, high attention, strong signal.

It's the perfect storm.

And then it's 100% seen, 100% correct.

And then again, kind of taking it loosely mapping to our experiences with other attention phenomena.

Like if you're sure about seeing, and of course, the way that you tuned prior or inconsistent prior, these are variables that are in the model.

So people can play around.

This isn't like a specific yes, no, but just examples of how rich the phase space is in terms of what the model can do and what it can capture.

So here's the question from the chat, and then it will go to Adam.

So the question from the chat is,

any thought about the possibility of using this model to investigate lucid dreaming related phenomena like the failure of implicit metacognition while dreaming thank you so how about dreaming or lucid dreaming so that stuff is super interesting i mean i don't think this model captures that at all except in like a very in like a computational sense like i don't know what task would be modeling specifically but like we can if we're being a bit metaphorical about it um


SPEAKER_07:
So presumably, I don't know anything about lucid dreaming.

I presume it's a case where basically you are dreaming, so you have kind of, you're not just asleep, you have kind of a experience that goes along with it, and also you are aware of the fact that you are dreaming.

That's really interesting.

I think it would have to come back to the

don't know how we one would capture metacognition in this framework i think that's one thing to say i don't know what it would be maybe ryan you have some thoughts on this but it would relate to basically how how one thinks about metacognition in these deep temporal models yeah i mean i mean i mean metacognition in and of itself comes in a bunch of different flavors right i mean so there's kind of you know beliefs about


SPEAKER_04:
how good your memory is, for example, right?

It can be kind of metacognition.

It's, you know, it's like beliefs about your beliefs or beliefs about patterns in your attention or, you know, there's a bunch of, bunch of different types of,

metacognition, right?

It's just sort of beliefs or operations over representations of your own cognitive processes.

So I mean, the tricky part is that you kind of have to have a higher level that's treating what you chose to do in terms of a voluntary cognitive process that observes that and then infers something about it.

And the way that that ends up influencing the cognitive operation level below, the way that that higher metacognitive level ends up setting priors on the lower level, I'm not sure the right way to think about that.

I mean, you'd have to look at literature on the way metacognition affects performance on cognitive tasks, which I'm definitely not an expert on.


SPEAKER_07:
One thing to say just very briefly would just be in the discussion section of our paper, we discussed alternative models.

I think probably the model that's closest to ours in the literature would be Steve Fleming's higher-order state space model.

And I think...

There he explicitly discusses ignition-related phenomena and kind of metacognitive inferences about the presence or absence of stimuli.

So I would say if you're interested in that kind of stuff and how to think about it in a Bayesian framework, look there.

It's a really great paper.


SPEAKER_04:
Yeah, I mean, another one, I mean, that comes to mind, I know, is that there's a paper by where the first author is Klaus Stefan from 2016.

where he talks about the way you can apply active inference models to model homeostatic and allostatic control processes.

So basically like anticipating, for example, that your temperature is going to drop and therefore increasing temperature, body temperature a little in advance so that when it subsequently drops, you stay within a kind of survivable homeostatic range.

I'm just using temperature here as an arbitrary example of any kind of variable, internal bodily variable, like glucose levels or hormone levels or anything like that.

But they do talk about this level, a metacognitive level that you can add above the allostatic regulation level that more or less tracks the efficacy or the kind of success of the allostatic level.

and how if the allostatic level repeatedly fails to keep homeostatic variables or keep bodily variables within homeostatic ranges, then the higher level can basically infer that allostasis will fail.

And they show how that can plausibly lead to certain sorts of symptoms of depression when you have low beliefs about the efficacy of your own allostatic processes.

So, I mean, that's a paper that I'd recommend looking at for one example of the way that you might apply one type of medical condition on top of the Bayesian.


SPEAKER_07:
I also just remembered a paper came out yesterday.

I shared it.

Sorry, I forgot about this.

I just shared it.

I shared it on Twitter yesterday, if you want to look.

It came out of Mike Allen's group.

I've forgotten the first author's name.

I'm really sorry about that.

But...

uh, they explicitly discuss like predictive processing and active inference theories of consciousness in relation to interoception and metacognition.

And they have a whole discussion of this type of thing.

So if you're interested, maybe just like check my Twitter page or at Mike Allen's Twitter page.


SPEAKER_08:
Sounds good.

Um, we'll go to Adam and then anyone else who raises their hand.


SPEAKER_06:
Hi.

Um,

perhaps tying a few of these things together.

I was wondering, with respect to hallucinations and psychosis and forms of delusions, if maybe some of it, you could potentially model it in terms of not having access to mental acts, which might have associated efforts copies that could potentially be adjusting precision

and are also having, uh, different components of the associated mental act, which you may or may not attribute to you.

Like some, so like aspects of interoceptive inference that could lead to senses of like willing or ownership, but then by not having access to this potentially where, where the access itself, uh, of the action generation could, um, basically adjust what you're, how you're attributing, um,

what sense you're making of the associated perceptual uh events so it's like um how do i say this so it's like if you're uh talking to yourself in thought or something like like thought is in your speech like ragotsky uh let's talk about like well if you don't have access to like the generation part on for some reason i don't know how much it can be separated but then like the voice might just like appear to you in a way where you don't have a sense of ownership if there's not like the right type of like

maybe interoceptive coupling to the generation process, or just like a lack of metacognition to contextualize it.


SPEAKER_04:
Yeah, I really like this idea.

I mean, one thing to keep in mind here is that, you know, the sort of thing that you're talking about when people have

sort of different experiences of, or where there are different contents and experience.

I mean, basically that's what you're talking about, right?

Different contents that can be an experience.

Those, those are just in a model like this going to amount to different sorts of state spaces that you can infer posteriors over, right?

So, you know, you can have beliefs about how much ownership you have over your body, or you can have beliefs about something going on in your body interoceptively.

And all of those things are just going to be, you're going to have some sort of afferent signal and you're going to have some prior expectations, obviously.

And then you're going to infer some posterior over whatever the contents are of the thing in question.

Right?

So, so anything along the lines of what is the, what is the content of one thing versus another is just going to amount to a particular level in a model that infers posteriors over particular contents.

So none of that is going to have to do directly with our model, other than the fact that given posteriors over any content, if our model were applicable to that, you could just treat those contents as the lower level.

So you'd have inference over some content, and then our model would say something about the processes that make that content conscious or not.

or consciously accessible or not.

So the only thing that our model might be able to say is if you swap out the squares versus lines in our model, whatever the beliefs are at the lower level, if you swap that out with representations of some other content, whatever it is you're interested in, something about the body or something about what sequence of speech

that you're hearing or anything like that.

What word you just heard.

Swap that out as the content of the lower level and our model might be able to say something about the processes that determine whether those contents become accessible or not.

But beyond that, our model doesn't have much to say.


SPEAKER_07:
Yeah, I mean, if I could say something on that as well.

I really like the idea, I think, just along similar lines to Ryan, because it just casts decisions about ownership as just having access to some content or another.

And so for me, the essence of any type of conscious access or awareness process is basically whether it's integrated into this temporally deep representation, and in doing so, available to all of these sub-processes a la global workspace theory.

And so I think this kind of relating this to kind of the metacognitive point, I think this is actually a spot where maybe this is a difference between our theory versus an active inference theory that gave an explicit role for metacognition.

So for me, it's just the fact that you can be aware that it is, you're aware of it because it's integrated into this representation, right?

Whereas maybe a higher order person would be, no, you have to also infer that you are seeing it or that I am seeing it or something like that.

And so I think there's actually like really interesting empirical questions.

That's all I can say.

But I hope that we can kind of, I don't know, maybe it would be nice to see a discussion of kind of

active inference theories of visual awareness of metacognition versus active inference theories of like visual awareness in general.


SPEAKER_05:
Yeah.


SPEAKER_07:
I don't know if you, I know you've written a lot about kind of different theories of consciousness, Adam.

So I don't know if you have any thoughts about that.


SPEAKER_04:
I mean, I know that, um, you know, there's this preprint, um, by, um, the first, the first author is, um, is Smith Lars.

Um, well, I can't remember, uh, uh, his first, the rest of his first name, but,

but his last name is Smith, that does propose a type of model of metacognition.

In theirs, attention works a little differently than in ours, but essentially you do have a second level that controls a precision parameter on the first level.

And then there's a third level in there that confers things about control of,

the control of the temperature parameter at the first level, if I'm remembering right.

And they use it as a way of, the way they talk about it as a way to think of meditation and mindfulness and things like that.

I don't remember it in enough detail to say a lot about my specific feelings on that, but that is an example that I'm aware of.

Again, it's just a preprint at this point, but of another attempt to try to model

something like metacognition.


SPEAKER_08:
Cool.

Interesting questions.

Adam, did you want to say something else?

Yep, go ahead.


SPEAKER_06:
Yeah, so I basically want to throw your model at everything.

But in terms of metacognition and what I've worked on in the past, mostly I've been focusing on trying to cross-reference various theories to see the points of overlap and non-overlap, mostly considering

accounts of dynamic modularity that might apply to a global neural workspace architecture or like integrate information theory where you can think of their complexes of integration those modules as themselves being like do an iit handling of those and seeing like where's the overlap basically trying to give semantic content to iit um via like gnwt is like an architecture for bayesian model selection and the whole thing kind of being adjudicated by free energy principle and

mostly an active inference, not nearly in the technical depth that you have.

I'm only beginning to move into things like metacognition and, uh, like higher order forms of consciousness.

And the ideas are like really, uh, speculative.

Like, so like one idea is that like, as you're like, um, let's say, um, imagining like enacting something, you might start out from like,

generating like experience from an egocentric perspective.

But then this enactment would auto associatively also have third person allocentric representations, like from like the ventral visual stream or something like this, where like you've seen people or yourself doing similar things.

And so you might then like be able to get access to in the mind's eye, this like third person point of view on you with some unfolding, like some sort of like moving back and forth between

In the models I'm working with, phenomenal consciousness and accessible consciousness, there's a sense in a Rudroff projective geometric modeling, it's always from this first-person, egocentric point of view.

But then the idea is you then are looking at an objectified you, and this would be part of metacognitive awareness and contextualizing you.

in terms of like you doing actions and then this third person little homunculus doing actions that you're seeing in your mind's eye and moving back and forth.

That's like the basic space I'm moving into.

I don't know how much of that's going to work or how you would model that.


SPEAKER_04:
I mean, like one thing, just kind of... Kind of, I mean, again, just to, I mean, when you think about this kind of in formal terms, right, then if you're in some sense, if you're

the content you're describing as a belief that you are doing something right um so that's just the content in the space space right so same kind of thing right swap out our lower level with a state space that is beliefs about what i am doing right then it's just the same thing right you have a representation about what you are doing and in our model you would either um

you would either gain access to the representation of that content or not via its integration with this deeper temporal level.

Granted, beliefs about what I am doing or not is already

right pretty pretty temporally deep um but were those sorts of contents to apply to our model and that's what it would say it would say that those contents need to be integrated with this deeper temporal uh model that is able to form posteriors over something that incorporates that um and uh

So again, I mean, at the end of the day, it really just comes down to this distinction about the content being represented and whether or not the posteriors associated with that content get integrated with the higher level, the sorts of things that the higher level described here can do.

So, so it's just a question about whether something like that is the content that you want to be, uh, using in a goal-directed way or reporting, um, or, or, or whether just, uh, just representations of the, of the lower level kind of visual content.

Right.

And that being integrated with this level is, is what's, is what's involved.

So to me, again, this just has to do with different levels of representation and what the contents are that you're becoming conscious of.

Um, so.

Oh, and by the way, just to mention, I looked up the paper I was mentioning.

So it's called Towards the Formal Neurophenomenology of Metacognition, and the first author is Lars Sandberg-Smith, just to do justice to remembering who the first author's full name was in the name of the paper I was talking about.

Sorry, I think you started saying something at the same time, Chris.


SPEAKER_08:
Yeah, Christopher, then Adam.


SPEAKER_07:
I was going to change the topic very slightly, but I was wondering, so how you think about phenomenology or phenomenal consciousness?

Because to me, it comes down to the nature of the representation that you're gaining access to.

So to talk about, like, you talked about the projective consciousness model, which is really cool work.

There, this perspectival nature of consciousness comes from

the structure of the generative model and the fact that they're using this projective geometry and that's built into the structure of the generative model.

I'm just curious about how you think about this and how that kind of maps on to IIT, because at least as I understand IIT, like phenomenal consciousness is kind of a... It's never really been clear how the content of consciousness fits into IIT for me.

Like it's all...

Anyway, yeah, I'm just interested to hear your thoughts on how it all fits together with global workspace theory.


SPEAKER_06:
I would say there's some kind of giant bag questions in IET.

I think, though, it's not nothing that they start from axioms of what they think should be part of phenomenology and then work their way to this sort of way of handling systems.

In terms of, I think it's a decent prior for like what we should look for in like physical substrates.

I wouldn't like completely like throw it off, but they then would say, if you have these axioms that characterize experience, intrinsic existence, composition, information, integration, exclusion, if you have these different properties, then it is sufficient to bring about phenomenality because they started from phenomenality.

Oh no, I don't think that follows at all.

That being said, I think it does potentially like give us some priors in trying to think about physical and computational substrates of consciousness.

So that would be like the cross-referencing.

And then the other part would be like, how do you think, so if you're thinking of global neuronal workspace as like potentially like a trading off of modularity as being part of the physical implementation, well then the question is like, so when do you have like larger big modules functioning as like

dynamic cores or workspaces?

And when do you have more fragmented local processing?

In theory, you could get Bayesian model selection with discrete updating with a bunch of small beta complexes close to the modalities.

Or it could be this big sprawling alpha complex that's multimodal in all these different ways.

But then you're moving back and forth between these degrees of synchrony.

It seems like IIT could potentially be useful there for

describing that.

So that was one of the ideas of that interrelation.

In terms of bringing it to Rudroff, for me, the object I would want that would be potentially a minimal condition for phenomenality would be some sort of joint distribution over your body pose and visuospatial awareness.

as like the minimal thing for me, at least for human like consciousness and like a series of, um, basically roughly at alpha frequencies, a series of estimates of that.

And, and, and then, and thinking of this as iterative Bayesian model selection via a global workspace architecture, but not necessarily one where there's like access.

That's like a more sophisticated kind of consciousness in terms of you wouldn't necessarily, I would call it like it's pretty darn global.

You have something like spanning all the posterior cortex, but it's not the kind of workspace you're dealing with where you're actually having knowledge and access.


SPEAKER_08:
Thanks, Adam.

So if anyone else wants to, they can raise their hand.

Otherwise, I think there was a very good request.

You did just ask one, so let's mix it up.

But there was a very good request, actually, if Ryan or Christopher, you could just...

look at this part of the figure which i know is a structure we walk through in the model stream as well and just kind of map some of these bigger ideas that we've been talking about to some of the letters just in a way where people can now look at this

outline, say, okay, the experiment, I'm kind of on the page, we talked about a bunch of visual and interoceptive kinds of experiments, just where did it map in this paper, so that people will always be able to go back to this paper, look at the code, and then kind of map some of these bigger metaphors to the experiment that you did here.


SPEAKER_07:
Do you want to take this Ryan?


SPEAKER_04:
Either way, it doesn't matter to me.


SPEAKER_07:
So how detailed would you like me to go?

Would you like me to explain what the little square, what the D's and A's and B's and S's are and all that stuff?


SPEAKER_08:
Let's go for all the letters on the right side and maybe even some on the left.


SPEAKER_07:
All right.

So this is a partially observable Markov decision process and it's a hierarchical one.

So let's just focus on essentially, let's start at the bottom.

O's are observations and S's are hidden or latent states.

And what the A matrix does is it provides a mapping between those two things.

So the circles are random variables and the squares are essentially functions, in this case, probability distributions that map between those two things.

The A and little arrow going down just says that that's essentially A with that little A superimposed on arrow is basically saying that's expressing a conditional probability distribution saying that O, observations, depends upon the hidden state.

Now, as we start to evolve through time, we need transitions between discrete hidden latent states in the world.

And those are what's described by B. So this is something like, what is the probability that at time one, what's the probability of each hidden state at the next time step?

And it will describe kind of S at T conditioned on S at T minus one.

Now, then what's special about the hierarchical models is that you have another layer on top of that.

where hidden states at the first level are now being treated as observations by the second level.

so the posterior probability at the first level is basically acts as an observation um for the second level a matrix um and then kind of at the second level there should be a d at the first level too but that's fine um basically what this prior what this second level hidden state does is it provides a prior over that first level

And then at the first level, you can then have, basically, we just have one time step at the first level, but you can have any amount of time steps as you want.

So if you imagine the metaphor that's often used is like the minute versus the hour clock on the hand, they're evolving at different time scales.

So you could, in this diagram, we show that the second level evolves at two time steps for every one time step at the first level, at the second level, sorry.

At the second level, there are again transitions between these latent states, and that's also where we have policy selections.

That's why we put it up there.

So policy selection is essentially, if you imagine a whole series of hidden Markov models, this is my favorite way of describing policy selection.

So these hidden Markov models are basically just a partially observable Markov decision process without the decision aspect.

If you imagine a whole series of basically these graphs,

and you are deciding what graph kind of is gonna be my future.

You are trying to find the graph with the highest model evidence.

That is what computing, doing policy selection does.

So you are choosing the transitions that will take you to basically the graphs with the highest model evidence.

I know that, Ryan, you wanna kind of clean up some of that explanation?


SPEAKER_08:
We're at the shoulders.

Let's go to the last few variables.


SPEAKER_04:
Yeah.

So, I mean, you didn't do G and C and E, right?

Yeah.

Yeah.

Okay.

So, pi, just like Chris said, right, is just your distribution over policies.

G is the expected free energy.

So, that's essentially the

the function that decides what the value is.

You can think about it about each policy.

And that is with respect to C, which is your preference distribution, which is essentially the thing that specifies which observations you want and which observations you don't.

Now, somehow I see that an E has been placed with an arrow down to G, and that's not correct.

Um, he should be, he should be going, you should be going straight to pie.

Um, um, if you're going to include E, um, I don't think we did anything with E in our model, so I don't think he needs to be there.

Um, but, um, but E if it's used is just a kind of separate prior over policies that can encode something like habits.

Um, so it competes with the expected free energy G over policies, um, to infer what, uh, when you're inferring what the posterior over policies is.

Um, so, uh, so yeah, I mean, otherwise I feel like Chris, uh, I feel like Chris described it.

Well, I mean, you know, it'd be a little.

It's always a little easier if you have like a, you know, if you're in control of the pointers, you can actually point to the things that you're talking about.


SPEAKER_07:
I think you also asked us to relate this to an experimental paradigm.

So if we maybe briefly click down to figure two, what you've labeled as figure two there.

So we can just look at that.

So the idea here is that there was a forwards mask.

Um, at the beginning of the paradigm, this was just a series of lines and there were these kind of stimuli on the outside.

At the second time step in the task that, or in our like simulated task, they, on some trials, the square kind of was self organized, some of the lines kind of self organized into a square.

So I either in our discrete state space model, a square was present or it wasn't.

And in the third time step, it was just replaced by more lines.

And then afterwards we ask the agent to construct a report of that.

So basically at, you can think of the first level of the model as being all of the features of the stimulus and also the location of attention.

So this might be something like saliency maps in posterior parietal cortex, which are directing wherever attention's being pointed to.

And then you might have the features of the stimulus, which are represented in various places.

At the second level, we now have our, this is basically our frontoparietal cortices.

This is tracking the evolution of states at the first level.

So it's tracking that at the first time step, there was lines presented.

At the second time step, there was a square presented.

Maybe it, maybe it wasn't, maybe it was presented, maybe it wasn't.

And at the third time step, there was more lines.

And you can imagine in like a real task, all of that unfolds in a space of about a second.


SPEAKER_04:
Yeah, so you can think about it, I mean, like, yeah, the second level is really the content in question that it's inferring is the sequence, right?

It's not whether it's square, it's whether it was lines, square lines, or just lines, lines, lines.

And so... Sorry, go ahead.


SPEAKER_07:
Oh, and so if you're thinking about where experience lives, obviously we don't experience a sequence, right?

I think, and I think Ryan and I have been chatting about this a lot lately because it relates to work we're currently doing.

But I think what's going on is that what we experience are essentially like the updates to our second level beliefs.

And that's really crucial.

So the reason why it's crucial is because we know empirically that kind of what could be, we might describe as brain processes that are at the first level of the model.

These can be either conscious or not.

And contents that are conscious, when you are conscious of something, it has all these really important functional consequences.

So here it might be kind of being able to see it for report, maintain it for report.

But also if I see something, I can then voluntarily maintain it in memory for as long as I want, as long as I don't get distracted.

Like there are limits on like human psychology.

But that's kind of a really important function that's enabled by being conscious of something.

And that allows us to do things like construct reports of our experience.

Or even, I think by report, I just mean something very general.

It could be in this model, we kind of have the toy thing of being able to put a sentence together, but it could also be more empirically real.

In terms of empirical paradigm, it'll just be like hitting a button or doing a confidence interval or something like that.


SPEAKER_08:
Cool.

Thank you for that.

Interesting answer.

We'll have Steven and then Adam and then anyone else who wants to raise their hand and also any last questions from the live chat as we kind of slowly land the plane.


SPEAKER_01:
Yeah, I was just going to ask, in relation to those lines and the way that the square appears, like you said, so this is actually like a graphic representation, but when someone looks at it, there's probably some special device that you've got that represents this.

Is it like...

that the lines rotate and at some point it just happens they rotate such that they line up and then they un-rotate or and just one other question is why didn't you use negative space like you like say they line up to make you know like a negative space square i just wondered whether that relates to the choice of how it pops in and out of perceptual con sorry phenomenological awareness phenomenological consciousness or access consciousness


SPEAKER_07:
So basically this is just a very, in our model, we have discrete hidden states, which are categorical distributions, which are lines or squares and external states, which are red.

And so this is kind of just a graphical representation of a way to think about that, but it's really nothing to do with how it works mathematically.

Then in terms of actually the empirical task,

Like this is a task that we basically like borrowed from Michael Pitts, who's a really done a lot, he's a cognitive neuroscientist, done a lot of really important work in the area of visual consciousness.

And as to why they chose to kind of have these, I guess the, I think from memory, there are videos attached to this paper.

From memory, I think the lines is kind of, they jitter around and then occasionally a subset of them will all line up into a square and then go back to like jittering.

As to why they did that versus other methods, I don't know, really.

There definitely are subtle things you can do to a stimulus which have major impacts on whether you have conscious access to it or not.

But the phenomenon of inattentional blindness is super general.

You can have people... So the really famous example is...

that everyone gets shown in undergrad psych is you're tracking whether a group of people playing basketball.

And you ask, how many times does someone with a white shirt get past a ball or something like that?

And your attention is distracted and so consumed with that that a bear or a gorilla can walk into the middle of the screen, be in the dead center of your phobia, wave at you, and then walk away.

And then about 50% of people don't see it at all.

And that's really well replicated across a bunch of, or they don't report seeing it, I think I should say.

I think there's some debate about whether this is a memory phenomenon or any of that.

There's lots of subtleties there.

So hopefully that answers the question.


SPEAKER_08:
Also one advantage and one rationale for these pure modeling papers that we're discussing today is the space of the possible human experiments or even any other kind of real world experiments is very limited.

It's very limited what you can actually have a real set of humans do.

And so it's great to have tools that help us explore some of the patterns that we're looking for and understand how variability and

do statistical calculations like how many participants might we need of a given variability range if we want to capture such and such an effect without distorting it this way.

That's critical information.

And if you don't have the model, then the experimental design phase is totally blind and it's very shot in the dark.

And so this helps inform structures for even thinking about how to design human experiments, for example,

as we're talking about here with visual awareness, but it could be for other kinds of experiments.

So, Adam, and then anyone else who wants to make a comment.

See you later, Alex.

Bye.

Adam?


SPEAKER_06:
Hi.

I'd be curious in knowing more of your thoughts on the potential, I guess, richness or lack thereof of experience.

So, at one point, could consciousness be phenomenal or access

more experienced as a series of snapshots that are like static sequential but we don't know it like a flip book or is it more like a continuous stream like if you're getting this discrete updating are these updates over like like the agent in like motion like if you take like a camera sometimes in your phone there's like a little brief like forward ahead like look ahead and back but like what would be the nature of these uh discrete updates i yeah i i


SPEAKER_04:
I mean, I know the, like, I'm trying to remember, I know, and I remember reading Stan DeHaan's book a few years ago.

And there is, there is some work, I think, showing, you know, there are kind of, there's like a minimal time for like, essentially updates to the, to the content of consciousness that have some kind of discrete character to them.

I don't, I don't remember what the-

yeah do you know what that do you remember the actual like milliseconds i can't remember like 50 milliseconds or something like that yeah i don't i don't remember offhand but there is there is something you know like that that maybe might be you know like similar to like a refresh rate or something like that um but i yeah i don't remember too much about that offhand um but but i mean one thing you know one thing to say is that um you know because

there are these sorts of discrete updates in our model, but I mean, it's also good to remember that

the higher level representation is specifically about this sequence, right?

Which is integrated in a sense that the hypothesis is about a whole sequence that doesn't necessarily need to be thought about as having sort of discrete chunks to it, right?

It's just a single hypothesis about what the whole sequence was.

And so you might think about that, right?

As having a more kind of continuous

character to it despite the fact that it's it is updated in some ultimately discreet fashion you know and kind of just really fast bins um you can think about it that way um but uh but in terms of richness i mean to me i mean i just think of i i just tend to think of rich as meaning there are more precise features right there's more features and they're more precise

um at the at the lower level right so instead of just you know a lines or a square right at our you know in our example um there could be a ton of different lower level representations about color and shape and size and you know all these sorts of things um but

are represented in some sort of joint way, right, and become accessible.

And so, I mean, that's one way to think about richness is just what are all the things being represented that become accessible together.

But I don't know, do you have something else to say?


SPEAKER_08:
Can I ask a question actually there about the continuous modeling?

Is it possible that active inference could have a continuous mode?

Um, I know that it would get rid of a lot of the discrete benefits and the sequential message passing or maybe other heuristics, but could there be a continuous format, continuous time or space?


SPEAKER_04:
Yeah.

There are continuous state space models and there are also mixed models.

Um, you know, mixed, mixed models are kind of especially nice and probably a lot more realistic here.

Um, you know, because visual, visual input, a lot of the things that are represented by the visual system are continuous, right?

Like, uh,

motion for example is continuous brightness um you know all these all these sorts of things um and so those can be those can be um perceived and represented in a continuous scale at the lower level but still get passed up to discrete representations at the higher level um we didn't do that here for simplicity but you could


SPEAKER_08:
So... Just to clarify, in Scaling Active Inference, we did talk about the continuous state space.

I was just wondering if it's possible to have a continuous time


SPEAKER_07:
active inference model rather than a discrete time model of t123 just just bringing it up okay but then christopher and then blue so yeah three things so just quickly to answer your question daniel um so hidden markov models are just like a super general thing it's basically just when you have a markov chain and but you have but each state of the markov chain links to some outcome those outcomes can then both

the outcomes and the latent states can be continuous.

When you have a discrete latent state continuous outcome, you end up with basically what ends up being a mixture of Gaussians model.

And if you have continuous states, you basically end up with a Kalman filter.

And you can stack those on top of each other and do all the same things.

Yeah, for all the reasons you say, things get really complicated when you start to move to continuous time.

But I completely agree with Ryan.

I actually, I think there are computational, there are functional reasons to think that decision-making is discrete.

And so at the level of decision-making, computationally speaking, we should use discrete state-based models.

I also think that at a certain level,

We need continuous state spaces.

Obviously, we represent continuous quantities.

And I think Thomas Parr, we've chatted about this before in the model stream, but Thomas Parr has a really nice paper on kind of the discrete continuous interface.

And that's something I'm actually really interested in.

I'm not sure.

I don't know to what extent we should take these things as idealizations or as general kind of neurophysiological predictions.

I think that's a really, I'm super interested in that.


SPEAKER_04:
In other words, but I don't know.

Another thing to say though, is, is that it's not as though only, um, only action, you know, only action selection has to do with, um, discreet spaces.

There, there are plausibly a lot of higher level, um, representational things that are also discreet.

So for instance, like concepts, right?

Like I can, I can have a bunch of continuously represented lower level of visual features.

but I can use those to infer something discreet as well.

Like for instance, that those features forced onto my concept of a dog, right?

Or my concept of a banana, you know, whatever, right?

Cause we do have these discreet categories that we map on patterns of continuous features too.

And I mean, that's a, that's a kind of whole nother discussion.

I think we talked about it briefly last time, right?

Gaining conscious access to the visual features versus gaining conscious access to the fact that it is a dog.

So those are different levels of representation that can be attended to and may be able to independently or semi-independently be accessible separately.

So I just want to point out that not everything is going to be continuous in perception when sort of cast broadly.


SPEAKER_08:
And that's where I was going to say that's where a lot of category theory discussion comes into play.

Let's do blue, then Christopher for final thoughts.


SPEAKER_03:
So I know like we operate under the assumption that the visual, like we perceive things continuously in visual space, but I mean, really the input is like 60 Hertz, right?

So, I mean, theoretically, like this thing, this object is consistently a dog, but if it's like flashing back and forth to a cat at sub-perceptible levels, like we wouldn't know, right?

So, I mean, I think anything can be modeled in discrete chunks if you break the chunks into small enough pieces.


SPEAKER_07:
Yeah, yeah.

I've sometimes seen people be a bit smarmy about this when someone says like, oh, discrete state space is unrealistic and the comeback is always like, mate, you can always discretize things up to some arbitrary level and then discrete state spaces work fine.

I don't know.

I actually think that's a bit of a dodge, to be honest, because the way we're using these discrete state space models is in an ultra-discrete kind of way, right?

So I think that if we had a hidden Markov model

where we had 60 hertz equivalent or something.

That would just be ridiculous.

You should just work in continuous state spaces at that point.

The computational and conceptual advantages of working in a discrete state space are gone.

It would be so difficult.


SPEAKER_08:
It's a really nice question though.

And it's like, if you do make it a finer and finer granularity, you keep some of the really big benefits of splitting.

And sometimes it's actually easier to even do like a protein folding.

They'll do a time step of the tiniest, tiniest amount and do millions of time steps are tiny because it's still easier to fit that ultra rich,

discrete time model with actual time steps that can be clustered on different computers, rather than rewrite the whole base to do continuous modeling.

So I agree, it's not like continuous is simply better.

And it really relates deeply to how we think about the continuum and the infinitesimal.

And so it's really interesting area for active.

So Steven, and then any other closing comments?


SPEAKER_01:
You know, I suppose if we also were to go back into the physics of it, you know, with active inference as a way of like if vision is feeling the surfaces out there rather than it being an input signal, even if we have these brainwaves, some of that might be an artifact of cognitive science, which has got like the input process output.

at some level it's it's extracting like alex constant talks about extracting you know information from quantum noise noise you know random fluctuations and all this sort of stuff so you know there's it could be it could be that discrete state space is kind of you know sort of also because the

there is quite choppy what's coming in.

And at some point, inferences have to be made that makes it more like a signal, if that makes sense.

Now, I'm just putting that out there as another layer at the lower levels of the retina and stuff like that.


SPEAKER_08:
Sounds good.

Any last comments?

Otherwise this was super interesting.

I guess a closing question for the authors would just be when, when's the next episode in this paper saga or what's the next, what's the next thing you're excited about here?


SPEAKER_04:
We have another paper that's kind of, I don't know, maybe like three quarters done or something like that.

That's kind of the next step.

Yeah.

Awesome.


SPEAKER_07:
We've run all the simulations.


SPEAKER_04:
Yeah, so they're done.

It's an expanded model that does a lot more with... So it allows for a no-report paradigm, and selective attention and working memory maintenance are explicit policies in the updated one, and there's a number of other advantages, but it's coming soon.


SPEAKER_07:
Yeah, I think it's a matter of me writing it, really.

So...


SPEAKER_08:
wow too real but thanks everyone for joining this is really fun for 18 overall and we look forward to probably seeing you again for 19 and beyond thanks to all the participants you can fill out the survey for feedback in your events calendar uh invitation otherwise we'll uh be talking through other channels so thanks everyone see you later thanks
SPEAKER_08:
hello and welcome to the active inference lab this is the active inference lab and the active inference live stream today we are in active inference live stream 18.1 on march 23rd 2021 welcome to the active inference lab everyone we're a participatory online lab that is communicating learning and practicing applied active inference you can find us at our links here the

This is a recorded and an archived live stream, so please provide us with feedback so we can improve our work.

All backgrounds and perspectives are welcome here, and we'll be using good video etiquette for live streams.

Today, we're in live stream 18.1 in our first of two participatory group discussions on the Predictive Global Neuronal Workspace paper, and we're with the authors here, Ryan Smith and Christopher White.

So thanks to both of you for joining today.

Today in 18.1, the goal is just going to be to discuss and learn and ask some questions about this awesome paper.

And we're going to go through some introductions and warmups.

And then I believe Christopher will have some slides to share with us and set some context.

And then we'll just proceed with probably a bunch of questions that we all have about the paper, as well as some questions from the live chat.

To get started, though, let's just go around and introduce ourselves and we can each just give a short introduction or check in and then pass it to somebody who hasn't spoken.

So I'm Daniel.

I'm in California and I will pass it first to Dean Dean.


SPEAKER_04:
I'm up here in Calgary in Canada and I'll pass to Stephen.


SPEAKER_01:
Hello, I'm Stephen.

I'm up in Toronto in Canada, and I'll pass it over to Christopher.


SPEAKER_07:
Hi, thanks.

I'm Christopher White.

I'm a PhD student at the University of Cambridge.

And this is, just to give some brief context, this paper is really a work that kind of came out of my master's thesis that I was doing with Ryan, but we've kind of continued on.

working on.

So hopefully we can chat about both this paper and then at some point chat about some things that we've been continuing on.

And so I'll pass to Blue.


SPEAKER_03:
Hi, I'm Blue Knight.

I am an independent research consultant based out of New Mexico, and I will pass it to Dave.


SPEAKER_02:
Dave Douglas, I'm in the mountains of the northern Philippines where it's almost the end of spring.

I'm retired from IT, especially from machine translation, working on the semi-technical aspects of our education project.

And then I believe Ryan.


SPEAKER_05:
Yeah, so I'm Ryan Smith.

I'm an investigator at the Laureate Institute for Brain Research.

And I, yeah, we're in Tulsa, Oklahoma.

Basically, I run a lab focused on computational neuroscience and computational psychiatry.


SPEAKER_08:
Well, I'm sure we'll talk a lot more about that.

Let's maybe just go right to Christopher's slides and then we can pick up just with conversation or with this slide deck as needed.

But Christopher, go ahead and share your slides and I'll make them the full screen.


SPEAKER_07:
Yeah, sure.

Okay.

Okay, so I'm assuming you can see my hand.

yeah screen are you seeing we're seeing our jitsi ah okay how about now yep looks good okay well i now i it seems like i can't both look at the even with the dual monitor set up it doesn't seem like it it wants to share the same screen that jitsi is on and that's okay so it means i just can't see anyone um so just give me a shout um so this this is these are slides that

from a talk I gave at UCL about Midway through last year.

And so I may need some reminding of bits of this paper because we did write it quite a while ago, although it did only just get published in December last year.

Okay, so the idea with this paper was really just to kind of formalize some things that had been around in the literature

for a while and namely putting global workspace theory into kind of a predictive processing framework.

And so there'd been, so Carl Friston has a really nice 2012 paper that kind of did this somewhat formally.

Jacob, but it didn't really connect that deeply with the global workspace literature at all really.

Jacob had in chapter 10 of his 2013 book, Jacob Hovey, sorry,

has a really wonderful chapter where he discusses the relationship between phenomenal unity and global workspace theory and how predictive processing framework in general and active inference kind of broadly conceived fit in with global workspace theory and how these ideas can kind of help make sense of some puzzles within global workspace theory and also kind of give predictive processing a theory of consciousness as it were.

And then in terms of, I published a brief paper kind of building on Jakob's work where I kind of said, hey, here's, that's where the predictive global neural workspace comes from, or that term, which is just a term that I kind of want to introduce just to denote basically kind of predictive processing approaches to a global workspace style architecture.

And so when Ryan and I started chatting because of that paper,

Really, our goal was to put some fairly vague conceptual ideas into a more computationally precise framework, and that's what we did here.

And so the idea was really just to use a fairly straightforward, deep, temporal, partially observable Markov decision process, solved via active inference, and show how

by kind of tuning some of the parameters, you can get basically all of the global work, a very large section of the global workspace literature and all of those empirical results kind of just falling straight out of the model.

And what's I think really important to me, so my background is in cognitive science and philosophy.

And so I did my undergrad in cognitive science and did my master's in a computational cognitive neuroscience lab.

And so, although I work on for a long time for a while is really interested in visual awareness and visual consciousness.

I was around a lot of experimentalists.

And it's really important to me that all of these ideas are testable and we actually specify testable predictions.

And so really what we were doing in this paper was trying as best we could to one reproduce a bunch of existing results, but also kind of extend the model and show where it actually gave testable predictions.

And some of those have actually been confirmed recently, which is kind of nice.

So I'll just kind of flip through some things.

There are just a couple of things I want to highlight.

And so the first thing is just to kind of define terms.

So we called it an active inference model of visual consciousness.

And that was really just a kind of, so consciousness is the way I think about it, at least maybe this is somewhat idiosyncratic, but consciousness to me is just a kind of a catch-all term.

It isn't one thing.

There are lots of sub things within consciousness that I think can come apart.

And so Block kind of very famously in 1995 proposed the term access consciousness, where this is just content that can be reported from the first person perspective.

And it is available to kind of cognitive control processes and working memory.

That's not a quote from him.

That's kind of me paraphrasing, but that's roughly speaking the idea.

And so that allows room for there to be other processes or unconscious processes, say in the visual system,

of which you aren't aware and, but can nonetheless influence behavior.

And he then also proposes, say his chief in that paper, his chief kind of worry is distinguished between access consciousness and phenomenal consciousness where phenomenal consciousness is kind of the experiential aspect of this.

And he thinks that they, so this is now, and he thinks that they can come apart and that's what's controversial.

So I don't personally agree that they can come apart, but I think it's a,

a useful distinction nonetheless and so i think in terms of thought experiments i don't remember where it might not be it might be in that paper it might be elsewhere but he's kind of talked about the example that at least to me was really intuitive was if you imagine someone with a jackhammer or if you're doing if you're working at a desk and you can hear a jackhammer in the background you might

At some point, you will become aware of the jackhammer, but it was kind of a part of your experience all along.

You just weren't aware of it, in a way.

And to me, that sounds rather odd, but that's fine.

We can set that aside.

We're just defining terms here.


SPEAKER_05:
Yeah, I mean, obviously, I agree.

I mean, I think there's lots of cases like inattentional blindness and change blindness and things like that where you might think you were aware of something the whole time, but actually you weren't.

But anyway, side thing.


SPEAKER_07:
Yeah, exactly.

I think like there are people I think I've forgotten who coined this term, but it's like the refrigerator light illusion.

Like you open the refrigerator and the lights on.

Do you think it was always on?

Yeah.

I have a nice analogy.

OK, so just briefly about the global workspace.

So this idea kind of started in the late 80s with Bernard Barr's and was then Stan Hahn's group really just took it and ran with it.

And so the idea is roughly speaking, there's this prefrontal parietal network that connects in like kind of motor processes, attentional processing, perceptual systems, memory, and all of these systems are in some sense competing for access.

And they are otherwise relatively kind of modular and disconnected, but when they, what is access to the global workspace or the content that kind of wins

Gains access to global workspaces broadcast throughout the system.

So it enables kind of otherwise isolated subsystems to share information in a common format.

And he's identified this with a prefrontal-parietal network.

And this, like prima facie, there's quite a bit of evidence for this.

So across meta-analysis in report paradigms,

So actually people kind of like actually report their experience in binocular rivalry and phenomenal masking.

There is a network of frontal parietal regions that is active for conscious, but not unconscious.


SPEAKER_05:
You're cut off.

We can't hear you.


SPEAKER_07:
Oh, really?


SPEAKER_08:
I can actually still hear him.


SPEAKER_05:
Oh, maybe it was just me then.


SPEAKER_08:
Oh, good.

Oh, good.

No worries.

Thanks for pointing it out there, Ryan.

And yeah, continue along.

Cool.


SPEAKER_07:
Yeah, so there's basically, the long and the short of it is that across meta-analyses, there actually does seem to be quite a bit of evidence for the view that there's this fronto-paranal network.

And this story gets more complicated when we get into no-report paradigms.

Maybe we can chat about that at the end or next session, because that's kind of the work that Ryan and I are really kind of concentrating on at the moment.

Okay, I think we can, so I'll just kind of talk briefly about these two and then kind of move on to the model.

Um, so the idea, so basically in a cross event related, um, potential studies.

So this is a kind of meta contrast marketing study on the right, on the left and on the right, there's an attentional blink.

I think goodness.

Yes, it is.

Um, basically what you can see is that there's this red line on the left and there is this little P3 it's literally labeled with a P3 B here.

and there is this late kind of seemingly non-linear all or nothing event-related potential over frontocentral electrodes that seems to characterize reported versus non-reported consciousness content sorry reportable rather not not and what this actually this is actually I should just flag for anyone who's kind of actually in the neuroscience of consciousness who is watching this this is no longer considered a

neural corollary of consciousness.

And I agree with that.

It seems to drive, be driven by task demands.

But nonetheless, it is still a question why it is that when something is conscious and task relevant, you get a P3B.

And that's kind of what we tackle in the paper.

And so really all of the evidence for the global workspace theory, I think it can be summed up quite nicely with this 2006 kind of taxonomy.

And so the idea is that you can orthogonally manipulate stimulus strength and attention.

So stimulus strength, you might just imagine that either might be like time of presentation.

So if you have a classic kind of masking experiment, you'll have, I don't know, it might be a Gabor patch.

Then just after it, there'll be some static noise.

And the static noise kind of masks, is the mask to the Gabor patch.

And depending on the time, kind of the distance between

the mask and stimulus, you can render something unreportable.

So, and nonetheless, it can still in some circumstances influence behavior.

And so you can imagine that stimulus strength might be something like either you can, you could get, it might be contrast.

It might be the distance between the mask.

You could do both.

It doesn't really matter for our very course purposes here.

And then in terms of,

you can also vary attention.

So either have attention to the task or attention to be distracted away.

And the idea is that when there's a weak bottom-up signal and attention is absent, you basically just get very little activity.

It's whatever activity there is, it's confined to very early striate cortex.

When you have attention is present, but the signal is still weak, you get some priming, but it's still unavailable for report.

and so at that level it starts to influence behavior then at kind of the pre-conscious level or what they call pre-conscious borrowing a term from freud um this is where this is this is attention inattentional blindness effectively so you have a very strong stimulus the gorilla can be walking across the room and wave at you it can be in the dead middle of the fovea and yet people still won't report it and the idea is the idea this is kind of activity that's still very strong it's within the visual system but it's kind of gated out of

awareness via a lack of attention at least most of the time and then when in this last quadrant here when you're attending something and there's a strong signal you get this widespread kind of bifurcation in neural activity that characterizes aware versus unaware states and so we kind of had or

Tim Jones- You personally, I had a couple of reasons, those are a couple of reasons to be a bit suspicious about this, and I actually do think web workspace theory has been updated to kind of accommodate these but nonetheless we'll go through them.

Tim Jones- So basically unconscious information can still be decoded from frontal regions so pfc activity is not synonymous with consciousness, so the idea is that you could might have.

The idea that there is just a global workspace and you enter it loosely associated with PFC and entering it is sufficient for consciousness.

This can't be right.

They have a solution to this.

They then basically introduced a Bayesian model of conscious report where consciousness also requires a stimulus representation to be separable from a noise distribution.

I really like this model.

In the beginning of my master's thesis, I spent a lot of time kind of like playing around with the code.

It's really cool.

But it's an ideal Bayesian observer.

It doesn't really give any neural predictions at all.

I don't know.

And that's fine limitation.

It's an ideal Bayesian observer.

That still would be nice if we had something a little bit more neurally detailed.


SPEAKER_08:
Christopher, could you actually just unpack that slide 13?

Just what are the disks representing and how does this relate to Bayesian statistics?


SPEAKER_07:
Oh yeah, sure.

So basically each of these axes is a source of evidence.

So evidence for stimulus Y, evidence for stimulus X, and then basically these two colored lines, the pink colored line or the green colored line correspond to different decision criteria that you're imposing on a space.

So the idea is that two alternative forced choice tasks where you're forced to choose between two things versus report something as present or absent actually employ different decision criteria.

And so you can imagine these as essentially a multivariate, each of these as a Gaussian distribution, each of these circles as kind of a univariate Gaussian distribution, where if you are in the blue, you

that's kind of stimulus to the left.

If you're all the way to the right or on the X axis, that is stimulus on the right.

Um, where, and then this green thing kind of close to the, around the origin is a noise distribution.

So this is just saying there's nothing there.

And basically what conscious audit awareness is on this account is basically these, um,

distributions or either left or right being far enough away from the origin that they don't overlap substantially with the noise distribution.

So when you impose the decision criteria on the space, you are confident that you have seen something.

Does that kind of make sense?


SPEAKER_08:
Nice.

Thank you.

Cool.


SPEAKER_07:
Okay.

And then seen versus unseen ignores the noise distribution.

It's just separates left versus right.

So really cool model.

I really like it, but yeah, unfortunately not quite as nearly detailed as I would like.

Second problem is the P3B does not equal awareness.

So these are two paradigms.

So it's now being replicated in a no report condition basically

There's really sophisticated experimental techniques that allow you to basically access or gain access to a person, make inferences about what a person saw without having them make explicit reports.

And when that happens, you see that basically the P3B, which was at one point taken to be a signature of awareness, just vanishes.

And that's a bit of a problem.

So I'd like an explanation of that.

And then the last one isn't really a problem so much as just a point of absence.

So there's a really large body of evidence showing that expectation seems to be a pretty crucial factor in determining the content of visual consciousness.

So across paradigms, continuous flash suppression, monocular rivalry, motion-induced blindness, blah, blah, blah.

There's all of these areas where to show that expectations probably along the lines of similar to attention

should be a part of part of any taxonomy or any taxonomy that claims to kind of try to be complete of the factors underlying conscious access and the global neural workspace as it stands doesn't really describe how expectations are implemented um and so our job or what we saw

ourselves is doing really in this paper was just to kind of show how we could account for all of this existing work, namely the taxonomy, and also explain how the P3B doesn't equal awareness or show how that dissociation arises very naturally.

And three, show how expectations could be plausibly be implemented and give kind of a more

give a model that generates predictions about expectations, I should say, and has a story about how they implement it.

Okay.

And I think the real thing is it's okay to revise a model.

Like revising models is, that's just good science.

And I think one of the parts

global workspace theory that made me a little bit suspicious lately is that the revisions of problem one and two, they didn't generate surplus neural predictions, really.

They were just kind of, they abandoned the P3B as a signature of conscious access and they endorsed an ideal Bayesian observer model of subjective reports.

But neither of those things, they were kind of responses

to problems that didn't themselves generate new predictions that go on.

And that's kind of a characteristic of what's been called degenerative research programs by Lakota.

And so, I mean, we can argue about whether this is a fair characterization of global workspace theory, but I think this is kind of at least a nice way of setting up where we were coming from.

Okay.

Would you like to give a primer on active inference of partial observable Markov decision processes?

I'm not sure how much more you want me to talk or whatever.

I mean, this was an hour long.

I have an hour worth of slides, so I could talk forever.

But maybe do you want me to skip to the results or just summarize the model or what?


SPEAKER_08:
Let's actually hear your take here.

This is just a really awesome presentation.

And then everybody here, maybe let's write down a question or two, so then we can have another, however many minutes you want of the slides, have about an hour or a little more for questions, and then we'll have next week for questions.

But it's really great to get this information out there, because the way you're sharing it is awesome.


SPEAKER_07:
Yeah, cool.

Okay.

Well, I'll try and be a little bit more detailed when I go through the slides, because I've kind of just been clicking through things at the moment.

Okay.

So what, just a brief, this is, this is a little bit of a point of confusion and Ryan may want to kind of come up, describe this as well.

So what is active inference?

Well, it's both a noun and a verb and as a noun, it's kind of, you can use it to describe two things.

It's a, it's just a mathematical framework that allows one to formulate models of kind of behavior using equations that are kind of justified a priori.

from a very general set of assumptions about the nature of self-organizing systems.

It's also that you can attach a process theory to that mathematical framework that allows you to give predictions about various neurophysiological variables, like firing rate of neuronal populations, ERPs, and kind of how neuromodulators like dopamine or noradrenaline affect things like firing rate and ERPs, and also behavior.

also used as a verb and especially in kind of maybe pre-2016 literature that's basically what it was it was basically an extension of predictive coding to motor control and so people often use active inference just as a catch-all for basically the predictive coding version of motor control and there's been all sorts of confusion in the literature where people are taking kind of active inference as

a theory basically of Markov decision processes, right?

And they're conflating that, and because Markov decision processes, being a decision process, is kind of a model of Bayesian decision, Bayes optimal decision making.

And then if that's conflated with a theory of motor control, or you use a motor control account of decision making, you're going to get, say, bizarre things that don't actually track what's going on.


SPEAKER_05:
Yeah, I mean, I would, that's the main point, if anything, that I would, you know, kind of, you know, fully emphasize is that, like, old active inference is a theory of motor control.

I think it's old, right?

I mean, people still do it.

But at the inference where that just means, you know, like, hold a prior constant and make a reflex arc, move the body to, you know, minimize the prediction error with respect to that prior about, say, like, you know, like, proprioceptive position or something like that, right?

That's just a theory about once you've decided what to do, how to get the body to move accordingly.


SPEAKER_06:
That doesn't have anything to do with deciding what to do accordingly.


SPEAKER_05:
you know before trying to implement right so another control thing is just you know how do i move how do i get the body to move how i want to using something predictive coding ish right whereas you know we're talking about kind of like current reference is is again it's a it's a theory of decision making um which is like totally different i really think these these these

There should be two different terms, right, for these things now.

Yeah, exactly.


SPEAKER_08:
Thanks.

And Stephen, do you have a quick question?


SPEAKER_01:
Yeah, I'd just be curious.

As you're here, you're doing work in this paper here with the neuronal context of how deductive choices are made, and it taps to the phenomenological context

questions as well do you think that's when these you see these problems happen a lot because when when people are you're trying to move between different aspects of um fields of practice which are sometimes thought about separately you know phenomenology phenomenology is often talked about in the inactivist field and you've got the neuroscience and they're often clean and separate but

when you need to bring them together and try and talk about this bigger picture, the distinction, the distinctions you're making become even more important, if that makes sense, because it seems to be a lot of conflation that happens with that.

So I'll just be interested in your thoughts.


SPEAKER_07:
Um, so I think one thing to say is, so I, I like an activism, um,

especially what's kind of been called computational and activism.

I've forgotten, there was a really awesome paper out in Synthes.

I've forgotten the name of that author, but sorry about that.

But to me, that stuff doesn't really have that much to do with what I would call consciousness or what I think of as consciousness as like a cognitive scientist, I suppose.

When I think of awareness, I think of a very sharp, my explanatory, the thing that I'm trying to explain is very specific.

It is there are these states of which you are unaware, some of which you can become aware of, say, for example, like the inattentional blindness case.

or in phenomenal masking.

In some cases, there is a visual stimulus.

In one case where it's unaware and there's a very small change, physical change to the stimulus, but there's this radical nonlinear change in whether you can report it or not.

I'm interested in that contrast between reportable stimuli and non-reportable stimuli.

Now, along with that comes when stimuli are reportable, generally speaking, there is a, we might also be interested in

in characterizing the content of experience, as it were, not just the availability for report.

So we're going to get to that actually towards the end of the presentation.

We actually do endorse a version of the Phenomenal Access Consciousness Distinction.

But really, I think to answer, to repeat myself somewhat and answer the question,

All that I'm trying to explain with this model is that distinction between content that's accessible and content that's not accessible.

Everything else, I don't really mind.

Like, people can call it whatever they like.

I'm not interested in modeling it.


SPEAKER_05:
Yeah.

So, I mean, in general, right, I mean, again, this just speaks to kind of having precise theoretical constructs and having a good kind of taxonomy of these things within a particular area.

And also, you know, if you're going to try to do any kind of interdisciplinary work, right, where you have constructs in one kind of field and you want to find some way to

map them onto, right, or translate them into what you think they ought to be, right, in some other, you know, in some other theoretical language, right, then, I mean, then also specifying that in a precise way and how you kind of recover one from the other in terms of the way they're operationalized empirically, right, I mean, it's also something that you have to do, right, to do this work well, you know, so, I mean, like, you know, Chris already started, right, to kind of describe people

really loosely, at least not in all cases.

Obviously, some people are very precise.

But in some cases, the word consciousness or awareness gets thrown around to mean a ton of different things, which makes it confusing and also makes the term imprecise and therefore not really all that useful.

For instance, there's a distinction that people make between what we're talking about, whether a particular content, particular represented content in the brain, if people report experiencing it versus if people report not experiencing it.

But then there's also some people talk about levels of consciousness, which is a totally different thing.

That's like being in a coma versus being in a minimal coma.

you know, like a vegetative state versus, um, you know, being like awake, but really sleepy, you know, versus being really alert, right.

That kind of thing.

Um, you know, and there's, which is a totally separate thing, right.

I mean, you can kind of think of one as a, as a precondition in some sense for another, right.

Like you have to be in a level of consciousness that corresponds to being awake, right.

To have any, you know, or, or again, or like dreaming, right.

But your brain has to be in a certain general state of consciousness level-wise to be able to have the experience of one particular content over another, right?

So that's just saying being awake is a precondition for studying the thing that we're interested in, which is why you become conscious of one thing versus another given very similar stimulus conditions.

Yeah, exactly.

You know, so there are, I mean, and there's more, right?

Different distinctions, right?

Like, I mean, like Bruce said, there's this, you know, distinction between, or, you know, potential distinction between phenomenal consciousness, right?

Just like the, what it's like to be experiencing something versus this access consciousness thing, which is roughly being aware of the thing you're experiencing.

And like I said, several people think

this kind of thing can come apart.

I also am very skeptical about that, you know, e.g.

the fridge light example that Chris mentioned earlier.

You know, I mean, like I said, at the end of this paper, we do in some sense describe how our model can identify something like

something like two different types of access, where one type of access is kind of what's minimally necessary for empirically verifiable phenomenology, whereas the other is more kind of what determines the actual reporting.

So there is something like a distinction in our model, but it's not the standard kind of phenomenal consciousness versus conscious consciousness distinction.

Um, but anyway, I mean, there's, so my point is there's all these different sorts of things, you know, you mentioned like, uh, so many mentioned like the, like, you know, phenomenal, like the literature where people just kind of describe, you know, the way that phenomenology is or kind of the dynamics and phenomenology and things like that.

And then also is like kind of completely different or kind of descriptive, you know, field, right.

That doesn't have to do with this distinction about what becomes, you know, experience versus not.

So, I mean, the general kind of, I mean, this is already kind of like long-winded answer, but just, I think very precise terms and very precise mappings between terms in different fields are necessary to do this kind of thing well at all.

And that's not always the case.


SPEAKER_07:
I, yeah, just to briefly follow up on that.

really what i'm interested in this is this kind of was put out in bernard bars he called it the goodness i might butcher this i've gone um basically a minimal contrast approach where you have some content of which you're aware some content which you're unaware and up to all of a up to some limit the the physical properties of the stimulus should be almost identical there might best be 10 milliseconds extra between the mask and the stimulus

And that's the only thing that renders it.

So then the idea there is that we can contrast the neuronal kind of consequences of being aware of a stimulus versus unaware, independent of hopefully the physical properties of the stimulus.

And so, so just to give an example and just to make sure that I'm being charitable.

So there was recently a special issue of, I'm fairly sure special issue of the journal of consciousness studies on like

consciousness and plants.

And I guess one thing that I'm not entirely convinced of is that we're really talking about the same thing.

I think that's great.

And people smart, we should let smart people do what they want and study all of these things.

But I'm not convinced, for example, that the process or it will take a lot of convincing to convince me that something like phenomenal conscious, whatever you want to call phenomenal consciousness in a tree or a plant is

really has anything at all to do with this minimal contrast approach in like the neuroscience of consciousness.


SPEAKER_05:
Yeah.

I mean, one, one other, one other thing, again, just like things kind of popping up in my head here.

Um, I mean, it seems like we have a fair amount of time, so I don't think it's problematic.


SPEAKER_06:
Um, and it's going on a bit here, but, um, um, well, okay, wait, I, now I totally, uh,

I totally had a blank on what I was going to say.

Well, I guess we can move forward.


SPEAKER_08:
Okay.

Thanks, Brian.

Also, maybe you could talk like a little closer into your microphone.

It gets a little quiet sometimes.

Okay.

Thank you.

All right.

Continue.

And I'm really curious to see how active inference is going to play into this.


SPEAKER_05:
Oh, sorry.

Sorry.

I just remember what I was going to say.

So I think another thing that sometimes gets plated is, is just representing things versus being aware of them.

Um, right.

So, so, you know, like a major, so it's a, for example, right.

I mean, in the, in the act of the inference literature, there are other papers that describe, um, you know, things like Jacobian monism, right.

Is something that like, I'm like one of Carl's recent papers, um, you know, where, where more or less it's a story about how via the free energy principle, you can come to have systems where the internal states of the system

From the parameterize or track in some way the states of the world outside of the system Right.

So so in a really broad sense you have some some some way in which the internal states are Representing or keeping track of right the external states That as a kind of representation Or that kind of seeds of internal representations of what's kind of going on out in the world but the idea of

you know, with this sort of literature is, is that the brain can represent a bunch of stuff at the same time, but only some of the stuff that's being represented becomes conscious at any given moment.

Right.

So like I can flash like a word.

I can flash like, so unlike some studies, for instance, I could flash the word guilt for you really fast.

Um, and you won't report ever having seen the word guilt, but you'll, you'll actually behave in ways that are more consistent with, uh, feeling guilty or something like that.

Right.

Or, or I mean, simpler like perceptual or semantic priming examples where you might flash something really quickly.

Again, a person doesn't, um, you know, say they are aware of having seen anything, but

They'll say complete word stems in one particular way versus another, right?

If I flash an elephant for you really fast, you don't know I see it.

You don't know you see it.

And I give you like a word stem like E-L, you know, and it's like complete the word.

People are more likely to say elephant than they would be if you didn't flash it, right?

Or something like that, right?

So behavior is affected by something that was clearly caught, right, by the brain.

But despite that it was represented, you didn't become aware of it.

Right.

So I just want to make that distinction as well, that being conscious of something is distinct from whether it, whether the brain actually, uh, in some sense knew it was there.


SPEAKER_07:
Yeah.


UNKNOWN:
Yeah.


SPEAKER_07:
Okay.

So, um, just the idea.

So we used a partial observer Markov decision process, um, as our model, a hierarchical one, I should say.

Um, so this is just a single level Markov decision process and just very briefly.

POMDPs, I'm just going to call them MDPs just for brevity.

They describe transitions among kind of hidden, unobservable variables in the sensory data they generate.

So you can have a hidden state of the world and the observations generated by that hidden state.

And the goal of active inference is to infer the states and the sequences or action sequences or policies, pi, from the set of observations.

And so I'll just kind of step through very basically how this would work.

So you could just have a very simple case right here where this is just a graphical representation of Bayesian inference.

We can just have a hidden state, and we have a prior over that state, and we have some likelihood mapping between that hidden state and some state of the world.

I think I'm getting feedback from someone's computer.


SPEAKER_08:
Okay, try again.

I think it's yours, Ryan.

I can see if I can... Continue, Christopher.

Thanks.


SPEAKER_07:
Okay, thanks.

That's better.

Yeah, it's funny how that messes with you.

We've got predictive models of when our speech timing should be coming in.


SPEAKER_08:
At least we know that you're awake.


UNKNOWN:
Okay.


SPEAKER_07:
So we've got, yeah, so this, this comes out to basically, if you then invert this model, this is just exact Bayesian inference.

Um, now if we start moving through time, we have to add in transition probabilities.

And so this just imagine this first level of model is just a Markov chain.

It's just, you have some initial state, you transition into your first state, then you have state transitions that govern how you, how those states change over time, but you also.

have variables that you don't ever get to see this Markov chain.

All you get to see is a set of observations.

And so the idea is these observations are generated by these hidden states.

And so the task of inverting these models is to infer the most likely hidden state conditioned on some stimulus.

And so you get this little sigma here is a softmax function, and basically you can derive message passing algorithms, or using message passing algorithms rather, you can derive very simple update rules that allow you to come up with posterior probabilities over states.

And then actions under this framework are then just kind of state transitions that the agent has control over.

So you can imagine in like a psychophysics experiment, the actions, this is a super limited set of action space for the agent.

Basically they can hit left or right and they might be able to move their eyes.

So those would be the policies that they can control.

Everything else, the state, the screen, those are hidden states, but they're just not transitions.

Transitions between those hidden states aren't things that the agent can control.

Okay, so then in a way, so how do we select actions?

Well, we select actions according to the frangifunctional.

So this has two component and you select the action or the policy rather that is kind of,

best minimized expected free energy, where this has two terms.

The first is the expected cost.

So this is kind of the KL divergence or the difference between our predicted outcomes, which is this Q here, and our predicted outcomes under some policy.

So what the outcome I expect, given that I take a certain action.

and the predict and just our um and our preferred observations so these are things like having preferences for winning versus losing um where the agent generally but you can encode the agent wants to win or not to lose or something like that the second term here this is the exploratory component this is the expected ambiguity where this is basically the expected entropy

of the likelihood mapping.

And so the idea here is that you seek out actions or seek out states of the world that have a precise mapping to sensory outcomes.

And so, for example, if you're in a hotel room that you've never been in before, it's really dark, the best way, best actions to take to minimize your uncertainty is just to turn the light on.

Because that will give you a really precise mapping between the hidden states of the world out there

And kind of the observations that you generate, all that they generate.

And so the idea is that expected free energy is minimized when both of these terms are minimized.

And it's actually a little bit more complicated than this, I should say.

So policy selection isn't just the minimization of expected free energy.

It also involves minimization of variational free energy.

But I would just say, what's the model stream to get into that?

For our purposes in this conversation, we can just say policies are selected that best minimize expected free energy and variational free energy, where variational free energy is also basically a stand-in where the optimal posterior here that you infer will also kind of, you infer the optimal posterior by minimizing variational free energy.

Okay.

So what about, I haven't really talked about the brain so far.

One of the beautiful things about active inference is that it comes with a process theory.

And so belief updates, that is changes to your posterior overstates correspond to changes in activation levels in response to input.

And the matrix entries here, D, B, H, C, blah, blah, blah, blah, these are all,

H actually isn't really a matrix.

I guess it's a matrix entry, sure.

What is H?

Entropy.

So you can just... These correspond to synaptic connection strengths.

I mean, this is just like a very standard assumption across neural network literature, really, and just in neuroscience in general, where synaptic changes in weights of synaptic connections...

correspond to some kind of like function approximation.

Okay, and so then you get these kind of somewhat intimidating looking equations.

We can just select the ones that are really relevant.

So this first one here is state prediction error.

And so minimizing state prediction error in perception corresponds to essentially the inference of the posterior probability

q of s. Whoops.

And you can cast this as a gradient descent on variational free energy.

And that's actually where we get our ERPs from.


SPEAKER_08:
Christopher, could you go into a little detail what the variational means independently or as related to the free energy here?


SPEAKER_07:
Yeah, sure.

So variational free energy is just to distinguish it from something like thermodynamic free energy.

So there are formal relationships between these things.

I would just say, read Carl Friston's work if you want to know about that.

But basically, variational frangie is the KL divergence between a generative model and your approximate posterior distribution.

And what you're trying to do is change the approximate posterior distribution slowly but surely.

So change it bit by bit.

So you nudge it in the direction of steepest descent.

until it basically, this error term here, which is basically the difference between the log difference between your generative model and your approximate posterior, you change S until that difference is at the minimum.

And the rate at which that changes, so this little V dot here is your ERP.

So the idea, um, and I'm going to go into that a little bit more in a moment.

Um, so the idea is that you have our log probability of our posterior is essentially a non-normalized log probability, and it can be positive or negative like a voltage.

Um, and then you have, you can have your ERPs and this is just like your change in membrane potential over neuronal populations.

And that corresponds to roughly speaking like these big potentials we see at the scalp.

And then we have a normalized firing rate where you will basically, we normalize the depolarization variable by branding it through a softmax function.

And the softmax function is just a generalization of a logistic function to more than one input, more than two classes.

And what's nice about using a generalization of a sigmoid function is that everyone who's looked at, like, rate models of population nodes, for example, or what's sometimes called mean field models, these basically, you can have this firing rate versus input curve, where on the x-axis, you have the amount of input that you're driving into the population, and on the y-axis, you'll have the firing rate of the population.

It's a logistic function, so it's going to be bounded, roughly speaking, between zero and one.

So what you'll end up with is this kind of sigmoid-style looking curve that we treat as probability distribution, but can also be interpreted as kind of this lovely biophysically plausible, well, I don't know about plausible, but biophysically sensible, maybe, interpretation as a normalized firing rate.

And that kind of allows us to translate between this kind of normative models, thinking about probability distributions and what the brain is actually doing.


SPEAKER_08:
Okay.


SPEAKER_07:
So everyone kind of happy with that?


SPEAKER_08:
Yes.

Steven, do you have a quick question?


SPEAKER_01:
Just one quick question.

Just is the expected free energy on policies more important at level two and level one

it kind of can soft max itself.

Um, or is it, that's a bit simplifying, but does, does expected free energy go all the way down or is it a bit more higher up?


SPEAKER_07:
It depends how you set up a model.

You can generally have models that, uh,

You can have policy selection at both levels, you can policy selection at just the lower level or just the higher level, depends what you're trying to model really.

Generally speaking in models, so in the model we're going to be using, I'm going to introduce in just a moment, we don't have policy selection at the first level.

We have policy selection at the second level.

But you might, for example, there's work by Thomas Parr and Ryan and I kind of working on some similar work as a part of my thesis where

Basically, the idea is that the higher working memory-ish prefrontal level, you have goal-directed policy selection.

And at the first level, you just have kind of epistemically guided policy selection.

And so the policies that are being selected at the first level might be something like where in the visual field to point your attention or to move your eyes.

And that's just kind of

determined by the salience of various items, which in this framework corresponds to essentially the information gain offered by all the precision of the first level A matrix.

So hopefully that answers your question.


SPEAKER_01:
Yeah, thanks.


SPEAKER_05:
So as just a little bit of kind of a generalization of that, I mean, really, and Chris will go into this, but

But the way this is modeled officially is just each lower level trial is just a full trial, as in if you were modeling it separately.

The higher level is just putting a prior on whatever the starting prior is for each lower level trial.

So I mean, in a sense, you're modeling these trials.

You could think about each trial either at the lower level or the higher level as being modeled

independently in some sense.

Each one can have a set of policies.

Each can have anything that a single-level model could have.

It's just that the higher level is putting priors of some kind on the lower level.

And then posterior observations at the lower level are then feeding back in as the observations for the higher level.

So just think about them as just separate kind of trials

on different timescales where they're just connected by their priors and posteriors.


SPEAKER_07:
Yeah, exactly.

And so I guess what we did here was we wanted to have something like to implement something global neural workspace like in a deep active inference architecture.

And what I want to highlight, I have mixed feelings about defining the term the predictive global neural workspace because people lump this model in.

The few people who have been kind enough to cite this work have

lumped us in into like models of this as being kind of just another version of the global neural workspace.

I don't think that's accurate.

We took the global neural workspace kind of and all of the phenomena that it encompasses kind of as something that we want to explain.

Both Ryan and I really like a global neural workspace, like Stan DeHaan, Bernard Barthes, huge heroes of mine.

And aside from kind of like my personal love of the theory, it's also interesting

among kind of neuroscientists and philosophers who work on consciousness about 30, the majority of them, which is about 35% or something like that, uh, in 2018 said that they regarded it as the most promising neuroscientific theory of consciousness.

And so we thought, okay, if we're going to go into this literature, this is what we should be modeling.

Like the phenomena encompassed by this theory is where it should be the starting point.

Um, but with that said, it's, our model is an active inference model through and through.

Um,

And so it really does kind of stand on its own independently of the global neural workspace.

And so I just kind of wanted to flag that.

And so I don't use those words or in the draft of the paper that I'm writing at the moment, kind of following up on this one, I don't use those words and I don't do it.

That's very deliberate.

But anyway, sorry, that was an aside.

That was a very esoteric aside of that.

Anyway, so the idea here is that we implement something global neural workspace like in a deep temporal

deep temporal architecture.

So this second level, we identify, centrally deep level, we identify with the workspace.

And so as Ryan was just talking about, you have these states, the second level, which are really like beliefs about how trials at the first level evolve over time.

And so they provide a prior at each time step of the lower level, they provide a prior over the hidden state.

And then when there's posterior inference at the first level, after that's happened, it then shoots back up to the second level.

And that acts as an observation for the second level.

Okay.

So the idea here is that the functions we typically associate with conscious processing take place at the level of the hierarchy that's kind of deep and temporally deep enough to abstract away from processes that are entrained by the moment-to-moment sensory flux.

And this allows the agent to kind of construct a report of its experience or to do any other kind of quite abstract cognitive task.

And so the idea is that ignition occurs when a first level state is inferred by the second level.

So ignition, sorry, is basically like what Dahan has called this nonlinear bifurcation of this frontoparietal network in the neuronal activity.

So the idea is that ignition for our model occurs when the first level state is inferred by the second level

with a high enough posterior probability to kind of influence policy selection at the second level.

Whoops.

So, okay, onto the task.

So this is a very kind of general task structure, which we borrowed from a really awesome empirical study done by Michael Pitts in 2014 and colleagues.

So the idea was at the first time step, there was a forwards mask presented and so,

Second time step, there's a target stimulus.

So these little lines in the center can rearrange themselves into a square.

And at the second time, there's then a backwards mask at the third time step.

And the agent is then asked to construct a report of its experience, so whether it's say whether it saw something or not.

And the idea, roughly speaking, these little colored circles on the outside, this is how we manipulate attention and how Michael Pitts manipulated attention.

And so they either had people attend to the inside and perform a task, basically click a button whenever the square changed or click a button whenever the circle changed.

And so they manipulated kind of what was task relevant and I'll get into how they manipulated awareness in a moment.

But roughly speaking, the way we implemented the model, this is kind of a Bayes net representation of the model.

And then if we just kind of click through it,

At time one, there is a forward mask presented, and this just corresponds to a black with a square trial.

So that's the full, this is kind of a, at least the second level, there is a full sequence.

This is beliefs about the full sequence over the full trial.

So this is a time step one.

There's a report, hidden state factor here.

It's in weight because we haven't asked it to construct a report yet.

Then at the first level, each of these second level states kind of implies a first level state.

that makes sense.

And each of these first level states kind of implies a observation and attention in our model was basically, we just hard coded in as a, but we didn't have to hard code it.

You can get these things to emerge.

We just, it's just a little bit easier in our case to do it this way.

We basically hard coded in so that attention corresponded to modulations of the mapping of the precision of the mapping between

his first level hidden states, kind of whether there was a square or lines out there in the world and the observations.

So, okay.

So first, first time point forwards mask, we see a sequence type is going to be a square.

There's a trial phase one report.

Okay.

Next one, we see trials moved to, we've now see the target has now rearranged itself into a square.

We did this for two time steps.

And we did the reason we did it for, we present the target stimulus.

The two time steps is to allow recurrent feedback processes from the first level to kind of the first level to second level and recognize what's going on and feed back on to the first level.

Kind of give that, that was kind of our way of modeling this ignition style process.

Then fourth time step, we take away the mask.

We replaced the little square of just the mask again.

And then from five to eight,

that we allow the model to construct a report of its experience.

So basically this scene hidden state here implies a sequence of language processing states at the first level.


SPEAKER_08:
I see a square.

Yeah, just read it.

It says, I see a square.

And then what would it say if it was unseen?


SPEAKER_07:
I didn't see anything.

I mean, and to be fair, just to be clear,

we could have and the model would be exactly the same in all aspects we could just chop off this kind of language processing part of the model and the model would behave exactly the same way and we could still use it to generate reports and all of that stuff what we wanted to do was really just kind of use this as kind of an intuition pump for what having this deep temporal representation

allows models to do.

So holding something at that level allows sequences or a cascade of hidden states to kind of evolve at lower levels.

That might be something like constructing a sentence with a goal in mind, or it might be something like, I don't know, making a cup of coffee, where you have to hold multiple tasks in mind and break things down into subcomponents, blah, blah, blah.


SPEAKER_08:
Yep.

As opposed to mapping to button that says yes or no, if it's just a two button option, then it's a one time step paradigm.


SPEAKER_07:
Yeah, exactly.

I mean, we, we could do that.

I mean, it'd be completely identical.

Um, this is just kind of a, a nice way of showing what these models can do really.


SPEAKER_05:
Yeah.

I mean, so this was, this was partially for a, for a kind of, to make a theoretical point, right.

That is you could say, right, we're showing a two level model here.

But, you know, the brain probably has, you know, I don't know how many levels, but a very large number of levels, right?

Not just two.

So, you know, you could kind of level the concern or question at us that like, you know, why is it that this second level is the one that corresponds to conscious access, right?

As opposed to just any other level that's above some level, right?

In the brain, since there's a big hierarchy in the brain.

know so the the idea here is is that you know we're appealing to a certain level of deep temporal structure right like um you can you can flash a stimulus at somebody and they can see it over a very short time scale right of like you know half a second or something really fast but actually but the sorts of cognitive processes that um can integrate that together

right and generate you know a set of thoughts right that corresponds to being aware of that right constructing a um constructing a sentence or any of the things that chris mentioned um those are much deeper slower integrative cognitive processes right so the idea is that there is a level in the brain that is integrative enough and that represents things over a deep enough temporal scale

that it has the minimal resources to integrate things together, hold them in mind, generate sequences of the sequences of words that would be necessary to communicate that you saw something or not, right?

So it's that abstract deep temporal structure that is the necessary conditions for what you need to get empirically confirmable awareness.


SPEAKER_07:
Yeah.

Okay.

And just some brief kind of modeling notes about what we did.

So to model attention and stimulus strength, we basically just altered the precision of the first level A matrix mapping by passing it through a softmax function twice where one of the softmax temperature.

So when you pass something through a softmax function, you have a temperature on it and the temperature decides how precise it is.

And we just had a different temperature for each iteration of the softmax function.

And so the idea was then we had an ordering basically of precision.

where the most precise was attention present, stimulus strength high.

The second was attention absent, stimulus strength high.

Third was blah, blah, blah.

You can imagine how that goes on.

And the reason, this sounds pretty abstract and a bit cooked up.

And that's true to an extent.

But I think this does actually really correspond to, so if you look at something like divisive normalization in vision,

where attention basically is a, you will have some tuning curve, neuronal tuning curve, and basically like how precise that tuning curve is, is modulated by both the contrast of the stimulus and then the contrast, the stimulus gets fed into some attentional process where that will together determine kind of the tuning curve.

That was a very kind of rough and ready explanation

tuning curves, but whatever.

And then our threshold for report, we basically just set up the model to have a preference for receiving correct versus incorrect feedback at the final time step.

And 2EF behavior was basically modeled by simply decreasing the preference for being incorrect to encourage guessing.

And so I think this was just kind of our first pass at doing report.

I don't think this is how the brain does report.

We actually, in our follow-up model, we've got a much nicer way of doing this, but I think it worked in the sense that it was able to kind of quite accurately capture a number of empirical findings.

So that's kind of good enough for me at this first model.

Okay, sorry, that was a lot of preamble, but we can get into the results.

So foundational simulation.

This was just like kind of full, like a very high precision,

stimulus was presented to the model kind of with either on a square present or a square absent trial.

And this is just kind of to test it.

So great.

When there was a square present, the model reported seeing it 100% of the time.

You see this nice kind of bump at the second level.

At the first level, where the firing rates for square increase when we present it, it goes down.


SPEAKER_05:
So Chris, let's just make sure that people understand how to interpret these things.

So just so people understand, so time in these plots goes from left to right, and the gray scale from white to black, black corresponds to 100% probability, and white corresponds to zero probability.

And the gray, so it's something like 50-50.

And these are cast in terms of firing rates here, so darker than implies a higher firing rate.

So the top plot on the left that says second-level firing rates, that just means it first sees the black circles on the outside.

So it's confident that it's either going to be the sequence that has black circles and black square, or the second row, the other sequence, which would be black circles and just lines with no square.

Um, so basically once it sees the black circles, then it knows, okay, this could either be a sequence where there's going to be a black square or when there's not going to be a black square.

Um, so then at the, and that, and you can tell what the stimulus was because if you look at the first level firing rates, so again, the next kind of plot down the bottom, um, row there corresponds to seeing the lines and the top one corresponds to seeing the square.

So basically what happens here is it starts out just seeing the lines.

And then at the second time step there, you can see it kind of bumps up and the agent's really confident that it's seeing a square, which at the second level corresponds to becoming fully confident that it's gonna be the sequence of black circle and square.

And then the stimulus goes away at time point four, which you can see at the first level and then stays again, just lines for the rest of it.

And then the bottom two plots show the actual outcomes, where again, black is the agent's sort of confidence in what it did and the cyan dots correspond to the actual ground truth.

So that's just showing that the agent waited for the first three time steps and then reported seen and kept that seen while it generated the sequence of words at the following four time steps.

And then the right one just kind of shows it started out silent for the first four time steps.

And then the words that it observed itself report where I see a square, which is just those dots moving down the diagonal to the right.

So that's just so people know how to interpret what these plots mean.

Yeah, exactly.


SPEAKER_08:
And another clarifying point would just be that the same simulation is leading to all of these behavioral outcomes and neural correlate predictions and a bunch of other things.

So it's like one integrated model and the model stream is kind of where they built it up.


SPEAKER_07:
Yeah, exactly.

Okay, so we kind of talked through all of that.

Okay, so in terms of the taxonomy here, what we wanted to do was basically recreate that taxonomy that we talked about earlier.

So this was modulating signal strength from high to low and from kind of, and whether attention was present or absent, and then looking at the corresponding firing rates and looking at like the percent scene and percent correct, that kind of thing.

Also, percent scene is report behavior and percent correct, which is forced choice behavior.

Okay, so kind of a couple of things to highlight.

Firing rate first level is enhanced kind of most strongly in the conscious quadrant.

That's nice.

That's again what we see in empirical findings.

High firing rate for kind of square sequence at the second level, even when reports are low.

This is kind of nice, right?

Because I talked about earlier, you can have kind of a firing rate at the set.

You can have kind of activity in prefrontal regions that is generally speaking higher

below the threshold for report, that is nevertheless, oh, sorry.

When I say below the threshold for report, report frequency of reports are relatively low, but you can still have a preference for, you can still have a higher firing rate or the representation of showing that there's a representation of the stimulus basically.

And then the last thing to highlight is this,

the highest firing rate for the kind of the square sequence at the second is at the second level is in the conscious quadrant, which we kind of might think about in terms of ignition.

Okay.

So then ERPs, we see the real thing to highlight here is that we see a large P3 like event related potential in the conscious quadrant.

Again, like a lot of the empirical findings in the report literature.

Whereas first level ERPs seem to be fairly unmodulated by conscious access, essentially, which is, again, what we see.

So now to a very, that was kind of a very abstract kind of taxonomy where we're just manipulating factors.

What we wanted to do here was show, take a concrete empirical result and show how we could basically use literally exactly the same model to reproduce it.

So this is a really clever paradigm by Michael Pitts that I talked about before.

And so the idea is that you will have these kind of three phases to an experiment.

In phase one, the participant is just told to attend to the dots on the outside of the screen and hit a button when they dim.

And what the participant isn't told is that every now and again, these lines in the middle will rearrange themselves into a square pattern.

Then at the end of the first phase, they seek like 300 trials.

And at the end of the first phase, they are then given a debrief and they asked, all right, so how many people saw a square pattern in the middle?

And roughly speaking, it's about 50%.

And this is now actually really well replicated result across kind of different varieties of inattentional blindness.

Roughly speaking, about 50% of people don't report seeing it when it's not task relevant.

So that's nice.

So then they,

Then all of those, so then they have 50% of people who didn't report seeing it.

And then in fact, but are now aware of it.

And then phase two, they do exactly the same tasks.

They're attending to the disks on the outside.

So it's irrelevant.

still task-relevant, but they're now all aware of it.

And in the debrief at the end of the second phase, everyone reports seeing it.

And so now they've got this contrast between aware versus unaware.

So phase one versus phase two, where the only thing that's different between them is whether you're aware of the stimulus.

And the stimulus in the middle is task-relevant.

And what you can see, there's no P3B.

There's no late positive central component in the ERPs.

Then the final phase, aware task, the kind of the aware phase, so all the participants are aware, but they now make a task relevant and have them basically attend to the dot, to the squares in the middle.

And what you see there is this big P3B.

Very noticeable.

Okay.

So just to kind of sum up this slide to summarize what I just said.

Then great.

So basically what we did was we just used a different inverse temperature parameter to modulate, to kind of represent each state.

So phase one, very low temperature parameter for attention, attention, attention, temperature parameter.

That was kind of, and so this is, you can kind of see the precision of the A matrix.

Here we bump up for phase two, we bump up the inverse temperature parameter just very slightly.

This is kind of roughly speaking to correspond to like a diffuse attention parameter.

So you're not particularly attending to one thing, one particular feature of the stimulus.

You're attending just kind of diffusely to the whole screen.

And in the last state, we basically model the attention to make task relevant by making a very high or very precise temperature parameter where you basically get almost something that looks like a likelihood, like an identity mapping.

Okay.

And so these are the ERPs and the firing rates that you get out of it.

And this is my favorite slide in the whole plot.

Oh, in the whole paper.

What you see is basically results that look very, very, like, shockingly similar to the empirical results, especially for a model that is so incredibly simple, like ours is, relatively speaking, at least in terms of biophysical detail.

And so what you see is at the first time step.

So there's basically no ERP.

So first phase, basically no ERPs.

And so we, we give the model feedback on every trial, but what you should see is it's a little bit darker for the square for the, until we give it feedback and tell it, no, you were wrong.

You did, we did actually present squares and the model changes its mind.

It was fairly sure that it had seen lines.

Then the,

The second one, we bump up the tension parameter, and now the model sees things, and it reports seeing the stimulus.

So I should say, in phase one, our model saw the stimulus 49% of the time, basically identical to the empirical result, which is 50%.

Phase two, it saw it 99% of the time, basically the same as the empirical results.

Report phase two is 100%.

And then phase three, it's 100% of the time.

Um, and there there's little, but by the way, I should say what the way I generated these results is basically I ran several hundred truck.

I basically set up, just ran a four loop over the stimulus, over the code and for each condition and had it generate, I think.

So each kind of iteration of the four loop generated 300 trials.

And I then averaged over maybe 10 iterations of that four loop.

So, um, each with a different random seed.

And so what you should see, sorry, that was a quick aside.

The crucial thing is that in this last point, there is this, when there's a very precise stimulus mapping at the first level, there is this abrupt change in firing rates at the second level.

And that abrupt change corresponds to the appearance or the emergence of the P3B, where the P3B, where remember that ERPs are generated, they are the rate of change of posterior beliefs.

And so the model suddenly becomes very confident that it saw a square.

And so you see this big ERP boost in the P3, which corresponds to a very precise input mapping that comes along with something with a stimulus being task relevant.

And so there we've kind of got this really nice explanation of this kind of somewhat counterintuitive result about the dissociation of the P3 and awareness.

Okay.

And then the last thing we're going to touch on is just kind of we then decide to extend the taxonomy.

So basically, this is exactly the same, but we also now manipulated whether a prior was consistent with, i.e., a valid expectation.

So we gave the model kind of we made it, roughly speaking, two times the model thought it was, roughly speaking, two times more likely to see a stimulus than see a square than lines.

And or two times more likely to see lines than squares.

And then we presented it with squares.

And so that's kind of the consistent and inconsistent prior.

And then the key result is that across all of this, expectations boost the effective feedback on firing rate, which is nice.

And in terms of ERPs, what we see is something, this very specific prediction.

when there is so these are kind of like fairly small differences but they are differences um only when attention is present do you really do you see a p3 that's again in line with the empirical literature but crucially when there's a consistent prior of or a valid base to say a valid expectation you should see a reduced p3b in contrast to when there's kind of flat priors

and when there is an inconsistent prior.

And this is actually something that has been empirically, that has been confirmed recently.

Again, this was something Michael Pitts was on this paper.

If anyone's interested, I can kind of post a link to the paper.

And that's really nice.

We didn't plan for that to happen.

Okay.

It is slightly annoying.

I would have liked to cite it in the paper, but the paper actually came out before ours was accepted for publication.

Oh, sorry.

After ours was accepted publication, rather.

That's what I meant to say.

Okay.

And just a very quick summary of the results with really what is a really kind of like surprisingly simple model.

We can reproduce a very wide range of canonical results from minimal contrast paradigms, including kind of fMRI and ERP findings.

So this includes, we can account for unconscious PFC activation.

We can account for the association of the P3 and conscious access.

where the P3, roughly speaking, reflects the velocity of conscious working memory updates.

I kind of like that phrase.

And four, we have an explicit and formally defined role for prior expectations and their implementation of process theory.

And then we've got a number of key predictions that emerge from this.

So P3 is attention dependent and should its amplitude be inversely proportional to expectation.

Feedback from frontal parietal regions.

This is a very specific prediction.

Feedback from frontal parietal regions should be enhanced on conscious trials, and it should disinhibit granular layers in the relevant sensory population sensory cortex.

That's a very specific prediction that falls out of the process theory that can be tested with DCM.

And then consistent expectation, again, consistent expectation should disinhibit granular layers in sensory cortices, while inconsistent priors should inhibit them.

Okay, so any questions at this point?


SPEAKER_08:
Christopher, thanks for this awesome presentation.

Do any of the panelists maybe want to queue up a question?

How about you do this last slide with a bat, leave on that sort of phenomenological note if you want to, otherwise we can return to the panel.

Yeah, perfect.

Yep, sounds good.

So all the panel, prepare a question or a thought and then raise your hand when you're ready and give a last thought here.

Thanks, Christopher.


SPEAKER_07:
Okay.

So just like a brief aside on phenomenal consciousness.

So we associate that.

So I'll just back up a little bit.

Sorry.

There might be a puzzle in that we associate global broadcast and the availability of information for report with the second temporary deep level.

Our experience, although it's kind of like there is a depth to it in the sense that experience is kind of smeared over time, right?

There is an integration window that's relevant for experience.

It certainly doesn't occur at the level of whole sequences.

And so there's a puzzle here.

So where does experience live, as it were?

Where is the content of experience?

And Brian and I have been chatting about this, and this is a view that's still evolving.

But broadly speaking, I think what's right about this or what we should say is that the first level states that the content that we are conscious of are the first level states that have

a high enough precision either through kind of bottom-up precision or because there's a really precise prior that comes down from the second level A matrix.

So basically the message that's being passed back up to the second level has high enough precision to influence policy selection or to pass the threshold, the posterior belief threshold for conscious access and report.

And so this kind of has the nice consequence that it keeps experience at the relevant level.

So it's updated at each time step when we get new stimuli in, and yet it has a distinct kind of link to report and behavior.


SPEAKER_05:
Yeah, so I mean, just to expand on that a little bit, I mean, the kind of idea is just that as opposed to, and again, Chris partially said this, but just to kind of reiterate, the idea is that

what you consciously experience is a consequence of both the posterior overstates at the first level and whatever the second level likelihood is.

So it's a function of both the posterior there, S1, and what the structure is of A2 there.

And a way to think about that is, if you were to rearrange A2 so the mapping was different,

And S1 had the same posterior, then that would update the beliefs at level two in a different way.

Right.

So if I arrange A2 one way, even though S1 there is still sensitive to the same stimulus, then if I rearrange A2 one way, then the posterior at S1 is going to lead to awareness of one thing, whereas if A2 is set up a different way, then the second level will gain awareness of a different thing.

So in some way, the phenomenal content that you become aware of

depends on the nature of this link between what messages the way that the messages at the second level are kind of decoded in a sense right what the second level takes to be the meaning of the messages that get passed off from the first level um so so there's this kind of nice thing like chris mentioned where it's the updates um

that the first level gives to the second level at each time point.

So phenomenology kind of stays at the temporal scale of experience.

But phenomenal content still depends on a certain type of access, right?

It depends on the first level having a specific type of influence on the second level.


SPEAKER_08:
Brian, thanks a lot.

It's really interesting.

So let's return to the guest, the panel mode.

Christopher, awesome presentation.

And the first question is going to be from Dave, and then anyone else can raise their hand.

So Dave, go ahead.


SPEAKER_02:
My question is about possible involvement of the Airtos, the reticulothalamic activating system.

either in your data or the data that are drawn on neurological data in the other relevant studies, is there any representation of possible contributions by the ERTAS, by the midbrain?

And is there any accommodation in your model of something, maybe not explicitly the ERTAS, but data that might originate there, say the endogenous attention?


SPEAKER_07:
um so subcortical structure is really interesting and as for the reticular thalamic activating system i'm curious did you did you get that from this book by bernard bars so a cognitive theory of consciousness where he first introduced global workspace theory and he has a whole chapter devoted to the reticular thalamic activating system oh that's great i will definitely have to get that no actually i got that this from uh


SPEAKER_02:
The work that Jaak Panksepp and Mark Solms did, especially the 2013 thesis of the conscious id.

He is, of course, a very close collaborator with Carl Friston.

He just brought out a full-length thesis.


SPEAKER_05:
book on the conscious id especially as um active inference interacts with that so so my my actual main focus of research is on um emotions um you know so applying computational models to emotions and um i'll just say that i um i have very strong disagreements with the um the kind of theory proposed by yach and mark we actually published kind of a joint like

on paper debate where me and, um, a colleague Richard Lane kind of debated back and forth with Yach and Mark about the right way to think of, um, the way consciousness works.

And I'll just say that, you know, their view places this really strong role of affect where they, they more or less say that these midbrain, you know, structures, um, like the PAG, right.

The, um, uh, or equiductal gray.

Um, send these afferent signals up to kind of activate the rest of the brain and that somehow permeates it with this kind of necessary emotion.

Um, and that's kind of primary.

Um, whereas, whereas kind of the way that, um, the way that we've, the kind of structure of the sort of thing that we've defended is just that no, like the global broadcasting ish kind of thing applies to all types of information.

Right.

So if you have a representation of your beliefs about your emotional state, that has to be attended to and sort of sent into the workspace, just like any other stimulus for you to become aware of it.

And so, I mean, these sorts of midbrain structures, they do a lot of really interesting, like, interoceptive and visceral regulation processes.

And, you know, they do have this sort of general, like, upward modulation role while they kind of

you know my view is they kind of keep the cortex in general in a state where it's capable of of representing things being sensitive to simulate holding different states active and other states not so it it essentially it allows things to be represented in general and then to become consciously accessible um but you know with respect to um


SPEAKER_07:
way that kind of emotions feed in and whether the uh the midbrain structures actually contribute content in and of themselves we just have theoretical disagreements about that can i also briefly follow up on that so the contrast the reason i think that frontal parietal regions in cortex are so important is because there are a huge just amount of empirical literature now in neuroimaging

And also very recently, data from kind of invasive neurophysiology in monkeys and most recently in mice showing that, yes, when there is this contrast between conscious versus unconscious, these are the things that are differentially modulated.

Now, there are some very impressive findings very recently coming from Matthew Larkham's group showing basically that

you can wake creatures up from general anesthesia by stimulating the medial dorsal nucleus and the thalamus and there are these crucial connect like reciprocal connections between layer five of the cortex and the medial dot so and the thalamus and when those goes out basically uh general they go out under general anesthesia and when you are awake or when like whiskers are stimulated in bowel cortex

when the mouse kind of reports as in it says its whiskers were stimulated, this circuit goes off essentially.

So I think the thalamus is crucial, but kind of Allah, Dahan and everyone else, I think the reticular thalamic activating system and all those subcortical structures, these are background conditions.

Similar to what Ryan said, it puts the cortex in kind of the state where it could be aware of something, but

The actual contrast between something being broadcast versus not broadcast or being reported versus not reported does not rely on those structures.

And I would just want to see from those authors, I would actually just want to see them engage with something like the visual awareness literature and show that those structures are differentially involved.


SPEAKER_08:
Thanks for that.

Steven, go for it, and then Dean, Blue, or actually then Scott, and then Dean or Blue if you want, but Steven with a question, then Scott.


SPEAKER_01:
Yes, so thanks.

Just a question, just to be clear, so effectively you tagged the degree in prediction confidence and that mapped against the data that's empirical, if I'm right in understanding.

So as the model showed a difference in confidence, that was showing a correlation to...

how firing happens.

And I'd just be curious to know if you think that this sort of key level or something where this firing or deductive piece can happen, where there's a shift between maybe a more Markovian, gothic process to something more semi-Markovian,


SPEAKER_07:
and less ergodic and you so that an organism can make non-ergodic choices in conscious awareness uh so i mean a couple of things so i'm not sure about the ergodicity comment so i think you can probably have i'm not sure how semi-markovian systems relate to ergodicity i assume that you can have a semi-markovian system in a ergodic system that doesn't like that wouldn't surprise me um yeah so basically when you move from a

I just don't know, actually, just to flag that.

But I'm not particularly up on a lot of the physical physics that relates to how these models work.

But yes, when you move from a first-level model to a second-level model, they are semi-Markovian.


SPEAKER_08:
And can you just maybe define that a little more?

What does that mean to you?

What does that enable?

And maybe that will get at what Stephen was asking, too.


SPEAKER_07:
Basically, the model is no longer just purely dependent upon the previous state at the first level of the model.

There is more information that is determined at the second level than what just goes on at each time step of the first level.

That's basically it.

Yeah, so I think we have a sentence about this somewhere.

Yes, what the temporally deep scale buys you is essentially what we were saying before.

All of those cognitive actions

that allow you to do things like construct reports, make plans, et cetera, et cetera, and kind of abstract away from moment by moment sensory flux.

All of these things depend upon there being a second level.


SPEAKER_01:
And that might be reflected in the brain, from the granular smooth.


SPEAKER_07:
No.

So granular cortex is, generally speaking, like,

So the cortex has six layers, roughly speaking across.

And as you kind of move up or move towards the center of some centrifugal hierarchy, depending on who you talk to and what kind of anatomical maps you believe, that type of thing, layer four of that, the granular cortex, which is essentially the input layer from the thalamus and the feed forward layer that gets smaller and smaller and smaller until when you're at kind of the center of this hierarchy or this centrifuge.

you essentially have no granular cortex at all.

This is kind of ties in Lisa Felden Barrett's done a lot of stuff with this.

I have problems with that view, to be honest.

They kind of say the top of the hierarchy is where they think consciousness lives.

I have a rather vehement disagreement in that I just think there's absolutely no evidence that that's true in terms of neuroimaging evidence.

The areas that are relevant seem to be these frontoparietal regions.

They do have a somewhat well-defined layer four.

It's not completely agranular.

It's not granular.

It's somewhere in between.

Yeah, I don't know.

I think they, one thing to say about, I know Ryan has very specific views about this.

And I should say that I really love Lisa Feldman Barrett's work.

I just think she's wrong about this.

I just want to go with kind of the empirical contrasts right and not speculate too much about very specific implementation details so look they kind of go from a very loose model of kind of predictive coding and then map it to these like.

cellular data kind of details about the structure of neurons.

I am more interested in going from these computational properties to basically what would measure the level of neuroimaging, because I think that's what is most tractable experimentally.

And then if we're, and there's still a lot of debates about that neuroimaging level, right?

If we kind of hammer it out and decide what's going on at the neuroimaging level of analysis, then good.

then we can start having more serious discussions about particular involvement, particular types of cortex.

But I just don't think we're there yet empirically.


SPEAKER_05:
Yeah, I mean, I would say, yeah, just to kind of echo that a little bit, you know, it's important to realize that it's not as though, you know, the particular sets of, you know, little node neurons and synaptic necrins and the kind of like

know hypothetical columns um you know that chris showed earlier in terms of the neural process theory um that's just kind of one example of a way you could set it up there's there's a very very large number of ways that you could connect up neurons together in different structures that would implement basically the same algorithm

And in addition to that, there are different message passing algorithms that you can use to solve the same graphs.

An active inference isn't defined by some particular message passing algorithm, right?

Like Thomas Parr has shown this specifically in a paper where you can have different neural implementations

that require more or less neuronal resources to implement.

It's kind of a trade-off between efficiency and accuracy where, you know, for instance, like really, really accurate methods passing algorithms like belief propagation, they require more neurons to do, but they're a little more accurate.

And the brain could use that, right?

Or the brain could use variational message passing, which is not quite as good of an approximation, but requires less resources to do.

So the point being is that there's not really that strong of commitment to that level of kind of circuit detail in active inference.

It's more just a theory of you can have this pattern of synaptic connections that implement the matrices, and we represent

posterior beliefs over states with these firing rate functions.

And then you take the rate of change in those to put the PRPs.

But so this is kind of the issue with it doesn't really make sense to me anyway at this point.

Again, it's echoing what Chris said.

To start with, hey, let's assume that this one very specific

way of setting up neurons to do this is the right one.

You know, let's just assume that and then figure out what must be true about different areas of cortex based on their layer, based on their cortical column layer structure.

Um, so, so then it's definitely, I mean, it's super interesting and probably has, um, you know, a really meaningful, interesting mapping between the differences between granular and granular and a granular.

Um, with respect to computational function.

Um, but, um, but again, you know, like, I don't think.

until we've really nailed down that there is this one particular circuit implementation, it doesn't really make sense, in my opinion anyway, to kind of infer from that direction the function.


SPEAKER_07:
I should say, just to kind of echo that, I think if you work in your imaging, it doesn't make sense.

If you work in mouse, in a mouse model, and you were literally testing hypotheses at the circuit level, then it would make sense something to do.


SPEAKER_05:
well right but in that case you're literally testing like the predictions of specific message passing algorithms yeah exactly so that was kind of a testing that's testing for message passing algorithms that's not assuming a particular one and inferring from that so anyway I mean but

I mean, if you, I mean, for people who are interested, I mean, we, you know, so I and a couple of previous papers have, you know, kind of built multi-level active inference models similar to this, but specifically about emotional awareness.

You know, the sort of show how you can get, you know, from simple kind of lack of awareness of single emotional states all the way up to being aware of

feeling multiple emotions at the same time and being able to report them and things like that that kind of show how a structure not too different from this one can generalize to emotions or really to any other sort of thing that you could experience and be aware of, whether that's coming from inferring things about your own kind of bodily state and its emotional meaning from interoception or whether it's something like vision.

So, I mean, we haven't covered that stuff in any of these sorts of things before, but that stuff is also out there that might kind of help at least show what kind of the view on our end is about how active inference can account for emotional phenomena and be consistent with the current literature on emotional awareness as well.


SPEAKER_08:
Thanks for these awesome answers.

Scott with a question, and then I'll ask one from the chat, and then Dean or Blue or someone else.


SPEAKER_00:
Here we go.

Thanks.

Great and fascinating presentation.

I was wondering, this book, The Intelligent Movement Machine.

Graziano.

Graziano.

Yeah, and it talks generally about the cortical mapping being predicted by the movement repertoire in the motor cortex and kind of that embodiment kind of element.

One of the things I wanted to ask about, and I think it follows from this most immediate prior part of the conversation, is when we have an active inference setting and that model is being employed for multiple things,

perception, and I won't use the phrasing right because my active inference terms aren't all up to snuff, but when the inputs into the individual who's performing the active inference, that system, are from various sensory inputs and various temporal distance, so there's several inputs, and then they're being synthesized into a

an action or an externality, something's going to reach out.

And again, my apologies for not internalizing all the definitionals.

What are some of the cortical mappings or have you found cortical mappings that embody that process of synthesizing multiple inputs into a single action?

What are the core, what are the structural correlates of the process of synthesis of multiple different strands of active inference into a single action potential?


SPEAKER_07:
I'm using the wrong words, but nice question.

Is this a question essentially about multimodal integration?


SPEAKER_00:
Yes.


SPEAKER_07:
So, yeah.

Um, so I guess I think active inference is probably, you could, you might be able to build a specific.

So I think there are two parts of this question.

That's an interesting question.

Um,

First part is I'll just kind of answer the active inference end of it.

The way I think about active inference at least is just as a really useful modeling and mathematical framework.

And I think it's possible to build multiple active inference models of any phenomena.

So I would want to hammer out an empirical paradigm and then figure out, okay, what models could I build of this?

So that's kind of the first part.

Second part is just about multimodal integration.

Are there kind of places that hows

I haven't looked at multimodal integration since I was an undergrad.

I'm just trying to remember like my third year Cognuro courses.

I think there are places in like posterior parietal cortex where there is integration between multiple modality, like you might say like vision and one other thing or whatever it is.

And then that gets shot off to motor cortex implemented in actions.

So I don't know.

I think I would just say, look at the motor planning literature.

There's lots of stuff about Bayesian visual writer integration.


SPEAKER_00:
And just to follow up, one of the things that leads to is I was wondering about the situated cognition opportunities for those syntheses.

So for instance, you have hysteresis, or my son used to say stereotypes are a real time saver.

He said it as a joke.

So the idea is you have externalities that facilitate the synthesis.

So they're actually not cortical structures, they're social and linguistic and rhetorical structures.

Where I'm exploring is could those be described with some of the same models as the cortical structures so you're actually able to have a scale-independent description of the processes both in a social and a cortical cognitive context

for the synthesis of multiple multimodal inputs.


SPEAKER_07:
So I'm not sure about the scale independent part.


SPEAKER_08:
yeah yeah i i i hear what you're at no scott it's a beautiful question i see where you're coming from because we've been talking about the scale free formulisms with chris fields and others and then this whole nested markov blankets like you could have the neuron and then the neural you could go within a model and just like you said um your model doesn't presuppose that awareness is actually at a higher level you're just saying there is a structure that's deep temporal structure and that's the state that's the timeline that you are at but there could be another level

And then mechanistically or functionally, how that level plays out is going to come back to a task specific measurement, which is and a plurality of models.

So still good points.


SPEAKER_05:
I mean, part of part of I mean, I think, you know, I mean, to go back to I mean, there's lots of like, nice opportunities, right to build active inference models would be very simple, you know, for like, multiple multimodal or cross modal, right integration tasks, you know, like, for instance, like,

um i haven't seen this done um but maybe i have to look back but things like the double flash illusion or the rubber hand illusion or you know any any of these kinds of yeah multimodal things i mean like like a double flash illusion is really nice right because i mean more or less for people aren't familiar um you either play one or two tones and they are coincident with a single flash of light

And it just turns out that when you play two tones really quick, then people perceive two flashes of light when there was really only one.

And the reason for that is, so there's just this kind of like prior in the mix, presumably, that coincident, you know, things that are coincident temporally have the same hidden cause.

But also at the same time, the temporal resolution of audition is just better.

um this is more trustable than the temporal resolution of vision um so so you can just say look like the system just trusts right it has beliefs that the auditory system has higher precision and that there's a high probability of common causes of coincident input therefore there must have been two flashes um given that there was two cones um you know i mean that's just one example of like a really simple well-known empirical task um of cross-modal integration

that would be pretty easy.

I mean, probably without a lot of without even a ton of tweaks to

that the kind of structure of the model that we're using here, you could do.

And all you'd really have to do probably is just assign the right precision to the auditory and visual input, where those are jointly generated by the same hidden cause or hidden state in the model.

And so basically, it's either you're just inferring one cause or two causes, and that either generates receiving one flash or two flashes.

I mean, anyway, it would be simple to do.

Um, and, uh, like I said, I don't know if I don't not aware of that having been done in a kind of formal active inference and DP literature, but, um, you know, I mean, there are known, right.

Like neural correlates of this stuff and, you know, they're in part where you'd expect them to be right there at like the borders, right.

Of like auditory and visual processing and like association cortex, um, like posterior association cortex, um,

and uh you know so it'd be really really really cool project right to try to build a model of something like that and then uh just see right whether the predicted uh neural time courses um actually better kind of single out right the the areas the association areas where the kind of borders of vision are different it seems like it's it's almost like a situated synesthesia


SPEAKER_00:
where you have the Gabor Square, where you have the frequency and time variance, but one of the variants is being hijacked by another perception in the visual field.


SPEAKER_08:
right you know so the you're you're it that you it's crossing over to that other sense is very interesting thank you thanks um i'm going to ask a question from the chat and then if anyone wants to give probably a short final note but then we'll end around the hour so that we can have a whole second discussion next week the question in the chat is um to the authors what would you say the main limitations of the model are or where are some next things you're building on but what are the main limitations of the current model and then how are you going to be building


SPEAKER_07:
So, I mean, there's lots, right?

So I'll just list them.

I think we list this in the paper somewhere too.

For starters, it's a discrete model.

We treat the whole visual system as basically like one discrete hidden state to be inferred by the high level.

That's obviously like not true, but it's a good enough approximation to actually think reproduce a bunch of findings.

It would be nice to have a multi-level model

Now, with that multi-level model, I think there are some empirical cases.

There's this thing called the partial awareness hypothesis by Sid Kuder, where they talk about having partial access, or the global workspace having partial access to different parts of the hierarchy, and that giving rise to kind of odd experiences.

So you might have something like, you might have really good access to the fact that the color of Ryan's shirt, but not to the fact that it's a shirt, or vice versa.

So that would be nice.

I'm not sure quite how to do that in a hidden Markov model situation, but that would be nice.

Second thing is we really just model kind of report paradigms here.

So where we're going with this is we're trying to extend this explicitly to model some no report paradigms and actually casting kind of thinking about that in a more precise way.

And so we've actually built another model and we're kind of writing it up at the moment.

I mean, it's not...

It's kind of a side project because it's not a part of my PhD thesis, but the idea is roughly speaking to actually have a model that is able to explain a lot of the weird findings in the no report literature.

And also I think we have a much more principled, we now have a much more principled account of report that I'm really quite excited about.

Then just like, I guess on a last limitation would just be,

And this isn't really a limitation because Ryan has got a lot of work in emotion and interoception on this.

And I know that's somewhere that his lab is like really pushing.

But it would just be nice if we could use these very similar architectures and apply them to something like interoceptive awareness.

And I would find it personally very compelling if you had one, I think it's too demanding to have one model, but one very general model structure that generalizes across

visual consciousness to interoceptive awareness, to like auditory awareness, to smatter sensory awareness.

That would be really nice.


SPEAKER_08:
Awesome answer.

Ryan, do you want to add anything?

And then we'll have any final comments.


SPEAKER_05:
You know, I mean, I think, I think Chris covered most of the kinds of limitations or, I mean, I don't know.

I mean, I would just say like, like, yes, I mean, it would be, it would definitely be nice to do something like have a,

So there are what are called mixed models where you have kind of a discrete model like ours on the top, but then the kind of posteriors, some predictions downward that essentially set set points for a continuous level below it.

So then you can have a continuous state space where, for instance, it's a lot more plausible, right, to think that

the sorts of inferences that are being made in kind of like early visual processing or on a, you know, on a continuous, a continuous state space, right?

It doesn't have to be like motion speed one, motion speed two, right?

It can be, it can be continuous values, um, for, for things like that.

Um, but, um, like another thing that I, you know, I mean, you know, Chris mentioned interception and emotion.

I mean, that's something, like I said, I mean, we have empirical paradigms that we've been setting up to test just this kind of thing in interoception.

And we have a paper under review right now where we applied a computational model to interoceptive perception and like a gastrointestinal interoception task.

That's not a multi-level model at the moment.

And that paradigm doesn't have, probably doesn't have enough trials to do contrast between

um conscious versus unconscious um stimulations but we're kind of working on that um so that would be nice um another thing

that Chris kind of mentioned the example of being aware that the shirt is red but not a shirt.

That kind of thing also is really important for modeling emotional awareness because a lot of times people might feel some kind of bodily sensations, kind of like a pit in their stomach or a fast heart rate or something like that.

They might be conscious of that but not infer one step above and be conscious of the fact that that's associated, say, with feeling fear.

Right.

Like like people with like panic disorder, for example.

Right.

They might feel like a strong heartbeat, but they might instead infer, oh, that means I'm having a panic attack or something like that.

Or they might infer, oh, I'm having a heart attack.

Right.

So there are different kind of conceptualizations that you can you can infer from.

Right.

Patterns in lower level interseptive experience, some of which are emotional and some of which are not.

But and this is where it becomes interesting is that

You might also prime an emotion category.

Like I mentioned with the guilt example, I could prime guilt so that higher level concept representation is activated, but does not enter into awareness.

So you can kind of have multiple levels in a hierarchy where each level can independently or compete for broadcasting.

You know, I've been thinking about ways to do this model-wise, and the best thing we've come up with right now, just in terms of the practical implementations we have, would be to, for instance, like have an interoceptive level and then have the emotional level above that, but have the interoception representations also get kind of like duplicated and passed up to the second level.

So then they can also kind of equally compute, right, for access to like a third level that would be like the workspace level.

Um, but anyways, I mean, these are all things that are kind of in progress, but just by way of saying, um, this additional limitation is you can only be in our model.

You could only be conscious of conscious or unconscious of one type of processing or love one level of abstraction.

Um, which is not, you know, the case in the, which is not the case in the, it's not the true case.


SPEAKER_08:
Thanks a lot for these awesome answers.

Always a great time with the two of you on the stream.

So just thank all the participants who joined us live.

And in the calendar event, there's a form.

So it'd be helpful if you want to add feedback on the form.

And then we're going to be having 18.2 next week.

At the same time, it's going to be the same paper.

So hopefully we'll all get another chance to reread the paper, listen to this stream because Christopher, what you said was really deep.

So again, much appreciated to all and we'll see everyone next week.


SPEAKER_07:
Thanks guys.
SPEAKER_00:
Hello, everyone.

Welcome to Team Calm podcast number two.

This is going to be a journal discussion of Ramstead et al.

2020, and it is going to be a great discussion.

I'm really looking forward to it.

So before we start off the discussion of the paper, could the other two panelists introduce themselves?

Maybe Sasha first.


SPEAKER_01:
Sure.

Hi, my name is Sasha.

I'm a neuroscientist and a free energy enthusiast.


SPEAKER_02:
Hello, my name is Juan, and I'm just interested in active inference stuff, and today we got to read some tough book.


SPEAKER_00:
Yes, absolutely, a tough book.

So, does the screen look okay for sharing my slides?

All right, here we go.

So the structure of this discussion will be as follows.

First, we'll talk about some warmup questions.

Then we're gonna jump into the paper discussion.

We'll go through the abstract and the big questions, making sure that we really understand how the authors summarize their own work.

Then we'll look at the roadmap of the paper, which are just the section headers to understand how they kind of get from A to Z.

We'll review some of the key claims and the aims of the paper so that we can evaluate later on whether the claims are justified or whether they end up achieving the aims that they set out to achieve.

Then we're going to go through each of the six figures of the paper

And then, if not addressed during A through D, we'll have extra time at the end to discuss any other questions about the paper or related topics, and we have some other figures that we could put up if we need to.

So here we go with the warm-up questions.

First, what is semantics?

Or what does the word make you think of when you first hear the word semantics?

And you might want to think about logical and lexical semantics.

Any thoughts there, Sasha?


SPEAKER_01:
Yeah, I think this is kind of the getting into the confusing theme of the paper where you have to be extremely specific with the language that you use.

And so to me, semantics means how we


SPEAKER_00:
use words and how we make meaning okay very nice and the way that we use words the meaning of words is often what people mean by semantics like syntax is how it's written and then semantics uh or at least the semantics of words is about how the words are related to each other and then also we have logical semantics which is just purely uh about logical statements and may or may not have any word representation behind it

So there's that word representation, and this is definitely one of the key words of the paper.

So maybe, Ivan, what do you think about, or what does a representation mean to you?


SPEAKER_02:
To me, as we can say, as we are saying, we actually represent in our mind different things.

different things behind different conceptions so it's always hard to understand not always but usually hard to understand what people saying when they use one or another concept


SPEAKER_00:
Yep.

And that vagueness about the term representation is really one of the things that the paper sets out to clarify, because a lot of times people will use the word representation assuming that one must exist.

Oh, if you're thinking of elephants, there must be some neural state in your head that represents an elephant.

Otherwise, how could you be thinking about an elephant?

And then this paper is going to really drill down.

What is the relationship between that conception and the neural representation?

Next question.

What is intentionality?

Who or what has it?

So just kind of off the cuff, because of course this is a big topic, but what does intentionality mean or who gets to have intentionality?

Either of you.


SPEAKER_02:
I think intention is became when

when someone wanted to do something, yes, intention, it's about the future, so I will do it in the future.


SPEAKER_00:
Any thoughts there?


SPEAKER_01:
Yeah, another wonderfully vague word, and we often talk about in interpreting other people's actions, whether or not we can infer their intent, and of course that gets quite messy, but

with how vague the word is, you could say that everything has intention because it, you know, even a rock knows that it will continue to stay a rock in its sort of thermodynamic landscape.


SPEAKER_00:
Very nice.

And this is sort of that pan-psychist twist, which is, do physical objects have intention?

Or what about criminal intent?

Do humans have intention?

Or how would we even know?

And then how are narratives related to semantics?

And then I'll just throw on the last one there.

Are narratives real?

Or in what sense are narratives real?

Any thoughts, Ivan, or we can move on.


SPEAKER_02:
It's really hard philosophical questions about are narratives real or not.


SPEAKER_00:
Agreed.

I mean, this is really and it's why it's the last question before we jump into the paper, because narratives are real, like Harry Potter is real.

It's really a narrative.

But then, of course, you go into it a little bit deeper and everybody has their own interpretation of the narrative.

Well, it's real, but everyone has a different reality.

And then is that real?

Well, that's kind of what we want to explore.

And yes, it's a different tool than people expect.

the usual tools that people would expect to discuss these topics would be very qualitative and wordy philosophy arguments.

And this is going to be a pretty totally different approach involving the free energy principle.

So here we go.

The paper that we're discussing today is a recently accepted paper from the Journal of Entropy, but we're reading off of the version that we found on ResearchGate.

And the paper is called Is the Free Energy Principle a Formal Theory of Semantics?

From Variational Density Dynamics to Neural and Phenotypic Representations.

So, a lot of new words, and for some people this is going to rankle them because they think it's a philosophical word being used improperly.

Other people, they might not have clarity on what all the words mean.

So what we're going to do is look at the big questions of the paper, which are thankfully the first two sentences of the abstract, and then we're going to run through the rest of the abstract before we go on to the rest of the paper.

So kind of just like you're reading this at home, this is how we're going to unpack the paper from the beginning to the end.

So there's two big questions.

And again, these are the first two sentences of the abstract.

Their first question or their first big goal is to assess whether the construct of neural representations plays an explanatory role under the variational free energy principle and its corollary process theory active inference.

So there's a lot happening here.

I would kind of jump into the middle and it's where you start with a free energy principle as a principle, meaning it's paradigmatic, it's axiomatic, it's foundational.

And then there's a corollary, which means like a subsidiary or a secondary theory.

And the secondary theory happens to be a process theory, which means it's not a state theory.

It's not about how things are.

It's about how they relate.

So another example of a process theory would be evolution by natural selection.

It doesn't tell you what state the system is going to be in, but it tells you if certain characteristics are present in the system, like replicators with differential success and heritability, you're going to end up with this process playing out.

So the process is active inference and the paradigm is the free energy principle.

And then the question that they're going to work on is to assess whether the construct of neural representations, so neural states, whether those have an explanatory role or are going to enrich us when we're thinking about free energy principle active inference.

And their second question that's tied to the first is if there is an explanatory role for neural representations under active inference and the free energy principle, which we think that there will be, then they're going to assess which philosophical stances in relationship to ontological and epistemological statuses of representations is most appropriate.

So they're going to use active inference free energy principle to explore how neural representations are related to the real world.

And then they're going to try to translate that back to the classical vocabulary and some of the classical theories from philosophy.

OK.

So let's continue going through the abstract.

And this is definitely where we're going to want to slow down and make sure that we understand what each of these terms mean from a philosophical perspective.

What are the baggage?

What are the assumptions?

What are the implications?

So they're going to focus on non realist, which are deflationary and fictionalist instrumentalist approaches.

And they define it in the next sentence a little bit, so we don't need to dwell there.

We consider a deflationary account of mental representation, according to which the explanatorily relevant contents of neural representations are mathematical rather than cognitive.

So that's one type of account of representation they're going to pursue.

The second type of representation that they're going to pursue is this fictionalist or instrumentalist account.

In this view or this account, the representations are scientifically useful fictions that serve explanatory and other aims.

So looking at this deflationary account in the middle and then the fictionalist instrumentalist account at the bottom, what does that make either of you to think about or how do those two ideas differentiate themselves?

Or what would be another way that you would restate either the middle one or the bottom one?


SPEAKER_01:
I kind of struggle with really specifically understanding what the deflationary account means and how it differs because I kind of imagine that something could be a mathematical representation as well as a useful story.

But I think this is a common theme that we'll be coming back to for this whole paper is that

Using active inference as a framework, it's not just a hypothesis, it's a lens that you apply to how you view things.

And so viewing things through the active inference lens, it seems like these can all coexist in a way that's not exclusionary.


SPEAKER_00:
Okay, very nice.

Yvonne?


SPEAKER_02:
Let me pass now.


SPEAKER_00:
Okay.

I think that you really said it well, Sasha, which is that from a different lens, the non-free energy principle lens, these two viewpoints might be very opposed to each other.

I'm just thinking at some psychology conference.

First, someone says, I propose a deflationary account according to which dot, dot, dot.

And the next person says, I propose a fictionalist account, dot, dot, dot.

But

free energy is this new lens and this new way of trying to approach these classical philosophical problems and the goal would be that we could get the best parts of both of them and use them under the free energy principle so from the deflationary account we could make mathematical representations of neural states that would be awesome and then using a fictionalist or instrumentalist accounts

maybe we could also have those mathematical states be useful.

So we kind of have like this triad.

We have neural states.

I mean, the neurons are definitely doing something.

We don't know what, but they're doing something.

Then we have mathematical inference, which whatever the neurons are doing, you can always mathematically represent it.

And then there's utility for the scientist and or for the organism itself.

And it's like,

You can choose one, two or three of them.

And the goal would be we'd want all of them to be under a common framework.

So that's really where they're going with this approach is instead of just butting heads.

Oh, well, this part of the triangle is better than this part of the triangle.

They're going to see a free energy can integrate across these different perspectives.

So the abstract a little long.

Here's the second part.

After reviewing the free energy principle and active inference, we argue that a model of adaptive phenotypes under the free energy principle can be used to furnish a formal semantics, enabling us to assign semantic content to specific phenotypic states, which are the internal states of a Markovian system that exists far from equilibrium.

So phenotype is being used here quite broadly.

A lot of times people think about phenotype as measurable components of an organism.

And that is like, oh, well, leg length is a phenotype or hair color is a phenotype.

And they are.

But also the instantaneous representation of the neural system is like a phenotype.

And so just like a phenotype could be reaction time, another type of phenotype could be when I play a 200 hertz tone into your ear,

your brain state in response to that tone is a phenotype.

Here, phenotype is just meaning measurable components of a biological system, whether you actually measured them or not.

Then,

they're going to ask whether they can assign semantic content.

So that's meaningful content, either lexical semantics, which is how words are related or logical semantics, which is how ideas and propositions are related.

And they're gonna ask whether they can make some sort of a mapping between semantics

and phenotypic states within a Markovian system.

And by Markovian system, far from equilibrium, they're highlighting two pieces.

To call the system Markovian means that it can be bounded within a Markov blanket, which is to say that there are states that are internal to the system and states that are external to the system.

And then the boundary nodes, where when you're inside looking out, it's what you see, or outside looking in is what you see.

And that's this Markov blanket idea that we kind of come back to in a lot of our discussions.

They're going to propose, and this is why I made this font big, a modified fictionalist account in organism centered fictionalism or instrumentalism.

So they're going to say, let's be pragmatic.

Let's make a modified account where the organism is in its own generative model, making an instrumental fiction about the world where I'm not exactly sure all the connotations of instrumentalism here, but it's kind of like the body is an instrument.

And then the song that's being played is the niche, is the fitness of the organism's phenotype to the challenges of its niche.

They argue that under the free energy principle, pursuing even a deflationary account of the content of neural representations licenses the appeal to the kind of semantic content involved in the aboutness or intentionality of cognitive systems.

Our position is thus coherent with but rests on distinct assumptions from the realist position.

And so here we get another peek at the difference potentially between the deflationary accounts and the inflationary accounts, which is not really used.

But what does it mean to deflate or inflate in this context?

Deflationary, it's like if somebody said, oh, it's just a party.

It's a deflationary account of the party.

It's like, it's nothing big.

Okay.

Whereas the inflationary account of the party would be like, this is going to be the most important party ever.

It's going to be so important that all these things happen in this really symbolic way.

And so the deflationary account of neuroscience is it's just what the brain is doing.

It's just cells that are linked up to each other.

And it's just time is passing and click by click in time.

It's just the physical system.

But then you get into a little bit of a bind when you want to reintroduce semantics.

Because if you just have this massive agent-based model or dynamical model that's just being simulated forward through time, then you can say, well, it's as if it's thinking about elephants.

But you're never going to be able to go back to the microstate and say, this is elephant in the microstate as it's represented.

Because it's deflationary.

You kind of deflated the meaning away.

And that's the realist position, whereas this semantic is an inflationary account.

It's like, yes, it's a brain state, but it actually is meaning something more.

And so that's what they're going to try to take the best of both worlds.

They want to have a formal semantic system like philosophy does formal semantics, but they want to ground this formal semantics in how it arises from phenotypic representation, specifically neural states.

And then this is their closing argument that we argue that the free energy principle thereby explains the boutness or intentionality in living systems and hence their capacity to parse their sensory streams using an ontology or set of semantic factors.

And so here is where they kind of reach their synthesis and their proposal, which is that if we can find some sort of a bridge between realism, like what the brain is actually doing without any extra fluff, just phenotype,

and a formal semantic theory, potentially, this will light the path forward for understanding how organisms parse their sensory data coming in into meaningful categories.

So instead of just saying, well, the photons hit my retina.

And within a predictive processing framework, my priors are updated.

And yep, there's Sasha in the window.

And there's Yvonne in the window.

Instead of that type of processing forward,

It's actually like we get to think about philosophically what the organism is doing through parsing its sensory stream, but also retain the realism of the sensory stream.

Okay, any thoughts on that relatively long abstract?


SPEAKER_02:
It's more clear now.


SPEAKER_00:
OK, that's the only hope.

If we aren't reducing our uncertainty about this paper and their claims and their aims and their goals, then we're not really in it.

So now that we have a little bit of a better sense of where they want to come from and where they want to go to, let's look at the roadmap.

This is going to be the path that we take from the introduction of the problem and its relevance

through the foundations to the claims that are synthetic.

So they're going to begin in the introduction by talking about the idea of neural representation and the contents of neural representation, as well as a little bit of a joke, the discontents, like discontented means to be unhappy.

So there's some issues with neural representations, and they're going to be discontent with them by talking about the problems with their contents.

So classical joke.

the faces of representationalism, realism and non realism.

And so here is those two different dueling perspectives on representing.

And it's another joke.

It's two faces, two faces of the same coin.

So if you're going to have representation, it's kind of sitting just like a coin on its edge on one face.

It's the semantics.

What does the representation mean?

You're thinking of an elephant.

It means elephant.

And then there's the deflationary, oh, it's just a neural state.

And there's actually no meaning attached to it, even though it is a representation of an elephant.

Then they're going to take a turn towards anti-realism.

They're going to talk about some of the deficiencies of the realist view.

And there's a lot of ways to approach this.

We'll look at the specifics.

But one example that I always come back to is most people don't perceive their blind spot.

or they perceive that they have color vision in their whole field of vision.

And so, yes, the world really does have wavelengths of different colors coming from all over our visual field.

But that's not really what you're perceiving in your retina.

So it really is being generated by your brain.

So it's really an organism centered fiction, which is why they're going to come back to that idea later.

Then they're going to address how representations are

discussed under the free energy principle as a paradigm.

The second section is where they turn more heartily to the free energy principle, and they talk about active inference, again, as a corollary process theory to the free energy principle as a paradigm.

And then specifically, they're going to bridge two topics here.

They're going to move from information geometry to the physics of phenotypes.

Those might be two very new terms or two new ways of thinking about information and geometry.

A lot of times people think about information theory is one type of science, and then geometry, triangles and squares, it's different.

So what is information geometry?

And then how is it related to not just phenotype, but the physics of phenotype?

So first they go through state spaces, non-equilibrium dynamics, and bears.

Oh, my.

Again, hilarious writing.

They're going to talk about Markov blankets and the dynamics of living systems.

So again, a Markov blanket is like this set of nodes you can think of around a system that is the ideal, perfect, clean cut, carving up nature at the joint between the internal and the external system.

And going into the internal system is sense data and leaving the internal states are action states.

So we'll look more through that soon.

then they're going to return to information geometry and the physics of not just phenotype, but sentient systems.

And that's going to have to do with that far from equilibrium nature of living systems.

And it's why the rock is not the person, the object is not alive, but a person who has far from equilibrium thermodynamic states, they're considered to be living.

Then they talk about phenotype as a tale of two den cities.

Again, trying to loop as many puns as possible into their titles.

then they close out section two with the discussion of living models which is a mechanistic view on goal directed probabilistic inference and decision making under the free energy principle so everything is under the free energy principle because that's um our working model and then here they're going to try to go from a mechanistic view so they want to talk about the mechanisms which are how things are actually happening

but they also want to include some things that are not traditionally included in mechanism, for example, goal-directedness, probabilistic inference, and even decision-making.

Then in section three is where they bring together their deflationary and fictionalist account of neural representation.

First, they're going to propose their deflationary account to neural representation.

In the other words, brains are just doing brain stuff, nothing more, nothing less.

That's what Friston calls deflationary.

In the sense, don't inflate your hopes that there's going to be some magical secret between the neurons and the brain.

The brain is just doing brain stuff, and it's tremendously complex, and it involves multiple cell types, but it's just that, nothing more and nothing less.

Then they're going to talk about fictionalism and how models come into play in scientific practice.

And then in the closing section four, they're going to finally reach their synthesis, which is a variational semantics from generate generative models to deflated semantic content.

So now they're going to try to deflate meaning itself.

So when I say that my name is Daniel and there's so many layers to interpret that on, it's a string of audio bits.

Now, it's a string of audio bits that is being transmitted through the computer or through the live stream.

It's just exactly what I mean.

It's just the signal I'm sending nothing more and nothing less.

So how are we going to get this kind of deflationary realism about the world that's so critical for an unbiased or at least transparent scientific approach?

We want that unbiasedness, that objectiveness.

Yet because we're semantic entities, we know that we're going to have to engage with the world in terms of meaning.

So how are we going to square this circle with realism and what is just there with meaning and all the power that meaning based approaches can bring to the table?

Then they're going to provide the deflationary account of content under the free energy principle.

and move from a computational theory that is proper to a formal semantics.

And then they're going to close out with the idea of phenotypic representations and ontologies.

Okay.

Big roadmap.

A lot of stops, a lot of gas stations, a lot of restaurants.

Any thoughts on that?

Okay.

Good, Sasha?


SPEAKER_01:
Yeah, that was really helpful.


SPEAKER_00:
um to talk through the the roadmap and um see where we end up okay so sorry for the wall of text but i wanted to put one of their key claims verbatim they write and this is a pretty early on in the paper i think this is on the bottom of page uh the actual paper uh

page three or four, bottom of page three in the PDF.

But the page numbers could be different for a future version.

They write, there are several well-accepted constraints for the appropriateness of representational explanations.

So we're talking about representations.

And now we want to think, what are the deserata?

What do we want?

What would be a great theory of representation?

And they're going to play by the rules of people who study representation.

They're going to say, these are the community of people who study representation.

They've decided that these are like the five rubrics.

If you can ace all five of these tests, then you're going to get an A plus for your theory.

So first, it should cohere broadly with actual practices that are used in computational cognitive science research.

So that means if you have some theory of representation that only applies on a different planet or outside of an fMRI machine, you're probably not going to get too far.

We want this representation model to be grounded with the real things that people are doing in computational cognitive science and also psychiatry.

Two, we want to allow for misrepresentation, which means that the representation has to be able to get it wrong.

And so if you say, think of an elephant, and then anything that the person thinks of, you're going to say, right, well, that's what their version of elephant is.

They started thinking about lunch, but

they can't be wrong because no representation is incorrect.

That's not going to be super useful.

We won't be able to actually parse the world into useful pieces of information and not useful.

And that's a lot like saying, if you can't have a model that gives you the wrong answer, it can't give you the right answer.

If it just gives you a one, no matter what you put into it, it's not useful.

Just because it might be right twice a day like a broken clock doesn't mean that it's actually a useful model.

Three, representation should provide the principled method for attributing determinate contents to specific states or structures internal to the system.

So we're looking for some sort of a bridge for a way that we can go from a specific brain state, like your amygdala is having a lot of blood flow and your prefrontal cortex isn't.

That would be a very coarse level representation.

Or you could imagine a one million dimensional representation.

This neuron is doing that.

And this glia is doing that.

We want to go from that specific state representation, which might look like a vector or some other map.

And we want to go from there to something that is internal and or external to the system, like thirsty or hungry or thinking of an elephant.

Finally, four, we want this representation theory to be naturalistic, meaning that the account of semantic content does not itself appeal to semantic terms when defining how the representational capacity is realized by the physical system on pain of circularity and reasoning.

pretty rich for the free energy principle to call someone else out on the circularity of reasoning.

And it's a complex claim to parse what is being said with number four, because in some deep sense, because our axioms are chosen without evidence by definition, our paradigms are non-explanatory, they can't be explained.

The question of what's a circular reasoning argument and what isn't is not perfectly well defined.

But I think what they're getting for getting out with number four is like we want the semantic content of the elephant to be an elephant, not just the word elephant.

We don't want to enter into this infinite recursion where like language explains language, and then that explains another kind of language.

And we just stay within this realm of abstracted representations, but we never actually come back even in a sort of unexpected way to the reality of the naturalistic situation.

which is as evolved entities, we actually have evolved to represent certain external patterns internally.

And so if it doesn't come back to that naturalism, then it's just gonna fly off into space.

Any thoughts on this four claims about what would make a good representational theory?


SPEAKER_01:
I found that last statement quite ironic and circular, because in a way, we're always using semantics to describe the systems that are semantic, but also maybe eventually linked to a natural state or

you know, a real elephant, if you will.

And I think, yeah, this paragraph was the most helpful in trying to parse out the kind of things that they're looking for, and the arguments that they're trying to make.

And I really, I think

Part two is quite interesting, and then they do get into it later in the paper.

But to me, that's always the most interesting part of the free energy principle to address.


SPEAKER_00:
Cool.

And here, just to put it up on the screen one more time, this is from page four in the accepted manuscript version.

where they return to the twofold aim of the paper, which is to determine whether or not neural representations are going to play an explanatory role or any role at all within this increasingly popular framework for the study of action and cognition

the variational free energy principle.

And then second, they wanna postulate whether the neural representation theory is gonna be warranted under the principle.

So this is related to those first two things that we put up on the screen.

The first two sentences, the abstract, the big questions.

OK, so let's jump into the figures and then we're going to work our way down the paper and sort of landmark at the different figures so that we're understanding how they're visually as well as conceptually representing these different topics along the roadmap.

And then at any point, if you have a question about the figure or the text specifically or just a related thought that comes to mind, just bring it up.

So the first figure and the caption is copied below.

So you can...

OK, is a Markov blanket.

And this is very similar to other images that we've seen.

But this is just a foundational term for the free energy principle is this idea of a Markov blanket.

So we've talked about Markov blankets before.

Maybe with either of you like to give it a shot at what a Markov blanket might mean or not just the total definition, but even just one aspect of it that you think is interesting or relevant to remember right now.


SPEAKER_02:
Morgan let me try.

Once have its internal states and sensation and action and also in space around there is external states that one doesn't know about and doesn't know how

how it will impact on its own internal state and there is a border that where external states it's out of the border and internal states and the actions and sensations in it


SPEAKER_00:
Perfect.

So here, the purple node, mu, is the system of interest.

This is the project in focus.

This is like the internal state.

And then the blanket actually goes one layer out from the internal states.

And it goes in two different directions, this blanket extends.

It extends upstream towards the green S's.

Those are the sensory states that are coming in to the internal state.

through these hidden external causes, the orange nodes.

And then leaving the Markov blanket are these A action states.

And action states are basically resulting from the internal states dynamics as well as other sensory causes.

So here that S on the left side is just going directly to action.

So when I was thinking, what does it look like for the sense to enter the Markov blanket, but not enter the model?

I was thinking of like, you know, having like a splint on the arm.

It's like the overall outcome action moving your arm.

It's going to be related to your internal states as well as some external influence that wasn't really part of you.

but it's part of your integrated unit of sense and action.

And so that external cause about the world having a constraint on your arm ends up changing your action states.

And therefore it makes sense to include its sensory input and action influencing ability within the Markov blanket

even though it's not within your epithelia.

And that is actually why there's such a natural transition from thinking about this type of Markov blanket framework to thinking about extended cognition, exocortex, ecological cognition, distributed cognition, because it's the realization that even one person's internal states, for example, the brain,

are in feedback with external tools like the computer.

And so they're external from the point of view of the epithelium, but actually from the perspective of the Markov blanket, they're internal to the same self-regulatory cybernetic system that's grasping at these external causes and trying to influence them.

So I'm going to return just to the paper so that we can scroll through.

and see where this figure comes up to.

But I just wanted to get that first Markov blanket in.

So here we are in this section 1.1 with the realism and non-realism.

And so here's where they talk about realism about neural representation.

What is real?

Finally, we get to find out.

This view combines two positions.

Ontologically, that neural representations really exist, which means that they're physically something that's instantiated happening in the brain.

And then epistemologically, that's how we come to know about things that representations are scientifically useful as well.

So realism, they're saying, has an ontological component, which is that they're real.

They're actually physically instantiated.

And epistemological realism is that scientists should find this realism useful.

Non-realist positions, which have not a complete overlap, and that's a little bit of the confusion, is it's not like the realists and the non-realists are totally opposed.

They do share some aspects of their models, so it's not like perfectly everything that realism is, non-realism isn't.

Yeah.

What does that mean?

Non-realists are either agnostic about the reality of neural representation.

So that's kind of just like saying they don't care what's real in the brain or they'll never know or we couldn't measure it anyway.

There's a lot of ways you could be agnostic about anything.

Or they explicitly reject the assumption.

And so the non-realist is kind of like the agnostic.

The anti-realist is like the strong atheist.

They say neural representations do not exist.

So that's the strong claim is anti-realism or realism.

Theism, atheism.

And then non-realists are kind of like the utilitarian, the agnostic in the middle.

And they continue talking through several of these different philosophical ideas or these schools of thought, like eliminationism,

which is anti-realist.

And it's basically saying the construct should be eliminated.

So we shouldn't even use it.

And that reminds me of like the idea of a genotype phenotype map or a mapping.

Some people say, yes, there is a map and we can find out about it.

Other people say, no, there's no map, but it's as if it exists.

So you can talk about it.

It's useful, but it doesn't really exist.

And then there's other people who say you should eliminate the concept of a genotype phenotype map because it just is simply misleading to scientists.

that's eliminationism and then instrumentalism or fictionalism is non-realism and is saying that they're useful so that is hearkening back to the work of dennett for example with the intentional stance as well as a lot of other folks research um just speeding through this because there's a lot of sections they're going to talk about parallel distributed processing they're going to talk about some of the issues of realism um and

So we're just going to move through this because the specific critiques of realism, I feel like they're going to end up moving beyond them.

So if you're curious, you can read more into their specific problems with realism.

Then they're going to return to representations under the free energy principle, and that closes out section one.

So I think we could then go to figure two to look at the free energy principle.

So here is where we get to figure two and where figure two is conveying the basic outline of active inference and the free energy principle.

So would either of you two like to take a stab?

What do you see in this figure, either on the left side or on the right side?


SPEAKER_01:
Sure, I'll take a stab.

Yeah, so this is kind of taking the model of the Markov blanket in figure one and trying to fit it to some of the parameters of an organism and, you know, with the image of a brain to remind us what we're doing here.

And

to explicitly say and mathematically say what external and internal states are representing and that they're linked by the sensory states and active states now sensory states are explicitly going into the internal states and active states are explicitly going out into the external states and so um and then there's feedback between um

some of the components in this system.


SPEAKER_00:
Very nice.

Ivan, any thoughts on this?


SPEAKER_02:
As was told at the start, that we can have a mathematical

expression of the states and there is such expressions in math so if we state different values we can describe how we propose to have it in math yes

It's also have W with a little note down.

How it's letter called?

I don't know.

Omega.


SPEAKER_01:
Omega.


SPEAKER_02:
Omega.

Okay.

And what is it?

I just forget it.


SPEAKER_01:
It's random noise.


SPEAKER_00:
Yeah.

So that is kind of like an error term in each of these four.

So this is adding another level of complexity to this figure.

So here we had hidden external states that are orange, sensory states coming in that are green, purple internal states, blue action states going out, and then influence on those latent causes in the world in orange at the bottom.

Now, on the left side, we have internal states with blue.

That's still the mu variable.

And it contains some internal function f of mu and b. And so b is the blanket.

And it's basically saying mu is going to be a function of mu and b. So the state of the internal state is a function of itself and the blanket states.

And there's a noise term.

Then we go to the active states.

Active states are a function of mu and b. They're a function of the internal state and the blanket states.

That makes sense.

Plus another noise term.

And then you can continue working your way around.

And you can see that external states are a function of external states and the blanket.

plus a noise term.

And sensory states are a function of external states and the blanket state plus a noise term.

So this gives us a few nice partitionings.

First, we can talk about doing inference on the variables that we care about by separating them away from a noise term.

And then also, you can already see that sensory states don't rely on internal states or action states.

other than the ones that are in the blanket.

And so this type of partitioning is moving us towards a very clean way of talking about these different factors and how they influence each other.

Let's do one more figure before we return to the paper.

So here is figure three.

this is going to be another representation of free energy principle active inference.

So here at the top of the figure, we have this magical G, expected free energy.

And the expected free energy, now we're like looking inside the brain.

That's the inference organ that's calculating expected free energy, which we'll come back to mathematically what it is.

And the brain, or the internal state of the system, guides policy selection.

Pi for policy, like a P.

And policy selection is intermediate between the model of the world and the actual action selection.

So for example, if I see the ball coming out in front of me and my policy is that I'm going to run and try to catch the ball, it's like halfway between action, which is actually the legs moving and running to catch the ball, and the pure internal generative model, which is like, wow, if I ran as fast as I could, I could catch that ball.

In between is the policy choice.

And so that's what the brain is trying to converge upon good solutions for.

And that's why free energy principle is adjacent to cybernetics, control theory, and a lot of other areas that actually are action oriented.

Because in the end, it's not just that we go from model to action.

It goes model to policy to realized action.

And it's a nuance, but it turns out to simplify the system and the model a lot.

Then we have initial prior beliefs about the initial hidden states are in D. So on the left side of the image, and time is going from left to right here, the left side is this our priors about the initial hidden state.

Like, I think it's dark outside.

Then the actual hidden states are these n's, these n's, I guess.

And actual hidden states are going to move through a likelihood mapping, A, and result in sensory observation.

So initially in D, I believe it's dark outside.

And let's just imagine that this n1 is actually dark.

So then there's a mapping where if it's dark, no photons appear.

And if it's light, there's a bunch of photons.

And then that results, I don't know about N, I don't know about A, but I do know about S, which is I don't get any photons.

So that's consistent with it being dark because there's a mapping between it being light and photons and it being dark and no photons.

Sometimes it seems like we're explaining things like multiple times and saying it the exact same way.

But it turns out that by observing this structure here, we can see how these beliefs change through time.

So then B are state transition probabilities.

And so then by observing the lack of photons in S1, that is confirming my belief that it's dark outside.

And so we continue iterating through time.

where basically the probabilities of state transitions are happening about the external world.

And then the external world keeps on emitting sense data to us through this likelihood mapping.

And then what the internal state is doing is by minimizing free energy, expected free energy across the whole thing, we're going to come to adaptive policy selection.

Okay, any thoughts on this sort of third representation of Markov blankets slash free energy, Sash?


SPEAKER_01:
Yeah, that was really useful to walk through because I really wasn't sure how to, which direction to approach this figure from.

And the way you described it, it just makes it very clear and reminds me a lot of kind of experimental design where you go from one state

uh one set of uh hidden states and then you observe something and then you change one variable and you observe something else but it really depends on what your understanding of the um kind of relationship between you know photons and darkness for example that helps you interpret your next observation um so i i think

It just highlights that we're always doing that.

We're always trying to link the observed state to what's causing it or kind of a hypothesis testing all the time.


SPEAKER_00:
Yep, and this is a discrete model.

On the bottom, it says the form of this generative model is basically discrete.

But we could imagine it.

Let's just flesh out that experimental metaphor.

So let's just say the initial hidden state is that a bacteria of interest is present or not in a sample.

So if it's present, there's going to be DNA corresponding to this bacteria.

If it's not, there won't be DNA.

That's the likelihood mapping A. And so initially, we don't know.

And so we do an experiment.

We perturb the system by adding some chemicals, by doing a PCR reaction, and then we get the sensory data output.

And it's inconclusive.

And then we can update our policy until we get a sensory observation that's consistent with what is happening inside the test tube.

And that's why we have positive and negative controls.

Because if you just run one experiment, then you get a negative result.

Well, on one hand, it might be that there's no bacteria there, but also the likelihood mapping makes it so that, well, there could be a bacteria, but then it maps onto a negative result because the PCR didn't work.

And so that's what comes back to the mind of the scientist, whereas the scientist has to look in a sense

at the sensory data at the measurements empirically measured results but actually look beyond them to reduce uncertainty about something that's real like whether or not the bacteria are in that sample because it's not just did the test come back positive or negative that's going to be misleading because that makes it so that the s is the same as the hidden state but it's not the hidden state it's the outcome as mapped by a from the hidden state given your experimental setup

all within a policy situation or an experimental research program that's dictated by minimization of expected free energy.

Any thoughts on that, Yvonne, or we'll jump back to the paper?

Cool.

So in section two, let me just look at what the next figure is.

The next figure.

OK, so we're going to a few minutes.

We'll jump to the paper.

So this is going to be an introduction to state spaces and non-equilibrium dynamics, though I'm not quite sure where the bears come into play.

This is a key sentence.

Living organisms maintain phenotypic integrity and resist the tendency towards thermodynamic equilibrium with their ambient surroundings.

Full stop.

This is what it means to be alive.

This is Schrodinger's 1944.

What is life?

He says life is going to be locally thermodynamically organizational.

It's going to have negentropic characteristics.

It's going to locally organize.

matter and energy not violating the laws of physics because in the end the global system is disordered um but still the living system creates local order that's why you need to eat a few thousand calories of food to build one pound of muscle for example

And usually the fluctuation theorems that generalize the second law of thermodynamics would tend towards dissipation, which is why things tend to break down unless they're being actively repaired and rejuvenated like living systems are.

Living systems maintain their phenotypic integrity.

So they keep their leg at the same length or they keep their brain alive by bounding the entropy, which is also the dispersion or spread of their constituent states.

And so it's like my arm exists in an organized state right now.

where the bone is bone and the muscle is muscle and the fat is fat.

But you could imagine that if that were to break down so that just the carbons went floating free, then the organism would cease to exist.

So organisms exist by virtue of and by merit of maintaining their physical phenotypic representations coherent.

If it's not going to be a coherent phenotypic representation, it's gone.

To get a better handle on this, they introduced two formal notions, the state space and non-equilibrium steady states.

So there's, of course, a lot that could be discussed about state space, but a state space is a formalism that allows us to describe time evolution of a system

by depicting a trajectory through state space.

So a physical state space could be like your latitude and longitude.

And then the state space trajectory is your path through latitude and longitude.

But also, you could have a state space that has other axes that aren't latitude and longitude that are, for example, neural state space.

And that's bringing us back to this question of neural representations.

probability density that describes the system at non-equilibrium steady states aka their phenotypic states are aptly called non-equilibrium steady state density so my arm appears to be at an equilibrium in the sense that it's not growing or shrinking so why is that called non-equilibrium steady state well it's a steady state because in one respect the phenotype is unchanging

But that unchangingness is actually an active compromise that's being reached between the forces of order and disorder.

And in that sense, the total system is not in equilibrium.

The equilibrium system is the rock at the bottom of the hill.

That's the water when it's just at the bottom of the river.

There's nowhere to go from.

But when you have these two counter-regulatory processes of organization and disorganization, you end up with this very dynamic location in state space that's always being re-optimized.

And the universe gives us the disorder for free.

That's easy.

It's easy to imagine how things fall apart.

But what life succeeds at doing is maintaining order, even though there is disorder.

And then...

This idea that there's a non-equilibrium steady state density is consistent with far from equilibrium thermodynamics.

And this is just related to, again, to Schrodinger's question, which is like the living systems have to maintain order somehow.

They have to maintain far from equilibrium states.

The equilibrium state is all of your carbons are unlinked and they're blasted off as CO2 into the atmosphere.

So that's going to be the most well-mixed thermodynamic state, but we're not that.

We're phenotypically coherent.

So what are living systems and what allows them to stay organized despite that dissipative force?

That is where the Markov blanket comes into play.

It turns out that by using the Markov blanket formalism, at first, just as a scientific heuristic, but we'll see it's actually deeper than that, we can make clean distinctions between organismal internal states, which are doing inference on the external states of the world,

active states, which are the active states that actually move the organism actively into a better realm of state space, like a better temperature for you to exist within.

And then, so we already talked about this partitioning, but now we're thinking about this in terms of the internal states being far from equilibrium.

Any thoughts or questions on that?

Okay, so here's where we're going to get to the information geometry, as well as the physics of sentient systems.

So this is a big topic.

We're not going to have 50 hours to unpack it, but this is why we're part of the Journal Club series, because these are the ideas that we want to come back to again and again.

The description of a system in terms of movements in internal phase space is the system's intrinsic information geometry, closely related to measure theory and statistical thermodynamics.

So regular geometry would be in the phase space of the piece of paper.

X and y-coordinates geometry is like a triangle has a certain shape.

And then you can imagine there's a different state space, like non-Euclidean geometry, where the triangle has different rules.

But you can still talk about geometry and maybe even things like triangles, but they don't necessarily need to correspond to the angles must sum to 180, because it's about the total system's representation, about the rules of the system, and about the geometry of the system and the state space the system exists within.

All those are really well linked.

So information geometry

is asking about how we can do geometry not on xy on the piece of paper not on latitude longitude with a map but how could we do geometry on the informational internal states so it's a it's a big idea and there's a lot to it but what is that um what does that ring with either of you two

And here's, go ahead, Sasha.


SPEAKER_01:
It's, yeah, it sounds like kind of a different way to phrase, like topology or mapping relationship between different ideas or different people.

So thinking of like network analysis on these different concepts.


SPEAKER_00:
Yep, and here's that physical metaphor where in Euclidean geometry, if you move 100 meters, then you'll move 100 meters.

So there's a one-to-one relationship between the physical movement and the Euclidean informational distance.

However, if you're on a sphere,

if you move in the same direction, you're gonna end up zero.

And so that's like kind of a modulus operation in mathematics.

And as opposed to saying, well, 10 plus 10 equals 20.

Okay, but what if you do modulus 10, then 10 goes to zero and 20 goes to zero.

So those numbers, zero, 10 and 20 are different in the number line in the Euclidean world, but there's another system, another state space representation where actually zero, 10 and 20

are the same point in that state space, which is to say that their informational distance is zero.

Another interesting thing about bringing it to the concept of information is that the informational divergence between two things is always positive.

So you can have, and that's kind of like the distance between two things is always going to be a positive number.

You could say on this number line, 10 minus 30 is negative 20.

But if you have something on the 30 yard line and the 10 yard line of a football field, the distance between them is positive.

So the number line is where we get things like negative numbers.

But within very well specified systems, information geometry also can tell us a lot more than other approaches.

Here, I don't think in this specific conversation we'll have too much time to go into the equations of information geometry, where they use the Fisher information metric as a sufficient statistic that corresponds to the expected thermodynamic and external states and internal states.

But let's put a flag here so that we know that we can come back to talk about this when we have some more colleagues on the conversation.

Then they get to phenotype as a tale of these densities.

And they say that the phenotype is a free energy principle is a story about two probability densities.

The first is this non-equilibrium steady state density itself, which is the statistical structure of the phenotype.

So that's that realist, that's the deflationary realism.

And the second, in the sense that this is the phenotype, like your arm is a representation of the arm.

It is itself.

The second is the variational density, which is parameterized by the internal states of the system.

And so that's where we're gonna already see that we can reach into that semantic world.

I'm going to just skip through this part so that we can get to the the synthesis of the paper and the last figures.

So here's where we get to figure three.

So now we're seeing that through this pretty elegant and minimalist structure, when the organism minimizes its expected free energy, which is always going to be a positive number because the informational divergence between any two distributions is positive.

If you minimize G, you're going to come to the best pie, the best policy selection.

And that policy selection is going to use input from the emitted sensory states to reduce uncertainty about hidden causes in the world, D.

And this is where they get to the questions about content of representation.

And so here is where they're gonna return to the desired features of neural representation.

But here is where they approach it, the mathematical contents of a neural representation and where they propose based upon Egan 2019, a computational theory proper for representation.

And so the aspects of representation that they're going to pursue are the mathematical function that's being realized.

That's the mathematical component.

Then there's the specific algorithms or the process theory by which the system actually does compute that.

So it's one thing to say, well, the ants are doing a NP-hard optimization problem.

It's another thing to ask, what are the rules that each ant is engaged in such that that optimization is the outcome?

Third are the questions about representational structures.

So that's like not what is the ant colony doing or what does one ant do, but what is the representation of the ants across the whole colony through time as it solves the problem?

What are the computational processes that are defined over representations?

So the representation is that distribution of ants in the colony, the distribution of active neural states in the brain.

And what is the computational process that is defined over representations that links them together so that they accomplish

the mathematical function.

And then lastly is this ecological component, which is really so critical, which is the recognition that the internal dynamics of the system, one through four, the math, the algorithm, the representation, the computational component, that's introspective.

And we're never going to understand function or adaptiveness or functionality without also understanding ecologically what is the context that the algorithm is occurring within.

Um, and then is there any thoughts on that from either of you two?

Cool.

I thought this was really a nice way to come from a really well-grounded, uh, definition of what all these different components of representation, and then in the end, ground it to the real system, which is the ecosystem.

Okay.

Um, now they're going to add this heuristic cognitive content, and here we can look at figure four.

So in figure four, this is the deflationary account of the contents of representation.

So remember, deflationary just means it's just a party.

OK, it's not a big deal.

It's just a party.

So here is how they're going to go about deflating representations.

And they're going to split it into two parts that aids in the deflation.

It's kind of like when you get a package and there's the air bubbles and it's partitioned, you need to deflate each one of those partitions.

So they're going to separate the world into two kinds of components, cognitive and computational, and then they're going to deflate

those two aspects.

So computationally, we're going to draw off of those Egan terms that we just went through, one through five.

So there we have mathematical functions, algorithms, representations, computational processes, and ecology.

Those are all computational.

Like in the sense of the ecological component, whatever it is, it's just that.

There's no magic in the ecology.

There's no magic in the computation.

And then over on the left side here, the cognitive component, it's actually, there's just one air bubble to the plate.

Cognition is just about intention.

The intentional gloss is what they call it here.

Here, the cognitive content is taken, I'll wait for that little thing to disappear.

is taken in an anti-realist sense in a type of explanatory gloss that only has an explanatory instrumentalist or, remember, fiction, organismal-centered fiction role as the interpretation given to the neural representation by the experimenter.

Like, oh, when this set of neurons is activated, the mouse wants to drink.

And so you could say computationally, here's the mathematics.

Here's the algorithm that's happening.

The representation, the computation, and the ecology with the water.

Those are all just what they are.

And then there's the cognitive element, which we've now isolated.

So that's really where we've gotten to is we can isolate the cognitive part and say, this is the intentional gloss.

This is the mouse thinking about being thirsty, experiencing being thirsty, wanting to be thirsty, wanting water.

So that's a really nice partitioning.

And that's still within the traditional frameworks of computational neuro as well as cognitive science.

Any thoughts on figure four?

Cool.

then we go back to the paper we hear a little bit more about fictionalism in the philosophy of science and suffice to say there's a long uh and storied history of fictionalism in science uh because science is always making models that are as if they're true about the world but

Here's where they say they're used by scientists to explain intentional behavior.

They're models used by scientists.

Again, this is kind of a circular definition.

It's like saying cognitive neuroscience is whatever cognitive neuroscientists do.

It's like, yes, it's true.

And then we can also look beyond that.

And they discuss this, this idea of empirical adequacy or true enough.

And that's a key idea.

And it's kind of like when people say, let the data speak or the data are clear.

Well, the data were clear on epicycles and people were making more and more epicycles and getting more and more empirical adequacy.

And it was true enough for the people who were using them.

But it was wrong.

So empirical adequacy always must be interpreted in a really holistic way where even if the model precision is extremely high, even if the model precision is increasing, you still could be just locally overfitting and totally barking up the wrong tree.

Okay, finally, near the end of our discussion, we reach this variational semantics where we're going to actually bring all these things together.

We have the generative models, active inference, and the free energy principle on one side.

We have semantics, and then we have phenotype.

And we're gonna say phenotype is just phenotype.

The brain's just doing brain stuff.

The arm is just an arm.

A frog is just a frog.

And semantics are just that.

They're just meaning.

but how are we gonna bridge these two worlds?

How are we gonna go from just neural representation to just meaning?

Well, first they bring up the viewpoint that the deflationary view of representation downplays the role of the fifth ecological component of the computational theory proper.

So what their claim is here is that deflationary representationalism from a computational perspective, which is like mainstream science,

discusses the first four points here a lot, but they ignore the ecological component.

So I certainly resonate with this critique.

And that's why earlier I said, this is one of the strengths of the free energy principle is it reminds us even when we're drilling down into the thirst center of the mouse,

How could you ever really extract that from the ecological context where there's water or no water, the body states where it's dehydrated or not?

And how could you even extract that from the evolutionary history of water being available or being associated with other sensory cues?

Here's where they connect it to the free energy principle.

They're going to say, the formalism that is underwriting the free energy principle licenses or allows us to have a crucial observation, which is that the mathematical structures and processes are defined over a state space, remember information geometry, and implicitly over an associated belief space or a statistical manifold.

So manifold is a lower dimensional representation of a higher dimensional

pattern.

So if you have some sort of high dimensional pattern, but you project it into low dimensions, it might exist on a line in two dimensions, even though it was a cluster in 100 dimensions.

And that's sort of the idea of these manifold techniques.

And another mapping that to bring it one level closer to the paper is like there's a manifold that's a continuum thirsty to not thirsty.

And then there's so many neural states.

I mean, we have billions of neurons and other cells.

Maybe individual neural states are never replicated ever.

because each one is so unique and the brain is always changing, but you can project down onto this internal manifold, which is thirsty to not thirsty across your life, even as the number of neurons that you have changes or the neurons fundamentally change themselves.

So we're going to use this idea that the environment is important and that the internal states are mapping onto an internal statistical manifold, specifically an action-oriented one.

And then we're going to bring that to semantics.

Any thoughts before we move forward?

Okay, cool.

So...

It is often noted that one does not obtain semantic content from mere systematic co-variation.

So that's sort of the Helen Keller initial learning approach which was like she learned famously by having water on her hand and then having sign language spell out water on her hand.

So that was an instantaneous co-variation between a symbol string of sign language and a sensory experience of water.

And so that was like the Rosetta Stone

for linking an external event and sensory sequence to symbols.

That's lexical semantics delivered as sign language.

But that's not how we get all of our semantic content.

Under the free energy principle, there is an implicit semantics at play that is baked into the system's dynamics by evolution.

So there's something about sugar that tastes good.

There's something about warmth at the right level that feels good.

That is something that evolution has endowed us with because we're not simply randomly assembled combinations of phenotypes.

We've been ecologically selected to experience a semantic quality to different types of sensory experiences.

And that's not to say that they're not plastic or they can't be learnt or nuance can't be added, but there is potentially within this trajectory of states on the internal manifold,

There actually is the hope of a formal semantics that could fall out of this system's dynamics and therefore be characterized mathematically.

But the hope for a formal semantics that arises from phenotype representation is there because the world actually does have order and stimuli actually do have valence in the world.

Any thoughts on that?

Okay.

Here's where we take stock in section 4.2.

We have retained the general description of representational content from the deflationary account.

So in other words, we got what we wanted out of the deflationary account, which is to say brains are just brains, ecosystems are just ecosystems, meaning is just meaning.

We deflated everything else away.

Now we're left with just what's there.

We now will use this deflationary model to specify a computational theory proper that leads to a formal semantics via the free energy principle.

So let's return here.

Here's the fallacy that people were making.

They would define mathematics, algorithms, representation, and computational processes of a system, bacteria, or the mouse, or a simulation.

But it was always focused on the system of interest.

And so without specifying the ecology, it was kind of like building a castle on nothing because the internal representations, the computational components were not grounded in the evolutionary or ecological niche of function.

And therefore, the cognitive component and the intentional gloss was kind of open ended.

It's like, okay, this neuron is activated when the mouse is drinking water.

So maybe that means the mouse is thirsty.

The missing piece was the ecological component.

Then they're going to, they say, figure four, I believe that they mean figure five, or maybe they are referring to the previous figure four.

But here is where they're going to reformat Egan's scheme, which is here on the screen.

They're going to reformat that in terms of the paradigm of free energy principle.

So this is like the big synthesis piece.

This is almost at the end of our discussion where we've really reached their synthesis.

They're saying, look, the common framework is computational dynamics, not computational components and cognitive components that you're going to split up.

But it's all about computational dynamics.

That's as deflationary as you can get.

We separated and deflated into a cognitive component.

And then guess what?

We realized that by having an ecological component, we don't even need intention.

Because intention, oh, well, maybe if this neuron is activated, the mouse is thirsty.

Now you don't need to worry about that.

You can just say the mouse is this ecological agent that has to reduce its uncertainty about how thirsty it is.

And using that partitioning of the Fisher information equations that we kind of passed over earlier.

We can formalize the ecological relationship of the mouse to the surroundings.

And by doing so, we can get intentional like claims like the mouse wants to drink water.

It's trying to open up the water bottle.

It's running as fast as it can to get water.

We get these types of intentional like claims coming out of here from the ecological component.

vis-a-vis dual information geometry.

Here is where they unpack each of these components of Egan within the free energy principle.

The mathematical function that is being enacted is the free energy functional, which is the minimization of expected free energy given the generative model of the organism.

The algorithm that the organism uses to carry this out is a stochastic variational gradient descent on free energy.

which we'll show one little thing after this.

The representational structure are the internal states of the model reflected by these lower dimensional manifolds.

And then the computational process by which these representations morph and evolve is active inference.

And then we're grounding it in the piece that is most lacking for mainstream science, which is the ecological component specifically formalized here as dual information geometry.

Okay.

That's the big claim.

Any thoughts on that?

Or how does that strike both of you after hearing that whole lead up?


SPEAKER_01:
I think this figure sums it up really nicely and draws the kind of parallels that encompass the setup of the aims.

So it's really nice.

And I agree that the ecological component is

is lacking and is quite important in actually understanding what the system is doing.


SPEAKER_00:
Cool.

Yvonne, anything?

Okay, so let's return to the paper.

Much hangs philosophically on what it means to represent a target domain in terms of the relationship between mental states, mind, and physical states that realize them, brain.

So now we're talking about the mind brain mapping.

Mind is the intentional gloss.

That's like, well, the mind wants this.

And then the question has always been, well, how is the mind related to the brain?

And we're not coming at this from the question of how is consciousness generated in the brain or other areas that also deal with mind and brain duality, but just specifically using representationism as the wedge.

There's a representation of wanting water, and then there's the brain state.

And that's what we've been all about is exploring this connection between wanting water and actual neural firing patterns and non neural things happening like hormone levels and all these other components that aren't even being attempted to be described by neural network models or other features.

Now, this is where they philosophically tie it all together.

And again, this Fristin et al.

2020 paper, there's a lot more philosophy to delve into in the citations.

And these will be papers we explore in future discussions.

Of the philosophical perspectives that relate mental and physical states, mind and brain, ours is most consonant with functionalism and therefore the multiple realizations that it entails.

Functionalism is the view that features that characterize mental states are not intrinsic features of a state, but rather the function, which is the input-output mapping between that state and other states of the system.

So here's where they're tying it to a lot of other previous ideas related to cognitive science and other areas.

Then they return to some of those earlier questions like, can you get a misrepresentation?

And they then talk about some future research where you actually are able to objectively ask how well does the sensory data conform to hypotheses about what caused them?

So that would open up the door to misrepresentation.

not that it's a binary state, correct or incorrect representation, but you'd be able to look at two different representations and ask which one was more accurate.

If you have two maps of the subway, and one of them is causing you no surprises when you're using it, and the other one, there's outdated stops and the schedule is off, it's pretty clear that that state representation of the map is less adequate.

It's less good of a model.

And that gives us this key feature of representations being able to misrepresent

which is important for being able to act well, but we're also not going to fall into this absolutist trap where there's like a best representation or only representation.

And that's actually the last paragraph.

They basically ask that, yes, under the free energy principle, there are structures internal to an organism that are the bearers of semantic content.

We can specify these internal structures in terms of deflationary computational theories, figure five,

And by virtue of the double information geometries that are in play under the free energy principle, that's the thermodynamic or the thermo-informational partitioning of internal and external states by Markov blankets.

So we'd make this dual information geometry ecological partitioning.

And then we actually can use our mathematical account to reach an implicit semantics, which is the set of hypotheses about underlying causal factors that the system is parsing in order to make sense of its sensory string.

This might be seen as vindicating the structural representationalist account.

In other words, so if it's deflationary, are we vindicating the idea that it's just neurons?

But there's a critical twist.

The critical twist is that those structures that bear content are not merely neural representations, but indeed phenotypic representations, for it is the internal states of an organism, given the Markovian partition, that bear this content.

And this is like a really amazing way to close the paper because it's saying, yes, neurons are involved, but depending on what level the cognitive process is playing out on, that is going to co-define where the Markovian blanket is.

So for example, if multiple people are on a team together and the team is engaged in collective cognition, then that team Markov blanket is actually the one that is bearing the representational content.

So a multipartite dialogue is actually being represented at the group level, though there also could be smaller Markov blankets that could be drawn around each person, each brain region, each neuron, each organelle.

Everything is always fractal, hierarchical, embedded, but for utility, we can actually draw out these partitions as they've suggested.

So that is the end of the paper.

It's almost the end of our time.

And I think it's just good if we had any closing thoughts before we shut it down.

But this was really an awesome discussion.

So maybe what's one thing that you took away from the paper or one thing that you're still wondering about at the end of it?


SPEAKER_02:
I just understand.

I need to read it again.

One more time.

I had your presentation and I go through the paper with it, but still I need to dig more.


SPEAKER_00:
Perfect.

Always we can dig deeper.

Sasha?


SPEAKER_01:
Yeah, that was really useful to walk through this whole paper.

Incredibly dense, but I like what you said about the last closing statement is that it's kind of a, yeah, free energy is the lens to look through.

And by placing the Markov blanket at different places in the system, it can be scientifically useful for us to understand the system.


SPEAKER_00:
as it's useful to the organism that's using that representation well said well said so that concludes our discussion thanks a lot for listening everybody who's listening live or in replay this has been a team com podcast number two and um we are always open to participants and to suggestions about how

we could discuss these ideas better, how we could bring it to the people in a more exciting and participatory way.

So get in touch with us.

Thanks again for listening.

And I'm going to.
SPEAKER_04:
Hello, everyone.

Welcome to ACT-INF Lab live stream number 25.2.

Today is July 13th, 2021, and we're going to be talking about this paper, The Computational Boundary of a Self, Developmental Bioelectricity Drives Multicellularity and Scale-Free Cognition.

We are here with the author, Mike Levin.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links here on this page.

This is a recorded and an archived live stream, so please provide us with feedback so that we can improve our work.

All backgrounds and perspectives are welcome here, and we will be following good video etiquette for live streams.

Here at the short link, you'll find all the live streams that we've done so far, and we're going to have a new series of live streams coming up in the next semester.

So we're going to be jumping off here with the author discussing this paper, and we will just do a brief round of introductions.

I am Blue Knight, and I am an independent research consultant based out of New Mexico.


SPEAKER_03:
Yeah, my name is Sarah Davis.

I'm a master's philosophy student at this point, past engineer and science artist, just generally interested in how the world works.

Michael?


SPEAKER_01:
Yeah, I'm Mike Levin.

I'm a professor in the biology department at Tufts University.

I run the Allen Discovery Center at Tufts, and I'm also an associate faculty at the Institute at Harvard.


SPEAKER_04:
Cool.

So, Sarah, what is something that you're excited to talk about today or something that you liked or remembered about the paper?

Maybe want to give a heads up?


SPEAKER_03:
The most interesting kind of through line for me in all of Michael's work that I've looked at is this connection or this maybe tension, I don't know, the way the morphology plays with electricity.

Just, yeah, which one's in the driver's seat or some kind of like understanding of how those two work together.


SPEAKER_04:
Awesome.

Is there any like burning questions that you have that you want to start off with or otherwise I can just start?


SPEAKER_03:
No, I feel like that is going to kind of happen down the road, that particular focus.

So please.

Yeah.


SPEAKER_04:
Okay.

So here we are on the paper we're going to be discussing today.

How do multiple nested scales of individuality work?


SPEAKER_01:
Yeah, so I want to preface this by saying that, and maybe this is obvious, but I just want to say that all of this is very much a work in progress.

So I'm going to tell you what I think about these things, but I certainly am not claiming that this is all worked out, or that I'm not going to change my mind at some point, or that this is not going to develop in some fashion.

This is under constant work.

So I think the fundamentals of all this are simply that

there are multiple interpenetrating systems at different scales that are all present simultaneously, all of which can be profitably looked at as individuals.

And so this diagram here sort of, it looks like all it's saying is the mere sort of fact of physical structure.

If you look, you see organisms and within that you've got organs and cells.

So that's sort of not the whole point of this.

The point isn't merely that they're arranged this way, but that you've got subunits, and those subunits have themselves subunits, all of which are selves.

And what I mean by them being selves, and this is the central claim of the paper, is that

They are to some extent, so not binary, but on a continuum of agency, they are to some extent goal directed agents.

So they're trying to achieve certain things.

And so, when I say that they're nested, all of this simply means that you can have systems where the different parts.

Are all actively trying to achieve various things in their own spaces and we should talk about that.

That's an important part that developed since this paper was written is this idea of different action spaces that these things are working in.

But, but they're nested in the sense that I think just because a higher level system acquires a goal, or is pursuing a goal that doesn't mean the lower systems aren't doing it and.

All of these things are, in fact, simultaneously doing their best to pursue various kinds of goals.

So that's what I meant by multiple nested individuals.


SPEAKER_04:
Awesome.

So is there a total 100% of individuality that gets partitioned?

And something that I've been really interested in is how is information passed forward and backward across levels?

Like is there some kind of core screening or like salience like dominates the information to pass forward?

So how does that kind of information flow work?


SPEAKER_01:
yeah so so the scaling and the relationship between layers is of course one of the most interesting things here so one of the things we are currently trying to do both experimentally and with modeling is to really show some very tight examples because this paper is largely qualitative but we're now trying to show some very specific examples where you where we can we can actually track the scaling so so you have lower level subunits so let's say you have cells

that have only local metabolic goals.

So all they know how to do is pursue some energy level in a kind of homeostasis to stay alive.

And so the question is, how do you connect those in a way that is then going to give rise to a larger agent that has much larger scale goals that can be working towards a state of affairs and to be actually stressed by the failure to reach a state of affairs that is way bigger than any individual cell.

And so I'm not sure that there's any kind of conservation here in the sense that there's only a total amount that you sort of have to partition it among the levels and that's all that there is.

I'm not sure that's, yeah, I would make that claim.

I think that the total amount of individuality can change over time and it can rise and fall.

And in fact, the boundaries between the different agents can grow and shrink.

That was a significant part of the paper talk about that.

um and what there's a couple of there's a couple of interesting things that get passed upwards so to speak um definitely kind of force graining in the sense that if you are a system whose parts are themselves competent in getting various things done despite changing conditions and and so on they're not just the hardwired mechanisms but actually competent to pursue specific outcomes despite perturbations what it means is that the larger system

is now working in a much easier space in the sense that if it doesn't have to micromanage all the microstates that the individual pieces are doing, but it can rely on those components to get their job done, then what you're really working in is a much lower dimensional, simpler space

Problem solving gets easier and just as a simple biological example, we now know that, for example, in the tadpole, if you activate a signal that triggers, it happens to be bioelectric signal that triggers eye formation in another part of the embryo, let's say on the tail.

then all kinds of interesting things will happen it will it will build an eye it will recruit cells that you never directly manipulated so it's a local self-organization process that i will form even though it's sitting in the middle of muscle instead of in the brain where where it belongs all of these things are going to happen anyway if you're if you're the larger and and by the way those tackles can then see out of that eye even though it's on the tail instead of in the head so if you are a system that um

uh needs to solve some problems in terms of uh i've got some eyes and i need to do this behavior that i'm doing and so on if you don't have to micromanage all of those components and saying each cell of that eye where exactly it's going to go what exactly it's going to attach to and so on if if there is an effector that you have access to an affordance maybe that says um build an eye and you as a larger system don't need to know how that's going to happen you can just

that is going to happen under a wide range of conditions.

You have a much simpler space to work in and the problem solving.

That's one of the things we're doing now is trying to map out some of the different spaces in which all these different agents are actually solving problems and trying to define what that means.

So one of the things that gets passed up is this course grading because of the lower level competency.

So that's I think a key thing and maybe we'll get to talking about what that does for evolution because I think it's massively important.


SPEAKER_03:
Nice, so I don't have a unfortunately, they've moved to change the interface.

So I don't know where the hand is anymore in this, but 1 question that keeps coming up for me and all the different.

Realms of discussion about this is like, well, in particular with the bio electricity part, you use the word.

Um, subroutines when you were talking about, you know, we, we think we're starting to understand the, the, the.

Whatever combinatorics of of how this works and and and also related to these nested levels and things like this.

I, I just keep wondering if it's your sense from all of this work that you've done that.

In nature in in in the communication between these levels.

that there's any amount of abstraction or semantic layer that's needed for that to happen between levels.

And also with the electricity, subroutine implies that there's some kind of semantics that something has to rest on.

And that just keeps coming back to me.

Do you have any sense about that?


SPEAKER_01:
Yeah, so a couple of things.

So one thing about bioelectricity is that

bioelectricity, I think, is interesting not because it's in some sense magical or there's something uniquely better about bioelectricity than biomechanical or biochemical modes of interaction.

What I think is really interesting about bioelectricity is that

that is what is being used.

And perhaps these other modes are too, we don't know.

But bioelectricity for sure is being used as the computational medium by which the lower levels bind together into the higher level cognition.

So the higher systems cognition, whatever level it may have.

So what the bioelectricity allows us to do is to directly peek into

that computation.

And so I think that's why it's interesting, which is no surprise.

I mean, that's exactly when people do neural decoding, that's exactly what they hope, right?

Is that by tracking the bioelectricity, they learn something about the information content of the global system, right?

What is the animal or the human thinking about

So it's exactly the same here.

So that's what's unique about bioelectricity.

And I think that what evolution discovered very early on is that, and then it capitalized on that by pivoting it into neural kinds of systems, and we do it in our computational devices and so on.

is that what evolution found really, really early, around the time of bacterial biofilms actually, is that electricity is a great way to process information, to integrate information across distance, and to do computations.

That shouldn't be a surprise to anybody.

So I don't know if I would claim that it has any kind of a formal syntax at these levels or anything like that.

But what it definitely does do is a kind of, when I say subroutine, what I'm mostly leaning on is this idea of modularity, but not just modularity in form, because that's been discussed a lot in evolutionary developmental biology.

actually modularity of function so so the key that the thing the thing about a good subroutine is that you're going to when you activate the subroutine you can move along and assume that everything that's necessary to get that job done given whatever local conditions or current events or whatever else is happening

that subroutine is going to encapsulate everything that's needed to get it done meaning it's it's it's not just it's it's it's the difference so for people who code it's the difference between sort of macro substitution where all you're doing is just you know it's a shorthand for a bunch of hardwired steps which plopping that in i don't think that's the the magic here at all i think what it is is

it's a degree of competency in that subroutine to get something accomplished.

And that has really important implications for evolvability and so on.

And what the bioelectricity allows you to do is to

Basically, to have to have a language in which the system can call up such circuits with specific behaviors at different points or at different times.

That makes it really, really powerful.

But the question of whether it has any kind of formal syntax or anything like that, I don't have good evidence for that.


SPEAKER_03:
I don't even mean like a formal syntax.

It's almost like a

And this also, you know, holds true between these layers of these nested layers of organism or whatever.

Is if there's if you have a sense that there's any abstraction, there's any abstraction layer needed and I guess you kind of answered it maybe by saying, you know, that electricity is this medium on which.

Things communicate or but yeah, anyway, it's I don't think it's an easy.

It's like.

You know, easy to answer question, but that's the theme that keeps coming back to me.

So.


SPEAKER_01:
I think there's a lot of abstraction in the sense of, and I don't know if this captures what you're asking, but there's a lot of abstraction in the sense of generalization in terms of

Another way to say it is coarse-grained.

Voltage itself, let's just start with that.

Voltage itself is an abstraction for cells because, and in fact, people will often ask if I say there's a bioelectrical signal, they say, well, is it the potassium level?

Is it the sodium level?

Is it the particular ion channel gene?


SPEAKER_03:
Is it the voltage or is it the current?

Yeah.


SPEAKER_01:
Yeah, and in particular, so the cool thing about bioelectrics is that that question has been answered, and we know the answer.

It's neither of those things.

So it's a pattern of voltage that is specific for the downstream effects, and it doesn't matter how, with very rare exceptions, it doesn't matter how you got to that voltage.

So you can use sodium channels, you can use potassium channels, chloride.

What I used all those micro details don't matter at all, because what the collective is heating off of is the spatial temporal distribution of voltage and you can get exactly the same results with sodium or potassium or chloride as long as you're.

You know you're you're driving the right voltage patterns so voltage itself is a really cool abstraction because it means that all the downs, the underlying mechanisms are free to diverge and evolution want to swap out of potassium channel.

for a sodium channel, you can do that as long as you've got the right functional properties so that your overall pattern stays.

And we use that quite often in our regenerative medicine applications with a really convenient property.

So yeah, so the bioelectric code itself is definitely an abstraction over what the micro states in terms of specific ions or the specific channel genes that got you there.


SPEAKER_04:
Nice.

Stephen, did you want to say hello?


SPEAKER_00:
Yes, hello.

Hi there.

Hi, Michael.

Thanks for joining us.

Yeah, I was curious.

We had a really interesting... Chris Field gave us a really good insight into the quantum contextuality idea of how things could...

build up from different sort of space states at very small levels and active inference often uses non-equilibrium steady state attractors as kind of a way to look at some sort of statistics between the scales you know there's some sort of flux or flow that's inferred and I find it really useful thinking about it with this cone idea that you you bring to the table because it it there's more

It's more of an idea of a distinct swarming going into a structure and some sort of system that might emerge from that, which is sometimes lost when it's very much in the math.

And I'm curious whether when you think about the bioelectric effects, and the same sort of question came up with quantum contextuality maybe, is there more of a distinct scale at which

there's a coherence, you know, like you've got the cell membrane, then you've got a coherence around which the bioelectrics can operate, so to speak.

And then you might have another distinct level at which the bioelectrics can operate, which is maybe more than the more gradated idea of steady state statistical sort of manifolds.

So I was wondering what your thoughts are about how and where there's sort of

scales at which bioelectrics become particularly coherent, or if that even happens?


SPEAKER_01:
Yeah, I mean, so again, prefacing this by saying that the bioelectrics is not an essential part of the story.

except insofar as I think morphogenesis is a nice example of an unconventional agent within which to play with these kind of ideas.

So what I would like to do, and this is a framework that I've been developing recently, is to get away from the standard sorts of living creatures that we're used to, so the products of that one trace of evolution in the biosphere on Earth,

and to go beyond that and to look at the space of possible agents, the space of possible bodies and the space of possible minds that those bodies will support and think more broadly.

So in doing that,

I think that morphogenesis is a nice example of an unconventional agent that lets us see how the framework is to be practically applied.

So people say, okay, this is all well and good, but what use is it?

And I say, okay, let's apply it to a specific thing that really no one thinks of as a cognitive agent.

And I'm going to show you how all this stuff maps and why it's useful at the bench and it helps do new experiments and so on.

so if we if we accept that morphogenesis is kind of a test case for and there are many others it's certainly not unique but it's a test case for an unconventional place to find um agency and all of that and to test out how well this theory helps us out then what's cool about bioelectricity is that it actually allows us to tell a very specific mechanistic testable empirically useful story about how the information flow happens and how the levels scale but it's just one

you know, it's just, that just happens to be how the morphogenetic self does things, and there are plenty of other interesting, you know, cells that wouldn't have anything to do with bioelectricity.

But I just want to make that clear, right, that bioelectricity is not, you know, some sort of essential element that's always going to be present whenever we apply these things.

It just sort of, it's central to the example case that I've trotted out, which is morphogenesis.

Now, having said that,

Bioelectricity is definitely multi-scale because you have bioelectric dynamics in organelles in a very small scale.

You have the resting potential across the cell membrane, which is mainly what we study.

But those scale into tissues, and you have global electrical fields, which are driven by trans-epithelial potentials on the scale of tissues and organs, which other people like Min Zhao and many other people study.

And then past that, you have whole animal scale, like whole body scale fields as well.

So all of these different levels exist.

Some of them more and some less rely on bioelectricity.

But yeah, it's relevant.

In this particular unconventional agent, it's relevant to all the scales that we know.


SPEAKER_00:
Okay, that's really helpful.

Thanks.

And would you say that the cone concept

could it extrapolated to other types of multi-scale piston blanket type ideas?

And I was just wondering what your thoughts on that, or whether that just confuses things to try that.


SPEAKER_01:
No, I don't think it confuses now.

I'll say I'm for sure not.

I mean, I like Karl's ideas very much.

I'm by no means an expert on the map, and I'm not going to pretend that I understand all that.

I think that the thing about these kind of diagrams is they're not meant to be unique or exhaustive.

The goal of this originally, this came about a couple years ago at a Templeton meeting where we were asked to brainstorm ways to define almost a sort of eigen space where you're able to compare directly really diverse intelligences, right?

So things where you can't just measure all their brains and then see how they shake out.

It's really, really diverse intelligence.

So what I tried to come up with is a scale and it's all focused around the spatial temporal scale of the goals that any given agent can work towards and the scale of states of affairs that can stress out that agent when they're not being met and so on.

as a way to pick on something that I think is central to all agents, no matter what they're made of, no matter how they got here, evolved, designed, combinations, chimeras, whatever.

Wherever they came from and whatever they are, what is essential to them?

And I picked this goal of directiveness, but this is compatible with all kinds of other systems.

So you could overlay on top of this almost anything else that you thought was critical about being an agent,

just start adding dimensions, you know, and absolutely, right now, you can just, this is compatible with all sorts of things.

This is not meant to rule out any other, anybody else's, you know, favorite framework.

This thing, it captures one slice of what's essential about being a self.


SPEAKER_00:
Thanks, that's really useful.

Actually, can I just, one more thing,

I'm actually really interested in this.

I do a lot of work with community psychology.

So there's this challenge of going from the self, there's like the self and then there's society, social, but the intersubjective kind of group dynamics, there's a big problem because it's like a big hole.

And I find this really useful because

a lot of stuff which talks about how groups they often try to project out into the future as if it's a light beam that you could measure whereas here it's much more you know like it's sitting in the swarming i like to think the bottom of the cone is almost the swarming dynamics that go down into the past you've got the kind of structure that the cell that's emerged and then the system that's able to go into temporal depth in the future and the code and i think that

that offers a nice way to think about group dynamics particularly maybe the more ironically the more challenging group dynamics where there isn't a coherent single narrative so there might be applications there


SPEAKER_01:
Yeah, this is super interesting.

And I should say, one of the things about the cone, and we had a discussion about this yesterday, and Pranab Das pointed out that my cone compared to Minkowski's cone is upside down, right?

And the reason I did it this way was because if you think about

Where any given agent, the only thing the given agent is really certain of at the moment is what's going on right now.

So your future obviously is uncertain.

And when you're trying to predict, so let's say you're trying to manage goals and you're trying to predict, you've got some predictive capacity going forward.

That predictive capacity doesn't get better over time.

It gets worse over time, right?

you're sort of exponentially more uncertain about what's going to happen later on.

So your ability to pursue those goals in the future gets smaller and smaller.

Same thing goes back in the past, although you could argue that that's linear and not exponential.

But again, what you're doing at any given moment in time is inferring what your past was from the engrams of memory that are available to you.

They might be a brain, they might be a stigmergic medium that you live in, you know, within if you're an ant colony, whatever it's going to be.

You've got some kind of material in which events leave memories, they leave traces.

At any given moment, you are reading those traces, recalling them, and reconstructing what you think the past was.

That's your only evidence for what the past was.

You have no idea what actually happened.

And so even going backwards, my cone comes down because your ability to go backwards in time and be certain of what went on and what are the facts on which you can now build your goal-directed strategy going forward become more and more uncertain going back.

So that's why my cones are upside down in this, so to speak, also because there's no particular reason this has to map exactly onto Minkowski's

Informalism, but I think it's close that just because that your level of certainty is highest right now, right?

Both in space and time.

And when you step away from that rapidly, at least for all of us rapidly get smaller.


SPEAKER_04:
Yeah, we were really actually talking about this last week in our discussion, and we were kind of thinking about this as like an information cone, because you have the most information about your environment at this point in space and less information as you go forward and backward in time.

So do you think that that might be the correct way to think about it?


SPEAKER_01:
So that's true.

That's absolutely true.

It's definitely also an information cone.

But I want to be clear about this because this diagram is not a diagram of what you can sense or how far your effectors range.

What you just said about the information certainty is the consequences of that are that you also have a limited cone in what the diagram really is supposed to show, which is the scale of the goals towards which you can work.

So, because the information is smaller, your ability to pursue goals towards, you know, how far into the future am I actively, you know, doing, it just becomes limited, right?

Your ability to do so.

So, it's sort of a secondary thing.

consequence of that, but I just want to be clear that this is not meant to be fundamentally a picture of the information certainty of sensory capacity of effector range or anything like that.

This is a diagram in gold space.


SPEAKER_04:
Nice.

So one of the other ways I've been thinking about it is kind of like a possibility cone.

So like, as you progress forward into the future, we all are left with one possibility and that's like death, right?

So your goal at that moment is death.

And similarly in the past, like we all come from a single celled, like, you know, fertilization event.

And so like we all started an event and ended an event and like the maximum possibility is now.

So what do you think about that?


SPEAKER_01:
Yeah, I mean, so that's a really interesting point, right?

And people talk about, you know, great transitions in cognition along the continuum and so on.

And so I think one great cognition, a great transition is the following.

If you are a creature with a horizon, with a cognitive horizon of, you know, a half an hour, you're a goldfish or something.

I don't know if this is factually true, but, you know, you're some sort of form that has a cognitive horizon of a half an hour, right?

What that means is that all of your major goals, survival and these other things, they're totally achievable, right?

Because it is completely plausible that you are going to make it for that half hour that you can project forward.

If you are a human and your goals are, I don't know, world peace or your buildings, the larger things, you are for the first time, perhaps along the Earth's lineage anyway, you are able to comprehend goals that you have no chance whatsoever of achieving because you have a limited lifespan.

So now that's a really sharp transition, kind of an emergent transition, because for the first time, you are able to undertake goals that you know you are not going to complete, guaranteed.

Whereas the fish essence, maybe you'll get eaten and maybe you won't, but it's totally plausible that you live for the half hour that you can see forward.

And so I suspect that this is, and this is sort of way above my pay grade to know details about this, but I suspect this drives a lot of the weird facts about human psychology because you now have this fundamental stressor.

that other creatures don't have because you're able to foresee all these goals that you are definitely not going to meet.

And that's a novel sort of conflict between the different modules that doesn't exist for most other creatures.


SPEAKER_04:
Yeah, that's really interesting.

Something that might be uniquely human is this possibility to see

past like the cone of our own existence like into the future with these like future possibilities or future goal states and also like backwards into the past in like a historical sense so we all like look at history um so i don't know i like thinking about like you know i've been reading your recent paper this integrating evolutionary developmental thinking into scale free biology this paper with chris fields

And I know like when we spoke yesterday, you mentioned you were integrating these ideas maybe into your next paper.

But in that paper, you guys were kind of blurring the boundary really between the processes of evolution and development.

And phylogenetic memory that's contained in DNA and results from evolution transforms into this ontogenetic memory.


SPEAKER_01:
Yeah, definitely.

So there's a couple of really interesting things related to evolution that we can talk about.

And that's something I'm working on now.

And this thing should be done in a couple of weeks.

We'll see.

Two things.

Number one.

One of the things, one of the amazing things about biological hardware that's given to us by evolution is that it is, there's a default way, by that I mean cells with their genomically determined proteins, whatever they have.

There's a default and a pretty robust, consistent default outcome, right?

And so embryo, you know, fish embryos make fish and frog embryos make frogs and so on.

but there's an incredible amount of plasticity there so that those exact same cells, genetically wild types, so the hardware is all the same, are in fact in other types of environments, are able to make completely novel creatures, right?

And then so some of this, so synthetic biology and synthetic morphology is already showing this, you know, as our xenobots and things like this, and there'll be tons more going forward.

One thing that's interesting about those novel creatures is that,

you don't have a long history of specific selection and frozen accidents and all this phylogenetics to lean on when you ask, why do they do certain things?

So for every normal animal in the biosphere, you say, why is this thing green and why does it fly and how come it has these antennae?

The answer is always the same.

Well, because for millions of years, the ancestors were selected for this and that.

The fact that we can take those embryos

those cells rather, cells from those kind of animals, and put them together into a new creature that has goals and behaviors, morphological goals, behavioral goals, physiological goals, that basically appear overnight, you know, within 24, 48 hours, they sort of

Show up, and then the, then the question is, where did these things come from?

Because what they didn't do was to be honed over millions of years by specific selection pressure.

So so that that's a very interesting to me to me.

aspect of the plasticity here that it is not, you know, the history doesn't necessarily determine them other than the default, you know, sort of variant and where does that come from?

And then there's the whole thing, which, you know, I don't know, you can tell me if you want to dive deeper into this or not, but the whole

the fact that you have multiple layers and that you have competency going down you know sort of all the way down has massive implications for why evolution works so fast right why why actually anything is able to evolve on the on the time scale that that we see so i think you know if you're interested we could get into that yeah that's great why don't we uh unpack that okay um yeah let's let's let's let's talk about that a little bit um imagine imagine two kinds of creatures and uh

you've got one kind of creature where the genome has a pretty direct influence over what the anatomy is going to be.

Everything is hardwired in some sense.

And so let's say, let's visualize, let's say it's a frog of some kind and there's the genome and it makes a frog and that's what it does.

And so now imagine evolution is sort of searching the space and now there's a mutation.

And the mutation does two things, because most mutations do multiple things.

Let's say we got a mutation that does two things.

It moves the eyes off kilter, right?

And it also has some really beneficial effects somewhere else.

You know, it helps with some other things.

Most mutations are pleiotropic, so it's a reasonable conjecture.

If your animal is the hardwired kind, the eyes are in the wrong place, nothing works,

evolution never gets to explore the positive benefits of this other mutation because the thing's going to be dead.

It's just not a workable animal anymore.

The fitness is very low.

It's gone.

And so most, if you think about these kind of hardwired things,

It's the same when I first, you know, 100 years ago when I first heard about how evolutionary algorithms are supposed to work, this idea that I've got a system that does something, I'm going to start throwing random changes in.

Anybody that's written any code, are you crazy?

Of course everything's going to get worse, not better.

That's never going to work.

Imagine a different... Now, of course, it works, but it takes massive amounts of time, and there's all these difficulties about evolving complex things.

Now imagine what we actually have.

The actual... And I'll explain the biological example.

You have a tadpole, and the tadpole has to move all of its organs, craniofacial organs around to become a frog.

So the jaws have to come out, the nostrils move, the eyes move forward, like everything, all this stuff moves around.

And you could have a hardwired animal in which every...

Every organ moves in the right direction, a particular distance, and that's it, right?

And that's your hardwired thing.

That actually is not how animals work.

So if we, and so what we did a few years ago is we made what we call these Picasso tadpoles.

The idea is you make these tadpoles, everything's in the wrong place.

The eyes are on the back of the head, the mouth is off to the side, the nostrils are up here, like it just scrams, like Mr. Potato, everything's scrambling.

So if it was a hardwired system, then all of these things would move the traditional amount in the traditional direction and everything would be way off because you're starting in the wrong spot.

Instead what happens is that all of these organs move in novel paths and they keep moving and sometimes they overshoot and have to come back, but they keep moving until they build a correct frog face and then they stop moving.

So now that system, so what the genetics actually gives you, it doesn't somehow give you a hardwired set of movements, what it does do

is give you an error minimization scheme that can progressively deform the system until the error from some sort of set point, and we've been studying what the set point is, we can talk about that, until the error from that set point gets to an acceptably low level.

So what you have here is a competent set of craniofacial organs that are going to move around and get to where they need to go, even if they start off in the wrong position.

So now look at what happens with evolution, right?

You got your mutation.

The eyes and the jaws now are off in the wrong place.

But now you've got this benefit somewhere else.

The thing is, the eyes and the jaws are going to get fixed.

You don't need to worry about that because they're going to they know where they belong.

They're going to get there even though they start out in the wrong position.

So that negative aspect of that mutation gets hidden from selection, for a while anyway, because eventually it'll catalyze into

Into the genome itself, but for a while, it gets hidden from selection, allowing evolution to explore the benefits of the other benefits of that mutation.

So, so here's all in summary.

Here's, here's what this means.

The fact that you have competent some units and they're competent morphologically, they're competent physiologically.

We can talk about what that means.

The fact that they're all competent means that.

the fitness landscape is much less rugged.

It's much smoother.

It means that the effects of mutations are much more linear, meaning that you can examine each consequence independently

because the other consequences can be masked by the local environment being off, it doesn't matter.

Things still get where they need to go and connect and so on.

So it makes the search process way more easier by

by allowing you to, and it gives you patience.

It means that evolution doesn't have to solve every problem at once.

You don't have to wait until you find a mutation that gives you a positive impact and leaves the eyes where they need to be.

You don't need to wait for that.

You can grab the first mutation you find that gives you the positive impact because the eye thing will get taken care of because those subunits are confident.

So it raises the IQ of the whole system.

The search becomes a much

A much smarter search, it has some patients, it's able to see linearly instead of sort of highly mixed up all of the consequences of these mutations, and it smooths the fitness landscape.

So I think when you can count on your components to do some of the heavy lifting, you don't have to micromanage all of it.

the evolutionary search gets massively more efficient.

And now it starts to become a little more plausible that we actually see this, you know, this amazing biosphere actually evolved by a process of random mutation.


SPEAKER_04:
Nice.

So in that same paper, you talk about random mutation, but then you also describe this kind of modularity of gene groups and the possibility of entire gene families

like the Hox cluster duplicating and driving the major evolutionary transitions.

Is this a sort of non-random process, sort of ergodicity breaking with perhaps like non-linear effects when these entire clusters?


SPEAKER_01:
Yeah, I hesitate to say too much because I don't think much is known there that I can say definitively.

I think that this whole issue of random and non-random mutations is very controversial, obviously.

I suspect that there is a notion of

of evolutionary search that's not quite blind.

So people often will argue, you know, either they, most people nowadays say evolution is completely blind.

You know, it just, it makes these mutations, whatever happens is not because of any directionality in the search space.

And then, you know, you've had people who think that it's directed towards a specific outcome.

I think there's a sort of in-between scenario where it's definitely not directed in the sense that

can see far forward in the search space but it doesn't have to be completely blind in that maybe what it has and this this comes out of some discussions i've had with both chris fields and richard watson that that maybe instead of a point in fitness space what you really have is a little bit of a vector you have almost like a comet trail right and then it gives you a little bit of um

a little bit of information about direction and about, you know, and you can imagine, I mean, I can sort of design in my head an epigenetic system of marking things on the DNA that give you a little bit of information.

Well, what was this allele last time?

So that you can sort of say, well, I've been turning this knob in this direction and things are going well, so maybe I want to turn it some more.

That kind of thing.

And this is totally made up.

I'm not saying that we know that biology actually does this.

Maybe it doesn't, and maybe synthetic biology, maybe we implement this for the first time, we engineer it.

But I think it's possible to make a system that does that.

And maybe collections of genes and some of the complex chromatin interactions

Epigenetics that go on, maybe they're part of that process, but this is this is total conjecture at this point.


SPEAKER_04:
So, Sarah, I may have a question, but I'm going to use my moderators privilege here and just ask, do you think that this vector driven mutation could be perhaps driven by active inference?

Some kind of vector between exploration and exploitation happening.


SPEAKER_01:
Yeah, and so one of the things that we have talked about and that I'm going to write after this next thing is really on this idea that maybe the whole evolutionary lineage of a particular species

the animal or plant or whatever it is, maybe that's an agent as well, just stretched out massively over space such that each at any given point in time, all of the existing examples of that species are in fact hypotheses about the space, right?

And that what you're doing is you're constantly generating genomes

as hypotheses.

And some of those genomes are proven to be some of those hypotheses are proven good and some are not.

And then the next sort of the next set of genomes is not completely random, but it's in some way part of this part of the active inference of this giant sort of virtual age.

Right.

So I think there's a story to be told like that, but it's way more hand waving than details at this point.

Awesome.


SPEAKER_04:
Thanks, sir.


SPEAKER_03:
Yeah, my comment is now maybe a little bit stale, but I think I'll put it out there anyway, just for the sake of saying something concrete, because a lot of this feels a bit abstract.

And I'm hoping I'm even remembering this anecdote right.

But I remember learning that some gene, once upon a time, that gave us an extra rod or cone in our eye, allowed us better color reception, also was the same gene that shut off our ability to make our own vitamin C.

And if that's correct, if I'm remembering that correct, that's such an interesting example of.

Yeah, of course, it's an example of the of the fact that genes do multiple things, but but it was kind of like, isn't that coincidental?

You know that that this.

that that gene is doing, you know, both something for the eye, but then also making it so that you can get your own vitamin C, you know, by looking at fruit.

And I just thought that was, that kept coming to me as an example of what you guys are talking about.

But it's stale, sort of.


SPEAKER_04:
Thanks, Sarah.

And sorry for making your comments stale.

Stephen?


SPEAKER_00:
yeah sort of following on it's this you have this idea of like um a goal space or a teleological space in a way that's really interesting and then with that exploit or explore you i wonder if there's a time when there's there's there's um a thought about when things are very directed and of course our western thinking we we tend to think about a nice pointed cone

um where we go into the future we because we sculpt the world around us we maybe have like a distinctive trajectory because we're kind of we're constructing our world more and more which is probably part of our problem to make it the way we want it to be um whereas particularly if you look at indigenous cultures they would have you know a cyclical the more cyclical approach which would go through seven generations or different levels of generations and would be a tune-in

the environment and i'm kind of curious whether and again this might be where we don't really know but is there a point at which there's a generally trying to come back uh or maintain in the allostasis the kind of the cyclical sort of pattern that is where we want to go to and is there a time when it's like okay we're we're branching out now like we've made a jump and we're going to try and

inscribe our environment at different scales from the body out to sort of now maintain some new kind of steady state attractor and how that might fit with this.


SPEAKER_01:
yeah i mean i think i i think you're absolutely right and that some future version of this is going to have to uh be able to deal with goals that are not only not static meaning they're cyclical but also um goals that are themselves you know let's say second order goals like like my goal is is it's a derivative my goal is to get better at something it's not a particular level that i'm looking for but i want a particular slope of

some variable being you know a particular right so something like that so so you're absolutely right i i started out with the simplest you know this is sort of the hydrogen atom of this thing so i started out with something very simple that it's homeostatic as in your your goal is a is to achieve a particular region in whatever space you're solving problems in

be that morphous space or physiological space your goal is to find that region and sit there and and that of course is not all there is to life obviously that's just the first you know kind of step i just did that for simplicity because i want to tackle everything at once and and there needs to be a lot done here before we get to that point um but yeah you're absolutely right your your goal state in that space may not be a point it may be some interesting you know bigger bigger bigger shape you know and this thing and and i just want to i'm going to say something about

These goal spaces, because I think it's, it's, it's interesting to point this out.

So, um.

1 of the in this in this, this all came out of a meeting on diverse intelligences and so it was the 1 thing I've been thinking about is what.

What is intelligence and what are these agents actually trying to do?

And.

Okay, so so we have lots of examples of.

problem-solving in three-dimensional space, right?

So you've got crows and monkeys and everything else, and they're running around and trying to solve problems by moving around in three-dimensional space.

But all the different subunits of bodies are solving problems in other kinds of spaces.

So we talked about morphospace.

So if you are an eye in the head of a frog, you are working in a space of possible configurations of where you are relative to everything else.

and you are trying to move and act in a way that is going to acquire, to a particular region of morphous space that describes the correct frog head, that's where you're trying to land.

There's also a physiological space, so I'll just give you a simple example that we found recently.

So these flatworms, these planaria,

you you take these planaria and you throw them in barium okay so a solution of barium now uh what happens is the barium is a non-selective potassium channel blocker it it blocks all the potassium channels all the cells freak out their head their heads literally explode right over overnight you know their heads just degenerate they're gone if you take those planaria and you leave them in fresh barium over the next you know 10 20 days what they will do is they will build new heads

that are completely barium insensitive.

Couldn't care less.

No problem at all.

They live in barium, which is fine.

So we asked the simple question, and not because the answer has to be this, but just because that's the tool that was available.

We asked the simple question, what is transcriptionally different about barium-adapted heads from normal heads?

So what genes are differently expressed in these heads?

And so what you find out is that out of tens of thousands of genes that these planaria have,

they managed to regulate a very small number on the order of a couple dozen that allow them now to do their business despite the fact that potassium is no longer usable for them.

So now this is incredible because think about the problem that they're solving.

In my head, I always visualize like one of those nuclear reactor control rooms with buttons everywhere.

So each button is a different gene.

What are you going to do?

You have a physiological stressor.

You can't pass potassium.

Everything's going to happen.

you have some tens of thousands of genes that you could turn on and off.

The combinatorics are astronomical.

You don't have time to try them all.

You don't even have time for gradient descent.

You don't reproduce fast enough.

It's not like bacteria where you could say, well, they're all random, and then somebody will survive and repopulate ahead.

They don't divide that fast.

So you don't have time for any kind of evolutionary explanation.

Within some small number of days, you have to hone in on a small number of genes

knowing that if you just start randomly flipping them up and down, you're going to make things worse long before you make them better.

You're going to kill everything off.

You just randomly start.

So now the other important fact about this is that planarian never see barium in the wild.

So planarian, there's no evolutionary history of being good at surviving in barium because they don't see barium.

So now to me, this is an amazing example of problem solving in a high dimensional virtual space.

You've got this, you've got this, um,

physiological space and your problem is, your problem is physiological, your effectors are transcriptional largely, although of course you have physiological effectors too, but you're trying to walk in this space and you have to somehow map, it's unbelievable how they do this, you somehow have to map your effectors in transcriptional space

to what's happening to you in physiological space and then rapidly get to a workable region.

So that gives you an idea of what we're talking about, right?

These spaces can be transcriptional, physiological, all kinds.

And these things, and they solve their problems.

And like you pointed out, the correct, the thing you're trying to achieve may not be a point, it's certainly not a point in that space, but it may not even be a static area.

It may be some sort of, I would like to sort of wander in this, you know, kind of weird loop.

That's my goal, right?

Is to, you know, is to wander in some sort of weirdly shaped attractor.

Yeah, we'll get to that.

This is the just, you know, first step.


SPEAKER_00:
Just one question following up from that, just before Sarah.

So if I'm right, what you're saying is gradient descent, because that would have to come from where you are and slowly keep incrementing, there's somehow some goal states.

I mean, maybe there's some evolutionary history, even if it's not barium, something like barium, but somehow it's not only going from...

adjacent possibilities, it's somehow jumping ahead.

Is that what you're saying?


SPEAKER_01:
So the first thing I'm saying is that I don't have any idea how it works, right?

So I am not claiming I have a good model for what's going on here.

But I think you're onto something, which is that I think, and I think we talked about this a few minutes ago, which was abstraction and generalization.

I think you're exactly right.

I think what the system is able to do is to say,

Okay, I don't know what this barium is, but I've been terribly depolarized before.

It was an epileptic, you know, kind of a thing.

And this is an awful lot like that.

So I'm going to generalize what just happened to this other thing that I do know how to deal with, right?

And I'm going to move in that direction.

So that's my gut feeling.

I don't have a solid model for this.

That's not been proven in any way.

But I have a feeling that that's where the answer is going to be.

It's going to be that it's able to do this because it is able to recognize this physiological problem as an instance of a larger class of things for which it does know how to do.

And I think that when we talk about intelligence for all of these agents, what we're really saying is it's your ability to navigate these various spaces efficiently and without getting trapped in local minima that kill you.

Right.

Being being able to step temporarily.

You know, it's that patience.

It's that it's that being able to step away from the direct line to your goal to then come around and.

And again, I have, sometimes when I talk about this, I have a picture of this, there's a fence, and there's a couple of dogs on either side of the fence, and they're just like, you know, trying to get at each other, right?

There's a hole in the fence four feet away.

So they could do it.

The problem is it temporarily requires you to get away from, get further from your goal, right?

So that ability, you're trapped in this local maximum, and your intelligence is directly sort of proportional to your ability to temporarily

get further from their goal in order to actually get into a better position later on as part of the IQ.

But I think the other part of the IQ is this kind of generalization, right?

And I think you're right.

I think that's probably what this is going to end up explaining.


SPEAKER_04:
Nice.

Sarah?


SPEAKER_03:
Man, this is great.

This question is probably out of bounds, but if you have any thoughts on it, then great, if not, pass.

I mean, the cone-type diagram and the

the agential teleos that it assumes, I'm just wondering if you feel like there's any applicability to non-agential type phenomena, like complex systems or hurricanes or whatever, because when we're talking about abstraction, yeah, I'm just wondering if you have any thoughts about where that could go, if anywhere.


SPEAKER_01:
Yeah, so the thing I'll say about that, and this is the kind of thing that probably not a lot of people will agree with this, but this is my view.

To say non-agential presupposes that you can draw a binary sharp line between things that are agents and things that are not.

i don't really believe in that i think that's a view that gets us into a lot of trouble by looking at these looking for these sharp lines i think that all there is is a degree of agency and i'm not sure we are we talked about yesterday whether there's such a thing as zero i i don't actually i'm not even sure there's a zero but certainly there are there are things that are very

very sort of non, you know, very, very low levels of agency and, and things that are very high and everything is a smooth continuum.

So now then, so, so then, then, then the question is, okay, so what do you do with hurricanes and things like this?

So I drew, and this will be this, you know, in the next, in the next paper, there's a thing, there's a thing that I call, it's an axis of persuadability.

And it's, it's, it's sort of related to Dennett's intentional scenes in the following way.

There's a continuum, and for any given system, so my claim is that it's an empirical problem.

You can't say whether something is or is not an agent.

Lots of people do.

They'll say, well, that's not an agent.

I think you can't say that until you've done the empirical work, and the empirical work looks like this.

The question is this.

As an engineer, this is very much an engineering perspective, as an engineer, how much work do you have to do

What do you need to know about the system and what kind of work do you have to do to be able to control and predict its outcome?

So I'll just give you a couple of examples.

All the way on the left, you have things that are like mechanical clocks.

So if you're dealing with a system that's like a mechanical clock,

you know you can put it on the left of this diagram because you are not going to reason with it, you are not going to train it, you're not going to be able to punish it, you're not going to be able to rewrite its goal states.

You have to know how it works, and you have to micromanage the hardware, and you have to rewire the physics of it to get it to do something else.

Then moving on from that, you have something like a thermostat.

And with a thermostat, you're also not going to reason with it or punish it, but there is a separate set point.

And if you want the room to be kept in a different temperature range, you don't have to rewire the thermostat.

You can change the set point.

So as in, this has massive implications for regenerative medicine that we can talk about.

The way you relate to the system is quite a bit different.

Now, after that, you might get to systems that have

preferences in the sense that you can actually train them, meaning you can provide rewards and punishments.

They will change their behavior because what you're doing is you're relying on them to be a kind of learning system.

They're able to do associative learning.

So they associate certain things you've done to them with things that they did and they will change their behavior.

So now this means that we can now think of what this means.

You know, humanity has been

training animals for, I don't know, 10,000 years, whatever, with knowing nothing about neuroscience.

It means that you don't actually have to know what's going on inside your system.

You don't need that micro-level hardware information.

All you need is to know that that system has that level of agency that it's

susceptible to this kind of interaction.

And how do you know?

You don't know until you've tried.

So we have no idea.

So one of the things we're doing now with Charles Abramson, who's a

behaviorist scientists, we're actually writing a how-to manual, a how-to paper on if you are given some sort of weird new synthetic system, how do you know where along the, and it actually has a bunch of experiments that you do to figure out where you are along this path.

So you can go further, and there are other systems where you know nothing about

What's inside and in fact, the amount of energy that it takes to manipulate them as tiny, because you can, you can give them reasons as opposed to causes and you can, you can give them information and they will act on this information and do various things.

Of course, sophisticated patients.

So, all that is to say.

the question of where something like a hurricane lands on the spectrum, I think we need to really resist the urge to just make pronouncements, sort of armchair pronouncements about that.

And we need to ask a simple question, which type of manipulation is that system amenable to?

And when you find out, then you know how much agency it has.

Now, I'll give you a simple example of how this works.

And other people have done something similar.

If you think about gene regulatory networks, so gene regulatory networks are a quintessential example of a dynamical system.

It's deterministic and all that.

And one of the things we asked is we simply said, how do we know where on this spectrum it lands?

Let's just try to train it and see what happens.

And it turns out, and this is, so we did this computationally, and now we're sort of starting to do this at the bench.

We simply ask the question, could gene regulatory networks have associative learning capacity?

And that means that you could do interesting things.

You can do Pavlov-Dogg's experiment.

So you have a gene regulatory network.

You have some stimulus that causes it to do something.

So you have your unconditioned stimulus that causes a response.

You have this other thing that's a neutral stimulus that normally doesn't do anything.

you pair presentations of the UCS and the neutral stimulus together.

And after a while, what you might find is that now take away the original unconditioned stimulus and use the neutral stimulus.

And that's now enough to trigger the response because there's an associative memory that formed

in the network right so now we actually tried this in the sense that we we took a bunch of um known gene regulatory networks from literature we made an algorithm that basically checks for associative learning and lo and behold we found two things we found that there are six different kinds of memories that these things can do including associative memories for many of them

And that the biological networks have way more memories than you find in random networks that you would just randomly concoct.

So apparently, either directly or indirectly, selection likes that.

So that's just an example of what you would do under this view with dynamical systems, right?

That you would think, oh, you know, these are trouble cases.

Are they agency or aren't they?

I think that's where we get in trouble because we try to draw a sharp line.

I think the answer is they're somewhere on the line.

They could be way over on the left or not.

And you find out when you figure out what kind of intervention and strategy avails you of the most predictive power with the least amount of effort.


SPEAKER_04:
Nice.

So we talked yesterday kind of about, you know, cells that are contiguous and

There's like connections.

And so this concept of like a shared information making up an individual or like what constitutes the boundary.

You know, if cells are connected via gap junctions, and like you talked about how if there's a calcium molecule inside my cell, I don't know if it's from me or from my neighbor because we share this kind of hole between us.

And so I just wonder if there's some degree of information sharing or some kind of like proximal level, does it have to be immediately contiguous to have that same

like cognitive capacity or or like over a longer distance can this also function like for example if i get some kind of like necrotizing flesh in my leg is my toe necessarily going to know like what's going on with my leg it's going to take a while

for that signal to diffuse over such a long distance.

So where is the boundary or container space for this kind of information sharing that has to occur to make this cognitive individual a whole thing by itself?

So I don't know if you can say a little bit more about that.


SPEAKER_01:
Yeah, yeah.

So I think one thing that I really need to improve in this whole thing for next time is a better visualization, and I frankly don't yet know how to do it because

This way of visualizing it looks very physical.

It looks like what the cone is, that the cone is at the edge of the cells that are connected, right?

It sort of gives this idea that what you're measuring is the actual physical boundary of the system.

And that's not quite what I'm trying to get across because what, and this is actually, let's just think about the cells in the cancer versus multicellularity, right?

Which is a nice example of how this works.

When you have a bunch of cells and they get together and they're going to build a kidney, the size of the, at least the spatial size of that particular agent is going to be roughly the size of the thing they're working towards.

It's going to be the size of that kidney, right?

That's the project they're all working on.

Each individual cell has no idea what a kidney is, but the collective intelligence of that group of cells, it has a

a goal that's trying to build that kidney.

If you try to deviate it from that goal, it will do its best to get back and do it anyway.

So what you would be measuring there is the size of the state that it's trying to achieve, not necessarily where the edges

Now, there's certainly some relationship there, because in this particular example, as you said, what enables the cells to work together is the fact that they're electrically coupled into one and chemically coupled into one giant network.

So there's some relationship, but it's not quite always the edge, right?

You know what I mean?

And the same thing with the individual cells.

It's related, but it's not exactly the same thing.

When one cell defects, so let's say an oncogene turns on, the cell closes all the gap junctions, disconnects from its neighbors, and now that computational boundary is literally just that one cell.

Again, it's not quite at the edge of the cell because if you look at the scale of things the cell is trying to manage, it's a little bit bigger than that because it also really cares about some of the

you know, the nutrients, the wastes, the conspecifics, the prey, whatever else is going on right at that edge, right?

So it's not quite at the cell boundary.

So I just want to make that clear that this diagram isn't meant to be an anatomical, the cone isn't meant to be a map exactly on the anatomy.

It's a cone of the states that the thing is trying to actively manage.


SPEAKER_04:
Yeah, I think that the cones were inflicted by us, though.


SPEAKER_01:
Yeah.

But you're absolutely right.

And as soon as you did that, I realized that what, because that has to be right, that those two diagrams eventually have to meet, right?

And so there has to be, there has to be a way to, so I think, I don't know, my visual imagination isn't great.

I have to think a little more about a better way to kind of represent it.

But of course, you're right, those two things have to come together.


SPEAKER_04:
Yes, Stephen?


SPEAKER_00:
Yeah, I was just sort of following on from what you were saying is,

often as well as useful when we think of entities and processes, I'm maybe scaling up to more psychological scales, but you know, we often think about this is this person, this is what this entity is diagnosed as, and this entity, and it's all kind of, and there's a lot more now moved towards what's the process, but then what kind of processes do we have on that continuum?

Just like we're talking about with the hurricane, you know, you maybe got a static entity, like a rock or,

And then you've got your kind of systems, some of which could be mechanical, like a clock.

But then you sort of hit structures, there's some sort of structure, like a, I don't know, like a skeleton or a network of processes coming together.

Then you get into them, which is what I find really interesting with the work you do is you the sort of nonlinear swarming dynamics, you know, and it's sort of, and I was wondering how,

When you think of, you know, you've got a body and I'm a body and I'm trying to work in my space around me.

So I see things in my environment out there and I can interact to some extent, depending on what my available processes afford me in my structures.

And then there's other stuff inside, which to some extent, I kind of have to let take care of themselves unless I want to tattoo myself or do some sort of

internal mutilation or something.

So I'm curious about how this entity process dynamic going in and out can be thought of.


SPEAKER_01:
Yeah.

Well, I want to say a couple of things.

And again, this is sort of going kind of far stream, but I think it might be relevant.

When you say you are this body,

sort of in the sense that we are talking to the verbal entity there, but there are all sorts of other entities there that we're not hearing from that.

And so, so we know that from split brain patients, right?

We know that after a commissurotomy, the right side of the brain, you can communicate with, and it'll give you opinions that the verbal patient, part of the patient doesn't agree with, doesn't know anything about, right?

So really interesting.

I also think about multiple personality, you know, they call it multiple personality disorder, but the reality is, you know, a single brain body system apparently can host any number of distinct behaviors.

and how in how i am i'm not i'm not an expert in this area so i don't know how psychologically deep so some of these other personalities go but enough that you can have a conversation certainly enough that you can have a conversation with them which means they're doing better than all of the ai that we've ever produced right and and they uh and in fact i i read once i forget where it was but there was an interesting comment from a

a therapist who was asked to he was he was he was asked to deal with a multiple personality patient and he was talking about integration and he was you know that the the goal of the therapy was going to integrate because the person's very disruptive for the for the individual and we're going to we're going to integrate all of these into one and one of the personalities said to that patient

integration, you're going to kill me, right?

That's what you're talking about.

I thought you were a doctor.

What are we talking about here?

And so, you know, it was outraged that there was going to be this integration of, you know, of what, at least ostensibly, you've got this entity that certainly passes the Turing test and is not happy about the fact that you're about to integrate it in a way, basically, right?

So,

I think that goes back to this idea that we're an interpenetrating set of selves, and we tend to only hear from the verbal one.

So there's all kinds of other ones in there that are competing and cooperating and everything else.

And the other thing I can say about this, so that's more questions than answers, I guess, but the other thing I think that's interesting is that

You're right in the sense that the one kind of entity that we have direct experience with, which is the verbal sort of one that presumably we share, is completely ignorant of most of the things that are going on there.

And so the space that you're working with nicely coarse grains over all kinds of stuff, your liver function and everything else, you don't need to micromanage all this.

However, there are techniques that will add

axes to your option space so if you would like to learn to uh also consciously control your heart rate or your you know skin temperature or whatever you can do biofeedback there are tools that will enable you to cross that that that boundary and you will now have a new axis in your option space that you never did had before do you want it i don't know there's a good reason why most of these you don't you don't use it's a good to sort of simplify your space

But there was a cool experiment where they measured the temperature between a rat's ears, and they gave it a reward proportional to the difference in temperature.

And so rats readily learned to get their ears to, I think, like five degrees Celsius or something different between their ears to get the reward, right?

That's amazing for a number of reasons.

One is that now you've added an effector in your space that is kind of this physiological blood flow, I don't know how they do it, that you didn't have before.

The other thing, of course, is the credit assignment.

Here you are, you're a rat, and you just got some reward.

And so, well, let me see, why did I get this?

So while my tail is pointing up and my left fingers are clenched and my whiskers are going, which of those things actually were...

this is, we still haven't cracked this in machine learning, is how living things are so good at figuring out what exactly they're being rewarded for, for a small number of trials, actually.

So, but you can, I think, I think you can break through, at least to some extent, and, you know, break through, and actually even further, right?

So there are claims that certain types of, you know, mental practices will have effects on your immune system, for example, right?

So you can, I think you can go pretty far down, right, from

if you wanted to add these kinds of things to your options space.


SPEAKER_00:
Ah, interesting.

Actually, just bouncing on that, going, that's really helpful, actually.

I've not even thought about that.

But that different parts of us ties in with, there's this big push all the time, if you say, about integrate, integrate, integrate.

And actually, when you scale out, because I do a lot of work with multi-service service providers, say, in Africa or other countries where they're, and you tend to find that

any type of transdisciplinary project has the problem about whichever regime of attention or field of practice the voice is in at that time.

And then when we talk about integrating, it normally means they dominate.

What about configuring?

So think about this idea of configuring different fields of practice, i.e.

different ways or different

ways of knowing and also ways the other voices is strategic conversations with communities who don't normally get a voice who know something in the state space beyond what the the actors who've got the job in the agency know and it's this sort of ties in a little bit with what you're talking about here so i think it's quite interesting in the sense of can we con because in a way the biology isn't trying to integrate

is actually configuring and allowing things to be but it has a way to still know their different teleological spaces and either just let it get on with it or yeah knows where and when to get involved i suppose is is the thing yeah i mean so so so i think i think the the


SPEAKER_01:
One way of looking at that is to ask, what are the optimal policies for this binding?

So I'll give you an example.

So in the case of the multicellularity, we say, OK,

Individual cells are pretty cool, but when they connect via gap junctions into a multicellular collective, they can do these marvelous things and they resist cancer and turning off the connection and becoming a cancer cell and going metastatic is bad.

And so what you would really like to do is to reconnect these cells back in.

So the thing is that, right,

tightly coupling individual units into a global entity avails you of all kinds of interesting things, right?

So it raises the IQ of the whole and gives you all kinds of capabilities, fine.

But one of the things that gets lost when you do that is that the larger system now can dominate the goals of the lower system.

And in fact, the larger system may have

very little uh care for the goals of the welfare of the subunit you know when was the last time you worried about all the skin cells you're shedding right you just you know you're going to do what you're going to do and if you lose some skin cells you know no no no problem um so what that suggests is that block you know maximum maximizing out this connectivity into some giant board like uh you know thing is is and and obviously it's been you know it's kind of been attempted on you know on the on the social scale we know how that turns out so

you want to, somewhere in between there, there's an optimal policy, right?

Where I hope that there may not, you know, there may be maybe there's some no-go theorem somewhere that says there is no good policy, but I hope that somewhere in there, there's a good policy that allows us to reap the rewards of appropriate kinds of large-scale organization without a sort of

blanket collectivism that completely sort of, you know, wipes out the needs of the lower level subunits.

So that, I hope, is an empirical question that we should be looking for what kind of policies give us, that optimize the various things we want to optimize.

Josh Vanguard and I planned a grant at one point for this human flood, this human flourishing program somewhere, and I think it's actually an empirical question.

Can we identify the optimal policies that preserve the needs of the subunits and reap the rewards of becoming a greater whole?


SPEAKER_04:
Nice.

So I learned this word yesterday, egregore.

It's the occult concept representing a distinct non-physical entity that arises from a collective group of people.

And I thought it was really cool.

And I think of this concept of emergence.

So like at what level does this collective kind of body form?

And I don't know if you've read the information theory of individuality paper, uh, by David Krakauer, but we really think about, you know, emergence, you know, like the emergence causing the formation of the collective entity, but then there also has to be like some kind of downward causation at some, some point in there.

Like, so is there, is this some kind of like critical point, like a phase transition that starts to occur or when does that kind of start to happen?

That, that bi-directional information flow.


SPEAKER_01:
Yeah.

For sure.

I mean, this in particular is interesting because I thought about this a little while ago that, you know, especially people who are interested in this kind of stuff, you know, and you can think about the really old concept of group karma, right?

This idea that not only you have your own individual cause and effect that you're causing, but in a strong sense, the group of which you are a part has its own

right, cause and effect that is as real as, so that idea, and I don't know how many thousands of years old that idea is, I think it is an amazingly prescient because in Western science, the science of collective intelligence and this idea that, right, the group can be a swarm of whatever can be an agent on its own is relatively recent, right, that people have started taking that seriously in science.

but i think i think that kind of thinking must have been extremely old because there are these traditions that take it very seriously that the group is not just a heap of stuff but it actually has its own agency in the sense that it can even be the subject of blame and reward in the sense of you know the sense of like rewarded punishment or karma or whatever that that's a very strong view of group agency that i think we've only said you know scientifically we've only um really appreciated recently

I think that's pretty cool actually.


SPEAKER_03:
Random linguistic thing here that egregore, I was thinking about egregious and egregious in my mind is like out of bounds, but yet egregore is not that.

It's the opposite.

It's almost like a group thing.

Maybe somebody else can think about that differently, but I thought that was an interesting conflict.

Yeah, very interesting.


SPEAKER_04:
So I think about this concept of collective karma a lot and really kind of whether or not like I have free will in the in the present, like in terms of am I just some kind of computation like my like, you know, just because I've been doing active inference and so

Am I just my generative model that effectively I can't make any decision because of my model?

Whatever input I'm getting at that moment, I have to decide based on the condition of my model at that time.

I think about collective karma much in the same way and whether or not there exists some agency in the present or not.

If all the things that just made me who I am right now,

have just led ultimately to be where I am.

And in collective karma too, it's like a hurricane.

That group of people caused that hurricane to happen.

Like they created the causes for the collective karma for the hurricane to come.

So I don't know.

It's something that is really intriguing to me.

And, you know, I don't know if you want to maybe elaborate a little bit more on that.


SPEAKER_01:
yeah yeah and and and so and so two two thoughts about that uh one one is that this goes back to this idea of of spaces and the different spaces that each of your um each of the subunits is working in one way that and and after after i drew this cone and i realized that it kind of looked like the the special relativity cone and all that uh some other some other pieces of relativity sort of started started popping into my head and this idea of

deforming the space around you, right?

So if you're a mass, you deform the space around you, which in turn alters your next possible movements and the possible movements of the other masses around you, right?

So by deforming that by your actions, you are deforming the possibility space for yourself, for things next to you, for your components, and so on.

And so one thing that I think happens is that these larger systems

deform the action space for their lower systems so that all they have to do is roll downhill in effect.

And so if you zoom in, so this gets back, and I'm going to connect this back to your point about karma and action and choices and so on.

When you zoom in, all you see is physics, and you say, oh, well, of course that's what it's doing.

That's all it could do.

All the gradients are pointing this way.

That's what it had to do.

But the reason it can do that and still get to some global goal is because the higher-level system has already deformed that space.

So I see this, and I don't know how I'm going to ever draw this.

That's a different story.

But I see all these different levels as deforming their simpler space that they're working in

which then changes the option space for their subunits so that they don't have to be as smart.

This is sort of Dennett's idea of progressively dumber robots all the way down.

That works if each system

makes it so that so that the the lower level systems don't have to do quite as much of the work and there's a stupid example i always go to from from you know from like common everyday life if if if you ever had a friend who tried to stop smoking and they like to smoke at night one of the things they might do is put their car keys somewhere that's really hard to get to why because they know that when they you know at midnight when they want to smoke

they're not going to feel like going hunting for the keys and then they could find, and then that's that.

What they're doing is, who are you doing this to?

You're doing this to your future self.

You're deforming the space for your future self, in that case, consciously.

Of course, we all do things unconsciously too.

But in that case, you're consciously deforming the action space

for your future self in order to uh so that you can be on autopilot at that point right you don't have to have the willpower at midnight that i'm gonna you know not do this it's you've already somebody's already sort of twisted the space for you so that you're just going to get your keys it's just too high of a barrier and forget it so uh right so so i think i think that that that that deformation of these action spaces is a really important way in which um lower and higher level systems relate

The lower level systems simplify the action space for the higher level system because they're competent.

So the higher level system doesn't need all the dimensions.

And the higher system bends the space of the lower system to make things easier to get to.

And that goes back to your other point that you just made about

in the moment of taking actions in the moment that are the consequence of all the pressures that you have set up and other things have set up for you, where, yeah, I see this as, so if we think about will in the sense of, you know, free will or decision making or whatever, I see this, again, this like, four dimensional block where each one of us at any point in time is a slice of it.

And

If you zoom in on that one, that selflet that exists at that one time slice, your free will, so to speak, is limited to none because at that moment on the local scale, you are going to do whatever the local forces have arranged that you're going to do, right?

And this is one, like a simple example is, can you control what your next thought is going to be?

You really can't.

Your next thought is going to be whatever it is.

So at the local level, when you zoom in,

there's no useful sense of freedom there.

However, if you zoom out on the long scale, what is true, I think, is that you can take repeated actions, whether they be practice or whatever they're going to be, you can take repeated actions that alter your cognitive causal structure so that over time,

you have made changes.

And so no, you can't control the next thought you're going to have now, but you can control what percentage of your thoughts are negative or whatever it is,

down the line, right?

So to me, that question of choice and freedom is very much relative to what scale you're asking about.

If you zoom in to the low, you know, to the physics and the sort of short time scale, no, I don't think there's any useful sense of that.

But over the long term, absolutely, because you can make changes both in your environment, your own structure, that will get you to a different place.


SPEAKER_04:
Thanks.

Stephen?


SPEAKER_00:
Yeah, this is really helpful, actually, to hear it thought about this way.

And just as we were saying there about this sort of almost transcendental or the way we often think of things being excluded, egregious, as Sarah was saying, we need to move beyond

The inner outer, you know, there's an inner thought process and there's an outer world, but the two are linked.

Like we say, so with active inference, the idea is that I don't necessarily purely have a model of those cones in my head.

I buy, I've got an action model for how to draw them.

I haven't got the whole thing that can be dumped down like a data source, but I have a way to do that, which sort of ties in.

And when you mentioned there about looking at the future,

One thing that adds that Active Influence hasn't really done is it talks about, say, the car keys example is the sequence at which something might have to happen to get to the future being disrupted, which is kind of that quantum contextuality idea, whereas normally in Active Influence, they talk about the niche

construction or the niche modification, but not just about modifying itself is modifying the trajectories you might take through that niche, which that, that gives you more near time possibilities for like sculpting what can happen when it can happen.

So that, I think that's quite useful.

I was wondering how,

how you see that relationship to niche construction and models being sort of embedded in the world.

So if systems are really models in the world that we then read or interact with using our structures, how do we link up that kind of cone with the kind of niche construction that's maybe being used in other areas of active inference?


SPEAKER_01:
Yeah, I mean, so that's that's a very difficult question.

I can sort of address a small part of it.

I think at this point and I can, you know, I think I think one of the key.

questions of all of this is to what extent what when we look at systems and we see the dimensions with let's just do the third person case first and then we'll talk about that I think the first person case that you're talking about so when we look at a system and we make a map of its its problem solving in some space and its goal directed activity in some space the question is uh is that in any sense objective in the meaning meaning could we um

Could we make a case that that model of that space is better or the best?

Or is there an infinity of ways to look at it?

And in particular, how does that relate to now the first person perspective of that system itself?

what does the space look like?

It's a little bit like, you know, the whole Umbel thing, right, where you're sort of in some space.

What does it look like to you?

And Chris Fields made a really nice example in our computational meeting a couple weeks ago where he said, imagine a bacterium.

and there's some sugar gradient that the bacterium is in and it wants to have more energy.

So now one thing we are tempted to do is to draw a space whereby rotating its flagella or whatever else, this thing is going to move up the gradient.

But there's something else it can do, which is that it can turn on a gene for an enzyme that lets it use a completely different sugar or let it use that sugar better or something else that solves that same problem with an effector that to us is an entirely different space.

So we look at this and we say, ah, that solution has a problem in physical space and it has a solution in transcriptional space.

If you're a bacterium, is there any sense in which those are not part of the same space?

I mean, I have no idea what it's like to be that bacterium.

One could make an argument that our dissection of these things as two different spaces is totally biased because we're scientists and we like to look at things.

That to the bacterium, both of those are effectors that live together.

That's kind of a mix.

It's almost like, you know, it's almost like you can ask about some sort of PCA, right?

If you have principle component analysis where you're learning about your world and you're going to end up with a picture of what the axes are.

that don't necessarily make any sense to us looking at the system, right?

You find, you get these components that, you know, these control knobs, and we look at it and say, what the hell is, you know, what is it doing?

But to the system, those are efficient ways to build an axis.

And people like, you know, Robert Printner and Don Hoffman have these models of these cognitive systems from scratch sort of building these worlds, these virtual spaces for themselves, right?

That you don't actually know what spaces you're in.

So I think there's a research agenda there, which is, can we build some sort of appliance, meaning some sort of, I don't know, machine learning thing or something that will try to extract these in a, I'm not going to say unbiased, but at least unsupervised manner that can we build an agency detector?

Can we build something that looks at a system and says,

I tell you what, if you assume this axis, this axis, and this axis, then I can paint you a really efficient search strategy that will allow you to predict what the system is going to do next.

Because we could view this as problem solving in this space, right?

And maybe it generates a palette of

solutions like that.

And there's one that's head and shoulders above the others.

And you say, fine, that's my scientific theory of what's going on here.

And maybe that works.

I don't know.

So that's a research kind of agenda for the future.

And the other cool thing about that is if you do that, of course, one thing you might want to do is then build a more sophisticated

synthetic intelligence by basically closing the loop and sticking that module inside the agent itself.

And here's why I'm saying that.

All animals, I think, at least past a certain point,

certainly have agency detectors.

This is why we love our visual systems, love symmetry and so on, because you really want to know, looking out into the world, you really want to know what are the passive elements and what are the other agents that might eat you, that you might communicate with, that you might convince them to do something else.

So we are constantly estimating

you know, theory of mind, right, and agency for all the things we're looking at, because that helps you get around.

You need to know if it's a rock or if it's something that's just going to come eat you no matter what, or if it's a conspecific that you might actually lie to or, you know, have a relationship, or whatever you're going to do, some sort of complicated interaction, right?

So we're all trying to gauge that.

And probably what one of the things that kind of module also does

is turn inwards to tell you stories about yourself and the reason we know that exists is because of confabulation so just as a simple example as this is a modern they're older examples from the brain split brain studies but the modern example there's um there's somebody who's got an electrode and this was this is an actual experiment that was done he had epilepsy or something and they put an electrode in their brain in his brain and it happened to land in an area that when you stimulate it it stimulates laughter the person just starts laughing right

So you're sitting there, so this person is sitting there talking to, you know, having a serious conversation with the doctor, somebody off-scene pushes the button, person starts laughing, the doctor says to him, why are you laughing?

Zero percent of the time, the answer is, geez, I don't know, that was super weird, I was all serious, and suddenly my mouth starts laughing.

That never happens.

What happens is the person says, oh, I came up with a funny story, and I was just thinking of a funny thing that happened, right?

What happens is, and this has been, you know, I'm not the first person to point this out, that we are massively unaware of lots of stuff that goes on in there.

And we have a module, apparently, whose job it is to tell coherent stories about what that all adds up to.

So not just turning outwards to see what kind of agents we see out there, but actually to look inwards and come up with plausible explanations that no, you're not an automaton because somebody pushed a button.

That's not a preferred explanation.

The preferred explanation is you're an agent, you come up with funny stories, and sometimes they made you laugh.

That's a better explanation.

So if we had such a thing, we could imagine already piece by piece starting to put this together into what might be a fairly sophisticated cognitive system that has a bit of metacognition and a bit of, much like all of us, sort of faulty access to what it is that we're doing and all these kinds of interesting things.


SPEAKER_04:
Nice.

So I'm going to kind of take it back to this time-space cube because I just really can't wrap my mind around the fact that if you slice up the cube, I think about it like a confocal Z-stack.

So if you have a slice, that's the present.

If I can't change this slice, you can take a projection of the confocal image and look at the whole picture.

If I can't change this one slice, how is it that I'm going to change the future?

All the slices up until this point lead me to make this current existing slice.

And then do I can I change any point in the future?

I don't see how that's possible if I can't change the point in the present.

So are we just like I think about we had Shanna Dobson and Chris Fields on and we were talking about expressing, you know, compressing and expanding time and space and just

time in all kinds of contexts.

Like, is it even a construct or how does it work?

And like non-Archimedean time and all that stuff.

So I don't know what, if you want to maybe unpack that a little bit for me, because I'm just not kind of seeing the possibility of this changing the future without being able to change the present.


SPEAKER_01:
Yeah, totally possible.

And so, for sure, I'm not going to try to make the argument that we have a mature theory of free will of that kind.

I don't think we do.

And I think you're absolutely right in that that is, in fact, a really critical point.

If your freedom in each slice is zero,

then many times zero it's still zero right so totally totally true i i think we can make two moves and again this is this is super early i you know i'm not claiming this is in any way gonna survive uh you know careful careful thought into the future um i think we can make two shifts one shift is that i think it's uh again uh and then that you know i'm like a broken record with this stuff but but i think i think that binary um

This binary distinction between you have freedom or you do not.

I think that's wrong.

So I think what you have, it goes by amounts.

So you may have very little, you know, if you're a mechanical clock, you have very little.

And if you're some other type of living thing, you may have more and then you may have more than that.

So then what you have is, and this is super, super amateurish, but just to kind of visualize, what I see is the kind of addition that you get in calculus, where you start with extremely tiny things that are kind of zero, but not really, and eventually they add up to something, right?

That's how I see it.

I think that what you have at each point is a teeny tiny amount, and if you look close, basically it's an epsilon that's not worth talking about.

It's basically you don't have any.

But it isn't quite zero.

And over time, that mental effort, whatever that is that you put into, you know, working on your free will or on your kindness or whatever you're working on, over time, it integrates into an area, even though you're integrating infinitely thin strips.

Like, that's the vision that I have in my head, that if we just do away with the fact that it's actually zero, that I don't think it's actually zero.

then you can build up something over time and you can and you can magnify and if you know i'll say one more thing which if we're gonna if we're gonna abuse physics like this right which is i mean i realize this is all like completely informal and whatever if we're gonna abuse it um i'll make i'll make one more one more analogy if you think about what is the

What is a minimal level of agency?

So, so, so sometimes people ask me, like, well, is there a 0 on your on your continue?

Right?

Is there an actual 0?

And like, what's the minimum?

What is the smallest?

Okay.

If you're going to pick, if you're going to design the absolute minimal.

Piece that still has some non 0 agency.

What does it need?

I feel like it needs 2 things.

It needs some minimal amount of decision making that isn't obviously caused by local causes, right?

It needs at least something that reaches outward in terms of space or time or complexity or something that isn't obviously a locally determined necessity.

That's one.

And the second thing it needs is some amount of goal-directed activity, something that looks like

A goal directed activity.

So so once you've said that, it seems to me that that individual particles already have this right?

Because because quantum indeterminacy gives you the 1st, 1 and least action principles give you the 2nd.

So you've already got that.

And so now, when you so, so I think and again, this is amateurish as far as the physics goes, but my gut feeling is that.

there isn't a zero, that you already start with some minimal amount, and now you can do one of two things.

So you've got these particles.

You can do one of two things.

You can make a rock out of them, so you aggregate them in one way, which basically gets rid of all those nice properties, and it ends up having very low agency.

It just aggregates all of it in a way that doesn't do anything useful.

Or if you're alive, you can amplify those properties and end up further down on the continuum.

So that's kind of my fuzzy story at the moment is that I think we start off at non-zero, and then you can either stay there if you don't have the right organization, or you can amplify the hell out of it and get to be more agential.


SPEAKER_04:
Thanks.

Thanks.

I definitely think that non-zero is the right answer.

So we've got maybe like 15 minutes left and, you know, I just, there's some closing thoughts, some room for some last minute questions.

Sarah or Steven, do you have any last minute things you want to ask?


SPEAKER_03:
No, my head is sufficiently exploded.


SPEAKER_00:
Yeah, no, I think that was a nice... I mean, I think there's a lot to think about because we've covered an awful lot of territory.

But I think it's... I was interested to see that you're bringing the thingness and I know Carl Friston talks about that.

So I suppose one last bit would be, would you call yourself...

a non-dual monist in the way that Carl sort of talks about that idea or is that something a bit different or maybe not even relevant in this context?


SPEAKER_01:
Yeah, that's a good question.

So I hesitate to put a name on it for two reasons.

Number one,

everything that we've talked about so far has been very much from a functional third person perspective so so we haven't really touched the the so-called hard problem of consciousness or what so in order and the thing is nothing that i've said touches this question of why is it like to be something that's one of these systems right so from that perspective

assuming that we believe, I mean, some people, you know, we'll say that that's a non-problem and that there is no such thing and then whatever, but I, I'm not a hundred percent convinced of that actually.

And from, so, so, so there are that, that I do think there's a hard problem and I don't have anything that addresses it.

And so, uh, I don't know, you know, what camp that puts me in.

The other, the other thing is that, um, I have a lot of, um, sort of, uh,

panpsychist sympathies in the following way.

So I think that if you scale down... So one reason people don't like panpsychism is it means that you think that rocks have hopes and dreams like the rest of us.

So that's obviously not what I'm saying.

The reason people go to that extreme is because they've scaled down the structures that are needed for agency, but they haven't scaled down the expectations.

So yes, it's silly to think that rocks have the same hopes and dreams that we do.

However, if you scale both sides of the equation, I'm not at all sure why.

Again, if you're not into binary classifications, it is conscious or it's not.

That I think is definitely wrong.

So if phenomenal consciousness is a kind of...

continuum two i don't see any reason why there couldn't be a tiny bit of something that's super hard for us to imagine because we're used to a much a much bigger level of perception and consciousness that that isn't associated with a lot of other systems they'll be normally that you know that the people who normally work in cognitive science would never buy as as as as having consciousness so so so i have a lot of you know kind of sympathies on that front mainly because i don't buy this binary distinction um and we could talk about this actually interesting

i don't even i and i'll tell you why a lot basically um bioengineering and the kinds of things that are possible now are they they are really dissolving a lot of things that seem to be uh sharp categories before and i'm not even sure at this point that um the distinction between first person and third person knowledge is even sharp and and i'll just i'll just draw you a simple picture and i preface this by saying i'm i i made a picture of this and i sent it to a um

a well-known philosopher, and he said it was horrible and not useful, but we'll see what you think.

Imagine that you're looking at, there's a brain and there's some electrophysiology being done, some electrodes stuck in there, and it's being processed, and you're on the outside, you're the scientist, you're on the outside, you're looking at that data.

Uncontroversially third person, right?

You have no idea what it's like to be that subject.

You are studying some signals that come off of that.

Now, what we do is we change the scenario a little bit, and what we do is we take some of those interfaces that, you know, they have these interfaces for the blind that either go on your tongue or they go sometimes in your retina, but basically they turn camera signals into something that's directly plugged into your brain with electrodes, okay?

So now we take that setup, and instead of the camera, we plug it directly into the electrophysiology apparatus.

So now what you're receiving is you're receiving a heavily processed signal, but it's coming directly from that brain into your brain.

And what's interesting is that people who use these kind of sensory augmentation devices, they will eventually report that it's just like seeing.

They learn to get around like the electric lollipop.

It's this thing you stick on your tongue, and it shocks your tongue in a way that maps onto the pixels of a camera, right?

And they say, oh, it's just like seeing.

So, okay, so now you've moved.

So, you know, it's like...

It's sort of, you know, third person, but, you know, it's maybe like 2.5 person or something.

And then you can do another experiment.

You can say, well, what we're going to do is instead of using all this, like, heavily, you know, all this electronics and processing in between, we're just going to connect the brains.

And we can do this.

And the reason we know we can do this is that there are conjoint twins whose brains are in fact connected.

so that there's no sort of clumsy electronic interface in the middle.

The brains are directly connected.

And so now, he said, OK, so if my brain is now connected to this other person having the experience,

It's sort of, is that still third person?

And then you ask the question, yeah, but in your own brain, so some people say, ah, that's an aberrant, you know, that's an aberrant case.

But in your own brain, you've got pieces of the brain that have to talk to each other.

You're not an indivisible monad of some type, right?

You are a bunch of pieces that also have to talk to each other.

So the left side has to talk to the right side.

In fact, the front talks to the brain.

You are anyway pieces of the brain having to talk to each other.

And somehow that ends up being first person.

So now what we've just done is built a continuum where you can smoothly, and we can fill in anywhere between these two systems.

I can fill in as, you know, sort of finally as you want.

And you can smoothly move from first-person experience to third-person science because the bioengineering tells you that you can do it.

So I'm not even sure that's a, you know, I'm not even sure that's a distinction.

So I, you know, it's hard to say.

Anyway, that's a long-winded answer to your question.


SPEAKER_03:
Wow.

I was going to ask one question, now I'm going to ask another one.

This makes me think, you know, like related to core screening, related to layers and levels of different organisms, like it just, what you, the kind of example you just laid out, you know, makes me question whether you can even stratify in that way or whether it makes sense to stratify in that way.

but let me go back to um something we didn't cover and and i i heard it on your podcast and um you ended with a question related to moral philosophy um or ethics i guess not even i don't know the distinction there but yeah um i mean when you know if you're coming from the perspective of not believing in a binary uh between

You said it better than I did living, non-living, whatever, like agent versus not.

And then you're making things like xenobots.

It's like that kind of if that's if you held those two views of not believing in the binary, then there it seems like there really wouldn't be an ethical question.

And so I'm wondering where you're where you're thinking is along that.


SPEAKER_01:
yeah no no i think i think there's definitely an ethical question uh and and i think it's not quite what we think it is but and i'll tell you what it is but let's just let's just take a step back so so long before anybody made xenobots

We made humans, right?

The old fashioned way.

And so this is like, you know, then it's a competence without comprehension, right?

So we have no idea how it worked.

But for, you know, hundreds of thousands of years, we made other humans.

And so we take care of them as best we can, most of the time, sometimes not really very well, you know, so we already did that.

Then we make all sorts of animals in terms of food production and other things.

We make hybrids, so we make mules that are animals that never existed before.

We've made new plants.

So the first thing that's really critical and often the reason I'm sort of sensitized to all this is that people always say,

oh my god, you're making Xenobots, it's a new, like, let's not forget what we've already been doing, right?

And let's be super clear that this is a massive problem with the food industry, right?

Long before we get to worrying about Xenobots, there's all kinds of things that we need to fix.

So I just want to be, you know, I kind of want to be clear that we've already been making animals and, in fact, other humans for which we sometimes take good responsibility and sometimes we don't.

I think this absolutely raises an ethical problem, and the ethical problem is the following.

When we go to synthetic biology and bioengineering kinds of seminars, there's usually now a session that's about ethics, and the session goes like this.

Somebody will show a brain organoid made of human cells,

And someone will say, oh, my God, that you shouldn't be doing that.

And somebody else will say, well, let's just see how much like a human brain it is.

And then people spend a couple hours arguing about whether it is or is not enough like a human brain that they need to worry about it.

And maybe they end up saying it's fine or maybe they end up saying it's not fine.

I think the much bigger issue is that whether or not something looks anything like a human brain is a very poor guide to how much you need to worry about it.

And in the past, the way that we would figure out whether or not how much moral responsibility we have for a particular system went roughly like this.

You would sort of look at it and if it was squishy and warm

you would say, yeah.

And if it was metallic and, you know, and it came from an assembly line, you'd say, do what you want.

No problem.

Right.

So, so two, two things used to be a guide.

And even that we didn't do so well, we can obviously with those principles, but, but, but two things that used to be a guide were, where did it come from?

Did it evolve or was it designed?

And what is it, what is it made of?

And, and it was a, and that was pretty good, right?

You could,

Because if it was evolved, you could look on the tree of life and you could say, it's a worm.

We don't need to fill out any forms when we do these experiments.

Or you could say, it's a frog.

I need to fill out a lot of forms before we can do experiments.

Then that's, in fact, how it works.

Because you know something about that lineage and you make some sort of guesses about where things are.

And then if it's an octopus, people really don't know what to do and so on.

all of that is going completely out the window, right?

So in the next, I don't know, couple of decades, we are going to be surrounded by, and this is another thing that I'm writing on,

the option space of possible creatures.

We're talking about hybrids, cyborgs, hybrids.

There's a million different ways to recombine evolved, designed, living, non-living, and software agents into new forms that have never existed before.

So what something looks like is no guide to what the cognition is.

Where it came from, meaning evolved or designed, is going to be a bad question because half of the stuff we're building with is now evolved and so on.

All of the things, you know, it's just a machine versus, oh, but you know, it's a nice mammal.

All of that stuff is gonna go out the window.

So to me, the ethical problem is much bigger than just asking about whether something's like a human brain.

What we really are going to need

is a new ethical system for learning how to deal with agents that look nothing like us and that don't look like anything we've ever seen before and yet are going to have all sorts of cognitive capacities.

And we have to let go of categories like machine, like robot, like all these other things that really don't mean anything.

They never did before, but it was fine in the past.

It's no longer going to be useful.

And that's the ethical problem.


SPEAKER_04:
wow uh so great so i'm gonna go stare at a wall and try to integrate this information uh it was a lot um thank you so much for coming on thank you so much yeah this was really fun yeah great great great discussion thank you so much thanks bye
SPEAKER_00:
Hello and welcome everyone.

This is Act-Inf live stream number 49.1.

Welcome to the Active Inference Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

This is a recorded and an archived livestream, so please provide feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll be following video etiquette for livestreams.

Head over to ActiveInference.org to learn more about the Institute and how to get involved in projects such as the livestreams and others.

okay we are here in live stream number 49.1 we are having our second discussion on the paper a worked example of the bayesian mechanics of classical objects in number 49.0 along with yakup and ali we did some background and context and now the gloves come off the curtain comes up and we'll begin slash continue our discussion

and we're really appreciative Dalton that you've joined today looking forward to how these discussions go so we can begin just by saying hello and anything that we want to explore or discuss today and then we'll just jump right into it so I'm Daniel I'm a researcher in California and I

think some of the questions that are really motivating me today is what is mechanical about Bayesian mechanics and how do we take concepts that have physical interpretations like center of gravity or even some of these more atomic level terms that were brought into play how do we apply those kinds of terms and ideas to cognitive science and I'll pass to Ali


SPEAKER_01:
Hello, I'm Ali.

I'm an independent researcher from Iran, and I can't find enough words to express how excited I am to be here.

Actually, we talk a lot about Dalton's work and how

groundbreaking it is.

And personally, I had immersed myself for the past several months in Dalton's paper.

And for the past couple of months, we had fascinating discussions around this particular paper.

So I'm feeling kind of starstruck right now.

And I'm very much looking forward to our discussion.

And I'll pass it to Jakob.


SPEAKER_04:
Hello, I'm Jakob.

I'm a student in the UK and

I suppose I'm also very excited to be here today and to discuss the different implications of Bayesian mechanics and how it's related to other areas in contemporary physics and how that can then be applied to cognitive sciences and neuroscience.


SPEAKER_00:
Awesome.

So welcome.

Thanks again, Dalton.

Feel free to just begin where you'd like and bring us to the paper.


SPEAKER_03:
Of course, yeah.

I guess I'll also introduce myself, just in case there are people in the audience that don't recognize me or have a face to a name.

I'm Dalton.

I'm a researcher.

I'm based at Stony Brook University in New York, where I spend most of my time

in the Department of Mathematics and the Department of Physics and Astronomy.

I'm also at the Versys Lab, where I'm the mathematics and the physics of the free energy principle.

So it's a pleasure to meet all three of you.

And thank you, of course, for the kind words.

I think we can maybe begin with

very broad overview of the paper.

I will say, so one of my...

One of the aims of my research program is to get a better understanding of the mathematics of random dynamical systems, especially coupled random dynamical systems or systems that model non-equilibria phenomena.

And one of the interesting things to me

is the idea that within cognitive science or neuroscience, there are some algorithms that have been developed over time that have been built specifically to cope with these kinds of problems, which are problems that we don't know really how to make sense of mathematically, even physically.

We're on the back foot.

But in neuroscience, the primary object of study is a non-equilibrium system.

It is non-equilibria and understanding non-equilibrium phenomena in the brain.

And so one supposes that the methods that have been developed over time in this area, just maybe by trial and error,

or at least trial and error by the standards of perhaps a pure mathematician, may have some insights into maths and physics in the same way that there's a very rich history of physics inspiring more advanced mathematics and then mathematics circling back and making physics more rigorous.

I think there's a great opportunity to do that in biophysical systems.

And I think the free energy principle is a source of great inspiration along those lines.

So with that meta commentary out of the way, one of the things that I wanted to do with this paper is to actually drill down on that point, because I think it's something that maybe gets lost in translation if you just read kind of traditional active inference papers that are all about modeling cognition.

I think there's something on the other side of the aisle to be said about

what do those models of cognition say about modeling systems that are cognitive?

Or what does it say about complex systems that are cognitive-like?

And how can we make sense of those things mathematically?

So that's in some sense the motivation for Bayesian mechanics.

And that goes back to... Well, it goes back a long time.

It goes back to probably Carl himself.

He mentions something like this at a couple of points in the 2019 monograph.

But it was worth writing a paper specifically with the motivation that...

If we consider what's been written about the free energy principle so far, there's a very nice foundation for better understanding the mathematics and physics of coupled or non-equilibrium stochastic processes.

And so that was the motivation for this particular paper.

And in addition to building out the fundamental mathematics in the first few sections, it's then possible to contextualize all that work with a kind of nice worked example of what does this mean for some kind of simple systems?

How can we actually take this foundational maths and make it algorithmic again?

And then foreshadowing maybe even greater complexity in the very last section.

So, yes, a lot of this goes back to a paper that I co-authored with a number of other people earlier in the summer called On Bayesian Mechanics, where it's proposed that, you know,

complex systems have a kind of informational physics about them, and that maybe the material physics or the maths for these systems is quite challenging in general.

But maybe if you map these systems into the informational world, you can utilize new techniques and make sense of these kinds of systems

by approaching it from a different kind of viewpoint.

That is, I think, the strength of Bayesian mechanics, as, again, Carl and others have construed it, is it is a way of talking about informational physics or the physics of Bayesian beliefs, the laws of motion on a statistical manifold.

There's various ways of kind of couching that in.

more formal language.

But again, that was the motivation for the paper.

And then as that gets developed, eventually there's a point where we really get into the trenches and actually try and deploy this in some useful way.

I think that those are really the kind of two aims that come together in some sense.


SPEAKER_00:
Thank you for the summary.

Well, there's many ways to begin and start.

Perhaps we could start with the context that we brought, which was thinking about this.

And you touched on some of these threads as well, such as the key literature and some of the history.

Let's continue on in this context setting before we get to the formalisms and the worked examples and talk about the three kinds of theories, including what you mean by a theory, and then the three non-Bayesian mechanics, and that'll set us up really well to see where Bayesian mechanics comes into play.

so this was in that paper just mentioned on bayesian mechanics of physics of and by beliefs so what are dynamics mechanics and principles and how were you using theory and why was it so important to be clear about what a theory was and the distinctions amongst these three terms


SPEAKER_03:
Yeah, of course.

So in that paper, there is this kind of trichotomy that gets introduced where we talk both about dynamics and mechanics and principles and disentangle them in a very particular way.

So we think about dynamics as a kind of observational thing.

And it's important to separate that from mechanics because complex systems...

as a science, is very interested in dynamics.

Can I kind of infer the laws of these kinds of systems just by modeling them using some kind of curve fitting or using some kind of set of canonical models that I know sort of fits this situation?

But at the end of the day, yes, dynamics are mostly about the particular trajectories that a system takes through state space or physical space.

And this is conflated sometimes with what I call mechanics in that paper, which is something a little bit different.

It is the laws from which these trajectories are determined.

So, for instance, dynamical systems people, especially in pure maths, really like the example of billiards.

So you send one billiard ball crashing into another one and you could talk about the trajectory that the ball takes and you could write down the equations of motion for this system and just add up all the forces acting on it and figure out what direction is this ball that got collided with, what direction is that going to go into, with what momentum and all of these things.

But then

Figuring out that trajectory, once you plug in all of these kind of boundary conditions, the mass of the ball, the angle of the initial contact, and so forth, the way that you can kind of compose all that information together, that needs to come from somewhere.

And that's what we call, or what I call, a mechanical principle.

And this is not...

a distinction that is new to the literature.

I'm not breaking new ground here, but I think it is one of those things that's not made sufficiently clear in the FEP literature, or at least in the places where it's adjacent to physics.

So mechanics are about the laws that give you those trajectories.

So we know that the forces acting on one or another billiard ball sum together because of Newton's law of motion.

That's a mechanical theory.

If I have F, then I get M times A. And I can plug in F, and I can plug in A, and I can get a trajectory.

But if I abstract away from those particulars, I get some kind of law that tells me, OK, how are trajectories generated in the physical world?

um abstracting one more step we can then ask okay so where do mechanical theories come from what tells me that i have the right mechanical theory generating the right dynamics and this is about um

the kinds of very fundamental laws that we know hold everywhere in the universe at a given scale.

These are principles like conservation of energy, and this gives you the behavior of collisions or the principle of stationary action, which actually gives you back Newton's law.

So you can now chain these things together and say one ball hits the other one,

It goes in this direction because if you sum the forces and you add up all the directions, you get a net movement.

And you can do that because these billiard balls minimize or at least make stationary their action.

So their motion must follow this particular mechanical law.

And that's the law that generates these trajectories.

So the reason why this is mentioned in the Bayesian mechanics paper is because these have, I think, been conflated in the literature previously, because you have

the idea of the free energy principle, which is a principle.

It's a thing that gives you back some kind of generative rule for the dynamics of specific systems.

And that is usually something to do with the motion of internal states on a statistical manifold in the sense of, for instance, Thomas Parr, Lance de Carsten, and Carl Friston's article, Stochastic Thermodynamics, Markov Blankets, and Information Geometry.

I think that's the title anyway.

So if you say, okay, the dynamics of a system whose parameter performs approximate Bayesian inference minimizes the surprisal,

then you get approximate Bayesian inference as a law of motion for the dynamics of some object.

And this law is usually written as a particular gradient flow relation on the space of beliefs.

So it tells you how do beliefs change as those parameters doing the inference change.

And then if you add in progressively more details, okay, what do those internal states actually mean in the context of the system?

What are those beliefs about?

These are the kind of boundary conditions that give you the trajectories, the specific dynamics of the system.

And there's an interesting sense in which these boundary conditions are literal boundary conditions because understanding what's internal and what's external and being inferred is precisely the specification of a Markov blanket.

So as soon as you actually give the system a very particular partition, as soon as you carve it up in a specific way, you start to get a sense of the actual evolution of a system as one might see it in the real world.

But this is, I think...

um separating things out in a formal sense like this is useful because it avoids skipping from step one which is minimize variation of free energy or surprisal and i'm going straight away to this is how um something like an active inference system uh does prediction and action you know there's there are some steps in the middle that haven't really been filled out if you just go from a to b directly and when you when you begin to talk about the maths and the physics

collapsing things together like that starts to throw away some important details that actually do make a difference in the way that you tell the story.


SPEAKER_00:
Awesome.

Ali or Jakob, anything you want to add in or ask there?


SPEAKER_04:
Yeah, Jakob.

Yeah, I would maybe ask regarding the kind of laws of motion of Bayesian mechanics.

You mentioned approximate Bayesian inference.

Would that imply that the three phases of Bayesian mechanics, the mode matching, mode tracking, and path tracking, would you say those are also specific laws of motion within Bayesian mechanics?


SPEAKER_03:
You could say that, yeah.

I think...

At this point, the distinction gets very fine.

And so there is a place to usefully truncate it.

But yeah, these are three different kinds of approximate Bayesian inference.

And so they are three different kinds of motion under free energy principle minimization.

And so this is kind of, I guess, a middle area between dynamics and mechanics.

You've started to specify some things like, am I doing approximate Bayesian inference over states or am I doing it over paths?

But you're still lacking actual dynamical details about what do those states of paths mean.

So this is maybe a middle area that blends the idea of approximate Bayesian inference as a law of motion under surprise minimization and actually getting down to the very bottom of this pyramid.

So if I were to put it somewhere into this figure, I'd maybe slot it in.

Yeah, that's pretty much what I'm imagining is you have approximate Bayesian inference

sort of roughly up here, and then there are different expressions of that, just as there are different expressions of Newton's laws, depending on if you're in fluids or rigid bodies or celestial mechanics.

So you can introduce even finer partitions than these three.


SPEAKER_00:
There's many places to go.

One question is, what are these modes about?

Who or what is tracking what and when?


SPEAKER_03:
Yeah, so this is kind of about

The idea being approximate Bayesian inference tells you that systems match their parameters in such a way that they perform inference.

So when we think about Bayesian inference, we can talk about it as simply inferring the parameters of a probability density.

So in the free energy principle, when you get approximate Bayesian inference saying, OK, systems that are coupled synchronize certain parameters, this is Bayesian inference.

And conversely, if you have that synchronization, you have Bayesian inference, or you have approximate Bayesian inference if you're inferring the parameters of an approximate density, which is where you get into the factorization of the free energy functional that we are familiar with, where you have...

a variational density matched to a joint density, but you can split it into the variational density matched to a conditional density plus an extra surprisal term that kind of measures how good that approximation is.

So this descends from that idea that you can deduce what a system is doing just by saying, okay, it's performing approximate Bayesian inference, because that allows you to say its parameters are synchronizing to the parameters of the environment or of another system.

So that's what we mean when we say modes in this case.

A mode is just a parameter of the distribution.

And it comes from some of Carl's earlier literature where he uses the Laplace approximation.

So specifically, we're kind of fitting a Gaussian density to some arbitrary probability.

And then the mode of that Gaussian is one of the parameters because it tells you, OK, where is the center of that density?

And if you also get a sense of where the variance of that density is now suddenly by fitting those two parameters, you can talk about the system

Because now you can say, okay, the physical parameters of the system are encoding these parameters of some density.

And in a very natural way, this gets you back much of statistical physics, because it's all about how does a system reflect the statistics of another system?

probabilistically, how does this look?

But this is kind of upgraded in Bayesian mechanics to noting that, well, that inference happens because the physical parameters of the system are matching the physical parameters of the environment.

And that's where the probabilities come from.

That's why we talk about beliefs.

So when we say modes, we're referring to parameters that are fit for synchronization.


SPEAKER_00:
Excellent, thank you.

Jakob?


SPEAKER_04:
Since we have the schema on the slide there, I just wanted to ask, where do you think that the g-theory formulation fits into this diagram?

Because with g-theory, there is approximate Bayesian inference, but

It's not necessarily density over state.

So is it another node directly from the Bayesian mechanics?


SPEAKER_03:
Yeah.

So in this figure, the approximate Bayesian inference lemma is a very specific thing that originally goes back to

I believe originally it's in a 2012 paper by Carl in Entropy called The Free Energy Principle for Biological Systems.

But certainly a much bigger deal is made in the 2019 monograph.

And it's the approximate Bayesian inference lemma, as he calls it, is specifically about mode matching.

approximate Bayesian inference is a bit more general so if you were to kind of slot that in somewhere in here it would maybe be parallel to Bayesian mechanics because just like classical mechanics the law of motion is Newton's laws for Bayesian mechanics the law of motion the thing that generates specific physical dynamics is approximate Bayesian inference or the law that when you do a

have synchronized systems, you can write it as variational free energy minimization in virtue of the parameters of the two systems matching up.

And that's the parameter that minimizes the distance between a held density and an observed density.

So approximate Bayesian inference lemma is that result specifically.

But that's more general.

The idea of approximate Bayesian inference is more general and probably

someone needs to write down a generalization of the approximate Bayesian inference lemma to paths specifically to talk about path tracking.

But once you have a path that fits the bill as a mode, and once you have two systems evolving together, tracking each other's modes or each other's paths, and if you have a probability density over those paths, you can now talk about approximate Bayesian inference in the path setting.

Now, where that has to do with g-theory, so g-theory, the name and kind of the vision is actually due to Maxwell Ramstad, a colleague of mine, also here in the Versys lab.

And he has suggested that

G-theory kind of lies in the intersection between a few different things that fall out of the path-based formalism.

So if you have approximate Bayesian inference over paths, if you're doing path tracking,

then there are lots of things that do come out of this, almost canonically.

One of them is everything that comes along with path integral formulations in quantum mechanics.

And this is, I take advantage of this at various points in the paper.

At one point, I use it to explain, okay, why does Bayesian mechanics actually make sense in the classical world?

And then later on, I use it to relate it to chaos.

So that's a specific example.

I think more generally, path tracking deals with non-stationary systems very well, because you're allowed to talk about now how does the density, how does the belief change in time?

How does the mode of a single belief change over time to make a kind of modal path?

And what are the marginals?

If you slice up the density along that path, what do you get?

So you can talk about non-stationary systems.

You can talk about systems out of equilibrium this way because you can relate

the free energy principle over paths to what's called the principle of maximum caliber in statistical physics, which is all about non-equilibria and entry production and non-equilibrium steady states.

So I think...

As we have written about it, g-theory is the duality between path integrals and the FEP over paths, or the duality between maximum caliber and the FEP over paths.

But it's...

I think that's selling it short a little bit because it's also the thing that brings in all these other ideas.

And stochastic thermodynamics is a really interesting one as well, right?

Also falls out of the path-based formulas very well.

Also does very well with non-equilibria.

So g-theory is kind of this larger object that makes sense of why that column of Bayesian mechanics actually works and explains a little of the true power of the path tracking as opposed to mode tracking.

Why are we interested in forgetting about states?

And that's the explanation for that, or the justification for that resides in g-theory, which is supposed to explain why do all of these things just come out of the path-based formalism?

Why does it make sense?

And why does the FEP over paths lie at the center of a lot of disparate fields, which I think is one of the real problems.

very powerful statements that may be possible in the near future is, is the FEP actually canonically the theory of everything in the sense that there are lots of approaches in statistical physics and stochastic process theory that trace back to the FEP or the FEP subsumes or implies or something like this.

G-theory is supposed to kind of

plug into that last column, but that is somewhat outside of the hierarchy in that sense.


SPEAKER_00:
Excellent.

Thank you, Ali.


SPEAKER_01:
Well, yeah, when I was reading about geo-theory, I thought it somehow relates to Ralph Landor's principle, which basically says that if an observer loses information about a system, the corresponding entropy in a non-information-bearing process

degrees of freedom of the information processing system or its environment is increased.

Because Landar believed that information is physical.

So he somehow put it in terms of a kind of thermodynamical

point of view, but how does it fit or can g-theory be viewed through the lens of Landar's principle, which I think is not a completely non-controversial principle as well?


SPEAKER_03:
Yeah, that's an interesting question.

I think insofar as it does give you some technology to talk about entropy production at non-equilibrium steady state.

So it does give you a way of saying, okay, if I'm exchanging with my environment, if information is flowing out and flowing back in, then there is some increase in the entropy.

And with respect to things like control theory, this is pretty interesting because it allows you to talk about, okay, if the system is...

observing something about its environment, if it's constantly maintaining a steady state, so engaging in control, then you can talk about that entropically, and you can talk about the irreversibility of processes, even at steady state.

And there are things that connect to that, like

the Crookes fluctuation theorem or Yasinski equality and things of that nature.

So my guess would be it probably covers something like Landauer's principle in that context.

Talking about the energy required to erase a bit of information, say, is ultimately the energy required to insulate yourself from the environment.

That minimizes your surprise.

It means the environment does not offer information to you.

But that does take energy and sometimes quite a lot.

So that's probably where that connects to Landau's principle, is talking about and quantifying the kind of cost of maintaining a Markov blanket in probably entropic terms.


SPEAKER_00:
Great.

I think this points towards somewhere we may explore over this multi-scale engagement, which is unconventional computing and low energy computing.

For example, the often remarked upon energy expenditure of the brain.

compared to, say, a neural network operating on traditional computational architectures using many, many light bulbs, many, many calories of energy versus a brain which is able to carry out some tasks with seemingly low energy expenditure while other tasks it does not seem to be able to perform at all.

I want to pull back

before we go into the three non-Bayesian mechanics and ask, what is Bayesian?

What does it mean for something to be Bayesian?


SPEAKER_03:
Yeah, in my mind,

Bayesian is about inference.

It's about a particular method of doing inference that gives you consistent unbiased results and fits in very nicely with the rest of probability theory.

In the context of Bayesian mechanics, it is the idea that the physics of material systems, or the material physics of complex systems, maps into a physics of beliefs.

So you can talk about

the parameters that are doing approximate Bayesian inference, or you can talk about approximate Bayesian inference.

You can talk about, okay, what are the beliefs parameterized by that mode doing?

And how does that reflect something about the dynamics of the system?

The reason why this is interesting is not just because it's a different approach to the problem and dealing with interacting systems and systems out of equilibrium is an area that does need new approaches because it is a very poorly understood area within mathematics and physics.

It still is.

It has been for some time, I think.

But it's also interesting because complex systems are informational.

So there's some sense in which this brings us closer to the metal.

So it's not just a new view.

It is the right new view because complex systems are systems that compute and capture data.

They are systems that have memory.

So it's not as simple as even here's a new way of writing down the dynamics of random or stochastic processes based on the parameterization of some action functional.

That's valuable because that's a new thing, but it's also...

It fits very nicely into our understanding of complex systems as things that have an informational physics.

Understanding the dynamics of things like the brain is contingent upon understanding what the brain is doing.

And the brain is capturing and computing over data.

It is storing memories.

It is learning and sensing and inferring.

it's not just that it's a new view, it's really the right view because it gets to the idea that complex systems do things physically because they are doing something in informational space.

And the brain is a wonderful example of that.

The brain is a coupled random dynamical system.

It's coupled to its environment.

So you can talk about this parameterization thing, this synchronization thing, but it's even more insightful than just that because the brain is doing things

approximate Bayesian inference.

So I suppose it's no coincidence that the synchronization of coupled random dynamical systems gets you back this idea because I think that is

empirically just based on observations, I think that is as close to a physical law as one might be able to get at this stage in complex systems theory, is that things that synchronize do perform approximate Bayesian inference, because things that synchronize are things that are controlled systems or computational systems.

So in that sense, I think it's right on the mark.

And as I said, it's interesting that this goes way back to trying to figure out things about the brain.

It goes way back to Carl Friston's original work on generalized filtering and SPM and these kinds of algorithms that were designed to make sense of what is the brain doing physically based on what we think it's doing inferentially.

The relationship between structural and functional connectivity and the relationship between neural representations and neural firing patterns.

But it has become something massively more general because it is the right kind of maths to talk about these sorts of systems.

And that's in turn because these sorts of systems do this kind of approximate Bayesian inference.

So when you ask, yeah, what is Bayesian about Bayesian mechanics?

It is about the relationship between Bayesian beliefs and Bayesian inference and physical parameters representing something about the information available to that system or the statistics of a system coupled to that system.


SPEAKER_00:
Awesome.

Just a few notes before we then go into the non-Bayesian mechanics.

One is from ecological psychology, where many of our terms are drawn from, such as affordances, people have long pointed to the way that we use language like, can you grasp it, optimal grasp of ideas, and so tangible and peripersonal spatial ways of talking about ideas and cognitive processes.

and then well what is the first thing you want to know after you get optimal grasp how much does it weigh and you said it has become massively more general well is that in grams or in what so i see this as part of the development of the physicality of cognition in that the ecological psychology brought in relationality and uh action orientation the pragmatic turn

And then now what about those objects?

If we drop the idea, what happens?

If we drop it from the discussion, what happens?

And then the second piece is the very interesting historical note about the development of complex systems.

and about the appropriateness of this kind of a formalism because at the very least standing as extremely on the instrumental spectrum as we can be we're going to need to bake relationality into our analysis of any system whatever it is that it is doing

And then it may come to be the case that if those systems are also cognitive or understood in terms of informational manifolds...

then it's a doubly appropriate approach and the kind of double ratcheting of cognition makes it difficult to see how a framework that doesn't have some of these characteristics could be adequate to describe that kind of a relation so great points let us go to the non-bayesian mechanics so

The non-Bayesian mechanics are classical, statistical, and quantum.

so however you'd like maybe just uh on each of the following three slides just what are classical mechanics statistical mechanics and quantum mechanics uh kind of steel personning the argument and just what do they explain what will they continue to be useful for and how can we see them as special cases or adjacencies or however you'd like to frame it with respect to Bayesian mechanics


SPEAKER_03:
Sure.

Yeah, so beginning with classical mechanics, I think, is probably the simplest example.

You have kind of the physics of very large, very slow things.

Large is kind of a relative term, large compared to atoms maybe, and slow compared to the speed of light.

But nonetheless,

This is a broad region of physics that describes things that don't have relativistic effects, things where observations commute, so it's non-quantum, things that are not noisy, so there's no probabilities, it's not statistical, and things that do not require relativistic corrections, so things that are below the speed of light.

When you have all four of those ingredients, I think I listed four anyway, you have classical physics, or at least you have a classical object, and the physics of that thing is classical mechanics.

So this arises from, you may have inferred, four limits of other kinds of physics.

So you want to be now infinitely precise.

So this is the kind of no-noise limit of statistical mechanics.

It's the slow speeds limit of special relativity.

And it's a very interesting limit of quantum mechanics that I can get into a little bit, but it begins to get a bit complicated.

Maybe I'll save that for the third slide.

So yes, but...

Until then, I'll say there is a classical limit of quantum mechanics, that you take a quantum field theory and you can produce a classical theory out of that in a relatively well-defined way.

And again, I can talk a little about the nuances to that statement.

There are some.

But regardless, classical physics is about...

large things, really.

The interesting thing is that all four of these limits kind of coincide to some extent when you have a

very large system.

So very large systems allow you to use the law of large numbers to kind of sharpen your beliefs about a system.

So you can imagine this as placing greater precision on the measurement of your system.

And that recreates sort of the no noise limit, because you get more and more precise in your estimate of the dynamics of the system.

Many body systems with very small objects are systems where you start to see quantum corrections.

So particles, like atomic particles, for instance, you begin to run into a regime where things are small enough that quantum physics becomes relevant.

And also large systems tend to move pretty slowly as just a general rule in what's called effective field theory, which talks about how one theory is a limit of another theory.

And in particular, it talks about as you increase in scale, you get simpler theories.

So I think we would probably all agree classical physics is a lot simpler than quantum physics, and that's because effective field theory says that large things have simpler physics in very rigorous senses.

One expression of that is

kind of the momentum scale is inverse to the mass scale.

So larger things generally have lower momenta, lower energy.

Very small things whiz around at very fast speeds.

They have high momenta.

So once you start to talk about large systems, things begin simplifying in such a way that more often than not, you run into classical physics as a sufficient description of the scale of observation that you're at.

So classical physics, that's where it comes from.

It is about Newton's law.

Classical mechanics, Newtonian mechanics, as we might call it, is about Newton's law.

It says that

well, Newton's three laws, I suppose I should say.

They say that every action has an equal and opposite reaction.

So this is the idea that things that...

So contact forces have, at the classical level, an equivalent force in the opposite direction.

You can think of this as a kind of, if you're familiar with exercises in school in this area, you're often asked to calculate the normal force of something.

The normal force is just what pushes up on me when I push down on something else.

So this is why I don't...

phase through the ground when I stand up is because there is an equal and opposite reaction.

I'm not only putting a force on the ground, but the ground is acting on me and keeping me up.

That's one of them.

The other one is the very famous F equals MA.

It says that the force acting on a system is the mass times the acceleration of the system.

And likewise, you can deduce the acceleration of a system by taking the force acting on it and dividing out the mass.

So this tells you, okay, how do we generate dynamics out of some boundary conditions, like what is the mass, what direction is the force acting in?

And classical mechanics comes from a, what we call, least action principle.

And there are a number of equivalent ways of writing this, but this generalizes to what some people call Lagrangian mechanics, and that tells you

Imagining what are the energies that those forces came from, how does the system behave with respect to its energies?

And if you talk about stationary action, you talk about minimizing energy, then you can recover Newton's laws as a system accelerates along force gradients.

And it doesn't waste energy by going any faster or going any slower.

It does exactly what it's told to do.

So it uses precisely the energy budget available to it.

There's a very nice, well, maybe I shouldn't say it about my own paper, but there's a very lengthy exposition of this in the physics of and by beliefs paper that we've talked about once already.

I think it's section two where I run through why is the analogy to classical mechanics even useful here?

And it's because of the stuff that we talked about previously with

the very nice dichotomy or trichotomy between principles, mechanics, and dynamics.

But anyway, more broadly than that, classical mechanics is a very good example of a least action principle because it's the idea that the dynamics of classical things comes from a particular scale-appropriate limiting least action principle.


SPEAKER_00:
Excellent.

Ali or Jakob, anything to add, or we'll move to statistical mechanics?

Okay.

Classical descriptions.

On to statistical.


SPEAKER_03:
Yeah, so statistical mechanics is...

A bit of an interesting one.

Statistical mechanics is a lot broader than I think one might initially think, because at the end of the day, it is to some extent just about systems with, as you've written here, probabilistic degrees of freedom.

And that is...

That covers a lot of things.

Probability theory is just maths.

So anywhere you can use probability theory to describe some system, you can use statistical mechanics.

And we know that this is...

This generality does exist because you can talk about quantum statistical mechanics.

So you can take the physics of very small, very fast things and use statistical mechanical methods.

You can use probability theory and entropy in these things.

And you can also go all the way up to black hole thermodynamics, and you can do the same.

So black holes are very large things, but you can still talk about the thermodynamics or statistical mechanics of black holes.

So it's worth saying that...

We can talk about the no noise limit of a particular kind of statistical mechanics, but then we need to be a little bit more precise about what that generates.

In this case, I'm thinking of classical statistical mechanics.

So it's the statistical mechanics of interacting particles or systems with microstates that relate to macrostates via some kind of Gibbs relation.

and things that have free energy, and in particular, Shannon entropy.

So this is a little bit more of a narrower idea of what statistical mechanics is, just because we need to constrain ourselves a little bit even to get off the ground.

Otherwise, we'd be covering way too much.

But it is the case that if you take statistical descriptions of things

ensembles of objects and you become more and more precise and you get rid of the noise, then you can reproduce kind of classical equations of motion.

And there are ways of doing that.

One such thing is called mean field theory.

So in mean field theory, you get rid of the noise in a system and you just say, okay, here is the trajectory of the system in the same way as we might do with...

classical mechanics.

I'm skipping some details there, and the analogy is a little bit more nuanced than that, but it's kind of a useful mental picture, I think.

And one of the interesting things to note, I think, is what you've written here, is that there is a principle that's relevant for statistical mechanics.

Because StatMech is very general, it's very mathematical, it requires an equally mathematical principle.

And so in probability theory, we have what's called the maximum entropy principle, which is just a way of designing probability densities around inferences that satisfy some mathematical desiderata.

So they should be consistent.

They should be unbiased.

They should lead back to Bayesian inferences under suitable conditions.

And MaxEnt is that thing.

And we know due to, well, work by James, but more recently work by people like Ken Dill, also here at Stony Brook, and before him, mathematicians like Markovic and Villani,

All of this is a large body of work that points to the idea that if you maximize entropy, you get the mechanics of diffusion.

So you get the mechanical rules that generate mass action like diffusing particles.


SPEAKER_00:
Excellent.

I just want to ask about one word, which was unbiased.

So what does unbiased mean here?


SPEAKER_03:
I think you can think of it as avoiding overfitting.

So one of the things that you can consider MaxEnt to be is a recipe for doing inference that...

maximizes ignorance subject to some constraints.

So if you have some known unknowns, then you want to incorporate those.

But if you have unknown unknowns, you don't want to risk overfitting for those things that you're not aware of.

So if you're not aware of something, you don't want to falsely incorporate it into your inferences.

You could consider this as avoiding overfitting, or you could consider it as

kind of optimizing for subclasses.

If you're thinking of probabilistic stuff as a problem of classification, then you can think of it as optimizing for subclasses that you don't have information on.

But either way, it is the idea that you don't want to introduce extraneous information into your inference because that extraneous information biases you in a direction that the data doesn't support.

So that's really the idea behind Unbiased.

And one of the things that Janes did, and not only Janes, but also Shannon before him, who worked on the probabilistic aspects of this, and then after Janes and Shannon did,

Shore and Johnson, who also were probability theorists and worked on MaxEnt axiomatically, not like Jaynes, who took it more as a physical rule for a long time and then got into the probability theory of it.

Either way, all of these people are able to justify with rigorous proofs that maximum entropy is the best way of making unbiased inferences.

And there's lots of literature on this.

Those are three useful names, but there's a lot more due to Ariel Katicha.

I already mentioned Ken Dill, Adam Giffen.

Lots of people have taken an interest in this area.

Rightly so, I think.


SPEAKER_00:
Awesome.

We're going to keep moving to now quantum, and then already many of the seeds that bring us to Bayesian are planted.

So quantum, what is it?

How did it come into play in your work?


SPEAKER_03:
Yeah, so quantum mechanics is...

I think, qualitatively different from classical and statistical physics.

It's about very small things or very fast things.

And as I was saying, effective field theory tells us these things usually go together.

And it talks about kind of when you get things that are not classical.

Well, at least it started as when you get things that are not classical, what do you need to add to your theory to make the right predictions?

So quantum mechanics began as a kind of modeling effort.

It grew from what we might call semi-classical physics, which is just about quantum corrections to classical stuff.

It grew from semi-classical physics to its own genuine branch of physics in and around the 20s, I suppose, certainly the early 1900s anyway.

So one of the things that becomes relevant is probability theory, because quantum mechanics, you may have heard about Heisenberg uncertainty.

It's about the fact that measurements do not commute.

So the order that you do a measurement in matters.

A consequence of this is that there is always some inherent uncertainty.

when you measure so-called conjugate variables, which are just things that fit into the theory as noncommuting pairs.

So you measure two noncommuting pairs, there is some uncertainty.

You can measure one precisely.

You can't measure both precisely.

And moreover, there's a trade-off.

If you measure one with infinite precision, you know absolutely nothing about the other one.

If you try and play half and half, then

that's what you get.

You get kind of blanket uncertainty about both measurements.

So because of the non-commuting of variables, and this is kind of an empirical fact, this doesn't come out of nowhere, but it is one of those things where it just happens to be right because it's so elegant that it wouldn't make a lot of sense if it weren't, much like, I guess, the coincidence of synchronization and Bayesian inference in complex systems theory.

because non-commuting variables gives us all of the rest of quantum mechanics, the stuff that we really know and love, and also is a relatively well-defined way of getting back classical physics, because all you have to do is take the theory and make things commute, and then you get classical physics back.

um i'm i'm brushing some difficulties under the rug it's as you might imagine it's not that simple and um technically it doesn't work but it sort of does uh and and if you're interested i guess um you can look into what's called deformation quantization which is exactly the idea that if you take a classical system and you uh deform it in such a way that things no longer commute then you get back quantum physics but um

You can't really do it the other way around.

There's a bit of subtlety to the other direction.

Nonetheless, that is one route to making quantum stuff, classical stuff.

Another route is just focusing on the probabilities.

So because of this inherent uncertainty, quantum particles, their equations of motion,

have what we call transition amplitudes, which gives the probability of moving from one state to another.

But again, nothing is certain in quantum mechanics.

If you, however, make it certain, mathematically, you can go in and just take the no-noise limit

More properly, you should do something called wick rotation first, which just allows that limit to make sense.

Although, well, yes, you should do that first.

If you do that, you take the kind of no-noise limit, then you get certain state transitions, and this just gives you back a classical dynamical system.

You know where it's going once you have the equations of motion.

And this is the route that I take in the paper because it fits very nicely into the path integral formulation in a way that kind of just makes things make sense.

And one of the nice things that I then go on to say is if you have a parameter sitting inside that path integral,

then you can make sense of why classical physics comes from the no-noise limit by writing it as a Bayesian mechanical problem.

You can write about synchronization and get back the Bayesian mechanics of classical stuff just by doing this very well-defined procedure.

This is something that already exists in physics and is fairly uncontroversial.

So that's what you get in quantum mechanics.

It's about small stuff.

It's fundamentally probabilistic because there is fundamental uncertainty.

And again, this traces back to empirical facts.

I think that about covers it.

I don't know if I should have said more or if you have other questions.

I think that's sufficient at this benchmark for that slide.

Oh, it's great.

Jakob?


SPEAKER_04:
Yeah, I realize the paper dealt mainly with Bayesian mechanics for classical objects, but do you see Bayesian mechanics being applied to non-classical phenomena like quantum tunneling or entanglement?

which cannot really be explained in the limit of large numbers.

And then also, second question, which is kind of on the intersection of quantum mechanics and statistical physics.

Is there any change in the formalism when you introduce the notion of indistinguishability between particles?


SPEAKER_03:
Sure.

Well, those are really good questions.

I think, yes, there is probably an idea

of um quantum tunneling that that could be achieved maybe um because uh quantum tunneling if if you're a physicist this is something that you're um familiar with on the nose if you're a mathematician this is what you would think of as uh an instanton uh so instanton solutions to the uh

equations of motion of a quantum field theory are tunneling dynamics.

And so the idea becomes, okay, can we describe instantan solutions using Bayesian mechanics?

And the answer is, I don't know.

But it seems like

it should maybe be possible, because there is at least an understanding of where points A and B are, if not how to get from A to B. But that is something that would be interesting to do more work with.

And more generally, it would be interesting to think about quantum physics through the Bayesian mechanical lens, just because you do get this...

you do recover this classical limit of the path integral.

And morally, path integral quantization should be equivalent to things like deformation quantization and geometric quantization and other methods even.

And actually, one other one is worth saying.

There's a thing called stochastic quantization, which begins from real valued probabilities and then makes quantum stuff out of that.

the Bayesian mechanics as it is construed today is very close to stochastic quantization in a few very key senses.

So one supposes it should be possible.

That would be nice.

So that's the answer to one question, is should be, but I don't know.

It would be cool.

As far as entanglement and other kinds of information theory,

I actually don't know very much about this.

Quantum information theory is something that is relatively new, and a lot of people kind of in and adjacent to things like condensed matter theory are very interested in this.

But I am behind on the literature, I think.

One of the things that does kind of lend itself to information theory in general is the fact that Bayesian mechanics has these connections to Maxent and Bayesian inference.

And these are of course ways of talking about information and information theory So

So yes, my thought is something like entanglement, which has information theoretic undertones, should also be possible.

But again, I don't know initially what precisely that would look like.

There is some literature out there that has to do with the free energy principle in the context of information theory.

I am thinking of work due to Carl James Glazebrook and Chris Fields.

They've published a lot of work in this direction.

which is also something I'm not totally familiar with.

But again, it seems like a very rich area to eventually pursue connections in.


SPEAKER_00:
Great.

And then the second question was about indistinguishability of particles, FN Ali.


SPEAKER_03:
Ah, yes.


SPEAKER_00:
Okay.

Okay.


SPEAKER_03:
Yeah, so I suppose you're thinking of passing to these kinds of bosonic field theories where you just pack a whole bunch of equivalent field states into your Fox space and

see if you can make that make sense.

I think you may notice this when we get into the supersymmetry results, is there is a conceptualization that yes, things do change when you introduce bosonic-looking things as opposed to fermionic-looking things.

So it does become important to keep track of the statistics of these particles, because that recovers things like BRST charges relating the two genres of particle, and that is what gets the classical chaos in the last section.

So yes, there is a reason to distinguish between

the behavior of the particles in each theory.

I think the precise nature of that will become clearer once someone actually knuckles down and does out Bayesian quantum mechanics, where we'll be able to couple directly to that kind of idea.


SPEAKER_00:
Awesome.

A lot of fun topics.

The tunneling.

Do we have to visit every belief on the way from A to B?

Or, as it sometimes can seem, are we in one position and then we are in a different cognitive position?

All different kinds of experimenter, experimented questions in cognitive science, even potentially collective thought questions.

and ways of communication in terms of these formalisms.


SPEAKER_03:
Yeah, and the instanton question is pretty interesting as well, right?

Because it talks about, okay, if we have a system with multiple classical minima, so if we can say, okay, there are multiple optimal Bayesian beliefs, instantons are the things that zip between those minima.

So it's a qualitatively different way of thinking about dynamical systems.

It's not a single minima, and it's not a single classical trajectory.

It's multiple minima, and it's almost like phase transitions between the two.

Certainly that's interesting from the point of view of physics, excuse me, from physics, but tracing it back to something like cognitive science, thinking about, okay, what are the sort of phase transitions between our beliefs and can we actually access other phases, right?

Do those instant and solutions actually exist?

And do they describe some kind of zipping between beliefs

maybe generative models or perceptory regimes or something of that nature.

It's very kind of nebulous, but it's a provocative question.

Excellent.


SPEAKER_01:
Ali?

Well, I was on the understanding that as we go from classical to statistical and ultimately to quantum mechanics, the role of information and uncertainty in

describing and formulating the dynamics and mechanics of the physical systems becomes more explicit.

So is it a correct assumption to say that Bayesian mechanics somehow aims to put information back in the game and make it equally explicit through all of those mechanics?


SPEAKER_03:
Yeah, absolutely.

I think that's a good way of thinking about it.

The reason why Bayesian mechanics is so general, much like statistical mechanics is very general, is because it's just about information.

And lots of things have information, even at the classical level.

We just usually neglect it because we don't need to think about it.

But it becomes very apparent...

and this is part of the explanatory power of doing something like that, it becomes very apparent that classical stuff comes about when that information goes away or rather when those probabilities, that uncertainty goes away, that information becomes

sort of limitingly large or infinite.

So what Bayesian mechanics allows you to do is actually keep track of how does information change between these different scales, which is one particular payoff that I think makes it very valuable.

but not just bridging scales, also applying to many different scales, because lots of things do have information.

Information is just a mathematical apparatus that we can use to describe lots of things.

So if that's all that your physics is built on, then it will be, by consequence, something very general.


SPEAKER_00:
Awesome.

The way that it's...

coming to me today is in classical mechanics we get uh especially once we understand these limits which classical mechanics is like a point within a broader space of physics or mechanics

we get principles of least action and stationarity and laws of large numbers not unlike the central limit theorem in statistics and a lot of the esoteric gaussian statistics in statistical mechanics the probabilistic nature becomes formal

For example, in the billiards case in classical mechanics, you might want to do statistics across many slightly similar billiards games.

And so at that point, you could either do the plug and chug frequentist statistics, or at some point, you're going to want to think a little bit more broadly about the distribution of billiards.

And then quantum, especially,

makes the relationality and the modeler modeled dialectic formal because just by doing statistics you're already in an area of explicit relationality map territory distinguishing and then quantum takes that to another level and could it be said that with bayesian we're even going a layer deeper


SPEAKER_03:
into the modeler's approach to the system where do you see this yeah i think so i think so um the idea would be eventually we introduce an actual idea of information um which naturally kind of depends on the observer or the modeler

because information is kind of modeler dependent, right?

Information has a context.

It has, or is contingent on observation.

So when we start talking about surprisal and the dynamics of beliefs and inference, all of these are very explicitly putting the modeler back in the situation.

And that may be in the very metaphorical sense of a system,

synchronizing with its environment.

It doesn't necessarily have to be a cognitive observer or a panpsychist kind of modeler.

I don't really think there's remit for that within the maths.

But the point is that the maths works with or without those assumptions.

So in a sense, it's actually agnostic to that.

And you can conjure an image of a modeler that is pure metaphor, just talking about synchronization.

However, the language is pretty instructive.

And it does relate back to the idea that lots of systems have and can capture information, maybe in a very trivial, not...

cognitive, not conscious-like sense, but nonetheless, they do capture and synchronize to information, and therefore they do model and they do inference.


SPEAKER_00:
Great.

It's the classical, if a tree falls in the forest, does anyone hear it?

Exactly.

If a tree falls, does it make a noise?


SPEAKER_03:
Yeah, I mean, I think the answer would be...

It depends on what's synchronizing to the tree, right?

If the tree falls on, okay, this is going to be a pretty grim example, I guess, but if a tree falls on some sort of small woodland critter, then yes, I think the tree definitely fell because the statistics of one thing synchronizing to the other one would say, yeah, pretty unequivocally.

the tree just fell.

But if you're not there, you're not synchronizing to the tree.

And so that information has no meaning to you whatsoever.

So it is very much about, as you've written, modeler-dependent information in a very mathematically rigorous way.


SPEAKER_00:
really powerful and also connects to a lot of qualitative and contemporary ideas.

So it's really exciting to think about where and how Bayesian mechanics can be applied.

This brings us as we head into the z-score of plus one downhill on the Gaussian of the dot one into the paper itself.

So we'll pause, Ali and Jakob, if you have any questions.

Otherwise, we're going to go to the roadmap and have some overview notes on the paper and its structure.

Does that sound good?

All right.

so pardon the meme but um here's the road map of the paper the sections and subsections so maybe just a general comment what did you include in the paper what did you not include in the paper why did you structure it how you structured it


SPEAKER_03:
Yeah, absolutely.

So the structure was kind of meant to reflect, I think, an unmet need in the literature, which is both

a single place for all of these results to be reviewed.

And then once that review has been done, an application of all those results.

So organize everything and then use it in a kind of insightful way.

And having all the rigorous stuff in the beginning makes the application make sense.

And having the application makes or provides context for all the very rigorous stuff at the beginning.

So to do that, it's kind of split almost down the middle.

You have the sections one through four, all the foundational stuff, and then sections five, six, and seven.

are the actual applications.

So I began with a very brief introduction, just motivating precisely this.

Here are the questions that we're going to answer.

Here's how we're going to answer them.

Here's why it even matters.

In section two, I just very briefly review classical physics to set the stage for some of the stuff that we've already discussed.

then I talk about Bayesian mechanics itself.

And 2B through 2D are fairly long because this sets up all the basic machinery.

We begin with, I believe, to begin with, we talk about what does it mean for Bayesian mechanics to be like classical mechanics?

What are the basics of it?

What is variational free energy?

What is surprisal?

What is least surprisal?

And then in C and D, these basic ideas can be used to figure out, OK, what does it mean then to minimize surprisal?

How does that matter to Bayesian inference, physics of beliefs?

Physics by beliefs is then inverting the story and talking about, OK,

So we know now that systems, when they minimize their free energy or minimize their surprisal, they synchronize to their environment.

So you can write this nice story about parameters.

Can you do it the other way around?

Can you begin from the...

Now, I think I gave that in the wrong direction.

So when systems synchronize, they minimize their free energy.

So you can read this as Bayesian inference.

So then I ask, OK, can you do this the other way around?

Can you begin from Bayesian inference?

And can you get the dynamics of parameters?

Which is just kind of flipping the story, but it involves...

a bit of work to actually make it follow through.

In the end, however, it does.

And the relationship between the two, so I say physics of beliefs, because one is taking the motion of parameters and relating it to the motion on statistical manifolds, Bayesian inference.

Physics by beliefs is saying, OK, if we begin with some information theoretic idea of Bayesian inference, do we get back the mechanics of systems carrying those beliefs?

That relies on some work that I did earlier this year, which espoused a kind of duality between

the free energy principle and maximum entropy.

So using that as a kind of technology, you can flip the story and say, okay, if I start from an inferential principle, can I get physical stuff back?

Whereas the FEP is, I would think of it more as if I start with physical stuff, can I then understand the beliefs?

They are two sides of the same coin.

And

On the nose, it's a very trivial symmetry between the two.

Of course, it takes a lot more work to make it formally make sense, but in the end it does.

So that's section two.

Section three moves us into the first idea of actual results in the paper is to say, okay, so if I have a classical system doing Bayesian inference or approximate Bayesian inference, what does that look like?

And does it look like the free energy principle?

And then conversely, if I have an inferential principle or if I have an informational physics, can I use MaxEnt as the dual to the FEP to get back the physical mechanics of this classical object?

And in both cases, the answer is yes.

And at the end of that section, I give an equation that does precisely that.

It takes the idea of mode matching

and says, okay, here is how probabilities behave when you do mode matching.

And conversely, here is how modes behave when you minimize the free energy of those probabilities.

So in Section 4, I then talk about, okay, does this actually make sense in a physical setting?

Does it tell us anything insightful?

Or is it just a tautology that if you build in a classical mode and then take the limit to the mode, then you get classical physics?

If you phrase it just like that, it doesn't seem very remarkable.

But for the reasons that I described about...

path integrals and the classical limit of the path integral, it becomes a little bit more subtle.

And so we can actually derive something kind of interesting from the idea that classical stuff is the mode of quantum stuff.

So in the classical limit of quantum mechanics, systems that do approximate Bayesian inference exhibit classical physics.

I think that's really the payoff of that section.

And so it kind of justifies the rest of the set of questions that I ask, because it becomes not just a worked example, but it does say something interesting about physics more broadly, I think.

Now we get into the application section.

So once that's done, we cool off a little bit and we just do an example of what does it mean to do mode matching.

This is simple enough to formulate.

And you can see in this case, the mode, so mode matching is about stationary modes.

And it makes sense to talk about the classical physics of something at rest.

Now things that are at rest,

don't move.

So synchronization, in this case, is matching a mode to a stationary environment.

So when a force is not acting on you, you don't move.

And this requires very little maths.

And at the end, one of Newton's other laws pops out, that objects in motion stay in motion, objects at rest stay at rest.

So you get that back from Section 5.

From Section 6, you can talk about modes that move.

Well, you can talk about systems that move.

And there's actually a finer distinction in there that I precipitated just saying, which is in one case, you can talk about...

a system that moves towards a mode, so a system that tracks a mode, but eventually finishes in a stationary place.

And you can also talk about a system that chases a mode.

So we call this infinite mode tracking, or

You could think of this as mode chasing.

You could think of this as a primitive form of path tracking.

It's just the idea that things that go around in a circle, you can look at it one of two ways.

You can look at it as an object.

And the example I use is satellite motion, so the classical physics of celestial bodies, say, one thing orbiting around another.

You can either talk about this as a mode that is being tracked, that is constantly in motion, so you get infinite mode tracking, or you can talk about this as path tracking, where

the path is the circular path around the object.

And what we track is just that path.

And that is what is in 7a.

So I say in 6b, you can talk about this example as infinite mode tracking, but it's kind of ugly.

It doesn't work very well.

So this justifies going to path tracking instead, which is a much more elegant picture.

Conceptually, it makes a lot more sense.

And one of the problems specifically with 6b is that the idea of minimizing the surprisal of a mode becomes kind of difficult because this system is non-stationary, the mode is constantly moving, and so the surprisal is constantly fluctuating.

But when you talk about the surprisal over paths, it's much easier.

You can say, okay, we are actually genuinely minimizing the surprisal of paths because we are picking the path of least surprisal.

one thing that I didn't include

in this section, which is possibly interesting.

And I don't know, one of the nice things about preprints is they are, to some extent, living documents.

So I could come back and add this in if I was so inclined.

In fact, I might.

It's the idea of moving centers of mass.

So path tracking combined with some kind of moving frame of reference.

The reason why I didn't include this is because the maths for that is not quite worked out yet.

But since then...

Lance da Costa and others in a paper that is probably about to be released.

It's co-authored between myself, Lance da Costa, Carl Friston, Thomas Parr, Connor Hines, and Greg Pavliatis.

So all of the big mathematical minds behind the last few years of the FEP.

I think I've named all the names.

If I forgot anyone, then you'll have to excuse me.

There's some work in that paper that's about to be pre-printed that formulates path tracking in the context of moving frames of reference.

So not just moving modes along a path, but also moving paths themselves.

So one extra kind of layer of generalization is now you can talk about, okay, what about when the path that I'm tracking is itself moving?

And there is probably the machinery to handle that in this paper that's about to be released.

Again, it's Lance, it's Carl, it's Greg, Thomas, Connor, and myself.

So keep an eye out for that, because there will be something about moving frames of reference in that paper, and I may decide to go back and add something like, if I'm orbiting a body that is itself in motion, so if the center of mass that I'm orbiting is itself orbiting around something else, what does that look like from the perspective of the thing in orbit?

That's an interesting case.

It's something that I didn't include because it was still... I think there wasn't an elegant way forward with that.

Now there probably is.

But carrying on, 7b is now relating the idea of path trackings, the idea of a surprisal over paths.

relating that to the um path integral approach to uh basing mechanics and extracting some insights from what it means to be a path integral and one of the things that one is allowed to do in that context is to then develop a story about um classical chaos and i think uh

We have probably some slides later about that.

That deserves a much longer conversation.

But the idea of that section is to say, okay, so we've done all this arguably pretty trivial stuff.

Now, what about interesting systems like systems that exhibit chaos?

Can we actually describe those things using Bayesian mechanics?

And the answer is we can, and it comes out of the path integral description in the nice way that we have gestured at

with the name G-theory.


SPEAKER_00:
Nice.

Thank you.


SPEAKER_04:
Jakob?

I was waiting to ask this a bit later on, but since you mentioned reference frames, I'm wondering

what different sorts of reference frame Bayesian mechanics deals with, especially in connection to recovering classical mechanics from Bayesian mechanics.

The way I understand it right now, reference frame is basically imposed by the Markov blanket.

So when we're talking about the tree branch performing inference over its environment and being shaken by the incoming wind,

that's from the reference frame of the tree.

How would that be different if we considered our own reference frame looking at the tree?

Could that still have a formulation within Bayesian mechanics?

And also, how could this relate to multi-agent systems where we have the

classic example that comes to mind is clock synchronization, where pendulum clocks initially start out of phase, but over time go, again, in phase.

I think there's a lot of different ways to partition that system.

One clock is performing inference over the other, is performing inference over the other, but then maybe we can also treat it as a coupled system altogether, where we consider both at the same time.

So I'm thinking there might be some equivalent of like a non-inertial reference frame in Bayesian mechanics versus an inertial one given by the Markov blanket on that specific system.

So I would be interested to hear your thoughts.


SPEAKER_03:
Yeah, that's also a good question.

So one of the things that you can begin to talk about in this setting is that, to some extent, the precise placement of a Markov blanket is a bit arbitrary.

And this kind of flexibility is actually a good thing, because you do want to be able to nest blankets within blankets.

There's reason to believe that that kind of multi-scale

self-organization, well, that multi-scale patterning behavior, that multi-scale separation and being able to infer not only what my neighbors are doing, but being able to infer what the scale above me is doing.

There's reason to believe that that is where self-organization comes from under the free energy principle.

And one of the reasons to believe that is work by, for instance, Mike Levin in the context of the kind of cognitive nature of developing cells, right?

Cells that are in a kind of soup of things.

How do they know to develop into tissues?

There must be some kind of notion of nested blankets, not only knowing what am I in relation to others, but what are we in relation to a larger scale other.

So that's very important, nesting blankets.

Another really good example of this is in a paper by Maxwell, Alec Chance,

some other people that i don't remember offhand so i apologize uh called neural and phenotypic representations under the free energy principle and it's exactly the same sort of thing if you have a soup and they know something about themselves and also something about something at a higher level then uh you get self-organization so it's it's not just about um

kind of self-organization at the constituent level, but it's knowing something about the pattern that one is organizing into.

And these are two good examples of that.

There are lots of others.

So one imagines the arbitrariness of the Markov blanket is important because it allows you to nest things.

More generally, it allows you to change context.

And the context of information is kind of observer dependent.

In a way, you do want to be able to introduce kind of arbitrary frames and get different kinds of contexts for a given observation set.

You want to be able to reassign self and other and still get approximate Bayesian inference.

This is what we would think of as a gauge theory.

It's a symmetry at the level of the system

that is not reflected in the physics of the system, by which I mean, you can make free changes at the level of the system that don't change the physics of the system.

So in this case, it's the idea that we can change a reference frame, we can change the context of an inference, but it shouldn't change the idea that systems do approximate Bayesian inference, much in the same way that we can change the way we label coordinates in general relativity.

But it shouldn't change the idea that gravitation is the curvature of spacetime in some suitable sense that has to do with metric tensors that I won't talk too much about.

So something like that is absolutely, I think, not only kind of useful, but is absolutely essential to not only making Bayesian mechanics make sense.

It would be kind of suspicious if inference just fell apart if you redrew the boundaries.

especially given our conversation about the scale friendliness of the FEP.

It should apply and reapply, but also that application and reapplication and contextualizing inferences within inferences

is kind of right now what is reflected in the literature when we really think about, OK, how does self-organization work in these situations?

And how does individuation work in these situations?

Or how are patterns formed?

And I think there's lots of work in this direction about reference frames and the contextuality of information outside the FEP as well.

And I happen to be familiar with very specific literature about this in category theory, but I'm sure it's more general than that.

So you can talk about how does context shape an observation.

So yes, I think, yeah, on the subject of reference frames and synchronization, it's important to be able to redraw boundaries, to take new frames of reference and still get inference.

In the case of synchronizing pendulums, you can imagine, okay, maybe the pendulums do inference on each other, or maybe they do inference on the kind of

if they're on some sort of table or moving beam, maybe they do inference on the beam.

And they synchronize with the beam.

And therefore, they synchronize with each other.

That's one frame.

The other frame is one kind of looking at the rhythmic motions of the other one and just matching up like that.

In both cases, you get inference.

We probably prefer one reference frame or another.

but both are in principle mathematically valid, even if they tell kind of different physical stories about what's being synchronized to.

And the example of half-tracking in a moving frame of reference is interesting as well, because if you think about something like the Moon orbiting the Earth,

you can zoom way out and you can look at it as the moon spiraling around the earth as the earth goes around the sun, but you can also pick a reference frame where the earth is stationary and the moon is just orbiting around the earth.

So this is the relativity of motion.

Or if you want to imagine sitting in a train, it looks like the outside is moving and you're stationary.

So that would be mode matching.

You're not moving.

But from the perspective of the outside, the train has just gone past pretty quickly.

And you're in the train, and so you're moving.

But from your perspective, you're not.

So the relativity of what is being matched to is an important consideration for those reasons.

Just to produce a consistent theory, we have to acknowledge that we can freely assign these inferences.

The important aspect is that we get the same inferences back, or at least we get inference.

They might not be the same inferences if you wrote them down.

They might be in different coordinates, so to speak.

But at the end of the day, you should still get Bayesian inference.

Now, I don't remember your second question, so you'll have to ask it again.


SPEAKER_04:
Yeah.

I'm actually not aware if there was a second question, but I do have a follow-up question to your answer.

Since you mentioned nested Markov blankets, I'm just wondering whether in the formalism of Bayesian mechanics, we can also admit a kind of dynamics on the

topological structure of the of the blanket itself because in the case of cells forming other layers uh intuitively it seems to me that it's a kind of discrete way of thinking about how we're adding an extra layer of the markov blanket and maybe the markov blanket initially admits uh some uh

some non-integer index connecting to your prior work on the weak Markov blankets, and then it evolves over time to form into actually statistically significant Markov blanket.

But perhaps there might be also a way for having just one Markov blanket that evolves through time, and that might also change the inferences.

So I'm wondering whether...

mechanics, whether the dynamics of particular blankets can also be taken into account in the path tracking formulation or any other one within approximate Bayesian inference.


SPEAKER_03:
Yeah, so there are a couple of interesting things there.

For one thing, the idea that perspective matters, the idea that you pick a reference frame and carve up your environment that way does kind of go back to this question of

How do we understand blankets forming in developing cells, say?

And the idea would be that blankets have characteristic scales.

The reason why we see something like a stone in nature and not a lattice of crystalline molecules is because at our level of observation,

The blankets that allow these molecules to be self-organized is completely meaningless.

But what is interesting to our level of observation, our kind of canonical reference frame, is the stone itself, the blanket around the stone itself.

It's the stone's blanket.

It's not the blankets that make the molecules that make the stone.

So, yeah, you know, eventually we would want a much more complete theory of individuation that covers not just dynamical blankets, but nested blankets and blankets with perspective and things of that nature.

The perspective-based idea is an interesting one to me because it allows you to talk about things like sparse coupling at certain levels.

And if we acknowledge that the Markov blanket is coming to a blanket store near you, yeah, exactly.

If we acknowledge that the blanket is somewhat arbitrary, then we're free to say that

Any system with enough couplings has a blanket at some scale because the kind of multi-scale systems in physics that we're interested in at some scale are sparsely coupled.

Probably not every scale.

And certainly within a blanket there's not sparse coupling, given a particular reference frame.

Because given the reference frame of myself looking at the stone, looking at the stone's blanket, within that blanket

I would want to see the stone's molecules cohere in a way that is not sparsely coupled.

Otherwise, I wouldn't see things.

But then at the level of the stone, it is sparsely coupled.

So this kind of arbitrariness allows us to actually do the carving up in a very interesting way.

As for dynamical systems perspectives on Markov blankets, I think this is also possible.

I am working on something right now that takes the whole story about physical boundaries

and conditional independence and writes it in terms of random variables.

So you have a whole suite of possible blanket states.

And you can look at blankets kind of flickering on and off or blankets that occupy different states over time.

This is not nested yet, but it's one step towards that direction where you can think about how the blanket at a higher level turns on or off, given the dynamics of its constituents, whether they're cohering or not.

So yeah, maybe that answers your question.


SPEAKER_00:
this is a great topic to chill on for a few more minutes in our final bit of the dot one there's really a lot here i'll just give a few dots and then ali happy to hear your perspective as well um you even discussed it as scale friendly but it's almost like a one and a half layer model

And that is actually preceded by even simple Bayesian hidden state observed variable distinctions, expectation maximization, just partially observable Bayesian models in general.

under an interpretation where observables are happening at a given scale of a system of interest and a hidden phenomena or a hidden cause is at a higher level of organization connects the sort of multi-scale systems approach

with a self-organization approach.

And we've talked many times about that lateral and the vertical.

There's the nest mates interacting laterally, and then there's the real or as if colony level organization.

Another point I think it's really interesting to pull out and explore is you discussed how important it is to be able to redraw blankets and still get inference.

it made me feel that there has to be some kind of a locally distributed, local sense-making process such that potentially a vast range of partitionings can also remain valid.

can think of different metaphors there and uh that potentially the blanket provides exactly that partitioning surprise surprise and then moving towards all these features that had been in the uh spectator seats at the math conference

dynamical nested multi-perspective systems and the way that some of these formalisms are pointing towards an integration that's quite far out in some ways, yet also may bring an intuitiveness to a lot of the ways of talking about different physical systems, including the physics of cognition.

And as far as nature at the joints,

perspectives would be one not the only one but one of those joints or sparse connectivities or factorizabilities various ways of thinking about it but multiple entities engaging in a multi-perspectival multi-scale inference question it's like two people looking at the boat they might disagree on how to partition different parts of the boat or the boat from the water

but at least their perspectives and their communication interface with each other and their observation inference interfaces with the boat could be specified.

And so then it could be like my map of me, my map of you, my map of the boat, your map of you, your map of me, your map of the boat.

And then there's a conversation with all six of those features

instead of us only looking at the boat and you did it one way, I did it differently.

And that's the end of the discussion.

So I think it's going to open up degrees of freedom in perspective, harmonizing that wouldn't be classically understood.

And that'll be seen as extremely non-linear and unconventional behaviors.

And it just remains to be seen what that is.


SPEAKER_03:
I agree.

I'll take this opportunity to point out there are other people in the Versys lab who are thinking about that kind of question.

How does collaboration and information synthesis and kind of

agreement or goal emergence?

How do all these things fit into Bayesian mechanics?

So people like Maxwell and Maholt are thinking about, if I write down

know these kinds of problems uh is there a way to talk about agreement as it emerges through um you know collective discussions or collaboration and it is likely something um like this it's agreeing on a reference frame for our collective observations and and agreeing on you know not just sense making but it's agreeing on how to make sense of something

So there's a kind of two-step process.

I learn something, figure it out.

I tell you.

You have your own picture of what this thing looks like.

We agree on a picture that serves us best.

We agree on a best reference for it.


SPEAKER_00:
Yes.

Agreeing on the sense made is deciding what to order after the dish has been cooked.

Exactly.

We need to have a conversation about the recipe and so many other things.

Ali?


SPEAKER_01:
Well, I believe this whole argument around Markov blankets and especially the nested Markov blanket or the dynamic nature of them

can potentially be connected to somehow the opposing stances, one of which is the preformism or preformationism.

The other one is the progressive differentiation mechanism, which

In the preformism, some people believe that the topology of the whole state-space manifold is completely preformed.

We just don't have enough data to model that accurately, model the state-space accurately.

But for the advocates of the progressive differentiation model,

viewpoint, they, on the other hand, believe that it literally goes, I mean, the state space manifold literally goes, undergoes lots of transformations, according to the

I mean, some inflection points or some singularities, and it's not preformed at all.

And the dynamic nature of the state-space manifold is something inherent to that.

It's not because of our lack of enough data to model that.

So I'm not sure how much you agree with that viewpoint, but yeah, the...


SPEAKER_03:
something that's... Yeah, I think, sure, I think the kind of drawing and redrawing of Markov blankets is probably a point in support of the idea of progressive differentiation because at one level we think of drawing and redrawing as development or

Well, yes, development.

And one of the things that drives development is something like fitness pressure.

So fitness changes the constraints on what sort of patterns are available to you, what ought you organize into.

And these are not things that we know a priori at the level of the system.

It may be possible that if you write down a whole kind of a world model of the entire universe from

you know, the very first point in time

after the Big Bang, or maybe even at the instant at which the singularity expanded, and you had enough computational power to trace it all through, that you could get to something like, what is it, Preformationism?

But that strikes me quite a lot as a kind of super determinism or determinism argument.

It sounds a lot like some kind of...

argument that, you know, there is no uncertainty in how you will develop.

There is just uncertainty in what you're aware of as to your development.

or some kind of hidden variables theory that says there is something mediating the way your state space looks, you just don't know it.

And so you have the illusion of free will.

So this becomes a bit of a philosophical question.

I myself don't really buy into determinism.

So I would be tempted to say that

The Markov blanket story, especially dynamic blankets or however you want to call them, are to do with progressive differentiation.

As we live our lives, we draw and redraw our own blankets as we mesh and individuate ourselves from other systems and the environment.

And this is something that we are in control of.

Certainly locally in the state space, we're in control of it.

Whether there is some hidden variable guiding our own actions is something that is...

I think, strictly speaking, a different question.

The nice thing about it is, whether it's true or not, it doesn't break the locality of the Markov blanket construction.

So we get this idea of locally progressive differentiation as a Markov blanket.

And then when we zoom way out, it may be kind of a different story.

But certainly, yeah, locally, it works as this kind of dynamic process.


SPEAKER_00:
Well, we almost got to the paper.

This was an awesome discussion.

So, Ali and Jakob, if you would like to have any closing remarks.


SPEAKER_01:
Well, I just want to say that

I truly enjoyed our discussion today, and I very much look forward to that too, because I have a bunch of questions that I didn't have a chance to ask about some of the sections of the paper.

So I just wanted to thank Dalton for his time and

Also for Jakob and Daniel for organizing these extremely fruitful and useful discussion sessions and live streams.

So yeah, I'm very much looking forward to the next one.


SPEAKER_00:
Thank you, Jakob.


SPEAKER_04:
Yeah, this was an amazing .1.

Thanks a lot, Dalton, for all the great answers and overviews.

Of course, Ali and Daniel for doing bulk of the work on the slides.

And yeah, looking forward to the .2.

Dalton, any last thoughts?


SPEAKER_03:
No, just thank you for having me.

I did really enjoy it.

And thank you, of course, for all of the background work that you put in.

It's, as I said, it's very flattering to see such detailed working through of my work.

So thank you for that.

And I am looking forward to the next episode.


SPEAKER_00:
Excellent.

Well, next week, us and anyone else who'd like to join, just get in touch or submit any questions or topics.

So thank you, fellows.

See you later.


SPEAKER_02:
Okay.

Cheers.
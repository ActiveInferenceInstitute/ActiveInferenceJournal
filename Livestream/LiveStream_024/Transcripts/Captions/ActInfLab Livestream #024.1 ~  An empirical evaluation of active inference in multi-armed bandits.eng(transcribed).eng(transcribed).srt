1
00:00:08,320 --> 00:00:09,920
hello everyone

2
00:00:09,920 --> 00:00:12,480
thanks for joining and welcome to actin

3
00:00:12,480 --> 00:00:12,960
flab

4
00:00:12,960 --> 00:00:16,320
live stream number 24.1 today it's june

5
00:00:16,320 --> 00:00:20,400
22 2021 and we're going to be discussing

6
00:00:20,400 --> 00:00:22,560
the paper and empirical evaluation of

7
00:00:22,560 --> 00:00:24,960
active inference in multi-armed bandits

8
00:00:24,960 --> 00:00:26,960
with several of the authors so thanks

9
00:00:26,960 --> 00:00:29,439
everyone for joining

10
00:00:29,439 --> 00:00:32,000
welcome to the active inference lab we

11
00:00:32,000 --> 00:00:33,360
are a participatory

12
00:00:33,360 --> 00:00:35,440
online lab that is communicating

13
00:00:35,440 --> 00:00:37,520
learning and practicing applied active

14
00:00:37,520 --> 00:00:38,160
inference

15
00:00:38,160 --> 00:00:40,559
you can find us at the links here on

16
00:00:40,559 --> 00:00:42,640
this screen

17
00:00:42,640 --> 00:00:44,480
this is recorded in an archived live

18
00:00:44,480 --> 00:00:45,920
stream so please provide us with

19
00:00:45,920 --> 00:00:46,480
feedback

20
00:00:46,480 --> 00:00:49,280
so that we can improve on our work all

21
00:00:49,280 --> 00:00:50,879
backgrounds and perspectives

22
00:00:50,879 --> 00:00:53,199
are welcome here and we'll be following

23
00:00:53,199 --> 00:00:56,320
good video etiquette for live streams

24
00:00:56,320 --> 00:00:59,199
this short link has a schedule of all

25
00:00:59,199 --> 00:01:01,120
the live streams that we've been doing

26
00:01:01,120 --> 00:01:03,440
and will do for 2021

27
00:01:03,440 --> 00:01:06,159
and we're here today on june 22nd in

28
00:01:06,159 --> 00:01:07,840
number 24.1

29
00:01:07,840 --> 00:01:10,159
which is the middle of the three-part

30
00:01:10,159 --> 00:01:12,240
series on this paper about multi-armed

31
00:01:12,240 --> 00:01:14,080
bandits

32
00:01:14,080 --> 00:01:17,520
the zero video 24.0 had

33
00:01:17,520 --> 00:01:19,840
some context and some background on this

34
00:01:19,840 --> 00:01:21,040
paper that we're going to be

35
00:01:21,040 --> 00:01:23,520
discussing an empirical evaluation of

36
00:01:23,520 --> 00:01:25,200
active inference in multi-armed

37
00:01:25,200 --> 00:01:28,320
bandits and today we're here with

38
00:01:28,320 --> 00:01:30,799
three of the authors so thanks so much

39
00:01:30,799 --> 00:01:32,400
for all of you who are joining today

40
00:01:32,400 --> 00:01:34,079
because we'll have a lot to discuss and

41
00:01:34,079 --> 00:01:36,720
learn about and

42
00:01:36,720 --> 00:01:40,079
in today's discussion for 24.1

43
00:01:40,079 --> 00:01:41,920
we're gonna first just go for some

44
00:01:41,920 --> 00:01:43,920
introductions and then we're going to

45
00:01:43,920 --> 00:01:44,240
have

46
00:01:44,240 --> 00:01:46,720
a presentation by the author and then

47
00:01:46,720 --> 00:01:48,320
we're going to just open it up for

48
00:01:48,320 --> 00:01:49,759
discussion so if you're

49
00:01:49,759 --> 00:01:52,240
here on the video call or if you're

50
00:01:52,240 --> 00:01:53,200
watching

51
00:01:53,200 --> 00:01:55,360
live in the live chat just feel free to

52
00:01:55,360 --> 00:01:56,719
ask any question

53
00:01:56,719 --> 00:01:58,960
and we can go wherever people are

54
00:01:58,960 --> 00:01:59,840
interested

55
00:01:59,840 --> 00:02:03,119
in going so that being said here we are

56
00:02:03,119 --> 00:02:04,479
in the introductions

57
00:02:04,479 --> 00:02:06,479
we'll just go around and introduce

58
00:02:06,479 --> 00:02:07,600
ourselves and

59
00:02:07,600 --> 00:02:09,520
say hello and then especially for the

60
00:02:09,520 --> 00:02:11,520
authors who are joining for the first

61
00:02:11,520 --> 00:02:12,160
time

62
00:02:12,160 --> 00:02:15,280
it'd be awesome to hear anything you

63
00:02:15,280 --> 00:02:15,760
want to

64
00:02:15,760 --> 00:02:18,640
say or we'll ask you questions as well

65
00:02:18,640 --> 00:02:19,200
so

66
00:02:19,200 --> 00:02:21,599
i'm daniel i'm a postdoctoral researcher

67
00:02:21,599 --> 00:02:23,360
in california and i'll pass to

68
00:02:23,360 --> 00:02:26,400
dave i'm

69
00:02:26,400 --> 00:02:29,920
in the tropical mountainous rainforest

70
00:02:29,920 --> 00:02:33,040
120 miles north of manila

71
00:02:33,040 --> 00:02:35,760
my background is in cybernetic learning

72
00:02:35,760 --> 00:02:37,360
theory

73
00:02:37,360 --> 00:02:40,319
general psychology and machine

74
00:02:40,319 --> 00:02:42,000
translation

75
00:02:42,000 --> 00:02:44,480
but not much math so i'm floundering

76
00:02:44,480 --> 00:02:48,480
with much of the active inference

77
00:02:48,840 --> 00:02:51,360
world we'll go to sarah

78
00:02:51,360 --> 00:02:54,640
and then continue on

79
00:02:54,640 --> 00:02:56,720
i'm sarah i'm a postdoc in treeson

80
00:02:56,720 --> 00:02:58,239
together with tim t

81
00:02:58,239 --> 00:03:00,560
my original background is physics and

82
00:03:00,560 --> 00:03:01,519
biophysics

83
00:03:01,519 --> 00:03:03,599
but i wrote my dissertation essentially

84
00:03:03,599 --> 00:03:05,519
about active inference

85
00:03:05,519 --> 00:03:09,120
and especially applied to habit learning

86
00:03:09,120 --> 00:03:13,840
and yeah that's it for me

87
00:03:14,080 --> 00:03:17,280
and going on to dimitri

88
00:03:17,280 --> 00:03:19,840
should we let her introduce himself i

89
00:03:19,840 --> 00:03:21,200
don't know i'm not sure if he wants to

90
00:03:21,200 --> 00:03:23,200
do that

91
00:03:23,200 --> 00:03:25,519
okay i mean i can also introduce him

92
00:03:25,519 --> 00:03:26,640
shortly so

93
00:03:26,640 --> 00:03:29,840
there was our colleague from ucl

94
00:03:29,840 --> 00:03:32,319
previously so he worked in max planck

95
00:03:32,319 --> 00:03:34,480
ucl center for computational psychiatry

96
00:03:34,480 --> 00:03:36,319
and aging research and currently he has

97
00:03:36,319 --> 00:03:39,040
switched to industry so he is working in

98
00:03:39,040 --> 00:03:40,000
second mind

99
00:03:40,000 --> 00:03:42,640
uh on the applied reinforcement learning

100
00:03:42,640 --> 00:03:43,680
there so

101
00:03:43,680 --> 00:03:47,599
basically he is a expert on this other

102
00:03:47,599 --> 00:03:49,519
part of staffordshire

103
00:03:49,519 --> 00:03:51,120
which doesn't cover active inference

104
00:03:51,120 --> 00:03:53,360
right

105
00:03:53,519 --> 00:03:56,959
so and uh yeah i'm also the postdoc

106
00:03:56,959 --> 00:03:59,760
technical university of dresden uh both

107
00:03:59,760 --> 00:04:01,920
me and sarah are at the chair for neuro

108
00:04:01,920 --> 00:04:03,200
imaging

109
00:04:03,200 --> 00:04:05,760
uh where like the head of the chair is

110
00:04:05,760 --> 00:04:07,120
stefan kiebel

111
00:04:07,120 --> 00:04:11,360
uh and and uh yeah we have been involved

112
00:04:11,360 --> 00:04:13,920
with active inference for a while now

113
00:04:13,920 --> 00:04:16,399
since probably 2015-16

114
00:04:16,399 --> 00:04:19,759
and have been applying it to various

115
00:04:19,759 --> 00:04:21,600
questions regarding human behavior

116
00:04:21,600 --> 00:04:23,840
cognitive control decision making in

117
00:04:23,840 --> 00:04:26,960
dynamic environments and similar

118
00:04:26,960 --> 00:04:32,080
um and yes so um

119
00:04:32,080 --> 00:04:33,680
should they kind of switch to the slides

120
00:04:33,680 --> 00:04:35,120
now or

121
00:04:35,120 --> 00:04:36,800
sure we can go to the students or if i

122
00:04:36,800 --> 00:04:38,400
can just ask one

123
00:04:38,400 --> 00:04:40,400
general question to any of the authors

124
00:04:40,400 --> 00:04:41,680
who wanted to respond

125
00:04:41,680 --> 00:04:44,080
was it that you were interested in

126
00:04:44,080 --> 00:04:45,520
active inference and then

127
00:04:45,520 --> 00:04:48,560
looking for domains to apply it in

128
00:04:48,560 --> 00:04:50,720
or were you interested in a domain and

129
00:04:50,720 --> 00:04:51,759
then sort of found

130
00:04:51,759 --> 00:04:53,759
active inference as a way to integrate

131
00:04:53,759 --> 00:04:57,840
what you were working on

132
00:04:58,400 --> 00:05:01,280
i yeah well i mean so my background is

133
00:05:01,280 --> 00:05:02,560
also like right in physics

134
00:05:02,560 --> 00:05:05,840
complex systems uh and computational

135
00:05:05,840 --> 00:05:07,759
neuroscience and this physicist we like

136
00:05:07,759 --> 00:05:08,479
to

137
00:05:08,479 --> 00:05:11,680
think about this unifying theories so

138
00:05:11,680 --> 00:05:13,360
in this sense active inference has this

139
00:05:13,360 --> 00:05:15,759
appeal that it can collect

140
00:05:15,759 --> 00:05:18,720
connect a very kind of distinct way of

141
00:05:18,720 --> 00:05:22,560
basically thinking about decision making

142
00:05:22,840 --> 00:05:25,600
thermodynamics um right uh stochastics

143
00:05:25,600 --> 00:05:26,400
dynamics and

144
00:05:26,400 --> 00:05:29,759
uh very kind of different areas of

145
00:05:29,759 --> 00:05:31,919
of understanding dynamical systems so to

146
00:05:31,919 --> 00:05:33,199
say

147
00:05:33,199 --> 00:05:36,639
uh and kind of in control so that

148
00:05:36,639 --> 00:05:41,280
that would be the appeal and uh right uh

149
00:05:41,440 --> 00:05:45,440
i mean definitely uh as a tool to apply

150
00:05:45,440 --> 00:05:47,280
like to decision making human

151
00:05:47,280 --> 00:05:49,199
understanding human behavior

152
00:05:49,199 --> 00:05:51,199
this is kind of where this all started

153
00:05:51,199 --> 00:05:53,680
right

154
00:05:53,680 --> 00:05:56,960
so what new can we learn basically

155
00:05:56,960 --> 00:06:01,680
uh using this approach

156
00:06:02,720 --> 00:06:06,000
and sarah any thoughts about especially

157
00:06:06,000 --> 00:06:08,560
i'm curious about the biology side

158
00:06:08,560 --> 00:06:09,840
because we hear a lot about the

159
00:06:09,840 --> 00:06:12,240
math and the physics verging towards

160
00:06:12,240 --> 00:06:14,000
active inference but it's also cool to

161
00:06:14,000 --> 00:06:15,680
hear about biology and that was

162
00:06:15,680 --> 00:06:19,440
my background as well my background is

163
00:06:19,440 --> 00:06:20,880
rather in biophysics

164
00:06:20,880 --> 00:06:24,960
so i'm only tangentially in biology

165
00:06:24,960 --> 00:06:28,080
but what i like is also that it connects

166
00:06:28,080 --> 00:06:30,479
with some general information processing

167
00:06:30,479 --> 00:06:32,080
scheme in the brain

168
00:06:32,080 --> 00:06:34,400
and actually my masters see this i was

169
00:06:34,400 --> 00:06:35,840
working way way down

170
00:06:35,840 --> 00:06:39,520
in the abstraction hierarchy on spiking

171
00:06:39,520 --> 00:06:41,360
neural networks and like receptor

172
00:06:41,360 --> 00:06:42,720
dynamics

173
00:06:42,720 --> 00:06:47,120
and actually in this area i find it easy

174
00:06:47,120 --> 00:06:50,479
to not see the forest for the trees

175
00:06:50,479 --> 00:06:53,520
and i think actually connecting upwards

176
00:06:53,520 --> 00:06:55,440
and asks the question what is a general

177
00:06:55,440 --> 00:06:58,080
information processing that's going on

178
00:06:58,080 --> 00:07:00,319
and then connecting it back downwards

179
00:07:00,319 --> 00:07:02,160
again is what also

180
00:07:02,160 --> 00:07:05,280
attracted me to active influence

181
00:07:05,280 --> 00:07:09,039
awesome so as usual people will be uh

182
00:07:09,039 --> 00:07:11,840
i guess joining or leaving i'll unshare

183
00:07:11,840 --> 00:07:12,160
this

184
00:07:12,160 --> 00:07:16,000
screen and dimitri if you want to uh

185
00:07:16,000 --> 00:07:18,319
jump into the presentation that'd be

186
00:07:18,319 --> 00:07:19,599
awesome

187
00:07:19,599 --> 00:07:21,840
yes

188
00:07:26,000 --> 00:07:30,160
okay so you see yeah

189
00:07:30,160 --> 00:07:33,680
and uh all crop everything so go for it

190
00:07:33,680 --> 00:07:37,199
excellent okay so

191
00:07:37,199 --> 00:07:40,639
let me start with a bit kind of

192
00:07:40,639 --> 00:07:44,479
motivation for this work uh

193
00:07:44,479 --> 00:07:47,120
so as well you realize kind of our

194
00:07:47,120 --> 00:07:48,560
involvement with active inference

195
00:07:48,560 --> 00:07:50,960
came from this kind of cognitive

196
00:07:50,960 --> 00:07:51,840
neuroscience

197
00:07:51,840 --> 00:07:55,039
uh direction uh

198
00:07:55,039 --> 00:07:58,400
and originally not so interested in the

199
00:07:58,400 --> 00:08:00,479
technicals or machine learning side of

200
00:08:00,479 --> 00:08:01,680
it

201
00:08:01,680 --> 00:08:05,039
uh however if one thinks about uh

202
00:08:05,039 --> 00:08:07,599
multi-arm problem bandits right that's a

203
00:08:07,599 --> 00:08:08,800
very general uh

204
00:08:08,800 --> 00:08:10,800
problem which kind of generalizes

205
00:08:10,800 --> 00:08:13,680
resource allocation problems

206
00:08:13,680 --> 00:08:16,160
then when they realize that this is most

207
00:08:16,160 --> 00:08:18,080
of kind of behavioral

208
00:08:18,080 --> 00:08:20,160
experiments can be cast into this kind

209
00:08:20,160 --> 00:08:22,080
of framework right

210
00:08:22,080 --> 00:08:24,960
and once these this in the range of kind

211
00:08:24,960 --> 00:08:25,759
of uh

212
00:08:25,759 --> 00:08:27,520
extreme experimental cognitive

213
00:08:27,520 --> 00:08:29,039
neuroscience uh

214
00:08:29,039 --> 00:08:30,879
domains like decision-making in dynamic

215
00:08:30,879 --> 00:08:32,159
environments

216
00:08:32,159 --> 00:08:34,080
value-based decision-making structure

217
00:08:34,080 --> 00:08:36,799
learning and similar uh

218
00:08:36,799 --> 00:08:39,440
one can also think about like um

219
00:08:39,440 --> 00:08:40,240
attention

220
00:08:40,240 --> 00:08:42,640
as a resource allocation problem right

221
00:08:42,640 --> 00:08:43,519
so it's

222
00:08:43,519 --> 00:08:46,320
kind of there are many other domains

223
00:08:46,320 --> 00:08:47,120
where

224
00:08:47,120 --> 00:08:48,800
maybe they're not explicitly talking

225
00:08:48,800 --> 00:08:50,560
about multi-arm bandits

226
00:08:50,560 --> 00:08:53,920
as such but they can also be cast uh

227
00:08:53,920 --> 00:08:55,279
into this uh

228
00:08:55,279 --> 00:08:58,240
general framework and for example for

229
00:08:58,240 --> 00:08:59,680
from decision making dynamic

230
00:08:59,680 --> 00:09:01,839
environments one of the most

231
00:09:01,839 --> 00:09:05,519
well-known kind of tasks is a

232
00:09:05,519 --> 00:09:09,279
probably like reversal learning task

233
00:09:09,279 --> 00:09:13,120
right that's kind of used in uh

234
00:09:13,120 --> 00:09:16,880
hundreds of papers and this is also kind

235
00:09:16,880 --> 00:09:17,120
of

236
00:09:17,120 --> 00:09:20,000
what initially motivated me to make this

237
00:09:20,000 --> 00:09:20,560
paper

238
00:09:20,560 --> 00:09:23,519
uh it's a kind of just uh understanding

239
00:09:23,519 --> 00:09:25,920
so if i apply active inference to liver

240
00:09:25,920 --> 00:09:28,080
reversal learning tasks how does this

241
00:09:28,080 --> 00:09:29,680
compare to other

242
00:09:29,680 --> 00:09:32,720
like alternative decision making uh

243
00:09:32,720 --> 00:09:36,640
approaches we can apply to that right

244
00:09:36,640 --> 00:09:38,480
however right besides this kind of

245
00:09:38,480 --> 00:09:40,800
cognitive neuroscience uh

246
00:09:40,800 --> 00:09:44,480
uh direction uh multiple bandits have

247
00:09:44,480 --> 00:09:47,440
like a range of industrial applications

248
00:09:47,440 --> 00:09:49,360
uh they really use like

249
00:09:49,360 --> 00:09:51,120
you know starting from marketing to

250
00:09:51,120 --> 00:09:53,440
finance like recommendation systems in

251
00:09:53,440 --> 00:09:55,440
finance for trading applications

252
00:09:55,440 --> 00:09:59,040
uh i even in many like optimization

253
00:09:59,040 --> 00:10:00,080
problems in the

254
00:10:00,080 --> 00:10:02,800
like deep learning right you know there

255
00:10:02,800 --> 00:10:03,360
are lots of

256
00:10:03,360 --> 00:10:05,040
papers actually showing how you can

257
00:10:05,040 --> 00:10:06,560
speed up learning by

258
00:10:06,560 --> 00:10:09,839
um uh finding kind of better sets of

259
00:10:09,839 --> 00:10:11,360
examples for

260
00:10:11,360 --> 00:10:14,480
uh for the deep learning uh uh

261
00:10:14,480 --> 00:10:18,000
systems uh and right so basically the

262
00:10:18,000 --> 00:10:19,279
way this also kind of

263
00:10:19,279 --> 00:10:21,279
allows them active inference to bridge

264
00:10:21,279 --> 00:10:23,360
this into a machine learning gap and

265
00:10:23,360 --> 00:10:23,760
find

266
00:10:23,760 --> 00:10:27,120
new uh applications there if it's fine

267
00:10:27,120 --> 00:10:29,279
if it kind of shows to be useful for

268
00:10:29,279 --> 00:10:31,120
this kind of multi-armored setup

269
00:10:31,120 --> 00:10:33,279
as kind of adding something new to the

270
00:10:33,279 --> 00:10:35,440
existing work

271
00:10:35,440 --> 00:10:38,800
right and

272
00:10:38,800 --> 00:10:40,880
when we talk kind of multi-arm bandits

273
00:10:40,880 --> 00:10:42,000
have a

274
00:10:42,000 --> 00:10:45,519
range of different formulations

275
00:10:45,519 --> 00:10:47,360
today we will talk about stationary

276
00:10:47,360 --> 00:10:48,959
bandits and dynamic bandits

277
00:10:48,959 --> 00:10:52,000
uh where just means that uh reward

278
00:10:52,000 --> 00:10:54,079
probabilities either are fixed over time

279
00:10:54,079 --> 00:10:55,519
or change

280
00:10:55,519 --> 00:10:58,399
uh in different ways however people also

281
00:10:58,399 --> 00:11:00,000
write uh

282
00:11:00,000 --> 00:11:02,320
discuss adversarial bandits risk of air

283
00:11:02,320 --> 00:11:04,480
bandits uh contextual bandits

284
00:11:04,480 --> 00:11:06,320
one can also talk about non-markovian

285
00:11:06,320 --> 00:11:07,760
bandits or that

286
00:11:07,760 --> 00:11:10,320
kind of rewards uh depend on a sequence

287
00:11:10,320 --> 00:11:11,600
of actions or

288
00:11:11,600 --> 00:11:14,160
have kind of some memory dependence in

289
00:11:14,160 --> 00:11:15,120
the system

290
00:11:15,120 --> 00:11:16,959
and so on right they're really like a

291
00:11:16,959 --> 00:11:18,240
range of

292
00:11:18,240 --> 00:11:20,880
different uh kind of structural

293
00:11:20,880 --> 00:11:23,120
definitions of multi-arm bandits which

294
00:11:23,120 --> 00:11:26,000
kind of then require a different way of

295
00:11:26,000 --> 00:11:28,399
thinking about the problem

296
00:11:28,399 --> 00:11:30,000
uh and this is potentially like

297
00:11:30,000 --> 00:11:31,680
something uh also interesting for the

298
00:11:31,680 --> 00:11:32,399
future

299
00:11:32,399 --> 00:11:35,120
expanding uh what we did here uh into

300
00:11:35,120 --> 00:11:35,519
this

301
00:11:35,519 --> 00:11:38,560
other domains and seeing if again this

302
00:11:38,560 --> 00:11:40,839
the results are generalizable to other

303
00:11:40,839 --> 00:11:42,720
uh

304
00:11:42,720 --> 00:11:45,920
definitions of multi-unbending problems

305
00:11:45,920 --> 00:11:49,680
uh okay so just

306
00:11:49,680 --> 00:11:53,120
um how i structure the slide so far

307
00:11:53,120 --> 00:11:55,279
we have like two parts one isn't about

308
00:11:55,279 --> 00:11:57,360
stationary bandits

309
00:11:57,360 --> 00:11:59,440
and then we will switch which will go to

310
00:11:59,440 --> 00:12:00,480
switching bandits

311
00:12:00,480 --> 00:12:04,240
yeah uh and uh yeah given that we have

312
00:12:04,240 --> 00:12:07,120
two hours i hope that will be enough

313
00:12:07,120 --> 00:12:07,839
time but

314
00:12:07,839 --> 00:12:10,160
if yeah if not we can also continue for

315
00:12:10,160 --> 00:12:11,040
the next session

316
00:12:11,040 --> 00:12:12,959
for example with switching bandits or so

317
00:12:12,959 --> 00:12:16,880
let's see let's see how this goes

318
00:12:17,279 --> 00:12:21,279
um okay so what is stationary bandit so

319
00:12:21,279 --> 00:12:21,680
the

320
00:12:21,680 --> 00:12:23,440
definition of the problem is as follow

321
00:12:23,440 --> 00:12:25,040
right on any trial

322
00:12:25,040 --> 00:12:28,720
an agent makes a choice between k-pops

323
00:12:28,720 --> 00:12:32,160
uh choice outcomes are binary variables

324
00:12:32,160 --> 00:12:35,279
so we kind of focus here on bernoulli

325
00:12:35,279 --> 00:12:36,079
bandits

326
00:12:36,079 --> 00:12:38,000
outcomes can also be drawn from any

327
00:12:38,000 --> 00:12:39,680
other distribution so

328
00:12:39,680 --> 00:12:41,360
and then for example you talk about

329
00:12:41,360 --> 00:12:43,600
gaussian bandits or

330
00:12:43,600 --> 00:12:46,000
uh yeah depending on the under how the

331
00:12:46,000 --> 00:12:46,800
rewards are

332
00:12:46,800 --> 00:12:49,839
generated we will just uh work with

333
00:12:49,839 --> 00:12:50,639
bernoulli here

334
00:12:50,639 --> 00:12:54,079
here bandits and we have

335
00:12:54,079 --> 00:12:56,240
we have fixed uh reward probabilities in

336
00:12:56,240 --> 00:12:58,000
a very specific way

337
00:12:58,000 --> 00:13:00,560
so there is only one arm which has a

338
00:13:00,560 --> 00:13:02,800
reward probability associated with p max

339
00:13:02,800 --> 00:13:04,240
which is one half

340
00:13:04,240 --> 00:13:08,000
plus some uh term epsilon like which is

341
00:13:08,000 --> 00:13:11,200
larger than zero and all other uh

342
00:13:11,200 --> 00:13:14,079
um all reward probabilities associated

343
00:13:14,079 --> 00:13:15,600
with other arms are just fixed to one

344
00:13:15,600 --> 00:13:16,399
half

345
00:13:16,399 --> 00:13:20,000
but this kind of allows us to

346
00:13:20,000 --> 00:13:22,399
like control the difficulty of the task

347
00:13:22,399 --> 00:13:23,680
so because the

348
00:13:23,680 --> 00:13:25,760
more smaller the epsilon is the closer

349
00:13:25,760 --> 00:13:27,920
to zero is uh the more difficult is to

350
00:13:27,920 --> 00:13:29,600
distinguish the best arm from

351
00:13:29,600 --> 00:13:31,920
from others and the more samples one he

352
00:13:31,920 --> 00:13:33,680
needs to draw to kind of realize the

353
00:13:33,680 --> 00:13:35,760
difference right

354
00:13:35,760 --> 00:13:39,120
uh and right beside this like the best

355
00:13:39,120 --> 00:13:40,480
arm advantage term

356
00:13:40,480 --> 00:13:44,160
uh one also realizes that right number

357
00:13:44,160 --> 00:13:44,399
of

358
00:13:44,399 --> 00:13:47,760
arms k is also increasing the difficulty

359
00:13:47,760 --> 00:13:49,600
the more arms you have the more time you

360
00:13:49,600 --> 00:13:50,880
need to figure out

361
00:13:50,880 --> 00:13:54,160
what is the correct arm to play okay

362
00:13:54,160 --> 00:13:55,920
so just to give you an illustration of

363
00:13:55,920 --> 00:13:57,760
the example right we have

364
00:13:57,760 --> 00:14:02,240
forearm banded here uh

365
00:14:02,240 --> 00:14:04,639
so whenever kind of agent pulls one of

366
00:14:04,639 --> 00:14:06,160
the arms uh it gets

367
00:14:06,160 --> 00:14:08,079
it gets either zero or one so we can

368
00:14:08,079 --> 00:14:09,440
think of this as a

369
00:14:09,440 --> 00:14:12,720
reward or absence of a reward right

370
00:14:12,720 --> 00:14:15,360
uh beliefs about the reward

371
00:14:15,360 --> 00:14:17,600
probabilities uh we will assume that

372
00:14:17,600 --> 00:14:19,920
right these are beta distributed so we

373
00:14:19,920 --> 00:14:22,079
will use beta distribution to model kind

374
00:14:22,079 --> 00:14:22,399
of

375
00:14:22,399 --> 00:14:25,920
a representation of reward probabilities

376
00:14:25,920 --> 00:14:29,120
associated with each arm

377
00:14:29,120 --> 00:14:31,360
and we will use um bayesian belief

378
00:14:31,360 --> 00:14:32,240
updating for

379
00:14:32,240 --> 00:14:34,720
all action selection algorithms so this

380
00:14:34,720 --> 00:14:36,079
also kind of limits

381
00:14:36,079 --> 00:14:39,199
uh the range of algorithms which we want

382
00:14:39,199 --> 00:14:39,920
to compare

383
00:14:39,920 --> 00:14:42,719
here in this study

384
00:14:43,199 --> 00:14:46,800
i will explain a bit uh later

385
00:14:46,800 --> 00:14:50,320
why i mean we focus on that

386
00:14:50,320 --> 00:14:53,360
uh so

387
00:14:53,360 --> 00:14:56,240
just to give you a kind of visual

388
00:14:56,240 --> 00:14:58,399
example of what this does

389
00:14:58,399 --> 00:15:01,519
is right uh whenever an arm kind of

390
00:15:01,519 --> 00:15:03,760
an agent starts with with kind of flat

391
00:15:03,760 --> 00:15:04,959
beliefs over

392
00:15:04,959 --> 00:15:06,880
reward probabilities associated with

393
00:15:06,880 --> 00:15:08,000
each

394
00:15:08,000 --> 00:15:09,519
arm so this is like a uniform

395
00:15:09,519 --> 00:15:11,199
distribution

396
00:15:11,199 --> 00:15:13,199
uh this is a special case of beta

397
00:15:13,199 --> 00:15:14,240
distribution

398
00:15:14,240 --> 00:15:16,800
and whenever uh it pulls like selects

399
00:15:16,800 --> 00:15:17,600
one arm

400
00:15:17,600 --> 00:15:20,160
uh it gets uh either one or zero as an

401
00:15:20,160 --> 00:15:20,639
outcome

402
00:15:20,639 --> 00:15:23,920
and it updates right uh the beliefs

403
00:15:23,920 --> 00:15:26,000
about the reward probabilities uh in

404
00:15:26,000 --> 00:15:29,199
one of the directions right uh so

405
00:15:29,199 --> 00:15:31,040
in here right we're kind of it's

406
00:15:31,040 --> 00:15:33,199
sampling one more often so it

407
00:15:33,199 --> 00:15:35,120
has increased belief that there is a

408
00:15:35,120 --> 00:15:36,560
higher reward probability

409
00:15:36,560 --> 00:15:40,480
associated associated with this

410
00:15:40,480 --> 00:15:43,839
uh so formally how

411
00:15:43,839 --> 00:15:46,240
this is kind of implemented what you see

412
00:15:46,240 --> 00:15:47,519
here

413
00:15:47,519 --> 00:15:49,120
is like to a very simple you know

414
00:15:49,120 --> 00:15:51,519
generative model uh

415
00:15:51,519 --> 00:15:54,560
so with each k arm we assume there is

416
00:15:54,560 --> 00:15:55,680
some associated

417
00:15:55,680 --> 00:15:58,560
unknown reward probability that okay

418
00:15:58,560 --> 00:16:00,160
which is just a number between zero and

419
00:16:00,160 --> 00:16:01,279
one

420
00:16:01,279 --> 00:16:03,759
and uh choice outcomes of choices are

421
00:16:03,759 --> 00:16:05,199
either zero or one so they're just

422
00:16:05,199 --> 00:16:06,240
binary choices

423
00:16:06,240 --> 00:16:08,880
so this kind of constraints are like uh

424
00:16:08,880 --> 00:16:10,160
observation likely

425
00:16:10,160 --> 00:16:12,800
to a bernoulli distribution so this is

426
00:16:12,800 --> 00:16:13,839
just kind of

427
00:16:13,839 --> 00:16:15,600
product of different bernoulli

428
00:16:15,600 --> 00:16:17,519
distribution depending on which arm one

429
00:16:17,519 --> 00:16:19,120
is selecting

430
00:16:19,120 --> 00:16:22,720
and this kind of uh term below like this

431
00:16:22,720 --> 00:16:23,199
is

432
00:16:23,199 --> 00:16:25,279
our prior belief about reward

433
00:16:25,279 --> 00:16:26,240
probabilities

434
00:16:26,240 --> 00:16:27,839
and this will assume is just kind of

435
00:16:27,839 --> 00:16:29,680
product of uh many uh

436
00:16:29,680 --> 00:16:32,560
beta distributions where here alpha zero

437
00:16:32,560 --> 00:16:34,560
and beta0 are just fixed to one so this

438
00:16:34,560 --> 00:16:36,639
corresponds to the uniform

439
00:16:36,639 --> 00:16:40,000
right right one can also start with

440
00:16:40,000 --> 00:16:41,440
other values but uh

441
00:16:41,440 --> 00:16:44,480
i think this makes some sense again

442
00:16:44,480 --> 00:16:48,839
reflects uh no knowledge a priori about

443
00:16:48,839 --> 00:16:50,560
report

444
00:16:50,560 --> 00:16:54,000
uh so now given some choice at

445
00:16:54,000 --> 00:16:57,920
trial t80 uh we just apply

446
00:16:57,920 --> 00:17:00,320
by subject rule and we get from some

447
00:17:00,320 --> 00:17:02,000
prior beliefs

448
00:17:02,000 --> 00:17:04,880
uh a posterior beliefs and then i think

449
00:17:04,880 --> 00:17:06,880
about the stationary case is if

450
00:17:06,880 --> 00:17:08,880
our priors are beta distributed our

451
00:17:08,880 --> 00:17:10,240
posteriors will be also

452
00:17:10,240 --> 00:17:13,839
beta distributed and the update

453
00:17:13,839 --> 00:17:15,919
one only needs to take care about how we

454
00:17:15,919 --> 00:17:17,280
update write the

455
00:17:17,280 --> 00:17:19,199
parameters of beta distribution alpha

456
00:17:19,199 --> 00:17:20,640
and beta

457
00:17:20,640 --> 00:17:22,640
and this just works in a form of

458
00:17:22,640 --> 00:17:24,160
accounting basically

459
00:17:24,160 --> 00:17:26,559
alpha counts how many times you observe

460
00:17:26,559 --> 00:17:28,799
one as an outcome and beta counts how

461
00:17:28,799 --> 00:17:29,679
many times you

462
00:17:29,679 --> 00:17:33,840
observe zero

463
00:17:33,919 --> 00:17:36,880
and so because of all this uh in this

464
00:17:36,880 --> 00:17:37,919
stationary case

465
00:17:37,919 --> 00:17:40,960
inference is uh exact so that because

466
00:17:40,960 --> 00:17:42,320
both prior and possible belong to the

467
00:17:42,320 --> 00:17:43,919
same distribution family you have kind

468
00:17:43,919 --> 00:17:45,440
of conjugate prior

469
00:17:45,440 --> 00:17:47,120
here setup so you can just track

470
00:17:47,120 --> 00:17:48,480
effectively

471
00:17:48,480 --> 00:17:51,840
the updating there is another kind of

472
00:17:51,840 --> 00:17:54,160
uh representation of the update rules

473
00:17:54,160 --> 00:17:56,559
one can use here so for example

474
00:17:56,559 --> 00:18:00,640
if we express the updating in terms of

475
00:18:00,640 --> 00:18:03,840
expected kind of outcome mu

476
00:18:03,840 --> 00:18:06,880
and the scale parameter nu here we see

477
00:18:06,880 --> 00:18:08,880
that actually the expectations are

478
00:18:08,880 --> 00:18:11,919
updated in the form of a

479
00:18:11,919 --> 00:18:14,799
delta like learning room so right where

480
00:18:14,799 --> 00:18:16,480
this kind of learning rate is something

481
00:18:16,480 --> 00:18:18,960
which decreases over time so basically

482
00:18:18,960 --> 00:18:21,120
new is just increasing over time

483
00:18:21,120 --> 00:18:22,720
whenever you select specific

484
00:18:22,720 --> 00:18:25,840
arm and you will just increase by one

485
00:18:25,840 --> 00:18:27,360
and this means that the learning rate

486
00:18:27,360 --> 00:18:28,880
decreases over time so

487
00:18:28,880 --> 00:18:31,200
the more you sample from one arm the

488
00:18:31,200 --> 00:18:32,400
less you will update

489
00:18:32,400 --> 00:18:35,600
your beliefs about it all right so this

490
00:18:35,600 --> 00:18:36,160
is uh

491
00:18:36,160 --> 00:18:40,320
in practically what's happening here

492
00:18:40,320 --> 00:18:44,799
very very simple um

493
00:18:44,799 --> 00:18:47,520
okay so and uh basically this is

494
00:18:47,520 --> 00:18:49,440
everything we need to know about kind of

495
00:18:49,440 --> 00:18:50,640
this uh

496
00:18:50,640 --> 00:18:52,240
let's call it perceptual part of the

497
00:18:52,240 --> 00:18:54,799
generity model so how we update beliefs

498
00:18:54,799 --> 00:18:55,520
given some

499
00:18:55,520 --> 00:18:57,039
outcomes now we have to kind of

500
00:18:57,039 --> 00:18:59,360
introduce how we select actions right

501
00:18:59,360 --> 00:19:02,640
based on these beliefs and for this

502
00:19:02,640 --> 00:19:03,440
right i

503
00:19:03,440 --> 00:19:04,960
we kind of can talk about action

504
00:19:04,960 --> 00:19:07,120
selection algorithms

505
00:19:07,120 --> 00:19:11,039
and so uh

506
00:19:11,039 --> 00:19:14,640
uh they are in kind of in the literature

507
00:19:14,640 --> 00:19:16,799
there are lots of red examples one of

508
00:19:16,799 --> 00:19:18,320
the oldest one is probably epsilon

509
00:19:18,320 --> 00:19:19,120
greedy

510
00:19:19,120 --> 00:19:21,620
you also mention it during the first

511
00:19:21,620 --> 00:19:23,039
[Music]

512
00:19:23,039 --> 00:19:26,160
presentation point zero tyler people

513
00:19:26,160 --> 00:19:28,240
also talk about right uh ucb

514
00:19:28,240 --> 00:19:30,400
uh upper confidence bound this is also

515
00:19:30,400 --> 00:19:31,600
one of the oldest one

516
00:19:31,600 --> 00:19:35,600
kl ucb uh it's kind of extension which

517
00:19:35,600 --> 00:19:36,240
uses uh

518
00:19:36,240 --> 00:19:39,840
kl as uh exploration okay kl divergence

519
00:19:39,840 --> 00:19:40,400
is a

520
00:19:40,400 --> 00:19:43,520
estimate of exploration uh bound

521
00:19:43,520 --> 00:19:46,640
there is thompson sampling etc uh here

522
00:19:46,640 --> 00:19:47,200
we will

523
00:19:47,200 --> 00:19:50,559
focus mostly on on this

524
00:19:50,559 --> 00:19:52,960
tree right we will use the optimistics

525
00:19:52,960 --> 00:19:53,919
thompson sampling

526
00:19:53,919 --> 00:19:56,480
as a one kind of comparative example and

527
00:19:56,480 --> 00:19:58,160
another is bayesian upper confidence

528
00:19:58,160 --> 00:19:59,200
bound

529
00:19:59,200 --> 00:20:01,520
so first both of these algorithms have

530
00:20:01,520 --> 00:20:02,559
been extensively

531
00:20:02,559 --> 00:20:04,559
analyzed in the literature and people

532
00:20:04,559 --> 00:20:05,760
just find something

533
00:20:05,760 --> 00:20:09,679
they work in in many examples lots a lot

534
00:20:09,679 --> 00:20:11,679
much better than kind of let's say

535
00:20:11,679 --> 00:20:14,080
non-bayesian approaches

536
00:20:14,080 --> 00:20:17,679
uh and secondly right we can

537
00:20:17,679 --> 00:20:19,679
use basically for all three algorithms

538
00:20:19,679 --> 00:20:21,200
the same uh

539
00:20:21,200 --> 00:20:22,720
update rules because they are just

540
00:20:22,720 --> 00:20:24,480
bayesian uh

541
00:20:24,480 --> 00:20:26,080
they're just algorithms corresponding to

542
00:20:26,080 --> 00:20:27,840
bayesian and bandits so

543
00:20:27,840 --> 00:20:30,480
we can kind of our motivation here is to

544
00:20:30,480 --> 00:20:31,679
compare kind of

545
00:20:31,679 --> 00:20:34,400
action selection principles based on the

546
00:20:34,400 --> 00:20:34,880
uh

547
00:20:34,880 --> 00:20:37,039
action selection algorithm and not based

548
00:20:37,039 --> 00:20:39,120
on potentially different ways how you

549
00:20:39,120 --> 00:20:40,960
update beliefs about

550
00:20:40,960 --> 00:20:42,880
uh about the history of observations

551
00:20:42,880 --> 00:20:44,480
right

552
00:20:44,480 --> 00:20:48,159
and uh so right this would be kind of uh

553
00:20:48,159 --> 00:20:51,280
motivation and also a bit to simplify uh

554
00:20:51,280 --> 00:20:54,480
the comparison i mean you can like add

555
00:20:54,480 --> 00:20:57,679
to the list at least 10 other ways of

556
00:20:57,679 --> 00:21:00,960
uh doing the rate action selection and

557
00:21:00,960 --> 00:21:01,840
even like

558
00:21:01,840 --> 00:21:06,400
decision making in multi-armed bandits

559
00:21:06,400 --> 00:21:09,520
uh okay so just a bit too

560
00:21:09,520 --> 00:21:11,840
kind of historic example important this

561
00:21:11,840 --> 00:21:12,799
is just like

562
00:21:12,799 --> 00:21:15,039
upper confidence bound although i will

563
00:21:15,039 --> 00:21:16,880
not directly compare it it has some

564
00:21:16,880 --> 00:21:18,559
relevance to understand also what is

565
00:21:18,559 --> 00:21:19,919
happening kind of in

566
00:21:19,919 --> 00:21:23,760
active inference later uh

567
00:21:23,760 --> 00:21:26,159
and this uh this form here uh just

568
00:21:26,159 --> 00:21:27,440
corresponds to

569
00:21:27,440 --> 00:21:30,480
a version uh which is adapted to

570
00:21:30,480 --> 00:21:32,400
specifically bernoulli bandit so some

571
00:21:32,400 --> 00:21:34,000
people might be more familiar with

572
00:21:34,000 --> 00:21:37,360
other version which was derived for

573
00:21:37,360 --> 00:21:37,919
gaussian

574
00:21:37,919 --> 00:21:40,640
bandits right so this is kind of in this

575
00:21:40,640 --> 00:21:41,679
paper here

576
00:21:41,679 --> 00:21:44,159
one can see uh these different

577
00:21:44,159 --> 00:21:45,919
derivations and examples

578
00:21:45,919 --> 00:21:48,159
how how this works what is important

579
00:21:48,159 --> 00:21:50,799
right to notice here is basically that

580
00:21:50,799 --> 00:21:54,159
so this first term is just the expected

581
00:21:54,159 --> 00:21:57,679
probability of reward on arm k

582
00:21:57,679 --> 00:21:59,520
and the second two terms correspond to

583
00:21:59,520 --> 00:22:02,559
this exploration bonus or the bound

584
00:22:02,559 --> 00:22:05,760
uh and uh

585
00:22:05,760 --> 00:22:08,559
so this is uh typical for ucb is that

586
00:22:08,559 --> 00:22:09,039
the

587
00:22:09,039 --> 00:22:11,679
bound kind of increases over time

588
00:22:11,679 --> 00:22:12,600
logarithmic

589
00:22:12,600 --> 00:22:14,799
logarithmically with the number of kind

590
00:22:14,799 --> 00:22:16,799
of trials you're doing so basically

591
00:22:16,799 --> 00:22:19,440
the more you're not selecting one arm

592
00:22:19,440 --> 00:22:21,440
the more you're kind of pushed

593
00:22:21,440 --> 00:22:23,440
again to select it at some point in time

594
00:22:23,440 --> 00:22:25,200
in the future

595
00:22:25,200 --> 00:22:27,600
all right so basically this necessarily

596
00:22:27,600 --> 00:22:28,960
increases over time

597
00:22:28,960 --> 00:22:31,600
uh if you are not interacting with the

598
00:22:31,600 --> 00:22:33,360
specific r

599
00:22:33,360 --> 00:22:36,320
right uh but still although like where

600
00:22:36,320 --> 00:22:38,320
the algorithm is very simple it has nice

601
00:22:38,320 --> 00:22:39,120
theoretical

602
00:22:39,120 --> 00:22:41,520
results so because it's kind of it's

603
00:22:41,520 --> 00:22:43,200
efficient algorithm

604
00:22:43,200 --> 00:22:47,039
it converges in infinite number of

605
00:22:47,039 --> 00:22:49,520
samples

606
00:22:51,039 --> 00:22:52,640
however right here we'll talk on this

607
00:22:52,640 --> 00:22:54,080
kind of asian variant

608
00:22:54,080 --> 00:22:55,840
of the upper confidence bound which

609
00:22:55,840 --> 00:22:58,400
actually make a

610
00:22:58,400 --> 00:23:02,000
select arms based on kind of the

611
00:23:02,000 --> 00:23:07,039
percentile uh upper percentage of

612
00:23:07,039 --> 00:23:08,880
cumulative distribution function right

613
00:23:08,880 --> 00:23:10,559
so so as

614
00:23:10,559 --> 00:23:14,240
time uh progresses uh you uh

615
00:23:14,240 --> 00:23:17,280
one is trying to estimate the uh like

616
00:23:17,280 --> 00:23:17,840
the

617
00:23:17,840 --> 00:23:20,640
extreme value of a believe distribution

618
00:23:20,640 --> 00:23:21,360
which in this

619
00:23:21,360 --> 00:23:24,640
case is just like a beta distribution

620
00:23:24,640 --> 00:23:27,120
and and one one gets this extreme by

621
00:23:27,120 --> 00:23:28,480
solving this equation which just

622
00:23:28,480 --> 00:23:29,520
corresponds to

623
00:23:29,520 --> 00:23:32,240
inverse regularized incomplete beta

624
00:23:32,240 --> 00:23:33,760
function so basically

625
00:23:33,760 --> 00:23:36,240
the more extreme point your beliefs

626
00:23:36,240 --> 00:23:37,919
contain the more likely is that

627
00:23:37,919 --> 00:23:40,240
i mean the more likely that you will

628
00:23:40,240 --> 00:23:41,919
select that arm so

629
00:23:41,919 --> 00:23:44,640
uh the more relevant is to select this

630
00:23:44,640 --> 00:23:46,640
arm because you're expecting that right

631
00:23:46,640 --> 00:23:49,679
uh this high value is still possible in

632
00:23:49,679 --> 00:23:50,880
the

633
00:23:50,880 --> 00:23:54,159
uh as a reward probability in this uh

634
00:23:54,159 --> 00:23:57,440
in this setup and so but this kind of

635
00:23:57,440 --> 00:23:59,360
algorithm has couple of parameters but

636
00:23:59,360 --> 00:23:59,919
what

637
00:23:59,919 --> 00:24:02,799
authors kind of show in this paper is if

638
00:24:02,799 --> 00:24:03,360
you have

639
00:24:03,360 --> 00:24:05,840
a very good results for just fix by

640
00:24:05,840 --> 00:24:06,559
fixing c

641
00:24:06,559 --> 00:24:08,400
to zero so basically this term just

642
00:24:08,400 --> 00:24:10,080
becomes one

643
00:24:10,080 --> 00:24:13,440
uh and so we we just

644
00:24:13,440 --> 00:24:15,760
use their advice here in the paper so we

645
00:24:15,760 --> 00:24:17,279
will not uh

646
00:24:17,279 --> 00:24:19,520
and we didn't kind of try to analyze

647
00:24:19,520 --> 00:24:20,960
other possibilities

648
00:24:20,960 --> 00:24:24,159
or other kind of values of these

649
00:24:24,159 --> 00:24:26,400
parameters in in the

650
00:24:26,400 --> 00:24:29,520
uh in this case

651
00:24:29,520 --> 00:24:33,200
uh so for um thompson sampling and this

652
00:24:33,200 --> 00:24:34,480
is again one of the

653
00:24:34,480 --> 00:24:38,240
classical algorithms first probably um

654
00:24:38,240 --> 00:24:41,039
attempt with bayesian bandits so with

655
00:24:41,039 --> 00:24:42,720
bayesian decision making

656
00:24:42,720 --> 00:24:44,960
and it's also extremely simple right

657
00:24:44,960 --> 00:24:46,000
given some beliefs

658
00:24:46,000 --> 00:24:48,799
about probabilities at each arm you you

659
00:24:48,799 --> 00:24:50,720
sample one point

660
00:24:50,720 --> 00:24:52,640
uh so right if there are like 10 arms

661
00:24:52,640 --> 00:24:54,559
you would sample from each arm one point

662
00:24:54,559 --> 00:24:54,960
and

663
00:24:54,960 --> 00:24:58,080
you will just select an arm which gives

664
00:24:58,080 --> 00:24:58,480
you the

665
00:24:58,480 --> 00:25:02,080
high largest values right uh

666
00:25:02,080 --> 00:25:06,159
a variant uh of that which was shown

667
00:25:06,159 --> 00:25:08,320
kind of last five to ten years that it

668
00:25:08,320 --> 00:25:10,240
works slightly better is this optimistic

669
00:25:10,240 --> 00:25:13,120
stomps on sampling

670
00:25:13,279 --> 00:25:16,480
where uh one is constraining the samples

671
00:25:16,480 --> 00:25:17,200
only to

672
00:25:17,200 --> 00:25:19,279
uh values which are larger than the

673
00:25:19,279 --> 00:25:20,960
expectation right so

674
00:25:20,960 --> 00:25:22,960
so one is first making a sample from

675
00:25:22,960 --> 00:25:24,559
each arm and then

676
00:25:24,559 --> 00:25:26,320
if the sample is larger than the

677
00:25:26,320 --> 00:25:28,799
expected probability

678
00:25:28,799 --> 00:25:31,440
uh of reward then uh one keeps the

679
00:25:31,440 --> 00:25:33,679
sample otherwise uh you just use the

680
00:25:33,679 --> 00:25:35,679
expected value as uh

681
00:25:35,679 --> 00:25:39,279
uh as a value a kind of a reward

682
00:25:39,279 --> 00:25:42,080
probability associated with that right

683
00:25:42,080 --> 00:25:44,799
and you're just again over different

684
00:25:44,799 --> 00:25:47,039
arms maximizing taking the uh selecting

685
00:25:47,039 --> 00:25:48,640
the arm which

686
00:25:48,640 --> 00:25:52,080
gives you the maximum reward probability

687
00:25:52,080 --> 00:25:55,039
but this is a kind of a very stochastic

688
00:25:55,039 --> 00:25:55,600
approach

689
00:25:55,600 --> 00:25:57,760
where this exploration actually kind of

690
00:25:57,760 --> 00:26:01,200
bonus to this algorithm comes from

691
00:26:01,200 --> 00:26:04,240
basically this random sampling from a

692
00:26:04,240 --> 00:26:06,080
probability distribution and right the

693
00:26:06,080 --> 00:26:08,799
broader your beliefs are about something

694
00:26:08,799 --> 00:26:10,240
the more likely that you will get the

695
00:26:10,240 --> 00:26:11,840
kind of large value

696
00:26:11,840 --> 00:26:14,080
hence the more likely that you will

697
00:26:14,080 --> 00:26:15,039
explore more

698
00:26:15,039 --> 00:26:19,520
uh select that that arm uh

699
00:26:20,880 --> 00:26:23,600
different runs and basically exploration

700
00:26:23,600 --> 00:26:24,240
is kind of

701
00:26:24,240 --> 00:26:27,840
here completely driven by uh the

702
00:26:27,840 --> 00:26:31,600
noisiness of the sampling process itself

703
00:26:31,600 --> 00:26:35,840
um okay so now we come to

704
00:26:35,840 --> 00:26:39,279
basically active inference uh

705
00:26:39,279 --> 00:26:42,880
version of this

706
00:26:42,880 --> 00:26:46,240
so um we are kind of simplifying

707
00:26:46,240 --> 00:26:48,880
here things from what people maybe know

708
00:26:48,880 --> 00:26:50,640
uh

709
00:26:50,640 --> 00:26:53,440
how active infrared is used normally so

710
00:26:53,440 --> 00:26:54,559
first we will use like

711
00:26:54,559 --> 00:26:56,880
rolling behavioral policies which means

712
00:26:56,880 --> 00:26:58,080
that

713
00:26:58,080 --> 00:27:00,400
agent is not like tracking history of

714
00:27:00,400 --> 00:27:01,600
actions that perform

715
00:27:01,600 --> 00:27:04,159
it just kind of a repeating step of

716
00:27:04,159 --> 00:27:06,559
policies on every trial

717
00:27:06,559 --> 00:27:09,200
and in this case the behavioral policy

718
00:27:09,200 --> 00:27:09,679
just

719
00:27:09,679 --> 00:27:13,039
corresponds to a single choice so

720
00:27:13,039 --> 00:27:16,400
in this type of bandit problems we

721
00:27:16,400 --> 00:27:18,960
we are kind of analyzing here like agent

722
00:27:18,960 --> 00:27:20,399
cannot change anything in the

723
00:27:20,399 --> 00:27:21,760
environment

724
00:27:21,760 --> 00:27:24,799
hence planning is completely irrelevant

725
00:27:24,799 --> 00:27:27,039
uh in a way you cannot kind of position

726
00:27:27,039 --> 00:27:29,600
yourself in us

727
00:27:29,600 --> 00:27:32,080
in space better over time so that you

728
00:27:32,080 --> 00:27:33,039
kind of

729
00:27:33,039 --> 00:27:36,000
need to plan something which means that

730
00:27:36,000 --> 00:27:36,320
right

731
00:27:36,320 --> 00:27:38,399
just kind of single choice policy

732
00:27:38,399 --> 00:27:40,159
evaluation a single time step in the

733
00:27:40,159 --> 00:27:42,960
future is sufficient to make uh

734
00:27:42,960 --> 00:27:46,399
good decisions uh

735
00:27:46,399 --> 00:27:49,840
and uh right uh uh generally expected

736
00:27:49,840 --> 00:27:50,320
free

737
00:27:50,320 --> 00:27:53,200
we will base our action selection on

738
00:27:53,200 --> 00:27:55,279
expected free energy here

739
00:27:55,279 --> 00:27:58,559
uh where this would be uh

740
00:27:58,559 --> 00:28:02,159
a form which uh decouples into a risk

741
00:28:02,159 --> 00:28:05,840
and ambiguity term but we can also think

742
00:28:05,840 --> 00:28:07,600
about this problem as freight uh

743
00:28:07,600 --> 00:28:10,720
estimating uh expected value of

744
00:28:10,720 --> 00:28:12,720
of different arms plus the expected

745
00:28:12,720 --> 00:28:14,960
information gain so how much information

746
00:28:14,960 --> 00:28:17,039
we can extract from different arms if we

747
00:28:17,039 --> 00:28:19,440
select

748
00:28:22,399 --> 00:28:24,719
so

749
00:28:25,600 --> 00:28:27,520
now let's assume that we know how to

750
00:28:27,520 --> 00:28:31,919
compute expected free energy i will uh

751
00:28:31,919 --> 00:28:35,520
go through details on next slides

752
00:28:35,520 --> 00:28:37,679
uh normally like right posterior oval

753
00:28:37,679 --> 00:28:39,120
policy

754
00:28:39,120 --> 00:28:42,880
is is estimated as a kind of mixture

755
00:28:42,880 --> 00:28:43,360
between

756
00:28:43,360 --> 00:28:46,480
expected free energy so kind of a future

757
00:28:46,480 --> 00:28:49,360
uh expectation about what behavior will

758
00:28:49,360 --> 00:28:50,080
do

759
00:28:50,080 --> 00:28:52,000
plus the kind of the second part which

760
00:28:52,000 --> 00:28:54,320
is the free energy about the past

761
00:28:54,320 --> 00:28:56,240
kind of outcomes however because we have

762
00:28:56,240 --> 00:28:57,679
like rolling policies

763
00:28:57,679 --> 00:29:01,279
this term is just constant uh

764
00:29:01,279 --> 00:29:03,120
for each policy is the same it has the

765
00:29:03,120 --> 00:29:05,520
same value basically so the posterior

766
00:29:05,520 --> 00:29:07,600
policy or where action just corresponds

767
00:29:07,600 --> 00:29:09,039
then to the

768
00:29:09,039 --> 00:29:12,240
to the soft max right over the expected

769
00:29:12,240 --> 00:29:14,080
free energy

770
00:29:14,080 --> 00:29:17,200
uh and normally one one things like

771
00:29:17,200 --> 00:29:19,520
uh about choice selection this is

772
00:29:19,520 --> 00:29:20,240
assembly

773
00:29:20,240 --> 00:29:22,880
samples from the posterior however we

774
00:29:22,880 --> 00:29:24,480
are here only interested in kind of

775
00:29:24,480 --> 00:29:26,399
optimal choices so for us

776
00:29:26,399 --> 00:29:28,880
basically gamma is just infinite value

777
00:29:28,880 --> 00:29:30,240
and we are just selecting the

778
00:29:30,240 --> 00:29:32,480
uh making sure choices about actions

779
00:29:32,480 --> 00:29:35,679
which minimize expected free energy

780
00:29:35,679 --> 00:29:38,640
so this this is useful parameter to have

781
00:29:38,640 --> 00:29:41,360
if you need to kind of uh

782
00:29:41,360 --> 00:29:44,480
fit a model to the behavior but for this

783
00:29:44,480 --> 00:29:45,360
kind of a

784
00:29:45,360 --> 00:29:48,000
practical application there is not much

785
00:29:48,000 --> 00:29:48,480
gain

786
00:29:48,480 --> 00:29:52,840
in adding another source of noise here

787
00:29:52,840 --> 00:29:56,480
um okay so how do we compute the

788
00:29:56,480 --> 00:29:58,720
expected free energy right

789
00:29:58,720 --> 00:30:01,600
well that's in a way quite simple so we

790
00:30:01,600 --> 00:30:02,399
just

791
00:30:02,399 --> 00:30:05,360
have a couple of terms here one is uh

792
00:30:05,360 --> 00:30:07,840
posterior over

793
00:30:07,840 --> 00:30:11,039
posterior beliefs over reward

794
00:30:11,039 --> 00:30:12,080
probabilities right

795
00:30:12,080 --> 00:30:14,320
given is this like q term this is just

796
00:30:14,320 --> 00:30:16,720
the product of beta distributions

797
00:30:16,720 --> 00:30:18,399
we have marginal likelihood so

798
00:30:18,399 --> 00:30:20,080
probability of observing

799
00:30:20,080 --> 00:30:22,799
o outcome given some action 80. so this

800
00:30:22,799 --> 00:30:23,360
is just

801
00:30:23,360 --> 00:30:26,559
marginalizing likelihood over uh our

802
00:30:26,559 --> 00:30:29,039
current posterior beliefs

803
00:30:29,039 --> 00:30:31,679
and another term is here just the prior

804
00:30:31,679 --> 00:30:33,039
preference over

805
00:30:33,039 --> 00:30:35,360
different outcomes right and this is

806
00:30:35,360 --> 00:30:38,000
really easy to parameterize because we

807
00:30:38,000 --> 00:30:40,720
we work with binary outcomes and we can

808
00:30:40,720 --> 00:30:42,480
just define here a single parameter

809
00:30:42,480 --> 00:30:44,000
lambda

810
00:30:44,000 --> 00:30:46,399
uh so the higher the lambda is the

811
00:30:46,399 --> 00:30:47,679
higher the preference

812
00:30:47,679 --> 00:30:50,720
is of the agent to observe once

813
00:30:50,720 --> 00:30:54,480
com relative to zeros right

814
00:30:54,480 --> 00:30:57,440
and this uh lambda parameter also kind

815
00:30:57,440 --> 00:30:58,159
of has a

816
00:30:58,159 --> 00:31:00,320
role of regularizing the amount of

817
00:31:00,320 --> 00:31:01,279
exploration an

818
00:31:01,279 --> 00:31:03,919
agent does because the larger the lambda

819
00:31:03,919 --> 00:31:06,159
is the more um

820
00:31:06,159 --> 00:31:10,159
uh the more focus is uh

821
00:31:10,159 --> 00:31:12,399
is set on the exploitation of kind of

822
00:31:12,399 --> 00:31:15,120
making selections based on the expected

823
00:31:15,120 --> 00:31:19,360
value instead of expected information

824
00:31:19,360 --> 00:31:21,840
uh and so this final term right

825
00:31:21,840 --> 00:31:25,600
computing ambiguity is basically just uh

826
00:31:25,600 --> 00:31:28,640
computing expectation over the entropy

827
00:31:28,640 --> 00:31:29,600
of different outcomes

828
00:31:29,600 --> 00:31:32,000
right so which also has relatively

829
00:31:32,000 --> 00:31:34,240
simple form here

830
00:31:34,240 --> 00:31:37,600
uh because of bernoulli likelihood

831
00:31:37,600 --> 00:31:40,640
so uh without going into all the details

832
00:31:40,640 --> 00:31:42,720
this is basically how the expected free

833
00:31:42,720 --> 00:31:44,720
energy looks like so this first term

834
00:31:44,720 --> 00:31:45,760
here

835
00:31:45,760 --> 00:31:48,080
is correspond just to the expected

836
00:31:48,080 --> 00:31:50,799
negative of the expected reward

837
00:31:50,799 --> 00:31:52,880
so because we are minimizing expected

838
00:31:52,880 --> 00:31:54,720
free energy this effectively means that

839
00:31:54,720 --> 00:31:57,840
we are maximizing expected reward

840
00:31:57,840 --> 00:32:00,480
and the second term is just a very

841
00:32:00,480 --> 00:32:02,559
complex sets of

842
00:32:02,559 --> 00:32:05,840
set of like equations which

843
00:32:05,840 --> 00:32:07,840
gives you estimate of the expected

844
00:32:07,840 --> 00:32:10,879
information gain right

845
00:32:11,840 --> 00:32:13,919
and the in a way kind of motivation is

846
00:32:13,919 --> 00:32:15,679
just because one cannot really

847
00:32:15,679 --> 00:32:17,679
understand what is going on here right

848
00:32:17,679 --> 00:32:19,760
we have kind of logarithms of

849
00:32:19,760 --> 00:32:22,880
expected uh reward probabilities but

850
00:32:22,880 --> 00:32:24,640
then we have a d gamma function of

851
00:32:24,640 --> 00:32:26,399
parameters and

852
00:32:26,399 --> 00:32:29,760
uh so instead uh we can kind of try to

853
00:32:29,760 --> 00:32:31,600
simplify the term by

854
00:32:31,600 --> 00:32:34,080
approximating right effectively the

855
00:32:34,080 --> 00:32:36,559
information gain part

856
00:32:36,559 --> 00:32:39,039
uh just to get better understanding of

857
00:32:39,039 --> 00:32:40,480
uh

858
00:32:40,480 --> 00:32:42,480
how basically expected free energy

859
00:32:42,480 --> 00:32:44,159
scales uh

860
00:32:44,159 --> 00:32:47,120
like with repeated choices and uh so

861
00:32:47,120 --> 00:32:47,760
what

862
00:32:47,760 --> 00:32:50,159
one ends up with very simple form right

863
00:32:50,159 --> 00:32:52,640
basically the exploration term

864
00:32:52,640 --> 00:32:56,559
just corresponds to 1 over

865
00:32:56,559 --> 00:32:59,120
2 divided by the number of time when

866
00:32:59,120 --> 00:33:00,080
selected

867
00:33:00,080 --> 00:33:03,039
different arms right so this is in a way

868
00:33:03,039 --> 00:33:05,760
quite similar to

869
00:33:05,760 --> 00:33:10,000
what what we had in the in ucb algorithm

870
00:33:10,000 --> 00:33:12,399
right the like first exploration term

871
00:33:12,399 --> 00:33:14,559
was also divided by something which is

872
00:33:14,559 --> 00:33:16,480
proportional to how many times you

873
00:33:16,480 --> 00:33:17,919
selected

874
00:33:17,919 --> 00:33:20,640
uh an arm with but without write this

875
00:33:20,640 --> 00:33:21,200
kind of

876
00:33:21,200 --> 00:33:25,679
um x expansion of exploration bonus with

877
00:33:25,679 --> 00:33:28,320
time so there is no logarithm of t

878
00:33:28,320 --> 00:33:30,480
and i mean this will have a consequence

879
00:33:30,480 --> 00:33:31,919
on

880
00:33:31,919 --> 00:33:34,080
efficiency of active inference so i will

881
00:33:34,080 --> 00:33:36,080
show you this in a moment

882
00:33:36,080 --> 00:33:38,640
uh so how you wanna achieves basically

883
00:33:38,640 --> 00:33:40,080
this simple form is just by

884
00:33:40,080 --> 00:33:43,279
uh assuming that uh for large uh the

885
00:33:43,279 --> 00:33:45,519
this the gamma function has this type of

886
00:33:45,519 --> 00:33:47,120
approximation when x is

887
00:33:47,120 --> 00:33:49,600
sufficiently large right so this just

888
00:33:49,600 --> 00:33:50,720
means when you

889
00:33:50,720 --> 00:33:53,039
sample sufficient number of time each

890
00:33:53,039 --> 00:33:54,480
arm this will be

891
00:33:54,480 --> 00:33:57,279
more like very accurate approximation

892
00:33:57,279 --> 00:33:58,159
and we see that

893
00:33:58,159 --> 00:34:00,880
this approximate so algorithm and the

894
00:34:00,880 --> 00:34:03,120
exact they behave very similar so

895
00:34:03,120 --> 00:34:05,679
it is in a way reasonably good

896
00:34:05,679 --> 00:34:08,320
approximation

897
00:34:08,320 --> 00:34:11,040
so you actually uh danielle i think you

898
00:34:11,040 --> 00:34:13,679
asked like a question uh

899
00:34:13,679 --> 00:34:16,399
about the scaling properties of this

900
00:34:16,399 --> 00:34:16,800
right

901
00:34:16,800 --> 00:34:20,639
one kind of uh

902
00:34:20,639 --> 00:34:22,879
motivation for introducing approximation

903
00:34:22,879 --> 00:34:24,639
is that in a way the algorithm is much

904
00:34:24,639 --> 00:34:26,560
simpler so it scales better so you can

905
00:34:26,560 --> 00:34:27,359
kind of run it

906
00:34:27,359 --> 00:34:30,639
on much more arms easier

907
00:34:30,639 --> 00:34:33,199
uh however like right the advantage is

908
00:34:33,199 --> 00:34:34,719
not huge so maybe

909
00:34:34,719 --> 00:34:37,918
uh it scales slightly better but you're

910
00:34:37,918 --> 00:34:39,599
maybe gaining like 10

911
00:34:39,599 --> 00:34:43,280
or 20 performance in computation time

912
00:34:43,280 --> 00:34:46,000
uh so it's not something like uh which

913
00:34:46,000 --> 00:34:48,239
destroys completely the exact form

914
00:34:48,239 --> 00:34:50,639
at least not in this example right

915
00:34:50,639 --> 00:34:52,960
because the problem itself is simple

916
00:34:52,960 --> 00:34:54,960
however it helps us a bit understand but

917
00:34:54,960 --> 00:34:57,839
just get intuition what is happening so

918
00:34:57,839 --> 00:35:01,040
and uh at least like right how the

919
00:35:01,040 --> 00:35:06,000
exploration bonus changes with time

920
00:35:06,640 --> 00:35:10,000
uh okay so

921
00:35:10,000 --> 00:35:12,320
before i kind of just show some results

922
00:35:12,320 --> 00:35:13,440
uh uh

923
00:35:13,440 --> 00:35:15,440
like comparison of different algorithms

924
00:35:15,440 --> 00:35:16,800
i want to introduce

925
00:35:16,800 --> 00:35:20,240
uh just concepts which come from uh

926
00:35:20,240 --> 00:35:22,240
machine learning analysis of multi-arm

927
00:35:22,240 --> 00:35:24,079
bandits uh which people use like

928
00:35:24,079 --> 00:35:27,520
just to kind of rate the uh and compare

929
00:35:27,520 --> 00:35:28,880
different algorithms right

930
00:35:28,880 --> 00:35:30,800
how good they are in in solving this

931
00:35:30,800 --> 00:35:32,079
task

932
00:35:32,079 --> 00:35:34,960
and this is done by using something

933
00:35:34,960 --> 00:35:36,400
defining a regret

934
00:35:36,400 --> 00:35:38,400
so basically uh regret is simply a

935
00:35:38,400 --> 00:35:39,440
difference between

936
00:35:39,440 --> 00:35:42,960
uh what you did at trial t minus what

937
00:35:42,960 --> 00:35:44,400
was the best choice

938
00:35:44,400 --> 00:35:47,760
that uh at that trial

939
00:35:47,760 --> 00:35:50,880
and uh so kind of

940
00:35:50,880 --> 00:35:53,040
uh assumption here is that there is some

941
00:35:53,040 --> 00:35:53,920
oracle

942
00:35:53,920 --> 00:35:55,839
which is solving the task which knows

943
00:35:55,839 --> 00:35:57,280
exactly what was the best

944
00:35:57,280 --> 00:36:01,119
choice in in every trial right

945
00:36:02,720 --> 00:36:06,720
and so normally people kind of consider

946
00:36:06,720 --> 00:36:10,000
two uh two quantities either a

947
00:36:10,000 --> 00:36:11,680
cumulative regret which is just

948
00:36:11,680 --> 00:36:15,359
some uh of of regret

949
00:36:15,359 --> 00:36:18,400
over or up from first trial till the

950
00:36:18,400 --> 00:36:19,520
current one

951
00:36:19,520 --> 00:36:21,280
or a regret rate which is just the

952
00:36:21,280 --> 00:36:24,839
average over time of cumulative regret

953
00:36:24,839 --> 00:36:26,160
right

954
00:36:26,160 --> 00:36:29,280
and we will here use both just for

955
00:36:29,280 --> 00:36:32,000
visualizing different aspects of the

956
00:36:32,000 --> 00:36:36,560
algorithm uh and a kind of in stationary

957
00:36:36,560 --> 00:36:39,200
case at least this is a very kind of

958
00:36:39,200 --> 00:36:41,119
important result

959
00:36:41,119 --> 00:36:43,359
because all for all good algorithms one

960
00:36:43,359 --> 00:36:44,960
would expect that this regret

961
00:36:44,960 --> 00:36:48,240
goes to zero over time right so as you

962
00:36:48,240 --> 00:36:48,960
go to

963
00:36:48,960 --> 00:36:51,200
infinite number of trials you should be

964
00:36:51,200 --> 00:36:52,640
able to

965
00:36:52,640 --> 00:36:56,880
always do like a good choice

966
00:36:56,880 --> 00:36:58,960
uh and if this is the case then one can

967
00:36:58,960 --> 00:37:01,839
show that uh

968
00:37:02,560 --> 00:37:04,800
algorithms which have this property they

969
00:37:04,800 --> 00:37:06,000
are called also sympathetically

970
00:37:06,000 --> 00:37:06,560
efficient

971
00:37:06,560 --> 00:37:09,040
and they scale for large t's as

972
00:37:09,040 --> 00:37:09,920
something

973
00:37:09,920 --> 00:37:12,320
some terms times a logarithm over t

974
00:37:12,320 --> 00:37:14,079
right

975
00:37:14,079 --> 00:37:17,680
uh and so this is kind of um

976
00:37:17,680 --> 00:37:21,119
uh one important aspect of uh

977
00:37:21,119 --> 00:37:23,599
a slice for stationary case of different

978
00:37:23,599 --> 00:37:25,359
decision making algorithms

979
00:37:25,359 --> 00:37:28,560
so multi-arm bandit algorithms so they

980
00:37:28,560 --> 00:37:28,960
should

981
00:37:28,960 --> 00:37:31,760
at least scale as logarithm of t when

982
00:37:31,760 --> 00:37:32,720
you expand

983
00:37:32,720 --> 00:37:36,560
uh right when you go to large till limit

984
00:37:36,560 --> 00:37:39,200
otherwise right this first thing will

985
00:37:39,200 --> 00:37:39,760
not

986
00:37:39,760 --> 00:37:45,280
probably hold well or they can also

987
00:37:45,599 --> 00:37:48,720
scale slower than logarithmic but

988
00:37:48,720 --> 00:37:50,720
for example if they increase linear

989
00:37:50,720 --> 00:37:51,920
limit t

990
00:37:51,920 --> 00:37:55,359
this will not hold anymore

991
00:37:55,359 --> 00:37:59,440
uh okay so um

992
00:37:59,440 --> 00:38:03,040
now let's go to the comparison right uh

993
00:38:03,040 --> 00:38:05,760
we will look into how the misting

994
00:38:05,760 --> 00:38:07,119
thompson sampling bayesian upper

995
00:38:07,119 --> 00:38:08,480
confidence bound

996
00:38:08,480 --> 00:38:10,640
exact different inference algorithm and

997
00:38:10,640 --> 00:38:11,760
the approximate one

998
00:38:11,760 --> 00:38:14,960
uh compared to each other uh

999
00:38:14,960 --> 00:38:18,240
i will start first with just trying to

1000
00:38:18,240 --> 00:38:20,560
see what would be kind of a good value

1001
00:38:20,560 --> 00:38:23,040
for this lambda parameter

1002
00:38:23,040 --> 00:38:26,160
uh in different settings uh

1003
00:38:26,160 --> 00:38:29,359
and also to give some kind of initial

1004
00:38:29,359 --> 00:38:30,480
comparison of

1005
00:38:30,480 --> 00:38:33,200
exact uh algorithm and the approximate

1006
00:38:33,200 --> 00:38:34,079
one

1007
00:38:34,079 --> 00:38:37,200
so what we are i'm showing here is a

1008
00:38:37,200 --> 00:38:40,320
regret rate right for different kind of

1009
00:38:40,320 --> 00:38:41,119
snapshots

1010
00:38:41,119 --> 00:38:43,520
so these dotted lines are after 100

1011
00:38:43,520 --> 00:38:44,880
trials

1012
00:38:44,880 --> 00:38:47,040
uh the dotted dashed line is after a

1013
00:38:47,040 --> 00:38:48,000
thousand trials

1014
00:38:48,000 --> 00:38:51,280
and uh the solid line will be after 10

1015
00:38:51,280 --> 00:38:53,200
000 trials right

1016
00:38:53,200 --> 00:38:56,240
and so what one kind of

1017
00:38:56,240 --> 00:38:59,280
notices here is that there is some

1018
00:38:59,280 --> 00:39:01,280
obviously kind of minima for different

1019
00:39:01,280 --> 00:39:03,119
lambda so this kind of

1020
00:39:03,119 --> 00:39:07,520
preference parameter uh

1021
00:39:07,760 --> 00:39:12,079
and that the longer kind of uh well

1022
00:39:12,079 --> 00:39:14,000
the more trials you do the smaller the

1023
00:39:14,000 --> 00:39:16,320
lambda should be so this this is not

1024
00:39:16,320 --> 00:39:19,040
quite nice and this kind of has a

1025
00:39:19,040 --> 00:39:20,240
consequence

1026
00:39:20,240 --> 00:39:23,040
however we can like just pick some value

1027
00:39:23,040 --> 00:39:23,440
which

1028
00:39:23,440 --> 00:39:25,599
uh so for example this purple dotted

1029
00:39:25,599 --> 00:39:26,880
line shows like

1030
00:39:26,880 --> 00:39:30,079
around 0.1 lambda 0.1 which seems to

1031
00:39:30,079 --> 00:39:30,480
have

1032
00:39:30,480 --> 00:39:32,480
be close to minimum for most of these

1033
00:39:32,480 --> 00:39:36,560
cases right so we don't want to kind of

1034
00:39:36,560 --> 00:39:38,160
have different value for different

1035
00:39:38,160 --> 00:39:40,720
examples because i mean this is just not

1036
00:39:40,720 --> 00:39:43,839
practically feasible

1037
00:39:45,200 --> 00:39:46,640
you want to have a kind of general

1038
00:39:46,640 --> 00:39:48,720
algorithm which can be applied to many

1039
00:39:48,720 --> 00:39:50,640
different situations

1040
00:39:50,640 --> 00:39:54,240
at the same time so

1041
00:39:54,240 --> 00:39:58,000
what when we compare so bayesian ucb

1042
00:39:58,000 --> 00:39:59,839
domestic thompson sampling and just the

1043
00:39:59,839 --> 00:40:01,520
approximate active inference so

1044
00:40:01,520 --> 00:40:03,839
i'm just excluding this here the exact

1045
00:40:03,839 --> 00:40:04,560
one because

1046
00:40:04,560 --> 00:40:06,079
they will behave the same pretty much

1047
00:40:06,079 --> 00:40:08,960
for this specific parameter value

1048
00:40:08,960 --> 00:40:10,960
if we compare them in terms of

1049
00:40:10,960 --> 00:40:12,960
cumulative regret

1050
00:40:12,960 --> 00:40:16,560
we see that right the approximate active

1051
00:40:16,560 --> 00:40:18,000
inference is not asymptotically

1052
00:40:18,000 --> 00:40:19,040
efficient so this

1053
00:40:19,040 --> 00:40:23,359
curve just goes diverges over time

1054
00:40:23,359 --> 00:40:26,079
uh where's those if if you if you look

1055
00:40:26,079 --> 00:40:26,560
at the

1056
00:40:26,560 --> 00:40:29,359
kind of the great uh sorry the green and

1057
00:40:29,359 --> 00:40:31,119
the yellow curve they they kind of

1058
00:40:31,119 --> 00:40:32,640
flatten out

1059
00:40:32,640 --> 00:40:35,200
after some time and they get the slope

1060
00:40:35,200 --> 00:40:36,880
proportional to this dotted

1061
00:40:36,880 --> 00:40:39,359
lines which actually shows the slope of

1062
00:40:39,359 --> 00:40:41,359
this kind of uh

1063
00:40:41,359 --> 00:40:43,359
asymptotic limit right what you should

1064
00:40:43,359 --> 00:40:45,839
see for large t's

1065
00:40:45,839 --> 00:40:49,599
now uh so why is this happening so the

1066
00:40:49,599 --> 00:40:50,839
thing is that

1067
00:40:50,839 --> 00:40:53,599
um because this exploration

1068
00:40:53,599 --> 00:40:55,760
bound is kind of just decreasing over

1069
00:40:55,760 --> 00:40:56,960
time

1070
00:40:56,960 --> 00:41:00,000
uh active inference algorithm kind of

1071
00:41:00,000 --> 00:41:00,800
gets stuck

1072
00:41:00,800 --> 00:41:04,000
into the wrong solution with some

1073
00:41:04,000 --> 00:41:05,760
probability which depends on the

1074
00:41:05,760 --> 00:41:07,680
difficulty of the task so right

1075
00:41:07,680 --> 00:41:10,400
the smaller the epsilon is uh the more

1076
00:41:10,400 --> 00:41:11,440
likely and

1077
00:41:11,440 --> 00:41:15,200
the more arms you have

1078
00:41:15,200 --> 00:41:19,680
sorry for smaller epsilon and for small

1079
00:41:19,680 --> 00:41:20,000
number

1080
00:41:20,000 --> 00:41:21,839
of arms there is a higher probability

1081
00:41:21,839 --> 00:41:23,359
that you get kind of stuck

1082
00:41:23,359 --> 00:41:26,640
right and one can see this if kind of we

1083
00:41:26,640 --> 00:41:28,000
take a snapshot

1084
00:41:28,000 --> 00:41:30,880
of of different runs so this is kind of

1085
00:41:30,880 --> 00:41:33,599
a distribution of cumulative regret

1086
00:41:33,599 --> 00:41:36,240
i'm just plotting the logarithm here so

1087
00:41:36,240 --> 00:41:36,640
like

1088
00:41:36,640 --> 00:41:38,720
making a histogram of over different

1089
00:41:38,720 --> 00:41:40,160
runs of this is like

1090
00:41:40,160 --> 00:41:44,079
10 000 runs of different algorithms

1091
00:41:44,079 --> 00:41:47,200
and just showing at what value for the

1092
00:41:47,200 --> 00:41:49,200
logarithm of the cumulative regret they

1093
00:41:49,200 --> 00:41:50,240
end up

1094
00:41:50,240 --> 00:41:53,920
as you can see here there is for kind of

1095
00:41:53,920 --> 00:41:56,079
active inference based uh algorithm you

1096
00:41:56,079 --> 00:41:57,520
see this kind of

1097
00:41:57,520 --> 00:42:00,839
well spike here and in the tail of the

1098
00:42:00,839 --> 00:42:02,160
distribution

1099
00:42:02,160 --> 00:42:04,400
which is proportional to just doing

1100
00:42:04,400 --> 00:42:06,240
random choices so that means that

1101
00:42:06,240 --> 00:42:08,400
basically algorithm was just selecting

1102
00:42:08,400 --> 00:42:10,240
wrong arm constantly

1103
00:42:10,240 --> 00:42:13,119
it never it never converged to a correct

1104
00:42:13,119 --> 00:42:14,160
solution

1105
00:42:14,160 --> 00:42:16,800
in a way it gets stuck into a rock

1106
00:42:16,800 --> 00:42:18,880
solution because the exploration bow

1107
00:42:18,880 --> 00:42:22,240
was reduced to too soon

1108
00:42:22,240 --> 00:42:26,560
uh right so in it

1109
00:42:26,560 --> 00:42:30,079
so this is kind of uh say limitation or

1110
00:42:30,079 --> 00:42:31,760
application of active inference to

1111
00:42:31,760 --> 00:42:33,599
stationary problems

1112
00:42:33,599 --> 00:42:35,280
because this is not the feature of an

1113
00:42:35,280 --> 00:42:36,960
algorithms you would like to have

1114
00:42:36,960 --> 00:42:40,079
in a way uh normally if

1115
00:42:40,079 --> 00:42:43,040
if you're kind of so example of this

1116
00:42:43,040 --> 00:42:43,520
would be

1117
00:42:43,520 --> 00:42:45,520
for example optimization problems that

1118
00:42:45,520 --> 00:42:47,040
you want to find

1119
00:42:47,040 --> 00:42:49,680
the best solution for a set of

1120
00:42:49,680 --> 00:42:51,520
parameters

1121
00:42:51,520 --> 00:42:54,560
uh now for example

1122
00:42:54,560 --> 00:42:56,560
by asian optimization finding the

1123
00:42:56,560 --> 00:43:00,720
minimum of what some unknown function

1124
00:43:00,800 --> 00:43:03,359
uses a thompson sampling this is kind of

1125
00:43:03,359 --> 00:43:05,280
seems to be a very efficient way of

1126
00:43:05,280 --> 00:43:08,560
finding the minimum however if you would

1127
00:43:08,560 --> 00:43:10,000
apply to such situation

1128
00:43:10,000 --> 00:43:13,359
an active inference basis um kind of

1129
00:43:13,359 --> 00:43:16,720
arm selection or sample selection uh

1130
00:43:16,720 --> 00:43:18,160
there is a chance that the algorithm

1131
00:43:18,160 --> 00:43:20,160
fails right you just kind of get stuck

1132
00:43:20,160 --> 00:43:20,880
in the wrong

1133
00:43:20,880 --> 00:43:22,800
minimum wrong optima it doesn't do

1134
00:43:22,800 --> 00:43:25,359
sufficient exploration

1135
00:43:25,359 --> 00:43:28,400
uh so this kind of uh requires

1136
00:43:28,400 --> 00:43:30,720
potentially some adjustments to how

1137
00:43:30,720 --> 00:43:32,720
actions are selected in at least in the

1138
00:43:32,720 --> 00:43:34,960
stationary case

1139
00:43:34,960 --> 00:43:38,839
uh so just to

1140
00:43:38,839 --> 00:43:41,839
okay

1141
00:43:42,240 --> 00:43:44,560
strange yeah i got some string slides

1142
00:43:44,560 --> 00:43:45,440
just to kind of

1143
00:43:45,440 --> 00:43:48,800
uh know a bit uh what the short term

1144
00:43:48,800 --> 00:43:51,200
behavior looks like

1145
00:43:51,200 --> 00:43:54,000
so from the perspective of kind of

1146
00:43:54,000 --> 00:43:55,680
cognitive neuroscience or like

1147
00:43:55,680 --> 00:43:57,440
human decision making you don't really

1148
00:43:57,440 --> 00:44:00,480
care about this asymptotic limit

1149
00:44:00,480 --> 00:44:02,240
because you don't usually expect people

1150
00:44:02,240 --> 00:44:03,839
to be in a either

1151
00:44:03,839 --> 00:44:06,240
stationary environment things always

1152
00:44:06,240 --> 00:44:07,520
change

1153
00:44:07,520 --> 00:44:09,599
or write that they kind of have to

1154
00:44:09,599 --> 00:44:10,800
repeat actions

1155
00:44:10,800 --> 00:44:15,040
so many times so what i'm showing here

1156
00:44:15,040 --> 00:44:15,920
now just

1157
00:44:15,920 --> 00:44:18,880
in a very reduced example so if we have

1158
00:44:18,880 --> 00:44:20,640
only three arms

1159
00:44:20,640 --> 00:44:22,640
and we just use kind of different

1160
00:44:22,640 --> 00:44:23,920
epsilon values so

1161
00:44:23,920 --> 00:44:26,160
uh task difficulties what is the

1162
00:44:26,160 --> 00:44:27,280
probability that

1163
00:44:27,280 --> 00:44:30,240
uh for different algorithms that you

1164
00:44:30,240 --> 00:44:33,200
select actually the optimal are

1165
00:44:33,200 --> 00:44:38,480
and as one can see that right initially

1166
00:44:40,880 --> 00:44:45,119
so by asian ucb for first maybe 25

1167
00:44:45,119 --> 00:44:45,680
trials

1168
00:44:45,680 --> 00:44:48,880
has the highest probability to select an

1169
00:44:48,880 --> 00:44:52,480
arm however there is a range of trials

1170
00:44:52,480 --> 00:44:53,359
like from

1171
00:44:53,359 --> 00:44:56,720
50 to maybe 1000 where

1172
00:44:56,720 --> 00:44:58,400
active inference based algorithm takes

1173
00:44:58,400 --> 00:45:00,000
over so right

1174
00:45:00,000 --> 00:45:03,680
in a way because of this information

1175
00:45:03,680 --> 00:45:04,079
game

1176
00:45:04,079 --> 00:45:06,560
term active inference is more efficient

1177
00:45:06,560 --> 00:45:08,480
in uh

1178
00:45:08,480 --> 00:45:12,319
targeting like the arms which will give

1179
00:45:12,319 --> 00:45:14,400
you the most information has

1180
00:45:14,400 --> 00:45:18,880
hence it can recover uh the best arm in

1181
00:45:18,880 --> 00:45:20,960
kind of some intermediate interval with

1182
00:45:20,960 --> 00:45:22,560
the highest probability

1183
00:45:22,560 --> 00:45:26,400
however as you kind of expand this after

1184
00:45:26,400 --> 00:45:28,800
like thousand trials or so you see that

1185
00:45:28,800 --> 00:45:30,720
this probability gets stuck so it never

1186
00:45:30,720 --> 00:45:32,319
converges to one

1187
00:45:32,319 --> 00:45:34,400
unlike the other ogre this is especially

1188
00:45:34,400 --> 00:45:35,920
evident for this difficult right

1189
00:45:35,920 --> 00:45:37,040
difficult problem small

1190
00:45:37,040 --> 00:45:40,720
epsilon and and so this is uh

1191
00:45:40,720 --> 00:45:42,720
in a way explanations of what happens so

1192
00:45:42,720 --> 00:45:44,400
basically algorithm although

1193
00:45:44,400 --> 00:45:47,920
it reaches good solutions after

1194
00:45:47,920 --> 00:45:50,000
it's hyper higher probability than other

1195
00:45:50,000 --> 00:45:51,520
algorithms

1196
00:45:51,520 --> 00:45:55,520
there are still a lot of uh um

1197
00:45:56,720 --> 00:45:58,880
lots of examples so in this kind of

1198
00:45:58,880 --> 00:46:00,240
simulations uh

1199
00:46:00,240 --> 00:46:02,720
parallel simulations which get stuck to

1200
00:46:02,720 --> 00:46:03,599
a wrong

1201
00:46:03,599 --> 00:46:06,640
solution and they cannot get out of this

1202
00:46:06,640 --> 00:46:08,480
so in a way right this kind of asked

1203
00:46:08,480 --> 00:46:10,079
question okay what can one do

1204
00:46:10,079 --> 00:46:12,079
to make active inference also synthotic

1205
00:46:12,079 --> 00:46:14,560
efficiency so how can the

1206
00:46:14,560 --> 00:46:16,960
maybe either generative model be changed

1207
00:46:16,960 --> 00:46:17,760
to

1208
00:46:17,760 --> 00:46:21,280
support increasing the exploration bound

1209
00:46:21,280 --> 00:46:22,400
over time

1210
00:46:22,400 --> 00:46:25,040
or maybe introduce kind of instead of

1211
00:46:25,040 --> 00:46:25,680
computing

1212
00:46:25,680 --> 00:46:29,920
expected expectations in um

1213
00:46:29,920 --> 00:46:32,319
of expected free energy one can also

1214
00:46:32,319 --> 00:46:34,160
just draw samples

1215
00:46:34,160 --> 00:46:36,880
from a posterior and kind of also

1216
00:46:36,880 --> 00:46:38,079
compute this

1217
00:46:38,079 --> 00:46:40,319
these two terms like information gain

1218
00:46:40,319 --> 00:46:42,480
and similar to like thompson sampling so

1219
00:46:42,480 --> 00:46:44,079
right there are kind of

1220
00:46:44,079 --> 00:46:46,079
different ways one can think of how to

1221
00:46:46,079 --> 00:46:47,680
add uh

1222
00:46:47,680 --> 00:46:50,240
exploration bonus another third option

1223
00:46:50,240 --> 00:46:51,280
would be to actually

1224
00:46:51,280 --> 00:46:53,200
introduce learning of the lambda

1225
00:46:53,200 --> 00:46:54,560
parameter

1226
00:46:54,560 --> 00:46:57,119
so that lambda itself kind of goes down

1227
00:46:57,119 --> 00:46:57,920
over time

1228
00:46:57,920 --> 00:47:01,440
so it kind of goes towards zero uh

1229
00:47:01,440 --> 00:47:04,480
in with specific uh

1230
00:47:04,480 --> 00:47:07,280
uh specific rules however uh yeah

1231
00:47:07,280 --> 00:47:09,119
currently we don't have very a good

1232
00:47:09,119 --> 00:47:10,000
solution for this

1233
00:47:10,000 --> 00:47:14,160
so we just leave it to this that has a

1234
00:47:14,160 --> 00:47:16,720
uh obvious limitation of just applying

1235
00:47:16,720 --> 00:47:18,640
active inference to this type of

1236
00:47:18,640 --> 00:47:22,800
problems um

1237
00:47:22,800 --> 00:47:24,880
are there kind of maybe any questions i

1238
00:47:24,880 --> 00:47:25,920
think uh

1239
00:47:25,920 --> 00:47:27,440
this would be like the half where we

1240
00:47:27,440 --> 00:47:29,680
switch now to the

1241
00:47:29,680 --> 00:47:32,720
perfect the other

1242
00:47:32,720 --> 00:47:36,160
um if anybody has any thoughts

1243
00:47:36,160 --> 00:47:36,880
definitely

1244
00:47:36,880 --> 00:47:41,040
i could ask some things or also

1245
00:47:41,040 --> 00:47:42,800
we'll ask if in the live chat people

1246
00:47:42,800 --> 00:47:46,000
want to post any questions

1247
00:47:46,000 --> 00:47:50,079
but um how much longer of a presentation

1248
00:47:50,079 --> 00:47:52,000
did you estimate that you had so that we

1249
00:47:52,000 --> 00:47:54,000
could kind of

1250
00:47:54,000 --> 00:47:56,000
also address some general points here

1251
00:47:56,000 --> 00:47:58,640
during this point one

1252
00:47:58,640 --> 00:48:00,880
um well i don't think there is more than

1253
00:48:00,880 --> 00:48:02,160
maybe 20 30

1254
00:48:02,160 --> 00:48:05,200
minutes max i didn't really

1255
00:48:05,200 --> 00:48:08,480
uh gauge it but there is less less

1256
00:48:08,480 --> 00:48:10,559
slides in the second question

1257
00:48:10,559 --> 00:48:13,119
would it make more sense to go through

1258
00:48:13,119 --> 00:48:14,000
it quickly

1259
00:48:14,000 --> 00:48:16,240
here yeah or would it make sense to do

1260
00:48:16,240 --> 00:48:17,280
it in the

1261
00:48:17,280 --> 00:48:21,680
next weeks uh i see which i think uh

1262
00:48:21,680 --> 00:48:23,839
both are fine for me i mean maybe

1263
00:48:23,839 --> 00:48:25,040
additional 15 minutes

1264
00:48:25,040 --> 00:48:27,760
okay so we'll complete the presentation

1265
00:48:27,760 --> 00:48:28,880
and in live chat

1266
00:48:28,880 --> 00:48:32,079
and on the video chat here we'll compile

1267
00:48:32,079 --> 00:48:33,200
our questions and then

1268
00:48:33,200 --> 00:48:35,440
in the remainder after your presentation

1269
00:48:35,440 --> 00:48:37,040
today and then next week we can have

1270
00:48:37,040 --> 00:48:37,520
more

1271
00:48:37,520 --> 00:48:39,440
open discussions so please continue

1272
00:48:39,440 --> 00:48:42,559
thanks okay

1273
00:48:42,559 --> 00:48:46,400
okay so uh now we

1274
00:48:46,400 --> 00:48:49,040
go to this like dynamic non-stationary

1275
00:48:49,040 --> 00:48:51,040
problem

1276
00:48:51,040 --> 00:48:55,359
uh and on any trial uh

1277
00:48:55,359 --> 00:48:57,440
uh in this case right it's this very

1278
00:48:57,440 --> 00:48:59,200
similar setup on any trial an agent

1279
00:48:59,200 --> 00:49:00,319
makes a choice between k

1280
00:49:00,319 --> 00:49:02,880
arms again we are focusing on the

1281
00:49:02,880 --> 00:49:04,319
bernoulli bandits so

1282
00:49:04,319 --> 00:49:06,400
outcomes are either just binary

1283
00:49:06,400 --> 00:49:08,960
variables

1284
00:49:09,119 --> 00:49:12,480
however what happens here is that the

1285
00:49:12,480 --> 00:49:14,720
reward probabilities associated with

1286
00:49:14,720 --> 00:49:17,040
charm change over time right and we kind

1287
00:49:17,040 --> 00:49:18,800
of differentiate between switching

1288
00:49:18,800 --> 00:49:23,680
bandits uh where we will assume that the

1289
00:49:23,680 --> 00:49:25,760
changes happen at the same time on all

1290
00:49:25,760 --> 00:49:26,720
arms

1291
00:49:26,720 --> 00:49:28,400
and furthermore in switching bandits

1292
00:49:28,400 --> 00:49:30,480
they are kind of also called piecewise

1293
00:49:30,480 --> 00:49:32,720
stationary so there is like a period

1294
00:49:32,720 --> 00:49:34,400
where nothing changes and then there is

1295
00:49:34,400 --> 00:49:35,920
just one moment in time when

1296
00:49:35,920 --> 00:49:38,480
uh rewards reward probabilities on all

1297
00:49:38,480 --> 00:49:40,480
arms change

1298
00:49:40,480 --> 00:49:43,520
or uh we can think about a call of

1299
00:49:43,520 --> 00:49:45,200
another variant of dynamic bandits would

1300
00:49:45,200 --> 00:49:46,480
be restless bandits

1301
00:49:46,480 --> 00:49:48,240
where changes happen independently on

1302
00:49:48,240 --> 00:49:50,160
each arm and they are continuously

1303
00:49:50,160 --> 00:49:50,640
changing

1304
00:49:50,640 --> 00:49:52,720
over time so for example following a

1305
00:49:52,720 --> 00:49:53,839
random walk

1306
00:49:53,839 --> 00:49:56,000
i will only talk about switching bandits

1307
00:49:56,000 --> 00:49:57,599
but

1308
00:49:57,599 --> 00:50:00,720
from some testing i did

1309
00:50:00,720 --> 00:50:02,880
all the results generalized also to the

1310
00:50:02,880 --> 00:50:05,599
restless case

1311
00:50:07,119 --> 00:50:10,319
and in this kind of beside this

1312
00:50:10,319 --> 00:50:12,480
number of arms and the difference

1313
00:50:12,480 --> 00:50:14,240
between the best arm and the

1314
00:50:14,240 --> 00:50:15,680
other arms of kind of reward

1315
00:50:15,680 --> 00:50:17,520
probabilities epsilon and

1316
00:50:17,520 --> 00:50:20,480
k we have another task difficulty this

1317
00:50:20,480 --> 00:50:21,680
is the rate of change

1318
00:50:21,680 --> 00:50:24,319
or like just change probability so the

1319
00:50:24,319 --> 00:50:25,839
more often things change the more

1320
00:50:25,839 --> 00:50:28,400
difficult the task is first

1321
00:50:28,400 --> 00:50:31,520
especially if you have uh many arms

1322
00:50:31,520 --> 00:50:34,800
um and uh

1323
00:50:34,800 --> 00:50:37,119
i will consider like switching bandits

1324
00:50:37,119 --> 00:50:38,559
with fixed difficulty which

1325
00:50:38,559 --> 00:50:42,079
is just extrapolation of the stationary

1326
00:50:42,079 --> 00:50:42,720
case to

1327
00:50:42,720 --> 00:50:46,800
uh by introducing changes to about

1328
00:50:46,800 --> 00:50:48,400
to which arm is associated with the

1329
00:50:48,400 --> 00:50:50,400
maximal reward right

1330
00:50:50,400 --> 00:50:52,880
we will always have the same rewards on

1331
00:50:52,880 --> 00:50:55,440
on all arms it is just that from time to

1332
00:50:55,440 --> 00:50:56,319
time

1333
00:50:56,319 --> 00:50:58,720
optimal arm changes with probability rho

1334
00:50:58,720 --> 00:51:00,559
right

1335
00:51:00,559 --> 00:51:03,359
and in this case we have right just

1336
00:51:03,359 --> 00:51:04,880
three parameters which

1337
00:51:04,880 --> 00:51:08,559
uh define our task that's difficult

1338
00:51:08,559 --> 00:51:10,559
we can also think about switching

1339
00:51:10,559 --> 00:51:11,680
bandits with

1340
00:51:11,680 --> 00:51:14,559
varying difficulty uh which then just

1341
00:51:14,559 --> 00:51:16,319
means that

1342
00:51:16,319 --> 00:51:18,400
with probability rho uh the reward

1343
00:51:18,400 --> 00:51:20,079
probabilities associated with

1344
00:51:20,079 --> 00:51:23,359
each arm either remain fixed on the next

1345
00:51:23,359 --> 00:51:24,559
trial so they are just kind of

1346
00:51:24,559 --> 00:51:26,160
translated

1347
00:51:26,160 --> 00:51:29,119
or they are sampled uh sorry with some

1348
00:51:29,119 --> 00:51:30,000
probability

1349
00:51:30,000 --> 00:51:32,160
one minus row should be here right they

1350
00:51:32,160 --> 00:51:34,000
are staying fixed or with probability

1351
00:51:34,000 --> 00:51:34,400
rho

1352
00:51:34,400 --> 00:51:35,920
they are just sampled from a uniform

1353
00:51:35,920 --> 00:51:37,440
distribution in this case

1354
00:51:37,440 --> 00:51:41,119
beta distribution with parameters one

1355
00:51:41,119 --> 00:51:44,160
uh and in this

1356
00:51:44,160 --> 00:51:46,720
in this variant of the task right we we

1357
00:51:46,720 --> 00:51:47,440
just have

1358
00:51:47,440 --> 00:51:50,640
ks and row as a fixed uh difficult as

1359
00:51:50,640 --> 00:51:51,920
difficulty parameters

1360
00:51:51,920 --> 00:51:54,000
so active one kind of disappears so

1361
00:51:54,000 --> 00:51:55,920
you're kind of averaging over epsilon in

1362
00:51:55,920 --> 00:51:57,920
the task

1363
00:51:57,920 --> 00:52:00,880
um and just to give you an example also

1364
00:52:00,880 --> 00:52:02,160
we will not discuss this but how

1365
00:52:02,160 --> 00:52:03,200
restless bandits

1366
00:52:03,200 --> 00:52:05,680
uh setup looks like is basically you can

1367
00:52:05,680 --> 00:52:06,880
uh

1368
00:52:06,880 --> 00:52:11,599
uh you assume for example that the logic

1369
00:52:11,599 --> 00:52:12,800
transform of

1370
00:52:12,800 --> 00:52:14,480
of reward probability just follows a

1371
00:52:14,480 --> 00:52:17,040
random walk so it's just a

1372
00:52:17,040 --> 00:52:20,160
brownian exploration in the logic space

1373
00:52:20,160 --> 00:52:21,920
of the reward probabilities

1374
00:52:21,920 --> 00:52:23,920
and this would also require potentially

1375
00:52:23,920 --> 00:52:25,680
changing the generative model which i

1376
00:52:25,680 --> 00:52:26,960
will introduce but

1377
00:52:26,960 --> 00:52:30,319
it's not necessary so one gets pretty

1378
00:52:30,319 --> 00:52:33,599
similar solutions

1379
00:52:33,599 --> 00:52:36,720
and behavior so um

1380
00:52:36,720 --> 00:52:39,760
now uh to come back to

1381
00:52:39,760 --> 00:52:43,520
the example from before uh if we have

1382
00:52:43,520 --> 00:52:43,839
like

1383
00:52:43,839 --> 00:52:47,119
multi-arm bandit with four arms

1384
00:52:47,119 --> 00:52:50,559
uh the setup is exactly the same

1385
00:52:50,559 --> 00:52:53,200
and in addition we assume that the agent

1386
00:52:53,200 --> 00:52:55,280
has access to the underlying probability

1387
00:52:55,280 --> 00:52:57,119
of change so this is not something which

1388
00:52:57,119 --> 00:52:58,559
is unknown

1389
00:52:58,559 --> 00:53:01,920
this simplifies uh the

1390
00:53:01,920 --> 00:53:03,760
learning rules and belief update

1391
00:53:03,760 --> 00:53:05,119
equations

1392
00:53:05,119 --> 00:53:08,240
uh however one can extend this what i

1393
00:53:08,240 --> 00:53:09,599
will introduce today to

1394
00:53:09,599 --> 00:53:11,920
the setups where the probability of

1395
00:53:11,920 --> 00:53:12,960
change has to be

1396
00:53:12,960 --> 00:53:15,599
learned also or that the probability of

1397
00:53:15,599 --> 00:53:17,280
change is also something which changes

1398
00:53:17,280 --> 00:53:18,319
over time so that

1399
00:53:18,319 --> 00:53:22,079
right you can have to track how often uh

1400
00:53:22,079 --> 00:53:23,920
reward probabilities change in different

1401
00:53:23,920 --> 00:53:25,520
times

1402
00:53:25,520 --> 00:53:27,839
uh so this will be kind of example of

1403
00:53:27,839 --> 00:53:30,480
decision making in volatile right

1404
00:53:30,480 --> 00:53:34,480
environments uh

1405
00:53:34,480 --> 00:53:38,640
so uh to visualize the algorithm

1406
00:53:38,640 --> 00:53:41,359
and the basically the only difference is

1407
00:53:41,359 --> 00:53:42,240
practically

1408
00:53:42,240 --> 00:53:45,280
that now you have an effective

1409
00:53:45,280 --> 00:53:46,800
forgetting

1410
00:53:46,800 --> 00:53:49,920
of what you uh you know what what agent

1411
00:53:49,920 --> 00:53:51,359
learned before

1412
00:53:51,359 --> 00:53:53,280
and one can see this for example if you

1413
00:53:53,280 --> 00:53:55,119
look uh

1414
00:53:55,119 --> 00:53:58,480
uh on this square to the left

1415
00:53:58,480 --> 00:54:01,440
uh as kind of time evolves and agent is

1416
00:54:01,440 --> 00:54:03,760
selecting other arms

1417
00:54:03,760 --> 00:54:06,800
uh this value which uh

1418
00:54:06,800 --> 00:54:08,559
reward probability associated with the

1419
00:54:08,559 --> 00:54:10,160
leftmost arm

1420
00:54:10,160 --> 00:54:13,520
just decays back to uniform probability

1421
00:54:13,520 --> 00:54:15,119
right so in a way

1422
00:54:15,119 --> 00:54:17,599
agent is forgetting information or

1423
00:54:17,599 --> 00:54:19,440
expectations it had about

1424
00:54:19,440 --> 00:54:22,559
this arm and it assumes that with time

1425
00:54:22,559 --> 00:54:26,000
the reward probability beliefs about

1426
00:54:26,000 --> 00:54:28,000
reward probability will revert back to a

1427
00:54:28,000 --> 00:54:31,040
uniform distribution

1428
00:54:32,640 --> 00:54:35,119
and the algorithm is really a

1429
00:54:35,119 --> 00:54:36,799
straightforward

1430
00:54:36,799 --> 00:54:39,599
extension of what i already described

1431
00:54:39,599 --> 00:54:40,559
before

1432
00:54:40,559 --> 00:54:43,599
so the generator model uh now is

1433
00:54:43,599 --> 00:54:45,359
slightly more complex so be

1434
00:54:45,359 --> 00:54:49,520
besides the the likelihood

1435
00:54:49,520 --> 00:54:51,280
term so observation likelihood which

1436
00:54:51,280 --> 00:54:52,960
remains the same is just the bernoulli

1437
00:54:52,960 --> 00:54:54,480
distribution

1438
00:54:54,480 --> 00:54:56,559
now we have a kind of state transition

1439
00:54:56,559 --> 00:54:58,000
term

1440
00:54:58,000 --> 00:55:01,440
uh which tells us how reward

1441
00:55:01,440 --> 00:55:03,280
probabilities change over time so and

1442
00:55:03,280 --> 00:55:06,319
what this means is that if uh

1443
00:55:06,319 --> 00:55:09,359
one believes that there is a change uh

1444
00:55:09,359 --> 00:55:11,280
reward probability will be independent

1445
00:55:11,280 --> 00:55:12,720
from the previous values

1446
00:55:12,720 --> 00:55:15,839
and they are just drawn they just

1447
00:55:15,839 --> 00:55:17,920
belong to a uniform distribution so this

1448
00:55:17,920 --> 00:55:19,680
uh kind of prior

1449
00:55:19,680 --> 00:55:23,280
belief and if there is no change uh

1450
00:55:23,280 --> 00:55:25,119
our the transition corresponds to a

1451
00:55:25,119 --> 00:55:26,720
delta function which just means that

1452
00:55:26,720 --> 00:55:30,880
uh uh that the reward probabilities stay

1453
00:55:30,880 --> 00:55:33,599
unchanged from trial t to t minus one to

1454
00:55:33,599 --> 00:55:35,119
t

1455
00:55:35,119 --> 00:55:38,640
and finally uh like the prior on

1456
00:55:38,640 --> 00:55:41,119
each trial we have the same prior about

1457
00:55:41,119 --> 00:55:43,119
probability of changes and this is just

1458
00:55:43,119 --> 00:55:43,839
again a

1459
00:55:43,839 --> 00:55:46,319
bernoulli distribution with probability

1460
00:55:46,319 --> 00:55:47,119
rho

1461
00:55:47,119 --> 00:55:49,280
which we here we will just assume this

1462
00:55:49,280 --> 00:55:51,040
is a known parameter

1463
00:55:51,040 --> 00:55:55,280
to the agents right so uh

1464
00:55:55,280 --> 00:55:57,760
the problem here in like dynamic cases

1465
00:55:57,760 --> 00:55:59,839
uh you can still apply

1466
00:55:59,839 --> 00:56:03,760
the the bias rule and you can compute

1467
00:56:03,760 --> 00:56:05,040
the posterior

1468
00:56:05,040 --> 00:56:07,200
both for the change probability terms of

1469
00:56:07,200 --> 00:56:08,480
jt

1470
00:56:08,480 --> 00:56:10,559
and for the post marginal posterior

1471
00:56:10,559 --> 00:56:12,880
about reward probabilities however

1472
00:56:12,880 --> 00:56:16,000
as you see here the exact kind of form

1473
00:56:16,000 --> 00:56:17,839
of the posterior is

1474
00:56:17,839 --> 00:56:20,000
not anymore doesn't belong to like

1475
00:56:20,000 --> 00:56:20,960
conjugate

1476
00:56:20,960 --> 00:56:23,119
so the prior is not any more conjugate

1477
00:56:23,119 --> 00:56:24,400
probability distribution to the

1478
00:56:24,400 --> 00:56:26,480
likelihood

1479
00:56:26,480 --> 00:56:28,880
and this is not anymore a simple beta

1480
00:56:28,880 --> 00:56:30,079
distribution but it's

1481
00:56:30,079 --> 00:56:32,240
becomes a mixture of beta distributions

1482
00:56:32,240 --> 00:56:34,400
and as you are kind of evolving into the

1483
00:56:34,400 --> 00:56:36,559
future this becomes larger and larger

1484
00:56:36,559 --> 00:56:39,119
mixture of beta distributions which is

1485
00:56:39,119 --> 00:56:42,240
uh well practically intractable right if

1486
00:56:42,240 --> 00:56:43,839
you kind of expand this to

1487
00:56:43,839 --> 00:56:47,119
open any and the number of trials

1488
00:56:47,119 --> 00:56:48,799
so because of that we want to have

1489
00:56:48,799 --> 00:56:50,480
something which is a bit more efficient

1490
00:56:50,480 --> 00:56:51,440
algorithm

1491
00:56:51,440 --> 00:56:53,760
uh we can basically introduce a mean

1492
00:56:53,760 --> 00:56:55,440
field approximation

1493
00:56:55,440 --> 00:56:57,280
when we now say that okay our

1494
00:56:57,280 --> 00:56:59,200
probability distribution can be

1495
00:56:59,200 --> 00:57:02,240
described as a product of a bunch of

1496
00:57:02,240 --> 00:57:04,319
beta distribution and a categorical

1497
00:57:04,319 --> 00:57:05,440
distribution which

1498
00:57:05,440 --> 00:57:07,760
just tells us the probability of change

1499
00:57:07,760 --> 00:57:10,799
right on trial t

1500
00:57:10,839 --> 00:57:14,558
uh and uh

1501
00:57:14,720 --> 00:57:17,200
how one what what this corresponds to

1502
00:57:17,200 --> 00:57:18,799
here so basically uh

1503
00:57:18,799 --> 00:57:22,640
uh what is actually we are using here is

1504
00:57:22,640 --> 00:57:25,839
a bit of uh it's not a standard

1505
00:57:25,839 --> 00:57:27,599
variational inference so where you would

1506
00:57:27,599 --> 00:57:29,839
have to kind of compute the gradient

1507
00:57:29,839 --> 00:57:32,000
over the radiation free energy to find

1508
00:57:32,000 --> 00:57:34,079
the optima this simplifies the things

1509
00:57:34,079 --> 00:57:36,559
because you just need one step

1510
00:57:36,559 --> 00:57:40,720
to update parameters both about change

1511
00:57:40,720 --> 00:57:43,359
and change probability and about right

1512
00:57:43,359 --> 00:57:45,680
reward probabilities

1513
00:57:45,680 --> 00:57:47,919
uh

1514
00:57:49,200 --> 00:57:51,440
this makes it not super optimal so there

1515
00:57:51,440 --> 00:57:53,359
are better solutions how i can do this

1516
00:57:53,359 --> 00:57:56,160
but it's very efficient so and

1517
00:57:56,160 --> 00:57:58,400
in in the end for the bernoulli bandits

1518
00:57:58,400 --> 00:58:00,240
this is there is will not be much

1519
00:58:00,240 --> 00:58:03,839
difference uh you can use better

1520
00:58:03,839 --> 00:58:05,440
algorithms

1521
00:58:05,440 --> 00:58:07,920
by uh this gives you just marginal

1522
00:58:07,920 --> 00:58:08,799
advantage

1523
00:58:08,799 --> 00:58:12,160
on the long run uh just because the

1524
00:58:12,160 --> 00:58:14,000
the problem is very noisy and it's very

1525
00:58:14,000 --> 00:58:15,200
difficult to actually

1526
00:58:15,200 --> 00:58:17,839
figure out the correct choice so what

1527
00:58:17,839 --> 00:58:18,640
this

1528
00:58:18,640 --> 00:58:20,240
variational smile does so it was

1529
00:58:20,240 --> 00:58:22,079
introduced by basilic

1530
00:58:22,079 --> 00:58:25,720
uh in well quite recently

1531
00:58:25,720 --> 00:58:29,440
2021 uh

1532
00:58:29,440 --> 00:58:32,000
so they provide you a bit more detailed

1533
00:58:32,000 --> 00:58:34,319
justification for what i'm

1534
00:58:34,319 --> 00:58:35,599
what i'm saying here i'm just

1535
00:58:35,599 --> 00:58:37,680
paraphrasing paraphrasing a bit

1536
00:58:37,680 --> 00:58:40,240
uh how the algorithm is defined so

1537
00:58:40,240 --> 00:58:42,000
basically we can

1538
00:58:42,000 --> 00:58:44,240
associate the marginal about change

1539
00:58:44,240 --> 00:58:46,559
probability with the exact

1540
00:58:46,559 --> 00:58:48,480
posterior marginal because you can

1541
00:58:48,480 --> 00:58:50,559
compute this analytically

1542
00:58:50,559 --> 00:58:54,400
and then we use this as a

1543
00:58:54,400 --> 00:58:57,280
basically uh

1544
00:59:00,000 --> 00:59:02,319
known known belief about change

1545
00:59:02,319 --> 00:59:05,040
probability to estimate uh

1546
00:59:05,040 --> 00:59:08,079
to estimate re reward probabilities by

1547
00:59:08,079 --> 00:59:10,880
right averaging in the log space uh over

1548
00:59:10,880 --> 00:59:11,680
over different

1549
00:59:11,680 --> 00:59:14,799
uh prior beliefs about

1550
00:59:14,799 --> 00:59:16,400
reward probabilities right so basically

1551
00:59:16,400 --> 00:59:18,160
you're instead of

1552
00:59:18,160 --> 00:59:19,839
averaging in the probability space

1553
00:59:19,839 --> 00:59:22,079
you're you're kind of um averaging in

1554
00:59:22,079 --> 00:59:24,640
the in the log space here

1555
00:59:24,640 --> 00:59:27,440
uh what this kind of results is very in

1556
00:59:27,440 --> 00:59:29,839
a very simple set of update rules so

1557
00:59:29,839 --> 00:59:32,880
on the left side we just are just

1558
00:59:32,880 --> 00:59:34,160
showing how omega t

1559
00:59:34,160 --> 00:59:38,960
is updated and this is um

1560
00:59:39,119 --> 00:59:42,160
just correspond basically to

1561
00:59:42,160 --> 00:59:44,960
forming beliefs about change probability

1562
00:59:44,960 --> 00:59:47,520
based on bias factor

1563
00:59:47,520 --> 00:59:48,960
shown here right which is just the

1564
00:59:48,960 --> 00:59:51,280
likelihood between uh

1565
00:59:51,280 --> 00:59:53,200
observing ot given that the change

1566
00:59:53,200 --> 00:59:55,280
didn't happen and observing ot given to

1567
00:59:55,280 --> 00:59:56,400
the change happened

1568
00:59:56,400 --> 01:00:00,319
right at the current trial

1569
01:00:00,319 --> 01:00:03,920
and then using that estimate to update

1570
01:00:03,920 --> 01:00:06,799
uh your beliefs about different arm

1571
01:00:06,799 --> 01:00:07,599
probabilities

1572
01:00:07,599 --> 01:00:10,640
and basically um depending whether you

1573
01:00:10,640 --> 01:00:12,240
selected the arm or not so

1574
01:00:12,240 --> 01:00:15,040
right basically the omega term here

1575
01:00:15,040 --> 01:00:16,559
plays this a forgetting

1576
01:00:16,559 --> 01:00:20,559
rate so the uh the larger the omega the

1577
01:00:20,559 --> 01:00:21,760
closer to one so the

1578
01:00:21,760 --> 01:00:23,760
larger the probability the change occur

1579
01:00:23,760 --> 01:00:26,799
the more you will revert back to the

1580
01:00:26,799 --> 01:00:28,880
beta0 and alpha zero parameters so the

1581
01:00:28,880 --> 01:00:30,079
initial prior belief

1582
01:00:30,079 --> 01:00:32,640
unless you will depend on your current

1583
01:00:32,640 --> 01:00:34,640
uh current beliefs

1584
01:00:34,640 --> 01:00:38,559
from the kind of previous trial uh

1585
01:00:38,559 --> 01:00:42,480
and this also has a important limit like

1586
01:00:42,480 --> 01:00:44,079
right if the agent believes that there

1587
01:00:44,079 --> 01:00:45,839
is no change in the environment you will

1588
01:00:45,839 --> 01:00:47,520
you will revert back to the exact

1589
01:00:47,520 --> 01:00:49,440
inference and the update rules which we

1590
01:00:49,440 --> 01:00:52,319
had for stationary case right so

1591
01:00:52,319 --> 01:00:54,880
uh and that's kind of also nice thing

1592
01:00:54,880 --> 01:00:56,480
about this

1593
01:00:56,480 --> 01:00:59,200
algorithm it can be just generalized to

1594
01:00:59,200 --> 01:00:59,839
any

1595
01:00:59,839 --> 01:01:01,760
any knowledge about probability a change

1596
01:01:01,760 --> 01:01:04,319
probability

1597
01:01:04,319 --> 01:01:07,040
okay so uh the action selection

1598
01:01:07,040 --> 01:01:08,319
algorithms did not

1599
01:01:08,319 --> 01:01:10,720
change so so the learning rules will

1600
01:01:10,720 --> 01:01:11,599
change but

1601
01:01:11,599 --> 01:01:13,599
we are practically doing still the same

1602
01:01:13,599 --> 01:01:15,119
way of of making

1603
01:01:15,119 --> 01:01:17,760
action selection so for thomson sampling

1604
01:01:17,760 --> 01:01:19,280
there's a sampling from the posterior

1605
01:01:19,280 --> 01:01:20,319
beliefs

1606
01:01:20,319 --> 01:01:22,720
uh for the asian upper confidence bound

1607
01:01:22,720 --> 01:01:24,880
it's slightly different so we are

1608
01:01:24,880 --> 01:01:28,240
using kind of the mixture between

1609
01:01:28,240 --> 01:01:31,040
uh again the mixture of possible

1610
01:01:31,040 --> 01:01:32,000
parameter

1611
01:01:32,000 --> 01:01:35,040
values to estimate the inverse uh

1612
01:01:35,040 --> 01:01:38,079
because from the this predictive

1613
01:01:38,079 --> 01:01:40,319
posterior it's difficult to inverse it

1614
01:01:40,319 --> 01:01:41,440
it's a mixture of two

1615
01:01:41,440 --> 01:01:44,079
beta distributions so this is just kind

1616
01:01:44,079 --> 01:01:46,079
of approximation one uh can use for

1617
01:01:46,079 --> 01:01:47,520
bayesian ucb and for

1618
01:01:47,520 --> 01:01:50,480
action selection uh approximate uh

1619
01:01:50,480 --> 01:01:52,640
expected free energy one gets with

1620
01:01:52,640 --> 01:01:55,680
uh a very same uh

1621
01:01:55,680 --> 01:01:58,240
uh set of equations because basically

1622
01:01:58,240 --> 01:01:58,720
raw

1623
01:01:58,720 --> 01:02:01,039
row can just so probability of change

1624
01:02:01,039 --> 01:02:01,839
drops out

1625
01:02:01,839 --> 01:02:05,359
it can be incan being nor easily there

1626
01:02:05,359 --> 01:02:08,799
uh okay so now if we

1627
01:02:08,799 --> 01:02:11,760
do kind of the same uh comparison first

1628
01:02:11,760 --> 01:02:12,799
of uh

1629
01:02:12,799 --> 01:02:14,880
exact and approximate after inference

1630
01:02:14,880 --> 01:02:16,559
algorithms we see a slightly different

1631
01:02:16,559 --> 01:02:17,440
picture

1632
01:02:17,440 --> 01:02:20,559
to what we had before

1633
01:02:20,559 --> 01:02:23,680
first it seems that as you increase the

1634
01:02:23,680 --> 01:02:25,119
number of trials

1635
01:02:25,119 --> 01:02:27,680
and you kind of compute this regret rate

1636
01:02:27,680 --> 01:02:30,880
for this specific number of trials

1637
01:02:30,880 --> 01:02:33,760
the regret rate does not change so in a

1638
01:02:33,760 --> 01:02:34,079
way

1639
01:02:34,079 --> 01:02:38,240
algorithm converges very fast to a

1640
01:02:38,240 --> 01:02:41,520
specific regret rate and

1641
01:02:41,520 --> 01:02:44,640
for different and there is a clear kind

1642
01:02:44,640 --> 01:02:45,359
of

1643
01:02:45,359 --> 01:02:48,559
stable minima independent of number of

1644
01:02:48,559 --> 01:02:49,680
trials you're

1645
01:02:49,680 --> 01:02:53,280
you're exposing algorithm to uh

1646
01:02:53,280 --> 01:02:55,440
similarly again we see like very similar

1647
01:02:55,440 --> 01:02:56,640
behavior between

1648
01:02:56,640 --> 01:02:58,960
exact active inference algorithm and the

1649
01:02:58,960 --> 01:03:00,640
approximate right

1650
01:03:00,640 --> 01:03:02,880
and for this example i will just i just

1651
01:03:02,880 --> 01:03:03,839
kind of fixed

1652
01:03:03,839 --> 01:03:07,280
lambda uh to uh 0.5

1653
01:03:07,280 --> 01:03:10,480
so in it just seems to be

1654
01:03:10,480 --> 01:03:13,599
reasonably well uh valued parameter for

1655
01:03:13,599 --> 01:03:14,720
many uh

1656
01:03:14,720 --> 01:03:17,599
many situations which we see here uh

1657
01:03:17,599 --> 01:03:18,400
however so

1658
01:03:18,400 --> 01:03:22,480
here i'm showing for 10 arms if we

1659
01:03:22,480 --> 01:03:24,799
go to 80 irons the picture slightly

1660
01:03:24,799 --> 01:03:26,400
changes so it seems that

1661
01:03:26,400 --> 01:03:28,880
optimal lambda parameter although it

1662
01:03:28,880 --> 01:03:29,520
doesn't

1663
01:03:29,520 --> 01:03:32,559
change with t it changes with the number

1664
01:03:32,559 --> 01:03:34,480
of arms and obviously with all the other

1665
01:03:34,480 --> 01:03:37,920
row and epsilon all the other uh

1666
01:03:37,920 --> 01:03:39,920
parameters so this makes things a bit

1667
01:03:39,920 --> 01:03:41,280
difficult

1668
01:03:41,280 --> 01:03:44,640
and basically suggests that it's

1669
01:03:44,640 --> 01:03:45,920
important to kind of find

1670
01:03:45,920 --> 01:03:47,839
potentially a learning rule like

1671
01:03:47,839 --> 01:03:48,960
self-optimizing

1672
01:03:48,960 --> 01:03:51,680
way of of estimating lambda and there

1673
01:03:51,680 --> 01:03:52,640
there is some work

1674
01:03:52,640 --> 01:03:54,559
actually in active inference literature

1675
01:03:54,559 --> 01:03:56,319
which potentially has

1676
01:03:56,319 --> 01:03:58,559
solution for that so uh we just have to

1677
01:03:58,559 --> 01:04:00,880
test it out at some point

1678
01:04:00,880 --> 01:04:05,039
uh so however or we can still use the

1679
01:04:05,039 --> 01:04:05,520
same

1680
01:04:05,520 --> 01:04:08,559
value this does not uh influence

1681
01:04:08,559 --> 01:04:11,920
that much the performance later on so

1682
01:04:11,920 --> 01:04:12,640
what we see

1683
01:04:12,640 --> 01:04:14,720
when we kind of compare it to uh

1684
01:04:14,720 --> 01:04:16,160
approximate active inference with

1685
01:04:16,160 --> 01:04:17,359
bayesian ucb and

1686
01:04:17,359 --> 01:04:20,319
thomas sampling is that for a range of

1687
01:04:20,319 --> 01:04:20,720
uh

1688
01:04:20,720 --> 01:04:23,359
right settings uh after either with like

1689
01:04:23,359 --> 01:04:24,000
fixed

1690
01:04:24,000 --> 01:04:27,680
um uh advantage of the

1691
01:04:27,680 --> 01:04:30,240
of the best like reward probability

1692
01:04:30,240 --> 01:04:32,559
advantage of the best arm

1693
01:04:32,559 --> 01:04:34,720
uh we see that active inference is just

1694
01:04:34,720 --> 01:04:36,480
right uh

1695
01:04:36,480 --> 01:04:39,440
performs better

1696
01:04:39,520 --> 01:04:42,400
already after like 1000 trials or so

1697
01:04:42,400 --> 01:04:45,039
compared to the other two algorithms

1698
01:04:45,039 --> 01:04:48,480
this is uh uh just for different

1699
01:04:48,480 --> 01:04:50,480
change probabilities so for example row

1700
01:04:50,480 --> 01:04:53,359
0.05 corresponds to a change every 200

1701
01:04:53,359 --> 01:04:54,400
trials

1702
01:04:54,400 --> 01:04:56,559
it should be every 100 trials on average

1703
01:04:56,559 --> 01:04:57,839
and the last

1704
01:04:57,839 --> 01:05:00,079
step here is every 25 trials so this

1705
01:05:00,079 --> 01:05:01,920
would be the most difficult right uh

1706
01:05:01,920 --> 01:05:04,319
scenario so one sees is that kind of the

1707
01:05:04,319 --> 01:05:06,319
more more difficult scenario here's the

1708
01:05:06,319 --> 01:05:09,200
the differences uh disappear so you're

1709
01:05:09,200 --> 01:05:11,760
kind of losing the advantage

1710
01:05:11,760 --> 01:05:15,119
uh however um if sim okay similarly if

1711
01:05:15,119 --> 01:05:15,520
we

1712
01:05:15,520 --> 01:05:18,160
look at the if we fix number of arms for

1713
01:05:18,160 --> 01:05:19,520
example 240 and we

1714
01:05:19,520 --> 01:05:22,400
just change the epsilon parameter a

1715
01:05:22,400 --> 01:05:23,359
similar trend

1716
01:05:23,359 --> 01:05:27,039
is visible right that uh the more easier

1717
01:05:27,039 --> 01:05:28,720
the task is the

1718
01:05:28,720 --> 01:05:30,960
bigger the difference uh in like

1719
01:05:30,960 --> 01:05:34,000
non-stationary scenario you get

1720
01:05:34,000 --> 01:05:37,119
relative to to other algorithms

1721
01:05:37,119 --> 01:05:40,799
of course one would potentially uh

1722
01:05:40,799 --> 01:05:43,680
kind of destroy this advantage if uh

1723
01:05:43,680 --> 01:05:46,559
changes become very slow like every

1724
01:05:46,559 --> 01:05:48,319
10 000 trials or something where you are

1725
01:05:48,319 --> 01:05:51,920
kind of approximately in the stationary

1726
01:05:51,920 --> 01:05:55,440
stationary world right

1727
01:05:57,440 --> 01:06:00,720
and okay so this would be like this

1728
01:06:00,720 --> 01:06:02,240
switching bandits with varying

1729
01:06:02,240 --> 01:06:04,160
difficulty we can do the same kind of

1730
01:06:04,160 --> 01:06:05,119
analysis

1731
01:06:05,119 --> 01:06:08,000
for switching bandit uh sorry previously

1732
01:06:08,000 --> 01:06:09,760
we looked at the switching bands with

1733
01:06:09,760 --> 01:06:12,880
uh fixed difficulty similar analysis

1734
01:06:12,880 --> 01:06:15,520
can be done for varying difficulty and

1735
01:06:15,520 --> 01:06:17,599
uh interestingly one sees here uh

1736
01:06:17,599 --> 01:06:20,000
notes because epsilon drops out one sees

1737
01:06:20,000 --> 01:06:21,920
here uh actually for the

1738
01:06:21,920 --> 01:06:24,160
exact active inference there is like a

1739
01:06:24,160 --> 01:06:25,039
clear

1740
01:06:25,039 --> 01:06:29,039
minima which uh seems independent on the

1741
01:06:29,039 --> 01:06:31,839
kind of any of these parameter terms the

1742
01:06:31,839 --> 01:06:32,720
so rho or

1743
01:06:32,720 --> 01:06:37,119
k so basically we can fix lambda to 0.25

1744
01:06:37,119 --> 01:06:38,720
for the exact acting difference

1745
01:06:38,720 --> 01:06:41,200
algorithm and we can still keep for the

1746
01:06:41,200 --> 01:06:42,400
approximate

1747
01:06:42,400 --> 01:06:44,400
lambda to for the approximate active

1748
01:06:44,400 --> 01:06:47,280
inference algorithm lambda to 0.5

1749
01:06:47,280 --> 01:06:50,319
and just to right show what happens

1750
01:06:50,319 --> 01:06:54,079
here is that there seems to be like

1751
01:06:54,079 --> 01:06:55,839
invariant difficulty there seems to be

1752
01:06:55,839 --> 01:06:57,680
also advantage of using

1753
01:06:57,680 --> 01:07:00,240
exact active inference decision making

1754
01:07:00,240 --> 01:07:01,440
algorithm

1755
01:07:01,440 --> 01:07:03,440
uh it outperforms the approx

1756
01:07:03,440 --> 01:07:06,079
approximated quite clearly

1757
01:07:06,079 --> 01:07:08,160
uh in this settings and it doesn't

1758
01:07:08,160 --> 01:07:10,160
require a fine tuning in this sense

1759
01:07:10,160 --> 01:07:12,240
right for the range of problems you can

1760
01:07:12,240 --> 01:07:13,920
have much better performance with the

1761
01:07:13,920 --> 01:07:15,680
single uh

1762
01:07:15,680 --> 01:07:18,720
choice of lambda value uh

1763
01:07:18,720 --> 01:07:22,319
and again so the in this case

1764
01:07:22,319 --> 01:07:23,920
interestingly the easier the problem

1765
01:07:23,920 --> 01:07:25,599
becomes so

1766
01:07:25,599 --> 01:07:28,480
in this kind of quadrant the less the

1767
01:07:28,480 --> 01:07:29,119
difference

1768
01:07:29,119 --> 01:07:31,359
uh between algorithms is and the

1769
01:07:31,359 --> 01:07:33,520
actually bayesian ucb algorithm becomes

1770
01:07:33,520 --> 01:07:34,880
quite uh

1771
01:07:34,880 --> 01:07:36,640
quite efficient in these settings also

1772
01:07:36,640 --> 01:07:38,799
here right you see that

1773
01:07:38,799 --> 01:07:42,400
beijing ucb also achieves uh quite good

1774
01:07:42,400 --> 01:07:43,119
performance

1775
01:07:43,119 --> 01:07:44,720
interestingly much better than

1776
01:07:44,720 --> 01:07:46,640
optimistic thompson sampling which

1777
01:07:46,640 --> 01:07:48,880
for me at least i didn't found any paper

1778
01:07:48,880 --> 01:07:49,920
who

1779
01:07:49,920 --> 01:07:52,799
previously uh showed something like this

1780
01:07:52,799 --> 01:07:53,760
so yeah

1781
01:07:53,760 --> 01:07:56,559
this is also a bit probably new result

1782
01:07:56,559 --> 01:07:57,839
for

1783
01:07:57,839 --> 01:08:01,119
machine learning people uh

1784
01:08:01,119 --> 01:08:04,160
okay so just to conclude right

1785
01:08:04,160 --> 01:08:05,599
active inference does not result in

1786
01:08:05,599 --> 01:08:07,520
asymptotically efficient decision making

1787
01:08:07,520 --> 01:08:08,000
uh

1788
01:08:08,000 --> 01:08:09,760
additional work is required to establish

1789
01:08:09,760 --> 01:08:11,680
theoretical bounce and regret

1790
01:08:11,680 --> 01:08:14,799
and derived improved algorithms in

1791
01:08:14,799 --> 01:08:17,040
non-stationary bandits however

1792
01:08:17,040 --> 01:08:20,880
we see better performance

1793
01:08:20,880 --> 01:08:22,719
in comparison to other algorithms and

1794
01:08:22,719 --> 01:08:24,719
especially noticeable in more difficult

1795
01:08:24,719 --> 01:08:26,399
settings right that

1796
01:08:26,399 --> 01:08:29,839
you're kind of uh getting uh

1797
01:08:29,839 --> 01:08:32,399
in the more difficult task is you're

1798
01:08:32,399 --> 01:08:35,040
kind of forgetting the better results

1799
01:08:35,040 --> 01:08:37,520
a tentative to-do list for like next

1800
01:08:37,520 --> 01:08:39,120
steps here is like

1801
01:08:39,120 --> 01:08:40,640
introduce learning for the lambda

1802
01:08:40,640 --> 01:08:42,158
parameter establish

1803
01:08:42,158 --> 01:08:44,319
a really like kind of theoretical bounce

1804
01:08:44,319 --> 01:08:45,920
on cumulative regret so what

1805
01:08:45,920 --> 01:08:48,960
can one expect to see

1806
01:08:48,960 --> 01:08:51,759
given different choices of the algorithm

1807
01:08:51,759 --> 01:08:53,679
for action selection

1808
01:08:53,679 --> 01:08:57,759
uh based on expected free energy and

1809
01:08:57,759 --> 01:09:00,158
so right this would hopefully improve uh

1810
01:09:00,158 --> 01:09:01,839
behavior and stationary cast

1811
01:09:01,839 --> 01:09:04,399
a case and potentially also apply to

1812
01:09:04,399 --> 01:09:05,279
some like

1813
01:09:05,279 --> 01:09:07,920
real world examples so this kind of

1814
01:09:07,920 --> 01:09:09,279
right uh

1815
01:09:09,279 --> 01:09:10,960
in into in machine learning field i

1816
01:09:10,960 --> 01:09:12,799
would say right you know

1817
01:09:12,799 --> 01:09:15,279
this kind of recommendation systems uh

1818
01:09:15,279 --> 01:09:17,679
optimization problems and similar right

1819
01:09:17,679 --> 01:09:19,920
just to see how it performs in this kind

1820
01:09:19,920 --> 01:09:21,279
of scenarios

1821
01:09:21,279 --> 01:09:24,560
yeah yeah okay that

1822
01:09:24,560 --> 01:09:26,880
i would just like to thank all the

1823
01:09:26,880 --> 01:09:29,040
collaborators on the project

1824
01:09:29,040 --> 01:09:31,600
uh and the people who helped me with

1825
01:09:31,600 --> 01:09:32,719
different advices

1826
01:09:32,719 --> 01:09:35,920
and you can also uh

1827
01:09:35,920 --> 01:09:38,238
find the slides here and the code is

1828
01:09:38,238 --> 01:09:39,679
available

1829
01:09:39,679 --> 01:09:42,238
my github page i would just not

1830
01:09:42,238 --> 01:09:43,920
recommend to

1831
01:09:43,920 --> 01:09:45,759
use it in the next two weeks because the

1832
01:09:45,759 --> 01:09:48,000
paper is under revision and

1833
01:09:48,000 --> 01:09:53,040
i'm breaking stuff constantly yeah

1834
01:09:53,120 --> 01:09:56,080
awesome thank you you can unshare and

1835
01:09:56,080 --> 01:09:57,360
we'll

1836
01:09:57,360 --> 01:10:00,560
return to just um discussion for the

1837
01:10:00,560 --> 01:10:00,960
last

1838
01:10:00,960 --> 01:10:02,960
45 minutes or so here but thanks for

1839
01:10:02,960 --> 01:10:04,080
that awesome

1840
01:10:04,080 --> 01:10:08,239
presentation with always good to get

1841
01:10:08,239 --> 01:10:11,679
multiple um multiple times just to sort

1842
01:10:11,679 --> 01:10:11,920
of

1843
01:10:11,920 --> 01:10:13,280
see some of those figures in the paper

1844
01:10:13,280 --> 01:10:14,960
then also there are some different

1845
01:10:14,960 --> 01:10:18,080
figures and some different views so yeah

1846
01:10:18,080 --> 01:10:20,640
again anyone in the live chat is welcome

1847
01:10:20,640 --> 01:10:22,080
to ask a question

1848
01:10:22,080 --> 01:10:26,000
or anyone who's here in this jitsi

1849
01:10:26,000 --> 01:10:27,760
i'll ask um a question from the live

1850
01:10:27,760 --> 01:10:29,280
chat first and then if you're here in

1851
01:10:29,280 --> 01:10:30,000
the jitsi

1852
01:10:30,000 --> 01:10:33,199
please feel free to raise your hand so

1853
01:10:33,199 --> 01:10:35,840
um it's written in a live chat since the

1854
01:10:35,840 --> 01:10:37,360
bandit problem here

1855
01:10:37,360 --> 01:10:40,000
is not markovian does this mean that we

1856
01:10:40,000 --> 01:10:41,199
only need to consider

1857
01:10:41,199 --> 01:10:43,120
the current time to calculate the

1858
01:10:43,120 --> 01:10:46,320
expected free energy

1859
01:10:47,600 --> 01:10:52,159
uh why why is not the mercurian

1860
01:10:52,159 --> 01:10:55,360
well let's let's actually clarify what

1861
01:10:55,360 --> 01:10:59,360
makes a problem or a situation markovian

1862
01:10:59,360 --> 01:11:02,639
or non-markovian

1863
01:11:03,280 --> 01:11:06,400
okay so uh for example

1864
01:11:06,400 --> 01:11:09,600
uh the problem is markovian because the

1865
01:11:09,600 --> 01:11:12,239
changes are only dependent on the

1866
01:11:12,239 --> 01:11:16,400
last trial or the current trial uh

1867
01:11:16,400 --> 01:11:19,920
right so the reward probability in this

1868
01:11:19,920 --> 01:11:23,280
non-stationary case uh will

1869
01:11:23,280 --> 01:11:24,960
be a function only on the reward

1870
01:11:24,960 --> 01:11:26,880
probability in the previous time step

1871
01:11:26,880 --> 01:11:28,960
and this is what uh makes the problem

1872
01:11:28,960 --> 01:11:30,960
actually markovian so i i didn't

1873
01:11:30,960 --> 01:11:33,120
talk about there are non-markovians

1874
01:11:33,120 --> 01:11:35,040
bandits but this is not

1875
01:11:35,040 --> 01:11:36,960
what we are discussing here right it is

1876
01:11:36,960 --> 01:11:39,760
really just markovian

1877
01:11:39,760 --> 01:11:42,080
but this is not the case why you don't

1878
01:11:42,080 --> 01:11:42,880
need

1879
01:11:42,880 --> 01:11:44,880
well maybe yes actually i mean if you

1880
01:11:44,880 --> 01:11:48,320
would go to non-markovian

1881
01:11:48,640 --> 01:11:50,880
bend it then you would potentially need

1882
01:11:50,880 --> 01:11:53,440
to plan ahead for longer because

1883
01:11:53,440 --> 01:11:55,360
that would require that there are kind

1884
01:11:55,360 --> 01:11:56,880
of

1885
01:11:56,880 --> 01:11:59,120
dependencies between your actions and

1886
01:11:59,120 --> 01:12:01,199
outcomes which you observe in a sequence

1887
01:12:01,199 --> 01:12:02,400
and different sequences

1888
01:12:02,400 --> 01:12:04,880
might result in very different outcomes

1889
01:12:04,880 --> 01:12:05,920
so right

1890
01:12:05,920 --> 01:12:08,640
because here we are in markovian case

1891
01:12:08,640 --> 01:12:10,640
and uh

1892
01:12:10,640 --> 01:12:13,360
agent cannot change the dynamics of the

1893
01:12:13,360 --> 01:12:14,800
environment in any way

1894
01:12:14,800 --> 01:12:18,080
so it's in a way just kind of passive

1895
01:12:18,080 --> 01:12:21,360
uh sampler from the environment uh then

1896
01:12:21,360 --> 01:12:22,480
you just

1897
01:12:22,480 --> 01:12:24,880
there is no gain in planning ahead so

1898
01:12:24,880 --> 01:12:27,679
you can just re-evaluate your beliefs on

1899
01:12:27,679 --> 01:12:30,080
each trial

1900
01:12:30,080 --> 01:12:33,360
so it's kind of like a memoryless

1901
01:12:33,360 --> 01:12:34,400
process which you

1902
01:12:34,400 --> 01:12:36,480
brought up and then what would have to

1903
01:12:36,480 --> 01:12:38,159
change to maybe account

1904
01:12:38,159 --> 01:12:40,480
for situations where sequence of actions

1905
01:12:40,480 --> 01:12:43,120
does matter

1906
01:12:43,920 --> 01:12:48,840
uh so for example one could

1907
01:12:48,840 --> 01:12:50,960
introduce structural dependencies

1908
01:12:50,960 --> 01:12:52,960
between different arms

1909
01:12:52,960 --> 01:12:56,159
so that for example rewards uh

1910
01:12:56,159 --> 01:12:57,840
reward probabilities depends on the

1911
01:12:57,840 --> 01:12:59,440
location

1912
01:12:59,440 --> 01:13:02,880
and allowing agent only to

1913
01:13:02,880 --> 01:13:06,239
to select uh to make their choices from

1914
01:13:06,239 --> 01:13:07,360
the nearby uh

1915
01:13:07,360 --> 01:13:09,040
right kind of introducing spatial

1916
01:13:09,040 --> 01:13:11,120
dependency

1917
01:13:11,120 --> 01:13:13,040
that would be one example where then

1918
01:13:13,040 --> 01:13:14,480
depending on in which

1919
01:13:14,480 --> 01:13:16,800
part of the space i am in kind of where

1920
01:13:16,800 --> 01:13:17,600
i selected

1921
01:13:17,600 --> 01:13:20,080
one arm this limits me to what what's

1922
01:13:20,080 --> 01:13:22,080
the next arm which i can select and this

1923
01:13:22,080 --> 01:13:24,000
would require you to then plan

1924
01:13:24,000 --> 01:13:26,800
depending on uh where you should be in

1925
01:13:26,800 --> 01:13:28,320
different trials depending on how you

1926
01:13:28,320 --> 01:13:28,800
expect

1927
01:13:28,800 --> 01:13:32,480
things to change right

1928
01:13:32,480 --> 01:13:34,159
all right this this would kind of

1929
01:13:34,159 --> 01:13:36,000
introduce then uh

1930
01:13:36,000 --> 01:13:39,040
the requirement for planning

1931
01:13:39,040 --> 01:13:42,080
cool very interesting um

1932
01:13:42,080 --> 01:13:45,440
blue or dave or sarah wanna ask anything

1933
01:13:45,440 --> 01:13:46,000
otherwise i

1934
01:13:46,000 --> 01:13:49,199
have some questions

1935
01:13:52,080 --> 01:13:54,320
you mentioned a few industrial

1936
01:13:54,320 --> 01:13:55,440
applications

1937
01:13:55,440 --> 01:13:57,360
and a few ways in which people do use

1938
01:13:57,360 --> 01:13:58,480
the multi-armed

1939
01:13:58,480 --> 01:14:01,280
bandit this is just kind of like um a

1940
01:14:01,280 --> 01:14:02,239
logistical

1941
01:14:02,239 --> 01:14:04,400
question like what is the rate limiting

1942
01:14:04,400 --> 01:14:05,360
step in

1943
01:14:05,360 --> 01:14:08,320
those use cases deploying active

1944
01:14:08,320 --> 01:14:09,840
inference agents

1945
01:14:09,840 --> 01:14:12,880
is there a way to kind of wrap the

1946
01:14:12,880 --> 01:14:14,880
inputs and outputs of multi-armed

1947
01:14:14,880 --> 01:14:16,000
bandits

1948
01:14:16,000 --> 01:14:20,000
uh in a way that's sort of interoperable

1949
01:14:20,000 --> 01:14:21,600
like you talked about how the learning

1950
01:14:21,600 --> 01:14:23,360
rules were similar but then you

1951
01:14:23,360 --> 01:14:25,679
you juxtapose different action selection

1952
01:14:25,679 --> 01:14:26,719
approaches

1953
01:14:26,719 --> 01:14:29,600
so in the context of pipelines that

1954
01:14:29,600 --> 01:14:31,040
people are already using

1955
01:14:31,040 --> 01:14:32,880
is there a way to kind of hot swap

1956
01:14:32,880 --> 01:14:34,320
active inference

1957
01:14:34,320 --> 01:14:37,199
and maybe have it be deployed in

1958
01:14:37,199 --> 01:14:39,840
industrial settings very rapidly

1959
01:14:39,840 --> 01:14:41,440
well yeah i mean that would be one idea

1960
01:14:41,440 --> 01:14:44,639
i mean for example uh

1961
01:14:45,280 --> 01:14:47,199
if you have these non-stationary

1962
01:14:47,199 --> 01:14:50,080
problems and you have already been used

1963
01:14:50,080 --> 01:14:53,199
using thomson sampling for example or

1964
01:14:53,199 --> 01:14:55,280
even like optimistic thompson sinclair

1965
01:14:55,280 --> 01:14:58,320
as a choice of the algorithm in a way

1966
01:14:58,320 --> 01:14:59,920
you already have a way to

1967
01:14:59,920 --> 01:15:03,040
form beliefs about relevant aspects of

1968
01:15:03,040 --> 01:15:03,440
the

1969
01:15:03,440 --> 01:15:06,560
of the problem then you can like really

1970
01:15:06,560 --> 01:15:07,600
easily swap

1971
01:15:07,600 --> 01:15:10,159
right the two and you just can then

1972
01:15:10,159 --> 01:15:11,600
apply

1973
01:15:11,600 --> 01:15:13,360
the the difficulty just to figure out

1974
01:15:13,360 --> 01:15:15,280
how to compute basically expected free

1975
01:15:15,280 --> 01:15:16,080
energy

1976
01:15:16,080 --> 01:15:18,880
and test it out right for the different

1977
01:15:18,880 --> 01:15:20,080
generative model

1978
01:15:20,080 --> 01:15:24,159
that people might probably have

1979
01:15:24,159 --> 01:15:27,600
yeah uh again for

1980
01:15:27,600 --> 01:15:30,080
stationary problems that's maybe

1981
01:15:30,080 --> 01:15:32,320
trickier

1982
01:15:32,320 --> 01:15:35,679
it might work in some situations but

1983
01:15:35,679 --> 01:15:36,640
yeah

1984
01:15:36,640 --> 01:15:38,400
you you don't have this kind of nice

1985
01:15:38,400 --> 01:15:40,159
asymptotic guarantees that you will

1986
01:15:40,159 --> 01:15:42,719
always find good solutions

1987
01:15:42,719 --> 01:15:44,800
in a way it's an advantage of active

1988
01:15:44,800 --> 01:15:47,040
inference in a changing world like

1989
01:15:47,040 --> 01:15:47,520
people's

1990
01:15:47,520 --> 01:15:50,400
preferences for a given advertisement or

1991
01:15:50,400 --> 01:15:51,600
the the situation for

1992
01:15:51,600 --> 01:15:54,880
trading is always changing and so

1993
01:15:54,880 --> 01:15:58,080
it's a false allure to have something

1994
01:15:58,080 --> 01:15:58,640
that has

1995
01:15:58,640 --> 01:16:01,040
extremely well behaved behavior in the

1996
01:16:01,040 --> 01:16:03,520
asymptotic or infinite case because

1997
01:16:03,520 --> 01:16:05,120
we're not in the infinite case we're in

1998
01:16:05,120 --> 01:16:07,199
the finite and dynamic case

1999
01:16:07,199 --> 01:16:09,679
and so it's almost like the sort of

2000
01:16:09,679 --> 01:16:10,239
strong

2001
01:16:10,239 --> 01:16:13,920
pillar that purportedly is underpinning

2002
01:16:13,920 --> 01:16:16,320
these other approaches isn't so much of

2003
01:16:16,320 --> 01:16:18,320
a gain pragmatically so it's pretty cool

2004
01:16:18,320 --> 01:16:19,199
to hear about that

2005
01:16:19,199 --> 01:16:21,679
blue thanks for the raised hand what

2006
01:16:21,679 --> 01:16:23,679
would you like to ask

2007
01:16:23,679 --> 01:16:25,679
so i have a question that was left over

2008
01:16:25,679 --> 01:16:27,280
from the dot zero video and something

2009
01:16:27,280 --> 01:16:28,880
you kind of alluded today

2010
01:16:28,880 --> 01:16:32,239
in your talk can you kind of detail the

2011
01:16:32,239 --> 01:16:34,560
difference between the switching bandits

2012
01:16:34,560 --> 01:16:36,560
and the restless bandits i was like

2013
01:16:36,560 --> 01:16:38,480
unclear on like the timing of the

2014
01:16:38,480 --> 01:16:39,360
switching

2015
01:16:39,360 --> 01:16:41,360
in the switching bandits and then also

2016
01:16:41,360 --> 01:16:43,600
um just like a part b of that question

2017
01:16:43,600 --> 01:16:45,840
in the case of the restless bandits um

2018
01:16:45,840 --> 01:16:46,800
what are

2019
01:16:46,800 --> 01:16:48,800
are the similar like algorithms that are

2020
01:16:48,800 --> 01:16:50,320
optimal for switching bandits

2021
01:16:50,320 --> 01:16:52,480
also optimal for restless bandits you

2022
01:16:52,480 --> 01:16:53,360
had mentioned

2023
01:16:53,360 --> 01:16:54,960
the active inference was good but what

2024
01:16:54,960 --> 01:16:57,679
about the others that are commonly used

2025
01:16:57,679 --> 01:17:01,199
no it's the same right i mean

2026
01:17:01,199 --> 01:17:03,839
so

2027
01:17:04,400 --> 01:17:06,560
let's say in restless bandits the only

2028
01:17:06,560 --> 01:17:08,159
kind of

2029
01:17:08,159 --> 01:17:09,920
so in switching bandits we have like

2030
01:17:09,920 --> 01:17:12,159
piecewise linear problem or piecewise

2031
01:17:12,159 --> 01:17:13,920
stationary problem

2032
01:17:13,920 --> 01:17:17,199
which means that between trial one

2033
01:17:17,199 --> 01:17:19,600
and trial ten everything behaves

2034
01:17:19,600 --> 01:17:21,360
stationary and then when the change

2035
01:17:21,360 --> 01:17:22,239
comes

2036
01:17:22,239 --> 01:17:25,520
you're just getting uh new reward

2037
01:17:25,520 --> 01:17:27,280
probabilities associated with each other

2038
01:17:27,280 --> 01:17:28,719
right

2039
01:17:28,719 --> 01:17:30,719
for example that would be a kind of idea

2040
01:17:30,719 --> 01:17:32,159
of a switching bandit so that

2041
01:17:32,159 --> 01:17:36,480
between changes everything is like fixed

2042
01:17:36,480 --> 01:17:39,600
and the restless

2043
01:17:39,600 --> 01:17:42,400
bandit assumes that things continuously

2044
01:17:42,400 --> 01:17:43,120
change and

2045
01:17:43,120 --> 01:17:45,120
here the example i gave was one can

2046
01:17:45,120 --> 01:17:47,280
assume for example that

2047
01:17:47,280 --> 01:17:50,800
reward probabilities can be described as

2048
01:17:50,800 --> 01:17:51,840
a

2049
01:17:51,840 --> 01:17:54,880
random walk in this

2050
01:17:54,880 --> 01:17:58,400
in this logic space of the probability

2051
01:17:58,400 --> 01:18:00,159
so basically because probability is like

2052
01:18:00,159 --> 01:18:01,280
between zero and one

2053
01:18:01,280 --> 01:18:03,199
you transform into one constraint space

2054
01:18:03,199 --> 01:18:04,960
between minus infinity and infinity

2055
01:18:04,960 --> 01:18:07,360
you just have a then gaussian random

2056
01:18:07,360 --> 01:18:08,560
variable so to say

2057
01:18:08,560 --> 01:18:12,560
and then you back map it back into the

2058
01:18:12,560 --> 01:18:15,199
into the with the sigmoid function for

2059
01:18:15,199 --> 01:18:18,080
example into the probability space

2060
01:18:18,080 --> 01:18:21,679
uh and and uh so but so for example the

2061
01:18:21,679 --> 01:18:23,440
algorithm i show like for belief

2062
01:18:23,440 --> 01:18:24,320
updating you

2063
01:18:24,320 --> 01:18:26,800
one could also just apply it there so it

2064
01:18:26,800 --> 01:18:28,640
just one one doesn't know what would be

2065
01:18:28,640 --> 01:18:30,159
the row

2066
01:18:30,159 --> 01:18:32,239
so what is the change probability in the

2067
01:18:32,239 --> 01:18:33,280
restless case

2068
01:18:33,280 --> 01:18:35,600
so in that sense you would need an

2069
01:18:35,600 --> 01:18:37,280
algorithm which can also infer the

2070
01:18:37,280 --> 01:18:40,560
change probability

2071
01:18:40,560 --> 01:18:42,159
and the restless case doesn't

2072
01:18:42,159 --> 01:18:43,760
necessarily translates

2073
01:18:43,760 --> 01:18:46,080
to kind of potentially fixed change

2074
01:18:46,080 --> 01:18:48,080
probability

2075
01:18:48,080 --> 01:18:51,520
uh so it's a bit more

2076
01:18:51,520 --> 01:18:54,960
different problem so because the maximum

2077
01:18:54,960 --> 01:18:58,880
arm the changes between

2078
01:18:58,880 --> 01:19:01,280
right between arms which are optimal uh

2079
01:19:01,280 --> 01:19:03,120
do not follow specifically like

2080
01:19:03,120 --> 01:19:05,520
the same structure as in the switching

2081
01:19:05,520 --> 01:19:06,960
case so

2082
01:19:06,960 --> 01:19:09,120
uh one can either take a different

2083
01:19:09,120 --> 01:19:11,679
generating model which actually assumes

2084
01:19:11,679 --> 01:19:14,239
explicitly the random walk and like for

2085
01:19:14,239 --> 01:19:14,800
example

2086
01:19:14,800 --> 01:19:17,520
uh hierarchical duration filter is

2087
01:19:17,520 --> 01:19:19,600
something which one could apply there

2088
01:19:19,600 --> 01:19:22,000
but they're also uh like what other

2089
01:19:22,000 --> 01:19:24,800
belief updating

2090
01:19:24,800 --> 01:19:28,080
algorithms for that so the local or the

2091
01:19:28,080 --> 01:19:28,560
global

2092
01:19:28,560 --> 01:19:30,400
maxima and minima are changing in the

2093
01:19:30,400 --> 01:19:33,120
restless as well

2094
01:19:33,120 --> 01:19:35,120
yes yes yes so this would be kind of

2095
01:19:35,120 --> 01:19:36,640
this case of uh

2096
01:19:36,640 --> 01:19:39,120
varying difficulties over the relative

2097
01:19:39,120 --> 01:19:40,000
uh

2098
01:19:40,000 --> 01:19:42,159
probability between the best arm and the

2099
01:19:42,159 --> 01:19:43,600
second best arm

2100
01:19:43,600 --> 01:19:46,560
varies all the time

2101
01:19:47,600 --> 01:19:50,719
yeah i don't have

2102
01:19:50,719 --> 01:19:52,320
i could i could have just drawn the

2103
01:19:52,320 --> 01:19:53,840
lines

2104
01:19:53,840 --> 01:19:55,760
to show but i don't know i didn't do

2105
01:19:55,760 --> 01:19:57,120
that i i could hear that

2106
01:19:57,120 --> 01:19:59,120
i just don't have it right now something

2107
01:19:59,120 --> 01:20:00,159
to

2108
01:20:00,159 --> 01:20:03,600
illustrate this so one other thought on

2109
01:20:03,600 --> 01:20:05,920
the advantage possibly of active

2110
01:20:05,920 --> 01:20:07,440
inference is that with a deep

2111
01:20:07,440 --> 01:20:11,600
generative model using the same skeleton

2112
01:20:11,600 --> 01:20:14,320
of maybe even the same code it could be

2113
01:20:14,320 --> 01:20:16,239
possible to

2114
01:20:16,239 --> 01:20:20,000
do model testing between two different

2115
01:20:20,000 --> 01:20:21,360
types of bandits like what kind of

2116
01:20:21,360 --> 01:20:23,120
scenario am i in

2117
01:20:23,120 --> 01:20:26,560
or even have deep temporal models so

2118
01:20:26,560 --> 01:20:28,400
it could be extended in a way where a

2119
01:20:28,400 --> 01:20:30,400
sort of instantaneous sampler

2120
01:20:30,400 --> 01:20:33,920
might be led astray yes

2121
01:20:33,920 --> 01:20:38,800
yes yes i mean in principle you can um

2122
01:20:38,800 --> 01:20:42,080
any hierarchical asian generative model

2123
01:20:42,080 --> 01:20:44,239
which consists of multiple models

2124
01:20:44,239 --> 01:20:45,360
potentially you could

2125
01:20:45,360 --> 01:20:47,920
also write generalized to thomson

2126
01:20:47,920 --> 01:20:48,960
sampling

2127
01:20:48,960 --> 01:20:51,440
or by asian ucb i just wouldn't assume

2128
01:20:51,440 --> 01:20:53,040
that this would be very efficient

2129
01:20:53,040 --> 01:20:56,400
way to figure out which is the correct

2130
01:20:56,400 --> 01:20:57,199
model

2131
01:20:57,199 --> 01:20:58,639
which you should be currently applying

2132
01:20:58,639 --> 01:21:00,159
to the specific task and this is

2133
01:21:00,159 --> 01:21:02,880
potentially also where active inferences

2134
01:21:02,880 --> 01:21:06,159
uh would would provide an advantage in

2135
01:21:06,159 --> 01:21:06,719
such

2136
01:21:06,719 --> 01:21:09,360
scenarios right where you can also kind

2137
01:21:09,360 --> 01:21:09,760
of

2138
01:21:09,760 --> 01:21:12,719
learn about the generating model itself

2139
01:21:12,719 --> 01:21:14,320
better over time

2140
01:21:14,320 --> 01:21:17,840
and that made me wonder what would it

2141
01:21:17,840 --> 01:21:21,120
look like at the sort of human level as

2142
01:21:21,120 --> 01:21:22,400
we're making decisions that are

2143
01:21:22,400 --> 01:21:23,040
sequential

2144
01:21:23,040 --> 01:21:26,080
in our day our decision making what

2145
01:21:26,080 --> 01:21:26,800
would it

2146
01:21:26,800 --> 01:21:28,560
look like or what would we keep in mind

2147
01:21:28,560 --> 01:21:30,719
if we were going to be making decisions

2148
01:21:30,719 --> 01:21:32,719
more like an active inference agent than

2149
01:21:32,719 --> 01:21:35,600
like a thompson sampling agent

2150
01:21:35,600 --> 01:21:36,800
like what would be you know when you're

2151
01:21:36,800 --> 01:21:38,239
in the grocery store looking at the

2152
01:21:38,239 --> 01:21:40,719
cereals that you've had before or not

2153
01:21:40,719 --> 01:21:42,239
how would an active inference agent

2154
01:21:42,239 --> 01:21:45,280
behave differently um

2155
01:21:45,280 --> 01:21:48,480
just kind of wondering yeah i mean

2156
01:21:48,480 --> 01:21:51,199
that's a good question i mean we could

2157
01:21:51,199 --> 01:21:53,120
actually we have some data sets

2158
01:21:53,120 --> 01:21:56,960
which which we could use exactly

2159
01:21:56,960 --> 01:22:00,000
to ask this kind of questions yeah i

2160
01:22:00,000 --> 01:22:02,719
mean i don't have on top of my head any

2161
01:22:02,719 --> 01:22:06,800
clear answer um

2162
01:22:09,040 --> 01:22:12,880
what what would be your expectation sir

2163
01:22:14,159 --> 01:22:18,080
well i guess um it would tell you one is

2164
01:22:18,080 --> 01:22:18,639
a

2165
01:22:18,639 --> 01:22:20,560
or better tell you when is a good time

2166
01:22:20,560 --> 01:22:22,239
to try a different cereal

2167
01:22:22,239 --> 01:22:24,719
because you may like it even more

2168
01:22:24,719 --> 01:22:26,320
especially if cereal

2169
01:22:26,320 --> 01:22:30,880
recipes change over time

2170
01:22:31,600 --> 01:22:34,480
and yeah then you have some likelihood

2171
01:22:34,480 --> 01:22:35,760
to be stuck maybe

2172
01:22:35,760 --> 01:22:38,159
with the super bad cereal but also i

2173
01:22:38,159 --> 01:22:39,199
like you to

2174
01:22:39,199 --> 01:22:42,400
have the one you like most

2175
01:22:42,400 --> 01:22:44,159
yeah i mean i guess this exploration

2176
01:22:44,159 --> 01:22:46,239
wouldn't would be more structured in a

2177
01:22:46,239 --> 01:22:46,960
way more

2178
01:22:46,960 --> 01:22:50,560
directed right um

2179
01:22:50,560 --> 01:22:53,840
right like i mean when the ingredients

2180
01:22:53,840 --> 01:22:55,440
change you check the ingredients and

2181
01:22:55,440 --> 01:22:57,280
then now you've updated your likelihood

2182
01:22:57,280 --> 01:22:57,600
of

2183
01:22:57,600 --> 01:23:00,320
trying something new yeah all right i

2184
01:23:00,320 --> 01:23:00,639
mean

2185
01:23:00,639 --> 01:23:03,440
thompson sampling doesn't have directed

2186
01:23:03,440 --> 01:23:04,800
part of the exploration

2187
01:23:04,800 --> 01:23:07,840
just the random exploration

2188
01:23:07,840 --> 01:23:10,080
whereas they're here the focus would be

2189
01:23:10,080 --> 01:23:11,679
more and right directed

2190
01:23:11,679 --> 01:23:14,800
exploration and potentially

2191
01:23:14,800 --> 01:23:18,239
thinking how to add random

2192
01:23:18,239 --> 01:23:21,600
so yeah one other piece is like

2193
01:23:21,600 --> 01:23:24,560
we're often comparing and contrasting

2194
01:23:24,560 --> 01:23:26,320
active inference to

2195
01:23:26,320 --> 01:23:28,320
reward learning and reinforcement

2196
01:23:28,320 --> 01:23:30,480
learning so it's almost like

2197
01:23:30,480 --> 01:23:33,199
instead of making that decision based

2198
01:23:33,199 --> 01:23:35,760
upon the highest expected value

2199
01:23:35,760 --> 01:23:38,239
like choosing something proportional to

2200
01:23:38,239 --> 01:23:40,320
its relative value or always going with

2201
01:23:40,320 --> 01:23:42,239
the one with the best likelihood

2202
01:23:42,239 --> 01:23:45,360
of having the best tasting cereal there

2203
01:23:45,360 --> 01:23:46,080
can be some

2204
01:23:46,080 --> 01:23:49,840
other heuristic and so it opens the door

2205
01:23:49,840 --> 01:23:53,040
to just purely curiosity driven

2206
01:23:53,040 --> 01:23:55,520
sampling like i've just never had this

2207
01:23:55,520 --> 01:23:56,719
and it's not

2208
01:23:56,719 --> 01:23:59,440
even as much a reward maximizing

2209
01:23:59,440 --> 01:24:01,120
maneuver as it is just a purely

2210
01:24:01,120 --> 01:24:02,000
epistemic

2211
01:24:02,000 --> 01:24:05,360
gaining maneuver and then as we've seen

2212
01:24:05,360 --> 01:24:07,520
when there's a pragmatic and an

2213
01:24:07,520 --> 01:24:08,960
epistemic component

2214
01:24:08,960 --> 01:24:11,600
to the function that's being optimized

2215
01:24:11,600 --> 01:24:12,159
then

2216
01:24:12,159 --> 01:24:14,800
those decisions can kind of co-exist and

2217
01:24:14,800 --> 01:24:17,040
be put on a common grounding

2218
01:24:17,040 --> 01:24:19,600
unlike in a purely value driven

2219
01:24:19,600 --> 01:24:20,400
framework

2220
01:24:20,400 --> 01:24:22,800
where even the exploration has to be

2221
01:24:22,800 --> 01:24:23,520
kind of

2222
01:24:23,520 --> 01:24:27,679
coerced back into reward

2223
01:24:28,840 --> 01:24:32,000
yes mean so i think uh syed noor and she

2224
01:24:32,000 --> 01:24:34,000
had like as a first author she had an

2225
01:24:34,000 --> 01:24:35,520
interesting paper the mystify

2226
01:24:35,520 --> 01:24:38,159
active inference and they discuss a bit

2227
01:24:38,159 --> 01:24:38,639
about

2228
01:24:38,639 --> 01:24:40,400
learning the preferences themselves

2229
01:24:40,400 --> 01:24:42,400
right and what kind of consequence this

2230
01:24:42,400 --> 01:24:43,679
has and this kind of

2231
01:24:43,679 --> 01:24:46,320
puts a different perspective on

2232
01:24:46,320 --> 01:24:48,960
understanding what the reward is because

2233
01:24:48,960 --> 01:24:52,239
in real world you don't necessarily know

2234
01:24:52,239 --> 01:24:54,159
why should something be more rewarding

2235
01:24:54,159 --> 01:24:55,760
than something else but you learn

2236
01:24:55,760 --> 01:24:58,400
this over time and you just learn to

2237
01:24:58,400 --> 01:24:59,040
prefer

2238
01:24:59,040 --> 01:25:01,199
different outcomes they don't even

2239
01:25:01,199 --> 01:25:02,480
necessarily have to be

2240
01:25:02,480 --> 01:25:04,639
in somehow rewarding more in the

2241
01:25:04,639 --> 01:25:06,000
absolute sense

2242
01:25:06,000 --> 01:25:07,920
you just build experience with some

2243
01:25:07,920 --> 01:25:09,440
outcomes over others and you'll

2244
01:25:09,440 --> 01:25:12,639
start to prefer them and this

2245
01:25:12,639 --> 01:25:14,960
then appears as if you would doing

2246
01:25:14,960 --> 01:25:15,920
reward based

2247
01:25:15,920 --> 01:25:18,560
decision making but i mean it's actually

2248
01:25:18,560 --> 01:25:20,320
in a way preference based decision

2249
01:25:20,320 --> 01:25:22,320
making right

2250
01:25:22,320 --> 01:25:24,560
cool i i had another question about the

2251
01:25:24,560 --> 01:25:28,159
approximation of active inference

2252
01:25:28,159 --> 01:25:30,880
is that the only way to approximate

2253
01:25:30,880 --> 01:25:32,560
active inference are there

2254
01:25:32,560 --> 01:25:34,800
other moves that you could have made to

2255
01:25:34,800 --> 01:25:35,840
approximate

2256
01:25:35,840 --> 01:25:37,920
the sections that you did approximate

2257
01:25:37,920 --> 01:25:39,360
are there other pieces that can be

2258
01:25:39,360 --> 01:25:40,560
approximated

2259
01:25:40,560 --> 01:25:43,600
what can be swapped or approximated

2260
01:25:43,600 --> 01:25:45,440
but still retain this essential

2261
01:25:45,440 --> 01:25:49,199
structure of an active inference model

2262
01:25:49,220 --> 01:25:52,840
[Laughter]

2263
01:25:52,840 --> 01:25:55,840
um

2264
01:25:57,360 --> 01:25:59,679
well i don't have many ideas what else

2265
01:25:59,679 --> 01:26:01,199
one could do

2266
01:26:01,199 --> 01:26:03,840
i mean the problem is in this scenario

2267
01:26:03,840 --> 01:26:04,960
it's relatively simple

2268
01:26:04,960 --> 01:26:07,360
right but for example one can kind of

2269
01:26:07,360 --> 01:26:08,639
think of okay

2270
01:26:08,639 --> 01:26:11,679
given that we have a

2271
01:26:11,840 --> 01:26:15,280
to compute the expected information gain

2272
01:26:15,280 --> 01:26:18,239
and we are kind of learning of having a

2273
01:26:18,239 --> 01:26:19,360
way to

2274
01:26:19,360 --> 01:26:21,440
to estimate it efficiently like just

2275
01:26:21,440 --> 01:26:23,679
with this approximation

2276
01:26:23,679 --> 01:26:26,000
uh there is other way how you can

2277
01:26:26,000 --> 01:26:27,920
compute this just by sampling

2278
01:26:27,920 --> 01:26:29,360
and you can draw a couple of samples

2279
01:26:29,360 --> 01:26:31,040
from your beliefs and you get some

2280
01:26:31,040 --> 01:26:32,800
estimate on the expected information

2281
01:26:32,800 --> 01:26:34,960
right so this this would still be in the

2282
01:26:34,960 --> 01:26:36,880
kind of

2283
01:26:36,880 --> 01:26:40,000
active inference frame framework

2284
01:26:40,000 --> 01:26:42,480
but just like a different way of how you

2285
01:26:42,480 --> 01:26:44,239
actually compute expectations and what

2286
01:26:44,239 --> 01:26:44,800
they will

2287
01:26:44,800 --> 01:26:46,800
kind of represent there right you will

2288
01:26:46,800 --> 01:26:49,679
this would be a way to add a bit of

2289
01:26:49,679 --> 01:26:52,159
random randomness to the decision-making

2290
01:26:52,159 --> 01:26:54,400
process

2291
01:26:54,400 --> 01:26:58,159
i mean active inferences also general

2292
01:26:58,159 --> 01:27:00,320
enough to work with many kinds of

2293
01:27:00,320 --> 01:27:02,560
approximations right and you can

2294
01:27:02,560 --> 01:27:05,520
do approximations and many different

2295
01:27:05,520 --> 01:27:07,280
points but

2296
01:27:07,280 --> 01:27:10,159
as long as you they get some sort of

2297
01:27:10,159 --> 01:27:12,159
variation of free energy

2298
01:27:12,159 --> 01:27:14,960
i guess it's still within this framework

2299
01:27:14,960 --> 01:27:16,320
and

2300
01:27:16,320 --> 01:27:19,679
immediately separated the

2301
01:27:19,679 --> 01:27:22,000
inference part from the action selection

2302
01:27:22,000 --> 01:27:24,159
part but i think

2303
01:27:24,159 --> 01:27:27,280
actually you can use it as a model of

2304
01:27:27,280 --> 01:27:27,760
both

2305
01:27:27,760 --> 01:27:30,719
right and then you can do approximations

2306
01:27:30,719 --> 01:27:33,040
and different points in this joint model

2307
01:27:33,040 --> 01:27:35,520
and as long as you can still write down

2308
01:27:35,520 --> 01:27:37,199
a variation every energy

2309
01:27:37,199 --> 01:27:41,199
i guess it's a counter active inference

2310
01:27:43,760 --> 01:27:45,840
well i mean i don't know even if that's

2311
01:27:45,840 --> 01:27:47,520
necessary i mean i would say

2312
01:27:47,520 --> 01:27:50,560
any kind of bayesian belief updating

2313
01:27:50,560 --> 01:27:52,560
even if it's not specifically motivated

2314
01:27:52,560 --> 01:27:54,800
by variational inference

2315
01:27:54,800 --> 01:27:57,840
would it the consequence have minimizing

2316
01:27:57,840 --> 01:28:00,639
variational free energy once once you

2317
01:28:00,639 --> 01:28:02,480
compute it from some posterior which you

2318
01:28:02,480 --> 01:28:03,600
obtain

2319
01:28:03,600 --> 01:28:06,239
and i mean right it's it will still be

2320
01:28:06,239 --> 01:28:07,840
in a way

2321
01:28:07,840 --> 01:28:10,800
active inference so right in a way one

2322
01:28:10,800 --> 01:28:12,080
can replace

2323
01:28:12,080 --> 01:28:14,719
like smile variational updating with any

2324
01:28:14,719 --> 01:28:16,800
belief updating rule

2325
01:28:16,800 --> 01:28:20,320
and still keep the same

2326
01:28:20,320 --> 01:28:22,719
concept there so because we are just

2327
01:28:22,719 --> 01:28:24,159
getting a better bound

2328
01:28:24,159 --> 01:28:27,040
on the marginal likelihood if you have a

2329
01:28:27,040 --> 01:28:28,239
better posterior

2330
01:28:28,239 --> 01:28:32,000
so so what would you say you know anyone

2331
01:28:32,000 --> 01:28:35,120
is necessary or sufficient for

2332
01:28:35,120 --> 01:28:36,800
a model to be considered active

2333
01:28:36,800 --> 01:28:43,840
inference versus not active inference

2334
01:28:46,400 --> 01:28:49,120
just including sense and action or

2335
01:28:49,120 --> 01:28:49,840
perception

2336
01:28:49,840 --> 01:28:54,239
inference action or agents in a niche

2337
01:28:54,239 --> 01:28:58,320
or blanket states these things are

2338
01:28:58,320 --> 01:29:00,960
sort of we're in an overlap of venn

2339
01:29:00,960 --> 01:29:02,719
diagram with certainly many

2340
01:29:02,719 --> 01:29:05,760
classes of models different approaches

2341
01:29:05,760 --> 01:29:09,280
and is it like that blurry intersection

2342
01:29:09,280 --> 01:29:11,280
that we're looking at and that's where

2343
01:29:11,280 --> 01:29:13,520
action and inference

2344
01:29:13,520 --> 01:29:16,560
are being just applied together or is

2345
01:29:16,560 --> 01:29:18,000
there something unique or something that

2346
01:29:18,000 --> 01:29:20,960
we can use as a diagnostic

2347
01:29:20,960 --> 01:29:22,320
well i guess the difference would be

2348
01:29:22,320 --> 01:29:24,239
more than this kind of uh

2349
01:29:24,239 --> 01:29:27,120
this action selection i mean planning is

2350
01:29:27,120 --> 01:29:28,639
inferencing part not then

2351
01:29:28,639 --> 01:29:30,400
not necessarily as perception in its

2352
01:29:30,400 --> 01:29:32,400
inference because as i said

2353
01:29:32,400 --> 01:29:34,719
one can think of many ways how one can

2354
01:29:34,719 --> 01:29:37,120
solve that part

2355
01:29:37,120 --> 01:29:38,800
but kind of once you go into this

2356
01:29:38,800 --> 01:29:41,040
planning as inference part and

2357
01:29:41,040 --> 01:29:44,000
also concept that right your actions

2358
01:29:44,000 --> 01:29:45,199
themselves

2359
01:29:45,199 --> 01:29:48,400
will change beliefs and you will

2360
01:29:48,400 --> 01:29:50,400
choose actions which are the best in

2361
01:29:50,400 --> 01:29:51,840
changing your beliefs or what you want

2362
01:29:51,840 --> 01:29:52,800
to achieve

2363
01:29:52,800 --> 01:29:55,520
then this is like i mean idea of active

2364
01:29:55,520 --> 01:29:58,000
inference in a way it's uh

2365
01:29:58,000 --> 01:30:01,600
let's say it's circular

2366
01:30:01,600 --> 01:30:04,560
inference problem right

2367
01:30:04,719 --> 01:30:08,159
uh it's not in a way it's any

2368
01:30:08,159 --> 01:30:12,080
more so easy to disconnect in the

2369
01:30:12,080 --> 01:30:14,400
effects of choices and effective

2370
01:30:14,400 --> 01:30:16,080
perception

2371
01:30:16,080 --> 01:30:18,880
they all depend on each other and also

2372
01:30:18,880 --> 01:30:20,000
being aware of your

2373
01:30:20,000 --> 01:30:24,320
own uncertainties and i think

2374
01:30:24,320 --> 01:30:26,560
if you do standard reinforcement

2375
01:30:26,560 --> 01:30:27,760
learning where you just

2376
01:30:27,760 --> 01:30:31,199
calculate expected rewards i don't

2377
01:30:31,199 --> 01:30:34,159
think you're always aware of how certain

2378
01:30:34,159 --> 01:30:35,679
or uncertain you actually

2379
01:30:35,679 --> 01:30:37,840
are about your own generative model for

2380
01:30:37,840 --> 01:30:39,199
example

2381
01:30:39,199 --> 01:30:41,360
and then potentially with active

2382
01:30:41,360 --> 01:30:42,400
inference

2383
01:30:42,400 --> 01:30:45,679
it's easier to know which action to take

2384
01:30:45,679 --> 01:30:49,280
in order to better learn your model

2385
01:30:49,280 --> 01:30:52,800
very interesting it's almost like by

2386
01:30:52,800 --> 01:30:56,239
carrying and propagating our uncertainty

2387
01:30:56,239 --> 01:30:58,480
and having a self model of action and

2388
01:30:58,480 --> 01:30:59,679
learning

2389
01:30:59,679 --> 01:31:02,159
then we get that almost like second

2390
01:31:02,159 --> 01:31:02,719
order

2391
01:31:02,719 --> 01:31:05,760
cybernetics where we're acting

2392
01:31:05,760 --> 01:31:07,760
in a way where in the future we can

2393
01:31:07,760 --> 01:31:09,840
expect to learn better or expect to act

2394
01:31:09,840 --> 01:31:10,719
better

2395
01:31:10,719 --> 01:31:14,159
as opposed to just this hungry search

2396
01:31:14,159 --> 01:31:16,880
for the best action and then learning is

2397
01:31:16,880 --> 01:31:18,719
only a one-step

2398
01:31:18,719 --> 01:31:20,800
projection into like what action is

2399
01:31:20,800 --> 01:31:21,920
gonna be informative

2400
01:31:21,920 --> 01:31:24,000
right now uh what's the next wikipedia

2401
01:31:24,000 --> 01:31:25,760
article that's most informative

2402
01:31:25,760 --> 01:31:29,120
rather than what's like the trajectory

2403
01:31:29,120 --> 01:31:31,679
that i expect is going to be resulting

2404
01:31:31,679 --> 01:31:33,040
in more effective learning

2405
01:31:33,040 --> 01:31:36,400
or action again on a common grounding

2406
01:31:36,400 --> 01:31:38,960
so that's pretty interesting looks like

2407
01:31:38,960 --> 01:31:39,840
you have a thought though

2408
01:31:39,840 --> 01:31:44,880
dmitry no i mean uh

2409
01:31:44,880 --> 01:31:46,480
the yeah that that's i mean definitely

2410
01:31:46,480 --> 01:31:48,560
important part but i mean i would then

2411
01:31:48,560 --> 01:31:50,000
call thompson sampling would also

2412
01:31:50,000 --> 01:31:52,239
satisfy that beijing ucb right i mean

2413
01:31:52,239 --> 01:31:53,920
they all kind of

2414
01:31:53,920 --> 01:31:56,159
are any bayesian decision making

2415
01:31:56,159 --> 01:31:57,280
algorithm

2416
01:31:57,280 --> 01:31:59,360
would necessarily have to take into

2417
01:31:59,360 --> 01:32:01,920
account uncertainty

2418
01:32:01,920 --> 01:32:04,159
uh

2419
01:32:05,040 --> 01:32:08,159
if it's derived from a bayesian

2420
01:32:08,159 --> 01:32:12,000
decision-making theory but

2421
01:32:12,000 --> 01:32:15,199
yeah it is not that clear that by this

2422
01:32:15,199 --> 01:32:15,920
aspect that

2423
01:32:15,920 --> 01:32:19,679
okay the actions also have a consequence

2424
01:32:19,679 --> 01:32:21,199
of reducing your uncertainty in the

2425
01:32:21,199 --> 01:32:23,199
future and this is what you can use also

2426
01:32:23,199 --> 01:32:24,320
for

2427
01:32:24,320 --> 01:32:26,639
that's a gauge on which one which action

2428
01:32:26,639 --> 01:32:29,120
is better

2429
01:32:32,960 --> 01:32:40,159
so yeah um

2430
01:32:40,159 --> 01:32:42,560
subtle differences and not that obvious

2431
01:32:42,560 --> 01:32:44,400
always

2432
01:32:44,400 --> 01:32:46,480
it's it's one reason why we're so

2433
01:32:46,480 --> 01:32:47,760
interested in

2434
01:32:47,760 --> 01:32:50,719
in ontology and slowly scaffolding the

2435
01:32:50,719 --> 01:32:52,840
research so that we can actually

2436
01:32:52,840 --> 01:32:54,800
juxtapose different models and

2437
01:32:54,800 --> 01:32:55,440
understand

2438
01:32:55,440 --> 01:32:57,199
where they differ like okay they're kind

2439
01:32:57,199 --> 01:32:59,360
of it's like two road trips and then

2440
01:32:59,360 --> 01:33:00,800
this person just took a little extra

2441
01:33:00,800 --> 01:33:01,679
loop or

2442
01:33:01,679 --> 01:33:03,520
this bridge they crossed this way versus

2443
01:33:03,520 --> 01:33:04,880
this person you know

2444
01:33:04,880 --> 01:33:07,360
took a different route right here we can

2445
01:33:07,360 --> 01:33:08,000
understand like

2446
01:33:08,000 --> 01:33:11,440
you talked about the smile variational

2447
01:33:11,440 --> 01:33:13,840
updating but then just recently you

2448
01:33:13,840 --> 01:33:15,199
mentioned that there's

2449
01:33:15,199 --> 01:33:17,120
that's sort of like a module you can

2450
01:33:17,120 --> 01:33:18,719
switch out

2451
01:33:18,719 --> 01:33:22,239
so what is smile or

2452
01:33:22,239 --> 01:33:24,639
because i also noticed a very recent

2453
01:33:24,639 --> 01:33:25,679
citation

2454
01:33:25,679 --> 01:33:27,199
so what does it do or what is it

2455
01:33:27,199 --> 01:33:28,960
different in regards to other ways you

2456
01:33:28,960 --> 01:33:30,320
could have fit that module

2457
01:33:30,320 --> 01:33:32,480
in

2458
01:33:34,159 --> 01:33:37,120
i mean i have implicitly used smile for

2459
01:33:37,120 --> 01:33:39,520
years now it's just a very simple way of

2460
01:33:39,520 --> 01:33:42,960
updating your beliefs so uh

2461
01:33:42,960 --> 01:33:45,120
so the problem a bit with kind of uh

2462
01:33:45,120 --> 01:33:46,880
variational inference if you want to

2463
01:33:46,880 --> 01:33:48,480
find the

2464
01:33:48,480 --> 01:33:51,600
minima of approximate posterior you kind

2465
01:33:51,600 --> 01:33:51,920
of

2466
01:33:51,920 --> 01:33:55,199
necessarily need to iterate uh

2467
01:33:55,199 --> 01:33:57,120
through several loops like minimizing

2468
01:33:57,120 --> 01:33:58,400
the gradient uh

2469
01:33:58,400 --> 01:33:59,840
so following the gradient of the

2470
01:33:59,840 --> 01:34:02,239
variation of free energy right

2471
01:34:02,239 --> 01:34:05,600
and i mean this also doesn't make it

2472
01:34:05,600 --> 01:34:07,679
very efficient

2473
01:34:07,679 --> 01:34:10,880
uh in for

2474
01:34:10,880 --> 01:34:14,000
this kind of applications so for me at

2475
01:34:14,000 --> 01:34:15,600
least right this variation smile

2476
01:34:15,600 --> 01:34:16,560
approach

2477
01:34:16,560 --> 01:34:18,719
side steps this iteration so you can

2478
01:34:18,719 --> 01:34:20,719
kind of transform variational inference

2479
01:34:20,719 --> 01:34:23,280
into just a single update step

2480
01:34:23,280 --> 01:34:25,679
uh just because there is a way part

2481
01:34:25,679 --> 01:34:26,800
which you can

2482
01:34:26,800 --> 01:34:28,639
compute explicitly and you just assume

2483
01:34:28,639 --> 01:34:31,040
okay this is now my fixed belief about

2484
01:34:31,040 --> 01:34:32,080
that

2485
01:34:32,080 --> 01:34:33,600
and the other part which you need still

2486
01:34:33,600 --> 01:34:35,280
to update

2487
01:34:35,280 --> 01:34:36,970
through variational approximation

2488
01:34:36,970 --> 01:34:38,719
[Music]

2489
01:34:38,719 --> 01:34:41,760
uh and so this is i guess

2490
01:34:41,760 --> 01:34:45,199
a small advantage here

2491
01:34:45,199 --> 01:34:47,280
but as i said right there are other

2492
01:34:47,280 --> 01:34:49,600
approaches how you can com

2493
01:34:49,600 --> 01:34:52,560
update beliefs in these scenarios and uh

2494
01:34:52,560 --> 01:34:54,159
you know in the bayesian sense there can

2495
01:34:54,159 --> 01:34:55,040
also be more

2496
01:34:55,040 --> 01:34:57,040
uh optimal but they just bring you

2497
01:34:57,040 --> 01:35:00,719
closer to the exact posterior

2498
01:35:00,800 --> 01:35:04,320
so but i mean we tested these things

2499
01:35:04,320 --> 01:35:07,600
uh with some examples and

2500
01:35:07,600 --> 01:35:09,520
the story remains the same so there is

2501
01:35:09,520 --> 01:35:10,880
no gain on

2502
01:35:10,880 --> 01:35:12,719
making things more complex on that on

2503
01:35:12,719 --> 01:35:14,960
that

2504
01:35:15,840 --> 01:35:18,480
but one can also imagine that like in

2505
01:35:18,480 --> 01:35:19,760
different

2506
01:35:19,760 --> 01:35:23,040
environments better generality models

2507
01:35:23,040 --> 01:35:25,199
and approximation rules

2508
01:35:25,199 --> 01:35:27,520
would take you further so but for

2509
01:35:27,520 --> 01:35:29,360
bernoulli bandit this is just

2510
01:35:29,360 --> 01:35:33,040
not the case what kind of empirical

2511
01:35:33,040 --> 01:35:36,159
data sets are almost

2512
01:35:36,159 --> 01:35:37,840
whether they're open source or just

2513
01:35:37,840 --> 01:35:40,000
obtainable are amenable

2514
01:35:40,000 --> 01:35:41,760
to this kind of analysis like if

2515
01:35:41,760 --> 01:35:43,760
somebody hears about the algorithm

2516
01:35:43,760 --> 01:35:45,119
and they kind of want to see it in

2517
01:35:45,119 --> 01:35:46,880
action or play with it themselves once

2518
01:35:46,880 --> 01:35:47,600
you're

2519
01:35:47,600 --> 01:35:49,119
you know two weeks from the recording of

2520
01:35:49,119 --> 01:35:51,760
this when your code is available like

2521
01:35:51,760 --> 01:35:54,239
is it possible is your code set up for

2522
01:35:54,239 --> 01:35:56,880
more of a simulation or is it something

2523
01:35:56,880 --> 01:35:57,280
where

2524
01:35:57,280 --> 01:35:59,520
we can plug in a type of data set that

2525
01:35:59,520 --> 01:36:00,639
might already be structured

2526
01:36:00,639 --> 01:36:03,280
appropriately

2527
01:36:03,520 --> 01:36:06,960
uh it's currently just for simulations

2528
01:36:06,960 --> 01:36:10,000
but i mean one could potentially write

2529
01:36:10,000 --> 01:36:12,719
play out with different algorithms add

2530
01:36:12,719 --> 01:36:14,639
other algorithms

2531
01:36:14,639 --> 01:36:17,280
both under like a learning part and the

2532
01:36:17,280 --> 01:36:19,119
action selection

2533
01:36:19,119 --> 01:36:21,920
i mean that that would be super useful

2534
01:36:21,920 --> 01:36:24,400
for everybody

2535
01:36:24,400 --> 01:36:26,960
yeah if there are people interested in

2536
01:36:26,960 --> 01:36:28,560
that where that would be

2537
01:36:28,560 --> 01:36:31,920
our repository is open and welcoming any

2538
01:36:31,920 --> 01:36:34,480
contribution

2539
01:36:34,480 --> 01:36:37,280
it was part of the structuring of your

2540
01:36:37,280 --> 01:36:38,080
paper that

2541
01:36:38,080 --> 01:36:42,000
like made us excited to juxtapose

2542
01:36:42,000 --> 01:36:43,760
it with these other approaches is you

2543
01:36:43,760 --> 01:36:45,600
kind of showed that you can

2544
01:36:45,600 --> 01:36:47,280
directly compare active inference

2545
01:36:47,280 --> 01:36:49,520
alongside other models

2546
01:36:49,520 --> 01:36:51,040
obviously it's something we're coming

2547
01:36:51,040 --> 01:36:52,560
back to because that's the crux of the

2548
01:36:52,560 --> 01:36:53,920
results of the paper

2549
01:36:53,920 --> 01:36:56,960
is really the different dynamics as

2550
01:36:56,960 --> 01:36:58,719
time increased or as the relative

2551
01:36:58,719 --> 01:37:00,159
challenge increased as the number of

2552
01:37:00,159 --> 01:37:01,440
arms was changing

2553
01:37:01,440 --> 01:37:04,960
between these different styles uh

2554
01:37:04,960 --> 01:37:07,280
and the two styles of bandit so it's

2555
01:37:07,280 --> 01:37:09,040
kind of a cool thing that people could

2556
01:37:09,040 --> 01:37:09,600
both

2557
01:37:09,600 --> 01:37:12,159
build on directly what you're working on

2558
01:37:12,159 --> 01:37:13,119
but also

2559
01:37:13,119 --> 01:37:15,600
maybe more broadly instead of just a

2560
01:37:15,600 --> 01:37:18,239
single model being presented in a paper

2561
01:37:18,239 --> 01:37:21,360
people could just include multiple

2562
01:37:21,360 --> 01:37:24,320
types of models in their sort of

2563
01:37:24,320 --> 01:37:25,840
baseline paper

2564
01:37:25,840 --> 01:37:27,280
so that we wouldn't just have to read

2565
01:37:27,280 --> 01:37:29,040
the paper that says this algorithm works

2566
01:37:29,040 --> 01:37:30,239
well

2567
01:37:30,239 --> 01:37:33,440
we could see them directly compared

2568
01:37:33,440 --> 01:37:34,719
and that's something that more and more

2569
01:37:34,719 --> 01:37:37,440
papers are doing with active inference

2570
01:37:37,440 --> 01:37:40,560
yeah i mean the

2571
01:37:40,560 --> 01:37:43,440
the kind of also

2572
01:37:44,000 --> 01:37:47,199
the libraries we use is also this is

2573
01:37:47,199 --> 01:37:48,000
something

2574
01:37:48,000 --> 01:37:50,080
which was developed relatively recently

2575
01:37:50,080 --> 01:37:51,679
like

2576
01:37:51,679 --> 01:37:56,800
uh by google jacks it's kind of uh

2577
01:37:59,760 --> 01:38:04,000
acceleration for linear algebra algebra

2578
01:38:04,000 --> 01:38:06,400
libraries so which allows you to run

2579
01:38:06,400 --> 01:38:07,679
very fast

2580
01:38:07,679 --> 01:38:10,000
this code and it also integrates well

2581
01:38:10,000 --> 01:38:11,840
with the probabilistic programming

2582
01:38:11,840 --> 01:38:13,280
language so it's a

2583
01:38:13,280 --> 01:38:16,960
numpyro which was also uh

2584
01:38:16,960 --> 01:38:18,960
developed really recently so one could

2585
01:38:18,960 --> 01:38:20,000
be in practice

2586
01:38:20,000 --> 01:38:22,400
link this to any behavioral data or to

2587
01:38:22,400 --> 01:38:23,600
create some test data

2588
01:38:23,600 --> 01:38:26,639
uh very easily uh

2589
01:38:26,639 --> 01:38:29,360
with a few lines of code so yeah there

2590
01:38:29,360 --> 01:38:31,440
is also this perspective

2591
01:38:31,440 --> 01:38:34,719
what language is the code in one or

2592
01:38:34,719 --> 01:38:35,360
multiple

2593
01:38:35,360 --> 01:38:39,119
and then what areas of conceptual math

2594
01:38:39,119 --> 01:38:41,520
do you think somebody would want to know

2595
01:38:41,520 --> 01:38:44,800
before diving in

2596
01:38:44,800 --> 01:38:46,719
so the language isn't i mean like

2597
01:38:46,719 --> 01:38:48,320
program language is python and just

2598
01:38:48,320 --> 01:38:50,480
depends on as i said couple of libraries

2599
01:38:50,480 --> 01:38:50,800
like

2600
01:38:50,800 --> 01:38:54,480
jacks numpy very standard stuff except

2601
01:38:54,480 --> 01:38:55,520
the jacks that's that's

2602
01:38:55,520 --> 01:38:59,440
new so yeah somebody

2603
01:38:59,440 --> 01:39:01,280
interested in using it would have to

2604
01:39:01,280 --> 01:39:04,000
learn a bit about jacks but that's

2605
01:39:04,000 --> 01:39:08,320
a long term also useful so go for it

2606
01:39:08,400 --> 01:39:13,440
but from the matte side yeah i'm

2607
01:39:13,440 --> 01:39:17,119
well i mean maybe getting a couple of

2608
01:39:17,119 --> 01:39:18,880
introductions to multi-arm bandits that

2609
01:39:18,880 --> 01:39:20,239
would be kind of

2610
01:39:20,239 --> 01:39:23,600
a good place to start uh right there

2611
01:39:23,600 --> 01:39:27,040
there is a

2612
01:39:27,040 --> 01:39:30,560
there is a recent uh

2613
01:39:30,560 --> 01:39:33,360
quite recent introductory paper mostly

2614
01:39:33,360 --> 01:39:34,960
on blended which exposes

2615
01:39:34,960 --> 01:39:36,320
many different algorithms in the

2616
01:39:36,320 --> 01:39:40,159
stationary concept so uh

2617
01:39:40,159 --> 01:39:42,960
provides a bit insights about this uh

2618
01:39:42,960 --> 01:39:44,000
historically

2619
01:39:44,000 --> 01:39:46,480
historic way how they analyze uh these

2620
01:39:46,480 --> 01:39:47,040
problems

2621
01:39:47,040 --> 01:39:50,400
so yeah uh i would suggest that that's

2622
01:39:50,400 --> 01:39:51,600
the kind of potential

2623
01:39:51,600 --> 01:39:55,280
place to start cool blue

2624
01:39:55,280 --> 01:39:57,119
so i'm curious about this google docs i

2625
01:39:57,119 --> 01:39:58,880
haven't heard of heard about it but

2626
01:39:58,880 --> 01:40:00,239
like i thought google had their own

2627
01:40:00,239 --> 01:40:01,520
language like don't they have like

2628
01:40:01,520 --> 01:40:03,199
golang right isn't that google

2629
01:40:03,199 --> 01:40:07,280
or tensorflow right

2630
01:40:07,280 --> 01:40:11,519
well yeah tensorflow

2631
01:40:12,560 --> 01:40:14,800
yes they have tensorflow and i think the

2632
01:40:14,800 --> 01:40:15,760
basis is

2633
01:40:15,760 --> 01:40:19,920
uh similar xla

2634
01:40:19,920 --> 01:40:22,239
uh it's called like accel accelerated

2635
01:40:22,239 --> 01:40:24,080
linear algebra

2636
01:40:24,080 --> 01:40:27,199
uh for both tensorflow and jx

2637
01:40:27,199 --> 01:40:29,760
so what jax is is basically accelerated

2638
01:40:29,760 --> 01:40:31,600
numpy

2639
01:40:31,600 --> 01:40:34,719
so you can just run standard nupi

2640
01:40:34,719 --> 01:40:38,800
code and in pure python more or less

2641
01:40:38,800 --> 01:40:41,920
uh and get very um

2642
01:40:41,920 --> 01:40:45,040
so you you get for free like uh um

2643
01:40:45,040 --> 01:40:48,000
computational gradients so it's kind of

2644
01:40:48,000 --> 01:40:48,880
autograd

2645
01:40:48,880 --> 01:40:52,480
library so you can kind of the compute

2646
01:40:52,480 --> 01:40:55,520
gradients on very complex graphs

2647
01:40:55,520 --> 01:40:57,440
uh this is also something which

2648
01:40:57,440 --> 01:40:59,040
tensorflow allows you

2649
01:40:59,040 --> 01:41:02,239
but this seems to provide more more

2650
01:41:02,239 --> 01:41:04,880
benefits for this dynamical scenarios

2651
01:41:04,880 --> 01:41:08,239
so i have problems when i tried learning

2652
01:41:08,239 --> 01:41:10,080
or using tensorflow it's very difficult

2653
01:41:10,080 --> 01:41:10,800
to think

2654
01:41:10,800 --> 01:41:14,239
into right code which is dynamic there

2655
01:41:14,239 --> 01:41:17,760
and that's why i i never actually start

2656
01:41:17,760 --> 01:41:20,239
started using it i started with pytorch

2657
01:41:20,239 --> 01:41:21,119
at some point

2658
01:41:21,119 --> 01:41:24,880
and now that jax showed up that has some

2659
01:41:24,880 --> 01:41:26,960
speed up advantages it's also kind of

2660
01:41:26,960 --> 01:41:28,480
quite lucrative

2661
01:41:28,480 --> 01:41:30,639
so one code question just to stay on

2662
01:41:30,639 --> 01:41:32,080
this theme and then ask a question from

2663
01:41:32,080 --> 01:41:32,800
the chat

2664
01:41:32,800 --> 01:41:37,199
what about the um python implementation

2665
01:41:37,199 --> 01:41:40,320
like uh i think infer actively or

2666
01:41:40,320 --> 01:41:43,360
what alec chants at all have

2667
01:41:43,360 --> 01:41:45,199
worked on what are the similarities or

2668
01:41:45,199 --> 01:41:46,400
differences with their

2669
01:41:46,400 --> 01:41:49,840
python approach there

2670
01:41:49,840 --> 01:41:52,719
i think currently none because i'm also

2671
01:41:52,719 --> 01:41:54,159
involved in that so

2672
01:41:54,159 --> 01:41:57,360
they will also start using text okay i

2673
01:41:57,360 --> 01:41:58,000
guess

2674
01:41:58,000 --> 01:42:00,960
cool so that those threads will join

2675
01:42:00,960 --> 01:42:03,280
together

2676
01:42:03,280 --> 01:42:06,000
okay uh but i mean this also code just

2677
01:42:06,000 --> 01:42:07,280
like produces some of the

2678
01:42:07,280 --> 01:42:09,199
i mean one could also write use just the

2679
01:42:09,199 --> 01:42:11,679
spm code but this is super

2680
01:42:11,679 --> 01:42:16,320
complex uh right it would be super slow

2681
01:42:16,320 --> 01:42:17,920
so for example just to examine

2682
01:42:17,920 --> 01:42:19,920
numerically this stationary scenario

2683
01:42:19,920 --> 01:42:20,719
this

2684
01:42:20,719 --> 01:42:22,290
would take months probably too

2685
01:42:22,290 --> 01:42:24,480
[Music]

2686
01:42:24,480 --> 01:42:28,560
okay nice somebody's ranking

2687
01:42:28,560 --> 01:42:32,159
uh so yeah um

2688
01:42:32,159 --> 01:42:35,360
right that's uh uh that's kind of the

2689
01:42:35,360 --> 01:42:37,840
the problem there and this i wrote the

2690
01:42:37,840 --> 01:42:39,520
code to be kind of very efficient in a

2691
01:42:39,520 --> 01:42:40,880
way so just kind of

2692
01:42:40,880 --> 01:42:42,400
removes lots of complexity that you

2693
01:42:42,400 --> 01:42:44,239
don't need

2694
01:42:44,239 --> 01:42:46,719
cool uh all the right in some scenarios

2695
01:42:46,719 --> 01:42:48,159
you want to have general

2696
01:42:48,159 --> 01:42:51,679
uh description of the program so yeah

2697
01:42:51,679 --> 01:42:53,840
so here's a question from stephen in the

2698
01:42:53,840 --> 01:42:55,199
chat

2699
01:42:55,199 --> 01:42:57,280
do you think that parallel modeling

2700
01:42:57,280 --> 01:42:59,199
processes might be used more in the

2701
01:42:59,199 --> 01:42:59,840
future

2702
01:42:59,840 --> 01:43:01,520
with different model approaches

2703
01:43:01,520 --> 01:43:03,040
highlighting different patterns of

2704
01:43:03,040 --> 01:43:03,679
behavior

2705
01:43:03,679 --> 01:43:10,239
happening in different niche contexts

2706
01:43:10,239 --> 01:43:11,600
i would say in some ways it's what your

2707
01:43:11,600 --> 01:43:13,440
paper did but what are you thinking

2708
01:43:13,440 --> 01:43:15,199
about yeah

2709
01:43:15,199 --> 01:43:18,719
well i mean from practical side yes i

2710
01:43:18,719 --> 01:43:19,600
mean that's what

2711
01:43:19,600 --> 01:43:22,800
you should be doing because just figure

2712
01:43:22,800 --> 01:43:24,159
out what works best

2713
01:43:24,159 --> 01:43:27,679
and just don't don't think

2714
01:43:27,679 --> 01:43:31,119
too much about well the philosophical

2715
01:43:31,119 --> 01:43:33,280
side of things

2716
01:43:33,280 --> 01:43:36,159
uh but yeah but i mean i guess long term

2717
01:43:36,159 --> 01:43:38,000
one could i mean imagine scenario where

2718
01:43:38,000 --> 01:43:39,920
more and more things become

2719
01:43:39,920 --> 01:43:43,040
generalizable to this description

2720
01:43:43,040 --> 01:43:46,159
so it's just in a way a

2721
01:43:46,159 --> 01:43:49,360
difficult process takes time even for

2722
01:43:49,360 --> 01:43:50,159
those who are

2723
01:43:50,159 --> 01:43:52,480
just learning about the technical

2724
01:43:52,480 --> 01:43:54,239
details like the visual

2725
01:43:54,239 --> 01:43:56,960
tell for me was that the figures were a

2726
01:43:56,960 --> 01:43:57,679
grid

2727
01:43:57,679 --> 01:44:01,040
of graphs so it was kind of like

2728
01:44:01,040 --> 01:44:03,600
three different settings of difficulty

2729
01:44:03,600 --> 01:44:04,320
or three

2730
01:44:04,320 --> 01:44:06,560
or five different settings of arms it

2731
01:44:06,560 --> 01:44:07,679
wasn't just showing

2732
01:44:07,679 --> 01:44:10,719
we ran it with one parameter combination

2733
01:44:10,719 --> 01:44:14,159
each of those graphs was a grid of

2734
01:44:14,159 --> 01:44:17,360
combinatorics and then you presented

2735
01:44:17,360 --> 01:44:19,119
what i guess could be considered two

2736
01:44:19,119 --> 01:44:20,400
different niches

2737
01:44:20,400 --> 01:44:22,480
with the static and the dynamic and then

2738
01:44:22,480 --> 01:44:23,920
today in the presentation we

2739
01:44:23,920 --> 01:44:26,159
heard about all these other variations

2740
01:44:26,159 --> 01:44:27,679
and so those are like

2741
01:44:27,679 --> 01:44:30,960
kind of toggles in the code you can say

2742
01:44:30,960 --> 01:44:31,679
i'll take

2743
01:44:31,679 --> 01:44:34,800
no a1 alpha or i'll take

2744
01:44:34,800 --> 01:44:38,239
a2 beta as far as the combinations of

2745
01:44:38,239 --> 01:44:39,360
how to run it

2746
01:44:39,360 --> 01:44:42,320
and so as the code becomes more

2747
01:44:42,320 --> 01:44:43,440
interoperable

2748
01:44:43,440 --> 01:44:46,400
and pruned down to really the necessary

2749
01:44:46,400 --> 01:44:47,360
pieces

2750
01:44:47,360 --> 01:44:49,600
then it becomes easier and easier to

2751
01:44:49,600 --> 01:44:51,520
expand it back out

2752
01:44:51,520 --> 01:44:54,080
so that we can choose amongst different

2753
01:44:54,080 --> 01:44:54,880
options

2754
01:44:54,880 --> 01:44:57,360
for a given piece and so that's like

2755
01:44:57,360 --> 01:44:59,280
this skeleton

2756
01:44:59,280 --> 01:45:03,840
that so many variations flow off of

2757
01:45:04,239 --> 01:45:07,360
yeah um

2758
01:45:07,360 --> 01:45:11,440
cool well in our last few minutes here

2759
01:45:11,440 --> 01:45:14,800
where would you like to uh take it

2760
01:45:14,800 --> 01:45:18,159
next week or beyond i mean what are your

2761
01:45:18,159 --> 01:45:22,480
current interests or curiosities

2762
01:45:22,960 --> 01:45:24,719
well i mean for me it's just important

2763
01:45:24,719 --> 01:45:26,639
to have this paper out so next time when

2764
01:45:26,639 --> 01:45:28,000
a reviewer asked me

2765
01:45:28,000 --> 01:45:31,119
what about that and that algorithm i can

2766
01:45:31,119 --> 01:45:33,760
say okay look

2767
01:45:33,760 --> 01:45:35,600
we analyze this it's like that it's

2768
01:45:35,600 --> 01:45:37,199
similar or different

2769
01:45:37,199 --> 01:45:39,119
you can you can get whatever you want

2770
01:45:39,119 --> 01:45:40,560
basically there

2771
01:45:40,560 --> 01:45:42,719
uh right i mean because you have these

2772
01:45:42,719 --> 01:45:44,000
adapted parameters

2773
01:45:44,000 --> 01:45:45,840
in a way you can generate very different

2774
01:45:45,840 --> 01:45:48,320
behavior one can also think okay if you

2775
01:45:48,320 --> 01:45:49,840
have like an agent which

2776
01:45:49,840 --> 01:45:51,679
behaves as a thompson sampling what

2777
01:45:51,679 --> 01:45:52,880
would be the corresponding

2778
01:45:52,880 --> 01:45:54,719
parameter in active inference framework

2779
01:45:54,719 --> 01:45:56,719
which would emulate that

2780
01:45:56,719 --> 01:45:58,159
can you actually even differentiate

2781
01:45:58,159 --> 01:45:59,840
between them

2782
01:45:59,840 --> 01:46:02,639
so uh i mean this is uh potentially

2783
01:46:02,639 --> 01:46:03,840
interesting questions which

2784
01:46:03,840 --> 01:46:06,960
one can try to answer and which are just

2785
01:46:06,960 --> 01:46:07,840
from

2786
01:46:07,840 --> 01:46:10,840
my work kind of relevant because of this

2787
01:46:10,840 --> 01:46:13,280
constant uh questions i'm getting in

2788
01:46:13,280 --> 01:46:15,360
during the review process

2789
01:46:15,360 --> 01:46:19,040
uh and that was actually

2790
01:46:19,520 --> 01:46:22,960
yeah but then there is also this like

2791
01:46:22,960 --> 01:46:24,840
quite interesting side in like machine

2792
01:46:24,840 --> 01:46:26,000
learning

2793
01:46:26,000 --> 01:46:28,400
uh where this can find potentially quite

2794
01:46:28,400 --> 01:46:30,480
interesting applications

2795
01:46:30,480 --> 01:46:34,719
uh i i got started a bit uh

2796
01:46:34,719 --> 01:46:36,480
recently working with kind of monte

2797
01:46:36,480 --> 01:46:38,480
carlo research

2798
01:46:38,480 --> 01:46:40,960
and interestingly uh sorry this is

2799
01:46:40,960 --> 01:46:42,880
something which was applied in active

2800
01:46:42,880 --> 01:46:44,639
inference as a way to compute

2801
01:46:44,639 --> 01:46:47,520
expected free energy in complex problem

2802
01:46:47,520 --> 01:46:48,960
but turns out that

2803
01:46:48,960 --> 01:46:50,560
if you have kind of very complex

2804
01:46:50,560 --> 01:46:53,440
problems you can also

2805
01:46:53,440 --> 01:46:55,760
use thomson sampling in monte carlo

2806
01:46:55,760 --> 01:46:58,080
research

2807
01:46:58,080 --> 01:47:01,360
uh right so as a way to

2808
01:47:01,360 --> 01:47:04,400
just figure out what is the best

2809
01:47:04,400 --> 01:47:08,480
path to follow uh in a sample

2810
01:47:08,480 --> 01:47:11,199
and this potentially provides if this

2811
01:47:11,199 --> 01:47:13,040
stationary scenario can be improved

2812
01:47:13,040 --> 01:47:13,760
somehow and

2813
01:47:13,760 --> 01:47:15,760
uh this is also kind of you can apply

2814
01:47:15,760 --> 01:47:18,320
active inference to

2815
01:47:18,320 --> 01:47:21,280
planning inside active inference itself

2816
01:47:21,280 --> 01:47:22,320
you know in a kind of

2817
01:47:22,320 --> 01:47:25,360
uh hierarchical circular way though

2818
01:47:25,360 --> 01:47:30,000
right so what does that look like or

2819
01:47:30,000 --> 01:47:32,320
how does it get implemented or how is it

2820
01:47:32,320 --> 01:47:34,000
different than just

2821
01:47:34,000 --> 01:47:37,199
straightforward active inference

2822
01:47:37,199 --> 01:47:40,239
uh so the problem is like that when you

2823
01:47:40,239 --> 01:47:42,480
have quite a complex decision

2824
01:47:42,480 --> 01:47:44,320
making in a planning problem where you

2825
01:47:44,320 --> 01:47:46,159
have multiple

2826
01:47:46,159 --> 01:47:48,000
branches in the future which you have to

2827
01:47:48,000 --> 01:47:50,159
go it's like not practical to compute

2828
01:47:50,159 --> 01:47:53,280
everything uh so what people do is

2829
01:47:53,280 --> 01:47:55,760
a very popular way to do is like monte

2830
01:47:55,760 --> 01:47:57,440
carlo research

2831
01:47:57,440 --> 01:47:59,199
where you just sample different paths

2832
01:47:59,199 --> 01:48:01,280
you you

2833
01:48:01,280 --> 01:48:05,280
estimate us on a sub sample of possible

2834
01:48:05,280 --> 01:48:06,320
paths what is like

2835
01:48:06,320 --> 01:48:09,440
uh the best path to to go and this would

2836
01:48:09,440 --> 01:48:10,960
correspond in active inference like

2837
01:48:10,960 --> 01:48:13,040
you're

2838
01:48:13,040 --> 01:48:15,360
estimating expected free energy of a

2839
01:48:15,360 --> 01:48:16,159
path

2840
01:48:16,159 --> 01:48:19,679
in the future uh just through a sample

2841
01:48:19,679 --> 01:48:21,440
but then you have a problem okay how do

2842
01:48:21,440 --> 01:48:23,600
i select the same the paths

2843
01:48:23,600 --> 01:48:25,360
in the sample and then you can apply it

2844
01:48:25,360 --> 01:48:26,800
active inference

2845
01:48:26,800 --> 01:48:30,320
to the pet selection itself so right you

2846
01:48:30,320 --> 01:48:31,600
can kind of choose

2847
01:48:31,600 --> 01:48:33,840
which paths i should sample randomly

2848
01:48:33,840 --> 01:48:35,920
when i'm trying to approximate

2849
01:48:35,920 --> 01:48:38,239
uh expected free energy for this problem

2850
01:48:38,239 --> 01:48:39,679
itself

2851
01:48:39,679 --> 01:48:43,119
so that's kind of what makes it kind of

2852
01:48:43,119 --> 01:48:46,320
interesting is a possibility to explore

2853
01:48:46,320 --> 01:48:49,600
and the way that you just framed it

2854
01:48:49,600 --> 01:48:52,080
as well as what we've seen which is the

2855
01:48:52,080 --> 01:48:52,800
relative

2856
01:48:52,800 --> 01:48:55,199
strength of active inference in dynamic

2857
01:48:55,199 --> 01:48:56,159
settings

2858
01:48:56,159 --> 01:48:58,480
it's consistent with a lot of the

2859
01:48:58,480 --> 01:49:00,639
qualitative and philosophical ways that

2860
01:49:00,639 --> 01:49:01,119
people

2861
01:49:01,119 --> 01:49:03,119
are talking about active inference as

2862
01:49:03,119 --> 01:49:04,320
like a sense making

2863
01:49:04,320 --> 01:49:07,199
or a way finding or a navigation

2864
01:49:07,199 --> 01:49:08,159
approach

2865
01:49:08,159 --> 01:49:11,360
rather than a sort of cut and dry

2866
01:49:11,360 --> 01:49:13,599
calculus of decision making just

2867
01:49:13,599 --> 01:49:14,719
resulting in

2868
01:49:14,719 --> 01:49:17,520
the total you know crystal path just

2869
01:49:17,520 --> 01:49:19,440
being laid out before you

2870
01:49:19,440 --> 01:49:21,119
it's really about the instantaneous

2871
01:49:21,119 --> 01:49:22,880
actions that we take now

2872
01:49:22,880 --> 01:49:24,639
in light of uncertainty about the

2873
01:49:24,639 --> 01:49:26,000
present and really the past and the

2874
01:49:26,000 --> 01:49:27,360
future as well

2875
01:49:27,360 --> 01:49:30,719
so it's always cool to see how the

2876
01:49:30,719 --> 01:49:32,320
technical developments while they're

2877
01:49:32,320 --> 01:49:35,760
like kind of weaving and recombining

2878
01:49:35,760 --> 01:49:37,440
they proliferate and then we see oh

2879
01:49:37,440 --> 01:49:38,800
actually these three are kind of

2880
01:49:38,800 --> 01:49:40,400
interchangeable or these three are

2881
01:49:40,400 --> 01:49:41,520
complementary

2882
01:49:41,520 --> 01:49:44,960
and then we get more technical detail

2883
01:49:44,960 --> 01:49:47,520
and speed ups while we also get more and

2884
01:49:47,520 --> 01:49:48,000
more

2885
01:49:48,000 --> 01:49:50,480
clarity on what the structure of this

2886
01:49:50,480 --> 01:49:53,678
sense making problem is

2887
01:49:55,040 --> 01:49:57,840
yeah i agree

2888
01:49:58,159 --> 01:50:00,960
any final thoughts here from anyone

2889
01:50:00,960 --> 01:50:02,800
otherwise this was a super

2890
01:50:02,800 --> 01:50:06,080
interesting presentation and questions

2891
01:50:06,080 --> 01:50:06,800
so we really

2892
01:50:06,800 --> 01:50:10,719
appreciate it

2893
01:50:10,719 --> 01:50:15,040
thank you cool yeah thank you

2894
01:50:15,040 --> 01:50:16,639
yeah we're looking forward to for the

2895
01:50:16,639 --> 01:50:18,480
next week great

2896
01:50:18,480 --> 01:50:21,520
so thanks everyone for joining and

2897
01:50:21,520 --> 01:50:22,560
everyone's welcome

2898
01:50:22,560 --> 01:50:25,760
to join live for next week when we'll

2899
01:50:25,760 --> 01:50:26,960
continue the discussion

2900
01:50:26,960 --> 01:50:29,199
and the dot 2 is kind of like our

2901
01:50:29,199 --> 01:50:30,000
jumping off

2902
01:50:30,000 --> 01:50:32,400
into the the unknown unknown instead of

2903
01:50:32,400 --> 01:50:33,840
just the known unknown

2904
01:50:33,840 --> 01:50:36,400
so thanks again for joining and we'll

2905
01:50:36,400 --> 01:50:38,080
see you next week

2906
01:50:38,080 --> 01:50:43,760
thanks bye bye


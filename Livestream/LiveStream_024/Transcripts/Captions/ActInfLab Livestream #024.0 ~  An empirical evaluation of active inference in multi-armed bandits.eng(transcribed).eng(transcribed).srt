1
00:00:08,480 --> 00:00:09,040
hello

2
00:00:09,040 --> 00:00:12,000
everyone welcome to actin flab live

3
00:00:12,000 --> 00:00:14,400
stream number 24.0

4
00:00:14,400 --> 00:00:18,000
today is june 16th 2021

5
00:00:18,000 --> 00:00:19,920
and we're going to be talking about this

6
00:00:19,920 --> 00:00:22,560
paper in empirical evaluation

7
00:00:22,560 --> 00:00:25,039
of active inference in multi-armed

8
00:00:25,039 --> 00:00:26,000
bandits

9
00:00:26,000 --> 00:00:30,320
i'm daniel and i'm here with blue hi

10
00:00:30,320 --> 00:00:32,880
awesome welcome to the active inference

11
00:00:32,880 --> 00:00:34,239
lab everyone

12
00:00:34,239 --> 00:00:37,120
we are a participatory online lab that

13
00:00:37,120 --> 00:00:38,239
is communicating

14
00:00:38,239 --> 00:00:40,320
learning and practicing applied active

15
00:00:40,320 --> 00:00:41,920
inference you can find us

16
00:00:41,920 --> 00:00:44,800
at the links here on this page this is

17
00:00:44,800 --> 00:00:45,760
recorded in an

18
00:00:45,760 --> 00:00:48,000
archived live stream so please provide

19
00:00:48,000 --> 00:00:50,000
us with feedback so that we can improve

20
00:00:50,000 --> 00:00:52,239
our work all backgrounds and

21
00:00:52,239 --> 00:00:54,000
perspectives are welcome here

22
00:00:54,000 --> 00:00:55,280
and we'll be following good video

23
00:00:55,280 --> 00:00:57,760
etiquette for live streams

24
00:00:57,760 --> 00:01:00,640
here at the short link you'll find all

25
00:01:00,640 --> 00:01:02,000
of the live streams

26
00:01:02,000 --> 00:01:04,400
and different series that we do in the

27
00:01:04,400 --> 00:01:06,880
communications unit of actin flab

28
00:01:06,880 --> 00:01:08,720
and today we're going to be

29
00:01:08,720 --> 00:01:11,200
contextualizing in the dot zero video

30
00:01:11,200 --> 00:01:13,520
for two upcoming discussions in the

31
00:01:13,520 --> 00:01:14,400
second half

32
00:01:14,400 --> 00:01:17,600
of june 2021 on the 22nd

33
00:01:17,600 --> 00:01:19,920
and the 29th when we have discussion

34
00:01:19,920 --> 00:01:21,040
24.1

35
00:01:21,040 --> 00:01:24,479
and 24.2 on this paper and hopefully

36
00:01:24,479 --> 00:01:27,280
with the authors joining

37
00:01:27,280 --> 00:01:31,520
today in actin livestream number 24.0

38
00:01:31,520 --> 00:01:33,680
we're going to be trying to set some

39
00:01:33,680 --> 00:01:35,840
context and give an introduction

40
00:01:35,840 --> 00:01:38,400
to the following paper an empirical

41
00:01:38,400 --> 00:01:40,479
evaluation of active inference in

42
00:01:40,479 --> 00:01:41,600
multi-armed

43
00:01:41,600 --> 00:01:45,040
bandits by the authors listed here

44
00:01:45,040 --> 00:01:47,200
and the video is just an introduction to

45
00:01:47,200 --> 00:01:48,799
some of the ideas

46
00:01:48,799 --> 00:01:51,360
it's not a review or a final word it's

47
00:01:51,360 --> 00:01:53,200
kind of like a three-way

48
00:01:53,200 --> 00:01:55,280
intersection we have people who are

49
00:01:55,280 --> 00:01:56,960
maybe within the active inference

50
00:01:56,960 --> 00:01:58,320
community and looking to be

51
00:01:58,320 --> 00:02:00,079
exposed to some different areas like

52
00:02:00,079 --> 00:02:02,479
bayesian statistics or machine learning

53
00:02:02,479 --> 00:02:04,799
the second road is those who are coming

54
00:02:04,799 --> 00:02:06,640
from bayesian statistics or machine

55
00:02:06,640 --> 00:02:07,600
learning approaches

56
00:02:07,600 --> 00:02:09,840
and curious about active inference and

57
00:02:09,840 --> 00:02:11,440
then of course we hope that this will be

58
00:02:11,440 --> 00:02:12,720
exciting and interesting

59
00:02:12,720 --> 00:02:14,560
even if you're unfamiliar with active

60
00:02:14,560 --> 00:02:16,239
inference or machine learning

61
00:02:16,239 --> 00:02:17,920
we'll hopefully try to connect it to

62
00:02:17,920 --> 00:02:19,360
some broader questions

63
00:02:19,360 --> 00:02:21,840
in behavior and decision making more

64
00:02:21,840 --> 00:02:22,959
broadly

65
00:02:22,959 --> 00:02:24,480
we're going to walk through the aims and

66
00:02:24,480 --> 00:02:26,720
claims of the paper the abstract in the

67
00:02:26,720 --> 00:02:28,080
roadmap

68
00:02:28,080 --> 00:02:29,920
covering a few big questions and then

69
00:02:29,920 --> 00:02:31,519
we're going to go through all the

70
00:02:31,519 --> 00:02:32,239
figures

71
00:02:32,239 --> 00:02:34,080
and some of the key formalisms of the

72
00:02:34,080 --> 00:02:36,879
paper so that whether you read the paper

73
00:02:36,879 --> 00:02:37,519
or not

74
00:02:37,519 --> 00:02:39,920
you'll hopefully be in a good spot to

75
00:02:39,920 --> 00:02:41,760
ask questions and learn more

76
00:02:41,760 --> 00:02:44,400
and of course in the dot one and dot two

77
00:02:44,400 --> 00:02:46,000
in the coming weeks we'll be discussing

78
00:02:46,000 --> 00:02:47,200
this same paper

79
00:02:47,200 --> 00:02:49,360
so save and submit your questions and

80
00:02:49,360 --> 00:02:50,720
let us know if you'd like to

81
00:02:50,720 --> 00:02:54,800
participate or contribute in any way

82
00:02:55,120 --> 00:02:58,480
here we are on the paper itself which

83
00:02:58,480 --> 00:03:01,680
has a screenshot of the cover on this

84
00:03:01,680 --> 00:03:05,360
slide i'll uh read the aims and claims

85
00:03:05,360 --> 00:03:06,400
and then blue you can

86
00:03:06,400 --> 00:03:08,000
give a first thought on what you thought

87
00:03:08,000 --> 00:03:09,840
were kind of cool pieces about

88
00:03:09,840 --> 00:03:13,120
what they aimed for or claimed in this

89
00:03:13,120 --> 00:03:13,840
paper

90
00:03:13,840 --> 00:03:15,920
we provided an empirical comparison

91
00:03:15,920 --> 00:03:17,760
between active inference

92
00:03:17,760 --> 00:03:19,440
a bayesian information theoretic

93
00:03:19,440 --> 00:03:21,599
framework and two state-of-the-art

94
00:03:21,599 --> 00:03:23,440
machine learning algorithms

95
00:03:23,440 --> 00:03:26,720
bayesian upper confidence bound ucb and

96
00:03:26,720 --> 00:03:28,799
optimistic thompson sampling in

97
00:03:28,799 --> 00:03:30,799
stationary and non-stationary

98
00:03:30,799 --> 00:03:33,840
stochastic multi-armed bandits we

99
00:03:33,840 --> 00:03:35,519
introduced an approximate active

100
00:03:35,519 --> 00:03:36,720
inference algorithm

101
00:03:36,720 --> 00:03:38,799
for which our checks on the stationary

102
00:03:38,799 --> 00:03:40,080
bandit problem

103
00:03:40,080 --> 00:03:41,760
showed that its performance closely

104
00:03:41,760 --> 00:03:43,280
follows that of the exact

105
00:03:43,280 --> 00:03:46,720
version and hence we derived an

106
00:03:46,720 --> 00:03:48,319
active inference algorithm that is

107
00:03:48,319 --> 00:03:50,319
efficient and easily scalable

108
00:03:50,319 --> 00:03:53,200
to high dimensional problems so what was

109
00:03:53,200 --> 00:03:55,439
cool about that or what made you excited

110
00:03:55,439 --> 00:03:55,920
to

111
00:03:55,920 --> 00:03:59,680
make this dot zero this paper was really

112
00:03:59,680 --> 00:04:00,239
awesome

113
00:04:00,239 --> 00:04:03,680
it um it shows how active inference can

114
00:04:03,680 --> 00:04:04,319
be used

115
00:04:04,319 --> 00:04:06,560
to solve problems that are sometimes

116
00:04:06,560 --> 00:04:07,760
computationally

117
00:04:07,760 --> 00:04:10,640
intractable and really pretty difficult

118
00:04:10,640 --> 00:04:12,159
so i think that the authors did a great

119
00:04:12,159 --> 00:04:13,280
job of

120
00:04:13,280 --> 00:04:15,519
deriving this active inference algorithm

121
00:04:15,519 --> 00:04:17,199
and proving that it's

122
00:04:17,199 --> 00:04:21,120
useful agreed and they did an awesome

123
00:04:21,120 --> 00:04:24,160
job bringing it analytically like with

124
00:04:24,160 --> 00:04:25,919
equations

125
00:04:25,919 --> 00:04:28,320
in line or at least being juxtaposed to

126
00:04:28,320 --> 00:04:29,919
other approaches in machine learning

127
00:04:29,919 --> 00:04:32,240
rather than appealing to a qualitative

128
00:04:32,240 --> 00:04:33,440
body of theory

129
00:04:33,440 --> 00:04:35,520
which is also great this is definitely

130
00:04:35,520 --> 00:04:36,880
one where the

131
00:04:36,880 --> 00:04:40,000
claims are specific and exact and that's

132
00:04:40,000 --> 00:04:40,720
what we'll be

133
00:04:40,720 --> 00:04:43,840
following up on

134
00:04:44,479 --> 00:04:46,400
so the abstract i'll read the first half

135
00:04:46,400 --> 00:04:48,960
and then you can go for the second half

136
00:04:48,960 --> 00:04:51,280
a key feature of sequential decision

137
00:04:51,280 --> 00:04:53,120
making under uncertainty

138
00:04:53,120 --> 00:04:56,400
is a need to balance between exploiting

139
00:04:56,400 --> 00:04:58,000
choosing the best action according to

140
00:04:58,000 --> 00:05:00,080
the current knowledge and exploring

141
00:05:00,080 --> 00:05:02,320
obtaining information about values of

142
00:05:02,320 --> 00:05:03,680
other actions

143
00:05:03,680 --> 00:05:05,759
the multi-armed bandit problem a

144
00:05:05,759 --> 00:05:07,600
classical task that captures this

145
00:05:07,600 --> 00:05:08,560
trade-off

146
00:05:08,560 --> 00:05:10,560
served as a vehicle in machine learning

147
00:05:10,560 --> 00:05:12,800
for developing bandit algorithms that

148
00:05:12,800 --> 00:05:14,560
proved to be useful in numerous

149
00:05:14,560 --> 00:05:17,440
industrial applications the active

150
00:05:17,440 --> 00:05:18,800
inference framework

151
00:05:18,800 --> 00:05:20,320
in approach to sequential decision

152
00:05:20,320 --> 00:05:22,320
making recently developed in

153
00:05:22,320 --> 00:05:24,000
neuroscience for understanding

154
00:05:24,000 --> 00:05:26,080
human and animal behavior is

155
00:05:26,080 --> 00:05:27,840
distinguished by its sophisticated

156
00:05:27,840 --> 00:05:28,720
strategy

157
00:05:28,720 --> 00:05:30,479
for resolving the exploration

158
00:05:30,479 --> 00:05:32,479
exploitation trade-off

159
00:05:32,479 --> 00:05:34,400
this makes active inference an exciting

160
00:05:34,400 --> 00:05:36,720
alternative to already established

161
00:05:36,720 --> 00:05:39,360
bandit algorithms so it's kind of

162
00:05:39,360 --> 00:05:39,919
awesome

163
00:05:39,919 --> 00:05:42,160
first few words it's about sequential

164
00:05:42,160 --> 00:05:43,039
decision making

165
00:05:43,039 --> 00:05:46,160
under uncertainty and the bandit task

166
00:05:46,160 --> 00:05:46,560
was

167
00:05:46,560 --> 00:05:49,199
made to explore that machine learning

168
00:05:49,199 --> 00:05:50,000
has already

169
00:05:50,000 --> 00:05:52,160
gotten so much value out of pursuing

170
00:05:52,160 --> 00:05:54,000
algorithms for approaching

171
00:05:54,000 --> 00:05:56,960
this multi-arm bandit problem and then

172
00:05:56,960 --> 00:05:57,440
enter

173
00:05:57,440 --> 00:06:00,240
active inference which was developed to

174
00:06:00,240 --> 00:06:01,199
uh initially

175
00:06:01,199 --> 00:06:03,600
cover human and animal cases of behavior

176
00:06:03,600 --> 00:06:05,680
but as we're seeing it also it goes

177
00:06:05,680 --> 00:06:06,080
beyond

178
00:06:06,080 --> 00:06:08,960
that and so that motivates studying

179
00:06:08,960 --> 00:06:10,319
active inference

180
00:06:10,319 --> 00:06:12,080
in the context of multi-arm bandit

181
00:06:12,080 --> 00:06:13,840
problems so go for the second

182
00:06:13,840 --> 00:06:17,440
part here we derive an efficient and

183
00:06:17,440 --> 00:06:19,520
scalable approximate active inference

184
00:06:19,520 --> 00:06:20,080
algorithm

185
00:06:20,080 --> 00:06:22,080
and compare it to two state-of-the-art

186
00:06:22,080 --> 00:06:23,600
banded algorithms

187
00:06:23,600 --> 00:06:25,600
bayesian upper confidence bound and

188
00:06:25,600 --> 00:06:27,680
optimistic thompson sampling

189
00:06:27,680 --> 00:06:29,759
this comparison is done on two types of

190
00:06:29,759 --> 00:06:30,880
bandit problems

191
00:06:30,880 --> 00:06:32,880
a stationary and a dynamic switching

192
00:06:32,880 --> 00:06:35,440
bandit our empirical evaluation

193
00:06:35,440 --> 00:06:36,800
shows that the active inference

194
00:06:36,800 --> 00:06:38,639
algorithm does not produce efficient

195
00:06:38,639 --> 00:06:39,759
long-term behavior

196
00:06:39,759 --> 00:06:42,160
in stationary bandits however in the

197
00:06:42,160 --> 00:06:43,759
more challenging switching bandit

198
00:06:43,759 --> 00:06:44,400
algorithm

199
00:06:44,400 --> 00:06:46,160
or switching bandit active inference

200
00:06:46,160 --> 00:06:47,759
performs substantially better

201
00:06:47,759 --> 00:06:49,599
than the two state-of-the-art banded

202
00:06:49,599 --> 00:06:50,880
algorithms

203
00:06:50,880 --> 00:06:52,880
the results open exciting venues for

204
00:06:52,880 --> 00:06:54,720
further research in theoretical and

205
00:06:54,720 --> 00:06:55,919
applied machine learning

206
00:06:55,919 --> 00:06:58,240
as well as lend additional credibility

207
00:06:58,240 --> 00:06:59,919
to active inference as a general

208
00:06:59,919 --> 00:07:01,199
framework for studying

209
00:07:01,199 --> 00:07:03,759
human and animal behavior so this is

210
00:07:03,759 --> 00:07:05,120
really nice because it's

211
00:07:05,120 --> 00:07:07,840
it it takes like active inference as an

212
00:07:07,840 --> 00:07:09,360
approximate way that

213
00:07:09,360 --> 00:07:11,599
humans deduce reason balance this

214
00:07:11,599 --> 00:07:13,520
exploration exploitation

215
00:07:13,520 --> 00:07:16,960
paradigm and it programs that into the

216
00:07:16,960 --> 00:07:17,840
framework

217
00:07:17,840 --> 00:07:19,759
so it just shows that you know it's it's

218
00:07:19,759 --> 00:07:21,840
getting closer approximating human

219
00:07:21,840 --> 00:07:22,560
thinking

220
00:07:22,560 --> 00:07:25,919
in a machine it more and more i think

221
00:07:25,919 --> 00:07:28,479
yes and there's sort of a pair of pairs

222
00:07:28,479 --> 00:07:29,759
that we're going to return to

223
00:07:29,759 --> 00:07:31,759
a bunch of times in this discussion the

224
00:07:31,759 --> 00:07:33,840
first is the two types of problems

225
00:07:33,840 --> 00:07:36,240
the stationary and the dynamic bandit

226
00:07:36,240 --> 00:07:36,960
which

227
00:07:36,960 --> 00:07:38,400
one of them is static one of them is

228
00:07:38,400 --> 00:07:39,919
changing we're going to talk more about

229
00:07:39,919 --> 00:07:40,560
that

230
00:07:40,560 --> 00:07:42,639
and the other pair that it comes up

231
00:07:42,639 --> 00:07:45,039
often is the bayesian upper confidence

232
00:07:45,039 --> 00:07:47,680
bound and optimistic thompson sampling

233
00:07:47,680 --> 00:07:49,440
we're also going to come back to that so

234
00:07:49,440 --> 00:07:51,520
they're kind of doing a pair of pairs

235
00:07:51,520 --> 00:07:54,319
approach and then the results are just

236
00:07:54,319 --> 00:07:55,680
really fascinating and we're going to

237
00:07:55,680 --> 00:07:56,720
unpack that

238
00:07:56,720 --> 00:07:59,599
that actually in the simpler case we see

239
00:07:59,599 --> 00:08:01,440
that the active inference algorithm

240
00:08:01,440 --> 00:08:04,160
doesn't do perfectly but in this more

241
00:08:04,160 --> 00:08:05,360
challenging dynamic

242
00:08:05,360 --> 00:08:07,039
case it does better than state of the

243
00:08:07,039 --> 00:08:08,560
art so what do you call something that

244
00:08:08,560 --> 00:08:11,520
goes beyond state of the art

245
00:08:11,520 --> 00:08:14,720
here's the roadmap for the paper and of

246
00:08:14,720 --> 00:08:15,360
course

247
00:08:15,360 --> 00:08:18,160
as with these.0 videos if you're curious

248
00:08:18,160 --> 00:08:18,800
about

249
00:08:18,800 --> 00:08:20,400
how it was specifically stated in the

250
00:08:20,400 --> 00:08:22,319
paper or you want to dive in

251
00:08:22,319 --> 00:08:24,400
for more and you want to see how they

252
00:08:24,400 --> 00:08:26,400
cited or how they built up a claim you

253
00:08:26,400 --> 00:08:27,919
should go to the paper itself

254
00:08:27,919 --> 00:08:30,400
this is just the road map for it they

255
00:08:30,400 --> 00:08:31,919
start with an introduction

256
00:08:31,919 --> 00:08:34,719
of the multi-armed bandit and cover the

257
00:08:34,719 --> 00:08:36,559
two kinds of bandits that they explore

258
00:08:36,559 --> 00:08:38,240
which is the stationary and switching

259
00:08:38,240 --> 00:08:40,320
versions and talk about how you evaluate

260
00:08:40,320 --> 00:08:40,958
performance

261
00:08:40,958 --> 00:08:43,839
in different versions they then

262
00:08:43,839 --> 00:08:45,600
introduce the algorithms that they're

263
00:08:45,600 --> 00:08:46,320
going to

264
00:08:46,320 --> 00:08:49,600
utilize and talk about this variational

265
00:08:49,600 --> 00:08:51,600
smile approach which we'll cover just a

266
00:08:51,600 --> 00:08:53,120
little bit of but

267
00:08:53,120 --> 00:08:54,640
look forward to hearing from the authors

268
00:08:54,640 --> 00:08:57,200
about what it does

269
00:08:57,200 --> 00:09:00,320
then there's results for the pair of

270
00:09:00,320 --> 00:09:02,000
problems that they explore stationary

271
00:09:02,000 --> 00:09:03,440
and switching bandits

272
00:09:03,440 --> 00:09:05,279
and then have five figures which we'll

273
00:09:05,279 --> 00:09:08,000
go through showing some results and

274
00:09:08,000 --> 00:09:08,720
hopefully

275
00:09:08,720 --> 00:09:11,839
even if the algorithms or the formalisms

276
00:09:11,839 --> 00:09:13,680
were a little bit hard to follow the

277
00:09:13,680 --> 00:09:15,040
figures are really clear

278
00:09:15,040 --> 00:09:17,279
and they show how the algorithms perform

279
00:09:17,279 --> 00:09:19,360
through time and close with a discussion

280
00:09:19,360 --> 00:09:21,440
that hits on some awesome general points

281
00:09:21,440 --> 00:09:22,959
for the active inference and machine

282
00:09:22,959 --> 00:09:25,360
learning communities

283
00:09:25,360 --> 00:09:26,880
here are the keywords that were provided

284
00:09:26,880 --> 00:09:29,120
with the paper so as usual we're going

285
00:09:29,120 --> 00:09:29,440
to

286
00:09:29,440 --> 00:09:31,920
use these keywords as jumping off but

287
00:09:31,920 --> 00:09:32,560
also

288
00:09:32,560 --> 00:09:35,279
jumping in points for those who might be

289
00:09:35,279 --> 00:09:37,519
familiar with sequential decision making

290
00:09:37,519 --> 00:09:40,720
aka life but not be familiar with

291
00:09:40,720 --> 00:09:42,720
bayesian statistics and we're going to

292
00:09:42,720 --> 00:09:44,800
start from sequential decision making

293
00:09:44,800 --> 00:09:45,360
and then

294
00:09:45,360 --> 00:09:47,360
go on to talk more broadly about

295
00:09:47,360 --> 00:09:48,640
bayesian inference

296
00:09:48,640 --> 00:09:51,120
multi-armed bandits so it's totally fine

297
00:09:51,120 --> 00:09:52,800
if you've never even heard of the bandit

298
00:09:52,800 --> 00:09:53,680
problem

299
00:09:53,680 --> 00:09:55,200
then we're going to talk about those two

300
00:09:55,200 --> 00:09:56,800
different algorithms that they're going

301
00:09:56,800 --> 00:09:57,760
to be exploring

302
00:09:57,760 --> 00:09:59,440
upper confidence bound and thompson

303
00:09:59,440 --> 00:10:01,440
sampling and then

304
00:10:01,440 --> 00:10:02,959
see how they talked about active

305
00:10:02,959 --> 00:10:04,240
inference which is going to be a really

306
00:10:04,240 --> 00:10:05,200
distinct way

307
00:10:05,200 --> 00:10:07,600
relative than a lot of approaches we've

308
00:10:07,600 --> 00:10:10,160
seen previously

309
00:10:10,160 --> 00:10:12,320
so let's go to sequential decision

310
00:10:12,320 --> 00:10:13,200
making

311
00:10:13,200 --> 00:10:16,399
blue what would you say about this so

312
00:10:16,399 --> 00:10:18,240
sequential decision making is dependent

313
00:10:18,240 --> 00:10:18,560
on

314
00:10:18,560 --> 00:10:21,360
time t right so one time step you have a

315
00:10:21,360 --> 00:10:22,640
state of the system

316
00:10:22,640 --> 00:10:25,360
you perform an action and that alters

317
00:10:25,360 --> 00:10:26,160
the system

318
00:10:26,160 --> 00:10:28,000
right so that at the second time step

319
00:10:28,000 --> 00:10:29,839
the system is now in a new state and

320
00:10:29,839 --> 00:10:31,440
then you have another decision to make

321
00:10:31,440 --> 00:10:32,560
about what you're going to do

322
00:10:32,560 --> 00:10:34,480
with the system so this is like driving

323
00:10:34,480 --> 00:10:35,680
a vehicle

324
00:10:35,680 --> 00:10:37,440
managing a stock portfolio playing a

325
00:10:37,440 --> 00:10:39,519
game of chess sequential policy

326
00:10:39,519 --> 00:10:40,480
decisions

327
00:10:40,480 --> 00:10:43,040
and so like to kind of juxtapose what is

328
00:10:43,040 --> 00:10:44,320
a not sequential

329
00:10:44,320 --> 00:10:46,320
problem so that be something like image

330
00:10:46,320 --> 00:10:48,079
classification is not sequential like

331
00:10:48,079 --> 00:10:49,680
you can just classify all the images

332
00:10:49,680 --> 00:10:51,040
parallel or at once

333
00:10:51,040 --> 00:10:52,480
at the state of the system doesn't

334
00:10:52,480 --> 00:10:54,480
depend on how you classified the

335
00:10:54,480 --> 00:10:56,320
previous image

336
00:10:56,320 --> 00:10:58,560
awesome so definitely all kinds of

337
00:10:58,560 --> 00:11:00,959
organismal behavior in decision making

338
00:11:00,959 --> 00:11:03,839
that happens in time so think decisions

339
00:11:03,839 --> 00:11:04,480
you're thinking

340
00:11:04,480 --> 00:11:07,040
usually sequential decision making

341
00:11:07,040 --> 00:11:08,720
especially when the decision

342
00:11:08,720 --> 00:11:11,200
influences the future those are all the

343
00:11:11,200 --> 00:11:12,640
kinds of problems

344
00:11:12,640 --> 00:11:14,560
that we're going to be talking about but

345
00:11:14,560 --> 00:11:17,040
also to even make it broader than that

346
00:11:17,040 --> 00:11:19,120
some non-sequential problems can be

347
00:11:19,120 --> 00:11:20,480
framed sequentially

348
00:11:20,480 --> 00:11:21,760
so let's just say that we had that

349
00:11:21,760 --> 00:11:24,000
classification problem where it's one

350
00:11:24,000 --> 00:11:26,720
blob one big data set and so there's no

351
00:11:26,720 --> 00:11:28,079
temporal sequence

352
00:11:28,079 --> 00:11:30,240
we might want an algorithm that treats

353
00:11:30,240 --> 00:11:32,240
it as if it were sequential

354
00:11:32,240 --> 00:11:33,760
so that we could just start reading in

355
00:11:33,760 --> 00:11:35,440
examples and then after 10

356
00:11:35,440 --> 00:11:38,720
okay got it so then by looking at it as

357
00:11:38,720 --> 00:11:40,399
if it were a sequential problem

358
00:11:40,399 --> 00:11:42,959
and we were solving things through time

359
00:11:42,959 --> 00:11:44,880
sometimes we can approach non-sequential

360
00:11:44,880 --> 00:11:46,399
problems that way because in the

361
00:11:46,399 --> 00:11:48,640
computer it is going to be sequential on

362
00:11:48,640 --> 00:11:49,600
the processor

363
00:11:49,600 --> 00:11:51,680
so it's for things that actually are

364
00:11:51,680 --> 00:11:52,959
sequential in the world

365
00:11:52,959 --> 00:11:54,880
or they're for things that we want to

366
00:11:54,880 --> 00:11:57,600
act as if they're sequential

367
00:11:57,600 --> 00:12:00,240
one big tension that comes up with all

368
00:12:00,240 --> 00:12:01,760
kinds of modeling

369
00:12:01,760 --> 00:12:05,040
is the explore exploit dilemma slash

370
00:12:05,040 --> 00:12:08,160
trade-off and this is an awesome 2015

371
00:12:08,160 --> 00:12:08,880
paper of

372
00:12:08,880 --> 00:12:11,440
hills at all exploration versus

373
00:12:11,440 --> 00:12:12,639
exploitation

374
00:12:12,639 --> 00:12:15,760
in space mind and society

375
00:12:15,760 --> 00:12:17,360
so it's pretty cool because it talks

376
00:12:17,360 --> 00:12:19,519
about multiple different domains

377
00:12:19,519 --> 00:12:21,760
here in the table 1 there's animal

378
00:12:21,760 --> 00:12:22,639
forging

379
00:12:22,639 --> 00:12:25,519
visual search information search memory

380
00:12:25,519 --> 00:12:26,399
search

381
00:12:26,399 --> 00:12:28,320
searching and problem solving and social

382
00:12:28,320 --> 00:12:29,839
group learning those are like

383
00:12:29,839 --> 00:12:33,120
all of our favorite topics blue and it

384
00:12:33,120 --> 00:12:34,880
gives just a little bit of a visual

385
00:12:34,880 --> 00:12:36,160
example how

386
00:12:36,160 --> 00:12:38,639
in different domains what does the

387
00:12:38,639 --> 00:12:40,240
archetype of exploration

388
00:12:40,240 --> 00:12:42,560
look like and what does the archetype of

389
00:12:42,560 --> 00:12:43,760
exploitation

390
00:12:43,760 --> 00:12:46,800
look like so some of these are

391
00:12:46,800 --> 00:12:49,600
very uh related to kind of a physical

392
00:12:49,600 --> 00:12:50,160
movement

393
00:12:50,160 --> 00:12:52,560
like patch forging exploration moving to

394
00:12:52,560 --> 00:12:53,600
a different tree

395
00:12:53,600 --> 00:12:56,000
exploitation staying on the same tree

396
00:12:56,000 --> 00:12:57,440
but also visual focus

397
00:12:57,440 --> 00:13:00,720
exploration scanning around exploring

398
00:13:00,720 --> 00:13:03,200
exploitation having a fixed gaze and

399
00:13:03,200 --> 00:13:04,160
staring at

400
00:13:04,160 --> 00:13:06,720
something and then also we can think

401
00:13:06,720 --> 00:13:07,519
about

402
00:13:07,519 --> 00:13:09,680
even in word association like memory

403
00:13:09,680 --> 00:13:10,560
exploration

404
00:13:10,560 --> 00:13:12,720
is doing big jumps from different

405
00:13:12,720 --> 00:13:13,920
semantic neighborhoods

406
00:13:13,920 --> 00:13:16,800
whereas exploitation might be like all

407
00:13:16,800 --> 00:13:18,720
the livestock animals or animals with

408
00:13:18,720 --> 00:13:20,000
the same letter there's different

409
00:13:20,000 --> 00:13:21,920
ways that you might exploit because

410
00:13:21,920 --> 00:13:23,519
there's different dimensions to the

411
00:13:23,519 --> 00:13:25,040
semantic landscape

412
00:13:25,040 --> 00:13:27,279
but there's still this trade-off between

413
00:13:27,279 --> 00:13:28,320
jumping far

414
00:13:28,320 --> 00:13:30,480
or staying relatively close in the

415
00:13:30,480 --> 00:13:31,360
neighborhood

416
00:13:31,360 --> 00:13:34,000
so it's a big trade-off it's studied in

417
00:13:34,000 --> 00:13:35,279
animal behavior

418
00:13:35,279 --> 00:13:36,800
it's studied in all kinds of

419
00:13:36,800 --> 00:13:39,120
decision-making tasks

420
00:13:39,120 --> 00:13:42,240
anything to add on that only that i

421
00:13:42,240 --> 00:13:44,160
got to interact a lot with peter todd

422
00:13:44,160 --> 00:13:45,760
last summer who's the second author on

423
00:13:45,760 --> 00:13:46,639
this paper and

424
00:13:46,639 --> 00:13:50,079
it was so fun cool and also

425
00:13:50,079 --> 00:13:52,240
ian cuisine has done awesome collective

426
00:13:52,240 --> 00:13:53,600
behavior work

427
00:13:53,600 --> 00:13:55,440
so it's exciting to think about how

428
00:13:55,440 --> 00:13:56,880
these algorithms

429
00:13:56,880 --> 00:13:59,920
are able to apply to individual agents

430
00:13:59,920 --> 00:14:00,959
but also maybe

431
00:14:00,959 --> 00:14:02,720
groups and that's hinted at multiple

432
00:14:02,720 --> 00:14:05,040
times here like role assignment and

433
00:14:05,040 --> 00:14:08,160
social connectivity

434
00:14:08,560 --> 00:14:11,040
as we heard about in the abstract of the

435
00:14:11,040 --> 00:14:13,120
paper that we're discussing today

436
00:14:13,120 --> 00:14:15,600
they went right from sequential decision

437
00:14:15,600 --> 00:14:17,519
making under uncertainty

438
00:14:17,519 --> 00:14:20,639
to multi-armed bandit is

439
00:14:20,639 --> 00:14:22,800
a way that we can study that and from

440
00:14:22,800 --> 00:14:23,680
the paper

441
00:14:23,680 --> 00:14:25,839
this is where we're introducing that

442
00:14:25,839 --> 00:14:27,760
that uh double problem the two problems

443
00:14:27,760 --> 00:14:28,720
are solving

444
00:14:28,720 --> 00:14:30,800
we consider two types of bandit problems

445
00:14:30,800 --> 00:14:33,279
in our empirical evaluation

446
00:14:33,279 --> 00:14:35,360
a stationary bandit as a classical

447
00:14:35,360 --> 00:14:36,880
machine learning problem and a switching

448
00:14:36,880 --> 00:14:37,519
bandit

449
00:14:37,519 --> 00:14:39,600
commonly used in neuroscience so we're

450
00:14:39,600 --> 00:14:41,760
gonna go into more detail about

451
00:14:41,760 --> 00:14:43,199
how those are technically defined but

452
00:14:43,199 --> 00:14:45,440
let's just start with what are people

453
00:14:45,440 --> 00:14:47,519
getting at with multi-arm bandit

454
00:14:47,519 --> 00:14:50,000
why is it something that so many people

455
00:14:50,000 --> 00:14:51,839
have addressed

456
00:14:51,839 --> 00:14:53,519
this will make the presented results

457
00:14:53,519 --> 00:14:54,880
that they're doing here

458
00:14:54,880 --> 00:14:56,399
directly relevant not only for the

459
00:14:56,399 --> 00:14:58,000
machine learning community

460
00:14:58,000 --> 00:14:59,600
but also for learning and decision

461
00:14:59,600 --> 00:15:01,600
making studies in neuroscience which are

462
00:15:01,600 --> 00:15:02,880
often utilizing

463
00:15:02,880 --> 00:15:04,480
the active inference framework for a

464
00:15:04,480 --> 00:15:06,720
wide range of research questions

465
00:15:06,720 --> 00:15:08,560
so there's all these different areas

466
00:15:08,560 --> 00:15:10,399
here's a multi-arm bandit deciding which

467
00:15:10,399 --> 00:15:11,600
area to study

468
00:15:11,600 --> 00:15:14,160
computer science machine learning

469
00:15:14,160 --> 00:15:15,120
economics

470
00:15:15,120 --> 00:15:18,160
neuroscience these are all areas

471
00:15:18,160 --> 00:15:20,079
that the multi-armed bandit is like a

472
00:15:20,079 --> 00:15:21,760
bridge amongst

473
00:15:21,760 --> 00:15:24,160
so now we're sitting active inference

474
00:15:24,160 --> 00:15:26,320
right at that nexus

475
00:15:26,320 --> 00:15:27,760
we've talked a lot about connecting

476
00:15:27,760 --> 00:15:30,160
computer science to behavior

477
00:15:30,160 --> 00:15:32,639
or connecting neuroscience to economics

478
00:15:32,639 --> 00:15:33,759
what if we could all

479
00:15:33,759 --> 00:15:36,639
meet at a common nexus what if that were

480
00:15:36,639 --> 00:15:37,839
active inference

481
00:15:37,839 --> 00:15:39,519
these are the kind of fun things to talk

482
00:15:39,519 --> 00:15:40,880
about

483
00:15:40,880 --> 00:15:43,920
but it's not just a nexus of conceptual

484
00:15:43,920 --> 00:15:46,160
connection the multi-iron bandit and

485
00:15:46,160 --> 00:15:47,279
active inference

486
00:15:47,279 --> 00:15:48,959
there's a lot of really specific use

487
00:15:48,959 --> 00:15:51,040
cases and a lot of the algorithms that

488
00:15:51,040 --> 00:15:54,639
power our experience online are actually

489
00:15:54,639 --> 00:15:55,440
trained

490
00:15:55,440 --> 00:15:58,959
and fit with multi-arm bandit problems

491
00:15:58,959 --> 00:16:01,279
so here's a few fun examples that we

492
00:16:01,279 --> 00:16:02,000
came across

493
00:16:02,000 --> 00:16:05,680
while researching here on the top left

494
00:16:05,680 --> 00:16:09,120
is showing how you have a

495
00:16:09,120 --> 00:16:11,360
multi-arm bandit playing a role in music

496
00:16:11,360 --> 00:16:12,720
recommendations

497
00:16:12,720 --> 00:16:15,519
so it's like the the arm that's which

498
00:16:15,519 --> 00:16:16,560
arm is being chosen

499
00:16:16,560 --> 00:16:18,399
that's the song so it's thinking about

500
00:16:18,399 --> 00:16:20,320
it from the point of view of the music

501
00:16:20,320 --> 00:16:21,120
platform

502
00:16:21,120 --> 00:16:23,920
it's sending a song and then the target

503
00:16:23,920 --> 00:16:24,560
user

504
00:16:24,560 --> 00:16:26,959
which is the bandit machine here gives a

505
00:16:26,959 --> 00:16:28,880
payoff back to the algorithm

506
00:16:28,880 --> 00:16:31,839
thumbs up or thumbs down and then the

507
00:16:31,839 --> 00:16:34,079
algorithm sends you another song

508
00:16:34,079 --> 00:16:35,839
and then a payoff is given so it's

509
00:16:35,839 --> 00:16:37,759
sequential decision making

510
00:16:37,759 --> 00:16:40,320
and it's that same relationship that's

511
00:16:40,320 --> 00:16:40,880
shown

512
00:16:40,880 --> 00:16:43,040
graphically the goal is to maximize the

513
00:16:43,040 --> 00:16:44,800
sum of the ratings versus maximizing the

514
00:16:44,800 --> 00:16:46,560
sum of the payoffs in an abstract

515
00:16:46,560 --> 00:16:47,680
problem

516
00:16:47,680 --> 00:16:50,560
here's an example of website a b testing

517
00:16:50,560 --> 00:16:52,560
testing versions of a website

518
00:16:52,560 --> 00:16:54,880
so in the top there's four versions of

519
00:16:54,880 --> 00:16:56,000
the website

520
00:16:56,000 --> 00:16:58,320
and then they're each being allocated to

521
00:16:58,320 --> 00:16:59,839
a quarter of the users

522
00:16:59,839 --> 00:17:01,920
and then each of the users are staying

523
00:17:01,920 --> 00:17:03,440
on the site for a certain amount of time

524
00:17:03,440 --> 00:17:05,679
or staying for different amount of times

525
00:17:05,679 --> 00:17:07,439
in the standard a b testing for the

526
00:17:07,439 --> 00:17:09,520
whole duration of your test

527
00:17:09,520 --> 00:17:13,119
you keep that one-fourth for each site

528
00:17:13,119 --> 00:17:15,280
but in the multi-armed bandit you start

529
00:17:15,280 --> 00:17:16,319
with a quarter

530
00:17:16,319 --> 00:17:18,000
but then very rapidly we see that this

531
00:17:18,000 --> 00:17:20,720
blue one starts performing better

532
00:17:20,720 --> 00:17:22,720
and then we keep exploring and having

533
00:17:22,720 --> 00:17:25,199
some time allocated to these other

534
00:17:25,199 --> 00:17:27,839
colors but we see that the blue pretty

535
00:17:27,839 --> 00:17:28,559
steadily

536
00:17:28,559 --> 00:17:31,440
dominates and then that black line ends

537
00:17:31,440 --> 00:17:32,000
up staying

538
00:17:32,000 --> 00:17:34,960
higher so overall a higher number at the

539
00:17:34,960 --> 00:17:36,960
end of the epoch so you're sort of

540
00:17:36,960 --> 00:17:39,280
earning while learning because you're

541
00:17:39,280 --> 00:17:40,240
able to

542
00:17:40,240 --> 00:17:43,840
be making exploration while you're also

543
00:17:43,840 --> 00:17:45,039
exploiting

544
00:17:45,039 --> 00:17:47,840
and there's papers specifically on that

545
00:17:47,840 --> 00:17:49,200
like bayesian bandits

546
00:17:49,200 --> 00:17:51,280
in the context of online personalized

547
00:17:51,280 --> 00:17:52,960
recommendations

548
00:17:52,960 --> 00:17:56,480
any thoughts on that blue

549
00:17:56,480 --> 00:17:57,919
cool so these are yeah these are

550
00:17:57,919 --> 00:18:00,000
algorithms that are in use

551
00:18:00,000 --> 00:18:02,640
every single day and they power a lot of

552
00:18:02,640 --> 00:18:03,520
our experience

553
00:18:03,520 --> 00:18:07,120
and a lot of decision making support

554
00:18:07,120 --> 00:18:09,200
here's where we bring in some of the

555
00:18:09,200 --> 00:18:11,440
bayesian algorithms

556
00:18:11,440 --> 00:18:12,880
they're going to be using these two

557
00:18:12,880 --> 00:18:15,039
types of bandit problems stationary and

558
00:18:15,039 --> 00:18:15,840
switching

559
00:18:15,840 --> 00:18:18,880
and here's the second pair of terms

560
00:18:18,880 --> 00:18:20,480
they're going to be empirically

561
00:18:20,480 --> 00:18:22,000
comparing active inference

562
00:18:22,000 --> 00:18:24,320
to two other state-of-the-art banded

563
00:18:24,320 --> 00:18:26,080
algorithms from machine learning

564
00:18:26,080 --> 00:18:27,679
so those who are familiar with machine

565
00:18:27,679 --> 00:18:29,760
learning will have seen these two

566
00:18:29,760 --> 00:18:31,280
algorithms a lot

567
00:18:31,280 --> 00:18:33,679
the first is an upper confidence bound

568
00:18:33,679 --> 00:18:34,480
algorithm

569
00:18:34,480 --> 00:18:37,760
ucb and the second is a variant of

570
00:18:37,760 --> 00:18:39,200
thomson sampling called

571
00:18:39,200 --> 00:18:42,080
optimistic thompson sampling and then

572
00:18:42,080 --> 00:18:43,039
they note here

573
00:18:43,039 --> 00:18:45,760
the two algorithms the ucb and thomson

574
00:18:45,760 --> 00:18:47,360
sampling

575
00:18:47,360 --> 00:18:48,880
reach state-of-the-art performance on

576
00:18:48,880 --> 00:18:51,360
various stationary bandit problems

577
00:18:51,360 --> 00:18:53,200
achieving regret which is the difference

578
00:18:53,200 --> 00:18:55,200
between actual and optimal performance

579
00:18:55,200 --> 00:18:56,480
will return to it soon

580
00:18:56,480 --> 00:18:58,240
that's close to the best possible

581
00:18:58,240 --> 00:18:59,919
logarithmic regret

582
00:18:59,919 --> 00:19:01,440
and in switching bandits learning is

583
00:19:01,440 --> 00:19:03,280
more complex but once this is accounted

584
00:19:03,280 --> 00:19:03,760
for

585
00:19:03,760 --> 00:19:05,200
both of the algorithms exhibit

586
00:19:05,200 --> 00:19:07,039
state-of-the-art performance

587
00:19:07,039 --> 00:19:10,080
so you can never play perfectly but

588
00:19:10,080 --> 00:19:12,000
you're approximating about as good as

589
00:19:12,000 --> 00:19:12,720
you could play

590
00:19:12,720 --> 00:19:15,280
with these algorithms what does that

591
00:19:15,280 --> 00:19:16,559
look like before we go

592
00:19:16,559 --> 00:19:18,880
into the technical details of what these

593
00:19:18,880 --> 00:19:20,880
algorithms are

594
00:19:20,880 --> 00:19:22,960
so on the top left we're going to be

595
00:19:22,960 --> 00:19:24,559
thinking about again whether it's

596
00:19:24,559 --> 00:19:26,559
deciding which version of a website

597
00:19:26,559 --> 00:19:28,480
we want to be presenting or which songs

598
00:19:28,480 --> 00:19:29,919
we want to be presenting

599
00:19:29,919 --> 00:19:32,559
or just keeping it kind of abstract on

600
00:19:32,559 --> 00:19:34,160
the top left we start

601
00:19:34,160 --> 00:19:36,559
with zero trials so this is before we

602
00:19:36,559 --> 00:19:38,000
have any information at

603
00:19:38,000 --> 00:19:41,120
all and then as trials

604
00:19:41,120 --> 00:19:44,160
occur and payoffs are observed and the

605
00:19:44,160 --> 00:19:44,720
trials

606
00:19:44,720 --> 00:19:48,640
count count after 28 trials

607
00:19:48,640 --> 00:19:50,799
we've reached a very markedly different

608
00:19:50,799 --> 00:19:52,799
set of distributions

609
00:19:52,799 --> 00:19:55,679
and so what this overall looks like is

610
00:19:55,679 --> 00:19:57,360
you show up at the casino and you don't

611
00:19:57,360 --> 00:19:58,559
know the payoff of

612
00:19:58,559 --> 00:20:01,039
any of the slot machines and then you're

613
00:20:01,039 --> 00:20:02,080
going to be

614
00:20:02,080 --> 00:20:04,880
choosing some policy some way of

615
00:20:04,880 --> 00:20:05,520
approaching

616
00:20:05,520 --> 00:20:07,840
those slot machines and switching

617
00:20:07,840 --> 00:20:10,080
between them as needed

618
00:20:10,080 --> 00:20:12,720
some approach that's going to hopefully

619
00:20:12,720 --> 00:20:13,600
result with you

620
00:20:13,600 --> 00:20:15,760
getting the most money and so in this

621
00:20:15,760 --> 00:20:17,600
case it's sort of like as you

622
00:20:17,600 --> 00:20:19,200
get more and more information you're

623
00:20:19,200 --> 00:20:21,120
fine-tuning your estimate

624
00:20:21,120 --> 00:20:23,360
of what the distributions look like like

625
00:20:23,360 --> 00:20:25,440
we can see how the red distribution gets

626
00:20:25,440 --> 00:20:27,760
tuned as it gets tried more and more

627
00:20:27,760 --> 00:20:29,200
and similarly for the other

628
00:20:29,200 --> 00:20:31,360
distributions so that's kind of

629
00:20:31,360 --> 00:20:34,320
what this algorithm does is it starts

630
00:20:34,320 --> 00:20:35,200
with

631
00:20:35,200 --> 00:20:37,760
no specific bias towards a given slot

632
00:20:37,760 --> 00:20:38,799
machine

633
00:20:38,799 --> 00:20:41,840
and then it tries to develop a strategy

634
00:20:41,840 --> 00:20:43,840
for staying with slot machines that are

635
00:20:43,840 --> 00:20:46,080
known about versus trying new ones or

636
00:20:46,080 --> 00:20:48,240
ones you haven't tried in a while

637
00:20:48,240 --> 00:20:49,679
in order to get the best possible

638
00:20:49,679 --> 00:20:51,520
outcome and as you can imagine

639
00:20:51,520 --> 00:20:53,840
if there's a static one then it's an

640
00:20:53,840 --> 00:20:55,039
easier problem

641
00:20:55,039 --> 00:20:57,360
if it's dynamic you have to keep the

642
00:20:57,360 --> 00:21:00,479
rate of change in mind

643
00:21:00,640 --> 00:21:03,440
any thoughts on that good good

644
00:21:03,440 --> 00:21:04,240
explanation

645
00:21:04,240 --> 00:21:07,520
okay so bernoulli bandits are

646
00:21:07,520 --> 00:21:11,120
the uh class of bandits that they're

647
00:21:11,120 --> 00:21:12,799
going to constrain themselves to

648
00:21:12,799 --> 00:21:15,280
so they say we're constraining ourselves

649
00:21:15,280 --> 00:21:17,440
to a well-studied version of bandits

650
00:21:17,440 --> 00:21:20,159
the so-called bernoulli bandits for

651
00:21:20,159 --> 00:21:21,520
bernoulli bandits

652
00:21:21,520 --> 00:21:23,600
choice outcomes are drawn from an

653
00:21:23,600 --> 00:21:26,240
arm-specific bernoulli distribution

654
00:21:26,240 --> 00:21:28,080
bernoulli bandits together with gaussian

655
00:21:28,080 --> 00:21:30,640
bandits are the most commonly studied

656
00:21:30,640 --> 00:21:32,960
variant of multiarm bandit both in

657
00:21:32,960 --> 00:21:35,120
theoretical and applied machine learning

658
00:21:35,120 --> 00:21:38,480
and experimental cognitive science so

659
00:21:38,480 --> 00:21:41,760
you can fit any kind of distribution

660
00:21:41,760 --> 00:21:44,720
for the rewards underlying each of these

661
00:21:44,720 --> 00:21:46,000
slot machines

662
00:21:46,000 --> 00:21:47,840
but it turns out that if you use the

663
00:21:47,840 --> 00:21:49,679
bernoulli distribution

664
00:21:49,679 --> 00:21:52,960
the math works out nicely which makes it

665
00:21:52,960 --> 00:21:55,200
easy to study that's why there's been a

666
00:21:55,200 --> 00:21:56,799
lot of study in the bernoulli and the

667
00:21:56,799 --> 00:21:57,679
gaussian

668
00:21:57,679 --> 00:22:00,400
so a gaussian distribution reward would

669
00:22:00,400 --> 00:22:01,200
be like okay

670
00:22:01,200 --> 00:22:04,000
you get five with a certain you know

671
00:22:04,000 --> 00:22:05,360
bell curve

672
00:22:05,360 --> 00:22:07,280
of how much you might win bernoulli is

673
00:22:07,280 --> 00:22:08,400
going to be a different shaped

674
00:22:08,400 --> 00:22:09,520
distribution

675
00:22:09,520 --> 00:22:10,960
but just the idea is that you're going

676
00:22:10,960 --> 00:22:12,640
to be learning the parameters

677
00:22:12,640 --> 00:22:16,159
that describe the reward returned by

678
00:22:16,159 --> 00:22:18,480
that arm

679
00:22:18,480 --> 00:22:20,960
so it's just the sub category or the

680
00:22:20,960 --> 00:22:23,039
function that underlies the reward

681
00:22:23,039 --> 00:22:24,320
payoff

682
00:22:24,320 --> 00:22:27,840
on these machines

683
00:22:29,600 --> 00:22:32,640
let's talk about the two algorithms that

684
00:22:32,640 --> 00:22:34,960
they're going to be discussing a lot

685
00:22:34,960 --> 00:22:37,280
and then also where these algorithms

686
00:22:37,280 --> 00:22:39,520
come into play in terms of strategy

687
00:22:39,520 --> 00:22:41,200
so again you're sitting there at the

688
00:22:41,200 --> 00:22:43,280
slot machine at the casino

689
00:22:43,280 --> 00:22:46,640
and how are you going to decide

690
00:22:46,640 --> 00:22:49,679
how to stay or leave a given machine

691
00:22:49,679 --> 00:22:51,360
stay in the casino though that's that's

692
00:22:51,360 --> 00:22:53,200
where you want to be

693
00:22:53,200 --> 00:22:57,280
this is from a nice blog post from 2019

694
00:22:57,280 --> 00:22:58,960
data scientists have developed several

695
00:22:58,960 --> 00:23:00,880
solutions to tackle this problem

696
00:23:00,880 --> 00:23:02,799
and the three most common algorithms are

697
00:23:02,799 --> 00:23:04,240
epsilon greedy

698
00:23:04,240 --> 00:23:06,000
then upper confidence bound and thompson

699
00:23:06,000 --> 00:23:08,000
sampling so epsilon greedy is

700
00:23:08,000 --> 00:23:11,600
not given in this paper because it's not

701
00:23:11,600 --> 00:23:13,360
the best performing algorithm but it's

702
00:23:13,360 --> 00:23:14,000
really

703
00:23:14,000 --> 00:23:17,039
a nice sort of starter algorithm it's

704
00:23:17,039 --> 00:23:18,799
the simplest algorithm to address the

705
00:23:18,799 --> 00:23:21,600
exploration exploitation trade-off

706
00:23:21,600 --> 00:23:23,840
basically during exploration or

707
00:23:23,840 --> 00:23:25,120
exploitation sorry

708
00:23:25,120 --> 00:23:26,880
the lever with the highest known payout

709
00:23:26,880 --> 00:23:28,320
is always pulled so

710
00:23:28,320 --> 00:23:30,640
whatever the running best estimate of

711
00:23:30,640 --> 00:23:32,960
the top performing slot machine is

712
00:23:32,960 --> 00:23:36,400
defaults there however from time to time

713
00:23:36,400 --> 00:23:38,880
some random fraction epsilon with some

714
00:23:38,880 --> 00:23:39,440
fraction

715
00:23:39,440 --> 00:23:40,880
five percent of the time or one percent

716
00:23:40,880 --> 00:23:42,480
of the time select

717
00:23:42,480 --> 00:23:45,039
another random arm to explore the other

718
00:23:45,039 --> 00:23:45,600
arms

719
00:23:45,600 --> 00:23:48,159
with an unknown payoff so you're

720
00:23:48,159 --> 00:23:49,440
sticking with the one that has the

721
00:23:49,440 --> 00:23:50,960
highest point estimate

722
00:23:50,960 --> 00:23:53,279
and then just some fraction of the time

723
00:23:53,279 --> 00:23:54,880
you flip to a different one just to

724
00:23:54,880 --> 00:23:55,919
check

725
00:23:55,919 --> 00:23:58,000
and then you update your estimate of how

726
00:23:58,000 --> 00:23:59,039
each of them are doing

727
00:23:59,039 --> 00:24:01,600
so that's one strategy now here's two

728
00:24:01,600 --> 00:24:02,559
strategies that

729
00:24:02,559 --> 00:24:04,320
do better than that those are the ones

730
00:24:04,320 --> 00:24:06,080
that we're going to be contrasting with

731
00:24:06,080 --> 00:24:08,000
active inference

732
00:24:08,000 --> 00:24:09,840
one of them is the upper confidence

733
00:24:09,840 --> 00:24:12,080
bound which is sometimes referred to as

734
00:24:12,080 --> 00:24:14,320
optimism in the face of uncertainty that

735
00:24:14,320 --> 00:24:16,080
sounds like active inference

736
00:24:16,080 --> 00:24:18,640
it assumes that the unknown mean payoffs

737
00:24:18,640 --> 00:24:21,279
of each arm will be as high as possible

738
00:24:21,279 --> 00:24:24,240
based upon historical data so we don't

739
00:24:24,240 --> 00:24:25,279
know the payoff

740
00:24:25,279 --> 00:24:28,000
of each arm but we want to assume given

741
00:24:28,000 --> 00:24:30,240
what we've already gotten from the data

742
00:24:30,240 --> 00:24:32,240
which is as far as we can speculate we

743
00:24:32,240 --> 00:24:33,600
want to assume that it's as good as it

744
00:24:33,600 --> 00:24:34,400
could have been

745
00:24:34,400 --> 00:24:36,240
which is why we see the upper confidence

746
00:24:36,240 --> 00:24:39,200
bound on the top of this distribution

747
00:24:39,200 --> 00:24:41,840
and then thompson sampling is

748
00:24:41,840 --> 00:24:43,600
fundamentally a bayesian optimization

749
00:24:43,600 --> 00:24:44,320
technique

750
00:24:44,320 --> 00:24:45,679
with a core principle known as

751
00:24:45,679 --> 00:24:47,679
probability matching that can be summed

752
00:24:47,679 --> 00:24:48,240
up as

753
00:24:48,240 --> 00:24:50,720
play an arm according to its probability

754
00:24:50,720 --> 00:24:51,440
of being

755
00:24:51,440 --> 00:24:54,480
the best arm so in contrast with epsilon

756
00:24:54,480 --> 00:24:56,159
which says just go with the one that you

757
00:24:56,159 --> 00:24:57,520
think is best

758
00:24:57,520 --> 00:24:59,440
and then ten percent of the time do

759
00:24:59,440 --> 00:25:00,559
something else

760
00:25:00,559 --> 00:25:02,080
thompson sampling is like you kind of

761
00:25:02,080 --> 00:25:03,919
have a pie chart with a relative

762
00:25:03,919 --> 00:25:05,120
performance

763
00:25:05,120 --> 00:25:08,000
of different arms and then you pick them

764
00:25:08,000 --> 00:25:09,200
based upon

765
00:25:09,200 --> 00:25:12,640
how big of a pie it is so you do rarely

766
00:25:12,640 --> 00:25:14,880
choose ones that don't have high payoffs

767
00:25:14,880 --> 00:25:16,400
just to sort of check in on them

768
00:25:16,400 --> 00:25:17,919
but then if you check back on one and it

769
00:25:17,919 --> 00:25:19,919
got does really well then that slice of

770
00:25:19,919 --> 00:25:21,679
the pie starts growing

771
00:25:21,679 --> 00:25:23,360
and then the whole point of training

772
00:25:23,360 --> 00:25:25,760
these algorithms is how fast should you

773
00:25:25,760 --> 00:25:26,080
be

774
00:25:26,080 --> 00:25:28,720
re-weighting in the dynamic case etc so

775
00:25:28,720 --> 00:25:29,279
just

776
00:25:29,279 --> 00:25:30,640
laying it out like that is not the

777
00:25:30,640 --> 00:25:32,960
solution but this is getting at a few

778
00:25:32,960 --> 00:25:33,679
ways

779
00:25:33,679 --> 00:25:35,279
whether optimism in the face of

780
00:25:35,279 --> 00:25:37,919
uncertainty or this sort of conservative

781
00:25:37,919 --> 00:25:39,600
probability matching

782
00:25:39,600 --> 00:25:41,600
these are two ways that as we've seen

783
00:25:41,600 --> 00:25:43,039
are state-of-the-art because they

784
00:25:43,039 --> 00:25:45,200
basically perform as well as possible

785
00:25:45,200 --> 00:25:47,520
and so we're locking it down with upper

786
00:25:47,520 --> 00:25:50,320
confidence and a lower confidence bound

787
00:25:50,320 --> 00:25:53,520
it's a pretty nice choice of algorithms

788
00:25:53,520 --> 00:25:55,279
they're brought up as an instructional

789
00:25:55,279 --> 00:25:57,360
pair all over the machine learning

790
00:25:57,360 --> 00:26:00,240
educational space so nice choice by the

791
00:26:00,240 --> 00:26:02,159
authors to juxtapose it so

792
00:26:02,159 --> 00:26:05,679
clearly to active inference

793
00:26:05,679 --> 00:26:08,320
so is thompson sampling then pessimism

794
00:26:08,320 --> 00:26:11,360
in the face of uncertainty

795
00:26:11,360 --> 00:26:13,520
nice things couldn't be better than they

796
00:26:13,520 --> 00:26:16,080
have been in the past

797
00:26:16,080 --> 00:26:18,240
let's go a little bit more into detail

798
00:26:18,240 --> 00:26:19,679
into the two

799
00:26:19,679 --> 00:26:22,559
different uh algorithms and then talk

800
00:26:22,559 --> 00:26:24,400
about regret

801
00:26:24,400 --> 00:26:27,600
so here's from another blog post lillian

802
00:26:27,600 --> 00:26:29,520
wang's blog

803
00:26:29,520 --> 00:26:32,559
talking about bandit strategies so

804
00:26:32,559 --> 00:26:35,360
again it's about explore and exploit

805
00:26:35,360 --> 00:26:35,919
even though

806
00:26:35,919 --> 00:26:38,000
active inference is going to help us

807
00:26:38,000 --> 00:26:39,120
reconceptualize

808
00:26:39,120 --> 00:26:40,799
of explore exploit which we can maybe

809
00:26:40,799 --> 00:26:42,320
get to at the end

810
00:26:42,320 --> 00:26:44,000
but we don't want to be exploring

811
00:26:44,000 --> 00:26:45,919
inefficiently because

812
00:26:45,919 --> 00:26:48,240
we're kind of spending our time playing

813
00:26:48,240 --> 00:26:49,440
on losing machines

814
00:26:49,440 --> 00:26:51,039
while we know that there's a better way

815
00:26:51,039 --> 00:26:53,520
to play so to avoid such inefficient

816
00:26:53,520 --> 00:26:54,240
exploration

817
00:26:54,240 --> 00:26:57,600
one approach is to do epsilon sampling

818
00:26:57,600 --> 00:27:00,159
and then in the case of a static set of

819
00:27:00,159 --> 00:27:00,880
payoffs

820
00:27:00,880 --> 00:27:03,039
you decrease that parameter epsilon in

821
00:27:03,039 --> 00:27:04,960
time so again how fast should you

822
00:27:04,960 --> 00:27:06,240
decrease it

823
00:27:06,240 --> 00:27:07,919
still have to fit parameters but that's

824
00:27:07,919 --> 00:27:09,919
just one approach

825
00:27:09,919 --> 00:27:13,840
the other approach to sort of prevent of

826
00:27:13,840 --> 00:27:17,200
this inefficient exploration so avoid

827
00:27:17,200 --> 00:27:18,159
pain

828
00:27:18,159 --> 00:27:20,720
is to be optimistic about values that

829
00:27:20,720 --> 00:27:21,200
are

830
00:27:21,200 --> 00:27:22,720
optimistic about options with high

831
00:27:22,720 --> 00:27:24,480
uncertainty and thus

832
00:27:24,480 --> 00:27:26,960
prefer actions implicitly for which we

833
00:27:26,960 --> 00:27:28,799
haven't yet had a confidence value

834
00:27:28,799 --> 00:27:30,559
estimate that's why this is optimism in

835
00:27:30,559 --> 00:27:32,320
the face of uncertainty

836
00:27:32,320 --> 00:27:34,480
in other words we favor exploration of

837
00:27:34,480 --> 00:27:36,559
actions with a strong potential to have

838
00:27:36,559 --> 00:27:38,799
optimal value so that's exactly what the

839
00:27:38,799 --> 00:27:39,600
ucb

840
00:27:39,600 --> 00:27:42,720
upper confidence bound algorithm does is

841
00:27:42,720 --> 00:27:44,799
it measures it with an upper confidence

842
00:27:44,799 --> 00:27:47,520
bound so that the true value is always

843
00:27:47,520 --> 00:27:48,320
below

844
00:27:48,320 --> 00:27:52,559
that bound and then we are trying to

845
00:27:52,559 --> 00:27:55,279
push up the upper bound knowing that

846
00:27:55,279 --> 00:27:56,559
somewhere below it

847
00:27:56,559 --> 00:28:00,799
hopefully not too far is the true value

848
00:28:00,799 --> 00:28:03,440
and then the ucv algorithm does

849
00:28:03,440 --> 00:28:06,559
optimization with this arg max

850
00:28:06,559 --> 00:28:08,159
selecting the greediest action to

851
00:28:08,159 --> 00:28:10,559
maximize that upper confidence bound

852
00:28:10,559 --> 00:28:12,960
so as it's laid out in the blog

853
00:28:12,960 --> 00:28:14,000
basically you can do

854
00:28:14,000 --> 00:28:17,440
no exploration that's just sort of

855
00:28:17,440 --> 00:28:19,279
pick the first one you sit down at stay

856
00:28:19,279 --> 00:28:21,600
there you can explore at random

857
00:28:21,600 --> 00:28:24,960
that's epsilon greedy or you can explore

858
00:28:24,960 --> 00:28:25,600
smartly

859
00:28:25,600 --> 00:28:27,840
with a preference for uncertainty so

860
00:28:27,840 --> 00:28:29,279
we're just kind of building on that

861
00:28:29,279 --> 00:28:29,840
epsilon

862
00:28:29,840 --> 00:28:31,760
idea of sticking with the one that

863
00:28:31,760 --> 00:28:33,279
usually we like

864
00:28:33,279 --> 00:28:35,600
but then spending sometimes elsewhere

865
00:28:35,600 --> 00:28:37,279
how much time spending elsewhere and

866
00:28:37,279 --> 00:28:38,720
which ones should we choose well don't

867
00:28:38,720 --> 00:28:40,240
just go to any old machine if you're

868
00:28:40,240 --> 00:28:41,919
going to select somewhere else where

869
00:28:41,919 --> 00:28:44,080
let's choose ones that still have a good

870
00:28:44,080 --> 00:28:46,159
probability of having a high expected

871
00:28:46,159 --> 00:28:47,200
payoff

872
00:28:47,200 --> 00:28:50,320
so that's ucb any comments on that

873
00:28:50,320 --> 00:28:53,279
no so in contrast we have thompson

874
00:28:53,279 --> 00:28:54,880
sampling

875
00:28:54,880 --> 00:28:58,799
from a nice slide deck from agrowall

876
00:28:58,799 --> 00:29:01,760
columbia and thompson sampling goes back

877
00:29:01,760 --> 00:29:03,840
to 1933

878
00:29:03,840 --> 00:29:05,679
so like many other algorithms the

879
00:29:05,679 --> 00:29:07,760
classical variants

880
00:29:07,760 --> 00:29:10,480
are pre-computational sometimes they're

881
00:29:10,480 --> 00:29:12,640
even just thought experiments

882
00:29:12,640 --> 00:29:14,720
and in the slides it's talked about how

883
00:29:14,720 --> 00:29:16,159
thompson sampling it's a

884
00:29:16,159 --> 00:29:18,399
natural and efficient heuristic that

885
00:29:18,399 --> 00:29:19,520
maintains belief

886
00:29:19,520 --> 00:29:21,760
about the effectiveness which is the

887
00:29:21,760 --> 00:29:23,760
main reward of each arm

888
00:29:23,760 --> 00:29:25,440
we're going to be kind of tracking how

889
00:29:25,440 --> 00:29:27,440
well we think each arm is doing through

890
00:29:27,440 --> 00:29:28,399
time

891
00:29:28,399 --> 00:29:30,000
and basically the way it works is we're

892
00:29:30,000 --> 00:29:32,000
going to observe feedback

893
00:29:32,000 --> 00:29:34,159
then update our beliefs about different

894
00:29:34,159 --> 00:29:36,240
arms in a bayesian manner

895
00:29:36,240 --> 00:29:38,159
and then pull arms with a posterior

896
00:29:38,159 --> 00:29:40,399
probability of being the best arm

897
00:29:40,399 --> 00:29:42,240
so not the same as choosing the arm

898
00:29:42,240 --> 00:29:44,399
that's most likely to be best

899
00:29:44,399 --> 00:29:47,039
this is like again a pie chart that is

900
00:29:47,039 --> 00:29:48,240
the proportions

901
00:29:48,240 --> 00:29:49,760
of how well they're performing and then

902
00:29:49,760 --> 00:29:52,480
we choose based upon that proportion

903
00:29:52,480 --> 00:29:56,399
and the pseudo code looks like this

904
00:29:56,399 --> 00:29:59,679
we start we initialize the model

905
00:29:59,679 --> 00:30:03,919
with a prior as well as the family of

906
00:30:03,919 --> 00:30:06,240
distributions that our rewards are

907
00:30:06,240 --> 00:30:08,559
expected to be drawn from so this is a

908
00:30:08,559 --> 00:30:09,279
gaussian

909
00:30:09,279 --> 00:30:10,960
case but this is where you could see a

910
00:30:10,960 --> 00:30:12,960
bernoulli bandit come into play

911
00:30:12,960 --> 00:30:16,720
and then the algorithm is as follows

912
00:30:16,720 --> 00:30:20,159
first there's a sampling of a

913
00:30:20,159 --> 00:30:23,440
mean with an estimate from the posterior

914
00:30:23,440 --> 00:30:26,799
for a given arm i then the

915
00:30:26,799 --> 00:30:28,799
arms are played according to the

916
00:30:28,799 --> 00:30:29,919
probability

917
00:30:29,919 --> 00:30:33,279
of them being the best arm

918
00:30:33,279 --> 00:30:36,799
the reward is observed and then

919
00:30:36,799 --> 00:30:39,520
everything is updated so it's like

920
00:30:39,520 --> 00:30:41,679
sample

921
00:30:41,679 --> 00:30:44,559
from your posterior from your prediction

922
00:30:44,559 --> 00:30:44,960
then

923
00:30:44,960 --> 00:30:48,399
act then observe and update it's sort of

924
00:30:48,399 --> 00:30:49,600
like that closing time

925
00:30:49,600 --> 00:30:51,440
song you know every every every

926
00:30:51,440 --> 00:30:52,640
beginning comes from some other

927
00:30:52,640 --> 00:30:54,159
beginnings end

928
00:30:54,159 --> 00:30:56,559
and it's called a posterior in bayesian

929
00:30:56,559 --> 00:30:57,600
statistics you know

930
00:30:57,600 --> 00:31:00,159
prior gets updated the posterior but

931
00:31:00,159 --> 00:31:01,679
then that posterior

932
00:31:01,679 --> 00:31:04,399
is the prior for the next round so it's

933
00:31:04,399 --> 00:31:05,840
not like it's just a one

934
00:31:05,840 --> 00:31:08,960
round learning the posterior feeds back

935
00:31:08,960 --> 00:31:10,480
into the model which is why we just talk

936
00:31:10,480 --> 00:31:12,559
about continual bayesian updating

937
00:31:12,559 --> 00:31:15,600
so it's kind of like ooda like

938
00:31:15,600 --> 00:31:18,240
observe orient decide act these kinds of

939
00:31:18,240 --> 00:31:19,679
action loops which

940
00:31:19,679 --> 00:31:22,240
of course include active inference is

941
00:31:22,240 --> 00:31:22,880
what

942
00:31:22,880 --> 00:31:26,080
makes these models really similar or at

943
00:31:26,080 --> 00:31:27,679
least in the same neighborhood

944
00:31:27,679 --> 00:31:28,960
and then this paper that we're

945
00:31:28,960 --> 00:31:32,240
discussing brings them into alignment

946
00:31:32,240 --> 00:31:34,320
analytically with the equations and

947
00:31:34,320 --> 00:31:37,440
through simulation and juxtaposition

948
00:31:37,440 --> 00:31:39,360
that's what thompson sampling is though

949
00:31:39,360 --> 00:31:42,240
any comments on thompson

950
00:31:42,240 --> 00:31:44,399
just how well it plays into the

951
00:31:44,399 --> 00:31:46,320
sequential decision making process that

952
00:31:46,320 --> 00:31:47,679
we were talking about earlier right so

953
00:31:47,679 --> 00:31:49,200
it's you have a next time step and you

954
00:31:49,200 --> 00:31:51,120
update and you update and you update

955
00:31:51,120 --> 00:31:52,960
yeah so again like even for those

956
00:31:52,960 --> 00:31:54,960
non-sequential tasks like the image

957
00:31:54,960 --> 00:31:56,640
classification you brought up

958
00:31:56,640 --> 00:31:59,679
you could sample from your big database

959
00:31:59,679 --> 00:32:01,600
then update your model and then once

960
00:32:01,600 --> 00:32:02,880
you're sampling and you're like i'm not

961
00:32:02,880 --> 00:32:05,120
really updating my model that much

962
00:32:05,120 --> 00:32:06,960
you might not need to look through the

963
00:32:06,960 --> 00:32:08,640
entire data set

964
00:32:08,640 --> 00:32:10,960
and so by framing that non-sequential

965
00:32:10,960 --> 00:32:12,480
problem sequentially

966
00:32:12,480 --> 00:32:15,679
you get big computational speed up

967
00:32:15,679 --> 00:32:16,960
and then of course for sequential

968
00:32:16,960 --> 00:32:18,799
decision making then you need an

969
00:32:18,799 --> 00:32:21,519
approach like this

970
00:32:22,240 --> 00:32:25,360
how about regret learning regrets i've

971
00:32:25,360 --> 00:32:27,279
had a few

972
00:32:27,279 --> 00:32:30,240
haven't we all right um so i think you

973
00:32:30,240 --> 00:32:31,840
put this in here the introduction to

974
00:32:31,840 --> 00:32:33,600
regret and reinforcement learning which

975
00:32:33,600 --> 00:32:33,919
is

976
00:32:33,919 --> 00:32:37,039
um a medium post correct

977
00:32:37,039 --> 00:32:38,320
uh so if you wanted to learn more about

978
00:32:38,320 --> 00:32:40,080
regret you can check it out there but

979
00:32:40,080 --> 00:32:40,960
regret is

980
00:32:40,960 --> 00:32:43,679
um just the the difference between the

981
00:32:43,679 --> 00:32:45,519
optimal performance how good you could

982
00:32:45,519 --> 00:32:46,320
do

983
00:32:46,320 --> 00:32:48,159
and the actual performance and so you

984
00:32:48,159 --> 00:32:50,320
see in this little image here

985
00:32:50,320 --> 00:32:52,880
um you can see that the best policy is

986
00:32:52,880 --> 00:32:54,559
the red dotted line and then the choices

987
00:32:54,559 --> 00:32:55,600
that the agent makes

988
00:32:55,600 --> 00:32:58,799
this is like the logarithmic performance

989
00:32:58,799 --> 00:33:01,039
logarithmic regret and so

990
00:33:01,039 --> 00:33:02,799
they're they're converging the idea is

991
00:33:02,799 --> 00:33:04,080
to converge with the best

992
00:33:04,080 --> 00:33:06,720
possible policy based on your previous

993
00:33:06,720 --> 00:33:07,600
decisions right

994
00:33:07,600 --> 00:33:10,320
exploring exploiting um and then the

995
00:33:10,320 --> 00:33:11,519
authors here use

996
00:33:11,519 --> 00:33:13,360
regret when they're looking at the

997
00:33:13,360 --> 00:33:14,880
stationary bandits

998
00:33:14,880 --> 00:33:16,240
they use regret as a measure of

999
00:33:16,240 --> 00:33:18,559
performance but then they use regret

1000
00:33:18,559 --> 00:33:20,159
rate when they're looking at the

1001
00:33:20,159 --> 00:33:22,320
switching bandits and so regret rate is

1002
00:33:22,320 --> 00:33:23,039
just

1003
00:33:23,039 --> 00:33:25,039
the regret over time and they use the

1004
00:33:25,039 --> 00:33:26,399
rate they said it was

1005
00:33:26,399 --> 00:33:28,000
had been illustrated to be a better

1006
00:33:28,000 --> 00:33:29,440
estimator of this

1007
00:33:29,440 --> 00:33:33,279
logarithmic regret in the dynamic case

1008
00:33:33,279 --> 00:33:37,360
awesome and the big idea of regret

1009
00:33:37,360 --> 00:33:40,399
is looking back over your history

1010
00:33:40,399 --> 00:33:42,240
so either for all of history that's

1011
00:33:42,240 --> 00:33:44,240
cumulative regret which applies really

1012
00:33:44,240 --> 00:33:46,000
well to the stationary problem

1013
00:33:46,000 --> 00:33:49,279
or recent history which applies well to

1014
00:33:49,279 --> 00:33:50,720
the dynamical case

1015
00:33:50,720 --> 00:33:52,080
because if things are always changing

1016
00:33:52,080 --> 00:33:53,840
you're kind of more interested in how

1017
00:33:53,840 --> 00:33:54,159
well

1018
00:33:54,159 --> 00:33:57,600
you're doing recently rather than

1019
00:33:57,600 --> 00:33:59,279
over all time you don't want some

1020
00:33:59,279 --> 00:34:00,960
fluctuations early on to be

1021
00:34:00,960 --> 00:34:03,279
playing a role in your cumulative value

1022
00:34:03,279 --> 00:34:04,159
because you don't want to fit your

1023
00:34:04,159 --> 00:34:05,440
strategy now

1024
00:34:05,440 --> 00:34:07,120
to reducing regret from a different

1025
00:34:07,120 --> 00:34:09,119
epoch which can happen if you don't do

1026
00:34:09,119 --> 00:34:09,918
it this way

1027
00:34:09,918 --> 00:34:11,839
so looking back over history again

1028
00:34:11,839 --> 00:34:13,359
whether all of history

1029
00:34:13,359 --> 00:34:16,000
cumulative regret for the fixed case or

1030
00:34:16,000 --> 00:34:18,639
recent history for the dynamic case

1031
00:34:18,639 --> 00:34:21,199
update your strategy so that you would

1032
00:34:21,199 --> 00:34:23,359
have minimized regret to xero

1033
00:34:23,359 --> 00:34:26,079
by playing that strategy all along so

1034
00:34:26,079 --> 00:34:26,800
you know

1035
00:34:26,800 --> 00:34:30,000
looking back at the bitcoin price what

1036
00:34:30,000 --> 00:34:31,199
strategy

1037
00:34:31,199 --> 00:34:34,480
would have been no regrets

1038
00:34:34,480 --> 00:34:36,800
those are the big questions or just in

1039
00:34:36,800 --> 00:34:39,119
the last little window of time

1040
00:34:39,119 --> 00:34:42,159
given what i recently have found out how

1041
00:34:42,159 --> 00:34:44,399
could i make my rate of regret

1042
00:34:44,399 --> 00:34:48,000
gain as low as possible so

1043
00:34:48,000 --> 00:34:50,320
it's a way to look back and then

1044
00:34:50,320 --> 00:34:51,359
optimistically

1045
00:34:51,359 --> 00:34:54,719
think about how you could have performed

1046
00:34:54,719 --> 00:34:57,359
by reducing regret to zero and we see

1047
00:34:57,359 --> 00:34:57,760
this

1048
00:34:57,760 --> 00:34:59,920
all the time in computer science like

1049
00:34:59,920 --> 00:35:01,200
maximizing something

1050
00:35:01,200 --> 00:35:03,200
you do it by minimizing whether there's

1051
00:35:03,200 --> 00:35:04,880
a negative thrown in there

1052
00:35:04,880 --> 00:35:07,359
or whether there's a natural log thrown

1053
00:35:07,359 --> 00:35:08,560
in there or whether it's

1054
00:35:08,560 --> 00:35:10,640
one over something or it's just framed

1055
00:35:10,640 --> 00:35:12,240
in an opposite way

1056
00:35:12,240 --> 00:35:13,760
a lot of times if you know that you're

1057
00:35:13,760 --> 00:35:15,760
bounded at zero something can't go below

1058
00:35:15,760 --> 00:35:16,560
zero

1059
00:35:16,560 --> 00:35:19,280
then you want to go as low as possible

1060
00:35:19,280 --> 00:35:19,920
or

1061
00:35:19,920 --> 00:35:21,760
if you want to maximize something it's

1062
00:35:21,760 --> 00:35:23,839
sometimes easier to do one over the

1063
00:35:23,839 --> 00:35:25,359
maximum and then try to get that number

1064
00:35:25,359 --> 00:35:26,400
to zero

1065
00:35:26,400 --> 00:35:28,560
rather than this unbounded maximization

1066
00:35:28,560 --> 00:35:30,960
like okay i'm at 5 million should i stop

1067
00:35:30,960 --> 00:35:32,560
you don't know maybe that's nowhere even

1068
00:35:32,560 --> 00:35:35,520
close because there's no highest number

1069
00:35:35,520 --> 00:35:37,200
so that's regret learning and that's how

1070
00:35:37,200 --> 00:35:38,960
they're going to be calculating their

1071
00:35:38,960 --> 00:35:41,119
performance

1072
00:35:41,119 --> 00:35:44,400
let's get to active inference so one

1073
00:35:44,400 --> 00:35:45,200
thing

1074
00:35:45,200 --> 00:35:46,960
that you'll see on this slide right away

1075
00:35:46,960 --> 00:35:48,400
is we're not seeing

1076
00:35:48,400 --> 00:35:51,920
a sensory motor loop we're not seeing an

1077
00:35:51,920 --> 00:35:54,720
agent in an environment with arrows and

1078
00:35:54,720 --> 00:35:56,160
nodes

1079
00:35:56,160 --> 00:35:57,839
we're coming at active inference from

1080
00:35:57,839 --> 00:35:59,520
this point of view of sequential

1081
00:35:59,520 --> 00:36:00,400
decision making

1082
00:36:00,400 --> 00:36:03,920
under uncertainty and so in this section

1083
00:36:03,920 --> 00:36:05,200
which is a great title

1084
00:36:05,200 --> 00:36:09,680
three two one active inference

1085
00:36:09,680 --> 00:36:12,320
they write that the exploration

1086
00:36:12,320 --> 00:36:14,320
exploitation trade-off can be formulated

1087
00:36:14,320 --> 00:36:16,160
as an uncertainty reduction

1088
00:36:16,160 --> 00:36:19,280
problem where choices aim to resolve

1089
00:36:19,280 --> 00:36:20,800
expected and unexpected

1090
00:36:20,800 --> 00:36:22,640
uncertainty about hidden properties of

1091
00:36:22,640 --> 00:36:24,640
the environment so already we're seeing

1092
00:36:24,640 --> 00:36:26,640
that active inference formulization

1093
00:36:26,640 --> 00:36:28,160
there's hidden states in the environment

1094
00:36:28,160 --> 00:36:30,079
which we don't directly have access to

1095
00:36:30,079 --> 00:36:32,640
but we get admitted outcomes from

1096
00:36:32,640 --> 00:36:34,320
decisions

1097
00:36:34,320 --> 00:36:36,320
this leads to casting choice behavior

1098
00:36:36,320 --> 00:36:37,440
and planning

1099
00:36:37,440 --> 00:36:39,839
aka planning as inference as a

1100
00:36:39,839 --> 00:36:42,079
probabilistic inference problem

1101
00:36:42,079 --> 00:36:44,800
as expressed by active inference so

1102
00:36:44,800 --> 00:36:45,359
action

1103
00:36:45,359 --> 00:36:47,920
and inference our inference is about

1104
00:36:47,920 --> 00:36:49,839
what actions we're planning and about

1105
00:36:49,839 --> 00:36:51,440
the hidden states of the world given the

1106
00:36:51,440 --> 00:36:52,640
outcomes that we're receiving

1107
00:36:52,640 --> 00:36:55,839
sensory data using this approach

1108
00:36:55,839 --> 00:36:58,240
different types of exploitative and

1109
00:36:58,240 --> 00:37:00,079
exploratory

1110
00:37:00,079 --> 00:37:03,119
behavior naturally emerge in active

1111
00:37:03,119 --> 00:37:04,800
inference decision strategies

1112
00:37:04,800 --> 00:37:08,079
behavioral policies are chosen based on

1113
00:37:08,079 --> 00:37:09,680
a single optimization

1114
00:37:09,680 --> 00:37:12,480
principle or yesterday i think it was a

1115
00:37:12,480 --> 00:37:14,320
what was it a functional

1116
00:37:14,320 --> 00:37:15,920
a common functional footing or something

1117
00:37:15,920 --> 00:37:18,160
like that the single optimization

1118
00:37:18,160 --> 00:37:19,119
principle

1119
00:37:19,119 --> 00:37:21,680
is minimizing expected surprisal about

1120
00:37:21,680 --> 00:37:23,680
observed and future outcomes

1121
00:37:23,680 --> 00:37:26,560
that is the expected free energy so it's

1122
00:37:26,560 --> 00:37:27,920
kind of cool like

1123
00:37:27,920 --> 00:37:30,960
the backwards looking approach

1124
00:37:30,960 --> 00:37:34,160
is to minimize regret like how

1125
00:37:34,160 --> 00:37:37,280
would i have performed best in

1126
00:37:37,280 --> 00:37:39,599
given what i know about the past how

1127
00:37:39,599 --> 00:37:40,400
does that

1128
00:37:40,400 --> 00:37:43,520
change the way i act now and then free

1129
00:37:43,520 --> 00:37:46,480
energy minimization expected free energy

1130
00:37:46,480 --> 00:37:48,880
in the current and the future moments is

1131
00:37:48,880 --> 00:37:49,599
like

1132
00:37:49,599 --> 00:37:52,880
given the state of my model for now

1133
00:37:52,880 --> 00:37:55,040
and moving forward how can i minimize

1134
00:37:55,040 --> 00:37:56,720
expected free energy

1135
00:37:56,720 --> 00:37:59,280
because regret is not anticipatory

1136
00:37:59,280 --> 00:38:01,599
regret is only looking back at it

1137
00:38:01,599 --> 00:38:03,280
which is why it's really easy for

1138
00:38:03,280 --> 00:38:05,200
training and then here's something

1139
00:38:05,200 --> 00:38:06,480
that's looking forward

1140
00:38:06,480 --> 00:38:07,920
so it's kind of cool how like the

1141
00:38:07,920 --> 00:38:10,560
current moment is this handoff

1142
00:38:10,560 --> 00:38:13,200
between the retrospective regret

1143
00:38:13,200 --> 00:38:14,079
learning

1144
00:38:14,079 --> 00:38:16,839
and then the prospective free energy

1145
00:38:16,839 --> 00:38:18,320
minimization

1146
00:38:18,320 --> 00:38:20,079
that's how they frame active inference

1147
00:38:20,079 --> 00:38:22,320
here any general comments or what do you

1148
00:38:22,320 --> 00:38:24,800
think was kind of cool about that

1149
00:38:24,800 --> 00:38:27,280
so i just i'm thinking back to like alex

1150
00:38:27,280 --> 00:38:29,280
chance's paper that we did way back i

1151
00:38:29,280 --> 00:38:30,720
think that was number eight but

1152
00:38:30,720 --> 00:38:32,800
um i i still always go back to that when

1153
00:38:32,800 --> 00:38:34,480
i'm looking at these computational

1154
00:38:34,480 --> 00:38:36,000
frameworks because he broke it up very

1155
00:38:36,000 --> 00:38:36,800
nicely

1156
00:38:36,800 --> 00:38:39,440
into like the reward the pragmatic value

1157
00:38:39,440 --> 00:38:41,200
which could be like in this case

1158
00:38:41,200 --> 00:38:43,599
the minimization of regret can be seen

1159
00:38:43,599 --> 00:38:45,119
as like the pragmatic value or the

1160
00:38:45,119 --> 00:38:46,000
reward

1161
00:38:46,000 --> 00:38:48,880
and then the epistemic value right so

1162
00:38:48,880 --> 00:38:50,240
you have both of these things that play

1163
00:38:50,240 --> 00:38:51,040
into

1164
00:38:51,040 --> 00:38:54,079
um giving the system how helping this

1165
00:38:54,079 --> 00:38:55,280
the agent update

1166
00:38:55,280 --> 00:38:58,320
relevant to the system cool and i'll

1167
00:38:58,320 --> 00:38:59,440
look forward to

1168
00:38:59,440 --> 00:39:01,599
hearing from the authors were they

1169
00:39:01,599 --> 00:39:03,440
studying active inference and then

1170
00:39:03,440 --> 00:39:05,440
approached bayesian decision making

1171
00:39:05,440 --> 00:39:05,839
appro

1172
00:39:05,839 --> 00:39:09,119
uh algorithms were they coming from a

1173
00:39:09,119 --> 00:39:10,960
non-active inference computer science

1174
00:39:10,960 --> 00:39:12,079
background and then

1175
00:39:12,079 --> 00:39:13,599
found active inference as something that

1176
00:39:13,599 --> 00:39:15,599
was exciting how did they

1177
00:39:15,599 --> 00:39:19,440
converge upon this approach

1178
00:39:20,480 --> 00:39:22,320
shortly after introducing active

1179
00:39:22,320 --> 00:39:23,680
inference they turned to

1180
00:39:23,680 --> 00:39:26,800
an approximate active inference and they

1181
00:39:26,800 --> 00:39:28,320
described that as saying

1182
00:39:28,320 --> 00:39:30,320
active inference in its initial form was

1183
00:39:30,320 --> 00:39:32,800
developed for small state spaces and toy

1184
00:39:32,800 --> 00:39:33,520
problems

1185
00:39:33,520 --> 00:39:35,520
without consideration for applications

1186
00:39:35,520 --> 00:39:36,960
to typical machine learning

1187
00:39:36,960 --> 00:39:39,119
problems so that's like when there's two

1188
00:39:39,119 --> 00:39:41,440
decisions in sort of a two by two matrix

1189
00:39:41,440 --> 00:39:42,800
and two outcomes in the world

1190
00:39:42,800 --> 00:39:46,000
and two decisions you can make this has

1191
00:39:46,000 --> 00:39:47,359
recently changed

1192
00:39:47,359 --> 00:39:49,280
and various scalable solutions have been

1193
00:39:49,280 --> 00:39:50,960
proposed i think one of these citations

1194
00:39:50,960 --> 00:39:51,599
would be that

1195
00:39:51,599 --> 00:39:53,920
chance at all uh scaling active

1196
00:39:53,920 --> 00:39:56,079
inference active stream eight

1197
00:39:56,079 --> 00:39:58,800
in addition to complex sequential policy

1198
00:39:58,800 --> 00:39:59,920
optimization

1199
00:39:59,920 --> 00:40:01,839
that involves sophisticated deep tree

1200
00:40:01,839 --> 00:40:04,400
searches so that's sophisticated active

1201
00:40:04,400 --> 00:40:06,720
inference with these tree rollouts

1202
00:40:06,720 --> 00:40:10,160
followed by tree pruning approaches

1203
00:40:10,160 --> 00:40:12,160
therefore to make the active inference

1204
00:40:12,160 --> 00:40:13,280
approach practical

1205
00:40:13,280 --> 00:40:15,119
and scalable to the high dimensional

1206
00:40:15,119 --> 00:40:16,880
bandit problems typically used in

1207
00:40:16,880 --> 00:40:17,839
machine learning

1208
00:40:17,839 --> 00:40:19,920
we introduce here an approximate active

1209
00:40:19,920 --> 00:40:21,920
inference algorithm

1210
00:40:21,920 --> 00:40:23,839
definitely will be cool to hear from the

1211
00:40:23,839 --> 00:40:25,520
authors like what

1212
00:40:25,520 --> 00:40:28,640
exactly are we approximating what else

1213
00:40:28,640 --> 00:40:30,400
could be approximated

1214
00:40:30,400 --> 00:40:32,640
and still have it active inference or

1215
00:40:32,640 --> 00:40:35,040
what can't be approximated

1216
00:40:35,040 --> 00:40:37,520
but let's look at how and why they

1217
00:40:37,520 --> 00:40:40,720
approximated active inference

1218
00:40:40,720 --> 00:40:43,599
so this is so kind of yeah so to kind of

1219
00:40:43,599 --> 00:40:45,200
speak to the why you and i have both

1220
00:40:45,200 --> 00:40:45,599
done

1221
00:40:45,599 --> 00:40:47,920
um you know tree construction in in

1222
00:40:47,920 --> 00:40:49,839
terms of phylogenetics right in the tree

1223
00:40:49,839 --> 00:40:51,440
switching and the switching of nodes

1224
00:40:51,440 --> 00:40:53,280
and that can get really computationally

1225
00:40:53,280 --> 00:40:54,560
overwhelming

1226
00:40:54,560 --> 00:40:56,640
like can run for days and days and days

1227
00:40:56,640 --> 00:40:58,800
so um it's it's really

1228
00:40:58,800 --> 00:41:00,720
nice to see such a sweet clean

1229
00:41:00,720 --> 00:41:03,200
implementation of approximation here

1230
00:41:03,200 --> 00:41:06,240
yes here

1231
00:41:06,240 --> 00:41:09,359
we're not going to go into depth into

1232
00:41:09,359 --> 00:41:11,680
formalism 16 and 17 but we'll look

1233
00:41:11,680 --> 00:41:13,680
forward to the authors describing it

1234
00:41:13,680 --> 00:41:17,200
but here is the key piece the exact

1235
00:41:17,200 --> 00:41:19,520
marginal posterior beliefs over reward

1236
00:41:19,520 --> 00:41:21,839
probabilities

1237
00:41:21,839 --> 00:41:25,200
theta can be expressed this way okay

1238
00:41:25,200 --> 00:41:27,119
just looking at that you go okay it's

1239
00:41:27,119 --> 00:41:29,119
gnarly how

1240
00:41:29,119 --> 00:41:32,319
are we going to win if that's what we

1241
00:41:32,319 --> 00:41:33,520
have to solve

1242
00:41:33,520 --> 00:41:36,640
so the exact beliefs over uh

1243
00:41:36,640 --> 00:41:38,960
the exact correct beliefs over reward

1244
00:41:38,960 --> 00:41:40,640
would look this way

1245
00:41:40,640 --> 00:41:42,240
and then they write the exact marginal

1246
00:41:42,240 --> 00:41:44,640
posterior in equation 17

1247
00:41:44,640 --> 00:41:46,800
will not belong to the beta distribution

1248
00:41:46,800 --> 00:41:48,880
making the exact inference analytically

1249
00:41:48,880 --> 00:41:50,240
intractable

1250
00:41:50,240 --> 00:41:53,280
so there might be a way to numerically

1251
00:41:53,280 --> 00:41:56,000
approach it and simulate it or calculate

1252
00:41:56,000 --> 00:41:56,640
it but

1253
00:41:56,640 --> 00:41:58,000
if we want to be looking through big

1254
00:41:58,000 --> 00:41:59,760
data sets and doing it fast

1255
00:41:59,760 --> 00:42:01,680
without massive amounts of computational

1256
00:42:01,680 --> 00:42:04,000
effort we need a different way

1257
00:42:04,000 --> 00:42:07,280
however constrain if we constrain the

1258
00:42:07,280 --> 00:42:09,359
joint posterior to this approximate

1259
00:42:09,359 --> 00:42:11,200
fully factorized form

1260
00:42:11,200 --> 00:42:13,680
and we've seen factorization of

1261
00:42:13,680 --> 00:42:16,800
variables before factorization of

1262
00:42:16,800 --> 00:42:18,000
variables is kind of a

1263
00:42:18,000 --> 00:42:21,280
big topic uh so we're not going to

1264
00:42:21,280 --> 00:42:22,560
totally

1265
00:42:22,560 --> 00:42:24,400
go into detail here again it'd be

1266
00:42:24,400 --> 00:42:26,000
awesome to hear from the authors how did

1267
00:42:26,000 --> 00:42:27,920
they derive and factorize

1268
00:42:27,920 --> 00:42:31,119
but one way to think about it is that

1269
00:42:31,119 --> 00:42:34,079
constraining a big open-ended long

1270
00:42:34,079 --> 00:42:35,119
equation

1271
00:42:35,119 --> 00:42:37,839
into a factorizable form reduces the

1272
00:42:37,839 --> 00:42:38,800
solution space

1273
00:42:38,800 --> 00:42:42,000
a lot and that enables certain kinds of

1274
00:42:42,000 --> 00:42:44,800
optimization to become tractable so for

1275
00:42:44,800 --> 00:42:45,359
example

1276
00:42:45,359 --> 00:42:47,760
for linear regression we have least

1277
00:42:47,760 --> 00:42:48,880
squares

1278
00:42:48,880 --> 00:42:50,800
which is a way where if you constrain

1279
00:42:50,800 --> 00:42:52,880
the problem to saying i'm trying to

1280
00:42:52,880 --> 00:42:54,640
solve the sum of squares

1281
00:42:54,640 --> 00:42:56,720
not the sum of cubes or the sum of some

1282
00:42:56,720 --> 00:42:58,000
other function but i'm just trying to

1283
00:42:58,000 --> 00:42:59,359
solve this one

1284
00:42:59,359 --> 00:43:02,880
then there's computations that scale

1285
00:43:02,880 --> 00:43:03,920
really well

1286
00:43:03,920 --> 00:43:05,520
and so that is what is happening a

1287
00:43:05,520 --> 00:43:07,599
factorization and another way to think

1288
00:43:07,599 --> 00:43:08,960
about factorization

1289
00:43:08,960 --> 00:43:11,280
is if you have a bayesian graph where

1290
00:43:11,280 --> 00:43:13,040
the nodes are the variables

1291
00:43:13,040 --> 00:43:14,400
and then the edges are like the

1292
00:43:14,400 --> 00:43:16,960
relationships between the variables

1293
00:43:16,960 --> 00:43:19,280
you could just dump it all into a

1294
00:43:19,280 --> 00:43:20,800
parameter fitting framework

1295
00:43:20,800 --> 00:43:23,119
with all the edges possible that would

1296
00:43:23,119 --> 00:43:24,960
be like an unfactorized model

1297
00:43:24,960 --> 00:43:27,200
so the number of edges that you have to

1298
00:43:27,200 --> 00:43:28,560
figure out the

1299
00:43:28,560 --> 00:43:31,920
exact values for is going to be a lot

1300
00:43:31,920 --> 00:43:33,520
and it's going to increase exponentially

1301
00:43:33,520 --> 00:43:34,800
with the amount of parameters you want

1302
00:43:34,800 --> 00:43:35,680
to fit

1303
00:43:35,680 --> 00:43:38,240
but if you factorize things you go okay

1304
00:43:38,240 --> 00:43:39,119
variable a

1305
00:43:39,119 --> 00:43:41,599
only is associated with b and b is only

1306
00:43:41,599 --> 00:43:42,880
associated with c

1307
00:43:42,880 --> 00:43:45,200
and a all of a sudden you're reducing

1308
00:43:45,200 --> 00:43:47,599
the amount of edges that you have to fit

1309
00:43:47,599 --> 00:43:50,720
which helps you get to hopefully

1310
00:43:50,720 --> 00:43:54,000
a solution that is attractively reached

1311
00:43:54,000 --> 00:43:54,880
but also

1312
00:43:54,880 --> 00:43:57,280
approximately accurate so that's like

1313
00:43:57,280 --> 00:43:59,760
probably probably approximately correct

1314
00:43:59,760 --> 00:44:02,960
and approximate bayesian computation but

1315
00:44:02,960 --> 00:44:05,359
we've seen that factorization a few

1316
00:44:05,359 --> 00:44:06,560
times

1317
00:44:06,560 --> 00:44:10,079
won't go into too much more detail here

1318
00:44:10,079 --> 00:44:13,040
but what they do is they take this

1319
00:44:13,040 --> 00:44:15,359
factorized form

1320
00:44:15,359 --> 00:44:18,319
and they use variational calculus to

1321
00:44:18,319 --> 00:44:19,520
recover the following

1322
00:44:19,520 --> 00:44:21,680
variational smile rule which we'll go to

1323
00:44:21,680 --> 00:44:23,280
in a second

1324
00:44:23,280 --> 00:44:24,960
and so they get to a different set of

1325
00:44:24,960 --> 00:44:27,119
formalisms we'll hear from the authors

1326
00:44:27,119 --> 00:44:27,520
about

1327
00:44:27,520 --> 00:44:30,319
what's different about those formalisms

1328
00:44:30,319 --> 00:44:30,880
and

1329
00:44:30,880 --> 00:44:34,400
then what's kind of cool is that for the

1330
00:44:34,400 --> 00:44:35,599
stationary bandit

1331
00:44:35,599 --> 00:44:38,800
where you can change the parameters to

1332
00:44:38,800 --> 00:44:42,560
like zero for the rate of change

1333
00:44:42,560 --> 00:44:44,480
it corresponds to the exact beijing

1334
00:44:44,480 --> 00:44:46,160
inference over stationary bernoulli

1335
00:44:46,160 --> 00:44:47,760
bandit problem

1336
00:44:47,760 --> 00:44:50,000
so that was just kind of interesting how

1337
00:44:50,000 --> 00:44:50,960
the exact

1338
00:44:50,960 --> 00:44:54,640
form of an algorithm solution might

1339
00:44:54,640 --> 00:44:57,359
uh how active inference might be doing

1340
00:44:57,359 --> 00:44:57,760
something

1341
00:44:57,760 --> 00:45:01,040
exact in the asymptote

1342
00:45:01,040 --> 00:45:04,560
what is smile here's the smile paper

1343
00:45:04,560 --> 00:45:06,480
variational methods as a prize to

1344
00:45:06,480 --> 00:45:08,560
surprise minimization learning

1345
00:45:08,560 --> 00:45:10,079
so that sounds like a lot of things that

1346
00:45:10,079 --> 00:45:11,839
we do with variational methods

1347
00:45:11,839 --> 00:45:13,200
variational base

1348
00:45:13,200 --> 00:45:16,079
and surprise minimization and the pseudo

1349
00:45:16,079 --> 00:45:16,560
code

1350
00:45:16,560 --> 00:45:18,560
is presented here for the exponential

1351
00:45:18,560 --> 00:45:20,319
family

1352
00:45:20,319 --> 00:45:22,560
this is just a question for the authors

1353
00:45:22,560 --> 00:45:24,160
what is the smile

1354
00:45:24,160 --> 00:45:26,880
doing how does it help us approximate

1355
00:45:26,880 --> 00:45:28,480
active inference

1356
00:45:28,480 --> 00:45:32,000
what else can we use the smile for

1357
00:45:32,000 --> 00:45:33,839
so that's this approximating active

1358
00:45:33,839 --> 00:45:35,440
inference interlude

1359
00:45:35,440 --> 00:45:38,880
they take a big messy form

1360
00:45:38,880 --> 00:45:41,280
and then by constraining how variables

1361
00:45:41,280 --> 00:45:43,920
can be related to each other

1362
00:45:43,920 --> 00:45:45,839
and using variational methods on that

1363
00:45:45,839 --> 00:45:48,640
factorized representation

1364
00:45:48,640 --> 00:45:52,560
it's possible to get an approximation

1365
00:45:52,560 --> 00:45:55,759
that is going to be effective

1366
00:45:56,480 --> 00:45:58,800
so it's interesting that um you know

1367
00:45:58,800 --> 00:46:00,000
there's this direct

1368
00:46:00,000 --> 00:46:02,319
correspondence for the stationary uh

1369
00:46:02,319 --> 00:46:03,760
bandit case

1370
00:46:03,760 --> 00:46:05,920
because the in the stationary case like

1371
00:46:05,920 --> 00:46:07,040
we didn't see

1372
00:46:07,040 --> 00:46:08,720
improvement right using the active

1373
00:46:08,720 --> 00:46:10,240
inference algorithm that totally

1374
00:46:10,240 --> 00:46:12,880
explains why right

1375
00:46:12,880 --> 00:46:14,640
yep and they have a few more pieces on

1376
00:46:14,640 --> 00:46:16,319
that i think in uh

1377
00:46:16,319 --> 00:46:19,680
bigger figure four or something like

1378
00:46:19,680 --> 00:46:20,560
that

1379
00:46:20,560 --> 00:46:24,560
okay let's go to the two

1380
00:46:24,560 --> 00:46:26,720
problems that they tackle and then the

1381
00:46:26,720 --> 00:46:28,000
results for those two sections

1382
00:46:28,000 --> 00:46:29,920
so we'll cover first the stationary

1383
00:46:29,920 --> 00:46:32,720
bandit and then the dynamic bandit

1384
00:46:32,720 --> 00:46:34,880
so here's the definition and where they

1385
00:46:34,880 --> 00:46:36,800
work through

1386
00:46:36,800 --> 00:46:39,280
the stationary bandit so a stationary

1387
00:46:39,280 --> 00:46:42,800
bandit has a finite number of arms

1388
00:46:42,800 --> 00:46:46,800
uh it has uh k arms

1389
00:46:46,800 --> 00:46:49,599
and then it's going to be playing

1390
00:46:49,599 --> 00:46:50,000
through

1391
00:46:50,000 --> 00:46:53,359
t time steps and then it's big k

1392
00:46:53,359 --> 00:46:56,720
big k arms and then each little little k

1393
00:46:56,720 --> 00:46:59,440
k is the set of little arms little k

1394
00:46:59,440 --> 00:47:02,480
yeah k is the action

1395
00:47:03,520 --> 00:47:05,119
so here's how they define the stationary

1396
00:47:05,119 --> 00:47:07,119
bandit and

1397
00:47:07,119 --> 00:47:09,200
again you can think about t and time

1398
00:47:09,200 --> 00:47:10,240
actions

1399
00:47:10,240 --> 00:47:13,359
and uh arms and then in stationary

1400
00:47:13,359 --> 00:47:15,920
bandits the reward probabilities

1401
00:47:15,920 --> 00:47:18,640
theta sub k are fixed for all trials so

1402
00:47:18,640 --> 00:47:19,760
theta is just like

1403
00:47:19,760 --> 00:47:21,280
it's funny it's like the parameter that

1404
00:47:21,280 --> 00:47:22,960
means just like variable

1405
00:47:22,960 --> 00:47:24,800
sometimes i guess a lot of them do but

1406
00:47:24,800 --> 00:47:26,559
data especially

1407
00:47:26,559 --> 00:47:30,800
so just a fixed reward value

1408
00:47:30,800 --> 00:47:33,520
the fixed yeah reward probability so

1409
00:47:33,520 --> 00:47:35,839
that's why it's not stationary

1410
00:47:35,839 --> 00:47:38,079
in the casino this would be like one of

1411
00:47:38,079 --> 00:47:39,040
them pays out

1412
00:47:39,040 --> 00:47:41,359
um on average one to one one of them is

1413
00:47:41,359 --> 00:47:42,960
pays out fifty percent one pays out two

1414
00:47:42,960 --> 00:47:46,240
hundred percent but they never change

1415
00:47:46,240 --> 00:47:47,440
so here's a sort of visual

1416
00:47:47,440 --> 00:47:50,079
representation of that with

1417
00:47:50,079 --> 00:47:52,640
here's the multi-armed and then the

1418
00:47:52,640 --> 00:47:54,960
mouse is getting the cheese

1419
00:47:54,960 --> 00:47:57,280
and each arm has a different fixed

1420
00:47:57,280 --> 00:47:58,240
probability

1421
00:47:58,240 --> 00:47:59,599
but those don't change so the

1422
00:47:59,599 --> 00:48:01,520
probability of cheese

1423
00:48:01,520 --> 00:48:06,480
does not move

1424
00:48:06,480 --> 00:48:10,800
here's figure one so in figure one

1425
00:48:10,800 --> 00:48:14,319
we're going to see regret rate analysis

1426
00:48:14,319 --> 00:48:18,240
so the regret rate you can imagine uh

1427
00:48:18,240 --> 00:48:20,400
is we'll get to the second but the

1428
00:48:20,400 --> 00:48:22,240
regret rate analysis for the stationary

1429
00:48:22,240 --> 00:48:23,440
bandit

1430
00:48:23,440 --> 00:48:25,200
and then we're going to be comparing

1431
00:48:25,200 --> 00:48:28,000
approximate active inference in red

1432
00:48:28,000 --> 00:48:31,680
to the exact active inference in blue

1433
00:48:31,680 --> 00:48:33,359
so that will be cool to talk to the

1434
00:48:33,359 --> 00:48:34,880
authors about

1435
00:48:34,880 --> 00:48:37,359
what was the relative change in how much

1436
00:48:37,359 --> 00:48:40,480
time it took to compute

1437
00:48:41,119 --> 00:48:44,480
we'll find out and what they do is

1438
00:48:44,480 --> 00:48:48,319
they are showing that

1439
00:48:48,319 --> 00:48:49,920
the blue and the red are always kind of

1440
00:48:49,920 --> 00:48:51,599
tracking together

1441
00:48:51,599 --> 00:48:53,520
so that suggests that the approximate

1442
00:48:53,520 --> 00:48:54,960
form does

1443
00:48:54,960 --> 00:48:57,520
a really good job because it follows the

1444
00:48:57,520 --> 00:48:58,160
behavior

1445
00:48:58,160 --> 00:49:01,520
of the exact form pretty well

1446
00:49:01,520 --> 00:49:04,240
and then just to sort of how to read

1447
00:49:04,240 --> 00:49:04,800
this

1448
00:49:04,800 --> 00:49:08,240
uh graph so there's four columns

1449
00:49:08,240 --> 00:49:10,960
corresponding to k equals 10 20 40 and

1450
00:49:10,960 --> 00:49:12,240
80 arms

1451
00:49:12,240 --> 00:49:14,000
so that's changing just the number of

1452
00:49:14,000 --> 00:49:15,440
arms

1453
00:49:15,440 --> 00:49:18,800
then there's the rows which are

1454
00:49:18,800 --> 00:49:22,319
for uh epsilon

1455
00:49:22,319 --> 00:49:23,760
being the the so this is like the

1456
00:49:23,760 --> 00:49:26,079
differential between the arms

1457
00:49:26,079 --> 00:49:29,200
so kind of how easy the um task

1458
00:49:29,200 --> 00:49:32,559
is i believe and then that's

1459
00:49:32,559 --> 00:49:35,440
point one point two five and point four

1460
00:49:35,440 --> 00:49:35,920
and then

1461
00:49:35,920 --> 00:49:38,960
within each cell

1462
00:49:38,960 --> 00:49:42,800
there's lambda uh which is

1463
00:49:42,800 --> 00:49:45,680
a function of precision over prior

1464
00:49:45,680 --> 00:49:47,280
preferences

1465
00:49:47,280 --> 00:49:50,559
and then r sub t

1466
00:49:50,800 --> 00:49:53,200
which is the regret as a function of

1467
00:49:53,200 --> 00:49:55,440
trials

1468
00:49:55,440 --> 00:49:57,520
and the dash black line denotes the

1469
00:49:57,520 --> 00:49:58,960
upper bound on the regret rate

1470
00:49:58,960 --> 00:50:00,319
corresponding to the random

1471
00:50:00,319 --> 00:50:04,720
agent so this is

1472
00:50:04,800 --> 00:50:10,079
i guess as bad as you could get

1473
00:50:10,640 --> 00:50:13,599
and then we see that active inference is

1474
00:50:13,599 --> 00:50:14,319
performing

1475
00:50:14,319 --> 00:50:16,559
better like there's it converges upon a

1476
00:50:16,559 --> 00:50:17,520
lower

1477
00:50:17,520 --> 00:50:20,640
rate of regret than

1478
00:50:20,640 --> 00:50:23,680
0.1 or around 0.1 which is the amount of

1479
00:50:23,680 --> 00:50:25,760
regret that's accumulating

1480
00:50:25,760 --> 00:50:29,839
for the randomly behaving agent

1481
00:50:29,839 --> 00:50:33,200
so so just to footnote that um

1482
00:50:33,200 --> 00:50:35,359
which i thought was interesting and made

1483
00:50:35,359 --> 00:50:37,119
this more meaningful for me that lambda

1484
00:50:37,119 --> 00:50:38,880
parameter the precision

1485
00:50:38,880 --> 00:50:40,640
so it says when the active inference

1486
00:50:40,640 --> 00:50:42,559
agent has very imprecise

1487
00:50:42,559 --> 00:50:46,079
preferences so lambda closer to zero

1488
00:50:46,079 --> 00:50:49,200
it engages in exploration for longer and

1489
00:50:49,200 --> 00:50:50,880
reduces the uncertainty

1490
00:50:50,880 --> 00:50:53,040
that way at the expense of accumulating

1491
00:50:53,040 --> 00:50:54,240
reward so just

1492
00:50:54,240 --> 00:50:56,079
um just to kind of footnote that in

1493
00:50:56,079 --> 00:50:58,160
there

1494
00:50:58,160 --> 00:50:59,440
interesting and that's definitely going

1495
00:50:59,440 --> 00:51:02,240
to come back when we talk about

1496
00:51:02,240 --> 00:51:03,920
active inference and how we rethink

1497
00:51:03,920 --> 00:51:05,760
explore exploit

1498
00:51:05,760 --> 00:51:07,760
so what is this showing it's showing

1499
00:51:07,760 --> 00:51:08,880
that

1500
00:51:08,880 --> 00:51:12,319
as you uh though they're

1501
00:51:12,319 --> 00:51:15,119
that as a function of precision you're

1502
00:51:15,119 --> 00:51:15,520
getting

1503
00:51:15,520 --> 00:51:17,839
different behavior in the active

1504
00:51:17,839 --> 00:51:18,800
inference agents

1505
00:51:18,800 --> 00:51:20,640
but the approximation is basically

1506
00:51:20,640 --> 00:51:22,640
always working pretty well

1507
00:51:22,640 --> 00:51:25,200
and then also the dotted lines are fewer

1508
00:51:25,200 --> 00:51:26,880
trials and the solid lines are more

1509
00:51:26,880 --> 00:51:27,520
trials

1510
00:51:27,520 --> 00:51:29,680
so we can see like in all these cases

1511
00:51:29,680 --> 00:51:31,440
the dotted line and then the dash and

1512
00:51:31,440 --> 00:51:32,880
then the solid

1513
00:51:32,880 --> 00:51:34,839
you're always doing better with more

1514
00:51:34,839 --> 00:51:36,480
trials that's what we

1515
00:51:36,480 --> 00:51:38,640
kind of see like if you only have 100

1516
00:51:38,640 --> 00:51:41,839
trials so just a few trials on 80 arms

1517
00:51:41,839 --> 00:51:43,760
your regret is almost as if you're

1518
00:51:43,760 --> 00:51:45,680
playing randomly because you've barely

1519
00:51:45,680 --> 00:51:47,680
tried every arm once so you're kind of

1520
00:51:47,680 --> 00:51:50,960
not able to do so well but

1521
00:51:50,960 --> 00:51:53,200
if you're in the right area of precision

1522
00:51:53,200 --> 00:51:54,640
not too much precision

1523
00:51:54,640 --> 00:51:57,359
but still um in this area down here and

1524
00:51:57,359 --> 00:51:58,000
you have

1525
00:51:58,000 --> 00:52:01,280
10 000 trials even with 80 arms

1526
00:52:01,280 --> 00:52:03,440
the regret rate can drop to very very

1527
00:52:03,440 --> 00:52:05,040
low so playing like

1528
00:52:05,040 --> 00:52:08,160
almost optimally even on 80 arms

1529
00:52:08,160 --> 00:52:10,880
given the trials and given a significant

1530
00:52:10,880 --> 00:52:13,040
enough difference between the

1531
00:52:13,040 --> 00:52:16,079
outcomes so that's what figure one shows

1532
00:52:16,079 --> 00:52:17,040
which is that

1533
00:52:17,040 --> 00:52:19,200
active inference can learn to reduce its

1534
00:52:19,200 --> 00:52:22,240
regret given the right parameters

1535
00:52:22,240 --> 00:52:23,920
the things that we'd expect to make the

1536
00:52:23,920 --> 00:52:26,319
situation harder like more arms

1537
00:52:26,319 --> 00:52:28,720
or less differentiability amongst the

1538
00:52:28,720 --> 00:52:30,079
arms or

1539
00:52:30,079 --> 00:52:34,079
super high um or low precision vari uh

1540
00:52:34,079 --> 00:52:36,559
rates those are the things that

1541
00:52:36,559 --> 00:52:40,160
influence active inference algorithms

1542
00:52:40,400 --> 00:52:42,400
um and also they said that they found

1543
00:52:42,400 --> 00:52:44,559
the minimal regret rate

1544
00:52:44,559 --> 00:52:47,599
um at around lambda is 0.1

1545
00:52:47,599 --> 00:52:49,839
so that's why they fixed the the regret

1546
00:52:49,839 --> 00:52:52,160
rate for the upcoming trials or for the

1547
00:52:52,160 --> 00:52:55,200
upcoming tests because there's there's a

1548
00:52:55,200 --> 00:52:55,920
bunch of

1549
00:52:55,920 --> 00:52:59,440
knobs to tweak and so they do

1550
00:52:59,440 --> 00:53:01,520
their best to do parameter sweeps and

1551
00:53:01,520 --> 00:53:03,280
show like here's for

1552
00:53:03,280 --> 00:53:05,119
you know the whole distribution of

1553
00:53:05,119 --> 00:53:06,400
lambda

1554
00:53:06,400 --> 00:53:09,280
for three different sampling regimes for

1555
00:53:09,280 --> 00:53:11,040
four different arm styles for three

1556
00:53:11,040 --> 00:53:12,400
different difficulties like already

1557
00:53:12,400 --> 00:53:14,480
the combinatorics get really high and if

1558
00:53:14,480 --> 00:53:15,760
you were going to be using this in an

1559
00:53:15,760 --> 00:53:17,200
industrial situation

1560
00:53:17,200 --> 00:53:20,319
you would optimize it with all these

1561
00:53:20,319 --> 00:53:22,400
parameters in play

1562
00:53:22,400 --> 00:53:25,680
and then um they write that they're only

1563
00:53:25,680 --> 00:53:26,559
going to consider

1564
00:53:26,559 --> 00:53:28,480
this approximate active inference

1565
00:53:28,480 --> 00:53:31,440
variant for the between agent comparison

1566
00:53:31,440 --> 00:53:33,200
because it was doing pretty well in

1567
00:53:33,200 --> 00:53:35,040
figure one so they're just gonna

1568
00:53:35,040 --> 00:53:36,880
follow up with the approximate but it's

1569
00:53:36,880 --> 00:53:38,400
something we can ask them like

1570
00:53:38,400 --> 00:53:40,160
what is the difference in computation

1571
00:53:40,160 --> 00:53:42,160
time for the general

1572
00:53:42,160 --> 00:53:44,720
or exact all right figure two so we're

1573
00:53:44,720 --> 00:53:45,359
still

1574
00:53:45,359 --> 00:53:47,520
thinking about the stationary bernoulli

1575
00:53:47,520 --> 00:53:49,119
bandit

1576
00:53:49,119 --> 00:53:51,119
here is going to be a slightly different

1577
00:53:51,119 --> 00:53:52,960
plot

1578
00:53:52,960 --> 00:53:54,960
it's a comparison of the cumulative

1579
00:53:54,960 --> 00:53:57,119
regret trajectories for the approximate

1580
00:53:57,119 --> 00:53:59,680
active inference that's aai

1581
00:53:59,680 --> 00:54:02,000
optimistic tops and sampling that's ots

1582
00:54:02,000 --> 00:54:03,200
purple

1583
00:54:03,200 --> 00:54:05,440
and bayesian upper confidence bound in

1584
00:54:05,440 --> 00:54:06,480
the teal

1585
00:54:06,480 --> 00:54:08,160
so those are the three lines and that's

1586
00:54:08,160 --> 00:54:11,200
the legend on the top left

1587
00:54:11,200 --> 00:54:13,520
and as blue just said the prior

1588
00:54:13,520 --> 00:54:15,920
precision is fixed to point one

1589
00:54:15,920 --> 00:54:18,079
which is what they found from figure one

1590
00:54:18,079 --> 00:54:19,760
like this sort of dip

1591
00:54:19,760 --> 00:54:22,319
that they all have at near point one

1592
00:54:22,319 --> 00:54:24,079
that seems to be a good

1593
00:54:24,079 --> 00:54:26,480
precision variable setting so they're

1594
00:54:26,480 --> 00:54:28,079
just going to roll with that instead of

1595
00:54:28,079 --> 00:54:30,400
also sweeping across positions

1596
00:54:30,400 --> 00:54:33,359
and here we see a similar columns and

1597
00:54:33,359 --> 00:54:33,920
rows

1598
00:54:33,920 --> 00:54:36,480
where k different number of arms in the

1599
00:54:36,480 --> 00:54:37,440
columns

1600
00:54:37,440 --> 00:54:40,160
and then different problem difficulties

1601
00:54:40,160 --> 00:54:40,720
in

1602
00:54:40,720 --> 00:54:43,520
the rows

1603
00:54:43,760 --> 00:54:47,200
and then the cumulative regret

1604
00:54:47,200 --> 00:54:50,240
so you want lower regret that

1605
00:54:50,240 --> 00:54:51,680
makes sense that's the whole thing that

1606
00:54:51,680 --> 00:54:54,079
we're training on um

1607
00:54:54,079 --> 00:54:56,480
what do we see there's a lot that can be

1608
00:54:56,480 --> 00:54:57,599
seen because there's so many

1609
00:54:57,599 --> 00:55:00,319
combinatorics of viewing at it but what

1610
00:55:00,319 --> 00:55:00,799
do we

1611
00:55:00,799 --> 00:55:03,920
see right off the bat well as the number

1612
00:55:03,920 --> 00:55:04,640
of samples

1613
00:55:04,640 --> 00:55:08,000
increases beyond say a hundred in

1614
00:55:08,000 --> 00:55:10,640
almost all cases the red line

1615
00:55:10,640 --> 00:55:12,480
approximate active inference

1616
00:55:12,480 --> 00:55:16,079
is below the other ones

1617
00:55:16,079 --> 00:55:18,559
so you're getting less regret with

1618
00:55:18,559 --> 00:55:20,000
active inference

1619
00:55:20,000 --> 00:55:22,880
on the stationary bandit for many

1620
00:55:22,880 --> 00:55:24,799
parameter combinations

1621
00:55:24,799 --> 00:55:27,599
okay but there's a few interesting

1622
00:55:27,599 --> 00:55:28,720
pieces

1623
00:55:28,720 --> 00:55:31,920
so one of them is that sometimes active

1624
00:55:31,920 --> 00:55:32,400
inference

1625
00:55:32,400 --> 00:55:34,799
early on has a higher regret so maybe

1626
00:55:34,799 --> 00:55:37,200
it's like a little bit more exploratory

1627
00:55:37,200 --> 00:55:39,040
in the beginning that's one thing to see

1628
00:55:39,040 --> 00:55:40,559
in that bottom left corner

1629
00:55:40,559 --> 00:55:41,839
there's a few other ones where it's sort

1630
00:55:41,839 --> 00:55:43,599
of like this elbow

1631
00:55:43,599 --> 00:55:46,000
early on but then there's this very

1632
00:55:46,000 --> 00:55:48,240
interesting behavior that actually shows

1633
00:55:48,240 --> 00:55:48,799
a lot

1634
00:55:48,799 --> 00:55:51,839
on the very top left so it's not always

1635
00:55:51,839 --> 00:55:53,040
the case that you can just

1636
00:55:53,040 --> 00:55:54,400
set your model to some parameter

1637
00:55:54,400 --> 00:55:56,079
combination and draw a really

1638
00:55:56,079 --> 00:55:57,520
generalized conclusion

1639
00:55:57,520 --> 00:55:59,119
but this is an awesome point by the

1640
00:55:59,119 --> 00:56:00,559
authors

1641
00:56:00,559 --> 00:56:04,559
so they're looking um

1642
00:56:04,559 --> 00:56:07,119
at an ensemble of agents so here it's a

1643
00:56:07,119 --> 00:56:07,599
thousand

1644
00:56:07,599 --> 00:56:09,599
agents like they're sort of running a

1645
00:56:09,599 --> 00:56:11,040
thousand uh

1646
00:56:11,040 --> 00:56:13,599
of each of these and then taking the

1647
00:56:13,599 --> 00:56:14,960
average which is why the lines are

1648
00:56:14,960 --> 00:56:16,079
smooth

1649
00:56:16,079 --> 00:56:18,400
and so what we see in the very top left

1650
00:56:18,400 --> 00:56:19,680
is that

1651
00:56:19,680 --> 00:56:21,359
the active inference asian's doing as

1652
00:56:21,359 --> 00:56:22,960
well as the other algorithms then it's

1653
00:56:22,960 --> 00:56:24,720
doing better right lower regret

1654
00:56:24,720 --> 00:56:27,920
under those lines but then the error

1655
00:56:27,920 --> 00:56:28,480
bound

1656
00:56:28,480 --> 00:56:30,880
increases that red shading and it starts

1657
00:56:30,880 --> 00:56:32,079
to head really far

1658
00:56:32,079 --> 00:56:34,960
up so that's pretty fascinating what's

1659
00:56:34,960 --> 00:56:36,240
happening and they write

1660
00:56:36,240 --> 00:56:37,920
this divergence is driven by a small

1661
00:56:37,920 --> 00:56:40,400
percentage of the agents in the ensemble

1662
00:56:40,400 --> 00:56:42,880
that did not find the accurate solution

1663
00:56:42,880 --> 00:56:43,680
and were over

1664
00:56:43,680 --> 00:56:45,440
confident in their estimate of the arm

1665
00:56:45,440 --> 00:56:47,680
with the highest reward probability

1666
00:56:47,680 --> 00:56:50,799
so that sort of pie chart of which arm

1667
00:56:50,799 --> 00:56:51,440
you should choose

1668
00:56:51,440 --> 00:56:54,319
it got off to a bad start and then their

1669
00:56:54,319 --> 00:56:56,160
precision was such that they just sort

1670
00:56:56,160 --> 00:56:56,640
of

1671
00:56:56,640 --> 00:56:58,799
rolled with that and they kept on seeing

1672
00:56:58,799 --> 00:57:00,559
the information they were getting within

1673
00:57:00,559 --> 00:57:01,599
that way of

1674
00:57:01,599 --> 00:57:03,839
setting up the problem and so they ended

1675
00:57:03,839 --> 00:57:04,880
up for a small

1676
00:57:04,880 --> 00:57:06,960
fraction of the agents kind of getting

1677
00:57:06,960 --> 00:57:09,440
derailed with continuing to regret the

1678
00:57:09,440 --> 00:57:11,359
decisions they were making

1679
00:57:11,359 --> 00:57:13,680
because they were locked in the

1680
00:57:13,680 --> 00:57:14,799
divergence is not

1681
00:57:14,799 --> 00:57:16,880
visible in the earlier setting easier

1682
00:57:16,880 --> 00:57:18,160
settings

1683
00:57:18,160 --> 00:57:21,359
with larger i'm not sure what variables

1684
00:57:21,359 --> 00:57:22,160
must see there

1685
00:57:22,160 --> 00:57:24,079
as one requires larger ensembles and

1686
00:57:24,079 --> 00:57:25,760
bigger number of trials to observe

1687
00:57:25,760 --> 00:57:28,240
sub-optimal instances

1688
00:57:28,240 --> 00:57:29,520
it may appear surprising that the

1689
00:57:29,520 --> 00:57:31,200
divergence is evident only for the

1690
00:57:31,200 --> 00:57:32,880
smallest number of arms considered

1691
00:57:32,880 --> 00:57:34,000
because that's supposed to be like the

1692
00:57:34,000 --> 00:57:34,640
easier

1693
00:57:34,640 --> 00:57:36,880
problem however the reason for this is

1694
00:57:36,880 --> 00:57:39,280
the smaller the number of arms is

1695
00:57:39,280 --> 00:57:41,440
the more chance an agent has to explore

1696
00:57:41,440 --> 00:57:43,599
each individual arm for a limited trial

1697
00:57:43,599 --> 00:57:44,559
number

1698
00:57:44,559 --> 00:57:46,799
so it's almost like it's easier to lock

1699
00:57:46,799 --> 00:57:48,559
in your beliefs

1700
00:57:48,559 --> 00:57:51,040
and maybe even false beliefs for a small

1701
00:57:51,040 --> 00:57:52,240
social group

1702
00:57:52,240 --> 00:57:54,640
rather than a party with k equals 80

1703
00:57:54,640 --> 00:57:56,160
people

1704
00:57:56,160 --> 00:57:58,079
it's like there's so much to learn that

1705
00:57:58,079 --> 00:57:59,680
you're less likely to get locked in

1706
00:57:59,680 --> 00:58:02,880
early but then at the smaller party

1707
00:58:02,880 --> 00:58:04,400
some of these active inference agents

1708
00:58:04,400 --> 00:58:06,319
are getting locked in

1709
00:58:06,319 --> 00:58:09,119
really early to what they think the most

1710
00:58:09,119 --> 00:58:10,880
rewarding arms are

1711
00:58:10,880 --> 00:58:13,200
and then they end up not sampling

1712
00:58:13,200 --> 00:58:14,480
efficaciously

1713
00:58:14,480 --> 00:58:17,839
so they end up implementing regrettable

1714
00:58:17,839 --> 00:58:20,880
policies so it's pretty

1715
00:58:20,880 --> 00:58:23,280
fascinating so i just want to footnote

1716
00:58:23,280 --> 00:58:24,160
here also

1717
00:58:24,160 --> 00:58:26,960
um so you're looking at this top left

1718
00:58:26,960 --> 00:58:27,520
where k

1719
00:58:27,520 --> 00:58:30,559
equals 10 and then this epsilon is 0.1

1720
00:58:30,559 --> 00:58:33,680
the top left is the most easy and then

1721
00:58:33,680 --> 00:58:35,359
the bottom right is the most difficult

1722
00:58:35,359 --> 00:58:36,319
where k

1723
00:58:36,319 --> 00:58:38,799
is the number of arms and then this

1724
00:58:38,799 --> 00:58:40,000
epsilon factor

1725
00:58:40,000 --> 00:58:42,880
is the difference between the arms like

1726
00:58:42,880 --> 00:58:43,280
so

1727
00:58:43,280 --> 00:58:44,960
it's the it's the outcome difference if

1728
00:58:44,960 --> 00:58:46,640
i go to an arm that has a reward versus

1729
00:58:46,640 --> 00:58:48,640
an arm that doesn't

1730
00:58:48,640 --> 00:58:52,240
yeah i think well point one is harder

1731
00:58:52,240 --> 00:58:54,160
because there's less of a distinction

1732
00:58:54,160 --> 00:58:55,839
between a better and a worse

1733
00:58:55,839 --> 00:58:59,760
um so it's the smaller number of arms is

1734
00:58:59,760 --> 00:59:02,839
easier but then point four is a bigger

1735
00:59:02,839 --> 00:59:05,359
contrast

1736
00:59:05,359 --> 00:59:08,000
so point one is harder than point four

1737
00:59:08,000 --> 00:59:08,480
okay

1738
00:59:08,480 --> 00:59:10,720
so then it's the bottom left that's the

1739
00:59:10,720 --> 00:59:11,680
the

1740
00:59:11,680 --> 00:59:15,040
that would be the easiest well depends

1741
00:59:15,040 --> 00:59:16,960
on what you mean easiest i mean all

1742
00:59:16,960 --> 00:59:18,240
things being equal

1743
00:59:18,240 --> 00:59:21,040
fewer arms is easier and then all things

1744
00:59:21,040 --> 00:59:22,000
being equal

1745
00:59:22,000 --> 00:59:24,319
the more contrast between good and bad

1746
00:59:24,319 --> 00:59:25,119
arms

1747
00:59:25,119 --> 00:59:27,920
the easier so got it so then the bottom

1748
00:59:27,920 --> 00:59:28,720
left would be

1749
00:59:28,720 --> 00:59:32,160
the easier yes the fewest arms and the

1750
00:59:32,160 --> 00:59:33,440
biggest contrast and

1751
00:59:33,440 --> 00:59:34,880
interestingly that's where we see the

1752
00:59:34,880 --> 00:59:37,440
biggest um elbow of active inference

1753
00:59:37,440 --> 00:59:38,640
like the biggest

1754
00:59:38,640 --> 00:59:41,680
um initial regret so it's like when the

1755
00:59:41,680 --> 00:59:43,599
game is easy and there's few arms active

1756
00:59:43,599 --> 00:59:45,280
inference stays

1757
00:59:45,280 --> 00:59:48,559
exploring a little bit longer but then

1758
00:59:48,559 --> 00:59:51,760
it ends up locking in on what's right

1759
00:59:51,760 --> 00:59:54,079
and then especially when there's not

1760
00:59:54,079 --> 00:59:56,319
that big of a difference between arms

1761
00:59:56,319 --> 00:59:59,359
all of them have this slight uptick and

1762
00:59:59,359 --> 01:00:00,400
then the variance

1763
01:00:00,400 --> 01:00:02,079
increasing with this shading that's

1764
01:00:02,079 --> 01:00:03,599
showing us that it's not like

1765
01:00:03,599 --> 01:00:05,520
all of the ensemble is behaving

1766
01:00:05,520 --> 01:00:06,720
differently but

1767
01:00:06,720 --> 01:00:09,520
perhaps again that a subset are getting

1768
01:00:09,520 --> 01:00:11,359
deranged

1769
01:00:11,359 --> 01:00:13,280
and then yes that's why i think the

1770
01:00:13,280 --> 01:00:15,040
bigger epsilon would be the

1771
01:00:15,040 --> 01:00:16,880
more difficult but we'll we'll just have

1772
01:00:16,880 --> 01:00:18,240
to save that one for the authors i would

1773
01:00:18,240 --> 01:00:19,760
think that the less contrast would be

1774
01:00:19,760 --> 01:00:21,359
easier so i guess that's

1775
01:00:21,359 --> 01:00:23,680
it's like not super clear so we'll just

1776
01:00:23,680 --> 01:00:24,480
have to ask

1777
01:00:24,480 --> 01:00:27,599
yeah so this is a cool

1778
01:00:27,599 --> 01:00:30,720
outcome and it's almost like it's a

1779
01:00:30,720 --> 01:00:34,079
qualified critique that makes a bigger

1780
01:00:34,079 --> 01:00:36,960
point about where active inference can

1781
01:00:36,960 --> 01:00:38,799
really succeed and where it might still

1782
01:00:38,799 --> 01:00:39,920
need challenges which we're going to

1783
01:00:39,920 --> 01:00:40,880
return to

1784
01:00:40,880 --> 01:00:44,000
so that's stationary bandit

1785
01:00:44,000 --> 01:00:47,119
let's go to the switching bandit the key

1786
01:00:47,119 --> 01:00:50,240
difference is that at each time step

1787
01:00:50,240 --> 01:00:53,040
any uh particular arm has a maximum

1788
01:00:53,040 --> 01:00:54,559
expected reward

1789
01:00:54,559 --> 01:00:57,839
and this reward probability is going to

1790
01:00:57,839 --> 01:01:00,480
change with a probability so there's

1791
01:01:00,480 --> 01:01:01,520
some probability

1792
01:01:01,520 --> 01:01:05,119
row um which is uh

1793
01:01:05,119 --> 01:01:09,599
is it p or is it row yeah uh with row

1794
01:01:09,599 --> 01:01:13,119
that is the probability of the reward

1795
01:01:13,119 --> 01:01:16,000
changing so now you can't just learn it

1796
01:01:16,000 --> 01:01:17,760
in any old order because things are

1797
01:01:17,760 --> 01:01:19,280
changing through time the probability of

1798
01:01:19,280 --> 01:01:22,240
the cheese is time dependent

1799
01:01:22,240 --> 01:01:25,119
section 2.2 oh yeah go ahead blue oh i

1800
01:01:25,119 --> 01:01:26,559
don't have anything no that's it so

1801
01:01:26,559 --> 01:01:28,799
you did it it's good so 2.2 is where

1802
01:01:28,799 --> 01:01:29,760
they

1803
01:01:29,760 --> 01:01:31,359
give the formalism for the switching

1804
01:01:31,359 --> 01:01:33,280
bandit and so

1805
01:01:33,280 --> 01:01:35,280
in contrast to the stationary bandit

1806
01:01:35,280 --> 01:01:37,599
problem outcomes are drawn from a time

1807
01:01:37,599 --> 01:01:39,440
dependent bernoulli probability

1808
01:01:39,440 --> 01:01:40,559
distribution

1809
01:01:40,559 --> 01:01:43,920
so that is provided and um

1810
01:01:43,920 --> 01:01:46,799
those who want to look into the details

1811
01:01:46,799 --> 01:01:47,200
can

1812
01:01:47,200 --> 01:01:49,520
do so but the key difference is that

1813
01:01:49,520 --> 01:01:50,960
there's some probability

1814
01:01:50,960 --> 01:01:53,599
that things change as you're playing the

1815
01:01:53,599 --> 01:01:54,480
game

1816
01:01:54,480 --> 01:01:57,520
so so i i did have a question here for

1817
01:01:57,520 --> 01:01:58,240
the authors

1818
01:01:58,240 --> 01:02:00,240
and uh looking forward to getting to ask

1819
01:02:00,240 --> 01:02:01,359
them so it's

1820
01:02:01,359 --> 01:02:03,920
the probability is the same and then it

1821
01:02:03,920 --> 01:02:06,160
just suddenly changes so like after

1822
01:02:06,160 --> 01:02:09,039
20 time steps or 50 time steps like it's

1823
01:02:09,039 --> 01:02:10,160
the same the same the same

1824
01:02:10,160 --> 01:02:12,720
and then it suddenly changes or it's the

1825
01:02:12,720 --> 01:02:14,720
it's changing at each time step i feel

1826
01:02:14,720 --> 01:02:15,280
like it's

1827
01:02:15,280 --> 01:02:17,359
because it says it it changes with some

1828
01:02:17,359 --> 01:02:19,599
probability but it's otherwise constant

1829
01:02:19,599 --> 01:02:21,920
so i'm curious as a constant like across

1830
01:02:21,920 --> 01:02:23,760
all of the arms is that the probability

1831
01:02:23,760 --> 01:02:25,599
that's constant or is it always changing

1832
01:02:25,599 --> 01:02:26,240
i don't know

1833
01:02:26,240 --> 01:02:28,799
good question

1834
01:02:30,400 --> 01:02:33,119
here we get to the between agent

1835
01:02:33,119 --> 01:02:35,039
comparisons of different algorithms in

1836
01:02:35,039 --> 01:02:36,799
the switching bernoulli bandit

1837
01:02:36,799 --> 01:02:38,799
with a fixed mean outcome difference so

1838
01:02:38,799 --> 01:02:40,799
epsilon equals 25.

1839
01:02:40,799 --> 01:02:44,000
so they're setting epsilon fixed so now

1840
01:02:44,000 --> 01:02:45,920
it's going to be a different row and

1841
01:02:45,920 --> 01:02:48,160
column situation

1842
01:02:48,160 --> 01:02:51,599
so in this figure the three columns are

1843
01:02:51,599 --> 01:02:55,039
row equals 0.01 02 and o4

1844
01:02:55,039 --> 01:02:56,799
so that's the probability of changing so

1845
01:02:56,799 --> 01:02:58,559
here on the left column

1846
01:02:58,559 --> 01:03:00,240
it's a dynamic environment with one

1847
01:03:00,240 --> 01:03:02,319
percent of time steps there's a change

1848
01:03:02,319 --> 01:03:04,000
here it's on the right side it's up to

1849
01:03:04,000 --> 01:03:05,680
four percent changing

1850
01:03:05,680 --> 01:03:08,960
so all things being equal the more

1851
01:03:08,960 --> 01:03:11,440
change the harder it's gonna be because

1852
01:03:11,440 --> 01:03:12,480
it's gonna be like

1853
01:03:12,480 --> 01:03:15,520
changing um faster and then the less

1854
01:03:15,520 --> 01:03:17,280
change it's more like the static

1855
01:03:17,280 --> 01:03:20,559
case and then the rows now are the arms

1856
01:03:20,559 --> 01:03:21,119
so k

1857
01:03:21,119 --> 01:03:24,880
10 20 and 40. so all things be equal

1858
01:03:24,880 --> 01:03:27,039
it's easier when there's fewer arms

1859
01:03:27,039 --> 01:03:30,160
because there's fewer decisions to make

1860
01:03:30,160 --> 01:03:32,079
and then we're going to be looking

1861
01:03:32,079 --> 01:03:33,760
within each cell

1862
01:03:33,760 --> 01:03:37,359
as the regret rate

1863
01:03:37,359 --> 01:03:40,880
this r sub t with a tilde regret rate

1864
01:03:40,880 --> 01:03:43,039
through time where again you want to

1865
01:03:43,039 --> 01:03:45,440
have less regret so lower is better

1866
01:03:45,440 --> 01:03:47,920
as a function of the samples so up to

1867
01:03:47,920 --> 01:03:48,880
several thousand

1868
01:03:48,880 --> 01:03:51,520
samples and then here are the algorithms

1869
01:03:51,520 --> 01:03:52,640
that are being compared

1870
01:03:52,640 --> 01:03:55,119
active inference in red bayesian upper

1871
01:03:55,119 --> 01:03:57,200
confidence in blue

1872
01:03:57,200 --> 01:03:59,920
optimistic thompson in purple and then

1873
01:03:59,920 --> 01:04:02,480
random control in the black dashed line

1874
01:04:02,480 --> 01:04:06,000
so the random play is um as is a good

1875
01:04:06,000 --> 01:04:09,200
it's um it's not the uh adversarial play

1876
01:04:09,200 --> 01:04:10,799
so it's not like you're you're choosing

1877
01:04:10,799 --> 01:04:13,119
to lose but this is the random play

1878
01:04:13,119 --> 01:04:14,880
and what we're seeing is that it

1879
01:04:14,880 --> 01:04:18,000
accumulates a static amount of regret

1880
01:04:18,000 --> 01:04:21,760
rate it kind of starts the same value

1881
01:04:21,760 --> 01:04:25,200
the purple and the teal algorithms

1882
01:04:25,200 --> 01:04:27,839
they also converge upon a regret rate

1883
01:04:27,839 --> 01:04:28,240
that's

1884
01:04:28,240 --> 01:04:30,799
lower than the random play so they're

1885
01:04:30,799 --> 01:04:32,960
playing better than random

1886
01:04:32,960 --> 01:04:36,400
but we see in essentially all cases

1887
01:04:36,400 --> 01:04:39,119
the active inference is lower than these

1888
01:04:39,119 --> 01:04:39,440
other

1889
01:04:39,440 --> 01:04:41,920
algorithms so when we say like oh active

1890
01:04:41,920 --> 01:04:43,839
inference has better performance

1891
01:04:43,839 --> 01:04:47,599
on this task than some other algorithm

1892
01:04:47,599 --> 01:04:50,559
this is kind of what it looks like this

1893
01:04:50,559 --> 01:04:50,960
is

1894
01:04:50,960 --> 01:04:54,079
saying that as time goes on octave

1895
01:04:54,079 --> 01:04:54,559
inference

1896
01:04:54,559 --> 01:04:57,200
is acquiring regret at a lower rate than

1897
01:04:57,200 --> 01:04:58,640
the other algorithms it's just

1898
01:04:58,640 --> 01:05:00,400
performing better it's implementing

1899
01:05:00,400 --> 01:05:00,799
better

1900
01:05:00,799 --> 01:05:04,880
policy then we can see that for the

1901
01:05:04,880 --> 01:05:08,240
less less changing and fewer arms

1902
01:05:08,240 --> 01:05:11,440
on the top left so that's like easier

1903
01:05:11,440 --> 01:05:14,079
we see that the the difference between

1904
01:05:14,079 --> 01:05:15,599
random play and the algorithm

1905
01:05:15,599 --> 01:05:19,200
is significant and active inference does

1906
01:05:19,200 --> 01:05:19,520
like

1907
01:05:19,520 --> 01:05:22,559
super well whereas when you have a very

1908
01:05:22,559 --> 01:05:23,760
dynamic environment

1909
01:05:23,760 --> 01:05:27,520
and more arms you don't get

1910
01:05:27,520 --> 01:05:29,599
that much of a regret you're still doing

1911
01:05:29,599 --> 01:05:31,119
relatively better and still active

1912
01:05:31,119 --> 01:05:32,480
inference outperforms the other

1913
01:05:32,480 --> 01:05:33,440
algorithms

1914
01:05:33,440 --> 01:05:34,960
but it's like imagine if it was changing

1915
01:05:34,960 --> 01:05:36,640
every few time steps and there's 100

1916
01:05:36,640 --> 01:05:37,520
arms

1917
01:05:37,520 --> 01:05:40,000
you would just you you would never be

1918
01:05:40,000 --> 01:05:40,880
able to sample

1919
01:05:40,880 --> 01:05:43,599
enough to make a meaningful update to

1920
01:05:43,599 --> 01:05:44,640
your model

1921
01:05:44,640 --> 01:05:47,440
so that's why the faster things change

1922
01:05:47,440 --> 01:05:49,119
and the more arms there are

1923
01:05:49,119 --> 01:05:51,359
the more the overall performance of any

1924
01:05:51,359 --> 01:05:52,720
machine learning algorithm is going to

1925
01:05:52,720 --> 01:05:54,880
converge to random play

1926
01:05:54,880 --> 01:05:57,440
whereas the more static the problem is

1927
01:05:57,440 --> 01:05:59,839
and then the fewer options there are

1928
01:05:59,839 --> 01:06:03,200
then better and better strategies

1929
01:06:03,200 --> 01:06:04,720
are going to perform better and better

1930
01:06:04,720 --> 01:06:06,480
relative to random

1931
01:06:06,480 --> 01:06:09,359
so this is just pretty cool that they

1932
01:06:09,359 --> 01:06:11,359
they do simulations they do

1933
01:06:11,359 --> 01:06:14,480
um i guess a thousand simulations

1934
01:06:14,480 --> 01:06:16,000
and then the switching schedule is

1935
01:06:16,000 --> 01:06:17,760
generated randomly for each agent

1936
01:06:17,760 --> 01:06:19,599
instance within the ensemble

1937
01:06:19,599 --> 01:06:23,359
so they do a full simulation a thousand

1938
01:06:23,359 --> 01:06:25,039
times for each of these cases and then

1939
01:06:25,039 --> 01:06:26,400
average it out

1940
01:06:26,400 --> 01:06:29,839
and yeah well done active inference

1941
01:06:29,839 --> 01:06:32,720
pretty cool to see that it is doing

1942
01:06:32,720 --> 01:06:33,200
better

1943
01:06:33,200 --> 01:06:35,440
on the switching bernoulli bandit for a

1944
01:06:35,440 --> 01:06:36,880
wide range

1945
01:06:36,880 --> 01:06:40,640
of situations and it sort of it rapidly

1946
01:06:40,640 --> 01:06:41,599
figures out

1947
01:06:41,599 --> 01:06:44,400
a good way to work and then it stays at

1948
01:06:44,400 --> 01:06:48,559
that low rate of regret accumulation

1949
01:06:48,640 --> 01:06:52,160
any thoughts on three super cool

1950
01:06:52,160 --> 01:06:55,200
yep now um in

1951
01:06:55,200 --> 01:06:59,599
four we're gonna fix the number of arms

1952
01:06:59,599 --> 01:07:02,000
so now we're at k equals 20 arms in the

1953
01:07:02,000 --> 01:07:03,680
switching bernoulli bandit

1954
01:07:03,680 --> 01:07:04,880
and then we're going to be comparing

1955
01:07:04,880 --> 01:07:07,200
these algorithms again so here

1956
01:07:07,200 --> 01:07:09,440
the columns are just as they were before

1957
01:07:09,440 --> 01:07:12,400
with less changing row equals 0.01 o2

1958
01:07:12,400 --> 01:07:14,000
and o4 on the right

1959
01:07:14,000 --> 01:07:16,480
but now as we kind of saw on previous

1960
01:07:16,480 --> 01:07:18,640
figures we have epsilon

1961
01:07:18,640 --> 01:07:21,359
which is that differential between the

1962
01:07:21,359 --> 01:07:23,680
rewarding and less rewarding arms

1963
01:07:23,680 --> 01:07:28,960
as the rows so we uh we can think about

1964
01:07:28,960 --> 01:07:30,480
the less changing

1965
01:07:30,480 --> 01:07:33,599
and the most contrast between rows

1966
01:07:33,599 --> 01:07:37,039
is where you see the most

1967
01:07:37,039 --> 01:07:40,319
um gain relative to

1968
01:07:40,319 --> 01:07:43,359
random play whereas when you're in the

1969
01:07:43,359 --> 01:07:46,240
more dynamic setting and when there's

1970
01:07:46,240 --> 01:07:47,920
less differences between which choice

1971
01:07:47,920 --> 01:07:50,000
you make in terms of the outcome

1972
01:07:50,000 --> 01:07:53,280
then there's less of a regret rate

1973
01:07:53,280 --> 01:07:54,799
differential with the algorithms

1974
01:07:54,799 --> 01:07:57,920
but again broadly across the board

1975
01:07:57,920 --> 01:08:00,000
active inference is performing better

1976
01:08:00,000 --> 01:08:01,920
than these other algorithms

1977
01:08:01,920 --> 01:08:05,359
so as sampling increases

1978
01:08:05,359 --> 01:08:07,200
active inference locks into a pretty

1979
01:08:07,200 --> 01:08:09,039
good spot by around

1980
01:08:09,039 --> 01:08:12,160
200 or 500 samples

1981
01:08:12,160 --> 01:08:14,720
and this is for the 20 arms so it's kind

1982
01:08:14,720 --> 01:08:15,440
of like

1983
01:08:15,440 --> 01:08:17,600
it's visited each arm probably a few

1984
01:08:17,600 --> 01:08:18,479
times

1985
01:08:18,479 --> 01:08:20,719
maybe some more than others but again

1986
01:08:20,719 --> 01:08:22,479
they're always changing

1987
01:08:22,479 --> 01:08:26,158
and so active inference is able to

1988
01:08:26,158 --> 01:08:28,158
cope with that and also it's interesting

1989
01:08:28,158 --> 01:08:29,759
that especially this teal

1990
01:08:29,759 --> 01:08:33,839
one it almost has a lower

1991
01:08:33,839 --> 01:08:36,839
regret rate and then it actually creeps

1992
01:08:36,839 --> 01:08:38,319
up

1993
01:08:38,319 --> 01:08:40,880
whereas at least just visually we're

1994
01:08:40,880 --> 01:08:43,120
seeing active inference kind of flatline

1995
01:08:43,120 --> 01:08:45,439
but not creep back up and so that's

1996
01:08:45,439 --> 01:08:46,719
something that can happen

1997
01:08:46,719 --> 01:08:48,158
with all kinds of algorithms that

1998
01:08:48,158 --> 01:08:49,759
they'll get locked into an aberrant

1999
01:08:49,759 --> 01:08:50,839
precision

2000
01:08:50,839 --> 01:08:53,520
regime and

2001
01:08:53,520 --> 01:08:57,520
um so there's just another way to slice

2002
01:08:57,520 --> 01:08:59,040
it in figure three

2003
01:08:59,040 --> 01:09:01,759
they fixed the epsilon the difference

2004
01:09:01,759 --> 01:09:02,799
between

2005
01:09:02,799 --> 01:09:05,439
the more and less rewarding arms and

2006
01:09:05,439 --> 01:09:06,880
then they explored how the number of

2007
01:09:06,880 --> 01:09:08,719
arms was associated with how dynamic of

2008
01:09:08,719 --> 01:09:10,319
a problem was being solved

2009
01:09:10,319 --> 01:09:13,520
in figure four we fixed arms

2010
01:09:13,520 --> 01:09:15,759
so that we could explore how rho the

2011
01:09:15,759 --> 01:09:16,880
dynamic

2012
01:09:16,880 --> 01:09:19,679
changing probability and epsilon the

2013
01:09:19,679 --> 01:09:21,040
differential

2014
01:09:21,040 --> 01:09:24,799
how those change performance

2015
01:09:24,799 --> 01:09:27,920
and then um

2016
01:09:28,399 --> 01:09:31,120
okay good

2017
01:09:32,158 --> 01:09:36,238
now um okay figure five

2018
01:09:36,238 --> 01:09:39,439
here it's uh yet another similar looking

2019
01:09:39,439 --> 01:09:40,719
figure

2020
01:09:40,719 --> 01:09:44,319
we're going to have similarly the row

2021
01:09:44,319 --> 01:09:46,560
for the column so less changing and left

2022
01:09:46,560 --> 01:09:48,640
column more changing on the right column

2023
01:09:48,640 --> 01:09:49,759
and we're going to have the number of

2024
01:09:49,759 --> 01:09:52,238
arms just like figure three with 10

2025
01:09:52,238 --> 01:09:55,159
20 and 40 but there's going to be a

2026
01:09:55,159 --> 01:09:58,080
non-stationary difficulty

2027
01:09:58,080 --> 01:10:02,480
and the difficulty of the problem is

2028
01:10:02,480 --> 01:10:04,239
non-stationary as the advantage of the

2029
01:10:04,239 --> 01:10:05,760
best arm over the second best arm

2030
01:10:05,760 --> 01:10:07,600
changes with time

2031
01:10:07,600 --> 01:10:11,120
ah yeah so epsilon is varied

2032
01:10:11,120 --> 01:10:14,159
yes exactly so switching changing

2033
01:10:14,159 --> 01:10:15,360
epsilon to time

2034
01:10:15,360 --> 01:10:17,760
and they fix the precision over

2035
01:10:17,760 --> 01:10:18,960
preferences to lambda

2036
01:10:18,960 --> 01:10:22,159
equals 0.5 so that's a different lambda

2037
01:10:22,159 --> 01:10:22,880
than they used

2038
01:10:22,880 --> 01:10:25,440
elsewhere and this is just sort of

2039
01:10:25,440 --> 01:10:26,080
showing

2040
01:10:26,080 --> 01:10:28,880
i mean the the variable values they

2041
01:10:28,880 --> 01:10:29,280
choose

2042
01:10:29,280 --> 01:10:30,800
clearly work because they're

2043
01:10:30,800 --> 01:10:32,400
outperforming the state of the art but

2044
01:10:32,400 --> 01:10:34,159
here they chose a different variable

2045
01:10:34,159 --> 01:10:37,040
yeah blue the lambda of 0.5 they used in

2046
01:10:37,040 --> 01:10:37,440
all

2047
01:10:37,440 --> 01:10:40,320
of the switching graphs oh yes figure 3

2048
01:10:40,320 --> 01:10:41,679
figure 4 and figure 5.

2049
01:10:41,679 --> 01:10:43,679
and they changed it to 0.5 in the

2050
01:10:43,679 --> 01:10:45,360
switching because they just optimized

2051
01:10:45,360 --> 01:10:46,400
for that but didn't show

2052
01:10:46,400 --> 01:10:48,800
that variable optimization okay thank

2053
01:10:48,800 --> 01:10:50,000
you

2054
01:10:50,000 --> 01:10:53,199
and so here we can see that

2055
01:10:53,199 --> 01:10:55,440
um yep active inference the red line is

2056
01:10:55,440 --> 01:10:56,480
on bottom

2057
01:10:56,480 --> 01:10:58,640
and it doesn't have this sort of

2058
01:10:58,640 --> 01:10:59,520
secondary

2059
01:10:59,520 --> 01:11:02,800
increase in regret rate so for even

2060
01:11:02,800 --> 01:11:06,560
dynamic on the right and many arms

2061
01:11:06,560 --> 01:11:08,640
on the bottom right we're seeing active

2062
01:11:08,640 --> 01:11:09,840
inference perform

2063
01:11:09,840 --> 01:11:13,120
well so figure three four and five

2064
01:11:13,120 --> 01:11:16,080
really make a super compelling case for

2065
01:11:16,080 --> 01:11:17,600
active inference outperforming

2066
01:11:17,600 --> 01:11:20,800
state-of-the-art algorithms nice

2067
01:11:20,800 --> 01:11:22,640
work and that's something that we're

2068
01:11:22,640 --> 01:11:24,719
really excited about

2069
01:11:24,719 --> 01:11:26,400
let's go to the discussion and just

2070
01:11:26,400 --> 01:11:27,760
spend a couple minutes

2071
01:11:27,760 --> 01:11:30,640
on the discussion and the closing notes

2072
01:11:30,640 --> 01:11:31,440
and then we'll

2073
01:11:31,440 --> 01:11:34,640
look forward to the dot one and dot two

2074
01:11:34,640 --> 01:11:37,280
they open the discussion by uh

2075
01:11:37,280 --> 01:11:38,000
rehearsing

2076
01:11:38,000 --> 01:11:40,719
what they have previously mentioned that

2077
01:11:40,719 --> 01:11:42,480
they're comparing active inference to

2078
01:11:42,480 --> 01:11:43,920
two state-of-the-art machine learning

2079
01:11:43,920 --> 01:11:45,920
algorithms the bayesian upper confidence

2080
01:11:45,920 --> 01:11:46,320
bound

2081
01:11:46,320 --> 01:11:48,400
and the optimistic thompson sampling

2082
01:11:48,400 --> 01:11:49,760
that's the pair of algorithms

2083
01:11:49,760 --> 01:11:52,159
in the pair of problems the stationary

2084
01:11:52,159 --> 01:11:53,679
and the non-stationary

2085
01:11:53,679 --> 01:11:56,400
stochastic multi-armed bandits their

2086
01:11:56,400 --> 01:11:57,440
contribution

2087
01:11:57,440 --> 01:11:59,280
among other things was the introduction

2088
01:11:59,280 --> 01:12:00,960
of the approximate active inference

2089
01:12:00,960 --> 01:12:02,159
algorithm

2090
01:12:02,159 --> 01:12:04,239
and then they perform some checks to

2091
01:12:04,239 --> 01:12:05,440
show that it's performing

2092
01:12:05,440 --> 01:12:08,960
again as well as you could do exactly

2093
01:12:08,960 --> 01:12:10,880
so they derived an active inference

2094
01:12:10,880 --> 01:12:12,800
algorithm that's efficient and easily

2095
01:12:12,800 --> 01:12:15,280
scalable to high dimensional problems

2096
01:12:15,280 --> 01:12:17,920
leading us to ask of course where could

2097
01:12:17,920 --> 01:12:18,800
it be cool

2098
01:12:18,800 --> 01:12:21,600
or useful or important to apply active

2099
01:12:21,600 --> 01:12:22,800
inference

2100
01:12:22,800 --> 01:12:25,280
and we know that there's many thoughts

2101
01:12:25,280 --> 01:12:26,159
on this

2102
01:12:26,159 --> 01:12:27,920
and hopefully will be many more but

2103
01:12:27,920 --> 01:12:29,920
we'll just note for those who are

2104
01:12:29,920 --> 01:12:32,880
listening at the right time that there's

2105
01:12:32,880 --> 01:12:34,719
been recently announced a net hack

2106
01:12:34,719 --> 01:12:37,600
challenge and we're going to be starting

2107
01:12:37,600 --> 01:12:38,400
work on this

2108
01:12:38,400 --> 01:12:40,480
in the end of july or the beginning of

2109
01:12:40,480 --> 01:12:42,320
august 2021

2110
01:12:42,320 --> 01:12:44,719
so if you're interested in applying

2111
01:12:44,719 --> 01:12:46,080
active inference to

2112
01:12:46,080 --> 01:12:49,280
a video game performance challenge we

2113
01:12:49,280 --> 01:12:52,239
hope that having a collaborative team

2114
01:12:52,239 --> 01:12:54,080
make an entry for this challenge

2115
01:12:54,080 --> 01:12:56,560
will be an awesome opportunity to really

2116
01:12:56,560 --> 01:12:58,320
demonstrate to the world what active

2117
01:12:58,320 --> 01:12:59,600
inference can do

2118
01:12:59,600 --> 01:13:01,440
but we'll also look forward to the

2119
01:13:01,440 --> 01:13:02,840
authors and all

2120
01:13:02,840 --> 01:13:06,080
guests sharing what kinds of questions

2121
01:13:06,080 --> 01:13:07,760
they think might be interesting to

2122
01:13:07,760 --> 01:13:09,120
explore using

2123
01:13:09,120 --> 01:13:10,560
approximate active inference type

2124
01:13:10,560 --> 01:13:13,520
algorithms like this

2125
01:13:16,159 --> 01:13:18,880
here's a really nice point that speaks

2126
01:13:18,880 --> 01:13:19,600
to

2127
01:13:19,600 --> 01:13:22,719
that um again in figure two

2128
01:13:22,719 --> 01:13:26,159
where we saw that sort of runaway regret

2129
01:13:26,159 --> 01:13:29,679
on a a small fraction of the agents

2130
01:13:29,679 --> 01:13:31,199
that were just getting locked into

2131
01:13:31,199 --> 01:13:33,440
extremely regrettable decisions

2132
01:13:33,440 --> 01:13:36,880
in that k equals 10 case

2133
01:13:36,880 --> 01:13:40,560
so what what do they say about that

2134
01:13:40,560 --> 01:13:42,560
to our surprise the empirical algorithm

2135
01:13:42,560 --> 01:13:44,239
comparison in the stationary bandit

2136
01:13:44,239 --> 01:13:45,679
problem figure 2

2137
01:13:45,679 --> 01:13:47,040
showed that the active inference

2138
01:13:47,040 --> 01:13:48,880
algorithm is not asymptotically

2139
01:13:48,880 --> 01:13:49,920
efficient

2140
01:13:49,920 --> 01:13:51,920
the cumulative regret increased faster

2141
01:13:51,920 --> 01:13:53,280
than logarithmic

2142
01:13:53,280 --> 01:13:56,480
in the limit of large trials so in other

2143
01:13:56,480 --> 01:13:58,000
words even though it's not

2144
01:13:58,000 --> 01:14:00,320
all agents that get deranged the ones

2145
01:14:00,320 --> 01:14:01,040
that do

2146
01:14:01,040 --> 01:14:03,440
are dragging down the group in a way

2147
01:14:03,440 --> 01:14:04,400
that's

2148
01:14:04,400 --> 01:14:08,480
damaging this cause for the behavior

2149
01:14:08,480 --> 01:14:10,320
seems to be the fixed prior precision

2150
01:14:10,320 --> 01:14:12,080
over preferences lambda

2151
01:14:12,080 --> 01:14:13,679
which acts as a balancing parameter

2152
01:14:13,679 --> 01:14:16,320
between exploration and exploitation

2153
01:14:16,320 --> 01:14:19,120
so this is a pretty interesting piece

2154
01:14:19,120 --> 01:14:19,920
instead of like

2155
01:14:19,920 --> 01:14:22,480
two modes explore and exploit and then

2156
01:14:22,480 --> 01:14:24,080
we're gonna have a parameter that flips

2157
01:14:24,080 --> 01:14:25,520
us one way to the other like a light

2158
01:14:25,520 --> 01:14:26,320
switch

2159
01:14:26,320 --> 01:14:28,560
or we have one extreme explore mode and

2160
01:14:28,560 --> 01:14:30,800
one extreme exploit mode in the model

2161
01:14:30,800 --> 01:14:32,080
and then we're going to have a dimmer

2162
01:14:32,080 --> 01:14:34,239
that goes between the two in active

2163
01:14:34,239 --> 01:14:35,360
inference

2164
01:14:35,360 --> 01:14:38,480
the knob that kind of does that is

2165
01:14:38,480 --> 01:14:40,080
actually the prior precision

2166
01:14:40,080 --> 01:14:43,760
over preferences so if you have no

2167
01:14:43,760 --> 01:14:47,600
if you have no preferences aka you have

2168
01:14:47,600 --> 01:14:50,080
no precision over preferences you're

2169
01:14:50,080 --> 01:14:51,679
going to act in the maximally

2170
01:14:51,679 --> 01:14:53,600
exploratory way

2171
01:14:53,600 --> 01:14:55,600
if you have an incredibly strong

2172
01:14:55,600 --> 01:14:57,920
incredibly precise prior

2173
01:14:57,920 --> 01:15:01,199
over your preferences then you're

2174
01:15:01,199 --> 01:15:03,840
going to act in a very exploitative way

2175
01:15:03,840 --> 01:15:05,520
like if there's two restaurants

2176
01:15:05,520 --> 01:15:08,239
and one of them you like you know 60 the

2177
01:15:08,239 --> 01:15:10,159
other one you like 40

2178
01:15:10,159 --> 01:15:11,440
if you have no preference you're going

2179
01:15:11,440 --> 01:15:13,199
to go to them 50 50. that's like

2180
01:15:13,199 --> 01:15:15,360
explore but then if you have a super

2181
01:15:15,360 --> 01:15:17,360
high precision over your preferences

2182
01:15:17,360 --> 01:15:18,960
you're always going to go to the one you

2183
01:15:18,960 --> 01:15:19,280
that

2184
01:15:19,280 --> 01:15:21,199
you prefer even if it's only slightly

2185
01:15:21,199 --> 01:15:22,800
preferred

2186
01:15:22,800 --> 01:15:24,960
an analysis of how performance of the

2187
01:15:24,960 --> 01:15:27,120
algorithm depends on that parameter

2188
01:15:27,120 --> 01:15:28,800
showed that parameter values that give

2189
01:15:28,800 --> 01:15:31,600
the best performance decrease over time

2190
01:15:31,600 --> 01:15:33,199
suggesting that this parameter should be

2191
01:15:33,199 --> 01:15:35,280
adaptive and decay over time as the need

2192
01:15:35,280 --> 01:15:37,520
for exploration decreases

2193
01:15:37,520 --> 01:15:40,320
so start with low precision over your

2194
01:15:40,320 --> 01:15:41,440
preferences

2195
01:15:41,440 --> 01:15:44,640
and then you learn

2196
01:15:44,640 --> 01:15:46,800
and you update so that eventually you

2197
01:15:46,800 --> 01:15:49,040
increase your precision later

2198
01:15:49,040 --> 01:15:50,800
attempts to remedy the situation with a

2199
01:15:50,800 --> 01:15:52,480
simple and widely used decay scheme

2200
01:15:52,480 --> 01:15:53,760
we're not successful

2201
01:15:53,760 --> 01:15:55,679
so the logarithm of time so you just

2202
01:15:55,679 --> 01:15:57,600
make the precision

2203
01:15:57,600 --> 01:16:00,080
directly a function of time like you

2204
01:16:00,080 --> 01:16:00,960
know one

2205
01:16:00,960 --> 01:16:02,880
over the number of trials just that kind

2206
01:16:02,880 --> 01:16:04,159
of thing it's like that's a

2207
01:16:04,159 --> 01:16:05,600
function that scales with the number of

2208
01:16:05,600 --> 01:16:07,280
trials so here

2209
01:16:07,280 --> 01:16:10,400
you scale it with the logarithm of time

2210
01:16:10,400 --> 01:16:12,000
this indicates that it's not a simple

2211
01:16:12,000 --> 01:16:13,679
relationship and a proper theoretical

2212
01:16:13,679 --> 01:16:15,360
analysis will be needed to identify

2213
01:16:15,360 --> 01:16:17,760
whether a scheme exists

2214
01:16:17,760 --> 01:16:20,239
so that's pretty interesting like how

2215
01:16:20,239 --> 01:16:20,880
are we gonna

2216
01:16:20,880 --> 01:16:23,440
pull back a level okay we think that

2217
01:16:23,440 --> 01:16:25,520
active inference architecture

2218
01:16:25,520 --> 01:16:28,000
is gonna be a really productive way to

2219
01:16:28,000 --> 01:16:30,239
model learning under uncertainty

2220
01:16:30,239 --> 01:16:32,080
but we're also just pushing the problem

2221
01:16:32,080 --> 01:16:33,440
up a level which is

2222
01:16:33,440 --> 01:16:36,960
okay how uncertain should we be and then

2223
01:16:36,960 --> 01:16:39,360
how should we change that uncertainty

2224
01:16:39,360 --> 01:16:40,800
over preferences moving

2225
01:16:40,800 --> 01:16:45,360
through time so nice point there

2226
01:16:45,360 --> 01:16:48,640
and then um one

2227
01:16:48,640 --> 01:16:52,239
last thought is in the non-stationary

2228
01:16:52,239 --> 01:16:54,400
so the dynamic switching bandit the

2229
01:16:54,400 --> 01:16:57,120
active inference algorithm generally

2230
01:16:57,120 --> 01:16:58,560
outperformed the bayesian upper

2231
01:16:58,560 --> 01:17:00,960
confidence bound and the optimistic

2232
01:17:00,960 --> 01:17:02,400
thompson sampling

2233
01:17:02,400 --> 01:17:04,159
this provides evidence that the active

2234
01:17:04,159 --> 01:17:06,000
inference framework may provide a good

2235
01:17:06,000 --> 01:17:08,159
solution for optimization problems that

2236
01:17:08,159 --> 01:17:11,040
require continuous adaptation

2237
01:17:11,040 --> 01:17:13,040
active inference provides the most

2238
01:17:13,040 --> 01:17:14,159
efficient way

2239
01:17:14,159 --> 01:17:16,880
of gaining information and this property

2240
01:17:16,880 --> 01:17:17,840
of the algorithm

2241
01:17:17,840 --> 01:17:21,679
pays off in the non-stationary setting

2242
01:17:21,679 --> 01:17:23,440
cool points that it'll be awesome to

2243
01:17:23,440 --> 01:17:25,760
hear the author's perspective on

2244
01:17:25,760 --> 01:17:28,239
so how do we compare active inference

2245
01:17:28,239 --> 01:17:29,360
framework algorithms

2246
01:17:29,360 --> 01:17:32,560
to machine learning more broadly

2247
01:17:32,560 --> 01:17:35,520
and then especially for problems where

2248
01:17:35,520 --> 01:17:36,960
informed

2249
01:17:36,960 --> 01:17:40,000
foraging is helpful like

2250
01:17:40,000 --> 01:17:42,960
not that epsilon greedy just sometimes

2251
01:17:42,960 --> 01:17:44,159
look away from the best and pick

2252
01:17:44,159 --> 01:17:45,679
randomly

2253
01:17:45,679 --> 01:17:49,360
and not even just the thompson sampling

2254
01:17:49,360 --> 01:17:51,199
remember pick each arm according to how

2255
01:17:51,199 --> 01:17:53,600
likely it is to be the best arm

2256
01:17:53,600 --> 01:17:56,480
we could have an even more informed or

2257
01:17:56,480 --> 01:17:57,360
empirically

2258
01:17:57,360 --> 01:18:00,480
better performing scheme for doing the

2259
01:18:00,480 --> 01:18:01,440
trade-off

2260
01:18:01,440 --> 01:18:03,440
for earning while learning you know

2261
01:18:03,440 --> 01:18:04,880
earning while learning doesn't mean that

2262
01:18:04,880 --> 01:18:05,360
you're

2263
01:18:05,360 --> 01:18:08,320
learning optimally or earning optimally

2264
01:18:08,320 --> 01:18:08,880
or

2265
01:18:08,880 --> 01:18:12,000
how are we going to balance that out and

2266
01:18:12,000 --> 01:18:13,440
it seems like octave inference is going

2267
01:18:13,440 --> 01:18:15,679
to be a strong contender

2268
01:18:15,679 --> 01:18:18,239
in the coming years as more and more

2269
01:18:18,239 --> 01:18:19,840
people in the machine learning community

2270
01:18:19,840 --> 01:18:20,719
start to

2271
01:18:20,719 --> 01:18:22,560
get keyed into these results tune the

2272
01:18:22,560 --> 01:18:23,760
regime of attention

2273
01:18:23,760 --> 01:18:25,280
to what these authors and others are

2274
01:18:25,280 --> 01:18:27,280
doing and it starts to become

2275
01:18:27,280 --> 01:18:30,480
more broadly understood that hey rather

2276
01:18:30,480 --> 01:18:31,280
than just

2277
01:18:31,280 --> 01:18:33,040
having more and more parameters and

2278
01:18:33,040 --> 01:18:34,800
billions and billions of parameters in

2279
01:18:34,800 --> 01:18:35,679
our model

2280
01:18:35,679 --> 01:18:37,520
and training in on larger and larger

2281
01:18:37,520 --> 01:18:39,440
networks of you know computers

2282
01:18:39,440 --> 01:18:42,320
what if we just had a curious agent who

2283
01:18:42,320 --> 01:18:44,800
learns how to learn and prefers to win

2284
01:18:44,800 --> 01:18:47,159
it's sort of like a ground floor

2285
01:18:47,159 --> 01:18:48,320
reinterpretation

2286
01:18:48,320 --> 01:18:51,360
of these problems and i think hearing

2287
01:18:51,360 --> 01:18:52,400
the author's

2288
01:18:52,400 --> 01:18:55,840
views on where the active inference and

2289
01:18:55,840 --> 01:18:56,880
machine learning

2290
01:18:56,880 --> 01:18:58,800
communities and fields are heading will

2291
01:18:58,800 --> 01:19:00,080
be

2292
01:19:00,080 --> 01:19:02,640
for sure informative for us what do you

2293
01:19:02,640 --> 01:19:05,120
think about that blue

2294
01:19:05,120 --> 01:19:07,199
i just i'm thinking about problems like

2295
01:19:07,199 --> 01:19:09,199
um like the mountain car problem

2296
01:19:09,199 --> 01:19:12,159
like we're actually learning is winning

2297
01:19:12,159 --> 01:19:13,520
right like when you have to

2298
01:19:13,520 --> 01:19:15,600
if you have to explore you want to go up

2299
01:19:15,600 --> 01:19:17,520
the higher you want to go up the hill

2300
01:19:17,520 --> 01:19:19,440
so that you can get back up to the other

2301
01:19:19,440 --> 01:19:20,960
side of the hill and the flag

2302
01:19:20,960 --> 01:19:23,120
so like what other problems are out

2303
01:19:23,120 --> 01:19:24,320
there like that that

2304
01:19:24,320 --> 01:19:27,360
doesn't don't necessarily that have like

2305
01:19:27,360 --> 01:19:29,199
a reward that's uncoupled to

2306
01:19:29,199 --> 01:19:32,560
ex uncoupled from exploration so i i'm

2307
01:19:32,560 --> 01:19:34,159
thinking about that and i don't know

2308
01:19:34,159 --> 01:19:36,800
just where is where is learning winning

2309
01:19:36,800 --> 01:19:37,760
great

2310
01:19:37,760 --> 01:19:39,120
question it's almost like in the

2311
01:19:39,120 --> 01:19:41,280
mountain car the altitude

2312
01:19:41,280 --> 01:19:43,840
is in some senses a reward heuristic you

2313
01:19:43,840 --> 01:19:45,600
want to get to the top of a hill so

2314
01:19:45,600 --> 01:19:46,960
it seems like altitude would be the way

2315
01:19:46,960 --> 01:19:48,880
to go but then the way that the problem

2316
01:19:48,880 --> 01:19:50,239
is set up

2317
01:19:50,239 --> 01:19:52,960
you could also optimize the bounds and

2318
01:19:52,960 --> 01:19:53,679
say i

2319
01:19:53,679 --> 01:19:56,800
seek to be expanding my bounds laterally

2320
01:19:56,800 --> 01:19:57,520
more

2321
01:19:57,520 --> 01:19:59,280
oh yeah well of course we're changing in

2322
01:19:59,280 --> 01:20:01,280
height you know we're in the mountains

2323
01:20:01,280 --> 01:20:05,440
but especially for cases where we focus

2324
01:20:05,440 --> 01:20:06,800
on exploration

2325
01:20:06,800 --> 01:20:09,920
like innovation or maybe other areas

2326
01:20:09,920 --> 01:20:11,440
it's cool to think about how active

2327
01:20:11,440 --> 01:20:13,520
inference can perform well

2328
01:20:13,520 --> 01:20:15,840
so what are the next steps for the

2329
01:20:15,840 --> 01:20:16,719
authors and

2330
01:20:16,719 --> 01:20:20,639
for us an important next step

2331
01:20:20,639 --> 01:20:22,560
in examining active inference in the

2332
01:20:22,560 --> 01:20:24,159
context of multi-armed bandits is

2333
01:20:24,159 --> 01:20:26,000
establishing theoretical bounds

2334
01:20:26,000 --> 01:20:27,520
on the cumulative regret for the

2335
01:20:27,520 --> 01:20:29,280
stationary bandit problem

2336
01:20:29,280 --> 01:20:31,600
because as we saw it kind of diverged in

2337
01:20:31,600 --> 01:20:32,800
an unexpected way

2338
01:20:32,800 --> 01:20:35,280
that made sense but still wasn't quite

2339
01:20:35,280 --> 01:20:36,320
the behavior that

2340
01:20:36,320 --> 01:20:39,040
was initially expected and a key part of

2341
01:20:39,040 --> 01:20:40,560
these theoretical studies will be to

2342
01:20:40,560 --> 01:20:42,400
investigate whether it's possible

2343
01:20:42,400 --> 01:20:44,639
to devise a sound decay scheme for that

2344
01:20:44,639 --> 01:20:46,000
lambda parameter

2345
01:20:46,000 --> 01:20:49,199
so how rapidly should we change

2346
01:20:49,199 --> 01:20:51,920
our precision over preferences that

2347
01:20:51,920 --> 01:20:53,040
provably works

2348
01:20:53,040 --> 01:20:54,719
for all instances of the canonical

2349
01:20:54,719 --> 01:20:56,639
stationary bandit

2350
01:20:56,639 --> 01:20:58,159
what would that enable that's our

2351
01:20:58,159 --> 01:21:00,159
favorite question this would lead to the

2352
01:21:00,159 --> 01:21:02,080
development of new active inference

2353
01:21:02,080 --> 01:21:03,520
inspired algorithms

2354
01:21:03,520 --> 01:21:06,719
which can achieve asymptotic efficiency

2355
01:21:06,719 --> 01:21:08,320
these theoretical bounds would also

2356
01:21:08,320 --> 01:21:10,400
allow us to more rigorously compare

2357
01:21:10,400 --> 01:21:12,080
active inference algorithms to the

2358
01:21:12,080 --> 01:21:14,000
already established banded algorithms

2359
01:21:14,000 --> 01:21:16,800
for which regret bounds are known

2360
01:21:16,800 --> 01:21:18,719
moreover we would potentially be able to

2361
01:21:18,719 --> 01:21:20,400
generalize beyond settings

2362
01:21:20,400 --> 01:21:23,440
we have empirically tested here future

2363
01:21:23,440 --> 01:21:23,920
work

2364
01:21:23,920 --> 01:21:26,000
may also consider an information

2365
01:21:26,000 --> 01:21:28,800
theoretic analysis of active inference

2366
01:21:28,800 --> 01:21:30,800
which might be more appropriate than

2367
01:21:30,800 --> 01:21:33,280
regret analysis

2368
01:21:33,280 --> 01:21:36,480
also kind of a cool idea like

2369
01:21:36,480 --> 01:21:40,480
regret analysis was performance oriented

2370
01:21:40,480 --> 01:21:43,520
it's saying you're calculating your

2371
01:21:43,520 --> 01:21:45,040
regret based upon

2372
01:21:45,040 --> 01:21:48,560
the difference in performance between

2373
01:21:48,560 --> 01:21:51,199
optimal versus what you did but another

2374
01:21:51,199 --> 01:21:52,159
way to look at that

2375
01:21:52,159 --> 01:21:55,120
is using information theory and looking

2376
01:21:55,120 --> 01:21:56,000
as a function of

2377
01:21:56,000 --> 01:21:58,320
information being gained perhaps through

2378
01:21:58,320 --> 01:21:59,360
time

2379
01:21:59,360 --> 01:22:02,159
which just like you brought up it really

2380
01:22:02,159 --> 01:22:03,360
helps us focus on

2381
01:22:03,360 --> 01:22:06,320
exploration because if the whole project

2382
01:22:06,320 --> 01:22:08,639
is framed in terms of performance and

2383
01:22:08,639 --> 01:22:10,400
reward and value

2384
01:22:10,400 --> 01:22:12,960
it's almost like favoring exploitation

2385
01:22:12,960 --> 01:22:14,080
off the bat

2386
01:22:14,080 --> 01:22:16,080
because exploration is always going to

2387
01:22:16,080 --> 01:22:17,760
have to get couched in terms of

2388
01:22:17,760 --> 01:22:18,719
successful

2389
01:22:18,719 --> 01:22:21,280
exploitation like this basic research is

2390
01:22:21,280 --> 01:22:22,159
important

2391
01:22:22,159 --> 01:22:24,320
because later on we'll be able to apply

2392
01:22:24,320 --> 01:22:26,320
it which is fair

2393
01:22:26,320 --> 01:22:27,920
but when we have an information

2394
01:22:27,920 --> 01:22:30,000
theoretic analysis not just a regret

2395
01:22:30,000 --> 01:22:32,480
performance analysis

2396
01:22:32,480 --> 01:22:35,520
there might be a way to have exploration

2397
01:22:35,520 --> 01:22:37,920
for exploration's sake and what that

2398
01:22:37,920 --> 01:22:40,000
might enable

2399
01:22:40,000 --> 01:22:43,440
or even exploitation can be

2400
01:22:43,440 --> 01:22:47,280
thrown in in in terms of surprise uh

2401
01:22:47,280 --> 01:22:49,199
surprise minimization right like if

2402
01:22:49,199 --> 01:22:51,040
we're going to frame active inference in

2403
01:22:51,040 --> 01:22:52,000
terms of minimization

2404
01:22:52,000 --> 01:22:54,000
of surprise think about something like

2405
01:22:54,000 --> 01:22:55,760
the stock market problem right like this

2406
01:22:55,760 --> 01:22:57,360
is very much like a switching bandit

2407
01:22:57,360 --> 01:22:58,239
problem like

2408
01:22:58,239 --> 01:23:00,080
everything's changing all the time like

2409
01:23:00,080 --> 01:23:01,360
it's the same for a little while

2410
01:23:01,360 --> 01:23:03,520
like you're you're your stock that's

2411
01:23:03,520 --> 01:23:04,560
gonna win

2412
01:23:04,560 --> 01:23:06,800
so it like you know you're surprised

2413
01:23:06,800 --> 01:23:08,239
when you deviate from

2414
01:23:08,239 --> 01:23:10,639
say the s p 500 so you want to track

2415
01:23:10,639 --> 01:23:12,960
with the s p 500 and surprise happens

2416
01:23:12,960 --> 01:23:15,520
when you deviate from that

2417
01:23:15,520 --> 01:23:17,199
one more thought there on the

2418
01:23:17,199 --> 01:23:19,199
information theoretic analysis

2419
01:23:19,199 --> 01:23:21,280
and how it might say something different

2420
01:23:21,280 --> 01:23:22,719
than regret

2421
01:23:22,719 --> 01:23:25,440
let's go back to picking a restaurant so

2422
01:23:25,440 --> 01:23:27,360
we often hear about like controlled

2423
01:23:27,360 --> 01:23:28,159
novelty

2424
01:23:28,159 --> 01:23:30,159
you want surprise but not too much

2425
01:23:30,159 --> 01:23:31,199
surprise

2426
01:23:31,199 --> 01:23:33,440
so this gives an explanation for why

2427
01:23:33,440 --> 01:23:35,199
people might like their favorite

2428
01:23:35,199 --> 01:23:36,159
restaurant

2429
01:23:36,159 --> 01:23:38,480
it doesn't even need to be thought of in

2430
01:23:38,480 --> 01:23:39,280
the context

2431
01:23:39,280 --> 01:23:43,040
of maximizing reward like the 60 likely

2432
01:23:43,040 --> 01:23:44,960
to be the most rewarding restaurant

2433
01:23:44,960 --> 01:23:46,960
it could be like i'm trying to reduce my

2434
01:23:46,960 --> 01:23:48,159
uncertainty

2435
01:23:48,159 --> 01:23:50,159
so i'm likely to choose a place that i'm

2436
01:23:50,159 --> 01:23:51,199
familiar with

2437
01:23:51,199 --> 01:23:54,320
so that maybe i'll choose a different um

2438
01:23:54,320 --> 01:23:56,480
meal or maybe i'll choose the meal i

2439
01:23:56,480 --> 01:23:57,520
always order

2440
01:23:57,520 --> 01:24:00,480
but it's like the choice was driven by

2441
01:24:00,480 --> 01:24:02,960
reducing uncertainty

2442
01:24:02,960 --> 01:24:06,080
not even per se by maximizing reward

2443
01:24:06,080 --> 01:24:07,440
which is something that we come back

2444
01:24:07,440 --> 01:24:10,000
all the time to the difference between

2445
01:24:10,000 --> 01:24:11,199
value driven

2446
01:24:11,199 --> 01:24:14,000
reinforcement learning reward learning

2447
01:24:14,000 --> 01:24:16,000
and implicitly regret which is regret

2448
01:24:16,000 --> 01:24:19,440
about reward and reduction of

2449
01:24:19,440 --> 01:24:20,880
uncertainty

2450
01:24:20,880 --> 01:24:23,440
which puts us in a whole different space

2451
01:24:23,440 --> 01:24:25,280
yes we're able to enter that whole

2452
01:24:25,280 --> 01:24:27,440
physics of information flows area and

2453
01:24:27,440 --> 01:24:29,120
where we get the pragmatic

2454
01:24:29,120 --> 01:24:32,480
um straight components and then the

2455
01:24:32,480 --> 01:24:34,960
solenoidal the flow the iso contour yes

2456
01:24:34,960 --> 01:24:36,960
we access that whole area

2457
01:24:36,960 --> 01:24:39,360
in the information theory reduction of

2458
01:24:39,360 --> 01:24:40,560
uncertainty world

2459
01:24:40,560 --> 01:24:43,600
but also we just get qualitatively a

2460
01:24:43,600 --> 01:24:44,880
different story in a different

2461
01:24:44,880 --> 01:24:46,560
explanation for behavior

2462
01:24:46,560 --> 01:24:48,719
again you don't need to grasp for why

2463
01:24:48,719 --> 01:24:50,719
somebody's maximizing reward

2464
01:24:50,719 --> 01:24:52,320
with why they went to that restaurant or

2465
01:24:52,320 --> 01:24:54,480
why they ordered that one thing

2466
01:24:54,480 --> 01:24:57,600
when a lot of times just it stands alone

2467
01:24:57,600 --> 01:24:58,000
as an

2468
01:24:58,000 --> 01:25:00,159
uncertainty reduction and prior

2469
01:25:00,159 --> 01:25:01,760
preferences

2470
01:25:01,760 --> 01:25:04,080
understanding rather than needing to

2471
01:25:04,080 --> 01:25:06,239
make it about some sort of

2472
01:25:06,239 --> 01:25:09,120
maximization one one i'll flip the last

2473
01:25:09,120 --> 01:25:10,800
slide and then we have a

2474
01:25:10,800 --> 01:25:15,040
awesome question so um alex v

2475
01:25:15,040 --> 01:25:18,639
wrote is there connections between

2476
01:25:18,639 --> 01:25:21,360
exploration on epistemic value and

2477
01:25:21,360 --> 01:25:23,360
exploitation for pragmatic value

2478
01:25:23,360 --> 01:25:25,199
i think that's related to what you were

2479
01:25:25,199 --> 01:25:26,800
just speaking about

2480
01:25:26,800 --> 01:25:29,520
um which is that we're putting it on a

2481
01:25:29,520 --> 01:25:30,639
common

2482
01:25:30,639 --> 01:25:32,880
footing with active inference in terms

2483
01:25:32,880 --> 01:25:34,800
of the reduction of the expected free

2484
01:25:34,800 --> 01:25:35,760
energy

2485
01:25:35,760 --> 01:25:38,400
which has this epistemic or knowledge

2486
01:25:38,400 --> 01:25:39,600
gain component

2487
01:25:39,600 --> 01:25:41,920
and this pragmatic or functional

2488
01:25:41,920 --> 01:25:42,639
component

2489
01:25:42,639 --> 01:25:44,960
so the epistemic is like learning and

2490
01:25:44,960 --> 01:25:45,920
the pragmatic

2491
01:25:45,920 --> 01:25:49,280
is earning so we're we're learning while

2492
01:25:49,280 --> 01:25:50,159
we're earning

2493
01:25:50,159 --> 01:25:53,360
or the other way around we're

2494
01:25:53,360 --> 01:25:55,679
acquiring epistemically valuable

2495
01:25:55,679 --> 01:25:57,199
information

2496
01:25:57,199 --> 01:26:00,320
while we're also acquiring pragmatically

2497
01:26:00,320 --> 01:26:03,280
valuable information because the

2498
01:26:03,280 --> 01:26:04,880
decision making

2499
01:26:04,880 --> 01:26:08,400
of expected free energy minimization

2500
01:26:08,400 --> 01:26:11,120
conditioned on policy we're choosing

2501
01:26:11,120 --> 01:26:12,639
policies

2502
01:26:12,639 --> 01:26:15,760
based upon a function that

2503
01:26:15,760 --> 01:26:18,480
looks at both of those jointly and so

2504
01:26:18,480 --> 01:26:18,880
that

2505
01:26:18,880 --> 01:26:21,199
is one of the ways that we're rethinking

2506
01:26:21,199 --> 01:26:23,840
exploration and exploitation

2507
01:26:23,840 --> 01:26:26,480
by thinking okay that's not really the

2508
01:26:26,480 --> 01:26:27,520
duality

2509
01:26:27,520 --> 01:26:29,920
of policies that you want to talk about

2510
01:26:29,920 --> 01:26:31,520
because explore exploit makes it sound

2511
01:26:31,520 --> 01:26:33,040
like it's two behaviors that the bird

2512
01:26:33,040 --> 01:26:34,480
can be doing

2513
01:26:34,480 --> 01:26:36,320
and so here what we're doing is we're

2514
01:26:36,320 --> 01:26:38,719
kind of taking that idea

2515
01:26:38,719 --> 01:26:40,880
of sometimes you want to be doing a more

2516
01:26:40,880 --> 01:26:41,920
exploitative act

2517
01:26:41,920 --> 01:26:44,159
other times a more exploratory act and

2518
01:26:44,159 --> 01:26:45,600
then we're asking

2519
01:26:45,600 --> 01:26:48,320
given the policy of which a whole

2520
01:26:48,320 --> 01:26:49,679
spectrum might exist

2521
01:26:49,679 --> 01:26:53,199
of different policies how is each policy

2522
01:26:53,199 --> 01:26:56,719
contributing to epistemic and pragmatic

2523
01:26:56,719 --> 01:27:00,480
gain and then which policy

2524
01:27:00,480 --> 01:27:02,639
is going to have the best combination of

2525
01:27:02,639 --> 01:27:03,760
the two

2526
01:27:03,760 --> 01:27:06,639
maybe there's strategies where you're

2527
01:27:06,639 --> 01:27:07,920
maximizing both and maybe there's

2528
01:27:07,920 --> 01:27:09,280
strategies where you're maximizing

2529
01:27:09,280 --> 01:27:10,159
neither

2530
01:27:10,159 --> 01:27:11,760
but if you're only looking at one or the

2531
01:27:11,760 --> 01:27:14,000
other then you might uh

2532
01:27:14,000 --> 01:27:15,360
choose one that's like a poor

2533
01:27:15,360 --> 01:27:17,040
combination

2534
01:27:17,040 --> 01:27:19,120
well so a perfect example is like you

2535
01:27:19,120 --> 01:27:20,239
know so

2536
01:27:20,239 --> 01:27:23,199
your policy is based on your previous

2537
01:27:23,199 --> 01:27:24,480
chance of of

2538
01:27:24,480 --> 01:27:26,880
reward of minimizing surprise so i have

2539
01:27:26,880 --> 01:27:29,679
a policy that i'm not going to

2540
01:27:29,679 --> 01:27:31,840
eat a meal i've never had before in a

2541
01:27:31,840 --> 01:27:33,360
restaurant i've never been to before

2542
01:27:33,360 --> 01:27:33,920
because i

2543
01:27:33,920 --> 01:27:35,199
like i don't know what i'm getting into

2544
01:27:35,199 --> 01:27:37,040
that's like way over there right so my

2545
01:27:37,040 --> 01:27:39,040
policy would be to order a new meal in a

2546
01:27:39,040 --> 01:27:40,560
restaurant that i like

2547
01:27:40,560 --> 01:27:43,760
or order a meal that i'm used to

2548
01:27:43,760 --> 01:27:45,840
in a new restaurant so that would be

2549
01:27:45,840 --> 01:27:46,960
like the policy

2550
01:27:46,960 --> 01:27:50,239
in that just to continue that example

2551
01:27:50,239 --> 01:27:54,080
cool well what a fun discussion

2552
01:27:54,080 --> 01:27:56,800
i hope this was useful to those who are

2553
01:27:56,800 --> 01:27:57,600
familiar

2554
01:27:57,600 --> 01:28:01,040
with active inference as well as uh

2555
01:28:01,040 --> 01:28:03,679
bayesian statistics machine learning or

2556
01:28:03,679 --> 01:28:04,719
not

2557
01:28:04,719 --> 01:28:07,760
it's all chill because your questions

2558
01:28:07,760 --> 01:28:08,159
will

2559
01:28:08,159 --> 01:28:10,320
really help move the whole project

2560
01:28:10,320 --> 01:28:12,560
forward the parts that do and don't make

2561
01:28:12,560 --> 01:28:13,199
sense

2562
01:28:13,199 --> 01:28:15,040
and the parts that you want to explore

2563
01:28:15,040 --> 01:28:16,960
more i think that's how

2564
01:28:16,960 --> 01:28:19,199
our ensemble is going to proceed in

2565
01:28:19,199 --> 01:28:21,600
these transdisciplinary areas

2566
01:28:21,600 --> 01:28:24,480
is by welcoming those who have different

2567
01:28:24,480 --> 01:28:25,679
backgrounds so

2568
01:28:25,679 --> 01:28:27,920
whether you're really familiar or not

2569
01:28:27,920 --> 01:28:30,159
you're totally welcome to participate

2570
01:28:30,159 --> 01:28:32,560
in our live discussions or just to ask

2571
01:28:32,560 --> 01:28:33,840
questions or

2572
01:28:33,840 --> 01:28:37,199
participate in some other way

2573
01:28:37,199 --> 01:28:39,440
thanks blue for the awesome help with

2574
01:28:39,440 --> 01:28:41,280
preparing the slides and for this

2575
01:28:41,280 --> 01:28:44,080
conversation and we'll see everybody

2576
01:28:44,080 --> 01:28:45,760
another time

2577
01:28:45,760 --> 01:28:52,639
bye bye


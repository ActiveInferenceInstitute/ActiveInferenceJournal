1
00:00:07,120 --> 00:00:07,839
all right

2
00:00:07,839 --> 00:00:11,360
hello everyone and welcome to actin flab

3
00:00:11,360 --> 00:00:14,799
live stream number 24.2 today is

4
00:00:14,799 --> 00:00:18,160
june 29th 2021 and

5
00:00:18,160 --> 00:00:20,480
we're looking forward to this follow up

6
00:00:20,480 --> 00:00:22,480
or jump off discussion

7
00:00:22,480 --> 00:00:24,560
with some of the authors as well as

8
00:00:24,560 --> 00:00:26,320
other lab participants on this

9
00:00:26,320 --> 00:00:28,880
awesome paper an empirical evaluation of

10
00:00:28,880 --> 00:00:30,000
active inference

11
00:00:30,000 --> 00:00:33,760
in multi-armed bandits welcome to the

12
00:00:33,760 --> 00:00:35,440
active inference lab

13
00:00:35,440 --> 00:00:37,920
we are a participatory online lab that

14
00:00:37,920 --> 00:00:38,879
is communicating

15
00:00:38,879 --> 00:00:40,800
learning and practicing applied active

16
00:00:40,800 --> 00:00:43,440
inference you can find out more about us

17
00:00:43,440 --> 00:00:45,680
at the links on this slide

18
00:00:45,680 --> 00:00:47,600
this is recorded in an archived live

19
00:00:47,600 --> 00:00:49,440
stream so please provide us with

20
00:00:49,440 --> 00:00:50,079
feedback

21
00:00:50,079 --> 00:00:53,120
so that we can be improving on our work

22
00:00:53,120 --> 00:00:55,280
all backgrounds and perspectives are

23
00:00:55,280 --> 00:00:56,239
welcome here

24
00:00:56,239 --> 00:00:58,320
and we'll be following good video

25
00:00:58,320 --> 00:01:00,000
etiquette for live streams it's great

26
00:01:00,000 --> 00:01:00,640
that there's

27
00:01:00,640 --> 00:01:02,320
so many on the call and i'm sure we'll

28
00:01:02,320 --> 00:01:05,040
have many questions and opportunities to

29
00:01:05,040 --> 00:01:07,360
raise our hand as well as for those in

30
00:01:07,360 --> 00:01:09,280
the live chat to ask questions

31
00:01:09,280 --> 00:01:12,799
whenever they feel like it we're closing

32
00:01:12,799 --> 00:01:13,119
out

33
00:01:13,119 --> 00:01:16,159
june with this dot 2 discussion

34
00:01:16,159 --> 00:01:20,080
on paper number 24. so last week we

35
00:01:20,080 --> 00:01:22,159
also spoke with sarah and dimitri and

36
00:01:22,159 --> 00:01:23,360
others that was very

37
00:01:23,360 --> 00:01:25,520
informative and now we're in the dot 2

38
00:01:25,520 --> 00:01:27,119
we're able to take it to a few different

39
00:01:27,119 --> 00:01:27,840
places

40
00:01:27,840 --> 00:01:29,759
and raise up a few questions that were

41
00:01:29,759 --> 00:01:31,200
asked over the last

42
00:01:31,200 --> 00:01:34,320
week and the goal is really just to

43
00:01:34,320 --> 00:01:37,840
learn and discuss this paper and related

44
00:01:37,840 --> 00:01:39,040
topics we have a few

45
00:01:39,040 --> 00:01:40,720
cool related areas that we're going to

46
00:01:40,720 --> 00:01:42,960
be going into

47
00:01:42,960 --> 00:01:46,079
so we can just begin with a introduction

48
00:01:46,079 --> 00:01:47,520
round

49
00:01:47,520 --> 00:01:50,399
each person can say hello or check in

50
00:01:50,399 --> 00:01:52,159
say anything that they're thinking about

51
00:01:52,159 --> 00:01:53,040
today

52
00:01:53,040 --> 00:01:54,720
and then pass it to somebody who hasn't

53
00:01:54,720 --> 00:01:56,479
spoken yet so

54
00:01:56,479 --> 00:01:59,520
i'm daniel i'm a postdoc in california

55
00:01:59,520 --> 00:02:03,280
and i'll pass it first to blue

56
00:02:04,960 --> 00:02:08,080
hi i'm blue i am an independent research

57
00:02:08,080 --> 00:02:10,800
consultant based out of new mexico

58
00:02:10,800 --> 00:02:14,239
and i will pass it to ryan

59
00:02:14,239 --> 00:02:16,560
um yeah i am ryan smith i'm an

60
00:02:16,560 --> 00:02:17,520
investigator

61
00:02:17,520 --> 00:02:19,280
at the laureate institute for brain

62
00:02:19,280 --> 00:02:21,920
research um and i'll

63
00:02:21,920 --> 00:02:25,840
ask you sir hi i'm sarah i'm a postdoc

64
00:02:25,840 --> 00:02:28,160
at the technical university in prison

65
00:02:28,160 --> 00:02:29,680
and i'm looking forward to the

66
00:02:29,680 --> 00:02:31,120
discussions today

67
00:02:31,120 --> 00:02:34,879
and i pass on to dimitri

68
00:02:34,879 --> 00:02:37,360
hi everyone i'm dmitry markovic postdoc

69
00:02:37,360 --> 00:02:39,760
at technical university of raisin also

70
00:02:39,760 --> 00:02:43,120
uh chair of neuroimaging and i pass it

71
00:02:43,120 --> 00:02:43,680
to

72
00:02:43,680 --> 00:02:48,840
dave can he talk actually

73
00:02:48,840 --> 00:02:51,440
no my background's

74
00:02:51,440 --> 00:02:54,879
a basic psych and cybernetic learning

75
00:02:54,879 --> 00:02:56,480
thing

76
00:02:56,480 --> 00:03:00,319
and i'll pass this to stephen please

77
00:03:00,319 --> 00:03:03,040
hello i'm stephen i'm based in toronto

78
00:03:03,040 --> 00:03:06,159
i'm doing a practice-based phd around uh

79
00:03:06,159 --> 00:03:09,200
social topographies and

80
00:03:09,200 --> 00:03:12,480
immersive experiences and i think i'll

81
00:03:12,480 --> 00:03:13,200
pass it back

82
00:03:13,200 --> 00:03:16,480
to daniel if i'm not mistaken yep

83
00:03:16,480 --> 00:03:20,879
great thanks for the cool intro round

84
00:03:20,879 --> 00:03:23,920
so sarah prepared this slide which we

85
00:03:23,920 --> 00:03:25,120
just want to

86
00:03:25,120 --> 00:03:29,120
start off with so go for it sarah

87
00:03:29,120 --> 00:03:31,519
how does this set the uh intention in

88
00:03:31,519 --> 00:03:32,560
the mood for what we're going to be

89
00:03:32,560 --> 00:03:34,159
going into today

90
00:03:34,159 --> 00:03:36,879
uh wait how can i make it so that i see

91
00:03:36,879 --> 00:03:38,959
it pick again

92
00:03:38,959 --> 00:03:42,239
no okay so um after

93
00:03:42,239 --> 00:03:44,400
the discussion last week and also the

94
00:03:44,400 --> 00:03:46,480
questions we've got we thought it would

95
00:03:46,480 --> 00:03:48,239
be a good idea

96
00:03:48,239 --> 00:03:51,280
to just make a short overview slide

97
00:03:51,280 --> 00:03:54,319
about what we understand as active

98
00:03:54,319 --> 00:03:55,760
inference

99
00:03:55,760 --> 00:03:58,560
and as cognitive neuroscientists you

100
00:03:58,560 --> 00:04:00,879
said in the discussion before also

101
00:04:00,879 --> 00:04:03,599
we are often or mainly interested in

102
00:04:03,599 --> 00:04:04,400
this

103
00:04:04,400 --> 00:04:08,159
action perception loop that an agent

104
00:04:08,159 --> 00:04:10,319
where we as living agents are in with

105
00:04:10,319 --> 00:04:12,080
our environment

106
00:04:12,080 --> 00:04:15,360
and that's what you see on the right

107
00:04:15,360 --> 00:04:17,358
in this loop and on the left side on the

108
00:04:17,358 --> 00:04:19,759
right you see the environment it may

109
00:04:19,759 --> 00:04:20,798
have

110
00:04:20,798 --> 00:04:23,360
hidden states that are the states that

111
00:04:23,360 --> 00:04:24,160
you may know

112
00:04:24,160 --> 00:04:26,960
from markov decision process processes

113
00:04:26,960 --> 00:04:28,639
in reinforcement learning

114
00:04:28,639 --> 00:04:30,800
but that may also be more abstract

115
00:04:30,800 --> 00:04:32,560
things that aren't observable tone

116
00:04:32,560 --> 00:04:34,080
agents like

117
00:04:34,080 --> 00:04:38,479
contexts volatility parameters

118
00:04:38,479 --> 00:04:41,360
anything that describes the environment

119
00:04:41,360 --> 00:04:43,280
and these hidden states may also

120
00:04:43,280 --> 00:04:46,080
evolve with time and then such an

121
00:04:46,080 --> 00:04:48,720
environment generates an observation

122
00:04:48,720 --> 00:04:51,759
that the agent then integrates together

123
00:04:51,759 --> 00:04:52,080
with

124
00:04:52,080 --> 00:04:55,680
its prior beliefs in order to infer the

125
00:04:55,680 --> 00:04:56,000
hidden

126
00:04:56,000 --> 00:04:57,840
states of the environment which is what

127
00:04:57,840 --> 00:04:59,600
we call perception

128
00:04:59,600 --> 00:05:01,680
then you can use this information to

129
00:05:01,680 --> 00:05:02,960
plan ahead

130
00:05:02,960 --> 00:05:06,080
and select an action accordingly and in

131
00:05:06,080 --> 00:05:07,440
turn the action may

132
00:05:07,440 --> 00:05:10,960
change the environment the agent is in

133
00:05:10,960 --> 00:05:13,039
and concretely how the agent side of

134
00:05:13,039 --> 00:05:14,000
this works

135
00:05:14,000 --> 00:05:16,880
in active inference is so that you

136
00:05:16,880 --> 00:05:17,840
specify

137
00:05:17,840 --> 00:05:20,880
a probabilistic generative model

138
00:05:20,880 --> 00:05:25,039
that contains the hidden states

139
00:05:25,039 --> 00:05:27,039
you are a priori knowledge or your

140
00:05:27,039 --> 00:05:28,960
assumptions about how these hidden

141
00:05:28,960 --> 00:05:30,960
states relate to each other

142
00:05:30,960 --> 00:05:33,039
which states may follow up on which

143
00:05:33,039 --> 00:05:34,560
states and so on

144
00:05:34,560 --> 00:05:38,320
it contains observation generation rules

145
00:05:38,320 --> 00:05:41,919
it contains actions or policies

146
00:05:41,919 --> 00:05:43,759
which in active inference are also

147
00:05:43,759 --> 00:05:45,600
treated as hidden variables that need to

148
00:05:45,600 --> 00:05:46,800
be inferred

149
00:05:46,800 --> 00:05:50,000
and additionally the generative model

150
00:05:50,000 --> 00:05:53,600
may contain its own parameters

151
00:05:53,600 --> 00:05:57,199
as a random variable so that they can be

152
00:05:57,199 --> 00:05:57,919
inferred

153
00:05:57,919 --> 00:06:01,280
and there was learned then you

154
00:06:01,280 --> 00:06:05,039
specify a probability distribution of

155
00:06:05,039 --> 00:06:06,720
approximate beliefs

156
00:06:06,720 --> 00:06:10,560
in theory you could also

157
00:06:10,560 --> 00:06:12,880
solve the beliefs of the generative

158
00:06:12,880 --> 00:06:13,600
model

159
00:06:13,600 --> 00:06:15,759
analytically but that becomes very

160
00:06:15,759 --> 00:06:17,759
computationally expensive

161
00:06:17,759 --> 00:06:19,919
really quickly or even analytically

162
00:06:19,919 --> 00:06:21,039
intractable

163
00:06:21,039 --> 00:06:23,120
so what you do instead is formulate

164
00:06:23,120 --> 00:06:24,720
these approximate beliefs

165
00:06:24,720 --> 00:06:27,199
which are supposed to be of a simpler

166
00:06:27,199 --> 00:06:28,160
form

167
00:06:28,160 --> 00:06:30,080
and for example in your approximate

168
00:06:30,080 --> 00:06:31,680
beliefs you could assume

169
00:06:31,680 --> 00:06:34,240
that all hidden variables are

170
00:06:34,240 --> 00:06:35,199
independent

171
00:06:35,199 --> 00:06:37,919
random variables which makes the beliefs

172
00:06:37,919 --> 00:06:40,240
very easy to calculate

173
00:06:40,240 --> 00:06:43,759
but may not properly capture the

174
00:06:43,759 --> 00:06:45,680
temporal dynamics for example of your

175
00:06:45,680 --> 00:06:47,840
environment

176
00:06:47,840 --> 00:06:51,440
and this this independence assumption

177
00:06:51,440 --> 00:06:53,039
is also called the mean field

178
00:06:53,039 --> 00:06:55,520
approximation but you could also

179
00:06:55,520 --> 00:06:58,240
say no with some hidden variables it's

180
00:06:58,240 --> 00:06:59,520
very important that they

181
00:06:59,520 --> 00:07:02,000
are allowed to co-vary and be dependent

182
00:07:02,000 --> 00:07:03,199
on each other

183
00:07:03,199 --> 00:07:06,160
and then you would have pairwise

184
00:07:06,160 --> 00:07:07,840
dependencies in these approximate

185
00:07:07,840 --> 00:07:09,680
beliefs which is then called the beta

186
00:07:09,680 --> 00:07:11,199
approximation

187
00:07:11,199 --> 00:07:14,319
you may have different approximations

188
00:07:14,319 --> 00:07:16,000
for different variables some may

189
00:07:16,000 --> 00:07:20,479
vary together others are independent

190
00:07:20,560 --> 00:07:22,960
then you plug in the generative model

191
00:07:22,960 --> 00:07:24,880
and your approximate beliefs

192
00:07:24,880 --> 00:07:28,160
into the free energy and then you find

193
00:07:28,160 --> 00:07:30,560
your approximate beliefs at the minimum

194
00:07:30,560 --> 00:07:32,160
of this free energy

195
00:07:32,160 --> 00:07:34,560
and this way you can form about beliefs

196
00:07:34,560 --> 00:07:35,199
about

197
00:07:35,199 --> 00:07:37,360
hidden states of the environment which

198
00:07:37,360 --> 00:07:38,800
really can be anything

199
00:07:38,800 --> 00:07:41,680
your location

200
00:07:43,120 --> 00:07:45,120
volatility of the environment the

201
00:07:45,120 --> 00:07:46,879
context you're in

202
00:07:46,879 --> 00:07:49,520
you infer believes about policies which

203
00:07:49,520 --> 00:07:51,759
is a probability distribution

204
00:07:51,759 --> 00:07:53,759
over actions from which you can then

205
00:07:53,759 --> 00:07:54,879
choose

206
00:07:54,879 --> 00:07:57,199
and you can even form beliefs about

207
00:07:57,199 --> 00:07:59,039
parameters of the model

208
00:07:59,039 --> 00:08:01,680
and there with update your knowledge of

209
00:08:01,680 --> 00:08:05,039
the model in each time step

210
00:08:05,039 --> 00:08:07,520
and then in the end you can use all this

211
00:08:07,520 --> 00:08:08,800
inferred knowledge

212
00:08:08,800 --> 00:08:12,319
to make actions um

213
00:08:12,319 --> 00:08:14,560
yeah we as cognitive neuroscientists are

214
00:08:14,560 --> 00:08:16,960
often interested in this whole loop

215
00:08:16,960 --> 00:08:19,680
but um what dimitri showed last week is

216
00:08:19,680 --> 00:08:21,280
also that in principle this

217
00:08:21,280 --> 00:08:24,720
can be really modular right so um

218
00:08:24,720 --> 00:08:28,560
in our paper for example perception

219
00:08:28,560 --> 00:08:31,599
was the same for all different

220
00:08:31,599 --> 00:08:34,719
agents whereas dimitri then

221
00:08:34,719 --> 00:08:36,880
plugged in different action selection

222
00:08:36,880 --> 00:08:38,000
algorithms

223
00:08:38,000 --> 00:08:41,360
one corresponding to the expected free

224
00:08:41,360 --> 00:08:43,440
energy and active inference but other

225
00:08:43,440 --> 00:08:45,519
well-known action selections algorithm

226
00:08:45,519 --> 00:08:46,959
too so

227
00:08:46,959 --> 00:08:49,519
um here you can really also play around

228
00:08:49,519 --> 00:08:51,440
with this framework

229
00:08:51,440 --> 00:08:54,880
and um use its modular nature you could

230
00:08:54,880 --> 00:08:56,959
on the other hand use the same action

231
00:08:56,959 --> 00:08:58,399
selection algorithm

232
00:08:58,399 --> 00:08:59,839
and compare different learning

233
00:08:59,839 --> 00:09:01,680
algorithms which is something

234
00:09:01,680 --> 00:09:04,399
we're also often interested about in our

235
00:09:04,399 --> 00:09:05,839
work

236
00:09:05,839 --> 00:09:08,320
and yeah with that that's it from me for

237
00:09:08,320 --> 00:09:10,480
now

238
00:09:11,040 --> 00:09:14,240
cool very interesting and anyone can

239
00:09:14,240 --> 00:09:16,480
raise their hand but i think the part

240
00:09:16,480 --> 00:09:17,360
that spoke to me

241
00:09:17,360 --> 00:09:19,839
there was just that it's a framework and

242
00:09:19,839 --> 00:09:21,920
so it's kind of like the framework of a

243
00:09:21,920 --> 00:09:23,440
desktop computer you can

244
00:09:23,440 --> 00:09:26,000
take out the memory or change out the

245
00:09:26,000 --> 00:09:27,040
hard drive

246
00:09:27,040 --> 00:09:29,360
and so we can talk about changing the

247
00:09:29,360 --> 00:09:31,519
environmental dynamics

248
00:09:31,519 --> 00:09:33,279
with respect to the hidden states and

249
00:09:33,279 --> 00:09:34,959
how they evolve through time

250
00:09:34,959 --> 00:09:37,440
we can talk about changing perception

251
00:09:37,440 --> 00:09:38,560
but we can also like

252
00:09:38,560 --> 00:09:41,839
lock in multiple of the components

253
00:09:41,839 --> 00:09:43,600
and then explore how different learning

254
00:09:43,600 --> 00:09:46,000
rules influence system behavior that's

255
00:09:46,000 --> 00:09:46,480
what we

256
00:09:46,480 --> 00:09:48,640
explored in this multi-arm bandit paper

257
00:09:48,640 --> 00:09:50,000
but it could be the same

258
00:09:50,000 --> 00:09:52,480
learning and policy selection rule but

259
00:09:52,480 --> 00:09:54,080
then a different perception

260
00:09:54,080 --> 00:09:57,600
rule and then those might map onto

261
00:09:57,600 --> 00:10:00,880
very subtle differences in um decision

262
00:10:00,880 --> 00:10:01,519
making of

263
00:10:01,519 --> 00:10:02,959
agents that we care about like what's

264
00:10:02,959 --> 00:10:04,640
the difference between somebody who has

265
00:10:04,640 --> 00:10:07,120
blurry vision but they know the language

266
00:10:07,120 --> 00:10:08,399
versus clear

267
00:10:08,399 --> 00:10:10,480
vision but then they're semantically

268
00:10:10,480 --> 00:10:11,600
unsure

269
00:10:11,600 --> 00:10:14,720
about what the symbols are representing

270
00:10:14,720 --> 00:10:18,959
um yeah stephen and then anyone else

271
00:10:18,959 --> 00:10:22,160
who raises their hand yeah this is

272
00:10:22,160 --> 00:10:23,360
really

273
00:10:23,360 --> 00:10:24,959
interesting and helpful to see your

274
00:10:24,959 --> 00:10:27,680
thinking here i'll just be curious how

275
00:10:27,680 --> 00:10:31,680
action states or um you know the idea of

276
00:10:31,680 --> 00:10:33,279
different types of states

277
00:10:33,279 --> 00:10:36,160
alongside these hidden states and just

278
00:10:36,160 --> 00:10:36,800
how that

279
00:10:36,800 --> 00:10:40,959
you treat those in your thinking um

280
00:10:40,959 --> 00:10:43,120
because there could be kind of the

281
00:10:43,120 --> 00:10:45,519
action states that are imagined

282
00:10:45,519 --> 00:10:49,440
um and the action states which are

283
00:10:49,440 --> 00:10:52,560
part of the kind of external environment

284
00:10:52,560 --> 00:10:56,719
that you're somehow immersed in

285
00:10:57,200 --> 00:11:00,800
so in the end yes actions are also

286
00:11:00,800 --> 00:11:03,120
treated as hidden variables i think they

287
00:11:03,120 --> 00:11:04,480
are very separate from

288
00:11:04,480 --> 00:11:07,040
other from the hidden states for example

289
00:11:07,040 --> 00:11:07,920
because

290
00:11:07,920 --> 00:11:10,640
the space of actions determines what an

291
00:11:10,640 --> 00:11:12,000
agent can choose

292
00:11:12,000 --> 00:11:15,040
and you cannot choose a state

293
00:11:15,040 --> 00:11:18,160
um in terms of other hidden variables

294
00:11:18,160 --> 00:11:21,200
i think so for example if i think

295
00:11:21,200 --> 00:11:24,320
location versus context then um in my

296
00:11:24,320 --> 00:11:26,079
head the generative model becomes a

297
00:11:26,079 --> 00:11:28,800
hierarchical model exactly

298
00:11:28,800 --> 00:11:32,079
and in the end yeah

299
00:11:32,079 --> 00:11:34,959
that's also what i meant when i said you

300
00:11:34,959 --> 00:11:35,360
plug

301
00:11:35,360 --> 00:11:37,519
in your a priori knowledge or your

302
00:11:37,519 --> 00:11:40,240
assumptions about the environment

303
00:11:40,240 --> 00:11:42,800
because different types of hidden states

304
00:11:42,800 --> 00:11:43,760
will have different

305
00:11:43,760 --> 00:11:45,279
relationships to which and then you

306
00:11:45,279 --> 00:11:46,720
really end up with a completely

307
00:11:46,720 --> 00:11:47,279
different

308
00:11:47,279 --> 00:11:50,720
uh generative model

309
00:11:50,720 --> 00:11:53,120
just to follow up there sarah you said

310
00:11:53,120 --> 00:11:53,920
that when

311
00:11:53,920 --> 00:11:56,480
location and context are differentiated

312
00:11:56,480 --> 00:11:57,760
that it's a hierarchical

313
00:11:57,760 --> 00:12:00,079
model so what does that look like on

314
00:12:00,079 --> 00:12:01,040
this

315
00:12:01,040 --> 00:12:03,439
layout

316
00:12:04,160 --> 00:12:07,040
you mean in this figure here like what's

317
00:12:07,040 --> 00:12:08,399
the setting where

318
00:12:08,399 --> 00:12:11,120
location and context are identical and

319
00:12:11,120 --> 00:12:12,959
so it's a non-hierarchical model versus

320
00:12:12,959 --> 00:12:14,959
a case where they are differentiated and

321
00:12:14,959 --> 00:12:17,440
then it is a hierarchical model

322
00:12:17,440 --> 00:12:20,959
so for example um if i want to model

323
00:12:20,959 --> 00:12:21,839
myself

324
00:12:21,839 --> 00:12:24,160
walking through my flat that wouldn't be

325
00:12:24,160 --> 00:12:26,000
a hierarchical model

326
00:12:26,000 --> 00:12:28,160
where just each location maybe the

327
00:12:28,160 --> 00:12:29,279
different room

328
00:12:29,279 --> 00:12:33,040
rooms count as states that i infer based

329
00:12:33,040 --> 00:12:34,560
upon

330
00:12:34,560 --> 00:12:37,680
the curtains i see for example

331
00:12:37,680 --> 00:12:40,880
but then if i want to introduce another

332
00:12:40,880 --> 00:12:42,720
person's flat

333
00:12:42,720 --> 00:12:46,320
then the transition dynamics

334
00:12:46,320 --> 00:12:48,399
may be different for their flat so for

335
00:12:48,399 --> 00:12:50,800
example in my flat i can go from the

336
00:12:50,800 --> 00:12:53,760
living room into the kitchen but in my

337
00:12:53,760 --> 00:12:54,639
friend's flat

338
00:12:54,639 --> 00:12:57,440
i cannot make the state transition and

339
00:12:57,440 --> 00:12:59,279
that's actually something i'm working

340
00:12:59,279 --> 00:13:01,440
with currently where an agent

341
00:13:01,440 --> 00:13:04,399
like the underlying decision process is

342
00:13:04,399 --> 00:13:07,839
very similar but an agent needs to infer

343
00:13:07,839 --> 00:13:10,880
which context he's in in order to

344
00:13:10,880 --> 00:13:13,120
then load the correct transition

345
00:13:13,120 --> 00:13:15,680
dynamics

346
00:13:16,160 --> 00:13:20,240
that reminds me of the spm textbook

347
00:13:20,240 --> 00:13:23,440
how it talks about hidden states being

348
00:13:23,440 --> 00:13:26,560
just the state that

349
00:13:26,560 --> 00:13:30,000
a process is in that generates outcomes

350
00:13:30,000 --> 00:13:31,920
so we have a clear distinction between

351
00:13:31,920 --> 00:13:33,440
the hidden stage which aren't directly

352
00:13:33,440 --> 00:13:34,240
observed

353
00:13:34,240 --> 00:13:36,399
but there is this hidden state like

354
00:13:36,399 --> 00:13:37,600
which

355
00:13:37,600 --> 00:13:41,040
flat am i in and then that is going to

356
00:13:41,040 --> 00:13:42,639
change the transition dynamics between

357
00:13:42,639 --> 00:13:43,360
the rooms

358
00:13:43,360 --> 00:13:46,399
but the observations are what are

359
00:13:46,399 --> 00:13:48,160
coming to the agent and then also

360
00:13:48,160 --> 00:13:49,600
there's like learning

361
00:13:49,600 --> 00:13:51,279
which we don't have here but that would

362
00:13:51,279 --> 00:13:53,360
reflect the prior beliefs being updated

363
00:13:53,360 --> 00:13:55,920
through time

364
00:13:56,240 --> 00:13:59,760
what about this planning stage like what

365
00:13:59,760 --> 00:14:03,839
is happening in this planning module

366
00:14:05,279 --> 00:14:07,120
um that's a good question actually

367
00:14:07,120 --> 00:14:09,600
nowadays um so this figures from my

368
00:14:09,600 --> 00:14:10,639
previous paper

369
00:14:10,639 --> 00:14:13,760
nowadays i um lamp in perception and

370
00:14:13,760 --> 00:14:15,440
planning together because it's really

371
00:14:15,440 --> 00:14:17,199
not that easy separable

372
00:14:17,199 --> 00:14:20,240
um the pro so in this case what i meant

373
00:14:20,240 --> 00:14:21,040
is

374
00:14:21,040 --> 00:14:24,160
perception is really inferring passed

375
00:14:24,160 --> 00:14:28,160
up into the current hidden states um

376
00:14:28,160 --> 00:14:30,880
to which i had the observations that

377
00:14:30,880 --> 00:14:32,480
belong to each state

378
00:14:32,480 --> 00:14:34,639
whereas planning is inferring future

379
00:14:34,639 --> 00:14:36,639
hidden states and future observations

380
00:14:36,639 --> 00:14:39,279
and potentially future rewards

381
00:14:39,279 --> 00:14:42,000
but in the end it's all one model and

382
00:14:42,000 --> 00:14:44,079
it's a

383
00:14:44,079 --> 00:14:46,880
chain to go through the states so the

384
00:14:46,880 --> 00:14:48,720
differentiation

385
00:14:48,720 --> 00:14:51,120
only makes medium sense i think it's

386
00:14:51,120 --> 00:14:52,160
rather one

387
00:14:52,160 --> 00:14:55,680
inference problem

388
00:14:55,680 --> 00:14:58,399
interesting thanks blue and then anyone

389
00:14:58,399 --> 00:15:00,560
else

390
00:15:00,959 --> 00:15:04,079
so just thinking about metacognition and

391
00:15:04,079 --> 00:15:04,639
where

392
00:15:04,639 --> 00:15:09,680
that might fit into this model

393
00:15:10,839 --> 00:15:13,360
um i didn't actually have some

394
00:15:13,360 --> 00:15:17,279
um has published a paper on a meta

395
00:15:17,279 --> 00:15:19,040
cognition recently also

396
00:15:19,040 --> 00:15:22,240
so map control actually right uh

397
00:15:22,240 --> 00:15:24,800
but but yeah i mean one can just see uh

398
00:15:24,800 --> 00:15:26,959
one can stack

399
00:15:26,959 --> 00:15:30,160
multiple agents on top of each other

400
00:15:30,160 --> 00:15:32,079
and imagine as a kind of higher level

401
00:15:32,079 --> 00:15:34,160
agent controlling what the

402
00:15:34,160 --> 00:15:37,120
bottom level agent is doing and so this

403
00:15:37,120 --> 00:15:38,800
would be like

404
00:15:38,800 --> 00:15:41,839
well deep active inference models and

405
00:15:41,839 --> 00:15:43,600
in one of the recent papers we're also

406
00:15:43,600 --> 00:15:45,120
exploring this in a kind of

407
00:15:45,120 --> 00:15:48,000
as a meta control approach for

408
00:15:48,000 --> 00:15:49,440
describing

409
00:15:49,440 --> 00:15:52,560
uh like cognitive control

410
00:15:52,560 --> 00:15:54,079
so if there's some kind of downward

411
00:15:54,079 --> 00:15:56,399
causation does that then inhibit like

412
00:15:56,399 --> 00:15:57,839
what's going on at the

413
00:15:57,839 --> 00:16:00,959
like lower agent level

414
00:16:00,959 --> 00:16:02,720
yeah so basically right you can see kind

415
00:16:02,720 --> 00:16:05,440
of actions on the higher level

416
00:16:05,440 --> 00:16:08,720
uh are defining flyers on the lower

417
00:16:08,720 --> 00:16:09,759
level and

418
00:16:09,759 --> 00:16:13,600
selecting the policy space right so what

419
00:16:13,600 --> 00:16:16,800
kind of your level age lev lower level

420
00:16:16,800 --> 00:16:19,839
agent can actually do

421
00:16:20,560 --> 00:16:23,759
where this right this isn't

422
00:16:23,759 --> 00:16:26,480
now just kind of illustration of kind of

423
00:16:26,480 --> 00:16:27,839
separating maybe

424
00:16:27,839 --> 00:16:29,519
higher level cognition from the

425
00:16:29,519 --> 00:16:31,680
executive part which is kind of

426
00:16:31,680 --> 00:16:35,040
moving uh uh arms or muscles on your

427
00:16:35,040 --> 00:16:37,600
body

428
00:16:37,600 --> 00:16:41,519
uh it's it's interesting about that like

429
00:16:41,519 --> 00:16:43,040
kind of like a virtual machine

430
00:16:43,040 --> 00:16:45,360
or a hypervisor that there's this like

431
00:16:45,360 --> 00:16:46,399
emulated

432
00:16:46,399 --> 00:16:49,279
agent you know what would my better self

433
00:16:49,279 --> 00:16:49,680
do

434
00:16:49,680 --> 00:16:53,040
that's a classic metacognition question

435
00:16:53,040 --> 00:16:55,199
and then also it was just interesting

436
00:16:55,199 --> 00:16:56,880
that you know blue you asked about

437
00:16:56,880 --> 00:16:58,560
metacognition and then

438
00:16:58,560 --> 00:17:00,959
dmitry you write to meta control and so

439
00:17:00,959 --> 00:17:01,680
that really

440
00:17:01,680 --> 00:17:03,519
speaks to the way in which we're

441
00:17:03,519 --> 00:17:05,439
thinking about cognition

442
00:17:05,439 --> 00:17:08,959
as control and that's part of a control

443
00:17:08,959 --> 00:17:11,199
theoretic perspective which is that like

444
00:17:11,199 --> 00:17:12,400
the cognition

445
00:17:12,400 --> 00:17:14,720
planning as inference the cognition is

446
00:17:14,720 --> 00:17:15,439
about

447
00:17:15,439 --> 00:17:18,799
action selection but then one piece that

448
00:17:18,799 --> 00:17:20,240
differs potentially

449
00:17:20,240 --> 00:17:22,959
is an active inference relative to just

450
00:17:22,959 --> 00:17:23,439
sort of

451
00:17:23,439 --> 00:17:26,160
the um way that control theory is often

452
00:17:26,160 --> 00:17:27,119
discussed

453
00:17:27,119 --> 00:17:29,120
is the insight of perceptual control

454
00:17:29,120 --> 00:17:30,160
theory

455
00:17:30,160 --> 00:17:32,880
which is that the planning as inference

456
00:17:32,880 --> 00:17:33,600
is being done

457
00:17:33,600 --> 00:17:37,039
in service of the expected observations

458
00:17:37,039 --> 00:17:39,760
and so we're planning and acting in

459
00:17:39,760 --> 00:17:42,480
order to control our perceptions

460
00:17:42,480 --> 00:17:44,320
and that's happening as part of this

461
00:17:44,320 --> 00:17:46,480
integrated loop

462
00:17:46,480 --> 00:17:48,080
so it's kind of cool that there's like

463
00:17:48,080 --> 00:17:49,760
this like good regulator

464
00:17:49,760 --> 00:17:53,120
theorem in terms of cybernetics and then

465
00:17:53,120 --> 00:17:56,320
it is like there actually is an emulated

466
00:17:56,320 --> 00:18:02,000
regulator agent and then um

467
00:18:02,000 --> 00:18:05,039
yeah pretty cool where does that

468
00:18:05,039 --> 00:18:07,679
play in with like art thought does that

469
00:18:07,679 --> 00:18:08,559
i don't know

470
00:18:08,559 --> 00:18:10,000
what does that relate to our actual

471
00:18:10,000 --> 00:18:11,919
experience of thought or does this

472
00:18:11,919 --> 00:18:12,480
happen

473
00:18:12,480 --> 00:18:16,880
all at the sub personal scale

474
00:18:20,080 --> 00:18:22,400
well i mean how i knew it it's really

475
00:18:22,400 --> 00:18:23,919
like a problem of

476
00:18:23,919 --> 00:18:26,960
separating different time skills and

477
00:18:26,960 --> 00:18:29,039
certain uncertainty associated with

478
00:18:29,039 --> 00:18:31,440
different time skills

479
00:18:31,440 --> 00:18:34,559
uh what you can do now and for example

480
00:18:34,559 --> 00:18:35,360
in the next

481
00:18:35,360 --> 00:18:36,960
couple of seconds is very different from

482
00:18:36,960 --> 00:18:38,640
what you can do in the next couple of

483
00:18:38,640 --> 00:18:40,160
months

484
00:18:40,160 --> 00:18:42,080
and potentially your long-term goals and

485
00:18:42,080 --> 00:18:44,240
plans have an impact on what you're

486
00:18:44,240 --> 00:18:46,000
doing currently

487
00:18:46,000 --> 00:18:48,240
uh and there needs to be a way how to

488
00:18:48,240 --> 00:18:49,360
resolve this

489
00:18:49,360 --> 00:18:51,039
uncertainty on different levels of

490
00:18:51,039 --> 00:18:53,200
representation

491
00:18:53,200 --> 00:18:56,320
and right from active inference

492
00:18:56,320 --> 00:18:58,799
what one's solution for that is just

493
00:18:58,799 --> 00:19:00,559
like you stack

494
00:19:00,559 --> 00:19:02,480
agents on top of each other which just

495
00:19:02,480 --> 00:19:04,559
representing increasingly

496
00:19:04,559 --> 00:19:07,520
longer time skills

497
00:19:08,080 --> 00:19:11,360
interesting um so it and i mean

498
00:19:11,360 --> 00:19:13,520
as each agent i mean we can see it right

499
00:19:13,520 --> 00:19:14,799
as a little

500
00:19:14,799 --> 00:19:18,320
as a something which is separated from

501
00:19:18,320 --> 00:19:19,600
an environment through the marker

502
00:19:19,600 --> 00:19:21,440
blanket so basically through the actions

503
00:19:21,440 --> 00:19:23,200
and the observations it's making

504
00:19:23,200 --> 00:19:25,120
i mean you can also separate your brain

505
00:19:25,120 --> 00:19:26,480
in multiple

506
00:19:26,480 --> 00:19:28,799
nested blankets right where one part of

507
00:19:28,799 --> 00:19:31,280
the brain just informs and

508
00:19:31,280 --> 00:19:33,200
sends information to other parts and

509
00:19:33,200 --> 00:19:34,960
accepts signals as

510
00:19:34,960 --> 00:19:38,320
actions right so uh and

511
00:19:38,320 --> 00:19:40,000
i mean we know this from experimental

512
00:19:40,000 --> 00:19:41,520
research a lot of this is here are he

513
00:19:41,520 --> 00:19:44,400
constructed so

514
00:19:44,799 --> 00:19:47,039
all both in like temporal and like

515
00:19:47,039 --> 00:19:49,919
spatial hierarchies

516
00:19:49,919 --> 00:19:52,400
thanks dimitri stephen and then anyone

517
00:19:52,400 --> 00:19:54,640
else

518
00:19:55,200 --> 00:19:57,440
yeah would i be correct in between

519
00:19:57,440 --> 00:19:58,799
perception and planning you've

520
00:19:58,799 --> 00:20:01,679
kind of got this more conscious belief

521
00:20:01,679 --> 00:20:03,039
awareness

522
00:20:03,039 --> 00:20:05,919
being sort of available and and the

523
00:20:05,919 --> 00:20:06,880
prior beliefs

524
00:20:06,880 --> 00:20:08,799
once you get from around the

525
00:20:08,799 --> 00:20:10,400
observations to perceptions

526
00:20:10,400 --> 00:20:13,520
might be below consciousness in the

527
00:20:13,520 --> 00:20:14,799
sense that it might be part of the

528
00:20:14,799 --> 00:20:16,880
visual system or whatever

529
00:20:16,880 --> 00:20:19,440
and i was just wondering how you see

530
00:20:19,440 --> 00:20:20,559
that transition

531
00:20:20,559 --> 00:20:23,840
from you know below

532
00:20:23,840 --> 00:20:26,320
awareness to phenomenological

533
00:20:26,320 --> 00:20:28,000
consciousness

534
00:20:28,000 --> 00:20:30,000
uh playing out with the types of models

535
00:20:30,000 --> 00:20:31,039
i know sometimes

536
00:20:31,039 --> 00:20:33,280
like ryan smith uses some semi-markovian

537
00:20:33,280 --> 00:20:34,240
processes which

538
00:20:34,240 --> 00:20:37,600
start with some um

539
00:20:37,600 --> 00:20:40,320
you know self-reports about beliefs so

540
00:20:40,320 --> 00:20:42,080
it's working at a level which is

541
00:20:42,080 --> 00:20:44,799
kind of building on top of the dynamics

542
00:20:44,799 --> 00:20:46,400
that might be going on

543
00:20:46,400 --> 00:20:48,480
in the kind of biology so i was just

544
00:20:48,480 --> 00:20:50,799
wondering how you see those

545
00:20:50,799 --> 00:20:52,799
different dynamics playing out and where

546
00:20:52,799 --> 00:20:54,240
you fit your models with some other

547
00:20:54,240 --> 00:20:56,400
models if they were sort of to play

548
00:20:56,400 --> 00:20:59,280
play with each other

549
00:21:01,200 --> 00:21:03,760
um if i may say so i think this relates

550
00:21:03,760 --> 00:21:04,880
back to dimity's

551
00:21:04,880 --> 00:21:08,159
um statement about the hierarchy of time

552
00:21:08,159 --> 00:21:09,679
scales

553
00:21:09,679 --> 00:21:12,559
because um obviously if it's like my

554
00:21:12,559 --> 00:21:13,200
conscious

555
00:21:13,200 --> 00:21:16,080
thought is on a slower time scale for

556
00:21:16,080 --> 00:21:19,200
example than my visual perception is

557
00:21:19,200 --> 00:21:22,880
and i think the

558
00:21:22,880 --> 00:21:25,280
yeah my conscious thought is maybe

559
00:21:25,280 --> 00:21:27,520
rather responsible for the slower time

560
00:21:27,520 --> 00:21:28,960
scales

561
00:21:28,960 --> 00:21:32,720
where it helps me to narrate through

562
00:21:32,720 --> 00:21:35,760
the slow markov decision process

563
00:21:35,760 --> 00:21:38,960
whereas it would be not useful if i have

564
00:21:38,960 --> 00:21:39,360
to

565
00:21:39,360 --> 00:21:43,200
critically think about any inference i

566
00:21:43,200 --> 00:21:47,280
make on my retinal image for example

567
00:21:47,919 --> 00:21:49,840
it's interesting you said there that the

568
00:21:49,840 --> 00:21:51,280
conscious thought

569
00:21:51,280 --> 00:21:54,000
it narrates because first off we've

570
00:21:54,000 --> 00:21:55,200
talked about narrative

571
00:21:55,200 --> 00:21:57,280
and active inference but also just like

572
00:21:57,280 --> 00:21:59,200
you described how there was this hidden

573
00:21:59,200 --> 00:21:59,919
state

574
00:21:59,919 --> 00:22:02,080
of which flat i'm in and then that sets

575
00:22:02,080 --> 00:22:04,880
the transition probability of the rooms

576
00:22:04,880 --> 00:22:07,600
maybe you have a narrative that sets up

577
00:22:07,600 --> 00:22:08,000
like

578
00:22:08,000 --> 00:22:09,679
which policies you're transitioning

579
00:22:09,679 --> 00:22:11,679
between like in

580
00:22:11,679 --> 00:22:14,559
baseball you know there's a story to the

581
00:22:14,559 --> 00:22:16,400
alternation of the innings and then it's

582
00:22:16,400 --> 00:22:17,679
like okay we're going to be enacting

583
00:22:17,679 --> 00:22:19,679
this policy because we're in this

584
00:22:19,679 --> 00:22:22,159
phase and so there's like a higher level

585
00:22:22,159 --> 00:22:23,760
narrative that helps connect

586
00:22:23,760 --> 00:22:27,440
when policy transitions might

587
00:22:27,440 --> 00:22:30,720
be um plausible to be

588
00:22:30,720 --> 00:22:33,520
adaptive and then that is almost by

589
00:22:33,520 --> 00:22:35,600
definition at a slower time scale

590
00:22:35,600 --> 00:22:37,600
than the play-by-play because the

591
00:22:37,600 --> 00:22:38,960
play-by-play is going to be a lot more

592
00:22:38,960 --> 00:22:40,240
like motor control

593
00:22:40,240 --> 00:22:43,520
the icicating and then as you slow down

594
00:22:43,520 --> 00:22:46,640
you get slower transition dynamics

595
00:22:46,640 --> 00:22:49,520
that are more and more narrative in

596
00:22:49,520 --> 00:22:51,200
nature because they narrate like

597
00:22:51,200 --> 00:22:51,679
different

598
00:22:51,679 --> 00:22:54,080
phases of action but then the sub

599
00:22:54,080 --> 00:22:57,280
actions are very rapid

600
00:22:57,919 --> 00:22:59,679
i mean in a sense it does have to do

601
00:22:59,679 --> 00:23:01,919
with a level of abstraction right if i

602
00:23:01,919 --> 00:23:04,320
see a leaf

603
00:23:04,320 --> 00:23:07,919
it's just a leaf it has a very concrete

604
00:23:07,919 --> 00:23:08,799
meaning

605
00:23:08,799 --> 00:23:11,120
in terms of my physical environment

606
00:23:11,120 --> 00:23:12,000
whereas

607
00:23:12,000 --> 00:23:15,200
my study program for example

608
00:23:15,200 --> 00:23:18,559
um is something a lot more abstract

609
00:23:18,559 --> 00:23:21,679
that is a lot harder to grasp without

610
00:23:21,679 --> 00:23:24,720
abstract states that you maybe need a

611
00:23:24,720 --> 00:23:25,679
narrative

612
00:23:25,679 --> 00:23:28,720
for more

613
00:23:30,799 --> 00:23:36,320
um ryan and then anyone else

614
00:23:36,320 --> 00:23:38,080
oh i mean i was just gonna say i mean

615
00:23:38,080 --> 00:23:39,679
you know like when we went through

616
00:23:39,679 --> 00:23:41,760
mine and chris's paper on consciousness

617
00:23:41,760 --> 00:23:43,520
a couple months ago or whatever

618
00:23:43,520 --> 00:23:45,120
i mean the this question about how

619
00:23:45,120 --> 00:23:47,679
conscious our conscious processes

620
00:23:47,679 --> 00:23:51,520
relate to active inference really any

621
00:23:51,520 --> 00:23:54,000
related kind of computational model is

622
00:23:54,000 --> 00:23:55,200
is a complicated one

623
00:23:55,200 --> 00:23:57,919
in part because what consciousness what

624
00:23:57,919 --> 00:23:59,760
the word consciousness is used to mean

625
00:23:59,760 --> 00:24:00,640
can be different

626
00:24:00,640 --> 00:24:03,679
in different contexts you know i don't

627
00:24:03,679 --> 00:24:04,559
personally

628
00:24:04,559 --> 00:24:08,080
think that um yeah i mean

629
00:24:08,080 --> 00:24:09,760
the question of what processes are

630
00:24:09,760 --> 00:24:11,360
conscious or unconscious in and of

631
00:24:11,360 --> 00:24:12,000
themselves

632
00:24:12,000 --> 00:24:13,520
doesn't really fall out directly from

633
00:24:13,520 --> 00:24:15,440
active inference right all we're doing

634
00:24:15,440 --> 00:24:16,720
is modeling things at different time

635
00:24:16,720 --> 00:24:18,159
scales and their relationships to one

636
00:24:18,159 --> 00:24:19,120
another

637
00:24:19,120 --> 00:24:22,159
um you know i mean in in the model of

638
00:24:22,159 --> 00:24:25,120
you know uh visual consciousness uh you

639
00:24:25,120 --> 00:24:26,480
know in that paper that we published a

640
00:24:26,480 --> 00:24:27,840
few months ago

641
00:24:27,840 --> 00:24:30,080
um you know the idea was very similar to

642
00:24:30,080 --> 00:24:31,520
what other people were saying

643
00:24:31,520 --> 00:24:32,880
you know which is this idea about

644
00:24:32,880 --> 00:24:34,880
different time scales um

645
00:24:34,880 --> 00:24:37,120
but but here i mean the idea the idea

646
00:24:37,120 --> 00:24:38,400
was something more like

647
00:24:38,400 --> 00:24:39,919
you have a particular level in a

648
00:24:39,919 --> 00:24:41,600
hierarchy that

649
00:24:41,600 --> 00:24:43,360
operates over a sufficiently long time

650
00:24:43,360 --> 00:24:45,600
scale that you can do the kind of

651
00:24:45,600 --> 00:24:48,159
goal-directed things that consciousness

652
00:24:48,159 --> 00:24:49,919
allows including generating verbal

653
00:24:49,919 --> 00:24:51,679
reports which themselves are very kind

654
00:24:51,679 --> 00:24:53,039
of extended

655
00:24:53,039 --> 00:24:56,080
um you know integrative policies um

656
00:24:56,080 --> 00:24:58,799
but that um you know but that also

657
00:24:58,799 --> 00:25:00,400
requires that it integrates information

658
00:25:00,400 --> 00:25:02,640
from enough different low-level sources

659
00:25:02,640 --> 00:25:06,480
right to do that um but beyond that

660
00:25:06,480 --> 00:25:08,960
you know talking about consciousness as

661
00:25:08,960 --> 00:25:11,200
itself the slower time scale process can

662
00:25:11,200 --> 00:25:13,600
also i think be a little bit

663
00:25:13,600 --> 00:25:17,120
um sort of subtle because

664
00:25:17,120 --> 00:25:18,320
what a lot of people mean by

665
00:25:18,320 --> 00:25:20,400
consciousness um

666
00:25:20,400 --> 00:25:22,000
you know has to do with the kind of

667
00:25:22,000 --> 00:25:23,520
subjective character of the kind of like

668
00:25:23,520 --> 00:25:24,559
moments moment

669
00:25:24,559 --> 00:25:26,559
you know phenomenological aspects of

670
00:25:26,559 --> 00:25:28,480
experience and that's clearly not

671
00:25:28,480 --> 00:25:30,240
happening on a slow time scale

672
00:25:30,240 --> 00:25:32,000
right like the those the

673
00:25:32,000 --> 00:25:33,679
moment-to-moment changes in

674
00:25:33,679 --> 00:25:36,880
conscious perceptual experience are fast

675
00:25:36,880 --> 00:25:39,440
um you know so what what we're doing in

676
00:25:39,440 --> 00:25:40,320
this kind of higher

677
00:25:40,320 --> 00:25:41,679
narrative level that i think you're

678
00:25:41,679 --> 00:25:43,360
talking about is something about

679
00:25:43,360 --> 00:25:45,200
you know integrating evidence for those

680
00:25:45,200 --> 00:25:47,200
over time and you know using them to

681
00:25:47,200 --> 00:25:48,400
come up with

682
00:25:48,400 --> 00:25:50,559
um you know certain sorts of longer time

683
00:25:50,559 --> 00:25:52,080
skill policies

684
00:25:52,080 --> 00:25:53,520
to adverse certain sorts of longer term

685
00:25:53,520 --> 00:25:55,600
skill policies um

686
00:25:55,600 --> 00:25:57,760
and you know so the way that you know

687
00:25:57,760 --> 00:25:59,360
we've talked about it previously is just

688
00:25:59,360 --> 00:26:00,080
that

689
00:26:00,080 --> 00:26:02,000
you know you have you can have these

690
00:26:02,000 --> 00:26:03,919
sort of depending on the

691
00:26:03,919 --> 00:26:06,320
precision of the interactions between

692
00:26:06,320 --> 00:26:08,080
the first and the second level

693
00:26:08,080 --> 00:26:10,640
uh in a model um if that precision is

694
00:26:10,640 --> 00:26:12,640
low then the lower level can

695
00:26:12,640 --> 00:26:14,400
um to a certain extent just kind of

696
00:26:14,400 --> 00:26:17,039
operate semi-autonomously

697
00:26:17,039 --> 00:26:18,960
in which case the higher level doesn't

698
00:26:18,960 --> 00:26:20,480
really need to have all that much

699
00:26:20,480 --> 00:26:22,400
influence or know that much about

700
00:26:22,400 --> 00:26:25,279
right what's going on at the lower level

701
00:26:25,279 --> 00:26:26,960
um so you probably have as a kind of

702
00:26:26,960 --> 00:26:28,400
selection process

703
00:26:28,400 --> 00:26:30,320
where the precision gets turned up or

704
00:26:30,320 --> 00:26:32,080
down um

705
00:26:32,080 --> 00:26:33,760
at different times and for different

706
00:26:33,760 --> 00:26:36,559
hidden states um so the second level can

707
00:26:36,559 --> 00:26:38,720
kind of selectively become aware of and

708
00:26:38,720 --> 00:26:39,760
start integrating

709
00:26:39,760 --> 00:26:42,240
evidence with respect to certain lower

710
00:26:42,240 --> 00:26:43,520
level processes

711
00:26:43,520 --> 00:26:45,360
which simultaneously allows this kind of

712
00:26:45,360 --> 00:26:46,559
top-down control

713
00:26:46,559 --> 00:26:49,360
over those first level processes um you

714
00:26:49,360 --> 00:26:49,919
know so

715
00:26:49,919 --> 00:26:52,000
so with the perceptual phenomenology

716
00:26:52,000 --> 00:26:54,080
stuff it's probably more about

717
00:26:54,080 --> 00:26:56,720
um the moment-to-moment updates between

718
00:26:56,720 --> 00:26:58,559
lower level perceptual processes

719
00:26:58,559 --> 00:27:00,240
and these higher levels or timescale

720
00:27:00,240 --> 00:27:02,240
processes when the precision is

721
00:27:02,240 --> 00:27:03,120
sufficient

722
00:27:03,120 --> 00:27:04,840
for those updates to the second level to

723
00:27:04,840 --> 00:27:07,679
matter but anyway i mean that's

724
00:27:07,679 --> 00:27:09,360
more long-winded than i meant it to be

725
00:27:09,360 --> 00:27:11,120
but mainly just trying to point out that

726
00:27:11,120 --> 00:27:11,760
it's

727
00:27:11,760 --> 00:27:13,200
it's subtle and it means a lot and it

728
00:27:13,200 --> 00:27:14,320
depends a lot on what you mean by

729
00:27:14,320 --> 00:27:15,279
consciousness

730
00:27:15,279 --> 00:27:17,919
and it's not specific to active

731
00:27:17,919 --> 00:27:21,120
inference necessarily

732
00:27:21,120 --> 00:27:23,840
nice great i i think i mean the

733
00:27:23,840 --> 00:27:25,120
difficulty when talking about

734
00:27:25,120 --> 00:27:26,159
consciousness is

735
00:27:26,159 --> 00:27:29,600
i don't i mean we don't know what is

736
00:27:29,600 --> 00:27:32,799
what it is doing right

737
00:27:33,520 --> 00:27:35,120
what kind of problem is consciousness

738
00:27:35,120 --> 00:27:36,720
solving

739
00:27:36,720 --> 00:27:39,679
what would be like a hyper-intelligent

740
00:27:39,679 --> 00:27:40,480
uh

741
00:27:40,480 --> 00:27:41,840
organism without the conscious

742
00:27:41,840 --> 00:27:43,919
consciousness yeah i mean i think there

743
00:27:43,919 --> 00:27:44,880
are definitely there are definitely a

744
00:27:44,880 --> 00:27:46,480
number of ideas in the literature it's

745
00:27:46,480 --> 00:27:47,039
not

746
00:27:47,039 --> 00:27:48,320
you know it's not like it's completely

747
00:27:48,320 --> 00:27:50,720
pinned down right i mean like

748
00:27:50,720 --> 00:27:53,120
there are there are things for instance

749
00:27:53,120 --> 00:27:54,240
like requiring

750
00:27:54,240 --> 00:27:57,200
um working memory maintenance right like

751
00:27:57,200 --> 00:27:58,799
like extended working memory maintenance

752
00:27:58,799 --> 00:28:00,159
beyond a specific time scale

753
00:28:00,159 --> 00:28:02,000
looks like it can't happen unconsciously

754
00:28:02,000 --> 00:28:03,679
at least in current experiments

755
00:28:03,679 --> 00:28:06,720
um any kind of like multi-step um

756
00:28:06,720 --> 00:28:08,880
mental processes like for instance like

757
00:28:08,880 --> 00:28:10,559
um you can get unconscious priming

758
00:28:10,559 --> 00:28:11,840
effects where people can do something

759
00:28:11,840 --> 00:28:13,279
like two plus two

760
00:28:13,279 --> 00:28:15,520
um but you can't them but they can't

761
00:28:15,520 --> 00:28:16,480
unconsciously do

762
00:28:16,480 --> 00:28:18,960
two plus two minus four um right so

763
00:28:18,960 --> 00:28:20,240
anything that requires holding this kind

764
00:28:20,240 --> 00:28:21,520
of intermediate result

765
00:28:21,520 --> 00:28:24,080
in mind um to do a further operation on

766
00:28:24,080 --> 00:28:25,760
it is something that at least thus far

767
00:28:25,760 --> 00:28:27,200
nobody's able to

768
00:28:27,200 --> 00:28:29,440
do an experiment to um you know that

769
00:28:29,440 --> 00:28:30,480
people can do that

770
00:28:30,480 --> 00:28:34,320
unconsciously um there's um

771
00:28:34,320 --> 00:28:36,399
but you know what ryan would you think

772
00:28:36,399 --> 00:28:38,320
that consciousness is like a zero one

773
00:28:38,320 --> 00:28:39,200
state i mean

774
00:28:39,200 --> 00:28:42,000
from that respect right i mean right for

775
00:28:42,000 --> 00:28:43,520
example giulio tanoni right

776
00:28:43,520 --> 00:28:46,000
kind of sees this as a continuum like

777
00:28:46,000 --> 00:28:47,120
consciousness

778
00:28:47,120 --> 00:28:48,880
is really a continuum of states where

779
00:28:48,880 --> 00:28:50,559
you can just have higher levels and

780
00:28:50,559 --> 00:28:52,320
lower levels of consciousness

781
00:28:52,320 --> 00:28:54,880
well so so again i mean this gets back

782
00:28:54,880 --> 00:28:55,919
to what

783
00:28:55,919 --> 00:28:57,600
what you're using consciousness to mean

784
00:28:57,600 --> 00:28:58,960
right so in consciousness research

785
00:28:58,960 --> 00:29:00,480
there's a distinction between levels of

786
00:29:00,480 --> 00:29:01,200
consciousness

787
00:29:01,200 --> 00:29:04,240
and content of consciousness um now like

788
00:29:04,240 --> 00:29:05,919
content of consciousness is

789
00:29:05,919 --> 00:29:08,640
i think most people agree it's fairly

790
00:29:08,640 --> 00:29:09,760
binary

791
00:29:09,760 --> 00:29:12,399
i'm actually not totally sure what the

792
00:29:12,399 --> 00:29:13,520
um

793
00:29:13,520 --> 00:29:16,880
um what the iit crowd would

794
00:29:16,880 --> 00:29:19,760
say i'd have to kind of look back at um

795
00:29:19,760 --> 00:29:21,360
look back at some of the more recent

796
00:29:21,360 --> 00:29:22,080
stuff there

797
00:29:22,080 --> 00:29:24,559
but but um in terms of neuroscience you

798
00:29:24,559 --> 00:29:26,000
know it's it's

799
00:29:26,000 --> 00:29:27,520
it's very well characterized we're

800
00:29:27,520 --> 00:29:29,120
called like ignition events

801
00:29:29,120 --> 00:29:31,840
that are that are non-linear all or none

802
00:29:31,840 --> 00:29:33,360
you know sorts of processes where if

803
00:29:33,360 --> 00:29:35,039
something becomes conscious you get this

804
00:29:35,039 --> 00:29:37,200
global non-linear just kind of

805
00:29:37,200 --> 00:29:40,640
broad uh activation widely across

806
00:29:40,640 --> 00:29:42,000
large-scale networks

807
00:29:42,000 --> 00:29:43,919
whereas if you don't get it you don't

808
00:29:43,919 --> 00:29:45,600
pass this like ignition threshold then

809
00:29:45,600 --> 00:29:47,120
you still get the local

810
00:29:47,120 --> 00:29:49,840
like perceptual like local activation in

811
00:29:49,840 --> 00:29:52,000
let's say visual cortex provision

812
00:29:52,000 --> 00:29:55,200
or whatever sensory cortex um but you

813
00:29:55,200 --> 00:29:55,679
don't

814
00:29:55,679 --> 00:29:58,399
get but it's just kind of linear and it

815
00:29:58,399 --> 00:29:58,880
doesn't

816
00:29:58,880 --> 00:30:00,320
kind of percolate up or pass this

817
00:30:00,320 --> 00:30:01,919
threshold to cause this large scale kind

818
00:30:01,919 --> 00:30:02,159
of

819
00:30:02,159 --> 00:30:04,480
more all or not kind of thing where that

820
00:30:04,480 --> 00:30:05,840
where the information becomes kind of

821
00:30:05,840 --> 00:30:07,200
broadly accessible throughout the rest

822
00:30:07,200 --> 00:30:08,320
of the system

823
00:30:08,320 --> 00:30:11,440
um so that that aspect i think has a

824
00:30:11,440 --> 00:30:13,679
fairly binary character to it

825
00:30:13,679 --> 00:30:16,159
um that's different than this kind of

826
00:30:16,159 --> 00:30:17,919
levels of consciousness

827
00:30:17,919 --> 00:30:19,600
and issue which is something kind of

828
00:30:19,600 --> 00:30:21,279
like the

829
00:30:21,279 --> 00:30:23,279
being in a predisposition to represent

830
00:30:23,279 --> 00:30:24,799
conscious contents

831
00:30:24,799 --> 00:30:26,240
um which would kind of be like a

832
00:30:26,240 --> 00:30:28,159
continuum from say like coma

833
00:30:28,159 --> 00:30:31,279
to like alert awareness um and that that

834
00:30:31,279 --> 00:30:32,559
probably just has to do with kind of

835
00:30:32,559 --> 00:30:34,080
like the state of cortical processing

836
00:30:34,080 --> 00:30:34,840
that

837
00:30:34,840 --> 00:30:38,399
um allows for uh

838
00:30:38,399 --> 00:30:40,720
the the kind of dynamics that support

839
00:30:40,720 --> 00:30:42,399
this kind of all or none

840
00:30:42,399 --> 00:30:44,559
um the the stuff that allows for

841
00:30:44,559 --> 00:30:45,919
selective contents

842
00:30:45,919 --> 00:30:48,240
um but anyway i mean all this is going

843
00:30:48,240 --> 00:30:49,600
pretty far afield from

844
00:30:49,600 --> 00:30:52,000
from uh active inference in your guys's

845
00:30:52,000 --> 00:30:52,960
paper now so

846
00:30:52,960 --> 00:30:54,880
i don't want to uh go to the tractor

847
00:30:54,880 --> 00:30:56,159
delay

848
00:30:56,159 --> 00:30:59,919
it's fun stuff and it's important so um

849
00:30:59,919 --> 00:31:03,440
we'll go to a more applied

850
00:31:03,440 --> 00:31:06,320
question and then following this kind of

851
00:31:06,320 --> 00:31:07,919
round on an applied question

852
00:31:07,919 --> 00:31:11,919
we're going to go into a few code

853
00:31:11,919 --> 00:31:14,320
walk slash talk throughs as well as

854
00:31:14,320 --> 00:31:16,240
learn a little bit about a few

855
00:31:16,240 --> 00:31:18,799
approximations and some of other work by

856
00:31:18,799 --> 00:31:19,679
sarah

857
00:31:19,679 --> 00:31:22,320
so lars asked us a question on twitter

858
00:31:22,320 --> 00:31:22,799
which

859
00:31:22,799 --> 00:31:26,320
anyone is always welcome to do and wrote

860
00:31:26,320 --> 00:31:29,120
i would love to hear any thoughts on how

861
00:31:29,120 --> 00:31:29,519
this

862
00:31:29,519 --> 00:31:31,600
work on the multi-armed bandit might be

863
00:31:31,600 --> 00:31:34,000
related to real world problem spaces or

864
00:31:34,000 --> 00:31:35,279
applications

865
00:31:35,279 --> 00:31:37,120
how might active improvements in

866
00:31:37,120 --> 00:31:38,960
multi-armed bandit tasks

867
00:31:38,960 --> 00:31:41,679
translate to improving how some problems

868
00:31:41,679 --> 00:31:42,000
or

869
00:31:42,000 --> 00:31:45,200
decisions are currently solved we talked

870
00:31:45,200 --> 00:31:46,000
a little bit about

871
00:31:46,000 --> 00:31:49,440
this in dot zero and dot one but i'll

872
00:31:49,440 --> 00:31:52,640
go to any of the authors for a first

873
00:31:52,640 --> 00:31:55,039
take and then everyone else is welcome

874
00:31:55,039 --> 00:31:56,799
to give any thoughts or like ask a

875
00:31:56,799 --> 00:31:57,440
follow-up

876
00:31:57,440 --> 00:32:00,240
question so when you present the work

877
00:32:00,240 --> 00:32:02,240
and somebody just goes to this kind of

878
00:32:02,240 --> 00:32:04,320
obvious applied active inference

879
00:32:04,320 --> 00:32:05,440
question

880
00:32:05,440 --> 00:32:08,399
what is your thought

881
00:32:08,799 --> 00:32:10,960
well i mean as i said right multi-arm

882
00:32:10,960 --> 00:32:13,039
bandits are applied to

883
00:32:13,039 --> 00:32:15,519
a super wide range of real world

884
00:32:15,519 --> 00:32:16,559
applications

885
00:32:16,559 --> 00:32:19,840
so i opened a survey like a survey on

886
00:32:19,840 --> 00:32:21,600
practical applications of multi-arm

887
00:32:21,600 --> 00:32:24,799
bandits and contextual bandits

888
00:32:24,799 --> 00:32:29,279
one bone f and irish

889
00:32:29,279 --> 00:32:32,640
irina rich and jalal bonifo right uh

890
00:32:32,640 --> 00:32:34,080
so this is like one of the recent

891
00:32:34,080 --> 00:32:36,399
surveys i found 2019

892
00:32:36,399 --> 00:32:40,840
on archive uh and they hear the list for

893
00:32:40,840 --> 00:32:42,640
example couple of

894
00:32:42,640 --> 00:32:44,080
domains where they are applied

895
00:32:44,080 --> 00:32:46,159
healthcare finance dynamic pricing

896
00:32:46,159 --> 00:32:47,360
recommender system

897
00:32:47,360 --> 00:32:51,039
maximi maximization

898
00:32:51,039 --> 00:32:54,840
dialog system telecommunications anomaly

899
00:32:54,840 --> 00:32:56,240
detection and

900
00:32:56,240 --> 00:32:59,360
i mean just by looking from that like i

901
00:32:59,360 --> 00:33:00,000
would

902
00:33:00,000 --> 00:33:01,840
first try things out with active

903
00:33:01,840 --> 00:33:03,760
inference based bandits in

904
00:33:03,760 --> 00:33:06,960
kind of non-stationary problems

905
00:33:06,960 --> 00:33:10,159
and this is from what i see dynamic

906
00:33:10,159 --> 00:33:11,760
pricing and your commander system they

907
00:33:11,760 --> 00:33:12,480
are kind of

908
00:33:12,480 --> 00:33:16,000
in this domain quite obviously

909
00:33:16,000 --> 00:33:19,440
uh but one can also imagine that right

910
00:33:19,440 --> 00:33:21,519
uh telecommunication systems would also

911
00:33:21,519 --> 00:33:23,600
be a non-stationary problem and as kind

912
00:33:23,600 --> 00:33:24,080
of

913
00:33:24,080 --> 00:33:26,960
searching for the fastest routing path

914
00:33:26,960 --> 00:33:28,559
and similar

915
00:33:28,559 --> 00:33:30,720
uh where different routes can change

916
00:33:30,720 --> 00:33:32,000
over time and then you have

917
00:33:32,000 --> 00:33:34,399
constantly ready to explore different

918
00:33:34,399 --> 00:33:36,159
channels

919
00:33:36,159 --> 00:33:38,880
for for passing along the information

920
00:33:38,880 --> 00:33:40,799
more optimally and right this is a kind

921
00:33:40,799 --> 00:33:41,120
of

922
00:33:41,120 --> 00:33:44,799
uh practical uh

923
00:33:44,799 --> 00:33:48,880
uh domains where one could first try

924
00:33:48,880 --> 00:33:51,519
uh multi uh active inference based

925
00:33:51,519 --> 00:33:53,760
bandits

926
00:33:53,760 --> 00:33:56,320
um in as i study non-stationary kind of

927
00:33:56,320 --> 00:33:57,279
problems

928
00:33:57,279 --> 00:33:59,120
which is most of the other things i

929
00:33:59,120 --> 00:34:01,200
listed um this is a bit more difficult

930
00:34:01,200 --> 00:34:02,720
but

931
00:34:02,720 --> 00:34:05,840
first to see if there is a way to deal

932
00:34:05,840 --> 00:34:07,039
with um

933
00:34:07,039 --> 00:34:10,000
this kind of uh asymptotic bad

934
00:34:10,000 --> 00:34:11,359
asymptotic behavior

935
00:34:11,359 --> 00:34:14,560
basically over optimistic uh

936
00:34:14,560 --> 00:34:20,399
information search

937
00:34:20,399 --> 00:34:22,800
awesome sarah any thoughts on that or

938
00:34:22,800 --> 00:34:25,280
anyone else

939
00:34:27,359 --> 00:34:31,199
stephen yeah could you just repeat that

940
00:34:31,199 --> 00:34:32,239
last bit this

941
00:34:32,239 --> 00:34:34,800
you said something about isotropic

942
00:34:34,800 --> 00:34:36,560
behavior or some sort of type of

943
00:34:36,560 --> 00:34:38,000
behavior i didn't quite catch

944
00:34:38,000 --> 00:34:40,560
asymptotic right so this is like in in

945
00:34:40,560 --> 00:34:41,359
stationary

946
00:34:41,359 --> 00:34:43,359
bandits people are interested in the

947
00:34:43,359 --> 00:34:45,679
asymptotic behavior after like infinite

948
00:34:45,679 --> 00:34:47,040
many

949
00:34:47,040 --> 00:34:50,239
actions so to say uh and how the

950
00:34:50,239 --> 00:34:52,639
how different algorithms scale there

951
00:34:52,639 --> 00:34:54,639
whether they converge to a good solution

952
00:34:54,639 --> 00:34:55,760
or not

953
00:34:55,760 --> 00:34:57,839
and i mean this is what we find in uh

954
00:34:57,839 --> 00:34:58,960
right

955
00:34:58,960 --> 00:35:00,640
in the stationary problems active

956
00:35:00,640 --> 00:35:02,960
inferences uh too optimistic in a way

957
00:35:02,960 --> 00:35:07,119
it converges to a solution too fast

958
00:35:07,119 --> 00:35:11,119
as the way we define it at least

959
00:35:11,520 --> 00:35:13,359
and and that that's why it doesn't have

960
00:35:13,359 --> 00:35:15,599
good asymptotic behavior so if you would

961
00:35:15,599 --> 00:35:19,119
apply to non stationary problem

962
00:35:19,119 --> 00:35:20,560
when you would like with high

963
00:35:20,560 --> 00:35:22,720
probability to find good solution

964
00:35:22,720 --> 00:35:25,440
i mean active inference algorithm is not

965
00:35:25,440 --> 00:35:28,320
one to go for

966
00:35:28,839 --> 00:35:31,839
there

967
00:35:33,520 --> 00:35:36,480
again we have some ideas how to change

968
00:35:36,480 --> 00:35:37,200
this but

969
00:35:37,200 --> 00:35:39,200
i mean just based on expected free

970
00:35:39,200 --> 00:35:40,720
energy this is not something which

971
00:35:40,720 --> 00:35:42,800
behaves nice nicely

972
00:35:42,800 --> 00:35:45,119
so you almost have to do like a hack on

973
00:35:45,119 --> 00:35:46,160
it to keep

974
00:35:46,160 --> 00:35:49,680
bringing it back away from premature or

975
00:35:49,680 --> 00:35:51,920
yeah exactly it kind of just gets stuck

976
00:35:51,920 --> 00:35:53,839
prematurely into a

977
00:35:53,839 --> 00:35:56,480
into a solution which the agent believes

978
00:35:56,480 --> 00:35:58,000
it's a good one

979
00:35:58,000 --> 00:35:59,760
and the reason for this one can see

980
00:35:59,760 --> 00:36:01,440
immediately just from this

981
00:36:01,440 --> 00:36:04,560
from the term which drives the

982
00:36:04,560 --> 00:36:06,910
exploration which is this uh

983
00:36:06,910 --> 00:36:08,560
[Music]

984
00:36:08,560 --> 00:36:11,680
let's just expected information game

985
00:36:11,680 --> 00:36:13,839
right in reinforcement learning this is

986
00:36:13,839 --> 00:36:15,440
called like exploration bonus

987
00:36:15,440 --> 00:36:18,480
or something like this and this this

988
00:36:18,480 --> 00:36:20,880
doesn't increase with time

989
00:36:20,880 --> 00:36:24,000
so in a way uh all these previous

990
00:36:24,000 --> 00:36:25,520
algorithms like uh

991
00:36:25,520 --> 00:36:28,240
upper confidence bound ucb uh the

992
00:36:28,240 --> 00:36:29,760
reinforcement learning based algorithms

993
00:36:29,760 --> 00:36:31,839
they have a bound which increases with

994
00:36:31,839 --> 00:36:33,599
time so if you are not

995
00:36:33,599 --> 00:36:36,720
sampling from one arm uh this bound

996
00:36:36,720 --> 00:36:37,920
becomes bigger so your

997
00:36:37,920 --> 00:36:40,720
algorithm is kind of forced to switch at

998
00:36:40,720 --> 00:36:41,440
some point

999
00:36:41,440 --> 00:36:44,000
uh and this is not happening here so

1000
00:36:44,000 --> 00:36:45,119
right

1001
00:36:45,119 --> 00:36:46,400
either one would need a different

1002
00:36:46,400 --> 00:36:48,640
generality model or a different way

1003
00:36:48,640 --> 00:36:51,200
to introduce more randomness into into

1004
00:36:51,200 --> 00:36:53,200
the behavior

1005
00:36:53,200 --> 00:36:56,800
one aspect of um it was uh figure

1006
00:36:56,800 --> 00:36:59,520
one or no this figure figure two in the

1007
00:36:59,520 --> 00:37:00,320
paper

1008
00:37:00,320 --> 00:37:04,000
was that that uh on the top left there

1009
00:37:04,000 --> 00:37:06,800
that the variance across active

1010
00:37:06,800 --> 00:37:08,000
inference agents was

1011
00:37:08,000 --> 00:37:10,720
increasing so it wasn't that like every

1012
00:37:10,720 --> 00:37:11,839
instance

1013
00:37:11,839 --> 00:37:14,880
was um slightly degrading in performance

1014
00:37:14,880 --> 00:37:17,599
it was actually that a small subset what

1015
00:37:17,599 --> 00:37:18,800
you wrote there

1016
00:37:18,800 --> 00:37:21,200
a small percentage of the ensemble did

1017
00:37:21,200 --> 00:37:22,800
not find the accurate solution

1018
00:37:22,800 --> 00:37:24,240
and were over confident in their

1019
00:37:24,240 --> 00:37:26,240
estimate so it

1020
00:37:26,240 --> 00:37:28,960
it does suggest a few ways in which

1021
00:37:28,960 --> 00:37:30,640
maybe you know when the variance of a

1022
00:37:30,640 --> 00:37:31,920
few parallel

1023
00:37:31,920 --> 00:37:33,440
instances of active inference starts

1024
00:37:33,440 --> 00:37:35,359
diverging that could be like a

1025
00:37:35,359 --> 00:37:38,800
a warning sign that some of them are

1026
00:37:38,800 --> 00:37:41,280
getting too confident too early and then

1027
00:37:41,280 --> 00:37:42,960
also it's interesting how you explored

1028
00:37:42,960 --> 00:37:43,440
that

1029
00:37:43,440 --> 00:37:46,880
um the learning rate um or

1030
00:37:46,880 --> 00:37:49,680
lambda through time and just kind of

1031
00:37:49,680 --> 00:37:50,240
said that

1032
00:37:50,240 --> 00:37:53,359
okay there's no simple answer here but

1033
00:37:53,359 --> 00:37:56,079
it's it's an area of future work for

1034
00:37:56,079 --> 00:37:57,520
sure

1035
00:37:57,520 --> 00:37:58,960
yeah i mean the problem with like

1036
00:37:58,960 --> 00:38:01,040
parallel runs in practical applications

1037
00:38:01,040 --> 00:38:02,880
you don't have that

1038
00:38:02,880 --> 00:38:06,480
uh how do you say um

1039
00:38:06,480 --> 00:38:08,480
that advantage is to keep in like

1040
00:38:08,480 --> 00:38:10,800
simulations i can just run

1041
00:38:10,800 --> 00:38:12,320
i mean any number of simulations just

1042
00:38:12,320 --> 00:38:14,400
see how it behaves right in practice

1043
00:38:14,400 --> 00:38:15,760
when you're solving the problem you just

1044
00:38:15,760 --> 00:38:18,720
have one trajectory and then you have to

1045
00:38:18,720 --> 00:38:21,599
kind of provide insurance that this

1046
00:38:21,599 --> 00:38:24,079
sample or this sequence of actions

1047
00:38:24,079 --> 00:38:28,560
will behave better than random or better

1048
00:38:28,560 --> 00:38:30,400
and that there is some probability to

1049
00:38:30,400 --> 00:38:31,599
converge over

1050
00:38:31,599 --> 00:38:34,480
over long over long run and this is

1051
00:38:34,480 --> 00:38:35,760
something which you don't get

1052
00:38:35,760 --> 00:38:39,200
right with uh

1053
00:38:39,200 --> 00:38:40,800
uh with active inference in stationary

1054
00:38:40,800 --> 00:38:42,160
problems at least right in

1055
00:38:42,160 --> 00:38:43,839
non-stationary we don't see this

1056
00:38:43,839 --> 00:38:46,400
uh issue anymore because of the

1057
00:38:46,400 --> 00:38:49,119
basically the generative model itself

1058
00:38:49,119 --> 00:38:51,280
because the more you are the less you're

1059
00:38:51,280 --> 00:38:52,480
exploring the more

1060
00:38:52,480 --> 00:38:54,960
your uncertainty rises on the arms which

1061
00:38:54,960 --> 00:38:56,800
you haven't observed

1062
00:38:56,800 --> 00:38:58,800
uh because the agent believes that

1063
00:38:58,800 --> 00:39:00,160
things will change

1064
00:39:00,160 --> 00:39:03,359
over time simply this is also what

1065
00:39:03,359 --> 00:39:06,400
helps it shift

1066
00:39:06,400 --> 00:39:09,040
between arms and it's also very

1067
00:39:09,040 --> 00:39:10,720
efficient to

1068
00:39:10,720 --> 00:39:12,839
kind of extract information from them

1069
00:39:12,839 --> 00:39:14,320
because

1070
00:39:14,320 --> 00:39:16,400
agent picks up really fast what what

1071
00:39:16,400 --> 00:39:18,079
kind of arms were not simple

1072
00:39:18,079 --> 00:39:22,880
and it's aligned with its beliefs

1073
00:39:23,359 --> 00:39:25,520
i wonder if this also might reflect the

1074
00:39:25,520 --> 00:39:26,400
difference between

1075
00:39:26,400 --> 00:39:29,040
optimizing the model for practical

1076
00:39:29,040 --> 00:39:30,320
application

1077
00:39:30,320 --> 00:39:33,680
in i don't know a computer simulation

1078
00:39:33,680 --> 00:39:34,640
program

1079
00:39:34,640 --> 00:39:36,960
and the way that organisms behave you

1080
00:39:36,960 --> 00:39:38,720
know it may be that

1081
00:39:38,720 --> 00:39:42,079
you know we we prematurely shut off and

1082
00:39:42,079 --> 00:39:44,480
let things become a habit

1083
00:39:44,480 --> 00:39:46,800
but that may have downsides you know

1084
00:39:46,800 --> 00:39:47,839
maybe that

1085
00:39:47,839 --> 00:39:50,720
you know organisms just to minimize use

1086
00:39:50,720 --> 00:39:52,160
of energy

1087
00:39:52,160 --> 00:39:54,079
prematurely converge on something

1088
00:39:54,079 --> 00:39:55,680
inaccurate and so

1089
00:39:55,680 --> 00:39:57,440
maybe that's like gambling than that

1090
00:39:57,440 --> 00:39:58,800
could it could show

1091
00:39:58,800 --> 00:40:02,320
a fragility at times so there's

1092
00:40:02,320 --> 00:40:04,240
maybe two ways that it's being applied

1093
00:40:04,240 --> 00:40:06,480
you know

1094
00:40:06,480 --> 00:40:08,240
yeah i mean i would say that organisms

1095
00:40:08,240 --> 00:40:09,920
are never exposed to a stationary

1096
00:40:09,920 --> 00:40:11,040
environment so

1097
00:40:11,040 --> 00:40:12,720
right i mean in a way if you are not

1098
00:40:12,720 --> 00:40:14,640
doing that you are suboptimal

1099
00:40:14,640 --> 00:40:16,319
because you are living in environment it

1100
00:40:16,319 --> 00:40:17,920
changes constantly

1101
00:40:17,920 --> 00:40:21,920
uh right so in a way you can exploit

1102
00:40:21,920 --> 00:40:22,720
this by now

1103
00:40:22,720 --> 00:40:26,160
creating situations where uh

1104
00:40:26,160 --> 00:40:28,880
well people behave weird and you have

1105
00:40:28,880 --> 00:40:30,640
gambling issues and stuff

1106
00:40:30,640 --> 00:40:34,400
but right um i don't think that's kind

1107
00:40:34,400 --> 00:40:35,599
of disadvantage for

1108
00:40:35,599 --> 00:40:39,040
well for what we evolved to do actually

1109
00:40:39,040 --> 00:40:42,960
uh for that we are very good in doing

1110
00:40:42,960 --> 00:40:46,640
finding good solutions reasonably fast

1111
00:40:46,640 --> 00:40:48,720
yeah sorry i was just i was just curious

1112
00:40:48,720 --> 00:40:49,839
i mean because

1113
00:40:49,839 --> 00:40:51,119
you know i mean this is something that

1114
00:40:51,119 --> 00:40:52,720
you know like with some of our like

1115
00:40:52,720 --> 00:40:54,400
empirical work we've run into

1116
00:40:54,400 --> 00:40:56,160
you know so like when trying to model uh

1117
00:40:56,160 --> 00:40:57,839
like change point detection tasks for

1118
00:40:57,839 --> 00:40:58,720
example

1119
00:40:58,720 --> 00:41:01,680
um using active inference where um yeah

1120
00:41:01,680 --> 00:41:03,119
like same thing like the thing becomes

1121
00:41:03,119 --> 00:41:05,119
too confident too quickly

1122
00:41:05,119 --> 00:41:08,240
um but i wondered i mean because i mean

1123
00:41:08,240 --> 00:41:08,720
in

1124
00:41:08,720 --> 00:41:10,240
in a lot of other empirical work like in

1125
00:41:10,240 --> 00:41:11,920
neuroscience especially like it's it's

1126
00:41:11,920 --> 00:41:12,960
pretty clear that people

1127
00:41:12,960 --> 00:41:16,240
don't just kind of learn and then

1128
00:41:16,240 --> 00:41:19,200
unlearn the like like reward

1129
00:41:19,200 --> 00:41:20,960
probabilities or just whatever the kind

1130
00:41:20,960 --> 00:41:21,280
of

1131
00:41:21,280 --> 00:41:23,440
environmental statistics are like when

1132
00:41:23,440 --> 00:41:25,119
there's like abrupt changes

1133
00:41:25,119 --> 00:41:27,599
right i mean what what people do is

1134
00:41:27,599 --> 00:41:29,200
instead they infer that there's some new

1135
00:41:29,200 --> 00:41:29,680
hidden

1136
00:41:29,680 --> 00:41:31,200
cause right there's some new light in

1137
00:41:31,200 --> 00:41:32,960
context and then

1138
00:41:32,960 --> 00:41:35,440
under that new context you basically

1139
00:41:35,440 --> 00:41:36,240
just have

1140
00:41:36,240 --> 00:41:39,200
really flat low you know really really

1141
00:41:39,200 --> 00:41:40,319
small

1142
00:41:40,319 --> 00:41:42,000
you know like magnitude concentration

1143
00:41:42,000 --> 00:41:44,240
parameters and then you just build up

1144
00:41:44,240 --> 00:41:46,880
right like your beliefs anew under that

1145
00:41:46,880 --> 00:41:48,240
new context

1146
00:41:48,240 --> 00:41:50,560
um and so there's there's like something

1147
00:41:50,560 --> 00:41:52,240
so there's like that kind of approach

1148
00:41:52,240 --> 00:41:52,960
which

1149
00:41:52,960 --> 00:41:55,920
either would require having some

1150
00:41:55,920 --> 00:41:57,599
something hierarchical or having some

1151
00:41:57,599 --> 00:41:58,720
kind of like

1152
00:41:58,720 --> 00:42:00,160
additional hidden state factor that

1153
00:42:00,160 --> 00:42:02,160
would correspond to contact

1154
00:42:02,160 --> 00:42:03,520
right and you know it seems like you

1155
00:42:03,520 --> 00:42:05,200
could do it either of those ways

1156
00:42:05,200 --> 00:42:08,880
um the um then the the other thing to do

1157
00:42:08,880 --> 00:42:10,000
would be to have some kind of

1158
00:42:10,000 --> 00:42:11,839
like in the hp like in the hierarchical

1159
00:42:11,839 --> 00:42:13,440
calcium filter where you have some kind

1160
00:42:13,440 --> 00:42:14,319
of uh

1161
00:42:14,319 --> 00:42:16,720
like dynamically adjusted learning rate

1162
00:42:16,720 --> 00:42:17,440
um

1163
00:42:17,440 --> 00:42:19,040
you know or or really something more

1164
00:42:19,040 --> 00:42:21,520
like um you know so in

1165
00:42:21,520 --> 00:42:23,760
like recently when we were updating um

1166
00:42:23,760 --> 00:42:26,400
some of this with our tutorial paper

1167
00:42:26,400 --> 00:42:29,200
um you know like after talking with carl

1168
00:42:29,200 --> 00:42:29,680
um

1169
00:42:29,680 --> 00:42:30,960
you know it seemed like it would be a

1170
00:42:30,960 --> 00:42:32,800
good idea to also include this kind of

1171
00:42:32,800 --> 00:42:34,160
like forgetting rate parameter as

1172
00:42:34,160 --> 00:42:35,280
opposed to just like the standard

1173
00:42:35,280 --> 00:42:36,640
learning rate parameter

1174
00:42:36,640 --> 00:42:37,839
which is just kind of like a scalar

1175
00:42:37,839 --> 00:42:40,640
parameter on the actual um

1176
00:42:40,640 --> 00:42:42,880
on the actual concentration parameters

1177
00:42:42,880 --> 00:42:44,079
right prior to

1178
00:42:44,079 --> 00:42:46,640
prior to adding on the new counts um you

1179
00:42:46,640 --> 00:42:47,680
know and so you could

1180
00:42:47,680 --> 00:42:49,839
that's not dynamic but but at least it

1181
00:42:49,839 --> 00:42:51,359
does um

1182
00:42:51,359 --> 00:42:54,079
prevent you know it can act as kind of

1183
00:42:54,079 --> 00:42:54,640
like a

1184
00:42:54,640 --> 00:42:56,160
something like an implicit prior

1185
00:42:56,160 --> 00:42:57,440
volatility that can prevent the thing

1186
00:42:57,440 --> 00:42:59,680
from becoming too confident too quickly

1187
00:42:59,680 --> 00:43:03,280
um but um but yeah i guess i just um

1188
00:43:03,280 --> 00:43:06,240
um i wondered if you know you guys had

1189
00:43:06,240 --> 00:43:07,680
like thought about or you know like

1190
00:43:07,680 --> 00:43:09,280
played around at all with with something

1191
00:43:09,280 --> 00:43:11,359
like that something like inferring

1192
00:43:11,359 --> 00:43:13,520
inferring new kind of like latent

1193
00:43:13,520 --> 00:43:15,359
context as opposed to just having to

1194
00:43:15,359 --> 00:43:16,240
kind of like

1195
00:43:16,240 --> 00:43:17,760
you know like the thing becoming too

1196
00:43:17,760 --> 00:43:19,839
confident and uh having to spend a bunch

1197
00:43:19,839 --> 00:43:21,200
of time over writing

1198
00:43:21,200 --> 00:43:23,359
right it's uh you know it's old it's old

1199
00:43:23,359 --> 00:43:26,720
beliefs which happens way too slow

1200
00:43:26,720 --> 00:43:30,640
i mean in the end um my current

1201
00:43:30,640 --> 00:43:33,359
model um is actually based on such a

1202
00:43:33,359 --> 00:43:35,359
hierarchical model as you describe

1203
00:43:35,359 --> 00:43:38,880
where my agent instead of unlearning

1204
00:43:38,880 --> 00:43:41,520
all action outcome contingencies it um

1205
00:43:41,520 --> 00:43:44,880
opens up a new context

1206
00:43:45,440 --> 00:43:47,440
of have you inferred that the context

1207
00:43:47,440 --> 00:43:48,640
changed

1208
00:43:48,640 --> 00:43:52,400
and i think um when we were looking for

1209
00:43:52,400 --> 00:43:56,000
proper learning algorithms for the paper

1210
00:43:56,000 --> 00:43:58,720
we most definitely looked in contextual

1211
00:43:58,720 --> 00:43:59,760
learning

1212
00:43:59,760 --> 00:44:03,920
and um i think yeah in the stationary

1213
00:44:03,920 --> 00:44:04,720
case

1214
00:44:04,720 --> 00:44:08,400
there are no contexts um

1215
00:44:08,400 --> 00:44:10,800
that's why we didn't introduce it there

1216
00:44:10,800 --> 00:44:12,319
and um

1217
00:44:12,319 --> 00:44:16,160
i think the uh smile algorithm we used

1218
00:44:16,160 --> 00:44:16,880
for the

1219
00:44:16,880 --> 00:44:19,040
non-stationary bandits also has a

1220
00:44:19,040 --> 00:44:20,800
forgetting rate

1221
00:44:20,800 --> 00:44:23,119
um just like you say for getting right

1222
00:44:23,119 --> 00:44:25,200
on the concentration parameters

1223
00:44:25,200 --> 00:44:27,359
which i think is another reason why this

1224
00:44:27,359 --> 00:44:28,400
algorithm

1225
00:44:28,400 --> 00:44:30,880
worked well in the non-stationary case

1226
00:44:30,880 --> 00:44:33,440
or better than in the stationary case

1227
00:44:33,440 --> 00:44:36,560
and in theory i think we also tried some

1228
00:44:36,560 --> 00:44:38,880
version of this where we have both

1229
00:44:38,880 --> 00:44:41,200
contacts and

1230
00:44:41,200 --> 00:44:44,240
getting parameters but yeah in the end

1231
00:44:44,240 --> 00:44:46,079
then

1232
00:44:46,079 --> 00:44:50,160
for the randomly moving bandits we had

1233
00:44:50,160 --> 00:44:52,160
also thought about then having another

1234
00:44:52,160 --> 00:44:53,040
layer

1235
00:44:53,040 --> 00:44:54,960
to this hierarchical model that

1236
00:44:54,960 --> 00:44:56,560
impersonal volatility

1237
00:44:56,560 --> 00:44:58,960
to adjust the learn uh the floating rate

1238
00:44:58,960 --> 00:44:59,839
and

1239
00:44:59,839 --> 00:45:02,319
if you start that your learning model

1240
00:45:02,319 --> 00:45:03,040
becomes

1241
00:45:03,040 --> 00:45:05,920
very abstract pretty quickly maybe but

1242
00:45:05,920 --> 00:45:06,960
overkill

1243
00:45:06,960 --> 00:45:08,400
but i guess i guess what i don't

1244
00:45:08,400 --> 00:45:10,480
completely understand still is

1245
00:45:10,480 --> 00:45:13,839
so even in the stationary case if uh if

1246
00:45:13,839 --> 00:45:15,760
you're getting rid of sufficiently high

1247
00:45:15,760 --> 00:45:17,280
i mean that all that's going to happen

1248
00:45:17,280 --> 00:45:19,200
is your the actual like

1249
00:45:19,200 --> 00:45:20,480
magnitudes of your concentration

1250
00:45:20,480 --> 00:45:22,000
parameters are never going to build up

1251
00:45:22,000 --> 00:45:23,680
to too high of a value right

1252
00:45:23,680 --> 00:45:26,079
so like does that not still help with

1253
00:45:26,079 --> 00:45:27,200
the overconfidence issue

1254
00:45:27,200 --> 00:45:30,560
i just would have thought it would uh

1255
00:45:30,560 --> 00:45:32,240
well we don't have a per kind of

1256
00:45:32,240 --> 00:45:34,240
forgetting parameter in non-stationary

1257
00:45:34,240 --> 00:45:34,960
case

1258
00:45:34,960 --> 00:45:38,319
at least this shuts off because uh

1259
00:45:38,319 --> 00:45:40,640
well assume agent believes it's in a

1260
00:45:40,640 --> 00:45:42,400
stationary environment

1261
00:45:42,400 --> 00:45:45,040
yeah i got a reasonable you could still

1262
00:45:45,040 --> 00:45:46,880
put an agent in a stationary environment

1263
00:45:46,880 --> 00:45:48,319
but it seems like it could be plausible

1264
00:45:48,319 --> 00:45:49,040
that it

1265
00:45:49,040 --> 00:45:52,560
has a kind of yeah

1266
00:45:52,640 --> 00:45:54,960
right so yeah i agree i mean that's one

1267
00:45:54,960 --> 00:45:55,839
way one could

1268
00:45:55,839 --> 00:45:59,200
try out right to resolve the

1269
00:45:59,200 --> 00:46:01,760
uh exploration problem mean stationary

1270
00:46:01,760 --> 00:46:03,280
case right just simply

1271
00:46:03,280 --> 00:46:05,839
uh giving agent to the wrong beliefs so

1272
00:46:05,839 --> 00:46:06,400
that

1273
00:46:06,400 --> 00:46:09,200
things will continue

1274
00:46:09,760 --> 00:46:13,359
just tell this we use the same

1275
00:46:13,359 --> 00:46:15,359
learning the same bayesian belief

1276
00:46:15,359 --> 00:46:17,280
updating algorithm for all the different

1277
00:46:17,280 --> 00:46:19,119
action selection methods that

1278
00:46:19,119 --> 00:46:22,720
we compared and then it's a weird

1279
00:46:22,720 --> 00:46:25,280
interplay between

1280
00:46:25,280 --> 00:46:27,920
the learning model that we chose for the

1281
00:46:27,920 --> 00:46:29,280
stationary case

1282
00:46:29,280 --> 00:46:31,280
and the action selection rule and i mean

1283
00:46:31,280 --> 00:46:32,400
thompson sandling which

1284
00:46:32,400 --> 00:46:35,280
uses the same learning does not have

1285
00:46:35,280 --> 00:46:36,800
this issue

1286
00:46:36,800 --> 00:46:40,839
yeah and that i think is also

1287
00:46:40,839 --> 00:46:42,560
interesting

1288
00:46:42,560 --> 00:46:45,599
so to the second part of the question

1289
00:46:45,599 --> 00:46:47,440
where how would that translate to

1290
00:46:47,440 --> 00:46:48,960
improving how some problems our

1291
00:46:48,960 --> 00:46:50,319
decisions are currently solved

1292
00:46:50,319 --> 00:46:52,560
i'm hearing a few things we talked about

1293
00:46:52,560 --> 00:46:53,599
speeding up

1294
00:46:53,599 --> 00:46:55,920
computation relative to other approaches

1295
00:46:55,920 --> 00:46:58,079
but that's not a solved problem for

1296
00:46:58,079 --> 00:46:59,599
example if there's a 10 times speed up

1297
00:46:59,599 --> 00:47:01,119
but then you have to run 10 agents in

1298
00:47:01,119 --> 00:47:02,400
parallel to get a good

1299
00:47:02,400 --> 00:47:05,680
ensemble estimate then it's a wash so

1300
00:47:05,680 --> 00:47:07,520
potentially for speeding up just the

1301
00:47:07,520 --> 00:47:09,040
computational requirements

1302
00:47:09,040 --> 00:47:12,319
for certain challenges a second would be

1303
00:47:12,319 --> 00:47:14,240
that it might be possible to more

1304
00:47:14,240 --> 00:47:15,920
rapidly lock in

1305
00:47:15,920 --> 00:47:19,040
to dynamically changing regimes and to

1306
00:47:19,040 --> 00:47:20,960
avoid some of the pathologies

1307
00:47:20,960 --> 00:47:24,319
of model fitting in

1308
00:47:24,319 --> 00:47:26,800
multi-armed bandit contexts and then a

1309
00:47:26,800 --> 00:47:27,520
third

1310
00:47:27,520 --> 00:47:28,960
way that it could translate to

1311
00:47:28,960 --> 00:47:31,599
improvements would be it might reveal

1312
00:47:31,599 --> 00:47:33,839
some hidden similarities between these

1313
00:47:33,839 --> 00:47:35,520
different problems and settings

1314
00:47:35,520 --> 00:47:36,720
like we already know that they're

1315
00:47:36,720 --> 00:47:38,720
somewhat similar because we can apply a

1316
00:47:38,720 --> 00:47:40,400
multi-arm bandit to

1317
00:47:40,400 --> 00:47:42,160
you know health finance recommendation

1318
00:47:42,160 --> 00:47:44,000
systems etc so we know that there's a

1319
00:47:44,000 --> 00:47:45,119
lot of problems

1320
00:47:45,119 --> 00:47:48,079
involving data that have similar enough

1321
00:47:48,079 --> 00:47:49,599
structures such that a

1322
00:47:49,599 --> 00:47:51,599
similar kind of general algorithm can be

1323
00:47:51,599 --> 00:47:53,920
applied but then it could be interesting

1324
00:47:53,920 --> 00:47:55,440
once we have them on the common

1325
00:47:55,440 --> 00:47:57,440
grounding of active inference

1326
00:47:57,440 --> 00:47:59,359
to say actually you know the structure

1327
00:47:59,359 --> 00:48:00,960
of the decision making

1328
00:48:00,960 --> 00:48:03,599
is similar across these two settings or

1329
00:48:03,599 --> 00:48:05,680
you know the telecommunications routing

1330
00:48:05,680 --> 00:48:08,319
and the logistics routing are similar in

1331
00:48:08,319 --> 00:48:09,599
this unexpected way

1332
00:48:09,599 --> 00:48:12,559
so maybe insights relating to what kinds

1333
00:48:12,559 --> 00:48:13,599
of tweaks

1334
00:48:13,599 --> 00:48:15,920
an agent uh could improve on their

1335
00:48:15,920 --> 00:48:16,800
performance with

1336
00:48:16,800 --> 00:48:18,079
like what we're talking about here with

1337
00:48:18,079 --> 00:48:20,559
the category categorical

1338
00:48:20,559 --> 00:48:24,000
hidden states or learning and forgetting

1339
00:48:24,000 --> 00:48:26,559
tweaks maybe some of those insights

1340
00:48:26,559 --> 00:48:27,599
could be

1341
00:48:27,599 --> 00:48:29,520
implemented in active inference and then

1342
00:48:29,520 --> 00:48:31,440
more easily transferred across different

1343
00:48:31,440 --> 00:48:32,800
domains

1344
00:48:32,800 --> 00:48:36,559
so hope that conveyed some of our um

1345
00:48:36,559 --> 00:48:39,599
uh thoughts on this question to lars and

1346
00:48:39,599 --> 00:48:40,800
anyone else

1347
00:48:40,800 --> 00:48:43,760
yeah yes i mean one issue there is just

1348
00:48:43,760 --> 00:48:44,880
that

1349
00:48:44,880 --> 00:48:46,400
in different domains you will in

1350
00:48:46,400 --> 00:48:47,760
principle have different generality

1351
00:48:47,760 --> 00:48:49,359
models right i mean

1352
00:48:49,359 --> 00:48:51,200
although the problem is the same

1353
00:48:51,200 --> 00:48:53,040
multi-unbended i mean you would need

1354
00:48:53,040 --> 00:48:54,720
different representation of the

1355
00:48:54,720 --> 00:48:56,720
environment and this is then where the

1356
00:48:56,720 --> 00:48:57,680
challenge comes

1357
00:48:57,680 --> 00:49:01,440
potentially uh as

1358
00:49:01,440 --> 00:49:02,880
if you can represent it as a

1359
00:49:02,880 --> 00:49:04,559
multi-embedded problem you can

1360
00:49:04,559 --> 00:49:06,800
uh choose different whatever action

1361
00:49:06,800 --> 00:49:07,599
selection

1362
00:49:07,599 --> 00:49:09,760
algorithm you find works best and in

1363
00:49:09,760 --> 00:49:11,520
non-stationary situation

1364
00:49:11,520 --> 00:49:13,839
at least from for what we investigated

1365
00:49:13,839 --> 00:49:15,440
this seems to work well this doesn't

1366
00:49:15,440 --> 00:49:17,839
mean necessarily that this generalizes

1367
00:49:17,839 --> 00:49:19,680
one will still have to try out different

1368
00:49:19,680 --> 00:49:22,079
things just to make sure

1369
00:49:22,079 --> 00:49:23,599
but in the end the bigger challenge is

1370
00:49:23,599 --> 00:49:25,280
like okay what is a good generative

1371
00:49:25,280 --> 00:49:26,000
model for

1372
00:49:26,000 --> 00:49:30,160
this dynamic problem which i have

1373
00:49:30,160 --> 00:49:31,839
one can go there right with many

1374
00:49:31,839 --> 00:49:33,839
different things so i mean for example

1375
00:49:33,839 --> 00:49:34,160
what

1376
00:49:34,160 --> 00:49:37,839
uh ryan also mentioned this kind of

1377
00:49:37,839 --> 00:49:40,240
uh open-ended contextual learning i mean

1378
00:49:40,240 --> 00:49:42,800
one can represent simple in

1379
00:49:42,800 --> 00:49:45,200
in the environments where you don't know

1380
00:49:45,200 --> 00:49:46,240
anything about what

1381
00:49:46,240 --> 00:49:47,920
what's going on you can just do a

1382
00:49:47,920 --> 00:49:49,599
non-parametric generative model like

1383
00:49:49,599 --> 00:49:51,920
dirichlet process or gaussian process

1384
00:49:51,920 --> 00:49:54,880
you just try to learn even what the

1385
00:49:54,880 --> 00:49:56,160
model itself should be

1386
00:49:56,160 --> 00:49:57,280
[Music]

1387
00:49:57,280 --> 00:50:00,480
awesome so let's go

1388
00:50:00,480 --> 00:50:04,160
to this little sub discussion

1389
00:50:04,160 --> 00:50:06,880
on sarah one of your previous papers and

1390
00:50:06,880 --> 00:50:08,240
then we're gonna turn

1391
00:50:08,240 --> 00:50:11,040
to some notebooks and walkthroughs of

1392
00:50:11,040 --> 00:50:11,599
the

1393
00:50:11,599 --> 00:50:14,960
bandit project but hopefully this will

1394
00:50:14,960 --> 00:50:16,240
be informative because

1395
00:50:16,240 --> 00:50:19,520
first off belief propagation and message

1396
00:50:19,520 --> 00:50:20,720
passing and these types of

1397
00:50:20,720 --> 00:50:22,640
approximations are of interest

1398
00:50:22,640 --> 00:50:25,440
to the lab in the community and also

1399
00:50:25,440 --> 00:50:26,720
we're

1400
00:50:26,720 --> 00:50:29,280
seeing a few faces that we can ascend

1401
00:50:29,280 --> 00:50:31,280
active inference mountain on

1402
00:50:31,280 --> 00:50:34,720
we have ryan with a matrix based matlab

1403
00:50:34,720 --> 00:50:37,200
approach we will walk through in just a

1404
00:50:37,200 --> 00:50:38,000
few minutes

1405
00:50:38,000 --> 00:50:39,920
with a python based approach of the

1406
00:50:39,920 --> 00:50:41,680
bandit and then this is a slightly

1407
00:50:41,680 --> 00:50:42,480
different approach

1408
00:50:42,480 --> 00:50:45,760
based upon the beth approximation

1409
00:50:45,760 --> 00:50:49,119
so sarah anything you'd like to

1410
00:50:49,119 --> 00:50:50,960
describe i'm sure this will be new to

1411
00:50:50,960 --> 00:50:52,800
many people so it will be helpful to

1412
00:50:52,800 --> 00:50:56,559
convey what you were working on here

1413
00:50:56,640 --> 00:51:01,200
sure so in figure 2 the upper one what

1414
00:51:01,200 --> 00:51:01,839
you see

1415
00:51:01,839 --> 00:51:06,960
is the generative model of a normal

1416
00:51:06,960 --> 00:51:10,000
observable markov decision process

1417
00:51:10,000 --> 00:51:13,359
where the um in the upper row

1418
00:51:13,359 --> 00:51:15,040
the unfilled circuits are the hidden

1419
00:51:15,040 --> 00:51:17,119
states sorry they were called as

1420
00:51:17,119 --> 00:51:20,839
in the previous slide and below are the

1421
00:51:20,839 --> 00:51:22,319
observations

1422
00:51:22,319 --> 00:51:26,000
and then the whole um the dynamics of

1423
00:51:26,000 --> 00:51:28,400
which states follow upon each other

1424
00:51:28,400 --> 00:51:31,359
is determined by the policy pi on the

1425
00:51:31,359 --> 00:51:33,359
left side

1426
00:51:33,359 --> 00:51:36,640
and then what

1427
00:51:36,640 --> 00:51:39,839
people often do is assume in this

1428
00:51:39,839 --> 00:51:43,280
queue that we saw two slides before

1429
00:51:43,280 --> 00:51:45,920
um the mean sheet approximation which

1430
00:51:45,920 --> 00:51:47,200
means

1431
00:51:47,200 --> 00:51:50,240
exactly which means that

1432
00:51:50,240 --> 00:51:52,720
um for example here we have a bunch of

1433
00:51:52,720 --> 00:51:53,599
hidden states

1434
00:51:53,599 --> 00:51:56,880
h t h t plus one and so on

1435
00:51:56,880 --> 00:51:58,640
and then if you assume the mean field

1436
00:51:58,640 --> 00:52:00,079
approximation

1437
00:52:00,079 --> 00:52:03,839
um the approximate belief distribution

1438
00:52:03,839 --> 00:52:05,280
would be just

1439
00:52:05,280 --> 00:52:08,480
q of h t times q of

1440
00:52:08,480 --> 00:52:11,119
h t plus one times and so on and so

1441
00:52:11,119 --> 00:52:12,000
forth

1442
00:52:12,000 --> 00:52:15,839
and there with you create an implicitly

1443
00:52:15,839 --> 00:52:17,599
yeah you essentially treat all the

1444
00:52:17,599 --> 00:52:19,040
hidden states

1445
00:52:19,040 --> 00:52:21,839
as independent in your approximate

1446
00:52:21,839 --> 00:52:23,040
beliefs

1447
00:52:23,040 --> 00:52:25,200
and then their dependencies will be

1448
00:52:25,200 --> 00:52:28,078
averaged out

1449
00:52:29,040 --> 00:52:32,400
in figure 3 you actually see the

1450
00:52:32,400 --> 00:52:35,680
inverted model and in with

1451
00:52:35,680 --> 00:52:37,599
m you see the messages that are being

1452
00:52:37,599 --> 00:52:38,800
passed in between

1453
00:52:38,800 --> 00:52:42,000
notes so if you it doesn't matter what

1454
00:52:42,000 --> 00:52:43,920
approximation you use in the end you can

1455
00:52:43,920 --> 00:52:45,440
calculate your beliefs

1456
00:52:45,440 --> 00:52:47,680
with some sort of message passing

1457
00:52:47,680 --> 00:52:50,240
algorithm

1458
00:52:51,680 --> 00:52:53,760
except that now if you chose a mean

1459
00:52:53,760 --> 00:52:55,920
field approximation

1460
00:52:55,920 --> 00:52:59,359
and you estimate all hidden sets

1461
00:52:59,359 --> 00:53:01,280
separately

1462
00:53:01,280 --> 00:53:03,280
what we found is that actually they may

1463
00:53:03,280 --> 00:53:05,520
not fit very well to each other so for

1464
00:53:05,520 --> 00:53:06,079
example

1465
00:53:06,079 --> 00:53:09,200
when my agent predicted um how it will

1466
00:53:09,200 --> 00:53:10,640
go through the

1467
00:53:10,640 --> 00:53:13,920
grid under a certain policy it actually

1468
00:53:13,920 --> 00:53:14,880
often predicted

1469
00:53:14,880 --> 00:53:18,800
it will jump and go places that don't

1470
00:53:18,800 --> 00:53:19,440
adhere

1471
00:53:19,440 --> 00:53:23,760
to say transitions

1472
00:53:24,559 --> 00:53:26,720
and there is of course leading to

1473
00:53:26,720 --> 00:53:27,680
decreased

1474
00:53:27,680 --> 00:53:30,880
goal reaching success

1475
00:53:30,880 --> 00:53:33,440
and then instead of doing this mean fit

1476
00:53:33,440 --> 00:53:35,200
approximation

1477
00:53:35,200 --> 00:53:38,640
we assume the beta approximation

1478
00:53:38,640 --> 00:53:42,240
which instead of having qht times q

1479
00:53:42,240 --> 00:53:46,079
h t plus 1 and so on you have

1480
00:53:46,079 --> 00:53:49,520
small pairs of joint distributions so

1481
00:53:49,520 --> 00:53:53,839
q of h t and h t plus 1

1482
00:53:53,839 --> 00:53:56,960
times q of h t plus 1

1483
00:53:56,960 --> 00:54:00,800
and h t plus 2. and then doing the math

1484
00:54:00,800 --> 00:54:02,720
you can show that

1485
00:54:02,720 --> 00:54:05,920
if you assume this type of approximation

1486
00:54:05,920 --> 00:54:07,680
and you plug it into the free energy you

1487
00:54:07,680 --> 00:54:09,520
want to minimize the free energy

1488
00:54:09,520 --> 00:54:12,559
the belief propagation method path

1489
00:54:12,559 --> 00:54:15,839
passing algorithm comes out

1490
00:54:15,839 --> 00:54:17,520
and so you can use the belief

1491
00:54:17,520 --> 00:54:19,680
propagation and message passing

1492
00:54:19,680 --> 00:54:21,359
algorithm

1493
00:54:21,359 --> 00:54:24,880
under the beta approximation

1494
00:54:24,880 --> 00:54:26,480
to calculate beliefs and this is

1495
00:54:26,480 --> 00:54:28,240
actually exact on

1496
00:54:28,240 --> 00:54:31,439
graphs with our loops

1497
00:54:32,480 --> 00:54:35,280
and then you get a more appropriate

1498
00:54:35,280 --> 00:54:36,720
joint representation

1499
00:54:36,720 --> 00:54:39,119
or in this case of temporarily dependent

1500
00:54:39,119 --> 00:54:42,160
hidden variables

1501
00:54:42,640 --> 00:54:44,319
but i mean in the end yeah what i do

1502
00:54:44,319 --> 00:54:46,480
nowadays that i also

1503
00:54:46,480 --> 00:54:48,480
have a hierarchical model where now for

1504
00:54:48,480 --> 00:54:50,480
example the parameters of the markov

1505
00:54:50,480 --> 00:54:52,000
decision process are context

1506
00:54:52,000 --> 00:54:55,920
dependent i look into my model and think

1507
00:54:55,920 --> 00:54:58,559
which variables belong together and then

1508
00:54:58,559 --> 00:54:59,520
i apply the

1509
00:54:59,520 --> 00:55:01,680
mean the bit approximation in these

1510
00:55:01,680 --> 00:55:02,880
parts

1511
00:55:02,880 --> 00:55:05,599
but then some slower varying variables

1512
00:55:05,599 --> 00:55:06,000
like the

1513
00:55:06,000 --> 00:55:08,880
context i just use an infinite

1514
00:55:08,880 --> 00:55:12,240
approximation because it

1515
00:55:12,880 --> 00:55:16,640
it varies differently anyways

1516
00:55:17,359 --> 00:55:21,359
thanks for that breakdown um

1517
00:55:21,359 --> 00:55:23,839
what is message passing like who are the

1518
00:55:23,839 --> 00:55:26,240
messages being passed between

1519
00:55:26,240 --> 00:55:29,440
and does that reflect a variant

1520
00:55:29,440 --> 00:55:32,720
on active inference or is it

1521
00:55:32,720 --> 00:55:35,520
the same exact active inference model

1522
00:55:35,520 --> 00:55:36,880
can be

1523
00:55:36,880 --> 00:55:39,040
approximated or can be calculated

1524
00:55:39,040 --> 00:55:40,799
through message passing or through other

1525
00:55:40,799 --> 00:55:43,440
mechanisms

1526
00:55:44,480 --> 00:55:46,559
uh correct me if i'm wrong dimitri but i

1527
00:55:46,559 --> 00:55:49,040
think in the end all active influence

1528
00:55:49,040 --> 00:55:51,119
agents do message passing

1529
00:55:51,119 --> 00:55:53,280
depends on the approximation which type

1530
00:55:53,280 --> 00:55:56,160
of message passing algorithm

1531
00:55:56,160 --> 00:55:58,559
so um yeah another the better

1532
00:55:58,559 --> 00:55:59,839
approximation it's

1533
00:55:59,839 --> 00:56:02,559
belief propagation propagation message

1534
00:56:02,559 --> 00:56:04,480
passing i think there are equivalences

1535
00:56:04,480 --> 00:56:05,200
for the

1536
00:56:05,200 --> 00:56:08,480
sum product algorithm also you just get

1537
00:56:08,480 --> 00:56:09,599
different messages

1538
00:56:09,599 --> 00:56:12,160
that may be better or worse but okay

1539
00:56:12,160 --> 00:56:13,359
what are the messages

1540
00:56:13,359 --> 00:56:15,119
essentially each node in the graph so

1541
00:56:15,119 --> 00:56:17,680
each hidden variable

1542
00:56:17,680 --> 00:56:20,160
sends other hidden variables that it's

1543
00:56:20,160 --> 00:56:21,520
connected to

1544
00:56:21,520 --> 00:56:24,160
a message about which state it should be

1545
00:56:24,160 --> 00:56:25,599
in so

1546
00:56:25,599 --> 00:56:30,640
ht would say hey we're currently here

1547
00:56:30,640 --> 00:56:33,839
then i think in the next state we should

1548
00:56:33,839 --> 00:56:36,400
be there

1549
00:56:36,400 --> 00:56:39,440
and then vice versa uh ht plus one can

1550
00:56:39,440 --> 00:56:41,280
send a message back that says

1551
00:56:41,280 --> 00:56:44,480
hey we wanna be there next time sir

1552
00:56:44,480 --> 00:56:47,680
where should we be now and so

1553
00:56:47,680 --> 00:56:50,559
yeah these variables essentially send

1554
00:56:50,559 --> 00:56:52,640
each other messages on what they should

1555
00:56:52,640 --> 00:56:53,359
be

1556
00:56:53,359 --> 00:56:56,839
so that they are in agreement with each

1557
00:56:56,839 --> 00:56:59,119
other that's how you

1558
00:56:59,119 --> 00:57:00,880
get essentially a probability

1559
00:57:00,880 --> 00:57:03,839
distribution over what you think

1560
00:57:03,839 --> 00:57:07,040
your which state you're in

1561
00:57:07,040 --> 00:57:11,119
and will be in yet dimitri any

1562
00:57:11,119 --> 00:57:13,280
thoughts on message passing or where do

1563
00:57:13,280 --> 00:57:16,240
you see message passing fitting into

1564
00:57:16,240 --> 00:57:18,720
bayesian statistics and a few other

1565
00:57:18,720 --> 00:57:20,480
topics

1566
00:57:20,480 --> 00:57:22,960
well i mean as sarah said mo i mean they

1567
00:57:22,960 --> 00:57:24,880
would say all the algorithms are message

1568
00:57:24,880 --> 00:57:25,440
passing

1569
00:57:25,440 --> 00:57:26,799
so when you talk about mean field

1570
00:57:26,799 --> 00:57:29,200
approximation this would be

1571
00:57:29,200 --> 00:57:30,640
traditionally variational message

1572
00:57:30,640 --> 00:57:32,799
passing it's called the algorithm right

1573
00:57:32,799 --> 00:57:35,280
uh here uh this is like belief

1574
00:57:35,280 --> 00:57:36,079
propagation

1575
00:57:36,079 --> 00:57:39,920
it's a message passing algorithm based

1576
00:57:39,920 --> 00:57:40,160
on

1577
00:57:40,160 --> 00:57:44,160
uh marginal probabilities uh

1578
00:57:44,160 --> 00:57:46,000
instead of in the variational message

1579
00:57:46,000 --> 00:57:48,000
passing you have like uh

1580
00:57:48,000 --> 00:57:50,640
expectations of the log of conditional

1581
00:57:50,640 --> 00:57:52,319
probabilities right i mean this is these

1582
00:57:52,319 --> 00:57:54,400
are kind of the differences

1583
00:57:54,400 --> 00:57:57,760
uh what you're what you're getting and

1584
00:57:57,760 --> 00:57:58,799
losing

1585
00:57:58,799 --> 00:58:00,720
and i mean we have another paper with

1586
00:58:00,720 --> 00:58:02,559
thomas parr

1587
00:58:02,559 --> 00:58:04,640
and carl neuronal message passing using

1588
00:58:04,640 --> 00:58:06,000
mean field beta and marginal

1589
00:58:06,000 --> 00:58:07,599
approximations

1590
00:58:07,599 --> 00:58:09,440
where we kind of contrast these

1591
00:58:09,440 --> 00:58:11,440
different ways so who is interested in

1592
00:58:11,440 --> 00:58:13,440
this topic can

1593
00:58:13,440 --> 00:58:16,240
uh look into a bit unpacked discussion

1594
00:58:16,240 --> 00:58:19,359
of similarities and differences

1595
00:58:19,359 --> 00:58:20,960
in practice the difficulty with mean

1596
00:58:20,960 --> 00:58:22,559
field approximation it's

1597
00:58:22,559 --> 00:58:24,319
i mean for dynamical problems for

1598
00:58:24,319 --> 00:58:25,760
numerical decision making it's not

1599
00:58:25,760 --> 00:58:27,520
really a good approximation this is not

1600
00:58:27,520 --> 00:58:28,000
something

1601
00:58:28,000 --> 00:58:31,520
which one would use and

1602
00:58:31,520 --> 00:58:33,520
implementation wise in inactive

1603
00:58:33,520 --> 00:58:35,359
inference mean field approximation is

1604
00:58:35,359 --> 00:58:38,000
not used on the dynamical level

1605
00:58:38,000 --> 00:58:39,920
uh what what they're using in matlab for

1606
00:58:39,920 --> 00:58:41,440
example is all this marginal

1607
00:58:41,440 --> 00:58:43,119
approximation it's still kind of

1608
00:58:43,119 --> 00:58:45,680
gradient-based method but

1609
00:58:45,680 --> 00:58:48,839
it's computed slightly differently so

1610
00:58:48,839 --> 00:58:54,078
uh yeah sorry uh

1611
00:58:54,720 --> 00:58:57,440
well i just wanted to kind of wrap up is

1612
00:58:57,440 --> 00:58:58,640
basically the more complex

1613
00:58:58,640 --> 00:59:02,240
problem it is the more uncertainties you

1614
00:59:02,240 --> 00:59:04,079
have on the state transitions

1615
00:59:04,079 --> 00:59:05,680
the more difficult you would

1616
00:59:05,680 --> 00:59:07,359
difficulties you will have it mean field

1617
00:59:07,359 --> 00:59:09,040
and marginal approximation basically the

1618
00:59:09,040 --> 00:59:11,040
bt approximation is the only thing which

1619
00:59:11,040 --> 00:59:12,559
kind of

1620
00:59:12,559 --> 00:59:15,599
uh corresponds to actually exact

1621
00:59:15,599 --> 00:59:18,160
inference in the

1622
00:59:18,160 --> 00:59:20,480
in non-cyclic graph so basically this is

1623
00:59:20,480 --> 00:59:21,440
uh this is

1624
00:59:21,440 --> 00:59:23,680
theoretical solution you know that you

1625
00:59:23,680 --> 00:59:24,720
can be exact

1626
00:59:24,720 --> 00:59:27,839
under specific conditions

1627
00:59:28,160 --> 00:59:31,359
on the marginal um a yes

1628
00:59:31,359 --> 00:59:34,160
i can only warmly recommend by adida and

1629
00:59:34,160 --> 00:59:37,359
vice i think in 2001

1630
00:59:37,359 --> 00:59:39,520
called understanding belief propagation

1631
00:59:39,520 --> 00:59:41,520
and its generalizations

1632
00:59:41,520 --> 00:59:43,920
and i find it very didactic it hurt me a

1633
00:59:43,920 --> 00:59:46,319
lot and they explain

1634
00:59:46,319 --> 00:59:49,520
in detail how uh variational inference

1635
00:59:49,520 --> 00:59:51,760
is also connected to

1636
00:59:51,760 --> 00:59:55,440
message passing algorithms

1637
00:59:55,760 --> 00:59:58,079
interesting so just to capture that one

1638
00:59:58,079 --> 00:59:58,880
interesting

1639
00:59:58,880 --> 01:00:00,880
thing you said there about the gradients

1640
01:00:00,880 --> 01:00:02,720
the gradient it's sort of like your

1641
01:00:02,720 --> 01:00:04,319
model is in a given spot

1642
01:00:04,319 --> 01:00:07,119
on the landscape and then it checks the

1643
01:00:07,119 --> 01:00:08,640
temperature and it goes

1644
01:00:08,640 --> 01:00:10,799
in the direction of the gradient so

1645
01:00:10,799 --> 01:00:12,720
we've talked a lot about gradient based

1646
01:00:12,720 --> 01:00:15,359
methods with the straight line versus

1647
01:00:15,359 --> 01:00:16,799
the iso contour

1648
01:00:16,799 --> 01:00:20,000
message passing kind of breaks that down

1649
01:00:20,000 --> 01:00:22,960
into a process so at each click of the

1650
01:00:22,960 --> 01:00:24,400
model

1651
01:00:24,400 --> 01:00:26,400
messages are being passed back and forth

1652
01:00:26,400 --> 01:00:27,839
to one another

1653
01:00:27,839 --> 01:00:31,280
which is both computationally tractable

1654
01:00:31,280 --> 01:00:34,400
uh it's also shown through some work of

1655
01:00:34,400 --> 01:00:35,760
uh

1656
01:00:35,760 --> 01:00:38,960
the bias lab uh burt devries and others

1657
01:00:38,960 --> 01:00:41,119
that four specific categories of

1658
01:00:41,119 --> 01:00:42,559
bayesian graphs

1659
01:00:42,559 --> 01:00:44,559
that message passing algorithms are

1660
01:00:44,559 --> 01:00:46,160
basically equivalent in the forney

1661
01:00:46,160 --> 01:00:48,000
factor graphs the ffg

1662
01:00:48,000 --> 01:00:49,280
which we're going to be learning about

1663
01:00:49,280 --> 01:00:51,119
in the future and also

1664
01:00:51,119 --> 01:00:54,640
this topologically puts you more

1665
01:00:54,640 --> 01:00:57,520
into touch with the predictive

1666
01:00:57,520 --> 01:00:58,400
processing

1667
01:00:58,400 --> 01:01:00,319
for example the messages that neurons

1668
01:01:00,319 --> 01:01:01,920
are passing to each other

1669
01:01:01,920 --> 01:01:04,160
so it's one thing to say well it's as if

1670
01:01:04,160 --> 01:01:05,040
the neurons

1671
01:01:05,040 --> 01:01:06,960
are messaging to each other and that's

1672
01:01:06,960 --> 01:01:08,720
doing a gradient descent

1673
01:01:08,720 --> 01:01:10,079
but it's another thing to actually

1674
01:01:10,079 --> 01:01:13,680
saying we have a message passing scheme

1675
01:01:13,680 --> 01:01:16,079
for modeling how these message passing

1676
01:01:16,079 --> 01:01:17,040
agents

1677
01:01:17,040 --> 01:01:20,799
do inference so there's a few

1678
01:01:20,799 --> 01:01:23,760
points of contact there that i think are

1679
01:01:23,760 --> 01:01:25,040
pretty important and it's pretty

1680
01:01:25,040 --> 01:01:26,480
it's also interesting that you brought

1681
01:01:26,480 --> 01:01:28,240
up that early paper

1682
01:01:28,240 --> 01:01:30,559
so it's we'll check that one out stephen

1683
01:01:30,559 --> 01:01:33,280
did do you have a question

1684
01:01:33,280 --> 01:01:36,079
i think it's been sort of covered really

1685
01:01:36,079 --> 01:01:37,280
it's

1686
01:01:37,280 --> 01:01:40,960
cool area though so thanks sarah for um

1687
01:01:40,960 --> 01:01:44,079
sharing that just one last question on

1688
01:01:44,079 --> 01:01:47,200
this before we go to the bandit codes

1689
01:01:47,200 --> 01:01:48,400
and walk through like

1690
01:01:48,400 --> 01:01:51,599
where do these approaches um

1691
01:01:51,599 --> 01:01:53,440
are they converging and they're going to

1692
01:01:53,440 --> 01:01:54,799
weave together

1693
01:01:54,799 --> 01:01:58,400
more closely or is one of them

1694
01:01:58,400 --> 01:02:00,720
like an umbrella over the other such

1695
01:02:00,720 --> 01:02:02,559
that work will continue mainly

1696
01:02:02,559 --> 01:02:05,920
under the generalized form where do

1697
01:02:05,920 --> 01:02:07,520
these different approaches that we're

1698
01:02:07,520 --> 01:02:09,200
talking about to implementing active

1699
01:02:09,200 --> 01:02:13,200
inference where are they heading

1700
01:02:14,160 --> 01:02:16,640
like is it is more development happening

1701
01:02:16,640 --> 01:02:18,880
on the message passing

1702
01:02:18,880 --> 01:02:22,960
approximation or on other

1703
01:02:22,960 --> 01:02:26,799
modes of breaking down active inference

1704
01:02:30,240 --> 01:02:32,880
i'm not sure i have the overview of

1705
01:02:32,880 --> 01:02:34,880
hundreds of papers

1706
01:02:34,880 --> 01:02:36,559
publish every month and that's active

1707
01:02:36,559 --> 01:02:39,200
influence to say something like

1708
01:02:39,200 --> 01:02:41,759
about uh

1709
01:02:43,280 --> 01:02:46,480
as far as i mean our motivation is here

1710
01:02:46,480 --> 01:02:47,920
we just want to have in different

1711
01:02:47,920 --> 01:02:49,760
situation good enough

1712
01:02:49,760 --> 01:02:54,559
inference approximations so

1713
01:02:56,720 --> 01:02:59,920
in a way for me more important question

1714
01:02:59,920 --> 01:03:02,000
is like what is a good representation of

1715
01:03:02,000 --> 01:03:04,079
different tasks and environments to have

1716
01:03:04,079 --> 01:03:05,520
rather than

1717
01:03:05,520 --> 01:03:07,680
what is the best kind of inference

1718
01:03:07,680 --> 01:03:10,400
algorithm to use

1719
01:03:10,400 --> 01:03:13,760
uh because especially if

1720
01:03:13,760 --> 01:03:15,359
i mean depending again on the

1721
01:03:15,359 --> 01:03:17,039
environment you're working on and but in

1722
01:03:17,039 --> 01:03:19,520
dynamic context it's

1723
01:03:19,520 --> 01:03:22,960
uh difficult to get like

1724
01:03:22,960 --> 01:03:26,240
lots of advantage with just improving

1725
01:03:26,240 --> 01:03:27,599
slightly on the

1726
01:03:27,599 --> 01:03:31,039
inference performance great

1727
01:03:31,039 --> 01:03:32,720
just because there's lots of uncertainty

1728
01:03:32,720 --> 01:03:34,559
in things and anyway change

1729
01:03:34,559 --> 01:03:37,599
all the time so um

1730
01:03:37,599 --> 01:03:40,559
yeah i mean with this we also kind of

1731
01:03:40,559 --> 01:03:42,400
write for for this multi-unbanded paper

1732
01:03:42,400 --> 01:03:43,440
we tested very

1733
01:03:43,440 --> 01:03:46,720
lots of different algorithms uh there is

1734
01:03:46,720 --> 01:03:48,319
kind of one of the notebooks in

1735
01:03:48,319 --> 01:03:50,400
repository just kind of lists different

1736
01:03:50,400 --> 01:03:52,960
things we tried out

1737
01:03:52,960 --> 01:03:55,359
but in the end one doesn't see you i

1738
01:03:55,359 --> 01:03:56,559
mean

1739
01:03:56,559 --> 01:03:58,559
any reason why one algorithm would be

1740
01:03:58,559 --> 01:04:00,240
specifically

1741
01:04:00,240 --> 01:04:02,079
a way is better than another it's just

1742
01:04:02,079 --> 01:04:04,000
they're very similar uh

1743
01:04:04,000 --> 01:04:07,359
subtle subtle differences great

1744
01:04:07,359 --> 01:04:10,079
point that there's a lot of work on the

1745
01:04:10,079 --> 01:04:11,440
comparability of different

1746
01:04:11,440 --> 01:04:13,440
approximations and different algorithms

1747
01:04:13,440 --> 01:04:17,119
but actually it might be more beneficial

1748
01:04:17,119 --> 01:04:17,440
for

1749
01:04:17,440 --> 01:04:20,079
a given application to focus more on how

1750
01:04:20,079 --> 01:04:22,480
they're specifying the generative model

1751
01:04:22,480 --> 01:04:23,839
and making sure that that really

1752
01:04:23,839 --> 01:04:25,520
captures essential features of the

1753
01:04:25,520 --> 01:04:26,400
environment

1754
01:04:26,400 --> 01:04:28,559
because it's like okay let's just roll

1755
01:04:28,559 --> 01:04:30,000
with active inference

1756
01:04:30,000 --> 01:04:32,480
and spend our attention on the

1757
01:04:32,480 --> 01:04:34,079
generative process and the generative

1758
01:04:34,079 --> 01:04:35,280
model

1759
01:04:35,280 --> 01:04:37,920
rather than try to finesse potentially a

1760
01:04:37,920 --> 01:04:39,039
a grossly

1761
01:04:39,039 --> 01:04:41,440
inferior generative model with some

1762
01:04:41,440 --> 01:04:42,720
better approximation

1763
01:04:42,720 --> 01:04:44,799
there might be limited returns there so

1764
01:04:44,799 --> 01:04:46,000
stephen and then we'll

1765
01:04:46,000 --> 01:04:49,520
go to looking at some of the bandit code

1766
01:04:49,520 --> 01:04:52,319
yeah this is a kind of general question

1767
01:04:52,319 --> 01:04:53,440
related to that

1768
01:04:53,440 --> 01:04:56,480
is i often think about

1769
01:04:56,480 --> 01:05:01,440
sort of didactic deontique type

1770
01:05:01,599 --> 01:05:05,119
sort of um ways of making inference

1771
01:05:05,119 --> 01:05:08,400
um this sort of deductive thinking and

1772
01:05:08,400 --> 01:05:09,200
such like

1773
01:05:09,200 --> 01:05:11,119
and i'm wondering whether the message

1774
01:05:11,119 --> 01:05:13,359
passing is is is more

1775
01:05:13,359 --> 01:05:16,559
present in those types of models because

1776
01:05:16,559 --> 01:05:18,480
it's it's something that's been detected

1777
01:05:18,480 --> 01:05:19,760
in the environment

1778
01:05:19,760 --> 01:05:22,400
and decisions based on that are being

1779
01:05:22,400 --> 01:05:23,440
made

1780
01:05:23,440 --> 01:05:25,119
and you've got then you've got your kind

1781
01:05:25,119 --> 01:05:26,799
of inductive where you're trying to

1782
01:05:26,799 --> 01:05:28,400
narrow the gap

1783
01:05:28,400 --> 01:05:30,880
between a goal and then you've got your

1784
01:05:30,880 --> 01:05:32,559
abductive where you're trying to build

1785
01:05:32,559 --> 01:05:33,599
something up

1786
01:05:33,599 --> 01:05:36,960
and infer from like a landscape

1787
01:05:36,960 --> 01:05:38,720
that you're trying to work out what what

1788
01:05:38,720 --> 01:05:40,960
is out there so to speak

1789
01:05:40,960 --> 01:05:44,319
and i'm wondering if this

1790
01:05:44,319 --> 01:05:46,079
is correct in my thinking that the

1791
01:05:46,079 --> 01:05:48,160
message passing is more

1792
01:05:48,160 --> 01:05:52,000
used when you have like a particular

1793
01:05:52,000 --> 01:05:55,599
deductive reasoning approach and

1794
01:05:55,599 --> 01:05:59,760
ad inductive or abducted ones would be

1795
01:05:59,760 --> 01:06:04,000
different interesting question about

1796
01:06:04,000 --> 01:06:07,039
how those different modes and types of

1797
01:06:07,039 --> 01:06:07,520
logic

1798
01:06:07,520 --> 01:06:10,640
are connected to message passing

1799
01:06:10,640 --> 01:06:13,599
one thought which might be on or off

1800
01:06:13,599 --> 01:06:14,000
base

1801
01:06:14,000 --> 01:06:17,760
is that we're thinking a lot about how

1802
01:06:17,760 --> 01:06:21,039
variables in a model are like nodes

1803
01:06:21,039 --> 01:06:22,880
and then there's edges connecting the

1804
01:06:22,880 --> 01:06:24,240
nodes that reflect

1805
01:06:24,240 --> 01:06:27,039
the relationship between those variables

1806
01:06:27,039 --> 01:06:27,440
and

1807
01:06:27,440 --> 01:06:29,440
message passing is just one way to

1808
01:06:29,440 --> 01:06:30,559
describe

1809
01:06:30,559 --> 01:06:33,599
as a model updates through time which

1810
01:06:33,599 --> 01:06:35,520
information is being passed between the

1811
01:06:35,520 --> 01:06:36,880
variables

1812
01:06:36,880 --> 01:06:39,440
so it doesn't say anything about the

1813
01:06:39,440 --> 01:06:41,760
mode of operation of an agent

1814
01:06:41,760 --> 01:06:43,440
which might be engaging in different

1815
01:06:43,440 --> 01:06:45,760
kinds of logics and i think that's

1816
01:06:45,760 --> 01:06:49,280
in really excellent question like how

1817
01:06:49,280 --> 01:06:52,319
how do we um break out of

1818
01:06:52,319 --> 01:06:55,599
the known with respect to how our

1819
01:06:55,599 --> 01:06:57,200
algorithms update

1820
01:06:57,200 --> 01:06:59,280
and it does actually touch upon this

1821
01:06:59,280 --> 01:07:01,039
mean field approximation

1822
01:07:01,039 --> 01:07:03,200
for example if you think that all

1823
01:07:03,200 --> 01:07:05,599
through all time past present future

1824
01:07:05,599 --> 01:07:07,520
that there's some stationarity of the

1825
01:07:07,520 --> 01:07:08,720
hidden state

1826
01:07:08,720 --> 01:07:10,880
then the mean field approximation will

1827
01:07:10,880 --> 01:07:11,760
work

1828
01:07:11,760 --> 01:07:14,640
but then if there's going to be a change

1829
01:07:14,640 --> 01:07:15,359
in

1830
01:07:15,359 --> 01:07:18,240
the state then the mean field

1831
01:07:18,240 --> 01:07:19,760
approximation

1832
01:07:19,760 --> 01:07:21,680
is potentially going to give a

1833
01:07:21,680 --> 01:07:24,000
misleading outcome

1834
01:07:24,000 --> 01:07:26,160
but in any case message passing is just

1835
01:07:26,160 --> 01:07:27,680
describing sort of the

1836
01:07:27,680 --> 01:07:30,720
mechanics of how the model updates and

1837
01:07:30,720 --> 01:07:32,079
which information between

1838
01:07:32,079 --> 01:07:34,559
variables is connected but importantly

1839
01:07:34,559 --> 01:07:36,960
which variables are not connected

1840
01:07:36,960 --> 01:07:39,200
like the observation at a time point

1841
01:07:39,200 --> 01:07:40,960
doesn't influence the observation at a

1842
01:07:40,960 --> 01:07:41,440
different

1843
01:07:41,440 --> 01:07:45,119
time point directly but it could via

1844
01:07:45,119 --> 01:07:49,760
a specified path of message passing

1845
01:07:54,839 --> 01:07:59,039
let's look a little bit at the bandit

1846
01:07:59,039 --> 01:08:02,960
code um so the the links to the

1847
01:08:02,960 --> 01:08:06,000
github dye markov

1848
01:08:06,000 --> 01:08:08,839
you have the right name to work in the

1849
01:08:08,839 --> 01:08:11,010
area

1850
01:08:11,010 --> 01:08:12,720
[Music]

1851
01:08:12,720 --> 01:08:15,119
um so we have a few of these notebooks

1852
01:08:15,119 --> 01:08:16,319
up

1853
01:08:16,319 --> 01:08:20,000
and uh is there we can look at the

1854
01:08:20,000 --> 01:08:22,158
overall notebooks folder or do you have

1855
01:08:22,158 --> 01:08:23,120
a sense of

1856
01:08:23,120 --> 01:08:25,920
which of the notebooks might be

1857
01:08:25,920 --> 01:08:26,880
interesting to

1858
01:08:26,880 --> 01:08:29,198
walk the record go to a few or if you

1859
01:08:29,198 --> 01:08:32,879
want to jump to a first one

1860
01:08:36,238 --> 01:08:38,960
i think just this first one think about

1861
01:08:38,960 --> 01:08:39,920
this maybe

1862
01:08:39,920 --> 01:08:42,799
oh go ahead uh

1863
01:08:43,600 --> 01:08:46,319
yeah there are a couple of things i mean

1864
01:08:46,319 --> 01:08:47,920
a couple of notebooks which are not

1865
01:08:47,920 --> 01:08:50,960
immediately relevant for the paper

1866
01:08:50,960 --> 01:08:53,839
so which we can focus just on the things

1867
01:08:53,839 --> 01:08:55,198
uh

1868
01:08:55,198 --> 01:08:57,679
which are part of the paper or i can

1869
01:08:57,679 --> 01:09:00,158
just generally kind of

1870
01:09:00,158 --> 01:09:01,839
talk also about these other things which

1871
01:09:01,839 --> 01:09:03,679
are just process of thinking about the

1872
01:09:03,679 --> 01:09:05,279
problem

1873
01:09:05,279 --> 01:09:08,799
okay how about before we even how do you

1874
01:09:08,799 --> 01:09:12,319
as a researcher working on this area

1875
01:09:12,319 --> 01:09:14,799
keep that separate like the paper

1876
01:09:14,799 --> 01:09:16,640
specific developments but then your

1877
01:09:16,640 --> 01:09:17,279
overall

1878
01:09:17,279 --> 01:09:19,439
developments do you find that you're on

1879
01:09:19,439 --> 01:09:21,198
an overall development

1880
01:09:21,198 --> 01:09:24,238
mood and then you dip into specifying a

1881
01:09:24,238 --> 01:09:26,719
paper or do you pursue the paper

1882
01:09:26,719 --> 01:09:29,120
and find that you have more general

1883
01:09:29,120 --> 01:09:30,399
insights while you're working through

1884
01:09:30,399 --> 01:09:32,640
the problems

1885
01:09:32,640 --> 01:09:35,040
well i mean i pursued the paper but i

1886
01:09:35,040 --> 01:09:36,080
mean then

1887
01:09:36,080 --> 01:09:38,960
i mean there are lots of branching pads

1888
01:09:38,960 --> 01:09:41,600
on that way in a way

1889
01:09:41,600 --> 01:09:43,520
you have to figure out uh what's

1890
01:09:43,520 --> 01:09:46,399
potentially interesting what's relevant

1891
01:09:46,399 --> 01:09:48,000
and so part of this code is just

1892
01:09:48,000 --> 01:09:49,600
exploration a bit of things

1893
01:09:49,600 --> 01:09:51,359
topics which were interesting for me but

1894
01:09:51,359 --> 01:09:53,279
which turned out not to be so

1895
01:09:53,279 --> 01:09:56,960
important in the end just

1896
01:09:56,960 --> 01:10:00,159
maybe for some other paper uh and

1897
01:10:00,159 --> 01:10:02,560
things which are just focusing exactly

1898
01:10:02,560 --> 01:10:03,520
on the

1899
01:10:03,520 --> 01:10:05,199
comparison of multi-arm-banded

1900
01:10:05,199 --> 01:10:06,960
algorithms and

1901
01:10:06,960 --> 01:10:11,120
right uh discussing this part uh

1902
01:10:11,120 --> 01:10:14,560
so in a way it's difficult to

1903
01:10:14,560 --> 01:10:17,280
combine lots of potentially unrelated

1904
01:10:17,280 --> 01:10:19,199
things into one paper so

1905
01:10:19,199 --> 01:10:21,199
one always has to make some favorites in

1906
01:10:21,199 --> 01:10:23,040
the end

1907
01:10:23,040 --> 01:10:26,080
i knew it would be a a both type of

1908
01:10:26,080 --> 01:10:28,000
question because it's something that

1909
01:10:28,000 --> 01:10:29,440
researchers are often

1910
01:10:29,440 --> 01:10:31,040
you know interested in general questions

1911
01:10:31,040 --> 01:10:33,360
but we need to deliver on specific

1912
01:10:33,360 --> 01:10:35,440
research projects with a defined scope

1913
01:10:35,440 --> 01:10:36,480
and conclusions

1914
01:10:36,480 --> 01:10:39,040
as well so it's just cool to see that

1915
01:10:39,040 --> 01:10:40,880
this repository holds a little bit of

1916
01:10:40,880 --> 01:10:41,840
both

1917
01:10:41,840 --> 01:10:44,880
so so for example this uh notebook which

1918
01:10:44,880 --> 01:10:46,000
you opened first like

1919
01:10:46,000 --> 01:10:48,560
expected free energy comparison this was

1920
01:10:48,560 --> 01:10:50,640
just my contemplation of just

1921
01:10:50,640 --> 01:10:53,120
different ways you can define expected

1922
01:10:53,120 --> 01:10:55,199
free energy

1923
01:10:55,199 --> 01:10:57,600
so typically people will think about

1924
01:10:57,600 --> 01:10:59,920
expected free energy in this terms of

1925
01:10:59,920 --> 01:11:03,199
expectations of their outcomes

1926
01:11:03,199 --> 01:11:06,239
but for example another question is okay

1927
01:11:06,239 --> 01:11:07,440
but why not

1928
01:11:07,440 --> 01:11:09,040
why not computing in terms of

1929
01:11:09,040 --> 01:11:11,679
expectations over states

1930
01:11:11,679 --> 01:11:13,600
and there is the relation between these

1931
01:11:13,600 --> 01:11:16,000
two right uh one is an upper bound on

1932
01:11:16,000 --> 01:11:18,000
another so

1933
01:11:18,000 --> 01:11:20,400
basically you see this last relation

1934
01:11:20,400 --> 01:11:21,040
there is

1935
01:11:21,040 --> 01:11:23,360
s of pi there is g of pi and there is i

1936
01:11:23,360 --> 01:11:24,880
of pi

1937
01:11:24,880 --> 01:11:28,000
uh and um basically g of pi would be

1938
01:11:28,000 --> 01:11:30,320
expected free energy in terms of

1939
01:11:30,320 --> 01:11:32,960
expectations over latin states

1940
01:11:32,960 --> 01:11:35,120
so this first and this is upper bound on

1941
01:11:35,120 --> 01:11:36,800
what this would be like expected

1942
01:11:36,800 --> 01:11:38,840
surprise for me

1943
01:11:38,840 --> 01:11:41,679
uh in equivalence to the free energy

1944
01:11:41,679 --> 01:11:43,360
being the bound on marginal

1945
01:11:43,360 --> 01:11:46,800
surprise like log likelihood or

1946
01:11:46,800 --> 01:11:49,920
surprising uh

1947
01:11:49,920 --> 01:11:52,320
and then there is the i of pi which is

1948
01:11:52,320 --> 01:11:55,040
just kl divergence between

1949
01:11:55,040 --> 01:11:56,960
posterior prior which gives you then

1950
01:11:56,960 --> 01:11:58,960
something else

1951
01:11:58,960 --> 01:12:01,120
and anyone can also think well we can

1952
01:12:01,120 --> 01:12:03,120
select policies or make decision-making

1953
01:12:03,120 --> 01:12:04,640
algorithm based on any of

1954
01:12:04,640 --> 01:12:06,640
on these quantities and what happens

1955
01:12:06,640 --> 01:12:09,600
when you use one or over another

1956
01:12:09,600 --> 01:12:12,880
so this is something which uh i was just

1957
01:12:12,880 --> 01:12:14,080
testing out

1958
01:12:14,080 --> 01:12:18,560
for myself uh and

1959
01:12:18,560 --> 01:12:20,239
i'm still not clear what to think about

1960
01:12:20,239 --> 01:12:22,960
this so that's why

1961
01:12:22,960 --> 01:12:26,320
i don't have a paper it this is very

1962
01:12:26,320 --> 01:12:29,199
interesting um it's like it's all

1963
01:12:29,199 --> 01:12:31,840
conditioned on policy with pi

1964
01:12:31,840 --> 01:12:34,960
and then we're approaching it from the

1965
01:12:34,960 --> 01:12:36,400
top and from the bottom

1966
01:12:36,400 --> 01:12:38,480
and so the free energy is like being

1967
01:12:38,480 --> 01:12:40,159
sandwiched in between

1968
01:12:40,159 --> 01:12:43,520
these other approximations um and then

1969
01:12:43,520 --> 01:12:46,159
you wrote here that the minima of i and

1970
01:12:46,159 --> 01:12:46,960
g of pi

1971
01:12:46,960 --> 01:12:50,480
match but s is giving a different minima

1972
01:12:50,480 --> 01:12:53,679
so what what was curious about dot co no

1973
01:12:53,679 --> 01:12:55,920
at least in this example right uh so i

1974
01:12:55,920 --> 01:12:58,159
just kind of build up simple example and

1975
01:12:58,159 --> 01:13:00,560
one can see that optimal policy uh or

1976
01:13:00,560 --> 01:13:02,080
like minima of

1977
01:13:02,080 --> 01:13:04,480
these different quantities is different

1978
01:13:04,480 --> 01:13:05,920
and one can also think probably of

1979
01:13:05,920 --> 01:13:07,840
different examples well this will

1980
01:13:07,840 --> 01:13:09,440
i mean this relation will not hold

1981
01:13:09,440 --> 01:13:10,960
anything but what i just wrote as a

1982
01:13:10,960 --> 01:13:12,400
comment there

1983
01:13:12,400 --> 01:13:14,400
uh but i mean this is more like than

1984
01:13:14,400 --> 01:13:15,840
practical question

1985
01:13:15,840 --> 01:13:19,280
so if you would then build an agent

1986
01:13:19,280 --> 01:13:20,800
uh which one of these quantities should

1987
01:13:20,800 --> 01:13:24,400
you use they are all kind of effectively

1988
01:13:24,400 --> 01:13:28,080
can be seen as a right expat expectation

1989
01:13:28,080 --> 01:13:28,960
of a future

1990
01:13:28,960 --> 01:13:32,239
surprise uh and

1991
01:13:32,239 --> 01:13:34,719
having different bounds on that uh

1992
01:13:34,719 --> 01:13:36,320
expectation as an approximate

1993
01:13:36,320 --> 01:13:39,679
quantity uh so just the question is uh

1994
01:13:39,679 --> 01:13:41,679
and i also found in different uh

1995
01:13:41,679 --> 01:13:43,600
problems depending how i formulate

1996
01:13:43,600 --> 01:13:44,800
problem

1997
01:13:44,800 --> 01:13:47,679
one of these algorithms works well not

1998
01:13:47,679 --> 01:13:48,880
algorithms but

1999
01:13:48,880 --> 01:13:50,320
objective functions let's call them

2000
01:13:50,320 --> 01:13:53,199
works better so

2001
01:13:53,199 --> 01:13:55,519
all right

2002
01:13:58,000 --> 01:14:00,320
so in this particular multi-methods task

2003
01:14:00,320 --> 01:14:01,120
which we if

2004
01:14:01,120 --> 01:14:04,159
we explore the g of pi is not

2005
01:14:04,159 --> 01:14:06,159
so what should i mean what should

2006
01:14:06,159 --> 01:14:08,159
corresponding to expected free energy

2007
01:14:08,159 --> 01:14:10,159
this is not uh

2008
01:14:10,159 --> 01:14:12,320
behaving that well so in a way you don't

2009
01:14:12,320 --> 01:14:13,520
get

2010
01:14:13,520 --> 01:14:16,080
uh such a good performance but i can

2011
01:14:16,080 --> 01:14:17,440
slightly change the task

2012
01:14:17,440 --> 01:14:19,920
uh and i i can get better performance

2013
01:14:19,920 --> 01:14:21,199
with geophy or

2014
01:14:21,199 --> 01:14:24,320
sf5 so as i said i'm not sure what to

2015
01:14:24,320 --> 01:14:26,080
think about this still so

2016
01:14:26,080 --> 01:14:29,040
cool well it's really like exploration

2017
01:14:29,040 --> 01:14:29,360
of

2018
01:14:29,360 --> 01:14:31,760
different things we'll we'll have you

2019
01:14:31,760 --> 01:14:32,800
back on this

2020
01:14:32,800 --> 01:14:36,159
topic when you're in a different phase

2021
01:14:36,159 --> 01:14:38,000
do you want to look at inference

2022
01:14:38,000 --> 01:14:39,440
algorithms comparison

2023
01:14:39,440 --> 01:14:41,600
or is or well yeah so inference

2024
01:14:41,600 --> 01:14:43,600
algorithms as i said this notebook

2025
01:14:43,600 --> 01:14:45,840
and just list some of the things which

2026
01:14:45,840 --> 01:14:47,120
we consider right

2027
01:14:47,120 --> 01:14:49,440
in the literature when you think about

2028
01:14:49,440 --> 01:14:52,000
this um

2029
01:14:52,000 --> 01:14:55,600
well approximate difference problems in

2030
01:14:55,600 --> 01:14:58,159
changing environments one can think of

2031
01:14:58,159 --> 01:14:59,199
different ways how

2032
01:14:59,199 --> 01:15:02,400
to solve this uh what we used

2033
01:15:02,400 --> 01:15:05,440
here is the radius representation which

2034
01:15:05,440 --> 01:15:08,400
comes from change point models

2035
01:15:08,400 --> 01:15:10,400
as an approximation for the task which

2036
01:15:10,400 --> 01:15:13,120
is a good approximation for this kind of

2037
01:15:13,120 --> 01:15:16,239
kind of pitching bandits which is what

2038
01:15:16,239 --> 01:15:16,640
we

2039
01:15:16,640 --> 01:15:18,400
uh what you show our job showing here

2040
01:15:18,400 --> 01:15:20,320
right for example this is the question

2041
01:15:20,320 --> 01:15:21,040
blue had

2042
01:15:21,040 --> 01:15:23,840
last week also so this would be how

2043
01:15:23,840 --> 01:15:24,800
probability

2044
01:15:24,800 --> 01:15:27,840
changes in us in switching bandits

2045
01:15:27,840 --> 01:15:30,880
on one arm uh over time

2046
01:15:30,880 --> 01:15:34,159
right now probability of generating uh

2047
01:15:34,159 --> 01:15:38,320
one or let's call this a reward uh

2048
01:15:38,320 --> 01:15:42,719
this is what this plot is showing uh

2049
01:15:42,719 --> 01:15:44,080
and right and this is the switching

2050
01:15:44,080 --> 01:15:46,000
concept that after each

2051
01:15:46,000 --> 01:15:48,159
uh whenever switch occurs we are just

2052
01:15:48,159 --> 01:15:50,159
sampling the new probability for each

2053
01:15:50,159 --> 01:15:52,080
arm so this is this kind of uh setting

2054
01:15:52,080 --> 01:15:53,159
with

2055
01:15:53,159 --> 01:15:56,719
non-stationary uh uh difficulty right so

2056
01:15:56,719 --> 01:15:58,159
that basically this difference between

2057
01:15:58,159 --> 01:15:58,880
the best

2058
01:15:58,880 --> 01:16:01,120
arm and second best is it varies over

2059
01:16:01,120 --> 01:16:02,480
time

2060
01:16:02,480 --> 01:16:04,320
uh and then you have like a drifting

2061
01:16:04,320 --> 01:16:06,480
dynamic for which just like the

2062
01:16:06,480 --> 01:16:08,400
different generative approaches uh model

2063
01:16:08,400 --> 01:16:09,040
would also

2064
01:16:09,040 --> 01:16:12,480
be better uh

2065
01:16:12,480 --> 01:16:14,400
but then one can also say well we can

2066
01:16:14,400 --> 01:16:16,080
use any generative model for any of

2067
01:16:16,080 --> 01:16:17,600
these problems and let's just see

2068
01:16:17,600 --> 01:16:20,000
which does inference better are there

2069
01:16:20,000 --> 01:16:21,360
any differences there right

2070
01:16:21,360 --> 01:16:24,800
so can you just uh

2071
01:16:24,800 --> 01:16:27,840
use different representations in for

2072
01:16:27,840 --> 01:16:29,840
different underlying uh

2073
01:16:29,840 --> 01:16:33,040
environmental dynamics in a way miss

2074
01:16:33,040 --> 01:16:34,719
specified in the model but still

2075
01:16:34,719 --> 01:16:37,040
doing reasonably well in for the

2076
01:16:37,040 --> 01:16:38,480
inference part and

2077
01:16:38,480 --> 01:16:41,600
right when you kind of look at the

2078
01:16:41,600 --> 01:16:42,719
results kind of

2079
01:16:42,719 --> 01:16:45,920
posterior expectations you get over time

2080
01:16:45,920 --> 01:16:48,159
uh different approaches to lead to very

2081
01:16:48,159 --> 01:16:49,199
diff similar

2082
01:16:49,199 --> 01:16:51,360
results in the end so there is no kind

2083
01:16:51,360 --> 01:16:52,960
of strong

2084
01:16:52,960 --> 01:16:55,040
advantage or disadvantage of one or

2085
01:16:55,040 --> 01:16:56,960
another and that's that's the reason why

2086
01:16:56,960 --> 01:16:58,480
we just picked the simplest

2087
01:16:58,480 --> 01:17:01,040
thing which can the more most efficient

2088
01:17:01,040 --> 01:17:02,560
basically algorithm because then it's

2089
01:17:02,560 --> 01:17:05,040
much easier to scale to

2090
01:17:05,040 --> 01:17:06,510
more arms more time steps

2091
01:17:06,510 --> 01:17:09,520
[Music]

2092
01:17:09,520 --> 01:17:12,640
okay very interesting this is a

2093
01:17:12,640 --> 01:17:16,239
pretty thorough walk through the

2094
01:17:16,239 --> 01:17:19,280
hierarchical variational smile yeah

2095
01:17:19,280 --> 01:17:19,760
exactly

2096
01:17:19,760 --> 01:17:22,320
it just describes right the generative

2097
01:17:22,320 --> 01:17:22,960
model

2098
01:17:22,960 --> 01:17:25,120
some of the steps one needs to take to

2099
01:17:25,120 --> 01:17:26,560
get to the

2100
01:17:26,560 --> 01:17:31,760
posteriors uh they're also uh

2101
01:17:32,400 --> 01:17:34,000
then just implementation of the

2102
01:17:34,000 --> 01:17:36,719
algorithm is unpacked there

2103
01:17:36,719 --> 01:17:38,719
but beside this right what we used in

2104
01:17:38,719 --> 01:17:41,040
the paper we also

2105
01:17:41,040 --> 01:17:44,480
implemented some of other approaches

2106
01:17:44,480 --> 01:17:47,360
which are non-variational kind of asian

2107
01:17:47,360 --> 01:17:48,880
inference

2108
01:17:48,880 --> 01:17:52,480
uh which is seems to be quite good and i

2109
01:17:52,480 --> 01:17:53,120
mean right

2110
01:17:53,120 --> 01:17:55,440
on average it performs better it's like

2111
01:17:55,440 --> 01:17:57,440
more optimal

2112
01:17:57,440 --> 01:18:00,400
representation now this comes from i

2113
01:18:00,400 --> 01:18:02,239
think also recent paper and multi-armed

2114
01:18:02,239 --> 01:18:02,880
bandits

2115
01:18:02,880 --> 01:18:06,320
um do you remember sarah they'll first

2116
01:18:06,320 --> 01:18:08,960
start a louis at all maybe or

2117
01:18:08,960 --> 01:18:11,600
unfortunately

2118
01:18:15,780 --> 01:18:18,840
[Music]

2119
01:18:21,840 --> 01:18:24,239
written paper and one can see that it

2120
01:18:24,239 --> 01:18:25,440
does slightly

2121
01:18:25,440 --> 01:18:27,360
better job so that would be i guess this

2122
01:18:27,360 --> 01:18:28,960
algorithm you're

2123
01:18:28,960 --> 01:18:32,480
showing now yeah what are the

2124
01:18:32,480 --> 01:18:35,679
lines representing the red blue

2125
01:18:35,679 --> 01:18:37,520
green and then the sort of flat the

2126
01:18:37,520 --> 01:18:39,360
green is just for the ex posterior

2127
01:18:39,360 --> 01:18:40,640
expectation

2128
01:18:40,640 --> 01:18:43,679
or like reward probability so right the

2129
01:18:43,679 --> 01:18:45,760
perfect algorithm should just match

2130
01:18:45,760 --> 01:18:48,640
green with blue

2131
01:18:48,800 --> 01:18:52,320
and the red is basically a

2132
01:18:52,320 --> 01:18:54,000
change point inference so basic

2133
01:18:54,000 --> 01:18:55,840
posterior probability that the change

2134
01:18:55,840 --> 01:18:56,960
occurred at

2135
01:18:56,960 --> 01:19:00,239
this specific uh moment of time

2136
01:19:00,239 --> 01:19:02,239
uh and as you can see this process is

2137
01:19:02,239 --> 01:19:04,239
quite noisy in a way right you

2138
01:19:04,239 --> 01:19:07,360
you have lots of small errors in a way

2139
01:19:07,360 --> 01:19:11,360
or slight jumps in in places where

2140
01:19:11,360 --> 01:19:13,600
change didn't necessarily occur you know

2141
01:19:13,600 --> 01:19:15,920
what what this reminds me of is um

2142
01:19:15,920 --> 01:19:19,600
the blue is some hidden true price

2143
01:19:19,600 --> 01:19:22,960
of an asset and then the green

2144
01:19:22,960 --> 01:19:26,560
is like the markets tracking that

2145
01:19:26,560 --> 01:19:30,000
price or value and then the red are like

2146
01:19:30,000 --> 01:19:32,080
orders like buy and sell orders on the

2147
01:19:32,080 --> 01:19:33,040
market

2148
01:19:33,040 --> 01:19:36,320
that represents the underlying

2149
01:19:36,320 --> 01:19:39,360
situation changing and

2150
01:19:39,360 --> 01:19:43,440
it's like just uh maybe

2151
01:19:43,440 --> 01:19:45,679
well i mean markets yes ma'am i can see

2152
01:19:45,679 --> 01:19:47,760
markets is doing some kind of difference

2153
01:19:47,760 --> 01:19:50,719
on the true friend by value of the price

2154
01:19:50,719 --> 01:19:52,320
uh right this will be kind of

2155
01:19:52,320 --> 01:19:54,400
distributed inference problem

2156
01:19:54,400 --> 01:19:56,000
yep i mean it's the whole no one knows

2157
01:19:56,000 --> 01:19:58,080
the price of a pencil but maybe

2158
01:19:58,080 --> 01:20:00,239
the person making the eraser knows when

2159
01:20:00,239 --> 01:20:01,840
the price of rubber

2160
01:20:01,840 --> 01:20:05,280
changes a little bit but but their

2161
01:20:05,280 --> 01:20:08,880
colors are just the coincidence

2162
01:20:10,960 --> 01:20:15,440
okay and then here we see something

2163
01:20:15,440 --> 01:20:19,679
um yes here is this this is a if i

2164
01:20:19,679 --> 01:20:20,159
remember

2165
01:20:20,159 --> 01:20:23,199
correctly this would be the hierarchical

2166
01:20:23,199 --> 01:20:25,600
ocean filter applied to to the same

2167
01:20:25,600 --> 01:20:26,560
problem

2168
01:20:26,560 --> 01:20:29,440
so this is work from matisse christmas

2169
01:20:29,440 --> 01:20:30,880
so so

2170
01:20:30,880 --> 01:20:33,120
he actually in his first paper he he did

2171
01:20:33,120 --> 01:20:33,920
apply to

2172
01:20:33,920 --> 01:20:36,400
inferring the price of an asset over

2173
01:20:36,400 --> 01:20:37,280
time

2174
01:20:37,280 --> 01:20:40,320
uh so when he introduced that but but

2175
01:20:40,320 --> 01:20:40,639
it's

2176
01:20:40,639 --> 01:20:43,600
found lots of uh applications in the

2177
01:20:43,600 --> 01:20:46,239
cognitive neuroscience and just

2178
01:20:46,239 --> 01:20:49,120
uh understanding how people adjust to

2179
01:20:49,120 --> 01:20:51,360
volatile environments so there

2180
01:20:51,360 --> 01:20:53,040
so what we have here is that this kind

2181
01:20:53,040 --> 01:20:55,120
of change probability is constant over

2182
01:20:55,120 --> 01:20:55,920
time but

2183
01:20:55,920 --> 01:20:57,280
you can also think of the environment

2184
01:20:57,280 --> 01:20:59,040
the change probability itself changes

2185
01:20:59,040 --> 01:21:01,600
over time so that you you need to kind

2186
01:21:01,600 --> 01:21:02,800
of adjust

2187
01:21:02,800 --> 01:21:07,520
uh to these changes also uh

2188
01:21:07,520 --> 01:21:09,120
so that would be also quite one uh

2189
01:21:09,120 --> 01:21:11,280
straightforward extension of

2190
01:21:11,280 --> 01:21:13,600
this multi-arm banding problem like

2191
01:21:13,600 --> 01:21:16,000
different types of dynamics

2192
01:21:16,000 --> 01:21:18,800
and uh testing out then more complex

2193
01:21:18,800 --> 01:21:20,320
generality models and

2194
01:21:20,320 --> 01:21:24,400
approximations to deal with that problem

2195
01:21:25,440 --> 01:21:28,000
and so right we are just comparing now

2196
01:21:28,000 --> 01:21:29,360
how this algorithm

2197
01:21:29,360 --> 01:21:31,360
tracks the underlying price value in

2198
01:21:31,360 --> 01:21:33,440
different environments

2199
01:21:33,440 --> 01:21:36,080
so i i have a question about um what

2200
01:21:36,080 --> 01:21:36,800
action

2201
01:21:36,800 --> 01:21:40,840
does so here the underlying generative

2202
01:21:40,840 --> 01:21:42,000
process

2203
01:21:42,000 --> 01:21:44,239
is stochastic but the actions don't

2204
01:21:44,239 --> 01:21:45,920
change the process

2205
01:21:45,920 --> 01:21:48,239
like choosing a different slot machine

2206
01:21:48,239 --> 01:21:50,320
isn't changing the probability of slot

2207
01:21:50,320 --> 01:21:51,280
machines

2208
01:21:51,280 --> 01:21:53,840
so it's almost like a little bit more of

2209
01:21:53,840 --> 01:21:54,840
a niche

2210
01:21:54,840 --> 01:21:58,159
modification setting or

2211
01:21:58,159 --> 01:22:02,239
can it just um be directly put into the

2212
01:22:02,239 --> 01:22:02,960
model that

2213
01:22:02,960 --> 01:22:05,840
certain actions actually change aspects

2214
01:22:05,840 --> 01:22:08,080
of the underlying generative process

2215
01:22:08,080 --> 01:22:11,520
or is that kind of a feedback between

2216
01:22:11,520 --> 01:22:14,639
action and then future hidden states is

2217
01:22:14,639 --> 01:22:17,040
that like another module that has to be

2218
01:22:17,040 --> 01:22:19,840
constructed in

2219
01:22:22,080 --> 01:22:24,639
well for what we have implemented here

2220
01:22:24,639 --> 01:22:26,800
that would definitely need an extension

2221
01:22:26,800 --> 01:22:27,199
right

2222
01:22:27,199 --> 01:22:29,920
so we kind of this is more than this

2223
01:22:29,920 --> 01:22:32,080
general representation which

2224
01:22:32,080 --> 01:22:34,560
of uh also implementation of active

2225
01:22:34,560 --> 01:22:36,639
inference which for example is

2226
01:22:36,639 --> 01:22:39,920
implemented in spm which then helps you

2227
01:22:39,920 --> 01:22:40,480
deal with

2228
01:22:40,480 --> 01:22:44,320
uh these more extended problems

2229
01:22:44,320 --> 01:22:48,000
so here we really are working with the

2230
01:22:48,000 --> 01:22:51,440
uh simplest algorithm just for

2231
01:22:51,440 --> 01:22:52,960
having it like very efficient and

2232
01:22:52,960 --> 01:22:56,080
compact so that it can scale easily

2233
01:22:56,080 --> 01:22:58,719
uh what the more general you are and

2234
01:22:58,719 --> 01:23:00,159
trying to capture

2235
01:23:00,159 --> 01:23:01,600
many different problems the more

2236
01:23:01,600 --> 01:23:03,520
difficult to have with scaling

2237
01:23:03,520 --> 01:23:06,960
right uh it's it's like

2238
01:23:06,960 --> 01:23:09,040
inference all the way up or all the way

2239
01:23:09,040 --> 01:23:11,120
down and it's it's a theme that we'll

2240
01:23:11,120 --> 01:23:14,239
return to many times which is that

2241
01:23:14,239 --> 01:23:16,320
it's all good to track the absolute

2242
01:23:16,320 --> 01:23:17,920
value of what you're interested in but

2243
01:23:17,920 --> 01:23:20,000
then the uncertainty on that and then

2244
01:23:20,000 --> 01:23:20,960
the uncertainty

2245
01:23:20,960 --> 01:23:23,199
on your uncertainty about that can get

2246
01:23:23,199 --> 01:23:24,000
you into this

2247
01:23:24,000 --> 01:23:27,120
infinite recursion so you just do you go

2248
01:23:27,120 --> 01:23:28,239
quick and dirty

2249
01:23:28,239 --> 01:23:30,400
and just have a simple idea of how

2250
01:23:30,400 --> 01:23:32,000
variance and higher order

2251
01:23:32,000 --> 01:23:34,800
uncertainties propagate or does one

2252
01:23:34,800 --> 01:23:36,239
fully specify

2253
01:23:36,239 --> 01:23:38,320
all the possible ways that uncertainty

2254
01:23:38,320 --> 01:23:40,239
can exist across multiple levels which

2255
01:23:40,239 --> 01:23:41,440
can get

2256
01:23:41,440 --> 01:23:43,199
to an explosion of the computational

2257
01:23:43,199 --> 01:23:48,400
requirements really fast

2258
01:23:48,400 --> 01:23:51,199
well i don't think that necessarily

2259
01:23:51,199 --> 01:23:54,400
hierarchy is the issue

2260
01:23:55,600 --> 01:23:57,679
but it's more right if you then start

2261
01:23:57,679 --> 01:23:59,280
assuming that your actions

2262
01:23:59,280 --> 01:24:02,400
change the states

2263
01:24:02,400 --> 01:24:06,400
state changes uh or transition matrices

2264
01:24:06,400 --> 01:24:07,360
in this stuff right

2265
01:24:07,360 --> 01:24:08,639
so basically you're effective with

2266
01:24:08,639 --> 01:24:10,560
actions you're modifying the state

2267
01:24:10,560 --> 01:24:13,600
transition matrices

2268
01:24:14,800 --> 01:24:17,760
then this requires that i mean you also

2269
01:24:17,760 --> 01:24:20,320
think about the planning problem and

2270
01:24:20,320 --> 01:24:23,040
it's not any more simple um action

2271
01:24:23,040 --> 01:24:24,480
selection problem but it's

2272
01:24:24,480 --> 01:24:27,840
then becomes a planning problem uh

2273
01:24:27,840 --> 01:24:30,480
and this makes things more complicated

2274
01:24:30,480 --> 01:24:32,719
if you introduce such an environment

2275
01:24:32,719 --> 01:24:35,199
because then it depends where you are at

2276
01:24:35,199 --> 01:24:36,639
different moments of time in different

2277
01:24:36,639 --> 01:24:38,400
states

2278
01:24:38,400 --> 01:24:41,679
yes because we've seen in the markov

2279
01:24:41,679 --> 01:24:43,040
decision processes

2280
01:24:43,040 --> 01:24:46,480
that policy pi plugs into

2281
01:24:46,480 --> 01:24:49,199
b which is the transition matrix between

2282
01:24:49,199 --> 01:24:50,639
hidden states

2283
01:24:50,639 --> 01:24:53,040
and so that's like actions changing the

2284
01:24:53,040 --> 01:24:54,000
way in which

2285
01:24:54,000 --> 01:24:57,280
hidden states are inferred to change

2286
01:24:57,280 --> 01:24:58,800
through time

2287
01:24:58,800 --> 01:25:02,080
whereas the one-step decision making

2288
01:25:02,080 --> 01:25:04,239
is um doesn't have to be done in that

2289
01:25:04,239 --> 01:25:05,679
same uh

2290
01:25:05,679 --> 01:25:08,880
way but yeah

2291
01:25:08,880 --> 01:25:12,560
and i mean i think also i mean at least

2292
01:25:12,560 --> 01:25:14,719
original implementation in spm what is

2293
01:25:14,719 --> 01:25:17,120
used this doesn't scale also very well

2294
01:25:17,120 --> 01:25:20,080
for this type of problems and different

2295
01:25:20,080 --> 01:25:21,360
groups or people have

2296
01:25:21,360 --> 01:25:24,560
started exploring like monte carlo

2297
01:25:24,560 --> 01:25:26,800
research and other methods which

2298
01:25:26,800 --> 01:25:30,800
actually allow you to then

2299
01:25:30,800 --> 01:25:33,120
figure out potentially best policy in a

2300
01:25:33,120 --> 01:25:34,159
very complex

2301
01:25:34,159 --> 01:25:37,360
high dimensional problems

2302
01:25:37,360 --> 01:25:39,760
all right but but i mean as long as

2303
01:25:39,760 --> 01:25:41,440
you're kind of in domain of cognitive

2304
01:25:41,440 --> 01:25:43,120
behavior in neuroscience you can like

2305
01:25:43,120 --> 01:25:45,120
get away with this by just making your

2306
01:25:45,120 --> 01:25:48,880
task reasonably simple

2307
01:25:49,520 --> 01:25:53,600
you have this control so yes

2308
01:25:53,600 --> 01:25:56,159
but even as as ryan like pointed out

2309
01:25:56,159 --> 01:25:57,440
earlier that uh

2310
01:25:57,440 --> 01:26:00,480
real humans even when you control

2311
01:26:00,480 --> 01:26:02,800
the experiment or you think that you're

2312
01:26:02,800 --> 01:26:03,600
introducing like

2313
01:26:03,600 --> 01:26:06,159
a gradual change in a parameter they

2314
01:26:06,159 --> 01:26:06,960
might actually

2315
01:26:06,960 --> 01:26:10,000
be cognitively doing a different type of

2316
01:26:10,000 --> 01:26:14,480
inference yeah humans are problematic

2317
01:26:14,480 --> 01:26:17,840
i don't like them oh humans

2318
01:26:17,840 --> 01:26:20,480
we should do experiments with robots

2319
01:26:20,480 --> 01:26:21,520
well

2320
01:26:21,520 --> 01:26:24,560
maybe that will be the uh i mean we

2321
01:26:24,560 --> 01:26:25,760
brought up the conversation

2322
01:26:25,760 --> 01:26:27,840
of course logistics planning motor

2323
01:26:27,840 --> 01:26:28,880
behavior

2324
01:26:28,880 --> 01:26:31,600
exploration exploitation spatially those

2325
01:26:31,600 --> 01:26:32,719
are things where maybe

2326
01:26:32,719 --> 01:26:35,440
having a defined digital twin for some

2327
01:26:35,440 --> 01:26:36,400
robotics

2328
01:26:36,400 --> 01:26:38,400
and then we go from in silico to

2329
01:26:38,400 --> 01:26:39,520
robotics

2330
01:26:39,520 --> 01:26:41,920
to starting to introduce the element of

2331
01:26:41,920 --> 01:26:42,639
the human

2332
01:26:42,639 --> 01:26:45,760
and the unknown um

2333
01:26:45,760 --> 01:26:48,159
stephen

2334
01:26:48,800 --> 01:26:50,880
sorry i think i mean like but in in

2335
01:26:50,880 --> 01:26:52,560
behavioral experiments you always have

2336
01:26:52,560 --> 01:26:54,719
this problem of

2337
01:26:54,719 --> 01:26:58,560
uh just like convincing yourself that

2338
01:26:58,560 --> 01:27:00,639
the model you're using

2339
01:27:00,639 --> 01:27:02,320
is something which reasonably well

2340
01:27:02,320 --> 01:27:04,239
represents with humans

2341
01:27:04,239 --> 01:27:06,960
are doing and you can be quite certain

2342
01:27:06,960 --> 01:27:08,320
that this is not what they're doing

2343
01:27:08,320 --> 01:27:10,000
exactly right this is just another

2344
01:27:10,000 --> 01:27:11,920
approximation of all the complexity

2345
01:27:11,920 --> 01:27:13,760
which we kind of

2346
01:27:13,760 --> 01:27:16,800
uh have in well our non-parametric

2347
01:27:16,800 --> 01:27:18,960
representation of the world

2348
01:27:18,960 --> 01:27:22,080
uh right and i mean

2349
01:27:22,080 --> 01:27:23,840
kind of there's this issue right you can

2350
01:27:23,840 --> 01:27:26,159
kind of get people to perform very well

2351
01:27:26,159 --> 01:27:28,800
on the task through lots of training

2352
01:27:28,800 --> 01:27:30,800
but then it's like okay this really what

2353
01:27:30,800 --> 01:27:32,080
i want to kind of

2354
01:27:32,080 --> 01:27:35,440
test experimentally whether like how

2355
01:27:35,440 --> 01:27:37,600
can humans learn to do this task well or

2356
01:27:37,600 --> 01:27:39,360
i'm actually interested what people are

2357
01:27:39,360 --> 01:27:42,000
doing when they are solving any tasks

2358
01:27:42,000 --> 01:27:44,639
how they represent the environment

2359
01:27:44,639 --> 01:27:47,040
date the time representation how is this

2360
01:27:47,040 --> 01:27:48,800
incorporated in their decision

2361
01:27:48,800 --> 01:27:52,159
uh model so and

2362
01:27:52,159 --> 01:27:55,120
i i think it's i don't know how other

2363
01:27:55,120 --> 01:27:57,440
people feel maybe ryan

2364
01:27:57,440 --> 01:27:59,840
i can comment on this but for me it's

2365
01:27:59,840 --> 01:28:02,960
always like a difficulty to

2366
01:28:02,960 --> 01:28:05,520
to deal with it's like it always

2367
01:28:05,520 --> 01:28:06,960
certainly or whether you're doing the

2368
01:28:06,960 --> 01:28:07,760
right thing like

2369
01:28:07,760 --> 01:28:10,880
simply enforcing something on

2370
01:28:10,880 --> 01:28:14,159
some task on people

2371
01:28:14,159 --> 01:28:18,159
yes thank you stephen

2372
01:28:18,320 --> 01:28:20,639
yeah so talking about this the problem

2373
01:28:20,639 --> 01:28:22,159
with humans

2374
01:28:22,159 --> 01:28:25,040
but um is i'm interested in how the

2375
01:28:25,040 --> 01:28:26,560
precision parameter you talk about the

2376
01:28:26,560 --> 01:28:28,000
precision parameter and it helps

2377
01:28:28,000 --> 01:28:30,480
to sort of determine whether someone's

2378
01:28:30,480 --> 01:28:32,320
going to explore or exploit

2379
01:28:32,320 --> 01:28:35,600
the context i'm interested in how

2380
01:28:35,600 --> 01:28:40,000
exploration can become a pragmatic

2381
01:28:40,000 --> 01:28:43,920
uh this precision about the usefulness

2382
01:28:43,920 --> 01:28:47,120
of something that's exploratory i.e what

2383
01:28:47,120 --> 01:28:47,679
art

2384
01:28:47,679 --> 01:28:50,800
or an experience of some sort and how

2385
01:28:50,800 --> 01:28:52,960
that meaningfulness

2386
01:28:52,960 --> 01:28:56,719
in the future could offset a pragmatic

2387
01:28:56,719 --> 01:28:57,840
gain

2388
01:28:57,840 --> 01:29:00,239
in the near term so i i i'm interested

2389
01:29:00,239 --> 01:29:00,960
in this

2390
01:29:00,960 --> 01:29:04,239
the way that precision fits in with that

2391
01:29:04,239 --> 01:29:06,320
um and sort of the evolution of that and

2392
01:29:06,320 --> 01:29:08,000
they even talk about that a bit with the

2393
01:29:08,000 --> 01:29:09,840
um

2394
01:29:09,840 --> 01:29:13,360
charge with casper hess by affective

2395
01:29:13,360 --> 01:29:14,480
charge is

2396
01:29:14,480 --> 01:29:17,920
how your precision

2397
01:29:17,920 --> 01:29:21,840
about an expectation has been violated

2398
01:29:21,840 --> 01:29:23,280
or not you know it's not necessarily

2399
01:29:23,280 --> 01:29:24,080
whether it's good or bad

2400
01:29:24,080 --> 01:29:27,520
it's whether your your prediction of how

2401
01:29:27,520 --> 01:29:28,800
well you could expect something to

2402
01:29:28,800 --> 01:29:30,800
happen suddenly got violated and that

2403
01:29:30,800 --> 01:29:34,239
amplifies everything um so

2404
01:29:34,239 --> 01:29:35,440
i'm interested in that because i'm

2405
01:29:35,440 --> 01:29:37,360
trying to create immersive experiences

2406
01:29:37,360 --> 01:29:38,239
for

2407
01:29:38,239 --> 01:29:40,800
theater and places like that but i was

2408
01:29:40,800 --> 01:29:42,000
just wondering what your thoughts about

2409
01:29:42,000 --> 01:29:42,719
the precis

2410
01:29:42,719 --> 01:29:46,800
how that precision parameter fits in

2411
01:29:46,800 --> 01:29:49,440
with that dynamic and how that could be

2412
01:29:49,440 --> 01:29:51,040
extended or if there's other

2413
01:29:51,040 --> 01:29:53,360
other parameters that kind of can fit in

2414
01:29:53,360 --> 01:29:55,920
there as well

2415
01:30:00,960 --> 01:30:04,159
um i'm not

2416
01:30:04,159 --> 01:30:06,560
quite sure that i understand the

2417
01:30:06,560 --> 01:30:08,480
questions

2418
01:30:08,480 --> 01:30:11,519
but let's see the

2419
01:30:17,199 --> 01:30:19,760
are you asking like can is precision

2420
01:30:19,760 --> 01:30:21,199
always relevant

2421
01:30:21,199 --> 01:30:23,440
yeah can you stack the precision with

2422
01:30:23,440 --> 01:30:24,400
the pragmatic

2423
01:30:24,400 --> 01:30:26,480
if that makes sense so say you've got

2424
01:30:26,480 --> 01:30:28,800
low precision

2425
01:30:28,800 --> 01:30:32,480
but you have a high precision over the

2426
01:30:32,480 --> 01:30:33,280
fact

2427
01:30:33,280 --> 01:30:35,360
that exploring the low precision would

2428
01:30:35,360 --> 01:30:36,719
be useful

2429
01:30:36,719 --> 01:30:39,520
so the two things kind of stack on top

2430
01:30:39,520 --> 01:30:40,320
of each other

2431
01:30:40,320 --> 01:30:43,040
as being a sort of a pragmatic epistemic

2432
01:30:43,040 --> 01:30:44,159
game

2433
01:30:44,159 --> 01:30:47,199
i can see that being used yes so i mean

2434
01:30:47,199 --> 01:30:51,120
we in this other paper on meta control

2435
01:30:51,120 --> 01:30:52,960
we are playing with this a bit

2436
01:30:52,960 --> 01:30:55,840
differently so we are saying that uh

2437
01:30:55,840 --> 01:30:58,239
you can kind of control your exploration

2438
01:30:58,239 --> 01:30:59,679
tendencies

2439
01:30:59,679 --> 01:31:01,760
if you learn over time the exploration

2440
01:31:01,760 --> 01:31:04,320
is bad

2441
01:31:04,400 --> 01:31:06,159
and this is kind of where this kind of

2442
01:31:06,159 --> 01:31:08,480
stacking of different levels of the

2443
01:31:08,480 --> 01:31:10,000
hierarchy here like

2444
01:31:10,000 --> 01:31:13,760
a higher level kind of agent controlling

2445
01:31:13,760 --> 01:31:16,480
the lower level comes in play

2446
01:31:16,480 --> 01:31:18,719
all right because simply if this higher

2447
01:31:18,719 --> 01:31:20,480
level observes that over time

2448
01:31:20,480 --> 01:31:22,880
uh you're like the agent is not

2449
01:31:22,880 --> 01:31:24,480
performing well

2450
01:31:24,480 --> 01:31:27,600
in a way it's not reaching the goals

2451
01:31:27,600 --> 01:31:29,360
efficiently then it kind of punishes

2452
01:31:29,360 --> 01:31:30,000
exploration

2453
01:31:30,000 --> 01:31:32,960
and learns uh just to be more

2454
01:31:32,960 --> 01:31:33,760
exploitative

2455
01:31:33,760 --> 01:31:37,199
in this specific settings uh

2456
01:31:37,199 --> 01:31:38,639
and the other way around where we can

2457
01:31:38,639 --> 01:31:40,239
make kind of a setting where the

2458
01:31:40,239 --> 01:31:43,440
exploration is beneficial always and uh

2459
01:31:43,440 --> 01:31:46,080
right yeah and this is then the agent

2460
01:31:46,080 --> 01:31:48,239
learns to kind of behave in this way

2461
01:31:48,239 --> 01:31:51,600
better so one can like

2462
01:31:51,600 --> 01:31:54,080
then i mean assume in real situations

2463
01:31:54,080 --> 01:31:55,600
this is something which people learn

2464
01:31:55,600 --> 01:31:59,199
right so just if they observe that

2465
01:31:59,199 --> 01:32:01,280
depending on what they do they gain more

2466
01:32:01,280 --> 01:32:03,120
or less they will also adjust

2467
01:32:03,120 --> 01:32:06,159
their uh tendencies uh and associate

2468
01:32:06,159 --> 01:32:09,440
this with different contexts right

2469
01:32:09,440 --> 01:32:12,000
so you could have almost like it's not

2470
01:32:12,000 --> 01:32:13,040
just precision

2471
01:32:13,040 --> 01:32:14,320
you could have like another generative

2472
01:32:14,320 --> 01:32:17,679
model managing those

2473
01:32:17,679 --> 01:32:19,280
yeah exactly i mean you can just have

2474
01:32:19,280 --> 01:32:21,280
another kind of higher level

2475
01:32:21,280 --> 01:32:24,080
representation which controls

2476
01:32:24,080 --> 01:32:27,120
uh priors or precisions on the lower

2477
01:32:27,120 --> 01:32:27,600
level

2478
01:32:27,600 --> 01:32:30,880
so right and basically

2479
01:32:30,880 --> 01:32:32,560
inducing different behaviors through

2480
01:32:32,560 --> 01:32:34,400
that

2481
01:32:34,400 --> 01:32:37,920
to connect that to the um ostensive cues

2482
01:32:37,920 --> 01:32:40,960
it's almost like if you had a a sound

2483
01:32:40,960 --> 01:32:43,679
or a cue that said okay now we're in a

2484
01:32:43,679 --> 01:32:44,880
brainstorming

2485
01:32:44,880 --> 01:32:49,120
period okay now we're gonna drill down

2486
01:32:49,120 --> 01:32:51,440
and by alternating between a

2487
01:32:51,440 --> 01:32:52,639
brainstorming period

2488
01:32:52,639 --> 01:32:56,560
and by drilling down we're going to have

2489
01:32:56,560 --> 01:32:59,280
the right kind of outcomes but giving a

2490
01:32:59,280 --> 01:32:59,679
cue

2491
01:32:59,679 --> 01:33:01,679
for when one should be in a more

2492
01:33:01,679 --> 01:33:02,960
exploratory

2493
01:33:02,960 --> 01:33:05,360
mode versus potentially like more

2494
01:33:05,360 --> 01:33:07,520
working with what is already known

2495
01:33:07,520 --> 01:33:11,120
but this looks like an interesting paper

2496
01:33:11,120 --> 01:33:13,199
this meta control of exploration

2497
01:33:13,199 --> 01:33:16,320
exploitation dilemma

2498
01:33:17,840 --> 01:33:20,480
and also framing it in terms of the

2499
01:33:20,480 --> 01:33:22,560
hierarchy of time scales whereas often

2500
01:33:22,560 --> 01:33:24,960
it's framed in terms of um

2501
01:33:24,960 --> 01:33:28,320
i guess just instantaneously what

2502
01:33:28,320 --> 01:33:31,520
maneuver would be best whether one is

2503
01:33:31,520 --> 01:33:32,159
instantly

2504
01:33:32,159 --> 01:33:35,120
preferring exploration or exploitation

2505
01:33:35,120 --> 01:33:35,840
rather

2506
01:33:35,840 --> 01:33:40,239
than uh through deep time

2507
01:33:42,960 --> 01:33:44,880
the interaction with reviewers during

2508
01:33:44,880 --> 01:33:46,800
that paper is what motivated these

2509
01:33:46,800 --> 01:33:49,199
papers

2510
01:33:49,199 --> 01:33:52,000
so that's that's like sticking like okay

2511
01:33:52,000 --> 01:33:52,320
i

2512
01:33:52,320 --> 01:33:55,040
really need something to just explain uh

2513
01:33:55,040 --> 01:33:56,800
unpack some of these similarities and

2514
01:33:56,800 --> 01:33:58,480
differences between

2515
01:33:58,480 --> 01:34:00,080
active inference and everything else

2516
01:34:00,080 --> 01:34:00,690
around

2517
01:34:00,690 --> 01:34:02,159
[Music]

2518
01:34:02,159 --> 01:34:06,159
nice um well if there's any other

2519
01:34:06,159 --> 01:34:08,560
sort of closing thoughts this has been

2520
01:34:08,560 --> 01:34:10,000
an awesome

2521
01:34:10,000 --> 01:34:12,400
set of discussions i think we'll have a

2522
01:34:12,400 --> 01:34:14,000
lot to think about and hopefully people

2523
01:34:14,000 --> 01:34:15,440
will work through these

2524
01:34:15,440 --> 01:34:18,400
notebooks keep an eye out for when the

2525
01:34:18,400 --> 01:34:19,360
final

2526
01:34:19,360 --> 01:34:23,920
paper is uh published with so

2527
01:34:23,920 --> 01:34:27,280
you got this one fully published

2528
01:34:27,280 --> 01:34:29,760
while the multi-armed abandoned is is

2529
01:34:29,760 --> 01:34:30,400
still not

2530
01:34:30,400 --> 01:34:33,600
ready it's still under review yeah

2531
01:34:33,600 --> 01:34:36,639
we are finished yeah hopefully soon

2532
01:34:36,639 --> 01:34:38,800
cool it's a first revision around

2533
01:34:38,800 --> 01:34:40,800
currently

2534
01:34:40,800 --> 01:34:44,320
well to anyone who's on here any other

2535
01:34:44,320 --> 01:34:47,199
thoughts or questions or what do we take

2536
01:34:47,199 --> 01:34:50,000
moving forward

2537
01:34:50,320 --> 01:34:52,400
i'm just thinking about different tabs i

2538
01:34:52,400 --> 01:34:53,520
could be

2539
01:34:53,520 --> 01:34:55,360
having a regime of attention on as the

2540
01:34:55,360 --> 01:34:57,199
multi-arm bandit you know

2541
01:34:57,199 --> 01:34:58,800
do i check something i haven't checked

2542
01:34:58,800 --> 01:35:00,960
in a while because i'm uncertain

2543
01:35:00,960 --> 01:35:02,800
or should i pull back to my higher level

2544
01:35:02,800 --> 01:35:04,000
and be like you know what it doesn't

2545
01:35:04,000 --> 01:35:06,159
even matter if i'm uncertain about

2546
01:35:06,159 --> 01:35:11,199
that tab i should just stick on this

2547
01:35:12,840 --> 01:35:15,360
one

2548
01:35:15,360 --> 01:35:18,320
i'm sure there'll be some fun um yeah

2549
01:35:18,320 --> 01:35:19,920
dimitri

2550
01:35:19,920 --> 01:35:21,760
well i mean it's a very general problem

2551
01:35:21,760 --> 01:35:23,360
as i said so

2552
01:35:23,360 --> 01:35:24,960
most of the problems are multi are

2553
01:35:24,960 --> 01:35:26,639
invented

2554
01:35:26,639 --> 01:35:30,960
it's not surprising right awesome well

2555
01:35:30,960 --> 01:35:33,440
sarah and dimitri thanks so much it's

2556
01:35:33,440 --> 01:35:34,560
great to have your like

2557
01:35:34,560 --> 01:35:37,360
engagement from the 1.0 on through it

2558
01:35:37,360 --> 01:35:38,000
really

2559
01:35:38,000 --> 01:35:40,320
made it awesome for the lab so thanks

2560
01:35:40,320 --> 01:35:41,679
everyone for

2561
01:35:41,679 --> 01:35:44,239
watching and until next time thank you

2562
01:35:44,239 --> 01:35:46,639
for having us here yeah thank you

2563
01:35:46,639 --> 01:35:48,400
yep yeah thanks very much until next

2564
01:35:48,400 --> 01:35:54,400
time bye bye bye


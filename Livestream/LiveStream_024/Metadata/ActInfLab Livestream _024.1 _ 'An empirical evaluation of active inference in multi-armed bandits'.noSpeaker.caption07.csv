"Speaker Name","Start Time","End Time","Transcript"
"Unknown","00;00;08;10","00;00;37;29","Hello everyone. Thanks for joining and welcome to Active Flav, live stream number 24.1. Today, it's June 22nd 2021. And we're going to be discussing the paper and empirical evaluation of active inference in multi armed bandits with several of the authors. So thanks everyone for joining. Welcome to the Active Inference Lab. We are a participatory online lab that is communicating, learning and practicing applied active inference."
"Unknown","00;00;38;03","00;01;03;01","You can find us at the links here on this screen. This is a recorded and an archived live stream. So please provide us with feedback so that we can improve on our work. All backgrounds and perspectives are welcome here, and we'll be following good video etiquette for live streams this short link has a schedule of all the live streams that we've been doing and will do for 2021."
"Unknown","00;01;03;13","00;01;29;11","And we're here today on June 22nd in number 24.1, which is the middle of the three part series on this paper about multi armband. It's the dot zero video 24.0 had some context and some background on this paper that we're going to be discussing in empirical evaluation of active inference in multi armed bandits. And today we're here with three of the authors."
"Unknown","00;01;29;11","00;02;00;07","So thanks so much for all of you who are joining today because we have a lot to discuss and learn about and in today's discussion for 24.1, we're going to first just go for some introductions and then we're going to have a presentation by the author and then we're going to just open it up for discussion. So if you're here on the video call or if you're watching live in the live chat, just feel free to ask any question and we can go wherever people are interested in going."
"Unknown","00;02;01;17","00;02;31;29","So that being said, here we are in the introductions. We'll just go around and introduce ourself and say hello. And then especially for the authors who are joining for the first time, it'd be awesome to hear anything you want to say or we'll ask you questions as well. So I'm Daniel. I'm a post-doctoral researcher in California, and I'll pass to Dave I'm in the tropical mountainous rainforest 120 miles north of Manila."
"Unknown","00;02;33;02","00;03;11;27","My background is in cybernetic learning, so theory, general psychology and machine translation, but not much math. So I floundered with much of the active inference world pool. We'll go to Sara and then continue on. I'm Sara and a personal interest and together with my original background is physics and biophysics that I wrote my dissertation essentially about experience and especially applied to learning and that's it for me."
"Unknown","00;03;12;23","00;03;42;17","I and going on to Dimitry should be able introduce him, so I am not sure if he wants to do that. OK, I mean I can also introduce him shortly. So there was our colleague from UCLA previously, so he worked in Max Planck UCLA Center for Computational Psychiatry and Aging Research, and currently his switch to industry. So he is working in Second Mind on the Applied Reinforcement Learning."
"Unknown","00;03;42;24","00;04;29;29","So basically he is a expert on this other part of the staff, which which does not color active inference with so. And yeah, I'm also postdoc Technical University of Dresden. Both me and Sara are in the chair for neuroimaging are like the head of the chairs, Stefan Kibo, and and yeah, we have been involved with active inference for a while now since probably 20, 15, 16 and have been applying it to various questions regarding human behavior, cognitive control, decision making in dynamic environments and similar so yes."
"Unknown","00;04;29;29","00;04;59;11","So should they kind of switch to the slides now? For sure we can go to do things or if I can just ask one general question to any of the authors you wanted to respond. Was it that you were interested in active inference and then looking for domains to apply it in, or were you interested in a domain then sort of found active inference as a way to integrate what you were working on yeah."
"Unknown","00;04;59;21","00;05;35;19","Well, I mean, some of my background is also in physics, complex systems and computational neuroscience. And this physicist, we like to think about these unifying theories. So in this sense, active inference has this appeal that it can collect connect very kind of distinct way of basically thinking about decision making, uh, thermodynamics, right? Stochastic. And it's very kind of different areas of understanding the dynamical systems of the set and the control."
"Unknown","00;05;35;29","00;06;16;21","So that would be the appeal. And right I mean, definitely it's a tool to apply to decision making, human understanding, human behavior. This is kind of where this'll started that so what more can we learn basically, uh, using this approach cool. And Sarah, any thoughts about especially I'm curious about the biology side because we hear a lot about the math and the physics verging towards active inference, but it's also cool to hear about biology and that was my background as well."
"Unknown","00;06;18;03","00;06;57;12","And my background is rather in biophysics and only tangentially in biology. But what I like is also that it connects with some general information processing scheme in the brain. And actually in analysis to this, I was working way, way down in the abstraction hierarchy and backing neural networks and like receptor dynamics. And actually in this area I find it easy to not see the forest for the trees and I think actually connecting outwards and ask the question, what is a general information processing that's going on?"
"Unknown","00;06;58;02","00;07;41;11","And then of unlocked again is what also attracted me to active inference. Awesome. So as usual, people will be, I guess joining or leaving. I'll unshared this screen and Dmitri, if you want to jump into the presentation, that'd be awesome. Yes, OK, so you see yeah, that's great. And all crop everything, so go for it. Excellent. OK, so let me start with the Bitcoin of motivations."
"Unknown","00;07;41;11","00;08;20;29","For this. What so well you realize kind of our involvement with active inference came from the cognitive neuroscience direction and originally I'm not so interested in the technical side of machine learning side of it. However, if one thinks about multi amp from the band, it's very it's a very general problem which kind of generalized this resource allocation problems then one realizes that this is most of kind of behavioral experiments can be crossed into this kind of framework."
"Unknown","00;08;20;29","00;08;56;11","Right. And once there's this in the range of kind of experimental experimental cognitive neuroscience domains like decision making in the Nemec environments, value based decision making, structure learning and similar to one can also think about like attention. So resource allocation problem right? So it's kind of there are many other domains where maybe they're not explicitly talking about multi and benefits as such, but they can also be crossed into this general framework."
"Unknown","00;08;57;08","00;09;33;08","And for example, for from decision making dynamic environments, one of the most well known kind of tasks is probably like a reverse learning task. Right? That's kind of used in hundreds of papers. And this is also kind of what initially motivated me to make this paper it's a kind of just an understanding. So if I apply active inference, delivers a reversal of tasks, how does this compare to other alternative decision making approaches?"
"Unknown","00;09;33;16","00;10;16;22","We can apply to that. We however read besides in this kind of cognitive neuroscience direction, looking backwards at a range of industrial applications, there really use like no starting from marketing to finance like recommendation systems in finance for trading applications. I even in many like optimization problems in the like deep learning, right. You know, there are lots of papers sexually showing how you can speed up learning rate and finding better sets of examples for deep learning systems and right."
"Unknown","00;10;16;25","00;10;56;22","So basically in a way, this also kind of allows then active inference to bridge this into machine learning gap and find new applications there. If it's if it's kind of shows to be useful for this kind of quality and then this that that this is kind of adding something new to the existing work right and when we talk kind of looked bend it's have a range of different formulations today we will talk about stationary benefits and dynamic benefits where it just means that a reward probabilities either are fixed over time or change in different ways."
"Unknown","00;10;57;06","00;11;27;03","However, people also discuss adversarial benefits, risk of error bandits, contextual bandits that I can also talk about nonmaterial benefits. That kind of rewards depend on the sequence of actions or you have kind of some memory dependance in the system and so on. There's really like a range of different kind of structural definitions of multi and vendors, which then require a different way of thinking about the problem."
"Unknown","00;11;28;24","00;12;00;12","And this is potentially something I'll see interesting for the future. Expanding what we did here into this other domains and seeing if again, this the results are generalizable to other definitions of more common problems OK, so just how I structured the slides so far, we have two parts. One is about stationary dependance and then we will switch between quote the switching benefits."
"Unknown","00;12;00;15","00;12;32;02","Yeah, and yeah, given that we have 2 hours. So I hope that will be enough time. But if yeah, if not, we can also continue it for the next session. For example, with switching benefits or so. Let's, let's see how this goes OK, so what is stationary bandwidth? So the definition of the problem is just follow, right? On any trail, an agent makes a choice between perhaps a choice outcomes are binary variables."
"Unknown","00;12;32;02","00;13;14;22","So we've kind of focus here on Bernoulli. Bandwidth outcomes can also be drawn from any other distribution so and then for example, you talk about Goshen benefits. Yeah. Depending on the how the rewards are generated, we will just work with Bernoulli here, here benefits and we may have fixed a reward probabilities some very specific way. So there is only one arm which has reward probability associated B max, which is one half plus some which is larger than zero and all other reward probabilities associated with other arm."
"Unknown","00;13;14;22","00;13;47;21","So just fixed the one for this kind of allows us to like control the difficulty of the task. So because the smaller the epsilon is, the, the closer to zero is, the more difficult to distinguish the best arm from from others and the more samples one needs to draw to kind of realize the difference for a R. And right beside this, like the best arm advantage there, one also realizes that the right number of arms OK is also increasing the difficulty."
"Unknown","00;13;47;23","00;13;58;29","The more arms you have, the more time you need to figure out what is the correct way. So just to give you an illustration of the example, right, we have four arm banded here."
"Unknown","00;14;02;04","00;14;34;03","So whenever an agent pulls one of the arms against it gets either zero or one. So we can think of this as a reward or absence of 30 worked right beliefs about the reward probabilities. We will assume that right these a beta distributed so we will use beta distribution to model kind of a representation of reward probabilities associated with the far and we will use Bayesian belief updating for all action selection algorithms."
"Unknown","00;14;34;10","00;15;08;08","So this also kind of limits the range of algorithms which we want to compare instead I will explain a bit later why I mean we focus on that so this could give you a kind of visual example of what this does is right whenever a kind of an agent starts with kind of flat beliefs over reward probabilities associated with each other."
"Unknown","00;15;08;09","00;15;37;11","So this is a uniform distribution this is a special case of beta distribution. And whenever it pulls select, select one arm, it gets either one or zero same outcome and it updates, right the beliefs about the reward probabilities in one of the directions or in here. Right. It kind of it's sampling one more often. So it has increased belief that there is a higher reward probability association."
"Unknown","00;15;37;17","00;16;06;03","So this so formally how this is kind of implemented, what you see here is like traversing a general to model. So with each arm, we assume there is some associated unknown reward probability that the key rate, which is just a number between zero and one and choice of outcomes of choices are either zero or one. So they're just binary choices."
"Unknown","00;16;06;08","00;16;34;08","So this kind of constraints are like observation likelihood or Bernoulli distribution. So this is just kind of product of different Bernoulli distribution depending on which arm one is selecting. And this kind of the below like is our prior belief about reward probabilities. And this will assume is just kind of product of many beta distributions where here alpha080 are just fixed to one."
"Unknown","00;16;34;08","00;17;30;03","So this corresponds to the uniform right? One can also start with other values but think this makes some sense and reflects no knowledge apriori about ok, so now given some choice at trial 80 we just apply by a subject rule and we get from some prior beliefs are posterior beliefs. And that I think about the stationary case is if our priorities are distributed our posterior so be all submitted and that one only needs to take care about how we update rates the parameters of the distribution alpha and beta and this just works in the form of accounting basically alpha counts how many times you observe 1% outcome and beta counts how many times you observe"
"Unknown","00;17;30;04","00;18;13;09","zero, right? And so because of all this, in this stationary case, inference is exact so that because both prior and possibly belong to the same distribution family, we have kind of conjugate prior to your setup so you can just track effectively the updating. There is another kind of representation of the update rules one can use here. So for example, if we expressed the updating in terms of expected kind of outcome and the scale parameter new here, we see that actually the expectations are updated in the form of a delta like learning group."
"Unknown","00;18;14;06","00;18;33;19","So right where this kind of learning rate is something which decreases over time. So basically new is just increasing over time whenever you select specific are a new rule just increase by one. And this means that the learning rate decreasing over time. So the more you sample from one arm, the less you will update your beliefs about that."
"Unknown","00;18;34;29","00;19;06;04","Right? So this is practically what's happening here. But very, very simple OK? So and basically this is everything we need to know about kind of this let's call it perceptual part of the charity model. So how we update beliefs given some outcomes. Now we have to kind of introduce how we select actions based on these beliefs. And for this we kind of can talk about actions, selection, algorithms."
"Unknown","00;19;07;04","00;19;42;19","And so they are kind of in the literature. There are lots of examples. One of the oldest one is probably Epsilon Greedy. You also mentioned that during the first presentation, .05 are people also talk about, right, the UCB, upper confidence bound. This is also one of the oldest one L UCB. Uh, it's kind of expansion which uses K l as an exploration of divergence, an estimate of exploration bound."
"Unknown","00;19;43;16","00;20;13;00","There is dubuisson, sampling, etc. Over here we will focus mostly on this tree right we will use the mystic function sampling. And so one kind of comparative example and another is Bayesian upper confidence bar. So first, most of these algorithms have been extensively analyzed in the literature and people just found some that they work in many examples lot much better than kind of let's say non Bayesian approaches."
"Unknown","00;20;15;16","00;20;42;25","And secondly, right, we can use basically for all three algorithms the same update rules because they are just Bayesian that they're just algorithms corresponding to Bayesian. And then so we can kind of our motivation here is to compare kind of actual selection principles based on the actual selection algorithm and not based on potentially different ways how you update beliefs about about the history of observations."
"Unknown","00;20;44;11","00;21;13;28","And so this would be kind of motivation and also a bit to simplify the comparison. I mean, you can like add to the list at least ten other ways of doing the rate action selection and even like decision making in German OK, so just a bit to kind of historic example important is it just like upper confidence bond?"
"Unknown","00;21;14;12","00;21;46;29","Although I will not directly compare it to just some relevance to understand also what is happening kind of in active inference later and this this form here just corresponds to a version which is adapted to specifically Bernoulli bending. So some people might be more familiar with the other version which was derived from Boston. And so this is kind of in this paper here, one can see a these different derivations and examples how this works."
"Unknown","00;21;47;08","00;22;25;15","What is important to notice here is basically that so this first time is just the expected probability of reward on arm two. And the second two terms correspond to this exploration bonus or the bound and so this is typical for UCB is that the bound kind of increases over time a lot longer than weekly with the number of kind of trials and doing so basically the more you are not selecting one arm, the more you're kind of pushed again to select it at some point in time in the future, all right."
"Unknown","00;22;25;16","00;23;08;18","So basically, this necessarily increases over time if you are not interacting with the specific car right but still older like. Right. The time is very simple. It has nice theoretical results. So because it's kind of it's efficient algorithm, it converges in infinite number of samples however. Right will talk on this kind of Bayesian variant of the upper confidence bar, which actually make a select arm's based on kind of the percentile upper percentiles of of cumulative distribution function."
"Unknown","00;23;08;18","00;23;44;25","Right. So so as time progresses, year one is trying to estimate the like the extreme value of a believed distribution, which in this case is just like a beta distribution. And one gets this extreme by solving this equation which just corresponds to inverse regularized, incomplete beta function. So basically, the more extreme point your beliefs contain, the more likely that I mean, the more likely that you will select the types of the more relevant is to select this arm."
"Unknown","00;23;44;26","00;24;15;08","Because you're expecting that, right? This high value is still possible in the reward probability in this and this at the. And so but this kind of algorithm has a couple of parameters. But what are authors kind of showing this paper is if you have very good results for just by fixing see to zero. So basically this term just becomes one and so we just use their advice in the paper."
"Unknown","00;24;15;08","00;24;48;07","So we will not and we didn't kind of try to analyze other possibilities or other kind of values of these parameters in the in this case, so for some some sampling, this is again one of the classical algorithms first probably attempts with Bayesian and Bayesian decision making. And it's also extremely simple, right? Give us some beliefs about probabilities at each arm."
"Unknown","00;24;48;15","00;25;20;01","You'll sample one point so right to bear arms, you would sample from each arm one point, and you will just select an arm which gives you the largest that's right. A variant of that which was shown last five to ten years that it works slightly better in this optimistic stance on sampling where one is constraining the samples, only two values which are larger than the expectation."
"Unknown","00;25;20;01","00;25;50;24","Right. So so one is first making the sample from each arm. And then if the sample is larger than the expected probability of reward, then one gives the sample. Otherwise you just use the expected value as as a value. What kind of reward probability associated with that, right? And you're just again over different arms maximizing, taking selecting the arm, which gives you the maximum reward probability."
"Unknown","00;25;52;02","00;26;21;19","Right. This is a kind of a very stochastic approach where this exploration actually kind of bonus to this algorithm comes from basically this random sampling from a probability distribution and the broader your beliefs are about something, the more likely is that you will get the kind of large value chance, the more likely is that you will explore more select that that arm them in different tracks."
"Unknown","00;26;22;08","00;26;30;00","And basically exploration is going on here, completely driven by the Noisiness of the sampling process itself."
"Unknown","00;26;33;26","00;27;05;16","OK, so now we come to basically active inference version of this so we are kind of simplifying things from what people maybe know how to infer it is used normally. The first we will videos like caroling behavioral policies, which means that agent is not like tracking history of actions that perform it just kind of repeating step of policies on every trial."
"Unknown","00;27;06;17","00;28;03;11","And in this case, the behavioral policy just corresponds to a single choice. So in this type of bandwidth problems, we we are kind of analyzing sure. Like agent cannot change anything in the environment since planning is completely irrelevant in a way you cannot kind of position yourself in us in space better over time so that you kind of need to plan something, which means that this kind of single choice policy evaluation, a single time step in the future is sufficient to make a good decision and right a generally expected we will base our action selection on expected free energy here where this will be a form which decouples into a risk and ambiguity term."
"Unknown","00;28;04;27","00;28;44;16","But we can also think about this problem as estimating expected value of different arms plus the expected information gain. So how much information we can extract from different arms if we select so now let's assume that we know how to compute expected free energy at a go to details on max slides. Normally like right posterior, our policy now is these estimated this kind of mixture between expected free energy."
"Unknown","00;28;44;24","00;29;10;12","So kind of a future expectation about what behavior will do plus the kind of the second part, which is the free energy about the past kind of outcomes. However, because we have like rolling policies, this term is just constant for each policy is the same. It has the same value basically. So on the posterior policy where action just corresponds going to the to the soft max right."
"Unknown","00;29;10;12","00;29;51;03","Or the expected free energy and normally 11 things like about choice selection, this is a sample of samples from the posterior. So we are actually only interested in kind of optimal choices. So for us, basically, gamma is just infinite value and we are just selecting the making. So choices about actions which minimize expected choice. So this this is useful parameter to have if you need to kind of fit a model to the behavior but for this kind of practical application, there is not much gain in adding another source of nice here."
"Unknown","00;29;54;26","00;30;27;28","OK, so how do we compute the expected free energy? Right. Well, that's in a way quite simple. So we just have a couple of terms here. One is posterior over posterior beliefs, over reward probabilities. But given this like cute term, this is just the product of beta distributions. We have marginal outputs, so probability of observing the outcome, given some action, it is so this is just marginalizing likelihood our or our current posterior beliefs."
"Unknown","00;30;29;02","00;31;27;00","And another term is here in just the prior preference over different outcomes. Right. And this is really easy to parameterize because we work with binary outcomes and we can just define a single parameter lambda. So the higher the lambda is, the higher the preference is of the agent to observe once relative to zero. Right. And this lambda parameter also kind of has a role of regularizing the amount of exploration that the agent does because the larger the lambda is, the more the more focus is is set on the exploitation of kind of making selections based on the expected value instead of expected information and so this final term computing ambiguity is basically just computing expectation"
"Unknown","00;31;27;00","00;31;56;13","over the entropy of different outcomes. Right? So it's also relatively simple for sure. Because of Bernoulli like. So without going into all the details, this is basically how the expected free energy looks like. So this first term here is correspond just to the expected negative of the expected reward. So because we are minimizing expected free energy, this effectively means that there are maximizing expected reward."
"Unknown","00;31;57;24","00;32;25;11","And the second term is just a very complex set. So that's sort of like equations, which gives you an estimate of the expected information gain. Right and in a way kind of motivation is just because one cannot really understand what is going on here, right? We have kind of logarithms of expected reversal abilities, but then we have a diagram, a function of parameters."
"Unknown","00;32;25;11","00;33;01;03","And so instead we can kind of try to simplify the term by approximating right effectively the information gain part just to get a better understanding of how basically expect this for energy scales like with repeated choices. And so one ends up with a very simple form where basically the exploration term just corresponds to one over two divided by the number of time one selected different answer."
"Unknown","00;33;02;02","00;33;33;09","So this is in a way quite similar to what we had in the in UCB algorithm. Right. The first exploration term was also divided by something which is proportional to how many times you selected an and with but without this kind of expansion of exploration bonus, it's time. So there is no logarithm of T, and I mean this will have a consequence on efficiency of active inference."
"Unknown","00;33;33;09","00;34;02;15","So I will show you show this in a moment. So how one achieves basically this simple formula just by assuming that for large the big function shows this type of approximation that X is sufficiently large, right? So there just means when you sample sufficient number of time each arm, this will be more or less the very accurate approximation and we see that this approximates so algorithm and the exact they became very similar."
"Unknown","00;34;02;15","00;34;42;11","So it is in a way reasonably good approximation so if you actually then I think you ask like a question about the scaling properties of this one kind of motivation for introducing approximation is that in a way the algorithm is much simpler, so it scales better so you can kind of run it on much more. Arm's easier however like right, the advantage is not huge, so maybe it scales slightly better but you are maybe gaining like 10% or 20% performance in computation time."
"Unknown","00;34;43;18","00;35;06;29","So it's not something like which destroys completely the exact for at least not in this example. Right? Because the problem itself is simple. However, it helps us to better understand better just get the intuition of what is happening. So and it means like right now the exploration bonus changes with that up."
"Unknown","00;35;09;08","00;35;36;02","OK, so before I kind of just show some result like comparison of different algorithms I want to introduce just concepts which come from a machine learning analysis of multi amp and this which people use like this just to kind of rate the and compare different algorithms like how good they are in solving this task. And this is done by using something defining the regret."
"Unknown","00;35;36;12","00;36;24;13","So basically regret is simply a difference between what you did at trial. T minus what was the best choice that and that's trial and so kind of assumption here is that there is some Oracle which is solving the test which knows exactly what was the best choice. And in every trial and so normally people kind of consider two quantities as either a cumulative regret, which is just some of of regret or up from first trial to the current one or a regret rate, which is just the average or kind of come on to regret."
"Unknown","00;36;26;07","00;36;55;27","And we will use both just for visualizing different aspects of the algorithm and kind of in stationary case at least this is a very kind of important result because with all for all good algorithms, one would expect that this regret goes to zero over time. Right. So I ask about the infinite number of trials. You should be able to always do like a good choice."
"Unknown","00;36;57;06","00;37;12;17","And if this is the case, then one can show that algorithms which have this property that they are close are asymptotically efficient and they scale for large. This is something some terms times a logarithm over t right."
"Unknown","00;37;14;29","00;37;47;24","And so this is kind of a one important aspect of this first stationary case of different decision making algorithms. So multiple embedded algorithms so they should at least scale as logarithm of t when you expand, right when you go too large to limit otherwise right. This first thing will not probably hold well or they can also scale slower than lower degree."
"Unknown","00;37;48;13","00;37;53;20","But for example, if they increase linearly with that, this will not hold any anymore."
"Unknown","00;37;56;23","00;38;33;12","OK, so now let's go to the comparison, right? We will look into how the missing tops and something Bayesian upper confidence bounds exact different inference algorithm and the approximate one compared to each other I will start first with just trying to see what would be kind of a good value for this lambda parameter. In different settings and also to give some kind of initial comparison of exact algorithm and the approximate one."
"Unknown","00;38;34;06","00;39;01;23","So what I'm showing here is a regret rate, right, for different kind of snapshots. So these dotted lines are after 100 trials. The dashed line is a 4000 trials and the solid line will be after 10,000 trials. Right and so what one kind of notice is here is that there is some obviously kind of a minima for different lambda."
"Unknown","00;39;01;23","00;39;32;27","So this kind of preference parameter uh, and that the longer kind of, well the more trials you do, the smaller the lambda should be. So this, this is not quite nice and this kind of havoc as a consequence. However, we can just pick some value which so for example, this purple dotted line shows like around 0.1, Lambda 0.1, which seems to have be close to minimum for most of these cases."
"Unknown","00;39;32;27","00;40;01;06","Right? So we don't want to kind of have different value for different examples because I mean, this is just not practically feasible you want to have a kind of general algorithm which can be applied to many different situations at the same time. So what when we compare so Bayesian, you'll see Mr. Thompson sampling and just the approximate active inference."
"Unknown","00;40;01;06","00;40;21;25","So I'm just excluding this here the exact one, because they will behave the same pretty much for this specific parameter value. If we compare them in terms of cumulative regret, we see that, right? The approximate active inference is not the syntactically efficient. So this code just goes divergence over time."
"Unknown","00;40;24;06","00;41;06;25","Whereas if if you if you look at the kind of the green and the yellow curve, they kind of flatten out after some time and they get the slope proportional to this dotted line, which actually shows the slope of this kind of asymptotically right. But you should see for large teeth now. So why is this happening? So the thing is that because of this exploration bond is kind of just decreasing over time active inference algorithm kind of gets stuck into the wrong solution with some probability, which depends on the difficulty of the task."
"Unknown","00;41;06;25","00;41;32;29","So right the smaller the epsilon is, the more likely and the more arms you have. So for a small epsilon and for a small number of arms, there is a higher probability that you get kind of stuck. Right. And one can see this if kind of we take a snapshot of different runs. So this is kind of a distribution of cumulative regret."
"Unknown","00;41;33;19","00;42;05;05","I'm just blocking the logarithm here. So like making your histogram of over different amounts of this is like that thousand runs of different algorithms and just showing at what value for the logarithm of the cumulative regret they end up. And so you can see here, there is for kind of active inference based algorithm, you see this kind of well spike here and the tail of the distribution, which is proportional to just doing random choice."
"Unknown","00;42;05;08","00;42;46;11","So that means that the algorithm was just selecting wrong arm constantly. It never it never converge to a correct solution. You know, it gets stuck to solution because the exploration bar was reduced to two. So right. So so this is kind of say limitation or application effective inference to stationary problems because this is not a feature of an algorithm you would like to have in a way a normal life if you are kind of example of this would be, for example, optimization problems that you want to find."
"Unknown","00;42;46;27","00;43;18;22","The best solution for a set of parameters are now, for example, Bayesian optimization, finding the minimum of some unknown function uses dumps and sampling. This kind of seems to be a very efficient way of finding the minimum. However, if you would apply it to such situation, and active inference, Bayesian kind of arm selection or sample selection, there is a chance that the algorithm fails, right?"
"Unknown","00;43;18;24","00;43;49;29","You'll just kind of get stuck in the wrong meaning. Wrong up to me doesn't do sufficient exploration. So this kind of requires potentially some adjustments to how actions so selected, at least in the stationary case, so just to walk strange stuff, I got some strange slope just to kind of do a bit what the short term behavior looks like."
"Unknown","00;43;51;05","00;44;32;00","So from the perspective of kind of cognitive neuroscience or like human decision making are don't really care about this asymptotic limit because you don't usually expect people to be in either stationary environment. Things always change or rather they kind of have to repeat actions so many times. So what I am showing here now just in a very reduced example, so if we have only three arms and we just use kind of different epsilon values, so ask the difficulties, what is the probability that for different algorithms that you select actually the optimal arm?"
"Unknown","00;44;33;05","00;45;22;01","And as one can see that right initially no so Bayesian used to be for first maybe 25 trials have the highest probability to select an arm. However, there is a range of trials like from 50 to maybe thousand where active inference based algorithm takes over so right in a way because of this information game term active inference is more efficient in the target like the arms, which will give you the most information because then it can recover the best arm in kind of some intermediate interval with the highest probability."
"Unknown","00;45;22;18","00;45;57;21","However, I still kind of expand this after like a thousand trials of so you'll see that this probability gets stuck. So it never converges to one unlike the other over this, especially evidence for this difficult, difficult problem. Small AppSumo and so this is in a way explanation. So what's happened? So basically algorithm, although it reaches good solutions after its high higher probability than other algorithms, there are still lots of lots of examples."
"Unknown","00;45;57;21","00;46;21;26","So in this kind of simulations, part of simulations which get stuck to a wrong solution and they cannot get out of this. So in a way, this kind of ask question, OK, what can one do to make active inference also syntactically efficient? So how can the maybe either generative model be changed to support increasing the exploration bound over work time?"
"Unknown","00;46;22;14","00;46;57;26","Or maybe introduce kind of instead of computing expected expectations in of expected free energy, one can also just draw samples from a posterior and kind of also compute this this two term slick information game and similar to like bumps something. So right there are kind of different ways one can think of how to add exploration bonus and a third option would be to actually introduce learning of the lambda parameter so that lambda itself then goes down over time."
"Unknown","00;46;57;26","00;47;56;00","So it kind of goes towards zero in specific specific rules. However. Yeah, currently we don't have very good solution for this, so we just leave it to this as that as a obvious limitation of just applying active inference to this type of problems now there are kind of maybe any questions. I think this would be like the stuff where we switch now to the perfect other if anybody has any thoughts, definitely I could ask some things or also we'll ask if in the live chat people want to post any questions, but how much longer of a presentation did you estimate that you had so that we could kind of also address some general points here"
"Unknown","00;47;56;00","00;48;22;21","during this point? One of well, I don't think there is more than maybe 20, 30 minutes max. I didn't really gauge it, but there is less slides in the second one. Would it make more sense to go through it quickly here? Yeah. Or would it make sense to do it in the next weeks? I serious. I think both are fine for me."
"Unknown","00;48;22;21","00;48;58;07","I mean maybe an additional 15 minutes. OK, fine. So we'll complete the presentation and in live chat and on the video chat here we'll compile our questions and then in the remainder after your presentation today and the next week, we can have more open discussion. So please continue next. OK, OK, so now we go to this like dynamic non stationary problem and and the trial in this case, it's a very similar setup."
"Unknown","00;48;58;07","00;49;26;05","On any trial, an agent makes a choice between arms again we are focusing on the Bernoulli banded. So outcome side the just binary variables what happens here is that the reward probabilities associated with time change over time right and we kind of differentiate between switching and it's where we will assume that the changes happen at the same time on all arms."
"Unknown","00;49;26;24","00;49;51;05","And furthermore in switching bandwidth, they're kind of also called piecewise stationary so there is like a period where nothing changes and then there is just one moment in time the rewards reward probabilities on all arms change or we can think about a call of another variant of dynamic band. This would be Restless Bandits, where changes happen independently on each arm and they are continuously changing over time."
"Unknown","00;49;51;05","00;50;20;08","So like for example, for a random walk, I will only talk about switching bands, but from some testing I did a all the results generalize also to the Restless case and in this kind of beside this number of arms and the difference between the best arm and the other arms of kind of revert probabilities, epsilon and K, we have another thousand difficulty."
"Unknown","00;50;20;08","00;50;58;04","This is the rate of change or like just change probability. So the more often things change, the more difficult task, most especially if you have many parts and I will consider like switching bandits with fixed difficulty with just the extrapolation of the stationary case or by introducing changes about which arm is associated with the maximal reward. Right. We will always have the same reward so not only arms did just that from time to time, optimal arm changes with probability."
"Unknown","00;50;58;19","00;51;32;00","Right? And in this case we have right just three parameters which define our task thus difficult. We can also think about switching bandits with varying difficulty, which then just means that with probability arrow, the reward probabilities associated with each arm either remain fixed on until next trial. So they are just kind of translated or they are sampled. I sorry with some probability, one -4 should be right."
"Unknown","00;51;32;00","00;52;15;06","They are staying fixed or with probability they're just sample from a uniform distribution. In this case beta distribution with parameters one and in this and this the variant of the task, right? We would just have a s and draw as a fixed as difficulty parameters. So epsilon kind of disappears so your can averaging or absolute and does and just to give you an example, so we will discuss this with how restless bandits set up looks like as basically you can you are for example that the logit transform of of reward probability just follows a random walk."
"Unknown","00;52;15;06","00;52;49;17","So just the Brownian exploration in the logic space of the reward. And this would also require potentially changing the generative model which I would introduce. But it's not necessary. So one gets for similar solutions and behavior. So now to come back to the example from before, if we have like multi and blended with four arms, the setup is exactly the same."
"Unknown","00;52;50;14","00;53;17;24","And in addition, we assume that the agent has access to the underlying probability of change. So this is not something which is unknown. This simplifies the learning rules I believe. Update the questions however, one can extend this. What I will introduce today to the setups where the probability of change has to be returned also or that probability of change is also something which changes over time."
"Unknown","00;53;17;24","00;54;13;14","So that right you can have to track how often reward probabilities change of different so this will be kind of example of decision making in volatile environments so to visualize the algorithm and basically the only difference is practically that now you have an effective forgetting of what you and of what what agent learned before. And one can see this for example, if you look on this square to the left as kind of time evolves and agent is selecting other arms, this value which reward probability associated with the leftmost arm just decays back the uniform probability."
"Unknown","00;54;13;14","00;54;49;25","Right. So in a way, agent is forgetting information or expectations it had about this arm. And it assumes that with time the reward probability will beliefs about zero probability will revert back to a uniform distribution and the algorithm is really a straightforward extension of what they already described before so the generative model now is slightly more complex. So this besides the likelihood term."
"Unknown","00;54;49;25","00;55;33;25","So observation likelihood which remains the same is just the Bernoulli distribution. Now, if you have a kind of state transition term which tells us how reward probabilities change over time. So and what this means is that if one believes that there is a change, a reward probability will be independent from the previous values and they are just drawn, they just belong to a uniform distribution so this kind of prior belief and if there is no change around the transition corresponds to a delta function, which just means that the that the reward travel to stay unchanged from trial to be -1 to two."
"Unknown","00;55;35;03","00;56;11;27","And finally, the prior on each trial we share the same prior about probability of changes. And this is just again a Bernoulli distribution with probability right which the theory will just assume this is a known parameter to the agents so the problem here in like dynamic cases, you can still apply the the bias rule and you can compute the posterior both for the change probability terms on JT and for the marginal posterior about reward probabilities."
"Unknown","00;56;11;27","00;56;42;02","However, as you see here, the exact kind of before of the posterior is not anymore doesn't belong to the conjugate. So the prior is not any more conjugate probability distribution to the likelihood and this is not any a simple beta distribution, but it becomes a mixture of beta distributions and as you are kind of evolving into the future, this becomes larger and larger mixture of beta distributions, which is well practically intractable right?"
"Unknown","00;56;42;02","00;57;08;03","If you kind of expand this to open and, and the number of trials. So because of that we want to have something which is a bit more efficient algorithm we can basically introduce some meaningful approximation when we now say that, okay, our probability distribution can be described as a product of a bunch of beta distribution and a categorical distribution, which just tells us the probability of change, right?"
"Unknown","00;57;08;03","00;57;51;05","And trial are and how what this corresponds to here. So basically what is actually we are using here a bit of it's not a standard variation inference. So you would have to kind of compute the gradient over the variation free energy to find the optimal. This simplifies the things because you just need one step to update parameters both about change and change probability and about right to reward probabilities are so this makes it not super optimal."
"Unknown","00;57;51;05","00;58;16;21","So there are better solutions. So what can do this? But it's very efficient. So and in the end for the Bernoulli bandit's this there is will not be much difference you can use better algorithms, but this gives you just marginal advantage on the long run just because the the problem is very noisy and it's very difficult to actually figure out the correct choice."
"Unknown","00;58;17;12","00;59;15;18","So what this variation on Smile does. So it was introduced by Vasily in well quite recently 2021 so that they provided a bit more detailed justifications for what I, what I'm saying here, I'm just paraphrasing, paraphrasing a bit how the algorithm is defined. So basically we can associate the marginal about change probability with the exact posterior marginal because you can compute this analytically and then we use this as a basically a known known belief about change probability to estimate the to estimate the reward probabilities by averaging in the log space over our different prior beliefs about reward from it."
"Unknown","00;59;15;21","00;59;48;04","Right. So basically, instead of averaging in the probability space, you're you're kind of averaging in the in the log space here what this kind of results is very in a very simple set of update rules. So on the left side, we just have just showing how omega t is updated and this is just correspond basically to forming beliefs about change probability based on bias factor shown on here."
"Unknown","00;59;48;04","01;00;16;28","Right. Which is just the likelihood between observing odds given that the change didn't happen and observing the given that the change happened right at the current drop. So and then using that estimate to update your beliefs about different probabilities and basically depending whether you selected the item or not. So basically the omega term here plays this forgetting rate."
"Unknown","01;00;18;01","01;00;42;21","So the the larger the omega, the closer to one. So the larger the probability that change occurs, the more you will revert back to the beta zero and alpha zero parameter. So the initial prior beliefs and less you will depend on your current current beliefs from the kind of previous trial and this also has an important limit like, right?"
"Unknown","01;00;42;21","01;01;02;07","If the agent believes that there is no change in the environment, you will you will revert back to the exact inference and the update rules that you had for stationary case. So and that's kind of the nice thing about this algorithm, it's kind of just generalized to any, any knowledge about probability of change from it."
"Unknown","01;01;04;09","01;01;40;04","OK, so the action selection algorithms did not change. So so the learning rules will change, but we are practically doing still the same way of of making action selection. So for Thomson sampling we are sampling from the posterior beliefs. For the Bayesian upper confidence bomb, it's slightly different so we are using kind of the mixture between again the mixture of possible parameter values to estimate the inverse because from this predictive posterior it's difficult to inverse."
"Unknown","01;01;40;04","01;02;18;20","It's a mixture of two beta distributions. So this is just kind of approximation one can use for Bayesian, you'll see B and for action selection approximate expected three energy, one gets the very same set of equations because basically zero can just so probability of change drops out it can be in can be ignore easily there are OK so now if we do kind of the same comparison first of exact and approximate after inference algorithms we see a slightly different picture to what we had before."
"Unknown","01;02;20;16","01;02;48;28","Whereas it seems that as you increase the number of trials and you kind of compute this regret rate for this specific number of trials, the regret trade does not change. So in a way algorithm converges very fast to a specific regret rate and for different and there is a clear kind of stable minima independent of number of trials."
"Unknown","01;02;48;28","01;03;21;20","You're exposing algorithm to a similar again we see like very similar behavior between exact of the active inference algorithm and the approximate rate. And for this example, I will I just kind of fixed lambda to 0.5. So it just seems to be reasonably well a value parameter for many many situations which we see here. However, so here I am solving for ten arms."
"Unknown","01;03;21;20","01;03;51;06","If we go to 80 ions the picture slightly changes. So it seems the optimal lambda parameter, although it doesn't change with it, it changes with the number of arms and obviously with all the other Rho and Epsilon, all the other parameter. So this makes things a bit difficult and basically suggests that it's important to kind of find potentially a learning rule like self optimizing way of estimating lambda."
"Unknown","01;03;51;06","01;03;59;19","And there there is some work actually in active inference literature which potentially has a solution for that. So we just have to test it out at some point."
"Unknown","01;04;02;20","01;04;49;10","So however, we can still use the same value. This does not influence that much. The performance later on. So what we see when we kind of compare it to its approximate active inference with Bayesian CBN sampling is that for a range of rate settings of either it's like fixed advantage of the of the best like reward probability, advantage of the best time we see that active inference is just right the performs better well already after like a thousand trials or so compared to the other two algorithms this is just four different change probabilities."
"Unknown","01;04;49;10","01;05;24;10","So for example, the role 0.05 corresponds to a change every 200 trials it should be every 100 trials on average. And the last step here is 25 trials. So this would be the most difficult rate scenario so one seizes that look at the more more difficult scenario is that the differences disappear. So you're kind of losing the advantage however, if simple OK, similarly, if we look at the if if X number of arms for example, 240 and we just change the epsilon parameter a similar trend is visible, right?"
"Unknown","01;05;24;12","01;06;02;27","That the more easier the task is, the bigger the difference in like non stationary scenario relative to two other algorithms, of course, one would potentially kind of destroyed this advantage if changes become very slow like every 10,000 trials or something where you are kind of approximately in the stationary is stationary around right and OK, so this would be like this switching bandage with varying difficulty."
"Unknown","01;06;02;27","01;06;33;02","We can do the same kind of analysis for a switching bandwidth. So previously we looked at switching methods with fixed difficulty. Similar analysis can be done for a varying difficulty. And interestingly, one is here now because Epsilon drops out once is here actually for the exact time for active inference. There is like a two year minima which seems independent on the kind of any of these parameter terms, the zero or K."
"Unknown","01;06;33;18","01;07;10;08","So basically we can fix lambda to 0.25 for the exact active inference algorithm and we can still keep for the approximate lambda A2 for the approximate active inference, our lambda to 0.5 and just right. So what happens here is that there seems to like invariant difficulty. There seems to be also advantage of using exact active. In France, the decision making algorithm outperforms the approximate approximated quite clearly in this setting and it doesn't require a fine tuning in this sense."
"Unknown","01;07;10;08","01;07;52;23","Right. For the range of problems, you can have much better performance with a single choice of lambda value. And again, so the in this case, interestingly, the easier the problem becomes. So in this kind of quandary, the less the difference between algorithms is and that actually Bayesian UCB algorithm becomes quite, quite efficient in these settings also here right to see that Bayesian used to be also and quite good performance interestingly much better than optimistic terms and sampling which for me at least I didn't found any people who previously showed something like this."
"Unknown","01;07;52;23","01;08;30;06","So this is also probably new results for machine learning people OK, so just to conclude, right now, active inference does not result in a syntactically efficient decision making. Additional work is required to establish theoretical balance and regret and derived improved algorithms in non stationary abundance. However, we see much better performance in comparison to other algorithms, and it's especially noticeable in more difficult settings that you are kind of getting in."
"Unknown","01;08;31;04","01;08;56;03","The more difficult task is you're kind of forgetting that better results are tentative to do lists for. Like the next steps here is like introduce learning for the lambda parameter, establish a really like kind of theoretical bounce and to regret. So what can one expect to see given different choices of the algorithm for action selection based on expected free energy?"
"Unknown","01;08;56;08","01;09;32;22","And so right this will hopefully improve behavior in stationary test case and potentially also apply to some like real world example. So this kind of right into in machine learning field, I would say this kind of recommendation systems optimization problems is similar just to see how it performs in this kind of scenarios. Yeah, yeah. OK. That I would just like to thank all the collaborators on the project and the people who helped me with different advices."
"Unknown","01;09;32;22","01;10;02;10","And you can also find the slides here and the code is available. My GitHub page I would just not recommend to use it in the next two weeks because the paper is under revision and I am breaking stuff constantly. Yeah, awesome. Thank you. You can unsure and we'll return to just discussion for the last 45 minutes or so here."
"Unknown","01;10;02;10","01;10;31;21","But thanks for that awesome presentation with always good to get again multiple multiple times just to sort of see some of those figures in the paper. Then also there are some different figures and some different views. So again, anyone in the live chat is welcome to ask a question or anyone who's here in this could see. I'll ask a question from the live chat first and then if you're here in the Guzzi, please feel free to raise your hand."
"Unknown","01;10;32;03","01;11;16;23","So it's written in a live chat since the bandit problem here is not Markov Eon, does this mean that we only need to consider the current time to calculate the expected free energy why? Why is not Merkel? Well, let's, let's actually clarify what makes a problem or a situation Markov Eon or non Markov in OK so for example, now the problem is Mark Colvin because the changes are only dependent on the last trial or the current trial right."
"Unknown","01;11;16;23","01;11;41;02","So the well probability in this not stationary case will be a function only on the probability in the previous time step. And this is what makes the problem actually my quote. And so I didn't talk about there are none Markov suspended, but this is not what we are discussing here. Right. It is really just Markov, but this is not the case."
"Unknown","01;11;41;02","01;12;14;14","Why you're done. Well, maybe. Yes, actually. I mean, if you would go to non Markov Bandit, then you would potentially need to plan ahead for longer because that would require that there are kind of dependencies between your actions and outcomes that you observe in a sequence. And different sequences might result in very different outcomes. So. Right, because here we are in Markov case, and the agent cannot change the dynamics of the environment in any way."
"Unknown","01;12;14;25","01;13;09;22","So in a way, just kind of passive sampler from the environment, then you just there is no gain in planning ahead. So you can just reevaluate your beliefs on each trial. So it's kind of like a memory loss process which you brought up. And then what would have to change to maybe account for situations where a sequence of actions does matter so for example, one could introduce structural dependencies between different arms so that, for example, rewards or remote probabilities depends on the location and allowing agent only to to select make their choices from the nearby right kind of introduce and spatial dependency."
"Unknown","01;13;11;04","01;13;29;19","That would be one example where then depending on in which part of the space I am in, kind of where I selected one arm. This limits me to what what's the next arm which I can select and this would require you to then plan depending on where you should be in different trials, depending on how you expect things to change."
"Unknown","01;13;29;19","01;13;29;25","Right."
"Unknown","01;13;32;07","01;14;06;25","All right. This would kind of introduce then the requirements for one cool, very interesting Blue or Dave or Sarah want to ask anything. Otherwise, I have some questions OK, you mentioned a few industrial applications and a few ways in which people do use the multi armed bandit this is just kind of like a logistical question, like what is the rate limiting step in those use cases?"
"Unknown","01;14;07;10","01;14;39;00","Deploying active inference agents? Is there a way to kind of wrap the inputs and outputs of multiple armed bandits in a way that's sort of interoperable? Like you talked about how the learning rules were similar, but then you juxtaposed different action selection approaches. So in the context of pipelines that people are already using, is there a way to kind of hot swap active inference and maybe have it be deployed in industrial settings very rapidly?"
"Unknown","01;14;39;26","01;15;19;24","Well, yeah, I mean, that would be one idea in, for example, a if you have this non stationary problems and you have already been used using dumps and sampling, for example, or even like optimistic essentially as a choice of the algorithm, in a way, you already have a way to form beliefs about relevant aspects of the of the problem then you can like really easily swap the two and you just can then apply the difficulty just to figure out how to compute basically expected free energy and test it out or the different generative model."
"Unknown","01;15;20;05","01;15;53;22","It's people might probably have different know again for stationary problems, that's maybe trickier than it might work in some situations, but you don't have this kind of nice asymptotic guarantees that you will always find good solutions in a way, it's an advantage of active inference in a changing world, like people's preferences for a given advertisement or the situation for trading is always changing."
"Unknown","01;15;54;05","01;16;19;04","And so it's a false allure to have something that has extremely well behaved behavior in the asymptotic or infinite case, because we're not in the infinite case, we're in the finite and dynamic case. And so it's almost like the sort of strong pillar that purported Adley is underpinning these other approaches isn't so much of a gain pragmatically. So it's pretty cool to hear about that."
"Unknown","01;16;19;04","01;16;40;14","Blue, thanks for the raised hand. What would you like to ask? So I have a question that was left over from the zero video and something you kind of alluded today in your talk. Can you kind of detail the difference between the switching bandits and the restless bandits? I was like unclear on like the timing of the switching in the switching bandits."
"Unknown","01;16;40;14","01;17;09;10","And then also just like a part of that question. In the case of the Restless Bandits, what are the similar like algorithms that are optimal for switching bandits? Also optimal for restless bandits? You had mentioned the active inference was good, but what about the others that are commonly used? No, it's the same, right? I mean, so let's say Erasmus bandits, the only kind of soon switching bandits."
"Unknown","01;17;09;10","01;17;42;23","We have like peace fires, linear problem or peace stationary problem, which means that between trial one and trial, then everything behaves stationary. And then when the change comes, you are just getting new, remarkable probabilities associated with each other, for example, that would be kind of ideal for switching bandit, so that between changes, everything is like fixed, and the restless bandit assumes that things continuously change."
"Unknown","01;17;42;23","01;18;08;17","And here the example I gave was one can assume, for example, that a reward probabilities can be described as a random walk in this in this logic space of the probability. So basically because probability is like between zero and one, you transform it to unconstrained space between minus. If anything definitive, you just have to then go shown random variable, so to say."
"Unknown","01;18;08;17","01;18;32;26","And then you map it back into the into to the sigmoid function, for example, into the probability space and so but so for example, the algorithm I show like for belief, everything you want could also just apply there. So it just 11 doesn't know what would be the wrong so what is the change probability in the restless case."
"Unknown","01;18;33;07","01;19;05;25","So in that sense you would need an algorithm which can also infer the change probability and restless case does not necessarily translates to kind of potentially fixed change probability so it's a bit more different problem. So because the maximum arm that changes between right between arms which are optimal, do not follow specifically like the same structure as in the switching case."
"Unknown","01;19;05;25","01;19;34;07","So one can either take a different generative model which actually assumes explicitly the random walk and like for example, hierarchical ocean filter is something which one could apply there. But there are also like other belief updating algorithms for that. So that's right. So the local or the global maximum minima are changing in the Restless as well. Yes, yes, yes."
"Unknown","01;19;34;07","01;20;01;00","So this would be kind of this case of varying difficulties so. Right. The relative probability between the best arm and the second best arm varies all the time. So yeah, I don't know. I could have just drawn the lines though, so but I don't know. I I could give them a bunch of right now something to illustrate this."
"Unknown","01;20;01;22","01;20;32;10","So one other thought on the advantage possibly of active inference is that with a deep generative model using the same skeleton of maybe even the same code, it could be possible to do model testing between two different types of band. It's like, what kind of scenario am I in? Or even have deep temporal model. So it could be extended in a way where a sort of instantaneous sampler might be led astray."
"Unknown","01;20;33;09","01;20;59;22","Yes, yes, yes. I mean, in principle you can learn any hierarchical Bayesian generative model which consists of multiple models potentially you could also write generalize 2007 or Bayesian. You'll see me. I just assume that this would be a very efficient way to figure out which is the correct model. It's just would be currently applying for the specific task."
"Unknown","01;20;59;22","01;21;35;00","And this is potentially also where active inference would would provide an advantage in such a scenario. So there you can also kind of learn about the generative model. And so that better over time and that made me wonder what would it look like at the sort of human level as we're making decisions that are sequential in our day, our decision making, what would it look like or what would we keep in mind if we were going to be making decisions more like an active inference agent than like a Thomson sampling agents?"
"Unknown","01;21;35;17","01;21;58;25","Like what would be, you know, when you're in the grocery store looking at the cereals that you've had before or not, how would an active inference agent behave differently I'm just kind of wondering. Yeah, I mean, that's a good question. I mean, we could actually have some datasets which which we could use exactly. To ask these kinds of questions, right?"
"Unknown","01;21;59;23","01;22;28;27","I mean, I don't have a top of my head any clear answer but what would be your expectations are well, I guess it would tell you one or better tell me when is a good time to try a different scenario because you may like it even more, especially if cereal recipes change over time."
"Unknown","01;22;31;16","01;22;47;26","And then you have some likelihood to be stacked maybe with the soup of that cereal. But also, I'd like you to pick the one you like most yeah. I mean, I guess this exploration would then would be more structured in a way more directed, right?"
"Unknown","01;22;50;16","01;23;18;19","Right. Like where I mean, when the ingredients change, you check the ingredients and then now you've updated your likelihood of trying something new yeah, right. I mean, Thompson's sampling doesn't have direct that part of the exploration and just the random exploration, whereas is there here the focus would be more on practice exploration and another potential thinking how to add the random to that."
"Unknown","01;23;20;03","01;23;53;12","One other piece is like we're often comparing and contrasting active inference to reward learning and reinforcement learning. So it's almost like instead of making that decision based upon the highest expected value, like choosing something proportional to its relative value or always going with the one with the best likelihood of having the best tasting cereal, there can be some other heretic and so it opens the door to just purely curiosity driven sampling."
"Unknown","01;23;53;12","01;24;26;29","Like I've just never had this, and it's not even as much a reward maximizing maneuver as it is just a purely epistemic gaining maneuver. And then as we've seen when there's a pragmatic and an epistemic component to the function that's being optimized, then those decisions can kind of coexist and be put on a common grounding unlike in a purely value driven framework, where even the exploration has to be kind of coerced back into reward mm hmm."
"Unknown","01;24;28;04","01;25;12;20","Yes. I mean, so I think said Nora, and she had like as a first thought or she kind of interesting paper demystified the reference and they discuss a bit about learning the preferences themselves. Right. And what kind of concept was discussed. And this kind of puts a different perspective on them. Understanding what the reward is, because in real world, you don't necessarily know why should something be more rewarding to something else, but to learn this over time and you just learn to prefer different outcomes, they don't even necessarily have to be in somehow rewarding more in the absolute sense it does build experience with some outcomes over others and you start to prefer and this"
"Unknown","01;25;12;20","01;25;47;08","then appears as if you would doing reward based decision making. But I mean it's actually no way preference based decision making. Right. Cool. I had another question about the approximation of active inference is that the only way to approximate active inference, are there other moves that you could have made to approximate the sections that you did approximate? Are there other pieces that can be approximated, what can be swapped or approximated but still retain this essential structure of an active inference model?"
"Unknown","01;25;57;10","01;26;32;17","Well, I don't have many ideas what else one would go. I mean, the problem is in this scenario, it's relatively simple right? But for example, one can kind of think of, OK, given that we have a to compute the expected information gain and we are kind of learning of having the way to to estimate it efficiently, like just with this approximation there is other way how you can compute this just by sampling and you can draw a couple of sample from your beliefs and you get some estimate on the expected information."
"Unknown","01;26;33;03","01;26;52;16","So this is would still be in the kind of active inference brain framework, but just slightly different way of how you actually compute the expectations and what they kind of represent, that this would be a way to add a bit of random randomness to the decision making process."
"Unknown","01;26;56;13","01;27;28;05","Of inference, as I thought, general enough to work with many kinds of approximations. Right. And you can do an approximation and many different points. And as long as you still get some sort of variation of free energy, I guess it's still within this framework and is liberated the inference part from the action selection part. But I think actually you can use it as a model of both, right?"
"Unknown","01;27;28;26","01;28;21;06","And then you can do approximations and different points in this trying model and as long as you can still write down a variation of free energy and I guess it's sequences like different fruits well, I don't know if that's necessary. I mean, I would say any kind of Bayesian belief updating even if it's not specifically motivated by variation on friends, would, if the consequence have minimizing variation, free energy, less advanced you computed from some posterior which you obtain and it's it will still be in the way active inference right in a way you can replace like smile variation updating with any belief updating rule and still keep the same concepts there."
"Unknown","01;28;21;06","01;28;56;11","So because we are just getting a better bound on the margin likelihood if you have a better posterior so so what would you say, you know, anyone is necessary or sufficient for a model to be considered active inference versus not active inference you know just including sense and action or perception inference action or agents in a niche or blanket states."
"Unknown","01;28;57;06","01;29;26;05","These things are sort of we're in an overlap of Venn diagram with certainly many classes of models, different approaches and is it like that blurry intersection that we're looking at and that's where action and inference are being just applied together, or is there something unique or something that we can use as a diagnostic? Well, I guess the difference would be more than this kind of this action selection."
"Unknown","01;29;26;06","01;29;51;07","I mean, planning, this inference bias, not then, not necessarily it's perception. It's inference, because as I said, one can think of many ways how one can solve that or but kind of once you go into this planning as the inference part and also concept that your actions themselves will change beliefs and you will choose actions which are the best in changing your beliefs."
"Unknown","01;29;51;07","01;30;47;19","So what you want to achieve then this is right I mean, idea of active inference and it's let's say it's still color inference for one right it's not in a way, it's more so easy to disconnect effects of choices and aspects of perception. They all depend on each other. And also being aware of your own uncertainties and I think if you're doing standard reinforcement learning where you just like the expected rewards, I don't think you're always aware of how certain or uncertain you actually are about your own generative model, for example, and then potentially with active inference, easier to know which action to take in order to better learn your model."
"Unknown","01;30;49;08","01;31;15;16","Very interesting it's almost like by hearing and propagating our uncertainty and having a self model of action and learning, then we get that almost like second order cybernetics where we're acting in a way where in the future we can expect to learn better or expect to act better. As opposed to just this hungry search for the best action."
"Unknown","01;31;15;22","01;31;40;12","And then learning is only a one step projection into like what action is going to be informative right now? What's the next Wikipedia article that's most informative rather than what's like the trajectory that I expect that is going to be resulting in more effective learning or action again on a common grounding. So that's pretty interesting. Looks like you have a thought, though, Dimitri."
"Unknown","01;31;42;15","01;32;15;01","No, I mean that that's the main difference. The important part, I mean, I would then call on some sampling would also satisfy that Bayesian. You should be right. I mean, they all kind of are anybody's decision making algorithm who would necessarily have to take into account and of if it's derived from Bayesian decision making theory. But yeah, it is not that clear that right."
"Unknown","01;32;15;01","01;32;56;06","It's that OK, the actions also have a consequence of reducing the uncertainty in the future. And this is what you can use also for as a gauge on which one, which action is better so yeah, from a subtle differences. Not that obviously it's it's one reason why we're so interested in in ontology and slowly scaffolding the research so that we can actually juxtapose different models and understand where they differ."
"Unknown","01;32;56;06","01;33;25;07","Like, OK, they're kind of it's like two road trips and then this person just took a little extra loop or this bridge they crossed this way versus this person, you know, took a different route right here. We can understand like you talked about the smile variational updating. But then just recently you mentioned that there's that sort of like a module you can switch out so what is smile or I also noticed is a very recent citation."
"Unknown","01;33;25;21","01;34;12;07","So what does it do or what is it different in regards to other ways? You could have fit that module and I mean, I have implicitly used smile for years now. It's just a very simple way of updating the so so the problem a bit with kind of variation on inference, if you want to find the minima of approximate posterior, you kind of necessarily need to iterate through several loop like minimizing the gradient, the following, the gradient of the variation, the free energy, right and I mean, it also doesn't make it very efficient in for this kind of applications."
"Unknown","01;34;12;23","01;34;43;28","So for me at least, this variation smile approach five steps, this iteration so that it can kind of transform a relation in principle, just a single obviates that does because there is a way part which you can compute explicitly and you just assume, OK, this is now my fixed belief about that. And the other part which you need still to update through relational approximation and so this is I guess the small advantage here."
"Unknown","01;34;45;08","01;35;26;17","But as I said, the other approach is how you can update beliefs in the scenarios and you know, in the sense there can also be more optimal, but they just bring you closer to the exact posterior so but I mean, we tested the things with some examples and the theory remains the same. So there is no gain on making things more complex on the Underflow but one can also imagine that right and different environments, better generative model and approximation rules would take you for that thought."
"Unknown","01;35;26;17","01;36;01;10","But the newly blended this is just not the case. What kind of empirical data sets are almost whether they're open source or just obtainable, are amenable to this kind of analysis. Like if somebody hears about the algorithm and they kind of want to see it in action or play with it themself, once you're, you know, two weeks from the recording of this, when your code is available, like is it passed, is your code set up for more of a simulation or is it something where we can plug in a type of data set that might already be structured appropriately?"
"Unknown","01;36;03;22","01;36;48;26","It's currently just for simulations, but I mean, one could potentially play out with different algorithms and other algorithms both on the like a learning path and the actual selection system. And that that's the super useful program where if there are people interested in that repositories open and welcoming any contribution it was part of the structuring of your paper that like made us excited to juxtapose it with these other approaches is you kind of showed that you can directly compare active inference alongside other models."
"Unknown","01;36;49;14","01;37;29;14","Obviously, it's something we're coming back to because that's the crux of the results of the paper is really the different dynamics as time increased or as the relative challenge increased, as the number of arms was changing between these different styles. And the two styles of banded. So it's kind of a cool thing that people could both build on directly what you're working on, but also maybe more broadly, instead of just a single model being presented in a paper, people could just include multiple types of models in their sort of baseline paper so that we wouldn't just have to read the paper that says This algorithm works well."
"Unknown","01;37;30;06","01;38;08;06","We could see them directly compared and that's something that more and more papers are doing with active inference. Yeah, I mean, the the kind of also the libraries being used is also this is something which was developed relatively recently like uh, by Google Jacks. It's kind of uh, acceleration for a linear algebra algebra library, so which allows you to run very fast on this code."
"Unknown","01;38;08;25","01;38;54;27","And it also integrates well with probabilistic programing language. So it's that non-paper which was also developed really recently. So one could be in practice linguists or any behavioral data acquired sentence data very easily with few lines of code. So yeah, there is also this perspective. What language is the code in one or multiple and then what areas of conceptual math do you think somebody would want to know before diving in so the language is, I mean like programing, languages, python and just depends on, as I said, couple of libraries like objects, you know, by very standard stuff except objects."
"Unknown","01;38;54;28","01;39;24;00","That's, that's the new so yeah, somebody's interested in using it would have to learn a bit about X, but that's a long term also useful. So go for it. Well, but from the outside, yeah, I'm a well I mean maybe getting a couple of introductions to multi armbands that would be kind of good place to start uh, right there is."
"Unknown","01;39;25;01","01;40;00;19","Hmm. There's a recent of quite recent introductory paper of multi and it exposes many different algorithms in the stationary concept. So uh, provides a bit insights about this historically historic work, how they analyze this problem. So yeah, I would suggest that that's a kind of a place to start. Cool Blue. So I'm curious about this Google text. I haven't heard it, heard about it, but like I thought Google had their own language."
"Unknown","01;40;00;19","01;40;09;18","Like don't they have like Googling? It's not Google or TensorFlow, right? Well, yeah, things are slow."
"Unknown","01;40;12;16","01;40;49;10","Yes, they have TensorFlow and I think the basis is similar. Excel A, it's called Excel Accelerator Linear Algebra for both TensorFlow and Ajax. So what projects is is basically accelerated by so you can just run standard by code and in pure Python more or less and get very mm. So you get for free like computational gradients. So it's kind of out of grab library."
"Unknown","01;40;49;28","01;41;18;25","So you can kind of compute gradients and very complex graphs. This is also something which TensorFlow allows you, but this seems to provide more and more benefits for this dynamic of scenarios. So I have problems when they've tried learning or using TensorFlow. It's very difficult to think to write code, which is dynamic there, and that's why I never actually start started using it."
"Unknown","01;41;18;25","01;41;47;28","I started with PyTorch at some point, and now that Ajax showed up, that has some speedup advantages. It's also kind of quite lucrative. So one code question just to stay on this theme and ask a question from the chat, what about the Python implementation? Like I think in for actively or what Alex Chance at all have worked on, what are the similarities or differences with their Python approach?"
"Unknown","01;41;47;28","01;42;15;23","There I think currently none because I'm also involved in that. So they will also start using text. OK, yes. Cool. So that those threads will join together merging. OK, but I mean, this also just like reduces some of the I mean, one could also write you as just the SVM code. But this is super complex right? It would be super slow."
"Unknown","01;42;15;29","01;42;45;16","So for example, just to examine numerically this stationary scenario, this would take months probably to OK, nice. Somebody is ranking so. Yeah, right. That's that's kind of the the problem there. And this I wrote the code to be kind of very efficient in way. So just kind of remorse, lots of complexity that you don't need. Cool. All right."
"Unknown","01;42;45;25","01;43;05;27","And some scenarios you want to have a general of description of the problem. So so here's a question from Stephen in the chat. Do you think that parallel modeling processes might be used more in the future with different model approaches, highlighting different patterns of behavior happening in different niche contexts?"
"Unknown","01;43;10;08","01;43;31;24","I would say in some ways it's what your paper did, but what do you think you mean? Yeah, well, I mean, from a practical side. Yes. I mean, that's what you should be doing because just figure out what works best and don't don't think too much about, well, the philosophical side of things."
"Unknown","01;43;34;03","01;44;10;07","But yeah, I mean, I guess long term one could imagine a scenario where more and more things become generalizable to this description. So it's just, you know, a difficult process. Well, explain it. Even for those who are just learning about the technical details, like the visual tell for me was that the figures were a grid of graphs. So it was kind of like three different settings of difficulty or three or five different settings of arms it wasn't just showing we ran it with one parameter combination."
"Unknown","01;44;10;21","01;44;38;29","Each of those graphs was a grid of combinatorics and then you presented what I guess could be considered two different niches with the static and the dynamic. And then today in the presentation, we heard about all these other variations and so those are like kind of toggles in the code. You can say, I'll take a one alpha or I'll take a two beta as far as the combinations of how to run it."
"Unknown","01;44;39;11","01;45;04;18","And so as the code becomes more interoperable and pruned down to really the necessary pieces, then it becomes easier and easier to expand it back out so that we can choose among different options for a given piece. And so that's like this skeleton that so many variations flow off of yeah."
"Unknown","01;45;07;10","01;45;34;21","Cool. Well, in our last few minutes here, where would you like to take it next week or beyond? I mean, what are your current interests or curiosities well, I mean, for me it's just important to have this paper out. So next time when the reviewers asked me a lot about that in-depth algorithm, I can say, okay, look, we analyze this."
"Unknown","01;45;34;21","01;45;58;18","It's like that, it's similar and different. You can you can get whatever you want, basically. Yeah, right. I mean, because you have this adaptive parameters, you know, where you can generate very different behavior and can also things, OK, if you have like an agent, which behaves as a thumb. So sampling, what would be the corresponding parameter in active inference framework which would emulate that and can you actually differentiate between them?"
"Unknown","01;45;59;19","01;46;29;23","So I mean, this is potentially interesting questions which one can try to answer and which are just from my work kind of irrelevant because of this constant questions that I'm getting in during the review process. A and that was actually a good. You do. Yeah. But then there is also this like quite interesting side and like machine learning where this can find potentially quite interesting applications."
"Unknown","01;46;30;13","01;47;24;05","Uh, as I got started a bit recently working with kind of Monte Carlo research and interestingly, so this is something which was applied in active inference as a way to compute expected for managing complex problem. But turns out that if you have kind of very complex problems you can also use Thomsen sampling in Monte Carlo research right? So a way to just figure out what is the best path to follow in a sample and this potentially provides if this stationary scenario can be improved somehow and this is also kind of you can apply active in France to planning inside active inference and so you know in a kind of cyclical circular way."
"Unknown","01;47;25;11","01;47;59;07","Right so what is that look like or how does it get implemented or how is it different than just straightforward active inference so the problem is like that when you have quite a complex decision making and a planning problem or you have multiple branches in the future, which you have to go, it's not practical to compute everything. So what people do is a very popular way to do is like Monte Carlo Research, where you just sample different parts."
"Unknown","01;47;59;07","01;48;28;07","You you estimate up on a subsample of possible parts what is the best part to go? And this would correspond in active inference, like your estimate, the expected free energy of a pack in the future just for a sample, but then you have a problem. OK, how do I select the parts in the sample? And then you can apply active inference to the parts election itself."
"Unknown","01;48;29;16","01;48;55;20","So right. You can kind of choose which parts I should sample randomly when I'm trying to approximate expected strategy for this moment. So that's kind of what makes it kind of interesting is the possibility to experiment and the way that you just framed it as well as what we've seen, which is the relative strength of active inference in dynamic settings."
"Unknown","01;48;56;04","01;49;26;27","It's consistent with a lot of the qualitative and philosophical ways that people are talking about active inference as like a sense making or a wayfinding or a navigation approach rather than a sort of cut and dry calculus of decision making, just resulting in the total you know, crystal path just being laid out before you. It's really about the instantaneous actions that we take now in light of uncertainty about the presence and really the past in the future as well."
"Unknown","01;49;27;11","01;49;51;25","So it's always cool to see how the tactical developments while they're like kind of weaving and recombining they proliferate. And then we see, oh, actually these three are kind of interchangeable or these three are complementary. And then we get more technical detail and speed ups while we also get more and more clarity on what the structure of this sense making problem is."
"Unknown","01;49;55;00","01;49;55;27","Yeah, I agree."
"Unknown","01;49;58;04","01;50;33;16","Any final thoughts here from anyone? Otherwise, this was a super interesting presentation and questions, so we really appreciate it thank you. Cool. Yeah. Thank you. Yeah, we're looking forward to for the next week. Great. So thanks everyone for joining and everyone's welcome to Join Live for next week when we'll continue the discussion. And the DOT two is kind of like our jumping off into the unknown unknown instead of just the known unknown."
"Unknown","01;50;33;21","01;50;39;18","So thanks again for joining and we'll see you next week. Thanks. Bye bye bye."

"Speaker Name","Start Time","End Time","Transcript"
"Unknown","00;00;08;13","00;00;40;21","Hello, everyone. Welcome to Active Lab. Livestream number 24.0. Today is June 16th. 2021. And we're going to be talking about this paper in empirical evaluation of active inference in multi arm band. It's I'm Daniel and I'm here with Blue. Hi. Awesome. Welcome to the Active Inference Lab, everyone. We are a participatory online lab that is communicating, learning and practicing applied active inference."
"Unknown","00;00;41;01","00;01;06;09","You can find us at the links here on this page. This is a recorded in an archived live stream. So please provide us with feedback so that we can improve our work. All backgrounds and perspectives are welcome here and we'll be following good video etiquette for livestreams here. At the short link, you'll find all of the live streams and different series that we do in the communications unit of Active Lab."
"Unknown","00;01;06;26","00;01;48;05","And today we're going to be contextualizing the dot zero video for two upcoming discussions in the second half of June. 2021 on the 22nd and the 29th when we have discussion 24.1 and 24.2 on this paper and hopefully with the authors joining today in active livestream number 24.0, we're going to be trying to set some context and given introduction to the following paper in empirical evaluation of active inference in multi armed bandits by the authors listed here and the video is just an introduction to some of the ideas."
"Unknown","00;01;48;22","00;02;09;13","It's not a review or a final word. It's kind of like a three way intersection. We have people who are maybe within the active inference community and looking to be exposed to some different areas like Bayesian statistics or machine learning. The second road is those who are coming from Bayesian statistics or machine learning approaches and curious about active inference."
"Unknown","00;02;09;23","00;02;29;15","And then, of course, we hope that this will be exciting and interesting. Even if you're unfamiliar with active inference or machine learning, we'll hopefully try to connect it to some broader questions in behavior and decision making. More broadly, we're going to walk through the aims and claims of the paper, the abstract and the roadmap, covering a few big questions."
"Unknown","00;02;29;15","00;02;52;22","And then we're going to go through all the figures and some of the key formalisms of the paper so that whether you read the paper or not, you'll hopefully be in a good spot to ask questions and learn more. And of course, in the one and two in the coming weeks, we'll be discussing this same paper. So save and submit your questions and let us know if you'd like to participate or contribute in any way."
"Unknown","00;02;55;03","00;03;43;19","Here we are on the paper itself, which has a screenshot of the cover on this slide and all read the aims and claims and then Blue, you can give a first thought on what you thought were kind of cool pieces about what they aim for or plate in this paper. We provided an empirical comparison between Active Inference, a Bayesian information theoretic framework and two state of the art machine learning algorithms Bayesian upper confidence balance, UCB and optimistic Thompson sampling in stationary and non stationary stochastic multi armed bandits we introduced an approximate active inference algorithm for which our checks on the stationary band a problem showed that its performance closely follows that of the exact version,"
"Unknown","00;03;44;24","00;04;17;21","and hence we derive an active inference algorithm that is efficient and easily scalable to high dimensional problems. So what was cool about that, or what made you excited to make this dart zero? This paper was really awesome. It's it's shows how active inference can be used to solve problems that are sometimes computationally intractable and really pretty difficult. So I think that the authors did a great job of driving this active inference algorithm and proving that it's useful."
"Unknown","00;04;18;28","00;04;47;24","Agreed. And they did an awesome job bringing it analytically, like with equations in line, or at least being juxtaposed to other approaches in machine learning rather than appealing to a qualitative body, a theory which is also great. This is definitely one where the claims are specific and exact, and that's what we'll be following up on so the abstract, I'll read the first half and then you can go for the second half."
"Unknown","00;04;48;29","00;05;15;29","A key feature of sequential decision making under uncertainty is a need to balance between exploiting choosing the best action according to the current knowledge, and exploring obtaining information about values of other actions. The multi armband IT problem, a classical task that captures this trade off, served as a vehicle in machine learning for developing branded algorithms that proved to be useful in numerous industrial applications."
"Unknown","00;05;17;00","00;05;47;27","The Active Inference Framework in approach to sequential decision making recently developed in neuroscience for understanding human and animal behavior, is distinguished by its sophisticated strategy for resolving the exploration exploitation trade off. This makes active inference an exciting alternative to already established banded algorithms. So it's kind of awesome. First few words it's about sequential decision making under uncertainty, and the bandit task was made to explore that."
"Unknown","00;05;48;14","00;06;22;14","Machine learning has already gotten so much value out of pursuing algorithms for approaching this multi armband app problem and then enter active inference, which was developed to initially cover human in animal cases of behavior. But as we're seeing, it also goes beyond that. And so that motivates studying active inference in the context of multi armband. It problems so go for the second part here we derive an efficient and scalable approximate active inference algorithm and compare it to state of the art bandit."
"Unknown","00;06;22;14","00;06;50;08","Algorithms, Bayesian upper confidence bound and optimistic. Thomson Sampling this comparison is done on two types of bounded problems a stationary and the dynamics switching band it. Our empirical evaluation shows that the active inference algorithm does not produce efficient long term behavior in stationary bandits. However, in the more challenging switching banded algorithm switching band it act of inference performs substantially better than the two state of the art bounded algorithms."
"Unknown","00;06;50;26","00;07;17;15","The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behavior. So this is really nice because it takes like active inference as an approximate way that humans deduce reason balances exploitation, exploitation paradigm, and it's programs that into the framework."
"Unknown","00;07;17;25","00;07;37;29","So it just shows that it's getting closer approximating human thinking in a machine more and more, I think. Yes. And there's sort of a pair of pairs that we're going to return to a bunch of times in this discussion. The first is the two types of problems, the stationary and the dynamic bandit, which one of them is static."
"Unknown","00;07;37;29","00;08;07;10","One of them was changing. We're going to talk more about that and the other pair that it comes up often is the Bayesian upper confidence bound and optimistic Thompson sampling. We're also going to come back to that. So they're kind of doing a pair of pairs approach. And then the results are just really fascinating. And we're going to unpack that, that actually in the simpler case, we see that the active inference algorithm doesn't do perfectly, but in this more challenging dynamic case, it does better than state of the art."
"Unknown","00;08;07;12","00;08;27;28","So what do you call something that goes beyond state of the art here's the road map for the paper. And of course, as with these dot zero videos, if you're curious about how it was specifically stated in the paper or you want to dove in for more and you want to see how they cited or how they built up a claim, you should go to the paper itself."
"Unknown","00;08;27;28","00;08;55;26","This is just the road map for it. They start with an introduction of the multi armed bandit and cover the two kinds of bandits that they explore, which is the stationary and switching versions, and talk about how you evaluate performance in different versions. They then introduce the algorithms that they're going to utilize and talk about this variational smile approach, which will cover just a little bit of but look forward to hearing from the authors about what it does."
"Unknown","00;08;57;05","00;09;23;24","Then there's results for the pair of problems that they explore stationary and switching bandits, and then have five figures which we'll go through showing some results and hopefully even if the algorithms or the formalisms were a little bit hard to follow. The figures are really clear and they show how the algorithms perform through time and close with the discussion that hits on some awesome general points for the active inference and machine learning communities."
"Unknown","00;09;25;14","00;09;50;03","Here are the key words that were provided with the paper. So as usual, we're going to use these keywords as jumping off, but also jumping in points for those who might be familiar with sequential decision making a.k.a life but not be familiar with Bayesian statistics. And we're going to start from sequential decision making and then go on to talk more broadly about Bayesian inference multi armed bandits."
"Unknown","00;09;50;03","00;10;08;09","So it's totally fine. If you've never even heard of the bandit problem, then we're going to talk about those two different algorithms that they're going to be exploring upper confidence bound in Thompson sampling, and then see how they talked about active inference, which is going to be a really distinct way relative than a lot of approaches we've seen previously."
"Unknown","00;10;10;03","00;10;33;04","So let's go to sequential decision making. What would you say about this? So the sequential decision making is dependent on time t right so a one time step, you have a state of the system, you perform an action and that alters the system, right? So that the second time step, the system is now in a new state and then you have another decision to make about what you're going to do with the system."
"Unknown","00;10;33;04","00;10;55;05","So this is like driving a vehicle, managing a stock portfolio, playing a game of chess, sequential policy decisions. And so like to kind of juxtapose what is a not sequential problem. So that would be something like image classification is not sequential. Like you can just classify all of the images parallel or at once. The state of the system doesn't depend on how you classify the previous image."
"Unknown","00;10;56;07","00;11;20;07","Awesome. So definitely all kinds of organismal behavior in decision making that happens in time. So think decisions you're thinking usually sequential decision making, especially when the decision influences the future. Those are all the kinds of problems that we're going to be talking about. But also to even make it broader than that, some non sequential problems can be framed sequentially."
"Unknown","00;11;20;16","00;11;49;08","So let's just say that we had that classification problem where it's one blob, one big data set, and so there's no temporal sequence we might want an algorithm that treats it as if it were sequential so that we could just start reading in examples. And then after ten. OK, got it. So then by looking at it as if it were a sequential problem and we were solving things through time, sometimes we can approach non sequential problems that way because in the computer it is going to be sequential on the processor."
"Unknown","00;11;49;18","00;12;14;28","So it's for things that actually are sequential in the world or they're for things that we want to act as if they're sequential one big tension that comes up with all kinds of modeling is the explore exploit dilemma, slash tradeoff. And this is an awesome 2015 paper of hills at all exploration versus exploitation in space, mind and society."
"Unknown","00;12;15;25","00;12;44;08","So it's pretty cool because it talks about multiple different domains here in the table. One, there's animal foraging, visual search information, search, memory search, searching and problem solving and social group learning. Those are like all of our favorite topics. Blue and it gives just a little bit of a visual example how in different domains what does the archetype of exploration look like and what does the archetype of exploitation look like?"
"Unknown","00;12;44;26","00;13;19;15","So some of these are very related to kind of a physical movement like patch foraging exploration, moving to a different tree exploitation, staying on the same tree, but also visual focus exploration, scanning around exploring exploitation, having a fixed gaze and staring at something. And then also we can think about even in word association like memory exploration is doing big jumps from different semantic neighborhoods, whereas exploitation might be like all the livestock, animals or animals with the same letter."
"Unknown","00;13;19;15","00;13;47;13","There's different ways that you might exploit because there's different dimensions to the semantic landscape, but there's still this tradeoff between jumping far or staying relatively close in the neighborhood. So it's a big trade off. It's studied in animal behavior, it's studied in all kinds of decision making tasks. Anything to add on that? Only that I got to interact a lot with Peter Todd last summer, who's the second author on this paper, and it was so fun."
"Unknown","00;13;48;15","00;14;19;08","Cool. And also, Ian Cuisine has done an awesome collective behavior work, so it's exciting to think about how these algorithms are able to apply to individual agents, but also maybe groups. And that's hinted out multiple times here, like role assignment and social connectivity as we heard about in the abstract of the paper that we're discussing today, they went right from sequential decision making under uncertainty to multi armband."
"Unknown","00;14;19;08","00;14;43;05","It is a way that we can study that. And from the paper, this is where we're introducing that, that double problem, the two problems are solving, we consider two types of bandit problems in our empirical evaluation a stationary bandit as a classical machine learning problem and a switching bandit commonly used in neuroscience. So we're going to go into more detail about how those are technically defined."
"Unknown","00;14;43;05","00;15;06;12","But let's just start with what are people getting at with multi armband it? Why is it something that so many people have addressed? This will make the presented results that they're doing here directly relevant, not only for the machine learning community but also for learning and decision making studies in neuroscience, which are often utilizing the active inference framework for a wide range of research questions."
"Unknown","00;15;06;24","00;15;32;09","So there's all these different areas. Here's a multi hour bandit deciding which area to study computer science, machine learning, economics, neuroscience. These are all areas that the multi armband, it is like a bridge among. So now we're sitting active inference right at that nexus. We've talked a lot about connecting computer science to behavior or connecting neuroscience to economics."
"Unknown","00;15;32;18","00;15;58;05","What if we could all meet at a common nexus? What if that were active inference? These are the kind of fun things to talk about, but it's not just a nexus of conceptual connection. The motivation behind it and active inference. There's a lot of really specific use cases and a lot of the algorithms that power our experience online are actually trained and fit with multi arm bandit problems."
"Unknown","00;15;58;28","00;16;30;08","So here's a few fun examples that we came across while researching here. On the top left is showing how you have a multi arm band playing a role in music recommendations. So it's like the arm, that's which arm is being chosen. That's the song. So thinking about it from the point of view of the music platform, it's sending a song and then the target user, which is the bandit machine here, gives a payoff back to the algorithm, thumbs up or thumbs down."
"Unknown","00;16;30;24","00;16;59;17","And then the algorithm sends you another song and then a payoff is given so it's sequential decision making and it's that same relationship that's shown graphically. The goal is to maximize the sum of the ratings versus maximizing the sum of the payoffs in an abstract problem. Here's an example of website AB testing testing versions of a website. So in the top there's four versions of the website, and then they're each being allocated to a quarter of the users."
"Unknown","00;16;59;24","00;17;25;21","And then each of the users are staying on the site for a certain amount of time or staying for different amount of times in the standard AB testing for the whole duration of your test. You keep that one forth for each site, but in a multi armband it, you start with a quarter. But then very rapidly we see that this blue one starts performing better and then we keep exploring and having some time allocated to these other colors."
"Unknown","00;17;26;01","00;17;56;23","But we see that the blue pretty steadily dominates and then that black line ends up staying higher. So overall, a higher number at the end of the epoch. So you're sort of earning wall learning because you're able to be making exploration while you're also exploiting. And there's paper specifically on that, like Bayesian bandits, in the context of online personalized recommendations, any thoughts on that blue of course."
"Unknown","00;17;56;23","00;18;25;23","So these are yeah, these are algorithms that are in use every single day, and they power a lot of our experience and a lot of decision making support here's where we bring in some of the Bayesian algorithms. They're going to be using these two types of bandit problems, stationary and switching and here's the second pair of terms. They're going to be empirically comparing active inference to two other state of the art bandit algorithms from machine learning."
"Unknown","00;18;26;03","00;18;56;08","So those who are familiar with machine learning will have seen these two algorithms a lot. The first is an upper confidence bound algorithm, UCB and the second is a variance of Thompson sampling called Optimistic Thompson sampling. And then they note here the two algorithms, the UCB and Thompson sampling reach state of the art performance on various stationary bad problems, achieving regret, which is the difference between actual and optimal performance we'll return to it soon."
"Unknown","00;18;56;16","00;19;19;24","That's close to the best possible logarithmic regret. And in switching, band learning is more complex. But once this is accounted for, both of the algorithms exhibit state of the art performance. So you can never play perfectly, but you're approximating about as good as you can play with these algorithms. What does that look like before we go into the technical details of what these algorithms are?"
"Unknown","00;19;20;29","00;19;52;01","So on the top left, we're going to be thinking about, again, whether it's deciding which version of a website we want to be presenting or which songs we want to be presenting, or just keeping it kind of abstract on the top left, we start with zero trials, so this is before we have any information at all. And then as trials occur and payoffs are observed and the trials can calculate count, after 28 trials, we've reached a very markedly different set of distributions."
"Unknown","00;19;52;25","00;20;22;22","And so what this overall looks like is you show up at the casino and you don't know the payoff of any of the slot machines, and then you're going to be choosing some policy some way of approaching those slot machines and switching between them as needed. Some approach that's going to hopefully result with you getting the most money and so in this case, it's sort of like as you get more and more information, you're fine tuning your estimate of what the distributions look like."
"Unknown","00;20;23;07","00;20;50;02","Like we can see how the registries union gets tuned as it gets tried more and more and similarly for the other distributions. So that's kind of what this algorithm does is it starts with no specific bias towards a given slot machine and then it tries to develop a strategy for staying with slot machines that are known about versus trying new ones or ones you haven't tried in a while in order to get the best possible outcome."
"Unknown","00;20;50;08","00;20;58;13","And as you can imagine, if there's a static one, then it's an easier problem if it's dynamic, you have to keep the rate of change in mind."
"Unknown","00;21;00;21","00;21;37;08","Any thoughts on that? Good, good explanation. OK, so Bernoulli Bandits are the class of bandits that they're going to constrain themselves to. So they say we're constraining ourselves to a well-studied version of bandits, the so-called Bernoulli bandits for Bernoulli bandits, choice outcomes are drawn from an arm specific Bernoulli distribution. Bernoulli bandits, together with Gaussian bandits, are the most commonly studied variants of multi armed bandit, both in theoretical and applied machine learning and experimental cognitive science."
"Unknown","00;21;37;26","00;22;06;14","So you can fit any kind of distribution for the rewards underlying each of these slot machines. But it turns out that if you use the for newly distribution the math works out nicely, which makes it easy to study. That's why there's been a lot of study in the Bernoulli and the Gaussian. So a Gaussian distribution in reward would be like, OK, you get five with a certain, you know, bell curve of how much you might win."
"Unknown","00;22;06;22","00;22;25;04","Bernoulli is going to be a different shaped distribution but just the idea is that you're going to be learning the parameters that describe the reward returned by that arm. So it's just the subcategory or the function that underlies the reward payoff on these machines."
"Unknown","00;22;29;19","00;22;52;00","Let's talk about the two algorithms that they're going to be discussing a lot. And then also where these algorithms come into play in terms of strategy. So again, you're sitting there at the slot machine at the casino, and how are you going to decide how to stay or leave a given machine? Stay in the casino, though. That's where you want to be."
"Unknown","00;22;53;06","00;23;24;13","This is from a nice blog post from 2019 data. Scientists have developed several solutions to tackle this problem, and the three most common algorithms are epsilon greedy. Then upper confidence bowed and Thompson sampling. So Epsilon greedy is not given in this paper because it's not the best performing algorithm, but it's really a nice sort of starter algorithm. It's the simplest algorithm to address the exploration exploitation tradeoff basically during exploration or exploitation."
"Unknown","00;23;24;13","00;23;58;21","Sorry, the lever with the highest known payout is always pulled. So whatever the running best estimate of the top performing slot machine is default there. However, from time to time, some random fraction epsilon with some fraction 5% of the time or 1% of the time, select another random arm to explore the other arms with an unknown payoff. So you're sticking with one that has the highest point estimate and then just some fraction of the time you flip to a different one just to check, and then you update your estimate of how each of them are doing."
"Unknown","00;23;59;01","00;24;22;28","So that's one strategy. Now, here's two strategies that do better than that. Those are the ones that we're going to be contrasting with active inference. One of them is the upper confidence bounds, which is sometimes referred to as optimism in the face of uncertainty that sounds like active inference. It assumes that the unknown mean payoffs of each arm will be as high as possible based upon historical data."
"Unknown","00;24;23;20","00;24;52;07","So we don't know the payoff of each arm, but we want to assume, given what we've already gotten from the data, which is as far as we can speculate, we want to assume that it's as good as it could have been, which is why we see the upper confidence bound on the top of this distribution. And then Thompson sampling is fundamentally a Bayesian optimization technique with a core principle known as probability matching that can be summed up as play arm according to its probability of being the best arm."
"Unknown","00;24;52;29","00;25;16;10","So in contrast with Epsilon, which says Just go with the one that you think is best, and then 10% of the time do something else. Thompson sampling is like you kind of have a pie chart with a relative performance of different arms, and then you pick them based upon how big of a pie it is. So you do rarely choose ones that don't have high payoffs just to sort of check in on them."
"Unknown","00;25;16;15","00;25;39;03","But then if you check back on one and it does really well, then that slice of the pie starts growing. And then the whole point of training these algorithms is how fast should you be reweighting in the dynamic case, et cetera. So just laying it out like that is not the solution, but this is getting a few ways, whether optimism in the face of uncertainty or this sort of conservative probability matching."
"Unknown","00;25;39;18","00;26;01;22","These are two ways that as we've seen, our state of the art because they basically perform as well as possible. And so we're locking it down with upper confidence and a lower confidence balance. So a pretty nice choice of algorithms. They're brought up as an instructional pair all over the machine learning educational space. So nice choice by the authors to juxtapose it."
"Unknown","00;26;01;22","00;26;41;16","So clearly to active inference. So is Thompson sampling then pessimism in the face of uncertainty? Nice things couldn't be better than they have been in the past. So let's go a little bit more into detail into the two different algorithms and then talk about regret. So here's from another blog post, Lillian Wang's blog talking about Bandit Strategies So again, it's about explore and exploit, even though active inference is going to help us reconceptualize of explore exploit, which we can maybe get to at the end."
"Unknown","00;26;42;11","00;27;05;14","But we don't want to be exploring inefficiently because we're kind of spending our time playing on losing machines while we know that there's a better way to play. So to avoid such inefficient exploration, one approach is to do Epsilon sampling. And then in the case of a static set of payoffs, you decrease that parameter epsilon in time. So again, how fast should you decrease?"
"Unknown","00;27;05;14","00;27;31;19","It still have to fit parameters, but that's just one approach. The other approach to sort of prevent of this inefficient exploration. So avoid pain is to be optimistic about values that are optimistic about options with high uncertainty and thus prefer actions implicitly for which we haven't yet had a confidence value estimate. That's why this is optimism in the face of uncertainty."
"Unknown","00;27;32;10","00;27;59;18","In other words, we favor exploration of actions with a strong potential to have optimal value. So that's exactly what the UCB upper confidence bound algorithm does, is it measures it with an upper confidence bound so that the true value is always below that bound and then we are trying to push up the upper bound, knowing that somewhere below it, hopefully not too far, is the true value."
"Unknown","00;28;00;25","00;28;27;06","And then the UCB algorithm does optimization with this RS max selecting the greediest action to maximize that upper confidence belt. So as it's laid out in the blog, basically you can do no exploration. That's just sort of pick the first one. You sit down and stay there. You can explore at random. That's Epsilon greedy, or you can explore smartly with a preference for uncertainty."
"Unknown","00;28;27;22","00;28;46;16","So we're just kind of building on that Epsilon idea of sticking with the one that usually we like, but then spending sometimes elsewhere. How much time spending elsewhere and which ones should we choose? Well, don't just go to any old machine. If you're going to select somewhere elsewhere, let's choose ones that still have a good probability of having a high expected payoff."
"Unknown","00;28;47;08","00;29;23;03","So that's UCB any comments on that now? So in contrast, we have Thompson sampling from a nice slide deck from Agora Wall Columbia and Thompson sampling goes back to 1933. So like many other algorithms, the classical variants are pre computational, sometimes they're even just thought experiments and in the slides it's talked about how Thompson sampling it's a natural and efficient heretic that maintains belief about the effectiveness which is the main reward of each arm."
"Unknown","00;29;23;26","00;29;43;26","We're going to be kind of tracking how well we think each arm is doing through time. And basically the way it works is we're going to observe feedback, then update our beliefs about different arms in a Bayesian manner and then pull arms with a posterior probability of being the best arm. So not the same as choosing the arm that's most likely to be best."
"Unknown","00;29;44;12","00;30;12;12","This is like again, a pie chart that is the proportions of how well they're performing. And then we choose based upon that proportion. And the pseudocode looks like this we start we initialize the model with a prior as well as the family of distributions that our rewards are expected to be drawn from. So this is a Gaussian case, but this is where you could see a Bernoulli bandit come into play."
"Unknown","00;30;12;28","00;30;47;13","And then the algorithm is as follows. First, there's a sampling of a mean with an estimate from the posterior for a given arm. I then the arms are played according to the probability of them being the best arm. The reward is observed and then everything is updated. So it's like sample from your posterior, from your prediction then act, then observe and update."
"Unknown","00;30;47;27","00;31;12;07","It's sort of like that closing time song, you know, every, every, every beginning comes from some other beginning's end and it's called a posterior invasion statistics, you know, prior gets updated the posterior, but then that posterior is the prior for the next round. So it's not like it's just a one round learning the posterior feeds back into the model, which is why we just talk about continual Bayesian updating."
"Unknown","00;31;12;19","00;31;39;06","So it's kind of like OTA, like Jer of Orient, Decide, Act, these kinds of action loops, which of course include active inference is what makes these models really similar or at least in the same neighborhood. And then this paper that we're discussing brings them into alignment analytically with the equations and through simulation and juxtaposition. That's Robert Thompson sampling and stuff."
"Unknown","00;31;39;09","00;32;01;01","Any comments on Thompson? Just how well it plays into the sequential decision making process that we were talking about earlier? Right. So it's you have a next time step and you update and update. I mean, update. Yeah. So again, like even for those non sequential tasks, like the image classification you brought up, you could sample from your big database, then update your model."
"Unknown","00;32;01;01","00;32;25;07","And then once you're sampling and you're like I'm not really updating my model that much, you might not need to look through the entire data set. And so by framing that non sequential problem sequentially, you get big computational speed up. And then of course for sequential decision making, then you need an approach like this how about regret, learning regrets?"
"Unknown","00;32;25;07","00;32;47;20","I've had a few, haven't we? All right. So I think you put this in here the introduction to Regret and Reinforcement Learning, which is a medium post, correct? So if you want to learn more about regret, you can check it out there. But regret is just the difference between the optimal performance, how good you could do and the actual performance."
"Unknown","00;32;47;20","00;33;14;20","And so you see in this little image here you can see that the best policy is the red dotted line. And then the choices that the agent makes. This is like the logarithmic performance, logarithmic regret. And so they're converging. The idea is to converge with the best possible policy based on your previous decisions, right? Exploring, exploiting, and then the authors here use regret when they're looking at the stationery bandits."
"Unknown","00;33;14;29","00;33;50;11","They use regret as a measure of performance, but then they use regret rate when they're looking at the switching bandits. And so regret rate is just the regret over time. And they use the rate they said it was had been illustrated to be a better estimate of this logarithm mic regret in the dynamic case. Awesome. And the big idea of regret is, looking back over your history, so either for all of history, that's cumulative regret, which applies really well to the stationary problem or recent history which applies well to the dynamical case."
"Unknown","00;33;50;21","00;34;09;17","Because if things are always changing, you're kind of more interested in how well you're doing recently rather than over all time. You don't want some fluctuations early on to be playing a role in your cumulative value because you don't want to fit your strategy now to reducing regret from a different epoch, which can happen if you don't do it this way."
"Unknown","00;34;09;29","00;34;35;28","So looking back over history again, whether all of history, cumulative regret for the fixed case or recent history for the dynamic case, update your strategy so that you would have minimized regret to zero by playing that strategy all along. So, you know, looking back at the Bitcoin price what strategy would have been hashtag no regrets? Those are the big questions."
"Unknown","00;34;35;28","00;35;01;03","Or just in the last little window of time given what I recently have found out, how could I make my rate of regret gain as low as possible? So it's a way to look back and then optimistically think about how you could have performed by reducing regret to zero. And we see this all the time in computer science, like maximizing something."
"Unknown","00;35;01;07","00;35;18;29","You do it by minimizing whether there's a negative thrown in there or whether there's a natural log thrown in there or whether it's one over something or it's just framed in an opposite way. A lot of times, if you know that you're bounded at zero, something can't go below zero, then you want to go as low as possible."
"Unknown","00;35;19;09","00;35;39;18","Or if you want to maximize something, it's sometimes easier to do one over the maximum and then try to get that number to zero rather than this unbounded maximizing like, OK, I'm at 5 million. Should I stop? You don't know. Maybe that's nowhere even close because there's no highest number. So that's regret learning and that's how they're going to be calculating their performance."
"Unknown","00;35;41;03","00;36;23;09","Let's get to active inference. So one thing that you'll see on this slide right away is we're not seeing a sensory motor loop. We're not seeing an agent in an environment with arrows and nodes. We're coming at active inference from this point. Of view of sequential decision making under uncertainty. And so in this section, which is a great title, three, two, one, active interest they write that the exploration exploitation trade off can be formulated as an uncertainty reduction problem where choices aim to resolve expected and unexpected uncertainty about hidden properties of the environment."
"Unknown","00;36;23;23","00;36;52;19","So already we're seeing that active inference formalization. There's hidden states in the environment which we don't directly have access to, but we get admitted outcomes from decisions. This leads to casting choice behavior and planning a.k.a planning as inference as a probabilistic inference problem as expressed by active inference. So action and inference, our inference is about what actions we're planning and about the hidden state to the world, given the outcomes that we're receiving."
"Unknown","00;36;52;19","00;37;23;05","Sensory data using this approach, different types of exploitative and exploratory behavior naturally emerge in active inference decision strategies. Behavioral policies are chosen based on a single optimization principle or yesterday I think it was a what was it a functional, a common functional fitting or something like that. The single optimization principle is minimizing expected surprise about observed and future outcomes."
"Unknown","00;37;23;19","00;37;56;08","That is the expected free energy. So it's kind of cool like the backwards looking approach is to minimize regret. Like how would I have performed best in given what I know about the past? How does that change the way I act now? And then free energy minimization, expected free energy in the current and the future moments is like given the state of my model for now and moving forward, how can I minimize expected free energy?"
"Unknown","00;37;56;20","00;38;23;19","Because regret is not anticipatory. Regret is only looking back at it, which is why it's really easy for training. And then here's something that's looking forward. So it's kind of cool how like the current moment is this handoff between the retrospective regret learning and then the prospective free energy minimization that's how they frame active inference here. Any general comments or what do you think was kind of cool about that?"
"Unknown","00;38;24;23","00;38;45;20","So I just I'm thinking back to like Alex Francis paper that we did way back. I think that was number eight. But I still always go back to that when I'm looking at these computational frameworks because he broke it up very nicely into like the reward, the pragmatic value, which could be like in this case, the minimization of regret can be be seen as like the pragmatic value or the reward."
"Unknown","00;38;45;29","00;39;14;16","And then the epistemic value. Right. So so you have both of these things that play into giving the system how helping the agent update relevant to the system cool. And I'll look forward to hearing from the authors. Were they studying an active inference and then approached Bayesian decision making approach algorithms? Were they coming from a non active inference computer science background then found active inference as something that was exciting."
"Unknown","00;39;15;00","00;39;17;17","How did they converge upon this approach?"
"Unknown","00;39;20;14","00;39;44;14","Shortly after introducing active inference, they turned to an approximate active inference, and they described that as saying active inference in its initial form was developed for small state spaces and toy problems without consideration for applications to typical machine learning problems. So that's like when there's two decisions and sort of a two by two matrix and two outcomes in the world and two decisions you can make."
"Unknown","00;39;45;20","00;40;20;24","This has recently changed and various scalable solutions have been proposed. I think one of these citations would be that chance at all scaling active inference active stream eight in addition to complex sequential policy optimization that involve sophisticated deep tree searches. So that's sophisticated active inference with these tree rollouts followed by tree pruning approaches therefore, to make the active inference approach practical and scalable to the high dimensional bandit problems typically used in machine learning, we introduce here an approximate active inference algorithm."
"Unknown","00;40;21;26","00;40;49;10","Definitely will be cool to hear from the authors. Like What exactly are we approximating? What else could be approximated and still have it active inference or what can't be approximated. But let's look at how and why they approximated active inference so this is to kind of oh, so to kind of speak to the way you and I have both done, you know, tree construction and in terms of phylogenetic, right."
"Unknown","00;40;49;10","00;41;13;03","And the tree switching and the switching of nodes and that can get really computationally overwhelming like can run for days and days and days. So it's really nice to see such a sweet, clean implementation of approximation here. Yes. Here we're not going to go into depth into the formalism 16 and 17, but we'll look forward to the authors describing it."
"Unknown","00;41;13;21","00;41;56;04","But here is the key piece, the exact marginal posterior beliefs over reward probabilities theta can be expressed this way. OK, just looking at that, you go, OK, it's gnarly. How are we going to win if that's what we have to solve? So the exact beliefs over these are correct beliefs over reward would look this way and then they write the exact marginal posterior in equation 17 will not belong to the beta distribution, making the exact inference analytically intractable so there might be a way to numerically approach it and simulate it or calculate it."
"Unknown","00;41;56;04","00;42;23;13","But if we want to be looking through big data sets, doing it fast without massive amounts of computational effort, we need a different way. However, constraint, if we constrain the joint posterior to this approximate fully factor ized form and we've seen factorization of variables before, factorization of variables is kind of a big topic so we're not going to totally go into detail here."
"Unknown","00;42;23;13","00;42;58;25","Again, it'd be awesome to hear from the authors how do they derive in factor ice, but one way to think about it is that constraining a big open ended long equation into a factor sizable form reduces the solution space a lot, and that enables certain kinds of optimization to become tractable. So, for example, for linear regression, we have least squares which is a way where if you constrain the problem to saying, I'm trying to solve the sum of squares, not sum of cubes or the sum of some other function, but I'm just trying to solve this one."
"Unknown","00;42;59;07","00;43;24;24","Then there's computations that scale really well. So that is what is happening in factorization. And another way to think about factorization is if you have a Bayesian graph where the nodes are the variables and then the edges are like the relationships between the variables, you could just dump it all into a parameter fitting framework with all the edges possible that would be like an on factor ice model."
"Unknown","00;43;25;00","00;44;01;23","So the number of edges that you have to figure out the exact values for is going to be a lot and it's going to increase exponentially with the amount of parameters you want to fit. But if you factor is things you go, OK, variable a only is associated with B and B is only associated with C, and all of a sudden you're reducing the amount of edges that you have to fit which helps you get to hopefully a solution that is tractable reached but also approximating accurate so it's like probab probably approximately correct and approximate Bayesian computation."
"Unknown","00;44;01;23","00;44;29;07","But we've seen that factorization a few times won't go into too much more detail here, but what they do is they take this factor ized form and they use variational calculus to recover the following variational SMIL rule, which we'll go to in a second. And so they get to a different set of formalisms we'll hear from the authors about what's different about those formalisms."
"Unknown","00;44;30;08","00;44;59;17","And then what's kind of cool is that for the stationary band, it where you can change the parameters to like zero for the rate of change. It corresponds to the exact Bayesian inference over stationary Bernoulli band it problem. So that was just kind of interesting how the exact form of an algorithm solution might act of inference might be doing something exact in the asymptote."
"Unknown","00;45;00;17","00;45;27;25","Well, what is smile? Here's the smile paper variational methods as a prize to surprise minimization learning. So that sounds like a lot of things that we do with variational methods, variational Bayes and surprise minimization and the pseudocode is presented here for the Exponential Family. This is just a question for the authors. What is the smile doing how does it help us approximate active inference?"
"Unknown","00;45;28;14","00;46;07;28","What else can we use the smile for? So that's this approximating active inference interlude. They take a big messy form and then by constraining how variables can be related to each other, and using variational methods on that fact, arise representation. It's possible to get an approximation that is going to be effective so it's interesting that, you know, there's this direct correspondence for the stationary bandit case because the in the stationary case, like we didn't see improvement, right?"
"Unknown","00;46;08;04","00;46;37;26","Using the active inference algorithm. That totally explains why, right? Yep. And I have a few more pieces on that. I think in bigger figures like that. OK, let's go to the two problems that they tackle and then the results for those two sections. So we'll cover first the stationary bandit and then the dynamic bandit. So here's the definition and where they work through the stationary bandit."
"Unknown","00;46;38;15","00;47;15;07","So a stationary bandit has a finite number of arms it has K arms, and then it's going to be playing through t time steps and then it's big, big K arms. And then each little little k k is the set of little arms. Little K, yeah. K is the action so here's how they define the stationary bandit. And again, you can think about T in time, actions and arms and then in stationary band, it's the reward probabilities."
"Unknown","00;47;15;27","00;47;44;15","Theta sub k are fixed for all trials. So theta is just like it's funny, it's like the parameter that means just like a variable sometimes. I guess a lot of them do, but theta especially. So just a fixed reward value the fixed reward probability. So that's why it's not stationary in the casino. This would be like one of them pays out on average one to 11 of them is pays out 50%, one pays out 200%, but they never change."
"Unknown","00;47;46;08","00;48;30;25","So here's a sort of visual representation of that with here's the multi armed and the mouse is getting the cheese and each arm has a different fixed probability, but those don't change. So the probability of cheese does not move here's FIG. one. So in FIG. one, we're going to see regret rate analysis so the regret rate you can imagine is we'll get we'll get to their second, but the regret rate analysis for the stationary bandit and then we're going to be comparing approximate active inference in red to the exact active inference in blue."
"Unknown","00;48;31;22","00;49;00;17","So that will be cool to talk to the authors about what was the relative change in how much time it took to compute we'll find out. And what they do is they are showing that the blue and the red are always kind of tracking together. So that suggests that the approximate form does a really good job because it follows the behavior of the exact form pretty well."
"Unknown","00;49;01;17","00;49;31;02","And then just to sort of how to read this graph, so there's four columns corresponding to K equals ten, 20, 40 and 80 Rs. So that's changing just the number of arms. Then there's the rows which are four epsilon being the. So this is like the differential between the arms. So kind of how easy the task is, I believe."
"Unknown","00;49;31;25","00;50;34;13","And then that's point one, point two, five and point four. And then within each cell there's lambda, which is a function of precision over prior preferences and then r sub t which is the regret as a function of trials. And the dashed black line denotes the upper bound on the regret rate corresponding to the random agent. So this is I guess as bad as you could get and then we see that active, for instance, performing better, like there's it converges upon a lower rate of regret than point one or around point one, which is the amount of regret that's accumulating for the randomly behaving agent so so just to footnote that, which I thought was"
"Unknown","00;50;34;13","00;51;04;21","interesting and made this more meaningful for me, that lambda parameter, the precision so it says when the active inference agent has very imprecise preferences, so lambda closer to zero, it engages in exploration for longer and reduces the uncertainty that way at the expense of accumulating reward. So just just to kind of footnote that in there. Hmm. Interesting. And that's definitely going to come back when we talk about active inference and how we rethink, explore, exploit."
"Unknown","00;51;05;22","00;51;32;04","So what is this showing it showing that as you though there, that as a function of precision, you're getting different behavior in the active inference agents, but the approximation is basically always working pretty well. And then also the dotted lines are fewer trials and the solid lines are more trials. So we can see, like in all these cases, the dotted line and then the dashed and then the solid."
"Unknown","00;51;32;27","00;52;03;21","You're always doing better with more trials. That's what we kind of see like if you only have 100 trials, so just a few trials on arms, your regret is almost as if you were playing randomly because you've barely tried every arm once. So you're kind of not able to do so well. But if you're in the right area of precision, not too much precision, but still in this area, down here and you have 10,000 trials, even with 80 arms, the regret rate can drop to very, very low."
"Unknown","00;52;04;01","00;52;47;17","So playing like almost optimally even on 80 arms, given the trials and given a significant enough difference between the outcomes. So that's what figure one shows which is that active inference can learn to reduce its regret given the right parameters, the things that we'd expect to make this situation harder, like more arms or less differential ability among the arms or super high or low precision very rates those are the things that influence active inference algorithms and also they said that they found the minimal regret rate at around lambda is 0.1."
"Unknown","00;52;47;17","00;53;11;24","So that's why they fixed the the regret rate for the upcoming trials or for the upcoming tests because there's there's a bunch of knobs to tweak. And so they do their best to do a parameter sweeps and show like here's for, you know, the whole distribution of lambda for three different sampling regimes for four different arm styles, for three different difficulties."
"Unknown","00;53;11;24","00;53;36;07","Like already the combinatorics get really high. And if you were going to be using this in an industrial situation, you would optimize it with all of these parameters in play. And then they write that they're only going to consider this approximate active inference variant for the between agent comparison because it was doing pretty well in figure one. So they're just going to follow up with the approximate."
"Unknown","00;53;36;07","00;54;01;08","But it's something we can ask them, like what is the difference in computation, time for the general or exact right figure two? So we're still thinking about the stationary Bernoulli band. It here is going to be a slightly different plot. It's a comparison of the cumulative regret trajectories for the approximate active inference that's A.I. optimistic tops and sampling."
"Unknown","00;54;01;08","00;54;26;07","That's Osseous purple and Bayesian upper confidence bounce in the teal. So those are the three lines and that's the legend on the top left. And as Lou just said, the prior precision is fixed. 2.1, which is what they found from Fig. one, like this sort of dip that they all have at near point one. That seems to be a good precision variable setting."
"Unknown","00;54;26;07","00;54;52;11","So they're just going to roll with that instead of also sweeping across precisions. And here we see a similar columns and rows or K different number of arms in the columns and then different problem difficulties in the rows and then the cumulative regret. So you want lower regret? That makes sense. That's the whole thing that we're training on."
"Unknown","00;54;54;01","00;55;21;04","What do we see? There's a lot that can be seen because there's so many combinatorics of going at it. But what do we see right off the bat? Well, as the number of samples increases beyond, say, 100 in almost all cases, the red line, approximate active inference is below the other ones. So you're getting less regret with active inference on the stationary band."
"Unknown","00;55;21;04","00;55;57;09","It for many parameter combinations. OK, but there's a few interesting pieces. So one of them is that sometimes active inference early on has a higher regret. So maybe it's like a little bit more exploratory in the beginning. That's one thing to see in that bottom left corner. There's a few other ones where it's sort of like this elbow early on, but then there's this very interesting behavior that actually shows a lot on the very top left so it's not always the case that you can just set your model to some parameter combination and draw a really generalized conclusion."
"Unknown","00;55;57;17","00;56;23;20","But this is an awesome point by the authors so they're looking at an ensemble of agents. So here it's a thousand agents, like they're sort of running a thousand of each of these and then taking the average which is why the lines are smooth. And so what we see in the very top left is that the active inference agents doing as well as the other algorithms, then it's doing better."
"Unknown","00;56;23;21","00;56;47;04","Right lower regret under those lines. But then the error bound increases that red shading and it starts to head really far up. So that's pretty fascinating what's happening. And they write this divergence is driven by a small percentage of the agents in the ensemble that did not find the accurate solution and were overconfident in their estimate of the arm with the highest reward probability."
"Unknown","00;56;47;21","00;57;16;16","So that's sort of pie chart of which arm you should choose. It got off to a bad start and then their precision was such that they just sort of rolled with that and they kept on seeing the information they were getting within that way of setting up the problem. And so they ended up for a small fraction of the agents kind of getting derailed with continuing to regret the decisions they were making because they were locked in the divergence is not visible in the earlier setting."
"Unknown","00;57;16;16","00;57;43;29","Easier settings with larger. I'm not sure what various misses there as one requires larger ensembles and bigger number of trials to observe suboptimal instances. It may appear surprising that the divergence is evident only for the smallest number of arms considered because that's supposed to be like the easier problem. However, the reason for this is the smaller the number of arms is, the more chance the agent has to explore each individual arm for a limited trial number."
"Unknown","00;57;44;17","00;58;10;12","So it's almost like it's easier to walk in your beliefs and maybe even false beliefs for a small social group rather than a party with K equals 80 people. It's like there's so much to learn that you're less likely to get locked in early. But then at the smaller party, some of these active inference agents are getting locked in really early to what they think the most rewarding arms are."
"Unknown","00;58;10;28","00;58;37;28","And then they end up not sampling efficaciously, so they end up implementing regrettable policies. So pretty fascinating so I just want to footnote here also. So you're looking at this top left where K equals ten, and then this epsilon is 0.1. The top left is the most easy and then the bottom right is the most difficult where K is the number of arms."
"Unknown","00;58;38;08","00;59;08;01","And then this epsilon factor is the difference between the arms like. So it's it's the outcome difference. If I go to an arm that has a reward versus an arm that doesn't. Yeah, I think 0.1 is harder because there's less of a distinction between a better and a worse so the smaller number of arms is easier. But then point four is a bigger contrast so point one is harder than than point four."
"Unknown","00;59;08;01","00;59;33;02","OK, so then it's the bottom left. That's the the there would be the easiest well depends on what you mean. Easiest. I mean, all things being equal, fewer arms is easier. And then all things being equal, the more contrast between good and bad arms, the easier. So got it. So then the bottom left would be the easier. Yes, the fewest arms and the biggest contrast."
"Unknown","00;59;33;02","01;00;10;04","And interestingly, that's where we see the biggest elbow of active inference, like the biggest initial regret. So it's like when the game is easy and there's few arms active inference stays exploring a little bit longer, but then it ends up locking in on what's right. And then especially when there's not that big of a difference between arms, all of them have this slight uptick and then the variance increasing with the shading that's showing us that it's not like all of the ensemble is behaving differently, but perhaps again, that a subset are getting deranged."
"Unknown","01;00;11;10","01;00;40;10","And then, yes, that's why I think the bigger epsilon would be the more difficult the well, we'll just have to save that one for the others. I would think that the less contrast would be easier. So I guess that's it's like not super clear. So we'll just have to ask. Yeah. So that's a cool outcome and it's almost like it's a qualified critique that makes a bigger point about where active inference can really succeed and where it might still need challenges, which we're going to return to."
"Unknown","01;00;40;26","01;01;18;16","So that's stationary bandit. Let's go to the switching bandit. The key difference is that each time step, any particular arm has a maximum expected reward and this reward probability is going to change with the probability. So there's some probability rho which is, is it P or zero? Yeah, with Rho, that is the probability of the reward changing. So now you can't just learn it in any old order because things are changing through time."
"Unknown","01;01;18;16","01;01;47;21","The probability of the cheese is time dependent section 2.2. Oh, yeah, go ahead. Oh, I don't know if anything. No, that's it. So you did it gets 2.2 is where they give the formalism for the switching bandit. And so in contrast to the stationary bandit problem, outcomes are drawn from a time dependent Bernoulli probability distribution. So that is provided and those who want to look into the details can do so."
"Unknown","01;01;48;01","01;02;14;09","But the key difference is that there's some probability that things change as you're playing the game. So so I don't have a question here for the authors and looking forward to getting to ask them. So it's the probability is the same and then it just suddenly changes. So like after 20 time steps or 50 time steps, like it's the same, the same, the same and then it suddenly changes or it's the, it's changing at each time step."
"Unknown","01;02;14;09","01;02;43;07","I feel like it's because it says it changes with some probability, but it's otherwise constant. So I'm curious, is a constant like across all of the arms, is that the probability that's constant or is it always changing? I don't know. Question Here we get to the between agent comparisons of different algorithms in the switching Bernoulli bandit with a fixed mean outcome difference so epsilon equals 25 so they're setting epsilon fixed."
"Unknown","01;02;43;07","01;03;14;18","So now it's going to be a different row and column situation so in this figure the three columns are row equals .0102 and 04. So that's the probability of changing. So here on the left column it's a dynamic environment with 1% of time steps. There's a change here. It's on the right side. It's up to 4% changing. So all things being equal, the more change, the harder it's going to be because it's going to be like changing faster."
"Unknown","01;03;14;26","01;03;47;00","And then the last change, it's more like the static case and then the rows now are the arms. So K ten, 20 and 40. So all things being equal, it's easier when there's fewer arms because there's fewer decisions to make and then we're going to be looking within each cell as the regret rate. This R sub t with the tilde regret rate through time where again you want to have less regret so lowers better as a function of the samples."
"Unknown","01;03;47;08","01;04;11;09","So up to several thousand samples and then here are the algorithms that are being compared active inference in red Bayesian upper confidence and blue optimistic Thompson in purple and then random control in the black dashed line. So the random play is as is a good rule, it's it's not the adversarial play so it's not like you're you're choosing to lose."
"Unknown","01;04;11;09","01;04;40;02","But this is the random play. And what we're seeing is that it accumulates a static amount of regret rate it kind of starts the same value the purple and the teal algorithms they also converge upon a regret rate that's lower than the random play. So they're playing better than random. But we see in essentially all cases the active inference is lower than these other algorithms."
"Unknown","01;04;40;14","01;05;09;11","So when we say like, oh, active inference has better performance on this task than some other algorithm, this is kind of what it looks like. This is saying that as time goes on, active inference is acquiring regret at a lower rate than the other algorithms it's just performing better, it's implementing better policy. Then we can see that for the list less changing and fewer arms on the top left."
"Unknown","01;05;09;11","01;05;36;00","So that's like easier. We see that the difference between random play and the algorithm is significant and active inference does like super well. Whereas when you have a very dynamic environment and more arms, you don't get that much of a regression. You still doing relatively better and still active inference outperforms the other algorithms, but it's like imagine if it was changing every few time steps."
"Unknown","01;05;36;00","01;06;05;26","So there's 100 arms you would just you would never be able to sample enough to make a meaningful update to your model. So that's why the faster things change and the more arms there are, the more the overall performance of any machine learning algorithm is going to converge to random play. Whereas the more static the problem is and then the fewer options there are, then better and better strategies are going to perform better and better relative to random."
"Unknown","01;06;06;16","01;06;33;03","So this is just pretty cool that they they do simulations, they do, I guess, a thousand simulations. And then the switching schedule is generated randomly for each agent instance within the ensemble. So they do a full simulation a thousand times for each of these cases and then average it out and yeah, well done. Active inference. Pretty cool to see that it is doing better."
"Unknown","01;06;33;03","01;07;13;15","On the switching for newly banded for a wide range of situations and it sort of it rapidly figures out a good way to work and then it stays at that low rate of regret, accumulation any thoughts on three? Super cool. Yep. Now in four we're going to fix the number of arms so now we're at K equals 20 arms in a switching for newly banded and then we're going to be comparing these algorithms again so here the columns are just as they were before with less changing rho equals .0102 and 04 on the right."
"Unknown","01;07;14;01","01;08;01;13","But now as we kind of saw on previous figures, we have Epsilon, which is that differential between the rewarding and less rewarding arms as the rows. So we, we can think about the less changing and the most contrast between rows is where you see the most gain relative to random play. Whereas when you're in the more dynamic setting, and when there's less differences between which choice you make in terms of the outcome, then there's less of a regret rate differential with the algorithms but again, broadly across the board, active inference is performing better than these other algorithms."
"Unknown","01;08;01;27","01;08;44;15","So as sampling increases active inference locks into a pretty good spot by around 200 or 500 samples, and this is for the 20 arms. So it's kind of like it's visited each arm probably a few times, maybe some more than others. But again, they're always changing and so active inference is able to cope with that. And also it's interesting that especially this teal one, it almost has a lower regret rate and then it actually creeps up, whereas at least just visually we're seeing active inference kind of flatline but not creeped back up."
"Unknown","01;08;45;00","01;09;10;01","And so that's something that can happen with all kinds of algorithms that they'll get locked into an average precision regime and so this is just another way to slice it. In figure three, they fixed the epsilon. The difference between the more and less rewarding arms, and then they explored how the number of arms was associated with how dynamic of a problem was being solved."
"Unknown","01;09;10;12","01;09;29;05","In Figure four, we fixed arms so that we could explore how rho the dynamic changing probability and epsilon the differential, how those change performance and then OK, good."
"Unknown","01;09;32;04","01;10;06;18","Now, OK figure five here it's yet another similar looking figure we're going to have similarly the row for the column so less changing and left column more changing on the right column and we're going to have the number of arms just like figure three with ten, 20 and 40. But there's going to be a non stationary difficulty and the difficulty, the problem is non stationary as the advantage of the best arm over the second best arm changes with time."
"Unknown","01;10;07;25","01;10;48;28","Yeah. So so epsilon is varied. Yes, exactly. So switching changing epsilon to time and they fixed the precision over preferences to lambda equals point five. So that's a different lambda than they used elsewhere. And this is just sort of showing that the variable values they choose clearly work because they're outperforming state of the art. But here they chose a different variable blue, the lambda point five they used in all of the switching six Sigma Theta four and figured five and they changed it to point five in the switching because they've just optimized for that but didn't show that variable optimization OK, thank you."
"Unknown","01;10;49;28","01;11;18;24","And so here we can see that. Yep, active inference red line is on bottom and it doesn't have this sort of secondary increase in regret rate. So for even dynamic on the right and many arms on the bottom right, we're seeing active inference perform well. So figure three, four and five really make a super compelling case for active inference outperforming state of the art algorithms."
"Unknown","01;11;20;02","01;11;47;29","Nice work. And that's something that we're really excited about. Let's go to the discussion and just spend a couple of minutes on the discussion in the closing notes and then we'll look forward to the DART one and top two. They open the discussion by reversing what they have previously mentioned that they're comparing active inference to two state of the art machine learning algorithms, the Bayesian upper confidence bounds and the optimistic types and sampling."
"Unknown","01;11;48;11","01;12;22;04","That's the pair of algorithms in the pair of problems, the stationary and the non stationary stochastic multi arm bandits. There are contribution, among other things was the introduction of the approximate active inference algorithm, and then they perform some checks to show that it's performing again. As well as you could do. Exactly. So they derived an active inference algorithm that's efficient and easily scalable to higher dimensional problems leading us to ask, of course, where could it be cool or useful or important to apply active inference?"
"Unknown","01;12;22;22","01;12;59;06","And we know that there's many thoughts on this and hopefully will be many more. But we'll just note for those who are listening at the right time that there's been recently announced a net hack challenge and we're going to be starting work on this in the end of July or the beginning of August. 20, 21. So if you're interested in applying active inference to a video game performance challenge, we hope that having a collaborative team make an entry for this challenge will be an awesome opportunity to really demonstrate to the world what active inference can do."
"Unknown","01;12;59;17","01;13;35;26","But we'll also look forward to the authors and all guests sharing what kinds of questions they think might be interesting to explore using approximate active inference type algorithms like this here's a really nice point that speaks to that again in Figure two, where we saw that sort of runaway regret on a small fraction of the agents that were just getting locked into extremely regrettable decisions in that K equals ten case."
"Unknown","01;13;36;25","01;14;04;28","So what what do they say about that? To our surprise, the empirical algorithm comparison in the stationary bandit problem, figure two showed that the active inference algorithm is not asymptotically efficient. The cumulative regret increased faster than logarithmic in the limit of large trials. So in other words, even though it's not all agents, they get deranged. The ones that do are dragging down the group in a way that's damaging."
"Unknown","01;14;06;20","01;14;30;20","This cause for the behavior seems to be the fixed prior precision over preferences. Lambda, which acts as a balancing parameter between exploration and exploitation. So this is a pretty interesting piece. Instead of like two modes explore and exploit and then we're going to have a parameter that flips us one way to the other, like a light switch, or we have one extreme explore mode and one extreme exploit mode in the model."
"Unknown","01;14;30;25","01;15;03;19","And then we're going to have a dimmer that goes between the two in active inference. The knob that kind of does that is actually the prior precision over preferences. So if you have no if you have no preferences a.k.a of no precision over preferences, you're going to act in the maximally exploratory way. If you have an incredibly strong, incredibly precise prior over your preferences, then you're going to act in a very exploitative way."
"Unknown","01;15;03;28","01;15;21;19","Like if there's two restaurants and one of them you like, you know, 60%, the other one you like 40% if you have no preference you're going to go to them 5050, that's like explore. But then if you have a super high precision over your preferences, you're always going to go to the one you that you prefer, even if it's only slightly preferred."
"Unknown","01;15;22;25","01;15;47;26","An analysis of how performance of the algorithm depends on that parameter showed that parameter values that give the best performance decrease over time, suggesting that this parameter should be adaptive and decay over time as the need for exploration decreases. So start with low precision over your preferences and then you learn and you update so that eventually you increase your precision."
"Unknown","01;15;47;26","01;16;16;24","Later attempts to remedy this situation with a simple and widely used a case scheme. We're not successful. So the logarithm of time. So you just make the precision directly a function of time like, you know, one over the number of trials, just that kind of thing. It's like that's a function. It scales with the number of trials. So here you scale it with the logarithm of time this indicates that it's not a simple relationship and a proper theoretical analysis will be needed to identify whether a scheme exists."
"Unknown","01;16;17;25","01;16;41;08","So that's pretty interesting. Like how are we going to pull back a level? OK, we think that active inference architecture is going to be a really productive way to model learning under uncertainty. But we're also just pushing the problem up a level which is, OK, how uncertain should we be and then how should we change that uncertainty over preferences moving through time."
"Unknown","01;16;42;12","01;17;19;29","So nice point there. And then one last thought is in the non stationary. So in the dynamic switching band it the active inference algorithm generally outperformed the Bayesian upper confidence bound and the optimistic Thompson sampling. This provides evidence that the active inference framework may provide a good solution for optimization problems that require continuous adaptation. Active inference provides the most efficient way of gaining information and this property of the algorithm pays off in the non stationary setting."
"Unknown","01;17;21;20","01;18;02;26","Cool points. That'll be awesome to hear the author's perspective on. So how do we compare active inference framework algorithms to machine learning more broadly and then especially for problems where informed foraging is helpful, like not that epsilon greedy just sometimes look away from the best and pick randomly and not even just the Thompson sampling. Remember, pick each arm according to how likely it is to be the best arm we could have an even more informed or empirically better performing scheme for doing the tradeoff for earning while learning."
"Unknown","01;18;03;07","01;18;25;23","You know, earning while learning doesn't mean that you're learning optimally or earning optimally or how are we going to balance that out. And it seems like active inference is going to be a strong contender in the coming years. As more and more people in the machine learning community start to get keyed in to these results, tune the regime of attention to what these authors and others are doing."
"Unknown","01;18;26;05","01;18;49;08","And it starts to become more broadly understood that, hey, rather than just having more and more parameters and billions and billions of parameters in our model and training it on larger and larger networks of computers, what if we just had a curious agent who learns how to learn and prefers to win? It's sort of like a ground floor reinterpretation of these problems."
"Unknown","01;18;49;24","01;19;20;22","And I think hearing the author's views on where the active inference and machine learning communities and fields are heading will be for sure informative for us. What do you think about that? Lou, I just I'm thinking about problems like like the mountain car problem, like we're actually learning is winning, right? Like when you have to if you have to explore, you want to go up the higher you want to go up the hill so that you can get back up to the other side of the hill and the flag."
"Unknown","01;19;21;01","01;19;47;10","So like one of the problems are out there like that that doesn't don't necessarily have like a reward that's uncoupled to uncoupled from exploration. So I'm thinking about that. And I don't know just where is where is learning winning? Great question. It's almost like in the mountain car, the altitude is in some sense as a reward heuristic, you want to get to the top of the hill so it seems like altitude would be the way to go."
"Unknown","01;19;47;23","01;20;12;17","But then the way that the problem is set up, you could also optimize the bounce and say, I seek to be expanding my bounds laterally more. Oh yeah. Well, of course we're changing in height. You know, we're in the mountains, but especially for cases where we focus on exploration like innovation or maybe other areas, it's cool to think about how active inference could perform well."
"Unknown","01;20;13;17","01;20;17;09","So what are the next steps for the authors and for us?"
"Unknown","01;20;19;11","01;20;45;18","An important next step in examining active inference in the context of multi arm bandits is establishing theoretical bounds on the cumulative regret for the stationary bandit problem. Because, as we saw, it kind of diverged in an unexpected way that made sense but still wasn't quite the behavior that was initially expected. And a key part of these theoretical studies will be to investigate whether it's possible to devise a sound decay scheme for that lambda parameter."
"Unknown","01;20;46;00","01;21;15;27","So how rapidly should we change our precision over preferences that provably works for all instances of the canonical stationary bandit? What would that enable? That's our favorite question. This would lead to the development of new active inference inspired algorithms which can achieve asymptotic efficiency. These theoretical bounds would also allow us to more rigorously compare active inference algorithms to the already established bandit algorithms for which regret bounds are known."
"Unknown","01;21;16;24","01;21;50;01","Moreover, we would potentially be able to generalize beyond settings we haven't empirically tested here. Future work may also consider an information theoretic analysis of active inference which might be more appropriate than regret analysis. Also kind of a cool idea like regret analysis was performance oriented. It's saying you're calculating your regret based upon the difference in performance between optimal versus what you did."
"Unknown","01;21;50;22","01;22;19;19","But another way to look at that is using information theory and looking as a function of information being gained. Perhaps through time, which just like you brought up, it really helps us focus on exploration. Because if the whole project is framed in terms of performance and reward and value, it's almost like favoring exploitation off the bat because exploration is always going to have to get couched in terms of successful exploitation."
"Unknown","01;22;19;27","01;22;48;29","Like this basic research is important because later on we'll be able to apply it, which is fair. But when we have an information theoretic analysis, not just a regret performance analysis, there might be a way to have exploration for exploration sake. And what that might enable or even exploitation can be thrown in. In terms of surprise surprise minimization, right?"
"Unknown","01;22;48;29","01;23;09;19","Like if we're going to frame active inference in terms of minimization of surprise, think about something like the stock market problem, right? Like this is very much like a switching bandit problem. Like everything's changing all the time. Like it's the same for a little while, like your, your, your stock that's going to win. So it's like, you know, you're surprised when you deviate from, say, the S&P 500."
"Unknown","01;23;09;24","01;23;52;01","So you want to track with the S&P. 500 and surprise happens when you deviate from that one more thought there on the information theoretic analysis and how it might say something different than regret. Let's go back to picking a restaurant so we often hear about like controlled novelty. You want surprise but not too much surprise. So this gives an explanation for why people might like their favorite restaurant it doesn't even need to be thought of in the context of maximizing reward, like the 60% likely to be the most rewarding restaurant it can be like I'm trying to reduce my uncertainty, so I'm likely to choose a place that I'm familiar with so that maybe I'll"
"Unknown","01;23;52;01","01;24;22;28","choose a different meal, or maybe I'll choose the meal I always order. But it's like the choice was driven by reducing uncertainty, not even per se, by maximizing reward, which is something that we come back all the time to the difference between value driven reinforcement learning, reward learning and implicitly regret, which is regret about reward and reduction of uncertainty, which puts us in a whole different space."
"Unknown","01;24;23;12","01;25;07;00","Yes, we're able to enter that whole physics of information flows area and where we get the pragmatic straight component and then the solenoid or the flow, the isoquant, where, yes, we access that whole area in the information theory, reduction of uncertainty world. But also we just get qualitatively a different story in a different explanation. For behavior. Again, you don't need to grasp for why somebody is maximizing reward with why they went to that restaurant or why they ordered that one thing when a lot of times just it stands alone as an uncertainty reduction and prior preferences understanding rather than needing to make it about some sort of maximization."
"Unknown","01;25;07;13","01;25;42;13","11. I'll flipped last slide then we have an awesome question. So Alex, we wrote is there connections between exploration on epistemic value and exploitation for pragmatic value? I think that's related to what you were just speaking about, which is that we're putting it on a common footing with active inference in terms of the reduction of the expected free energy, which has this epistemic or knowledge gain component and this pragmatic or functional component."
"Unknown","01;25;42;19","01;26;17;21","So the epistemic is like learning and the pragmatic is earning. So we're, we're learning while were earning or the other way round, we're acquiring epistemic valuable information while we're also acquiring pragmatically valuable information because the decision making of expected free energy minimization conditioned on policy, we're choosing policies based upon a function that looks at both of those jointly."
"Unknown","01;26;18;06","01;26;43;15","And so that is one of the ways that we're rethinking exploration and exploitation by thinking OK, that's not really the duality of policies that you want to talk about, because explore exploit makes it sound like it's two behaviors that the bird can be doing and so here what we're doing is we're kind of taking that idea of sometimes you want to be doing a more exploitative act, other times more exploratory act."
"Unknown","01;26;44;01","01;27;16;01","And then we're asking, given the policy of which a whole spectrum might exists of different policies, how is each policy contributing to epistemic and pragmatic gain? And then which policy is going to have the best combination of the two maybe there's strategies where you're maximizing both and maybe their strategies where you're maximizing neither, but if you're only looking at one or the other, then you might choose one that's like a poor combination."
"Unknown","01;27;17;01","01;27;36;21","Well, so a perfect example is like, you know, so your policy is based on your previous chance of of reward, of minimizing surprise. So I have a policy that I'm not going to eat a meal I've never had before in a restaurant. I've never been to before because I like I don't know what I'm getting into. And that's like way over there, right?"
"Unknown","01;27;36;23","01;28;10;27","So my policy would be to order a new meal in a restaurant that I like or order a meal that I'm used to in a new restaurant. So that would be like the policy in that just to continue that example or. Well, what if on discussion, I hope this was useful to those who are familiar with active inference as well as Bayesian statistics machine learning or not, it's all chill because your questions will really help move the whole project forward."
"Unknown","01;28;11;08","01;28;35;20","The parts that do and don't make sense and the parts that you want to explore more. I think that's how our ensemble is going to proceed in these transdisciplinary areas is by welcoming those who have different backgrounds. So whether you're really familiar or not, you're totally welcome to participate in our live discussions or just to ask questions or participate in some other way."
"Unknown","01;28;37;07","01;28;47;00","Thanks Blue for the awesome help with preparing the slides and for this conversation and we'll see everybody another time. All right. Bye."

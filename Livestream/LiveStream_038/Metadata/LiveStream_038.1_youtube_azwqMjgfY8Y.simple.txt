SPEAKER_03:
All right.

Hello and welcome everyone to ACT-INF Lab live stream number 38.1.

It's February 16th, 2022.

Welcome to the ACT-INF Lab.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links on this slide.

This is a recorded and an archived live stream.

So please provide us with feedback so that we can improve our work.

all backgrounds and perspectives are welcome and we'll be following video etiquette for live stream just release an unending torrent of emojis if you have to speak or just raise your hand i'm sure that we'll get to it if you're watching live please feel free to write questions in the live chat and we'll have enough time to hang out and discuss during this dot one where we'll be opening into the paper

Check out ActiveInference.org for updated information on participating in any of the lab's activities.

I hope you'll find something that resonates with you.

Today in ACT-INF,

Livestream number 38.1.

We're going to be learning and discussing this cool paper, The Evolution of Brain Architectures for Predictive Coding and Active Inference by Pizzulo, Parr, and Friston from December of 2021.

And we're just going to enjoy discussing it and opening up any ideas or questions that us here on the panel have or those who are live chatting us.

And we have some ideas and thoughts prepared, a few things that we know that we can go into.

And also, I hope that everyone has brought some other prepared seeds and also, of course, spontaneously feeling like new things are arising.

So we'll just start with some introduction and warmup.

We can each say hi, and then maybe it'd be cool to just also mention like what got you excited about the paper or what made you want to discuss this paper?

What's something that stayed with you?

So I'm Daniel.

I'm a researcher in California, and I was very excited by the evolutionary focus.

A lot of my research over the last years has been in evolution and ecology.

So it's always awesome to see how people are thinking about how active inference, free energy principle, and evolutionary studies can all be learning from one another.

And I'll pass to Stephen.


SPEAKER_00:
Hello, I'm Stephen Sillett.

I'm in Toronto.

I'm really interested in how this paper connects to my work with spatial meaning making and social topographies.

Because this paper talks about a more biological, nascent stage of development, rather than some higher order meaning that often people think about.

And I'm really

interested in how this sort of grounded, bottom-up, inactive ecological approaches can be thought about as something where we can actually ground a lot of our meaning making.

So I'm interested in how this might match with some of that thinking.

And I will pass this over to Dean.


SPEAKER_01:
I'm in Calgary.

What I found interesting about the paper is that given that I've worked with a lot of young learners for a lot of my life, I

It was really interesting to see how this affirmed a lot of the thinking that I was doing when I was trying to get people past the idea that science is only about the biology and the chemistry and the physics, that there is a statistical and predictive component to this.

And those relationships in that realm can build certain sense of what the underlying architecture is.

So as I said,

A lot of this is affirmations, and it's kind of seen as being not part of what is typically addressed as core learning, but I think it should be.

Back to you, Dan.


SPEAKER_03:
Cool.

Nice intro.

Let's just start with a big question, and then...

Either of you have any reflections on the big question, or we'll go over to blank slide and just bring up some questions we have.

And also it might be good to go over just some of the key points in the paper, like what did they actually do in terms of their contribution?

So the big question, or at least one way to phrase something that might bring someone to approach this paper,

What is the evolutionary neurophysiological basis of cognition?

And how do complex cognitive phenotypes arise?

Like you don't go from zero to colony in one time step.

How does it happen that evolution arises from precursor, sometimes simpler, but also sometimes more complicated, precursors?

Stephen mentioned like, what is the basis for sense-making and cognition and sense-making are very related.

And what's been fun over these last weeks and months is we've explored basic

or simple or reduced or basal cognition from a variety of perspectives.

Like we talked about the bioelectric components of thinking about basal cognition with Mike Levin.

Now we're thinking about a slightly different approach to understanding basal cognition, which we'll be discussing here.

And then also we've looked at some of these more complex cognitive phenotypes like mental action, counterfactuals, deep

temporal inference all the abductive logic which we'll get into probably later um how can we use integrative models of perception cognition action and impact like active inference to study this whole continuum and diversity of relatively simpler in our perspective cognition and also relatively more complex cognitive phenotypes

and everything in between.

So yes, Stephen, and then on with it.


SPEAKER_00:
Thanks, Daniel.

Yeah, I mean, I suppose also what some of the paradigm impacts of that, of taking that

all the way through.

And one question that might come up, which you may or may not have an answer to or thoughts on, but, you know, Mike Levin's work with the teleological cones at the different nested levels, and how this sort of could be thinking about what kind of action control potential

you know, in these different nested levels might exist.

I would be interested to think about how this may either be able to connect to that.

I can see it connecting philosophically or whether there's another bridge needed between this modeling and that type of modeling.

So just something I've mentioned.


SPEAKER_03:
Great.

Dean, anything to add?


SPEAKER_01:
I'm going to wait until you get to figure one.


SPEAKER_03:
Okay.

So we won't go through all of the finer points in the paper.

That's for the reading of the paper itself.

Nor will we even go over all of the overview, which is what the .0 video, 38.0, is for.

So, you know, turn back time and watch that one if you haven't, or pause the video if you're not watching it live.

But

What this paper does, as evidenced by their roadmap, is introduce a few basic principles of cognition, predictive regulation and control, and then also structure learning in generative models.

That's not how every neurophysiology text is gonna begin

building blocks of cognition it's not how every evolutionary neurophysiology text is going to be framing cognition but that's what they do here and that will play into how it's similar and different than other approaches then with those building blocks in hand or on the floor they provide three examples of motifs that ancestral brains may be modeled as having or may have actually had

With those three examples in hand, it's then possible for them to state a more general way of thinking about the transitions amongst different structures.

These structures represent brain design as structure learning in generative models.

That is called an evolutionary algebra, and they introduce five operators that can basically either leave unchanged or change

the structure of brain design with respect to generative models.

And each of those are explored in terms of what are the architectural changes that that evolutionary transition is?

And then what are the functional consequences of that kind of an evolutionary transition?

Then they have some discussions about how sequences or patterns of application of this evolutionary algebra could lead to different evolutionary phenomena.

So, for example, increasing temporal depth in the future means that models are increasingly prospective.

Increasing temporal depth in the past is like memory, etc.

And then they close with mapping to a phylogenetic tree and thinking about that evolutionary algebra of state transitions being mapped onto the bifurcating tree structure that's called a phylogenetic tree, which shows the relatedness of different life forms.

That's the roadmap.

Let's go to the long awaited figure one.

and talk about the action perception cycle and predictive regulation.

And so they're, again, discussing this in the context of predictive regulation, anticipatory regulation, cybernetics, and control as a basic design principle for the brain.

So Dean, what do you see here or what would be cool to think about?


SPEAKER_01:
So what this follows, I think, is the usual pattern when we're trying to explain where we're going to go.

And so you just had a roadmap up.

And I think a roadmap is a way for the people who are reading the roadmap to find their way.

But I think what this diagram shows is that in the middle of there, there's something called a discrepancy.

And that discrepancy is later going to be given a label Y. And

What I think is really interesting here is there's a flip potentially that can happen here.

That discrepancy is where what I would describe as a rule factory and rule in the sense that we find pattern.

That's kind of our place where, as it says, the prediction and the observation come together.

And I believe that that's different than a roadmap, which is find your way.

I believe that that discrepancy or that rule factory is wayfinding.

And I think...

What we have the potential to do with this paper because of the way that they have sort of presented the information is the words eventually become explicated rules, which then become action in the phenomenological space.

That's where it gets really, really interesting because I think most of the time when people look at things as a subject, they've taken the world and they've collapsed down to words and diagrams and models.

What this potentially allows us to do is flip that and move from the words back out into the space with a little bit more confidence.

So yeah, that's why I wanted to kind of start here because I think the word discrepancy, it's the first time I've seen it in this kind of figure


SPEAKER_03:
model and i really like it cool yeah it makes me think about uh setting off on the road trip on the mental actions that

reflect the paper and there's the roadmap which is super informative but it is like instructionalism it's saying you're going to go two streets and then take a left turn at the stop sign and then when this happens then you'll do that and if you've seen this then you've gone too far kind of classic instruction type sequences

Here we have a figuring out because it's visually arranged with some local connectivity that's suggestive of a causal connection through time.

But by no means is there only one way to read this simultaneous figure.

And so that allows potentially for more of a figuring out, including a figuring out of rules.

Yes, it is indeed a little different with discrepancy at the intersection here.

Stephen?


SPEAKER_00:
Yeah, so this ties in with the lower bound evidence control approach of action.

So, you know, the idea is that action is what we can tractably approximate and perception is...

something that we can access but can't, and we can try to make more sense of, but is a harder piece of the equation to get a handle on.

So I'm wondering if that's... I mean, how do you feel about that declaration in terms of its use across other areas of applied active inference?

I think it's a...

it could make things a little bit more digestible for a number of contexts.


SPEAKER_03:
Okay.

Thinking about this in the context of recognizing that it's different than other action perception loops that we've seen, which is just really important to keep in mind that we're not perceiving something that we're projecting too much because other times the outgoing arrow from the entity is what?

If this were a Markov blanket type diagram, which they often are, the outgoing arrow would reflect active states and then the statistical dependencies that are outgoing.

But where is action?

It's on the bottom right.

So let's really try to understand why the pieces are placed this way.

The outgoing feature of the cognitive entity is the prediction.

And here's observation coming in.

If this were a Markov blanket diagram, we'd have the world and then the incoming statistical arrow would be the observation.

That would be the sensory states.

So here it's the outgoing prediction of the entity and the incoming sensory data or observation.

Those two are being differentiated to form a discrepancy.

which is just a qualitative term, but it could be then brought a little bit more formally into a prediction error or a little bit even beyond that into like a free energy differential.

This discrepancy has two arrows coming out of it.

From the discrepancy is arising perception and the changing of beliefs.

Does perception always involve changing beliefs?

And discrepancy is also giving rise to action.

So what kind of a thing is discrepancy such that the inputs are prediction and observation and the outputs are perception and action?

So at the very least, this is not the Bayesian graph representations that we've seen before or that we'll see

in just a few slides with a more traditional interpretation of nodes as random variables and edges as statistical dependencies.

This is a little bit more like a thought map

that then connects to the variational free energy equation which we talked about in number 37 a lot more but just to recall there's the red and the blue lines these two different components of the variational free energy and those are shown again here so check out 37 to learn more about the red and the blue and about

variational and expected free energy but for here we're starting with this action prediction discrepancy motif and then connecting it to perception and action as variational free energy minimization steven i think one thing that's useful with this um more nascent


SPEAKER_00:
representation is discrepancy can go in different directions.

So it can be discrepancy in terms of temporal occurrence.

When was the prediction predicted to happen?

When was it observed?

But it could also be what kind of prediction

and what kind of observation, or whereabouts was the prediction, whereabouts was the observation.

There's many different, and at different scales.

So there may be that at the kind of lower levels, there's quite a big jump between when I predict the baseball coming to my hand, when that prediction was made, when that observation occurs,

And also when that is chained up at different, slower, bigger steps of the nested Markov blanket sort of sequence.

So this idea of discrepancy is, I suppose, is probably the biggest bucket they could find, I would imagine, at that spot.

And that may be partly why it's there.


SPEAKER_01:
i just add to that stephen i think one of the things that's interesting is especially after just doing the 37 paper where we were talking about guides to me the timing of this was absolutely immaculate because now all of a sudden we've gone from guiding to almost taking potentially a referee's position

on the world, which is a whole different thing than taking up the position of being a guide.

And that was, I mean, that's not nuanced.

That's not subtle.

That's quite an identity shift.

And we're going to actually get into identity when we look at some of the evolutionary steps later on.

But I wanted to slow down on this because I thought that's not a minor change.

That's a whole different perspective shift.

And I think we need to really make note of that because I think it colors a lot of what's to follow.


SPEAKER_03:
What makes you say that we're a referee in this situation?


SPEAKER_01:
Discrepancy.

There's a difference between what the world is telling us and then how we rule on that.

So think of any game where you get into an argument with the refs.

Are they wrong?

Are you wrong?

It doesn't really matter.

The point is that there's a difference.

That's when you said differential graphing is maybe one of the ways to be able to make that explicit.

But that's not how it's typically framed out when we're talking about Bayesian or Markovian stuff.

So I think this is a big move.

I know it might seem like a tiny one, but I think as we go, again, as we go deeper into this paper, it affirms a lot of the stuff that I actually saw when you're trying to go out there and forage and figure.

So I'm excited to keep going.


SPEAKER_03:
Yeah, it could be like the observation, the baseball player is running towards the base and the coach is observing and there's this ongoing prediction and observation with no discrepancy because part of the generative model includes the person moving through time.

And then,

Given the observation and the generative model, the coach predicts slash expects and prefers, which we'll come to again later with the three Ps, that the baseball player is safe.

You rarely see super animated refusals when a call has gone towards somebody, but it's when it has gone against, it's violated their fitness, that there's a discrepancy.

with what the referee has called and what the interested coach has called, and that is going to lead to some consequences.

And again, we're still not at the Bayesian graph level, but that's going to be the next figure.

Steven?


SPEAKER_00:
This can then build on...

the idea of what's cognitive, what can you have a perspective on?

So I think what you're saying there with this referee position is as well as the kind of the swarming dynamics and the kind of inactive processes, much of which is beyond our ability to sense or integrate,

what is it once it starts to hit what's the big bucket at the kind of cognition level that still isn't too much of a of an inflation so in this case here discrepancy there is a sense of being able to distinguish perception and action at some level and find a discrepancy which in some ways

I would imagine covers a lot of what cognition would require to be thought of in a cognition way.

Other types of action, for instance, how we heal or grow, maybe action and perception won't be so separated, but this is trying to come at the idea of cognition.

So maybe that also informs this process.


SPEAKER_03:
Cool, so section two, again, was just about the two concepts of predictive regulation and control.

That's what we saw here, predictive and control.

So it's just conceptually laid out.

In figure two, they formalized brain design as structured learning in generative models.

So here we have a different figure.

We still have the cognitive entity and the world.

We add in one layer of absolutely essential active terms, which is generative model, generative process.

The generative model is the cognitive entity's model of.

generative process is the underlying phenomena that gives rise to observations it's the difference between the cognitive model of vision a generative model of vision and the generative process of visual input which is like photons and the sun and all of that so these are very different they're complementary

but just so that we're really clear going forward that we're going to be using those in their specific sense, not using them like, oh, well, it's a generative model because this is a model of cognition that makes me excited and think of ideas.

That's not how we're using generative model here.

So just to be clear on that.

Now we see action as changing the world represented by you, and the cognition involves partially action selection, policy selection, planning as inference, but not going to all those details yet, which can have some influence on actual unobserved hidden states of the world, X star.

x is the cognitive model of that hidden state of the world that is being inferred and y is the observation that then feeds back into the cognitive model so that's what these nodes mean it's about partitioning the cognitive model

from the generative process.

Separating the generative model from the generative process, and we're starting to see the traditional blanket form with observations having incoming statistical dependencies and actions having outgoing statistical dependencies.

What else do either of you see in this model?

Yeah, Dean?

Let's see.


SPEAKER_01:
Let's go back to being a referee for a second.

The assumption is that you're already attending.

And most of these models that we have taken up in the past, especially in 37, what they try to focus on is getting from A to B.

what this is essentially saying is, let's add something to that translation, right, from A to B. Whatever we're trying to incorporate in this representation, let's maybe look at the interpretation part of it now and the recitation part of it.

So if I'm the umpire, and we can all be umpires, we don't have to do that just in a baseball game, this is incorporating now a certain critical process

I was attending and I saw this, but then there were 60,000 other people also paying attention and they saw that.

That's where I think this is getting really interesting now.

This is actually bringing it back to sort of, I still think it's an instrumental piece, but I think now we're going to incorporate some of the reality.

And were the 60,000 wrong?

Or is the one person who yelled out safe wrong?

That's what I see in this, because that you is definitely embedded in the generative process, not the generative model.

Okay, thanks, Dean.

Stephen?


SPEAKER_00:
So the question that comes into mind is where the body is in all of this.

And I sense that

The generative process is the bigger process with the world.

The generative model gives a way to access those priors and to give away the hidden states.

And the cognitive model, generally speaking, cognition is thought of more in terms of deductive and inductive reasoning and logic.

And it could be that there's elements of the abductive that could be held within the kind of body.

And I suppose there's a question there to how and where that is.

I don't think anyone quite knows the answer to that, but that's my thought.


SPEAKER_03:
So let's remember that this partitioning

is specific to a given instantiation of model-based science, just like Majeed was talking about.

So we're not going around and assigning aspects or phenomena of the world into either generative model or generative process.

So where does the body fit in?

Well, with respect to a model of the body being a structurally real thing that gives rise to observations, it's a generative process.

looking at the coin from the other side and thinking of the body as a generative model of its niche doing inference on certain things or acting as if in that sense it's generative model so different kinds of entities are not going to be just assigned simply to one side or the other it's going to come down to what is specifically being discussed

There's a few other, not complexifiers, but first off, just note that the notation here is not the same that's used elsewhere.

So we will move towards better and cleaner or reformatable notation, but like X star as an external state and X as an internal state might be clearer for some people.

It also carries a little bit of a baggage that the hidden state is exactly what is being inferred about the external world.

Like there's a temperature parameter in the brain and then there's a temperature parameter outside in the world.

as we explored in the representations paper, it doesn't necessarily have to be that way.

There could be a hidden state internally having to do with movement left or right, and then there's a temperature variable outside, and then the model, the generative model of the cognizer is about movement conditioned on temperature observations, but not necessarily simply a thermometer being instantiated in the head.

It's like good to look at this graphically and think about what is being connected to what without worrying too much about all of these side questions, but this is what they're setting up as the basis of their further discussion, which is it's about prediction and control.

And we can use Bayesian graphical approaches to represent that.

Steven.


SPEAKER_00:
And that inferred state, as you mentioned, in some ways that's always slightly hidden from us.

What does that mean to be an inferred state in terms of is that the cognition that's coming out of that in some sort of deductive, or can it be an effective sense of how well something's going?

So again, it's not directly asked here, but

Accessing that is one of the big problems that happens is how do you access what has been inferred when it's not necessarily something you can access through sort of direct reporting from a subject matter or from a participant.


SPEAKER_03:
Another example of that might be somebody who has skilled action with respect to investing, but they may not be able to give an estimate for a number that a certain asset is expected to be at.

Because it's not like they're doing the asset price prediction and then doing a strategy.

The cognitive model may have a very different structure.

So we explored it in the representations paper, but like there would be ways in which therefore that investment decision is not a representation of the actual stock market because it's not the same variable, but then the aboutness of the investment decision would be

representation with respect to what was happening in the generative process so those are some of the like side avenues that we've looked at previously but they're all in play at once and so the question is just how to linearly structure to respect the specific contributions that are made here and the insight that can be gleaned without every single time

pulling back to some of these questions but it's great that we have like specific papers and memes and core terms that we can refer to and then carry on with what they actually contribute okay any thoughts on one or two before we get into the structure of the um allostat or the homeostat first i guess

Let's go.

Let's begin.

So we're going to go from this black and white, you know, Dorothy still in Kansas mode to some predictive motifs of ancestral brains.

And the three motifs, the red, green, and blue, are homeostasis, so returning to a set point, allostasis,

anticipating or approaching a set point in the future, which could be a fixed or changing one, and then implementing behavioral control, not just scalar homeostasis or allostasis on an interoceptive variable.

Okay.

Citation 20 in the paper, Chance et al.

from the future, March 22, is where to look for more details on the homeostatic formulation that they're using here.

But we can see it in terms of their figure three.

Okay, so the left side, just for reference, there's figure two, so that we can remember the structure of perception, cognition, action, impact that the authors are working with here.

And we're going to connect this black and white figure two to figure 3A.

This is a graphical model, graphical in both senses, meaning visual, like we're perceiving it through computer graphics, and graphical meaning like a network, so nodes and edges, because there's computer graphics that aren't network topologies.

But this happens to be a computer graphic that we're perceiving visually that also is reflecting a graph in terms of nodes and connected edges.

It's a generative model for the regulation of a single interoceptive variable.

So here's A and B with the homeostat and C we split out to talk about later, but first A and B. This generative model includes an interoceptive thermoreceptor and a belief about body temperature.

The prior over X, which is body temperature, is kept fixed, and hence it acts as a cybernetic set point.

Any discrepancy between the predictive thermoreceptor activity given beliefs about X, so Y conditioned on X, and the measured Y is registered as a prediction error that is canceled out by an autonomic response U, for example, a thermoregulatory response.

So hidden state,

On temperature, beliefs about temperature.

Why?

The thermometer.

Perception.

And then there's the selection of action.

So some sort of like vasodilation or thermoregulatory response.

And then that's going to change the underlying unobserved true temperature.

But that's not...

needing to be shown so any comments on that first part we're looking just at the top half of 3a and connecting x y to um you action can you still hear me because i think i've


SPEAKER_01:
Can you take the cursor now and just reinforce the feedback and the feed forward part of this?

Because the authors spent a bit, I don't want it sort of unpacked like they have at the figure three unpacking that we have in the bottom of the slide.

Can you just run the cursor over all the examples of feedback and feed forward going on concurrently?

Because I think that's really important to see that it's happening in both directions at once.


SPEAKER_03:
Let's label everything and then definitely can do that.


SPEAKER_01:
Perfect.

Thank you.


SPEAKER_03:
Okay.

So light blue is action.

These are subtracted, so red circles represent expected values of x, the red circles, which are used to make predictions about y. These are subtracted to form a prediction error.


SPEAKER_01:
Because those lines with the arrows on the end of them are just dependencies.

They don't really show both the feedback and the feed forward.


SPEAKER_03:
Yes, agreed.

Like having some rounded...

edges and other um directed edges let's see whether it's whether they're like kind or not um let's start with action action influences the state of the world which an edge can be drawn

to how that changes the state of the thermoreceptor.


SPEAKER_01:
I thought Y here was the observation.


SPEAKER_03:
Yeah.

That's the state of the thermoreceptor is the observation.


SPEAKER_01:
Okay.


SPEAKER_03:
Yeah.

So action changes the observation, the state of the thermoreceptor, which is being contrasted with the belief about temperature.


SPEAKER_02:
Yes.


SPEAKER_03:
From there, a prediction error

is generated.

It could be zero if there's no difference, or it could be higher.


SPEAKER_01:
Can you just show with a green, some colored line that has an arrow on each end, a connection that demonstrates within that diagram, the feedback and the feed forward at once.

Is that possible using this diagram?


SPEAKER_03:
Let's see.


SPEAKER_01:
This representation, so a single arrow or a single line that has an arrow at both ends is a different color, so we've superimposed it over this, but that it shows that there's a feed-back, feed-forward loop going off at the same time.


SPEAKER_03:
Okay, so here's the temperature information measurement flowing in to contrast with the beliefs.

And this is, it is in general really important to label the edges, not just use color coding.

We'll let it roll for now.

The information is flowing from the measurement to the belief.

And then that gives us the prediction error.

Then the prediction error is used in the selection of action, which then influences future observations.

Now the red circles represent expected values of X. I'm actually not sure what exactly the red bottom larger circle is meaning because the prediction error.


SPEAKER_01:
Well, there has to be some way of representing that discrepancy, right?

So they needed the second ball on the bottom to show difference.

Between prediction error and expectation, I think that's all that's trying to show.


SPEAKER_03:
Yeah, or another possibility might be that the prior is staying fixed.


SPEAKER_01:
Correct.

That's X to the prior.

Yeah, yeah.


SPEAKER_03:
Yeah.

Priors can also... They can be flexible, but in this case, it's a fixed prior.

That's because we're talking about homeostasis.


UNKNOWN:
Yep.


SPEAKER_03:
Then...

the observations are diverging from the prior and the posterior is kind of like the realized perception, which is a compromise between the sensory data coming in and the prior.

And so, yes, there's a lot of degrees of freedom depending on how parameters are weighted.

This green line might approximate the red a lot more sharply.

We'd call that a weaker prior.

because sensory data updates the posterior to be more like it.

Or it could be the case that having a lot of observations different from your prior don't change it.

That's a strong prior where sensory data do not change it as much.

But then here is the prediction error in relationship to X. I don't know, we can look at the paper, but is mu

shown in an equation i don't have it copied out if it is let me look here while you're doing that yeah but that is the perhaps here okay um stephen anything yeah i'm just noting how the effectively the belief on temperature it's it flows through the thermosector


SPEAKER_00:
down into the to get the expectations prediction error so it's you know it's it's it's like you've shown there there is a a dynamic going down from the belief through to prediction error being mediated it's like the thermoceptor is kind of like a mediator between belief and prediction error on what to do for action


SPEAKER_01:
And another point, the reason why you can't find the Mew is because it's not actually pointed to in the description.


SPEAKER_03:
It's not in the caption.


SPEAKER_01:
Yeah, and I remember reading and looking and looking and looking and not being able to find it and then looping back up paper to go, okay, so how about feedback feed forward?


SPEAKER_03:
Now, there are cases where Mew is used to describe internal states.

That may be implicitly how it's being used, but it's super important that all variables are defined in a paper.

I wish they all had a table for every single variable and expression that were used.

Yeah, it would make the dot zeros easier, but also it would reduce uncertainty.

For example, is this epsilon even described?

Okay, Steven.


SPEAKER_00:
mean I suppose in a thermostat in some ways the the internal states of the bioelectric strip and in some ways it's kind of it holds the kind of the the way that the expectations of action can happen because it in some ways it dictates the way that

thermostat will behave it's uh in some sort of ways even if it's kind of a uh an analog uh process so i'm i'm thinking like where where would the kind of in a thermostat where would um i know this is a homeostat so it's a bit more sophisticated now but extrapolating that out you've you've kind of got a belief

And there's a belief of what something is, and then there's an expectation of what you can do about it.

For instance, my beliefs can go bigger than what I can do in my actions.

I could have beliefs about temperature, which exceeds where I could even exist or where I am able to change it.

It depends on the scenario.

So it's sort of held in both scenarios, in both the body


SPEAKER_03:
and the context and the kind of the the probabilities available okay so for a non-living thermostat it doesn't have a cognitive belief on temperature but there could be something that's computationally like that reflected by like just a digital prior on temperature and again these aren't cognitive personal affective experienced beliefs

Bayesian belief, this is just saying random variable reflecting in a model variable on temperature.

So even a sincerely held incorrect psychological belief is not the Bayesian belief.

They might coincide at times if it were a parameter, but the belief here being the prior,

It must be adaptive.

That's the evolutionary twist that actually helps resolve a lot of this because otherwise, right, the design space of all edges by all nodes and then any variable, I mean, it's just like saying, here's all the words in the dictionary.

And so evolution helps restrict the discussion to cases that actually do manage to achieve adaptive control.

Dean?


SPEAKER_01:
And this is where that translation from the silhouette of a head to a statistical density is assumed that the person who's following along with this just sees that.

But you can also see how easy it is to slip into the idea that, oh, wait a second, we've gone from the physical space and we just held on to the physical space when really now we're talking about a statistical density space.

And again, if you're not really, really careful,

You can see how people can carry forward something, but the actual thing that they're talking about has changed.

That's why the word discrepancy was such a big deal to me because I normally gloss over things, but this time it actually, I went, oh, okay, so there's going to be some moments here where we're actually talking about different things, even though in the continuum, we kind of think that they're talking about the same thing.

No, we're not.


SPEAKER_03:
yep it's um the travails of realism and instrumentalism for biological active inference episode 55 because are these terms are they an example or is this an example of a model being used to discuss a real system but this suffice to say is the architecture of the homeostat it undertakes action

to reduce discrepancy relative to a prior held belief, fixed in this case, about what temperatures are expected slash preferred.

The dialectic of the first two Ps from livestream number 37.

This is an expectation and a preference because expectations having to do with survival are as good as preferences for survival over evolutionary time.

Okay.

This is going to be contrasted with figure 3b, which is the Allostat.

And so we see the same stack of X1, Y1, light blue, dark blue, red, except there's now a second column next to it, and there's some cross connectivity.

So again, the Xs are going to be beliefs about, so priors on.

then these are observations and so they're saying this generative model extends the homeostat by including a second set of exteroceptive variables that correspond to light intensity y2 observations of light and beliefs about sunrise so like beliefs about the generative process so generative model of the generative process

Furthermore, the model includes a predictive relationship between sunrise and body temperature .

So, again, I wish we could label every edge because the edges are meaning different things even at different times.

Like, this is a predictive relationship, so it's an anticipatory one.

then the one about light intensity and the belief about sunrise there would be ways to make that an anticipatory or an anticip or an instantaneous relationship um but the result of this sketched architecture is that inferring a sunrise which will only happen with high posterior confidence if the visual observations are consistent so low prediction error

Inferring a sunrise, not quote, seeing a sunrise, that is not what is happening in the model.

Inferring a sunrise and finding that visual observations are compatible with it can trigger the autonomic response, the behavior, you,

of thermoregulation in an anticipatory manner that is before sunlight actually increases body temperature well how would the parameter be set that way because if that were adaptive then other parameter combinations have been weeded out already by evolution so that is where we tuck the thread back into the ball of yarn which is the parameter combinations that are non-adaptive die

They dissipate.

They fail to exist.

They're not going to be measured as things in the future, empirically, however you want to take it.

So the quote, like, how does it work?

It's the same as it was 50 years ago, which is it has to do with survival of the persistent.

Stephen?


SPEAKER_00:
And these graphs, what they're used for as well is they do show beliefs tied to...

the observation space, the space available around the observation, you know, whatever the sensorium that is available, that's what gives the beliefs its scope.

And then the expectations are tied in here more clearly to action.

So the errors, the errors or the

What's important, the actual dynamic that's driving action to change something is coming out of the prediction error that's feeding into action.

I think that's quite important because this is like the proto-animal.

This is like the proto kind of piece here that ties into a lot of how

we think about knowledge and meaning.

And I'm not saying that, I'm just extrapolating a lot, but I think it does show how beliefs are tied to the type of sensorium that is being used to shape the observation space.

And I think that's quite useful.


SPEAKER_03:
Yeah, and these are just, yeah, Dean, first go ahead.


SPEAKER_01:
I just want to get both your guys' opinions about that orange barbell at the bottom of the diagram.

Because we're talking identity, but we're also still talking about dependency.

We're talking about duplication.

We're talking about anticipation.

So that's a lot of stuff packed in one orange barbell.

What do you think?


SPEAKER_03:
Yeah, I'm going back to the full caption.

the red circles represent the expected values of x so if we're interpreting these as the posteriors on x because the expectations the priors are the top blue ones so it wouldn't have to be the red circles and they don't mention the word orange

Note the lateral... It's there, and it's identity.

Note the lateral modulatory connections in the allostatic network C24 for details.

And 24 is the graphical brain with Frist and Parr and DeVries.

So yeah, again, this is just a sketch model.

They're not using it to fit any data or even simulate any data.

And it does relate to what Steven said about the intersensory or the intermodal inference, which is one could imagine a cognitive model where if there's noises that are associated with sunrise,

Then beliefs about sunrise can be a variable with edges coming from different sensory modalities.

So it gives us a separation of the organs of sense and internal cognitive modeling.

Steven?


SPEAKER_00:
Yeah, it gives a sense of when would something be important to act upon.

So there's many, many things that I could believe I'm seeing or being perceived in terms of this is a sunrise or this is the type of light coming in.

This is the nature of the light.

This is where the light's coming from.

There's all sorts of stuff.

The stuff that's really filtering down is what is...

important and this is where the affordance is coming what affords me to make some action

If it was expanding out, there can be lots of things I notice.

I might notice that there's a line there, and I may notice and believe it's orange, but I may not act upon it until I take my attention to it and think it's useful, as there's many things going on on the page.

So when we have beliefs, but then there's what's being acted upon out of all of that to then create some sort of change.

So I think that...

is also quite useful in this architecture.


SPEAKER_01:
Cool.

Can I add to this?

Because I think it's really important.

So we're talking about in the context of a sun coming up.

But let's see whether it still is true, and I think it is, just to prove a point, whether that orange barbell still makes sense if we're talking about Daniel anticipating 61-year-old Daniel and Stephen anticipating 61-year-old Stephen and Dean anticipating 61-year-old Dean.

Now, I'm much closer to 61 than the two of you, but do I need the Monte Carlo?

Do I need the play out?

to be able to make that anticipation as long as I have the identity and the parallelism and the backwards and forwards looking dependency that we see between X1 and X2 and Y1 and Y2.

That's what I think makes this a very interesting way of being able to sort of build on the previous homeostatic example.


SPEAKER_00:
Thanks.

Stephen?

Yeah, I think also this speaks to traditionally we think about the meaning.

What does this mean?

What's all the meaning out there?

As you mentioned, all the data, the stuff that it could mean to relate all these variables.

But ultimately, and the bit that is actually what active inference gives us is that barbell is where the meaning comes in.

It's like the way to know...

how being 61 is for you, Dean, is for you to imagine what it would be like to make choices and to act in the world as a 61-year-old Dean, not necessarily to go out and look at all the

the data, so to speak, all the beliefs and the things we think about the world, but to actually bring that in.

And of course, you've got a better chance to do that because you're closer to being that kind of dean.

So I think that's quite interesting, that barbell at the bottom in terms of pairing together the kind of beliefs about what actions and prediction areas are available.

And it's more into that meaningful action, meaningfulness,

realm, rather than what does it mean?

What's the data tell us?


SPEAKER_01:
We might get there, and I'm not pushing back on what you're saying, Stephen.

We might get there.

But I think for now, what we're saying is that in order to get there, we have to have more than a single stack of dependencies.

We have to have a double stack in order to be able to extend the homeostatic nature of how we've evolved to where we are.

I don't know.

Again, I'm not pushing back on you.

I'm just saying I don't want to jump that

I don't want to jump that firing pistol just yet because I think that orange barbell is going to turn into a green arrow and then all hell is going to break loose anyway.

So we should carry on.

I don't want to take Stephen's stuff away, but I think I want to park it because we're going to go somewhere in a minute.


SPEAKER_03:
I'm hesitant to ascribe too much specificity to something that wasn't labeled in the caption, doesn't have a clear labeling in the figure either, but I think it is already demonstrated that using graphical frameworks and partitionings like we have in Active Inference, we can start to approach some of these questions, like how would different kinds of variables be connected

How does that relate to future inference on action, counterfactuals, et cetera?

Okay, so these sections four, five, six, seven, were just laying out the relationship between an evolutionary or a physiological function like homeostasis, allostasis, or simple behavioral control, models of things that are core evolutionary features and functions,

connecting them to the kinds of graphical representations, which are like a little bit of a hybrid of a Bayesian graph and a factor graph, because there's some kind of computations being implied here.

Whereas in a Bayesian graph, the edges only reflect statistical dependencies.

Whereas we're bringing in a little bit more of like a nuanced type relationship that one could imagine

could be unpacked a bit more were it to be specified.

But it's just to show how different graphical models relate to different functions.

Architectural underpinnings, whether you think this is the actual architecture or whether it's just an instrumental architecture, like a model structure on a given phenomena,

how do model architectures relate to evolutionary functions where is evolutionary time in this model we started out by asking like how do cognitive phenotypes arise specifically over evolutionary but also over developmental time and that's where we get to one of the main pieces and contributions of the paper which is figure four

the five main dimensions of elaboration of generative models introduced in this paper.

So there's other kinds of changes and ways that this structure could evolve, but they're going to focus on these five.

There's starting with the homeostat.

Does it have to be our starting point?

Maybe not.

starting with a homeostat there's the i operation which is identity unchanged there's i plus i which is just a parallel isolated duplication that's like from one photoreceptor to going to two photoreceptors so to speak there's the allostat which is a duplication as well as a cross-linking

So these are just sketches.

One could also probably write that one as like a duplication followed by a linking, and there's no like change in linking, for example, here.

So this is like a few of a taxonomy of operations.

There's increases in temporal depth within a level of the hierarchy.

So looking one more unit further in time,

at a given time scale.

And then there's hierarchical nesting with an H. And the figure is shown like this because these are like the things that can happen to the homeostat.

And then they can have a second round and so on and so on.

there's so many other ones that could happen.

Like where is reduction?

Where's loss?

Where's deduplication?

Where's the uncoupling of the allostat?

Reduction of temporal depth, reduction of hierarchy.

So there's a broad space, but one of the main contributions of this paper is to connect functionally oriented graphical models of homeostatic and physiological function to

operations that result in the elaboration of simpler model architectures into different architectures.


SPEAKER_01:
This is an interesting part.

I know Stephen wants to say something too, but really quickly, there's no plus minus multiply and divide.

There's an operation, but there, there's no symbolism for that.

And again, again,

that keeps it safe on a statistical level.

I think that's a nice tell there.

And I want to, again, for somebody who's just sort of looking at this for the first time, pointing that out, it's not there.

And because it's not present, that tells us something.

To your point about maybe how do we get deduplication?


SPEAKER_03:
Yes, it's... Well, I plus I, it is like... It's a...

suggestive use of the addition operator, but we're not adding these just like integers.

So yes, they're kind of like categorical operations that constitute this evolutionary algebra.

Okay, Steven, anything on figure four?


SPEAKER_00:
Yeah, I was just saying that they've set a kind of a paradigm from the homeostat and they've taken that through and they've shown that it plausibly can

carry through to cognition.

And then, like you say, there can be other ways, but what they've now given is they've given some legs another way to think of the legs that can be attached to that paradigm.

Because this modeling approach, while they could have chosen a lot of others, by taking it, there are certain things which are hard-baked into that early stage choice-making, which then makes other paradigms wouldn't fit with it, right?

You know, even if you go instrumental or realist,


SPEAKER_03:
know your instrumental models your realist models don't make sense unless you with this unless you take in some of the paradigms that this structuring creates yeah agreed thanks so just maybe think like is this functional model realism like the function of the model is what is being tracked through time and so that

there's many other alterations that could occur.

And then there's the even more granular, like parametric changes or changes in connectivity or edge types.

And then I think, as you said, Dean, when all hell breaks loose.

So then it just, what's interesting here, let's look at the Allostat in a little bit closer detail.

Okay, so previously we looked at the allostat and we were like, okay, there's an orange bar.

Now, the allostat has a green bar.


SPEAKER_01:
It's a green arrow and it's a unidirectional one.


SPEAKER_03:
This one... The dependency.

The allostat...

has the green bar.


SPEAKER_01:
Oh, okay.

Well, those are my bad eyes.

Cause I saw, I saw the little arrow from the blue actually turning.

I didn't actually see that there was two circles.

Thank you.

There's my 60 year old eyes for you.


SPEAKER_03:
Yeah.

okay cool um and so each of these are in quite a different domain um or at least like a different aspect of cognition and these are the transitions that it can engage in so like duplication retains the exact same function but it's a very important step and that's a little bit what i explored in the dot zero with genetic duplications like on the left side here at the screen

having a region of the genome duplicate, yes, one of the duplicate copies can just degenerate.

That may be like opening up space or creating motifs for other kinds of later evolutionary steps.

So even this is not like a failure, it's just a change.

There's neo-functionalization where then those two models can start to subspecify.

Like if you only have one photoreceptor, it has to be doing what it's doing.

But duplicating it,

then allows one to focus on a different wavelength than the other, for example.

And then there's sub-functionalization, which is where initially a composite function, AB, becomes two, like enzymes start to sub-specialize on one of the two functions of the ancestral.

So this is kind of like a role duplication.

Okay, so duplication is an important event.

then there's the allostat and multiple things happen with a but we're seeing it as there is a duplication occurring and there's also cross-linking occurring between beliefs in one modality or beliefs about one type of thing influencing observations um or in about another kind of thing as well as this lateral modulatory uh green orange

bar um we have and then we have two kinds of expansions into temporal depth a sort of local expansion with the t operation just pushing the model one step deeper and then the h hierarchical nesting operation which brings it to another time scale of analysis dean


SPEAKER_01:
And this is where I really struggled.

And that is when they were at Alistat, it wasn't just comparative.

I felt like you had to build in time, especially if we're talking about identity.

But then it was like, no, we're just going to leave it comparative.

And then we're going to introduce the when-ness of this.

Now, I mean, I understand why they're doing that to try to make it tractable.

But I don't know how they can say it can be comparative, meaning it's

I know the difference between A and B right now, but not sort of address the fact that in order to differentiate into A and B, some amount of time had to have been applied to that split.

So I was saying you have to go back in time in order to just be able to do a comparison.

So it's temporal only going forward in time.

I don't mean all hell broke loose, but I mean, I did ask this question.


SPEAKER_03:
you're right like here there's um there's the triggering in an anticipatory manner and so in one view it's like but this is a single time step model so how could that occur because it doesn't have a temporal depth

on the other hand again like kind of that those two senses of representation in the structural sense if the model is a single time step model it cannot have a representation through deep time ergo it cannot be anticipatory functionally right the other side of the coin would be if it's enough at this time step such that beliefs about sunrise trigger and a response

the model can be functionally anticipatory without a temporal depth representation.

Which is why the representation discussion was very important leading up to this one, because we're all over those eight quadrants here.

Because in evolution, and this is something that like cognitive psych people talk about and is like,

our cognition being shaped towards effective which is to say productive reproductive success not towards like ultimate truth discovery and so should we be too surprised that our instrumentation which has had a certain objective function is then susceptible to

being convinced about things that are not true.


SPEAKER_01:
Well, I don't socialize with those people, so I don't know.

I just knew that when I was looking at it, I was thinking in real practical terms, what is built into this in order for it to still make sense?

And that wasn't necessarily parsed out in the diagram.

You kind of had to fill that in through your own interpretation of what was going on.

So thus, back to discrepancy.

That discrepancy thing is going to come up again and again.

And I'm really glad that they inserted it at the beginning.


SPEAKER_00:
Thanks, Steven.

I think it's useful here to hear this temporal depth and hierarchical depth being sort of spelt out.

because I think sometimes it's a little confusing in other papers quite where it fits.

So if temporal depth, and this again might be where some of this ontology work can be useful, just to sort of see if that can be kind of consistent.

But if temporal depth is, you know, you've got variables for past, present, and future states, but they're operating maybe within the same kind of dynamics,

And then if you've got different temporal rates operating simultaneously, then you get that hierarchical depth.

And I thought in the past, I'd sometimes thought temporal depth would have covered that more hierarchical depth.

So this is interesting to think about whether this might be actually the way that that differentiation is best made.


SPEAKER_03:
Differentiation is probably best made with reference to a specific model.

And this gives us the design language and the grammar and the motifs.

So if we want to predict 100 years in the future, should we have a decade model with 10 year models?

That's a times depth of 10 decades, and then a nested model with a depth of 10 years.

Or will we have a one-layer model with 100?

They're very different.

And it's not to say that one is more accurate or less.

And depending on how the exact situation is set up, maybe the computational requirements of one are higher or different than the other.

But that's the discussion to have.

Do we want to have a B transition matrix?

Let's look at a nested model with temporal depth.

So here...

we have on the top level of the model s state two this is temporal depth happening at the upper level of the model three discrete time points in an upper level that is temporal depth there's also nesting within each um time step

there's a cognitive rollout on three time steps this is one type of nested model but there's other nestings that can exist so here we see a depth of two and then at each level of the nesting there's three temporal depth but they're totally different in architecture and in function so it is the interesting discussion like

especially when we have to do multi-scale prediction, like Fermi estimation.

So if the prediction is within one timescale,

there isn't necessarily a need to nest.

But once we get into larger timescales or where there's recurrence over multiple timescales, it makes sense to have a nesting model.

Like if we wanted to predict someone's activity over the next seven days, it might make sense to have a day model with a nested model inside of that.

because then within each day there could be parameter setting versus just trying to fit one time series and find the parameters that make that one time series oscillate in a circadian way instead we just need a day model with a very specific kind of simple transition and then a transition at a nested level

with a depth that's measured at the hourly or the minute time scale rather than the days.

And that becomes especially important when doing family-based model fitting like variational inference.

So we're not just getting all the data points and fitting a spline through them, but we actually need to have appropriate model structure.

Otherwise, free energy minimization will drive right off a cliff, just like we talked about with Axles bacterium.

So if we want to do free energy minimization and model selection on model structures, we can find the best model relatively easily given the families that we've specified.

But there's no guarantees once we step outside of that spotlight that we're even in the right category structurally.

And so that's why humans having these design motifs in mind and many, many, many more

helps us not get into what seems to be a global optimization based upon free energy minimization, but actually is a very relatively local optimization based upon unimaginative model structure learning.

Steven.


SPEAKER_00:
Yeah, thanks.

That's really helpful.

And it also ties into something I've been really thinking about in terms of the model in itself is these scales, these steps, you know, you're talking about the multi-year, decades,

When we're modeling, it's like, what do we have access to?

So we tend to ascribe that in terms of what can we externalize and what can we use to put something into a model if it's instrumental.

And then, of course, there's a question of, well, what is the realist perspective on that?

And what is there that's within our conscious awareness of being in the world, something that can be reported as an event, a story,

And what time scales are present there?

When you get into 100 years, it's beyond that.

So then you're saying, okay, what are you going to extrapolate?

What sort of temporal structure?

But on the other scale, if I'm asking what do I think I expect you to do next or Dean to do next, and it might seem intuitive, I can have a whole series of temporal and nested scales going on, which is not even available to me today.

in a cognitive conscious way.

I might have a sense, have a feel for what I think is going on, but a lot of my predictive processing will be happening at rates unavailable to my conscious awareness.

So that also ties in, I suppose, I'm not saying it's an answer, it sort of adds to the challenge, I suppose, but it is the challenge of, you know, how much ends up being what we can get a measure on to put into our model as much as what it is.

And that's the same challenge in a way that comes even from the realist perspective, at some level, as an organism, what is it that I even can access in some realistic way?

Not saying we can model it or no,

could be put into something like this.

Sorry, Dean, you were going to say something?


SPEAKER_01:
Well, all I was going to add is that, I mean, so the paper's talking about how do we use certain math to be able to understand maybe the evolution of cognition, right?

That's essentially what the paper's trying to give us, you know, point us in that direction.

So...

Again, in order to make this accessible, in order to lower the barrier for people who actually want to have this make sense, because they don't necessarily spend sums and sums of hours on trying to figure this stuff out, like the authors or maybe even us, I want to point out something that I think is obvious to us, but maybe not so obvious to people who are looking at this and going, I have no idea what they're saying.

I think first of all is that there's a chronology which we keep going back to, not just at the temporal depth level.

So that's relativity math.

There's an evolutionary aspect to this, so that's algebraic math.

And there's a dependency, so there's a statistical math.

Do we have to be polymath in order to be able to feel our way through this?

And I'm gonna go back to what I said at the end of the 37,

Carl Friston said, you don't have to be a polymath, but you better be at least somewhat comfortable, feeling comfortable with the math, because there's no one math that's going to get you here.

It's a blending of all of them, and then it's how you feel about that that's probably going to be the thing that lowers your

the impediment to really being able to use this now in real practical terms going forward.

So what's the next evolutionary step, right?

It's how you're able to turn this into something.

So I think that's one of the great gifts of this paper.

If you're not afraid of being, if you don't consider, if you don't identify now, or you don't duplicate now, or you don't allostat now as a polymath,

Maybe that's something we owe people that want to get into this.


SPEAKER_03:
Maybe the British pronunciation of maths helps reflect that there's already a plurality of maths.

And Kirby Erner, coming more from the synergetic side, talks a lot about this actually, like how the discourse around math as universal language makes it sound like math is a singular language when actually math is a pluriverse.

And so math reflects that a little better.

It's not like these are just sort of slopes on one mountain and that's Mount Math and it's so high and only a few people get to the top and scale every side.

This is just like we're using maths.

Yes.

Of all different kinds.

Stephen?


SPEAKER_00:
And often you see a lot of this in the papers that are published around active inference.

There's often a team of people, a group of people, where maybe one member has that higher level understanding of the math.

But the other question is to understand what would it mean to make an observation?

Now, the mathematician may not be the one.

Some of the work with Ryan Smith's work on the gut, it was like, well, they're going to do gut inference.

Well, how do you get some tractable source

on what's the gut doing.

So they created some electrode, I'm not quite sure how they did it, but they had some way of getting sensory information, effectively sensory information, or effectively some sort of information to put into their model.

And so knowing how action and observations

can flow in and the implications of that i think is also kind of a a big part of of this right and um in some cases it may be a trivial part because it's fairly obvious in other parts that may be the biggest barrier

And that may be the biggest barrier as well, often in organisms, right?

How do you even access, plausibly, the approximation science, which isn't so approximate that it's just chaos, that it's tractable?


SPEAKER_03:
Yeah, the rate-limiting step for impact and improvement in the real world is unlikely to be any single individual's conceptualization of math on a team.

if it's structured appropriately.

And for those pushing the frontiers of math, their understanding of math is quite literally a rate limiting step for them.

Or maybe it might be something very mundane like time availability.

But when it comes to thinking about real model-based science and translational applications of active inference,

I think that we're working towards new ways of combining skills and having shared knowledge resources that help make that make sense and using shared language and ontologies, narratives, formal documents, tools, ONFT, because we're doing it on Teams and we're online.

Let's just, in the last 20, 30 minutes,

go through the final pieces of the paper so that this dot one will have been like a first sweep, initial pheromone deposition, and then we can return and take some cul-de-sacs, et cetera.

So section nine explores a little bit how the duplication operation allows for multiple behaviors to arise.

And that's sort of by analogy to the genomic duplications and specializations and all those different routes that can occur.

Here, they connected duplication a little bit more directly to the factor graph models.

in the sense that duplicated motifs have dynamics that are conserved over different sensory motor domains.

So let's just say that we just had a visual model with the column of visual, and then we duplicated it.

So now we have two photoreceptors with parallel columns.

And now the photoreceptor in the second one changes into a chemoreceptor because instead of expressing rhodopsin protein, it's expressing an olfactory receptor protein.

can be a conservation of the dynamics of inference even when the observation has changed but it was a slot for observation and so we went from having like monocular vision to binocular vision to monocular vision and olfactory perception but you can't get there in one jump to just like duplicate and transpose

It's not likely to happen in an evolutionary context.

And we can see the structure of these graphical models as equivalent to being factorized probability distributions, which is to say that the sparse connectivity of variables

means that we can make parts of the model that can be like fine-tuned independently in a way that can be fit very tractably.

That's the factor graph, that's factorized Bayesian inference.

And we've talked about that in other places, but just to here, all we need to say is duplication enables like a control C, control V, copy, paste, and then let's edit the other version.

But that can now happen with evolutionary features and functions, Dean.


SPEAKER_01:
Real quick, your hand gestures in this section point to the orthogonal mesh beginning.

And then point two, I'd like to pull that apart a little bit, but let's carry on.

But that's what I took away from this.

The mesh began.

And in the beginning, there was a mesh.


SPEAKER_03:
Begins.

We'll return to that for dot two.


UNKNOWN:
Yeah.


SPEAKER_03:
Okay.

Section 10.

talks about temporal depth, it's about time, and about the endowing of generative models with temporal depth and the way that that supports prospective inference, anticipation, or retrospective inference, which is like memory.

So here's what the operator looks like.

We have X sub tau, X at a time point, and then now there's X tau plus one, the next time point.

or T plus one could be now, and then T could be the last moment.

So memory and expectation- Like figure five now?


SPEAKER_01:
Because I still see figure four.


SPEAKER_03:
Slide 29.

Okay.

Yeah.

So the structure of anticipation and of memory are very similar.

It just depends whether one has the stream of observations happening on the left side and a prospection or the stream of observations are on the right side and in which case it's retrospection.

Stephen?


SPEAKER_00:
Yeah, I mean, it sort of ties in.

They talk about what the police do.

It's very hard for someone to lie backwards.

So you always get them to tell their story backwards because it's a...

It's very hard for them to do that because, as you say, if we're creating things retrospectively but sort of playing them forward, so to speak, it would kind of tie in with that.


SPEAKER_01:
I want to emancipate that.


SPEAKER_03:
I looked up police in the paper.

I see policies.

is that polices um it's a great opportunity for my favorite joke but i won't um this section talks about temporal depth okay from whence temporal depth various researchers have speculated that a major driving force for the development of deep temporal models was foraging

And there's a lot of interesting empirical and conceptual reasons to think about foraging in terms of temporal and spatial depth of model.

And that's true in the vertebrates where they're discussing mainly like hippocampus and entorhinal system and in the invertebrates.

don't have a hippocampus entorhinal system but it's one reason why comparative neuroscience is so important because it prevents us from getting fixed on specific anatomical realizations of given functional attributes of evolutionary systems like if the story of memory is just about some brain region in humans

it may be a useful model it's not even to say that it's inaccurate it's not even to say that it's a partial model it just is a model of that whereas if we want to understand a given cognitive function in a broader context we have to pull back somewhere a little bit beyond or in complement to the anatomy because we need the empirical anatomy to have anything specific to be talking about but

It isn't just the case that vertebrate anatomy is the way to do active inference.

Stephen?


SPEAKER_00:
I think this is really helpful because it shows that the traps you were just mentioning there that people fall into is like

Humans have the frontal cortex.

It's all about this new, what's the new parts of the brain that are there?

But we don't say that with a Formula One car.

We say, well, it's just got a primitive engine thing in there and the rest of it's evolved.

It's like, well, that's itself different.

And the same with the Mark Soames conversation.

The evolution of what's often dismissed as the primitive parts of the brain,

They, they themselves can have been more sophisticated.

I know they're doing more things, but that they, they, they, they may be helped to be doing more than they were, as opposed to this, something else, um, that is doing all the heavy lifting, which I think is certainly common, a common misconception, I think in psychology anyway.


SPEAKER_01:
Can I read quickly from the paper?

Sure.

The evolution of temporally deep models from simpler models could have been realized during evolution via the progressive, keyword here, parcellization,

of an initially undifferentiated model, so a relative sense of an invariance, i.e.

a model that does not distinguish present from past and future, into a model that features separate latent states for the past, present, and future.

This is the part I loved.

A key drive for this factorization or parcellation may have been the observation and progressive internalization of the sensory motor sequences sequentially

that the animal creates and experiences while acting, while acting.

In other words, the self-modeling of one's own sequential behavior patterns.

See 53 for a computational example.

I didn't open up 53, but again, if we want to talk about the polymath piece of this and the fact that our hex cells

have to do some parcelization, it's right there in terms of sort of the next layer on top of this as we move up through that evolutionary cycle.


SPEAKER_03:
Yeah, 53 is Stoyanov et al., the hippocampal formation as a hierarchical generative model supporting generative replay and continual learning.

There's a ton that could be said about that from like a computational and a neuro anatomical perspective.

The internalization of the sensory motor sequences relates to the sensory motor detachment that we talked about in the representation paper.

So if some motor region in the brain

if it's like a marionette and the fingers and then the motor plant.

So that motor region has to be coupled to the activity of that motor region, like either in a one directional way or maybe even in a bi-directional way, let's just say.

So that system, like as it thinks, so it does and vice versa.

It cannot engage in counterfactuals because any,

direction that the neural system turns, the motor system is just simply doing that.

So it is not able to engage over evolutionary time in maladaptive action.

ones that do no longer persist it's just it's always what ties it back to reality and to like the finite amount of entities on the finite spaceship earth like darwin's famous calculation like if the number of elephants slowly reproducing like they'll cover the earth unless their population levels are kept in check and so um

once there's sensory motor detachment so there's some brain region either a motor region or some supplemental area or some ancillary area that's able to intervene in that process or somehow play a role that's detached from the motor activity now there can be like a motor planning occurring that opens up the affordance of temporal depth or of counterfactuals all these other cognitive functions arising

via the sensory motor detachment and so um foraging is an awesome place to look at that for a lot of reasons in different life forms and computational foraging and so like just a few of the notes were like what are the real cognitive demands of foraging for different creatures and

What about internal foraging, like mental foraging?

And these papers are very good.

Foraging in mind and foraging in semantic fields, both very useful because they have to do with the way that, and they have some nice maths too, but they have to do with how actions that are spatial can have conserved dynamics structurally

just like we explored here to mental actions.

And then we see them come together like with a memory palace or something like that.

So forging is cool, it's good to study.

11, endowing generative models with hierarchical depth affords multi-scale inference.

We kind of addressed that earlier with like the decades and years.

Nested temporal modeling is not the same as just deep temporal modeling.

then um and another way to look at that is like how many operations you would need to get to 100 years well if you're going to do only temporal depth it's going to take plus 100 or 99 time steps versus if it's one nesting and then 10 on the higher order than 10 on the lower order it was 21 operations and so then there's shorter


SPEAKER_00:
sequence of events to achieve a higher performance model assuming that they're appropriate um section 12 uh yeah go ahead just one thing just on that model and it's also interesting thinking about how we think of time because like indigenous approaches tends to be well as a cyclical time right so things in the future will be a repeat of the cycle

So the times of a thought in a cyclic motion of this sort of passing of the day, passing of the... And of course we have this assumption of time stretching out, you know?

So is it that we will, over time, is it thought of, well, what will things be like in four generations?

And, um, and if my world or my ecology follows certain cyclical patterns and it's fairly stable, maybe that's quite a good way to think about, um, you know, it's like say plant, plant the tree now to help the people in 150 years, you know?

So, um, it sort of comes back to there's different ways to construct how we think about time.


SPEAKER_02:
Cool.


SPEAKER_03:
so 12 looks into a phylogenetic tree of the evolution of generative model so what is a phylogenetic tree and what's the relationship between active inference fep and evolution well we'll have more to share in the coming papers as always but how do they address it specifically here in figure five

they map on, we'll root the tree.

So technically as shown right here, it's an unrooted tree.

There is an implicit rooting here, but it's a very interesting feature of phylogenetics that few outside of the field know about that computationally, the tree is often inferred in unrooted fashion, like by clustering the ones that are close and finding the relationships, et cetera.

However,

that is leaving an ambiguity as to where the root of the tree is.

So like, for example, if this is the overall connectivity of the, based upon the phylogenetic inference, this is what the tree topology has been inferred to look like.

And somebody might say, well, it looks like this one, the red one, and then the green one below it, they're clearly very closely related.

But actually if the root comes in here,

then that's not necessarily the case so the rooting of a phylogenetic tree is very important and a tree that's rooted inappropriately or has an inappropriate outgroup selection is just it's worse than illegible because it's highly legible and it can have a high statistical confidence but it can have an absolutely non-biological topology but assuming that they meant the tree to be rooted here

Here, the branches are where there's going to be different operators occurring, including the eye identity, unchanging one.

And then the edges reflect like bifurcations, speciation events.

So here there's an ancestral homeostat.

And then the lineage leading to orange did not undergo any changes that were non-conservative.

on the branch leading to these sister species there was a duplication and then it stayed the same and stayed the same and then the purple node is going to have two duplicated homeostats and so they're just overlying evolutionary algebra the steps that they gave onto the topology of speciation and so that um as they discuss

opens up a way to think from this sort of graphical functional perspective, which is compatible with ACT-INF, to think about how, for example, from the species today, we could infer some of the schema of the early vertebrate brain or the early primate brain.

Or push it back further.

What about the common ancestors of the vertebrates and the invertebrates?

And all these other questions.

But that's kind of what they lay out here.

And so these are familiar models to evolutionary biologists, like phylogenetic ancestral state reconstructions of phenotype, which are often even done in a Bayesian way.

Like there's an algorithm beast, Bayesian estimation of ancestral states.

This is also a Bayesian model.

model that allows us to do reconstruction of ancestral states but it's in a very very different way than it's been approached outside of the active world but that's figure five and that's where we see the evolutionary algebra the design patterns or the um pattern language for structural changes in generative model superimposed on species relatedness in a phylogenetic tree

Any thoughts on five?


SPEAKER_01:
I tried to convert this and reapply it to my picking of teams in the March Madness table, and I wasn't able to take phylogenetic trees and convert them into winning thousands of dollars on betting on college basketball.

So despite my best efforts, it's kind of contained for what it represents.


SPEAKER_03:
I mean, not to foreshadow too hard, but I think live stream number 39 and 40 might constitute March Madness.

And then also here's another, you know, for those who watch.

Okay, here, now we have a little bit of a March Madness bracket going, don't we?

Here's bracketology.


SPEAKER_01:
That is so fantastic.


SPEAKER_03:
So here we go.

Yeah, so now we have, now here, the root could have been here.

And then here's like two big species clades, like two, like this is like vertebrates and invertebrates, and then there's some rooted clade.

So that's like why it's important to root the tree because here it looks like all of these are sister to each other at the exclusion of this one.

But then if we were to have rooted that tree like here, then actually this is a small sister out group to all of these.

And so the rooting and the contextualizing

is really important.

It's just like, it's super interesting because it seems like you put in so much data into these phylogenetic models and it seems like literally how could this not converge on a super obvious answer?

I mean, isn't it clear how these answer related to each other phylogenetically?

But then there's a little bit of nuance that enters the picture.

Steven?


SPEAKER_00:
Yeah, I was curious that the allostatic can recombine with the homeostatic

to give a base level temporal.

So it doesn't deflate back to just staying allostatic.

I suppose they would be, they just don't show that.

If it did that, I suppose you just wouldn't talk about it.

You know, it's got A. When it goes A, then it goes A, I, goes to HT.

So, yeah, it's a TH.


SPEAKER_03:
Yeah, so we have a homeostat here.

We have two homeostats.

Two homeostats.

And now here we have one homeostat and one allostat in this blue node.

And so that...

but they're just kind of giving general examples like it would have been cool to see this is where we're talking about mammals and here's non-mammal vertebrates like here is a specific brain region and also that is in this Chakraborty and Jarvis 2015 paper this paper gets more into the nuances of neuroanatomy and neurogenetics

and how duplication in brain regions can be underpinned by developmental neurogenetic changes.

And they do link that a little bit more closely to studying the bird brain and the song and motor system in parrots, which is also cool because birdsong has been studied with Act-Inf.

So there could be a nice...

building upon this work and connecting it to some of the structure fitting in figure five.


SPEAKER_00:
So it could almost be like you got parts of the brain which are more allostatic orientated or homeostatic orientated and

So in a way, by the structuring of that gives some sort of differentiation in terms of how information and control would be carried out.


SPEAKER_03:
Yeah, totally.

And extending temporal depth within a model, it's like a brain region getting a little bigger, let's just say.

Not that size is correlated with function in that specific way, but then duplicating brain regions is like duplicating...

laterally or nesting a brain region that's hierarchical.

Think about how the eyes developed, the insect eye.

It has a relatively simple module, but then insect eyes can change in size over evolutionary time very radically from like taking up, you know, half of the head to being just one photoreceptor or even being lost.

because they're existing in more of a duplicable motif.

Whereas the binocular eye system that mammals have, it's not as amenable to like just doubling into four eyes or splitting into two in the middles or something like that.

Whereas the insect compound eye has like some of those evolutionary affordances.

So then understanding like parameter change within a model

then structure learning on populations of models and what are the mutational adjacencies there's so many awesome areas and it's like truly just beginning for evolutionary bio and active steven yeah it was really helpful like having someone like yourself who's got that background to go through that's quite helpful because it's a


SPEAKER_00:
It's a little bit intimidating, actually, at first sight, seeing this for me.

And I see the logic, what you're saying.

It's quite exciting actually seeing that.

Again, one of the things this paper does is it's tying threads together, which seem very distant, almost in traditional terms like,

well, I don't like using the word kamikaze, but it's a little bit like, yeah, it's a bit overwhelming often.

So I think it's, but of course that's the beauty of Act-Inf is it has that unifying potential.

So thanks.


SPEAKER_03:
Cool.

Yeah, thank you.

I agree, like there's some labelling

This is rarely included, unless it's a time-calibrated phylogenetic tree, but we're looking at the passage of time.

But that's not even always shown.

And so, yes, Dean, we'll have our final thoughts.


SPEAKER_01:
Can you just go back to that?

Because I didn't prompt you to put that up there, but I really like that because...

At some point, we're going to have to, in point two, talk about that sort of basal gating part of this.

And most things are organized as if-then causality, but you just said it.

It's then-if.

Go or don't go is very dependent on then before we choose to go or don't go, right?

Go or don't go is still held open until one of the others decided.

So

Another thank you for that.

Cause that again, that in the point two, we should maybe talk a little bit about that.


SPEAKER_03:
Cool.

Yeah.

There's so many ways to think about like this garden of forking paths, branching paths.

how that relates to parameter updating, Bayesian belief updating and structure learning.

What about when the structure of a model is a parameter in a nested model?

So then that sort of blurs the line from the bottom looking up, it looks like structure learning, but then from the top looking down, it looks like parameter fitting.

And if only we had like some mathematics to describe that.

Okay, any final thoughts or what are we excited about for The Dot 2 next week?


SPEAKER_00:
Well, my last thought is, yeah, I'm excited to keep this going because I think it's got a lot of legs to it.

I think one thing that comes to mind as well with all this progressively increased tempo and hierarchical depth

is is maybe where some of the more basic uh elements still have a really useful part to play is is what mark mike levin said about when do we stop like so we've got all these ways where we can start to okay so we've got all these ways that we can think how do we know when to stop and to sit down i mean maybe in the ultimate ways when our tummy maybe that's where the gut is so useful like at the end of the day the brain can go running off and maybe the gut and the heart need to say okay

sit down get some food so which i might well i'm going to get a drink of honey now but uh i'll bid you farewell and thank you for a great dark time but i'll i'll hear your last thoughts as well dean


SPEAKER_01:
Uh, I hate having the last word, so don't worry.

You'll have it.

And I just appreciate what Steven just said.

Cause I, I think that that's a big part in this ability to go from word back into the phenomenological space.

And, and I think there are other, other brains that we know of now that play a significant role in that, not just the one that turns everything into symbols.


SPEAKER_03:
Great.

My last thought is there's no better drink after a foraging trip than honey.

All right.

Thank you.

And talk to you later.

Bye.


SPEAKER_00:
Bye.
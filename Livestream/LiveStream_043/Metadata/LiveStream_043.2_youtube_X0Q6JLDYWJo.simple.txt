SPEAKER_02:
Hello everyone.

Welcome to ACT-INF Lab live stream number 43.2.

It is May 11th, 2022.

Welcome to the ACT-INF Lab.

We're a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at the links on this slide.

This is a recorded and archived live stream, so please provide us with feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll follow good video etiquette for live streams.

If you want to learn more about what's happening at ActInfLab, greetings.

Head over to actininference.org, and you can find out more about participating on live streams or other modes of participation.

And today we're in 43.2, where as always, it's up to the last minute with what we actually do.

And welcome also to Stephen.

We're going to continue this discussion in our third part of the conversations we've been having on this paper, predictive coding, a theoretical and experimental review.

And we'll begin with the introductions and then we can

move around the paper we brought up some things that we'd like to cover last week for today but also we can take any questions or areas or sections that any of them want to talk more about we can absolutely go there so we'll just start with introductions and say hi and feel free to

add anything you'd like, what brought you to the paper or what's just one thing that you took away from it.

And then I'm sure we'll have some fun today.

So I'm Daniel, and I'm a researcher in California.

And I'll pass it to Dean.

How goes it Dean?

sorry i was muted in jitsi so it was on the live stream but welcome to dean and steven and uh i'll also share the slides with you here in the chat but we're live and in the game so feel free to say hello and anything um uh just to get us going with this discussion i'm getting a bunch of feedback on from somewhere daniel let's go it's going good um

I'll open up another window so that you can see what slides I'm working on.

But yeah, how goes it?

Or what was interesting to you about this paper?

Do you hear me, Dean?


SPEAKER_03:
That's me.


SPEAKER_01:
okay all right all right okay i can hear you now yeah i could hear you i couldn't hear you before something was weird sound fine dean i can hear you daniel but there's a long delay and uh there was a i can hear i could hear feedback like a second audio loop going off while you were talking okay did you have the stream open or anything

I just, no, I just have the picture of the three of us.


SPEAKER_02:
Okay, I'll just reload just to be sure.

So it goes with live streams.

All right.

Okay, so is that better?

Well...

roll with it then.

So, okay.

Or we'll see what happens.

So yeah, Stephen, how goes it?

And what did you like about the paper as we start?


SPEAKER_00:
Well, I'm just interested in the general scope as per normal.

So I'm sort of going to go with the overall vibe of pushing the boundaries of active inference.

So I'm fairly relaxed with this one.

Okay.

Hey, Dean.

Work better?


SPEAKER_01:
Yeah, my bad.


SPEAKER_02:
All good.

All good.


SPEAKER_01:
Thank you for telling me not to have both GHC and the YouTube open at the same time.

Yes.

These little technical things that I should have been more on top of.


SPEAKER_02:
there's like some reflex where if your own audio is played back to you at a certain delay it makes it like basically implausible to speak it's pretty interesting yeah well apologies guys that's not the start i wanted all good so how goes it or what like brought to you to this paper or what did you find interesting in what they wrote well


SPEAKER_01:
I'm not really totally up on what predictive coding is.

I think I know what predictive processing is.

And when you and Maria were talking in the .0 or you or Blue were talking in the .1, just sort of the translation of that into operations that you can...

identify with a certain amount of confidence is the part that i'm really interested in i'm not really sure what the applications are beyond the psychology realm i'm sure there are quite a few and that's kind of why i showed up today was to sort of piggyback on what what people who obviously know a lot more about this than i do are saying about it and where it's going and what the potentials of it are all right well let's start with a few


SPEAKER_02:
reminders and recalls.

Is there any area or figure or formalism that either of you would like to suggest that we jump to to remind ourselves of anything critical about the paper?

Otherwise, I can bring this to some.


SPEAKER_01:
Is it okay to start with the back propagation of error?


SPEAKER_02:
Yeah.

All right.

So I'll bring in, this was in section four.

And so I'll be bringing in some, but let's, we're gonna ramp into it just as a reminder about the roadmap.

So the first section of the paper is an introduction.

The second section of the paper describes with quite a lot of detail, the core kernel of the predictive coding architecture.

This is a computational architecture and it involves information transmission,

That is in this predictive coding way.

So check out the paper the dot zero the dot one That's where we're going into a lot of detail on predictive coding as an information architecture and then as Maria was

bringing out for us, predictive processing might be used to refer perhaps more holistically to systems engaging in predictive coding.

And there's a bidirectionality there to predictive processing, whereas predictive coding might be something that could be done in a bit more of a uni-directional way.

That was section two on predictive coding.

introducing the kernel, and then going into a few specific generalizations of that kernel and modifications that help make it more computationally powerful, as well as linking it closer to certain biological architectures of tissues, like in the retina and in the cortex of the mammal brain.

in predictive coding in the brain.

Three, there's some exploration on different paradigms of predictive coding.

And these are referring to kind of like settings where the core and elaborations can be applied.

And that's in the supervised and unsupervised cases, as well as thinking about spatial and temporal predictive coding algorithms and connections with modern machine learning, like with deep predictive coding.

Then after that sort of kernel and generalization of the predictive coding kernel and exploring some of the paradigms for where that kernel and its elaborations have been applied in section four, there's the relationships between and among the algorithms.

And so that's where we get into this section on predictive coding and the back propagation of error.

So that's where we'll go.

And

I'll copy in some formalisms or find out which ones would be the best to bring in.

But here we are on slide 56, which I'll share in the chat here.

What is interesting to you about backpropagation of error?

And then we'll start there.


SPEAKER_01:
Well, what I'm curious about is that bottom line of that slide.

This recursion is identical to that of the propagation.

to the propagation of error algorithm, which I'd never actually encountered before this.

So I was wondering, I know a bit about recursion.

I think we all have sort of a basic understanding of what's involved in recursive loops.

But I was wondering,

how sort of the predictive, where is this relationship between predictive coding and this other algorithm, which I've heard that, I don't know if it's an expression or label back propagation of error, but I've never really looked at it from a operational standpoint or how it's laid out mathematically.

And I'm not sure if either of you have that familiarity, but I was hoping maybe you did.


SPEAKER_02:
Great.

So how does predictive coding relate to the back propagation of error?

Let's look at these sections, which we didn't annotate too heavily leading up to, because there's many equations in the paper, many areas.

Even before we introduce the formalism and to give me 15 seconds to read it, qualitatively, what would backpropagation of error mean to you or where is an applied setting where you think this could matter?


SPEAKER_01:
Well, and again, this is me talking out loud, hanging my butt out there because I don't really know what I'm talking about.

But am I correct in assuming that it's anytime you find yourself in feedback and feed-forward loops?

I know that they're acyclical, and I know that usually implies adirectional.

But isn't anytime you're getting sort of a...

a looping condition where you get an opportunity to sort of reset?

Isn't that the situation that we're examining?


SPEAKER_02:
Okay, we're in the arena of

statistical Bayesian graphs.

So we're talking about models here.

We're not necessarily talking about what an actual system is doing, a computer system, nor some sort of biological system.

We're talking about a modeling statistical architecture, and it would be a secondary conversation about what systems are actually doing with back propagation.

So this isn't just like saltwater flowing from the ocean into the river.

Okay.

just to sort of forestall any preliminary connections we might want to draw.

But we can see when we have a bit more sense of what backpropagation of error is on this graphical setting, how it could apply.

So they're introducing this concept of a computational graph, fancy G.

not free energy related, just G for graph.

And it consists of two kinds of elements, just like all graphs or networks do, which is vertices and edges.

The vertices are representing intermediate computation products.

For example, the activations at each level, this multi-level predictive processing architecture.

So we have like nodes within the level, and then there's edges that represent influence here as differentiable functions.

So this is

related to some of the graphical Bayesian approaches we've looked at previously, but also it shares a lot with some neural network representations where the edges are not just like a correlation or a mutual information, but the edges are more like a differentiable function.


SPEAKER_01:
Okay.


SPEAKER_02:
We're gonna restrict to looking at specific kind of graphs here, which are computation graphs.

They're saying are directed acyclic graphs.

So that means that it's directed.

There's an arrow such that the differentiability of the function is going from one way to the other.

Like we can think of that as the forward direction.

And then the acyclicness is just ensuring that there's like sort of one place where we could start and we could just forward propagate everything out to the edges.

And that wouldn't trap us into an infinite recursion by going forward.

And we've talked about some of the differences between like cyclic and acyclic graphs with Majid when we were talking about the DCM and all the ambiguities of that acronym.

Okay.

Stephen, anything, and then we'll continue.


SPEAKER_00:
Just one quick question.

Ontologically, how useful or problematic is the word coding?

Now we've moved a bit further down with active inference.

And in a way, is back propagation of error needed to be that, needs to be distinguished out, even in a descriptive term, just so that

the connotations of coding don't sort of confuse all the discourse.

What are the connotations of coding?

Well, that it's basically kind of almost like a representational kind of script almost that's been written somewhere.

It's kind of how, you know, like the coding that you get in a computer program or something.

And I kind of think that that

does sort of speak to the nature in a way, because this is the nature of how for the future, but back propagation of error kind of speaks in some ways to the nature of how the code is processed.

And I may be off topic here, but I just kind of wonder, as we're going through this and trying to link it all together, how much that ontological term is also possibly something that could be thought of in another way to make it easier to understand what's going on.


SPEAKER_02:
Okay, we are dealing here with computational architectures.

So I don't think we need to run away from these questions that were brought up more in a philosophical non-computational version into the anti-computationalist perspective.

Like we're not fighting for what is happening in the brain here.

We're using predictive coding to talk about information coding and transmission approaches.

So that's one thing.

And yeah, I mean, we don't, we can't just go around flipping different words.

Predictive coding is what for decades has been referenced by a certain architecture here.

And then there's a lot of openness with how that architecture is applied.

But I'm not a hundred percent sure, like this is about predictive coding architectures.

yeah no that's cool i'm i'm just putting that out there just seeing how that floats a little bit i think that's a good good response like what else would what else could be in this position of coding it's about how signals are encoded and what is being transferred between different nodes is what is being transferred the raw value or

In the predictive coding architecture, we have two directions that signals are being passed in this compute graph, not in the brain.

And that would be coming from one direction, the expectation, that's the predictive part, and then the error residual.

That's the deviation from the expectation.

And so that is what the predictive coding architecture is.

It's just about having graphical systems that are passing expectations and differences or errors rather than passing just the plain values themselves.


SPEAKER_00:
Yeah.

I suppose there's also that question of how much is in...

the dynamics of what's going on and the hyperparameters when looked at, and how much difference that makes to the story.

Okay, so you've got the predictions, the prediction errors being passed down, but coding tends to be like the numbers are there.

But in some ways, the numbers can kind of evolve once the,

the system the dynamics of the systems or the dynamical sort of patterns of the system start to sort of fire up so to speak you know it takes a little bit of time for the active influence model to start really purring along sometimes so that kind of coding is in a way sort of almost more live than traditional coding which is kind of sort of baked in


SPEAKER_02:
Okay, I got you.

We're not even in active inference yet.

So I see some back later, but we're discussing predictive coding, back propagation of error, and then action does get brought into the picture in section four or five, but up until section four or five, there's no action even in the model.

So we're just looking at the architecture that does computation.

And then understanding how action could be brought in.

And that's kind of the fun surprise that we uncovered in the .0 and .1 and in the layout of the paper was when the architecture is sufficiently general and powerful, incorporating another variable that is about action, like policy selection, becomes fluent.

And that's...

why these architectures are composable with different sensory modalities with different scopes of action with different affordances all these things because there's a level of generalizability that we can pull back to okay so let's return to predictive coding and back propagation of error we have this graph the vertices are representing intermediate computational project products so like a number or like a variable

because it's kind of like where you're storing the intermediate computation product.

And then the edge of differentiable functions could be like y equals 2x is a differentiable function.

Anything that has smoothness in the graph is differentiable.

And as the orange part says, we're restricting this to directed acyclic graphs where we're not going to get into an infinite regress just by going forward.

Now, what happens when we're trying to tune this architecture

so that it has a minimized loss function.

Loss function, whether we're dealing with least squares error, like a regular linear regression, or whether we're dealing with some other loss function, the loss function is like what is trying to be minimized.

So when the ball is rolling downhill, we're minimizing like the loss or the energy function.

But the energy function approach and wording is coming more from like a physics

side whereas the loss function is just a purely statistical claim like what is the function that we're minimizing okay given the output vertex v out so this is like the one that we actually want um to be reducing the loss around the loss function is going to be l l is a function of v out

Backpropagation can be performed upon a computation graph.

So we're going to be minimizing loss at a certain vertex by backpropagating upstream a lot of what led to it being the way it is.

And so that could be done on just the last tip of the spear, or it could be done throughout the whole system.

Backpropagation is an extremely straightforward algorithm

which simply uses the chain rule of multivariable calculus to recursively compute the derivatives of children nodes from the derivatives of their parents.

Okay, so formalism 28.

This fancy D is partial differential.

So the change of the loss function with respect to the change in an intermediate variable I is what we're looking at.

We're looking at how

we're going on that bowl, are we dropping on the loss function?

Is the model getting better or worse with respect to changes of intermediate computational products, vertices?

And that overall loss function

is the sum of v the subset of p which is the parents so we're looking at vi like the um node that we're focused on node of interest or something and we're going to sum over all of its parents and then for all of its parents we're going to compute something that's basically like the change in loss function over the change in that parent's value v sub j

and how it changes with respect to change in the child.

So we're taking something that's like this graphical network with nodes and edges and differentiability, and then figuring out which way is the overall downhill based upon all of the back propagation of the gradients.

By starting with this output gradient, that's vout,

And if all the gradients are known, so like we're dealing with a model that we specified,

then the derivatives of every vertex with respect to the loss can be recursively computed.

So because all the edges are differentiable, the big function is also differentiable.

So like 2x is differentiable, x squared is differentiable, x cubed and x to the 11th power are all differentiable functions.

So you could also differentiate their concatenation through addition.

that'd be like the derivative of two X plus the derivative of this.

And also the chain rule in calculus allows us to like nest those.

So if we can differentiate two X and if we can differentiate X squared, we could differentiate two X inside of parentheses squared.

That's basically the chain rule in calculus.

And then that is what allows this sort of,

big downhill computation based upon the specification of the graph and then the back propagation of all the relevant nodes and finding out how those need to be tuned so that the final output node that's being optimized

can be going downhill defined as going down on the loss function.

Less loss is better here.

Okay.


SPEAKER_01:
Yes.

Excellent.

Thank you.

Because this was one big opaque Markov blank to me when I read it.

I just couldn't make any sense of it.

But is the ability to differentiate or what they're saying here is the ability to differentiate an ability to build

relationship that parent-child thing is is it that is it the capacity to differentiate that allows for the relationship to be identified or is it the other way around because

When when they're talking here and saying Backpropagation perform on a computation graph that Propagation is extremely straightforward and uses the chain rule of multivariate stuff when it's talking about that.

It's it's It's pointing back to something that you don't necessarily have to see but you know that there is some relationship just like there is between 2x and x squared am I getting

Is your explanation helping me move closer to a better understanding of that?

Because I understand derivatives, but I'm not sure how the derivatives fit into this explanation.


SPEAKER_02:
Great.

Yeah, I think we're solenoidally exploring.

Now, we haven't even brought in predictive derivatives.

These equations here are for 28 and 29 are about back propagation on computation graphs.

So with these papers, it's really important to know like what has not been brought onto this diction and there isn't error.

This is just about the nodes and edges like the intermediate computation values of the abstractist kind and their differentiable relationships.

So now they're going to show how predictive coding relates to backpropagation of error.

So they write, predictive coding can also be straightforwardly extended to arbitrary computation graphs.

To do this, we simply augment the standard computation graphs with additional error units for each vertex.

So let's recall what some of these graphs that we previously looked at.

Here in figure one of the paper, we have a graph where the nodes are intermediate computational products, represent differentiable relationships.

And importantly, some of these intermediate products

have the semantics or the interpretation of being estimates about mean, mu, at that level or estimates about variance and error at that level, epsilon.

So this is kind of like

a subtype of graph, like the abstract is kind, it's just intermediate computational products.

And then predictive coding are specific kinds of computational graphs where some of the nodes represent mean estimates and some of them are error estimates.

In contrast, and hopefully not to confuse, here's from 26.2, where we talked about like integrator chains.

So notice that all of these nodes can be interpreted as intermediate computational products, but there's no error term here.

So this is an example of a computational graph that doesn't have error terms.

This is not a predictive coding architecture.

Not that it's inconsistent with or somehow disjoint from, but this is not having epsilons, so it's not a predictive coding architecture.

whereas this is sort of a canonical multi-level predictive coding architecture.

Great.

Hey, Jax, good to see you in the chat.

Okay, now we're going to return back to the back prop.

Okay, so...

we're adding in these additional error units epsilon sub i for each level of the graph.

Formally, the augmented graph becomes now g tilde.

So previously, fancy g was just edges and vertices, nodes and edges.

And now the vertices is being unpacked so that v can remain for the mean estimators, which are ultimately the ones that we would like to have the most accuracy on.

accounting also for variance estimators.

And so this big E has been pulled out.

It's the set of all error neurons.

We then adapt the core predictive coding dynamics equations from a hierarchy of layers to arbitrary graphs.

Change in V the nodes with respect to time

prediction is happening through a sequence of observations in the predictive coding architecture, that's that now casting, Kalman filter, frame differencing, a lot of these other sort of real-time algorithms for mean and variance estimation that we talked about, equals the change in F free energy with respect to change in the node.

So now the loss function through time

is going to be related to the f that had been previously um previously defined in in earlier equations and then that is going to be defined as basically the error this is in formalism 30 the epsilon sub i so the the error variance at that last level minus a sum of c

So C are the children, and then there's just some more terms involving basically tuning the nodes, accounting for the fact that we want to like tune the variance estimators and the computation nodes that represent the mean estimates.

And so...

Then the dynamics of the parameters on the vertices v and the edge functions theta, so they kind of slightly evolve the edge notation towards theta, can then be derived as a gradient descent on f, where f is the sum of prediction errors of every node in the graph.

So previously they had specified how the composability of the multi-level predictive coding architecture is such that the levels are

can be factorized from each other which allows us to sum their free energy contributions into like a free energy total and so that is how a multi-level model can be fit because it's finding like the free energy minimum for that whole multi-level system which might entail one of the levels not being at its like perfect perfect lowest free energy state

And in this section, they've connected the way that back propagation in 28 and 29, without any action, without any prediction, is able to make a big loss function that's composed because of the tractability of the compute graph.

And all of these little loss functions can be summed into a big loss function that you use to train a neural network, for example.

And then...

moving that into the predictive coding in free energy minimizing space by saying, well, this is a special kind of compute graph that has the mean estimators mu and the variance estimators, the error terms.

And then we're going to do something similar where we're going to compute what amounts to a loss function, f, the sum of prediction errors of every node in the graph,

based upon internal prediction errors being minimized.

So in the non-predictive coding frame, we're minimizing a big loss function that's composed of smaller loss functions.

In the predictive coding frame, we're minimizing our divergence from expectations composed of the divergences from many smaller expectations.

Okay, yeah, go for it.


SPEAKER_00:
sort of extrapolating hopefully again from this the the predictive coding then like by working with the errors which are kind of almost uh i know i'm making a little bit of a jump here but um are kind of implicit to the to whatever the system of interpretation is on the data

is quite, it's kind of, that's why you're going into this biological world, because it's, it's so bottom up, in a way.

Because it can start just with whatever the errors are, in the nature of the system that's been set up for doing the sensing, even forgetting action, just the nature of how the sensing set up, will start to generate the errors at some implicit level.


SPEAKER_02:
that be a fair assumption so anytime there's going to be a prediction of observations so yeah we don't need action here yet there's two options either you have a hundred percent predictive power and your error is zero or there was some error so you're always in one of those two cases and which one are we usually in

not having the total perfect 100% accuracy down to the 100th decimal point.

If for no reason other, then that's beyond the point of diminishing returns, or we're also trying to balance accuracy with model complexity.

So because we have these metrics like the AIC, the BIC, all these metrics that find us a model that's a balance between accuracy and complexity, so it's not overfitting, the cost of not overfitting

of cost of having a simpler model is literally increased error.

Because every new parameter that you add into a model, you always explain more variance.

The first principal component takes the most variance of the data.

The second principal component takes the next most variance of the data.

Every principal component on a data set is going to continue to eat more and more variance out of the data, leaving less and less in the error term.

But it's a situational thing, whether you need to explain 80% of the data or 90% of the data or 95% of the data, but there's always going to be some difference between the prediction and the observation.

Again, unless one is in this super edge case where it's perfectly known.

So yeah, go for it.


SPEAKER_00:
Yeah, that's helpful.

And it's also implicit there that

that even if you, whatever you set that complexity and accuracy to at this very deflated level is really helpful.

So the nature of the model will change the nature of the errors and basically the best it gets to by the most it can reduce will vary a little bit as you change that model.

And not making too much of an extrapolation, but that,

is a nice way to be able to unify a lot more complex and rarefied ideas about models and what is really happening.

You can sort of drop back to that complexity accuracy principle.


SPEAKER_02:
here's some more sections from 4.1 towards the end of 4.1 on page 30 in the paper so yeah hopefully we've been representing this at least mostly accurately and then for those who want to go deeper that's why there's a paper and that's why there's like

a continued discussion and development on these ideas because they drop many little crumbs of, well, we've held this to be zero for simplicity and but they're right.

We can think of this as a predictive coding network in which all of the error is initially focused at the output of the network where the loss is.

And then through dynamical minimization of prediction errors at multiple levels, layers, this error is slowly spread out through the network in parallel until the optimum distribution

The error at each vertex is precisely the credit that should be assigned to causing the error in the first place.

That's pretty interesting.

It's kind of like let the chips fall where they may with respect to error and variance attribution.

Dean?


SPEAKER_01:
This is awesome.

So is the way the architecture is and the way the graph is set up as kind of a mirror of what the architecture is of this layering thing,

Are they also there to kind of contain little fires of mistakes and errors?

Is that why it's effective?

Because it sort of contains something, you know, suddenly causing a chain reaction and having everything.

When we read Majeed's paper about, I think it was Majeed or maybe it was Axel's paper, about you can make predictions going right off of a cliff.

And the result is you no longer exist.

Is this kind of an architecture that helps us avoid some of those really consequential prediction consequences?


SPEAKER_02:
That's an interesting idea.

I think we should engage in some slide play and try to get a little bit of clarity on that.

And also, hopefully we'll hit on

lot of the ideas that we've been bringing up in a more abstract way because i know it it's helpful for like everyone to see multiple coats of paint on this and like it it's the the epitome of abstraction is the austerity of those formalisms about compute graphs so let's think about it um we won't take it to the um the cafe

level um that was even though that was i remember that being quite fun um with the we're gonna be we're gonna be between the cafe um and uh you know something else so we'll be intermediate because we're gonna be talking about a specific graph but we're not gonna like necessarily for the first pass on it go all the way in okay so

The node that we care about being most accurate on is 7.

So this is going to be our focal node.

This is like...

the one that we want to have an image classifier algorithm, this is the classifier node.

We want to have prediction on what the thermometer is going to say, that's what this is.

We want to have prediction on the temperature in the room, that's what this is.

We want to have inference on action and think about the upstream

of action, then that's what this node is.

So seven is gonna be like the node that we're interested in minimizing our loss function on.

This can be carried out on multiple variables, but we're gonna just look at minimizing on this.

Okay.

what is the back propagation chain here well let's think about what the forward and the reverse arrows are so first just to clarify like this is a directed acyclic graph there's no loops in the graph and the arrows are all directed um they reflect

Differentiable functions, causal influences amongst these variables, these intermediate computational products.

So like variable one is then there's some function, differentiable function that translates it into two, two influences four and three.

Three influences five and six, six influences seven.

We've seen this in the Bayesian context where the sparsity of the connections

allow for the factorization of this graph.

Like it allows us to sort of hold one part

unchanging and change other parts through factorization.

That's a lot of the variational inference stuff that we've talked about.

Okay, so what are the parents of seven?

Which nodes do you think that we should consider if we're going to back propagate?

So the forward model, the actual like causal chain in the model, not in the world, is like one, you can imagine some perturbation happens to one, every node is going to be influenced.

That's the forward perturbation in the forward generative model.

What is going to be invoked if we start at seven and do a back propagation?

Which nodes are not important?

Five and four.

Right, yeah.

Five and four will not be invoked in this optimization scheme.

So if we were...

So we'll make these ones gray that we're going to be back propagating on.

But I also hope I'm representing this accurately, but you could imagine that like changing the dials on five or the function between three and five will not reduce the loss on seven.

So it still might be an important thing in the forward model, but it's not in the back propagation from 7.

It's in the forward propagation from 1.

Okay.

So then...

This is not a predictive coding architecture because in the predictive coding architecture, some of these, that's what we saw in like, for example, in figure two is what we saw in figure one.

So there's the means and the variance estimators.

So let's bring in figure one and try to adapt it to our graph.

So our graph is sort of up to formalism 29.

Now we want to think about a graph that's not just nodes and edges of the abstract type, but it's going to be nodes and edges that include these error neurons, as they're calling them here.

So here's a reminder on what that architecture looked like.

Jax wrote, how is the back propagation of error related to Holland's, John Holland, bucket brigade, the bucket brigade in signals and boundaries?

It sounds, explain a little bit what is the bucket brigade or we'll look, but it sounds like a bunch of people helping and sharing buckets or something like that.

Okay.

So let's just say that the one that we actually want to minimize is going to be like a mu.

it's gonna be a mean estimator.

Now let's just modify this graph so that this one's gonna be error.

I'll just use the, well, type the variables.

So this is a mean estimator.

And then we're gonna alternate so that this one is an error and this one is a mean.

this one's a mean again this is not the exact same architecture that they laid out and this might not be um traditional or um

classical standards but like this is getting us towards this idea that within a cortical column so to speak we have um the forward propagation of alternating layers of means and errors um we want this loss function to be minimized by diffusing

the uncertainty across these error nodes.

That is like letting the chips lie in terms of the variance attribution.

And as they write precisely the credit, it should be assigned to cause the error in the first place.

So if like node two in the forward causal model,

was causing 90% of the variance.

And so this was like, one is a very tight distribution and it gets blurred a ton on the way to three.

And then from three, it only gets blurred a little bit on the way to seven.

So like two is going to be big.

This is like a big error estimator.

And six is small.

tiny variance estimator.

So it'd be like, if we were in classical statistics and we were gonna do, we were doing brain imaging on a bunch of people from two different categories, like with and without this diagnosis, you could imagine that in that two level model, one world, there would be like massive variance amongst people and very little differences between those two higher groups.

In another world, it could be the opposite.

There could be big difference between the two groups, but very small differences amongst the people within a group.

And so this is the parameterization and finding the parameters that are going to minimize the loss function means that we want the variance to be partitioned appropriately

across error terms, just like we would like the average, the mean estimators to be accurate, the equivalent in the error world is we want the errors to be appropriate in how they're distributed.

So it's like, if you're doing a mean estimator on something, you don't just want like the highest mean possible, you want like the most accurate mean possible.

And the most accurate error possible doesn't mean the lowest error possible.

That would be the fallacy of, well, we're estimating means, well, then let's just make it bigger.

So we're estimating errors, not because we want bigger errors or want smaller errors.

That's the thumb on the scale.

Estimating errors accurately entails appropriately distributing the error terms on this graphical architecture.

And then they're adding in one extra point, which is basically that the dynamics of predictive coding are purely local, requiring prediction errors from the current vertex and the child.

That is...

on the computational front, a win for tractability and for the actual algorithm, because you don't need to have the whole model loaded into RAM, for example, because you know which neighborhood of variables you're going to engage when you're doing a certain parameter update.

And then on the biological side, it starts also leaning towards plausibility because

they even call it an error neuron well neurons are signaling forward let's just call that the direction that the action potential travels down the axon but there's also retrograde signaling at the synaptic junction so it's plausible at the multiple neuron architecture level

or even at the two neuron biological system level that there is like, even if there's a direction at which signaling is assumed or perceived to be normatively happening, there's also a retrograde signaling of multiple kinds.

And we won't go into that like complexity of retrograde synaptic regulation, but that at the circuit level and at even the synapse level, the idea that signaling is happening both ways

is something that is seen as the strength of the predictive coding architecture.

Again, the locality allows it to be more computationally implementable and this sort of bi-directionality and signaling is more reminiscent of biological signaling processes, synapses, conversations, brain regions, so on.

Okay, let's... Okay, yeah, any thoughts on this?


SPEAKER_01:
I think that's a pretty good explanation.


SPEAKER_00:
Go for it, Stephen.

Just checking.

So if you added all the errors out, are the errors kind of independent of each other, or is there like a total error across the chain, so to speak, of like...

uh, how much error there can be, or is it like kind of contained within the streets?

Like it's six relatively contained and two relatively contained.


SPEAKER_01:
okay dean can i take a guess steven yeah sure i'm get i'm guessing so just keep the wild a money money part of this front of mind um i think what the i think what the graph is telling us or at least when we formalize it in this way is that you could always

find the cumulative error, but the point of the system is to try and keep the error where it actually exists within each layer.

So sort of partnering the error

each level with with whatever the prior or the hierarchical L plus one or L minus one layer is but me again I'm just guessing but I'm kind of making sure that I understand completely what what Daniel laid out here and that would be my guess I mean you could you can add it all up but I think the I think the point of it is is

is that before it runs away from you, the architecture is set up so that there will be some cumulative effect, but it probably wouldn't be, it shouldn't be, there's the word I'm looking for, it shouldn't be as abrupt as if you didn't have the layers and you didn't have the ability to differentiate and you didn't have a kind of understanding of what is happening, i.e.,


SPEAKER_02:
back propagationally now again i'm just throwing that out there but because i don't have a reputation around this i could be completely wrong there's a lot to say and the authors like do provide many citations but i i believe that's right like just as appropriate mean determination we're thinking in a gaussian world like right um

There's a lot to say on Gaussian processes, but a Gaussian is a mean and an uncertainty.

So there's going to be somewhere where the hump of that distribution is, and then there's going to be some width to it.

Now, sometimes the process is actually Gaussian.

Other times we can use nested Gaussian processes so that it's like, well, there's some process that's moving it, and then there's wiggle around it.

So we're going to see that moving as one Gaussian, and then the other one is like a ripple that's like a smaller Gaussian on that.

And then even in cases where there's a non-Gaussian distribution, that's where the Laplace approximation comes into play, which is the second derivative

We have the Jacobian and then the Hessian matrix in 26.

And then we talked about how when you take the second derivative, you're basically making that bowl and then lying in it.

So you're going to the bottom of the bowl that you created, and you know that it's gonna be the sort of hump structure that's amenable to going downhill.

Because once you go beyond quadratic,

beyond the hump, anything that's like a multi-hump distribution or even just X cubed, you're going to have challenges

getting globally optimal from local optimization processes because you won't know if you're in a local energy well or not.

And so many of these computational tools and approaches that we're discussing like across these live streams and papers can be understood as taking something that isn't like a bowl

and making it so that it can go downhill and finding, well, what is the appropriate level of how many layers have to be added to this onion?

Or how many oven gloves do I have to put on?

How many approximations should we carry out in our approximation science so that there can be something like a parabola

where we know that we're on parabola territory and that's what allows us to then minimize a loss function because if one believed that they were on a rugged landscape then minimizing something locally would be you'd have no way to know if that was on the right track or not um yeah steven


SPEAKER_00:
could i say then would i be okay to say that it's almost like each of these errors say number six number two in here and you could have those would be like little mini bowls or mini um at different and the further up the chain the bigger the potential um impact of the positioning in that bowl or whatever the landscape is out towards whatever's been you know the mean that's been understood okay


SPEAKER_02:
Let's look at figure one.

So this is a cortically inspired, so inspired by the Cortex.

And Blue wrote, Dean, I understand the architecture the same way error correction is retained locally.

So let's look at this semi-cortically inspired predictive coding hierarchical system.

Just like in the example with the two clinical populations doing brain imaging, there's one world where the patients within the group vary a lot and the groups don't differ.

There's another world where the groups differ a ton and then the people within each group do not have any variance amongst them.

So depending on how it's parameterized, one could have a model where the highest level has a major influence or not.

And that's the whole question.

It's like in a linear model, is it more important like the MX part, the slope, or the B, how much it shifted?

If the regime of attention is on linear modeling as an architecture, it doesn't make sense because they're both just parameters in a model.

Now for any given data set, it's an empirical question to what extent changes in the slope or the intercept

are associated with changes in the loss function and that's linear regression.

Here's an architecture that gives us an approach to finding loss functions, but how do we know how many layers?

How do we know how many of these side connections to make?

There's a few ways to go about that.

One could just a priori model the structure of the model of the graph around something else like previous work or some inspiration from a biological system.

They might also be interested in this structure being a parameter of a model.

Like

And that's the whole structure learning and meta Bayesian approaches and all of that, but that's pulling back another level.

And so that's the, the challenge of structure learning, because we can get that loss function within a given static graph, but that doesn't mean that we have the best possible graph architecture.

It's like if you have 100 pieces of data, biometric data on people, and you're making a multiple linear regression to find out their risk for some condition.

Do you just use 100?

That might be overfitting.

It might be too costly computationally.

So how many of those variables do you use?

There's a process for determining what's at the trade-off between accuracy and complexity, where it's fitting the outcome well, the loss function on the condition that you're trying to actually predict, but it's not past the point of diminishing return or using variables that are uninformative.

Or for example, if there's two variables that are perfectly correlated with each other, that's called multilinearity, and you wouldn't want to use those variables.

So that's the challenge in the linear regression world.

And this is like taking it into another space, but it has more analogies than not, which is why SPM, statistical parametric mapping, that textbook is, for those who want to engage, vital.

prerequisite because it addresses a lot of these questions on model fitting including on dynamical systems and it puts it with six feet on the ground in classical parametric and non-parametric statistics dean


SPEAKER_01:
This just raises the question in my mind, I haven't answered it, as to whether or not a mean is constructed, or if a mean is actually sculpted, or some combination of the two.

I mean, being a min-two guy, I'm going to assume that it's probably both.

Like you can have the figure, but it's your understanding of...

what the difference is between an edge and a directional link, which you have to kind of carve out from the overall image.

But that's, again, that's one of these things now where if you get into this predictive coding piece, I think the product looks like a construct.

But in actuality, there's got to be a huge element of this, of being able to isolate the mistakes so that they don't overwhelm.

And if you're isolating something, if you're removing some factor, you're actually sculpting as well.

So maybe that's a bit of a philosophical overlay on the sort of mathematical operational side.

But you know me, I'm always trying to turn it into what's the practical function of this.


SPEAKER_02:
Yeah, it's like statistics as world building, because if you just want the mean, then just take the grand average.

If you want the mean and the variance, there's a method.

Do you want a variance on the variance?

How confident are you that it's 10 plus or minus one?

Is it 10 plus or minus one plus or minus 0.5?

Or is it 10 plus or minus one plus or minus 0.01?

So a tight estimator.

on the error as a parameter and so we're like playing with which parameters are seen as like the things in and of themselves the means and which ones with respect to means have this interpretation as being error neurons or being variance descriptors now one properly fit model which you could take into the you know 50 stacked levels 10 plus or minus one

And then on that one variance estimator, we have plus or minus 0.1.

And then how sure are we about the plus or minus 0.1?


UNKNOWN:
0.01.


SPEAKER_02:
Okay, now how sure are we about that 0.01?


UNKNOWN:
0.001.


SPEAKER_02:
That would be like a well-fit model where as we kept on adding uncertainty parameters, we would be making appropriate estimators.

Now, what would be like a mal-fit model?

And I hope this is accurate.

10 plus or minus one.

And you go, how sure are you on that one?

plus or minus 50.

It's like the variance just exploded.

It's like, well, but you're saying that the variance could be like zero or the variance could be 50.

How much confidence should we then have in the mean estimator?

And so that's what this appropriate distribution of uncertainty is.

Now let's think about node seven, an uninformative node seven.

We're predicting temperature, but it's just, it's unchanging.

We don't have a lot of information there to fit a world model because it could just be a single level estimate.

It's 10 and it's not changing.

But as we deal with progressively more and more nuanced and informative things that we want to minimize loss functions on like natural language,

images from natural scenes, action, including the unknown consequences of real-time unfolding action.

Those, because the uncertainty becomes higher and the information content of what you're trying to minimize your loss function on is higher, it not authorizes or licenses, but kind of

moves us into potentially having higher levels of model complexity as simple as possible and not simpler with that principle being carried through again if you're just trying to predict a constant number you don't need a six level predictive coding architecture whereas if there was something that did actually have that many levels of depth

that's the kind of architecture that would do well to predict it and we talked about this with the par and pazulo on the architectures for homeostasis and allostasis and it was like yeah to just recognize if you're out of bounds and then come back in that's a given model but then there's a different graph that's going to do something like intermodal or anticipatory or have memory so all of those cognitive or computational features

license or engender more complex causal models but these are being sculpted ab initio from nothing by the statistician and then we're in sort of brackish or gray waters when we um juxtapose the architecture of the graph with the architecture of some natural system


SPEAKER_00:
Okay?

Yes?

Stephen?

Yeah.

And you've got this excitory and inhibitory dynamic going on.

And even though the arrow is going from the arrow down to the U, the U is exciting the layer above, isn't it?

It's exciting.

It's almost like...

It's like one, bottom up is kind of exciting, higher up areas, and then things that are further up are also sort of saying, hey, calm down, calm down.

I can reduce some of this.

You're getting a bit out of hand here, you know?

They've only scored two goals.

Let's relax, you know?


SPEAKER_02:
That, again, I also hope this is not inaccurate.

It's almost like there's a suppression of error, like a dampening being carried forward

with the ultimate dampening being you predict it perfectly.

You have totally quelled all error yet in real settings because of just simply model simplification or uncertainty in the world,

error is coming in from the observables, which is often what we're trying to reduce our loss function on.

And that's this whole difference between reinforcement learning, finding oneself in reinforcing or rewarding states, and active inference with reducing surprise about outcomes.

we're dampening error and carrying that forward in the forward generative model and then also able to run it back in a tractable way because it's like you're dampening the vibration, but then the vibration is still entering in and you'd want that vibration to be appropriately distributed according to where it should be.

Okay, Dina, then Steven.


SPEAKER_01:
Yeah, it seems to me that the way, what this is saying to me is that because of the way things are organized, two things can live simultaneously at once.

One is sort of that local mean correction, and the other is the more generalized global or

the other direction in the bidirectionality parallel of mean correction.

One is kind of reinforced through external evidence, while the other one is reinforced through localized, almost, it is distributed, but it's almost like the ability to localize and isolate if necessary.

So I know we've talked a lot about top down and bottom up, but this brings a lot more color in my mind to some of those conversations we've had before in terms of, you know, arrows going in opposite directions.

And it's not so much that they're going in opposite directions, but that you can be going two different directions quite literally at once because of the way things are set up.

So, yeah, I think this has been really, really helpful for me in terms of not only understanding what back propagation is, but actually in sort of giving me a little bit more foundational ideas around, again, how do we turn these sort of statistical ways of representing it into, so I'm confused, should I go into the cafe or not?

I mean, not all of us have to go to that level of detail to really understand what the final thing was that pushed us over the door threshold or not.

But it appears that this has been around for a while and being able to take that, if you really are kind of stuck because you really don't know what the parents are of your current child,

action because we haven't got as you say we haven't got to action yet but that's the whole point there is an architecture there that's going to allow us to have choice thanks um this minimum of two um


SPEAKER_02:
comes up so much.

So it's awesome to hear about that.

And then to connect like expectation maximization and the tale of two densities, epistemic and pragmatic.

It's like wherever we look, there's partitions into two.

into multiplicity and unity through plurality.

And then other times, even within the one road, there's the movement in both directions, forward and backwards in time, or the forward propagation of a generative model, which could from just one, if all you knew was one, you could simulate sevens.

If all you knew was seven and the structure of the network, of course,

you could backpropagate the best possible mean and error estimates using backpropagation of error.

Okay, Stephen?


SPEAKER_00:
This is helpful for thinking about how we perceive things as well.

Dean was saying about making choices.

It can be when we get zero error or whatever's seen as negligible error.

You want to just allow that to carry on and stay with your imagination.

So I go to the bar and a lot of things are the same, right?

Maybe there's a happy hour sign.

And if that's unusual, maybe I'm happy about that.

I note it.

But if a lot of things are exactly or very similar to how I expected, then in a way, my perception is probably mostly wrong.

my predictions just not being

contradicted.

Or actually, I just don't see it.

I think I've seen it, but I just don't need to see it.

And where there's something which is ambiguous, like you were saying, you need that number seven, if we're going to use this diagram, starts to have to be looked at.

You start peering at things, straining their eyes, trying to sort of see, well, what's that?

Is it a dog?

Is it a man running into the woods?

Is it a dog?

Is it a man running into the woods?

You start to notice

So I think this is also quite useful for that perspective of, you know, when, when you just, you don't actually need to see a lot of stuff.

It's quite an efficient model, right?

If it's, if, if the, if the area goes below a certain level, you just move on and carry on.

Of course, that's where the challenge with autism is, they think, because too many things have been looked at that are possibly negligibly relevant at the time.


SPEAKER_02:
Okay, thank you.


SPEAKER_01:
Just one last thing, Dan, before, because I know you want to clean this up.

I think one of the analogies that I draw is with the emphasis on the word minimum two, min two, doesn't mean it's always a hard bifurcation or a binary.

It means that

the minimum amount of things, like even when you take your shoelace and wind it through all the eyes of your shoe, you've got two ends that you eventually will loop back together and entangle in some way to hold your shoe on.

So even though it's one lace, I think what active inference reinforces, because it's got active and inference, is that ability to be able to

see two things, both seemingly contradictory and both actually true.

Now, it can be multiple things.

You can have multiple surprises, some of which make you quite happy and others where

you walked in and said, well, why hasn't it been happy hour at this hour when I've been at this bar on multiple other days?

Like you could be disappointed in that as well as being pleasantly surprised.

So I think what this, again, I just reinforce this, the minimum of two allows for the integration.

The minimum of two recognizes the differentiation.

And when we're talking about statistical issues and when we're talking about errors and when we're talking about

energy, I think the way that these folks have been able to discover what architecture would allow for that is what's really interesting to me now.

Because I wasn't even thinking in these sort of, so what does the scaffold look like?

But it looks like there's been a lot of work done in this area.


SPEAKER_02:
So let's take it to action as we head into our last little bit on this.

And of course, if anyone is more author or not familiar with the formalisms, just contribute to Acting Flab, help us annotate the papers and read these deeply because there's so much to learn and somebody getting involved like a few days or weeks before these streams is a leverage point if you want to help other people understand this area.

So I've changed the colors just all to gray because they're all just random variables now.

We had the seven.

So we were back propagating error on temperature prediction.

But also maybe this system is predicting pressure.

And so then three might be like estimating, you know, is the water boiling?

Again, we're getting one step closer to the cafe.

So if we were only trying to predict temperature, we would back propagate along the seven, six, three, two, one route.

If we were only predicting pressure, we could back propagate along eight, five, three, two, one.

The fact that there's a parent that is upstream, it's like the last common ancestor of pressure and temperature.

Like we might be able to reduce these errors a lot on the temperatures by having this higher order variable, is the water boiling?

And so one can imagine like if the answer is yes, then it's like, it's expecting that this is like higher on both.

And if it's low, it's lower on both.

So the generative model can go from the water's boiling, what pressure and temperatures do I expect?

And you could add a third node.

It allows you to, conditioned on a cognitive structure, go from observations of just temperature around the horn to pressure, or from pressure and temperature up to the water boiling.

But the influence of these observations is like limited through this choke point.

It's actually a Markov blanket with respect to what's on the either side of it.

Let's bring action into the picture.

Action can also be understood as parametric.

What angle should my elbow be at?

What angle should the eyes be oriented?

Those types of things, again, not whether that's what the actual neural signaling in the brain is doing, but this is how we model action.

It's like the car, how many miles per hour should I be going?

those kinds of questions is parametric modeling of action.

And that's what was so fun and exciting was the first 50 equations in this paper have scaffolded this tremendous general node computation architecture.

multi-level means and variances forward and backward up down left right all of these interesting ways and connections but not yet action not because we hadn't brought action in as a dancing partner but actually this was above any interpretation of the nodes as inference or action and then it's as easy to condition on action

as it is to condition on any other inference, because action is just a parameter for parametric active entities that are engaging in action as inference, planning as inference and so on.

And so then one can think about what is the compute graph, whether it's composed of just nodes and edges, or whether we think of some of those nodes as being like variance estimators,

And then, you know, this maybe, again, the connections, I'm not going to rewire this whole graph, but just like, is this a water boiling situation?

Okay, so we have a thermometer.

It says that it's 100 C. It's boiling temp.

The pressure sensor is broken, but we back propagate it.

we're high confidence on the water boiling.

Then this is like another variable that's going to require like a different sort of question that it's answering.

But one can imagine that if this is a water boiling situation and the water's not boiling, there might be an error.

And so how can we reduce loss function on the total compute graph, including observation and action?

How can we reduce uncertainty about a cognitive model, including observation and action?

There's two ways to do it.

And it's how they introduce it in the very beginning of the paper, which is that the minimization of the loss function

of the free energy, expected free energy, variational free energy, can be achieved in multiple ways.

Through immediate inference about the hidden states of the world, explaining perception, updating a global world model to make better predictions, slower parametric inference updates that have the interpretation of learning, and when action is a variable that we have agency over,

It introduces some complexities like the need to have preference and the need to specify a time horizon and to have uncertainty about the consequences of action, right?

Wouldn't it be easy if we were just watching the movie and our actions didn't have any influence upon the story that we saw?

It'd be a different inference.

And finally through action to sample sensory data from the world that conforms to the predictions, potentially providing this integrated account of adaptive behavior and control.

So it took us 50 equations at the general before we could jump into action, but it's a deflationary approach to action because action is a parameter in a broader cognitive and computational framework.

Okay.

Steven.


SPEAKER_00:
Thanks, Daniel.

Yeah, that's really helpful.

Actually, going back to the example of water as well, you can even bring in the higher level of affect in the sense that one thing that's really hard with water compared with a lot of other things when you heat up oil and stuff is you can have the water boil and you've probably had it.

And you can have a thermometer theoretically saying 100, but the steam can go above 100.

But it's very hard to really  it can trick you, right?

It's like it's hard to  it's actually quite hard to get  I find it hard to get my head around steam being hotter than water because it's kind of 

just kind of how I know, you just think of it as being 100 degrees water, right?

And it's almost like if you were to get into that situation or after I burnt myself one time or there's almost, maybe I just start to have a feeling of fear if I have a sense that I'm somewhere where

what might be coming out is high temperature steam, right?

And that will override the fact that perceptually, I haven't got a lot of ways to easily sort of gauge that visually, unlike, say, a ring getting red and, you know, other things.


SPEAKER_02:
Okay.

Here's what that makes me think of.

Sometimes we...

have indirect cues that reduce our uncertainty about something that's not directly observed, like the mood ring.

If everybody was wearing a mood ring and that was an accurate predictor, you know, that would be this other alternate world.

And so there are certain kinds of variables that are more like the magical mood ring or like the temperature of metal within a range, because I mean,

it's only going to start to get glowing when it's super, super hot.

So it doesn't help you differentiate whether it's free, like at zero C or 50 C or 200 C probably even.

But there's some range for certain things in the world where we can use indirect sensory cues.

Affect is not quite here in this model, in this graph.

but we've talked a lot about how uncertainty can be understood as like anxiety and or negative valence.

In the world of reward maximization, less reward is bad.

In the reward, in the world of precision optimization,

Excess variance is bad.

Not any variance.

It's not a simple, you know, destroy the variance estimators.

It's about the appropriate applications of variance.

But excess variance, more variability than expected.

You told me it was 10 plus or minus one, but I just pulled three, 50, negative 80.

That's confusing.

It's more confusing than I thought.

So then some of these mean and error estimators in a computational model might have, this could be a mean estimator on valence, or one could choose to write a paper where an error estimator is framed as being an anxiety parameter.

And that's always that jump between what any parameter is in a graphical model

and anything about the world or attaching it to some word because, oh, it's anxiety.

Well, then that makes me think of this other situation.

So you can send someone really too easily down a road of associations when the deflationary perspective is just stating what it is.

Okay, fun.

Didn't expect that we would go

this way with the back prop but blue mentioned it last time and and it is important to uh bring up and also it connects to modern methods for training neural network architectures and that's what a lot of the author's research has focused on so let's just continue to explore action a little bit they reiterate that when we're thinking about

active inference about inference involving action.

So cybernetics, control theory, any area where some parameters represent observations or latent states and other parameters represent active things.

The first way that you can minimize prediction error

is to update predictions to match sensory data corresponding to perception.

And the second is to take action.

And earlier from the introduction of the paper, they kind of did a little minimum of two even on this, which is to split out perception as rapid inference and learning as a slower updating.

But we've talked about the continuity between perception and learning.

Like if you're seeing the ball move across the screen,

Is that perceiving the movement of the ball or is it learning a parameter representing the location of the ball?

So whether we call something more perceptual or more learning oriented, and even abstract learning can be seen as like perception in interior spaces, which has been the focus of a lot of work by Fields and Levin with competency in navigating these abstract spaces.

But perception sometimes brings us a little bit

not too close, but a little bit just linguistically to like our experience of perception when the phenomenology and the qualia is not what is in play here.

It's about parameter updating in the model.

Yeah, Dean.


SPEAKER_01:
Do you think, Daniel, it's about sample size, but sample size is related to maybe the time commitment?

Like if I'm inferring something, that's a very, very short volume of sample versus prediction, which is slightly or relatively larger volume of time commitment to move to that

volume and then modeling, for example, which takes the perception and tries to build something material from that.

Do you think that that counts here?

Because at least that's what I was telling participants we had to take under consideration was sort of the time factor of the sampling process.


SPEAKER_02:
sure i think um if we allow inference to um be just broad and including everything then that's what it is but yeah just in its connotation inference using a model often has the um it's sort of like the machine is ready the model is ready and we're gonna do inference the new patient comes in with the biometric data we're gonna do inference using this model and we're gonna predict their risk of the diagnosis

That's the fast perceptual timeframe.

Learning is, unless it's one-shot learning, is taking in multiple patients and updating the parameters of those models.

Now, those two stages can be alternated like an expectation maximization or in the tale of two densities, but that doesn't get away from the fact that there's sort of this like one data point mode, like plug and chug inference perception mode,

variational free energy.

And then there's a little bit more of like the updating the model parameters.

It's like, are you tuning the engine or are you using the engine loosely in the non-action space in this first way to minimize prediction error?

Those given the ongoing stream of patients coming to your office, there's two ways to reduce your surprise.

Those are the perceptual and learning continuum, which does come down to the specifics about the sample size and the architecture of learning and perception.

You could also reduce surprise by choosing what to sample and or modifying the world.

So that's how they bring action in.

And...

It's very interesting to think how many pages of prose I've gone into wondering where action fits in.

And there's still many more pages to go in terms of where does Formalism 51 take us?

Have we cashed the check of pragmatism and activism, extended cognition and so on?

Like what is 51 with respect to all the qualitative insights of action with all of its depth?

What is 51?

A short answer might be that it's like the Y equals MX plus B.

with respect to all the ways that linear models are used in the world.

This is just like a sort of skeleton archetype that is just the trace or a hint for someone who then wants to analyze specific actions over specific timeframes for specific cognitive computational models.

But that's where 51 takes us, Stephen.


SPEAKER_00:
yeah one other piece that could be kind of interesting to throw in there as well as matching my uh the sensory uh data with my predictions i could reimagine my reality to explain my beliefs which in a way would mean i create my own sensory data

by proxy if that makes sense which is a kind of an interesting slight add-on to that but um it's it's the same idea because you're not strictly taking action you're taking actions to match sensory data but not necessarily the real world sensory data it could be the the kind of uh you know the imagined world data i i am seeing an angel coming over the top of


SPEAKER_02:
that hill now you know yeah one could engage in um mental reverie and it would either be reducing their surprise in the long run or that system would fail to persist in an entropic world and people have made

all kinds of arguments like in evolutionary psychology about the basis of why we perceive or believe different things along similar arguments so definitely makes sense let's just look a little bit in 452 which we didn't um previously get to but of course anyone with any other like questions in the live chat welcome to do so so

Here in 26, Livestream 26, we talked about PID control.

We talked about integrator chains and about generalized coordinates of motion.

So just a reminder on what 26 looked like.

We had from left to right is time.

These are different time points.

And then at each time point, there's this vector, it's like a stack.

reflecting the observables, and then also things that are latent states.

They're not observables of the world, real or imagined.

They're the derivatives of how the observables are changing.

And those can be the position, it's like the zeroth derivative, but then also velocity, jerk, so on, acceleration, so on.

So we looked at that in 26, but it wasn't connected

formally to predictive processing, predictive coding.

PID control is restated in 53 using the formalisms, using the notation that is going to prepare for the merge with predictive coding, like A for action, epsilon for errors.

And so the error term is included in all of the three levels.

To obtain equivalence to predictive coding, we utilize a linear identity generative model with three levels of dynamical orders.

So the joint distribution of all the O's, all the observables, and all the X's through time is going to be this factorized expression.

So the distribution of the observation conditioned on the latent state, and then that is conditionally independent of, so it allows it to be factorized and calculated out separately.

Here's like the first derivatives, second derivatives with O double prime and X double prime.

Here is X conditioned upon its own derivative,

the derivative of x conditioned on its derivative, and the sort of root node is the double derivative of x, because this one like stands alone.

That's like where our model kind of taps out.

That's equivalent to node one here.

Like there's no variance term leading to one.

There could be, but then that would be a different model.

So these equations in 54,

restate this dynamical model and introduce um omega which i believe would be an error term this w and also the muse which is the desired set point for x at that order so at the first second or third um derivative what what is desired there um

then that allows so so this is like a graph where instead of these being different things like what if what if kind of mixing architectures a little bit because the generalized coordinates doesn't have errors interleaving it just has the

state the first derivative the second derivative and so on um rather than interleaving error terms but we can imagine another similar graph where as you back propagate it's like higher and higher derivatives that's the integrator chain but i guess in 55 56 and 57 and 58 they do something similar to what we had done with the generic computation graph

or to the predictive coding architecture with the interleaving of the means and the errors.

This is just that on the graph architecture where the nodes have this interpretation of being derivatives of one another.

What is there to say?

That's an interesting connection.


SPEAKER_01:
Just bringing back flashbacks to try to get my head around 500-level economics courses and when you'd have somebody walk in and start pounding stuff up on a whiteboard on second, third, and fourth derivatives and just feeling like you'd been tossed into a blender and you had no way of avoiding the blades.

Yeah.

I'm glad that there's people that can do this stuff, but I would have to dedicate way more time to trying to parse what the heck they're talking about because it's a wall to me.

It's really hard.


SPEAKER_02:
Yeah, I feel you.

Maybe for the last few minutes, we can kind of pull back to...

okay as we swoop out of this bathtub out of the dot too into the great beyond like

What have we taken with us?

Why was this lengthy journey through predictive coding and the 60 formalisms and what Maria brought with the thousands of years of philosophy and history on perception?

So where are we and how are we different now and going forward than we were before we started 43?


SPEAKER_01:
I'll say what it's done for me.

First of all, bringing up the historical piece of it was really, really helpful because this didn't just sort of come up yesterday or in the last couple of years.

It's been around for a while.

I think for me, I wouldn't have seen predictive coding as a sort of formalized way of retracing a navigational route, but that's what it screamed to me.

And after the point one, I...

and hearing you and Blue talk about it, when the question around back propagation came up, that's when I sort of did a deeper dive on that.

And again, I wasn't picking up on how to do that path retracement.

The formalisms weren't really addressing what I thought was being said.

I think what today helped me with is

sort of examining if you don't have traces laid down that are material that you can see, if you weren't actively part of the building of the path, that there's still some graphical, statistical ways of organizing information to give you a better sense of

Okay, so you didn't lay the breadcrumbs down, but we can still give you a better sense of where you came from as a result of being able to use this kind of formalism.

I mean, honestly, Daniel, 53, 53?

Come on.

I mean, if you're one of the people that was able to write those 53 out and actually

presented in paper form.

I mean, props to you.

You get many thank yous, but holy mackerel.

Other than the fact that there is some evidence now that you can retrace, even though you don't have necessarily those markers or

You didn't self-correct.

You weren't aware of how you self-corrected, whereas the predictive coding at least gives you an opportunity to maybe retrace that back, even though you didn't spend a lot of time trying to organize the evidence.

The other thing that it does is I think it reinforces the idea that the way that these live streams are structured, which is just introduce the paper, just give the paper its

it's opening, like open up the box in the point two, you can actually go into one particular area of the unknown, which for me was the back propagation, which you and Stephen helped me with.

In the point two, we are still doing the part where

The .2 could always, even if you didn't have a record of the .1 and the .0, this paper told me that you would still have some way of knowing where that elusive .0 exists relative to this live stream today.

And again, I could be overfitting, but I don't think I am.

Otherwise, why would you string out 53 operations?


SPEAKER_02:
A lot of interesting stuff there.


SPEAKER_01:
Because this is level.

This is the exact same.

I mean, what we're doing in the live stream and the levels that they're talking about, they affect each other in exactly the same way.

It's diagrammed out here as a stack.

And we tend to think of ours because it's over multiple days as sort of a horizontal stack.


SPEAKER_02:
continuation but they're essentially one and the same oh that's that's very interesting um so it's almost like well there's the dot zero and the one of the two they can go both ways so in in the temporal sense

The dot zero is a precursor temporarily to the dot one, the dot two.

So for the YouTube channel subscriber, this is their observation.

Oh, 43.0.

Oh, 43.1 has been live streamed.

43.2 has been live streamed.

But what is like the derivative that sort of sets the initial conditions for dot two?

It's the dot one.


SPEAKER_01:
Yeah.


SPEAKER_02:
And then what of course sets for the dot one, dot zero.

And so there's almost this sense in which as we dampen our uncertainty through action and inference moving forward in a world that's always continually vibrating and transcending our model,

like it pushes the earlier actions that we take that high school teacher that book that you read all the way back even beyond one's own corporal birth as like parameters that

dampen or specify if one doesn't want to think about dampening the position acceleration and velocity and so on um they're like higher and higher chains and that means that they have like they're further and further away from where the rubber hits the road for the YouTube subscriber on the outside but they're absolutely like part of the process by which something comes to be

then the dot two you said can exist without the dot one and then then the first time it made me think of like the negative numbers like even before the zero what is what is there before zero what happens before zero is there a negative number there's no number that's positive that's smaller but what is is it is it a letter is it a shape like

Is it its own big bang?

It's like, and there's a sense that it is, and there's a sense that it isn't.


SPEAKER_01:
Part of it's the pre-reading.

The other thing that I think maybe doesn't sort of register explicitly is that

The error propagation in the way that we do the live streams remains.

There are some corrections and there are some, there is a sort of a gradual movement towards a bit of minimization.

But every time we come together and try to figure out, we don't come from a position of we figured it out.

We come from a position of, well, what will we do with this today?

So we retain that error propagation.

And I think that that's not convention.

Convention is why would we spend time on something where we leave a bunch of error baked in?

But I think, again, if we're going to differentiate, if we're going to truly integrate, if we're going to be able to say that you can have a point two existing

without a point one and a point zero?

Of course you can.

But how superficial is that relative to the method because of the architecture, because of the way it's structured, how much deeper you can actually go and how many times you can bump into a dead end, immediately find out how quickly you're going or going off the rails, so to speak, because I can't remember which one of the live streams was that you had the train moving down the track.

But you can find out whether or not something is a dead end quickly and re-correct.

That's what that retracing part today, I think, is one of the, well, it's the takeaway from this set of live streams.

And I wouldn't have anticipated, based on the title of the paper,

that that retracement piece was going to be the thing that was going to sort of percolate and make the most sense in terms of why the structure is, yes, there's an optimizing piece to it, but there's also a piece that allows for mistakes.

I mean, sometimes it's very consequential, but lots of times,

The consequence is what's minimized, not the error.

And I think that's what the architecture affords.

You want to talk about affordances.


SPEAKER_02:
So when you said the consequence is minimized and not the error, that makes me think about high-reliability organizations.

Of course...

One way to minimize the consequences of error would be to never have errors.

Zero tolerance for anything.

But that is challenged in a world, again, that's always throwing us challenges.

So in that setting, it's almost like the observation...

Here, the one that we actually want to reduce our loss function on is not the so-called proxy of error occurring, but we could reduce our loss function on function, on the thing working, and just say we are minimizing the consequences of all this upstream stuff, stuff that might have the interpretation of error and things that might not.

They're just nodes in this graph.

And then what is going to dampen

surprise about the consequences and so like if you say you know anything that you say to me right now uh i'm going to accept it and believe it and then it's like in a in a trust space that opens up there to be like a communication that goes deep if it's being honestly said

and that's because like what's being minimized like don't tell me something surprising it's like my consequence you can have precision on my consequence that's one direction of the conversation and that is what allows that second direction to really come into play

with the transmission of variability, rather than, well, how am I going to reduce my uncertainty about action relative to what the person is going to tell me?

I won't talk to them, or I'll tell them what to tell me, or I'll leave them in the dark about how I'm going to act, and then they'll have uncertainty about how I'll act if they say something even trivial.

And so I think this really spans many levels and ways here because there's nested levels and deep chains.

But I agree.

I think on the more topical, if such a thing could exist, some of the takeaways that I had also really appreciated Maria providing a lot of the philosophical context.

And then that one part where we're like,

Wasn't this a meme like thousands of years ago and in many cultures in the world?

What is it that still makes the predictive mind and surfing uncertainty and being you, what makes these novel and surprising and even somehow counter-cultural, what is happening there?

What chain is leading to that where the idea that we're generators with agency rather than passive receivers, why is that?

such a hill to climb and die on that's one piece i absolutely take away then it was awesome with blue um to work on this in the dot one where we had like on the bottom left the qualitative philosophical insights and and connecting to technical and formal insights on the bottom right

biological systems and it's sort of like where are we engaging deeply with one of these vertices where are we engaging deeply with an edge where are we trying to grab two vertices and the edge or go around the horn or where are we trying to really go for the triple play um every baseball home base inference and action

um in the bottom of the ninth of the dot two it's it's a full count and it's a power play that and then also i think the last piece that the kind of takeaway is just when the computational and cognitive framework are defined generally enough

like at the level of a graphical network, which opens up message passing, variational inference, back propagation of error, predictive processing architectures, hybrids of all of these approaches, then action is deflated.

or it's inflated maybe, or whatever it is that we even say, it becomes composable and tractable to discuss open-ended combinations of perception, cognition, and action and impact, which is the home base in the niche.

And so if even that model has to be structurally revised, like we end up having another way of framing entity and the niche interaction,

It may fit within this framework or it may necessitate revision of the framework, but within the entity niche interaction space and the nested systems and the counterfactual cognitive systems, there's almost like less and less to action than it seems.

It doesn't remove the challenge of action.

It reduces something else that I'm not quite sure about what it is.

Like it doesn't make it easy

but something is being removed as we're learning and working through this but it doesn't make the climb well it makes the climb different to think differently but it doesn't move doesn't mean that fewer foot pounds of force have to be applied but there is something different about the navigation around action and inference

via active inference that I think is will continue to develop for a long time okay Dean and also Steven thanks a lot wasn't sure there for a second at the beginning what would happen I gotta thank you again Daniel because


SPEAKER_01:
Obviously, you did understand, have a reasonable understanding far more than mine in terms of what that back propagation implication was in terms of marrying it to the predictive coding.

So didn't mean to put you into the tutor's seat today, but appreciate the fact that you were able to sort of pull it apart and then put it back together in a way that made a lot more sense than just reading it off the page.

So thank you, sir.


SPEAKER_02:
Thank you, Dean.

See you later.


SPEAKER_01:
All right.


SPEAKER_03:
Take care.

Bye.
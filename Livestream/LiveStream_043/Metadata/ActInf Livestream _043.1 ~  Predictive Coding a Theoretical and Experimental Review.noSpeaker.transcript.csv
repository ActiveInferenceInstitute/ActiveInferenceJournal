"Speaker Name","Start Time","End Time","Transcript"
"Unknown","00;00;29;14","00;01;02;10","Right. Hello and welcome, everyone. It is just adjusting things. It is May 4th. May the fourth of 20 to be with you. And we're here in active live stream number 43.12. I don't know how you do it with always joining during the theme song welcome to the Active Lab. We're a participatory online lab that is communicating, learning and practicing applied active inference."
"Unknown","00;01;02;23","00;01;31;11","You can find us at links on this slide. This is recorded in an archived live stream, so please provide us with feedback so we can improve our work all backgrounds and perspectives are welcome and we'll be following video etiquette for live streams. Check out active inference dot org if you want to learn more about what's happening in the lab and get involved and participate in the live streams or any other activities all right."
"Unknown","00;01;31;21","00;02;03;29","Well, we're here in live stream number 43.1 and are continuing the discussion of the paper predictive coding a theoretical and experimental review by Barron Milledge and yourself and Christopher Buckley in 43.0 with Maria We went over some of the background and context and did like a first pass on various aspects of the paper, but by no means exhaustive it because it's a long and very intricate paper."
"Unknown","00;02;03;29","00;02;29;02","And also it touches on a ton of other areas of more general interest. So there's a lot to discuss and in the 43.1 today, we'll see where things go. If you're watching live, it would be awesome to have any questions in the live chat. Otherwise we will just start with some introductions and pick up with whatever people are finding salient and exciting."
"Unknown","00;02;29;25","00;03;11;00","So we'll just start by saying hello and going from there. I'm Daniel and I'm a researcher in California, and thanks a lot blue for joining I'm Blue. I'm a researcher in New Mexico. And yeah, I'm excited to be here today. This paper was really intricate and detailed and provided a lot of background that I don't have because there are so many active papers that I want to read, but I don't have time to get to them all because I like to read things very thoroughly and slowly and carefully and look up the things that I don't know."
"Unknown","00;03;11;00","00;03;40;08","And the references traveled down the scientific rabbit hole, so to speak. So yeah, this was a good paper that kind of helped me know shirk all those active responsibilities that I have and came out with more questions than answers, which is always a good sign of a good paper. Awesome. Well, how about going into the paper what questions did you have?"
"Unknown","00;03;40;08","00;04;26;19","And maybe we'll get to how our questions evolved, but what were you expecting to find in the paper and what questions were motivating you to seek it? So I always have had a hard time. Like, what is the difference between predictive coding variational Bayes and Actin Right? And like, I was really hoping that the paper would kind of lay that out for me and like know so like, I mean, I kind of got a good historical overview of how they all are interlocking and overlapping which I didn't know before because, you know, all these things, none of them are simple and there are intricate details in all of them."
"Unknown","00;04;26;20","00;05;05;08","Like you know, that I don't know all the details, ins and outs, but I think that this gave a very good like historical overview of how these things kind of layer into a nice little sandwich great. Definitely a good starting place. Let's also add Bayesian brain there to the mix and start to look at maybe some similarities too, because they're not exactly like addressing the same area and they're compatible, often more compatible than not."
"Unknown","00;05;05;08","00;05;34;05","So getting at what distinguishes them is very important. And like one way that that happened and it came up in the dot zero was so much of the paper was dealing with inference on the relationship between observations and on observed latent states. So think like trying to infer the temperature of the room given the thermometer readings and that was so much of the paper."
"Unknown","00;05;34;16","00;06;17;07","And then action comes into the picture in section four or five. It's like predictive coding the authors write can also be extended to include action allowing for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms. But that's 50 equations deep. And so it was very interesting to see like if those 50 equations are kind of like the stem, I know that the equations are not sequential and building on each other in all of these cases, but then action gets tucked into a bigger picture very seamlessly."
"Unknown","00;06;17;16","00;06;46;26","And I think that speaks to the the relevance of distinguishing how they're similar and difference and why it matters, I guess. So how should we even begin to look at that? So even action actually came up earlier in the paper before 4.5. So action came up in a section 2.3 with dynamical predictive coding and generalized coordinates. And this I thought was a really good example."
"Unknown","00;06;47;00","00;07;15;27","Like I was craving and still am craving like a really simple example. Like, like it's one thing and we talk about the equations, we go through all the letters and compare this and that, but it would be really great to see a super simple example works to like from start to finish using all of these observable states latency, complexity, accuracy, like it would be really great to see that at some point."
"Unknown","00;07;16;28","00;07;43;18","And if you're listening to this livestream and want to come on and give like a guest tutorial on like how do we actually work these equations into some very simple problem that we can solve in real life? Like I would be more than happy to facilitate that presentation. Yeah. Because I can't even like I can't jump from like all these equations into like how does that apply?"
"Unknown","00;07;45;07","00;08;19;16","But even in 2.3, this was kind of a good a good a good ish example, a real life example about like inferring, you know, the velocity and the acceleration and the jerk just give in that the observed coordinates. So that was kind of and it talks here about also free action, which I've not heard before. So instead of minimizing the variation of free energy, it says we must instead minimize the free action, which is F bar."
"Unknown","00;08;20;18","00;08;43;09","And I've not seen or heard it referred to in that way, but it's modeling like time series of observations so like, you know, you know, the coordinates set time one time to time, three of the swinging pendulum, for example. So, you know, this coordinates and you can kind of guess you know, how fast is the objects moving. And you can infer these other is other variables."
"Unknown","00;08;43;09","00;09;14;05","So I haven't seen that reaction, but it did even come up before four or five yes. And it speaks to that expected free energy EAFE, which is what do you expect the free energy to be? And then there's the free energy of the expected future, which one of the authors has worked on, which is kind of a slightly different variant and then here's another way to look at the free energy of some future time steps."
"Unknown","00;09;14;19","00;09;47;06","And and it speaks to that difference between the variational free energy, which is like what you're able to calculate right now with the information at hand and looking backwards. And so the more perceptual aspects of inference can be calculated using the variational free energy approach. But then any time the future is coming into play, there's uncertainty about the observations as well as even uncertainty about what actions one will take, let alone the consequences of the actions that one will take."
"Unknown","00;09;47;15","00;10;23;02","In the future. So we do need a different way to calculate it. And then as you're kind of getting out, this is a different way than we've seen it before with a minimization of free action. But let's return to this page and think about how we can distinguish with like a Venn diagram or something that helps us understand how these core ideas are linked one couple that we can highlight is predictive coding and predictive processing."
"Unknown","00;10;23;17","00;11;04;01","Do you have any thoughts or questions on that? Yeah, so maybe not a Venn diagram, but like maybe we should build perhaps like a pyramid or like a level building because I think, you know, based on the, the what I read in the paper I think really predictive coding kind of starts it all off. And also, b it makes sense to make a layer cake like being that these all kind of start off with the layers of cortical processing that happen quite literally in the brain."
"Unknown","00;11;04;11","00;11;33;17","So I think making the layer cake maybe makes the most sense. OK, so we're in the kitchen where we're in the active restaurant kitchen and what are we going to build in or what is the bottom of this layer cake? So I think the bottom at least so so there might be two layers on the bottom one from Helmholtz."
"Unknown","00;11;34;00","00;12;26;17","So we can start with like the perception as unconscious inference view of how Helmholtz and then predictive coding as described by Ro and Ballard in their description of the visual system in the visual cortex. OK, and again, kind of calling back to the dot zero, the two qualitative philosophical ideas that the authors invoke in that Maria helped us unpack were Helmholtz notion of perception as unconscious inference and Kant's concentration that a priori or beforehand structure like prior structure is needed to make sense of sensory data."
"Unknown","00;12;27;02","00;13;10;02","So we're going to have one bottom layer of the cake with Helmholtz and then also with the Katia. So this is going to be our qualitative philosophical layer of the cake, these two notions. And then you mentioned another critical work which was round balance 1999 work, building on a broader history of applying predictive and anticipatory approaches to different neural systems and in the zero, the two neural systems that we looked at were first the retina system with the Sreenivasan 1982."
"Unknown","00;13;10;11","00;13;45;07","And this is looking at retinal physiology and like the electrophysiology of photons hitting the retina and then another area where predictive approaches especially early on were getting built out around and towards was the cortical hierarchy and the different layers in the mammalian cortex. So these two anatomical exemplars of some micro circuitry, some histology, that is compatible with a predictive approach."
"Unknown","00;13;45;07","00;14;41;02","OK, so we have these two basal layers, but we even had more basal layers, but we have biological systems that are doing predictive things and that includes the retinal as well as the cortex also, of course, if anyone has questions, we'll get to it in the right way. So please ask any questions or doing any comments. So we have the retina, the cortex as examples, but there's others, OK, and then if there's a third layer that we can add to the bottom, I think it would just be Bayesian statistics because so I think it comes maybe before that also or maybe like maybe now maybe Bayesian statistics is like a a layer too."
"Unknown","00;14;41;02","00;15;20;25","But like I think before the Bayesian statistics and even before biological systems doing prediction, there's the information theory component that I think is important also. OK, so well we have this is qualitative and philosophical then we have biological and actual and then we have a formal area and that'll be information and base information. Yeah, the information goes under the base."
"Unknown","00;15;20;25","00;15;58;13","I think just like the Helmholtz comes under the count. Right. I think it's the really the relationship between information and probability, which we started to explore in or we explored a lot in the genetic quantum systems livestream number 40. That was 40 I think so many streams ago. OK, this is fun and I think we're we're going to start building this into a tasty structure."
"Unknown","00;15;58;25","00;16;27;11","So we have the philosophical coming in on the left, and that's one onramp that just like we had discussed earlier, like this is something that has been almost imminently available to thinkers for thousands of years. These are the kinds of qualitative claims that anyone can experience, whether by thinking about the blind spot or about how their sense making in response to some stimuli."
"Unknown","00;16;27;18","00;16;56;08","This is just the idea that the sensory observations are first off, not the direct contact with the world. Like we're not seeing the lightning strike event. We're seeing photons hit the retina and sound hit the eardrum. And so we're receiving sensory input. That's not kind of Plato's cave angle and then contact Helmholtz elaborated on that to include this idea of unconscious inference that requires a priori structure or like what we would call like a Bayesian prior."
"Unknown","00;16;57;02","00;17;24;06","Then we have this biological area and these are just reflecting natural systems that are exhibiting some kind of behavior. Like we can think about the 1999 paper of round power themselves about the functional interpretation of extra classical receptive field effects. And we talked about that in the zero, like the classical effects are kind of the simple normative ones."
"Unknown","00;17;24;07","00;18;01;11","Like the joke is it's classical because of the papers in the order they were published, but that doesn't like make it even true, let alone the best model. The classical is just like, it's like classic rock. You might like other genres, but then there's the classics or whatever so that's the biological systems that are exhibiting certain kinds of outcomes and then we have these two formal or quantitative areas and those are the Bayesian statistics and probability as well as information theory."
"Unknown","00;18;01;26","00;18;12;28","And they're definitely linked as well. But we'll just leave them as kind of two conjoined layers here. OK, now where do we go from here?"
"Unknown","00;18;16;25","00;19;04;18","Well, this is not the question, but so maybe predictive coding and predictive processing and like really what's the the difference between these two? I think predictive coding came from neuroscience. So maybe that layers on top of the biological part, the biological piece of the cake and then predictive processing might go on to the quantitative piece also. Yeah. And it really is what Maria highlighted and drew out in the dot zero, which was using a quote from an Andy Clark book."
"Unknown","00;19;04;29","00;19;33;18","And Clark wrote that predictive processing is not simply the use of the data compression strategy known as predictive coding. So at least in this take, predictive coding is like MP for it's like an encoding strategy, which is also why we connected it to frame different seeing and video encoding. But predictive coding is going to be something that's data oriented and it's related to information theory, compression information encapsulation."
"Unknown","00;19;34;03","00;20;01;07","And then Clark is writing predictive processing is not simply that. And so Maria spoke to how another way to think about the difference between predictive coding and predictive processing is to use coding for the formalisms and implementations and processing for the philosophical understanding that prediction is the basis of signal interpretation as opposed to merely recognition or descriptive models."
"Unknown","00;20;01;10","00;20;40;07","So if anyone has like a thought on that, then, you know, put it in live chat or come get involved, but let's work with that delineation going forward. So predictive processing is going to be somehow related to the formal sorry, predictive coding is going to be more close to the formal whether it's predictive coding in a biological context or whether it's going to be in a philosophical context or something it's going to be more on the formal side, whereas predictive processing is going to be where a little bit closer to the philosophical."
"Unknown","00;20;40;23","00;21;15;00","Yeah, I think so. And then maybe like does the Bayesian brain like make up the the third part of our predictive coding layers over the Bayesian statistics? And predictive processing layers over the philosophical than does the Bayesian brain somehow make the in-between sandwich that goes over the biological OK, OK, so let's see, we have predictive processing as being a philosophical approach."
"Unknown","00;21;16;11","00;21;56;22","And in consideration of these philosophic goal memes and themes in the biological case, which we're certainly bound to then we can have predictive coding as kind of the formal way that these predictive anticipatory systems are implementing predictive processing. And then what is the Bayesian? It seems pretty fair to put it at the intersection between Bayesian and Brain, but what is the Bayesian brain to you?"
"Unknown","00;22;01;24","00;22;51;29","So to me, the Bayesian brain is kind of, you know, I'm trying to look back to the paper, to their description. But in my recollection, recollection of the Bayesian brain, which is like probably my first introduction to Bayesian statistics, I was like, Oh, this is great. And it wasn't even the Bayesian thing. It was like the Jane in interpretation of the Bayesian Brain paper, but like that paper was like probably the my like turn on to Bayesian statistics and implementing this and so in my mind, the Bayesian brain is just this idea that the brain uses Bayesian reasoning or Bayesian updating to process information and, and go forward."
"Unknown","00;22;52;01","00;23;19;29","So but that's probably a very simplified interpretation I think there's going to be one more realist interpretation, like the brain is doing Bayesian stuff, and then there's the more instrumentalist interpretation we can use Bayesian statistics to model what it is that the brain is doing. So is Bayesian the territory or the map of the brain. But we'll just leave it as an edge for now."
"Unknown","00;23;20;18","00;24;05;08","So I'll read a question from Deane, and then I think we're going to be exploring a lot where action and active inference comes into play. So Blue Deane writes, What do you think of this as coding you are doing something in coding as processing. You determine by comparing processing and or not processing as you're doing. So process entails processing types what I'm hearing there is encoding doesn't require two arguments."
"Unknown","00;24;05;23","00;24;43;25","It just takes one. You take one file and you zip it. You've encoded that in a way so predictive coding can encode and or implement some type of informational algorithm without a reference or a comparator, whereas Deane saying As processing, you determine not with this single arguments plug and chug, but you determine by comparing as you're doing. And so predictive processing, it does have that sort of top down and bottom up as it's often visually represented."
"Unknown","00;24;43;25","00;25;14;16","But processing is entailing that full stack and therefore the processing is requiring something like a minimum of two to have consideration. If you're processing nuts, then you're sorting them into two different kinds or you're removing something from something else about those. Like you're doing something that has composition ology or some type of multiplicity to it. It's not just something you can take that one file and just zip it."
"Unknown","00;25;14;26","00;25;43;13","So that's a great point. Dean, I think I'll add two little arrows here yes. OK, yes. Blue. Let me share the slides. Thank you. I'm left in the dark here. I definitely. Yeah, agree. With Dean's point about the coding versus processing. Like, I can write a whole lot of things, right? Like, I can take a whole lot of notes and that's coding."
"Unknown","00;25;43;13","00;26;08;16","That's like encoding the information from our conversation, this live stream. But like, until I do something with those notes, like turn them into elements of a paper that I want to write or something like this, like I have not processed what I have encoded until I take them forward. To a forward step. So I think like there is kind of this temporal implication with processing versus coding."
"Unknown","00;26;09;06","00;26;35;05","Hmm. Yeah. Also like the timeliness of processing, like you want to process the food before it has become bad or something. So, OK, we have predictive coding again, these are just tentative slide play. If anyone thinks differently, they can join or write a comment, but predictive coding might entail something of a unique directionality, just like a dot sip."
"Unknown","00;26;35;05","00;27;09;21","And you say, well, somebody will unzip it later, but we can encode it without that part of the loop being acquired. And then we have this two directional relationship with predictive processing being like top down and bottom up OK, what do you see now? Or I can bring up something yeah. So I'm not really seeing anything. So why don't you bring up what what is on your mind or what you're thinking of where is action?"
"Unknown","00;27;10;16","00;27;47;29","And then I think that'll start to walk us towards where active inferences because of the temporal nature of processing. I think action might start in the processing elements like even in something like mental action, like people do like thought correction or thought remediation. So like if you're you you know, you have this prediction and it like surfaces in your mind, but then like you carets the prediction or like you, you know, you think you see a snake on the floor and then say, oh, that's a snake."
"Unknown","00;27;47;29","00;28;13;24","But then you look, let me see that snake like and then you have to zoom in, look at it closer, and it ends up being like a coiled up rope. Right? And so it's not really a snake, but like you direct your attention because of some prediction and then can continue to process that prediction. So so possibly action starts in the processing in the processing area or maybe it's there in the middle."
"Unknown","00;28;13;24","00;28;38;21","Well, I'm going to start it in the middle. We can see where it goes because I think it's going to have an important edge to each of these three areas. So there's a few ways to represent it, of course. So how does action relate to biological systems? Or first, let's let's start with some philosophical frameworks that highlight the importance of action."
"Unknown","00;28;39;12","00;29;10;04","So this includes and activism embodied the forties, fifties, seventies, et cetera, et cetera, et cetera, everything involving like this sort of embodied, enacted and cultured, extended, et cetera, approach to cognition and philosophy. And those can be quality. So putting that edge here, these are different qualitative and philosophical areas yes. They can be formalized. We'll get to that edge."
"Unknown","00;29;10;14","00;29;26;26","But these are qualitative memes that come from this philosophical area, and they're going to draw us to action. OK, now how about biological systems and action? What do you think about when you think of biology and action?"
"Unknown","00;29;29;19","00;29;44;20","I don't know. I think like those things are kind of inextricably linked in my mind. Because, like, you can't have life without having some action. I mean, cells replicate yeah."
"Unknown","00;29;46;28","00;30;16;02","I think this is going to be just something that doesn't demand to be that but is totally true. Biological systems are active there. It's active matter and life is this multi scale organization of activity. So there's many ways biological systems are doing action. And what are some areas that are formal, whether or not they have to do with biology or any philosophy?"
"Unknown","00;30;16;19","00;30;41;16","Can you name a few areas of like formal theory and science that you think relate to action? So I mean, I think just that's maybe more related to processing. But, but as I said earlier, I think processing is related to action. But like Bayesian inference and and Variational inference, I think which also tie in a lot to active inference."
"Unknown","00;30;42;00","00;31;24;13","I think that they come in there in the processing, slash processing through action or action through processing. OK, great. So Bayesian planning as inference as well as areas like cybernetics control theory, and different formal ways of modeling active systems and modeling decision making these could be related to a biological system or not, and they can be drawing on a philosophical framework implicitly, explicitly or not."
"Unknown","00;31;25;08","00;31;46;13","All right. So now this is starting to get fun. You added another term there. I think this will merit a detour. But then I return to here, which is the variational Bayesian approach. So what do you think Variational is meaning or doing here?"
"Unknown","00;31;49;29","00;32;25;19","So it's interesting and part of what makes this complicated paper and also like a complicated detour that like the authors cast predictive coding as variational inference, like even though we've kind of made this distinction between predictive coding and predictive processing where coding does just it's like a one way street and processing is a two way street. The authors nevertheless use the term predictive coding as variational inference."
"Unknown","00;32;26;14","00;32;58;12","And this predictive coding or this variational inference. I think the the key construct here and that's probably I don't know, it overlaps with with Bayesian brain maybe too is the idea of having a model of of the data generating process. And so I think variational inference kind of throws the idea of, of a, of a model into the loop."
"Unknown","00;33;00;07","00;33;40;12","OK, so let's zoom in on this blue corner of formal and quantitative areas. And so these can be taken in an action independent way. They can be explicitly about action like planning as inference, or they could just more implicitly rest on inference. I'm sorry, more implicitly rest upon action. For example, in the case of we're inferring the hidden state of the temperature of the room and we're observing the thermometer that doesn't have pie it doesn't have a it doesn't have action in that model."
"Unknown","00;33;40;19","00;34;03;16","It might just have those two parameters of like the rooms, temperature, hidden state and the observations of a thermometer. But then we can kind of take a step back and see. All right. Well, there's the person who's engaging in this experimental action, the person's ocular motor, zooming in on the thermometer maybe we can just abstract away, but action is always baked into it because we're talking about active systems."
"Unknown","00;34;04;02","00;34;31;00","But let's just put some of that aside for a second and take a quick detour to talk about variational inference, because it's a really important topic. Let's think about and we've also explored this in active live stream, 26 and 37 and a lot of other types. So let's look at three different ways of doing Bayes, Bayes ways, Bayes areas as they were."
"Unknown","00;34;31;19","00;35;04;06","So and maybe there are more, but the three that we can list up here are exact Bayes, Monte Carlo and Variational Base. So pick one and then what is like something important to know about that way? Of doing Bayesian statistics well, so I am by no means an expert in Bayesian statistics actually like cracked out my Bayesian data analysis book while I was reading this paper because it was one of those papers."
"Unknown","00;35;04;06","00;35;26;18","Like I said, I'm a, I'm anal retentive and detailed. I like, what does that mean? I have to look up every little thing I don't understand. So like, OK, so let me start with exactly BS, so exact Bayes, but I think about exact Bayes it's like the probability of A given B like the probability of rain given clouds or something like that."
"Unknown","00;35;27;07","00;36;00;06","And so it's like an alternate alternative to frequentist to statistics where the probability of rain and the probability of clouds or like a coin flip is easier to talk about, right? Like so each flip of the coin is half. And so like, you know, the idea that what are the chances that you're going to get heads you know, five times in a row is half times, half times, half times aftermaths."
"Unknown","00;36;00;21","00;36;41;09","So in Bayesian statistics, there's or the probability that you're going to get rain. Here in Frequentist statistics, the probability that you're going to get rain and the probability that you're going to get clouds is the probability of rain times the probability of clouds. That's a great way to put it. And then in Bayesian statistics, there is it evaluates the probability of rain with clouds, like given what you already have observed before, like if it's rained, it's usually cloudy this percent of the time and it takes into consideration the probability of rain with clouds, probability of rain, probability of clouds, all of those things separately."
"Unknown","00;36;41;09","00;37;06;14","So it gives you a prior estimation, I think. So I don't know if that's probably super confusing. Great note. You mentioned a lot of really important points, which is it's an alternative to Frequentist statistic. Analysis. So we're not getting a P value out of this and we're able to explicitly say our priors and not to go down this rabbit hole."
"Unknown","00;37;06;14","00;37;29;04","But a lot of Frequentist implicitly has a uniform prior and we'll just leave it at that for now. So the exact Bayes is using the BS equation essentially as written. And just as you said it, there's the probability of something given something else. And then you kind of find the probability of that other thing given the first thing multiplied by these other terms."
"Unknown","00;37;29;15","00;38;00;25","And then you fill out all of those variables and you literally do that division and so we explore it like this in the case of actual constants paper with the bacteria. And so there is the exact Bayesian bacteria that was having its prior and then taking in new information and updating how it thought about what was out there based upon the new incoming information and it was like in a formula and it was like dividing exactly the numbers that you see here in this way."
"Unknown","00;38;01;08","00;38;38;22","OK, so that's exact base. Now what are some issues with exact base? It might be straightforward when you're talking about that coin flip, but if you're talking about high dimensional space or something that is just bigger than the RAM or whatever of your computer, when it comes to the implementation, there's often challenges with exact base. And so there's several techniques that have been developed to approximate what an exact Bayesian approach would be in a more tractable way."
"Unknown","00;38;38;26","00;39;02;28","So there's two alternatives to exact base implementation and the spirit is going to be exactly the same of taking in new information and updating our priors as specified. But it's going to be done in a few different ways. So first, what about Monte Carlo? What is Monte Carlo? So Monte Carlo, you probably know more about the history than I do."
"Unknown","00;39;02;28","00;39;56;19","But but Monte Carlo is like a sampling technique. So, you know, instead of trying to calculate the probability of rain and clouds using exact bars, you just sample like out of, you know, a thousand rainy days. How many of them were cloudy and out of a thousand cloudy days? How many of them were rainy? And so instead of like getting an exact measurement of your distribution, like what is the where is the overlap between clouds and rain, you just randomly pick out from all of the possible days, you estimate your sample that way so it's based upon sampling."
"Unknown","00;39;57;10","00;40;24;00","And so it's like we might not know one of these terms, but we can at least draw a sample. And so this does a few different things. It connects us first to empirical and specific data as implemented, like in a row of a program, not just B, sub I or some sort of analytical representation, but like this is where we're getting into this specific sample was pulled by this algorithm at this time."
"Unknown","00;40;24;22","00;41;06;16","And one method that's a common thing is the Markov chain, Monte Carlo and this is using a local re sampling approach like assessing local managers of a given chain that's engaged in sampling. And you kind of drop these chains into different parts of the probability distribution and then have them evaluate different aspects of that distribution. So it's as easy or hard as dropping those paratroopers into different parts of the physical landscape and then having them accept or reject different proposed moves so that's sort of a two dimensional landscape with the height being elevation."
"Unknown","00;41;06;20","00;41;33;24","And maybe that's what you want to map because you might want to know, is there one central peak are there two peaks that are very similar? And take that into the statistical case. And this is especially helpful for when there's no why don't I see no idea, but there is not the desire to explicitly formalize what shape the distribution looks like."
"Unknown","00;41;34;00","00;41;57;26","So we might want to do like Bayesian pilot genetics. And so we need to talk somehow about the likelihood of a given DNA sequence being that way. But like where do you go from there? And so that's where the Monte Carlo sampling based approach really comes into play. And then because it's based upon sampling the advantages are you can run that sampling for three iterations or 3 million."
"Unknown","00;41;58;05","00;42;26;23","So it's very flexible with the amounts of computing power that you bring to bear on this challenge. But also there are the risks of over sampling, which is that you have use some unneeded electrical power and computational power, which is really non-trivial. But also there's the risk of under sampling. And you might even be like in a regime where you think that you've sampled because you're getting samples that are just confirming what you already know."
"Unknown","00;42;27;05","00;43;03;28","But, you know, there's a whole other island that you didn't do the paragraph to. So one can be lulled into a false precision with bootstrapping and sampling based approaches because they can give ultra high precisions but that can be based upon actually more like a biased sampling approach or all these other features. And just the one anecdote that I have on that is in the Bayesian phylogenetic case, I remember this one professor in undergrad, Professor Moore, and he'd always say Fuzzy Caterpillar you want it to look like a fuzzy caterpillar."
"Unknown","00;43;04;06","00;43;30;03","There's some technical details, but it's like you want it to be exploring the full range of a parameter like if something could be zero to one, you want to be exploring the full range. But returning to the best estimate, and that shows that you're sampling like the diversity of what that parameter can be, but also spending more or most of your time in the most likely and in the best regions."
"Unknown","00;43;30;09","00;43;48;15","And so if the value were at 0.5 and the range of what's possible is like zero to one, that looks like a fuzzy caterpillar with more thickness in the middle and then a lot of fluctuation but it's not like it's spending a ton of time, that one, and then it drops suddenly to zero because that would suggest that you're not converged yet."
"Unknown","00;43;48;23","00;44;25;07","Whereas when you see that fuzzy caterpillar it's like you're sampling the extremes and getting novelty and testing different combinations, but you're also returning to something that is working. So Monte-Carlo sampling based approach brings in all of these opportunities and challenges associated with sampling well. So I was under the impression like you talked about over sampling, but I think over sampling, like as you as sampling goes to infinity, like your your accuracy increases like gear closer able to get the true posterior."
"Unknown","00;44;25;27","00;44;48;18","But I think it's it's uneven sampling. That's the danger in the Monte Carlo method. Like if you're sampling too far in one end or the other and perhaps like asymptotically if you exhausted the state space, then you would know that you had the right answer. But the whole reason why we're using any kind of terroristic approach rather than exact bars is because like we don't have the full state space."
"Unknown","00;44;48;29","00;45;16;02","So yeah, there's relatively low cost to oversampling, but we don't always know when that is and then under sampling can be just totally misleading. If we take that as our only representation of that distribution. And so there's all kinds of cool techniques that involve like multiple chains happening in parallel. And some of them are like higher temperature and less temperature."
"Unknown","00;45;16;02","00;45;42;21","So for one of those paratrooper teams, it's very cold. And so they're only making the best very local moves. It's like the elevation is sharpened for them. It's very hard to get out of a local rut, and there might be another team that's very high temperature and for them the landscape is flat maybe even completely flat, and then they're like a hot molecule that's able to move over something that might be a barrier for another team."
"Unknown","00;45;42;28","00;46;15;25","So that team is going to like fill in the details and locally explore quality solutions, whereas there's another team that's like it or another chain. And so that's the multiple chain Monte Carlo approach. And there's a lot to that in phylogenetic, which is where I've seen it the most, but also probably many other areas OK, so I've, I've, I've run it in phylogenetic too, and like, I mean, the program can take forever to find because, you know, too many parameters and whatever, they can really take a long time."
"Unknown","00;46;16;09","00;47;03;24","The computational power is no joke. Well, like what goes into the Monte Carlo sampling. So we have that phase which is calculating the true posterior. And then we have the Monte Carlo, which is estimating the true posterior to sampling. And then we come to the variational, which is trying to minimize the divergence between what the true posterior and your approximation of the posterior rate, minimizing divergence between the true posterior like what we would really have wanted to actually know what is the actual temperature in the room conditioned upon the noisy thermometer estimates that we're getting."
"Unknown","00;47;04;10","00;47;26;07","So are we going to sample our way out of that one? The variational approach is going to be very different and in 26 I think we had like the cat and then one approach was like the Monte Carlo was like pointillism. It was like dots were sampling pixels from the cat and then it becomes like a pointillist picture of a cat that if you do sample densely enough, it looks like a cat or it's recognizable as a cat."
"Unknown","00;47;26;29","00;47;44;17","And the variational approach was like a clip art template and so let's just say that you had a template of different curves or of like you knew that, that it wanted to be a cat, you knew you were looking at a cat. So then you have like a cat template that can get stretched or zoomed in or out."
"Unknown","00;47;45;10","00;48;11;22","And so that might be very straightforward to optimize because you're just trying to minimize the divergence between these very limited parametric changes that you can implement. And of course you run into a situation which we'll explore various aspects of variational inference. What if you try to stretch that cat emoji onto a different animal? Or what if it's a totally different kind of data set?"
"Unknown","00;48;12;00","00;48;45;23","So you will find some divergence minimizing solution that doesn't mean it's sufficient or even in the right ballpark just like doing owl to least squares, minimizing linear regression will always find a best fitting linear regression but that best fitting linear regression can be like hilariously inaccurate. Like if the data are like a parabola like a you, the best fitting regression might be like a flat line through the middle."
"Unknown","00;48;46;07","00;49;13;12","Or if 80% of the points are here and 20% are over here, there might be a regression that's very misleading. That's like The Simpsons so-called paradox. So in Variational, we're not going to use sampling, we're going to use the minimization of a divergence, which is a real divergence. But they mentioned in this paper it can also be a different kind of divergence, and the Rainey divergence has been explored in some recent work of Saget at all."
"Unknown","00;49;13;29","00;49;52;12","So we're going to minimize the divergence between the true posterior and a distribution. Q that we control and is tractable to optimize so it's like instead of that sampling based approach, we are dropping the teams at different parts of the landscape and then they were going to report back on information here in the variational approach, it's like we have a template and then we're going to just do stretching and bending again."
"Unknown","00;49;52;12","00;50;16;16","This is kind of a tortured metaphor, but we're going to stretch and bend and there's only a few parameters on the stretching and bending and we're going to find the best fit of those stretching and bending parameters. Like if it were a linear regression, Y equals X plus B, the two dials that you get to change are like the M, the slope of X and B like how high or low the regression is."
"Unknown","00;50;16;27","00;50;57;24","And then you're finding M and B that are the best fitting about the data Y. And so you're kind of like minimizing the least squares error term here. It's not a linear regression that we're fitting by minimizing the least squares. It's a variational Bayesian approximation that we're fitting by minimizing the divergence between the true posterior and the distribution cue that we control and is optimal or is able to be optimized well, else would you add on that I think that's it."
"Unknown","00;50;57;24","00;51;34;15","So the, the true posterior and the, the distribution that is tractable and we can optimize over. Yeah, that's it. So, so let's close this. We'll look at computational intensity on very emotional is like much less right then because instead of calculating all of the parameters for every little dot, every little point on the sample, we are calculating for like a big blob."
"Unknown","00;51;35;04","00;52;04;12","It's like one big blob instead of several little points but we calculate for that big blob this is the density of that of that function. These are the parameters of that. So, so now we're going to back out of this formal cul de sac that we've been in. And just remember to ourselves, even if it's your first time hearing Variational Inference and or you're kind of like Blue or I where it's like we've read it in how many papers, but still every time we kind of start on square zero."
"Unknown","00;52;04;24","00;52;31;08","So why do we use Variational Bayesian inference? So the first thing is it allows us to use Bayesian statistics and probability, which we might prefer over for example, Frequentist, OK, Variational Bayes then allows us to use a heuristic Bayesian implementation. So if there's something that's too challenging or implausible for exact Bayes, now we can approach it like Monte Carlo or Variational."
"Unknown","00;52;31;08","00;53;03;06","These are both his mistakes. Unlike Monte Carlo, it's not based upon sampling. Variational is based upon a family of equation fitting, and so it can be implemented very efficiently, which isn't always the case for Monte Carlo. It rests upon an appropriate factorization of the problem. And one other piece is this connects to analytical equations in physics. So it's almost like Monte Carlo is like a computer scientists approach."
"Unknown","00;53;03;18","00;53;22;08","It's like how much RAM do we have? How many processors do we have? How big is this data set going to be? And then there's no analytical equation or closed form for Monte Carlo. It's like engineering. It's kind of like you have to have some art and skill and science coming together for the Monte Carlo to be the best it can be."
"Unknown","00;53;22;22","00;53;46;24","And then it's more about like an adequacy and an efficacy question because you're not converging to the infinite asymptote again, otherwise you could have done something else. And then the variational is like the physicists approach and it draws on the variational calculus of Feynman and others. And this is the part that's very amenable and connected to the equations of least action and all these other things like factor ising equations."
"Unknown","00;53;47;18","00;54;18;27","So it's funny that you bring up the computer scientist approach in Monte Carlo and like so I am a horrible computer person. I mean, I will brute force wrangle my data like just give me more ram, just give me more processing power. Like I, I just want to make things work out of any patient for finesse. Right? So in my mind I think about the Monte Carlo like the brute force, just shove your data through the algorithm no matter how much ram it takes."
"Unknown","00;54;19;05","00;54;41;20","I think about Monte Carlo like that and I think about the variational Bayes boy like the people that I can do, like, you know, very simple things in code to make it run much faster that have that like very good skill with like, oh, well, we can just, you know, run this little factorization or, you know, we can make the problem run."
"Unknown","00;54;41;20","00;55;02;12","So much better on the computer. So I think about it, about the variational like that be bigger, right? It does have that connection to statistical physics and to, to Feynman. But I think about it as like a highly optimized way to run Bayesian statistics. But why, Daniel, would someone want to run Bayesian statistics in the first place? So, so maybe you want to give us an example."
"Unknown","00;55;02;13","00;55;25;21","Why would that happen? Well, yeah, we will pull back. Just the last point there is the variational methods. We might even have a higher mystic for those or an algorithm or approximation of those. For example, message passing. And for any factor graphs, those draw an equivalence between Bayes graphs, where nodes are random variables and edges are statistical relationships."
"Unknown","00;55;26;10","00;55;52;19","These graphs can undergo variational inference and there's a tractable way to use toolkits like that developed by the bias lab and use message passing and a form factor graph representation to have like a level of implements ability for even the variational. Whereas it's not possible to imagine like a horrific for Monte Carlo, it is the horrific sampling is sampling."
"Unknown","00;55;52;19","00;56;16;18","You can do better sampling, but that's the game that you have chosen. Whereas in Variational even this question of minimizing divergence does require kind of like a proximate or an operational approach because how can we minimize the divergence between the true posterior and something we control and we don't know what the true posterior is. So there have to be a little bit more added."
"Unknown","00;56;16;18","00;56;44;29","But this is like very awesome discussion because it's helping us pull out. Like this is the sort of exact approach. And then there's a more computer science like sampling approach and then a more analytic goal and physics related approach. OK, so why do we want to do this? Let's pull back to our triangle here. Why do we want to do base and thinking about the concepts here?"
"Unknown","00;56;45;06","00;56;46;12","Why do we want to do this?"
"Unknown","00;56;48;14","00;57;13;08","I asked you, you're not allowed to turn the question around on me. That was me asking you. You're not allowed to just talk and then ask me well, there's a few ways to take it. I guess one would be what is the alternative policy selection if we're doing research or application, what is our alternative to picking some Bayesian approach?"
"Unknown","00;57;13;13","00;58;01;20","So if we take the Bayesian road, we know that there's going to be some sort of try emergence later. Exac Bayes, Monte Carlo, Variational, Bayes, something else. Or we could go down a different statistical path, like we could use Frequentist statistics, and maybe that's totally adequate, ineffective for the situation and both are just maps. So a well-fitting linear model of height and weight doesn't make height and weight a linear model, a really good fitting action perception loop, whether it rests upon a kernel of Frequentist statistics or Bayesian statistics doesn't make those systems that way that you modeled it so I would say Bayesian statistics is useful when we want to do some kind of formal"
"Unknown","00;58;01;20","00;58;32;29","or quantitative inference where we want to be specific and explicit about our prior beliefs and how we want incoming information to update those beliefs. So I agree, and it's what I always try to tell people, you know, when I'm explaining, like what is Bayesian statistics, which a lot of people have never heard of, like in college, at least when I graduated, there's there's no Bayesian statistics course."
"Unknown","00;58;32;29","00;58;55;04","Now many schools have them. And, you know, I'm outdated already, but it's really like when you're doing frequentist statistics, you go out to the desert and count mice every year, right? Every year you go and count how many mice you see in the desert or something like this. And if you I mean, you expect to see some mice in the desert, otherwise you wouldn't be there counting them."
"Unknown","00;58;55;16","00;59;18;04","So you expect a non-zero answer. Every year you go out, you find ten mice, 20 mice, 50 mice, 30 mice, 20 -10 mice, and every year you go count mice. So if you were to go like you have some expectation that you will find mice, you also have some expectation that there is going to be less than a thousand mice because every year you sample this place."
"Unknown","00;59;18;04","00;59;50;25","Right? And so what Bayesian statistics does is it takes actually that you're not like Frequentist. They don't care if you ever found my said before, like your probability of finding zero mice and 30 mice and 50 mice and 100 mice and a thousand mice is all the same. Doesn't matter how many years you've been there before, like you are starting with nothing in frequentist statistics, you don't get to have any guess that there's even going to be one mouse in the desert where with basic statistics, like obviously you think that there are going to be some mice and you think it's going to be some number you can count reasonably so with whatever area."
"Unknown","00;59;50;25","01;00;16;17","And so what Bayesian statistics statistics allows for is that prior belief or that expectation that you have as experimenter. Awesome. And that's kind of what we were getting out with that idea of frequencies of having this implicit uniform distribution. You flip the coin ten times, you get three heads, you're maximum likelihood estimate is it comes up 30% of the time on heads."
"Unknown","01;00;16;18","01;00;43;27","Point three. However, if you want to take like a bigger picture view, you might have some sort of precision about where it's likely to be like is it likely to be 0.5? And you're going to be surprised if it were three because you just pulled it out of a cash register. And then in that case, you still might say, I want to have a uniform prior, and if it is point three, then it's point three or you might want to say, I'm very confident it's something else."
"Unknown","01;00;43;27","01;01;19;03","But Bayesian Statistics opens up the space to be specific about how we want to use our priors, how confident we want to be, how much precision we want to have in those priors, and then how we want to incorporate new information. So any other thought on that or let's continue on that and connect it to some Formalisms. So I have one more thought on the coin flip with with Bayesian with Bayesian idea, like if you know it's a fair coin and the probability is 50% and like if you flip a coin and we're betting each time, like, you know, a dollar, a dollar, a dollar, if you flip that coin, you flip the coin ten"
"Unknown","01;01;19;03","01;01;41;22","times in a row and it's heads every single time like on the 11th time, my bet is on tails because like it's not, it's a fair point. Like it's not going to come up every single time. Heads and so frequency statistics like does never allow for that previous information and also like where is that information? Which is something that always baffles me, like does the coin."
"Unknown","01;01;41;22","01;01;57;00","I already know that it's been flipped ten times and it's come up heads all the sense and that it then has to come up tails sooner or later. I mean, if the probability is so low that you're going to get like a thousand coin flips of a fair coin in a row, that is heads like I don't know if it's ever happened to anyone ever."
"Unknown","01;01;57;06","01;02;14;03","So like as we increase in the number of flips and if it's head to head to head to head, like it becomes increasingly probable that the next flip is going to be tails in my view. Right. Which is maybe totally wrong. But but where is that information stored and the Bayesian kind of allows for that to be there."
"Unknown","01;02;15;25","01;02;45;19","And I wonder if this even connects to where is that information stored in the quantum reference frame of the observer and nowhere else. But let's look at how that gets implemented in these Formalisms. So you just describe that setting where somebody has a belief that the coin is fair. Maybe that's an empirically grounded belief like previously they flipped it a thousand times and they got 5050, or they have just an a priori belief that is fair."
"Unknown","01;02;46;01","01;03;14;15","If that belief were generated by a real other data set, we would call that parametric empirical Bayes because it's the process where you set the priors as well as their confidence based upon some collected data set that's parametric empirical Bayes. Or one could just take that a priori concept synthetic apriori and just say coins ought to be 5050."
"Unknown","01;03;14;23","01;03;40;12","And that's what I'm sticking with. Let's just say that then we play this game, we flip it ten times and likely or not, ten heads happen. That's the trace of behavior. In the neat should actually happened. Now that might be seen as a totally likely outcome by somebody who believes that it always comes up heads, hence they're unsurprised or depending on what your priors were that could be variably surprising."
"Unknown","01;03;40;22","01;04;09;28","And then you talked about both when and how should you update your beliefs on that coin like maybe something changed while you weren't looking at it. And so you want like your best estimates to very heavily reflect the recency. And maybe that should be like a moving average of the most recent three flips or maybe the most recent 30 so let's look at two places in the paper where they do something like that."
"Unknown","01;04;11;05","01;04;37;12","The first one has to do with predictive coding and frame differences in algorithms, in video compression. And so in this area we can think about the way that the video looks. People who are watching this video, the way that the video looks is like the output. And then we want to encode like how surprised are we, what is happening when the frames are changing."
"Unknown","01;04;37;28","01;05;09;15","And so the simplest method is just count where it's different and convey that information. But it might also be important to use as they write more advanced methods that predict each new frame using a number of past frames weighted by a coefficient, an approach known as linear predictive coding. So that's one case where you're determining how many frames back, how many coin flips back should we use for that now casting and how should we wait them?"
"Unknown","01;05;10;01","01;05;44;17","So that's one area. And then a second area that they bring up and connect is the common filter. So here's the common filter, and this is their formalisms 33 and 34. And it has a lot to do with Bayesian statistics. So on this and Bayesian Kalman filter is very common we can see it in two different ways. So on the left side of this slide, the top image shows a prior prediction that's the prediction."
"Unknown","01;05;44;27","01;06;07;26","And then two is the measurements, and then there's the fusion. So we have the prediction, the measurement here with GPS, and then there's the Fusion so that is very similar to having a prior and then some updated sensory information and then the posterior. So it's just labeled slightly differently. But we can already see how this is like totally related to Bayesian statistics."
"Unknown","01;06;08;24","01;06;32;05","What this also brings into the picture is like a pseudocode that unrolls through time. So Bayesian statistics approach doesn't have to be about time. It can be about a static data set and then you could be modeling like OK, per extra sample of the population of their height and weight. We're going to update our inference on that relationship."
"Unknown","01;06;32;18","01;07;02;26","But that's a timeless analysis. It doesn't incorporate some sort of unfolding through time that can be incorporated into the common filter using this pseudocode. Down here on the left, there's some prior knowledge of state, and this is like two dimensions. And then there's a cloud representing a distribution of precision or uncertainty. So a more precise estimate would be like a tighter cloud there or a sharper peak or a sharper valley, depending on how you want to think about it."
"Unknown","01;07;03;19","01;07;27;19","And then less precision would be like a broader basin or like a more diffuse ink drop. And then there's a prediction, then a measurement occurs, which can be precise or can also have an associated measurement error term, which could be fit with parametric empirical Bayes based upon testing, or it could be determined on the fly with expectation maximization."
"Unknown","01;07;28;03","01;08;01;02","And then that prior gets juxtaposed in this update step and the outputs. So this takes sort of this timeless Bayesian update scheme and specifically adapts it to the case of something happening through time. And that's shown on the right side here with this image of like time is happening and we're getting these noisy X measurements and the true temperatures, the purple and let's just say it's unchanging in this case, but that could also be changing."
"Unknown","01;08;01;12","01;08;28;06","One can just imagine that, especially if it's very noisy, it's hard to get increasingly complex dynamical patterns, but that's all part of the game. And then the prior starts high and then the prior is weighted. It's kind of like a spline. It's weighted through all of these X's. And you can imagine one extreme case is like move to the last one that you saw."
"Unknown","01;08;28;25","01;08;54;05","And so that would be like basically recapitulating the measurements distribution through time. The other extreme case would be weight all the time, points evenly so that like we're kind of converging to a moving average. So if we have 100 hours of video that is like at a two and then all of a sudden it switches to an eight and then you're moving average would very, very slowly start moving up from a two."
"Unknown","01;08;55;00","01;09;28;05","And then one could imagine that there's some intermediate solution that doesn't use like the whole dataset because that's too slow to adapt and maybe takes up too much memory, but isn't just like a one step instantly switch to the nearest measurements and that parameter of how fast should you update your Kalman filter is a parameter in the model and so that is exactly what is being statistically optimized, is how much through time should we update our ongoing estimate."
"Unknown","01;09;28;15","01;10;04;08","And so in the static or the timeless Bayesian case, that's where we talk about precision. How much should we update our inference as new information is added to our dataset, but that doesn't mean appended in a temporal way. The common filter is making it explicit that these data points are being added sequentially, and that is providing us with this pseudocode by which the latency up to me, the latent state is updated not just as a function of adding data but actually adding data in a sequential and unrolling way."
"Unknown","01;10;04;16","01;10;34;17","What do you think blue yeah, great. I think that that was a super good explanation and very clear and yeah, my first interaction with common filtering was an imaging and image processing. So I think that's maybe where people come across that a lot, especially like doing laser scanning microscopy or a video I guess of the video update also is like that."
"Unknown","01;10;34;17","01;11;11;20","So yes. So let's just look at some of their writing, but these are like the smaller the formalism, the less we've paid attention to it and the more that we would love to know about what it actually means. But let's kind of pick up above there. 35, they write common filtering procedures in two steps. First, the state is projected forwards using the dynamics model or prior, and that's the P of beauty plus one that's the means at the next time step conditioned upon the mean at this time step."
"Unknown","01;11;11;28","01;11;45;15","So that's like what is going to happen next, conditioned upon how it is now these estimates are corrected or sort of compromised coming to a compromise by new sensory data by inverting the likelihood mapping P of observations at the next time points conditioned upon our estimate of where the latent state will be. So this is what temperature will the room be conditioned upon how it is now at the next time step, what will it be conditioned upon what it is now?"
"Unknown","01;11;46;05","01;12;16;09","This likelihood mapping, which is like the a matrix in the poem piece, is what is the probability of the outcome in the next time step being a certain way conditioned upon how we think the rooms temperature is going to be. And so these are the equations and some of the variables. And then they kind of conclude or in this little section, the derivation of the rules is relatively involved and that's in Appendix A and some other places."
"Unknown","01;12;16;24","01;13;15;13","But also the common filtering can be interpreted as finding the optimum of a maximum, a posteriori estimation problem. So this is almost like we have contact with the exhortation of the a priori what information are we bringing to the table qualitatively count or quantitatively base? That's the Apriori. And then Bayes kind of bridges that a priority the prior with a posterior URI Dutch, if that's the Latin for that one, but it's the posterior what happens after the sensory update and then that space between like the before and the after is the during and that's the during of the now that we're doing action and inference with within let's look at how they introduce anything otherwise"
"Unknown","01;13;15;13","01;13;48;13","we'll look at how they introduce action and then connect it back to active inference so I think the common filtering and the updating. So I think we, we kind of skipped over updating when we were talking when we were making our layer cake earlier and just how how messages in the nervous system biologically and in predictive coding are passed forward and backward."
"Unknown","01;13;48;13","01;14;21;21","So it might be helpful to back up and talk about error propagation and propagation of back propagation and propagation of signal through times just as like a very basic way because I think the common filter is kind of an advanced way to do that. OK, can we go back to we're back at the Triangle and so now we've added Variational as like a little badge to the Bayesian."
"Unknown","01;14;22;08","01;14;49;01","And also we're going to add Carmen as a badge. Where have we visited on this journey? And one can imagine that they could take a Monte Carlo or a Variational or an exact approach to common filtering and so on. And then you mentioned a few more biologically inspired or compatible features like error propagation and so on. So what is an area in the paper or an idea that's relevant here?"
"Unknown","01;14;51;18","01;15;28;24","I think that the signal propagation and, and bottom up prediction error, like how does error propagate like through visual systems, through like neural networks? I think that there's applications here. And is it the error that's fed forward or is it the signal that sped forward or where does the error go in the system? So I think these are some confusing things that that maybe they they touched on in the beginning of the paper."
"Unknown","01;15;28;24","01;16;00;22","So let me see if I can find exactly or there's some debate here. And while you're looking so I've I've pulled out this right edge of the triangle. So we're going to just leave some of the baggage at home and take out to the table just this link between Bayesian statistics. And probability and biological systems. And then you raise this really important question about like how do we think about error estimation?"
"Unknown","01;16;01;07","01;16;59;12","And also some related terms. There would be like precision ambiguity, risk compression, and so those are error estimates. And then how are error estimates propagated and communicated how are they propagated and communicated in language? I'm not sure if I had to guess in our error estimates communicated in systems. And so we might be interested in mathematical systems. So then the way that you communicate the error is just you multiply the two variables, you have a precision matrix and then you have like some sort of hidden state matrix, and then you basically just buzz this kind of pristine matrix by some error matrix."
"Unknown","01;16;59;26","01;17;31;04","And so if your error on something is like zero, this is not exactly how the multiplication would work because multiplying by zero, but if the error were zero on something one would want their estimates to be passed through without being blurred. If the estimate of the uncertainty were super high in the extreme approaching total noise and uncertainty and variance estimate, then no matter where that pristine estimate or word you'd want it to be like totally fuzzed over."
"Unknown","01;17;31;11","01;17;57;05","And then one could imagine that in a per variable ongoing way, you'd want to be updating these variance estimates. And that is exactly what happens in the common filter, which is this unrolling and ongoing estimate of observations. Latent states and that the variance that links them and that's done like through matrix multiplication. But how does it happen in biological systems is a different question."
"Unknown","01;18;00;12","01;18;43;16","So I think some things come into play here, like redundancy and signal compression and then error propagation forward and background. And so there is this in predictive coding slash predictive processing because I'm still like not 100% on the on the difference there. But but I do think this is a predictive it seems like it's a predictive processing thing, but the authors do say predictive coding there is like a bottom up construction of a model."
"Unknown","01;18;44;01","01;19;13;28","So the authors say that but it's not only that the authors say that perception is not the result of an unbiased feedforward if bottom up processing of sensory data, but a subset of process of using sensory data to update predictions generated internally by the brain. So there's the idea of a model comes in here. Yeah. Generative model, where do you see generative model in these discussions?"
"Unknown","01;19;17;28","01;19;51;06","I mean, it's kind of threaded throughout. So I think the idea of variational inference brings in a model because the model constructs the estimate of the posterior, like where we're consistently there's a consistent effort to update the estimate of the earth, sorry to minimize the divergence between the actual posterior and the estimate of the posterior so the estimate of the posterior is like, you know, trying to make the model match reality."
"Unknown","01;19;51;14","01;20;19;08","So trying to make the picture of the cat match stretch Ben folds into the model of the cat. Yes. Awesome. So generative model with no conflict complications is being put into the Bayesian statistics area because if you specify a model that's generating thermometer outputs, given the temperature, it's quite literally the generative model. And we know that there's kind of two directions."
"Unknown","01;20;19;08","01;20;42;00","There's like the recognition model and the generative model that's the tail of two densities because of a distribution can also be understood as a density. And so in the realm of statistics, generative model is quite literally the approach that is taking parameters of a model and using it to generate, for example, observations. In this case, no need to complicate that."
"Unknown","01;20;42;16","01;21;12;14","But then there's a few ways that we hear different people in different papers talking about the relationship between biological systems and generative models as well. As recognition models. But we're focused on the generative model here, and sometimes these are even coherently or incoherently used differently in the same paper or the same sentence, which is our favorite. And we'll eagerly await the kind of automated detection systems that will enable us to do high throughput analysis."
"Unknown","01;21;12;14","01;21;45;09","But here's just a few of the kind of ways that people can talk about that. We hear sometimes that the biological organism or cell is a generative model, has a generative model, enacts a generative model, or the more instrumentalist we can model that system with a generative model. And so this bottom one is kind of like saying, I'm just planning to use Bayesian statistics to model a biological system."
"Unknown","01;21;46;02","01;22;23;14","And so perhaps we could say that this is the least bringing assumptions to the table of what that system is doing. Because by saying we can model it using a generative model, we can use or fit drive a generative model for this behavioral cognitive system. We're just remaining purely with both feet in scientific instrumental ism and empiricism. We're making only claims about the map and not about the territory per se, which might be throwing out the whatever with the whatever."
"Unknown","01;22;24;09","01;22;47;21","However, these ones are when people are making claims that are about the territory what is the brain? Is Bayesian brain what the brain is doing? Well, it's not passing around screenshots of these equations like we are. So what is it that it is doing? Or again, is it on the instrumental side? It's just something that we can use to model the brain."
"Unknown","01;22;48;01","01;23;31;11","So yeah. So going back to predictive coding and, and also linking to what is the brain doing? And really this is why I wanted to stop here. I found, I found this out in the paper, but that was when we were talking about common filtering and image processing and then error propagation in a system. They took early in the paper about predictive coding as a means to remove redundancy and applied it to signal processing to reduce transmission bandwidth for video."
"Unknown","01;23;33;04","01;24;04;11","And this they also say here Barlow applied this principle to signaling in neural circuits, arguing that the brain faces considerable evolutionary pressure for information theoretic efficiency. Since neurons are energetically costly and redundant, firing would be potentially wasteful and damaging to an organism's evolutionary fitness. Because of this, we should expect the brain to utilize a highly optimized code which is minimally redundant."
"Unknown","01;24;05;00","01;24;43;18","And they say predictive coding minimizes this redundancy by only transmitting errors or residuals of sensory sensory input that cannot be explained by top down predictions. And so they remove redundancy at every point in the layers. So it's like when you're watching a video and or even like the flip of click. So we see like, you know, the flip flipbook that sort of a little figure kicking a ball from the point that changes is the point that's prioritized, the point that's different in each frame not to the point that stays consistent because that is where the action is here, where the motion is happening."
"Unknown","01;24;43;29","01;25;16;24","So that that's what's prioritized here. And that totally connects to information theory. Informative updates are the ones that update your prior to move it to a different posterior. That's the Bayesian information concept as opposed to, for example, the change in entropy which is like based upon simple entropy. So they're totally compatible. And if this seems like a technical detail, it is, but also this is the Bayesian concept of surprise and information."
"Unknown","01;25;17;06","01;25;49;25","But that was a great section you pulled out. So here where we're really highlighting as following Barlow, applying this principle of redundancy removal, but already we can caveat how much redundancy to remove because of more redundancy removed system is also potentially more fragile. Like if we only have one copy of each saved file now our threat of losing it is very high."
"Unknown","01;25;50;08","01;26;15;14","So redundancy is always in a trade off with like resilience and other system features. So this is one principle, but it's not the only principle at play. So redundancy doesn't mean we're going to pare it down to nothing. It just means that for an organ like the brain which is using like 20% or however much of an organism's energy budget, changes in its efficiency by several percents can be really important."
"Unknown","01;26;15;26","01;26;43;00","So that principle of redundancy removal is being applied. And the argument is that the evolutionary pressure is for information theoretic efficiency. So if you don't have anything to update, you don't want to spend the energy to do something extraneous, nor do you want to spend any more energy than you might have to on updating. And that's again because of the wastefulness of potentially excess signaling."
"Unknown","01;26;43;24","01;27;16;15","And then with evolution there's always a twist, like what if it is wasteful? And that's just some second level reason why. So it's not the whole story. This is just sort of short sentences because of this. We should expect the brain to utilize a highly optimized neural signaling neural firing code, which is minimally redundant, and then they connect it back from what the brain ought to be doing normatively at a first pass to the Formalisms that were being described with predictive coding."
"Unknown","01;27;17;03","01;27;36;18","So in the frame difference, in case it was like only tell me about the pixels that are changing and then if nothing's changing, just send me like a little beep and I'll just know it's just one piece of information just to keep it exactly the same. Even if it's 4K video, and it's a gigabyte per seconds or whatever."
"Unknown","01;27;36;23","01;28;05;05","But we don't have to change anything for that video. And then as pixels are changing, update me on how to change them. And so that would be like the maximum efficiency encoding. And so you would give a full resolution of the first frame in the movie. That's the prior, that's D, and then all one has to do is think about how to encode how the subsequent frames change."
"Unknown","01;28;05;24","01;28;27;08","And sometimes they introduce things like a key frame. So every second so every minute it's it starts with a fresh prior. And so these are the kind of computational things that one can do when they're not bound by like the constraints of biological systems. But this is an awesome connection. Any other thoughts on that theme or like where does error propagation come into play?"
"Unknown","01;28;27;10","01;28;50;01","Or we'll turn back to the triangle so I don't know. The error propagation is, is interesting. It would be great to have like the input of somebody who is more like versed in machine learning because I think the idea of like back propagation, like this is an error and like sending that back to the previous layer in the neural network is super interesting."
"Unknown","01;28;50;01","01;29;23;23","And part of what like enhances the efficiency of their own networks, like, hey, fix this. And so I think I can't really find in the paper where they say that. But, but there's like a local correction for error within each layer of processing. So in the visual system and yeah, in signal processing itself, it's like each layer prioritizes just fixing their own error and then that contributes to the overall efficiency and accuracy of the system at large."
"Unknown","01;29;24;10","01;30;05;03","Yes, back propagation is mentioned 45 times in the paper with a quick search so it's definitely an important concept and also something that authors have worked in a lot. So good error propagation and back propagation and also that reminds me of when Professor Levin was here in 40.2 with the black propaganda and the imperatives for like the different parts of the neural network and the kind of information that they want or expect or prefer, and then how they want to escape being trained and engage and learning, but not being trained but still updating."
"Unknown","01;30;05;19","01;30;39;22","So they're learning and being trained both involve like updating priors, potentially appropriately. But in the case of training, there's a feeling like another entity or agents is imposing their will versus learning can also be associated with behavioral and cognitive changes, but as a different sense potentially than being trained anyway. So let's go back to the triangle as we're sort of not quite landing the dot one plane."
"Unknown","01;30;39;24","01;31;05;05","You know, the check has not arrived at our restaurant, but maybe we're working our way up to our first star. Where do we sit on where action is? And then we have a few terms that we want to put, which is message passing and then active inference. Oh, yeah. So what is another term that we can add in or something that we're seeing differently in a new light?"
"Unknown","01;31;05;22","01;31;47;24","Or where do you see message passing or active inference who so I think, yeah, so do we have action? Are we happy with where action is sitting because we have enough where it is? I think it's I have used every feature of Google slides sufficiently. I like action as sort of a fulcrum in the center of the merry go round is action."
"Unknown","01;31;47;24","01;32;21;25","And yes, more arrows can be drawn, but we'll keep it a little sparse. But go ahead. So I think maybe message passing sits between predictive processing and action. And then that's exactly what I was going to put active in France was you just typed in inference because I think if we're or yeah, it's hard to say we're active in France really goes maybe not right."
"Unknown","01;32;21;25","01;33;05;18","We're inferences but like somehow if action and the Bayesian brain and biological systems could form a a triad maybe active inference would sit in the middle of those things so message passing to Bayesian statistics and probability because it's something that we can do on a Bayesian graph it's a implementation approach, but it's also leaning a little bit towards the biological because when people talk about like how does a given piece of information or signaling or stimulus in the tail end up having an effect on the other side of the body or on the brain?"
"Unknown","01;33;05;18","01;33;39;17","Like there's some kind of now it's being used in a technical Bayesian sense with message passing, whereas in a more just sort of like conversational way with biological message passing, but whether it's a synapse in immune synapse or a neural glial synapse or it's a mechanical transduction or a hormone broadly, we can just consider these different biological mechanisms that convey information or updates or anything to be like past messages."
"Unknown","01;33;39;27","01;34;04;17","So message passing is going to be a formal technique, but it does lean towards modeling the ways in which different biological nested entities are exchanging information. Yeah, and so why I would put it with predictive processing is because when I think about the things that we were talking about, like how are signals compressed, how is redundancy eliminated, how is error propagated through a system or back propagated?"
"Unknown","01;34;04;17","01;34;25;20","So which is why I like I liked it there in the processing because like you can't legitimately process that information without like knowing how the message is going to be passed. Like if you're only going to get every fourth letter of the message I pass, well, I'm going to encode my message in every fourth letter, not in every single letter."
"Unknown","01;34;26;10","01;34;56;14","And that changes. Right. OK, thanks. Yes. So we have a message passing now as part of the philosophical concept of systems that engage in holistically considered predictive processing because it connects what biological systems are doing with some idea about like information sharing without any formalism, but then message passing via the form factor graphs is going to be in the Bayesian side because that's like the implementation."
"Unknown","01;34;56;14","01;35;27;24","So maybe message passing here. Yeah, it's like on the sort of predictive coding implementation side, whereas we know we're not saying biological systems are doing 40 factor graphs and message passing in that technical sense. But for sure we can think about it in this qualitative way. OK, so we have action in the center and then kind of bifurcated it to include action and inference a.k.a active inference for a few reasons."
"Unknown","01;35;28;09","01;36;08;01","First off, action is not just blind flailing action. When we talk about planning as inference and inference on action and inference about future latent states and observations that aren't explicitly about action, but we know in active inference that they actually do conditional on action so inference and action, how we think and how we act, how the entity's cognitive model is, and how the entities behavior is through time those two are like inseparable, dual, and that's why active inference says it all in the title."
"Unknown","01;36;08;19","01;36;40;15","It's about action and inference and all of the ways that they are related to each other. Again, whether it's like planning as inference about future action or inference on the consequences of action, or it's inference about something that isn't explicitly action itself, like what is the temperature but it requires or is conditioned upon policy selection in this framework, not the only way to look at it, but that's how they come together in active inference."
"Unknown","01;36;40;28","01;37;10;27","And like, let's move over to 4.5 where they introduce action formally the basic approach to including action within the predictive coding framework is to simply minimize the variation of free energy with respect to action. Free energy is not explicitly a function of action up until Equation 51, it can be made so implicitly by noticing the dependance of sensory observations on action."
"Unknown","01;37;11;12","01;37;39;01","So if your future sensory observations don't depend on action or don't depend on that kind of action, this is like an extraneous calculation. Like if you have a coin flip and you're rolling the dice and they don't influence each other, then they're conditionally independent. And so there's no need to to calculate the joint distribution if you're just interested in the coin flip because it's not having a causal relationship with the rolling the dice."
"Unknown","01;37;40;09","01;38;16;10","However, if we're going to have agency or model systems that appear to have agency, then there's some kind of dependance on future sensory observations related to action. In the visual case, the visual input is conditioned upon the ocular motor decisions, and those are actions of a musculoskeletal system. And then the way that it gets tucked into the equations is the change in action with respect to time is going to be related to a gradient."
"Unknown","01;38;16;10","01;38;59;21","So like a partial differential equation, but then it's amenable to gradient descent. It's going to be like a partial derivative of free energy and how it changes with respect to action so like am I minimizing free energy by steering the wheel more to the left or by steering the wheel more to the right? That's like d f over dx a change in free energy over change in action and then free energy F here is a function not just of the joint distribution of O and MU, not just a joint distribution of the observations on the thermometer and the mean estimate of temperature in the room that's kind of the pure inference take."
"Unknown","01;39;00;07","01;39;28;10","But again, we're suggesting that sensory observations have a dependance could be partial could be complete on action. And so now observations themselves are a function of action, and then that can be unpacked a little bit more in 51 and they discuss it further of course in the paper and elsewhere. But that's like how this inferential framework gets built in predictive coding."
"Unknown","01;39;29;12","01;39;57;19","And then action becomes introduced as something that inference can be about and that's what they say allows for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms. It's not like there was the temperature thermometer module, and then now there's this totally ad hoc decision making module that's bringing in all this thing about well, what is the reward of different temperatures?"
"Unknown","01;39;57;24","01;40;46;05","What is the reward of different thermometer observations rather within this same parsimonious? And first principles inference framework, we can model active inference, which is inference, including action let's kind of continue on this action theme and then we'll we'll end in a few minutes so under such a scheme, the prediction error becomes the difference between the current observation and the target or set point that raises the question of where these set points of targets come from, and that's where they just sort of have that road leading off into the distance saying, well, in the evolutionary case, it's inherited."
"Unknown","01;40;46;26","01;41;13;27","In the computational case, it's just simply there. It could be just a priori speculated or it could have been a parametric empirical phase. So that's one really interesting point is like when you do introduce action, you have to discuss not just the target or the set point and how it's generated, but also you have to make explicit in the forward model as they write the dependance of the observations upon action."
"Unknown","01;41;14;09","01;41;35;21","And that has to be provided or learns because just that temperature thermometer model is not going to include a forward model of what happens when you turn on the heater or when you put on a jacket. So that's something that has to be learned. And then a second interesting thing that starts to come out of their framing here is like the costs of action."
"Unknown","01;41;36;02","01;42;05;23","And that closes the live really well with discussion that you raised of like Barlow and the information efficiency argument for why the brains ought to be doing something, something like predictive coding, efficient signal transfer and hence why it's either what they ought to or are doing or and how we ought to or could model it so that's why the costs of this are important."
"Unknown","01;42;05;23","01;42;43;15","Course of action are important. And then just one last piece and four or five was active inference in the control which is related to the generalized coordinates of motion. And we unpack that a lot in life through number 26 with the Bayesian Mechanics Paper of De Costa at all. And this connects control in active inference systems to a very commonly used engineering framework for controlling processes, which is PID control, which is framing action through time as being related to these three terms, which are described above."
"Unknown","01;42;44;02","01;43;22;00","So that's how they brought action into this paper was by spending many of the early sections providing like the single layer predictive coding model and connecting that to Variational Bayes that was in like Sections two and then in Section three and also in section two, they explored all these different interesting generalizations. The spatial case, the hierarchical case, and then they also connected it to biological systems and reviewed some evidence for predictive coding of different kinds."
"Unknown","01;43;22;15","01;43;50;05","And then instance in Section four, when they connect to several other inference algorithms like predictive coding and the back propagation of error, linear predictive coding and the Kalman filter normalization and the normalizing flows, which we didn't talk about predictive coding as biased competition also something we didn't talk about. And then after all of that action gets tucked into the picture and it's just like very clean in a very elegant way."
"Unknown","01;43;50;25","01;43;58;08","To think about it and helps us understand like maybe even how active inference is similar or different to some of these other frameworks."
"Unknown","01;44;01;14","01;44;25;07","So in the spirit of a layer cake, I think today, like we covered it. If this paper is a sandwich, we covered both slices of bread. That's like the very beginning and the very end, but we left out like all of the things that kind of come in between the meat and potatoes. You have potatoes on a sandwich ever."
"Unknown","01;44;27;04","01;45;07;27","They do it with some French fries sometimes but I know what you mean. Go to Subway. I'm going to go to Subway and be like, Can I get some potatoes on my mind so that I hope predicting coding special and then they go, then they go, What do you expect is going to be on the sandwich? It would be cool next week to really kind of dove in more to error, back to back propagation predictions are sent, you know, up an error sent down or is it the other way around and really kind of discuss these these computational graphs that they start to talk about."
"Unknown","01;45;08;06","01;45;35;14","So I think next week would be really cool to kind of dove into what's inside of the of the sandwich. Oh, awesome. OK, so we'll definitely go into some some so what's inside the sandwich back? Propagation of error, the biological evidence and examples. So I think there's a ton of exciting stuff that we'll be able to talk about next week."
"Unknown","01;45;36;27","01;45;50;15","The different paradigms like we could come over figure three, the paradigms of predictive coding and the normalizing flows. So we definitely we covered the bread today. Yes."
"Unknown","01;45;53;09","01;46;18;08","Well, anyone who is listening, we really appreciate it. Look forward to your comments on the video or joining us next week if that's going to work. And Blue. This was like super interesting and very helpful. So I think it's a great dot one and it's funny because like dot one, it's, it's the meat and potatoes of the 012 sandwich."
"Unknown","01;46;18;29","01;46;46;03","So it's like our middle of the bottom of the bathtub phase, the one where we're opening up all these ideas and just trying to like give a second coat of paint on a few things, go into a few technicalities re represent some knowledge. It's like that was in a very paradoxical or delightful way about not the potatoes itself, but rather about the beginning and the ends."
"Unknown","01;46;46;16","01;47;12;01","Well, to be fair, for those of you who haven't yet read the paper there's like 60 equations and like 55 pages plus an appendix. So no surprise that we covered the bread but it would be cool to tear apart a little bit of what's inside of the paper because I think that these layers are cool and important and the paradigms of predictive coding were super interesting to me."
"Unknown","01;47;12;01","01;47;44;23","And I love like the intersection between the brain and machine learning. And I learned a lot reading this paper, especially like I was taking neuroscience classes. I'm going to say how old I am right now, but I was thinking like neuro in college and like what at like in 2003, probably maybe 2004 with no idea learning about, you know, the work of Robert Ballard and Hubble and WHISTLE and, and all of these you know, centers around like visual experiments with no idea like how new they were."
"Unknown","01;47;45;11","01;48;13;07","No idea like how new of an idea it was like the processing through the visual cortex really. Like I didn't know how cutting edge the work I was learning about was. So it's cool to kind of have that framed for me and put in perspective and yeah, the building upon that that's been done. It's nice, excellent. Thank you very much blue and everybody else who's participating and see you next week."

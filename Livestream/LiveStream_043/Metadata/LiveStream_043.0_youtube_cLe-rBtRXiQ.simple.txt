SPEAKER_00:
Hello and welcome everyone.

It's April 28th, 2022 and we're here in Act-Inf Lab live stream number 43.0 discussing predictive coding, a theoretical experimental review.

welcome to the actinv lab we're a participatory online lab that is communicating learning practicing applied active inference you can find us at links here on the slide this is recorded in an archived live stream so please provide us with feedback so we can improve our work all backgrounds and perspectives are welcome and we'll be following video etiquette for live streams

Head over to ActiveInference.org if you want to learn more about how to participate in the live streams or anything else happening in Act In Flat.

Okay.

we're here today to learn and discuss the paper predictive coding a theoretical and experimental review by baron millage anil seth and christopher buckley the video is just an introduction and a contextualization for some of the ideas and some of the details of the paper and the broad sense it's not a review or a final word

It's, as with some of these other technical dense papers, it's like an opening for those who have technical questions at the learning side of things or at the more advanced research side of things to come get involved because this is a technical paper, but also hopefully as we'll unpack, it has a lot of cool biological and philosophical implications.

So in 43.0, we're going to say hello and introduce some big questions, then cover the aims and claims of the paper, the abstract and the roadmap, and then pretty much just jump right in.

And we're going to focus a lot on the introduction, the context, and the single layer predictive coding model.

And then we'll go a little bit faster through the later sections of the paper, talking about some generalizations of predictive coding and some important points that the authors raise.

So if you want to participate in 43.1 or 43.2 in the coming weeks, just let us know.

Okay, so we can say hello and give any information that we'd like and also just maybe something that we thought was exciting or something that motivated us to get involved in this quite involved paper.

So I'm Daniel, I'm a researcher in California, and I was curious to learn more about how predictive coding and predictive processing related to active inference, and also just about how different models framed anticipatory systems.

So over to you, Maria.


SPEAKER_01:
Hello, Daniel and everyone.

I have a bachelor degree in psychology and I am a master's candidate in philosophy of science in the University of SÃ£o Paulo, Brazil.

And I am researching the relationship between predictive processing and illusionist theories of consciousness.

And I think what brought me here today was...

my wish to start learning about the formalisms because I don't really see it in philosophy and I don't actually need it for my dissertation but eventually I wanted to continue my work on predictive processing so eventually I had to start it and I just thought well maybe that's a good idea to come here and minimize my uncertainties


SPEAKER_00:
Awesome, through inference and or through action.

And also thanks a lot, Brock, for helping in the dot zero.

So we'll just start with the big questions.

And these are the kind of questions that might motivate someone or interest them in this paper without even mentioning active inference per se.

But these are like some big questions that get touched upon.

What is the formal basis of predictive coding?

How is predictive coding used or useful?

What are some areas of current and future development?

And what is the relationship between predictive coding and active inference?

And hopefully we'll draw out more questions.

Anything else to add about this?


SPEAKER_01:
No, not really.


SPEAKER_00:
The paper is Predictive Coding, A Theoretical and Experimental Review.

I think the first version was from 2021, but the second revised version was 22 by Millage, Seth, and Buckley.

And just to review the core claim and then some of the aims and directions they set out, they write that no comprehensive review of predictive coding theory and especially of recent developments in the field exists.

That's their claim.

And then they aim towards providing a comprehensive review, both of the core mathematical structure and logic of predictive coding.

So there's a mathematical review and summarization aspect to the paper.

And then they also review a wide range of classic and recent work within the framework, ranging from biologically realistic microcircuits that could implement predictive coding to the close relationship between predictive coding and the widely used back propagation of error algorithm,

as well as surveying the close relationship between predictive coding and modern machine learning techniques.

That's how they set out where they want to go.

Would you like to read the first slide of the abstract?


SPEAKER_01:
Okay.

predictive coding offers a potentially unifying account of cortical function, postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world.

The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience.

A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating the potential biological plausibility for implementation in the brain and concrete neurophysiological and psychological predictions made by the theory.

Despite this enduring popularity, however, no comprehensive review of predictive coding theory and especially of recent developments in this field exists.


SPEAKER_00:
Here we provide a comprehensive review of both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature,

And we also review a wide range of classic and recent work within the framework, which is what we had read previously in the aims.

So that's how the authors describe their own work.

And now we're going to see their roadmap, see how they went about getting there.

And then we're going to jump right into the introduction and then the formalisms.

So here we see the roadmap.

and the rough outline of the paper is as follows.

Section one provides an introduction that contextualizes predictive coding and we'll also be unpacking it in a philosophical and historical view.

Section two provides an initial pass on the one level predictive coding model, sort of the kernel or the archetype or the motif of predictive coding and connects it importantly to variational inference.

Then predictive coding is generalized or extended or elaborated in a few important directions.

So like in section 2.2, it's generalized into a multi-level case.

In section 2.3, the concept of generalized coordinates are introduced.

In section 4, precision is introduced as a concept.

And in section 2.5, it's connected back to the brain.

Section three discusses paradigms of predictive coding, both unsupervised and supervised, and also connects it through those sections and others to some research in machine learning.

And then in section four, there's connections between predictive coding and some other algorithms,

both those algorithms that are common in machine learning and computational statistics, like the error backpropagation, linear predictive coding and common filtering and normalizing flows and biased competition.

And then in section 4.5,

explicit connection is drawn between predictive coding and active inference, which brings in the cost of action as well as the idea of PID control.

And the discussion section closes with just a few other thoughts and summaries and directions and challenges.

there's several appendices so it's a long ish paper or monograph but it's an excellent paper and we're just going to cover more of an emphasis on the context and the beginning and then sort of skip over or trail off over a few things but there's a lot to unpack so i hope that we can like learn more in the coming weeks if the authors want to join or if anybody else wants to join

Anything to add on roadmap or ready for introduction?


SPEAKER_01:
Let's go.


SPEAKER_00:
All right.

Awesome.

So I'll just add some notes from this section of the introduction and then Maria, feel free to add anything.

So how do they introduce predictive coding?

They introduce predictive coding theory as an influential theory in computational and cognitive neurosciences, which proposes a potential unifying theory of cortical function.

That's like related to the cortex of the brain.

Namely, that the core function of the brain is to minimize prediction error where the prediction error signals mismatches between predictive input and the input actually received.

Minimizing the divergence between expectations and observations is going to be a big theme.

And approaching that minimization, it can be achieved in multiple ways, and they're colored differently here.

Through immediate inference about the hidden states of the world, like I thought the ball was over here, I'm getting evidence it's over here, so I should just update how I think about the world.

which can explain perception like the perception of a ball moving across the visual field through updating a global world model to make better predictions which could explain learning like learning a little bit higher level than just where the ball is in the visual field but like where balls tend to be in the visual field or how fast they tend to move around

and finally through action to sample sensory data from the world that conforms to the predictions and that's when we'll introduce action into predictive coding in the latter sections of the paper and then just last point they say that predictive coding boasts an extremely developed and principled mathematical framework in terms of variational inference as well as many empirically tested computational models that have close links to machine learning

So that's how they introduce this is as a unifying theory that integrates inference as well as action, inference of different kinds, more like perceptual and more like learning.

And they're saying that it has a very rich mathematical framework and it's been connected to insights in biology and in machine learning.

Anything to add?

Nope.

so they write predictive coding as a theory also offers a single mechanism that accounts for diverse perceptual and neurobiological phenomena such as end stopping by stable perception like illusions repet repetition suppression illusory motions and attentional modulation of neural activity

So they're pointing to the integrative nature of this theory or framework to model all of these diverse phenomena as being an advantage.

Like we could have a special theory for just bistable perception, a special theory for just repetition suppression, and so on.

But this is a framework by which those outcomes are occurring as kind of

outcomes of an underlying framework predictive coding rather than needing special theories.

So that kind of like parsimony or consilience is pursued in the areas like cognitive neuroscience.

And they write, as such, and perhaps uniquely among neuroscientific theories, predictive coding encompasses all three layers of Mars hierarchy by providing well-characterized and empirically supported view of what the brain is doing at all of the computational, algorithmic, and implementational levels.

so mars very influential hierarchy it's kind of like tinbergen's questions or a little bit like maslow's hierarchy you can think of it as like an organizing or a sense making framework again to the question what are brains doing and so we're going to have like three different kinds of answers to what brains are doing there's going to be an answer at the level of computation

algorithm and implementation.

And so here with the example of a bird flying, the computational level is like flight.

It's like the why.

What is the algorithm doing?

It's a sorting algorithm.

What is the brain doing in this situation?

It's tracking a moving object.

Then there's the flapping level, which is the algorithm, which is kind of like pseudocode, and that's suggesting the what of how that why is realized.

Like what is happening in this sorting algorithm?

Well, it's doing this, this, this, this, this.

What is the flight function being realized by?

It's being realized by flapping of the wings.

And then there's the implementation.

So it's not just the flapping of anything,

it's the flapping of something specific and so that's the implementational details and those levels of analysis have been helpful and provocative in connecting computational cognitive theories to neurobiological theories so it's been a very influential and um discussion

provoking framework that has helped people connect as well as challenge some of the similarities and differences between computers and brains.

Great.

This is still in the introduction and they're going to kind of give us a qualitative perspective on predictive coding and then we're going to be diving into a lot more historical and technical detail.

But how does predictive coding work?

The core intuition behind predictive coding is that the brain is composed of a hierarchy of layers which are each making predictions about the activity of the layer immediately below them in the hierarchy.

So there's two directions in this predictive coding multilayer scheme, which we're just introducing here as kind of a heuristic.

And then we're going to zoom in to just what's happening at one level.

And then we're going to pull back out to kind of recovering this multilevel formalism.

The downward descending predictions at each level, that's the blue arrow, are compared with the activity and inputs of each level to form prediction errors.

This is the information in each layer which could not be successfully predicted.

These prediction errors are then fed upwards, that's the red arrow, to serve as inputs to higher levels, which can then be utilized to reduce their own prediction error.

So that's kind of the bigger picture.

Like the idea at the top level is like, I'm tracking a soccer ball that's moving from the left to the right.

And then that gets transduced in certain ways.

And that is in this case, resulting in kind of a visual perceptive level prediction about what one should expect to see.

And this is again, just the multi-level predictive coding scheme that is often used, but

We're going to zoom back to just what's happening at a single level, and then what is happening at multiple levels.

Awesome.

Here's a fun 2018 paper by Carl Friston, Does Predictive Coding Have a Future?

And this is going to provide just a little bit of context, and then we're going to jump more into, though,

Carl wrote, in the 20th century, we thought the brain extracted knowledge from sensation.

That's like the recognition model.

And that's sort of the incoming sensory processing model.

The 21st century witnessed a strange inversion in which the brain became an organ of inference, actively constructing explanations for what's going on out there beyond its sensory epithelia.

One paper played a key role in this paradigm shift.

and carl goes on to write that a 1999 paper by rao and ballard which we're going to talk more about for him was one of those papers that's a once in a decade find

So we're going to talk more about it, but this is just kind of connecting it early to Carl Friston's line of research, the connection between active inference and predictive coding, and the idea that sometimes it's these early transdisciplinary papers that can stitch two different fields together, and that that can have long-term consequences for essentially perennial questions like how do perception, learning, attention, and sensation

all work together and under what imperative might they work together.

And then the authors provide a very preliminary discussion that they're going to go into more later about how predictive coding matters for machine learning.

And so they write, predictive coding proposes that using a simple unsupervised lost function, such as simply attempting to predict incoming sensory data,

is sufficient to develop a complex general and hierarchically rich representations of the world.

They're suggesting that this has found support in the successes of modern machine learning models that are trained on unsupervised predictive or autoregressive objectives.

So here's some of the papers that they cite.

This is the Brown et al and the Kaplan et al.

And these both have to do with the training of what are increasingly becoming important modern machine learning methods, which are the language models.

In contrast to modern machine learning algorithms, which are trained to end with a global loss at the output.

So, given the total big dataset, give me the lowest total error on the whole dataset.

In predictive coding, prediction errors are computed at every layer, which means that each layer only has to focus on minimizing local errors than global loss.

This property potentially enables predictive coding to learn in a biologically plausible way using only local and Hebbian learning rules.

And this is going to connect to common filtering and PID control.

So they're kind of laying out the land and showing where they're going to connect in the introduction very, very qualitatively.

And then again, we're going to zoom back down to find out what's happening at one level of predictive coding, and then try to rebuild some of the formalisms related to Kalman filtering and PID control.

and the bigger questions that people might be curious about asking would be like what does it mean for computation to be biologically inspired what is possible when we're thinking about some of the similarities and differences and complementarities between biological and different kinds of conventional and unconventional computation the next

heart maria please take it away i'll be changing slides so thanks a lot for adding a lot of the historical context and looking forward to learning about what you have to share here


SPEAKER_01:
Okay, so this is actually the paragraph that is in the paper, okay, that they start talking about the history of predictive coding.

So, while predictive coding as a neuroscientific theory originated in the 80s and 90s with Monfort, Rowe and Ballard, Schneveson...

or something, Laughlin and Dubs, and was first developed into its modern mathematical form and comprehensive theory of cortical responses in the mid-2000s with Friston, it has deep intellectual antecedents.

These precursors include Helmholtz's notion of perception as an unconscious inference and Kant's notion that a priori structure is needed to make sense of sensory dictum, okay?

and as well as early ideas of compression and feedback control in cybernetics and information theory.

And on the next slides, I will be talking a little bit more about history and philosophy of predictive coding, and then some core formalisms and various generalizations and elaborations of predictive coding.

If there is anything you want to say, Daniel, just feel free to jump in, okay?

So, the historical and philosophical background of perception is huge.

However, seen in the context of predictive coding framework, authors usually come up with a similar reasoning about perception.

And then we can start thinking with the Greek philosopher Plato on his Allegory of the Cave.

from 514 to 520 before Christ, which he said basically that some people would be trapped in a cave forever watching shadows cast by objects moving near a fire behind them.

And then they would like be there forever, have children that would grow like that inside the cave.

And, you know, eventually these shadows would,

from the objects near fire behind them would be the real thing and the only thing for these people.

They wouldn't believe that there is anything outside the cave.

So this would be it.

These shadows would be the only thing.

So what Plato would claim here is that our own conscious perceptions are just like these shadows, meaning that they are indirect reflections of hidden causes that we can never directly encounter.

A few centuries later, the Arab medieval scholar

Al-Hazen wrote a lot of interesting notes on visual perception.

Actually, he wrote six volumes, if I'm not mistaken, of his book of optics, where he explored the view that human perception often depends on mechanisms of judgment and inference instead of providing access straight to the world.

You can see that very early in our Western thinking, we have this notion that we do not have direct access to the world, okay?

That there is some inference going on.

Okay?


SPEAKER_00:
Great.


SPEAKER_01:
All right.

Jumping to the 18th century, we can find two important philosophers, the English David Hume and the German Immanuel Kant.

Hume, in 1739 to 1740, explored the inductive inferences and causation.

The problem of induction that he brought was an analysis of cause and effect in perception and the conclusion

that he gets is that all our mental life could be traced back to the effects of such experience because since we do not have direct contact with the world we could never have one-to-one relationship with the objects there and therefore it is possible to have one effect with many possible causes and one cause with many effects

The solution here would be then extracting statistical regularities and imagining what happens when the world is intervened upon a controlled manner, as the philosopher Jacob Howie developed in his book, The Predictive Mind, in 2014.

Equally important in the critique of pure reason in 1781, Kant will add an important element to this story among

you know, countless other elements to this story.

He will add that the brain uses existing information regarding space and time to make sense of the chaotic sensory data it constantly receives to provide an organism the perception as they know.

So all of these scholars would tell us right in the early ages that we do not have direct access to the things in the world and we need pre-existing information to make sense of all the data we constantly receive.

Lots of predictive coding stuff.


SPEAKER_00:
It really makes me wonder why it's sometimes still so ineffable or poorly understood or even seen as controversial, some of these perspectives which have been, as you are describing, thought of and convergently come to for many thousands of years in different cultures.

So really interesting.


SPEAKER_01:
Yes.

In the late 19th century, the German scientist Hermann von Helmholtz in 1860 depicted perception as involving probabilistic inference.

He is one of our gods in the community.

He was really inspired by Kant.

With that inspiration, he developed the idea of the brain as a hypothesis tester and that perception is a process of unconscious inference.

More specifically, this idea that he developed in experiments was that perception has to be inferred by combining sensory signals with the brain expectations and beliefs about their causes.

Such inferences happen without the subject's awareness.

I mean, it has to happen without the subject's awareness.

Because, you know, imagine all the chaotic sensory data coming and you open your eyes and you have to try to organize everything.

I mean, it doesn't happen that way.

You just open your eyes and everything is still stable.

And so, it's unconscious.

And...

it needs to keep track of the causes in the world by updating perceptual best guesses, or you can say hypothesis, as new sensory signals arrive.

So here he built the idea that we use sensory signals with the brain expectations to update the hypothesis we make for the causes

of the effects we receive in the world.

And this means that he somehow turned Hume's and Kant's insights scientific, claiming that we can only infer things out there in the world behind a sensory veil, which, if I'm not mistaken, will be eventually called Markov blanket.

Is that right, Daniel?

Not sure.

Maybe unveiled.

Hamholtz was crucial to many different works in machine learning.

one that was even nicely named as Helmholtz machine.

He was crucial in psychology, neuroscience, cognitive science, and so on.

Particularly in psychology, Helmholtz's insights inspired Yuri Schneisser in 1967 and later Richard Gregory in 1980 and Yves Arvin Roque in 1983 to develop the analysis by synthesis approach, which is the process that analyzes a signal or image by reproducing

it using a model.

There is a picture of analysis by synthesis approach with some schema available in this link.

In fact, Gregory built some kind of neurohypothesis testing model based on his Helmholtzian understanding of the brain as continually formulating perceptual hypotheses about the world and testing them by acquiring data from sensory organs.

okay so many things happened after the analysis by synthesis stuff but i will just jump to what we have in philosophy today because we will touch the formalisms and the computation stuff afterwards okay so i'll just make a jump and go to what we have in philosophy today so uh

Recent works in philosophy and cognitive neuroscience have contributed to the expansive predictive coding framework and the Bayesian paradigm in the brain.

And many of the seminal works we have in the field addresses predictive coding as predictive processing without, you know, any relevant difference in the first instance.

And, well,

For instance, the philosopher Jacob Howey, that is based in Monash University, Australia, aimed on the improvement of prediction error minimization mechanism and the concept of winning hypothesis for conscious perception.

Also, he is interested in what PPE, predictive processing studies, can bring to people with mental disorders such as autism and schizophrenia.

Actually, the three of them, of the philosophers here, are interested in what PPE can bring to improvement in psychiatry and medicine in general.

So, in 2014, he published an important textbook on the matter, that is, The Predictive Mind, the red book you can see.

where he tries to solve the problem of perception inspired on humans' induction problem with Bayesian inference and predictive processes.

Later, in 2016, the philosopher Andy Clarke, based on University of Sussex, England, carefully developed what he called action-oriented predictive processing in his book, Surfering Uncertainty, where he adds and advances the crucial role of action in our perception.

And he makes lots of interesting connections between recent works in computational neuroscience and artificial intelligence and his eclectic philosophy of mind.

And last year, in 2021, the neuroscientist and USF, one of the authors of this paper we're talking here, and also based at the University of Sussex, made an amazing contribution to the field with his book Being You, where he claims that we are beast machines whose perception is a controlled hallucination.

And he improves consciousness descriptions using predictive processing framework and establishing the real problem of consciousness that, in his own words, requires explaining why a particular pattern of brain activity or other physical process maps to a particular kind of conscious experience, not merely establishing that it does.

Two, you know, philosophers and scientists work on.

And they, here they are.

Okay.

All right.

So I was talking about predictive coding, and then I went to predictive processing.

Is there a difference?

Well, many authors would use the terms freely, okay?

But then I found Clark saying in his book the difference that he understands from predictive coding to predictive processing.

So he puts as the fact that predictive processing is not simply the use of the data compression strategy known as predictive coding.

Rather, it is the use of that strategy in a very special context of hierarchical

multi-level systems deploying probabilistic generative models.

Such systems exhibit powerful forms of learning and deliver rich forms of context-sensitive processing and are able flexibly to combine top-down and bottom-up flows of information within multi-layer cascade.

Predictive processing does combine the use within a multi-level bi-directional cascade of top-down probabilistic generic models with the core predictive coding strategy of efficient encoding and transmission.

Okay.

From this, I'm not sure if you agree with me, Daniel, but reading this paragraph makes me wonder, I mean,

Okay, predictive coding is a local bottom-up, top-down cascade and stuff, but it also is multi-layer.

Yes, it also has some hierarchy.

So I'm not sure what he means.

I mean, I'm not sure if I understand the difference between predictive coding and predictive processing as he puts it.


SPEAKER_00:
Yeah, very insightful.

And it made me think about the difference between like

coding like programming and like data coding, like Python coding for data and data processing, which is a little bit like it could be multiple computer languages.

And so you're really highlighting something important, which is that when the data are compressed according to a strategy where the errors are being transmitted rather than the actual sort of estimate itself, that's like that information channel is in a predictive coding way.

And then this helps us use predictive processing for the bigger picture, for systems that implement predictive coding modules, but that's not the only module they're deploying.

So that'll be something cool to hear people's perspective on.

Really nice distinction here, though.


SPEAKER_01:
Yeah.

And OK, so I wrote something here that

just like you said, that maybe coding would be better to use for formalisms and implementations and processing for the philosophical understanding that prediction is the basis of signal interpretation, e.g.

as opposed to descriptive.

Okay.

So, jumping back to formalism.

predictive coding in information theory and signal processing.

Okay, so another deep intellectual influence in predictive coding comes from information theory from Shannon in 1948, and especially the minimum redundancy principle of Barlow in 1961, 89, and 61, too.

Information theory tells us that information is inseparable from a lack of predictability.

If something is predictable before observing it, it cannot give us much information.

I loved this sentence.

Conversely, to maximize the rate of information transfer, the message must be minimally predictable and hence minimally redundant.

Predictive coding as a means to remove redundancy in a signal was first applied in signal processing, where it was used to reduce transmission bandwidth for video transmission.

And do you want to say something?


SPEAKER_00:
i'll just describe this without reading it it was a a common uh and effective although it can be improved on by other methods it's a method for video encoding known as frame differencing where basically uh one frame gets described and then subsequent frames only the pixels that change have to be described so that is a lot like an analogy to predictive coding in the sense that

it's able to connect the differences between subsequent time steps and use that to give really only the informative pieces.

Like I have a 100-digit number, and then I'm just going to tell you the third digit changed.

Now it's an 8.

Instead of having to repeat the same number and then 99 of those digits, they're informative, like in the sense that they're good info.

It's true, but they're not informative in the information theory context because they were quite predictable before observing it.


SPEAKER_01:
Initio Schemes used a simple approach of subtracting the new-to-be-transmitted frame from the old frame, in effect using a trivial prediction that the new frame is always the same as the old frame, which works well in reducing bandwidth in many settings where there are only a few objects moving in the video against a static background.

And more advanced methods often predict each new frame using a number of past frames weighted by some coefficient, an approach known as linear predictive coding.

OK, so now predictive coding in the eye and the brain.

The first concrete discussion of predictive coding in the neural system arose as a model of neural properties of the retina, with Srinivasan .

specifically as a model of center-surrounded cells which fire when presented with either a light spot against a dark background on center of surround or alternatively a dark spot against a light background of center on surround cells.

It was argued that this coding scheme helps to minimize redundancy in the visual scheme specifically by removing the spatial redundancy in natural visual scenes

that the intensity of one pixel helps predict quite well the intensity of neighboring pixels.

And then the first picture related to it.

Do you want to talk about it or just?


SPEAKER_00:
We're just going to look at two different tissues of the nervous system that have some predictive elements.

And this is referring to some of the photosensitive cells that are on the retina.

And that's work that had been done qualitatively and empirically for hundreds of years, but then was connected to predictive perspectives in the 80s.

then we're also going to look as you'll now unpack at what's happening in the cortex of the brain amongst different layers


SPEAKER_01:
with Manfred in 1992, was perhaps the first to extend this theory of the retina and the LGN to a fully fledged general theory of cortical function.

His theory was motivated by simple observations about the neurophysiology of cortico-cortical connections.

Specifically, the existence of separate feed-forward and feed-back paths

where the feedforward pathways originated in the superficial layers of the cortex and the feedback pathways originated primarily in the deep layers.


SPEAKER_00:
So we kind of had this philosophy development happening over thousands of years.

And then in the 1900s, people were starting to connect the neuroanatomy and the histology to some of these predictive ideas.

And then now let's kind of close the loop and bring it back to the critical work that Friston highlighted in his 2018 paper.


SPEAKER_01:
Yep.

And now predictive coding in the brain gets computational, mathematical.

While Montfort's theory contained most aspects of classical predictive coding theory in the cortex, it was not accompanied by any simulations or empirical work, and so its potential as a framework for understanding the cortex was not fully appreciated.

And so again, the similar work of Rao and Ballard in 1999 had its impact precisely by doing this.

And so they created a small predictive coding network according to the principles.

Oh, sorry.

They created a small predictive coding network according to the principles identified by Mumford and empirically investigated its behavior, demonstrating that the complex and dynamic interplay of predictions and prediction errors could explain several otherwise perplexing neurophysiological phenomena, specifically

extra-closical receptive field effects such as end-stopping neurons.


SPEAKER_00:
Yeah, this is pretty interesting because even the idea that the neurons that are active while a certain stimuli is being presented, that those are neurons that are receptive to that stimuli.

That's within the paradigm of signal reception.

And then it's like there's classical signal reception.

You know, the pitcher throws the ball, the catcher catches the ball.

It's classic.

But unfortunately, or not, there's all these so-called extra-classical effects.

And so this paper proposed a simple architecture that was able to encompass the so-called classical as well as some of the important extra-classical effects.

effects under this predictive processing framework and so that really led to a lot of developments in neuroscience in the last 20 plus years and that's exactly what we're going to go into now so thanks for providing that that awesome context like it really helps i think situate some of the details that we're about to look into okay so let's get a little technical

what happened after round ballard's 1999 synthesis so remember that carl friston said that that was like one of those once in a decade papers for him so how did he change his action selection publication policies after reading that paper well anyone could guess but what friston did was that he cast the predictive coding algorithm as approximate bayesian inference

upon Gaussian generative models.

And this is going to be connected.

This is all in the paper for people to read, but this connects basically all of the previous themes that we had been talking about, like the information theory, minimum redundancy, and the Helmholtzian idea of perception as inference come together in the Bayesian perspective on the predictive coding architecture by Rao and Bollard.

so the authors write friston's approach reformulates the mostly heuristic round ballard model in the language of variational bayesian inference which we're going to look more at and friston showed that the energy function in row and ballard can be understood as a variational free energy of the kind that is minimized through variational inference so that really connected the dots between

predictive coding architectures, and the empirical biological findings to all this work on variational Bayesian inference.

And those are the 2003, 5, and 8 single-authored papers by Carl.

So.

what is variational and bayesian and variational bayesian approaches and we're just going to look at the author's words and there's many many awesome places to look for educational materials and also check out some of the live streams like 26 32 34 37 and 39. so to be brief about it the author's right

Variational inference approximates an intractable inference problem with a tractable optimization problem.

So like a super hard problem to solve just by thinking about it and then guessing at the right answer.

with a tractable optimization problem.

So it's like 20 questions.

It's going to be hard to guess on your first question, but if you take this iterative optimization approach, maybe there's a way to actually resolve it.

So it's not exactly like that, but it's like kind of, again, flipping out something that's hard to solve in one shot with an approach and this iterative gradient descent type improvement that actually gets you to a good solution.

And so this is where we're gonna introduce some of the variables and the letters that we're going to use.

Following a lot of perceptual work in Bayesian statistics, we're gonna use O for observations, which are like data, either the actual data that the sensor provides us or generated data of the kind that we would expect the sensor to give us.

And then X are the latent or the unobserved states of the system.

So like X would be the temperature in the room and O is what the thermometer reading is.

And that's sort of a Bayesian approach that helps us have a generative model of the data generating process, which is also known as the joint distribution because it is jointly over both the observations and the latent states.

Where variational inference comes into play is hinted at in this paragraph on base.

So in order to do exact base, one would have to find this normalizing factor.

However, that can be intractable because it requires basically integrating or summing over all latent variable states.

And that's not always easy or known.

And so the approach of the variational method

is aiming to approximate the posterior using an auxiliary posterior using a different set of parameters

So the Q is going to be reflecting the distribution, the variational distribution that we control that is of a family of functions that is more amenable to optimization.

So P could be like a super messy function, but Q is going to be constructed by the modeler to be a lot simpler.

And so it is going to have its own parameters.

And those parameters phi are mu and sigma.

And so it's kind of like we would be interested in the mean and the variance of the x, the actual temperature in the room, given the thermometer.

But what if it were too hard to even get that?

Well, what we might want to do would be imagine that the temperature were normally distributed like a Gaussian distribution with this mean mu and the variance sigma.

So even if the temperature in the room weren't actually Gaussian distributed, maybe it's approximatable enough.

And so the variational approach to the Bayesian method is to introduce this variational distribution Q that's going to have like a lot of good features moving forward.

Here's how that cue comes into play.

This equation one is expressing using the variables that we've seen.

It's saying, and also introducing this divergence D. So first just the definition of D and then what this formalism says.

So D of P and double line Q. So any double line means between this and that.

D is a function that measures the divergence between two distributions.

So for example, P and Q. And here, the divergence is going to be calculated as the KL divergence, although other divergences are possible.

So when the divergence between the one that we simplified and controlled Q and that intractable true posterior, if that divergence were minimized to zero, we would be like fitting P as well as we could with Q.

And this is saying in one, it's saying Q star, the best Q of X, the latent states, given the data and the variational parameters.

So the best possible room temperature prediction given thermometer readings and variational parameters are a minimization over all the variational parameters.

of the divergence between the Q distribution that we control of exactly what we want the best answer for and the P generative model of exactly what would be the best to compute, like the temperature given the observations on the thermometer.

And so that goes a long way towards rewriting an inference problem as a divergence problem.

But they write, however, merely writing the problem this way does not solve it because the divergence that we need to optimize still contains the intractable true posterior.

So it's rewritten as a divergence between like something we control and something that we kind of set out to do this because we couldn't calculate it.

So we have prepared it, but still this is intractable.

So this is not usable alone.

They write, the beauty of variational inference is that it instead optimizes a tractable upper bound on this divergence called the variational free energy, VFE.

To generate the bound, we apply Bayes' rule to the true posterior to rewrite it in the form of the generative model and the evidence.

So here's that P of X given O. That's rewritten as now not of X given O, but P of O comma X. That's the joint distribution divided by the observations.

So this, by the Bayes rule, P of X given O is equivalent to this.

That's why this is the first line here.

They provide some rewritings and then they write, in the third line, the expectation around P of O, so the actual likelihood of the observations themselves, like how likely is the thermometer saying 21?

It vanishes.

since the expectation is over the variable X, which is not in P of O. So it's like, what is the expectation of this coin flip over the temperature tomorrow?

It's like, because they're different variables, it makes them very easy to separate out if we're only interested in one of them.

then they write that this f the free energy is a very is a tractable quantity since it's a divergence between two quantities we assume that we as the modeler know the variational approximate posterior q of x given o that's the distribution that we control and the generative joint distribution p of o comma x

So we traded out this intractable true posterior, where it's like, if we knew it, then we would just stop there.

But we don't know it.

So we've traded it out for a divergence between something that we totally control, q of x conditioned on these other parameters, and a joint distribution, which we might have uncertainty over, but at least can be modeled.

So that's one of the key pieces of variational Bayes.

And that's not specific to predictive coding.

It's just an important way that the authors are introducing it here.

Anything to add?

All right.

So let's kind of continue on this theme.

Since f is an upper bound, by minimizing f, we drive the variational distribution that we control closer to the true posterior.

As an additional bonus, under certain conditions,

f can be used for model selection.

So that means it can be used not just to kind of fine tune a model that we've already chosen, but it can actually do choosing from parametrically or structurally different models.

They write, we can gain an important intuition about f by showing it can be decomposed into a likelihood maximization term

in the KL divergence term, which penalizes deviation from the Bayesian prior.

These terms are often called accuracy and complexity, and this decomposition is often used in different machine learning algorithms.

So that's one rewriting of free energy as this divergence between the Q and the P, and then rewriting that as an accuracy and a complexity, which we'll go into maybe more another time.

And they write, in many practical cases, we must relax the assumption that we know the generative model P of O and X, the joint distribution.

Luckily, this is not fatal.

Instead, it is possible to learn the generative model alongside the variational posterior on the fly and in parallel using expectation maximization.

So this is basically the alternation

shown by equations three of how we can be setting one side of this kl divergence fixed and optimizing the other side and then going back and doing it the other way and so that kind of back and forth expectation maximization we're going to be like reducing this divergence from both sides and so formalism 3 are saying the variational parameters phi at the next time step t point t plus one

are an optimization argmin of those parameters holding the theta constant.

And then the second part of formalism three is the exact opposite.

Now the generative model parameters, theta at t plus one, are minimization over literally the same thing, but holding the variational parameters constant.

So there's a lot more to say about expectation maximization, but this is like converging to a small variable

divergence by whittling away at both sides and kind of alternating there so that's how expectation maximization can be used as a heuristic algorithm for variational Bayesian optimization

How do we go from variational inference, which all of those previous slides are things that we've basically talked about before and that are not applying to predictive coding specifically.

So how are we going to get to predictive coding?

They write, having reviewed the general principles of variational inference, we can see how they relate to predictive coding.

First, to make any variational inference algorithm concrete, we must specify the forms of the variational posterior and the generative model.

It's like if you want to do formalism three, those are the two pieces you need.

You need like the phi stuff and the theta stuff.

And so they specify it here.

N means a normal distribution with like a mean comma variance format.

And so they're gonna define a Gaussian form for the generative model

mean of this likelihood gaussian is assumed to be some function of hidden states which can be parameterized with theta while the mean of the prior gaussian distribution is going to be a g of mu so we're going to have like f of theta and g of mu the variances of these two gaussian distributions of the generative model are sigma 1 and sigma 2.

This is a slight technical detail, but they're going to assume that the variational posterior is a Dirac delta, which is like a spiking function distribution that's centered at the mean.

However, they explore that differently with Laplace assumption.

So this is basically taking what we discussed about variational inference and preparing it to be entered into a predictive coding way.

So they're setting up the problem with which parameters you're going to want to do variational inference on in order to implement predictive coding.

And there's of course more to say, but we're just giving a first pass.

In Appendix A is where they describe the difference between using the Laplacian approximation and the Dirac delta.

So it's footnote eight, which suggests moving to Appendix A and looking at the Buckley 2017 for a walkthrough.

But this on the bottom right is kind of like a summary of the difference between the Dirac delta approach and the Laplace method approach.

So the Dirac delta is like we're trying to find the spike that's at the mean or the median.

There's some other details in play that...

is where the bulk of the probability distribution mass is whereas the laplace method tries to fit a polynomial like just a second degree polynomial like a quadratic over the probability distribution also in the way that is best fitting so there's some similarities and some differences and it's explored more in the buckley 2017 paper on the fep for action perception mathematical review

So let's continue with our exploration of how we do variational Bayes and make it a predictive coding model.

the top they write we define the prediction errors epsilon o that's like the error on observation and epsilon x that's the error on the mean states so the epsilon o is like o minus f of some function you know f of some parameters and then there's also the epsilon x so how much prediction error do you have about the observation how much prediction error do you have about the hidden state

Epsilons are the prediction errors, and they're parameterized by these theta 1 and 2 variances.

Given all of this, we can derive dynamics for all the variables of interest.

So that's the actual underlying hidden state, the temperature in the room that we want to be actually predicting, mu.

And then, well, the variational parameter of it.

And then theta one and theta two, which are the variances.

You can think of it as like the variance of the room temperature and the variance of the thermometer.

I hope that's not...

wrong or an oversimplification but those are like the two variances and we want the real temperature of the room they write we can derive dynamics for all these variables of interest by taking the derivatives of the variational free energy f the update rules are as follows and so this is change in mu theta 1 and theta 2 over time d mu dt d theta 1 over dt and d theta 2 over dt

and that has some equivalences with some f's so it's going to be going from just the change in the mu estimate through time to something involving free energy and then that's defined more formally importantly these update rules are very similar to the ones derived in rao and ballard 1999 and therefore can be interpreted as recapitulating core predictive coding update rules

For instance, the mus are typically interpreted as rapidly changing neural firing rates, while thetas are slowly changing synaptic weight values.

So they're weaving together, connecting the biology with the formalisms here.

And they write also that that mu can be understood as the process of perception.

That's like, how hot is it in the room?

since mu is meant to correspond to the estimate of the latent state of the environment generating the observations on the thermometer.

And by contrast, the dynamics of thetas can be thought of as corresponding to learning, since theta effectively defined the mapping between the latent state and the observations.

So it's like you think you know how the thermometer is related to the temperature in the room, and then you see your thermometer changing.

You're changing how hot you think the room is.

That happens over a shorter time scale.

And then over a longer time scale, you might come to learn the variability of the temperature or how noisy the thermometer is.

But that's more like learning than perception.

However, they're on a continuum using this model.

so actually this up to formalism 9 completes the core formalism of predictive coding which is it's a variational Bayesian approach to having this ongoing prediction error minimizing approach to latent state estimation that is the heart of predictive coding and now we're going to jump into like a few different elaborations that we're going to be moving through a little bit more quickly anything to add Maria

Awesome.

The previous examples focused on predictive coding with a single level of latent variables, mu1.

However, the expressiveness of such a scheme is limited.

Deep neural networks in machine learning have demonstrated that having hierarchical sets of latent variables is a key to enabling methods that learn powerful abstractions and handle intrinsically hierarchical dynamics of the sort that humans intuitively perceive.

Predictive coding schemes introduced can be straightforwardly extended to handle hierarchical dynamics of arbitrary depth, equivalent to deep neural networks in machine learning.

This is done through postulating multiple layers of latent variables, X sub 1 through X sub L, and then defining the generative model as follows.

So like P, the generative model is going to be basically over all the layers.

So just as the generative model and the variational distribution had to be defined for the single layer model, here they're going to define the P distribution and now they need the Q. We define a separate variational posterior for each layer.

So they define the P's and the Q's just like they did in the single layer.

And then that allows them to calculate the variational free energy, which is a sum of the prediction errors of each level.

so this is f now not just over one layer but multiple levels so the variational posteriors that need to be calculated are partitioned across the layers which allows them to be summed in a very straightforward way given that free energy divides nicely into the sum of layerwise prediction errors it comes as no surprise the dynamics of the mu and the theta are similarly separable across layers

that allows different layers of this prediction hierarchy to be precise or imprecise.

and allow those movements to happen in a way that's uncorrelated, which is not just because the real world presents itself with settings where there's like confidence at lower and higher and vice versa, but it makes this calculation of the free energy of the whole system more like a simple sum.

Whereas if there was these like really complex interactions with the first layer by the third layer, if the fifth layer is this way, then doing the statistics would be a lot more challenging.

So visually, here's what that looks like.

We have the mu, which is the mean estimate of what's happening at that level.

And then there's passing of these epsilon error terms.

So they write, this is the architecture of a, in figure one, multi-layer predictive coding network, shown here with two value and error neurons in each layer.

The value neurons project to the error neurons of the layer below, and the error neurons represent the current activity.

So this is like starting to walk us back towards that cortical layout where there's a cortical column, where there's some so-called upwards and downwards signaling, but also there's lateral signaling.

So this is a graphical model that's reflecting the way that predictive coding can be arranged almost like in series and in parallel to have like wide models and deep models.

Just like you could have a neural network where it was

um four neurons four neurons four neurons four neurons or you could have 64 and then 64. that would be like a shallower but wider model and so that notion of shallowness and depth is also going to come into play with predictive coding here we're in formalism 12 and 13. and so now we're looking at the

rate of change the derivative of mu sub l so the estimate of the mean at that layer and theta l the variance at that layer so these are the update rules the gradients for the mean and the variance of different levels and they're also written as free energy functionals of those layers and

The dynamics of the variational means depends only on the prediction errors at their layer and the prediction errors on the layer below.

So again, we don't have this epsilon at L minus one connecting up all the way to mu.

It's only through these local connections within and between layers by which we're needing to calculate anything.

We can think of the muse as trying to compromise between causing error by deviating from the prediction from the layer above and adjusting their own prediction to resolve error at the layer below.

So it's kind of like a hierarchy of bosses or tasks.

And there's like top-down expectations and there's the bottom-up reality, which might be like ahead or behind schedule.

And then it puts each person like in this compromise situation.

Crucially for conceptual readings of predictive coding, and this is where there's like a doorway to the philosophy and some of the broader discussions, this means that sensory data is not directly transmitted up through the hierarchy as is assumed in much of perceptual neuroscience.

And so it totally returns us to these questions like, what is perception?

What is cognition?

What is action?

What is coming from the eye to the brain?

Or what goes from the brain to the eye?

What is the eye doing?

What is the brain doing with respect to the eye?

And how does it relate to predictive coding?

Any quick answers on those?


SPEAKER_01:
Not really.


SPEAKER_00:
Cool.

So after introducing that kernel of the predictive coding formalism and taking it into the multi-level context, now we can look at another elaboration or generalization.

So they write in 2.3, we have considered the modeling of just a single static stimulus O. However, most interesting data that the brain receives comes in temporal sequences O bar, which is O through time.

To model such temporal sequences, it's often useful to split the latent variables into states, which can vary with time, and parameters, which cannot.

In the case of sequences, instead of minimizing the variational free energy, we must instead minimize the free action, which is the path integral of the variational free energy through time.

We're not going to go into the formalisms related to the generalized coordinates 14, 15, 16, and 17, but it's something that we can explore later.

And just as a reminder, just to kind of...

excite somebody who might want to explore it.

We explored the idea of the generalized coordinates of motion a lot in Acton Livestream number 26 with DaCosta et al.

And so here was a model where we have from left to right time at different time steps.

And then there's the observables of something.

And then it is part of this like column of higher derivatives of its motion.

So it's like position, it's,

its velocity, its acceleration, and so on.

And so at each time step moving forward, you're computing the generalized coordinates of motion for like the movement of a baseball.

And that is what ties generalized coordinates of motion to PID control, which will also come back at the end.

also known as integrator chains, and this is all happening within this discussion of Bayesian mechanics, just like if we had been talking about the baseball, we would be talking about physical mechanics or statistical mechanics.

Well, it's Bayesian, so it's Bayesian mechanics and the physics of action and control.

That's what they explore in Section 2.3.

Section 2.4 introduces and goes into a little bit more detail on precision.

One core aspect of predictive coding absent in the original Rao and Ballard 1999 formulation

is the notion of precision or inverse variance.

So precision and variance are like one over each other.

So if someone said it's very high precision, that's a very narrow, very sharp distribution, and one over the variance is another way to write that.

So sometimes it's used with like a beta and a gamma in other models.

Precision serves to multiplicatively modulate the importance of the prediction errors and thus possess a significant influence in the overall dynamics of the model.

And so we won't go into every detail with how the precisions are themselves fit, but just wanted to note that this big sigma is the free energy being summed over the layers.

So big sigma is a multiplicative sum

oh sorry it's just a sum but then this smaller Sigma inside is Sigma as the precision Matrix at a given level so it's kind of like different uses of Sigma but this is a way where the variances and the prediction errors at a given level are being summed across levels and that's helping fit these precision terms

statistical detail but it's important for making it work in real inference okay in section 2.5 they they pull back from elaborating on predictive coding and they talk more about how it has some biologically plausible bases so this was quite interesting

While technically predictive coding is simply variational inference and filtering algorithm under Gaussian assumptions, from the beginning it has been claimed to be biologically plausible theory of cortical computation and also for other brain regions like the eye and for other cognitive systems.

The literature has consistently drawn close connections between the theory and potential computations that may be performed in brains.

For example, Rao and Ballard explicitly claimed to model the early visual cortex, and Friston explicitly proposed predictive coding as a general theory of cortical computation.

This is like the realism and instrumentalism discussion.

Are we just modeling these systems and the kind of data that they provide us?

And it just fits really well as a model, but we're not saying that the architecture of the statistical model has anything to do with the architecture of the physical pieces of the brain?

Or is it a little bit in the gray zone where it's kind of like, wow, that model fits so well and the anatomy looks so good.

Maybe there's something there to it.

And so in this section, they review empirical work that has attempted to validate or falsify key tenants of predictive coding in the cortex, as well as some issues with the approach.

So it's mainly focused on the mammalian brain and the neural cell type, but

As always, we can sort of think about what is the adjacent here, like what are the roles of other cell types in predictive coding in the mammalian brain?

And then how might like the insect nervous system or how might other nervous systems or non-nervous systems perform predictive coding?

And then they just provide one example where they take what they had shown in figure one

with this multi-level predictive coding scheme that was sort of predictively programming, as they say, one to think about cortical cognition, and now they're gonna make that connection clear in figure two.

So on the right side is their figure two, and that's the canonical microcircuit model of Bostos et al.

mapped onto the connectivity of a cortical region and so then in red are those parameters that we've been discussing the mus and the epsilons the means and then the prediction errors and then here's just a few other representations of what others have written

about like that cortical column so here's like an evolutionary perspective that connects some of the mammalian cortex with the avian pallium brain region and so just the way that people work out these kinds of circuit diagrams like by looking at the actual tissue and then here's the statistics and so how does the tissue um relate to

statistical model but that's what they explore in figure two okay any thoughts okay in section three they continue to review empirical work but with more of a focus on machine learning of the unsupervised and supervised setting

And they're going to also explore how the predictive coding architecture can be made more biologically plausible by relaxing certain assumptions implicit in this canonical model and introduce action into the picture.

Figure three.

They summarize a few different, what they call paradigms of predictive coding.

Summary of the input output relationships for each paradigm of predictive coding.

So it's the input and the prediction, the output of different paradigms.

So here's the classical predictive coding.

Observation data are coming in, predictions about those observations are coming out.

It's an instantaneous real-time snapshot.

the real-time anticipatory unfolding is still getting data as input, but it's predicting the observation at the next time step, which is a similar problem to predicting it at this time step, but it's like a little bit different.

That can happen not just through time, but also in the context of space with like a spatial predictive coding.

And then these two on the right, supervised predictive coding in the generative and in the discriminative direction, have to do with the relationship with reading in labels and giving out observations or vice versa.

Make me a cat.

Here's a picture of a cat.

Here's a picture.

That's a cat.

So these are some of the ways that predictive coding models have been applied.

3.1, they explore the way that unsupervised training relates to predictive coding.

So that is machine learning on unlabeled data.

Like here's...

100,000 hours of human speaking, learn how to speak.

That's like an unsupervised approach.

Whereas the supervised approach would be, here's 1,000 hours of human speaking, here's 1,000 hours of a river, here's 1,000 hours of some other noise.

And so by the label, then the algorithm would learn what is speech versus non-speech versus the unsupervised.

But there's also a lot of continuum and complexity and it's a whole topic.

In some of the subsections of 3.1, they go a little bit deeper into the temporal predictive and the auto-encoding aspects, but we're not going to go into those right now.

In figure four, in section three two, they look at supervised predictive coding and look at it in that forwards and backwards direction.

And so here they're looking at the classic MNIST dataset for evaluation, where there's some pictures of handwritten digits and then it's predicting.

And so here's the generative predictive coding and the discriminative direction,

where here the data are coming in, like the picture of the five with the pixel intensities, and that is relating to the more high-level estimate of it being a five.

So this is observation in, label out.

That's a five.

Here's label in, pixels reflecting it out.

So that's connecting machine learning to predictive coding.

section three three they're also exploring some um relaxed predictive coding and it has to do with relaxing certain mathematical assumptions that they had introduced earlier for simplicity and for clarity we're not gonna go into it but it has to do with making it more useful in the real world by relaxing certain assumptions

And then in 3.4, they explore deep predictive coding.

So they write, so far in this review, we've only considered direct variations on the Friston, Rao, and Bollard models of predictive coding, which are relatively pure.

use only local biologically plausible learning rules there also exists a small literature experimenting with these models often achieve better performance on more challenging tasks than the pure models can achieve so pure again here meaning that it uses only local biologically plausible learning rules so only connected entities locally and so

These models, they suggest, provide a vital thread of evidence about scaling properties and performance of deep and complex predictive coding networks.

And they write, the first major work in this area is PredNet, which uses multiple layers of recurrent convolutional LSTMs, neural networks, to implement a deep predictive coding network.

So here is what PredNet looked like.

This is the original PredNet architecture

And so they have convolutional LSTMs that are passing information in a top-down way and in a bottom-up way.

So maybe we could look at it more later.

And then I also just found this 2019, 2020 paper where they also explored like whether PredNet actually is doing predictive coding, but again, a whole nother rabbit hole.

So section three reviewed through figure three and some discussions on the subsections, a few of the different paradigms and the ways that this predictive coding architecture has been applied.

In Section 4, they're going to connect it to other algorithms.

So we're going to basically speed through all of the connections except for active inference.

First, they connect predictive coding in Section 4.1 to the back propagation of error.

This is really important for neural network training, but we're not going to talk about it here, but it sounds really interesting to discuss.

One that we'll introduce but then skip over most of the details is the relationship between linear predictive coding and Kalman filtering.

So we can remember that the linear predictive coding is not just the frame differencing, it's where there's like that coefficient of how much you should forget each of the previous frames.

And this is known in signal processing as the Kalman filter.

The Kalman filter, as this slide shows, is an iterative mathematical process to quickly estimate true values, position, and velocity.

So that's also like getting towards the generalized coordinates of motion.

And so the Xs are like our observations.

That's the thermometer.

And then here's the red is our Kalman filtered temperature unfolding through time.

So our initial estimate gets quickly corrected.

And then even though there's a lot of noise in the observations,

the Kalman filter is like giving a good prediction on the actual temperature.

And this has been implemented in a ton of Bayesian approaches.

It's a totally standard signal processing Bayesian approach.

And in the following sections in 4.2, they have a lot of formalisms about the Kalman filter.

And in Appendix D, they provide even more formalisms on the Kalman filter.

So we're not going to really go into either of those.

In Section 4.3, they introduced this idea of normalization and normalizing flows.

And so they wrote, the deep link between predictive coding and normalization has been extended by Marino 2020.

situating predictive coding with general recipe for building or representing a complex distribution from a simple and tractable one so we encourage people to look at this merino 2020 paper if they want to learn more about predictive coding variational auto encoders and biological connections but not going to discuss today

One other model that they connect predictive coding to is known as the biased competition model.

And that is unpacked in 4.4, but I just wanted to pull up one example of this biased competition model.

and so this is from the sprotling paper of 2008 within each processing stage nodes compete to be active in response to the current pattern of feed forward activity received from the sensory input or previous processing stage so it's like there's competitions amongst the subunits to be activated by certain kinds of inputs and then there's some technical details okay

Now, finally, to active inference as we're near the end of the discussion.

This will be a little fun to explore also in the coming weeks, of course.

So they write in 4.5, predictive coding can also be extended to include action

allowing for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms.

The key insight is to note there are two ways of minimizing prediction errors.

The first is to update predictions to match sensory data, which corresponds to classical perception.

I thought that the ball was gonna be here, but I guess it's over here.

Perception.

The second is to take actions in the world to force the incoming sensory data to match the prediction.

I'm going to move the ball to where I expect it to be, or I expected the ball to be in the left side of my visual field.

So I'm going to move my eyes to the right so that it's in the left side of my visual field.

So action is not always like reaching out and moving the ball.

It can also be the action of choosing where to look, for example.

And this is what is really fascinating because we spent so long in the earlier sections, in the first 50 equations, you know, without action.

So it's like, oh no, are there going to be like another 250 equations with action coming into the picture?

But actually it's quite a simple introduction.

The basic approach to including action in predictive coding is to minimize the variational free energy with respect to action.

Although free energy is not explicitly a function of action, it can be made so implicitly by noticing the dependence of sensory observations on actions.

So what you see is gonna depend on where you look.

We can make this implicit dependence explicit using the chain rule of calculus.

Active inference, it's inference about action.

So here's the rate of change of action

through time is the rate of change of a free energy functional of, previously it was of observations and of states through time.

And now O is O of A. Observations are a function of action.

They're partially or completely dependent on action.

And that is being understood as a derivative of change in action.

O of A over A, partial O of A over A, is a forward model, which makes explicit the dependence of the observation upon action and must be provided or learnt by the algorithm.

Because that is, how do observations, which are a function of action, change as actions change?

So that is like a layer that hadn't been brought up.

So they're saying, because you do need to solve that ratio or that derivative,

you do need to like learn that or be provided that.

And it's a separate thing to learn in addition to this standard generative model for perception.

So we tuck action as being like a dependency of observations, which is also what connects active inference to perceptual control theory.

So that's how we introduce action into the picture just from a first pass.

Let's go one more level in and then any thoughts you have would be great.

So if we were talking about just the inference case, prediction error minimization would be like the picture that we're seeing is exactly the one that we're predicting or vice versa.

So what does that look like for action?

The prediction error simply becomes the difference between the current observation and the target or set point, already bringing in this kind of homeostasis, allostasis perspective.

However, this raises the question of where the set points comes from.

Where do the targets come from and how are they computed?

Wentz-Priors.

A generic answer to the question is that set points can be inherited from evolutionary or ontogenetic imperatives, or supplied by other neural circuits involved in goal-directed behavior and planning.

For present purposes, we can simply take them as exogenously given variables.

So what isn't addressed in this review is this big question of priors and learning on preferences.

Like how do you know what to prefer?

How do you know how much to change how you prefer?

How tight should your preferences be?

Those are like really, really important areas, but they're just like opening the door, but not gonna go into it.

In 4.5.1, it's also straightforward to model the potential costs of action.

In biological organisms, there's different costs of action.

And so how can that be modeled mathematically?

Well, they say by explicitly including action within the generative model as follows.

And so now they've added in this extra term with a cost of action.

So it just shows how like once action has been introduced as a dependency of the observation function, then there can be like other things that can be calculated related to action.

Any thoughts on action or like in your readings, where has action been integrated well or not with predictive coding?


SPEAKER_01:
Well, I think that from my perspective, predictive coding wasn't really about action before this text.

So it was,

new to me because I always relate action to predictive processing and active inference, not really predictive coding, so it was new.

And from all of this that we've been through, I found fascinating how the complexity grows and how many different applications predictive coding can be found on, especially in machine learning and

You know, all of this mathematical heaviness brings many, many different solutions to problems that we faced for a long time.

So fascinating.


SPEAKER_00:
Nice.

Yeah, agreed.

It's really interesting.

So just one more take on active inference, which we explored in 26 as well.

Like predictive coding, PID control optimizes a system towards a set point and is ideal for simple regulatory systems like thermostats.

Action, A of T, so we're taking that same notion of A as being like something that influences observations, so that's how we introduce action into the model, is we make observations depend on action.

Action is determined by three terms.

A proportional term, which minimizes the distance between the current location and the set point, P. An integral term, which minimizes the integral of this error over time, I. And a derivative term, which minimizes the derivative of the error, D.

combination of the three terms produces a robust and simple control system which can be applied with some tuning to control almost any simple regulatory process and higher coordinates of motion could do even better but as we explored in 26 like often the pid is sufficient and so that's a very common engineering method and so it connects

the Bayesian mechanics of action and perception and cognition of active inference on generalized coordinates to PID control in the engineering setting.

And then they provide a bunch of other formalisms.

In Appendix B,

They give some detail on this idea of natural gradients, which I think we could go into in the dot one and in the dot two.

So we won't talk about it here, but Appendix B is about precision as natural gradients, and Appendix C are providing some challenges for the neural implementation of backpropagation by predictive coding.

So also we're not gonna explore it here, but those are both really interesting, short,

and very topical appendices in section five it's uh of quite good length and we're not going to cover actually any of it today because it's been a great and long enough dot zero but in the dot one and the dot two and beyond we hope to unpack the discussion and the future directions so

We have a lot of room and space for questions.

And I think we both or all walk out of this with more questions than answers.

But what would be your closing thoughts, Maria?


SPEAKER_01:
I'm not sure what to think now.

I have to digest all the information we have here today.

But...

I have this wish to continue with the understanding of formalisms.

I want to understand better what you said here.

And I'm really excited about the biological part of predictive coding implementation and predictive processing.

So as it is not really developed in this paper, I think that the author said

that it's not really explored, actually.

I have this idea of looking to it to see what's been doing in the past years.


SPEAKER_00:
Awesome.

Yeah, I think it'll be some fun upcoming discussions.

Thanks so much for all the help and for this .zero and to Brock as well.

So see you around.

Thanks a lot.


SPEAKER_01:
See you.


SPEAKER_00:
Bye.
"Speaker Name","Start Time","End Time","Transcript"
"Unknown","00;00;26;29","00;00;58;16","Hello and welcome everyone. It's April 28th, 20, 22. And we're here in Acton five live stream number 43.0 discussing predictive coding a theoretical experimental review. Welcome to the Acton Flop We're a participatory online lab that is communicating, learning and practicing applied active inference. You can find us at links here on the slide. This is recorded in an archived live stream, so please provide us with feedback so we can improve our work."
"Unknown","00;01;00;09","00;01;31;07","All backgrounds and perspectives are welcome and we'll be following the video etiquette for live streams head over to active inference dot org if you want to learn more about how to participate in the live streams or anything else happening in Acton Flat OK, we're here today to learn and discuss the paper. Predictive Coding A theoretical and experimental review by Barron Milledge and you'll Seth and Christopher Buckley."
"Unknown","00;01;32;05","00;02;01;23","The video is just an introduction and a contextualization for some of the ideas and some of the details of the paper and the broad sense. It's not a review or a final word. It's as with some of these other technical dense papers, it's like an opening for those who have technical questions at the learning side of things or at the more advanced research side of things to come get involved because this is a technical paper, but also hopefully as well unpack."
"Unknown","00;02;01;23","00;02;26;21","It has a lot of cool biological and philosophical implications so in 43.0, we're going to say hello and introduce some big questions, then cover the aims and claims of the paper. The abstract in the roadmap, and then pretty much just jump right in and we're going to focus a lot on the introduction, the context and the single layer predictive coding model."
"Unknown","00;02;27;01","00;02;49;18","And then we'll go a little bit faster through the later sections of the paper talking about some generalizations of predictive coding and some important points that the authors raise. So if you want to participate in 43.1 or point two in the coming weeks, just let us know. OK, so we can say hello and give any information that we'd like."
"Unknown","00;02;49;18","00;03;23;00","And also just maybe something that we thought was exciting or something that motivated us to get involved in this quite involved paper. So I'm Daniel, I'm a researcher in California, and I was curious to learn more about how predictive coding and predictive processing related to active inference and also just about how different models framed anticipatory systems. So over to you, Maria, hello, Daniel."
"Unknown","00;03;23;00","00;04;02;10","And everyone. I have a bachelor's degree in psychology and I am a master's candidate. In Philosophy of Science in the University of San Paulo, Brazil. And I am researching the relationship between predictive processing and illusionist theories of consciousness. And I think what brought me here today was my my wish to start learning about the Formalisms, because I don't really see it in philosophy and I don't actually needed for my dissertation, but eventually I wanted to continue my my work on predict destruction."
"Unknown","00;04;02;10","00;04;30;05","So eventually I had to study it and I just thought, well, maybe that's a good idea to come here and minimize my uncertainties. Awesome. Through inference and or through action. And also, thanks a lot, Brock, for helping in the Dark Zero So we'll just start with the big questions. And these are the kind of questions that might motivate someone or interest them in this paper without even mentioning active inference per se."
"Unknown","00;04;30;07","00;04;59;02","But these are like some big questions that get touched upon. What is the formal basis of predictive coding? How is predictive coding used or useful what are some areas of current and future developments and what is the relationship between predictive coding and active inference? And hopefully we'll draw out more questions. Anything else to add about this no, that's really cool."
"Unknown","00;05;00;18","00;05;29;26","The paper is predictive coding a theoretical and experimental view. I think the first version was from 2021, but the second revised version was 22 by Milledge, Seth and Buckley and just to review the core claim and then some of the aims and directions they set out, they write that no comprehensive review of predictive coding theory, and especially if recent developments in the field exists, that's their claim."
"Unknown","00;05;31;06","00;06;07;15","And then they aim towards providing a comprehensive review, both of the core mathematical structure and logic of predictive coding. So there's a mathematical review and summarization aspect to the paper. And then they also review a wide range of classic and recent work within the framework ranging from biologically realistic micro circuits that could implement predictive coding to the close relationship between predictive coding and the widely used back propagation of error algorithm, as well as surveying the close relationship between predictive coding and modern machine learning techniques."
"Unknown","00;06;10;00","00;06;47;13","That's how they set out where they want to go. Would you like to read the first slide of the abstract OK, so predictive coding offers a potentially unifying account of cortical function postulating that's the core function of the brain is to minimize prediction errors with respect to a generic model of the world. The theory is closely related to the Bayesian brain framework and over the last two decades has gained substantial influence in the fields of theoretical and cognitive neuroscience."
"Unknown","00;06;48;13","00;07;35;21","Such a large body of research has arisen based on both empirical testing, improved and extended theoretical and mathematical models. Of predictive coding, as well as in evaluating the potential biological possibility for implementation in the brain and co-create neurophysiological and psychological predictions made by this theory. Despite his enduring popularity, however, no comprehensive review of predictive coding theory and especially of recent development developments in this field, exists they right here we provide a comprehensive review of both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature."
"Unknown","00;07;36;18","00;08;05;07","And we also review a wide range of classic and recent work within the framework, which is what we had read previously in the AIMS. So that's how the authors describe their own work. And now we're going to see their roadmap, see how they went about getting there, and then we're going to jump right into the introduction. And then the Formalisms so here we see the roadmap, and the rough outline of the paper is as follows."
"Unknown","00;08;05;25","00;08;38;07","Section one provides an introduction that contextualizes predictive coding, and we'll also be unpacking it in a philosophical and historical view. Section two provides an initial pass on the one level predictive coding model, sort of the kernel or the archetype or the motif of predictive coding and connects it importantly to variational inference. Then predictive coding is generalized or extended or elaborated in a few important directions."
"Unknown","00;08;38;18","00;09;07;09","So like in section 2.2, it's generalized into a multi level case. In Section 2.3, the concept of generalized coordinates are introduced in section four, precision is introduced as a concept and in section 2.5, it's connected back to the brain. Section three discusses paradigms of predictive coding both unsupervised and supervised, and also can exit through those sections and others to some research in machine learning."
"Unknown","00;09;07;29","00;09;41;17","And then in Section four, there's connections between predictive coding and some other algorithms, both those algorithms that are common in machine learning and computational statistics, like the error back propagation, the near predictive coding and common filtering and normalizing flows and biased competition. And then in section 4.5, explicit connection is drawn between predictive coding and active inference, which brings in the course of action as well as the idea of PID control."
"Unknown","00;09;41;29","00;10;08;06","And the discussion section closes with just a few other thoughts and summaries and directions and challenges. And there are several appendices. So it's a long ish paper or monograph, but it's an excellent paper and we're just going to cover more of an emphasis on the context and the beginning and then sort of skip over or trail off over a few things but there's a lot to unpack."
"Unknown","00;10;08;06","00;10;37;00","So I hope that we can like learn more in the coming weeks if the authors want to join or if anybody else wants to join anything to add on roadmap or ready for introduction, let's go. All right. Awesome. So I'll just add some notes from this section of the introduction and then Maria, feel free to add anything. So how do they introduce predictive coding?"
"Unknown","00;10;37;15","00;11;11;01","They introduce predictive coding theory as an influential theory in computational and cognitive neurosciences, which proposes a potential unifying theory of cortical function that's like related to the cortex of the brain, namely that the core function of the brain is to minimize prediction error where the prediction error signals mismatches between predictive input and the input actually received, minimizing the divergence between expectations and observations is going to be a big theme and approaching that minimization."
"Unknown","00;11;11;20","00;11;30;14","It can be achieved in multiple ways, and they're colored differently here through immediate inference about the hidden states of the world. Like I thought, the ball is over here, I'm getting evidence it's over here. So I should just update how I think about the world, which can explain perception, but the perception of a ball moving across the visual field."
"Unknown","00;11;31;07","00;11;53;18","They're updating a global world model to make better predictions which could explain learning, like learning a little bit higher level than just where the ball is in the visual field, but like where balls tend to be in the visual field or how fast they tend to move around. And finally, through action to sample sensory data from the world that conforms to the predictions."
"Unknown","00;11;54;01","00;12;26;17","And that's when we'll introduce action into predictive coding in the later sections of the paper. And then just last point, they say that predictive coding both in extremely developed and principled mathematical framework in terms of variational inference, as well as many empirically tested computational models that have close links to machine learning. So that's how they introduce this is as a unifying theory that integrates inference as well as action inference of different kinds, more like perceptual and more like learning."
"Unknown","00;12;27;01","00;13;04;28","And they're saying that it has a very rich mathematical framework and it's been connected to insights in biology and in machine learning. Anything to add nope. So they write predictive coding as a theory also offers a single mechanism that accounts for diverse perceptual and neuro biological phenomena such as and stopping by stable perception like illusions, repeat repetition, suppression illusory emotions and attentional modulation of neural activity."
"Unknown","00;13;05;07","00;13;32;02","So they're pointing to the integrative nature of this theory or framework to model all of these diverse phenomena as being an advantage. Like we could have a special theory for just by stable perception, a special theory for just repetition, suppression and so on. But this is a framework by which those outcomes are occurring as kind of outcomes of an underlying framework, predictive coding, rather than needing special theories."
"Unknown","00;13;32;02","00;14;03;19","So that kind of like parsimony or concience is pursued in the areas like cognitive neuroscience and they write as such. And perhaps uniquely among neuroscientific theories, predictive coding encompasses all three layers of Mars hierarchy by providing well characterized and importantly empirically supported view of what the brain is doing at all of the computational algorithmic and implementation levels. So Mars very influential hierarchy."
"Unknown","00;14;04;02","00;14;34;09","It's kind of like Tin Bergen's questions or a little bit like Maslow's hierarchy you can think of it as like an organizing or a sense making framework. Again, to the question What are our brains doing? And so we're going to have like three different kinds of answers to what brains are doing. There's going to be an answer at the level of computation algorithm and implementation and so here with the example of a bird flying, the computational level is like flight."
"Unknown","00;14;34;17","00;14;57;15","It's like the why, what is the algorithm doing? It's a sorting algorithm. What is the brain doing in this situation? It's tracking a moving object. Then there's the flapping level, which is the algorithm which is kind of like pseudo code, and that's suggesting the what of how that Y is realized, like what is happening in this sorting algorithm?"
"Unknown","00;14;57;21","00;15;26;14","Well, it's doing this, this, this, this, this what is the flight function being replaced by? It's being realized by flapping of the wings. And then there's the implementation so it's not just the flapping of anything, it's the flapping of something specific. And so that's the implementation details. And those levels of analysis have been helpful and provocative in connecting computational cognitive theories to neuro biological theories."
"Unknown","00;15;26;19","00;15;41;16","So it's been a very influential and discussion provoking framework that has helped people connect as well as challenge some of the similarities and differences between like computers and brains."
"Unknown","00;15;45;09","00;16;12;29","Great this is still in the introduction, and they're going to kind of give us a qualitative perspective on predictive coding. And then we're going to be diving into a lot more historical and technical detail. But how does predictive coding work? The core intuition behind predictive coding is that the brain is composed of a hierarchy of layers, which are each making predictions about the activity of the layer immediately below them in the hierarchy."
"Unknown","00;16;13;10","00;16;39;25","So there's two directions in this predictive coding, multi-layer scheme, which we're just introducing here as kind of a heretic. And then we're going to zoom in to just what's happening at one level, and then we're going to pull back out to kind of recovering this multi-level formalism, the downward descending predictions at each level, that's the blue arrow are compared with the activity and inputs of each left level to form prediction errors."
"Unknown","00;16;40;06","00;17;09;15","This is the information in each layer which could not be successfully predicted. These prediction errors are then fed upwards. That's the red arrow to serve as inputs to higher levels, which can then be utilized to reduce their own prediction error. So that's kind of the bigger picture. Like the idea at the top level is like I'm tracking a soccer ball that's moving from the left to the right, and then that gets transducer in certain ways."
"Unknown","00;17;09;21","00;17;45;23","And that is in this case, resulting in kind of a visual perceptive level prediction about what one should expect to see. And this is again, just the multilevel predictive coding scheme that is often used. But we'll going to zoom back to that, just what's happening at a single level and then what is happening at multiple levels awesome. Here's a fun 2020 paper by your sorry 2018 paper by Karl Kristen does predictive coding have a future."
"Unknown","00;17;46;15","00;18;13;15","And this is going to provide just a little bit of context and then we're going to jump more into though Karl wrote in the 20th century we thought the brain extracted from sensation that's like the recognition model and that's sort of the incoming sensory processing model the 21st century witnessed a strange inversion in which the brain became an organ of inference, actively constructing explanations for what's going on out there beyond its sensory epithelium."
"Unknown","00;18;14;03","00;19;01;19","One paper played a key role in this paradigm shift and Carl goes on to write that a 1999 paper by Rao and Ballard which we're going to talk more about for him was one of those papers that's a once in a decade find so we're going to talk more about it. But this is just kind of connecting it early to call Kristen's line of research the connection between active inference and predictive coding and the idea that sometimes it's these early transdisciplinary papers that can stitch two different fields together and that that can have long term consequences for essentially perennial questions like how do perception, learning attention and sensation all work together and under what imperative"
"Unknown","00;19;01;19","00;19;02;29","might they work together?"
"Unknown","00;19;05;23","00;19;44;16","And then the authors provide very preliminary discussion that they're going to go into more later about how predictive coding matters for machine learning. And so they write, predictive coding proposes that using a simple, unsupervised, lost function, such as simply attempting to predict incoming sensory data is sufficient to develop a complex general and hierarchically rich representations of the world they're suggesting that this has found support in the successes of modern machine learning models that are trained on unsupervised, predictive or auto regressive objectives."
"Unknown","00;19;45;00","00;20;07;09","So here are some of the papers that they cite. This is the Brown at all and the Kaplan at all. And these both have to do with the training of what are increasingly becoming important modern machine learning methods, which are the language models in contrast to modern machine learning algorithms. Which are trained to end with a global loss at the output."
"Unknown","00;20;08;05","00;20;34;10","So given the total big data set, give me the lowest total error of the whole dataset in predictive coding, prediction errors are computed at every layer, which means that each layer only has to focus on minimizing local errors then global loss. This property potentially enables predictive coding to learn in a biologically plausible way, using only local and heben learning rules."
"Unknown","00;20;34;22","00;21;09;10","And this is going to connect to a common filtering and PID control. So they're kind of laying out the land and showing where they're going to connect in the introduction very, very qualitatively. And then again, we're going to zoom back down to find out what's happening at one level of predictive coding. And then try to rebuild some of the formalisms related to common filtering and PID control and the bigger questions that people might be curious about asking would be like, what does it mean for computation to be biologically inspired?"
"Unknown","00;21;10;02","00;21;22;02","What is possible when we're thinking about some of the similarities and differences and complementarities between biological and different kinds of conventional and unconventional computation?"
"Unknown","00;21;25;18","00;22;13;22","The next part, Maria, please take it away. I'll be changing slides so thanks a lot for adding a lot of the historical context and looking forward to learning about what you have to share here. OK, so this is actually the paragraph that is in the paper, OK, that they start talking about the history of predictive coding. So why predictive coding as a neuroscientific theory originated in the eighties and nineties, this multilayered, raw and Ballard's short lives are something laughing and depth and was first developed into it's more of a mathematical form and a comprehensive theory of cortical responses in the middle."
"Unknown","00;22;13;23","00;23;00;25","Thousands with crystal it has deep intellectual antecedents this precursors include Helmholtz notion of perception as an unconscious inference and council notion that a primary structure is needed to make sense of sensory dictum and as well as early ideas of compression and feedback control and cybernetics and information to and on the next slide, I we'll be talking a little bit more about history and philosophy of predicted scouting and then some core formalisms and various generalizations and elaborations of pretty good study if there's anything you want to say, then you just feel free to jump in."
"Unknown","00;23;01;11","00;23;42;24","OK, so the historical and philosophical background of perception is huge. OK, however, in the context of predictive coding framework, authors usually come up with a similar reasoning about perception, and then we can start thinking with the Greek laws. So Plato on his allegory of the cave from 514 to 520 before Christ, which he said basically that some people would be trapped in a cave forever, watching shadows cast by objects moving near fire behind them."
"Unknown","00;23;43;09","00;24;10;28","And then they would like be there forever, have children that would grow like that inside the cave. And you know, eventually these shadows from the objects near fire behind them would be the real thing. And the only thing for these people, they wouldn't believe that there is anything outside the cave. So this this would be it. The shadows would be the only thing."
"Unknown","00;24;11;06","00;24;59;21","OK, so what Plato would claim here is that our own conscious perceptions are just like these shadows, OK? Meaning that they are indirect reflections of hidden causes that we can never directly encounter. And but a few centuries later, the Arab medieval scholar al all Hasan wrote lots of interesting notes on visual perception. Actually, he wrote six volumes, if I'm not mistaken, in of his book of optics, where he explores the view that human perception often depends on mechanisms of judgment and inference instead of, you know, providing access as true to the word."
"Unknown","00;24;59;21","00;25;20;04","So you can see that very early. And in our Western thinking, we have this notion that we do not have direct access to the world, that there is some influence going on OK?"
"Unknown","00;25;23;24","00;26;09;14","Great. All right. So jumping to the 18th century, we can find two important philosophers the English David Hume and the German Emmanuel Haithem them in 1713 until 7040 explores the inductive influences and causation. The problem of induction that he brought was an analysis of cause and effect and perception and the conclusion that he gets is that all our mental life can be traced back to the effects of this experience because since we do not have direct contact with the world, we could never have once one relationship with the objects there."
"Unknown","00;26;09;23","00;26;50;03","And therefore it is possible to have one effect with many possible causes and one causal with no effect. The solution here would be then extracting statistical regularities and imagining what happens when the world is intervened upon a controlled manner. As this last logic of how we develop developed in his book The Predictive Minds in 2014, equally important in the critique of theories in 1781, Conte will add an important element to this story among countless other elements to this story."
"Unknown","00;26;50;17","00;27;23;06","You will add that the brain uses existing information regarding space and time to make sense of the chaotic sensory data. It constantly recedes to provide an organism the perception as they know. So all of these scholars would tell us right in the early days that we do not have direct access to the things in the world and we need pre existing formation to make sense of all the data."
"Unknown","00;27;23;06","00;28;09;22","We constantly receive lots of predictive coding stuff hmm. It really makes me wonder why it's sometimes still so ineffable or poorly understood or even seen as controversial. Some of these perspectives which have been, as you are describing, like thought of and convergent, we come to for many thousands of years in different cultures. So really interesting yes. And but in the late 19th century, the germ term of them, Helmholtz in 1860 depicted the perception as involving probabilistic inference."
"Unknown","00;28;10;00","00;28;45;00","Yeah he he's one of our gods and he has notes and he was really inspired by Conte. OK and with that inspiration he developed the idea of the brain as a hypothesis tester. And that perception is a process of unconscious inference. And more specifically, this idea that he developed in experiments was that perception has to be served by combining sensory signals with the brain expectations and beliefs about other causes."
"Unknown","00;28;45;20","00;29;20;12","And such inferences happen without the subject's awareness. I mean, it has to happen without the subject's lens. Because, you know, imagine all the chaotic sensory data coming and you open your eyes and you have to try to organize everything. I mean, it doesn't happen that way. You just open your eyes and everything is is still simple. And so it's unconscious and you need to keep track of the causes in the world by updating perceptual best guesses."
"Unknown","00;29;20;12","00;30;19;17","Or you can see hypotheses as new sensory signals. All right. So here he built the idea that we use commodity. We use sensory signals with the brain expectations still, you know, update the hypothesis we make upon the make or the causes of the effect effects immersive work. And this means that he somehow turned Hume's and concepts inside Scientifical claiming that we can only infer things out there in the world behind the sensory veil, which, if I'm not mistaken, will be eventually called mark of collected directs that are not sure a maybe a result how thoughts were crucial to many different words in machine learning, one that was even nicer they named a Helmholtz machine."
"Unknown","00;30;20;02","00;31;16;11","He was crucial in psychology, in your sense, given to science and so on particular these psychology Helmholtz insights inspired Uris nicer in 1967 later Richard Gregory in 1980 and if Arvin Ruck in 1983 to develop the analysis basins this is approach which is a process that analyzed a signal or image by reproducing it using a model up there that is a picture of NASA's analysis based on this approach let some schema available in this link and in fact we're going to build some kind of neural hypothesis testing model based on his hopes for an understanding of the brain is continually formulated perceptual hypotheses about the word and testing them by acquiring data from sensory organs"
"Unknown","00;31;21;03","00;32;19;21","OK, so many things happened after the analysis based on this stuff, but I would just jump to what we have in philosophy today because we will touch the formalisms and the computation stuff after words. So I would just make the Jonathan though to what we have in philosophy today. So reason to argue philosophy and cognitive neuroscience have contributed to the of the study framework and did Bayesian paradigm in the brain and many of the seminal works we have reviewed addresses predict coding as predictive fibrosis without, you know, any relevance in the first and well for instance, this logic of how that is based on this universe to Australia aims to the end on the improvement"
"Unknown","00;32;19;21","00;32;54;07","of prediction error minimization mechanism and the concept of winning hypothesis. Precocious perception also he is interesting to watch. Deeply predictive studies can bring to people with mental disorders such as autism and schizophrenia. Actually the three of them of the philosophers who are interested in what's depicted and read to the improvement in psychiatry and medicine. Yeah, so in 2014 he published an important textbook on the matter that is the Mind The Red Book."
"Unknown","00;32;54;17","00;33;24;11","You can see where he tries to solve the problem of perception. Inspired on human induction problem with Bayesian useless and predictive processing later in 2016, the philosopher Andy Clark, based on university physics, England carefully developed what he called action oriented predictive prophecy in his book Supposed Certainty, where, he adds in advance, is the crucial role of action in our perception."
"Unknown","00;33;25;06","00;34;15;04","And he makes lots of interesting connections between risen to work and competition. And your insight is not to be sure intelligence and he is eclectic philosophy of mind and thus here in 2021 do neuroscientists and yourself one of the authors of this paper we were talking here and also raised University of physics made an amazing contribution to the field with his book Being You where he claims that we are machines whose perception is our controlled hallucination and he proves consciousness descriptions using predictive brass and framework and establishing the real problem of consciousness that in his own words requires explaining why a particular part of brain activity or other physical process maps to a particular kind"
"Unknown","00;34;15;04","00;34;58;11","of conscious experience, not merely establishing that it's this to, you know, philosophers and scientists work and they here they are key all right. So I was talking about predictive studying and then I went to predictive processing. Is there a difference? Well, many alters would use the terms freely. OK, but then I found the clerk saying in his book the difference that he understands from predictions according to one expression."
"Unknown","00;34;58;25","00;35;35;24","So he puts as the fact that predictive processing is not simply the use of the data compression strategy known as predictive coding. Rather, it is the use of that strategy in the very spatial context of scope, multilevel systems, deploying probabilistic generative models. Such systems exhibits powerful forms of learning and deliver rich forms of context, sensitive processing and are able flexibly to combine top down and bottom up flows of information within multilayer cascade."
"Unknown","00;35;37;01","00;36;17;24","Predictive processing does combine the use within a multilevel by bidirectional cascade of top down probabilistic generic battles with the core predictive strategy of the patient and encoding and transmission ok, from this, I'm not sure if you agree with me, Daniel, but reading this this paragraph makes me wonder. I mean, ok, predictive coding is a local bottom up, top down the scale stuff, but it also is multi-layer."
"Unknown","00;36;18;06","00;36;50;02","Yes, it also has some hierarchy, so I'm not sure what what he I mean, I'm not sure if I understand the difference between predictive coding and predictive processing as he puts yeah. Very insightful. And it made me think about the difference between like coding, like programing and like data coding like Python coding for data and data processing, which is a little bit like it could be multiple computer languages."
"Unknown","00;36;50;13","00;37;19;08","And so you're really highlighting something important, which is that when the data are compressed according to a strategy where the errors are being transmitted rather than the actual sort of estimate itself, that's like that information channel is in a predictive coding way. And then this helps us use predictive processing for like the bigger picture for systems that implements predictive coding modules."
"Unknown","00;37;19;08","00;38;02;06","But that's not like the only module they're deploying. So that that'll be something cool to like hear people's perspective on. Really nice distinction here though, yet. And OK, so I wrote something here that just like you said that maybe coding would be better to use formalisms and limitations of processing for this philosophical understanding. That prediction is the basis of a interpretation as opposed to descriptive code key."
"Unknown","00;38;03;09","00;38;47;07","So jumping back to formalism, predictive coding and information theory and signal processing things, so another deep intellect actual influence and predictive sculpting comes from information theory from channel in 1948 and especially the minimal redundancy principle of Barlow in 1960 189 and 61 to information theory. It tell us that information is inseparable from a lack of predictability. If something is predictable before observing it, it cannot give us much information."
"Unknown","00;38;47;07","00;39;18;08","I love this sense. It's a conversely, to maximize the rate of information transfer the message must be minimal to predict the well and hence minimally redundant predictive coding as a means to remove redundancy in a signal was first applied in signal processing where it was used to reduce transmission bandwidth for video transmission and don't you want to see something?"
"Unknown","00;39;18;23","00;39;53;05","I'll just describe this without reading it. It was a common and effective, although it can be improved on by other methods. It's a method for video encoding known as frame different thing where basically one frame gets described and then subsequent frames only the pixels that change have to be described. So that is a lot like an analogy to predictive coding in the sense that it's able to connect the differences between subsequent time steps and use that to give really only the informative pieces."
"Unknown","00;39;53;05","00;40;39;04","Like I have a 100 digit number and then I'm just going to tell you the third digit changed. Now it's an eight instead of having to repeat the same number and then 99 of those digits there inform itive like in the sense that they're good info. It's true, but they're not informative in the information theory context because they were quite predictable before observing it OK, so initial schemes used a simple approach of subtracting the new to be transmitted frame from the old frame, in effect using a trivial prediction that the new frame is always the same as the old frame, which works well in reducing bandwidth in many settings where there are only a few"
"Unknown","00;40;39;04","00;41;43;04","objects moving in the video against the stack, static background and more advanced methods often predict each new frame using a number of best frames weighted by some coefficient, an approach known as linear predictive coding ok, so now predictive coding in the eye and embrace the first concrete discussion of predictive coding in the nearest system arose as a model of neural properties of the retina with Sherine the vessel 1982 specifically as a model of sense center surrounded cells which fire when presented with either a light spot against the dark background on center of surround or alternate tively the dark spot against a light background of sensor on surround cells."
"Unknown","00;41;43;27","00;42;21;07","It was argued that this coding scheme helps to minimize redundancy individuals teams specifically by removing spatial redundancy in natural visual scenes, that the intensity of one pixel helps predict why welding to key of neighboring vehicles and then the first picture related to it you are talking about it and we're just going to look at two different tissues of the nervous system that have some predictive elements."
"Unknown","00;42;21;22","00;42;59;29","And this is referring to some of the photos sensitive cells that are on the retina. And that's work that has been done qualitatively and empirically for hundreds of years, but then was connected to predictive perspectives in the eighties. And then we're also going to look as you'll now unpack what's happening in the cortex of the brain among different layers with munchausen's in 1982 was perhaps the first to extend this theory of the retina and the algae and to a fully fledged general theory of cortical function."
"Unknown","00;43;00;16","00;43;29;29","This theory was motivated by simple observations about the neurophysiology of cortical cortical connections, specifically the existence of separate sit forwards and feedback. That's where the feedforward first is originated. In the superficial layers of the cortex, and the feedback pathways originated from primarily in the declared gray. So we kind of had this philosophy development happening over thousands of years."
"Unknown","00;43;30;14","00;44;02;02","And then in the 1900s people were starting to connect the neuroanatomy and the histology to some of these predictive ideas. And then now let's kind of close the loop and bring it back to the critical work the first and highlighted in his 2018 paper yep. And our predictive coding in the brain yet computational mathematical. Well most of our theory contained most aspects of classical predictive coding theory in the cortex."
"Unknown","00;44;02;20","00;44;40;15","It was not a concern confined by any simulations or empirical work and so its potential as a framework for understanding the cortex was not fully appreciated. And so again similar work of Robin Ballard in 1999 had it in fact precisely by doing this. And so they created a small predictive coding network of according to the principles outside they created a small predictive coding network according to the principles identified by Mozart and Berkeley investigators."
"Unknown","00;44;40;15","00;45;15;05","Its behavior demonstrating that the complex and dynamic interplay of predictions and prediction errors could explain several otherwise perplexing neurophysiological phenomena specifically extra for receptive field effects such as and stopping years yeah. This is pretty interesting because even the idea that the neurons that are active while a certain stimuli is being presented, that those are neurons that are receptive to that stimuli that's within the paradigm of signal reception."
"Unknown","00;45;15;22","00;45;42;25","And then it's like there's classical signal reception you know, the pitcher throws the ball, the catcher catches the ball. It's classic. But unfortunately or not, there's all these so-called extra classical effects. And so this paper proposed a simple architecture that was able to encompass the so-called classical as well as some of the important extra classical effects under this predictive processing framework."
"Unknown","00;45;43;03","00;46;13;26","And so that really led to a lot of developments in neuroscience in the last 20 plus years, and that's exactly what we're going to go into now. So thanks for providing that awesome context. Like it really helps, I think, situate some of the details that we're about to look into. OK, so let's get a little technical what happened after Robin Fowler's 1999 synthesis."
"Unknown","00;46;14;02","00;46;42;09","So remember that Carl Feresten said that that was like one of those once in a decade papers for him. So how did he change his action selection publication policies after reading that paper? Well, anyone could guess, but what Feresten did was that he cast the predictive coding algorithm as approximate Bayesian inference upon Gaussian generative models. And this is going to be connected."
"Unknown","00;46;42;09","00;47;29;09","This is all in the paper for people to read, but this connects basically all of the previous themes that we had been talking about, like the information theory, minimum redundancy and the Helmholtz in idea of perception as inference come together in the Bayesian perspective on the predictive coding architecture by route and bollard so the authors write versions approach re formulates the mostly Harris stick round Ballard model in the language of variational Bayesian inference which we're going to look more at and first and show that the energy function in Rao and Ballard can be understood as a variation on free energy of the kind that is minimized through variational inference."
"Unknown","00;47;29;24","00;48;00;15","So that really connected the dots between predictive coding architectures and the empirical biological findings to all of this work on Variational Bayesian inference. And those are the 2003 five and eight single authored papers by Carl. So what is Variational and Bayesian and Variational Bayesian approaches and we're just going to look at the author's words and there's many, many awesome places to look for educational materials."
"Unknown","00;48;00;15","00;48;29;22","And also check out some of the live streams like 2630 to 3437 and 39. So to be brief about it, the authors write Variational inference approximates an intractable inference problem with a tractable optimization problem. So like a super hard problem to solve just by thinking about it and then guessing at the right answer with a tractable optimization problem."
"Unknown","00;48;30;07","00;48;51;16","So it's like 20 questions. It's going to be hard to guess. On your first question, but if you take this iterative optimization approach, maybe there's a way to actually resolve it. So it's not exactly like that, but it's like kind of again, flipping out something that's hard to solve in one shot with an approach, and it's iterative gradient descent type improvement that actually gets you to a good solution."
"Unknown","00;48;52;25","00;49;15;14","And so this is where we're going to introduce some of the variables and the letters that we're going to use following a lot of perceptual work in Bayesian statistics, we're going to use O for observations which are like data, either the actual data that the sensor provides us or generated data of the kind that we would expect the sensor to give us."
"Unknown","00;49;16;02","00;49;51;27","And then X are the latents or the unobserved states of the system. So like X would be the temperature in the room and O is what the thermometer reading is. And that's sort of a Bayesian approach that helps us have a generative model of the data generating process, which is also known as the joint distribution, because it is jointly over both the observations and the latent states where variational inference comes into play is that it's hinted at in this paragraph on base."
"Unknown","00;49;52;01","00;50;23;06","So in order to do exact bars, one would have to find this normalizing factor. However, that can be intractable because it requires basically integrating or summing over all latent variable states, and that's not always easy or no. So the approach of the variational method is aiming to approximate the posterior, using an auxiliary posterior, using a different set of parameters PHI."
"Unknown","00;50;24;04","00;50;54;13","So the Q is going to be reflecting like the distribution, the variational distribution that we control, that is of a family of functions that is more amenable to optimization. So P could be like a super messy function, but Q is going to be constructed by the modeler to be a lot simpler. And so it is going to have its own parameters and those parameters Phi are Mu and Sigma."
"Unknown","00;50;55;01","00;51;18;17","And so it's kind of like we would be interested in the mean and the variance of the X, the actual temperature in the room given the thermometer. But what if it were too hard to even get that? Well, what we might want to do would be imagine that the temperature normally distributed like a Gaussian distribution with this mean view in the variance sigma."
"Unknown","00;51;18;24","00;51;55;00","So even if the temperature in the room weren't actually Gaussian distributed, maybe it's approximated well enough. And so the variational approach to the Bayesian method is to introduce this variational distribution in Q that's going to have like a lot of good features moving forward here's how that cue comes into play. This equation one is expressing using the variables that we've seen it's saying and also introducing this divergence dy so first, just the definition of D and then what this formalism says."
"Unknown","00;51;55;26","00;52;20;19","So D of P and double line Q So any means between this and that D is a function that measures the divergence between two distributions. So for example P and Q, and here the divergence is going to be calculated as the K L divergence, although other divergences are possible so in the divergence between the one that we simplified and controlled."
"Unknown","00;52;20;19","00;53;11;00","Q in that intractable true posterior, if that divergence were minimized to zero, we would be like fitting P as well as we could with Q and this is saying in one is so in Q star the best Q of X, the Clayton States given the data and the variational parameters. So the best possible room temperature prediction given thermometer readings and variational parameters are a minimization over all the variational parameters of the divergence between the Q distribution that we control of exactly what we want the best answer for and the P generative model of exactly what would be the best to compute like the temperature given the observations on the thermometer."
"Unknown","00;53;12;04","00;53;35;03","And so that goes a long way towards rewriting an inference problem as a divergence problem. But they write however, merely writing the problem this way does not solve it because the divergence that we need to optimize still contains the intractable true posterior. So it's rewritten as a divergence between like something we control and something that we kind of set out to do this because we couldn't calculate it."
"Unknown","00;53;35;03","00;53;58;23","So we have prepared it. But still, this is intractable, so this is not usable. Although they write, the beauty of variational inference is that instead optimizes a tractable upper bound on this divergence called the Variational Free Energy v f e. To generate the bound, we apply Bayes rule to the true posterior to rewrite it in the form of the generative model and the evidence."
"Unknown","00;53;59;11","00;54;27;14","So here's that P of x given. Oh, that's rewritten as now, not of X given. Oh, but p of oh come x. That's the joint distribution divided by the observations. So this by the Bayes rule p of x given. Oh is equivalent to this. That's why this is the first line here. They provide some writings and then they write in the third line the expectation around P of O."
"Unknown","00;54;27;15","00;54;57;08","So the actual likelihood of the observations themselves, like how likely is the thermometer saying 21 it vanishes since the expectation is over the variable X, which is not in P of oh. So it's like what is the expectation of this coin flip over the temperature tomorrow? It's like because they're different variables, it makes them very easy to separate out if we're only interested in one of them."
"Unknown","00;54;58;11","00;55;27;21","And then they write that this f the free energy is a very is a tractable quantity since it's a divergence between two quantities, we assume that we as the modeler know the variational approximate posterior Q of X given oh that's the distribution that we control and the generative joint distribution p o comma x. So we traded out this intractable true posterior where it's like if we knew it, then we would just stop there, but we don't know it."
"Unknown","00;55;28;06","00;55;50;29","So we've traded it out for a divergence between something that we totally control. Q Of X conditioned on these other parameters and a joint distribution, which we might have uncertainty over, but at least can be models. So that's one of the key pieces of Variational Bayes, and that's not specific to predictive coding. It's just an important way that the authors are introducing it here."
"Unknown","00;55;51;28","00;56;26;20","Anything to add all right. So let's kind of continue on this theme. Since F is an upper bound by minimizing F, we drive the variational distribution that we control closer to the true posterior as an additional bonus under certain conditions F can be used for model selection. So that means it can be used not just to kind of fine tune a model that we've already chosen, but it can actually do choosing from parametric or structurally different models."
"Unknown","00;56;28;07","00;57;03;19","They right. We can gain an important intuition about F by showing it can be decomposed into a likelihood maximization term, the tail divergence term, which penalizes deviation from the Bayesian prior. These terms are often called accuracy and complexity, and this decomposition is often used in different machine learning algorithms. So that's one rewriting of free energy as this divergence between the Q and A P and then rewriting that as in accuracy and a complexity, which we'll go into maybe more another time."
"Unknown","00;57;05;08","00;57;46;16","And they write in many practical cases, we must relax the assumption that we know the generative model P of O and X, the joint distribution. Luckily, this is not fatal. Instead, it is possible to learn the generative model alongside the variational posterior on the fly and in parallel using expectation maximization. So this is basically the alternation shown by equations three of how we can be setting one side of this tail divergence fixed and optimizing the other side and then going back and doing it the other way."
"Unknown","00;57;46;17","00;58;05;29","And so that kind of back and forth expectation maximization, we're going to be like reducing this divergence from both sides. And so formalism three are saying the variational parameters by at the next time step two point T plus one are an optimization argument of those parameters."
"Unknown","00;58;08;06","00;58;41;00","Holding the theta constant. And then the second part of formalism three is the exact opposite. Now the generative model parameters theta at T plus one are minimization over literally the same thing, but holding the variational parameters constant so there's a lot more to say about expectation maximization, but this is like converging to a small divergence by whittling away at both sides and kind of alternating there."
"Unknown","00;58;41;14","00;59;14;17","So that's how expectation maximization can be used as a horrific algorithm. For Variational Bayesian optimization how do we go from Variational Inference, which all of those previous slides are things that we've basically talked about before and that are not applying to predictive coding specifically. So how are we going to get to predictive coding? They right. Having reviewed the general principles of variational inference, we can see how they relate to predictive coding."
"Unknown","00;59;15;06","00;59;44;00","First, to make any variational inference algorithm a concrete, we must specify the forms of the variational posterior and the generative model. So if you want to do formalism three, those are the two pieces you need. You need like the five stuff and the theta stuff. And so they specify it here and it means a normal distribution with like a mean comma variance format and so they're going to define a Gaussian form for the generative model."
"Unknown","00;59;45;22","01;00;24;01","The mean of this likelihood, Gaussian is assumed to be some function of hidden states which can be parameter ized with theta, while the mean of the prior Gaussian distribution is going to be a g of U. So we're going to have like F of theta and g of MU. The variances of these two Gaussian distributions of the generative model are sigma one and sigma two this is a slight technical detail, but they're going to assume that the variational posterior is a Dirac delta, which is like a spiking function, distribute ation that's centered at the mean."
"Unknown","01;00;24;25","01;01;02;01","However, they explore that differently with Laplace assumption. So this is basically taking what we discussed about variational inference and preparing it to be entered into a predictive coding way. So they're setting up the problem with which parameters you're going to want to do variation inference on in order to implement predictive coding. And there's of course more to say, but we're just giving a first pass in in Appendix A is where they describe the difference between using the Laplacian approximation and the direct delta."
"Unknown","01;01;02;01","01;01;27;13","So it's footnote eight, which suggests moving to appendix A and looking at the Buckley 2017 for a walkthrough but this on the bottom right is kind of like a summary of the difference between the Dirac Delta approach and the Laplace method approach. So the Dirac Delta is like we're trying to find the spike that's at the mean or the median."
"Unknown","01;01;27;14","01;02;09;20","There's some other details in play that is where the bulk of the probability distribution mass is, whereas the Laplace method tries to fit a polynomial like just a second degree polynomial like a quadratic over the probability distribution also in the way that is best fitting. So there are some similarities and some differences and it's explored more in the Berkeley 2017 paper on the AP for Action, Perception and Mathematical Review let's continue with our exploration of how we do Variational Bayes and make it a predictive coding model at the top."
"Unknown","01;02;09;20","01;02;37;18","They right we defined the prediction errors epsilon. Oh that's like the error on observation and Epsilon X that's the error on the mean states so the epsilon O is like O minus F of some function, F of some parameters. And then there's also the epsilon X. So how much prediction error do you have about the observation? How much prediction error do you have about the hidden state?"
"Unknown","01;02;39;13","01;03;07;11","Epsilons are the prediction errors and their parameter i's by these theta one and two variances? Given all of this, we can derive dynamic ice for all the variables of interest. So that's the actual underlying hidden state. The temperature in the room that we want to be actually predicting mu and then will the variational parameter of it, and then theta one and theta two, which are the variances."
"Unknown","01;03;07;11","01;03;28;26","You can think of it as like the variance of the room temperature and the variance of the terms of the thermometer. I hope that's not wrong or an oversimplification, but those are like the two variances and we want the real temperature of the room they right. We can derive dynamics for all these variables of interest by taking the derivatives of the variational free energy."
"Unknown","01;03;28;26","01;03;52;20","F The update rules are as follows and so this is change in mu theta one and theta two over time d meu d t d theta one over T and theta two over DC and that has some equivalencies with some F's. So it's going to be going from just the change in the mue estimate through time to something involving free energy."
"Unknown","01;03;53;03","01;04;23;04","And then that's defined more formally. Importantly, these update rules are very similar to the ones derived in rule in Ballard 1999 and therefore can be interpreted as recapitulating core predictive coding update rules. For instance, the views are typically interpreted as rapidly changing neural firing rates, while thetas are slowly changing synaptic weight values. So they're weaving together connecting the biology with the formalisms here."
"Unknown","01;04;23;25","01;05;04;16","And they write also that that view can be understood as the process of perception. That's like how hot is it in the room? Since Muir's meant to correspond to the estimate of the latent state of the environment generating the observations on the thermometer and by contrast, the dynamics of status can be thought of as corresponding to learning. Since Theta effectively defined the mapping between the latent state and the observations so it's like you think you know how the thermometers related to the temperature in the room and then you see your thermometer changing, you're changing how hot you think the room is that happens over a shorter timescale and then over a longer timescale you might"
"Unknown","01;05;04;16","01;05;36;12","come to learn the variability of the temperature or how noisy the thermometer is. But that's more like learning than perception. However, they're on a continuum using this model. So actually this up to formalism nine completes the core formalism of predictive coding, which is it's a variational Bayesian approach to having this ongoing prediction error minimizing approach to latent state estimation that is the heart of predictive coding."
"Unknown","01;05;37;01","01;05;44;23","And now we're going to jump into like a few different elaborations that we're going to be moving through a little bit more quickly. Anything to add, Maria?"
"Unknown","01;05;47;21","01;06;16;27","Awesome. The previous examples focused on predictive coding with a single level of latent variables. You want however, the expressiveness of such a scheme is limited. Deep neural networks in machine learning have demonstrated that having hierarchical sets of latent variables is a key to enabling methods that learn powerful abstractions and handle intrinsically hierarchical dynamics of the sort that humans intuitively perceive."
"Unknown","01;06;17;27","01;06;43;18","Predictive coding schemes introduced can be straightforwardly extended to handle hierarchical dynamics of arbitrary depth equivalent to deep neural networks in machine learning. This is done through postulating multiple layers of latent variables x sub one through x sub l, and then defining the generative model as follows. So like p, the generative model is going to be basically over all the layers."
"Unknown","01;06;45;07","01;07;13;14","So just as the generative model and the variational distribution had to be defined for the single layer model here, they're going to define the P distribution, and now they need the Q, we define a separate variational posterior for each layer. So they define the PS in the Qs, just like they did in the single layer. And then that allows them to calculate the variation of free energy, which is a some of the prediction errors of each level."
"Unknown","01;07;14;17","01;08;03;08","And so this is F now, not just over one layer but multiple levels. So the variational posteriors that need to be calculated are partitioned across the layers, which allows them to be summed in a very straightforward way, given that free energy divides nicely into the sum of Layer Y's prediction errors, it comes as no surprise the dynamics of the MU and the theta are similarly separable across layers that allows different layers of this prediction hierarchy to be like precise or imprecise, and allow those movements to happen in a way that's uncorrelated, which is not just because the real world presents itself with settings where there's like confidence that lower and higher and vice versa, but"
"Unknown","01;08;03;14","01;08;28;09","it makes this calculation of the free energy of the whole system more like a simple sum. Whereas if there was these like really complex interactions with a first layer by the third layer, if the fifth layers this way, then doing the statistics would be a lot more challenging. So visually, here's what that looks like. We have the view, which is the mean estimate of what's happening at that level."
"Unknown","01;08;28;28","01;09;06;12","And then there's passing of these epsilon error terms. So they write, this is the architecture of a in FIG. one multilayer predictive coding network shown here with two value and error neurons in each layer, the value neurons project to the error neurons of the layer below and the error neurons represent the current activity. So this is like starting to walk us back towards that cortical layout where there's a cortical column where there's some so-called upwards and downwards signaling, but also there's lateral signaling."
"Unknown","01;09;07;19","01;09;32;24","So this is a graphical model that's reflecting the way that predictive coding can be arranged almost like in series and in parallel to have like wide models and deep models, just like you could have a neural network where it was for neurons, for neurons, for neurons, for neurons, or you could have 64 of them, 64. That would be like a shallower but wider model."
"Unknown","01;09;32;27","01;10;06;26","And so that notion of shallowness and depth is also going to come into play with predictive coding here we're in formalism, 12 and 13, and so now we're looking at the rate of change, the derivative of view sub l, so the estimate of the mean at that layer and theta l the variance at that layer. So these are the update rules, the gradients for the mean and the variance of different levels."
"Unknown","01;10;06;26","01;10;31;13","And they're also written as free energy, functional of those layers. And the dynamics of the variational means depends only on the prediction errors at their layer and the prediction errors on the layer below. So again, we don't have this epsilon at L -1 connecting up all the way to MU. It's only through these local connections within and between layers by which we're needing to calculate anything."
"Unknown","01;10;33;05","01;10;54;13","We can think of the muse as trying to compromise between causing error by deviating from the prediction from the layer above and adjusting their own prediction to resolve error at the layer below. So it's kind of like a hierarchy of of bosses or tasks and there's like top down expectations and there's the bottom up reality, which might be like ahead or behind schedule."
"Unknown","01;10;54;21","01;11;31;02","And then it puts each person like in this compromise situation crucially for conceptual readings of predictive coding. And this is where there's like a doorway to the philosophy and some of the broader discussions so this means that sensory data is not directly transmitted up through the hierarchy as is assumed in much of perceptual neuroscience. And so it totally returns us to these questions like what is perception, what is cognition, what is action, what what is coming from the eye to the brain or what goes from the brain to the eye?"
"Unknown","01;11;31;03","01;11;59;23","What is the eye doing? What is the brain doing with respect to the eye? And how does it relate to predictive coding any quick answers on those, but not really cool. So after introducing that kernel of the predictive coding formalism and taking it into the multi-level context, now we can look at another elaboration or generalization so they write in 2.3."
"Unknown","01;12;00;14","01;12;32;13","We have considered the modeling of just a single static stimulus. However, most interesting data that the brain receives comes in temporal sequences oh bar which is oh through time to model such temporal sequences, it's often useful to split the latent variables into states which can vary with time and parameters, which cannot. In the case of sequences, instead of minimizing the variation of free energy, we must instead minimize the free action, which is the path integral of the variation of free energy."
"Unknown","01;12;32;13","01;13;01;20","Through time. We're not going to go into the formalisms related to the generalized coordinates 15, 14, 15, 16 and 17. But it's something that we can explore later. And just as a reminder just to kind of excite somebody who might want to explore it, we explored the idea of the generalized coordinates of motion a lot in active live stream number 26 with De Costa at all."
"Unknown","01;13;02;11","01;13;31;28","And so here was a model where we have from left to right time at different time steps. And then there's the observables of something and then it is part of this like column of higher derivatives of its motion. So it's like position its velocity, its acceleration and so on. And so each time step moving forward, you're computing the generalized coordinates of motion for like the movement of a baseball."
"Unknown","01;13;32;12","01;14;13;04","And that is what ties, generalized coordinates of Motion to Control, which will also come back at the end also known as integrator chains. And this is all happening within this discussion of Bayesian mechanics. Just like if we had been talking about the baseball, we would be talking about physical mechanics or statistical mechanics. Well, it's Bayesian, so it's Bayesian mechanics in the physics of action and control that's what they explore in section 23, Section 2.4 introduces and goes into a little bit more detail on precision one core aspect of predictive coding absent in the original role in Ballard."
"Unknown","01;14;13;07","01;14;49;06","1999 formulation is the notion of precision or inverse variance. So precision and variance are like one over each other. So if someone said it's very high precision that's a very narrow, very sharp distribution and one over the variance is another way to write that. So sometimes it's used with like a beta and a gamma in other models, precision serves to multiplicative, modulate the importance of the prediction errors and thus possess a significant influence in the overall dynamics of the model."
"Unknown","01;14;50;03","01;15;19;07","And so we won't go into every detail with how the precisions are themselves fit, but just wanted to note that this big sigma is the free energy being summed over the layers. So Big Sigma is a is a multiplicative, some others are is just a sum. But then this smaller sigma inside is sigma as the precision matrix at a given level."
"Unknown","01;15;20;05","01;15;34;26","So it's kind of like different uses of sigma, but this is a way where the variances and the prediction errors at a given level are being summed across levels, and that's helping fit these precision terms."
"Unknown","01;15;37;03","01;16;15;03","Statistical detail, but it's important for making it work in real inference. OK, in section 2.5, they pull back from elaborating on predictive coding and they talk more about how it has some biologically plausible basis so this was quite interesting. Well, technically predictive coding is simply variational inference and filtering algorithm under Gaussian assumptions. From the beginning, it has been claimed to be biologically plausible theory of cortical computation and also for other brain regions like the AI and for other cognitive systems."
"Unknown","01;16;16;04","01;16;42;11","The literature has consistently drawn close connections between the theory and potential computations that may be performed in brains. For example, round ballot explicitly claimed to model the early visual cortex and freeston explicitly proposed predictive coding as a general theory of cortical computation. This is like the realism and instrumentalist discussion. Are we just modeling these systems in the kind of data that they provide us?"
"Unknown","01;16;42;24","01;17;01;08","And it just fits really well as a model. But we're not saying that the architecture of the statistical model has anything to do with the architecture of the physical pieces of the brain. Or is it a little bit in the gray zone where it's kind of like, Wow, that model fits so well and the anatomy looks so good."
"Unknown","01;17;01;17","01;17;32;28","Maybe there's something there to it. And so in this section, they review empirical work that has attempted to validate or falsify key tenets of predictive coding in the cortex, as well as some issues with the approach. So it's mainly focused on the mammalian brain and the neural cell type, but as always, we can sort of think about what is the adjacent here like, what are the roles of other cell types in predictive coding in the mammalian brain?"
"Unknown","01;17;33;13","01;18;03;18","And then how might like the insect nervous system or how other nervous systems or non nervous systems perform predictive coding and then they just provide one example where they take what they had shown in FIG. one with this multilevel predictive coding scheme that was sort of predictively programing, as they say, one to think about cortical cognition. And now they're going to make that connection clear in figure two."
"Unknown","01;18;03;28","01;18;32;27","So on the right side is their figure two, and that's the canonical micro circuit model of us at all. Mapped on to the connectivity of a cortical region. And so then in red are those parameters that we've been discussing, the muse and the Epsilons the means and then the prediction errors. And then here's just a few other representations of what others have written about like that cortical column."
"Unknown","01;18;33;18","01;18;59;28","So here's like an evolutionary perspective that connects some of the mammalian cortex with the avian pallium brain region. And so just the way that people work out these kinds of circuit diagrams, like by looking at the actual tissue and then here's the statistics and so how does the tissue relate to the statistical model? But that's what they explore in Fig.."
"Unknown","01;19;00;20","01;19;31;26","OK, and he does OK in section three. They continue to review empirical work, but with more of a focus on machine learning of the unsupervised and supervised setting. And they're going to also explore how the predictive coding architecture can be made more biologic with possible by relaxing certain assumptions implicit in this canonical model and introduce action into the picture."
"Unknown","01;19;33;14","01;20;01;00","Fig. three They summarize a few different what they call paradigms of predictive coding, a summary of the input output relationships for each paradigm of predictive coding. So it's the input and the prediction, the output of different paradigms. So here's the classical predictive coding observation data are coming in. Predictions about those observations are coming out. It's an instantaneous, real time snapshot."
"Unknown","01;20;02;18","01;20;37;20","The real time anticipatory unfolding is still getting data as input, but it's predicting the observation at the next time step, which is a similar problem to predicting at this time step. But it's like a little bit different. That can happen not just through time, but also in the context of space with a spatial predictive coding and then these two on the right supervised predictive coding in the generative and in the discriminative direction have to do with the relationship with reading in labels and giving out observations or vice versa."
"Unknown","01;20;38;04","01;21;09;21","Make me a cat. Here's a picture of a cat, here's a picture that's a cat. So these are some of the ways that predictive coding models have been applied 3.1 they explore the way that unsupervised training relates to predictive coding. So that is machine learning. On unlabeled data, like here's 100000 hours of human speaking, learn how to speak. That's like an unsupervised approach."
"Unknown","01;21;10;00","01;21;56;02","Whereas the supervised approach would be, Here's 1000 hours of human speaking, here's 1000 hours of a river, here's 1000 hours of, you know, of some other noise. And so by the label, then the algorithm would learn what is speech versus non speech versus the unsupervised. But there's also a lot of like continuum and complexity and it's a whole topic in some of the subsections of 3.1, they go a little bit deeper into the temporal predictive and the audio and coding aspects, but we're not going to go into those right now in figure four in Section 32, they look at supervised predictive coding and look at it in that forwards and backwards direction."
"Unknown","01;21;56;16","01;22;23;21","And so here they're looking at the classic M, this data set for evaluation where there are some pictures of handwritten digits and then it's predicting. And so here's the generative predictive coding and the discriminative direction where here the data are coming in like the picture of the five with the pixel intensities and that is relating to the more high level estimate of it being a five."
"Unknown","01;22;24;04","01;23;14;11","So this is observation in label out that's a five, here's a label in pixels reflecting it out so that's connecting machine learning to predictive coding in section 33. They're also exploring some relaxed predictive coding and it has to do with relaxing certain mathematical assumptions that they had introduced earlier for simplicity and for clarity. We're not going to go into it, but it has to do with making it more useful in the real world by relaxing certain assumptions and then in 34, they explore deep predictive coding."
"Unknown","01;23;15;02","01;23;57;23","So they write. So far in this review, we've only considered direct variations on the first in rude, blurred models of predictive coding, which are relatively pure and use only local, biologically plausible learning rules there also exists a small literature. Experimenting with these models often achieve better performance on more challenging tasks than the pure models can achieve. So pure again, here, meaning that it uses only local biologically plausible learning rules, so only connected entities locally and so these models, they suggest, provide a vital thread of evidence about scaling properties and performance of deep and complex predictive coding networks and they write."
"Unknown","01;23;57;23","01;24;27;21","The first major work in this area is Spread Net, which uses multiple layers of recurrent convolutional lstm neural networks to implement a deep predictive coding network. So here is what it looked like this is the original pregnant architecture. And so they have convolutional teams that are passing information in a top down way and in a bottom up way."
"Unknown","01;24;28;20","01;24;58;24","So maybe we can look at it more later. And then I also just found this 2019 2020 paper where they also explored like whether pregnant actually is doing predictive coding. But again, a whole nother rabbit hole so section three reviewed through FIG. three and some discussions on the subsections, a few of the different paradigms and the ways that this predictive coding architecture has been applied in section four."
"Unknown","01;24;59;03","01;25;31;05","They're going to connect it to other algorithms. So we're going to basically speed through all of the connections except for active inference first they connect predictive coding in section 41 to the back propagation of error. This is really important like for neural network training but we're not going to talk about it here, but it sounds like really interesting to discuss one that we'll introduce but then skip over most of the details is the relationship between linear predictive coding and common filtering."
"Unknown","01;25;31;20","01;25;56;06","So we can remember that the linear predictive coding is not just the frame different thing. It's where there's like that coefficient of how much you should forget each of the previous frames and this is known in signal processing as the Kalman filter. The common filter, as this slide shows, is an iterative mathematical process to quickly estimate true values, position and velocity."
"Unknown","01;25;56;16","01;26;27;03","So that's also like getting towards the generalized coordinates of motion. And so the X's are like our observations so that's the thermometer. And then here's the red is our Kalman filter temperature unfolding through time. So our initial estimate gets quickly corrected. And then even though there's a lot of noise in the observations, the common filter is like giving a good prediction on the actual temperature and this has been implemented in a ton of Bayesian approaches."
"Unknown","01;26;27;03","01;26;57;05","It's a totally standard signal processing Bayesian approach and in the following sections in 4.2, they have a lot of formalisms about the Kalman filter. And in Appendix D they provide even more formalisms on the common filter. So we're not going to really go into either of those in section 4.3, they introduce this idea of normalization and normalizing flows."
"Unknown","01;26;57;27","01;27;26;21","And so they wrote the deep link between predictive coding and normalization and has been extended by Moreno. 20, 20 by situating predictive coding with a general recipe for building or representing a complex distribution from a simple and tractable one. So we encourage people to look at this Marino 20, 20 paper if they want to learn more about predictive coding variational auto coders and biological connections but not going to discuss today."
"Unknown","01;27;28;04","01;27;59;14","One other model that they connect predictive coding to is known as the biased competition model and that is unpacked in 4.4. But I just wanted to pull up one example of this biased competition model. And so this is from the sprawling paper of 28 within each processing stage nodes compete to be active in response to the current pattern of feed forward activity received from the sensory input or previous processing stage."
"Unknown","01;28;00;03","01;28;40;14","So it's like there's competitions among the sub units to be activated by certain kinds of inputs. And then there's some technical details OK. Now finally to active inference as we're near the end of the discussion this will be a little fun to explore also in the coming weeks, of course. So they write in 4.5. Predictive coding can also be extended to include action allowing for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms."
"Unknown","01;28;41;09","01;29;05;14","The key insight is to note there are two ways of minimizing prediction errors. The first is to update predictions to match sensory data, which corresponds to classical perception I thought that the ball was going to be here, but I guess it's over here perception. The second is to take actions in the world to force the incoming sensory data to match the prediction."
"Unknown","01;29;06;03","01;29;26;24","I'm going to move the ball to where I expect it to be, or I expected the ball to be in the left side of my visual field. So I'm going to move my eyes to the right so that it's in the left side of my visual field. So its action is not always like reaching out and moving the ball it can also be the action of choosing where to look, for example."
"Unknown","01;29;28;06","01;29;54;14","And this is what is really fascinating because we spent so long in the earlier sections in the first 50 equations, you know, without action. So it's like, oh no, are there going to be like another 250 equations with action coming into the picture? But actually it's quite a simple introduction. The basic approach to including action in predictive coding is to minimize the variation of free energy with respect to action."
"Unknown","01;29;55;10","01;30;36;00","Although free energy is not explicitly a function of action, it can be made so implicitly by noticing the dependance of sensory observations on actions. So where what you see is going to depend on where you look, we can make this implicit dependent dependance explicit using the chain rule of calculus, act of inference. It's inference about action so here's the rate of change of action through time is the rate of change of a free energy functional of previously it was of observations and of states through time."
"Unknown","01;30;36;24","01;31;14;02","And now O is o of a observations are a function of action. They're partially or completely dependent on action. And that is being understood as a derivative of change. In action. Oh of a over a partial of a over a is a forward model which makes explicit the dependance of the observation upon action and must be provided or learned by the action by the algorithm, because that is how do observations which are a function of action change as actions change."
"Unknown","01;31;14;22","01;31;42;04","So that is like a layer that hadn't been brought up. So they're saying because you do need to solve that ratio or that derivative, you do need to like learn that or be provided that. And it's a separate thing to learn. In addition to this standard generative model for perception. So we took action as being like a dependency of observations, which is also what connects active inference to perceptual control theory."
"Unknown","01;31;42;17","01;32;11;24","So that's how we introduce action into the picture just from a first pass let's go one more level in and then any thoughts you have would be great. So if we were talking about just the inference case prediction, air minimization would be like the picture that we're seeing is exactly the one that we're predicting or vice versa. So what does that look like for action?"
"Unknown","01;32;13;09","01;32;44;18","The prediction error simply becomes the difference between the current observation and the target or set point already bringing in this kind of homeostasis. ALO status perspective. However, this raises the question of where the set points comes from where do the targets come from and how are they computed? Whence priors? A generic answer to the question is that set points can be inherited from evolutionary or auto genetic, which are developmental imperatives or supplied by other neural circuits involved in goal directed behavior and planning."
"Unknown","01;32;45;05","01;33;10;19","For present purposes, we can simply take them as exogenous given variables. So what isn't addressed in this review is this big question of priors and learning on preferences like how do you know what to prefer? How do you know how much to change, how you prefer, how tight should your preferences be? Those are like really, really important areas but they're just like opening the door but not going to go into it."
"Unknown","01;33;12;24","01;33;25;15","In 4.5.1. It's also straightforward to model the potential costs of action in biological organisms. There's different costs of action, and so how can that be modeled mathematically?"
"Unknown","01;33;28;05","01;33;52;16","Well, they say by explicitly including action within the generative model as follows. And so now they've added in this extra term with a cost of action. So it just shows how like once action has been introduced as a dependency of the observation function, then there can be like other things that can be calculated as related to action."
"Unknown","01;33;54;27","01;34;52;13","Any thoughts on action or like in your readings, where has action been integrated well or not with predictive coding well, I think that from the from my perspective, predictive coding wasn't really about action before this text. So it was new to me because I always relate action to predictive thrusting and active inference, not really predictive coding. So it was new and from all of these that we've been through and found fascinating how the complexity grows and how many different applications for just coding can be found on, especially in machine learning."
"Unknown","01;34;52;13","01;35;19;04","And, you know, all of this mathematical happiness brings many, many different solutions to problems that we face for a long time. So that's nice. Yeah, great. It's really interesting. So just one more take on active inference, which we explored in 26 as well."
"Unknown","01;35;21;13","01;35;57;23","Like predictive coding control optimizes a system towards a set point and is ideal for simple regulatory systems like thermostats action, HFT. So we're taking that same notion of a as being like something that influencers observations. So that's how we introduce action into the model is we make observations depend on action. Action is determined by three terms, a proportional term, which minimizes the distance between the current location and the set point P an integral term which minimizes the integral of this error over time."
"Unknown","01;35;57;25","01;36;24;24","I and a derivative term which minimizes the derivative of the error d the combination of the three terms produces a robust and simple control system which can be applied with some tuning to control. Almost any simple regulatory process and higher coordinates of motion could do even better. But as we explored in 26, often the ideas are sufficient. And so that's a very common engineering method."
"Unknown","01;36;25;01","01;36;53;01","And so it connects the Bayesian mechanics of action and perception and cognition of active inference on generalized coordinates to control in the engineering setting. And then they provide a bunch of other formalisms in appendix B, they give some detail on this idea of natural gradients, which I think we could go into in the dark one and in the dark two."
"Unknown","01;36;53;17","01;37;26;03","So we won't talk about it here, but appendix B is about precision as natural gradients, and Appendix C are providing some challenges for the neural implementation of back propagation by predictive coding. So also we're not going to explore it here, but those are both really interesting, short and very topical appendices in Section five. It's of quite good length, and we're not going to cover actually any of it today because it's been a great and long enough zero."
"Unknown","01;37;26;15","01;37;47;22","But in the dark one in the dot two and beyond, we hope to unpack the discussion and the future directions. So we have a lot of room and space for questions, and I think we both or all walk out of this with more questions than answers. But what would be your closing thoughts, Maria?"
"Unknown","01;37;51;26","01;38;34;14","Um, I'm not sure what to think. Now I have to digest all the information we have today. But I have this the wish to continue with the understanding of formalism that wants to you know, understand better what you said here. And I'm really excited about the biological part of predictive coding implementation and predictive processing. So as it is not really developed in this paper, I think that the author said that it's not really explored."
"Unknown","01;38;34;14","01;39;07;21","Actually, I, I have this this idea of, you know, looking to it to see what's, what's been, been doing and in the best awesome. Yeah. I think it'll be some fun. Upcoming discussions. Thanks so much for all the help and for this dot zero and to Brock as well. So see you around. Thanks a lot. See you. Bye."

"Speaker Name","Start Time","End Time","Transcript"
"Speaker 1","00;00;28;26","00;00;37;27","Hello, everyone. Welcome to Acton Flat Live Stream number 43.2. It is May 11th. 20, 22."
"Speaker 1","00;00;40;02","00;01;04;26","Welcome to the acting flop. We're a participatory online lab that is communicating, learning and practicing. Apply active inference. You can find us at the links on this slide. This is a recorded and archived live stream, so please provide us with feedback so we can improve our work All backgrounds and perspectives are welcome and we'll follow good video etiquette for live streams."
"Speaker 1","00;01;06;01","00;01;44;18","If you want to learn more about what's happening, active love greetings head over to Active Inference dot org and you can find out more about participating on live streams or other modes of participation. And today we're in 43.2, where as always, it's up to the last minute with what we actually do. And welcome also to Steven. We're going to continue this discussion in our third part of the conversations we've been having on this paper, predictive coding, a theoretical and experimental review."
"Speaker 1","00;01;45;13","00;02;11;25","And we'll begin with the introductions and then we can move around the paper. We brought up some things that we'd like to cover last week for today, but also we can take any questions or areas or sections that any of them want to talk more about. We can absolutely go there. So we'll just start with introductions and say hi and feel free to add anything you'd like."
"Speaker 1","00;02;12;24","00;02;56;10","What brought you to the paper or what's just one thing that you took away from it? And then I'm sure we'll have some fun today. So I'm Daniel and I'm a researcher in California, and I'll pass it to Dean Hugo's Dean. Oh, sorry. I was muted in Gypsy, so it was on the livestream, but welcome to Dean and Stephen, and I'll also share the slides with you here in the chat but we're live and in the game, so feel free to say hello and anything just to get us going with this discussion."
"Speaker 2","00;02;56;10","00;03;02;05","Getting a bunch of feedback on from somewhere. Daniel, let's go. It's going good."
"Speaker 1","00;03;08;03","00;03;25;27","I'll open up another window so that you can see what slides I'm working on. But yeah, how does it or what what was interesting to you about this paper do you hear me, Dean?"
"Speaker 1","00;03;34;24","00;03;44;18","That's, that's what OK. All right, all right. OK."
"Speaker 3","00;03;45;05","00;03;50;05","OK. And you can hear I can hear you now. I can hear you. I couldn't any before. And it was with."
"Speaker 1","00;03;50;13","00;03;51;12","Some fine dean."
"Speaker 2","00;03;54;14","00;04;03;08","I can hear you, Daniel, but there's a long delay, and there was I can hear I could hear feedback, like a second audio loop going off while you were talking."
"Speaker 1","00;04;03;28","00;04;05;29","OK? Did you have the stream open or anything?"
"Speaker 2","00;04;07;14","00;04;12;01","I just know. I just have the picture of the of the three of us."
"Speaker 1","00;04;12;01","00;04;13;22","OK, I'll just reload just to be sure."
"Speaker 1","00;04;15;22","00;04;40;02","So it goes with live streams. All right. OK, so is that better well, roll with it then. So OK. Or we'll see what happens. So, yeah. Stephen, how goes it and what did you like about the paper as we start."
"Speaker 3","00;04;42;17","00;04;58;21","Well, I'm just interested in the general scope as per normal Sam. I'm so we're going to go with the go with the the overall vibe of pushing the boundaries of active impact. So I'm fairly I'm fairly relaxed with this one."
"Speaker 1","00;05;00;09","00;05;02;07","OK, 18 work better."
"Speaker 2","00;05;02;25","00;05;04;06","Yeah. My, my bad."
"Speaker 1","00;05;05;01","00;05;06;01","All good. All good."
"Speaker 2","00;05;06;01","00;05;18;13","Thank you for telling me not to have both GSC and the YouTube open at the same time. So yes, that these little technical things that I should have been more on topic."
"Speaker 1","00;05;19;09","00;05;28;19","There's like some reflex where if your own audio is played back to you at a certain delay, it makes it like basically implausible to speak. It's pretty interesting."
"Speaker 2","00;05;29;07","00;05;34;22","Now. Well, apologies guys. That's not the start. I wanted to start all good."
"Speaker 1","00;05;34;22","00;05;40;00","So how goes it or what like brought you to this paper or what did you find interesting in what they wrote?"
"Speaker 2","00;05;41;27","00;06;12;13","Well, I'm not really totally up on on what predictive coding is. I think I know what predictive processing is. And when when you and Maria were talking in the dot zero, you were blue. We're talking in the dark one just the sort of the translation of that into operations that you can you can identify with a certain amount of confidence is the part that I'm really interested in."
"Speaker 2","00;06;12;13","00;06;33;22","I'm not really sure what the applications are beyond the psychology realm. I'm sure there are quite a few. And that's kind of why I showed up today was to sort of piggyback on what what people who obviously know a lot more about this than I do or are saying about it and where it's going and what the potentials of it are all right."
"Speaker 1","00;06;34;02","00;06;50;26","Well, let's start with a few reminders and recalls. Is there any area or figure or formalism that either of you would like to suggest that we jump to, to remind ourselves of anything critical about the paper? Otherwise, I can bring much to some."
"Speaker 2","00;06;51;23","00;06;54;12","Is it OK to start with the back propagation of error?"
"Speaker 1","00;06;55;00","00;06;55;11","Yeah."
"Speaker 1","00;07;01;01","00;07;25;16","All right. So I'll bring in this was in Section four, so I'll be bringing in some. But let's some we're going to ramp into it just as a reminder about the roadmap. So the first section of the paper is an introduction the second section of the paper describes with quite a lot of detail the core kernel of the predictive coding architecture."
"Speaker 1","00;07;25;27","00;07;53;19","This is a computational architecture and it involves information transmission that is in this predictive coding way. So check out the paper the dot zero, the dot one, that's where we're going into a lot of detail on predictive coding as an information architecture. And then as Maria was bringing out for us, predictive processing might be used to refer perhaps more holistically to systems engaging in predictive coding."
"Speaker 1","00;07;54;03","00;08;32;27","And there's a bi directionality there to predictive processing, whereas predictive coding might be something that could be done in a bit more of a uni directional way. That was section two on predictive coding, introducing the kernel and then going into a few specific generalizations of that kernel and modifications that help make it more computationally powerful, as well as linking it closer to certain biological architectures of tissues like in the retina and in the cortex of the mammal brain in predictive coding in the brain, there's some exploration on different paradigms of predictive coding."
"Speaker 1","00;08;33;11","00;09;12;05","And these are referring to kind of like settings where the core and elaborations can be applied, and that's in the supervised and unsupervised cases as well as thinking about spatial and temporal predictive coding algorithms and connections with modern machine learning, like with deep predictive coding then after that sort of kernel and generalization of the predictive coding kernel and exploring some of the paradigms for where that kernel and its elaborations have been applied in section four, there's the relationships between and among algorithms."
"Speaker 1","00;09;12;20","00;09;37;28","And so that's where we get into this section on predictive coding and the back propagation of error. So that's where we'll go and I'll copy in some formalisms or find out which ones would be the best to bring in. But here we are on slide 56, which I'll share in the chat here what is interesting to you about back propagation of error, then we'll start there."
"Speaker 2","00;09;39;25","00;10;08;12","Well what I'm curious about is that bottom line of that slide, this recursion is identical to that of the propagate to the propagation of error algorithm which are not, which I never actually encountered before this. So I was wondering, I know a bit about recursion. I think we all have a sort of a basic understanding of what's involved in recursive loops."
"Speaker 2","00;10;08;12","00;10;34;24","But I was wondering how sort of the predictive where's that where is this relationship between predictive coding and this other algorithm, which I've heard that I've heard that that I don't know if it's an expression or label back propagation of error, but I've never really looked at it from a from operational standpoint or, you know, how it's how it's laid out mathematically."
"Speaker 2","00;10;34;24","00;10;42;12","And I'm not sure if either of you have that familiarity, but I was hoping maybe you did OK."
"Speaker 1","00;10;44;06","00;11;17;12","Great. So how does predictive coding relate to the back propagation of error? Let's look at these sections, which we didn't annotate too heavily leading up to, because, you know, there's like many equations in the paper, many areas even before we introduce the formalism. And to give me 15 seconds to read it qualitatively, what would be propagation of error mean to you or whereas in applied setting where you think this could matter."
"Speaker 2","00;11;19;11","00;11;49;07","Well, and again, this is this is me talking out loud hanging my butt out there because I don't really know what I'm talking about. But am I am I correct in assuming that it's any time you find yourself in feedback and feedforward loops? I know that they're a cyclical and I know that usually implies a directional, but isn't any time you're getting sort of a looping condition where you get an opportunity to sort of reset?"
"Speaker 2","00;11;49;29","00;11;59;18","Isn't that what we're isn't that the situation that we're examining OK."
"Speaker 1","00;12;00;04","00;12;26;27","We're in the arena of statistical Bayesian graphs, so we're talking about models here. We're not necessarily talking about what an actual system is doing, a computer system, nor some sort of biological system. We're talking about a modeling statistical architecture, and it would be a secondary conversation about what systems are actually doing with back propagation. So this isn't just like saltwater or flowing from the ocean into the river."
"Speaker 1","00;12;27;05","00;12;56;29","OK, well, just to sort of forestall any preliminary connections you might want to draw, but we can see when we have a bit more sense of what back propagation of errors on this graphical setting, how it could apply. So they're introducing this concept of a computational graph, fancy gee, not free energy related, just G for graph. And it consists of two kinds of elements, just like all graphs or networks do, which is vertices and edges."
"Speaker 1","00;12;57;22","00;13;34;21","The vertices are representing intermediate computation products. For example, the activations at each level, this multilevel predictive processing architecture. So we have like nodes within the level and then there's edges that represent influence here as differentiable functions. So this is related to some of the graphical Bayesian approaches we've looked at previously, but also it shares a lot with some neural network representations where the edges are not just like a correlation or mutual information, but the edges are more like a differentiable function."
"Speaker 1","00;13;35;10","00;14;11;21","OK, and we're going to restrict to looking at specific kind of graphs here, which are computation graphs they're saying are directed acyclic graphs. So that means that it's directed. There's an arrow such that the differential ability of the function is going from one way to the other. Like we could think of that as the forward direction. And then the basic likeness is just ensuring that there's like sort of one place where we could start and we could just forward propagate everything out to the edges and that wouldn't trap us into an infinite recursion by going forward."
"Speaker 1","00;14;12;09","00;14;28;04","And we've talked about some of the differences between like cyclic and acyclic graphs with Majid when we were talking about the, the DCM, the and all the ambiguities of that acronym. OK, Stephen, anything. And then we'll continue."
"Speaker 3","00;14;31;03","00;14;32;18","Just one quick question."
"Speaker 3","00;14;35;03","00;14;59;24","Ontologically, how useful or problematic is the word code in now? We've moved a bit further down with active inference and in a way back propagation of error needed to be that needs to be distinguished now even in the descriptive term, just so that the connotations to coding so confuse all the discourse."
"Speaker 1","00;15;01;00","00;15;02;10","Documentation, the coding."
"Speaker 3","00;15;03;16","00;16;05;21","Well that it's basically kind of almost like a representational kind of script almost that's been written somewhere. It's kind of how you know that the coding that you get in a computer program or something and I kind of think that that does sort of speak to the nature in a way because this is the, the nature of how for the future but back propagation of error kind of speaks in some ways the nature of how the code is processed and I may be off topic here, but I just I just kind of wonder as we're going through this and trying to link it all together, how much that ontological term is, is also possibly something"
"Speaker 3","00;16;05;21","00;16;17;08","that could be thought up in another way, make it easier to to to, to to understand what's going on OK."
"Speaker 1","00;16;19;06","00;16;46;18","We are dealing here with computational architectures. So I don't think we need to run away from these questions. That were brought up more in a philosophical non computational version into the anti computational perspective. Like we're not fighting for what is happening in the brain here. We're using predictive coding to talk about information coding and transmission approaches. So that's one thing."
"Speaker 1","00;16;46;18","00;17;11;14","And yeah, I mean, we don't we can't just go around flipping different words. Predictive coding is what for decades has been referenced by certain architecture here. And then there's a lot of openness with how that architecture is applied. But I, I, I'm not 100% sure like this is about predictive coding architectures."
"Speaker 3","00;17;12;09","00;17;18;26","Yeah. No, that's cool. I'm, I'm just putting that out there, just seeing how that floats a little bit. That's a good, good response."
"Speaker 1","00;17;19;06","00;18;01;12","Like, what else would, what else could be in this position of coding it's about how signals are encoded and what is being transferred between different nodes is what is being transferred. The raw value or in the predictive coding architecture, we have two directions that signals are being passed in this compute graph, not in the brain. And that would be coming from one direction the expectation that's the predictive part and then the error residual, that's the deviation from the expectation."
"Speaker 1","00;18;01;29","00;18;20;04","And so that is what the predictive coding architecture is. It's just about having both graphical systems that are passing expectations and differences or errors rather than passing like just the plain values themselves."
"Speaker 3","00;18;21;02","00;18;58;19","Yeah, OK. I suppose there's also that question of how much is in the dynamics of what's going on and the hyper parameters when looked at and how much difference that makes too to this story. OK, so you've got you've got the the predictions, the prediction error has been passed down. The coding tends to be like the numbers are there, but in some ways the numbers can kind of evolve once the that's the system."
"Speaker 3","00;18;58;19","00;19;24;28","The dynamics of the systems or the dynamical sort of patterns of the system start to sort of fire up, so to speak. You know, it takes a little bit of time for the active inference models start really purring along sometimes so that kind of coding is in a way sort of almost more live than traditional coding, which is kind of sort of baked in."
"Speaker 1","00;19;26;13","00;19;54;19","OK, I got you. We're not even in active inference yet, so I see some back later, but we're discussing predictive coding back propagation of error, and then action does get brought into the picture in section four or five. But up until Section four or five there's no action even in the model. So we're just looking at the architecture that does computation and then understanding how action could be brought in."
"Speaker 1","00;19;54;19","00;20;24;23","And that's kind of the fun surprise that we uncovered in the dot zero, dot one and in the layout of the paper was when the architecture is sufficiently general and powerful incorporating another variable that is about action, like policy selection becomes fluent. And that's why these architectures are composable with different sensory modalities, with different scopes of action, with different affordances, all these things because there's a level of generalizability that we can pull back to."
"Speaker 1","00;20;25;17","00;20;51;10","OK, so let's return to predictive coding and back propagation of error. We have this graph, the vertices are representing an intermediate computational project products. So like a number or like a variable because it's kind of like where you're storing the intermediate computation product and then the edge of Differentiable functions could be, you know, like Y equals to X is a differentiable function."
"Speaker 1","00;20;51;10","00;21;27;12","Anything that has smoothness in the graph is differentiable and as the orange part says, we're restricting this to directly similar graphs where we're not going to get into an infinite regress just by going forward now what happens when we're trying to tune this architecture so that it has a minimized loss function, loss function, whether we're dealing with least squares error like a regular linear regression, or whether we're dealing with some other loss function the loss function is like what is trying to be minimized."
"Speaker 1","00;21;27;28","00;21;57;20","So when the ball is rolling downhill, we're minimizing like the loss or the energy function, but the energy function approach and wording is coming more from like a physics side, whereas the loss function is just a purely statistical claim. Like what is the function that we're minimizing? OK, given the output vertex v out. So this is like the one that we actually wants to be reducing the loss around."
"Speaker 1","00;21;58;12","00;22;26;10","OK, the loss function is going to be l l is a function of V out back propagation can be performed upon a computation graph. So we're going to be minimizing loss at a certain vertex by back propagating upstream a lot of what led to it being the way it is. And so that could be done on just the last tip of the spear or it could be done throughout the whole system."
"Speaker 1","00;22;27;08","00;22;57;07","Back propagation is an extremely straightforward algorithm which simply uses the chain rule of multivariable calculus to recursively compute the derivatives of children nodes from the derivatives of their parents. OK, so formalism. 28. This fancy D is partial differential. So the change of the loss function with respect to the change in intermediate variable AI is what we're looking at."
"Speaker 1","00;22;57;09","00;23;30;14","We're looking at how we're going on that ball. Are we dropping on the lost function or is the model getting better or worse? With respect to changes of intermediate computational products, vertices and that overall loss function is the sum of the the subset of P, which is the parents. So we're looking at VI like the node that we're focus on node of interest or something."
"Speaker 1","00;23;30;29","00;24;06;24","And we're going to sum over all of its parents. And then for all of its parents, we're going to compute something that's basically like the change in loss function over the change in that parent's value the sub j and how it changes with respect to change in the child. So we're taking something that's like this graphical network with nodes and edges and differential ability and then figuring out which way is the overall downhill based upon all of the back propagation of the gradients."
"Speaker 1","00;24;07;22","00;24;40;03","OK, by starting with this output output gradients, that's V out and if all the gradients are known, so like we're dealing with a model that we specified, then the derivatives of every vertex with respect to the loss can be recursively computed. So because all the edges are differentiable, the big function is also differentiable so two x is differentiable, x squared is differentiable x cubed and x to the 11th power are all differentiable functions."
"Speaker 1","00;24;40;18","00;25;07;03","So you could also differentiate their concatenation through addition. That would be like the derivative of two X plus the derivative of this and also the chain rule in calculus allows us to like nest those. So if we can differentiate to X and if we can differentiate X squared, we could differentiate two x inside of parentheses squared. That's basically the chain rule in calculus."
"Speaker 1","00;25;08;22","00;25;38;17","And then that is what allows this sort of big downhill computation based upon the specification of the graph and then the back propagation of all the relevant nodes and finding out how those need to be tuned so that the final output node that's being optimized can be going downhill defined as going down on the loss function. Less loss is better here."
"Speaker 1","00;25;39;10","00;25;40;05","OK, yes."
"Speaker 2","00;25;40;27","00;26;29;02","Excellent. Thank you. Because this was one big opaque mark off blank to me when I read it just couldn't make any sense of it. But is the ability to differentiate are what they is what they're saying here is the ability to differentiate an ability to build relationship that parent child thing is, is it the is it the capacity to differentiate that allows for the relationship to be identified or is it the other way around because when they're talking here and saying that propagation for want of computation drop back propagation is extremely straightforward and uses the chain rule of multivariate stuff when it's talking about that, it's it's it's pointing back to something that you don't"
"Speaker 2","00;26;29;02","00;26;50;09","necessarily have to see, but you know that there is some relationship just like there is between two X and X squared. Am I getting is that is your explanation helping me move closer to a better understanding of that? Because I understand derivatives, but I'm not sure how the derivatives fit into this explanation."
"Speaker 1","00;26;51;12","00;27;29;25","Great. Yeah. I think we're, we're so solenoid exploring now we haven't even brought in predictive in these equations. Here are 428 and 29 are about back propagation on computation graphs. So Right. With these papers it's really important to know like what has not been brought on to this discussion and there isn't error. This is just about the nodes and edges like the intermediate computation values of the abstract just kind and there are differentiable relationships."
"Speaker 1","00;27;30;03","00;28;19;22","So now they're going to show how predictive coding relates to back propagation of error. So they write predictive coding can also be straightforwardly extended to arbitrary computation graphs. To do this, we simply augment the standard computation graphs with additional error units for each vertex. So let's recall what some of these graphs that we previously looked at here in FIG. one of the paper, we have a graph where the nodes are intermediate computational products represent differentiable relationships and importantly, some of these intermediate products of the semantics or the interpretation of being estimates about mean MU at that level or estimates about variance and error at that level."
"Speaker 1","00;28;19;22","00;28;48;23","Epsilon. So this is kind of like a subtype of graph, like the abstract is kind, it's just intermediate computational products. And then predictive coding are specific kinds of computational graphs where some of the nodes represent mean estimates and some of them are error estimates in contrast and hopefully not to confuse. Here's from 26.2 where we talked about like integrator chains."
"Speaker 1","00;28;49;08","00;29;16;15","So notice that all of these nodes can be interpreted as intermediate computational products, but there's no error term here. So this is an example of a computational graph that doesn't have error terms. This is not a predictive coding architecture, not that it's inconsistent with or somehow disjoint from, but this is not having epsilons, so it's not predictive coding architecture, whereas this is sort of a canonical multilevel predictive coding architecture OK, great."
"Speaker 1","00;29;17;10","00;29;56;13","Hey, Jack, it's good to see you in the chat. OK, now we're going to return back to the back problem OK, so we're adding in these additional air units, Epsilon Sub I for each level of the graph, formally the augmented graph becomes now g tilde. So previously fancy g was just edges and vertices nodes and edges and now the vertices is being unpacked so that we can remain for the mean estimates, which are ultimately the ones that we would like to have the most accuracy on true accounting also for variance estimates."
"Speaker 1","00;29;56;25","00;30;39;18","So Epsilon, this big E has been pulled out. It's the set of all error neurons. We then adapt the core predictive coding dynamics equations from a hierarchy of layers to arbitrary graphs change in V. The nodes with respect to time prediction is happening through a sequence of observations in the predictive coding architecture that's now casting Kalman filter frame different seeing a lot of these other sort of real time algorithms for mean and variance estimation that we talked about equals the change in F three energy with respect to change in the node."
"Speaker 1","00;30;40;11","00;31;14;13","So now the loss function time is going to be related to the F that had been previously previously defined in earlier equations, and then that is going to be defined as basically the error. This is in formalism 30 the epsilons of AI. So the error variance at that last level minus a sum of C, so C are the children."
"Speaker 1","00;31;15;11","00;31;56;28","And then there's just some more terms involving basically tuning the nodes accounting for the fact that we want to like tune the variance estimate hours and the compute computation nodes that represent the mean estimates. And so then the dynamics of the parameters on the vertices V and the edge functions theta. So they kind of slightly evolve the edge notation towards theta can then be derived as a gradient descent on F or after the sum of prediction errors of every node in the graph."
"Speaker 1","00;31;57;23","00;32;32;04","So previously they had specified how the, the composed ability of the multilevel predictive coding architecture is such that the levels are can be factorization from each other, which allows us to sum their free energy contributions into like a free energy total and so that is how a multi-level model can be fit because it's finding like the free energy minimum for that whole multilevel system which might entail one of the levels not being at."
"Speaker 1","00;32;32;05","00;33;27;06","It's like perfect, perfect lowest free energy state. And in this section they connected the way that back propagation in 20, 1829 without any action, without any prediction is able to make a big loss function that's composed because of the tracked ability of the compute graph. And all of these little lost functions can be summed into a big loss function that you use to like train a neural network for example and then moving that into the predictive coding and free energy, minimizing space by saying, well, this is a special kind of compute graph that has the mean estimated view and the variance estimate is the error terms and then we're going to do something similar where"
"Speaker 1","00;33;27;07","00;33;59;27","we're going to compute like what amounts to a loss function f the sum of prediction errors of every node in the graph based upon the internal prediction errors being minimized. So in the non predictive coding frame, we're minimizing a big loss function that's composed of smaller loss functions. In the predictive coding frame, we're minimizing our divergence from expectations composed of the divergences from many smaller expectations."
"Speaker 1","00;34;00;21","00;34;01;25","OK, yeah, go for it."
"Speaker 3","00;34;05;00","00;34;51;15","So extrapolating hopefully again from this the predictive coding, then like by working with the errors which are kind of almost now making a little bit of a jump here but are kind of implicit to the to whatever the system of interpretation is on the data is quite it's kind of that's why you're going into this biological world because it it's so bottom up in a way because it can start just with whatever the errors are in the nature of the system that's been set up for doing the sense in even forgetting action, just the nature of how the sensing sets up."
"Speaker 3","00;34;52;09","00;34;59;20","We would start to generate the errors at some implicit level. Would that be a fair assumption?"
"Speaker 1","00;35;02;20","00;35;30;14","So any time there's going to be a prediction of observations. So yeah, we don't need action here yet. There's two options. Either you have 100% predictive power and your error is zero, or there was some error. So you're always in one of those two cases. And which one are we usually in not having the total perfect 100% accuracy down to the hundredth decimal point if for no reason."
"Speaker 1","00;35;30;14","00;36;05;12","Other than that's beyond the point of of diminishing returns. Or we're also trying to balance accuracy with model complexity. So because we have these metrics like the AIC, the BE, I see all these metrics that find us a model that's a balance between accuracy and complexity. So it's not overfitting. The cost of not overfitting of cost of having a simpler model is literally increased error because every new parameter that you add into a model, you always explain more variance."
"Speaker 1","00;36;05;23","00;36;28;14","The first principle component takes the most variance of the data. The second principle component takes the next most variance of the data. Every principle component on a dataset is going to continue to eat more and more variance out of the data, leaving less and less in the error term. But it's a situational thing whether you need to explain 80% of the data or 90% of the data or 95% of the data."
"Speaker 1","00;36;28;23","00;36;39;05","But there's always going to be some difference between the prediction and the observation again, unless one is in this super edge case where it's perfectly known. So yeah, go for it."
"Speaker 3","00;36;41;27","00;37;14;08","Yeah, that's helpful. And it's also implicit there that if even if you if whatever you set that complexity and accuracy to at this very deflated level is, is, is really helpful. So the nature of the model will change the nature of the errors and basically the best it gets to the most it can reduce will vary a little bit as you change that model and not make it too much of an extrapolation."
"Speaker 3","00;37;14;08","00;37;31;28","But that that is a nice way to be able to unify a lot more complex and verified ideas about models. And you know what is really happening you can so drop back to that complexity, accuracy."
"Speaker 1","00;37;33;12","00;38;01;09","Principle yeah. So here's some more sections from 4.1 towards the end of 4.1 on page 30 in the paper. So yeah, hopefully we've been representing this at least mostly accurately. And then for those who want to go deeper, that's why there's a paper and that's why there's like a continued discussion and development on these ideas because they, they drop many little crumbs of what we've held this to be zero for simplicity."
"Speaker 1","00;38;01;09","00;38;28;17","And but they write, we can think of this as a predictive coding network in which all the error is initially focused at the output of the network where the loss is, and then through dynamical minimization of prediction errors at multiple levels layers, this error is slowly spread out through the network in parallel until the optimum distribution, the error at each vertex is precisely the credit that should be assigned to causing the error in the first place."
"Speaker 1","00;38;29;28","00;38;39;04","That's pretty interesting. It's kind of like let the chips fall where they may with respect to error and variance attribution."
"Speaker 2","00;38;39;25","00;39;04;23","Dean this is this is awesome. So are they also are they also is this is the way the architecture is and the way the graph the graph is set up as kind of a mirror of what the architecture is of this layering thing. Are they also there to kind of contain little fires of, of, of mistakes and errors?"
"Speaker 2","00;39;04;23","00;39;30;00","Is that why it's effective? Because it sort of contains something, you know, suddenly causing a chain reaction and having and having everything that when we read Majid's paper about, I think it was. Mm. Maybe it was axis paper about you can, you could make predictions going right off of a cliff and, and the result is you no longer exist."
"Speaker 2","00;39;30;00","00;39;41;03","Is this kind of an architecture that helps us avoid some of those really consequential prediction consequences hmm."
"Speaker 1","00;39;42;17","00;40;19;01","That's an interesting idea that I think we should engage in some slide play and try to try to get a little bit of clarity on that. And also hopefully we'll hit on a lot of the ideas that we've been bringing up in a more abstract way, because I know it is helpful for like everyone to see multiple Corsa paint on this and like it's the epitome of abstraction or that is the austerity of those formalisms about compute graphs so let's think about it."
"Speaker 1","00;40;19;21","00;40;45;09","We want to take it to the, the cafe level that was even though that was I remember that being quite fun with we're going to be we're going to be between the cafe and, you know, something else. So it will be intermediate because we're going to be talking about a specific graph, but we're not going to like necessarily for the first pass on it go all the way in."
"Speaker 1","00;40;45;14","00;41;12;04","OK, so the node that we care about being most accurate on is seven. So like this is going to be like our our vocal node. This is like the one that we want. We want to have an image classifier algorithm. This is the classifier node. We want to have prediction on what the thermometer is going to say. That's what this is."
"Speaker 1","00;41;12;16","00;41;35;07","We want to have prediction on the temperature in the room. That's what this is. We want to have inference on action and think about the upstream parents of action. Then that's what this note. So seven is going to be like the node that we're interesting interested in minimizing our loss function on."
"Speaker 2","00;41;36;06","00;41;36;13","OK."
"Speaker 1","00;41;40;07","00;42;25;29","This can be carried out on multiple variables, but we're going to just look at minimizing on this OK, what is the back propagation chain here? Well, let's think about what the forward and the reverse arrows are. So first, just to clarify like this, is a directed acyclic graph. There's no loops in the graph and the arrows are all directed they reflect differentiable functions, causal influences among these variables, these intermediate computational products so like variable one is then there's some function differentiable function that translates into 22 influences four and three, three influences five and six, six influences seven."
"Speaker 1","00;42;26;21","00;42;56;08","OK, we've seen this in the Bayesian context where the Spark City of the connections allow for the factorization of this graph, like it allows us to sort of hold one part unchanging and change other parts through factorization. That's a lot of the variational inference stuff that we've talked about. OK, so what are the parents of seven which nodes do you think that we should consider if we're going to back propagate?"
"Speaker 1","00;42;56;10","00;43;25;19","So the forward model, the actual like causal chain in the model not in the world, is like one you can imagine some perturbation happens to one. Every node is going to be influenced. That's the forward perturbation in the forward generative model. What is going to be invoked if we start at seven and do a back propagation which nodes are not important."
"Speaker 3","00;43;27;29","00;43;28;27","Five and four."
"Speaker 1","00;43;29;07","00;43;58;18","Right? Yeah. Five and four will not be invoked in this optimization scheme. So if we were so we'll make these ones gray that we're going to be back propagating on. But I also hope I'm representing this accurately but you could imagine that like changing the dials on five or the function between three and five will not reduce the loss on seven."
"Speaker 1","00;44;00;04","00;44;27;19","So it still might be an important thing in the forward model, but it's not in the back propagation from seven, it's in the forward propagation from one. OK, so then this is not a predictive coding architecture because in the predictive coding architecture, some of these that's what we saw in like for example in FIG. two is what we saw in FIG. one."
"Speaker 1","00;44;28;09","00;44;57;24","So there's the means and the variance estimates. So let's bring in figure one and try to adapt it to our graph. So our graph is sort of up to formalism. 29th, OK, now we want to think about a graph that's not just nodes and edges of the abstract type, but it's going to be nodes and edges that include these error neurons as they're calling them here."
"Speaker 1","00;44;58;22","00;45;23;25","So here's a reminder on what that architecture looks like Jaques wrote How is the back propagation of error related to Holland's John Holland Bucket Brigade? The Bucket Brigade in Signals and boundaries? It sounds explain a little bit what is the bucket brigade or will look, but sounds like a bunch of people helping and sharing buckets or something like that."
"Speaker 1","00;45;25;07","00;45;47;04","OK, so let's just say that the one that we actually want to minimize is going to be like a AMU it's going to be a mean estimated. Now let's just modify this graph so that this one is going to be error. I'll just use the bucket type the variables. So this is a mean estimate here."
"Speaker 1","00;45;51;00","00;46;48;06","And then we're going to alternate so that this one is an error and this one is a mean this one's a mean again, I check this is not the exact same architecture that they laid out and this might not be traditional or classical standards, but like this is getting us towards this idea that within a cortical column, so to speak, we have the forward propagation of alternating layers of means and errors we want this loss function to be minimized by diffusing the uncertainty across these error nodes."
"Speaker 1","00;46;51;16","00;47;22;22","That is like letting the chips lie in terms of the variance attribution and as they write precisely the credit, it should be assigned to cause the error in the first place. So if like Node two in the forward causal model was causing 90% of the variance. And so this was like, like one was, you know, is a very tight distribution and it gets blurred a ton on the way to three."
"Speaker 1","00;47;23;17","00;47;52;12","And then from three it only gets blurred a little bit on the way to seven. So like two is going to be big. This is like a big error estimate here and six is small tiny, very variance estimate here. So it'd be like if we were in classical statistics and we were going to do, you know, we were doing brain imaging on a bunch of people from two different categories."
"Speaker 1","00;47;52;17","00;48;14;16","Like with and without this diagnosis, you could imagine that in that two level model one world, there would be like massive variance among people and very little differences between those two higher groups. In another world, it could be the opposite there could be big difference between the two groups, but very small differences, but among the people within a group."
"Speaker 1","00;48;15;01","00;49;03;01","Right. And so this is the parameter zation and finding the parameters that are going to minimize the loss function means that we want the variance to be partitions appropriately across error returns, just like we would like the average the mean estimators to be accurate the equivalent in the Arab world is we want the errors to be appropriate in how they're distributed so it's like like if you're doing a mean estimate on something, you don't just want like the highest mean possible, you want like the most accurate mean possible and the most accurate error possible doesn't mean the lowest error possible."
"Speaker 1","00;49;03;02","00;49;35;12","That would be the fallacy of what we're estimating means. Then let's just make it bigger so we're estimating errors, not because we want bigger errors or want smaller errors. That's the thumb on the scale. Estimating errors accurately entails appropriately, distributing the error terms on this graphical architecture. And then they're adding in one extra point, which is basically that the dynamics of predictive coding are purely local, requiring prediction errors from the context of a current vertex."
"Speaker 1","00;49;35;12","00;50;04;26","And the child that is on the computational front, a win for tracking ability and for the actual algorithm, because you don't need to have the whole model loaded into RAM, for example, because you might only you know which neighborhood of variables you're going to engage when you're doing a certain parameter update and then on the biological side, it starts also leaning towards plausibility because like they even call it an error neuron."
"Speaker 1","00;50;05;02","00;50;39;01","Well, neurons are signaling forward. Let's just call that the direction that the action potential travels down the axon. But there's also retrograde signaling at the synaptic junction. So it's plausible at the multiple neuron architecture level or even at the two neuron biological system. Level, that there is like even if there's a direction at which signaling is assumed or perceived to be normatively happening, there's also a retrograde signaling of multiple kinds."
"Speaker 1","00;50;39;01","00;51;10;19","So we won't go into that like complexity of retrograde synaptic regulation, but that at the circuit level and at even the synapse level, the idea that signaling is happening both ways is something that is seen as a strength of the predictive coding architecture. Again, the locality allows to be more computationally implementable and this sort of directionality and signaling is more reminiscent of biological signaling processes, synapses, conversations, brain regions, so on."
"Speaker 1","00;51;18;27","00;51;21;28","OK, let's OK. Yeah. Any thoughts on this."
"Speaker 2","00;51;25;21","00;51;29;12","That I think that that's pretty good explanation."
"Speaker 1","00;51;29;26","00;51;30;15","Good. First, even."
"Speaker 3","00;51;31;15","00;51;58;12","Just just checking. So if you added all the errors are like all the errors kind of independent of each other, what would they is there like a total error across the chain so to speak of like how much error there can be or is it like kind of contained within this like is six relatively contained and two relatively contained."
"Speaker 1","00;52;06;27","00;52;07;15","OK, Dean."
"Speaker 2","00;52;08;00","00;52;08;25","Can I take a guess?"
"Speaker 1","00;52;08;25","00;52;09;27","Stephen is Fisher."
"Speaker 2","00;52;10;00","00;52;39;13","Again? I'm guessing so let's just keep the wild money money part of this front of mind. I think what the I think what the graph is telling us, or at least when we formalize it in this way, is that you could always find accumulative error. But the point of the system is to try and keep the error where it actually exists within each layer."
"Speaker 2","00;52;39;13","00;53;11;22","So sort of sort of partnering the error at each level with with whatever the prior, the hierarchical L plus one or L -1 layer is. But me again, I'm just guessing, but I'm kind of making sure that I understand completely what what Daniel laid out here. And that would be my guess. I mean, you could you can add it all up, but I think the I think the point of it is, is that before it, it runs away from you."
"Speaker 2","00;53;12;01","00;53;40;04","The architecture is set up so that there will be some cumulative effect. But it probably wouldn't be it shouldn't be there's the where to look for. It shouldn't be as abrupt as if you didn't have the layers and you didn't have the ability to differentiate and you didn't have a kind of understanding of what what is happening, i.e., back propagation only."
"Speaker 2","00;53;41;00","00;53;47;02","Now, again, I'm just throwing that out there. But because I don't have a reputation around this, I could be completely wrong."
"Speaker 1","00;53;50;29","00;54;21;01","There's a lot to say, and the authors like do provide many citations, but I believe that's right. Like just as appropriate mean determination. We're thinking in a Gaussian world, like right there's a lot to say on Gaussian processes, but a Gaussian is a mean and an uncertainty. So it's there's going to be somewhere where the hump of that distribution is, and then there's going to be some width to it."
"Speaker 1","00;54;21;24","00;54;57;12","Now, sometimes the process is actually Gaussian. Other times we can use nested Gaussian processes so that it's like, well, there's some process that's moving it and then there's wiggle around it. So we're going to see that moving as one Gaussian. And then the other one is like a ripple that's like a smaller Gaussian on that and then even in cases where there's a non Gaussian distribution, that's where the Laplace approximation comes into play, which is the second derivative."
"Speaker 1","00;54;58;06","00;55;43;08","We have the Jacobian and then the hessian matrix in 26. And then we talked about how when you take the second derivative you're basically making that bowl and then lying in it. So you're going to the bottom of the bowl that you created and you know that it's going to be the sort of hump structure that's amenable to going downhill because once you go beyond quadratic beyond the hump, anything that's like a multi hump distribution or even just x cubed, you're going to have challenges getting globally optimal from local optimization processes because you won't know if you're in a local energy well or not."
"Speaker 1","00;55;43;23","00;56;09;26","And so many of these computational tools and approaches that we're discussing, like across these live streams and papers can be understood as taking something that isn't like a bowl and making it so that it can go downhill and finding, well, what is the appropriate level of how many layers have to be added to this onion or how many oven gloves do I have to put on?"
"Speaker 1","00;56;10;12","00;56;40;16","How many approximations should we carry out in our approximation science so that there can be something like a parabola where we know that we're on parabola territory and that's what allows us to then minimize a loss function. Because if one believes that they were on a rugged landscape, then minimizing something locally would be you'd have no way to know if that was on the right track or not."
"Speaker 1","00;56;41;11","00;56;41;17","Yes."
"Speaker 3","00;56;41;17","00;57;11;18","Steven could I say then, would I be OK to say that you so much like each of these errors, say number six, number two in here and you could have others would be like little mini bowls or mini at different. And the further up the chain, the bigger the potential impact of the position and in that bowl or whatever the the landscape is out towards whatever's been, you know, the mean that's been understood."
"Speaker 1","00;57;12;25","00;57;46;05","OK, let's look at FIG. one. So this is a cordingly inspired so inspired by the cortex and blue wrote Deane, I understand the architecture the same way error correction is retained locally. So let's look at this semi critically inspired predictive coding hierarchical system just like in the example with the two clinical populations doing brain imaging there's one world where the patients within the group vary a lot and the groups don't differ."
"Speaker 1","00;57;46;18","00;58;17;02","There's another world where the groups differ a ton and then the people within each group do not have any variance among them. So depending on how it's parametrized, one could have a model where the highest level one has a major influence or not and that's the whole question. It's like in the linear model, is it more important like the max part, the slope or the B, how much it's shifted."
"Speaker 1","00;58;18;13","00;58;49;23","But if the regime of attention is on linear modeling as an architecture, it doesn't make sense because they're both just parameters in a model. Now, for any given dataset, it's an empirical question to what extent changes in the slope or the intercept are associated with changes in loss function, and that's linear regression. Here's an architecture that gives us an approach to finding loss functions."
"Speaker 1","00;58;50;24","00;59;30;16","But how do we know how many layers? How do we know how many of these side connections to make but there's a few ways to go about that. One could just a priori model the structure of the model of the graph around something else, like previous work or some inspiration from a biological system. They might also be interested in this structure being a parameter of a model like, and that's the whole structure learning and meta Bayesian approaches and all of that."
"Speaker 1","00;59;31;04","01;00;07;03","But that's pulling back another level. And so that's the challenge of structure learning because we can get that lost function within on a given static graph. But that doesn't mean that we have the best possible graph architecture it's like if you have 100 pieces of data, biometric data on people and you're making a multiple linear regression to find out their risk for some condition do you just use 100 that might be overfitting, it might be too costly computationally."
"Speaker 1","01;00;07;03","01;00;42;24","So how many of those variables do you use? There's a process for determining that's what's at the tradeoff between accuracy and complexity, where it's fitting the outcome well, the loss function on the condition that you're trying to actually predict, but it's not past the point of diminishing return or using variables that are uninformative or for example, if there's two variables that are perfectly correlated with each other, that's called multi linearity and like you wouldn't want to use those variables so that's the challenge in the linear regression world."
"Speaker 1","01;00;43;07","01;01;11;29","And this is like taking it into another space, but it has more analogies than not, which is why SPM statistical parametric mapping that textbook is for those who want to engage vital prerequisite because it addresses a lot of these questions on model fitting, including on dynamical systems and it puts it with six feet on the ground in classical parametric and nonparametric statistics."
"Speaker 2","01;01;12;08","01;01;49;11","Then this just raises the question in my mind, I haven't answered it as to whether or not a mean is constructed or if a mean is actually sculpted or some combination of the two. I mean being a mean to guy, I'm going to assume that it's probably both like you can have the figure, but it's your understanding of of what the difference is between an edge and a a and a directional link, which you have to kind of carve out from the overall image."
"Speaker 2","01;01;50;09","01;02;15;19","But that's again, that's one of these things now where if you get into this predictive coding piece, I think the product looks like a construct, but an actual but in actuality, there's there's got to be a huge element of this of being able to isolate the mistakes so that they don't overwhelm. And if you're isolating something, if you're removing some factor you're actually sculpting as well."
"Speaker 2","01;02;16;17","01;02;28;29","So maybe that's a bit of a philosophical overlay on the sort of mathematical operational side but you know me, I'm always trying to turn it into what's the what's the practical function of this."
"Speaker 1","01;02;29;28","01;02;57;01","Yeah, it's like statistics as world building because if you just want the mean, then just take the grand average. If you want the mean and the variance, there's a method do you want a variance on the variance? How confident are you that it's ten plus or -1? Is it ten plus or -1 plus or minus point five, or is it ten plus or -1 plus or minus .01?"
"Speaker 1","01;02;57;11","01;03;26;06","So a tight estimate here on the error as a parameter and so we're like playing with which parameters are seen as like the things in and of themselves, the means and which ones with respect to means have this interpretation as being error neurons or being variance descriptors. Now one properly fit model which you could take into the, you know, 50 stacked level's ten plus or -1."
"Speaker 1","01;03;27;00","01;03;50;11","And then on that one variance estimate here we have plus or minus point one. And then how sure are we about the plus or minus .1.01. OK, and how sure are we about that .01.01. That would be like a well fit model where as we kept on adding uncertainty parameters, we would be making appropriate estimates. Now what would be like a mal fit model?"
"Speaker 1","01;03;50;14","01;04;22;25","And I hope this is accurate, ten plus or -1 and you go, Well, how sure are you on that one plus or -50? It's like the variance just exploded. It's like, well, you're saying the variance could be like zero or the variance could be 50. How much confidence should we then have in the mean estimate or and so that's what this appropriate distribution of uncertainty is."
"Speaker 1","01;04;22;25","01;05;04;15","Now let's think about Node seven in uninformative nodes seven, we're predicting temperature, but it's just it's unchanging we don't have a lot of information there to fit a world model because it could just be a single level estimate. It's ten and it's not changing but as we deal with progressively more and more nuanced and informative things that we want to minimize loss functions on, like natural language images from natural scenes action, including the unknown consequences of real time unfolding action."
"Speaker 1","01;05;05;09","01;05;42;04","Those because the uncertainty becomes higher and the information the contents of what you're trying to minimize your loss function on is higher. It not authorizes or licenses, but kind of moves us into potentially having higher levels of model complexity as simple as possible and not simpler with that principle being carried through. Again, if you're just trying to predict a constant number, you don't need a six level predictive coding architecture."
"Speaker 1","01;05;43;09","01;06;08;27","Whereas if there was something that did actually have that many levels of depth, that's the kind of architecture that would do well to predict it. And we talked about this with the Pas and Posselt on the architecture errors for homeostasis and Allo stasis. And it was like, yeah, to just recognize if you're out of bounds and then come back in, that's a given model."
"Speaker 1","01;06;09;15","01;06;55;06","But then there's a different graph that's going to do something like intermodal or anticipatory or have memory. So all of those cognitive or computational features, license or in gender, more complex causal models. But these are being sculpted up in a year from nothing by the statistician and then we're in sort of brackish or gray waters when we juxtapose the architecture of the graph with the architecture of some natural system OK, yes."
"Speaker 1","01;06;55;21","01;06;56;00","Steven."
"Speaker 3","01;06;56;17","01;07;27;10","Yeah. And you've got this and you've got this excitatory and inhibitory dynamic going on and even though the arrows going from the area down to the you there is a the the U is exciting. The labor part, isn't it is exciting. There's almost like it's like one autumn up is kind of exciting higher up areas. And then things that are further up also sort of saying, hey, calm down, calm down."
"Speaker 3","01;07;27;10","01;07;33;10","I can reduce some of this. You can have a hand, you know, that only scored two goals. Let's relax. You know."
"Speaker 1","01;07;35;14","01;08;12;16","That that again, I also hope this is not inaccurate. It's almost like there's a suppression of error, like a dampening being carried forward with the ultimate dampening being. You predict it perfectly you have totally quelled all error yet in real settings because of just simply model simplification or uncertainty in the world. Error is coming in from the observables, which is often what we're trying to reduce our loss function on."
"Speaker 1","01;08;12;16","01;08;47;26","And that's this whole difference between reinforcement learning, finding oneself in reinforcing or rewarding states and active inference with reducing surprise about outcomes. We're dampening error and carrying that forward in the form of generative model and then also able to run it back in a tractable way because it's like you're, you're, you're dampening the vibration, but then the vibration is still entering in and you'd want that vibration to be appropriately distributed according to where it should be."
"Speaker 1","01;08;47;26","01;08;49;08","OK, Dina, then Stephen."
"Speaker 2","01;08;50;21","01;09;28;08","Yeah, it seems to me that the way what this is, is, what this is saying to me is that because of the way things are organized two, two things can live simultaneously at once. One is sort of that local mean correction, and the other is the more generalized global or the other direction in the by direct by directionality parallel of a mean correction one is kind of reinforced through external evidence."
"Speaker 2","01;09;28;17","01;09;55;24","Well, the other one is reinforced through localized almost. Almost. It's it is distributed, but it's almost like the ability to localize and isolate if necessary. So I know we've talked a lot about top down and bottom up, but this brings a lot more color in my mind to some of those conversations we've had before in terms of, you know, arrows going in opposite directions."
"Speaker 2","01;09;55;24","01;10;31;04","And it's not so much that they're going in opposite directions, but that you can be going two different directions, quite literally, one literally at once because of the way things are set up. So, yeah, I think this this has been really really helpful for me in terms of not only understanding what back propagate propagation is, but actually in sort of giving me a little bit more foundational ideas around, again, how do we turn this to this sort of statistical ways of representing it into so I'm confused."
"Speaker 2","01;10;31;04","01;11;03;17","Should I should I go into the cafe or not? I mean, we know not all of us have to go to that level of detail to really understand what the final thing was that pushed us over the door threshold or not. But it appears that it's this has been around for a while and being able to take that if you really are kind of stuck because you really don't know what the the parents are of your current action because we haven't got as you say, we haven't got to action yet."
"Speaker 2","01;11;04;02","01;11;14;21","But that's the whole point. There is an architecture there that's going to allow us to have choice thanks."
"Speaker 1","01;11;15;11","01;11;51;27","This minimum of two thumbs up so much. So it's awesome to hear about that and then to connect like expectation maximization and the tale of two densities, epistemic and pragmatic. It's like wherever we look, there's partitions into two into multiplicity and unity through plurality. And then other times even within the one road, there's the movement in both directions, forward and backwards in time or the forward propagation of a generative model which could from just one."
"Speaker 1","01;11;51;27","01;12;09;29","If all you knew was one, you could simulate sevens. If all you knew was seven and the structure of the network, of course you could back propagate the best possible mean and error estimates using back propagation of error. OK, Stephen."
"Speaker 3","01;12;11;28","01;12;31;12","This is helpful for thinking about how we perceive things as well for I was Dave was saying about making choices, it can be when we get zero error or whatever seen as negligible error, you want to just allow that to carry on and stay with your imagination. So I go to the bar and I a lot of things are the same, right?"
"Speaker 3","01;12;31;13","01;12;56;16","Maybe there's a happy hour sign and if that's unusual, maybe I'm happy about that. I noticed and if a lot of things are exactly or very similar to how I expected, then in a way my perception is probably mostly my predictions. Just not being contradicted is, oh, actually, I just don't see it. I just I think I've seen it, but I just don't need to see it."
"Speaker 3","01;12;57;00","01;13;12;22","And where there's something which is like ambiguous, like you were saying, that number seven, we're going to use this diagram, starts to have to be looked at. You know, you sort of peer into things, straining our eyes, trying to sort of see, well, what's that? Is it is it a dog? Is it a man running into the woods?"
"Speaker 3","01;13;12;22","01;13;35;21","Is it a dog? Is it men run into the woods you know, you start to notice things. I think this is also quite useful for that perspective of, you know, when you just you don't actually need to see a lot of stuff. It's quite an efficient model, right? If it's if the area goes below a certain level, you just move on and carry on because that's where the challenge with autism is."
"Speaker 3","01;13;35;21","01;13;43;29","They think because too many things have been looked at, the are possibly negligibly relevant at the time."
"Speaker 1","01;13;46;14","01;13;47;05","OK, thank."
"Speaker 2","01;13;47;05","01;14;22;00","You. Thanks. So just one last one. Not seeing death before because I know you want to clean this up. I think one of the analogies that I draw is with the emphasis on the word minimum to mean to, doesn't mean it's always a you know, a hard bifurcation and a or a binary. It means that the minimum amount of things like even when you take your shoelace and wind it through all the eyes of your shoe, you've got two ends that you will eventually will loop back together and and entangle in some way to hold your shoe on."
"Speaker 2","01;14;22;05","01;14;58;09","So even though it's one place I think what a lot I think what active inference reinforces because it's active and inference is that ability to be able to see two things, both seemingly contradictory and both actually true. It can be multiple things. You can have multiple as you can have multiple surprises, some of which are make you quite happy, and others where you walk in and say, well, why hasn't it been happy hour at this hour when I've been at it this far and multiple other days?"
"Speaker 2","01;14;58;09","01;15;43;27","Like you could be disappointed in that as well as being pleasantly surprised. So I think what this again just reinforces the minimum of two allows for the integration. The minimum of two recognizes the differentiation. And when we're talking about statistical issues and when we're talking about errors and when we're talking about energy, I think I think the way that the that these folks have been able to discover what so what architecture would allow for that is what's really interesting to me now because I wasn't even thinking in these sort of so what's what is the what is the scaffold look like?"
"Speaker 2","01;15;44;18","01;15;47;10","But it looks like there's been a lot of work done in this area."
"Speaker 1","01;15;47;10","01;16;14;20","So so let's take it to action as we head into our last little bit on this. And of course, if anyone is more author or not familiar with the Formalisms, just contribute to act and fly help us annotate the papers and read these deeply because there's so much to learn. And somebody getting involved like a few days or weeks before these streams is a leverage point if you want to help other people understand this area."
"Speaker 1","01;16;15;23","01;16;44;21","So I've changed the colors just all to great because they're all just random variables. Now, we had the seven, so we are back propagating error on temperature prediction. But also maybe this system is predicting pressure and so then three might be like estimating, you know, is the water boiling again? Just we're getting one, one step closer to the cafe."
"Speaker 1","01;16;45;22","01;17;21;16","So if if we were only trying to predict temperature, we would back propagate along the seven, 63 to one root. If we were only predicting pressure we could back propagate along 85321 the fact that there is a parents that is upstream, it's like the last common ancestor of pressure and temperature like we might be able to reduce these errors a lot on the temperatures by having this higher order variable is the water boiling."
"Speaker 1","01;17;21;29","01;17;52;08","And so we want to imagine like if the answer is yes, then it's like it's expecting that this is like higher on both. And if it's low, it's lower on both. So the generative model can go from the water's boiling. What pressure and temperatures do I expect? And you can add a third note, it allows you to conditioned on a cognitive structure, go from observations of just temperature around the haunt to pressure or from pressure and temperature up to the water boiling."
"Speaker 1","01;17;52;23","01;18;31;24","But the influence of these observations is like limited through this chokepoint point. It's actually a marcott blanket with respect to what's on the either side of it. Let's bring action into the picture. Action can also be understood as parametric. What angle should my elbow be at? What angle should the eyes be oriented? Those types of things. Again, not whether that's what the actual neural signaling in the brain is doing, but this is how we model actions like the car."
"Speaker 1","01;18;32;07","01;19;14;09","How many miles per hour should I be going those kinds of questions is parametric modeling of action. And that's what was so fun and exciting was the first 50 equations in this paper have scaffold of this tremendous general node computation architecture. Multilevel means and variances forward and backward, up, down, left, right, all of these interesting ways and connections but not yet action not because we hadn't brought action in as a dancing partner, but actually this was above any interpretation of the nodes as inference or action."
"Speaker 1","01;19;15;01","01;19;46;25","And then it's as easy to condition on action as it is to condition on any other inference because action is just a parameter for parametric active entities that are engaging in action as inference planning as inference and so on. And so then one can think about what is the compute graph, whether it's composed of just nodes and edges or whether we think of some of those nodes as being like variance estimators."
"Speaker 1","01;19;47;23","01;20;21;17","And then, you know, this maybe again, the connections are not going to rewire this whole graph, but just like is is this a water boiling situation OK, so we have a thermometer. It says that it's 100. See, it's boiling tap. We did the pressure sensors, broke it but we back propagate it. We're in a water, we're high confidence on the water boiling."
"Speaker 1","01;20;23;14","01;20;54;15","Then this is like another variable that's going to require like a different sort of question that it's answering. But one can imagine that if this is a water boiling situation, and the water's not boiling, there might be an error. And so how can we reduce loss function on the total compute graph, including observation and action? How can we reduce uncertainty about a cognitive model, including observation and action?"
"Speaker 1","01;20;55;04","01;21;29;19","There's two ways to do it, and it's how they introduce it in the very beginning of the of the paper, which is that the minimization of the loss function of the free energy expected for energy variation for energy can be achieved in multiple ways through immediate inference about the hidden states, the world explaining perception, updating a global world model to make better predictions, slower parametric inference updates that have the interpretation of learning."
"Speaker 1","01;21;30;15","01;21;51;04","And when action is a variable that we have agency over, it introduces some complexities like the need to have preference and the need to specify a time horizon and to have uncertainty about the consequences of action. Right. Wouldn't it be easy if we were just watching the movie and our actions didn't have any influence upon the story that we saw?"
"Speaker 1","01;21;51;12","01;22;20;12","It'd be a different inference and finally, through action to sample sensory data from the world that conforms to the predictions, potentially providing this integrated accounts of adaptive behavior and control. So it took us 50 equations at the general before we could jump into action. But it's a deflationary approach to action because action is a parameter in a broader cognitive and computational framework."
"Speaker 1","01;22;20;23","01;22;21;13","OK, Stephen."
"Speaker 3","01;22;22;24","01;22;46;16","Thanks, Daniel. Yeah, that's really helpful. Actually going back to that, the example with water as well, you can even bring in the higher level of aspects in the sense that one thing that's really hard water compares with a lot of other things. When you heat up oil on stuff is you can have the water boil and you probably had it and and and you can have a thermometer theoretically say 100, but the steam can go above 100."
"Speaker 3","01;22;46;27","01;23;14;24","But it's very hard to really it can trick you, right? It's like it's hard. It's actually quite hard to get. I find it hard to get my head around steam being hotter than water because it's kind of just kind of how I know you just think of it as being 100 degrees water, right? And it's almost like if you were to get into that situation where after I burned myself one time or there's almost maybe I just start to have a feeling of fear, if I have a sense that I'm somewhere where what might be coming out is high temperature steam."
"Speaker 3","01;23;15;01","01;23;32;19","Right. Even the and that will override the fact that perceptually I haven't got a lot of ways to easily sort of gauge that visually unlike, say, a ring getting red and you know, other things."
"Speaker 1","01;23;33;29","01;24;12;27","OK, OK, here's here's what that makes me think of. Sometimes we have indirect cues that reduce our uncertainty about something that's not directly observed like the mood ring. If everybody was wearing a mood ring and that was an accurate predictor, you know, that would be this other alternate world. And so there are certain kinds of variables that are more like the magical mood ring or like the temperature of metal within a range because I mean, it's only going to start to get glowing when it's super, super hot."
"Speaker 1","01;24;13;15","01;24;48;18","So it doesn't help you differentiate whether it's freeze like at zero C or 50 C or 200 C, probably even but there's some range for certain things in the worlds where we can use indirect sensory cues affect is not quite here in this model, in this graph, but we've talked a lot about how uncertainty in T can be understood as like anxiety and or negative valence in the world of reward maximization."
"Speaker 1","01;24;49;11","01;25;21;10","Less reward is bad in the reward in the world of precision optimization, excess variance is bad, not any variance. It's not a simple, you know, destroy the variance estimates. It's about the appropriate applications of variance but excess variance, more variability than expected. You told me it was ten plus or -1, but I just pulled 350 -80. That's confusing."
"Speaker 1","01;25;21;10","01;26;00;19","It's more confusing than I thought. So then some of these mean and an error estimate errors in the computational model might have this could be a mean estimate or on valence or one could choose to write a paper where an error estimate or is framed as being an anxiety parameter. That's always that that that jump between what any parameter is in a graphical model and anything about the world or attaching it to some word because oh it's anxiety."
"Speaker 1","01;26;00;19","01;26;40;22","Well then that makes me think of this other situation. But so you can send someone really to easily down a road of associations when the deflationary perspective is just stating what it is OK, fun didn't, didn't expect that we would go this way with the backdrop, but Blue mentioned it last time and and it is important to bring up and also it connects to modern methods for training, neural network architectures and that's what a lot of the author's research has focused on."
"Speaker 1","01;26;41;01","01;27;16;06","So let's just continue to explore action a little bit they reiterate that when we're thinking about active inference, about inference involving action. So cybernetics control theory, any area where some parameters represent observations or latent states, and other parameters represent active things, the first way that you can minimize prediction error is to update predictions to match sensory data corresponding to perception."
"Speaker 1","01;27;16;17","01;27;49;10","And the second is to take action and earlier from the introduction of the paper, they kind of did a little minimum of two even on this, which is to split out perception as rapid inference and learning as a slower updating. But we've talked about the continuity between perception and learning, like if you're seeing the ball move across the screen, is that perceiving the movement of the ball or is it learning a parameter representing the location of the ball?"
"Speaker 1","01;27;50;06","01;28;28;04","So the whether we call something more perceptual or more learning oriented and even abstract learning can be seen as a perception in interior spaces, which has been the focus of a lot of work by Fields and Levan with competency in navigating these abstract spaces. But perception sometimes brings us a little bit not too close, but a little bit just linguistically too, like our experience of perception when the phenomenology and the qualia is not what is in play here, it's about parameter updating in the model."
"Speaker 1","01;28;28;04","01;28;28;11","Yeah."
"Speaker 2","01;28;28;11","01;29;06;10","Dean Do you think? Daniel It's about sample size, but sample size is related to maybe the, the, the time commitment. Like if I, if I'm inferring something that's a very, very short volume of sample versus prediction, which is slightly or relatively larger volume of time commitment to move to that volume. And then modeling, for example, which which takes the the perception and tries to build some something material from that."
"Speaker 2","01;29;07;09","01;29;28;29","Is that do you think that that counts here? Because at least that's what I was that's what I was telling participants we had to take into consideration was sort of the time factor of the sample sample or sampling process sure."
"Speaker 1","01;29;29;13","01;29;54;25","I think if we allow inference to be just brought in, including everything then that's what it is. But yeah, just in its connotation, inference using a model often has the it's sort of like the machine is ready the models ready and we're going to do inference. The new patient comes in with the biometric data, we're going to do inference using this model and we're going to predict their risk of the diagnosis."
"Speaker 1","01;29;55;09","01;30;26;25","And that's the fast perceptual time frame learning is unless it's one shot, learning is taking in multiple patients and updating the parameters of those models. Now, those two stages can be alternated like an expectation maximization or in the tale of two densities, but that doesn't get away from the fact that there's sort of this like one data point mode, like plug and chug inference perception mode, variation of free energy."
"Speaker 1","01;30;27;16","01;31;16;13","And then there's a little bit more of like the updating the model parameters. It's like, are you tuning the engine or are you using the engine loosely in the non action space? In this first way to minimize prediction error, those given the ongoing stream of patients coming to your office, there's two ways to reduce your surprise. Those are the perceptual and learning continuum which does come down to the specifics about like the sample size, like you're in the architecture of learning and perception of you could also reduce surprise by choosing what to sample and or modifying the world."
"Speaker 1","01;31;17;02","01;32;11;08","So that's how they bring action in and it's very interesting to think how many pages of prose I've gone into wondering where action fits in. And there's still many more pages to go in terms of where does formalism 51 take us? Have we cashed the check of pragmatism and activism, extended cognition and so on, like what is 51 with respect to all the qualitative insights of action with all of this depth where what is 51 a short answer, it might be that it's like the Y equals X plus B with respect to all the ways that linear models are used in the world."
"Speaker 1","01;32;12;05","01;32;36;19","This is just like a sort of skeleton archetype that is just the trace or a hint for someone who then wants to analyze specific actions over specific time frames for specific cognitive computational models. But that's where 51 takes us even yeah."
"Speaker 3","01;32;36;19","01;33;14;08","One other piece that could be kind of interesting to throw in there is as well as matching my the sensory data with my predictions, I could reimagine my reality to explain my beliefs which in a way would be like creating my own sensory data by proxy. That makes sense, which is a kind of an interesting slight add on to that."
"Speaker 3","01;33;14;08","01;33;37;10","But it's the same idea because you're not strictly taking action, you're taking actions to match sensory data, but not necessarily the real world sensory data. It could be the, the kind of, you know, the imagined world data I am seeing an angel coming over the top of that hill now, you know."
"Speaker 1","01;33;38;20","01;34;18;17","Yeah, one could engage in mental reverie and it would either be reducing their surprise in the long run or that system would fail to persist in an entropic world. And people have made all kinds of arguments, like in evolutionary psychology, about the basis of why we perceive or believe different things along similar arguments so definitely make sense. Let's just look a little bit in four or 52, which we didn't previously get to, but of course anyone with any other like questions in the live chat welcome to do so."
"Speaker 1","01;34;19;29","01;34;52;04","So here in 26, Livestream 26, we talked about PID control, we talked about integrator chains and about generalized coordinates of motion. So just a reminder on what 26 looked like we had from left to right is time. These are different time points. And then on each time point there's this vector is like a stack reflecting the observable path."
"Speaker 1","01;34;52;23","01;35;22;19","And then also things that are latent states. They're not observable to the world, real or imagined. They're the derivatives of how the observables are changing. And those can be the position. It's like the zeroth derivative but then also the velocity jerk, so acceleration, so on. So we looked at that in 26, but it wasn't connected formally to predictive processing."
"Speaker 1","01;35;22;19","01;36;24;23","Predictive coding PD control is restated in 53 using the formalisms, using the notation that is going to prepare for the merge with predictive coding like a for action epsilon for errors and so the the error term is included in all of the three levels to obtain equivalence to predictive coding. We utilize a linear identity generative model with three levels of dynamical orders so the joint distribution of all those, all the observables and all the X's through time is going to be this factor sized expression so the distribution of the observation conditioned on the latent state."
"Speaker 1","01;36;25;25","01;36;59;14","And then that is conditionally independent of. So it allows it to be factor ized and calculated out separately. Here's like the first derivatives, second derivatives with o double prime and x double prime here is ex conditioned upon its own derivative, the derivative of x condition on its derivative and the sort of root node is the double derivative of X because this one like stands alone."
"Speaker 1","01;36;59;14","01;37;36;16","That's where our model kind of taps out. That's equivalent to node one here. Like there's no variance term leading to one. There could be, but then that would be a different model. So these equations in 54 restate this dynamical model and introduce Omega, which I believe would be an error term, the CW and also the views which is the desired set point for X at that order."
"Speaker 1","01;37;37;01","01;38;13;10","So at the first, second or third derivative, what, what is desired there then that allows so so this is like a graph where instead of these being different things like what if, what if the kind of mixing architecture is a little bit because the generalized coordinates doesn't have errors interleaving, it just has the state, the first derivative, the second derivative and so on rather than interleaving error terms."
"Speaker 1","01;38;13;10","01;38;44;07","But we can imagine another similar graph where as you back propagate, it's like higher and higher derivatives. That's the integrator chain but I guess in 55, 56 and 57 and 58, they do something similar to what we had done with the generic computation graph or to the predictive coding architecture with the interleaving of the means and the errors."
"Speaker 1","01;38;45;06","01;39;02;15","This is just that on the graph architecture where the nodes have this interpretation of being derivatives of one another, what is there to say? That's an interesting discussion."
"Speaker 2","01;39;03;10","01;39;35;03","This is bringing back flashbacks to try to try to get my head around 500 level economics courses. And when you'd have somebody walk in and start pounding stuff up on a whiteboard, on, you know, second, third and fourth derivatives and just just feeling like you'd been tossed into a blender and you had no way of avoiding the blades so I'm glad, I'm glad that there's people that can do this stuff, but I would have to dedicate way more time to trying to parse what the heck they're talking about."
"Speaker 2","01;39;35;13","01;39;42;11","Because it, it's a wall to me. It's really hard. So yeah."
"Speaker 1","01;39;42;22","01;40;12;24","I feel you maybe for the last few minutes we can kind of pull back to OK, as we swoop out of this bathtub, out of the dot to into the great beyond like what have we taken with us? Why was this lengthy journey through predictive coding and the 60 formalisms and what Maria brought with the thousands of years of philosophy and history on perception?"
"Speaker 1","01;40;13;04","01;40;22;06","So where are we and how are we different now and going forward than we were before we started? 43."
"Speaker 2","01;40;25;09","01;40;51;07","I'll say. But what it's done for me, first of all, bringing up the historical piece of it was really, really helpful because this didn't just sort of come up yesterday or in the last couple of years. It's been around for a while. I think for me, predictive, I wouldn't have seen predictive coding as a as a sort of formalized way of retracing a navigational route, but that's what it screamed to me in."
"Speaker 2","01;40;51;12","01;41;21;04","And after the point one I listened to you having hearing you in blue talk about it when when the idea of the when the question around back propagation came up, that's when I sort of did a deeper dove on that. And again, it wasn't I wasn't picking up on how to do that path retracement. The Formalisms weren't really addressing what I thought was being said."
"Speaker 2","01;41;21;04","01;42;02;25","I think what today helped me with is sort of examining if you if you don't have traces laid down that are material that you can see if you weren't actively part of the building of the path, that there are still some graphical statistical ways of organizing information to give you a better sense of, OK, so you didn't lay the bread crumbs down, but we can still give you a better sense of where you came from as a result of being able to use this kind of formalism."
"Speaker 2","01;42;03;14","01;42;51;19","I mean, honestly, Daniel, 53. 5053. Come on. I mean, if you if you're one of the people that was able to write those 53 out and actually present it in paper form, I mean, props to you, you get, you get many thank you's but holy mackerel other than the fact that that there is some evidence now that you can retrace even though you don't have necessarily those markers or you didn't self-correct, you didn't you weren't aware of how you self-corrected whereas the predictive coding at least gives you an opportunity to maybe trace that, retrace that back, even though you didn't spend a lot of time trying to organize the evidence."
"Speaker 2","01;42;51;19","01;43;40;07","The other thing that it does is I think it reinforces the idea that the way that these live streams are structured, which is just introduce the paper, just give the paper, it's it's opening, like open up the box in the point to you can actually go into one particular area of the unknown which for me was the back propagation which you and Stephen helped me with in the point to we are still doing the part where the point to could always even if you didn't have a record of the point one on the dot zero, this paper told me that you would still have some way of knowing where that elusive dot zero exists relative"
"Speaker 2","01;43;40;07","01;43;53;01","to this this live stream today and again, I could be over, I could be overfitting, but I don't think I am otherwise why would you, why would you bring out 53 operations."
"Speaker 1","01;43;57;14","01;43;59;10","A lot of interesting stuff there so."
"Speaker 2","01;44;02;24","01;44;27;26","Because this is the level this is exactly I mean what we're doing in the live stream in the levels that they're talking about they they affect each other in exactly the same way it's diagramed out here as a stack and we tend to think of ours because it's over multiple multiple days as sort of a horizontal continuation, but they're essentially one and the same."
"Speaker 1","01;44;29;21","01;45;02;05","So that's very interesting. So it's almost like, well, there's the zero in the one of the two, they can go both ways. So in, in the temporal sense, the dot zero is a precursor temporally to the dot one, the dot two. So for the for the for the YouTube channel subscriber, this is their observation. Oh, 43.0. Oh 43.1 has been Livestream."
"Speaker 1","01;45;02;05","01;45;13;02","43.2 has been livestreamed. But what is like the derivative that sort of sets the initial conditions for dot two? It's the dot one."
"Speaker 2","01;45;13;21","01;45;14;00","Yeah."
"Speaker 1","01;45;14;25","01;46;08;15","And then what of course sets the four, the dot one, dot zero and so there's almost this sense in which as we dampen our uncertainty through action and inference moving forward in a world that's always continually vibrating and transcending our model like it pushes the earlier actions that we take. That high school teacher, that book that you read all the way back even beyond one's own CPL birth as like parameters that dampen or specify if one doesn't want to think about dampening the position acceleration and velocity and so on, they're like higher and higher chains."
"Speaker 1","01;46;09;06","01;46;33;14","And that means that they have like they're further and further away from where the rubber hits the road for the YouTube subscriber on the outside. But they're absolutely like part of the process by which something comes to be and then the DOT two is that it can exist without the dot one. And then then the first time it made me think of like the negative of this, like even before that was zero."
"Speaker 1","01;46;33;22","01;46;37;24","What is, what is there before zero? What happens before zero?"
"Speaker 1","01;46;40;01","01;46;56;22","Is there a negative number? There's no number that's positive, that's smaller. But what is is it is it a letter? Is it a shape like is it its own big bang? It's like and there's a sense that it is and there's a sense that it isn't."
"Speaker 2","01;46;59;04","01;47;32;10","That's this part of it's the pre-reading. The other thing that I think maybe doesn't sort of register explicitly is that the error propagation in the way that we do the live streams remains. There are some corrections and there are some there is a sort of a gradual movement towards a bit of minimization. But every time we we come together and try to figure out, we don't come from a position of we figured it out."
"Speaker 2","01;47;33;01","01;48;10;07","We come from a position of, well, what will we what will we do with this today? So we retain that error propagation. And I think that that's a I mean, that's not convention convention is why would we spend time on something where we leave a bunch of error baked in? But I think when when again, if we're if we're going to differentiate, if we're going to truly integrate, if we're going to be able to say that you can have it a point to existing with or without a point, one or 2.0, of course you can."
"Speaker 2","01;48;10;17","01;48;42;13","But how superficial is that relative to the method because of the architecture, the because of the way it's structured, how much deeper you can actually go and how many times you can bump into a dead end immediately, find out how quickly you're going or, you know, going off the rails, so to speak. As I can't remember which one of the live themes was that you had the the train moving down the track, but you can find out whether or not something is a dead end quickly and re correct."
"Speaker 2","01;48;42;13","01;49;23;03","That's what that retracing part today I think is one of the well it's it's the take away from this this set of live streams that I wouldn't anticipate it based on the title of the paper that that retracement piece was going to be the thing that was going to sort of percolate and make most sense in terms of why the structure is is yes there's an optimizing piece to it but there's also a piece that that allows for mistakes without without you know, I mean, sometimes it's very consequential, but lots of times it's the consequences."
"Speaker 2","01;49;23;04","01;49;30;01","What's minimized not the error. And I think that's what the architecture affords. You want to talk about affordances."
"Speaker 1","01;49;34;03","01;50;24;08","So when you said the consequences minimized and not the error, that makes me think about like high reliability organizations. Of course, one way to minimize the consequences of error would be to never have errors, zero tolerance for anything but that is challenged in a world again, that's always throwing us challenges. So in in in that setting, it's almost like the observation here, the one that we actually want to reduce our loss function on is not the so called proxy of error occurring, but we could reduce our loss function on function on the thing working and just say we are minimizing the consequences of all this upstream stuff."
"Speaker 1","01;50;24;18","01;51;06;27","Some of the stuff that might have the interpretation of error and things that might not. They're just nodes in this graph. And then what is going to dampen surprise about the consequences. And so like if you say, you know, anything that you say to me right now, I'm going to accept it and believe it. And then it's like, you know, in a trust space that opens up there to be like a communication that goes deep if it's being honestly said."
"Speaker 1","01;51;08;10","01;51;50;06","And that's because like what's being minimized, like don't tell me something surprising. It's like my consequence. You can have precision on my consequence. That's one direction of the conversation. And that is what allows that second direction to really come into play with the transmission of variability and rather than, well, how am I going to reduce my uncertainty about action relative to what the person is going to tell me, I won't talk to them or I'll tell them what to tell me, or I'll leave them in the dark about how I'm going to act."
"Speaker 1","01;51;50;24","01;52;27;21","And then they'll have uncertainty about how I'll act if they say something even trivial. And so I think this like really it it spans many levels and ways here because there's nested levels and deep chains. But I agree. I think on the more topical if such a thing could exist some of the takeaways that that I had also really appreciated, Maria providing a lot of the philosophical context and then that one part where we're like, wasn't this a meme like thousands of years ago?"
"Speaker 1","01;52;27;21","01;52;57;24","And in many cultures in the world, what is it that still makes the predictive mind and surfing uncertainty and being you? What makes these novel and surprising and even somehow counter cultural? What is happening there? What chain is leading to that where the idea that word generators with agency rather than passive receivers. Why is that such a hill to climb and die on?"
"Speaker 1","01;52;58;12","01;53;33;05","That's one piece I top away then. It was awesome with blue to work on this in the one where we had like on the bottom left the qualitative philosophical insights and connecting to technical and formal insights on the bottom right and biological systems. Sort of like where are we engaging deeply with one of these vertices where are we engaging deeply with an edge?"
"Speaker 1","01;53;34;04","01;54;35;13","Where are we trying to grab two vertices and the edge or go around the horn or where are we trying to really go for the triple play every baseball home base, inference and action in the bottom of the ninth of the dot to it's it's a full count and it's a power play that. And then also I think the last piece that the kind of takeaway is just when the computational and cognitive framework are defined generally enough like at the level of a graphical network which opens up message passing variational inference back propagation of error predictive processing architectures hybrids of all of these approaches then action is deflated or it's inflated maybe or whatever it"
"Speaker 1","01;54;35;13","01;55;21;24","is that we even say it becomes composable and tractable to discuss open ended combinations of perception, cognition and action and impact, which is the home base in the niche. And so if even that model has to be structurally revised, like we end up having another way of framing entity and the niche interaction it may fit within this framework or it may necessitate revision of the framework, but within the entity, niche interaction space and the nested systems and the counterfactual cognitive systems, there's almost like less and less to action than it seems."
"Speaker 1","01;55;22;23","01;55;55;21","It doesn't remove the challenge of action. It it reduces something else that I'm not quite sure about what it is like. It doesn't make it easy but something is being removed as we're learning and working through this but it doesn't make the climb or it makes the climb different to think differently, but it doesn't move. Does it mean that fewer foot pounds of force have to be applied?"
"Speaker 1","01;55;57;12","01;56;11;00","But there is something different about the navigation around action and inference via active inference that I think is will continue to develop for a long time."
"Speaker 1","01;56;13;20","01;56;22;12","OK, Dean and also Stephen, thanks a lot. I wasn't sure there for a second at the beginning. What would I got to think?"
"Speaker 2","01;56;22;13","01;56;49;02","Well, I got it and I got it. Thank you again, Daniel, because obviously you did understand have it. I have a reasonable understanding far more than mine in terms of what that brought back. Propagation implication was in terms of marrying it to the predictive coding. So didn't mean to put you into the tutor seat today, but appreciate the fact that you were able to sort of pull it apart and then put it back together in a way that made a lot more sense than just reading it off the page."
"Speaker 2","01;56;49;02","01;56;51;02","So thank you, sir."
"Speaker 1","01;56;51;15","01;56;53;07","Thank you, Dean. See you later."
"Speaker 2","01;56;53;20","01;56;54;10","All right. Take care."
"Speaker 1","01;56;54;27","01;56;55;07","Bye."

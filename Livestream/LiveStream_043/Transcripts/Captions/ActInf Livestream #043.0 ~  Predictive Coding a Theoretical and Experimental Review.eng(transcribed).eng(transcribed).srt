1
00:00:05,000 --> 00:00:26,800
[Music]

2
00:00:26,800 --> 00:00:29,279
hello and welcome everyone

3
00:00:29,279 --> 00:00:33,280
it's april 28 2022 and we're here in

4
00:00:33,280 --> 00:00:36,800
actin flab live stream number 43.0

5
00:00:36,800 --> 00:00:38,719
discussing predictive coding a

6
00:00:38,719 --> 00:00:42,160
theoretical experimental review

7
00:00:42,160 --> 00:00:44,480
welcome to the actin actinflab we're a

8
00:00:44,480 --> 00:00:46,480
participatory online lab that is

9
00:00:46,480 --> 00:00:48,399
communicating learning

10
00:00:48,399 --> 00:00:50,559
practicing applied active inference you

11
00:00:50,559 --> 00:00:53,120
can find us at links here on the slide

12
00:00:53,120 --> 00:00:55,039
this is recorded in an archived live

13
00:00:55,039 --> 00:00:56,640
stream so please provide us with

14
00:00:56,640 --> 00:01:00,160
feedback so we can improve our work

15
00:01:00,160 --> 00:01:01,760
all backgrounds and perspectives are

16
00:01:01,760 --> 00:01:02,719
welcome

17
00:01:02,719 --> 00:01:04,559
and we'll be following video etiquette

18
00:01:04,559 --> 00:01:07,360
for live streams

19
00:01:07,600 --> 00:01:08,840
head over to

20
00:01:08,840 --> 00:01:11,040
activeinference.org if you want to learn

21
00:01:11,040 --> 00:01:13,119
more about how to participate in the

22
00:01:13,119 --> 00:01:15,119
live streams or anything else happening

23
00:01:15,119 --> 00:01:18,000
in actin flap

24
00:01:18,000 --> 00:01:19,280
okay

25
00:01:19,280 --> 00:01:23,040
we're here today to learn and discuss

26
00:01:23,040 --> 00:01:25,119
the paper predictive coding a

27
00:01:25,119 --> 00:01:27,680
theoretical and experimental review by

28
00:01:27,680 --> 00:01:30,640
baron millage anil seth and christopher

29
00:01:30,640 --> 00:01:32,079
buckley

30
00:01:32,079 --> 00:01:34,439
the video is just an introduction in a

31
00:01:34,439 --> 00:01:36,640
contextualization for some of the ideas

32
00:01:36,640 --> 00:01:38,720
and some of the details of the paper in

33
00:01:38,720 --> 00:01:40,400
the broad sense

34
00:01:40,400 --> 00:01:43,360
it's not a review or a final word

35
00:01:43,360 --> 00:01:45,680
it's uh as with some of these other

36
00:01:45,680 --> 00:01:48,079
technical dense papers it's like an

37
00:01:48,079 --> 00:01:50,240
opening for those who have technical

38
00:01:50,240 --> 00:01:52,960
questions at the learning side of things

39
00:01:52,960 --> 00:01:54,399
or at the more

40
00:01:54,399 --> 00:01:56,880
advanced research side of things to come

41
00:01:56,880 --> 00:01:58,719
get involved because this is a technical

42
00:01:58,719 --> 00:02:01,600
paper but also hopefully as will unpack

43
00:02:01,600 --> 00:02:04,000
it has a lot of cool biological and

44
00:02:04,000 --> 00:02:07,600
philosophical implications

45
00:02:07,600 --> 00:02:11,599
in 43.0 we're going to say hello and

46
00:02:11,599 --> 00:02:13,920
introduce some big questions then cover

47
00:02:13,920 --> 00:02:15,840
the aims and claims of the paper the

48
00:02:15,840 --> 00:02:18,239
abstract and the roadmap and then pretty

49
00:02:18,239 --> 00:02:20,160
much just jump right in

50
00:02:20,160 --> 00:02:22,400
and we're going to focus a lot on the

51
00:02:22,400 --> 00:02:24,879
introduction the context and the single

52
00:02:24,879 --> 00:02:27,680
layer predictive coding model and then

53
00:02:27,680 --> 00:02:29,440
we'll go a little bit faster through the

54
00:02:29,440 --> 00:02:31,440
later sections of the paper talking

55
00:02:31,440 --> 00:02:33,440
about some generalizations of predictive

56
00:02:33,440 --> 00:02:35,599
coding and some important points that

57
00:02:35,599 --> 00:02:37,440
the authors race so if you want to

58
00:02:37,440 --> 00:02:40,800
participate in 43.1 or 0.2 in the coming

59
00:02:40,800 --> 00:02:43,840
weeks just let us know

60
00:02:43,840 --> 00:02:46,959
okay so we can say hello

61
00:02:46,959 --> 00:02:49,599
and give any information that we'd like

62
00:02:49,599 --> 00:02:52,000
and also just maybe something that we

63
00:02:52,000 --> 00:02:54,000
thought was exciting or something that

64
00:02:54,000 --> 00:02:57,440
motivated us to get involved in this

65
00:02:57,440 --> 00:02:59,200
quite involved paper

66
00:02:59,200 --> 00:03:01,280
so i'm daniel i'm a researcher in

67
00:03:01,280 --> 00:03:02,640
california

68
00:03:02,640 --> 00:03:05,360
and i was curious to learn more about

69
00:03:05,360 --> 00:03:07,599
how predictive coding and predictive

70
00:03:07,599 --> 00:03:10,560
processing related to active inference

71
00:03:10,560 --> 00:03:14,080
and also just about how different models

72
00:03:14,080 --> 00:03:16,800
framed anticipatory systems

73
00:03:16,800 --> 00:03:20,080
so over to you maria

74
00:03:21,920 --> 00:03:24,159
hello daniel and everyone

75
00:03:24,159 --> 00:03:26,720
um i have a bi-color degree in

76
00:03:26,720 --> 00:03:29,680
psychology and i am a master's candidate

77
00:03:29,680 --> 00:03:31,920
in philosophy of science

78
00:03:31,920 --> 00:03:34,720
in the university of sao paulo brazil

79
00:03:34,720 --> 00:03:36,159
and i

80
00:03:36,159 --> 00:03:38,720
am researching the relationship between

81
00:03:38,720 --> 00:03:41,360
predictive processing and illusionist

82
00:03:41,360 --> 00:03:43,200
theories of consciousness

83
00:03:43,200 --> 00:03:45,599
and i think what brought me here today

84
00:03:45,599 --> 00:03:46,799
was

85
00:03:46,799 --> 00:03:47,680
uh

86
00:03:47,680 --> 00:03:50,879
my my wish to start learning about the

87
00:03:50,879 --> 00:03:53,360
formalisms because i don't really see it

88
00:03:53,360 --> 00:03:55,599
in philosophy and i don't actually need

89
00:03:55,599 --> 00:03:58,720
it for my dissertation but eventually i

90
00:03:58,720 --> 00:04:01,280
wanted to continue my my work on

91
00:04:01,280 --> 00:04:03,439
predictive processing so eventually i

92
00:04:03,439 --> 00:04:05,599
had to start it and i just thought about

93
00:04:05,599 --> 00:04:07,760
maybe that's a good idea

94
00:04:07,760 --> 00:04:10,239
to come here and minimize my

95
00:04:10,239 --> 00:04:12,239
uncertainties

96
00:04:12,239 --> 00:04:15,040
awesome through inference and or through

97
00:04:15,040 --> 00:04:17,519
action and also thanks a lot brock for

98
00:04:17,519 --> 00:04:19,918
helping in the dot zero

99
00:04:19,918 --> 00:04:21,600
so we'll just start with the big

100
00:04:21,600 --> 00:04:22,880
questions and these are the kind of

101
00:04:22,880 --> 00:04:25,600
questions that might motivate someone or

102
00:04:25,600 --> 00:04:28,240
interest them in this paper without even

103
00:04:28,240 --> 00:04:30,160
mentioning active inference per se but

104
00:04:30,160 --> 00:04:31,520
these are like some big questions that

105
00:04:31,520 --> 00:04:33,040
get touched upon

106
00:04:33,040 --> 00:04:35,120
what is the formal basis of predictive

107
00:04:35,120 --> 00:04:36,240
coding

108
00:04:36,240 --> 00:04:39,680
how is predictive coding used or useful

109
00:04:39,680 --> 00:04:41,520
what are some areas of current and

110
00:04:41,520 --> 00:04:43,759
future development and what is the

111
00:04:43,759 --> 00:04:46,000
relationship between predictive coding

112
00:04:46,000 --> 00:04:48,479
and active inference and hopefully we'll

113
00:04:48,479 --> 00:04:50,560
draw out more questions

114
00:04:50,560 --> 00:04:55,240
anything else to add about this

115
00:04:56,880 --> 00:04:58,639
no not really

116
00:04:58,639 --> 00:05:00,400
cool

117
00:05:00,400 --> 00:05:02,639
the paper is predictive coding a

118
00:05:02,639 --> 00:05:05,039
theoretical an experimental review i

119
00:05:05,039 --> 00:05:08,240
think the first version was from 2021

120
00:05:08,240 --> 00:05:11,759
but the second revised version was 22

121
00:05:11,759 --> 00:05:14,720
by millage seth and buckley and just to

122
00:05:14,720 --> 00:05:15,680
review

123
00:05:15,680 --> 00:05:16,720
the

124
00:05:16,720 --> 00:05:18,960
core claim and then some of the aims and

125
00:05:18,960 --> 00:05:20,880
directions they set out

126
00:05:20,880 --> 00:05:23,120
they write that no comprehensive review

127
00:05:23,120 --> 00:05:25,199
of predictive coding theory and

128
00:05:25,199 --> 00:05:27,120
especially of recent developments in the

129
00:05:27,120 --> 00:05:31,039
field exists that's their claim

130
00:05:31,039 --> 00:05:33,120
and then they aim towards

131
00:05:33,120 --> 00:05:35,199
providing a comprehensive review both of

132
00:05:35,199 --> 00:05:36,960
the core mathematical structure and

133
00:05:36,960 --> 00:05:40,479
logic of predictive coding so there's a

134
00:05:40,479 --> 00:05:42,479
mathematical review

135
00:05:42,479 --> 00:05:45,360
and summarization aspect to the paper

136
00:05:45,360 --> 00:05:47,440
and then they also review a wide range

137
00:05:47,440 --> 00:05:49,039
of classic and recent work within the

138
00:05:49,039 --> 00:05:51,360
framework ranging from biologically

139
00:05:51,360 --> 00:05:53,440
realistic micro circuits that could

140
00:05:53,440 --> 00:05:55,680
implement predictive coding to the close

141
00:05:55,680 --> 00:05:57,919
relationship between predictive coding

142
00:05:57,919 --> 00:06:00,000
and the widely used back propagation of

143
00:06:00,000 --> 00:06:01,440
error algorithm

144
00:06:01,440 --> 00:06:02,880
as well as surveying the close

145
00:06:02,880 --> 00:06:05,199
relationship between predictive coding

146
00:06:05,199 --> 00:06:09,280
and modern machine learning techniques

147
00:06:09,759 --> 00:06:11,759
that's how they set out where they want

148
00:06:11,759 --> 00:06:12,960
to go

149
00:06:12,960 --> 00:06:15,680
um would you like to read the first

150
00:06:15,680 --> 00:06:18,880
slide of the abstract

151
00:06:19,759 --> 00:06:20,720
okay

152
00:06:20,720 --> 00:06:22,479
um so

153
00:06:22,479 --> 00:06:25,199
predictive coding offers a potentially

154
00:06:25,199 --> 00:06:28,160
unifying account of cortical function

155
00:06:28,160 --> 00:06:30,479
postulating that the core function of

156
00:06:30,479 --> 00:06:32,319
the brain is to minimize prediction

157
00:06:32,319 --> 00:06:34,560
errors with respect to a generative

158
00:06:34,560 --> 00:06:36,880
model of the world

159
00:06:36,880 --> 00:06:38,880
the theory is closely related to the

160
00:06:38,880 --> 00:06:41,280
bayesian brain framework and over the

161
00:06:41,280 --> 00:06:43,840
last two decades has gained substantial

162
00:06:43,840 --> 00:06:45,919
influence in the fields of theoretical

163
00:06:45,919 --> 00:06:48,800
and cognitive neuroscience

164
00:06:48,800 --> 00:06:51,759
a large body of research has arisen

165
00:06:51,759 --> 00:06:54,319
based on both empirically testing

166
00:06:54,319 --> 00:06:56,639
improved and extended theoretical and

167
00:06:56,639 --> 00:06:59,280
mathematical models of predictive coding

168
00:06:59,280 --> 00:07:01,599
as well as in evaluating their potential

169
00:07:01,599 --> 00:07:03,840
biological possibility for

170
00:07:03,840 --> 00:07:06,360
implementation in the brain and concrete

171
00:07:06,360 --> 00:07:08,400
neurophysiological and psychological

172
00:07:08,400 --> 00:07:11,120
predictions made by the theory

173
00:07:11,120 --> 00:07:14,960
despite this enduring popularity however

174
00:07:14,960 --> 00:07:16,880
no comprehensive review of predictive

175
00:07:16,880 --> 00:07:19,360
coding theory and especially of recent

176
00:07:19,360 --> 00:07:22,160
development developments in these fields

177
00:07:22,160 --> 00:07:24,639
exists

178
00:07:24,720 --> 00:07:25,759
they write

179
00:07:25,759 --> 00:07:28,080
here we provide a comprehensive review

180
00:07:28,080 --> 00:07:28,880
of

181
00:07:28,880 --> 00:07:30,800
both of the core mathematical structure

182
00:07:30,800 --> 00:07:33,280
and logic of predictive coding thus

183
00:07:33,280 --> 00:07:35,039
complementing recent tutorials in the

184
00:07:35,039 --> 00:07:36,479
literature

185
00:07:36,479 --> 00:07:37,360
and

186
00:07:37,360 --> 00:07:39,520
we also review a wide range of classic

187
00:07:39,520 --> 00:07:41,520
and recent work within the framework

188
00:07:41,520 --> 00:07:43,599
which is what we had read previously in

189
00:07:43,599 --> 00:07:45,360
the ames so that's how the authors

190
00:07:45,360 --> 00:07:47,919
describe their own work and now we're

191
00:07:47,919 --> 00:07:48,960
going to

192
00:07:48,960 --> 00:07:50,639
see their road map see how they went

193
00:07:50,639 --> 00:07:53,360
about getting there and then we're gonna

194
00:07:53,360 --> 00:07:55,599
jump right into the introduction and

195
00:07:55,599 --> 00:07:58,560
then the formalisms

196
00:07:58,560 --> 00:08:01,440
so here we see the roadmap

197
00:08:01,440 --> 00:08:04,160
and the the rough outline of the paper

198
00:08:04,160 --> 00:08:05,680
is as follows

199
00:08:05,680 --> 00:08:08,479
section one provides an introduction

200
00:08:08,479 --> 00:08:11,039
that contextualizes predictive coding

201
00:08:11,039 --> 00:08:13,120
and we'll also be unpacking it in a

202
00:08:13,120 --> 00:08:16,879
philosophical and historical view

203
00:08:16,879 --> 00:08:18,240
section two

204
00:08:18,240 --> 00:08:19,440
provides

205
00:08:19,440 --> 00:08:21,599
an initial pass on the one level

206
00:08:21,599 --> 00:08:23,680
predictive coding model sort of the

207
00:08:23,680 --> 00:08:26,560
kernel or the archetype or the motif of

208
00:08:26,560 --> 00:08:28,720
predictive coding and connects it

209
00:08:28,720 --> 00:08:31,440
importantly to variational inference

210
00:08:31,440 --> 00:08:34,399
then predictive coding is generalized or

211
00:08:34,399 --> 00:08:36,880
extended or elaborated in a few

212
00:08:36,880 --> 00:08:39,279
important directions so like in section

213
00:08:39,279 --> 00:08:40,399
2.2

214
00:08:40,399 --> 00:08:43,360
it's generalized into a multi-level case

215
00:08:43,360 --> 00:08:45,680
in section 2.3 the concept of

216
00:08:45,680 --> 00:08:47,839
generalized coordinates are introduced

217
00:08:47,839 --> 00:08:49,360
in section 4

218
00:08:49,360 --> 00:08:51,839
precision is introduced as a concept and

219
00:08:51,839 --> 00:08:54,240
in section 2.5 it's connected back to

220
00:08:54,240 --> 00:08:56,080
the brain

221
00:08:56,080 --> 00:08:58,080
section 3 discusses paradigms of

222
00:08:58,080 --> 00:08:59,600
predictive coding

223
00:08:59,600 --> 00:09:02,320
both unsupervised and supervised

224
00:09:02,320 --> 00:09:04,240
and also connects it through those

225
00:09:04,240 --> 00:09:06,320
sections and others to some research in

226
00:09:06,320 --> 00:09:07,760
machine learning

227
00:09:07,760 --> 00:09:10,080
and then in section 4 there's

228
00:09:10,080 --> 00:09:13,120
connections between predictive coding

229
00:09:13,120 --> 00:09:15,360
and some other algorithms

230
00:09:15,360 --> 00:09:17,839
both those algorithms that are common in

231
00:09:17,839 --> 00:09:19,680
machine learning and computational

232
00:09:19,680 --> 00:09:21,920
statistics like the error back

233
00:09:21,920 --> 00:09:23,279
propagation

234
00:09:23,279 --> 00:09:25,120
linear predictive coding and column

235
00:09:25,120 --> 00:09:27,519
filtering and normalizing flows and

236
00:09:27,519 --> 00:09:29,120
biased competition

237
00:09:29,120 --> 00:09:31,839
and then in section 4.5

238
00:09:31,839 --> 00:09:34,000
explicit connection is drawn between

239
00:09:34,000 --> 00:09:36,399
predictive coding and active inference

240
00:09:36,399 --> 00:09:38,560
which brings in the cost of action as

241
00:09:38,560 --> 00:09:42,240
well as the idea of pid control and the

242
00:09:42,240 --> 00:09:44,160
discussion section closes with just a

243
00:09:44,160 --> 00:09:46,800
few other thoughts and summaries and

244
00:09:46,800 --> 00:09:48,800
directions and challenges

245
00:09:48,800 --> 00:09:51,839
and there's several appendices so

246
00:09:51,839 --> 00:09:55,920
it's a long ish paper or monograph but

247
00:09:55,920 --> 00:09:58,480
it's an excellent paper and we're just

248
00:09:58,480 --> 00:09:59,839
going to cover

249
00:09:59,839 --> 00:10:01,920
more of an emphasis on the context and

250
00:10:01,920 --> 00:10:04,800
the beginning and then sort of skip over

251
00:10:04,800 --> 00:10:07,040
or trail off over a few things but

252
00:10:07,040 --> 00:10:08,560
there's lots of unpacks so i hope that

253
00:10:08,560 --> 00:10:10,720
we can like learn more in the coming

254
00:10:10,720 --> 00:10:13,040
weeks if the authors want to join or if

255
00:10:13,040 --> 00:10:16,880
anybody else wants to join

256
00:10:16,880 --> 00:10:19,680
anything to add on roadmap or ready for

257
00:10:19,680 --> 00:10:22,079
introduction

258
00:10:22,079 --> 00:10:25,920
let's go all right awesome

259
00:10:25,920 --> 00:10:26,720
so

260
00:10:26,720 --> 00:10:30,480
um i'll just add uh some notes from this

261
00:10:30,480 --> 00:10:32,320
section of the introduction and then

262
00:10:32,320 --> 00:10:34,320
maria feel free to add anything

263
00:10:34,320 --> 00:10:36,560
so how do they introduce predictive

264
00:10:36,560 --> 00:10:38,959
coding they introduce predictive coding

265
00:10:38,959 --> 00:10:41,040
theory as an influential theory in

266
00:10:41,040 --> 00:10:42,480
computational and cognitive

267
00:10:42,480 --> 00:10:45,200
neurosciences which proposes a potential

268
00:10:45,200 --> 00:10:47,680
unifying theory of cortical function

269
00:10:47,680 --> 00:10:49,680
that's like related to the cortex of the

270
00:10:49,680 --> 00:10:50,880
brain

271
00:10:50,880 --> 00:10:52,800
namely that the core function of the

272
00:10:52,800 --> 00:10:53,760
brain

273
00:10:53,760 --> 00:10:56,000
is to minimize prediction error

274
00:10:56,000 --> 00:10:57,600
where the prediction error signals

275
00:10:57,600 --> 00:11:00,399
mismatches between predictive input and

276
00:11:00,399 --> 00:11:02,959
the input actually received minimizing

277
00:11:02,959 --> 00:11:05,360
the divergence between expectations and

278
00:11:05,360 --> 00:11:08,560
observations is going to be a big theme

279
00:11:08,560 --> 00:11:09,519
and

280
00:11:09,519 --> 00:11:11,519
approaching that minimization

281
00:11:11,519 --> 00:11:13,839
it can be achieved in multiple ways and

282
00:11:13,839 --> 00:11:15,920
they're colored differently here

283
00:11:15,920 --> 00:11:17,680
through immediate inference about the

284
00:11:17,680 --> 00:11:19,519
hidden states of the world

285
00:11:19,519 --> 00:11:21,200
like i thought the ball is over here i'm

286
00:11:21,200 --> 00:11:22,880
getting evidence it's over here so i

287
00:11:22,880 --> 00:11:24,720
should just update how i think about the

288
00:11:24,720 --> 00:11:25,839
world

289
00:11:25,839 --> 00:11:27,839
which can explain perception like the

290
00:11:27,839 --> 00:11:29,760
perception of a ball moving across the

291
00:11:29,760 --> 00:11:31,120
visual field

292
00:11:31,120 --> 00:11:33,519
through updating a global world model to

293
00:11:33,519 --> 00:11:35,360
make better predictions which could

294
00:11:35,360 --> 00:11:36,800
explain learning

295
00:11:36,800 --> 00:11:39,200
like learning a little bit higher level

296
00:11:39,200 --> 00:11:41,040
than just where the ball is in the

297
00:11:41,040 --> 00:11:43,600
visual field but like where balls tend

298
00:11:43,600 --> 00:11:45,760
to be in the visual field or how fast

299
00:11:45,760 --> 00:11:47,360
they tend to move around

300
00:11:47,360 --> 00:11:50,000
and finally through action to sample

301
00:11:50,000 --> 00:11:52,079
sensory data from the world that

302
00:11:52,079 --> 00:11:54,320
conforms to the predictions and that's

303
00:11:54,320 --> 00:11:56,000
when we'll introduce action into

304
00:11:56,000 --> 00:11:58,880
predictive coding in the latter sections

305
00:11:58,880 --> 00:12:01,120
of the paper and then just last point

306
00:12:01,120 --> 00:12:03,600
they say that predictive coding boasts

307
00:12:03,600 --> 00:12:05,440
an extremely developed and principled

308
00:12:05,440 --> 00:12:07,440
mathematical framework in terms of

309
00:12:07,440 --> 00:12:10,079
variational inference as well as many

310
00:12:10,079 --> 00:12:12,639
empirically tested computational models

311
00:12:12,639 --> 00:12:14,320
that have close links to machine

312
00:12:14,320 --> 00:12:15,519
learning

313
00:12:15,519 --> 00:12:18,240
so that's how they introduce this is as

314
00:12:18,240 --> 00:12:20,639
a unifying theory that integrates

315
00:12:20,639 --> 00:12:23,040
inference as well as action inference of

316
00:12:23,040 --> 00:12:25,519
different kinds more like perceptual and

317
00:12:25,519 --> 00:12:26,959
more like learning

318
00:12:26,959 --> 00:12:29,680
and they're saying that it has a very

319
00:12:29,680 --> 00:12:32,160
rich mathematical framework and it's

320
00:12:32,160 --> 00:12:34,959
been connected to insights in biology

321
00:12:34,959 --> 00:12:38,320
and in machine learning

322
00:12:38,320 --> 00:12:40,880
to add

323
00:12:43,360 --> 00:12:45,120
nope

324
00:12:45,120 --> 00:12:47,040
so they write predictive coding as a

325
00:12:47,040 --> 00:12:49,440
theory also offers a single mechanism

326
00:12:49,440 --> 00:12:52,279
that accounts for diverse perceptual and

327
00:12:52,279 --> 00:12:55,120
neurobiological phenomena such as end

328
00:12:55,120 --> 00:12:57,360
stopping by stable perception like

329
00:12:57,360 --> 00:12:58,720
illusions

330
00:12:58,720 --> 00:13:01,279
repeat repetition suppression illusory

331
00:13:01,279 --> 00:13:03,920
motions and attentional modulation of

332
00:13:03,920 --> 00:13:05,920
neural activity so they're pointing to

333
00:13:05,920 --> 00:13:08,480
the integrative nature of this theory or

334
00:13:08,480 --> 00:13:11,440
framework to model all of these diverse

335
00:13:11,440 --> 00:13:13,680
phenomena as being an advantage like we

336
00:13:13,680 --> 00:13:15,519
could have a special theory for just by

337
00:13:15,519 --> 00:13:17,680
stable perception a special theory for

338
00:13:17,680 --> 00:13:20,240
just repetition suppression and so on

339
00:13:20,240 --> 00:13:22,639
but this is a framework by which those

340
00:13:22,639 --> 00:13:26,560
outcomes are occurring as kind of

341
00:13:26,560 --> 00:13:28,800
outcomes of an underlying

342
00:13:28,800 --> 00:13:30,800
framework predictive coding rather than

343
00:13:30,800 --> 00:13:32,639
needing special theories so that kind of

344
00:13:32,639 --> 00:13:36,160
like parsimony or conciliance is pursued

345
00:13:36,160 --> 00:13:40,320
in the areas like cognitive neuroscience

346
00:13:40,320 --> 00:13:42,079
and they write as such and perhaps

347
00:13:42,079 --> 00:13:44,880
uniquely among neuroscientific theories

348
00:13:44,880 --> 00:13:47,199
predictive coding encompasses all three

349
00:13:47,199 --> 00:13:49,680
layers of mars hierarchy

350
00:13:49,680 --> 00:13:51,600
by providing well characterized and

351
00:13:51,600 --> 00:13:53,600
importantly an empirically supported

352
00:13:53,600 --> 00:13:55,680
view of what the brain is doing

353
00:13:55,680 --> 00:13:58,000
at all of the computational algorithmic

354
00:13:58,000 --> 00:14:00,160
and implementational levels

355
00:14:00,160 --> 00:14:00,880
so

356
00:14:00,880 --> 00:14:04,160
mars very influential hierarchy it's

357
00:14:04,160 --> 00:14:06,639
kind of like tin bergens questions or a

358
00:14:06,639 --> 00:14:08,720
little bit like maslow's hierarchy you

359
00:14:08,720 --> 00:14:10,720
can think of it as like an organizing or

360
00:14:10,720 --> 00:14:13,120
a sense-making framework again

361
00:14:13,120 --> 00:14:16,079
to the question what are brains doing

362
00:14:16,079 --> 00:14:17,519
and so we're going to have like three

363
00:14:17,519 --> 00:14:19,680
different kinds of answers to what

364
00:14:19,680 --> 00:14:21,839
brains are doing there's going to be an

365
00:14:21,839 --> 00:14:25,680
answer at the level of computation

366
00:14:25,680 --> 00:14:28,240
algorithm and implementation

367
00:14:28,240 --> 00:14:29,199
and so

368
00:14:29,199 --> 00:14:31,440
here with the example of a bird flying

369
00:14:31,440 --> 00:14:34,399
the computational level is like flight

370
00:14:34,399 --> 00:14:36,639
it's like the why what is the algorithm

371
00:14:36,639 --> 00:14:39,279
doing it's a sorting algorithm what is

372
00:14:39,279 --> 00:14:41,920
the brain doing in this situation it's

373
00:14:41,920 --> 00:14:44,560
tracking a moving object

374
00:14:44,560 --> 00:14:46,720
then there's the flapping level which is

375
00:14:46,720 --> 00:14:48,800
the algorithm which is kind of like

376
00:14:48,800 --> 00:14:51,760
pseudo code and that's suggesting

377
00:14:51,760 --> 00:14:54,959
the what of how that y is realized like

378
00:14:54,959 --> 00:14:56,720
what is happening in this sorting

379
00:14:56,720 --> 00:14:59,360
algorithm well it's doing this this this

380
00:14:59,360 --> 00:15:01,680
this this what is the flight function

381
00:15:01,680 --> 00:15:04,000
being realized by it's being realized by

382
00:15:04,000 --> 00:15:05,680
flapping of the wings

383
00:15:05,680 --> 00:15:08,000
and then there's the implementation so

384
00:15:08,000 --> 00:15:11,040
it's not just the flapping of anything

385
00:15:11,040 --> 00:15:13,440
it's the flapping of something specific

386
00:15:13,440 --> 00:15:15,199
and so that's the implementational

387
00:15:15,199 --> 00:15:16,320
details

388
00:15:16,320 --> 00:15:18,560
and those levels of analysis have been

389
00:15:18,560 --> 00:15:21,839
helpful and provocative in connecting

390
00:15:21,839 --> 00:15:24,839
computational cognitive theories to

391
00:15:24,839 --> 00:15:26,959
neurobiological theories so it's been a

392
00:15:26,959 --> 00:15:28,720
very influential

393
00:15:28,720 --> 00:15:29,519
and

394
00:15:29,519 --> 00:15:31,279
um

395
00:15:31,279 --> 00:15:33,600
discussion

396
00:15:33,600 --> 00:15:35,519
provoking framework that has helped

397
00:15:35,519 --> 00:15:37,839
people connect as well as challenge some

398
00:15:37,839 --> 00:15:39,440
of the similarities and differences

399
00:15:39,440 --> 00:15:43,360
between like computers and braids

400
00:15:45,199 --> 00:15:47,519
great

401
00:15:48,480 --> 00:15:50,399
this is still in the introduction and

402
00:15:50,399 --> 00:15:51,680
they're going to kind of give us a

403
00:15:51,680 --> 00:15:53,519
qualitative perspective on predictive

404
00:15:53,519 --> 00:15:55,519
coding and then we're going to be diving

405
00:15:55,519 --> 00:15:58,560
into a lot more historical and technical

406
00:15:58,560 --> 00:16:00,959
detail but how does predictive coding

407
00:16:00,959 --> 00:16:01,839
work

408
00:16:01,839 --> 00:16:03,680
the core intuition behind predictive

409
00:16:03,680 --> 00:16:05,920
coding is that the brain is composed of

410
00:16:05,920 --> 00:16:08,079
a hierarchy of layers which are each

411
00:16:08,079 --> 00:16:10,240
making predictions about the activity of

412
00:16:10,240 --> 00:16:12,160
the layer immediately below them in the

413
00:16:12,160 --> 00:16:14,720
hierarchy so there's two directions in

414
00:16:14,720 --> 00:16:16,639
this predictive coding multi-layer

415
00:16:16,639 --> 00:16:19,519
scheme which we're just introducing here

416
00:16:19,519 --> 00:16:21,440
as kind of a heuristic and then we're

417
00:16:21,440 --> 00:16:23,600
going to zoom in to just what's

418
00:16:23,600 --> 00:16:25,519
happening at one level and then we're

419
00:16:25,519 --> 00:16:27,279
going to pull back out to kind of

420
00:16:27,279 --> 00:16:30,160
recovering this multi-level formalism

421
00:16:30,160 --> 00:16:32,480
the downward descending predictions at

422
00:16:32,480 --> 00:16:34,800
each level that's the blue arrow

423
00:16:34,800 --> 00:16:36,320
are compared with the activity and

424
00:16:36,320 --> 00:16:38,639
inputs of each level level to form

425
00:16:38,639 --> 00:16:40,560
prediction errors this is the

426
00:16:40,560 --> 00:16:42,560
information in each layer which could

427
00:16:42,560 --> 00:16:45,040
not be successfully predicted

428
00:16:45,040 --> 00:16:46,959
these prediction errors are then fed

429
00:16:46,959 --> 00:16:50,240
upwards that's the red arrow to serve as

430
00:16:50,240 --> 00:16:52,480
inputs to higher levels which can then

431
00:16:52,480 --> 00:16:54,800
be utilized to reduce their own

432
00:16:54,800 --> 00:16:56,560
prediction error

433
00:16:56,560 --> 00:16:58,480
so that's kind of the bigger picture

434
00:16:58,480 --> 00:17:01,680
like the idea at the top level is like

435
00:17:01,680 --> 00:17:03,199
i'm tracking

436
00:17:03,199 --> 00:17:05,199
a soccer ball that's moving from the

437
00:17:05,199 --> 00:17:06,799
left to the right

438
00:17:06,799 --> 00:17:08,880
and then that gets transduced in certain

439
00:17:08,880 --> 00:17:10,799
ways and that is

440
00:17:10,799 --> 00:17:14,400
in this case resulting in kind of a

441
00:17:14,400 --> 00:17:17,119
visual perceptive level prediction about

442
00:17:17,119 --> 00:17:19,199
what one should expect to see

443
00:17:19,199 --> 00:17:21,199
and this is again just the multi-level

444
00:17:21,199 --> 00:17:24,240
predictive coding scheme that is often

445
00:17:24,240 --> 00:17:25,439
used

446
00:17:25,439 --> 00:17:26,559
but

447
00:17:26,559 --> 00:17:28,480
we're gonna zoom back to just what's

448
00:17:28,480 --> 00:17:30,720
happening at a single level and then

449
00:17:30,720 --> 00:17:34,480
what is happening at multiple levels

450
00:17:36,960 --> 00:17:38,320
awesome

451
00:17:38,320 --> 00:17:41,919
here's a fun 2020 paper by er sorry 2018

452
00:17:41,919 --> 00:17:44,559
paper by carl fristen does predictive

453
00:17:44,559 --> 00:17:46,400
coding have a future

454
00:17:46,400 --> 00:17:48,080
and this is going to provide just a

455
00:17:48,080 --> 00:17:49,360
little bit of context and then we're

456
00:17:49,360 --> 00:17:51,520
going to jump more into though

457
00:17:51,520 --> 00:17:53,360
carl wrote in the 20th century we

458
00:17:53,360 --> 00:17:55,039
thought the brain extracted knowledge

459
00:17:55,039 --> 00:17:56,960
from sensation that's like the

460
00:17:56,960 --> 00:17:59,039
recognition model and that's sort of the

461
00:17:59,039 --> 00:18:02,640
incoming sensory processing model

462
00:18:02,640 --> 00:18:04,880
the 21st century witnessed a strange

463
00:18:04,880 --> 00:18:06,880
inversion in which the brain became an

464
00:18:06,880 --> 00:18:09,520
organ of inference actively constructing

465
00:18:09,520 --> 00:18:11,120
explanations for what's going on out

466
00:18:11,120 --> 00:18:14,240
there beyond its sensory epithelia one

467
00:18:14,240 --> 00:18:16,720
paper played a key role in this paradigm

468
00:18:16,720 --> 00:18:17,600
shift

469
00:18:17,600 --> 00:18:20,400
and carl goes on to write that a 1999

470
00:18:20,400 --> 00:18:22,400
paper by rao and ballard which we're

471
00:18:22,400 --> 00:18:25,120
going to talk more about for him was one

472
00:18:25,120 --> 00:18:27,039
of those papers that's a once in a

473
00:18:27,039 --> 00:18:29,280
decade find

474
00:18:29,280 --> 00:18:30,320
so

475
00:18:30,320 --> 00:18:31,840
we're going to talk more about it but

476
00:18:31,840 --> 00:18:34,480
this is just kind of connecting it early

477
00:18:34,480 --> 00:18:35,360
to

478
00:18:35,360 --> 00:18:37,520
carl firsten's line of research the

479
00:18:37,520 --> 00:18:39,360
connection between active inference and

480
00:18:39,360 --> 00:18:41,679
predictive coding and the idea that

481
00:18:41,679 --> 00:18:43,480
sometimes it's these early

482
00:18:43,480 --> 00:18:46,160
transdisciplinary papers that can stitch

483
00:18:46,160 --> 00:18:48,320
two different fields together and that

484
00:18:48,320 --> 00:18:51,280
that can have long-term consequences for

485
00:18:51,280 --> 00:18:54,240
essentially perennial questions like how

486
00:18:54,240 --> 00:18:57,039
do perception learning attention and

487
00:18:57,039 --> 00:19:00,080
sensation all work together

488
00:19:00,080 --> 00:19:02,080
and under what imperative might they

489
00:19:02,080 --> 00:19:04,720
work together

490
00:19:05,520 --> 00:19:07,919
and then the authors provide a very

491
00:19:07,919 --> 00:19:09,600
preliminary discussion that they're

492
00:19:09,600 --> 00:19:11,520
going to go into more later

493
00:19:11,520 --> 00:19:14,480
about how predictive coding matters for

494
00:19:14,480 --> 00:19:16,080
machine learning

495
00:19:16,080 --> 00:19:18,240
and so they write predictive coding

496
00:19:18,240 --> 00:19:20,080
proposes that using a simple

497
00:19:20,080 --> 00:19:22,960
unsupervised lost function

498
00:19:22,960 --> 00:19:24,880
such as simply attempting to predict

499
00:19:24,880 --> 00:19:27,440
incoming sensory data

500
00:19:27,440 --> 00:19:29,440
is sufficient to develop a complex

501
00:19:29,440 --> 00:19:31,200
general and hierarchically rich

502
00:19:31,200 --> 00:19:33,840
representations of the world

503
00:19:33,840 --> 00:19:35,280
they're suggesting that this has found

504
00:19:35,280 --> 00:19:37,840
support in the successes of modern

505
00:19:37,840 --> 00:19:39,919
machine learning models that are trained

506
00:19:39,919 --> 00:19:42,480
on unsupervised predictive or

507
00:19:42,480 --> 00:19:44,880
auto-regressive objectives

508
00:19:44,880 --> 00:19:46,559
so here's some of the papers that they

509
00:19:46,559 --> 00:19:48,720
cite this is the brown at all and the

510
00:19:48,720 --> 00:19:50,240
kaplan at all

511
00:19:50,240 --> 00:19:51,919
and these both have to do with the

512
00:19:51,919 --> 00:19:53,919
training of what are increasingly

513
00:19:53,919 --> 00:19:56,559
becoming important modern machine

514
00:19:56,559 --> 00:19:58,799
learning methods which are the language

515
00:19:58,799 --> 00:20:00,799
models

516
00:20:00,799 --> 00:20:03,039
in contrast to modern machine learning

517
00:20:03,039 --> 00:20:05,280
algorithms which are trained to end with

518
00:20:05,280 --> 00:20:08,080
a global loss at the output

519
00:20:08,080 --> 00:20:11,039
so given the total big data set give me

520
00:20:11,039 --> 00:20:13,840
the lowest total error on the whole data

521
00:20:13,840 --> 00:20:15,440
set

522
00:20:15,440 --> 00:20:17,520
in predictive coding prediction errors

523
00:20:17,520 --> 00:20:19,760
are computed at every layer which means

524
00:20:19,760 --> 00:20:22,159
that each layer only has to focus on

525
00:20:22,159 --> 00:20:25,600
minimizing local errors than global loss

526
00:20:25,600 --> 00:20:27,280
this property potentially enables

527
00:20:27,280 --> 00:20:28,960
predictive coding to learn in a

528
00:20:28,960 --> 00:20:31,600
biologically plausible way using only

529
00:20:31,600 --> 00:20:34,960
local and hebian learning rules and this

530
00:20:34,960 --> 00:20:36,559
is going to connect to calm and

531
00:20:36,559 --> 00:20:39,360
filtering and pid control

532
00:20:39,360 --> 00:20:40,640
so they're kind of

533
00:20:40,640 --> 00:20:43,039
laying out the land and showing where

534
00:20:43,039 --> 00:20:44,320
they're going to connect in the

535
00:20:44,320 --> 00:20:47,039
introduction very very qualitatively and

536
00:20:47,039 --> 00:20:49,280
then again we're gonna zoom back down to

537
00:20:49,280 --> 00:20:51,200
find out what's happening at one level

538
00:20:51,200 --> 00:20:53,440
of predictive coding and then try to

539
00:20:53,440 --> 00:20:56,240
rebuild some of the formalisms related

540
00:20:56,240 --> 00:20:59,600
to calm and filtering and pid control

541
00:20:59,600 --> 00:21:02,000
and the bigger questions that people

542
00:21:02,000 --> 00:21:03,919
might be curious about asking would be

543
00:21:03,919 --> 00:21:06,400
like what does it mean

544
00:21:06,400 --> 00:21:08,559
for computation to be biologically

545
00:21:08,559 --> 00:21:09,919
inspired

546
00:21:09,919 --> 00:21:12,000
what is possible when we're thinking

547
00:21:12,000 --> 00:21:13,760
about some of the similarities and

548
00:21:13,760 --> 00:21:15,760
differences and complementarities

549
00:21:15,760 --> 00:21:18,720
between biological and different kinds

550
00:21:18,720 --> 00:21:21,280
of conventional and unconventional

551
00:21:21,280 --> 00:21:23,918
computation

552
00:21:25,440 --> 00:21:27,039
the next

553
00:21:27,039 --> 00:21:28,000
part

554
00:21:28,000 --> 00:21:30,640
maria please take it away i'll be

555
00:21:30,640 --> 00:21:32,799
changing slides so thanks a lot for

556
00:21:32,799 --> 00:21:35,200
adding a lot of the historical context

557
00:21:35,200 --> 00:21:36,880
and looking forward to learning about

558
00:21:36,880 --> 00:21:39,840
what you have to share here

559
00:21:39,840 --> 00:21:41,039
okay

560
00:21:41,039 --> 00:21:44,799
so uh this is actually uh the paragraph

561
00:21:44,799 --> 00:21:46,880
that is in the paper

562
00:21:46,880 --> 00:21:48,640
okay that they

563
00:21:48,640 --> 00:21:50,240
start talking about the history of

564
00:21:50,240 --> 00:21:52,320
predictive coding so

565
00:21:52,320 --> 00:21:54,080
while predictive coding as a

566
00:21:54,080 --> 00:21:56,559
neuroscientific theory originated in the

567
00:21:56,559 --> 00:21:59,440
80s and 90s with montford rao and

568
00:21:59,440 --> 00:22:00,799
ballard

569
00:22:00,799 --> 00:22:02,320
uh sure

570
00:22:02,320 --> 00:22:04,000
nevison

571
00:22:04,000 --> 00:22:05,039
or something

572
00:22:05,039 --> 00:22:07,280
laughlin and dubs

573
00:22:07,280 --> 00:22:09,600
and was first developed into its modern

574
00:22:09,600 --> 00:22:11,600
mathematical form and comprehensive

575
00:22:11,600 --> 00:22:13,600
theory of cortical responses in the mid

576
00:22:13,600 --> 00:22:16,480
2000s with freestyle it has deep

577
00:22:16,480 --> 00:22:18,799
intellectual anticipates

578
00:22:18,799 --> 00:22:20,960
these precursors include helmholtz

579
00:22:20,960 --> 00:22:23,120
notion of perception as an unconscious

580
00:22:23,120 --> 00:22:25,840
inference and kant's notion that a

581
00:22:25,840 --> 00:22:28,960
priory structure is needed to make sense

582
00:22:28,960 --> 00:22:31,600
of sensory dictum okay

583
00:22:31,600 --> 00:22:34,720
and as well as early ideas of

584
00:22:34,720 --> 00:22:36,400
compression and feedback control in

585
00:22:36,400 --> 00:22:39,200
cybernetics and information theory

586
00:22:39,200 --> 00:22:42,720
and on the next slides i will be talking

587
00:22:42,720 --> 00:22:44,480
a little bit more about history and

588
00:22:44,480 --> 00:22:47,600
philosophy of predictive coding

589
00:22:47,600 --> 00:22:50,640
and then some core formalisms and

590
00:22:50,640 --> 00:22:53,120
various generalizations and elaborations

591
00:22:53,120 --> 00:22:56,239
of predictive coding

592
00:22:56,559 --> 00:22:58,240
if there is anything you want to say

593
00:22:58,240 --> 00:23:02,960
daniel just feel free to jump in okay

594
00:23:02,960 --> 00:23:04,159
so

595
00:23:04,159 --> 00:23:06,400
uh the historical and philosophical

596
00:23:06,400 --> 00:23:09,200
background of perception is huge okay

597
00:23:09,200 --> 00:23:11,360
however seeing in the context of

598
00:23:11,360 --> 00:23:13,520
predictive coding framework authors

599
00:23:13,520 --> 00:23:15,760
usually come up with a similar reasoning

600
00:23:15,760 --> 00:23:17,840
about perception

601
00:23:17,840 --> 00:23:19,280
and

602
00:23:19,280 --> 00:23:22,320
then we can start thinking uh with the

603
00:23:22,320 --> 00:23:25,280
greek philosopher plato on his allegory

604
00:23:25,280 --> 00:23:26,799
of the cave

605
00:23:26,799 --> 00:23:29,080
uh from

606
00:23:29,080 --> 00:23:33,200
514 to 520 before christ

607
00:23:33,200 --> 00:23:35,200
which he said

608
00:23:35,200 --> 00:23:37,039
basically that some people would be

609
00:23:37,039 --> 00:23:39,120
trapped in a cave forever watching

610
00:23:39,120 --> 00:23:41,600
shadows cast by objects moving near a

611
00:23:41,600 --> 00:23:43,200
fire behind them

612
00:23:43,200 --> 00:23:45,600
and then they would like be there

613
00:23:45,600 --> 00:23:47,600
forever have children that would grow

614
00:23:47,600 --> 00:23:48,720
like that

615
00:23:48,720 --> 00:23:52,080
inside the cave and you know eventually

616
00:23:52,080 --> 00:23:53,600
these shadows

617
00:23:53,600 --> 00:23:56,080
uh from the objects near fire behind

618
00:23:56,080 --> 00:23:57,679
them would be

619
00:23:57,679 --> 00:23:59,760
the real thing and the only thing for

620
00:23:59,760 --> 00:24:03,520
these people they wouldn't believe that

621
00:24:03,520 --> 00:24:07,120
there is anything outside the cave so

622
00:24:07,120 --> 00:24:09,840
this this would be it these shadows

623
00:24:09,840 --> 00:24:12,640
would be the only thing okay so what

624
00:24:12,640 --> 00:24:15,520
plato would claim here is that our own

625
00:24:15,520 --> 00:24:18,240
conscious perceptions are just like

626
00:24:18,240 --> 00:24:20,799
these shadows okay meaning that they are

627
00:24:20,799 --> 00:24:23,200
indirect reflections of hidden causes

628
00:24:23,200 --> 00:24:27,200
that we can never directly encounter

629
00:24:27,200 --> 00:24:28,559
and

630
00:24:28,559 --> 00:24:29,919
but

631
00:24:29,919 --> 00:24:32,960
a few centuries later the arab medieval

632
00:24:32,960 --> 00:24:36,080
scholar al al hasan

633
00:24:36,080 --> 00:24:38,559
wrote a lot of interesting notes on

634
00:24:38,559 --> 00:24:42,399
visual perception actually he wrote

635
00:24:43,200 --> 00:24:44,080
six

636
00:24:44,080 --> 00:24:47,279
volumes if i'm not mistaken in uh of his

637
00:24:47,279 --> 00:24:49,200
book of optics

638
00:24:49,200 --> 00:24:51,520
where he explored the view that human

639
00:24:51,520 --> 00:24:54,159
perception often depends on mechanisms

640
00:24:54,159 --> 00:24:57,120
of judgment and inference instead of you

641
00:24:57,120 --> 00:24:59,360
know providing access traits to the

642
00:24:59,360 --> 00:25:01,039
world so

643
00:25:01,039 --> 00:25:03,039
you can see that uh

644
00:25:03,039 --> 00:25:05,600
very early in the in our western

645
00:25:05,600 --> 00:25:08,480
thinking we have this

646
00:25:08,480 --> 00:25:11,360
notion that we do not have

647
00:25:11,360 --> 00:25:14,159
direct access to the world okay that

648
00:25:14,159 --> 00:25:16,080
there is some inference

649
00:25:16,080 --> 00:25:18,639
going on

650
00:25:19,679 --> 00:25:22,000
okay

651
00:25:23,600 --> 00:25:26,159
great all right so

652
00:25:26,159 --> 00:25:29,279
jumping to the 18th century

653
00:25:29,279 --> 00:25:32,159
we can find two important philosophers

654
00:25:32,159 --> 00:25:34,400
okay the english david hume and the

655
00:25:34,400 --> 00:25:36,720
german emmanuel cut

656
00:25:36,720 --> 00:25:40,559
him in 1739 to 1740

657
00:25:40,559 --> 00:25:43,360
export the inductive inferences and

658
00:25:43,360 --> 00:25:44,799
causation

659
00:25:44,799 --> 00:25:48,480
the problem of induction that he brought

660
00:25:48,480 --> 00:25:50,799
was an analysis of cause and effect and

661
00:25:50,799 --> 00:25:54,320
perception and the conclusion

662
00:25:54,320 --> 00:25:57,200
that he gets is that all our mental life

663
00:25:57,200 --> 00:26:00,080
could be traced back to the effects of

664
00:26:00,080 --> 00:26:02,720
sense experience because since we do not

665
00:26:02,720 --> 00:26:03,520
have

666
00:26:03,520 --> 00:26:05,919
direct contact with the world we could

667
00:26:05,919 --> 00:26:08,080
never have one-to-one relationship with

668
00:26:08,080 --> 00:26:10,960
the objects there and therefore

669
00:26:10,960 --> 00:26:13,520
it is possible to have one effect with

670
00:26:13,520 --> 00:26:15,600
many possible causes and one cause it

671
00:26:15,600 --> 00:26:16,480
with

672
00:26:16,480 --> 00:26:18,559
effects

673
00:26:18,559 --> 00:26:20,400
the solution here would be then

674
00:26:20,400 --> 00:26:23,360
extracting statistical regularities and

675
00:26:23,360 --> 00:26:26,400
imagining what happens when the world is

676
00:26:26,400 --> 00:26:29,440
intervened upon a controlled manner as

677
00:26:29,440 --> 00:26:31,840
the philosopher jacob howie developed

678
00:26:31,840 --> 00:26:33,840
developed in his book the predictive

679
00:26:33,840 --> 00:26:36,880
mind in 2014

680
00:26:36,880 --> 00:26:38,720
equally important

681
00:26:38,720 --> 00:26:39,520
uh

682
00:26:39,520 --> 00:26:42,960
in the critique of pure reason in 1781

683
00:26:42,960 --> 00:26:43,919
kant

684
00:26:43,919 --> 00:26:46,240
will add an important element to this

685
00:26:46,240 --> 00:26:48,880
story among you know countless other

686
00:26:48,880 --> 00:26:51,600
elements to the story he will add that

687
00:26:51,600 --> 00:26:53,279
the brain uses

688
00:26:53,279 --> 00:26:55,840
existing information regarding space and

689
00:26:55,840 --> 00:26:58,799
time to make sense of the chaotic

690
00:26:58,799 --> 00:27:01,919
sensory data it constantly receives

691
00:27:01,919 --> 00:27:04,400
to provide an organism the perception as

692
00:27:04,400 --> 00:27:05,600
they know

693
00:27:05,600 --> 00:27:06,720
so

694
00:27:06,720 --> 00:27:08,640
uh all of these

695
00:27:08,640 --> 00:27:11,440
scholars would tell us

696
00:27:11,440 --> 00:27:13,840
right in the early ages that

697
00:27:13,840 --> 00:27:16,640
we do not have direct access to the

698
00:27:16,640 --> 00:27:18,159
things in the world

699
00:27:18,159 --> 00:27:21,200
and we need pre-existing information to

700
00:27:21,200 --> 00:27:24,159
make sense of all the data we constantly

701
00:27:24,159 --> 00:27:25,679
receive

702
00:27:25,679 --> 00:27:29,679
lots of predictive coding stuff

703
00:27:30,559 --> 00:27:32,559
it really makes me wonder why it's

704
00:27:32,559 --> 00:27:34,799
sometimes still so

705
00:27:34,799 --> 00:27:37,760
ineffable or poorly understood or even

706
00:27:37,760 --> 00:27:40,240
seen as controversial

707
00:27:40,240 --> 00:27:41,840
some of these perspectives which have

708
00:27:41,840 --> 00:27:42,230
been

709
00:27:42,230 --> 00:27:43,679
[Music]

710
00:27:43,679 --> 00:27:44,880
as you

711
00:27:44,880 --> 00:27:46,720
are describing like

712
00:27:46,720 --> 00:27:49,200
thought of and convergently come to for

713
00:27:49,200 --> 00:27:50,880
many thousands of years in different

714
00:27:50,880 --> 00:27:54,399
cultures so really interesting

715
00:27:55,760 --> 00:27:57,520
yes

716
00:27:57,520 --> 00:27:58,720
and

717
00:27:58,720 --> 00:28:01,760
well in the late 19th century the german

718
00:28:01,760 --> 00:28:04,960
scientist hermann from helmholtz in 1860

719
00:28:04,960 --> 00:28:07,279
depicted perception as

720
00:28:07,279 --> 00:28:09,840
uh involving probabilistic influence

721
00:28:09,840 --> 00:28:12,720
yeah he he is one of our gods

722
00:28:12,720 --> 00:28:14,880
and communities

723
00:28:14,880 --> 00:28:19,200
but and he was really inspired by kant

724
00:28:19,200 --> 00:28:20,159
okay

725
00:28:20,159 --> 00:28:20,960
uh

726
00:28:20,960 --> 00:28:21,840
and

727
00:28:21,840 --> 00:28:23,919
with that inspiration he developed the

728
00:28:23,919 --> 00:28:27,039
idea of the brain as a hypothesis tester

729
00:28:27,039 --> 00:28:28,960
and that perception is a process of

730
00:28:28,960 --> 00:28:31,840
unconscious inference okay

731
00:28:31,840 --> 00:28:34,880
and more specifically this idea that he

732
00:28:34,880 --> 00:28:36,159
developed

733
00:28:36,159 --> 00:28:38,720
in experiments was that perception has

734
00:28:38,720 --> 00:28:40,720
to be inferred by combining sensory

735
00:28:40,720 --> 00:28:43,200
signals with the brain expectations and

736
00:28:43,200 --> 00:28:45,520
beliefs about their causes

737
00:28:45,520 --> 00:28:48,240
and such inferences happen

738
00:28:48,240 --> 00:28:50,799
without the subject's awareness i mean

739
00:28:50,799 --> 00:28:53,039
it has to happen without the subject's

740
00:28:53,039 --> 00:28:54,640
awareness because

741
00:28:54,640 --> 00:28:57,200
you know imagine all the chaotic sensory

742
00:28:57,200 --> 00:28:58,399
data coming

743
00:28:58,399 --> 00:29:02,240
and you open your eyes and you have to

744
00:29:02,240 --> 00:29:04,080
try to organize everything i mean it

745
00:29:04,080 --> 00:29:06,000
doesn't happen that way you just open

746
00:29:06,000 --> 00:29:07,520
your eyes and

747
00:29:07,520 --> 00:29:10,000
everything is still stable

748
00:29:10,000 --> 00:29:14,720
and so uh it's unconscious and

749
00:29:14,720 --> 00:29:16,880
it needs to keep track of the causes in

750
00:29:16,880 --> 00:29:19,600
the world by updating perceptual best

751
00:29:19,600 --> 00:29:23,039
guesses or you can say hypothesis as new

752
00:29:23,039 --> 00:29:25,120
sensory signals arrive

753
00:29:25,120 --> 00:29:27,919
so here he built

754
00:29:27,919 --> 00:29:28,880
uh

755
00:29:28,880 --> 00:29:30,320
the idea

756
00:29:30,320 --> 00:29:33,440
that we use combini we use sensory

757
00:29:33,440 --> 00:29:34,559
signals

758
00:29:34,559 --> 00:29:37,440
with the brain expectations to

759
00:29:37,440 --> 00:29:39,520
you know update

760
00:29:39,520 --> 00:29:43,039
the hypothesis we make upon we make for

761
00:29:43,039 --> 00:29:44,399
the causes

762
00:29:44,399 --> 00:29:47,039
of the effects we receive in the world

763
00:29:47,039 --> 00:29:49,840
and this means that he somehow turned

764
00:29:49,840 --> 00:29:52,960
humes and inside scientifical

765
00:29:52,960 --> 00:29:57,039
claiming that we can only infer things

766
00:29:57,039 --> 00:29:59,679
out there in a word behind a sensory

767
00:29:59,679 --> 00:30:02,640
veil which if i'm not mistaken will be

768
00:30:02,640 --> 00:30:05,520
eventually called mark of blackness

769
00:30:05,520 --> 00:30:07,120
is that right then

770
00:30:07,120 --> 00:30:09,600
not sure uh

771
00:30:09,600 --> 00:30:12,159
maybe unveiled

772
00:30:12,159 --> 00:30:14,480
how hot was crucial to many different

773
00:30:14,480 --> 00:30:16,880
works in machine learning one that was

774
00:30:16,880 --> 00:30:19,840
even nicely named as helmholtz machine

775
00:30:19,840 --> 00:30:21,520
he was crucial in psychology

776
00:30:21,520 --> 00:30:24,720
neuroscience cognitive science and so on

777
00:30:24,720 --> 00:30:27,200
particularly in psychology helmholtz

778
00:30:27,200 --> 00:30:31,760
inside inspired yuri schneizer in 1967

779
00:30:31,760 --> 00:30:34,960
and later richard gregory in 1980 and

780
00:30:34,960 --> 00:30:38,799
eve arvind rock in 1983 to develop the

781
00:30:38,799 --> 00:30:41,360
analysis by synthesis approach which is

782
00:30:41,360 --> 00:30:44,080
the process that analyze a signal or

783
00:30:44,080 --> 00:30:46,000
image by reproducing

784
00:30:46,000 --> 00:30:49,279
it using a model

785
00:30:49,360 --> 00:30:51,520
there there's a picture of nicest

786
00:30:51,520 --> 00:30:54,320
analysis based into this approach

787
00:30:54,320 --> 00:30:58,799
some schema available in this link

788
00:30:58,799 --> 00:31:00,240
and in fact

789
00:31:00,240 --> 00:31:02,880
gregory built some kind of neural

790
00:31:02,880 --> 00:31:05,200
hypothesis testing model based on his

791
00:31:05,200 --> 00:31:07,519
how hot c and understanding of the brain

792
00:31:07,519 --> 00:31:09,519
as continually formulating perceptual

793
00:31:09,519 --> 00:31:12,080
hypothesis about the word and testing

794
00:31:12,080 --> 00:31:14,640
them by acquiring data

795
00:31:14,640 --> 00:31:18,600
from sensory organs

796
00:31:21,039 --> 00:31:24,880
okay so many things happened after the

797
00:31:24,880 --> 00:31:27,360
analysis by synthesis stuff but

798
00:31:27,360 --> 00:31:29,200
i will just jump to what we have in

799
00:31:29,200 --> 00:31:30,960
philosophy today because

800
00:31:30,960 --> 00:31:34,080
we will touch the formalisms and the

801
00:31:34,080 --> 00:31:37,519
computation stuff afterwards okay so i

802
00:31:37,519 --> 00:31:40,000
will just make a jump and go to what we

803
00:31:40,000 --> 00:31:42,399
have in philosophy today

804
00:31:42,399 --> 00:31:43,440
so

805
00:31:43,440 --> 00:31:45,200
uh

806
00:31:45,200 --> 00:31:47,120
recent works in philosophy and cognitive

807
00:31:47,120 --> 00:31:49,039
neuroscience have contributed to the

808
00:31:49,039 --> 00:31:51,200
expansive predictive coding framework

809
00:31:51,200 --> 00:31:54,159
and the beijing paradigm in the brain

810
00:31:54,159 --> 00:31:56,559
and many of the seminal works we have in

811
00:31:56,559 --> 00:31:59,440
the field addresses predictive coding as

812
00:31:59,440 --> 00:32:01,679
predictive processing

813
00:32:01,679 --> 00:32:02,799
without

814
00:32:02,799 --> 00:32:05,440
you know any relevant difference in the

815
00:32:05,440 --> 00:32:07,039
first

816
00:32:07,039 --> 00:32:08,799
instance

817
00:32:08,799 --> 00:32:09,919
and

818
00:32:09,919 --> 00:32:11,120
well

819
00:32:11,120 --> 00:32:13,519
for instance the philosophy jacob howie

820
00:32:13,519 --> 00:32:15,600
that is based in monash university of

821
00:32:15,600 --> 00:32:16,720
australia

822
00:32:16,720 --> 00:32:19,760
aimed to the amazon improvement of

823
00:32:19,760 --> 00:32:22,640
predictionary minimization mechanism and

824
00:32:22,640 --> 00:32:25,120
the concept of winning hypothesis for

825
00:32:25,120 --> 00:32:27,200
conscious perception

826
00:32:27,200 --> 00:32:29,120
also he is interested in what pp

827
00:32:29,120 --> 00:32:31,120
predictive processing studies can bring

828
00:32:31,120 --> 00:32:33,120
to people with mental disorders such as

829
00:32:33,120 --> 00:32:36,000
autism and schizophrenia actually the

830
00:32:36,000 --> 00:32:37,440
three of them

831
00:32:37,440 --> 00:32:40,080
of the philosophers here are interested

832
00:32:40,080 --> 00:32:42,480
in what people can bring to to

833
00:32:42,480 --> 00:32:45,760
improvement in psychiatry and medicine

834
00:32:45,760 --> 00:32:49,919
uh in general so in 2014 he published an

835
00:32:49,919 --> 00:32:52,080
important textbook on the matter that is

836
00:32:52,080 --> 00:32:54,880
the predictive mind the red book you can

837
00:32:54,880 --> 00:32:55,840
see

838
00:32:55,840 --> 00:32:57,840
where he tries to solve the problem of

839
00:32:57,840 --> 00:32:59,120
perception

840
00:32:59,120 --> 00:33:01,840
inspired on humans induction problem

841
00:33:01,840 --> 00:33:03,760
with bayesian inference and predictive

842
00:33:03,760 --> 00:33:06,000
processing

843
00:33:06,000 --> 00:33:10,000
later in 2016 the philosopher andy clark

844
00:33:10,000 --> 00:33:12,720
based on university of sussex england

845
00:33:12,720 --> 00:33:15,000
carefully developed what he called

846
00:33:15,000 --> 00:33:17,760
action-oriented predictive processing in

847
00:33:17,760 --> 00:33:20,399
his book surfing uncertainty where he

848
00:33:20,399 --> 00:33:22,559
adds and advances the crucial role of

849
00:33:22,559 --> 00:33:25,120
action in our perception

850
00:33:25,120 --> 00:33:26,799
and he makes a lot of interesting

851
00:33:26,799 --> 00:33:28,640
connections between recent works in

852
00:33:28,640 --> 00:33:30,159
computational neuroscience and

853
00:33:30,159 --> 00:33:32,000
artificial intelligence

854
00:33:32,000 --> 00:33:35,760
and his eclectic philosophy of might

855
00:33:35,760 --> 00:33:39,519
and last year in 2021 the neuroscientist

856
00:33:39,519 --> 00:33:42,399
and usf one of the authors of this paper

857
00:33:42,399 --> 00:33:43,840
we were talking here

858
00:33:43,840 --> 00:33:47,039
uh and also by the university of sussex

859
00:33:47,039 --> 00:33:49,279
made an amazing contribution to the

860
00:33:49,279 --> 00:33:52,000
fields with his book being you where he

861
00:33:52,000 --> 00:33:54,399
claims that we are beast machines whose

862
00:33:54,399 --> 00:33:58,240
perception is a controlled hallucination

863
00:33:58,240 --> 00:33:59,760
and he improves consciousness

864
00:33:59,760 --> 00:34:01,840
descriptions using predictive processing

865
00:34:01,840 --> 00:34:04,159
framework and establishing the real

866
00:34:04,159 --> 00:34:07,039
problem of consciousness that in his own

867
00:34:07,039 --> 00:34:09,280
words requires explaining why a

868
00:34:09,280 --> 00:34:12,000
particular pattern of brain activity or

869
00:34:12,000 --> 00:34:13,679
other physical process

870
00:34:13,679 --> 00:34:15,599
maps to a particular kind of conscious

871
00:34:15,599 --> 00:34:18,159
experience not merely establishing that

872
00:34:18,159 --> 00:34:19,520
it's this

873
00:34:19,520 --> 00:34:21,520
two you know philosophers and scientists

874
00:34:21,520 --> 00:34:23,359
work on

875
00:34:23,359 --> 00:34:25,440
and

876
00:34:25,440 --> 00:34:26,159
they

877
00:34:26,159 --> 00:34:28,720
here they are

878
00:34:32,839 --> 00:34:36,480
okay right so

879
00:34:36,480 --> 00:34:37,280
i

880
00:34:37,280 --> 00:34:39,199
was talking about predictive coding and

881
00:34:39,199 --> 00:34:41,520
then i went to predictive processing is

882
00:34:41,520 --> 00:34:43,918
there a difference

883
00:34:43,918 --> 00:34:45,918
well many authors

884
00:34:45,918 --> 00:34:49,839
would use the the terms freely okay but

885
00:34:49,839 --> 00:34:52,800
then i found clark saying in his book

886
00:34:52,800 --> 00:34:56,000
uh the difference that he understands

887
00:34:56,000 --> 00:34:57,839
from predictive coding to predictive

888
00:34:57,839 --> 00:35:00,480
processing so he puts

889
00:35:00,480 --> 00:35:03,359
as the fact that predictive processing

890
00:35:03,359 --> 00:35:06,160
is not simply the use of the data

891
00:35:06,160 --> 00:35:08,320
compression strategy known as predictive

892
00:35:08,320 --> 00:35:11,680
coding rather it is the use of that

893
00:35:11,680 --> 00:35:14,839
strategy in the very special context of

894
00:35:14,839 --> 00:35:17,200
hierarchical multi-level

895
00:35:17,200 --> 00:35:19,119
systems deploying probabilistic

896
00:35:19,119 --> 00:35:20,960
generative models

897
00:35:20,960 --> 00:35:23,599
such systems exhibit powerful forms of

898
00:35:23,599 --> 00:35:26,000
learning and deliver rich forms of

899
00:35:26,000 --> 00:35:28,839
context sensitive processing and are

900
00:35:28,839 --> 00:35:32,320
able uh flexibly to combine top-down and

901
00:35:32,320 --> 00:35:34,480
bottom-up flows of information within

902
00:35:34,480 --> 00:35:36,800
multi-layer cascade

903
00:35:36,800 --> 00:35:38,720
predictive processing does combine the

904
00:35:38,720 --> 00:35:41,839
use within a multi-level by

905
00:35:41,839 --> 00:35:43,839
bi-directional cascade of top-down

906
00:35:43,839 --> 00:35:46,079
probabilistic general models

907
00:35:46,079 --> 00:35:49,040
with the core predictive coding strategy

908
00:35:49,040 --> 00:35:54,079
of efficient encoding and transmission

909
00:35:55,040 --> 00:35:57,279
okay

910
00:35:57,359 --> 00:35:58,720
from this

911
00:35:58,720 --> 00:36:00,880
i'm not sure if you agree with me daniel

912
00:36:00,880 --> 00:36:02,640
but

913
00:36:02,640 --> 00:36:04,160
reading this

914
00:36:04,160 --> 00:36:07,040
this paragraph makes me wonder i mean

915
00:36:07,040 --> 00:36:10,480
okay predictive coding is a local uh

916
00:36:10,480 --> 00:36:14,240
bottom up top down cascade and stuff but

917
00:36:14,240 --> 00:36:16,560
it also is

918
00:36:16,560 --> 00:36:18,079
multi-layer

919
00:36:18,079 --> 00:36:22,400
yes it also has some hierarchy so

920
00:36:22,400 --> 00:36:24,800
i'm not sure what what he mean i mean

921
00:36:24,800 --> 00:36:26,960
i'm not sure if i understand

922
00:36:26,960 --> 00:36:28,880
the difference between predictive coding

923
00:36:28,880 --> 00:36:33,119
and predictive processing as he puts

924
00:36:33,440 --> 00:36:36,079
yeah very insightful

925
00:36:36,079 --> 00:36:37,760
and it made me think about the

926
00:36:37,760 --> 00:36:40,560
difference between like coding like

927
00:36:40,560 --> 00:36:43,599
programming and like data coding like

928
00:36:43,599 --> 00:36:45,920
python coding for data and data

929
00:36:45,920 --> 00:36:47,920
processing which is a little bit like it

930
00:36:47,920 --> 00:36:50,400
could be multiple computer languages and

931
00:36:50,400 --> 00:36:52,560
so you're really highlighting something

932
00:36:52,560 --> 00:36:55,280
important which is that when the data

933
00:36:55,280 --> 00:36:58,400
are compressed according to a strategy

934
00:36:58,400 --> 00:37:00,880
where the errors are being transmitted

935
00:37:00,880 --> 00:37:03,119
rather than the actual sort of estimate

936
00:37:03,119 --> 00:37:05,760
itself that's like that information

937
00:37:05,760 --> 00:37:09,440
channel is in a predictive coding way

938
00:37:09,440 --> 00:37:12,240
and then this helps us

939
00:37:12,240 --> 00:37:14,320
use predictive processing

940
00:37:14,320 --> 00:37:16,480
for like the bigger picture for systems

941
00:37:16,480 --> 00:37:19,040
that implement predictive coding modules

942
00:37:19,040 --> 00:37:20,800
but that's not like the only module

943
00:37:20,800 --> 00:37:22,800
they're deploying so that that'll be

944
00:37:22,800 --> 00:37:24,960
something cool to like hear people's

945
00:37:24,960 --> 00:37:26,720
perspective on really nice um

946
00:37:26,720 --> 00:37:29,680
distinction here though

947
00:37:30,000 --> 00:37:30,960
yep

948
00:37:30,960 --> 00:37:33,440
and

949
00:37:34,400 --> 00:37:39,920
okay so i wrote something here that

950
00:37:39,920 --> 00:37:42,560
just like was you said that maybe uh

951
00:37:42,560 --> 00:37:44,320
coding would be better to use for

952
00:37:44,320 --> 00:37:46,000
formalisms and implementations and

953
00:37:46,000 --> 00:37:47,440
processing for the philosophical

954
00:37:47,440 --> 00:37:49,520
understanding that prediction is the

955
00:37:49,520 --> 00:37:53,200
basis of a signal interpretation

956
00:37:53,200 --> 00:37:57,040
eg as opposed to descriptive

957
00:38:00,720 --> 00:38:03,200
okay

958
00:38:03,200 --> 00:38:04,720
so

959
00:38:04,720 --> 00:38:06,880
uh jumping back to

960
00:38:06,880 --> 00:38:09,040
formalism

961
00:38:09,040 --> 00:38:11,680
uh predictive coding and information

962
00:38:11,680 --> 00:38:14,720
theory and signal processing okay so

963
00:38:14,720 --> 00:38:17,280
another deep intellectual influence in

964
00:38:17,280 --> 00:38:20,240
predictive coding comes from information

965
00:38:20,240 --> 00:38:23,119
theory from channel in 1948

966
00:38:23,119 --> 00:38:25,599
and especially the minimum redundancy

967
00:38:25,599 --> 00:38:31,040
principle of barlow in 1961 89 and 61.

968
00:38:31,040 --> 00:38:33,040
two

969
00:38:33,040 --> 00:38:35,599
information theory tell us that

970
00:38:35,599 --> 00:38:38,720
information is inseparable from a lack

971
00:38:38,720 --> 00:38:40,880
of predictability

972
00:38:40,880 --> 00:38:43,280
if something is predictable before

973
00:38:43,280 --> 00:38:46,480
observing it it cannot give us much

974
00:38:46,480 --> 00:38:50,720
information i loved this sentence

975
00:38:51,520 --> 00:38:54,240
conversely to maximize the rate of

976
00:38:54,240 --> 00:38:56,880
information transfer the message must be

977
00:38:56,880 --> 00:38:59,520
minimally predictable and enhance

978
00:38:59,520 --> 00:39:01,119
minimally redundant

979
00:39:01,119 --> 00:39:03,760
predictive coding as a means to remove

980
00:39:03,760 --> 00:39:06,320
redundancy in a signal was first applied

981
00:39:06,320 --> 00:39:08,240
in signal processing

982
00:39:08,240 --> 00:39:10,880
where it was used to reduce transmission

983
00:39:10,880 --> 00:39:12,320
bandwidth

984
00:39:12,320 --> 00:39:15,839
for video transmission

985
00:39:16,079 --> 00:39:16,960
and

986
00:39:16,960 --> 00:39:18,960
do you want to say something i'll just

987
00:39:18,960 --> 00:39:20,960
describe this without reading it it was

988
00:39:20,960 --> 00:39:24,800
a a common uh and effective although it

989
00:39:24,800 --> 00:39:27,280
can be improved on by other methods

990
00:39:27,280 --> 00:39:29,680
it's a method for video encoding known

991
00:39:29,680 --> 00:39:33,040
as frame differencing where basically uh

992
00:39:33,040 --> 00:39:35,520
one frame gets described and then

993
00:39:35,520 --> 00:39:37,839
subsequent frames only the pixels that

994
00:39:37,839 --> 00:39:39,920
change have to be described

995
00:39:39,920 --> 00:39:42,640
so that is a lot like an analogy to

996
00:39:42,640 --> 00:39:44,240
predictive coding

997
00:39:44,240 --> 00:39:45,599
in the sense that

998
00:39:45,599 --> 00:39:47,280
it's able to connect the differences

999
00:39:47,280 --> 00:39:49,680
between subsequent time steps and use

1000
00:39:49,680 --> 00:39:52,400
that to give really only the informative

1001
00:39:52,400 --> 00:39:55,040
pieces like i have a 100 digit number

1002
00:39:55,040 --> 00:39:56,560
and then i'm just going to tell you the

1003
00:39:56,560 --> 00:39:59,200
third digit changed now it's an 8

1004
00:39:59,200 --> 00:40:01,119
instead of having to repeat the same

1005
00:40:01,119 --> 00:40:04,079
number and then 99 of those digits

1006
00:40:04,079 --> 00:40:05,760
they're informative like in the sense

1007
00:40:05,760 --> 00:40:08,079
that they're good info it's true

1008
00:40:08,079 --> 00:40:09,760
but they're not informative in the

1009
00:40:09,760 --> 00:40:12,480
information theory context because they

1010
00:40:12,480 --> 00:40:14,720
were quite predictable before observing

1011
00:40:14,720 --> 00:40:16,879
it

1012
00:40:18,079 --> 00:40:19,119
okay

1013
00:40:19,119 --> 00:40:22,800
and so initial schemes used a simple

1014
00:40:22,800 --> 00:40:25,200
approach of subtracting the new to be

1015
00:40:25,200 --> 00:40:28,319
transmitted frame from the old frame in

1016
00:40:28,319 --> 00:40:30,480
effect using a trivial prediction that

1017
00:40:30,480 --> 00:40:32,960
the new frame is always the same as the

1018
00:40:32,960 --> 00:40:35,440
old frame which works well in reducing

1019
00:40:35,440 --> 00:40:38,160
bandwidth in many settings where there

1020
00:40:38,160 --> 00:40:41,040
are only few objects moving in the video

1021
00:40:41,040 --> 00:40:42,240
against this

1022
00:40:42,240 --> 00:40:44,079
static background

1023
00:40:44,079 --> 00:40:46,800
and more advanced methods often predict

1024
00:40:46,800 --> 00:40:49,599
each new frame using a number of past

1025
00:40:49,599 --> 00:40:52,800
frames weighted by some coefficient

1026
00:40:52,800 --> 00:40:55,040
an approach known as linear predictive

1027
00:40:55,040 --> 00:40:57,440
coding

1028
00:41:03,680 --> 00:41:05,200
okay

1029
00:41:05,200 --> 00:41:07,839
so now

1030
00:41:08,319 --> 00:41:10,960
predictive coding in the eye

1031
00:41:10,960 --> 00:41:12,800
and the brain

1032
00:41:12,800 --> 00:41:14,880
the first concrete discussion of

1033
00:41:14,880 --> 00:41:16,960
predictive coding in the neurosystem

1034
00:41:16,960 --> 00:41:19,280
arose as a model of neural properties of

1035
00:41:19,280 --> 00:41:20,640
the retina

1036
00:41:20,640 --> 00:41:21,800
with

1037
00:41:21,800 --> 00:41:25,359
srinivasin 1982.

1038
00:41:25,359 --> 00:41:27,520
specifically as a model of

1039
00:41:27,520 --> 00:41:30,079
center surrounded cells

1040
00:41:30,079 --> 00:41:32,960
which fire when presented with either a

1041
00:41:32,960 --> 00:41:35,440
light spot against a dark background on

1042
00:41:35,440 --> 00:41:38,560
center of surround or alternatively a

1043
00:41:38,560 --> 00:41:41,520
dark spot against a light background of

1044
00:41:41,520 --> 00:41:43,760
center on surround cells

1045
00:41:43,760 --> 00:41:47,040
it was argued that this coding scheme

1046
00:41:47,040 --> 00:41:49,200
helps to minimize redundancy in the

1047
00:41:49,200 --> 00:41:51,440
visual scheme specifically by removing

1048
00:41:51,440 --> 00:41:53,920
the spatial redundancy in natural visual

1049
00:41:53,920 --> 00:41:55,440
scenes

1050
00:41:55,440 --> 00:41:57,839
that the intensity of

1051
00:41:57,839 --> 00:42:00,880
one pixel helps predict quite well the

1052
00:42:00,880 --> 00:42:04,960
intensity of neighboring pixels

1053
00:42:05,040 --> 00:42:09,759
and then the first picture related to it

1054
00:42:10,240 --> 00:42:14,560
uh do you want to talk about it or just

1055
00:42:14,560 --> 00:42:16,000
um we're just going to look at two

1056
00:42:16,000 --> 00:42:18,880
different tissues of the nervous system

1057
00:42:18,880 --> 00:42:21,599
that have some predictive elements

1058
00:42:21,599 --> 00:42:23,680
and this is referring to some of the

1059
00:42:23,680 --> 00:42:26,240
photosensitive cells that are on the

1060
00:42:26,240 --> 00:42:28,880
retina and that's work that had been

1061
00:42:28,880 --> 00:42:29,680
done

1062
00:42:29,680 --> 00:42:31,520
qualitatively and empirically for

1063
00:42:31,520 --> 00:42:33,839
hundreds of years but then was connected

1064
00:42:33,839 --> 00:42:36,880
to predictive perspectives in the 80s

1065
00:42:36,880 --> 00:42:38,319
and then we're also going to look as

1066
00:42:38,319 --> 00:42:41,359
you'll now unpack at what's happening in

1067
00:42:41,359 --> 00:42:43,760
the cortex of the brain amongst

1068
00:42:43,760 --> 00:42:46,480
different layers

1069
00:42:47,280 --> 00:42:50,880
with month fruit in 1992 was perhaps the

1070
00:42:50,880 --> 00:42:53,119
first to extend this theory of the

1071
00:42:53,119 --> 00:42:55,359
retina and the

1072
00:42:55,359 --> 00:42:58,800
lgn to a fully fledged general theory of

1073
00:42:58,800 --> 00:43:00,400
cortical function

1074
00:43:00,400 --> 00:43:02,720
his theory was motivated by simple

1075
00:43:02,720 --> 00:43:05,200
observations about the neurophysiology

1076
00:43:05,200 --> 00:43:08,160
of cortical cortical connections

1077
00:43:08,160 --> 00:43:10,480
specifically the existence of separate

1078
00:43:10,480 --> 00:43:13,359
feet forward and feed back paths where

1079
00:43:13,359 --> 00:43:15,760
the feet forward path is originated in

1080
00:43:15,760 --> 00:43:18,319
the superficial layers of the cortex and

1081
00:43:18,319 --> 00:43:20,240
the feedback pathways originated

1082
00:43:20,240 --> 00:43:22,079
primarily primarily

1083
00:43:22,079 --> 00:43:25,200
in the deep layers

1084
00:43:25,359 --> 00:43:27,520
great so we kind of had this philosophy

1085
00:43:27,520 --> 00:43:29,440
development happening over thousands of

1086
00:43:29,440 --> 00:43:30,400
years

1087
00:43:30,400 --> 00:43:33,520
and then in the 1900s people were

1088
00:43:33,520 --> 00:43:36,079
starting to connect the neuroanatomy and

1089
00:43:36,079 --> 00:43:37,680
the histology

1090
00:43:37,680 --> 00:43:39,760
to some of these predictive ideas and

1091
00:43:39,760 --> 00:43:42,160
then now let's kind of close the loop

1092
00:43:42,160 --> 00:43:44,480
and bring it back to the critical work

1093
00:43:44,480 --> 00:43:46,640
that fristen highlighted in his 2018

1094
00:43:46,640 --> 00:43:48,960
paper

1095
00:43:50,079 --> 00:43:52,079
yep and how predictive coding in the

1096
00:43:52,079 --> 00:43:55,359
brain gets computational mathematical

1097
00:43:55,359 --> 00:43:57,920
while montford's theory contained most

1098
00:43:57,920 --> 00:44:00,480
aspects of classical predictive coding

1099
00:44:00,480 --> 00:44:02,560
theory in the cortex

1100
00:44:02,560 --> 00:44:06,880
it was not a compound accompanied by any

1101
00:44:06,880 --> 00:44:09,680
simulations or empirical work and so

1102
00:44:09,680 --> 00:44:11,599
it's potential as a framework for

1103
00:44:11,599 --> 00:44:13,280
understanding the cortex was not fully

1104
00:44:13,280 --> 00:44:14,720
appreciated

1105
00:44:14,720 --> 00:44:17,359
and so again the seminal work of raw and

1106
00:44:17,359 --> 00:44:19,440
ballard in 1999

1107
00:44:19,440 --> 00:44:23,359
had its impact precisely by doing this

1108
00:44:23,359 --> 00:44:25,920
and so they created a small predictive

1109
00:44:25,920 --> 00:44:27,839
coding network

1110
00:44:27,839 --> 00:44:30,160
according to the principles

1111
00:44:30,160 --> 00:44:32,240
i'm sorry

1112
00:44:32,240 --> 00:44:34,480
uh they created a small predictive

1113
00:44:34,480 --> 00:44:36,640
coding network according to the

1114
00:44:36,640 --> 00:44:38,960
principles identified by mumford and

1115
00:44:38,960 --> 00:44:41,520
empirically investigated its behavior

1116
00:44:41,520 --> 00:44:43,359
demonstrating that the complex and

1117
00:44:43,359 --> 00:44:46,160
dynamic interplay of predictions and

1118
00:44:46,160 --> 00:44:48,640
prediction errors could explain several

1119
00:44:48,640 --> 00:44:50,440
otherwise perplexing

1120
00:44:50,440 --> 00:44:52,000
neurophysiological phenomena

1121
00:44:52,000 --> 00:44:54,800
specifically extracurricular receptive

1122
00:44:54,800 --> 00:44:57,119
field effects such as and stopping

1123
00:44:57,119 --> 00:44:59,200
neurons

1124
00:44:59,200 --> 00:45:02,160
yeah this is pretty interesting because

1125
00:45:02,160 --> 00:45:04,480
even the idea that the neurons that are

1126
00:45:04,480 --> 00:45:06,800
active while a certain stimuli is being

1127
00:45:06,800 --> 00:45:08,720
presented that those are neurons that

1128
00:45:08,720 --> 00:45:10,400
are receptive

1129
00:45:10,400 --> 00:45:12,560
to that stimuli that's within the

1130
00:45:12,560 --> 00:45:15,599
paradigm of signal reception

1131
00:45:15,599 --> 00:45:17,680
and then it's like there's classical

1132
00:45:17,680 --> 00:45:19,520
signal reception you know the pitcher

1133
00:45:19,520 --> 00:45:21,200
throws the ball the catcher catches the

1134
00:45:21,200 --> 00:45:22,880
ball it's classic

1135
00:45:22,880 --> 00:45:25,040
but unfortunately or not there's all

1136
00:45:25,040 --> 00:45:28,079
these so-called extra classical effects

1137
00:45:28,079 --> 00:45:30,240
and so this paper proposed a simple

1138
00:45:30,240 --> 00:45:33,599
architecture that was able to encompass

1139
00:45:33,599 --> 00:45:35,920
the so-called classical as well as some

1140
00:45:35,920 --> 00:45:39,280
of the important extra classical

1141
00:45:39,280 --> 00:45:42,160
effects under this predictive processing

1142
00:45:42,160 --> 00:45:44,800
framework and so that really led to a

1143
00:45:44,800 --> 00:45:47,119
lot of developments in neuroscience in

1144
00:45:47,119 --> 00:45:49,920
the last 20 plus years

1145
00:45:49,920 --> 00:45:52,160
and that's exactly what we're going to

1146
00:45:52,160 --> 00:45:55,040
go into now so thanks for providing that

1147
00:45:55,040 --> 00:45:58,000
awesome context like it really

1148
00:45:58,000 --> 00:45:59,280
helps

1149
00:45:59,280 --> 00:46:01,119
i think situate some of the details that

1150
00:46:01,119 --> 00:46:03,680
we're about to look into

1151
00:46:03,680 --> 00:46:09,118
okay so let's get a little technical

1152
00:46:09,280 --> 00:46:10,960
what happened after

1153
00:46:10,960 --> 00:46:14,160
round ballard's 1999 synthesis so

1154
00:46:14,160 --> 00:46:16,079
remember that carl fristen said that

1155
00:46:16,079 --> 00:46:17,680
that was like one of those once in a

1156
00:46:17,680 --> 00:46:21,200
decade papers for him so how did he

1157
00:46:21,200 --> 00:46:23,520
change his action selection publication

1158
00:46:23,520 --> 00:46:26,480
policies after reading that paper well

1159
00:46:26,480 --> 00:46:30,480
anyone can guess but what fristen did

1160
00:46:30,480 --> 00:46:33,440
was that he cast the predictive coding

1161
00:46:33,440 --> 00:46:34,720
algorithm

1162
00:46:34,720 --> 00:46:37,760
as approximate bayesian inference

1163
00:46:37,760 --> 00:46:40,079
upon gaussian generative models

1164
00:46:40,079 --> 00:46:42,319
and this is going to be connected this

1165
00:46:42,319 --> 00:46:44,800
is all in the paper for people to read

1166
00:46:44,800 --> 00:46:46,560
but this connects

1167
00:46:46,560 --> 00:46:49,599
basically all of the previous themes

1168
00:46:49,599 --> 00:46:51,599
that we had been talking about

1169
00:46:51,599 --> 00:46:53,520
like the information theory minimum

1170
00:46:53,520 --> 00:46:56,480
redundancy and the helmholtzian idea of

1171
00:46:56,480 --> 00:46:59,839
perception as inference come together

1172
00:46:59,839 --> 00:47:02,480
in the bayesian perspective

1173
00:47:02,480 --> 00:47:04,720
on the predictive coding architecture by

1174
00:47:04,720 --> 00:47:06,319
rao and ballard

1175
00:47:06,319 --> 00:47:07,040
so

1176
00:47:07,040 --> 00:47:09,440
the authors write firstens approach

1177
00:47:09,440 --> 00:47:12,560
reformulates the mostly heuristic

1178
00:47:12,560 --> 00:47:14,480
round ballard model in the language of

1179
00:47:14,480 --> 00:47:16,480
variational bayesian inference which

1180
00:47:16,480 --> 00:47:18,640
we're going to look more at

1181
00:47:18,640 --> 00:47:20,319
and fristen showed that the energy

1182
00:47:20,319 --> 00:47:22,880
function in rao and ballard

1183
00:47:22,880 --> 00:47:24,880
can be understood as a variational free

1184
00:47:24,880 --> 00:47:27,359
energy of the kind that is minimized

1185
00:47:27,359 --> 00:47:29,599
through variational inference

1186
00:47:29,599 --> 00:47:31,520
so that really connected the dots

1187
00:47:31,520 --> 00:47:32,720
between

1188
00:47:32,720 --> 00:47:35,119
predictive coding architectures and the

1189
00:47:35,119 --> 00:47:37,520
empirical biological findings to all

1190
00:47:37,520 --> 00:47:39,440
this work on variational bayesian

1191
00:47:39,440 --> 00:47:40,480
inference

1192
00:47:40,480 --> 00:47:43,520
and those are the 2003 5 and 8 single

1193
00:47:43,520 --> 00:47:46,800
author papers by carl

1194
00:47:46,800 --> 00:47:48,319
so

1195
00:47:48,319 --> 00:47:50,400
what is variational

1196
00:47:50,400 --> 00:47:53,119
and bayesian and variational bayesian

1197
00:47:53,119 --> 00:47:54,960
approaches and we're just going to look

1198
00:47:54,960 --> 00:47:56,559
at the author's words and there's many

1199
00:47:56,559 --> 00:47:58,640
many awesome places to look

1200
00:47:58,640 --> 00:48:01,359
for educational materials and also check

1201
00:48:01,359 --> 00:48:04,079
out some of the live streams like 26 32

1202
00:48:04,079 --> 00:48:07,440
34 37 and 39

1203
00:48:07,440 --> 00:48:10,319
so to be brief about it the authors

1204
00:48:10,319 --> 00:48:11,599
write

1205
00:48:11,599 --> 00:48:14,880
variational inference approximates an

1206
00:48:14,880 --> 00:48:17,359
intractable inference problem

1207
00:48:17,359 --> 00:48:20,319
with a tractable optimization problem so

1208
00:48:20,319 --> 00:48:23,280
like a super hard problem to solve just

1209
00:48:23,280 --> 00:48:25,599
by thinking about it and then guessing

1210
00:48:25,599 --> 00:48:27,280
at the right answer

1211
00:48:27,280 --> 00:48:30,160
with a tractable optimization problem so

1212
00:48:30,160 --> 00:48:31,760
it's like 20 questions it's going to be

1213
00:48:31,760 --> 00:48:34,000
hard to guess on your first question but

1214
00:48:34,000 --> 00:48:36,079
if you take this iterative optimization

1215
00:48:36,079 --> 00:48:38,240
approach maybe there's a way to actually

1216
00:48:38,240 --> 00:48:40,319
resolve it so it's not exactly like that

1217
00:48:40,319 --> 00:48:42,079
but it's like kind of again flipping out

1218
00:48:42,079 --> 00:48:43,839
something that's hard to solve in one

1219
00:48:43,839 --> 00:48:46,160
shot with an approach

1220
00:48:46,160 --> 00:48:48,800
and this iterative gradient descent type

1221
00:48:48,800 --> 00:48:50,800
improvement that actually gets you to a

1222
00:48:50,800 --> 00:48:52,640
good solution

1223
00:48:52,640 --> 00:48:53,839
and so this is where we're going to

1224
00:48:53,839 --> 00:48:56,240
introduce some of the variables and the

1225
00:48:56,240 --> 00:48:59,040
letters that we're going to use

1226
00:48:59,040 --> 00:49:02,559
following a lot of perceptual work in

1227
00:49:02,559 --> 00:49:05,440
bayesian statistics we're going to use o

1228
00:49:05,440 --> 00:49:08,319
for observations which are like data

1229
00:49:08,319 --> 00:49:10,319
either the actual data that the sensor

1230
00:49:10,319 --> 00:49:13,119
provides us or generated data of the

1231
00:49:13,119 --> 00:49:14,800
kind that we would expect the sensor to

1232
00:49:14,800 --> 00:49:17,359
give us and then x

1233
00:49:17,359 --> 00:49:20,160
are the latent or the unobserved states

1234
00:49:20,160 --> 00:49:22,480
of the system so like x would be the

1235
00:49:22,480 --> 00:49:25,200
temperature in the room and o is what

1236
00:49:25,200 --> 00:49:27,839
the thermometer reading is

1237
00:49:27,839 --> 00:49:29,920
and that's sort of a bayesian approach

1238
00:49:29,920 --> 00:49:32,559
that helps us have a generative model of

1239
00:49:32,559 --> 00:49:35,280
the data generating process which is

1240
00:49:35,280 --> 00:49:37,280
also known as the joint distribution

1241
00:49:37,280 --> 00:49:40,000
because it is jointly over both the

1242
00:49:40,000 --> 00:49:44,640
observations and the latent states

1243
00:49:44,640 --> 00:49:46,880
where variational inference comes into

1244
00:49:46,880 --> 00:49:47,920
play

1245
00:49:47,920 --> 00:49:49,599
um is that

1246
00:49:49,599 --> 00:49:52,000
is hinted at in this paragraph on base

1247
00:49:52,000 --> 00:49:55,200
so in order to do exact base

1248
00:49:55,200 --> 00:49:57,280
one would have to

1249
00:49:57,280 --> 00:49:59,599
find this normalizing factor

1250
00:49:59,599 --> 00:50:01,599
however that can be intractable because

1251
00:50:01,599 --> 00:50:03,760
it requires basically integrating or

1252
00:50:03,760 --> 00:50:07,119
summing over all latent variable states

1253
00:50:07,119 --> 00:50:08,720
and that's not always

1254
00:50:08,720 --> 00:50:10,480
easy or known

1255
00:50:10,480 --> 00:50:12,000
so the approach of the variational

1256
00:50:12,000 --> 00:50:13,119
method

1257
00:50:13,119 --> 00:50:14,880
is aiming to

1258
00:50:14,880 --> 00:50:17,200
approximate the posterior

1259
00:50:17,200 --> 00:50:20,079
using an auxiliary posterior

1260
00:50:20,079 --> 00:50:22,800
using a different set of parameters

1261
00:50:22,800 --> 00:50:24,000
phi

1262
00:50:24,000 --> 00:50:25,440
so the q

1263
00:50:25,440 --> 00:50:27,359
is going to be reflecting like the

1264
00:50:27,359 --> 00:50:28,960
distribution the variational

1265
00:50:28,960 --> 00:50:31,119
distribution that we control

1266
00:50:31,119 --> 00:50:33,920
that is of like a family of functions

1267
00:50:33,920 --> 00:50:35,920
that is more

1268
00:50:35,920 --> 00:50:39,200
amenable to optimization so p could be

1269
00:50:39,200 --> 00:50:42,319
like a super messy function but q is

1270
00:50:42,319 --> 00:50:44,720
going to be constructed

1271
00:50:44,720 --> 00:50:47,520
by the modeler to be a lot simpler

1272
00:50:47,520 --> 00:50:49,599
and so it is going to have its own

1273
00:50:49,599 --> 00:50:52,400
parameters and those parameters phi

1274
00:50:52,400 --> 00:50:53,599
are mu

1275
00:50:53,599 --> 00:50:54,960
and sigma

1276
00:50:54,960 --> 00:50:55,839
and so

1277
00:50:55,839 --> 00:50:58,079
it's kind of like we would be interested

1278
00:50:58,079 --> 00:51:02,240
in the mean and the variance of the x

1279
00:51:02,240 --> 00:51:04,640
the actual temperature in the room given

1280
00:51:04,640 --> 00:51:06,880
the thermometer but what if it were too

1281
00:51:06,880 --> 00:51:08,960
hard to even get that

1282
00:51:08,960 --> 00:51:10,720
well what we might want to do would be

1283
00:51:10,720 --> 00:51:12,559
imagine that the temperature were

1284
00:51:12,559 --> 00:51:14,720
normally distributed like a gaussian

1285
00:51:14,720 --> 00:51:15,920
distribution

1286
00:51:15,920 --> 00:51:18,640
with this mean mu in the variance sigma

1287
00:51:18,640 --> 00:51:20,319
so even if the temperature in the room

1288
00:51:20,319 --> 00:51:22,720
weren't actually gaussian distributed

1289
00:51:22,720 --> 00:51:25,280
maybe it's approximately enough and so

1290
00:51:25,280 --> 00:51:27,440
the variational approach to the bayesian

1291
00:51:27,440 --> 00:51:30,720
method is to introduce this variational

1292
00:51:30,720 --> 00:51:32,640
distribution queue that's going to have

1293
00:51:32,640 --> 00:51:34,960
like a lot of good features moving

1294
00:51:34,960 --> 00:51:37,440
forward

1295
00:51:37,440 --> 00:51:41,040
here's how that cue comes into play

1296
00:51:41,040 --> 00:51:43,359
this equation one

1297
00:51:43,359 --> 00:51:45,839
is expressing using the variables that

1298
00:51:45,839 --> 00:51:47,359
we've seen

1299
00:51:47,359 --> 00:51:49,680
it's saying and also introducing this

1300
00:51:49,680 --> 00:51:52,000
divergence d so first just the

1301
00:51:52,000 --> 00:51:53,760
definition of d and then what this

1302
00:51:53,760 --> 00:51:55,760
formalism says

1303
00:51:55,760 --> 00:51:56,960
so d

1304
00:51:56,960 --> 00:52:00,640
of p and double line q so any double

1305
00:52:00,640 --> 00:52:02,880
line means between this and that d is a

1306
00:52:02,880 --> 00:52:04,480
function that measures the divergence

1307
00:52:04,480 --> 00:52:06,880
between two distributions so for example

1308
00:52:06,880 --> 00:52:08,400
p and q

1309
00:52:08,400 --> 00:52:11,200
and here the divergence is going to be

1310
00:52:11,200 --> 00:52:14,000
calculated as the kl divergence

1311
00:52:14,000 --> 00:52:16,960
although other divergences are possible

1312
00:52:16,960 --> 00:52:18,960
so in the divergence between the one

1313
00:52:18,960 --> 00:52:21,440
that we simplified and controlled q

1314
00:52:21,440 --> 00:52:24,240
and that intractable true posterior

1315
00:52:24,240 --> 00:52:26,160
if that divergence were minimized to

1316
00:52:26,160 --> 00:52:29,040
zero we would be like fitting

1317
00:52:29,040 --> 00:52:32,480
p as well as we could with q

1318
00:52:32,480 --> 00:52:34,559
and this is saying in one it's saying q

1319
00:52:34,559 --> 00:52:36,960
star the best q

1320
00:52:36,960 --> 00:52:40,079
of x the latent states given the data

1321
00:52:40,079 --> 00:52:42,079
and the variational parameters

1322
00:52:42,079 --> 00:52:44,079
so the best possible room temperature

1323
00:52:44,079 --> 00:52:46,319
prediction given thermometer readings

1324
00:52:46,319 --> 00:52:49,200
and variational parameters are

1325
00:52:49,200 --> 00:52:50,960
a minimization

1326
00:52:50,960 --> 00:52:51,920
over

1327
00:52:51,920 --> 00:52:54,480
all the variational parameters

1328
00:52:54,480 --> 00:52:55,440
of

1329
00:52:55,440 --> 00:52:57,760
the divergence between

1330
00:52:57,760 --> 00:52:59,680
the q distribution that we control of

1331
00:52:59,680 --> 00:53:02,400
exactly what we want the best answer for

1332
00:53:02,400 --> 00:53:05,440
and the p generative model of exactly

1333
00:53:05,440 --> 00:53:07,839
what would be the best to compute like

1334
00:53:07,839 --> 00:53:09,839
the temperature given the observations

1335
00:53:09,839 --> 00:53:11,920
on the thermometer

1336
00:53:11,920 --> 00:53:13,839
and so that goes a long way towards

1337
00:53:13,839 --> 00:53:15,760
rewriting an inference problem as a

1338
00:53:15,760 --> 00:53:17,280
diversions problem

1339
00:53:17,280 --> 00:53:18,559
but they write

1340
00:53:18,559 --> 00:53:20,400
however merely writing the problem this

1341
00:53:20,400 --> 00:53:22,480
way does not solve it because the

1342
00:53:22,480 --> 00:53:24,640
divergence that we need to optimize

1343
00:53:24,640 --> 00:53:26,559
still contains the intractable true

1344
00:53:26,559 --> 00:53:28,880
posterior so it's rewritten as a

1345
00:53:28,880 --> 00:53:30,400
divergence between like something we

1346
00:53:30,400 --> 00:53:32,640
control and something that we kind of

1347
00:53:32,640 --> 00:53:34,319
set out to do this because we couldn't

1348
00:53:34,319 --> 00:53:36,559
calculate it so we have

1349
00:53:36,559 --> 00:53:38,160
prepared it but still this is

1350
00:53:38,160 --> 00:53:41,359
intractable so this is not usable alone

1351
00:53:41,359 --> 00:53:43,040
they write the beauty of variational

1352
00:53:43,040 --> 00:53:45,599
inference is that it instead optimizes a

1353
00:53:45,599 --> 00:53:48,240
tractable upper bound on this divergence

1354
00:53:48,240 --> 00:53:51,839
called the variational free energy vfe

1355
00:53:51,839 --> 00:53:54,000
to generate the bound we apply bayes

1356
00:53:54,000 --> 00:53:56,240
rule to the true posterior to rewrite it

1357
00:53:56,240 --> 00:53:58,079
in the form of the generative model and

1358
00:53:58,079 --> 00:53:59,200
the evidence

1359
00:53:59,200 --> 00:54:01,760
so here's that p of x given o

1360
00:54:01,760 --> 00:54:04,800
that's rewritten as now not of x given o

1361
00:54:04,800 --> 00:54:07,520
but p of o comma x that's the joint

1362
00:54:07,520 --> 00:54:10,880
distribution divided by the observations

1363
00:54:10,880 --> 00:54:12,640
so this

1364
00:54:12,640 --> 00:54:15,119
by the bayes rule p of x given o is

1365
00:54:15,119 --> 00:54:16,800
equivalent to this that's why this is

1366
00:54:16,800 --> 00:54:18,880
the first line here

1367
00:54:18,880 --> 00:54:21,520
they provide some rewritings

1368
00:54:21,520 --> 00:54:24,000
and then they write in the third line

1369
00:54:24,000 --> 00:54:25,680
the expectation

1370
00:54:25,680 --> 00:54:29,040
around p of o so the actual likelihood

1371
00:54:29,040 --> 00:54:31,359
of the observations themselves like how

1372
00:54:31,359 --> 00:54:34,800
likely is the thermometer saying 21

1373
00:54:34,800 --> 00:54:36,559
it vanishes

1374
00:54:36,559 --> 00:54:38,559
since the expectation is over the

1375
00:54:38,559 --> 00:54:42,000
variable x which is not in p of o

1376
00:54:42,000 --> 00:54:45,119
so it's like what is the expectation of

1377
00:54:45,119 --> 00:54:47,760
this coin flip over the temperature

1378
00:54:47,760 --> 00:54:50,880
tomorrow it's like because they're

1379
00:54:50,880 --> 00:54:53,119
different variables

1380
00:54:53,119 --> 00:54:55,200
it makes them very easy to separate out

1381
00:54:55,200 --> 00:54:58,240
if we're only interested in one of them

1382
00:54:58,240 --> 00:55:00,480
and then they write that this f

1383
00:55:00,480 --> 00:55:02,480
the free energy

1384
00:55:02,480 --> 00:55:04,720
is a very is attractable quantity since

1385
00:55:04,720 --> 00:55:06,799
it's a divergence between two quantities

1386
00:55:06,799 --> 00:55:09,680
we assume that we as the modeler know

1387
00:55:09,680 --> 00:55:12,799
the variational approximate posterior q

1388
00:55:12,799 --> 00:55:15,119
of x given o that's the distribution

1389
00:55:15,119 --> 00:55:17,440
that we control and the generative joint

1390
00:55:17,440 --> 00:55:20,480
distribution p of o comma x

1391
00:55:20,480 --> 00:55:22,720
so we traded out this intractable true

1392
00:55:22,720 --> 00:55:25,040
posterior where it's like if we knew it

1393
00:55:25,040 --> 00:55:26,960
then we would just stop there but we

1394
00:55:26,960 --> 00:55:29,280
don't know it so we've traded it out for

1395
00:55:29,280 --> 00:55:31,440
a divergence between something that we

1396
00:55:31,440 --> 00:55:33,280
totally control

1397
00:55:33,280 --> 00:55:35,839
q of x conditioned on these other

1398
00:55:35,839 --> 00:55:37,119
parameters

1399
00:55:37,119 --> 00:55:38,160
and

1400
00:55:38,160 --> 00:55:40,079
a joint distribution which we might have

1401
00:55:40,079 --> 00:55:42,160
uncertainty over but at least can be

1402
00:55:42,160 --> 00:55:43,119
modeled

1403
00:55:43,119 --> 00:55:44,720
so that's one of the key pieces of

1404
00:55:44,720 --> 00:55:46,960
variational bays and that's not specific

1405
00:55:46,960 --> 00:55:48,319
to predictive coding it's just an

1406
00:55:48,319 --> 00:55:49,920
important way that the authors are

1407
00:55:49,920 --> 00:55:51,839
introducing it here

1408
00:55:51,839 --> 00:55:54,640
anything to add

1409
00:55:56,799 --> 00:55:58,559
all right so let's kind of continue on

1410
00:55:58,559 --> 00:55:59,599
this theme

1411
00:55:59,599 --> 00:56:02,160
since f is an upper bound by minimizing

1412
00:56:02,160 --> 00:56:04,480
f we derive the variational distribution

1413
00:56:04,480 --> 00:56:06,640
that we control closer to the true

1414
00:56:06,640 --> 00:56:08,160
posterior

1415
00:56:08,160 --> 00:56:10,880
as an additional bonus

1416
00:56:10,880 --> 00:56:13,280
under certain conditions

1417
00:56:13,280 --> 00:56:15,920
f can be used for model selection so

1418
00:56:15,920 --> 00:56:17,520
that means it can be used not just to

1419
00:56:17,520 --> 00:56:19,599
kind of fine-tune a model that we've

1420
00:56:19,599 --> 00:56:22,160
already chosen but it can actually do

1421
00:56:22,160 --> 00:56:24,559
choosing from parametrically or

1422
00:56:24,559 --> 00:56:28,000
structurally different models

1423
00:56:28,000 --> 00:56:29,680
they write we can gain an important

1424
00:56:29,680 --> 00:56:31,520
intuition about f by showing it can be

1425
00:56:31,520 --> 00:56:33,040
decomposed into a likelihood

1426
00:56:33,040 --> 00:56:35,520
maximization term

1427
00:56:35,520 --> 00:56:37,280
and the kl divergence term which

1428
00:56:37,280 --> 00:56:39,520
penalizes deviation from the bayesian

1429
00:56:39,520 --> 00:56:41,839
prior these terms are often called

1430
00:56:41,839 --> 00:56:44,720
accuracy and complexity and this

1431
00:56:44,720 --> 00:56:47,359
decomposition is often used in different

1432
00:56:47,359 --> 00:56:49,440
machine learning algorithms so that's

1433
00:56:49,440 --> 00:56:51,119
one

1434
00:56:51,119 --> 00:56:52,640
rewriting

1435
00:56:52,640 --> 00:56:54,960
of free energy as this divergence

1436
00:56:54,960 --> 00:56:57,200
between the q and the p

1437
00:56:57,200 --> 00:56:59,920
and then rewriting that as an accuracy

1438
00:56:59,920 --> 00:57:01,680
and a complexity which we'll go into

1439
00:57:01,680 --> 00:57:05,119
maybe more another time

1440
00:57:05,119 --> 00:57:06,079
and

1441
00:57:06,079 --> 00:57:07,920
they write in many practical cases we

1442
00:57:07,920 --> 00:57:09,440
must relax the assumption that we know

1443
00:57:09,440 --> 00:57:11,040
the generative model

1444
00:57:11,040 --> 00:57:14,720
p of o and x the joint distribution

1445
00:57:14,720 --> 00:57:17,119
luckily this is not fatal

1446
00:57:17,119 --> 00:57:18,799
instead it is possible to learn the

1447
00:57:18,799 --> 00:57:20,559
generative model alongside the

1448
00:57:20,559 --> 00:57:23,599
variational posterior on the fly

1449
00:57:23,599 --> 00:57:25,520
and in parallel using

1450
00:57:25,520 --> 00:57:27,920
expectation maximization

1451
00:57:27,920 --> 00:57:28,880
so

1452
00:57:28,880 --> 00:57:32,559
this is basically the alternation

1453
00:57:32,559 --> 00:57:35,040
shown by equations three

1454
00:57:35,040 --> 00:57:37,200
of how we can be

1455
00:57:37,200 --> 00:57:40,880
setting one side of this kl divergence

1456
00:57:40,880 --> 00:57:44,000
fixed and optimizing the other side and

1457
00:57:44,000 --> 00:57:46,079
then going back and doing it the other

1458
00:57:46,079 --> 00:57:47,920
way and so that kind of back and forth

1459
00:57:47,920 --> 00:57:50,079
expectation maximization we're going to

1460
00:57:50,079 --> 00:57:52,000
be like reducing this divergence from

1461
00:57:52,000 --> 00:57:53,280
both sides

1462
00:57:53,280 --> 00:57:55,359
and so formalism three are saying the

1463
00:57:55,359 --> 00:57:57,119
variational parameters

1464
00:57:57,119 --> 00:57:58,079
phi

1465
00:57:58,079 --> 00:58:01,119
at the next time step t point t plus one

1466
00:58:01,119 --> 00:58:03,440
are an optimization

1467
00:58:03,440 --> 00:58:04,640
argument

1468
00:58:04,640 --> 00:58:07,200
of those parameters

1469
00:58:07,200 --> 00:58:09,200
um holding

1470
00:58:09,200 --> 00:58:11,280
the theta constant

1471
00:58:11,280 --> 00:58:12,319
and then

1472
00:58:12,319 --> 00:58:14,559
the second part of informalism three is

1473
00:58:14,559 --> 00:58:16,160
the exact opposite

1474
00:58:16,160 --> 00:58:18,559
now the generative model parameters

1475
00:58:18,559 --> 00:58:22,079
theta at t plus one are minimization

1476
00:58:22,079 --> 00:58:23,040
over

1477
00:58:23,040 --> 00:58:24,720
literally the same thing but holding the

1478
00:58:24,720 --> 00:58:27,280
variational parameters constant so

1479
00:58:27,280 --> 00:58:28,480
there's a lot more to say about

1480
00:58:28,480 --> 00:58:31,040
expectation maximization but this is

1481
00:58:31,040 --> 00:58:32,160
like

1482
00:58:32,160 --> 00:58:34,839
converging to a

1483
00:58:34,839 --> 00:58:37,440
small divergence

1484
00:58:37,440 --> 00:58:39,599
by whittling away at both sides and kind

1485
00:58:39,599 --> 00:58:42,640
of alternating there so that's how

1486
00:58:42,640 --> 00:58:45,200
expectation maximization can be used as

1487
00:58:45,200 --> 00:58:48,000
a heuristic algorithm for variational

1488
00:58:48,000 --> 00:58:51,480
bayesian optimization

1489
00:58:53,680 --> 00:58:55,760
how do we go from variational inference

1490
00:58:55,760 --> 00:58:57,680
which all of those previous slides are

1491
00:58:57,680 --> 00:58:59,680
things that we've basically talked about

1492
00:58:59,680 --> 00:59:02,000
before and that are not applying to

1493
00:59:02,000 --> 00:59:04,160
predictive coding specifically so how

1494
00:59:04,160 --> 00:59:07,359
are we going to get to predictive coding

1495
00:59:07,359 --> 00:59:09,280
they write having reviewed the general

1496
00:59:09,280 --> 00:59:11,839
principles of variational inference we

1497
00:59:11,839 --> 00:59:14,079
can see how they relate to predictive

1498
00:59:14,079 --> 00:59:15,119
coding

1499
00:59:15,119 --> 00:59:17,520
first to make any variational inference

1500
00:59:17,520 --> 00:59:20,000
algorithm concrete we must specify the

1501
00:59:20,000 --> 00:59:22,000
forms of the variational posterior and

1502
00:59:22,000 --> 00:59:23,520
the generative model it's like if you

1503
00:59:23,520 --> 00:59:25,520
want to do formalism three

1504
00:59:25,520 --> 00:59:27,280
those are the two pieces you need you

1505
00:59:27,280 --> 00:59:29,200
need like the phi stuff and the theta

1506
00:59:29,200 --> 00:59:30,480
stuff

1507
00:59:30,480 --> 00:59:32,559
and so they specify it here

1508
00:59:32,559 --> 00:59:35,119
n means a normal distribution

1509
00:59:35,119 --> 00:59:39,359
with like a mean comma variance format

1510
00:59:39,359 --> 00:59:41,200
and so they're going to define

1511
00:59:41,200 --> 00:59:45,520
a gaussian form for the generative model

1512
00:59:45,520 --> 00:59:47,520
the mean of this likelihood gaussian is

1513
00:59:47,520 --> 00:59:49,440
assumed to be some function of hidden

1514
00:59:49,440 --> 00:59:51,920
states which can be parameterized with

1515
00:59:51,920 --> 00:59:53,119
theta

1516
00:59:53,119 --> 00:59:54,960
while the mean of the prior gaussian

1517
00:59:54,960 --> 00:59:58,240
distribution is going to be a g of mu so

1518
00:59:58,240 --> 01:00:01,040
we're going to have like f of theta

1519
01:00:01,040 --> 01:00:03,760
and g of mu

1520
01:00:03,760 --> 01:00:05,839
the variances of these two gaussian

1521
01:00:05,839 --> 01:00:07,760
distributions of the generative model

1522
01:00:07,760 --> 01:00:11,760
are sigma 1 and sigma 2.

1523
01:00:12,480 --> 01:00:14,400
this is a slight technical detail but

1524
01:00:14,400 --> 01:00:15,520
they're going to assume that the

1525
01:00:15,520 --> 01:00:18,960
variational posterior is a drock delta

1526
01:00:18,960 --> 01:00:21,200
which is like a spiking function

1527
01:00:21,200 --> 01:00:24,720
distribution that's centered at the mean

1528
01:00:24,720 --> 01:00:27,760
however they explore that differently

1529
01:00:27,760 --> 01:00:29,839
with laplace assumption

1530
01:00:29,839 --> 01:00:31,920
so this is basically taking what we

1531
01:00:31,920 --> 01:00:34,319
discussed about variational inference

1532
01:00:34,319 --> 01:00:36,960
and preparing it to be entered into a

1533
01:00:36,960 --> 01:00:39,760
predictive coding way

1534
01:00:39,760 --> 01:00:41,839
so they're setting up the problem with

1535
01:00:41,839 --> 01:00:43,760
which parameters you're going to want to

1536
01:00:43,760 --> 01:00:46,480
do variational inference on in order to

1537
01:00:46,480 --> 01:00:48,880
implement predictive coding

1538
01:00:48,880 --> 01:00:50,240
and there's of course more to say but

1539
01:00:50,240 --> 01:00:52,880
we're just giving a first pass

1540
01:00:52,880 --> 01:00:54,640
in in appendix a

1541
01:00:54,640 --> 01:00:57,119
is where they describe the difference

1542
01:00:57,119 --> 01:00:58,240
between

1543
01:00:58,240 --> 01:01:01,040
using the laplacian approximation and

1544
01:01:01,040 --> 01:01:03,520
the direct delta so it's footnote 8

1545
01:01:03,520 --> 01:01:07,200
which suggests moving to appendix a

1546
01:01:07,200 --> 01:01:10,160
and looking at the buckley 2017 for a

1547
01:01:10,160 --> 01:01:11,280
walk through

1548
01:01:11,280 --> 01:01:13,839
but this on the bottom right is kind of

1549
01:01:13,839 --> 01:01:15,680
like a summary of the difference between

1550
01:01:15,680 --> 01:01:18,000
the dirac delta approach and the laplace

1551
01:01:18,000 --> 01:01:20,480
method approach so the diroc delta is

1552
01:01:20,480 --> 01:01:23,839
like we're trying to find the spike

1553
01:01:23,839 --> 01:01:26,319
that's at the mean

1554
01:01:26,319 --> 01:01:29,040
or the median there's some other details

1555
01:01:29,040 --> 01:01:30,079
in play

1556
01:01:30,079 --> 01:01:31,200
that

1557
01:01:31,200 --> 01:01:33,200
is where the bulk of the probability

1558
01:01:33,200 --> 01:01:35,119
distribution mass is

1559
01:01:35,119 --> 01:01:37,359
whereas the laplace method tries to fit

1560
01:01:37,359 --> 01:01:38,559
a polynomial

1561
01:01:38,559 --> 01:01:40,319
like just a second degree polynomial

1562
01:01:40,319 --> 01:01:41,760
like a quadratic

1563
01:01:41,760 --> 01:01:43,119
over

1564
01:01:43,119 --> 01:01:45,599
the probability distribution

1565
01:01:45,599 --> 01:01:47,680
also in the way that

1566
01:01:47,680 --> 01:01:49,839
is best fitting so there's some

1567
01:01:49,839 --> 01:01:51,599
similarities and some differences and

1568
01:01:51,599 --> 01:01:54,960
it's explored more in the buckley 2017

1569
01:01:54,960 --> 01:01:55,920
paper

1570
01:01:55,920 --> 01:01:58,319
on the fep for action perception

1571
01:01:58,319 --> 01:02:00,799
mathematical review

1572
01:02:00,799 --> 01:02:02,720
so let's continue with our

1573
01:02:02,720 --> 01:02:04,839
exploration of how we do variational

1574
01:02:04,839 --> 01:02:07,280
bays and make it a predictive coding

1575
01:02:07,280 --> 01:02:09,039
model

1576
01:02:09,039 --> 01:02:10,880
at the top they write we define the

1577
01:02:10,880 --> 01:02:13,440
prediction errors epsilon o

1578
01:02:13,440 --> 01:02:16,000
that's like the error on observation

1579
01:02:16,000 --> 01:02:17,760
and epsilon x

1580
01:02:17,760 --> 01:02:20,240
that's the error on the mean states so

1581
01:02:20,240 --> 01:02:21,920
the epsilon o

1582
01:02:21,920 --> 01:02:23,200
is like

1583
01:02:23,200 --> 01:02:25,280
o minus f

1584
01:02:25,280 --> 01:02:27,520
of some function you know f of some

1585
01:02:27,520 --> 01:02:29,359
parameters and then there's also the

1586
01:02:29,359 --> 01:02:31,680
epsilon x so how

1587
01:02:31,680 --> 01:02:33,200
much prediction error do you have about

1588
01:02:33,200 --> 01:02:34,720
the observation how much prediction

1589
01:02:34,720 --> 01:02:39,359
error do you have about the hidden state

1590
01:02:39,359 --> 01:02:42,000
epsilons are the prediction errors and

1591
01:02:42,000 --> 01:02:44,079
they're parameterized by these theta 1

1592
01:02:44,079 --> 01:02:47,359
and 2 variances

1593
01:02:47,359 --> 01:02:49,680
given all of this we can derive

1594
01:02:49,680 --> 01:02:51,599
dynamics for all the variables of

1595
01:02:51,599 --> 01:02:54,400
interest so that's the actual underlying

1596
01:02:54,400 --> 01:02:56,160
hidden state the temperature in the room

1597
01:02:56,160 --> 01:02:57,839
that we want to be

1598
01:02:57,839 --> 01:03:00,480
actually predicting mu

1599
01:03:00,480 --> 01:03:02,319
and then well the variational parameter

1600
01:03:02,319 --> 01:03:03,200
of it

1601
01:03:03,200 --> 01:03:04,160
and then

1602
01:03:04,160 --> 01:03:06,400
theta one and theta two which are the

1603
01:03:06,400 --> 01:03:08,079
variances you can think of it as like

1604
01:03:08,079 --> 01:03:10,240
the variance of the room temperature and

1605
01:03:10,240 --> 01:03:11,680
the variance of the term of the

1606
01:03:11,680 --> 01:03:13,920
thermometer i hope that's not

1607
01:03:13,920 --> 01:03:16,240
wrong or an oversimplification but those

1608
01:03:16,240 --> 01:03:18,160
are like the two variances

1609
01:03:18,160 --> 01:03:20,000
and we want the real temperature of the

1610
01:03:20,000 --> 01:03:22,240
room

1611
01:03:22,400 --> 01:03:24,079
they write we can derive dynamics for

1612
01:03:24,079 --> 01:03:25,839
all these variables of interest by

1613
01:03:25,839 --> 01:03:27,280
taking the derivatives of the

1614
01:03:27,280 --> 01:03:29,760
variational free energy f

1615
01:03:29,760 --> 01:03:32,240
the update rules are as follows

1616
01:03:32,240 --> 01:03:35,200
and so this is change in mu theta 1 and

1617
01:03:35,200 --> 01:03:39,599
theta 2 over time d mu dt d theta 1 over

1618
01:03:39,599 --> 01:03:43,039
dt and d theta 2 over dt

1619
01:03:43,039 --> 01:03:46,160
and that has some equivalences with some

1620
01:03:46,160 --> 01:03:49,039
f's so it's going to be going from just

1621
01:03:49,039 --> 01:03:50,559
the change in the mu estimate through

1622
01:03:50,559 --> 01:03:53,039
time to something involving free energy

1623
01:03:53,039 --> 01:03:56,400
and then that's defined more formally

1624
01:03:56,400 --> 01:03:58,319
importantly these update rules are very

1625
01:03:58,319 --> 01:04:00,559
similar to the ones derived in rao and

1626
01:04:00,559 --> 01:04:03,359
ballard 1999 and therefore can be

1627
01:04:03,359 --> 01:04:05,119
interpreted as recapitulating core

1628
01:04:05,119 --> 01:04:07,680
predictive coding update rules

1629
01:04:07,680 --> 01:04:09,520
for instance the muse are typically

1630
01:04:09,520 --> 01:04:11,599
interpreted as rapidly changing neural

1631
01:04:11,599 --> 01:04:14,240
firing rates while thetas are slowly

1632
01:04:14,240 --> 01:04:16,880
changing synaptic weight values

1633
01:04:16,880 --> 01:04:20,000
so they're weaving together connecting

1634
01:04:20,000 --> 01:04:23,680
the biology with the formalisms here

1635
01:04:23,680 --> 01:04:26,400
and they write also that that mu can be

1636
01:04:26,400 --> 01:04:29,680
understood as the process of perception

1637
01:04:29,680 --> 01:04:33,280
that's like how hot is it in the room

1638
01:04:33,280 --> 01:04:35,119
since mu is meant to correspond to the

1639
01:04:35,119 --> 01:04:36,559
estimate of the latent state of the

1640
01:04:36,559 --> 01:04:38,880
environment generating the observations

1641
01:04:38,880 --> 01:04:41,200
on the thermometer and by contrast the

1642
01:04:41,200 --> 01:04:43,599
dynamics of thetas can be thought of as

1643
01:04:43,599 --> 01:04:45,520
corresponding to learning

1644
01:04:45,520 --> 01:04:47,200
since theta effectively defined the

1645
01:04:47,200 --> 01:04:49,039
mapping between the latent state and the

1646
01:04:49,039 --> 01:04:50,480
observations

1647
01:04:50,480 --> 01:04:53,119
so it's like you think you know how the

1648
01:04:53,119 --> 01:04:54,319
thermometer is related to the

1649
01:04:54,319 --> 01:04:56,000
temperature in the room

1650
01:04:56,000 --> 01:04:57,280
and then you see your thermometer

1651
01:04:57,280 --> 01:04:58,799
changing you're changing how hot you

1652
01:04:58,799 --> 01:05:00,720
think the room is that happens over a

1653
01:05:00,720 --> 01:05:02,960
shorter time scale and then over a

1654
01:05:02,960 --> 01:05:04,880
longer time scale you might come to

1655
01:05:04,880 --> 01:05:07,599
learn the variability of a temperature

1656
01:05:07,599 --> 01:05:10,559
or how noisy the thermometer is but

1657
01:05:10,559 --> 01:05:12,480
that's more like learning

1658
01:05:12,480 --> 01:05:14,480
than perception however they're on a

1659
01:05:14,480 --> 01:05:17,039
continuum using this model

1660
01:05:17,039 --> 01:05:20,240
so actually this up to formalism 9

1661
01:05:20,240 --> 01:05:22,720
completes the core formalism of

1662
01:05:22,720 --> 01:05:24,559
predictive coding which is it's a

1663
01:05:24,559 --> 01:05:27,359
variational bayesian approach to having

1664
01:05:27,359 --> 01:05:30,640
this ongoing prediction error minimizing

1665
01:05:30,640 --> 01:05:34,000
approach to latent state estimation that

1666
01:05:34,000 --> 01:05:36,960
is the heart of predictive coding

1667
01:05:36,960 --> 01:05:38,720
and now we're going to jump into like a

1668
01:05:38,720 --> 01:05:40,240
few different elaborations that we're

1669
01:05:40,240 --> 01:05:41,760
going to be moving through a little bit

1670
01:05:41,760 --> 01:05:43,280
more quickly

1671
01:05:43,280 --> 01:05:46,640
anything to add maria

1672
01:05:47,520 --> 01:05:49,440
awesome

1673
01:05:49,440 --> 01:05:52,160
the previous examples

1674
01:05:52,160 --> 01:05:53,680
focused on predictive coding with a

1675
01:05:53,680 --> 01:05:58,000
single level of latent variables mu 1.

1676
01:05:58,000 --> 01:06:00,160
however the expressiveness of such a

1677
01:06:00,160 --> 01:06:02,240
scheme is limited

1678
01:06:02,240 --> 01:06:03,920
deep neural networks in machine learning

1679
01:06:03,920 --> 01:06:05,280
have demonstrated that having

1680
01:06:05,280 --> 01:06:07,599
hierarchical sets of latent variables is

1681
01:06:07,599 --> 01:06:09,760
a key to enabling methods that learn

1682
01:06:09,760 --> 01:06:11,599
powerful abstractions

1683
01:06:11,599 --> 01:06:13,599
and handle intrinsically hierarchical

1684
01:06:13,599 --> 01:06:15,280
dynamics of the sort that humans

1685
01:06:15,280 --> 01:06:17,680
intuitively perceive

1686
01:06:17,680 --> 01:06:20,160
predictive coding schemes introduced can

1687
01:06:20,160 --> 01:06:22,000
be straightforwardly extended to handle

1688
01:06:22,000 --> 01:06:24,640
hierarchical dynamics of arbitrary depth

1689
01:06:24,640 --> 01:06:26,480
equivalent to deep neural networks in

1690
01:06:26,480 --> 01:06:28,240
machine learning

1691
01:06:28,240 --> 01:06:29,839
this is done through postulating

1692
01:06:29,839 --> 01:06:33,039
multiple layers of latent variables x

1693
01:06:33,039 --> 01:06:35,839
sub 1 through x sub l and then defining

1694
01:06:35,839 --> 01:06:38,240
the generative model as follows

1695
01:06:38,240 --> 01:06:40,799
so like p the generative model

1696
01:06:40,799 --> 01:06:42,960
is going to be basically over all the

1697
01:06:42,960 --> 01:06:45,039
layers

1698
01:06:45,039 --> 01:06:47,599
so just as the generative model

1699
01:06:47,599 --> 01:06:50,400
and the variational distribution had to

1700
01:06:50,400 --> 01:06:53,119
be defined for the single layer model

1701
01:06:53,119 --> 01:06:55,440
here they're going to define the p

1702
01:06:55,440 --> 01:06:57,839
distribution and now they need the q

1703
01:06:57,839 --> 01:06:59,440
we define a separate variational

1704
01:06:59,440 --> 01:07:01,839
posterior for each layer so they define

1705
01:07:01,839 --> 01:07:03,520
the p's and the cues

1706
01:07:03,520 --> 01:07:05,839
just like they did in the single layer

1707
01:07:05,839 --> 01:07:08,400
and then that allows them to calculate

1708
01:07:08,400 --> 01:07:10,720
the variational free energy which is a

1709
01:07:10,720 --> 01:07:12,880
sum of the prediction errors of each

1710
01:07:12,880 --> 01:07:14,400
level

1711
01:07:14,400 --> 01:07:15,599
and so

1712
01:07:15,599 --> 01:07:18,480
this is f now not just over one layer

1713
01:07:18,480 --> 01:07:20,480
but multiple levels

1714
01:07:20,480 --> 01:07:22,960
so the variational posteriors that need

1715
01:07:22,960 --> 01:07:25,359
to be calculated are partitioned across

1716
01:07:25,359 --> 01:07:26,640
the layers

1717
01:07:26,640 --> 01:07:28,960
which allows them to be summed

1718
01:07:28,960 --> 01:07:31,440
in a very straightforward way

1719
01:07:31,440 --> 01:07:33,200
given that free energy divides nicely

1720
01:07:33,200 --> 01:07:35,039
into the sum of layer wise prediction

1721
01:07:35,039 --> 01:07:36,240
errors

1722
01:07:36,240 --> 01:07:38,559
it comes as no surprise the dynamics of

1723
01:07:38,559 --> 01:07:40,799
the mu and the theta are similarly

1724
01:07:40,799 --> 01:07:43,680
separable across layers

1725
01:07:43,680 --> 01:07:46,000
that allows different layers of this

1726
01:07:46,000 --> 01:07:49,039
prediction hierarchy to be like precise

1727
01:07:49,039 --> 01:07:51,280
or imprecise

1728
01:07:51,280 --> 01:07:53,680
and allow those movements to happen in a

1729
01:07:53,680 --> 01:07:54,839
way that's

1730
01:07:54,839 --> 01:07:57,119
uncorrelated which is not just because

1731
01:07:57,119 --> 01:07:58,799
the real world presents itself with

1732
01:07:58,799 --> 01:08:00,720
settings where there's like confidence

1733
01:08:00,720 --> 01:08:03,280
at lower and higher and vice versa but

1734
01:08:03,280 --> 01:08:05,680
it makes this calculation of the free

1735
01:08:05,680 --> 01:08:07,599
energy of the whole system

1736
01:08:07,599 --> 01:08:09,520
more like a simple sum whereas if there

1737
01:08:09,520 --> 01:08:10,640
was these like really complex

1738
01:08:10,640 --> 01:08:12,799
interactions with the first layer by the

1739
01:08:12,799 --> 01:08:14,640
third layer if the fifth layer is this

1740
01:08:14,640 --> 01:08:16,238
way then

1741
01:08:16,238 --> 01:08:18,158
doing the statistics would be a lot more

1742
01:08:18,158 --> 01:08:19,759
challenging

1743
01:08:19,759 --> 01:08:22,719
so visually here's what that looks like

1744
01:08:22,719 --> 01:08:25,198
we have the mu which is the mean

1745
01:08:25,198 --> 01:08:26,319
estimate

1746
01:08:26,319 --> 01:08:28,799
of what's happening at that level

1747
01:08:28,799 --> 01:08:29,759
and then

1748
01:08:29,759 --> 01:08:33,359
there's passing of these epsilon error

1749
01:08:33,359 --> 01:08:34,640
terms

1750
01:08:34,640 --> 01:08:36,399
so they write this is the architecture

1751
01:08:36,399 --> 01:08:38,719
of a in figure 1 multi-layer predictive

1752
01:08:38,719 --> 01:08:40,399
coding network

1753
01:08:40,399 --> 01:08:42,399
shown here with two value and error

1754
01:08:42,399 --> 01:08:44,560
neurons in each layer

1755
01:08:44,560 --> 01:08:46,719
the value neurons project to the error

1756
01:08:46,719 --> 01:08:48,719
neurons of the layer below

1757
01:08:48,719 --> 01:08:50,479
and the error neurons represent the

1758
01:08:50,479 --> 01:08:53,679
current activity so this is like

1759
01:08:53,679 --> 01:08:56,080
starting to walk us back towards that

1760
01:08:56,080 --> 01:08:58,399
cortical layout

1761
01:08:58,399 --> 01:09:00,560
where there's a cortical column

1762
01:09:00,560 --> 01:09:02,799
where there's some so-called upwards and

1763
01:09:02,799 --> 01:09:05,120
downward signaling but also there's

1764
01:09:05,120 --> 01:09:07,359
lateral signaling

1765
01:09:07,359 --> 01:09:09,600
so this is a graphical model

1766
01:09:09,600 --> 01:09:11,040
that's reflecting

1767
01:09:11,040 --> 01:09:12,960
the way that predictive coding

1768
01:09:12,960 --> 01:09:15,359
can be arranged almost like in series

1769
01:09:15,359 --> 01:09:16,960
and in parallel

1770
01:09:16,960 --> 01:09:19,600
to have like wide models

1771
01:09:19,600 --> 01:09:22,000
and deep models just like you could have

1772
01:09:22,000 --> 01:09:24,319
a neural network where it was

1773
01:09:24,319 --> 01:09:26,399
four neurons four neurons four neurons

1774
01:09:26,399 --> 01:09:28,719
four neurons or you could have 64 and

1775
01:09:28,719 --> 01:09:31,359
then 64. that would be like a shallower

1776
01:09:31,359 --> 01:09:34,000
but wider model and so that notion of

1777
01:09:34,000 --> 01:09:36,319
shallowness and depth is also going to

1778
01:09:36,319 --> 01:09:40,560
come into play with predictive coding

1779
01:09:42,799 --> 01:09:46,719
here we're in formalism 12 and 13.

1780
01:09:46,719 --> 01:09:51,359
and so now we're looking at the

1781
01:09:51,359 --> 01:09:54,880
rate of change the derivative of mu sub

1782
01:09:54,880 --> 01:09:55,760
l

1783
01:09:55,760 --> 01:09:57,760
so the estimate of the mean at that

1784
01:09:57,760 --> 01:10:01,040
layer and theta l the variance at that

1785
01:10:01,040 --> 01:10:03,600
layer so these are the update rules the

1786
01:10:03,600 --> 01:10:04,719
gradients

1787
01:10:04,719 --> 01:10:06,000
for the mean and the variance of

1788
01:10:06,000 --> 01:10:07,440
different levels and they're also

1789
01:10:07,440 --> 01:10:09,920
written as free energy functionals of

1790
01:10:09,920 --> 01:10:12,159
those layers

1791
01:10:12,159 --> 01:10:13,280
and

1792
01:10:13,280 --> 01:10:15,199
the dynamics of the variational means

1793
01:10:15,199 --> 01:10:17,520
depends only on the prediction errors at

1794
01:10:17,520 --> 01:10:19,600
their layer and the prediction errors on

1795
01:10:19,600 --> 01:10:21,600
the layer below so again we don't have

1796
01:10:21,600 --> 01:10:24,239
this epsilon at l minus 1 connecting up

1797
01:10:24,239 --> 01:10:26,400
all the way to mu it's only through

1798
01:10:26,400 --> 01:10:28,080
these local connections within and

1799
01:10:28,080 --> 01:10:30,400
between layers by which we're needing to

1800
01:10:30,400 --> 01:10:33,040
calculate anything

1801
01:10:33,040 --> 01:10:34,880
we can think of the muse as trying to

1802
01:10:34,880 --> 01:10:37,440
compromise between causing error

1803
01:10:37,440 --> 01:10:39,280
by deviating from the prediction from

1804
01:10:39,280 --> 01:10:40,640
the layer above

1805
01:10:40,640 --> 01:10:42,640
and adjusting their own prediction to

1806
01:10:42,640 --> 01:10:45,360
resolve error at the layer below so it's

1807
01:10:45,360 --> 01:10:48,000
kind of like a hierarchy of of bosses or

1808
01:10:48,000 --> 01:10:50,000
tasks and there's like top-down

1809
01:10:50,000 --> 01:10:51,760
expectations and there's the bottom-up

1810
01:10:51,760 --> 01:10:53,360
reality which might be like ahead or

1811
01:10:53,360 --> 01:10:55,679
behind schedule and then it puts each

1812
01:10:55,679 --> 01:10:58,080
person like in this compromise

1813
01:10:58,080 --> 01:11:00,239
situation

1814
01:11:00,239 --> 01:11:02,159
crucially for conceptual readings of

1815
01:11:02,159 --> 01:11:03,840
predictive coding

1816
01:11:03,840 --> 01:11:06,080
and this is where there's like a doorway

1817
01:11:06,080 --> 01:11:07,840
to the philosophy and some of the

1818
01:11:07,840 --> 01:11:09,440
broader discussions

1819
01:11:09,440 --> 01:11:11,520
this means that sensory data is not

1820
01:11:11,520 --> 01:11:13,520
directly transmitted up through the

1821
01:11:13,520 --> 01:11:16,239
hierarchy as is assumed in much of

1822
01:11:16,239 --> 01:11:18,159
perceptual neuroscience

1823
01:11:18,159 --> 01:11:20,560
and so it totally returns us to these

1824
01:11:20,560 --> 01:11:23,120
questions like what is perception what

1825
01:11:23,120 --> 01:11:25,360
is cognition what is action

1826
01:11:25,360 --> 01:11:27,040
what what is

1827
01:11:27,040 --> 01:11:29,520
coming from the eye to the brain or what

1828
01:11:29,520 --> 01:11:31,199
goes from the brain to the eye what is

1829
01:11:31,199 --> 01:11:33,440
the eye doing what is the brain doing

1830
01:11:33,440 --> 01:11:35,440
with respect to the eye

1831
01:11:35,440 --> 01:11:36,880
and how does it relate to predictive

1832
01:11:36,880 --> 01:11:38,800
coding

1833
01:11:38,800 --> 01:11:41,840
any quick answers on those

1834
01:11:43,360 --> 01:11:44,800
naturally

1835
01:11:44,800 --> 01:11:46,480
cool so

1836
01:11:46,480 --> 01:11:48,320
after introducing that kernel of the

1837
01:11:48,320 --> 01:11:50,400
predictive coding formalism and taking

1838
01:11:50,400 --> 01:11:53,440
it into the multi-level context now we

1839
01:11:53,440 --> 01:11:56,320
can look at another elaboration or

1840
01:11:56,320 --> 01:11:58,239
generalization

1841
01:11:58,239 --> 01:12:00,239
so they write in 2.3

1842
01:12:00,239 --> 01:12:01,920
we have considered the modeling of just

1843
01:12:01,920 --> 01:12:05,040
a single static stimulus up

1844
01:12:05,040 --> 01:12:06,960
however most interesting data that the

1845
01:12:06,960 --> 01:12:08,480
brain receives comes in temporal

1846
01:12:08,480 --> 01:12:12,000
sequences o bar which is o through time

1847
01:12:12,000 --> 01:12:14,000
to model such temporal sequences it's

1848
01:12:14,000 --> 01:12:15,600
often useful to split the latent

1849
01:12:15,600 --> 01:12:18,080
variables into states which can vary

1850
01:12:18,080 --> 01:12:21,600
with time and parameters which cannot

1851
01:12:21,600 --> 01:12:23,679
in the case of sequences

1852
01:12:23,679 --> 01:12:25,520
instead of minimizing the variational

1853
01:12:25,520 --> 01:12:27,920
free energy we must instead minimize the

1854
01:12:27,920 --> 01:12:30,719
free action which is the path integral

1855
01:12:30,719 --> 01:12:32,560
of the variational free energy through

1856
01:12:32,560 --> 01:12:33,840
time

1857
01:12:33,840 --> 01:12:35,760
we're not going to go into the

1858
01:12:35,760 --> 01:12:37,120
formalisms

1859
01:12:37,120 --> 01:12:38,800
related to the generalized coordinates

1860
01:12:38,800 --> 01:12:39,840
15

1861
01:12:39,840 --> 01:12:42,719
or 14 15 16 and 17

1862
01:12:42,719 --> 01:12:44,800
but it's something that we can explore

1863
01:12:44,800 --> 01:12:45,840
later

1864
01:12:45,840 --> 01:12:48,719
and just as a reminder just to kind of

1865
01:12:48,719 --> 01:12:50,560
um

1866
01:12:50,560 --> 01:12:52,000
excite somebody who might want to

1867
01:12:52,000 --> 01:12:53,120
explore it

1868
01:12:53,120 --> 01:12:55,280
we explored the idea of the generalized

1869
01:12:55,280 --> 01:12:58,080
coordinates of motion a lot in active

1870
01:12:58,080 --> 01:13:02,159
livestream number 26 with decosta at all

1871
01:13:02,159 --> 01:13:04,640
and so here was a model where we have

1872
01:13:04,640 --> 01:13:07,120
from left to right time at different

1873
01:13:07,120 --> 01:13:08,480
time steps

1874
01:13:08,480 --> 01:13:11,280
and then there's the observables of

1875
01:13:11,280 --> 01:13:12,560
something

1876
01:13:12,560 --> 01:13:16,000
and then it is part of this like column

1877
01:13:16,000 --> 01:13:18,480
of higher derivatives of its motion so

1878
01:13:18,480 --> 01:13:21,120
it's like position its

1879
01:13:21,120 --> 01:13:23,840
velocity its acceleration and so on and

1880
01:13:23,840 --> 01:13:25,679
so at each time step

1881
01:13:25,679 --> 01:13:27,600
moving forward you're computing the

1882
01:13:27,600 --> 01:13:29,840
generalized coordinates of motion

1883
01:13:29,840 --> 01:13:32,480
for like the movement of a baseball and

1884
01:13:32,480 --> 01:13:34,480
that is what ties generalized

1885
01:13:34,480 --> 01:13:37,360
coordinates of motion to pid control

1886
01:13:37,360 --> 01:13:39,360
which will also come back at the end

1887
01:13:39,360 --> 01:13:42,080
also known as integrator chains and this

1888
01:13:42,080 --> 01:13:43,920
is all happening within this discussion

1889
01:13:43,920 --> 01:13:46,080
of bayesian mechanics just like if we

1890
01:13:46,080 --> 01:13:48,239
had been talking about the baseball we

1891
01:13:48,239 --> 01:13:50,080
would be talking about physical

1892
01:13:50,080 --> 01:13:53,679
mechanics or statistical mechanics well

1893
01:13:53,679 --> 01:13:56,400
it's bayesian so it's bayesian mechanics

1894
01:13:56,400 --> 01:13:59,199
and the physics of action and control

1895
01:13:59,199 --> 01:14:02,320
that's what they explore in section 2-3

1896
01:14:02,320 --> 01:14:05,120
section 2.4 introduces and goes into a

1897
01:14:05,120 --> 01:14:08,320
little bit more detail on precision

1898
01:14:08,320 --> 01:14:10,640
one core aspect of predictive coding

1899
01:14:10,640 --> 01:14:13,120
absent in the original rao and ballard

1900
01:14:13,120 --> 01:14:15,920
1999 formulation

1901
01:14:15,920 --> 01:14:18,239
is the notion of precision

1902
01:14:18,239 --> 01:14:20,320
or inverse variance

1903
01:14:20,320 --> 01:14:23,440
so precision and variance are like one

1904
01:14:23,440 --> 01:14:26,159
over each other so if someone said it's

1905
01:14:26,159 --> 01:14:28,960
very high precision that's a very narrow

1906
01:14:28,960 --> 01:14:30,880
very sharp distribution

1907
01:14:30,880 --> 01:14:33,360
and one over the variance is another way

1908
01:14:33,360 --> 01:14:35,440
to write that so sometimes it's used

1909
01:14:35,440 --> 01:14:37,840
with like a beta and a gamma in other

1910
01:14:37,840 --> 01:14:39,440
models

1911
01:14:39,440 --> 01:14:41,440
precision serve to multiplicatively

1912
01:14:41,440 --> 01:14:42,400
modulate

1913
01:14:42,400 --> 01:14:44,800
the importance of the prediction errors

1914
01:14:44,800 --> 01:14:46,960
and thus possess a significant influence

1915
01:14:46,960 --> 01:14:50,000
in the overall dynamics of the model

1916
01:14:50,000 --> 01:14:51,120
and so

1917
01:14:51,120 --> 01:14:53,840
we won't go into every detail with how

1918
01:14:53,840 --> 01:14:56,159
the precisions are themselves fit but

1919
01:14:56,159 --> 01:14:59,520
just wanted to note that this big sigma

1920
01:14:59,520 --> 01:15:02,400
is the free energy being summed over the

1921
01:15:02,400 --> 01:15:03,360
layers

1922
01:15:03,360 --> 01:15:04,880
so big sigma

1923
01:15:04,880 --> 01:15:09,679
is a um is a multiplicative sum

1924
01:15:09,679 --> 01:15:11,520
sorry it's just a sum

1925
01:15:11,520 --> 01:15:14,880
but then this smaller sigma inside

1926
01:15:14,880 --> 01:15:18,320
is sigma as the precision matrix at a

1927
01:15:18,320 --> 01:15:20,000
given level

1928
01:15:20,000 --> 01:15:22,000
so it's kind of like different uses of

1929
01:15:22,000 --> 01:15:22,960
sigma

1930
01:15:22,960 --> 01:15:26,239
but this is a way where the variances

1931
01:15:26,239 --> 01:15:28,320
and the prediction errors at a given

1932
01:15:28,320 --> 01:15:29,280
level

1933
01:15:29,280 --> 01:15:32,080
are being summed across levels and

1934
01:15:32,080 --> 01:15:36,640
that's helping fit these precision terms

1935
01:15:36,960 --> 01:15:39,040
statistical detail but it's important

1936
01:15:39,040 --> 01:15:42,880
for making it work in real inference

1937
01:15:42,880 --> 01:15:44,080
okay

1938
01:15:44,080 --> 01:15:47,360
in section 2.5 they they pull back from

1939
01:15:47,360 --> 01:15:49,840
elaborating on predictive coding and

1940
01:15:49,840 --> 01:15:52,159
they talk more about how it has some

1941
01:15:52,159 --> 01:15:55,840
biologically plausible bases

1942
01:15:55,840 --> 01:15:58,080
so this was quite interesting

1943
01:15:58,080 --> 01:15:59,840
while technically predictive coding is

1944
01:15:59,840 --> 01:16:01,600
simply variational inference and

1945
01:16:01,600 --> 01:16:03,440
filtering algorithm under gaussian

1946
01:16:03,440 --> 01:16:05,360
assumptions from the beginning it has

1947
01:16:05,360 --> 01:16:06,800
been claimed to be biologically

1948
01:16:06,800 --> 01:16:10,000
plausible theory of cortical computation

1949
01:16:10,000 --> 01:16:12,159
and also for other brain regions like

1950
01:16:12,159 --> 01:16:13,199
the i

1951
01:16:13,199 --> 01:16:15,920
and for other cognitive systems

1952
01:16:15,920 --> 01:16:17,520
the literature has consistently drawn

1953
01:16:17,520 --> 01:16:19,360
close connections between the theory and

1954
01:16:19,360 --> 01:16:20,960
potential computations that may be

1955
01:16:20,960 --> 01:16:23,440
performed in brains for example rao and

1956
01:16:23,440 --> 01:16:25,520
ballard explicitly claimed to model the

1957
01:16:25,520 --> 01:16:27,280
early visual cortex

1958
01:16:27,280 --> 01:16:29,760
and fristen explicitly proposed

1959
01:16:29,760 --> 01:16:32,400
predictive coding as a general theory of

1960
01:16:32,400 --> 01:16:34,880
cortical computation this is like the

1961
01:16:34,880 --> 01:16:37,679
realism and instrumentalism discussion

1962
01:16:37,679 --> 01:16:39,600
are we just modeling

1963
01:16:39,600 --> 01:16:41,280
these systems and the kind of data that

1964
01:16:41,280 --> 01:16:42,640
they provide us

1965
01:16:42,640 --> 01:16:44,719
and it just fits really well as a model

1966
01:16:44,719 --> 01:16:45,920
but we're not saying that the

1967
01:16:45,920 --> 01:16:48,719
architecture of the statistical model

1968
01:16:48,719 --> 01:16:51,360
has anything to do with the architecture

1969
01:16:51,360 --> 01:16:53,679
of the physical pieces of the brain

1970
01:16:53,679 --> 01:16:55,840
or is it a little bit in the gray zone

1971
01:16:55,840 --> 01:16:57,679
where it's kind of like wow that model

1972
01:16:57,679 --> 01:16:59,280
fits so well

1973
01:16:59,280 --> 01:17:01,840
and the anatomy looks so good maybe

1974
01:17:01,840 --> 01:17:04,080
there's something there to it

1975
01:17:04,080 --> 01:17:07,040
and so in this section they review

1976
01:17:07,040 --> 01:17:09,440
empirical work that has attempted to

1977
01:17:09,440 --> 01:17:11,600
validate or falsify key tenets of

1978
01:17:11,600 --> 01:17:13,920
predictive coding in the cortex

1979
01:17:13,920 --> 01:17:16,480
as well as some issues with the approach

1980
01:17:16,480 --> 01:17:19,280
so it's mainly focused on the mammalian

1981
01:17:19,280 --> 01:17:21,760
brain and the neural cell type

1982
01:17:21,760 --> 01:17:23,280
but

1983
01:17:23,280 --> 01:17:25,679
as always we can sort of think about

1984
01:17:25,679 --> 01:17:26,640
what is

1985
01:17:26,640 --> 01:17:28,560
the adjacent here like what are the

1986
01:17:28,560 --> 01:17:30,960
roles of other cell types in predictive

1987
01:17:30,960 --> 01:17:33,280
coding in the mammalian brain

1988
01:17:33,280 --> 01:17:35,440
and then how might like the insect

1989
01:17:35,440 --> 01:17:37,600
nervous system or how my other nervous

1990
01:17:37,600 --> 01:17:40,640
systems or non-nervous systems perform

1991
01:17:40,640 --> 01:17:42,800
predictive coding

1992
01:17:42,800 --> 01:17:44,480
and then they just provide

1993
01:17:44,480 --> 01:17:46,080
one example

1994
01:17:46,080 --> 01:17:48,159
where they take what they had shown in

1995
01:17:48,159 --> 01:17:50,080
figure one

1996
01:17:50,080 --> 01:17:52,640
with this multi-level predictive coding

1997
01:17:52,640 --> 01:17:53,840
scheme

1998
01:17:53,840 --> 01:17:55,040
that was sort of

1999
01:17:55,040 --> 01:17:57,679
predictively programming as they say

2000
01:17:57,679 --> 01:18:00,239
one to think about cortical cognition

2001
01:18:00,239 --> 01:18:01,440
and now they're gonna make that

2002
01:18:01,440 --> 01:18:04,080
connection clear in figure two so on the

2003
01:18:04,080 --> 01:18:06,320
right side is there figure two

2004
01:18:06,320 --> 01:18:08,560
and that's the canonical micro circuit

2005
01:18:08,560 --> 01:18:11,199
model of bostos at all

2006
01:18:11,199 --> 01:18:12,880
mapped on to the connectivity of a

2007
01:18:12,880 --> 01:18:14,239
cortical region

2008
01:18:14,239 --> 01:18:17,040
and so then in red are those parameters

2009
01:18:17,040 --> 01:18:19,520
that we've been discussing the muse and

2010
01:18:19,520 --> 01:18:22,560
the epsilons the means and then the

2011
01:18:22,560 --> 01:18:24,239
prediction errors

2012
01:18:24,239 --> 01:18:25,120
and then

2013
01:18:25,120 --> 01:18:27,600
here's just a few other representations

2014
01:18:27,600 --> 01:18:30,560
of what others have written

2015
01:18:30,560 --> 01:18:33,440
about like that cortical column

2016
01:18:33,440 --> 01:18:34,960
so here's like an evolutionary

2017
01:18:34,960 --> 01:18:36,480
perspective that connects some of the

2018
01:18:36,480 --> 01:18:39,679
mammalian cortex with the avian

2019
01:18:39,679 --> 01:18:41,920
pallium brain region

2020
01:18:41,920 --> 01:18:44,000
and so just the way that people work out

2021
01:18:44,000 --> 01:18:46,800
these kinds of circuit diagrams

2022
01:18:46,800 --> 01:18:50,320
like by looking at the actual tissue

2023
01:18:50,320 --> 01:18:52,320
and then here's the statistics and so

2024
01:18:52,320 --> 01:18:55,120
how does the tissue

2025
01:18:55,120 --> 01:18:57,040
relate to

2026
01:18:57,040 --> 01:18:58,640
the statistical model but that's what

2027
01:18:58,640 --> 01:19:00,480
they explore in figure 2.

2028
01:19:00,480 --> 01:19:03,839
okay any thoughts

2029
01:19:05,360 --> 01:19:07,440
ok

2030
01:19:07,440 --> 01:19:08,960
in section 3

2031
01:19:08,960 --> 01:19:11,760
they continue to review empirical work

2032
01:19:11,760 --> 01:19:14,000
but with more of a focus on machine

2033
01:19:14,000 --> 01:19:14,960
learning

2034
01:19:14,960 --> 01:19:17,280
of the unsupervised and supervised

2035
01:19:17,280 --> 01:19:18,719
setting

2036
01:19:18,719 --> 01:19:20,719
and they're going to also explore how

2037
01:19:20,719 --> 01:19:22,560
the predictive coding architecture can

2038
01:19:22,560 --> 01:19:24,640
be made more biologically plausible by

2039
01:19:24,640 --> 01:19:27,040
relaxing certain assumptions implicit in

2040
01:19:27,040 --> 01:19:29,120
this canonical model

2041
01:19:29,120 --> 01:19:33,360
and introduce action into the picture

2042
01:19:33,360 --> 01:19:35,360
figure three

2043
01:19:35,360 --> 01:19:37,360
they summarize a few different what they

2044
01:19:37,360 --> 01:19:40,719
call paradigms of predictive coding

2045
01:19:40,719 --> 01:19:42,000
summary of the input output

2046
01:19:42,000 --> 01:19:43,520
relationships for each paradigm of

2047
01:19:43,520 --> 01:19:45,040
predictive coding

2048
01:19:45,040 --> 01:19:47,600
so it's the input and the prediction the

2049
01:19:47,600 --> 01:19:50,480
output of different paradigms

2050
01:19:50,480 --> 01:19:52,080
so here's the classical predictive

2051
01:19:52,080 --> 01:19:52,960
coding

2052
01:19:52,960 --> 01:19:55,520
in observation data are coming in

2053
01:19:55,520 --> 01:19:57,679
predictions about those observations are

2054
01:19:57,679 --> 01:19:59,760
coming out it's an instantaneous

2055
01:19:59,760 --> 01:20:02,400
real-time snapshot

2056
01:20:02,400 --> 01:20:05,199
the real-time anticipatory unfolding is

2057
01:20:05,199 --> 01:20:07,199
still getting data as input

2058
01:20:07,199 --> 01:20:09,040
but it's predicting the observation at

2059
01:20:09,040 --> 01:20:11,440
the next time step which is a similar

2060
01:20:11,440 --> 01:20:13,040
problem to predicting it at this time

2061
01:20:13,040 --> 01:20:14,719
step but it's like a little bit

2062
01:20:14,719 --> 01:20:16,080
different

2063
01:20:16,080 --> 01:20:18,159
that can happen not just through time

2064
01:20:18,159 --> 01:20:20,719
but also in the context of space with

2065
01:20:20,719 --> 01:20:23,440
like a spatial predictive coding

2066
01:20:23,440 --> 01:20:25,360
and then these two on the right

2067
01:20:25,360 --> 01:20:27,280
supervised predictive coding in the

2068
01:20:27,280 --> 01:20:28,880
generative and in the discriminative

2069
01:20:28,880 --> 01:20:30,000
direction

2070
01:20:30,000 --> 01:20:32,239
have to do with the relationship with

2071
01:20:32,239 --> 01:20:34,239
reading in labels

2072
01:20:34,239 --> 01:20:36,639
and giving out observations

2073
01:20:36,639 --> 01:20:38,080
or vice versa

2074
01:20:38,080 --> 01:20:39,600
make me a cat

2075
01:20:39,600 --> 01:20:41,600
here's a picture of a cat

2076
01:20:41,600 --> 01:20:43,040
here's a picture

2077
01:20:43,040 --> 01:20:44,400
that's a cat

2078
01:20:44,400 --> 01:20:45,520
so these are some of the ways that

2079
01:20:45,520 --> 01:20:46,960
predictive coding models have been

2080
01:20:46,960 --> 01:20:49,360
applied

2081
01:20:49,360 --> 01:20:53,120
3.1 they explore the

2082
01:20:53,120 --> 01:20:55,120
way that unsupervised training relates

2083
01:20:55,120 --> 01:20:57,040
to predictive coding

2084
01:20:57,040 --> 01:21:00,960
so that is machine learning on unlabeled

2085
01:21:00,960 --> 01:21:03,840
data like here's

2086
01:21:03,840 --> 01:21:05,440
a hundred thousand hours of human

2087
01:21:05,440 --> 01:21:08,159
speaking learn how to speak that's like

2088
01:21:08,159 --> 01:21:10,400
an unsupervised approach whereas the

2089
01:21:10,400 --> 01:21:12,400
supervised approach would be here's a

2090
01:21:12,400 --> 01:21:14,400
thousand hours of human speaking here's

2091
01:21:14,400 --> 01:21:16,560
a thousand hours of a river here's a

2092
01:21:16,560 --> 01:21:19,120
thousand hours of you know of some other

2093
01:21:19,120 --> 01:21:21,600
noise and so by the label

2094
01:21:21,600 --> 01:21:23,679
then the algorithm would learn what is

2095
01:21:23,679 --> 01:21:25,679
speech versus non-speech versus the

2096
01:21:25,679 --> 01:21:27,679
unsupervised but there's also a lot of

2097
01:21:27,679 --> 01:21:29,840
like continuum and complexity and it's a

2098
01:21:29,840 --> 01:21:31,760
whole topic

2099
01:21:31,760 --> 01:21:33,280
um in

2100
01:21:33,280 --> 01:21:35,920
some of the sub sections of 3.1 they go

2101
01:21:35,920 --> 01:21:37,280
a little bit deeper

2102
01:21:37,280 --> 01:21:39,120
into the

2103
01:21:39,120 --> 01:21:40,159
temporal

2104
01:21:40,159 --> 01:21:42,719
predictive and the auto encoding aspects

2105
01:21:42,719 --> 01:21:44,880
but we're not gonna go into those right

2106
01:21:44,880 --> 01:21:46,639
now

2107
01:21:46,639 --> 01:21:50,080
in figure four in section three two

2108
01:21:50,080 --> 01:21:52,239
they look at supervised predictive

2109
01:21:52,239 --> 01:21:54,800
coding and look at it in that forwards

2110
01:21:54,800 --> 01:21:56,400
and backwards direction

2111
01:21:56,400 --> 01:21:58,560
and so here they're looking at the

2112
01:21:58,560 --> 01:22:01,679
classic mnist data set for

2113
01:22:01,679 --> 01:22:04,080
evaluation where there's some pictures

2114
01:22:04,080 --> 01:22:06,239
of handwritten digits and then it's

2115
01:22:06,239 --> 01:22:09,360
predicting and so here's the

2116
01:22:09,360 --> 01:22:11,199
generative predictive coding and the

2117
01:22:11,199 --> 01:22:12,960
discriminative direction

2118
01:22:12,960 --> 01:22:15,920
where here the data are coming in like

2119
01:22:15,920 --> 01:22:17,840
the picture of the five with the pixel

2120
01:22:17,840 --> 01:22:19,120
intensities

2121
01:22:19,120 --> 01:22:21,440
and that is relating to the more high

2122
01:22:21,440 --> 01:22:24,159
level estimate of it being a five so

2123
01:22:24,159 --> 01:22:26,320
this is observation in

2124
01:22:26,320 --> 01:22:28,880
label out that's a five

2125
01:22:28,880 --> 01:22:32,080
here's label in

2126
01:22:32,159 --> 01:22:35,599
pixels reflecting it out

2127
01:22:35,760 --> 01:22:37,920
so that's connecting machine learning to

2128
01:22:37,920 --> 01:22:40,800
predictive coding

2129
01:22:40,800 --> 01:22:42,560
in section 3-3

2130
01:22:42,560 --> 01:22:45,840
they're also exploring some

2131
01:22:45,840 --> 01:22:48,320
relaxed predictive coding

2132
01:22:48,320 --> 01:22:51,280
and it has to do with relaxing certain

2133
01:22:51,280 --> 01:22:53,760
mathematical assumptions that they

2134
01:22:53,760 --> 01:22:55,840
had introduced earlier for simplicity

2135
01:22:55,840 --> 01:22:57,280
and for clarity

2136
01:22:57,280 --> 01:22:58,400
we're not going to

2137
01:22:58,400 --> 01:23:00,560
go into it but it has to do

2138
01:23:00,560 --> 01:23:02,080
with

2139
01:23:02,080 --> 01:23:03,760
making it more

2140
01:23:03,760 --> 01:23:05,920
useful in the real world by relaxing

2141
01:23:05,920 --> 01:23:08,719
certain assumptions

2142
01:23:09,199 --> 01:23:10,800
and then in three four

2143
01:23:10,800 --> 01:23:11,920
they

2144
01:23:11,920 --> 01:23:12,800
uh

2145
01:23:12,800 --> 01:23:14,960
explore deep predictive code

2146
01:23:14,960 --> 01:23:17,199
so they write so far in this review

2147
01:23:17,199 --> 01:23:19,040
we've only considered direct variations

2148
01:23:19,040 --> 01:23:21,520
on the fristen round bollard models of

2149
01:23:21,520 --> 01:23:23,600
predictive coding which are relatively

2150
01:23:23,600 --> 01:23:24,800
pure

2151
01:23:24,800 --> 01:23:26,719
and use only local biologically

2152
01:23:26,719 --> 01:23:28,719
plausible learning rules

2153
01:23:28,719 --> 01:23:30,400
there also exists a small literature

2154
01:23:30,400 --> 01:23:32,480
experimenting with these models often

2155
01:23:32,480 --> 01:23:34,239
achieve better performance on more

2156
01:23:34,239 --> 01:23:36,320
challenging tasks than the purer models

2157
01:23:36,320 --> 01:23:38,719
can achieve so pure again here meaning

2158
01:23:38,719 --> 01:23:41,280
that it uses only local biologically

2159
01:23:41,280 --> 01:23:43,840
plausible learning rules so only

2160
01:23:43,840 --> 01:23:46,880
connected entities locally

2161
01:23:46,880 --> 01:23:48,400
and so

2162
01:23:48,400 --> 01:23:50,000
these models they suggest provide a

2163
01:23:50,000 --> 01:23:51,760
vital threat of evidence about scaling

2164
01:23:51,760 --> 01:23:53,679
properties and performance of deep and

2165
01:23:53,679 --> 01:23:56,400
complex predictive coding networks

2166
01:23:56,400 --> 01:23:57,199
and

2167
01:23:57,199 --> 01:23:58,800
they write the first major work in this

2168
01:23:58,800 --> 01:24:00,719
area is prednet

2169
01:24:00,719 --> 01:24:02,880
which uses multiple layers of recurrent

2170
01:24:02,880 --> 01:24:06,080
convolutional lstms neural networks to

2171
01:24:06,080 --> 01:24:08,000
implement a deep predictive coding

2172
01:24:08,000 --> 01:24:09,120
network

2173
01:24:09,120 --> 01:24:11,040
so here is what

2174
01:24:11,040 --> 01:24:14,239
prednet looked like

2175
01:24:15,360 --> 01:24:16,880
this is the original prednet

2176
01:24:16,880 --> 01:24:18,719
architecture

2177
01:24:18,719 --> 01:24:22,400
and so they have convolutional lstms

2178
01:24:22,400 --> 01:24:24,480
that are passing information in a

2179
01:24:24,480 --> 01:24:25,760
top-down

2180
01:24:25,760 --> 01:24:28,560
way and in a bottom-up way

2181
01:24:28,560 --> 01:24:31,360
so maybe we could look at it more later

2182
01:24:31,360 --> 01:24:33,040
and then i also just

2183
01:24:33,040 --> 01:24:37,840
found this 2019 2020 paper um where they

2184
01:24:37,840 --> 01:24:39,920
also explored like whether prednet

2185
01:24:39,920 --> 01:24:42,239
actually is doing predictive coding but

2186
01:24:42,239 --> 01:24:44,800
again a whole another rabbit hole

2187
01:24:44,800 --> 01:24:46,560
so section three

2188
01:24:46,560 --> 01:24:49,040
reviewed through figure three and some

2189
01:24:49,040 --> 01:24:51,520
discussions on the subsections a few of

2190
01:24:51,520 --> 01:24:53,280
the different paradigms and the ways

2191
01:24:53,280 --> 01:24:55,280
this predictive coding architecture has

2192
01:24:55,280 --> 01:24:57,760
been applied

2193
01:24:57,760 --> 01:24:59,840
in section 4 they're going to connect it

2194
01:24:59,840 --> 01:25:02,159
to other algorithms so we're going to

2195
01:25:02,159 --> 01:25:03,679
basically speed through all of the

2196
01:25:03,679 --> 01:25:07,280
connections except for active inference

2197
01:25:07,280 --> 01:25:09,120
first they connect predictive coding in

2198
01:25:09,120 --> 01:25:11,600
section 41 to the back propagation of

2199
01:25:11,600 --> 01:25:13,120
error

2200
01:25:13,120 --> 01:25:15,040
this is really important like for neural

2201
01:25:15,040 --> 01:25:17,040
network training

2202
01:25:17,040 --> 01:25:18,400
but we're not going to talk about it

2203
01:25:18,400 --> 01:25:19,760
here but it sounds like really

2204
01:25:19,760 --> 01:25:23,040
interesting to discuss

2205
01:25:23,600 --> 01:25:25,760
one that we'll introduce but then skip

2206
01:25:25,760 --> 01:25:27,840
over most of the details is the

2207
01:25:27,840 --> 01:25:29,520
relationship between linear predictive

2208
01:25:29,520 --> 01:25:32,159
coding and calm and filtering so we can

2209
01:25:32,159 --> 01:25:34,000
remember that the linear predictive

2210
01:25:34,000 --> 01:25:35,679
coding is not just the frame

2211
01:25:35,679 --> 01:25:37,040
differencing it's where there's like

2212
01:25:37,040 --> 01:25:39,040
that coefficient of how much you should

2213
01:25:39,040 --> 01:25:42,400
forget each of the previous frames

2214
01:25:42,400 --> 01:25:44,960
and this is known in signal processing

2215
01:25:44,960 --> 01:25:47,199
as the kalman filter

2216
01:25:47,199 --> 01:25:50,239
the column filter as this slide shows

2217
01:25:50,239 --> 01:25:52,400
is an iterative mathematical process to

2218
01:25:52,400 --> 01:25:55,280
quickly estimate true values position

2219
01:25:55,280 --> 01:25:57,840
and velocity so that's also like getting

2220
01:25:57,840 --> 01:25:59,360
towards the generalized coordinates of

2221
01:25:59,360 --> 01:26:01,760
motion and so the x's are like our

2222
01:26:01,760 --> 01:26:04,320
observations that's the thermometer

2223
01:26:04,320 --> 01:26:06,159
and then here's the red

2224
01:26:06,159 --> 01:26:09,360
is our calm and filtered temperature

2225
01:26:09,360 --> 01:26:11,280
unfolding through time

2226
01:26:11,280 --> 01:26:12,880
so our initial estimate gets quickly

2227
01:26:12,880 --> 01:26:14,880
corrected and then even though there's a

2228
01:26:14,880 --> 01:26:18,000
lot of noise in the observations

2229
01:26:18,000 --> 01:26:19,920
the kalman filter is like giving a good

2230
01:26:19,920 --> 01:26:22,719
prediction on the actual temperature

2231
01:26:22,719 --> 01:26:25,520
and this has been implemented in a ton

2232
01:26:25,520 --> 01:26:27,679
of bayesian approaches it's a totally

2233
01:26:27,679 --> 01:26:30,159
standard signal processing bayesian

2234
01:26:30,159 --> 01:26:31,440
approach

2235
01:26:31,440 --> 01:26:35,520
and in the following sections in 4.2

2236
01:26:35,520 --> 01:26:37,920
they have a lot of formalisms

2237
01:26:37,920 --> 01:26:40,239
about the kalman filter

2238
01:26:40,239 --> 01:26:43,760
and in appendix d they provide even more

2239
01:26:43,760 --> 01:26:46,960
formalisms on the kalman filter

2240
01:26:46,960 --> 01:26:48,239
so we're not going to really go into

2241
01:26:48,239 --> 01:26:51,040
either of those

2242
01:26:51,040 --> 01:26:53,040
in section 4.3

2243
01:26:53,040 --> 01:26:54,880
they introduced this idea of

2244
01:26:54,880 --> 01:26:57,760
normalization and normalizing flows

2245
01:26:57,760 --> 01:26:59,199
and so they wrote

2246
01:26:59,199 --> 01:27:00,880
the deep link between predictive coding

2247
01:27:00,880 --> 01:27:02,960
and normalization has been extended by

2248
01:27:02,960 --> 01:27:04,719
mourinho 2020

2249
01:27:04,719 --> 01:27:06,480
by situating predictive coding with

2250
01:27:06,480 --> 01:27:07,840
general recipe for building or

2251
01:27:07,840 --> 01:27:10,480
representing a complex distribution from

2252
01:27:10,480 --> 01:27:13,040
a simple intractable one

2253
01:27:13,040 --> 01:27:15,199
so we encourage people to look at this

2254
01:27:15,199 --> 01:27:18,560
merino 2020 paper if they want to learn

2255
01:27:18,560 --> 01:27:21,280
more about predictive coding variational

2256
01:27:21,280 --> 01:27:24,880
auto encoders and biological connections

2257
01:27:24,880 --> 01:27:27,920
but not going to discuss today

2258
01:27:27,920 --> 01:27:29,360
one other model that they connect

2259
01:27:29,360 --> 01:27:30,880
predictive coding to

2260
01:27:30,880 --> 01:27:34,719
is known as the biased competition model

2261
01:27:34,719 --> 01:27:35,840
and

2262
01:27:35,840 --> 01:27:38,880
that is unpacked in 4.4 but

2263
01:27:38,880 --> 01:27:41,520
i just wanted to pull up one example of

2264
01:27:41,520 --> 01:27:44,080
this biased competition model

2265
01:27:44,080 --> 01:27:46,880
and so this is from the sprottling paper

2266
01:27:46,880 --> 01:27:48,560
of 2008

2267
01:27:48,560 --> 01:27:50,800
within each processing stage nodes

2268
01:27:50,800 --> 01:27:53,120
compete to be active in response to the

2269
01:27:53,120 --> 01:27:56,000
current pattern of feed-forward activity

2270
01:27:56,000 --> 01:27:57,840
received from the sensory input or

2271
01:27:57,840 --> 01:28:00,960
previous processing stage so

2272
01:28:00,960 --> 01:28:02,880
it's like there's competitions amongst

2273
01:28:02,880 --> 01:28:05,920
the sub units to be activated by certain

2274
01:28:05,920 --> 01:28:07,679
kinds of inputs

2275
01:28:07,679 --> 01:28:12,000
and then there's some technical details

2276
01:28:12,000 --> 01:28:13,280
okay

2277
01:28:13,280 --> 01:28:16,159
now finally to active inference as we're

2278
01:28:16,159 --> 01:28:20,000
near the end of the discussion

2279
01:28:20,880 --> 01:28:22,800
this will be a little fun to explore

2280
01:28:22,800 --> 01:28:24,880
also in the coming weeks of course so

2281
01:28:24,880 --> 01:28:25,840
they write

2282
01:28:25,840 --> 01:28:28,560
in 4.5 predictive coding can also be

2283
01:28:28,560 --> 01:28:31,920
extended to include action

2284
01:28:33,600 --> 01:28:35,679
allowing for predictive coding agents to

2285
01:28:35,679 --> 01:28:37,840
undertake adaptive actions without any

2286
01:28:37,840 --> 01:28:39,600
major change to their fundamental

2287
01:28:39,600 --> 01:28:41,120
algorithms

2288
01:28:41,120 --> 01:28:43,280
the key insight is to note there are two

2289
01:28:43,280 --> 01:28:46,159
ways of minimizing prediction errors

2290
01:28:46,159 --> 01:28:48,239
the first is to update predictions to

2291
01:28:48,239 --> 01:28:50,880
match sensory data which corresponds to

2292
01:28:50,880 --> 01:28:53,920
classical perception

2293
01:28:53,920 --> 01:28:55,600
i thought that the ball was going to be

2294
01:28:55,600 --> 01:28:58,480
here but i guess it's over here

2295
01:28:58,480 --> 01:28:59,920
perception

2296
01:28:59,920 --> 01:29:01,679
the second is to take actions in the

2297
01:29:01,679 --> 01:29:04,080
world to force the incoming sensory data

2298
01:29:04,080 --> 01:29:05,920
to match the prediction

2299
01:29:05,920 --> 01:29:07,679
i'm going to move the ball to where i

2300
01:29:07,679 --> 01:29:09,199
expect it to be

2301
01:29:09,199 --> 01:29:11,760
or i expected the ball to be in the left

2302
01:29:11,760 --> 01:29:13,520
side of my visual field

2303
01:29:13,520 --> 01:29:15,520
so i'm going to move my eyes to the

2304
01:29:15,520 --> 01:29:16,400
right

2305
01:29:16,400 --> 01:29:17,840
so that it's in the left side of my

2306
01:29:17,840 --> 01:29:20,000
visual field so it's action is not

2307
01:29:20,000 --> 01:29:21,840
always like reaching out and moving the

2308
01:29:21,840 --> 01:29:22,800
ball

2309
01:29:22,800 --> 01:29:25,120
it can also be the action of choosing

2310
01:29:25,120 --> 01:29:28,000
where to look for example

2311
01:29:28,000 --> 01:29:30,320
and this is what is really fascinating

2312
01:29:30,320 --> 01:29:33,360
because we spent so long in the earlier

2313
01:29:33,360 --> 01:29:35,520
sections in the first 50 equations you

2314
01:29:35,520 --> 01:29:36,400
know

2315
01:29:36,400 --> 01:29:37,760
without action

2316
01:29:37,760 --> 01:29:39,440
so it's like oh no are there going to be

2317
01:29:39,440 --> 01:29:42,480
like another 250 equations with action

2318
01:29:42,480 --> 01:29:43,920
coming into the picture

2319
01:29:43,920 --> 01:29:46,239
but actually it's quite a simple

2320
01:29:46,239 --> 01:29:47,520
introduction

2321
01:29:47,520 --> 01:29:49,520
the basic approach to including action

2322
01:29:49,520 --> 01:29:51,840
in predictive coding is to minimize the

2323
01:29:51,840 --> 01:29:53,920
variational free energy with respect to

2324
01:29:53,920 --> 01:29:55,199
action

2325
01:29:55,199 --> 01:29:57,600
although free energy is not explicitly a

2326
01:29:57,600 --> 01:29:59,440
function of action it can be made so

2327
01:29:59,440 --> 01:30:00,800
implicitly

2328
01:30:00,800 --> 01:30:02,960
by noticing the dependence of sensory

2329
01:30:02,960 --> 01:30:05,040
observations on actions

2330
01:30:05,040 --> 01:30:07,760
so where what you see is going to depend

2331
01:30:07,760 --> 01:30:10,159
on where you look

2332
01:30:10,159 --> 01:30:12,239
we can make this implicit dependent

2333
01:30:12,239 --> 01:30:14,719
dependence explicit using the chain rule

2334
01:30:14,719 --> 01:30:17,199
of calculus

2335
01:30:17,199 --> 01:30:19,520
active inference it's inference about

2336
01:30:19,520 --> 01:30:21,199
action

2337
01:30:21,199 --> 01:30:22,320
so here's

2338
01:30:22,320 --> 01:30:25,040
the rate of change of action

2339
01:30:25,040 --> 01:30:26,880
through time

2340
01:30:26,880 --> 01:30:28,880
is the rate of change of a free energy

2341
01:30:28,880 --> 01:30:30,880
functional of

2342
01:30:30,880 --> 01:30:34,719
previously it was of observations and of

2343
01:30:34,719 --> 01:30:36,719
states through time

2344
01:30:36,719 --> 01:30:38,080
and now

2345
01:30:38,080 --> 01:30:41,520
o is o of a observations are a function

2346
01:30:41,520 --> 01:30:43,520
of action they're partially or

2347
01:30:43,520 --> 01:30:46,080
completely dependent on action

2348
01:30:46,080 --> 01:30:48,960
and that is being understood as a

2349
01:30:48,960 --> 01:30:52,800
derivative of change in action

2350
01:30:52,800 --> 01:30:57,040
o of a over a partial of a over a

2351
01:30:57,040 --> 01:30:59,120
is a forward model which makes explicit

2352
01:30:59,120 --> 01:31:01,199
the dependence of the observation upon

2353
01:31:01,199 --> 01:31:03,280
action and must be provided or learned

2354
01:31:03,280 --> 01:31:05,679
by the act by the algorithm

2355
01:31:05,679 --> 01:31:06,960
because that is

2356
01:31:06,960 --> 01:31:09,040
how do observations

2357
01:31:09,040 --> 01:31:12,639
which are a function of action change

2358
01:31:12,639 --> 01:31:14,560
as actions change

2359
01:31:14,560 --> 01:31:16,960
so that is like a layer that hadn't been

2360
01:31:16,960 --> 01:31:19,360
brought up so they're saying because you

2361
01:31:19,360 --> 01:31:21,120
do need to solve that ratio or that

2362
01:31:21,120 --> 01:31:22,400
derivative

2363
01:31:22,400 --> 01:31:24,320
you do need to like learn that or be

2364
01:31:24,320 --> 01:31:25,760
provided that

2365
01:31:25,760 --> 01:31:27,679
and it's a separate

2366
01:31:27,679 --> 01:31:29,440
thing to learn in addition to this

2367
01:31:29,440 --> 01:31:32,159
standard generative model for perception

2368
01:31:32,159 --> 01:31:34,239
so we talk action

2369
01:31:34,239 --> 01:31:36,400
as being like a dependency

2370
01:31:36,400 --> 01:31:38,080
of observations

2371
01:31:38,080 --> 01:31:39,920
which is also what connects active

2372
01:31:39,920 --> 01:31:42,480
inference to perceptual control theory

2373
01:31:42,480 --> 01:31:44,800
so that's how we introduce action into

2374
01:31:44,800 --> 01:31:47,360
the picture just from a first

2375
01:31:47,360 --> 01:31:49,679
pass

2376
01:31:50,400 --> 01:31:52,800
let's go one more um

2377
01:31:52,800 --> 01:31:55,760
level in and then any thoughts you have

2378
01:31:55,760 --> 01:31:58,639
would be great so

2379
01:31:58,639 --> 01:32:01,040
if we were talking about just

2380
01:32:01,040 --> 01:32:03,280
the inference case

2381
01:32:03,280 --> 01:32:04,880
prediction error minimization would be

2382
01:32:04,880 --> 01:32:06,800
like the picture that we're seeing is

2383
01:32:06,800 --> 01:32:08,560
exactly the one that we're predicting or

2384
01:32:08,560 --> 01:32:11,120
vice versa so what does that look like

2385
01:32:11,120 --> 01:32:13,040
for action

2386
01:32:13,040 --> 01:32:14,880
the prediction error simply becomes the

2387
01:32:14,880 --> 01:32:16,000
difference between the current

2388
01:32:16,000 --> 01:32:18,800
observation and the target or set point

2389
01:32:18,800 --> 01:32:20,320
already bringing in this kind of

2390
01:32:20,320 --> 01:32:23,120
homeostasis allostasis perspective

2391
01:32:23,120 --> 01:32:24,880
however this raises the question of

2392
01:32:24,880 --> 01:32:27,280
where the set points comes from

2393
01:32:27,280 --> 01:32:28,800
where do the targets come from and how

2394
01:32:28,800 --> 01:32:30,480
are they computed

2395
01:32:30,480 --> 01:32:32,719
whence priors

2396
01:32:32,719 --> 01:32:34,639
a generic answer to the question is that

2397
01:32:34,639 --> 01:32:36,080
set points can be inherited from

2398
01:32:36,080 --> 01:32:38,400
evolutionary or ontogenetic which are

2399
01:32:38,400 --> 01:32:40,560
developmental imperatives

2400
01:32:40,560 --> 01:32:42,480
or supplied by other neural circuits

2401
01:32:42,480 --> 01:32:44,159
involved in goal-directed behavior and

2402
01:32:44,159 --> 01:32:45,040
planning

2403
01:32:45,040 --> 01:32:47,120
for present purposes we can simply take

2404
01:32:47,120 --> 01:32:50,320
them as exogenously given variables so

2405
01:32:50,320 --> 01:32:52,719
what isn't addressed in this review is

2406
01:32:52,719 --> 01:32:54,400
this big question of

2407
01:32:54,400 --> 01:32:57,679
priors and learning on preferences like

2408
01:32:57,679 --> 01:32:59,920
how do you know what to prefer

2409
01:32:59,920 --> 01:33:01,600
how do you know how much to change how

2410
01:33:01,600 --> 01:33:02,719
you prefer

2411
01:33:02,719 --> 01:33:04,880
how tight should your preferences be

2412
01:33:04,880 --> 01:33:06,560
those are like really really important

2413
01:33:06,560 --> 01:33:08,960
areas but they're just like opening the

2414
01:33:08,960 --> 01:33:12,480
door but not going to go into it

2415
01:33:12,719 --> 01:33:15,600
in 4.5.1 it's also straightforward to

2416
01:33:15,600 --> 01:33:19,360
model the potential costs of action

2417
01:33:19,360 --> 01:33:20,960
in biological organisms there's

2418
01:33:20,960 --> 01:33:23,120
different costs of action

2419
01:33:23,120 --> 01:33:24,560
and so how can that be modeled

2420
01:33:24,560 --> 01:33:27,360
mathematically

2421
01:33:28,000 --> 01:33:29,920
well they say by

2422
01:33:29,920 --> 01:33:32,159
explicitly including action within the

2423
01:33:32,159 --> 01:33:34,320
generative model as follows

2424
01:33:34,320 --> 01:33:36,239
and so now they've added in this extra

2425
01:33:36,239 --> 01:33:39,120
term with a cost of action

2426
01:33:39,120 --> 01:33:40,960
so it just shows how like once action

2427
01:33:40,960 --> 01:33:43,840
has been introduced as a dependency

2428
01:33:43,840 --> 01:33:46,159
of the observation function

2429
01:33:46,159 --> 01:33:48,840
then there can be like

2430
01:33:48,840 --> 01:33:51,280
other things that can be calculated

2431
01:33:51,280 --> 01:33:54,400
related to action

2432
01:33:54,880 --> 01:33:58,159
any thoughts on action or like in your

2433
01:33:58,159 --> 01:34:00,400
readings where has action been

2434
01:34:00,400 --> 01:34:03,440
integrated well or not

2435
01:34:03,440 --> 01:34:04,480
with

2436
01:34:04,480 --> 01:34:07,440
predictive coding

2437
01:34:09,520 --> 01:34:11,040
um

2438
01:34:11,040 --> 01:34:14,719
well i think that

2439
01:34:14,960 --> 01:34:16,320
from the

2440
01:34:16,320 --> 01:34:18,159
from my perspective

2441
01:34:18,159 --> 01:34:20,080
predictive coding

2442
01:34:20,080 --> 01:34:22,880
wasn't really about action before this

2443
01:34:22,880 --> 01:34:25,360
text so it was

2444
01:34:25,360 --> 01:34:27,679
new to me because i always relate to

2445
01:34:27,679 --> 01:34:30,639
action to predictive processing and

2446
01:34:30,639 --> 01:34:33,040
active inference not really predictive

2447
01:34:33,040 --> 01:34:34,880
coding so it was new

2448
01:34:34,880 --> 01:34:36,639
and

2449
01:34:36,639 --> 01:34:39,360
from all of these that we've been

2450
01:34:39,360 --> 01:34:43,119
through uh i found fascinating how

2451
01:34:43,119 --> 01:34:46,320
the complexity grows and how many

2452
01:34:46,320 --> 01:34:48,719
different applications predictive coding

2453
01:34:48,719 --> 01:34:51,760
can be found on especially in machine

2454
01:34:51,760 --> 01:34:53,360
learning and

2455
01:34:53,360 --> 01:34:55,040
you know all of

2456
01:34:55,040 --> 01:34:56,239
this

2457
01:34:56,239 --> 01:34:58,719
uh mathematical

2458
01:34:58,719 --> 01:35:00,800
um

2459
01:35:00,800 --> 01:35:02,800
heaviness

2460
01:35:02,800 --> 01:35:06,560
uh brings many many different solutions

2461
01:35:06,560 --> 01:35:09,199
to problems that we face for a long time

2462
01:35:09,199 --> 01:35:09,920
so

2463
01:35:09,920 --> 01:35:11,119
fascinating

2464
01:35:11,119 --> 01:35:13,920
nice yeah agreed it's really interesting

2465
01:35:13,920 --> 01:35:16,159
so just one more take on active

2466
01:35:16,159 --> 01:35:18,639
inference which we explored in 26 as

2467
01:35:18,639 --> 01:35:20,880
well

2468
01:35:21,280 --> 01:35:24,800
like predictive coding pid control

2469
01:35:24,800 --> 01:35:26,480
optimizes the system towards the set

2470
01:35:26,480 --> 01:35:28,800
point and is ideal for simple regulatory

2471
01:35:28,800 --> 01:35:30,960
systems like thermostats

2472
01:35:30,960 --> 01:35:32,880
action aft

2473
01:35:32,880 --> 01:35:36,000
so we're taking that same notion of a as

2474
01:35:36,000 --> 01:35:37,440
being like

2475
01:35:37,440 --> 01:35:39,199
something that influences observations

2476
01:35:39,199 --> 01:35:40,960
so that's how we introduce action into

2477
01:35:40,960 --> 01:35:43,280
the model is we make observations depend

2478
01:35:43,280 --> 01:35:45,199
on action

2479
01:35:45,199 --> 01:35:47,520
action is determined by three terms

2480
01:35:47,520 --> 01:35:49,520
a proportional term which minimizes the

2481
01:35:49,520 --> 01:35:51,360
distance between the current location

2482
01:35:51,360 --> 01:35:52,960
and the set point

2483
01:35:52,960 --> 01:35:53,920
p

2484
01:35:53,920 --> 01:35:55,679
an integral term which minimizes the

2485
01:35:55,679 --> 01:35:58,560
integral of this error over time i

2486
01:35:58,560 --> 01:36:00,320
and a derivative term which minimizes

2487
01:36:00,320 --> 01:36:03,119
the derivative of the error d

2488
01:36:03,119 --> 01:36:04,639
the combination of the three terms

2489
01:36:04,639 --> 01:36:07,040
produces a robust and simple control

2490
01:36:07,040 --> 01:36:09,199
system which can be applied with some

2491
01:36:09,199 --> 01:36:11,280
tuning to control almost any simple

2492
01:36:11,280 --> 01:36:13,199
regulatory process

2493
01:36:13,199 --> 01:36:15,520
and higher coordinates of motion could

2494
01:36:15,520 --> 01:36:18,400
do even better but as we explored in 26

2495
01:36:18,400 --> 01:36:20,960
like often the pid is sufficient

2496
01:36:20,960 --> 01:36:24,239
and so that's a very common engineering

2497
01:36:24,239 --> 01:36:26,400
method and so it connects

2498
01:36:26,400 --> 01:36:28,800
the bayesian mechanics of action and

2499
01:36:28,800 --> 01:36:30,960
perception and cognition of active

2500
01:36:30,960 --> 01:36:33,760
inference on generalized coordinates to

2501
01:36:33,760 --> 01:36:36,960
pid control in the engineering setting

2502
01:36:36,960 --> 01:36:38,639
and then they provide a bunch of other

2503
01:36:38,639 --> 01:36:41,199
formalisms

2504
01:36:41,199 --> 01:36:43,520
in appendix b

2505
01:36:43,520 --> 01:36:46,880
they give some detail on this idea of

2506
01:36:46,880 --> 01:36:48,960
natural gradients

2507
01:36:48,960 --> 01:36:51,119
which i think we could go into in the

2508
01:36:51,119 --> 01:36:53,440
dot one and in the dot two

2509
01:36:53,440 --> 01:36:55,360
so we won't talk about it here but

2510
01:36:55,360 --> 01:36:57,360
appendix b is about precision as natural

2511
01:36:57,360 --> 01:36:58,560
gradients

2512
01:36:58,560 --> 01:37:00,719
and appendix c

2513
01:37:00,719 --> 01:37:02,800
are providing some challenges for the

2514
01:37:02,800 --> 01:37:04,400
neural implementation of back

2515
01:37:04,400 --> 01:37:07,199
propagation by predictive coding so also

2516
01:37:07,199 --> 01:37:09,199
we're not gonna explore it here but

2517
01:37:09,199 --> 01:37:12,159
those are both really interesting short

2518
01:37:12,159 --> 01:37:15,600
and very topical appendices

2519
01:37:15,600 --> 01:37:17,920
in section five it's uh

2520
01:37:17,920 --> 01:37:20,000
of quite good length and we're not gonna

2521
01:37:20,000 --> 01:37:23,040
cover actually any of it today because

2522
01:37:23,040 --> 01:37:25,520
it's been a great and long enough dot

2523
01:37:25,520 --> 01:37:28,000
zero but in the dot one and the dot two

2524
01:37:28,000 --> 01:37:29,840
and beyond we hope to unpack the

2525
01:37:29,840 --> 01:37:32,080
discussion and the future

2526
01:37:32,080 --> 01:37:33,360
directions

2527
01:37:33,360 --> 01:37:34,560
so

2528
01:37:34,560 --> 01:37:37,280
we have a lot of room and space for

2529
01:37:37,280 --> 01:37:38,480
questions

2530
01:37:38,480 --> 01:37:40,800
and i think we both or all walk out of

2531
01:37:40,800 --> 01:37:44,080
this with more questions than answers

2532
01:37:44,080 --> 01:37:47,119
but what would be your closing thoughts

2533
01:37:47,119 --> 01:37:49,839
maria

2534
01:37:51,760 --> 01:37:53,119
um

2535
01:37:53,119 --> 01:37:56,000
i'm not sure what to think now i have to

2536
01:37:56,000 --> 01:37:58,480
digest all the information we have here

2537
01:37:58,480 --> 01:37:59,679
today

2538
01:37:59,679 --> 01:38:01,520
but

2539
01:38:01,520 --> 01:38:03,199
i have this

2540
01:38:03,199 --> 01:38:06,320
this wish to continue uh with the

2541
01:38:06,320 --> 01:38:09,360
understanding of formalisms i want to

2542
01:38:09,360 --> 01:38:12,639
um you know understand better what you

2543
01:38:12,639 --> 01:38:14,800
said here and

2544
01:38:14,800 --> 01:38:18,080
i'm really excited about the biological

2545
01:38:18,080 --> 01:38:20,000
part of

2546
01:38:20,000 --> 01:38:22,239
predictive coding implementation and

2547
01:38:22,239 --> 01:38:24,080
predictive processing

2548
01:38:24,080 --> 01:38:26,639
so as it is not

2549
01:38:26,639 --> 01:38:28,800
really developed in this

2550
01:38:28,800 --> 01:38:31,760
paper i think that the author said

2551
01:38:31,760 --> 01:38:34,400
that uh it's not really explored

2552
01:38:34,400 --> 01:38:35,840
actually

2553
01:38:35,840 --> 01:38:37,040
i

2554
01:38:37,040 --> 01:38:38,960
i have this

2555
01:38:38,960 --> 01:38:43,440
this idea of you know looking to it to

2556
01:38:43,440 --> 01:38:46,719
see what's what's been do been doing

2557
01:38:46,719 --> 01:38:51,119
uh in in the past years

2558
01:38:51,119 --> 01:38:52,800
awesome

2559
01:38:52,800 --> 01:38:55,280
yeah i think it'll be some fun upcoming

2560
01:38:55,280 --> 01:38:58,239
discussions thanks so much for all the

2561
01:38:58,239 --> 01:39:00,800
help and for this dot zero and to brock

2562
01:39:00,800 --> 01:39:02,960
as well so

2563
01:39:02,960 --> 01:39:06,080
see you around thanks a lot

2564
01:39:06,080 --> 01:39:07,119
see you

2565
01:39:07,119 --> 01:39:10,119
bye

2566
01:39:56,320 --> 01:39:58,400
you


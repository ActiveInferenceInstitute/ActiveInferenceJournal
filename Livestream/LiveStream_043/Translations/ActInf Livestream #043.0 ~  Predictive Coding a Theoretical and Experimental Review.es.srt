1
00:00:05,000 --> 00:00:26,800
[Música]

2
00:00:26,800 --> 00:00:29,279
hola y bienvenidos a

3
00:00:29,279 --> 00:00:33,280
todos es el 28 de abril de 2022 y estamos aquí en

4
00:00:33,280 --> 00:00:36,800
la transmisión en vivo número 43.0 de actin flab

5
00:00:36,800 --> 00:00:38,719
discutiendo la codificación predictiva una

6
00:00:38,719 --> 00:00:42,160
revisión experimental teórica

7
00:00:42,160 --> 00:00:44,480
bienvenidos a actin actinflab somos un

8
00:00:44,480 --> 00:00:46,480
laboratorio en línea participativo que está

9
00:00:46,480 --> 00:00:48,399
comunicando el aprendizaje

10
00:00:48,399 --> 00:00:50,559
practicando la inferencia activa aplicada

11
00:00:50,559 --> 00:00:53,120
puede encontrarnos en los enlaces aquí en la diapositiva

12
00:00:53,120 --> 00:00:55,039
esto está grabado en una transmisión en vivo archivada

13
00:00:55,039 --> 00:00:56,640
, así que envíenos sus

14
00:00:56,640 --> 00:01:00,160
comentarios para que podamos mejorar nuestro trabajo

15
00:01:00,160 --> 00:01:01,760
todos los antecedentes y perspectivas son

16
00:01:01,760 --> 00:01:02,719
bienvenidos

17
00:01:02,719 --> 00:01:04,559
y seguiremos la etiqueta de video

18
00:01:04,559 --> 00:01:07,360
para transmisiones en vivo

19
00:01:07,600 --> 00:01:08,840
diríjase a

20
00:01:08,840 --> 00:01:11,040
activeinference.org si desea obtener

21
00:01:11,040 --> 00:01:13,119
más información sobre cómo participar en las

22
00:01:13,119 --> 00:01:15,119
transmisiones en vivo o cualquier otra cosa que suceda

23
00:01:15,119 --> 00:01:18,000
en actin flap,

24
00:01:18,000 --> 00:01:19,280
está bien

25
00:01:19,280 --> 00:01:23,040
, estamos aquí hoy para aprender y discutir

26
00:01:23,040 --> 00:01:25,119
la codificación predictiva en papel, una

27
00:01:25,119 --> 00:01:27,680
revisión teórica y experimental de

28
00:01:27,680 --> 00:01:30,640
baron millage anil seth y christopher

29
00:01:30,640 --> 00:01:32,079
hebillay

30
00:01:32,079 --> 00:01:34,439
el video es solo una introduccion en una

31
00:01:34,439 --> 00:01:36,640
contextualizacion de algunas de las ideas

32
00:01:36,640 --> 00:01:38,720
y algunos de los detalles del pap  er en

33
00:01:38,720 --> 00:01:40,400
el sentido amplio

34
00:01:40,400 --> 00:01:43,360
no es una revisión o una palabra final

35
00:01:43,360 --> 00:01:45,680
es uh como con algunos de estos otros

36
00:01:45,680 --> 00:01:48,079
documentos técnicos densos es como una

37
00:01:48,079 --> 00:01:50,240
apertura para aquellos que tienen

38
00:01:50,240 --> 00:01:52,960
preguntas técnicas en el lado del aprendizaje

39
00:01:52,960 --> 00:01:54,399
o en el

40
00:01:54,399 --> 00:01:56,880
lado de la investigación más avanzada para  Venga y

41
00:01:56,880 --> 00:01:58,719
participe porque este es un

42
00:01:58,719 --> 00:02:01,600
documento técnico, pero también esperamos

43
00:02:01,600 --> 00:02:04,000
que tenga muchas

44
00:02:04,000 --> 00:02:07,600


45
00:02:07,600 --> 00:02:11,599
implicaciones biológicas

46
00:02:11,599 --> 00:02:13,920


47
00:02:13,920 --> 00:02:15,840
y filosóficas

48
00:02:15,840 --> 00:02:18,239
geniales.  y la hoja de ruta y luego,

49
00:02:18,239 --> 00:02:20,160
simplemente salte directamente

50
00:02:20,160 --> 00:02:22,400
y nos centraremos mucho en la

51
00:02:22,400 --> 00:02:24,879
introducción, el contexto y el

52
00:02:24,879 --> 00:02:27,680
modelo de codificación predictiva de una sola capa y luego

53
00:02:27,680 --> 00:02:29,440
avanzaremos un poco más rápido a través de las

54
00:02:29,440 --> 00:02:31,440
últimas secciones del documento hablando

55
00:02:31,440 --> 00:02:33,440
sobre algunas generalizaciones de la

56
00:02:33,440 --> 00:02:35,599
codificación predictiva y algunos puntos importantes que

57
00:02:35,599 --> 00:02:37,440
los autores compiten, por lo que si desea

58
00:02:37,440 --> 00:02:40,800
participar en 43.1 o 0.2 en las próximas

59
00:02:40,800 --> 00:02:43,840
semanas, háganoslo saber,

60
00:02:43,840 --> 00:02:46,959
está bien para que podamos  salude

61
00:02:46,959 --> 00:02:49,599
y brinde cualquier información que nos gustaría

62
00:02:49,599 --> 00:02:52,000
y también tal vez algo que

63
00:02:52,000 --> 00:02:54,000
pensamos que era emocionante o algo que

64
00:02:54,000 --> 00:02:57,440
nos motivó a involucrarnos en este artículo

65
00:02:57,440 --> 00:02:59,200
bastante complicado,

66
00:02:59,200 --> 00:03:01,280
así que soy Daniel, soy un investigador en

67
00:03:01,280 --> 00:03:02,640
California

68
00:03:02,640 --> 00:03:05,360
y tenía curiosidad  para obtener más información sobre

69
00:03:05,360 --> 00:03:07,599
cómo la codificación predictiva y el procesamiento predictivo se

70
00:03:07,599 --> 00:03:10,560
relacionan con la inferencia activa

71
00:03:10,560 --> 00:03:14,080
y también sobre cómo los diferentes modelos

72
00:03:14,080 --> 00:03:16,800
enmarcaron los sistemas anticipatorios,

73
00:03:16,800 --> 00:03:20,080
así que paso a ti maria

74
00:03:21,920 --> 00:03:24,159
hola daniel y a todos

75
00:03:24,159 --> 00:03:26,720
um tengo un título bicolor en

76
00:03:26,720 --> 00:03:29,680
psicología y soy un candidato a maestro

77
00:03:29,680 --> 00:03:31,920
en  filosofía de la ciencia

78
00:03:31,920 --> 00:03:34,720
en la universidad de sao paulo brasil

79
00:03:34,720 --> 00:03:36,159
y

80
00:03:36,159 --> 00:03:38,720
estoy investigando la relación entre el

81
00:03:38,720 --> 00:03:41,360
procesamiento predictivo y las

82
00:03:41,360 --> 00:03:43,200
teorías ilusionistas de la conciencia

83
00:03:43,200 --> 00:03:45,599
y creo que lo que me trajo aquí hoy

84
00:03:45,599 --> 00:03:46,799
fue

85
00:03:46,799 --> 00:03:47,680


86
00:03:47,680 --> 00:03:50,879
mi deseo de comenzar a aprender sobre los

87
00:03:50,879 --> 00:03:53,360
formalismos porque realmente no veo

88
00:03:53,360 --> 00:03:55,599
en filosofía y en realidad no lo

89
00:03:55,599 --> 00:03:58,720
necesito para mi disertación, pero eventualmente

90
00:03:58,720 --> 00:04:01,280
quería continuar con mi trabajo sobre el

91
00:04:01,280 --> 00:04:03,439
procesamiento predictivo, así que  Eventualmente

92
00:04:03,439 --> 00:04:05,599
tuve que comenzarlo y pensé

93
00:04:05,599 --> 00:04:07,760
que tal vez sea una buena

94
00:04:07,760 --> 00:04:10,239
idea venir aquí y minimizar mis

95
00:04:10,239 --> 00:04:12,239
incertidumbres.

96
00:04:12,239 --> 00:04:15,040
Impresionante a través de la inferencia y/o a través de la

97
00:04:15,040 --> 00:04:17,519
acción. También muchas gracias, brock, por

98
00:04:17,519 --> 00:04:19,918
ayudarme en el punto cero,

99
00:04:19,918 --> 00:04:21,600
así que comenzaremos con el  grandes

100
00:04:21,600 --> 00:04:22,880
preguntas y este es el tipo de

101
00:04:22,880 --> 00:04:25,600
preguntas que podría motivar a alguien o

102
00:04:25,600 --> 00:04:28,240
interesarle en este documento sin siquiera

103
00:04:28,240 --> 00:04:30,160
mencionar la inferencia activa per se, pero

104
00:04:30,160 --> 00:04:31,520
estas son como algunas grandes preguntas que

105
00:04:31,520 --> 00:04:33,040
se abordan

106
00:04:33,040 --> 00:04:35,120
cuál es la base formal de la codificación predictiva

107
00:04:35,120 --> 00:04:36,240


108
00:04:36,240 --> 00:04:39,680
cómo se usa la codificación predictiva  o útil,

109
00:04:39,680 --> 00:04:41,520
cuáles son algunas áreas de desarrollo actual y

110
00:04:41,520 --> 00:04:43,759
futuro y cuál es la

111
00:04:43,759 --> 00:04:46,000
relación entre la codificación predictiva

112
00:04:46,000 --> 00:04:48,479
y la inferencia activa y, con suerte,

113
00:04:48,479 --> 00:04:50,560
sacaremos más preguntas,

114
00:04:50,560 --> 00:04:55,240
cualquier otra cosa que agregar sobre esto,

115
00:04:56,880 --> 00:04:58,639
no, no es realmente

116
00:04:58,639 --> 00:05:00,400
genial,

117
00:05:00,400 --> 00:05:02,639
el documento es una codificación predictiva

118
00:05:02,639 --> 00:05:05,039
teórica y experimental  revisión,

119
00:05:05,039 --> 00:05:08,240
creo que la primera versión fue de 2021,

120
00:05:08,240 --> 00:05:11,759
pero la segunda versión revisada fue 22

121
00:05:11,759 --> 00:05:14,720
por Millage Seth y Buckley y solo para

122
00:05:14,720 --> 00:05:15,680
revisan

123
00:05:15,680 --> 00:05:16,720
la

124
00:05:16,720 --> 00:05:18,960
afirmación central y luego algunos de los objetivos y

125
00:05:18,960 --> 00:05:20,880
direcciones que

126
00:05:20,880 --> 00:05:23,120
establecen escriben que no existe una revisión exhaustiva

127
00:05:23,120 --> 00:05:25,199
de la teoría de la codificación predictiva y

128
00:05:25,199 --> 00:05:27,120
especialmente de los desarrollos recientes en el

129
00:05:27,120 --> 00:05:31,039
campo, esa es su afirmación

130
00:05:31,039 --> 00:05:33,120
y luego apuntan a

131
00:05:33,120 --> 00:05:35,199
proporcionar una revisión integral tanto de

132
00:05:35,199 --> 00:05:36,960
la teoría central  estructura matemática y la

133
00:05:36,960 --> 00:05:40,479
lógica de la codificación predictiva, por lo que hay un

134
00:05:40,479 --> 00:05:42,479


135
00:05:42,479 --> 00:05:45,360
aspecto de revisión y resumen matemático en el documento

136
00:05:45,360 --> 00:05:47,440
y luego también revisan una amplia gama

137
00:05:47,440 --> 00:05:49,039
de trabajos clásicos y recientes dentro del

138
00:05:49,039 --> 00:05:51,360
marco que va desde

139
00:05:51,360 --> 00:05:53,440
micro circuitos biológicamente realistas que podrían

140
00:05:53,440 --> 00:05:55,680
implementar la codificación predictiva hasta la estrecha

141
00:05:55,680 --> 00:05:57,919
relación entre  la codificación predictiva

142
00:05:57,919 --> 00:06:00,000
y el ampliamente utilizado algoritmo de propagación hacia atrás del

143
00:06:00,000 --> 00:06:01,440
error

144
00:06:01,440 --> 00:06:02,880
, así como también el estudio de la estrecha

145
00:06:02,880 --> 00:06:05,199
relación entre la codificación predictiva

146
00:06:05,199 --> 00:06:09,280
y las técnicas modernas de aprendizaje automático.

147
00:06:09,759 --> 00:06:11,759


148
00:06:11,759 --> 00:06:12,960


149
00:06:12,960 --> 00:06:15,680


150
00:06:15,680 --> 00:06:18,880


151
00:06:19,759 --> 00:06:20,720


152
00:06:20,720 --> 00:06:22,479
por lo que la

153
00:06:22,479 --> 00:06:25,199
codificación predictiva ofrece un potencial

154
00:06:25,199 --> 00:06:28,160
explicación unificadora de la función cortical

155
00:06:28,160 --> 00:06:30,479
que postula que la función central

156
00:06:30,479 --> 00:06:32,319
del cerebro es minimizar los

157
00:06:32,319 --> 00:06:34,560
errores de predicción con respecto a un

158
00:06:34,560 --> 00:06:36,880
modelo generativo del mundo

159
00:06:36,880 --> 00:06:38,880
la teoría está estrechamente relacionada con el

160
00:06:38,880 --> 00:06:41,280
marco del cerebro bayesiano y en las

161
00:06:41,280 --> 00:06:43,840
últimas dos décadas ha ganado una

162
00:06:43,840 --> 00:06:45,919
influencia sustancial en los campos  de

163
00:06:45,919 --> 00:06:48,800
la neurociencia teórica y cognitiva

164
00:06:48,800 --> 00:06:51,759
ha surgido un gran cuerpo de investigación

165
00:06:51,759 --> 00:06:54,319
basado tanto en la prueba empírica de

166
00:06:54,319 --> 00:06:56,639


167
00:06:56,639 --> 00:06:59,280
modelos teóricos y matemáticos mejorados y extendidos de codificación predictiva

168
00:06:59,280 --> 00:07:01,599
como en la evaluación de su

169
00:07:01,599 --> 00:07:03,840
posibilidad biológica potencial para la

170
00:07:03,840 --> 00:07:06,360
implementación en el cerebro y predicciones

171
00:07:06,360 --> 00:07:08,400
neurofisiológicas y psicológicas concretas

172
00:07:08,400 --> 00:07:11,120
realizadas por el  a

173
00:07:11,120 --> 00:07:14,960
pesar de esta popularidad perdurable, sin embargo,

174
00:07:14,960 --> 00:07:16,880
no existe una revisión exhaustiva de la

175
00:07:16,880 --> 00:07:19,360
teoría de la codificación predictiva y, especialmente, de los

176
00:07:19,360 --> 00:07:22,160
desarrollos recientes en estos campos.

177
00:07:22,160 --> 00:07:24,639


178
00:07:24,720 --> 00:07:25,759


179
00:07:25,759 --> 00:07:28,080


180
00:07:28,080 --> 00:07:28,880


181
00:07:28,880 --> 00:07:30,800


182
00:07:30,800 --> 00:07:33,280


183
00:07:33,280 --> 00:07:35,039
complementando tutoriales recientes en la

184
00:07:35,039 --> 00:07:36,479
literatura

185
00:07:36,479 --> 00:07:37,360
y

186
00:07:37,360 --> 00:07:39,520
también revisamos una amplia gama de

187
00:07:39,520 --> 00:07:41,520
trabajos clásicos y recientes dentro del marco

188
00:07:41,520 --> 00:07:43,599
que es lo que habíamos leído anteriormente en

189
00:07:43,599 --> 00:07:45,360
las ames así es como los autores

190
00:07:45,360 --> 00:07:47,919
describen su propio trabajo y ahora

191
00:07:47,919 --> 00:07:48,960
vamos a

192
00:07:48,960 --> 00:07:50,639
ver su camino  map vea cómo

193
00:07:50,639 --> 00:07:53,360
llegaron allí y luego vamos a

194
00:07:53,360 --> 00:07:55,599
saltar directamente a la introducción y

195
00:07:55,599 --> 00:07:58,560
luego a los formalismos,

196
00:07:58,560 --> 00:08:01,440
así que aquí vemos la hoja de ruta

197
00:08:01,440 --> 00:08:04,160
y el esquema general del documento

198
00:08:04,160 --> 00:08:05,680
es el siguiente: la

199
00:08:05,680 --> 00:08:08,479
sección uno proporciona una introducción

200
00:08:08,479 --> 00:08:11,039
que contextualiza la codificación predictiva

201
00:08:11,039 --> 00:08:13,120
y  también lo desglosaremos en una

202
00:08:13,120 --> 00:08:16,879
vista filosófica e histórica. La

203
00:08:16,879 --> 00:08:18,240
sección dos

204
00:08:18,240 --> 00:08:19,440
proporciona

205
00:08:19,440 --> 00:08:21,599
un pase inicial al

206
00:08:21,599 --> 00:08:23,680
modelo de codificación predictiva de un nivel, una especie de

207
00:08:23,680 --> 00:08:26,560
núcleo o arquetipo o motivo de la

208
00:08:26,560 --> 00:08:28,720
codificación predictiva y lo conecta de manera

209
00:08:28,720 --> 00:08:31,440
importante a la inferencia variacional,

210
00:08:31,440 --> 00:08:34,399
entonces la codificación predictiva es  generalizado o

211
00:08:34,399 --> 00:08:36,880
extendido o elaborado en algunas

212
00:08:36,880 --> 00:08:39,279
direcciones importantes, como en la sección

213
00:08:39,279 --> 00:08:40,399
2.2,

214
00:08:40,399 --> 00:08:43,360
se generaliza en un caso de varios niveles

215
00:08:43,360 --> 00:08:45,680
en la sección 2.3 se introduce el concepto de

216
00:08:45,680 --> 00:08:47,839
coordenadas generalizadas

217
00:08:47,839 --> 00:08:49,360
en la sección 4

218
00:08:49,360 --> 00:08:51,839
se introduce la precisión como concepto y

219
00:08:51,839 --> 00:08:54,240
en la sección 2.5 se conecta de nuevo con

220
00:08:54,240 --> 00:08:56,080
el cerebro la

221
00:08:56,080 --> 00:08:58,080
sección 3 analiza los paradigmas de la

222
00:08:58,080 --> 00:08:59,600
codificación predictiva

223
00:08:59,600 --> 00:09:02,320
tanto no supervisada como supervisada

224
00:09:02,320 --> 00:09:04,240
y también la conecta a través de esas

225
00:09:04,240 --> 00:09:06,320
secciones y otras a  algunas investigaciones sobre

226
00:09:06,320 --> 00:09:07,760
el aprendizaje automático

227
00:09:07,760 --> 00:09:10,080
y luego en la sección 4 hay

228
00:09:10,080 --> 00:09:13,120
conexiones entre la codificación predictiva

229
00:09:13,120 --> 00:09:15,360
y algunos otros algoritmos,

230
00:09:15,360 --> 00:09:17,839
tanto los algoritmos que son comunes en

231
00:09:17,839 --> 00:09:19,680
el aprendizaje automático y las

232
00:09:19,680 --> 00:09:21,920
estadísticas computacionales como el error de

233
00:09:21,920 --> 00:09:23,279
retropropagación,

234
00:09:23,279 --> 00:09:25,120
la codificación predictiva lineal y el

235
00:09:25,120 --> 00:09:27,519
filtrado de columnas y la normalización de flujos y

236
00:09:27,519 --> 00:09:29,120
la competencia sesgada

237
00:09:29,120 --> 00:09:31,839
y luego  en la sección 4.5,

238
00:09:31,839 --> 00:09:34,000
se establece una conexión explícita entre la

239
00:09:34,000 --> 00:09:36,399
codificación predictiva y la inferencia activa, lo

240
00:09:36,399 --> 00:09:38,560
que trae consigo el costo de la acción,

241
00:09:38,560 --> 00:09:42,240
así como la idea del control pid, y la

242
00:09:42,240 --> 00:09:44,160
sección de discusión se cierra con solo

243
00:09:44,160 --> 00:09:46,800
algunas otras ideas, resúmenes,

244
00:09:46,800 --> 00:09:48,800
instrucciones y desafíos,

245
00:09:48,800 --> 00:09:51,839
y hay varios apéndices, así que

246
00:09:51,839 --> 00:09:55,920
eso'  Es un artículo o una monografía bastante largo,

247
00:09:55,920 --> 00:09:58,480
pero es un artículo excelente y solo

248
00:09:58,480 --> 00:09:59,839
vamos a cubrir

249
00:09:59,839 --> 00:10:01,920
un mayor énfasis en el contexto y

250
00:10:01,920 --> 00:10:04,800
el comienzo y luego pasar por alto

251
00:10:04,800 --> 00:10:07,040
o dejar de lado algunas cosas, pero

252
00:10:07,040 --> 00:10:08,560
hay muchos desempaquetados, así que espero  que

253
00:10:08,560 --> 00:10:10,720
nos gustaría obtener más información en las próximas

254
00:10:10,720 --> 00:10:13,040
semanas si los autores quieren unirse o si

255
00:10:13,040 --> 00:10:16,880
alguien más quiere unirse a

256
00:10:16,880 --> 00:10:19,680
algo para agregar en la hoja de ruta o listo para la

257
00:10:19,680 --> 00:10:22,079
introducción

258
00:10:22,079 --> 00:10:25,920
, vamos bien, increíble,

259
00:10:25,920 --> 00:10:26,720
así

260
00:10:26,720 --> 00:10:30,480
que simplemente agregaré algunas notas de esta

261
00:10:30,480 --> 00:10:32,320
sección.  de la introducción y luego

262
00:10:32,320 --> 00:10:34,320
María siéntase libre de agregar cualquier cosa

263
00:10:34,320 --> 00:10:36,560
entonces, ¿cómo introducen la

264
00:10:36,560 --> 00:10:38,959
codificación predictiva? presentan la

265
00:10:38,959 --> 00:10:41,040
teoría de la codificación predictiva como una teoría influyente en

266
00:10:41,040 --> 00:10:42,480
las neurociencias cognitivas y computacionales

267
00:10:42,480 --> 00:10:45,200
que propone una

268
00:10:45,200 --> 00:10:47,680
teoría unificadora potencial de la función cortical

269
00:10:47,680 --> 00:10:49,680
que está relacionada con la corteza del

270
00:10:49,680 --> 00:10:50,880
cerebro

271
00:10:50,880 --> 00:10:52,800
a saber, que la función central del

272
00:10:52,800 --> 00:10:53,760
cerebro

273
00:10:53,760 --> 00:10:56,000
es minimizar el error de predicción

274
00:10:56,000 --> 00:10:57,600
donde el error de predicción indica

275
00:10:57,600 --> 00:11:00,399
discrepancias entre la entrada predictiva y

276
00:11:00,399 --> 00:11:02,959
la entrada real.  Totalmente recibido, minimizar

277
00:11:02,959 --> 00:11:05,360
la divergencia entre las expectativas y las

278
00:11:05,360 --> 00:11:08,560
observaciones será un gran tema

279
00:11:08,560 --> 00:11:09,519
y

280
00:11:09,519 --> 00:11:11,519
acercarse a esa minimización

281
00:11:11,519 --> 00:11:13,839
se puede lograr de múltiples maneras

282
00:11:13,839 --> 00:11:15,920
y están coloreadas de manera diferente aquí a

283
00:11:15,920 --> 00:11:17,680
través de la inferencia inmediata sobre los

284
00:11:17,680 --> 00:11:19,519
estados ocultos del mundo

285
00:11:19,519 --> 00:11:21,200
como pensé que es la pelota.  aquí estoy

286
00:11:21,200 --> 00:11:22,880
obteniendo evidencia de que está aquí, así que

287
00:11:22,880 --> 00:11:24,720
debería actualizar mi forma de pensar sobre el

288
00:11:24,720 --> 00:11:25,839
mundo

289
00:11:25,839 --> 00:11:27,839
que puede explicar la percepción como la

290
00:11:27,839 --> 00:11:29,760
percepción de una pelota que se mueve a través del

291
00:11:29,760 --> 00:11:31,120
campo visual

292
00:11:31,120 --> 00:11:33,519
mediante la actualización de un modelo mundial global para

293
00:11:33,519 --> 00:11:35,360
hacer mejores predicciones que podrían

294
00:11:35,360 --> 00:11:36,800
explicar el aprendizaje

295
00:11:36,800 --> 00:11:39,200
como aprender un nivel un poco más alto

296
00:11:39,200 --> 00:11:41,040
que solo dónde está la pelota en el

297
00:11:41,040 --> 00:11:43,600
campo visual, pero como dónde tienden

298
00:11:43,600 --> 00:11:45,760
a estar las pelotas en el campo visual o qué tan

299
00:11:45,760 --> 00:11:47,360
rápido tienden a moverse

300
00:11:47,360 --> 00:11:50,000
y finalmente a través de la acción para muestrear

301
00:11:50,000 --> 00:11:52,079
datos sensoriales del mundo que se

302
00:11:52,079 --> 00:11:54,320
ajusta a  las predicciones y ahí es

303
00:11:54,320 --> 00:11:56,000
cuando introduciremos la acción en la

304
00:11:56,000 --> 00:11:58,880
codificación predictiva en las últimas secciones

305
00:11:58,880 --> 00:12:01,120
del documento y luego simplemente  último

306
00:12:01,120 --> 00:12:03,600
punto, dicen que la codificación predictiva cuenta con

307
00:12:03,600 --> 00:12:05,440
un marco matemático extremadamente desarrollado y

308
00:12:05,440 --> 00:12:07,440
basado en principios en términos de

309
00:12:07,440 --> 00:12:10,079
inferencia variacional, así como muchos

310
00:12:10,079 --> 00:12:12,639
modelos computacionales probados empíricamente

311
00:12:12,639 --> 00:12:14,320
que tienen vínculos estrechos con el

312
00:12:14,320 --> 00:12:15,519
aprendizaje automático,

313
00:12:15,519 --> 00:12:18,240
así es como presentan esto como

314
00:12:18,240 --> 00:12:20,639
una teoría unificadora que integra la

315
00:12:20,639 --> 00:12:23,040
inferencia como  así como la inferencia de acción de

316
00:12:23,040 --> 00:12:25,519
diferentes tipos más como perceptiva y

317
00:12:25,519 --> 00:12:26,959
más como aprendizaje

318
00:12:26,959 --> 00:12:29,680
y dicen que tiene un

319
00:12:29,680 --> 00:12:32,160
marco matemático muy rico y se

320
00:12:32,160 --> 00:12:34,959
ha conectado a conocimientos en biología

321
00:12:34,959 --> 00:12:38,320
y en aprendizaje automático

322
00:12:38,320 --> 00:12:40,880
para agregar

323
00:12:43,360 --> 00:12:45,120


324
00:12:45,120 --> 00:12:47,040
no, por lo que escriben codificación predictiva como una

325
00:12:47,040 --> 00:12:49,440
teoría  también ofrece un mecanismo único

326
00:12:49,440 --> 00:12:52,279
que da cuenta de diversos fenómenos perceptivos y

327
00:12:52,279 --> 00:12:55,120
neurobiológicos, como la

328
00:12:55,120 --> 00:12:57,360
detención final por percepción estable, como

329
00:12:57,360 --> 00:12:58,720
ilusiones,

330
00:12:58,720 --> 00:13:01,279
supresión de repetición repetida,

331
00:13:01,279 --> 00:13:03,920
movimientos ilusorios y modulación atencional de

332
00:13:03,920 --> 00:13:05,920
la actividad neuronal, por lo que apuntan a

333
00:13:05,920 --> 00:13:08,480
la naturaleza integradora de esta teoría o

334
00:13:08,480 --> 00:13:11,440
marco para modelar todo.  de estos diversos

335
00:13:11,440 --> 00:13:13,680
fenómenos  ena como una ventaja como si

336
00:13:13,680 --> 00:13:15,519
pudiéramos tener una teoría especial solo para la

337
00:13:15,519 --> 00:13:17,680
percepción estable, una teoría especial solo para la

338
00:13:17,680 --> 00:13:20,240
supresión de la repetición, etc.,

339
00:13:20,240 --> 00:13:22,639
pero este es un marco mediante el cual esos

340
00:13:22,639 --> 00:13:26,560
resultados ocurren como una especie de

341
00:13:26,560 --> 00:13:28,800
resultados de un marco subyacente de

342
00:13:28,800 --> 00:13:30,800
codificación predictiva en lugar de

343
00:13:30,800 --> 00:13:32,639
Necesitan teorías especiales para que se

344
00:13:32,639 --> 00:13:36,160
busque una especie de parsimonia o conciliación

345
00:13:36,160 --> 00:13:40,320
en áreas como la neurociencia cognitiva

346
00:13:40,320 --> 00:13:42,079
y escriben como tal y quizás de

347
00:13:42,079 --> 00:13:44,880
manera única entre las teorías neurocientíficas.

348
00:13:44,880 --> 00:13:47,199


349
00:13:47,199 --> 00:13:49,680


350
00:13:49,680 --> 00:13:51,600


351
00:13:51,600 --> 00:13:53,600


352
00:13:53,600 --> 00:13:55,680
lo que el cerebro está haciendo

353
00:13:55,680 --> 00:13:58,000
en todos los niveles algorítmicos computacionales

354
00:13:58,000 --> 00:14:00,160
y de implementación,

355
00:14:00,160 --> 00:14:00,880
por lo que

356
00:14:00,880 --> 00:14:04,160
estropea una jerarquía muy influyente, es

357
00:14:04,160 --> 00:14:06,639
algo así como las preguntas de tin bergens o un

358
00:14:06,639 --> 00:14:08,720
poco como la jerarquía de maslow,

359
00:14:08,720 --> 00:14:10,720
puede pensar en ello como un

360
00:14:10,720 --> 00:14:13,120
marco organizativo o de creación de sentido nuevamente

361
00:14:13,120 --> 00:14:16,079
para  la pregunta qué están haciendo los cerebros

362
00:14:16,079 --> 00:14:17,519
y así vamos  para tener tres

363
00:14:17,519 --> 00:14:19,680
tipos diferentes de respuestas a lo que

364
00:14:19,680 --> 00:14:21,839
están haciendo los cerebros, habrá una

365
00:14:21,839 --> 00:14:25,680
respuesta a nivel de algoritmo de cálculo

366
00:14:25,680 --> 00:14:28,240
e implementación,

367
00:14:28,240 --> 00:14:29,199
y

368
00:14:29,199 --> 00:14:31,440
aquí, con el ejemplo de un pájaro volando,

369
00:14:31,440 --> 00:14:34,399
el nivel de cálculo es como

370
00:14:34,399 --> 00:14:36,639
volar, es como el por qué, ¿cuál es el  algoritmo

371
00:14:36,639 --> 00:14:39,279
haciendo es un algoritmo de clasificación qué está

372
00:14:39,279 --> 00:14:41,920
haciendo el cerebro en esta situación está

373
00:14:41,920 --> 00:14:44,560
rastreando un objeto en movimiento

374
00:14:44,560 --> 00:14:46,720
luego está el nivel de aleteo que es

375
00:14:46,720 --> 00:14:48,800
el algoritmo que es una especie de

376
00:14:48,800 --> 00:14:51,760
pseudo código y eso sugiere

377
00:14:51,760 --> 00:14:54,959
el qué de cómo se realiza y como

378
00:14:54,959 --> 00:14:56,720
lo que está sucediendo  en este

379
00:14:56,720 --> 00:14:59,360
algoritmo de clasificación, bueno, está haciendo esto esto

380
00:14:59,360 --> 00:15:01,680
esto esto cuál es la función de vuelo

381
00:15:01,680 --> 00:15:04,000
que se realiza mediante el

382
00:15:04,000 --> 00:15:05,680
aleteo de las alas

383
00:15:05,680 --> 00:15:08,000
y luego está la implementación, por

384
00:15:08,000 --> 00:15:11,040
lo que no es solo el aleteo de

385
00:15:11,040 --> 00:15:13,440
algo, es el aleteo de algo específico

386
00:15:13,440 --> 00:15:15,199
y eso es  los

387
00:15:15,199 --> 00:15:16,320
detalles de implementación

388
00:15:16,320 --> 00:15:18,560
y esos niveles de análisis han sido

389
00:15:18,560 --> 00:15:21,839
útiles y provocativos para conectar el

390
00:15:21,839 --> 00:15:24,839
conocimiento computacional  ive teorías a

391
00:15:24,839 --> 00:15:26,959
teorías neurobiológicas por lo que ha sido un

392
00:15:26,959 --> 00:15:28,720


393
00:15:28,720 --> 00:15:29,519


394
00:15:29,519 --> 00:15:31,279


395
00:15:31,279 --> 00:15:33,600


396
00:15:33,600 --> 00:15:35,519
marco muy influyente y provocador de discusión que ha ayudado a las

397
00:15:35,519 --> 00:15:37,839
personas a conectarse y a desafiar algunas

398
00:15:37,839 --> 00:15:39,440
de las similitudes y diferencias

399
00:15:39,440 --> 00:15:43,360
entre computadoras similares y trenzas

400
00:15:45,199 --> 00:15:47,519
genial,

401
00:15:48,480 --> 00:15:50,399
esto todavía está en la introducción

402
00:15:50,399 --> 00:15:51,680
y van a tipo  de darnos una

403
00:15:51,680 --> 00:15:53,519
perspectiva cualitativa sobre la

404
00:15:53,519 --> 00:15:55,519
codificación predictiva y luego vamos a sumergirnos

405
00:15:55,519 --> 00:15:58,560
en muchos más detalles históricos y técnicos,

406
00:15:58,560 --> 00:16:00,959
pero ¿cómo funciona la codificación predictiva?

407
00:16:00,959 --> 00:16:01,839


408
00:16:01,839 --> 00:16:03,680
La intuición central detrás de la

409
00:16:03,680 --> 00:16:05,920
codificación predictiva es que el cerebro está compuesto por

410
00:16:05,920 --> 00:16:08,079
una jerarquía de capas que  cada uno

411
00:16:08,079 --> 00:16:10,240
hace predicciones sobre la actividad de

412
00:16:10,240 --> 00:16:12,160
la capa inmediatamente debajo de ellos en la

413
00:16:12,160 --> 00:16:14,720
jerarquía, por lo que hay dos direcciones en

414
00:16:14,720 --> 00:16:16,639
este esquema multicapa de codificación predictiva

415
00:16:16,639 --> 00:16:19,519
que estamos introduciendo aquí

416
00:16:19,519 --> 00:16:21,440
como una especie de heurística y luego

417
00:16:21,440 --> 00:16:23,600
vamos a acercarnos a  solo lo que está

418
00:16:23,600 --> 00:16:25,519
sucediendo en un nivel y luego

419
00:16:25,519 --> 00:16:27,279
vamos a retirarnos para

420
00:16:27,279 --> 00:16:30,160
recuperar este formalismo de múltiples niveles

421
00:16:30,160 --> 00:16:32,480
las predicciones descendentes descendentes en

422
00:16:32,480 --> 00:16:34,800
cada nivel que es la flecha azul

423
00:16:34,800 --> 00:16:36,320
se comparan con la actividad y las

424
00:16:36,320 --> 00:16:38,639
entradas de cada nivel para formar

425
00:16:38,639 --> 00:16:40,560
errores de predicción esta es la

426
00:16:40,560 --> 00:16:42,560
información en cada capa que

427
00:16:42,560 --> 00:16:45,040
no se pudo predecir con éxito

428
00:16:45,040 --> 00:16:46,959
estos errores de predicción luego se alimentan

429
00:16:46,959 --> 00:16:50,240
hacia arriba esa es la flecha roja  para servir como

430
00:16:50,240 --> 00:16:52,480
entradas a niveles más altos que luego se

431
00:16:52,480 --> 00:16:54,800
pueden utilizar para reducir su propio

432
00:16:54,800 --> 00:16:56,560
error de predicción,

433
00:16:56,560 --> 00:16:58,480
por lo que es una

434
00:16:58,480 --> 00:17:01,680
idea general, como la idea en el nivel superior es como si

435
00:17:01,680 --> 00:17:03,199
estuviera siguiendo

436
00:17:03,199 --> 00:17:05,199
una pelota de fútbol que se mueve de

437
00:17:05,199 --> 00:17:06,799
izquierda a derecha

438
00:17:06,799 --> 00:17:08,880
y  luego, eso se transduce de ciertas

439
00:17:08,880 --> 00:17:10,799
maneras y,

440
00:17:10,799 --> 00:17:14,400
en este caso, resulta en una especie de

441
00:17:14,400 --> 00:17:17,119
predicción de nivel perceptivo visual sobre

442
00:17:17,119 --> 00:17:19,199
lo que uno debería esperar ver

443
00:17:19,199 --> 00:17:21,199
y esto es nuevamente solo el

444
00:17:21,199 --> 00:17:24,240
esquema de codificación predictivo de niveles múltiples que se usa a menudo,

445
00:17:24,240 --> 00:17:25,439


446
00:17:25,439 --> 00:17:26,559
pero

447
00:17:26,559 --> 00:17:28,480
vamos a  acerque nuevamente a lo que está

448
00:17:28,480 --> 00:17:30,720
sucediendo en un solo nivel y luego a

449
00:17:30,720 --> 00:17:34,480
lo que está sucediendo en múltiples niveles

450
00:17:36,960 --> 00:17:38,320
increíble

451
00:17:38,320 --> 00:17:41,919
aquí hay un artículo divertido de 2020 por er perdón

452
00:17:41,919 --> 00:17:44,559
artículo de 2018  por carl fristen si la

453
00:17:44,559 --> 00:17:46,400
codificación predictiva tiene futuro

454
00:17:46,400 --> 00:17:48,080
y esto proporcionará un

455
00:17:48,080 --> 00:17:49,360
poco de contexto y luego

456
00:17:49,360 --> 00:17:51,520
vamos a profundizar más en aunque

457
00:17:51,520 --> 00:17:53,360
carl escribió en el siglo XX que

458
00:17:53,360 --> 00:17:55,039
creíamos que el cerebro extraía conocimiento

459
00:17:55,039 --> 00:17:56,960
de la sensación que es como el

460
00:17:56,960 --> 00:17:59,039
reconocimiento  modelo y esa es una especie de

461
00:17:59,039 --> 00:18:02,640
modelo de procesamiento sensorial entrante

462
00:18:02,640 --> 00:18:04,880
el siglo XXI fue testigo de una extraña

463
00:18:04,880 --> 00:18:06,880
inversión en la que el cerebro se convirtió en un

464
00:18:06,880 --> 00:18:09,520
órgano de inferencia que construye activamente

465
00:18:09,520 --> 00:18:11,120
explicaciones de lo que sucede

466
00:18:11,120 --> 00:18:14,240
más allá de su epitelio sensorial un

467
00:18:14,240 --> 00:18:16,720
artículo desempeñó un papel clave en este

468
00:18:16,720 --> 00:18:17,600
cambio de paradigma

469
00:18:17,600 --> 00:18:20,400
y  carl continúa escribiendo que un

470
00:18:20,400 --> 00:18:22,400
artículo de 1999 de rao y ballard del que

471
00:18:22,400 --> 00:18:25,120
vamos a hablar más sobre él fue uno

472
00:18:25,120 --> 00:18:27,039
de esos artículos que se encuentran una vez en una

473
00:18:27,039 --> 00:18:29,280
década,

474
00:18:29,280 --> 00:18:30,320
así

475
00:18:30,320 --> 00:18:31,840
que vamos a hablar más al respecto, pero

476
00:18:31,840 --> 00:18:34,480
esto es solo  tipo de conexión temprana

477
00:18:34,480 --> 00:18:35,360
con

478
00:18:35,360 --> 00:18:37,520
la línea de investigación de carl firsten la

479
00:18:37,520 --> 00:18:39,360
conexión entre la inferencia activa y la

480
00:18:39,360 --> 00:18:41,679
codificación predictiva y la idea de que a

481
00:18:41,679 --> 00:18:43,480
veces son estas primeras

482
00:18:43,480 --> 00:18:46,160
transdis  artículos disciplinarios que pueden unir

483
00:18:46,160 --> 00:18:48,320
dos campos diferentes y

484
00:18:48,320 --> 00:18:51,280
que pueden tener consecuencias a largo plazo para

485
00:18:51,280 --> 00:18:54,240
preguntas esencialmente perennes como cómo la

486
00:18:54,240 --> 00:18:57,039
percepción, el aprendizaje, la atención y la

487
00:18:57,039 --> 00:19:00,080
sensación funcionan todos juntos

488
00:19:00,080 --> 00:19:02,080
y bajo qué imperativo podrían

489
00:19:02,080 --> 00:19:04,720
trabajar juntos

490
00:19:05,520 --> 00:19:07,919
y luego los autores brindan una

491
00:19:07,919 --> 00:19:09,600
discusión muy preliminar  que

492
00:19:09,600 --> 00:19:11,520
van a profundizar más adelante

493
00:19:11,520 --> 00:19:14,480
sobre cómo la codificación predictiva es importante para

494
00:19:14,480 --> 00:19:16,080
el aprendizaje automático,

495
00:19:16,080 --> 00:19:18,240
por lo que escriben que la codificación predictiva

496
00:19:18,240 --> 00:19:20,080
propone que el uso de una

497
00:19:20,080 --> 00:19:22,960
función perdida simple no supervisada

498
00:19:22,960 --> 00:19:24,880
, como simplemente intentar predecir

499
00:19:24,880 --> 00:19:27,440
los datos sensoriales entrantes,

500
00:19:27,440 --> 00:19:29,440
es suficiente para desarrollar un complejo

501
00:19:29,440 --> 00:19:31,200
general y jerárquicamente  ricas

502
00:19:31,200 --> 00:19:33,840
representaciones del mundo

503
00:19:33,840 --> 00:19:35,280
que sugieren que esto ha encontrado

504
00:19:35,280 --> 00:19:37,840
apoyo en los éxitos de

505
00:19:37,840 --> 00:19:39,919
los modelos modernos de aprendizaje automático que se entrenan

506
00:19:39,919 --> 00:19:42,480
en objetivos predictivos o autorregresivos no supervisados, por

507
00:19:42,480 --> 00:19:44,880


508
00:19:44,880 --> 00:19:46,559
lo que estos son algunos de los artículos que

509
00:19:46,559 --> 00:19:48,720
citan: este es el marrón en absoluto y el

510
00:19:48,720 --> 00:19:50,240
Kaplan en absoluto

511
00:19:50,240 --> 00:19:51,919
y ambos tienen que ver con el

512
00:19:51,919 --> 00:19:53,919
entrenamiento de lo que se están

513
00:19:53,919 --> 00:19:56,559
volviendo cada vez más importantes métodos modernos de

514
00:19:56,559 --> 00:19:58,799
aprendizaje automático que son los

515
00:19:58,799 --> 00:20:00,799
modelos de lenguaje

516
00:20:00,799 --> 00:20:03,039
en contraste con los algoritmos modernos de aprendizaje

517
00:20:03,039 --> 00:20:05,280
automático que están entrenados para terminar con

518
00:20:05,280 --> 00:20:08,080
una pérdida global en la salida,

519
00:20:08,080 --> 00:20:11,039
por lo que, dado el conjunto total de datos grandes, déme

520
00:20:11,039 --> 00:20:13,840
el error total más bajo en el  todo el

521
00:20:13,840 --> 00:20:15,440
conjunto de datos

522
00:20:15,440 --> 00:20:17,520
en la codificación predictiva los errores de predicción

523
00:20:17,520 --> 00:20:19,760
se calculan en cada capa, lo que significa

524
00:20:19,760 --> 00:20:22,159
que cada capa solo tiene que centrarse en

525
00:20:22,159 --> 00:20:25,600
minimizar los errores locales y no la pérdida global.

526
00:20:25,600 --> 00:20:27,280


527
00:20:27,280 --> 00:20:28,960


528
00:20:28,960 --> 00:20:31,600


529
00:20:31,600 --> 00:20:34,960
esto

530
00:20:34,960 --> 00:20:36,559
se conectará con la calma, el

531
00:20:36,559 --> 00:20:39,360
filtrado y el control de pid,

532
00:20:39,360 --> 00:20:40,640
por lo que están

533
00:20:40,640 --> 00:20:43,039
diseñando la tierra y mostrando dónde

534
00:20:43,039 --> 00:20:44,320
se conectarán en la

535
00:20:44,320 --> 00:20:47,039
introducción de manera muy cualitativa

536
00:20:47,039 --> 00:20:49,280
y, de nuevo, volveremos a acercarnos para

537
00:20:49,280 --> 00:20:51,200
averiguarlo.  lo que sucede en un nivel

538
00:20:51,200 --> 00:20:53,440
de codificación predictiva y luego tratar de

539
00:20:53,440 --> 00:20:56,240
reconstruir algunos de los formalismos relacionados

540
00:20:56,240 --> 00:20:59,600
con la calma y el filtrado  control de ering y pid

541
00:20:59,600 --> 00:21:02,000
y las preguntas más importantes que la gente

542
00:21:02,000 --> 00:21:03,919
podría sentir curiosidad por hacer

543
00:21:03,919 --> 00:21:06,400
serían qué significa

544
00:21:06,400 --> 00:21:08,559
que la computación esté inspirada biológicamente

545
00:21:08,559 --> 00:21:09,919


546
00:21:09,919 --> 00:21:12,000
qué es posible cuando pensamos

547
00:21:12,000 --> 00:21:13,760
en algunas de las similitudes,

548
00:21:13,760 --> 00:21:15,760
diferencias y complementariedades

549
00:21:15,760 --> 00:21:18,720
entre los tipos biológicos y diferentes

550
00:21:18,720 --> 00:21:21,280
de cómputo convencional y no convencional

551
00:21:21,280 --> 00:21:23,918


552
00:21:25,440 --> 00:21:27,039
la siguiente

553
00:21:27,039 --> 00:21:28,000
parte

554
00:21:28,000 --> 00:21:30,640
maria, por favor quítala, cambiaré

555
00:21:30,640 --> 00:21:32,799
las diapositivas, así que muchas gracias por

556
00:21:32,799 --> 00:21:35,200
agregar mucho del contexto histórico

557
00:21:35,200 --> 00:21:36,880
y espero aprender sobre

558
00:21:36,880 --> 00:21:39,840
lo que tienes que compartir aquí.

559
00:21:39,840 --> 00:21:41,039


560
00:21:41,039 --> 00:21:44,799
el párrafo

561
00:21:44,799 --> 00:21:46,880
que está en el documento

562
00:21:46,880 --> 00:21:48,640
está bien,

563
00:21:48,640 --> 00:21:50,240
comienzan a hablar sobre la historia de la

564
00:21:50,240 --> 00:21:52,320
codificación predictiva, por lo que,

565
00:21:52,320 --> 00:21:54,080
si bien la codificación predictiva como

566
00:21:54,080 --> 00:21:56,559
teoría neurocientífica se originó en los

567
00:21:56,559 --> 00:21:59,440
años 80 y 90 con montford rao y

568
00:21:59,440 --> 00:22:00,799
ballard,

569
00:22:00,799 --> 00:22:02,320
claro,

570
00:22:02,320 --> 00:22:04,000
nevison

571
00:22:04,000 --> 00:22:05,039
o algo así como risa

572
00:22:05,039 --> 00:22:07,280
y doblajes

573
00:22:07,280 --> 00:22:09,600
y se convirtió por primera vez en  su

574
00:22:09,600 --> 00:22:11,600
forma matemática moderna y la

575
00:22:11,600 --> 00:22:13,600
teoría integral de las respuestas corticales a mediados de la década de

576
00:22:13,600 --> 00:22:16,480
2000  con el estilo libre, tiene anticipaciones intelectuales profundas.

577
00:22:16,480 --> 00:22:18,799


578
00:22:18,799 --> 00:22:20,960
Estos precursores incluyen la

579
00:22:20,960 --> 00:22:23,120
noción de Helmholtz de la percepción como una

580
00:22:23,120 --> 00:22:25,840
inferencia inconsciente y la noción de Kant de que

581
00:22:25,840 --> 00:22:28,960
se necesita una estructura prioritaria para dar sentido

582
00:22:28,960 --> 00:22:31,600
al dicho sensorial

583
00:22:31,600 --> 00:22:34,720
, así como las primeras ideas de

584
00:22:34,720 --> 00:22:36,400
compresión y control de retroalimentación en la

585
00:22:36,400 --> 00:22:39,200
cibernética y la teoría de la información.

586
00:22:39,200 --> 00:22:42,720
y en las próximas diapositivas hablaré

587
00:22:42,720 --> 00:22:44,480
un poco más sobre la historia y la

588
00:22:44,480 --> 00:22:47,600
filosofía de la codificación predictiva

589
00:22:47,600 --> 00:22:50,640
y luego algunos formalismos básicos y

590
00:22:50,640 --> 00:22:53,120
varias generalizaciones y elaboraciones

591
00:22:53,120 --> 00:22:56,239
de la codificación predictiva

592
00:22:56,559 --> 00:22:58,240
si hay algo que quieras decir,

593
00:22:58,240 --> 00:23:02,960
daniel, siéntete libre de participar.

594
00:23:02,960 --> 00:23:04,159


595
00:23:04,159 --> 00:23:06,400
uh, el trasfondo histórico y filosófico

596
00:23:06,400 --> 00:23:09,200
de la percepción es enorme, está bien,

597
00:23:09,200 --> 00:23:11,360
sin embargo, al ver en el contexto del marco de

598
00:23:11,360 --> 00:23:13,520
codificación predictiva, los autores

599
00:23:13,520 --> 00:23:15,760
generalmente presentan un razonamiento similar

600
00:23:15,760 --> 00:23:17,840
sobre la percepción

601
00:23:17,840 --> 00:23:19,280
y

602
00:23:19,280 --> 00:23:22,320
luego podemos comenzar a pensar uh con el

603
00:23:22,320 --> 00:23:25,280
filósofo griego platón en su alegoría

604
00:23:25,280 --> 00:23:26,799
de la cueva

605
00:23:26,799 --> 00:23:29,080
uh de

606
00:23:29,080 --> 00:23:33,200
514 a 520 antes de cristo

607
00:23:33,200 --> 00:23:35,200
que dijo

608
00:23:35,200 --> 00:23:37,039
básicamente que s  Algunas personas estarían

609
00:23:37,039 --> 00:23:39,120
atrapadas en una cueva para siempre viendo las

610
00:23:39,120 --> 00:23:41,600
sombras proyectadas por los objetos que se mueven cerca de un

611
00:23:41,600 --> 00:23:43,200
fuego detrás de ellos

612
00:23:43,200 --> 00:23:45,600
y luego les gustaría estar allí

613
00:23:45,600 --> 00:23:47,600
para siempre tener niños que crecerían

614
00:23:47,600 --> 00:23:48,720
así

615
00:23:48,720 --> 00:23:52,080
dentro de la cueva y finalmente sabes

616
00:23:52,080 --> 00:23:53,600
estas

617
00:23:53,600 --> 00:23:56,080
sombras de los objetos cerca del fuego  detrás de

618
00:23:56,080 --> 00:23:57,679
ellos estaría

619
00:23:57,679 --> 00:23:59,760
lo real y lo único para

620
00:23:59,760 --> 00:24:03,520
estas personas, no creerían que

621
00:24:03,520 --> 00:24:07,120
hay algo fuera de la cueva, así que

622
00:24:07,120 --> 00:24:09,840
esto sería esto, estas sombras

623
00:24:09,840 --> 00:24:12,640
serían lo único correcto, así que lo que

624
00:24:12,640 --> 00:24:15,520
Platón afirmaría aquí es que es nuestro

625
00:24:15,520 --> 00:24:18,240
las percepciones conscientes son como

626
00:24:18,240 --> 00:24:20,799
estas sombras, está bien, lo que significa que son

627
00:24:20,799 --> 00:24:23,200
reflejos indirectos de causas ocultas

628
00:24:23,200 --> 00:24:27,200
que nunca podemos encontrar directamente

629
00:24:27,200 --> 00:24:28,559
y,

630
00:24:28,559 --> 00:24:29,919


631
00:24:29,919 --> 00:24:32,960
pero unos siglos más tarde, el erudito árabe medieval

632
00:24:32,960 --> 00:24:36,080
al al hasan

633
00:24:36,080 --> 00:24:38,559
escribió muchas notas interesantes sobre

634
00:24:38,559 --> 00:24:42,399
la percepción visual, en realidad escribió

635
00:24:43,200 --> 00:24:44,080
seis

636
00:24:44,080 --> 00:24:47,279
volúmenes.  si no me equivoco en uh de su

637
00:24:47,279 --> 00:24:49,200
libro de óptica

638
00:24:49,200 --> 00:24:51,520
donde exploró la opinión de que la percepción humana a

639
00:24:51,520 --> 00:24:54,159
menudo depende de mecanismos

640
00:24:54,159 --> 00:24:57,120
de juicio y  d inferencia en lugar de que

641
00:24:57,120 --> 00:24:59,360
sepas proporcionar rasgos de acceso al

642
00:24:59,360 --> 00:25:01,039
mundo para

643
00:25:01,039 --> 00:25:03,039
que puedas ver que,

644
00:25:03,039 --> 00:25:05,600
muy temprano en nuestro

645
00:25:05,600 --> 00:25:08,480
pensamiento occidental, tenemos esta

646
00:25:08,480 --> 00:25:11,360
noción de que no tenemos

647
00:25:11,360 --> 00:25:14,159
acceso directo al mundo, está bien,

648
00:25:14,159 --> 00:25:16,080
hay alguna

649
00:25:16,080 --> 00:25:18,639
inferencia en marcha,

650
00:25:19,679 --> 00:25:22,000
está bien,

651
00:25:23,600 --> 00:25:26,159
genial  bien,

652
00:25:26,159 --> 00:25:29,279
saltando al siglo 18

653
00:25:29,279 --> 00:25:32,159
podemos encontrar dos filósofos importantes,

654
00:25:32,159 --> 00:25:34,400
está bien, el inglés david hume y el

655
00:25:34,400 --> 00:25:36,720
alemán emmanuel

656
00:25:36,720 --> 00:25:40,559
lo cortaron en 1739 a 1740

657
00:25:40,559 --> 00:25:43,360
exportan las inferencias inductivas y la

658
00:25:43,360 --> 00:25:44,799
causalidad

659
00:25:44,799 --> 00:25:48,480
el problema de la inducción que trajo

660
00:25:48,480 --> 00:25:50,799
fue un análisis de causa y efecto y

661
00:25:50,799 --> 00:25:54,320
percepción y la conclusión

662
00:25:54,320 --> 00:25:57,200
que obtiene es que toda nuestra vida mental

663
00:25:57,200 --> 00:26:00,080
podría remontarse a los efectos de la experiencia de los

664
00:26:00,080 --> 00:26:02,720
sentidos porque dado que no tenemos

665
00:26:02,720 --> 00:26:03,520


666
00:26:03,520 --> 00:26:05,919
contacto directo con el mundo, nunca podríamos

667
00:26:05,919 --> 00:26:08,080
tener una relación uno a uno con

668
00:26:08,080 --> 00:26:10,960
los objetos allí y por lo tanto

669
00:26:10,960 --> 00:26:13,520
es posible tener un efecto con

670
00:26:13,520 --> 00:26:15,600
muchas causas posibles y una causa

671
00:26:15,600 --> 00:26:16,480
con

672
00:26:16,480 --> 00:26:18,559
efectos

673
00:26:18,559 --> 00:26:20,400
la solución aquí sería entonces

674
00:26:20,400 --> 00:26:23,360
extraer regularidades estadísticas y

675
00:26:23,360 --> 00:26:26,400
imaginando lo que sucede cuando el mundo es

676
00:26:26,400 --> 00:26:29,440
intervenido de manera controlada como

677
00:26:29,440 --> 00:26:31,840
el filósofo jacob howie

678
00:26:31,840 --> 00:26:33,840
desarrolló en su libro la

679
00:26:33,840 --> 00:26:36,880
mente predictiva en 2014

680
00:26:36,880 --> 00:26:38,720
igualmente importante

681
00:26:38,720 --> 00:26:39,520
eh

682
00:26:39,520 --> 00:26:42,960
en la crítica de la razón pura en 1781

683
00:26:42,960 --> 00:26:43,919
kant

684
00:26:43,919 --> 00:26:46,240
agregará un elemento importante a esta

685
00:26:46,240 --> 00:26:48,880
historia entre ustedes saben  innumerables otros

686
00:26:48,880 --> 00:26:51,600
elementos a la historia, agregará que

687
00:26:51,600 --> 00:26:53,279
el cerebro usa

688
00:26:53,279 --> 00:26:55,840
la información existente sobre el espacio y el

689
00:26:55,840 --> 00:26:58,799
tiempo para dar sentido a los

690
00:26:58,799 --> 00:27:01,919
datos sensoriales caóticos que recibe constantemente

691
00:27:01,919 --> 00:27:04,400
para proporcionarle a un organismo la percepción tal como

692
00:27:04,400 --> 00:27:05,600
ellos saben,

693
00:27:05,600 --> 00:27:06,720
así que

694
00:27:06,720 --> 00:27:08,640
todos estos

695
00:27:08,640 --> 00:27:11,440
eruditos nos dirían

696
00:27:11,440 --> 00:27:13,840
justo en  las edades tempranas en las que

697
00:27:13,840 --> 00:27:16,640
no tenemos acceso directo a las

698
00:27:16,640 --> 00:27:18,159
cosas del mundo

699
00:27:18,159 --> 00:27:21,200
y necesitamos información preexistente para

700
00:27:21,200 --> 00:27:24,159
dar sentido a todos los datos que recibimos constantemente

701
00:27:24,159 --> 00:27:25,679


702
00:27:25,679 --> 00:27:29,679
muchas cosas de codificación predictiva

703
00:27:30,559 --> 00:27:32,559
realmente me hace preguntarme por qué a

704
00:27:32,559 --> 00:27:34,799
veces todavía es tan

705
00:27:34,799 --> 00:27:37,760
inefable o  mal entendidas o incluso

706
00:27:37,760 --> 00:27:40,240
consideradas controvertidas

707
00:27:40,240 --> 00:27:41,840
algunas de estas perspectivas que han

708
00:27:41,840 --> 00:27:42,230
sido

709
00:27:42,230 --> 00:27:43,679
[Música]

710
00:27:43,679 --> 00:27:44,880
como

711
00:27:44,880 --> 00:27:46,720
estás describiendo li  ke

712
00:27:46,720 --> 00:27:49,200
pensó y llegó de manera convergente durante

713
00:27:49,200 --> 00:27:50,880
muchos miles de años en diferentes

714
00:27:50,880 --> 00:27:54,399
culturas, muy interesante,

715
00:27:55,760 --> 00:27:57,520
sí,

716
00:27:57,520 --> 00:27:58,720
y

717
00:27:58,720 --> 00:28:01,760
bueno, a fines del siglo XIX, el

718
00:28:01,760 --> 00:28:04,960
científico alemán Hermann de Helmholtz en 1860

719
00:28:04,960 --> 00:28:07,279
describió la percepción como

720
00:28:07,279 --> 00:28:09,840
uh, que involucra una influencia probabilística,

721
00:28:09,840 --> 00:28:12,720
sí, él es uno de nuestros dioses

722
00:28:12,720 --> 00:28:14,880
y  comunidades

723
00:28:14,880 --> 00:28:19,200
pero y él realmente se inspiró en Kant,

724
00:28:19,200 --> 00:28:20,159
está bien,

725
00:28:20,159 --> 00:28:20,960
eh,

726
00:28:20,960 --> 00:28:21,840
y

727
00:28:21,840 --> 00:28:23,919
con esa inspiración desarrolló la

728
00:28:23,919 --> 00:28:27,039
idea del cerebro como un probador de hipótesis

729
00:28:27,039 --> 00:28:28,960
y que la percepción es un proceso de

730
00:28:28,960 --> 00:28:31,840
inferencia inconsciente, está bien,

731
00:28:31,840 --> 00:28:34,880
y más específicamente, esta idea que

732
00:28:34,880 --> 00:28:36,159
desarrolló

733
00:28:36,159 --> 00:28:38,720
en los experimentos fue que la percepción tiene

734
00:28:38,720 --> 00:28:40,720
ser inferido mediante la combinación de

735
00:28:40,720 --> 00:28:43,200
señales sensoriales con las expectativas y creencias del cerebro

736
00:28:43,200 --> 00:28:45,520
sobre sus causas

737
00:28:45,520 --> 00:28:48,240
y tales inferencias suceden

738
00:28:48,240 --> 00:28:50,799
sin la conciencia del sujeto, quiero decir

739
00:28:50,799 --> 00:28:53,039
que tiene que suceder sin la conciencia del sujeto

740
00:28:53,039 --> 00:28:54,640
porque

741
00:28:54,640 --> 00:28:57,200
sabes, imagina todos los datos sensoriales caóticos

742
00:28:57,200 --> 00:28:58,399
que vienen

743
00:28:58,399 --> 00:29:02,240
y abres los ojos y  hay que

744
00:29:02,240 --> 00:29:04,080
tratar de organizar todo quiero decir que

745
00:29:04,080 --> 00:29:06,000
no pasa que w  ay, solo abre

746
00:29:06,000 --> 00:29:07,520
los ojos y

747
00:29:07,520 --> 00:29:10,000
todo sigue estable

748
00:29:10,000 --> 00:29:14,720
, por lo que es inconsciente

749
00:29:14,720 --> 00:29:16,880
y necesita realizar un seguimiento de las causas en

750
00:29:16,880 --> 00:29:19,600
el mundo actualizando las mejores

751
00:29:19,600 --> 00:29:23,039
conjeturas perceptivas o puede decir hipótesis a medida

752
00:29:23,039 --> 00:29:25,120
que llegan nuevas señales sensoriales,

753
00:29:25,120 --> 00:29:27,919
así que aquí construyó

754
00:29:27,919 --> 00:29:28,880
uh

755
00:29:28,880 --> 00:29:30,320
la idea

756
00:29:30,320 --> 00:29:33,440
que usamos combini, usamos señales sensoriales

757
00:29:33,440 --> 00:29:34,559


758
00:29:34,559 --> 00:29:37,440
con las expectativas del cerebro,

759
00:29:37,440 --> 00:29:39,520
ya sabes, actualice

760
00:29:39,520 --> 00:29:43,039
la hipótesis que hacemos sobre

761
00:29:43,039 --> 00:29:44,399
las causas

762
00:29:44,399 --> 00:29:47,039
de los efectos que recibimos en el mundo

763
00:29:47,039 --> 00:29:49,840
y esto significa que de alguna manera se convirtió en

764
00:29:49,840 --> 00:29:52,960
humes y [ __ ] dentro científico

765
00:29:52,960 --> 00:29:57,039
afirmando que podemos  solo infiero

766
00:29:57,039 --> 00:29:59,679
cosas en una palabra detrás de un

767
00:29:59,679 --> 00:30:02,640
velo sensorial que, si no me equivoco,

768
00:30:02,640 --> 00:30:05,520
eventualmente se llamará marca de negrura

769
00:30:05,520 --> 00:30:07,120
es que en ese momento

770
00:30:07,120 --> 00:30:09,600
no estoy seguro,

771
00:30:09,600 --> 00:30:12,159
tal vez reveló

772
00:30:12,159 --> 00:30:14,480
qué tan caliente fue crucial para muchos

773
00:30:14,480 --> 00:30:16,880
trabajos diferentes en el aprendizaje automático, uno que fue

774
00:30:16,880 --> 00:30:19,840
incluso  muy bien llamado máquina de helmholtz

775
00:30:19,840 --> 00:30:21,520
, fue crucial en la psicología, la

776
00:30:21,520 --> 00:30:24,720
neurociencia, la ciencia cognitiva, etc.,

777
00:30:24,720 --> 00:30:27,200
particularmente en la psicología,

778
00:30:27,200 --> 00:30:31,760
helmholtz en el interior inspiró a yuri schneizer en  1967

779
00:30:31,760 --> 00:30:34,960
y luego richard gregory en 1980 y

780
00:30:34,960 --> 00:30:38,799
eve arvind rock en 1983 para desarrollar el

781
00:30:38,799 --> 00:30:41,360
enfoque de análisis por síntesis, que es

782
00:30:41,360 --> 00:30:44,080
el proceso que analiza una señal o

783
00:30:44,080 --> 00:30:46,000
imagen al

784
00:30:46,000 --> 00:30:49,279
reproducirla usando un modelo.

785
00:30:49,360 --> 00:30:51,520
Hay una imagen del mejor

786
00:30:51,520 --> 00:30:54,320
análisis basado en este enfoque.

787
00:30:54,320 --> 00:30:58,799
Algunos esquemas disponibles.  en este enlace

788
00:30:58,799 --> 00:31:00,240
y, de hecho,

789
00:31:00,240 --> 00:31:02,880
gregory construyó algún tipo de

790
00:31:02,880 --> 00:31:05,200
modelo de prueba de hipótesis neuronal basado en su

791
00:31:05,200 --> 00:31:07,519
cuán caliente c y su comprensión del cerebro

792
00:31:07,519 --> 00:31:09,519
como formulando continuamente

793
00:31:09,519 --> 00:31:12,080
hipótesis perceptivas sobre la palabra y

794
00:31:12,080 --> 00:31:14,640
probándolas mediante la adquisición de datos

795
00:31:14,640 --> 00:31:18,600
de los órganos sensoriales,

796
00:31:21,039 --> 00:31:24,880
muchas cosas sucedieron después de la

797
00:31:24,880 --> 00:31:27,360
análisis por cosas de síntesis,

798
00:31:27,360 --> 00:31:29,200
pero simplemente saltaré a lo que tenemos

799
00:31:29,200 --> 00:31:30,960
hoy en filosofía porque

800
00:31:30,960 --> 00:31:34,080
tocaremos los formalismos y las

801
00:31:34,080 --> 00:31:37,519
cosas de computación después, está bien, así

802
00:31:37,519 --> 00:31:40,000
que solo daré un salto e iré a lo que

803
00:31:40,000 --> 00:31:42,399
tenemos hoy en filosofía,

804
00:31:42,399 --> 00:31:43,440
así que

805
00:31:43,440 --> 00:31:45,200


806
00:31:45,200 --> 00:31:47,120
trabajos recientes en filosofía  y la

807
00:31:47,120 --> 00:31:49,039
neurociencia cognitiva han contribuido al

808
00:31:49,039 --> 00:31:51,200
marco expansivo de codificación predictiva

809
00:31:51,200 --> 00:31:54,159
y al paradigma de Beijing en t  El cerebro

810
00:31:54,159 --> 00:31:56,559
y muchos de los trabajos seminales que tenemos en

811
00:31:56,559 --> 00:31:59,440
el campo abordan la codificación predictiva como

812
00:31:59,440 --> 00:32:01,679
procesamiento predictivo

813
00:32:01,679 --> 00:32:02,799
sin

814
00:32:02,799 --> 00:32:05,440
que sepas ninguna diferencia relevante en

815
00:32:05,440 --> 00:32:07,039
primera

816
00:32:07,039 --> 00:32:08,799
instancia

817
00:32:08,799 --> 00:32:09,919
y

818
00:32:09,919 --> 00:32:11,120
bueno,

819
00:32:11,120 --> 00:32:13,519
por ejemplo, la filosofía jacob howie

820
00:32:13,519 --> 00:32:15,600
que se basa en la universidad monash de

821
00:32:15,600 --> 00:32:16,720
australia

822
00:32:16,720 --> 00:32:19,760
destinada a la mejora de amazon.  del

823
00:32:19,760 --> 00:32:22,640
mecanismo de minimización predictivo y

824
00:32:22,640 --> 00:32:25,120
el concepto de hipótesis ganadora para

825
00:32:25,120 --> 00:32:27,200
la percepción consciente

826
00:32:27,200 --> 00:32:29,120
también está interesado en lo que

827
00:32:29,120 --> 00:32:31,120
los estudios de procesamiento predictivo pp pueden aportar

828
00:32:31,120 --> 00:32:33,120
a las personas con trastornos mentales como el

829
00:32:33,120 --> 00:32:36,000
autismo y la esquizofrenia en realidad, los

830
00:32:36,000 --> 00:32:37,440


831
00:32:37,440 --> 00:32:40,080
tres filósofos aquí presentes están interesados

832
00:32:40,080 --> 00:32:42,480
en lo que las personas pueden  llevar a la

833
00:32:42,480 --> 00:32:45,760
mejora en psiquiatría y

834
00:32:45,760 --> 00:32:49,919
medicina en general, así que en 2014 publicó un

835
00:32:49,919 --> 00:32:52,080
importante libro de texto sobre el tema que es

836
00:32:52,080 --> 00:32:54,880
la mente predictiva el libro rojo se puede

837
00:32:54,880 --> 00:32:55,840
ver

838
00:32:55,840 --> 00:32:57,840
donde trata de resolver el problema de la

839
00:32:57,840 --> 00:32:59,120
percepción

840
00:32:59,120 --> 00:33:01,840
inspirado en el problema de inducción humana

841
00:33:01,840 --> 00:33:03,760
con inferencia bayesiana  y procesamiento predictivo

842
00:33:03,760 --> 00:33:06,000


843
00:33:06,000 --> 00:33:10,000
más adelante en 2016 el p  el filósofo andy clark,

844
00:33:10,000 --> 00:33:12,720
basado en la universidad de sussex, inglaterra

845
00:33:12,720 --> 00:33:15,000
, desarrolló cuidadosamente lo que llamó

846
00:33:15,000 --> 00:33:17,760
procesamiento predictivo orientado a la acción en

847
00:33:17,760 --> 00:33:20,399
su libro navegando por la incertidumbre, donde

848
00:33:20,399 --> 00:33:22,559
agrega y avanza el papel crucial de la

849
00:33:22,559 --> 00:33:25,120
acción en nuestra percepción

850
00:33:25,120 --> 00:33:26,799
y establece muchas

851
00:33:26,799 --> 00:33:28,640
conexiones interesantes entre trabajos recientes en

852
00:33:28,640 --> 00:33:30,159
computación  neurociencia e

853
00:33:30,159 --> 00:33:32,000
inteligencia artificial

854
00:33:32,000 --> 00:33:35,760
y su filosofía ecléctica de poder

855
00:33:35,760 --> 00:33:39,519
y el año pasado en 2021 el neurocientífico

856
00:33:39,519 --> 00:33:42,399
y usf uno de los autores de este artículo

857
00:33:42,399 --> 00:33:43,840
estuvimos hablando aquí

858
00:33:43,840 --> 00:33:47,039
eh y también por la universidad de sussex

859
00:33:47,039 --> 00:33:49,279
hizo una contribución increíble a los

860
00:33:49,279 --> 00:33:52,000
campos con su libro siendo tú  donde

861
00:33:52,000 --> 00:33:54,399
afirma que somos máquinas bestias cuya

862
00:33:54,399 --> 00:33:58,240
percepción es una alucinación controlada

863
00:33:58,240 --> 00:33:59,760
y mejora las

864
00:33:59,760 --> 00:34:01,840
descripciones de la conciencia utilizando un

865
00:34:01,840 --> 00:34:04,159
marco de procesamiento predictivo y estableciendo el

866
00:34:04,159 --> 00:34:07,039
problema real de la conciencia que, en sus propias

867
00:34:07,039 --> 00:34:09,280
palabras, requiere explicar por qué un

868
00:34:09,280 --> 00:34:12,000
patrón particular de actividad cerebral u

869
00:34:12,000 --> 00:34:13,679
otro proceso físico

870
00:34:13,679 --> 00:34:15,599
mapea a  un tipo particular de co  La

871
00:34:15,599 --> 00:34:18,159
experiencia consciente no solo establece

872
00:34:18,159 --> 00:34:19,520
que son estos

873
00:34:19,520 --> 00:34:21,520
dos filósofos y científicos en los que

874
00:34:21,520 --> 00:34:23,359
trabajan

875
00:34:23,359 --> 00:34:25,440
y

876
00:34:25,440 --> 00:34:26,159


877
00:34:26,159 --> 00:34:28,720
aquí están

878
00:34:32,839 --> 00:34:36,480
bien, así que

879
00:34:36,480 --> 00:34:37,280


880
00:34:37,280 --> 00:34:39,199
estaba hablando de codificación predictiva y

881
00:34:39,199 --> 00:34:41,520
luego pasé al procesamiento predictivo. ¿

882
00:34:41,520 --> 00:34:43,918
Hay alguna diferencia?

883
00:34:43,918 --> 00:34:45,918
Bueno, muchos autores

884
00:34:45,918 --> 00:34:49,839
usarían los términos libremente.  está bien, pero

885
00:34:49,839 --> 00:34:52,800
luego encontré a Clark diciendo en su libro,

886
00:34:52,800 --> 00:34:56,000
uh, la diferencia que él entiende

887
00:34:56,000 --> 00:34:57,839
de la codificación predictiva al

888
00:34:57,839 --> 00:35:00,480
procesamiento predictivo, por lo que

889
00:35:00,480 --> 00:35:03,359
plantea el hecho de que el procesamiento predictivo

890
00:35:03,359 --> 00:35:06,160
no es simplemente el uso de la

891
00:35:06,160 --> 00:35:08,320
estrategia de compresión de datos conocida como codificación predictiva,

892
00:35:08,320 --> 00:35:11,680
sino el uso  de esa

893
00:35:11,680 --> 00:35:14,839
estrategia en el contexto muy especial de los

894
00:35:14,839 --> 00:35:17,200


895
00:35:17,200 --> 00:35:19,119
sistemas jerárquicos de múltiples niveles que implementan

896
00:35:19,119 --> 00:35:20,960
modelos generativos probabilísticos,

897
00:35:20,960 --> 00:35:23,599
tales sistemas exhiben formas poderosas de

898
00:35:23,599 --> 00:35:26,000
aprendizaje y brindan formas ricas de

899
00:35:26,000 --> 00:35:28,839
procesamiento sensible al contexto y son

900
00:35:28,839 --> 00:35:32,320
capaces de combinar de manera flexible

901
00:35:32,320 --> 00:35:34,480
flujos de información de arriba hacia abajo y de abajo hacia arriba  dentro

902
00:35:34,480 --> 00:35:36,800
del

903
00:35:36,800 --> 00:35:38,720
procesamiento predictivo en cascada multicapa combina el

904
00:35:38,720 --> 00:35:41,839
uso dentro de un m  cascada bidireccional de niveles múltiples de

905
00:35:41,839 --> 00:35:43,839


906
00:35:43,839 --> 00:35:46,079
modelos generales probabilísticos de arriba hacia abajo

907
00:35:46,079 --> 00:35:49,040
con la estrategia central de codificación predictiva

908
00:35:49,040 --> 00:35:54,079
de codificación y transmisión eficientes.

909
00:35:55,040 --> 00:35:57,279


910
00:35:57,359 --> 00:35:58,720


911
00:35:58,720 --> 00:36:00,880


912
00:36:00,880 --> 00:36:02,640


913
00:36:02,640 --> 00:36:04,160


914
00:36:04,160 --> 00:36:07,040


915
00:36:07,040 --> 00:36:10,480
De acuerdo, la codificación predictiva es una cascada local de

916
00:36:10,480 --> 00:36:14,240
abajo hacia arriba y esas cosas,

917
00:36:14,240 --> 00:36:16,560
pero también es

918
00:36:16,560 --> 00:36:18,079
multicapa,

919
00:36:18,079 --> 00:36:22,400
sí, también tiene cierta jerarquía, así

920
00:36:22,400 --> 00:36:24,800
que no estoy seguro de lo que quiere decir,

921
00:36:24,800 --> 00:36:26,960
no estoy seguro de si entiendo

922
00:36:26,960 --> 00:36:28,880
la diferencia entre  la codificación

923
00:36:28,880 --> 00:36:33,119
predictiva y el procesamiento predictivo, como dice

924
00:36:33,440 --> 00:36:36,079
sí, es muy perspicaz

925
00:36:36,079 --> 00:36:37,760
y me hizo pensar en la

926
00:36:37,760 --> 00:36:40,560
diferencia entre la codificación como la

927
00:36:40,560 --> 00:36:43,599
programación y la codificación de datos como la codificación de

928
00:36:43,599 --> 00:36:45,920
Python para datos y

929
00:36:45,920 --> 00:36:47,920
procesamiento de datos, que es un poco como

930
00:36:47,920 --> 00:36:50,400
podría ser múltiples lenguajes informáticos y

931
00:36:50,400 --> 00:36:52,560
así  realmente está destacando algo

932
00:36:52,560 --> 00:36:55,280
importante, que es que cuando los datos

933
00:36:55,280 --> 00:36:58,400
se comprimen de acuerdo con una estrategia

934
00:36:58,400 --> 00:37:00,880
en la que se transmiten los errores en lugar de los errores

935
00:37:00,880 --> 00:37:03,119
reales  una especie de estimación en

936
00:37:03,119 --> 00:37:05,760
sí misma, es como si ese

937
00:37:05,760 --> 00:37:09,440
canal de información estuviera en una forma de codificación predictiva

938
00:37:09,440 --> 00:37:12,240
y luego esto nos ayuda a

939
00:37:12,240 --> 00:37:14,320
usar el procesamiento predictivo

940
00:37:14,320 --> 00:37:16,480
para una imagen más amplia de los sistemas

941
00:37:16,480 --> 00:37:19,040
que implementan módulos de codificación predictiva,

942
00:37:19,040 --> 00:37:20,800
pero ese no es el único módulo

943
00:37:20,800 --> 00:37:22,800
que están implementando para eso.  Será

944
00:37:22,800 --> 00:37:24,960
algo genial escuchar la

945
00:37:24,960 --> 00:37:26,720
perspectiva de la gente sobre una distinción realmente agradable

946
00:37:26,720 --> 00:37:29,680
aquí, aunque

947
00:37:30,000 --> 00:37:30,960
sí,

948
00:37:30,960 --> 00:37:33,440


949
00:37:34,400 --> 00:37:39,920
está bien, así que escribí algo aquí que,

950
00:37:39,920 --> 00:37:42,560
al igual que dijiste, tal vez

951
00:37:42,560 --> 00:37:44,320
sería mejor usar la codificación para

952
00:37:44,320 --> 00:37:46,000
formalismos e implementaciones y

953
00:37:46,000 --> 00:37:47,440
procesamiento para la comprensión filosófica de

954
00:37:47,440 --> 00:37:49,520
que  la predicción es la

955
00:37:49,520 --> 00:37:53,200
base de la interpretación de una señal,

956
00:37:53,200 --> 00:37:57,040
por ejemplo, en oposición a la descripción,

957
00:38:00,720 --> 00:38:03,200
está bien,

958
00:38:03,200 --> 00:38:04,720
así que,

959
00:38:04,720 --> 00:38:06,880
volviendo al

960
00:38:06,880 --> 00:38:09,040
formalismo, la

961
00:38:09,040 --> 00:38:11,680
codificación predictiva, la

962
00:38:11,680 --> 00:38:14,720
teoría de la información y el procesamiento de señales, está bien,

963
00:38:14,720 --> 00:38:17,280
otra influencia intelectual profunda en la

964
00:38:17,280 --> 00:38:20,240
codificación predictiva proviene de la

965
00:38:20,240 --> 00:38:23,119
teoría de la información del canal en 1948

966
00:38:23,119 --> 00:38:25,599
y especialmente el mínimo.

967
00:38:25,599 --> 00:38:31,040
principio de redundancia de barlow en 1961 89 y 61.

968
00:38:31,040 --> 00:38:33,040
dos

969
00:38:33,040 --> 00:38:35,599
La teoría de la información nos dice que la

970
00:38:35,599 --> 00:38:38,720
información es inseparable de la falta

971
00:38:38,720 --> 00:38:40,880
de previsibilidad.

972
00:38:40,880 --> 00:38:43,280
Si algo es predecible antes de

973
00:38:43,280 --> 00:38:46,480
observarlo, no puede brindarnos mucha

974
00:38:46,480 --> 00:38:50,720
información. Me encantó esta oración. A la

975
00:38:51,520 --> 00:38:54,240
inversa, para maximizar la tasa de

976
00:38:54,240 --> 00:38:56,880
transferencia de información, el mensaje debe ser

977
00:38:56,880 --> 00:38:59,520
mínimamente predecible y mejorar la

978
00:38:59,520 --> 00:39:01,119


979
00:39:01,119 --> 00:39:03,760
codificación predictiva mínimamente redundante.  Como un medio para eliminar la

980
00:39:03,760 --> 00:39:06,320
redundancia en una señal, se aplicó por primera vez

981
00:39:06,320 --> 00:39:08,240
en el procesamiento de señales,

982
00:39:08,240 --> 00:39:10,880
donde se usó para reducir el

983
00:39:10,880 --> 00:39:12,320
ancho de banda de transmisión

984
00:39:12,320 --> 00:39:15,839
para la transmisión de video

985
00:39:16,079 --> 00:39:16,960


986
00:39:16,960 --> 00:39:18,960
. ¿Quiere decir algo? Simplemente

987
00:39:18,960 --> 00:39:20,960
describiré esto sin leerlo.

988
00:39:20,960 --> 00:39:24,800
se

989
00:39:24,800 --> 00:39:27,280
puede mejorar con otros

990
00:39:27,280 --> 00:39:29,680
métodos es un método para la codificación de video conocido

991
00:39:29,680 --> 00:39:33,040
como diferenciación de cuadros donde básicamente

992
00:39:33,040 --> 00:39:35,520
se describe un cuadro y luego

993
00:39:35,520 --> 00:39:37,839
los cuadros posteriores solo

994
00:39:37,839 --> 00:39:39,920
se deben describir los píxeles que cambian, por

995
00:39:39,920 --> 00:39:42,640
lo que es muy parecido a una analogía con la

996
00:39:42,640 --> 00:39:44,240
codificación predictiva

997
00:39:44,240 --> 00:39:45,599
en  el sentido de

998
00:39:45,599 --> 00:39:47,280
que es capaz de conectar las diferencias

999
00:39:47,280 --> 00:39:49,680
entre tim subsiguientes  e pasos y use

1000
00:39:49,680 --> 00:39:52,400
eso para dar realmente solo las

1001
00:39:52,400 --> 00:39:55,040
piezas informativas como si tuviera un número de 100 dígitos

1002
00:39:55,040 --> 00:39:56,560
y luego solo le diré que el

1003
00:39:56,560 --> 00:39:59,200
tercer dígito cambió ahora es un 8 en

1004
00:39:59,200 --> 00:40:01,119
lugar de tener que repetir el mismo

1005
00:40:01,119 --> 00:40:04,079
número y luego 99 de esos

1006
00:40:04,079 --> 00:40:05,760
dígitos son informativos en el sentido de

1007
00:40:05,760 --> 00:40:08,079
que son buena información, es cierto,

1008
00:40:08,079 --> 00:40:09,760
pero no son informativos en el

1009
00:40:09,760 --> 00:40:12,480
contexto de la teoría de la información porque

1010
00:40:12,480 --> 00:40:14,720
eran bastante predecibles antes de observarlo

1011
00:40:14,720 --> 00:40:16,879


1012
00:40:18,079 --> 00:40:19,119
bien

1013
00:40:19,119 --> 00:40:22,800
, por lo que los esquemas iniciales utilizaron un

1014
00:40:22,800 --> 00:40:25,200
enfoque simple de restar lo nuevo a  se

1015
00:40:25,200 --> 00:40:28,319
transmitirá el cuadro desde el cuadro anterior

1016
00:40:28,319 --> 00:40:30,480
usando una predicción trivial de que

1017
00:40:30,480 --> 00:40:32,960
el cuadro nuevo es siempre el mismo que

1018
00:40:32,960 --> 00:40:35,440
el cuadro anterior, lo que funciona bien para reducir el

1019
00:40:35,440 --> 00:40:38,160
ancho de banda en muchas configuraciones donde

1020
00:40:38,160 --> 00:40:41,040
solo hay unos pocos objetos moviéndose en el video

1021
00:40:41,040 --> 00:40:42,240
contra este

1022
00:40:42,240 --> 00:40:44,079
fondo estático

1023
00:40:44,079 --> 00:40:46,800
y más  los métodos avanzados a menudo predicen

1024
00:40:46,800 --> 00:40:49,599
cada nuevo cuadro utilizando una cantidad de

1025
00:40:49,599 --> 00:40:52,800
cuadros anteriores ponderados por algún coeficiente,

1026
00:40:52,800 --> 00:40:55,040
un enfoque conocido como codificación predictiva lineal,

1027
00:40:55,040 --> 00:40:57,440


1028
00:41:03,680 --> 00:41:05,200
está bien,

1029
00:41:05,200 --> 00:41:07,839
ahora

1030
00:41:08,319 --> 00:41:10,960
codificación predictiva  en el ojo

1031
00:41:10,960 --> 00:41:12,800
y el cerebro,

1032
00:41:12,800 --> 00:41:14,880
la primera discusión concreta sobre la

1033
00:41:14,880 --> 00:41:16,960
codificación predictiva en el neurosistema

1034
00:41:16,960 --> 00:41:19,280
surgió como un modelo de propiedades neuronales de

1035
00:41:19,280 --> 00:41:20,640
la retina

1036
00:41:20,640 --> 00:41:21,800
con

1037
00:41:21,800 --> 00:41:25,359
srinivasin 1982.

1038
00:41:25,359 --> 00:41:27,520
específicamente como un modelo de

1039
00:41:27,520 --> 00:41:30,079
células rodeadas por el centro

1040
00:41:30,079 --> 00:41:32,960
que se disparan cuando se les presenta un

1041
00:41:32,960 --> 00:41:35,440
punto claro contra un oscuro  fondo en el

1042
00:41:35,440 --> 00:41:38,560
centro del entorno o, alternativamente, un

1043
00:41:38,560 --> 00:41:41,520
punto oscuro sobre un fondo claro del

1044
00:41:41,520 --> 00:41:43,760
centro de las celdas del entorno

1045
00:41:43,760 --> 00:41:47,040
. Se argumentó que este esquema de codificación

1046
00:41:47,040 --> 00:41:49,200
ayuda a minimizar la redundancia en el

1047
00:41:49,200 --> 00:41:51,440
esquema visual específicamente eliminando

1048
00:41:51,440 --> 00:41:53,920
la redundancia espacial en escenas visuales naturales

1049
00:41:53,920 --> 00:41:55,440


1050
00:41:55,440 --> 00:41:57,839
que la intensidad de

1051
00:41:57,839 --> 00:42:00,880
un píxel  ayuda a predecir bastante bien la

1052
00:42:00,880 --> 00:42:04,960
intensidad de los píxeles vecinos

1053
00:42:05,040 --> 00:42:09,759
y luego la primera imagen relacionada con ella

1054
00:42:10,240 --> 00:42:14,560
, ¿quieres hablar de eso o

1055
00:42:14,560 --> 00:42:16,000
simplemente, vamos a ver dos

1056
00:42:16,000 --> 00:42:18,880
tejidos diferentes del sistema nervioso

1057
00:42:18,880 --> 00:42:21,599
que tienen algunos elementos predictivos

1058
00:42:21,599 --> 00:42:23,680
y esto?  se refiere a algunas de las

1059
00:42:23,680 --> 00:42:26,240
células fotosensibles que se encuentran en la

1060
00:42:26,240 --> 00:42:28,880
retina y ese es un trabajo que se ha

1061
00:42:28,880 --> 00:42:29,680
realizado con

1062
00:42:29,680 --> 00:42:31,520
calidad  itativamente y empíricamente durante

1063
00:42:31,520 --> 00:42:33,839
cientos de años, pero luego se conectó

1064
00:42:33,839 --> 00:42:36,880
con perspectivas predictivas en los años 80

1065
00:42:36,880 --> 00:42:38,319
y luego también veremos, ya

1066
00:42:38,319 --> 00:42:41,359
que ahora desempacará lo que sucede en

1067
00:42:41,359 --> 00:42:43,760
la corteza del cerebro entre

1068
00:42:43,760 --> 00:42:46,480
diferentes capas

1069
00:42:47,280 --> 00:42:50,880
con la fruta del mes en 1992 fue  quizás el

1070
00:42:50,880 --> 00:42:53,119
primero en extender esta teoría de la

1071
00:42:53,119 --> 00:42:55,359
retina y el

1072
00:42:55,359 --> 00:42:58,800
lgn a una teoría general completa de

1073
00:42:58,800 --> 00:43:00,400
la función cortical.

1074
00:43:00,400 --> 00:43:02,720


1075
00:43:02,720 --> 00:43:05,200


1076
00:43:05,200 --> 00:43:08,160


1077
00:43:08,160 --> 00:43:10,480


1078
00:43:10,480 --> 00:43:13,359


1079
00:43:13,359 --> 00:43:15,760
El camino hacia adelante se origina en

1080
00:43:15,760 --> 00:43:18,319
las capas superficiales de la corteza y

1081
00:43:18,319 --> 00:43:20,240
las vías de retroalimentación se originaron

1082
00:43:20,240 --> 00:43:22,079
principalmente

1083
00:43:22,079 --> 00:43:25,200
en las capas profundas,

1084
00:43:25,359 --> 00:43:27,520
así que tuvimos el

1085
00:43:27,520 --> 00:43:29,440
desarrollo de esta filosofía durante miles de

1086
00:43:29,440 --> 00:43:30,400
años

1087
00:43:30,400 --> 00:43:33,520
y luego, en la década de 1900, la gente

1088
00:43:33,520 --> 00:43:36,079
comenzó a conectar la neuroanatomía y

1089
00:43:36,079 --> 00:43:37,680
el  histología

1090
00:43:37,680 --> 00:43:39,760
a algunas de estas ideas predictivas y

1091
00:43:39,760 --> 00:43:42,160
luego ahora cerremos el lo  op

1092
00:43:42,160 --> 00:43:44,480
y traerlo de vuelta al trabajo crítico

1093
00:43:44,480 --> 00:43:46,640
que fristen destacó en su

1094
00:43:46,640 --> 00:43:48,960
artículo de 2018

1095
00:43:50,079 --> 00:43:52,079
yep y cómo la codificación predictiva en el

1096
00:43:52,079 --> 00:43:55,359
cerebro se vuelve matemática computacional

1097
00:43:55,359 --> 00:43:57,920
mientras que la teoría de montford contenía la mayoría de los

1098
00:43:57,920 --> 00:44:00,480
aspectos de la teoría clásica de codificación predictiva

1099
00:44:00,480 --> 00:44:02,560
en la corteza cerebral,

1100
00:44:02,560 --> 00:44:06,880
no era un compuesto acompañado de

1101
00:44:06,880 --> 00:44:09,680
simulaciones  o trabajo empírico, por

1102
00:44:09,680 --> 00:44:11,599
lo que su potencial como marco para

1103
00:44:11,599 --> 00:44:13,280
comprender la corteza no se

1104
00:44:13,280 --> 00:44:14,720
apreció por completo

1105
00:44:14,720 --> 00:44:17,359
y, nuevamente, el trabajo seminal de raw y

1106
00:44:17,359 --> 00:44:19,440
ballard en 1999

1107
00:44:19,440 --> 00:44:23,359
tuvo su impacto precisamente al hacer esto

1108
00:44:23,359 --> 00:44:25,920
y crearon una pequeña

1109
00:44:25,920 --> 00:44:27,839
red de codificación predictiva de

1110
00:44:27,839 --> 00:44:30,160
acuerdo con los principios

1111
00:44:30,160 --> 00:44:32,240
lo

1112
00:44:32,240 --> 00:44:34,480
siento, crearon una pequeña

1113
00:44:34,480 --> 00:44:36,640
red de codificación predictiva de acuerdo con los

1114
00:44:36,640 --> 00:44:38,960
principios identificados por mumford e

1115
00:44:38,960 --> 00:44:41,520
investigaron empíricamente su comportamiento,

1116
00:44:41,520 --> 00:44:43,359
lo que demuestra que la interacción compleja y

1117
00:44:43,359 --> 00:44:46,160
dinámica de las predicciones y

1118
00:44:46,160 --> 00:44:48,640
los errores de predicción podría explicar varios

1119
00:44:48,640 --> 00:44:50,440


1120
00:44:50,440 --> 00:44:52,000
fenómenos neurofisiológicos desconcertantes,

1121
00:44:52,000 --> 00:44:54,800
específicamente los

1122
00:44:54,800 --> 00:44:57,119
efectos del campo receptivo extracurricular su  ch como y detener las

1123
00:44:57,119 --> 00:44:59,200
neuronas,

1124
00:44:59,200 --> 00:45:02,160
sí, esto es bastante interesante porque

1125
00:45:02,160 --> 00:45:04,480
incluso la idea de que las neuronas que están

1126
00:45:04,480 --> 00:45:06,800
activas mientras se presentan ciertos estímulos

1127
00:45:06,800 --> 00:45:08,720
son neuronas que

1128
00:45:08,720 --> 00:45:10,400
son receptivas

1129
00:45:10,400 --> 00:45:12,560
a esos estímulos que están dentro del

1130
00:45:12,560 --> 00:45:15,599
paradigma de la recepción de señales

1131
00:45:15,599 --> 00:45:17,680
y luego es como si hubiera un clásico

1132
00:45:17,680 --> 00:45:19,520
la recepción de la señal sabes que el lanzador

1133
00:45:19,520 --> 00:45:21,200
lanza la pelota el receptor atrapa la

1134
00:45:21,200 --> 00:45:22,880
pelota es clásico

1135
00:45:22,880 --> 00:45:25,040
pero desafortunadamente o no, existen todos

1136
00:45:25,040 --> 00:45:28,079
estos llamados efectos extraclásicos,

1137
00:45:28,079 --> 00:45:30,240
por lo que este documento propuso una

1138
00:45:30,240 --> 00:45:33,599
arquitectura simple que pudo abarcar

1139
00:45:33,599 --> 00:45:35,920
los llamados efectos clásicos así como  algunos

1140
00:45:35,920 --> 00:45:39,280
de los efectos extraclásicos importantes

1141
00:45:39,280 --> 00:45:42,160
bajo este

1142
00:45:42,160 --> 00:45:44,800
marco de procesamiento predictivo y eso realmente condujo a

1143
00:45:44,800 --> 00:45:47,119
muchos desarrollos en neurociencia en

1144
00:45:47,119 --> 00:45:49,920
los últimos 20 años

1145
00:45:49,920 --> 00:45:52,160
y eso es exactamente en lo que vamos a

1146
00:45:52,160 --> 00:45:55,040
entrar ahora, así que gracias por proporcionar ese

1147
00:45:55,040 --> 00:45:58,000
contexto increíble como  Creo que realmente

1148
00:45:58,000 --> 00:45:59,280
ayuda a

1149
00:45:59,280 --> 00:46:01,119
ubicar algunos de los detalles que

1150
00:46:01,119 --> 00:46:03,680
estamos a punto de analizar. Está

1151
00:46:03,680 --> 00:46:09,118
bien, así que veamos un poco.  técnico

1152
00:46:09,280 --> 00:46:10,960
lo que sucedió después de

1153
00:46:10,960 --> 00:46:14,160
la síntesis de 1999 de round ballard así

1154
00:46:14,160 --> 00:46:16,079
que recuerde que carl fristen dijo que

1155
00:46:16,079 --> 00:46:17,680
ese era como uno de esos artículos de una vez en una

1156
00:46:17,680 --> 00:46:21,200
década para él, entonces, ¿cómo

1157
00:46:21,200 --> 00:46:23,520
cambió sus políticas de publicación de selección de

1158
00:46:23,520 --> 00:46:26,480
acciones después de leer ese artículo?

1159
00:46:26,480 --> 00:46:30,480
cualquiera puede adivinar, pero ¿qué hizo fristen?

1160
00:46:30,480 --> 00:46:33,440
fue que lanzó el algoritmo de codificación predictiva

1161
00:46:33,440 --> 00:46:34,720


1162
00:46:34,720 --> 00:46:37,760
como inferencia bayesiana aproximada

1163
00:46:37,760 --> 00:46:40,079
sobre modelos generativos gaussianos

1164
00:46:40,079 --> 00:46:42,319
y esto se conectará todo esto

1165
00:46:42,319 --> 00:46:44,800
está en el documento para que la gente lo lea,

1166
00:46:44,800 --> 00:46:46,560
pero esto conecta

1167
00:46:46,560 --> 00:46:49,599
básicamente todos los temas anteriores de los

1168
00:46:49,599 --> 00:46:51,599
que habíamos estado hablando

1169
00:46:51,599 --> 00:46:53,520
como el  la redundancia mínima de la teoría de la información

1170
00:46:53,520 --> 00:46:56,480
y la idea helmholtziana de la

1171
00:46:56,480 --> 00:46:59,839
percepción como inferencia se unen

1172
00:46:59,839 --> 00:47:02,480
en la perspectiva bayesiana

1173
00:47:02,480 --> 00:47:04,720
de la arquitectura de codificación predictiva de

1174
00:47:04,720 --> 00:47:06,319
rao y ballard,

1175
00:47:06,319 --> 00:47:07,040
por lo que

1176
00:47:07,040 --> 00:47:09,440
los autores escriben que el primer enfoque

1177
00:47:09,440 --> 00:47:12,560
reformula el

1178
00:47:12,560 --> 00:47:14,480
modelo redondo de ballard, en su mayoría heurístico, en el lenguaje de

1179
00:47:14,480 --> 00:47:16,480
la inferencia bayesiana variacional

1180
00:47:16,480 --> 00:47:18,640
que  va a mirar más

1181
00:47:18,640 --> 00:47:20,319
y fristen mostró que  la

1182
00:47:20,319 --> 00:47:22,880
función de energía en rao y ballard

1183
00:47:22,880 --> 00:47:24,880
se puede entender como una energía libre variacional

1184
00:47:24,880 --> 00:47:27,359
del tipo que se minimiza a

1185
00:47:27,359 --> 00:47:29,599
través de la inferencia variacional, de

1186
00:47:29,599 --> 00:47:31,520
modo que realmente conectó los puntos

1187
00:47:31,520 --> 00:47:32,720
entre

1188
00:47:32,720 --> 00:47:35,119
las arquitecturas de codificación predictiva y los

1189
00:47:35,119 --> 00:47:37,520
hallazgos biológicos empíricos a todo

1190
00:47:37,520 --> 00:47:39,440
este trabajo sobre la inferencia bayesiana variacional

1191
00:47:39,440 --> 00:47:40,480


1192
00:47:40,480 --> 00:47:43,520
y esos son  los artículos de 5 y 8 autores individuales de 2003

1193
00:47:43,520 --> 00:47:46,800
de carl,

1194
00:47:46,800 --> 00:47:48,319
entonces,

1195
00:47:48,319 --> 00:47:50,400
¿qué son los enfoques variacional

1196
00:47:50,400 --> 00:47:53,119
y bayesiano y bayesiano variacional

1197
00:47:53,119 --> 00:47:54,960
y solo vamos a

1198
00:47:54,960 --> 00:47:56,559
ver las palabras del autor y hay

1199
00:47:56,559 --> 00:47:58,640
muchos lugares increíbles para buscar

1200
00:47:58,640 --> 00:48:01,359
materiales educativos y también

1201
00:48:01,359 --> 00:48:04,079
ver algunos de los  transmisiones en vivo como 26 32

1202
00:48:04,079 --> 00:48:07,440
34 37 y 39,

1203
00:48:07,440 --> 00:48:10,319
así que para ser breve, los autores

1204
00:48:10,319 --> 00:48:11,599
escriben

1205
00:48:11,599 --> 00:48:14,880
que la inferencia variacional se aproxima a un

1206
00:48:14,880 --> 00:48:17,359
problema de inferencia intratable

1207
00:48:17,359 --> 00:48:20,319
con un problema de optimización tratable, por lo

1208
00:48:20,319 --> 00:48:23,280
que es un problema súper difícil de resolver con

1209
00:48:23,280 --> 00:48:25,599
solo pensarlo y luego

1210
00:48:25,599 --> 00:48:27,280
adivinar la respuesta correcta

1211
00:48:27,280 --> 00:48:30,160
con un problema de optimización manejable, por

1212
00:48:30,160 --> 00:48:31,760
lo que son como 20 preguntas, será

1213
00:48:31,760 --> 00:48:34,000
difícil  para adivinar su primera pregunta, pero

1214
00:48:34,000 --> 00:48:36,079
si toma este

1215
00:48:36,079 --> 00:48:38,240
enfoque de optimización iterativa, tal vez haya una manera de

1216
00:48:38,240 --> 00:48:40,319
resolverlo, por lo que no es exactamente así,

1217
00:48:40,319 --> 00:48:42,079
pero es como volver a sacar

1218
00:48:42,079 --> 00:48:43,839
algo que es difícil de resolver de una sola vez

1219
00:48:43,839 --> 00:48:46,160
con un enfoque

1220
00:48:46,160 --> 00:48:48,800
y este iterativo  mejora del tipo de descenso de gradiente

1221
00:48:48,800 --> 00:48:50,800
que realmente lo lleva a una

1222
00:48:50,800 --> 00:48:52,640
buena solución

1223
00:48:52,640 --> 00:48:53,839
y aquí es donde vamos a

1224
00:48:53,839 --> 00:48:56,240
presentar algunas de las variables y las

1225
00:48:56,240 --> 00:48:59,040
letras que vamos a usar

1226
00:48:59,040 --> 00:49:02,559
después de mucho trabajo perceptivo en

1227
00:49:02,559 --> 00:49:05,440
estadísticas bayesianas que vamos a  usar o

1228
00:49:05,440 --> 00:49:08,319
para observaciones que son como datos,

1229
00:49:08,319 --> 00:49:10,319
ya sea los datos reales que

1230
00:49:10,319 --> 00:49:13,119
nos proporciona el sensor o datos generados del

1231
00:49:13,119 --> 00:49:14,800
tipo que esperaríamos que nos proporcione el sensor

1232
00:49:14,800 --> 00:49:17,359
y luego x

1233
00:49:17,359 --> 00:49:20,160
son los estados latentes o no observados

1234
00:49:20,160 --> 00:49:22,480
del sistema, por lo que x sería  sea ​​la

1235
00:49:22,480 --> 00:49:25,200
temperatura en la habitación y o es

1236
00:49:25,200 --> 00:49:27,839
la lectura del termómetro

1237
00:49:27,839 --> 00:49:29,920
y eso es una especie de enfoque bayesiano

1238
00:49:29,920 --> 00:49:32,559
que nos ayuda a tener un modelo

1239
00:49:32,559 --> 00:49:35,280
generativo del proceso de generación de datos que es

1240
00:49:35,280 --> 00:49:37,280
un  También se conoce como la distribución conjunta

1241
00:49:37,280 --> 00:49:40,000
porque es conjuntamente sobre las

1242
00:49:40,000 --> 00:49:44,640
observaciones y los estados latentes

1243
00:49:44,640 --> 00:49:46,880
donde entra en

1244
00:49:46,880 --> 00:49:47,920
juego la

1245
00:49:47,920 --> 00:49:49,599
inferencia variacional. Es lo que

1246
00:49:49,599 --> 00:49:52,000
se insinúa en este párrafo sobre la base,

1247
00:49:52,000 --> 00:49:55,200
por lo que para obtener la base exacta

1248
00:49:55,200 --> 00:49:57,280
habría que

1249
00:49:57,280 --> 00:49:59,599
encontrar este factor de normalización.

1250
00:49:59,599 --> 00:50:01,599
sin embargo, eso puede ser intratable

1251
00:50:01,599 --> 00:50:03,760
porque requiere básicamente integrar o

1252
00:50:03,760 --> 00:50:07,119
sumar todos los estados variables latentes

1253
00:50:07,119 --> 00:50:08,720
y eso no siempre es

1254
00:50:08,720 --> 00:50:10,480
fácil o conocido,

1255
00:50:10,480 --> 00:50:12,000
por lo que el enfoque del método variacional

1256
00:50:12,000 --> 00:50:13,119


1257
00:50:13,119 --> 00:50:14,880
apunta a

1258
00:50:14,880 --> 00:50:17,200
aproximar el posterior

1259
00:50:17,200 --> 00:50:20,079
usando un posterior auxiliar

1260
00:50:20,079 --> 00:50:22,800
usando un conjunto diferente de parámetros

1261
00:50:22,800 --> 00:50:24,000
phi

1262
00:50:24,000 --> 00:50:25,440
para que el

1263
00:50:25,440 --> 00:50:27,359
q va a reflejar como la

1264
00:50:27,359 --> 00:50:28,960
distribución la distribución variacional

1265
00:50:28,960 --> 00:50:31,119
que controlamos,

1266
00:50:31,119 --> 00:50:33,920
que es como una familia de funciones

1267
00:50:33,920 --> 00:50:35,920
que es más

1268
00:50:35,920 --> 00:50:39,200
susceptible de optimización, por lo que p podría ser

1269
00:50:39,200 --> 00:50:42,319
como una función súper desordenada, pero q

1270
00:50:42,319 --> 00:50:44,720
va a ser construida

1271
00:50:44,720 --> 00:50:47,520
por el modelador para ser  mucho más simple

1272
00:50:47,520 --> 00:50:49,599
, por lo que tendrá sus propios

1273
00:50:49,599 --> 00:50:52,400
parámetros y esos parámetros phi

1274
00:50:52,400 --> 00:50:53,599
son mu

1275
00:50:53,599 --> 00:50:54,960
y sigma,

1276
00:50:54,960 --> 00:50:55,839
etc.

1277
00:50:55,839 --> 00:50:58,079
es como si estuviéramos interesados

1278
00:50:58,079 --> 00:51:02,240
en la media y la varianza de x

1279
00:51:02,240 --> 00:51:04,640
la temperatura real en la habitación dado

1280
00:51:04,640 --> 00:51:06,880
el termómetro, pero ¿qué pasaría si fuera demasiado

1281
00:51:06,880 --> 00:51:08,960
difícil incluso obtener tan

1282
00:51:08,960 --> 00:51:10,720
bien? Lo que podríamos querer hacer sería

1283
00:51:10,720 --> 00:51:12,559
imaginar que la temperatura

1284
00:51:12,559 --> 00:51:14,720
normalmente se distribuyeron como una distribución gaussiana

1285
00:51:14,720 --> 00:51:15,920


1286
00:51:15,920 --> 00:51:18,640
con esta media mu en la varianza sigma,

1287
00:51:18,640 --> 00:51:20,319
por lo que incluso si la temperatura en la

1288
00:51:20,319 --> 00:51:22,720
habitación no tuviera una distribución gaussiana,

1289
00:51:22,720 --> 00:51:25,280
tal vez sea aproximadamente suficiente, por lo que

1290
00:51:25,280 --> 00:51:27,440
el enfoque variacional del

1291
00:51:27,440 --> 00:51:30,720
método bayesiano es introducir esta

1292
00:51:30,720 --> 00:51:32,640
cola de distribución variacional que es  va a

1293
00:51:32,640 --> 00:51:34,960
tener muchas características buenas en el

1294
00:51:34,960 --> 00:51:37,440


1295
00:51:37,440 --> 00:51:41,040
futuro. Así es como entra en juego esa señal.

1296
00:51:41,040 --> 00:51:43,359
Esta ecuación

1297
00:51:43,359 --> 00:51:45,839
se expresa usando las variables que

1298
00:51:45,839 --> 00:51:47,359
hemos visto

1299
00:51:47,359 --> 00:51:49,680
que dice y también presenta esta

1300
00:51:49,680 --> 00:51:52,000
divergencia d, así que primero solo la

1301
00:51:52,000 --> 00:51:53,760
definición de d y luego qué es esto.

1302
00:51:53,760 --> 00:51:55,760
el formalismo

1303
00:51:55,760 --> 00:51:56,960
dice que d

1304
00:51:56,960 --> 00:52:00,640
de p y doble línea q por lo que cualquier doble

1305
00:52:00,640 --> 00:52:02,880
línea significa entre esto y que d es una

1306
00:52:02,880 --> 00:52:04,480
función que mide la divergencia

1307
00:52:04,480 --> 00:52:06,880
ser  entre dos distribuciones, por ejemplo,

1308
00:52:06,880 --> 00:52:08,400
p y q,

1309
00:52:08,400 --> 00:52:11,200
y aquí la divergencia se

1310
00:52:11,200 --> 00:52:14,000
calculará como la divergencia kl,

1311
00:52:14,000 --> 00:52:16,960
aunque son posibles otras divergencias,

1312
00:52:16,960 --> 00:52:18,960
por lo que en la divergencia entre la

1313
00:52:18,960 --> 00:52:21,440
que simplificamos y controlamos q

1314
00:52:21,440 --> 00:52:24,240
y la intratable posterior verdadera

1315
00:52:24,240 --> 00:52:26,160
si se minimiza esa divergencia  a

1316
00:52:26,160 --> 00:52:29,040
cero sería como ajustar

1317
00:52:29,040 --> 00:52:32,480
p lo mejor que pudiéramos con q

1318
00:52:32,480 --> 00:52:34,559
y esto es decir en uno está diciendo q

1319
00:52:34,559 --> 00:52:36,960
estrella el mejor q

1320
00:52:36,960 --> 00:52:40,079
de x los estados latentes dados los datos

1321
00:52:40,079 --> 00:52:42,079
y los parámetros variacionales

1322
00:52:42,079 --> 00:52:44,079
para la mejor predicción posible de la temperatura ambiente

1323
00:52:44,079 --> 00:52:46,319
dadas las lecturas del termómetro

1324
00:52:46,319 --> 00:52:49,200
y los parámetros variacionales son

1325
00:52:49,200 --> 00:52:50,960
una minimización

1326
00:52:50,960 --> 00:52:51,920
sobre

1327
00:52:51,920 --> 00:52:54,480
todos los parámetros variacionales

1328
00:52:54,480 --> 00:52:55,440
de

1329
00:52:55,440 --> 00:52:57,760
la divergencia entre

1330
00:52:57,760 --> 00:52:59,680
la distribución q que controlamos

1331
00:52:59,680 --> 00:53:02,400
exactamente para lo que queremos la mejor respuesta

1332
00:53:02,400 --> 00:53:05,440
y el modelo generativo p de exactamente

1333
00:53:05,440 --> 00:53:07,839
qué sería lo mejor para calcular como

1334
00:53:07,839 --> 00:53:09,839
la temperatura dadas las observaciones

1335
00:53:09,839 --> 00:53:11,920
en el termómetro

1336
00:53:11,920 --> 00:53:13,839
y eso contribuye en gran medida a

1337
00:53:13,839 --> 00:53:15,760
reescribir un problema de inferencia como una

1338
00:53:15,760 --> 00:53:17,280
desviación  problema,

1339
00:53:17,280 --> 00:53:18,559
pero escriben,

1340
00:53:18,559 --> 00:53:20,400
sin embargo, simplemente escribir el problema de esta

1341
00:53:20,400 --> 00:53:22,480
manera no lo resuelve porque la

1342
00:53:22,480 --> 00:53:24,640
divergencia que necesitamos optimizar

1343
00:53:24,640 --> 00:53:26,559
todavía contiene el verdadero posterior intratable,

1344
00:53:26,559 --> 00:53:28,880
por lo que se reescribe como una

1345
00:53:28,880 --> 00:53:30,400
divergencia entre algo que

1346
00:53:30,400 --> 00:53:32,640
controlamos y algo que nos

1347
00:53:32,640 --> 00:53:34,319
propusimos hacer.  esto porque no pudimos

1348
00:53:34,319 --> 00:53:36,559
calcularlo, así que lo hemos

1349
00:53:36,559 --> 00:53:38,160
preparado, pero aún

1350
00:53:38,160 --> 00:53:41,359
así es intratable, por lo que no se puede usar

1351
00:53:41,359 --> 00:53:43,040
solo, escriben que la belleza de la

1352
00:53:43,040 --> 00:53:45,599
inferencia variacional es que, en cambio, optimiza un

1353
00:53:45,599 --> 00:53:48,240
límite superior manejable en esta divergencia

1354
00:53:48,240 --> 00:53:51,839
llamada energía libre variacional vfe

1355
00:53:51,839 --> 00:53:54,000
para  generamos el límite, aplicamos la

1356
00:53:54,000 --> 00:53:56,240
regla de Bayes a la parte posterior verdadera para reescribirla

1357
00:53:56,240 --> 00:53:58,079
en la forma del modelo generativo y

1358
00:53:58,079 --> 00:53:59,200
la evidencia,

1359
00:53:59,200 --> 00:54:01,760
así que aquí está esa p de x dado o

1360
00:54:01,760 --> 00:54:04,800
que se reescribe como ahora no de x dado o

1361
00:54:04,800 --> 00:54:07,520
sino p de o coma x esa es la articulación

1362
00:54:07,520 --> 00:54:10,880
distribución dividida por las observaciones por

1363
00:54:10,880 --> 00:54:12,640
lo que esto

1364
00:54:12,640 --> 00:54:15,119
por la regla de bayes p de x dado o es

1365
00:54:15,119 --> 00:54:16,800
equivalente a esto por eso esta es

1366
00:54:16,800 --> 00:54:18,880
la primera línea

1367
00:54:18,880 --> 00:54:21,520
aquí proporcionan algunos rewr  itings

1368
00:54:21,520 --> 00:54:24,000
y luego escriben en la tercera línea

1369
00:54:24,000 --> 00:54:25,680
la expectativa

1370
00:54:25,680 --> 00:54:29,040
alrededor de p de o, por lo que la probabilidad real

1371
00:54:29,040 --> 00:54:31,359
de las observaciones en sí mismas, como qué tan

1372
00:54:31,359 --> 00:54:34,800
probable es que el termómetro diga

1373
00:54:34,800 --> 00:54:36,559
21, desaparece

1374
00:54:36,559 --> 00:54:38,559
ya que la expectativa es sobre la

1375
00:54:38,559 --> 00:54:42,000
variable x que no está en p de o,

1376
00:54:42,000 --> 00:54:45,119
por lo que es  como cuál es la expectativa de

1377
00:54:45,119 --> 00:54:47,760
esta moneda voltear la temperatura

1378
00:54:47,760 --> 00:54:50,880
mañana es como porque son

1379
00:54:50,880 --> 00:54:53,119
variables diferentes

1380
00:54:53,119 --> 00:54:55,200
, hace que sea muy fácil separarlas

1381
00:54:55,200 --> 00:54:58,240
si solo estamos interesados ​​​​en una de ellas

1382
00:54:58,240 --> 00:55:00,480
y luego escriben que esta es

1383
00:55:00,480 --> 00:55:02,480
la energía libre

1384
00:55:02,480 --> 00:55:04,720
una cantidad muy atrayente ya

1385
00:55:04,720 --> 00:55:06,799
que es una divergencia entre dos cantidades

1386
00:55:06,799 --> 00:55:09,680
, asumimos que nosotros, como modeladores, conocemos

1387
00:55:09,680 --> 00:55:12,799
la variación posterior aproximada q

1388
00:55:12,799 --> 00:55:15,119
de x dada o esa es la distribución

1389
00:55:15,119 --> 00:55:17,440
que controlamos y la distribución conjunta generativa

1390
00:55:17,440 --> 00:55:20,480
p de o coma x,

1391
00:55:20,480 --> 00:55:22,720
así que intercambiamos esto  posterior verdadero intratable

1392
00:55:22,720 --> 00:55:25,040
donde es como si lo supiéramos,

1393
00:55:25,040 --> 00:55:26,960
simplemente nos detendríamos allí, pero

1394
00:55:26,960 --> 00:55:29,280
no lo sabemos, así que lo cambiamos por

1395
00:55:29,280 --> 00:55:31,440
una divergencia entre algo que

1396
00:55:31,440 --> 00:55:33,280
controlamos totalmente

1397
00:55:33,280 --> 00:55:35,839
q de x condicionado a estos otros

1398
00:55:35,839 --> 00:55:37,119
parámetros

1399
00:55:37,119 --> 00:55:38,160
y

1400
00:55:38,160 --> 00:55:40,079
una distribución conjunta sobre la que podríamos tener

1401
00:55:40,079 --> 00:55:42,160
incertidumbre pero al menos se puede

1402
00:55:42,160 --> 00:55:43,119
modelar,

1403
00:55:43,119 --> 00:55:44,720
por lo que esa es una de las piezas clave de las

1404
00:55:44,720 --> 00:55:46,960
bahías variacionales y eso no es específico

1405
00:55:46,960 --> 00:55:48,319
de la codificación predictiva, es solo una

1406
00:55:48,319 --> 00:55:49,920
forma importante de que  los autores están

1407
00:55:49,920 --> 00:55:51,839
introduciendo aquí

1408
00:55:51,839 --> 00:55:54,640
cualquier cosa para

1409
00:55:56,799 --> 00:55:58,559
agregar bien, así que continuemos con

1410
00:55:58,559 --> 00:55:59,599
este tema

1411
00:55:59,599 --> 00:56:02,160
ya que f es un límite superior al minimizar

1412
00:56:02,160 --> 00:56:04,480
f derivamos la distribución variacional

1413
00:56:04,480 --> 00:56:06,640
que controlamos más cerca de la posterior verdadera

1414
00:56:06,640 --> 00:56:08,160


1415
00:56:08,160 --> 00:56:10,880
como una bonificación adicional

1416
00:56:10,880 --> 00:56:13,280
bajo ciertas condiciones

1417
00:56:13,280 --> 00:56:15,920
f puede  se puede usar para la selección de modelos, lo

1418
00:56:15,920 --> 00:56:17,520
que significa que se puede usar no solo

1419
00:56:17,520 --> 00:56:19,599
para ajustar un modelo que ya hemos

1420
00:56:19,599 --> 00:56:22,160
elegido, sino que también se puede

1421
00:56:22,160 --> 00:56:24,559
elegir entre modelos paramétricos o

1422
00:56:24,559 --> 00:56:28,000
estructuralmente diferentes

1423
00:56:28,000 --> 00:56:29,680
que escriben, podemos obtener una

1424
00:56:29,680 --> 00:56:31,520
intuición importante sobre f  mostrando que se puede

1425
00:56:31,520 --> 00:56:33,040
descomponer en un

1426
00:56:33,040 --> 00:56:35,520
término de maximización de verosimilitud

1427
00:56:35,520 --> 00:56:37,280
y el término de divergencia kl que

1428
00:56:37,280 --> 00:56:39,520
penaliza la desviación de la

1429
00:56:39,520 --> 00:56:41,839
p bayesiana  Antes, estos términos a menudo se denominan

1430
00:56:41,839 --> 00:56:44,720
precisión y complejidad, y esta

1431
00:56:44,720 --> 00:56:47,359
descomposición se usa a menudo en diferentes

1432
00:56:47,359 --> 00:56:49,440
algoritmos de aprendizaje automático, por lo que es

1433
00:56:49,440 --> 00:56:51,119
una

1434
00:56:51,119 --> 00:56:52,640
reescritura

1435
00:56:52,640 --> 00:56:54,960
de energía libre como esta divergencia

1436
00:56:54,960 --> 00:56:57,200
entre q y p

1437
00:56:57,200 --> 00:56:59,920
y luego reescribir eso como una precisión

1438
00:56:59,920 --> 00:57:01,680
y una complejidad que vamos a  entrar

1439
00:57:01,680 --> 00:57:05,119
tal vez más en otro momento

1440
00:57:05,119 --> 00:57:06,079


1441
00:57:06,079 --> 00:57:07,920
y escriben en muchos casos prácticos

1442
00:57:07,920 --> 00:57:09,440
debemos relajar la suposición de que conocemos

1443
00:57:09,440 --> 00:57:11,040
el modelo generativo

1444
00:57:11,040 --> 00:57:14,720
p de o y x la distribución conjunta

1445
00:57:14,720 --> 00:57:17,119
afortunadamente esto no es fatal, en

1446
00:57:17,119 --> 00:57:18,799
cambio, es posible aprender el

1447
00:57:18,799 --> 00:57:20,559
modelo generativo junto con el

1448
00:57:20,559 --> 00:57:23,599
posterior variacional  sobre la marcha

1449
00:57:23,599 --> 00:57:25,520
y en paralelo utilizando la

1450
00:57:25,520 --> 00:57:27,920
maximización de la expectativa,

1451
00:57:27,920 --> 00:57:28,880
por lo que

1452
00:57:28,880 --> 00:57:32,559
esta es básicamente la alternancia que se

1453
00:57:32,559 --> 00:57:35,040
muestra en las ecuaciones tres

1454
00:57:35,040 --> 00:57:37,200
de cómo podemos

1455
00:57:37,200 --> 00:57:40,880
establecer un lado de esta divergencia kl

1456
00:57:40,880 --> 00:57:44,000
fijo y optimizar el otro lado y

1457
00:57:44,000 --> 00:57:46,079
luego regresar y hacerlo de la otra

1458
00:57:46,079 --> 00:57:47,920
manera y para que  tipo de maximización de expectativas de ida y vuelta

1459
00:57:47,920 --> 00:57:50,079
vamos a

1460
00:57:50,079 --> 00:57:52,000
ser como reducir esta divergencia de

1461
00:57:52,000 --> 00:57:53,280
ambos lados

1462
00:57:53,280 --> 00:57:55,359
y tan formal  ism tres están diciendo que los

1463
00:57:55,359 --> 00:57:57,119
parámetros variacionales

1464
00:57:57,119 --> 00:57:58,079
phi

1465
00:57:58,079 --> 00:58:01,119
en el siguiente paso de tiempo t punto t más uno

1466
00:58:01,119 --> 00:58:03,440
son un

1467
00:58:03,440 --> 00:58:04,640
argumento

1468
00:58:04,640 --> 00:58:07,200
de optimización de esos parámetros

1469
00:58:07,200 --> 00:58:09,200
um

1470
00:58:09,200 --> 00:58:11,280
manteniendo theta constante

1471
00:58:11,280 --> 00:58:12,319
y luego

1472
00:58:12,319 --> 00:58:14,559
la segunda parte del informalismo tres

1473
00:58:14,559 --> 00:58:16,160
es exactamente lo opuesto

1474
00:58:16,160 --> 00:58:18,559
ahora los parámetros del modelo generativo

1475
00:58:18,559 --> 00:58:22,079
theta en t  más uno es la minimización

1476
00:58:22,079 --> 00:58:23,040
sobre

1477
00:58:23,040 --> 00:58:24,720
literalmente lo mismo, pero manteniendo

1478
00:58:24,720 --> 00:58:27,280
constantes los parámetros variacionales, por lo que

1479
00:58:27,280 --> 00:58:28,480
hay mucho más que decir sobre la

1480
00:58:28,480 --> 00:58:31,040
maximización de las expectativas, pero esto es

1481
00:58:31,040 --> 00:58:32,160
como

1482
00:58:32,160 --> 00:58:34,839
converger en una

1483
00:58:34,839 --> 00:58:37,440
pequeña

1484
00:58:37,440 --> 00:58:39,599
divergencia al reducir ambos lados

1485
00:58:39,599 --> 00:58:42,640
y alternar allí, así es como se

1486
00:58:42,640 --> 00:58:45,200
maximiza la expectativa.  se puede usar como

1487
00:58:45,200 --> 00:58:48,000
un algoritmo heurístico para

1488
00:58:48,000 --> 00:58:51,480
la optimización bayesiana variacional

1489
00:58:53,680 --> 00:58:55,760
¿cómo pasamos de la inferencia variacional en la

1490
00:58:55,760 --> 00:58:57,680
que todas esas diapositivas anteriores son

1491
00:58:57,680 --> 00:58:59,680
cosas de las que básicamente hemos hablado

1492
00:58:59,680 --> 00:59:02,000
antes y que no se aplican

1493
00:59:02,000 --> 00:59:04,160
específicamente a la codificación predictiva entonces

1494
00:59:04,160 --> 00:59:07,359
cómo vamos a obtener  a la codificación predictiva

1495
00:59:07,359 --> 00:59:09,280
escriben habiendo revisado los

1496
00:59:09,280 --> 00:59:11,839
principios generales de v  inferencia variacional

1497
00:59:11,839 --> 00:59:14,079
podemos ver cómo se relacionan con la codificación predictiva

1498
00:59:14,079 --> 00:59:15,119


1499
00:59:15,119 --> 00:59:17,520
primero para concretar cualquier

1500
00:59:17,520 --> 00:59:20,000
algoritmo de inferencia variacional debemos especificar las

1501
00:59:20,000 --> 00:59:22,000
formas del posterior variacional y

1502
00:59:22,000 --> 00:59:23,520
el modelo generativo es como si

1503
00:59:23,520 --> 00:59:25,520
quisiera hacer formalismo tres

1504
00:59:25,520 --> 00:59:27,280
esas son las dos piezas que necesita

1505
00:59:27,280 --> 00:59:29,200
necesita  como el material phi y el material theta, por

1506
00:59:29,200 --> 00:59:30,480


1507
00:59:30,480 --> 00:59:32,559
lo que lo especifican aquí

1508
00:59:32,559 --> 00:59:35,119
n significa una distribución normal

1509
00:59:35,119 --> 00:59:39,359
con un formato de varianza de coma media,

1510
00:59:39,359 --> 00:59:41,200
por lo que van a definir

1511
00:59:41,200 --> 00:59:45,520
una forma gaussiana para el modelo generativo. Se

1512
00:59:45,520 --> 00:59:47,520
supone que la media de esta probabilidad gaussiana es

1513
00:59:47,520 --> 00:59:49,440
ser alguna función de

1514
00:59:49,440 --> 00:59:51,920
estados ocultos que se puede parametrizar con

1515
00:59:51,920 --> 00:59:53,119
theta

1516
00:59:53,119 --> 00:59:54,960
mientras que la media de la distribución gaussiana anterior

1517
00:59:54,960 --> 00:59:58,240
va a ser una g de mu, por

1518
00:59:58,240 --> 01:00:01,040
lo que vamos a tener como f de theta

1519
01:00:01,040 --> 01:00:03,760
y g de mu

1520
01:00:03,760 --> 01:00:05,839
las varianzas de estas dos

1521
01:00:05,839 --> 01:00:07,760
distribuciones gaussianas de  el modelo generativo

1522
01:00:07,760 --> 01:00:11,760
es sigma 1 y sigma 2.

1523
01:00:12,480 --> 01:00:14,400
Este es un pequeño detalle técnico,

1524
01:00:14,400 --> 01:00:15,520
pero van a suponer que el

1525
01:00:15,520 --> 01:00:18,960
posterior variacional es un delta de drock

1526
01:00:18,960 --> 01:00:21,200
que es  como una distribución de función de pico

1527
01:00:21,200 --> 01:00:24,720
que está centrada en la media,

1528
01:00:24,720 --> 01:00:27,760
sin embargo, exploran eso de manera diferente

1529
01:00:27,760 --> 01:00:29,839
con la suposición de Laplace,

1530
01:00:29,839 --> 01:00:31,920
por lo que básicamente toma lo que

1531
01:00:31,920 --> 01:00:34,319
discutimos sobre la inferencia variacional

1532
01:00:34,319 --> 01:00:36,960
y lo prepara para ingresarlo en una

1533
01:00:36,960 --> 01:00:39,760
forma de codificación predictiva

1534
01:00:39,760 --> 01:00:41,839
para que establezcan el problema con el

1535
01:00:41,839 --> 01:00:43,760
cual  parámetros en los que querrá

1536
01:00:43,760 --> 01:00:46,480
hacer inferencia variacional para

1537
01:00:46,480 --> 01:00:48,880
implementar la codificación predictiva

1538
01:00:48,880 --> 01:00:50,240
y, por supuesto, hay más que decir, pero

1539
01:00:50,240 --> 01:00:52,880
solo estamos dando un primer paso

1540
01:00:52,880 --> 01:00:54,640
en el apéndice

1541
01:00:54,640 --> 01:00:57,119
a donde describen la diferencia

1542
01:00:57,119 --> 01:00:58,240
entre

1543
01:00:58,240 --> 01:01:01,040
usar la aproximación laplaciana y

1544
01:01:01,040 --> 01:01:03,520
el delta directo, por lo que es la nota al pie 8

1545
01:01:03,520 --> 01:01:07,200
que sugiere pasar al apéndice a

1546
01:01:07,200 --> 01:01:10,160
y mirar el Buckley 2017 para un

1547
01:01:10,160 --> 01:01:11,280
recorrido,

1548
01:01:11,280 --> 01:01:13,839
pero esto en la parte inferior derecha es

1549
01:01:13,839 --> 01:01:15,680
como un resumen de la diferencia entre

1550
01:01:15,680 --> 01:01:18,000
el enfoque delta de Dirac y el enfoque del

1551
01:01:18,000 --> 01:01:20,480
método de Laplace, por lo que el  delta de diroc es

1552
01:01:20,480 --> 01:01:23,839
como si estuviéramos tratando de encontrar el pico

1553
01:01:23,839 --> 01:01:26,319
que está en la media

1554
01:01:26,319 --> 01:01:29,040
o la mediana, hay algunos otros detalles

1555
01:01:29,040 --> 01:01:30,079
en juego

1556
01:01:30,079 --> 01:01:31,200
que

1557
01:01:31,200 --> 01:01:33,200
es donde está la mayor parte de la

1558
01:01:33,200 --> 01:01:35,119
masa de distribución de probabilidad,

1559
01:01:35,119 --> 01:01:37,359
mientras que el método de Laplace trata de ajustar

1560
01:01:37,359 --> 01:01:38,559
un polinomio

1561
01:01:38,559 --> 01:01:40,319
como un polinomio de segundo grado

1562
01:01:40,319 --> 01:01:41,760
como un cuadrático

1563
01:01:41,760 --> 01:01:43,119
sobre

1564
01:01:43,119 --> 01:01:45,599
la distribución de probabilidad

1565
01:01:45,599 --> 01:01:47,680
también de la manera

1566
01:01:47,680 --> 01:01:49,839
que mejor se ajusta, por lo que hay algunas

1567
01:01:49,839 --> 01:01:51,599
similitudes y algunas diferencias y

1568
01:01:51,599 --> 01:01:54,960
se explora  más en el artículo de Buckley 2017

1569
01:01:54,960 --> 01:01:55,920


1570
01:01:55,920 --> 01:01:58,319
sobre la fep para la revisión matemática de la percepción de la acción,

1571
01:01:58,319 --> 01:02:00,799


1572
01:02:00,799 --> 01:02:02,720
así que continuemos con nuestra

1573
01:02:02,720 --> 01:02:04,839
exploración de cómo hacemos

1574
01:02:04,839 --> 01:02:07,280
bahías variacionales y lo convertimos en un modelo de codificación predictivo

1575
01:02:07,280 --> 01:02:09,039


1576
01:02:09,039 --> 01:02:10,880
en la parte superior escriben que definimos los

1577
01:02:10,880 --> 01:02:13,440
errores de predicción épsilon o

1578
01:02:13,440 --> 01:02:16,000
eso es como el error en  observación

1579
01:02:16,000 --> 01:02:17,760
y epsilon x

1580
01:02:17,760 --> 01:02:20,240
ese es el error en los estados medios, por

1581
01:02:20,240 --> 01:02:21,920
lo que epsilon o

1582
01:02:21,920 --> 01:02:23,200
es como

1583
01:02:23,200 --> 01:02:25,280
o menos f

1584
01:02:25,280 --> 01:02:27,520
de alguna función, conoce f de algunos

1585
01:02:27,520 --> 01:02:29,359
parámetros y luego también está

1586
01:02:29,359 --> 01:02:31,680
epsilon x, entonces, ¿

1587
01:02:31,680 --> 01:02:33,200
cuánto error de predicción tiene sobre

1588
01:02:33,200 --> 01:02:34,720
la observación? ¿cuánta predicción?

1589
01:02:34,720 --> 01:02:39,359
error que tiene sobre el estado oculto

1590
01:02:39,359 --> 01:02:42,000
epsilons son los errores de predicción

1591
01:02:42,000 --> 01:02:44,079
y están parametrizados por estos theta  1

1592
01:02:44,079 --> 01:02:47,359
y 2 varianzas

1593
01:02:47,359 --> 01:02:49,680
dado todo esto, podemos derivar

1594
01:02:49,680 --> 01:02:51,599
dinámicas para todas las variables de

1595
01:02:51,599 --> 01:02:54,400
interés, por lo que ese es el estado oculto subyacente real,

1596
01:02:54,400 --> 01:02:56,160
la temperatura en la habitación

1597
01:02:56,160 --> 01:02:57,839
que queremos

1598
01:02:57,839 --> 01:03:00,480
predecir en realidad mu

1599
01:03:00,480 --> 01:03:02,319
y luego el parámetro variacional

1600
01:03:02,319 --> 01:03:03,200
de la misma

1601
01:03:03,200 --> 01:03:04,160
y luego

1602
01:03:04,160 --> 01:03:06,400
theta uno  y theta dos, que son las

1603
01:03:06,400 --> 01:03:08,079
variaciones, puede considerarlo como

1604
01:03:08,079 --> 01:03:10,240
la variación de la temperatura ambiente y

1605
01:03:10,240 --> 01:03:11,680
la variación del término del

1606
01:03:11,680 --> 01:03:13,920
termómetro. Espero que no esté

1607
01:03:13,920 --> 01:03:16,240
mal o sea una simplificación excesiva, pero esas

1608
01:03:16,240 --> 01:03:18,160
son como las dos variaciones

1609
01:03:18,160 --> 01:03:20,000
y queremos la temperatura real.  de la

1610
01:03:20,000 --> 01:03:22,240
sala en la

1611
01:03:22,400 --> 01:03:24,079
que escriben, podemos derivar dinámicas para

1612
01:03:24,079 --> 01:03:25,839
todas estas variables de interés

1613
01:03:25,839 --> 01:03:27,280
tomando las derivadas de la

1614
01:03:27,280 --> 01:03:29,760
energía libre variacional. Las

1615
01:03:29,760 --> 01:03:32,240
reglas de actualización son las siguientes

1616
01:03:32,240 --> 01:03:35,200
y, por lo tanto, esto es un cambio en mu theta 1 y

1617
01:03:35,200 --> 01:03:39,599
theta 2 a lo largo del tiempo d mu dt d theta  1 sobre

1618
01:03:39,599 --> 01:03:43,039
dt y d theta 2 sobre dt

1619
01:03:43,039 --> 01:03:46,160
y eso tiene algunas equivalencias con algunas

1620
01:03:46,160 --> 01:03:49,039
f, por lo que pasará

1621
01:03:49,039 --> 01:03:50,559
del cambio en la estimación de mu a través del

1622
01:03:50,559 --> 01:03:53,039
tiempo a algo que involucre  energía libre

1623
01:03:53,039 --> 01:03:56,400
y luego eso se define de manera más formal, lo que es más

1624
01:03:56,400 --> 01:03:58,319
importante, estas reglas de actualización son muy

1625
01:03:58,319 --> 01:04:00,559
similares a las derivadas de rao y

1626
01:04:00,559 --> 01:04:03,359
ballard 1999 y, por lo tanto, pueden

1627
01:04:03,359 --> 01:04:05,119
interpretarse como una recapitulación de las

1628
01:04:05,119 --> 01:04:07,680
reglas de actualización de codificación predictiva central,

1629
01:04:07,680 --> 01:04:09,520
por ejemplo, la musa generalmente se

1630
01:04:09,520 --> 01:04:11,599
interpreta como tasas de activación neuronal que cambian rápidamente

1631
01:04:11,599 --> 01:04:14,240
mientras  thetas están

1632
01:04:14,240 --> 01:04:16,880
cambiando lentamente los valores de peso sináptico, por

1633
01:04:16,880 --> 01:04:20,000
lo que se están entrelazando conectando

1634
01:04:20,000 --> 01:04:23,680
la biología con los formalismos aquí

1635
01:04:23,680 --> 01:04:26,400
y también escriben que ese mu puede

1636
01:04:26,400 --> 01:04:29,680
entenderse como el proceso de percepción

1637
01:04:29,680 --> 01:04:33,280
que es como qué tan caliente está en la habitación

1638
01:04:33,280 --> 01:04:35,119
ya que mu está destinado a corresponder  a la

1639
01:04:35,119 --> 01:04:36,559
estimación del estado latente del

1640
01:04:36,559 --> 01:04:38,880
entorno que genera las observaciones

1641
01:04:38,880 --> 01:04:41,200
en el termómetro y, por el contrario,

1642
01:04:41,200 --> 01:04:43,599
se puede pensar que la dinámica de thetas

1643
01:04:43,599 --> 01:04:45,520
corresponde al aprendizaje,

1644
01:04:45,520 --> 01:04:47,200
ya que theta definió efectivamente el

1645
01:04:47,200 --> 01:04:49,039
mapeo entre el estado latente y las

1646
01:04:49,039 --> 01:04:50,480
observaciones,

1647
01:04:50,480 --> 01:04:53,119
por lo que es como si pensaras que  saber cómo

1648
01:04:53,119 --> 01:04:54,319
se relaciona el termómetro con la

1649
01:04:54,319 --> 01:04:56,000
temperatura en la habitación

1650
01:04:56,000 --> 01:04:57,280
a  y luego ve que su termómetro

1651
01:04:57,280 --> 01:04:58,799
cambia, está cambiando qué tan caliente

1652
01:04:58,799 --> 01:05:00,720
cree que está la habitación, eso sucede en una

1653
01:05:00,720 --> 01:05:02,960
escala de tiempo más corta y luego en una

1654
01:05:02,960 --> 01:05:04,880
escala de tiempo más larga, puede llegar a

1655
01:05:04,880 --> 01:05:07,599
aprender la variabilidad de una temperatura

1656
01:05:07,599 --> 01:05:10,559
o qué tan ruidoso es el termómetro, pero

1657
01:05:10,559 --> 01:05:12,480
eso es  más como aprendizaje

1658
01:05:12,480 --> 01:05:14,480
que percepción, sin embargo, están en un

1659
01:05:14,480 --> 01:05:17,039
continuo usando este modelo,

1660
01:05:17,039 --> 01:05:20,240
por lo que en realidad esto hasta el formalismo 9

1661
01:05:20,240 --> 01:05:22,720
completa el formalismo central de la

1662
01:05:22,720 --> 01:05:24,559
codificación predictiva, que es un

1663
01:05:24,559 --> 01:05:27,359
enfoque bayesiano variacional para tener

1664
01:05:27,359 --> 01:05:30,640
este enfoque continuo de minimización de errores de predicción

1665
01:05:30,640 --> 01:05:34,000
para la estimación del estado latente que

1666
01:05:34,000 --> 01:05:36,960
es  el corazón de la codificación predictiva

1667
01:05:36,960 --> 01:05:38,720
y ahora vamos a saltar a

1668
01:05:38,720 --> 01:05:40,240
algunas elaboraciones diferentes que

1669
01:05:40,240 --> 01:05:41,760
vamos a avanzar un poco

1670
01:05:41,760 --> 01:05:43,280
más rápido

1671
01:05:43,280 --> 01:05:46,640
cualquier cosa para agregar maria

1672
01:05:47,520 --> 01:05:49,440
impresionante

1673
01:05:49,440 --> 01:05:52,160
los ejemplos anteriores se

1674
01:05:52,160 --> 01:05:53,680
centraron en la codificación predictiva con un

1675
01:05:53,680 --> 01:05:58,000
solo nivel de  variables latentes mu 1.

1676
01:05:58,000 --> 01:06:00,160
Sin embargo, la expresividad de dicho

1677
01:06:00,160 --> 01:06:02,240
esquema es limitada Las

1678
01:06:02,240 --> 01:06:03,920
redes neuronales profundas en el aprendizaje automático

1679
01:06:03,920 --> 01:06:05,280
han demostrado  d que tener

1680
01:06:05,280 --> 01:06:07,599
conjuntos jerárquicos de variables latentes

1681
01:06:07,599 --> 01:06:09,760
es clave para habilitar métodos que aprendan

1682
01:06:09,760 --> 01:06:11,599
abstracciones poderosas

1683
01:06:11,599 --> 01:06:13,599
y manejen dinámicas intrínsecamente jerárquicas

1684
01:06:13,599 --> 01:06:15,280
del tipo que los humanos

1685
01:06:15,280 --> 01:06:17,680
perciben intuitivamente. Los

1686
01:06:17,680 --> 01:06:20,160
esquemas de codificación predictiva introducidos pueden

1687
01:06:20,160 --> 01:06:22,000
extenderse directamente para manejar

1688
01:06:22,000 --> 01:06:24,640
dinámicas jerárquicas de profundidad arbitraria

1689
01:06:24,640 --> 01:06:26,480
equivalente a redes neuronales profundas  en

1690
01:06:26,480 --> 01:06:28,240
el aprendizaje automático,

1691
01:06:28,240 --> 01:06:29,839
esto se hace postulando

1692
01:06:29,839 --> 01:06:33,039
múltiples capas de variables latentes x

1693
01:06:33,039 --> 01:06:35,839
sub 1 a x sub l y luego definiendo

1694
01:06:35,839 --> 01:06:38,240
el modelo generativo de la siguiente

1695
01:06:38,240 --> 01:06:40,799
manera, como p, el modelo generativo

1696
01:06:40,799 --> 01:06:42,960
estará básicamente sobre todas las

1697
01:06:42,960 --> 01:06:45,039
capas,

1698
01:06:45,039 --> 01:06:47,599
así como el modelo generativo

1699
01:06:47,599 --> 01:06:50,400
y  la distribución variacional tuvo que

1700
01:06:50,400 --> 01:06:53,119
ser definida para el modelo de una sola capa

1701
01:06:53,119 --> 01:06:55,440
aquí van a definir la

1702
01:06:55,440 --> 01:06:57,839
distribución p y ahora necesitan la q

1703
01:06:57,839 --> 01:06:59,440
definimos un posterior variacional separado

1704
01:06:59,440 --> 01:07:01,839
para cada capa para que definan

1705
01:07:01,839 --> 01:07:03,520
las p y las señales

1706
01:07:03,520 --> 01:07:05,839
tal como lo hicieron en  la capa única

1707
01:07:05,839 --> 01:07:08,400
y luego eso les permite calcular

1708
01:07:08,400 --> 01:07:10,720
la variación  energía libre, que es una

1709
01:07:10,720 --> 01:07:12,880
suma de los errores de predicción de cada

1710
01:07:12,880 --> 01:07:14,400
nivel

1711
01:07:14,400 --> 01:07:15,599
, por

1712
01:07:15,599 --> 01:07:18,480
lo que ahora no se trata solo de una capa,

1713
01:07:18,480 --> 01:07:20,480
sino de múltiples niveles,

1714
01:07:20,480 --> 01:07:22,960
por lo que las variaciones posteriores

1715
01:07:22,960 --> 01:07:25,359
que deben calcularse se dividen entre

1716
01:07:25,359 --> 01:07:26,640
las capas, lo

1717
01:07:26,640 --> 01:07:28,960
que permite sumarlas

1718
01:07:28,960 --> 01:07:31,440
de una manera muy  forma sencilla

1719
01:07:31,440 --> 01:07:33,200
dado que la energía libre se divide muy bien

1720
01:07:33,200 --> 01:07:35,039
en la suma de los errores de predicción por capas

1721
01:07:35,039 --> 01:07:36,240


1722
01:07:36,240 --> 01:07:38,559
, no sorprende que la dinámica

1723
01:07:38,559 --> 01:07:40,799
de mu y theta sean igualmente

1724
01:07:40,799 --> 01:07:43,680
separables entre capas, lo

1725
01:07:43,680 --> 01:07:46,000
que permite que las diferentes capas de esta

1726
01:07:46,000 --> 01:07:49,039
jerarquía de predicción sean precisas

1727
01:07:49,039 --> 01:07:51,280
o imprecisas

1728
01:07:51,280 --> 01:07:53,680
y permitan  esos movimientos sucedan de una

1729
01:07:53,680 --> 01:07:54,839
manera que

1730
01:07:54,839 --> 01:07:57,119
no está correlacionada, lo cual no es solo porque

1731
01:07:57,119 --> 01:07:58,799
el mundo real se presenta con

1732
01:07:58,799 --> 01:08:00,720
configuraciones donde hay confianza

1733
01:08:00,720 --> 01:08:03,280
en lo más bajo y lo más alto y viceversa, sino

1734
01:08:03,280 --> 01:08:05,680
que hace que este cálculo de la

1735
01:08:05,680 --> 01:08:07,599
energía libre de todo el sistema sea

1736
01:08:07,599 --> 01:08:09,520
más como una simple suma  mientras que si

1737
01:08:09,520 --> 01:08:10,640
hubiera estas interacciones realmente complejas

1738
01:08:10,640 --> 01:08:12,799
con la primera capa por la

1739
01:08:12,799 --> 01:08:14,640
tercera capa si la quinta capa i  De esta

1740
01:08:14,640 --> 01:08:16,238
manera,

1741
01:08:16,238 --> 01:08:18,158
hacer las estadísticas sería mucho más

1742
01:08:18,158 --> 01:08:19,759
desafiante,

1743
01:08:19,759 --> 01:08:22,719
así que visualmente esto es lo que parece:

1744
01:08:22,719 --> 01:08:25,198
tenemos el mu, que es la estimación media

1745
01:08:25,198 --> 01:08:26,319


1746
01:08:26,319 --> 01:08:28,799
de lo que está sucediendo en ese nivel

1747
01:08:28,799 --> 01:08:29,759
y

1748
01:08:29,759 --> 01:08:33,359
luego pasan estos

1749
01:08:33,359 --> 01:08:34,640
términos de error épsilon,

1750
01:08:34,640 --> 01:08:36,399
por lo que escriben este es el  arquitectura

1751
01:08:36,399 --> 01:08:38,719
de una red de codificación predictiva multicapa en la figura 1 que se

1752
01:08:38,719 --> 01:08:40,399


1753
01:08:40,399 --> 01:08:42,399
muestra aquí con dos neuronas de valor y error

1754
01:08:42,399 --> 01:08:44,560
en cada capa,

1755
01:08:44,560 --> 01:08:46,719
las neuronas de valor se proyectan a las

1756
01:08:46,719 --> 01:08:48,719
neuronas de error de la capa inferior

1757
01:08:48,719 --> 01:08:50,479
y las neuronas de error representan la

1758
01:08:50,479 --> 01:08:53,679
actividad actual, por lo que es como

1759
01:08:53,679 --> 01:08:56,080
comenzar a caminar  volvamos hacia ese

1760
01:08:56,080 --> 01:08:58,399
diseño cortical

1761
01:08:58,399 --> 01:09:00,560
donde hay una columna cortical

1762
01:09:00,560 --> 01:09:02,799
donde hay algunas de las llamadas señales hacia arriba y hacia

1763
01:09:02,799 --> 01:09:05,120
abajo, pero también hay

1764
01:09:05,120 --> 01:09:07,359
señales laterales,

1765
01:09:07,359 --> 01:09:09,600
por lo que este es un modelo gráfico

1766
01:09:09,600 --> 01:09:11,040
que refleja

1767
01:09:11,040 --> 01:09:12,960
la forma en que la codificación predictiva

1768
01:09:12,960 --> 01:09:15,359
se puede organizar casi como en serie

1769
01:09:15,359 --> 01:09:16,960
y en paralelo

1770
01:09:16,960 --> 01:09:19,600
para  tener modelos anchos

1771
01:09:19,600 --> 01:09:22,000
y modelos profundos como si pudieras tener

1772
01:09:22,000 --> 01:09:24,319
una red neuronal donde hubiera

1773
01:09:24,319 --> 01:09:26,399
cuatro neuronas cuatro neuronas cuatro  neuronas

1774
01:09:26,399 --> 01:09:28,719
cuatro neuronas o podrías tener 64 y

1775
01:09:28,719 --> 01:09:31,359
luego 64. eso sería como un

1776
01:09:31,359 --> 01:09:34,000
modelo más superficial pero más amplio y esa noción de

1777
01:09:34,000 --> 01:09:36,319
superficialidad y profundidad también

1778
01:09:36,319 --> 01:09:40,560
entrará en juego con la codificación predictiva

1779
01:09:42,799 --> 01:09:46,719
aquí estamos en el formalismo 12 y 13.

1780
01:09:46,719 --> 01:09:51,359
y  ahora estamos viendo la

1781
01:09:51,359 --> 01:09:54,880
tasa de cambio, la derivada de mu sub

1782
01:09:54,880 --> 01:09:55,760
l,

1783
01:09:55,760 --> 01:09:57,760
la estimación de la media en esa

1784
01:09:57,760 --> 01:10:01,040
capa y theta l, la varianza en esa

1785
01:10:01,040 --> 01:10:03,600
capa, estas son las reglas de actualización, los

1786
01:10:03,600 --> 01:10:04,719
gradientes

1787
01:10:04,719 --> 01:10:06,000
para la media y la varianza de

1788
01:10:06,000 --> 01:10:07,440
diferentes  niveles y también están

1789
01:10:07,440 --> 01:10:09,920
escritos como funcionales de energía libre de

1790
01:10:09,920 --> 01:10:12,159
esas capas

1791
01:10:12,159 --> 01:10:13,280
y

1792
01:10:13,280 --> 01:10:15,199
la dinámica de los medios variacionales

1793
01:10:15,199 --> 01:10:17,520
depende solo de los errores de predicción en

1794
01:10:17,520 --> 01:10:19,600
su capa y los errores de predicción en

1795
01:10:19,600 --> 01:10:21,600
la capa inferior, por lo que nuevamente no tenemos

1796
01:10:21,600 --> 01:10:24,239
este épsilon en l menos  1 conectando

1797
01:10:24,239 --> 01:10:26,400
todo el camino a mu es solo a través de

1798
01:10:26,400 --> 01:10:28,080
estas conexiones locales dentro y

1799
01:10:28,080 --> 01:10:30,400
entre capas mediante las cuales necesitamos

1800
01:10:30,400 --> 01:10:33,040
calcular cualquier cosa

1801
01:10:33,040 --> 01:10:34,880
que podamos pensar que musa está tratando de

1802
01:10:34,880 --> 01:10:37,440
comprometerse entre causar un error

1803
01:10:37,440 --> 01:10:39,280
al desviarse  de la predicción de

1804
01:10:39,280 --> 01:10:40,640
la capa superior

1805
01:10:40,640 --> 01:10:42,640
y ajustando su propia predicción para

1806
01:10:42,640 --> 01:10:45,360
resolver el error en la capa inferior, por lo que

1807
01:10:45,360 --> 01:10:48,000
es como una jerarquía de jefes o

1808
01:10:48,000 --> 01:10:50,000
tareas y hay expectativas de arriba hacia abajo

1809
01:10:50,000 --> 01:10:51,760
y está la realidad de abajo hacia arriba

1810
01:10:51,760 --> 01:10:53,360
que podría ser como adelante  o

1811
01:10:53,360 --> 01:10:55,679
atrasado y luego pone a cada

1812
01:10:55,679 --> 01:10:58,080
persona en esta situación de compromiso de

1813
01:10:58,080 --> 01:11:00,239


1814
01:11:00,239 --> 01:11:02,159
manera crucial para las lecturas conceptuales de la

1815
01:11:02,159 --> 01:11:03,840
codificación predictiva

1816
01:11:03,840 --> 01:11:06,080
y aquí es donde hay una puerta

1817
01:11:06,080 --> 01:11:07,840
de entrada a la filosofía y algunas de las

1818
01:11:07,840 --> 01:11:09,440
discusiones más amplias,

1819
01:11:09,440 --> 01:11:11,520
esto significa que los datos sensoriales no

1820
01:11:11,520 --> 01:11:13,520
se transmiten directamente a través de la

1821
01:11:13,520 --> 01:11:16,239
jerarquía como se asume en gran parte de la

1822
01:11:16,239 --> 01:11:18,159
neurociencia perceptual

1823
01:11:18,159 --> 01:11:20,560
y por lo que nos devuelve totalmente a estas

1824
01:11:20,560 --> 01:11:23,120
preguntas como qué es percepción qué

1825
01:11:23,120 --> 01:11:25,360
es cognición qué es acción

1826
01:11:25,360 --> 01:11:27,040
qué

1827
01:11:27,040 --> 01:11:29,520
viene del ojo al cerebro o qué

1828
01:11:29,520 --> 01:11:31,199
va del cerebro al ojo qué es

1829
01:11:31,199 --> 01:11:33,440
el  ojo haciendo lo que hace el cerebro

1830
01:11:33,440 --> 01:11:35,440
con respecto al ojo

1831
01:11:35,440 --> 01:11:36,880
y cómo se relaciona con la codificación predictiva

1832
01:11:36,880 --> 01:11:38,800


1833
01:11:38,800 --> 01:11:41,840
cualquier respuesta rápida sobre esas respuestas

1834
01:11:43,360 --> 01:11:44,800
naturales

1835
01:11:44,800 --> 01:11:46,480
Genial, así que

1836
01:11:46,480 --> 01:11:48,320
después de introducir ese núcleo del

1837
01:11:48,320 --> 01:11:50,400
formalismo de codificación predictiva y

1838
01:11:50,400 --> 01:11:53,440
llevarlo al contexto de varios niveles, ahora

1839
01:11:53,440 --> 01:11:56,320
podemos ver otra elaboración o

1840
01:11:56,320 --> 01:11:58,239
generalización,

1841
01:11:58,239 --> 01:12:00,239
por lo que escriben en 2.3

1842
01:12:00,239 --> 01:12:01,920
, hemos considerado el modelado de

1843
01:12:01,920 --> 01:12:05,040
un solo estímulo estático,

1844
01:12:05,040 --> 01:12:06,960
sin embargo, lo más interesante.  los datos que

1845
01:12:06,960 --> 01:12:08,480
recibe el cerebro vienen en

1846
01:12:08,480 --> 01:12:12,000
secuencias temporales o barra que es o a través del tiempo

1847
01:12:12,000 --> 01:12:14,000
para modelar tales secuencias temporales a

1848
01:12:14,000 --> 01:12:15,600
menudo es útil dividir las

1849
01:12:15,600 --> 01:12:18,080
variables latentes en estados que pueden variar

1850
01:12:18,080 --> 01:12:21,600
con el tiempo y parámetros que no pueden

1851
01:12:21,600 --> 01:12:23,679
en el caso de secuencias en

1852
01:12:23,679 --> 01:12:25,520
lugar de minimizar la variación

1853
01:12:25,520 --> 01:12:27,920
energía libre, en cambio, debemos minimizar la

1854
01:12:27,920 --> 01:12:30,719
acción libre, que es la trayectoria integral

1855
01:12:30,719 --> 01:12:32,560
de la energía libre variacional a través del

1856
01:12:32,560 --> 01:12:33,840
tiempo

1857
01:12:33,840 --> 01:12:35,760
. No vamos a entrar en los

1858
01:12:35,760 --> 01:12:37,120
formalismos

1859
01:12:37,120 --> 01:12:38,800
relacionados con las coordenadas generalizadas

1860
01:12:38,800 --> 01:12:39,840
15

1861
01:12:39,840 --> 01:12:42,719
o 14, 15, 16 y 17,

1862
01:12:42,719 --> 01:12:44,800
pero es algo que podemos explorar.

1863
01:12:44,800 --> 01:12:45,840
más tarde

1864
01:12:45,840 --> 01:12:48,719
y solo como un recordatorio para

1865
01:12:48,719 --> 01:12:50,560


1866
01:12:50,560 --> 01:12:52,000
emocionar a alguien que quiera

1867
01:12:52,000 --> 01:12:53,120
explorarlo,

1868
01:12:53,120 --> 01:12:55,280
exploramos la idea de  f las

1869
01:12:55,280 --> 01:12:58,080
coordenadas generalizadas de movimiento mucho en la

1870
01:12:58,080 --> 01:13:02,159
transmisión en vivo activa número 26 con decosta en absoluto

1871
01:13:02,159 --> 01:13:04,640
, así que aquí había un modelo en el que tenemos

1872
01:13:04,640 --> 01:13:07,120
de izquierda a derecha en diferentes

1873
01:13:07,120 --> 01:13:08,480
pasos de tiempo

1874
01:13:08,480 --> 01:13:11,280
y luego están los observables de

1875
01:13:11,280 --> 01:13:12,560
algo

1876
01:13:12,560 --> 01:13:16,000
y luego es parte de esto como  columna

1877
01:13:16,000 --> 01:13:18,480
de derivadas superiores de su movimiento, por

1878
01:13:18,480 --> 01:13:21,120
lo que es como la posición, la

1879
01:13:21,120 --> 01:13:23,840
velocidad, la aceleración,

1880
01:13:23,840 --> 01:13:25,679
etc., en cada paso de tiempo

1881
01:13:25,679 --> 01:13:27,600
que avanza, está calculando las

1882
01:13:27,600 --> 01:13:29,840
coordenadas generalizadas de

1883
01:13:29,840 --> 01:13:32,480
movimiento como el movimiento de una pelota de béisbol y

1884
01:13:32,480 --> 01:13:34,480
eso es lo que vincula las

1885
01:13:34,480 --> 01:13:37,360
coordenadas generalizadas de  control de movimiento a pid

1886
01:13:37,360 --> 01:13:39,360
que también volverá al final,

1887
01:13:39,360 --> 01:13:42,080
también conocido como cadenas integradoras y todo esto

1888
01:13:42,080 --> 01:13:43,920
está sucediendo dentro de esta discusión

1889
01:13:43,920 --> 01:13:46,080
de la mecánica bayesiana como si

1890
01:13:46,080 --> 01:13:48,239
hubiéramos estado hablando de la pelota de béisbol,

1891
01:13:48,239 --> 01:13:50,080
estaríamos hablando de

1892
01:13:50,080 --> 01:13:53,679
mecánica física o mecánica estadística,

1893
01:13:53,679 --> 01:13:56,400
bueno, es  bayesiano, por lo que es la mecánica bayesiana

1894
01:13:56,400 --> 01:13:59,199
y la física de la acción y el control,

1895
01:13:59,199 --> 01:14:02,320
eso es lo que exploran en la sección 2-3,

1896
01:14:02,320 --> 01:14:05,120
sección 2.4, introducción  duce y entra en un

1897
01:14:05,120 --> 01:14:08,320
poco más de detalle sobre la precisión,

1898
01:14:08,320 --> 01:14:10,640
un aspecto central de la codificación predictiva

1899
01:14:10,640 --> 01:14:13,120
ausente en la formulación original de rao y ballard de

1900
01:14:13,120 --> 01:14:15,920
1999

1901
01:14:15,920 --> 01:14:18,239
es la noción de precisión

1902
01:14:18,239 --> 01:14:20,320
o varianza inversa,

1903
01:14:20,320 --> 01:14:23,440
por lo que la precisión y la varianza son como una

1904
01:14:23,440 --> 01:14:26,159
sobre la otra, por lo que si alguien dijera que es

1905
01:14:26,159 --> 01:14:28,960
muy  alta precisión que es una distribución muy estrecha

1906
01:14:28,960 --> 01:14:30,880
muy nítida

1907
01:14:30,880 --> 01:14:33,360
y una sobre la varianza es otra forma

1908
01:14:33,360 --> 01:14:35,440
de escribir eso, por lo que a veces se usa

1909
01:14:35,440 --> 01:14:37,840
con una beta y una gamma en otros

1910
01:14:37,840 --> 01:14:39,440
modelos la

1911
01:14:39,440 --> 01:14:41,440
precisión sirve para modular multiplicativamente

1912
01:14:41,440 --> 01:14:42,400


1913
01:14:42,400 --> 01:14:44,800
la importancia de los errores de predicción

1914
01:14:44,800 --> 01:14:46,960
y por lo tanto posee una significativa  influencia

1915
01:14:46,960 --> 01:14:50,000
en la dinámica general del modelo

1916
01:14:50,000 --> 01:14:51,120
, por

1917
01:14:51,120 --> 01:14:53,840
lo que no entraremos en todos los detalles sobre cómo

1918
01:14:53,840 --> 01:14:56,159
se ajustan las precisiones, pero

1919
01:14:56,159 --> 01:14:59,520
solo quería señalar que este gran sigma

1920
01:14:59,520 --> 01:15:02,400
es la energía libre que se suma sobre las

1921
01:15:02,400 --> 01:15:03,360
capas,

1922
01:15:03,360 --> 01:15:04,880
por lo que gran sigma

1923
01:15:04,880 --> 01:15:09,679
es un um es  una suma multiplicativa, lo

1924
01:15:09,679 --> 01:15:11,520
siento, es solo una suma,

1925
01:15:11,520 --> 01:15:14,880
pero este sigma más pequeño dentro

1926
01:15:14,880 --> 01:15:18,320
es sigma como la matriz de precisión en un

1927
01:15:18,320 --> 01:15:20,000
nivel dado,

1928
01:15:20,000 --> 01:15:22,000
por lo que es una especie de  Me gustan los diferentes usos de

1929
01:15:22,000 --> 01:15:22,960
sigma,

1930
01:15:22,960 --> 01:15:26,239
pero esta es una forma en la que las varianzas

1931
01:15:26,239 --> 01:15:28,320
y los errores de predicción en un nivel dado

1932
01:15:28,320 --> 01:15:29,280


1933
01:15:29,280 --> 01:15:32,080
se suman en todos los niveles y

1934
01:15:32,080 --> 01:15:36,640
eso ayuda a ajustar estos términos de precisión,

1935
01:15:36,960 --> 01:15:39,040
detalles estadísticos, pero es importante

1936
01:15:39,040 --> 01:15:42,880
para que funcione en la inferencia real.

1937
01:15:42,880 --> 01:15:44,080


1938
01:15:44,080 --> 01:15:47,360
se retiran de

1939
01:15:47,360 --> 01:15:49,840
elaborar sobre la codificación predictiva

1940
01:15:49,840 --> 01:15:52,159
y hablan más sobre cómo tiene algunas

1941
01:15:52,159 --> 01:15:55,840
bases biológicamente plausibles,

1942
01:15:55,840 --> 01:15:58,080
por lo que esto fue bastante interesante,

1943
01:15:58,080 --> 01:15:59,840
mientras que la codificación técnicamente predictiva es

1944
01:15:59,840 --> 01:16:01,600
simplemente una inferencia variacional y un

1945
01:16:01,600 --> 01:16:03,440
algoritmo de filtrado bajo

1946
01:16:03,440 --> 01:16:05,360
suposiciones gaussianas desde el principio, se

1947
01:16:05,360 --> 01:16:06,800
ha afirmado que es biológicamente

1948
01:16:06,800 --> 01:16:10,000
plausible.  teoría de la computación cortical

1949
01:16:10,000 --> 01:16:12,159
y también para otras regiones del cerebro como

1950
01:16:12,159 --> 01:16:13,199
la i

1951
01:16:13,199 --> 01:16:15,920
y para otros sistemas cognitivos,

1952
01:16:15,920 --> 01:16:17,520
la literatura ha establecido constantemente

1953
01:16:17,520 --> 01:16:19,360
conexiones estrechas entre la teoría y

1954
01:16:19,360 --> 01:16:20,960
las computaciones potenciales que pueden

1955
01:16:20,960 --> 01:16:23,440
realizarse en el cerebro, por ejemplo, Rao y

1956
01:16:23,440 --> 01:16:25,520
Ballard afirmaron explícitamente que modelan la

1957
01:16:25,520 --> 01:16:27,280
corteza visual temprana

1958
01:16:27,280 --> 01:16:29,760
y frister explícitamente  propuso la

1959
01:16:29,760 --> 01:16:32,400
codificación predictiva como una teoría general de

1960
01:16:32,400 --> 01:16:34,880
la computación cortical, esto es como la

1961
01:16:34,880 --> 01:16:37,679
discusión sobre el realismo y el instrumentalismo

1962
01:16:37,679 --> 01:16:39,600
, ¿simplemente modelamos

1963
01:16:39,600 --> 01:16:41,280
estos sistemas y el tipo de datos

1964
01:16:41,280 --> 01:16:42,640
que nos proporcionan

1965
01:16:42,640 --> 01:16:44,719
y encaja muy bien como modelo,

1966
01:16:44,719 --> 01:16:45,920
pero no estamos diciendo que el

1967
01:16:45,920 --> 01:16:48,719
la arquitectura del modelo estadístico

1968
01:16:48,719 --> 01:16:51,360
tiene algo que ver con la arquitectura

1969
01:16:51,360 --> 01:16:53,679
de las piezas físicas del cerebro

1970
01:16:53,679 --> 01:16:55,840
o está un poco en la zona gris

1971
01:16:55,840 --> 01:16:57,679
donde es como wow ese modelo

1972
01:16:57,679 --> 01:16:59,280
encaja tan bien

1973
01:16:59,280 --> 01:17:01,840
y la anatomía se ve tan bien tal vez

1974
01:17:01,840 --> 01:17:04,080
hay algo allí

1975
01:17:04,080 --> 01:17:07,040
por lo que en esta sección revisan el

1976
01:17:07,040 --> 01:17:09,440
trabajo empírico que ha intentado

1977
01:17:09,440 --> 01:17:11,600
validar o falsificar los principios clave de la

1978
01:17:11,600 --> 01:17:13,920
codificación predictiva en la corteza

1979
01:17:13,920 --> 01:17:16,480
, así como algunos problemas con el enfoque,

1980
01:17:16,480 --> 01:17:19,280
por lo que se centra principalmente en el

1981
01:17:19,280 --> 01:17:21,760
cerebro de los mamíferos y el tipo de célula neural,

1982
01:17:21,760 --> 01:17:23,280
pero

1983
01:17:23,280 --> 01:17:25,679
como siempre  podemos pensar en

1984
01:17:25,679 --> 01:17:26,640
qué es

1985
01:17:26,640 --> 01:17:28,560
lo adyacente aquí, como cuáles son los

1986
01:17:28,560 --> 01:17:30,960
roles de otros tipos de células en la

1987
01:17:30,960 --> 01:17:33,280
codificación predictiva en el cerebro de los mamíferos

1988
01:17:33,280 --> 01:17:35,440
y luego cómo podría  como el

1989
01:17:35,440 --> 01:17:37,600
sistema nervioso de los insectos o cómo mis otros

1990
01:17:37,600 --> 01:17:40,640
sistemas nerviosos o sistemas no nerviosos realizan la

1991
01:17:40,640 --> 01:17:42,800
codificación predictiva

1992
01:17:42,800 --> 01:17:44,480
y luego solo brindan

1993
01:17:44,480 --> 01:17:46,080
un ejemplo

1994
01:17:46,080 --> 01:17:48,159
en el que toman lo que habían mostrado en la

1995
01:17:48,159 --> 01:17:50,080
figura uno

1996
01:17:50,080 --> 01:17:52,640
con este esquema de codificación predictiva de niveles múltiples

1997
01:17:52,640 --> 01:17:53,840


1998
01:17:53,840 --> 01:17:55,040
que era una especie de

1999
01:17:55,040 --> 01:17:57,679
programación predictiva  como dicen

2000
01:17:57,679 --> 01:18:00,239
uno para pensar en la cognición cortical

2001
01:18:00,239 --> 01:18:01,440
y ahora van a aclarar esa

2002
01:18:01,440 --> 01:18:04,080
conexión en la figura dos, así que en el

2003
01:18:04,080 --> 01:18:06,320
lado derecho está la figura dos

2004
01:18:06,320 --> 01:18:08,560
y ese es el modelo de micro circuito canónico

2005
01:18:08,560 --> 01:18:11,199
de bostos

2006
01:18:11,199 --> 01:18:12,880
mapeado en la conectividad de un

2007
01:18:12,880 --> 01:18:14,239
cortical  región

2008
01:18:14,239 --> 01:18:17,040
y luego en rojo están esos parámetros

2009
01:18:17,040 --> 01:18:19,520
que hemos estado discutiendo la musa y

2010
01:18:19,520 --> 01:18:22,560
los épsilons los medios y luego los

2011
01:18:22,560 --> 01:18:24,239
errores de predicción

2012
01:18:24,239 --> 01:18:25,120
y luego

2013
01:18:25,120 --> 01:18:27,600
aquí hay algunas otras representaciones

2014
01:18:27,600 --> 01:18:30,560
de lo que otros han

2015
01:18:30,560 --> 01:18:33,440
escrito como esa columna cortical

2016
01:18:33,440 --> 01:18:34,960
así que aquí hay como un evolutivo

2017
01:18:34,960 --> 01:18:36,480
perspectiva que conecta parte de la corteza de los

2018
01:18:36,480 --> 01:18:39,679
mamíferos con la

2019
01:18:39,679 --> 01:18:41,920
región del cerebro de palio aviar

2020
01:18:41,920 --> 01:18:44,000
y, por lo tanto, la forma en que las personas resuelven

2021
01:18:44,000 --> 01:18:46,800
esto  Los tipos de diagramas de circuitos,

2022
01:18:46,800 --> 01:18:50,320
como mirar el tejido real

2023
01:18:50,320 --> 01:18:52,320
y luego aquí están las estadísticas, y

2024
01:18:52,320 --> 01:18:55,120
cómo se relaciona el tejido

2025
01:18:55,120 --> 01:18:57,040
con

2026
01:18:57,040 --> 01:18:58,640
el modelo estadístico, pero eso es lo

2027
01:18:58,640 --> 01:19:00,480
que exploran en la figura 2.

2028
01:19:00,480 --> 01:19:03,839
Está bien, cualquier idea, está

2029
01:19:05,360 --> 01:19:07,440
bien,

2030
01:19:07,440 --> 01:19:08,960
en la sección

2031
01:19:08,960 --> 01:19:11,760
3 continúan revisando el trabajo empírico.

2032
01:19:11,760 --> 01:19:14,000
pero con un mayor enfoque en el aprendizaje automático

2033
01:19:14,000 --> 01:19:14,960


2034
01:19:14,960 --> 01:19:17,280
del entorno no supervisado y

2035
01:19:17,280 --> 01:19:18,719
supervisado,

2036
01:19:18,719 --> 01:19:20,719
y también explorarán cómo

2037
01:19:20,719 --> 01:19:22,560
la arquitectura de codificación predictiva

2038
01:19:22,560 --> 01:19:24,640
puede hacerse más biológicamente plausible

2039
01:19:24,640 --> 01:19:27,040
relajando ciertas suposiciones implícitas en

2040
01:19:27,040 --> 01:19:29,120
este modelo canónico

2041
01:19:29,120 --> 01:19:33,360
e introduciendo acción en la figura de la imagen.

2042
01:19:33,360 --> 01:19:35,360


2043
01:19:35,360 --> 01:19:37,360
tres resumen algunos diferentes lo que ellos

2044
01:19:37,360 --> 01:19:40,719
llaman paradigmas de codificación predictiva

2045
01:19:40,719 --> 01:19:42,000
resumen de las

2046
01:19:42,000 --> 01:19:43,520
relaciones de entrada y salida para cada paradigma de

2047
01:19:43,520 --> 01:19:45,040
codificación predictiva

2048
01:19:45,040 --> 01:19:47,600
por lo que es la entrada y la predicción la

2049
01:19:47,600 --> 01:19:50,480
salida de diferentes paradigmas

2050
01:19:50,480 --> 01:19:52,080
así que aquí está la codificación predictiva clásica

2051
01:19:52,080 --> 01:19:52,960


2052
01:19:52,960 --> 01:19:55,520
en la observación los datos vienen en las

2053
01:19:55,520 --> 01:19:57,679
predicciones  sobre esas observaciones están

2054
01:19:57,679 --> 01:19:59,760
saliendo es un instante

2055
01:19:59,760 --> 01:20:02,400
Instantánea en

2056
01:20:02,400 --> 01:20:05,199
tiempo real: el despliegue anticipado en tiempo real

2057
01:20:05,199 --> 01:20:07,199
aún obtiene datos como entrada,

2058
01:20:07,199 --> 01:20:09,040
pero predice la observación en

2059
01:20:09,040 --> 01:20:11,440
el siguiente paso de tiempo, lo cual es un

2060
01:20:11,440 --> 01:20:13,040
problema similar a predecirlo en este

2061
01:20:13,040 --> 01:20:14,719
paso de tiempo, pero es un poco

2062
01:20:14,719 --> 01:20:16,080
diferente

2063
01:20:16,080 --> 01:20:18,159
que puede suceder no solo  a través del tiempo

2064
01:20:18,159 --> 01:20:20,719
sino también en el contexto del espacio

2065
01:20:20,719 --> 01:20:23,440
con una codificación predictiva espacial

2066
01:20:23,440 --> 01:20:25,360
y luego estos dos a la derecha la

2067
01:20:25,360 --> 01:20:27,280
codificación predictiva supervisada en la dirección

2068
01:20:27,280 --> 01:20:28,880
generativa y discriminativa

2069
01:20:28,880 --> 01:20:30,000


2070
01:20:30,000 --> 01:20:32,239
tiene que ver con la relación con la

2071
01:20:32,239 --> 01:20:34,239
lectura en etiquetas

2072
01:20:34,239 --> 01:20:36,639
y la entrega de observaciones

2073
01:20:36,639 --> 01:20:38,080
o viceversa

2074
01:20:38,080 --> 01:20:39,600
hazme un gato

2075
01:20:39,600 --> 01:20:41,600
aquí hay una imagen de un gato

2076
01:20:41,600 --> 01:20:43,040
aquí hay una imagen

2077
01:20:43,040 --> 01:20:44,400
que es un

2078
01:20:44,400 --> 01:20:45,520
gato estas son algunas de las formas en que

2079
01:20:45,520 --> 01:20:46,960
se han aplicado los modelos de codificación predictiva

2080
01:20:46,960 --> 01:20:49,360


2081
01:20:49,360 --> 01:20:53,120
3.1 exploran la

2082
01:20:53,120 --> 01:20:55,120
forma en que el entrenamiento no supervisado se relaciona

2083
01:20:55,120 --> 01:20:57,040
con la codificación predictiva,

2084
01:20:57,040 --> 01:21:00,960
por lo que es aprendizaje automático en datos no etiquetados

2085
01:21:00,960 --> 01:21:03,840
como aquí hay

2086
01:21:03,840 --> 01:21:05,440
cien mil horas de

2087
01:21:05,440 --> 01:21:08,159
habla humana aprender a hablar eso es como

2088
01:21:08,159 --> 01:21:10,400
un enfoque sin supervisión mientras que el

2089
01:21:10,400 --> 01:21:12,400
su  el enfoque percibido sería aquí hay

2090
01:21:12,400 --> 01:21:14,400
mil horas de habla humana aquí hay

2091
01:21:14,400 --> 01:21:16,560
mil horas de un río aquí hay

2092
01:21:16,560 --> 01:21:19,120
mil horas de saber de algún otro

2093
01:21:19,120 --> 01:21:21,600
ruido y así por la etiqueta

2094
01:21:21,600 --> 01:21:23,679
entonces el algoritmo aprendería qué es el

2095
01:21:23,679 --> 01:21:25,679
habla versus la no habla versus lo

2096
01:21:25,679 --> 01:21:27,679
no supervisado pero  también hay mucho

2097
01:21:27,679 --> 01:21:29,840
de continuo y complejidad y es un

2098
01:21:29,840 --> 01:21:31,760
tema completo,

2099
01:21:31,760 --> 01:21:33,280
um, en

2100
01:21:33,280 --> 01:21:35,920
algunas de las subsecciones de 3.1 profundizan

2101
01:21:35,920 --> 01:21:37,280
un poco más

2102
01:21:37,280 --> 01:21:39,120
en los

2103
01:21:39,120 --> 01:21:40,159


2104
01:21:40,159 --> 01:21:42,719
aspectos de predicción temporal y codificación automática,

2105
01:21:42,719 --> 01:21:44,880
pero no vamos a entrar en eso

2106
01:21:44,880 --> 01:21:46,639
ahora

2107
01:21:46,639 --> 01:21:50,080
en  la figura cuatro en la sección tres

2108
01:21:50,080 --> 01:21:52,239
dos miran la codificación predictiva supervisada

2109
01:21:52,239 --> 01:21:54,800
y la miran en esa

2110
01:21:54,800 --> 01:21:56,400
dirección hacia adelante y hacia atrás

2111
01:21:56,400 --> 01:21:58,560
y aquí están mirando el

2112
01:21:58,560 --> 01:22:01,679
conjunto de datos mnist clásico para

2113
01:22:01,679 --> 01:22:04,080
evaluación donde hay algunas imágenes

2114
01:22:04,080 --> 01:22:06,239
de dígitos escritos a mano y luego está

2115
01:22:06,239 --> 01:22:09,360
prediciendo y así  aquí está la

2116
01:22:09,360 --> 01:22:11,199
codificación predictiva generativa y la

2117
01:22:11,199 --> 01:22:12,960
dirección discriminativa

2118
01:22:12,960 --> 01:22:15,920
donde aquí están entrando los datos como

2119
01:22:15,920 --> 01:22:17,840
la imagen de los cinco con las

2120
01:22:17,840 --> 01:22:19,120
intensidades de píxeles

2121
01:22:19,120 --> 01:22:21,440
y eso se relaciona con la

2122
01:22:21,440 --> 01:22:24,159
estimación de nivel más alto de que es un cinco, por lo que

2123
01:22:24,159 --> 01:22:26,320
esta es una observación en la

2124
01:22:26,320 --> 01:22:28,880
etiqueta, que es un cinco

2125
01:22:28,880 --> 01:22:32,080
aquí, la etiqueta en

2126
01:22:32,159 --> 01:22:35,599
píxeles lo refleja, por

2127
01:22:35,760 --> 01:22:37,920
lo que conecta el aprendizaje automático con la

2128
01:22:37,920 --> 01:22:40,800
codificación predictiva

2129
01:22:40,800 --> 01:22:42,560
en la sección 3-3

2130
01:22:42,560 --> 01:22:45,840
que también están explorando  algo de

2131
01:22:45,840 --> 01:22:48,320
codificación predictiva relajada

2132
01:22:48,320 --> 01:22:51,280
y tiene que ver con relajar ciertos

2133
01:22:51,280 --> 01:22:53,760
supuestos matemáticos que

2134
01:22:53,760 --> 01:22:55,840
habían introducido anteriormente por simplicidad

2135
01:22:55,840 --> 01:22:57,280
y claridad

2136
01:22:57,280 --> 01:22:58,400
, no vamos a

2137
01:22:58,400 --> 01:23:00,560
entrar en eso, pero tiene que ver

2138
01:23:00,560 --> 01:23:02,080


2139
01:23:02,080 --> 01:23:03,760
con hacerlo más

2140
01:23:03,760 --> 01:23:05,920
útil en el mundo real relajando

2141
01:23:05,920 --> 01:23:08,719
ciertos  suposiciones

2142
01:23:09,199 --> 01:23:10,800
y luego en tres cuatro

2143
01:23:10,800 --> 01:23:11,920


2144
01:23:11,920 --> 01:23:12,800


2145
01:23:12,800 --> 01:23:14,960
exploran el código predictivo profundo,

2146
01:23:14,960 --> 01:23:17,199
por lo que escriben hasta ahora en esta revisión

2147
01:23:17,199 --> 01:23:19,040
, solo hemos considerado variaciones directas

2148
01:23:19,040 --> 01:23:21,520
en los modelos de codificación predictiva fristen round bollard

2149
01:23:21,520 --> 01:23:23,600
que son relativamente

2150
01:23:23,600 --> 01:23:24,800
puros

2151
01:23:24,800 --> 01:23:26,719
y usan solo

2152
01:23:26,719 --> 01:23:28,719
reglas de aprendizaje locales biológicamente plausibles

2153
01:23:28,719 --> 01:23:30,400
allí también  existe una pequeña literatura que

2154
01:23:30,400 --> 01:23:32,480
experimenta con estos modelos a menudo

2155
01:23:32,480 --> 01:23:34,239
logra un mejor rendimiento en

2156
01:23:34,239 --> 01:23:36,320
tareas más desafiantes que el mod más puro  els

2157
01:23:36,320 --> 01:23:38,719
puede volver a ser tan puro aquí, lo

2158
01:23:38,719 --> 01:23:41,280
que significa que usa solo

2159
01:23:41,280 --> 01:23:43,840
reglas de aprendizaje locales biológicamente plausibles, por lo que solo

2160
01:23:43,840 --> 01:23:46,880
conecta entidades localmente

2161
01:23:46,880 --> 01:23:48,400
y, por lo tanto,

2162
01:23:48,400 --> 01:23:50,000
estos modelos que sugieren proporcionan una

2163
01:23:50,000 --> 01:23:51,760
amenaza vital de evidencia sobre las

2164
01:23:51,760 --> 01:23:53,679
propiedades de escala y el rendimiento de

2165
01:23:53,679 --> 01:23:56,400
redes de codificación predictiva profundas y complejas

2166
01:23:56,400 --> 01:23:57,199


2167
01:23:57,199 --> 01:23:58,800
y escriben el  El primer trabajo importante en esta

2168
01:23:58,800 --> 01:24:00,719
área es prednet,

2169
01:24:00,719 --> 01:24:02,880
que utiliza múltiples capas de

2170
01:24:02,880 --> 01:24:06,080
redes neuronales de lstms convolucionales recurrentes para

2171
01:24:06,080 --> 01:24:08,000
implementar una red de codificación predictiva profunda.

2172
01:24:08,000 --> 01:24:09,120


2173
01:24:09,120 --> 01:24:11,040
Así es

2174
01:24:11,040 --> 01:24:14,239
como se veía prednet.

2175
01:24:15,360 --> 01:24:16,880
Esta es la arquitectura prednet original

2176
01:24:16,880 --> 01:24:18,719


2177
01:24:18,719 --> 01:24:22,400
y, por lo tanto, tienen lstms convolucionales

2178
01:24:22,400 --> 01:24:24,480
que pasan información en

2179
01:24:24,480 --> 01:24:25,760
de arriba hacia

2180
01:24:25,760 --> 01:24:28,560
abajo y de abajo hacia arriba,

2181
01:24:28,560 --> 01:24:31,360
así que tal vez podamos verlo más tarde

2182
01:24:31,360 --> 01:24:33,040
y luego también

2183
01:24:33,040 --> 01:24:37,840
encontré este artículo de 2019 2020 donde

2184
01:24:37,840 --> 01:24:39,920
también exploraron si prednet en

2185
01:24:39,920 --> 01:24:42,239
realidad está haciendo codificación predictiva, pero

2186
01:24:42,239 --> 01:24:44,800
nuevamente otra madriguera de conejo.

2187
01:24:44,800 --> 01:24:46,560
así que la sección tres se

2188
01:24:46,560 --> 01:24:49,040
revisó a través de la figura tres y algunas

2189
01:24:49,040 --> 01:24:51,520
discusiones sobre las subsecciones algunas  de

2190
01:24:51,520 --> 01:24:53,280
los diferentes paradigmas y las formas en que

2191
01:24:53,280 --> 01:24:55,280
se ha aplicado esta arquitectura de codificación predictiva

2192
01:24:55,280 --> 01:24:57,760


2193
01:24:57,760 --> 01:24:59,840
en la sección 4, la conectarán

2194
01:24:59,840 --> 01:25:02,159
con otros algoritmos, por lo que

2195
01:25:02,159 --> 01:25:03,679
básicamente aceleraremos todas las

2196
01:25:03,679 --> 01:25:07,280
conexiones, excepto la inferencia activa,

2197
01:25:07,280 --> 01:25:09,120
primero conectan la codificación predictiva en

2198
01:25:09,120 --> 01:25:11,600
sección 41 a la propagación hacia atrás del

2199
01:25:11,600 --> 01:25:13,120
error,

2200
01:25:13,120 --> 01:25:15,040
esto es realmente importante como para el

2201
01:25:15,040 --> 01:25:17,040
entrenamiento de redes neuronales,

2202
01:25:17,040 --> 01:25:18,400
pero no vamos a hablar de eso

2203
01:25:18,400 --> 01:25:19,760
aquí, pero parece realmente

2204
01:25:19,760 --> 01:25:23,040
interesante discutir

2205
01:25:23,600 --> 01:25:25,760
uno que presentaremos pero luego omitiremos la

2206
01:25:25,760 --> 01:25:27,840
mayoría de los detalles  es la

2207
01:25:27,840 --> 01:25:29,520
relación entre la codificación predictiva lineal

2208
01:25:29,520 --> 01:25:32,159
y la calma y el filtrado, por lo que podemos

2209
01:25:32,159 --> 01:25:34,000
recordar que la codificación predictiva lineal

2210
01:25:34,000 --> 01:25:35,679
no es solo la

2211
01:25:35,679 --> 01:25:37,040
diferenciación de fotogramas, es donde existe

2212
01:25:37,040 --> 01:25:39,040
ese coeficiente de cuánto debe

2213
01:25:39,040 --> 01:25:42,400
olvidar cada uno de los fotogramas anteriores

2214
01:25:42,400 --> 01:25:44,960
y esto se conoce en el procesamiento de señales

2215
01:25:44,960 --> 01:25:47,199
como el filtro kalman, el filtro de

2216
01:25:47,199 --> 01:25:50,239
columna, como muestra esta diapositiva,

2217
01:25:50,239 --> 01:25:52,400
es un proceso matemático iterativo para

2218
01:25:52,400 --> 01:25:55,280
estimar rápidamente el valor real  es la posición

2219
01:25:55,280 --> 01:25:57,840
y la velocidad, por lo que también es como llegar

2220
01:25:57,840 --> 01:25:59,360
a las coordenadas de movimiento generalizadas,

2221
01:25:59,360 --> 01:26:01,760
por lo que las x son como nuestras

2222
01:26:01,760 --> 01:26:04,320
observaciones, ese es el termómetro

2223
01:26:04,320 --> 01:26:06,159
y luego aquí está el rojo,

2224
01:26:06,159 --> 01:26:09,360
es nuestra temperatura tranquila y filtrada que se

2225
01:26:09,360 --> 01:26:11,280
desarrolla a través del tiempo, por

2226
01:26:11,280 --> 01:26:12,880
lo que nuestra estimación inicial se

2227
01:26:12,880 --> 01:26:14,880
corrige rápidamente y luego incluso  aunque hay

2228
01:26:14,880 --> 01:26:18,000
mucho ruido en las observaciones,

2229
01:26:18,000 --> 01:26:19,920
el filtro kalman es como dar una buena

2230
01:26:19,920 --> 01:26:22,719
predicción sobre la temperatura real

2231
01:26:22,719 --> 01:26:25,520
y esto se ha implementado en una tonelada

2232
01:26:25,520 --> 01:26:27,679
de enfoques bayesianos, es un enfoque

2233
01:26:27,679 --> 01:26:30,159
bayesiano de procesamiento de señal totalmente estándar

2234
01:26:30,159 --> 01:26:31,440


2235
01:26:31,440 --> 01:26:35,520
y en las siguientes secciones en

2236
01:26:35,520 --> 01:26:37,920
4.2 tienen  una gran cantidad de formalismos

2237
01:26:37,920 --> 01:26:40,239
sobre el filtro kalman

2238
01:26:40,239 --> 01:26:43,760
y en el apéndice d proporcionan aún más

2239
01:26:43,760 --> 01:26:46,960
formalismos sobre el filtro kalman

2240
01:26:46,960 --> 01:26:48,239
por lo que no vamos a entrar realmente en

2241
01:26:48,239 --> 01:26:51,040
ninguno de los que

2242
01:26:51,040 --> 01:26:53,040
en la sección

2243
01:26:53,040 --> 01:26:54,880
4.3 introdujeron esta idea de

2244
01:26:54,880 --> 01:26:57,760
normalización y flujos de normalización

2245
01:26:57,760 --> 01:26:59,199
y entonces escribieron

2246
01:26:59,199 --> 01:27:00,880
el vínculo profundo entre la codificación predictiva

2247
01:27:00,880 --> 01:27:02,960
y la normalización ha sido ampliado por

2248
01:27:02,960 --> 01:27:04,719
mourinho 2020

2249
01:27:04,719 --> 01:27:06,480
por s  Situando la codificación predictiva con

2250
01:27:06,480 --> 01:27:07,840
una receta general para construir o

2251
01:27:07,840 --> 01:27:10,480
representar una distribución compleja a partir de

2252
01:27:10,480 --> 01:27:13,040
una simple e intratable,

2253
01:27:13,040 --> 01:27:15,199
por lo que alentamos a las personas a consultar este documento de

2254
01:27:15,199 --> 01:27:18,560
Merino 2020 si desean obtener

2255
01:27:18,560 --> 01:27:21,280
más información sobre la codificación predictiva, los

2256
01:27:21,280 --> 01:27:24,880
codificadores automáticos variacionales y las conexiones biológicas,

2257
01:27:24,880 --> 01:27:27,920
pero no lo discutiremos hoy.

2258
01:27:27,920 --> 01:27:29,360
Otro modelo al que conectan la

2259
01:27:29,360 --> 01:27:30,880
codificación predictiva

2260
01:27:30,880 --> 01:27:34,719
se conoce como modelo de competencia sesgada

2261
01:27:34,719 --> 01:27:35,840
y

2262
01:27:35,840 --> 01:27:38,880
se desglosa en 4.4,

2263
01:27:38,880 --> 01:27:41,520
pero solo quería obtener un ejemplo de

2264
01:27:41,520 --> 01:27:44,080
este modelo de competencia sesgada

2265
01:27:44,080 --> 01:27:46,880
y esto es del artículo sprottling

2266
01:27:46,880 --> 01:27:48,560
de 2008

2267
01:27:48,560 --> 01:27:50,800
dentro de cada etapa de procesamiento.  los nodos

2268
01:27:50,800 --> 01:27:53,120
compiten para estar activos en respuesta al

2269
01:27:53,120 --> 01:27:56,000
patrón actual de actividad de retroalimentación

2270
01:27:56,000 --> 01:27:57,840
recibida de la entrada sensorial o la

2271
01:27:57,840 --> 01:28:00,960
etapa de procesamiento anterior, por

2272
01:28:00,960 --> 01:28:02,880
lo que es como si hubiera competencias entre

2273
01:28:02,880 --> 01:28:05,920
las subunidades para ser activadas por ciertos

2274
01:28:05,920 --> 01:28:07,679
tipos de entradas

2275
01:28:07,679 --> 01:28:12,000
y luego hay algunos detalles técnicos,

2276
01:28:12,000 --> 01:28:13,280
está bien

2277
01:28:13,280 --> 01:28:16,159
ahora  finalmente a la inferencia activa ya que estamos

2278
01:28:16,159 --> 01:28:20,000
cerca del final de la discusión

2279
01:28:20,880 --> 01:28:22,800
esto  será un poco divertido de explorar

2280
01:28:22,800 --> 01:28:24,880
también en las próximas semanas, por supuesto, para

2281
01:28:24,880 --> 01:28:25,840
que escriban

2282
01:28:25,840 --> 01:28:28,560
en 4.5. La codificación predictiva también se puede

2283
01:28:28,560 --> 01:28:31,920
ampliar para incluir acciones que

2284
01:28:33,600 --> 01:28:35,679
permitan a los agentes de codificación predictiva

2285
01:28:35,679 --> 01:28:37,840
emprender acciones adaptativas sin ningún

2286
01:28:37,840 --> 01:28:39,600
cambio importante en sus

2287
01:28:39,600 --> 01:28:41,120


2288
01:28:41,120 --> 01:28:43,280
algoritmos fundamentales. La idea clave es tener en cuenta.  Hay dos

2289
01:28:43,280 --> 01:28:46,159
formas de minimizar los errores de predicción.

2290
01:28:46,159 --> 01:28:48,239
La primera es actualizar las predicciones para que

2291
01:28:48,239 --> 01:28:50,880
coincidan con los datos sensoriales, lo que corresponde a

2292
01:28:50,880 --> 01:28:53,920
la percepción clásica

2293
01:28:53,920 --> 01:28:55,600
. Pensé que la pelota iba a estar

2294
01:28:55,600 --> 01:28:58,480
aquí, pero supongo que se acabó la

2295
01:28:58,480 --> 01:28:59,920
percepción.

2296
01:28:59,920 --> 01:29:01,679


2297
01:29:01,679 --> 01:29:04,080
forzar los datos sensoriales entrantes

2298
01:29:04,080 --> 01:29:05,920
para que coincidan con la

2299
01:29:05,920 --> 01:29:07,679
predicción voy a mover la pelota a donde

2300
01:29:07,679 --> 01:29:09,199
espero que esté

2301
01:29:09,199 --> 01:29:11,760
o esperaba que la pelota estuviera en el

2302
01:29:11,760 --> 01:29:13,520
lado izquierdo de mi campo visual,

2303
01:29:13,520 --> 01:29:15,520
así que voy a mover mis ojos hacia el

2304
01:29:15,520 --> 01:29:16,400
derecho

2305
01:29:16,400 --> 01:29:17,840
para que esté en el lado izquierdo de mi

2306
01:29:17,840 --> 01:29:20,000
campo visual, por lo que su acción no

2307
01:29:20,000 --> 01:29:21,840
siempre es como extender la mano y mover la

2308
01:29:21,840 --> 01:29:22,800
pelota

2309
01:29:22,800 --> 01:29:25,120
, también puede ser la acción de elegir

2310
01:29:25,120 --> 01:29:28,000
dónde mirar, por ejemplo.

2311
01:29:28,000 --> 01:29:30,320
y esto es lo que es realmente fascinante

2312
01:29:30,320 --> 01:29:33,360
porque pasamos tanto tiempo en las

2313
01:29:33,360 --> 01:29:35,520
secciones anteriores en las primeras 50 ecuaciones

2314
01:29:35,520 --> 01:29:36,400


2315
01:29:36,400 --> 01:29:37,760
sin acción,

2316
01:29:37,760 --> 01:29:39,440
así que es como, oh no, habrá

2317
01:29:39,440 --> 01:29:42,480
como otras 250 ecuaciones con acción

2318
01:29:42,480 --> 01:29:43,920
entrando en escena,

2319
01:29:43,920 --> 01:29:46,239
pero en realidad es bastante

2320
01:29:46,239 --> 01:29:47,520


2321
01:29:47,520 --> 01:29:49,520
Introducción simple El enfoque básico para incluir la acción

2322
01:29:49,520 --> 01:29:51,840
en la codificación predictiva es minimizar la

2323
01:29:51,840 --> 01:29:53,920
energía libre variacional con respecto a la

2324
01:29:53,920 --> 01:29:55,199
acción.

2325
01:29:55,199 --> 01:29:57,600


2326
01:29:57,600 --> 01:29:59,440


2327
01:29:59,440 --> 01:30:00,800


2328
01:30:00,800 --> 01:30:02,960


2329
01:30:02,960 --> 01:30:05,040


2330
01:30:05,040 --> 01:30:07,760
Verá, va a depender

2331
01:30:07,760 --> 01:30:10,159
de dónde mire

2332
01:30:10,159 --> 01:30:12,239
. Podemos hacer explícita esta dependencia

2333
01:30:12,239 --> 01:30:14,719
dependiente implícita usando la regla de la cadena

2334
01:30:14,719 --> 01:30:17,199
del cálculo.

2335
01:30:17,199 --> 01:30:19,520
Inferencia activa. Es una inferencia sobre la

2336
01:30:19,520 --> 01:30:21,199
acción.

2337
01:30:21,199 --> 01:30:22,320
Así que aquí está

2338
01:30:22,320 --> 01:30:25,040
la tasa de cambio de acción a

2339
01:30:25,040 --> 01:30:26,880
través del tiempo.

2340
01:30:26,880 --> 01:30:28,880
Es la tasa de cambio de un

2341
01:30:28,880 --> 01:30:30,880
funcional de energía libre.

2342
01:30:30,880 --> 01:30:34,719
anteriormente era de observaciones y de

2343
01:30:34,719 --> 01:30:36,719
estados a través del tiempo

2344
01:30:36,719 --> 01:30:38,080
y ahora

2345
01:30:38,080 --> 01:30:41,520
o es o de a observaciones son un fu  nción

2346
01:30:41,520 --> 01:30:43,520
de acción son parcial o

2347
01:30:43,520 --> 01:30:46,080
totalmente dependientes de la acción

2348
01:30:46,080 --> 01:30:48,960
y que se entiende como un

2349
01:30:48,960 --> 01:30:52,800
derivado del cambio en la acción

2350
01:30:52,800 --> 01:30:57,040
o de a sobre a parcial de a sobre a

2351
01:30:57,040 --> 01:30:59,120
es un modelo directo que hace explícita

2352
01:30:59,120 --> 01:31:01,199
la dependencia de la observación sobre la

2353
01:31:01,199 --> 01:31:03,280
acción y debe  ser proporcionada o aprendida

2354
01:31:03,280 --> 01:31:05,679
por el acto del algoritmo

2355
01:31:05,679 --> 01:31:06,960
porque así es

2356
01:31:06,960 --> 01:31:09,040
como

2357
01:31:09,040 --> 01:31:12,639
cambian las observaciones que son una función de la acción a

2358
01:31:12,639 --> 01:31:14,560
medida que cambian las acciones, de

2359
01:31:14,560 --> 01:31:16,960
modo que es como una capa que no se había

2360
01:31:16,960 --> 01:31:19,360
planteado, por lo que dicen porque

2361
01:31:19,360 --> 01:31:21,120
es necesario resolver  esa proporción o esa

2362
01:31:21,120 --> 01:31:22,400
derivada

2363
01:31:22,400 --> 01:31:24,320
que necesita aprender eso o

2364
01:31:24,320 --> 01:31:25,760
recibir eso

2365
01:31:25,760 --> 01:31:27,679
y es una

2366
01:31:27,679 --> 01:31:29,440
cosa separada para aprender además de este

2367
01:31:29,440 --> 01:31:32,159
modelo generativo estándar para la percepción, por

2368
01:31:32,159 --> 01:31:34,239
lo que hablamos de acción

2369
01:31:34,239 --> 01:31:36,400
como una dependencia

2370
01:31:36,400 --> 01:31:38,080
de las observaciones,

2371
01:31:38,080 --> 01:31:39,920
que también es lo que conecta la

2372
01:31:39,920 --> 01:31:42,480
inferencia activa  a la teoría del control perceptivo,

2373
01:31:42,480 --> 01:31:44,800
así es como introducimos la acción en

2374
01:31:44,800 --> 01:31:47,360
la imagen solo desde un primer

2375
01:31:47,360 --> 01:31:49,679


2376
01:31:50,400 --> 01:31:52,800
paso, avancemos un

2377
01:31:52,800 --> 01:31:55,760
nivel más y luego cualquier pensamiento que tenga

2378
01:31:55,760 --> 01:31:58,639
sería genial  entonces,

2379
01:31:58,639 --> 01:32:01,040
si estuviéramos hablando solo

2380
01:32:01,040 --> 01:32:03,280
del caso de inferencia, la

2381
01:32:03,280 --> 01:32:04,880
minimización del error de predicción sería

2382
01:32:04,880 --> 01:32:06,800
como si la imagen que estamos viendo fuera

2383
01:32:06,800 --> 01:32:08,560
exactamente la que estamos prediciendo o

2384
01:32:08,560 --> 01:32:11,120
viceversa, entonces, ¿cómo se ve eso

2385
01:32:11,120 --> 01:32:13,040
para la acción?

2386
01:32:13,040 --> 01:32:14,880
El error de predicción simplemente se convierte en la

2387
01:32:14,880 --> 01:32:16,000
diferencia.  entre la

2388
01:32:16,000 --> 01:32:18,800
observación actual y el objetivo o punto

2389
01:32:18,800 --> 01:32:20,320
de referencia que ya trae este tipo de

2390
01:32:20,320 --> 01:32:23,120
perspectiva de alostasis de homeostasis,

2391
01:32:23,120 --> 01:32:24,880
sin embargo, esto plantea la pregunta de

2392
01:32:24,880 --> 01:32:27,280
dónde provienen los puntos de referencia, de

2393
01:32:27,280 --> 01:32:28,800
dónde provienen los objetivos y cómo

2394
01:32:28,800 --> 01:32:30,480
se calculan, de

2395
01:32:30,480 --> 01:32:32,719
dónde procede

2396
01:32:32,719 --> 01:32:34,639
una respuesta genérica a la pregunta.  es que los

2397
01:32:34,639 --> 01:32:36,080
puntos fijos se pueden heredar de

2398
01:32:36,080 --> 01:32:38,400
evolutivos u ontogenéticos, que son

2399
01:32:38,400 --> 01:32:40,560
imperativos de desarrollo

2400
01:32:40,560 --> 01:32:42,480
o proporcionados por otros circuitos neuronales

2401
01:32:42,480 --> 01:32:44,159
involucrados en el comportamiento y la planificación dirigidos a objetivos

2402
01:32:44,159 --> 01:32:45,040


2403
01:32:45,040 --> 01:32:47,120
para los propósitos actuales, simplemente podemos

2404
01:32:47,120 --> 01:32:50,320
tomarlos como variables dadas exógenamente, por

2405
01:32:50,320 --> 01:32:52,719
lo que lo que no se aborda en esta revisión  es

2406
01:32:52,719 --> 01:32:54,400
esta gran pregunta de los

2407
01:32:54,400 --> 01:32:57,679
antecedentes y el aprendizaje de las preferencias, como

2408
01:32:57,679 --> 01:32:59,920
¿cómo saber qué preferir?

2409
01:32:59,920 --> 01:33:01,600
¿Cómo sabes cuánto cambiar? ¿Cómo

2410
01:33:01,600 --> 01:33:02,719
prefieres?

2411
01:33:02,719 --> 01:33:04,880
¿Qué tan estrictas deben ser tus preferencias?

2412
01:33:04,880 --> 01:33:06,560
Esas son áreas realmente muy importantes

2413
01:33:06,560 --> 01:33:08,960
, pero son como abrir la

2414
01:33:08,960 --> 01:33:12,480
puerta pero no entrar en ella

2415
01:33:12,719 --> 01:33:15,600
en 4.5.1. También es sencillo

2416
01:33:15,600 --> 01:33:19,360
modelar el  costos potenciales de acción

2417
01:33:19,360 --> 01:33:20,960
en organismos biológicos hay

2418
01:33:20,960 --> 01:33:23,120
diferentes costos de acción

2419
01:33:23,120 --> 01:33:24,560
y, entonces, ¿cómo se puede modelar

2420
01:33:24,560 --> 01:33:27,360
matemáticamente

2421
01:33:28,000 --> 01:33:29,920
bien? Dicen al

2422
01:33:29,920 --> 01:33:32,159
incluir explícitamente la acción dentro del

2423
01:33:32,159 --> 01:33:34,320
modelo generativo de la siguiente

2424
01:33:34,320 --> 01:33:36,239
manera y ahora han agregado este

2425
01:33:36,239 --> 01:33:39,120
término adicional con un costo de acción.

2426
01:33:39,120 --> 01:33:40,960
simplemente muestra cómo, una vez que la acción

2427
01:33:40,960 --> 01:33:43,840
se ha introducido como una dependencia

2428
01:33:43,840 --> 01:33:46,159
de la función de observación

2429
01:33:46,159 --> 01:33:48,840
, puede haber

2430
01:33:48,840 --> 01:33:51,280
otras cosas que se pueden calcular

2431
01:33:51,280 --> 01:33:54,400
relacionadas con la acción,

2432
01:33:54,880 --> 01:33:58,159
cualquier pensamiento sobre la acción o, como en sus

2433
01:33:58,159 --> 01:34:00,400
lecturas, dónde se ha

2434
01:34:00,400 --> 01:34:03,440
integrado bien o no la acción

2435
01:34:03,440 --> 01:34:04,480
con

2436
01:34:04,480 --> 01:34:07,440
predictivo  codificación,

2437
01:34:09,520 --> 01:34:11,040
bueno,

2438
01:34:11,040 --> 01:34:14,719
creo que,

2439
01:34:14,960 --> 01:34:16,320


2440
01:34:16,320 --> 01:34:18,159
desde mi perspectiva, la

2441
01:34:18,159 --> 01:34:20,080
codificación predictiva

2442
01:34:20,080 --> 01:34:22,880
no se trataba realmente de acción antes de este

2443
01:34:22,880 --> 01:34:25,360
texto, por lo que era

2444
01:34:25,360 --> 01:34:27,679
nuevo para mí porque yo  Siempre relaciono la

2445
01:34:27,679 --> 01:34:30,639
acción con el procesamiento predictivo y

2446
01:34:30,639 --> 01:34:33,040
la inferencia activa, no es realmente una codificación predictiva,

2447
01:34:33,040 --> 01:34:34,880
por lo que era nuevo

2448
01:34:34,880 --> 01:34:36,639
y

2449
01:34:36,639 --> 01:34:39,360
de todo esto por lo que hemos pasado,

2450
01:34:39,360 --> 01:34:43,119
me pareció fascinante cómo

2451
01:34:43,119 --> 01:34:46,320
crece la complejidad y cuántas

2452
01:34:46,320 --> 01:34:48,719
aplicaciones diferentes

2453
01:34:48,719 --> 01:34:51,760
se pueden encontrar en la codificación predictiva, especialmente en

2454
01:34:51,760 --> 01:34:53,360
aprendizaje automático y

2455
01:34:53,360 --> 01:34:55,040
ya sabes todo

2456
01:34:55,040 --> 01:34:56,239
esto

2457
01:34:56,239 --> 01:34:58,719
uh matemática

2458
01:34:58,719 --> 01:35:00,800
um

2459
01:35:00,800 --> 01:35:02,800
pesadez

2460
01:35:02,800 --> 01:35:06,560
uh trae muchas soluciones diferentes

2461
01:35:06,560 --> 01:35:09,199
a los problemas que enfrentamos durante mucho tiempo

2462
01:35:09,199 --> 01:35:09,920
tan

2463
01:35:09,920 --> 01:35:11,119
fascinante

2464
01:35:11,119 --> 01:35:13,920
agradable sí de acuerdo es realmente interesante

2465
01:35:13,920 --> 01:35:16,159
así que solo una toma más de la

2466
01:35:16,159 --> 01:35:18,639
inferencia activa que exploramos en 26

2467
01:35:18,639 --> 01:35:20,880
también

2468
01:35:21,280 --> 01:35:24,800
como la codificación predictiva, el control pid

2469
01:35:24,800 --> 01:35:26,480
optimiza el sistema hacia el

2470
01:35:26,480 --> 01:35:28,800
punto de ajuste y es ideal para sistemas reguladores simples

2471
01:35:28,800 --> 01:35:30,960
como la

2472
01:35:30,960 --> 01:35:32,880
acción de los termostatos en popa,

2473
01:35:32,880 --> 01:35:36,000
por lo que estamos tomando la misma noción de a como

2474
01:35:36,000 --> 01:35:37,440


2475
01:35:37,440 --> 01:35:39,199
algo que influye en las observaciones,

2476
01:35:39,199 --> 01:35:40,960
así es como introducimos la acción en

2477
01:35:40,960 --> 01:35:43,280
el modelo.  hacemos que las observaciones dependan

2478
01:35:43,280 --> 01:35:45,199
de la acción la

2479
01:35:45,199 --> 01:35:47,520
acción está determinada por tres términos

2480
01:35:47,520 --> 01:35:49,520
un término proporcional que  h minimiza la

2481
01:35:49,520 --> 01:35:51,360
distancia entre la ubicación actual

2482
01:35:51,360 --> 01:35:52,960
y el punto de ajuste

2483
01:35:52,960 --> 01:35:53,920
p

2484
01:35:53,920 --> 01:35:55,679
un término integral que minimiza la

2485
01:35:55,679 --> 01:35:58,560
integral de este error en el tiempo i

2486
01:35:58,560 --> 01:36:00,320
y un término derivado que minimiza

2487
01:36:00,320 --> 01:36:03,119
la derivada del error d

2488
01:36:03,119 --> 01:36:04,639
la combinación de los tres términos

2489
01:36:04,639 --> 01:36:07,040
produce un control robusto y simple

2490
01:36:07,040 --> 01:36:09,199
El sistema que se puede aplicar con algunos

2491
01:36:09,199 --> 01:36:11,280
ajustes para controlar casi cualquier

2492
01:36:11,280 --> 01:36:13,199
proceso regulatorio simple

2493
01:36:13,199 --> 01:36:15,520
y las coordenadas de movimiento más altas podrían funcionar

2494
01:36:15,520 --> 01:36:18,400
aún mejor, pero como exploramos en 26,

2495
01:36:18,400 --> 01:36:20,960
a menudo, el pid es suficiente

2496
01:36:20,960 --> 01:36:24,239
, por lo que es un método de ingeniería muy común

2497
01:36:24,239 --> 01:36:26,400
y conecta

2498
01:36:26,400 --> 01:36:28,800
el bayesiano.  mecánica de acción y

2499
01:36:28,800 --> 01:36:30,960
percepción y cognición de

2500
01:36:30,960 --> 01:36:33,760
inferencia activa en coordenadas generalizadas para

2501
01:36:33,760 --> 01:36:36,960
control pid en el entorno de ingeniería

2502
01:36:36,960 --> 01:36:38,639
y luego proporcionan un montón de otros

2503
01:36:38,639 --> 01:36:41,199
formalismos

2504
01:36:41,199 --> 01:36:43,520
en el apéndice

2505
01:36:43,520 --> 01:36:46,880
b dan algunos detalles sobre esta idea de

2506
01:36:46,880 --> 01:36:48,960
gradientes naturales

2507
01:36:48,960 --> 01:36:51,119
que creo que podríamos abordar en  el

2508
01:36:51,119 --> 01:36:53,440
punto uno y en el punto dos,

2509
01:36:53,440 --> 01:36:55,360
así que no hablaremos de eso aquí, pero el

2510
01:36:55,360 --> 01:36:57,360
apéndice b trata sobre la precisión como natural

2511
01:36:57,360 --> 01:36:58,560
los gradientes

2512
01:36:58,560 --> 01:37:00,719
y el apéndice

2513
01:37:00,719 --> 01:37:02,800
c presentan algunos desafíos para la

2514
01:37:02,800 --> 01:37:04,400
implementación neuronal de la propagación hacia atrás

2515
01:37:04,400 --> 01:37:07,199
mediante la codificación predictiva, por lo

2516
01:37:07,199 --> 01:37:09,199
que tampoco lo exploraremos aquí,

2517
01:37:09,199 --> 01:37:12,159
pero ambos son

2518
01:37:12,159 --> 01:37:15,600
apéndices cortos y muy temáticos realmente interesantes

2519
01:37:15,600 --> 01:37:17,920
en la sección cinco, es

2520
01:37:17,920 --> 01:37:20,000
de bastante buena longitud y nosotros  No voy a

2521
01:37:20,000 --> 01:37:23,040
cubrir nada de eso hoy

2522
01:37:23,040 --> 01:37:25,520
porque ha sido un punto cero excelente y lo suficientemente largo,

2523
01:37:25,520 --> 01:37:28,000
pero en el punto uno y el punto dos

2524
01:37:28,000 --> 01:37:29,840
y más allá esperamos desempacar la

2525
01:37:29,840 --> 01:37:32,080
discusión y las direcciones futuras

2526
01:37:32,080 --> 01:37:33,360


2527
01:37:33,360 --> 01:37:34,560
para

2528
01:37:34,560 --> 01:37:37,280
que tengamos mucho espacio.  para

2529
01:37:37,280 --> 01:37:38,480
preguntas

2530
01:37:38,480 --> 01:37:40,800
y creo que ambos o todos salimos de

2531
01:37:40,800 --> 01:37:44,080
esto con más preguntas que respuestas,

2532
01:37:44,080 --> 01:37:47,119
pero ¿cuáles serían sus pensamientos finales

2533
01:37:47,119 --> 01:37:49,839
maria

2534
01:37:51,760 --> 01:37:53,119
um

2535
01:37:53,119 --> 01:37:56,000
no estoy seguro de qué pensar ahora tengo que

2536
01:37:56,000 --> 01:37:58,480
digerir toda la información que tenemos aquí

2537
01:37:58,480 --> 01:37:59,679
hoy

2538
01:37:59,679 --> 01:38:01,520


2539
01:38:01,520 --> 01:38:03,199
pero tengo  esto

2540
01:38:03,199 --> 01:38:06,320
este deseo de continuar uh con la

2541
01:38:06,320 --> 01:38:09,360
comprensión de los formalismos quiero

2542
01:38:09,360 --> 01:38:12,639
um sabes entender mejor lo que

2543
01:38:12,639 --> 01:38:14,800
dijiste aquí y

2544
01:38:14,800 --> 01:38:18,080
estoy muy entusiasmado con la

2545
01:38:18,080 --> 01:38:20,000
parte biológica de la

2546
01:38:20,000 --> 01:38:22,239
implementación de la codificación predictiva  entación y

2547
01:38:22,239 --> 01:38:24,080
procesamiento predictivo,

2548
01:38:24,080 --> 01:38:26,639
así que como no está

2549
01:38:26,639 --> 01:38:28,800
realmente desarrollado en este

2550
01:38:28,800 --> 01:38:31,760
documento, creo que el autor dijo

2551
01:38:31,760 --> 01:38:34,400
que, en realidad, no está realmente explorado.

2552
01:38:34,400 --> 01:38:35,840


2553
01:38:35,840 --> 01:38:37,040


2554
01:38:37,040 --> 01:38:38,960


2555
01:38:38,960 --> 01:38:43,440


2556
01:38:43,440 --> 01:38:46,719


2557
01:38:46,719 --> 01:38:51,119
años pasados

2558
01:38:51,119 --> 01:38:52,800
increíble,

2559
01:38:52,800 --> 01:38:55,280
sí, creo que será divertido, las próximas

2560
01:38:55,280 --> 01:38:58,239
discusiones muchas gracias por toda la

2561
01:38:58,239 --> 01:39:00,800
ayuda y por este punto cero y

2562
01:39:00,800 --> 01:39:02,960
brock también, así que nos

2563
01:39:02,960 --> 01:39:06,080
vemos, muchas gracias, nos

2564
01:39:06,080 --> 01:39:07,119
vemos,

2565
01:39:07,119 --> 01:39:10,119


2566
01:39:56,320 --> 01:39:58,400
adiós.


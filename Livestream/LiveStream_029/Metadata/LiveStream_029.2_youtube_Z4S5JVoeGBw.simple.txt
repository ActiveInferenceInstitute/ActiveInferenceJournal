SPEAKER_02:
Welcome to the Active Inference Lab live stream number 29.2.

Today is September 28th, 2021, and we will be discussing the paper Active Inference, an active inference framework for ant colony behavior.

And today we are here with the first author, Daniel Friedman.

The Active Inference Lab is a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at all of the links shown here.

This is a recorded and an archived livestream, so please provide us with feedback so that we can improve upon our work.

All backgrounds and perspectives are welcome here, and we will be following good video etiquette for livestreams.

Here at the short link, you can find all of the upcoming and recent live streams that we have done.

So please get in touch if you would like to participate.

Today, our goal is to discuss the paper that we just said active in France.

And we are going to do some different papers in the upcoming weeks.

Like I said, get in touch if you want to participate.

We're just going to introduce ourselves and go through a little bit of warming up.

I am Blue Knight.

I'm an independent research consultant from New Mexico, and I'll pass it to Daniel.


SPEAKER_00:
Thanks, Blue, for facilitating.

I'm Daniel, and I'm a researcher in California.

And I guess I'll start with what I'm excited about today.

I was just excited that in 29.1, we could sort of forge out in a lot of different directions and make some initial trails to a few different ideas.

And then now in the 29.2, maybe we can develop some of those threads slash trails and maybe get some of it down in writing.

So it should look pretty good.


SPEAKER_02:
Nice.

Yeah, I'm excited, too.

There are some questions that I've been mulling over over the last week, and so I'm kind of excited to delve into those and also into the code for the model today if we have time.

Being that it's just us, I think we might get there.

Well, maybe more people will join.


SPEAKER_00:
There might be a little bit of lag.

I'm not sure.

So you might want to check that.


SPEAKER_02:
OK.


SPEAKER_00:
Yeah.


SPEAKER_02:
I don't know if it's my internet.

So the big question of this paper is how can a group of active inference agents solve complex group forging problems?


SPEAKER_00:
Well, first one I'll say is if you're recording it locally, as well as streaming, then we can always upload a high-quality recording.

And it does sort of bear on this big question.

This is a quote from the paper, and that's sort of the big question in collective behavioral studies, which is, okay, if...

if each agent if each subunit of the system had all the information needed well then there wouldn't necessarily be a challenge for coordination it's like if if it already knew what to do then it would be a challenge but how do systems make adaptive decisions when subunits don't have all the information so in the case of an online team each person's only seeing a different

part of the picture in the case the ants they're each in a little bit different part of their niche but those are some of the similarities which is that each nestmate is only going to be experiencing its local environment and um we want to have colonies that work so welcome steven maybe you can say hello while blue is just testing some stuff out oh great yeah good morning um


SPEAKER_01:
well, in North America anyway.

Yeah, I'm based in Toronto, and I work with communities and sort of systemic coaching approaches, and I'm very interested in how active influence scales in different ways.

So even though this is about small things, it's also about lots of small things.

So I'm going to pass to this person.


SPEAKER_00:
Well, your audio also just had some weirdness.

Steve, maybe talk directly into it.

Oh, okay.

Is that better?

Yeah.

Oh, okay.

But you mentioned scaling in the small.

So one funny thought, again, this picture is from the ants that I was studying for five years in Arizona.

And yes, the nestmate looks like a tiny guy, like a small ant.

But then we know that these colonies can live for 20, 30 plus years.

They're also several meters under the ground.

And they weigh, you know, dozens of kilograms.

So then I'm thinking, okay, it's a similar age, weight, and height to myself.

And it's subdued.

Let's see.


SPEAKER_02:
How's that?

Is it any better?


SPEAKER_00:
It still is not doing anything.

I just reopened it.

But if it's not, yeah.

Okay, now it's just the slides.

Or just the Jitsi.

So it looks like it's working now.


SPEAKER_02:
Okay, perfect.

Yeah, it's the slides open also.

It just is too much.


SPEAKER_00:
Yeah.

Yeah.

Welcome back, everybody.

Fun times.

The show goes on.

Foraging never stops.


SPEAKER_02:
It does.

And Stephen will rejoin us, hopefully.

He's having audio issues also.

It's a good day for technical mishaps.

We're having rainy weather in New Mexico, which never, ever happens.

So all of the ants are underground hiding, and the internet does not want to cooperate.

Apparently, this is so...

All right, so speaking of multi scale integration and and across systems as happens in ants and in the ant colony.

So we talked about making this table last time.

And so here, like what, how is each system related to

the agent, the niche, the interaction, and the stigma of you.

So we started this table.

Do you want to maybe take us through what you've started here, Daniel?


SPEAKER_00:
Yeah, um, see if you can get the slides again, because it's not going to be visible at all on the stream.


SPEAKER_02:
Okay, I can also switch.


SPEAKER_00:
Yeah, I can also do that.

Um, so a lot of informal analogies and connections were being drawn amongst different systems.

And that's kind of what complexity and systems thinking and active inference is about, like finding patterns across systems.

So

Maybe there's other ways to break it up, but at least one way we could at least start to classify and find similarities and differences between different systems would be this way with the columns.

So we have the system of interest, like the int colony is a system of interest.

Then we can think about what is the agent level?

What is the subunit level?

And that might be the level where the active inference agent is being specified.

We then have the niche that the agent is operating in.

So that's the generative process that's presenting observations or outcomes to the agent, which is embodying a generative model.

So that's related to this generative process versus generative model distinction.

The process actually outputs the temperature or the observations, whereas the generative model is a model of the process.

And then there's two kinds of interactions that are possible.

So there's sort of a transient interaction.

that might be amongst agents.

Welcome back, Steven.

There's the transient interactions amongst agents.

And in the case of the ants, that's like an antennal contact.

And then there's the stigmergic interactions where there's a modification of the niche.

And so sometimes it helps to think about these interactions amongst agents that are without memory.

as being like the transient horizontal interactions, like the common example of the bird flock.

Those are the birds looking at each other.

They're interactions amongst agents that modify the likelihood of agents doing certain future behaviors, but they don't leave a trace on the environment.

And that can be contrasted with Stigmargic,

or niche modification behaviors, which are defined by being the ones that do leave a trace on the environment.

So maybe we'll explore different sections today, but we can always say, okay, let's return to slide eight and let's add a row.

What's a system of interest?

Let's add a column or let's fill in some columns.

So, so far we have ant colony, brain and internet slash computer ABCs.

GEB, the systems that matter.

Stephen, what would be a fun system to throw into this mix and to look for parallelisms among?


SPEAKER_03:
We can't hear you, Stephen.


SPEAKER_00:
Yeah, we still don't hear you, Stephen.

Yeah, your microphone's definitely not working.


SPEAKER_03:
Okay.


SPEAKER_02:
So I started filling in for brain and, you know, it all really is so dependent on what the, what level you want to evaluate.

Right.

So, you know, on one level, there's like the little sub regions of the brain and you took us through, I think in the dog zero video, a very nice example for ants that has been kind of done.

describing the different like organoids or substructures within the brain because definitely there are structures that all have different functions like the cerebellum is motor function primarily and you know the cortex is like higher level thought and and so there's you know all these different sub regions of the brain but then also at the cellular level there are interactions you know that that

expand those regions that are from, you know, the cellular cellular level.

Um, and what is the, what the niches or the interaction, um, you know, might, might be different depending on like the, the cells are responsible for driving interactions between the organoids.

So I'm not sure what the level of, um, of, uh,

of the level of analysis, the right one is.

And then stigmatizing and modification, I think about, you know, long-term potentiation and like adaptation within the brain is very, maybe not stigmatizing, but it's a really important modification, I think, and sensitivity to different neurotransmitters and things like that.


SPEAKER_00:
Blue, it's still really lagging.

Can you just stop streaming?

And I'm going to pick it up on my side, okay?

Go for it.

So stop the streaming and then just let me know when you did it.

Let's see how this... Okay, let's see what happens.

This could be very, very interesting to do the live hack like this.

Okay, YouTube, we're going to wait for it to catch up.

Sorry, everybody.

So it goes.

Okay, wow.

We just hot swapped...

Livestream feeds.

So we're back in the game.


SPEAKER_02:
No level of preparation can prepare for laggy internet, I suppose.


SPEAKER_00:
No, and it's like half the time of mine and half the time on your side.

But if you're watching live, definitely add any comments or questions because we're just going to be kind of exploring and chilling today.

But continue to facilitate.

I'll just be broadcast or roll.


SPEAKER_02:
Cool.

So the...

Yeah, we were talking about the brain and the interaction and the sigmoid G and then for the internet, I don't know.

What do you think is the, is the agent or what, what is the subunit of the internet?

Like, is it each server or what, where do you have the subunit at Daniel or Steven?

What do you guys think?


SPEAKER_01:
Hmm.

Well, so this sort of ties into what they might say in research, you know, unit of analysis as well, but that's not necessarily quite the same thing as an agent.

I suppose on the internet, ultimately, it's whoever's triggering choices, right?

um in the system um which is normally a yeah it's normally a person um interacting with the screen in some way um i think the overwhelming amount of interactions on the internet are probably not human driven because any human interaction is going to trigger


SPEAKER_00:
truly a vast number of communications amongst other systems inside of the computer and probably remotely.

So maybe, and I think this is one of the unique challenges, but also interesting areas is like the internet is a cyber physical system.

So we could have one model where the agent, like the subunit are physical devices.

So we kind of draw

the blanket, we focus on the partitioning around a physical object, like internet of things.

We have partitionings around things.

We could also have a human centered partitioning.

So partitioning around humans, but then there's a lot of,

gray space in between the humans a lot of links that aren't human between the human links and then another approach might be purely informational or computational and seeing the agents as software agents but i think that interface where how will we talk about the software and the hardware agents coming together is where the internet of things is developing and what it's coming to mean


SPEAKER_01:
I mean, one thing you could think about there also is when, say, for instance, I make a choice or I do something on the Internet and there's a whole load of there's a cascade of other things going on through the different routers and the servers, etc.

Is that more like the pheromone trial?

that sort of dissipates off into the distance leaving records and how much of it's like other agents initiating because often you know every time you do something to some extent yeah databases might update um however and this may change to what extent that is alive is another question

You know, is it sort of a computational form of niche modification that's going on in most cases in terms of the internet?

But what they're trying to do, because it's that idea of the battle for your mind, they're trying to make it so that in most cases the user has less thinking to do.

in terms of the choices they make and the design.

There's a great book in web design called Don't Make Me Think.

And the idea is that you go somewhere and it's almost obvious what you need to do.

A bit like when you pick up a newspaper, you know that you're just going to turn the pages.

So how much of all this architecture in the background is to support giving you what you expect to be given when you expect to be given it?

And how much of it is...

Which, you know, how much of it is actually what you might term an agent with some sort of teleology?


SPEAKER_00:
Well, you mentioned expectation there, and expectation is something that agents engage in.

So maybe it's even, we could keep on forking this off, but what do we know from the active inference ontology?

What defines an agent?

external states blanket states internal generative model what does the internal generative model entail well it includes predictions about external states it includes policy affordances it includes preferences it includes expectations over some of these things as well so when we say agents

We can be referring to the whole Markov decision process or other ways of viewing the active agents.

And then the expectations of the agent are really important because it's not just that psychologically it's going to determine whether the agent likes it or not, but it helps us connect these systems like the ant colony and subunits of the brain.

So we don't need to use only the psychological interpretation.

But then we can have a psychological interpretation where one seems warranted, given the system of interest.

But also we're going to be able to compare that and look for similar functional features in systems without getting into just some sort of abstract debate over whether it's alive or whether it's certain feels a certain way to be that system.


SPEAKER_02:
So, Daniel, you brought up the generative model, and I was just going to bring it up right before this.

And so I've been really

you know, thinking a lot about the generative model and like, where is the generative model, which is like, I think probably my favorite podcast episode that I've ever cut is going to come out next week.

And it talks about like the location of the generative model.

And so, you know, it doesn't necessarily have to have a location.

because it's like software right like so i mean that yeah there's like a copy of the software running on disk maybe maybe it's in the cloud like it does but it doesn't matter because it's the program that's running so the location of the program is is almost not even relevant um so i just i think about like what is the generative model of the internet right like what does that look like


SPEAKER_00:
great question um inactivism for cyber physical systems and embodiment uh in encodement maybe there's some analogous term to what would describe what we call 4e in the embodied extended encultured etc for digital systems because they do have important attributes that

in a strange way, almost suggests that some of these features that are suggested to be quintessentially related to having a body and embodiment also exist for software.


SPEAKER_01:
Could I put in something onto that?

Say we think about, you've got the active inference generative model, but say you've got an active niche modification model.

algorithm so ie it's still working off the agents being making choices but what's happening is so for instance say you've got abc testing happening on a website so basically they send out three different campaigns with three different types of profiles now the user makes choices

Based on how the user makes a choice, they may be across the whole market.

The ABC choices might get modified in that niche.

But eventually, maybe then actual clients get sent down certain routes.

um i mean a classic case of this is also what happens with those in-store cards because i actually ended up going i didn't get go for it you know i had an interview with the nectar card people in the uk once and they uh they do these cards that you get in store and basically i think it worked out they have like a million permutations of offer that they can give you based on what your different buying options are so in a way they're kind of

But they only serve you up then whatever you see one of those, right?

You don't see what all the other people are getting.

And some of them are small changes.

Some of them might be bigger changes.

So in some ways, it's like the niche is modifying to the agent's adaptive behavior.

But the actual niche isn't necessarily doing adaptive active inference.

There may be some sort of active inference in a more kind of non-living way.


SPEAKER_00:
There might be, we'll probably have better words and ontologically informed ways of talking about it, but there's kind of like low bar active inference or like austere active inference that is just any input output structure.

it's going to be able to be fit into something like a partially observable Markov decision process.

So whatever it is that the niche is doing, it's emitting observations so we can model it under this certain low-bar active inference versus the sort of anti-dissipative, truly cognitive, strategic active inference and

Also, there's no reason why we can't have hybrid models where there's like active inference agents in a niche that isn't an active inference agent.

So like in this ANT model, the niche is just a landscape of pheromone densities with a random dissipation factor.

So that external niche could be made very much more complicated

and the observations that it passes back to the agents could be like customized to them.

And that could be calculated by an active inference niche or not.

And that's sort of the extremely interesting work of Axel, Constant, and others with some type of symmetry occurring with the agent and the niche jointly inferring each other.

So like what is the niche's generative model of the organism?

What is the internet's generative model of the user?

So it's interesting questions.


SPEAKER_02:
I love that idea of symmetry, especially between the agent and the niche and its relationship or potential relationship to the kind of Gaia theory that the niche has to expect or have an expectation of the organism that's going to interact with it or create it or modify it or exploit it in some way in order for there to be some kind of reciprocal interaction there.

So super nice.

Maybe we can come back to looking at this table and maybe think of other systems that might be interesting to go through and move on to some of the questions for 29.2.

But wait, before we go to this one, I want to go to this one.

Hold on.

So we were talking about the role of dopamine last time.


SPEAKER_00:
Which slide?


SPEAKER_02:
Sorry, this is slide... I skipped to... Sorry, I can't see.


SPEAKER_01:
I think it's number seven.


SPEAKER_02:
Yeah, number 11.

Okay.

So...

So we were talking about last time, the role of dopamine and like influencing the affordances.

And this really kind of speaks to the role of reward in a generative model.

And so, you know, we talk about one of the beautiful things about active inference is it kind of changes the reward structure in the model because, you know, there's some kind of reward that's given to

you know, actually finding the food, right?

So there's some reward for finding the food, but also, and even in this model, so the reward structure has changed, right?

So it's not just about finding the food.

And maybe this is a good opportunity to look through the code maybe as well.

But, you know, we think about the change in the reward structure and we place some value on the epistemic, you know, the knowledge about the model of the world.

So if we can build a more precise model of the world, we can also gain more food overall.

So I don't know if you guys have comments on that.


SPEAKER_00:
Yes, Stephen, go ahead.


SPEAKER_01:
Yeah, I think also, as well as, like you say, building that model of the world becomes, it's an action model, okay?

So it can be what might seem factually spurious but practically useful, you know?

So it may be really useful to have some bizarre ritual about how to get on a soccer pitch and try and kick the ball, right?

But if it does something better than what you're doing otherwise, go for it.


SPEAKER_00:
And I think the challenge comes with increasingly esoteric rituals if they're not related to the actual causal structure of the world.

If they work for a moment, they're not going to be adaptive.

Because if your only strategy is, you know, praying to the kicking God instead of practicing, it might work or it might indirectly induce some sort of outcome.

But is that a long-term strategy?

but agree that the generative model is including a model of action.

It's not just about getting precision about how things are out there, because what ultimately matters is getting precision on our actions.

And then there's sort of these two

parallel paths or two intertwining paths, which is, do we take a reward centered approach?

Is behavior dominated by reward?

And then the other side of the reward coin is like the punishment aversion.

So carrot and the stick, that's sort of a sister road, two lane road.

Do we pursue reward or do we avoid regret and suffering?

And then a third road is this

uncertainty minimization approach.

So I pasted in this paper by Colombo, explanatory pluralism, and they review three prominent hypotheses of dopamine activity.

So anhedonia, which is

unpleasurable states incentive salience and reward prediction errors and i i think this is a really fascinating paper and it does apply to the ant context because they make a very specific system point talking about the role of dopamine in the mammalian brain reviewing empirical evidence and then also pull back to the broader implications for the philosophy of science

So they conclude that the evidence currently vindicates explanatory pluralism.

In other words, these explanations do not subsume or knock out each other.

There's a pluralism.

There's multiple explanations that exist that capture different parts of how the system really works.

The vindication implies that grand unified claims of the advocates of the PTB are unwarranted.

We can look at what that is again, but more generally, we suggest that the form of scientific progress in the cognitive sciences is unlikely to be a single overarching grand unified theory.

Little possible indirect evidence

reference to those who claim that the value or the utility of, for example, the free energy principle or active inference is just simply that it explains everything.

I don't need to even say any more or detail where I used it or show you the code or show you that it's better than another model.

The fact that this model I can fit everything into

is the utility and it will subsume other models and other frameworks now that's a contentious point that has to be either empirically supported or not but it just is so interesting how dopamine as reward molecule or the other side of that coin with the punishment or as a precision molecule the empirical evidence alone

are not enough to disambiguate that.

And so, you know, trust the data, let the data speak.

But then this is just showing us a kind of real time that our models of the world get projected as we do sensemaking on the experimental outcomes.

And it's not always as clear as it seems.


SPEAKER_02:
Well, you know, it just reminds me like so I mean, just from a very literal sense, if we think about dopamine as

reward for minimizing prediction error like how much do we all like to be right like all of us so i mean like just just for being correct like that in in and of itself is a huge like dopamine reward and i think about like i'm a super ultra nerd and so i i geek out on things like difficult math problems or solving a rubik's cube or or these kinds of things and i'm just like

Finally, when I get, or even code, right?

Finally, when the code runs, there's this huge, something that you've been working on for a really long time, it's a huge dopamine rush for me, which is why I keep going back to doing these kinds of things.

But ultimately, that's a prediction error reward.

So it's interesting to think about that.


SPEAKER_00:
And here's sort of the two ways that they're phrased and it's somewhat subtle, but I think it gets at the difference between reinforcement learning and active inference.

Is there a drive for being right and then we prefer to be certain?

Or is the drive for certainty and then there's a preference over being right or being accurate?

So are there situations where it actually brings us solace

even though the direction of the outcome isn't what we expected or isn't what on some sense conversation is what we prefer, but actually the fact that we're able to have certainty outweighs the fact that the outcome went the way it went.

So, and maybe there's other phrasings too, but it's just, again, which one do we put in the driver's seat, precision or reward?


SPEAKER_02:
Well, so what's the difference between being certain and being correct?

Like being right and being certain?

Like, okay, if I'm really certain, but then that certainty is unwarranted, then that's when I'm wrong.

But then I have this prediction error gap, right?

So either way, so what's the difference really between being certain and being correct?


SPEAKER_01:
Well, I think one, if I could just jump in there, I think one thing that's really interesting is if everything's based on energy minimization, like you've got, you've got a process, you're going to keep doing it.

And it's optimizing to make it basically, if I use less energy,

I'm being more efficient, I'm being faster, I'm being smoother.

So it always seems like, yeah, go for the reward.

The thing is, though, once you get into something where it's more complex or uncertain, you know, the idea, and they talk about this now, actually, there's a whole area in...

development work and design thinking called failing forward fail forward so the idea is fail fast fail quick but part of this is like in you know and i always say within ethical bounds right because it might be great for the designer as long as it's not a community member being sort of

being frustrated by this but but it makes sense in the sense that and i don't think reinforcement learning can really account for this is the benefit of doing something because it might be something which doesn't work but now you can take from it it's almost like you're giving yourself a sample of the counterfactuals

in a way by something going wrong, but you don't, obviously we can then break it apart and say, well, there might be something right still in that, you know, and some bits which are more wrong and we can sort of do stuff with that information.


SPEAKER_00:
almost like succeeding forward would be better than failing forward.

I mean, failing forward has this psychological benefit of helping people recognize that it's okay to not get it right the first time and just get something on the paper and we'll iterate from there.

So the rhetoric of fail forward makes sense, but

having something that's even positive like succeed forward or win forward or contribute forward or learn forward or develop forward those actually capture the functional spirit that um is here and let's think about how that relates to blue's question so what's the difference between being certain and correct so there could probably be many takes on this but i thought of this classic science textbook

you know, when they give you the science textbook, the difference between accuracy and precision.

Now, in many cases, this also is not coming from the active inference ontology.

So they may be using these terms in a slightly different way, but the point is we're getting at two complimentary components of precision.

and um basically uh we can see that there's these two off diagonals the top left and the bottom right so high accuracy high precision it's like your average is on target so that's the accuracy is where you're aiming at accuracy aim and then the precision is referring to the tightness of the distribution around that mean

so you could have high accuracy like it's always at the bullseye but the precision is low or the uncertainty the spread is high you could also have like a thermometer with it giving you very tight readings high precision but it's systematically biased in a certain direction and um

This is just one way to get at two different aspects.

Now we've seen this in a few different cases in variational inference as being the difference between the expectations, the means of the distributions.

That's like the accuracy.

And then the precision is like the variance of that distribution.

So you could have a very tight distribution, the Q distribution very tight and close to the P distribution.

That would be accurate and precise.

You could also have a really overfit, overconfident model that is just simply quite divergent from the underlying P distribution.

So which one is being optimized?

Are we trying to aim for a tight scatter and then second order is we want it to be in the middle?

Or do we aim for the middle and then the second order is we want them to be accurate?

Steven?


SPEAKER_01:
Yeah, that that that question is a good one.

And I think distinguish because often they might get conflated in many ways.

But it's good to pull apart precision and accuracy because it is a, you know, you could have, you know, you could be very precise in the fuzzy set that you have, and you could, but it could still be fuzzy, you could have something

like you say, more accurate, but it moves around a lot.

And I think that that's an important distinction that might be lost in many cases.


SPEAKER_00:
Yes, and it's actually one reason why it's important to sort of stick with the uncertainty and pass the estimate, the state estimate, as well as like the uncertainty around a state estimate, pass that through.

Because if you collapse the uncertainty,

and you treat all estimates as being equally precise, there's this whole dimension of the data that has been dropped.

For example, in gene expression analysis, sometimes there's uncertainty over whether a gene is differentially expressed or not.

Like it has a distribution and you pick some cutoff.

But then sometimes people take the genes on one side of that line and then they just move forward with them like these are the differentially expressed genes as if it was a discrete categorization.

They take a continuum and then discretize it, which makes certain kinds of analyses really simple or straightforward.

But a whole aspect of the variability of the data has been completely crunched out.

Steven?

Steven?


SPEAKER_01:
Yeah, and this also then ties into scale as well.

You could, and how much, so like you mentioned the thermometer, so the thermometer gives a fairly

well fairly precise sense of the of the reading it's fairly consistent and it's averaging out the average is fairly relatively accurate in terms of the average but it's not going to give you the range or the

the spread within that and that that can apply to a number of things and then you as you go down to less and less if you for instance i suppose like you would say if someone's purely looking at one ant or one ant's behavior at a time then you you might have this sense of i'm being very accurate in terms of my ant analysis so to speak however at the level of the

you know, the larger colony, you're not as accurate, you know, so there's some interesting sort of scale things that can't be avoided really.


SPEAKER_00:
Are ants accurate and or are ants precise, the nest mates?

So it's almost like maybe we could think of them as being high accuracy but low precision.

Like, let's just imagine what that would be like for the colony.

So each forager is in the neighborhood of making adequate foraging decisions, but there's a lot of noise around that.

So over thousands of interactions and foraging trips...

you're going to end up getting a lot of your points in the middle.

So it's cheap because it's lower precision, so you can have simpler subunits, but it's centered around the target.

Whereas if you have it centered over something that's not the target, it's going to be suboptimally adaptive for the colony.

But I think ants just show that it's not about making every single action perfectly...

the right one, it's about being right more often than not, and then just doing it many, many, many times, which is related to that sort of develop slash fails slash succeed forward.


SPEAKER_02:
I'm sorry, go ahead, Stephen.


SPEAKER_01:
Just one thing.

I think this is also useful just in the sense of

And it's almost implicit with the agent-based modeling approaches that get used in biology.

But it's quite interesting in the sense that that brings in that intersubjective piece.

What are these ants trying to do, for instance?

How are they trying to organize?

What's going on?

Rather than

the first person perspective.

What do I see?

What's my perspective on the ant as an observer, which might be more psychological in our world?

Or what is it that I see the group doing on average, like the temperature?

But when you start to get into agent-based approaches, it does allow this second person perspective

stuff to happen which is really i know in psychology is the missing piece really in a lot of cases so um you know how this how this in a way also ties into thinking about agents rather than thinking about what i see someone or something doing from the outside and inferring from their behaviors externally


SPEAKER_02:
qualitatively sets up a different type of experiment or modeling so I wonder about the different scales like as we scale from the nest mate to the colony like are are they comparable like are they asking the same question like the colony wants to know like will there be enough food

And the forager never asks that question.

The forager always wants to know, where is the food?

So they're asking very different kinds of questions, because the forager knows, it has no uncertainty that its job is to find food.

And so the colony wants to have enough food, but the forager always wants to get the food.

And I wonder how this kind of asking different questions scales to things like the brain and components of the brain, like

or the internet and components of the internet like like what are is is what defines a scale whether we're asking the the same question or not like or is it the goal of the system that defines the scale of the system i don't know it just made me think of that yeah yeah i mean that's a good that's a really good point i mean yeah you could if you're starting to look from the agent


SPEAKER_01:
And you could even think about that in the brain.

Yeah, if different parts of the brain are foraging information, so it's not like that would explain a little bit why you have these kind of parts.

Different parts of the brain have different personality

um queries going on even when certain parts of the brain get sort of put to sleep so to speak so um rather than yeah i mean that could be that could explain and if they're if they're each working away i mean like a colony um and yeah who which one is in some of them get to be in the driving seat

And some just, yeah, some get to ask different questions or some get to sort of have their question to influence other questions.

And some of them just get on and do their thing, if that makes sense.

You're a forager.

Basically, you're going to forage, right?

That's the deal.

But someone else, it might be like, I'm hungry, right?


SPEAKER_00:
you gotta forage more um right come on oh just i i rapidly assembling some image memes it actually breaks down well into the chess and other fields other serious games of tactics and strategy

So strategy, so like Blue, your question, the colony was asking, do we slash I, we don't know what pronouns the colony uses.

Do we slash I have enough food?

It's as if it's asking that question.

Does it identify as a entity, an eye, or does it identify as an ensemble?

We don't know.

But then the Nesme is asking the more tactical and action-oriented question, which at a first pass is like, where is the food?

But we know that...

inactive inference what matters isn't just like simply inference on the location of the food out there but what matters is inference on our action selection for how we're going to get that food or how we're going to implement a strategy that has a good trade-off of us finding it so like here chess meme um

Tactics mean doing what you can with what you have.

So like whatever pieces are on the board, you're gonna have to do some sort of action selection for your next move.

Here, strategy requires thought, tactics require observation.

So that's kind of interesting because observation is one of our terms in the ontology, like the agent is receiving observation.

Without observations, tactics cannot be implemented.

And then strategy is requiring mental action, just like our previous discussions in 28.

The thought is actually the planning over observation strategies.

uh nice double blue and um there's other definitions of strategy and tactics and how they're similar or different so that's just kind of like cool that um it's a way to think about maybe not just that are the levels asking different or the same questions but might levels be defined or delineated by the difference in the questions that they ask

with independent of the spatial size of the subunit, the more tactical is like more agent level.

And then the more strategic requiring reflection or accumulative culture or mental action, that level is being implemented by higher and higher nestings.

Wait, Blue, you're muted because you went down to just one.


SPEAKER_02:
Can you hear me?


SPEAKER_00:
Yeah.


SPEAKER_02:
Perfect.

I went down to just one so you can share the memes with us because I'm missing out on the meme action and I can't stand it a little bit.


SPEAKER_00:
I'm so sorry.


SPEAKER_02:
So speaking of action and different levels, so this is something that I've really been thinking about over the course of the last week.

This is the next question on this slide 11.

is this prospect of intergenerational stigmergy.

And I just wonder, as we were both foraging for memes and slide content, like creating the 29.0, we have this internet, right?

And so we just Google, search for whatever.

And informational foraging, I know, is not the same as maybe ants foraging for food.

But, you know, we're both foraging and we come across the same kind of content and ended up like putting it into the 29.0 in different ways, in different places.

And I just wonder, you know, there's this concept of intergenerational stigmatry that's very, and I wonder if this is human centric or if there are other examples of this in animal kingdom and perhaps among ants.

I know, Daniel, we've talked at length about

the different foraging habits for example of different um different foraging habits of different species or of the same species of amp with different colonies and so where do they get this habit like to forge differently like at different times of the day or whatever and and it spans generations and so is there some trace left in the environment like humans we leave traces for the next generation i mean we directly do some cumulative cultural evolution transfer

And there are other species I know that teach like dolphins, I know teach like some foraging habits and so forth.

So what other examples do we have of this kind of intergenerational stigmergy or like, you know, we leave traces like the time capsule of, you know, like what to find like every decade we open up the time capsule from.

you know, whatever the previous decade was and find things like eight tracks and like whatever things that are put in the time capsule that are just like, Oh, look way back in the day.

Um, so I don't know, what do you think about that?

Um, just relative to ants and relative to this intergenerational stigmergy and the different foraging habits of colonies from the same species?


SPEAKER_00:
Good question.

And, uh,

where do the priors for foraging come from?

And then to what extent is there scaffolding of one nestmate in their foraging activity by older nestmates, which is actually within a generation, that's like sort of some skin cells being scaffolded by previous skin cells.

It's like, if you just kill all the foragers of a colony,

foragers that step into those roles will perform sub optimally that's actually related to colony collapse in the honeybees so there's some element of like they have an inkling of how to adapt their foraging yet um also there's flexibility um i mean

It just makes me think of how we could actually study it, like putting nest mates in different environments or having one colony and splitting it and training it a certain way and then reintegrating them and seeing if...

what mattered more was like the environment at that generation or whether it was possible to have epigenomic or genomic changes that sort of lock in and shape foraging.

And then on 12, you know, I'm doing a little live meme modification.

like forging is not an act but a habit with a probably misattributed aristotle quote so it's almost like forging is not an act it's a policy it's a policy over specific motor actions uh but yeah the intergenerational part is very interesting and it

I'm just thinking if there's other true intergenerational ant scaffolding, one example not in foraging is like when a leaf cutter ant queen goes on her nuptial flight, she takes a little sub sample of the fungal garden and uses that to start the next generation, which actually I think relates to the second point you had about 30.


SPEAKER_02:
So that's super interesting.

And also, um,

directly related to the microbe transfer, right?

Like you think about microbial transfer that happens like through the birth canal, like there's a difference between C-section delivered babies and babies that actually pass through the birth canal, just in terms of how they're inoculated as well as breastfed babies versus formula fed babies.

There's a huge like inoculation difference with your like just microbial flora.

And so that is the, and I wonder what, you know, this speaks to the holobiont concept.

Like we're not just humans.

We're humans inoculated with all kinds of things.

And maybe ants aren't just ants.

They're ants plus that fungal, and there's been studies on that fungus, like on the fungal composition, is it different from colony to colony?

Do you know?

I don't know.


SPEAKER_00:
I know that one,

group in my previous lab, they isolated nest mates and then they treated them with antibiotics and changed their microbiome.

And others have also shown that there's a role for the microbiome in recognition.

So because it modifies the smell of the ants, that modifies how they recognize each other.

Maybe that's more like a dialect.

Like that's kind of how they recognize that they're in the very short time range, like in the same group or they're likely to be collaborative.

But I returned to eight to think about what you just described about like the body and the inheritance of microbial forms.

Like, is there a way to refer to that?

Human is the species, but then, you know, if humans contain and are multitudes,

You know, there's the body, there's the other life forms, there's social interactors.


SPEAKER_02:
Humans aren't the species, but the host, which ties in very nicely to our next live stream, which I'm very excited about, which with Matthew Sims, he's going to come and talk to us about

uh, the symbiosis between squid and Vibrio fischeri in the light organ, um, which is something near and dear to my heart.

I've worked in New Mexico.

I don't know.

Uh, how is this possible in, um, right upstairs, you know, you have those labs that like, you know, everybody in the lab and you guys can come and work and, um, like share equipment sometimes.

And so I knew this lab and our lab were very close, uh, the lab of Michelle Nishiguchi.

And she studied Vibrio fischeri and the light organ.

So I knew all these people doing directly related projects.

But it's a really cool concept that maybe we're all in some form of symbiosis.

And I know in mice, they have sterilized mice.

And the mice grow up totally neurotic, like mice lacking the microbiome-free mice.

They're completely neurotic and have way abnormal behavior.

And they've linked a lot of

you know, human behavior or human abnormal or abnormal behavior or neuroatypical behavior to microbiome stuff as well.

So, so it's really interesting to think about maybe we're all symbionts in some fashion.

I mean, and, and like when maybe we're not the species, but we're just the host.


SPEAKER_00:
Reminds me of Demetrius Bolas speaking about the through others we become ourselves and it was situating active inference in the dialectical tradition and you have a coming together

of different pieces and they don't reduce their distinction.

They actually become, especially over evolutionary time, more polarized.

Like the nuclear genome becomes more nuclear.

The mitochondrial genome becomes more reduced in mitochondrial or the host becomes on one trajectory and the parasite goes in the other.

They don't converge and equalize each other.

They end up becoming increasingly specialized parts

in a newly emergent type of system,

And then these multi-specified systems can be fragile because maybe now there's a failure mode of the part and then that causes the whole system to fail.

So like Bucky Fuller would always talk about how all cases of biological extinction are due to over-specialization.

Might be a bit of an overstatement, but that's sort of the idea is like generalists can deal with changing niches better.

because they're always able to hedge their bets amidst uncertainty and average out over all the different foraging investments.

Whereas a specialist does really well until they don't.

But in life, it's not just like you can do well most of the time and then you can fail sometimes.

Living systems need to be succeeding at least adequately all the time.

If any part of your body

was under producing ATP for like five minutes, it would get tissue damage.

So it's better to be adequate all the time, rather than hyper successful for even almost all the time.

And then what kinds of strategies are resilient amongst that kind of uncertainty and situation.


SPEAKER_02:
So about halfway through what you were saying, I wondered when we started having a political discussion.

Because it just really, you know, speaks to

specialization or maybe polarization, which we see in society.

So like, you know, we have these different factions or different like sub types or sub classes of people and biological systems that don't become more like one another.

They become more separate from one another.

And I think, you know, in the resounding echo chamber that is

you know, a self-similar political environment or class environment, you know, it becomes this echo chamber that, you know, just increases our certainty, right?

Increases the validity of our own model.

And it makes us question the model of others.

So I wonder, and this brings us back to what we're going to discuss with Matthew Sims next week.

Is that right?

Or is the generative model becoming more similar for the Vibrio fischeri and the squid?

Or is it becoming more different?

I just wonder.


SPEAKER_00:
Yep.

So through interaction, do we become more similar or different?

Or what kinds of systems through interaction become more similar or different?

And then how can systems be more anti-fragile given that relationship amongst the subunits versus which failure modes are you de-risking on and which failure modes are you increasing the risk on?

So it's an interesting question.

I mean, it's not just political.

It is very broad question.

Blue, let's look at the code and maybe just scan through some sections of it and then start to see on 11, before we get to the code, is just like, what else could we include in an ant model?

Like this was, we talked about the natural history of the ant that we templated off of the formica species in the paper, but we used one pheromone.

We didn't have interactions.

We had such a sparse...

pulling over of features of ants into our model.

So that's why it's just like a introductory framework.

So what other pieces could exist for ants?

And we can kind of use the active inference breakdown of what perceptions could we add in, tactile interactions, other olfactory interactions, light, the intensity, the color, the polarization of light.

Another type of polarization.

What type of cognition could we add in?

So this was like a one step decision making ants.

It just makes a decision for the next time step based upon the local pheromone density.

what kind of cognitive processes actually exist in ants, or what kind might be interesting to see in different systems like memory, and then what actions exist.

Here we had a movement action affordance, any of the directions, and then we had sort of a pheromone deposit affordance,

that didn't actually have a specific control policy.

It was just an if-then.

If you already touched the seed, then you deposit pheromone.

If you're still looking for the seed, then you don't deposit pheromone.

So we can think, what are the perceptions, cognitions, and actions of the ants as currently modeled in the paper?

And then where can we go with that?

And then to go from that question about the simulation...

to meet the empirical data, the other side of the table, what kinds of ant data already exist?

There are several very fascinating databases that include genomics, gene expression, transcriptomic and epigenome or epigenomics data sets.

They're sparser for ants than they are for humans and mice, but they are out there and there will be more of them in the future.

What kind of behavioral data already exists?

Ethology, the study of animal behavior.

Video data, maybe they analyzed it under a certain framework, but now it could be readdressed.

And then like RFID and QR codes that can do semi-automated high throughput tracking.

So people attach RFID chips to honeybees and track their foraging activity.

And then we also have information on their ecology and morphology.

So features of which were not in our simulation.

So this is just this sort of prime looking at the code by asking like, how could the code be modified?

And then how could the world

already be seen as reaching out to us with these helpful pieces of data and then where are we going to be in the middle like what kind of model is going to connect the empirical data sets that do or could exist to the models that do or could exist and then what questions are most important because we want to be asking relevant questions and so it's better to be keeping the goals in mind as we look through everything

Okay.

Let me just re crop this window while I get the code up.

Anything you would just say on the overview or like general thoughts on the kind of multi-agent simulations or foraging simulations that you've made?


SPEAKER_02:
Uh, just, I have some questions about this simulation actually, or some, some things that I would think about to include as parameters, like in a dream simulation where we, you know, could, can simulate whatever with,

unlimited computational power and relative world data.

So I think that here I'm interested in nestmate interactions.

So I think that that would be something that would be interesting to model, like the different types of ants that exist, like the nursemaids and the foragers and the queen and the reproducers.

I can't remember, we went through all the different types of nestmates last time, but I would just be interested in kind of simulating how

As a colony, ants interact and function.

I know that that's like, you know, very, whatever, big.

And I mean, in most simulations that I've always done, it's like the same type of agent, right?

Like, so you give the agent different options, but it's never like,

this agent doesn't forage and this agent does, I don't know.

But I mean, I know that they do have different, people do do some modeling with different types of agents, but I just wonder what the compute power is and what the validity of that is.


SPEAKER_00:
Yes.

Great.

Okay.

So we're gonna, um,

take a sort of a stepwise approach.

And one thing that I think will be fun is there's people who know way more about Python and coding than both of us, for sure, including Alec Chance and others.

And then there might be others for whom

they've learned about active inference or they've read a lot about it, maybe even looked at the math, but here is going to be where we see how it looks in code.

So there are these multiple languages and ways of communicating and learning by doing active inference, like natural language, math as a language,

programming as a language art as a language maybe there's other ways too but at the very least we know that we have natural language mathematical formalisms like the equations that we're always putting up and then here's the code those threads might be looking more similar or different as we start to learn more about like interactive notebooks and ways to integrate our language use

of the natural kind with the programming kind.

So we're gonna do kind of a step down.

First, we're gonna look at the config file.

So the README file in this case is not super informative, which is fine.

But the config files where we're going to start, that's going to tell us about like what kinds of variables and parameters matter.

Then we're going to look at figure one, and that's just sort of going to be like, okay, this is how you run it.

And then we're going to pull back to the ans.py and look at what functions and subroutines are actually defined inside the code.

So inside of the config file, it's only 30 lines long, 31 lines long.

and it has some pretty clearly defined variables that are being set.

And the idea is that all the parameters are stored in this file so that you can kind of hit play and then it will look to this config file.

We'll see where that happens in a second.

And so the first block relates to if you want to add the ants in a staggered fashion, how many time steps should happen between adding new ants in?

What's the location of where the ants should begin?

Nest factor, we can look at.

I'm not exactly sure.

It is here.

I'm not exactly sure what the nest factor has to do.

It might have to do with the distance of the nest.

But let's ask Alec on that one.

The grid is the size of the map.

So here is a 40 by 40.

And then here is a very interesting block.

It's the focal area, which is three by three.

So that's the grid around each nestmate.

It's just three cells by three cells.

And so this could be changed to, you know, like four by four or something like what happens when the ant has a broader range of perception than it might have for action.

And that's the action map.

And so those are the actions that can be undertaken.

relating to basically moving like down into the left or up into the right the food location and the size of the food are provided then the wall so that specifies like within the grid what is the shape of the maze inside of the map 24 and 25 describe the the number of discretized levels of pheromone that exist and then a factor that is the stochastic decay of the pheromone trail

And then here, the observation states, so like the O in the active inference model are defined as the number of pheromone levels.

So in this case, those are both one through 10 discrete models, but it's so interesting because it opens up the door for the generative process to differ from the generative model.

So like the observations,

there could be three categories of observation, low, medium, and high.

And so you could still have a continuous variable or a discretized variable in the world in the generative process, but the generative model could be over a reduced version of that, which might be all you need to know.

Maybe there's enough stochasticity such that just knowing if you're in high, medium, low tells you whether to put on a jacket or not.

You don't need five decimal points on the temperature.

And the number of action states is also reduced to the focal size, but just as mentioned, that could also diverge.

And then the length of the simulation, I think, is specified in 31.

So those are some of the parameters that exist just in the base simulation.

Some of the degrees of freedom that do exist, and that's even before introducing these other sorts of pieces that could exist.

Okay, any thoughts here before we go to figure one?


SPEAKER_02:
just like, you know, I mean, I always want to know like, why, like, why did you choose this?

Or like, did you play with the decay factor or did you play with like the length, the time steps?

I'm sure like maybe your viewers commented that as well, but, um, I just want to, like, I'm always curious to see, like, was there any like unexpected outcomes or optimal outcomes that occurred?

Like peak, like, did you pick 500 time steps?

Because that was,

you know, best or the decay factor of 0.01 because that was most realistic or those types of things.


SPEAKER_00:
Good questions.

And I think we probably could have organized these with some sub headers and some comments a little bit more clearly.

So we're going to look at how

the simulation gets specified and then pull back to like what are the subroutines so the the top part of a python program is basically pulling in and then giving nicknames to other resources that are required so these resources can include packages like numpy which is a common toolkit used for um

math and statistics in Python.

I'm sure Blue can say more on that one.

And then there's this, the config as the CF.

So that's when we call like CF dot number of observations, that's going to call like this, which is defined as this, which is 10.

But of course that can be changed.

So that's where the variables that are set in the config file come into play.

So the header sort of introduces the

the background libraries and resources that need to be imported.

Then a few parameters are defined so that you can have like a base config set.

And then you can also just locally define some variables like the number of initial ants is like, you can define variables that are in the config file, or you can add some new variables in.

And then basically, for the observations in the length of the range of observations, this runs a routine.

And the routine is going to be defined better in ants.py.

But it basically says we're going to run it

For that many steps, for that many starting ants, for that many maximum ants, we're going to save it.

We're going to switch the location of the food.

And then we're going to just print some summary descriptors, like the number of round trips, and then save that in a file.

So that's like sort of the interface wrapper for running a simulation and then outputting some of the descriptive statistics and then also saving that file.

Now, ans.py is 353 lines long.

So it's a little bit longer, but there are some parts that we could, I think, walk through.

The first is that the ant is a class of object.

And it has a few routines.

It has its initiation routine, which is where its X and Y position, for example, are given the starting location.

And then there's updating a forward and a backwards step.

there's the class of the environment so this is kind of like if you do net logo modeling agent based modeling you know you have like the turtle like the agent and then the environment so here we're defining the agent and we're defining the environment and the relationship between the two of them is really whether we have um a stigmergic type multi-agent system or not

So you could have the environment influencing the behavior of agents, like the sunshine is influencing the behavior of the ants, but then the ants don't modify the sunshine.

So that wouldn't be stigmergy.

But if there's a possibility for the agent to modify the environment and for the environment to modify the probability of agent behaviors, there's some sort of bidirectional relationship, then it is stigmergy.

And basically some of the functions that are defined for the environment have to do with which observations are at a given cell, whether the food exists at a given cell, whether the walls exist at a given cell, and then the step forward and the step backward relate to the outgoing and the homecoming trip.

And this is basically just,

doing a delta, which I think is the action map.

It's the delta in position.

And then it checks whether the ant is returning, which is going to be, I believe, on the step backwards side versus the ant going forward.

Decay is a function that's also inside of environment because decay has to do with the decay of the pheromone at that cell.

And then the plotting has to do with just saving the file.

So that's some of this, like the gif and the save fig.

Another object type that's defined, and this is really where there was the main carryover from the previous models, is the Markov decision process.

So here, just like we've talked about in various contexts, like the A, B, C, and D are all

defined and initiated here.

Those are used as you'd expect.

And then the MDP needs to have these functions that relate to setting and resetting certain variables and taking a step.

And then I love this line 210 action inference.

Like this is sort of the kernel that is where it's happening.

Here's the negative expected free energy.

And then there's a calculation

of the negative expected energy of the negative utility calculations divided by the scale factor.

Pretty interesting.

And I don't think we experimented with modifying this scale factor.

And then... So can I just interrupt you?


SPEAKER_02:
What does that scale factor do, really?


SPEAKER_00:
Well, it's mentioned only two times in here.

And...

I'm not exactly sure what it does here.

I think that when you're comparing relative policies, as long as you're dividing them all by the same scale factor, it shouldn't change the relative preference for different policies if they're all being scaled down by the same level.

So I'm not sure why the calculation exists this way.


SPEAKER_02:
Yeah, I mean, it makes sense that they're all, like, whenever you scale it by something.

Maybe it's just to make the numbers more manageable.

Who knows?


SPEAKER_00:
Yes.

But, well, let's definitely think about that.

And then... Yeah, we could get more explanation from Alec.

But the negative expected free energies...

get saw okay well okay one possible option would be this is like over observations so we know that the observations are discretized between one and ten so if we divide by ten maybe we're going from like a one to ten scale to a zero to one scale which then lets us interpret more cleanly as like probabilities because um

the action ends up being selected based upon the softmax of the negative expected free energies.

So instead of like, you know, 8, 1 and 1, if you do 0.8, 0.1 and 0.1, it's really easy to then say, okay, right, 80% likelihood of choosing this action.

Here's a colony level function related to an ant being initiated and just appearing on the map.

plotting and saving functions.

And then here's the main.

So a lot of times in the Python programs, there's like a lot of sort of upstream functions that are going to get called.

And then main is called.

And then when main is called, that's like where the main program actually happens.

So this instantiates the environment, the ants and the paths.

For the number of ants that are to be specified, that many ants are created with reset MDPs.

All of the image files are set to zero.

The descriptive statistics, like how many completed trips and the ant locations are reset.

And then here is the time step based simulation.

So here for each ant in the ants that exist,

There's going to be some calculations.

This is related to the distance calculation.

Here's adding the ants.

Here's switching the food.

And then here is checking for each ant trajectory, whether it's returning or completed.

And then saving the file.

So...

Again, I think there could be a lot more organization and commenting to sort of understand what these different pieces do.

But yeah, I believe that's some of the main pieces.


SPEAKER_03:
Nice.


SPEAKER_02:
Are we going to move to the figures now?


SPEAKER_00:
Um.

Sure, I can actually try to download the videos and I can play the videos.


SPEAKER_02:
Oh yeah, that's great.

So yeah, I thought we were going to go switch back and forth from the code to the.


SPEAKER_00:
OK, so we'll go to the paper.

supplemental data let's see if we can play it in the stream directly will it play oh great it will play okay so here is like the ants you'll you won't see it blue but um uh diffusing around the red ones are going out and the blue ones are coming home so this is just as if you were um watching the colony with video

Um, video two, I think we're gonna add in the pheromone density.

So here, the brighter colors are the pheromone density.

And, like, there, like, one ant made it home, but then it didn't... Here comes another one.

It didn't, um, sort of lock on.

But now we can see there's a light ridge.

It's locked on to the trail, and it's gonna reinforce the trail.

And then...

it seems like the food has switched because now we're seeing some pheromone getting deposited on the left side.

And then there's not as much deposition on the right side.

But this is what really happens in ants.

Like if there's high density of pheromone on that top right, then an answer spending a lot of their time there, especially if they're carrying food, they might actually be reinforcing that part.

So some of the videos are described their similarities and differences in the text.

But I think the interesting one to look at would be

I think six and seven.

So here there's a preference for high pheromone density.

And then I believe it's seven where the ants can lay down pheromone, but they don't have a preference for it.

And so it ends up getting laid down, but then it doesn't get reinforced because they...

they just don't prefer it.

So they act randomly with respect to pheromone.

But these are just like fun simulations.

And yeah, a total obvious next step and important one would be to explore what sets of parameters certain outcomes occur within like, when do they always lock in?

When do they fail to lock in?

But yeah, good supplemental info.


SPEAKER_03:
Awesome.


SPEAKER_00:
Um, what else?

Yeah.


SPEAKER_02:
So what, what is the next obvious, um, step for you or where would you like to see this simulation go most?

Like what is the next, if you were to, you know, have unlimited time with Alec to say like, let's code XYZ, um, what, what would you, uh, dream of?


SPEAKER_00:
First, I'd probably go for the annotation and clarification because that's like where coding expertise really comes into play and that helps other people modify and build on the code the most.

Um,

When it were a little more reorganized, I think it would make sense where other sensory and action affordances could be plugged in.

But the piece that- Such as?

Such as?

Like adding another pheromone.

So instead of just having like, you know, pheromone one through 10 density, like make it specific, pheromone scent one.

And then that would allow us to go, okay, now let's just copy and paste pheromone scent two.

And then we could have more flavors.


SPEAKER_02:
So what would you make the pheromone scent 2 do?

Would you make it like pheromone scent 1 drives you to the food and pheromone scent 2 drives you home?

Or would you just make different decay rates?

Or what would you do with the different pheromones?


SPEAKER_00:
One duality would be like plus and minus.

if there was the appropriate behavior around using them.

Another one would be fast and slow decay, which maps on to the way that ants actually use the scents with like some longer chain hydrocarbons that are longer lasting and some shorter chain ones that are volatile.

Another interesting cognitive model would be like there's two or three pheromones and then what the inference is on is the ratio.

And then it could be shown like, you know, no cheating.

The ant is only perceiving the ratio of these two things.

So that would be kind of, it's kind of like bimetallism.

Like you tie to the idea of a ratio rather than to just the absolute amount of one.

So like it could be food could be a little sweet and a little salty or a lot sweet and a lot salty.

And then the inference of whether it's tasty is like whether it's balanced.

So then if we're thinking about the ant tasting balances in the world, maybe that could help it have an internal manifold that by definition would be simpler than the state space of the pheromone densities out there in the generative process.


SPEAKER_02:
so i i heard that there are two types of ants like that there are grease eating ants and sweet eating ants are they just like different like what is that what makes that distinction actually and like does the same ant really eat both things or what's up with that um just reminded me of you know like uh uh


SPEAKER_00:
evil neutral good like the classifications for characters and then what are the major macro molecules what are the nutritious molecules other than vitamins well there's like protein carbohydrates and fats lipids and

different parts of the colony life cycle might use these different nutrients differently but then what are the resources that could be obtained like sweet that could come from like an extra floral nectary so like a plant that's secreting a honeydew or scale insects and the carbohydrate-rich fluids that they secrete and ants that love sugar and pies and stuff then there's oil and then there's also like protein like there's meat ants

And the turtle ants in the canopy forests in some rainforests, I've heard from many ant researchers that the most effective traps use human urine.

And those ants, well, it's high nitrogen, high protein, relatively speaking, if you're a tiny guy.

So...

It's interesting why some species would prefer more of the other.

If you're putting on more biomass, like if you're making larger colonies, you probably need a lot more protein and lipids.

Whereas if you have longer lived workers, then you don't need as much lipid and protein, but you might use a lot of carbohydrates.

So that's sort of the fun way to think about the workers, like a car, like you need the metal to make the car, but then you only need gas.

So like you need the protein and lipids to make the ants, but then afterwards you really use more carbohydrates, even though they probably also digest other nutrients.

But I think the piece to connect would be like, find some,

student or person who loves meta analysis it could be us or it could be someone else and establish like which data sets are already there what's a recent impactful ants publication that already has done a lot of the meta and analytic work of at least compiling data sets and there are several ants databases that have compiled behavioral data morphological data ecological data genomic data and then um

how could that data format play into this model?

What are we going to put in?

And then I think a good step would just be to show like, what does the active inference ANT model explain, predict, suggest that other first principles approaches like reinforcement learning might not perceive?


SPEAKER_02:
Very nice.

So my question about the different types of food that ants eat was really linked to different pheromone

like types or ratios or concentrations like if i have a high preference for lipids but i'll eat sugar like you know maybe my ratio preference is different like this is another way to like enable different like types of ants or just to give them different preferences over the food that they eat and then where like if we initiate with the same kind of agent and the same kind of food do they all

like proportionately distribute or do some carbohydrate-preferring ants go to lipids and some lipid-preferring ants go to carbohydrates?

I just wonder, and like, does it have to do with proximity or the number of ants that were there before?

Or I think that all of that is interesting and kind of relates to

like this multi goal aspect of active inference.

That's always really kind of difficult to, I mean, it's, it's easy when we think about and get food, right?

Like, so the active inference model is, is easy.

It's manageable is what it is.

So it becomes computationally tractable and like mathematically tractable when it's a single type of agent with a single goal, or even like two different types of agents with two different types of goals that are overlapping.

I mean, so you can see just how exponentially,

expands when you have this kind of multi-goal thing and like some goals are like more pressing than others right like my immediate needs have to be met for me to like you know desire to go to college like I have to have food water shelter clothing etc and so these kinds of multi-goal and like prioritizing goal setups are um really interesting and a great way I think to start to

approach modeling real or us or complex systems that have multiple conflicting, sometimes conflicting goals.


SPEAKER_00:
Another setting to transpose and explore that where there's a ton of data would be first make active beference with the honeybees.

So there they don't use stigmergy on the trail because they're on the wing, they're flying.

But in the nest, you could have like in the hive, as it were, you could have a special dance floor.

And so that is where maybe like a active inference communication model could come into play.

so that this would be less on the stigma g side but more on like the multi-agent distributed communication side so flip out the trail um modification with a symbolic waggle dance whether it actually does a waggle dance or whether we just say okay there was a waggle dance that occurred and it had this precision and these estimates

Then honeybees are well studied for making the trade-offs between foraging for nectar, carbohydrate, pollen, which has like lipid and protein, and water.

And a lot of work on like, is it that 100% of the variance in that foraging outcome being different is about the needs of the colony and all workers respond to the needs of the colony?

Is it that the workers have their own nestmate level preferences and then the colony outcome just is what it is?

Or is it somewhere in between where there's preferences that differ between workers, maybe as a function of their genetic backgrounds, but then the colony outcomes are also in feedback with them?

So that could be a case.

And I think this kind of takes us back to slide eight, which is like when we have the systems lined up,

and we understand their similarities and differences, then, um, we'll be able to transpose across different systems in a really interesting way.

And then ask, um, it's kind of like biomimicry on behavioral systems.

Is this more like a honeybee situation where people are exchanging information and then going their own way?

Or is this more like a trail stigma G situation?

And, um,

So like there's gonna be some early work transposing into these major different domains.

Like again, this was the first paper that transposed active inference and included StigmaG, although that had been discussed by Axel Constant and others regarding ecology.

It had always been qualitatively a part of the discussion that niche modification was relevant and that agents were able to be in feedback with their surroundings.

And then it went from the natural language

through the formalism to the code.

And then now it can feed back, change how we think and write and speak.

And then we'll transpose into a few of these major domains, start to subspecify into situations and it'll be very cool.


SPEAKER_02:
So it's super interesting about like the communication versus like the trail, like which of these, like, and I just wonder what you think, like as humans,

which are we more like?

Are we more like bees or are we more like ants?

Like do we communicate and then go on our separate way or do we just like follow in a row?

Like where do you think we're at in the bee ant trajectory?


SPEAKER_00:
makes me think of a drive that we went on this weekend and um there was the street signs and then there was a big field and then there was the pleasure path that was cutting the hypotenuse across the field so it was like a big block with an empty field in it and so people were walking and so they were making that trail and so there was this juxtaposition between the um

societally imposed and scaffold infrastructure, like the streets and the symbolic signs, you know, the stop sign here, and this is where this street and that street intersect.

And then there's the people who were moving on that landscape.

And then even if you don't see any other people at that moment, it's their stigmergy with the trail.

And so it is an interesting question.

And it's almost like we have symbolic rules and scaffolds like law and norms and culture that are, even though there's ways to think about those as stigmergy themselves for the agent level, that's more like a symbolic communication.

And then it's like the ant stigmergy is like blurring the lines of

that the symbolism provides for us.

So I think there's, um, there's some components of both plus other insects, maybe not insects too, but that's quite contentious.


SPEAKER_02:
So that, that brings me back to like the concept of, of intergenerational stigmergy, but, but really like maybe my question was not like phrased correctly.

Like, sure.

We leave some stigmatic traces as humans for like,

the next generations, but also like we leave like stigmergy for strangers, right?

Like, so I wonder if like, and I know that the ant colonies, like the pheromones are different and the different species of ant, they're different.

And so I wonder, maybe is it like the fact that total strangers can go, you know, searching for collective intelligence or collective behavior or something like that on the internet and come to the same result or certain like total strangers see the stop sign

and know that there's some sort of behavioral norm that's supposed to be enacted just by seeing the signs in the street.

And so we leave this, like, you know, we do this externalization of cognition a lot as humans.

And so this is what I want to see in the animal kingdom.

Like, I want to see traces of like, can strange animals say like, oh, I see a rock stack.

I know the trail goes this way, right?

Like, does that happen in the animal kingdom or is that what's maybe uniquely human?


SPEAKER_00:
Good question.

And I think this is why we can study these systems and gain insight without naively copying their strategies or thinking that we must be them or they must be us.

Because

are, whatever it is that we're doing, it's at the intersection of like stigmergy and cumulative culture modification and symbolism.

So in a way, the ant and the bee foraging paradigms, hashtag not all ants because some don't do pheromone trails, et cetera, but one extreme is like the syntax of the foraging trail

and there's no symbolic communication.

It's just stigmergy.

And then the other extreme is no stigmergic modification.

There's no trace left in the environment at all.

There's just transient symbolic communication.

So it's this continuum from the transient symbolic

Like I'm going to pass you a message and then you're going to be off on your way to the stigmergic and often sub symbolic, like the path crossing that field.

It isn't directly a symbol saying walk on me, but it's,

it is an affordance.

We perceive it as an ecological affordance, but it doesn't have it.

That's not what it symbolizes.

That's what it is.

And that's how we enact it.

So like we could have some types of active inference models that highlight that transient among agents, meaning based communication versus models that don't need to take on any grandiose hermeneutic

communication and what does communication mean?

And how does thinking through other minds happen?

We just look to the ground and about how agents respond to their niche, which is a combination of biotic and abiotic.


SPEAKER_03:
Nice.


SPEAKER_02:
So yeah, just any final thoughts or, or any, anything else for me, I've given all my, all my thoughts out, I think.


SPEAKER_00:
Oh, me too, over the years.

But thanks for pushing for this paper for 29.

And 30 is going to be hot on the heels slash abdomen of this paper because I think we're going to be able to operationalize a lot of the discussions that we had about extended cognition.

I know it's also going to bring back

we talked about with the definitions of a self and individuality so um another one in the books on to our third decade of um papers on the live stream and uh yeah i hope people found it interesting as with other papers except this is one where hopefully they can get in touch with the authors and understand that it's a totally open area and a collaborative one too


SPEAKER_02:
Awesome.

Well, thanks for giving us your time and always giving us your time.


SPEAKER_00:
Thanks.

You too, Blue.

So I'll flip to the closing slide.

That's what we can still think about.

And awesome.

See everyone around next time.
SPEAKER_00:
All right.

Hello and welcome everyone.

It's ACT-INF Lab live stream number 46.0 and it's June 10th, 2022.

Welcome to the ACT-INF Lab everyone.

We're a participatory online lab that is communicating, learning and practicing applied active inference.

You can find us at links here on the slide.

This is a recorded and an archived live stream.

So please provide us with feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll be following video etiquette for live streams.

Head over to ActiveInference.org if you want to learn more about live streams at the lab or other projects that are ongoing.

all right we are here in the first discussion the dot zero for background and context in stream number 46.0 we are learning and discussing the paper active inference models do not contradict folk psychology by ryan smith maxwell ramsted and alex keifer

and the video just like all the videos are is an introduction and a contextualizer and an appetizer for some of these ideas it's not a review or a final word we're going to say hi and give introductions then we'll be covering the roadmap and abstract claims and aims and then we'll head through some of the core sections of the paper and just get some of the key arguments down and

during the dot one and the dot two in the coming two weeks we'll have open space for asking a lot of questions for those who want to participate live and ask questions as well so on we go

We'll start just by introducing ourself as much or as little as we'd like, and if we want to, mentioning something that we're excited about in the paper, like what brought us to want to contribute to this.zero.

And I'll start.

I'm Daniel.

I'm a researcher in California, and I think the title says it all, though there's still more to add, and it speaks to many of the hottest

and brightest debates in Active Inference, which is the rubber hitting the road with mind and brain and body and psychology and previous and different conceptions.

I'll pass it to Dean.


SPEAKER_02:
Thanks, Daniel.

I'm Dean.

I'm up here in Calgary.

I think

Well, a couple of things.

First of all, one of the first active inference that I was on was a paper that Ryan had written.

And so I was kind of impressed with that paper.

And I knew that if I sort of invested some time in this one, that I would become previous.

And the primary thing that came to my mind was looking at active inference from the decisioning and the movement standpoint.

sort of seeing that looked at from a higher order and the lower order type of processings.

And that, by the time I got to the end of the paper and started looking at the mind-to-world and world-to-mind piece and seeing where the

the potential connectors were between sort of coming into it, not necessarily knowing that there would be different things of active inference, much like I didn't realize there was an extended active inference until I read Axel Constant's paper.

That was kind of helpful and I'll pass it to Jacob.


SPEAKER_01:
Hi everyone.

I'm Jakub.

I'm a student from the Czech Republic and I'm excited to reduce my uncertainty about the

new formulation that was introduced in this paper, or at least it was the first time I came across it in the active inference literature.

As you mentioned, Dean, about the different higher order and lower order descriptions and how they conceptually map to the different mathematical formulation and also

to the full psychological ontology and overall just discuss what that means and what it might mean for other applications as well.

And I'll pass it on to Ryan.


SPEAKER_04:
Yeah, thanks.

So I'm Ryan Smith.

So I'm a research associate professor at the Laureate Institute for Brain Research.

um you know so i'm the the first author of this uh this paper so you know obviously i was sufficiently excited about the ideas to write about it um or sufficiently motivated at least to try to clarify some things that i um you know from my perspective are sort of common misconceptions um so um yeah i mean you know i'm i'm probably more gonna sit in the background here i'll just be here to um you know answer questions or clarify anything um you know if something comes up where

something in the paper wasn't sufficiently clear because I didn't do my job well enough.

So, yeah.

So I appreciate you guys being willing to talk about it.


SPEAKER_00:
thanks well an overall comment was we really appreciate the clarity of the argument and the writing and the weaving together of the the math and the formalisms with the argument the rhetoric and the ontology that's that's prime time so we'll just jump right in and of course anyone is welcome to share their comments so dean would you like to help us contextualize with a big question


SPEAKER_02:
Yeah, so I think that there's some questions surrounding, and I'm just going to take some of the important quotes that I lifted from the paper.

The concern that active infants also

psychology because they do not explicitly include terms for desires or other cognitive constructs at the mathematical level of description.

So given that distinction, that there are active inference models with motor control which need not have desires under folk psychology and active inference models do have desires within folk psychology then, the worry is that if those models can


SPEAKER_00:
Dean, could you, I think when you're talking, you like turn away a little bit or move back, it kind of cuts the audio.

So maybe, yeah, we'll try that.

Continue from the given.

Thank you.


SPEAKER_02:
Okay.

So given the, there's a distinction in active inference models of motor control.

which do not have desires under folk psychology and active interest models of decision processes, which do have desires with psychology, or so it's argued here, then the worry is that if active interest models can explain cognition in

without appealing to constructs that can be mapped onto the common sense notion of desires, and that's how I explain what that is, then this could be seen as threatening our intuitive folk psychology of ourselves as agents.

Such a situation would also pressure the traditional belief, desire, intention model of folk psychology that is prominent in philosophy.

The BDI model is a model of human agency that explains what it means to act intentionally.

So for me, I was introduced to the BDI model as well.


SPEAKER_00:
yes great and and one way that i kind of saw this big question even pulling back a layer is like is active inference recontextualizing reframing augmenting building on what is already familiar in a sense though there are many folk psychologies and i think that could be something we go into or is this the displacement of some

cherished framework for some other construct and the way that people speak like i want the cup of coffee or something like that are we going to need like a different word for an active compatible folk psychology and linguistics so jacob any thoughts on that well um


SPEAKER_01:
I guess one question I perhaps had was, even this formulation of the initial statement, given this distinction of motor control active inference and decision processes active inference, then we have this issue.

Well, one thing that I was a bit uncertain about was whether this distinction was made to fit the distinction that's within folk psychology, like there is some kind of ontology for motor control and an ontology for decision processes, so therefore we try to split active inference into these two parts.

or whether there is actually, whether this distinction follows directly from the same mathematical formulation of active.


SPEAKER_04:
Great.

I think one thing to clarify is I think the way that this kind of given the distinction followed by then here, I think could be leading to a little bit of a misunderstanding.

You know, so the

The idea that there's a concern about a threat to folk psychology comes not from that distinction.

It comes from the fact that at the mathematical level of description, it doesn't look like there's anything in there that's desire-ish, right?

I mean, that's where the worry comes from.

The worry doesn't come from the distinction between motor control and decision processes.

Right, the distinction between motor control and decision process is actually one thing that by making that more explicit, it actually helps to resolve the concern or it shows why it's not really a concern or that part of the concern stems from a failure to make that distinction explicitly.

So just to kind of clarify that.

So I think the order of the thought process here is a little bit different than the way it is in the paper.


SPEAKER_00:
thanks helpful times and we'll clarify and and go through the argument in order to so uh briefly just the aims of the paper as they present it are to provide a brief review of the historical progression from predictive coding to current active inference models

and show that despite a superficial tension when viewed at the mathematical level, the active inference formalism contains terms that are readily identifiable as desires and related cognitive constructs at the psychological level, which is downstream of that clarification of the distinction that Ryan just mentioned.

And then they discuss the additional insights offered by active inference and the implications it has for current debates about active inference.

Any other aims you'd want to add, Ryan?

I think this is fine at the moment.

I mean, I'm sure things will come up.

Great.

And then claims.

Jakob, could you read the claims?


SPEAKER_01:
Yeah, so firstly that the apparent problem posed by purely doxastic-looking constructs simply is not a problem.

There do not appear to be cases where the phenotype-consistent prior expectation in DAI, often called prior preferences, will ever conflict with or make distinct predictions than a traditional folk psychological account in which beliefs and desires are integrated to find intentions.

The second claim is that what we have referred to as DAI, the Partially Observable Markov Decision Process Formulation of Acton, is a corollary of the FEP, and it can be implemented using prediction error minimization, but there are many other aspects of the FEP and many other theories that fall under the umbrella of predictive processing.

And in some, there are beliefs and desires in the active inference framework.


SPEAKER_00:
And those are just a few claims that we pulled out, but many other declarative sentences will also be claims.


SPEAKER_04:
I wouldn't necessarily call those the primary claims of the paper.

I mean, there's certainly statements that we make, but I think the idea that the apparent problem isn't a problem, I think that's a claim.

And that there won't be a conflict with

with an account where you're combining beliefs and desires to form intentions.

That's true.

This idea about what falls under the predictive processing umbrella, I wouldn't necessarily say that's something we're arguing for in the paper.

It's just something that matters when you're trying to correctly frame this kind of debate because predictive processing is just a very

generic term, right?

Like it doesn't refer to any particular mathematical formalism.

It just refers to a really broad idea that in some way the brain's doing some sort of predicting, either with respect to resolving problems in perception or problems in decision-making and motor control.

So there really isn't, there's not enough, there's really just not enough mathematical specificity associated with predictive processing as a term to really even test any predictions that it would make.

Right.

So, I mean, I mean, what you, what you need to do is pick, you know, whatever specific mathematical formalism, what actual hypothesis you're talking about.

Um, and you evaluate and test claims with respect to that.

Right.

So there's, there just aren't, there's aren't clear, uh, like what even, uh, it's just a pretty good processing is just too general and vague is the point.

So, you know, we can only really say, look under,

under the current models used that are called active inference, right?

So these, you know, partially observable markup decision process models, you know, that minimize expected free energy as a way of making decisions, right?

Those arguably count as one particular, right, theory or class of models under the predictive processing umbrella.

And we can evaluate claims with respect to that model.

But, you know, what's true of that doesn't need to be true of, you know, the other

you know, 20 things out there that might, you know, might also fall under a predictive processing umbrella.

So, I mean, the main point is just we need to evaluate, you know, claims and predictions and things like that with respect to a specific model, not with respect to a kind of vague general category of models.


SPEAKER_00:
Thank you.

Okay.

So just rapidly through the abstract.

Active inference offers a unified theory of perception, learning, and decision-making at computational and neural levels of description.

In this article, we address the worry that active inference may be in tension with the belief-desire-intention model within folk psychology because it does not include terms for desires or other cognitive constructs at the mathematical level of description.

To resolve this concern, we first provide a brief review of the historical progression from predictive coding to active inference, enabling us to distinguish between active inference formulations of motor control

MAI, which need not have desires under folk psychology, and active inference formulations of decision processes, DAI, which do have desires within folk psychology.

We then show that despite a superficial tension, when viewed at the mathematical level of description, the active inference formalism contains terms that are readily identifiable as encoding both the objects of desire and the strength of desire at the psychological level of description.

We demonstrate this with simple simulations of an active inference agent motivated to leave a dark room for different reasons.

Despite their consistency, we further show how active inference may increase the granularity of folk psychological descriptions by highlighting distinctions between drives to seek information versus reward.

and how it may also offer more precise quantitative folk psychological predictions finally we consider how the implicitly quantitative components of active inference may have partial analogs i.e as if desires in other systems describable by the broader free energy principle to which it conforms here's the roadmap so

the dot zeros, all of them in the world, wouldn't be enough to hit every stop.

So we will go through roughly in order of these sections.

And the section titles are listed here.

But Ryan, what was the thought going into the ordering or the structuring and why there was such a comprehensive historical and preliminary consideration section?


SPEAKER_04:
I mean, a lot of

I mean, the vast majority of these sections are just kind of building up background, right?

I mean, sufficient background to kind of see, right, where the, you know, both I think historically where, you know, potential misunderstandings might come from, you know, and also just enough of the, you know, providing enough of the formalism, you know, one part of the formalism does kind of build on another, right, to see why the, you know, to see why the apparent problem just isn't there.

Right.

So, I mean, the vast majority of these sections are just are just preliminary, just kind of building up so that the reader has the information necessary to understand the argument.

I mean, only only the last couple of sections that really really are the meat of the argument itself.

I mean, so so, you know, for example, just section two, you know, there are what we just said from the predictive coding active inference is just kind of highlighting how, you know, these initials of.

brain as implemented through predictive coding was, you know, purely a model of perception, right?

So then, you know, so then when people, but a lot of times you'll see, especially in the, you know, older literature and philosophy literature, you know, predictive processing somehow both refers to predictive coding, but then also is talked about as though it's as though it's the same thing as active inference, you know, or there'll be these kinds of ideas where

if you just extend predictive coding as kind of the way it's talked about as a kind of way of thinking about what the whole brain does, you know, then all of a sudden, you know, controlling the body ends up also just being something about predicting what the body will do.

But, but, and especially when described that way, right, it sounds like something like desire and motivation and goals and things like that are just kind of completely out the window.

And it's very hard to make sense of what,

how a system like that would even work.

Right.

Um, and so it's, and, and part of that I think is because there's, you know, some, um, you know, some, uh, the way that certain things are written in some of the literature can be a little bit, um, confusing or it's not too, it's not too hard to understand why, you know, these sorts of, um, you know, these sorts of misunderstandings could, uh, you know, could happen.

Um, but, but, uh, but the, you know, section two, there was really,

really is to show how, you know, initially, right, when you're trying to move from predictive coding, again, which is purely a model of perception, it's not about decision making, it's not about action selection, you know, it's not even about motor control, right?

It's just a Bayesian, just a theory of how the brain can do approximate Bayesian inference and perception.

You know, how that was initially just extended to say, hey, like, if the brain works this way, then, you know, how do you get a system that actually controls the body using the same generic architecture?

Right.

And so then, you know, the story that Carl Friston, you know, um, and, um, you know, some other people started trying to put together was the story about how you can use predictions, you know, descending predictions, um, if weighted appropriately, right.

As a way of essentially setting the target states of the body, right.

Like, like, is this kind of, you know, the way you'll see it talked about is, you know, I predict that my arm will be here, but really it's down here.

But if I wait that correctly, then it'll kind of like,

know you'll kind of control the set point and reflex arc to you know so your arm kind of moves up with the correct position right but but that's just a theory about how you can use a descending prediction signal to essentially act as a motor command signal right there's nothing in that at all that that's deciding what that prediction ought to be right there's nothing deciding what that what which motor command should should be the one right that's getting sent down in in a you know in in the

with the form of a prediction, right?

So, so again, so even there, it's just a theory about how you can use predictions as motor commands, right?

Like there's no, there's a very big distinction between that and whatever the system is on top of that, right?

That's deciding, okay, what motor command, what, what motor prediction, right?

Do I send?

And so just making, you know, making, making that clear, right?

Because around 2015, there was this big shift, right?

From these, from these,

where from active inference being talked about as a, just kind of a motor control process, right?

Like a story about how you can extend predictive coding to, you know, also do motor commands, right?

Like from that to, you know, these larger, more comprehensive decision-making models, which are the current, right?

And those, you know, which was what we aim to show very explicitly, you know, have something that, you know, in the mathematics,

It just does, right?

It specifies the goals of the system.

It's pretty hard to get around the fact that you need something, a system needs something like goals, right?

Because it has to evaluate somehow why one decision is going to be better than another, right?

The only way it can really do that is with respect to how likely it is to get whatever the system wants.

So it's just, I mean, in my view, it's very difficult to see how you could ever get

a system to be able to evaluate one action as better than another without having some sort of target state.

And so you can call, right, like the target states in active inference a type of prior belief.

But at the end of the day, its functional role is to specify what observations are better than what, right?

So, I mean, you can really think of active inferences as using this kind of trick in a certain sense, right, of specifying

desired outcomes in the form of a probability distribution because that helps keep everything fully Bayesian, right?

But it's not playing the functional role of belief, right?

It's encoding higher probability just means, right?

Like the better, more rewarding outcome.

So a lot of it is just kind of working step-by-step through both kind of the historical progression so you can see where the misunderstanding could come from

and then also through the current formalism so that you can see exactly where the goal states are encoded in terms of something with the form of a probability distribution.


SPEAKER_00:
Thank you.

The keywords are active inference, poke psychology, predictive processing, Bayesian beliefs, and desires.

So now we're going to jump in.

And some of the slides have a lot of text topically arranged.

So we won't need to read all of the text on many of the slides.

But especially if somebody wants to pull out one of the highlighted sections and bring it to our attention, that would be awesome.

So, and also Ryan, thanks a lot for that great historical overview.

And I think there's so much more to be going into about what happened before 2015 and what happened in the last seven years.

In fact, I think you covered many of the key points in what you just described here, which had to do with the development of predictive coding framework into understanding brain and body.

Is there anything that anyone else wants to add about these points?

Great.

All discussions and dot zeros are like a two-way street where some people with active inference familiarity are learning about new ideas and frameworks, and also people who might have familiarity with a broadly used area outside of Act-Inf are learning about Act-Inf.

For both directions on that freeway, it's important to understand what is the target non-ACT-INF framework that's being juxtaposed and found concordances with active inference.

So the article is addressing the worry that ACT-INF models may be in tension with the belief-desire-intention model.

And there are some consequences to that worry.

In other words, given some priors, it's a founded worry.

Would anyone like to summarize what the belief, desire, intention model is?


SPEAKER_02:
I think you should, Daniel, because I think you put this slide together.


SPEAKER_00:
Ryan, what led you to select the BDI model?

And I wondered about almost this tension or paradox with like, it's folk psychology.

It's about what the people think, but we're gonna do a citation and one specific academic acronym for what people think.

So is this like the main game in town for folk psychology from an academic perspective or what is the BDI and what led you to select that?

rather than a small portfolio of alternate folk psychologies?


SPEAKER_04:
I think that the belief-desire-intention model is just a very generic but also widely known account of trying to capture folk psychology.

It's a major target of a lot of discussion within

you know, like philosophy of mind.

But I mean, it's just very generic, right?

I mean, it's just this idea that, you know, what are the necessary ingredients to make a decision?

Well, you know, I have to have some beliefs about how the world is and I have to have some desires about how the world I would like it to be.

And my intention involves integrating those two things, right?

And then if all goes well in terms of translating intentions into controlling the body, then I'll act out, you know, something according to my intentions

Right.

I mean, simple, you know, like the dumb example that you'll often hear is just, you know, something like, you know, I desire some ice cream, you know, I believe that there's an ice cream truck, you know, down the road, you know, given those two things and, and necessarily those two things, right.

I can inform and touch it and form an intention to go, you know, walk over to the ice cream truck and buy some ice cream, you know?

So, I mean, the question is how else, you know, are you going to explain,

why I went over to the truck to buy the ice cream.

That literally just is, you know, that is the general sort of thing that when we're reasoning about other people's, you know, decisions, other people's, um, you know, behavior and we're trying to figure out, okay, why did this person do what they did?

You know, if we're assuming it's a voluntary action, right.

If we're assuming it's not because like their arms spasmed or, you know, because they have some like really highly ingrained habit or compulsion or something like that.

Right.

And we're talking about voluntary behavior.

Um, that is just the way.

that we tend to reason about how people do what they do.

Either when somebody does something that we think is weird, then usually we have to explain that in terms of, okay, well, they just believed something that was incorrect.

Or they had some really funny desire that I don't relate.

It tends to be one of those sorts of things.

I don't know a lot of other separate

uh, both psychological models that would, uh, entail anything different than this, um, you know, at, at a very generic level.

Um, and it's also exactly the sort of thing, right.

That, that people try to contrast active inference with where, where instead of, you know, belief desires, beliefs and desires coming together to form intentions, instead it'd be something like one kind of belief and another kind of belief come together to form an intention.

which again, for a lot of reasons is confusing and rightfully so when it's presented that way.

Thanks.


SPEAKER_02:
Yes, Steve.

So Ryan, when we've had some conversations with other authors and other papers where we've talked about the scale three formalism, and we've also talked a little bit about the sort of scale friendly times when you have to find that history.

and that context.

What you just mentioned was sort of the generic, generalized sense that the BDI umbrella provides.

You're not saying that scale-free quantification and a generic way of modeling something are the same thing.

But what I think, without putting words in your mouth, are you saying that the scale-freeness, as it moves to something more

scale friendly sweeps up some of that generic big tent idea or model and now moves it along it allows you to sort of parse and be specific and precise precise but also be able to generalize as you move through those you know quantifications of the distribution and density and such because that's kind of what i was reading into it i just want to make sure that i got it i was kind of maybe you can help me clean that up a bit


SPEAKER_04:
I mean, I really don't know that anything that we're saying really depends on or even gets into a lot of those sorts of specifics.

I mean, really, really all we're saying is, is that, you know, if you want to take a model, like the current active inference formalism, the current kind of like vanilla formalism, right?

So in terms of interest to standard one level POMDP, or, you know, if you want to scale it up hierarchically, it doesn't ultimately matter.

It is if you want to use

the vanilla active inference framework to actually model some sort of voluntary decision process in a human, right?

So, I mean, a lot of my work has to do with people, you know, modeling behavior on decision-making tasks, right?

Often have to do with, you know, the person, the person has the goal of maximizing, you know, how much money they win or, you know, maximizing some sort of social reward, something like that.

The idea is just that when you're trying to, if you're going to take an active inference model and you're going to use it to model,

successfully model actual human behavior when they're making decisions, then there's always going to be a mapping between the different elements in the active inference model of voluntary behavior that whatever the prior preference distribution is, you just check that so that it just shows the level of reward associated with it.

So there's really nothing about

any scale or anything like that this is much more generic it's just if you're going to use active inference to model voluntary choice then um it will always have something analogous to a belief and a desire because that's just what you need right like to model to model voluntary behavior um it's true that you know that's probably if you're if you actually were going to try to capture something uh kind of like more of the actual architecture in the brain as opposed to just

this kind of like voluntary decision process aspect.

I mean, I'm sure there's a lot of kind of hierarchy below that under the hood, right.

That's translating more abstract policy selection processes into whatever the dynamic signals are that, that end up controlling, you know, moment by moment muscle movements and things like that.

Uh, but the point is, is the below, below this kind of top level where you're doing policy selection, um,

you know, at that point you don't really need the desires anymore, right?

You just need the policy to specify whatever the motor commands are going down that can take the form of predictions.

Um, so, so I don't, I don't know if that helps, but it's just, it's just that there's a certain level in any kind of hierarchy of a model that's going to actually be applicable successfully to human behavior.

And, um, at the level of policy selection, um,

Prior preferences will just include the desired outcomes.

They'll just encode the goals that the agent's trying to reach.

And the policy that's chosen will be evaluated as being the most likely policy because it's the thing that's predicted to have the highest probability of generating the desired observation.


SPEAKER_01:
If I may also comment on that, and I'm not sure if this is answering your question, Dean, but the way I thought about it when I was reading the paper was that

Part of the reason why the belief-desire-intention model works well for mapping the active inference ontology to the folk psychology ontology is because it's a discrete model in that it separates these different parts.

And I think that also might have motivated the choice to consider mainly discrete POMDPs and describe the decision active inference in that way as well, because then this discrete model can be more easily mapped to, say, the discrete model of the POMDP that's described like a factor graph.

Um, I'm not sure.

I was wondering how this would map to continuous tasks as well.


SPEAKER_04:
Well, I mean, I mean, I think the, I mean, part of the, I mean, uh, you know, part of the, part of the reason why you need something discreet, right.

Is, is because actions is discreet, right.

I mean, there's either one or there's policy two or there's policy three.

Right.

So those, those just are sort of necessarily different.

right?

I mean, you get, you know, continuous state spaces work well or are appropriate, you know, at lower levels in a hierarchy, you know, when you're talking about kind of dynamics and motor movement, you know, dynamics in the set point of some reflex arc or when you're trying to estimate something in perception that, you know, just as a continuous quantity, right?

Like brightness or, you know, orientation or things like that.

But at the level of decision-making,

Um, models that are necessarily right.

It's free.

Um, I mean, I mean, there is, you know, there are, there are kind of hybrid models, right.

That are out there in the literature, you know, like Thomas Parker, for example, you know, has published several papers using these sorts of models where the kind of policy at the higher level, which is, which is an industry model, um, generates right.

Some observation that then sets the set point, right.

For some continuous or some continuous lower level model.

that then, you know, can end up in a, in a way that's kind of like this kind of story, you know, like move the eyes around the God to different locations and things like that.

Um, so, um, so I, I, I kind of think that, that the, the discreteness is the, or the, the, the reason why, um, things move to a discrete state space architecture is just because of like the necessary, um, what's necessary with respect to a model of policy selection.

because it is kind of all or none.


SPEAKER_00:
Just to keep the .00-ish, we're going to move a lot faster through the following slides.

And there's many important questions in the chat and also arising.

So we're just going to carry on more rapidly so people can...

pick up on these key points in the zero and we'll have a lot of time to explore soon.

Section two, from predictive coding to active inference, traces the history and the development from various fields.

And just one sentence here that Dean and I both highlighted on, and then Dean, I'll let you describe the button, was that

Crucially, for the purposes of this paper, the first generation active inference was not a theory of decision making.

It did not explain how we decide or plan where to move our body.

It only explained how body movements can be executed using the predictive coding apparatus once a decision has been made.

So how did you connect that to the bottom right?


SPEAKER_02:
Yeah, so if you're on some sort of a website and you have to pass through to something, my question was based on that statement, when the expectation is to know that you've actually crossed a threshold, you've moved beyond one page and there's an expectation that there's something that you validated or confirmed or whatever, is the feedback that you received, does that happen?

how does the person who's actually expecting that they're on the other side, what kind of feedback mechanism do they need for that confirmation?

Is tactile feedback enough?

Do they need some sort of a visual confirmation as well?

Like what is the, what's the,

Is there, because we're talking about thresholds, is that different for every single person or is there sort of expectations built in depending upon the kind of situation that you're dealing with?

I do know that with the idea that like changes are saved or links are copied, sometimes the click isn't enough to give people confidence that in fact that process is carried through.


SPEAKER_04:
I mean, I would say that, I mean, a lot of those questions are really empirical questions as opposed to modeling questions.

I mean, in terms of how you would model that sort of thing, right?

You just specify what observations count as tactile, you specify what observations count as visual, and what ends up being thresholds for sufficient evidence and things like that just have to do with the way that processes just have the dynamics naturally unroll in the model under whatever the model parameterization is.

So you'll hit a threshold faster if the mapping between

whatever observations and states is more precise.

For example, I mean, there's, you know, in relation to that, when we're doing policy selection, right, we also have to check and see whether things are actually going as we expected them to go, you know, under a choice of policy, right?

So it's possible that you choose policy one and you start to get the subsequent observations and they don't actually match what you expected, you know, given that you had chosen policy one, you know, in which case,

you'll update your beliefs through perceptual inference, and that might influence how you act going forward.

So, but the specifics about, you know, what's enough and whether it's visual or tactile or anything like that, I mean, those are really just empirical questions that would have to be answered in studies as opposed to something about model choices.


SPEAKER_02:
Okay, so there was really not a good enough thing when you did the darkroom aspect of it?

Like the risk that you were prepared to take on, right?

Like somebody was prepared to take on the risk because they really urgently wanted the ice cream versus the person who was like,

eh, I'm agnostic, right?

So that's maybe, that's kind of what I was, because this was on the second reading of your paper, right?

So I was kind of now backfilling from further down in the paper.


SPEAKER_00:
Well, I'm into that.

We'll go in the order of the paper because we're mentioning things that we haven't brought up yet.

But let's continue on and we'll return to that.

And Ryan, I also agree that in any specific case, it's going to be a model parameterization.

Some of these are very difficult to...

answer in the abstract model selection setting um here the uh distinction between motor active inference and decision active inference is introduced and um I'll allow Jakob to just convey one pass what is the difference between motor active inference and decision active inference


SPEAKER_01:
So I'm not sure whether I can say much that hasn't been said yet, but briefly, the motor active inference does, well, as already mentioned, it does not consider desires in both the active inference and folk psychological sense as was then connected in the paper, but

Also, it is my understanding that the motor control version of active inference is modelled continuously, like the models of the movement of individual muscles, which does not entail decision-making, so it's modelled with continuous time rather than in discrete time.

And the decision-making act in, as Ryan already said as well,

is all about decision-making.

So it describes the discrete process of decision-making with prior beliefs and preferences.

And it can be modeled at different levels of cognition as well, which is one thing that I'm still a bit uncertain about how we can move through these different layers while still keeping the same mathematical formulation.

but I think we'll probably get to that in the other slides.


SPEAKER_00:
Yep, we're going to continue with this distinction of MAI for motor active inference and DAI for the decision active inference.

Here, we're showing some key sections from the paper and some citations.

i think we will continue on without going into this in depth but it describes some of the specific model quantities that are being described in active inference formalism and we're going to come to them when we look at some equations in the coming slides um in section three preliminary considerations

Two broad points are introduced and we might show a second one on a later slide.

How can we think, Ryan, about what is said here that decision-making AI models are largely taken to describe sub-personal, non-conscious processes?

And also it was asked in the chat, not that we have to address it now, by Duvid, has active inference produced a single reasonable model of a qualia?

So when we're thinking about this first broad point that was made here, where is experience and what is the distinction between the personal and the sub-personal in DAI?


SPEAKER_04:
Well, I mean, so there's a couple different questions you asked there, and I don't necessarily think they're synonymous.

So, you know, when you said, I think the first question you asked was something about, um, uh, has active inference provided some sort of, uh, explanation or, or model of, um, of qualia that would be satisfactory, um, in the strong sense.

So in the sense of the sense of quality as an explanation for like the hard problem of consciousness, then I think active inference is in the same place as anyone else in the, no one has a good explanation for, for, you know, how to deal with the hard problem of consciousness.

Right.

I mean, like,

massive literature out there, but I think everyone thinks it's just as mysterious as ever.

So no, right?

I mean, on the other hand, I mean, you know, myself and others, you know, have published multiple papers showing how you can successfully use active inference models to model conscious access processes, right?

So the distinction between when the brain is representing something unconsciously versus when it's representing something consciously, you know, in that it can, you know, self-report

right, what it's representing, um, can use the information to make voluntary choices.

Um, and we've shown that those sorts of models are able to, um, you know, reproduce, uh, like empirical results and like EEG studies and fMRI studies and things like that.

Um, that, and that also can make novel neurophysiological predictions.

Um, some of which seem like they, um, you know, in some subsequent work, uh, you know, have, um, like empirical support for those predictions.

So in terms of providing what seems like a useful account of the processes associated with what people self-report that they consciously experience and the brain basis for that, I think active inference has at least the starting point for being useful and explaining those sorts of things.

But why one little posterior inside a model versus another posterior in a model actually corresponds to like,

the experience of red versus blue or something.

I mean, no, I don't, I think active inferences is not any better than anyone else, you know, any other model for that question.

I guess the second thing, you know, that you asked had to do with this kind of like related question of, you know, why it's the case that one level has to do with a conscious process versus unconscious process.

And the best I really got there and, you know, what falls out of the models that we've, you know, shown.

really just has to do with temporal scale.

You know, so there will be a certain level of temporal representations in a hierarchical model that integrates enough and represents regularities over a long enough time scale that it can contribute to, you know, sort of prospective, it will be sufficiently compact to generate things like self-reports, right?

So, I mean, think about how complicated and temporally extended reporting and choosing to report something like, you know, I see something green and

me, right?

I mean, it's a very complicated and really deep, um, you know, type of policy to select and it requires integrating a bunch of information from stuff at lower levels that's happening over faster timescales.

Um, so at a minimum, right, you just need to be at a level of representation where the regularities are sufficiently, uh, sufficiently long and, um, especially temporarily deep and then have access to all the relevant information you need to integrate to be able to generate, um, reports like that.

Um,

Whereas much of the other kind of little pieces in the model, right, like, you know, the generated, say, like, expected error for, like, variational free energy gradients and things like that, that, you know, people might call, like, surprise, right?

I mean, no one's claiming that, and often they will not relate in any way to people's conscious reports about feeling surprised, because conscious reports about feeling surprised will have to do with representations of surprises in states, right, as opposed to, you know, some sort of prediction error, like,

you know, gradient minimization process.

Thanks.


SPEAKER_00:
Also in section three, there are several clarifications and some seeds that are planted, namely if computational and folk psychological predictions converge,

and no other available theory can account for behavior equally well this could entail the mathematical structure of dai as more than a convenient tool instead that it corresponds to the true information processing structure underlying and enabling folk psychology and related abilities and this is really nicely put here that the crucial point remains that one should not conflate mathematical and psychological levels of description

However, DAI models might nonetheless offer more detailed information about the true form of folk's psychological categories and processes.

And then they add that their aims are to demonstrate that there's a clear isomorphism between the elements of DAI models and those of the BDI model, and then show how

provided one does not assume probability distributions in computational models must be identified with beliefs at the psychological level there will be no tension between dai and bdi and that comparison or juxtaposition is clarified by separating mai from dai like has been mentioned anything else before we start to jump into some of the formalisms and keeping all of this in mind


SPEAKER_01:
I guess I had one question on which ontology we're working with when we say there's an isomorphism between the elements of DAI models and those of the BDI model.

because I think there are multiple ways to interpret this.

One thing that I'm wondering about is whether this means that we can basically construct a mathematical formulation of the BDI model that is like a subset of active inference, or maybe there are certain elements of the BDI model that aren't necessarily

encapsulated with enactant that weren't discussed in this paper and what that would mean in terms of formulating this mathematical formulation of the BDI model.


SPEAKER_04:
So I mean what the isomorphism just means is tell me the description at one level and I'll be able to translate it for you very quickly and directly into whatever the description is at the other level.

in both directions.

Right.

I mean, so it just means, you know, tell me, tell me what, you know, tell me what it is that you want.

You know, if I say, do you want ice cream or do you want a pizza?

Right.

Tell me, tell me that you like, if you tell me that you like, you want pizza twice as much as you like ice cream, then it's very easy to just put, you know, a four, you know, in the, you know, in the, in the distribution in, you know, in the preference distribution over the observation of pizza and the two for ice cream.

And then softmax it and log it.

Right.

And then like, you know, there's that's the description, right.

So just in a very direct way, it just encodes relative desires for one thing versus another, you know, tell me what your beliefs are.

Right.

I mean, do you believe that pizza is to the left or to the right?

I can put that in as the different categories of hidden states that you're going for a distribution over.

Right.

So like, it's just, the point is, is that in either direction, right.

Give me the full psychological description.

I can translate it very straightforwardly using the same elements every time as being desires or beliefs, you know, give it to me at the mathematical level.

I can translate that into a description in terms of beliefs and desires, right?

I mean, it's just one to one.

I can't think of an example offhand where there would be any kind of like mismatch or inability for one to account for the other, but

Um, you know, it's kind of hard to prove a negative, right?

So, I mean, maybe you can give me an example, but I can't think of one.


SPEAKER_00:
Awesome.


SPEAKER_01:
Yeah.

I was just going to say that I can't really think of an example, uh, right now.

I just found it really interesting with the usage of the word isomorphism, because as you said, it's just a bidirectional and subjective map, so it's basically equivalent, it's just set in a different way.

So I can imagine that, uh, meaning that we can basically, instead of the Wikipedia page for the BDI model, uh, using all of these terms, um, like believe, desire, intention, it could just be a set of active inference equations.

And that's the BDI model because it's an isomorphism.


SPEAKER_04:
Right.

I mean, literally your, your beliefs are just your cues, right?

And you're, uh,

and your desires are just your, uh, you know, P of O's, right.

I mean, and, and, and, uh, you know, the way that we've, uh, you know, we and others have tended to write it now, right.

It's to literally, it's just explicitly to say like that desires are, um, or the preference distribution is P of O given C, right.

Where C is just a matrix that defines your preferences, right.

So, I mean, it's a lot clearer to do that, um, than to just, just leave it just P of O, right.

I think that's part of the confusion is, is, is that, uh,


SPEAKER_00:
not clear where the preferences come from well but they're just it's just because they're conditioned on this other thing c and c can be fixed or learnt but that's the basis of the calculation there um yeah here we get into section four variational and expected free energy considerations and in large script we see the variational free energy equation

Dean, what is the skydiver doing?

And during the one and the two, we'll unpack more about the exact terms and what is coming into and coming out of this equation.

But just as an appetizer here, what's the skydiver doing?


SPEAKER_02:
Well, now that I've got the author here, I just put through that in there because I like the...

I don't think it's a metaphor, but the analogy of, of how, so how do you respond when you jump out of the airplane and you know, your parachute's not going to work?

Do you, uh, do you go ahead first and really attack it or like what now, what are you doing in terms of what your expectations are?

So maybe, maybe that, that, that was a good analogy, Ryan, maybe, but what, like, so where were you, where were you going with that?

Cause yeah.


SPEAKER_04:
Well, I mean, I think the point, the point of those sorts of examples are just, uh,

just to, um, sorry to put this to, to show why it's not, um, it's not really correct to think about the, um, like prior, uh, you know, this prior preference distribution, even though it's, you know, it's again, it's cast as a prior belief.

Right.

Um, it's not really appropriate to think about that as a, as a, um, uh, a belief per se.

Right.

I mean,

the, um, or an expectation at any, at a psychological level of description.

Um, so you can, and the way to contrast that, right.

As you can say, look like the person in a model where they don't want, right.

To fall to the ground and die.

Right.

That would be your, that would be your PO though.

Right.

It's your preference not to have the observation of hitting the ground and dying.

Um, but what you act, right.

Is your Q of O given pi, right?

It's what do I expect to observe given that I choose this policy versus that policy, right?

So that's the, you know, the belief-y part, right?

It's what I expect given that I do this or that.

And, you know, how close is that to how much does that diverge from what I want, right?

Which would be your P of O. So it's just a clear example of how your belief-y expectations and your desire-y expectations come apart, right?

And it just doesn't work conceptually to think of one as like the same kind of thing as the other.


SPEAKER_02:
Yeah, I just saw it as if I'm not going to get a soft landing, how big of a crater can I create?


SPEAKER_04:
Well, that then is going to be packed into your preference distribution, right?

You would have some desire to make a big conditional on you're going to die, right?

Like you'd prefer to make a big crater than a small crater.

and that would still just go in your P , right?


SPEAKER_01:
Yeah.

Jakob?

I was just wondering, and this might be me thinking too deep, or not too deeply about the equation, rather, but in this formulation where it's a variational free energy for each policy f ,

Does that imply, in the last sentence on this slide, that free energy is a functional of beliefs and a function of observations?

Does that mean that the free energy is essentially a function of three variables?

But in this case, we are only writing it f because it's clear that it's a functional of beliefs and observations.

Um, or should we think of it more as just a function of each policy?


SPEAKER_04:
Um, I mean, the way it's written here, I mean, I think that the most straightforward way to think about it is, I mean, it literally just means that, um, using this equation, you will calculate a different F value for each policy.

Right.

I mean, that's really it.

Like you just, there will be, you're going to be computing.

some some variational for energy value for each policy, and that's going to make up a distribution over policies.

And and I mean, the the tricky thing, I guess, here is, is that, you know, this isn't about this isn't, you know, f of f of pi here isn't about making decisions, right?

This is just about, you know, something like evaluating evidence for, you know, for various policies, because you you can't you calculate f when you already have the observation.

Right.

So you can't really do this perspective decision-making thing with that.

Um, so this is just basically a way of describing how, um, how under each policy, your beliefs will change, um, or wouldn't change right under, you know, when you get conditional on each new observation that you got.

Um, and, um, and so it's not, uh, yeah, so all, all it denotes really is, is just, you know, you have a value of F for each policy and that makes them a distribution.


SPEAKER_00:
And that takes us to G, expected free energy, the letter after F. And as shown here, not sure if there's another reason for why it's G. This is equation 2.6 from the Active Inference textbook, just for reference with some other versions of how G is framed.

G is a prospective value, and they write, decision-making does not only require beliefs about past and present states, it also requires making predictions about future states and future observations.

This requires taking the average, i.e.

expected, free energy, G of pi, given anticipated outcomes under each policy that one might choose.

And there's a lot to say on expected free energy.

What is just one short note that somebody could add?

Or Ryan, why was it placed here?

I'm not sure I understand the question.

Why is expected free energy required?

Why is variational free energy not enough?


SPEAKER_04:
I mean, expected free energy is just what the system is trying to minimize when it's selecting a policy, right?

So I mean,

it's just trying to, I mean, in this, I mean, this decomposition right here is often called like the risk, risk plus ambiguity decomposition, but, um, I guess, uh, I like, I think that, I think that showing the, um, you know, the risk term, the risk term here provides a nice kind of intuition for, um, for again, the separation between beliefs and desires and active inference, because, um, you know, your, your, uh, so the, the,

the term underlined in red there, right?

So the kale divergence between PO given pi and P of O, right?

I mean, that's just saying that, you know, how different are the observations I expect given that I choose a given policy and, you know, how different is that?

How different do I expect that to be from my preferences or my desires, right?

P of O. So the

the closer those are together, right?

The more similar I expect the observations to be under a policy to my preferred observations, the smaller that's going to be.

And so if I'm trying to select the policy that minimizes G, right, then that means I'm going to select the policy that's going to get me as close to my preferred observations as possible, right?

So literally just is that KL divergence literally just is

trying to choose the policy that brings beliefs closest to desires.


SPEAKER_00:
Very nice.

And then what does the blue term mean?


SPEAKER_04:
Um, that's just an entropy term, right?

So, I mean, it's just basically saying that the, it's what drives information seeking or part of what drives information seeking.

It's just, uh, it's just, yeah, basically the agents choose, you know, driven to choose policies that also are going to generate, um,

more precise expected observations that are going to disambiguate states more easily.

Great.


SPEAKER_00:
Here we'll explore it more in the dot one, but there's a description of the posterior probability distribution over policies, p , and an expression involving the softmax symbol with a sigma.

We'll return to that later, but this is just so that we can cover the key experiments and the simulation results today.

This is also one other area of formalism we can explore in the coming discussions about precision and the relationship of beta and gamma and what the convergence towards means.

But we won't go there.

There is precision parameter in this model.

let's go to the dark room problem and many lines of pdf screen and ink has been written on the dark room in the active world since um perhaps friston at all 2012 and also outside of active and um they write in a nutshell the concern is that if agents only act to minimize prediction error

as opposed to acting under the impetus of a cognitive desire-like state, then they ought to simply seek out very stable, predictable environments, such as a dark room, and stay there.

Would anybody like to add anything about this dark room problem, or what is the relationship between doxastic and cognitive ontology and act-inf?


SPEAKER_04:
I mean, are you just asking what those words mean?


SPEAKER_00:
Yes, how do you use these words?


SPEAKER_04:
I mean, doxastic is just a term that refers to the leafy, like epistemic sorts of things, right?

So a doxastic ontology would be an ontology that includes the leafy things, right?

Whereas conative just refers to being targeted towards something, right?

Like something about

you know, desires or, you know, something, things you want versus don't want, things you like versus don't like, right?

So it's just, you know, when we say it's apparently purely doxastic ontology, it's just saying it might look as though superficially like the ontology proposed in active inference, like the things that exist in active inference are purely belief-y.


SPEAKER_00:
That's all that means.

Great, and it's applied in the setting of this dark room.

For the following two slides, Dean help us understand what concerns are addressed and what the child on the top right of the slide is doing.


SPEAKER_02:
Yeah, well, so I'm not going to go through all of the text, but essentially this was my introduction to the darkroom problem because it was kind of the opposite of all my experience, which is that I tend to find people that were curious and were trying to move away from information

leveling off or being static to information gain.

So first of all, I had to learn what the argument was about why people would actually, who are normally social, why they would move away from that social realm and into this sort of dark space under the idea that active inference actually implies that that's what people would do.

And then secondly, it was

I was kind of trying to tie it into the idea of the refrigerator and where it is.

And until you actually, and Danny, you and I talked about this a bit, the pragmatic piece of until you actually open the refrigerator door, which is on the kind of the next slide, you don't really, I mean, you can have a belief that there's something of value in there, but it can turn out that when,

you open the door or Monty raises the curtain, the thing that you were expecting turns out to be a value, but not necessarily of that thing that you were setting yourself up to believe.

And so that young individual spitting the coin out, maybe I guess I was just trying to be a little bit facetious there that it holds value, but it doesn't hold the same kind of tasty value as say having a lick of an ice cream cone, so.

Again, it was trying to move away from the idea that the belief he stopped alone is satisfactory or necessary.

I always get those two mixed up, but I think I was just trying to reinforce the idea that you kind of need to have both so that you can compare and contrast so that you get a sense of what a person or what an agent is doing as they're getting past some of those shrouds or some of those

non-observables into the observable state.


SPEAKER_00:
Thanks.

We'll keep going.

this is unpacking some of the concerns about the apparent pure doxastic belief oriented ontology of actin so ryan described earlier like is it about beliefs colliding and if so if it's purely doxastic then where is this um desire and um is there anything else that anybody wants to add here


SPEAKER_01:
I would maybe comment that I think this also relates to the earlier thing that was discussed, how this confusion might stem from people misinterpreting or misinterpreting

confusing the terms predictive processing and active inference.

And I feel like this addresses it as well, where if it's only beliefs, then there is obviously a concern that once we're in a dark room, we...

Because if we're only in the regime of predictive processing, there is no way for us to move to a different state.

Or there isn't the formalism that describes planning and action.

But as I guess we'll get to in the other slides, when we do include this planning, we suddenly have desire-like terms appearing within.

the mathematical formulation as well.


SPEAKER_04:
Great.

Yeah, and I want to be clear.

It's not like they're thrown in ad hoc, right?

They emerge naturally as like necessary components of any kind of, you know, like using this sort of framework or building it out to involve planning.

So it's not like they emerge naturally and necessarily.

It's not because someone's just kind of tacking them on.


SPEAKER_00:
great um and this is referring to that minimization of g the minimization of g drives the agent to seek out observations that will reduce uncertainty about the best way to subsequently bring about phenotype congruent or preferred outcomes so that's the information seeking

Another way to put this is that under DAI, an agent doesn't simply seek to minimize prediction error with respect to its current sensory input.

It seeks instead to minimize prediction error with respect to its global beliefs about the environment, which entails seeking out observations that are expected to generate prediction errors, such that uncertainty is minimized for the generative model as a whole.

And another aspect of this type of global prediction error minimization process is that it pertains not only to beliefs about states in the present, but also to beliefs about the past and in the future.

And so this is leading to one of the claims, which is that in addition to desires, DAI captures folk

psychological experiences associated with the drive to both know about one's current states and learn what will happen when choosing to move to other states among other parameters in a generative model.

Let's go into this generative model and come to the darkroom simulation itself.

And we'll unpack it more in future times, but the prior over policies E of pi,

they write that um when the agent repeatedly chooses a policy this term increases the probability that the agent will continue to select that policy in the future at the level of the formalism this corresponds to an agent coming to expect that it will choose a policy simply because it has chosen that policy many times in the past

This can be thought of as a type of habitization process, but it doesn't have any direct connection to preferred outcomes because E of pi is not informed by other beliefs in the agent's model.

So here's the partially observable Markov decision process, just one representation of it.

and e is sort of floating there influencing but not being influenced by and we had some interesting discussions which we can return to another time about how we might be able to assess whether somebody is engaging in a behavior because they simply have habitually engaged in that behavior but they understand the actual mapping

of how policies relate to outcomes, or in another case where somebody might be engaging in a behavior, not because of habituation, but rather because they have some sort of deviation from the appropriate mapping between policies and outcomes.

and maybe there's other ways so I thought that was very interesting how even within the same sort of sparse and first principles model similar behavioral outcomes might be observed on like a behavioral manifold and then different kinds of perturbations or evaluations might reduce uncertainty about what is giving rise to those conversion behaviors in in that case

Okay, Dean, with the ice cream truck and the eye.


SPEAKER_02:
I'm going to wait because I think this one and the next one are better to take up in a one-two part because if Ryan can come back, there's some good questions I want to ask him about those two sections because this slide and the next one are kind of anti-dark rooms.

And so I want to kind of pick his brain about how we can sort of convince people that that dark room problem is for such a small subset of the population that majority of the stuff that we're talking about here is done in sort of social settings and social context.

And there's a third party.

sometimes it can observe an observer and what role that plays in trying to figure out what's going on here as we move from the idea of decisioning and so forth.

So I'll wait.


SPEAKER_00:
Great.

So here's part one of that.

And here's part two, looking at social learning theory a little bit.

But let's go to the figure and to the simulation.

This is figure one, simulation of an active inference agent deciding whether to eat some ice cream.

And there's various ways to go about describing it.

But one of the key pieces, if anyone can give a remark about what is occurring here, is that there's three cases that are being compared.

There's a no desire case.

a weak desire reflected by this p of o and a strong desire case where the ratio between the two alternatives between um wanting the ice cream and or between the ice cream state and the no ice cream state is a sharper distinction in this second row

So there's a case in which ice cream is in the fridge, but where the kitchen is currently dark, and so the agent doesn't know whether the fridge is to the left or right.

So it's like a tea maze setup with three possible cases.

One where there's no preference for observing ice cream or not, a weak or a strong preference for that.

um and then under these situations we can ask about what happens in terms of the action as well as the um beliefs and valence updates what else would you add about that ryan well i mean i think the point you know the point of this is just to show um well i guess make a couple you know i mean so dean mentioned the


SPEAKER_04:
you know, the issue with the dark room, right?

I mean, as you said, I mean, as you said, I mean, this is essentially just a particular, very similar to, to like a teammates, right?

I mean, just putting a, putting a different sort of semantics on top of it, but the, you know, but the idea is just that it's just set up to show that, you know, even without any kind of desire, right.

Just, just resolving uncertainty the agent will be driven by,

to leave a dark room or do something to get rid of the dark room.

Um, so you don't need a desire, right.

But, but in fact, any realistic agent is also going to have desires.

So, um, so in the case where there are desires, then there's two different reasons why, um, no active inference agent will ever, um, stay in a dark room or be motivated to stay in a dark room.

Like in all cases, it will have a motivation to, to leave.

So, I mean, the,

Point is just to show, right, in some explicit simulations that generically, right, like this kind of darkroom problem, it will just never apply to active inference.

Again, this darkroom thing only applies if you assume the thing is just doing something like predictive coding, but then somehow also assume that it's making decisions.

But that's not active inference.


SPEAKER_03:
because it's not a theory of decision-making.


SPEAKER_00:
Awesome.

Here is some description about the strength of the desire.

Could have been shown before the figure, but we wanted to introduce the simulation setting.

And as described, there is a desire

as well as an information seeking and i believe that's the two reasons why the agent will escape the dark room and there's there's other escape hatches depending on how one constructs their model um

potentially like a hierarchical model might have a slightly different explanation for why an agent does or doesn't stay in the room.

But even just within this single level model, there's an information seeking as well as a preference realizing reason.

Any other comments here?

Okay, Dean?

Just a quick thing.

I'll put you in the island.

I'll put you in the island.

Go for it.


SPEAKER_02:
Yeah.

So maybe, Ryan, just a quick, this last couple of sentences, however, preference distributions are set to zero.

Oh, I'm sorry.

I'm not still on the previous slide, Daniel.

Sorry.

Can you go back one?

Yeah, go for it.

Okay, so however, if preference distributions are set to zero, as in our no-desire simulation, such that no outcome is desired over any other net and active inferences

will nonetheless be driven to choose behaviors that will maximize information gain.

Okay, that all makes sense.

That's just the background.

This is the part I was curious about.

While this might reasonably be considered a motivational influence, it is prima facie less plausible that it should be considered cognitive.

and may therefore be better seen as a type of doxastic drive.

Here, the formalism may therefore help us to recover and potentially nuance the folk psychological distinction between desire and curiosity.

These types of drives seem to differ fundamentally.

I agree with you, but I was wondering if you could maybe explain why that...

should be something that we should keep separate the idea of desire and curiosity I know you go into explaining it a little more into the paper but why did you want to point that out to readers well I mean I think there's a couple things I mean I mean one I think that you know these things are both driving you know decisions about what to do right so in some way they motivate they both motivate behavior right but


SPEAKER_04:
but at the same time, um, you know, and, and I think, uh, this is something, you know, that Alex Keeper, um, you know, pointed out when we were writing this as opposed to just, you know, so it's not just me, um, but, you know, pointed out that, um, you know, there is this kind of, um, fairly clear difference, um, between the, um, this, uh, you know, curiosity or epistemically driven, um, sort of behavior, um, from the kind of desire-driven behavior and the, the, the kind of clear differences is that the, um,

this epistemically driven motivation isn't, um, it isn't driven toward anything, right.

It doesn't have a target.

It's not trying to get one thing versus another.

Um, all it's trying to do is just, um, you know, just become clearer about what's out there, um, essentially.

Right.

So, so the fact that it lacks a target, um, is really, I think what makes the, what makes the distinction clear.

Um, but the, I mean, beyond that, I think the motivation was just, uh, was just to show that, um,

you know, like just a, you know, generically, right?

Like the belief, desire, intention principle doesn't really, um, doesn't really say anything about, uh, you know, types of types of desires.

There's nothing really explicitly just in that very, very simple, um, um, way of, way of, um, describing the framework that, that separates out, uh, information seeking from, from reward seeking.

Right.

Um, so, um,

you know, so you could, right, if all you're doing is trying to, you know, compare active inference to, um, to, you know, the BDI model, very simply stated, then, um, then this is able to kind of nuance a little more, right?

Like different, different things that, that, um, might look like, you know, cognitive motivational kinds of things, but, but there are separate, um, um,

on why decisions are made for one thing versus another that are more tied to beliefs in a different way.

That being said, at the same time, I think that folk psychology more broadly, I think very intuitively and naturally does recognize the difference between things like curiosity and things like reward-driven behavior.

Those are concepts that we have just in our natural folk psychology.

Um, you know, so active inference just captures those, right.

I mean, I, um, yeah, I talked about this, um, just as an example of, you know, how, how intuitive I think in like natural, um, this sort of aspect is, um, you know, and I just like, think of like most of the behaviors that say like my little dog does, um, you know, if I look at, if I look at what my dog does 90% of the time, it's way more information seeking than it is reward seeking.

You know, she searches out for some food a couple of times a day.

but I take her in the car, she's pulling left and right to see what's out the window and how that changed all the time.

You know, any little sound perks up looks right.

None of those things are reward driven.

They're all just information seeking.

Um, so I mean, very, very clearly, that's a big part of, um, you know, what drives, um, us and other animals to do.

Um, so, so the point was just to, just to kind of show that active inference captures that in a certain sense, it adds some granularity to, um,

to at least what the BDI model says.

But in that case, the BDI model is kind of too horse-grained.

And I think normal folk psychology doesn't include those things already.


SPEAKER_00:
Great.

Also to be returned to, Dean?


SPEAKER_02:
Yeah, because Brian just basically answered it.

All these two images are basically doing is separating out the sort of goal directed from the curiosity, which is

know the the image on the right is basically a documentary about the franklin expedition where they were going out to the event horizon but they never returned so did they have a goal probably but was that their curiosity being carried out i think we i think we got to be able to make sure that we appreciate both so that's all i wanted about that this makes me think like um


SPEAKER_00:
are you driven to watch the sunset over the horizon or are you driven to find out what's at the end of the rainbow and what's over the horizon and that's the sort of like infinite open-ended curiosity drive beyond the horizon versus the preferred specific state that one can desire in terms of their um observations and reduce the divergence there but cool um

wishful thinking we'll come back to we're just leaving notes so that we can um have more to discuss later because these are all like there's such vital threads um Ryan and Maxwell and Alex because they um touch to our day-to-day experience in a way that

few other frameworks and even papers within Act-Inf do.

So affect and the role of affect and precision, curiosity, these are all just like terms that touch humans.


SPEAKER_04:
One thing that I think is probably worth just touching on, I mean, again, something maybe you guys would want to talk about in the future, maybe when I'm not around in any other sessions, not sure, but

Um, you know, uh, so there is a, a, probably a distinction that's worth keeping in mind between that curiosity per se and, um, and goal-directed information seeking.

Um, I think some of the examples you guys have mentioned have been more kind of along the lines of one versus the other.

I think it might be good to just, just keep that distinction clear that, um, you know, and, and vanilla active inference.

Um, it is really something more just kind of like curiosity.

You're just kind of like independently driven, just kind of, you know, look where you're going to gain the most information.

Whereas the construct of directed exploration and reinforcement learning and what also often something that can emerge, I think, a little more clearly in sophisticated active inference is information seeking specifically or in the service of knowing how to get your goal.

The curiosity version that we're showing here, it has the effect of helping the agent get to what it wants.

But the drive to seek out information isn't actually itself due to the fact that the agent thinks it will help it get its goal.

It's just driven to seek the information independently.

Whereas in other, like in sophisticated inference, the agent is actually doing something more like, I'm going to look over here because I think looking over here is actually going to help me get to what I want better.

So this is kind of strategic information seeking and that's a little different than just like intrinsic curiosity.

So it's just something to keep clear.


SPEAKER_00:
That's quite interesting.

It kind of ties a braid back, but let's explore that later.

Here we have a nice clean representation of the summary of the main argument.

Their proposed solution is somewhat deflationary in the sense that it simply argues that the functional role of desire, not the experience of desire, is straightforwardly present in the DAI formalism.

And then in more detail, they argue that and they provide four points, which it'll be great to go over with the authors.

Based on those considerations, the apparent problem posed by purely doxastic looking constructs is simply not a problem.

There are beliefs and desires in the active inference framework.

Perhaps we could have explored this more with the formalism.

So we'll bring it up to talking more about F.

but won't go into it now.

We'll return to the letters later.

And then the appendix is very informative.

There's a description of all the states and the factors that are used in the simulation.

And in the model stream one, which is by the way, Ryan, it's our, our most popular series was model stream one.

It was, it was, it was a fan favorite, but yeah,

It's some similar concepts and MATLAB scripts.

So the appendix describes how the figure one results were generated.

And maybe we'll see if anybody can run that and execute it.

We can play with a few different things and take some of these qualitative linguistics and even mutate the simulation a little bit, see where that takes us.

We'll close out with our usual closing slide.

So who would like to take the first last word?

Yes, Dean, then Jakob, then Ryan.


SPEAKER_02:
First of all, thanks for shepherding us, these three cats, through this paper, because there's an awful lot of stuff to try to cover in a pretty short period of time.

And it's easy, I think, to sort of go

I don't want to say tangentially, but go deeper into some of the parts of the paper, because for me, it was one of those ones where I would go back and have to reflect on something after I read a section, and then try to fit it into the larger picture that was right.

I think the paper did a very good job of sort of ordering through what you were trying to say in terms of

the reassurance of how the quantitative and the modeling of the psychosocial could be seen as working with each other, as opposed to, oh, this doesn't answer what this other thing questions, but it does cause, this was a paper that for me, I won't speak for the others, but I had to read the section and then try to plug it back into the overall narrative that was being told.

And again, I think part of that is because

the the translation from the stuff that's typically seen as the quantitative part is hard to move into the qualitative part i think that's that's a challenge for anybody that tries to move back and forth between those two things but uh yeah i mean in terms of

in terms of making a case and providing what I think people need to sort of see the two in the same light.

Thank you, appreciate it.


SPEAKER_04:
Sure, I'm happy if it's helpful.


SPEAKER_01:
Rocco?

Yeah, I think, I guess there are two points that I'd like to touch on.

Firstly, I thought it was very helpful to

map these concepts that everyone, even though everyone, I guess that's the nature of psychology, that everyone will have a slightly different probably distribution of what these terms actually mean.

But I think it's in just learning about active inference, it's helpful to link the mathematical formulation

And even just the active inference ontology, which itself can be very cumbersome at times to these very intuitive concepts.

And I think as we're starting to explore modeling within the accident flap as well, I think

it will be really helpful to use this isomorphism between active inference and folk psychology to explain the behavior of agents within these models.

And beyond a darkroom problem, when there is some unexpected behavior of an agent, we can directly look at

okay, how did this tensor change its values and describe it with this folk psychological ontology?

So I think personally, that's the most exciting thing about this paper for me.

Yeah.


SPEAKER_04:
Well, I mean, if that's the kind of thing that you're interested in, then, I mean, that's kind of the whole...

it's kind of the whole motivation of, you know, for instance, like computational psychiatry, right.

Which is like the area that I currently work in.

Right.

So you, you take, for instance, clinical populations where people may behave in unexpected or ways or ways that don't necessarily seem like they're all that adaptive.

Right.

Then you can just fit these models to their behavior and you can figure it out.

Okay.

Well, what is making their behavior abnormal?

Right.

Is it, is it something about an overly precise preference distribution or is it something about, um,

you know, uh, the belief that states, uh, states transition to, uh, to, um, into volatile or uncertain way or, you know, things like that.

So, I mean, it is the, yeah, I mean, the kind of, uh, you know, the major point is you can use these models and empirical studies to figure out what the mechanisms are that are leading to healthy and unhealthy behavior.

Um, and, um, you know, that can give you kind of guiding information toward, you know, design better treatments or,

you know, trying to measure these things in a more quantitative way, things like that.

And, and the, you know, and just as I've said, I mean, this is all using, using tasks that, you know, that involve some kind of goal, right?

Like seeking some kind of reward or social approval, or, you know, whatever it is that, you know, humans seek out, you could do that, right?

Unless you have a straightforward way to map the formalism, each element of the formalism to

you know, the reason that we think people behave in the way that they do.

Right.

So, so, so I guess I'm just saying if that's an interest of yours, then I would think that you would, you'd probably be a fan of a lot of the, a lot of the computational psychiatry literature more broadly, both, you know, both the active inference part, which is a lot of what my lab does, but, but also just the broader computational psychiatry community that uses reinforcement learning and direct diffusion models and all the other, you know, all the other classics that are out there.


SPEAKER_00:
Cool.

And it, it opens it up to what organizations want and what do cells want and all these other, um, transpositions.

Well, Ryan really appreciate that last minute, um,

belief or desire combination thereof to join us it certainly helped resolve our uncertainty a lot and it's a great conversation in the coming two weeks we're going to be with hopefully some more authors and more lab participants and just i'm looking forward to taking some notes thank you see you all soon bye
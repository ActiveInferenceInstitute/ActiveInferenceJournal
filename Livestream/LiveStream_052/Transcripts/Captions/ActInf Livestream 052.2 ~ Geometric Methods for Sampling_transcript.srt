1
00:00:00,000 --> 00:00:08,610
Daniel: Hello and welcome.

2
00:00:07,583 --> 00:00:14,592
It's ActInf Lab Livestream 52.2 on March

3
00:00:14,592 --> 00:00:14,592
9, 2023.

4
00:00:08,776 --> 00:00:17,897
Welcome to the Active Inference

5
00:00:17,897 --> 00:00:17,897
Institute.

6
00:00:15,685 --> 00:00:24,530
We're a participatory online institute

7
00:00:24,530 --> 00:00:24,530
that is communicating, learning, and

8
00:00:24,530 --> 00:00:24,530
practicing applied active inference.

9
00:00:18,913 --> 00:00:28,901
You can find more information on these

10
00:00:28,901 --> 00:00:28,901
links.

11
00:00:25,620 --> 00:00:33,490
This is a recorded and archive live

12
00:00:33,490 --> 00:00:33,490
stream, so please provide feedback so we

13
00:00:33,490 --> 00:00:33,490
can improve our work.

14
00:00:28,918 --> 00:00:47,862
All backgrounds and perspectives are

15
00:00:47,862 --> 00:00:47,862
welcome, and we'll be following video

16
00:00:47,862 --> 00:00:47,862
etiquette for Livestreams head over

17
00:00:47,862 --> 00:00:47,862
Active Inference.org to learn more about

18
00:00:47,862 --> 00:00:47,862
getting involved with projects and

19
00:00:47,862 --> 00:00:47,862
learning groups.

20
00:00:34,527 --> 00:00:48,905
All right!

21
00:00:48,984 --> 00:01:03,833
We are back in Livestream 52.2, in our

22
00:01:03,833 --> 00:01:03,833
third discussion on the paper geometric

23
00:01:03,833 --> 00:01:03,833
methods for sampling, optimization,

24
00:01:03,833 --> 00:01:03,833
inference and adaptive agents.

25
00:00:48,906 --> 00:01:16,146
We had a Dot-Zero background and context

26
00:01:16,146 --> 00:01:16,146
video and last week Lance joined for

27
00:01:16,146 --> 00:01:16,146
52.1, where we had a great overview

28
00:01:16,146 --> 00:01:16,146
discussion on the paper.

29
00:01:04,934 --> 00:01:27,199
So today we're going to see where it

30
00:01:27,199 --> 00:01:27,199
goes, see where our last week has taken

31
00:01:27,199 --> 00:01:27,199
us and how we're thinking about it or

32
00:01:27,199 --> 00:01:27,199
curious about it.

33
00:01:16,191 --> 00:01:32,709
If you're watching live, of course, feel

34
00:01:32,709 --> 00:01:32,709
free to write questions in a live chat.

35
00:01:27,210 --> 00:01:38,351
Otherwise, let us pick up on a mostly

36
00:01:38,351 --> 00:01:38,351
blank slide.

37
00:01:32,761 --> 00:01:43,830
And just thanks again, Lance, for

38
00:01:43,830 --> 00:01:43,830
joining.

39
00:01:38,385 --> 00:01:52,072
If you want to give any sort of

40
00:01:52,072 --> 00:01:52,072
introduction or recap opening here, go

41
00:01:52,072 --> 00:01:52,072
for it.

42
00:01:43,833 --> 00:01:56,117
Lance: Well, yeah, I mean, thanks a lot,

43
00:01:56,117 --> 00:01:56,117
Daniel, for organizing this.

44
00:01:52,079 --> 00:02:00,094
I'm super happy to be here, I think.

45
00:01:56,117 --> 00:02:06,151
Well, last time we went through most of

46
00:02:06,151 --> 00:02:06,151
the paper, we discussed a lot about

47
00:02:06,151 --> 00:02:06,151
sampling.

48
00:02:00,095 --> 00:02:07,169
I guess the idea of sampling.

49
00:02:06,152 --> 00:02:17,266
We discussed a bit less about Hamiltonia

50
00:02:17,266 --> 00:02:17,266
Monte Carlo, which is one of the, I

51
00:02:17,266 --> 00:02:17,266
guess, state of the art methods for

52
00:02:17,266 --> 00:02:17,266
sampling in continuous space.

53
00:02:07,169 --> 00:02:21,302
We discussed a lot about optimization.

54
00:02:18,278 --> 00:02:31,399
There's a nice figure actually that we

55
00:02:31,399 --> 00:02:31,399
can discuss today which gives some nice

56
00:02:31,399 --> 00:02:31,399
intuition about the kind of optimization

57
00:02:31,399 --> 00:02:31,399
methods that we reviewed in the paper.

58
00:02:21,309 --> 00:02:35,447
And then there's another section on

59
00:02:35,447 --> 00:02:35,447
statistical inference.

60
00:02:32,413 --> 00:02:44,537
So this is a bit of a different section

61
00:02:44,537 --> 00:02:44,537
than people in the free energy principle

62
00:02:44,537 --> 00:02:44,537
literature and active inference are used

63
00:02:44,537 --> 00:02:44,537
to.

64
00:02:35,448 --> 00:02:56,654
Because here the goal of inference is

65
00:02:56,654 --> 00:02:56,654
about approximating expectations as

66
00:02:56,654 --> 00:02:56,654
opposed to just approximating

67
00:02:56,654 --> 00:02:56,654
distributions in of themselves.

68
00:02:45,541 --> 00:03:01,645
It turns out to be these two perspectives

69
00:03:01,645 --> 00:03:01,645
turn out to be dual.

70
00:02:56,656 --> 00:03:15,780
But I guess here we want to develop

71
00:03:15,780 --> 00:03:15,780
notions of divergences and discrepancies

72
00:03:15,780 --> 00:03:15,780
that are a bit more general than the KL

73
00:03:15,780 --> 00:03:15,780
divergence and that can use to solve

74
00:03:15,780 --> 00:03:15,780
problems that the KL divergence cannot.

75
00:03:01,646 --> 00:03:27,900
And I guess the overall picture for why

76
00:03:27,900 --> 00:03:27,900
we want to do that is the Kale divergence

77
00:03:27,900 --> 00:03:27,900
turns out to have a lot of really nice

78
00:03:27,900 --> 00:03:27,900
properties that we can discuss.

79
00:03:16,790 --> 00:03:38,017
One of them is that if the Kale

80
00:03:38,017 --> 00:03:38,017
divergence is reduced, it means that the

81
00:03:38,017 --> 00:03:38,017
two distributions in play are more

82
00:03:38,017 --> 00:03:38,017
similar in terms of information.

83
00:03:27,902 --> 00:03:52,151
So there is this idea of information

84
00:03:52,151 --> 00:03:52,151
monotonicity where the KL divergence sort

85
00:03:52,151 --> 00:03:52,151
of gives an ordering as to what extent

86
00:03:52,151 --> 00:03:52,151
two distributions quantify.

87
00:03:38,019 --> 00:04:02,197
So if you had three distributions and you

88
00:04:02,197 --> 00:04:02,197
compute the KL divergence between them

89
00:04:02,197 --> 00:04:02,197
and now KL of AB is lower than KL of AC.

90
00:03:52,156 --> 00:04:08,256
It means that B is more closer in terms

91
00:04:08,256 --> 00:04:08,256
of information to A than C is to A.

92
00:04:02,198 --> 00:04:15,327
So you have this really nice thing that

93
00:04:15,327 --> 00:04:15,327
is captured by the Kel divergence, which

94
00:04:15,327 --> 00:04:15,327
makes sense when we're dealing with

95
00:04:15,327 --> 00:04:15,327
information.

96
00:04:08,257 --> 00:04:19,365
The Kel divergence also has so many other

97
00:04:19,365 --> 00:04:19,365
nice properties.

98
00:04:15,329 --> 00:04:25,427
It's not a distance, but it turns out to

99
00:04:25,427 --> 00:04:25,427
behave a bit like a square distance.

100
00:04:21,382 --> 00:04:30,470
So you have this kind of like, really

101
00:04:30,470 --> 00:04:30,470
nice pitagorian theorem.

102
00:04:25,429 --> 00:04:43,599
And I won't get into the exact statement,

103
00:04:43,599 --> 00:04:43,599
but it's like if you have a B and C

104
00:04:43,599 --> 00:04:43,599
distributions, then KLAB plus klac equals

105
00:04:43,599 --> 00:04:43,599
KLBC.

106
00:04:31,486 --> 00:04:54,709
If you have like a rectangle triangle, if

107
00:04:54,709 --> 00:04:54,709
ABC defines a rectangle triangle in

108
00:04:54,709 --> 00:04:54,709
information space, you have other

109
00:04:54,709 --> 00:04:54,709
properties like the KL divergence gives

110
00:04:54,709 --> 00:04:54,709
and so on.

111
00:04:43,600 --> 00:05:01,722
So the KL divergence is in general, I

112
00:05:01,722 --> 00:05:01,722
would say, the distribution, the

113
00:05:01,722 --> 00:05:01,722
divergence of choice.

114
00:04:54,718 --> 00:05:04,751
But it turns out that in many cases you

115
00:05:04,751 --> 00:05:04,751
just can't use it.

116
00:05:01,723 --> 00:05:12,835
For example, when you have samples and

117
00:05:12,835 --> 00:05:12,835
you want to approximate some samples with

118
00:05:12,835 --> 00:05:12,835
a distribution, then the Kale divergence

119
00:05:12,835 --> 00:05:12,835
just is not going to work there.

120
00:05:04,751 --> 00:05:14,859
So you need to derive some other things.

121
00:05:12,836 --> 00:05:24,954
So this is to say that we're considering

122
00:05:24,954 --> 00:05:24,954
statistical inference a bit more

123
00:05:24,954 --> 00:05:24,954
generally than what we do in active

124
00:05:24,954 --> 00:05:24,954
inference in general.

125
00:05:15,861 --> 00:05:35,060
And so this speaks to why the section

126
00:05:35,060 --> 00:05:35,060
here is a bit different from the standard

127
00:05:35,060 --> 00:05:35,060
inference literature that we usually

128
00:05:35,060 --> 00:05:35,060
consider.

129
00:05:25,959 --> 00:05:53,247
And then I think that the well, then this

130
00:05:53,247 --> 00:05:53,247
section five, which is about active

131
00:05:53,247 --> 00:05:53,247
inference, and I think we should discuss

132
00:05:53,247 --> 00:05:53,247
that a little bit more because the

133
00:05:53,247 --> 00:05:53,247
formation active inference lab that's

134
00:05:53,247 --> 00:05:53,247
presented there is to my mind the most

135
00:05:53,247 --> 00:05:53,247
general and also the simplest

136
00:05:53,247 --> 00:05:53,247
conceptually that's been out there.

137
00:05:35,064 --> 00:06:05,305
I mean, we sort of recap the derivation

138
00:06:05,305 --> 00:06:05,305
active inference lab and also like the

139
00:06:05,305 --> 00:06:05,305
properties of the expected community

140
00:06:05,305 --> 00:06:05,305
properties of active inference and also

141
00:06:05,305 --> 00:06:05,305
how to scale active inference and so on.

142
00:05:53,248 --> 00:06:09,342
And we do that in just seven pages or

143
00:06:09,342 --> 00:06:09,342
five pages.

144
00:06:05,306 --> 00:06:10,356
I mean, it's just very short.

145
00:06:09,342 --> 00:06:15,409
So it stands, sure, but it's like a

146
00:06:15,409 --> 00:06:15,409
really concise summary.

147
00:06:11,366 --> 00:06:21,463
And actually from there you can really

148
00:06:21,463 --> 00:06:21,463
derive a lot of technical papers.

149
00:06:16,410 --> 00:06:35,606
Active inference lab, to me, it's the

150
00:06:35,606 --> 00:06:35,606
most general, and if you as a reader can

151
00:06:35,606 --> 00:06:35,606
understand that section, then you can

152
00:06:35,606 --> 00:06:35,606
sort of really understand what active

153
00:06:35,606 --> 00:06:35,606
inference is about.

154
00:06:21,463 --> 00:06:39,647
This is kind of the overview for today.

155
00:06:37,623 --> 00:06:47,721
I would really like to be discussing

156
00:06:47,721 --> 00:06:47,721
questions because last time we read

157
00:06:47,721 --> 00:06:47,721
discussed about most of the papers.

158
00:06:41,661 --> 00:06:49,746
But yeah, whatever comes up.

159
00:06:47,725 --> 00:06:51,767
Daniel: Awesome.

160
00:06:51,760 --> 00:06:53,787
All right, great.

161
00:06:52,771 --> 00:07:17,961
Well, let us talk about some of the more

162
00:07:17,961 --> 00:07:17,961
foundational pieces hamiltonian Monte

163
00:07:17,961 --> 00:07:17,961
Carlo and then on through section four

164
00:07:17,961 --> 00:07:17,961
with the points that you raised about the

165
00:07:17,961 --> 00:07:17,961
KL, which generalized inference beyond

166
00:07:17,961 --> 00:07:17,961
how it may have been brought up in other

167
00:07:17,961 --> 00:07:17,961
octave.

168
00:06:54,789 --> 00:07:30,097
And then we can spend most of the time in

169
00:07:30,097 --> 00:07:30,097
section five and looking at some of those

170
00:07:30,097 --> 00:07:30,097
figures connecting some of the intuitions

171
00:07:30,097 --> 00:07:30,097
about the ball rolling down the bowl to

172
00:07:30,097 --> 00:07:30,097
the person running.

173
00:07:17,964 --> 00:07:32,112
So.

174
00:07:31,106 --> 00:07:35,140
Lance: Sounds good on Hamiltonian?

175
00:07:32,113 --> 00:07:35,148
Daniel: Monte Carlo.

176
00:07:35,140 --> 00:07:37,166
Where do you want to pick up?

177
00:07:35,148 --> 00:07:38,178
Lance: Sure.

178
00:07:38,173 --> 00:07:45,249
I meantonian so last time we discussed a

179
00:07:45,249 --> 00:07:45,249
lot about the problem of sampling and why

180
00:07:45,249 --> 00:07:45,249
that's the difficult problem.

181
00:07:39,179 --> 00:07:53,320
And we arrived at the conclusion, or I

182
00:07:53,320 --> 00:07:53,320
presented like Monte Carlo methods, how

183
00:07:53,320 --> 00:07:53,320
they work.

184
00:07:47,266 --> 00:08:14,472
So you're basically running a stochastic

185
00:08:14,472 --> 00:08:14,472
process, like a random motion and sort of

186
00:08:14,472 --> 00:08:14,472
the distribution defining the process is

187
00:08:14,472 --> 00:08:14,472
going to convert your target

188
00:08:14,472 --> 00:08:14,472
distribution, which means that when you

189
00:08:14,472 --> 00:08:14,472
run the process long enough, then every

190
00:08:14,472 --> 00:08:14,472
point that it's going to be in is going

191
00:08:14,472 --> 00:08:14,472
to be like a sample of the distribution

192
00:08:14,472 --> 00:08:14,472
that you want to sample from.

193
00:07:53,321 --> 00:08:17,504
Now, there's a lot of issues with that.

194
00:08:15,482 --> 00:08:36,694
Conceptually, it's not so difficult, but

195
00:08:36,694 --> 00:08:36,694
actually, when you want to implement this

196
00:08:36,694 --> 00:08:36,694
in practice, it turns out to be really

197
00:08:36,694 --> 00:08:36,694
hard because if you just implement the

198
00:08:36,694 --> 00:08:36,694
simplest stochastic process to sample

199
00:08:36,694 --> 00:08:36,694
your target distribution, it's going to

200
00:08:36,694 --> 00:08:36,694
be extremely slow.

201
00:08:18,514 --> 00:08:41,746
And I think that's the main bottleneck

202
00:08:41,746 --> 00:08:41,746
when developing Monte Carlo methods.

203
00:08:36,694 --> 00:08:44,775
Monte Carlo sampling in general is slow.

204
00:08:41,746 --> 00:08:53,863
And that's also one of the reasons why

205
00:08:53,863 --> 00:08:53,863
one might want to do variational

206
00:08:53,863 --> 00:08:53,863
inference instead of sampling.

207
00:08:45,780 --> 00:08:57,907
So something is slower, but it's also

208
00:08:57,907 --> 00:08:57,907
more accurate.

209
00:08:54,877 --> 00:09:01,888
It can approximate distributions that are

210
00:09:01,888 --> 00:09:01,888
completely arbitrary.

211
00:08:57,907 --> 00:09:08,950
So if you care about accuracy and you

212
00:09:08,950 --> 00:09:08,950
have time and computational resources,

213
00:09:08,950 --> 00:09:08,950
then for sure go for sampling.

214
00:09:01,889 --> 00:09:15,028
If you care about speed, about doing

215
00:09:15,028 --> 00:09:15,028
things online, and you don't care about

216
00:09:15,028 --> 00:09:15,028
accuracy so much, then the variational

217
00:09:15,028 --> 00:09:15,028
inference is the way to go.

218
00:09:08,951 --> 00:09:18,057
At least that's my understanding right

219
00:09:18,057 --> 00:09:18,057
now.

220
00:09:15,029 --> 00:09:24,116
So let's say you wanted to sample a

221
00:09:24,116 --> 00:09:24,116
distribution in a continuous space.

222
00:09:19,062 --> 00:09:27,144
So it could be just as last time.

223
00:09:24,117 --> 00:09:38,253
Let's imagine the state space is the desk

224
00:09:38,253 --> 00:09:38,253
where I'm at, and you sort of have the

225
00:09:38,253 --> 00:09:38,253
distribution, which could be like

226
00:09:38,253 --> 00:09:38,253
multimodal, very weird, and you just want

227
00:09:38,253 --> 00:09:38,253
to take samples from there.

228
00:09:27,145 --> 00:09:45,327
HamiltonI Monte Carlo is probably the

229
00:09:45,327 --> 00:09:45,327
state of the art methods method to do

230
00:09:45,327 --> 00:09:45,327
that.

231
00:09:39,263 --> 00:09:54,418
There's a lot of other methods out there,

232
00:09:54,418 --> 00:09:54,418
but Hamilton Monte Carlo typically just

233
00:09:54,418 --> 00:09:54,418
works very well and in a wide variety of

234
00:09:54,418 --> 00:09:54,418
situations.

235
00:09:45,329 --> 00:10:08,493
Instead, the idea of HamiltonI Monte

236
00:10:08,493 --> 00:10:08,493
Carlo is you're going to augment the

237
00:10:08,493 --> 00:10:08,493
state space with so let's say that the

238
00:10:08,493 --> 00:10:08,493
original state space you start with are

239
00:10:08,493 --> 00:10:08,493
the positions.

240
00:09:55,422 --> 00:10:13,540
And so you're going to a mend that with a

241
00:10:13,540 --> 00:10:13,540
velocity state space.

242
00:10:08,495 --> 00:10:21,626
If your original state space was

243
00:10:21,626 --> 00:10:21,626
Euclidean space of N dimensions, you just

244
00:10:21,626 --> 00:10:21,626
end up with a nuclear space of two N

245
00:10:21,626 --> 00:10:21,626
dimensions.

246
00:10:13,547 --> 00:10:24,654
So you double the size of the state

247
00:10:24,654 --> 00:10:24,654
space.

248
00:10:22,631 --> 00:10:34,750
And now you say, okay, well, the

249
00:10:34,750 --> 00:10:34,750
distribution that I want to stumble from

250
00:10:34,750 --> 00:10:34,750
actually defines an energy landscape.

251
00:10:25,660 --> 00:10:45,863
So technically it's like if you have

252
00:10:45,863 --> 00:10:45,863
distribution which is P, then minus lock,

253
00:10:45,863 --> 00:10:45,863
p is an energy landscape.

254
00:10:34,759 --> 00:10:52,930
So points where minus lock P is low are

255
00:10:52,930 --> 00:10:52,930
points where P is high.

256
00:10:45,864 --> 00:10:55,962
And so these are points that you want to

257
00:10:55,962 --> 00:10:55,962
sample a lot from.

258
00:10:52,931 --> 00:11:02,975
And contrary wise, if minus Lock t is

259
00:11:02,975 --> 00:11:02,975
very high, then P is low.

260
00:10:55,967 --> 00:11:07,023
And you don't really want to go there so

261
00:11:07,023 --> 00:11:07,023
much because these are points of low

262
00:11:07,023 --> 00:11:07,023
probability.

263
00:11:02,975 --> 00:11:16,112
So I think this minus log P, actually

264
00:11:16,112 --> 00:11:16,112
there's many reasons why minus Lock P is

265
00:11:16,112 --> 00:11:16,112
meaningful in physics.

266
00:11:08,031 --> 00:11:35,305
But just note for now that minus Lock P

267
00:11:35,305 --> 00:11:35,305
is just like a function and the minimum

268
00:11:35,305 --> 00:11:35,305
of the functions of that function are the

269
00:11:35,305 --> 00:11:35,305
high probability points and these are

270
00:11:35,305 --> 00:11:35,305
high energy points where you basically

271
00:11:35,305 --> 00:11:35,305
don't want to go there so much.

272
00:11:16,113 --> 00:11:40,354
So let's call this minus Lock P potential

273
00:11:40,354 --> 00:11:40,354
energy.

274
00:11:36,312 --> 00:11:44,392
And so this is what this really is in

275
00:11:44,392 --> 00:11:44,392
physics.

276
00:11:40,355 --> 00:11:52,471
Technically in physics, P would be a

277
00:11:52,471 --> 00:11:52,471
gives measure, what people call an

278
00:11:52,471 --> 00:11:52,471
equilibrium distribution.

279
00:11:45,405 --> 00:11:57,527
Last time we saw like soft max, minus

280
00:11:57,527 --> 00:11:57,527
something.

281
00:11:53,488 --> 00:12:03,520
The something is the potential energy

282
00:12:03,520 --> 00:12:03,520
anyway.

283
00:11:58,529 --> 00:12:07,561
And so minus Lock P would be the

284
00:12:07,561 --> 00:12:07,561
potential energy.

285
00:12:03,521 --> 00:12:19,682
And then if you add kinetic energy on

286
00:12:19,682 --> 00:12:19,682
your velocities, then you get what's

287
00:12:19,682 --> 00:12:19,682
called a Hamiltonium, which is the sum of

288
00:12:19,682 --> 00:12:19,682
the potential and kinetic energies.

289
00:12:08,570 --> 00:12:23,720
So what is exactly kinetic energy?

290
00:12:19,683 --> 00:12:26,752
The kinetic energy is like velocity

291
00:12:26,752 --> 00:12:26,752
squared pretty much.

292
00:12:23,720 --> 00:12:32,817
So if you add remember that your state

293
00:12:32,817 --> 00:12:32,817
space is position and velocity.

294
00:12:26,757 --> 00:12:46,957
If you add and you take a point in state

295
00:12:46,957 --> 00:12:46,957
space, so you would have kinetic energy,

296
00:12:46,957 --> 00:12:46,957
which is velocity squared, and a

297
00:12:46,957 --> 00:12:46,957
potential energy which would be minus

298
00:12:46,957 --> 00:12:46,957
lock peak.

299
00:12:32,819 --> 00:12:54,030
Now, if you add the two together, what

300
00:12:54,030 --> 00:12:54,030
you get is a Hamiltonian, which is the

301
00:12:54,030 --> 00:12:54,030
sum of the kinetic energy and potential

302
00:12:54,030 --> 00:12:54,030
energy.

303
00:12:46,959 --> 00:12:55,048
And this is just standard physics.

304
00:12:54,031 --> 00:13:01,047
Hamiltonian is the sum of kinetic and

305
00:13:01,047 --> 00:13:01,047
potential energy and it gives us the

306
00:13:01,047 --> 00:13:01,047
total energy of the system.

307
00:12:56,049 --> 00:13:06,097
So why we start with the problem of

308
00:13:06,097 --> 00:13:06,097
sampling.

309
00:13:02,058 --> 00:13:14,176
And here I just told you something very

310
00:13:14,176 --> 00:13:14,176
complicated where we get a Hamiltonian,

311
00:13:14,176 --> 00:13:14,176
there's actually a good reason why I want

312
00:13:14,176 --> 00:13:14,176
to do that.

313
00:13:06,098 --> 00:13:37,406
And it comes back to the idea of

314
00:13:37,406 --> 00:13:37,406
geometric integration that we talked

315
00:13:37,406 --> 00:13:37,406
about last time, which is that typically

316
00:13:37,406 --> 00:13:37,406
maybe I have a process that I know will

317
00:13:37,406 --> 00:13:37,406
give very efficient sampling, but

318
00:13:37,406 --> 00:13:37,406
actually when I implement it on the

319
00:13:37,406 --> 00:13:37,406
computer in discrete time, I just lose

320
00:13:37,406 --> 00:13:37,406
all the properties that make that

321
00:13:37,406 --> 00:13:37,406
sampling efficient.

322
00:13:14,177 --> 00:13:50,532
So actually it turns out that most people

323
00:13:50,532 --> 00:13:50,532
working in Monte Carlo sampling, they're

324
00:13:50,532 --> 00:13:50,532
working on efficient discretizations of

325
00:13:50,532 --> 00:13:50,532
continuous processes as opposed to on the

326
00:13:50,532 --> 00:13:50,532
continuous process themselves.

327
00:13:37,407 --> 00:13:51,546
Really the bottleneck.

328
00:13:50,534 --> 00:14:04,616
And the difficulty here is the

329
00:14:04,616 --> 00:14:04,616
implementation part, implementing process

330
00:14:04,616 --> 00:14:04,616
on a computer like you and I had in such

331
00:14:04,616 --> 00:14:04,616
a way that we retain all the good

332
00:14:04,616 --> 00:14:04,616
sampling properties.

333
00:13:51,546 --> 00:14:13,703
So one very powerful idea is that of

334
00:14:13,703 --> 00:14:13,703
geometric integration, which is of

335
00:14:13,703 --> 00:14:13,703
preserving geometric properties of a

336
00:14:13,703 --> 00:14:13,703
system.

337
00:14:05,623 --> 00:14:28,852
Geometric integration is the field that

338
00:14:28,852 --> 00:14:28,852
develops computational methods of

339
00:14:28,852 --> 00:14:28,852
numerical integration and numerical

340
00:14:28,852 --> 00:14:28,852
discretion in such a way that they

341
00:14:28,852 --> 00:14:28,852
preserve important geometric properties

342
00:14:28,852 --> 00:14:28,852
of the system.

343
00:14:14,710 --> 00:14:33,905
Here, the geometry in play is a

344
00:14:33,905 --> 00:14:33,905
Hamiltonian.

345
00:14:29,862 --> 00:14:36,935
And so you might think, well, that's

346
00:14:36,935 --> 00:14:36,935
pretty weird.

347
00:14:33,907 --> 00:14:44,009
I mean, a Hamiltonian is an energy here

348
00:14:44,009 --> 00:14:44,009
we're talking about preserving the

349
00:14:44,009 --> 00:14:44,009
geometry and preserving the Hamiltonian.

350
00:14:36,935 --> 00:14:46,036
How do these two things fit together?

351
00:14:44,010 --> 00:15:03,144
And so it turns out that the presence of

352
00:15:03,144 --> 00:15:03,144
a Hamiltonian and the fact that we have a

353
00:15:03,144 --> 00:15:03,144
state space that has positions and

354
00:15:03,144 --> 00:15:03,144
velocity, technically, in mathematics,

355
00:15:03,144 --> 00:15:03,144
what we then get is what people call

356
00:15:03,144 --> 00:15:03,144
symptom geometry.

357
00:14:46,038 --> 00:15:13,248
So symptom geometry just arises when we

358
00:15:13,248 --> 00:15:13,248
have those states that comprise positions

359
00:15:13,248 --> 00:15:13,248
and velocity and where you have

360
00:15:13,248 --> 00:15:13,248
Hamiltonians.

361
00:15:03,145 --> 00:15:25,365
So this is just like a way of explaining

362
00:15:25,365 --> 00:15:25,365
why the Hamiltonian is closely and

363
00:15:25,365 --> 00:15:25,365
intimately related to geometry.

364
00:15:14,253 --> 00:15:36,477
So geometry integration enables you to

365
00:15:36,477 --> 00:15:36,477
discrete processes in such a way that the

366
00:15:36,477 --> 00:15:36,477
Hamiltonian is preserved.

367
00:15:26,371 --> 00:15:44,553
Now, when you think about the

368
00:15:44,553 --> 00:15:44,553
Hamiltonian, the Hamiltonian gives you

369
00:15:44,553 --> 00:15:44,553
the energy of a particular point in

370
00:15:44,553 --> 00:15:44,553
space.

371
00:15:36,479 --> 00:15:59,707
And so if you simulate trajectories that

372
00:15:59,707 --> 00:15:59,707
preserve the Hamiltonian, what you're

373
00:15:59,707 --> 00:15:59,707
effectively doing is sampling from the

374
00:15:59,707 --> 00:15:59,707
contours of the probability distribution,

375
00:15:59,707 --> 00:15:59,707
contours that have the same probability.

376
00:15:45,562 --> 00:16:15,809
So basically going around, let's say, for

377
00:16:15,809 --> 00:16:15,809
example, in circles in a region that has

378
00:16:15,809 --> 00:16:15,809
the same probability, when we want to

379
00:16:15,809 --> 00:16:15,809
sample, we want to go everywhere.

380
00:16:00,656 --> 00:16:22,873
I mean, we want to sample regions of high

381
00:16:22,873 --> 00:16:22,873
probability, low probability, and want to

382
00:16:22,873 --> 00:16:22,873
be able to go from one to the other.

383
00:16:16,810 --> 00:16:35,004
So what geometric integration allows us

384
00:16:35,004 --> 00:16:35,004
to do is to simulate dynamics that are

385
00:16:35,004 --> 00:16:35,004
going to preserve the contours of the

386
00:16:35,004 --> 00:16:35,004
probability distribution, and they're

387
00:16:35,004 --> 00:16:35,004
going to do so very well.

388
00:16:22,878 --> 00:16:53,189
The advantage here is that when we use

389
00:16:53,189 --> 00:16:53,189
geometric integration to simulate

390
00:16:53,189 --> 00:16:53,189
Hamiltonian dynamics, which are

391
00:16:53,189 --> 00:16:53,189
conservative and again, staying the

392
00:16:53,189 --> 00:16:53,189
contours, what geometric integration

393
00:16:53,189 --> 00:16:53,189
allows us to do is to take time steps

394
00:16:53,189 --> 00:16:53,189
that are very long.

395
00:16:36,017 --> 00:17:00,193
So it enables us to travel very far in

396
00:17:00,193 --> 00:17:00,193
the landscape while still preserving the

397
00:17:00,193 --> 00:17:00,193
Hamiltonian.

398
00:16:54,190 --> 00:17:06,259
So you don't need to take very small time

399
00:17:06,259 --> 00:17:06,259
steps to remain accurate and preserve the

400
00:17:06,259 --> 00:17:06,259
Hamiltonian.

401
00:17:00,194 --> 00:17:13,323
But actually, dramatic integration allows

402
00:17:13,323 --> 00:17:13,323
you to take very long time steps and

403
00:17:13,323 --> 00:17:13,323
still preserve the Hamiltonian.

404
00:17:07,260 --> 00:17:25,446
So now we have a dynamic that's

405
00:17:25,446 --> 00:17:25,446
Hamiltonian preserving that's just very

406
00:17:25,446 --> 00:17:25,446
good because you can take very long time

407
00:17:25,446 --> 00:17:25,446
steps, and that enables you to go around

408
00:17:25,446 --> 00:17:25,446
the contours of the probability

409
00:17:25,446 --> 00:17:25,446
distribution.

410
00:17:13,326 --> 00:17:30,497
So what you want to do next is to change

411
00:17:30,497 --> 00:17:30,497
contours.

412
00:17:26,455 --> 00:17:37,564
You want to go to contours that are of

413
00:17:37,564 --> 00:17:37,564
high probability, contours of lower

414
00:17:37,564 --> 00:17:37,564
probability.

415
00:17:30,497 --> 00:17:48,677
And you want to do so in a way that the

416
00:17:48,677 --> 00:17:48,677
contours of high probability are sampled

417
00:17:48,677 --> 00:17:48,677
much more and much more often than the

418
00:17:48,677 --> 00:17:48,677
contours of lower probability.

419
00:17:38,570 --> 00:18:01,740
So what you do is that you augment your

420
00:18:01,740 --> 00:18:01,740
Hamiltonian dynamic with what people call

421
00:18:01,740 --> 00:18:01,740
a velocity refreshment or a momentum

422
00:18:01,740 --> 00:18:01,740
refreshment.

423
00:17:50,695 --> 00:18:04,776
And so this is how it works.

424
00:18:01,744 --> 00:18:23,968
So you're going to simulate your

425
00:18:23,968 --> 00:18:23,968
Hamiltonian dynamics for some time, and

426
00:18:23,968 --> 00:18:23,968
then after a while, you're going to be

427
00:18:23,968 --> 00:18:23,968
like, okay, well, now I'm going to change

428
00:18:23,968 --> 00:18:23,968
the velocity at random by just drawing a

429
00:18:23,968 --> 00:18:23,968
new velocity from a Gaussian

430
00:18:23,968 --> 00:18:23,968
distribution.

431
00:18:05,789 --> 00:18:31,003
And so by drawing a new velocity from a

432
00:18:31,003 --> 00:18:31,003
Gaussian distribution, you're just going

433
00:18:31,003 --> 00:18:31,003
to change contour completely.

434
00:18:24,974 --> 00:18:49,022
And then there's what's called the

435
00:18:49,022 --> 00:18:49,022
constraint that I just talked about,

436
00:18:49,022 --> 00:18:49,022
which is you want to change contours of

437
00:18:49,022 --> 00:18:49,022
the probability distribution in such a

438
00:18:49,022 --> 00:18:49,022
way that contours of high probability are

439
00:18:49,022 --> 00:18:49,022
visited much more often than contours of

440
00:18:49,022 --> 00:18:49,022
low probability.

441
00:18:32,005 --> 00:18:51,024
So you want to preserve that.

442
00:18:50,022 --> 00:18:55,028
You want to do that in a way.

443
00:18:51,024 --> 00:19:02,029
So the third ingredient of Hamiltonian

444
00:19:02,029 --> 00:19:02,029
Monte Carlo comes in, which is

445
00:19:02,029 --> 00:19:02,029
metropolis.

446
00:18:55,028 --> 00:19:03,030
Hastings.

447
00:19:02,029 --> 00:19:10,037
And so Metropolis Hastings is this accept

448
00:19:10,037 --> 00:19:10,037
reject that we discussed briefly last

449
00:19:10,037 --> 00:19:10,037
time.

450
00:19:03,030 --> 00:19:28,055
And so Metropolis Hastings is just super

451
00:19:28,055 --> 00:19:28,055
clever and it enables you to say it just

452
00:19:28,055 --> 00:19:28,055
tells you whether this momentum

453
00:19:28,055 --> 00:19:28,055
refreshing step is actually a good one or

454
00:19:28,055 --> 00:19:28,055
a bad one and should be rejected.

455
00:19:10,037 --> 00:19:42,069
A momentum refreshment is good if overall

456
00:19:42,069 --> 00:19:42,069
your sampling is going to remain faithful

457
00:19:42,069 --> 00:19:42,069
to the target distribution, but it is bad

458
00:19:42,069 --> 00:19:42,069
if it remains unfaithful.

459
00:19:30,057 --> 00:19:57,084
So actually running this metropolitate

460
00:19:57,084 --> 00:19:57,084
things except rejects, that allows you to

461
00:19:57,084 --> 00:19:57,084
change contours of the property

462
00:19:57,084 --> 00:19:57,084
distribution in a way that remains

463
00:19:57,084 --> 00:19:57,084
faithful to the target distribution.

464
00:19:42,069 --> 00:19:59,086
And so there you go.

465
00:19:58,084 --> 00:20:02,083
That's basically Hamiltonian Monte

466
00:20:02,083 --> 00:20:02,083
Carlo.

467
00:20:00,081 --> 00:20:07,088
So to recap, you a mental state space by

468
00:20:07,088 --> 00:20:07,088
adding velocities.

469
00:20:02,083 --> 00:20:25,106
This allows you to build the Hamiltonian

470
00:20:25,106 --> 00:20:25,106
by declaring that the original

471
00:20:25,106 --> 00:20:25,106
probability distribution gives you

472
00:20:25,106 --> 00:20:25,106
potential energy and you add a kinetic

473
00:20:25,106 --> 00:20:25,106
energy on the velocities, which is just

474
00:20:25,106 --> 00:20:25,106
velocity squared.

475
00:20:08,089 --> 00:20:35,116
Because you have a Hamiltonian, you can

476
00:20:35,116 --> 00:20:35,116
use geometric integration to simulate

477
00:20:35,116 --> 00:20:35,116
Hamiltonian dynamics very accurately and

478
00:20:35,116 --> 00:20:35,116
with very long time steps.

479
00:20:25,106 --> 00:20:42,123
The very long time steps is a crucial

480
00:20:42,123 --> 00:20:42,123
thing because it means that with very low

481
00:20:42,123 --> 00:20:42,123
computational costs, you can sample very

482
00:20:42,123 --> 00:20:42,123
far.

483
00:20:35,116 --> 00:20:51,132
So it means that you don't get stuck in a

484
00:20:51,132 --> 00:20:51,132
region of the probability distribution,

485
00:20:51,132 --> 00:20:51,132
but you can actually visit it much more

486
00:20:51,132 --> 00:20:51,132
fast and efficiently.

487
00:20:42,123 --> 00:20:53,134
So that's the first part.

488
00:20:51,132 --> 00:21:00,135
The problem with those dynamics again, is

489
00:21:00,135 --> 00:21:00,135
that they remain on the contour of the

490
00:21:00,135 --> 00:21:00,135
probability distribution.

491
00:20:54,135 --> 00:21:03,138
And so you want to sample the whole

492
00:21:03,138 --> 00:21:03,138
probability distribution.

493
00:21:00,135 --> 00:21:23,158
So what you do is every once in a while,

494
00:21:23,158 --> 00:21:23,158
and I think that's a hyper parameter in

495
00:21:23,158 --> 00:21:23,158
your, in your sampling algorithm, let's

496
00:21:23,158 --> 00:21:23,158
say every ten iterations of the

497
00:21:23,158 --> 00:21:23,158
Hamiltonian dynamic, every ten time steps

498
00:21:23,158 --> 00:21:23,158
of the Hamiltonian dynamic, you're going

499
00:21:23,158 --> 00:21:23,158
to sample you're, you're just going to

500
00:21:23,158 --> 00:21:23,158
randomly take a new velocity.

501
00:21:03,138 --> 00:21:28,163
So you're going to change contour in the

502
00:21:28,163 --> 00:21:28,163
probability distribution.

503
00:21:23,158 --> 00:21:38,173
And the crucial point there is you want

504
00:21:38,173 --> 00:21:38,173
to change contour in a way that's

505
00:21:38,173 --> 00:21:38,173
faithful of the probability distribution

506
00:21:38,173 --> 00:21:38,173
and that's faithful of the sampling

507
00:21:38,173 --> 00:21:38,173
problem that you want to do.

508
00:21:28,163 --> 00:21:54,189
There comes the last thing, which is

509
00:21:54,189 --> 00:21:54,189
Metropolis Hastings, which ensures that

510
00:21:54,189 --> 00:21:54,189
the momentum reflect freshman will be

511
00:21:54,189 --> 00:21:54,189
good for sampling, I mean, will preserve

512
00:21:54,189 --> 00:21:54,189
your target probability distribution.

513
00:21:39,174 --> 00:22:02,191
So with that, you just get a very

514
00:22:02,191 --> 00:22:02,191
efficient sampling algorithm.

515
00:21:55,190 --> 00:22:04,193
And so it seems a bit convoluted, right?

516
00:22:02,191 --> 00:22:10,199
And one bottleneck, of course, is that

517
00:22:10,199 --> 00:22:10,199
you need to double the dimension of the

518
00:22:10,199 --> 00:22:10,199
state space.

519
00:22:05,194 --> 00:22:16,205
And if your state space is already

520
00:22:16,205 --> 00:22:16,205
extremely large, that could be a

521
00:22:16,205 --> 00:22:16,205
bottleneck.

522
00:22:11,200 --> 00:22:23,212
It could be the case that actually

523
00:22:23,212 --> 00:22:23,212
doubling the dimension to add velocities,

524
00:22:23,212 --> 00:22:23,212
it could be a computational bottleneck.

525
00:22:16,205 --> 00:22:30,219
So that's a problem, but still, in most

526
00:22:30,219 --> 00:22:30,219
cases, that's not a problem.

527
00:22:23,212 --> 00:22:35,224
Again, Hamilton in Monte Carlo is

528
00:22:35,224 --> 00:22:35,224
convoluted.

529
00:22:32,221 --> 00:22:49,238
But the overall take home message I would

530
00:22:49,238 --> 00:22:49,238
want you to take from this is that it is

531
00:22:49,238 --> 00:22:49,238
a method that remains faithful to the

532
00:22:49,238 --> 00:22:49,238
probability distribution they want to

533
00:22:49,238 --> 00:22:49,238
sample.

534
00:22:35,224 --> 00:22:50,239
And this is crucial.

535
00:22:49,238 --> 00:23:00,243
And it is also a method that through

536
00:23:00,243 --> 00:23:00,243
geometric integration, it enables you to

537
00:23:00,243 --> 00:23:00,243
take time steps that are very long and

538
00:23:00,243 --> 00:23:00,243
still get accurate sampling.

539
00:22:50,239 --> 00:23:03,246
So this is the really cool thing.

540
00:23:00,243 --> 00:23:31,274
If you could come up with a whole bunch

541
00:23:31,274 --> 00:23:31,274
of other methods that did not require to

542
00:23:31,274 --> 00:23:31,274
double the dimension of the state space

543
00:23:31,274 --> 00:23:31,274
or that didn't use geometric integration,

544
00:23:31,274 --> 00:23:31,274
the problem that you would probably run

545
00:23:31,274 --> 00:23:31,274
into is that the thing that you came up

546
00:23:31,274 --> 00:23:31,274
with when you implemented in the

547
00:23:31,274 --> 00:23:31,274
computer, it doesn't exactly preserve the

548
00:23:31,274 --> 00:23:31,274
probability distribution that you want to

549
00:23:31,274 --> 00:23:31,274
sample.

550
00:23:04,247 --> 00:23:37,280
And so the problem with that is that this

551
00:23:37,280 --> 00:23:37,280
would lead to biased sampling.

552
00:23:32,275 --> 00:23:51,294
And biased sampling is like you're going

553
00:23:51,294 --> 00:23:51,294
to sample a different probability

554
00:23:51,294 --> 00:23:51,294
distribution, which might be just a bit

555
00:23:51,294 --> 00:23:51,294
different, but still a different

556
00:23:51,294 --> 00:23:51,294
probability distribution than what you

557
00:23:51,294 --> 00:23:51,294
really want to sample.

558
00:23:37,280 --> 00:23:55,298
And this would bias your predictions.

559
00:23:51,294 --> 00:24:05,302
So the really cool thing of Hamiltonia

560
00:24:05,302 --> 00:24:05,302
Monte Carlo is that you're actually able

561
00:24:05,302 --> 00:24:05,302
to have unbiased sampling through the

562
00:24:05,302 --> 00:24:05,302
Metropolis Hastings step.

563
00:23:55,298 --> 00:24:09,306
And this is a crucial thing.

564
00:24:08,305 --> 00:24:20,317
So it's computationally implementable in

565
00:24:20,317 --> 00:24:20,317
general, there's no computational

566
00:24:20,317 --> 00:24:20,317
bottlenecks apart from this doubling of

567
00:24:20,317 --> 00:24:20,317
state space, and it leads to unbiased

568
00:24:20,317 --> 00:24:20,317
sampling.

569
00:24:09,306 --> 00:24:22,319
It's also relatively simple.

570
00:24:21,318 --> 00:24:25,322
So this is why this is really used all

571
00:24:25,322 --> 00:24:25,322
over the place.

572
00:24:22,319 --> 00:24:33,330
There's another few perspectives that

573
00:24:33,330 --> 00:24:33,330
might explain why it works so well.

574
00:24:27,324 --> 00:24:54,351
So in the paper in the last subsection of

575
00:24:54,351 --> 00:24:54,351
section two, which is about optimization,

576
00:24:54,351 --> 00:24:54,351
so the action on optimization is about

577
00:24:54,351 --> 00:24:54,351
how to accelerate optimization, how to

578
00:24:54,351 --> 00:24:54,351
derive an optimization algorithm.

579
00:24:34,331 --> 00:24:56,353
2.6.

580
00:24:55,352 --> 00:24:57,354
That's the one.

581
00:24:56,353 --> 00:25:06,357
So the whole thing about section two is

582
00:25:06,357 --> 00:25:06,357
about deriving an optimization algorithm

583
00:25:06,357 --> 00:25:06,357
that accelerated in a physical way.

584
00:24:57,354 --> 00:25:27,378
And by accelerated, what I mean by

585
00:25:27,378 --> 00:25:27,378
accelerated, it's not about just going

586
00:25:27,378 --> 00:25:27,378
faster, but it's about having

587
00:25:27,378 --> 00:25:27,378
acceleration, which means that if you go

588
00:25:27,378 --> 00:25:27,378
slightly up in the paper on the figure, I

589
00:25:27,378 --> 00:25:27,378
think this figure is great.

590
00:25:06,357 --> 00:25:32,383
And by the way, I wasn't the one who came

591
00:25:32,383 --> 00:25:32,383
up with this figure, but I think this

592
00:25:32,383 --> 00:25:32,383
figure is great.

593
00:25:27,378 --> 00:25:37,388
It really gives a lot of intuition for

594
00:25:37,388 --> 00:25:37,388
what acceleration really is.

595
00:25:32,383 --> 00:25:51,402
So if you look at the green cup, or I

596
00:25:51,402 --> 00:25:51,402
know how you call this, the green well on

597
00:25:51,402 --> 00:25:51,402
the left, this would be like a bowl

598
00:25:51,402 --> 00:25:51,402
rolling down the well.

599
00:25:37,388 --> 00:25:56,407
And the well is still that honey to like

600
00:25:56,407 --> 00:25:56,407
a certain level.

601
00:25:52,403 --> 00:25:59,410
And so there you get a lot of friction.

602
00:25:56,407 --> 00:26:08,413
So your ball would just roll down very

603
00:26:08,413 --> 00:26:08,413
slowly, and the speed of the ball would

604
00:26:08,413 --> 00:26:08,413
be proportionate to the slope of the

605
00:26:08,413 --> 00:26:08,413
well.

606
00:25:59,410 --> 00:26:13,418
So this is not accelerated because

607
00:26:13,418 --> 00:26:13,418
there's a lot of friction.

608
00:26:10,415 --> 00:26:18,423
And so the speed is just proportional to

609
00:26:18,423 --> 00:26:18,423
the slope of the well.

610
00:26:14,419 --> 00:26:21,426
So this is actually what gradient descend

611
00:26:21,426 --> 00:26:21,426
does.

612
00:26:18,423 --> 00:26:32,437
But now if you go on the right, you get

613
00:26:32,437 --> 00:26:32,437
what people coin physics on under the

614
00:26:32,437 --> 00:26:32,437
hand system, which is also an accelerated

615
00:26:32,437 --> 00:26:32,437
system, a system that's meaningfully

616
00:26:32,437 --> 00:26:32,437
accelerated.

617
00:26:21,426 --> 00:26:47,452
So if you replace the honey in the well

618
00:26:47,452 --> 00:26:47,452
by some water, then there's going to be

619
00:26:47,452 --> 00:26:47,452
way less friction and your bowl is

620
00:26:47,452 --> 00:26:47,452
actually going to accelerate and

621
00:26:47,452 --> 00:26:47,452
overshoot the minimum of the well and

622
00:26:47,452 --> 00:26:47,452
then sort of stabilize.

623
00:26:32,437 --> 00:26:54,459
But the point here is that this bulb is

624
00:26:54,459 --> 00:26:54,459
just going to get so much faster to the

625
00:26:54,459 --> 00:26:54,459
minimum.

626
00:26:47,452 --> 00:27:06,465
This idea for optimization is just

627
00:27:06,465 --> 00:27:06,465
extremely powerful.

628
00:27:02,461 --> 00:27:15,474
And by the way, on the right hand side in

629
00:27:15,474 --> 00:27:15,474
the graph, you can see the improvements

630
00:27:15,474 --> 00:27:15,474
that you get when you implement these

631
00:27:15,474 --> 00:27:15,474
sort of ideas.

632
00:27:07,466 --> 00:27:19,478
So this was on a I'll come back to

633
00:27:19,478 --> 00:27:19,478
sampling in a bit, I promise.

634
00:27:15,474 --> 00:27:22,481
But this is actually very well, very

635
00:27:22,481 --> 00:27:22,481
related.

636
00:27:19,478 --> 00:27:42,501
You can see on the graph on the right

637
00:27:42,501 --> 00:27:42,501
what kind of improvements you get when

638
00:27:42,501 --> 00:27:42,501
you go from under damp, first order

639
00:27:42,501 --> 00:27:42,501
system grade in the sense system, when

640
00:27:42,501 --> 00:27:42,501
you go from an over damped first order

641
00:27:42,501 --> 00:27:42,501
grade in the sense system to an underdamp

642
00:27:42,501 --> 00:27:42,501
second order accelerated system.

643
00:27:24,482 --> 00:27:45,504
And so you can see the curve, right?

644
00:27:43,502 --> 00:27:50,509
So the black orange curve is over damp.

645
00:27:46,505 --> 00:27:51,510
There's no acceleration.

646
00:27:50,509 --> 00:27:53,512
It's very slow.

647
00:27:51,510 --> 00:28:00,513
The last curve, the blue one, it's

648
00:28:00,513 --> 00:28:00,513
accelerated and it's just super fast.

649
00:27:54,512 --> 00:28:12,525
So you can see it sort of gives you like

650
00:28:12,525 --> 00:28:12,525
quantitative data as to how many

651
00:28:12,525 --> 00:28:12,525
improvements you can get by implementing

652
00:28:12,525 --> 00:28:12,525
this acceleration in a physically

653
00:28:12,525 --> 00:28:12,525
meaningful way.

654
00:28:00,513 --> 00:28:19,532
Daniel: Could you just describe the axes

655
00:28:19,532 --> 00:28:19,532
of the graph and also what the league

656
00:28:19,532 --> 00:28:19,532
group is here?

657
00:28:13,526 --> 00:28:25,537
Lance: The lead group is in the figure

658
00:28:25,537 --> 00:28:25,537
legend.

659
00:28:21,534 --> 00:28:28,541
In the fourth line, you see the leg group

660
00:28:28,541 --> 00:28:28,541
is so n.

661
00:28:25,538 --> 00:28:36,549
So that's technically that's the special

662
00:28:36,549 --> 00:28:36,549
orthogonal group of dimensions n.

663
00:28:28,541 --> 00:28:46,559
If I remember correctly, these are all

664
00:28:46,559 --> 00:28:46,559
the matrices, all the square matrices of

665
00:28:46,559 --> 00:28:46,559
size n by n that have a determinant equal

666
00:28:46,559 --> 00:28:46,559
to one.

667
00:28:36,549 --> 00:28:54,566
So the determinant is just like a

668
00:28:54,566 --> 00:28:54,566
function where you give it a matrix and

669
00:28:54,566 --> 00:28:54,566
it gives you a number.

670
00:28:46,559 --> 00:29:14,581
The fact that the fact that the

671
00:29:14,581 --> 00:29:14,581
determinant is equal to one means that

672
00:29:14,581 --> 00:29:14,581
here we're just taking matrices which

673
00:29:14,581 --> 00:29:14,581
are, which produce not translations, but

674
00:29:14,581 --> 00:29:14,581
changes of coordinates that preserve the

675
00:29:14,581 --> 00:29:14,581
geometry in a system.

676
00:28:54,567 --> 00:29:25,592
Just to elaborate on this a little bit,

677
00:29:25,592 --> 00:29:25,592
because for people who are not intimately

678
00:29:25,592 --> 00:29:25,592
familiar with matrices, it might seem a

679
00:29:25,592 --> 00:29:25,592
little opaque.

680
00:29:16,583 --> 00:29:31,598
So when you have a matrix, a matrix can

681
00:29:31,598 --> 00:29:31,598
multiply vectors.

682
00:29:25,592 --> 00:29:41,608
So a square matrix takes in vectors of

683
00:29:41,608 --> 00:29:41,608
side of dimension n and it outputs

684
00:29:41,608 --> 00:29:41,608
vectors of dimension n by

685
00:29:41,608 --> 00:29:41,608
multiplication.

686
00:29:31,598 --> 00:29:57,624
So if you have a square matrix, it just

687
00:29:57,624 --> 00:29:57,624
induces a transformation of your state

688
00:29:57,624 --> 00:29:57,624
space because each vector, each point

689
00:29:57,624 --> 00:29:57,624
which is a vector, gets converted into

690
00:29:57,624 --> 00:29:57,624
another point which is another vector.

691
00:29:42,609 --> 00:30:04,625
Now there's a lot of, I guess, properties

692
00:30:04,625 --> 00:30:04,625
there, right?

693
00:29:58,625 --> 00:30:13,634
You could have all sorts of translations

694
00:30:13,634 --> 00:30:13,634
of state space or rotations in state

695
00:30:13,634 --> 00:30:13,634
space or transformations of state space.

696
00:30:04,625 --> 00:30:25,646
If the determinant of the matrix is equal

697
00:30:25,646 --> 00:30:25,646
to one, it means that the transformation

698
00:30:25,646 --> 00:30:25,646
of state space that the matrix induces

699
00:30:25,646 --> 00:30:25,646
will preserve the geometry of the sixes.

700
00:30:14,635 --> 00:30:33,654
It will preserve distances and it will

701
00:30:33,654 --> 00:30:33,654
preserve crucially orientation as well.

702
00:30:25,646 --> 00:30:47,668
Because if you only asked the determinant

703
00:30:47,668 --> 00:30:47,668
to be equal to one or minus one, then the

704
00:30:47,668 --> 00:30:47,668
matrix transformation would preserve the

705
00:30:47,668 --> 00:30:47,668
geometry, but it would not preserve the

706
00:30:47,668 --> 00:30:47,668
orientation.

707
00:30:33,654 --> 00:30:49,670
It could mirror things.

708
00:30:47,668 --> 00:31:02,677
So if you only asked the determinant to

709
00:31:02,677 --> 00:31:02,677
be equal to one or minus one, you would

710
00:31:02,677 --> 00:31:02,677
get a lead group that's called the

711
00:31:02,677 --> 00:31:02,677
orthogonal group, which is usually

712
00:31:02,677 --> 00:31:02,677
denoted O of N.

713
00:30:50,671 --> 00:31:07,682
And here we require the determinant to be

714
00:31:07,682 --> 00:31:07,682
just equal to one.

715
00:31:02,677 --> 00:31:10,685
And so you get the special orthogonal

716
00:31:10,685 --> 00:31:10,685
group.

717
00:31:08,683 --> 00:31:14,689
And so it's this group of matrices that

718
00:31:14,689 --> 00:31:14,689
have this property.

719
00:31:11,686 --> 00:31:30,705
Now this is not really important for the

720
00:31:30,705 --> 00:31:30,705
example here because you could have taken

721
00:31:30,705 --> 00:31:30,705
any other lead group and got something

722
00:31:30,705 --> 00:31:30,705
similar and got a similar difference in

723
00:31:30,705 --> 00:31:30,705
performance.

724
00:31:15,690 --> 00:31:36,711
I guess it's just interesting for its own

725
00:31:36,711 --> 00:31:36,711
sake.

726
00:31:33,708 --> 00:31:40,715
And so the graph though, the K is the

727
00:31:40,715 --> 00:31:40,715
number of iterations.

728
00:31:37,712 --> 00:31:50,724
So the number of time steps, so that's

729
00:31:50,724 --> 00:31:50,724
the x axis and the y axis is how close

730
00:31:50,724 --> 00:31:50,724
you are from the minimum.

731
00:31:41,715 --> 00:31:52,727
And, and it's in lock scale.

732
00:31:50,725 --> 00:32:05,734
So as you can see, the, the blue curve is

733
00:32:05,734 --> 00:32:05,734
going to get like in 100 iterations, it's

734
00:32:05,734 --> 00:32:05,734
going to get basically at ten to the

735
00:32:05,734 --> 00:32:05,734
minus 410 to the minus five distance from

736
00:32:05,734 --> 00:32:05,734
the minimum.

737
00:31:52,727 --> 00:32:06,735
So it's like super, super close.

738
00:32:05,734 --> 00:32:16,745
It basically gets there in 100 iterations

739
00:32:16,745 --> 00:32:16,745
when you take the other normal gradient

740
00:32:16,745 --> 00:32:16,745
descent methods where you see that it's

741
00:32:16,745 --> 00:32:16,745
going to take so much longer.

742
00:32:06,735 --> 00:32:23,752
So yeah, this is really the advantage of

743
00:32:23,752 --> 00:32:23,752
using second order methods.

744
00:32:19,748 --> 00:32:33,762
And what I mean by second order methods

745
00:32:33,762 --> 00:32:33,762
is that by second order means that you

746
00:32:33,762 --> 00:32:33,762
actually double the state space to

747
00:32:33,762 --> 00:32:33,762
introduce velocities.

748
00:32:23,752 --> 00:32:40,769
So that you not only have a dynamical

749
00:32:40,769 --> 00:32:40,769
system over positions, but you have a

750
00:32:40,769 --> 00:32:40,769
dynamical system of velocities.

751
00:32:34,762 --> 00:32:51,780
And so it means that you have a physical

752
00:32:51,780 --> 00:32:51,780
notion of acceleration because

753
00:32:51,780 --> 00:32:51,780
acceleration is a movement of

754
00:32:51,780 --> 00:32:51,780
velocities.

755
00:32:41,769 --> 00:33:06,789
And so if you double the state space by

756
00:33:06,789 --> 00:33:06,789
saying this is position, this is

757
00:33:06,789 --> 00:33:06,789
velocities, and you see I have a motion

758
00:33:06,789 --> 00:33:06,789
in this like a state space that's twice

759
00:33:06,789 --> 00:33:06,789
as big, then you have a meaningfully, a

760
00:33:06,789 --> 00:33:06,789
meaningful notion of acceleration.

761
00:32:52,781 --> 00:33:09,792
And this is really powerful.

762
00:33:07,790 --> 00:33:26,809
And so you can see already a parallel

763
00:33:26,809 --> 00:33:26,809
here, which is that Hamiltonia Monte

764
00:33:26,809 --> 00:33:26,809
Carlo also has this notion of

765
00:33:26,809 --> 00:33:26,809
acceleration in some way, at least just

766
00:33:26,809 --> 00:33:26,809
intuitively, because we also doubled the

767
00:33:26,809 --> 00:33:26,809
size of the state space to introduce

768
00:33:26,809 --> 00:33:26,809
velocities.

769
00:33:09,792 --> 00:33:32,815
And so it turns out that this intuition

770
00:33:32,815 --> 00:33:32,815
is actually you can make it into a formal

771
00:33:32,815 --> 00:33:32,815
correspondence.

772
00:33:26,809 --> 00:33:39,822
And I think this is something that quite

773
00:33:39,822 --> 00:33:39,822
interests me, to be honest.

774
00:33:34,817 --> 00:33:45,828
So if you remember, if we come back to

775
00:33:45,828 --> 00:33:45,828
the intuition for sampling.

776
00:33:40,823 --> 00:33:51,834
The intuition for sampling through Monte

777
00:33:51,834 --> 00:33:51,834
Carlo methods is we have a target

778
00:33:51,834 --> 00:33:51,834
distribution that we want to sample

779
00:33:51,834 --> 00:33:51,834
from.

780
00:33:46,828 --> 00:33:57,840
So we're going to run a dynamic, random

781
00:33:57,840 --> 00:33:57,840
dynamic towards that distribution.

782
00:33:51,834 --> 00:34:11,848
Now, what makes sampling efficient is

783
00:34:11,848 --> 00:34:11,848
that the distribution that characterizes

784
00:34:11,848 --> 00:34:11,848
the process because again the process is

785
00:34:11,848 --> 00:34:11,848
random so that each time it is at a

786
00:34:11,848 --> 00:34:11,848
random location that is characterized by

787
00:34:11,848 --> 00:34:11,848
distribution.

788
00:33:57,840 --> 00:34:20,857
What makes sampling efficient is that the

789
00:34:20,857 --> 00:34:20,857
distribution characterizing the process

790
00:34:20,857 --> 00:34:20,857
just converted as fast as possible to the

791
00:34:20,857 --> 00:34:20,857
thing that you want to sample from.

792
00:34:11,848 --> 00:34:30,867
This is to say that you can think of

793
00:34:30,867 --> 00:34:30,867
sampling as an optimization on the space

794
00:34:30,867 --> 00:34:30,867
of probability distributions.

795
00:34:23,860 --> 00:34:36,873
You want to move your probability

796
00:34:36,873 --> 00:34:36,873
distribution of the process as fast as

797
00:34:36,873 --> 00:34:36,873
possible to your target.

798
00:34:30,867 --> 00:34:44,881
And so you can see from there that

799
00:34:44,881 --> 00:34:44,881
sampling is actually not so different

800
00:34:44,881 --> 00:34:44,881
from variational inference.

801
00:34:37,874 --> 00:34:50,887
It's just the same idea, only variational

802
00:34:50,887 --> 00:34:50,887
inference.

803
00:34:46,883 --> 00:35:05,896
You're typically going to take a

804
00:35:05,896 --> 00:35:05,896
parameterized family of distributions and

805
00:35:05,896 --> 00:35:05,896
approximate the targets, while here you

806
00:35:05,896 --> 00:35:05,896
have a non parameterized family that's

807
00:35:05,896 --> 00:35:05,896
given by your dynamic that's going to

808
00:35:05,896 --> 00:35:05,896
perfectly match the target.

809
00:34:50,887 --> 00:35:25,916
Again, with that, I want you to have the

810
00:35:25,916 --> 00:35:25,916
take home message that sampling, you can

811
00:35:25,916 --> 00:35:25,916
think about it as an optimization on the

812
00:35:25,916 --> 00:35:25,916
space of probability distributions in the

813
00:35:25,916 --> 00:35:25,916
sense that you have a target distribution

814
00:35:25,916 --> 00:35:25,916
and you want to get there as fast as you

815
00:35:25,916 --> 00:35:25,916
can with the process.

816
00:35:08,899 --> 00:35:37,928
Now let's suppose, and so here is the

817
00:35:37,928 --> 00:35:37,928
crucial connection between sampling and

818
00:35:37,928 --> 00:35:37,928
optimization in the way that we've

819
00:35:37,928 --> 00:35:37,928
described it here.

820
00:35:25,916 --> 00:35:49,940
Let's suppose that you run this

821
00:35:49,940 --> 00:35:49,940
accelerated optimization scheme on the

822
00:35:49,940 --> 00:35:49,940
space of probability distributions then

823
00:35:49,940 --> 00:35:49,940
the process.

824
00:35:37,928 --> 00:36:02,947
So you would get the dynamic on the space

825
00:36:02,947 --> 00:36:02,947
of probability distributions that has a

826
00:36:02,947 --> 00:36:02,947
meaningful notion of acceleration through

827
00:36:02,947 --> 00:36:02,947
the accelerated method that was shown

828
00:36:02,947 --> 00:36:02,947
here.

829
00:35:49,940 --> 00:36:17,962
So if you look at what kind of dynamic

830
00:36:17,962 --> 00:36:17,962
this really gives you, it gives you a

831
00:36:17,962 --> 00:36:17,962
process which is given by a stochastic

832
00:36:17,962 --> 00:36:17,962
differential equation that is known as

833
00:36:17,962 --> 00:36:17,962
under damp launch of a dynamic.

834
00:36:02,947 --> 00:36:22,967
So there's an equation for it and I think

835
00:36:22,967 --> 00:36:22,967
action subsection 2.8.

836
00:36:18,963 --> 00:36:37,982
But the point is under dynamics is a

837
00:36:37,982 --> 00:36:37,982
stochastic differential equation whose

838
00:36:37,982 --> 00:36:37,982
density or probability distribution stole

839
00:36:37,982 --> 00:36:37,982
this accelerated optimulation problem on

840
00:36:37,982 --> 00:36:37,982
the space of probability distributions.

841
00:36:23,968 --> 00:36:50,995
So just from there, you know that under

842
00:36:50,995 --> 00:36:50,995
that logo on dynamics, it's going to be a

843
00:36:50,995 --> 00:36:50,995
very efficient sampler because it

844
00:36:50,995 --> 00:36:50,995
meaningfully accelerates and gets the

845
00:36:50,995 --> 00:36:50,995
target, I mean, quite fast.

846
00:36:38,983 --> 00:36:58,003
The problem is under label logo on

847
00:36:58,003 --> 00:36:58,003
dynamics, you cannot simulate it

848
00:36:58,003 --> 00:36:58,003
accurately.

849
00:36:51,996 --> 00:37:04,003
Well, you can simulate it accurately, but

850
00:37:04,003 --> 00:37:04,003
you cannot simulate it exactly on your

851
00:37:04,003 --> 00:37:04,003
computer in practice.

852
00:36:59,004 --> 00:37:10,009
So this comes back to the numerical

853
00:37:10,009 --> 00:37:10,009
integration problem that we discussed

854
00:37:10,009 --> 00:37:10,009
just before.

855
00:37:04,003 --> 00:37:30,029
And so you have to find a way to

856
00:37:30,029 --> 00:37:30,029
discretize or in other words implement

857
00:37:30,029 --> 00:37:30,029
under dumpling bound dynamics on your

858
00:37:30,029 --> 00:37:30,029
computer in such a way that you retain

859
00:37:30,029 --> 00:37:30,029
that the thing that you implemented on

860
00:37:30,029 --> 00:37:30,029
the computer retains this meaningful

861
00:37:30,029 --> 00:37:30,029
notion of acceleration.

862
00:37:12,011 --> 00:37:31,030
It turns.

863
00:37:31,030 --> 00:37:53,052
Out that you can see Hamiltonian Monte

864
00:37:53,052 --> 00:37:53,052
Carlo as a faithful numerical discretion

865
00:37:53,052 --> 00:37:53,052
or numerical implementation of under damp

866
00:37:53,052 --> 00:37:53,052
launcher and dynamics that's going to

867
00:37:53,052 --> 00:37:53,052
preserve these acceleration properties

868
00:37:53,052 --> 00:37:53,052
and therefore this efficient sampling.

869
00:37:31,030 --> 00:37:59,058
So this is all like, yeah, a lot of

870
00:37:59,058 --> 00:37:59,058
interesting connections.

871
00:37:54,052 --> 00:38:08,061
But basically what I want to get at is

872
00:38:08,061 --> 00:38:08,061
you really have this notion of

873
00:38:08,061 --> 00:38:08,061
acceleration that permeates well, I guess

874
00:38:08,061 --> 00:38:08,061
physics.

875
00:37:59,058 --> 00:38:12,065
But here it permeates sampling and it

876
00:38:12,065 --> 00:38:12,065
permeates optimization.

877
00:38:08,061 --> 00:38:21,074
And so the method that was shown here

878
00:38:21,074 --> 00:38:21,074
about optimization and Hamiltonian Monte

879
00:38:21,074 --> 00:38:21,074
Carlo, they're just the same in a way.

880
00:38:12,065 --> 00:38:30,083
Only one is applied to optimization, the

881
00:38:30,083 --> 00:38:30,083
other is applied to optimization on

882
00:38:30,083 --> 00:38:30,083
probability distributions of Aka

883
00:38:30,083 --> 00:38:30,083
sampling.

884
00:38:21,074 --> 00:38:33,086
Great.

885
00:38:32,085 --> 00:38:35,088
Daniel: I guess last question on this

886
00:38:35,088 --> 00:38:35,088
part.

887
00:38:33,086 --> 00:38:39,092
What is the shadow Hamiltonian and why

888
00:38:39,092 --> 00:38:39,092
does it sound so cool?

889
00:38:35,088 --> 00:38:41,094
Lance: I know, right?

890
00:38:40,093 --> 00:38:43,096
Yeah, it's really cool.

891
00:38:41,094 --> 00:38:45,098
It's a recall name.

892
00:38:44,097 --> 00:38:53,106
So the shadow of Hamiltonian is okay, so

893
00:38:53,106 --> 00:38:53,106
let's go back to the Hamiltonian.

894
00:38:45,098 --> 00:38:58,111
So we have a Hamiltonian, and we want to

895
00:38:58,111 --> 00:38:58,111
simulate the dynamic that preserve the

896
00:38:58,111 --> 00:38:58,111
Hamiltonian.

897
00:38:53,106 --> 00:39:00,107
We want to implement that on the

898
00:39:00,107 --> 00:39:00,107
computer.

899
00:38:58,111 --> 00:39:04,111
We're going to do that through geometric

900
00:39:04,111 --> 00:39:04,111
integration.

901
00:39:01,108 --> 00:39:10,117
Geometric integration gives you a bunch

902
00:39:10,117 --> 00:39:10,117
of algorithms to preserve the

903
00:39:10,117 --> 00:39:10,117
Hamiltonian.

904
00:39:04,111 --> 00:39:16,123
I mean, a bunch of algorithms they can

905
00:39:16,123 --> 00:39:16,123
implement on the computer, and they're

906
00:39:16,123 --> 00:39:16,123
going to preserve the Hamiltonian.

907
00:39:10,117 --> 00:39:20,127
Do they actually really preserve the

908
00:39:20,127 --> 00:39:20,127
Hamiltonian?

909
00:39:17,124 --> 00:39:22,129
It turns out that no.

910
00:39:20,127 --> 00:39:54,161
So what I said before is a bit of a

911
00:39:54,161 --> 00:39:54,161
shortcut, because the numerical

912
00:39:54,161 --> 00:39:54,161
integration that you get through any

913
00:39:54,161 --> 00:39:54,161
numerical method, including geometric

914
00:39:54,161 --> 00:39:54,161
integration, is not going to exactly

915
00:39:54,161 --> 00:39:54,161
preserve the Hamiltonian, but it's going

916
00:39:54,161 --> 00:39:54,161
to preserve what people call a shadow

917
00:39:54,161 --> 00:39:54,161
hamiltonian, which is almost the same as

918
00:39:54,161 --> 00:39:54,161
the Hamiltonian, but with extra terms

919
00:39:54,161 --> 00:39:54,161
that sort of vanish if the time step is

920
00:39:54,161 --> 00:39:54,161
very short.

921
00:39:22,129 --> 00:40:12,173
So this is to say that your numerical

922
00:40:12,173 --> 00:40:12,173
dynamic implemented through geometric

923
00:40:12,173 --> 00:40:12,173
integration is going to exactly preserve

924
00:40:12,173 --> 00:40:12,173
the shadow Hamiltonian and approximately

925
00:40:12,173 --> 00:40:12,173
preserve the true Hamiltonian.

926
00:39:56,163 --> 00:40:30,191
In the papers, we show I don't know if

927
00:40:30,191 --> 00:40:30,191
it's well, it's probably algorithm

928
00:40:30,191 --> 00:40:30,191
dependent, but basically, depending on

929
00:40:30,191 --> 00:40:30,191
the algorithm that you choose, you want

930
00:40:30,191 --> 00:40:30,191
to show to what extent the shadow

931
00:40:30,191 --> 00:40:30,191
Hamiltonian truly approximates the true

932
00:40:30,191 --> 00:40:30,191
Hamiltonian.

933
00:40:13,174 --> 00:40:47,208
The virtue of geometric integration

934
00:40:47,208 --> 00:40:47,208
methods is that actually the shadow

935
00:40:47,208 --> 00:40:47,208
Hamiltonian turns out to be extremely

936
00:40:47,208 --> 00:40:47,208
close to true Hamiltonian, which dean

937
00:40:47,208 --> 00:40:47,208
that you can take a very long time steps

938
00:40:47,208 --> 00:40:47,208
and still be very good at preserving the

939
00:40:47,208 --> 00:40:47,208
true Hamiltonian.

940
00:40:30,191 --> 00:40:57,218
So here, when you implement these

941
00:40:57,218 --> 00:40:57,218
methods, you don't exactly preserve the

942
00:40:57,218 --> 00:40:57,218
true Hamiltonian, but you still do it

943
00:40:57,218 --> 00:40:57,218
pretty well.

944
00:40:47,208 --> 00:41:17,232
And so in Hamiltonian Monte Carlo, the

945
00:41:17,232 --> 00:41:17,232
momentum refreshment, and Metropolis,

946
00:41:17,232 --> 00:41:17,232
usually the Metropolis Hastings accept

947
00:41:17,232 --> 00:41:17,232
rejects that is going to correct for

948
00:41:17,232 --> 00:41:17,232
those failures of truly preserving the

949
00:41:17,232 --> 00:41:17,232
Hamiltonian.

950
00:40:57,218 --> 00:41:33,248
So even though you don't exactly preserve

951
00:41:33,248 --> 00:41:33,248
the Hamiltonian in your numerical

952
00:41:33,248 --> 00:41:33,248
integration, in Hamiltonian Monte Carlo,

953
00:41:33,248 --> 00:41:33,248
the Metropolis hating accept reject step

954
00:41:33,248 --> 00:41:33,248
is going to correct for this inaccuracy.

955
00:41:17,232 --> 00:41:47,262
And so this is like, really the true

956
00:41:47,262 --> 00:41:47,262
beauty of Hamiltonian Monte Carlo is that

957
00:41:47,262 --> 00:41:47,262
even though you get a lot of things that

958
00:41:47,262 --> 00:41:47,262
are not exactly preserved when you

959
00:41:47,262 --> 00:41:47,262
implement things on a computer.

960
00:41:33,248 --> 00:42:00,269
Thanks to Metropolitan Hastings, overall

961
00:42:00,269 --> 00:42:00,269
your sampling is going to be perfect in

962
00:42:00,269 --> 00:42:00,269
the sense that you're going to truly

963
00:42:00,269 --> 00:42:00,269
preserve your target distribution.

964
00:41:48,263 --> 00:42:01,270
So this is really the key.

965
00:42:00,269 --> 00:42:19,288
Now, a follow up question to that is,

966
00:42:19,288 --> 00:42:19,288
okay, well, if Metropolis Hastings is

967
00:42:19,288 --> 00:42:19,288
just so crucial and it just gives your

968
00:42:19,288 --> 00:42:19,288
dynamic the property that it's going to

969
00:42:19,288 --> 00:42:19,288
preserve, whatever it does, even if it

970
00:42:19,288 --> 00:42:19,288
samples really badly if?

971
00:42:03,272 --> 00:42:25,294
You add Metropolis Hastings, it's still

972
00:42:25,294 --> 00:42:25,294
going to preserve your Target

973
00:42:25,294 --> 00:42:25,294
distribution.

974
00:42:19,288 --> 00:42:30,299
Then why can't You Just Come up with an

975
00:42:30,299 --> 00:42:30,299
average process, like whatever Kind Of

976
00:42:30,299 --> 00:42:30,299
process?

977
00:42:25,294 --> 00:42:33,302
And that Metropolis Hastings at the end?

978
00:42:30,299 --> 00:42:35,304
And so you can do that.

979
00:42:33,302 --> 00:42:38,307
You can take any kind of process.

980
00:42:35,304 --> 00:42:46,315
Let's say you take a random process, it

981
00:42:46,315 --> 00:42:46,315
could be the worst in the world.

982
00:42:43,312 --> 00:42:52,321
Let's say you wanted to sample from a

983
00:42:52,321 --> 00:42:52,321
gaussian and you actually take a Brown in

984
00:42:52,321 --> 00:42:52,321
motion.

985
00:42:46,315 --> 00:42:57,325
Now this is clearly not going to work

986
00:42:57,325 --> 00:42:57,325
because Brown Emotion just goes all over

987
00:42:57,325 --> 00:42:57,325
the place.

988
00:42:52,321 --> 00:42:58,327
It could go infinitely far.

989
00:42:57,326 --> 00:43:01,324
Brown in motion, it's just like random

990
00:43:01,324 --> 00:43:01,324
action, right?

991
00:42:58,327 --> 00:43:03,326
Completely random motion.

992
00:43:02,325 --> 00:43:16,339
There is structure to it, but the

993
00:43:16,339 --> 00:43:16,339
structure in Brown in Motion means that

994
00:43:16,339 --> 00:43:16,339
it's such that brown in motion is just

995
00:43:16,339 --> 00:43:16,339
going to just spread really far.

996
00:43:04,327 --> 00:43:23,346
So Brown in motion is not going to

997
00:43:23,346 --> 00:43:23,346
preserve your target distribution and

998
00:43:23,346 --> 00:43:23,346
it's also going to be very slow, by the

999
00:43:23,346 --> 00:43:23,346
way.

1000
00:43:16,339 --> 00:43:34,357
So it's not going to be a good sampler if

1001
00:43:34,357 --> 00:43:34,357
you add Metropolis Hastings on Brown

1002
00:43:34,357 --> 00:43:34,357
emotion, which is you integrate brand

1003
00:43:34,357 --> 00:43:34,357
action.

1004
00:43:23,346 --> 00:43:44,367
So you simulate a step of brand emotion

1005
00:43:44,367 --> 00:43:44,367
on the computer and then you do

1006
00:43:44,367 --> 00:43:44,367
Metropolis Hastings to know whether you

1007
00:43:44,367 --> 00:43:44,367
should accept or reject that step.

1008
00:43:34,357 --> 00:43:47,370
Metropolis Hastings will tell you whether

1009
00:43:47,370 --> 00:43:47,370
you should do it or not.

1010
00:43:44,367 --> 00:43:59,382
You either accept and you keep going from

1011
00:43:59,382 --> 00:43:59,382
that new position, or you reject and you

1012
00:43:59,382 --> 00:43:59,382
start from where you started from and

1013
00:43:59,382 --> 00:43:59,382
take a new sample.

1014
00:43:47,370 --> 00:44:00,377
Accept, reject.

1015
00:43:59,382 --> 00:44:01,378
If you accept, you keep going.

1016
00:44:00,377 --> 00:44:05,382
If you reject, you go back and you sort

1017
00:44:05,382 --> 00:44:05,382
of go like that.

1018
00:44:02,379 --> 00:44:10,387
So if you do materially facing there,

1019
00:44:10,387 --> 00:44:10,387
your standard is going to be exact.

1020
00:44:05,382 --> 00:44:15,392
So it is going to preserve the target and

1021
00:44:15,392 --> 00:44:15,392
so you're going to have accurate sound

1022
00:44:15,392 --> 00:44:15,392
plan.

1023
00:44:10,387 --> 00:44:18,395
And so this is crucial.

1024
00:44:17,394 --> 00:44:22,399
This really highlights the importance of

1025
00:44:22,399 --> 00:44:22,399
Metropolis Hastings.

1026
00:44:18,395 --> 00:44:31,408
But there is a big but your sampler here

1027
00:44:31,408 --> 00:44:31,408
is going to be extremely slow.

1028
00:44:23,400 --> 00:44:39,416
It's going to be extremely slow, first of

1029
00:44:39,416 --> 00:44:39,416
all, because brand in action in and of

1030
00:44:39,416 --> 00:44:39,416
itself, it doesn't at all preserve the

1031
00:44:39,416 --> 00:44:39,416
target distribution.

1032
00:44:31,408 --> 00:44:41,418
I mean, it just goes all over the place.

1033
00:44:39,416 --> 00:44:46,423
And you just want to sample a lot around

1034
00:44:46,423 --> 00:44:46,423
the mode of the gaussian, for example.

1035
00:44:41,418 --> 00:44:59,436
So it means that Metropolis hasting will

1036
00:44:59,436 --> 00:44:59,436
reject a lot of your steps because a lot

1037
00:44:59,436 --> 00:44:59,436
of your steps will try to go further away

1038
00:44:59,436 --> 00:44:59,436
while you want to stay around the mode

1039
00:44:59,436 --> 00:44:59,436
typically of the gapsin.

1040
00:44:46,423 --> 00:45:12,443
So you're going to do a lot of steps for

1041
00:45:12,443 --> 00:45:12,443
nothing and also Brown in Motion just

1042
00:45:12,443 --> 00:45:12,443
does not have the qualities that make a

1043
00:45:12,443 --> 00:45:12,443
sampler efficient.

1044
00:45:00,431 --> 00:45:31,462
And so the reason why Hamiltonian Monte

1045
00:45:31,462 --> 00:45:31,462
Carlo is so good is because it's able to

1046
00:45:31,462 --> 00:45:31,462
integrate Metropolis Hastings and still

1047
00:45:31,462 --> 00:45:31,462
preserve a lot of other properties that

1048
00:45:31,462 --> 00:45:31,462
make the sampler efficient.

1049
00:45:13,444 --> 00:45:47,478
So if we remember, HamiltonI Monte Carlo

1050
00:45:47,478 --> 00:45:47,478
has this geometric integration that does

1051
00:45:47,478 --> 00:45:47,478
not exactly preserve the Hamiltonian, but

1052
00:45:47,478 --> 00:45:47,478
it does still does it pretty well

1053
00:45:47,478 --> 00:45:47,478
approximately by preserving a shadow

1054
00:45:47,478 --> 00:45:47,478
Hamiltonian.

1055
00:45:31,462 --> 00:45:51,482
So this means already that integration

1056
00:45:51,482 --> 00:45:51,482
step is going to be really good.

1057
00:45:47,478 --> 00:46:07,492
So there's going to be a lot of

1058
00:46:07,492 --> 00:46:07,492
acceptance in the Metropolis Hastings and

1059
00:46:07,492 --> 00:46:07,492
also the way the scheme is set up,

1060
00:46:07,492 --> 00:46:07,492
there's going to be a lot of acceptance

1061
00:46:07,492 --> 00:46:07,492
set in the Metropolis Hastings.

1062
00:45:51,482 --> 00:46:15,500
So it means that most of the samples that

1063
00:46:15,500 --> 00:46:15,500
you would draw will actually be used

1064
00:46:15,500 --> 00:46:15,500
instead of being rejected and you have to

1065
00:46:15,500 --> 00:46:15,500
start over again.

1066
00:46:07,492 --> 00:46:17,502
That's one advantage.

1067
00:46:15,500 --> 00:46:32,517
The other advantage of Hamiltonium and T

1068
00:46:32,517 --> 00:46:32,517
Carlo, and this is a crucial one and we

1069
00:46:32,517 --> 00:46:32,517
kind of discussed this last week, is that

1070
00:46:32,517 --> 00:46:32,517
one crucial thing to be a good sampler is

1071
00:46:32,517 --> 00:46:32,517
this idea of time irreversibility.

1072
00:46:18,503 --> 00:46:41,526
So I'll emphasize it again because it's

1073
00:46:41,526 --> 00:46:41,526
really crucial and there's a lot of

1074
00:46:41,526 --> 00:46:41,526
literature on this and it's something

1075
00:46:41,526 --> 00:46:41,526
that we reviewed also in the paper.

1076
00:46:33,518 --> 00:46:56,541
So a sampler that is time reversible that

1077
00:46:56,541 --> 00:46:56,541
is time irreversible is always going to

1078
00:46:56,541 --> 00:46:56,541
be better or it's at least not going to

1079
00:46:56,541 --> 00:46:56,541
be worse than a sampler that is time

1080
00:46:56,541 --> 00:46:56,541
reversible.

1081
00:46:41,526 --> 00:46:58,543
So what do we mean by this?

1082
00:46:56,541 --> 00:47:00,539
A sample is time irreversible?

1083
00:46:58,543 --> 00:47:14,553
If and on the if were you to play the

1084
00:47:14,553 --> 00:47:14,553
dynamics forward or backward, there will

1085
00:47:14,553 --> 00:47:14,553
be going on your bullet point here.

1086
00:47:02,541 --> 00:47:29,568
A sample is time reversible if and on the

1087
00:47:29,568 --> 00:47:29,568
if you were to play the dynamics forward

1088
00:47:29,568 --> 00:47:29,568
or backward in time, so forward in time

1089
00:47:29,568 --> 00:47:29,568
and then maybe you would play them by

1090
00:47:29,568 --> 00:47:29,568
reversing the movie by playing the movie

1091
00:47:29,568 --> 00:47:29,568
backwards.

1092
00:47:14,553 --> 00:47:42,581
If you were to do that, then the two

1093
00:47:42,581 --> 00:47:42,581
movies that you would see would be

1094
00:47:42,581 --> 00:47:42,581
qualitatively the same and statistically

1095
00:47:42,581 --> 00:47:42,581
the same.

1096
00:47:30,569 --> 00:47:50,589
So the process is time reversible if and

1097
00:47:50,589 --> 00:47:50,589
only if the process, if you're running

1098
00:47:50,589 --> 00:47:50,589
forward or backward, it's basically

1099
00:47:50,589 --> 00:47:50,589
statistically the same.

1100
00:47:42,581 --> 00:47:53,592
Now, what does this mean in practice?

1101
00:47:51,590 --> 00:47:59,598
Well, if your process is time reversible,

1102
00:47:59,598 --> 00:47:59,598
then it's going to backtrack very often,

1103
00:47:59,598 --> 00:47:59,598
actually.

1104
00:47:53,592 --> 00:48:17,610
So it means that you would be somewhere

1105
00:48:17,610 --> 00:48:17,610
in the probability distribution, just

1106
00:48:17,610 --> 00:48:17,610
something there and you will go forward

1107
00:48:17,610 --> 00:48:17,610
and then you probably go backward and so

1108
00:48:17,610 --> 00:48:17,610
on and you kind of get stuck in a region

1109
00:48:17,610 --> 00:48:17,610
until you go somewhere else.

1110
00:48:01,594 --> 00:48:30,623
But it's just going to be very slow to

1111
00:48:30,623 --> 00:48:30,623
move around and so it's going to be very

1112
00:48:30,623 --> 00:48:30,623
slow to get good sampling because it's

1113
00:48:30,623 --> 00:48:30,623
going to take you a long time to visit

1114
00:48:30,623 --> 00:48:30,623
all the regions of the probate

1115
00:48:30,623 --> 00:48:30,623
distribution.

1116
00:48:17,610 --> 00:48:33,626
In other words, the distribution

1117
00:48:33,626 --> 00:48:33,626
characterizing the process.

1118
00:48:30,623 --> 00:48:39,632
It's going to move very slowly to the

1119
00:48:39,632 --> 00:48:39,632
target distribution that's another way to

1120
00:48:39,632 --> 00:48:39,632
look at it.

1121
00:48:34,627 --> 00:48:49,642
If the process is time irreversible, on

1122
00:48:49,642 --> 00:48:49,642
the other hand, then it's a lot less

1123
00:48:49,642 --> 00:48:49,642
likely to go backward during the sampling

1124
00:48:49,642 --> 00:48:49,642
process.

1125
00:48:41,634 --> 00:48:54,647
So it means that it's going to visit the

1126
00:48:54,647 --> 00:48:54,647
target distribution a lot more.

1127
00:48:50,643 --> 00:48:56,649
I mean, it's just going to go around.

1128
00:48:54,647 --> 00:49:07,654
Imagine if you're not allowed to go

1129
00:49:07,654 --> 00:49:07,654
backward just as a human being and you're

1130
00:49:07,654 --> 00:49:07,654
walking around, you're just going to end

1131
00:49:07,654 --> 00:49:07,654
up in many more places than if you were

1132
00:49:07,654 --> 00:49:07,654
allowed to go backward.

1133
00:48:56,649 --> 00:49:16,663
And if you were just going backward all

1134
00:49:16,663 --> 00:49:16,663
the time to where you started, then you

1135
00:49:16,663 --> 00:49:16,663
wouldn't be able to do a lot of

1136
00:49:16,663 --> 00:49:16,663
visiting.

1137
00:49:08,655 --> 00:49:18,665
So it would be a bad sampler.

1138
00:49:17,664 --> 00:49:23,670
So again, there's some, like

1139
00:49:23,670 --> 00:49:23,670
straightforward intuition there.

1140
00:49:18,665 --> 00:49:34,681
So one idea is in sampling is we want to

1141
00:49:34,681 --> 00:49:34,681
optimize the extent to which a sampler is

1142
00:49:34,681 --> 00:49:34,681
timely reversible.

1143
00:49:24,671 --> 00:49:42,689
We want to increase time irreversibility

1144
00:49:42,689 --> 00:49:42,689
as much as we can force a sampler to just

1145
00:49:42,689 --> 00:49:42,689
move around as much as possible.

1146
00:49:34,681 --> 00:49:43,690
So that's an idea.

1147
00:49:42,689 --> 00:49:46,693
It is explored currently.

1148
00:49:44,691 --> 00:49:56,703
I guess it's a bit of an open problem of

1149
00:49:56,703 --> 00:49:56,703
how do you really do that and how do you

1150
00:49:56,703 --> 00:49:56,703
implement that on the computer in a way

1151
00:49:56,703 --> 00:49:56,703
that works and preserves all the

1152
00:49:56,703 --> 00:49:56,703
properties.

1153
00:49:46,693 --> 00:50:06,707
But the point I wanted to get to is that

1154
00:50:06,707 --> 00:50:06,707
Metropolis Hastings is a blessing and

1155
00:50:06,707 --> 00:50:06,707
it's also a curse.

1156
00:49:57,704 --> 00:50:14,715
It is a blessing because when you add it

1157
00:50:14,715 --> 00:50:14,715
to any kind of process, you will make

1158
00:50:14,715 --> 00:50:14,715
your sampling unbiased.

1159
00:50:07,708 --> 00:50:21,722
So it means that you will sample the Ride

1160
00:50:21,722 --> 00:50:21,722
distribution even though you're

1161
00:50:21,722 --> 00:50:21,722
implementing this on a computer.

1162
00:50:14,715 --> 00:50:30,731
And there can be a lot, so many issues

1163
00:50:30,731 --> 00:50:30,731
with the numerical integration, numerical

1164
00:50:30,731 --> 00:50:30,731
approximation and so on.

1165
00:50:21,722 --> 00:50:41,742
But it's also a curse because whenever

1166
00:50:41,742 --> 00:50:41,742
you add Metropolis Hastings on the

1167
00:50:41,742 --> 00:50:41,742
process, it's going to make it time

1168
00:50:41,742 --> 00:50:41,742
reversible.

1169
00:50:30,731 --> 00:50:56,757
Actually, yeah, if you just took a random

1170
00:50:56,757 --> 00:50:56,757
process and added metropolisA, you would

1171
00:50:56,757 --> 00:50:56,757
get something that's inevitably going to

1172
00:50:56,757 --> 00:50:56,757
be quite slow.

1173
00:50:45,746 --> 00:51:01,756
And so now you get to the problem or the

1174
00:51:01,756 --> 00:51:01,756
conundrum.

1175
00:50:57,758 --> 00:51:02,757
What do I do?

1176
00:51:01,756 --> 00:51:05,760
Do I go for unbiased sampling?

1177
00:51:03,758 --> 00:51:06,761
Do I care about accuracy?

1178
00:51:05,760 --> 00:51:11,766
And then I should add Metropolis

1179
00:51:11,766 --> 00:51:11,766
Hastings.

1180
00:51:08,763 --> 00:51:21,776
Or do I not care about accuracy so much

1181
00:51:21,776 --> 00:51:21,776
and then I should not have Metropolis

1182
00:51:21,776 --> 00:51:21,776
Hastings and it's going to go faster

1183
00:51:21,776 --> 00:51:21,776
generally.

1184
00:51:11,766 --> 00:51:29,784
And so Hamiltonia Monte Carlo actually

1185
00:51:29,784 --> 00:51:29,784
has the blessing and does not have the

1186
00:51:29,784 --> 00:51:29,784
curse.

1187
00:51:23,778 --> 00:51:33,788
And this is why Hamiltonian Monte Carlo

1188
00:51:33,788 --> 00:51:33,788
is just so good.

1189
00:51:29,784 --> 00:51:41,796
And again, it's also complementary to all

1190
00:51:41,796 --> 00:51:41,796
of the things that we've been

1191
00:51:41,796 --> 00:51:41,796
discussing.

1192
00:51:34,789 --> 00:51:51,806
And the reason why Hamiltonian Monte

1193
00:51:51,806 --> 00:51:51,806
Carlo is blessed and not cursed is

1194
00:51:51,806 --> 00:51:51,806
because the momentum, how do you call

1195
00:51:51,806 --> 00:51:51,806
this?

1196
00:51:41,796 --> 00:52:01,810
The Metropolis Hastings is just done on

1197
00:52:01,810 --> 00:52:01,810
the momentum refreshment step as opposed

1198
00:52:01,810 --> 00:52:01,810
to being done on the overall dynamic.

1199
00:51:52,806 --> 00:52:20,829
So because if you remember, you do

1200
00:52:20,829 --> 00:52:20,829
geometric integration to simulate

1201
00:52:20,829 --> 00:52:20,829
Hamiltonian dynamics, every once in a

1202
00:52:20,829 --> 00:52:20,829
while you will take a momentum

1203
00:52:20,829 --> 00:52:20,829
refreshment and then do Metropolis

1204
00:52:20,829 --> 00:52:20,829
Hastings on that momentum refreshment, as

1205
00:52:20,829 --> 00:52:20,829
opposed to doing Metropolis Hastings on

1206
00:52:20,829 --> 00:52:20,829
both the Hamiltonian dynamic and the

1207
00:52:20,829 --> 00:52:20,829
momentum refreshment.

1208
00:52:01,810 --> 00:52:49,858
So it's not something that I can really

1209
00:52:49,858 --> 00:52:49,858
explain how that works, but it turns out

1210
00:52:49,858 --> 00:52:49,858
that by just doing the Metropolis method,

1211
00:52:49,858 --> 00:52:49,858
montacarlo is a way of having Metropolis

1212
00:52:49,858 --> 00:52:49,858
Hastings in a way that you preserve the

1213
00:52:49,858 --> 00:52:49,858
unbiasedness so that your sampling is

1214
00:52:49,858 --> 00:52:49,858
accurate, but you don't sacrifice the

1215
00:52:49,858 --> 00:52:49,858
time irreversibility.

1216
00:52:22,830 --> 00:52:51,859
And so you get both.

1217
00:52:49,858 --> 00:52:59,868
And this is because the Metropolis

1218
00:52:59,868 --> 00:52:59,868
hasting is just on one of the components

1219
00:52:59,868 --> 00:52:59,868
and not on both.

1220
00:52:53,862 --> 00:53:03,866
Daniel: To kind of connect that to an

1221
00:53:03,866 --> 00:53:03,866
example.

1222
00:53:01,864 --> 00:53:21,884
As we move into active inference here, we

1223
00:53:21,884 --> 00:53:21,884
have some probability isocontours, some,

1224
00:53:21,884 --> 00:53:21,884
we have the high probability carpool

1225
00:53:21,884 --> 00:53:21,884
lane, and then we have some lower

1226
00:53:21,884 --> 00:53:21,884
probability lanes, and we can go

1227
00:53:21,884 --> 00:53:21,884
different speeds in these lanes.

1228
00:53:03,866 --> 00:53:26,889
And we want to have a full accelerated

1229
00:53:26,889 --> 00:53:26,889
model here.

1230
00:53:21,884 --> 00:53:33,896
It's almost like the MH is refreshing our

1231
00:53:33,896 --> 00:53:33,896
velocity just asking when we want to

1232
00:53:33,896 --> 00:53:33,896
change the lanes.

1233
00:53:26,889 --> 00:53:39,902
But it's not our full self driving car

1234
00:53:39,902 --> 00:53:39,902
Metropolis Hastings algorithm.

1235
00:53:34,897 --> 00:53:52,915
But we're able to use our position and

1236
00:53:52,915 --> 00:53:52,915
acceleration when we're in our lane to

1237
00:53:52,915 --> 00:53:52,915
take full advantage of the acceleration

1238
00:53:52,915 --> 00:53:52,915
following the shadow road.

1239
00:53:39,902 --> 00:54:00,917
Not necessarily the true road, but the

1240
00:54:00,917 --> 00:54:00,917
shadow road is close enough, or the

1241
00:54:00,917 --> 00:54:00,917
shadows on the road, and then lane

1242
00:54:00,917 --> 00:54:00,917
changes.

1243
00:53:52,915 --> 00:54:10,927
Are these computationally costly and

1244
00:54:10,927 --> 00:54:10,927
reversible, but still super useful

1245
00:54:10,927 --> 00:54:10,927
propositions?

1246
00:54:00,917 --> 00:54:13,929
Lance: Exactly.

1247
00:54:12,929 --> 00:54:17,934
And they're actually not computationally

1248
00:54:17,934 --> 00:54:17,934
costly.

1249
00:54:13,930 --> 00:54:21,938
They would be if you were to reject many

1250
00:54:21,938 --> 00:54:21,938
samples.

1251
00:54:17,934 --> 00:54:27,944
But here in Hamiltonian Monte Carto,

1252
00:54:27,944 --> 00:54:27,944
typically you don't get to reject many

1253
00:54:27,944 --> 00:54:27,944
samples.

1254
00:54:21,938 --> 00:54:31,948
So yeah, that's such a great picture.

1255
00:54:27,944 --> 00:54:38,955
So the car follows the Shadow Road by

1256
00:54:38,955 --> 00:54:38,955
just doing geometric integration and

1257
00:54:38,955 --> 00:54:38,955
following the shadow.

1258
00:54:32,949 --> 00:54:42,959
Hamiltonian every once in a while, you

1259
00:54:42,959 --> 00:54:42,959
get a momentum refreshment.

1260
00:54:38,955 --> 00:54:48,965
Velocity refreshment, which is, oh, now I

1261
00:54:48,965 --> 00:54:48,965
move to the first lane, or the second

1262
00:54:48,965 --> 00:54:48,965
lane, third lane, and so on.

1263
00:54:42,959 --> 00:54:59,976
And you get a Metropolis Hastings

1264
00:54:59,976 --> 00:54:59,976
Corrections step that says, do I accept

1265
00:54:59,976 --> 00:54:59,976
this proposal of lane change or do I

1266
00:54:59,976 --> 00:54:59,976
reject it?

1267
00:54:48,965 --> 00:55:00,971
And still on the coast.

1268
00:54:59,976 --> 00:55:05,976
Daniel: So how does this connect active

1269
00:55:05,976 --> 00:55:05,976
inference lab?

1270
00:55:01,972 --> 00:55:08,979
Lance: Well, I mean, great question.

1271
00:55:06,977 --> 00:55:17,988
So active imprints in active inference

1272
00:55:17,988 --> 00:55:17,988
proper, you don't have any sampling in

1273
00:55:17,988 --> 00:55:17,988
active imprints.

1274
00:55:09,980 --> 00:55:31,002
Whenever you want to scale active

1275
00:55:31,002 --> 00:55:31,002
inference to implement it, to solve any

1276
00:55:31,002 --> 00:55:31,002
kind of problem in the world, you want to

1277
00:55:31,002 --> 00:55:31,002
do sampling because there's a lot of

1278
00:55:31,002 --> 00:55:31,002
computations that are not going to be

1279
00:55:31,002 --> 00:55:31,002
trackable otherwise.

1280
00:55:17,988 --> 00:55:39,009
And so one cool example is in the figure

1281
00:55:39,009 --> 00:55:39,009
after this, actually.

1282
00:55:31,002 --> 00:55:50,021
So for any kind of decision making active

1283
00:55:50,021 --> 00:55:50,021
inference lab, you need to approximate

1284
00:55:50,021 --> 00:55:50,021
the expected free energy.

1285
00:55:39,010 --> 00:55:57,027
The one just before yeah, perfect.

1286
00:55:53,024 --> 00:56:08,033
So if you look at the second equation

1287
00:56:08,033 --> 00:56:08,033
there, you get this minus lock p of an

1288
00:56:08,033 --> 00:56:08,033
action sequence equals expectation of

1289
00:56:08,033 --> 00:56:08,033
something.

1290
00:55:58,029 --> 00:56:16,041
This minus lock p of the action sequence

1291
00:56:16,041 --> 00:56:16,041
is our notation in this paper for the

1292
00:56:16,041 --> 00:56:16,041
expected free energy.

1293
00:56:09,034 --> 00:56:26,051
You probably all know this, the expected

1294
00:56:26,051 --> 00:56:26,051
free energy is given by the expectation

1295
00:56:26,051 --> 00:56:26,051
of something.

1296
00:56:20,045 --> 00:56:39,064
Now typically, and when you have a very

1297
00:56:39,064 --> 00:56:39,064
high dimensional model, which happens in

1298
00:56:39,064 --> 00:56:39,064
most applications, I would say you get a

1299
00:56:39,064 --> 00:56:39,064
very high dimensional expectation.

1300
00:56:27,052 --> 00:56:41,066
What is an expectation?

1301
00:56:40,065 --> 00:56:46,071
An expectation is an integral with

1302
00:56:46,071 --> 00:56:46,071
respect to probability distribution.

1303
00:56:41,066 --> 00:56:50,075
How do you compute expectations while you

1304
00:56:50,075 --> 00:56:50,075
do that?

1305
00:56:46,071 --> 00:56:58,083
Through sampling, or at least sampling is

1306
00:56:58,083 --> 00:56:58,083
the way to compute high dimensional

1307
00:56:58,083 --> 00:56:58,083
expectations.

1308
00:56:50,075 --> 00:57:00,079
That's the most efficient in statistics.

1309
00:56:58,083 --> 00:57:08,087
And this is the reason why sampling is

1310
00:57:08,087 --> 00:57:08,087
studied in statistics, is because that's

1311
00:57:08,087 --> 00:57:08,087
how you solve these problems.

1312
00:57:00,079 --> 00:57:20,099
I mean, expectations just come up

1313
00:57:20,099 --> 00:57:20,099
everywhere, just like so many things

1314
00:57:20,099 --> 00:57:20,099
about machine learning bowl down to

1315
00:57:20,099 --> 00:57:20,099
computing expectations.

1316
00:57:08,087 --> 00:57:33,112
And so here in particular, the

1317
00:57:33,112 --> 00:57:33,112
expectation that is very dear to us, very

1318
00:57:33,112 --> 00:57:33,112
close to our hearts, is the one that

1319
00:57:33,112 --> 00:57:33,112
gives you the expected free energy.

1320
00:57:21,100 --> 00:57:36,115
The expected is the expectation of

1321
00:57:36,115 --> 00:57:36,115
something.

1322
00:57:33,112 --> 00:57:42,121
So how do you compute that in a high

1323
00:57:42,121 --> 00:57:42,121
dimensional model where you have to use

1324
00:57:42,121 --> 00:57:42,121
something?

1325
00:57:36,115 --> 00:57:50,128
There comes the usefulness of Hamiltonian

1326
00:57:50,128 --> 00:57:50,128
Monte Carlo.

1327
00:57:45,124 --> 00:57:59,138
If you have a discrete state space based

1328
00:57:59,138 --> 00:57:59,138
model, you're not going to be able to use

1329
00:57:59,138 --> 00:57:59,138
Hamiltonian Monte Carlo because

1330
00:57:59,138 --> 00:57:59,138
Hamiltonian Monte Carlo is on continuous

1331
00:57:59,138 --> 00:57:59,138
states based models.

1332
00:57:50,129 --> 00:58:02,135
It's on continuous state space, right?

1333
00:58:00,133 --> 00:58:05,138
We talked about continuous positions,

1334
00:58:05,138 --> 00:58:05,138
continuous velocity.

1335
00:58:02,135 --> 00:58:12,144
I talked about something a probability

1336
00:58:12,144 --> 00:58:12,144
distribution on my desk, which is a

1337
00:58:12,144 --> 00:58:12,144
continuous state space.

1338
00:58:05,138 --> 00:58:21,154
So you're not going to be able to use

1339
00:58:21,154 --> 00:58:21,154
Hamiltonian Monte Carlo if you have

1340
00:58:21,154 --> 00:58:21,154
partially observed mark of decision

1341
00:58:21,154 --> 00:58:21,154
process and you're doing active inference

1342
00:58:21,154 --> 00:58:21,154
there.

1343
00:58:12,145 --> 00:58:47,180
But if you have for example, I've been

1344
00:58:47,180 --> 00:58:47,180
talking recently to Ryan Smith, who does

1345
00:58:47,180 --> 00:58:47,180
a lot of active inference, and he's

1346
00:58:47,180 --> 00:58:47,180
really leading a lot of the modeling work

1347
00:58:47,180 --> 00:58:47,180
with active inference and real data of

1348
00:58:47,180 --> 00:58:47,180
patients of all sorts, many medical data,

1349
00:58:47,180 --> 00:58:47,180
and also using active inference to model

1350
00:58:47,180 --> 00:58:47,180
psychological experiments.

1351
00:58:21,154 --> 00:58:58,191
And one of the things he told me about

1352
00:58:58,191 --> 00:58:58,191
well is, okay, well, I have a bunch of

1353
00:58:58,191 --> 00:58:58,191
data, and partially observed markup

1354
00:58:58,191 --> 00:58:58,191
decision process is just not going to do

1355
00:58:58,191 --> 00:58:58,191
it.

1356
00:58:47,180 --> 00:59:00,187
Why is he not going to do it?

1357
00:58:58,191 --> 00:59:04,191
Because the data that he has lives on a

1358
00:59:04,191 --> 00:59:04,191
continuous space.

1359
00:59:00,187 --> 00:59:22,209
I don't remember exactly what it was, but

1360
00:59:22,209 --> 00:59:22,209
let's just say, for the sake of example,

1361
00:59:22,209 --> 00:59:22,209
that that data was the temperature in the

1362
00:59:22,209 --> 00:59:22,209
room, and the subjects had to infer the

1363
00:59:22,209 --> 00:59:22,209
temperature in the room based on their

1364
00:59:22,209 --> 00:59:22,209
sensations.

1365
00:59:04,191 --> 00:59:25,212
So that's a continuous state space

1366
00:59:25,212 --> 00:59:25,212
problem.

1367
00:59:23,210 --> 00:59:32,219
Now, imagine that you ask someone to

1368
00:59:32,219 --> 00:59:32,219
infer the temperature in the room.

1369
00:59:26,213 --> 00:59:37,224
Then maybe you would change the

1370
00:59:37,224 --> 00:59:37,224
temperature in the room or not, and you

1371
00:59:37,224 --> 00:59:37,224
would ask them again.

1372
00:59:33,220 --> 00:59:40,227
Change the temperature or not and ask

1373
00:59:40,227 --> 00:59:40,227
them again, and so on.

1374
00:59:37,224 --> 00:59:54,241
When you have this sort of set up, you

1375
00:59:54,241 --> 00:59:54,241
have a discrete you have a phenomenon

1376
00:59:54,241 --> 00:59:54,241
that unfolds in discrete time because you

1377
00:59:54,241 --> 00:59:54,241
just repeatedly ask some questions and

1378
00:59:54,241 --> 00:59:54,241
sometimes elapses between the questions.

1379
00:59:41,228 --> 01:00:00,226
But you also have a state space that's

1380
01:00:00,226 --> 01:00:00,226
continuous.

1381
00:59:57,243 --> 01:00:12,368
When you have this sort of data, then the

1382
01:00:12,368 --> 01:00:12,368
kind of model that you want to use is a

1383
01:00:12,368 --> 01:00:12,368
partially observed mark of decision

1384
01:00:12,368 --> 01:00:12,368
process that we know and love but the

1385
01:00:12,368 --> 01:00:12,368
state space is going to be continuous

1386
01:00:12,368 --> 01:00:12,368
instead of discrete.

1387
01:00:00,418 --> 01:00:15,603
The planet is still going to be

1388
01:00:15,603 --> 01:00:15,603
discreet.

1389
01:00:12,373 --> 01:00:38,912
Now, when you have this sort of model, I

1390
01:00:38,912 --> 01:00:38,912
just want to say it or as an aside, that

1391
01:00:38,912 --> 01:00:38,912
for the moment, a lot of the modeling

1392
01:00:38,912 --> 01:00:38,912
work and simulation work in Active

1393
01:00:38,912 --> 01:00:38,912
inference uses the sweet state space

1394
01:00:38,912 --> 01:00:38,912
pound DPS, partially observed mark of the

1395
01:00:38,912 --> 01:00:38,912
same process just because it's been

1396
01:00:38,912 --> 01:00:38,912
sufficient.

1397
01:00:16,701 --> 01:00:49,052
And when you simulate agent is often like

1398
01:00:49,052 --> 01:00:49,052
in grid worlds and when you maybe have a

1399
01:00:49,052 --> 01:00:49,052
state spaces in experiments they often by

1400
01:00:49,052 --> 01:00:49,052
design discrete because it's just easier

1401
01:00:49,052 --> 01:00:49,052
to handle.

1402
01:00:38,921 --> 01:00:59,082
But that's not something, that's not an

1403
01:00:59,082 --> 01:00:59,082
assumption, a simplifying assumption that

1404
01:00:59,082 --> 01:00:59,082
we're going to be able to keep for that

1405
01:00:59,082 --> 01:00:59,082
much longer.

1406
01:00:49,068 --> 01:01:03,884
There's just so many things that you

1407
01:01:03,884 --> 01:01:03,884
cannot account with these sort of

1408
01:01:03,884 --> 01:01:03,884
models.

1409
01:01:00,501 --> 01:01:13,802
So one other model that is interesting to

1410
01:01:13,802 --> 01:01:13,802
have is partially observed Markov

1411
01:01:13,802 --> 01:01:13,802
decision process, but with a continuous

1412
01:01:13,802 --> 01:01:13,802
state space.

1413
01:01:03,895 --> 01:01:28,369
So with those you get the problem of

1414
01:01:28,369 --> 01:01:28,369
estimating the expected free energy which

1415
01:01:28,369 --> 01:01:28,369
would be then an expectation or in other

1416
01:01:28,369 --> 01:01:28,369
words, an integration, an integral in a

1417
01:01:28,369 --> 01:01:28,369
continuous takes place.

1418
01:01:13,863 --> 01:01:34,914
And so how do you do that efficiently if

1419
01:01:34,914 --> 01:01:34,914
you are in a high dimensional state

1420
01:01:34,914 --> 01:01:34,914
space?

1421
01:01:28,382 --> 01:01:35,023
Where do you use?

1422
01:01:34,923 --> 01:01:36,149
Hamilton, monte Carlo.

1423
01:01:35,025 --> 01:01:39,402
I think this wouldn't be really the

1424
01:01:39,402 --> 01:01:39,402
best.

1425
01:01:36,152 --> 01:01:44,934
So this is how you join the dots in this

1426
01:01:44,934 --> 01:01:44,934
paper.

1427
01:01:39,465 --> 01:01:52,075
We didn't talk about sampling in discrete

1428
01:01:52,075 --> 01:01:52,075
space because the methods are quite

1429
01:01:52,075 --> 01:01:52,075
different.

1430
01:01:44,941 --> 01:01:55,107
So we really had to choose what to focus

1431
01:01:55,107 --> 01:01:55,107
on.

1432
01:01:52,076 --> 01:02:00,097
There's of course a lot more methods in

1433
01:02:00,097 --> 01:02:00,097
the literature than there are in this

1434
01:02:00,097 --> 01:02:00,097
paper.

1435
01:01:55,108 --> 01:02:06,155
We just wanted to sort of review what

1436
01:02:06,155 --> 01:02:06,155
were the main ones and what people really

1437
01:02:06,155 --> 01:02:06,155
used in practice.

1438
01:02:00,098 --> 01:02:28,378
But when you have the expected principal

1439
01:02:28,378 --> 01:02:28,378
in a discrete state space, computing the

1440
01:02:28,378 --> 01:02:28,378
expectation that defines the expected

1441
01:02:28,378 --> 01:02:28,378
priority can often be a little bit easier

1442
01:02:28,378 --> 01:02:28,378
because it comes down computationally to

1443
01:02:28,378 --> 01:02:28,378
a bunch of matrix multiplications.

1444
01:02:08,172 --> 01:02:37,468
Matrix vector multiplications, which

1445
01:02:37,468 --> 01:02:37,468
generally is doable, unless the state

1446
01:02:37,468 --> 01:02:37,468
space is enormously high.

1447
01:02:29,388 --> 01:02:41,505
And then we'd have to think of sampling

1448
01:02:41,505 --> 01:02:41,505
methods in this sweet space.

1449
01:02:37,469 --> 01:03:03,662
But as soon as you move to a partially

1450
01:03:03,662 --> 01:03:03,662
observed markup decision process with a

1451
01:03:03,662 --> 01:03:03,662
continuous space, which are argu and Ryan

1452
01:03:03,662 --> 01:03:03,662
Smith argues and I'm sure a lot of people

1453
01:03:03,662 --> 01:03:03,662
have run into this as soon as you have

1454
01:03:03,662 --> 01:03:03,662
this kind of model, well, yeah, you just

1455
01:03:03,662 --> 01:03:03,662
have an integration problem in continuous

1456
01:03:03,662 --> 01:03:03,662
space.

1457
01:02:42,518 --> 01:03:06,692
And so Hamiltonium Altekaro is the way to

1458
01:03:06,692 --> 01:03:06,692
go there.

1459
01:03:03,663 --> 01:03:09,722
Daniel: Awesome.

1460
01:03:08,714 --> 01:03:18,813
And your 2020 paper was a synthesis on

1461
01:03:18,813 --> 01:03:18,813
some of the discrete statespace

1462
01:03:18,813 --> 01:03:18,813
formalisms of active.

1463
01:03:09,723 --> 01:03:38,015
So it's very interesting to see how

1464
01:03:38,015 --> 01:03:38,015
you're now talking about where continuous

1465
01:03:38,015 --> 01:03:38,015
time, continuous state spaces can come

1466
01:03:38,015 --> 01:03:38,015
into play and how interesting the Active

1467
01:03:38,015 --> 01:03:38,015
inference has the capacity to deal with

1468
01:03:38,015 --> 01:03:38,015
discrete and continuous date spaces.

1469
01:03:18,813 --> 01:03:51,144
And sometimes we lean on one leg or the

1470
01:03:51,144 --> 01:03:51,144
other leg more, but it spans the gap in a

1471
01:03:51,144 --> 01:03:51,144
way that's actually like a value adding,

1472
01:03:51,144 --> 01:03:51,144
not like there's some sort of missing

1473
01:03:51,144 --> 01:03:51,144
piece from one side or the other.

1474
01:03:38,017 --> 01:03:54,171
Lance: Yeah, definitely.

1475
01:03:52,158 --> 01:03:59,223
I think active inference is great because

1476
01:03:59,223 --> 01:03:59,223
it's so flexible in the sort of models

1477
01:03:59,223 --> 01:03:59,223
that you can consider.

1478
01:03:54,173 --> 01:04:08,252
But yeah, historically the first models

1479
01:04:08,252 --> 01:04:08,252
to be developed were continuous space and

1480
01:04:08,252 --> 01:04:08,252
continuous time.

1481
01:04:01,182 --> 01:04:17,341
And then it works quite well because you

1482
01:04:17,341 --> 01:04:17,341
can take gradients of the free energy and

1483
01:04:17,341 --> 01:04:17,341
just minimize them over time.

1484
01:04:08,256 --> 01:04:24,411
So you could get around basically

1485
01:04:24,411 --> 01:04:24,411
everything by doing a grain and assemble

1486
01:04:24,411 --> 01:04:24,411
free energy.

1487
01:04:18,350 --> 01:04:27,446
And then after that around like so that

1488
01:04:27,446 --> 01:04:27,446
was like 2010.

1489
01:04:24,414 --> 01:04:56,730
And then around 2015, people started

1490
01:04:56,730 --> 01:04:56,730
thinking, okay, well, how do I model

1491
01:04:56,730 --> 01:04:56,730
discrete time decision making with active

1492
01:04:56,730 --> 01:04:56,730
inference and typically decision making

1493
01:04:56,730 --> 01:04:56,730
tasks, at least the simple ones that are

1494
01:04:56,730 --> 01:04:56,730
studied in neuroscience and behavior

1495
01:04:56,730 --> 01:04:56,730
neuroscience, just to make things simple,

1496
01:04:56,730 --> 01:04:56,730
the number of actions that you have is

1497
01:04:56,730 --> 01:04:56,730
discrete.

1498
01:04:27,446 --> 01:04:59,769
So you have a finite number of actions,

1499
01:04:59,769 --> 01:04:59,769
which is pretty small.

1500
01:04:56,732 --> 01:05:03,747
You would also have a finite number of

1501
01:05:03,747 --> 01:05:03,747
states, which would be pretty small.

1502
01:05:00,710 --> 01:05:09,803
And then people started to think, okay,

1503
01:05:09,803 --> 01:05:09,803
well, how do we active inference lab to

1504
01:05:09,803 --> 01:05:09,803
actually account for that?

1505
01:05:04,754 --> 01:05:19,905
And so, yes, and so that's how the whole

1506
01:05:19,905 --> 01:05:19,905
expectation free energy and partially

1507
01:05:19,905 --> 01:05:19,905
observed mark of decision processes came

1508
01:05:19,905 --> 01:05:19,905
into play.

1509
01:05:10,811 --> 01:05:27,980
And now the community has grown a lot and

1510
01:05:27,980 --> 01:05:27,980
there's more and more data that we want

1511
01:05:27,980 --> 01:05:27,980
to account for.

1512
01:05:20,917 --> 01:05:35,065
There's more and more projects that are

1513
01:05:35,065 --> 01:05:35,065
going on and, you know, people are

1514
01:05:35,065 --> 01:05:35,065
realizing it's an obvious realization.

1515
01:05:27,981 --> 01:05:45,168
It's not surprising at all that these two

1516
01:05:45,168 --> 01:05:45,168
models developed in 2010, 2015.

1517
01:05:38,093 --> 01:05:48,192
They they're just not going to account

1518
01:05:48,192 --> 01:05:48,192
for everything.

1519
01:05:46,170 --> 01:05:50,217
You need to think about other kinds of

1520
01:05:50,217 --> 01:05:50,217
models.

1521
01:05:48,193 --> 01:05:53,249
It really depends on what kind of data

1522
01:05:53,249 --> 01:05:53,249
you have at hand.

1523
01:05:50,219 --> 01:06:02,277
And then one important and obvious type

1524
01:06:02,277 --> 01:06:02,277
of model would be partially observed

1525
01:06:02,277 --> 01:06:02,277
Markov decision process with continuous

1526
01:06:02,277 --> 01:06:02,277
space.

1527
01:05:54,251 --> 01:06:05,305
And actually, this is not a new thing.

1528
01:06:02,279 --> 01:06:12,370
I mean, people have been using these kind

1529
01:06:12,370 --> 01:06:12,370
of models in reinforcement learning and

1530
01:06:12,370 --> 01:06:12,370
control for a long time.

1531
01:06:05,306 --> 01:06:21,464
I don't know to what extent they've

1532
01:06:21,464 --> 01:06:21,464
arrived at practically I mean, they

1533
01:06:21,464 --> 01:06:21,464
arrived at practically instrumental

1534
01:06:21,464 --> 01:06:21,464
algorithms.

1535
01:06:12,372 --> 01:06:25,509
I don't know to what extent they're used,

1536
01:06:25,509 --> 01:06:25,509
to what extent their state of the art.

1537
01:06:21,465 --> 01:06:33,586
But the point is, it's something that

1538
01:06:33,586 --> 01:06:33,586
it's a kind of model that has existed for

1539
01:06:33,586 --> 01:06:33,586
a long time.

1540
01:06:26,516 --> 01:06:44,699
And so this sampling would be a way to

1541
01:06:44,699 --> 01:06:44,699
actually practically implement this and

1542
01:06:44,699 --> 01:06:44,699
scale it when you want to put that within

1543
01:06:44,699 --> 01:06:44,699
active inference.

1544
01:06:34,591 --> 01:06:49,740
So I think it's an important thing to

1545
01:06:49,740 --> 01:06:49,740
think about.

1546
01:06:45,700 --> 01:06:52,769
Daniel: Awesome.

1547
01:06:51,763 --> 01:06:52,774
Lance: Yeah.

1548
01:06:52,771 --> 01:06:58,832
Daniel: Thomas Parr, also recently in a

1549
01:06:58,832 --> 01:06:58,832
discussion on the textbook was sharing

1550
01:06:58,832 --> 01:06:58,832
some timelines.

1551
01:06:52,775 --> 01:07:13,927
And it's just so interesting how these

1552
01:07:13,927 --> 01:07:13,927
things have been developing and from

1553
01:07:13,927 --> 01:07:13,927
continuous through characterization of

1554
01:07:13,927 --> 01:07:13,927
the discrete, in a sense not culminating,

1555
01:07:13,927 --> 01:07:13,927
but being synthesized in your 2020

1556
01:07:13,927 --> 01:07:13,927
paper.

1557
01:06:58,833 --> 01:07:29,086
And now there's with an increased

1558
01:07:29,086 --> 01:07:29,086
emphasis on empirical data, the desire to

1559
01:07:29,086 --> 01:07:29,086
bring in a lot of these methods that

1560
01:07:29,086 --> 01:07:29,086
actually help us implement it rather than

1561
01:07:29,086 --> 01:07:29,086
just think about it, really

1562
01:07:29,086 --> 01:07:29,086
parsimoniously.

1563
01:07:13,928 --> 01:07:40,192
So what is your active inference

1564
01:07:40,192 --> 01:07:40,192
representation in the figure of the

1565
01:07:40,192 --> 01:07:40,192
running person?

1566
01:07:29,087 --> 01:07:49,283
And then how do the variables and the

1567
01:07:49,283 --> 01:07:49,283
processes described in this figure relate

1568
01:07:49,283 --> 01:07:49,283
to all of this?

1569
01:07:40,198 --> 01:07:54,334
5 hours of talking about Shadow

1570
01:07:54,334 --> 01:07:54,334
Hamiltonians.

1571
01:07:50,294 --> 01:07:56,359
Lance: And all of this, right?

1572
01:07:54,335 --> 01:07:59,383
Yeah, that's a broad question.

1573
01:07:57,361 --> 01:08:05,388
We might need another Daniel to answer

1574
01:08:05,388 --> 01:08:05,388
that in detail, but just very short.

1575
01:07:59,384 --> 01:08:15,485
Well, the active inference section was

1576
01:08:15,485 --> 01:08:15,485
meant to be as complete as possible, even

1577
01:08:15,485 --> 01:08:15,485
though it was very short.

1578
01:08:07,404 --> 01:08:25,584
And by completeness I mean that we

1579
01:08:25,584 --> 01:08:25,584
started by deriving active inference and

1580
01:08:25,584 --> 01:08:25,584
deriving active inference free energy

1581
01:08:25,584 --> 01:08:25,584
principle.

1582
01:08:15,487 --> 01:08:36,691
So this is really what the free energy

1583
01:08:36,691 --> 01:08:36,691
principle does, even though we weren't

1584
01:08:36,691 --> 01:08:36,691
able of course, to review the whole free

1585
01:08:36,691 --> 01:08:36,691
energy principle in like one or two

1586
01:08:36,691 --> 01:08:36,691
pages.

1587
01:08:26,591 --> 01:08:46,798
But what we focused on was deriving the

1588
01:08:46,798 --> 01:08:46,798
expected free energy free energy

1589
01:08:46,798 --> 01:08:46,798
principle and then from there you get the

1590
01:08:46,798 --> 01:08:46,798
full active inference algorithm.

1591
01:08:36,692 --> 01:09:00,877
So full active inference algorithm, I

1592
01:09:00,877 --> 01:09:00,877
think you just went past it's just below

1593
01:09:00,877 --> 01:09:00,877
if you go slightly below, I think maybe

1594
01:09:00,877 --> 01:09:00,877
page 29 or page 30 still, I think next

1595
01:09:00,877 --> 01:09:00,877
realizing an active agents.

1596
01:08:46,799 --> 01:09:09,969
So this is the active infrastructure

1597
01:09:09,969 --> 01:09:09,969
algorithm, like 0.12.3 or something, just

1598
01:09:09,969 --> 01:09:09,969
from a duration of the expected pre

1599
01:09:09,969 --> 01:09:09,969
energy.

1600
01:09:00,879 --> 01:09:16,033
You actually get as a corollary from

1601
01:09:16,033 --> 01:09:16,033
there, the active imprint algorithm that

1602
01:09:16,033 --> 01:09:16,033
we know and love.

1603
01:09:11,980 --> 01:09:24,118
This is actually a more general version

1604
01:09:24,118 --> 01:09:24,118
than what people use and I'm be excited

1605
01:09:24,118 --> 01:09:24,118
and I'm actually talking to people to

1606
01:09:24,118 --> 01:09:24,118
actually implement that.

1607
01:09:16,034 --> 01:09:42,289
But this is actually the most general

1608
01:09:42,289 --> 01:09:42,289
version that has been established in the

1609
01:09:42,289 --> 01:09:42,289
literature and it is more general in a

1610
01:09:42,289 --> 01:09:42,289
meaningful way in the sense that all the

1611
01:09:42,289 --> 01:09:42,289
beliefs, all the probability

1612
01:09:42,289 --> 01:09:42,289
distribution, they are over trajectories

1613
01:09:42,289 --> 01:09:42,289
or sequences of events.

1614
01:09:25,126 --> 01:09:58,450
So it means that all the computations,

1615
01:09:58,450 --> 01:09:58,450
they not only consider events at a

1616
01:09:58,450 --> 01:09:58,450
particular time in the future, for

1617
01:09:58,450 --> 01:09:58,450
example, but they consider trajectories

1618
01:09:58,450 --> 01:09:58,450
of sequences of events.

1619
01:09:44,312 --> 01:10:07,489
And so this is considering different

1620
01:10:07,489 --> 01:10:07,489
events at different points in time and

1621
01:10:07,489 --> 01:10:07,489
their independencies and dependencies

1622
01:10:07,489 --> 01:10:07,489
between them.

1623
01:09:58,451 --> 01:10:36,778
So this is just to say that if you would

1624
01:10:36,778 --> 01:10:36,778
take then this algorithm and you would

1625
01:10:36,778 --> 01:10:36,778
perform a mini field approximation over

1626
01:10:36,778 --> 01:10:36,778
time, which is saying that all the things

1627
01:10:36,778 --> 01:10:36,778
that you see in the future are sort of

1628
01:10:36,778 --> 01:10:36,778
independent of each other at different

1629
01:10:36,778 --> 01:10:36,778
points in time, then you would recover

1630
01:10:36,778 --> 01:10:36,778
what people typically use in the

1631
01:10:36,778 --> 01:10:36,778
literature, which is easily

1632
01:10:36,778 --> 01:10:36,778
implementable.

1633
01:10:09,509 --> 01:10:55,960
The point I want to make here is that you

1634
01:10:55,960 --> 01:10:55,960
can actually not have that limitation and

1635
01:10:55,960 --> 01:10:55,960
have things that are a bit more complex,

1636
01:10:55,960 --> 01:10:55,960
that are able to capture more complex

1637
01:10:55,960 --> 01:10:55,960
relationships in the time series and

1638
01:10:55,960 --> 01:10:55,960
input that.

1639
01:10:37,780 --> 01:11:02,972
They may be receiving, like, the kind of

1640
01:11:02,972 --> 01:11:02,972
sensory data that they may be receiving

1641
01:11:02,972 --> 01:11:02,972
and the kind of generative models that

1642
01:11:02,972 --> 01:11:02,972
they would have.

1643
01:10:55,961 --> 01:11:06,017
So anyway, this is the active inference

1644
01:11:06,017 --> 01:11:06,017
algorithm, the most general one.

1645
01:11:02,979 --> 01:11:12,075
We derive that from first principles by

1646
01:11:12,075 --> 01:11:12,075
deriving the expected community from

1647
01:11:12,075 --> 01:11:12,075
first principles.

1648
01:11:06,018 --> 01:11:17,129
And so this relates us to that figure

1649
01:11:17,129 --> 01:11:17,129
that you just showed.

1650
01:11:12,076 --> 01:11:28,230
So that figure that you showed with S on

1651
01:11:28,230 --> 01:11:28,230
A is the starting point of the free

1652
01:11:28,230 --> 01:11:28,230
energy principle described in this

1653
01:11:28,230 --> 01:11:28,230
paper.

1654
01:11:18,130 --> 01:11:48,438
And so the point is that we want to

1655
01:11:48,438 --> 01:11:48,438
describe decision making, want to

1656
01:11:48,438 --> 01:11:48,438
describe actions as a function of

1657
01:11:48,438 --> 01:11:48,438
sensations, and we want to come up with

1658
01:11:48,438 --> 01:11:48,438
the most general description of actions

1659
01:11:48,438 --> 01:11:48,438
as a function of sensations to be able to

1660
01:11:48,438 --> 01:11:48,438
account for everything.

1661
01:11:28,234 --> 01:11:51,462
That's kind of the idea of the pre

1662
01:11:51,462 --> 01:11:51,462
engineer principle.

1663
01:11:49,439 --> 01:11:59,546
So what do we do is we consider a world,

1664
01:11:59,546 --> 01:11:59,546
a world in which there is an environment

1665
01:11:59,546 --> 01:11:59,546
and an agent.

1666
01:11:52,471 --> 01:12:05,541
And so the environment here is denoted by

1667
01:12:05,541 --> 01:12:05,541
S, and S is a stochastic process.

1668
01:11:59,547 --> 01:12:07,560
Why a stochastic process?

1669
01:12:05,543 --> 01:12:14,634
Because a stochastic process is the most

1670
01:12:14,634 --> 01:12:14,634
general type of dynamic that exists, at

1671
01:12:14,634 --> 01:12:14,634
least as far as I know.

1672
01:12:08,572 --> 01:12:17,663
It's really a random dynamic.

1673
01:12:15,648 --> 01:12:19,683
And it could be random in all sorts of

1674
01:12:19,683 --> 01:12:19,683
ways.

1675
01:12:17,663 --> 01:12:21,700
It could also be non random.

1676
01:12:19,684 --> 01:12:31,807
We sort of take the stochastic, the

1677
01:12:31,807 --> 01:12:31,807
random aspect, as an extra ingredient,

1678
01:12:31,807 --> 01:12:31,807
just to include a lot more types of

1679
01:12:31,807 --> 01:12:31,807
scenarios and things you can be

1680
01:12:31,807 --> 01:12:31,807
confronted with.

1681
01:12:21,701 --> 01:12:40,893
So you partition the world into the

1682
01:12:40,893 --> 01:12:40,893
agent, which is O and A, and the

1683
01:12:40,893 --> 01:12:40,893
environment, which is S, and the agent.

1684
01:12:31,809 --> 01:12:45,944
Then you subdivide it into two more

1685
01:12:45,944 --> 01:12:45,944
components, which are O and A.

1686
01:12:40,894 --> 01:12:49,987
O are what we call the observable

1687
01:12:49,987 --> 01:12:49,987
states.

1688
01:12:45,946 --> 01:12:56,050
These are like the sensations of the

1689
01:12:56,050 --> 01:12:56,050
observations that you get at any point in

1690
01:12:56,050 --> 01:12:56,050
time, also a stochastic process.

1691
01:12:49,988 --> 01:12:59,087
And finally, you have the autonomous

1692
01:12:59,087 --> 01:12:59,087
states.

1693
01:12:56,056 --> 01:13:03,060
Or you could think about it loosely as

1694
01:13:03,060 --> 01:13:03,060
the active states.

1695
01:12:59,088 --> 01:13:05,089
They turn out to be the active states in

1696
01:13:05,089 --> 01:13:05,089
the implementation.

1697
01:13:03,061 --> 01:13:17,206
But so the active states, they're really

1698
01:13:17,206 --> 01:13:17,206
like the muscles, the things you can

1699
01:13:17,206 --> 01:13:17,206
actually activate and make and then

1700
01:13:17,206 --> 01:13:17,206
activate to influence the world.

1701
01:13:06,093 --> 01:13:28,310
So we just partitioned the world into

1702
01:13:28,310 --> 01:13:28,310
these three sets of states that interact

1703
01:13:28,310 --> 01:13:28,310
and evolve in some way.

1704
01:13:19,222 --> 01:13:31,347
They're both stochastic processes that

1705
01:13:31,347 --> 01:13:31,347
interact.

1706
01:13:28,311 --> 01:13:48,513
And the goal of the community principle,

1707
01:13:48,513 --> 01:13:48,513
at least when applied to decision making,

1708
01:13:48,513 --> 01:13:48,513
is that you want to describe A as a

1709
01:13:48,513 --> 01:13:48,513
function of O, because what happens as an

1710
01:13:48,513 --> 01:13:48,513
organism, you have control over A, you

1711
01:13:48,513 --> 01:13:48,513
have access.

1712
01:13:31,349 --> 01:13:52,550
To O, but you don't have direct control

1713
01:13:52,550 --> 01:13:52,550
of O over your sensations.

1714
01:13:48,513 --> 01:13:57,606
And you don't have access to S because

1715
01:13:57,606 --> 01:13:57,606
that's the environment and beyond the

1716
01:13:57,606 --> 01:13:57,606
markup.

1717
01:13:52,551 --> 01:14:01,580
Like it beyond you, beyond your

1718
01:14:01,580 --> 01:14:01,580
envelope.

1719
01:13:57,606 --> 01:14:02,596
So you don't have access to S.

1720
01:14:01,582 --> 01:14:07,647
You know, O and you can control A is what

1721
01:14:07,647 --> 01:14:07,647
you can choose from.

1722
01:14:02,596 --> 01:14:22,792
And so the free energy principle just

1723
01:14:22,792 --> 01:14:22,792
answers the question, okay, well, if I

1724
01:14:22,792 --> 01:14:22,792
take this very general description of the

1725
01:14:22,792 --> 01:14:22,792
world, what is the equation of A as a

1726
01:14:22,792 --> 01:14:22,792
function of O?

1727
01:14:07,648 --> 01:14:24,819
How can I describe A as a function of o?

1728
01:14:22,792 --> 01:14:31,886
And so it turns out there's some mild

1729
01:14:31,886 --> 01:14:31,886
assumptions in this.

1730
01:14:25,821 --> 01:14:43,001
And these assumptions, they're guided by

1731
01:14:43,001 --> 01:14:43,001
physical considerations about how humans

1732
01:14:43,001 --> 01:14:43,001
are and how humans interact with the

1733
01:14:43,001 --> 01:14:43,001
world, but they're very mild.

1734
01:14:31,886 --> 01:14:45,021
But whenever you take.

1735
01:14:43,006 --> 01:14:53,107
These assumptions, you get that the

1736
01:14:53,107 --> 01:14:53,107
active states or the autonomous states

1737
01:14:53,107 --> 01:14:53,107
minimize expected free energy.

1738
01:14:45,021 --> 01:15:01,125
So this is like a very 16th derivation of

1739
01:15:01,125 --> 01:15:01,125
the expected free energy.

1740
01:14:55,127 --> 01:15:04,157
From first principles, we start with the

1741
01:15:04,157 --> 01:15:04,157
partition of the world.

1742
01:15:01,125 --> 01:15:08,193
We describe active states as a function

1743
01:15:08,193 --> 01:15:08,193
of observations.

1744
01:15:04,158 --> 01:15:14,257
As it turns out, that the expected free

1745
01:15:14,257 --> 01:15:14,257
energy is what describes the active

1746
01:15:14,257 --> 01:15:14,257
states as a function of observations.

1747
01:15:08,195 --> 01:15:35,463
The basic active inference algorithm,

1748
01:15:35,463 --> 01:15:35,463
which everybody uses and which we have

1749
01:15:35,463 --> 01:15:35,463
also in paper, is about computing the

1750
01:15:35,463 --> 01:15:35,463
expected free energy and then selecting

1751
01:15:35,463 --> 01:15:35,463
actions that minimizes expected free

1752
01:15:35,463 --> 01:15:35,463
energy.

1753
01:15:19,303 --> 01:15:40,514
So the expected free energy, as we saw, I

1754
01:15:40,514 --> 01:15:40,514
think it's in the next figure.

1755
01:15:35,464 --> 01:15:46,570
So the one with all the panels, we've

1756
01:15:46,570 --> 01:15:46,570
seen it a couple of times already.

1757
01:15:40,518 --> 01:15:50,618
It's an expectation with posterior

1758
01:15:50,618 --> 01:15:50,618
distributions in it.

1759
01:15:46,578 --> 01:16:04,699
What I mean by posterior distributions

1760
01:16:04,699 --> 01:16:04,699
here is that these are distributions

1761
01:16:04,699 --> 01:16:04,699
conditioned on the history of the agent,

1762
01:16:04,699 --> 01:16:04,699
so on the observations that he has

1763
01:16:04,699 --> 01:16:04,699
already seen and on the action that he

1764
01:16:04,699 --> 01:16:04,699
has already taken.

1765
01:15:52,633 --> 01:16:15,802
In any case, the expected free energy is

1766
01:16:15,802 --> 01:16:15,802
an expectation with posterior

1767
01:16:15,802 --> 01:16:15,802
distributions within.

1768
01:16:08,733 --> 01:16:19,840
So it means that to compute the expected

1769
01:16:19,840 --> 01:16:19,840
free energy, you have two problems.

1770
01:16:15,803 --> 01:16:24,896
You have computing posterior distribution

1771
01:16:24,896 --> 01:16:24,896
through Bayes and inference, Bayesian

1772
01:16:24,896 --> 01:16:24,896
rule.

1773
01:16:19,841 --> 01:16:29,942
And once you have them, you can plug them

1774
01:16:29,942 --> 01:16:29,942
in into this equation.

1775
01:16:24,898 --> 01:16:34,996
And then you need to complete the the

1776
01:16:34,996 --> 01:16:34,996
expectation, and then you get the

1777
01:16:34,996 --> 01:16:34,996
expected free energy.

1778
01:16:29,942 --> 01:16:39,041
So the first thing is about computing the

1779
01:16:39,041 --> 01:16:39,041
posterior distributions.

1780
01:16:34,997 --> 01:16:46,114
Now, what has been proposed in the

1781
01:16:46,114 --> 01:16:46,114
literature so far is, well, how do you do

1782
01:16:46,114 --> 01:16:46,114
approximate inference?

1783
01:16:39,043 --> 01:16:48,135
How do you actually approximate

1784
01:16:48,135 --> 01:16:48,135
distributions?

1785
01:16:46,114 --> 01:16:53,186
You do that through variational inference

1786
01:16:53,186 --> 01:16:53,186
or approximate inference by minimizing

1787
01:16:53,186 --> 01:16:53,186
free energy.

1788
01:16:48,136 --> 01:17:05,246
The treatment here, the derivation, and

1789
01:17:05,246 --> 01:17:05,246
our aim was to do that, provide a

1790
01:17:05,246 --> 01:17:05,246
derivation that was as conceptually

1791
01:17:05,246 --> 01:17:05,246
simple as possible.

1792
01:16:55,207 --> 01:17:11,308
Actually, it highlights that the free

1793
01:17:11,308 --> 01:17:11,308
energy is not the most important thing

1794
01:17:11,308 --> 01:17:11,308
here.

1795
01:17:06,250 --> 01:17:15,342
The important thing is really the

1796
01:17:15,342 --> 01:17:15,342
expected free energy.

1797
01:17:12,316 --> 01:17:21,407
The free energy is just a tool to

1798
01:17:21,407 --> 01:17:21,407
approximate the posterior distributions,

1799
01:17:21,407 --> 01:17:21,407
to then get the expected free energy.

1800
01:17:15,343 --> 01:17:25,448
But you could actually use any other type

1801
01:17:25,448 --> 01:17:25,448
of divergence.

1802
01:17:21,409 --> 01:17:30,498
The free energy is just like a KL

1803
01:17:30,498 --> 01:17:30,498
divergence plus some term that makes the

1804
01:17:30,498 --> 01:17:30,498
whole thing tractable.

1805
01:17:26,453 --> 01:17:35,547
And so you can minimize the KL divergence

1806
01:17:35,547 --> 01:17:35,547
to approximate the target distribution.

1807
01:17:30,499 --> 01:17:42,618
But actually, what this view highlights,

1808
01:17:42,618 --> 01:17:42,618
and by the way, I'm not at all against

1809
01:17:42,618 --> 01:17:42,618
the free energy.

1810
01:17:36,550 --> 01:17:44,634
I think it's really cool.

1811
01:17:43,619 --> 01:17:47,666
And there's so many methods to minimize

1812
01:17:47,666 --> 01:17:47,666
free energy.

1813
01:17:45,641 --> 01:17:51,704
If you can do that, then fine and sure,

1814
01:17:51,704 --> 01:17:51,704
go for it.

1815
01:17:48,674 --> 01:17:52,715
Like, way to go.

1816
01:17:51,704 --> 01:17:54,737
But imagine you could not do that.

1817
01:17:52,717 --> 01:18:02,758
It would not at all be a problem because

1818
01:18:02,758 --> 01:18:02,758
you could use any other kind of

1819
01:18:02,758 --> 01:18:02,758
divergence to solve those inference

1820
01:18:02,758 --> 01:18:02,758
problems.

1821
01:17:54,738 --> 01:18:06,792
So that would be the first step.

1822
01:18:04,779 --> 01:18:15,880
You can approximate those particular

1823
01:18:15,880 --> 01:18:15,880
distributions by doing some kind of

1824
01:18:15,880 --> 01:18:15,880
approximate inference, which would be

1825
01:18:15,880 --> 01:18:15,880
through minimization of reenergy or

1826
01:18:15,880 --> 01:18:15,880
through minimization of something else.

1827
01:18:06,793 --> 01:18:19,925
And then you have the second step, which

1828
01:18:19,925 --> 01:18:19,925
is computing the expectation.

1829
01:18:15,886 --> 01:18:31,004
Now, either you're in a low number of

1830
01:18:31,004 --> 01:18:31,004
states, discrete states from the P like

1831
01:18:31,004 --> 01:18:31,004
thing and it's all a matter of vector

1832
01:18:31,004 --> 01:18:31,004
matrix multiplications that are

1833
01:18:31,004 --> 01:18:31,004
attractable.

1834
01:18:19,929 --> 01:18:44,017
Either you have a very high number of

1835
01:18:44,017 --> 01:18:44,017
discrete states and then you need to

1836
01:18:44,017 --> 01:18:44,017
think about sampling in discrete states,

1837
01:18:44,017 --> 01:18:44,017
which we didn't discuss in this paper.

1838
01:18:32,004 --> 01:18:55,028
Either you are in a continuous state

1839
01:18:55,028 --> 01:18:55,028
space and then you have an expectation in

1840
01:18:55,028 --> 01:18:55,028
continuous state space and then you want

1841
01:18:55,028 --> 01:18:55,028
to think about an alternative Monte

1842
01:18:55,028 --> 01:18:55,028
Carlo, for example.

1843
01:18:45,018 --> 01:19:08,035
So now with these two steps you have now

1844
01:19:08,035 --> 01:19:08,035
an estimate of the expected free energy

1845
01:19:08,035 --> 01:19:08,035
which gives you the quality of any action

1846
01:19:08,035 --> 01:19:08,035
sequence.

1847
01:18:56,029 --> 01:19:18,045
And so it's not only the quality, but

1848
01:19:18,045 --> 01:19:18,045
it's actually the negative Lock

1849
01:19:18,045 --> 01:19:18,045
probability of any action sequence

1850
01:19:18,045 --> 01:19:18,045
regarding in this formalism.

1851
01:19:08,035 --> 01:19:31,058
Because if you remember, the premise of

1852
01:19:31,058 --> 01:19:31,058
all this was to describe actions as

1853
01:19:31,058 --> 01:19:31,058
functional sensations in a physical

1854
01:19:31,058 --> 01:19:31,058
system, not even a physical system, but

1855
01:19:31,058 --> 01:19:31,058
in interacting stochastic processes.

1856
01:19:18,045 --> 01:19:46,073
And so the answer to that was well, the

1857
01:19:46,073 --> 01:19:46,073
expected free energy gives us how actions

1858
01:19:46,073 --> 01:19:46,073
relate to sensations, the expected free

1859
01:19:46,073 --> 01:19:46,073
energy.

1860
01:19:34,061 --> 01:20:06,087
And this is why we used the letter, we

1861
01:20:06,087 --> 01:20:06,087
used minus Lock p as opposed to G to

1862
01:20:06,087 --> 01:20:06,087
emphasize that the expected free energy

1863
01:20:06,087 --> 01:20:06,087
is not just like a function of action

1864
01:20:06,087 --> 01:20:06,087
sequences, but it is really the negative

1865
01:20:06,087 --> 01:20:06,087
log probability of an action sequence

1866
01:20:06,087 --> 01:20:06,087
given some sensations.

1867
01:19:46,073 --> 01:20:16,097
And so once you have computed the

1868
01:20:16,097 --> 01:20:16,097
expected free energy, you had this minus

1869
01:20:16,097 --> 01:20:16,097
log probability of an action sequence.

1870
01:20:07,088 --> 01:20:29,110
If you take the exponential negative of

1871
01:20:29,110 --> 01:20:29,110
that, the exponential negative of the

1872
01:20:29,110 --> 01:20:29,110
expected free energy, you get the

1873
01:20:29,110 --> 01:20:29,110
probability distribution over action

1874
01:20:29,110 --> 01:20:29,110
sequences.

1875
01:20:17,097 --> 01:20:38,119
And so by the way, this exponential

1876
01:20:38,119 --> 01:20:38,119
negative of expected free energy is what

1877
01:20:38,119 --> 01:20:38,119
we use all the time.

1878
01:20:32,113 --> 01:20:43,124
You might recognize this as the soft max

1879
01:20:43,124 --> 01:20:43,124
of negative expected free energy.

1880
01:20:38,119 --> 01:20:48,129
This is just all the time kind of

1881
01:20:48,129 --> 01:20:48,129
fundamental thing, active inference lab

1882
01:20:48,129 --> 01:20:48,129
models.

1883
01:20:43,124 --> 01:20:51,132
So I'm really talking about the same

1884
01:20:51,132 --> 01:20:51,132
thing here.

1885
01:20:49,130 --> 01:21:03,138
So once you take the soft max of negative

1886
01:21:03,138 --> 01:21:03,138
expected free energy, you get P of A,

1887
01:21:03,138 --> 01:21:03,138
which is the distribution over action

1888
01:21:03,138 --> 01:21:03,138
sequences.

1889
01:20:53,134 --> 01:21:05,140
And now you have two possibilities.

1890
01:21:03,138 --> 01:21:19,154
Either you want to stimulate the most

1891
01:21:19,154 --> 01:21:19,154
likely action sequence, in which case you

1892
01:21:19,154 --> 01:21:19,154
want to simulate the action sequence that

1893
01:21:19,154 --> 01:21:19,154
maximizes the probability distribution.

1894
01:21:06,141 --> 01:21:25,160
So in effect, you have an optimization

1895
01:21:25,160 --> 01:21:25,160
problem out of all.

1896
01:21:20,155 --> 01:21:39,174
Yeah, you need to find the action

1897
01:21:39,174 --> 01:21:39,174
sequence that is going to maximize this

1898
01:21:39,174 --> 01:21:39,174
probability distribution that's given by

1899
01:21:39,174 --> 01:21:39,174
the expected free energy or you want to

1900
01:21:39,174 --> 01:21:39,174
simulate a typical action.

1901
01:21:25,160 --> 01:21:41,176
So when do I mean a typical action?

1902
01:21:39,174 --> 01:21:49,184
A typical action or a typical action

1903
01:21:49,184 --> 01:21:49,184
sequence is just a sample from the

1904
01:21:49,184 --> 01:21:49,184
distribution.

1905
01:21:41,176 --> 01:22:04,193
And so if you want to sample from I mean,

1906
01:22:04,193 --> 01:22:04,193
if you want to do that, then you have a

1907
01:22:04,193 --> 01:22:04,193
sampling problem where again you need to

1908
01:22:04,193 --> 01:22:04,193
sample from a distribution over action

1909
01:22:04,193 --> 01:22:04,193
sequences given by the expected free

1910
01:22:04,193 --> 01:22:04,193
energy.

1911
01:21:49,184 --> 01:22:09,198
So this is really how all these

1912
01:22:09,198 --> 01:22:09,198
methodologies connect.

1913
01:22:05,194 --> 01:22:14,203
I think that's kind of it, yeah.

1914
01:22:13,202 --> 01:22:22,211
One last thing is, typically when we

1915
01:22:22,211 --> 01:22:22,211
stimulate active inference, we never do

1916
01:22:22,211 --> 01:22:22,211
the sampling at the end.

1917
01:22:14,203 --> 01:22:26,215
We always take the action sequence that

1918
01:22:26,215 --> 01:22:26,215
minimizes expected free energy.

1919
01:22:22,211 --> 01:22:35,224
In other words, the action sequence that

1920
01:22:35,224 --> 01:22:35,224
maximizes that probability distribution.

1921
01:22:29,218 --> 01:22:43,232
And this is because when we simulate

1922
01:22:43,232 --> 01:22:43,232
things, it turns out this is how people

1923
01:22:43,232 --> 01:22:43,232
have done it in the literature.

1924
01:22:35,224 --> 01:22:52,241
People are more interested in the most

1925
01:22:52,241 --> 01:22:52,241
likely action sequence that an organism

1926
01:22:52,241 --> 01:22:52,241
or an agent would produce.

1927
01:22:44,233 --> 01:23:08,251
But if you want to model data, if you

1928
01:23:08,251 --> 01:23:08,251
want to use active inference to model

1929
01:23:08,251 --> 01:23:08,251
data, then you actually want to not just

1930
01:23:08,251 --> 01:23:08,251
simulate the most likely action sequence,

1931
01:23:08,251 --> 01:23:08,251
but simulate any kind of action sequence

1932
01:23:08,251 --> 01:23:08,251
that would fall out of this.

1933
01:22:52,241 --> 01:23:16,259
And so this is really the case where you

1934
01:23:16,259 --> 01:23:16,259
need to sample from that final

1935
01:23:16,259 --> 01:23:16,259
distribution as opposed to do your

1936
01:23:16,259 --> 01:23:16,259
optimization.

1937
01:23:08,251 --> 01:23:27,270
So depending on the use case, you either

1938
01:23:27,270 --> 01:23:27,270
have a sampling or an optimization

1939
01:23:27,270 --> 01:23:27,270
problem after you've computed the

1940
01:23:27,270 --> 01:23:27,270
expected free energy.

1941
01:23:17,260 --> 01:23:30,273
So this is how these whole things fall

1942
01:23:30,273 --> 01:23:30,273
together.

1943
01:23:27,270 --> 01:23:35,278
And so you might be asking, okay, well,

1944
01:23:35,278 --> 01:23:35,278
you talked about sampling and

1945
01:23:35,278 --> 01:23:35,278
optimization.

1946
01:23:31,274 --> 01:23:37,280
There's also a section about inference.

1947
01:23:35,278 --> 01:23:39,282
Where does that fit in?

1948
01:23:38,281 --> 01:23:49,292
And so it fits in on the remark I gave

1949
01:23:49,292 --> 01:23:49,292
that we have these postered distributions

1950
01:23:49,292 --> 01:23:49,292
within the expected free energy.

1951
01:23:39,282 --> 01:23:51,294
How do we compute them?

1952
01:23:49,292 --> 01:23:52,295
How do we approximate them?

1953
01:23:51,294 --> 01:23:55,298
One way is by minimizing free energy.

1954
01:23:52,295 --> 01:23:59,302
Another way is by minimizing any other

1955
01:23:59,302 --> 01:23:59,302
kind of divergence.

1956
01:23:55,298 --> 01:24:14,311
And in that section over there, we just

1957
01:24:14,311 --> 01:24:14,311
reviewed some kind of divergences that

1958
01:24:14,311 --> 01:24:14,311
are very popular in the statistical

1959
01:24:14,311 --> 01:24:14,311
inference literature, mainly because they

1960
01:24:14,311 --> 01:24:14,311
have desirable properties.

1961
01:24:00,297 --> 01:24:32,329
And so if you weren't able to use the

1962
01:24:32,329 --> 01:24:32,329
variation of free energy for some reason,

1963
01:24:32,329 --> 01:24:32,329
or maybe something to be explored, and

1964
01:24:32,329 --> 01:24:32,329
something to be explored is just to use

1965
01:24:32,329 --> 01:24:32,329
other types of divergences and just see

1966
01:24:32,329 --> 01:24:32,329
what happens.

1967
01:24:15,312 --> 01:24:43,340
So the open problem to be explored is

1968
01:24:43,340 --> 01:24:43,340
whether we can get better performance by

1969
01:24:43,340 --> 01:24:43,340
using different kinds of divergences and

1970
01:24:43,340 --> 01:24:43,340
see what we get.

1971
01:24:34,331 --> 01:24:55,352
Maybe there's part of algorithms that are

1972
01:24:55,352 --> 01:24:55,352
out there with these other types of

1973
01:24:55,352 --> 01:24:55,352
divergences that we can make use of to do

1974
01:24:55,352 --> 01:24:55,352
better performance, to get better

1975
01:24:55,352 --> 01:24:55,352
performance.

1976
01:24:44,341 --> 01:25:02,353
Can we actually describe and quantify the

1977
01:25:02,353 --> 01:25:02,353
improvements in performance that we get

1978
01:25:02,353 --> 01:25:02,353
by using these other types of

1979
01:25:02,353 --> 01:25:02,353
algorithms?

1980
01:24:55,352 --> 01:25:06,357
When would these be appropriate and

1981
01:25:06,357 --> 01:25:06,357
useful?

1982
01:25:02,353 --> 01:25:08,359
These are all open questions.

1983
01:25:06,357 --> 01:25:22,373
Another open question that I think is

1984
01:25:22,373 --> 01:25:22,373
interesting is can we model maladaptive

1985
01:25:22,373 --> 01:25:22,373
behavior by using different kinds of

1986
01:25:22,373 --> 01:25:22,373
divergences that would not work as well

1987
01:25:22,373 --> 01:25:22,373
as the free energy?

1988
01:25:09,360 --> 01:25:29,380
And there's an interesting paper on that

1989
01:25:29,380 --> 01:25:29,380
by newer sajid and colleagues.

1990
01:25:22,373 --> 01:25:35,386
I think it's called Bayesian Brains and

1991
01:25:35,386 --> 01:25:35,386
the Rennie Divergence.

1992
01:25:29,380 --> 01:25:41,392
It's been published on neurocomputation,

1993
01:25:41,392 --> 01:25:41,392
I think, last year or the year before.

1994
01:25:35,386 --> 01:26:05,410
And so in that paper, instead of using

1995
01:26:05,410 --> 01:26:05,410
the variation of free energy to

1996
01:26:05,410 --> 01:26:05,410
approximate all these posterior

1997
01:26:05,410 --> 01:26:05,410
distributions, she used the rainy

1998
01:26:05,410 --> 01:26:05,410
divergence, which is which generalizes

1999
01:26:05,410 --> 01:26:05,410
the KL divergence in some way, and show

2000
01:26:05,410 --> 01:26:05,410
that for different rainy divergences, you

2001
01:26:05,410 --> 01:26:05,410
get different types of approximate

2002
01:26:05,410 --> 01:26:05,410
posteriors.

2003
01:25:42,393 --> 01:26:15,420
And she basically looked into what kind

2004
01:26:15,420 --> 01:26:15,420
of differences you get from there in

2005
01:26:15,420 --> 01:26:15,420
terms of perception, in terms of

2006
01:26:15,420 --> 01:26:15,420
decisionmaking.

2007
01:26:05,410 --> 01:26:29,434
And she showed that for that particular

2008
01:26:29,434 --> 01:26:29,434
divergence, divergence, I think the

2009
01:26:29,434 --> 01:26:29,434
conclusion was that you basically get

2010
01:26:29,434 --> 01:26:29,434
different phenomenology, you get

2011
01:26:29,434 --> 01:26:29,434
different behavior, suggest something to

2012
01:26:29,434 --> 01:26:29,434
be explored.

2013
01:26:15,420 --> 01:26:47,452
I think the upshot was that maybe I don't

2014
01:26:47,452 --> 01:26:47,452
remember how far the paper went in this,

2015
01:26:47,452 --> 01:26:47,452
but I think kind of the goal was to model

2016
01:26:47,452 --> 01:26:47,452
maladaptive behavior using some kinds of

2017
01:26:47,452 --> 01:26:47,452
rainy divergences that did not work as

2018
01:26:47,452 --> 01:26:47,452
well as the free energy.

2019
01:26:30,435 --> 01:26:52,457
So this is to say, so this is an

2020
01:26:52,457 --> 01:26:52,457
interesting work.

2021
01:26:48,452 --> 01:27:02,461
One could examine other kinds of

2022
01:27:02,461 --> 01:27:02,461
divergences and see whether you actually

2023
01:27:02,461 --> 01:27:02,461
get better or worse performance than with

2024
01:27:02,461 --> 01:27:02,461
the free energy.

2025
01:26:54,459 --> 01:27:14,473
One thing, one word of caution, though,

2026
01:27:14,473 --> 01:27:14,473
is the free energy is just so good in the

2027
01:27:14,473 --> 01:27:14,473
sense that it uses the KL divergence.

2028
01:27:03,462 --> 01:27:21,480
And as I mentioned at the beginning, the

2029
01:27:21,480 --> 01:27:21,480
KL divergence has so many properties and

2030
01:27:21,480 --> 01:27:21,480
it's just so fundamental.

2031
01:27:14,473 --> 01:27:27,486
It's not straightforward to see that when

2032
01:27:27,486 --> 01:27:27,486
you first come up with it.

2033
01:27:22,481 --> 01:27:42,501
But I guess the more I read in different

2034
01:27:42,501 --> 01:27:42,501
disciplines and the more the Chao

2035
01:27:42,501 --> 01:27:42,501
divergence comes up and the more the more

2036
01:27:42,501 --> 01:27:42,501
I see properties of Kale divergence in

2037
01:27:42,501 --> 01:27:42,501
different disciplines that just make it

2038
01:27:42,501 --> 01:27:42,501
so interesting.

2039
01:27:28,486 --> 01:27:46,505
Like, for example, the Kale divergence,

2040
01:27:46,505 --> 01:27:46,505
Bayesian Statistics Physics is the

2041
01:27:46,505 --> 01:27:46,505
relative entropy.

2042
01:27:42,501 --> 01:27:53,512
So it quantifies the amount of entropy

2043
01:27:53,512 --> 01:27:53,512
that a distribution has with respect to

2044
01:27:53,512 --> 01:27:53,512
another.

2045
01:27:47,506 --> 01:27:59,518
Entropy, as we know, is just a very

2046
01:27:59,518 --> 01:27:59,518
fundamental thing in information theory.

2047
01:27:53,512 --> 01:28:24,537
The KL divergence quantifies the

2048
01:28:24,537 --> 01:28:24,537
difference in information between two

2049
01:28:24,537 --> 01:28:24,537
distributions, the amount of bits that

2050
01:28:24,537 --> 01:28:24,537
you would need to if you take two

2051
01:28:24,537 --> 01:28:24,537
distributions, A and B, the KL between

2052
01:28:24,537 --> 01:28:24,537
the two quantifies the amount of

2053
01:28:24,537 --> 01:28:24,537
information that it takes to go from one

2054
01:28:24,537 --> 01:28:24,537
to the other.

2055
01:27:59,518 --> 01:28:30,543
So now you might object and say, okay,

2056
01:28:30,543 --> 01:28:30,543
but KL A and B is different from KL, B

2057
01:28:30,543 --> 01:28:30,543
and A.

2058
01:28:24,537 --> 01:28:41,554
And so how can it be that it quantifies

2059
01:28:41,554 --> 01:28:41,554
the amount of information they need to go

2060
01:28:41,554 --> 01:28:41,554
from A to B or B to A because it's not

2061
01:28:41,554 --> 01:28:41,554
symmetric.

2062
01:28:30,543 --> 01:28:50,563
So the answer is it's either KL of A and

2063
01:28:50,563 --> 01:28:50,563
B that has this meaning, or KL of B and A

2064
01:28:50,563 --> 01:28:50,563
that has this meaning.

2065
01:28:41,554 --> 01:29:00,567
And I just can never remember which

2066
01:29:00,567 --> 01:29:00,567
direction, but it's one of them that has

2067
01:29:00,567 --> 01:29:00,567
this interpretation in terms of the

2068
01:29:00,567 --> 01:29:00,567
difference in information anyway.

2069
01:28:50,563 --> 01:29:14,581
So Kia is just like something, it just

2070
01:29:14,581 --> 01:29:14,581
comes up everywhere and it's just so

2071
01:29:14,581 --> 01:29:14,581
useful, and it has all of these nice

2072
01:29:14,581 --> 01:29:14,581
properties which makes these free energy

2073
01:29:14,581 --> 01:29:14,581
really a construct of choice.

2074
01:29:01,568 --> 01:29:28,595
But that said, there's other divergences,

2075
01:29:28,595 --> 01:29:28,595
one of them which we describe a bit in

2076
01:29:28,595 --> 01:29:28,595
the paper called the maximum mean

2077
01:29:28,595 --> 01:29:28,595
discrepancy, which I think also has

2078
01:29:28,595 --> 01:29:28,595
properties.

2079
01:29:16,583 --> 01:29:37,604
It's not necessarily well, it's very

2080
01:29:37,604 --> 01:29:37,604
different in terms of how you construct

2081
01:29:37,604 --> 01:29:37,604
it from the KL divergence.

2082
01:29:28,595 --> 01:29:41,608
But I think my current understanding

2083
01:29:41,608 --> 01:29:41,608
right now.

2084
01:29:37,604 --> 01:29:49,616
And my current understanding right now is

2085
01:29:49,616 --> 01:29:49,616
that if you cannot use KL, then maximum

2086
01:29:49,616 --> 01:29:49,616
mean discrepancy is just like, be nice.

2087
01:29:41,608 --> 01:29:52,619
But yeah.

2088
01:29:51,618 --> 01:30:03,624
So the word of caution was that probably

2089
01:30:03,624 --> 01:30:03,624
most divergences out there are not going

2090
01:30:03,624 --> 01:30:03,624
to do as good of a job as a free energy,

2091
01:30:03,624 --> 01:30:03,624
but some clever ones might.

2092
01:29:52,619 --> 01:30:13,634
And it's an open problem of determining

2093
01:30:13,634 --> 01:30:13,634
which is how we get the link between all

2094
01:30:13,634 --> 01:30:13,634
these different sections.

2095
01:30:03,624 --> 01:30:17,638
So to do perception, we need the

2096
01:30:17,638 --> 01:30:17,638
inference.

2097
01:30:13,634 --> 01:30:25,646
To do decision making, that is, computing

2098
01:30:25,646 --> 01:30:25,646
the expected free energy, we need

2099
01:30:25,646 --> 01:30:25,646
sampling typically.

2100
01:30:18,639 --> 01:30:30,651
And then to do action selection, we

2101
01:30:30,651 --> 01:30:30,651
either need sampling or optimization.

2102
01:30:25,646 --> 01:30:35,656
And also inference is an optimization of

2103
01:30:35,656 --> 01:30:35,656
belief.

2104
01:30:30,651 --> 01:30:41,662
Updating also, as we've discussed, is

2105
01:30:41,662 --> 01:30:41,662
also an optimization of probability

2106
01:30:41,662 --> 01:30:41,662
distributions.

2107
01:30:36,657 --> 01:30:44,665
You can see that as an optimization of

2108
01:30:44,665 --> 01:30:44,665
beliefs as well.

2109
01:30:41,662 --> 01:30:46,667
So it's all very tightly interconnected.

2110
01:30:44,665 --> 01:30:51,671
Wow.

2111
01:30:50,671 --> 01:30:52,673
Daniel: Thank you, Lance.

2112
01:30:51,672 --> 01:30:55,676
That's very informative.

2113
01:30:52,673 --> 01:31:11,686
Brings up a lot of ways to go and it

2114
01:31:11,686 --> 01:31:11,686
really shines a different light on even

2115
01:31:11,686 --> 01:31:11,686
what, learning active inference or

2116
01:31:11,686 --> 01:31:11,686
learning free energy principle.

2117
01:30:56,677 --> 01:31:13,688
It was a roller coaster just listening.

2118
01:31:11,686 --> 01:31:31,706
When you describe the figure of the

2119
01:31:31,706 --> 01:31:31,706
running person and the new generalized

2120
01:31:31,706 --> 01:31:31,706
representation here, which is so sparse

2121
01:31:31,706 --> 01:31:31,706
it looks like it's pseudocode, but it's

2122
01:31:31,706 --> 01:31:31,706
actually basically necessary and

2123
01:31:31,706 --> 01:31:31,706
sufficient to describe action.

2124
01:31:14,689 --> 01:31:39,714
Lance: Yeah, it is pseudocode and it is

2125
01:31:39,714 --> 01:31:39,714
also exact like this is really what

2126
01:31:39,714 --> 01:31:39,714
active inference boils down to.

2127
01:31:33,708 --> 01:31:42,717
And that was kind of like where we got

2128
01:31:42,717 --> 01:31:42,717
at.

2129
01:31:39,714 --> 01:31:46,721
We're like, okay, well, we had this mess

2130
01:31:46,721 --> 01:31:46,721
of papers of active inference.

2131
01:31:42,717 --> 01:32:06,735
Not that they're actually immense, but,

2132
01:32:06,735 --> 01:32:06,735
you know, there's so much information and

2133
01:32:06,735 --> 01:32:06,735
we're really thinking, reading all the

2134
01:32:06,735 --> 01:32:06,735
latest free energy principle literature

2135
01:32:06,735 --> 01:32:06,735
that I've also worked on a lot and also

2136
01:32:06,735 --> 01:32:06,735
active inference lab literature and

2137
01:32:06,735 --> 01:32:06,735
really thinking, okay, well, how do we

2138
01:32:06,735 --> 01:32:06,735
strip all that of the neuroscience?

2139
01:31:46,721 --> 01:32:08,737
How do we strip all that of the cognitive

2140
01:32:08,737 --> 01:32:08,737
science?

2141
01:32:06,735 --> 01:32:13,742
How do we just retain the math and

2142
01:32:13,742 --> 01:32:13,742
present it in the simplest way possible?

2143
01:32:08,737 --> 01:32:18,747
That would be appealing for mathematician

2144
01:32:18,747 --> 01:32:18,747
and also hopefully for a computer

2145
01:32:18,747 --> 01:32:18,747
scientist.

2146
01:32:13,742 --> 01:32:25,754
It turns out that we got a way simpler

2147
01:32:25,754 --> 01:32:25,754
perspective than anything that's out

2148
01:32:25,754 --> 01:32:25,754
there, I think.

2149
01:32:19,748 --> 01:32:27,756
And really?

2150
01:32:26,755 --> 01:32:27,756
Yeah.

2151
01:32:27,756 --> 01:32:35,764
So this is the active inference algorithm

2152
01:32:35,764 --> 01:32:35,764
in pseudocode and in detail also in a

2153
01:32:35,764 --> 01:32:35,764
way.

2154
01:32:27,756 --> 01:32:38,767
Daniel: So totally agree.

2155
01:32:37,766 --> 01:32:48,777
Again, thanks for the work and for

2156
01:32:48,777 --> 01:32:48,777
sharing it this way because it is the

2157
01:32:48,777 --> 01:32:48,777
fewest pixels for the highest resolution

2158
01:32:48,777 --> 01:32:48,777
picture.

2159
01:32:38,767 --> 01:33:10,793
Then to describe the fundamental

2160
01:33:10,793 --> 01:33:10,793
cybernetic challenge as a partition, an

2161
01:33:10,793 --> 01:33:10,793
axiomatic partition, which one can then

2162
01:33:10,793 --> 01:33:10,793
say also has grounding in the spatial

2163
01:33:10,793 --> 01:33:10,793
temporal boundaries of the world or in

2164
01:33:10,793 --> 01:33:10,793
geometric boundaries in informational

2165
01:33:10,793 --> 01:33:10,793
spaces.

2166
01:32:50,779 --> 01:33:29,812
But the particular partition is used to

2167
01:33:29,812 --> 01:33:29,812
separate in the map, not necessarily

2168
01:33:29,812 --> 01:33:29,812
making claims about the territory and the

2169
01:33:29,812 --> 01:33:29,812
actual nature of the objects and their

2170
01:33:29,812 --> 01:33:29,812
articulations or anatomy, but on the map,

2171
01:33:29,812 --> 01:33:29,812
which we get to construct.

2172
01:33:10,793 --> 01:33:39,822
We make it in a way that's amenable to

2173
01:33:39,822 --> 01:33:39,822
the particular partition, which is not

2174
01:33:39,822 --> 01:33:39,822
too much more than the separation of

2175
01:33:39,822 --> 01:33:39,822
figure from ground or agent based

2176
01:33:39,822 --> 01:33:39,822
modeling.

2177
01:33:30,813 --> 01:33:45,828
It's just a separation of some autonomous

2178
01:33:45,828 --> 01:33:45,828
entity from some external process.

2179
01:33:39,822 --> 01:34:12,849
Then the task of the free energy

2180
01:34:12,849 --> 01:34:12,849
principle applied to decision making, as

2181
01:34:12,849 --> 01:34:12,849
you said, was to describe action as a

2182
01:34:12,849 --> 01:34:12,849
function of observation and everything in

2183
01:34:12,849 --> 01:34:12,849
between is broadly considered cognitive

2184
01:34:12,849 --> 01:34:12,849
but for any given system it's going to

2185
01:34:12,849 --> 01:34:12,849
play out in this immensely nuanced way

2186
01:34:12,849 --> 01:34:12,849
with a lot of bespoke mechanisms.

2187
01:33:47,830 --> 01:34:16,853
And why do we take that particular

2188
01:34:16,853 --> 01:34:16,853
partition step?

2189
01:34:12,849 --> 01:34:31,868
Well, it's kind of like read, write

2190
01:34:31,868 --> 01:34:31,868
access in the computer system there's

2191
01:34:31,868 --> 01:34:31,868
things we don't have access to hidden

2192
01:34:31,868 --> 01:34:31,868
states and also the reverse of that which

2193
01:34:31,868 --> 01:34:31,868
is we can't access hidden states nor them

2194
01:34:31,868 --> 01:34:31,868
influence or affect us.

2195
01:34:16,853 --> 01:34:34,871
So I think of that as like no

2196
01:34:34,871 --> 01:34:34,871
telekinesis, no telepathy.

2197
01:34:31,868 --> 01:34:37,874
You can't go directly across the

2198
01:34:37,874 --> 01:34:37,874
blanket.

2199
01:34:35,871 --> 01:34:41,878
You have to intermediate through the

2200
01:34:41,878 --> 01:34:41,878
blanket of observation and action.

2201
01:34:37,874 --> 01:34:54,891
And then with respect to our particular

2202
01:34:54,891 --> 01:34:54,891
states, our blanket and internal states,

2203
01:34:54,891 --> 01:34:54,891
we have access to observations, but not

2204
01:34:54,891 --> 01:34:54,891
direct control, nor necessarily would we

2205
01:34:54,891 --> 01:34:54,891
want to.

2206
01:34:42,879 --> 01:35:09,900
Because if we had the lever to change

2207
01:35:09,900 --> 01:35:09,900
what we directly received, then

2208
01:35:09,900 --> 01:35:09,900
algorithms might learn strategies that

2209
01:35:09,900 --> 01:35:09,900
basically self deceive so that the

2210
01:35:09,900 --> 01:35:09,900
observations look good, look good, look

2211
01:35:09,900 --> 01:35:09,900
good, until the whole system crashes.

2212
01:34:54,891 --> 01:35:13,904
So we want access to observations but not

2213
01:35:13,904 --> 01:35:13,904
direct control.

2214
01:35:09,900 --> 01:35:24,915
And then the autonomous states are what

2215
01:35:24,915 --> 01:35:24,915
we in the optimal ideal situation have

2216
01:35:24,915 --> 01:35:24,915
total control of which is like our mind

2217
01:35:24,915 --> 01:35:24,915
and our body, what we think and what we

2218
01:35:24,915 --> 01:35:24,915
do.

2219
01:35:13,904 --> 01:35:40,931
It turns out through the pragmatic turn

2220
01:35:40,931 --> 01:35:40,931
and the inactivist insights in cognitive

2221
01:35:40,931 --> 01:35:40,931
science that a lot of action sequences

2222
01:35:40,931 --> 01:35:40,931
have to do with changing what

2223
01:35:40,931 --> 01:35:40,931
observations are sought after like

2224
01:35:40,931 --> 01:35:40,931
epistemic affordance.

2225
01:35:24,915 --> 01:35:48,939
So there is an enmeshment of action but

2226
01:35:48,939 --> 01:35:48,939
it's also really important that we have

2227
01:35:48,939 --> 01:35:48,939
access but not control of observations.

2228
01:35:40,931 --> 01:36:05,950
We don't want to have our control on the

2229
01:36:05,950 --> 01:36:05,950
thermometer but we want the best possible

2230
01:36:05,950 --> 01:36:05,950
thermometer and then we want to control

2231
01:36:05,950 --> 01:36:05,950
the best possible interpretation that's

2232
01:36:05,950 --> 01:36:05,950
like signal processing and the best

2233
01:36:05,950 --> 01:36:05,950
possible action sequence which is like

2234
01:36:05,950 --> 01:36:05,950
decision making and control theory.

2235
01:35:48,939 --> 01:36:15,960
And then free energy principle is

2236
01:36:15,960 --> 01:36:15,960
addressing that question what is the

2237
01:36:15,960 --> 01:36:15,960
equation of action as a function of

2238
01:36:15,960 --> 01:36:15,960
observation?

2239
01:36:05,950 --> 01:36:24,969
But then it was quite a rollercoaster

2240
01:36:24,969 --> 01:36:24,969
when you said that the free energy wasn't

2241
01:36:24,969 --> 01:36:24,969
even necessarily the only way to do it.

2242
01:36:17,962 --> 01:36:26,971
But it's absolutely true.

2243
01:36:25,970 --> 01:36:41,986
The properties of the free energy are

2244
01:36:41,986 --> 01:36:41,986
inherited from one of the terms being KL

2245
01:36:41,986 --> 01:36:41,986
divergence and the other having the

2246
01:36:41,986 --> 01:36:41,986
ability to basically ignore in certain

2247
01:36:41,986 --> 01:36:41,986
relative expected free energy calculation

2248
01:36:41,986 --> 01:36:41,986
contexts.

2249
01:36:27,972 --> 01:36:48,993
So then in that situation differences in

2250
01:36:48,993 --> 01:36:48,993
free energy do come down to differences

2251
01:36:48,993 --> 01:36:48,993
in the KL which does have all these

2252
01:36:48,993 --> 01:36:48,993
properties.

2253
01:36:41,986 --> 01:37:06,005
But that doesn't mean that the free

2254
01:37:06,005 --> 01:37:06,005
energy is itself axiomatically posited

2255
01:37:06,005 --> 01:37:06,005
it's actually downstream of the

2256
01:37:06,005 --> 01:37:06,005
particular partition to use free energy

2257
01:37:06,005 --> 01:37:06,005
or anything like that at all and other

2258
01:37:06,005 --> 01:37:06,005
discrepancies may have other properties

2259
01:37:06,005 --> 01:37:06,005
in different ways.

2260
01:36:48,993 --> 01:37:08,007
Lance: Exactly.

2261
01:37:08,007 --> 01:37:22,021
There's a bit of a nuance in the sense

2262
01:37:22,021 --> 01:37:22,021
that here we presented version of the

2263
01:37:22,021 --> 01:37:22,021
free energy principle, just describing

2264
01:37:22,021 --> 01:37:22,021
decision making as you summarize though

2265
01:37:22,021 --> 01:37:22,021
describing A as a function of O.

2266
01:37:10,009 --> 01:37:31,029
And by the way, about that, we didn't

2267
01:37:31,029 --> 01:37:31,029
even talk about Markov blanket in this

2268
01:37:31,029 --> 01:37:31,029
paper.

2269
01:37:25,024 --> 01:37:40,039
I would say that the markup blanket is

2270
01:37:40,039 --> 01:37:40,039
under the hood because we're saying,

2271
01:37:40,039 --> 01:37:40,039
okay, well, these are the states that you

2272
01:37:40,039 --> 01:37:40,039
have access to and these are the states

2273
01:37:40,039 --> 01:37:40,039
that you do not.

2274
01:37:32,031 --> 01:37:43,042
The states that you have access to are A

2275
01:37:43,042 --> 01:37:43,042
and O.

2276
01:37:40,039 --> 01:37:44,043
The states are the agent.

2277
01:37:43,042 --> 01:37:47,046
The states that you do not have access to

2278
01:37:47,046 --> 01:37:47,046
are the environment.

2279
01:37:44,043 --> 01:37:54,053
So in some sense there is a markup

2280
01:37:54,053 --> 01:37:54,053
blanket, but we didn't even have to

2281
01:37:54,053 --> 01:37:54,053
mention that in the paper.

2282
01:37:47,046 --> 01:38:03,056
It's just you partition the world into

2283
01:38:03,056 --> 01:38:03,056
three sets of states, S, O and A, that

2284
01:38:03,056 --> 01:38:03,056
are by definition A.

2285
01:37:54,053 --> 01:38:11,064
And you just yeah, just from this trip

2286
01:38:11,064 --> 01:38:11,064
partition, you want to describe one as a

2287
01:38:11,064 --> 01:38:11,064
function of another, ignoring the third.

2288
01:38:04,057 --> 01:38:17,070
And so that's how you derive expected

2289
01:38:17,070 --> 01:38:17,070
green energy.

2290
01:38:13,066 --> 01:38:23,076
You see that action sequences are

2291
01:38:23,076 --> 01:38:23,076
described by the expected free energy.

2292
01:38:17,070 --> 01:38:34,087
And this is a functional sensations if we

2293
01:38:34,087 --> 01:38:34,087
want further in the and so this is just

2294
01:38:34,087 --> 01:38:34,087
what you need crucially, this is just

2295
01:38:34,087 --> 01:38:34,087
what you need to derive.

2296
01:38:23,076 --> 01:38:45,098
Active Inference Lab Algorithm if we went

2297
01:38:45,098 --> 01:38:45,098
further in reviewing the free energy

2298
01:38:45,098 --> 01:38:45,098
principle, then we would see the

2299
01:38:45,098 --> 01:38:45,098
fundamental role that the free energy

2300
01:38:45,098 --> 01:38:45,098
plays.

2301
01:38:34,087 --> 01:38:59,112
And so actually, when reading the latest

2302
01:38:59,112 --> 01:38:59,112
papers on the mathematical theory of the

2303
01:38:59,112 --> 01:38:59,112
free energy principle, the role of the

2304
01:38:59,112 --> 01:38:59,112
variation of free energy is pretty

2305
01:38:59,112 --> 01:38:59,112
clear.

2306
01:38:48,101 --> 01:39:13,120
But here the point is that if we just

2307
01:39:13,120 --> 01:39:13,120
care about decision making, if we just

2308
01:39:13,120 --> 01:39:13,120
care about the normal active inference

2309
01:39:13,120 --> 01:39:13,120
algorithm, then we don't even need the

2310
01:39:13,120 --> 01:39:13,120
variation of free energy.

2311
01:39:01,108 --> 01:39:16,123
So sure, the free energy should be

2312
01:39:16,123 --> 01:39:16,123
preferred.

2313
01:39:13,120 --> 01:39:19,126
Why not, if it is available?

2314
01:39:16,123 --> 01:39:24,131
But if it is not for some reason, then

2315
01:39:24,131 --> 01:39:24,131
there's no reason why not to use another

2316
01:39:24,131 --> 01:39:24,131
kind of divergence.

2317
01:39:19,126 --> 01:39:33,140
Daniel: Very interesting.

2318
01:39:32,139 --> 01:39:52,159
It's making me think about linear

2319
01:39:52,159 --> 01:39:52,159
regression and the sum of squares, the L

2320
01:39:52,159 --> 01:39:52,159
two norm is one approach that's often

2321
01:39:52,159 --> 01:39:52,159
used to fit a regression line because it

2322
01:39:52,159 --> 01:39:52,159
has good optimization properties, there's

2323
01:39:52,159 --> 01:39:52,159
good software packages, there's good

2324
01:39:52,159 --> 01:39:52,159
education, there's good communication

2325
01:39:52,159 --> 01:39:52,159
around it and so on.

2326
01:39:34,141 --> 01:40:00,161
But one can select other norms and choose

2327
01:40:00,161 --> 01:40:00,161
to fit a linear regression with an L one

2328
01:40:00,161 --> 01:40:00,161
norm or with an L three norm.

2329
01:39:52,159 --> 01:40:10,171
And so that entire question of fitting

2330
01:40:10,171 --> 01:40:10,171
the linear aggression is a degree of

2331
01:40:10,171 --> 01:40:10,171
freedom.

2332
01:40:01,162 --> 01:40:21,182
How the regression is fit, that's

2333
01:40:21,182 --> 01:40:21,182
downstream of a commitment to, for

2334
01:40:21,182 --> 01:40:21,182
example, model a system in a generalized

2335
01:40:21,182 --> 01:40:21,182
linear modeling framework.

2336
01:40:10,171 --> 01:40:36,197
And so analogously, the upstream

2337
01:40:36,197 --> 01:40:36,197
commitment or the first principles which

2338
01:40:36,197 --> 01:40:36,197
yes, can be understood as axiomatic and

2339
01:40:36,197 --> 01:40:36,197
also have some empirical status in terms

2340
01:40:36,197 --> 01:40:36,197
of this partitioning.

2341
01:40:21,182 --> 01:40:43,204
A figure from ground the particular

2342
01:40:43,204 --> 01:40:43,204
partition can simply be accepted

2343
01:40:43,204 --> 01:40:43,204
axiomatically, which is to say without

2344
01:40:43,204 --> 01:40:43,204
appeal to evidence.

2345
01:40:36,197 --> 01:40:51,212
Or somebody might have another upstream

2346
01:40:51,212 --> 01:40:51,212
axiom and choose to model things

2347
01:40:51,212 --> 01:40:51,212
according to a particular partition.

2348
01:40:43,204 --> 01:41:18,233
From there, just like we could have

2349
01:41:18,233 --> 01:41:18,233
chosen the L one, two, three norm there

2350
01:41:18,233 --> 01:41:18,233
are different discrepancy criteria or

2351
01:41:18,233 --> 01:41:18,233
measures that we might want to use, and

2352
01:41:18,233 --> 01:41:18,233
some of them apply better or worse or not

2353
01:41:18,233 --> 01:41:18,233
at all, depending on what software,

2354
01:41:18,233 --> 01:41:18,233
hardware, data set, and generative model

2355
01:41:18,233 --> 01:41:18,233
we have.

2356
01:40:52,213 --> 01:41:20,235
And so it makes sense.

2357
01:41:19,234 --> 01:41:33,248
Pull back into the upstream understanding

2358
01:41:33,248 --> 01:41:33,248
active inference lab as a process theory

2359
01:41:33,248 --> 01:41:33,248
for particular partitioned systems.

2360
01:41:22,237 --> 01:41:52,267
And then for those who want to engage in

2361
01:41:52,267 --> 01:41:52,267
the modeling to have that discussion

2362
01:41:52,267 --> 01:41:52,267
about the garden of the branching paths,

2363
01:41:52,267 --> 01:41:52,267
well, you could use a discrete time or

2364
01:41:52,267 --> 01:41:52,267
you could use a continuous time, and then

2365
01:41:52,267 --> 01:41:52,267
from here, you could do this sampling.

2366
01:41:34,249 --> 01:41:53,268
Or you could do that one.

2367
01:41:52,267 --> 01:41:56,271
And if we have this computer access, we

2368
01:41:56,271 --> 01:41:56,271
can do that.

2369
01:41:53,268 --> 01:41:58,273
But if we have to do it this way, we'll

2370
01:41:58,273 --> 01:41:58,273
do it like that.

2371
01:41:56,271 --> 01:42:15,284
And that's all operational and

2372
01:42:15,284 --> 01:42:15,284
logistical, but it's actually all under

2373
01:42:15,284 --> 01:42:15,284
the umbrella or under the auspice of the

2374
01:42:15,284 --> 01:42:15,284
theoretical or conceptual commitments

2375
01:42:15,284 --> 01:42:15,284
that are actually not being questioned

2376
01:42:15,284 --> 01:42:15,284
once one is in that modeling discussion.

2377
01:41:58,273 --> 01:42:22,291
Just like you could have the L one two

2378
01:42:22,291 --> 01:42:22,291
three norm conversation, and maybe a

2379
01:42:22,291 --> 01:42:22,291
reviewer asks you why you chose the two

2380
01:42:22,291 --> 01:42:22,291
norm versus the three.

2381
01:42:15,284 --> 01:42:30,299
But it's a broader level of questioning

2382
01:42:30,299 --> 01:42:30,299
why one took on the linear modeling

2383
01:42:30,299 --> 01:42:30,299
framework at all.

2384
01:42:22,291 --> 01:42:40,309
And our analogous upstream bottleneck,

2385
01:42:40,309 --> 01:42:40,309
not in terms of rate limiting, but just

2386
01:42:40,309 --> 01:42:40,309
in terms of like, eye of the needle is

2387
01:42:40,309 --> 01:42:40,309
the particular partition.

2388
01:42:30,299 --> 01:42:45,314
Lance: Yes, definitely.

2389
01:42:44,313 --> 01:42:55,324
And I just want to add something actually

2390
01:42:55,324 --> 01:42:55,324
about the choice of divergences or the

2391
01:42:55,324 --> 01:42:55,324
choice of discrepancy that you might want

2392
01:42:55,324 --> 01:42:55,324
to use to solve the inference problem.

2393
01:42:45,314 --> 01:42:59,328
I think the analogy with linear

2394
01:42:59,328 --> 01:42:59,328
regression is a really good one.

2395
01:42:56,325 --> 01:43:09,332
When you do linear regression, yeah, you

2396
01:43:09,332 --> 01:43:09,332
can use all types of norms, and it's

2397
01:43:09,332 --> 01:43:09,332
really a design choice here in the

2398
01:43:09,332 --> 01:43:09,332
algorithm, you also have that design

2399
01:43:09,332 --> 01:43:09,332
choice that you're going to use KL.

2400
01:42:59,328 --> 01:43:12,335
So free energy to do active inference.

2401
01:43:09,332 --> 01:43:24,347
By the way, the inferences are really

2402
01:43:24,347 --> 01:43:24,347
equation 43 and 44, which are approximate

2403
01:43:24,347 --> 01:43:24,347
actually, you had them in the slide.

2404
01:43:14,337 --> 01:43:29,352
Daniel: Already, 43 and 44 here, got it.

2405
01:43:24,347 --> 01:43:36,359
Lance: Which are approximate some

2406
01:43:36,359 --> 01:43:36,359
posterior distribution with an

2407
01:43:36,359 --> 01:43:36,359
approximate posterior distribution.

2408
01:43:30,353 --> 01:43:43,366
So you can do that with the pre energy,

2409
01:43:43,366 --> 01:43:43,366
which is the same as the KL divergence,

2410
01:43:43,366 --> 01:43:43,366
or you could do that with a whole bunch

2411
01:43:43,366 --> 01:43:43,366
of other divergences.

2412
01:43:36,359 --> 01:43:49,372
One that I mentioned and that I think is

2413
01:43:49,372 --> 01:43:49,372
particularly interesting is the maximum

2414
01:43:49,372 --> 01:43:49,372
mean discrepancy.

2415
01:43:43,366 --> 01:44:00,377
And so here's the difference between KL

2416
01:44:00,377 --> 01:44:00,377
and maximum dean discrepancy, at least, I

2417
01:44:00,377 --> 01:44:00,377
guess, an important conceptual

2418
01:44:00,377 --> 01:44:00,377
difference.

2419
01:43:49,372 --> 01:44:17,394
So the KL, when you look at distributions

2420
01:44:17,394 --> 01:44:17,394
that are very close to each other, it's

2421
01:44:17,394 --> 01:44:17,394
basically going to become symmetric when

2422
01:44:17,394 --> 01:44:17,394
the distributions are very close, and

2423
01:44:17,394 --> 01:44:17,394
it's going to measure the amount of

2424
01:44:17,394 --> 01:44:17,394
information that differs between them.

2425
01:44:00,377 --> 01:44:34,411
The maximum mean discrepancy, when you

2426
01:44:34,411 --> 01:44:34,411
take two distributions that are very

2427
01:44:34,411 --> 01:44:34,411
close, it reduces to what people call the

2428
01:44:34,411 --> 01:44:34,411
Earth movers distance, also called like

2429
01:44:34,411 --> 01:44:34,411
the vanishing distance so here's an

2430
01:44:34,411 --> 01:44:34,411
intuition.

2431
01:44:18,395 --> 01:44:45,422
If you take two distributions that are

2432
01:44:45,422 --> 01:44:45,422
very close, just imagine distribution A

2433
01:44:45,422 --> 01:44:45,422
as a pack of dirt and distribution B as a

2434
01:44:45,422 --> 01:44:45,422
pack of dirt or a sand with some shape.

2435
01:44:34,411 --> 01:45:01,432
The maximum mean discrepancy is going to

2436
01:45:01,432 --> 01:45:01,432
tell you the amount of work that you need

2437
01:45:01,432 --> 01:45:01,432
to put all that dirt from distribution A

2438
01:45:01,432 --> 01:45:01,432
and pile it in the shape of distribution

2439
01:45:01,432 --> 01:45:01,432
B.

2440
01:44:46,423 --> 01:45:17,448
So of course, there's so many different

2441
01:45:17,448 --> 01:45:17,448
ways in which you could take all that

2442
01:45:17,448 --> 01:45:17,448
dirt from distribution A and remodel it

2443
01:45:17,448 --> 01:45:17,448
in distribution B, but you're interested

2444
01:45:17,448 --> 01:45:17,448
in the minimal energetic cost that would

2445
01:45:17,448 --> 01:45:17,448
take.

2446
01:45:01,432 --> 01:45:22,453
So like the optimal way of doing that

2447
01:45:22,453 --> 01:45:22,453
movement, people call that optimal

2448
01:45:22,453 --> 01:45:22,453
transport.

2449
01:45:17,448 --> 01:45:45,476
Now, if you're familiar with optimal

2450
01:45:45,476 --> 01:45:45,476
transport, you know that the Vaster Stein

2451
01:45:45,476 --> 01:45:45,476
distance is a distance between

2452
01:45:45,476 --> 01:45:45,476
probability distribution that regardless

2453
01:45:45,476 --> 01:45:45,476
of how far they are, is going to measure,

2454
01:45:45,476 --> 01:45:45,476
is going to give you this cost of optimal

2455
01:45:45,476 --> 01:45:45,476
transfer.

2456
01:45:23,454 --> 01:45:55,486
So the energetic cost of moving all that

2457
01:45:55,486 --> 01:45:55,486
pack of dirt from place A to place B in

2458
01:45:55,486 --> 01:45:55,486
the optimal way.

2459
01:45:45,476 --> 01:46:00,485
So the vaster shine distance is something

2460
01:46:00,485 --> 01:46:00,485
that we could use here.

2461
01:45:56,487 --> 01:46:09,494
But maximum mean discrepancy, it has a

2462
01:46:09,494 --> 01:46:09,494
lot of very nice properties and basically

2463
01:46:09,494 --> 01:46:09,494
reduced it to the Vaster Stein distance.

2464
01:46:02,487 --> 01:46:34,519
When we consider very close

2465
01:46:34,519 --> 01:46:34,519
distributions, this is not something that

2466
01:46:34,519 --> 01:46:34,519
just a mathematical curiosity, but it's

2467
01:46:34,519 --> 01:46:34,519
done that when one builds a distance out

2468
01:46:34,519 --> 01:46:34,519
of a divergence, what one does is one

2469
01:46:34,519 --> 01:46:34,519
takes very close distributions, measures

2470
01:46:34,519 --> 01:46:34,519
the divergence between them, at which

2471
01:46:34,519 --> 01:46:34,519
point the divergence is pretty much

2472
01:46:34,519 --> 01:46:34,519
symmetrical.

2473
01:46:09,494 --> 01:46:39,524
And then you basically keep adding

2474
01:46:39,524 --> 01:46:39,524
divergences along the trajectory until

2475
01:46:39,524 --> 01:46:39,524
you get to the final one.

2476
01:46:34,519 --> 01:46:53,538
And this way, by aggregating divergences

2477
01:46:53,538 --> 01:46:53,538
between very close distributions and

2478
01:46:53,538 --> 01:46:53,538
doing that and adding that along a

2479
01:46:53,538 --> 01:46:53,538
trajectory, you get a distance, a

2480
01:46:53,538 --> 01:46:53,538
meaningful distance between distributions

2481
01:46:53,538 --> 01:46:53,538
that could be very far.

2482
01:46:39,524 --> 01:47:14,553
If you do that with the Kale divergence,

2483
01:47:14,553 --> 01:47:14,553
you get what's called the fisher

2484
01:47:14,553 --> 01:47:14,553
information distance, which measures

2485
01:47:14,553 --> 01:47:14,553
really the amount of information that

2486
01:47:14,553 --> 01:47:14,553
took you from to go from one distribution

2487
01:47:14,553 --> 01:47:14,553
to another distribution, regardless of

2488
01:47:14,553 --> 01:47:14,553
how far they are.

2489
01:46:54,539 --> 01:47:20,559
If you do that with the vast search time

2490
01:47:20,559 --> 01:47:20,559
distance, you will get the optimal

2491
01:47:20,559 --> 01:47:20,559
transport cost.

2492
01:47:15,554 --> 01:47:27,566
So the amount of energy that you need to

2493
01:47:27,566 --> 01:47:27,566
put all that dirt, place A to place B in

2494
01:47:27,566 --> 01:47:27,566
the optimal way.

2495
01:47:20,559 --> 01:47:34,573
If you do that with the maximum

2496
01:47:34,573 --> 01:47:34,573
discrepancy, you would also get that you

2497
01:47:34,573 --> 01:47:34,573
will also get the optimal transport

2498
01:47:34,573 --> 01:47:34,573
thing.

2499
01:47:27,566 --> 01:47:51,590
I just want to say that the maximum

2500
01:47:51,590 --> 01:47:51,590
discrepancy discrepancy between two

2501
01:47:51,590 --> 01:47:51,590
distributions that are far away will not

2502
01:47:51,590 --> 01:47:51,590
coincide with the Vaster Stein distance,

2503
01:47:51,590 --> 01:47:51,590
which gives you this optimal transport

2504
01:47:51,590 --> 01:47:51,590
cost.

2505
01:47:38,577 --> 01:48:10,603
But when you actually derive a distance

2506
01:48:10,603 --> 01:48:10,603
from these divergences, the distance

2507
01:48:10,603 --> 01:48:10,603
derived from the maximum discrepancy and

2508
01:48:10,603 --> 01:48:10,603
the Vaster Stein distance will end up

2509
01:48:10,603 --> 01:48:10,603
being the same and the distance that you

2510
01:48:10,603 --> 01:48:10,603
actually get or the topology that is

2511
01:48:10,603 --> 01:48:10,603
derived from the distance.

2512
01:47:52,591 --> 01:48:15,608
So by topology, a topology is really a

2513
01:48:15,608 --> 01:48:15,608
notion of closeness.

2514
01:48:10,603 --> 01:48:22,615
And so to understand closeness, you need

2515
01:48:22,615 --> 01:48:22,615
to understand infinitesimal distances.

2516
01:48:16,609 --> 01:48:28,621
So the infinitesimal distances derived

2517
01:48:28,621 --> 01:48:28,621
from Vaster Stein or Mmd, they're the

2518
01:48:28,621 --> 01:48:28,621
same.

2519
01:48:23,616 --> 01:48:38,631
The topology that results is the topology

2520
01:48:38,631 --> 01:48:38,631
of what people call weak convergence,

2521
01:48:38,631 --> 01:48:38,631
which is a standard topology that's

2522
01:48:38,631 --> 01:48:38,631
considered in probability theory.

2523
01:48:28,621 --> 01:49:00,647
So Mmd and Vaster Stein, they are very

2524
01:49:00,647 --> 01:49:00,647
natural in that sense, in the sense that

2525
01:49:00,647 --> 01:49:00,647
people say that they met rise, weak

2526
01:49:00,647 --> 01:49:00,647
conversions, like they give rise to the

2527
01:49:00,647 --> 01:49:00,647
standard topology between probability

2528
01:49:00,647 --> 01:49:00,647
distributions.

2529
01:48:40,633 --> 01:49:15,662
So coming back to the choice of

2530
01:49:15,662 --> 01:49:15,662
divergences, if you're interested in

2531
01:49:15,662 --> 01:49:15,662
approximating distributions in the sense

2532
01:49:15,662 --> 01:49:15,662
of approximating their information

2533
01:49:15,662 --> 01:49:15,662
content, then KL is very natural.

2534
01:49:02,649 --> 01:49:34,681
But if you were for some reason and maybe

2535
01:49:34,681 --> 01:49:34,681
would not be in this kind of application,

2536
01:49:34,681 --> 01:49:34,681
but another kind of application, if for

2537
01:49:34,681 --> 01:49:34,681
some reason you're interested in

2538
01:49:34,681 --> 01:49:34,681
approximating distributions for the sake

2539
01:49:34,681 --> 01:49:34,681
of how close they are when you look at

2540
01:49:34,681 --> 01:49:34,681
them, then you wouldn't use Mmd or

2541
01:49:34,681 --> 01:49:34,681
Vassagetein.

2542
01:49:15,662 --> 01:49:41,688
Daniel: Wow, great information.

2543
01:49:36,683 --> 01:49:48,695
And I'm just thinking about we're

2544
01:49:48,695 --> 01:49:48,695
switching lanes on the highway we're

2545
01:49:48,695 --> 01:49:48,695
trying to get from here to there.

2546
01:49:41,688 --> 01:50:03,704
Yes, we want to know about the

2547
01:50:03,704 --> 01:50:03,704
informational closeness of this tale of

2548
01:50:03,704 --> 01:50:03,704
two densities, but we also want to know

2549
01:50:03,704 --> 01:50:03,704
about the transport closeness because we

2550
01:50:03,704 --> 01:50:03,704
have a schedule and a budget and

2551
01:50:03,704 --> 01:50:03,704
decisions to make, and there are trade

2552
01:50:03,704 --> 01:50:03,704
offs.

2553
01:49:49,696 --> 01:50:15,716
So being able to move the Earth optimally

2554
01:50:15,716 --> 01:50:15,716
while we're switching lanes and

2555
01:50:15,716 --> 01:50:15,716
accelerating and slowing down, I know

2556
01:50:15,716 --> 01:50:15,716
this is mixing many angles informally.

2557
01:50:03,704 --> 01:50:31,732
We want to have a lot of options for how

2558
01:50:31,732 --> 01:50:31,732
to think about that challenge of moving

2559
01:50:31,732 --> 01:50:31,732
dirt between the tail of two densities

2560
01:50:31,732 --> 01:50:31,732
and make sure everybody is driving in the

2561
01:50:31,732 --> 01:50:31,732
right lane at the right time.

2562
01:50:16,717 --> 01:50:41,742
Lance: Definitely, yeah, it's a design

2563
01:50:41,742 --> 01:50:41,742
choice and it's an important one because

2564
01:50:41,742 --> 01:50:41,742
it depends on what kind of properties we

2565
01:50:41,742 --> 01:50:41,742
want to preserve.

2566
01:50:33,734 --> 01:50:54,755
All these divergences, some are not so

2567
01:50:54,755 --> 01:50:54,755
useful, I guess, but some of them,

2568
01:50:54,755 --> 01:50:54,755
they're just very natural and vasterstein

2569
01:50:54,755 --> 01:50:54,755
and Mmd, they're very natural in that

2570
01:50:54,755 --> 01:50:54,755
sense.

2571
01:50:43,744 --> 01:50:58,759
Very important to keep in mind.

2572
01:50:56,757 --> 01:51:06,761
And not only active inference lab, but

2573
01:51:06,761 --> 01:51:06,761
just in general for any kind of inference

2574
01:51:06,761 --> 01:51:06,761
problem.

2575
01:50:58,759 --> 01:51:21,776
Daniel: Well, let us each have a closing

2576
01:51:21,776 --> 01:51:21,776
round or reflections, any thoughts or any

2577
01:51:21,776 --> 01:51:21,776
next steps or any suggestions or other

2578
01:51:21,776 --> 01:51:21,776
information you'd like to provide.

2579
01:51:09,764 --> 01:51:25,780
Lance: It's hard to say.

2580
01:51:24,779 --> 01:51:29,784
I feel we've covered so much already.

2581
01:51:25,780 --> 01:51:47,802
I think to me, what's most exciting about

2582
01:51:47,802 --> 01:51:47,802
is scaling active inference right now,

2583
01:51:47,802 --> 01:51:47,802
because you get this active inference

2584
01:51:47,802 --> 01:51:47,802
algorithm in the paper that's the right

2585
01:51:47,802 --> 01:51:47,802
compares principles and just from the

2586
01:51:47,802 --> 01:51:47,802
derivation, you see.

2587
01:51:31,786 --> 01:51:55,810
Okay, well, there's actually so many

2588
01:51:55,810 --> 01:51:55,810
things I dean, the assumptions are so

2589
01:51:55,810 --> 01:51:55,810
small that there are so many things that

2590
01:51:55,810 --> 01:51:55,810
you can model with this active inference

2591
01:51:55,810 --> 01:51:55,810
algorithm.

2592
01:51:47,802 --> 01:52:02,811
And all the heavy lifting is done by the

2593
01:52:02,811 --> 01:52:02,811
generative model.

2594
01:51:59,814 --> 01:52:06,815
So if you use one generative model, you

2595
01:52:06,815 --> 01:52:06,815
will get one behavior.

2596
01:52:02,811 --> 01:52:11,820
If you use another generative model, you

2597
01:52:11,820 --> 01:52:11,820
will get another behavior, but all the

2598
01:52:11,820 --> 01:52:11,820
equations will remain the same.

2599
01:52:06,815 --> 01:52:16,825
It speaks to the generality.

2600
01:52:14,823 --> 01:52:20,829
Now in having something that's very

2601
01:52:20,829 --> 01:52:20,829
general, you.

2602
01:52:16,825 --> 01:52:22,831
Get something that's also very non

2603
01:52:22,831 --> 01:52:22,831
specific.

2604
01:52:20,829 --> 01:52:36,845
And I guess for neuroscientists and

2605
01:52:36,845 --> 01:52:36,845
people who are interested in

2606
01:52:36,845 --> 01:52:36,845
intelligence, generality comes at a cost

2607
01:52:36,845 --> 01:52:36,845
of being non specific and non specific

2608
01:52:36,845 --> 01:52:36,845
about the brain in general.

2609
01:52:23,832 --> 01:52:46,855
So really the big question to me long

2610
01:52:46,855 --> 01:52:46,855
term is what kind of generative models do

2611
01:52:46,855 --> 01:52:46,855
we need to simulate brain like behavior?

2612
01:52:37,846 --> 01:52:50,859
Because this is really the interesting

2613
01:52:50,859 --> 01:52:50,859
behavior or the most interesting

2614
01:52:50,859 --> 01:52:50,859
behavior.

2615
01:52:46,855 --> 01:52:58,867
So it speaks to a big research program

2616
01:52:58,867 --> 01:52:58,867
that a lot of people are carrying and

2617
01:52:58,867 --> 01:52:58,867
have been carrying for a long time.

2618
01:52:51,860 --> 01:53:05,868
But it's about what kind of

2619
01:53:05,868 --> 01:53:05,868
representations do we have of the world?

2620
01:52:58,867 --> 01:53:07,870
What kind of priors do we have?

2621
01:53:05,868 --> 01:53:13,876
Can we identify the priors that we are

2622
01:53:13,876 --> 01:53:13,876
born with?

2623
01:53:08,871 --> 01:53:27,890
There's a lot of research on

2624
01:53:27,890 --> 01:53:27,890
computational or just playing cognitive

2625
01:53:27,890 --> 01:53:27,890
science, like normal cognitive science,

2626
01:53:27,890 --> 01:53:27,890
just studying babies and seeing what kind

2627
01:53:27,890 --> 01:53:27,890
of priors, what kind of basic information

2628
01:53:27,890 --> 01:53:27,890
they have when they come out of the

2629
01:53:27,890 --> 01:53:27,890
womb.

2630
01:53:13,876 --> 01:53:30,893
There's a lot of things that they can

2631
01:53:30,893 --> 01:53:30,893
already do there.

2632
01:53:28,891 --> 01:53:41,904
Our genetic code is preconditioning us to

2633
01:53:41,904 --> 01:53:41,904
operate efficiently in this world and be

2634
01:53:41,904 --> 01:53:41,904
able to flexibly adapt to any kind of

2635
01:53:41,904 --> 01:53:41,904
situations that might arise in the

2636
01:53:41,904 --> 01:53:41,904
natural world.

2637
01:53:30,893 --> 01:53:47,910
And so it's a huge research program to

2638
01:53:47,910 --> 01:53:47,910
understand and model these priors.

2639
01:53:41,904 --> 01:53:56,918
And not only the priors, but also the

2640
01:53:56,918 --> 01:53:56,918
likelihood, all the representation, all

2641
01:53:56,918 --> 01:53:56,918
the state spaces.

2642
01:53:51,914 --> 01:53:59,922
So I think that's the way forward.

2643
01:53:56,919 --> 01:54:08,925
The creation principle is very elegant

2644
01:54:08,925 --> 01:54:08,925
because it gives you a very succinct

2645
01:54:08,925 --> 01:54:08,925
description in terms of a giant model

2646
01:54:08,925 --> 01:54:08,925
that enables you to simulate pretty much

2647
01:54:08,925 --> 01:54:08,925
everything.

2648
01:53:59,922 --> 01:54:11,928
But we're not interested in simulating

2649
01:54:11,928 --> 01:54:11,928
anything.

2650
01:54:08,925 --> 01:54:15,932
We're interested in simulating brain like

2651
01:54:15,932 --> 01:54:15,932
behavior.

2652
01:54:12,929 --> 01:54:22,939
And now we need to drill down even more

2653
01:54:22,939 --> 01:54:22,939
onto the kind of Janitor models that

2654
01:54:22,939 --> 01:54:22,939
would be amenable to that.

2655
01:54:15,932 --> 01:54:27,944
Daniel: Great.

2656
01:54:27,944 --> 01:54:38,955
My closing reflection I feel like I know

2657
01:54:38,955 --> 01:54:38,955
less about fep, but more about something

2658
01:54:38,955 --> 01:54:38,955
else.

2659
01:54:29,946 --> 01:54:41,957
For what we've discussed.

2660
01:54:38,955 --> 01:54:53,970
Earth was moved, Bayesian mechanics were

2661
01:54:53,970 --> 01:54:53,970
called in, decisions were made, and it's

2662
01:54:53,970 --> 01:54:53,970
been a really great series.

2663
01:54:42,959 --> 01:55:02,973
So I'm appreciative and thankful that you

2664
01:55:02,973 --> 01:55:02,973
suggested this paper in our

2665
01:55:02,973 --> 01:55:02,973
correspondence as one to discuss.

2666
01:54:53,970 --> 01:55:11,982
You were absolutely right that it is

2667
01:55:11,982 --> 01:55:11,982
relevant to bring to the attention of the

2668
01:55:11,982 --> 01:55:11,982
active community.

2669
01:55:03,974 --> 01:55:24,995
And I hope that everybody who reads or

2670
01:55:24,995 --> 01:55:24,995
listens thus far has shifted lanes so

2671
01:55:24,995 --> 01:55:24,995
they can be where they want to be too.

2672
01:55:11,982 --> 01:55:26,997
Lance: Well, yeah.

2673
01:55:25,996 --> 01:55:29,999
Thank you so much.

2674
01:55:27,998 --> 01:55:34,004
I also really enjoyed the session and

2675
01:55:34,004 --> 01:55:34,004
think we had a really cool discussion.

2676
01:55:29,000 --> 01:55:35,006
So thanks again.

2677
01:55:34,005 --> 01:55:38,009
I hope to do this again soon.

2678
01:55:35,006 --> 01:55:39,010
Daniel: Excellent.

2679
01:55:38,009 --> 01:55:40,011
Anytime you'd like.

2680
01:55:39,010 --> 01:55:41,012
Very well.

2681
01:55:40,011 --> 01:55:43,014
See you, Lance.


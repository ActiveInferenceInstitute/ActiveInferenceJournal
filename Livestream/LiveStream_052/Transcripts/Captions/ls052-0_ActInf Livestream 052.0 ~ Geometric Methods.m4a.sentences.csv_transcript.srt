1
00:00:00,000 --> 00:00:14,513
Daniel Friedman: Hello and welcome.

2
00:00:13,405 --> 00:00:22,371
This is ActInf Lab Livestream number

3
00:00:22,371 --> 00:00:22,371
52.0, and it is February 28, 2023.

4
00:00:14,531 --> 00:00:27,881
Welcome to the active inference

5
00:00:27,881 --> 00:00:27,881
institute.

6
00:00:25,624 --> 00:00:36,705
We're a participatory online institute

7
00:00:36,705 --> 00:00:36,705
that is communicating, learning and

8
00:00:36,705 --> 00:00:36,705
practicing applied active coherence.

9
00:00:28,898 --> 00:00:39,999
You can find us at the links here on this

10
00:00:39,999 --> 00:00:39,999
slide.

11
00:00:36,747 --> 00:00:45,629
This is a recorded and an archived live

12
00:00:45,629 --> 00:00:45,629
stream, so please provide feedback so we

13
00:00:45,629 --> 00:00:45,629
can improve our work.

14
00:00:39,015 --> 00:00:51,255
All backgrounds and perspectives are

15
00:00:51,255 --> 00:00:51,255
welcome and we'll be following video

16
00:00:51,255 --> 00:00:51,255
etiquette for live streams.

17
00:00:46,698 --> 00:01:02,784
Head over active coherence.org if you

18
00:01:02,784 --> 00:01:02,784
want to learn more and participate in

19
00:01:02,784 --> 00:01:02,784
learning groups and projects at the

20
00:01:02,784 --> 00:01:02,784
institute, including these live streams.

21
00:00:52,299 --> 00:01:36,173
Well, we're here today to learn and

22
00:01:36,173 --> 00:01:36,173
discuss the paper Geometric Methods for

23
00:01:36,173 --> 00:01:36,173
Sampling, Optimization, Inference and

24
00:01:36,173 --> 00:01:36,173
Adaptive Agents by Alessandro Barp

25
00:01:36,173 --> 00:01:36,173
Lancelot Costa Guilherme Franca Karl

26
00:01:36,173 --> 00:01:36,173
Friston Mark Girolami Michael Jordan

27
00:01:36,173 --> 00:01:36,173
Grigorios A Pavliotis this video, like

28
00:01:36,173 --> 00:01:36,173
all Zero videos, are is and introduction

29
00:01:36,173 --> 00:01:36,173
for some of the ideas.

30
00:01:04,962 --> 00:01:44,973
It is not a review or a final word, and

31
00:01:44,973 --> 00:01:44,973
as we have joked before, it's more than

32
00:01:44,973 --> 00:01:44,973
anything a call for help.

33
00:01:36,178 --> 00:02:03,124
So if you're curious about these topics,

34
00:02:03,124 --> 00:02:03,124
if you're knowledgeable about these

35
00:02:03,124 --> 00:02:03,124
topics, we would really look forward to

36
00:02:03,124 --> 00:02:03,124
you getting involved in the upcoming 52.1

37
00:02:03,124 --> 00:02:03,124
and 52.2 discussions, as well as in an

38
00:02:03,124 --> 00:02:03,124
ongoing basis to help us understand some

39
00:02:03,124 --> 00:02:03,124
of the technical details.

40
00:01:44,987 --> 00:02:24,331
This is going to get technical ant times

41
00:02:24,331 --> 00:02:24,331
and certainly beyond the technicalities I

42
00:02:24,331 --> 00:02:24,331
understand, though I am looking forward

43
00:02:24,331 --> 00:02:24,331
to presenting them and it'll be great to

44
00:02:24,331 --> 00:02:24,331
have those who have backgrounds of all

45
00:02:24,331 --> 00:02:24,331
different types to come together and talk

46
00:02:24,331 --> 00:02:24,331
about this awesome work.

47
00:02:03,126 --> 00:02:32,412
Also, a big thanks to Ali and Candon for

48
00:02:32,412 --> 00:02:32,412
the assistance, technical and moral.

49
00:02:24,338 --> 00:02:50,593
During the preparations here in 52.0,

50
00:02:50,593 --> 00:02:50,593
we're going to bring up some aims and

51
00:02:50,593 --> 00:02:50,593
claims, read the abstract, look at the

52
00:02:50,593 --> 00:02:50,593
roadmap and talk about the keywords, and

53
00:02:50,593 --> 00:02:50,593
then we will walk through the paper with

54
00:02:50,593 --> 00:02:50,593
an emphasis on the figures, formalisms

55
00:02:50,593 --> 00:02:50,593
and key points.

56
00:02:32,413 --> 00:02:55,642
In the coming weeks, we're going to be

57
00:02:55,642 --> 00:02:55,642
discussing this paper with one or more

58
00:02:55,642 --> 00:02:55,642
authors.

59
00:02:50,598 --> 00:03:03,666
So, as usual, get in touch if you want to

60
00:03:03,666 --> 00:03:03,666
participate or if it's after the fact,

61
00:03:03,666 --> 00:03:03,666
you can still get involved.

62
00:02:55,644 --> 00:03:08,715
We can start with an introduction or a

63
00:03:08,715 --> 00:03:08,715
warm up.

64
00:03:05,684 --> 00:03:22,856
I'm Daniel, I'm a researcher in

65
00:03:22,856 --> 00:03:22,856
California, and I'm tempted to say, just

66
00:03:22,856 --> 00:03:22,856
totally honestly, I'm happy to get this

67
00:03:22,856 --> 00:03:22,856
one over with, but that sounds a little

68
00:03:22,856 --> 00:03:22,856
bit different than I might intended to

69
00:03:22,856 --> 00:03:22,856
be.

70
00:03:08,718 --> 00:03:32,950
I'm really excited to dive into this

71
00:03:32,950 --> 00:03:32,950
work, which is going to be approaching

72
00:03:32,950 --> 00:03:32,950
active inference from an angle that we

73
00:03:32,950 --> 00:03:32,950
haven't necessarily highlighted on these

74
00:03:32,950 --> 00:03:32,950
streams.

75
00:03:22,857 --> 00:03:35,980
So I think it's going to be a fascinating

76
00:03:35,980 --> 00:03:35,980
discussion.

77
00:03:32,951 --> 00:03:46,092
It's going to run the gamut, span the gap

78
00:03:46,092 --> 00:03:46,092
however you choose to see it, between

79
00:03:46,092 --> 00:03:46,092
technical sophistication and intuition,

80
00:03:46,092 --> 00:03:46,092
which is a great place to be.

81
00:03:35,980 --> 00:03:57,204
So I've been really excited and motivated

82
00:03:57,204 --> 00:03:57,204
to prepare, and I'm looking forward to

83
00:03:57,204 --> 00:03:57,204
the dot zero we're doing right now and to

84
00:03:57,204 --> 00:03:57,204
the upcoming discussions.

85
00:03:46,093 --> 00:04:08,256
So let's jump in with the big questions

86
00:04:08,256 --> 00:04:08,256
that might motivate one to read this

87
00:04:08,256 --> 00:04:08,256
paper, this kind of paper, even if they

88
00:04:08,256 --> 00:04:08,256
were not familiar with the authors or

89
00:04:08,256 --> 00:04:08,256
topics.

90
00:03:58,210 --> 00:04:11,282
And these are just a few ways to write it

91
00:04:11,282 --> 00:04:11,282
up.

92
00:04:08,257 --> 00:04:12,299
Of course, not the only ways.

93
00:04:11,283 --> 00:04:22,396
So, first question how can we effectively

94
00:04:22,396 --> 00:04:22,396
and efficiently navigate information

95
00:04:22,396 --> 00:04:22,396
geometric or information theoretic

96
00:04:22,396 --> 00:04:22,396
landscapes?

97
00:04:13,300 --> 00:04:35,523
And how can we tackle that question from

98
00:04:35,523 --> 00:04:35,523
an analytical, which is to say equation

99
00:04:35,523 --> 00:04:35,523
based as well as a computational, which

100
00:04:35,523 --> 00:04:35,523
is to say real implication based

101
00:04:35,523 --> 00:04:35,523
perspective?

102
00:04:22,397 --> 00:04:46,633
Often it's really fun, intuitive, natural

103
00:04:46,633 --> 00:04:46,633
to talk about information theory, even

104
00:04:46,633 --> 00:04:46,633
for those who haven't taken the technical

105
00:04:46,633 --> 00:04:46,633
prerequisites.

106
00:04:35,528 --> 00:05:01,727
And this work may help us navigate to a

107
00:05:01,727 --> 00:05:01,727
space where we're able to think with good

108
00:05:01,727 --> 00:05:01,727
intuitions about information geometry,

109
00:05:01,727 --> 00:05:01,727
information theory, and also make sure

110
00:05:01,727 --> 00:05:01,727
that those intuitions are going to be

111
00:05:01,727 --> 00:05:01,727
caught by our technical tools.

112
00:04:46,635 --> 00:05:14,850
Second, how can we optimize inference in

113
00:05:14,850 --> 00:05:14,850
complexity models, including cases where

114
00:05:14,850 --> 00:05:14,850
we are doing inference on action as a

115
00:05:14,850 --> 00:05:14,850
parameter, also known active inference

116
00:05:14,850 --> 00:05:14,850
lab?

117
00:05:03,743 --> 00:05:29,005
Optimization and functional analysis, as

118
00:05:29,005 --> 00:05:29,005
we'll come to see soon has long been

119
00:05:29,005 --> 00:05:29,005
interested in these complex or

120
00:05:29,005 --> 00:05:29,005
challenging models that are right at the

121
00:05:29,005 --> 00:05:29,005
margin, right ant the border of what is

122
00:05:29,005 --> 00:05:29,005
trackable or not.

123
00:05:14,859 --> 00:05:52,235
Given the computational hardware that

124
00:05:52,235 --> 00:05:52,235
modelers have access to, for example, big

125
00:05:52,235 --> 00:05:52,235
data sets, high dimensional state spaces,

126
00:05:52,235 --> 00:05:52,235
complex dynamics and so on, how about

127
00:05:52,235 --> 00:05:52,235
cases where we're also interested in

128
00:05:52,235 --> 00:05:52,235
doing parametric inference and

129
00:05:52,235 --> 00:05:52,235
optimization on action or action plans as

130
00:05:52,235 --> 00:05:52,235
a parameter?

131
00:05:29,006 --> 00:05:54,258
Also known active inference lab.

132
00:05:52,237 --> 00:06:07,320
And third, what technical underpinnings

133
00:06:07,320 --> 00:06:07,320
support the rigor and applicability of

134
00:06:07,320 --> 00:06:07,320
active inference and what special cases

135
00:06:07,320 --> 00:06:07,320
are revealed when we consider

136
00:06:07,320 --> 00:06:07,320
possibilities and constraints?

137
00:05:55,262 --> 00:06:12,377
These are those two branches of

138
00:06:12,377 --> 00:06:12,377
analytical and computational coming back

139
00:06:12,377 --> 00:06:12,377
again.

140
00:06:07,326 --> 00:06:24,491
We want to make sure that when we're

141
00:06:24,491 --> 00:06:24,491
thinking within the active inference

142
00:06:24,491 --> 00:06:24,491
paradigm, the active coherence ontology

143
00:06:24,491 --> 00:06:24,491
is the language that we're speaking and

144
00:06:24,491 --> 00:06:24,491
that there's a technical rigor to what's

145
00:06:24,491 --> 00:06:24,491
being discussed.

146
00:06:12,379 --> 00:06:34,590
Additionally, we want to make sure that

147
00:06:34,590 --> 00:06:34,590
it's not just an analytical rigor, but

148
00:06:34,590 --> 00:06:34,590
it's also going to be computationally

149
00:06:34,590 --> 00:06:34,590
plausible tractable or maybe even

150
00:06:34,590 --> 00:06:34,590
preferable.

151
00:06:24,493 --> 00:06:35,605
That would be awesome.

152
00:06:34,591 --> 00:06:39,643
Those are the questions that at least

153
00:06:39,643 --> 00:06:39,643
appeared.

154
00:06:36,609 --> 00:06:53,788
We are discussing this paper Geometric

155
00:06:53,788 --> 00:06:53,788
Methods for Sampling, Optimization,

156
00:06:53,788 --> 00:06:53,788
Inference and Adaptive Agents by Barp and

157
00:06:53,788 --> 00:06:53,788
Costa at all shared first authorship and

158
00:06:53,788 --> 00:06:53,788
it was published on active.

159
00:06:40,653 --> 00:06:58,832
I'll just note a few key claims from this

160
00:06:58,832 --> 00:06:58,832
paper.

161
00:06:54,790 --> 00:07:11,900
They wrote Our goal in this chapter is to

162
00:07:11,900 --> 00:07:11,900
discuss the emergence of natural

163
00:07:11,900 --> 00:07:11,900
geometries within a few important areas

164
00:07:11,900 --> 00:07:11,900
of statistics and applied mathematics,

165
00:07:11,900 --> 00:07:11,900
namely optimization, sampling, inference

166
00:07:11,900 --> 00:07:11,900
and adaptive agents.

167
00:06:58,835 --> 00:07:12,915
That's the title.

168
00:07:11,902 --> 00:07:22,018
We provide a conceptual introduction to

169
00:07:22,018 --> 00:07:22,018
the underlying ideas rather than a

170
00:07:22,018 --> 00:07:22,018
technical discussion highlighting

171
00:07:22,018 --> 00:07:22,018
connections with various Fields of

172
00:07:22,018 --> 00:07:22,018
mathematics and physics.

173
00:07:13,922 --> 00:07:29,085
Well, I can tell you that they do make

174
00:07:29,085 --> 00:07:29,085
connections with various Fields of

175
00:07:29,085 --> 00:07:29,085
mathematics and physics, though.

176
00:07:23,020 --> 00:07:35,145
Whether it is a concept introduction or a

177
00:07:35,145 --> 00:07:35,145
technical discussion is all going to be

178
00:07:35,145 --> 00:07:35,145
about your perspective.

179
00:07:29,086 --> 00:07:43,220
Third, to illustrate a generic use case

180
00:07:43,220 --> 00:07:43,220
for the previous methodologies that are

181
00:07:43,220 --> 00:07:43,220
going to be discussed.

182
00:07:37,162 --> 00:07:51,303
We consider active inference a unifying

183
00:07:51,303 --> 00:07:51,303
formulation of behavior, subsuming

184
00:07:51,303 --> 00:07:51,303
perception, planning and learning as a

185
00:07:51,303 --> 00:07:51,303
process of inference.

186
00:07:43,222 --> 00:07:57,360
So inference on adaptive agents is not

187
00:07:57,360 --> 00:07:57,360
necessarily new.

188
00:07:51,305 --> 00:08:01,340
It goes by reinforcement learning,

189
00:08:01,340 --> 00:08:01,340
machine learning, and so on.

190
00:07:57,361 --> 00:08:12,459
However, to use those methodologies in

191
00:08:12,459 --> 00:08:12,459
the context of active inference a

192
00:08:12,459 --> 00:08:12,459
unifying formulation of behavior is

193
00:08:12,459 --> 00:08:12,459
something that the authors are bringing

194
00:08:12,459 --> 00:08:12,459
forth here.

195
00:08:01,341 --> 00:08:25,585
And last, we describe decision making

196
00:08:25,585 --> 00:08:25,585
under active inference using information

197
00:08:25,585 --> 00:08:25,585
geometry, revealing several special cases

198
00:08:25,585 --> 00:08:25,585
that are established notions in

199
00:08:25,585 --> 00:08:25,585
statistics, cognitive science and

200
00:08:25,585 --> 00:08:25,585
engineering.

201
00:08:13,467 --> 00:08:31,642
So active inference is not just unifying,

202
00:08:31,642 --> 00:08:31,642
it's going to be shown to be generalized

203
00:08:31,642 --> 00:08:31,642
as well.

204
00:08:25,586 --> 00:09:02,898
Which is to say that special cases of the

205
00:09:02,898 --> 00:09:02,898
generalization emergence different

206
00:09:02,898 --> 00:09:02,898
formalisms that were known disparately

207
00:09:02,898 --> 00:09:02,898
across Fields onto the abstract they

208
00:09:02,898 --> 00:09:02,898
write in this chapter, we identify

209
00:09:02,898 --> 00:09:02,898
fundamental geometric structures that

210
00:09:02,898 --> 00:09:02,898
underlie the problems of sampling,

211
00:09:02,898 --> 00:09:02,898
optimization, inference and adaptive

212
00:09:02,898 --> 00:09:02,898
decision making.

213
00:08:31,644 --> 00:09:12,990
Based on this identification, we derive

214
00:09:12,990 --> 00:09:12,990
algorithms that exploit these geometric

215
00:09:12,990 --> 00:09:12,990
structures to solve these problems

216
00:09:12,990 --> 00:09:12,990
efficiently.

217
00:09:03,905 --> 00:09:26,129
We show that a wide range of geometric

218
00:09:26,129 --> 00:09:26,129
theories emerge naturally in these

219
00:09:26,129 --> 00:09:26,129
Fields, foraging from measure preserving

220
00:09:26,129 --> 00:09:26,129
processes, information divergences,

221
00:09:26,129 --> 00:09:26,129
Poisson geometry, and geometric

222
00:09:26,129 --> 00:09:26,129
integration.

223
00:09:12,995 --> 00:09:37,248
Specifically, we explain how, one,

224
00:09:37,248 --> 00:09:37,248
leveraging the symplectic geometry of

225
00:09:37,248 --> 00:09:37,248
Hamiltonian systems enable us to

226
00:09:37,248 --> 00:09:37,248
construct accelerated sampling and

227
00:09:37,248 --> 00:09:37,248
optimization methods.

228
00:09:26,136 --> 00:09:57,444
Two, the theory of Hilbertian subspaces

229
00:09:57,444 --> 00:09:57,444
and Stein operators provides a general

230
00:09:57,444 --> 00:09:57,444
methodology to obtain robust estimators

231
00:09:57,444 --> 00:09:57,444
and three, preserving the information

232
00:09:57,444 --> 00:09:57,444
geometry of decision making yields

233
00:09:57,444 --> 00:09:57,444
adaptive agents that perform active

234
00:09:57,444 --> 00:09:57,444
inference throughout.

235
00:09:38,250 --> 00:10:11,521
We emphasize the rich connections between

236
00:10:11,521 --> 00:10:11,521
these field e G inference draws on

237
00:10:11,521 --> 00:10:11,521
sampling and optimization, and adaptive

238
00:10:11,521 --> 00:10:11,521
decision making assesses decisions by

239
00:10:11,521 --> 00:10:11,521
inferring their counterfactual

240
00:10:11,521 --> 00:10:11,521
consequences.

241
00:09:57,446 --> 00:10:22,638
Our exposition provides a conceptual

242
00:10:22,638 --> 00:10:22,638
overview of underlying ideas rather than

243
00:10:22,638 --> 00:10:22,638
a technical discussion, which can be

244
00:10:22,638 --> 00:10:22,638
found in the references herein.

245
00:10:12,530 --> 00:10:26,678
And indeed, there are several hundred

246
00:10:26,678 --> 00:10:26,678
references in this paper.

247
00:10:22,639 --> 00:10:29,700
Let's go to the roadmap.

248
00:10:27,683 --> 00:10:32,736
So first we can start on the right side.

249
00:10:29,702 --> 00:10:46,873
Here's an active agent room doing

250
00:10:46,873 --> 00:10:46,873
accelerated optimization in the carpool

251
00:10:46,873 --> 00:10:46,873
lane, looking ahead to the almost desert

252
00:10:46,873 --> 00:10:46,873
like visual representation of active

253
00:10:46,873 --> 00:10:46,873
inference.

254
00:10:32,738 --> 00:10:48,896
That's where we're going to go.

255
00:10:46,879 --> 00:10:52,932
And on the right side, it's just some

256
00:10:52,932 --> 00:10:52,932
cars.

257
00:10:49,899 --> 00:10:54,959
One of them is accelerated, the other one

258
00:10:54,959 --> 00:10:54,959
isn't.

259
00:10:52,933 --> 00:11:18,133
The paper begins with an introduction and

260
00:11:18,133 --> 00:11:18,133
then in section two gets into the topic

261
00:11:18,133 --> 00:11:18,133
of accelerated optimization covering the

262
00:11:18,133 --> 00:11:18,133
areas of principle of geometric

263
00:11:18,133 --> 00:11:18,133
integration, conservative flows and

264
00:11:18,133 --> 00:11:18,133
symplectic integrators rate matching,

265
00:11:18,133 --> 00:11:18,133
integrators for smooth optimization,

266
00:11:18,133 --> 00:11:18,133
manifold and constrained optimization,

267
00:11:18,133 --> 00:11:18,133
gradient flow as a high friction limit,

268
00:11:18,133 --> 00:11:18,133
and optimization on the space of

269
00:11:18,133 --> 00:11:18,133
probability measures.

270
00:10:55,964 --> 00:11:26,212
In section three, they turn from

271
00:11:26,212 --> 00:11:26,212
accelerated optimization in general to

272
00:11:26,212 --> 00:11:26,212
Hamiltonian based accelerated sampling.

273
00:11:18,135 --> 00:11:32,279
The subsections of three are optimizing

274
00:11:32,279 --> 00:11:32,279
diffusion processes for sampling and

275
00:11:32,279 --> 00:11:32,279
Hamiltonian Monte Carlo.

276
00:11:26,213 --> 00:11:41,367
From the Hamiltonian based accelerated

277
00:11:41,367 --> 00:11:41,367
sampling, they turn towards doing

278
00:11:41,367 --> 00:11:41,367
statistical inference with kernel based

279
00:11:41,367 --> 00:11:41,367
discrepancies.

280
00:11:33,286 --> 00:12:06,557
In section four, the subsections are

281
00:12:06,557 --> 00:12:06,557
topological methods for Mmds smooth

282
00:12:06,557 --> 00:12:06,557
measures and KSDS the canonical Stein

283
00:12:06,557 --> 00:12:06,557
operator and point Curray duality kernel

284
00:12:06,557 --> 00:12:06,557
Stein discrepancies and score matching

285
00:12:06,557 --> 00:12:06,557
information geometry of Mmds and natural

286
00:12:06,557 --> 00:12:06,557
gradient descent minimum Stein

287
00:12:06,557 --> 00:12:06,557
discrepancy estimators likelihood

288
00:12:06,557 --> 00:12:06,557
coherence with generative models.

289
00:11:41,368 --> 00:12:15,644
Finally, in section five adaptive agents

290
00:12:15,644 --> 00:12:15,644
through active coherence, they're going

291
00:12:15,644 --> 00:12:15,644
to bring it home to active inference.

292
00:12:06,559 --> 00:12:27,761
Section five one modeling adaptive

293
00:12:27,761 --> 00:12:27,761
decision making, behavior agents and

294
00:12:27,761 --> 00:12:27,761
environments, decision making in precise

295
00:12:27,761 --> 00:12:27,761
agents, the information geometry of

296
00:12:27,761 --> 00:12:27,761
decision making.

297
00:12:15,648 --> 00:12:40,893
And five two realizing adaptive agents,

298
00:12:40,893 --> 00:12:40,893
the basic active inference algorithm

299
00:12:40,893 --> 00:12:40,893
sequential decision making under

300
00:12:40,893 --> 00:12:40,893
uncertainty world model learning as

301
00:12:40,893 --> 00:12:40,893
inference and scaling active inference.

302
00:12:27,766 --> 00:12:50,991
A quick note we have all the formalisms

303
00:12:50,991 --> 00:12:50,991
entered Hinton our zero teams Coda

304
00:12:50,991 --> 00:12:50,991
document.

305
00:12:41,906 --> 00:12:56,058
There are 46 numbered equations in this

306
00:12:56,058 --> 00:12:56,058
paper and 83 overall.

307
00:12:50,993 --> 00:13:03,060
We're going to bring in some, but not all

308
00:13:03,060 --> 00:13:03,060
of these formalisms in the dot zero

309
00:13:03,060 --> 00:13:03,060
video.

310
00:12:57,064 --> 00:13:12,155
And we're ready to bring in more during

311
00:13:12,155 --> 00:13:12,155
the dot one one and dot two discussions

312
00:13:12,155 --> 00:13:12,155
with the authors just for those who are

313
00:13:12,155 --> 00:13:12,155
watching along.

314
00:13:03,062 --> 00:13:16,197
I'm going to scan through all of the

315
00:13:16,197 --> 00:13:16,197
formalisms from our Coda.

316
00:13:12,156 --> 00:13:21,245
And again, for those who want to help get

317
00:13:21,245 --> 00:13:21,245
involved with a dot zero preparation.

318
00:13:16,198 --> 00:13:25,284
This is a little bit of what dot zero

319
00:13:25,284 --> 00:13:25,284
preparation looks like.

320
00:13:21,247 --> 00:13:32,354
I'm going to now scroll through the

321
00:13:32,354 --> 00:13:32,354
formalisms just in case anybody wants to

322
00:13:32,354 --> 00:13:32,354
screenshot or see them.

323
00:13:25,286 --> 00:14:02,590
Great.

324
00:14:01,582 --> 00:14:05,624
On to the keywords.

325
00:14:03,602 --> 00:14:26,837
Keywords of the paper are information

326
00:14:26,837 --> 00:14:26,837
geometry, hamiltonian Monte Carlo,

327
00:14:26,837 --> 00:14:26,837
Stein's method, reproducing kernel

328
00:14:26,837 --> 00:14:26,837
variational, coherence, accelerated

329
00:14:26,837 --> 00:14:26,837
optimization, dissipative systems,

330
00:14:26,837 --> 00:14:26,837
decision theory and active inference in

331
00:14:26,837 --> 00:14:26,837
no particular pedagogical order.

332
00:14:06,638 --> 00:14:33,908
Let's start with decision theory from the

333
00:14:33,908 --> 00:14:33,908
Stanford Encyclopedia of Philosophy.

334
00:14:28,852 --> 00:14:49,063
The article begins Decision theory is

335
00:14:49,063 --> 00:14:49,063
concerned with the reasoning underlying

336
00:14:49,063 --> 00:14:49,063
an agent's choices, whether this is a

337
00:14:49,063 --> 00:14:49,063
mundane choice between taking the bus or

338
00:14:49,063 --> 00:14:49,063
getting a taxi or a more far reaching

339
00:14:49,063 --> 00:14:49,063
choice about whether to pursue a

340
00:14:49,063 --> 00:14:49,063
demanding political career.

341
00:14:34,909 --> 00:14:55,126
Decision theory is about agents making

342
00:14:55,126 --> 00:14:55,126
decisions, cognitive things making

343
00:14:55,126 --> 00:14:55,126
decisions.

344
00:14:49,066 --> 00:15:05,162
And we can take a peek ahead to figure

345
00:15:05,162 --> 00:15:05,162
three, which is shown here as well as

346
00:15:05,162 --> 00:15:05,162
invoke some of the keywords that we're

347
00:15:05,162 --> 00:15:05,162
about to walk into now.

348
00:14:55,128 --> 00:15:12,233
So if this is your first time hearing

349
00:15:12,233 --> 00:15:12,233
some of this vocabulary or you're super

350
00:15:12,233 --> 00:15:12,233
familiar, either way, it's great.

351
00:15:05,164 --> 00:15:42,535
In this paper and in this discussion,

352
00:15:42,535 --> 00:15:42,535
we're going to see how different decision

353
00:15:42,535 --> 00:15:42,535
theoretic settings such as no ambiguity,

354
00:15:42,535 --> 00:15:42,535
no ambiguity or no coherence, no

355
00:15:42,535 --> 00:15:42,535
extrinsic value and no intrinsic value

356
00:15:42,535 --> 00:15:42,535
are going to be operationalized within an

357
00:15:42,535 --> 00:15:42,535
information theoretic formalism an

358
00:15:42,535 --> 00:15:42,535
information geometric formalism and

359
00:15:42,535 --> 00:15:42,535
specifically one that we're able to do

360
00:15:42,535 --> 00:15:42,535
accelerated inference on using

361
00:15:42,535 --> 00:15:42,535
variational methods.

362
00:15:12,235 --> 00:15:45,564
We're going to be involving action as a

363
00:15:45,564 --> 00:15:45,564
parameter.

364
00:15:42,538 --> 00:15:54,654
So we can call that active coherence and

365
00:15:54,654 --> 00:15:54,654
that's how we're approaching decision

366
00:15:54,654 --> 00:15:54,654
theory and getting from here to there

367
00:15:54,654 --> 00:15:54,654
with all those fun stops in between.

368
00:15:45,564 --> 00:15:57,682
Let's go to information theory.

369
00:15:54,656 --> 00:16:17,825
The paper in the introduction writes of

370
00:16:17,825 --> 00:16:17,825
particular relevance to this chapter is

371
00:16:17,825 --> 00:16:17,825
information geometry, E g, the

372
00:16:17,825 --> 00:16:17,825
differential geometric treatment of

373
00:16:17,825 --> 00:16:17,825
smooth statistical manifolds whose origin

374
00:16:17,825 --> 00:16:17,825
stems from a seminal article by Rao 23,

375
00:16:17,825 --> 00:16:17,825
who introduced the Fishermetric tensor.

376
00:15:58,694 --> 00:16:31,960
On parameterized statistical models and

377
00:16:31,960 --> 00:16:31,960
thus a natural domain geometry that was

378
00:16:31,960 --> 00:16:31,960
later observed to correspond to an

379
00:16:31,960 --> 00:16:31,960
infinitesimal disturbance with respect to

380
00:16:31,960 --> 00:16:31,960
the Kolback libelr or Kale divergence.

381
00:16:17,825 --> 00:16:35,000
So going into citation 23 and 24.

382
00:16:31,962 --> 00:16:39,040
Citation 23 is Rao from 1992.

383
00:16:35,002 --> 00:16:52,179
And Rao wrote the objective of the paper

384
00:16:52,179 --> 00:16:52,179
is to derive certain inequality relations

385
00:16:52,179 --> 00:16:52,179
connecting the elements of the

386
00:16:52,179 --> 00:16:52,179
information matrix as defined by Fisher

387
00:16:52,179 --> 00:16:52,179
1921 and the variances and covariances of

388
00:16:52,179 --> 00:16:52,179
the estimating functions.

389
00:16:39,042 --> 00:17:01,200
A class of distribution functions which

390
00:17:01,200 --> 00:17:01,200
admit estimation of parameters with the

391
00:17:01,200 --> 00:17:01,200
minimum possible variance has been

392
00:17:01,200 --> 00:17:01,200
discussed.

393
00:16:53,183 --> 00:17:10,296
The concept of distance between

394
00:17:10,296 --> 00:17:10,296
populations of a given type has been

395
00:17:10,296 --> 00:17:10,296
developed, starting from a quadratic

396
00:17:10,296 --> 00:17:10,296
differential metric defining the element

397
00:17:10,296 --> 00:17:10,296
of length.

398
00:17:01,202 --> 00:17:13,324
So I'm only going to give the disclaimer

399
00:17:13,324 --> 00:17:13,324
one time.

400
00:17:10,297 --> 00:17:17,361
Everything in red text is beyond

401
00:17:17,361 --> 00:17:17,361
speculative.

402
00:17:13,325 --> 00:17:23,423
It's just priming the pump for

403
00:17:23,423 --> 00:17:23,423
discussions with those who know more and

404
00:17:23,423 --> 00:17:23,423
with those who know less.

405
00:17:17,363 --> 00:17:27,462
And it's just a first pass that we're

406
00:17:27,462 --> 00:17:27,462
going to continue to develop on.

407
00:17:23,425 --> 00:17:41,605
But broadly, if we can compute inequality

408
00:17:41,605 --> 00:17:41,605
relationships, which this paper developed

409
00:17:41,605 --> 00:17:41,605
in 1992, we can bound estimators, test

410
00:17:41,605 --> 00:17:41,605
for relative improvements and basically

411
00:17:41,605 --> 00:17:41,605
do stuff with those distinctions.

412
00:17:27,463 --> 00:18:06,798
And if we can compute certain kinds of

413
00:18:06,798 --> 00:18:06,798
statistical distances such that a

414
00:18:06,798 --> 00:18:06,798
distance requires a length metric, we can

415
00:18:06,798 --> 00:18:06,798
find optimal estimators of parameters,

416
00:18:06,798 --> 00:18:06,798
specifically their variances and

417
00:18:06,798 --> 00:18:06,798
covariance structure in principle and in

418
00:18:06,798 --> 00:18:06,798
practice by finding that maximum

419
00:18:06,798 --> 00:18:06,798
informational alignment to other

420
00:18:06,798 --> 00:18:06,798
estimators or empirical data.

421
00:17:41,606 --> 00:18:13,864
So we'll often say, like if you had the

422
00:18:13,864 --> 00:18:13,864
radial parameterisation, you'd be

423
00:18:13,864 --> 00:18:13,864
predicting as well as you could.

424
00:18:07,800 --> 00:18:28,001
And so we want to be able to

425
00:18:28,001 --> 00:18:28,001
operationalize that using distances and

426
00:18:28,001 --> 00:18:28,001
metrics and citation 24 this is an

427
00:18:28,001 --> 00:18:28,001
invariant form for the prior probability

428
00:18:28,001 --> 00:18:28,001
and estimation problems by Harold

429
00:18:28,001 --> 00:18:28,001
Jeffries.

430
00:18:13,865 --> 00:18:44,017
And Jefferies wrote It has shown that a

431
00:18:44,017 --> 00:18:44,017
certain differentiate form depending on

432
00:18:44,017 --> 00:18:44,017
the values of the parameters in a law of

433
00:18:44,017 --> 00:18:44,017
chance, is invariant for all

434
00:18:44,017 --> 00:18:44,017
transformations of the parameters when

435
00:18:44,017 --> 00:18:44,017
the law is differentiable with regard to

436
00:18:44,017 --> 00:18:44,017
all parameters.

437
00:18:29,002 --> 00:18:54,027
For laws containing a location in the

438
00:18:54,027 --> 00:18:54,027
scale parameter, a form with a somewhat

439
00:18:54,027 --> 00:18:54,027
restricted type of invariance is found

440
00:18:54,027 --> 00:18:54,027
even when the law is not everywhere

441
00:18:54,027 --> 00:18:54,027
differentiable with regard to the

442
00:18:54,027 --> 00:18:54,027
parameters.

443
00:18:44,017 --> 00:19:02,029
This form has the properties required to

444
00:19:02,029 --> 00:19:02,029
give a general rule for stating the prior

445
00:19:02,029 --> 00:19:02,029
probability in a large class of

446
00:19:02,029 --> 00:19:02,029
estimation problems.

447
00:18:55,027 --> 00:19:17,044
So if all the parameters in our model,

448
00:19:17,044 --> 00:19:17,044
whether there's one, two or more, are

449
00:19:17,044 --> 00:19:17,044
differentiable, which is to say they're

450
00:19:17,044 --> 00:19:17,044
smooth, et cetera, et cetera, there are

451
00:19:17,044 --> 00:19:17,044
technical details, then there are certain

452
00:19:17,044 --> 00:19:17,044
transformational invariances.

453
00:19:02,029 --> 00:19:25,052
So just like every Gaussian distribution,

454
00:19:25,052 --> 00:19:25,052
by shifting it and stretching and

455
00:19:25,052 --> 00:19:25,052
shrinking it, you can map those on to

456
00:19:25,052 --> 00:19:25,052
each other.

457
00:19:17,044 --> 00:19:26,053
It's like that.

458
00:19:25,052 --> 00:19:32,058
It turns out that some of those

459
00:19:32,058 --> 00:19:32,058
constraints can even be somewhat

460
00:19:32,058 --> 00:19:32,058
relaxed.

461
00:19:27,054 --> 00:19:38,065
For example, this approach may still be

462
00:19:38,065 --> 00:19:38,065
effective even where parameters are not

463
00:19:38,065 --> 00:19:38,065
differentiable everywhere.

464
00:19:32,059 --> 00:19:57,084
And this is big for helping us do

465
00:19:57,084 --> 00:19:57,084
Bayesian estimation, which can be seen as

466
00:19:57,084 --> 00:19:57,084
a special case of informational

467
00:19:57,084 --> 00:19:57,084
transformations or of manipulations of

468
00:19:57,084 --> 00:19:57,084
statistical distributions in the

469
00:19:57,084 --> 00:19:57,084
information theoretic or information

470
00:19:57,084 --> 00:19:57,084
geometric sense.

471
00:19:38,065 --> 00:20:01,082
Functional analysis.

472
00:19:59,086 --> 00:20:13,094
So let's begin as tradition in our year

473
00:20:13,094 --> 00:20:13,094
2023 with a brief quote from Chat GPT how

474
00:20:13,094 --> 00:20:13,094
is functional analysis helpful for us who

475
00:20:13,094 --> 00:20:13,094
are learning and applying active

476
00:20:13,094 --> 00:20:13,094
inference?

477
00:20:01,082 --> 00:20:15,096
I'll just read the last section.

478
00:20:13,094 --> 00:20:25,106
Overall, functional analysis provides a

479
00:20:25,106 --> 00:20:25,106
powerful mathematical toolset for

480
00:20:25,106 --> 00:20:25,106
understanding and analyzing the

481
00:20:25,106 --> 00:20:25,106
principles active coherence lab, and one

482
00:20:25,106 --> 00:20:25,106
can read more.

483
00:20:15,096 --> 00:20:30,111
Functional analysis is the study of

484
00:20:30,111 --> 00:20:30,111
functions.

485
00:20:25,106 --> 00:20:50,131
And here we see one way of showing what a

486
00:20:50,131 --> 00:20:50,131
function does in terms of being this box

487
00:20:50,131 --> 00:20:50,131
f of x that takes in an input, an

488
00:20:50,131 --> 00:20:50,131
argument x and outputs a result y and

489
00:20:50,131 --> 00:20:50,131
also that can be understood as a mapping

490
00:20:50,131 --> 00:20:50,131
between or amongst spaces.

491
00:20:30,111 --> 00:20:52,133
And there's a lot more to go into.

492
00:20:50,131 --> 00:21:09,144
Just wanted to bring up that in math,

493
00:21:09,144 --> 00:21:09,144
sometimes people have analyzed functions

494
00:21:09,144 --> 00:21:09,144
and their properties and we're interested

495
00:21:09,144 --> 00:21:09,144
in the properties of a special class of

496
00:21:09,144 --> 00:21:09,144
functions, which are probability

497
00:21:09,144 --> 00:21:09,144
distributions, well behaved probability

498
00:21:09,144 --> 00:21:09,144
distinctions.

499
00:20:52,133 --> 00:21:13,148
What do we do with those probability

500
00:21:13,148 --> 00:21:13,148
distinctions?

501
00:21:10,145 --> 00:21:16,151
Well, one thing we might want to do is

502
00:21:16,151 --> 00:21:16,151
sample.

503
00:21:13,148 --> 00:21:24,159
So the paper writes sampling methods are

504
00:21:24,159 --> 00:21:24,159
critical to the efficient implementations

505
00:21:24,159 --> 00:21:24,159
of many methodologies.

506
00:21:16,151 --> 00:21:35,170
Most Modern samplers are based on Markov

507
00:21:35,170 --> 00:21:35,170
chain Monte Carlo methods, which include

508
00:21:35,170 --> 00:21:35,170
slice samplers, piecewise deterministic,

509
00:21:35,170 --> 00:21:35,170
Markov chains and so on.

510
00:21:25,160 --> 00:21:48,183
The original Hamiltonian Monte Carlo or

511
00:21:48,183 --> 00:21:48,183
HMC, which we're going to get to

512
00:21:48,183 --> 00:21:48,183
algorithm, was introduced in physics to

513
00:21:48,183 --> 00:21:48,183
sample distributions on gauge groups for

514
00:21:48,183 --> 00:21:48,183
lattice quantum chromodynamics.

515
00:21:36,171 --> 00:21:58,193
It combined two approaches that emerged

516
00:21:58,193 --> 00:21:58,193
in previous decades, namely the

517
00:21:58,193 --> 00:21:58,193
Metropolis Hastings algorithm and the

518
00:21:58,193 --> 00:21:58,193
Hamiltonian correlation of molecular

519
00:21:58,193 --> 00:21:58,193
dynamics.

520
00:21:48,183 --> 00:22:15,204
So the way that they state it in the

521
00:22:15,204 --> 00:22:15,204
beginning of section three of the paper

522
00:22:15,204 --> 00:22:15,204
is the purpose of sampling methods is to

523
00:22:15,204 --> 00:22:15,204
efficiently draw samples from a given

524
00:22:15,204 --> 00:22:15,204
target distribution row or more commonly,

525
00:22:15,204 --> 00:22:15,204
to calculate expectations with respect to

526
00:22:15,204 --> 00:22:15,204
row.

527
00:21:59,194 --> 00:22:41,230
By equation 33 and other, they write,

528
00:22:41,230 --> 00:22:41,230
Modern Hamiltonian Monte Carlo which

529
00:22:41,230 --> 00:22:41,230
again we're going to get to in a second

530
00:22:41,230 --> 00:22:41,230
relies heavily on symplectic integrators

531
00:22:41,230 --> 00:22:41,230
another keyword to simulate a

532
00:22:41,230 --> 00:22:41,230
deterministic dynamics responsible for

533
00:22:41,230 --> 00:22:41,230
generative distant moves between samples

534
00:22:41,230 --> 00:22:41,230
and thus reduce their correlation while

535
00:22:41,230 --> 00:22:41,230
at the same time preserving important

536
00:22:41,230 --> 00:22:41,230
geometric properties.

537
00:22:15,204 --> 00:22:46,235
Now, we want to make our samples as

538
00:22:46,235 --> 00:22:46,235
informative as possible.

539
00:22:41,230 --> 00:22:52,241
If we're just foraging samples and it's

540
00:22:52,241 --> 00:22:52,241
the same sample again and again, one can

541
00:22:52,241 --> 00:22:52,241
imagine that it is uninformative.

542
00:22:46,235 --> 00:23:10,253
Conversely, one can imagine a situation

543
00:23:10,253 --> 00:23:10,253
where samples are informative and that is

544
00:23:10,253 --> 00:23:10,253
brought into practice by making sure that

545
00:23:10,253 --> 00:23:10,253
the samples are minimally correlation

546
00:23:10,253 --> 00:23:10,253
with each other, as noted above.

547
00:22:54,242 --> 00:23:13,256
And this also has analogy in

548
00:23:13,256 --> 00:23:13,256
cryptography.

549
00:23:10,253 --> 00:23:22,265
So if we're sampling something where the

550
00:23:22,265 --> 00:23:22,265
successive samples are 99% correlated,

551
00:23:22,265 --> 00:23:22,265
you can imagine that you're over

552
00:23:22,265 --> 00:23:22,265
sampling.

553
00:23:13,256 --> 00:23:28,271
So you're taking 10,000 frames per second

554
00:23:28,271 --> 00:23:28,271
of a YouTube video with 30 frames per

555
00:23:28,271 --> 00:23:28,271
second.

556
00:23:22,265 --> 00:23:32,275
I think I'm streaming at 25 frames per

557
00:23:32,275 --> 00:23:32,275
second right now.

558
00:23:28,271 --> 00:23:43,286
Conversely, if one were to sample every

559
00:23:43,286 --> 00:23:43,286
ten minutes, then their correlation

560
00:23:43,286 --> 00:23:43,286
structure would also not capture the

561
00:23:43,286 --> 00:23:43,286
videos 25 frames per second.

562
00:23:32,275 --> 00:24:02,299
So in order to generate good and large

563
00:24:02,299 --> 00:24:02,299
moves in the space, we would need perfect

564
00:24:02,299 --> 00:24:02,299
knowledge of what the space is or how

565
00:24:02,299 --> 00:24:02,299
it's generated, which is sometimes

566
00:24:02,299 --> 00:24:02,299
knowable in cryptography, however

567
00:24:02,299 --> 00:24:02,299
empirically, we need to use heuristics

568
00:24:02,299 --> 00:24:02,299
and statistical approximations.

569
00:23:43,286 --> 00:24:08,305
So this is the applied statistics angle

570
00:24:08,305 --> 00:24:08,305
on the analytical relationships that

571
00:24:08,305 --> 00:24:08,305
we're going to be talking about.

572
00:24:02,299 --> 00:24:20,317
And it's going to come up again and again

573
00:24:20,317 --> 00:24:20,317
this tension between analytical

574
00:24:20,317 --> 00:24:20,317
formulations, what's true in principle,

575
00:24:20,317 --> 00:24:20,317
and the statistical or numerical or

576
00:24:20,317 --> 00:24:20,317
computational applications.

577
00:24:08,305 --> 00:24:44,341
Two Hamiltonian Monte Carlo from

578
00:24:44,341 --> 00:24:44,341
Wikipedia the Hamiltonian Monte Carlo

579
00:24:44,341 --> 00:24:44,341
algorithm, originally known as hybrid

580
00:24:44,341 --> 00:24:44,341
Monte Carlo still HMC is a Markov chain

581
00:24:44,341 --> 00:24:44,341
Monte Carlo method for obtaining a

582
00:24:44,341 --> 00:24:44,341
sequence of random samples which converge

583
00:24:44,341 --> 00:24:44,341
to being distributed according to a

584
00:24:44,341 --> 00:24:44,341
target probability distribution, for

585
00:24:44,341 --> 00:24:44,341
which direct sampling is difficult.

586
00:24:21,318 --> 00:24:48,345
So just a few first pass reflections.

587
00:24:45,342 --> 00:24:51,348
We're sampling, which is to say Monte

588
00:24:51,348 --> 00:24:51,348
Carlo.

589
00:24:48,345 --> 00:25:03,354
And Monte Carlo is harkening back to a

590
00:25:03,354 --> 00:25:03,354
different time when gambling was done in

591
00:25:03,354 --> 00:25:03,354
Monte Carlo and the state space of all

592
00:25:03,354 --> 00:25:03,354
the hands of poker were too vast to

593
00:25:03,354 --> 00:25:03,354
compute analytically or equations were

594
00:25:03,354 --> 00:25:03,354
not known.

595
00:24:51,348 --> 00:25:18,369
And so what one used to do, and still

596
00:25:18,369 --> 00:25:18,369
does, is draw samples and say, well, I

597
00:25:18,369 --> 00:25:18,369
don't know if this is the final estimate

598
00:25:18,369 --> 00:25:18,369
on how likely a royal flush is, but from

599
00:25:18,369 --> 00:25:18,369
the 2 billion hands that I sampled, there

600
00:25:18,369 --> 00:25:18,369
were two royal flushes.

601
00:25:03,354 --> 00:25:21,372
And so it's one in a billion based upon

602
00:25:21,372 --> 00:25:21,372
my sample.

603
00:25:18,369 --> 00:25:32,383
So we're sampling because this is a

604
00:25:32,383 --> 00:25:32,383
complex distinctions for which an

605
00:25:32,383 --> 00:25:32,383
analytical solution is not possible, not

606
00:25:32,383 --> 00:25:32,383
known, not relevant, or not trackable

607
00:25:32,383 --> 00:25:32,383
computationally.

608
00:25:22,373 --> 00:25:37,388
So there's various situations where we

609
00:25:37,388 --> 00:25:37,388
want to sample and we don't necessarily

610
00:25:37,388 --> 00:25:37,388
have an analytical solution.

611
00:25:32,383 --> 00:25:50,401
When we're sampling from a converged

612
00:25:50,401 --> 00:25:50,401
stationarity, hashtag live stream 26 of a

613
00:25:50,401 --> 00:25:50,401
statistical landscape from a numerical

614
00:25:50,401 --> 00:25:50,401
perspective, we understand that landscape

615
00:25:50,401 --> 00:25:50,401
well enough to do coherence.

616
00:25:38,389 --> 00:26:00,405
So imagine that we're sampling height

617
00:26:00,405 --> 00:26:00,405
estimates from GPS locations in the X and

618
00:26:00,405 --> 00:26:00,405
the y coordinates, and we're getting a z

619
00:26:00,405 --> 00:26:00,405
value in our sample.

620
00:25:50,401 --> 00:26:06,411
At some point we can say we're

621
00:26:06,411 --> 00:26:06,411
unsurprised by new samples.

622
00:26:00,405 --> 00:26:11,416
At that point we understand the landscape

623
00:26:11,416 --> 00:26:11,416
well enough to do statistical inference.

624
00:26:06,411 --> 00:26:23,428
And just like the frames per second in

625
00:26:23,428 --> 00:26:23,428
the previous example, one can imagine if

626
00:26:23,428 --> 00:26:23,428
the landscape is changing on the spatial

627
00:26:23,428 --> 00:26:23,428
scale of 1 mile and you're sampling every

628
00:26:23,428 --> 00:26:23,428
millimeter, you might be over sampling.

629
00:26:11,416 --> 00:26:29,434
If you were sampling every league, maybe

630
00:26:29,434 --> 00:26:29,434
you'd be too coarse grained with your

631
00:26:29,434 --> 00:26:29,434
samples.

632
00:26:23,428 --> 00:26:35,440
And so one reason that comes up in

633
00:26:35,440 --> 00:26:35,440
science all the time for using sampling

634
00:26:35,440 --> 00:26:35,440
is that the search space might be just

635
00:26:35,440 --> 00:26:35,440
too large.

636
00:26:29,434 --> 00:26:41,446
For example, the combinatorics of a

637
00:26:41,446 --> 00:26:41,446
phylogenetic tree with a thousand species

638
00:26:41,446 --> 00:26:41,446
might be vast.

639
00:26:36,441 --> 00:26:55,460
And so it might be better to say

640
00:26:55,460 --> 00:26:55,460
something like, well, 99% of the million

641
00:26:55,460 --> 00:26:55,460
samples we drew were consistent with X,

642
00:26:55,460 --> 00:26:55,460
because an analytical solution or a brute

643
00:26:55,460 --> 00:26:55,460
force search, neither might be possible.

644
00:26:41,446 --> 00:27:04,463
And the Wikipedia wrote this sequence of

645
00:27:04,463 --> 00:27:04,463
samples can be used to estimate integrals

646
00:27:04,463 --> 00:27:04,463
with respect to the target distribution.

647
00:26:56,461 --> 00:27:13,472
So sometimes we're sampling not from the

648
00:27:13,472 --> 00:27:13,472
target distribution itself, but rather

649
00:27:13,472 --> 00:27:13,472
from a distribution that is related or

650
00:27:13,472 --> 00:27:13,472
transformed from the target

651
00:27:13,472 --> 00:27:13,472
distribution.

652
00:27:04,463 --> 00:27:20,479
This is because sampling directly from

653
00:27:20,479 --> 00:27:20,479
the target distribution might be

654
00:27:20,479 --> 00:27:20,479
difficult or less than perfectly

655
00:27:20,479 --> 00:27:20,479
informative.

656
00:27:13,472 --> 00:27:33,492
So we might, for example, hint hint

657
00:27:33,492 --> 00:27:33,492
sample from the derivative of the target

658
00:27:33,492 --> 00:27:33,492
which might help us identify points

659
00:27:33,492 --> 00:27:33,492
where, for example, the derivative is

660
00:27:33,492 --> 00:27:33,492
flat in all directions.

661
00:27:20,479 --> 00:27:41,500
In the case that we're sampling from a

662
00:27:41,500 --> 00:27:41,500
derived landscape of the target

663
00:27:41,500 --> 00:27:41,500
distribution, this is called a symplectic

664
00:27:41,500 --> 00:27:41,500
integrator.

665
00:27:34,493 --> 00:27:44,503
So let's look at what a symplectic

666
00:27:44,503 --> 00:27:44,503
integrator is.

667
00:27:41,500 --> 00:27:50,509
So first, backing up to what is

668
00:27:50,509 --> 00:27:50,509
differentiate geometry.

669
00:27:45,504 --> 00:27:58,516
Differential geometry is a mathematical

670
00:27:58,516 --> 00:27:58,516
discipline that studies the geometry of

671
00:27:58,516 --> 00:27:58,516
smooth shapes and smooth spaces,

672
00:27:58,516 --> 00:27:58,516
otherwise known as smooth manifolds.

673
00:27:50,509 --> 00:28:12,525
Symplectic geometry is a branch of

674
00:28:12,525 --> 00:28:12,525
differential geometry and differential

675
00:28:12,525 --> 00:28:12,525
topology that studies Symplectic

676
00:28:12,525 --> 00:28:12,525
manifolds, that is, differentiable

677
00:28:12,525 --> 00:28:12,525
manifolds equipped with a closed,

678
00:28:12,525 --> 00:28:12,525
nondegenerate two form.

679
00:27:59,518 --> 00:28:24,537
Symplectic geometry has its origins in

680
00:28:24,537 --> 00:28:24,537
the Hamiltonian formulation of classical

681
00:28:24,537 --> 00:28:24,537
mechanics, where the phase space of

682
00:28:24,537 --> 00:28:24,537
certain classical systems takes on the

683
00:28:24,537 --> 00:28:24,537
structure of a symplectic manifold.

684
00:28:12,525 --> 00:28:26,539
So the keyword density is high.

685
00:28:24,537 --> 00:28:31,544
These topics are all very closely linked

686
00:28:31,544 --> 00:28:31,544
from the paper.

687
00:28:27,540 --> 00:28:43,556
It has been known for a long time that

688
00:28:43,556 --> 00:28:43,556
the class of symplectic integrators is

689
00:28:43,556 --> 00:28:43,556
the preferred choice for simulating

690
00:28:43,556 --> 00:28:43,556
physical systems, only the finest for our

691
00:28:43,556 --> 00:28:43,556
simulations of physical systems.

692
00:28:31,544 --> 00:28:57,570
These discretization techniques are

693
00:28:57,570 --> 00:28:57,570
designed to preserve the underlying

694
00:28:57,570 --> 00:28:57,570
symplectic geometry of Hamiltonian

695
00:28:57,570 --> 00:28:57,570
systems and they also form the basis of

696
00:28:57,570 --> 00:28:57,570
Hamiltonian Monte Carlo or hybrid Monte

697
00:28:57,570 --> 00:28:57,570
Carlo methods.

698
00:28:44,557 --> 00:29:08,575
We're going to come back to this tension

699
00:29:08,575 --> 00:29:08,575
again and again, which is what good is an

700
00:29:08,575 --> 00:29:08,575
in principle, smooth and differentiable

701
00:29:08,575 --> 00:29:08,575
analytical correlation.

702
00:28:57,570 --> 00:29:23,590
If our discreditization scheme, the way

703
00:29:23,590 --> 00:29:23,590
that we actually implement the steps on a

704
00:29:23,590 --> 00:29:23,590
computer, are inferior, we might ruin all

705
00:29:23,590 --> 00:29:23,590
those nice properties that we work to get

706
00:29:23,590 --> 00:29:23,590
about the analytical form.

707
00:29:08,575 --> 00:29:29,596
We might just throw those out when we

708
00:29:29,596 --> 00:29:29,596
discretize it coarsely or

709
00:29:29,596 --> 00:29:29,596
inappropriately.

710
00:29:24,591 --> 00:29:31,598
So what is the symplectic integrator?

711
00:29:29,596 --> 00:29:35,602
It's a numerical integration scheme for

712
00:29:35,602 --> 00:29:35,602
Hamiltonian systems.

713
00:29:31,598 --> 00:29:42,609
Symplectic integrators form the subclass

714
00:29:42,609 --> 00:29:42,609
of geometric integrators, which by

715
00:29:42,609 --> 00:29:42,609
definition are canonical

716
00:29:42,609 --> 00:29:42,609
transformations.

717
00:29:35,602 --> 00:29:44,611
They're transformations we know a lot

718
00:29:44,611 --> 00:29:44,611
about.

719
00:29:42,609 --> 00:30:03,624
Symplectic integrator schemes are

720
00:30:03,624 --> 00:30:03,624
referring to, again, just broadly, both

721
00:30:03,624 --> 00:30:03,624
the analytical formulations and the

722
00:30:03,624 --> 00:30:03,624
software packages and approaches that we

723
00:30:03,624 --> 00:30:03,624
can use to implement those formalisms.

724
00:29:47,614 --> 00:30:21,642
And these integration schemes are useful

725
00:30:21,642 --> 00:30:21,642
across difficult estimation problems,

726
00:30:21,642 --> 00:30:21,642
which is why they're used to study

727
00:30:21,642 --> 00:30:21,642
nonlinear dynamics, molecular dynamics

728
00:30:21,642 --> 00:30:21,642
like protein simulation, discrete element

729
00:30:21,642 --> 00:30:21,642
methods, accelerator physics, plasma

730
00:30:21,642 --> 00:30:21,642
physics, quantum physics, celestial

731
00:30:21,642 --> 00:30:21,642
mechanics.

732
00:30:03,624 --> 00:30:32,653
The time evolution of Hamilton's equation

733
00:30:32,653 --> 00:30:32,653
is a symplectomorphism, meaning that it

734
00:30:32,653 --> 00:30:32,653
conserves the symplectic two form.

735
00:30:23,644 --> 00:30:38,659
A numerical scheme is a symplectic

736
00:30:38,659 --> 00:30:38,659
integrator if it also conserves this two

737
00:30:38,659 --> 00:30:38,659
form.

738
00:30:32,653 --> 00:30:44,665
Let's hear more if this is not correct or

739
00:30:44,665 --> 00:30:44,665
not complete.

740
00:30:40,661 --> 00:30:48,669
But the one form is like the

741
00:30:48,669 --> 00:30:48,669
differentiation and integration of a

742
00:30:48,669 --> 00:30:48,669
line.

743
00:30:44,665 --> 00:30:53,674
The two form is like the differentiation

744
00:30:53,674 --> 00:30:53,674
and integration of a surface.

745
00:30:49,670 --> 00:30:58,679
And the three form is like integration or

746
00:30:58,679 --> 00:30:58,679
differentiate of a volume.

747
00:30:53,674 --> 00:31:04,679
And John Denker has a great blog post on

748
00:31:04,679 --> 00:31:04,679
the basic properties of a symplectic

749
00:31:04,679 --> 00:31:04,679
integrator.

750
00:30:58,679 --> 00:31:12,687
Some really interesting quotes that are

751
00:31:12,687 --> 00:31:12,687
good to just keep in mind when we're

752
00:31:12,687 --> 00:31:12,687
hearing about all these avenues we're

753
00:31:12,687 --> 00:31:12,687
going to be exploring in the paper.

754
00:31:04,679 --> 00:31:19,694
A symplectic integrator can serve the

755
00:31:19,694 --> 00:31:19,694
area in phase space delimited by an

756
00:31:19,694 --> 00:31:19,694
ensemble of systems.

757
00:31:13,688 --> 00:31:26,701
For a periodic system, there is an area

758
00:31:26,701 --> 00:31:26,701
that is conserved, namely the area inside

759
00:31:26,701 --> 00:31:26,701
the phase space orbit of the system.

760
00:31:19,694 --> 00:31:33,708
The main reason for mentioning the orbit

761
00:31:33,708 --> 00:31:33,708
is to make the point that there are lots

762
00:31:33,708 --> 00:31:33,708
of different things with dimensions of

763
00:31:33,708 --> 00:31:33,708
area in phase space.

764
00:31:27,702 --> 00:31:36,711
Some are conserved and some not.

765
00:31:34,709 --> 00:31:38,713
Some are interesting and some not.

766
00:31:36,711 --> 00:31:43,718
You have to specify which sort of area

767
00:31:43,718 --> 00:31:43,718
you are talking about.

768
00:31:39,714 --> 00:31:58,733
So another thing that's going to arise is

769
00:31:58,733 --> 00:31:58,733
we're interested in very well behaved

770
00:31:58,733 --> 00:31:58,733
outcomes from a very specific or

771
00:31:58,733 --> 00:31:58,733
constrained, still broad, but definitely

772
00:31:58,733 --> 00:31:58,733
constrained set of distributions.

773
00:31:43,718 --> 00:32:06,735
For example, distributions that can be

774
00:32:06,735 --> 00:32:06,735
interpreted in an information theory

775
00:32:06,735 --> 00:32:06,735
sense as probability distinctions.

776
00:31:59,733 --> 00:32:12,741
What was that mention of the Hamiltonian

777
00:32:12,741 --> 00:32:12,741
and the shadow Hamiltonian?

778
00:32:08,737 --> 00:32:14,743
So the author is right.

779
00:32:12,741 --> 00:32:27,756
For symplectic brackets, which is the

780
00:32:27,756 --> 00:32:27,756
kind of operation that reflects the

781
00:32:27,756 --> 00:32:27,756
symplectic integrator, the existence of a

782
00:32:27,756 --> 00:32:27,756
shadow Hamiltonian can be guaranteed

783
00:32:27,756 --> 00:32:27,756
beyond the case of splitting methods, e.

784
00:32:16,745 --> 00:32:35,764
G for variational integrators, which use

785
00:32:35,764 --> 00:32:35,764
variational inference, which use a

786
00:32:35,764 --> 00:32:35,764
discrete version of Hamilton's principle

787
00:32:35,764 --> 00:32:35,764
of least action.

788
00:32:27,756 --> 00:32:53,782
And we've heard it before that free

789
00:32:53,782 --> 00:32:53,782
energy principle is a principle of least

790
00:32:53,782 --> 00:32:53,782
action for inference and action and more

791
00:32:53,782 --> 00:32:53,782
generally for most symplectic integrators

792
00:32:53,782 --> 00:32:53,782
in which the symplectic bracket is

793
00:32:53,782 --> 00:32:53,782
preserved up to topological

794
00:32:53,782 --> 00:32:53,782
considerations described by some

795
00:32:53,782 --> 00:32:53,782
technicalities that we can learn about.

796
00:32:35,764 --> 00:33:06,789
As we shall see, such geometric

797
00:33:06,789 --> 00:33:06,789
integrators can be constructed by

798
00:33:06,789 --> 00:33:06,789
leveraging the shadow Hamiltonian

799
00:33:06,789 --> 00:33:06,789
property of symplectic methods on higher

800
00:33:06,789 --> 00:33:06,789
dimensional conservative Hamiltonian

801
00:33:06,789 --> 00:33:06,789
systems.

802
00:32:54,783 --> 00:33:18,801
In short, as a consequence of having

803
00:33:18,801 --> 00:33:18,801
shadow Hamiltonian, such geometric

804
00:33:18,801 --> 00:33:18,801
integrators are able to reproduce all the

805
00:33:18,801 --> 00:33:18,801
relevant properties of the continuum

806
00:33:18,801 --> 00:33:18,801
system.

807
00:33:08,791 --> 00:33:21,804
These arguments are completely general.

808
00:33:19,802 --> 00:33:52,835
So even before one knows what the shadow

809
00:33:52,835 --> 00:33:52,835
Hamiltonian necessarily is, the authors

810
00:33:52,835 --> 00:33:52,835
are letting us know that if we can

811
00:33:52,835 --> 00:33:52,835
discretize the shadow Hamiltonian

812
00:33:52,835 --> 00:33:52,835
appropriately through symplectic

813
00:33:52,835 --> 00:33:52,835
integration, we can provide a inclination

814
00:33:52,835 --> 00:33:52,835
scheme that ends up staying consistent

815
00:33:52,835 --> 00:33:52,835
and compatible and all down the middle

816
00:33:52,835 --> 00:33:52,835
with the analytical properties that we've

817
00:33:52,835 --> 00:33:52,835
worked so hard to get.

818
00:33:21,804 --> 00:33:59,841
And there were some different papers that

819
00:33:59,841 --> 00:33:59,841
were cited and some that were found

820
00:33:59,841 --> 00:33:59,841
during research.

821
00:33:53,836 --> 00:34:11,848
This paper Time Step in Shadow

822
00:34:11,848 --> 00:34:11,848
Hamiltonian and Molecular Dynamics

823
00:34:11,848 --> 00:34:11,848
Simulations by Kim demonstrates

824
00:34:11,848 --> 00:34:11,848
symplectic integrators on the simplest

825
00:34:11,848 --> 00:34:11,848
possible system a simple harmonic

826
00:34:11,848 --> 00:34:11,848
oscillator.

827
00:33:59,842 --> 00:34:18,855
So if one wants to go to a technical

828
00:34:18,855 --> 00:34:18,855
example, but one that builds intuition,

829
00:34:18,855 --> 00:34:18,855
that's a great place to start.

830
00:34:11,848 --> 00:34:26,863
And from the Wikipedia article on energy

831
00:34:26,863 --> 00:34:26,863
drift, these integrators do not in fact,

832
00:34:26,863 --> 00:34:26,863
reproduce the actual Hamiltonian

833
00:34:26,863 --> 00:34:26,863
mechanics of the system.

834
00:34:18,855 --> 00:34:35,872
Instead, they reproduce a closely related

835
00:34:35,872 --> 00:34:35,872
shadow Hamiltonian whose value they

836
00:34:35,872 --> 00:34:35,872
conserved many orders of magnitude more

837
00:34:35,872 --> 00:34:35,872
closely.

838
00:34:26,863 --> 00:34:41,878
The accuracy of the energy conversation

839
00:34:41,878 --> 00:34:41,878
for the true Hamiltonian is independent

840
00:34:41,878 --> 00:34:41,878
on the time step.

841
00:34:35,872 --> 00:35:11,902
So if we can, for examples, just

842
00:35:11,902 --> 00:35:11,902
metaphorically speaking, capture a high

843
00:35:11,902 --> 00:35:11,902
evolution image of the shadow, then we'll

844
00:35:11,902 --> 00:35:11,902
be able to know something, do something

845
00:35:11,902 --> 00:35:11,902
with the actual some technical details on

846
00:35:11,902 --> 00:35:11,902
Stein's method, which is a general method

847
00:35:11,902 --> 00:35:11,902
in probability theory to obtain bounds on

848
00:35:11,902 --> 00:35:11,902
the distance between two probability

849
00:35:11,902 --> 00:35:11,902
distributions with respect to a

850
00:35:11,902 --> 00:35:11,902
probability metric.

851
00:34:41,878 --> 00:35:20,911
And Stein's method is used in the context

852
00:35:20,911 --> 00:35:20,911
of Stein operators and Stein class, which

853
00:35:20,911 --> 00:35:20,911
we're going to get to in the subsequent

854
00:35:20,911 --> 00:35:20,911
discussions.

855
00:35:11,902 --> 00:35:29,920
Reproducing Kernel our discussion of

856
00:35:29,920 --> 00:35:29,920
inference builds upon the theory of

857
00:35:29,920 --> 00:35:29,920
Hilbertian subspaces and in particular,

858
00:35:29,920 --> 00:35:29,920
reproducing kernels.

859
00:35:22,912 --> 00:35:37,928
These inference schemes rely on the

860
00:35:37,928 --> 00:35:37,928
continuity of linear functionals such as

861
00:35:37,928 --> 00:35:37,928
probability and Schwartz distributions

862
00:35:37,928 --> 00:35:37,928
over a class of functions.

863
00:35:29,920 --> 00:35:55,946
To geometrize the analysis of integral

864
00:35:55,946 --> 00:35:55,946
probability metrics, which measure the

865
00:35:55,946 --> 00:35:55,946
worst case integration error, we shall

866
00:35:55,946 --> 00:35:55,946
explain how maximum mean kernelized and

867
00:35:55,946 --> 00:35:55,946
score matching discrepancies arise

868
00:35:55,946 --> 00:35:55,946
naturally from topological

869
00:35:55,946 --> 00:35:55,946
considerations.

870
00:35:37,928 --> 00:36:01,946
So we've added some background links here

871
00:36:01,946 --> 00:36:01,946
to the slide, but it'll be a Brea place

872
00:36:01,946 --> 00:36:01,946
to start with.

873
00:35:55,946 --> 00:36:07,952
Authors, what is a reproducing kernel and

874
00:36:07,952 --> 00:36:07,952
how is it used?

875
00:36:01,946 --> 00:36:14,959
In this paper onwards, through the

876
00:36:14,959 --> 00:36:14,959
keywords we go accelerated optimization

877
00:36:14,959 --> 00:36:14,959
and variational inference.

878
00:36:07,952 --> 00:36:22,967
So here's a 2016 paper from Michael

879
00:36:22,967 --> 00:36:22,967
Jordan and other authors and some quotes

880
00:36:22,967 --> 00:36:22,967
from the abstract.

881
00:36:14,959 --> 00:36:30,975
Accelerated methods achieve faster

882
00:36:30,975 --> 00:36:30,975
coherence rates than gradient methods,

883
00:36:30,975 --> 00:36:30,975
and indeed, under certain conditions they

884
00:36:30,975 --> 00:36:30,975
achieve optimal rates.

885
00:36:22,967 --> 00:36:36,981
However, accelerated methods are not

886
00:36:36,981 --> 00:36:36,981
dissent methods and remain a conceptual

887
00:36:36,981 --> 00:36:36,981
mystery.

888
00:36:30,975 --> 00:36:41,986
We propose a variational continuous time

889
00:36:41,986 --> 00:36:41,986
framework for understanding accelerated

890
00:36:41,986 --> 00:36:41,986
methods.

891
00:36:36,981 --> 00:36:49,994
We provide a systematic methodology for

892
00:36:49,994 --> 00:36:49,994
converting accelerated higher order

893
00:36:49,994 --> 00:36:49,994
methods from continuous time to discrete

894
00:36:49,994 --> 00:36:49,994
time.

895
00:36:42,986 --> 00:36:56,001
Our work illuminates a class of dynamics

896
00:36:56,001 --> 00:36:56,001
that may be useful for designing better

897
00:36:56,001 --> 00:36:56,001
algorithms for optimization.

898
00:36:50,995 --> 00:37:01,000
And this is going to be, again, something

899
00:37:01,000 --> 00:37:01,000
awesome to discuss.

900
00:36:57,002 --> 00:37:11,010
We've talked about, on many streams,

901
00:37:11,010 --> 00:37:11,010
variational inference in the context of

902
00:37:11,010 --> 00:37:11,010
gradient based optimization.

903
00:37:02,001 --> 00:37:24,023
For example, the ball is rolling to the

904
00:37:24,023 --> 00:37:24,023
bottom of the bowl, bottom of the hill,

905
00:37:24,023 --> 00:37:24,023
and what you can do is you can take the

906
00:37:24,023 --> 00:37:24,023
gradient div, grab curl and all that of

907
00:37:24,023 --> 00:37:24,023
where the ball is and just go downhill.

908
00:37:11,010 --> 00:37:38,037
And if you designed the hill to be the

909
00:37:38,037 --> 00:37:38,037
right shape, or you're using a chosen

910
00:37:38,037 --> 00:37:38,037
family of variational estimators that are

911
00:37:38,037 --> 00:37:38,037
the right shape, are a good shape, well

912
00:37:38,037 --> 00:37:38,037
behaved shape, then you follow the

913
00:37:38,037 --> 00:37:38,037
gradient on down.

914
00:37:24,023 --> 00:37:41,039
And that's how we've talked about

915
00:37:41,039 --> 00:37:41,039
variational inference.

916
00:37:38,037 --> 00:37:46,045
Accelerated optimization is going to be

917
00:37:46,045 --> 00:37:46,045
accelerated from that.

918
00:37:41,040 --> 00:37:50,049
And so it's fun to think about and it'll

919
00:37:50,049 --> 00:37:50,049
be great to talk about.

920
00:37:46,045 --> 00:37:53,052
Dissipative systems.

921
00:37:51,050 --> 00:37:59,058
The vast majority of statistics and

922
00:37:59,058 --> 00:37:59,058
machine learning applications involve

923
00:37:59,058 --> 00:37:59,058
solving optimization problems.

924
00:37:53,052 --> 00:38:01,054
The author is right.

925
00:37:59,058 --> 00:38:07,060
Accelerated gradientbased methods and

926
00:38:07,060 --> 00:38:07,060
several variations thereof have become

927
00:38:07,060 --> 00:38:07,060
workhorses in these Fields.

928
00:38:01,054 --> 00:38:13,066
Recently, there has been great interest

929
00:38:13,066 --> 00:38:13,066
in studying such methods from a

930
00:38:13,066 --> 00:38:13,066
continuous time limiting perspective.

931
00:38:08,061 --> 00:38:19,072
Such methods can be seen as first order

932
00:38:19,072 --> 00:38:19,072
integrators to a classical Hamiltonian

933
00:38:19,072 --> 00:38:19,072
system with dissipation.

934
00:38:13,066 --> 00:38:32,085
This raises the question on how to

935
00:38:32,085 --> 00:38:32,085
discretize the system such that important

936
00:38:32,085 --> 00:38:32,085
properties are preserved, assuming the

937
00:38:32,085 --> 00:38:32,085
system has fast conversions to critical

938
00:38:32,085 --> 00:38:32,085
point and desirable stability

939
00:38:32,085 --> 00:38:32,085
properties.

940
00:38:20,073 --> 00:38:37,090
Originally, such a theory of geometric

941
00:38:37,090 --> 00:38:37,090
integration was developed with

942
00:38:37,090 --> 00:38:37,090
conservative systems in.

943
00:38:33,086 --> 00:38:44,097
Mind, while optimization in optimization,

944
00:38:44,097 --> 00:38:44,097
the associated system is naturally a

945
00:38:44,097 --> 00:38:44,097
dissipative one.

946
00:38:37,090 --> 00:38:58,110
More recently, it has been proved that a

947
00:38:58,110 --> 00:38:58,110
generalization of symplectic integrators

948
00:38:58,110 --> 00:38:58,110
to dissipative Hamiltonian systems is

949
00:38:58,110 --> 00:38:58,110
indeed able to preserve rates of

950
00:38:58,110 --> 00:38:58,110
coherence and stability, which are the

951
00:38:58,110 --> 00:38:58,110
main properties of interest for

952
00:38:58,110 --> 00:38:58,110
optimization.

953
00:38:45,098 --> 00:39:07,114
So from the Wikipedia on Hamiltonian

954
00:39:07,114 --> 00:39:07,114
systems an example of a time independent

955
00:39:07,114 --> 00:39:07,114
Hamiltonian system is the harmonic

956
00:39:07,114 --> 00:39:07,114
oscillator.

957
00:38:58,111 --> 00:39:17,124
So it's just a frictionless spring

958
00:39:17,124 --> 00:39:17,124
oscillating around the Hamiltonian of the

959
00:39:17,124 --> 00:39:17,124
system does not depend on time, and thus

960
00:39:17,124 --> 00:39:17,124
the energy of the system is conserved.

961
00:39:07,114 --> 00:39:22,129
And so we can say that that is a

962
00:39:22,129 --> 00:39:22,129
conservative Hamiltonian.

963
00:39:17,124 --> 00:39:26,133
If the Hamiltonian decays in time, it is

964
00:39:26,133 --> 00:39:26,133
dissipative.

965
00:39:23,130 --> 00:39:46,153
Recently, the advances of these authors

966
00:39:46,153 --> 00:39:46,153
and others have extended our ability to

967
00:39:46,153 --> 00:39:46,153
make distinctions of dissipative

968
00:39:46,153 --> 00:39:46,153
Hamiltonians that respect the rates of

969
00:39:46,153 --> 00:39:46,153
conversions and stability, which are the

970
00:39:46,153 --> 00:39:46,153
main properties of interest for

971
00:39:46,153 --> 00:39:46,153
optimization.

972
00:39:28,134 --> 00:39:55,162
And we've talked about Hamiltonians on

973
00:39:55,162 --> 00:39:55,162
Livestream number 49 with Dalton

974
00:39:55,162 --> 00:39:55,162
Sakthivadivel Devil on Bayesian

975
00:39:55,162 --> 00:39:55,162
mechanics.

976
00:39:47,153 --> 00:39:56,163
So pretty cool.

977
00:39:55,162 --> 00:39:58,165
What is conservative?

978
00:39:57,164 --> 00:39:59,166
What is dissipative?

979
00:39:58,165 --> 00:40:11,172
We'll explore as the final keyword here,

980
00:40:11,172 --> 00:40:11,172
I just wanted to start with a blank slide

981
00:40:11,172 --> 00:40:11,172
on active inference and see what happens

982
00:40:11,172 --> 00:40:11,172
in 52.1.

983
00:40:00,161 --> 00:40:20,181
So consider the slide to be blank and for

984
00:40:20,181 --> 00:40:20,181
the authors and discussants looking

985
00:40:20,181 --> 00:40:20,181
forward to hearing Hohwy.

986
00:40:11,172 --> 00:40:22,183
Are we approaching active inference?

987
00:40:20,181 --> 00:40:28,189
Given everything that we've just loaded

988
00:40:28,189 --> 00:40:28,189
onto the table and everywhere that we're

989
00:40:28,189 --> 00:40:28,189
about to go with the paper?

990
00:40:22,183 --> 00:40:30,191
Let's get into it.

991
00:40:28,189 --> 00:40:45,206
Section One introduction so, the authors

992
00:40:45,206 --> 00:40:45,206
begin with differential geometry plays a

993
00:40:45,206 --> 00:40:45,206
fundamental role in applied mathematics,

994
00:40:45,206 --> 00:40:45,206
statistics and computer science,

995
00:40:45,206 --> 00:40:45,206
including various domains.

996
00:40:30,191 --> 00:40:50,211
Citations one through 22, which can be

997
00:40:50,211 --> 00:40:50,211
shown here.

998
00:40:45,206 --> 00:40:53,214
Various citations from this well cited

999
00:40:53,214 --> 00:40:53,214
paper.

1000
00:40:50,211 --> 00:41:12,227
The geometric study of statistical models

1001
00:41:12,227 --> 00:41:12,227
has had many successes, ranging from

1002
00:41:12,227 --> 00:41:12,227
statistical inference, where it was used

1003
00:41:12,227 --> 00:41:12,227
to prove the optimality of maximum

1004
00:41:12,227 --> 00:41:12,227
likelihood estimator, to the construction

1005
00:41:12,227 --> 00:41:12,227
of the category of mathematical

1006
00:41:12,227 --> 00:41:12,227
statistics generated by Markov

1007
00:41:12,227 --> 00:41:12,227
morphisms.

1008
00:40:54,215 --> 00:41:16,231
So what are Markov morphisms from Ncat

1009
00:41:16,231 --> 00:41:16,231
lab?

1010
00:41:12,227 --> 00:41:26,241
The formalism of Markov categories can be

1011
00:41:26,241 --> 00:41:26,241
thought of as a way to express certain

1012
00:41:26,241 --> 00:41:26,241
aspects of probability and statistics

1013
00:41:26,241 --> 00:41:26,241
synthetically.

1014
00:41:16,231 --> 00:41:39,254
In other words, it consists of structure

1015
00:41:39,254 --> 00:41:39,254
and axioms, which one can think of as

1016
00:41:39,254 --> 00:41:39,254
fundamental in probability and

1017
00:41:39,254 --> 00:41:39,254
statistics, which one can use to prove

1018
00:41:39,254 --> 00:41:39,254
theorems without having to use measure

1019
00:41:39,254 --> 00:41:39,254
theory directly.

1020
00:41:26,241 --> 00:41:51,266
Intuitively, for the purposes of

1021
00:41:51,266 --> 00:41:51,266
probability, a Markov category can be

1022
00:41:51,266 --> 00:41:51,266
seen as a category where morphisms behave

1023
00:41:51,266 --> 00:41:51,266
like random functions or Markov kernels,

1024
00:41:51,266 --> 00:41:51,266
hence the name.

1025
00:41:40,255 --> 00:41:55,270
So, just some red text speculation.

1026
00:41:52,267 --> 00:42:10,279
It was a big success and a breakthrough

1027
00:42:10,279 --> 00:42:10,279
to think about the category from a formal

1028
00:42:10,279 --> 00:42:10,279
category theoretic perspective, to think

1029
00:42:10,279 --> 00:42:10,279
about the category of statistical

1030
00:42:10,279 --> 00:42:10,279
distinctions from an information

1031
00:42:10,279 --> 00:42:10,279
geometric perspective.

1032
00:41:55,270 --> 00:42:25,294
This means that we can understand many

1033
00:42:25,294 --> 00:42:25,294
analytical properties and transformations

1034
00:42:25,294 --> 00:42:25,294
of statistical distributions hashtag

1035
00:42:25,294 --> 00:42:25,294
Bayesian and develop general methods that

1036
00:42:25,294 --> 00:42:25,294
work for that whole category, like

1037
00:42:25,294 --> 00:42:25,294
accelerated optimization.

1038
00:42:11,280 --> 00:42:28,297
And the applications are vast.

1039
00:42:26,295 --> 00:42:37,306
Originally, such a theory of geometric

1040
00:42:37,306 --> 00:42:37,306
integration was developed with

1041
00:42:37,306 --> 00:42:37,306
conservative systems in mind.

1042
00:42:31,300 --> 00:42:42,311
While in optimization the associated

1043
00:42:42,311 --> 00:42:42,311
system is naturally a dissipative one.

1044
00:42:37,306 --> 00:42:46,315
Nevertheless, symplectic integrators were

1045
00:42:46,315 --> 00:42:46,315
exploited in this concept.

1046
00:42:42,311 --> 00:43:01,324
More recently, it has been proved that a

1047
00:43:01,324 --> 00:43:01,324
generalization of symplectic integrators

1048
00:43:01,324 --> 00:43:01,324
to dissipative Hamiltonian systems is

1049
00:43:01,324 --> 00:43:01,324
indeed able to preserve rates of

1050
00:43:01,324 --> 00:43:01,324
convergence and stability, which are the

1051
00:43:01,324 --> 00:43:01,324
main properties of interest for

1052
00:43:01,324 --> 00:43:01,324
optimization.

1053
00:42:47,316 --> 00:43:20,343
So, citation nine, what is it about 2021

1054
00:43:20,343 --> 00:43:20,343
paper on dissipative symplectic

1055
00:43:20,343 --> 00:43:20,343
integration with applications to gradient

1056
00:43:20,343 --> 00:43:20,343
based optimization by some of the

1057
00:43:20,343 --> 00:43:20,343
authors, and it's interesting to pull out

1058
00:43:20,343 --> 00:43:20,343
some quotes from the abstract and just

1059
00:43:20,343 --> 00:43:20,343
hear a little bit about what they're up

1060
00:43:20,343 --> 00:43:20,343
to.

1061
00:43:01,324 --> 00:43:31,354
Recently, continuous time dynamical

1062
00:43:31,354 --> 00:43:31,354
systems have proved useful in providing

1063
00:43:31,354 --> 00:43:31,354
conceptual and quantitative insights into

1064
00:43:31,354 --> 00:43:31,354
gradient based optimization widely used

1065
00:43:31,354 --> 00:43:31,354
in Modern machine learning and

1066
00:43:31,354 --> 00:43:31,354
statistics.

1067
00:43:21,344 --> 00:43:39,362
An important question that arises in this

1068
00:43:39,362 --> 00:43:39,362
line of work is how to discritize the

1069
00:43:39,362 --> 00:43:39,362
system in such a way that its stability

1070
00:43:39,362 --> 00:43:39,362
and rates of convergence are preserved.

1071
00:43:32,355 --> 00:43:41,364
So this is the sampling problem.

1072
00:43:40,362 --> 00:43:45,368
In continuum time, the math is continuous

1073
00:43:45,368 --> 00:43:45,368
and nice.

1074
00:43:41,364 --> 00:44:00,377
However, the sampling problem which

1075
00:44:00,377 --> 00:44:00,377
actually comes into play when we use

1076
00:44:00,377 --> 00:44:00,377
Modern computational methods that are

1077
00:44:00,377 --> 00:44:00,377
implemented on discrete time, space and

1078
00:44:00,377 --> 00:44:00,377
constraints, what happens when we use

1079
00:44:00,377 --> 00:44:00,377
unconventional computing?

1080
00:43:45,368 --> 00:44:06,383
That's an interesting discussion

1081
00:44:06,383 --> 00:44:06,383
question, but in active today, the

1082
00:44:06,383 --> 00:44:06,383
discretization approach is going to

1083
00:44:06,383 --> 00:44:06,383
matter a lot.

1084
00:44:00,377 --> 00:44:18,395
In this paper, we propose a geometric

1085
00:44:18,395 --> 00:44:18,395
framework in which such discretizations

1086
00:44:18,395 --> 00:44:18,395
can be realized, systematically enabling

1087
00:44:18,395 --> 00:44:18,395
the derivation of ratematching algorithms

1088
00:44:18,395 --> 00:44:18,395
without the need for a discrete coherence

1089
00:44:18,395 --> 00:44:18,395
analysis.

1090
00:44:07,384 --> 00:44:29,406
More specifically, we show that a

1091
00:44:29,406 --> 00:44:29,406
generalization of symplectic integrators

1092
00:44:29,406 --> 00:44:29,406
to nonconservative, and in particular,

1093
00:44:29,406 --> 00:44:29,406
discipline of Hamiltonian systems is able

1094
00:44:29,406 --> 00:44:29,406
to preserve rates of convergence up to a

1095
00:44:29,406 --> 00:44:29,406
controlled errors.

1096
00:44:18,395 --> 00:44:40,417
So this is the advance of this paper that

1097
00:44:40,417 --> 00:44:40,417
within a controlled error good enough,

1098
00:44:40,417 --> 00:44:40,417
they can model conservative and

1099
00:44:40,417 --> 00:44:40,417
nonconservative, even dissipative

1100
00:44:40,417 --> 00:44:40,417
Hamiltonian systems.

1101
00:44:29,406 --> 00:44:57,434
Moreover, such methods preserve a shadow

1102
00:44:57,434 --> 00:44:57,434
Hamiltonian despite the absence of a

1103
00:44:57,434 --> 00:44:57,434
conversation law extending key results of

1104
00:44:57,434 --> 00:44:57,434
symplectic integrators to nonconservative

1105
00:44:57,434 --> 00:44:57,434
cases, our arguments rely on a

1106
00:44:57,434 --> 00:44:57,434
combination of backwards error analysis

1107
00:44:57,434 --> 00:44:57,434
with fundamental results from symplectic

1108
00:44:57,434 --> 00:44:57,434
geometry.

1109
00:44:41,418 --> 00:45:15,446
We stress that although the original

1110
00:45:15,446 --> 00:45:15,446
motivation for this work was the

1111
00:45:15,446 --> 00:45:15,446
application of optimization, where

1112
00:45:15,446 --> 00:45:15,446
dissipative systems play a natural role,

1113
00:45:15,446 --> 00:45:15,446
they are fully general and not only

1114
00:45:15,446 --> 00:45:15,446
provide a differential geometric analysis

1115
00:45:15,446 --> 00:45:15,446
for discipline of Hamiltonian systems,

1116
00:45:15,446 --> 00:45:15,446
but also substantially extend the theory

1117
00:45:15,446 --> 00:45:15,446
of structure preserving inclination.

1118
00:44:57,434 --> 00:45:18,449
So they made this generalization.

1119
00:45:15,446 --> 00:45:26,457
In the case of optimization, however,

1120
00:45:26,457 --> 00:45:26,457
it's exciting that it extends even deeper

1121
00:45:26,457 --> 00:45:26,457
into the math and the symmetry.

1122
00:45:19,450 --> 00:45:32,463
Those are a few highlights from section

1123
00:45:32,463 --> 00:45:32,463
one on to section two.

1124
00:45:27,458 --> 00:45:34,465
Accelerated optimization vroom, vroom.

1125
00:45:32,463 --> 00:45:35,466
Here we go.

1126
00:45:34,465 --> 00:45:47,478
So there are going to be a lot of slides

1127
00:45:47,478 --> 00:45:47,478
from sections two through five, and to

1128
00:45:47,478 --> 00:45:47,478
keep the video a reasonable length, I'm

1129
00:45:47,478 --> 00:45:47,478
going to just highlight a few pieces.

1130
00:45:36,467 --> 00:45:54,485
Not even necessarily what I intended to

1131
00:45:54,485 --> 00:45:54,485
highlight, but I'm just going to go for

1132
00:45:54,485 --> 00:45:54,485
it so we can get through it.

1133
00:45:47,478 --> 00:46:17,502
Section two we shall be concerned with a

1134
00:46:17,502 --> 00:46:17,502
problem of optimization of a function

1135
00:46:17,502 --> 00:46:17,502
finding a point that maximizes V of Q or

1136
00:46:17,502 --> 00:46:17,502
minimizes negative V of Q over a smooth

1137
00:46:17,502 --> 00:46:17,502
manifold M r in the real numbers, many

1138
00:46:17,502 --> 00:46:17,502
algorithms and optimization are given as

1139
00:46:17,502 --> 00:46:17,502
a sequence of active inference.

1140
00:45:55,485 --> 00:46:25,510
Even when these algorithms are seen as

1141
00:46:25,510 --> 00:46:25,510
distinctions of a continuum system whose

1142
00:46:25,510 --> 00:46:25,510
behavior is presumably understood.

1143
00:46:18,503 --> 00:46:32,517
It is well known that most

1144
00:46:32,517 --> 00:46:32,517
discretizations break important

1145
00:46:32,517 --> 00:46:32,517
properties of a system, and they continue

1146
00:46:32,517 --> 00:46:32,517
to write.

1147
00:46:26,511 --> 00:46:45,530
The analysis of such finite difference

1148
00:46:45,530 --> 00:46:45,530
iterations is usually challenging,

1149
00:46:45,530 --> 00:46:45,530
relying on painstaking algebra to obtain

1150
00:46:45,530 --> 00:46:45,530
theoretical guarantees such as

1151
00:46:45,530 --> 00:46:45,530
conversation to a critical point,

1152
00:46:45,530 --> 00:46:45,530
stability and rates of conversation to a

1153
00:46:45,530 --> 00:46:45,530
critical point.

1154
00:46:32,517 --> 00:46:58,543
Even when these algorithms are seen as

1155
00:46:58,543 --> 00:46:58,543
discreditizations of a continuum system

1156
00:46:58,543 --> 00:46:58,543
whose behavior is presumably understood,

1157
00:46:58,543 --> 00:46:58,543
it is well known that most

1158
00:46:58,543 --> 00:46:58,543
discreditizations break important

1159
00:46:58,543 --> 00:46:58,543
properties of the system.

1160
00:46:46,531 --> 00:47:09,548
It can't be highlighted enough when we

1161
00:47:09,548 --> 00:47:09,548
implement optimization algorithms on

1162
00:47:09,548 --> 00:47:09,548
Modern computers, we have to make

1163
00:47:09,548 --> 00:47:09,548
discreditizations in space and time and

1164
00:47:09,548 --> 00:47:09,548
in practice.

1165
00:46:58,543 --> 00:47:29,568
So even if the analytical properties of

1166
00:47:29,568 --> 00:47:29,568
the distribution are totally fire, when

1167
00:47:29,568 --> 00:47:29,568
we try to solve and fit models to

1168
00:47:29,568 --> 00:47:29,568
empirical data, unless we also match that

1169
00:47:29,568 --> 00:47:29,568
elegance and power with a inclination

1170
00:47:29,568 --> 00:47:29,568
approach, we end up failing to realize

1171
00:47:29,568 --> 00:47:29,568
analytical promises.

1172
00:47:09,548 --> 00:47:32,571
2.1.

1173
00:47:31,570 --> 00:47:40,579
The principle of Geometric Integration

1174
00:47:40,579 --> 00:47:40,579
fortunately, the author's right here

1175
00:47:40,579 --> 00:47:40,579
comes into play one of the most

1176
00:47:40,579 --> 00:47:40,579
fundamental ideas of geometric

1177
00:47:40,579 --> 00:47:40,579
integrations.

1178
00:47:32,571 --> 00:47:48,587
Many numerical integrators are very close

1179
00:47:48,587 --> 00:47:48,587
exponentially in the step size to a

1180
00:47:48,587 --> 00:47:48,587
smooth dynamics generated by a shadow

1181
00:47:48,587 --> 00:47:48,587
vector field.

1182
00:47:40,579 --> 00:47:56,595
A little whisper of a shadow Hamiltonian

1183
00:47:56,595 --> 00:47:56,595
and the shadow vector field is a

1184
00:47:56,595 --> 00:47:56,595
perturbation of the original vector

1185
00:47:56,595 --> 00:47:56,595
field.

1186
00:47:49,588 --> 00:48:08,601
This allows us to analyze the discrete

1187
00:48:08,601 --> 00:48:08,601
trajectory implemented by the algorithm

1188
00:48:08,601 --> 00:48:08,601
using powerful tools from dynamical

1189
00:48:08,601 --> 00:48:08,601
systems and differential geometry, which

1190
00:48:08,601 --> 00:48:08,601
are a priori reserved to smooth systems.

1191
00:47:56,595 --> 00:48:15,608
So we're going to be able to discretize

1192
00:48:15,608 --> 00:48:15,608
our cake and have it be smooth too.

1193
00:48:09,602 --> 00:48:27,620
Crucially, while numerical integrators

1194
00:48:27,620 --> 00:48:27,620
typically diverge significantly from the

1195
00:48:27,620 --> 00:48:27,620
dynamics they aim to simulate, geometric

1196
00:48:27,620 --> 00:48:27,620
integrators respect the main properties

1197
00:48:27,620 --> 00:48:27,620
of a system in the context of

1198
00:48:27,620 --> 00:48:27,620
optimization.

1199
00:48:15,608 --> 00:48:36,629
This means respecting stability and rates

1200
00:48:36,629 --> 00:48:36,629
of coherence seems like a good idea to

1201
00:48:36,629 --> 00:48:36,629
respect the main properties of the

1202
00:48:36,629 --> 00:48:36,629
system.

1203
00:48:27,620 --> 00:48:40,633
This was first demonstrated in nine and

1204
00:48:40,633 --> 00:48:40,633
further extended in ten.

1205
00:48:36,629 --> 00:48:43,636
Our discussion will be based on these

1206
00:48:43,636 --> 00:48:43,636
work.

1207
00:48:40,633 --> 00:49:16,663
So citation Nine, Franca, Jordan and

1208
00:49:16,663 --> 00:49:16,663
Vidal mentioned previously and citation

1209
00:49:16,663 --> 00:49:16,663
ten, Franca, Mark, Girolami and Jordan

1210
00:49:16,663 --> 00:49:16,663
optimization on manifolds a symplectic

1211
00:49:16,663 --> 00:49:16,663
approach authors previous work So big

1212
00:49:16,663 --> 00:49:16,663
development we can use geometric rather

1213
00:49:16,663 --> 00:49:16,663
than course numerical integrators, so our

1214
00:49:16,663 --> 00:49:16,663
sampling based optimization schemes,

1215
00:49:16,663 --> 00:49:16,663
including their discordization, respect

1216
00:49:16,663 --> 00:49:16,663
the main properties of the system.

1217
00:48:43,636 --> 00:49:17,664
Sounds great.

1218
00:49:16,663 --> 00:49:27,674
Section Two Two conservative flows and

1219
00:49:27,674 --> 00:49:27,674
symplectic integrators as a stepping

1220
00:49:27,674 --> 00:49:27,674
stone, we first discussed the

1221
00:49:27,674 --> 00:49:27,674
construction of suitable conservative

1222
00:49:27,674 --> 00:49:27,674
flows.

1223
00:49:18,665 --> 00:49:33,680
These are very well studied and they're

1224
00:49:33,680 --> 00:49:33,680
very intuitive.

1225
00:49:30,677 --> 00:49:35,682
More intuitive.

1226
00:49:34,681 --> 00:49:50,697
To construct vector Fields along the

1227
00:49:50,697 --> 00:49:50,697
derivative of x, which is the function of

1228
00:49:50,697 --> 00:49:50,697
flows along which some function is

1229
00:49:50,697 --> 00:49:50,697
constant, we shall need brackets

1230
00:49:50,697 --> 00:49:50,697
geometrically.

1231
00:49:36,683 --> 00:49:56,703
These are morphisms x star to x, also

1232
00:49:56,703 --> 00:49:56,703
known as contravariant tensors of rank

1233
00:49:56,703 --> 00:49:56,703
two in physics.

1234
00:49:50,697 --> 00:50:10,711
So calling back the two form and the

1235
00:50:10,711 --> 00:50:10,711
brackets, importantly, vector Fields that

1236
00:50:10,711 --> 00:50:10,711
preserve f correspond to bracket vector

1237
00:50:10,711 --> 00:50:10,711
Fields in which B is antisemitic.

1238
00:49:56,703 --> 00:50:14,715
Constructing conservative flows is thus

1239
00:50:14,715 --> 00:50:14,715
straightforward.

1240
00:50:10,711 --> 00:50:20,721
Unfortunately, it is a rather more

1241
00:50:20,721 --> 00:50:20,721
challenging task to construct efficient

1242
00:50:20,721 --> 00:50:20,721
discretizations that retain this

1243
00:50:20,721 --> 00:50:20,721
property.

1244
00:50:14,715 --> 00:50:30,731
Most well known procedures, namely

1245
00:50:30,731 --> 00:50:30,731
discrete gradient and projection methods,

1246
00:50:30,731 --> 00:50:30,731
only give rise to integrators that

1247
00:50:30,731 --> 00:50:30,731
require solving implicit equations at

1248
00:50:30,731 --> 00:50:30,731
every step.

1249
00:50:21,722 --> 00:50:36,737
And they may break other important

1250
00:50:36,737 --> 00:50:36,737
properties of the system inclination.

1251
00:50:30,731 --> 00:50:40,741
96 this is the issue again.

1252
00:50:36,737 --> 00:50:53,754
No matter how nicely behaved in

1253
00:50:53,754 --> 00:50:53,754
principle, our analytical underlying

1254
00:50:53,754 --> 00:50:53,754
smooth differentiable function is if we

1255
00:50:53,754 --> 00:50:53,754
discretize it and we chop it up in a way

1256
00:50:53,754 --> 00:50:53,754
that's inappropriate.

1257
00:50:40,741 --> 00:51:00,755
We don't respect the properties of the

1258
00:51:00,755 --> 00:51:00,755
system, we don't end up with being able

1259
00:51:00,755 --> 00:51:00,755
to realize those analytical promises.

1260
00:50:54,755 --> 00:51:18,773
Indeed, in practice, the Hamiltonian

1261
00:51:18,773 --> 00:51:18,773
usually decomposes into a potential

1262
00:51:18,773 --> 00:51:18,773
energy associated to position and

1263
00:51:18,773 --> 00:51:18,773
independent of momentum, and a kinetic

1264
00:51:18,773 --> 00:51:18,773
energy associated to momentum and

1265
00:51:18,773 --> 00:51:18,773
invariant under position changes, both

1266
00:51:18,773 --> 00:51:18,773
generating tractable flows.

1267
00:51:02,757 --> 00:51:25,780
Thanks to this decomposition, we are able

1268
00:51:25,780 --> 00:51:25,780
to construct numerical methods through

1269
00:51:25,780 --> 00:51:25,780
splitting the vector field.

1270
00:51:19,774 --> 00:51:39,794
A few ideas coming together here,

1271
00:51:39,794 --> 00:51:39,794
recalling some of our generalized

1272
00:51:39,794 --> 00:51:39,794
coordinate based approaches to non

1273
00:51:39,794 --> 00:51:39,794
equilibrium steady states and Bayesian

1274
00:51:39,794 --> 00:51:39,794
mechanics, and also decomposition of

1275
00:51:39,794 --> 00:51:39,794
complex functions.

1276
00:51:25,780 --> 00:51:48,803
Note also that for symplectic brackets,

1277
00:51:48,803 --> 00:51:48,803
the evidence of a shadow Hamiltonian can

1278
00:51:48,803 --> 00:51:48,803
be guaranteed beyond the case of

1279
00:51:48,803 --> 00:51:48,803
splitting methods, eg.

1280
00:51:40,795 --> 00:52:04,813
For variational integrators, which use a

1281
00:52:04,813 --> 00:52:04,813
discrete version of Hamiltonians

1282
00:52:04,813 --> 00:52:04,813
Hamilton's principle of Least action and

1283
00:52:04,813 --> 00:52:04,813
for most symplectic integrators in which

1284
00:52:04,813 --> 00:52:04,813
the symplectic bracket is preserved up to

1285
00:52:04,813 --> 00:52:04,813
topological considerations described by

1286
00:52:04,813 --> 00:52:04,813
the first Dean cohomology of FaceBase.

1287
00:51:48,803 --> 00:52:11,820
So for discussion with authors annual

1288
00:52:11,820 --> 00:52:11,820
what are the splitting integrators?

1289
00:52:05,814 --> 00:52:13,822
What is split from what and why?

1290
00:52:11,820 --> 00:52:17,826
What is the bracket notation or

1291
00:52:17,826 --> 00:52:17,826
operation?

1292
00:52:14,823 --> 00:52:20,828
And what is a piss on bracket?

1293
00:52:17,826 --> 00:52:21,830
What is a piss on system?

1294
00:52:20,829 --> 00:52:36,845
Section 23 rate matching integrators for

1295
00:52:36,845 --> 00:52:36,845
smooth Optimization so, having obtained a

1296
00:52:36,845 --> 00:52:36,845
vast family of smooth dynamics and

1297
00:52:36,845 --> 00:52:36,845
integrators that closely preserve F, we

1298
00:52:36,845 --> 00:52:36,845
can now apply these ideas to

1299
00:52:36,845 --> 00:52:36,845
optimization.

1300
00:52:22,831 --> 00:52:40,849
What is being set up in this section and

1301
00:52:40,849 --> 00:52:40,849
why?

1302
00:52:37,846 --> 00:52:57,866
In equation seven we see that the damping

1303
00:52:57,866 --> 00:52:57,866
coefficient gamma of t being greater than

1304
00:52:57,866 --> 00:52:57,866
zero reflects the dissipative component,

1305
00:52:57,866 --> 00:52:57,866
or the second term in that right hand

1306
00:52:57,866 --> 00:52:57,866
side of equation seven.

1307
00:52:40,849 --> 00:53:01,864
So damping coefficient controls the

1308
00:53:01,864 --> 00:53:01,864
strength of the dissipation.

1309
00:52:57,866 --> 00:53:11,874
One can imagine that if the damping

1310
00:53:11,874 --> 00:53:11,874
coefficient is zero, the dissipative side

1311
00:53:11,874 --> 00:53:11,874
zeros out and b has conservative behavior

1312
00:53:11,874 --> 00:53:11,874
and so on.

1313
00:53:01,864 --> 00:53:14,877
So what is being set up here and why?

1314
00:53:11,874 --> 00:53:28,891
Dot dot dot dot dot the existence of such

1315
00:53:28,891 --> 00:53:28,891
a Leopold function described above

1316
00:53:28,891 --> 00:53:28,891
implies that trajectories starting in the

1317
00:53:28,891 --> 00:53:28,891
neighborhood of Q star will converge to Q

1318
00:53:28,891 --> 00:53:28,891
star.

1319
00:53:15,878 --> 00:53:33,896
So it's like a ball rolling to the bottom

1320
00:53:33,896 --> 00:53:33,896
of a hill, just like we've always

1321
00:53:33,896 --> 00:53:33,896
wanted.

1322
00:53:28,891 --> 00:53:44,907
In other words, the above system provably

1323
00:53:44,907 --> 00:53:44,907
solves the optimization problem minimum

1324
00:53:44,907 --> 00:53:44,907
of V on Q, such that Q is in the D

1325
00:53:44,907 --> 00:53:44,907
dimension.

1326
00:53:34,897 --> 00:53:47,910
Real punchline.

1327
00:53:44,907 --> 00:54:08,925
We're setting up a system with good

1328
00:54:08,925 --> 00:54:08,925
smooth optimization characteristics ball

1329
00:54:08,925 --> 00:54:08,925
rolling to the bottom of a smooth hill

1330
00:54:08,925 --> 00:54:08,925
that is also on the path or using the

1331
00:54:08,925 --> 00:54:08,925
notation of or prepared to make the

1332
00:54:08,925 --> 00:54:08,925
transformation to attractable

1333
00:54:08,925 --> 00:54:08,925
discretization approach.

1334
00:53:47,910 --> 00:54:11,928
So some more on the damping coefficient.

1335
00:54:08,925 --> 00:54:16,933
They bring up some common choices for the

1336
00:54:16,933 --> 00:54:16,933
damping coefficient.

1337
00:54:11,928 --> 00:54:21,938
And Big O notation describes on the order

1338
00:54:21,938 --> 00:54:21,938
of which something occurs.

1339
00:54:16,933 --> 00:54:30,947
For example, linearly order of the

1340
00:54:30,947 --> 00:54:30,947
variable or sub or super linearly as a

1341
00:54:30,947 --> 00:54:30,947
function of time or data points.

1342
00:54:21,938 --> 00:54:41,958
So in computational complexity analysis,

1343
00:54:41,958 --> 00:54:41,958
people are often interested as I double

1344
00:54:41,958 --> 00:54:41,958
the amount of data I'm analyzing, does

1345
00:54:41,958 --> 00:54:41,958
that make the algorithm take twice as

1346
00:54:41,958 --> 00:54:41,958
long?

1347
00:54:30,947 --> 00:54:44,961
That's linear computational complexity?

1348
00:54:41,958 --> 00:54:46,963
Does it take four times as long?

1349
00:54:44,961 --> 00:54:49,966
Does it take the same amount of time?

1350
00:54:46,963 --> 00:54:50,967
And so on.

1351
00:54:49,966 --> 00:55:08,979
And it'll be great to talk about the

1352
00:55:08,979 --> 00:55:08,979
intuitions and implications and

1353
00:55:08,979 --> 00:55:08,979
generalizations about how the damping

1354
00:55:08,979 --> 00:55:08,979
coefficient influences the computational

1355
00:55:08,979 --> 00:55:08,979
complexity estimates for convergence in

1356
00:55:08,979 --> 00:55:08,979
different settings.

1357
00:54:50,967 --> 00:55:17,988
They write the conservative system from

1358
00:55:17,988 --> 00:55:17,988
equation 16 reduces precisely the

1359
00:55:17,988 --> 00:55:17,988
original dissipative system 13.

1360
00:55:11,982 --> 00:55:27,998
The second equation in 16 reproduces 14,

1361
00:55:27,998 --> 00:55:27,998
and the remaining equations are

1362
00:55:27,998 --> 00:55:27,998
equivalent to the equations of motion

1363
00:55:27,998 --> 00:55:27,998
associated to 13, which in turn are

1364
00:55:27,998 --> 00:55:27,998
equivalent to eight.

1365
00:55:18,989 --> 00:55:28,999
As previously noted.

1366
00:55:27,998 --> 00:55:45,016
Formally, what we have done is to embed

1367
00:55:45,016 --> 00:55:45,016
the original dissipative system with

1368
00:55:45,016 --> 00:55:45,016
phase space r 2D, so real with 2D

1369
00:55:45,016 --> 00:55:45,016
dimension into a higher dimension

1370
00:55:45,016 --> 00:55:45,016
conservative system with phase space r 2D

1371
00:55:45,016 --> 00:55:45,016
plus two.

1372
00:55:29,000 --> 00:55:52,023
The dissipative dynamics thus lies on a

1373
00:55:52,023 --> 00:55:52,023
hyper surface of constant energy k equals

1374
00:55:52,023 --> 00:55:52,023
zero in high dimension.

1375
00:55:45,016 --> 00:55:58,029
The reason for doing this procedure,

1376
00:55:58,029 --> 00:55:58,029
called implication, is purely

1377
00:55:58,029 --> 00:55:58,029
theoretical.

1378
00:55:53,024 --> 00:55:59,030
Oh, come on.

1379
00:55:59,030 --> 00:56:01,026
It's not purely theoretical.

1380
00:55:59,030 --> 00:56:16,041
Since the theory of symplectic

1381
00:56:16,041 --> 00:56:16,041
integrators only accounts for

1382
00:56:16,041 --> 00:56:16,041
conservative systems, we can now extend

1383
00:56:16,041 --> 00:56:16,041
this theory to dissipative systems

1384
00:56:16,041 --> 00:56:16,041
settings by applying a symplectic

1385
00:56:16,041 --> 00:56:16,041
integrator to 13 and then fixing the

1386
00:56:16,041 --> 00:56:16,041
relevant coordinates 17 in the resulting

1387
00:56:16,041 --> 00:56:16,041
methods.

1388
00:56:01,026 --> 00:56:20,045
Geometrically, this corresponds to

1389
00:56:20,045 --> 00:56:20,045
integrating the time flow exactly.

1390
00:56:17,041 --> 00:56:25,050
We're going to talk about a relationship

1391
00:56:25,050 --> 00:56:25,050
between time and dissipative systems.

1392
00:56:21,046 --> 00:56:30,055
After all, it's dissipative systems that

1393
00:56:30,055 --> 00:56:30,055
are dissipating in time.

1394
00:56:26,051 --> 00:56:38,063
And so discreditization of time plays a

1395
00:56:38,063 --> 00:56:38,063
role in the appropriate inclination of a

1396
00:56:38,063 --> 00:56:38,063
dissipative Hamiltonian.

1397
00:56:30,055 --> 00:56:45,070
And in citation nine previously raised,

1398
00:56:45,070 --> 00:56:45,070
such a procedure was defined under the

1399
00:56:45,070 --> 00:56:45,070
name of presymplectic integrators.

1400
00:56:38,063 --> 00:56:53,078
And these connections hold not only for

1401
00:56:53,078 --> 00:56:53,078
the specific example above, but also for

1402
00:56:53,078 --> 00:56:53,078
general non conservative Hamiltonian

1403
00:56:53,078 --> 00:56:53,078
systems.

1404
00:56:45,070 --> 00:56:56,081
So what is happening here?

1405
00:56:53,078 --> 00:56:59,084
What's and intuition for conservative and

1406
00:56:59,084 --> 00:56:59,084
dissipative systems?

1407
00:56:56,081 --> 00:57:03,082
And how have the recent works of Franca

1408
00:57:03,082 --> 00:57:03,082
at all expanded what is possible?

1409
00:56:59,084 --> 00:57:10,089
We are now ready to explain why this

1410
00:57:10,089 --> 00:57:10,089
approach is suitable to construct

1411
00:57:10,089 --> 00:57:10,089
practical optimization methods.

1412
00:57:04,083 --> 00:57:20,099
The coordinate t sub k becomes simply the

1413
00:57:20,099 --> 00:57:20,099
time discreditization, which is exact,

1414
00:57:20,099 --> 00:57:20,099
and so is U sub k, since it is a function

1415
00:57:20,099 --> 00:57:20,099
of time alone.

1416
00:57:11,090 --> 00:57:24,103
Importantly, U does not couple to any of

1417
00:57:24,103 --> 00:57:24,103
the other degrees of freedom.

1418
00:57:21,100 --> 00:57:36,115
So it is irrelevant whether we have

1419
00:57:36,115 --> 00:57:36,115
access to U or not, because we're looking

1420
00:57:36,115 --> 00:57:36,115
to solve a function of t 17.

1421
00:57:24,103 --> 00:57:43,122
A can be substituted in 15 to get 18, and

1422
00:57:43,122 --> 00:57:43,122
you can replace 13 to get 19.

1423
00:57:36,115 --> 00:57:46,125
We'll talk more about it with the

1424
00:57:46,125 --> 00:57:46,125
authors.

1425
00:57:44,123 --> 00:57:51,130
Therefore, the known rates.

1426
00:57:49,128 --> 00:58:01,134
Equation eleven for the Continuum system

1427
00:58:01,134 --> 00:58:01,134
are nearly preserved, and so would be any

1428
00:58:01,134 --> 00:58:01,134
rates of more general time dependent

1429
00:58:01,134 --> 00:58:01,134
dissipative Hamiltonian systems.

1430
00:57:51,130 --> 00:58:07,140
Let us now present and explicit algorithm

1431
00:58:07,140 --> 00:58:07,140
to solve the optimization problem.

1432
00:58:02,135 --> 00:58:19,152
This is all happening as a consequence of

1433
00:58:19,152 --> 00:58:19,152
having a shadow Hamiltonian such that

1434
00:58:19,152 --> 00:58:19,152
geometric integrators are able to

1435
00:58:19,152 --> 00:58:19,152
reproduce all the relevant properties of

1436
00:58:19,152 --> 00:58:19,152
the Continuum system.

1437
00:58:09,142 --> 00:58:32,165
Section Two Four manifold and Constrained

1438
00:58:32,165 --> 00:58:32,165
Optimization following ten, we briefly

1439
00:58:32,165 --> 00:58:32,165
mentioned how the previous approach can

1440
00:58:32,165 --> 00:58:32,165
be extended in great generality to an

1441
00:58:32,165 --> 00:58:32,165
optimization problem.

1442
00:58:21,154 --> 00:58:40,173
So equation 21 we present our

1443
00:58:40,173 --> 00:58:40,173
minimization problem V of Q variational

1444
00:58:40,173 --> 00:58:40,173
distribution on Q.

1445
00:58:32,165 --> 00:58:43,176
Citation ten Franca et al.

1446
00:58:40,173 --> 00:58:48,181
They write There are essentially two ways

1447
00:58:48,181 --> 00:58:48,181
to solve this problem through a

1448
00:58:48,181 --> 00:58:48,181
dissipative Hamiltonian approach.

1449
00:58:43,176 --> 00:58:58,191
One is to simulate a Hamiltonian dynamics

1450
00:58:58,191 --> 00:58:58,191
on T star M by incorporating the metric

1451
00:58:58,191 --> 00:58:58,191
of M in the kinetic moving part of the

1452
00:58:58,191 --> 00:58:58,191
Hamiltonian.

1453
00:58:49,182 --> 00:59:08,195
Another is to consider a Hamiltonian

1454
00:59:08,195 --> 00:59:08,195
dynamics on RN and embed M into RN by

1455
00:59:08,195 --> 00:59:08,195
imposing several constraints.

1456
00:58:59,192 --> 00:59:12,199
The first approach uses a Lee group.

1457
00:59:09,196 --> 00:59:14,201
We'll talk more about it.

1458
00:59:12,199 --> 00:59:29,216
An example of a second approach one can

1459
00:59:29,216 --> 00:59:29,216
constrain the integrator on RN to define

1460
00:59:29,216 --> 00:59:29,216
a symplectic integrator on M via the

1461
00:59:29,216 --> 00:59:29,216
discrete constrained variational approach

1462
00:59:29,216 --> 00:59:29,216
by using some techniques.

1463
00:59:14,201 --> 00:59:38,225
The above method consists in a

1464
00:59:38,225 --> 00:59:38,225
dissipative generalization of the well

1465
00:59:38,225 --> 00:59:38,225
known rattle integrator from molecular

1466
00:59:38,225 --> 00:59:38,225
dynamics.

1467
00:59:31,218 --> 00:59:44,231
Citations 100 through 103 used in

1468
00:59:44,231 --> 00:59:44,231
computational biology.

1469
00:59:38,225 --> 00:59:55,242
Section Two Five gradient Flow as a High

1470
00:59:55,242 --> 00:59:55,242
Friction Limit let us provide some

1471
00:59:55,242 --> 00:59:55,242
intuition why simulating second order

1472
00:59:55,242 --> 00:59:55,242
systems is expected to field faster

1473
00:59:55,242 --> 00:59:55,242
algorithms.

1474
00:59:45,232 --> 00:59:58,245
As an illustration, consider figure one.

1475
00:59:56,243 --> 01:00:06,802
On the left, we'll look at in a second

1476
01:00:06,802 --> 01:00:06,802
where a particle immersed in a fluid

1477
01:00:06,802 --> 01:00:06,802
falls under the influence of a potential

1478
01:00:06,802 --> 01:00:06,802
force.

1479
00:59:58,245 --> 01:00:10,145
Negative delta sub q on V in Q.

1480
01:00:06,978 --> 01:00:21,219
So partial differential with respect to Q

1481
01:00:21,219 --> 01:00:21,219
of V on Q that plays a role of gravity

1482
01:00:21,219 --> 01:00:21,219
and is constrained to move on a surface.

1483
01:00:10,165 --> 01:00:23,470
So ball rolling down the hill.

1484
01:00:21,233 --> 01:00:25,646
We weren't joking about it.

1485
01:00:23,482 --> 01:00:32,390
In the underdamp case, the particle is

1486
01:00:32,390 --> 01:00:32,390
underwater, which is not so viscous, so

1487
01:00:32,390 --> 01:00:32,390
it has acceleration and moves fast.

1488
01:00:25,683 --> 01:00:34,565
It may even oscillate.

1489
01:00:33,403 --> 01:00:48,942
In the over damped case, the particle is

1490
01:00:48,942 --> 01:00:48,942
in a highly viscous fluid such as honey,

1491
01:00:48,942 --> 01:00:48,942
and the drag force that damping

1492
01:00:48,942 --> 01:00:48,942
coefficient gamma is comparable or

1493
01:00:48,942 --> 01:00:48,942
stronger to the gradient.

1494
01:00:34,583 --> 01:00:53,491
Thus the particle moves slowly since it

1495
01:00:53,491 --> 01:00:53,491
cannot accelerate during the same elapsed

1496
01:00:53,491 --> 01:00:53,491
time.

1497
01:00:48,956 --> 01:00:58,924
Delta T, an accelerated particle would

1498
01:00:58,924 --> 01:00:58,924
travel a longer disturbance.

1499
01:00:54,494 --> 01:01:01,624
Here's figure one.

1500
01:00:59,080 --> 01:01:07,238
So here y simulating second order systems

1501
01:01:07,238 --> 01:01:07,238
yields accelerated methods.

1502
01:01:01,661 --> 01:01:11,650
So on the left, constrained particle

1503
01:01:11,650 --> 01:01:11,650
falling in fluids of differential

1504
01:01:11,650 --> 01:01:11,650
viscosity.

1505
01:01:07,239 --> 01:01:15,084
Here on the left, we have the particle

1506
01:01:15,084 --> 01:01:15,084
falling through honey.

1507
01:01:11,686 --> 01:01:23,846
It's slowly making its way to the bottom

1508
01:01:23,846 --> 01:01:23,846
of the bowl, but it never really builds

1509
01:01:23,846 --> 01:01:23,846
speed.

1510
01:01:16,126 --> 01:01:38,302
Its terminal velocity is being dominated

1511
01:01:38,302 --> 01:01:38,302
by the viscosity of the fluid, whereas

1512
01:01:38,302 --> 01:01:38,302
this bowl falling through water or

1513
01:01:38,302 --> 01:01:38,302
falling through air is with respect to

1514
01:01:38,302 --> 01:01:38,302
the honey dampened bowl.

1515
01:01:23,864 --> 01:01:48,030
This is like an accelerated optimization,

1516
01:01:48,030 --> 01:01:48,030
and they present some numerical results

1517
01:01:48,030 --> 01:01:48,030
that help us bolster that intuition.

1518
01:01:38,344 --> 01:01:54,096
Really fun and embodied way to think

1519
01:01:54,096 --> 01:01:54,096
about optimization.

1520
01:01:48,039 --> 01:02:00,093
We've talked about that ball rolling to

1521
01:02:00,093 --> 01:02:00,093
the bottom of the hill and what the ball

1522
01:02:00,093 --> 01:02:00,093
does when it hits a small bump.

1523
01:01:54,098 --> 01:02:10,197
But we have not talked about what media

1524
01:02:10,197 --> 01:02:10,197
the Bull is floating through and the

1525
01:02:10,197 --> 01:02:10,197
media is the message and the fish doesn't

1526
01:02:10,197 --> 01:02:10,197
know what it's swimming through.

1527
01:02:00,093 --> 01:02:16,252
Section 2.6 optimization in the space of

1528
01:02:16,252 --> 01:02:16,252
probability measures.

1529
01:02:12,211 --> 01:02:24,339
It'll be great to explore more because

1530
01:02:24,339 --> 01:02:24,339
we're going to see free energy

1531
01:02:24,339 --> 01:02:24,339
calculations on the stationary density,

1532
01:02:24,339 --> 01:02:24,339
KL divergence and more.

1533
01:02:16,253 --> 01:02:38,475
For now, we can just say all those

1534
01:02:38,475 --> 01:02:38,475
optimization techniques we were bringing

1535
01:02:38,475 --> 01:02:38,475
up earlier, we're going to be able to do

1536
01:02:38,475 --> 01:02:38,475
them on information geometric spaces that

1537
01:02:38,475 --> 01:02:38,475
correspond to probability measures that

1538
01:02:38,475 --> 01:02:38,475
are well behaved.

1539
01:02:25,343 --> 01:02:44,530
Not all distinctions are probability

1540
01:02:44,530 --> 01:02:44,530
measures or probability distributions.

1541
01:02:38,477 --> 01:02:50,592
Just because you draw a line doesn't mean

1542
01:02:50,592 --> 01:02:50,592
you can use that in part of your

1543
01:02:50,592 --> 01:02:50,592
variational inference scheme.

1544
01:02:44,531 --> 01:02:51,607
For Bayesian statistics.

1545
01:02:50,592 --> 01:02:54,629
That's section two.

1546
01:02:52,616 --> 01:02:58,676
On to section three Hamiltonian based

1547
01:02:58,676 --> 01:02:58,676
accelerated sampling.

1548
01:02:54,639 --> 01:03:07,706
So we talked about the Hamiltonian

1549
01:03:07,706 --> 01:03:07,706
conservative and dissipative and the

1550
01:03:07,706 --> 01:03:07,706
shadow Hamiltonian, which is going to be

1551
01:03:07,706 --> 01:03:07,706
orders of magnitude better to

1552
01:03:07,706 --> 01:03:07,706
approximate.

1553
01:02:58,676 --> 01:03:15,786
We talked about gradientbased methods and

1554
01:03:15,786 --> 01:03:15,786
how accelerated sampling is going to help

1555
01:03:15,786 --> 01:03:15,786
us accelerate those sampling techniques.

1556
01:03:07,709 --> 01:03:20,832
And now section three brings it together

1557
01:03:20,832 --> 01:03:20,832
with Hamiltonian based accelerated

1558
01:03:20,832 --> 01:03:20,832
sampling.

1559
01:03:15,787 --> 01:03:31,941
Section Three the purpose of sampling

1560
01:03:31,941 --> 01:03:31,941
methods is to efficiently draw samples

1561
01:03:31,941 --> 01:03:31,941
from a target distribution row, or more

1562
01:03:31,941 --> 01:03:31,941
commonly, to calculate expectations with

1563
01:03:31,941 --> 01:03:31,941
respect to row.

1564
01:03:20,839 --> 01:03:33,963
And from the end of the paragraph.

1565
01:03:31,942 --> 01:03:40,036
An efficient sampling scheme is one that

1566
01:03:40,036 --> 01:03:40,036
minimizes the variance of the Monte Carlo

1567
01:03:40,036 --> 01:03:40,036
Markov chain.

1568
01:03:34,973 --> 01:03:52,159
Estimator Monte Carlo, that means that

1569
01:03:52,159 --> 01:03:52,159
we're sampling hands from the poker table

1570
01:03:52,159 --> 01:03:52,159
and Markov chain, which means that the

1571
01:03:52,159 --> 01:03:52,159
past only influences the future through

1572
01:03:52,159 --> 01:03:52,159
the present.

1573
01:03:40,037 --> 01:03:55,183
It's a quote memoryless process.

1574
01:03:53,160 --> 01:04:00,175
In other words, fewer samples will be

1575
01:04:00,175 --> 01:04:00,175
needed to obtain a good estimate.

1576
01:03:56,197 --> 01:04:06,233
Intuitively good samplers are Markov

1577
01:04:06,233 --> 01:04:06,233
chains that converge as fast as possible

1578
01:04:06,233 --> 01:04:06,233
to the target distribution.

1579
01:04:00,176 --> 01:04:10,276
It's like if you laid down a jump rope on

1580
01:04:10,276 --> 01:04:10,276
a mountain.

1581
01:04:06,237 --> 01:04:20,375
If that jump rope converged quickly to

1582
01:04:20,375 --> 01:04:20,375
the topography of the mountain, it would

1583
01:04:20,375 --> 01:04:20,375
have been a fast converging jump rope.

1584
01:04:10,277 --> 01:04:24,416
And we're doing something like that, but

1585
01:04:24,416 --> 01:04:24,416
with sampling from the jump rope.

1586
01:04:20,376 --> 01:04:26,434
3.1.

1587
01:04:25,426 --> 01:04:35,529
Optimizing diffusion processes for

1588
01:04:35,529 --> 01:04:35,529
sampling as many MCMC methods are based

1589
01:04:35,529 --> 01:04:35,529
on discretizing continuous time

1590
01:04:35,529 --> 01:04:35,529
stochastic processes.

1591
01:04:26,435 --> 01:04:52,696
The analysis of continuous time processes

1592
01:04:52,696 --> 01:04:52,696
is informative of the properties of

1593
01:04:52,696 --> 01:04:52,696
efficient samplers and diffusion

1594
01:04:52,696 --> 01:04:52,696
processes possess a rich geometric theory

1595
01:04:52,696 --> 01:04:52,696
extending that of vector Fields and have

1596
01:04:52,696 --> 01:04:52,696
been widely studied in the context of

1597
01:04:52,696 --> 01:04:52,696
sampling.

1598
01:04:36,533 --> 01:04:56,731
Lot of very fascinating ideas here.

1599
01:04:53,704 --> 01:05:00,716
Stratinovich stochastic differential

1600
01:05:00,716 --> 01:05:00,716
equations are going to come into play.

1601
01:04:56,732 --> 01:05:11,821
Equation 34 more technical details on

1602
01:05:11,821 --> 01:05:11,821
diffusion.

1603
01:05:06,771 --> 01:05:21,922
The calculus of twisted differentiate

1604
01:05:21,922 --> 01:05:21,922
forms allows us to have a measure

1605
01:05:21,922 --> 01:05:21,922
informed calculus on multivector Fields.

1606
01:05:12,833 --> 01:05:25,965
What are twisted differential forms?

1607
01:05:23,941 --> 01:05:27,988
I'm looking forward to that

1608
01:05:27,988 --> 01:05:27,988
conversation.

1609
01:05:25,967 --> 01:05:30,017
What is the untwisted differential form?

1610
01:05:28,989 --> 01:05:34,049
Have we been twisted all along or not?

1611
01:05:30,019 --> 01:05:41,126
And they go on to write a fundamental

1612
01:05:41,126 --> 01:05:41,126
criterion for efficient sampling is non

1613
01:05:41,126 --> 01:05:41,126
reversibility.

1614
01:05:35,067 --> 01:05:50,218
A process is non reversible if it is

1615
01:05:50,218 --> 01:05:50,218
statistically distinguishable from its

1616
01:05:50,218 --> 01:05:50,218
time reversal when initialized at the

1617
01:05:50,218 --> 01:05:50,218
target distribution.

1618
01:05:42,131 --> 01:05:57,284
So once the jump rope is lying flat on

1619
01:05:57,284 --> 01:05:57,284
the mountain, it's like in a position of

1620
01:05:57,284 --> 01:05:57,284
reversibility.

1621
01:05:51,220 --> 01:06:01,263
Mixing metaphors at will.

1622
01:05:58,295 --> 01:06:17,423
Here measure preserving diffusions are

1623
01:06:17,423 --> 01:06:17,423
non reversible precisely when some

1624
01:06:17,423 --> 01:06:17,423
cognition are met intuitively non

1625
01:06:17,423 --> 01:06:17,423
reversible processes backtrack less often

1626
01:06:17,423 --> 01:06:17,423
and thus furnish more diverse samples.

1627
01:06:01,264 --> 01:06:33,582
So if you're that ball rolling on the

1628
01:06:33,582 --> 01:06:33,582
mountain, you want to just plow forward

1629
01:06:33,582 --> 01:06:33,582
and whether you're going up or downhill,

1630
01:06:33,582 --> 01:06:33,582
you want to be sampling and making moves,

1631
01:06:33,582 --> 01:06:33,582
but you don't want to be going back and

1632
01:06:33,582 --> 01:06:33,582
forth because at that point it's not and

1633
01:06:33,582 --> 01:06:33,582
efficient sampling path.

1634
01:06:17,424 --> 01:06:41,663
It's well known that removing

1635
01:06:41,663 --> 01:06:41,663
nonreversibility worsens the spectral gap

1636
01:06:41,663 --> 01:06:41,663
and the asymptotic variants of the MCMC

1637
01:06:41,663 --> 01:06:41,663
estimator.

1638
01:06:34,591 --> 01:06:53,785
So time discreditization spectral gap in

1639
01:06:53,785 --> 01:06:53,785
dimension with linear coefficients, one

1640
01:06:53,785 --> 01:06:53,785
can construct the optimal non reversible

1641
01:06:53,785 --> 01:06:53,785
matrix a to optimize the spectral gap.

1642
01:06:41,665 --> 01:07:12,913
We can have well behaved parameters that

1643
01:07:12,913 --> 01:07:12,913
help us get at optimal discretization

1644
01:07:12,913 --> 01:07:12,913
schemes so that that shadow Hamiltonian

1645
01:07:12,913 --> 01:07:12,913
can be preserved when we do discretize it

1646
01:07:12,913 --> 01:07:12,913
during optimization, so that we can

1647
01:07:12,913 --> 01:07:12,913
respect the key properties of the

1648
01:07:12,913 --> 01:07:12,913
system.

1649
01:06:54,790 --> 01:07:18,977
However, there are no generic guidelines

1650
01:07:18,977 --> 01:07:18,977
on how to optimize nonreversibility in

1651
01:07:18,977 --> 01:07:18,977
arbitrary diffusions.

1652
01:07:13,921 --> 01:07:38,175
This suggests a two step strategy to

1653
01:07:38,175 --> 01:07:38,175
construct efficient samplers one,

1654
01:07:38,175 --> 01:07:38,175
optimize reversible distinctions

1655
01:07:38,175 --> 01:07:38,175
dimension and two and a non reversible

1656
01:07:38,175 --> 01:07:38,175
perturbation equation 36 citation 125

1657
01:07:38,175 --> 01:07:38,175
diffusions are manifolds.

1658
01:07:18,979 --> 01:07:45,244
Diffusion on manifolds are reversible

1659
01:07:45,244 --> 01:07:45,244
when certain things are the case, and

1660
01:07:45,244 --> 01:07:45,244
they're not when other things are the

1661
01:07:45,244 --> 01:07:45,244
case.

1662
01:07:38,176 --> 01:07:53,319
Equation 37 we got a triangle pointing

1663
01:07:53,319 --> 01:07:53,319
down and a triangle pointing up.

1664
01:07:46,258 --> 01:07:56,350
We're going to talk more about it.

1665
01:07:53,324 --> 01:08:02,354
Under damped Langevin dynamics combine

1666
01:08:02,354 --> 01:08:02,354
all the desirable properties of an

1667
01:08:02,354 --> 01:08:02,354
efficient sampler.

1668
01:07:56,352 --> 01:08:08,414
It is reversible, has degenerate noise,

1669
01:08:08,414 --> 01:08:08,414
and achieves accelerated coherence to the

1670
01:08:08,414 --> 01:08:08,414
target density.

1671
01:08:02,355 --> 01:08:26,592
So all of this groundwork is helping us

1672
01:08:26,592 --> 01:08:26,592
get to an analytical formalization that's

1673
01:08:26,592 --> 01:08:26,592
going to have the right kind of

1674
01:08:26,592 --> 01:08:26,592
properties so we can do that discretion

1675
01:08:26,592 --> 01:08:26,592
right on the shadow Hamiltonian, so we

1676
01:08:26,592 --> 01:08:26,592
can agent the accelerated optimization

1677
01:08:26,592 --> 01:08:26,592
done.

1678
01:08:08,418 --> 01:08:37,702
3.2 hamiltonian Monte Carlo a challenging

1679
01:08:37,702 --> 01:08:37,702
task consists of constructing efficient

1680
01:08:37,702 --> 01:08:37,702
sampling algorithms with strong

1681
01:08:37,702 --> 01:08:37,702
theoretical guarantees.

1682
01:08:27,605 --> 01:08:51,848
We now discuss an important family of

1683
01:08:51,848 --> 01:08:51,848
wellstudied methods known as Hamiltonian

1684
01:08:51,848 --> 01:08:51,848
Monte Carlo HMC, which can be implemented

1685
01:08:51,848 --> 01:08:51,848
on any manifold for any smooth, fully

1686
01:08:51,848 --> 01:08:51,848
supported target measure that is known up

1687
01:08:51,848 --> 01:08:51,848
to a normalizing constant.

1688
01:08:37,706 --> 01:09:01,881
Some of these methods can be seen as an

1689
01:09:01,881 --> 01:09:01,881
appropriate geometric integration of the

1690
01:09:01,881 --> 01:09:01,881
underdamped Langevin diffusion diffusing

1691
01:09:01,881 --> 01:09:01,881
down that hill.

1692
01:08:52,855 --> 01:09:12,992
But in simpler, it is in general simpler

1693
01:09:12,992 --> 01:09:12,992
to view them as combining a geometrically

1694
01:09:12,992 --> 01:09:12,992
integrated deterministic dynamics with a

1695
01:09:12,992 --> 01:09:12,992
simple stochastic process that ensures

1696
01:09:12,992 --> 01:09:12,992
ergodicity.

1697
01:09:01,883 --> 01:09:36,234
Equation 38 if we interpret the negative

1698
01:09:36,234 --> 01:09:36,234
log density v of q as a potential energy

1699
01:09:36,234 --> 01:09:36,234
a function depending on position q, one

1700
01:09:36,234 --> 01:09:36,234
can then plug in the potential within

1701
01:09:36,234 --> 01:09:36,234
Newton's equation to obtain a

1702
01:09:36,234 --> 01:09:36,234
deterministic proposal that is well

1703
01:09:36,234 --> 01:09:36,234
defined on any manifold.

1704
01:09:12,998 --> 01:09:41,288
As soon as the acceleration and

1705
01:09:41,288 --> 01:09:41,288
derivative operators have been replaced

1706
01:09:41,288 --> 01:09:41,288
by their curved analogues.

1707
01:09:36,235 --> 01:09:45,328
So what pulls the ball down the hill?

1708
01:09:42,290 --> 01:09:52,392
Or, if you're an instrumentalist, what

1709
01:09:52,392 --> 01:09:52,392
force describes the movement of the ball

1710
01:09:52,392 --> 01:09:52,392
moving down the hill?

1711
01:09:45,329 --> 01:09:59,459
Gravity, I think there's a John Mayer

1712
01:09:59,459 --> 01:09:59,459
song about it surprises like gravity.

1713
01:09:52,394 --> 01:10:09,508
It can be understood as the minimizing

1714
01:10:09,508 --> 01:10:09,508
force that pulls the ball down the hill

1715
01:10:09,508 --> 01:10:09,508
or describes the movement of the ball on

1716
01:10:09,508 --> 01:10:09,508
its path of least action down the hill.

1717
01:09:59,461 --> 01:10:11,520
How cool.

1718
01:10:10,510 --> 01:10:29,701
More discussion around 38 bringing these

1719
01:10:29,701 --> 01:10:29,701
ingredients together, we thus have the

1720
01:10:29,701 --> 01:10:29,701
following HMC algorithm given dot dot

1721
01:10:29,701 --> 01:10:29,701
dot.

1722
01:10:12,539 --> 01:10:40,812
We're going to compute a function

1723
01:10:40,812 --> 01:10:40,812
according to one, a heat bath, two shadow

1724
01:10:40,812 --> 01:10:40,812
Hamiltonian dynamics and three metropolis

1725
01:10:40,812 --> 01:10:40,812
correction.

1726
01:10:29,703 --> 01:10:42,829
That's the recipe.

1727
01:10:40,814 --> 01:10:44,857
We're bringing those ingredients

1728
01:10:44,857 --> 01:10:44,857
together.

1729
01:10:42,835 --> 01:11:07,023
The above Rudimentary HMC method was

1730
01:11:07,023 --> 01:11:07,023
proposed for simulations in lattice

1731
01:11:07,023 --> 01:11:07,023
quantum chromodynamics, with M being the

1732
01:11:07,023 --> 01:11:07,023
special unitary group Su N, and used a

1733
01:11:07,023 --> 01:11:07,023
Hamiltonian dynamics ingeniously

1734
01:11:07,023 --> 01:11:07,023
constructed from the Mauer carton frame

1735
01:11:07,023 --> 01:11:07,023
to compute the partition function of

1736
01:11:07,023 --> 01:11:07,023
discretized gauge theories.

1737
01:10:45,865 --> 01:11:11,068
This method has later been applied in

1738
01:11:11,068 --> 01:11:11,068
molecular dynamics and statistics.

1739
01:11:07,028 --> 01:11:20,155
There are three critical properties

1740
01:11:20,155 --> 01:11:20,155
underpinning the success of HMC in

1741
01:11:20,155 --> 01:11:20,155
practice.

1742
01:11:14,093 --> 01:11:29,240
The first two are the preservation of the

1743
01:11:29,240 --> 01:11:29,240
reference measure and the existence of a

1744
01:11:29,240 --> 01:11:29,240
conserved shadow Hamiltonian for the

1745
01:11:29,240 --> 01:11:29,240
numerical method.

1746
01:11:21,162 --> 01:11:41,361
The third critical property is the

1747
01:11:41,361 --> 01:11:41,361
existence of splittings methods, for

1748
01:11:41,361 --> 01:11:41,361
which all the composing flows are either

1749
01:11:41,361 --> 01:11:41,361
tractable or have adequate

1750
01:11:41,361 --> 01:11:41,361
approximations, namely the geodesic

1751
01:11:41,361 --> 01:11:41,361
integrators.

1752
01:11:29,244 --> 01:11:53,488
So geodesic methods buckminster, fuller,

1753
01:11:53,488 --> 01:11:53,488
tensegrity, cybernetics, or however you

1754
01:11:53,488 --> 01:11:53,488
think about geodesics as paths and curved

1755
01:11:53,488 --> 01:11:53,488
space.

1756
01:11:41,365 --> 01:12:03,529
That is the kind of approximation and

1757
01:12:03,529 --> 01:12:03,529
splitting we're gaining access to, with

1758
01:12:03,529 --> 01:12:03,529
appropriate distinctions on those balls

1759
01:12:03,529 --> 01:12:03,529
rolling to the bottom of the hill.

1760
01:11:54,498 --> 01:12:10,591
Let us also briefly mention some useful

1761
01:12:10,591 --> 01:12:10,591
upgrades that have been proposed in

1762
01:12:10,591 --> 01:12:10,591
recent years.

1763
01:12:04,538 --> 01:12:16,651
So for those who it's been a few years

1764
01:12:16,651 --> 01:12:16,651
since you checked in with this domain,

1765
01:12:16,651 --> 01:12:16,651
this is a great place to look.

1766
01:12:10,593 --> 01:12:38,874
First, you can grant the method extra

1767
01:12:38,874 --> 01:12:38,874
integration steps when the proposal is

1768
01:12:38,874 --> 01:12:38,874
rejected, or you can use criterion that

1769
01:12:38,874 --> 01:12:38,874
aim to ensure the motion is long enough

1770
01:12:38,874 --> 01:12:38,874
to avoid random walks, but short enough

1771
01:12:38,874 --> 01:12:38,874
that we do not waste computational

1772
01:12:38,874 --> 01:12:38,874
effort, such as the no uturn examples,

1773
01:12:38,874 --> 01:12:38,874
which is integrated in a lot of software

1774
01:12:38,874 --> 01:12:38,874
packages.

1775
01:12:16,653 --> 01:12:53,022
Second, Modern HMC methods bypass this

1776
01:12:53,022 --> 01:12:53,022
issue of slow convergence by replacing

1777
01:12:53,022 --> 01:12:53,022
the heat bath with an ornstein olbeck

1778
01:12:53,022 --> 01:12:53,022
process, which ensures the overall

1779
01:12:53,022 --> 01:12:53,022
algorithm is irreversible.

1780
01:12:39,880 --> 01:13:05,085
An ou process is like a Brownian

1781
01:13:05,085 --> 01:13:05,085
diffusion with a linear regression, so it

1782
01:13:05,085 --> 01:13:05,085
vibrates and has volatility, and it has a

1783
01:13:05,085 --> 01:13:05,085
trend line that's linear.

1784
01:12:53,023 --> 01:13:12,157
Third, many modifications of the

1785
01:13:12,157 --> 01:13:12,157
Rudimentary HMC algorithm only provide

1786
01:13:12,157 --> 01:13:12,157
improvements when the acceptance rate is

1787
01:13:12,157 --> 01:13:12,157
sufficiently high.

1788
01:13:06,092 --> 01:13:21,248
A third class of upgrades improve the

1789
01:13:21,248 --> 01:13:21,248
acceptance rate by using the fact that

1790
01:13:21,248 --> 01:13:21,248
the shadow Hamiltonian is exactly

1791
01:13:21,248 --> 01:13:21,248
preserved by the integrator big point of

1792
01:13:21,248 --> 01:13:21,248
the whole paper.

1793
01:13:12,159 --> 01:13:34,371
Finally, the metropolis step can be

1794
01:13:34,371 --> 01:13:34,371
replaced with a multinomial correction

1795
01:13:34,371 --> 01:13:34,371
that uses the entire numerical

1796
01:13:34,371 --> 01:13:34,371
trajectory, accepting a given point along

1797
01:13:34,371 --> 01:13:34,371
it according to the degree by which it

1798
01:13:34,371 --> 01:13:34,371
distorts the target measure.

1799
01:13:22,252 --> 01:13:45,485
Some path based inference methods with

1800
01:13:45,485 --> 01:13:45,485
some caveats all right, on to section

1801
01:13:45,485 --> 01:13:45,485
four statistical inference with kernel

1802
01:13:45,485 --> 01:13:45,485
based discrepancies.

1803
01:13:34,372 --> 01:14:12,692
So, section four the problem of parameter

1804
01:14:12,692 --> 01:14:12,692
inference consists of estimating an

1805
01:14:12,692 --> 01:14:12,692
element theta star within big theta using

1806
01:14:12,692 --> 01:14:12,692
a sequence of random functions or

1807
01:14:12,692 --> 01:14:12,692
estimators theta hat n of omega onto big

1808
01:14:12,692 --> 01:14:12,692
theta equations conveniently reproducing

1809
01:14:12,692 --> 01:14:12,692
Colonel Hilbert spaces.

1810
01:13:45,487 --> 01:14:28,850
RKHS are precisely Hilbert's spaces over

1811
01:14:28,850 --> 01:14:28,850
which the Diroc distinctions, which is a

1812
01:14:28,850 --> 01:14:28,850
statistical distinctions type where it's

1813
01:14:28,850 --> 01:14:28,850
like a spike at one location and zero

1814
01:14:28,850 --> 01:14:28,850
elsewhere, act continuously.

1815
01:14:12,693 --> 01:14:33,901
So whether we're talking about events or

1816
01:14:33,901 --> 01:14:33,901
spiking neurons, this is a really good

1817
01:14:33,901 --> 01:14:33,901
property.

1818
01:14:28,852 --> 01:14:45,021
And more generally, the probability

1819
01:14:45,021 --> 01:14:45,021
distributions that act continuously by

1820
01:14:45,021 --> 01:14:45,021
integration on an RKHS h are exactly

1821
01:14:45,021 --> 01:14:45,021
those for which all elements of h are

1822
01:14:45,021 --> 01:14:45,021
integral.

1823
01:14:33,904 --> 01:15:08,195
So analytically we have that nice

1824
01:15:08,195 --> 01:15:08,195
property denoting by fancy piece of h,

1825
01:15:08,195 --> 01:15:08,195
the set of such probability measures such

1826
01:15:08,195 --> 01:15:08,195
that dot, dot, dot, we can define the

1827
01:15:08,195 --> 01:15:08,195
maximum mean discrepancy, or Mmd, as such

1828
01:15:08,195 --> 01:15:08,195
more definitions, a practical expression

1829
01:15:08,195 --> 01:15:08,195
for the squared Mmd.

1830
01:14:45,025 --> 01:15:14,258
So just like in linear regression, we're

1831
01:15:14,258 --> 01:15:14,258
often interested in the sum of squared

1832
01:15:14,258 --> 01:15:14,258
errors.

1833
01:15:08,197 --> 01:15:18,297
Here we're interested in the sum of

1834
01:15:18,297 --> 01:15:18,297
squared Mmd.

1835
01:15:15,259 --> 01:15:21,320
4.1.

1836
01:15:20,312 --> 01:15:28,399
Topological methods for Mmds just

1837
01:15:28,399 --> 01:15:28,399
remember Mmds maximum mean discrepancy.

1838
01:15:21,320 --> 01:15:40,512
A key feature of RKHS is that there are

1839
01:15:40,512 --> 01:15:40,512
Hilbertian subspaces, more details about

1840
01:15:40,512 --> 01:15:40,512
embedding in Hilbert spaces and

1841
01:15:40,512 --> 01:15:40,512
subspaces.

1842
01:15:31,420 --> 01:15:51,621
The geometric Analysis the geometric

1843
01:15:51,621 --> 01:15:51,621
description of RKHS and Mmd allows us to

1844
01:15:51,621 --> 01:15:51,621
swiftly apply topological methods in

1845
01:15:51,621 --> 01:15:51,621
their analysis.

1846
01:15:41,527 --> 01:15:57,683
There are reasons this reduces the matter

1847
01:15:57,683 --> 01:15:57,683
to a topological question.

1848
01:15:51,627 --> 01:16:09,743
Instead of defining T star to be the set

1849
01:16:09,743 --> 01:16:09,743
of probability measures it is commonly

1850
01:16:09,743 --> 01:16:09,743
done to define statistical manifolds.

1851
01:15:59,707 --> 01:16:22,879
It's desirable to embed franca p within a

1852
01:16:22,879 --> 01:16:22,879
more structured space, such as the space

1853
01:16:22,879 --> 01:16:22,879
of finite radon measures, which enables

1854
01:16:22,879 --> 01:16:22,879
the method to learn the target function

1855
01:16:22,879 --> 01:16:22,879
independently of the data generating

1856
01:16:22,879 --> 01:16:22,879
distribution.

1857
01:16:09,745 --> 01:16:25,907
What is a radon measure?

1858
01:16:23,887 --> 01:16:27,922
We're going to find out.

1859
01:16:25,908 --> 01:16:44,098
However, it enables the method to learn

1860
01:16:44,098 --> 01:16:44,098
the target function independently of the

1861
01:16:44,098 --> 01:16:44,098
data generative distinctions sounds

1862
01:16:44,098 --> 01:16:44,098
pretty important if we want that tale of

1863
01:16:44,098 --> 01:16:44,098
two densities Mmd can discriminate

1864
01:16:44,098 --> 01:16:44,098
distinctions that makes it useful to do

1865
01:16:44,098 --> 01:16:44,098
other things.

1866
01:16:27,924 --> 01:16:46,119
Let's find out more.

1867
01:16:45,104 --> 01:16:58,236
Section 4.2 smooth Measures and KSDS So

1868
01:16:58,236 --> 01:16:58,236
more information on Mmds and about making

1869
01:16:58,236 --> 01:16:58,236
them computationally.

1870
01:16:48,137 --> 01:17:14,338
Tractable and citations to Stein's Method

1871
01:17:14,338 --> 01:17:14,338
are provided for 195, which is the 1972

1872
01:17:14,338 --> 01:17:14,338
Stein paper, and 196, which is a paper

1873
01:17:14,338 --> 01:17:14,338
written by some of the authors about

1874
01:17:14,338 --> 01:17:14,338
Stein's Method meets statistics.

1875
01:16:58,236 --> 01:17:25,440
A review of some recent developments from

1876
01:17:25,440 --> 01:17:25,440
2021, and I thought it'd be fun to look

1877
01:17:25,440 --> 01:17:25,440
at some figures from this paper and look

1878
01:17:25,440 --> 01:17:25,440
at the abstract, so I'll put the figures

1879
01:17:25,440 --> 01:17:25,440
up above.

1880
01:17:14,339 --> 01:17:37,569
Stein's method compares probability

1881
01:17:37,569 --> 01:17:37,569
distributions through the study of a

1882
01:17:37,569 --> 01:17:37,569
class of linear operators called Stein

1883
01:17:37,569 --> 01:17:37,569
operators.

1884
01:17:31,501 --> 01:17:46,652
While mainly studied in probability and

1885
01:17:46,652 --> 01:17:46,652
used to underpin theoretical statistics,

1886
01:17:46,652 --> 01:17:46,652
stein's method has led to significant

1887
01:17:46,652 --> 01:17:46,652
advances in computational statistics.

1888
01:17:38,570 --> 01:17:52,712
The topics that are discussed in that

1889
01:17:52,712 --> 01:17:52,712
review include tools to benchmark and

1890
01:17:52,712 --> 01:17:52,712
compare.

1891
01:17:46,657 --> 01:18:03,763
Sampling methods such as approximate

1892
01:18:03,763 --> 01:18:03,763
Markov chain, Monte Carlo deterministic,

1893
01:18:03,763 --> 01:18:03,763
alternatives to sampling methods, control

1894
01:18:03,763 --> 01:18:03,763
variant techniques, parameter

1895
01:18:03,763 --> 01:18:03,763
estimations, and goodness of fit

1896
01:18:03,763 --> 01:18:03,763
testing.

1897
01:17:52,713 --> 01:18:22,959
And from the paper they write the list of

1898
01:18:22,959 --> 01:18:22,959
results given in this paper are but a

1899
01:18:22,959 --> 01:18:22,959
mere sample of the ongoing activity in

1900
01:18:22,959 --> 01:18:22,959
this newly established area of research

1901
01:18:22,959 --> 01:18:22,959
at the boundary between probability,

1902
01:18:22,959 --> 01:18:22,959
functional analysis, data science, and

1903
01:18:22,959 --> 01:18:22,959
computational statistics.

1904
01:18:03,765 --> 01:18:34,007
For instance, Stein's method has been

1905
01:18:34,007 --> 01:18:34,007
used for designing sampling based

1906
01:18:34,007 --> 01:18:34,007
algorithms for nonconvex optimization

1907
01:18:34,007 --> 01:18:34,007
learning semiparametric multi index

1908
01:18:34,007 --> 01:18:34,007
models and high dimensions and Bayesian

1909
01:18:34,007 --> 01:18:34,007
statistics physics.

1910
01:18:23,960 --> 01:18:54,027
Stein discrepancies have been used as

1911
01:18:54,027 --> 01:18:54,027
variational objectives for posterior

1912
01:18:54,027 --> 01:18:54,027
optimization, which is what we're all

1913
01:18:54,027 --> 01:18:54,027
about free energy functionals, using

1914
01:18:54,027 --> 01:18:54,027
variational inference as objectives for

1915
01:18:54,027 --> 01:18:54,027
posterior approximation of inference and

1916
01:18:54,027 --> 01:18:54,027
action free energy principle.

1917
01:18:34,007 --> 01:18:57,030
And these images have some nice

1918
01:18:57,030 --> 01:18:57,030
intuitions too.

1919
01:18:54,027 --> 01:19:03,030
Here we see, depending on our step size,

1920
01:19:03,030 --> 01:19:03,030
we're getting different sampling.

1921
01:18:57,030 --> 01:19:12,039
When our step size is to the negative

1922
01:19:12,039 --> 01:19:12,039
fifth power, we're over sampling one part

1923
01:19:12,039 --> 01:19:12,039
of the space.

1924
01:19:04,031 --> 01:19:19,046
Then, as we make the step size larger,

1925
01:19:19,046 --> 01:19:19,046
our samples are drawn from different

1926
01:19:19,046 --> 01:19:19,046
distributions.

1927
01:19:13,040 --> 01:19:29,056
So that's selecting the step size epsilon

1928
01:19:29,056 --> 01:19:29,056
for a stochastic gradient Langvin dynamic

1929
01:19:29,056 --> 01:19:29,056
and also from the anastasio paper.

1930
01:19:19,046 --> 01:19:31,058
Here's another fun way to see that.

1931
01:19:29,056 --> 01:19:49,076
Here we have that bowl that we're rolling

1932
01:19:49,076 --> 01:19:49,076
the ball to the bottom of, and the Stein

1933
01:19:49,076 --> 01:19:49,076
points and the Stein thinning, which is

1934
01:19:49,076 --> 01:19:49,076
helping us understand maybe how starting

1935
01:19:49,076 --> 01:19:49,076
the ball in different locations or

1936
01:19:49,076 --> 01:19:49,076
critical locations accelerates what we're

1937
01:19:49,076 --> 01:19:49,076
doing with optimization.

1938
01:19:32,059 --> 01:19:52,078
So cool to talk about.

1939
01:19:49,076 --> 01:20:07,088
4.2.1 the canonical Stein operator and

1940
01:20:07,088 --> 01:20:07,088
Poincare duality there are two

1941
01:20:07,088 --> 01:20:07,088
fundamental theorems that help us

1942
01:20:07,088 --> 01:20:07,088
understand the integral differentiate

1943
01:20:07,088 --> 01:20:07,088
geometry of the manifold durham's theorem

1944
01:20:07,088 --> 01:20:07,088
and the Poncare duality.

1945
01:19:52,079 --> 01:20:16,097
The former Durham's theorem relates the

1946
01:20:16,097 --> 01:20:16,097
topology of the manifold to information

1947
01:20:16,097 --> 01:20:16,097
on the solutions of differential

1948
01:20:16,097 --> 01:20:16,097
equations defined over the manifold.

1949
01:20:07,088 --> 01:20:30,111
The latter, which contains the

1950
01:20:30,111 --> 01:20:30,111
fundamental theorem of calculus,

1951
01:20:30,111 --> 01:20:30,111
describes the properties of the integral

1952
01:20:30,111 --> 01:20:30,111
pairing alpha beta of differential forms,

1953
01:20:30,111 --> 01:20:30,111
which include the pairing of test

1954
01:20:30,111 --> 01:20:30,111
functions with smooth measures.

1955
01:20:16,097 --> 01:20:33,114
Let's learn more about it.

1956
01:20:31,112 --> 01:20:45,126
Four, two, two kernel stein discrepancies

1957
01:20:45,126 --> 01:20:45,126
and score matchings technical definitions

1958
01:20:45,126 --> 01:20:45,126
facilitating Stein variational gradient

1959
01:20:45,126 --> 01:20:45,126
descent.

1960
01:20:33,114 --> 01:20:54,135
4.3 information geometry of Mmds and

1961
01:20:54,135 --> 01:20:54,135
natural gradient descent.

1962
01:20:47,128 --> 01:20:58,139
These tools have proved to be useful in a

1963
01:20:58,139 --> 01:20:58,139
wide range of contexts.

1964
01:20:55,135 --> 01:21:10,145
More information about the divergence and

1965
01:21:10,145 --> 01:21:10,145
how divergence can improve the speed of

1966
01:21:10,145 --> 01:21:10,145
coherence by following the natural

1967
01:21:10,145 --> 01:21:10,145
gradient descent.

1968
01:21:00,135 --> 01:21:16,151
Okay, red text speculation beware.

1969
01:21:12,147 --> 01:21:25,160
So the operator, which is a phone routing

1970
01:21:25,160 --> 01:21:25,160
system like ring ring, hello, operator is

1971
01:21:25,160 --> 01:21:25,160
constructed in action.

1972
01:21:16,151 --> 01:21:34,169
4.2.22 colonel Stein discrepancies and

1973
01:21:34,169 --> 01:21:34,169
score matching you can't have a Ttest

1974
01:21:34,169 --> 01:21:34,169
without the T distribution.

1975
01:21:25,160 --> 01:21:40,175
The Ttest statistic is using a T

1976
01:21:40,175 --> 01:21:40,175
distribution.

1977
01:21:34,169 --> 01:22:03,192
So a class of fancy V, a set fancy V of

1978
01:22:03,192 --> 01:22:03,192
vector Fields, or more generally,

1979
01:22:03,192 --> 01:22:03,192
tensorfields whose image f under the

1980
01:22:03,192 --> 01:22:03,192
operator ring ring has mean zero under Mu

1981
01:22:03,192 --> 01:22:03,192
that's going to give us this discrepancy,

1982
01:22:03,192 --> 01:22:03,192
the SD and the Stein variational gradient

1983
01:22:03,192 --> 01:22:03,192
descent.

1984
01:21:41,176 --> 01:22:13,202
So there's some class of V vector Fields

1985
01:22:13,202 --> 01:22:13,202
such that the mean of something about

1986
01:22:13,202 --> 01:22:13,202
them is zero.

1987
01:22:03,192 --> 01:22:26,215
Does this enable the central limit

1988
01:22:26,215 --> 01:22:26,215
theorem or just statistics more broadly,

1989
01:22:26,215 --> 01:22:26,215
like the parametric and non parametric

1990
01:22:26,215 --> 01:22:26,215
methods that we know and love from SPM?

1991
01:22:14,203 --> 01:22:37,226
Does that mean zero enable us to use

1992
01:22:37,226 --> 01:22:37,226
Gaussian methods, generalized Gaussian

1993
01:22:37,226 --> 01:22:37,226
methods, generalized linear methods,

1994
01:22:37,226 --> 01:22:37,226
SPM?

1995
01:22:27,216 --> 01:22:53,241
And does it enable the proposed smooth

1996
01:22:53,241 --> 01:22:53,241
distribution, the one generating the

1997
01:22:53,241 --> 01:22:53,241
shadow Hamiltonian that's getting

1998
01:22:53,241 --> 01:22:53,241
inferred over, to be interpreted or used

1999
01:22:53,241 --> 01:22:53,241
truly as a statistical distribution?

2000
01:22:38,227 --> 01:22:59,248
It's easy to forget that not all

2001
01:22:59,248 --> 01:22:59,248
distributions are formal probability

2002
01:22:59,248 --> 01:22:59,248
distributions.

2003
01:22:53,242 --> 01:23:06,249
For example, the area under the curve

2004
01:23:06,249 --> 01:23:06,249
from zero to one of a probability

2005
01:23:06,249 --> 01:23:06,249
distribution is one.

2006
01:22:59,248 --> 01:23:08,251
Something has to happen.

2007
01:23:06,249 --> 01:23:14,257
But not all functions have an area under

2008
01:23:14,257 --> 01:23:14,257
the curve of one between zero and one.

2009
01:23:08,251 --> 01:23:25,268
So in variational Bayesian inference, the

2010
01:23:25,268 --> 01:23:25,268
kind which we do in active inference, in

2011
01:23:25,268 --> 01:23:25,268
fact, the kind that's done in machine

2012
01:23:25,268 --> 01:23:25,268
learning and statistics more broadly.

2013
01:23:15,258 --> 01:23:29,272
But we're interested when action is a

2014
01:23:29,272 --> 01:23:29,272
parameter that we're doing inference on.

2015
01:23:25,268 --> 01:23:37,280
We're concerned with the extremely well

2016
01:23:37,280 --> 01:23:37,280
behavior properties of a certain subset

2017
01:23:37,280 --> 01:23:37,280
of distributions.

2018
01:23:29,272 --> 01:23:40,283
So why are we doing this?

2019
01:23:37,280 --> 01:23:52,294
Why is it important that we didn't need

2020
01:23:52,294 --> 01:23:52,294
the input data that we could construct

2021
01:23:52,294 --> 01:23:52,294
measures that are independent of the

2022
01:23:52,294 --> 01:23:52,294
input data?

2023
01:23:40,283 --> 01:23:55,298
Because we want to enable the tail of two

2024
01:23:55,298 --> 01:23:55,298
density.

2025
01:23:52,295 --> 01:24:06,303
We want to make generative models that

2026
01:24:06,303 --> 01:24:06,303
can be used generatively, generative AI

2027
01:24:06,303 --> 01:24:06,303
in the forward or the generative

2028
01:24:06,303 --> 01:24:06,303
direction.

2029
01:23:56,298 --> 01:24:16,313
But we also want to be able to enable the

2030
01:24:16,313 --> 01:24:16,313
recognition distribution, which is from

2031
01:24:16,313 --> 01:24:16,313
empirical data, to do hidden state

2032
01:24:16,313 --> 01:24:16,313
inference.

2033
01:24:06,303 --> 01:24:34,331
And so just like least squares regression

2034
01:24:34,331 --> 01:24:34,331
in the linear modeling case, where the

2035
01:24:34,331 --> 01:24:34,331
sum of squares above and below the

2036
01:24:34,331 --> 01:24:34,331
regression line should be as low as

2037
01:24:34,331 --> 01:24:34,331
possible l two norm sum of squares

2038
01:24:34,331 --> 01:24:34,331
minimization always works, never

2039
01:24:34,331 --> 01:24:34,331
complains.

2040
01:24:16,313 --> 01:24:58,355
We want that kind of ball rolling to the

2041
01:24:58,355 --> 01:24:58,355
bottom of the hill with free energy

2042
01:24:58,355 --> 01:24:58,355
functionals variational and expected free

2043
01:24:58,355 --> 01:24:58,355
energy functionals as the optimized or

2044
01:24:58,355 --> 01:24:58,355
satisfied imperative for optimal

2045
01:24:58,355 --> 01:24:58,355
perception, which is signal processing,

2046
01:24:58,355 --> 01:24:58,355
signals intelligence and control which is

2047
01:24:58,355 --> 01:24:58,355
control theory or action selection.

2048
01:24:35,332 --> 01:25:06,357
More technical details and equations 39 B

2049
01:25:06,357 --> 01:25:06,357
and 39 C.

2050
01:25:01,352 --> 01:25:16,367
So what does it mean that the resulting

2051
01:25:16,367 --> 01:25:16,367
Stein discrepancy can be thought of as an

2052
01:25:16,367 --> 01:25:16,367
Mmd that depends only on row and is known

2053
01:25:16,367 --> 01:25:16,367
as a kernel Stein discrepancy.

2054
01:25:07,358 --> 01:25:17,368
What is that?

2055
01:25:16,367 --> 01:25:28,379
As we did previously, we can remove the

2056
01:25:28,379 --> 01:25:28,379
super mum by rewriting the above as a

2057
01:25:28,379 --> 01:25:28,379
super mum over some unit ball of

2058
01:25:28,379 --> 01:25:28,379
continuous linear functional.

2059
01:25:19,370 --> 01:25:36,387
Is this like simulating or modeling a

2060
01:25:36,387 --> 01:25:36,387
sphere rolling on a landscape like we've

2061
01:25:36,387 --> 01:25:36,387
been talking about?

2062
01:25:29,380 --> 01:25:41,391
Is the ball of optimal radius for rolling

2063
01:25:41,391 --> 01:25:41,391
on that landscape?

2064
01:25:37,388 --> 01:25:42,393
Or what is being optimized?

2065
01:25:41,392 --> 01:25:51,402
And what is the unit, the scaling or the

2066
01:25:51,402 --> 01:25:51,402
scale specificity that this scale

2067
01:25:51,402 --> 01:25:51,402
friendly sphere is scaled to?

2068
01:25:42,393 --> 01:25:59,410
Equation 42 and 42 a while in the

2069
01:25:59,410 --> 01:25:59,410
Euclidean space yields the diffusion

2070
01:25:59,410 --> 01:25:59,410
score matching.

2071
01:25:53,404 --> 01:26:21,426
Citation 204 Barp et al again minimum

2072
01:26:21,426 --> 01:26:21,426
Stein discrepancy estimators from 2019

2073
01:26:21,426 --> 01:26:21,426
they write the main strength of our

2074
01:26:21,426 --> 01:26:21,426
methodology is its flexibility, which

2075
01:26:21,426 --> 01:26:21,426
allows us to design estimators with

2076
01:26:21,426 --> 01:26:21,426
desirable properties for specific models

2077
01:26:21,426 --> 01:26:21,426
at hand by carefully selecting a Stein

2078
01:26:21,426 --> 01:26:21,426
discrepancy.

2079
01:25:59,410 --> 01:26:29,434
We illustrate this advantage for several

2080
01:26:29,434 --> 01:26:29,434
challenging problems for score matching,

2081
01:26:29,434 --> 01:26:29,434
such as non smooth, heavy tailed or light

2082
01:26:29,434 --> 01:26:29,434
tailed densities.

2083
01:26:21,426 --> 01:26:38,443
So again, just with a little bit of

2084
01:26:38,443 --> 01:26:38,443
speculation here and I think my camera

2085
01:26:38,443 --> 01:26:38,443
has frozen.

2086
01:26:29,434 --> 01:26:47,452
It's all good.

2087
01:26:46,451 --> 01:26:52,457
I'll just go without the camera.

2088
01:26:49,454 --> 01:26:55,460
Okay.

2089
01:26:54,459 --> 01:27:04,463
One can estimate infer or optimize the

2090
01:27:04,463 --> 01:27:04,463
zero point and hence the variance

2091
01:27:04,463 --> 01:27:04,463
structure of the distribution.

2092
01:26:56,461 --> 01:27:22,481
This is going to ensemble generalized

2093
01:27:22,481 --> 01:27:22,481
wellbehaved modeling again, from all of

2094
01:27:22,481 --> 01:27:22,481
those nice perspectives that we raised

2095
01:27:22,481 --> 01:27:22,481
earlier, like Gaussian central Limit

2096
01:27:22,481 --> 01:27:22,481
theorem, smoothness, statistical

2097
01:27:22,481 --> 01:27:22,481
disturbance, Euclidean, all of those well

2098
01:27:22,481 --> 01:27:22,481
behaved properties that we're looking

2099
01:27:22,481 --> 01:27:22,481
for.

2100
01:27:04,463 --> 01:27:24,483
This is going to help us get there.

2101
01:27:22,481 --> 01:27:32,491
We are only talking about a specific

2102
01:27:32,491 --> 01:27:32,491
subset or type of landscape here, the one

2103
01:27:32,491 --> 01:27:32,491
that we're doing variational inference

2104
01:27:32,491 --> 01:27:32,491
on, that's the map.

2105
01:27:24,483 --> 01:27:36,495
This is not the structure of the

2106
01:27:36,495 --> 01:27:36,495
territory.

2107
01:27:33,492 --> 01:27:44,503
These well behaved attributes of models

2108
01:27:44,503 --> 01:27:44,503
for better, worse and different through

2109
01:27:44,503 --> 01:27:44,503
sickness and health is because they're

2110
01:27:44,503 --> 01:27:44,503
maps.

2111
01:27:36,495 --> 01:27:54,513
It is always the case that our

2112
01:27:54,513 --> 01:27:54,513
statistically nice generative models are

2113
01:27:54,513 --> 01:27:54,513
of a different structure or form than the

2114
01:27:54,513 --> 01:27:54,513
generative process.

2115
01:27:45,504 --> 01:27:59,518
So this is actually not a criticism of

2116
01:27:59,518 --> 01:27:59,518
quantitative modeling as a process.

2117
01:27:54,513 --> 01:28:03,516
In fact, this is the entire basis of

2118
01:28:03,516 --> 01:28:03,516
quantitative modeling.

2119
01:27:59,518 --> 01:28:06,519
So we've approached this map territory

2120
01:28:06,519 --> 01:28:06,519
distinction.

2121
01:28:03,516 --> 01:28:18,531
Map territory fallacy, fallacy, fallacy

2122
01:28:18,531 --> 01:28:18,531
from many angles and one will still hear

2123
01:28:18,531 --> 01:28:18,531
things like well, the structure of the

2124
01:28:18,531 --> 01:28:18,531
statistical model is not the same as the

2125
01:28:18,531 --> 01:28:18,531
territory.

2126
01:28:06,519 --> 01:28:25,538
Or how can you say that the organism has

2127
01:28:25,538 --> 01:28:25,538
these well behaved properties just

2128
01:28:25,538 --> 01:28:25,538
because the model does?

2129
01:28:18,531 --> 01:28:41,554
And this brings a really sharp light on

2130
01:28:41,554 --> 01:28:41,554
it, which is we're doing this insane

2131
01:28:41,554 --> 01:28:41,554
amount of analytical groundwork so that

2132
01:28:41,554 --> 01:28:41,554
the map has the good properties not to

2133
01:28:41,554 --> 01:28:41,554
constrain what the territory is.

2134
01:28:25,538 --> 01:28:47,560
So map territory, all of the math we've

2135
01:28:47,560 --> 01:28:47,560
been talking about is map.

2136
01:28:41,554 --> 01:28:55,568
We want wellbehaved maps so that we can

2137
01:28:55,568 --> 01:28:55,568
describe wellbehaved and unruly

2138
01:28:55,568 --> 01:28:55,568
territories.

2139
01:28:47,560 --> 01:29:01,568
But it isn't the case that the organism

2140
01:29:01,568 --> 01:29:01,568
minimizes variational free energy.

2141
01:28:56,569 --> 01:29:05,572
It's the generative model that minimizes

2142
01:29:05,572 --> 01:29:05,572
variational free energy.

2143
01:29:01,568 --> 01:29:10,577
So, little bit of a leading discussion

2144
01:29:10,577 --> 01:29:10,577
topic or question for you all.

2145
01:29:05,572 --> 01:29:14,581
Feel free to give a thought in the live

2146
01:29:14,581 --> 01:29:14,581
chat or write a comment or join our

2147
01:29:14,581 --> 01:29:14,581
discussions.

2148
01:29:10,577 --> 01:29:21,588
How is this line of basic and fundamental

2149
01:29:21,588 --> 01:29:21,588
math research by Barp et al.

2150
01:29:15,582 --> 01:29:31,598
Creating new models that have the

2151
01:29:31,598 --> 01:29:31,598
operational or denotational semantics

2152
01:29:31,598 --> 01:29:31,598
that we want for computational

2153
01:29:31,598 --> 01:29:31,598
statistics, which is to say applied

2154
01:29:31,598 --> 01:29:31,598
information theory.

2155
01:29:21,588 --> 01:29:47,614
For example, distributions that can be

2156
01:29:47,614 --> 01:29:47,614
interpreted or used as statistical

2157
01:29:47,614 --> 01:29:47,614
distributions, ideally directly

2158
01:29:47,614 --> 01:29:47,614
compatible with current computational

2159
01:29:47,614 --> 01:29:47,614
methods SPM in MATLAB, Pi, Mdp in Python

2160
01:29:47,614 --> 01:29:47,614
and fourney Lab in Julia and so on.

2161
01:29:32,599 --> 01:29:58,625
Section 4.3.1 minimum Stein discrepancy

2162
01:29:58,625 --> 01:29:58,625
estimators, more technical details, more

2163
01:29:58,625 --> 01:29:58,625
inclination to 204.

2164
01:29:48,615 --> 01:30:07,628
The parameters can be adjusted to active

2165
01:30:07,628 --> 01:30:07,628
characteristicness consistency, bias,

2166
01:30:07,628 --> 01:30:07,628
robustness and obtain central Limit

2167
01:30:07,628 --> 01:30:07,628
Theorems C 204.

2168
01:29:59,626 --> 01:30:09,630
2019 paper with Barp et al.

2169
01:30:07,628 --> 01:30:14,635
And let's just look at a really cool

2170
01:30:14,635 --> 01:30:14,635
image from that paper.

2171
01:30:09,630 --> 01:30:18,639
Figure one and figure two pretty cool.

2172
01:30:14,635 --> 01:30:21,642
SD estimators looking nice.

2173
01:30:18,639 --> 01:30:47,668
Section 4.3.2 likelihood Free Inference

2174
01:30:47,668 --> 01:30:47,668
with Generative Models so for many

2175
01:30:47,668 --> 01:30:47,668
applications of interests, the densities

2176
01:30:47,668 --> 01:30:47,668
of the model mu sub data cannot be

2177
01:30:47,668 --> 01:30:47,668
evaluated or differentiated.

2178
01:30:23,644 --> 01:30:57,678
We thus need density free inference

2179
01:30:57,678 --> 01:30:57,678
methods super convenient for Bayesian

2180
01:30:57,678 --> 01:30:57,678
statistics, more technical details,

2181
01:30:57,678 --> 01:30:57,678
information tensor.

2182
01:30:48,668 --> 01:31:08,683
Under appropriate choices of kernels and

2183
01:31:08,683 --> 01:31:08,683
models, one can derive theoretical

2184
01:31:08,683 --> 01:31:08,683
guarantees such as concentration and

2185
01:31:08,683 --> 01:31:08,683
generalization boundary consistency

2186
01:31:08,683 --> 01:31:08,683
asymptotic normality and robustness.

2187
01:30:57,678 --> 01:31:20,695
So, little bit of a summary we have good

2188
01:31:20,695 --> 01:31:20,695
analytical and computational footing in

2189
01:31:20,695 --> 01:31:20,695
certain kinds of situations or under

2190
01:31:20,695 --> 01:31:20,695
certain constraints.

2191
01:31:09,684 --> 01:31:37,712
Certainly not all constraints, but

2192
01:31:37,712 --> 01:31:37,712
definitely for some that might enable the

2193
01:31:37,712 --> 01:31:37,712
efficient computation, not just

2194
01:31:37,712 --> 01:31:37,712
specification, but actual implementation

2195
01:31:37,712 --> 01:31:37,712
of large generative models such as

2196
01:31:37,712 --> 01:31:37,712
described in the paper of Friston et al.

2197
01:31:20,695 --> 01:31:42,717
2022 designing Ecosystems of Intelligence

2198
01:31:42,717 --> 01:31:42,717
from shared sorry.

2199
01:31:37,712 --> 01:31:50,725
Designing Ecosystems of Intelligence free

2200
01:31:50,725 --> 01:31:50,725
Energy Principle And they use the term

2201
01:31:50,725 --> 01:31:50,725
ecosystems of shared intelligence in that

2202
01:31:50,725 --> 01:31:50,725
paper.

2203
01:31:42,717 --> 01:31:55,730
And so it's relevant to learn about the

2204
01:31:55,730 --> 01:31:55,730
actuality and build intuition.

2205
01:31:50,725 --> 01:31:56,731
Why?

2206
01:31:55,730 --> 01:31:57,732
How do we know?

2207
01:31:56,731 --> 01:32:13,741
Well, here's the one figure in that paper

2208
01:32:13,741 --> 01:32:13,741
of Friston at all, the one figure in this

2209
01:32:13,741 --> 01:32:13,741
absolutely positional paper that they

2210
01:32:13,741 --> 01:32:13,741
have released believes as parameters of a

2211
01:32:13,741 --> 01:32:13,741
probability distribution.

2212
01:31:57,732 --> 01:32:30,759
Here's the sharpening of a belief as a

2213
01:32:30,759 --> 01:32:30,759
distance belief updating as traversing a

2214
01:32:30,759 --> 01:32:30,759
statistical manifold, which is to say a

2215
01:32:30,759 --> 01:32:30,759
lower dimension projected space and

2216
01:32:30,759 --> 01:32:30,759
what's being shown as the parameter space

2217
01:32:30,759 --> 01:32:30,759
of a probability distribution.

2218
01:32:13,742 --> 01:32:47,776
So whether we see this as gradient ascent

2219
01:32:47,776 --> 01:32:47,776
to climb to the top of the hill or we

2220
01:32:47,776 --> 01:32:47,776
take the negative and we have gradient

2221
01:32:47,776 --> 01:32:47,776
descent to the bottom of the hill, this

2222
01:32:47,776 --> 01:32:47,776
is the figure chosen by some very well

2223
01:32:47,776 --> 01:32:47,776
informed authors to describe Bayesian

2224
01:32:47,776 --> 01:32:47,776
mechanics.

2225
01:32:30,759 --> 01:32:58,787
And long last we get to section five

2226
01:32:58,787 --> 01:32:58,787
adaptive agents through active

2227
01:32:58,787 --> 01:32:58,787
coherence.

2228
01:32:49,778 --> 01:33:13,796
We close, as the authors write, with a

2229
01:33:13,796 --> 01:33:13,796
generic use case called Active Coherence,

2230
01:33:13,796 --> 01:33:13,796
a general framework for describing and

2231
01:33:13,796 --> 01:33:13,796
designing adaptive agents that unifies

2232
01:33:13,796 --> 01:33:13,796
all aspects of behavior, including

2233
01:33:13,796 --> 01:33:13,796
perception, planning and learning as

2234
01:33:13,796 --> 01:33:13,796
processes of inference.

2235
01:32:59,788 --> 01:33:32,815
By exploiting this geometric structure in

2236
01:33:32,815 --> 01:33:32,815
a generic framework for designing

2237
01:33:32,815 --> 01:33:32,815
adaptive agents, we derive the objective

2238
01:33:32,815 --> 01:33:32,815
functional overarching decision making

2239
01:33:32,815 --> 01:33:32,815
and describe its information geometric

2240
01:33:32,815 --> 01:33:32,815
structure, revealing several special

2241
01:33:32,815 --> 01:33:32,815
cases that are established notions in

2242
01:33:32,815 --> 01:33:32,815
statistics, cognitive science and

2243
01:33:32,815 --> 01:33:32,815
engineering.

2244
01:33:14,797 --> 01:33:34,817
So section five one modeling.

2245
01:33:32,815 --> 01:33:35,818
Adaptive decision making.

2246
01:33:34,817 --> 01:33:39,822
First, they're going to talk about

2247
01:33:39,822 --> 01:33:39,822
behavior, agents and environments.

2248
01:33:35,818 --> 01:33:44,827
Behavior is going to be defined as the

2249
01:33:44,827 --> 01:33:44,827
interaction between an agent and its

2250
01:33:44,827 --> 01:33:44,827
environment.

2251
01:33:40,823 --> 01:33:49,832
And the system is going to describe the

2252
01:33:49,832 --> 01:33:49,832
agent plus its environment.

2253
01:33:45,828 --> 01:33:59,841
There's going to be a set of states that

2254
01:33:59,841 --> 01:33:59,841
are partitions from a statespace big x

2255
01:33:59,841 --> 01:33:59,841
external states.

2256
01:33:51,833 --> 01:34:06,843
S are going to be unknown to the agent

2257
01:34:06,843 --> 01:34:06,843
and constitute the environment states

2258
01:34:06,843 --> 01:34:06,843
belonging to the agent.

2259
01:33:59,842 --> 01:34:13,850
Pi particular states are going to be a

2260
01:34:13,850 --> 01:34:13,850
subset of two different things.

2261
01:34:06,843 --> 01:34:17,854
So here is S, which is here external

2262
01:34:17,854 --> 01:34:17,854
states.

2263
01:34:13,850 --> 01:34:21,858
Note that in some other settings S will

2264
01:34:21,858 --> 01:34:21,858
refer to sensory states.

2265
01:34:17,854 --> 01:34:31,868
So look at the notation in this paper and

2266
01:34:31,868 --> 01:34:31,868
Pi, which in other papers is sometimes

2267
01:34:31,868 --> 01:34:31,868
used to describe policy inference here is

2268
01:34:31,868 --> 01:34:31,868
going to describe particular states.

2269
01:34:21,858 --> 01:34:36,873
So Pi is O and A.

2270
01:34:32,869 --> 01:34:50,887
O are the observable states, states that

2271
01:34:50,887 --> 01:34:50,887
the agent can see but cannot directly

2272
01:34:50,887 --> 01:34:50,887
control sense states and A are the

2273
01:34:50,887 --> 01:34:50,887
autonomous states that the agent sees and

2274
01:34:50,887 --> 01:34:50,887
can directly control.

2275
01:34:36,873 --> 01:34:54,891
And those are going to be internal states

2276
01:34:54,891 --> 01:34:54,891
and active states.

2277
01:34:50,887 --> 01:35:04,895
Figure two has a great representation and

2278
01:35:04,895 --> 01:35:04,895
again note the way that Pi, So and A are

2279
01:35:04,895 --> 01:35:04,895
being used in this paper.

2280
01:34:55,892 --> 01:35:11,902
S is the external process hidden state,

2281
01:35:11,902 --> 01:35:11,902
latent state, causal coherence.

2282
01:35:05,896 --> 01:35:24,915
O are the observations and the agent

2283
01:35:24,915 --> 01:35:24,915
process concepts of Pi, which is the

2284
01:35:24,915 --> 01:35:24,915
observations and alpha or A.

2285
01:35:12,903 --> 01:35:37,928
A is consisting of, according to the

2286
01:35:37,928 --> 01:35:37,928
particular partition, two different kinds

2287
01:35:37,928 --> 01:35:37,928
of states, which is internal states,

2288
01:35:37,928 --> 01:35:37,928
cognitive states and action states.

2289
01:35:25,916 --> 01:35:46,937
And so we're going to compute bounds for

2290
01:35:46,937 --> 01:35:46,937
free energy functionals with a special

2291
01:35:46,937 --> 01:35:46,937
focus on autonomous processes.

2292
01:35:37,928 --> 01:35:57,948
Because controlling our perception is not

2293
01:35:57,948 --> 01:35:57,948
possible, and maybe not even preferable

2294
01:35:57,948 --> 01:35:57,948
by controlling it at the level of what we

2295
01:35:57,948 --> 01:35:57,948
observe.

2296
01:35:46,937 --> 01:36:09,954
But rather, if we control our internal

2297
01:36:09,954 --> 01:36:09,954
states, which is our interpretation of

2298
01:36:09,954 --> 01:36:09,954
the perception and our action states,

2299
01:36:09,954 --> 01:36:09,954
we'll be doing optimal perception and

2300
01:36:09,954 --> 01:36:09,954
optimal action.

2301
01:35:57,948 --> 01:36:17,962
So those are the two sides of the coin

2302
01:36:17,962 --> 01:36:17,962
with inference as perception learning and

2303
01:36:17,962 --> 01:36:17,962
action selection.

2304
01:36:09,954 --> 01:36:19,964
That's how we unify action and

2305
01:36:19,964 --> 01:36:19,964
inference.

2306
01:36:17,962 --> 01:36:29,974
Inactive inference is by caring about the

2307
01:36:29,974 --> 01:36:29,974
bounding of selfsurprisal, just like

2308
01:36:29,974 --> 01:36:29,974
gravity on our autonomous processes.

2309
01:36:19,964 --> 01:36:32,977
So this will be super fun to discuss

2310
01:36:32,977 --> 01:36:32,977
more.

2311
01:36:29,974 --> 01:36:44,989
5.1.2 decision making and precise agents

2312
01:36:44,989 --> 01:36:44,989
so, what distinctions people from small

2313
01:36:44,989 --> 01:36:44,989
particles people are subject to classical

2314
01:36:44,989 --> 01:36:44,989
as opposed to statistical mechanics.

2315
01:36:34,979 --> 01:36:53,998
Check out Active Livestream 49 for more

2316
01:36:53,998 --> 01:36:53,998
on Bayesian mechanics and some of these

2317
01:36:53,998 --> 01:36:53,998
distinctions with classical statistical

2318
01:36:53,998 --> 01:36:53,998
quantum and thermo.

2319
01:36:45,990 --> 01:36:58,002
In other words, they are precise agents

2320
01:36:58,002 --> 01:36:58,002
with conservative dynamics.

2321
01:36:54,999 --> 01:37:05,004
Precise agent definition 5.1 an agent is

2322
01:37:05,004 --> 01:37:05,004
precise when it evolves deterministically

2323
01:37:05,004 --> 01:37:05,004
in a possibly stochastic environment.

2324
01:36:58,003 --> 01:37:13,012
More definitions and it's useful here to

2325
01:37:13,012 --> 01:37:13,012
highlight Friston et al's.

2326
01:37:07,006 --> 01:38:09,062
Recent paper path integrals particular

2327
01:38:09,062 --> 01:38:09,062
kinds and Lagrange things which provides

2328
01:38:09,062 --> 01:38:09,062
a taxonomy of things taxonomy of

2329
01:38:09,062 --> 01:38:09,062
particular entities that run the gamut of

2330
01:38:09,062 --> 01:38:09,062
sophistication from inert particles which

2331
01:38:09,062 --> 01:38:09,062
have no active states active particles

2332
01:38:09,062 --> 01:38:09,062
which have active states generative

2333
01:38:09,062 --> 01:38:09,062
process which have cognitive dynamics

2334
01:38:09,062 --> 01:38:09,062
that are described as classical and

2335
01:38:09,062 --> 01:38:09,062
strange particles which represent in this

2336
01:38:09,062 --> 01:38:09,062
visualization the highest level of

2337
01:38:09,062 --> 01:38:09,062
cognitive sophistication in which those

2338
01:38:09,062 --> 01:38:09,062
internal hidden states themselves have

2339
01:38:09,062 --> 01:38:09,062
this kind of what would happen if this

2340
01:38:09,062 --> 01:38:09,062
happens counterfactual type or thinking

2341
01:38:09,062 --> 01:38:09,062
through other minds type cognitive model

2342
01:38:09,062 --> 01:38:09,062
structure doing forward and inverse

2343
01:38:09,062 --> 01:38:09,062
inference on these great times.

2344
01:37:14,013 --> 01:38:12,065
5.1.2.

2345
01:38:11,064 --> 01:38:22,075
Decision making and precise agents we

2346
01:38:22,075 --> 01:38:22,075
have mathematical formalisms for

2347
01:38:22,075 --> 01:38:22,075
decisions, preferences and predictions

2348
01:38:22,075 --> 01:38:22,075
all using the partitioned variables.

2349
01:38:12,065 --> 01:38:30,083
5.12 more formalisms we get to expected

2350
01:38:30,083 --> 01:38:30,083
free energy.

2351
01:38:23,076 --> 01:38:35,088
Expected free energy is equation 41K.

2352
01:38:30,083 --> 01:39:07,114
Active inference is Hamilton's principle

2353
01:39:07,114 --> 01:39:07,114
of leased action on expected free energy

2354
01:39:07,114 --> 01:39:07,114
and expresses the most likely decision

2355
01:39:07,114 --> 01:39:07,114
where certain features are met principle

2356
01:39:07,114 --> 01:39:07,114
of least action on manifolds of inference

2357
01:39:07,114 --> 01:39:07,114
and action subsuming or bringing together

2358
01:39:07,114 --> 01:39:07,114
inference and action active inference,

2359
01:39:07,114 --> 01:39:07,114
free energy principle 5.1.3.

2360
01:38:36,089 --> 01:39:12,119
Active Inference Framework AIF looks like

2361
01:39:12,119 --> 01:39:12,119
it describes agents that engage in

2362
01:39:12,119 --> 01:39:12,119
purposeful behavior.

2363
01:39:07,114 --> 01:39:21,128
We can rearrange the expected free energy

2364
01:39:21,128 --> 01:39:21,128
EFE in several ways, each of which

2365
01:39:21,128 --> 01:39:21,128
reveals a fundamental trade off that

2366
01:39:21,128 --> 01:39:21,128
underwrites decision making.

2367
01:39:12,119 --> 01:39:30,137
This allows us to relate active inference

2368
01:39:30,137 --> 01:39:30,137
to information theoretic formalizations

2369
01:39:30,137 --> 01:39:30,137
of decision making that predominate in

2370
01:39:30,137 --> 01:39:30,137
statistics, cognitive science, and

2371
01:39:30,137 --> 01:39:30,137
engineering.

2372
01:39:21,128 --> 01:39:37,144
Figure three, as hinted ant in the early,

2373
01:39:37,144 --> 01:39:37,144
early keywords.

2374
01:39:31,138 --> 01:39:41,148
Here on the top, we have the general

2375
01:39:41,148 --> 01:39:41,148
formalizations of active inference.

2376
01:39:37,144 --> 01:40:03,164
And one of the partitions, or better to

2377
01:40:03,164 --> 01:40:03,164
say competition, is shown here, where

2378
01:40:03,164 --> 01:40:03,164
extrinsic value is the surprising about

2379
01:40:03,164 --> 01:40:03,164
observations, making sure that what we're

2380
01:40:03,164 --> 01:40:03,164
getting in observations are what we

2381
01:40:03,164 --> 01:40:03,164
expect slash prefer if the body wants to

2382
01:40:03,164 --> 01:40:03,164
be expecting homeostatic temperature.

2383
01:39:41,148 --> 01:40:06,167
That's what this is going to determine.

2384
01:40:04,165 --> 01:40:27,188
And it plays a role equivalent to reward

2385
01:40:27,188 --> 01:40:27,188
in reward and reinforcement learning

2386
01:40:27,188 --> 01:40:27,188
based approaches, because we don't need

2387
01:40:27,188 --> 01:40:27,188
to actually set a reward function, but we

2388
01:40:27,188 --> 01:40:27,188
get something that looks like

2389
01:40:27,188 --> 01:40:27,188
selfsurprisal aligned with reward using

2390
01:40:27,188 --> 01:40:27,188
the preference variable over

2391
01:40:27,188 --> 01:40:27,188
observations.

2392
01:40:06,167 --> 01:40:30,191
A lot more to say there.

2393
01:40:28,189 --> 01:40:39,200
And the second term is the intrinsic

2394
01:40:39,200 --> 01:40:39,200
value, the epistemic value, curiosity,

2395
01:40:39,200 --> 01:40:39,200
novelty, learning, reduction of

2396
01:40:39,200 --> 01:40:39,200
uncertainty.

2397
01:40:30,191 --> 01:40:42,203
And another partitioning is between risk

2398
01:40:42,203 --> 01:40:42,203
and ambiguity.

2399
01:40:39,200 --> 01:40:52,213
There are four colored dots red, tan,

2400
01:40:52,213 --> 01:40:52,213
gray, and Bleu corresponding to special

2401
01:40:52,213 --> 01:40:52,213
cases.

2402
01:40:43,204 --> 01:41:08,223
So under the setting where there's no

2403
01:41:08,223 --> 01:41:08,223
ambiguity, we realize or manifest the

2404
01:41:08,223 --> 01:41:08,223
special case where control can be seen as

2405
01:41:08,223 --> 01:41:08,223
inference, maximum entropy, reinforcement

2406
01:41:08,223 --> 01:41:08,223
learning, prospect theory, and KL optimal

2407
01:41:08,223 --> 01:41:08,223
control.

2408
01:40:52,213 --> 01:41:17,232
This is like doing as good as you can

2409
01:41:17,232 --> 01:41:17,232
strategically or tactically, given that

2410
01:41:17,232 --> 01:41:17,232
there's no ambiguity in how your

2411
01:41:17,232 --> 01:41:17,232
decisions play out.

2412
01:41:08,223 --> 01:41:26,241
Now, under the setting where there's no

2413
01:41:26,241 --> 01:41:26,241
ambiguity or preferences, a maximum

2414
01:41:26,241 --> 01:41:26,241
entropy principle is realized.

2415
01:41:18,233 --> 01:41:29,244
And work with Bayesian mechanics.

2416
01:41:27,241 --> 01:41:30,245
And Dalton et al.

2417
01:41:29,244 --> 01:41:41,256
Has shown that the constrained maximum

2418
01:41:41,256 --> 01:41:41,256
entropy principle is dual to the FEP in

2419
01:41:41,256 --> 01:41:41,256
the case of no extrinsic value.

2420
01:41:31,246 --> 01:41:45,260
So no quote reward, no quote goals.

2421
01:41:41,256 --> 01:42:04,273
We don't use reward or goals in the

2422
01:42:04,273 --> 01:42:04,273
active inference ontology, but borrowing

2423
01:42:04,273 --> 01:42:04,273
those words from some long, long

2424
01:42:04,273 --> 01:42:04,273
forgotten ontology, in the case of no

2425
01:42:04,273 --> 01:42:04,273
extrinsic value, we realized the special

2426
01:42:04,273 --> 01:42:04,273
case of maximum information gain,

2427
01:42:04,273 --> 01:42:04,273
Bayesian experimental design.

2428
01:41:45,260 --> 01:42:18,287
So not to prove or disprove, but the

2429
01:42:18,287 --> 01:42:18,287
maximally informative experiment that's

2430
01:42:18,287 --> 01:42:18,287
the Bayesian scientific epistemology

2431
01:42:18,287 --> 01:42:18,287
intrinsic implication and Bayesian

2432
01:42:18,287 --> 01:42:18,287
surprise seeking out optimally

2433
01:42:18,287 --> 01:42:18,287
informative stimuli.

2434
01:42:04,273 --> 01:42:24,293
And contrast that with a setting where

2435
01:42:24,293 --> 01:42:24,293
there's no intrinsic value.

2436
01:42:19,288 --> 01:42:36,305
So where there's nothing to learn, we

2437
01:42:36,305 --> 01:42:36,305
realize expected utility theory, Bayesian

2438
01:42:36,305 --> 01:42:36,305
decision theory, reinforcement learning,

2439
01:42:36,305 --> 01:42:36,305
and optimal control.

2440
01:42:24,293 --> 01:43:08,331
So special cases from many different

2441
01:43:08,331 --> 01:43:08,331
Fields, ranging from statistical

2442
01:43:08,331 --> 01:43:08,331
mechanics to Bayesian decision making and

2443
01:43:08,331 --> 01:43:08,331
statistics integrated or perhaps better

2444
01:43:08,331 --> 01:43:08,331
generalized across in the formalisms of

2445
01:43:08,331 --> 01:43:08,331
active inference, which is something like

2446
01:43:08,331 --> 01:43:08,331
a Hamilton's principle of least action on

2447
01:43:08,331 --> 01:43:08,331
variational or expected free energy,

2448
01:43:08,331 --> 01:43:08,331
expressing the most likely inference and

2449
01:43:08,331 --> 01:43:08,331
action under certain constraints.

2450
01:42:36,305 --> 01:43:11,334
Not the most rewarding, but the most

2451
01:43:11,334 --> 01:43:11,334
likely.

2452
01:43:08,331 --> 01:43:15,338
5.1.3.

2453
01:43:13,336 --> 01:43:18,341
Decision making minimizes both risk and

2454
01:43:18,341 --> 01:43:18,341
ambiguity.

2455
01:43:15,338 --> 01:43:30,353
Risk, first term on the right hand side,

2456
01:43:30,353 --> 01:43:30,353
ambiguity second, term minimizing

2457
01:43:30,353 --> 01:43:30,353
ambiguity leads to a type of

2458
01:43:30,353 --> 01:43:30,353
observational bias commonly known as the

2459
01:43:30,353 --> 01:43:30,353
streetlight effect.

2460
01:43:18,341 --> 01:43:34,357
When a person loses their keys at night,

2461
01:43:34,357 --> 01:43:34,357
they initially search for them under the

2462
01:43:34,357 --> 01:43:34,357
streetlight.

2463
01:43:30,353 --> 01:43:46,369
Because of the resulting observations, I

2464
01:43:46,369 --> 01:43:46,369
see my keys under the streetlight, or I

2465
01:43:46,369 --> 01:43:46,369
do not see my keys under the streetlight

2466
01:43:46,369 --> 01:43:46,369
accurately disambiguate external states

2467
01:43:46,369 --> 01:43:46,369
of affairs.

2468
01:43:34,357 --> 01:43:54,377
First place to look make sense under the

2469
01:43:54,377 --> 01:43:54,377
streetlight, and the last place you look

2470
01:43:54,377 --> 01:43:54,377
is where you find it more in.

2471
01:43:46,369 --> 01:43:55,378
5.1.3.

2472
01:43:54,377 --> 01:44:09,386
Decision making maximizes extrinsic and

2473
01:44:09,386 --> 01:44:09,386
intrinsic value another decomposition of

2474
01:44:09,386 --> 01:44:09,386
the formalisms of active inference

2475
01:44:09,386 --> 01:44:09,386
maximizing information gain leads to a

2476
01:44:09,386 --> 01:44:09,386
goal directed form of exploration driven

2477
01:44:09,386 --> 01:44:09,386
to answer what would happen if I did.

2478
01:43:55,378 --> 01:44:23,400
That true counterfactuals this decision

2479
01:44:23,400 --> 01:44:23,400
making procedure underwrites Bayesian

2480
01:44:23,400 --> 01:44:23,400
Experimental Design in Statistics, which

2481
01:44:23,400 --> 01:44:23,400
describes optimal experiments as those

2482
01:44:23,400 --> 01:44:23,400
that maximize expected information gain.

2483
01:44:09,386 --> 01:44:44,421
And we've had some great discussions

2484
01:44:44,421 --> 01:44:44,421
recently in textbook groups and in

2485
01:44:44,421 --> 01:44:44,421
discussion hours, where we contrasted

2486
01:44:44,421 --> 01:44:44,421
that falsificationist concept of accept

2487
01:44:44,421 --> 01:44:44,421
or reject hypothesis, and even the idea

2488
01:44:44,421 --> 01:44:44,421
of a scientist or a researcher setting

2489
01:44:44,421 --> 01:44:44,421
out to accept or reject hypothesis.

2490
01:44:23,400 --> 01:44:56,433
Whether you take that Positivist or you

2491
01:44:56,433 --> 01:44:56,433
take the other path of falsificationism,

2492
01:44:56,433 --> 01:44:56,433
in both cases, you might end up with an

2493
01:44:56,433 --> 01:44:56,433
informative experiment or not.

2494
01:44:44,421 --> 01:45:10,441
But you can easily imagine cases where

2495
01:45:10,441 --> 01:45:10,441
you end up with an uninformative

2496
01:45:10,441 --> 01:45:10,441
experiment because you set out to confirm

2497
01:45:10,441 --> 01:45:10,441
something you knew, or you set out to

2498
01:45:10,441 --> 01:45:10,441
disprove something by constraining your

2499
01:45:10,441 --> 01:45:10,441
experiment so that it was locally

2500
01:45:10,441 --> 01:45:10,441
disproven.

2501
01:44:56,433 --> 01:45:31,462
In contrast, when we take into account

2502
01:45:31,462 --> 01:45:31,462
the richness of our generative model, we

2503
01:45:31,462 --> 01:45:31,462
motivate this Bayesian epistemology and

2504
01:45:31,462 --> 01:45:31,462
ultimately professional Bayesianism,

2505
01:45:31,462 --> 01:45:31,462
where we make optimal experiments in

2506
01:45:31,462 --> 01:45:31,462
terms of their maximum expected

2507
01:45:31,462 --> 01:45:31,462
information gain, which requires you to

2508
01:45:31,462 --> 01:45:31,462
also state your generative model.

2509
01:45:10,441 --> 01:45:42,473
Decision making under active inference

2510
01:45:42,473 --> 01:45:42,473
weighs the imperatives of maximizing

2511
01:45:42,473 --> 01:45:42,473
utility and information gain, which

2512
01:45:42,473 --> 01:45:42,473
suggests a principled solution to the

2513
01:45:42,473 --> 01:45:42,473
exploration exploitation dilemma.

2514
01:45:32,463 --> 01:45:45,476
Great point to jump into.

2515
01:45:43,474 --> 01:45:54,485
Does active inference, simply by written

2516
01:45:54,485 --> 01:45:54,485
down, by being written down on a paper,

2517
01:45:54,485 --> 01:45:54,485
resolve or transcend or dissolve

2518
01:45:54,485 --> 01:45:54,485
exploration exploitation?

2519
01:45:45,476 --> 01:45:56,487
No.

2520
01:45:55,486 --> 01:46:08,493
Does it provide a space or a framework, a

2521
01:46:08,493 --> 01:46:08,493
method, an approach, software packages, a

2522
01:46:08,493 --> 01:46:08,493
research community that can help us

2523
01:46:08,493 --> 01:46:08,493
address and navigate and surf on the edge

2524
01:46:08,493 --> 01:46:08,493
of that dilemma?

2525
01:45:56,487 --> 01:46:09,494
Absolutely.

2526
01:46:09,494 --> 01:46:12,497
5.2.1.

2527
01:46:11,496 --> 01:46:20,505
The basic active inference algorithm

2528
01:46:20,505 --> 01:46:20,505
we're going to go through it with authors

2529
01:46:20,505 --> 01:46:20,505
preferential inference.

2530
01:46:12,497 --> 01:46:24,509
We want to infer preferences about

2531
01:46:24,509 --> 01:46:24,509
external and observable trajectories.

2532
01:46:20,505 --> 01:46:29,514
Two for each possible sequence of past,

2533
01:46:29,514 --> 01:46:29,514
present and future actions.

2534
01:46:25,510 --> 01:46:38,523
A we're going to engage in both sides of

2535
01:46:38,523 --> 01:46:38,523
the coin perceptual inference what would

2536
01:46:38,523 --> 01:46:38,523
I perceive if that happened?

2537
01:46:29,514 --> 01:46:44,529
And planning as inference assess the

2538
01:46:44,529 --> 01:46:44,529
action sequence by evaluating its

2539
01:46:44,529 --> 01:46:44,529
expected free energy.

2540
01:46:39,523 --> 01:47:05,544
And then three, decision making execute

2541
01:47:05,544 --> 01:47:05,544
the most likely decision at T plus one,

2542
01:47:05,544 --> 01:47:05,544
according to some distinctions, sample

2543
01:47:05,544 --> 01:47:05,544
from your action posterior, action prior

2544
01:47:05,544 --> 01:47:05,544
that's, like your habits, it gets

2545
01:47:05,544 --> 01:47:05,544
sharpened or modified with expected free

2546
01:47:05,544 --> 01:47:05,544
energy, and then you sample from the

2547
01:47:05,544 --> 01:47:05,544
action posterior.

2548
01:46:45,529 --> 01:47:08,547
5.2.1.

2549
01:47:07,546 --> 01:47:20,559
Sequential Decision Making under

2550
01:47:20,559 --> 01:47:20,559
Uncertainty a partially observable Markov

2551
01:47:20,559 --> 01:47:20,559
decision process POMDP is a discrete time

2552
01:47:20,559 --> 01:47:20,559
model of how actions influence external

2553
01:47:20,559 --> 01:47:20,559
states.

2554
01:47:08,547 --> 01:47:26,565
In a POMDP, each external state depends

2555
01:47:26,565 --> 01:47:26,565
only on the current action and previous

2556
01:47:26,565 --> 01:47:26,565
external state.

2557
01:47:20,559 --> 01:47:28,567
That's the Markov property.

2558
01:47:26,565 --> 01:47:37,576
Each observation depends only on the

2559
01:47:37,576 --> 01:47:37,576
current external state, and one can

2560
01:47:37,576 --> 01:47:37,576
additionally specify a distribution of

2561
01:47:37,576 --> 01:47:37,576
preferences over external trajectories.

2562
01:47:28,567 --> 01:47:41,580
One and two form the agent's POMDP

2563
01:47:41,580 --> 01:47:41,580
prediction model.

2564
01:47:37,576 --> 01:47:50,589
Two and three form the agent's hidden

2565
01:47:50,589 --> 01:47:50,589
Markov preference model, which defines

2566
01:47:50,589 --> 01:47:50,589
the active inference agent.

2567
01:47:43,582 --> 01:47:55,594
A simple simulation of active inference

2568
01:47:55,594 --> 01:47:55,594
on a POMDP is provided in figure four.

2569
01:47:50,589 --> 01:47:59,598
Implementation details on generic POMDPs

2570
01:47:59,598 --> 01:47:59,598
are available.

2571
01:47:55,594 --> 01:48:03,596
For more complex simulations of

2572
01:48:03,596 --> 01:48:03,596
sequential decision making, e.

2573
01:47:59,598 --> 01:48:09,602
G involving hierarchical POMDPs, please

2574
01:48:09,602 --> 01:48:09,602
see citations.

2575
01:48:03,596 --> 01:48:11,604
Figure four.

2576
01:48:10,603 --> 01:48:13,606
It's a prediction model.

2577
01:48:11,604 --> 01:48:17,610
This is sequential decision making in a

2578
01:48:17,610 --> 01:48:17,610
Team A's environment.

2579
01:48:13,606 --> 01:48:23,616
On the left, the agent's prediction model

2580
01:48:23,616 --> 01:48:23,616
as a POMDP, represented here as a

2581
01:48:23,616 --> 01:48:23,616
Bayesian network.

2582
01:48:17,610 --> 01:48:26,619
On the right, information on the team

2583
01:48:26,619 --> 01:48:26,619
A's.

2584
01:48:24,617 --> 01:48:42,635
So here we have some type of Bayesian

2585
01:48:42,635 --> 01:48:42,635
graph reflecting hopefully a statistical

2586
01:48:42,635 --> 01:48:42,635
system that's going to have all of these

2587
01:48:42,635 --> 01:48:42,635
well behaved properties that we've been

2588
01:48:42,635 --> 01:48:42,635
talking about in the paper.

2589
01:48:26,619 --> 01:48:46,639
And then the team AZ is played out.

2590
01:48:42,635 --> 01:48:48,641
We'll talk about it.

2591
01:48:46,639 --> 01:48:50,643
5.2.3.

2592
01:48:49,641 --> 01:48:56,649
World model learning is inference due to

2593
01:48:56,649 --> 01:48:56,649
a lack of domain knowledge, it may be

2594
01:48:56,649 --> 01:48:56,649
challenging to specify an agent's

2595
01:48:56,649 --> 01:48:56,649
prediction and preference model.

2596
01:48:50,643 --> 01:48:59,652
For example, how do external states map

2597
01:48:59,652 --> 01:48:59,652
to observations?

2598
01:48:56,649 --> 01:49:03,650
Should external states be represented in

2599
01:49:03,650 --> 01:49:03,650
a discrete or continuous state space?

2600
01:48:59,652 --> 01:49:07,654
Also, some great questions that come up

2601
01:49:07,654 --> 01:49:07,654
every day.

2602
01:49:03,650 --> 01:49:17,664
And Clark addressed in chapter six a

2603
01:49:17,664 --> 01:49:17,664
recipe for active inference modeling by

2604
01:49:17,664 --> 01:49:17,664
the Par Pazulo and Friston 2022

2605
01:49:17,664 --> 01:49:17,664
textbook.

2606
01:49:07,654 --> 01:49:24,671
So how do we get the right priors, or at

2607
01:49:24,671 --> 01:49:24,671
least approximately adequate priors?

2608
01:49:18,665 --> 01:49:32,679
One way to answer the question lies in

2609
01:49:32,679 --> 01:49:32,679
optimizing a free energy functional F,

2610
01:49:32,679 --> 01:49:32,679
which is an evidence lower bound.

2611
01:49:25,672 --> 01:49:42,689
We see here variational free energy

2612
01:49:42,689 --> 01:49:42,689
decomposed into an energy minus entropy

2613
01:49:42,689 --> 01:49:42,689
and decomposed into a complexity minus

2614
01:49:42,689 --> 01:49:42,689
accuracy.

2615
01:49:32,679 --> 01:50:03,704
Framing maximizing accuracy usually

2616
01:50:03,704 --> 01:50:03,704
results in generative models involving

2617
01:50:03,704 --> 01:50:03,704
universal function approximators

2618
01:50:03,704 --> 01:50:03,704
minimizing complexity results action

2619
01:50:03,704 --> 01:50:03,704
action oriented representation sparse

2620
01:50:03,704 --> 01:50:03,704
compartmentalized in hierarchical

2621
01:50:03,704 --> 01:50:03,704
generative models where higher levels of

2622
01:50:03,704 --> 01:50:03,704
the hierarchical model more abstract

2623
01:50:03,704 --> 01:50:03,704
representations, and vice versa.

2624
01:49:42,689 --> 01:50:16,717
A computationally efficient method to

2625
01:50:16,717 --> 01:50:16,717
compare priors by their free energy is

2626
01:50:16,717 --> 01:50:16,717
Bayesian model reduction, free energy,

2627
01:50:16,717 --> 01:50:16,717
unifies inference, and model selection

2628
01:50:16,717 --> 01:50:16,717
under a single objective function.

2629
01:50:04,705 --> 01:50:20,721
5.24.

2630
01:50:18,719 --> 01:50:26,727
Scaling active inference planning for all

2631
01:50:26,727 --> 01:50:26,727
possible courses of action is

2632
01:50:26,727 --> 01:50:26,727
computationally expensive.

2633
01:50:20,721 --> 01:50:35,736
One way to finesse this is by planning

2634
01:50:35,736 --> 01:50:35,736
only for intelligently chosen subsets of

2635
01:50:35,736 --> 01:50:35,736
action sequences using sampling

2636
01:50:35,736 --> 01:50:35,736
algorithms like Monte Carlo Tree Search.

2637
01:50:27,728 --> 01:50:41,742
If you're interested in that, check out

2638
01:50:41,742 --> 01:50:41,742
the recent research on branching time

2639
01:50:41,742 --> 01:50:41,742
active inference.

2640
01:50:36,736 --> 01:50:48,749
Similarly, Monte Carlo sampling finesses

2641
01:50:48,749 --> 01:50:48,749
the expectations inherent in assessing

2642
01:50:48,749 --> 01:50:48,749
action sequences.

2643
01:50:42,743 --> 01:50:59,760
A complementary approach is to assess

2644
01:50:59,760 --> 01:50:59,760
actions instead of action sequences by

2645
01:50:59,760 --> 01:50:59,760
conditioning all future actions to be

2646
01:50:59,760 --> 01:50:59,760
optimal in the sense that they minimize

2647
01:50:59,760 --> 01:50:59,760
the expected free energy.

2648
01:50:48,749 --> 01:51:09,764
It leads to smarter agents whose

2649
01:51:09,764 --> 01:51:09,764
computational complexity scales linearly

2650
01:51:09,764 --> 01:51:09,764
as opposed to exponentially in the length

2651
01:51:09,764 --> 01:51:09,764
of action sequence.

2652
01:51:00,755 --> 01:51:11,766
Let's learn more about it.

2653
01:51:10,765 --> 01:51:18,773
Scalable inference methods can be used to

2654
01:51:18,773 --> 01:51:18,773
make active inference more efficient.

2655
01:51:13,768 --> 01:51:39,793
We can train neural networks to predict

2656
01:51:39,793 --> 01:51:39,793
the various posterior distributions,

2657
01:51:39,793 --> 01:51:39,793
including the posterior over action while

2658
01:51:39,793 --> 01:51:39,793
training the output of the neural network

2659
01:51:39,793 --> 01:51:39,793
can be used as an initial conditions for

2660
01:51:39,793 --> 01:51:39,793
variational coherence, resulting active

2661
01:51:39,793 --> 01:51:39,793
inference whose computational costs

2662
01:51:39,793 --> 01:51:39,793
decrease as the network learns.

2663
01:51:18,773 --> 01:51:49,804
Additionally, optimizing free energy

2664
01:51:49,804 --> 01:51:49,804
reduces to efficient message passing

2665
01:51:49,804 --> 01:51:49,804
schemes when one imposes certain

2666
01:51:49,804 --> 01:51:49,804
simplifying distinctions to the family of

2667
01:51:49,804 --> 01:51:49,804
candidate distributions.

2668
01:51:39,794 --> 01:52:06,815
Lots of really exciting work in

2669
01:52:06,815 --> 01:52:06,815
neurobiology and statistics around

2670
01:52:06,815 --> 01:52:06,815
message passing getting close to the end

2671
01:52:06,815 --> 01:52:06,815
here a much cheaper implication of active

2672
01:52:06,815 --> 01:52:06,815
inference exists for continuous states

2673
01:52:06,815 --> 01:52:06,815
evolving in continuous time.

2674
01:51:49,804 --> 01:52:08,817
Pretty cool.

2675
01:52:07,816 --> 01:52:09,818
Par et al.

2676
01:52:08,817 --> 01:52:28,837
2022 Textbook Chapter Seven Discrete time

2677
01:52:28,837 --> 01:52:28,837
Active Inference generative Models

2678
01:52:28,837 --> 01:52:28,837
Chapter Eight Continuous time Active

2679
01:52:28,837 --> 01:52:28,837
Inference Models this method frames

2680
01:52:28,837 --> 01:52:28,837
perception and decision making as

2681
01:52:28,837 --> 01:52:28,837
variational inference by simulating a

2682
01:52:28,837 --> 01:52:28,837
gradient flow on free energy in an

2683
01:52:28,837 --> 01:52:28,837
extended state space.

2684
01:52:09,818 --> 01:52:35,844
It can be combined with discrete active

2685
01:52:35,844 --> 01:52:35,844
inference to operate efficiently in

2686
01:52:35,844 --> 01:52:35,844
generative models combining discrete and

2687
01:52:35,844 --> 01:52:35,844
continuous states.

2688
01:52:28,837 --> 01:52:56,865
We talked about that in live stream 46

2689
01:52:56,865 --> 01:52:56,865
Active inference does not contradict folk

2690
01:52:56,865 --> 01:52:56,865
psychology, discrete active inference

2691
01:52:56,865 --> 01:52:56,865
decision making, active inference dai,

2692
01:52:56,865 --> 01:52:56,865
and continuous time motor active

2693
01:52:56,865 --> 01:52:56,865
inference being used together, also seen

2694
01:52:56,865 --> 01:52:56,865
in the paradigm textbook.

2695
01:52:36,845 --> 01:53:00,863
As an example, high dimensional

2696
01:53:00,863 --> 01:53:00,863
observations in the continuous domain,

2697
01:53:00,863 --> 01:53:00,863
e.

2698
01:52:56,865 --> 01:53:08,871
G speech processed through continuous

2699
01:53:08,871 --> 01:53:08,871
active inference are converted into

2700
01:53:08,871 --> 01:53:08,871
discrete abstract representations, e.

2701
01:53:00,863 --> 01:53:19,882
G semantics, and we can even go further

2702
01:53:19,882 --> 01:53:19,882
and say rhetorical and narrative

2703
01:53:19,882 --> 01:53:19,882
information spaces based on these

2704
01:53:19,882 --> 01:53:19,882
representations, the agent makes high

2705
01:53:19,882 --> 01:53:19,882
level categorical decisions, eg.

2706
01:53:08,871 --> 01:53:25,888
I want to move over there, which

2707
01:53:25,888 --> 01:53:25,888
contextualize low level continuous

2708
01:53:25,888 --> 01:53:25,888
actions, eg.

2709
01:53:19,882 --> 01:53:33,896
The continuous motion of a limb towards

2710
01:53:33,896 --> 01:53:33,896
the goal location and that is how the

2711
01:53:33,896 --> 01:53:33,896
paper ends.

2712
01:53:25,888 --> 01:53:38,901
So, closing thoughts if anybody wants to

2713
01:53:38,901 --> 01:53:38,901
write a comment live, feel free to do

2714
01:53:38,901 --> 01:53:38,901
so.

2715
01:53:33,896 --> 01:53:40,903
We'll be over very shortly.

2716
01:53:39,902 --> 01:53:45,908
What are the implications of this work?

2717
01:53:41,904 --> 01:53:51,914
We have an open space to talk about it.

2718
01:53:46,909 --> 01:53:58,921
What questions and discussion topics are

2719
01:53:58,921 --> 01:53:58,921
you interested in?

2720
01:53:53,916 --> 01:54:06,923
Please write comments before or after the

2721
01:54:06,923 --> 01:54:06,923
dot one and the dot two so we can have

2722
01:54:06,923 --> 01:54:06,923
those interesting discussions.

2723
01:53:58,921 --> 01:54:32,949
And just as a little bit of a closer,

2724
01:54:32,949 --> 01:54:32,949
I'll share some stable diffusion images

2725
01:54:32,949 --> 01:54:32,949
that were generated using the paper title

2726
01:54:32,949 --> 01:54:32,949
as well as various other terms free

2727
01:54:32,949 --> 01:54:32,949
energy, lots of fun images, a lot of good

2728
01:54:32,949 --> 01:54:32,949
balance to some of the technical aspects

2729
01:54:32,949 --> 01:54:32,949
that were being described in the paper.

2730
01:54:06,923 --> 01:54:43,960
Was good to look at what diffusion looks

2731
01:54:43,960 --> 01:54:43,960
like aesthetically and so that is the end

2732
01:54:43,960 --> 01:54:43,960
of this Livestream.

2733
01:54:32,949 --> 01:55:05,976
52.0 I hope that you found it useful that

2734
01:55:05,976 --> 01:55:05,976
you're interested to act in first, serve

2735
01:55:05,976 --> 01:55:05,976
to learn more, contribute more, write a

2736
01:55:05,976 --> 01:55:05,976
comment, make it happen in your own life

2737
01:55:05,976 --> 01:55:05,976
or in your own way with this honestly

2738
01:55:05,976 --> 01:55:05,976
challenging paper.

2739
01:54:43,960 --> 01:55:21,992
So if you've listened this far, thanks a

2740
01:55:21,992 --> 01:55:21,992
lot for your attention and looking

2741
01:55:21,992 --> 01:55:21,992
forward to 520 One and Zero Two when we

2742
01:55:21,992 --> 01:55:21,992
will speak with some authors and some of

2743
01:55:21,992 --> 01:55:21,992
you.

2744
01:55:05,976 --> 01:55:27,998
So till next time, thanks again and see

2745
01:55:27,998 --> 01:55:27,998
you in the dot.

2746
01:55:21,992 --> 01:55:28,999
One.

2747
01:55:27,998 --> 01:55:29,000
Bye.


SPEAKER_00:
Hello and welcome.

This is ACT-INF Livestream number 52.0 and it is February 28th, 2023.

Welcome to the Active Inference Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

You can find us at the links here on this slide.

This is a recorded and an archived livestream, so please provide feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll be following video etiquette for livestreams.

Head over to ActiveInference.org if you want to learn more and participate in learning groups and projects at the Institute, including these live streams.

Well.

We're here today to learn and discuss the paper Geometric Methods for Sampling Optimization Inference and Adaptive Agents by Alessandra Barp, Lancelot DeCosta, Guilherme Franca, Karl Fristen, Mark Ghiralami, Michael Jordan, and Gregorios Pavliotis.

This video, like all DOT Zero videos are,

is a introduction for some of the ideas.

It is not a review or a final word.

And as we've joked before, it's more than anything, a call for help.

So if you're curious about these topics, if you're knowledgeable about these topics, we would really look forward to you getting involved in the upcoming 52.1 and 52.2 discussions, as well as in an ongoing basis to help us understand some of the technical details

This is going to get technical at times and certainly beyond the technicalities I understand, though I am looking forward to presenting them.

And it'll be great to have those who have backgrounds of all different types to come together and talk about this awesome work.

Also, a big thanks to Ali and Kandon for the assistance, technical and moral, during the preparations here.

In 52.0, we're going to bring up some aims and claims, read the abstract, look at the roadmap, and talk about the keywords.

And then we will walk through the paper with an emphasis on the figures, formalisms, and key points.

in the coming weeks we're going to be discussing this paper with one or more authors so as usual get in touch if you want to participate or if it's after the fact you can still get involved we can start with an introduction or a warm-up I'm Daniel I'm a researcher in California and

i'm tempted to say just totally honestly i'm happy to get this one over with but that sounds a little bit different than i might intend it to be i'm really excited to dive into this work which is going to be approaching active inference from an angle that we haven't necessarily highlighted on these streams so i think it's going to be a fascinating discussion it's going to run the gamut span the gap however you choose to see it between

technical sophistication and intuition which is a great place to be so i've been really excited and motivated to prepare and i'm looking forward to the dot zero we're doing right now and to the upcoming discussions

so let's jump in with the big questions that might motivate one to read this paper this kind of paper even if they were not familiar with the authors or topics and these are just a few ways to write it up of course not the only ways so first question

how can we effectively and efficiently navigate information geometric or information theoretic landscapes and how can we tackle that question from an analytical which is to say equation based as well as a computational which is to say real implementation based perspective often it's really

fun, intuitive, natural to talk about information theory, even for those who haven't taken the technical prerequisites.

And this work may help us navigate to a space where we're able to think with good intuitions about information geometry, information theory, and also make sure that those intuitions are going to be caught by our technical tools.

Second, how can we optimize inference in complex models, including cases where we are doing inference on action as a parameter, also known as active inference?

Optimization and functional analysis, as we'll come to see soon, has long been interested in these complex or challenging models that are right at the margin, right at the border of what is tractable or not, given the computational hardware that modelers have access to.

For example, big data sets, high dimensional state spaces, complex dynamics, and so on.

How about cases where we're also interested in doing parametric inference and optimization on action or action plans as a parameter, also known as active inference.

And third, what technical underpinnings support the rigor and applicability of active inference?

And what special cases are revealed when we consider possibilities and constraints?

These are those two branches of analytical and computational coming back again.

We want to make sure that when we're thinking within the active inference paradigm, the active inference ontology is the language that we're speaking and that there's a technical rigor to what's being discussed.

Additionally, we want to make sure that it's not just an analytical rigor, but it's also going to be computationally plausible, tractable, or maybe even preferable.

That would be awesome.

Those are the questions that at least appeared.

We are discussing this paper, Geometric Methods for Sampling, Optimization, Inference, and Adaptive Agents by Barp and DaCosta et al., shared first authorship, and it was published on Archive.

I'll just note a few key claims from this paper.

They wrote, our goal in this chapter is to discuss the emergence of natural geometries within a few important areas of statistics and applied mathematics, namely optimization, sampling, inference, and adaptive agents.

That's the title.

We provide a conceptual introduction to the underlying ideas rather than a technical discussion, highlighting connections with various fields of mathematics and physics.

Well, I can tell you that they do make connections with various fields of mathematics and physics, though whether it is a conceptual introduction or a technical discussion is all going to be about your perspective.

Third, to illustrate a generic use case for the previous methodologies that are going to be discussed, we consider active inference, a unifying formulation of behavior, subsuming perception, planning, and learning as a process of inference.

So inference on adaptive agents is not necessarily new.

It goes by reinforcement learning, machine learning, and so on.

However, to use those methodologies

in the context of active inference a unifying formulation of behavior is something that the authors are bringing forth here and last we describe decision making under active inference using information geometry revealing several special cases that are established notions in statistics cognitive science and engineering so active inference is not just unifying it's going to be shown to be generalized as well which is to say that special cases of the generalization

emerge different formalisms that were known disparately across fields.

On to the abstract.

They write, in this chapter, we identify fundamental geometric structures that underlie the problems of sampling, optimization, inference, and adaptive decision-making.

Based on this identification, we derive algorithms that exploit these geometric structures to solve these problems efficiently.

We show that a wide range of geometric theories emerge naturally in these fields, ranging from measure-preserving processes, information divergences, Poisson geometry, and geometric integration.

Specifically, we explain how.

1.

Leveraging the symplectic geometry of Hamiltonian systems enable us to construct accelerated sampling and optimization methods.


UNKNOWN:
2.


SPEAKER_00:
The theory of Hilbertian subspaces and Stein operators provides a general methodology to obtain robust estimators.


UNKNOWN:
3.


SPEAKER_00:
Preserving the information geometry of decision making yields adaptive agents that perform active inference.

Throughout, we emphasize the rich connections between these fields, e.g., inference draws on sampling and optimization, and adaptive decision-making assesses decisions by inferring their counterfactual consequences.

Our exposition provides a conceptual overview of underlying ideas rather than a technical discussion, which can be found in the references herein.

And indeed, there are several hundred references in this paper.

Let's go to the roadmap.

So, first, we can start on the right side.

Here's an active agent.

Vroom, vroom.

doing accelerated optimization in the carpool lane, looking ahead to the almost dessert-like visual representation of active inference, that's where we're going to go.

And on the right side, it's just some cars.

One of them is accelerated.

The other one isn't.

The paper begins with an introduction, and then in section two gets into the topic of accelerated optimization, covering the areas of principle of geometric integration, conservative flows and symplectic integrators, rate matching integrators for smooth optimization, manifold and constrained optimization, gradient flow as a high friction limit, and optimization on the space of probability measures.

In Section 3, they turn from accelerated optimization in general to Hamiltonian-based accelerated sampling.

The subsections of 3 are Optimizing Diffusion Processes for Sampling and Hamiltonian Monte Carlo.

From the Hamiltonian-based accelerated sampling, they turn towards doing statistical inference with kernel-based discrepancies in section four.

The subsections are topological methods for MMDs, smooth measures and KSDs,

the canonical Stein operator and Poincar√© duality, kernel Stein discrepancies and score matching, information geometry of MMDs and natural gradient descent, minimum Stein discrepancy estimators, likelihood inference with generative models.

Finally, in Section 5, Adaptive Agents Through Active Inference, they're going to bring it home to active inference.

Section 5.1, Modeling Adaptive Decision Making, Behavior, Agents, and Environments, Decision Making in Precise Agents, the Information Geometry of Decision Making,

and 5.2 realizing adaptive agents the basic active inference algorithm sequential decision making under uncertainty world model learning as inference and scaling active inference a quick note we have all the formalisms entered into our dot zero teams coda document there are 46 numbered equations in this paper and 83 overall

We're going to bring in some, but not all of these formalisms in the .zero video, and we're ready to bring in more during the .one and .two discussions with the authors.

Just for those who are watching along, I'm going to scan through all of the formalisms from our coda.

And again, for those who want to help get involved with a .zero preparation, this is a little bit of what .zero preparation looks like.

I'm going to now scroll through the formalisms just in case anybody wants to screenshot or see them.

Great.

On to the keywords.

Keywords of the paper are information geometry, Hamiltonian Monte Carlo, Stein's method, reproducing kernel, variational inference, accelerated optimization, dissipative systems, decision theory, and active inference in no particular pedagogical order.

Let's start with decision theory.

From the Stanford Encyclopedia of Philosophy, the article begins, decision theory is concerned with the reasoning underlying an agent's choices, whether this is a mundane choice between taking the bus or getting a taxi, or a more far reaching choice about whether to pursue a demanding political career.

Decision theory is about agents making decisions, cognitive things making decisions.

And we can take a peek ahead to figure three, which is shown here, as well as invoke some of the keywords that we're about to walk into now.

So if this is your first time hearing some of this vocabulary or you're super familiar, either way, it's great.

In this paper and in this discussion, we're going to see how different decision theoretic settings

such as no ambiguity, no ambiguity or no preference, no extrinsic value, and no intrinsic value, are going to be operationalized within an information theoretic formalism, an information geometric formalism, and specifically one that we're able to do accelerated inference on using variational methods.

we're going to be involving action as a parameter, so we can call that active inference.

And that's how we're approaching decision theory and getting from here to there with all those fun stops in between.

Let's go to information theory.

The paper in the introduction writes, of particular relevance to this chapter is information geometry, e.g.

the differential geometric treatment of smooth statistical manifolds.

whose origin stems from a seminal article by Rao , who introduced the Fischer metric tensor on parameterized statistical models, and thus a natural Riemannian geometry that was later observed to correspond to an infinitesimal distance with respect to the Kolbach-Leibler divergence.

So, going into citation 23 and 24.

Citation 23 is Rao from 1992, and Rao wrote, The objective of the paper is to derive certain inequality relations connecting the elements of the information matrix as defined by Fisher and the variances and covariances of the estimating functions.

A class of distribution functions which admit estimation of parameters with the minimum possible variance has been discussed.

The concept of distance between populations of a given type has been developed starting from a quadratic differential metric defining the element of length.

So I'm only going to give the disclaimer one time.

Everything in red text is beyond speculative.

It's just priming the pump for discussions with those who know more and with those who know less.

And it's just a first pass that we're going to continue to develop on.

But broadly...

If we can compute inequality relationships, which this paper developed in 1992, we can bound estimators, test for relative improvements, and basically do stuff with those distributions.

And if we can compute certain kinds of statistical distances, such that a distance requires a length metric,

we can find optimal estimators of parameters, specifically their variances and covariance structure, in principle and in practice, by finding the maximum informational alignment to other estimators or empirical data.

So we'll often say like, if you had the optimum parameterization, you'd be predicting as well as you could.

And so we want to be able to operationalize that using distances and metrics.

and citation 24. this is an invariant form for the prior probability and estimation problems by harold jeffries and jeffries wrote it has shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters

For laws containing a location and a scale parameter, a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters.

This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.

So, if all the parameters in our model, whether there's one, two, or more,

are differentiable, which is to say they're smooth, et cetera, et cetera.

There are technical details.

Then there are certain transformational invariances.

So just like every Gaussian distribution, by shifting it and stretching and shrinking it, you can map those onto each other.

It's like that.

It turns out that some of those constraints can even be somewhat relaxed.

For example, this approach may still be effective even where parameters are not differentiable everywhere.

And this is big for helping us do Bayesian estimation, which can be seen as a special case of informational transformations or of manipulations of statistical distributions

in the information theoretic or information geometric sense.

Functional analysis.

So let's begin as tradition in our year 2023 with a brief quote from ChatGPT.

How is functional analysis helpful for us who are learning and applying active inference?

I'll just read the last section.

Overall, functional analysis provides a powerful mathematical toolset for understanding and analyzing the principles of active inference.

and one can read more.

Functional analysis is the study of functions.

And here we see one way of showing what a function does in terms of being this box f of x that takes in an input, an argument x, and outputs a result y. And also that can be understood as a mapping

between or amongst spaces.

And there's a lot more to go into.

Just wanted to bring up that in math, sometimes people have analyzed functions and their properties.

And we're interested in the properties of a special class of functions, which are probability distributions, well-behaved probability distributions.

What do we do with those probability distributions?

Well, one thing we might want to do is sample.

So the paper writes, sampling methods are critical to the efficient implementations of many methodologies.

Most modern samplers are based on Markov chain Monte Carlo methods, which includes slice samplers, piecewise deterministic Markov chains, and so on.

The original Hamiltonian Monte Carlo, or HMC, which we're gonna get to, algorithm was introduced in physics to sample distributions on gauge groups for lattice quantum chromodynamics.

It combined two approaches that emerged in previous decades, namely the Metropolis-Hastings algorithm and the Hamiltonian formulation of molecular dynamics.

So the way that they state it in the beginning of section three of the paper is,

The purpose of sampling methods is to efficiently draw samples from a given target distribution, rho, or more commonly to calculate expectations with respect to rho by equation 33 and other.

They write, modern Hamiltonian Monte Carlo, which again we're going to get to in a second, relies heavily on symplectic integrators, another keyword, to simulate a deterministic dynamic responsible for generating distant moves between samples and thus reduce their correlation, while at the same time preserving important geometric properties.

Now, we want to make our samples as informative as possible.

If we're just drawing samples and it's the same sample again and again, one can imagine that it is uninformative.

Conversely, one can imagine a situation where samples are informative.

And that is brought into practice by making sure that the samples are minimally correlated with each other, as noted above.

And this also has analogy in cryptography.

So if we're sampling something where the successive samples are 99% correlated, you can imagine that you're oversampling.

So you're taking 10,000 frames per second of a YouTube video with 30 frames per second.

I think I'm streaming at 25 frames per second right now.

Conversely, if one were to sample every 10 minutes, then their correlation structure would also not capture the video's 25 frames per second.

So in order to generate good and large moves in the space, we would need perfect knowledge of what the space is or how it's generated, which is sometimes knowable in cryptography.

However, empirically, we need to use heuristics and statistical approximations.

So this is the applied statistics angle on the analytical relationships that we're going to be talking about.

And it's going to come up again and again, this tension between analytical formulations, what's true in principle, and the statistical or numerical or computational applications.

To Hamiltonian Monte Carlo from Wikipedia.

The Hamiltonian Monte Carlo algorithm, originally known as hybrid Monte Carlo, still HMC, is a Markov chain Monte Carlo method for obtaining a sequence of random samples which converge to being distributed according to a target probability distribution for which direct sampling is difficult.

So just a few first pass reflections.

We're sampling, which is to say Monte Carlo, and Monte Carlo is hearkening back to a different time when gambling was done in Monte Carlo and the state space of all the hands of poker were too vast to compute analytically or equations were not known.

And so what one used to do and still does

is draw samples and say, well, I don't know if this is the final estimate on how likely a royal flush is, but from the two billion hands that I sampled, there were two royal flushes.

And so it's one in a billion based upon my sample.

So we're sampling because this is a complex distribution for which an analytical solution is not possible, not known, not relevant, or not tractable computationally.

So there's various situations where we want to sample and we don't necessarily have an analytical solution.

When we're sampling from a converged stationarity of a statistical landscape, from a numerical perspective, we understand that landscape well enough to do inference.

So imagine that we're sampling height estimates from GPS locations in the X and the Y coordinates, and we're getting a Z value in our sample.

At some point, we can say we're unsurprised by new samples.

At that point, we understand the landscape well enough to do statistical inference.

And just like the frames per second in the previous example, one can imagine if the landscape is changing on the spatial scale of one mile and you're sampling every millimeter, you might be oversampling.

If you were sampling every league, maybe you'd be too coarse grained with your samples.

And so one reason that comes up in science all the time for using sampling is that the search space might be just too large.

For example, the combinatorics of a phylogenetic tree with a thousand species might be vast.

And so it might be better to say something like, well, 99% of the million samples we drew were consistent with X because an analytical solution or a brute force search, neither might be possible.

And the Wikipedia wrote, this sequence of samples can be used to estimate integrals with respect to the target distribution.

So sometimes we're sampling not from the target distribution itself, but rather from a distribution that is related or transformed from the target distribution.

This is because sampling directly from the target distribution might be difficult or less than perfectly informative.

So we might, for example,

Hint, hint, sample from the derivative of the target, which might help us identify points where, for example, the derivative is flat in all directions.

In the case that we're sampling from a derived landscape of the target distribution, this is called a symplectic integrator.

So let's look at what a symplectic integrator is.

So first, backing up to what is differential geometry.

Differential geometry is a mathematical discipline that studies the geometry of smooth shapes and smooth spaces, otherwise known as smooth manifolds.

Symplectic geometry is a branch of differential geometry and differential topology that studies symplectic manifolds, that is, differentiable manifolds equipped with a closed non-degenerate two-form.

Symplectic geometry has its origins in the Hamiltonian formulation of classical mechanics, where the phase space of certain classical systems takes on the structure of a symplectic manifold.

So, the keyword density is high, these topics are all very closely linked.

From the paper, it has been known for a long time that the class of symplectic integrators is the preferred choice for simulating physical systems, only the finest for our simulations of physical systems.

These discretization techniques are designed to preserve the underlying symplectic geometry of Hamiltonian systems.

And they also form the basis of Hamiltonian Monte Carlo or hybrid Monte Carlo methods.

We're gonna come back to this tension again and again, which is what good

is a in principle smooth and differentiable analytical formulation if our discretization scheme the way that we actually implement the steps on a computer are inferior we might ruin all those nice properties that we work to get about the analytical form we might just throw those out when we discretize it coarsely or inappropriately

So what is a symplectic integrator?

It's a numerical integration scheme for Hamiltonian systems.

Symplectic integrators form the subclass of geometric integrators, which by definition are canonical transformations.

They're transformations we know a lot about.

Symplectic integrator schemes

are referring to, again, just broadly, both the analytical formulations and the software packages and approaches that we can use to implement those formalisms.

And these integration schemes are useful across difficult estimation problems, which is why they're used to study nonlinear dynamics, molecular dynamics, like protein simulation,

discrete element methods, accelerator physics, plasma physics, quantum physics, celestial mechanics.

The time evolution of Hamilton's equation is a symplectomorphism, meaning that it conserves the symplectic two-form.

A numerical scheme is a symplectic integrator if it also conserves this two-form.

Let's hear more if this is not correct or not complete.

But the one form is like the differentiation and integration of a line.

The two form is like the differentiation and integration of a surface.

And the three form is like integration or differentiation of a volume.

And John Denker has a great blog post on the basic properties of a symplectic integrator.

Some really interesting quotes that are good to just keep in mind when we're hearing about all these avenues we're going to be exploring in the paper.

A symplectic integrator conserves the area in phase space delimited by an ensemble of systems.

For a periodic system, there is an area that is conserved, namely the area inside the phase space orbit of the system.

The main reason for mentioning the orbit is to make the point that there are lots of different things with dimensions of area in face space.

Some are conserved and some not.

Some are interesting and some not.

You have to specify which sort of area you are talking about.

So another theme that's going to arise is we're interested in very well-behaved outcomes,

from a very specific or constrained, still broad, but definitely constrained set of distributions.

For example, distributions that can be interpreted in an information theory sense as probability distributions.

What was that mention of the Hamiltonian and the shadow Hamiltonian?

So the author's right.

For symplectic brackets, which is the kind of operation that reflects the symplectic integrator, the existence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, e.g.

for variational integrators, which use variational inference, which use a discrete version of Hamilton's principle of least action.

And we've heard it before, that free energy principle is a principle of least action for inference and action.

and more generally for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by some technicalities that we can learn about.

As we shall see, such geometric integrators can be constructed by leveraging the shadow Hamiltonian property of symplectic methods on higher dimensional conservative Hamiltonian systems.

In short, as a consequence of having shadow Hamiltonian,

such geometric integrators are able to reproduce all the relevant properties of the continuum system these arguments are completely general so even before one knows what the shadow hamiltonian necessarily is the authors are letting us know that if we can discretize the shadow hamiltonian appropriately

Through symplectic integration, we can provide a discretization scheme that ends up staying consistent and compatible and all down the middle with the analytical properties that we worked so hard to get.

And there were some different papers that were cited and some that were found during research.

This paper, Timestep and Shadow Hamiltonian in Molecular Dynamics Simulations by Kim

demonstrates symplectic integrators on the simplest possible system, a simple harmonic oscillator.

So if one wants to go to a technical example, but one that builds intuition, that's a great place to start.

And from the Wikipedia article on energy drift, these integrators do not in fact reproduce the actual Hamiltonian mechanics of the system.

Instead, they reproduce a closely related shadow Hamiltonian whose value they conserved many orders of magnitude more closely.

The accuracy of the energy conservation for the true Hamiltonian is dependent on the time step.

So if we can, for example, just metaphorically speaking, capture a high resolution image of the shadow, then we'll be able to know something, do something with the actual.

Some technical details on Stein's method.

which is a general method in probability theory to obtain bounds on the distance between two probability distributions with respect to a probability metric.

And Stein's method is used in the context of Stein operators and Stein class, which we're going to get to in the subsequent discussions.

Reproducing kernel.

Our discussion of inference builds upon the theory of Hilbertian subspaces and in particular, reproducing kernels.

These inference schemes rely on the continuity of linear functionals such as probability and Schwartz distributions over a class of functions to geometrize

the analysis of integral probability metrics, which measure the worst case integration error.

We shall explain how maximum mean, kernelized, and score matching discrepancies arise naturally from topological considerations.

So we've added some background links here to the slide, but it'll be a great place to start with authors.

What is a reproducing kernel?

And how is it used in this paper?

Onwards through the keywords we go.

Accelerated optimization and variational inference.

So here's a 2016 paper from Michael Jordan and other authors and some quotes from the abstract.

Accelerated methods achieve faster convergent rates than gradient methods and indeed under certain conditions they achieve optimal rates.

However, accelerated methods are not descent methods and remain a conceptual mystery.

We propose a variational continuous time framework for understanding accelerated methods.

We provide a systematic methodology for converting accelerated higher order methods from continuous time to discrete time.

Our work illuminates a class of dynamics that may be useful for designing better algorithms for optimization.

And this is going to be, again, something awesome to discuss.

We've talked about on many streams variational inference in the context of gradient-based analysis.

optimization for example the ball is rolling to the bottom of the bowl bottom of the hill and what you can do is you can take the gradient div grab curl and all that of where the ball is and just go downhill and if you designed the hill to be the right shape

or you're using a chosen family of variational estimators that are the right shape, are a good shape, well-behaved shape, then you follow the gradient on down.

And that's how we've talked about variational inference.

Accelerated optimization is going to be accelerated from that.

And so it's fun to think about, and it'll be great to talk about.

Dissipative systems.

The vast majority of statistics and machine learning applications involve solving optimization problems.

The author is right.

Accelerated gradient based methods and several variations thereof have become workhorses in these fields.

Recently, there has been great interest in studying such methods from a continuous time limiting perspective.

Such methods can be seen as first order integrators to a classical Hamiltonian system with dissipation.

This raises the question,

on how to discretize the system such that important properties are preserved, assuming the system has fast conversions to critical point and desirable stability properties.

Originally, such a theory of geometric integration was developed with conservative systems in mind while optimization,

In optimization, the associated system is naturally a dissipative one.

More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of convergence and stability, which are the main properties of interest for optimization.

So, from the Wikipedia on Hamiltonian systems.

example of a time independent hamiltonian system is the harmonic oscillator so it's just a frictionless spring oscillating around the hamiltonian of the system does not depend on time and thus the energy of the system is conserved and so we can say that that is a conservative hamiltonian if the hamiltonian decays in time it is dissipative

recently the advances of these authors and others have extended our ability to make discretizations of dissipative hamiltonians that respect the rates of convergence and stability which are the main properties of interest for optimization

and we've talked about hamiltonians on live stream number 49 with dalton on bayesian mechanics so pretty cool what is conservative what is dissipative we'll explore as the final keyword here

i just wanted to start with a blank slide on active inference and see what happens in 52.1 so consider the slide to be blank and for the authors and discussants looking forward to hearing how are we approaching active inference given everything that we've just loaded onto the table and everywhere that we're about to go with the paper let's get into it section one introduction so

the authors begin with differential geometry

a fundamental role in applied mathematics statistics and computer science including various domains citations 1 through 22 which can be shown here various citations from this well-cited paper the geometric study of statistical models has had many successes ranging from statistical inference where it was used to prove the optimality of maximum likelihood estimator

to the construction of the category of mathematical statistics generated by Markov morphisms.

So what are Markov morphisms from NCAT lab?

The formalism of Markov categories can be thought of as a way to express certain aspects of probability and statistics synthetically.

In other words, it consists of structure and axioms, which one can think of as fundamental in probability and statistics,

which one can use to prove theorems without having to use measure theory directly.

Intuitively, for the purposes of probability, a Markov category can be seen as a category where morphisms behave like random functions or Markov kernels, hence the name.

So just some red text speculation.

It was a big success and a breakthrough to think about the category from a formal category theoretic perspective

to think about the category of statistical distributions from an information geometric perspective.

This means that we can understand many analytical properties and transformations of statistical distributions, hashtag Bayesian, and develop general methods that work for that whole category, like accelerated optimization.

And the applications are vast.

Originally, such a theory of geometric integration was developed with conservative systems in mind.

While in optimization, the associated system is naturally a dissipative one.

Nevertheless, symplectic integrators were exploited in this context.

More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of convergence and stability, which are the main properties of interest for optimization.

So, citation nine, what is it about?

2021 paper on dissipative symplectic integration with applications to gradient-based optimization by some of the authors.

And it's interesting to pull out some quotes from the abstract and just hear a little bit about what they're up to.

Recently, continuous time dynamical systems have proved useful in providing conceptual and quantitative insights into gradient-based optimization, widely used in modern machine learning and statistics.

An important question that arises in this line of work is how to discretize the system in such a way that its stability and rates of convergence are preserved.

So this is the sampling problem.

In continuous time, the math is continuous and nice.

However, the sampling problem, which actually comes into play when we use modern computational methods that are implemented on discrete time, space, and constraints,

What happens when we use unconventional computing?

That's an interesting discussion question, but in practice today, the discretization approach is going to matter a lot.

In this paper, we propose a geometric framework in which such discretizations can be realized systematically, enabling the derivation of rate matching algorithms without the need for a discrete convergence analysis.

More specifically, we show that a generalization of symplectic integrators to non-conservative and in particular dissipative Hamiltonian systems is able to preserve rates of convergence up to a controlled error.

So this is the advance of this paper, that within a controlled error, good enough, they can model conservative and non-conservative, even dissipative Hamiltonian systems.

Moreover, such methods preserve a shadow Hamiltonian despite the absence of a conservation law, extending key results of symplectic integrators to non-conservative cases.

Our arguments rely on a combination of backwards error analysis with fundamental results from symplectic geometry.

We stress that although the original motivation for this work was the application of optimization, where dissipative systems play a natural role, they are fully general and not only provide a differential geometric analysis for dissipative Hamiltonian systems, but also substantially extend the theory of structure-preserving integration.

So, they made this generalization in the case of optimization.

However, it's exciting that it extends even deeper into the math and the symmetry.

Those are a few highlights from section one.

On to section two, accelerated optimization.

Vroom, vroom, here we go.

So there are going to be a lot of slides from sections two through five.

And to keep the video a reasonable length, I'm going to just highlight a few pieces, not even necessarily what I intended to highlight, but I'm just going to go for it so we can get through it.

Section two, we shall be concerned with a problem of optimization of a function, finding a point that maximizes V of Q or minimizes negative V of Q over a smooth manifold M, R in the real numbers.

Many algorithms and optimization are given as a sequence of finite differences.

Even when these algorithms are seen

discretizations of a continuum system whose behavior is presumably understood it is well known that most discretizations break important properties of a system and they continue to write the analysis of such finite difference iterations is usually challenging relying on painstaking algebra to obtain theoretical guarantees such as conversions to a critical point stability and rates of convergence to a critical point

Even when these algorithms are seen as discretizations of a continuum system whose behavior is presumably understood, it is well known that most discretizations break important properties of the system.

It can't be highlighted enough.

When we implement optimization algorithms on modern computers, we have to make discretizations in space and time and in practice.

So even if the analytical properties of the distribution are totally fire,

When we try to solve and fit models to empirical data, unless we also match that elegance and power with a discretization approach, we end up failing to realize analytical promises.

2.1 The Principle of Geometric Integration Fortunately, the authors write, here comes into play one of the most fundamental ideas of geometric integrations.

Many numerical integrators are very close, exponentially in the step size, to a smooth dynamics generated by a shadow vector field.

A little whisper of a shadow Hamiltonian.

and the shadow vector field is a perturbation of the original vector field.

This allows us to analyze the discrete trajectory implemented by the algorithm using powerful tools from dynamical systems and differential geometry, which are a priori reserved to smooth systems.

So we're going to be able to discretize our cake and have it be smooth too.

Crucially, while numerical integrators typically diverge significantly from the dynamics they aim to simulate, geometric integrators respect the main properties of a system.

In the context of optimization, this means respecting stability and rates of convergence.

Seems like a good idea to respect the main properties of the system.

This was first demonstrated in 9 and further extended in 10.

Our discussion will be based on these work.

So, citation 9, Franca, Jordan, and Vidal, mentioned previously.

And citation 10, Franca, Barp, Girolami, and Jordan, Optimization on Manifolds, A Symplectic Approach, author's previous work.

So, big development.

We can use geometric rather than coarse numerical integrators, so our sampling-based optimization schemes, including their discretization, respect the main properties of the system.

Sounds great.

Section 2.2, conservative flows and symplectic integrators.

As a stepping stone, we first discussed the construction of suitable conservative flows.

These are very well studied and they're very intuitive, more intuitive.

To construct vector fields along the derivative of x, which is the function of flows along which some function is constant,

we shall need brackets.

Geometrically, these are morphisms, x star to x, also known as contravariant tensors of rank two in physics.

So calling back the two form and the brackets.

Importantly, vector fields that preserve F correspond to bracket vector fields in which B is anti-symmetric.

Constructing conservative flows is thus straightforward.

Unfortunately, it is a rather more challenging task to construct efficient discretizations that retain this property.

Most well-known procedures, namely discrete gradient and projection methods, only give rise to integrators that require solving implicit equations at every step, and they may break other important properties of the system.

Citation 96.

This is the issue.

Again, no matter how nicely behaved, in principle, our analytical underlying smooth differentiable function is, if we discretize it and we chop it up in a way that's inappropriate, we don't respect the properties of the system.

We don't end up with being able to realize those analytical promises.

Indeed, in practice, the Hamiltonian usually decomposes into a potential energy associated to position and independent of momentum, and a kinetic energy associated to momentum and invariant under position changes, both generating tractable flows.

Thanks to this decomposition, we are able to construct numerical methods through splitting the vector field.

few ideas coming together here recalling some of our generalized coordinate based approaches to non-equilibrium steady states in bayesian mechanics and also decomposition of complex functions

Note also that for symplectic brackets, the existence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, e.g.

for variational integrators, which use a discrete version of Hamiltonian's, Hamilton's principle of least action, and for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by the first Dirham cohomology of face space.

So for discussion with authors and you all,

what are the splitting integrators what is split from what and why what is the bracket notation or operation and what is a Poisson bracket what is a Poisson system section 2 3 rate matching integrators for smooth optimization so having obtained a vast family of smooth Dynamics and integrators that closely preserve F we can now apply these ideas to optimization

What is being set up in this section and why?

In equation seven, we see that the damping coefficient gamma of T being greater than zero reflects the dissipative component or the second term in that right-hand side of equation seven.

So damping coefficient controls the strength of the dissipation.

One can imagine that if the damping coefficient is zero, the dissipative side zeroes out and B has conservative behavior.

and so on so what is being set up here and why dot dot dot dot dot the existence of such a lyopin of function described above implies that trajectory starting in the neighborhood of q star will converge to q star so it's like a ball rolling to the bottom of a hill just like we've always wanted

In other words, the above system provably solves the optimization problem, minimum of V on Q, such that Q is in the D dimensional real.

Punchline, we're setting up a system with good smooth optimization characteristics, ball rolling to the bottom of a smooth hill, that is also on the path or using the notation of or prepared to make the

Transformation two, a tractable discretization approach.

So some more on the damping coefficient.

They bring up some common choices for the damping coefficient and big O notation describes on the order of which something occurs.

For example, linearly order of the variable or sub or super linearly as a function of time or data points.

So in computational complexity analysis, people are often interested as I double the amount of data I'm analyzing,

Does that make the algorithm take twice as long?

That's linear computational complexity.

Does it take four times as long?

Does it take the same amount of time?

And so on.

And it'll be great to talk about the intuitions and implications and generalizations about how the damping coefficient influences the computational complexity estimates for convergence in different settings.

They write, the conservative system from equation 16 reduces precisely the original dissipative system, 13.

The second equation in 16 reproduces 14 and the remaining equations are equivalent to the equations of motion associated to 13, which in turn are equivalent to eight as previously noted.

Formally,

What we have done is to embed the original dissipative system with phase space R2D, so real with 2D dimensions, into a higher dimensional conservative system with phase space R2D plus two.

The dissipative dynamics thus lies on a hyper surface of constant energy k equals zero in high dimensions.

The reason for doing this procedure called symplectification is purely theoretical.

Oh come on, it's not purely theoretical.

Since the theory of symplectic integrators only accounts for conservative systems, we can now extend this theory to dissipative systems settings by applying a symplectic integrator to 13 and then fixing the relevant coordinates, 17, in the resulting methods.

Geometrically, this corresponds to integrating the time flow exactly.

going to talk about a relationship between time and dissipative systems after all it's dissipative systems that are dissipating in time and so discretization of time plays a role in the appropriate discretization of a dissipative hamiltonian

And in citation 9, previously raised, such a procedure was defined under the name of presymplectic integrators, and these connections hold not only for the specific example above, but also for general non-conservative Hamiltonian systems.

So, what is happening here?

What's an intuition for conservative and dissipative systems, and how have the recent works of Franke et al.

expanded what is possible?

are now ready to explain why this approach is suitable to construct practical optimization methods the coordinate t sub k becomes simply the time discretization which is exact and so is u sub k since it is a function of time alone importantly u does not couple to any of the other degrees of freedom so it is irrelevant whether we have access to u or not because we're looking to solve a function of t

17a can be substituted in 15 to get 18 and you can replace 13 to get 19. we'll talk more about it with the authors therefore the known rates equation 11 for the continuum system are nearly preserved and so would be any rates of more general time dependent dissipative hamiltonian systems

Let us now present an explicit algorithm to solve the optimization problem.

This is all happening as a consequence of having a shadow Hamiltonian such that geometric integrators are able to reproduce all the relevant properties of the continuum system.

Section 2.4.

Manifold and constrained optimization.

Following 10, we briefly mentioned how the previous approach can be extended in great generality to an optimization problem.

So, equation 21, we present our minimization problem, V of Q, variational distribution on Q, citation 10, Franke et al.

They write, there are essentially two ways to solve this problem through a dissipative Hamiltonian approach.

One is to simulate a Hamiltonian dynamics on T star M by incorporating the metric of M in the kinetic moving part of the Hamiltonian.

Another is to consider a Hamiltonian dynamics on Rn and embed M into Rn by imposing several constraints.

The first approach uses a Lie group.

We'll talk more about it.

An example of a second approach

One can constrain the integrator on Rn to define a symplectic integrator on M via the discrete constrained variational approach by using some techniques.

above method consists in a dissipative generalization of the well-known rattle integrator from molecular dynamics citations 100 through 103 used in computational biology section 2.5 gradient flow as a high friction limit let us provide some intuition why simulating second order systems is expected to yield faster algorithms as an illustration consider figure one on the left we'll look at in a second

where a particle immersed in a fluid falls under the influence of a potential force negative delta sub q on v and q so partial differential with respect to q of v on q that plays a role of gravity and is constrained to move on a surface so ball rolling down the hill we weren't joking about it

In the underdamped case, the particle is underwater, which is not so viscous, so it has acceleration and moves fast.

It may even oscillate.

In the overdamped case, the particle is in a highly viscous fluid, such as honey, and the drag force, that damping coefficient, gamma, is comparable or stronger to the gradient.

Thus, the particle moves slowly since it cannot accelerate.

During the same elapsed time, delta T, an accelerated particle would travel a longer distance.

Here's figure one.

So here, why simulating second order systems yields accelerated methods.

So on the left, constrained particle falling in fluids of differential viscosity.

Here on the left, we have the particle falling through honey.

It's slowly making its way to the bottom of the bowl, but it never really,

builds speed its terminal velocity is being dominated by the viscosity of the fluid whereas this ball falling through water or falling through air is with respect to the honey dampened bowl this is like an accelerated optimization and they present some numerical results that help us bolster that intuition

Really fun and embodied way to think about optimization.

We've talked about that ball rolling to the bottom of the hill and what the ball does when it hits a small bump, but we have not talked about what media the ball is floating through and the media is the message and the fish doesn't know what it's swimming through.

Section 2.6, optimization in the space of probability measures.

It'll be great to explore more because we're gonna see free energy calculations on the stationary density, KL divergence and more.

For now, we can just say all those optimization techniques we were bringing up earlier, we're gonna be able to do them on information geometric spaces that correspond to probability measures that are well-behaved.

not all distributions are probability measures or probability distributions.

Just because you draw a line doesn't mean you can use that in part of your variational inference scheme for Bayesian statistics.

That's section two.

On to section three, Hamiltonian-based accelerated sampling.

So we talked about the Hamiltonian, conservative and dissipative, and the shadow Hamiltonian, which is going to be orders of magnitude better to approximate.

We talked about gradient-based methods and how accelerated sampling is going to help us accelerate those sampling techniques.

And now Section 3 brings it together with Hamiltonian-based accelerated sampling.

Section 3.

The purpose of sampling methods is to efficiently draw samples from a target distribution row, or more commonly to calculate expectations with respect to row.

And from the end of the paragraph.

efficient sampling scheme is one that minimizes the variance of the monte carlo markov chain estimator monte carlo that means that we're sampling hands from the poker table and markov chain which means that the past only influences the future through the present it's a quote memoryless process

other words fewer samples will be needed to obtain a good estimate intuitively good samplers are markov chains that converge as fast as possible to the target distribution it's like if you laid down a jump rope on a mountain if that jump rope converged quickly to the topography of the mountain it would have been a fast converging jump rope and we're doing something like that but with sampling from the jump rope

3.1, optimizing diffusion processes for sampling.

As many MCMC methods are based on discretizing continuous time stochastic processes, the analysis of continuous time processes is informative of the properties of efficient samplers.

And diffusion processes possess a rich geometric theory, extending that of vector fields and have been widely studied in the context of sampling.

lot of very fascinating ideas here stratanovich stochastic differential equations are going to come into play equation 34 more technical details on diffusion the calculus of twisted differential forms allows us to have a measure-informed calculus on multi-vector fields

What are twisted differential forms?

I'm looking forward to that conversation.

What is the untwisted differential form?

Have we been twisted all along or not?

and they go on to write.

A fundamental criterion for efficient sampling is non-reversibility.

A process is non-reversible if it is statistically distinguishable from its time reversal when initialized at the target distribution.

So once the jump rope is lying flat on the mountain, it's like in a position of reversibility.

Mixing metaphors at will here.

Measure-preserving diffusions are non-reversible precisely when some conditions are met.

Intuitively non-reversible processes backtrack less often and thus furnish more diverse samples.

So if you're that ball rolling on the mountain, you want to just plow forward.

And whether you're going up or downhill, you want to be sampling and making moves, but you don't want to be going back and forth because at that point, it's not an efficient sampling path.

It's well known that removing non-reversibility worsens the spectral gap and the asymptotic variance of the MCMC estimator.

So, time discretization, spectral gap.

In diffusions with linear coefficients, one can construct the optimal non-reversible matrix A to optimize the spectral gap.

we can have well-behaved parameters that help us get at optimal discretization schemes so that that shadow hamiltonian can be preserved when we do discretize it during optimization so that we can respect the key properties of the system however there are no generic guidelines on how to optimize non-reversibility in arbitrary diffusions

This suggests a two-step strategy to construct efficient samplers.

One, optimize reversible distributions, diffusions.

And two, add a non-reversible perturbation.

Equation 36, citation 125.

Diffusions or manifolds, diffusion on manifolds are reversible when certain things are the case and they're not when other things are the case.

Equation 37, we got a triangle pointing down and a triangle pointing up.

We're gonna talk more about it.

Underdamped Langvin dynamics combine all the desirable properties of an efficient sampler.

It is reversible, has degenerate noise, and achieves accelerated conversions to the target density.

So all of this groundwork

is helping us get to an analytical formalization that's going to have the right kind of properties so we can do that discretization right on the shadow hamiltonian so we can get the accelerated optimization done 3.2 hamiltonian monte carlo a challenging task consists of constructing efficient sampling algorithms with strong theoretical guarantees

We now discuss an important family of well-studied methods known as Hamiltonian Monte Carlo, HMC, which can be implemented on any manifold for any smooth, fully supported target measure that is known up to a normalizing constant.

Some of these methods can be seen as an appropriate geometric integration of the underdamped Langvin diffusion

diffusing down that hill but in simpler it is in general simpler to view them as combining a geometrically integrated deterministic dynamics with a simple stochastic process that ensures ergodicity equation 38. if we interpret the negative log density v of q as a potential energy

e.g.

a function depending on position Q, one can then plug in the potential within Newton's equation to obtain a deterministic proposal that is well-defined on any manifold, as soon as the acceleration and derivative operators have been replaced by their curved analogs.

So, what pulls the ball down the hill?

Or, if you're an instrumentalist, what force describes the movement of the ball moving down the hill?

Gravity.

I think there's a John Mayer song about it.

Surprises like gravity.

It can be understood as the minimizing force that pulls the ball down the hill or describes the movement of the ball on its path of least action down the hill.

How cool.

More discussion around 38.

Bringing these ingredients together

We thus have the following HMC algorithm.

Given dot, dot, dot, we're going to compute a function according to one, a heat bath, two, shadow Hamiltonian dynamics, and three, metropolis correction.

That's the recipe.

We're bringing those ingredients together.

The above rudimentary HMC method

was proposed for simulations in lattice quantum chromodynamics, with M being the special unitary group and used a Hamiltonian dynamics ingeniously constructed from the Maurer-Cartan frame to compute the partition function of discretized gauge theories.

This method has later been applied in molecular dynamics and statistics.

There are three critical properties underpinning the success of HMC in practice.

The first two are the preservation of the reference measure and the existence of a conserved shadow Hamiltonian for the numerical method.

The third critical property is the existence of splittings methods for which all the composing flows are either tractable or have adequate approximations, namely the geodesic integrators.

so geodesic geodesic methods buckminster fuller tensegrity synergetics or however you think about geodesics as paths in curved space that is the kind of approximation and splitting we're gaining access to with appropriate discretizations on those balls rolling to the bottom of the hill

Let us also briefly mention some useful upgrades that have been proposed in recent years.

So, for those who it's been a few years since you checked in with this domain, this is a great place to look.

First, you can grant the method extra integration steps when the proposal is rejected.

Or,

You can use criterion that aim to ensure the motion is long enough to avoid random walks, but short enough that we do not waste computational effort, such as the no U-turn sampler, which is integrated in a lot of software packages.

Second,

Modern HMC methods bypass this issue of slow convergence by replacing the heat bath with an Ornstein-Ohlbeck process, which ensures the overall algorithm is irreversible.

An OU process is like a Brownian diffusion with a linear regression.

So it vibrates and has volatility, and it has a trend line that's linear.

Third, many modifications of the rudimentary HMC algorithm only provide improvements when the acceptance rate is sufficiently high.

A third class of upgrades improve the acceptance rate by using the fact that the shadow Hamiltonian is exactly preserved by the integrator.

Big point of the whole paper.

Finally, the metropolis step can be replaced with a multinomial correction that uses the entire numerical trajectory accepting a given point along it according to the degree by which it distorts the target measure.

Some path-based inference methods with some caveats.

All right, on to section four, statistical inference with kernel-based discrepancies.

So section four, the problem of parameter inference consists of estimating an element theta star within big theta using a sequence of random functions or estimators theta hat n of omega onto big theta equations.

Conveniently, reproducing kernel Hilbert spaces are precisely Hilbert spaces over which the Dirac distributions, which is

a statistical distribution type where it's like a spike at one location and zero elsewhere, act continuously.

So whether we're talking about events or spiking neurons, this is a really good property.

And more generally, the probability distributions that act continuously by integration on an RKHS, H, are exactly those for which all elements of H are integrable.

So analytically, we have that nice property.

Denoting by fancy p sub h the set of such probability measures such that dot dot dot, we can define the maximum mean discrepancy, or MMD, as such.

More definitions.

A practical expression for the squared MMD.

So just like in linear regression, we're often interested in the sum of squared errors, here we're interested in the sum of squared MMD.

4.1, topological methods for MMDs.

Just remember, MMDs, maximum mean discrepancy.

A key feature of RKHS is that they are Hilbertian subspaces.

More details about embedding in Hilbert spaces and subspaces later.

The geometric description of RKHS and MMD allows us to swiftly apply topological methods in their analysis.

There are reasons.

This reduces the matter to a topological question.

Instead of defining T star to be the set of probability measures, it is commonly done to define statistical manifolds

It's desirable to embed fancy P within a more structured space, such as the space of finite radon measures, which enables the method to learn the target function independently of the data generating distribution.

What is a radon measure?

We're going to find out.

However, it enables the method to learn the target function independently of the data generating distribution.

Sounds pretty important if we want that tail of two densities.

MMD can discriminate distributions.

That makes it useful to do other things.

Let's find out more.

Section 4.2.

Smooth measures and KSDs.

So,

more information on MMDs and about making them computationally tractable.

And citations to Stein's method are provided for 195, which is the 1972 Stein paper, and 196, which is a paper written by some of the authors about Stein's method meets statistics, a review of some recent developments from 2021.

And I thought it'd be fun to look at some figures from this paper and look at the abstract.

So I'll put the figures up above.

Stein's method compares probability distributions through the study of a class of linear operators called Stein operators.

While mainly studied in probability and used to underpin theoretical statistics, Stein's method has led to significant advances in computational statistics.

topics that are discussed in that review include tools to benchmark and compare sampling methods such as approximate markov chain monte carlo deterministic alternatives to sampling methods control variant techniques parameter estimations and goodness of fit testing and from the paper they write the list of results given in this paper are but a mere sample

of the ongoing activity in this newly established area of research at the boundary between probability, functional analysis, data science, and computational statistics.

For instance, Stein's method has been used for designing sampling-based algorithms for non-convex optimization, learning semi-parametric multi-index models in high dimensions, and in Bayesian statistics, Stein discrepancies have been used as variational objectives for posterior optimization, which is what we're all about.

Free energy functionals?

using variational inference as objectives for posterior approximation of inference and action, free energy principle.

And these images have some nice intuitions too.

Here we see, depending on our step size, we're getting different sampling.

When our step size is to the negative fifth power, we're oversampling one part of the space.

Then as we make the step size larger, our samples are drawn from different distributions.

So that's selecting the step size epsilon for a stochastic gradient Langvin dynamic.

And also from the Anastasio paper.

Here's another fun way to see that.

Here we have that bowl that we're rolling the ball to the bottom of and the Stein points and the Stein thinning, which is helping us understand maybe how starting the ball in different locations or critical locations accelerates what we're doing with optimization.

So cool to talk about.

4.2.1, the canonical Stein operator and Poincar√© duality.

There are two fundamental theorems that help us understand the integral differential geometry of the manifold, Durham's theorem and the Poincar√© duality.

The former, Durham's theorem, relates the topology of the manifold to information on the solutions of differential equations defined over the manifold.

The latter, which contains the fundamental theorem of calculus, describes the properties of the integral pairing, alpha-beta, of differential forms,

which include the pairing of test functions with smooth measures.

Let's learn more about it.

4.2.2, kernel Stein discrepancies and score matchings.

Technical definitions, facilitating Stein variational gradient descent.

4.3, information geometry of MMDs and natural gradient descent.

these tools have proved to be useful in a wide range of contexts more information about the divergence and how divergence can improve the speed of convergence by following the natural gradient descent okay red text speculation beware so the operator

which is a phone routing system like Ring Ring Hello Operator, is constructed in Section 4.2.2, Colonel Stein discrepancies and score matching.

You can't have a t-test without the t-distribution.

The t-test statistic is using a t-distribution.

So, a class of Fancy V

a set, fancy V, of vector fields, or more generally tensor fields, whose image F under the operator, ring, ring, has mean zero under mu.

That's going to give us this discrepancy, the SD, and the Stein variational gradient descent.

So, there's some class of V vector fields such that the mean of something about them is zero.

does this enable the central limit theorem or just statistics more broadly like the parametric and non-parametric methods that we know and love from spm does that mean zero enable us to use gaussian methods generalized gaussian methods generalized linear methods spm and does it enable the proposed smooth distribution

the one generating the shadow Hamiltonian that's getting inferred over, to be interpreted or used truly as a statistical distribution.

It's easy to forget that not all distributions are formal probability distributions.

For example, the area under the curve from zero to one of a probability distribution is one.

Something has to happen.

But not all functions have an area under the curve of one between zero and one.

so in variational bayesian inference the kind which we do in active inference in fact the kind that's done in machine learning and statistics more broadly but we're interested when action is a parameter that we're doing inference on we're concerned with the extremely well behaved properties of a certain subset of distributions so why are we doing this why is it important that

We didn't need the input data that we could construct measures that are independent of the input data because we want to enable the tail of two densities.

We want to make generative models that can be used generatively, generative AI, in the forward or the generative direction.

but we also want to be able to enable the recognition distribution, which is from empirical data to do hidden state inference.

And so just like least squares regression, in the linear modeling case, where the sum of squares above and below the regression line should be as low as possible, L2 norm, sum of squares minimization, always works, never complains,

We want that kind of ball rolling to the bottom of the hill with free energy functionals, variational and expected free energy functionals as the optimized or satisficed imperative for optimal perception, which is signal processing, signals intelligence, and control, which is control theory or action selection.

More technical details and equations 39b and 39c.

So what does it mean that the resulting Stein discrepancy can be thought of as an MMD that depends only on rho and is known as a kernel Stein discrepancy?

What is that?

As we did previously, we can remove the supremum by rewriting the above as a supremum over some unit ball of continuous linear functional.

Is this like simulating or modeling a sphere rolling on a landscape like we've been talking about?

Is the ball of optimal radius for rolling on that landscape or what is being optimized?

And what is the unit, the scaling or the scale specificity that this scale friendly sphere is scaled to?

Equation 42 and 42a.

While in the Euclidean space yields the diffusion score matching.

Citation 204.

Barp et al.

Again.

Minimum Stein discrepancy estimators from 2019.

They write, the main strength of our methodology is its flexibility, which allows us to design estimators with desirable properties for specific models at hand by carefully selecting the Stein discrepancy.

We illustrate this advantage

for several challenging problems for score matching, such as non-smooth, heavy-tailed, or light-tailed densities.

So again, just with a little bit of speculation here, and I think my camera has frozen.

It's all good.

I'll just go without the camera.

okay um one can estimate infer or optimize the zero point and hence the variance structure of the distribution this is going to enable generalized well-behaved modeling again from all of those nice perspectives that we raised earlier like gaussian central limit theorem smoothness statistical distance euclidean all of those well-behaved properties that we're looking for this is going to help us get there

We are only talking about a specific subset or type of landscape here, the one that we're doing variational inference on.

That's the map.

This is not the structure of the territory.

These well-behaved attributes of models, for better, worse, and different, through sickness and health, is because they're maps.

It is always the case that our statistically nice generative models are of a different structure or form than the generative process.

So this is actually not a criticism of quantitative modeling as a process.

In fact, this is the entire basis of quantitative modeling.

So we've approached this map territory distinction,

map territory fallacy fallacy fallacy from many angles and one will still hear things like well the structure of the statistical model is not the same as the territory or how can you say that the organism has these well-behaved properties just because the model does

And this brings a really sharp light on it, which is we're doing this insane amount of analytical groundwork so that the map has the good properties, not to constrain what the territory is.

So map territory.

all of the math we've been talking about is map we want well-behaved maps so that we can describe well-behaved and unruly territories but it isn't the case that the organism minimizes variational free energy it's the generative model that minimizes variational free energy so

a little bit of a leading discussion topic or question for you all.

Feel free to give a thought in the live chat or write a comment or join our discussions.

How is this line of basic and fundamental math research

by Barp et al., creating new models that have the operational or denotational semantics that we want for computational statistics, which is to say applied information theory.

For example, distributions that can be interpreted or used as statistical distributions, ideally directly compatible with current computational methods, SPM in MATLAB, PyMDP in Python, and Forney Lab in Julia, and so on.

Section 4.3.1.

Minimum Stein discrepancy estimators.

More technical details.

More citation to 204.

The parameters can be adjusted to achieve characteristicness, consistency, bias, robustness, and obtain central limit theorems.

See 204.

2019 paper with Barp et al.

And let's just look at a really cool image from that paper.

Figure one and figure two.

Pretty cool SD estimators, looking nice.

Section 4.3.2, likelihood-free inference with generative models.

So for many applications of interests, the densities of the model mu sub data cannot be evaluated or differentiated.

thus need density-free inference methods super convenient for bayesian statistics more technical details information tensor under appropriate choices of kernels and models one can derive theoretical guarantees such as concentration and generalization bounds consistency asymptotic normality and robustness so a little bit of a summary

we have good analytical and computational footing in certain kinds of situations or under certain constraints certainly not all constraints but definitely for some

that might enable the efficient computation, not just specification, but actual implementation of large generative models, such as described in the paper of Friston et al, 2022, designing ecosystems of intelligence from shared, sorry, designing ecosystems of intelligence from first principles.

And they use the term ecosystems of shared intelligence in that paper.

and so it's relevant to learn about the actuality and build intuition why slash how do we know well here's the one figure in that paper of friston at all the one figure in this absolutely positional paper that they have released beliefs as parameters of a probability distribution here's the sharpening of a belief as a distance belief updating

as traversing a statistical manifold, which is to say a lower dimensional projected space, and what's being shown as the parameter space of a probability distribution.

So whether we see this as gradient ascent to climb to the top of the hill, or we take the negative and we have gradient descent to the bottom of the hill, this is the figure chosen by some very well-informed authors to describe Bayesian mechanics.

and long last we get to section five adaptive agents through active inference we close as the authors write with a generic use case called active inference a general framework for describing and designing adaptive agents that unifies all aspects of behavior including perception planning and learning as processes of inference

By exploiting this geometric structure in a generic framework for designing adaptive agents, we derive the objective functional overarching decision-making and describe its information geometric structure, revealing several special cases that are established notions in statistics, cognitive science, and engineering.

So section 5.1, modeling adaptive decision-making.

First, they're going to talk about behavior agents and environments.

Behavior is going to be defined as the interaction between an agent and its environment, and the system is going to describe the agent plus its environment.

There's going to be a set of states that are partitions from a state space, big X. External states, S, are going to be unknown to the agent and constitute the environment.

States belonging to the agent, pi, particular states,

are going to be a subset of two different things.

So here is S, which is here, external states.

Note that in some other settings, S will refer to sensory states.

So look at the notation in this paper.

And pi, which in other papers is sometimes used to describe policy inference, here is gonna describe particular states.

So pi is O and A. O are the observable states, states that the agent can see but cannot directly control, sense states.

And A are the autonomous states, states that the agent sees and can directly control.

And those are going to be internal states and active states.

Figure two has a great representation.

And again, note the way that pi, s, o, and a are being used in this paper.

S is the external process, hidden state, latent state, causal inference.

O are the observations.

And the agent process consists of pi, which is the observations, and alpha, or a.

a is consisting of according to the particular partition two different kinds of states which is internal states cognitive states and action states and so we're going to compute bounds for free energy functionals with a special focus on autonomous processes because controlling our perception

is not possible and maybe not even preferable by controlling it at the level of what we observe but rather if we control our internal states which is our interpretation of the perception and our action states we'll be doing optimal perception and optimal action so those are the two sides of the coin with

inference as perception slash learning and action selection that's how we unify action and inference inactive inference is by caring about the bounding of self-surprisal just like gravity on our autonomous processes so this will be super fun to discuss more

5.1.2.

Decision-making in precise agents.

So what distinguishes people from small particles?

People are subject to classical as opposed to statistical mechanics.

Check out ACDIMF Livestream 49 for more on Bayesian mechanics and some of these distinctions with classical, statistical, quantum, and thermo.

In other words, they are precise agents with conservative dynamics.

Precise agent, definition 5.1.

An agent is precise when it evolves deterministically in a possibly stochastic environment.

More definitions.

And it's useful here to highlight Friston et al.

's recent paper, Path Integrals, Particular Kinds, and Strange Things, which provides a taxonomy of things, taxonomy of particular entities that run the gamut of sophistication from inert particles, which have no active states, active particles, which have active states,

conservative particles, which have cognitive dynamics that are described as classical, and strange particles, which represent, in this visualization, the highest level of cognitive sophistication, in which those internal hidden states themselves have this kind of what would happen if this happens counterfactual type or thinking through other minds type cognitive model structure.

doing forward and inverse inference on these great times.

5.1.2, decision-making and precise agents.

We have mathematical formalisms for decisions, preferences, and predictions, all using the partitioned variables.

5.1.2, more formalisms.

We get to expected free energy.

Expected free energy is...

Equation 41k.

Active inference is Hamilton's principle of least action on expected free energy and expresses the most likely decision where certain features are met.

Principle of least action on manifolds of inference and action subsuming or bringing together inference and action.

Active inference.

Free energy principle.


UNKNOWN:
5.1.3.


SPEAKER_00:
Active inference framework, AIF, looks like it describes agents that engage in purposeful behavior.

we can rearrange the expected free energy, EFE, in several ways, each of which reveals a fundamental trade-off that underwrites decision-making.

This allows us to relate active inference to information theoretic formalizations of decision-making that predominate in statistics, cognitive science, and engineering.

Figure three.

as hinted at in the early early keywords here on the top we have the general formalizations of active inference and one of the partitions or better to say decompositions is shown here where extrinsic value is the surprisal about observations

making sure that what we're getting in observations are what we expect slash prefer.

If the body wants to be expecting homeostatic temperature, that's what this is going to determine.

And it plays a role equivalent to reward

in reward and reinforcement learning based approaches, because we don't need to actually set a reward function, but we get something that looks like self-surprisal aligned with reward using the preference variable over observations.

A lot more to say there.

And the second term is the intrinsic value, the epistemic value, curiosity, novelty, learning, reduction of uncertainty.

And another partitioning is between risk and ambiguity.

There are four colored dots, red, tan, gray, and blue corresponding to special cases.

So under the setting where there's no ambiguity,

we realize or manifest the special case where control can be seen as inference, maximum entropy reinforcement learning, prospect theory, and KL optimal control.

This is like doing as good as you can strategically or tactically, given that there's no ambiguity in how your decisions play out.

Now, under the setting where there's no ambiguity or preferences,

a maximum entropy principle is realized.

And work with Bayesian mechanics and Dalton et al has shown that the constrained maximum entropy principle is dual to the FEP.

In the case of no extrinsic value, so no quote reward, no quote goals.

We don't use reward or goals in the active inference ontology, but borrowing those words from some long, long forgotten ontology.

In the case of no extrinsic value,

we realize the special case of maximum information gain, Bayesian experimental design.

So not to prove or disprove, but the maximally informative experiment.

That's the Bayesian scientific epistemology.

Intrinsic motivation and Bayesian surprise.

Seeking out optimally informative stimuli.

And contrast that with a setting where there's no intrinsic value.

So where there's nothing to learn,

we realize expected utility theory, Bayesian decision theory, reinforcement learning, and optimal control.

So special cases from many different fields ranging from statistical mechanics to Bayesian decision-making and statistics integrated or perhaps better generalized across in the formalisms of active inference, which

is something like a Hamilton's principle of least action on variational or expected free energy expressing the most likely inference and action under certain constraints.

Not the most rewarding, but the most likely.

5.1.3, decision-making minimizes both risk and ambiguity.

Risk, first term on the right-hand side.

Ambiguity, second term.

Minimizing ambiguity leads to a type of observational bias commonly known as the streetlight effect.

When a person loses their keys at night, they initially search for them under the streetlight because of the resulting observations.

I see my keys under the streetlight, or I do not see my keys under the streetlight, accurately disambiguate external states of affairs.

First place to look makes sense under the streetlight, and the last place you look is where you find it.

more in 5.1.3 decision making maximizes extrinsic and intrinsic value another decomposition of the formalisms of active inference maximizing information gain leads to a goal-directed form of exploration driven to answer what would happen if i did that true counterfactuals

This decision-making procedure underwrites Bayesian experimental design in statistics, which describes optimal experiments as those that maximize expected information gain.

And we've had some great discussions recently in textbook groups and in discussion hours where we contrasted that falsificationist concept of accept or reject a hypothesis

And even the idea of a scientist or a researcher setting out to accept or reject hypothesis, whether you take that positivist or you take the other path of falsificationism,

In both cases, you might end up with an informative experiment or not, but you can easily imagine cases where you end up with an uninformative experiment because you set out to confirm something you knew, or you set out to disprove something by constraining your experiment so that it was locally disproven.

In contrast, when we take into account the richness of our generative model,

We motivate this Bayesian epistemology and ultimately professional Bayesianism, where we make optimal experiments in terms of their maximum expected information gain, which requires you to also state your generative model.

Decision-making under active inference weighs the imperatives of maximizing utility and information gain, which suggests a principled solution to the exploration exploitation dilemma.

great point to jump into does active inference simply by written down by being written down on a paper resolve or transcend or dissolve exploration exploitation no does it provide a space or a framework a method an approach software packages research community that can help us address and navigate and surf on the edge of that dilemma absolutely

5.2.1 the basic active inference algorithm we're going to go through it with authors preferential inference we want to infer preferences about external and observable trajectories two for each possible sequence of past present and future actions a we're going to engage in both sides of the coin perceptual inference

What would I perceive if that happened?

And planning as inference.

Assess the action sequence by evaluating its expected free energy.

And then three, decision-making.

Execute the most likely decision at T plus one according to some distributions.

Sample from your action posterior.

Action prior, that's like your habits.

It gets sharpened or modified with expected free energy, and then you sample from the action posterior.

5.2.1 sequential decision making under uncertainty a partially observable markov decision process pomdp is a discrete time model of how actions influence external states in a pomdp each external state depends only on the current action and previous external state that's the markov property each observation depends only on the current external state and one can additionally specify a distribution of preferences over external trajectories

1 and 2 form the agent's POMDP prediction model.

2 and 3 form the agent's hidden Markov preference model, which defines the active inference agent.

A simple simulation of active inference on a POMDP is provided in Figure 4.

Implementation details on generic POMDPs are available.

For more complex simulations of sequential decision making, e.g.

involving hierarchical POMDPs, please see citations.

Figure 4.

It's a prediction model.

This is sequential decision-making in a T maze environment.

On the left, the agent's prediction model as a POMDP, represented here as a Bayesian network.

On the right, information on the T maze.

So here we have some type of Bayesian graph reflecting, hopefully,

a statistical system that's going to have all of these well-behaved properties that we've been talking about in the paper.

And then the team A's is played out.

We'll talk about it.

5.2.3, world model learning as inference.

Due to a lack of domain knowledge, it may be challenging to specify an agent's prediction and preference model.

For example, how do external states map to observations?

Should external states be represented in a discrete or continuous state space?

Also some great questions that come up every day and are addressed in chapter six, a recipe for active inference modeling by the Parr, Pizzullo and Friston 2022 textbook.

So how do we get the right priors or at least approximately adequate priors?

One way to answer the question lies in optimizing a free energy functional F, which is an evidence lower bound.

We see here variational free energy decomposed into an energy minus entropy and decomposed into a complexity minus accuracy framing.

Maximizing accuracy usually results in generative models involving universal function approximators.

Minimizing complexity results in organizing representations in sparse, compartmentalized and hierarchical generative models, where higher levels of a hierarchy encode more abstract representations and vice versa.

A computationally efficient method to compare priors by their free energy is Bayesian model reduction.

Free energy unifies inference and model selection under a single objective function.

5.2.4 Scaling Active Inference Planning for all possible courses of action is computationally expensive.

One way to finesse this is by planning only for intelligently chosen subsets of action sequences using sampling algorithms like Monte Carlo tree search.

If you're interested in that, check out the recent research on branching time active inference.

Similarly, Monte Carlo sampling finesses the expectations inherent in assessing action sequences.

A complementary approach is to assess actions instead of action sequences by conditioning all future actions to be optimal in the sense that they minimize the expected free energy.

It leads to smarter agents whose computational complexity scales linearly as opposed to exponentially in the length of action sequence.

Let's learn more about it.

Scalable inference methods can be used to make active inference more efficient.

We can train neural networks to predict the various posterior distributions, including the posterior overactions.

While training, the output of the neural network can be used as an initial conditions for variational inference, resulting in accurate inferences whose computational costs decrease as the network learns.

Additionally, optimizing free energy reduces to efficient message passing schemes when one imposes certain simplifying restrictions to the family of candidate distributions.

Lots of really exciting work in neurobiology and statistics around message passing.

Getting close to the end here.

A much cheaper implementation of active inference exists for continuous states evolving in continuous time.

Pretty cool.

Parr et al.

2022 textbook.

Chapter 7.

Discrete-time active inference generative models.

Chapter 8.

Continuous-time active inference models.

This method frames perception and decision-making as variational inference by simulating a gradient flow on free energy in an extended state space.

It can be combined with discrete active inference to operate efficiently in generative models combining discrete and continuous states.

We talked about that in Livestream 46.

Active inference does not contradict folk psychology.

Discrete active inference, decision-making active inference, DAI, and continuous time motor active inference.

being used together, also seen in the Pardal textbook.

As an example, high-dimensional observations in the continuous domain, e.g.

speech, processed through continuous active inference are converted into discrete abstract representations, e.g.

semantics, and we can even go further and say rhetorical and narrative information spaces.

Based on these representations, the agent makes high-level categorical decisions, e.g.

I want to move over there, which contextualize low-level continuous actions, e.g.

the continuous motion of a limb towards the goal location.

And that is how the paper ends.

So, closing thoughts.

If anybody wants to write a comment live, feel free to do so.

We'll be over very shortly.

what are the implications of this work we have an open space to talk about it what questions and discussion topics are you interested in please write comments before or after the dot one and the dot two so we can have those interesting discussions

and just as a little bit of a closer I'll share some stable diffusion images that were generated using the paper title as well as various other terms free energy lots of fun images a lot of good balance to some of the technical aspects that were being described in the paper was good to look at what diffusion looks like aesthetically

and so that is the end of this live stream 52.0 i hope that you found it useful that you're interested to act and for serve to learn more contribute more write a comment make it happen in your own life or in your own way with this

honestly challenging paper.

So if you've listened this far, thanks a lot for your attention and looking forward to 51, I'm sorry, 52.1 and 0.2 when we will speak with some authors and some of you.

So till next time, thanks again and see you in the dot one.
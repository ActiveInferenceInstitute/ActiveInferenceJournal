start	end	sentNum	speaker	confidence	text
13070	14146	1	A	0.99973	Hello and welcome.
14328	22740	2	A	0.99972	This is ActInf Lab Livestream number 52.0, and it is February 28, 2023.
25270	27838	3	A	0.99998	Welcome to the active inference institute.
28014	36090	4	A	0.92423	We're a participatory online institute that is communicating, learning and practicing applied active coherence.
36510	39030	5	A	1.0	You can find us at the links here on this slide.
39190	45340	6	A	0.60605	This is a recorded and an archived live stream, so please provide feedback so we can improve our work.
46030	51610	7	A	0.99985	All backgrounds and perspectives are welcome and we'll be following video etiquette for live streams.
52050	62850	8	A	0.99997	Head over active coherence.org if you want to learn more and participate in learning groups and projects at the institute, including these live streams.
64630	96774	9	A	0.99964	Well, we're here today to learn and discuss the paper Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents by Alessandro Barp Lancelot Costa Guilherme Franca Karl Friston Mark Girolami Michael Jordan Grigorios A Pavliotis this video, like all Zero videos, are is and introduction for some of the ideas.
96822	104782	10	A	0.99925	It is not a review or a final word, and as we have joked before, it's more than anything a call for help.
104916	123426	11	A	0.9994	So if you're curious about these topics, if you're knowledgeable about these topics, we would really look forward to you getting involved in the upcoming 52.1 and 52.2 discussions, as well as in an ongoing basis to help us understand some of the technical details.
123618	144220	12	A	0.99997	This is going to get technical ant times and certainly beyond the technicalities I understand, though I am looking forward to presenting them and it'll be great to have those who have backgrounds of all different types to come together and talk about this awesome work.
144830	152282	13	A	0.99997	Also, a big thanks to Ali and Candon for the assistance, technical and moral.
152426	170420	14	A	0.99996	During the preparations here in 52.0, we're going to bring up some aims and claims, read the abstract, look at the roadmap and talk about the keywords, and then we will walk through the paper with an emphasis on the figures, formalisms and key points.
170870	175346	15	A	0.99998	In the coming weeks, we're going to be discussing this paper with one or more authors.
175458	183670	16	A	0.99791	So, as usual, get in touch if you want to participate or if it's after the fact, you can still get involved.
185450	188520	17	A	0.99999	We can start with an introduction or a warm up.
188830	202686	18	A	0.94532	I'm Daniel, I'm a researcher in California, and I'm tempted to say, just totally honestly, I'm happy to get this one over with, but that sounds a little bit different than I might intended to be.
202788	212110	19	A	0.99963	I'm really excited to dive into this work, which is going to be approaching active inference from an angle that we haven't necessarily highlighted on these streams.
212190	215054	20	A	0.72637	So I think it's going to be a fascinating discussion.
215102	226278	21	A	0.99734	It's going to run the gamut, span the gap however you choose to see it, between technical sophistication and intuition, which is a great place to be.
226364	237530	22	A	0.98112	So I've been really excited and motivated to prepare, and I'm looking forward to the dot zero we're doing right now and to the upcoming discussions.
238110	248618	23	A	0.99723	So let's jump in with the big questions that might motivate one to read this paper, this kind of paper, even if they were not familiar with the authors or topics.
248794	251278	24	A	0.61	And these are just a few ways to write it up.
251364	252974	25	A	0.67	Of course, not the only ways.
253092	262638	26	A	0.99338	So, first question how can we effectively and efficiently navigate information geometric or information theoretic landscapes?
262814	275430	27	A	1.0	And how can we tackle that question from an analytical, which is to say equation based as well as a computational, which is to say real implication based perspective?
275930	286422	28	A	0.99988	Often it's really fun, intuitive, natural to talk about information theory, even for those who haven't taken the technical prerequisites.
286566	301710	29	A	1.0	And this work may help us navigate to a space where we're able to think with good intuitions about information geometry, information theory, and also make sure that those intuitions are going to be caught by our technical tools.
303330	314050	30	A	1.0	Second, how can we optimize inference in complexity models, including cases where we are doing inference on action as a parameter, also known active inference lab?
314950	329558	31	A	0.99768	Optimization and functional analysis, as we'll come to see soon has long been interested in these complex or challenging models that are right at the margin, right ant the border of what is trackable or not.
329644	352618	32	A	0.93249	Given the computational hardware that modelers have access to, for example, big data sets, high dimensional state spaces, complex dynamics and so on, how about cases where we're also interested in doing parametric inference and optimization on action or action plans as a parameter?
352794	354910	33	A	0.99996	Also known active inference lab.
355330	367010	34	A	1.0	And third, what technical underpinnings support the rigor and applicability of active inference and what special cases are revealed when we consider possibilities and constraints?
367670	372770	35	A	0.99716	These are those two branches of analytical and computational coming back again.
372920	384210	36	A	1.0	We want to make sure that when we're thinking within the active inference paradigm, the active coherence ontology is the language that we're speaking and that there's a technical rigor to what's being discussed.
384370	394118	37	A	0.99965	Additionally, we want to make sure that it's not just an analytical rigor, but it's also going to be computationally plausible tractable or maybe even preferable.
394214	395580	38	A	0.99997	That would be awesome.
396030	399390	39	A	0.99996	Those are the questions that at least appeared.
400370	413918	40	A	0.99999	We are discussing this paper Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents by Barp and Costa at all shared first authorship and it was published on active.
414094	418260	41	A	0.50044	I'll just note a few key claims from this paper.
418630	431110	42	A	0.64178	They wrote Our goal in this chapter is to discuss the emergence of natural geometries within a few important areas of statistics and applied mathematics, namely optimization, sampling, inference and adaptive agents.
431260	432550	43	A	0.98468	That's the title.
433290	442870	44	A	0.99999	We provide a conceptual introduction to the underlying ideas rather than a technical discussion highlighting connections with various Fields of mathematics and physics.
443030	449614	45	A	0.99994	Well, I can tell you that they do make connections with various Fields of mathematics and physics, though.
449652	455630	46	A	0.99983	Whether it is a concept introduction or a technical discussion is all going to be about your perspective.
457250	463134	47	A	1.0	Third, to illustrate a generic use case for the previous methodologies that are going to be discussed.
463262	471438	48	A	0.99998	We consider active inference a unifying formulation of behavior, subsuming perception, planning and learning as a process of inference.
471614	477094	49	A	0.99962	So inference on adaptive agents is not necessarily new.
477212	481062	50	A	0.99993	It goes by reinforcement learning, machine learning, and so on.
481196	492940	51	A	0.99998	However, to use those methodologies in the context of active inference a unifying formulation of behavior is something that the authors are bringing forth here.
493790	505562	52	A	1.0	And last, we describe decision making under active inference using information geometry, revealing several special cases that are established notions in statistics, cognitive science and engineering.
505706	511298	53	A	0.99993	So active inference is not just unifying, it's going to be shown to be generalized as well.
511464	542830	54	A	0.85224	Which is to say that special cases of the generalization emergence different formalisms that were known disparately across Fields onto the abstract they write in this chapter, we identify fundamental geometric structures that underlie the problems of sampling, optimization, inference and adaptive decision making.
543520	552080	55	A	0.99994	Based on this identification, we derive algorithms that exploit these geometric structures to solve these problems efficiently.
552580	566020	56	A	0.99999	We show that a wide range of geometric theories emerge naturally in these Fields, foraging from measure preserving processes, information divergences, Poisson geometry, and geometric integration.
566680	577876	57	A	0.99995	Specifically, we explain how, one, leveraging the symplectic geometry of Hamiltonian systems enable us to construct accelerated sampling and optimization methods.
578068	597528	58	A	1.0	Two, the theory of Hilbertian subspaces and Stein operators provides a general methodology to obtain robust estimators and three, preserving the information geometry of decision making yields adaptive agents that perform active inference throughout.
597704	611200	59	A	0.85211	We emphasize the rich connections between these field e G inference draws on sampling and optimization, and adaptive decision making assesses decisions by inferring their counterfactual consequences.
612100	622848	60	A	0.70818	Our exposition provides a conceptual overview of underlying ideas rather than a technical discussion, which can be found in the references herein.
622944	626890	61	A	1.0	And indeed, there are several hundred references in this paper.
627420	629092	62	A	0.99977	Let's go to the roadmap.
629236	632664	63	A	0.98681	So first we can start on the right side.
632862	646380	64	A	0.68061	Here's an active agent room doing accelerated optimization in the carpool lane, looking ahead to the almost desert like visual representation of active inference.
646960	648670	65	A	0.9943	That's where we're going to go.
649040	652268	66	A	0.99	And on the right side, it's just some cars.
652364	654960	67	A	1.0	One of them is accelerated, the other one isn't.
655460	678356	68	A	1.0	The paper begins with an introduction and then in section two gets into the topic of accelerated optimization covering the areas of principle of geometric integration, conservative flows and symplectic integrators rate matching, integrators for smooth optimization, manifold and constrained optimization, gradient flow as a high friction limit, and optimization on the space of probability measures.
678548	686228	69	A	0.99985	In section three, they turn from accelerated optimization in general to Hamiltonian based accelerated sampling.
686404	692940	70	A	1.0	The subsections of three are optimizing diffusion processes for sampling and Hamiltonian Monte Carlo.
693680	701772	71	A	0.96065	From the Hamiltonian based accelerated sampling, they turn towards doing statistical inference with kernel based discrepancies.
701916	726788	72	A	0.99992	In section four, the subsections are topological methods for Mmds smooth measures and KSDS the canonical Stein operator and point Curray duality kernel Stein discrepancies and score matching information geometry of Mmds and natural gradient descent minimum Stein discrepancy estimators likelihood coherence with generative models.
726964	735480	73	A	0.99993	Finally, in section five adaptive agents through active coherence, they're going to bring it home to active inference.
735900	747150	74	A	0.80962	Section five one modeling adaptive decision making, behavior agents and environments, decision making in precise agents, the information geometry of decision making.
747680	760400	75	A	1.0	And five two realizing adaptive agents, the basic active inference algorithm sequential decision making under uncertainty world model learning as inference and scaling active inference.
761700	770208	76	A	1.0	A quick note we have all the formalisms entered Hinton our zero teams Coda document.
770384	776900	77	A	0.99999	There are 46 numbered equations in this paper and 83 overall.
777480	783048	78	A	0.61905	We're going to bring in some, but not all of these formalisms in the dot zero video.
783214	792540	79	A	1.0	And we're ready to bring in more during the dot one one and dot two discussions with the authors just for those who are watching along.
792690	796776	80	A	0.9992	I'm going to scan through all of the formalisms from our Coda.
796888	801572	81	A	0.99	And again, for those who want to help get involved with a dot zero preparation.
801736	805504	82	A	0.99981	This is a little bit of what dot zero preparation looks like.
805702	812450	83	A	0.99678	I'm going to now scroll through the formalisms just in case anybody wants to screenshot or see them.
841260	842010	84	A	0.93614	Great.
843260	845480	85	A	0.99983	On to the keywords.
846880	866770	86	A	0.98157	Keywords of the paper are information geometry, hamiltonian Monte Carlo, Stein's method, reproducing kernel variational, coherence, accelerated optimization, dissipative systems, decision theory and active inference in no particular pedagogical order.
868260	873904	87	A	0.99889	Let's start with decision theory from the Stanford Encyclopedia of Philosophy.
874032	889400	88	A	0.54	The article begins Decision theory is concerned with the reasoning underlying an agent's choices, whether this is a mundane choice between taking the bus or getting a taxi or a more far reaching choice about whether to pursue a demanding political career.
889740	895700	89	A	0.96629	Decision theory is about agents making decisions, cognitive things making decisions.
895860	905260	90	A	0.99	And we can take a peek ahead to figure three, which is shown here as well as invoke some of the keywords that we're about to walk into now.
905410	912384	91	A	0.99711	So if this is your first time hearing some of this vocabulary or you're super familiar, either way, it's great.
912582	942600	92	A	0.99997	In this paper and in this discussion, we're going to see how different decision theoretic settings such as no ambiguity, no ambiguity or no coherence, no extrinsic value and no intrinsic value are going to be operationalized within an information theoretic formalism an information geometric formalism and specifically one that we're able to do accelerated inference on using variational methods.
942940	945460	93	A	0.99927	We're going to be involving action as a parameter.
945540	954508	94	A	0.99998	So we can call that active coherence and that's how we're approaching decision theory and getting from here to there with all those fun stops in between.
954674	957340	95	A	0.99922	Let's go to information theory.
958480	977520	96	A	0.51	The paper in the introduction writes of particular relevance to this chapter is information geometry, E g, the differential geometric treatment of smooth statistical manifolds whose origin stems from a seminal article by Rao 23, who introduced the Fishermetric tensor.
977600	991108	97	A	0.77088	On parameterized statistical models and thus a natural domain geometry that was later observed to correspond to an infinitesimal disturbance with respect to the Kolback libelr or Kale divergence.
991284	995096	98	A	0.99947	So going into citation 23 and 24.
995278	999112	99	A	0.99959	Citation 23 is Rao from 1992.
999246	1012960	100	A	0.67	And Rao wrote the objective of the paper is to derive certain inequality relations connecting the elements of the information matrix as defined by Fisher 1921 and the variances and covariances of the estimating functions.
1013380	1021036	101	A	0.52	A class of distribution functions which admit estimation of parameters with the minimum possible variance has been discussed.
1021228	1030624	102	A	1.0	The concept of distance between populations of a given type has been developed, starting from a quadratic differential metric defining the element of length.
1030752	1033460	103	A	0.99944	So I'm only going to give the disclaimer one time.
1033530	1037216	104	A	0.99993	Everything in red text is beyond speculative.
1037328	1043400	105	A	0.79324	It's just priming the pump for discussions with those who know more and with those who know less.
1043550	1047284	106	A	1.0	And it's just a first pass that we're going to continue to develop on.
1047422	1061548	107	A	0.99993	But broadly, if we can compute inequality relationships, which this paper developed in 1992, we can bound estimators, test for relative improvements and basically do stuff with those distinctions.
1061724	1086896	108	A	1.0	And if we can compute certain kinds of statistical distances such that a distance requires a length metric, we can find optimal estimators of parameters, specifically their variances and covariance structure in principle and in practice by finding that maximum informational alignment to other estimators or empirical data.
1087018	1093464	109	A	0.99662	So we'll often say, like if you had the radial parameterisation, you'd be predicting as well as you could.
1093582	1108952	110	A	0.96	And so we want to be able to operationalize that using distances and metrics and citation 24 this is an invariant form for the prior probability and estimation problems by Harold Jeffries.
1109096	1124060	111	A	1.0	And Jefferies wrote It has shown that a certain differentiate form depending on the values of the parameters in a law of chance, is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters.
1124220	1134500	112	A	0.99947	For laws containing a location in the scale parameter, a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters.
1135000	1142744	113	A	0.9999	This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.
1142942	1157784	114	A	0.50234	So if all the parameters in our model, whether there's one, two or more, are differentiable, which is to say they're smooth, et cetera, et cetera, there are technical details, then there are certain transformational invariances.
1157912	1165568	115	A	0.9995	So just like every Gaussian distribution, by shifting it and stretching and shrinking it, you can map those on to each other.
1165734	1166930	116	A	0.9992	It's like that.
1167860	1172024	117	A	0.99994	It turns out that some of those constraints can even be somewhat relaxed.
1172172	1178496	118	A	0.9652	For example, this approach may still be effective even where parameters are not differentiable everywhere.
1178688	1197870	119	A	1.0	And this is big for helping us do Bayesian estimation, which can be seen as a special case of informational transformations or of manipulations of statistical distributions in the information theoretic or information geometric sense.
1199440	1201144	120	A	0.99877	Functional analysis.
1201272	1213180	121	A	0.99675	So let's begin as tradition in our year 2023 with a brief quote from Chat GPT how is functional analysis helpful for us who are learning and applying active inference?
1213260	1215260	122	A	0.99803	I'll just read the last section.
1215420	1225350	123	A	0.55331	Overall, functional analysis provides a powerful mathematical toolset for understanding and analyzing the principles active coherence lab, and one can read more.
1225960	1230608	124	A	0.99996	Functional analysis is the study of functions.
1230784	1250296	125	A	1.0	And here we see one way of showing what a function does in terms of being this box f of x that takes in an input, an argument x and outputs a result y and also that can be understood as a mapping between or amongst spaces.
1250488	1252364	126	A	1.0	And there's a lot more to go into.
1252482	1269120	127	A	0.99942	Just wanted to bring up that in math, sometimes people have analyzed functions and their properties and we're interested in the properties of a special class of functions, which are probability distributions, well behaved probability distinctions.
1270740	1273744	128	A	0.99996	What do we do with those probability distinctions?
1273792	1276832	129	A	0.99916	Well, one thing we might want to do is sample.
1276976	1284900	130	A	0.99101	So the paper writes sampling methods are critical to the efficient implementations of many methodologies.
1285580	1295130	131	A	0.99998	Most Modern samplers are based on Markov chain Monte Carlo methods, which include slice samplers, piecewise deterministic, Markov chains and so on.
1296060	1308380	132	A	0.9	The original Hamiltonian Monte Carlo or HMC, which we're going to get to algorithm, was introduced in physics to sample distributions on gauge groups for lattice quantum chromodynamics.
1308980	1318640	133	A	0.98137	It combined two approaches that emerged in previous decades, namely the Metropolis Hastings algorithm and the Hamiltonian correlation of molecular dynamics.
1319140	1335492	134	A	0.99138	So the way that they state it in the beginning of section three of the paper is the purpose of sampling methods is to efficiently draw samples from a given target distribution row or more commonly, to calculate expectations with respect to row.
1335636	1361100	135	A	0.99922	By equation 33 and other, they write, Modern Hamiltonian Monte Carlo which again we're going to get to in a second relies heavily on symplectic integrators another keyword to simulate a deterministic dynamics responsible for generative distant moves between samples and thus reduce their correlation while at the same time preserving important geometric properties.
1361540	1366320	136	A	0.99991	Now, we want to make our samples as informative as possible.
1366470	1372980	137	A	0.99999	If we're just foraging samples and it's the same sample again and again, one can imagine that it is uninformative.
1374040	1390068	138	A	0.99844	Conversely, one can imagine a situation where samples are informative and that is brought into practice by making sure that the samples are minimally correlation with each other, as noted above.
1390244	1393700	139	A	0.98	And this also has analogy in cryptography.
1393860	1402488	140	A	0.9978	So if we're sampling something where the successive samples are 99% correlated, you can imagine that you're over sampling.
1402584	1408780	141	A	0.99956	So you're taking 10,000 frames per second of a YouTube video with 30 frames per second.
1408850	1412130	142	A	1.0	I think I'm streaming at 25 frames per second right now.
1412740	1423270	143	A	0.99935	Conversely, if one were to sample every ten minutes, then their correlation structure would also not capture the videos 25 frames per second.
1423720	1442532	144	A	0.99899	So in order to generate good and large moves in the space, we would need perfect knowledge of what the space is or how it's generated, which is sometimes knowable in cryptography, however empirically, we need to use heuristics and statistical approximations.
1442676	1448476	145	A	0.99679	So this is the applied statistics angle on the analytical relationships that we're going to be talking about.
1448578	1460480	146	A	0.74	And it's going to come up again and again this tension between analytical formulations, what's true in principle, and the statistical or numerical or computational applications.
1461060	1484884	147	A	0.94	Two Hamiltonian Monte Carlo from Wikipedia the Hamiltonian Monte Carlo algorithm, originally known as hybrid Monte Carlo still HMC is a Markov chain Monte Carlo method for obtaining a sequence of random samples which converge to being distributed according to a target probability distribution, for which direct sampling is difficult.
1485082	1488548	148	A	0.99837	So just a few first pass reflections.
1488724	1491396	149	A	0.96654	We're sampling, which is to say Monte Carlo.
1491508	1503528	150	A	1.0	And Monte Carlo is harkening back to a different time when gambling was done in Monte Carlo and the state space of all the hands of poker were too vast to compute analytically or equations were not known.
1503624	1518476	151	A	1.0	And so what one used to do, and still does, is draw samples and say, well, I don't know if this is the final estimate on how likely a royal flush is, but from the 2 billion hands that I sampled, there were two royal flushes.
1518508	1521760	152	A	0.99	And so it's one in a billion based upon my sample.
1522360	1532736	153	A	0.97593	So we're sampling because this is a complex distinctions for which an analytical solution is not possible, not known, not relevant, or not trackable computationally.
1532848	1537800	154	A	0.99651	So there's various situations where we want to sample and we don't necessarily have an analytical solution.
1538380	1550424	155	A	0.7939	When we're sampling from a converged stationarity, hashtag live stream 26 of a statistical landscape from a numerical perspective, we understand that landscape well enough to do coherence.
1550552	1560480	156	A	0.99695	So imagine that we're sampling height estimates from GPS locations in the X and the y coordinates, and we're getting a z value in our sample.
1560900	1566400	157	A	1.0	At some point we can say we're unsurprised by new samples.
1566740	1571244	158	A	1.0	At that point we understand the landscape well enough to do statistical inference.
1571292	1583216	159	A	1.0	And just like the frames per second in the previous example, one can imagine if the landscape is changing on the spatial scale of 1 mile and you're sampling every millimeter, you might be over sampling.
1583328	1589316	160	A	0.97944	If you were sampling every league, maybe you'd be too coarse grained with your samples.
1589508	1595896	161	A	1.0	And so one reason that comes up in science all the time for using sampling is that the search space might be just too large.
1596078	1601592	162	A	1.0	For example, the combinatorics of a phylogenetic tree with a thousand species might be vast.
1601736	1615490	163	A	1.0	And so it might be better to say something like, well, 99% of the million samples we drew were consistent with X, because an analytical solution or a brute force search, neither might be possible.
1616340	1624020	164	A	0.65	And the Wikipedia wrote this sequence of samples can be used to estimate integrals with respect to the target distribution.
1624360	1633620	165	A	0.99441	So sometimes we're sampling not from the target distribution itself, but rather from a distribution that is related or transformed from the target distribution.
1633960	1640708	166	A	0.99998	This is because sampling directly from the target distribution might be difficult or less than perfectly informative.
1640884	1653100	167	A	0.99958	So we might, for example, hint hint sample from the derivative of the target which might help us identify points where, for example, the derivative is flat in all directions.
1654160	1661120	168	A	0.99996	In the case that we're sampling from a derived landscape of the target distribution, this is called a symplectic integrator.
1661540	1664850	169	A	0.99877	So let's look at what a symplectic integrator is.
1665380	1670188	170	A	0.9804	So first, backing up to what is differentiate geometry.
1670364	1678020	171	A	0.99921	Differential geometry is a mathematical discipline that studies the geometry of smooth shapes and smooth spaces, otherwise known as smooth manifolds.
1679240	1692090	172	A	0.92764	Symplectic geometry is a branch of differential geometry and differential topology that studies Symplectic manifolds, that is, differentiable manifolds equipped with a closed, nondegenerate two form.
1692860	1704776	173	A	0.93462	Symplectic geometry has its origins in the Hamiltonian formulation of classical mechanics, where the phase space of certain classical systems takes on the structure of a symplectic manifold.
1704888	1706988	174	A	0.99815	So the keyword density is high.
1707074	1711552	175	A	0.99994	These topics are all very closely linked from the paper.
1711686	1723380	176	A	0.99998	It has been known for a long time that the class of symplectic integrators is the preferred choice for simulating physical systems, only the finest for our simulations of physical systems.
1724440	1737640	177	A	0.99995	These discretization techniques are designed to preserve the underlying symplectic geometry of Hamiltonian systems and they also form the basis of Hamiltonian Monte Carlo or hybrid Monte Carlo methods.
1737980	1748500	178	A	0.99958	We're going to come back to this tension again and again, which is what good is an in principle, smooth and differentiable analytical correlation.
1748660	1763888	179	A	0.99994	If our discreditization scheme, the way that we actually implement the steps on a computer, are inferior, we might ruin all those nice properties that we work to get about the analytical form.
1764054	1769200	180	A	0.99996	We might just throw those out when we discretize it coarsely or inappropriately.
1769700	1771768	181	A	0.75051	So what is the symplectic integrator?
1771884	1775300	182	A	0.64778	It's a numerical integration scheme for Hamiltonian systems.
1775640	1782336	183	A	0.7165	Symplectic integrators form the subclass of geometric integrators, which by definition are canonical transformations.
1782528	1784710	184	A	0.65717	They're transformations we know a lot about.
1787260	1803432	185	A	0.93398	Symplectic integrator schemes are referring to, again, just broadly, both the analytical formulations and the software packages and approaches that we can use to implement those formalisms.
1803576	1821840	186	A	0.99	And these integration schemes are useful across difficult estimation problems, which is why they're used to study nonlinear dynamics, molecular dynamics like protein simulation, discrete element methods, accelerator physics, plasma physics, quantum physics, celestial mechanics.
1823160	1832390	187	A	0.76	The time evolution of Hamilton's equation is a symplectomorphism, meaning that it conserves the symplectic two form.
1832920	1838570	188	A	0.89	A numerical scheme is a symplectic integrator if it also conserves this two form.
1840620	1844200	189	A	0.89832	Let's hear more if this is not correct or not complete.
1844350	1848876	190	A	0.99992	But the one form is like the differentiation and integration of a line.
1849058	1853464	191	A	1.0	The two form is like the differentiation and integration of a surface.
1853512	1858060	192	A	1.0	And the three form is like integration or differentiate of a volume.
1858400	1864400	193	A	1.0	And John Denker has a great blog post on the basic properties of a symplectic integrator.
1864980	1872944	194	A	0.99987	Some really interesting quotes that are good to just keep in mind when we're hearing about all these avenues we're going to be exploring in the paper.
1873142	1879220	195	A	1.0	A symplectic integrator can serve the area in phase space delimited by an ensemble of systems.
1879640	1886630	196	A	0.99807	For a periodic system, there is an area that is conserved, namely the area inside the phase space orbit of the system.
1887240	1893912	197	A	1.0	The main reason for mentioning the orbit is to make the point that there are lots of different things with dimensions of area in phase space.
1894046	1896170	198	A	0.99991	Some are conserved and some not.
1896640	1898908	199	A	0.99994	Some are interesting and some not.
1899074	1903452	200	A	1.0	You have to specify which sort of area you are talking about.
1903586	1918972	201	A	0.99392	So another thing that's going to arise is we're interested in very well behaved outcomes from a very specific or constrained, still broad, but definitely constrained set of distributions.
1919036	1926980	202	A	0.99999	For example, distributions that can be interpreted in an information theory sense as probability distinctions.
1928200	1932736	203	A	0.99974	What was that mention of the Hamiltonian and the shadow Hamiltonian?
1932928	1934890	204	A	0.99758	So the author is right.
1936300	1947548	205	A	0.97507	For symplectic brackets, which is the kind of operation that reflects the symplectic integrator, the existence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, e.
1947554	1955528	206	A	0.8	G for variational integrators, which use variational inference, which use a discrete version of Hamilton's principle of least action.
1955624	1973990	207	A	1.0	And we've heard it before that free energy principle is a principle of least action for inference and action and more generally for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by some technicalities that we can learn about.
1974600	1986760	208	A	0.99991	As we shall see, such geometric integrators can be constructed by leveraging the shadow Hamiltonian property of symplectic methods on higher dimensional conservative Hamiltonian systems.
1988300	1998830	209	A	0.99997	In short, as a consequence of having shadow Hamiltonian, such geometric integrators are able to reproduce all the relevant properties of the continuum system.
1999520	2001692	210	A	0.99995	These arguments are completely general.
2001826	2032150	211	A	0.99754	So even before one knows what the shadow Hamiltonian necessarily is, the authors are letting us know that if we can discretize the shadow Hamiltonian appropriately through symplectic integration, we can provide a inclination scheme that ends up staying consistent and compatible and all down the middle with the analytical properties that we've worked so hard to get.
2033240	2039050	212	A	1.0	And there were some different papers that were cited and some that were found during research.
2039900	2051848	213	A	0.99977	This paper Time Step in Shadow Hamiltonian and Molecular Dynamics Simulations by Kim demonstrates symplectic integrators on the simplest possible system a simple harmonic oscillator.
2051944	2058476	214	A	0.99853	So if one wants to go to a technical example, but one that builds intuition, that's a great place to start.
2058658	2066704	215	A	1.0	And from the Wikipedia article on energy drift, these integrators do not in fact, reproduce the actual Hamiltonian mechanics of the system.
2066822	2075060	216	A	0.99998	Instead, they reproduce a closely related shadow Hamiltonian whose value they conserved many orders of magnitude more closely.
2075640	2081732	217	A	1.0	The accuracy of the energy conversation for the true Hamiltonian is independent on the time step.
2081866	2111020	218	A	0.99026	So if we can, for examples, just metaphorically speaking, capture a high evolution image of the shadow, then we'll be able to know something, do something with the actual some technical details on Stein's method, which is a general method in probability theory to obtain bounds on the distance between two probability distributions with respect to a probability metric.
2111180	2120400	219	A	1.0	And Stein's method is used in the context of Stein operators and Stein class, which we're going to get to in the subsequent discussions.
2122020	2129440	220	A	0.95648	Reproducing Kernel our discussion of inference builds upon the theory of Hilbertian subspaces and in particular, reproducing kernels.
2129520	2137348	221	A	0.99993	These inference schemes rely on the continuity of linear functionals such as probability and Schwartz distributions over a class of functions.
2137364	2155288	222	A	1.0	To geometrize the analysis of integral probability metrics, which measure the worst case integration error, we shall explain how maximum mean kernelized and score matching discrepancies arise naturally from topological considerations.
2155464	2161872	223	A	0.99594	So we've added some background links here to the slide, but it'll be a Brea place to start with.
2161926	2167632	224	A	0.72484	Authors, what is a reproducing kernel and how is it used?
2167686	2174864	225	A	0.99973	In this paper onwards, through the keywords we go accelerated optimization and variational inference.
2174992	2182704	226	A	0.99917	So here's a 2016 paper from Michael Jordan and other authors and some quotes from the abstract.
2182832	2190548	227	A	0.98176	Accelerated methods achieve faster coherence rates than gradient methods, and indeed, under certain conditions they achieve optimal rates.
2190724	2196200	228	A	0.99999	However, accelerated methods are not dissent methods and remain a conceptual mystery.
2196560	2201848	229	A	0.9939	We propose a variational continuous time framework for understanding accelerated methods.
2202024	2209810	230	A	0.81126	We provide a systematic methodology for converting accelerated higher order methods from continuous time to discrete time.
2210260	2216800	231	A	0.8503	Our work illuminates a class of dynamics that may be useful for designing better algorithms for optimization.
2217300	2221090	232	A	0.72	And this is going to be, again, something awesome to discuss.
2222200	2231360	233	A	0.94736	We've talked about, on many streams, variational inference in the context of gradient based optimization.
2231520	2244900	234	A	0.99986	For example, the ball is rolling to the bottom of the bowl, bottom of the hill, and what you can do is you can take the gradient div, grab curl and all that of where the ball is and just go downhill.
2244980	2258428	235	A	1.0	And if you designed the hill to be the right shape, or you're using a chosen family of variational estimators that are the right shape, are a good shape, well behaved shape, then you follow the gradient on down.
2258514	2261020	236	A	1.0	And that's how we've talked about variational inference.
2261180	2266688	237	A	0.98913	Accelerated optimization is going to be accelerated from that.
2266774	2270690	238	A	1.0	And so it's fun to think about and it'll be great to talk about.
2271960	2273152	239	A	0.33981	Dissipative systems.
2273216	2279270	240	A	1.0	The vast majority of statistics and machine learning applications involve solving optimization problems.
2279960	2281268	241	A	0.54	The author is right.
2281434	2287720	242	A	0.99939	Accelerated gradientbased methods and several variations thereof have become workhorses in these Fields.
2288060	2293080	243	A	0.99939	Recently, there has been great interest in studying such methods from a continuous time limiting perspective.
2293820	2299660	244	A	1.0	Such methods can be seen as first order integrators to a classical Hamiltonian system with dissipation.
2300080	2312640	245	A	0.99999	This raises the question on how to discretize the system such that important properties are preserved, assuming the system has fast conversions to critical point and desirable stability properties.
2313540	2317856	246	A	0.82198	Originally, such a theory of geometric integration was developed with conservative systems in.
2317878	2324916	247	A	0.99999	Mind, while optimization in optimization, the associated system is naturally a dissipative one.
2325098	2338040	248	A	0.93608	More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of coherence and stability, which are the main properties of interest for optimization.
2338620	2347092	249	A	0.94975	So from the Wikipedia on Hamiltonian systems an example of a time independent Hamiltonian system is the harmonic oscillator.
2347236	2357608	250	A	0.99893	So it's just a frictionless spring oscillating around the Hamiltonian of the system does not depend on time, and thus the energy of the system is conserved.
2357704	2362480	251	A	0.73	And so we can say that that is a conservative Hamiltonian.
2363060	2366960	252	A	0.77921	If the Hamiltonian decays in time, it is dissipative.
2368020	2386600	253	A	0.99976	Recently, the advances of these authors and others have extended our ability to make distinctions of dissipative Hamiltonians that respect the rates of conversions and stability, which are the main properties of interest for optimization.
2387020	2395060	254	A	1.0	And we've talked about Hamiltonians on Livestream number 49 with Dalton Sakthivadivel Devil on Bayesian mechanics.
2395220	2396970	255	A	0.9973	So pretty cool.
2397280	2398536	256	A	0.94274	What is conservative?
2398648	2399928	257	A	0.99993	What is dissipative?
2400104	2411696	258	A	0.98854	We'll explore as the final keyword here, I just wanted to start with a blank slide on active inference and see what happens in 52.1.
2411798	2420784	259	A	0.98807	So consider the slide to be blank and for the authors and discussants looking forward to hearing Hohwy.
2420822	2422688	260	A	0.99932	Are we approaching active inference?
2422784	2428596	261	A	0.99916	Given everything that we've just loaded onto the table and everywhere that we're about to go with the paper?
2428778	2430340	262	A	0.98513	Let's get into it.
2430490	2445748	263	A	0.99992	Section One introduction so, the authors begin with differential geometry plays a fundamental role in applied mathematics, statistics and computer science, including various domains.
2445844	2450220	264	A	0.99356	Citations one through 22, which can be shown here.
2450370	2453550	265	A	0.99318	Various citations from this well cited paper.
2454720	2472384	266	A	1.0	The geometric study of statistical models has had many successes, ranging from statistical inference, where it was used to prove the optimality of maximum likelihood estimator, to the construction of the category of mathematical statistics generated by Markov morphisms.
2472512	2476070	267	A	0.99921	So what are Markov morphisms from Ncat lab?
2476760	2486200	268	A	1.0	The formalism of Markov categories can be thought of as a way to express certain aspects of probability and statistics synthetically.
2486860	2499820	269	A	0.99999	In other words, it consists of structure and axioms, which one can think of as fundamental in probability and statistics, which one can use to prove theorems without having to use measure theory directly.
2500640	2511410	270	A	0.96416	Intuitively, for the purposes of probability, a Markov category can be seen as a category where morphisms behave like random functions or Markov kernels, hence the name.
2512100	2515600	271	A	0.99488	So, just some red text speculation.
2515940	2530420	272	A	0.9999	It was a big success and a breakthrough to think about the category from a formal category theoretic perspective, to think about the category of statistical distinctions from an information geometric perspective.
2531240	2545480	273	A	0.99994	This means that we can understand many analytical properties and transformations of statistical distributions hashtag Bayesian and develop general methods that work for that whole category, like accelerated optimization.
2546560	2548460	274	A	0.89	And the applications are vast.
2551360	2557324	275	A	0.99971	Originally, such a theory of geometric integration was developed with conservative systems in mind.
2557522	2562784	276	A	0.99761	While in optimization the associated system is naturally a dissipative one.
2562982	2566960	277	A	0.57472	Nevertheless, symplectic integrators were exploited in this concept.
2567860	2581824	278	A	0.53081	More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of convergence and stability, which are the main properties of interest for optimization.
2581952	2600510	279	A	0.99255	So, citation nine, what is it about 2021 paper on dissipative symplectic integration with applications to gradient based optimization by some of the authors, and it's interesting to pull out some quotes from the abstract and just hear a little bit about what they're up to.
2601200	2611928	280	A	0.96476	Recently, continuous time dynamical systems have proved useful in providing conceptual and quantitative insights into gradient based optimization widely used in Modern machine learning and statistics.
2612104	2619884	281	A	0.99224	An important question that arises in this line of work is how to discritize the system in such a way that its stability and rates of convergence are preserved.
2620012	2621740	282	A	0.9991	So this is the sampling problem.
2621910	2625190	283	A	0.53849	In continuum time, the math is continuous and nice.
2625560	2640180	284	A	0.60675	However, the sampling problem which actually comes into play when we use Modern computational methods that are implemented on discrete time, space and constraints, what happens when we use unconventional computing?
2640340	2646890	285	A	0.99995	That's an interesting discussion question, but in active today, the discretization approach is going to matter a lot.
2647580	2658140	286	A	0.99999	In this paper, we propose a geometric framework in which such discretizations can be realized, systematically enabling the derivation of ratematching algorithms without the need for a discrete coherence analysis.
2658560	2669500	287	A	0.99977	More specifically, we show that a generalization of symplectic integrators to nonconservative, and in particular, discipline of Hamiltonian systems is able to preserve rates of convergence up to a controlled errors.
2669660	2680660	288	A	0.99815	So this is the advance of this paper that within a controlled error good enough, they can model conservative and nonconservative, even dissipative Hamiltonian systems.
2681480	2697796	289	A	0.77852	Moreover, such methods preserve a shadow Hamiltonian despite the absence of a conversation law extending key results of symplectic integrators to nonconservative cases, our arguments rely on a combination of backwards error analysis with fundamental results from symplectic geometry.
2697988	2715756	290	A	0.99979	We stress that although the original motivation for this work was the application of optimization, where dissipative systems play a natural role, they are fully general and not only provide a differential geometric analysis for discipline of Hamiltonian systems, but also substantially extend the theory of structure preserving inclination.
2715948	2718640	291	A	0.99741	So they made this generalization.
2719060	2726660	292	A	0.99996	In the case of optimization, however, it's exciting that it extends even deeper into the math and the symmetry.
2727560	2732260	293	A	0.99992	Those are a few highlights from section one on to section two.
2732330	2734944	294	A	0.82513	Accelerated optimization vroom, vroom.
2734992	2735910	295	A	0.99955	Here we go.
2736860	2747604	296	A	0.99591	So there are going to be a lot of slides from sections two through five, and to keep the video a reasonable length, I'm going to just highlight a few pieces.
2747652	2754430	297	A	0.99972	Not even necessarily what I intended to highlight, but I'm just going to go for it so we can get through it.
2755040	2777220	298	A	0.92042	Section two we shall be concerned with a problem of optimization of a function finding a point that maximizes V of Q or minimizes negative V of Q over a smooth manifold M r in the real numbers, many algorithms and optimization are given as a sequence of active inference.
2778280	2785940	299	A	0.99995	Even when these algorithms are seen as distinctions of a continuum system whose behavior is presumably understood.
2786100	2792232	300	A	0.99963	It is well known that most discretizations break important properties of a system, and they continue to write.
2792366	2805390	301	A	1.0	The analysis of such finite difference iterations is usually challenging, relying on painstaking algebra to obtain theoretical guarantees such as conversation to a critical point, stability and rates of conversation to a critical point.
2806960	2818144	302	A	0.99995	Even when these algorithms are seen as discreditizations of a continuum system whose behavior is presumably understood, it is well known that most discreditizations break important properties of the system.
2818342	2829200	303	A	0.99978	It can't be highlighted enough when we implement optimization algorithms on Modern computers, we have to make discreditizations in space and time and in practice.
2829360	2849260	304	A	0.99846	So even if the analytical properties of the distribution are totally fire, when we try to solve and fit models to empirical data, unless we also match that elegance and power with a inclination approach, we end up failing to realize analytical promises.
2851360	2852268	305	A	0.89	2.1.
2852354	2860240	306	A	1.0	The principle of Geometric Integration fortunately, the author's right here comes into play one of the most fundamental ideas of geometric integrations.
2860660	2868050	307	A	0.99996	Many numerical integrators are very close exponentially in the step size to a smooth dynamics generated by a shadow vector field.
2869460	2876390	308	A	1.0	A little whisper of a shadow Hamiltonian and the shadow vector field is a perturbation of the original vector field.
2876840	2888916	309	A	0.99906	This allows us to analyze the discrete trajectory implemented by the algorithm using powerful tools from dynamical systems and differential geometry, which are a priori reserved to smooth systems.
2889028	2895050	310	A	0.99966	So we're going to be able to discretize our cake and have it be smooth too.
2895740	2907640	311	A	0.57243	Crucially, while numerical integrators typically diverge significantly from the dynamics they aim to simulate, geometric integrators respect the main properties of a system in the context of optimization.
2907720	2916210	312	A	0.99998	This means respecting stability and rates of coherence seems like a good idea to respect the main properties of the system.
2916980	2920672	313	A	0.99991	This was first demonstrated in nine and further extended in ten.
2920806	2923236	314	A	0.94213	Our discussion will be based on these work.
2923338	2956588	315	A	0.99055	So citation Nine, Franca, Jordan and Vidal mentioned previously and citation ten, Franca, Mark, Girolami and Jordan optimization on manifolds a symplectic approach authors previous work So big development we can use geometric rather than course numerical integrators, so our sampling based optimization schemes, including their discordization, respect the main properties of the system.
2956754	2957870	316	A	0.99977	Sounds great.
2958320	2967920	317	A	0.99983	Section Two Two conservative flows and symplectic integrators as a stepping stone, we first discussed the construction of suitable conservative flows.
2970260	2973940	318	A	0.99994	These are very well studied and they're very intuitive.
2974520	2975940	319	A	0.99931	More intuitive.
2976840	2990084	320	A	1.0	To construct vector Fields along the derivative of x, which is the function of flows along which some function is constant, we shall need brackets geometrically.
2990132	2996512	321	A	0.99991	These are morphisms x star to x, also known as contravariant tensors of rank two in physics.
2996676	3010220	322	A	0.99971	So calling back the two form and the brackets, importantly, vector Fields that preserve f correspond to bracket vector Fields in which B is antisemitic.
3010980	3014316	323	A	0.99756	Constructing conservative flows is thus straightforward.
3014508	3020690	324	A	0.93005	Unfortunately, it is a rather more challenging task to construct efficient discretizations that retain this property.
3021140	3030260	325	A	0.99999	Most well known procedures, namely discrete gradient and projection methods, only give rise to integrators that require solving implicit equations at every step.
3030410	3036532	326	A	1.0	And they may break other important properties of the system inclination.
3036596	3040600	327	A	1.0	96 this is the issue again.
3040750	3053992	328	A	0.99996	No matter how nicely behaved in principle, our analytical underlying smooth differentiable function is if we discretize it and we chop it up in a way that's inappropriate.
3054136	3060720	329	A	0.99999	We don't respect the properties of the system, we don't end up with being able to realize those analytical promises.
3062340	3078660	330	A	0.99988	Indeed, in practice, the Hamiltonian usually decomposes into a potential energy associated to position and independent of momentum, and a kinetic energy associated to momentum and invariant under position changes, both generating tractable flows.
3079320	3085530	331	A	0.99995	Thanks to this decomposition, we are able to construct numerical methods through splitting the vector field.
3085900	3099660	332	A	1.0	A few ideas coming together here, recalling some of our generalized coordinate based approaches to non equilibrium steady states and Bayesian mechanics, and also decomposition of complex functions.
3100400	3108264	333	A	0.6687	Note also that for symplectic brackets, the evidence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, eg.
3108312	3124896	334	A	0.99435	For variational integrators, which use a discrete version of Hamiltonians Hamilton's principle of Least action and for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by the first Dean cohomology of FaceBase.
3125008	3131184	335	A	0.99052	So for discussion with authors annual what are the splitting integrators?
3131312	3133590	336	A	0.99992	What is split from what and why?
3134060	3137188	337	A	0.99988	What is the bracket notation or operation?
3137364	3140004	338	A	1.0	And what is a piss on bracket?
3140052	3141930	339	A	0.99826	What is a piss on system?
3142940	3156140	340	A	0.99963	Section 23 rate matching integrators for smooth Optimization so, having obtained a vast family of smooth dynamics and integrators that closely preserve F, we can now apply these ideas to optimization.
3157280	3160510	341	A	0.99997	What is being set up in this section and why?
3160820	3177284	342	A	0.99981	In equation seven we see that the damping coefficient gamma of t being greater than zero reflects the dissipative component, or the second term in that right hand side of equation seven.
3177402	3181520	343	A	0.99908	So damping coefficient controls the strength of the dissipation.
3181680	3191656	344	A	1.0	One can imagine that if the damping coefficient is zero, the dissipative side zeros out and b has conservative behavior and so on.
3191758	3194970	345	A	0.99793	So what is being set up here and why?
3195580	3208716	346	A	0.67	Dot dot dot dot dot the existence of such a Leopold function described above implies that trajectories starting in the neighborhood of Q star will converge to Q star.
3208898	3213490	347	A	0.99963	So it's like a ball rolling to the bottom of a hill, just like we've always wanted.
3214100	3224776	348	A	0.99998	In other words, the above system provably solves the optimization problem minimum of V on Q, such that Q is in the D dimension.
3224908	3227376	349	A	0.62	Real punchline.
3227568	3248060	350	A	0.96874	We're setting up a system with good smooth optimization characteristics ball rolling to the bottom of a smooth hill that is also on the path or using the notation of or prepared to make the transformation to attractable discretization approach.
3248480	3251340	351	A	0.9983	So some more on the damping coefficient.
3251760	3256060	352	A	0.99997	They bring up some common choices for the damping coefficient.
3256400	3261852	353	A	1.0	And Big O notation describes on the order of which something occurs.
3261996	3270432	354	A	0.99907	For example, linearly order of the variable or sub or super linearly as a function of time or data points.
3270566	3281652	355	A	0.82095	So in computational complexity analysis, people are often interested as I double the amount of data I'm analyzing, does that make the algorithm take twice as long?
3281706	3284464	356	A	0.99974	That's linear computational complexity?
3284592	3286552	357	A	0.98541	Does it take four times as long?
3286606	3289096	358	A	0.99439	Does it take the same amount of time?
3289278	3290584	359	A	1.0	And so on.
3290782	3308780	360	A	1.0	And it'll be great to talk about the intuitions and implications and generalizations about how the damping coefficient influences the computational complexity estimates for convergence in different settings.
3311300	3317900	361	A	0.98207	They write the conservative system from equation 16 reduces precisely the original dissipative system 13.
3318060	3327604	362	A	1.0	The second equation in 16 reproduces 14, and the remaining equations are equivalent to the equations of motion associated to 13, which in turn are equivalent to eight.
3327642	3328980	363	A	0.99995	As previously noted.
3329640	3345050	364	A	0.99575	Formally, what we have done is to embed the original dissipative system with phase space r 2D, so real with 2D dimension into a higher dimension conservative system with phase space r 2D plus two.
3345420	3352300	365	A	1.0	The dissipative dynamics thus lies on a hyper surface of constant energy k equals zero in high dimension.
3353120	3358780	366	A	0.92	The reason for doing this procedure, called implication, is purely theoretical.
3359120	3359884	367	A	0.92	Oh, come on.
3359922	3361520	368	A	0.99857	It's not purely theoretical.
3361940	3376660	369	A	0.99988	Since the theory of symplectic integrators only accounts for conservative systems, we can now extend this theory to dissipative systems settings by applying a symplectic integrator to 13 and then fixing the relevant coordinates 17 in the resulting methods.
3377000	3380950	370	A	0.96102	Geometrically, this corresponds to integrating the time flow exactly.
3381560	3385924	371	A	0.99957	We're going to talk about a relationship between time and dissipative systems.
3386052	3390152	372	A	0.99999	After all, it's dissipative systems that are dissipating in time.
3390286	3398140	373	A	1.0	And so discreditization of time plays a role in the appropriate inclination of a dissipative Hamiltonian.
3398640	3405912	374	A	0.61	And in citation nine previously raised, such a procedure was defined under the name of presymplectic integrators.
3405976	3413260	375	A	1.0	And these connections hold not only for the specific example above, but also for general non conservative Hamiltonian systems.
3413340	3416064	376	A	0.99643	So what is happening here?
3416262	3419548	377	A	0.99926	What's and intuition for conservative and dissipative systems?
3419644	3423590	378	A	1.0	And how have the recent works of Franca at all expanded what is possible?
3424520	3430820	379	A	1.0	We are now ready to explain why this approach is suitable to construct practical optimization methods.
3431640	3440920	380	A	1.0	The coordinate t sub k becomes simply the time discreditization, which is exact, and so is U sub k, since it is a function of time alone.
3441260	3444660	381	A	0.99703	Importantly, U does not couple to any of the other degrees of freedom.
3444740	3456504	382	A	0.99998	So it is irrelevant whether we have access to U or not, because we're looking to solve a function of t 17.
3456552	3463680	383	A	0.53	A can be substituted in 15 to get 18, and you can replace 13 to get 19.
3464500	3466640	384	A	0.65629	We'll talk more about it with the authors.
3469140	3471212	385	A	0.56757	Therefore, the known rates.
3471276	3481700	386	A	0.99864	Equation eleven for the Continuum system are nearly preserved, and so would be any rates of more general time dependent dissipative Hamiltonian systems.
3482600	3487450	387	A	0.99989	Let us now present and explicit algorithm to solve the optimization problem.
3489340	3499950	388	A	0.99987	This is all happening as a consequence of having a shadow Hamiltonian such that geometric integrators are able to reproduce all the relevant properties of the Continuum system.
3501600	3512176	389	A	0.935	Section Two Four manifold and Constrained Optimization following ten, we briefly mentioned how the previous approach can be extended in great generality to an optimization problem.
3512278	3520672	390	A	0.99853	So equation 21 we present our minimization problem V of Q variational distribution on Q.
3520806	3523170	391	A	0.99214	Citation ten Franca et al.
3523880	3528900	392	A	0.94678	They write There are essentially two ways to solve this problem through a dissipative Hamiltonian approach.
3529560	3538840	393	A	1.0	One is to simulate a Hamiltonian dynamics on T star M by incorporating the metric of M in the kinetic moving part of the Hamiltonian.
3539180	3548620	394	A	0.99985	Another is to consider a Hamiltonian dynamics on RN and embed M into RN by imposing several constraints.
3549040	3552396	395	A	0.83	The first approach uses a Lee group.
3552578	3554190	396	A	0.9932	We'll talk more about it.
3554960	3569200	397	A	0.97493	An example of a second approach one can constrain the integrator on RN to define a symplectic integrator on M via the discrete constrained variational approach by using some techniques.
3571480	3578416	398	A	1.0	The above method consists in a dissipative generalization of the well known rattle integrator from molecular dynamics.
3578608	3584260	399	A	0.99624	Citations 100 through 103 used in computational biology.
3585320	3595880	400	A	0.99985	Section Two Five gradient Flow as a High Friction Limit let us provide some intuition why simulating second order systems is expected to field faster algorithms.
3596780	3598796	401	A	0.99926	As an illustration, consider figure one.
3598818	3606808	402	A	0.99852	On the left, we'll look at in a second where a particle immersed in a fluid falls under the influence of a potential force.
3606984	3610464	403	A	0.99734	Negative delta sub q on V in Q.
3610662	3621212	404	A	0.99534	So partial differential with respect to Q of V on Q that plays a role of gravity and is constrained to move on a surface.
3621356	3623724	405	A	0.99462	So ball rolling down the hill.
3623852	3625490	406	A	0.99997	We weren't joking about it.
3625860	3632932	407	A	0.99975	In the underdamp case, the particle is underwater, which is not so viscous, so it has acceleration and moves fast.
3633066	3634688	408	A	0.99517	It may even oscillate.
3634864	3648472	409	A	0.99999	In the over damped case, the particle is in a highly viscous fluid such as honey, and the drag force that damping coefficient gamma is comparable or stronger to the gradient.
3648616	3653964	410	A	0.99976	Thus the particle moves slowly since it cannot accelerate during the same elapsed time.
3654002	3658300	411	A	0.99908	Delta T, an accelerated particle would travel a longer disturbance.
3659860	3661250	412	A	0.95196	Here's figure one.
3661620	3667388	413	A	0.98132	So here y simulating second order systems yields accelerated methods.
3667404	3671520	414	A	0.96934	So on the left, constrained particle falling in fluids of differential viscosity.
3671880	3675860	415	A	0.99293	Here on the left, we have the particle falling through honey.
3676280	3683488	416	A	0.98688	It's slowly making its way to the bottom of the bowl, but it never really builds speed.
3683664	3698060	417	A	0.83839	Its terminal velocity is being dominated by the viscosity of the fluid, whereas this bowl falling through water or falling through air is with respect to the honey dampened bowl.
3698480	3708140	418	A	0.99999	This is like an accelerated optimization, and they present some numerical results that help us bolster that intuition.
3708960	3714748	419	A	0.99984	Really fun and embodied way to think about optimization.
3714924	3720332	420	A	0.99963	We've talked about that ball rolling to the bottom of the hill and what the ball does when it hits a small bump.
3720396	3730790	421	A	0.9999	But we have not talked about what media the Bull is floating through and the media is the message and the fish doesn't know what it's swimming through.
3732120	3736228	422	A	0.99968	Section 2.6 optimization in the space of probability measures.
3736324	3744970	423	A	0.93046	It'll be great to explore more because we're going to see free energy calculations on the stationary density, KL divergence and more.
3745340	3758600	424	A	0.99993	For now, we can just say all those optimization techniques we were bringing up earlier, we're going to be able to do them on information geometric spaces that correspond to probability measures that are well behaved.
3758760	3764092	425	A	0.99998	Not all distinctions are probability measures or probability distributions.
3764156	3770268	426	A	0.99979	Just because you draw a line doesn't mean you can use that in part of your variational inference scheme.
3770284	3771760	427	A	0.99976	For Bayesian statistics.
3772660	3774050	428	A	0.98619	That's section two.
3774980	3778704	429	A	0.99913	On to section three Hamiltonian based accelerated sampling.
3778752	3787620	430	A	0.99685	So we talked about the Hamiltonian conservative and dissipative and the shadow Hamiltonian, which is going to be orders of magnitude better to approximate.
3787960	3795652	431	A	0.99972	We talked about gradientbased methods and how accelerated sampling is going to help us accelerate those sampling techniques.
3795796	3800300	432	A	1.0	And now section three brings it together with Hamiltonian based accelerated sampling.
3800960	3811148	433	A	0.99964	Section Three the purpose of sampling methods is to efficiently draw samples from a target distribution row, or more commonly, to calculate expectations with respect to row.
3811324	3813360	434	A	1.0	And from the end of the paragraph.
3814340	3820732	435	A	0.94646	An efficient sampling scheme is one that minimizes the variance of the Monte Carlo Markov chain.
3820796	3832996	436	A	0.99968	Estimator Monte Carlo, that means that we're sampling hands from the poker table and Markov chain, which means that the past only influences the future through the present.
3833098	3835370	437	A	0.98903	It's a quote memoryless process.
3836780	3840580	438	A	0.99998	In other words, fewer samples will be needed to obtain a good estimate.
3840660	3846360	439	A	0.6497	Intuitively good samplers are Markov chains that converge as fast as possible to the target distribution.
3846720	3850616	440	A	0.64642	It's like if you laid down a jump rope on a mountain.
3850808	3860556	441	A	0.99392	If that jump rope converged quickly to the topography of the mountain, it would have been a fast converging jump rope.
3860668	3864720	442	A	1.0	And we're doing something like that, but with sampling from the jump rope.
3865700	3866512	443	A	0.86	3.1.
3866566	3875940	444	A	0.99202	Optimizing diffusion processes for sampling as many MCMC methods are based on discretizing continuous time stochastic processes.
3876360	3892680	445	A	1.0	The analysis of continuous time processes is informative of the properties of efficient samplers and diffusion processes possess a rich geometric theory extending that of vector Fields and have been widely studied in the context of sampling.
3893500	3896200	446	A	0.80035	Lot of very fascinating ideas here.
3896350	3900670	447	A	0.09233	Stratinovich stochastic differential equations are going to come into play.
3906200	3911140	448	A	0.99985	Equation 34 more technical details on diffusion.
3912380	3921240	449	A	0.98	The calculus of twisted differentiate forms allows us to have a measure informed calculus on multivector Fields.
3923200	3925608	450	A	0.99993	What are twisted differential forms?
3925784	3927896	451	A	0.99225	I'm looking forward to that conversation.
3928008	3930828	452	A	0.99998	What is the untwisted differential form?
3930994	3934030	453	A	0.99971	Have we been twisted all along or not?
3935780	3941680	454	A	1.0	And they go on to write a fundamental criterion for efficient sampling is non reversibility.
3942180	3950896	455	A	1.0	A process is non reversible if it is statistically distinguishable from its time reversal when initialized at the target distribution.
3951088	3957460	456	A	0.99919	So once the jump rope is lying flat on the mountain, it's like in a position of reversibility.
3958600	3961352	457	A	0.99945	Mixing metaphors at will.
3961406	3977352	458	A	0.65474	Here measure preserving diffusions are non reversible precisely when some cognition are met intuitively non reversible processes backtrack less often and thus furnish more diverse samples.
3977496	3993280	459	A	0.99972	So if you're that ball rolling on the mountain, you want to just plow forward and whether you're going up or downhill, you want to be sampling and making moves, but you don't want to be going back and forth because at that point it's not and efficient sampling path.
3994180	4001424	460	A	0.69233	It's well known that removing nonreversibility worsens the spectral gap and the asymptotic variants of the MCMC estimator.
4001552	4013640	461	A	0.99846	So time discreditization spectral gap in dimension with linear coefficients, one can construct the optimal non reversible matrix a to optimize the spectral gap.
4014140	4032350	462	A	0.99999	We can have well behaved parameters that help us get at optimal discretization schemes so that that shadow Hamiltonian can be preserved when we do discretize it during optimization, so that we can respect the key properties of the system.
4033200	4038748	463	A	0.99998	However, there are no generic guidelines on how to optimize nonreversibility in arbitrary diffusions.
4038924	4058608	464	A	0.51952	This suggests a two step strategy to construct efficient samplers one, optimize reversible distinctions dimension and two and a non reversible perturbation equation 36 citation 125 diffusions are manifolds.
4058704	4065530	465	A	0.99545	Diffusion on manifolds are reversible when certain things are the case, and they're not when other things are the case.
4066940	4073050	466	A	0.99987	Equation 37 we got a triangle pointing down and a triangle pointing up.
4073500	4076104	467	A	0.50742	We're going to talk more about it.
4076302	4082456	468	A	0.97538	Under damped Langevin dynamics combine all the desirable properties of an efficient sampler.
4082568	4088480	469	A	0.99795	It is reversible, has degenerate noise, and achieves accelerated coherence to the target density.
4088820	4106310	470	A	0.95689	So all of this groundwork is helping us get to an analytical formalization that's going to have the right kind of properties so we can do that discretion right on the shadow Hamiltonian, so we can agent the accelerated optimization done.
4107560	4117240	471	A	0.92	3.2 hamiltonian Monte Carlo a challenging task consists of constructing efficient sampling algorithms with strong theoretical guarantees.
4117660	4131900	472	A	0.99998	We now discuss an important family of wellstudied methods known as Hamiltonian Monte Carlo HMC, which can be implemented on any manifold for any smooth, fully supported target measure that is known up to a normalizing constant.
4132560	4141180	473	A	0.99994	Some of these methods can be seen as an appropriate geometric integration of the underdamped Langevin diffusion diffusing down that hill.
4141340	4152240	474	A	0.99853	But in simpler, it is in general simpler to view them as combining a geometrically integrated deterministic dynamics with a simple stochastic process that ensures ergodicity.
4152840	4176472	475	A	0.99038	Equation 38 if we interpret the negative log density v of q as a potential energy a function depending on position q, one can then plug in the potential within Newton's equation to obtain a deterministic proposal that is well defined on any manifold.
4176616	4181912	476	A	0.98365	As soon as the acceleration and derivative operators have been replaced by their curved analogues.
4182056	4185860	477	A	0.99761	So what pulls the ball down the hill?
4185960	4192316	478	A	0.99935	Or, if you're an instrumentalist, what force describes the movement of the ball moving down the hill?
4192508	4199036	479	A	0.95286	Gravity, I think there's a John Mayer song about it surprises like gravity.
4199228	4209856	480	A	0.9995	It can be understood as the minimizing force that pulls the ball down the hill or describes the movement of the ball on its path of least action down the hill.
4210048	4211030	481	A	0.99985	How cool.
4212940	4229176	482	A	0.96588	More discussion around 38 bringing these ingredients together, we thus have the following HMC algorithm given dot dot dot.
4229368	4240316	483	A	0.79314	We're going to compute a function according to one, a heat bath, two shadow Hamiltonian dynamics and three metropolis correction.
4240508	4242000	484	A	0.99909	That's the recipe.
4242580	4244770	485	A	0.68295	We're bringing those ingredients together.
4245620	4267320	486	A	1.0	The above Rudimentary HMC method was proposed for simulations in lattice quantum chromodynamics, with M being the special unitary group Su N, and used a Hamiltonian dynamics ingeniously constructed from the Mauer carton frame to compute the partition function of discretized gauge theories.
4267900	4271880	487	A	0.99997	This method has later been applied in molecular dynamics and statistics.
4274400	4280540	488	A	0.99624	There are three critical properties underpinning the success of HMC in practice.
4281280	4289120	489	A	0.79	The first two are the preservation of the reference measure and the existence of a conserved shadow Hamiltonian for the numerical method.
4289460	4301220	490	A	1.0	The third critical property is the existence of splittings methods, for which all the composing flows are either tractable or have adequate approximations, namely the geodesic integrators.
4301640	4313930	491	A	0.99038	So geodesic methods buckminster, fuller, tensegrity, cybernetics, or however you think about geodesics as paths and curved space.
4314940	4323960	492	A	0.99994	That is the kind of approximation and splitting we're gaining access to, with appropriate distinctions on those balls rolling to the bottom of the hill.
4324860	4330188	493	A	0.99994	Let us also briefly mention some useful upgrades that have been proposed in recent years.
4330354	4336184	494	A	0.99396	So for those who it's been a few years since you checked in with this domain, this is a great place to look.
4336322	4358500	495	A	0.82	First, you can grant the method extra integration steps when the proposal is rejected, or you can use criterion that aim to ensure the motion is long enough to avoid random walks, but short enough that we do not waste computational effort, such as the no uturn examples, which is integrated in a lot of software packages.
4359080	4373264	496	A	0.78	Second, Modern HMC methods bypass this issue of slow convergence by replacing the heat bath with an ornstein olbeck process, which ensures the overall algorithm is irreversible.
4373412	4385580	497	A	0.8606	An ou process is like a Brownian diffusion with a linear regression, so it vibrates and has volatility, and it has a trend line that's linear.
4386240	4392768	498	A	1.0	Third, many modifications of the Rudimentary HMC algorithm only provide improvements when the acceptance rate is sufficiently high.
4392934	4401910	499	A	1.0	A third class of upgrades improve the acceptance rate by using the fact that the shadow Hamiltonian is exactly preserved by the integrator big point of the whole paper.
4402280	4414148	500	A	0.99993	Finally, the metropolis step can be replaced with a multinomial correction that uses the entire numerical trajectory, accepting a given point along it according to the degree by which it distorts the target measure.
4414324	4425608	501	A	0.99967	Some path based inference methods with some caveats all right, on to section four statistical inference with kernel based discrepancies.
4425784	4452304	502	A	0.99234	So, section four the problem of parameter inference consists of estimating an element theta star within big theta using a sequence of random functions or estimators theta hat n of omega onto big theta equations conveniently reproducing Colonel Hilbert spaces.
4452352	4468100	503	A	0.50982	RKHS are precisely Hilbert's spaces over which the Diroc distinctions, which is a statistical distinctions type where it's like a spike at one location and zero elsewhere, act continuously.
4468260	4473210	504	A	0.99386	So whether we're talking about events or spiking neurons, this is a really good property.
4473520	4485180	505	A	0.98	And more generally, the probability distributions that act continuously by integration on an RKHS h are exactly those for which all elements of h are integral.
4485620	4508560	506	A	0.61434	So analytically we have that nice property denoting by fancy piece of h, the set of such probability measures such that dot, dot, dot, we can define the maximum mean discrepancy, or Mmd, as such more definitions, a practical expression for the squared Mmd.
4508720	4514884	507	A	0.99988	So just like in linear regression, we're often interested in the sum of squared errors.
4515012	4518760	508	A	0.99943	Here we're interested in the sum of squared Mmd.
4520220	4521032	509	A	0.96	4.1.
4521086	4528940	510	A	0.7115	Topological methods for Mmds just remember Mmds maximum mean discrepancy.
4531120	4540240	511	A	1.0	A key feature of RKHS is that there are Hilbertian subspaces, more details about embedding in Hilbert spaces and subspaces.
4541780	4551220	512	A	0.84	The geometric Analysis the geometric description of RKHS and Mmd allows us to swiftly apply topological methods in their analysis.
4551800	4557430	513	A	0.99938	There are reasons this reduces the matter to a topological question.
4559800	4569348	514	A	0.99994	Instead of defining T star to be the set of probability measures it is commonly done to define statistical manifolds.
4569524	4582940	515	A	0.91181	It's desirable to embed franca p within a more structured space, such as the space of finite radon measures, which enables the method to learn the target function independently of the data generating distribution.
4583760	4585752	516	A	0.99209	What is a radon measure?
4585896	4587280	517	A	0.95815	We're going to find out.
4587430	4604870	518	A	0.9997	However, it enables the method to learn the target function independently of the data generative distinctions sounds pretty important if we want that tale of two densities Mmd can discriminate distinctions that makes it useful to do other things.
4605480	4606950	519	A	0.99912	Let's find out more.
4608760	4618676	520	A	0.99987	Section 4.2 smooth Measures and KSDS So more information on Mmds and about making them computationally.
4618708	4634904	521	A	0.44509	Tractable and citations to Stein's Method are provided for 195, which is the 1972 Stein paper, and 196, which is a paper written by some of the authors about Stein's Method meets statistics.
4634952	4645040	522	A	0.94	A review of some recent developments from 2021, and I thought it'd be fun to look at some figures from this paper and look at the abstract, so I'll put the figures up above.
4651220	4657976	523	A	0.42643	Stein's method compares probability distributions through the study of a class of linear operators called Stein operators.
4658108	4666280	524	A	0.99659	While mainly studied in probability and used to underpin theoretical statistics, stein's method has led to significant advances in computational statistics.
4666780	4672324	525	A	1.0	The topics that are discussed in that review include tools to benchmark and compare.
4672372	4683368	526	A	0.99934	Sampling methods such as approximate Markov chain, Monte Carlo deterministic, alternatives to sampling methods, control variant techniques, parameter estimations, and goodness of fit testing.
4683544	4702944	527	A	1.0	And from the paper they write the list of results given in this paper are but a mere sample of the ongoing activity in this newly established area of research at the boundary between probability, functional analysis, data science, and computational statistics.
4703072	4714412	528	A	0.99997	For instance, Stein's method has been used for designing sampling based algorithms for nonconvex optimization learning semiparametric multi index models and high dimensions and Bayesian statistics physics.
4714576	4734312	529	A	0.97208	Stein discrepancies have been used as variational objectives for posterior optimization, which is what we're all about free energy functionals, using variational inference as objectives for posterior approximation of inference and action free energy principle.
4734456	4737756	530	A	0.7	And these images have some nice intuitions too.
4737938	4743868	531	A	0.99809	Here we see, depending on our step size, we're getting different sampling.
4744044	4752390	532	A	0.99988	When our step size is to the negative fifth power, we're over sampling one part of the space.
4753560	4759376	533	A	0.99995	Then, as we make the step size larger, our samples are drawn from different distributions.
4759408	4769448	534	A	0.99962	So that's selecting the step size epsilon for a stochastic gradient Langvin dynamic and also from the anastasio paper.
4769614	4771930	535	A	0.93592	Here's another fun way to see that.
4772700	4789576	536	A	0.9999	Here we have that bowl that we're rolling the ball to the bottom of, and the Stein points and the Stein thinning, which is helping us understand maybe how starting the ball in different locations or critical locations accelerates what we're doing with optimization.
4789768	4792030	537	A	0.98956	So cool to talk about.
4792560	4807200	538	A	0.84	4.2.1 the canonical Stein operator and Poincare duality there are two fundamental theorems that help us understand the integral differentiate geometry of the manifold durham's theorem and the Poncare duality.
4807800	4816120	539	A	0.92	The former Durham's theorem relates the topology of the manifold to information on the solutions of differential equations defined over the manifold.
4816460	4830380	540	A	1.0	The latter, which contains the fundamental theorem of calculus, describes the properties of the integral pairing alpha beta of differential forms, which include the pairing of test functions with smooth measures.
4831360	4833150	541	A	0.99743	Let's learn more about it.
4833840	4845920	542	A	0.98	Four, two, two kernel stein discrepancies and score matchings technical definitions facilitating Stein variational gradient descent.
4847780	4854180	543	A	0.93	4.3 information geometry of Mmds and natural gradient descent.
4855000	4858900	544	A	0.99997	These tools have proved to be useful in a wide range of contexts.
4860120	4870920	545	A	0.99987	More information about the divergence and how divergence can improve the speed of coherence by following the natural gradient descent.
4872620	4876400	546	A	0.99428	Okay, red text speculation beware.
4876580	4885496	547	A	0.99734	So the operator, which is a phone routing system like ring ring, hello, operator is constructed in action.
4885528	4894080	548	A	0.95	4.2.22 colonel Stein discrepancies and score matching you can't have a Ttest without the T distribution.
4894420	4900480	549	A	1.0	The Ttest statistic is using a T distribution.
4901480	4923380	550	A	0.9996	So a class of fancy V, a set fancy V of vector Fields, or more generally, tensorfields whose image f under the operator ring ring has mean zero under Mu that's going to give us this discrepancy, the SD and the Stein variational gradient descent.
4923540	4933390	551	A	0.99119	So there's some class of V vector Fields such that the mean of something about them is zero.
4934880	4946800	552	A	0.99996	Does this enable the central limit theorem or just statistics more broadly, like the parametric and non parametric methods that we know and love from SPM?
4947620	4957860	553	A	0.99994	Does that mean zero enable us to use Gaussian methods, generalized Gaussian methods, generalized linear methods, SPM?
4958600	4973000	554	A	0.96	And does it enable the proposed smooth distribution, the one generating the shadow Hamiltonian that's getting inferred over, to be interpreted or used truly as a statistical distribution?
4973500	4979544	555	A	0.99508	It's easy to forget that not all distributions are formal probability distributions.
4979592	4986540	556	A	0.99999	For example, the area under the curve from zero to one of a probability distribution is one.
4986690	4988108	557	A	0.99997	Something has to happen.
4988274	4994450	558	A	0.99999	But not all functions have an area under the curve of one between zero and one.
4995060	5005216	559	A	0.99703	So in variational Bayesian inference, the kind which we do in active inference, in fact, the kind that's done in machine learning and statistics more broadly.
5005248	5009350	560	A	0.99961	But we're interested when action is a parameter that we're doing inference on.
5009720	5017620	561	A	0.55568	We're concerned with the extremely well behavior properties of a certain subset of distributions.
5017780	5020104	562	A	0.54104	So why are we doing this?
5020142	5032028	563	A	0.99999	Why is it important that we didn't need the input data that we could construct measures that are independent of the input data?
5032194	5035816	564	A	0.99992	Because we want to enable the tail of two density.
5036008	5046160	565	A	0.99917	We want to make generative models that can be used generatively, generative AI in the forward or the generative direction.
5046820	5056528	566	A	0.99967	But we also want to be able to enable the recognition distribution, which is from empirical data, to do hidden state inference.
5056704	5074200	567	A	0.99	And so just like least squares regression in the linear modeling case, where the sum of squares above and below the regression line should be as low as possible l two norm sum of squares minimization always works, never complains.
5075260	5098400	568	A	0.99998	We want that kind of ball rolling to the bottom of the hill with free energy functionals variational and expected free energy functionals as the optimized or satisfied imperative for optimal perception, which is signal processing, signals intelligence and control which is control theory or action selection.
5101700	5106916	569	A	0.99962	More technical details and equations 39 B and 39 C.
5107098	5116468	570	A	0.99916	So what does it mean that the resulting Stein discrepancy can be thought of as an Mmd that depends only on row and is known as a kernel Stein discrepancy.
5116564	5117610	571	A	0.99987	What is that?
5119820	5128860	572	A	0.99988	As we did previously, we can remove the super mum by rewriting the above as a super mum over some unit ball of continuous linear functional.
5129360	5136670	573	A	0.99996	Is this like simulating or modeling a sphere rolling on a landscape like we've been talking about?
5137280	5141036	574	A	0.99995	Is the ball of optimal radius for rolling on that landscape?
5141068	5142476	575	A	0.99993	Or what is being optimized?
5142588	5151730	576	A	1.0	And what is the unit, the scaling or the scale specificity that this scale friendly sphere is scaled to?
5153880	5159840	577	A	0.99826	Equation 42 and 42 a while in the Euclidean space yields the diffusion score matching.
5159920	5181512	578	A	0.99776	Citation 204 Barp et al again minimum Stein discrepancy estimators from 2019 they write the main strength of our methodology is its flexibility, which allows us to design estimators with desirable properties for specific models at hand by carefully selecting a Stein discrepancy.
5181656	5189124	579	A	0.99998	We illustrate this advantage for several challenging problems for score matching, such as non smooth, heavy tailed or light tailed densities.
5189272	5198640	580	A	0.81509	So again, just with a little bit of speculation here and I think my camera has frozen.
5206760	5207990	581	A	0.94349	It's all good.
5209240	5212340	582	A	0.67784	I'll just go without the camera.
5214360	5215110	583	A	0.90609	Okay.
5216920	5224200	584	A	1.0	One can estimate infer or optimize the zero point and hence the variance structure of the distribution.
5224540	5242076	585	A	0.99976	This is going to ensemble generalized wellbehaved modeling again, from all of those nice perspectives that we raised earlier, like Gaussian central Limit theorem, smoothness, statistical disturbance, Euclidean, all of those well behaved properties that we're looking for.
5242258	5244110	586	A	0.99995	This is going to help us get there.
5244480	5252880	587	A	0.99998	We are only talking about a specific subset or type of landscape here, the one that we're doing variational inference on, that's the map.
5253620	5256080	588	A	0.99997	This is not the structure of the territory.
5256680	5264900	589	A	0.99963	These well behaved attributes of models for better, worse and different through sickness and health is because they're maps.
5265720	5274280	590	A	0.99997	It is always the case that our statistically nice generative models are of a different structure or form than the generative process.
5274430	5279064	591	A	0.99868	So this is actually not a criticism of quantitative modeling as a process.
5279262	5283128	592	A	0.99998	In fact, this is the entire basis of quantitative modeling.
5283224	5286552	593	A	0.99765	So we've approached this map territory distinction.
5286696	5298780	594	A	0.97127	Map territory fallacy, fallacy, fallacy from many angles and one will still hear things like well, the structure of the statistical model is not the same as the territory.
5298940	5305250	595	A	0.9993	Or how can you say that the organism has these well behaved properties just because the model does?
5305720	5321672	596	A	0.99	And this brings a really sharp light on it, which is we're doing this insane amount of analytical groundwork so that the map has the good properties not to constrain what the territory is.
5321806	5327764	597	A	0.60259	So map territory, all of the math we've been talking about is map.
5327892	5335980	598	A	0.99993	We want wellbehaved maps so that we can describe wellbehaved and unruly territories.
5336320	5341004	599	A	0.99938	But it isn't the case that the organism minimizes variational free energy.
5341122	5345180	600	A	0.98763	It's the generative model that minimizes variational free energy.
5345330	5350576	601	A	0.99264	So, little bit of a leading discussion topic or question for you all.
5350678	5354960	602	A	0.99957	Feel free to give a thought in the live chat or write a comment or join our discussions.
5355560	5361236	603	A	0.99957	How is this line of basic and fundamental math research by Barp et al.
5361418	5371720	604	A	0.99729	Creating new models that have the operational or denotational semantics that we want for computational statistics, which is to say applied information theory.
5372060	5387230	605	A	0.99999	For example, distributions that can be interpreted or used as statistical distributions, ideally directly compatible with current computational methods SPM in MATLAB, Pi, Mdp in Python and fourney Lab in Julia and so on.
5388560	5398850	606	A	0.86423	Section 4.3.1 minimum Stein discrepancy estimators, more technical details, more inclination to 204.
5399220	5407250	607	A	1.0	The parameters can be adjusted to active characteristicness consistency, bias, robustness and obtain central Limit Theorems C 204.
5407620	5409764	608	A	0.94	2019 paper with Barp et al.
5409882	5414516	609	A	0.99	And let's just look at a really cool image from that paper.
5414698	5418208	610	A	0.99764	Figure one and figure two pretty cool.
5418394	5421770	611	A	0.84999	SD estimators looking nice.
5423420	5447190	612	A	0.9994	Section 4.3.2 likelihood Free Inference with Generative Models so for many applications of interests, the densities of the model mu sub data cannot be evaluated or differentiated.
5448010	5457370	613	A	0.99998	We thus need density free inference methods super convenient for Bayesian statistics, more technical details, information tensor.
5457870	5468590	614	A	0.9998	Under appropriate choices of kernels and models, one can derive theoretical guarantees such as concentration and generalization boundary consistency asymptotic normality and robustness.
5469090	5480590	615	A	0.99645	So, little bit of a summary we have good analytical and computational footing in certain kinds of situations or under certain constraints.
5480750	5497462	616	A	0.99995	Certainly not all constraints, but definitely for some that might enable the efficient computation, not just specification, but actual implementation of large generative models such as described in the paper of Friston et al.
5497596	5502458	617	A	0.97	2022 designing Ecosystems of Intelligence from shared sorry.
5502544	5510620	618	A	0.66477	Designing Ecosystems of Intelligence free Energy Principle And they use the term ecosystems of shared intelligence in that paper.
5510990	5515466	619	A	0.99	And so it's relevant to learn about the actuality and build intuition.
5515658	5516398	620	A	0.9974	Why?
5516564	5517214	621	A	0.9988	How do we know?
5517252	5533006	622	A	0.99963	Well, here's the one figure in that paper of Friston at all, the one figure in this absolutely positional paper that they have released believes as parameters of a probability distribution.
5533198	5550530	623	A	0.99863	Here's the sharpening of a belief as a distance belief updating as traversing a statistical manifold, which is to say a lower dimension projected space and what's being shown as the parameter space of a probability distribution.
5550610	5567470	624	A	0.99983	So whether we see this as gradient ascent to climb to the top of the hill or we take the negative and we have gradient descent to the bottom of the hill, this is the figure chosen by some very well informed authors to describe Bayesian mechanics.
5569730	5578290	625	A	1.0	And long last we get to section five adaptive agents through active coherence.
5579270	5593990	626	A	0.99992	We close, as the authors write, with a generic use case called Active Coherence, a general framework for describing and designing adaptive agents that unifies all aspects of behavior, including perception, planning and learning as processes of inference.
5594330	5612198	627	A	0.99999	By exploiting this geometric structure in a generic framework for designing adaptive agents, we derive the objective functional overarching decision making and describe its information geometric structure, revealing several special cases that are established notions in statistics, cognitive science and engineering.
5612374	5614186	628	A	0.99359	So section five one modeling.
5614218	5615694	629	A	0.99601	Adaptive decision making.
5615892	5619550	630	A	1.0	First, they're going to talk about behavior, agents and environments.
5620130	5624910	631	A	0.54219	Behavior is going to be defined as the interaction between an agent and its environment.
5625270	5629810	632	A	0.93	And the system is going to describe the agent plus its environment.
5631030	5639014	633	A	0.87025	There's going to be a set of states that are partitions from a statespace big x external states.
5639212	5646726	634	A	0.99	S are going to be unknown to the agent and constitute the environment states belonging to the agent.
5646908	5653690	635	A	0.91354	Pi particular states are going to be a subset of two different things.
5653760	5657530	636	A	0.9935	So here is S, which is here external states.
5657600	5661370	637	A	0.99824	Note that in some other settings S will refer to sensory states.
5661440	5671680	638	A	0.91841	So look at the notation in this paper and Pi, which in other papers is sometimes used to describe policy inference here is going to describe particular states.
5672050	5676706	639	A	0.5006	So Pi is O and A.
5676888	5690502	640	A	0.93	O are the observable states, states that the agent can see but cannot directly control sense states and A are the autonomous states that the agent sees and can directly control.
5690636	5694760	641	A	0.64	And those are going to be internal states and active states.
5695530	5704300	642	A	0.95821	Figure two has a great representation and again note the way that Pi, So and A are being used in this paper.
5705390	5711690	643	A	0.99	S is the external process hidden state, latent state, causal coherence.
5712370	5724480	644	A	0.64	O are the observations and the agent process concepts of Pi, which is the observations and alpha or A.
5725350	5737630	645	A	0.95	A is consisting of, according to the particular partition, two different kinds of states, which is internal states, cognitive states and action states.
5737800	5746630	646	A	0.98	And so we're going to compute bounds for free energy functionals with a special focus on autonomous processes.
5746970	5757286	647	A	0.99998	Because controlling our perception is not possible, and maybe not even preferable by controlling it at the level of what we observe.
5757478	5769626	648	A	0.99953	But rather, if we control our internal states, which is our interpretation of the perception and our action states, we'll be doing optimal perception and optimal action.
5769738	5777182	649	A	0.99949	So those are the two sides of the coin with inference as perception learning and action selection.
5777246	5779854	650	A	0.99995	That's how we unify action and inference.
5779902	5789426	651	A	0.88251	Inactive inference is by caring about the bounding of selfsurprisal, just like gravity on our autonomous processes.
5789538	5792680	652	A	0.99366	So this will be super fun to discuss more.
5794090	5804982	653	A	0.87	5.1.2 decision making and precise agents so, what distinctions people from small particles people are subject to classical as opposed to statistical mechanics.
5805126	5813710	654	A	0.99999	Check out Active Livestream 49 for more on Bayesian mechanics and some of these distinctions with classical statistical quantum and thermo.
5814290	5818030	655	A	0.95926	In other words, they are precise agents with conservative dynamics.
5818850	5825890	656	A	0.96244	Precise agent definition 5.1 an agent is precise when it evolves deterministically in a possibly stochastic environment.
5827830	5833982	657	A	0.99929	More definitions and it's useful here to highlight Friston et al's.
5834046	5889400	658	A	0.99983	Recent paper path integrals particular kinds and Lagrange things which provides a taxonomy of things taxonomy of particular entities that run the gamut of sophistication from inert particles which have no active states active particles which have active states generative process which have cognitive dynamics that are described as classical and strange particles which represent in this visualization the highest level of cognitive sophistication in which those internal hidden states themselves have this kind of what would happen if this happens counterfactual type or thinking through other minds type cognitive model structure doing forward and inverse inference on these great times.
5891130	5892422	659	A	0.87	5.1.2.
5892476	5902390	660	A	0.99492	Decision making and precise agents we have mathematical formalisms for decisions, preferences and predictions all using the partitioned variables.
5903210	5910378	661	A	0.94	5.12 more formalisms we get to expected free energy.
5910544	5915440	662	A	0.99982	Expected free energy is equation 41K.
5916850	5947190	663	A	0.98637	Active inference is Hamilton's principle of leased action on expected free energy and expresses the most likely decision where certain features are met principle of least action on manifolds of inference and action subsuming or bringing together inference and action active inference, free energy principle 5.1.3.
5947340	5952790	664	A	0.98627	Active Inference Framework AIF looks like it describes agents that engage in purposeful behavior.
5952950	5961578	665	A	1.0	We can rearrange the expected free energy EFE in several ways, each of which reveals a fundamental trade off that underwrites decision making.
5961744	5970990	666	A	0.62262	This allows us to relate active inference to information theoretic formalizations of decision making that predominate in statistics, cognitive science, and engineering.
5971810	5977198	667	A	0.99904	Figure three, as hinted ant in the early, early keywords.
5977374	5981902	668	A	0.99958	Here on the top, we have the general formalizations of active inference.
5981966	6003926	669	A	1.0	And one of the partitions, or better to say competition, is shown here, where extrinsic value is the surprising about observations, making sure that what we're getting in observations are what we expect slash prefer if the body wants to be expecting homeostatic temperature.
6004038	6006086	670	A	0.95561	That's what this is going to determine.
6006198	6027810	671	A	1.0	And it plays a role equivalent to reward in reward and reinforcement learning based approaches, because we don't need to actually set a reward function, but we get something that looks like selfsurprisal aligned with reward using the preference variable over observations.
6028630	6030194	672	A	0.57	A lot more to say there.
6030392	6039110	673	A	0.77	And the second term is the intrinsic value, the epistemic value, curiosity, novelty, learning, reduction of uncertainty.
6039530	6042950	674	A	1.0	And another partitioning is between risk and ambiguity.
6043850	6052646	675	A	0.99998	There are four colored dots red, tan, gray, and Bleu corresponding to special cases.
6052838	6068446	676	A	0.98858	So under the setting where there's no ambiguity, we realize or manifest the special case where control can be seen as inference, maximum entropy, reinforcement learning, prospect theory, and KL optimal control.
6068628	6077460	677	A	0.99996	This is like doing as good as you can strategically or tactically, given that there's no ambiguity in how your decisions play out.
6078230	6086862	678	A	0.99983	Now, under the setting where there's no ambiguity or preferences, a maximum entropy principle is realized.
6087006	6089474	679	A	0.99	And work with Bayesian mechanics.
6089602	6090966	680	A	1.0	And Dalton et al.
6091068	6101094	681	A	0.99748	Has shown that the constrained maximum entropy principle is dual to the FEP in the case of no extrinsic value.
6101292	6105622	682	A	0.99883	So no quote reward, no quote goals.
6105686	6124318	683	A	0.99256	We don't use reward or goals in the active inference ontology, but borrowing those words from some long, long forgotten ontology, in the case of no extrinsic value, we realized the special case of maximum information gain, Bayesian experimental design.
6124404	6138870	684	A	0.99986	So not to prove or disprove, but the maximally informative experiment that's the Bayesian scientific epistemology intrinsic implication and Bayesian surprise seeking out optimally informative stimuli.
6139530	6144280	685	A	1.0	And contrast that with a setting where there's no intrinsic value.
6144810	6156220	686	A	0.99059	So where there's nothing to learn, we realize expected utility theory, Bayesian decision theory, reinforcement learning, and optimal control.
6156990	6188402	687	A	0.99938	So special cases from many different Fields, ranging from statistical mechanics to Bayesian decision making and statistics integrated or perhaps better generalized across in the formalisms of active inference, which is something like a Hamilton's principle of least action on variational or expected free energy, expressing the most likely inference and action under certain constraints.
6188546	6191990	688	A	0.99996	Not the most rewarding, but the most likely.
6193770	6195318	689	A	0.86	5.1.3.
6195484	6198578	690	A	0.99991	Decision making minimizes both risk and ambiguity.
6198754	6210490	691	A	0.98853	Risk, first term on the right hand side, ambiguity second, term minimizing ambiguity leads to a type of observational bias commonly known as the streetlight effect.
6210640	6214362	692	A	0.99991	When a person loses their keys at night, they initially search for them under the streetlight.
6214426	6226110	693	A	0.99996	Because of the resulting observations, I see my keys under the streetlight, or I do not see my keys under the streetlight accurately disambiguate external states of affairs.
6226450	6234066	694	A	1.0	First place to look make sense under the streetlight, and the last place you look is where you find it more in.
6234088	6235346	695	A	0.89	5.1.3.
6235448	6249734	696	A	0.47442	Decision making maximizes extrinsic and intrinsic value another decomposition of the formalisms of active inference maximizing information gain leads to a goal directed form of exploration driven to answer what would happen if I did.
6249772	6263354	697	A	0.99993	That true counterfactuals this decision making procedure underwrites Bayesian Experimental Design in Statistics, which describes optimal experiments as those that maximize expected information gain.
6263482	6284222	698	A	0.99	And we've had some great discussions recently in textbook groups and in discussion hours, where we contrasted that falsificationist concept of accept or reject hypothesis, and even the idea of a scientist or a researcher setting out to accept or reject hypothesis.
6284286	6296630	699	A	0.99987	Whether you take that Positivist or you take the other path of falsificationism, in both cases, you might end up with an informative experiment or not.
6296780	6310598	700	A	0.52784	But you can easily imagine cases where you end up with an uninformative experiment because you set out to confirm something you knew, or you set out to disprove something by constraining your experiment so that it was locally disproven.
6310774	6331060	701	A	0.99997	In contrast, when we take into account the richness of our generative model, we motivate this Bayesian epistemology and ultimately professional Bayesianism, where we make optimal experiments in terms of their maximum expected information gain, which requires you to also state your generative model.
6332710	6342950	702	A	0.94704	Decision making under active inference weighs the imperatives of maximizing utility and information gain, which suggests a principled solution to the exploration exploitation dilemma.
6343770	6345718	703	A	0.99998	Great point to jump into.
6345884	6354970	704	A	0.99963	Does active inference, simply by written down, by being written down on a paper, resolve or transcend or dissolve exploration exploitation?
6355390	6356140	705	A	0.99913	No.
6356670	6368858	706	A	0.99965	Does it provide a space or a framework, a method, an approach, software packages, a research community that can help us address and navigate and surf on the edge of that dilemma?
6369034	6369760	707	A	0.99985	Absolutely.
6371650	6372862	708	A	0.87	5.2.1.
6372916	6380382	709	A	1.0	The basic active inference algorithm we're going to go through it with authors preferential inference.
6380526	6384690	710	A	0.99994	We want to infer preferences about external and observable trajectories.
6385830	6389762	711	A	0.86	Two for each possible sequence of past, present and future actions.
6389826	6398838	712	A	1.0	A we're going to engage in both sides of the coin perceptual inference what would I perceive if that happened?
6399004	6404826	713	A	0.97	And planning as inference assess the action sequence by evaluating its expected free energy.
6405008	6425810	714	A	1.0	And then three, decision making execute the most likely decision at T plus one, according to some distinctions, sample from your action posterior, action prior that's, like your habits, it gets sharpened or modified with expected free energy, and then you sample from the action posterior.
6427350	6428562	715	A	0.96	5.2.1.
6428616	6440646	716	A	0.98668	Sequential Decision Making under Uncertainty a partially observable Markov decision process POMDP is a discrete time model of how actions influence external states.
6440828	6446614	717	A	0.99998	In a POMDP, each external state depends only on the current action and previous external state.
6446652	6448120	718	A	0.99959	That's the Markov property.
6448650	6457130	719	A	0.99998	Each observation depends only on the current external state, and one can additionally specify a distribution of preferences over external trajectories.
6457950	6461980	720	A	1.0	One and two form the agent's POMDP prediction model.
6463330	6470160	721	A	0.99	Two and three form the agent's hidden Markov preference model, which defines the active inference agent.
6470930	6475698	722	A	0.87	A simple simulation of active inference on a POMDP is provided in figure four.
6475864	6479346	723	A	0.99978	Implementation details on generic POMDPs are available.
6479528	6483746	724	A	0.99962	For more complex simulations of sequential decision making, e.
6483768	6489110	725	A	0.78	G involving hierarchical POMDPs, please see citations.
6490090	6491334	726	A	0.99508	Figure four.
6491532	6493174	727	A	0.9861	It's a prediction model.
6493372	6497666	728	A	0.99983	This is sequential decision making in a Team A's environment.
6497858	6503878	729	A	0.97909	On the left, the agent's prediction model as a POMDP, represented here as a Bayesian network.
6504054	6506726	730	A	0.99999	On the right, information on the team A's.
6506838	6522080	731	A	0.58477	So here we have some type of Bayesian graph reflecting hopefully a statistical system that's going to have all of these well behaved properties that we've been talking about in the paper.
6522530	6526830	732	A	1.0	And then the team AZ is played out.
6526980	6528320	733	A	0.99737	We'll talk about it.
6529010	6530114	734	A	0.98	5.2.3.
6530152	6536466	735	A	0.99702	World model learning is inference due to a lack of domain knowledge, it may be challenging to specify an agent's prediction and preference model.
6536568	6539362	736	A	0.99972	For example, how do external states map to observations?
6539506	6543240	737	A	0.99997	Should external states be represented in a discrete or continuous state space?
6543690	6547382	738	A	0.99983	Also, some great questions that come up every day.
6547516	6557450	739	A	0.5	And Clark addressed in chapter six a recipe for active inference modeling by the Par Pazulo and Friston 2022 textbook.
6558190	6564990	740	A	0.99962	So how do we get the right priors, or at least approximately adequate priors?
6565330	6572618	741	A	1.0	One way to answer the question lies in optimizing a free energy functional F, which is an evidence lower bound.
6572794	6582290	742	A	0.99887	We see here variational free energy decomposed into an energy minus entropy and decomposed into a complexity minus accuracy.
6582710	6603930	743	A	0.91401	Framing maximizing accuracy usually results in generative models involving universal function approximators minimizing complexity results action action oriented representation sparse compartmentalized in hierarchical generative models where higher levels of the hierarchical model more abstract representations, and vice versa.
6604270	6616540	744	A	1.0	A computationally efficient method to compare priors by their free energy is Bayesian model reduction, free energy, unifies inference, and model selection under a single objective function.
6618830	6620126	745	A	0.76	5.24.
6620228	6626690	746	A	0.97582	Scaling active inference planning for all possible courses of action is computationally expensive.
6627030	6635858	747	A	1.0	One way to finesse this is by planning only for intelligently chosen subsets of action sequences using sampling algorithms like Monte Carlo Tree Search.
6636024	6641990	748	A	0.99995	If you're interested in that, check out the recent research on branching time active inference.
6642570	6648070	749	A	0.99991	Similarly, Monte Carlo sampling finesses the expectations inherent in assessing action sequences.
6648730	6659660	750	A	0.76	A complementary approach is to assess actions instead of action sequences by conditioning all future actions to be optimal in the sense that they minimize the expected free energy.
6660510	6669550	751	A	0.99995	It leads to smarter agents whose computational complexity scales linearly as opposed to exponentially in the length of action sequence.
6670210	6671920	752	A	0.90742	Let's learn more about it.
6673730	6678638	753	A	0.98697	Scalable inference methods can be used to make active inference more efficient.
6678814	6699030	754	A	0.99998	We can train neural networks to predict the various posterior distributions, including the posterior over action while training the output of the neural network can be used as an initial conditions for variational coherence, resulting active inference whose computational costs decrease as the network learns.
6699370	6709702	755	A	0.99444	Additionally, optimizing free energy reduces to efficient message passing schemes when one imposes certain simplifying distinctions to the family of candidate distributions.
6709846	6726420	756	A	0.99979	Lots of really exciting work in neurobiology and statistics around message passing getting close to the end here a much cheaper implication of active inference exists for continuous states evolving in continuous time.
6727670	6728658	757	A	0.99992	Pretty cool.
6728824	6729714	758	A	0.99613	Par et al.
6729752	6748280	759	A	0.99	2022 Textbook Chapter Seven Discrete time Active Inference generative Models Chapter Eight Continuous time Active Inference Models this method frames perception and decision making as variational inference by simulating a gradient flow on free energy in an extended state space.
6748810	6755900	760	A	0.99994	It can be combined with discrete active inference to operate efficiently in generative models combining discrete and continuous states.
6756270	6776750	761	A	1.0	We talked about that in live stream 46 Active inference does not contradict folk psychology, discrete active inference decision making, active inference dai, and continuous time motor active inference being used together, also seen in the paradigm textbook.
6776910	6780946	762	A	0.99999	As an example, high dimensional observations in the continuous domain, e.
6780968	6788166	763	A	0.89	G speech processed through continuous active inference are converted into discrete abstract representations, e.
6788188	6799842	764	A	0.64	G semantics, and we can even go further and say rhetorical and narrative information spaces based on these representations, the agent makes high level categorical decisions, eg.
6799906	6805838	765	A	0.81	I want to move over there, which contextualize low level continuous actions, eg.
6805874	6813178	766	A	0.54	The continuous motion of a limb towards the goal location and that is how the paper ends.
6813354	6818878	767	A	0.99354	So, closing thoughts if anybody wants to write a comment live, feel free to do so.
6819044	6820910	768	A	0.78799	We'll be over very shortly.
6821730	6825940	769	A	0.89118	What are the implications of this work?
6826870	6831060	770	A	0.99974	We have an open space to talk about it.
6833190	6838200	771	A	0.9999	What questions and discussion topics are you interested in?
6838730	6846310	772	A	0.99994	Please write comments before or after the dot one and the dot two so we can have those interesting discussions.
6846970	6872238	773	A	1.0	And just as a little bit of a closer, I'll share some stable diffusion images that were generated using the paper title as well as various other terms free energy, lots of fun images, a lot of good balance to some of the technical aspects that were being described in the paper.
6872404	6883518	774	A	0.91	Was good to look at what diffusion looks like aesthetically and so that is the end of this Livestream.
6883694	6905020	775	A	0.91	52.0 I hope that you found it useful that you're interested to act in first, serve to learn more, contribute more, write a comment, make it happen in your own life or in your own way with this honestly challenging paper.
6905470	6921610	776	A	0.98413	So if you've listened this far, thanks a lot for your attention and looking forward to 520 One and Zero Two when we will speak with some authors and some of you.
6921760	6927726	777	A	0.96678	So till next time, thanks again and see you in the dot.
6927758	6928340	778	A	0.95	One.
6928950	6929230	779	A	0.41304	Bye.

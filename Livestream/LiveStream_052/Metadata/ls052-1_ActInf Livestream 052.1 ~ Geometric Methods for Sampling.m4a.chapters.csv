start	end	startTime	summary	headline	gist
7960	49960	00:07	Activem Livestream 52.1. Welcome to the active institute. We're a participatory online institute that is communicating, learning and practicing applied active coherence. All backgrounds and perspectives are welcome. Please provide us feedback so we can improve our work.	Activem is a participatory online institute that is practicing applied active coherence	Active Institute Livestream
50940	106510	00:50	We are in 52.1 and our goal today is to learn and discuss this awesome paper geometric Methods for Sampling Optimization, Inference and Adoptive Agents of Barp and Acosta. Thanks a lot Lance for joining.	We are discussing geometric Methods for Sampling Optimization, Inference	52.1
107280	333110	01:47	Today we're going to be discussing geometric methods for sampling optimization, inference and adaptive agents. Lance de Costa wanted to see how different approaches to statistics can benefit from each other. Can we put all these Fields together and have a connecting line which is based on geometry?	This paper discusses geometric methods for sampling optimization, inference and adaptive agents	Invergence of statistics, inference and decision-making
335880	630070	05:35	Sampling is like drawing a data point or an observation from an underlying probability distribution. Why is this useful in the first place? Well, it just happens all the time in the things that you want to computer integrals. You can use it to describe perception in humans and to solve all kinds of problems.	What is sampling and how is it being used	Bayesian Inference, sampling and inference
630440	765710	10:30	In order to do Bayesian inference, you need to compute this, like, log likelihood term, which is a high dimension integral. One way to do it is by using Monte Carlo methods. These are based on sampling. Sampling also has advantages with respect to variational inference.	In order to do Bayesian inference, you need to compute log likelihood term	Sampling in Bayesian Inference
770020	1169330	12:50	An integral is basically, let's say you have a state space. What sampling does is it's going to pick a few locations on the desk, as many as you wish. The beauty of this is that it works in the arbitrary dimensions and it doesn't suffer from the curse of dimension.	An integral is basically, let's say you have a state space	Integral sampling in machine learning
1172340	1988230	19:32	To do sampling, you need to pick places at random. There are ways to sample that do not require what this paper is talking about. It's very hard in general to sample from a gives measure. The most common way to solve from this distribution is what's called Markov Chain Monte Carlo.	This is basically like the heart of sampling. To do sampling, you need to pick places at random	Bayesian Inference 6, Sampling
1990600	2703290	33:10	In order to have adaptive agents and to scale adaptive agents, one useful tool is sampling. How could sampling be used in the context of active coherence lab and adaptive agents? Are we thinking about this sampling process as being guided by an adaptive agent?	Hamiltonian monte Carlo has guarantees. Hamiltonia Monte Carlo has an accept rejects step	Hamiltonian Monte Carlo and the sampling of inference
2704320	2952600	45:04	Expected PNG is used in expected utility theory, basic decision theory, RL, optimal control. It's also used in Bayesian experimental design which is in statistics. Fuentes et al use it to solve nontrivial machine learning, slash reinforcement learning problems. But to actually do decision making efficiently in practice efficiently you actually need sampling.	You can view expected PNG as weighing expected reward and expected information gain	Expected PNG and Bayesian experimental design
2952750	3630560	49:12	 adaptive sampling is vital to maintain the coherence of that generative model. There's other ways to scale active inference. One other way could be what's called amortizing. But it's not something that you can use right away.	Adaptive sampling is vital to maintaining coherence of generative model	Active Inference: Scale the Coherence
3631000	4168930	1:00:31	The challenges with active inference are the scaling up. In doing so, active imprint becomes closer to reinforcement learning. People want to build reinforcement learning agents that are increasingly learn in an unsupervised manner. These two approaches are converging.	There are some challenges with active inference and anybody using active inference would be familiar	Inactive Inference and its challenges
4170740	4783570	1:09:30	The paper shows how far the community has gone between making a theory that meets the state of the art. It's very beautiful and quite unique and also quite rare when the theory meets the practice. It'll be quite interesting to see how this develops on the theory, practice and social frontiers.	Axel Costa: It's rare when theory meets practice in optimization	Inferring from the Theory of Agents
4784760	5560560	1:19:44	Expected free energy does this in some way. It provides an answer to this question based on your preferences and how much you would gain by observing this new data. This is what expected PNG does, and it gives you a ranking.	The expected free energy gives you a ranking based on preferences and information gain	The expected free energy of policies
5560930	6145210	1:32:40	There's so many ways, so many approaches to optimization. Here we chose to focus on optimization of smooth functions. The idea is that you have a smooth landscape, you want to find the minimum. One way to do so is through the use of a tool called geometric inclination.	The sections broadly cover optimization of smooth functions	Introduction to Accelerated Optimization (4)
6146830	6502700	1:42:26	All of Newtonian mechanics can be reformulated as Hamiltonian mechanics. The Hamilton Jacobi equations of motion can be implemented on a computer using Symplectic integration. These optimization methods are accelerated in a physical sense.	All Newtonian mechanics can be reformulated as Hamiltonian mechanics	Hamiltonian Flows in optimization
6505650	6693510	1:48:25	In the third action. We have sampling. The sampling can be seen as an optimization of probability distributions. And then in free. 2, we put this all together in Hamiltonian Monte Carlo, which is a state of the art method for sampling.	Sampling can be seen as an optimization of probability distributions	Advocated Sampling 3.6
6695710	6975060	1:51:35	In statistics and machine learning, we often need to compute expectations with respect to some probability distribution. This section derives a whole bunch of divergences. Like the Kel divergence. It can be used to approximate this unknown distribution with some other distributions that are tractable.	Section four of the book is on variational inference	Inference 4, Exploring the KL divergence
6975430	7034010	1:56:15	And then finally ant activation through active coherence. Well, I think we've discussed that already extensively. Thanks a lot for that overview. Looking forward to starting with some questions from the live chaos and people's comments on the video between now and next week.	And then finally ant activation through active coherence. Well, I think we've discussed that already extensively	Anticipatory Coherence: An Overview

start	end	speaker	sentiment	confidence	text
7590	8618	A	0.872096598148346	Hello and welcome.
8784	9302	A	0.6326377391815186	It's.
9366	14940	A	0.9391399025917053	ActInf Lab Livestream 52.22 on March 9, 2023.
15870	17990	A	0.7161543369293213	Welcome to the Octemf Institute.
18150	24330	A	0.5296252369880676	We're a participatory online institute that is communicating, learning, and practicing applied active inference.
25230	28038	A	0.9076153635978699	You can find more information on these links.
28214	33940	A	0.5522209405899048	This is a recorded and archive live streams, so please provide feedback so we can improve our work.
34310	47670	A	0.8773496747016907	All backgrounds and perspectives are welcome, and we'll be following video etiquette for Livestreams head over Active Inference.org to learn more about getting involved with projects and learning groups.
48890	54120	A	0.6253659129142761	All right, we are back in Livestream 52.2.
55150	63340	A	0.9045312404632568	In our third discussion on the paper geometric methods for sampling, optimization, inference and adaptive agents.
64350	76480	A	0.9084270596504211	We had a zero background and context video and last week Lance joined for 52.1, where we had a great overview discussion on the paper.
76930	87026	A	0.6753029227256775	So today we're going to see where it goes, see where our last week has taken us and how we're thinking about it or curious about it.
87128	92130	A	0.6649476885795593	If you're watching live, of course, feel free to write questions in a live chat.
92650	98550	A	0.8261182308197021	Otherwise, let us pick up on a mostly blank slide.
98890	103346	A	0.9307208061218262	And just thanks again, Lance, for joining.
103378	112300	A	0.7943338751792908	If you want to give any sort of introduction or recap opening here, go for it.
112990	116762	B	0.948283314704895	Well, yeah, I mean, thanks a lot, Daniel, for organizing this.
116816	120462	B	0.9909055233001709	I'm super happy to be here, I think.
120516	126122	B	0.8258535265922546	Well, last time we went through most of the paper, we discussed a lot about sampling.
126266	127914	B	0.7494072318077087	I guess the idea of sampling.
127962	137620	B	0.7733634114265442	We discussed a bit less about Hamiltonia Monte Carlo, which is one of the, I guess, state of the art methods for sampling in continuous space.
138870	141250	B	0.7051839828491211	We discussed a lot about optimization.
141990	151000	B	0.966555655002594	There's a nice figure actually that we can discuss today which gives some nice intuition about the kind of optimization methods that we reviewed in the paper.
152410	155750	B	0.805625855922699	And then there's another section on statistical inference.
155910	164780	B	0.8171494603157043	So this is a bit of a different section than people in the free energy principle literature and active inference are used to.
165230	176510	B	0.8056471347808838	Because here the goal of inference is about approximating expectations as opposed to just approximating distributions in of themselves.
176660	181518	B	0.8435831069946289	It turns out to be these two perspectives turn out to be dual.
181694	195030	B	0.6752769947052002	But I guess here we want to develop notions of divergences and discrepancies that are a bit more general than the KL divergence and that can use to solve problems that the KL divergence cannot.
196090	207078	B	0.8588318824768066	And I guess the overall picture for why we want to do that is the Kale divergence turns out to have a lot of really nice properties that we can discuss.
207244	218798	B	0.7352036833763123	One of them is that if the Kale divergence is reduced, it means that the two distributions in play are more similar in terms of information.
218964	232210	B	0.8909074068069458	So there is this idea of information monotonicity where the KL divergence sort of gives an ordering as to what extent two distributions quantify.
232710	242706	B	0.7860121130943298	So if you had three distributions and you compute the KL divergence between them and now KL of AB is lower than KL of AC.
242808	248678	B	0.7559788227081299	It means that B is more closer in terms of information to A than C is to A.
248764	255734	B	0.8550676703453064	So you have this really nice thing that is captured by the Kel divergence, which makes sense when we're dealing with information.
255932	259530	B	0.9594688415527344	The Kel divergence also has so many other nice properties.
261230	265798	B	0.8170661330223083	It's not a distance, but it turns out to behave a bit like a square distance.
265974	270030	B	0.8397483229637146	So you have this kind of like, really nice pitagorian theorem.
271730	283038	B	0.8147531747817993	And I won't get into the exact statement, but it's like if you have a B and C distributions, then KLAB plus klac equals KLBC.
283134	294020	B	0.8975421786308289	If you have like a rectangle triangle, if ABC defines a rectangle triangle in information space, you have other properties like the KL divergence gives and so on.
294950	301266	B	0.8706421256065369	So the KL divergence is in general, I would say, the distribution, the divergence of choice.
301378	304134	B	0.859900951385498	But it turns out that in many cases you just can't use it.
304172	312538	B	0.754143238067627	For example, when you have samples and you want to approximate some samples with a distribution, then the Kale divergence just is not going to work there.
312624	314986	B	0.727860152721405	So you need to derive some other things.
315168	324480	B	0.8718718886375427	So this is to say that we're considering statistical inference a bit more generally than what we do in active inference in general.
325010	335040	B	0.7054154276847839	And so this speaks to why the section here is a bit different from the standard inference literature that we usually consider.
335490	353766	B	0.6612529158592224	And then I think that the well, then this section five, which is about active inference, and I think we should discuss that a little bit more because the formation active inference lab that's presented there is to my mind the most general and also the simplest conceptually that's been out there.
353868	365578	B	0.8951016664505005	I mean, we sort of recap the derivation active inference lab and also like the properties of the expected community properties of active inference and also how to scale active inference and so on.
365664	369226	B	0.7577393651008606	And we do that in just seven pages or five pages.
369258	370640	B	0.6564356088638306	I mean, it's just very short.
371650	375978	B	0.5339925289154053	So it stands, sure, but it's like a really concise summary.
376074	381386	B	0.5667319893836975	And actually from there you can really derive a lot of technical papers.
381418	395640	B	0.6440050601959229	Active inference lab, to me, it's the most general, and if you as a reader can understand that section, then you can sort of really understand what active inference is about.
397370	399800	B	0.8864392638206482	This is kind of the overview for today.
401210	407210	B	0.5983370542526245	I would really like to be discussing questions because last time we read discussed about most of the papers.
407550	409740	B	0.7045061588287354	But yeah, whatever comes up.
411070	411820	A	0.9184247851371765	Awesome.
412190	413850	A	0.8581835031509399	All right, great.
414000	437170	A	0.8560387492179871	Well, let us talk about some of the more foundational pieces hamiltonian Monte Carlo and then on through section four with the points that you raised about the KL, which generalized inference beyond how it may have been brought up in other octave.
437510	450760	A	0.922050416469574	And then we can spend most of the time in section five and looking at some of those figures connecting some of the intuitions about the ball rolling down the bowl to the person running.
451690	452246	A	0.617027759552002	So.
452348	455074	B	0.7797701358795166	Sounds good on Hamiltonian?
455122	455842	A	0.7497230768203735	Monte Carlo.
455906	457640	A	0.8212675452232361	Where do you want to pick up?
458410	458918	B	0.4753468334674835	Sure.
459004	465980	B	0.6136986613273621	I meantonian so last time we discussed a lot about the problem of sampling and why that's the difficult problem.
467650	473134	B	0.7852327823638916	And we arrived at the conclusion, or I presented like Monte Carlo methods, how they work.
473172	494280	B	0.8526775240898132	So you're basically running a stochastic process, like a random motion and sort of the distribution defining the process is going to convert your target distribution, which means that when you run the process long enough, then every point that it's going to be in is going to be like a sample of the distribution that you want to sample from.
495290	497480	B	0.8356938362121582	Now, there's a lot of issues with that.
498490	516438	B	0.5471005439758301	Conceptually, it's not so difficult, but actually, when you want to implement this in practice, it turns out to be really hard because if you just implement the simplest stochastic process to sample your target distribution, it's going to be extremely slow.
516534	521674	B	0.5585731863975525	And I think that's the main bottleneck when developing Monte Carlo methods.
521722	524590	B	0.7965779900550842	Monte Carlo sampling in general is slow.
525090	533390	B	0.8271455764770508	And that's also one of the reasons why one might want to do variational inference instead of sampling.
534770	537758	B	0.4799391031265259	So something is slower, but it's also more accurate.
537854	541806	B	0.8134602308273315	It can approximate distributions that are completely arbitrary.
541918	548034	B	0.7008858323097229	So if you care about accuracy and you have time and computational resources, then for sure go for sampling.
548162	555898	B	0.586751401424408	If you care about speed, about doing things online, and you don't care about accuracy so much, then the variational inference is the way to go.
555984	558780	B	0.6400212049484253	At least that's my understanding right now.
559230	564698	B	0.8877734541893005	So let's say you wanted to sample a distribution in a continuous space.
564784	567514	B	0.7264935970306396	So it could be just as last time.
567552	578400	B	0.5955881476402283	Let's imagine the state space is the desk where I'm at, and you sort of have the distribution, which could be like multimodal, very weird, and you just want to take samples from there.
579410	585842	B	0.7588160037994385	HamiltonI Monte Carlo is probably the state of the art methods method to do that.
585976	594930	B	0.9415321350097656	There's a lot of other methods out there, but Hamilton Monte Carlo typically just works very well and in a wide variety of situations.
595350	608406	B	0.8634635806083679	Instead, the idea of HamiltonI Monte Carlo is you're going to augment the state space with so let's say that the original state space you start with are the positions.
608598	613100	B	0.8529919981956482	And so you're going to a mend that with a velocity state space.
613790	621710	B	0.7934537529945374	If your original state space was Euclidean space of N dimensions, you just end up with a nuclear space of two N dimensions.
622130	624480	B	0.7927359938621521	So you double the size of the state space.
625090	634050	B	0.6006459593772888	And now you say, okay, well, the distribution that I want to stumble from actually defines an energy landscape.
634950	645406	B	0.8836233019828796	So technically it's like if you have distribution which is P, then minus lock, p is an energy landscape.
645518	652150	B	0.8481480479240417	So points where minus lock P is low are points where P is high.
652220	655320	B	0.7449562549591064	And so these are points that you want to sample a lot from.
655850	662522	B	0.730699360370636	And contrary wise, if minus Lock t is very high, then P is low.
662576	667370	B	0.643608808517456	And you don't really want to go there so much because these are points of low probability.
668110	676282	B	0.724147617816925	So I think this minus log P, actually there's many reasons why minus Lock P is meaningful in physics.
676346	695560	B	0.6417208909988403	But just note for now that minus Lock P is just like a function and the minimum of the functions of that function are the high probability points and these are high energy points where you basically don't want to go there so much.
696330	700502	B	0.8441047072410583	So let's call this minus Lock P potential energy.
700636	704310	B	0.7980833649635315	And so this is what this really is in physics.
705630	712170	B	0.8677977919578552	Technically in physics, P would be a gives measure, what people call an equilibrium distribution.
713950	717838	B	0.8796207308769226	Last time we saw like soft max, minus something.
718004	723086	B	0.8050673604011536	The something is the potential energy anyway.
723188	727200	B	0.8177226185798645	And so minus Lock P would be the potential energy.
728050	739230	B	0.8855037093162537	And then if you add kinetic energy on your velocities, then you get what's called a Hamiltonium, which is the sum of the potential and kinetic energies.
739390	743062	B	0.8809929490089417	So what is exactly kinetic energy?
743116	746280	B	0.8785428404808044	The kinetic energy is like velocity squared pretty much.
746730	752786	B	0.8614341020584106	So if you add remember that your state space is position and velocity.
752978	766806	B	0.8765193819999695	If you add and you take a point in state space, so you would have kinetic energy, which is velocity squared, and a potential energy which would be minus lock peak.
766998	774062	B	0.8550965785980225	Now, if you add the two together, what you get is a Hamiltonian, which is the sum of the kinetic energy and potential energy.
774196	775930	B	0.5556060075759888	And this is just standard physics.
776010	781780	B	0.8169292211532593	Hamiltonian is the sum of kinetic and potential energy and it gives us the total energy of the system.
782870	786750	B	0.5015711784362793	So why we start with the problem of sampling.
786830	794630	B	0.6730159521102905	And here I just told you something very complicated where we get a Hamiltonian, there's actually a good reason why I want to do that.
794780	817642	B	0.5593991279602051	And it comes back to the idea of geometric integration that we talked about last time, which is that typically maybe I have a process that I know will give very efficient sampling, but actually when I implement it on the computer in discrete time, I just lose all the properties that make that sampling efficient.
817786	830338	B	0.7393562197685242	So actually it turns out that most people working in Monte Carlo sampling, they're working on efficient discretizations of continuous processes as opposed to on the continuous process themselves.
830504	831694	B	0.5211597681045532	Really the bottleneck.
831742	844630	B	0.656792402267456	And the difficulty here is the implementation part, implementing process on a computer like you and I had in such a way that we retain all the good sampling properties.
845370	853400	B	0.7917187809944153	So one very powerful idea is that of geometric integration, which is of preserving geometric properties of a system.
854090	868320	B	0.7374863624572754	Geometric integration is the field that develops computational methods of numerical integration and numerical discretion in such a way that they preserve important geometric properties of the system.
869250	873626	B	0.8533109426498413	Here, the geometry in play is a Hamiltonian.
873818	876554	B	0.713550329208374	And so you might think, well, that's pretty weird.
876602	884014	B	0.8643797039985657	I mean, a Hamiltonian is an energy here we're talking about preserving the geometry and preserving the Hamiltonian.
884062	886706	B	0.6839877367019653	How do these two things fit together?
886888	903410	B	0.8207389116287231	And so it turns out that the presence of a Hamiltonian and the fact that we have a state space that has positions and velocity, technically, in mathematics, what we then get is what people call symptom geometry.
903570	913850	B	0.8577916622161865	So symptom geometry just arises when we have those states that comprise positions and velocity and where you have Hamiltonians.
914350	925550	B	0.7948856353759766	So this is just like a way of explaining why the Hamiltonian is closely and intimately related to geometry.
926130	936798	B	0.6470468640327454	So geometry integration enables you to discrete processes in such a way that the Hamiltonian is preserved.
936974	944360	B	0.810041069984436	Now, when you think about the Hamiltonian, the Hamiltonian gives you the energy of a particular point in space.
945290	959770	B	0.859153151512146	And so if you simulate trajectories that preserve the Hamiltonian, what you're effectively doing is sampling from the contours of the probability distribution, contours that have the same probability.
960670	975994	B	0.8257609009742737	So basically going around, let's say, for example, in circles in a region that has the same probability, when we want to sample, we want to go everywhere.
976042	982340	B	0.8133566379547119	I mean, we want to sample regions of high probability, low probability, and want to be able to go from one to the other.
982870	995480	B	0.8147867918014526	So what geometric integration allows us to do is to simulate dynamics that are going to preserve the contours of the probability distribution, and they're going to do so very well.
996810	1013962	B	0.5588381886482239	The advantage here is that when we use geometric integration to simulate Hamiltonian dynamics, which are conservative and again, staying the contours, what geometric integration allows us to do is to take time steps that are very long.
1014096	1020346	B	0.6063318848609924	So it enables us to travel very far in the landscape while still preserving the Hamiltonian.
1020458	1026986	B	0.803878664970398	So you don't need to take very small time steps to remain accurate and preserve the Hamiltonian.
1027018	1033330	B	0.498520165681839	But actually, dramatic integration allows you to take very long time steps and still preserve the Hamiltonian.
1033670	1045670	B	0.8772408962249756	So now we have a dynamic that's Hamiltonian preserving that's just very good because you can take very long time steps, and that enables you to go around the contours of the probability distribution.
1046570	1050786	B	0.8742460608482361	So what you want to do next is to change contours.
1050818	1057530	B	0.8336510062217712	You want to go to contours that are of high probability, contours of lower probability.
1058110	1068830	B	0.7322444319725037	And you want to do so in a way that the contours of high probability are sampled much more and much more often than the contours of lower probability.
1070610	1081010	B	0.8132196664810181	So what you do is that you augment your Hamiltonian dynamic with what people call a velocity refreshment or a momentum refreshment.
1081430	1084660	B	0.6975398063659668	And so this is how it works.
1085990	1103910	B	0.8329535722732544	So you're going to simulate your Hamiltonian dynamics for some time, and then after a while, you're going to be like, okay, well, now I'm going to change the velocity at random by just drawing a new velocity from a Gaussian distribution.
1104430	1111020	B	0.76978999376297	And so by drawing a new velocity from a Gaussian distribution, you're just going to change contour completely.
1112110	1129910	B	0.8132754564285278	And then there's what's called the constraint that I just talked about, which is you want to change contours of the probability distribution in such a way that contours of high probability are visited much more often than contours of low probability.
1130010	1131394	B	0.791304349899292	So you want to preserve that.
1131432	1135474	B	0.7870128154754639	You want to do that in a way.
1135672	1142546	B	0.8905092477798462	So the third ingredient of Hamiltonian Monte Carlo comes in, which is metropolis.
1142578	1143490	B	0.7493305206298828	Hastings.
1143650	1150486	B	0.760989248752594	And so Metropolis Hastings is this accept reject that we discussed briefly last time.
1150668	1168970	B	0.4309239685535431	And so Metropolis Hastings is just super clever and it enables you to say it just tells you whether this momentum refreshing step is actually a good one or a bad one and should be rejected.
1170670	1182222	B	0.5070536136627197	A momentum refreshment is good if overall your sampling is going to remain faithful to the target distribution, but it is bad if it remains unfaithful.
1182366	1197826	B	0.810975193977356	So actually running this metropolitate things except rejects, that allows you to change contours of the property distribution in a way that remains faithful to the target distribution.
1198018	1199926	B	0.49779775738716125	And so there you go.
1200108	1202402	B	0.8220455050468445	That's basically Hamiltonian Monte Carlo.
1202466	1207750	B	0.7656365036964417	So to recap, you a mental state space by adding velocities.
1208650	1225230	B	0.7263503074645996	This allows you to build the Hamiltonian by declaring that the original probability distribution gives you potential energy and you add a kinetic energy on the velocities, which is just velocity squared.
1225730	1235662	B	0.5128338932991028	Because you have a Hamiltonian, you can use geometric integration to simulate Hamiltonian dynamics very accurately and with very long time steps.
1235806	1242226	B	0.7766776084899902	The very long time steps is a crucial thing because it means that with very low computational costs, you can sample very far.
1242328	1251190	B	0.7731005549430847	So it means that you don't get stuck in a region of the probability distribution, but you can actually visit it much more fast and efficiently.
1251850	1253400	B	0.7238011956214905	So that's the first part.
1254570	1260182	B	0.7168237566947937	The problem with those dynamics again, is that they remain on the contour of the probability distribution.
1260246	1263174	B	0.8389890193939209	And so you want to sample the whole probability distribution.
1263302	1283342	B	0.8478573560714722	So what you do is every once in a while, and I think that's a hyper parameter in your, in your sampling algorithm, let's say every ten iterations of the Hamiltonian dynamic, every ten time steps of the Hamiltonian dynamic, you're going to sample you're, you're just going to randomly take a new velocity.
1283486	1288050	B	0.8725230693817139	So you're going to change contour in the probability distribution.
1288630	1298840	B	0.8304177522659302	And the crucial point there is you want to change contour in a way that's faithful of the probability distribution and that's faithful of the sampling problem that you want to do.
1299530	1314810	B	0.6143505573272705	There comes the last thing, which is Metropolis Hastings, which ensures that the momentum reflect freshman will be good for sampling, I mean, will preserve your target probability distribution.
1315550	1322170	B	0.721332311630249	So with that, you just get a very efficient sampling algorithm.
1322250	1324560	B	0.6222962141036987	And so it seems a bit convoluted, right?
1325650	1330500	B	0.5873371958732605	And one bottleneck, of course, is that you need to double the dimension of the state space.
1331430	1336062	B	0.5176988244056702	And if your state space is already extremely large, that could be a bottleneck.
1336206	1343762	B	0.7500209212303162	It could be the case that actually doubling the dimension to add velocities, it could be a computational bottleneck.
1343826	1350200	B	0.6376979947090149	So that's a problem, but still, in most cases, that's not a problem.
1352730	1355702	B	0.4851246178150177	Again, Hamilton in Monte Carlo is convoluted.
1355766	1369138	B	0.6975634694099426	But the overall take home message I would want you to take from this is that it is a method that remains faithful to the probability distribution they want to sample.
1369174	1370270	B	0.6846453547477722	And this is crucial.
1370930	1380030	B	0.5623409152030945	And it is also a method that through geometric integration, it enables you to take time steps that are very long and still get accurate sampling.
1380690	1383760	B	0.9841596484184265	So this is the really cool thing.
1384530	1411850	B	0.5734358429908752	If you could come up with a whole bunch of other methods that did not require to double the dimension of the state space or that didn't use geometric integration, the problem that you would probably run into is that the thing that you came up with when you implemented in the computer, it doesn't exactly preserve the probability distribution that you want to sample.
1412350	1417590	B	0.8848439455032349	And so the problem with that is that this would lead to biased sampling.
1417750	1431250	B	0.5148739814758301	And biased sampling is like you're going to sample a different probability distribution, which might be just a bit different, but still a different probability distribution than what you really want to sample.
1431750	1435166	B	0.6002944707870483	And this would bias your predictions.
1435358	1445480	B	0.9713829755783081	So the really cool thing of Hamiltonia Monte Carlo is that you're actually able to have unbiased sampling through the Metropolis Hastings step.
1448010	1449414	B	0.7068555951118469	And this is a crucial thing.
1449452	1460966	B	0.6076254844665527	So it's computationally implementable in general, there's no computational bottlenecks apart from this doubling of state space, and it leads to unbiased sampling.
1461158	1462698	B	0.6827906370162964	It's also relatively simple.
1462784	1465660	B	0.7070376873016357	So this is why this is really used all over the place.
1467970	1473280	B	0.8303852081298828	There's another few perspectives that might explain why it works so well.
1474130	1494390	B	0.8673930764198303	So in the paper in the last subsection of section two, which is about optimization, so the action on optimization is about how to accelerate optimization, how to derive an optimization algorithm.
1495610	1496614	B	0.6027781367301941	2.6.
1496732	1497558	B	0.4795912802219391	That's the one.
1497644	1506794	B	0.8623972535133362	So the whole thing about section two is about deriving an optimization algorithm that accelerated in a physical way.
1506912	1527134	B	0.9532734155654907	And by accelerated, what I mean by accelerated, it's not about just going faster, but it's about having acceleration, which means that if you go slightly up in the paper on the figure, I think this figure is great.
1527172	1532498	B	0.9666374921798706	And by the way, I wasn't the one who came up with this figure, but I think this figure is great.
1532584	1537186	B	0.591580867767334	It really gives a lot of intuition for what acceleration really is.
1537368	1551960	B	0.8879684805870056	So if you look at the green cup, or I know how you call this, the green well on the left, this would be like a bowl rolling down the well.
1552810	1556746	B	0.8398160338401794	And the well is still that honey to like a certain level.
1556928	1559302	B	0.524512767791748	And so there you get a lot of friction.
1559446	1568860	B	0.8882344961166382	So your ball would just roll down very slowly, and the speed of the ball would be proportionate to the slope of the well.
1570530	1573978	B	0.7025607824325562	So this is not accelerated because there's a lot of friction.
1574074	1578174	B	0.8525999188423157	And so the speed is just proportional to the slope of the well.
1578292	1581120	B	0.7047240734100342	So this is actually what gradient descend does.
1581830	1592350	B	0.7744941115379333	But now if you go on the right, you get what people coin physics on under the hand system, which is also an accelerated system, a system that's meaningfully accelerated.
1592510	1607126	B	0.6720970869064331	So if you replace the honey in the well by some water, then there's going to be way less friction and your bowl is actually going to accelerate and overshoot the minimum of the well and then sort of stabilize.
1607238	1614570	B	0.4594394862651825	But the point here is that this bulb is just going to get so much faster to the minimum.
1622370	1626430	B	0.9433020949363708	This idea for optimization is just extremely powerful.
1627330	1635646	B	0.8134080171585083	And by the way, on the right hand side in the graph, you can see the improvements that you get when you implement these sort of ideas.
1635758	1639886	B	0.6770618557929993	So this was on a I'll come back to sampling in a bit, I promise.
1639918	1642690	B	0.827059805393219	But this is actually very well, very related.
1644010	1662860	B	0.5160635113716125	You can see on the graph on the right what kind of improvements you get when you go from under damp, first order system grade in the sense system, when you go from an over damped first order grade in the sense system to an underdamp second order accelerated system.
1663470	1665420	B	0.8513015508651733	And so you can see the curve, right?
1666110	1670370	B	0.729514479637146	So the black orange curve is over damp.
1670470	1671738	B	0.718852698802948	There's no acceleration.
1671834	1673150	B	0.7489203214645386	It's very slow.
1674050	1680314	B	0.595761775970459	The last curve, the blue one, it's accelerated and it's just super fast.
1680452	1692260	B	0.6353569626808167	So you can see it sort of gives you like quantitative data as to how many improvements you can get by implementing this acceleration in a physically meaningful way.
1693690	1699320	A	0.9093607068061829	Could you just describe the axes of the graph and also what the league group is here?
1701130	1705014	B	0.8789708614349365	The lead group is in the figure legend.
1705062	1708410	B	0.7917925119400024	In the fourth line, you see the leg group is so n.
1708560	1716060	B	0.8883214592933655	So that's technically that's the special orthogonal group of dimensions n.
1716830	1726782	B	0.8620389699935913	If I remember correctly, these are all the matrices, all the square matrices of size n by n that have a determinant equal to one.
1726916	1734020	B	0.80147784948349	So the determinant is just like a function where you give it a matrix and it gives you a number.
1734630	1754200	B	0.8223667740821838	The fact that the fact that the determinant is equal to one means that here we're just taking matrices which are, which produce not translations, but changes of coordinates that preserve the geometry in a system.
1756910	1765450	B	0.6089913845062256	Just to elaborate on this a little bit, because for people who are not intimately familiar with matrices, it might seem a little opaque.
1765870	1771610	B	0.8564531803131104	So when you have a matrix, a matrix can multiply vectors.
1771770	1781250	B	0.8872829079627991	So a square matrix takes in vectors of side of dimension n and it outputs vectors of dimension n by multiplication.
1782150	1797590	B	0.8737566471099854	So if you have a square matrix, it just induces a transformation of your state space because each vector, each point which is a vector, gets converted into another point which is another vector.
1798170	1804594	B	0.7854934334754944	Now there's a lot of, I guess, properties there, right?
1804652	1813866	B	0.8861587047576904	You could have all sorts of translations of state space or rotations in state space or transformations of state space.
1814048	1825786	B	0.8505901098251343	If the determinant of the matrix is equal to one, it means that the transformation of state space that the matrix induces will preserve the geometry of the sixes.
1825818	1833058	B	0.6386039853096008	It will preserve distances and it will preserve crucially orientation as well.
1833224	1847282	B	0.7036158442497253	Because if you only asked the determinant to be equal to one or minus one, then the matrix transformation would preserve the geometry, but it would not preserve the orientation.
1847346	1849240	B	0.7995383143424988	It could mirror things.
1850810	1862346	B	0.877454936504364	So if you only asked the determinant to be equal to one or minus one, you would get a lead group that's called the orthogonal group, which is usually denoted O of N.
1862528	1867982	B	0.7751747369766235	And here we require the determinant to be just equal to one.
1868036	1870974	B	0.7761027216911316	And so you get the special orthogonal group.
1871092	1874560	B	0.7995328903198242	And so it's this group of matrices that have this property.
1875090	1890370	B	0.6113430857658386	Now this is not really important for the example here because you could have taken any other lead group and got something similar and got a similar difference in performance.
1893750	1896630	B	0.7560111284255981	I guess it's just interesting for its own sake.
1897210	1900930	B	0.8486270308494568	And so the graph though, the K is the number of iterations.
1901010	1910010	B	0.8945680856704712	So the number of time steps, so that's the x axis and the y axis is how close you are from the minimum.
1910670	1912646	B	0.7695884704589844	And, and it's in lock scale.
1912758	1925162	B	0.8686907887458801	So as you can see, the, the blue curve is going to get like in 100 iterations, it's going to get basically at ten to the minus 410 to the minus five distance from the minimum.
1925306	1926942	B	0.5652540326118469	So it's like super, super close.
1926996	1936660	B	0.516808271408081	It basically gets there in 100 iterations when you take the other normal gradient descent methods where you see that it's going to take so much longer.
1939030	1943474	B	0.7632656097412109	So yeah, this is really the advantage of using second order methods.
1943522	1953874	B	0.7229105830192566	And what I mean by second order methods is that by second order means that you actually double the state space to introduce velocities.
1954002	1960870	B	0.8017520308494568	So that you not only have a dynamical system over positions, but you have a dynamical system of velocities.
1961030	1971390	B	0.8568591475486755	And so it means that you have a physical notion of acceleration because acceleration is a movement of velocities.
1972290	1986690	B	0.7678288221359253	And so if you double the state space by saying this is position, this is velocities, and you see I have a motion in this like a state space that's twice as big, then you have a meaningfully, a meaningful notion of acceleration.
1987510	1989054	B	0.9439478516578674	And this is really powerful.
1989182	2006250	B	0.7990078330039978	And so you can see already a parallel here, which is that Hamiltonia Monte Carlo also has this notion of acceleration in some way, at least just intuitively, because we also doubled the size of the state space to introduce velocities.
2006590	2012970	B	0.5367958545684814	And so it turns out that this intuition is actually you can make it into a formal correspondence.
2014190	2019120	B	0.9367548823356628	And I think this is something that quite interests me, to be honest.
2020210	2025914	B	0.8745421171188354	So if you remember, if we come back to the intuition for sampling.
2026042	2031502	B	0.8937128782272339	The intuition for sampling through Monte Carlo methods is we have a target distribution that we want to sample from.
2031636	2037438	B	0.9012625217437744	So we're going to run a dynamic, random dynamic towards that distribution.
2037614	2051458	B	0.6586681604385376	Now, what makes sampling efficient is that the distribution that characterizes the process because again the process is random so that each time it is at a random location that is characterized by distribution.
2051634	2060220	B	0.6286706924438477	What makes sampling efficient is that the distribution characterizing the process just converted as fast as possible to the thing that you want to sample from.
2063630	2070554	B	0.8498766422271729	This is to say that you can think of sampling as an optimization on the space of probability distributions.
2070682	2076830	B	0.8639379739761353	You want to move your probability distribution of the process as fast as possible to your target.
2077570	2084610	B	0.7817292213439941	And so you can see from there that sampling is actually not so different from variational inference.
2086310	2090334	B	0.665729820728302	It's just the same idea, only variational inference.
2090382	2105690	B	0.6835020780563354	You're typically going to take a parameterized family of distributions and approximate the targets, while here you have a non parameterized family that's given by your dynamic that's going to perfectly match the target.
2108670	2125280	B	0.7401714324951172	Again, with that, I want you to have the take home message that sampling, you can think about it as an optimization on the space of probability distributions in the sense that you have a target distribution and you want to get there as fast as you can with the process.
2125890	2137220	B	0.8762328028678894	Now let's suppose, and so here is the crucial connection between sampling and optimization in the way that we've described it here.
2137990	2149478	B	0.8833788633346558	Let's suppose that you run this accelerated optimization scheme on the space of probability distributions then the process.
2149644	2162300	B	0.8855950832366943	So you would get the dynamic on the space of probability distributions that has a meaningful notion of acceleration through the accelerated method that was shown here.
2162910	2177230	B	0.8650314807891846	So if you look at what kind of dynamic this really gives you, it gives you a process which is given by a stochastic differential equation that is known as under damp launch of a dynamic.
2178130	2182898	B	0.8707016706466675	So there's an equation for it and I think action subsection 2.8.
2183064	2197670	B	0.5869613289833069	But the point is under dynamics is a stochastic differential equation whose density or probability distribution stole this accelerated optimulation problem on the space of probability distributions.
2198250	2210940	B	0.798549234867096	So just from there, you know that under that logo on dynamics, it's going to be a very efficient sampler because it meaningfully accelerates and gets the target, I mean, quite fast.
2211790	2218886	B	0.8331096768379211	The problem is under label logo on dynamics, you cannot simulate it accurately.
2219078	2224362	B	0.48922064900398254	Well, you can simulate it accurately, but you cannot simulate it exactly on your computer in practice.
2224506	2230160	B	0.8221752643585205	So this comes back to the numerical integration problem that we discussed just before.
2232390	2250390	B	0.8307745456695557	And so you have to find a way to discretize or in other words implement under dumpling bound dynamics on your computer in such a way that you retain that the thing that you implemented on the computer retains this meaningful notion of acceleration.
2251290	2251906	B	0.6797984838485718	It turns.
2251938	2273470	B	0.7153452634811401	Out that you can see Hamiltonian Monte Carlo as a faithful numerical discretion or numerical implementation of under damp launcher and dynamics that's going to preserve these acceleration properties and therefore this efficient sampling.
2274050	2279146	B	0.925164520740509	So this is all like, yeah, a lot of interesting connections.
2279338	2288158	B	0.813342273235321	But basically what I want to get at is you really have this notion of acceleration that permeates well, I guess physics.
2288254	2292446	B	0.8182650804519653	But here it permeates sampling and it permeates optimization.
2292638	2301302	B	0.8228687047958374	And so the method that was shown here about optimization and Hamiltonian Monte Carlo, they're just the same in a way.
2301356	2310330	B	0.8382788896560669	Only one is applied to optimization, the other is applied to optimization on probability distributions of Aka sampling.
2312590	2313050	B	0.7671424746513367	Great.
2313120	2315162	A	0.7922129034996033	I guess last question on this part.
2315216	2319040	A	0.7422563433647156	What is the shadow Hamiltonian and why does it sound so cool?
2320370	2321310	B	0.6045114994049072	I know, right?
2321380	2323360	B	0.9796029925346375	Yeah, it's really cool.
2324130	2325358	B	0.744939386844635	It's a recall name.
2325444	2333150	B	0.5750322341918945	So the shadow of Hamiltonian is okay, so let's go back to the Hamiltonian.
2333970	2338894	B	0.8598870038986206	So we have a Hamiltonian, and we want to simulate the dynamic that preserve the Hamiltonian.
2338942	2340900	B	0.5965672135353088	We want to implement that on the computer.
2341270	2344070	B	0.8017905950546265	We're going to do that through geometric integration.
2344650	2350258	B	0.7120765447616577	Geometric integration gives you a bunch of algorithms to preserve the Hamiltonian.
2350354	2356570	B	0.7755450010299683	I mean, a bunch of algorithms they can implement on the computer, and they're going to preserve the Hamiltonian.
2357470	2360550	B	0.7171940207481384	Do they actually really preserve the Hamiltonian?
2360710	2362378	B	0.6220932602882385	It turns out that no.
2362464	2394200	B	0.6455554366111755	So what I said before is a bit of a shortcut, because the numerical integration that you get through any numerical method, including geometric integration, is not going to exactly preserve the Hamiltonian, but it's going to preserve what people call a shadow hamiltonian, which is almost the same as the Hamiltonian, but with extra terms that sort of vanish if the time step is very short.
2396170	2412170	B	0.8513649702072144	So this is to say that your numerical dynamic implemented through geometric integration is going to exactly preserve the shadow Hamiltonian and approximately preserve the true Hamiltonian.
2413790	2430410	B	0.8131930232048035	In the papers, we show I don't know if it's well, it's probably algorithm dependent, but basically, depending on the algorithm that you choose, you want to show to what extent the shadow Hamiltonian truly approximates the true Hamiltonian.
2430570	2447070	B	0.7398847937583923	The virtue of geometric integration methods is that actually the shadow Hamiltonian turns out to be extremely close to true Hamiltonian, which dean that you can take a very long time steps and still be very good at preserving the true Hamiltonian.
2447230	2457434	B	0.5768508911132812	So here, when you implement these methods, you don't exactly preserve the true Hamiltonian, but you still do it pretty well.
2457552	2477786	B	0.8311459422111511	And so in Hamiltonian Monte Carlo, the momentum refreshment, and Metropolis, usually the Metropolis Hastings accept rejects that is going to correct for those failures of truly preserving the Hamiltonian.
2477898	2493410	B	0.7751117944717407	So even though you don't exactly preserve the Hamiltonian in your numerical integration, in Hamiltonian Monte Carlo, the Metropolis hating accept reject step is going to correct for this inaccuracy.
2493570	2507420	B	0.7517282962799072	And so this is like, really the true beauty of Hamiltonian Monte Carlo is that even though you get a lot of things that are not exactly preserved when you implement things on a computer.
2508190	2520090	B	0.9563125371932983	Thanks to Metropolitan Hastings, overall your sampling is going to be perfect in the sense that you're going to truly preserve your target distribution.
2520250	2521920	B	0.6823731064796448	So this is really the key.
2523170	2539186	B	0.7085250020027161	Now, a follow up question to that is, okay, well, if Metropolis Hastings is just so crucial and it just gives your dynamic the property that it's going to preserve, whatever it does, even if it samples really badly if?
2539208	2545282	B	0.6382904052734375	You add Metropolis Hastings, it's still going to preserve your Target distribution.
2545426	2550598	B	0.6254463195800781	Then why can't You Just Come up with an average process, like whatever Kind Of process?
2550764	2553480	B	0.8521144390106201	And that Metropolis Hastings at the end?
2553850	2555770	B	0.7318177819252014	And so you can do that.
2555840	2558220	B	0.8360792398452759	You can take any kind of process.
2563790	2566922	B	0.9436642527580261	Let's say you take a random process, it could be the worst in the world.
2566976	2572474	B	0.8620027899742126	Let's say you wanted to sample from a gaussian and you actually take a Brown in motion.
2572602	2577054	B	0.932173490524292	Now this is clearly not going to work because Brown Emotion just goes all over the place.
2577092	2578414	B	0.6888195276260376	It could go infinitely far.
2578452	2581922	B	0.8088632822036743	Brown in motion, it's just like random action, right?
2582056	2583570	B	0.7483783960342407	Completely random motion.
2584950	2596582	B	0.7320865392684937	There is structure to it, but the structure in Brown in Motion means that it's such that brown in motion is just going to just spread really far.
2596716	2603414	B	0.8435731530189514	So Brown in motion is not going to preserve your target distribution and it's also going to be very slow, by the way.
2603452	2614374	B	0.6765150427818298	So it's not going to be a good sampler if you add Metropolis Hastings on Brown emotion, which is you integrate brand action.
2614422	2624318	B	0.7997097373008728	So you simulate a step of brand emotion on the computer and then you do Metropolis Hastings to know whether you should accept or reject that step.
2624484	2627630	B	0.7799234390258789	Metropolis Hastings will tell you whether you should do it or not.
2627780	2639230	B	0.720295786857605	You either accept and you keep going from that new position, or you reject and you start from where you started from and take a new sample.
2639310	2640302	B	0.5261781811714172	Accept, reject.
2640446	2641970	B	0.6957208514213562	If you accept, you keep going.
2642040	2645442	B	0.6395435929298401	If you reject, you go back and you sort of go like that.
2645576	2650198	B	0.7885899543762207	So if you do materially facing there, your standard is going to be exact.
2650284	2655900	B	0.7246712446212769	So it is going to preserve the target and so you're going to have accurate sound plan.
2657550	2658822	B	0.6912357807159424	And so this is crucial.
2658886	2662966	B	0.9009294509887695	This really highlights the importance of Metropolis Hastings.
2663158	2671250	B	0.6500259637832642	But there is a big but your sampler here is going to be extremely slow.
2671430	2679514	B	0.8639910221099854	It's going to be extremely slow, first of all, because brand in action in and of itself, it doesn't at all preserve the target distribution.
2679562	2681102	B	0.5453633666038513	I mean, it just goes all over the place.
2681156	2686260	B	0.8389251232147217	And you just want to sample a lot around the mode of the gaussian, for example.
2686630	2699910	B	0.502708375453949	So it means that Metropolis hasting will reject a lot of your steps because a lot of your steps will try to go further away while you want to stay around the mode typically of the gapsin.
2700810	2712650	B	0.8861500024795532	So you're going to do a lot of steps for nothing and also Brown in Motion just does not have the qualities that make a sampler efficient.
2713310	2731390	B	0.9024741053581238	And so the reason why Hamiltonian Monte Carlo is so good is because it's able to integrate Metropolis Hastings and still preserve a lot of other properties that make the sampler efficient.
2731730	2747346	B	0.5587732791900635	So if we remember, HamiltonI Monte Carlo has this geometric integration that does not exactly preserve the Hamiltonian, but it does still does it pretty well approximately by preserving a shadow Hamiltonian.
2747458	2751334	B	0.9735423922538757	So this means already that integration step is going to be really good.
2751372	2767718	B	0.7104555368423462	So there's going to be a lot of acceptance in the Metropolis Hastings and also the way the scheme is set up, there's going to be a lot of acceptance set in the Metropolis Hastings.
2767734	2775630	B	0.5778926014900208	So it means that most of the samples that you would draw will actually be used instead of being rejected and you have to start over again.
2775780	2777310	B	0.5269935727119446	That's one advantage.
2778370	2792610	B	0.5551663041114807	The other advantage of Hamiltonium and T Carlo, and this is a crucial one and we kind of discussed this last week, is that one crucial thing to be a good sampler is this idea of time irreversibility.
2793350	2801320	B	0.6031450033187866	So I'll emphasize it again because it's really crucial and there's a lot of literature on this and it's something that we reviewed also in the paper.
2801850	2816262	B	0.4716339707374573	So a sampler that is time reversible that is time irreversible is always going to be better or it's at least not going to be worse than a sampler that is time reversible.
2816406	2818254	B	0.8172000050544739	So what do we mean by this?
2818452	2820750	B	0.8163437247276306	A sample is time irreversible?
2822050	2834754	B	0.9064400792121887	If and on the if were you to play the dynamics forward or backward, there will be going on your bullet point here.
2834872	2849430	B	0.8124839663505554	A sample is time reversible if and on the if you were to play the dynamics forward or backward in time, so forward in time and then maybe you would play them by reversing the movie by playing the movie backwards.
2850250	2862794	B	0.7746080160140991	If you were to do that, then the two movies that you would see would be qualitatively the same and statistically the same.
2862992	2870974	B	0.7459794282913208	So the process is time reversible if and only if the process, if you're running forward or backward, it's basically statistically the same.
2871172	2873290	B	0.8568044900894165	Now, what does this mean in practice?
2873370	2879440	B	0.7068225741386414	Well, if your process is time reversible, then it's going to backtrack very often, actually.
2881110	2897094	B	0.7579139471054077	So it means that you would be somewhere in the probability distribution, just something there and you will go forward and then you probably go backward and so on and you kind of get stuck in a region until you go somewhere else.
2897132	2910310	B	0.6976955533027649	But it's just going to be very slow to move around and so it's going to be very slow to get good sampling because it's going to take you a long time to visit all the regions of the probate distribution.
2910470	2913660	B	0.8322455286979675	In other words, the distribution characterizing the process.
2914270	2919840	B	0.8772688508033752	It's going to move very slowly to the target distribution that's another way to look at it.
2921410	2929982	B	0.6452334523200989	If the process is time irreversible, on the other hand, then it's a lot less likely to go backward during the sampling process.
2930116	2934754	B	0.7590060830116272	So it means that it's going to visit the target distribution a lot more.
2934792	2936450	B	0.7832282781600952	I mean, it's just going to go around.
2936600	2947190	B	0.728531539440155	Imagine if you're not allowed to go backward just as a human being and you're walking around, you're just going to end up in many more places than if you were allowed to go backward.
2948570	2956950	B	0.5763916373252869	And if you were just going backward all the time to where you started, then you wouldn't be able to do a lot of visiting.
2957030	2958790	B	0.8447416424751282	So it would be a bad sampler.
2958950	2963980	B	0.859877347946167	So again, there's some, like straightforward intuition there.
2964510	2974474	B	0.795271635055542	So one idea is in sampling is we want to optimize the extent to which a sampler is timely reversible.
2974522	2982334	B	0.7168189287185669	We want to increase time irreversibility as much as we can force a sampler to just move around as much as possible.
2982532	2983920	B	0.7526998519897461	So that's an idea.
2984850	2986578	B	0.8220142722129822	It is explored currently.
2986664	2996710	B	0.6304318904876709	I guess it's a bit of an open problem of how do you really do that and how do you implement that on the computer in a way that works and preserves all the properties.
2997690	3006886	B	0.5945974588394165	But the point I wanted to get to is that Metropolis Hastings is a blessing and it's also a curse.
3007078	3014310	B	0.9387835264205933	It is a blessing because when you add it to any kind of process, you will make your sampling unbiased.
3014390	3021182	B	0.7910349369049072	So it means that you will sample the Ride distribution even though you're implementing this on a computer.
3021316	3030160	B	0.6067726016044617	And there can be a lot, so many issues with the numerical integration, numerical approximation and so on.
3030790	3041650	B	0.5823845863342285	But it's also a curse because whenever you add Metropolis Hastings on the process, it's going to make it time reversible.
3045110	3056170	B	0.6476821303367615	Actually, yeah, if you just took a random process and added metropolisA, you would get something that's inevitably going to be quite slow.
3057150	3061370	B	0.531248152256012	And so now you get to the problem or the conundrum.
3061710	3062940	B	0.7335650324821472	What do I do?
3063390	3065126	B	0.8835182189941406	Do I go for unbiased sampling?
3065158	3066730	B	0.6628066301345825	Do I care about accuracy?
3068370	3071130	B	0.8331207036972046	And then I should add Metropolis Hastings.
3071290	3081730	B	0.6014165282249451	Or do I not care about accuracy so much and then I should not have Metropolis Hastings and it's going to go faster generally.
3083750	3089790	B	0.6725342869758606	And so Hamiltonia Monte Carlo actually has the blessing and does not have the curse.
3089950	3093560	B	0.9780194759368896	And this is why Hamiltonian Monte Carlo is just so good.
3094730	3101362	B	0.6698370575904846	And again, it's also complementary to all of the things that we've been discussing.
3101506	3111882	B	0.5540696382522583	And the reason why Hamiltonian Monte Carlo is blessed and not cursed is because the momentum, how do you call this?
3112016	3121210	B	0.8233046531677246	The Metropolis Hastings is just done on the momentum refreshment step as opposed to being done on the overall dynamic.
3121550	3140530	B	0.8412494659423828	So because if you remember, you do geometric integration to simulate Hamiltonian dynamics, every once in a while you will take a momentum refreshment and then do Metropolis Hastings on that momentum refreshment, as opposed to doing Metropolis Hastings on both the Hamiltonian dynamic and the momentum refreshment.
3142010	3169446	B	0.7322965860366821	So it's not something that I can really explain how that works, but it turns out that by just doing the Metropolis method, montacarlo is a way of having Metropolis Hastings in a way that you preserve the unbiasedness so that your sampling is accurate, but you don't sacrifice the time irreversibility.
3169638	3171020	B	0.618298351764679	And so you get both.
3173950	3179280	B	0.7486810684204102	And this is because the Metropolis hasting is just on one of the components and not on both.
3181030	3183634	A	0.873798668384552	To kind of connect that to an example.
3183752	3201522	A	0.869059145450592	As we move into active inference here, we have some probability isocontours, some, we have the high probability carpool lane, and then we have some lower probability lanes, and we can go different speeds in these lanes.
3201666	3206140	A	0.7135283946990967	And we want to have a full accelerated model here.
3206750	3213926	A	0.7835407853126526	It's almost like the MH is refreshing our velocity just asking when we want to change the lanes.
3214038	3219818	A	0.60105961561203	But it's not our full self driving car Metropolis Hastings algorithm.
3219994	3232258	A	0.5152871608734131	But we're able to use our position and acceleration when we're in our lane to take full advantage of the acceleration following the shadow road.
3232424	3240340	A	0.8860536217689514	Not necessarily the true road, but the shadow road is close enough, or the shadows on the road, and then lane changes.
3240870	3250710	A	0.5387845635414124	Are these computationally costly and reversible, but still super useful propositions?
3252250	3253000	B	0.5662814974784851	Exactly.
3253610	3257366	B	0.5962367653846741	And they're actually not computationally costly.
3257478	3261130	B	0.5248475074768066	They would be if you were to reject many samples.
3261630	3267186	B	0.7561160922050476	But here in Hamiltonian Monte Carto, typically you don't get to reject many samples.
3267318	3271994	B	0.985612690448761	So yeah, that's such a great picture.
3272042	3278586	B	0.9126226902008057	So the car follows the Shadow Road by just doing geometric integration and following the shadow.
3278618	3282394	B	0.517397403717041	Hamiltonian every once in a while, you get a momentum refreshment.
3282522	3288770	B	0.8657048344612122	Velocity refreshment, which is, oh, now I move to the first lane, or the second lane, third lane, and so on.
3288920	3299334	B	0.6361905932426453	And you get a Metropolis Hastings Corrections step that says, do I accept this proposal of lane change or do I reject it?
3299372	3300790	B	0.7593908905982971	And still on the coast.
3301610	3305430	A	0.8846960067749023	So how does this connect active inference lab?
3306810	3308998	B	0.6566616296768188	Well, I mean, great question.
3309164	3317270	B	0.7582323551177979	So active imprints in active inference proper, you don't have any sampling in active imprints.
3317430	3331406	B	0.5723472237586975	Whenever you want to scale active inference to implement it, to solve any kind of problem in the world, you want to do sampling because there's a lot of computations that are not going to be trackable otherwise.
3331598	3339010	B	0.8760346174240112	And so one cool example is in the figure after this, actually.
3339080	3350600	B	0.8650635480880737	So for any kind of decision making active inference lab, you need to approximate the expected free energy.
3353130	3357020	B	0.8593749403953552	The one just before yeah, perfect.
3358830	3368954	B	0.8546900749206543	So if you look at the second equation there, you get this minus lock p of an action sequence equals expectation of something.
3369152	3376080	B	0.9069437980651855	This minus lock p of the action sequence is our notation in this paper for the expected free energy.
3380530	3386820	B	0.7216011881828308	You probably all know this, the expected free energy is given by the expectation of something.
3387590	3399910	B	0.7725412249565125	Now typically, and when you have a very high dimensional model, which happens in most applications, I would say you get a very high dimensional expectation.
3400410	3401650	B	0.716259241104126	What is an expectation?
3401730	3406330	B	0.8438032865524292	An expectation is an integral with respect to probability distribution.
3406670	3410202	B	0.7002770900726318	How do you compute expectations while you do that?
3410256	3418202	B	0.8408759832382202	Through sampling, or at least sampling is the way to compute high dimensional expectations.
3418266	3420458	B	0.7470363974571228	That's the most efficient in statistics.
3420634	3428078	B	0.5899043083190918	And this is the reason why sampling is studied in statistics, is because that's how you solve these problems.
3428164	3440846	B	0.5232430696487427	I mean, expectations just come up everywhere, just like so many things about machine learning bowl down to computing expectations.
3441038	3453942	B	0.6247087717056274	And so here in particular, the expectation that is very dear to us, very close to our hearts, is the one that gives you the expected free energy.
3453996	3456730	B	0.7566698789596558	The expected is the expectation of something.
3456880	3462780	B	0.7341398000717163	So how do you compute that in a high dimensional model where you have to use something?
3465150	3470010	B	0.8219928741455078	There comes the usefulness of Hamiltonian Monte Carlo.
3470350	3479870	B	0.531158983707428	If you have a discrete state space based model, you're not going to be able to use Hamiltonian Monte Carlo because Hamiltonian Monte Carlo is on continuous states based models.
3480870	3482674	B	0.8715338110923767	It's on continuous state space, right?
3482712	3485534	B	0.8709902167320251	We talked about continuous positions, continuous velocity.
3485662	3492002	B	0.8822125196456909	I talked about something a probability distribution on my desk, which is a continuous state space.
3492136	3501542	B	0.4915253818035126	So you're not going to be able to use Hamiltonian Monte Carlo if you have partially observed mark of decision process and you're doing active inference there.
3501676	3527418	B	0.7564855813980103	But if you have for example, I've been talking recently to Ryan Smith, who does a lot of active inference, and he's really leading a lot of the modeling work with active inference and real data of patients of all sorts, many medical data, and also using active inference to model psychological experiments.
3527594	3538180	B	0.6283851265907288	And one of the things he told me about well is, okay, well, I have a bunch of data, and partially observed markup decision process is just not going to do it.
3538790	3540274	B	0.5204675793647766	Why is he not going to do it?
3540312	3544710	B	0.8483200669288635	Because the data that he has lives on a continuous space.
3544860	3562650	B	0.7870138883590698	I don't remember exactly what it was, but let's just say, for the sake of example, that that data was the temperature in the room, and the subjects had to infer the temperature in the room based on their sensations.
3563310	3565660	B	0.5759007930755615	So that's a continuous state space problem.
3566110	3572942	B	0.7449197173118591	Now, imagine that you ask someone to infer the temperature in the room.
3573076	3577598	B	0.8405522704124451	Then maybe you would change the temperature in the room or not, and you would ask them again.
3577684	3580640	B	0.801071047782898	Change the temperature or not and ask them again, and so on.
3581330	3594680	B	0.7887009382247925	When you have this sort of set up, you have a discrete you have a phenomenon that unfolds in discrete time because you just repeatedly ask some questions and sometimes elapses between the questions.
3597050	3600226	B	0.8239869475364685	But you also have a state space that's continuous.
3600418	3612694	B	0.8025805950164795	When you have this sort of data, then the kind of model that you want to use is a partially observed mark of decision process that we know and love but the state space is going to be continuous instead of discrete.
3612742	3615050	B	0.7521724700927734	The planet is still going to be discreet.
3616030	3638158	B	0.807818591594696	Now, when you have this sort of model, I just want to say it or as an aside, that for the moment, a lot of the modeling work and simulation work in Active inference uses the sweet state space pound DPS, partially observed mark of the same process just because it's been sufficient.
3638254	3649570	B	0.6046713590621948	And when you simulate agent is often like in grid worlds and when you maybe have a state spaces in experiments they often by design discrete because it's just easier to handle.
3649730	3659882	B	0.4974988102912903	But that's not something, that's not an assumption, a simplifying assumption that we're going to be able to keep for that much longer.
3660016	3663846	B	0.749457836151123	There's just so many things that you cannot account with these sort of models.
3663958	3673040	B	0.8592478036880493	So one other model that is interesting to have is partially observed Markov decision process, but with a continuous state space.
3673650	3688722	B	0.7491912841796875	So with those you get the problem of estimating the expected free energy which would be then an expectation or in other words, an integration, an integral in a continuous takes place.
3688856	3694178	B	0.7648396492004395	And so how do you do that efficiently if you are in a high dimensional state space?
3694264	3695266	B	0.7762342691421509	Where do you use?
3695288	3696526	B	0.8169923424720764	Hamilton, monte Carlo.
3696558	3699060	B	0.9399396777153015	I think this wouldn't be really the best.
3699690	3704390	B	0.812132716178894	So this is how you join the dots in this paper.
3704460	3712618	B	0.7384074330329895	We didn't talk about sampling in discrete space because the methods are quite different.
3712704	3715802	B	0.6915882229804993	So we really had to choose what to focus on.
3715936	3720750	B	0.6883618831634521	There's of course a lot more methods in the literature than there are in this paper.
3720820	3726590	B	0.8941587805747986	We just wanted to sort of review what were the main ones and what people really used in practice.
3728210	3748870	B	0.4715562164783478	But when you have the expected principal in a discrete state space, computing the expectation that defines the expected priority can often be a little bit easier because it comes down computationally to a bunch of matrix multiplications.
3749850	3757866	B	0.6170692443847656	Matrix vector multiplications, which generally is doable, unless the state space is enormously high.
3757968	3761580	B	0.7870734930038452	And then we'd have to think of sampling methods in this sweet space.
3762910	3783262	B	0.4850003123283386	But as soon as you move to a partially observed markup decision process with a continuous space, which are argu and Ryan Smith argues and I'm sure a lot of people have run into this as soon as you have this kind of model, well, yeah, you just have an integration problem in continuous space.
3783316	3786260	B	0.7224869132041931	And so Hamiltonium Altekaro is the way to go there.
3788470	3789218	A	0.9184247851371765	Awesome.
3789384	3798330	A	0.8690016865730286	And your 2020 paper was a synthesis on some of the discrete statespace formalisms of active.
3798350	3818598	A	0.9189488291740417	So it's very interesting to see how you're now talking about where continuous time, continuous state spaces can come into play and how interesting the Active inference has the capacity to deal with discrete and continuous date spaces.
3818774	3831540	A	0.7810607552528381	And sometimes we lean on one leg or the other leg more, but it spans the gap in a way that's actually like a value adding, not like there's some sort of missing piece from one side or the other.
3832950	3834242	B	0.5906103849411011	Yeah, definitely.
3834376	3839380	B	0.9456067681312561	I think active inference is great because it's so flexible in the sort of models that you can consider.
3841270	3848280	B	0.841677725315094	But yeah, historically the first models to be developed were continuous space and continuous time.
3848650	3857180	B	0.8070285320281982	And then it works quite well because you can take gradients of the free energy and just minimize them over time.
3858110	3864140	B	0.7693002223968506	So you could get around basically everything by doing a grain and assemble free energy.
3864510	3867646	B	0.899090588092804	And then after that around like so that was like 2010.
3867668	3896130	B	0.7372386455535889	And then around 2015, people started thinking, okay, well, how do I model discrete time decision making with active inference and typically decision making tasks, at least the simple ones that are studied in neuroscience and behavior neuroscience, just to make things simple, the number of actions that you have is discrete.
3896290	3899974	B	0.6957446336746216	So you have a finite number of actions, which is pretty small.
3900092	3903800	B	0.7458644509315491	You would also have a finite number of states, which would be pretty small.
3904410	3909340	B	0.6638286113739014	And then people started to think, okay, well, how do we active inference lab to actually account for that?
3910190	3919520	B	0.853418231010437	And so, yes, and so that's how the whole expectation free energy and partially observed mark of decision processes came into play.
3920770	3927102	B	0.9320751428604126	And now the community has grown a lot and there's more and more data that we want to account for.
3927156	3935550	B	0.5643340349197388	There's more and more projects that are going on and, you know, people are realizing it's an obvious realization.
3938390	3945922	B	0.5996109247207642	It's not surprising at all that these two models developed in 2010, 2015.
3946056	3948290	B	0.6167188286781311	They they're just not going to account for everything.
3948440	3950786	B	0.7773242592811584	You need to think about other kinds of models.
3950978	3953974	B	0.8395668864250183	It really depends on what kind of data you have at hand.
3954172	3962794	B	0.5613343119621277	And then one important and obvious type of model would be partially observed Markov decision process with continuous space.
3962992	3965562	B	0.7633320689201355	And actually, this is not a new thing.
3965616	3972078	B	0.8219267129898071	I mean, people have been using these kind of models in reinforcement learning and control for a long time.
3972244	3981454	B	0.7696629166603088	I don't know to what extent they've arrived at practically I mean, they arrived at practically instrumental algorithms.
3981582	3985940	B	0.6460230946540833	I don't know to what extent they're used, to what extent their state of the art.
3986710	3993640	B	0.7865248918533325	But the point is, it's something that it's a kind of model that has existed for a long time.
3994170	4004958	B	0.6515936851501465	And so this sampling would be a way to actually practically implement this and scale it when you want to put that within active inference.
4005074	4009100	B	0.6678744554519653	So I think it's an important thing to think about.
4011390	4012042	A	0.9184247851371765	Awesome.
4012176	4012522	B	0.5491447448730469	Yeah.
4012576	4018346	A	0.8992523550987244	Thomas Parr, also recently in a discussion on the textbook was sharing some timelines.
4018378	4033714	A	0.9151173233985901	And it's just so interesting how these things have been developing and from continuous through characterization of the discrete, in a sense not culminating, but being synthesized in your 2020 paper.
4033912	4049682	A	0.5440358519554138	And now there's with an increased emphasis on empirical data, the desire to bring in a lot of these methods that actually help us implement it rather than just think about it, really parsimoniously.
4049826	4060300	A	0.9054182767868042	So what is your active inference representation in the figure of the running person?
4060910	4069420	A	0.8503702282905579	And then how do the variables and the processes described in this figure relate to all of this?
4070450	4074538	A	0.877816379070282	5 hours of talking about Shadow Hamiltonians.
4074554	4076974	B	0.7258524298667908	And all of this, right?
4077172	4079422	B	0.7515506148338318	Yeah, that's a broad question.
4079476	4085860	B	0.8267077207565308	We might need another Daniel to answer that in detail, but just very short.
4087430	4095574	B	0.5350866913795471	Well, the active inference section was meant to be as complete as possible, even though it was very short.
4095772	4105510	B	0.8633519411087036	And by completeness I mean that we started by deriving active inference and deriving active inference free energy principle.
4106170	4116214	B	0.7352089881896973	So this is really what the free energy principle does, even though we weren't able of course, to review the whole free energy principle in like one or two pages.
4116262	4126922	B	0.8219711184501648	But what we focused on was deriving the expected free energy free energy principle and then from there you get the full active inference algorithm.
4126986	4140786	B	0.8584327101707458	So full active inference algorithm, I think you just went past it's just below if you go slightly below, I think maybe page 29 or page 30 still, I think next realizing an active agents.
4140968	4149960	B	0.8968905806541443	So this is the active infrastructure algorithm, like 0.12.3 or something, just from a duration of the expected pre energy.
4151050	4156390	B	0.620239794254303	You actually get as a corollary from there, the active imprint algorithm that we know and love.
4156460	4164860	B	0.9648074507713318	This is actually a more general version than what people use and I'm be excited and I'm actually talking to people to actually implement that.
4165630	4182030	B	0.6626532077789307	But this is actually the most general version that has been established in the literature and it is more general in a meaningful way in the sense that all the beliefs, all the probability distribution, they are over trajectories or sequences of events.
4184290	4198114	B	0.8545109033584595	So it means that all the computations, they not only consider events at a particular time in the future, for example, but they consider trajectories of sequences of events.
4198242	4207980	B	0.8642338514328003	And so this is considering different events at different points in time and their independencies and dependencies between them.
4209950	4236910	B	0.7036738395690918	So this is just to say that if you would take then this algorithm and you would perform a mini field approximation over time, which is saying that all the things that you see in the future are sort of independent of each other at different points in time, then you would recover what people typically use in the literature, which is easily implementable.
4237070	4255126	B	0.7469764351844788	The point I want to make here is that you can actually not have that limitation and have things that are a bit more complex, that are able to capture more complex relationships in the time series and input that.
4255228	4262220	B	0.8912404179573059	They may be receiving, like, the kind of sensory data that they may be receiving and the kind of generative models that they would have.
4262990	4266730	B	0.8643218874931335	So anyway, this is the active inference algorithm, the most general one.
4266880	4272522	B	0.8354315161705017	We derive that from first principles by deriving the expected community from first principles.
4272666	4277946	B	0.8853452205657959	And so this relates us to that figure that you just showed.
4278058	4288100	B	0.8802385330200195	So that figure that you showed with S on A is the starting point of the free energy principle described in this paper.
4288470	4308938	B	0.7803658246994019	And so the point is that we want to describe decision making, want to describe actions as a function of sensations, and we want to come up with the most general description of actions as a function of sensations to be able to account for everything.
4309024	4311290	B	0.8440381288528442	That's kind of the idea of the pre engineer principle.
4312190	4319690	B	0.8668010830879211	So what do we do is we consider a world, a world in which there is an environment and an agent.
4319840	4325166	B	0.7497013807296753	And so the environment here is denoted by S, and S is a stochastic process.
4325348	4327040	B	0.7161785364151001	Why a stochastic process?
4328290	4334500	B	0.7205699682235718	Because a stochastic process is the most general type of dynamic that exists, at least as far as I know.
4335830	4337326	B	0.7105538249015808	It's really a random dynamic.
4337358	4339394	B	0.7229284048080444	And it could be random in all sorts of ways.
4339432	4341038	B	0.8012729287147522	It could also be non random.
4341134	4351814	B	0.825219452381134	We sort of take the stochastic, the random aspect, as an extra ingredient, just to include a lot more types of scenarios and things you can be confronted with.
4351932	4360378	B	0.7894860506057739	So you partition the world into the agent, which is O and A, and the environment, which is S, and the agent.
4360464	4365530	B	0.8774817585945129	Then you subdivide it into two more components, which are O and A.
4365680	4369774	B	0.7418556809425354	O are what we call the observable states.
4369892	4376080	B	0.8559542298316956	These are like the sensations of the observations that you get at any point in time, also a stochastic process.
4376690	4379838	B	0.7557090520858765	And finally, you have the autonomous states.
4379924	4383058	B	0.862616777420044	Or you could think about it loosely as the active states.
4383144	4385970	B	0.8206868767738342	They turn out to be the active states in the implementation.
4386310	4397640	B	0.8012951016426086	But so the active states, they're really like the muscles, the things you can actually activate and make and then activate to influence the world.
4399290	4408122	B	0.8648690581321716	So we just partitioned the world into these three sets of states that interact and evolve in some way.
4408176	4411830	B	0.8057331442832947	They're both stochastic processes that interact.
4411990	4428350	B	0.8326492309570312	And the goal of the community principle, at least when applied to decision making, is that you want to describe A as a function of O, because what happens as an organism, you have control over A, you have access.
4428420	4432062	B	0.684463620185852	To O, but you don't have direct control of O over your sensations.
4432206	4437694	B	0.4971587359905243	And you don't have access to S because that's the environment and beyond the markup.
4437742	4441038	B	0.7146804928779602	Like it beyond you, beyond your envelope.
4441214	4442642	B	0.5201667547225952	So you don't have access to S.
4442696	4447750	B	0.8436412811279297	You know, O and you can control A is what you can choose from.
4447900	4462234	B	0.8426992297172546	And so the free energy principle just answers the question, okay, well, if I take this very general description of the world, what is the equation of A as a function of O?
4462272	4464954	B	0.8448541760444641	How can I describe A as a function of o?
4465152	4471662	B	0.6199586987495422	And so it turns out there's some mild assumptions in this.
4471716	4483170	B	0.6997087597846985	And these assumptions, they're guided by physical considerations about how humans are and how humans interact with the world, but they're very mild.
4483670	4485154	B	0.7492143511772156	But whenever you take.
4485192	4493800	B	0.714212954044342	These assumptions, you get that the active states or the autonomous states minimize expected free energy.
4495770	4501542	B	0.8654237985610962	So this is like a very 16th derivation of the expected free energy.
4501596	4504706	B	0.7830254435539246	From first principles, we start with the partition of the world.
4504828	4508390	B	0.8759817481040955	We describe active states as a function of observations.
4508550	4514810	B	0.8319576978683472	As it turns out, that the expected free energy is what describes the active states as a function of observations.
4519330	4535346	B	0.8146679401397705	The basic active inference algorithm, which everybody uses and which we have also in paper, is about computing the expected free energy and then selecting actions that minimizes expected free energy.
4535528	4540500	B	0.8720679879188538	So the expected free energy, as we saw, I think it's in the next figure.
4540870	4546120	B	0.8543822169303894	So the one with all the panels, we've seen it a couple of times already.
4546890	4550920	B	0.7974277138710022	It's an expectation with posterior distributions in it.
4552410	4564940	B	0.864340603351593	What I mean by posterior distributions here is that these are distributions conditioned on the history of the agent, so on the observations that he has already seen and on the action that he has already taken.
4568350	4575230	B	0.870394766330719	In any case, the expected free energy is an expectation with posterior distributions within.
4575380	4579038	B	0.5760014653205872	So it means that to compute the expected free energy, you have two problems.
4579204	4584702	B	0.8488243222236633	You have computing posterior distribution through Bayes and inference, Bayesian rule.
4584846	4589230	B	0.840886652469635	And once you have them, you can plug them in into this equation.
4589310	4594674	B	0.7492284774780273	And then you need to complete the the expectation, and then you get the expected free energy.
4594792	4599202	B	0.8445115685462952	So the first thing is about computing the posterior distributions.
4599346	4606454	B	0.891363263130188	Now, what has been proposed in the literature so far is, well, how do you do approximate inference?
4606502	4608646	B	0.8307991623878479	How do you actually approximate distributions?
4608678	4613740	B	0.8661447167396545	You do that through variational inference or approximate inference by minimizing free energy.
4615790	4625680	B	0.5077073574066162	The treatment here, the derivation, and our aim was to do that, provide a derivation that was as conceptually simple as possible.
4626050	4631860	B	0.5923187136650085	Actually, it highlights that the free energy is not the most important thing here.
4632710	4635234	B	0.6360158324241638	The important thing is really the expected free energy.
4635352	4641794	B	0.8381502032279968	The free energy is just a tool to approximate the posterior distributions, to then get the expected free energy.
4641992	4645890	B	0.7897269129753113	But you could actually use any other type of divergence.
4646330	4650914	B	0.6416592001914978	The free energy is just like a KL divergence plus some term that makes the whole thing tractable.
4650962	4655750	B	0.870716392993927	And so you can minimize the KL divergence to approximate the target distribution.
4656090	4662938	B	0.5143055319786072	But actually, what this view highlights, and by the way, I'm not at all against the free energy.
4663024	4664540	B	0.9856874346733093	I think it's really cool.
4665150	4667740	B	0.7069664597511292	And there's so many methods to minimize free energy.
4668510	4671486	B	0.7898216247558594	If you can do that, then fine and sure, go for it.
4671508	4672622	B	0.5905004739761353	Like, way to go.
4672756	4674830	B	0.611361026763916	But imagine you could not do that.
4674900	4682900	B	0.6663652658462524	It would not at all be a problem because you could use any other kind of divergence to solve those inference problems.
4684950	4686274	B	0.746707558631897	So that would be the first step.
4686312	4695080	B	0.8954510688781738	You can approximate those particular distributions by doing some kind of approximate inference, which would be through minimization of reenergy or through minimization of something else.
4695690	4699590	B	0.8133032917976379	And then you have the second step, which is computing the expectation.
4699930	4711610	B	0.7863459587097168	Now, either you're in a low number of states, discrete states from the P like thing and it's all a matter of vector matrix multiplications that are attractable.
4712030	4724960	B	0.7368841171264648	Either you have a very high number of discrete states and then you need to think about sampling in discrete states, which we didn't discuss in this paper.
4725330	4735300	B	0.8636559247970581	Either you are in a continuous state space and then you have an expectation in continuous state space and then you want to think about an alternative Monte Carlo, for example.
4736790	4748150	B	0.8560490608215332	So now with these two steps you have now an estimate of the expected free energy which gives you the quality of any action sequence.
4748730	4758102	B	0.624169647693634	And so it's not only the quality, but it's actually the negative Lock probability of any action sequence regarding in this formalism.
4758246	4771950	B	0.8008872866630554	Because if you remember, the premise of all this was to describe actions as functional sensations in a physical system, not even a physical system, but in interacting stochastic processes.
4774050	4786130	B	0.8123092651367188	And so the answer to that was well, the expected free energy gives us how actions relate to sensations, the expected free energy.
4786200	4806890	B	0.7733919620513916	And this is why we used the letter, we used minus Lock p as opposed to G to emphasize that the expected free energy is not just like a function of action sequences, but it is really the negative log probability of an action sequence given some sensations.
4807950	4816650	B	0.8421646356582642	And so once you have computed the expected free energy, you had this minus log probability of an action sequence.
4817010	4829870	B	0.7088324427604675	If you take the exponential negative of that, the exponential negative of the expected free energy, you get the probability distribution over action sequences.
4832210	4838674	B	0.5692906379699707	And so by the way, this exponential negative of expected free energy is what we use all the time.
4838712	4843222	B	0.7421658635139465	You might recognize this as the soft max of negative expected free energy.
4843276	4848870	B	0.7302092909812927	This is just all the time kind of fundamental thing, active inference lab models.
4849210	4851640	B	0.7933374047279358	So I'm really talking about the same thing here.
4853370	4863318	B	0.787348747253418	So once you take the soft max of negative expected free energy, you get P of A, which is the distribution over action sequences.
4863414	4865770	B	0.7460287809371948	And now you have two possibilities.
4866270	4879070	B	0.8656709790229797	Either you want to stimulate the most likely action sequence, in which case you want to simulate the action sequence that maximizes the probability distribution.
4880370	4885220	B	0.6889986991882324	So in effect, you have an optimization problem out of all.
4885830	4899534	B	0.8571478724479675	Yeah, you need to find the action sequence that is going to maximize this probability distribution that's given by the expected free energy or you want to simulate a typical action.
4899582	4901742	B	0.7943265438079834	So when do I mean a typical action?
4901886	4909210	B	0.806886613368988	A typical action or a typical action sequence is just a sample from the distribution.
4909710	4924560	B	0.5337206721305847	And so if you want to sample from I mean, if you want to do that, then you have a sampling problem where again you need to sample from a distribution over action sequences given by the expected free energy.
4925250	4929360	B	0.8265703320503235	So this is really how all these methodologies connect.
4933430	4934850	B	0.789521336555481	I think that's kind of it, yeah.
4934920	4942726	B	0.6234384179115295	One last thing is, typically when we stimulate active inference, we never do the sampling at the end.
4942828	4946760	B	0.7613165378570557	We always take the action sequence that minimizes expected free energy.
4949450	4955110	B	0.8393478989601135	In other words, the action sequence that maximizes that probability distribution.
4955950	4963958	B	0.6668931841850281	And this is because when we simulate things, it turns out this is how people have done it in the literature.
4964054	4972538	B	0.692384660243988	People are more interested in the most likely action sequence that an organism or an agent would produce.
4972714	4988340	B	0.8389546871185303	But if you want to model data, if you want to use active inference to model data, then you actually want to not just simulate the most likely action sequence, but simulate any kind of action sequence that would fall out of this.
4988870	4996310	B	0.6510154604911804	And so this is really the case where you need to sample from that final distribution as opposed to do your optimization.
4997050	5007402	B	0.5979143381118774	So depending on the use case, you either have a sampling or an optimization problem after you've computed the expected free energy.
5007536	5010940	B	0.8554558157920837	So this is how these whole things fall together.
5011790	5015542	B	0.7543129920959473	And so you might be asking, okay, well, you talked about sampling and optimization.
5015686	5017914	B	0.8788955211639404	There's also a section about inference.
5018042	5019598	B	0.7571502923965454	Where does that fit in?
5019764	5029822	B	0.810906708240509	And so it fits in on the remark I gave that we have these postered distributions within the expected free energy.
5029876	5031234	B	0.746164858341217	How do we compute them?
5031272	5032754	B	0.8488842844963074	How do we approximate them?
5032872	5035234	B	0.758179247379303	One way is by minimizing free energy.
5035432	5039410	B	0.7060796618461609	Another way is by minimizing any other kind of divergence.
5040070	5054470	B	0.8448272347450256	And in that section over there, we just reviewed some kind of divergences that are very popular in the statistical inference literature, mainly because they have desirable properties.
5055690	5072160	B	0.783072829246521	And so if you weren't able to use the variation of free energy for some reason, or maybe something to be explored, and something to be explored is just to use other types of divergences and just see what happens.
5074210	5083520	B	0.7202911972999573	So the open problem to be explored is whether we can get better performance by using different kinds of divergences and see what we get.
5084210	5095190	B	0.5351274013519287	Maybe there's part of algorithms that are out there with these other types of divergences that we can make use of to do better performance, to get better performance.
5095770	5102834	B	0.6663272380828857	Can we actually describe and quantify the improvements in performance that we get by using these other types of algorithms?
5102962	5106090	B	0.8588970899581909	When would these be appropriate and useful?
5106430	5108060	B	0.8096912503242493	These are all open questions.
5109630	5122240	B	0.5372644066810608	Another open question that I think is interesting is can we model maladaptive behavior by using different kinds of divergences that would not work as well as the free energy?
5122770	5129494	B	0.8761678338050842	And there's an interesting paper on that by newer sajid and colleagues.
5129642	5135742	B	0.8974530696868896	I think it's called Bayesian Brains and the Rennie Divergence.
5135806	5141620	B	0.9222671389579773	It's been published on neurocomputation, I think, last year or the year before.
5142170	5165574	B	0.843018114566803	And so in that paper, instead of using the variation of free energy to approximate all these posterior distributions, she used the rainy divergence, which is which generalizes the KL divergence in some way, and show that for different rainy divergences, you get different types of approximate posteriors.
5165622	5175626	B	0.8718583583831787	And she basically looked into what kind of differences you get from there in terms of perception, in terms of decisionmaking.
5175818	5189902	B	0.8499873876571655	And she showed that for that particular divergence, divergence, I think the conclusion was that you basically get different phenomenology, you get different behavior, suggest something to be explored.
5190046	5207660	B	0.7537193298339844	I think the upshot was that maybe I don't remember how far the paper went in this, but I think kind of the goal was to model maladaptive behavior using some kinds of rainy divergences that did not work as well as the free energy.
5208030	5212620	B	0.9545050263404846	So this is to say, so this is an interesting work.
5214110	5222160	B	0.6841573119163513	One could examine other kinds of divergences and see whether you actually get better or worse performance than with the free energy.
5223250	5234130	B	0.7829123735427856	One thing, one word of caution, though, is the free energy is just so good in the sense that it uses the KL divergence.
5234550	5241810	B	0.6161341667175293	And as I mentioned at the beginning, the KL divergence has so many properties and it's just so fundamental.
5242730	5247974	B	0.6428871154785156	It's not straightforward to see that when you first come up with it.
5248012	5262186	B	0.9010772109031677	But I guess the more I read in different disciplines and the more the Chao divergence comes up and the more the more I see properties of Kale divergence in different disciplines that just make it so interesting.
5262368	5266962	B	0.7823630571365356	Like, for example, the Kale divergence, Bayesian Statistics Physics is the relative entropy.
5267126	5273182	B	0.8403634428977966	So it quantifies the amount of entropy that a distribution has with respect to another.
5273316	5279130	B	0.7453998923301697	Entropy, as we know, is just a very fundamental thing in information theory.
5279210	5304262	B	0.8251628279685974	The KL divergence quantifies the difference in information between two distributions, the amount of bits that you would need to if you take two distributions, A and B, the KL between the two quantifies the amount of information that it takes to go from one to the other.
5304396	5310826	B	0.7114831209182739	So now you might object and say, okay, but KL A and B is different from KL, B and A.
5310928	5321070	B	0.5084478855133057	And so how can it be that it quantifies the amount of information they need to go from A to B or B to A because it's not symmetric.
5321490	5330426	B	0.8183233141899109	So the answer is it's either KL of A and B that has this meaning, or KL of B and A that has this meaning.
5330458	5340500	B	0.6564977765083313	And I just can never remember which direction, but it's one of them that has this interpretation in terms of the difference in information anyway.
5341110	5354870	B	0.9739173054695129	So Kia is just like something, it just comes up everywhere and it's just so useful, and it has all of these nice properties which makes these free energy really a construct of choice.
5356030	5368330	B	0.8277714848518372	But that said, there's other divergences, one of them which we describe a bit in the paper called the maximum mean discrepancy, which I think also has properties.
5368670	5377178	B	0.495259553194046	It's not necessarily well, it's very different in terms of how you construct it from the KL divergence.
5377354	5381538	B	0.8497483134269714	But I think my current understanding right now.
5381704	5389460	B	0.5364258289337158	And my current understanding right now is that if you cannot use KL, then maximum mean discrepancy is just like, be nice.
5391590	5392114	B	0.650388777256012	But yeah.
5392152	5403160	B	0.5147890448570251	So the word of caution was that probably most divergences out there are not going to do as good of a job as a free energy, but some clever ones might.
5403530	5413462	B	0.591472864151001	And it's an open problem of determining which is how we get the link between all these different sections.
5413606	5417898	B	0.8128737211227417	So to do perception, we need the inference.
5418074	5425226	B	0.8744189739227295	To do decision making, that is, computing the expected free energy, we need sampling typically.
5425418	5430398	B	0.7833780646324158	And then to do action selection, we either need sampling or optimization.
5430574	5435998	B	0.8257149457931519	And also inference is an optimization of belief.
5436174	5441230	B	0.8765897750854492	Updating also, as we've discussed, is also an optimization of probability distributions.
5441310	5444102	B	0.8788114786148071	You can see that as an optimization of beliefs as well.
5444236	5446870	B	0.7140670418739319	So it's all very tightly interconnected.
5450170	5451014	B	0.7093686461448669	Wow.
5451212	5452194	A	0.901231586933136	Thank you, Lance.
5452242	5455110	A	0.9201524257659912	That's very informative.
5456990	5471146	A	0.7654559016227722	Brings up a lot of ways to go and it really shines a different light on even what, learning active inference or learning free energy principle.
5471338	5473870	A	0.5396705865859985	It was a roller coaster just listening.
5474210	5491730	A	0.6649430990219116	When you describe the figure of the running person and the new generalized representation here, which is so sparse it looks like it's pseudocode, but it's actually basically necessary and sufficient to describe action.
5493350	5499846	B	0.7019906640052795	Yeah, it is pseudocode and it is also exact like this is really what active inference boils down to.
5499948	5502310	B	0.819665789604187	And that was kind of like where we got at.
5502380	5506274	B	0.5404688715934753	We're like, okay, well, we had this mess of papers of active inference.
5506322	5526282	B	0.6788203716278076	Not that they're actually immense, but, you know, there's so much information and we're really thinking, reading all the latest free energy principle literature that I've also worked on a lot and also active inference lab literature and really thinking, okay, well, how do we strip all that of the neuroscience?
5526346	5528826	B	0.5734038949012756	How do we strip all that of the cognitive science?
5528938	5533074	B	0.7351275682449341	How do we just retain the math and present it in the simplest way possible?
5533272	5538930	B	0.8926223516464233	That would be appealing for mathematician and also hopefully for a computer scientist.
5539430	5545240	B	0.7559744715690613	It turns out that we got a way simpler perspective than anything that's out there, I think.
5546490	5547142	B	0.6613861918449402	And really?
5547196	5547414	B	0.5491447448730469	Yeah.
5547452	5555080	B	0.8836310505867004	So this is the active inference algorithm in pseudocode and in detail also in a way.
5557150	5558698	A	0.8223100304603577	So totally agree.
5558784	5568490	A	0.9376362562179565	Again, thanks for the work and for sharing it this way because it is the fewest pixels for the highest resolution picture.
5570370	5590046	A	0.8620169758796692	Then to describe the fundamental cybernetic challenge as a partition, an axiomatic partition, which one can then say also has grounding in the spatial temporal boundaries of the world or in geometric boundaries in informational spaces.
5590078	5609690	A	0.867155134677887	But the particular partition is used to separate in the map, not necessarily making claims about the territory and the actual nature of the objects and their articulations or anatomy, but on the map, which we get to construct.
5610110	5619450	A	0.7237273454666138	We make it in a way that's amenable to the particular partition, which is not too much more than the separation of figure from ground or agent based modeling.
5619530	5625520	A	0.8295994997024536	It's just a separation of some autonomous entity from some external process.
5627410	5652146	A	0.6938410401344299	Then the task of the free energy principle applied to decision making, as you said, was to describe action as a function of observation and everything in between is broadly considered cognitive but for any given system it's going to play out in this immensely nuanced way with a lot of bespoke mechanisms.
5652338	5656040	A	0.6725342273712158	And why do we take that particular partition step?
5656350	5671374	A	0.48964712023735046	Well, it's kind of like read, write access in the computer system there's things we don't have access to hidden states and also the reverse of that which is we can't access hidden states nor them influence or affect us.
5671412	5674874	A	0.7731111645698547	So I think of that as like no telekinesis, no telepathy.
5675002	5677386	A	0.5301902890205383	You can't go directly across the blanket.
5677498	5681902	A	0.8309520483016968	You have to intermediate through the blanket of observation and action.
5682046	5694758	A	0.8102822303771973	And then with respect to our particular states, our blanket and internal states, we have access to observations, but not direct control, nor necessarily would we want to.
5694844	5709366	A	0.4794228971004486	Because if we had the lever to change what we directly received, then algorithms might learn strategies that basically self deceive so that the observations look good, look good, look good, until the whole system crashes.
5709478	5713386	A	0.6376197934150696	So we want access to observations but not direct control.
5713568	5724254	A	0.7552141547203064	And then the autonomous states are what we in the optimal ideal situation have total control of which is like our mind and our body, what we think and what we do.
5724452	5740702	A	0.7609443068504333	It turns out through the pragmatic turn and the inactivist insights in cognitive science that a lot of action sequences have to do with changing what observations are sought after like epistemic affordance.
5740846	5748258	A	0.7728660106658936	So there is an enmeshment of action but it's also really important that we have access but not control of observations.
5748434	5765130	A	0.6192466020584106	We don't want to have our control on the thermometer but we want the best possible thermometer and then we want to control the best possible interpretation that's like signal processing and the best possible action sequence which is like decision making and control theory.
5765470	5775870	A	0.8964024186134338	And then free energy principle is addressing that question what is the equation of action as a function of observation?
5777650	5784978	A	0.6391056776046753	But then it was quite a rollercoaster when you said that the free energy wasn't even necessarily the only way to do it.
5785064	5786820	A	0.5234559178352356	But it's absolutely true.
5787670	5801202	A	0.7673506140708923	The properties of the free energy are inherited from one of the terms being KL divergence and the other having the ability to basically ignore in certain relative expected free energy calculation contexts.
5801346	5808342	A	0.8733884692192078	So then in that situation differences in free energy do come down to differences in the KL which does have all these properties.
5808486	5826800	A	0.7666658163070679	But that doesn't mean that the free energy is itself axiomatically posited it's actually downstream of the particular partition to use free energy or anything like that at all and other discrepancies may have other properties in different ways.
5828130	5828880	B	0.5662814974784851	Exactly.
5830050	5842660	B	0.8905647397041321	There's a bit of a nuance in the sense that here we presented version of the free energy principle, just describing decision making as you summarize though describing A as a function of O.
5845510	5851000	B	0.5145253539085388	And by the way, about that, we didn't even talk about Markov blanket in this paper.
5852090	5860570	B	0.588778555393219	I would say that the markup blanket is under the hood because we're saying, okay, well, these are the states that you have access to and these are the states that you do not.
5860720	5863114	B	0.8705258369445801	The states that you have access to are A and O.
5863152	5864490	B	0.4982931613922119	The states are the agent.
5864640	5867222	B	0.6716693043708801	The states that you do not have access to are the environment.
5867366	5874590	B	0.7539800405502319	So in some sense there is a markup blanket, but we didn't even have to mention that in the paper.
5874660	5883440	B	0.8244958519935608	It's just you partition the world into three sets of states, S, O and A, that are by definition A.
5884050	5891860	B	0.6146377325057983	And you just yeah, just from this trip partition, you want to describe one as a function of another, ignoring the third.
5893430	5897654	B	0.7511364817619324	And so that's how you derive expected green energy.
5897692	5903158	B	0.8709314465522766	You see that action sequences are described by the expected free energy.
5903244	5914294	B	0.6826677918434143	And this is a functional sensations if we want further in the and so this is just what you need crucially, this is just what you need to derive.
5914342	5925950	B	0.828951895236969	Active Inference Lab Algorithm if we went further in reviewing the free energy principle, then we would see the fundamental role that the free energy plays.
5928530	5939940	B	0.6002840995788574	And so actually, when reading the latest papers on the mathematical theory of the free energy principle, the role of the variation of free energy is pretty clear.
5941990	5953222	B	0.6705551743507385	But here the point is that if we just care about decision making, if we just care about the normal active inference algorithm, then we don't even need the variation of free energy.
5953356	5956402	B	0.6350269317626953	So sure, the free energy should be preferred.
5956466	5959114	B	0.7258390188217163	Why not, if it is available?
5959232	5964970	B	0.6404794454574585	But if it is not for some reason, then there's no reason why not to use another kind of divergence.
5972610	5973840	A	0.9324204325675964	Very interesting.
5974210	5992398	A	0.7794707417488098	It's making me think about linear regression and the sum of squares, the L two norm is one approach that's often used to fit a regression line because it has good optimization properties, there's good software packages, there's good education, there's good communication around it and so on.
5992504	6000854	A	0.8966524600982666	But one can select other norms and choose to fit a linear regression with an L one norm or with an L three norm.
6001052	6010422	A	0.7617841362953186	And so that entire question of fitting the linear aggression is a degree of freedom.
6010566	6021070	A	0.8859283328056335	How the regression is fit, that's downstream of a commitment to, for example, model a system in a generalized linear modeling framework.
6021490	6036366	A	0.8757191896438599	And so analogously, the upstream commitment or the first principles which yes, can be understood as axiomatic and also have some empirical status in terms of this partitioning.
6036398	6043410	A	0.8455855846405029	A figure from ground the particular partition can simply be accepted axiomatically, which is to say without appeal to evidence.
6043570	6051750	A	0.8699623942375183	Or somebody might have another upstream axiom and choose to model things according to a particular partition.
6052410	6078160	A	0.7383331060409546	From there, just like we could have chosen the L one, two, three norm there are different discrepancy criteria or measures that we might want to use, and some of them apply better or worse or not at all, depending on what software, hardware, data set, and generative model we have.
6079490	6080960	A	0.5658016204833984	And so it makes sense.
6082230	6093990	A	0.9039595127105713	Pull back into the upstream understanding active inference lab as a process theory for particular partitioned systems.
6094730	6112406	A	0.8738416433334351	And then for those who want to engage in the modeling to have that discussion about the garden of the branching paths, well, you could use a discrete time or you could use a continuous time, and then from here, you could do this sampling.
6112438	6113418	A	0.7932153940200806	Or you could do that one.
6113504	6116266	A	0.5748830437660217	And if we have this computer access, we can do that.
6116288	6118574	A	0.7976453304290771	But if we have to do it this way, we'll do it like that.
6118772	6135294	A	0.7750750780105591	And that's all operational and logistical, but it's actually all under the umbrella or under the auspice of the theoretical or conceptual commitments that are actually not being questioned once one is in that modeling discussion.
6135422	6142806	A	0.7078117728233337	Just like you could have the L one two three norm conversation, and maybe a reviewer asks you why you chose the two norm versus the three.
6142988	6150598	A	0.7557327747344971	But it's a broader level of questioning why one took on the linear modeling framework at all.
6150764	6160810	A	0.8103501200675964	And our analogous upstream bottleneck, not in terms of rate limiting, but just in terms of like, eye of the needle is the particular partition.
6164350	6165242	B	0.824190080165863	Yes, definitely.
6165376	6175886	B	0.8254960775375366	And I just want to add something actually about the choice of divergences or the choice of discrepancy that you might want to use to solve the inference problem.
6176068	6179614	B	0.9527152180671692	I think the analogy with linear regression is a really good one.
6179812	6189342	B	0.8323777318000793	When you do linear regression, yeah, you can use all types of norms, and it's really a design choice here in the algorithm, you also have that design choice that you're going to use KL.
6189406	6192710	B	0.6914507150650024	So free energy to do active inference.
6194570	6204626	B	0.8497890830039978	By the way, the inferences are really equation 43 and 44, which are approximate actually, you had them in the slide.
6204658	6209080	A	0.6507489085197449	Already, 43 and 44 here, got it.
6210090	6216502	B	0.9099298715591431	Which are approximate some posterior distribution with an approximate posterior distribution.
6216646	6223498	B	0.8458822965621948	So you can do that with the pre energy, which is the same as the KL divergence, or you could do that with a whole bunch of other divergences.
6223594	6229150	B	0.7597703337669373	One that I mentioned and that I think is particularly interesting is the maximum mean discrepancy.
6229670	6240450	B	0.7744119763374329	And so here's the difference between KL and maximum dean discrepancy, at least, I guess, an important conceptual difference.
6240600	6257820	B	0.8275598287582397	So the KL, when you look at distributions that are very close to each other, it's basically going to become symmetric when the distributions are very close, and it's going to measure the amount of information that differs between them.
6258190	6274250	B	0.772260308265686	The maximum mean discrepancy, when you take two distributions that are very close, it reduces to what people call the Earth movers distance, also called like the vanishing distance so here's an intuition.
6274410	6285810	B	0.8531322479248047	If you take two distributions that are very close, just imagine distribution A as a pack of dirt and distribution B as a pack of dirt or a sand with some shape.
6286150	6301766	B	0.4958977997303009	The maximum mean discrepancy is going to tell you the amount of work that you need to put all that dirt from distribution A and pile it in the shape of distribution B.
6301948	6317406	B	0.7205039858818054	So of course, there's so many different ways in which you could take all that dirt from distribution A and remodel it in distribution B, but you're interested in the minimal energetic cost that would take.
6317428	6322670	B	0.7616227269172668	So like the optimal way of doing that movement, people call that optimal transport.
6323730	6345846	B	0.8494371175765991	Now, if you're familiar with optimal transport, you know that the Vaster Stein distance is a distance between probability distribution that regardless of how far they are, is going to measure, is going to give you this cost of optimal transfer.
6345948	6355240	B	0.7596909999847412	So the energetic cost of moving all that pack of dirt from place A to place B in the optimal way.
6356750	6360380	B	0.7795665860176086	So the vaster shine distance is something that we could use here.
6362110	6369814	B	0.7723844647407532	But maximum mean discrepancy, it has a lot of very nice properties and basically reduced it to the Vaster Stein distance.
6369862	6394050	B	0.5837664008140564	When we consider very close distributions, this is not something that just a mathematical curiosity, but it's done that when one builds a distance out of a divergence, what one does is one takes very close distributions, measures the divergence between them, at which point the divergence is pretty much symmetrical.
6394470	6399606	B	0.7993095517158508	And then you basically keep adding divergences along the trajectory until you get to the final one.
6399708	6413340	B	0.7807453870773315	And this way, by aggregating divergences between very close distributions and doing that and adding that along a trajectory, you get a distance, a meaningful distance between distributions that could be very far.
6414110	6434020	B	0.879014790058136	If you do that with the Kale divergence, you get what's called the fisher information distance, which measures really the amount of information that took you from to go from one distribution to another distribution, regardless of how far they are.
6435190	6440626	B	0.6263387203216553	If you do that with the vast search time distance, you will get the optimal transport cost.
6440728	6447718	B	0.7939220070838928	So the amount of energy that you need to put all that dirt, place A to place B in the optimal way.
6447884	6454040	B	0.7692809700965881	If you do that with the maximum discrepancy, you would also get that you will also get the optimal transport thing.
6458330	6471600	B	0.6089121699333191	I just want to say that the maximum discrepancy discrepancy between two distributions that are far away will not coincide with the Vaster Stein distance, which gives you this optimal transport cost.
6472130	6490574	B	0.8451643586158752	But when you actually derive a distance from these divergences, the distance derived from the maximum discrepancy and the Vaster Stein distance will end up being the same and the distance that you actually get or the topology that is derived from the distance.
6490622	6495430	B	0.8397900462150574	So by topology, a topology is really a notion of closeness.
6496890	6502310	B	0.8500978946685791	And so to understand closeness, you need to understand infinitesimal distances.
6503050	6508380	B	0.8786494731903076	So the infinitesimal distances derived from Vaster Stein or Mmd, they're the same.
6508830	6518650	B	0.7976586818695068	The topology that results is the topology of what people call weak convergence, which is a standard topology that's considered in probability theory.
6520990	6540610	B	0.5538727641105652	So Mmd and Vaster Stein, they are very natural in that sense, in the sense that people say that they met rise, weak conversions, like they give rise to the standard topology between probability distributions.
6542330	6555602	B	0.523680567741394	So coming back to the choice of divergences, if you're interested in approximating distributions in the sense of approximating their information content, then KL is very natural.
6555746	6574190	B	0.6045965552330017	But if you were for some reason and maybe would not be in this kind of application, but another kind of application, if for some reason you're interested in approximating distributions for the sake of how close they are when you look at them, then you wouldn't use Mmd or Vassagetein.
6576930	6581220	A	0.9619736075401306	Wow, great information.
6581910	6588980	A	0.756279468536377	And I'm just thinking about we're switching lanes on the highway we're trying to get from here to there.
6589430	6603234	A	0.8281393647193909	Yes, we want to know about the informational closeness of this tale of two densities, but we also want to know about the transport closeness because we have a schedule and a budget and decisions to make, and there are trade offs.
6603362	6615530	A	0.6996645927429199	So being able to move the Earth optimally while we're switching lanes and accelerating and slowing down, I know this is mixing many angles informally.
6616930	6631520	A	0.652933657169342	We want to have a lot of options for how to think about that challenge of moving dirt between the tail of two densities and make sure everybody is driving in the right lane at the right time.
6633170	6641490	B	0.5401840209960938	Definitely, yeah, it's a design choice and it's an important one because it depends on what kind of properties we want to preserve.
6643930	6654520	B	0.6581658720970154	All these divergences, some are not so useful, I guess, but some of them, they're just very natural and vasterstein and Mmd, they're very natural in that sense.
6656350	6658220	B	0.6350929141044617	Very important to keep in mind.
6658910	6666460	B	0.8134635090827942	And not only active inference lab, but just in general for any kind of inference problem.
6669330	6681940	A	0.9117320775985718	Well, let us each have a closing round or reflections, any thoughts or any next steps or any suggestions or other information you'd like to provide.
6684070	6685122	B	0.610451340675354	It's hard to say.
6685176	6689140	B	0.5615814924240112	I feel we've covered so much already.
6691270	6707114	B	0.9462460875511169	I think to me, what's most exciting about is scaling active inference right now, because you get this active inference algorithm in the paper that's the right compares principles and just from the derivation, you see.
6707152	6715930	B	0.5769097208976746	Okay, well, there's actually so many things I dean, the assumptions are so small that there are so many things that you can model with this active inference algorithm.
6719470	6722634	B	0.7630358934402466	And all the heavy lifting is done by the generative model.
6722832	6726474	B	0.8319457769393921	So if you use one generative model, you will get one behavior.
6726522	6731760	B	0.8534289002418518	If you use another generative model, you will get another behavior, but all the equations will remain the same.
6734310	6736366	B	0.8102869391441345	It speaks to the generality.
6736558	6740066	B	0.8373836278915405	Now in having something that's very general, you.
6740088	6742680	B	0.769500195980072	Get something that's also very non specific.
6743050	6756700	B	0.624697208404541	And I guess for neuroscientists and people who are interested in intelligence, generality comes at a cost of being non specific and non specific about the brain in general.
6757550	6766470	B	0.7879037261009216	So really the big question to me long term is what kind of generative models do we need to simulate brain like behavior?
6766550	6770990	B	0.9221968650817871	Because this is really the interesting behavior or the most interesting behavior.
6771810	6778750	B	0.7006344795227051	So it speaks to a big research program that a lot of people are carrying and have been carrying for a long time.
6778900	6785250	B	0.6501073837280273	But it's about what kind of representations do we have of the world?
6785320	6787780	B	0.7175998091697693	What kind of priors do we have?
6788950	6793142	B	0.8520788550376892	Can we identify the priors that we are born with?
6793276	6807926	B	0.7810091376304626	There's a lot of research on computational or just playing cognitive science, like normal cognitive science, just studying babies and seeing what kind of priors, what kind of basic information they have when they come out of the womb.
6808038	6810460	B	0.7840346097946167	There's a lot of things that they can already do there.
6810910	6821518	B	0.5589888691902161	Our genetic code is preconditioning us to operate efficiently in this world and be able to flexibly adapt to any kind of situations that might arise in the natural world.
6821684	6827950	B	0.7035109996795654	And so it's a huge research program to understand and model these priors.
6831090	6836050	B	0.749280571937561	And not only the priors, but also the likelihood, all the representation, all the state spaces.
6836790	6839330	B	0.6667702198028564	So I think that's the way forward.
6839480	6848774	B	0.9265022873878479	The creation principle is very elegant because it gives you a very succinct description in terms of a giant model that enables you to simulate pretty much everything.
6848972	6851974	B	0.6180498600006104	But we're not interested in simulating anything.
6852092	6855046	B	0.6312428712844849	We're interested in simulating brain like behavior.
6855078	6862860	B	0.7358378767967224	And now we need to drill down even more onto the kind of Janitor models that would be amenable to that.
6867030	6867780	A	0.7671424746513367	Great.
6869110	6878360	A	0.5044376850128174	My closing reflection I feel like I know less about fep, but more about something else.
6878730	6881030	A	0.8556347489356995	For what we've discussed.
6882830	6893814	A	0.9640879034996033	Earth was moved, Bayesian mechanics were called in, decisions were made, and it's been a really great series.
6893862	6902400	A	0.9852495789527893	So I'm appreciative and thankful that you suggested this paper in our correspondence as one to discuss.
6903250	6911300	A	0.7982114553451538	You were absolutely right that it is relevant to bring to the attention of the active community.
6911910	6924100	A	0.5836899280548096	And I hope that everybody who reads or listens thus far has shifted lanes so they can be where they want to be too.
6925190	6926660	B	0.6031550168991089	Well, yeah.
6927670	6929010	B	0.9793016910552979	Thank you so much.
6929160	6934010	B	0.9887388944625854	I also really enjoyed the session and think we had a really cool discussion.
6934430	6935482	B	0.8881668448448181	So thanks again.
6935536	6938140	B	0.9368001818656921	I hope to do this again soon.
6938510	6939094	A	0.5708940029144287	Excellent.
6939142	6940460	A	0.7168500423431396	Anytime you'd like.
6940910	6941900	A	0.6945193409919739	Very well.
6942750	6943350	A	0.7207356691360474	See you, Lance.

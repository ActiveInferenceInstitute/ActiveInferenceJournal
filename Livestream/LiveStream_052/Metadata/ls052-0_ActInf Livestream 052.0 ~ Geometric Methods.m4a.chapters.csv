start	end	startTime	summary	headline	gist
13070	62850	00:13	This is ActInf Lab Livestream number 52.0, and it is February 28, 2023. We're a participatory online institute that is practicing applied active coherence. All backgrounds and perspectives are welcome and we'll be following video etiquette for live streams.	ActInf Lab is a participatory online institute practicing applied active coherence	Live Stream
64630	237530	01:04	We're here to discuss the paper Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents. Like all Zero videos, this one is an introduction for some of the ideas. We would like you to get involved in the upcoming 52.1 and 52.2 discussions.	This video discusses Geometric Methods for Sampling, Optimization, Inference	Learning and Discussing Geometric Methods for Sampling, Optimization
238110	399390	03:58	How can we effectively and efficiently navigate information geometric or information theoretic landscapes? How can we optimize inference in complexity models, including cases where we are doing inference on action as a parameter? What technical underpinnings support the rigor and applicability of active inference?	How can we effectively and efficiently navigate information geometric or information theoretic landscapes	The Inferring complexity
400370	842010	06:40	Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents by Barp and Costa at all shared first authorship and it was published on active. Their goal is to discuss the emergence of natural geometries within a few important areas of statistics and applied mathematics.	Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents	Geometric Methods for Sampling, Optimization, Inference and
843260	954508	14:03	Keywords of the paper are information geometry, hamiltonian Monte Carlo, Stein's method, reproducing kernel variational, coherence, accelerated optimization, dissipative systems, decision theory and active inference. Let's start with decision theory from the Stanford Encyclopedia of Philosophy.	Let's start with decision theory from the Stanford Encyclopedia of Philosophy	Information Geometry, Decision Theory and Active Inference
954674	1197870	15:54	Let's go to information theory. The paper in the introduction writes of particular relevance to this chapter. If we can compute certain kinds of statistical distances such that a distance requires a length metric, we can find optimal estimators of parameters. This is big for helping us do Bayesian estimation.	Let's go to information theory. Citation 23 is Rao from 1992	Information Geometry in Bayesian estimation
1199440	1769200	19:59	Functional analysis is the study of functions. Sampling methods are critical to the efficient implementations of many methodologies. Overall, functional analysis provides a powerful mathematical toolset for understanding and analyzing the principles of active coherence lab.	Functional analysis provides a powerful mathematical toolset for understanding and analyzing active inference	Functional Analysis in Computational Intelligence
1769700	1903452	29:29	The symplectic integrator is a numerical integration scheme for Hamiltonian systems. These integration schemes are useful across difficult estimation problems. They're used to study nonlinear dynamics, molecular dynamics like protein simulation.	Symplectic integrators form the subclass of geometric integrators	Symplectic Integrator
1903586	2161872	31:43	Brea: We're interested in very well behaved outcomes from a very specific or constrained set of distributions. We can leverage the shadow Hamiltonian property of symplectic methods on higher dimensional conservative Hamiltonian systems. Brea explains how maximum mean kernelized and score matching discrepancies arise naturally from topological considerations.	Brea: Shadow Hamiltonian is a property of symplectic integrators	Inference with symplectic integrators and Shadow Hamiltonian
2161926	2270690	36:01	 Accelerated methods achieve faster coherence rates than gradient methods. We propose a variational continuous time framework for understanding accelerated methods. Our work illuminates a class of dynamics that may be useful for designing better algorithms for optimization.	In this paper, through the keywords we go accelerated optimization and variational inference	Accelerated optimization and variational inference
2271960	2399928	37:51	Dissipative systems. The vast majority of statistics and machine learning applications involve solving optimization problems. Accelerated gradientbased methods and several variations thereof have become workhorses in these Fields. There has been great interest in studying such methods from a continuous time limiting perspective.	The vast majority of statistics and machine learning applications involve optimization problems	Hamiltonian Systems
2400104	2734944	40:00	A paper on dissipative symplectic integration with applications to gradient based optimization. This means that we can model conservative and nonconservative, even dissipative Hamiltonian systems. And the applications are vast. In active today, the discretization approach is going to matter a lot.	Hohwy: Are we approaching active inference with this paper	Active Inference 6, Explained
2734992	2957870	45:34	Many algorithms and optimization are given as a sequence of active inference. It is well known that most discretizations break important properties of a system. While numerical integrators typically diverge significantly from the dynamics they aim to simulate, geometric integrators respect the main properties of the system.	Section two discusses optimization of function over smooth manifold M r	Optimization on Smooth Systems
2958320	3141930	49:18	Section Two Two conservative flows and symplectic integrators as a stepping stone. We first discussed the construction of suitable conservative flows. Unfortunately, it is a more challenging task to construct efficient discretizations that retain this property.	Section Two Two conservative flows and symplectic integrators as a stepping stone	Construction of conservative flows and symplectic integrators
3142940	3290584	52:22	Section 23 rate matching integrators for smooth Optimization. damping coefficient controls the strength of the dissipation. We're setting up a system with good smooth optimization characteristics ball rolling to the bottom of a smooth hill.	Section 23 rate matching integrators for smooth Optimization	Slowing the Processes and Smooth Optimization
3290782	3774050	54:50	We're going to talk about a relationship between time and dissipative systems. There are essentially two ways to solve this problem through a dissipative Hamiltonian approach. Simulating second order systems is expected to field faster algorithms.	Damping coefficient influences the computational complexity estimates for convergence in different settings	Conservative and Dissipative Systems
3774980	4211030	1:02:54	On to section three Hamiltonian based accelerated sampling. The purpose of sampling methods is to efficiently draw samples from a target distribution row. A fundamental criterion for efficient sampling is non reversibility.	Section Three brings together Hamiltonian based accelerated sampling with gradientbased methods	Hamiltonian Based Accelerated Sampling
4212940	4414148	1:10:12	The Rudimentary HMC method was proposed for simulations in lattice quantum chromodynamics. There are three critical properties underpinning the success of HMC in practice. Modern HMC methods bypass the issue of slow convergence. Some useful upgrades have been proposed in recent years.	Rudimentary HMC algorithm was proposed for simulations in lattice quantum chromodynamics	HMC Algorithm
4414324	4606950	1:13:34	Section four statistical inference with kernel based discrepancies. RKHS are precisely Hilbert's spaces over which the Diroc distinctions act continuously. Topological methods for Mmds just remember MMDs maximum mean discrepancy.	Section four statistical inference with kernel based discrepancies uses path based inference methods	Statistical Inference with Kernel Based Differences (MKM)
4608760	4792030	1:16:48	 Stein's method compares probability distributions through the study of a class of linear operators called Stein operators. While mainly studied in probability and used to underpin theoretical statistics, stein's method has led to significant advances in computational statistics.	Stein's method has led to significant advances in computational statistics	Stein's Method and Computational Statistics
4792560	5354960	1:19:52	Stein discrepancies and score matchings facilitate Stein variational gradient descent. These tools have proved to be useful in a wide range of contexts. The strength of our methodology is its flexibility, which allows us to design specific models.	The canonical Stein operator and Poincare duality facilitate Stein variational gradient descent	4.2 Stein Variational Gradient descent and Poincare
5355560	5567470	1:29:15	How is this line of basic and fundamental math research by Barp et al. Creating new models that have the operational or denotational semantics that we want for computational statistics. For example, distributions that can be interpreted or used as statistical distributions.	Creating new models that have the operational or denotational semantics for computational statistics	Developing generative models in Bayesian statistics
5569730	5792680	1:32:49	Active Coherence is a general framework for describing and designing adaptive agents. It unifies all aspects of behavior, including perception, planning and learning as processes of inference. By exploiting this geometric structure, we derive the objective functional overarching decision making.	Active Coherence is a general framework for describing and designing adaptive agents	Advocates of Adaptive Agents Through Active Coherence
5794090	6342950	1:36:34	Active Livestream 49 focuses on Bayesian mechanics. In other words, they are precise agents with conservative dynamics. Decision making minimizes both risk and ambiguity. This allows us to relate active inference to information theoretic formalizations of decision making.	Bayesian mechanics allows us to relate active inference to information theoretic decision making	Case 8, Decision Making and Precise Agents
6343770	6528320	1:45:43	Active inference can provide a space or a framework, a method, an approach, software packages. A simple simulation of active inference on a POMDP is provided in figure four. For more complex simulations of sequential decision making, e. g. G involving hierarchical POM DPs, please see citations.	A POMDP is a partially observable model of how actions influence external states	Active Inference in Sequential Decision Making under Uncertainty
6529010	6726420	1:48:49	Clark addresses a recipe for active inference modeling by the Par Pazulo and Friston 2022 textbook. Scaling active inference planning for all possible courses of action is computationally expensive. Scalable inference methods can be used to make active inference more efficient.	Scaling active inference planning for all possible courses of action is computationally expensive	Inverse Bayesian inference 5.2.
6727670	6929230	1:52:07	The method frames perception and decision making as variational inference. Can be combined with discrete active inference to operate efficiently in generative models combining discrete and continuous states. What questions and discussion topics are you interested in? Please write comments before or after the dot one and the dot two.	Continuous active inference can be combined with discrete active inference to operate efficiently	Dissecting Continuous Active Inference

start	end	sentNum	speaker	confidence	text
7960	9150	1	A	0.99907	Hello and welcome.
9520	19310	2	A	1.0	It is March 2023 and we're here in Activem Livestream 52.1.
20080	22440	3	A	1.0	Welcome to the active institute.
22600	29792	4	A	0.76857	We're a participatory online institute that is communicating, learning and practicing applied active coherence.
29936	35830	5	A	0.97633	This is a recorded and an archived Livestream, so please provide us feedback so we can improve our work.
36200	49960	6	A	1.0	All backgrounds and perspectives are welcome and we'll be following video etiquette for livestreams head over active coherence.org to learn more about getting involved in learning groups and projects such as live streams.
50940	69710	7	A	0.97	All right, we are in 52.1 and our goal today is to learn and discuss this awesome paper geometric Methods for Sampling Optimization, Inference and Adoptive Agents of Barp and Acosta at all.
70100	73568	8	A	0.95	And we're going to just jump right in.
73734	83430	9	A	0.99801	Thanks a lot Lance for joining and those who have listened to 52.0 will know me.
83800	88272	10	A	0.97977	I'm a researcher and learner with respect to this topic.
88416	99290	11	A	0.98162	So thanks again for joining and it'll be great to have you introduce yourself and also tell a little bit of the story of how this paper came to be.
100380	101032	12	B	0.86993	Sure.
101166	103300	13	B	0.99922	Thanks Daniel for organizing.
103460	106510	14	B	0.99996	This is really great and I'm very happy to be here.
107280	120540	15	B	0.97992	So my name is Lance de Costa, some of you may know me, I'm a PhD student both at Imperial College of London and UCL, mostly working on active inference and the free energy principle.
120980	123170	16	B	0.99568	Also some probabilistic machine learning.
123620	130550	17	B	0.96	And today we're going to be discussing geometric methods for sampling optimization, inference and adaptive agents.
132120	141064	18	B	0.9909	So how this paper came to be where I happen to know people in who do more like statistics, machine learning and so on.
141102	156668	19	B	0.9	And we've always been talking about the similarities and differences between what we do in two inference and what they do in sampling optimization, which are kind of the workhorses of statistics and machine learning.
156754	177392	20	B	0.91	And at some point we had a paper invitation for the hamburger of statistics and we decided to team up and make a review article to see how all these Fields that come from very different places scientifically, that have very different contributors as well.
177446	195124	21	B	1.0	And we wanted to know how they're interconnected and also how they can help each other because we realized for all this time that we're actually doing similar things, but the communities are very different, they're dot zero much talking to each other.
195242	200724	22	B	0.95	And I think the main barrier is difference in Lagrange, difference in jargon.
200772	221810	23	B	0.9983	So that was kind of an issue when we started and so I wanted to put that all as much as possible in a paper and see how these different approaches to statistics, whether it is sampling, optimization, inference or decision making, how they can benefit from each other and how they relate to each other.
223540	236630	24	B	0.99965	Long story short, we're going to delve into this into more detail, but long story short, if you want to ensemble from a distribution, you can also view that as an optimization problem.
237320	241376	25	B	0.96303	So optimization is kind of you can see it under the roots of sampling.
241408	246164	26	B	1.0	And you want to optimize sampling, make sampling as efficient as possible inference.
246212	251476	27	B	1.0	You can also see it as an optimization of beliefs, optimization of probability distinctions.
251508	253660	28	B	0.99681	That's also an optimization procedure.
255200	268120	29	B	0.99	And if you want to do decision making, which is what we do in active inference and reinforcement learning and many different places, well, through decision making, you actually need optimization and inference.
268280	273600	30	B	1.0	And if you want to do decision making efficiently, you often are going to need something as well.
273750	285590	31	B	0.9984	So decision making, an active inference is kind of like what comes at the very end and requires all of the sophisticated machinery of sampling coherence of optimization in order to work.
286200	293668	32	B	0.9856	So this was really the idea of the paper, how does optimization come about, how do you do efficient optimization?
293844	296664	33	B	1.0	And of course there's many different ways to do that.
296862	301864	34	B	0.98186	So we decided to focus on natural ways and we'll get to that.
301902	309340	35	B	0.99748	But one way to do optimization that is very natural is through techniques from geometry.
311520	313128	36	B	0.62	And so same with sampling.
313224	319490	37	B	0.99886	In sampling and inference, you have geometry that just comes up very naturally when you want to solve these problems.
319860	322770	38	B	0.82	And so decision making puts it all together.
323220	325600	39	B	0.78327	So that was really the idea of the paper.
325670	331504	40	B	0.99935	Can we put all these Fields together and have a connecting line which is based on geometry?
331552	333110	41	B	0.81	And so that's where we did.
335880	338512	42	A	0.99613	Awesome, great overview.
338656	343370	43	A	0.94326	So, so many places to jump in.
345740	349332	44	A	0.55	I barely even know where to sample from this distinctions.
349476	352216	45	A	0.93054	Let's just start with sampling, though.
352318	356860	46	A	0.97748	So what is sampling and how is it being used?
356930	358510	47	A	1.0	How has it been used?
359040	367740	48	A	0.99	And let's characterize sampling and then move on to these other Fields, right?
367810	375744	49	B	0.56894	Well, sampling, I mean, the idea of sampling is very simple and it's also a very difficult thing to do.
375862	377730	50	B	0.98877	So first explain the idea.
378200	383940	51	B	0.98	The idea is, let's say you have a Bayesian in one dimension, so that's very easy to picture.
384440	392712	52	B	0.90553	Sampling would be so, for example, the Gaussian I just mentioned describes the temperature in the room.
392846	401240	53	B	0.51574	So in my room, it's maybe currently 18 degrees Celsius, plus or minus some variants representing my uncertainty.
401660	413304	54	B	1.0	Now, sampling from the distribution would be saying it is maybe 19 degrees or maybe 17 degrees, drawing a lot of different temperatures.
413352	427804	55	B	0.99987	So that if you put them all together as a histogram with their frequency, for example, if you ask me what temperature it is in my room, I would say there's a higher chance that I say 18 degrees.
427932	445240	56	B	0.93278	So I would say more often if you ask me this question many times, and if you put my answers, if you collect all my answers in a histogram, for example, I'd say 18 degrees more often, so that barp around 18 degrees would be higher and the other temperatures would be slightly lower.
445310	449800	57	B	0.99	And if you aggregate that, you would get a Gaussian.
450140	451896	58	B	0.99364	Get the Gaussian we started with.
451998	454356	59	B	0.98156	So sampling, in other words.
454398	460620	60	B	0.99	And very simply, it's like drawing a data point or an observation from an underlying probability distribution.
460960	470444	61	B	0.73	You have a distribution, let's say a Bayesian that represents your most likely value of what the temperature in the room is and some uncertainty.
470572	476720	62	B	1.0	And you just want to draw one or several observations from that distribution.
477140	483670	63	B	0.96	And you want to draw these observations so as to preserve the overall statistics of the distribution that you started with.
484200	494308	64	B	0.99996	So that if you draw infinitely many data points or infinitely many observations from a distribution, then you can recover the original distribution.
494404	495930	65	B	0.96552	So that's what we want to do.
496300	500440	66	B	1.0	And so why is this useful in the first place?
500510	506860	67	B	0.99988	Well, it just happens all the time in the things that you want to computer integrals.
508080	509484	68	B	1.0	It just happens all the time.
509522	526512	69	B	0.72736	So, for example, if you have a Bayesian inference problem, you observe some data in the world, it could be medical data, it could really be anything, and you have a prior and a likelihood of observing that data.
526566	529376	70	B	0.58666	So these are two terms in, like, Bayes rule.
529568	531030	71	B	0.86114	Then you want to know.
532120	539312	72	B	0.9606	Then if you apply Bayes rule, you get a posterior belief about what caused that data that you observed.
539456	547768	73	B	0.97874	So, for example, a practical use case would be some medical data of a patient that's sick, and you want to know.
547934	553916	74	B	0.58952	So you get, for example, it's blood pressure, collection of symptoms and so on.
553938	562476	75	B	0.99787	So that's your data, and you want to know what kind of disease or what kind of function caused that data.
562658	573360	76	B	0.86487	So let's say typically you would have a model of, if I have this malfunction of this disease, I would have these symptoms.
573860	576748	77	B	0.8434	If I have this other disease, I would get these other symptoms.
576764	588950	78	B	1.0	And so Bazrul enables you to infer in the best way possible, the disease that caused the symptoms that you just saw.
589400	616284	79	B	0.86529	So this is like a common thing, something that people use every day in statistics all the time, and arguably everybody uses it every day because the thesis of the Franca principle, or the result that it provides, is that you can describe exteroception of humans as doing basic inference.
616412	620610	80	B	0.89762	So basic inference is a very ubiquitous thing.
621940	630070	81	B	1.0	You can use it to describe perception in humans, and you can also use it to do statistics and solve all kinds of problems.
630440	642090	82	B	0.85859	So, coming back to sampling, actually doing Bayesian inference, you have this problem of how do you compute the posterior distribution in the first place?
642460	654540	83	B	1.0	And it turns out that this distinctions is generally intractable because you have kind of a term in the denominator that's very hard to compute.
655600	661064	84	B	0.99	And so one ways to compute it is to use what's called Monte Carlo methods.
661192	663880	85	B	1.0	And Monte Carlo methods are based on sampling.
664040	678832	86	B	0.99373	So, long story short, in order to do Bayesian inference, you need to compute this, like, log likelihood term, which is a high dimension integral and one way to do it is by using Monte Carlo methods.
678976	681856	87	B	0.68	And Monte Carlo methods are based on sampling.
682048	688790	88	B	0.99706	So the sampling enables you to approximate the high dimensional integral they started with.
689480	709192	89	B	0.94614	So now many listeners, or also the readers of this work will wonder, well, one thing that people do in the free hundred principle and also kind of like the red on that of the 300 principle, is you're actually not going to compute this denominator.
709256	720320	90	B	0.7274	That's very expensive to the Bayesian inference, but you're going to approximate the posterior by minimizing free energy, which is called variational inference.
720660	727244	91	B	0.72537	That's another method of doing Brett and coherence and it has many advantages over sampling.
727292	731644	92	B	0.71115	Sampling also has advantages with respect to variational inference.
731772	733330	93	B	0.7995	We can get into that.
735940	743320	94	B	0.74795	So if that's what you're thinking, well, this is not necessarily interesting because we can do variational coherence.
743900	749320	95	B	0.80945	There's so many other places where we need sampling in statistics.
750540	757310	96	B	0.98	I would say basically anytime you need to compute an integral and integral are all over the place.
758240	765710	97	B	0.99805	But I think the Bayesian inference implication is a nice one because it just speaks to all of us.
770020	774604	98	B	0.64392	So how does actually sampling getting to the picture of integration?
774652	777600	99	B	0.79705	Let's say we have an integral that we need to compute.
778340	783444	100	B	0.96501	An integral is basically, let's say you have a state space.
783562	787024	101	B	0.94465	So to be very simple, let's say the state space is my desk.
787152	792596	102	B	0.67	It would be like a planner surface and we have a function that's defined on the desk.
792628	796730	103	B	0.50103	So this would be like a landscape over the desk, right?
797180	801690	104	B	0.99776	Like a 2D surface landscape that could be rocked or whatever.
802300	808300	105	B	1.0	The integral of this function is going to be the volume between the landscape and the desk.
811200	816190	106	B	0.95688	This is kind of like the geometric picture of what an integral is.
818500	832340	107	B	0.99335	This turns out to be very hard to compute in practice because let's say you have a very rough surface and it's not clear how to compute all this volume, which is very irregular.
833640	841880	108	B	0.99835	What sampling does is it's going to pick a few locations on the desk, as many as you wish.
842460	844024	109	B	0.99844	So let's start by one.
844142	860780	110	B	1.0	And it's going to compute the height under the function and then pick another location and compute that height as well, since sampling is going to pick locations on the desk at random.
861600	870352	111	B	1.0	And from there it's going to build a histogram that's going to approximate the surface that you started with.
870486	878530	112	B	0.79	And so histogram is very simple because you get a whole bunch of skyscrapers if you wish to approximate the surface that we started with.
878920	886500	113	B	1.0	And computing the volume of skyscrapers is very simple because they're like squares.
887720	891184	114	B	0.95097	It's like depth times height times width.
891312	894250	115	B	0.84281	It's very simple to compute the volume of those things.
895660	900548	116	B	0.96744	So this is kind of the idea we want to computer the volume under a rock surface.
900724	910056	117	B	1.0	And to do that, we're going to pick arbitrary locations on the desk, on the page space and build it.
910078	916412	118	B	0.63	And at those locations build like skyscrapers at the height of the surface.
916556	923250	119	B	0.72467	So you get like a skyline, a histogram that approximates your original function.
923700	929836	120	B	1.0	And it's very simple to know the volume of the histogram, the volume of the skyline.
930028	935430	121	B	0.69048	So from then you get an estimate of the integral that you started with.
936360	939152	122	B	0.95842	So that's kind of like the geometric picture.
939296	947668	123	B	1.0	The beauty of this is that it works in the arbitrary dimensions and it doesn't suffer from the curse of dimension.
947764	952168	124	B	0.96241	So this is why it's so powerful, if you want to know.
952254	965180	125	B	0.97545	So there's a formula that tells you how well the volume of your skyline will approximate the volume of the volume that you initially wanted to compute the integral.
965940	981476	126	B	0.95664	So this difference between the integral and your estimate basically decreases as a function of the number of skyscrapers that you put in.
981578	984550	127	B	1.0	The more skyscrapers, the more precise you get.
987160	998600	128	B	0.86	And this bound on the error that you have on your integral that you started with, it doesn't depend on the dimension.
998940	1004552	129	B	0.79761	So as the higher dimension on your state space.
1004606	1014830	130	B	0.94576	So let's say let's imagine I had a high dimension desk, which is very hard to imagine, and I had a function on top of it, and I wanted to compute the area under the function.
1016500	1027212	131	B	0.99242	If my desk was very high dimensional, the higher the dimension, the harder it would be to solve this problem numerically.
1027356	1036500	132	B	0.99779	But with sampling, actually, the degree of accuracy that you get through sampling would be agnostic to the dimension.
1037320	1045320	133	B	0.99988	This is why it's so powerful and just used all the time in physics and machine learning.
1045470	1066412	134	B	0.99984	When you imagine practical problems that you have in machine learning, going back to medical data, you could get a whole bunch of data about a patient, which could be heart rate over time, blood oxygen levels, blood glucose levels, body temperature and whatnot.
1066476	1070144	135	B	0.9557	There's so many data modalities that you would have.
1070182	1073760	136	B	0.99832	So your state space would have many, many dimensions.
1075540	1088500	137	B	1.0	And also the state space of the different diseases that could have given rise to all these symptoms would be also very high.
1088570	1096440	138	B	0.99895	So it could be that the patient has fever, or it could be that it has flu.
1096780	1097844	139	B	0.98903	Fever is a symptom.
1097892	1099732	140	B	0.98	It could be that the disease is flu.
1099796	1102180	141	B	1.0	It could be that the disease is tuberculosis.
1102260	1113020	142	B	1.0	It could be that the disease there's so many different conditions or problems that one might want to examine.
1114080	1125950	143	B	0.98855	So in this particular case of medical data, and this is just like a very simple use case, I would say you have a very high dimensional state space.
1126320	1131616	144	B	0.6	And to debate an inference, you have a very high dimensional integral to estimate.
1131808	1140932	145	B	0.61	And so one way to do that efficiently, or maybe efficiently, is disingenuous because it's just very hard.
1141066	1145256	146	B	0.5	I mean, it's just very computationally intensive to do these things.
1145438	1163230	147	B	0.99771	But one way to do this, that state of the art, is through sampling just by drawing different points at random on this safe space and approximating the functional landscape and approximating the integral like that.
1164720	1169330	148	B	0.71478	So, yeah, this is really why this is so important.
1172340	1173090	149	A	0.98743	Awesome.
1173540	1188688	150	A	0.97816	So we've made a sample where does these topics of accepting or rejecting a move amongst examples or discretizing, how do we pick that next sample?
1188784	1192120	151	A	1.0	And then what does it mean to accept or reject that sample?
1193100	1194120	152	B	0.99908	That's a great question.
1194190	1197320	153	B	0.99967	This is basically like the heart of sampling.
1198540	1205848	154	B	0.90616	If you knew how to do that in full generality, sampling would be sold, which it isn't.
1205944	1210030	155	B	0.6266	So I'm going to tell you a bit like where things stand.
1214960	1220524	156	B	1.0	To do sampling, you need to pick places at random and you need to pick places at random.
1220572	1223596	157	B	0.9996	So that's to preserve probability distribution.
1223788	1234100	158	B	0.99741	So going back to the example of the desk, you want to pick in that case, you want to pick examples uniformly on the desk.
1235560	1246292	159	B	0.99997	If we come back to the previous example of like a Gaussian distribution, you want to pick samples that preserve the Gaussian distribution.
1246356	1252760	160	B	0.99905	So you ant to preserve to take a lot of samples around the mean very few samples on the tails.
1253840	1271650	161	B	0.93514	So it turns out that for those very simple distributions like uniform distribution, Galveston distribution, there are ways to sample that do not require what this paper is talking about.
1272580	1280630	162	B	0.99846	So there are ways to sample exactly that do not lead to any kind of error and that are very fast.
1281480	1291444	163	B	0.98763	So for Gaussians and uniform and a bunch of other distributions from the exponential family, you have ways to sample.
1291492	1292090	164	B	0.99994	Exactly.
1293020	1300904	165	B	0.88664	So the problem is these distinctions, they're not there all the time.
1301022	1309148	166	B	0.98	And it turns out that very often you don't have these kind of very simple distributions, otherwise we would be done.
1309314	1317600	167	B	0.86324	So one simple example coming back to Bayesian inference is the idea of Jeffrey's Prior.
1318260	1331270	168	B	0.6922	So Jeffrey's Prior is the idea that you should take the highest entropy prior that's consistent with what you know about the system.
1335160	1337856	169	B	0.98173	Why entropy maximizing?
1337888	1343880	170	B	0.99558	So we know that entropy is a measure of the uncertainty present in the distribution.
1344860	1362808	171	B	0.99	And so what Jeffrey's Prior says is that you should take a prior belief about a system, the belief that is maximally uncertain, uncertain, but that is also compatible with your knowledge.
1362984	1369516	172	B	0.99746	So out of all priors that are compatible with what you know about the system, you would take the one that's most uncertain.
1369708	1371970	173	B	0.53	And this seems pretty intuitive, right?
1372740	1384820	174	B	0.996	So when you take Jeffrey's Friar, what you get is often a distribution that's in the form of what people call a Gibbs measure.
1385560	1394250	175	B	0.95475	So Gibbs measures or Gibbs distributions there of the form exponential minus V.
1395660	1401864	176	B	0.99926	So they're like proportional to exponential minus V, and V could be an arbitrary function.
1402062	1414008	177	B	0.85033	So just as an example, if V were like a parabola, a quadratic function, then the associated Gibbs distribution is a Gaussian.
1414184	1416652	178	B	0.99083	So we all know what Gaussians are, right?
1416786	1419824	179	B	0.99997	But v could be pretty much anything.
1419942	1431412	180	B	0.99998	If you take V to be like X to the power of four minus X to the power of two, then V is kind of like a camel upside down.
1431466	1433380	181	B	0.97546	So, you know, it has like two humps.
1433800	1442040	182	B	0.97	And therefore the exponential minus V distribution, the Gibbs measure, would be like a gaussian, but with two humps.
1443980	1452970	183	B	0.9357	So these gibs measures, they're just there like everywhere and they come up all the time.
1453840	1458140	184	B	0.89092	So one other example is in statistical mechanics.
1458720	1471884	185	B	0.96549	So let's say you have a system of molecules that just interact with each other, and this system of molecules, they could also be subject to thermal fluctuations.
1471932	1479830	186	B	0.6	And whatnot this system is going to relax to what people call a steady state, often a non equilibrium steady state.
1480280	1491160	187	B	0.9926	These steady states, which is kind of like the end configuration of the system, is in general a gives measure.
1495180	1504600	188	B	1.0	Now, you can see that in statistical mechanics and also in statistics, gives measures come up also in basic inference.
1505020	1509980	189	B	0.98	And it's very hard in general to sample from a gives measure.
1510880	1515920	190	B	0.92899	There's ways to do it and we're going to come back to it, but we need to solve this problem.
1515990	1517890	191	B	0.58305	It's a very important problem.
1518420	1528800	192	B	0.82	And so the most common way to solve from this distribution is what's called Markov Chain Monte Carlo.
1529400	1532064	193	B	1.0	The idea is to run a Markov chain.
1532112	1533988	194	B	0.97967	So to run like a stochastic process.
1534074	1552120	195	B	1.0	And you could think of it as stimulating molecules that interact with each other with some thermal fluctuations and that are going to relax to this gift distribution, this steady state that you originally specified.
1554060	1560604	196	B	0.95038	So coming back to the original examples, the steady state could be like a gaussian they want to sample from.
1560802	1563000	197	B	0.78156	Coming back to the example of the desk.
1563160	1567180	198	B	1.0	It could be the uniform distribution on the desk that we want to sample from.
1567330	1569260	199	B	1.0	It could also be a gift measure.
1571300	1589860	200	B	0.9616	So once we know the distribution that we want to examples from, we're going to design a stochastic process, some kind of random motion whose distribution is going to converge to the target distribution.
1590380	1600200	201	B	0.80345	So as you run the process, think of it as simulating those molecules that interact and that converge to your target.
1602540	1605176	202	B	0.95609	You're just going to simulate that.
1605278	1617580	203	B	1.0	And once your system has converged to its steady state, which is the target distribution, then each movement on the steady state density is going to give you a new sample.
1620260	1629084	204	B	0.99837	Just for intuition, let's come back to the desk and say that we have this uniform measure, uniform distribution on the desk that we want to sample from.
1629222	1646410	205	B	0.99524	So we're going to design like a dynamic that moves randomly around and that's going to concept to the uniform distribution, which means that if you run it long enough, you're just going to sample everywhere on the desk and it's going to examples each point equally often.
1647100	1661150	206	B	0.9	And so you're just going to run this dynamic for a very long time and then you're going to collect the samples, collect the examples and use them to approximate the integral that you started with.
1662640	1665230	207	B	0.99653	So this is how sampling is done.
1665860	1674960	208	B	0.91	Now, there's two basically one big question, one big outstanding question is how do you choose this dynamics?
1675620	1683700	209	B	1.0	How do you choose this random motion to converge to your target distinctions to sample from the distribution?
1684120	1696650	210	B	0.99065	So given a target distribution there's like a closed form formulas for all the possible choices of processes that you can use to sample from them.
1697100	1702810	211	B	0.95809	That's a closed problem and that's also explained in the paper.
1703520	1714270	212	B	0.7065	But then out of all of these processes you would like to know which one is going to sample the most efficiently and you also want to know.
1715380	1722832	213	B	0.63	And second of all, a lot of the analysis of stochastic processes is done in continuous time.
1722966	1730000	214	B	0.9973	So you know that if you implement this process it's going to converge at this rate to the target distinctions.
1730080	1733030	215	B	1.0	And so the faster it converges, the better.
1733960	1744856	216	B	0.66136	But it could be that when you simulate this on a computer, in a computer you can only have like discrete time steps or like discrete operations in discrete time.
1744878	1748730	217	B	0.52563	So you cannot really simulate this dynamic in continuous time.
1749100	1752900	218	B	0.97599	So every time you simulate it, it's going to introduce some error.
1753060	1762504	219	B	1.0	Now it could be that the thing that you simulate on your computer actually goes a lot slower than the original process that you wanted to simulate.
1762632	1765672	220	B	0.86462	So there's these two questions.
1765826	1772000	221	B	1.0	How do you take a process that samples very efficiently, converges very fast to your target?
1772900	1787350	222	B	0.99681	But in addition to that, how do you stimulate the process accurately on your computer so that's to retain all the important characteristics of that process?
1789720	1803000	223	B	0.98231	So there's many different criteria and this is what we discussed essentially one method that state of the art or among the state of the art is called Hamiltonian Monte Carlo.
1803340	1815140	224	B	1.0	Now, Hamiltonian Monte Carlo, without getting Hinton too much detail, it has two advantages.
1815320	1822364	225	B	0.62	The first advantage is that it can be analyzed using mathematical methods.
1822412	1826780	226	B	0.99787	So you can get guarantees based on your target distribution.
1826860	1833430	227	B	1.0	You can get guarantees of how fast you're going to converge, how accurate your samples are, gain to be and so on.
1835160	1839456	228	B	1.0	It has also many other desirable properties.
1839648	1846840	229	B	0.99455	So one of them which we discuss a lot in the paper is the importance of being irreversible.
1847340	1863800	230	B	0.95299	So irreversible means being irreversible means that if you run the process forward in time let's say you run the process forward in time for like 10 seconds.
1863960	1866044	231	B	0.96479	Let's say you run the process backward in time.
1866082	1870540	232	B	0.93788	So you kind of run the process backward.
1870700	1877196	233	B	1.0	You just take a movie and play it backward and look at how the process behaves.
1877388	1897400	234	B	1.0	Now, a process is reversible if the forward and the Ballard movies are statistically indistinguishable and a process is irreversible if you run time as time goes forward.
1897470	1904140	235	B	0.99805	If you run time forward, it would look statistically different from if you run the process backward.
1910160	1914732	236	B	0.99	I think you can kind of picture what irreversibility is.
1914866	1925276	237	B	0.51	Now, irreversibility is crucial to do efficient sampling because if you're reversible, it means that you're going to backtrack.
1925308	1941140	238	B	0.99497	Very often when you're sampling positions on the decks, on the desk, say you're going to start somewhere, go somewhere else, and then there's a nontrivial chance, significant chance that you're going to come back to where you started.
1941290	1946360	239	B	0.94461	So it's going to be very slow to explore all the desk and build skyscrapers everywhere.
1948540	1951428	240	B	0.58478	So then sampling turns out to be very slow.
1951604	1958428	241	B	0.99	Now, when you're irreversible, there's way less chances that you just come back to where you started.
1958514	1966510	242	B	0.99348	So you're going to explore the desk a lot faster because you're not allowed to come back to the same place so much.
1967360	1972080	243	B	0.58	And so you're going to assemble faster and it's going to be much more efficient.
1973060	1986064	244	B	0.71268	So it's actually, by going from a reversible process for sampling to an irreversible process, you can gain orders and orders of magnitude, efficiency.
1986112	1988230	245	B	0.99858	It's actually pretty crazy.
1990600	1995000	246	B	0.45	The Hamiltonian monte Carlo has guarantees.
1995980	1997480	247	B	0.84454	Is irreversible.
1998060	2000520	248	B	1.0	It has other desirable properties.
2001660	2022930	249	B	1.0	And the main one is that you can actually stimulate it on a computer in a way that the discretized version, which you actually simulate on the computer is going to be very close to the true Hamilton in Monte Carlo that you get by writing the equation down.
2024420	2032230	250	B	0.83849	So this is like why it's so big and why people use it.
2036520	2039030	251	B	0.5	Yeah, I think this is pretty much it.
2039800	2053690	252	B	0.61498	Hamiltonia Monte Carlo has an accept rejects step, which is coming back to when you simulate a process on your computer.
2054140	2058632	253	B	0.9999	Every time you discreditize a process, you introduce some error.
2058776	2072080	254	B	1.0	Now you want to make that discretization error as small as possible so that the process that you actually simulate on your computer is very close to what you want to simulate.
2073140	2092600	255	B	0.9578	So Hamiltonia Monte Carlo has a very important step called Metropolis Hastings, which is going to say, should I keep the sample or should I drop the sample and go to the next examples?
2093020	2101560	256	B	0.66	And by using Metropolis Hastings, it's a very clever but also simple procedure.
2102060	2112664	257	B	0.9999	By using Metropolis Hastings, you guarantee that the distribution that you're going to sample is exactly the same as the distribution that you want to sample.
2112792	2122800	258	B	0.98539	So just to put in other, maybe simpler terms, so you have this process that you want to simulate.
2123380	2128800	259	B	0.99624	This process is going to sample your target distribution.
2129140	2138976	260	B	1.0	Now, it could be that when you discrete the process, because the discrete process is different, you're going to sample from a slightly different distinctions.
2139168	2150456	261	B	1.0	Now, by using Metropolis Hastings, you're going to guarantee that the distribution that you sample from in practice is exactly the same as what you really want to sample from.
2150638	2160776	262	B	0.85	And so that's very important because it just tells you that asymptotically as the number of examples just growth to infinity.
2160968	2166690	263	B	0.71567	You're going to recover exactly what you wanted to recover, which is the adamant shrine to grow.
2172820	2173570	264	A	0.99971	Awesome.
2173940	2192308	265	A	1.0	I remember the Metropolis Hastings and the Monte Carlo Markov chains in phylogenetics where you might have many, many species and many, many locations in the genome that you're doing phylogenetic inference over and you just sample, sample, sample.
2192404	2199800	266	A	0.99	And so all these techniques help accelerate what's possible with any given computational hardware.
2201500	2206796	267	A	0.96707	So how would you bring this towards adaptive agents?
2206978	2220290	268	A	0.99987	Are we thinking about this sampling process as being guided by an adaptive agent and or are we sampling distributions about an adaptive agent?
2221140	2222976	269	B	0.89448	That's a great question.
2223078	2228610	270	B	1.0	And I think it sort of brings us to the conclusion of this work.
2231080	2247780	271	B	0.99967	In order to have adaptive agents and to scale adaptive agents so that they're able to solve complex problems and you're able to implement them with finite computational resources, one useful tool is sampling.
2247940	2253900	272	B	0.88	And we'll see many different places where sampling can be used and has been used in active inference.
2257280	2278240	273	B	0.93	And that's basically what you said sampling is at the heart of sampling is choosing data points intelligently so as to approximate your integral or whatever it is that you want to do with those samples.
2278680	2286484	274	B	0.86309	So you could think of active agents as solving the sampling problem.
2286682	2290150	275	B	1.0	You could think of an active inference agent.
2290920	2293808	276	B	0.98549	Let's say that you have a problem of sampling.
2293984	2302090	277	B	1.0	You could think of an agent, an active inference agent that's going to choose, oh, I'm going to sample here, and I'm going to sample here, and I'm going to sample here.
2302460	2308060	278	B	0.91263	So active inference agent in the proper sense of the word.
2308130	2310030	279	B	0.99997	They do sampling all the time.
2311040	2329830	280	B	1.0	The only I would say, slight difference with what we've just discussed is that active inference agents, they sample observations so that they comply with their preferences and they observations that also bring them new information about the world.
2330520	2339430	281	B	0.91603	So the objective that active inference agents use to select new sample is the expected free energy.
2340380	2356670	282	B	0.81258	Typically we select actions or policies that have the lowest expected free energy, which means that the resulting observations will be close to your preferences or goal and also bring you new information about the world.
2358560	2371040	283	B	0.87743	But you could also conceivably think of an agent as selecting actions so that the resulting samples or observations accurately sample a distribution.
2373460	2382256	284	B	0.97871	So how could sampling be used in the context active coherence lab and adaptive agents?
2382438	2395124	285	B	0.64	And so this is actually a great slide, if you look at the upper panel is what I just discussed.
2395172	2410276	286	B	0.86549	So in the first equation that's up there, you see that the sequence of actions that you chaos in active inference is the one that minimizes this minus lot T, this probes distribution over action sequences.
2410468	2413790	287	B	0.98723	So this minus lot T is actually the expected free energy.
2414640	2421916	288	B	0.99858	So we choose action sequences that mean by the expected free energy, as you see on the equation just below the expected.
2421948	2425436	289	B	1.0	Free energy is a sum of risk and ambiguity.
2425628	2432390	290	B	0.99914	So you're going to choose action sequences that minimize risk, but that also minimize ambiguity about the world.
2432840	2438948	291	B	0.94	And in other words, as shown below, you're going to choose action sequences that maximize extrinsic value.
2439034	2443320	292	B	0.98526	So you could think of that as an expected reward and intrinsic value.
2443390	2449850	293	B	0.99856	So that would be expected information gain value that's intrinsic because you're getting information.
2452300	2463260	294	B	0.85896	So really the key here is that this expected free energy, as its name suggests, it's given by an expectation.
2463840	2468430	295	B	0.99765	So you see this minus lock p on the left equals expectation of something.
2468800	2493960	296	B	1.0	Now, when you have a very big generative model, a very big world model, which is going to be the case all the time, when you have, like, a real world application or a complex application, because the world around us is so high dimensional, so complex, well, the expected energy then is going to be a high dimensional expectation.
2494300	2495668	297	B	0.99805	What is an expectation?
2495764	2499480	298	B	0.99894	It's an integral with respect to probability distribution.
2500140	2513100	299	B	0.84918	So the point I want to get to is that to do decision making, you need to evaluate the expected free energy on different action sequences.
2513600	2519200	300	B	0.81	And this expected free energy is an expectation with respect to probability distribution.
2520020	2527600	301	B	0.99955	It's a very high dimensional integration problem, very high dimension integration expectation.
2528180	2531752	302	B	0.99	And so how do you approximate that while you use template?
2531916	2541750	303	B	0.76	And I think the first work that actually used that in the context of active inference is the work by Fuentes et al.
2542680	2544150	304	B	0.96	2020.
2546220	2563400	305	B	0.79	I think it's called scaling active inference with Monte Carlo methods.
2563560	2565230	306	B	0.94456	So this is what they did.
2566160	2568620	307	B	0.96	I think it was published in New Ritz.
2569280	2571576	308	B	0.46414	It's a very big machine learning conference.
2571768	2590390	309	B	0.66	And their observation was, okay, well, we have this active inference method which theoretically is very powerful, I mean, in the sense that you have this expected free energy objectives, that puts a lot of known constructs together.
2591480	2597444	310	B	0.99612	So the expected free energy, as shown in figure three here, puts together this risk.
2597492	2608110	311	B	0.96752	So this is the divergence between your predicted distribution of where you're going to be in the world and your target distribution of where you want to be in the world.
2608480	2618910	312	B	0.99	And it has this ambiguity term as well, which has some nice links with phenomena in psychology, like the streetlight affect.
2619600	2623580	313	B	0.9966	So when you minimize we introduce actions that minimize ambiguity.
2623660	2630470	314	B	1.0	You can recover phenomenology that's known in psychology, like the streetlight affect or the drunkard search.
2631880	2639008	315	B	0.88454	Risk is also an objective that's used in engineering, in controller inference.
2639024	2641148	316	B	0.7895	So you can see it on the left panel.
2641344	2647476	317	B	0.95452	So there are a few methods for doing adaptive agents that use this KL term.
2647508	2654350	318	B	1.0	One would be controlled coherence, which is also known as maximum entropy reinforcement learning and KL control.
2655040	2678210	319	B	1.0	And also, I think, although it needs to be investigated further, it hasn't been formally, I think, so far, but with the KL turn there, you can recover predictions that are made by prospect theory which is a big theory from the end of the 20th century about decision making in human agents.
2679240	2691724	320	B	0.61105	So this is, I think, worthwhile to look into further but I think there's some very clear links between that theory and the scale term in the expected franca.
2691872	2697780	321	B	0.99762	Anyway, so you have this expected free energy objective that's very powerful.
2697860	2700728	322	B	0.49628	Chaos puts a lot of things together.
2700814	2703290	323	B	0.96493	So I think theoretically it's very nice.
2704320	2718352	324	B	0.50682	In the last line of the upper panel you can also see that you can view expected PNG as weighing expected reward and expected information gain, extrinsic value, intrinsic value and extrinsic value.
2718406	2726400	325	B	0.84303	It's used in expected utility theory, basic decision theory, RL, optimal control.
2726470	2736064	326	B	0.98831	So these are all like equivalent names for maximizing this extrinsic value and intrinsic value, expected information gain.
2736192	2746452	327	B	0.99723	It's also used in different places and for example in Bayesian experimental design which is in statistics.
2746516	2760648	328	B	1.0	It was a seminal paper by Lindley in 1956 and Lindley basically said, okay, well, suppose that you you can do two experiments or many different experiments.
2760744	2762524	329	B	0.99	How should you choose between them?
2762562	2765292	330	B	0.99845	What's the best experiment that you should do?
2765426	2773810	331	B	0.6	And the answer that he came up with is you should do the experiment that gives you the most information about the world.
2775460	2780224	332	B	0.93664	So you have to keep in mind that in that paper there was no extrinsic goal.
2780272	2787504	333	B	1.0	It was not like we're going to do an experiment to solve this problem or achieve these results.
2787552	2791816	334	B	0.99	It was only doing an experiment for the sake of it.
2791838	2798212	335	B	1.0	And so he came to the conclusion that the best experiment was the one that maximized the expected information gain.
2798356	2818412	336	B	0.89	Now suppose that you have a goal in mind target, target result or whatnot then you would weigh reasonably and one thing you could do is weigh the expected information gain with your expected reward or expected utility or expected how close you're going to get to your goal.
2818556	2821440	337	B	0.44	And so this is exactly what the expected free energy does.
2821510	2825440	338	B	1.0	It puts these two things on the same footing.
2828580	2848810	339	B	0.89993	So you have this expected free energy which is very nice and which is what active inference brings to the table and what Fuentes et al did in their paper is, okay, well, we have this cool method, active inference, let's use it to solve nontrivial machine learning, slash reinforcement learning problems.
2850300	2867380	340	B	0.99	And so when you actually want to solve those problems you have some high dimensional generative models, high dimensional state spaces that just come up and you need time plane to evaluate to approximate the expected principle.
2867400	2875330	341	B	0.80791	So this is what they did and they also put together a lot of other cool tricks based on neural networks and so on.
2876420	2879010	342	B	0.99108	So a very cool paper, totally recommend.
2879400	2894724	343	B	0.61241	But I think the point it suggests, and this is also the point that we make, although in a very different way in this paper is that to actually do decision making efficiently in practice efficiently.
2894772	2897720	344	B	0.99817	But also you want to do take good decisions.
2898220	2899880	345	B	1.0	You actually need sampling.
2905680	2908028	346	B	0.86	And this is not a new point, by the way.
2908194	2912350	347	B	0.63	I think many people would agree with this.
2913040	2924364	348	B	0.99501	But I think the active inference approach is nice because you have this explicit characterization of expected PNG as an expectation of something.
2924482	2931430	349	B	0.85501	So it becomes pretty clear that you want to do sampling because this is what sampling is made for.
2933080	2941060	350	B	0.94203	So I initially introduction sampling as you know, wanting to approximate integral, but expectations are integral.
2942040	2944632	351	B	0.96314	So yes, sampling is really what you want.
2944686	2950010	352	B	0.98	It just comes up like extremely directly when you want to do these kind of things.
2951900	2952600	353	B	0.94689	Awesome.
2952750	2953256	354	A	0.99926	Thanks.
2953358	2968160	355	A	0.98	And one sensory example of information driven sampling is the eye circade the movement of the eyes, which also has been modeled in many works from the active inference perspective.
2968500	2996084	356	A	1.0	And it's even below our level of conscious awareness with the eyes darting around to reduce their uncertainty such that our visual generative model can have high resolution and color throughout the visual field, even though that doesn't reflect the anatomy of the retina, which has a blind spot and differential resolution and color attention in the periphery.
2996212	3008460	357	A	0.9908	So I kind of see that as slam dunk evidence that our visual experience and by extension other sensory modalities are coming from the generative model.
3008610	3022690	358	A	0.99943	They're not just received and processed in an inward bound fashion and that adaptive sampling is vital to maintain the coherence of that generative model.
3024420	3025170	359	B	0.99013	Absolutely.
3026260	3031856	360	B	0.96	You want to do adaptive sampling in order to preserve the structure.
3031888	3039300	361	B	0.87	I mean, you have this beautiful structure in the expected free energy of expected information gain.
3039640	3046356	362	B	0.99597	So let's say we're just like wanting we're just like looking at stuff, so we don't have any direct goal.
3046548	3064750	363	B	0.88	The expected energy reduces to expect information gain or approximates that and you just want to sample sampling is the key to approximate this expected information gain term while preserving the statistics of the generative model.
3065360	3070800	364	B	0.63856	So it's very central to scaling active coherence.
3071460	3082500	365	B	1.0	I wouldn't say it's central, active inference lab proper just the theory because in the theory you get these expectations and you know how to select actions.
3082840	3093560	366	B	0.99812	But anytime you want to scale active coherence, deploy active inference to solve a problem you want to, sampling is going to come into play inevitably.
3099900	3103160	367	B	0.94955	There's other ways to scale active inference.
3104060	3108472	368	B	1.0	One other way could be what's called amortizing.
3108616	3109368	369	B	0.99733	Amortization.
3109464	3110856	370	B	0.70759	So with deep neural networks.
3110888	3119730	371	B	0.98047	So one thing you could do is train a neural network to predict the expected free energy based on your genital model and near sensory data.
3120820	3127632	372	B	0.99982	But this amortization procedure, as does any training of neural networks, it's going to require a lot of data.
3127686	3128968	373	B	0.58073	So it's going to be slow.
3129084	3131590	374	B	0.90847	It's not something that you can deploy right away.
3132440	3146852	375	B	0.97103	So this is very nice way of I mean, amortization is a very nice way of doing things, but it's not something that you can use like the first time you active inference lab to solve a new task.
3146996	3171368	376	B	0.65679	So a nice way of scaling active coherence and it's not the only way, or it's not going to solve all the problems, but it would be to use sampling to approximate the expected free energy to make decisions in real time as you accumulate data, then you can train a neural network to predict the expected free energy.
3171554	3174272	377	B	0.99	I think this is exactly what Fuentes et al.
3174326	3176192	378	B	0.99901	Did in their paper, by the way.
3176326	3178850	379	B	0.88159	So if you're interested, make sure to check it out.
3182020	3183170	380	A	0.99911	Figure three.
3183620	3204170	381	A	0.96929	It's something that Vaughn can just look ant for so long, because the imperative, what is being utilized in action selection by an active inference agent is something like a generalization of all the words that we see written lower.
3204800	3214988	382	A	0.97	And unless we knew about that generalization or that unified imperative, it seems like these are quite literally orthogonal or disparate from each other.
3215074	3226556	383	A	0.92	I mean, what could be more different than maximum information, gain oriented strategies and maximum reward driven strategies?
3226748	3235716	384	A	1.0	And sometimes to get both those flavors in the same model, people might try to coerce one into the other.
3235818	3247784	385	A	0.99997	Usually we see that in terms of a novelty bonus or an exploratory impulse bolted on to a reward or a pragmatic value driven agent.
3247982	3257944	386	A	0.99	And so once you start modifying the models and just adding in these arbitrarily constructed components, you may get adaptive behavior.
3258072	3263020	387	A	0.99663	It's never been a claim in the active space that other models don't have efficacy.
3263760	3282470	388	A	0.99999	Rather that by thinking about all of these special cases as being situationally arising from a more general imperative with expected free energy for future oriented policy selection, we might gain the ability to do what?
3284440	3306780	389	A	0.52493	What will we gain conceptually or in practice by taking situations where today people are using maybe just one of these terms as an imperative for what their models are doing and what do we gain or what might we expect from potentially generalizing those imperatives?
3308960	3312350	390	B	0.962	That's a brilliant and very difficult question.
3313360	3344100	391	B	0.99991	It's also a question that I would say everybody active coherence lab gets asked at some point because, let's say people doing reinforcement learning, they use many different objectives that would be tailored to solving one particular task or a different task, and they would have these ad hoc novelty bonuses that would work very well in certain tasks.
3346300	3347960	392	B	0.92256	It's a different approach.
3350540	3357500	393	B	0.82985	This approach that I was just mentioning with reinforcement learning tests to be it's much more bottom up.
3357650	3363996	394	B	0.54115	It's like we want to have an agent that solves this problem, makes this work, and so on.
3364018	3370210	395	B	0.98772	So we're going to start building and test it until it works.
3371460	3374530	396	B	0.64593	Active inference lab approach is top down.
3375380	3381776	397	B	0.91717	So it starts from the furniture principle and it's very theoretical.
3381888	3395480	398	B	0.94	And the approach is like, okay, well, let's model how an agent interacts with its environment, the force generic kind of agent, and let's impose constraints that an agent has to satisfy.
3395900	3406120	399	B	0.92548	So the basic constraint that we have in this paper is that you have three sets of variables.
3406280	3412444	400	B	0.65	You have variables in the external world that you cannot directly have access to.
3412642	3416830	401	B	0.98346	So this would be maybe the temperature in the room.
3418100	3422400	402	B	0.99108	Then you have sensory variables or observations.
3423140	3433732	403	B	0.99606	So by the way, the environmental variables that are denoted as S, the sensory observations, are the observatory or the observations that are denoted as O.
3433866	3445368	404	B	0.79584	So this is what belongs to the world and you have direct access to this would be like your sensations, what you see and so on.
3445534	3458300	405	B	0.99	And then you have the action denoted in the paper by A, which is basically what you can do, and the different things, the different options that you can choose from at any point in time.
3458450	3464168	406	B	0.82	And this is what will enable you to influence the world S and influence your observations.
3464264	3465004	407	B	0.58	O.
3465202	3499640	408	B	0.97333	So we start from a very generic model of an agent and how it interacts with the world, these three sets of variables that evolve in time and just by adding a few other, I would say, constraints that agents satisfy, or intelligent agents hopefully satisfy, that these agents can be described as taking decisions that minimize expected free energy.
3499790	3505740	409	B	0.99877	So the expected free energy just comes up from this general theory of how agents behave.
3507680	3514700	410	B	0.67349	So to answer your question, what does this actually bring in practice?
3515040	3522720	411	B	0.99749	Well, first of all, there's many methods out there that are similar to active inference.
3523540	3526850	412	B	1.0	In some cases, they're almost the same.
3528100	3535248	413	B	0.89	The first difference, which is not necessarily an advantage or a disadvantage, but it's the approach.
3535424	3539940	414	B	0.99975	These are the methods that you see out there that come very close to active inference.
3540600	3553770	415	B	0.96054	They have been designed from a bottom up approach of like, we want to model this system or we want to solve this task, so we're going to add all the components that are needed until our agent does what we want it to do.
3554320	3560892	416	B	0.97	And it turns out that sometimes all these ingredients turn out to be exactly what we get.
3560946	3563150	417	B	0.99793	Active Inference lab or very similar.
3565200	3570480	418	B	0.99706	Other times, maybe the task would be simpler and you would get less ingredients.
3571060	3577600	419	B	0.67	The disadvantage of that, I would say, is that it can turn out to be very messy.
3577940	3592760	420	B	0.99638	So you get a new task or a new thing to model, maybe a new paradigm that some subject would do, and you end up using a very different model, a very different objective to describe the behavior at hand.
3592910	3601050	421	B	0.90835	So it's harder to have like a unified perspective of what's going on.
3601660	3609244	422	B	0.99558	Active Inference I would say the key contribution is that you get this unified perspective from it.
3609282	3618464	423	B	1.0	You get this objective that puts together a lot of other constructs that we know and love and puts them together.
3618582	3630560	424	B	0.51	And you know that with this objective you can do a lot of things, which is, for example, decision making that weighs exploration and exploration.
3631000	3639972	425	B	0.64	Now, there are some challenges with active inference and anybody using active inference would be familiar with them.
3640106	3648824	426	B	1.0	The first one is the scaling up because you have this approach, active inference, which is so complex in a way.
3648942	3652250	427	B	0.58	I mean, you need to have a generative model of the world.
3652700	3657100	428	B	1.0	You need to have to be able to compute all these expectations.
3657440	3663900	429	B	1.0	First of all, you need to be able to compute all these posterior distributions using bayes rule or variational inference.
3664800	3682050	430	B	0.99867	Then once you have those posterior or approximate posterior distributions based on your high dimensional model of the world, then you need to compute some expectations, very high dimensional expectation using sampling or whatnot, to then get the expected free energy.
3682420	3684724	431	B	0.77287	So the expected free energy doesn't come up for free.
3684762	3690068	432	B	0.65724	It's actually a very I would say it's a quite sophisticated thing.
3690234	3693450	433	B	0.6	It puts all these things together, but it doesn't come for free.
3694460	3705080	434	B	0.99895	It's hard to implement on a practical problem that's nontrivial beyond toys and nations and people have done it, but it's a challenge.
3706860	3728240	435	B	0.89219	So the problem that active coherence has is, okay, well, we have this night theory, we have this very nice objective that theoretically works very well, and theoretically it's like or it's similar to what you would like to have for any kind of agenda.
3729640	3733430	436	B	0.99217	But we need to find ways of scaling this up.
3736440	3757340	437	B	0.77	Here comes the beauty of this, is that in scaling up active inference, you're going to use sampling, you're going to use techniques from optimization, from coherence, and so you're going to get something that's slightly different from the active inference scheme that you started with.
3757490	3766300	438	B	0.98346	So you had the active inference scheme proper, which was in the picture in the previous slide.
3769540	3773650	439	B	0.99891	This is active inference as you would like to implement it if you could.
3774580	3788710	440	B	0.69083	But because the problem is complicated and you have a high dimension state, space and so on, you have many things that are computationally intensive, you're going to use something, you're going to use optimization, you're gain to use inference, and so you're going to get something.
3789560	3798440	441	B	0.70298	Active inference lab algorithm that scales, but that's slightly different from what you started because it has all these other steps.
3799020	3805656	442	B	1.0	Now, in doing so, the active coherence agent becomes engineered.
3805848	3819336	443	B	0.7525	So it becomes closer to these reinforcement learning and bottom up agents that are built in a bottom up way to solve certain tasks.
3819448	3827490	444	B	0.99981	Because you want to engineer the active impress algorithm to be able to work to solve a certain problem, to scale it in a certain way.
3828020	3832644	445	B	0.99951	So in doing so, active imprint becomes closer to reinforcement learning.
3832842	3843400	446	B	1.0	Now, conversely, in reinforcement learning, the amount of compute is increasing.
3844780	3855930	447	B	0.99989	People want to build reinforcement learning agents that are increasingly learn in an unsupervised manner just because this scale is better.
3858480	3867052	448	B	1.0	You can't always design all the components of your reinforcement learning agent to solve any single tax.
3867106	3873840	449	B	0.90603	It's just too impractical, especially when tasks become more and more complicated.
3874500	3885616	450	B	1.0	You just would like general solutions that are going to work and are going to handle a lot of different types of situations.
3885808	3892520	451	B	0.68726	So reinforcement learning is adding or subtracting ingredients to their algorithms.
3892940	3894920	452	B	1.0	All these algorithms are evolving.
3895260	3926156	453	B	0.9999	But in some sense you can see this is debatable, I suppose, but you can see a general tendency towards moving to world models and generative models, moving to intrinsic motivation, which is another word for expected information gain or variants thereof, and an intrinsic motivation or intrinsic value that's added onto extrinsic value which is your expected reward.
3926348	3954410	454	B	0.99805	So you see many I mean, I think we're seeing reinforcement learning evolve in a direction also that is closer active inference lab because everybody wants algorithms that stole different tasks and not to have to change the algorithms for changing environments and so on.
3955040	3964620	455	B	0.90523	So in my view, even though we started with two different approaches, a bottom up approach and a top down approach, these are converging.
3965280	3982320	456	B	0.98	Now to come back to the contribution of active inference, if one reads through the paper and one looks at the derivation active inference lab, the point is that it's very agnostic.
3982400	3985316	457	B	0.97097	It's agnostic to the environment that we're in.
3985498	3987860	458	B	0.90512	It's an agnostic to many things.
3987930	3989510	459	B	0.99475	So it's very general.
3990200	4006780	460	B	0.96	And so if you take many practical reinforcement learning agent, you will see that they interact with their environment in a way that's compatible with the assumptions of the free hundred principle.
4007440	4020076	461	B	0.58014	In other words, the reinforcement learning agents can be recast as specific active inference agents with specific giant models.
4020268	4022208	462	B	0.99943	So this is really the key thing.
4022294	4025980	463	B	0.5592	It's this unifying perspective.
4026060	4037030	464	B	0.99164	I'm not saying it unifies everything, but I'm saying that many different reinforcement learning algorithms can be recast as active inference agents.
4037960	4059564	465	B	0.66	And so this is not to say that active inference lab implementation of those reinforcement algorithms is going to be better, but it's more like a theoretical contribution of like this set of equations given by the expected primary is a complete recipe or almost complete recipe to generate adaptive agents.
4059762	4063676	466	B	0.99657	So in some sense you don't have to go any further than that.
4063858	4079330	467	B	0.97	And so you can view all these reinforcement learning algorithms that comply with the assumptions so that can be recast as active inference algorithms as ways of scaling active inference and making active inference work.
4079780	4097492	468	B	0.99406	So it's not like I don't see any kind of conflict between active inference and reinforcement learning, but it's more like the reinforcement learning algorithms that work well and that can be recast as active inference algorithms.
4097556	4110456	469	B	0.99	And there are many, I would say more algorithms that can be recast and algorithm that can help when you look at the assumptions of the free energy principle that are super generic.
4110648	4121090	470	B	0.99807	So you can view all these algorithms as specific implementations of active inference in a way that active inference is meant to scale well.
4121540	4124720	471	B	0.98173	So this is kind of like the thing.
4124790	4134980	472	B	0.83	And so I think ultimately, again, these two Fields are going to converge, but I do not see any tension between them.
4135050	4138950	473	B	0.96956	It's more like the tools from one can be used in the other.
4142380	4144328	474	B	0.56	I see great work going forward.
4144414	4148216	475	B	1.0	And this is kind of like what we want to do with this paper.
4148398	4162590	476	B	0.99954	Put this active coherence where it comes from very succinctly, and also make it accessible for people who don't know the jargon from the fringe principal and active coherence community.
4163120	4168930	477	B	0.9	And yes, see the exciting developments that will follow.
4170740	4171200	478	A	0.62493	Wow.
4171270	4172476	479	A	0.9999	Great comments.
4172588	4192340	480	A	1.0	Two things that reflects to me the constraints that you mentioned, which we sometimes call the particular partition essentially cleaving the particle, which you show in figure two, cleaving the particle off from the environment.
4192700	4196852	481	A	0.99998	This is something that's widely applied in agent based modeling.
4196916	4218400	482	A	0.99977	Just anytime you're talking about some field of action and a player, we're basically at least qualitatively within the space of partitioning agents from environments and then further saying, well, there's no edge between the internal and the external states, so there's no telepathy and there's no telekinesis and incoming information.
4218470	4220300	483	A	0.8239	We're going to call sense outgoing.
4220380	4230576	484	A	0.98256	We're going to call action that qualitatively and formally is basically consistent with almost any cybernetics formulation of adaptive action.
4230688	4240628	485	A	0.99984	So Axel constant are really quite minimal and seemingly being generalized year after year with the work so many colleagues are involved in.
4240794	4245160	486	A	0.99484	So Axel Costa are not onerous.
4245660	4251684	487	A	1.0	And I think it's a really interesting question which reinforcement learnings are compatible?
4251812	4262840	488	A	1.0	And then coming from the other side, in a lot of active modeling contexts, we find ourselves sometimes proposing like auxiliary variables.
4263000	4265756	489	A	0.99366	Like, let's just do a parameter sweep over this.
4265938	4269052	490	A	1.0	And then we'll look back to the textbook or to the paper.
4269106	4271552	491	A	0.99932	We think, well, where was that in expected free energy?
4271686	4274304	492	A	0.99998	We just proposed this random thing.
4274502	4275916	493	A	0.99996	Where was that in the equation?
4276028	4281656	494	A	1.0	Or we'll look at step by step guide to active inference where it's all written in terms of matrices.
4281708	4283844	495	A	0.95894	It's all very read.
4283882	4285088	496	A	1.0	It like a sentence.
4285264	4289392	497	A	1.0	And all of a sudden it's like, wait a could be a neural network.
4289456	4291072	498	A	1.0	It doesn't just have to be a matrix.
4291216	4300280	499	A	0.99916	So it's kind of like we bring in these methods and ingredients that are being used widely, empirically.
4301020	4303620	500	A	1.0	And these are the two approaches.
4303700	4310030	501	A	0.99992	Do we build the bottom up mosaic of approaches stopping when it works?
4311040	4326560	502	A	0.67	And or do we start with this most general agent based cybernetic formulation and then kind of build the castle in the sky and meet in the middle again with something that works?
4326630	4330244	503	A	0.93736	It's just incredibly laid out.
4330442	4347770	504	A	1.0	And this paper is at that saddle point because it has one hand or antenna or whatever in this smooth information geometry conceptual area.
4348220	4356750	505	A	0.9998	But the motivator of all of the conceptual moves that are made, which honestly are extensive, like page after page, it was just like, where's it going?
4357680	4362860	506	A	0.98212	It's going somewhere that can be simulated on everyday hardware.
4363280	4373360	507	A	0.99995	So many of the moves were about reshaping or reframing what was to be done in a way that could be simulated.
4373780	4396120	508	A	1.0	And so those two paths are connecting and like you said, there's not necessarily attention, but it'll be quite interesting to see how this develops on the theory, practice and social frontiers as more and more of these threads start to combine.
4397420	4398570	509	B	0.74	Yeah, I agree.
4398940	4404520	510	B	0.99	I think it's particularly beautiful when theory meets practice.
4405520	4415976	511	B	0.98	You oftentimes have practice that works, but you have no theory or you have a theory that's beautiful but that doesn't work in practice.
4416088	4419730	512	B	0.97	And this just happens all the time.
4421300	4433524	513	B	0.99989	Typically you would have active that works and then you would try to build a theory out of it, but then often the theory turns out to be too hard, so you make some extra assumptions and so on.
4433562	4437424	514	B	1.0	You end up with a theory, but the theory turns out to be quite removed from the practice.
4437472	4439140	515	B	0.90811	So you kind of get that gap.
4440040	4446440	516	B	0.54077	It's very beautiful and quite unique and also quite rare when the theory meets the practice.
4447580	4454620	517	B	0.99	And I would say when the theory meets the practice and in particular meets the state of the art, then you kind of have a complete theory.
4456640	4474610	518	B	0.99299	So what we wanted to do in this paper is review these different Fields in a way that we could show how far the community has gone between making a theory that meets the state of the art.
4475380	4491884	519	B	0.97	And every section was started with theoretical considerations about what should be when you need to solve a particular problem, let's say adaptive agents or sampling or optimization.
4492032	4506300	520	B	0.99	And then from there, and from logical steps about, okay, well, it should be like this and not like that, because there is whatever kind of geometric argument or other arguments.
4506800	4514380	521	B	0.96	And so from there, logically arriving at state of the art methods that are used in practice.
4516320	4525970	522	B	1.0	The other kind of like sequence of logical steps was, okay, well, we're going to start with optimization which is in my view, at the root of everything.
4526980	4536724	523	B	0.99744	So sampling could be about optimizing samples to obtain the best approximation to your integral inferences about optimizing beliefs and decision making.
4536762	4540832	524	B	0.3648	It's about optimizing decisions based on their counterfactuals consequences.
4540896	4551130	525	B	0.9996	So we started up with optimization because it's the simplest thing both conceptually and also in terms of use case you need optimization to do everything else.
4552300	4560910	526	B	0.89665	We then went to sampling and to inference and finally to decision making because it just brings all these three ingredients together.
4561600	4573890	527	B	0.93878	We didn't go as far as showing, okay, well, this is how you should scale active inference in order to solve all the problems in the world because this just hasn't been done right.
4574420	4588096	528	B	0.99929	But we listed a few ingredients that have gone into the literature and also a few ingredients that this perspective offers to how to scale active inference and make it more effective.
4588208	4604160	529	B	0.99918	So it's actually future work and open problem to use the techniques that have been developed in the sampling section and the sampling literature to make active inference even more scalable and same with optimization and same with coherence.
4604180	4615532	530	B	0.97166	So there's many things that have been put forward in this article and also elsewhere that can be used to scale active imprints but ant least.
4615586	4630900	531	B	0.54	Yeah, the goal was to be as comprehensive as possible without sacrificing too much of actually understanding what's really going on and show how the theory means to practice throughout and really where the field stands.
4632360	4642516	532	A	0.97591	So one kind of reflection on this, again, is if we start with reward as our basal imperative pragmatism.
4642628	4648660	533	A	0.89	I mean, after all, don't we want to get things done, achieve results in the world, realize our preferences?
4648820	4661260	534	A	0.99984	We start with pragmatism and then it is empirically ad hoc how people introduce these novelty bonus or intrinsic motivations.
4662560	4678880	535	A	0.99988	In contrast, we can start with this information gain approach and then the thumb is put on the scale to bring more and more emphasis onto the alignment with preference.
4679320	4685700	536	A	0.88988	So it's like two roads, two paths.
4686120	4717328	537	A	1.0	And maybe this is our bias or corner of the information space that starting with a broader epistemic imperative allows the careful introduction of pragmatic value whereas a pragmatic foundation that it's hard to then recast epistemic value in terms of pragmatic value.
4717414	4720130	538	A	0.99985	That's kind of like the question of valuing basic research.
4720500	4731940	539	A	1.0	And the approach taken here is develop an imperative that can look entirely like one or the other or mixtures.
4732600	4748570	540	A	0.99996	But what's always so perplexing is that it has this extrinsic and intrinsic value phrasing but there are other ways to decompose the function.
4749660	4758604	541	A	0.99925	So even intrinsic and extrinsic value are not necessarily the ingredients that were put in.
4758802	4768080	542	A	0.86943	It's more like they were two ingredients that were split out of something else that is much more integrated.
4768740	4783570	543	A	1.0	It wasn't like the free energy was constructed by composition of any given decomposition of which there are multiple for variational free energy and expected free energy.
4784760	4800232	544	A	0.99454	So then a question that I've often wondered is there might be two policies that are very close or have essentially identical expected free energy but they could be radically different.
4800366	4808968	545	A	0.99999	For example, one could be having a good expected free energy because it realizes a lot of preferences.
4809144	4813500	546	A	0.97199	Another might because it provides a lot of information gain.
4814160	4846148	547	A	0.85009	So is it really as simple asterisk asterisk as a unified imperative or how do we make sense of the fact that one value could rank policies that might have, for example, extremely different danger profiles or envelopes of outcomes?
4846324	4857500	548	A	1.0	How can that seemingly multidimensional space be projected into essentially a ranking?
4859040	4862510	549	B	0.84	Yeah, it's a complicated question.
4864240	4868168	550	B	0.66347	As we can see, the expected free energy does this in some way.
4868354	4877920	551	B	1.0	It provides an answer to this question based on your preferences and how much you would gain by observing this new data, how much information you would gain.
4878420	4881232	552	B	0.91	You can put these two things on the same.
4881286	4881644	553	B	0.99922	Footing.
4881692	4885060	554	B	0.99999	This is what expected PNG does, and it gives you a ranking.
4889080	4894490	555	B	0.57988	You're right to say that two very different courses of action could have the same expected free energy.
4895980	4897960	556	B	0.9	And so how do you choose between them?
4898110	4909484	557	B	0.99358	Well, the standard formulation that you would see in textbooks or papers has, okay, well, G of Action Sequence equals this and G of Sequence equals that.
4909602	4913324	558	B	0.99242	So these two Action sequences have the same G and you don't know what to do.
4913522	4923516	559	B	0.99337	But here, this theoretical development in this paper makes it very clear that the expected PNG is a minus lock probability.
4923708	4924992	560	B	1.0	And this is why.
4925126	4934144	561	B	0.55749	So to make it more transparent, instead of using the letter G that's commonly used in the literature, we kept the minus log P throughout.
4934192	4936070	562	B	0.99721	So we never used the letter G.
4936520	4941536	563	B	1.0	And so the expected free energy is this minus log P of Action Sequence.
4941728	4954192	564	B	0.95	And so it says, okay, well, if two Action Sequences have the same expected free energy, well, then you have, let's say you just have two of them with the same expected force energy, the lowest one, well, then you choose probabilistically.
4954276	4959710	565	B	0.75615	Like half of the time you choose one, and half of the time you use the other.
4960320	4973344	566	B	0.3738	Actually, it's even a bit more complicated because since it's a minus lot P, you could have an Action Sequence with a very high expected free energy.
4973542	4976400	567	B	0.98	And so this means that you can still take it.
4976470	4979250	568	B	0.57828	But if you would take it, like, not very often.
4979800	4985488	569	B	0.9561	So you have an exponentially low chance of taking that Action Sequence.
4985584	4988580	570	B	0.64718	So it's a probabilistic description.
4990600	4997112	571	B	0.99	It says that the most likely Action Sequence would be the argument, the minimum of the expected free energy.
4997166	4999848	572	B	0.99969	So this is like the top line in the figure.
5000014	5008030	573	B	0.96	And so if you have two most likely Action Sequences, well, then you choose like half of the time you choose one, half of the time you choose the other.
5008800	5017730	574	B	0.99988	But then in general, if you didn't want to simulate the most likely Action Sequence, then you choose probabilistically between all of them.
5018820	5030640	575	B	1.0	And the Action Sequences that have the lowest expected reality has an exponentially higher chance of being selected and the others don't.
5034340	5035490	576	B	0.75	One last thing.
5035880	5053592	577	B	0.90776	Another very nice perspective is that the probability over Action sequences, so these P of a greater than equal to T is then the exponential of minus the expected free energy.
5053726	5064824	578	B	0.99998	Because if you take exponential minus of what you get in the equality on the left, then you recover this probability distribution of our Action Sequences.
5064952	5073408	579	B	0.99873	So this distribution of our Action Sequences is exponential minus expected free energy, exponential minus what's in there.
5073574	5079052	580	B	0.98	And we saw before, that gives measures, they arise everywhere.
5079116	5081152	581	B	0.78	And so this is an example of a gives measure.
5081216	5095480	582	B	1.0	You have exponential minus something basically, based on the formalism of the free free energy principle comes up that the probability distribution over Actions is a gives measure.
5096860	5102250	583	B	0.92	It is the gives measure where what's inside is the expected free energy.
5103260	5113800	584	B	0.72732	So, yeah, it's kind of like a full circle thing and this is basically this sort of like connectivity.
5113880	5125484	585	B	0.51451	They just happen all the time when we were writing this paper and discussing because there's so many things that just there's so many crossovers at so many different levels.
5125532	5139030	586	B	0.88415	Like on the sort of object level that you're studying, but also on the ecological level, like methodologies that are used in some field, they're used in a different field to do something else.
5139560	5141548	587	B	0.98864	So there's so many crossovers.
5141664	5151412	588	B	0.92	And so this is, I would say, an interesting example of why gives measures, gives distinctions, they just arise everywhere.
5151476	5157608	589	B	0.99925	They arise active inference Lab and as we saw before, they're hard to sample from.
5157774	5168988	590	B	0.56864	So if you wanted to so let's say you have your gives distribution that says, okay, well, this is my distribution of our action sequences that I get from the expected free energy.
5169154	5170760	591	B	0.23712	Then you have two choices.
5170920	5189780	592	B	0.98448	Either you sample from it to sample like an average, I would say not a typical action sequence, or you could optimize the distribution or optimize the expected free energy to sample the most likely action sequence.
5190760	5197560	593	B	0.99967	In the two cases after you've done your planning with the expected free energy, you have a choice between sampling and optimization.
5198220	5220648	594	B	0.98809	So you kind of go back full circle of like if I want to select my action gain, I either select the most likely action by optimizing the expected free energy or the gives measure by optimizing this minus lot P by taking the action sequence with the minimal expected free energy.
5220834	5227308	595	B	0.85	Or I have my gives measure and it will sample from it to like get a typical action sequence.
5227484	5229840	596	B	0.78942	So it's all like super interrelated.
5233480	5240420	597	A	0.99962	Very interesting in the Par et al.
5240570	5265340	598	A	0.96492	Textbook from 2022, that dialectic is mapped onto the mammalian nervous system where policies can be selected by essentially passing through habit proportionally or expected free energy can be applied to sharpen or optimize the posterior on action.
5269380	5274720	599	A	0.94304	Does this influence how you select micro meso or macro actions?
5279300	5286740	600	B	0.98273	Do you mean for human being like actions at the cellular level, actions at the Global level?
5286810	5288788	601	B	0.76	Or is it something different like at.
5288794	5301480	602	A	1.0	The personal, grasping a coffee cup at the micro, larger scale decisions, attention allocation, communication emissions.
5303100	5311736	603	B	0.99	Yeah, I think all of these things can be formulated in this treatment.
5311768	5319570	604	B	0.98854	So, for example, attention would be where you choose to place your focus or your mental eye, so to speak.
5320580	5326370	605	B	0.99557	So that's and internal action, it could be unconscious, it could be undirected, but still happening.
5328740	5336980	606	B	0.79024	I'm not an expert, but there's a lot of talking consciousness research of like, you being conscious.
5337640	5349800	607	B	0.68546	There's a very low amount of processes that are conscious and these change according to where you're inner eye or your conscious eye, whatever you want to call it, like moves.
5351020	5360380	608	B	0.99721	This could be thought of a mental action or an inner action that you may or may not control, but still and action.
5361600	5381060	609	B	0.51874	So all these, all other action, whether it is planning at the short time scale, taking a coffee cup standing, or planning at the longer time scale, like what am I going to do after my PhD and so on, all of these things are action.
5382600	5394980	610	B	0.9757	So typically, as you know, of course, we we model that by using hierarchical models where you have different levels in your giant model.
5395050	5406540	611	B	0.93003	So in your model of the world and the higher levels are represent more abstract representations and also longer time scales.
5407760	5421952	612	B	0.71557	So it could be, let's say at some higher level I'm going to take the plane to go to Amsterdam, for example, and at the lower level, okay, well, once in Amsterdam, I'm going to do this, this and this.
5422006	5429410	613	B	0.92215	It's like this action at the higher level of taking the plane predates everything that you're going to do at the lower level.
5430040	5446116	614	B	0.71	And so factorizing these different decisions into different levels of a model allow you to be completionally, practical, practically implementable when you make those decisions.
5446148	5457790	615	B	1.0	And presumably this is why we do in our everyday life factorize decisions and representations into low level and high level things.
5459440	5464670	616	B	0.87	The beauty of the free energy principle and of the treatment here.
5466000	5480930	617	B	0.83878	But the treatment here is again, just the free energy principle in its most bare bones and general form is that the expected free energy is formulated for any kind of gentle model.
5481700	5488144	618	B	1.0	The gentle models that you have in the figure on the bottom right there, they could be arbitrary.
5488192	5503050	619	B	0.99784	So you could use this form of expected Lagrange for hierarchical generative models that have all these like high level, low level decisions and representations, but you could also use it for any other kind of generative model.
5504160	5523680	620	B	1.0	The only difference is that with some world models, it would be very hard to implement this in practice and with other world models, and in particular those that are highly factorised and distributed like hierarchical models, like the wounds we just talked about.
5523830	5545060	621	B	1.0	With these it would typically Dean much simpler to take decisions that minimize expected frequency when it comes to practical computation, just because when you action oriented representation things, just computations just factor out a lot and it's just like way simpler to do.
5545210	5554120	622	B	0.99846	But the formulation here is like entirely generic and it could apply to all kinds of models and all kinds of situations.
5559410	5560560	623	A	0.85471	Very interesting.
5560930	5574302	624	A	0.99611	Well, in our last little segment on the dot one, let's hear your perspective or overview on the roadmap.
5574446	5584598	625	A	0.99995	Just walk through what the sections are and you mentioned some related topics, but what do the subjects broadly cover?
5584684	5586760	626	A	0.99541	Why are they structured in that order?
5588010	5606350	627	B	1.0	Right, well, first of all, the introduction just puts like everything in context, the conducting line or one of the common points between all of these sections is that there's a lot of geometry.
5607330	5615946	628	B	0.88918	So this was not purely because the journal is like the handbook of statistics, it was called Geometry and Statistics.
5615978	5623570	629	B	0.96287	So this is not just for that, but it's because geometry just comes up naturally in all these places.
5624390	5637922	630	B	0.9281	So this is what we explained in the introduction, like how geometry comes up, the different branches of geometry that come up in this different field and how they're interrelated.
5637986	5651930	631	B	0.98	And so just from there you kind of get a picture of, okay, well, this branch, let's say symplectic geometry, for example, it comes up in accelerated optimization and it also comes up in sampling.
5652510	5668820	632	B	0.99	And so you can already see from there what is going to be discussed, of course, but also which branches of geometry occur where and what kind of parallels there are already.
5670230	5675086	633	B	0.99543	So going from the introduction to accelerated optimization.
5675278	5679602	634	B	0.99269	So optimization is conceptually, it's a very simple thing.
5679736	5681778	635	B	0.80892	It's like, okay, we have a function.
5681944	5692950	636	B	0.98908	So coming back to, let's say that my Dean state space come back to where we started with and you have a function and a landscape over it, let's say like a mountain.
5694650	5698662	637	B	0.94	And so the goal is to find the minimum of the mountain.
5698726	5701910	638	B	0.33	I go from where you start to the minimum.
5702070	5708320	639	B	0.78422	There's so many different ways of solving this problem, practically speaking.
5709090	5717200	640	B	1.0	The main challenge, of course, is that you don't know, you cannot speak anywhere else except where you have been.
5717990	5723294	641	B	0.74625	So you start somewhere and you're completely blind and you need to find a minimum.
5723342	5724660	642	B	0.99919	So how do you do that?
5725590	5729790	643	B	0.98388	There's so many ways, so many approaches to optimization.
5729870	5734200	644	B	0.95	I mean, it's a whole field and it's in my opinion, a very messy one.
5734890	5757760	645	B	0.57	And this is understandable because there's no free lunch theorems that are very well known that say that under some hypotheses, two ways of doing optimization will behave the same on average if you test them again, all possible sorts of problems.
5759170	5773940	646	B	0.99589	So this is to say that if you have a given problem, then you need to give then the optimization methods that will work well are those that implicitly use the prior information that you have about the problem.
5774710	5786840	647	B	0.99986	In other words, according to the theorems, there's no optimization methods that will beat everything, it just doesn't exist because they're all on average the same.
5787210	5803242	648	B	0.99996	But this is not to say that you cannot make any practical, meaningful steps by doing optimization because not all optimization problems are the optimization problems that we have in life.
5803296	5810106	649	B	0.61211	They're not completely random, they're not completely random landscapes that we need to optimize, but they actually have some structure.
5810218	5821394	650	B	0.99995	So we can actually explore that in algorithms and have things that just behave, that just work a lot better for the type of problems that we're interested in than others.
5821592	5828530	651	B	0.98766	So this is what kind of like the starting point for all optimization.
5830970	5836390	652	B	0.98	The type of optimization that we considered here is optimization of smooth functions.
5837130	5841330	653	B	0.82924	This is already like a big restriction there's.
5841490	5852140	654	B	0.99515	So many methods that are designed to optimize functions that are rugged or non smooth or have discontinuities and so on.
5853070	5859006	655	B	0.99981	When you actually get rugged or discontinuous landscapes, it's a whole different problem.
5859188	5863870	656	B	0.91	Here we chose to focus on smooth functions.
5866050	5873250	657	B	0.53	The idea is that you have a smooth landscape, you want to find the minimum.
5874310	5884470	658	B	0.99	And one way to do so, and to do so well, is through the use of a tool called geometric inclination.
5885610	5890630	659	B	0.92888	So I'll walk through the main ideas of this section.
5892410	5920610	660	B	1.0	The idea, first of all, is that you may design a method that theoretically works very well, but when you implement it in practice on your computer, your discretion errors, because you can only implement it in discrete time, what you actually end up implementing ends up deferring significantly from what you initially wanted to implement.
5921350	5931538	661	B	0.99996	This is a problem that we discussed previously in sampling and it just comes up everywhere in applied mathematics and numerical mathematics.
5931714	5942022	662	B	1.0	The fact that you can only do discrete computations on your computer is a big implication and it's something that needs to be looked into.
5942076	5946202	663	B	0.61826	It's maybe even the most important thing that needs to be looked into.
5946256	5961470	664	B	0.99906	When building a numerical method, you want to be able to implement numerical methods that are very close from what you theoretically would like to implement.
5962450	5969466	665	B	0.99393	So typically you would have some theoretical algorithm that would have some convergence guarantees or performance guarantees.
5969578	5975490	666	B	0.57	And so you would like to wait a way to implement it on your computer so as to preserve those guarantees.
5976550	5979140	667	B	1.0	And this turns out to be very hard in general.
5980710	5994070	668	B	0.99	One way to do it, I would say maybe the most powerful way out there to do this is through procedures from the field of geometric inclination.
5995450	6019220	669	B	0.6	And so geometric integration is a set of techniques that allow you under certain cognition to discretize your theoretical, dynamic or theoretical optimization procedure like dynamical system, and make it into an algorithm that you can practically implement on a computer.
6019670	6025220	670	B	0.87	And that's going to respect the performance guarantees that you already have.
6026550	6032450	671	B	0.99655	So now the tricky part, geometric inclination is based on geometry.
6032610	6045474	672	B	0.99377	So you need to find it's not as simple as like, okay, well, I have my landscape, my function landscape, and I'd like to just go down and find the minimum.
6045522	6050522	673	B	1.0	One way to do that is just by doing grain in descent in continuous time.
6050576	6055222	674	B	1.0	And you know that that's going to go down until it reaches a local minimum at the very least.
6055376	6056800	675	B	0.94459	So it's not too bad.
6057490	6067338	676	B	0.99996	If you were to implement gradient descent numerically, it wouldn't be exactly the same as like the continuous time dynamic.
6067434	6080114	677	B	1.0	It could be that actually by implementing gradient descent numerically, you get all sorts of numerical problems or whatnot maybe grade and send.
6080152	6084082	678	B	0.48883	It's so simple it's not actually a good example.
6084216	6093690	679	B	0.99717	But the point I want to make is that it's not as simple as like, okay, well, I have this dynamic that works really well, and that gets to the minimum.
6094030	6097580	680	B	0.68	And I know that it's going to get to the minimum this fast.
6099150	6108926	681	B	0.93	And so geometric integration will give me an algorithm that I can implement on my computer, and that's going to get to the minimum this fast as well.
6108948	6110720	682	B	0.56002	It's not as simple as this.
6111330	6119060	683	B	0.99109	Geometric integration works by preserving geometric structures in the dynamic that you started with.
6119510	6138374	684	B	0.87914	So in order to be able to apply geometric integration, to have algorithms that work for optimization, you need to design a dynamical system that converges to the minimum and that has some important geometric features that can be preserved in this way.
6138572	6145210	685	B	0.97	And so this is basically the goal of the whole section, and it goes into the following steps.
6146830	6164750	686	B	0.95261	So one thing that can be discretized or implemented on a computer in a way that is close to the true dynamical system is what's called Hamiltonian flows.
6165090	6181410	687	B	0.99702	So if you specify a Hamiltonian, which is a kinetic energy and potential energy, then you get the Hamilton Jacobi equations of motion that gives you the behavior of the system that has the prescribed Hamiltonian.
6181930	6195250	688	B	0.90076	So for those who've taken a course in physics, you probably know or may have seen that all of Newtonian mechanics can be reformulated as Hamiltonian mechanics.
6195410	6204054	689	B	0.99947	So all of that can be described as with Newton, lows of motion can be described through Hamiltonian mechanics.
6204102	6225780	690	B	0.98994	It's a very general framework or a very general way to look at physics, because it's such a general point of view on physics and such a, and just Hamiltonians, they come up everywhere, this mixture of kinetic and potential energy, to concisely summarize the behavior of a system.
6226790	6235490	691	B	0.99999	Because they occur so much, they have been studied extensively, and it turns out that Hamiltonians and specifically the motion they entail.
6235650	6250566	692	B	0.86618	So the Hamilton Jacobi equations of motion, they can be discretized or implemented accurately on a computer using Symplectic integration, which is a part of geometric integration.
6250758	6259360	693	B	0.9769	So in short, you can use Symplectic integration to accurately simulate a Hamiltonian dynamical system.
6260130	6269710	694	B	1.0	Now, the problem is, these dynamics are conservative, so they preserve the Hamiltonian.
6269870	6282262	695	B	0.9857	So if you're at some point hidden states, space with a certain kinetic energy, potential energy, and you run the dynamics forward, your overall energy is not going to change.
6282316	6283602	696	B	0.77311	So that standard physics.
6283666	6296966	697	B	0.9998	Energy is always conserved when there's no friction, all kind of like ideal physical systems with no friction, the energy is conserved.
6296998	6306198	698	B	0.99965	So when you're going to run this Symplectic optimization scheme to simulate these Hamilton and Jacobi equations of motion, the energy is going to be conserved.
6306294	6317802	699	B	1.0	Now, we're in trouble if we want to do optimization, because in optimization, if you think of the function as the energy, you want to dissipate energy as much as possible to reach the minimum.
6317866	6319380	700	B	0.95804	So you want to lose energy.
6320870	6335240	701	B	0.50448	So what the section does so this section, by the Way, is a review of more technical papers what these papers do, it's a very clever trick is they say, okay, well, we want to minimize a function.
6336490	6343750	702	B	0.99583	We're going to say the function is potential energy and also we're going to add a kinetic energy on top of that to get a Hamiltonian.
6346670	6357054	703	B	0.95	Now this Hamiltonian, we could simulate it through symplectic integration, but actually, if we did, we wouldn't dissipate the function.
6357252	6376180	704	B	0.64993	So now what the section does is it introduces a higher dimensional state space with the Hamiltonian that if you simulate those dynamics, it actually optimizes the function.
6379030	6388690	705	B	0.56581	So this is what was a convoluted way to say it reformulates optimization as a conservative system with a different Hamiltonian.
6388850	6405580	706	B	0.54	And so by and because we know how to simulate dynamics with Hamiltonian so well, a good way of doing optimization is by finding a Hamiltonian so that when you discretize it, you get the optimization process that you wanted.
6406190	6411834	707	B	0.91519	If you do that, you get a numerical method that's stable, that works well, that has good coherence guarantees.
6411962	6422682	708	B	1.0	And it turns out the numerical methods that are developed, they work for optimization on any kind of smooth manifold, for function, on any smooth functions, on any smooth manifold.
6422826	6423742	709	B	0.99972	They work well.
6423796	6429758	710	B	0.87782	They are generalized gradient descent in the sense that you recover gradient descent in a certain limit.
6429934	6443080	711	B	1.0	And they can be seen as accelerated because they have this notion of acceleration in the sense that, let's say the function that you want to optimize, it like a hill and you want to go down.
6443450	6456534	712	B	0.99049	If you just did great, in the sense you're like blindingly going down at the same speed, or your speed is like proportional to the slope of the landscape.
6456662	6462030	713	B	0.9994	But when you actually let a Bull rolling down, this is not really what happens.
6462100	6471790	714	B	0.99	The Bull is going to roll faster and faster and it's going to accelerate, it's going to go down, it's going to overshoot the minimum and then it's going to like oscillate until it converges to a minimum.
6472450	6475954	715	B	0.7088	So these are the kind of optimization methods that are developed here.
6475992	6490950	716	B	0.57018	They're optimization methods that are accelerated in a physical sense, that has physical sense acceleration and physical meaning because they're going to accelerate as they go down, overshoot and then stabilize.
6491690	6495398	717	B	0.99839	So this is the whole point of this section.
6495574	6502700	718	B	0.71	Now briefly, because I think we're running out of time, but we can of course, revisit these sections later.
6505650	6506874	719	B	0.9913	In the third action.
6506922	6507706	720	B	0.99992	We have sampling.
6507738	6510506	721	B	0.9963	So we talked extensively about sampling.
6510698	6518042	722	B	0.98	The sampling, as we discussed, can be seen as an optimization of probability distributions.
6518186	6533230	723	B	0.93625	So we have the distribution that we want to sample from, and we just run a process that is random so it can be described by its own distribution and want the process to concept as fast as possible to the target.
6533390	6541670	724	B	0.9869	So we want that distribution of the process to concept as fast as possible to the target distinctions.
6542910	6549402	725	B	0.99997	So in a way that can be seen as I guess complicated for many people.
6549456	6551370	726	B	1.0	I think it's pretty abstract.
6552590	6560350	727	B	0.99124	But it's been known for a long time that you can reformulate sampling as just optimization of distributions.
6561810	6589186	728	B	1.0	Now it turns out that when you use the accelerated optimization method of the second section to sampling so when you use, when you do accelerated optimization on the space of probability distributions in order to solve the sampling problem, you get a process that's called under damp launch one dynamics.
6589378	6605974	729	B	0.92108	So if you solve that process, you're basically going to get good samples, very good samples of your target distribution because that process can be seen as doing an accelerated optimization on the space of probability distributions.
6606102	6610634	730	B	0.99991	In other words, the distribution describing the process is going to accelerate towards the target.
6610682	6615280	731	B	0.57105	So you're going to get there very fast and your symbols are going to be very good.
6617090	6624130	732	B	0.77816	So this is 2.6 where this is kind of derived.
6624470	6633320	733	B	0.55565	In 3.1, we basically talk about the the good properties of samplers in general.
6633770	6647450	734	B	0.99965	So sampling is not like a complete field, but there's a lot of work showing that certain processes have certain characteristics of processes make them better samplers and others make them worse.
6648350	6649786	735	B	0.99648	So this is what we discuss.
6649888	6658650	736	B	1.0	And then in free .2, we put this all together in Hamiltonian Monte Carlo, which is a state of the art method for sampling.
6663250	6678338	737	B	0.93577	Hamiltonian Monte Carlo is a way of using, if you use symplectic integration under Downplantromodynamics, which is your accelerated sampling method, that you cannot stimulate directly, again, you get the same problem.
6678504	6684142	738	B	0.99929	If you use symplectic integration to simulate that, then you get HamiltonI Monte Carlo.
6684286	6693510	739	B	0.99585	So Hamiltonian Monte Carlo is attributes Google way of doing accelerated sampling that's based on ideas of symplectic integration.
6695710	6700970	740	B	0.97	Now, very briefly, the section four is on inference.
6702750	6707546	741	B	0.97	The inference in its most general form is variational inference.
6707578	6713802	742	B	0.99091	So what we typically do know and love from active inference.
6713866	6720734	743	B	0.99	And I have a target probability distribution that I'm not able to compute directly.
6720782	6723460	744	B	0.99985	So I'm going to approximate it as best as I can.
6724150	6743850	745	B	1.0	Now, these distributions, they're often used to compute expectations, or in other words, in statistics and machine learning, we often need to compute expectations with respect to some probability distribution.
6744830	6752090	746	B	0.99997	What this section does is it derives a whole bunch of divergences.
6752670	6754422	747	B	0.99991	Like the Kel divergence.
6754486	6758042	748	B	0.95	I mean, the Kale divergence is not discussed so much in this action.
6758186	6773170	749	B	0.59638	I'll explain why, but it derives a whole bunch of divergences that can be used to approximate this unknown distribution with some other distributions that are tractable.
6775590	6797850	750	B	0.76922	So to put in a different way, you have this abstract variational inference problem where you have this target distribution and then you have this family of distribution that you know, and you want to select the distribution in your family that best approximates your target distribution.
6798670	6811002	751	B	1.0	Now, in order to do this, you first need to derive a measure of similarity between distributions to know, okay, well, what is the best distribution.
6811066	6817690	752	B	0.72172	Like, how can you quantify being the best distribution, being the best approximation to your target distribution?
6817770	6822610	753	B	0.99	And to do that, you need to specify a emergence, like the KL divergence.
6823830	6831430	754	B	0.98825	But the problem is, the KL divergence is not always practically applicable.
6832330	6836978	755	B	1.0	One example is when the target distribution is just given by samples.
6837154	6840470	756	B	0.99135	So maybe you don't even know the target distribution.
6841690	6844134	757	B	0.76703	Maybe it's a gaussian, but you don't know it.
6844172	6846046	758	B	1.0	You just have samples from the gaussian.
6846098	6853786	759	B	0.8664	So you cannot just have, like, a histogram, and you need to best approximate the histogram with the distribution in some family.
6853968	6860746	760	B	0.867	So when you have a problem like this, which just happens all the time, you cannot use the KL divergence.
6860778	6862830	761	B	0.99811	So you need to think about other divergences.
6863250	6883080	762	B	0.99	And so what this section does is deriving other classes of divergences that are more generally applicable than the KL divergence, and they have also some very nice properties and then showing, okay, well, how is it that I can minimize these divergences to solve a variation of inference problem?
6884250	6899880	763	B	0.98286	So I think there's a lot of things to be done there in the sense that in active inference, we always use the Kale divergence to do variational inference and minimize free energy.
6900250	6911470	764	B	0.97	And this Kale divergence is available because we typically have adjuncted models that are nice and they're expressed graphical models.
6913010	6921970	765	B	0.97674	But there's this whole other plethora of divergences and algorithms to minimize the divergences that's out there.
6922040	6934070	766	B	0.51	And it could be the case that there's very interesting recipes and methods that can be either brought back to the KL case that we use or just used altogether in our setting.
6935690	6950410	767	B	0.99981	In particular, when, for example, let's say they wanted to approximate the expected free energy in active coherence, the expected free energy is an expectation and want to approximate that expectation.
6950750	6952220	768	B	1.0	It could be the case.
6953630	6970718	769	B	0.69273	Well, I think these methods that are summarizing section four could be very useful because they basically develop divergences that quantify how far the expectations are going to be if you choose a different distribution.
6970814	6975060	770	B	0.51906	So it's all like, very natural in that computational sense.
6975430	6979806	771	B	0.8	And then finally ant activation through active coherence.
6979838	6990040	772	B	0.99003	Well, I think we've discussed that already extensively, but I won't spoil it more today and we can come back to it next time.
6995910	6997694	773	A	0.99305	Thanks a lot for that overview.
6997742	6999650	774	A	0.99882	That was very powerful.
7001270	7005490	775	A	0.99962	Incredible dot one very informative.
7006710	7014854	776	A	1.0	Looking forward to starting with some questions from the live chaos and people's comments on the video between now and next week.
7014972	7024566	777	A	0.9281	If anybody else of your colleagues or anyone in the institute wants to join, we'll be here next week for 52.2.
7024668	7026710	778	A	0.99492	So thank you, Lance, for the time.
7026780	7028280	779	A	0.99671	Really appreciate it.
7028650	7029910	780	B	0.63935	Thanks, Daniel.
7030490	7031510	781	A	0.84909	Farewell.
7032770	7033710	782	B	0.99727	See you next week.
7033780	7034010	783	B	0.95	Bye.

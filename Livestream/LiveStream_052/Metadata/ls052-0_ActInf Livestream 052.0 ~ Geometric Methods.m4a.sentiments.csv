start	end	speaker	sentiment	confidence	text
13070	14146	A	0.8720964789390564	Hello and welcome.
14328	22740	A	0.917802631855011	This is ActInf Lab Livestream number 52.0, and it is February 28, 2023.
25270	27838	A	0.6894928812980652	Welcome to the active inference institute.
28014	36090	A	0.6512011885643005	We're a participatory online institute that is communicating, learning and practicing applied active coherence.
36510	39030	A	0.911372721195221	You can find us at the links here on this slide.
39190	45340	A	0.5262759923934937	This is a recorded and an archived live stream, so please provide feedback so we can improve our work.
46030	51610	A	0.8522964715957642	All backgrounds and perspectives are welcome and we'll be following video etiquette for live streams.
52050	62850	A	0.5109705924987793	Head over active coherence.org if you want to learn more and participate in learning groups and projects at the institute, including these live streams.
64630	96774	A	0.7936996221542358	Well, we're here today to learn and discuss the paper Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents by Alessandro Barp Lancelot Costa Guilherme Franca Karl Friston Mark Girolami Michael Jordan Grigorios A Pavliotis this video, like all Zero videos, are is and introduction for some of the ideas.
96822	104782	A	0.5716153383255005	It is not a review or a final word, and as we have joked before, it's more than anything a call for help.
104916	123426	A	0.8912539482116699	So if you're curious about these topics, if you're knowledgeable about these topics, we would really look forward to you getting involved in the upcoming 52.1 and 52.2 discussions, as well as in an ongoing basis to help us understand some of the technical details.
123618	144220	A	0.9702308177947998	This is going to get technical ant times and certainly beyond the technicalities I understand, though I am looking forward to presenting them and it'll be great to have those who have backgrounds of all different types to come together and talk about this awesome work.
144830	152282	A	0.9485456943511963	Also, a big thanks to Ali and Candon for the assistance, technical and moral.
152426	170420	A	0.9005033373832703	During the preparations here in 52.0, we're going to bring up some aims and claims, read the abstract, look at the roadmap and talk about the keywords, and then we will walk through the paper with an emphasis on the figures, formalisms and key points.
170870	175346	A	0.7836733460426331	In the coming weeks, we're going to be discussing this paper with one or more authors.
175458	183670	A	0.6454498767852783	So, as usual, get in touch if you want to participate or if it's after the fact, you can still get involved.
185450	188520	A	0.843003511428833	We can start with an introduction or a warm up.
188830	202686	A	0.47579628229141235	I'm Daniel, I'm a researcher in California, and I'm tempted to say, just totally honestly, I'm happy to get this one over with, but that sounds a little bit different than I might intended to be.
202788	212110	A	0.9783363342285156	I'm really excited to dive into this work, which is going to be approaching active inference from an angle that we haven't necessarily highlighted on these streams.
212190	215054	A	0.967789888381958	So I think it's going to be a fascinating discussion.
215102	226278	A	0.8401692509651184	It's going to run the gamut, span the gap however you choose to see it, between technical sophistication and intuition, which is a great place to be.
226364	237530	A	0.989621639251709	So I've been really excited and motivated to prepare, and I'm looking forward to the dot zero we're doing right now and to the upcoming discussions.
238110	248618	A	0.5837405323982239	So let's jump in with the big questions that might motivate one to read this paper, this kind of paper, even if they were not familiar with the authors or topics.
248794	251278	A	0.5650085806846619	And these are just a few ways to write it up.
251364	252974	A	0.7485981583595276	Of course, not the only ways.
253092	262638	A	0.8384189605712891	So, first question how can we effectively and efficiently navigate information geometric or information theoretic landscapes?
262814	275430	A	0.874891996383667	And how can we tackle that question from an analytical, which is to say equation based as well as a computational, which is to say real implication based perspective?
275930	286422	A	0.9201046824455261	Often it's really fun, intuitive, natural to talk about information theory, even for those who haven't taken the technical prerequisites.
286566	301710	A	0.8170398473739624	And this work may help us navigate to a space where we're able to think with good intuitions about information geometry, information theory, and also make sure that those intuitions are going to be caught by our technical tools.
303330	314050	A	0.8478343486785889	Second, how can we optimize inference in complexity models, including cases where we are doing inference on action as a parameter, also known active inference lab?
314950	329558	A	0.6100070476531982	Optimization and functional analysis, as we'll come to see soon has long been interested in these complex or challenging models that are right at the margin, right ant the border of what is trackable or not.
329644	352618	A	0.7790924906730652	Given the computational hardware that modelers have access to, for example, big data sets, high dimensional state spaces, complex dynamics and so on, how about cases where we're also interested in doing parametric inference and optimization on action or action plans as a parameter?
352794	354910	A	0.8849382996559143	Also known active inference lab.
355330	367010	A	0.8969831466674805	And third, what technical underpinnings support the rigor and applicability of active inference and what special cases are revealed when we consider possibilities and constraints?
367670	372770	A	0.8614182472229004	These are those two branches of analytical and computational coming back again.
372920	384210	A	0.7845355272293091	We want to make sure that when we're thinking within the active inference paradigm, the active coherence ontology is the language that we're speaking and that there's a technical rigor to what's being discussed.
384370	394118	A	0.5395804643630981	Additionally, we want to make sure that it's not just an analytical rigor, but it's also going to be computationally plausible tractable or maybe even preferable.
394214	395580	A	0.9712504744529724	That would be awesome.
396030	399390	A	0.8140255212783813	Those are the questions that at least appeared.
400370	413918	A	0.814889132976532	We are discussing this paper Geometric Methods for Sampling, Optimization, Inference and Adaptive Agents by Barp and Costa at all shared first authorship and it was published on active.
414094	418260	A	0.8883131146430969	I'll just note a few key claims from this paper.
418630	431110	A	0.6438261270523071	They wrote Our goal in this chapter is to discuss the emergence of natural geometries within a few important areas of statistics and applied mathematics, namely optimization, sampling, inference and adaptive agents.
431260	432550	A	0.6860014200210571	That's the title.
433290	442870	A	0.8281310200691223	We provide a conceptual introduction to the underlying ideas rather than a technical discussion highlighting connections with various Fields of mathematics and physics.
443030	449614	A	0.7192931771278381	Well, I can tell you that they do make connections with various Fields of mathematics and physics, though.
449652	455630	A	0.8831202983856201	Whether it is a concept introduction or a technical discussion is all going to be about your perspective.
457250	463134	A	0.8911895155906677	Third, to illustrate a generic use case for the previous methodologies that are going to be discussed.
463262	471438	A	0.8579203486442566	We consider active inference a unifying formulation of behavior, subsuming perception, planning and learning as a process of inference.
471614	477094	A	0.8507822751998901	So inference on adaptive agents is not necessarily new.
477212	481062	A	0.7683998942375183	It goes by reinforcement learning, machine learning, and so on.
481196	492940	A	0.8651162981987	However, to use those methodologies in the context of active inference a unifying formulation of behavior is something that the authors are bringing forth here.
493790	505562	A	0.7950083017349243	And last, we describe decision making under active inference using information geometry, revealing several special cases that are established notions in statistics, cognitive science and engineering.
505706	511298	A	0.8171031475067139	So active inference is not just unifying, it's going to be shown to be generalized as well.
511464	542830	A	0.7912851572036743	Which is to say that special cases of the generalization emergence different formalisms that were known disparately across Fields onto the abstract they write in this chapter, we identify fundamental geometric structures that underlie the problems of sampling, optimization, inference and adaptive decision making.
543520	552080	A	0.5310497283935547	Based on this identification, we derive algorithms that exploit these geometric structures to solve these problems efficiently.
552580	566020	A	0.647650957107544	We show that a wide range of geometric theories emerge naturally in these Fields, foraging from measure preserving processes, information divergences, Poisson geometry, and geometric integration.
566680	577876	A	0.6314902305603027	Specifically, we explain how, one, leveraging the symplectic geometry of Hamiltonian systems enable us to construct accelerated sampling and optimization methods.
578068	597528	A	0.7783389091491699	Two, the theory of Hilbertian subspaces and Stein operators provides a general methodology to obtain robust estimators and three, preserving the information geometry of decision making yields adaptive agents that perform active inference throughout.
597704	611200	A	0.5150153636932373	We emphasize the rich connections between these field e G inference draws on sampling and optimization, and adaptive decision making assesses decisions by inferring their counterfactual consequences.
612100	622848	A	0.8112613558769226	Our exposition provides a conceptual overview of underlying ideas rather than a technical discussion, which can be found in the references herein.
622944	626890	A	0.6476091146469116	And indeed, there are several hundred references in this paper.
627420	629092	A	0.7384133338928223	Let's go to the roadmap.
629236	632664	A	0.5786892175674438	So first we can start on the right side.
632862	646380	A	0.8413063287734985	Here's an active agent room doing accelerated optimization in the carpool lane, looking ahead to the almost desert like visual representation of active inference.
646960	648670	A	0.7544951438903809	That's where we're going to go.
649040	652268	A	0.6531911492347717	And on the right side, it's just some cars.
652364	654960	A	0.7031069993972778	One of them is accelerated, the other one isn't.
655460	678356	A	0.8447896838188171	The paper begins with an introduction and then in section two gets into the topic of accelerated optimization covering the areas of principle of geometric integration, conservative flows and symplectic integrators rate matching, integrators for smooth optimization, manifold and constrained optimization, gradient flow as a high friction limit, and optimization on the space of probability measures.
678548	686228	A	0.900107741355896	In section three, they turn from accelerated optimization in general to Hamiltonian based accelerated sampling.
686404	692940	A	0.8251597881317139	The subsections of three are optimizing diffusion processes for sampling and Hamiltonian Monte Carlo.
693680	701772	A	0.8902466297149658	From the Hamiltonian based accelerated sampling, they turn towards doing statistical inference with kernel based discrepancies.
701916	726788	A	0.9096408486366272	In section four, the subsections are topological methods for Mmds smooth measures and KSDS the canonical Stein operator and point Curray duality kernel Stein discrepancies and score matching information geometry of Mmds and natural gradient descent minimum Stein discrepancy estimators likelihood coherence with generative models.
726964	735480	A	0.6577665209770203	Finally, in section five adaptive agents through active coherence, they're going to bring it home to active inference.
735900	747150	A	0.871039092540741	Section five one modeling adaptive decision making, behavior agents and environments, decision making in precise agents, the information geometry of decision making.
747680	760400	A	0.8548308610916138	And five two realizing adaptive agents, the basic active inference algorithm sequential decision making under uncertainty world model learning as inference and scaling active inference.
761700	770208	A	0.890861451625824	A quick note we have all the formalisms entered Hinton our zero teams Coda document.
770384	776900	A	0.9078354239463806	There are 46 numbered equations in this paper and 83 overall.
777480	783048	A	0.8631837368011475	We're going to bring in some, but not all of these formalisms in the dot zero video.
783214	792540	A	0.6052131056785583	And we're ready to bring in more during the dot one one and dot two discussions with the authors just for those who are watching along.
792690	796776	A	0.8808464407920837	I'm going to scan through all of the formalisms from our Coda.
796888	801572	A	0.6955700516700745	And again, for those who want to help get involved with a dot zero preparation.
801736	805504	A	0.6600922346115112	This is a little bit of what dot zero preparation looks like.
805702	812450	A	0.8513315916061401	I'm going to now scroll through the formalisms just in case anybody wants to screenshot or see them.
841260	842010	A	0.7671424746513367	Great.
843260	845480	A	0.7714195847511292	On to the keywords.
846880	866770	A	0.8446832299232483	Keywords of the paper are information geometry, hamiltonian Monte Carlo, Stein's method, reproducing kernel variational, coherence, accelerated optimization, dissipative systems, decision theory and active inference in no particular pedagogical order.
868260	873904	A	0.880441427230835	Let's start with decision theory from the Stanford Encyclopedia of Philosophy.
874032	889400	A	0.7762974500656128	The article begins Decision theory is concerned with the reasoning underlying an agent's choices, whether this is a mundane choice between taking the bus or getting a taxi or a more far reaching choice about whether to pursue a demanding political career.
889740	895700	A	0.8440317511558533	Decision theory is about agents making decisions, cognitive things making decisions.
895860	905260	A	0.8639771938323975	And we can take a peek ahead to figure three, which is shown here as well as invoke some of the keywords that we're about to walk into now.
905410	912384	A	0.974692165851593	So if this is your first time hearing some of this vocabulary or you're super familiar, either way, it's great.
912582	942600	A	0.7810541987419128	In this paper and in this discussion, we're going to see how different decision theoretic settings such as no ambiguity, no ambiguity or no coherence, no extrinsic value and no intrinsic value are going to be operationalized within an information theoretic formalism an information geometric formalism and specifically one that we're able to do accelerated inference on using variational methods.
942940	945460	A	0.8624376058578491	We're going to be involving action as a parameter.
945540	954508	A	0.6115015149116516	So we can call that active coherence and that's how we're approaching decision theory and getting from here to there with all those fun stops in between.
954674	957340	A	0.8284633159637451	Let's go to information theory.
958480	977520	A	0.8645182251930237	The paper in the introduction writes of particular relevance to this chapter is information geometry, E g, the differential geometric treatment of smooth statistical manifolds whose origin stems from a seminal article by Rao 23, who introduced the Fishermetric tensor.
977600	991108	A	0.8710211515426636	On parameterized statistical models and thus a natural domain geometry that was later observed to correspond to an infinitesimal disturbance with respect to the Kolback libelr or Kale divergence.
991284	995096	A	0.9103456139564514	So going into citation 23 and 24.
995278	999112	A	0.8175392150878906	Citation 23 is Rao from 1992.
999246	1012960	A	0.9072104692459106	And Rao wrote the objective of the paper is to derive certain inequality relations connecting the elements of the information matrix as defined by Fisher 1921 and the variances and covariances of the estimating functions.
1013380	1021036	A	0.9196079969406128	A class of distribution functions which admit estimation of parameters with the minimum possible variance has been discussed.
1021228	1030624	A	0.9121456742286682	The concept of distance between populations of a given type has been developed, starting from a quadratic differential metric defining the element of length.
1030752	1033460	A	0.8201649785041809	So I'm only going to give the disclaimer one time.
1033530	1037216	A	0.7310649156570435	Everything in red text is beyond speculative.
1037328	1043400	A	0.7744594812393188	It's just priming the pump for discussions with those who know more and with those who know less.
1043550	1047284	A	0.49945008754730225	And it's just a first pass that we're going to continue to develop on.
1047422	1061548	A	0.6781622767448425	But broadly, if we can compute inequality relationships, which this paper developed in 1992, we can bound estimators, test for relative improvements and basically do stuff with those distinctions.
1061724	1086896	A	0.679076611995697	And if we can compute certain kinds of statistical distances such that a distance requires a length metric, we can find optimal estimators of parameters, specifically their variances and covariance structure in principle and in practice by finding that maximum informational alignment to other estimators or empirical data.
1087018	1093464	A	0.7328474521636963	So we'll often say, like if you had the radial parameterisation, you'd be predicting as well as you could.
1093582	1108952	A	0.79735267162323	And so we want to be able to operationalize that using distances and metrics and citation 24 this is an invariant form for the prior probability and estimation problems by Harold Jeffries.
1109096	1124060	A	0.8788110017776489	And Jefferies wrote It has shown that a certain differentiate form depending on the values of the parameters in a law of chance, is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters.
1124220	1134500	A	0.8375006914138794	For laws containing a location in the scale parameter, a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters.
1135000	1142744	A	0.8758249282836914	This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.
1142942	1157784	A	0.8666449785232544	So if all the parameters in our model, whether there's one, two or more, are differentiable, which is to say they're smooth, et cetera, et cetera, there are technical details, then there are certain transformational invariances.
1157912	1165568	A	0.8712974786758423	So just like every Gaussian distribution, by shifting it and stretching and shrinking it, you can map those on to each other.
1165734	1166930	A	0.5705305337905884	It's like that.
1167860	1172024	A	0.7360647320747375	It turns out that some of those constraints can even be somewhat relaxed.
1172172	1178496	A	0.7501072287559509	For example, this approach may still be effective even where parameters are not differentiable everywhere.
1178688	1197870	A	0.5600526332855225	And this is big for helping us do Bayesian estimation, which can be seen as a special case of informational transformations or of manipulations of statistical distributions in the information theoretic or information geometric sense.
1199440	1201144	A	0.641684889793396	Functional analysis.
1201272	1213180	A	0.638985812664032	So let's begin as tradition in our year 2023 with a brief quote from Chat GPT how is functional analysis helpful for us who are learning and applying active inference?
1213260	1215260	A	0.8130884766578674	I'll just read the last section.
1215420	1225350	A	0.8764399886131287	Overall, functional analysis provides a powerful mathematical toolset for understanding and analyzing the principles active coherence lab, and one can read more.
1225960	1230608	A	0.8203682899475098	Functional analysis is the study of functions.
1230784	1250296	A	0.8917688131332397	And here we see one way of showing what a function does in terms of being this box f of x that takes in an input, an argument x and outputs a result y and also that can be understood as a mapping between or amongst spaces.
1250488	1252364	A	0.7704026699066162	And there's a lot more to go into.
1252482	1269120	A	0.7856279015541077	Just wanted to bring up that in math, sometimes people have analyzed functions and their properties and we're interested in the properties of a special class of functions, which are probability distributions, well behaved probability distinctions.
1270740	1273744	A	0.8427939414978027	What do we do with those probability distinctions?
1273792	1276832	A	0.8601812720298767	Well, one thing we might want to do is sample.
1276976	1284900	A	0.5810600519180298	So the paper writes sampling methods are critical to the efficient implementations of many methodologies.
1285580	1295130	A	0.8969240784645081	Most Modern samplers are based on Markov chain Monte Carlo methods, which include slice samplers, piecewise deterministic, Markov chains and so on.
1296060	1308380	A	0.8998833298683167	The original Hamiltonian Monte Carlo or HMC, which we're going to get to algorithm, was introduced in physics to sample distributions on gauge groups for lattice quantum chromodynamics.
1308980	1318640	A	0.8401929140090942	It combined two approaches that emerged in previous decades, namely the Metropolis Hastings algorithm and the Hamiltonian correlation of molecular dynamics.
1319140	1335492	A	0.9043008089065552	So the way that they state it in the beginning of section three of the paper is the purpose of sampling methods is to efficiently draw samples from a given target distribution row or more commonly, to calculate expectations with respect to row.
1335636	1361100	A	0.8191958665847778	By equation 33 and other, they write, Modern Hamiltonian Monte Carlo which again we're going to get to in a second relies heavily on symplectic integrators another keyword to simulate a deterministic dynamics responsible for generative distant moves between samples and thus reduce their correlation while at the same time preserving important geometric properties.
1361540	1366320	A	0.6610590815544128	Now, we want to make our samples as informative as possible.
1366470	1372980	A	0.6356885433197021	If we're just foraging samples and it's the same sample again and again, one can imagine that it is uninformative.
1374040	1390068	A	0.6862426400184631	Conversely, one can imagine a situation where samples are informative and that is brought into practice by making sure that the samples are minimally correlation with each other, as noted above.
1390244	1393700	A	0.86476731300354	And this also has analogy in cryptography.
1393860	1402488	A	0.4887022376060486	So if we're sampling something where the successive samples are 99% correlated, you can imagine that you're over sampling.
1402584	1408780	A	0.6801674365997314	So you're taking 10,000 frames per second of a YouTube video with 30 frames per second.
1408850	1412130	A	0.8593308925628662	I think I'm streaming at 25 frames per second right now.
1412740	1423270	A	0.5939750075340271	Conversely, if one were to sample every ten minutes, then their correlation structure would also not capture the videos 25 frames per second.
1423720	1442532	A	0.6649898886680603	So in order to generate good and large moves in the space, we would need perfect knowledge of what the space is or how it's generated, which is sometimes knowable in cryptography, however empirically, we need to use heuristics and statistical approximations.
1442676	1448476	A	0.9046734571456909	So this is the applied statistics angle on the analytical relationships that we're going to be talking about.
1448578	1460480	A	0.801983654499054	And it's going to come up again and again this tension between analytical formulations, what's true in principle, and the statistical or numerical or computational applications.
1461060	1484884	A	0.8120678663253784	Two Hamiltonian Monte Carlo from Wikipedia the Hamiltonian Monte Carlo algorithm, originally known as hybrid Monte Carlo still HMC is a Markov chain Monte Carlo method for obtaining a sequence of random samples which converge to being distributed according to a target probability distribution, for which direct sampling is difficult.
1485082	1488548	A	0.8543833494186401	So just a few first pass reflections.
1488724	1491396	A	0.8676759004592896	We're sampling, which is to say Monte Carlo.
1491508	1503528	A	0.6515317559242249	And Monte Carlo is harkening back to a different time when gambling was done in Monte Carlo and the state space of all the hands of poker were too vast to compute analytically or equations were not known.
1503624	1518476	A	0.8166112303733826	And so what one used to do, and still does, is draw samples and say, well, I don't know if this is the final estimate on how likely a royal flush is, but from the 2 billion hands that I sampled, there were two royal flushes.
1518508	1521760	A	0.820712149143219	And so it's one in a billion based upon my sample.
1522360	1532736	A	0.7425366640090942	So we're sampling because this is a complex distinctions for which an analytical solution is not possible, not known, not relevant, or not trackable computationally.
1532848	1537800	A	0.7058498859405518	So there's various situations where we want to sample and we don't necessarily have an analytical solution.
1538380	1550424	A	0.6502034068107605	When we're sampling from a converged stationarity, hashtag live stream 26 of a statistical landscape from a numerical perspective, we understand that landscape well enough to do coherence.
1550552	1560480	A	0.9033252596855164	So imagine that we're sampling height estimates from GPS locations in the X and the y coordinates, and we're getting a z value in our sample.
1560900	1566400	A	0.7307870388031006	At some point we can say we're unsurprised by new samples.
1566740	1571244	A	0.7717429399490356	At that point we understand the landscape well enough to do statistical inference.
1571292	1583216	A	0.5377817749977112	And just like the frames per second in the previous example, one can imagine if the landscape is changing on the spatial scale of 1 mile and you're sampling every millimeter, you might be over sampling.
1583328	1589316	A	0.5669299364089966	If you were sampling every league, maybe you'd be too coarse grained with your samples.
1589508	1595896	A	0.6155024170875549	And so one reason that comes up in science all the time for using sampling is that the search space might be just too large.
1596078	1601592	A	0.7944028377532959	For example, the combinatorics of a phylogenetic tree with a thousand species might be vast.
1601736	1615490	A	0.6554105877876282	And so it might be better to say something like, well, 99% of the million samples we drew were consistent with X, because an analytical solution or a brute force search, neither might be possible.
1616340	1624020	A	0.8980912566184998	And the Wikipedia wrote this sequence of samples can be used to estimate integrals with respect to the target distribution.
1624360	1633620	A	0.8426055312156677	So sometimes we're sampling not from the target distribution itself, but rather from a distribution that is related or transformed from the target distribution.
1633960	1640708	A	0.6515303254127502	This is because sampling directly from the target distribution might be difficult or less than perfectly informative.
1640884	1653100	A	0.8409138917922974	So we might, for example, hint hint sample from the derivative of the target which might help us identify points where, for example, the derivative is flat in all directions.
1654160	1661120	A	0.8801260590553284	In the case that we're sampling from a derived landscape of the target distribution, this is called a symplectic integrator.
1661540	1664850	A	0.8900050520896912	So let's look at what a symplectic integrator is.
1665380	1670188	A	0.8423934578895569	So first, backing up to what is differentiate geometry.
1670364	1678020	A	0.8412448167800903	Differential geometry is a mathematical discipline that studies the geometry of smooth shapes and smooth spaces, otherwise known as smooth manifolds.
1679240	1692090	A	0.8570305109024048	Symplectic geometry is a branch of differential geometry and differential topology that studies Symplectic manifolds, that is, differentiable manifolds equipped with a closed, nondegenerate two form.
1692860	1704776	A	0.8782587647438049	Symplectic geometry has its origins in the Hamiltonian formulation of classical mechanics, where the phase space of certain classical systems takes on the structure of a symplectic manifold.
1704888	1706988	A	0.8189588785171509	So the keyword density is high.
1707074	1711552	A	0.5397469401359558	These topics are all very closely linked from the paper.
1711686	1723380	A	0.6114362478256226	It has been known for a long time that the class of symplectic integrators is the preferred choice for simulating physical systems, only the finest for our simulations of physical systems.
1724440	1737640	A	0.8671878576278687	These discretization techniques are designed to preserve the underlying symplectic geometry of Hamiltonian systems and they also form the basis of Hamiltonian Monte Carlo or hybrid Monte Carlo methods.
1737980	1748500	A	0.5863755941390991	We're going to come back to this tension again and again, which is what good is an in principle, smooth and differentiable analytical correlation.
1748660	1763888	A	0.8344907164573669	If our discreditization scheme, the way that we actually implement the steps on a computer, are inferior, we might ruin all those nice properties that we work to get about the analytical form.
1764054	1769200	A	0.7656124830245972	We might just throw those out when we discretize it coarsely or inappropriately.
1769700	1771768	A	0.8724719882011414	So what is the symplectic integrator?
1771884	1775300	A	0.8635299801826477	It's a numerical integration scheme for Hamiltonian systems.
1775640	1782336	A	0.8766894936561584	Symplectic integrators form the subclass of geometric integrators, which by definition are canonical transformations.
1782528	1784710	A	0.8078466057777405	They're transformations we know a lot about.
1787260	1803432	A	0.8777090907096863	Symplectic integrator schemes are referring to, again, just broadly, both the analytical formulations and the software packages and approaches that we can use to implement those formalisms.
1803576	1821840	A	0.7026914358139038	And these integration schemes are useful across difficult estimation problems, which is why they're used to study nonlinear dynamics, molecular dynamics like protein simulation, discrete element methods, accelerator physics, plasma physics, quantum physics, celestial mechanics.
1823160	1832390	A	0.8058655858039856	The time evolution of Hamilton's equation is a symplectomorphism, meaning that it conserves the symplectic two form.
1832920	1838570	A	0.7120463848114014	A numerical scheme is a symplectic integrator if it also conserves this two form.
1840620	1844200	A	0.5655643939971924	Let's hear more if this is not correct or not complete.
1844350	1848876	A	0.800067663192749	But the one form is like the differentiation and integration of a line.
1849058	1853464	A	0.7988080978393555	The two form is like the differentiation and integration of a surface.
1853512	1858060	A	0.8210445642471313	And the three form is like integration or differentiate of a volume.
1858400	1864400	A	0.9583185911178589	And John Denker has a great blog post on the basic properties of a symplectic integrator.
1864980	1872944	A	0.9744918346405029	Some really interesting quotes that are good to just keep in mind when we're hearing about all these avenues we're going to be exploring in the paper.
1873142	1879220	A	0.7948351502418518	A symplectic integrator can serve the area in phase space delimited by an ensemble of systems.
1879640	1886630	A	0.808835506439209	For a periodic system, there is an area that is conserved, namely the area inside the phase space orbit of the system.
1887240	1893912	A	0.8329622745513916	The main reason for mentioning the orbit is to make the point that there are lots of different things with dimensions of area in phase space.
1894046	1896170	A	0.7443467974662781	Some are conserved and some not.
1896640	1898908	A	0.5318585634231567	Some are interesting and some not.
1899074	1903452	A	0.8321061134338379	You have to specify which sort of area you are talking about.
1903586	1918972	A	0.5657117366790771	So another thing that's going to arise is we're interested in very well behaved outcomes from a very specific or constrained, still broad, but definitely constrained set of distributions.
1919036	1926980	A	0.8613389730453491	For example, distributions that can be interpreted in an information theory sense as probability distinctions.
1928200	1932736	A	0.9143950343132019	What was that mention of the Hamiltonian and the shadow Hamiltonian?
1932928	1934890	A	0.5440241694450378	So the author is right.
1936300	1947548	A	0.8187696933746338	For symplectic brackets, which is the kind of operation that reflects the symplectic integrator, the existence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, e.
1947554	1955528	A	0.8960765600204468	G for variational integrators, which use variational inference, which use a discrete version of Hamilton's principle of least action.
1955624	1973990	A	0.8527477979660034	And we've heard it before that free energy principle is a principle of least action for inference and action and more generally for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by some technicalities that we can learn about.
1974600	1986760	A	0.8493085503578186	As we shall see, such geometric integrators can be constructed by leveraging the shadow Hamiltonian property of symplectic methods on higher dimensional conservative Hamiltonian systems.
1988300	1998830	A	0.7906982898712158	In short, as a consequence of having shadow Hamiltonian, such geometric integrators are able to reproduce all the relevant properties of the continuum system.
1999520	2001692	A	0.5251971483230591	These arguments are completely general.
2001826	2032150	A	0.530976414680481	So even before one knows what the shadow Hamiltonian necessarily is, the authors are letting us know that if we can discretize the shadow Hamiltonian appropriately through symplectic integration, we can provide a inclination scheme that ends up staying consistent and compatible and all down the middle with the analytical properties that we've worked so hard to get.
2033240	2039050	A	0.9015218019485474	And there were some different papers that were cited and some that were found during research.
2039900	2051848	A	0.6597875952720642	This paper Time Step in Shadow Hamiltonian and Molecular Dynamics Simulations by Kim demonstrates symplectic integrators on the simplest possible system a simple harmonic oscillator.
2051944	2058476	A	0.8836129903793335	So if one wants to go to a technical example, but one that builds intuition, that's a great place to start.
2058658	2066704	A	0.544718325138092	And from the Wikipedia article on energy drift, these integrators do not in fact, reproduce the actual Hamiltonian mechanics of the system.
2066822	2075060	A	0.7656989693641663	Instead, they reproduce a closely related shadow Hamiltonian whose value they conserved many orders of magnitude more closely.
2075640	2081732	A	0.8862953186035156	The accuracy of the energy conversation for the true Hamiltonian is independent on the time step.
2081866	2111020	A	0.8258093595504761	So if we can, for examples, just metaphorically speaking, capture a high evolution image of the shadow, then we'll be able to know something, do something with the actual some technical details on Stein's method, which is a general method in probability theory to obtain bounds on the distance between two probability distributions with respect to a probability metric.
2111180	2120400	A	0.8860098123550415	And Stein's method is used in the context of Stein operators and Stein class, which we're going to get to in the subsequent discussions.
2122020	2129440	A	0.882502019405365	Reproducing Kernel our discussion of inference builds upon the theory of Hilbertian subspaces and in particular, reproducing kernels.
2129520	2137348	A	0.8696836829185486	These inference schemes rely on the continuity of linear functionals such as probability and Schwartz distributions over a class of functions.
2137364	2155288	A	0.7769777774810791	To geometrize the analysis of integral probability metrics, which measure the worst case integration error, we shall explain how maximum mean kernelized and score matching discrepancies arise naturally from topological considerations.
2155464	2161872	A	0.8437258005142212	So we've added some background links here to the slide, but it'll be a Brea place to start with.
2161926	2167632	A	0.874272882938385	Authors, what is a reproducing kernel and how is it used?
2167686	2174864	A	0.7106773257255554	In this paper onwards, through the keywords we go accelerated optimization and variational inference.
2174992	2182704	A	0.9175070524215698	So here's a 2016 paper from Michael Jordan and other authors and some quotes from the abstract.
2182832	2190548	A	0.800861656665802	Accelerated methods achieve faster coherence rates than gradient methods, and indeed, under certain conditions they achieve optimal rates.
2190724	2196200	A	0.712442934513092	However, accelerated methods are not dissent methods and remain a conceptual mystery.
2196560	2201848	A	0.7311993837356567	We propose a variational continuous time framework for understanding accelerated methods.
2202024	2209810	A	0.6777544021606445	We provide a systematic methodology for converting accelerated higher order methods from continuous time to discrete time.
2210260	2216800	A	0.8584772348403931	Our work illuminates a class of dynamics that may be useful for designing better algorithms for optimization.
2217300	2221090	A	0.975996732711792	And this is going to be, again, something awesome to discuss.
2222200	2231360	A	0.8586246371269226	We've talked about, on many streams, variational inference in the context of gradient based optimization.
2231520	2244900	A	0.8607766628265381	For example, the ball is rolling to the bottom of the bowl, bottom of the hill, and what you can do is you can take the gradient div, grab curl and all that of where the ball is and just go downhill.
2244980	2258428	A	0.49811893701553345	And if you designed the hill to be the right shape, or you're using a chosen family of variational estimators that are the right shape, are a good shape, well behaved shape, then you follow the gradient on down.
2258514	2261020	A	0.8642578721046448	And that's how we've talked about variational inference.
2261180	2266688	A	0.6329727172851562	Accelerated optimization is going to be accelerated from that.
2266774	2270690	A	0.9694949984550476	And so it's fun to think about and it'll be great to talk about.
2271960	2273152	A	0.5987140536308289	Dissipative systems.
2273216	2279270	A	0.5293534994125366	The vast majority of statistics and machine learning applications involve solving optimization problems.
2279960	2281268	A	0.5659719109535217	The author is right.
2281434	2287720	A	0.8378819823265076	Accelerated gradientbased methods and several variations thereof have become workhorses in these Fields.
2288060	2293080	A	0.8092365264892578	Recently, there has been great interest in studying such methods from a continuous time limiting perspective.
2293820	2299660	A	0.9091708064079285	Such methods can be seen as first order integrators to a classical Hamiltonian system with dissipation.
2300080	2312640	A	0.7445476055145264	This raises the question on how to discretize the system such that important properties are preserved, assuming the system has fast conversions to critical point and desirable stability properties.
2313540	2317856	A	0.8846654295921326	Originally, such a theory of geometric integration was developed with conservative systems in.
2317878	2324916	A	0.7927361130714417	Mind, while optimization in optimization, the associated system is naturally a dissipative one.
2325098	2338040	A	0.7012775540351868	More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of coherence and stability, which are the main properties of interest for optimization.
2338620	2347092	A	0.9034621119499207	So from the Wikipedia on Hamiltonian systems an example of a time independent Hamiltonian system is the harmonic oscillator.
2347236	2357608	A	0.7709047794342041	So it's just a frictionless spring oscillating around the Hamiltonian of the system does not depend on time, and thus the energy of the system is conserved.
2357704	2362480	A	0.8415319919586182	And so we can say that that is a conservative Hamiltonian.
2363060	2366960	A	0.8358664512634277	If the Hamiltonian decays in time, it is dissipative.
2368020	2386600	A	0.5062623620033264	Recently, the advances of these authors and others have extended our ability to make distinctions of dissipative Hamiltonians that respect the rates of conversions and stability, which are the main properties of interest for optimization.
2387020	2395060	A	0.9214036464691162	And we've talked about Hamiltonians on Livestream number 49 with Dalton Sakthivadivel Devil on Bayesian mechanics.
2395220	2396970	A	0.9821175932884216	So pretty cool.
2397280	2398536	A	0.7916091084480286	What is conservative?
2398648	2399928	A	0.8228473663330078	What is dissipative?
2400104	2411696	A	0.8732076287269592	We'll explore as the final keyword here, I just wanted to start with a blank slide on active inference and see what happens in 52.1.
2411798	2420784	A	0.7843107581138611	So consider the slide to be blank and for the authors and discussants looking forward to hearing Hohwy.
2420822	2422688	A	0.9265146255493164	Are we approaching active inference?
2422784	2428596	A	0.8076294660568237	Given everything that we've just loaded onto the table and everywhere that we're about to go with the paper?
2428778	2430340	A	0.6736361980438232	Let's get into it.
2430490	2445748	A	0.8379482626914978	Section One introduction so, the authors begin with differential geometry plays a fundamental role in applied mathematics, statistics and computer science, including various domains.
2445844	2450220	A	0.896427571773529	Citations one through 22, which can be shown here.
2450370	2453550	A	0.7907721400260925	Various citations from this well cited paper.
2454720	2472384	A	0.8526437878608704	The geometric study of statistical models has had many successes, ranging from statistical inference, where it was used to prove the optimality of maximum likelihood estimator, to the construction of the category of mathematical statistics generated by Markov morphisms.
2472512	2476070	A	0.9186058044433594	So what are Markov morphisms from Ncat lab?
2476760	2486200	A	0.8960909247398376	The formalism of Markov categories can be thought of as a way to express certain aspects of probability and statistics synthetically.
2486860	2499820	A	0.7897394895553589	In other words, it consists of structure and axioms, which one can think of as fundamental in probability and statistics, which one can use to prove theorems without having to use measure theory directly.
2500640	2511410	A	0.8651196956634521	Intuitively, for the purposes of probability, a Markov category can be seen as a category where morphisms behave like random functions or Markov kernels, hence the name.
2512100	2515600	A	0.5066932439804077	So, just some red text speculation.
2515940	2530420	A	0.9562088251113892	It was a big success and a breakthrough to think about the category from a formal category theoretic perspective, to think about the category of statistical distinctions from an information geometric perspective.
2531240	2545480	A	0.7206912040710449	This means that we can understand many analytical properties and transformations of statistical distributions hashtag Bayesian and develop general methods that work for that whole category, like accelerated optimization.
2546560	2548460	A	0.7051510214805603	And the applications are vast.
2551360	2557324	A	0.8800686001777649	Originally, such a theory of geometric integration was developed with conservative systems in mind.
2557522	2562784	A	0.8292591571807861	While in optimization the associated system is naturally a dissipative one.
2562982	2566960	A	0.7288362979888916	Nevertheless, symplectic integrators were exploited in this concept.
2567860	2581824	A	0.697659969329834	More recently, it has been proved that a generalization of symplectic integrators to dissipative Hamiltonian systems is indeed able to preserve rates of convergence and stability, which are the main properties of interest for optimization.
2581952	2600510	A	0.7231168150901794	So, citation nine, what is it about 2021 paper on dissipative symplectic integration with applications to gradient based optimization by some of the authors, and it's interesting to pull out some quotes from the abstract and just hear a little bit about what they're up to.
2601200	2611928	A	0.9117743968963623	Recently, continuous time dynamical systems have proved useful in providing conceptual and quantitative insights into gradient based optimization widely used in Modern machine learning and statistics.
2612104	2619884	A	0.7929069399833679	An important question that arises in this line of work is how to discritize the system in such a way that its stability and rates of convergence are preserved.
2620012	2621740	A	0.7246671915054321	So this is the sampling problem.
2621910	2625190	A	0.8464133739471436	In continuum time, the math is continuous and nice.
2625560	2640180	A	0.7859396934509277	However, the sampling problem which actually comes into play when we use Modern computational methods that are implemented on discrete time, space and constraints, what happens when we use unconventional computing?
2640340	2646890	A	0.69295334815979	That's an interesting discussion question, but in active today, the discretization approach is going to matter a lot.
2647580	2658140	A	0.5425600409507751	In this paper, we propose a geometric framework in which such discretizations can be realized, systematically enabling the derivation of ratematching algorithms without the need for a discrete coherence analysis.
2658560	2669500	A	0.5666816830635071	More specifically, we show that a generalization of symplectic integrators to nonconservative, and in particular, discipline of Hamiltonian systems is able to preserve rates of convergence up to a controlled errors.
2669660	2680660	A	0.6525027751922607	So this is the advance of this paper that within a controlled error good enough, they can model conservative and nonconservative, even dissipative Hamiltonian systems.
2681480	2697796	A	0.8090696930885315	Moreover, such methods preserve a shadow Hamiltonian despite the absence of a conversation law extending key results of symplectic integrators to nonconservative cases, our arguments rely on a combination of backwards error analysis with fundamental results from symplectic geometry.
2697988	2715756	A	0.5563117265701294	We stress that although the original motivation for this work was the application of optimization, where dissipative systems play a natural role, they are fully general and not only provide a differential geometric analysis for discipline of Hamiltonian systems, but also substantially extend the theory of structure preserving inclination.
2715948	2718640	A	0.5638359189033508	So they made this generalization.
2719060	2726660	A	0.9536793231964111	In the case of optimization, however, it's exciting that it extends even deeper into the math and the symmetry.
2727560	2732260	A	0.8798038959503174	Those are a few highlights from section one on to section two.
2732330	2734944	A	0.7405617237091064	Accelerated optimization vroom, vroom.
2734992	2735910	A	0.5884679555892944	Here we go.
2736860	2747604	A	0.5923957228660583	So there are going to be a lot of slides from sections two through five, and to keep the video a reasonable length, I'm going to just highlight a few pieces.
2747652	2754430	A	0.5764709711074829	Not even necessarily what I intended to highlight, but I'm just going to go for it so we can get through it.
2755040	2777220	A	0.8394750356674194	Section two we shall be concerned with a problem of optimization of a function finding a point that maximizes V of Q or minimizes negative V of Q over a smooth manifold M r in the real numbers, many algorithms and optimization are given as a sequence of active inference.
2778280	2785940	A	0.8809387683868408	Even when these algorithms are seen as distinctions of a continuum system whose behavior is presumably understood.
2786100	2792232	A	0.48509231209754944	It is well known that most discretizations break important properties of a system, and they continue to write.
2792366	2805390	A	0.6159731149673462	The analysis of such finite difference iterations is usually challenging, relying on painstaking algebra to obtain theoretical guarantees such as conversation to a critical point, stability and rates of conversation to a critical point.
2806960	2818144	A	0.5502346158027649	Even when these algorithms are seen as discreditizations of a continuum system whose behavior is presumably understood, it is well known that most discreditizations break important properties of the system.
2818342	2829200	A	0.5173738598823547	It can't be highlighted enough when we implement optimization algorithms on Modern computers, we have to make discreditizations in space and time and in practice.
2829360	2849260	A	0.5481362342834473	So even if the analytical properties of the distribution are totally fire, when we try to solve and fit models to empirical data, unless we also match that elegance and power with a inclination approach, we end up failing to realize analytical promises.
2851360	2852268	A	0.5673735737800598	2.1.
2852354	2860240	A	0.5231844782829285	The principle of Geometric Integration fortunately, the author's right here comes into play one of the most fundamental ideas of geometric integrations.
2860660	2868050	A	0.699891209602356	Many numerical integrators are very close exponentially in the step size to a smooth dynamics generated by a shadow vector field.
2869460	2876390	A	0.8884015083312988	A little whisper of a shadow Hamiltonian and the shadow vector field is a perturbation of the original vector field.
2876840	2888916	A	0.5070736408233643	This allows us to analyze the discrete trajectory implemented by the algorithm using powerful tools from dynamical systems and differential geometry, which are a priori reserved to smooth systems.
2889028	2895050	A	0.720697283744812	So we're going to be able to discretize our cake and have it be smooth too.
2895740	2907640	A	0.7680606842041016	Crucially, while numerical integrators typically diverge significantly from the dynamics they aim to simulate, geometric integrators respect the main properties of a system in the context of optimization.
2907720	2916210	A	0.5892267823219299	This means respecting stability and rates of coherence seems like a good idea to respect the main properties of the system.
2916980	2920672	A	0.7887629270553589	This was first demonstrated in nine and further extended in ten.
2920806	2923236	A	0.8482595086097717	Our discussion will be based on these work.
2923338	2956588	A	0.7063824534416199	So citation Nine, Franca, Jordan and Vidal mentioned previously and citation ten, Franca, Mark, Girolami and Jordan optimization on manifolds a symplectic approach authors previous work So big development we can use geometric rather than course numerical integrators, so our sampling based optimization schemes, including their discordization, respect the main properties of the system.
2956754	2957870	A	0.9556306600570679	Sounds great.
2958320	2967920	A	0.8771309852600098	Section Two Two conservative flows and symplectic integrators as a stepping stone, we first discussed the construction of suitable conservative flows.
2970260	2973940	A	0.9186818599700928	These are very well studied and they're very intuitive.
2974520	2975940	A	0.6370380520820618	More intuitive.
2976840	2990084	A	0.8867394328117371	To construct vector Fields along the derivative of x, which is the function of flows along which some function is constant, we shall need brackets geometrically.
2990132	2996512	A	0.8887082934379578	These are morphisms x star to x, also known as contravariant tensors of rank two in physics.
2996676	3010220	A	0.5426695942878723	So calling back the two form and the brackets, importantly, vector Fields that preserve f correspond to bracket vector Fields in which B is antisemitic.
3010980	3014316	A	0.7394798994064331	Constructing conservative flows is thus straightforward.
3014508	3020690	A	0.7107601761817932	Unfortunately, it is a rather more challenging task to construct efficient discretizations that retain this property.
3021140	3030260	A	0.7624456882476807	Most well known procedures, namely discrete gradient and projection methods, only give rise to integrators that require solving implicit equations at every step.
3030410	3036532	A	0.633985161781311	And they may break other important properties of the system inclination.
3036596	3040600	A	0.7552419304847717	96 this is the issue again.
3040750	3053992	A	0.5722859501838684	No matter how nicely behaved in principle, our analytical underlying smooth differentiable function is if we discretize it and we chop it up in a way that's inappropriate.
3054136	3060720	A	0.8488212823867798	We don't respect the properties of the system, we don't end up with being able to realize those analytical promises.
3062340	3078660	A	0.8859093189239502	Indeed, in practice, the Hamiltonian usually decomposes into a potential energy associated to position and independent of momentum, and a kinetic energy associated to momentum and invariant under position changes, both generating tractable flows.
3079320	3085530	A	0.6011777520179749	Thanks to this decomposition, we are able to construct numerical methods through splitting the vector field.
3085900	3099660	A	0.6994052529335022	A few ideas coming together here, recalling some of our generalized coordinate based approaches to non equilibrium steady states and Bayesian mechanics, and also decomposition of complex functions.
3100400	3108264	A	0.8265244960784912	Note also that for symplectic brackets, the evidence of a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, eg.
3108312	3124896	A	0.9033549427986145	For variational integrators, which use a discrete version of Hamiltonians Hamilton's principle of Least action and for most symplectic integrators in which the symplectic bracket is preserved up to topological considerations described by the first Dean cohomology of FaceBase.
3125008	3131184	A	0.9246959090232849	So for discussion with authors annual what are the splitting integrators?
3131312	3133590	A	0.7259977459907532	What is split from what and why?
3134060	3137188	A	0.8879172801971436	What is the bracket notation or operation?
3137364	3140004	A	0.7800754308700562	And what is a piss on bracket?
3140052	3141930	A	0.8905141949653625	What is a piss on system?
3142940	3156140	A	0.5711818337440491	Section 23 rate matching integrators for smooth Optimization so, having obtained a vast family of smooth dynamics and integrators that closely preserve F, we can now apply these ideas to optimization.
3157280	3160510	A	0.7115381956100464	What is being set up in this section and why?
3160820	3177284	A	0.8971152305603027	In equation seven we see that the damping coefficient gamma of t being greater than zero reflects the dissipative component, or the second term in that right hand side of equation seven.
3177402	3181520	A	0.800626814365387	So damping coefficient controls the strength of the dissipation.
3181680	3191656	A	0.7414652705192566	One can imagine that if the damping coefficient is zero, the dissipative side zeros out and b has conservative behavior and so on.
3191758	3194970	A	0.7107829451560974	So what is being set up here and why?
3195580	3208716	A	0.8862902522087097	Dot dot dot dot dot the existence of such a Leopold function described above implies that trajectories starting in the neighborhood of Q star will converge to Q star.
3208898	3213490	A	0.6325544118881226	So it's like a ball rolling to the bottom of a hill, just like we've always wanted.
3214100	3224776	A	0.5794607400894165	In other words, the above system provably solves the optimization problem minimum of V on Q, such that Q is in the D dimension.
3224908	3227376	A	0.6180870532989502	Real punchline.
3227568	3248060	A	0.6373393535614014	We're setting up a system with good smooth optimization characteristics ball rolling to the bottom of a smooth hill that is also on the path or using the notation of or prepared to make the transformation to attractable discretization approach.
3248480	3251340	A	0.8983675837516785	So some more on the damping coefficient.
3251760	3256060	A	0.7656577825546265	They bring up some common choices for the damping coefficient.
3256400	3261852	A	0.8627082109451294	And Big O notation describes on the order of which something occurs.
3261996	3270432	A	0.8409865498542786	For example, linearly order of the variable or sub or super linearly as a function of time or data points.
3270566	3281652	A	0.6038942933082581	So in computational complexity analysis, people are often interested as I double the amount of data I'm analyzing, does that make the algorithm take twice as long?
3281706	3284464	A	0.7223736643791199	That's linear computational complexity?
3284592	3286552	A	0.6907729506492615	Does it take four times as long?
3286606	3289096	A	0.7805364727973938	Does it take the same amount of time?
3289278	3290584	A	0.6936384439468384	And so on.
3290782	3308780	A	0.8145813941955566	And it'll be great to talk about the intuitions and implications and generalizations about how the damping coefficient influences the computational complexity estimates for convergence in different settings.
3311300	3317900	A	0.874448299407959	They write the conservative system from equation 16 reduces precisely the original dissipative system 13.
3318060	3327604	A	0.8925737738609314	The second equation in 16 reproduces 14, and the remaining equations are equivalent to the equations of motion associated to 13, which in turn are equivalent to eight.
3327642	3328980	A	0.8309777975082397	As previously noted.
3329640	3345050	A	0.8633530139923096	Formally, what we have done is to embed the original dissipative system with phase space r 2D, so real with 2D dimension into a higher dimension conservative system with phase space r 2D plus two.
3345420	3352300	A	0.8127238154411316	The dissipative dynamics thus lies on a hyper surface of constant energy k equals zero in high dimension.
3353120	3358780	A	0.8198486566543579	The reason for doing this procedure, called implication, is purely theoretical.
3359120	3359884	A	0.49538490176200867	Oh, come on.
3359922	3361520	A	0.7894716262817383	It's not purely theoretical.
3361940	3376660	A	0.807497501373291	Since the theory of symplectic integrators only accounts for conservative systems, we can now extend this theory to dissipative systems settings by applying a symplectic integrator to 13 and then fixing the relevant coordinates 17 in the resulting methods.
3377000	3380950	A	0.8265677690505981	Geometrically, this corresponds to integrating the time flow exactly.
3381560	3385924	A	0.9059092402458191	We're going to talk about a relationship between time and dissipative systems.
3386052	3390152	A	0.6801742315292358	After all, it's dissipative systems that are dissipating in time.
3390286	3398140	A	0.8294454216957092	And so discreditization of time plays a role in the appropriate inclination of a dissipative Hamiltonian.
3398640	3405912	A	0.9010108113288879	And in citation nine previously raised, such a procedure was defined under the name of presymplectic integrators.
3405976	3413260	A	0.8781073689460754	And these connections hold not only for the specific example above, but also for general non conservative Hamiltonian systems.
3413340	3416064	A	0.7707463502883911	So what is happening here?
3416262	3419548	A	0.8872758746147156	What's and intuition for conservative and dissipative systems?
3419644	3423590	A	0.9049625992774963	And how have the recent works of Franca at all expanded what is possible?
3424520	3430820	A	0.6385313868522644	We are now ready to explain why this approach is suitable to construct practical optimization methods.
3431640	3440920	A	0.8056113123893738	The coordinate t sub k becomes simply the time discreditization, which is exact, and so is U sub k, since it is a function of time alone.
3441260	3444660	A	0.7118346095085144	Importantly, U does not couple to any of the other degrees of freedom.
3444740	3456504	A	0.7281075119972229	So it is irrelevant whether we have access to U or not, because we're looking to solve a function of t 17.
3456552	3463680	A	0.9035360217094421	A can be substituted in 15 to get 18, and you can replace 13 to get 19.
3464500	3466640	A	0.8280870318412781	We'll talk more about it with the authors.
3469140	3471212	A	0.7827863097190857	Therefore, the known rates.
3471276	3481700	A	0.8289099931716919	Equation eleven for the Continuum system are nearly preserved, and so would be any rates of more general time dependent dissipative Hamiltonian systems.
3482600	3487450	A	0.864076554775238	Let us now present and explicit algorithm to solve the optimization problem.
3489340	3499950	A	0.8245928883552551	This is all happening as a consequence of having a shadow Hamiltonian such that geometric integrators are able to reproduce all the relevant properties of the Continuum system.
3501600	3512176	A	0.5515794157981873	Section Two Four manifold and Constrained Optimization following ten, we briefly mentioned how the previous approach can be extended in great generality to an optimization problem.
3512278	3520672	A	0.9120301604270935	So equation 21 we present our minimization problem V of Q variational distribution on Q.
3520806	3523170	A	0.8706411123275757	Citation ten Franca et al.
3523880	3528900	A	0.7873114943504333	They write There are essentially two ways to solve this problem through a dissipative Hamiltonian approach.
3529560	3538840	A	0.8957712054252625	One is to simulate a Hamiltonian dynamics on T star M by incorporating the metric of M in the kinetic moving part of the Hamiltonian.
3539180	3548620	A	0.9110515117645264	Another is to consider a Hamiltonian dynamics on RN and embed M into RN by imposing several constraints.
3549040	3552396	A	0.8826647996902466	The first approach uses a Lee group.
3552578	3554190	A	0.6957451701164246	We'll talk more about it.
3554960	3569200	A	0.8536472320556641	An example of a second approach one can constrain the integrator on RN to define a symplectic integrator on M via the discrete constrained variational approach by using some techniques.
3571480	3578416	A	0.891927182674408	The above method consists in a dissipative generalization of the well known rattle integrator from molecular dynamics.
3578608	3584260	A	0.9155709743499756	Citations 100 through 103 used in computational biology.
3585320	3595880	A	0.868397057056427	Section Two Five gradient Flow as a High Friction Limit let us provide some intuition why simulating second order systems is expected to field faster algorithms.
3596780	3598796	A	0.8532072305679321	As an illustration, consider figure one.
3598818	3606808	A	0.9146958589553833	On the left, we'll look at in a second where a particle immersed in a fluid falls under the influence of a potential force.
3606984	3610464	A	0.7036784291267395	Negative delta sub q on V in Q.
3610662	3621212	A	0.9013347625732422	So partial differential with respect to Q of V on Q that plays a role of gravity and is constrained to move on a surface.
3621356	3623724	A	0.8251715898513794	So ball rolling down the hill.
3623852	3625490	A	0.6887803673744202	We weren't joking about it.
3625860	3632932	A	0.7686742544174194	In the underdamp case, the particle is underwater, which is not so viscous, so it has acceleration and moves fast.
3633066	3634688	A	0.8340299129486084	It may even oscillate.
3634864	3648472	A	0.866470992565155	In the over damped case, the particle is in a highly viscous fluid such as honey, and the drag force that damping coefficient gamma is comparable or stronger to the gradient.
3648616	3653964	A	0.645177960395813	Thus the particle moves slowly since it cannot accelerate during the same elapsed time.
3654002	3658300	A	0.8653327822685242	Delta T, an accelerated particle would travel a longer disturbance.
3659860	3661250	A	0.7626291513442993	Here's figure one.
3661620	3667388	A	0.8342223763465881	So here y simulating second order systems yields accelerated methods.
3667404	3671520	A	0.8730430603027344	So on the left, constrained particle falling in fluids of differential viscosity.
3671880	3675860	A	0.7134193778038025	Here on the left, we have the particle falling through honey.
3676280	3683488	A	0.5911095142364502	It's slowly making its way to the bottom of the bowl, but it never really builds speed.
3683664	3698060	A	0.8408131003379822	Its terminal velocity is being dominated by the viscosity of the fluid, whereas this bowl falling through water or falling through air is with respect to the honey dampened bowl.
3698480	3708140	A	0.6835691928863525	This is like an accelerated optimization, and they present some numerical results that help us bolster that intuition.
3708960	3714748	A	0.9399247169494629	Really fun and embodied way to think about optimization.
3714924	3720332	A	0.9094557762145996	We've talked about that ball rolling to the bottom of the hill and what the ball does when it hits a small bump.
3720396	3730790	A	0.5363707542419434	But we have not talked about what media the Bull is floating through and the media is the message and the fish doesn't know what it's swimming through.
3732120	3736228	A	0.8633503317832947	Section 2.6 optimization in the space of probability measures.
3736324	3744970	A	0.931037425994873	It'll be great to explore more because we're going to see free energy calculations on the stationary density, KL divergence and more.
3745340	3758600	A	0.5386809706687927	For now, we can just say all those optimization techniques we were bringing up earlier, we're going to be able to do them on information geometric spaces that correspond to probability measures that are well behaved.
3758760	3764092	A	0.7674517035484314	Not all distinctions are probability measures or probability distributions.
3764156	3770268	A	0.7006698846817017	Just because you draw a line doesn't mean you can use that in part of your variational inference scheme.
3770284	3771760	A	0.8065522313117981	For Bayesian statistics.
3772660	3774050	A	0.8319486379623413	That's section two.
3774980	3778704	A	0.8875272274017334	On to section three Hamiltonian based accelerated sampling.
3778752	3787620	A	0.5587992072105408	So we talked about the Hamiltonian conservative and dissipative and the shadow Hamiltonian, which is going to be orders of magnitude better to approximate.
3787960	3795652	A	0.574991762638092	We talked about gradientbased methods and how accelerated sampling is going to help us accelerate those sampling techniques.
3795796	3800300	A	0.88340824842453	And now section three brings it together with Hamiltonian based accelerated sampling.
3800960	3811148	A	0.902817964553833	Section Three the purpose of sampling methods is to efficiently draw samples from a target distribution row, or more commonly, to calculate expectations with respect to row.
3811324	3813360	A	0.7637699842453003	And from the end of the paragraph.
3814340	3820732	A	0.7303157448768616	An efficient sampling scheme is one that minimizes the variance of the Monte Carlo Markov chain.
3820796	3832996	A	0.8062973618507385	Estimator Monte Carlo, that means that we're sampling hands from the poker table and Markov chain, which means that the past only influences the future through the present.
3833098	3835370	A	0.5320269465446472	It's a quote memoryless process.
3836780	3840580	A	0.6511567831039429	In other words, fewer samples will be needed to obtain a good estimate.
3840660	3846360	A	0.7155120968818665	Intuitively good samplers are Markov chains that converge as fast as possible to the target distribution.
3846720	3850616	A	0.6134327054023743	It's like if you laid down a jump rope on a mountain.
3850808	3860556	A	0.8458353281021118	If that jump rope converged quickly to the topography of the mountain, it would have been a fast converging jump rope.
3860668	3864720	A	0.8685333728790283	And we're doing something like that, but with sampling from the jump rope.
3865700	3866512	A	0.5812974572181702	3.1.
3866566	3875940	A	0.8806551098823547	Optimizing diffusion processes for sampling as many MCMC methods are based on discretizing continuous time stochastic processes.
3876360	3892680	A	0.5946487188339233	The analysis of continuous time processes is informative of the properties of efficient samplers and diffusion processes possess a rich geometric theory extending that of vector Fields and have been widely studied in the context of sampling.
3893500	3896200	A	0.9624818563461304	Lot of very fascinating ideas here.
3896350	3900670	A	0.8923110365867615	Stratinovich stochastic differential equations are going to come into play.
3906200	3911140	A	0.9200727939605713	Equation 34 more technical details on diffusion.
3912380	3921240	A	0.8160340189933777	The calculus of twisted differentiate forms allows us to have a measure informed calculus on multivector Fields.
3923200	3925608	A	0.7155264019966125	What are twisted differential forms?
3925784	3927896	A	0.9712782502174377	I'm looking forward to that conversation.
3928008	3930828	A	0.8720332980155945	What is the untwisted differential form?
3930994	3934030	A	0.5058083534240723	Have we been twisted all along or not?
3935780	3941680	A	0.7603974938392639	And they go on to write a fundamental criterion for efficient sampling is non reversibility.
3942180	3950896	A	0.750526487827301	A process is non reversible if it is statistically distinguishable from its time reversal when initialized at the target distribution.
3951088	3957460	A	0.849616527557373	So once the jump rope is lying flat on the mountain, it's like in a position of reversibility.
3958600	3961352	A	0.8206411004066467	Mixing metaphors at will.
3961406	3977352	A	0.7131446599960327	Here measure preserving diffusions are non reversible precisely when some cognition are met intuitively non reversible processes backtrack less often and thus furnish more diverse samples.
3977496	3993280	A	0.5312352180480957	So if you're that ball rolling on the mountain, you want to just plow forward and whether you're going up or downhill, you want to be sampling and making moves, but you don't want to be going back and forth because at that point it's not and efficient sampling path.
3994180	4001424	A	0.65291827917099	It's well known that removing nonreversibility worsens the spectral gap and the asymptotic variants of the MCMC estimator.
4001552	4013640	A	0.8095522522926331	So time discreditization spectral gap in dimension with linear coefficients, one can construct the optimal non reversible matrix a to optimize the spectral gap.
4014140	4032350	A	0.5451456308364868	We can have well behaved parameters that help us get at optimal discretization schemes so that that shadow Hamiltonian can be preserved when we do discretize it during optimization, so that we can respect the key properties of the system.
4033200	4038748	A	0.6595141291618347	However, there are no generic guidelines on how to optimize nonreversibility in arbitrary diffusions.
4038924	4058608	A	0.7632798552513123	This suggests a two step strategy to construct efficient samplers one, optimize reversible distinctions dimension and two and a non reversible perturbation equation 36 citation 125 diffusions are manifolds.
4058704	4065530	A	0.780797004699707	Diffusion on manifolds are reversible when certain things are the case, and they're not when other things are the case.
4066940	4073050	A	0.8721314668655396	Equation 37 we got a triangle pointing down and a triangle pointing up.
4073500	4076104	A	0.6348171234130859	We're going to talk more about it.
4076302	4082456	A	0.6214264035224915	Under damped Langevin dynamics combine all the desirable properties of an efficient sampler.
4082568	4088480	A	0.5921583771705627	It is reversible, has degenerate noise, and achieves accelerated coherence to the target density.
4088820	4106310	A	0.5465754270553589	So all of this groundwork is helping us get to an analytical formalization that's going to have the right kind of properties so we can do that discretion right on the shadow Hamiltonian, so we can agent the accelerated optimization done.
4107560	4117240	A	0.5278021693229675	3.2 hamiltonian Monte Carlo a challenging task consists of constructing efficient sampling algorithms with strong theoretical guarantees.
4117660	4131900	A	0.7357114553451538	We now discuss an important family of wellstudied methods known as Hamiltonian Monte Carlo HMC, which can be implemented on any manifold for any smooth, fully supported target measure that is known up to a normalizing constant.
4132560	4141180	A	0.8218668103218079	Some of these methods can be seen as an appropriate geometric integration of the underdamped Langevin diffusion diffusing down that hill.
4141340	4152240	A	0.6862541437149048	But in simpler, it is in general simpler to view them as combining a geometrically integrated deterministic dynamics with a simple stochastic process that ensures ergodicity.
4152840	4176472	A	0.7822383046150208	Equation 38 if we interpret the negative log density v of q as a potential energy a function depending on position q, one can then plug in the potential within Newton's equation to obtain a deterministic proposal that is well defined on any manifold.
4176616	4181912	A	0.6390872597694397	As soon as the acceleration and derivative operators have been replaced by their curved analogues.
4182056	4185860	A	0.8685473799705505	So what pulls the ball down the hill?
4185960	4192316	A	0.9021764397621155	Or, if you're an instrumentalist, what force describes the movement of the ball moving down the hill?
4192508	4199036	A	0.5653778314590454	Gravity, I think there's a John Mayer song about it surprises like gravity.
4199228	4209856	A	0.8975102305412292	It can be understood as the minimizing force that pulls the ball down the hill or describes the movement of the ball on its path of least action down the hill.
4210048	4211030	A	0.9700974225997925	How cool.
4212940	4229176	A	0.7915421724319458	More discussion around 38 bringing these ingredients together, we thus have the following HMC algorithm given dot dot dot.
4229368	4240316	A	0.8928702473640442	We're going to compute a function according to one, a heat bath, two shadow Hamiltonian dynamics and three metropolis correction.
4240508	4242000	A	0.6757951974868774	That's the recipe.
4242580	4244770	A	0.5650674104690552	We're bringing those ingredients together.
4245620	4267320	A	0.8327618837356567	The above Rudimentary HMC method was proposed for simulations in lattice quantum chromodynamics, with M being the special unitary group Su N, and used a Hamiltonian dynamics ingeniously constructed from the Mauer carton frame to compute the partition function of discretized gauge theories.
4267900	4271880	A	0.8861393332481384	This method has later been applied in molecular dynamics and statistics.
4274400	4280540	A	0.5188294649124146	There are three critical properties underpinning the success of HMC in practice.
4281280	4289120	A	0.888887882232666	The first two are the preservation of the reference measure and the existence of a conserved shadow Hamiltonian for the numerical method.
4289460	4301220	A	0.8034376502037048	The third critical property is the existence of splittings methods, for which all the composing flows are either tractable or have adequate approximations, namely the geodesic integrators.
4301640	4313930	A	0.8786372542381287	So geodesic methods buckminster, fuller, tensegrity, cybernetics, or however you think about geodesics as paths and curved space.
4314940	4323960	A	0.733718991279602	That is the kind of approximation and splitting we're gaining access to, with appropriate distinctions on those balls rolling to the bottom of the hill.
4324860	4330188	A	0.7330751419067383	Let us also briefly mention some useful upgrades that have been proposed in recent years.
4330354	4336184	A	0.9637566208839417	So for those who it's been a few years since you checked in with this domain, this is a great place to look.
4336322	4358500	A	0.6995286345481873	First, you can grant the method extra integration steps when the proposal is rejected, or you can use criterion that aim to ensure the motion is long enough to avoid random walks, but short enough that we do not waste computational effort, such as the no uturn examples, which is integrated in a lot of software packages.
4359080	4373264	A	0.6114109754562378	Second, Modern HMC methods bypass this issue of slow convergence by replacing the heat bath with an ornstein olbeck process, which ensures the overall algorithm is irreversible.
4373412	4385580	A	0.8188780546188354	An ou process is like a Brownian diffusion with a linear regression, so it vibrates and has volatility, and it has a trend line that's linear.
4386240	4392768	A	0.6106386184692383	Third, many modifications of the Rudimentary HMC algorithm only provide improvements when the acceptance rate is sufficiently high.
4392934	4401910	A	0.6390403509140015	A third class of upgrades improve the acceptance rate by using the fact that the shadow Hamiltonian is exactly preserved by the integrator big point of the whole paper.
4402280	4414148	A	0.7514277696609497	Finally, the metropolis step can be replaced with a multinomial correction that uses the entire numerical trajectory, accepting a given point along it according to the degree by which it distorts the target measure.
4414324	4425608	A	0.864328920841217	Some path based inference methods with some caveats all right, on to section four statistical inference with kernel based discrepancies.
4425784	4452304	A	0.8133552670478821	So, section four the problem of parameter inference consists of estimating an element theta star within big theta using a sequence of random functions or estimators theta hat n of omega onto big theta equations conveniently reproducing Colonel Hilbert spaces.
4452352	4468100	A	0.8052393198013306	RKHS are precisely Hilbert's spaces over which the Diroc distinctions, which is a statistical distinctions type where it's like a spike at one location and zero elsewhere, act continuously.
4468260	4473210	A	0.9616746306419373	So whether we're talking about events or spiking neurons, this is a really good property.
4473520	4485180	A	0.8567206859588623	And more generally, the probability distributions that act continuously by integration on an RKHS h are exactly those for which all elements of h are integral.
4485620	4508560	A	0.5462114810943604	So analytically we have that nice property denoting by fancy piece of h, the set of such probability measures such that dot, dot, dot, we can define the maximum mean discrepancy, or Mmd, as such more definitions, a practical expression for the squared Mmd.
4508720	4514884	A	0.8102558255195618	So just like in linear regression, we're often interested in the sum of squared errors.
4515012	4518760	A	0.8614643216133118	Here we're interested in the sum of squared Mmd.
4520220	4521032	A	0.5749770402908325	4.1.
4521086	4528940	A	0.7097377777099609	Topological methods for Mmds just remember Mmds maximum mean discrepancy.
4531120	4540240	A	0.5072022676467896	A key feature of RKHS is that there are Hilbertian subspaces, more details about embedding in Hilbert spaces and subspaces.
4541780	4551220	A	0.7160823345184326	The geometric Analysis the geometric description of RKHS and Mmd allows us to swiftly apply topological methods in their analysis.
4551800	4557430	A	0.5085654258728027	There are reasons this reduces the matter to a topological question.
4559800	4569348	A	0.7446256279945374	Instead of defining T star to be the set of probability measures it is commonly done to define statistical manifolds.
4569524	4582940	A	0.49940788745880127	It's desirable to embed franca p within a more structured space, such as the space of finite radon measures, which enables the method to learn the target function independently of the data generating distribution.
4583760	4585752	A	0.8630338311195374	What is a radon measure?
4585896	4587280	A	0.739383339881897	We're going to find out.
4587430	4604870	A	0.7903928160667419	However, it enables the method to learn the target function independently of the data generative distinctions sounds pretty important if we want that tale of two densities Mmd can discriminate distinctions that makes it useful to do other things.
4605480	4606950	A	0.710509181022644	Let's find out more.
4608760	4618676	A	0.8989139795303345	Section 4.2 smooth Measures and KSDS So more information on Mmds and about making them computationally.
4618708	4634904	A	0.8368049263954163	Tractable and citations to Stein's Method are provided for 195, which is the 1972 Stein paper, and 196, which is a paper written by some of the authors about Stein's Method meets statistics.
4634952	4645040	A	0.8374593257904053	A review of some recent developments from 2021, and I thought it'd be fun to look at some figures from this paper and look at the abstract, so I'll put the figures up above.
4651220	4657976	A	0.9012119770050049	Stein's method compares probability distributions through the study of a class of linear operators called Stein operators.
4658108	4666280	A	0.7899060845375061	While mainly studied in probability and used to underpin theoretical statistics, stein's method has led to significant advances in computational statistics.
4666780	4672324	A	0.7655603885650635	The topics that are discussed in that review include tools to benchmark and compare.
4672372	4683368	A	0.8172923922538757	Sampling methods such as approximate Markov chain, Monte Carlo deterministic, alternatives to sampling methods, control variant techniques, parameter estimations, and goodness of fit testing.
4683544	4702944	A	0.8389460444450378	And from the paper they write the list of results given in this paper are but a mere sample of the ongoing activity in this newly established area of research at the boundary between probability, functional analysis, data science, and computational statistics.
4703072	4714412	A	0.8565102219581604	For instance, Stein's method has been used for designing sampling based algorithms for nonconvex optimization learning semiparametric multi index models and high dimensions and Bayesian statistics physics.
4714576	4734312	A	0.8299550414085388	Stein discrepancies have been used as variational objectives for posterior optimization, which is what we're all about free energy functionals, using variational inference as objectives for posterior approximation of inference and action free energy principle.
4734456	4737756	A	0.9305051565170288	And these images have some nice intuitions too.
4737938	4743868	A	0.8851029276847839	Here we see, depending on our step size, we're getting different sampling.
4744044	4752390	A	0.5762209296226501	When our step size is to the negative fifth power, we're over sampling one part of the space.
4753560	4759376	A	0.8663308620452881	Then, as we make the step size larger, our samples are drawn from different distributions.
4759408	4769448	A	0.927164614200592	So that's selecting the step size epsilon for a stochastic gradient Langvin dynamic and also from the anastasio paper.
4769614	4771930	A	0.9292073249816895	Here's another fun way to see that.
4772700	4789576	A	0.6974118947982788	Here we have that bowl that we're rolling the ball to the bottom of, and the Stein points and the Stein thinning, which is helping us understand maybe how starting the ball in different locations or critical locations accelerates what we're doing with optimization.
4789768	4792030	A	0.9725248217582703	So cool to talk about.
4792560	4807200	A	0.6978804469108582	4.2.1 the canonical Stein operator and Poincare duality there are two fundamental theorems that help us understand the integral differentiate geometry of the manifold durham's theorem and the Poncare duality.
4807800	4816120	A	0.8875661492347717	The former Durham's theorem relates the topology of the manifold to information on the solutions of differential equations defined over the manifold.
4816460	4830380	A	0.8960734605789185	The latter, which contains the fundamental theorem of calculus, describes the properties of the integral pairing alpha beta of differential forms, which include the pairing of test functions with smooth measures.
4831360	4833150	A	0.7789154052734375	Let's learn more about it.
4833840	4845920	A	0.8553781509399414	Four, two, two kernel stein discrepancies and score matchings technical definitions facilitating Stein variational gradient descent.
4847780	4854180	A	0.7787067890167236	4.3 information geometry of Mmds and natural gradient descent.
4855000	4858900	A	0.9542950987815857	These tools have proved to be useful in a wide range of contexts.
4860120	4870920	A	0.5317158102989197	More information about the divergence and how divergence can improve the speed of coherence by following the natural gradient descent.
4872620	4876400	A	0.5072543025016785	Okay, red text speculation beware.
4876580	4885496	A	0.8941785097122192	So the operator, which is a phone routing system like ring ring, hello, operator is constructed in action.
4885528	4894080	A	0.6626573204994202	4.2.22 colonel Stein discrepancies and score matching you can't have a Ttest without the T distribution.
4894420	4900480	A	0.8501259088516235	The Ttest statistic is using a T distribution.
4901480	4923380	A	0.8155505657196045	So a class of fancy V, a set fancy V of vector Fields, or more generally, tensorfields whose image f under the operator ring ring has mean zero under Mu that's going to give us this discrepancy, the SD and the Stein variational gradient descent.
4923540	4933390	A	0.7114301323890686	So there's some class of V vector Fields such that the mean of something about them is zero.
4934880	4946800	A	0.802781879901886	Does this enable the central limit theorem or just statistics more broadly, like the parametric and non parametric methods that we know and love from SPM?
4947620	4957860	A	0.7664684653282166	Does that mean zero enable us to use Gaussian methods, generalized Gaussian methods, generalized linear methods, SPM?
4958600	4973000	A	0.882489025592804	And does it enable the proposed smooth distribution, the one generating the shadow Hamiltonian that's getting inferred over, to be interpreted or used truly as a statistical distribution?
4973500	4979544	A	0.5597196817398071	It's easy to forget that not all distributions are formal probability distributions.
4979592	4986540	A	0.8253012895584106	For example, the area under the curve from zero to one of a probability distribution is one.
4986690	4988108	A	0.6899359822273254	Something has to happen.
4988274	4994450	A	0.7913223505020142	But not all functions have an area under the curve of one between zero and one.
4995060	5005216	A	0.840481698513031	So in variational Bayesian inference, the kind which we do in active inference, in fact, the kind that's done in machine learning and statistics more broadly.
5005248	5009350	A	0.7005554437637329	But we're interested when action is a parameter that we're doing inference on.
5009720	5017620	A	0.8167286515235901	We're concerned with the extremely well behavior properties of a certain subset of distributions.
5017780	5020104	A	0.6383338570594788	So why are we doing this?
5020142	5032028	A	0.6587868928909302	Why is it important that we didn't need the input data that we could construct measures that are independent of the input data?
5032194	5035816	A	0.7372810244560242	Because we want to enable the tail of two density.
5036008	5046160	A	0.5697332620620728	We want to make generative models that can be used generatively, generative AI in the forward or the generative direction.
5046820	5056528	A	0.7838836908340454	But we also want to be able to enable the recognition distribution, which is from empirical data, to do hidden state inference.
5056704	5074200	A	0.6166549921035767	And so just like least squares regression in the linear modeling case, where the sum of squares above and below the regression line should be as low as possible l two norm sum of squares minimization always works, never complains.
5075260	5098400	A	0.8008621335029602	We want that kind of ball rolling to the bottom of the hill with free energy functionals variational and expected free energy functionals as the optimized or satisfied imperative for optimal perception, which is signal processing, signals intelligence and control which is control theory or action selection.
5101700	5106916	A	0.9078913927078247	More technical details and equations 39 B and 39 C.
5107098	5116468	A	0.8408711552619934	So what does it mean that the resulting Stein discrepancy can be thought of as an Mmd that depends only on row and is known as a kernel Stein discrepancy.
5116564	5117610	A	0.7506719827651978	What is that?
5119820	5128860	A	0.8713102340698242	As we did previously, we can remove the super mum by rewriting the above as a super mum over some unit ball of continuous linear functional.
5129360	5136670	A	0.881790041923523	Is this like simulating or modeling a sphere rolling on a landscape like we've been talking about?
5137280	5141036	A	0.8981265425682068	Is the ball of optimal radius for rolling on that landscape?
5141068	5142476	A	0.6967973113059998	Or what is being optimized?
5142588	5151730	A	0.9059453010559082	And what is the unit, the scaling or the scale specificity that this scale friendly sphere is scaled to?
5153880	5159840	A	0.8900271058082581	Equation 42 and 42 a while in the Euclidean space yields the diffusion score matching.
5159920	5181512	A	0.5871087908744812	Citation 204 Barp et al again minimum Stein discrepancy estimators from 2019 they write the main strength of our methodology is its flexibility, which allows us to design estimators with desirable properties for specific models at hand by carefully selecting a Stein discrepancy.
5181656	5189124	A	0.6013979911804199	We illustrate this advantage for several challenging problems for score matching, such as non smooth, heavy tailed or light tailed densities.
5189272	5198640	A	0.5922383069992065	So again, just with a little bit of speculation here and I think my camera has frozen.
5206760	5207990	A	0.8811412453651428	It's all good.
5209240	5212340	A	0.8072664737701416	I'll just go without the camera.
5214360	5215110	A	0.584351658821106	Okay.
5216920	5224200	A	0.8700321316719055	One can estimate infer or optimize the zero point and hence the variance structure of the distribution.
5224540	5242076	A	0.7913673520088196	This is going to ensemble generalized wellbehaved modeling again, from all of those nice perspectives that we raised earlier, like Gaussian central Limit theorem, smoothness, statistical disturbance, Euclidean, all of those well behaved properties that we're looking for.
5242258	5244110	A	0.8434417247772217	This is going to help us get there.
5244480	5252880	A	0.8907243013381958	We are only talking about a specific subset or type of landscape here, the one that we're doing variational inference on, that's the map.
5253620	5256080	A	0.5805063247680664	This is not the structure of the territory.
5256680	5264900	A	0.5956647396087646	These well behaved attributes of models for better, worse and different through sickness and health is because they're maps.
5265720	5274280	A	0.7600353956222534	It is always the case that our statistically nice generative models are of a different structure or form than the generative process.
5274430	5279064	A	0.7954266667366028	So this is actually not a criticism of quantitative modeling as a process.
5279262	5283128	A	0.6595210433006287	In fact, this is the entire basis of quantitative modeling.
5283224	5286552	A	0.9089168310165405	So we've approached this map territory distinction.
5286696	5298780	A	0.6546354293823242	Map territory fallacy, fallacy, fallacy from many angles and one will still hear things like well, the structure of the statistical model is not the same as the territory.
5298940	5305250	A	0.7608866095542908	Or how can you say that the organism has these well behaved properties just because the model does?
5305720	5321672	A	0.5414015054702759	And this brings a really sharp light on it, which is we're doing this insane amount of analytical groundwork so that the map has the good properties not to constrain what the territory is.
5321806	5327764	A	0.8387938737869263	So map territory, all of the math we've been talking about is map.
5327892	5335980	A	0.5403041839599609	We want wellbehaved maps so that we can describe wellbehaved and unruly territories.
5336320	5341004	A	0.6524184942245483	But it isn't the case that the organism minimizes variational free energy.
5341122	5345180	A	0.7527499198913574	It's the generative model that minimizes variational free energy.
5345330	5350576	A	0.8354535102844238	So, little bit of a leading discussion topic or question for you all.
5350678	5354960	A	0.6773351430892944	Feel free to give a thought in the live chat or write a comment or join our discussions.
5355560	5361236	A	0.8910605311393738	How is this line of basic and fundamental math research by Barp et al.
5361418	5371720	A	0.8064573407173157	Creating new models that have the operational or denotational semantics that we want for computational statistics, which is to say applied information theory.
5372060	5387230	A	0.5752891898155212	For example, distributions that can be interpreted or used as statistical distributions, ideally directly compatible with current computational methods SPM in MATLAB, Pi, Mdp in Python and fourney Lab in Julia and so on.
5388560	5398850	A	0.8662967681884766	Section 4.3.1 minimum Stein discrepancy estimators, more technical details, more inclination to 204.
5399220	5407250	A	0.8948861360549927	The parameters can be adjusted to active characteristicness consistency, bias, robustness and obtain central Limit Theorems C 204.
5407620	5409764	A	0.8974959254264832	2019 paper with Barp et al.
5409882	5414516	A	0.9479432702064514	And let's just look at a really cool image from that paper.
5414698	5418208	A	0.9745879769325256	Figure one and figure two pretty cool.
5418394	5421770	A	0.9254553914070129	SD estimators looking nice.
5423420	5447190	A	0.6462675929069519	Section 4.3.2 likelihood Free Inference with Generative Models so for many applications of interests, the densities of the model mu sub data cannot be evaluated or differentiated.
5448010	5457370	A	0.502894401550293	We thus need density free inference methods super convenient for Bayesian statistics, more technical details, information tensor.
5457870	5468590	A	0.748225748538971	Under appropriate choices of kernels and models, one can derive theoretical guarantees such as concentration and generalization boundary consistency asymptotic normality and robustness.
5469090	5480590	A	0.7084458470344543	So, little bit of a summary we have good analytical and computational footing in certain kinds of situations or under certain constraints.
5480750	5497462	A	0.6375014185905457	Certainly not all constraints, but definitely for some that might enable the efficient computation, not just specification, but actual implementation of large generative models such as described in the paper of Friston et al.
5497596	5502458	A	0.7661375403404236	2022 designing Ecosystems of Intelligence from shared sorry.
5502544	5510620	A	0.758131206035614	Designing Ecosystems of Intelligence free Energy Principle And they use the term ecosystems of shared intelligence in that paper.
5510990	5515466	A	0.6899338364601135	And so it's relevant to learn about the actuality and build intuition.
5515658	5516398	A	0.6120101809501648	Why?
5516564	5517214	A	0.7220153212547302	How do we know?
5517252	5533006	A	0.78693687915802	Well, here's the one figure in that paper of Friston at all, the one figure in this absolutely positional paper that they have released believes as parameters of a probability distribution.
5533198	5550530	A	0.8715264201164246	Here's the sharpening of a belief as a distance belief updating as traversing a statistical manifold, which is to say a lower dimension projected space and what's being shown as the parameter space of a probability distribution.
5550610	5567470	A	0.7418455481529236	So whether we see this as gradient ascent to climb to the top of the hill or we take the negative and we have gradient descent to the bottom of the hill, this is the figure chosen by some very well informed authors to describe Bayesian mechanics.
5569730	5578290	A	0.7018561363220215	And long last we get to section five adaptive agents through active coherence.
5579270	5593990	A	0.7141988277435303	We close, as the authors write, with a generic use case called Active Coherence, a general framework for describing and designing adaptive agents that unifies all aspects of behavior, including perception, planning and learning as processes of inference.
5594330	5612198	A	0.7253140211105347	By exploiting this geometric structure in a generic framework for designing adaptive agents, we derive the objective functional overarching decision making and describe its information geometric structure, revealing several special cases that are established notions in statistics, cognitive science and engineering.
5612374	5614186	A	0.8189439177513123	So section five one modeling.
5614218	5615694	A	0.6519814133644104	Adaptive decision making.
5615892	5619550	A	0.8881375193595886	First, they're going to talk about behavior, agents and environments.
5620130	5624910	A	0.8694963455200195	Behavior is going to be defined as the interaction between an agent and its environment.
5625270	5629810	A	0.8173736929893494	And the system is going to describe the agent plus its environment.
5631030	5639014	A	0.9006019234657288	There's going to be a set of states that are partitions from a statespace big x external states.
5639212	5646726	A	0.8233179450035095	S are going to be unknown to the agent and constitute the environment states belonging to the agent.
5646908	5653690	A	0.7952693700790405	Pi particular states are going to be a subset of two different things.
5653760	5657530	A	0.8582247495651245	So here is S, which is here external states.
5657600	5661370	A	0.8901026844978333	Note that in some other settings S will refer to sensory states.
5661440	5671680	A	0.8927063345909119	So look at the notation in this paper and Pi, which in other papers is sometimes used to describe policy inference here is going to describe particular states.
5672050	5676706	A	0.8490911722183228	So Pi is O and A.
5676888	5690502	A	0.780949056148529	O are the observable states, states that the agent can see but cannot directly control sense states and A are the autonomous states that the agent sees and can directly control.
5690636	5694760	A	0.857340931892395	And those are going to be internal states and active states.
5695530	5704300	A	0.9317115545272827	Figure two has a great representation and again note the way that Pi, So and A are being used in this paper.
5705390	5711690	A	0.8223881721496582	S is the external process hidden state, latent state, causal coherence.
5712370	5724480	A	0.8564860820770264	O are the observations and the agent process concepts of Pi, which is the observations and alpha or A.
5725350	5737630	A	0.8354867696762085	A is consisting of, according to the particular partition, two different kinds of states, which is internal states, cognitive states and action states.
5737800	5746630	A	0.7500913143157959	And so we're going to compute bounds for free energy functionals with a special focus on autonomous processes.
5746970	5757286	A	0.6214777231216431	Because controlling our perception is not possible, and maybe not even preferable by controlling it at the level of what we observe.
5757478	5769626	A	0.7013204097747803	But rather, if we control our internal states, which is our interpretation of the perception and our action states, we'll be doing optimal perception and optimal action.
5769738	5777182	A	0.8678065538406372	So those are the two sides of the coin with inference as perception learning and action selection.
5777246	5779854	A	0.5246601700782776	That's how we unify action and inference.
5779902	5789426	A	0.8223080039024353	Inactive inference is by caring about the bounding of selfsurprisal, just like gravity on our autonomous processes.
5789538	5792680	A	0.9833810329437256	So this will be super fun to discuss more.
5794090	5804982	A	0.8528918027877808	5.1.2 decision making and precise agents so, what distinctions people from small particles people are subject to classical as opposed to statistical mechanics.
5805126	5813710	A	0.9202630519866943	Check out Active Livestream 49 for more on Bayesian mechanics and some of these distinctions with classical statistical quantum and thermo.
5814290	5818030	A	0.8688086271286011	In other words, they are precise agents with conservative dynamics.
5818850	5825890	A	0.8605017066001892	Precise agent definition 5.1 an agent is precise when it evolves deterministically in a possibly stochastic environment.
5827830	5833982	A	0.49740156531333923	More definitions and it's useful here to highlight Friston et al's.
5834046	5889400	A	0.6619424223899841	Recent paper path integrals particular kinds and Lagrange things which provides a taxonomy of things taxonomy of particular entities that run the gamut of sophistication from inert particles which have no active states active particles which have active states generative process which have cognitive dynamics that are described as classical and strange particles which represent in this visualization the highest level of cognitive sophistication in which those internal hidden states themselves have this kind of what would happen if this happens counterfactual type or thinking through other minds type cognitive model structure doing forward and inverse inference on these great times.
5891130	5892422	A	0.6567074060440063	5.1.2.
5892476	5902390	A	0.8770007491111755	Decision making and precise agents we have mathematical formalisms for decisions, preferences and predictions all using the partitioned variables.
5903210	5910378	A	0.8562191724777222	5.12 more formalisms we get to expected free energy.
5910544	5915440	A	0.87306147813797	Expected free energy is equation 41K.
5916850	5947190	A	0.873932421207428	Active inference is Hamilton's principle of leased action on expected free energy and expresses the most likely decision where certain features are met principle of least action on manifolds of inference and action subsuming or bringing together inference and action active inference, free energy principle 5.1.3.
5947340	5952790	A	0.8837182521820068	Active Inference Framework AIF looks like it describes agents that engage in purposeful behavior.
5952950	5961578	A	0.8489813208580017	We can rearrange the expected free energy EFE in several ways, each of which reveals a fundamental trade off that underwrites decision making.
5961744	5970990	A	0.5321924090385437	This allows us to relate active inference to information theoretic formalizations of decision making that predominate in statistics, cognitive science, and engineering.
5971810	5977198	A	0.8879642486572266	Figure three, as hinted ant in the early, early keywords.
5977374	5981902	A	0.8721093535423279	Here on the top, we have the general formalizations of active inference.
5981966	6003926	A	0.8226403594017029	And one of the partitions, or better to say competition, is shown here, where extrinsic value is the surprising about observations, making sure that what we're getting in observations are what we expect slash prefer if the body wants to be expecting homeostatic temperature.
6004038	6006086	A	0.8120117783546448	That's what this is going to determine.
6006198	6027810	A	0.6126992106437683	And it plays a role equivalent to reward in reward and reinforcement learning based approaches, because we don't need to actually set a reward function, but we get something that looks like selfsurprisal aligned with reward using the preference variable over observations.
6028630	6030194	A	0.8387328386306763	A lot more to say there.
6030392	6039110	A	0.6019981503486633	And the second term is the intrinsic value, the epistemic value, curiosity, novelty, learning, reduction of uncertainty.
6039530	6042950	A	0.7352122068405151	And another partitioning is between risk and ambiguity.
6043850	6052646	A	0.8953525424003601	There are four colored dots red, tan, gray, and Bleu corresponding to special cases.
6052838	6068446	A	0.8415188193321228	So under the setting where there's no ambiguity, we realize or manifest the special case where control can be seen as inference, maximum entropy, reinforcement learning, prospect theory, and KL optimal control.
6068628	6077460	A	0.5749571323394775	This is like doing as good as you can strategically or tactically, given that there's no ambiguity in how your decisions play out.
6078230	6086862	A	0.796405553817749	Now, under the setting where there's no ambiguity or preferences, a maximum entropy principle is realized.
6087006	6089474	A	0.8292679786682129	And work with Bayesian mechanics.
6089602	6090966	A	0.8891514539718628	And Dalton et al.
6091068	6101094	A	0.8495292067527771	Has shown that the constrained maximum entropy principle is dual to the FEP in the case of no extrinsic value.
6101292	6105622	A	0.4862997233867645	So no quote reward, no quote goals.
6105686	6124318	A	0.7145017385482788	We don't use reward or goals in the active inference ontology, but borrowing those words from some long, long forgotten ontology, in the case of no extrinsic value, we realized the special case of maximum information gain, Bayesian experimental design.
6124404	6138870	A	0.655875563621521	So not to prove or disprove, but the maximally informative experiment that's the Bayesian scientific epistemology intrinsic implication and Bayesian surprise seeking out optimally informative stimuli.
6139530	6144280	A	0.4983513653278351	And contrast that with a setting where there's no intrinsic value.
6144810	6156220	A	0.7186267971992493	So where there's nothing to learn, we realize expected utility theory, Bayesian decision theory, reinforcement learning, and optimal control.
6156990	6188402	A	0.8115853071212769	So special cases from many different Fields, ranging from statistical mechanics to Bayesian decision making and statistics integrated or perhaps better generalized across in the formalisms of active inference, which is something like a Hamilton's principle of least action on variational or expected free energy, expressing the most likely inference and action under certain constraints.
6188546	6191990	A	0.547216534614563	Not the most rewarding, but the most likely.
6193770	6195318	A	0.6621715426445007	5.1.3.
6195484	6198578	A	0.7655540704727173	Decision making minimizes both risk and ambiguity.
6198754	6210490	A	0.7312251925468445	Risk, first term on the right hand side, ambiguity second, term minimizing ambiguity leads to a type of observational bias commonly known as the streetlight effect.
6210640	6214362	A	0.627048671245575	When a person loses their keys at night, they initially search for them under the streetlight.
6214426	6226110	A	0.5280254483222961	Because of the resulting observations, I see my keys under the streetlight, or I do not see my keys under the streetlight accurately disambiguate external states of affairs.
6226450	6234066	A	0.6528211236000061	First place to look make sense under the streetlight, and the last place you look is where you find it more in.
6234088	6235346	A	0.6621715426445007	5.1.3.
6235448	6249734	A	0.8101080656051636	Decision making maximizes extrinsic and intrinsic value another decomposition of the formalisms of active inference maximizing information gain leads to a goal directed form of exploration driven to answer what would happen if I did.
6249772	6263354	A	0.7242435812950134	That true counterfactuals this decision making procedure underwrites Bayesian Experimental Design in Statistics, which describes optimal experiments as those that maximize expected information gain.
6263482	6284222	A	0.7478890419006348	And we've had some great discussions recently in textbook groups and in discussion hours, where we contrasted that falsificationist concept of accept or reject hypothesis, and even the idea of a scientist or a researcher setting out to accept or reject hypothesis.
6284286	6296630	A	0.690910279750824	Whether you take that Positivist or you take the other path of falsificationism, in both cases, you might end up with an informative experiment or not.
6296780	6310598	A	0.5377240777015686	But you can easily imagine cases where you end up with an uninformative experiment because you set out to confirm something you knew, or you set out to disprove something by constraining your experiment so that it was locally disproven.
6310774	6331060	A	0.5493068099021912	In contrast, when we take into account the richness of our generative model, we motivate this Bayesian epistemology and ultimately professional Bayesianism, where we make optimal experiments in terms of their maximum expected information gain, which requires you to also state your generative model.
6332710	6342950	A	0.7151330709457397	Decision making under active inference weighs the imperatives of maximizing utility and information gain, which suggests a principled solution to the exploration exploitation dilemma.
6343770	6345718	A	0.8819507956504822	Great point to jump into.
6345884	6354970	A	0.8933245539665222	Does active inference, simply by written down, by being written down on a paper, resolve or transcend or dissolve exploration exploitation?
6355390	6356140	A	0.4936186671257019	No.
6356670	6368858	A	0.852979838848114	Does it provide a space or a framework, a method, an approach, software packages, a research community that can help us address and navigate and surf on the edge of that dilemma?
6369034	6369760	A	0.4650449752807617	Absolutely.
6371650	6372862	A	0.647846519947052	5.2.1.
6372916	6380382	A	0.8918532729148865	The basic active inference algorithm we're going to go through it with authors preferential inference.
6380526	6384690	A	0.8449297547340393	We want to infer preferences about external and observable trajectories.
6385830	6389762	A	0.8632168769836426	Two for each possible sequence of past, present and future actions.
6389826	6398838	A	0.8974857330322266	A we're going to engage in both sides of the coin perceptual inference what would I perceive if that happened?
6399004	6404826	A	0.8784758448600769	And planning as inference assess the action sequence by evaluating its expected free energy.
6405008	6425810	A	0.8866214156150818	And then three, decision making execute the most likely decision at T plus one, according to some distinctions, sample from your action posterior, action prior that's, like your habits, it gets sharpened or modified with expected free energy, and then you sample from the action posterior.
6427350	6428562	A	0.647846519947052	5.2.1.
6428616	6440646	A	0.8773277401924133	Sequential Decision Making under Uncertainty a partially observable Markov decision process POMDP is a discrete time model of how actions influence external states.
6440828	6446614	A	0.8456095457077026	In a POMDP, each external state depends only on the current action and previous external state.
6446652	6448120	A	0.8440948724746704	That's the Markov property.
6448650	6457130	A	0.8680592775344849	Each observation depends only on the current external state, and one can additionally specify a distribution of preferences over external trajectories.
6457950	6461980	A	0.8811075091362	One and two form the agent's POMDP prediction model.
6463330	6470160	A	0.9050988554954529	Two and three form the agent's hidden Markov preference model, which defines the active inference agent.
6470930	6475698	A	0.8858913779258728	A simple simulation of active inference on a POMDP is provided in figure four.
6475864	6479346	A	0.8878427147865295	Implementation details on generic POMDPs are available.
6479528	6483746	A	0.8731255531311035	For more complex simulations of sequential decision making, e.
6483768	6489110	A	0.8864997029304504	G involving hierarchical POMDPs, please see citations.
6490090	6491334	A	0.7192431688308716	Figure four.
6491532	6493174	A	0.7460686564445496	It's a prediction model.
6493372	6497666	A	0.7836636900901794	This is sequential decision making in a Team A's environment.
6497858	6503878	A	0.9067251682281494	On the left, the agent's prediction model as a POMDP, represented here as a Bayesian network.
6504054	6506726	A	0.9029195308685303	On the right, information on the team A's.
6506838	6522080	A	0.6627060174942017	So here we have some type of Bayesian graph reflecting hopefully a statistical system that's going to have all of these well behaved properties that we've been talking about in the paper.
6522530	6526830	A	0.5202289819717407	And then the team AZ is played out.
6526980	6528320	A	0.7545015215873718	We'll talk about it.
6529010	6530114	A	0.673769474029541	5.2.3.
6530152	6536466	A	0.5777111053466797	World model learning is inference due to a lack of domain knowledge, it may be challenging to specify an agent's prediction and preference model.
6536568	6539362	A	0.8893880248069763	For example, how do external states map to observations?
6539506	6543240	A	0.893671452999115	Should external states be represented in a discrete or continuous state space?
6543690	6547382	A	0.9608801603317261	Also, some great questions that come up every day.
6547516	6557450	A	0.9206610918045044	And Clark addressed in chapter six a recipe for active inference modeling by the Par Pazulo and Friston 2022 textbook.
6558190	6564990	A	0.845574140548706	So how do we get the right priors, or at least approximately adequate priors?
6565330	6572618	A	0.7389630675315857	One way to answer the question lies in optimizing a free energy functional F, which is an evidence lower bound.
6572794	6582290	A	0.8559960722923279	We see here variational free energy decomposed into an energy minus entropy and decomposed into a complexity minus accuracy.
6582710	6603930	A	0.7665578126907349	Framing maximizing accuracy usually results in generative models involving universal function approximators minimizing complexity results action action oriented representation sparse compartmentalized in hierarchical generative models where higher levels of the hierarchical model more abstract representations, and vice versa.
6604270	6616540	A	0.638912558555603	A computationally efficient method to compare priors by their free energy is Bayesian model reduction, free energy, unifies inference, and model selection under a single objective function.
6618830	6620126	A	0.6342717409133911	5.24.
6620228	6626690	A	0.6077184081077576	Scaling active inference planning for all possible courses of action is computationally expensive.
6627030	6635858	A	0.7917354106903076	One way to finesse this is by planning only for intelligently chosen subsets of action sequences using sampling algorithms like Monte Carlo Tree Search.
6636024	6641990	A	0.8579730987548828	If you're interested in that, check out the recent research on branching time active inference.
6642570	6648070	A	0.8237903714179993	Similarly, Monte Carlo sampling finesses the expectations inherent in assessing action sequences.
6648730	6659660	A	0.8230231404304504	A complementary approach is to assess actions instead of action sequences by conditioning all future actions to be optimal in the sense that they minimize the expected free energy.
6660510	6669550	A	0.5412275195121765	It leads to smarter agents whose computational complexity scales linearly as opposed to exponentially in the length of action sequence.
6670210	6671920	A	0.7789154052734375	Let's learn more about it.
6673730	6678638	A	0.7430663704872131	Scalable inference methods can be used to make active inference more efficient.
6678814	6699030	A	0.6678265333175659	We can train neural networks to predict the various posterior distributions, including the posterior over action while training the output of the neural network can be used as an initial conditions for variational coherence, resulting active inference whose computational costs decrease as the network learns.
6699370	6709702	A	0.7374742031097412	Additionally, optimizing free energy reduces to efficient message passing schemes when one imposes certain simplifying distinctions to the family of candidate distributions.
6709846	6726420	A	0.9604784250259399	Lots of really exciting work in neurobiology and statistics around message passing getting close to the end here a much cheaper implication of active inference exists for continuous states evolving in continuous time.
6727670	6728658	A	0.9739539623260498	Pretty cool.
6728824	6729714	A	0.7701930999755859	Par et al.
6729752	6748280	A	0.8961023092269897	2022 Textbook Chapter Seven Discrete time Active Inference generative Models Chapter Eight Continuous time Active Inference Models this method frames perception and decision making as variational inference by simulating a gradient flow on free energy in an extended state space.
6748810	6755900	A	0.5698839426040649	It can be combined with discrete active inference to operate efficiently in generative models combining discrete and continuous states.
6756270	6776750	A	0.6888407468795776	We talked about that in live stream 46 Active inference does not contradict folk psychology, discrete active inference decision making, active inference dai, and continuous time motor active inference being used together, also seen in the paradigm textbook.
6776910	6780946	A	0.853987991809845	As an example, high dimensional observations in the continuous domain, e.
6780968	6788166	A	0.8565055727958679	G speech processed through continuous active inference are converted into discrete abstract representations, e.
6788188	6799842	A	0.8831472992897034	G semantics, and we can even go further and say rhetorical and narrative information spaces based on these representations, the agent makes high level categorical decisions, eg.
6799906	6805838	A	0.7576681971549988	I want to move over there, which contextualize low level continuous actions, eg.
6805874	6813178	A	0.8986888527870178	The continuous motion of a limb towards the goal location and that is how the paper ends.
6813354	6818878	A	0.6911368370056152	So, closing thoughts if anybody wants to write a comment live, feel free to do so.
6819044	6820910	A	0.7408362627029419	We'll be over very shortly.
6821730	6825940	A	0.8223645091056824	What are the implications of this work?
6826870	6831060	A	0.5551594495773315	We have an open space to talk about it.
6833190	6838200	A	0.858169436454773	What questions and discussion topics are you interested in?
6838730	6846310	A	0.6745406985282898	Please write comments before or after the dot one and the dot two so we can have those interesting discussions.
6846970	6872238	A	0.9479738473892212	And just as a little bit of a closer, I'll share some stable diffusion images that were generated using the paper title as well as various other terms free energy, lots of fun images, a lot of good balance to some of the technical aspects that were being described in the paper.
6872404	6883518	A	0.8763953447341919	Was good to look at what diffusion looks like aesthetically and so that is the end of this Livestream.
6883694	6905020	A	0.911736249923706	52.0 I hope that you found it useful that you're interested to act in first, serve to learn more, contribute more, write a comment, make it happen in your own life or in your own way with this honestly challenging paper.
6905470	6921610	A	0.9608300924301147	So if you've listened this far, thanks a lot for your attention and looking forward to 520 One and Zero Two when we will speak with some authors and some of you.
6921760	6927726	A	0.8821375966072083	So till next time, thanks again and see you in the dot.
6927758	6928340	A	0.5377302765846252	One.
6928950	6929230	A	0.5137446522712708	Bye.

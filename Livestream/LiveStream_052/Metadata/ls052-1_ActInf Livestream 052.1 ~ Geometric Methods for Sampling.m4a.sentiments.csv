start	end	speaker	sentiment	confidence	text
7960	9150	A	0.8720964789390564	Hello and welcome.
9520	19310	A	0.7794957756996155	It is March 2023 and we're here in Activem Livestream 52.1.
20080	22440	A	0.8058606386184692	Welcome to the active institute.
22600	29792	A	0.6512011885643005	We're a participatory online institute that is communicating, learning and practicing applied active coherence.
29936	35830	A	0.5193906426429749	This is a recorded and an archived Livestream, so please provide us feedback so we can improve our work.
36200	49960	A	0.8679398894309998	All backgrounds and perspectives are welcome and we'll be following video etiquette for livestreams head over active coherence.org to learn more about getting involved in learning groups and projects such as live streams.
50940	69710	A	0.9285892844200134	All right, we are in 52.1 and our goal today is to learn and discuss this awesome paper geometric Methods for Sampling Optimization, Inference and Adoptive Agents of Barp and Acosta at all.
70100	73568	A	0.7568700909614563	And we're going to just jump right in.
73734	83430	A	0.9600855112075806	Thanks a lot Lance for joining and those who have listened to 52.0 will know me.
83800	88272	A	0.5576781630516052	I'm a researcher and learner with respect to this topic.
88416	99290	A	0.9783980846405029	So thanks again for joining and it'll be great to have you introduce yourself and also tell a little bit of the story of how this paper came to be.
100380	101032	B	0.4753468930721283	Sure.
101166	103300	B	0.9297141432762146	Thanks Daniel for organizing.
103460	106510	B	0.9926361441612244	This is really great and I'm very happy to be here.
107280	120540	B	0.857316792011261	So my name is Lance de Costa, some of you may know me, I'm a PhD student both at Imperial College of London and UCL, mostly working on active inference and the free energy principle.
120980	123170	B	0.7467848658561707	Also some probabilistic machine learning.
123620	130550	B	0.8190921545028687	And today we're going to be discussing geometric methods for sampling optimization, inference and adaptive agents.
132120	141064	B	0.592972993850708	So how this paper came to be where I happen to know people in who do more like statistics, machine learning and so on.
141102	156668	B	0.7289248108863831	And we've always been talking about the similarities and differences between what we do in two inference and what they do in sampling optimization, which are kind of the workhorses of statistics and machine learning.
156754	177392	B	0.8513664603233337	And at some point we had a paper invitation for the hamburger of statistics and we decided to team up and make a review article to see how all these Fields that come from very different places scientifically, that have very different contributors as well.
177446	195124	B	0.6454294919967651	And we wanted to know how they're interconnected and also how they can help each other because we realized for all this time that we're actually doing similar things, but the communities are very different, they're dot zero much talking to each other.
195242	200724	B	0.4949404001235962	And I think the main barrier is difference in Lagrange, difference in jargon.
200772	221810	B	0.747974157333374	So that was kind of an issue when we started and so I wanted to put that all as much as possible in a paper and see how these different approaches to statistics, whether it is sampling, optimization, inference or decision making, how they can benefit from each other and how they relate to each other.
223540	236630	B	0.7823241353034973	Long story short, we're going to delve into this into more detail, but long story short, if you want to ensemble from a distribution, you can also view that as an optimization problem.
237320	241376	B	0.8294442892074585	So optimization is kind of you can see it under the roots of sampling.
241408	246164	B	0.6232910752296448	And you want to optimize sampling, make sampling as efficient as possible inference.
246212	251476	B	0.855510950088501	You can also see it as an optimization of beliefs, optimization of probability distinctions.
251508	253660	B	0.8681565523147583	That's also an optimization procedure.
255200	268120	B	0.787354588508606	And if you want to do decision making, which is what we do in active inference and reinforcement learning and many different places, well, through decision making, you actually need optimization and inference.
268280	273600	B	0.7111055850982666	And if you want to do decision making efficiently, you often are going to need something as well.
273750	285590	B	0.791341245174408	So decision making, an active inference is kind of like what comes at the very end and requires all of the sophisticated machinery of sampling coherence of optimization in order to work.
286200	293668	B	0.8135319948196411	So this was really the idea of the paper, how does optimization come about, how do you do efficient optimization?
293844	296664	B	0.7052510976791382	And of course there's many different ways to do that.
296862	301864	B	0.5654111504554749	So we decided to focus on natural ways and we'll get to that.
301902	309340	B	0.5154703855514526	But one way to do optimization that is very natural is through techniques from geometry.
311520	313128	B	0.774919331073761	And so same with sampling.
313224	319490	B	0.5010440945625305	In sampling and inference, you have geometry that just comes up very naturally when you want to solve these problems.
319860	322770	B	0.8185345530509949	And so decision making puts it all together.
323220	325600	B	0.864709198474884	So that was really the idea of the paper.
325670	331504	B	0.8899739384651184	Can we put all these Fields together and have a connecting line which is based on geometry?
331552	333110	B	0.752999484539032	And so that's where we did.
335880	338512	A	0.9556739926338196	Awesome, great overview.
338656	343370	A	0.6072327494621277	So, so many places to jump in.
345740	349332	A	0.7638829350471497	I barely even know where to sample from this distinctions.
349476	352216	A	0.8181008100509644	Let's just start with sampling, though.
352318	356860	A	0.8106161952018738	So what is sampling and how is it being used?
356930	358510	A	0.8011912703514099	How has it been used?
359040	367740	A	0.8914007544517517	And let's characterize sampling and then move on to these other Fields, right?
367810	375744	B	0.6539185047149658	Well, sampling, I mean, the idea of sampling is very simple and it's also a very difficult thing to do.
375862	377730	B	0.7637816071510315	So first explain the idea.
378200	383940	B	0.7680675387382507	The idea is, let's say you have a Bayesian in one dimension, so that's very easy to picture.
384440	392712	B	0.8913403153419495	Sampling would be so, for example, the Gaussian I just mentioned describes the temperature in the room.
392846	401240	B	0.8422466516494751	So in my room, it's maybe currently 18 degrees Celsius, plus or minus some variants representing my uncertainty.
401660	413304	B	0.8691425919532776	Now, sampling from the distribution would be saying it is maybe 19 degrees or maybe 17 degrees, drawing a lot of different temperatures.
413352	427804	B	0.8423591256141663	So that if you put them all together as a histogram with their frequency, for example, if you ask me what temperature it is in my room, I would say there's a higher chance that I say 18 degrees.
427932	445240	B	0.7791666388511658	So I would say more often if you ask me this question many times, and if you put my answers, if you collect all my answers in a histogram, for example, I'd say 18 degrees more often, so that barp around 18 degrees would be higher and the other temperatures would be slightly lower.
445310	449800	B	0.6359720230102539	And if you aggregate that, you would get a Gaussian.
450140	451896	B	0.7008421421051025	Get the Gaussian we started with.
451998	454356	B	0.6916697025299072	So sampling, in other words.
454398	460620	B	0.7466819286346436	And very simply, it's like drawing a data point or an observation from an underlying probability distribution.
460960	470444	B	0.8713717460632324	You have a distribution, let's say a Bayesian that represents your most likely value of what the temperature in the room is and some uncertainty.
470572	476720	B	0.8870223760604858	And you just want to draw one or several observations from that distribution.
477140	483670	B	0.8922306299209595	And you want to draw these observations so as to preserve the overall statistics of the distribution that you started with.
484200	494308	B	0.8054336309432983	So that if you draw infinitely many data points or infinitely many observations from a distribution, then you can recover the original distribution.
494404	495930	B	0.5035158395767212	So that's what we want to do.
496300	500440	B	0.5806565284729004	And so why is this useful in the first place?
500510	506860	B	0.7359846234321594	Well, it just happens all the time in the things that you want to computer integrals.
508080	509484	B	0.57396399974823	It just happens all the time.
509522	526512	B	0.8248289227485657	So, for example, if you have a Bayesian inference problem, you observe some data in the world, it could be medical data, it could really be anything, and you have a prior and a likelihood of observing that data.
526566	529376	B	0.8402413725852966	So these are two terms in, like, Bayes rule.
529568	531030	B	0.6794587969779968	Then you want to know.
532120	539312	B	0.8625732064247131	Then if you apply Bayes rule, you get a posterior belief about what caused that data that you observed.
539456	547768	B	0.741083025932312	So, for example, a practical use case would be some medical data of a patient that's sick, and you want to know.
547934	553916	B	0.6520089507102966	So you get, for example, it's blood pressure, collection of symptoms and so on.
553938	562476	B	0.5194863080978394	So that's your data, and you want to know what kind of disease or what kind of function caused that data.
562658	573360	B	0.5159646272659302	So let's say typically you would have a model of, if I have this malfunction of this disease, I would have these symptoms.
573860	576748	B	0.7331330180168152	If I have this other disease, I would get these other symptoms.
576764	588950	B	0.6014450788497925	And so Bazrul enables you to infer in the best way possible, the disease that caused the symptoms that you just saw.
589400	616284	B	0.7799922823905945	So this is like a common thing, something that people use every day in statistics all the time, and arguably everybody uses it every day because the thesis of the Franca principle, or the result that it provides, is that you can describe exteroception of humans as doing basic inference.
616412	620610	B	0.5690315961837769	So basic inference is a very ubiquitous thing.
621940	630070	B	0.5399605631828308	You can use it to describe perception in humans, and you can also use it to do statistics and solve all kinds of problems.
630440	642090	B	0.49630802869796753	So, coming back to sampling, actually doing Bayesian inference, you have this problem of how do you compute the posterior distribution in the first place?
642460	654540	B	0.8183098435401917	And it turns out that this distinctions is generally intractable because you have kind of a term in the denominator that's very hard to compute.
655600	661064	B	0.8645722270011902	And so one ways to compute it is to use what's called Monte Carlo methods.
661192	663880	B	0.8566393852233887	And Monte Carlo methods are based on sampling.
664040	678832	B	0.822030246257782	So, long story short, in order to do Bayesian inference, you need to compute this, like, log likelihood term, which is a high dimension integral and one way to do it is by using Monte Carlo methods.
678976	681856	B	0.8566393852233887	And Monte Carlo methods are based on sampling.
682048	688790	B	0.8702874183654785	So the sampling enables you to approximate the high dimensional integral they started with.
689480	709192	B	0.6568971276283264	So now many listeners, or also the readers of this work will wonder, well, one thing that people do in the free hundred principle and also kind of like the red on that of the 300 principle, is you're actually not going to compute this denominator.
709256	720320	B	0.47266632318496704	That's very expensive to the Bayesian inference, but you're going to approximate the posterior by minimizing free energy, which is called variational inference.
720660	727244	B	0.8434816598892212	That's another method of doing Brett and coherence and it has many advantages over sampling.
727292	731644	B	0.6308000683784485	Sampling also has advantages with respect to variational inference.
731772	733330	B	0.7886413335800171	We can get into that.
735940	743320	B	0.631275475025177	So if that's what you're thinking, well, this is not necessarily interesting because we can do variational coherence.
743900	749320	B	0.7288158535957336	There's so many other places where we need sampling in statistics.
750540	757310	B	0.7751941084861755	I would say basically anytime you need to compute an integral and integral are all over the place.
758240	765710	B	0.9215285181999207	But I think the Bayesian inference implication is a nice one because it just speaks to all of us.
770020	774604	B	0.8337543606758118	So how does actually sampling getting to the picture of integration?
774652	777600	B	0.8528271913528442	Let's say we have an integral that we need to compute.
778340	783444	B	0.8623960018157959	An integral is basically, let's say you have a state space.
783562	787024	B	0.8208144307136536	So to be very simple, let's say the state space is my desk.
787152	792596	B	0.8691476583480835	It would be like a planner surface and we have a function that's defined on the desk.
792628	796730	B	0.8850492835044861	So this would be like a landscape over the desk, right?
797180	801690	B	0.6943301558494568	Like a 2D surface landscape that could be rocked or whatever.
802300	808300	B	0.8828346133232117	The integral of this function is going to be the volume between the landscape and the desk.
811200	816190	B	0.8677603602409363	This is kind of like the geometric picture of what an integral is.
818500	832340	B	0.8223941922187805	This turns out to be very hard to compute in practice because let's say you have a very rough surface and it's not clear how to compute all this volume, which is very irregular.
833640	841880	B	0.8882694840431213	What sampling does is it's going to pick a few locations on the desk, as many as you wish.
842460	844024	B	0.7553699612617493	So let's start by one.
844142	860780	B	0.8765745162963867	And it's going to compute the height under the function and then pick another location and compute that height as well, since sampling is going to pick locations on the desk at random.
861600	870352	B	0.8936803340911865	And from there it's going to build a histogram that's going to approximate the surface that you started with.
870486	878530	B	0.5751794576644897	And so histogram is very simple because you get a whole bunch of skyscrapers if you wish to approximate the surface that we started with.
878920	886500	B	0.5718815326690674	And computing the volume of skyscrapers is very simple because they're like squares.
887720	891184	B	0.7865641713142395	It's like depth times height times width.
891312	894250	B	0.6921674013137817	It's very simple to compute the volume of those things.
895660	900548	B	0.8103100061416626	So this is kind of the idea we want to computer the volume under a rock surface.
900724	910056	B	0.8844141364097595	And to do that, we're going to pick arbitrary locations on the desk, on the page space and build it.
910078	916412	B	0.5608956813812256	And at those locations build like skyscrapers at the height of the surface.
916556	923250	B	0.8388785719871521	So you get like a skyline, a histogram that approximates your original function.
923700	929836	B	0.719944417476654	And it's very simple to know the volume of the histogram, the volume of the skyline.
930028	935430	B	0.870845377445221	So from then you get an estimate of the integral that you started with.
936360	939152	B	0.8487318754196167	So that's kind of like the geometric picture.
939296	947668	B	0.8905975818634033	The beauty of this is that it works in the arbitrary dimensions and it doesn't suffer from the curse of dimension.
947764	952168	B	0.8953648805618286	So this is why it's so powerful, if you want to know.
952254	965180	B	0.8275569677352905	So there's a formula that tells you how well the volume of your skyline will approximate the volume of the volume that you initially wanted to compute the integral.
965940	981476	B	0.7845749855041504	So this difference between the integral and your estimate basically decreases as a function of the number of skyscrapers that you put in.
981578	984550	B	0.5009868144989014	The more skyscrapers, the more precise you get.
987160	998600	B	0.7452245354652405	And this bound on the error that you have on your integral that you started with, it doesn't depend on the dimension.
998940	1004552	B	0.8381607532501221	So as the higher dimension on your state space.
1004606	1014830	B	0.5780254602432251	So let's say let's imagine I had a high dimension desk, which is very hard to imagine, and I had a function on top of it, and I wanted to compute the area under the function.
1016500	1027212	B	0.6410989165306091	If my desk was very high dimensional, the higher the dimension, the harder it would be to solve this problem numerically.
1027356	1036500	B	0.8777948617935181	But with sampling, actually, the degree of accuracy that you get through sampling would be agnostic to the dimension.
1037320	1045320	B	0.9252296090126038	This is why it's so powerful and just used all the time in physics and machine learning.
1045470	1066412	B	0.47674259543418884	When you imagine practical problems that you have in machine learning, going back to medical data, you could get a whole bunch of data about a patient, which could be heart rate over time, blood oxygen levels, blood glucose levels, body temperature and whatnot.
1066476	1070144	B	0.8068793416023254	There's so many data modalities that you would have.
1070182	1073760	B	0.8363603353500366	So your state space would have many, many dimensions.
1075540	1088500	B	0.5236325860023499	And also the state space of the different diseases that could have given rise to all these symptoms would be also very high.
1088570	1096440	B	0.5054206252098083	So it could be that the patient has fever, or it could be that it has flu.
1096780	1097844	B	0.8616876006126404	Fever is a symptom.
1097892	1099732	B	0.5398349165916443	It could be that the disease is flu.
1099796	1102180	B	0.53347247838974	It could be that the disease is tuberculosis.
1102260	1113020	B	0.7533479928970337	It could be that the disease there's so many different conditions or problems that one might want to examine.
1114080	1125950	B	0.7874219417572021	So in this particular case of medical data, and this is just like a very simple use case, I would say you have a very high dimensional state space.
1126320	1131616	B	0.7946285009384155	And to debate an inference, you have a very high dimensional integral to estimate.
1131808	1140932	B	0.7390232086181641	And so one way to do that efficiently, or maybe efficiently, is disingenuous because it's just very hard.
1141066	1145256	B	0.6468465328216553	I mean, it's just very computationally intensive to do these things.
1145438	1163230	B	0.8266047239303589	But one way to do this, that state of the art, is through sampling just by drawing different points at random on this safe space and approximating the functional landscape and approximating the integral like that.
1164720	1169330	B	0.8939985036849976	So, yeah, this is really why this is so important.
1172340	1173090	A	0.918424665927887	Awesome.
1173540	1188688	A	0.8887039422988892	So we've made a sample where does these topics of accepting or rejecting a move amongst examples or discretizing, how do we pick that next sample?
1188784	1192120	A	0.7069787979125977	And then what does it mean to accept or reject that sample?
1193100	1194120	B	0.8635595440864563	That's a great question.
1194190	1197320	B	0.7066941857337952	This is basically like the heart of sampling.
1198540	1205848	B	0.533549427986145	If you knew how to do that in full generality, sampling would be sold, which it isn't.
1205944	1210030	B	0.8883886337280273	So I'm going to tell you a bit like where things stand.
1214960	1220524	B	0.8209726214408875	To do sampling, you need to pick places at random and you need to pick places at random.
1220572	1223596	B	0.8655645847320557	So that's to preserve probability distribution.
1223788	1234100	B	0.8417662978172302	So going back to the example of the desk, you want to pick in that case, you want to pick examples uniformly on the desk.
1235560	1246292	B	0.8649221062660217	If we come back to the previous example of like a Gaussian distribution, you want to pick samples that preserve the Gaussian distribution.
1246356	1252760	B	0.8516245484352112	So you ant to preserve to take a lot of samples around the mean very few samples on the tails.
1253840	1271650	B	0.6540917754173279	So it turns out that for those very simple distributions like uniform distribution, Galveston distribution, there are ways to sample that do not require what this paper is talking about.
1272580	1280630	B	0.5446512699127197	So there are ways to sample exactly that do not lead to any kind of error and that are very fast.
1281480	1291444	B	0.8718111515045166	So for Gaussians and uniform and a bunch of other distributions from the exponential family, you have ways to sample.
1291492	1292090	B	0.5662814974784851	Exactly.
1293020	1300904	B	0.6610894203186035	So the problem is these distinctions, they're not there all the time.
1301022	1309148	B	0.4973815977573395	And it turns out that very often you don't have these kind of very simple distributions, otherwise we would be done.
1309314	1317600	B	0.8810279369354248	So one simple example coming back to Bayesian inference is the idea of Jeffrey's Prior.
1318260	1331270	B	0.8653218150138855	So Jeffrey's Prior is the idea that you should take the highest entropy prior that's consistent with what you know about the system.
1335160	1337856	B	0.6896809339523315	Why entropy maximizing?
1337888	1343880	B	0.7375404238700867	So we know that entropy is a measure of the uncertainty present in the distribution.
1344860	1362808	B	0.8028578162193298	And so what Jeffrey's Prior says is that you should take a prior belief about a system, the belief that is maximally uncertain, uncertain, but that is also compatible with your knowledge.
1362984	1369516	B	0.7546787858009338	So out of all priors that are compatible with what you know about the system, you would take the one that's most uncertain.
1369708	1371970	B	0.72706139087677	And this seems pretty intuitive, right?
1372740	1384820	B	0.865246593952179	So when you take Jeffrey's Friar, what you get is often a distribution that's in the form of what people call a Gibbs measure.
1385560	1394250	B	0.871199369430542	So Gibbs measures or Gibbs distributions there of the form exponential minus V.
1395660	1401864	B	0.8627339601516724	So they're like proportional to exponential minus V, and V could be an arbitrary function.
1402062	1414008	B	0.8646393418312073	So just as an example, if V were like a parabola, a quadratic function, then the associated Gibbs distribution is a Gaussian.
1414184	1416652	B	0.670544445514679	So we all know what Gaussians are, right?
1416786	1419824	B	0.7444304823875427	But v could be pretty much anything.
1419942	1431412	B	0.7948645949363708	If you take V to be like X to the power of four minus X to the power of two, then V is kind of like a camel upside down.
1431466	1433380	B	0.7949564456939697	So, you know, it has like two humps.
1433800	1442040	B	0.8422262668609619	And therefore the exponential minus V distribution, the Gibbs measure, would be like a gaussian, but with two humps.
1443980	1452970	B	0.6771551966667175	So these gibs measures, they're just there like everywhere and they come up all the time.
1453840	1458140	B	0.8543921113014221	So one other example is in statistical mechanics.
1458720	1471884	B	0.8589495420455933	So let's say you have a system of molecules that just interact with each other, and this system of molecules, they could also be subject to thermal fluctuations.
1471932	1479830	B	0.6370729804039001	And whatnot this system is going to relax to what people call a steady state, often a non equilibrium steady state.
1480280	1491160	B	0.864870548248291	These steady states, which is kind of like the end configuration of the system, is in general a gives measure.
1495180	1504600	B	0.8032054901123047	Now, you can see that in statistical mechanics and also in statistics, gives measures come up also in basic inference.
1505020	1509980	B	0.6697787642478943	And it's very hard in general to sample from a gives measure.
1510880	1515920	B	0.5004652142524719	There's ways to do it and we're going to come back to it, but we need to solve this problem.
1515990	1517890	B	0.5108493566513062	It's a very important problem.
1518420	1528800	B	0.668763279914856	And so the most common way to solve from this distribution is what's called Markov Chain Monte Carlo.
1529400	1532064	B	0.8851051330566406	The idea is to run a Markov chain.
1532112	1533988	B	0.8162903785705566	So to run like a stochastic process.
1534074	1552120	B	0.8625413775444031	And you could think of it as stimulating molecules that interact with each other with some thermal fluctuations and that are going to relax to this gift distribution, this steady state that you originally specified.
1554060	1560604	B	0.8840217590332031	So coming back to the original examples, the steady state could be like a gaussian they want to sample from.
1560802	1563000	B	0.8692235350608826	Coming back to the example of the desk.
1563160	1567180	B	0.8930559754371643	It could be the uniform distribution on the desk that we want to sample from.
1567330	1569260	B	0.7811675667762756	It could also be a gift measure.
1571300	1589860	B	0.8604428768157959	So once we know the distribution that we want to examples from, we're going to design a stochastic process, some kind of random motion whose distribution is going to converge to the target distribution.
1590380	1600200	B	0.8610782623291016	So as you run the process, think of it as simulating those molecules that interact and that converge to your target.
1602540	1605176	B	0.5690306425094604	You're just going to simulate that.
1605278	1617580	B	0.8669321537017822	And once your system has converged to its steady state, which is the target distribution, then each movement on the steady state density is going to give you a new sample.
1620260	1629084	B	0.8713575601577759	Just for intuition, let's come back to the desk and say that we have this uniform measure, uniform distribution on the desk that we want to sample from.
1629222	1646410	B	0.7008802890777588	So we're going to design like a dynamic that moves randomly around and that's going to concept to the uniform distribution, which means that if you run it long enough, you're just going to sample everywhere on the desk and it's going to examples each point equally often.
1647100	1661150	B	0.866500973701477	And so you're just going to run this dynamic for a very long time and then you're going to collect the samples, collect the examples and use them to approximate the integral that you started with.
1662640	1665230	B	0.8084285855293274	So this is how sampling is done.
1665860	1674960	B	0.7532378435134888	Now, there's two basically one big question, one big outstanding question is how do you choose this dynamics?
1675620	1683700	B	0.903390645980835	How do you choose this random motion to converge to your target distinctions to sample from the distribution?
1684120	1696650	B	0.8837687373161316	So given a target distribution there's like a closed form formulas for all the possible choices of processes that you can use to sample from them.
1697100	1702810	B	0.7896592020988464	That's a closed problem and that's also explained in the paper.
1703520	1714270	B	0.7828046083450317	But then out of all of these processes you would like to know which one is going to sample the most efficiently and you also want to know.
1715380	1722832	B	0.8265167474746704	And second of all, a lot of the analysis of stochastic processes is done in continuous time.
1722966	1730000	B	0.8278328776359558	So you know that if you implement this process it's going to converge at this rate to the target distinctions.
1730080	1733030	B	0.6608873605728149	And so the faster it converges, the better.
1733960	1744856	B	0.7594028115272522	But it could be that when you simulate this on a computer, in a computer you can only have like discrete time steps or like discrete operations in discrete time.
1744878	1748730	B	0.5150863528251648	So you cannot really simulate this dynamic in continuous time.
1749100	1752900	B	0.694855809211731	So every time you simulate it, it's going to introduce some error.
1753060	1762504	B	0.6756100654602051	Now it could be that the thing that you simulate on your computer actually goes a lot slower than the original process that you wanted to simulate.
1762632	1765672	B	0.7912206649780273	So there's these two questions.
1765826	1772000	B	0.5446436405181885	How do you take a process that samples very efficiently, converges very fast to your target?
1772900	1787350	B	0.8110823631286621	But in addition to that, how do you stimulate the process accurately on your computer so that's to retain all the important characteristics of that process?
1789720	1803000	B	0.8934522271156311	So there's many different criteria and this is what we discussed essentially one method that state of the art or among the state of the art is called Hamiltonian Monte Carlo.
1803340	1815140	B	0.5801689624786377	Now, Hamiltonian Monte Carlo, without getting Hinton too much detail, it has two advantages.
1815320	1822364	B	0.6577730178833008	The first advantage is that it can be analyzed using mathematical methods.
1822412	1826780	B	0.8718938827514648	So you can get guarantees based on your target distribution.
1826860	1833430	B	0.787166178226471	You can get guarantees of how fast you're going to converge, how accurate your samples are, gain to be and so on.
1835160	1839456	B	0.8863415718078613	It has also many other desirable properties.
1839648	1846840	B	0.8165625929832458	So one of them which we discuss a lot in the paper is the importance of being irreversible.
1847340	1863800	B	0.7846112251281738	So irreversible means being irreversible means that if you run the process forward in time let's say you run the process forward in time for like 10 seconds.
1863960	1866044	B	0.772000253200531	Let's say you run the process backward in time.
1866082	1870540	B	0.5688272714614868	So you kind of run the process backward.
1870700	1877196	B	0.7698405981063843	You just take a movie and play it backward and look at how the process behaves.
1877388	1897400	B	0.7294064164161682	Now, a process is reversible if the forward and the Ballard movies are statistically indistinguishable and a process is irreversible if you run time as time goes forward.
1897470	1904140	B	0.7772075533866882	If you run time forward, it would look statistically different from if you run the process backward.
1910160	1914732	B	0.8365182876586914	I think you can kind of picture what irreversibility is.
1914866	1925276	B	0.7053534984588623	Now, irreversibility is crucial to do efficient sampling because if you're reversible, it means that you're going to backtrack.
1925308	1941140	B	0.7713639140129089	Very often when you're sampling positions on the decks, on the desk, say you're going to start somewhere, go somewhere else, and then there's a nontrivial chance, significant chance that you're going to come back to where you started.
1941290	1946360	B	0.49534130096435547	So it's going to be very slow to explore all the desk and build skyscrapers everywhere.
1948540	1951428	B	0.8122009634971619	So then sampling turns out to be very slow.
1951604	1958428	B	0.5511756539344788	Now, when you're irreversible, there's way less chances that you just come back to where you started.
1958514	1966510	B	0.5636274218559265	So you're going to explore the desk a lot faster because you're not allowed to come back to the same place so much.
1967360	1972080	B	0.8548659682273865	And so you're going to assemble faster and it's going to be much more efficient.
1973060	1986064	B	0.5400381088256836	So it's actually, by going from a reversible process for sampling to an irreversible process, you can gain orders and orders of magnitude, efficiency.
1986112	1988230	B	0.39599576592445374	It's actually pretty crazy.
1990600	1995000	B	0.8219718933105469	The Hamiltonian monte Carlo has guarantees.
1995980	1997480	B	0.6607088446617126	Is irreversible.
1998060	2000520	B	0.8117198348045349	It has other desirable properties.
2001660	2022930	B	0.6454331278800964	And the main one is that you can actually stimulate it on a computer in a way that the discretized version, which you actually simulate on the computer is going to be very close to the true Hamilton in Monte Carlo that you get by writing the equation down.
2024420	2032230	B	0.6436646580696106	So this is like why it's so big and why people use it.
2036520	2039030	B	0.6489751935005188	Yeah, I think this is pretty much it.
2039800	2053690	B	0.8394190073013306	Hamiltonia Monte Carlo has an accept rejects step, which is coming back to when you simulate a process on your computer.
2054140	2058632	B	0.8806372880935669	Every time you discreditize a process, you introduce some error.
2058776	2072080	B	0.6989268660545349	Now you want to make that discretization error as small as possible so that the process that you actually simulate on your computer is very close to what you want to simulate.
2073140	2092600	B	0.6682532429695129	So Hamiltonia Monte Carlo has a very important step called Metropolis Hastings, which is going to say, should I keep the sample or should I drop the sample and go to the next examples?
2093020	2101560	B	0.9244716763496399	And by using Metropolis Hastings, it's a very clever but also simple procedure.
2102060	2112664	B	0.7665324807167053	By using Metropolis Hastings, you guarantee that the distribution that you're going to sample is exactly the same as the distribution that you want to sample.
2112792	2122800	B	0.8429892659187317	So just to put in other, maybe simpler terms, so you have this process that you want to simulate.
2123380	2128800	B	0.9153571724891663	This process is going to sample your target distribution.
2129140	2138976	B	0.8606705069541931	Now, it could be that when you discrete the process, because the discrete process is different, you're going to sample from a slightly different distinctions.
2139168	2150456	B	0.7258619070053101	Now, by using Metropolis Hastings, you're going to guarantee that the distribution that you sample from in practice is exactly the same as what you really want to sample from.
2150638	2160776	B	0.6358885765075684	And so that's very important because it just tells you that asymptotically as the number of examples just growth to infinity.
2160968	2166690	B	0.6420217752456665	You're going to recover exactly what you wanted to recover, which is the adamant shrine to grow.
2172820	2173570	A	0.918424665927887	Awesome.
2173940	2192308	A	0.8016921281814575	I remember the Metropolis Hastings and the Monte Carlo Markov chains in phylogenetics where you might have many, many species and many, many locations in the genome that you're doing phylogenetic inference over and you just sample, sample, sample.
2192404	2199800	A	0.7473383545875549	And so all these techniques help accelerate what's possible with any given computational hardware.
2201500	2206796	A	0.9068362712860107	So how would you bring this towards adaptive agents?
2206978	2220290	A	0.8817732930183411	Are we thinking about this sampling process as being guided by an adaptive agent and or are we sampling distributions about an adaptive agent?
2221140	2222976	B	0.8635595440864563	That's a great question.
2223078	2228610	B	0.8528600931167603	And I think it sort of brings us to the conclusion of this work.
2231080	2247780	B	0.6172723770141602	In order to have adaptive agents and to scale adaptive agents so that they're able to solve complex problems and you're able to implement them with finite computational resources, one useful tool is sampling.
2247940	2253900	B	0.8385580778121948	And we'll see many different places where sampling can be used and has been used in active inference.
2257280	2278240	B	0.8607945442199707	And that's basically what you said sampling is at the heart of sampling is choosing data points intelligently so as to approximate your integral or whatever it is that you want to do with those samples.
2278680	2286484	B	0.8338425755500793	So you could think of active agents as solving the sampling problem.
2286682	2290150	B	0.7839557528495789	You could think of an active inference agent.
2290920	2293808	B	0.5647286176681519	Let's say that you have a problem of sampling.
2293984	2302090	B	0.79062420129776	You could think of an agent, an active inference agent that's going to choose, oh, I'm going to sample here, and I'm going to sample here, and I'm going to sample here.
2302460	2308060	B	0.7874343991279602	So active inference agent in the proper sense of the word.
2308130	2310030	B	0.7639434337615967	They do sampling all the time.
2311040	2329830	B	0.841355562210083	The only I would say, slight difference with what we've just discussed is that active inference agents, they sample observations so that they comply with their preferences and they observations that also bring them new information about the world.
2330520	2339430	B	0.8770402073860168	So the objective that active inference agents use to select new sample is the expected free energy.
2340380	2356670	B	0.7340620756149292	Typically we select actions or policies that have the lowest expected free energy, which means that the resulting observations will be close to your preferences or goal and also bring you new information about the world.
2358560	2371040	B	0.8837197422981262	But you could also conceivably think of an agent as selecting actions so that the resulting samples or observations accurately sample a distribution.
2373460	2382256	B	0.7711046934127808	So how could sampling be used in the context active coherence lab and adaptive agents?
2382438	2395124	B	0.9679906368255615	And so this is actually a great slide, if you look at the upper panel is what I just discussed.
2395172	2410276	B	0.8604651093482971	So in the first equation that's up there, you see that the sequence of actions that you chaos in active inference is the one that minimizes this minus lot T, this probes distribution over action sequences.
2410468	2413790	B	0.7922601103782654	So this minus lot T is actually the expected free energy.
2414640	2421916	B	0.8382057547569275	So we choose action sequences that mean by the expected free energy, as you see on the equation just below the expected.
2421948	2425436	B	0.6969391107559204	Free energy is a sum of risk and ambiguity.
2425628	2432390	B	0.7852687835693359	So you're going to choose action sequences that minimize risk, but that also minimize ambiguity about the world.
2432840	2438948	B	0.7642988562583923	And in other words, as shown below, you're going to choose action sequences that maximize extrinsic value.
2439034	2443320	B	0.8310847282409668	So you could think of that as an expected reward and intrinsic value.
2443390	2449850	B	0.6906255483627319	So that would be expected information gain value that's intrinsic because you're getting information.
2452300	2463260	B	0.8077990412712097	So really the key here is that this expected free energy, as its name suggests, it's given by an expectation.
2463840	2468430	B	0.8039047718048096	So you see this minus lock p on the left equals expectation of something.
2468800	2493960	B	0.6014170050621033	Now, when you have a very big generative model, a very big world model, which is going to be the case all the time, when you have, like, a real world application or a complex application, because the world around us is so high dimensional, so complex, well, the expected energy then is going to be a high dimensional expectation.
2494300	2495668	B	0.7162591218948364	What is an expectation?
2495764	2499480	B	0.8684354424476624	It's an integral with respect to probability distribution.
2500140	2513100	B	0.8698642253875732	So the point I want to get to is that to do decision making, you need to evaluate the expected free energy on different action sequences.
2513600	2519200	B	0.8549668192863464	And this expected free energy is an expectation with respect to probability distribution.
2520020	2527600	B	0.4819284677505493	It's a very high dimensional integration problem, very high dimension integration expectation.
2528180	2531752	B	0.7985036373138428	And so how do you approximate that while you use template?
2531916	2541750	B	0.8432750105857849	And I think the first work that actually used that in the context of active inference is the work by Fuentes et al.
2542680	2544150	B	0.6641857028007507	2020.
2546220	2563400	B	0.8436266183853149	I think it's called scaling active inference with Monte Carlo methods.
2563560	2565230	B	0.5366709232330322	So this is what they did.
2566160	2568620	B	0.90191650390625	I think it was published in New Ritz.
2569280	2571576	B	0.9051231741905212	It's a very big machine learning conference.
2571768	2590390	B	0.7020980715751648	And their observation was, okay, well, we have this active inference method which theoretically is very powerful, I mean, in the sense that you have this expected free energy objectives, that puts a lot of known constructs together.
2591480	2597444	B	0.7490842342376709	So the expected free energy, as shown in figure three here, puts together this risk.
2597492	2608110	B	0.6462442874908447	So this is the divergence between your predicted distribution of where you're going to be in the world and your target distribution of where you want to be in the world.
2608480	2618910	B	0.8169910907745361	And it has this ambiguity term as well, which has some nice links with phenomena in psychology, like the streetlight affect.
2619600	2623580	B	0.8069502711296082	So when you minimize we introduce actions that minimize ambiguity.
2623660	2630470	B	0.7661454677581787	You can recover phenomenology that's known in psychology, like the streetlight affect or the drunkard search.
2631880	2639008	B	0.8379670977592468	Risk is also an objective that's used in engineering, in controller inference.
2639024	2641148	B	0.8742926716804504	So you can see it on the left panel.
2641344	2647476	B	0.9111934304237366	So there are a few methods for doing adaptive agents that use this KL term.
2647508	2654350	B	0.8619559407234192	One would be controlled coherence, which is also known as maximum entropy reinforcement learning and KL control.
2655040	2678210	B	0.7940248250961304	And also, I think, although it needs to be investigated further, it hasn't been formally, I think, so far, but with the KL turn there, you can recover predictions that are made by prospect theory which is a big theory from the end of the 20th century about decision making in human agents.
2679240	2691724	B	0.6536708474159241	So this is, I think, worthwhile to look into further but I think there's some very clear links between that theory and the scale term in the expected franca.
2691872	2697780	B	0.7878621220588684	Anyway, so you have this expected free energy objective that's very powerful.
2697860	2700728	B	0.5829493403434753	Chaos puts a lot of things together.
2700814	2703290	B	0.967191219329834	So I think theoretically it's very nice.
2704320	2718352	B	0.8852275013923645	In the last line of the upper panel you can also see that you can view expected PNG as weighing expected reward and expected information gain, extrinsic value, intrinsic value and extrinsic value.
2718406	2726400	B	0.8432595729827881	It's used in expected utility theory, basic decision theory, RL, optimal control.
2726470	2736064	B	0.7296043634414673	So these are all like equivalent names for maximizing this extrinsic value and intrinsic value, expected information gain.
2736192	2746452	B	0.8432883024215698	It's also used in different places and for example in Bayesian experimental design which is in statistics.
2746516	2760648	B	0.8039807677268982	It was a seminal paper by Lindley in 1956 and Lindley basically said, okay, well, suppose that you you can do two experiments or many different experiments.
2760744	2762524	B	0.8559693098068237	How should you choose between them?
2762562	2765292	B	0.7705315947532654	What's the best experiment that you should do?
2765426	2773810	B	0.7082439661026001	And the answer that he came up with is you should do the experiment that gives you the most information about the world.
2775460	2780224	B	0.818840503692627	So you have to keep in mind that in that paper there was no extrinsic goal.
2780272	2787504	B	0.6446006894111633	It was not like we're going to do an experiment to solve this problem or achieve these results.
2787552	2791816	B	0.7956556677818298	It was only doing an experiment for the sake of it.
2791838	2798212	B	0.5005502700805664	And so he came to the conclusion that the best experiment was the one that maximized the expected information gain.
2798356	2818412	B	0.8862728476524353	Now suppose that you have a goal in mind target, target result or whatnot then you would weigh reasonably and one thing you could do is weigh the expected information gain with your expected reward or expected utility or expected how close you're going to get to your goal.
2818556	2821440	B	0.6142399311065674	And so this is exactly what the expected free energy does.
2821510	2825440	B	0.721270740032196	It puts these two things on the same footing.
2828580	2848810	B	0.9557996392250061	So you have this expected free energy which is very nice and which is what active inference brings to the table and what Fuentes et al did in their paper is, okay, well, we have this cool method, active inference, let's use it to solve nontrivial machine learning, slash reinforcement learning problems.
2850300	2867380	B	0.6975759863853455	And so when you actually want to solve those problems you have some high dimensional generative models, high dimensional state spaces that just come up and you need time plane to evaluate to approximate the expected principle.
2867400	2875330	B	0.787295401096344	So this is what they did and they also put together a lot of other cool tricks based on neural networks and so on.
2876420	2879010	B	0.9867618083953857	So a very cool paper, totally recommend.
2879400	2894724	B	0.6466518640518188	But I think the point it suggests, and this is also the point that we make, although in a very different way in this paper is that to actually do decision making efficiently in practice efficiently.
2894772	2897720	B	0.5192157626152039	But also you want to do take good decisions.
2898220	2899880	B	0.670101523399353	You actually need sampling.
2905680	2908028	B	0.715196967124939	And this is not a new point, by the way.
2908194	2912350	B	0.8553129434585571	I think many people would agree with this.
2913040	2924364	B	0.8522648811340332	But I think the active inference approach is nice because you have this explicit characterization of expected PNG as an expectation of something.
2924482	2931430	B	0.6638972759246826	So it becomes pretty clear that you want to do sampling because this is what sampling is made for.
2933080	2941060	B	0.8324581980705261	So I initially introduction sampling as you know, wanting to approximate integral, but expectations are integral.
2942040	2944632	B	0.48890218138694763	So yes, sampling is really what you want.
2944686	2950010	B	0.8174776434898376	It just comes up like extremely directly when you want to do these kind of things.
2951900	2952600	B	0.918424665927887	Awesome.
2952750	2953256	A	0.6283750534057617	Thanks.
2953358	2968160	A	0.8647913932800293	And one sensory example of information driven sampling is the eye circade the movement of the eyes, which also has been modeled in many works from the active inference perspective.
2968500	2996084	A	0.4883096516132355	And it's even below our level of conscious awareness with the eyes darting around to reduce their uncertainty such that our visual generative model can have high resolution and color throughout the visual field, even though that doesn't reflect the anatomy of the retina, which has a blind spot and differential resolution and color attention in the periphery.
2996212	3008460	A	0.8380268812179565	So I kind of see that as slam dunk evidence that our visual experience and by extension other sensory modalities are coming from the generative model.
3008610	3022690	A	0.7217418551445007	They're not just received and processed in an inward bound fashion and that adaptive sampling is vital to maintain the coherence of that generative model.
3024420	3025170	B	0.4650449752807617	Absolutely.
3026260	3031856	B	0.860055685043335	You want to do adaptive sampling in order to preserve the structure.
3031888	3039300	B	0.8380554914474487	I mean, you have this beautiful structure in the expected free energy of expected information gain.
3039640	3046356	B	0.6753988265991211	So let's say we're just like wanting we're just like looking at stuff, so we don't have any direct goal.
3046548	3064750	B	0.7858863472938538	The expected energy reduces to expect information gain or approximates that and you just want to sample sampling is the key to approximate this expected information gain term while preserving the statistics of the generative model.
3065360	3070800	B	0.5657603740692139	So it's very central to scaling active coherence.
3071460	3082500	B	0.8146977424621582	I wouldn't say it's central, active inference lab proper just the theory because in the theory you get these expectations and you know how to select actions.
3082840	3093560	B	0.7498539090156555	But anytime you want to scale active coherence, deploy active inference to solve a problem you want to, sampling is going to come into play inevitably.
3099900	3103160	B	0.7899154424667358	There's other ways to scale active inference.
3104060	3108472	B	0.8386553525924683	One other way could be what's called amortizing.
3108616	3109368	B	0.6035633087158203	Amortization.
3109464	3110856	B	0.8052310347557068	So with deep neural networks.
3110888	3119730	B	0.8839114904403687	So one thing you could do is train a neural network to predict the expected free energy based on your genital model and near sensory data.
3120820	3127632	B	0.5476895570755005	But this amortization procedure, as does any training of neural networks, it's going to require a lot of data.
3127686	3128968	B	0.5110225081443787	So it's going to be slow.
3129084	3131590	B	0.515727162361145	It's not something that you can deploy right away.
3132440	3146852	B	0.7251466512680054	So this is very nice way of I mean, amortization is a very nice way of doing things, but it's not something that you can use like the first time you active inference lab to solve a new task.
3146996	3171368	B	0.4918423891067505	So a nice way of scaling active coherence and it's not the only way, or it's not going to solve all the problems, but it would be to use sampling to approximate the expected free energy to make decisions in real time as you accumulate data, then you can train a neural network to predict the expected free energy.
3171554	3174272	B	0.7045766711235046	I think this is exactly what Fuentes et al.
3174326	3176192	B	0.8558547496795654	Did in their paper, by the way.
3176326	3178850	B	0.7106313109397888	So if you're interested, make sure to check it out.
3182020	3183170	A	0.7147812247276306	Figure three.
3183620	3204170	A	0.7441824078559875	It's something that Vaughn can just look ant for so long, because the imperative, what is being utilized in action selection by an active inference agent is something like a generalization of all the words that we see written lower.
3204800	3214988	A	0.5468045473098755	And unless we knew about that generalization or that unified imperative, it seems like these are quite literally orthogonal or disparate from each other.
3215074	3226556	A	0.6890316605567932	I mean, what could be more different than maximum information, gain oriented strategies and maximum reward driven strategies?
3226748	3235716	A	0.8224030137062073	And sometimes to get both those flavors in the same model, people might try to coerce one into the other.
3235818	3247784	A	0.8716835379600525	Usually we see that in terms of a novelty bonus or an exploratory impulse bolted on to a reward or a pragmatic value driven agent.
3247982	3257944	A	0.8273022174835205	And so once you start modifying the models and just adding in these arbitrarily constructed components, you may get adaptive behavior.
3258072	3263020	A	0.6623053550720215	It's never been a claim in the active space that other models don't have efficacy.
3263760	3282470	A	0.82560133934021	Rather that by thinking about all of these special cases as being situationally arising from a more general imperative with expected free energy for future oriented policy selection, we might gain the ability to do what?
3284440	3306780	A	0.8252924084663391	What will we gain conceptually or in practice by taking situations where today people are using maybe just one of these terms as an imperative for what their models are doing and what do we gain or what might we expect from potentially generalizing those imperatives?
3308960	3312350	B	0.5422143936157227	That's a brilliant and very difficult question.
3313360	3344100	B	0.7364433407783508	It's also a question that I would say everybody active coherence lab gets asked at some point because, let's say people doing reinforcement learning, they use many different objectives that would be tailored to solving one particular task or a different task, and they would have these ad hoc novelty bonuses that would work very well in certain tasks.
3346300	3347960	B	0.8074573874473572	It's a different approach.
3350540	3357500	B	0.8203505873680115	This approach that I was just mentioning with reinforcement learning tests to be it's much more bottom up.
3357650	3363996	B	0.6308146119117737	It's like we want to have an agent that solves this problem, makes this work, and so on.
3364018	3370210	B	0.5508816838264465	So we're going to start building and test it until it works.
3371460	3374530	B	0.6342679262161255	Active inference lab approach is top down.
3375380	3381776	B	0.8446887135505676	So it starts from the furniture principle and it's very theoretical.
3381888	3395480	B	0.8030169606208801	And the approach is like, okay, well, let's model how an agent interacts with its environment, the force generic kind of agent, and let's impose constraints that an agent has to satisfy.
3395900	3406120	B	0.874029815196991	So the basic constraint that we have in this paper is that you have three sets of variables.
3406280	3412444	B	0.5555802583694458	You have variables in the external world that you cannot directly have access to.
3412642	3416830	B	0.8720809817314148	So this would be maybe the temperature in the room.
3418100	3422400	B	0.7681023478507996	Then you have sensory variables or observations.
3423140	3433732	B	0.8657026886940002	So by the way, the environmental variables that are denoted as S, the sensory observations, are the observatory or the observations that are denoted as O.
3433866	3445368	B	0.8458332419395447	So this is what belongs to the world and you have direct access to this would be like your sensations, what you see and so on.
3445534	3458300	B	0.8736046552658081	And then you have the action denoted in the paper by A, which is basically what you can do, and the different things, the different options that you can choose from at any point in time.
3458450	3464168	B	0.6384032368659973	And this is what will enable you to influence the world S and influence your observations.
3464264	3465004	B	0.5056697130203247	O.
3465202	3499640	B	0.714535117149353	So we start from a very generic model of an agent and how it interacts with the world, these three sets of variables that evolve in time and just by adding a few other, I would say, constraints that agents satisfy, or intelligent agents hopefully satisfy, that these agents can be described as taking decisions that minimize expected free energy.
3499790	3505740	B	0.8442035913467407	So the expected free energy just comes up from this general theory of how agents behave.
3507680	3514700	B	0.9049167633056641	So to answer your question, what does this actually bring in practice?
3515040	3522720	B	0.7051496505737305	Well, first of all, there's many methods out there that are similar to active inference.
3523540	3526850	B	0.7605882287025452	In some cases, they're almost the same.
3528100	3535248	B	0.8123792409896851	The first difference, which is not necessarily an advantage or a disadvantage, but it's the approach.
3535424	3539940	B	0.649267852306366	These are the methods that you see out there that come very close to active inference.
3540600	3553770	B	0.8230685591697693	They have been designed from a bottom up approach of like, we want to model this system or we want to solve this task, so we're going to add all the components that are needed until our agent does what we want it to do.
3554320	3560892	B	0.5965623259544373	And it turns out that sometimes all these ingredients turn out to be exactly what we get.
3560946	3563150	B	0.9073621034622192	Active Inference lab or very similar.
3565200	3570480	B	0.7118140459060669	Other times, maybe the task would be simpler and you would get less ingredients.
3571060	3577600	B	0.8476135730743408	The disadvantage of that, I would say, is that it can turn out to be very messy.
3577940	3592760	B	0.7819100618362427	So you get a new task or a new thing to model, maybe a new paradigm that some subject would do, and you end up using a very different model, a very different objective to describe the behavior at hand.
3592910	3601050	B	0.636603832244873	So it's harder to have like a unified perspective of what's going on.
3601660	3609244	B	0.6723703742027283	Active Inference I would say the key contribution is that you get this unified perspective from it.
3609282	3618464	B	0.5735658407211304	You get this objective that puts together a lot of other constructs that we know and love and puts them together.
3618582	3630560	B	0.5447750687599182	And you know that with this objective you can do a lot of things, which is, for example, decision making that weighs exploration and exploration.
3631000	3639972	B	0.6614935398101807	Now, there are some challenges with active inference and anybody using active inference would be familiar with them.
3640106	3648824	B	0.7256639003753662	The first one is the scaling up because you have this approach, active inference, which is so complex in a way.
3648942	3652250	B	0.8035887479782104	I mean, you need to have a generative model of the world.
3652700	3657100	B	0.691512942314148	You need to have to be able to compute all these expectations.
3657440	3663900	B	0.8015390634536743	First of all, you need to be able to compute all these posterior distributions using bayes rule or variational inference.
3664800	3682050	B	0.8349354267120361	Then once you have those posterior or approximate posterior distributions based on your high dimensional model of the world, then you need to compute some expectations, very high dimensional expectation using sampling or whatnot, to then get the expected free energy.
3682420	3684724	B	0.5933631062507629	So the expected free energy doesn't come up for free.
3684762	3690068	B	0.5349498987197876	It's actually a very I would say it's a quite sophisticated thing.
3690234	3693450	B	0.7051099538803101	It puts all these things together, but it doesn't come for free.
3694460	3705080	B	0.4605933427810669	It's hard to implement on a practical problem that's nontrivial beyond toys and nations and people have done it, but it's a challenge.
3706860	3728240	B	0.6294653415679932	So the problem that active coherence has is, okay, well, we have this night theory, we have this very nice objective that theoretically works very well, and theoretically it's like or it's similar to what you would like to have for any kind of agenda.
3729640	3733430	B	0.7972264289855957	But we need to find ways of scaling this up.
3736440	3757340	B	0.539770781993866	Here comes the beauty of this, is that in scaling up active inference, you're going to use sampling, you're going to use techniques from optimization, from coherence, and so you're going to get something that's slightly different from the active inference scheme that you started with.
3757490	3766300	B	0.8983513116836548	So you had the active inference scheme proper, which was in the picture in the previous slide.
3769540	3773650	B	0.6770856976509094	This is active inference as you would like to implement it if you could.
3774580	3788710	B	0.5263655781745911	But because the problem is complicated and you have a high dimension state, space and so on, you have many things that are computationally intensive, you're going to use something, you're going to use optimization, you're gain to use inference, and so you're going to get something.
3789560	3798440	B	0.8110990524291992	Active inference lab algorithm that scales, but that's slightly different from what you started because it has all these other steps.
3799020	3805656	B	0.7762894034385681	Now, in doing so, the active coherence agent becomes engineered.
3805848	3819336	B	0.7241904139518738	So it becomes closer to these reinforcement learning and bottom up agents that are built in a bottom up way to solve certain tasks.
3819448	3827490	B	0.7193351984024048	Because you want to engineer the active impress algorithm to be able to work to solve a certain problem, to scale it in a certain way.
3828020	3832644	B	0.5355494618415833	So in doing so, active imprint becomes closer to reinforcement learning.
3832842	3843400	B	0.7904486060142517	Now, conversely, in reinforcement learning, the amount of compute is increasing.
3844780	3855930	B	0.6812536120414734	People want to build reinforcement learning agents that are increasingly learn in an unsupervised manner just because this scale is better.
3858480	3867052	B	0.6397735476493835	You can't always design all the components of your reinforcement learning agent to solve any single tax.
3867106	3873840	B	0.8961119651794434	It's just too impractical, especially when tasks become more and more complicated.
3874500	3885616	B	0.4995417594909668	You just would like general solutions that are going to work and are going to handle a lot of different types of situations.
3885808	3892520	B	0.880946695804596	So reinforcement learning is adding or subtracting ingredients to their algorithms.
3892940	3894920	B	0.7777285575866699	All these algorithms are evolving.
3895260	3926156	B	0.8221906423568726	But in some sense you can see this is debatable, I suppose, but you can see a general tendency towards moving to world models and generative models, moving to intrinsic motivation, which is another word for expected information gain or variants thereof, and an intrinsic motivation or intrinsic value that's added onto extrinsic value which is your expected reward.
3926348	3954410	B	0.6706238985061646	So you see many I mean, I think we're seeing reinforcement learning evolve in a direction also that is closer active inference lab because everybody wants algorithms that stole different tasks and not to have to change the algorithms for changing environments and so on.
3955040	3964620	B	0.8081890344619751	So in my view, even though we started with two different approaches, a bottom up approach and a top down approach, these are converging.
3965280	3982320	B	0.7899476885795593	Now to come back to the contribution of active inference, if one reads through the paper and one looks at the derivation active inference lab, the point is that it's very agnostic.
3982400	3985316	B	0.8281010389328003	It's agnostic to the environment that we're in.
3985498	3987860	B	0.8263018131256104	It's an agnostic to many things.
3987930	3989510	B	0.812286913394928	So it's very general.
3990200	4006780	B	0.570887565612793	And so if you take many practical reinforcement learning agent, you will see that they interact with their environment in a way that's compatible with the assumptions of the free hundred principle.
4007440	4020076	B	0.8551238179206848	In other words, the reinforcement learning agents can be recast as specific active inference agents with specific giant models.
4020268	4022208	B	0.5207100510597229	So this is really the key thing.
4022294	4025980	B	0.5906533002853394	It's this unifying perspective.
4026060	4037030	B	0.8252269625663757	I'm not saying it unifies everything, but I'm saying that many different reinforcement learning algorithms can be recast as active inference agents.
4037960	4059564	B	0.6974992156028748	And so this is not to say that active inference lab implementation of those reinforcement algorithms is going to be better, but it's more like a theoretical contribution of like this set of equations given by the expected primary is a complete recipe or almost complete recipe to generate adaptive agents.
4059762	4063676	B	0.8308742642402649	So in some sense you don't have to go any further than that.
4063858	4079330	B	0.6814004778862	And so you can view all these reinforcement learning algorithms that comply with the assumptions so that can be recast as active inference algorithms as ways of scaling active inference and making active inference work.
4079780	4097492	B	0.5625584721565247	So it's not like I don't see any kind of conflict between active inference and reinforcement learning, but it's more like the reinforcement learning algorithms that work well and that can be recast as active inference algorithms.
4097556	4110456	B	0.5969970226287842	And there are many, I would say more algorithms that can be recast and algorithm that can help when you look at the assumptions of the free energy principle that are super generic.
4110648	4121090	B	0.6341050863265991	So you can view all these algorithms as specific implementations of active inference in a way that active inference is meant to scale well.
4121540	4124720	B	0.7393815517425537	So this is kind of like the thing.
4124790	4134980	B	0.7724905610084534	And so I think ultimately, again, these two Fields are going to converge, but I do not see any tension between them.
4135050	4138950	B	0.8209860324859619	It's more like the tools from one can be used in the other.
4142380	4144328	B	0.9692632555961609	I see great work going forward.
4144414	4148216	B	0.8045368194580078	And this is kind of like what we want to do with this paper.
4148398	4162590	B	0.6275927424430847	Put this active coherence where it comes from very succinctly, and also make it accessible for people who don't know the jargon from the fringe principal and active coherence community.
4163120	4168930	B	0.9396905303001404	And yes, see the exciting developments that will follow.
4170740	4171200	A	0.7093686461448669	Wow.
4171270	4172476	A	0.8596616983413696	Great comments.
4172588	4192340	A	0.9013110399246216	Two things that reflects to me the constraints that you mentioned, which we sometimes call the particular partition essentially cleaving the particle, which you show in figure two, cleaving the particle off from the environment.
4192700	4196852	A	0.6895968317985535	This is something that's widely applied in agent based modeling.
4196916	4218400	A	0.7439512014389038	Just anytime you're talking about some field of action and a player, we're basically at least qualitatively within the space of partitioning agents from environments and then further saying, well, there's no edge between the internal and the external states, so there's no telepathy and there's no telekinesis and incoming information.
4218470	4220300	A	0.8541373610496521	We're going to call sense outgoing.
4220380	4230576	A	0.6943551898002625	We're going to call action that qualitatively and formally is basically consistent with almost any cybernetics formulation of adaptive action.
4230688	4240628	A	0.6919435262680054	So Axel constant are really quite minimal and seemingly being generalized year after year with the work so many colleagues are involved in.
4240794	4245160	A	0.7704817652702332	So Axel Costa are not onerous.
4245660	4251684	A	0.8653509020805359	And I think it's a really interesting question which reinforcement learnings are compatible?
4251812	4262840	A	0.792923092842102	And then coming from the other side, in a lot of active modeling contexts, we find ourselves sometimes proposing like auxiliary variables.
4263000	4265756	A	0.6605504751205444	Like, let's just do a parameter sweep over this.
4265938	4269052	A	0.8242542743682861	And then we'll look back to the textbook or to the paper.
4269106	4271552	A	0.8041602373123169	We think, well, where was that in expected free energy?
4271686	4274304	A	0.7875723838806152	We just proposed this random thing.
4274502	4275916	A	0.7514488101005554	Where was that in the equation?
4276028	4281656	A	0.9067192077636719	Or we'll look at step by step guide to active inference where it's all written in terms of matrices.
4281708	4283844	A	0.5053215622901917	It's all very read.
4283882	4285088	A	0.6315510272979736	It like a sentence.
4285264	4289392	A	0.7410699725151062	And all of a sudden it's like, wait a could be a neural network.
4289456	4291072	A	0.7781679034233093	It doesn't just have to be a matrix.
4291216	4300280	A	0.8613646626472473	So it's kind of like we bring in these methods and ingredients that are being used widely, empirically.
4301020	4303620	A	0.8218074440956116	And these are the two approaches.
4303700	4310030	A	0.849084198474884	Do we build the bottom up mosaic of approaches stopping when it works?
4311040	4326560	A	0.7753780484199524	And or do we start with this most general agent based cybernetic formulation and then kind of build the castle in the sky and meet in the middle again with something that works?
4326630	4330244	A	0.4425966739654541	It's just incredibly laid out.
4330442	4347770	A	0.8572709560394287	And this paper is at that saddle point because it has one hand or antenna or whatever in this smooth information geometry conceptual area.
4348220	4356750	A	0.8150429725646973	But the motivator of all of the conceptual moves that are made, which honestly are extensive, like page after page, it was just like, where's it going?
4357680	4362860	A	0.7693594694137573	It's going somewhere that can be simulated on everyday hardware.
4363280	4373360	A	0.8430994153022766	So many of the moves were about reshaping or reframing what was to be done in a way that could be simulated.
4373780	4396120	A	0.8363940119743347	And so those two paths are connecting and like you said, there's not necessarily attention, but it'll be quite interesting to see how this develops on the theory, practice and social frontiers as more and more of these threads start to combine.
4397420	4398570	B	0.5375386476516724	Yeah, I agree.
4398940	4404520	B	0.9774952530860901	I think it's particularly beautiful when theory meets practice.
4405520	4415976	B	0.5344339609146118	You oftentimes have practice that works, but you have no theory or you have a theory that's beautiful but that doesn't work in practice.
4416088	4419730	B	0.6615071296691895	And this just happens all the time.
4421300	4433524	B	0.4753882586956024	Typically you would have active that works and then you would try to build a theory out of it, but then often the theory turns out to be too hard, so you make some extra assumptions and so on.
4433562	4437424	B	0.5156877040863037	You end up with a theory, but the theory turns out to be quite removed from the practice.
4437472	4439140	B	0.7415814995765686	So you kind of get that gap.
4440040	4446440	B	0.9802727699279785	It's very beautiful and quite unique and also quite rare when the theory meets the practice.
4447580	4454620	B	0.8206186294555664	And I would say when the theory meets the practice and in particular meets the state of the art, then you kind of have a complete theory.
4456640	4474610	B	0.7975829839706421	So what we wanted to do in this paper is review these different Fields in a way that we could show how far the community has gone between making a theory that meets the state of the art.
4475380	4491884	B	0.8588773012161255	And every section was started with theoretical considerations about what should be when you need to solve a particular problem, let's say adaptive agents or sampling or optimization.
4492032	4506300	B	0.8325284123420715	And then from there, and from logical steps about, okay, well, it should be like this and not like that, because there is whatever kind of geometric argument or other arguments.
4506800	4514380	B	0.8731436729431152	And so from there, logically arriving at state of the art methods that are used in practice.
4516320	4525970	B	0.8320698738098145	The other kind of like sequence of logical steps was, okay, well, we're going to start with optimization which is in my view, at the root of everything.
4526980	4536724	B	0.7211796045303345	So sampling could be about optimizing samples to obtain the best approximation to your integral inferences about optimizing beliefs and decision making.
4536762	4540832	B	0.7776536345481873	It's about optimizing decisions based on their counterfactuals consequences.
4540896	4551130	B	0.6489261984825134	So we started up with optimization because it's the simplest thing both conceptually and also in terms of use case you need optimization to do everything else.
4552300	4560910	B	0.6357688903808594	We then went to sampling and to inference and finally to decision making because it just brings all these three ingredients together.
4561600	4573890	B	0.7583805322647095	We didn't go as far as showing, okay, well, this is how you should scale active inference in order to solve all the problems in the world because this just hasn't been done right.
4574420	4588096	B	0.5204963088035583	But we listed a few ingredients that have gone into the literature and also a few ingredients that this perspective offers to how to scale active inference and make it more effective.
4588208	4604160	B	0.674855649471283	So it's actually future work and open problem to use the techniques that have been developed in the sampling section and the sampling literature to make active inference even more scalable and same with optimization and same with coherence.
4604180	4615532	B	0.7032123804092407	So there's many things that have been put forward in this article and also elsewhere that can be used to scale active imprints but ant least.
4615586	4630900	B	0.6678239703178406	Yeah, the goal was to be as comprehensive as possible without sacrificing too much of actually understanding what's really going on and show how the theory means to practice throughout and really where the field stands.
4632360	4642516	A	0.8113481998443604	So one kind of reflection on this, again, is if we start with reward as our basal imperative pragmatism.
4642628	4648660	A	0.6545265316963196	I mean, after all, don't we want to get things done, achieve results in the world, realize our preferences?
4648820	4661260	A	0.8534163236618042	We start with pragmatism and then it is empirically ad hoc how people introduce these novelty bonus or intrinsic motivations.
4662560	4678880	A	0.8029389977455139	In contrast, we can start with this information gain approach and then the thumb is put on the scale to bring more and more emphasis onto the alignment with preference.
4679320	4685700	A	0.7860996723175049	So it's like two roads, two paths.
4686120	4717328	A	0.6640782952308655	And maybe this is our bias or corner of the information space that starting with a broader epistemic imperative allows the careful introduction of pragmatic value whereas a pragmatic foundation that it's hard to then recast epistemic value in terms of pragmatic value.
4717414	4720130	A	0.7437034249305725	That's kind of like the question of valuing basic research.
4720500	4731940	A	0.8320114612579346	And the approach taken here is develop an imperative that can look entirely like one or the other or mixtures.
4732600	4748570	A	0.6055014729499817	But what's always so perplexing is that it has this extrinsic and intrinsic value phrasing but there are other ways to decompose the function.
4749660	4758604	A	0.7203273177146912	So even intrinsic and extrinsic value are not necessarily the ingredients that were put in.
4758802	4768080	A	0.7802822589874268	It's more like they were two ingredients that were split out of something else that is much more integrated.
4768740	4783570	A	0.739431619644165	It wasn't like the free energy was constructed by composition of any given decomposition of which there are multiple for variational free energy and expected free energy.
4784760	4800232	A	0.7988862991333008	So then a question that I've often wondered is there might be two policies that are very close or have essentially identical expected free energy but they could be radically different.
4800366	4808968	A	0.661957323551178	For example, one could be having a good expected free energy because it realizes a lot of preferences.
4809144	4813500	A	0.5591030120849609	Another might because it provides a lot of information gain.
4814160	4846148	A	0.6297844648361206	So is it really as simple asterisk asterisk as a unified imperative or how do we make sense of the fact that one value could rank policies that might have, for example, extremely different danger profiles or envelopes of outcomes?
4846324	4857500	A	0.5392518639564514	How can that seemingly multidimensional space be projected into essentially a ranking?
4859040	4862510	B	0.6195951104164124	Yeah, it's a complicated question.
4864240	4868168	B	0.8619553446769714	As we can see, the expected free energy does this in some way.
4868354	4877920	B	0.767797589302063	It provides an answer to this question based on your preferences and how much you would gain by observing this new data, how much information you would gain.
4878420	4881232	B	0.807929515838623	You can put these two things on the same.
4881286	4881644	B	0.5463999509811401	Footing.
4881692	4885060	B	0.7744832634925842	This is what expected PNG does, and it gives you a ranking.
4889080	4894490	B	0.7358490228652954	You're right to say that two very different courses of action could have the same expected free energy.
4895980	4897960	B	0.8022545576095581	And so how do you choose between them?
4898110	4909484	B	0.8046368360519409	Well, the standard formulation that you would see in textbooks or papers has, okay, well, G of Action Sequence equals this and G of Sequence equals that.
4909602	4913324	B	0.496164470911026	So these two Action sequences have the same G and you don't know what to do.
4913522	4923516	B	0.6396084427833557	But here, this theoretical development in this paper makes it very clear that the expected PNG is a minus lock probability.
4923708	4924992	B	0.6691916584968567	And this is why.
4925126	4934144	B	0.7998448610305786	So to make it more transparent, instead of using the letter G that's commonly used in the literature, we kept the minus log P throughout.
4934192	4936070	B	0.5350099205970764	So we never used the letter G.
4936520	4941536	B	0.8381887674331665	And so the expected free energy is this minus log P of Action Sequence.
4941728	4954192	B	0.7826225161552429	And so it says, okay, well, if two Action Sequences have the same expected free energy, well, then you have, let's say you just have two of them with the same expected force energy, the lowest one, well, then you choose probabilistically.
4954276	4959710	B	0.752856433391571	Like half of the time you choose one, and half of the time you use the other.
4960320	4973344	B	0.5091747641563416	Actually, it's even a bit more complicated because since it's a minus lot P, you could have an Action Sequence with a very high expected free energy.
4973542	4976400	B	0.7660158276557922	And so this means that you can still take it.
4976470	4979250	B	0.6390078067779541	But if you would take it, like, not very often.
4979800	4985488	B	0.7664394974708557	So you have an exponentially low chance of taking that Action Sequence.
4985584	4988580	B	0.7636770009994507	So it's a probabilistic description.
4990600	4997112	B	0.849240243434906	It says that the most likely Action Sequence would be the argument, the minimum of the expected free energy.
4997166	4999848	B	0.8915347456932068	So this is like the top line in the figure.
5000014	5008030	B	0.8248172402381897	And so if you have two most likely Action Sequences, well, then you choose like half of the time you choose one, half of the time you choose the other.
5008800	5017730	B	0.7023653388023376	But then in general, if you didn't want to simulate the most likely Action Sequence, then you choose probabilistically between all of them.
5018820	5030640	B	0.6163660287857056	And the Action Sequences that have the lowest expected reality has an exponentially higher chance of being selected and the others don't.
5034340	5035490	B	0.8018792271614075	One last thing.
5035880	5053592	B	0.8300396203994751	Another very nice perspective is that the probability over Action sequences, so these P of a greater than equal to T is then the exponential of minus the expected free energy.
5053726	5064824	B	0.8417720794677734	Because if you take exponential minus of what you get in the equality on the left, then you recover this probability distribution of our Action Sequences.
5064952	5073408	B	0.8284757137298584	So this distribution of our Action Sequences is exponential minus expected free energy, exponential minus what's in there.
5073574	5079052	B	0.8124010562896729	And we saw before, that gives measures, they arise everywhere.
5079116	5081152	B	0.6699593663215637	And so this is an example of a gives measure.
5081216	5095480	B	0.8651019334793091	You have exponential minus something basically, based on the formalism of the free free energy principle comes up that the probability distribution over Actions is a gives measure.
5096860	5102250	B	0.8368096351623535	It is the gives measure where what's inside is the expected free energy.
5103260	5113800	B	0.8361801505088806	So, yeah, it's kind of like a full circle thing and this is basically this sort of like connectivity.
5113880	5125484	B	0.771564781665802	They just happen all the time when we were writing this paper and discussing because there's so many things that just there's so many crossovers at so many different levels.
5125532	5139030	B	0.8233592510223389	Like on the sort of object level that you're studying, but also on the ecological level, like methodologies that are used in some field, they're used in a different field to do something else.
5139560	5141548	B	0.705871045589447	So there's so many crossovers.
5141664	5151412	B	0.522638201713562	And so this is, I would say, an interesting example of why gives measures, gives distinctions, they just arise everywhere.
5151476	5157608	B	0.6204925179481506	They arise active inference Lab and as we saw before, they're hard to sample from.
5157774	5168988	B	0.8641307353973389	So if you wanted to so let's say you have your gives distribution that says, okay, well, this is my distribution of our action sequences that I get from the expected free energy.
5169154	5170760	B	0.6349196434020996	Then you have two choices.
5170920	5189780	B	0.8469387292861938	Either you sample from it to sample like an average, I would say not a typical action sequence, or you could optimize the distribution or optimize the expected free energy to sample the most likely action sequence.
5190760	5197560	B	0.8450832962989807	In the two cases after you've done your planning with the expected free energy, you have a choice between sampling and optimization.
5198220	5220648	B	0.8566834330558777	So you kind of go back full circle of like if I want to select my action gain, I either select the most likely action by optimizing the expected free energy or the gives measure by optimizing this minus lot P by taking the action sequence with the minimal expected free energy.
5220834	5227308	B	0.9054813385009766	Or I have my gives measure and it will sample from it to like get a typical action sequence.
5227484	5229840	B	0.7487248778343201	So it's all like super interrelated.
5233480	5240420	A	0.9490638971328735	Very interesting in the Par et al.
5240570	5265340	A	0.8849228024482727	Textbook from 2022, that dialectic is mapped onto the mammalian nervous system where policies can be selected by essentially passing through habit proportionally or expected free energy can be applied to sharpen or optimize the posterior on action.
5269380	5274720	A	0.8821364045143127	Does this influence how you select micro meso or macro actions?
5279300	5286740	B	0.8523964285850525	Do you mean for human being like actions at the cellular level, actions at the Global level?
5286810	5288788	B	0.8269888758659363	Or is it something different like at.
5288794	5301480	A	0.8332909345626831	The personal, grasping a coffee cup at the micro, larger scale decisions, attention allocation, communication emissions.
5303100	5311736	B	0.7860203981399536	Yeah, I think all of these things can be formulated in this treatment.
5311768	5319570	B	0.8828755617141724	So, for example, attention would be where you choose to place your focus or your mental eye, so to speak.
5320580	5326370	B	0.8404238224029541	So that's and internal action, it could be unconscious, it could be undirected, but still happening.
5328740	5336980	B	0.7948849201202393	I'm not an expert, but there's a lot of talking consciousness research of like, you being conscious.
5337640	5349800	B	0.7893893122673035	There's a very low amount of processes that are conscious and these change according to where you're inner eye or your conscious eye, whatever you want to call it, like moves.
5351020	5360380	B	0.8648356199264526	This could be thought of a mental action or an inner action that you may or may not control, but still and action.
5361600	5381060	B	0.878082811832428	So all these, all other action, whether it is planning at the short time scale, taking a coffee cup standing, or planning at the longer time scale, like what am I going to do after my PhD and so on, all of these things are action.
5382600	5394980	B	0.8378431797027588	So typically, as you know, of course, we we model that by using hierarchical models where you have different levels in your giant model.
5395050	5406540	B	0.7974250316619873	So in your model of the world and the higher levels are represent more abstract representations and also longer time scales.
5407760	5421952	B	0.7613276243209839	So it could be, let's say at some higher level I'm going to take the plane to go to Amsterdam, for example, and at the lower level, okay, well, once in Amsterdam, I'm going to do this, this and this.
5422006	5429410	B	0.6690903306007385	It's like this action at the higher level of taking the plane predates everything that you're going to do at the lower level.
5430040	5446116	B	0.49690186977386475	And so factorizing these different decisions into different levels of a model allow you to be completionally, practical, practically implementable when you make those decisions.
5446148	5457790	B	0.7555827498435974	And presumably this is why we do in our everyday life factorize decisions and representations into low level and high level things.
5459440	5464670	B	0.8695599436759949	The beauty of the free energy principle and of the treatment here.
5466000	5480930	B	0.8271756172180176	But the treatment here is again, just the free energy principle in its most bare bones and general form is that the expected free energy is formulated for any kind of gentle model.
5481700	5488144	B	0.8167423605918884	The gentle models that you have in the figure on the bottom right there, they could be arbitrary.
5488192	5503050	B	0.7968870997428894	So you could use this form of expected Lagrange for hierarchical generative models that have all these like high level, low level decisions and representations, but you could also use it for any other kind of generative model.
5504160	5523680	B	0.5530593395233154	The only difference is that with some world models, it would be very hard to implement this in practice and with other world models, and in particular those that are highly factorised and distributed like hierarchical models, like the wounds we just talked about.
5523830	5545060	B	0.5103318691253662	With these it would typically Dean much simpler to take decisions that minimize expected frequency when it comes to practical computation, just because when you action oriented representation things, just computations just factor out a lot and it's just like way simpler to do.
5545210	5554120	B	0.7406721115112305	But the formulation here is like entirely generic and it could apply to all kinds of models and all kinds of situations.
5559410	5560560	A	0.9324204325675964	Very interesting.
5560930	5574302	A	0.8736140131950378	Well, in our last little segment on the dot one, let's hear your perspective or overview on the roadmap.
5574446	5584598	A	0.9116264581680298	Just walk through what the sections are and you mentioned some related topics, but what do the subjects broadly cover?
5584684	5586760	A	0.5874700546264648	Why are they structured in that order?
5588010	5606350	B	0.7757858633995056	Right, well, first of all, the introduction just puts like everything in context, the conducting line or one of the common points between all of these sections is that there's a lot of geometry.
5607330	5615946	B	0.8392548561096191	So this was not purely because the journal is like the handbook of statistics, it was called Geometry and Statistics.
5615978	5623570	B	0.8119668364524841	So this is not just for that, but it's because geometry just comes up naturally in all these places.
5624390	5637922	B	0.8760951161384583	So this is what we explained in the introduction, like how geometry comes up, the different branches of geometry that come up in this different field and how they're interrelated.
5637986	5651930	B	0.8383933901786804	And so just from there you kind of get a picture of, okay, well, this branch, let's say symplectic geometry, for example, it comes up in accelerated optimization and it also comes up in sampling.
5652510	5668820	B	0.8744654655456543	And so you can already see from there what is going to be discussed, of course, but also which branches of geometry occur where and what kind of parallels there are already.
5670230	5675086	B	0.8664427995681763	So going from the introduction to accelerated optimization.
5675278	5679602	B	0.5107576847076416	So optimization is conceptually, it's a very simple thing.
5679736	5681778	B	0.6303084492683411	It's like, okay, we have a function.
5681944	5692950	B	0.8775504231452942	So coming back to, let's say that my Dean state space come back to where we started with and you have a function and a landscape over it, let's say like a mountain.
5694650	5698662	B	0.8127307891845703	And so the goal is to find the minimum of the mountain.
5698726	5701910	B	0.8096638917922974	I go from where you start to the minimum.
5702070	5708320	B	0.6000781059265137	There's so many different ways of solving this problem, practically speaking.
5709090	5717200	B	0.5178360939025879	The main challenge, of course, is that you don't know, you cannot speak anywhere else except where you have been.
5717990	5723294	B	0.6689292192459106	So you start somewhere and you're completely blind and you need to find a minimum.
5723342	5724660	B	0.7359687089920044	So how do you do that?
5725590	5729790	B	0.658012330532074	There's so many ways, so many approaches to optimization.
5729870	5734200	B	0.7519821524620056	I mean, it's a whole field and it's in my opinion, a very messy one.
5734890	5757760	B	0.5051854848861694	And this is understandable because there's no free lunch theorems that are very well known that say that under some hypotheses, two ways of doing optimization will behave the same on average if you test them again, all possible sorts of problems.
5759170	5773940	B	0.7144529819488525	So this is to say that if you have a given problem, then you need to give then the optimization methods that will work well are those that implicitly use the prior information that you have about the problem.
5774710	5786840	B	0.5930298566818237	In other words, according to the theorems, there's no optimization methods that will beat everything, it just doesn't exist because they're all on average the same.
5787210	5803242	B	0.6067925095558167	But this is not to say that you cannot make any practical, meaningful steps by doing optimization because not all optimization problems are the optimization problems that we have in life.
5803296	5810106	B	0.7493705153465271	They're not completely random, they're not completely random landscapes that we need to optimize, but they actually have some structure.
5810218	5821394	B	0.79457688331604	So we can actually explore that in algorithms and have things that just behave, that just work a lot better for the type of problems that we're interested in than others.
5821592	5828530	B	0.8715397715568542	So this is what kind of like the starting point for all optimization.
5830970	5836390	B	0.8441472053527832	The type of optimization that we considered here is optimization of smooth functions.
5837130	5841330	B	0.5154014229774475	This is already like a big restriction there's.
5841490	5852140	B	0.5520045757293701	So many methods that are designed to optimize functions that are rugged or non smooth or have discontinuities and so on.
5853070	5859006	B	0.5803329348564148	When you actually get rugged or discontinuous landscapes, it's a whole different problem.
5859188	5863870	B	0.5423968434333801	Here we chose to focus on smooth functions.
5866050	5873250	B	0.7327014207839966	The idea is that you have a smooth landscape, you want to find the minimum.
5874310	5884470	B	0.5943616628646851	And one way to do so, and to do so well, is through the use of a tool called geometric inclination.
5885610	5890630	B	0.9172869324684143	So I'll walk through the main ideas of this section.
5892410	5920610	B	0.48867836594581604	The idea, first of all, is that you may design a method that theoretically works very well, but when you implement it in practice on your computer, your discretion errors, because you can only implement it in discrete time, what you actually end up implementing ends up deferring significantly from what you initially wanted to implement.
5921350	5931538	B	0.5681548714637756	This is a problem that we discussed previously in sampling and it just comes up everywhere in applied mathematics and numerical mathematics.
5931714	5942022	B	0.645980954170227	The fact that you can only do discrete computations on your computer is a big implication and it's something that needs to be looked into.
5942076	5946202	B	0.4880687892436981	It's maybe even the most important thing that needs to be looked into.
5946256	5961470	B	0.6406688690185547	When building a numerical method, you want to be able to implement numerical methods that are very close from what you theoretically would like to implement.
5962450	5969466	B	0.8812650442123413	So typically you would have some theoretical algorithm that would have some convergence guarantees or performance guarantees.
5969578	5975490	B	0.823586106300354	And so you would like to wait a way to implement it on your computer so as to preserve those guarantees.
5976550	5979140	B	0.8818985819816589	And this turns out to be very hard in general.
5980710	5994070	B	0.759354829788208	One way to do it, I would say maybe the most powerful way out there to do this is through procedures from the field of geometric inclination.
5995450	6019220	B	0.6868619322776794	And so geometric integration is a set of techniques that allow you under certain cognition to discretize your theoretical, dynamic or theoretical optimization procedure like dynamical system, and make it into an algorithm that you can practically implement on a computer.
6019670	6025220	B	0.7365932464599609	And that's going to respect the performance guarantees that you already have.
6026550	6032450	B	0.7213740944862366	So now the tricky part, geometric inclination is based on geometry.
6032610	6045474	B	0.6336671710014343	So you need to find it's not as simple as like, okay, well, I have my landscape, my function landscape, and I'd like to just go down and find the minimum.
6045522	6050522	B	0.8448565006256104	One way to do that is just by doing grain in descent in continuous time.
6050576	6055222	B	0.5736070275306702	And you know that that's going to go down until it reaches a local minimum at the very least.
6055376	6056800	B	0.7597362399101257	So it's not too bad.
6057490	6067338	B	0.5570492148399353	If you were to implement gradient descent numerically, it wouldn't be exactly the same as like the continuous time dynamic.
6067434	6080114	B	0.5098434686660767	It could be that actually by implementing gradient descent numerically, you get all sorts of numerical problems or whatnot maybe grade and send.
6080152	6084082	B	0.7662104368209839	It's so simple it's not actually a good example.
6084216	6093690	B	0.657396137714386	But the point I want to make is that it's not as simple as like, okay, well, I have this dynamic that works really well, and that gets to the minimum.
6094030	6097580	B	0.5389930009841919	And I know that it's going to get to the minimum this fast.
6099150	6108926	B	0.6078991293907166	And so geometric integration will give me an algorithm that I can implement on my computer, and that's going to get to the minimum this fast as well.
6108948	6110720	B	0.6148961186408997	It's not as simple as this.
6111330	6119060	B	0.7618759274482727	Geometric integration works by preserving geometric structures in the dynamic that you started with.
6119510	6138374	B	0.5653095841407776	So in order to be able to apply geometric integration, to have algorithms that work for optimization, you need to design a dynamical system that converges to the minimum and that has some important geometric features that can be preserved in this way.
6138572	6145210	B	0.8777468204498291	And so this is basically the goal of the whole section, and it goes into the following steps.
6146830	6164750	B	0.7656775116920471	So one thing that can be discretized or implemented on a computer in a way that is close to the true dynamical system is what's called Hamiltonian flows.
6165090	6181410	B	0.8960031270980835	So if you specify a Hamiltonian, which is a kinetic energy and potential energy, then you get the Hamilton Jacobi equations of motion that gives you the behavior of the system that has the prescribed Hamiltonian.
6181930	6195250	B	0.8539818525314331	So for those who've taken a course in physics, you probably know or may have seen that all of Newtonian mechanics can be reformulated as Hamiltonian mechanics.
6195410	6204054	B	0.8755230903625488	So all of that can be described as with Newton, lows of motion can be described through Hamiltonian mechanics.
6204102	6225780	B	0.7513095736503601	It's a very general framework or a very general way to look at physics, because it's such a general point of view on physics and such a, and just Hamiltonians, they come up everywhere, this mixture of kinetic and potential energy, to concisely summarize the behavior of a system.
6226790	6235490	B	0.690839946269989	Because they occur so much, they have been studied extensively, and it turns out that Hamiltonians and specifically the motion they entail.
6235650	6250566	B	0.7081205248832703	So the Hamilton Jacobi equations of motion, they can be discretized or implemented accurately on a computer using Symplectic integration, which is a part of geometric integration.
6250758	6259360	B	0.7048017382621765	So in short, you can use Symplectic integration to accurately simulate a Hamiltonian dynamical system.
6260130	6269710	B	0.4902043342590332	Now, the problem is, these dynamics are conservative, so they preserve the Hamiltonian.
6269870	6282262	B	0.8057186007499695	So if you're at some point hidden states, space with a certain kinetic energy, potential energy, and you run the dynamics forward, your overall energy is not going to change.
6282316	6283602	B	0.6795823574066162	So that standard physics.
6283666	6296966	B	0.48636457324028015	Energy is always conserved when there's no friction, all kind of like ideal physical systems with no friction, the energy is conserved.
6296998	6306198	B	0.8806251287460327	So when you're going to run this Symplectic optimization scheme to simulate these Hamilton and Jacobi equations of motion, the energy is going to be conserved.
6306294	6317802	B	0.6722474694252014	Now, we're in trouble if we want to do optimization, because in optimization, if you think of the function as the energy, you want to dissipate energy as much as possible to reach the minimum.
6317866	6319380	B	0.5871987342834473	So you want to lose energy.
6320870	6335240	B	0.5625507831573486	So what the section does so this section, by the Way, is a review of more technical papers what these papers do, it's a very clever trick is they say, okay, well, we want to minimize a function.
6336490	6343750	B	0.8860536813735962	We're going to say the function is potential energy and also we're going to add a kinetic energy on top of that to get a Hamiltonian.
6346670	6357054	B	0.7673704028129578	Now this Hamiltonian, we could simulate it through symplectic integration, but actually, if we did, we wouldn't dissipate the function.
6357252	6376180	B	0.5629185438156128	So now what the section does is it introduces a higher dimensional state space with the Hamiltonian that if you simulate those dynamics, it actually optimizes the function.
6379030	6388690	B	0.736024796962738	So this is what was a convoluted way to say it reformulates optimization as a conservative system with a different Hamiltonian.
6388850	6405580	B	0.6004883646965027	And so by and because we know how to simulate dynamics with Hamiltonian so well, a good way of doing optimization is by finding a Hamiltonian so that when you discretize it, you get the optimization process that you wanted.
6406190	6411834	B	0.8206589221954346	If you do that, you get a numerical method that's stable, that works well, that has good coherence guarantees.
6411962	6422682	B	0.6501927375793457	And it turns out the numerical methods that are developed, they work for optimization on any kind of smooth manifold, for function, on any smooth functions, on any smooth manifold.
6422826	6423742	B	0.776578962802887	They work well.
6423796	6429758	B	0.8215511441230774	They are generalized gradient descent in the sense that you recover gradient descent in a certain limit.
6429934	6443080	B	0.6517943739891052	And they can be seen as accelerated because they have this notion of acceleration in the sense that, let's say the function that you want to optimize, it like a hill and you want to go down.
6443450	6456534	B	0.5189778208732605	If you just did great, in the sense you're like blindingly going down at the same speed, or your speed is like proportional to the slope of the landscape.
6456662	6462030	B	0.6753935217857361	But when you actually let a Bull rolling down, this is not really what happens.
6462100	6471790	B	0.7159493565559387	The Bull is going to roll faster and faster and it's going to accelerate, it's going to go down, it's going to overshoot the minimum and then it's going to like oscillate until it converges to a minimum.
6472450	6475954	B	0.8646251559257507	So these are the kind of optimization methods that are developed here.
6475992	6490950	B	0.7819720506668091	They're optimization methods that are accelerated in a physical sense, that has physical sense acceleration and physical meaning because they're going to accelerate as they go down, overshoot and then stabilize.
6491690	6495398	B	0.8120583891868591	So this is the whole point of this section.
6495574	6502700	B	0.7446446418762207	Now briefly, because I think we're running out of time, but we can of course, revisit these sections later.
6505650	6506874	B	0.7301478981971741	In the third action.
6506922	6507706	B	0.7834464311599731	We have sampling.
6507738	6510506	B	0.8316184282302856	So we talked extensively about sampling.
6510698	6518042	B	0.8755719661712646	The sampling, as we discussed, can be seen as an optimization of probability distributions.
6518186	6533230	B	0.8193378448486328	So we have the distribution that we want to sample from, and we just run a process that is random so it can be described by its own distribution and want the process to concept as fast as possible to the target.
6533390	6541670	B	0.814541220664978	So we want that distribution of the process to concept as fast as possible to the target distinctions.
6542910	6549402	B	0.5199530720710754	So in a way that can be seen as I guess complicated for many people.
6549456	6551370	B	0.5828786492347717	I think it's pretty abstract.
6552590	6560350	B	0.8209788203239441	But it's been known for a long time that you can reformulate sampling as just optimization of distributions.
6561810	6589186	B	0.7778937220573425	Now it turns out that when you use the accelerated optimization method of the second section to sampling so when you use, when you do accelerated optimization on the space of probability distributions in order to solve the sampling problem, you get a process that's called under damp launch one dynamics.
6589378	6605974	B	0.760367751121521	So if you solve that process, you're basically going to get good samples, very good samples of your target distribution because that process can be seen as doing an accelerated optimization on the space of probability distributions.
6606102	6610634	B	0.8427286148071289	In other words, the distribution describing the process is going to accelerate towards the target.
6610682	6615280	B	0.9575856328010559	So you're going to get there very fast and your symbols are going to be very good.
6617090	6624130	B	0.8829679489135742	So this is 2.6 where this is kind of derived.
6624470	6633320	B	0.7230960130691528	In 3.1, we basically talk about the the good properties of samplers in general.
6633770	6647450	B	0.5021767616271973	So sampling is not like a complete field, but there's a lot of work showing that certain processes have certain characteristics of processes make them better samplers and others make them worse.
6648350	6649786	B	0.8358461260795593	So this is what we discuss.
6649888	6658650	B	0.7724944353103638	And then in free .2, we put this all together in Hamiltonian Monte Carlo, which is a state of the art method for sampling.
6663250	6678338	B	0.6485593318939209	Hamiltonian Monte Carlo is a way of using, if you use symplectic integration under Downplantromodynamics, which is your accelerated sampling method, that you cannot stimulate directly, again, you get the same problem.
6678504	6684142	B	0.839667022228241	If you use symplectic integration to simulate that, then you get HamiltonI Monte Carlo.
6684286	6693510	B	0.6860259771347046	So Hamiltonian Monte Carlo is attributes Google way of doing accelerated sampling that's based on ideas of symplectic integration.
6695710	6700970	B	0.9123099446296692	Now, very briefly, the section four is on inference.
6702750	6707546	B	0.7893686294555664	The inference in its most general form is variational inference.
6707578	6713802	B	0.49217164516448975	So what we typically do know and love from active inference.
6713866	6720734	B	0.7389843463897705	And I have a target probability distribution that I'm not able to compute directly.
6720782	6723460	B	0.7013838291168213	So I'm going to approximate it as best as I can.
6724150	6743850	B	0.8452255129814148	Now, these distributions, they're often used to compute expectations, or in other words, in statistics and machine learning, we often need to compute expectations with respect to some probability distribution.
6744830	6752090	B	0.6398545503616333	What this section does is it derives a whole bunch of divergences.
6752670	6754422	B	0.7566389441490173	Like the Kel divergence.
6754486	6758042	B	0.6856616735458374	I mean, the Kale divergence is not discussed so much in this action.
6758186	6773170	B	0.8460688591003418	I'll explain why, but it derives a whole bunch of divergences that can be used to approximate this unknown distribution with some other distributions that are tractable.
6775590	6797850	B	0.8318291902542114	So to put in a different way, you have this abstract variational inference problem where you have this target distribution and then you have this family of distribution that you know, and you want to select the distribution in your family that best approximates your target distribution.
6798670	6811002	B	0.8183916807174683	Now, in order to do this, you first need to derive a measure of similarity between distributions to know, okay, well, what is the best distribution.
6811066	6817690	B	0.8254241943359375	Like, how can you quantify being the best distribution, being the best approximation to your target distribution?
6817770	6822610	B	0.8672971129417419	And to do that, you need to specify a emergence, like the KL divergence.
6823830	6831430	B	0.6917753219604492	But the problem is, the KL divergence is not always practically applicable.
6832330	6836978	B	0.7910439372062683	One example is when the target distribution is just given by samples.
6837154	6840470	B	0.5357860922813416	So maybe you don't even know the target distribution.
6841690	6844134	B	0.5544294118881226	Maybe it's a gaussian, but you don't know it.
6844172	6846046	B	0.7501726746559143	You just have samples from the gaussian.
6846098	6853786	B	0.6071760058403015	So you cannot just have, like, a histogram, and you need to best approximate the histogram with the distribution in some family.
6853968	6860746	B	0.75809645652771	So when you have a problem like this, which just happens all the time, you cannot use the KL divergence.
6860778	6862830	B	0.76614910364151	So you need to think about other divergences.
6863250	6883080	B	0.7406073808670044	And so what this section does is deriving other classes of divergences that are more generally applicable than the KL divergence, and they have also some very nice properties and then showing, okay, well, how is it that I can minimize these divergences to solve a variation of inference problem?
6884250	6899880	B	0.7983183860778809	So I think there's a lot of things to be done there in the sense that in active inference, we always use the Kale divergence to do variational inference and minimize free energy.
6900250	6911470	B	0.6546924114227295	And this Kale divergence is available because we typically have adjuncted models that are nice and they're expressed graphical models.
6913010	6921970	B	0.6676101088523865	But there's this whole other plethora of divergences and algorithms to minimize the divergences that's out there.
6922040	6934070	B	0.9318698048591614	And it could be the case that there's very interesting recipes and methods that can be either brought back to the KL case that we use or just used altogether in our setting.
6935690	6950410	B	0.8612455129623413	In particular, when, for example, let's say they wanted to approximate the expected free energy in active coherence, the expected free energy is an expectation and want to approximate that expectation.
6950750	6952220	B	0.8130879402160645	It could be the case.
6953630	6970718	B	0.6483557820320129	Well, I think these methods that are summarizing section four could be very useful because they basically develop divergences that quantify how far the expectations are going to be if you choose a different distribution.
6970814	6975060	B	0.7173259258270264	So it's all like, very natural in that computational sense.
6975430	6979806	B	0.7849768400192261	And then finally ant activation through active coherence.
6979838	6990040	B	0.6505070328712463	Well, I think we've discussed that already extensively, but I won't spoil it more today and we can come back to it next time.
6995910	6997694	A	0.9627098441123962	Thanks a lot for that overview.
6997742	6999650	A	0.9413807392120361	That was very powerful.
7001270	7005490	A	0.9265322685241699	Incredible dot one very informative.
7006710	7014854	A	0.8218917846679688	Looking forward to starting with some questions from the live chaos and people's comments on the video between now and next week.
7014972	7024566	A	0.6071470379829407	If anybody else of your colleagues or anyone in the institute wants to join, we'll be here next week for 52.2.
7024668	7026710	A	0.9713979363441467	So thank you, Lance, for the time.
7026780	7028280	A	0.9529626369476318	Really appreciate it.
7028650	7029910	B	0.7800779342651367	Thanks, Daniel.
7030490	7031510	A	0.5942320823669434	Farewell.
7032770	7033710	B	0.6621690392494202	See you next week.
7033780	7034010	B	0.5137446522712708	Bye.

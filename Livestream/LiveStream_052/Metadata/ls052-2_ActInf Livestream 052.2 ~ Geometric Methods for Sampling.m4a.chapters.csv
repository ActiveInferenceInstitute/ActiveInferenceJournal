start	end	startTime	summary	headline	gist
7590	47670	00:07	ActInf Lab Livestream 52.22 on March 9, 2023. We're a participatory online institute that is communicating, learning, and practicing applied active inference. This is a recorded and archive live streams, so please provide feedback so we can improve our work.	Octemf Institute is communicating, learning, and practicing applied active inference	
48890	411820	00:48	In our third discussion on the paper geometric methods for sampling, optimization, inference and adaptive agents. If you're watching live, of course, feel free to write questions in a live chat.	We discuss geometric methods for sampling, optimization, inference and adaptive agents	Geometric methods for sampling, optimization, inference and adaptive agents
412190	844630	06:52	HamiltonI Monte Carlo typically just works very well and in a wide variety of situations. Monte Carlo sampling in general is slow. That's one of the reasons why one might want to do variational inference instead of sampling.	Last time we discussed the problem of sampling and why that's difficult problem	Hamiltonian Monte Carlo
845370	1465660	14:05	Geometric integration is the field that develops computational methods of numerical integration. It aims to preserve important geometric properties of the system. Dramatic integration allows you to take very long time steps and still preserve the Hamiltonian. Third ingredient of Hamiltonian Monte Carlo is metropolis. Hastings.	Geometric integration is the process of preserving geometric properties of a system	Geometric Integration and Hamiltonian Monte Carlo
1467970	1692260	24:27	The action on optimization is about how to accelerate optimization, how to derive an optimization algorithm. By accelerated, what I mean by accelerated, it's not about just going faster, but it's about having acceleration. This idea for optimization is just extremely powerful.	Section two of the paper is about how to accelerate optimization	Advancement in optimization 1.6
1693690	2310330	28:13	The advantage of using second order methods is that you actually double the state space to introduce velocities. This is similar to Hamiltonia Monte Carlo also has this notion of acceleration. What makes sampling efficient is that the characterizing process is characterized by distribution.	Using second order methods allows you to introduce velocities into a system	Hamiltonia Monte Carlo
2312590	2777310	38:32	Geometric integration gives you a bunch of algorithms to preserve the Hamiltonian. But it's going to preserve what people call a shadow hamiltonian. This is close to true Hamiltonian, which means that you can take a very long time steps and still be very good at preserving your target distribution.	Geometric integration gives you a bunch of algorithms to preserve the Hamiltonian	Hamiltonian Monte Carlo
2778370	3482674	46:18	One crucial thing to be a good sampler is this idea of time irreversibility. If your process is time reversible, then it's going to backtrack very often. Metropolis Hastings is a blessing and it's also a curse.	One crucial thing to be a good sampler is time irreversibility	Meeting the Hamilton and T Carlo sampler
3482712	3663846	58:02	Active inference uses partially observed mark of decision process. But the data that he has lives on a continuous space. There's just so many things that you cannot account with these sort of models. The planet is still going to be discreet.	Active inference requires a state space that's continuous instead of discrete	Inactive Inference: The continuous state space
3663958	4049682	1:01:03	One important and obvious type of model would be partially observed Markov decision process with continuous space. Active inference has the capacity to deal with discrete and continuous date spaces. There's more and more projects that are going on and, you know, people are realizing it's an obvious realization.	Your 2020 paper talks about how active inference can deal with discrete and continuous spaces	Including discrete and continuous time decision making
4049826	4735300	1:07:29	Daniel: What is your active inference representation in the figure of the running person? And then how do the variables and the processes described in this figure relate to all of this?	So what is your active inference representation in the figure of the running person	Active Inference Algorithmmas
4736790	5451014	1:18:56	Expected free energy is the negative log probability of an action sequence given some sensations. Either you want to stimulate the most likely action sequence or simulate the action sequence that maximizes the probability distribution. The open problem to be explored is whether we can get better performance by different kinds of divergences.	The expected free energy gives us how actions relate to sensations	Inference and the expected free energy
5451212	5568490	1:30:51	Lance: It really shines a different light on even what, learning active inference or learning free energy principle. It turns out that we got a way simpler perspective than anything that's out there, I think. Again, thanks for the work.	This work sheds new light on learning active inference or learning free energy principle	Learning Active Inference from Pseudocode
5570370	6160810	1:32:50	The free energy principle applied to decision making was to describe action as a function of observation. But for any given system it's going to play out in this immensely nuanced way with a lot of bespoke mechanisms. And so the role of free energy in decision making is pretty clear.	Free energy principle applied to decision making, as you said	The free energy principle in decision-making
6164350	6666460	1:42:44	The choice of divergences or the choice of discrepancy that you might want to use to solve the inference problem. Using the maximum mean discrepancy, when you take two distributions that are very close, it reduces to what people call the Earth movers distance.	Kale divergence is a choice of discrepancy that you might use for inference	Kale divergence and the maximum mean discrepancy
6669330	6862860	1:51:09	What's most exciting about is scaling active inference right now. The big question to me long term is what kind of generative models do we need to simulate brain like behavior. We need to drill down even more onto the kind of Janitor models that would be amenable to that.	I think what's most exciting about active inference right now is scaling	In the search for the brain's generative models
6867030	6943350	1:54:27	Great. My closing reflection I feel like I know less about fep, but more about something else. Earth was moved, Bayesian mechanics were called in, decisions were made, and it's been a really great series. I hope to do this again soon.	Great. Excellent. Thank you so much. For what we've discussed. Earth was moved, Bayesian mechanics were called in, decisions were made	Fep 1

start	end	speaker	sentiment	confidence	text
250	800	A	0.546256959438324	You.
6820	8144	B	0.872096598148346	Hello and welcome.
8342	11152	B	0.853792667388916	It's June 15, 2023.
11286	12860	B	0.6570366621017456	We're in active Livestream.
12940	14290	B	0.6184638142585754	54.2.
14660	17052	B	0.9514968991279602	So welcome to the active institute.
17196	23708	B	0.5296252369880676	We're a participatory online institute that is communicating, learning, and practicing applied active inference.
23804	25744	B	0.8719960451126099	You can find us at the links here.
25942	31576	B	0.5467144250869751	This is a recorded and an arch Live Livestream, so please provide feedback so we can improve our work.
31758	37640	B	0.825783908367157	All backgrounds and perspectives are welcome and we'll follow video etiquette for Live Streams.
38540	45820	B	0.7832002639770508	Head over to activeinference.org to learn more about participating in Live Streams and other projects.
46480	50540	B	0.6020633578300476	All right, we're in Live stream 54.2.
50690	62348	B	0.8547428846359253	Continuing with our third discussion on the work, mathematical foundations for a compositional account of the Bayesian brain, we're back with Dean Ali and Toby.
62524	77904	B	0.9127089977264404	So if each want to say hello and anything to begin with otherwise, Ali has some questions ready and we have some questions that people have submitted in the meantime as well.
78042	84088	B	0.7745529413223267	So shall we just proceed to the questions as we have the same people as dot one?
84254	85064	B	0.7671424746513367	Great.
85262	90010	B	0.8713367581367493	Okay, I'm going to jump into some of the presubmitted questions.
90320	102700	B	0.8995271325111389	So would it be correct to call equation 4.2 compositional Bayes theorem, which is or are Bayes's theorem categorically?
103440	112348	B	0.8963444232940674	And we have the document ready or anything that you want to share, but what does it mean for Bayes theorem to be categorified?
112524	114800	B	0.7042688131332397	And where is that in the dissertation?
115140	117664	A	0.9171260595321655	Yeah, okay, so that's a nice question.
117862	134440	A	0.8874115943908691	I would say that equation 4.2 is like a sort of basic is like a sort of it's like a basic representation of the product law of probability theory.
135100	139300	A	0.7650159597396851	And so I wouldn't say it's particularly inherently compositional.
139380	146472	A	0.6667766571044922	It's just something that is just like an axiom that is just like a consequence of the sort of standard axioms of probability theory.
146616	160336	A	0.8018386363983154	The thing that makes it a little bit different from the way that it's normally written in probability theory is that I've kind of annotated those P symbols which represent the probability of a certain event.
160518	172416	A	0.889859139919281	I've annotated them each with a subscript, and that subscript represents where the probability is coming from or the probability according to whom.
172608	174550	A	0.8208416700363159	That's kind of how you can think of it.
175400	194540	A	0.5518724322319031	And so normally I find this kind of like P notation to be a little bit confusing because people often write P of B given A, and they also write like P of X given Y or like P of W comma a comma z.
194690	201020	A	0.7431281805038452	And so in each of those cases, the sort of content of the P is kind of different.
201090	218932	A	0.7748289108276367	Like the A and the B might be events of a different kind to X and Y, and yet they're all put into this P symbol, whereas in this case, with the annotations, it says, like, as I say, it says according to what?
219066	233716	A	0.9060878157615662	And so here the little subscript C represents what's typically called in compositional probability theory like a channel or a kernel.
233908	239668	A	0.785250186920166	But that is like a kind of abstract way of just saying conditional probability distribution.
239844	249852	A	0.8695456981658936	And so the little C means it's a conditional probability distribution from space X to Y, a space Y.
249906	260144	A	0.669359028339386	And here I'm thinking of the B as being an event of type Y and A as being an event of type A, sorry, of type x.
260262	272564	A	0.8898075222969055	And so the PC says it's the probability of the event B of type Y given the event A of type x.
272762	280504	A	0.7354755997657776	And so that means that when you plug these kind of flows of information together, you make sure the types match.
280622	284120	A	0.7576771974563599	It's kind of like working with a strongly typed programming language.
284780	299150	A	0.8881602883338928	So that type information is like the main thing that's different between this and the standard product rule of probability and it's from that product rule of probability theory that you can just write down Bayes theorem, which is the equation 4.3.
299520	321120	A	0.5051534175872803	I would say that the reason why I slightly hesitate to call equation 4.3 like a sort of abstract or categorical version of Bayes theorem, even though it's annotated by these channels, is because you have this division.
321280	332820	A	0.6615239381790161	And that division isn't something that's well defined in all categories, in all situations that you might want to consider probability theory.
332900	354270	A	0.9121296405792236	For instance, you might have this P of C dot pi thing and that means the kind of Y marginal of the joint distribution or it's otherwise known as the push forward of the distribution pi through the channel C.
355120	360432	A	0.6708253026008606	That thing might have what's known as measure zero, which means the probability might be zero.
360486	365410	A	0.7371528744697571	And so the division isn't very well defined and so that expression doesn't always make sense.
366600	379796	A	0.7750375866889954	Now the thing that does always make sense on the other hand is that just product law equation which tells you how the two ways you can write down this joint distribution.
379988	395580	A	0.9071466326713562	So if you have a joint distribution like on the space x times y, I've written it there, then you can decompose it into a prior on x and a channel from x to y.
395730	407170	A	0.8969902396202087	So that then if you put the information from the prior through the channel you get the marginal on y and that's that push forward thing.
407540	418420	A	0.90888512134552	Or you could write the joint distribution as a prior on Y and a conditional distribution from Y to x.
418570	422864	A	0.6578073501586914	And the thing is, obviously we're talking about the same joint distribution in both cases.
422912	433530	A	0.7889965176582336	So those two decompositions, they're sometimes called disintegrations, those two disintegrations need to be equal and that's what that equation is saying.
434460	446140	A	0.7596647143363953	So that's kind of because it doesn't have this division in it and because Bayes'law follows from it and because it's something you can write down in all settings, it's kind of more fundamental.
447200	454720	A	0.8563752770423889	But I quite like the kind of string diagrammatic form of it which is something that is drawn.
455060	456572	A	0.5379235744476318	It's not my invention.
456716	464588	A	0.915431022644043	I think I originally saw it in a paper by Bart Jakobs and I think his student at the time, Kenta Cho.
464764	479160	A	0.9093023538589478	So in, let's see equation 4.5 so that's section 4.1.2 on abstract Bayesian inversion.
479980	492540	A	0.5852490067481995	So yeah, if you scroll down a little, it's on like page 130 or 130 in my version, which might be hopefully around the same place in your version.
496480	498460	A	0.8090426921844482	Yeah, a little further.
501560	503396	A	0.514511227607727	Oh man, no, maybe not.
503578	507960	A	0.7648583054542542	Let me get up the version on because I've realized I've opened the wrong version.
510220	526480	A	0.8460761308670044	Meanwhile, see it so so the the thing I'm going for here is the sort of, as I say, the string diagrammatic depiction.
527140	527552	A	0.46103888750076294	Yes.
527606	529968	A	0.8837452530860901	4.1.2, it's page 120.
530054	531170	A	0.6856347918510437	Sorry about that.
535160	539220	A	0.8004305362701416	Yeah, so it kind of shows you how the information flows.
541000	547940	A	0.749045193195343	And so this makes the two processes quite explicit.
548020	552600	A	0.9203653931617737	So here on the left, the prior of the joint distribution is pi.
553180	562392	A	0.8978690505027771	And then you take the sort of probability, the information from pi and you copy it and you keep one of the copies and that gives you your X marginal.
562536	569960	A	0.8519167900085449	And then you feed the other one through the likelihood or the conditional channel C, and that gives you the other marginal.
570120	576044	A	0.6902657151222229	And so if you throw away this X information, you get the Y marginal.
576092	578050	A	0.7046816945075989	And that's what marginalization is.
578420	587456	A	0.8700240850448608	And that's why the Y marginal, if you look on the right hand side, the prior is C after pi, that's the Y marginal.
587488	591910	A	0.767703652381897	So this is like depicting that Product Law.
592600	609528	A	0.8860085010528564	And then you say you can call something a Bayesian inversion of C with respect to the prior pi if it is something that can go in the box on the right hand side where I've written C dagger pi.
609624	623390	A	0.8547763824462891	And the reason I've used this dagger symbol is because under certain technical conditions, this Bayesian inversion thing gives you what's known in category theory as a dagger function.
624740	628960	A	0.8687986731529236	And that's something that swaps the direction of a morphism.
629620	636496	A	0.8649377226829529	And so here you've taken the thing that goes from X to Y and you've swapped it and it goes from Y to X, but it depends on the prior.
636528	640420	A	0.807685911655426	And so I've written this little subscript and so for me, this picture.
642840	643168	C	0.6453943848609924	It'S.
643184	646950	A	0.8515860438346863	Kind of equivalent to that Product Law equation 4.2.
647420	658844	A	0.524146556854248	But I think although it's a kind of new kind of language, I think it's a kind of more intuitive one because you can sort of see how the information flows through the system.
659042	665564	A	0.5924599170684814	Sorry, that was a bit of a long answer, but I think that's how I think of it.
665682	666350	B	0.7671424746513367	Great.
667120	670508	B	0.6958566904067993	Okay, next question.
670594	672208	B	0.7997760772705078	Could be a short one.
672374	676348	B	0.8124298453330994	I'm mostly wondering about how chapter six will be rewritten.
676524	677520	B	0.8955574631690979	Will it be major?
677590	685856	B	0.6305612325668335	And when we promise we didn't write this question primarily to select the best policy as to where to direct my regime of attention?
685968	692112	B	0.8665600419044495	Like, should I wait to dig deeper into chapter six until the updated observations arrive?
692256	697064	B	0.9183037877082825	Is reflecting deeply up until chapter five where I should focus for now?
697102	699304	B	0.7800357937812805	Or is chapter six something that won't change too much?
699342	702344	B	0.6546874046325684	And I can still explore that next week.
702542	704810	A	0.8675068020820618	So this is a great question.
706140	714140	A	0.4974876642227173	And I'm sorry that the rewritten version isn't online yet, but it is really close, I promise.
714800	721356	A	0.7155977487564087	And I am kind of aiming for it to be done like next week or maybe let's not hold myself hostage to fortune.
721388	723490	A	0.7970221042633057	So let's call it the next couple of weeks.
724340	735140	A	0.763657808303833	But it's quite a change in the sense that it's kind of annoying because it's simpler but also kind of more structured.
735960	758990	A	0.4900550842285156	And so that means that there were a lot of finicky details that I didn't anticipate needing to deal with, that I had to deal with, even though fundamentally the basic idea is kind of the same but also neater and simpler at the same time.
762080	769630	A	0.8907663822174072	I can go into some details about the differences if you'd like, or I can just leave it at that.
770000	771052	A	0.8266415596008301	What would you rather?
771186	773410	B	0.7795282006263733	Let's return to this in the three.
773860	775360	A	0.5369534492492676	Okay, yeah, let's do that.
775430	776560	A	0.962164044380188	That's a nice idea.
776710	788612	A	0.5221955180168152	So, yeah, I would say don't dig too deeply into it, because basically the thing that I've managed to do away with and sorry for all the people who've kind of tried to bang their heads on it already, is.
788666	791440	A	0.8214338421821594	The kind of pro functor, like optics.
791520	807370	A	0.7352092266082764	Kind of like the sort of pro functor stuff with those weird coen things, the kind of integral symbols that I was using to define context for statistical games, that whole thing.
808320	812056	A	0.6178989410400391	It turns out you don't actually need the complexity of that structure.
812248	818780	A	0.8013095259666443	And the notion of context is really given just by two things.
818850	825680	A	0.8758347630500793	It's really just the prior, which we've already talked about, and also the observation you take from the environment.
826340	841780	A	0.8285526037216187	And so before I thought you needed to consider not just the observation, but also the stuff that might go on in the environment around the observation that gives you correlations.
842280	846360	A	0.5514271855354309	But it turns out actually you don't need that, at least not for the free energy principle.
847500	852200	A	0.4940980076789856	And so that makes things a bit simpler because you don't need that kind of weird structure.
853980	869084	A	0.9574811458587646	But there's, as I say, some technical stuff which is very annoying, which made it also a little bit slightly annoying to write down and involved by categories and sorry about that.
869202	893430	A	0.7973294258117676	But it's actually the fundamental idea of a statistical game being like a Bayesian lens which describes the inference process, maybe like a parameterized Bayesian lens that allows you to learn, but a Bayesian lens that comes with a loss function that's still the same idea.
894440	904436	A	0.8634263873100281	And it turns out that loss functions like the relative entropy and the free energy, they actually have a really neat categorical structure.
904468	914748	A	0.5612049698829651	So the things I was calling like free energy games before or Bayesian inference games, those are actually now a lot even neater still.
914834	916510	A	0.9874103665351868	So that's really nice.
918960	928396	A	0.7945886850357056	And the way you get predictive coding is still as like a functor from these statistical games to dynamical systems.
928508	930736	A	0.6637686491012573	So that idea is still the same.
930918	935430	A	0.6942508816719055	It's just actually it turned out that the statistical games themselves.
935880	940790	A	0.9786388278007507	There was actually a lot more nice stuff going on there than I had realized before.
942200	942950	B	0.9184247851371765	Awesome.
944920	945830	A	0.4896698594093323	All right.
947800	953876	B	0.8961281180381775	You cite Erisman and Van Bermesh work on memory evolution systems.
953988	956532	B	0.917457640171051	To what extent have you used their work in the thesis?
956596	969310	B	0.9027327299118042	Would it be correct to say that there might exist a functor from that book to your dissertation or indeed to any of your citations, is there work someone could do to explore the connection between the two?
969680	979868	B	0.8736374378204346	To me, this is a question of one instance if a learner or researcher performs active inference on the interface between a work and a citation more generally?
979964	980640	B	0.6860781908035278	Great question.
980710	1000132	A	0.7181445956230164	Yeah, I think it's a really nice so I I read this work by Erisman of Rambamish on memory reevoltative systems a few years ago, and I found it very abstract and confusing.
1000196	1008350	A	0.5393038988113403	And I was like, yeah, this is all really nice, but how does it actually connect to the brain and how things like brains work?
1011040	1030800	A	0.9273242950439453	But I've kind of softened my stance on it recently, and I think actually there's some really cool stuff in that work, some really great ideas about hierarchical complex systems and emergence and even things like neural coding.
1032180	1047290	A	0.6532760262489319	It's kind of, I think, slightly in danger, I think, of being forgotten because people see it and think, okay, well, how does it actually make contact with things I know about now, things that actually scientists use?
1048620	1056410	A	0.8443841934204102	And so I think it would be great to explore the connections in a kind of more detailed way.
1056780	1060430	A	0.5409404635429382	I don't know precisely what that would look like.
1062000	1070320	A	0.8080512881278992	I heard a talk, actually, by don't know.
1070470	1073424	A	0.864602267742157	There may be some on YouTube, I don't know.
1073542	1101160	A	0.5551970601081848	This was in Consciousness Maths of Consciousness like workshop at the end of April, and she explained some of the more recent stuff they'd been doing, and it sounded like actually there is a proof, I think, that Hebian learning does actually give you sort of what she calls cat neurons.
1102940	1112540	A	0.6502137184143066	And so before I've been like, okay, well, it's a nice story, but does it actually relate to things like heavy learning, or is it just like an analogy?
1113920	1117180	A	0.5357347726821899	But it sounds like it's actually a bit more precise than I had thought.
1117330	1132140	A	0.7522114515304565	And obviously, because of the connection between things like heavy and learning and predictive coding, it sounds like there should be a way to relate something like predictive coding to memory evolutive systems.
1132220	1144192	A	0.8778242468833923	And that would be really nice because I sometimes think about this is kind of a direction that I'm trying to go in slightly, but try to think about nested active inference systems.
1144256	1158860	A	0.8456010222434998	Like, you might have a little region of the brain and all the cells are doing active inference, and the circuits do predictive coding, and they make up a brain and the brains make up society and an organization.
1159280	1171216	A	0.829619824886322	And that story of hierarchical complex systems is really the story that this work is trying to tell as well.
1171318	1172704	A	0.7548863291740417	Just in a different kind of way.
1172742	1190116	A	0.870003342628479	And so I think it's a really fruitful area, and I really hope somebody does dig into it, because I think it's great work, and I think it's just work that was kind of maybe ahead of its time, so it kind of got left by the wayside maybe a little bit.
1190138	1194230	A	0.9318037629127502	But I'm really glad to see somebody ask about it now.
1195260	1196056	A	0.84200119972229	Cool.
1196238	1199224	B	0.8676079511642456	Just one short remark on this, and then alir Dean, please.
1199262	1204964	B	0.6406944394111633	It's almost like when people draw associations, like a line of sight.
1205092	1207204	B	0.5210432410240173	Oh, well, emergence is like this.
1207262	1212648	B	0.674980103969574	It's like they're surveying and can have quite far vision.
1212824	1218476	B	0.7215068936347961	But category theory is like building the high speed rail where it's, like, locked in.
1218658	1226172	B	0.8419437408447266	And then we can say there really is a relationship of this exact formal type between Hebian learning and predictive coding.
1226236	1234992	B	0.5710861086845398	Let's just say some type so then that can be really relied on as a foundational building block.
1235136	1256052	B	0.7470939755439758	And it kind of brings those literatures and merges those streams or at least adjoins the streams or naturally transforms the streams, or whatever the appropriate terminology would be, but in a really solid way that's not just like, well, one person on a mountaintop saw that these two things could be connected.
1256116	1259710	B	0.6232922673225403	It's more like an engineering project that really locks it in for everybody.
1260160	1261484	A	0.6140062808990479	Yeah, I think so.
1261522	1266568	A	0.5726804733276367	And I hope people are incentivized to do that engineering.
1266664	1268030	A	0.9643183350563049	I think that'd be great.
1268720	1272130	B	0.8613573908805847	All right, last of the presubmitted questions.
1273700	1288196	B	0.8851856589317322	So, in the par, pazulo and Friston 2022 active inference textbook, the first question in the recipe for designing an active inference model this is chapter six is which system?
1288378	1298250	B	0.874300479888916	This entails three pieces of data one, the agent generative model, two, the interface sensory data and actions, and three, the external environment, the generative process.
1298860	1314620	B	0.9134459495544434	So how do these relevant big chunks of notions from the recipe for making an active inference model in the par at all textbook map to the sections of what you've done in your dissertation?
1314960	1316060	A	0.9147537350654602	That's a nice question.
1316130	1328800	A	0.8753910660743713	And it takes me on to a question of action and active inference beyond the sort of approximate inference that I was talking about in my thesis, predominantly.
1329300	1335220	A	0.5098527073860168	Maybe here I could draw some stuff, and so I don't know how easy that is for you, but I can share my screen.
1335370	1336164	A	0.46103888750076294	Yes.
1336362	1338470	A	0.8020122647285461	Let's see how this will work.
1341880	1342388	A	0.5491447448730469	Yeah.
1342474	1343110	A	0.584351658821106	Okay.
1343480	1344230	A	0.6863258481025696	Share.
1345900	1346808	B	0.4896698594093323	All right.
1346974	1349508	A	0.8574463725090027	Can you see my cursor thingy?
1349604	1350250	B	0.5491447448730469	Yeah.
1350780	1351850	A	0.8488619923591614	Okay, cool.
1354060	1365660	A	0.5479132533073425	So I like to draw this picture, and this is like, the little channel c, which does prediction, and it goes from here, and it goes outwards.
1366560	1378780	A	0.7175187468528748	And this is the inversion, the approximate inversion, and it takes in, like, a prior, which comes from here, and it takes in an observation.
1378940	1392230	A	0.7714002132415771	So we could think of this as, like this is, like, high level belief and, like, lower level.
1396110	1411360	A	0.6078514456748962	And this is like somehow you get something which gives you an observation, and then this gives you like new belief, high level.
1415220	1417612	A	0.7331990003585815	And this thing is a Bayesian lens.
1417756	1421970	A	0.8360695242881775	So that's what the Bayesian lenses are all about.
1423160	1433510	A	0.8661102652549744	And the thing that I was talking about in my thesis is turning this into like a predictive coding dynamical system.
1434200	1443544	A	0.8119750022888184	Because you've got neurons which do this sort of prediction part and then neurons which do this passing back of error signals to do this updating part.
1443582	1446676	A	0.9657712578773499	And they have this kind of like bi directional form and that's really neat.
1446708	1456652	A	0.8981671333312988	And so you can put in another one here, another one here, or you can just start with your prior which is like something that I often write as pi up here.
1456786	1460190	A	0.7521523237228394	And then this gives you like a new pi at this point.
1460820	1465788	A	0.5406603217124939	And so this is going always from higher level beliefs to lower level beliefs.
1465804	1490120	A	0.5614114999771118	And down here you might eventually get to the lowest level, which is like if you're thinking about visual systems, then this would be like the V one visual cortex stuff or like the stuff you get in the retina and you get prediction about let's say visual signals.
1491420	1495364	A	0.5717443227767944	But then the world, you get this predicted.
1495412	1512360	A	0.7923636436462402	But then the world comes back and it actually gives you actual this is predicted, gives you actual signals and then the brain uses that to update its beliefs up here.
1512510	1521036	A	0.5375353693962097	And so really the interface is somehow like this, okay?
1521218	1529330	A	0.7904334664344788	And so this is like lens world and this is like external world.
1531060	1535516	A	0.7052330374717712	So that's where the external environment comes into the picture.
1535708	1539590	A	0.7687232494354248	And you could think of everything in here as like agent.
1541160	1548260	A	0.5873551368713379	But this doesn't really tell us much about action and it doesn't tell us much about interaction.
1548680	1562920	A	0.8408670425415039	And so there's another kind of story for that which is kind of similar in many respects, which is the story that is told at least in category theory by what's known as categorical systems theory.
1563440	1565980	A	0.5122718811035156	And there you get pictures like this.
1566130	1568524	A	0.5931519865989685	So this is like your world.
1568642	1577730	A	0.6432484984397888	And there you've got some system inside the world and here you've got another system inside the world.
1578820	1585200	A	0.8502717018127441	And so people then draw like things that are called wiring diagrams to kind of represent how these things interact.
1586100	1590676	A	0.8286024332046509	So let's call this a for a.
1590858	1594292	A	0.694730281829834	Maybe there's another system over here and this is B.
1594346	1596132	A	0.8198907971382141	And maybe that's C for A.
1596266	1602890	A	0.5536681413650513	Its environment is like everything that isn't A, it's B and C.
1605260	1619756	A	0.7617310285568237	So maybe it's connected like this, I don't know, some weird wiring thing actually, maybe that should go round to here and then this goes like to that.
1619858	1621390	A	0.6686354875564575	I don't know, something like this.
1622640	1624712	A	0.6095479130744934	This is just what's known as a wiring diagram.
1624776	1627810	A	0.7991275191307068	It just tells you how the systems are coupled together.
1628660	1635292	A	0.8501373529434204	And that would be like specifying how the information flows across the Markov blankets of your systems.
1635436	1641060	A	0.7714225053787231	And so this box is like the Markov blanket of A.
1641130	1645940	A	0.7610691785812378	And then this wiring is like the data of the coupling of the blankets.
1647720	1660040	A	0.8479691743850708	And so there's a whole theory, a whole sort of story in category theory about how to sort of compose if you've got like, you know, if you you know, it has to compose dynamical systems of this kind.
1660110	1661576	A	0.7764240503311157	And so, you know, what?
1661598	1681280	A	0.8854497075080872	What we do is we say, okay, well, we we write down like the type of if we say if we call if we write IA to represent the interface like the data of this is like X and this is like Y or something like that the kind of shape of the Markov blanket.
1682180	1694790	A	0.8452593088150024	Then what system theory tells us is that for each interface we get a whole category of systems that you could put in that box to animate it to make it like a living system.
1696360	1701128	A	0.588005006313324	And then this is like a closed box at the moment.
1701294	1706920	A	0.80082106590271	And so maybe it just represents the whole universe which doesn't have any external environment.
1707260	1724380	A	0.8911808729171753	But you might also have something like a company, so you call it like company X, and that has some agent which decides what the company does, or some agent here which takes in information from the environment.
1724880	1736370	A	0.8829958438873291	And so this wiring is then something that gives you a morphism from like A and B and C to X.
1737300	1767310	A	0.8597750663757324	And what this dynamical systems theory tells you is that let's call that W for wiring is that you get this functor from din A and din B and din C to din X, which tells you then how to wire all these dynamical systems together to give you one that fits into this outer box.
1767920	1772776	A	0.7956017255783081	And so this tells you how to do all this for just dynamical systems.
1772808	1777004	A	0.7276868224143982	But of course, we're kind of interested in active inference systems.
1777052	1785330	A	0.7883204817771912	And so here you don't just have some DX by DT thing to go in here.
1786180	1799670	A	0.8685353398323059	You've got the data of prediction and plans and goals and maybe then something about how to actually do the inference, like the statistical game stuff.
1800620	1802488	A	0.7213276624679565	But really the story is the same.
1802574	1822572	A	0.8944553136825562	And so if you have these boxes, again, what we can do is if we have a box like this x to Y is we can think of an agent as having as a part of it a way of predicting, say, X.
1822626	1829452	A	0.5515319108963013	We think of let's call these something else, sorry, let's call these S.
1829586	1832448	A	0.763738751411438	So this is like the sense data that goes into the system.
1832534	1836370	A	0.8652287125587463	And then let's call this A, and then this is maybe like the actions that it takes.
1836740	1839430	A	0.7517468929290771	Let's now call this box, I don't know B.
1840440	1841732	A	0.936752438545227	Oh, no, I don't like B.
1841786	1843190	A	0.7789809703826904	Let's call it y.
1846280	1847540	A	0.7754958868026733	So now let's think.
1847610	1872332	A	0.8702552318572998	Okay, well, part of what an active inference agent is doing is that they're predicting they've got some high level belief of type Y, and then they make a prediction about not just the sort of sense data, but also about their actions and then their body samples from this belief about how they should act.
1872466	1881500	A	0.8521574139595032	Or you've got these little cells, these little mechanisms which try to correct the prediction errors to make those actions into reality.
1881580	1886960	A	0.8701174855232239	And then maybe that gets coupled to some other system which receives those actions as inputs.
1887540	1908280	A	0.7845561504364014	And you might have another part here which goes in the other direction which says, okay, given my actual sense data and my actual actions, maybe my body is still learning how to play tennis or something, what are my new high level beliefs?
1908860	1913080	A	0.8363319635391235	And so you can do this whole story again of Bayesian lenses.
1916620	1921028	A	0.5155472159385681	And this goes like that, and this is now just why?
1921214	1923052	A	0.6791739463806152	And then this is like S times A.
1923106	1948356	A	0.8589625954627991	And this is s times a So you can tell this story and you can build like, hierarchical systems, but now you have a way of filling in each box and in principle, a way of coupling them together to give composite active inference systems, like companies or like colonies of cells or things like that.
1948538	1953610	A	0.493996798992157	This isn't a story yet that I've written down, but I know it works.
1954220	1957210	A	0.8603819608688354	And I'm in the process of slowly writing it down.
1958300	1964090	A	0.8771113753318787	So the kind of fundamental structures that are in the thesis are like part of the story.
1965280	1980240	A	0.5220116376876831	The main thing that's missing is kind of like reorienting it's, just like reorienting this picture so that it's not like going from here from left to right.
1980310	1988096	A	0.6733356714248657	It's like sort of poking down into the box, if you see what I mean.
1988198	1991990	A	0.6112301349639893	And this is like the stuff you put in.
1992440	2000260	A	0.8011478781700134	So this is like your lens thing here, and you sort of fill in the box with your lens.
2002220	2012250	A	0.8745806217193604	So I think that tells I mean, that tells you how the chunks of the thesis kind of fit into the standard act of inference story.
2013100	2025980	A	0.8109391331672668	The main job, at least as far as I'm concerned now, is to spell out the details and then start to talk about things like what happens if we've got Y and you've got Z's here?
2026050	2031730	A	0.4945119619369507	What if they're trying to form part of an organization, but they disagree about what to do?
2032180	2035120	A	0.6815546154975891	How does an organization resolve disagreements?
2035860	2041852	A	0.8406328558921814	What if you've got two signals from your brain to your muscles that tell you to do different things?
2041926	2043140	A	0.7754871249198914	How do you resolve?
2045880	2049830	A	0.841168224811554	How do you get harmonious active inference systems in some sense?
2050680	2056772	A	0.744443953037262	So I'll stop sharing my screen now, and then we could sort of return to the discussion.
2056836	2059690	A	0.5701650977134705	But that's, again, my kind of long winded answer.
2060940	2061690	B	0.9184247851371765	Awesome.
2063100	2065390	A	0.7183485627174377	I can share the pictures I see.
2065920	2070430	A	0.8362286686897278	Dean you said you shared something, but I can share some more things.
2072080	2077656	B	0.8325702548027039	Dina unmute and then go for it and then Ali no, I just shared.
2077688	2088192	D	0.8439017534255981	Something from about nine years ago that I was doing in my programming that everything you just wrote out was I just wanted to share one part of it.
2088246	2096944	D	0.5709438323974609	That confirms that you could come from completely different places and arrive at the same destination.
2096992	2102756	D	0.7265663146972656	The whole bi directionality, the whole predictive parabolic.
2102868	2103624	D	0.5491447448730469	Yeah.
2103822	2108360	D	0.49245935678482056	You can bring this into classrooms and have it make sense.
2108430	2115500	D	0.7881969809532166	It doesn't have to be esoteric or too remote, I guess.
2115570	2117884	A	0.9732856750488281	Yeah, that's great to hear.
2117922	2119596	A	0.9861032962799072	I'm really glad to hear that.
2119778	2122110	D	0.9810715317726135	Yeah, it's really awesome.
2122800	2123548	A	0.6283750534057617	Thanks.
2123714	2132192	A	0.6638520359992981	Yeah, as I say, there's lots left to do, but I think an important part of that is like spreading the word.
2132326	2137490	A	0.9920910596847534	It's really great that process is beginning, thanks to you guys.
2137860	2148644	B	0.8338006138801575	So, yeah, the rehearse play reflect cycle actually begins when you reverse the direction of thought reading from Dean's email.
2148762	2149430	B	0.7792782783508301	Interesting.
2151480	2152432	D	0.71476811170578	Self organized.
2152496	2153110	D	0.5491447448730469	Yeah.
2153960	2155620	D	0.7581989169120789	It's all part of this whole cat.
2155770	2176120	D	0.8964511752128601	This is the like, I love the questions that we've had so far and I want to hear Ali's questions and then I want to be able to take it and have people not be intimidated by it because you can really find yourself doing these things on a daily basis.
2176200	2181312	D	0.6843919157981873	You just don't necessarily identify them the way that we are.
2181446	2188960	D	0.553378164768219	But that doesn't mean that we have somehow lost access to the things that we're discussing today.
2189030	2191350	D	0.7665980458259583	So I'll just leave it at that for now.
2192280	2196484	A	0.9012332558631897	Yeah, cool.
2196602	2204920	B	0.5784373879432678	All right, in the rest of the time that we have with you for the two Toby.
2205820	2206840	B	0.6077883839607239	Ali.
2214140	2215530	C	0.7883909344673157	Okay, thank you.
2218140	2219720	B	0.5772350430488586	Your video is frozen.
2220640	2221608	B	0.5772350430488586	Your video is frozen.
2221624	2223150	B	0.6953489780426025	Maybe you can just turn it off.
2226260	2227632	B	0.5779587030410767	Yeah, go for it.
2227766	2228610	C	0.8529649972915649	Thank you.
2229060	2229970	A	0.4896698594093323	All right.
2233860	2240236	C	0.7251201272010803	In basically three to ridiculously.
2240268	2240850	C	0.5343338251113892	Next.
2244100	2247940	C	0.6189178824424744	Sorry, the notion of Marko, I noticed in your thesis.
2250800	2253544	B	0.7748008966445923	Sorry, Ali there's just a lot of lag.
2253672	2254350	B	0.5491447448730469	Yeah.
2274450	2277870	B	0.8945299983024597	What would you say is the chapter six equivalent?
2278370	2290420	B	0.8898592591285706	Is there yet a recipe for designing the kind of categorical, kind of compositional organization that you alluded to?
2294470	2296674	A	0.662102997303009	I can answer that quickly.
2296872	2299174	B	0.663339376449585	Yeah, go for a quick answer and then I think now it's better.
2299212	2303254	A	0.800051212310791	Ali yeah, so the quick answer?
2303452	2306840	A	0.564141035079956	Well, it's a quick answer, but it's not a quick thing to do.
2307450	2328880	A	0.8682195544242859	But there's a great book on categorical systems theory in its kind of great generality, which doesn't explicitly talk about active inference like I've been, but it does talk about this kind of composing dynamical systems in this hierarchical way idea.
2329890	2334910	A	0.9169212579727173	And that's the book that's currently being drafted by David Jazz Myers.
2335910	2338580	A	0.7504913210868835	I can circulate a link to it.
2340550	2375520	A	0.80326247215271	Myers that sort of spells out the kind of general structure and I believe he and Mateo Capucci have got a paper upcoming at the upcoming Act Conference on Applied Category Theory this year, talking about using that framework to do kind of cybernetic like things.
2376930	2396306	A	0.8868497610092163	And so what I've been thinking about lately is trying to fit active inference and the stuff I've been doing into that framework, which has been sort of developing in parallel and kind of fortunately and maybe slightly unsurprisingly, they fit very nicely together.
2396488	2402450	A	0.893314778804779	And so the pictures I was sketching are kind of a fragment of that other framework.
2402530	2405000	A	0.9269528388977051	And as I say, I think they go quite well together.
2406250	2407000	B	0.9184247851371765	Awesome.
2407610	2410700	B	0.5370058417320251	Okay, Ali, go for it.
2411230	2411738	C	0.584351658821106	Okay.
2411824	2412922	C	0.6856347918510437	Sorry about that.
2413056	2431630	C	0.8149308562278748	All right, so I noticed in your thesis that you don't rely too much on the notion of markup blankets, but instead you use this notion of polynomial functions as its more formal or perhaps more rigorous replacement.
2432930	2442420	C	0.7296815514564514	With regard to that in your paper Polynomial life, you say in the literature on active inference sorry.
2444470	2453190	C	0.8724009394645691	And the free energy principle, there is much debate about the concept and role of markup blankets, which are sometimes conceived to represent the boundary of an adaptive system.
2453340	2462150	C	0.5403736233711243	We believe that the algebraic polynomials will help to make such thinking precise where it departs from the established usage of markup blankets and Bayesian networks.
2462230	2464682	C	0.9034085273742676	So would you care to elaborate a bit on that?
2464736	2473698	C	0.8684099316596985	And how do you see polynomial functions as overcoming some of the limitations of the notion of markup blankets?
2473894	2475134	A	0.5509032607078552	Yeah, okay.
2475332	2497170	A	0.6936812996864319	So I will preface this by saying I think some people might disagree with me, but there is a notion of Markov blanket that for me is nicely defined, that exists in the sort of Bayesian statistics literature.
2497990	2508242	A	0.9005409479141235	And it says if you've got a Bayesian network, you can consider the nodes in that network and you can ask what are their Markov blankets?
2508306	2517030	A	0.8900436758995056	And in particular, you can just define the Markov blanket of a particular node to be its parents.
2517110	2535860	A	0.7637150287628174	So those nodes that it's conditionally sort of dependent on its children, sort of those nodes which are conditionally dependent on it and what are known as its coparent, so the other parents of its children.
2537750	2548686	A	0.7220419645309448	And that's a very nicely well defined notion, but it's only nicely and well defined in this context of Bayesian networks.
2548878	2561734	A	0.6520010828971863	And so it's like generalizing that to kind of situations where you've got interacting dynamical systems is something that informally makes total sense.
2561772	2572166	A	0.7829871773719788	You want to say, okay, well, I have this thing in the world and it behaves so it changes in time, and it's sort of a coherent unity in some sense.
2572208	2574270	A	0.7762414216995239	And so it has a kind of boundary.
2575090	2587220	A	0.8770356178283691	And because it has a boundary, information and energy from the environment always has to flow through this boundary in order to interact with it.
2587990	2595650	A	0.7623666524887085	And that sort of boundary notion, it seems, is like what is in other contexts also called a Markov blanket.
2595990	2609886	A	0.8256845474243164	And I think if you set up your dynamics in a particular way or your definition of Bayesian network in a particular way, that the two ideas that might coincide.
2610018	2612300	A	0.4946492314338684	But I don't think that's true in general.
2612670	2626030	A	0.7174220681190491	And I think what people mean in the literature when they're talking about Markov blankets, but in this dynamical context, I think all they really mean is like boundary or interface.
2626450	2657318	A	0.6626681685447693	So the data of the system, like capacity for interaction or something like that, the stuff that is on the edge of the system through which interactions with its environment are mediated, that sort of general idea is something, again, that appears nicely in categorical systems theory, but in a very general way.
2657404	2676154	A	0.6169229745864868	And it doesn't require any of the notions of Bayesian statistics, and it doesn't require conditional dependence or parents or children of a graph because you might have something more complicated than a graph.
2676202	2681706	A	0.8206550478935242	Like you might have edges which are allowed to connect more than one node, and that gives you a hypergraph.
2681738	2691682	A	0.6063339114189148	And there, of course the notion of Bayesian of marker blanket in the standard sense that doesn't make sense at all, but of course the notion of interface does.
2691736	2695678	A	0.7456846833229065	And there are certain categories of interfaces.
2695854	2704210	A	0.7757937908172607	And those wiring diagrams I was drawing before are an example of a category of interfaces.
2704290	2707400	A	0.5319188237190247	Like they don't have any data of the systems themselves.
2707770	2711980	A	0.824600875377655	The category just tells you how to sort of plug interfaces together.
2713150	2730160	A	0.6104760766029358	And so I think it's kind of neater to think about this connectivity in the interfaces sort of separately from the stuff that fills the interfaces, like the data of the dynamical systems or of the active inference, like games that I've been talking about.
2732450	2749030	A	0.9374491572380066	And so a really nice kind of category that you can use to talk about interfaces and indeed to talk about lots of other things, but in particular to talk about interfaces is this category of polynomial functions.
2750090	2756178	A	0.8427671790122986	And it kind of generalizes these wiring diagrams that I was drawing earlier.
2756354	2778350	A	0.7815153002738953	And it does so in a particularly useful way, I think, which is that if you sort of formalize the interface of a system using this polynomial language, you can start to talk about interfaces that themselves might change depending on the context.
2779330	2801334	A	0.7547673583030701	So when you get into the car to drive somewhere effectively, you've sort of changed your interface because now you've got this car around you and it behaves in a different way than if you're just on your own or if you're just normally going about your life.
2801532	2810730	A	0.5956899523735046	You might blink right, or you might close your eyes, and that means that you're no longer receiving visual signals from the world.
2810880	2826880	A	0.8264683485031128	And so if you just say you're writing down your active inference model and part of the model is that it receives visual signals, then talking about situations where those visual signals go away.
2829810	2830126	D	0.546256959438324	You.
2830148	2832634	A	0.6547466516494751	Can do it right, but it's not very neat.
2832682	2845540	A	0.525965690612793	And so to sort of account for interfaces that might change like this, it's nicer to be able to say, okay, well, my interface actually is allowed to depend on what I'm doing right now.
2846870	2862540	A	0.5838663578033447	So I'm currently on a video call and it means I can do certain things, but I'm currently wired into the computer so I can't so easily go and walk around like the set of signals I receive or actions I might take might change.
2862990	2878960	A	0.6926446557044983	And so I think it's easy to start by saying, okay, when we're talking about Markov blankets in dynamical systems, we just mean like, there is some set that is like the action set and some set which is the sort of sensory set.
2879970	2897986	A	0.6801479458808899	For me it's just not enough because of the fact that the world is dynamical context change coupling might be dependent on like you might be in an organization and there's some executive who's in charge of defining the team structure.
2898178	2909894	A	0.827725887298584	And that means the team structure is like the wiring of the agents in that organization, but how they behave depends on the decisions that that executive makes.
2910092	2915526	A	0.7786664366722107	And so their markup blankets are conditional on those decisions.
2915638	2924794	A	0.550971508026123	And that's something you can talk about nicely in this polynomial language because you're allowed to have systems which affect the wiring of other systems.
2924922	2937246	A	0.5191266536712646	But it's really not something that's easy to talk about in this standard framework of interfaces and boundaries that is dominant in the active inference literature.
2937438	2949010	A	0.5195456743240356	And I think that's because really in the active inference literature, there isn't a framework for interfaces and wiring and that framework is supplied by these categories of interfaces.
2949590	2965850	A	0.5598111748695374	And so you can just choose one and you can hope it does choose one that's appropriate for your situation that you're trying to model, but it helps to actually have one that allows you to do this wiring and that's something that using category theory allows you to do because it's all about composition.
2968670	2969420	B	0.9184247851371765	Awesome.
2970990	2972986	B	0.6467340588569641	The textbooks will be rewritten.
2973098	2981680	D	0.8376257419586182	Dean quick thing, Toby, does the timing of the choice in your experience really matter?
2984050	2988482	A	0.8017372488975525	The timing in what whenever, whenever you.
2988616	3003874	D	0.8487734794616699	Do decide to make a choice, doesn't the effect of the timing, the whenness of activating that choice play a huge part in what follows in terms of the category?
3004002	3004390	A	0.5491447448730469	Yeah.
3004460	3017050	A	0.8292586207389832	So do you mean like if you're choosing to use a particular kind of interface language, say or describe a thing with a particular interface or like particular kind of boundary?
3017630	3018378	A	0.5491447448730469	Yeah.
3018544	3020010	A	0.6921051740646362	So I agree.
3020160	3023514	A	0.7504923939704895	And that's why I kind of tend to work quite abstractly.
3023642	3035620	A	0.6764503121376038	So rather than saying I'm working with a particular kind of interface language, I'm just saying I will assume my interface language allows me to do these certain things.
3036310	3041380	A	0.8017164468765259	And so that's not so useful for actually implementing a particular model.
3042550	3051000	A	0.5340878963470459	But if you're trying to think about systems in general, it's helpful to work agnostically in that way.
3051530	3064170	A	0.844100296497345	And so the reason I sort of advocate like this for polynomial functions is I think that they sort of fit at a nice sort of intermediate level of abstraction.
3064750	3072880	A	0.5489487648010254	It's not to say, okay, well, there is a collection of interfaces and some collection wiring them together because that doesn't tell you actually how that works.
3073730	3076458	A	0.7659497261047363	So it is slightly opinionated.
3076634	3082560	A	0.824749231338501	It's different from certain choices that people in other bits of applied category theory make.
3083810	3096750	A	0.84730064868927	But it does generalize this simple case where you have just a single set of outputs and a single set of inputs or single set of actions and a single set of observations.
3096910	3105830	A	0.9482886791229248	And so I think because of that it's a nice sort of intermediate level and I think it's one I've tried to work with, because I think it's quite natural.
3106170	3109062	A	0.5749527215957642	But yeah, you can sort of totally work at a very abstract level.
3109116	3117260	A	0.8566107153892517	And that's what David Jazz does with his systems, just he just talks about interfaces in general.
3119790	3121980	A	0.575584888458252	You might not have actions at all.
3123950	3125610	A	0.8373503684997559	It's completely agnostic.
3130700	3131208	B	0.9184247851371765	Awesome.
3131294	3132040	B	0.6561701893806458	Ollie.
3134860	3136412	C	0.9688443541526794	OK, thank you so much.
3136466	3165290	C	0.8986151814460754	So my second question is about the relation between your compositional account of active inference and the recent line of research done in Bayesian mechanics, namely the discovery of the duality between FEP and constraint maximum entropy principle as discovered primarily through the work of Dalton Saktive Adebel in the last couple of years.
3166060	3178824	C	0.6584743857383728	There's this nice duality between the observer dependent the viewpoint from the inside observer of the agent and the outside observer.
3178872	3187560	C	0.8626075983047485	So, respectively, the perspective of FEP as opposed to constrained maximum entropy principle.
3187720	3200984	C	0.5431218147277832	But Dalton has demonstrated that actually, those two perspectives can be unified through showing the equivalency between the Bayesian optimality and James optimality.
3201052	3211780	C	0.6165302395820618	So from Bayesian optimality, we can obtain FEP, and from Jane's optimality, we can acquire CMAP.
3212120	3241520	C	0.8560227751731873	So, given that the notion of Bayesian lens also comprise both Bayesian inference and its inverse process, do you think the Bayesian lens as providing a kind of, I don't know, a tool to account for showing this kind of duality between FEP and constraint maximum entropy principle?
3242180	3244800	A	0.6733293533325195	I think the answer is probably yes.
3244870	3260292	A	0.7553322911262512	But I think this is a really great question, because genuinely, I don't know the story mean, I know a bit about Dalton's work, and I think it's wonderful, but I really don't know yet how it connects to the stuff I've been doing.
3260346	3263110	A	0.7665802836418152	And it's really like an open question for me.
3263480	3268712	A	0.8360101580619812	But there are some tantalizing signs that, as I say, I think the answer to your question is yes.
3268766	3280044	A	0.8642293810844421	There is a kind of unification of the two kinds of story, and some of the ingredients of that story are kind of floating around out there.
3280162	3297356	A	0.8649992346763611	So I know that there seems to be a connection, or like I know that there is a connection between quantities like the relative entropy and energetic concepts you get in physics.
3297548	3304000	A	0.8898885250091553	And through that, you can make a link to the kinds of things that Dalton has been doing with gauge theory.
3304160	3318884	A	0.8253522515296936	And I feel like there is a kind of geometrical structure that you can put on top of these statistical games that I've been working with, because, remember, they are these lenses with these loss functions.
3318932	3325610	A	0.6378797888755798	And these loss functions seem to have this kind of, as I say, this kind of geometric thing going on with them.
3326480	3346770	A	0.6416056752204895	But really explaining or putting constrained maximum entropy into statistical games language is not something that I've done and it's something that I've been thinking about doing, and I'd be delighted to talk to people about more.
3347380	3350640	A	0.5792402029037476	And so I don't know precisely the details.
3351560	3356870	A	0.9537930488586426	There is a kind of other duality, which I think is quite nice which is related to this.
3359640	3380670	A	0.8262017369270325	And I think again, at the nexus of these is this systems theoretic idea, because of course, all of this work of Dalton's lives in this kind of dynamical systems world where you have dynamical systems that are interacting and information flowing between them.
3383280	3390076	A	0.826505184173584	And there are some really tantalizing connections, again, of course, between physics and information processing.
3390108	3407540	A	0.7169914841651917	And I learned the other day, for instance, that there are certain classes of generative adversarial networks sorry, this is just a slight tangent, but I think you might be interested to know certain classes of generative adversarial networks that you can set them up.
3407690	3424436	A	0.5768607258796692	And the two parts of the network tend to instead of settling into an equilibrium, they seem to exhibit some orbits around each other so they don't achieve some equilibrium state in the game they're playing.
3424468	3426430	A	0.8053207397460938	They kind of just got around.
3428960	3441796	A	0.8187220096588135	But somebody was telling me about if you have an orbit and you're a physicist, you think, okay, well, what is that orbit conserving?
3441848	3444268	A	0.8894789218902588	Is there like a quantity that it's conserving?
3444444	3454576	A	0.8113922476768494	And in this case, and I don't know the details, but in this case, this person found that the quantity that was conserved in this orbit was the relative entropy.
3454688	3456756	A	0.5637140274047852	And so there's this weird thing going on.
3456858	3463540	A	0.5209735631942749	These things are playing a game, but they're still at this kind of somehow steady state of relative entropy.
3464780	3475610	A	0.8418988585472107	And so there seems to be this deep connection between these kinds of game like systems and sort of information physics in some sense.
3476460	3479924	A	0.6534722447395325	And I'm aware I need to head off for the moment, but I just want to draw one more picture.
3479972	3488030	A	0.5097495913505554	And I'm sorry, Elliot, you probably had loads of questions, but we kind of ran out of managed to answer many of them, but hopefully we'll get to the rest of them soon.
3488740	3503780	A	0.7313041090965271	But if I just draw, as I say, one more picture, which is a kind of slightly different kind of duality, but a similar kind of similar kind of flavor.
3504360	3509408	A	0.835533857345581	So what we have in systems theory is this category.
3509504	3521800	A	0.8852797150611877	You have this category of interfaces of systems and the morphisms tell you how to wire these systems together to give like hierarchical or like composite systems.
3523020	3544640	A	0.8551901578903198	And if you have something like this thing which I wrote before Din, which I said for each interface I was like a category, then you can sort of think of this as a functor from int to like cat the category of categories.
3545140	3556176	A	0.8712444305419922	And one thing you can do with index categories of this kind is you can sort of turn them into what are called like vibrations using what's known as the Groton deconstruction.
3556208	3566490	A	0.8037047386169434	And if you know about physics, you've probably heard of a concept called a fiber bundle, which is just like a collection of spaces indexed by points of another space.
3567580	3585660	A	0.8295493721961975	And so what you can do is you can write this vibration which says, okay, well, for each interface I've got a category of systems on that interface and they're all like glued together correspondingly with the wirings of your interface category.
3586320	3593088	A	0.48693379759788513	And so you write this as like another function which kind of forgets about the dynamical systems and just returns the interface again.
3593254	3599090	A	0.8671355843544006	And so you have this vibration thing here and this is like where kind of like physics lives.
3601800	3620020	A	0.8939698338508606	And so what I've been trying to tell this story about, in this story about active inference is that you have a similar one where you have for each type of interface, the category of agents that might have that interface.
3620180	3637080	A	0.7648165225982666	Like we're all humans and so we are all like the type that might fit in like a human shaped box and so we're all in that particular category and we come together to form organizations and that's another object of this category of interfaces and another kind of agent category on those interfaces.
3637240	3639416	A	0.8143197298049927	And so that gives you another one of these vibrations.
3639448	3652324	A	0.751570999622345	And the thing that something like predictive coding tells you is it says, okay, well, given a certain story about a type of agent, can I turn that into a dynamical system which actually animates it?
3652442	3655830	A	0.7889097332954407	And so that's like a functor like this.
3656680	3669770	A	0.8632010817527771	So maybe what we could do is we could think of this as maybe like some version of biology or we could think of this as like we could think of this as like body and we could sort of think of this as like mind.
3670700	3676730	A	0.6984156370162964	And so this is telling us how to go from a description of a mental system to a sort of embodied system.
3677100	3688430	A	0.8461330533027649	And I think some of what the free energy principle is trying to say is that for certain kinds of systems there's a way to turn your description of a dynamical system into something that looks like an agent.
3688820	3697570	A	0.6702839732170105	And so the thing I'm wondering is that whether there's some adjunction or maybe this is like I don't know.
3698100	3716410	A	0.8626368641853333	I don't know which way we might write labels on these, but maybe there's, like, some adjunction here that allows us to compare these two processes of turning a sort of mental system into a physical one and then going in the other direction from a physical system into a mental one.
3717100	3721530	A	0.8695232272148132	And so maybe this is also where information comes in.
3723340	3740224	A	0.8433125019073486	And it's a kind of similar story in category theory where there's a sort of duality between geometry and logic that that's sort of sometimes written like this.
3740262	3761764	A	0.7438052892684937	You have truth values down here and you have logic over here and you have space over here and there's a way to sort of draw a picture like this that says, okay, well, logic is somehow like dual to space in some sort of mumbo Jumboy way.
3761802	3773930	A	0.6954730153083801	I mean, again, I've run out of time, but I think there's a story that people try to tell about science in this kind of abstract world and I think it's a similar story to maybe something that's being told over here.
3775180	3782540	A	0.8388194441795349	Again, lots of details, and I don't know precisely the connection with Dalton's work, but I think it's kind of pointing in this kind of direction.
3784480	3785832	B	0.9332045912742615	Thank you, topy.
3785976	3797180	B	0.8585233688354492	After you have revised and proceeded, let's have 54.3 and talk more about action, active inference.
3797260	3798368	B	0.9191112518310547	So thank you.
3798534	3799232	A	0.8782029747962952	Yeah, great.
3799286	3800288	A	0.9890522360801697	Thank you so much for having me.
3800294	3802980	A	0.9215041399002075	And I look forward to speaking again and answering your questions.
3803050	3805700	C	0.9797496795654297	Ali, thank you so much.
3805850	3806804	C	0.9827191829681396	That was great.
3806922	3807700	A	0.8942707777023315	Okay, great.
3807770	3808630	A	0.6298522353172302	Bye, guys.
3809560	3810420	C	0.5137447118759155	Bye.
3811960	3824276	B	0.9059605002403259	Well, I'd like to ask Dean a oh, Ali, please make a remark.
3824308	3840908	C	0.5462459921836853	And then I have a yeah, you know, I really think of Toby's work as really provocative as it really opens up Pandora's box of many, many different fertile questions.
3841074	3854004	C	0.684840738773346	So I'm a firm believer of this old saying that a sign of a great book is not how many answers it provides, but actually how many questions it gives rise to.
3854042	3886236	C	0.7344213724136353	So I really wanted to congratulate Toby on writing such a I mean, it looks as if he began from the perspective of trying to address some of the I don't know, some long held questions, but then it turned out to be much more deeper than that in terms of actually giving rise to many fertile questions.
3886338	3889010	C	0.9867466688156128	So, yeah, that's really great.
3892050	3902660	B	0.8713397979736328	Dean, I'd like to ask, what is the difference or how can we think about being on time in time and out of time?
3904950	3908580	B	0.5810807943344116	Because Toby said he was out of time with the answer.
3909270	3918866	B	0.8831437230110168	And I know that we've spoken about this when we consider some of the time consciousness and category theory work previously.
3919058	3921240	B	0.755685031414032	So what is it to be out of time?
3922010	3924470	D	0.8799397945404053	Yeah, that's a great question.
3924540	3946510	D	0.6569072604179382	All I can say at this point is that it's easier to discretize when you're in time and out of time as a comparator than it is to say when you're on time versus those first two positions.
3949010	3973200	D	0.7260157465934753	That's all I can say about that for now, because I'm sure Toby, from a diagrammatic perspective, could probably tell us what is happening when he's actually using his stylus versus not right.
3973270	3977890	D	0.7299640774726868	Like the result of that leaves something in and out.
3978580	3986470	D	0.6311152577400208	But while he's actually on point, he's literally in a different place.
3989160	3993956	D	0.7803354859352112	Here's an interesting thing, back to the provocative Ali in the Dot.
3993988	4004250	D	0.5450826287269592	One was talking about virtual and actual models and was probably saying it in a way that's completely different than how I use.
4005280	4009710	D	0.797941267490387	I believe those same two models are in effect.
4010560	4043210	D	0.8720887899398804	And so if you, for example, draw up a play, say, in sports, because we're talking a lot of strategy while Toby was on today and a lot of gaming theory and stuff, you draw up a play, then you confirm that you have all of the elements or resources that you need in order to be able to carry that out.
4044220	4052344	D	0.7273651361465454	And then you're as a group communicating with one another like we are doing on this live stream, right?
4052382	4057384	D	0.6961867809295654	Like you're literally calling out the play in North American football.
4057432	4074880	D	0.7473713159561157	You'd huddle up and literally call out a sequence of code and then you'd have to literally reverse that and turn the inference as the shortest time window into a prediction.
4075300	4084470	D	0.8604015707969666	Like you would literally line up and then you would carry out the simulation in actual terms.
4086440	4107288	D	0.7809175252914429	All of that together means that I believe that the virtual version of the model is, as Toby was saying, in the bi directional sense, going in one direction, and the actual model is going in 180 degrees the opposite direction.
4107464	4119650	D	0.8461425304412842	And then when he mentioned the dagger symbol today, I was just like, okay, where does that fit in to this bi directionality and the difference between.
4121380	4121792	A	0.5665541291236877	The.
4121846	4125970	D	0.8315510749816895	SIM, the digital, the virtual, and the actual?
4126340	4142560	D	0.7505115270614624	So these are the things that literally pop up at any given moment, aren't necessarily part of what you consider to be the space that is inhabited by virtuality and actuality.
4142640	4144840	D	0.47585004568099976	And yet there it is.
4144990	4146804	D	0.6506764888763428	It just appears.
4146932	4151450	D	0.594010591506958	And then you have to sit down and figure out, well, where does that categorically fit?
4151980	4170156	D	0.48022353649139404	So again, don't want to go off on a deep rabbit hole philosophical take on this, but I just find that at any given moment, there is again, so much math in this philosophy that it's kind of staggering.
4170268	4178640	D	0.8132756948471069	And just to be able to have somebody who can explain that and draw it out is pretty impressive.
4180200	4183284	D	0.5883363485336304	I don't know if it's provocative what I just said.
4183402	4194360	D	0.4748324155807495	I hope it isn't because, man, I did this work for almost a decade and it seemed like young people weren't scared by it.
4194510	4196890	D	0.7408919930458069	Maybe older folks are, but I don't know.
4200910	4213470	C	0.8854442238807678	So if I may add a brief comment on just what you said, I was recently watching this talk by John Bayes about the history of applied category theory.
4214290	4227182	C	0.7647030353546143	So he had this nice diagram showing the relationship between different areas of knowledge, I mean, going from business to pure mathematics.
4227326	4252470	C	0.7879459857940674	And he claimed that most people in category theory, or in most other areas, consider category theory as being on the top of the pyramid, as being the purest of the pure, so as being even kind of being on a meta level than even the pure mathematics.
4252630	4262826	C	0.862341046333313	But then he said that I believe category theory can actually be seen as kind of permeating all of these levels.
4262938	4269150	C	0.5411648750305176	So it's not something that sits on top of the hierarchy.
4269230	4274382	C	0.6215129494667053	It's something that actually can be seen as influencing and permeating all the levels.
4274526	4294330	C	0.6601104140281677	So at least from the perspective of applied category theory, it seems to be, if not self evident, but something that gathers much, I mean, more evidence as the research goes by.
4294400	4300714	C	0.9671538472175598	So I'm really optimistic in the research direction that applied category theory is going.
4300832	4310030	C	0.6157602667808533	And hopefully in near future we can see it applied to many, many different areas, many different unforeseen areas.
4312770	4314654	D	0.7630781531333923	I think I'll just tag on to that.
4314692	4332630	D	0.8130751848220825	Ali, if you use an abductive logic lens, then it's simply a question of when did you enter, at what point did you enter the abstracted timeline?
4332970	4340454	D	0.7403802275657654	Because if you see it that way, it's not just top down and it's not just bottom up.
4340492	4355802	D	0.7609590888023376	It's back to my question to Toby was, does the whenness of this really matter if you want to be able to take it off the shelf and use it?
4355936	4373554	D	0.7524247765541077	And of course, his answer was, yeah, if you want to deal with idiosyncrasies, you're left with no choice but actually to acknowledge the point in the continuum where you take this up.
4373752	4380114	D	0.7388055324554443	But I think that actually makes it quite easy to see it permeating every level.
4380312	4390262	D	0.6495484113693237	But again, it's the abduction, the abductive piece of this back to last year in August, right?
4390316	4403722	D	0.5296418070793152	We're coming up on one year since we talked about how important that is, and yet here we are, what, ten and a half months later, and we're circling back.
4403776	4406394	D	0.7882711291313171	We have another predictive parabolic on our.
4406432	4407210	C	0.6563846468925476	Heads.
4411900	4414010	B	0.7107053995132446	Few comments on that.
4414700	4426316	B	0.6759909391403198	It's reminding me of Professor Mike Levin's work on basal cognition and the idea that intelligence isn't on the top of the pyramid either.
4426498	4434684	B	0.8297863602638245	Rather, it's something that's distributed compositionally across levels and something that exists at all levels.
4434732	4454980	B	0.8656291961669922	And so we can then still talk about higher order properties of systems or properties of composites or aggregate systems while also grounding in the foundations.
4456440	4457220	B	0.6071171164512634	Um.
4462230	4472002	D	0.7361826300621033	Yeah, and that, and that grounding and anchoring and stabilizing, that's by itself is nothing but a feature.
4472066	4488640	D	0.7709019184112549	I think you also want to be able to, though, because there was a lot of talk today about being able to see this in the way that things dynamically play out.
4489010	4499870	D	0.8409616947174072	So, again, we're back to the what's the minimum two ways that we want to incorporate this observation, both as stabilized and as searching.
4499950	4500580	D	0.5664746165275574	Right.
4501110	4508770	D	0.7869556546211243	If we can have one eye on each at all times, it does take a bit of rethinking.
4512890	4517480	D	0.8655502796173096	There is a there is a there is an interoceptive piece to this.
4517930	4522460	D	0.7990378737449646	What we feel from that is different.
4526030	4538106	B	0.9211398959159851	That brings us to the sensor fusion aspects of the dissertation 7.1.1 Bayesian sensor fusion.
4538298	4539040	D	0.5491447448730469	Yeah.
4540930	4547810	B	0.5601260662078857	A situation that is common in natural embodied systems, but which is not yet well treated by current statistical machine learning methods.
4548390	4553380	B	0.943867027759552	Also, Ali, thanks so much for asking the Markov blanket question.
4553910	4567718	B	0.49061593413352966	I think his response is worthy, honestly, to review and unpack in the time we have, we can just sort of review a little bit and think about what other questions we want to prepare for.
4567884	4591360	B	0.7211402654647827	Two, how would you, Ali, restate the limitations of the Markov blanket concept for all the positive things that we've said in the previous 300 live streams and where category theory can help address and move past some of that?
4593810	4614070	C	0.620817244052887	Yeah, you see, this notion obviously is one of the most or, I don't know, probably one of the most controversial concepts in whole FEP literature because it has received a fair amount of criticism from many various perspectives.
4614810	4626822	C	0.6400173306465149	But some criticisms, at least as far as I know, they're basically directed toward the universality of Markov blanket.
4626886	4637760	C	0.7245923280715942	How universal is this notion as it's claimed to be ranging from, I don't know, inert rocks to the brains, right.
4638130	4650850	C	0.8486878275871277	So does it apply equally for modeling inert rocks as well as the most complex system we know of in the whole universe, namely the brain?
4651270	4678630	C	0.7578558325767517	So the response this line of criticism got from the researchers in the active inference area, again, to the best of my understanding, is to the effect that although theoretically, it can be applied to many different systems with many different levels of complexity.
4678790	4693518	C	0.49173834919929504	But then again, for simple linear systems such as I don't know, many non complex systems, it doesn't have anything interesting to say.
4693684	4700834	C	0.5940917730331421	So rather than being applicable to all of these systems, the relevant question should.
4700872	4701460	A	0.5836908221244812	Be.
4705190	4716418	C	0.7168610692024231	There are enough insights we can gain from applying this notion to many different situations and systems.
4716514	4736694	C	0.7137967944145203	So I believe Toby's alternative for this problem by proposing the polynomial function as a replacement for Markup blanket can cover this line of criticism pretty nicely.
4736742	4753074	C	0.7404522895812988	It can address this criticism pretty nicely due to its abstractness and a kind of more rigorous way of talking about both the universality and the individuality of each system.
4753192	4768018	C	0.8116827607154846	So I think that's a nice way to at least a viable answer to this question that needs a lot more consideration.
4768114	4772380	C	0.8020758628845215	So that's my take on it as far as.
4774350	4793854	B	0.6837646961212158	Okay, I'll give my take two, which is the Markov blanket concept, since the work of Pearl in the 1990s is actually not doing that much philosophical work in and of itself.
4793972	4800334	B	0.8473029732704163	It just describes the parents, the children and the coparents of any given node within a graph.
4800382	4803006	B	0.6634711623191833	So first, we're talking about maps, not territories.
4803118	4808798	B	0.6675208210945129	We're not talking about features of the world, carving nature of the joints, necessarily hashtag malandrus.
4808894	4810802	B	0.890022337436676	We're talking about the Bayesian graph.
4810946	4821894	B	0.7661786675453186	And secondly, even on the graph, it's not that a node is like by its essence an internal state or a blanket state or an action state if you use the Friston blanket.
4821942	4824202	B	0.7549416422843933	Rather it's always in relationship to others.
4824336	4827434	B	0.7458276152610779	So the concept by itself does very little work.
4827552	4842702	B	0.8452949523925781	However, people have deployed the concept implicitly and explicitly to describe interfaces of cybernetic entities, the kinds of entities that it makes sense to describe as an active inference agent.
4842836	4861750	B	0.8600371479988098	So we know from the particular physics and the strange kinds work the path integrals paper that we can describe using the formalism, the rocks, the simple active systems and the sophisticated active systems.
4863610	4863926	A	0.5416930317878723	In.
4863948	4880250	B	0.7290439009666443	Order to have an interface concept that does justice to on one hand the relatively inert interfaces, on the other hand, potentially extremely conditional or contextual interfaces.
4882830	4893882	B	0.7236537933349609	Just having some tagging system of nodes on a Bayes graph is going to get extremely messy.
4894026	4900398	B	0.7242479920387268	It's not that it couldn't be done, but just in terms of actually doing the programming, it's going to be extremely messy.
4900574	4911350	B	0.8199278712272644	You're going to need to have a lot of accessory or auxiliary variables that indicate what the current interface is at this time or what it would be if this happened.
4911500	4926038	B	0.48516109585762024	And all these kinds of conditionalities would be like a less elegant way of describing what our category theory colleagues have been developing explicitly with the wiring diagrams and interfaces.
4926214	4935470	B	0.8493857979774475	So just to kind of conclude there people draw the Bayesian graphs and the particular physics is using a Bayesian graph.
4935970	4950130	B	0.6479347944259644	However, Bayesian graphs don't naturally have a lot of these properties that were really being conflated with interfaces more generally.
4951030	4971606	B	0.5095158219337463	And by using an interface concept, as Toby said, at an intermediate level of abstraction, that's slightly opinionated but also really flexible, that is kind of the right coarse graining, the right optimal grasp on the interfaces that we actually want to talk about for active inference agents.
4971788	4978854	B	0.5704056024551392	And again, just trying to get down to the firmware with the Bayesian graph, we're going to get lost in the details.
4978902	4987530	B	0.629239559173584	And any accounting system that we develop is going to be ad hoc and certainly less principled or powerful than what the category theorists are developing.
4988990	4989354	A	0.46103888750076294	Yes.
4989392	4993840	D	0.7232840657234192	Dean, I'm really curious what you think of this.
4994610	5004242	D	0.880698025226593	If we're talking about the relationship with the interface, could we see and I'm really asking this question.
5004296	5023242	D	0.7861286401748657	I don't know, I'm spitballing could we see the Markov blanket as the more stabilized version or form of the relationship and see the polynomial version of that as the search version of that relationship?
5023376	5024940	D	0.8471575379371643	What do you guys think of that?
5031070	5044400	D	0.7408474087715149	And I'm really asking, just listening to both of you talk here, I'm just wondering if I'm having one of those unitary moments myself or if I'm way out to.
5047010	5050820	B	0.8570461273193359	Ali, give a first thought, then I'll also give a speculative answer.
5052870	5053282	C	0.46103888750076294	Yes.
5053336	5071730	C	0.6067628264427185	So one thing that comes to mind is, you see, sometimes people consider markup blanket as being a kind of static boundary or interface that just separates the agent from its environment.
5071810	5084650	C	0.6964997053146362	But actually what it really is is kind of providing really a dynamic interface between those two internal and external states.
5084720	5089150	C	0.753986656665802	So it's not a static or stabilized in any way.
5089300	5100038	C	0.7007222175598145	But of course, in some situations we treat it as if it's stable or, I don't know, static coding.
5100154	5113400	C	0.8720574378967285	Maxwell Ramstead every theory of markup blanket is inherently per se, so there isn't any static conception of markup blanket at all.
5114410	5140938	C	0.8326702117919922	But on the other hand, well, obviously for the polynomial function, not just polynomial function, but the whole area, category theory deals with objects not in their static sense, but rather in the meaning they acquire through their relations, through their morphisms.
5141034	5170390	C	0.651955246925354	And it's not about so one of the main differences between category theory and the set theory is exactly hinges on that distinction between the static viewpoint of mathematical or, I don't know even non mathematical objects and the inherently dynamical or relational identity of the objects as defined in categories.
5170890	5198720	C	0.8089431524276733	So I believe polynomial functions again, by themselves are inherently and intrinsically dynamical, perceptual, and in the terms of some continental philosophers, such as Alan Badu, they acquire existence through their imminent relations.
5201810	5212690	C	0.7304044961929321	They don't need any notion of pre existence as some of the Platonic notions of mathematical objects and ideas.
5213210	5239520	C	0.7324027419090271	So yes, in this sense, I think polynomial functions, I'm not sure which one can be seen as, I don't know, more dynamic or processual than the other, but at least to a degree, both of those concepts can be seen as sufficiently processual, in my view.
5241170	5241774	B	0.9184247851371765	Awesome.
5241892	5245962	B	0.8693193197250366	Okay, I'll just add with a few other associative comments.
5246106	5256238	B	0.7865342497825623	One is to blanket something is to dampen and stabilize it, whereas polynomial seems to invoke, like many names, polynomos, polytopos.
5256414	5263018	B	0.5616493821144104	So potentially there's a multiplicity in the polynomial that is not acknowledged.
5263214	5277050	B	0.9027445912361145	So lexically in the Markov blanket and then a more formal way to say that would be a Markov blanket might be like a specific realization of some little neighborhood within the polynomial.
5277470	5281558	B	0.8506879806518555	So polynomial functor is a generalization of the Markov blanket.
5281734	5288766	B	0.7296971082687378	And so in that sense, it requires more searching and it is more abstract and it is more general.
5288948	5297200	B	0.7758983373641968	But when you do stabilize it down, then the Markov blanket formalism may absolutely apply.
5298930	5309540	B	0.8422099351882935	But wherever the Markov blanket formalism does apply, like, let's just say that we stabilize down to a computer system and we have an API and we consider that that's one of our blanket states.
5310310	5316726	B	0.7488976120948792	That would also be seen as an interface, of course, which is happy we'll talk about it.
5316748	5319080	B	0.7541056871414185	And so it would be a polynomial function.
5320250	5344270	B	0.509509265422821	And so this helps us look back at the literature and see, as has been done many, many times in FEP and other literatures, recent developments enable previous developments not to be invalidated, but understood as special cases or constrained or stabilized cases.
5345010	5347520	B	0.6320521831512451	And it only makes sense that we would start.
5349990	5357758	B	0.7973868250846863	Well, putting aside what makes sense in active inference, it seems that we are taking both paths.
5357934	5373720	B	0.8446525931358337	In some cases leading with the stabilized and generalizing, but in other situations making unstable generalizations that then precipitate stability inside of them.
5376010	5377930	D	0.8595374822616577	So more bi directionality.
5383530	5384182	B	0.7671424746513367	Great.
5384316	5393574	B	0.7738987803459167	Well, we will have a 54.3 with a focus more on action.
5393702	5398694	B	0.9263616800308228	Are there any other topics that you fellas would like to raise?
5398742	5402880	B	0.749932050704956	Or if anyone in the chat wants to ask a question, please go for it.
5403570	5410000	B	0.6496597528457642	In our last bit here, let's look back to the table of contents or Ali, please go for it.
5410930	5447370	C	0.8074318170547485	No, just I wanted to ask Toby about the relationship between his theory or compositional account of statistical inference as opposed to the category theoretical account developed about, I don't know, more than probably ten years ago by people like Bob Kirky and others specifically related to the distinction between classical and non classical Bayesian inference.
5448270	5457034	C	0.8834948539733887	That's been clearly distinguished in one of the papers from 2011.
5457082	5458894	C	0.6850260496139526	I guess so, yeah.
5458932	5473182	C	0.8683046698570251	That's one question I wanted to ask Toby, and of course a subsequent question about the relation of this distinction nondistinction to the accounts of quantum FEP.
5473246	5485640	C	0.8091292381286621	Because in quantum FEP markup blanket is kind of replaced, or rather blended with the notion of holographic inner screen.
5486170	5511230	C	0.7538975477218628	So how can we see quantum FEP as arising from quantum Bayesianism rather than from, I don't know, predictive quantum theory and the relation between those two frameworks of Bayesian inference to that topic?
5513750	5514500	B	0.9184247851371765	Awesome.
5514870	5522690	B	0.7782306671142578	Yes, the quantum category theory active FEP triangle.
5526220	5542396	B	0.7898391485214233	And also, I think the question about reading the literature in terms of the interfaces between a work and the citations, I think is extremely rich.
5542588	5553860	B	0.7908387184143066	It's a really novel way to perform bibliographic metaanalysis, but also just in our own learning journeys.
5555960	5579228	B	0.8470496535301208	This following this category theory dictum of knowing a thing by its relations understanding the paper within the web of the past papers which it cites, all of which are past and actual and then its future time cone of possible relationships to things that haven't happened yet.
5579394	5582488	B	0.7358882427215576	The co parents haven't even happened yet, necessarily.
5582664	5586910	B	0.8628984093666077	But that becomes part of the evaluation of the significance in the moment.
5587600	5602728	B	0.4910847246646881	And if we could have the right generative model and understand how much attention to weight onto different things in the moment, that would be like being able to see the whole time cone at the right core screening.
5602764	5605572	B	0.6237966418266296	But of course, as limited nest mates we can't do that.
5605706	5612020	B	0.8696662187576294	But we have our rules, our heuristics and our approaches that address that nonetheless.
5614940	5620090	B	0.9300482273101807	Anyone in the Chat or Dean, what would you like to explore in 54.3?
5623180	5627108	D	0.6075635552406311	I'm just going to follow the two of you.
5627294	5630524	D	0.6620871424674988	I got two big workhorses here.
5630722	5633340	D	0.633177638053894	I just got to get out of the way so I don't get trampled.
5637520	5643744	B	0.9161868095397949	Let's look at the table of contents here.
5643862	5655270	B	0.8187887668609619	We're just shifting to the code of document, the back end that we use to coordinate a lot of our work just so that it can all be seen in the video.
5659160	5660790	B	0.6925610303878784	Chapter one is short.
5663480	5671370	B	0.7190908193588257	Chapter two, if someone is looking for another account of like, why does category theory matter?
5672300	5693760	B	0.6135185360908508	Toby begins chapter two with three examples of situations neural circuits, Bayesian networks and computations where people often just use a kind of schematic diagram but it's not necessary or sufficient to really convey all of the richness that we can add on with category theory.
5698900	5713750	B	0.5277822017669678	Chapter three starts getting into more detail with string diagrams and developing towards circuit diagrams in this rate coding neural setting, a lot more new vocabulary arising along the way.
5715340	5725640	B	0.9123284220695496	Chapter four, which the question the beginning of this Livestream brought a focus to, is on the compositional aspects of probability.
5726780	5733468	B	0.8549935221672058	The copy discard structure is introduced, which we saw in the diagram today.
5733554	5748130	B	0.6095698475837708	That's when the piece of information is duplicated and then one of them can be thrown away Bayesian inversion, the dagger, Lady Macbeth's dagger always just dangling there.
5748580	5765690	B	0.5780715346336365	Then the growth and deek construction, bringing back strong recall of Shauna Dobson, Chris Field's, math, art and the complexity of the self.
5768940	5773400	B	0.8643285632133484	Chapter five went into dynamical systems.
5774780	5786700	B	0.8513824343681335	Markov chains open and closed dynamical systems with this polynomial interface concept.
5787040	5793650	B	0.5255360007286072	It's really built up in an extremely methodical way.
5794500	5803292	B	0.8329561352729797	First the dynamical system is kept, within a nutshell, extremely bounded, but then it's progressively opened up.
5803366	5812470	B	0.619439959526062	And so even though it's going to be a lot of new information reading the dissertation, it's done in a way that does also clearly build upon itself.
5813240	5840928	B	0.8319396376609802	Chapter six goes into the Bayesian lenses, connects to several more prevalent concepts in statistics maximum likelihood estimation, approximate Bayesian inference, auto encoders, and connects this to the emerging concepts of Bayesian lenses and statistical games in terms of the approximate inference doctrines in chapter seven.
5841094	5842450	B	0.724521815776825	Yeah, Ali, please.
5843140	5851520	C	0.6312755346298218	No, I just wanted to mention this particular chapter is being rewritten at the moment, so yeah, that's thank you.
5851670	5852236	B	0.4650449752807617	Absolutely.
5852358	5852692	B	0.46103888750076294	Yes.
5852746	5859488	B	0.6601691842079163	As we discussed a little earlier, chapter six is going to get simpler but better, but it was tricky.
5859584	5860640	B	0.5947525501251221	But it's simpler.
5860720	5861556	B	0.583800196647644	But it's better.
5861658	5862980	B	0.5954691767692566	But it was tricky.
5865260	5888380	B	0.6557377576828003	And then chapter seven closes it out, but really opens it up with the compositional, cognitive cartography sailing the Three Seas society of systems more alliteration, many, many connections to areas of interest for all system scientists and complexity enthusiasts.
5891940	5896780	B	0.6041869521141052	What a tour de force and a journey.
5896860	5904790	B	0.9838782548904419	And it's going to be awesome that we'll get to see some of the next increments in the work.
5905720	5907380	B	0.8411465883255005	Talk more with Toby.
5907800	5908900	B	0.883088231086731	Yes, Dean?
5910200	5911924	D	0.7954796552658081	Can I just throw this in?
5911962	5914896	D	0.6262509822845459	And it's going to indicate how old I am.
5914938	5930040	D	0.7988119721412659	But a long time ago, the first time a needle dropped on the first Van Halen album, something changed about everything that we thought about guitar playing.
5930200	5936312	D	0.5780211091041565	And if you weren't there in that moment, you wouldn't know how everything changed.
5936376	5963876	D	0.7511820793151855	I'm not sure what category theory taken up on a mass scale will do if it will be the equivalent moment for all people who use math in all the different ways that they use it, but I think that we are at the beginning of something that it is going to change things.
5963978	5982344	D	0.7816343903541565	And I don't know exactly how it's going to change things, but I think this will always now be included as how we try to both figure things out and out the figures in that three dimensional space.
5982382	5992770	D	0.8066239953041077	I mean, one of the things that I thought was interesting in the dot one was when Toby said, well, I work in 2D space and now I'm curious about I wonder what that would look like.
5994020	6001010	D	0.7847336530685425	Well, we exist in a 3D space and so I think the applicability of it is going off at any given moment.
6001540	6004470	D	0.563083827495575	But I do really think this is the start of something.
6006600	6012500	D	0.7923563122749329	I'm not sure exactly what, but I don't think we'll be able to unlearn this.
6012650	6015690	D	0.8397796154022217	I think it's going to remain influential for a long time.
6017580	6021370	B	0.8123513460159302	For those about to categorify, we salute you.
6022700	6023352	A	0.5491447448730469	Yeah.
6023486	6024120	D	0.5664746165275574	Right.
6024270	6025320	D	0.5760965347290039	There we go.
6025470	6026410	D	0.8529649972915649	Thank you.
6027760	6029470	D	0.8093723058700562	You can always make it better.
6033360	6035708	B	0.8169754147529602	Any other thoughts on this?
6035794	6036620	B	0.5559722185134888	Otherwise.
6041750	6042500	C	0.7671424746513367	Great.
6042950	6043314	A	0.5491447448730469	Yeah.
6043352	6045970	B	0.8716030716896057	Ali, please with any last thoughts?
6046950	6051166	C	0.9684136509895325	No, I just wanted to say I'm really looking forward for the next installment.
6051278	6057880	C	0.5363715887069702	I hope it happens soon because again, I have so many burning questions.
6058250	6066966	C	0.6804403066635132	I try mean whittle it down to some I don't know, not too much time consuming questions.
6067068	6074582	C	0.7185631990432739	But yeah, I'm really interested in I'm curious to know Toby's answers to them.
6074636	6079790	C	0.9876886010169983	So thank you so much for everything, for organizing this and inviting Toby.
6080210	6081600	B	0.5544491410255432	Oh, absolutely.
6082850	6085200	B	0.9048513174057007	All right, thank you, everybody.
6085570	6087760	B	0.7558448910713196	See you in 54.3.
6092370	6092650	A	0.5137447118759155	Bye.

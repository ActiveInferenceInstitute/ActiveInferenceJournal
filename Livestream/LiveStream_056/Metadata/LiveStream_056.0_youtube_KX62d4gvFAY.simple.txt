SPEAKER_00:
hello and welcome everyone it is november 2nd 2023 and blue and i are here in active livestream 56.0 kicking off the 56 series on a set of four papers welcome to the active inference institute we are a participatory online institute that is communicating learning and practicing applied active inference you can find us at links here

This is a recorded and an archived livestream, so please provide feedback so we can improve our work.

All backgrounds and perspectives are welcome and will follow video etiquette for livestreams.

Head over to ActiveInference.org if you want to learn more about the Institute, get involved in any activities or learning groups.

Let's begin with a little bit of an introduction and warmup, say hello, and then maybe just one thing that we found interesting about these papers or that made us want to jump in for the 56 series.

So I'm Daniel, I'm a researcher in California, and I was very interested to learn about what the relationship is between different neural inspired approaches to machine learning and active inference.

Bloop.


SPEAKER_01:
I mean, same, this kind of always like the neurobiologically inspired machine learning is always kind of something that piques my interest.

And I'm still here, still excited about it.

And in this like set of papers, I was actually like particularly interested in like sparsity and how that is maybe something that's particularly biological or not, or just kind of learning and diving deeper into sparsity as like a topic of consideration.


SPEAKER_00:
cool all right well in this series and in this video we're going to discuss four papers we'll get to why we have four soon four papers are the neural coding framework for learning generative models 2022

Convolutional Neural Generative Coding, Scaling Predictive Coding to Natural Images, 2022.

Spiking Neural Predictive Coding for Continually Learning from Data Streams, 2023.

And COG and GEN, Building the Kernel for Hyperdimensional Predictive Processing Cognitive Architecture, 2022.

And as there are four papers, we're gonna use acronyms for each paper and also go through them in this order.

again we'll return to why there's four and why this order the four acronyms are ncf22 cgnc22 snpc23 ppca22 so the video is going to be an introduction we're going to cover the big questions and then basically jump immediately into the four papers and this is just an introduction

feel free to write comments or if you're around in time to join for these upcoming discussions that we're going to have with author so blue what big questions are you excited about here


SPEAKER_01:
So I think I already talked about sparsity, but I would like to know, like, what other types of tasks that are perhaps maybe non-visual could neural generative coding be suited to?

And then, like, the concept of, in the last paper, we'll get to it, of, like, a memory echo as a prediction.

I think, like, is prediction just a memory echo?

And then, like, that kind of leads into...

um how like what is the difference between i predict or i expect or i hope right like i predict that the eagles might win the super bowl with like maybe

50 to 60%, you know, I predict, that's like the, where I, that's the probability, right?

50 to 60% probability that the Eagles win the Super Bowl, but like, I totally hope that the Eagles win the Super Bowl 100%.

So like, where's the difference between hoping, and like, I can actually, like, I thought about it, and it's like action, the role, like, you know, you hope something happens, you hope you get into a good college, you predict that you'll go to college, you hope you get into a good college, you, you know, get good grades, do all the things, and then you can act,

based on that hope, but like, I actually can take no action to affect whether or not the Eagles win the Super Bowl.

So action is not necessarily a key role there, but like, how is hope different from prediction and expectation?

And how would you approach modeling something like that?

It's just something that derailed me anyway.


SPEAKER_00:
that's interesting yep so my big questions first just on the scope how is it different now that we're in our first quadruple dot zero we've done one paper at once we've done two at once side by side four is double side by side and we love the four about the topics though we are going to get very

micro and zoomed in and we're gonna have some of the formalisms brought in but there's a lot more in the paper so how do these really mechanistic and or abstract micro level discussions of like neuron spiking

help us get closer to real cognitive architectures and modeling and then i think to kind of for um war of the fourth paper here's where we're gonna see something like the cognitive architecture that includes not just perception and memory and all these things but also the action component so

how does this connect to active inference like is this driving down the low road coming from the high road is it another road so where is the work that we're going to be looking at in this sequence of research related to Bayesian mechanics free energy principle some of these more first principles

intelligence conversations that we have.

So let's talk about why there are four papers.

So I hope this is okay that from Aurobio, I share something that he wrote.

He suggested the four papers and said, almost in that sort of order from the more base neural generative model coding, predictive coding framework and CF22,

going to be the first paper we discuss to its convolutional generalization cngc22 down to its spike level generalization snpc23 and back to its part in generalizing active inference to a robotic setting ppca22 in a sort of progression slash series

So hopefully this lays out our approach to these four papers, which we were happy to accept this curriculum and challenge.

The first paper that we're going to look at is going to set the kind of base canvas or overarching umbrella model, the NGC.

then there's going to be a convolutional and a spike level generalization or kind of additional motif and then we'll get to this question of more integrated cognitive modeling in the fourth bit any overall thoughts just on the four papers blue before we go to the first one


SPEAKER_01:
I mean, I'm curious to ask, what was the order that these started in?

Sometimes you just finish everything at once, or sometimes you start something and it takes forever.

So I'd like to know, what were the order that the projects were started in, as opposed to, I mean, we'd have the publication dates and whatever, but I would like to know, did they start and go in this order, just as a narrative from the author?


SPEAKER_00:
That's a great question.

All right.

First paper, Neural Coding Framework for Learning Generative Models, Aurobia and Kiffer 2022.

So.

Just let me know when to switch slides for this section, Blue.

Thank you.


SPEAKER_01:
Awesome.

OK, so we'll read the abstract.

Maybe I'll take a couple of sentences and then pass it to you, Daniel, if you don't mind.

Neural generative models can be used to learn complex probability distributions from data, to sample from them, and to produce probability density estimates.

We propose a computational framework for developing neural generative models inspired by the theory of predictive processing in the brain.

According to predictive processing theory, the neurons in the brain form a hierarchy in which neurons in one level form expectations about sensory inputs from another level.

Do you want to read the rest?


SPEAKER_00:
These neurons update their local models based on differences between their expectations and the observed signals.

In a similar way, artificial neurons in our generative models predict what neighboring neurons will do and adjust their parameters based on how well the predictions match reality.

In this work, we show that the neural generative models learned within our framework perform well in practice across several benchmark datasets and metrics, and either remain competitive with or significantly outperform other generative models with similar functionality, such as variational autoencoder.


SPEAKER_01:
Cool.

Thanks.

So the roadmap, this is a nature paper and it's dense.

So there are like 18 different formalisms in this paper and like many, many different models and sets of experiments.

So the roadmap is here and we are not going to go that deep into this because we're doing four papers and, you know, hopefully under 90 minutes.

And so we're going to go deep into the maths here, but hopefully we can get more heavy into it in the dot one.

So pretty much introduction results, discussion and methods.

But really what happens here is the results section kind of gives a lot more introductory information about like the back propagation and what learning looks like in the brain and the neurocoding framework and how this kind of forms a bridge here.

Then training the model and how the model updates everything.

And then what can the model actually do?

So we're going to kind of get into a little bit about the model and what the model can actually do.

And then Daniel, I think, will delve deeper into the next couple of papers into the model and then finally tie it back into active inference in the final paper.

So just that's an overall roadmap and then the roadmap for this paper, too.

So go ahead and flip it to the next one.

All right, so in this paper, the author states that in neurobiological systems, neurons make use of the information immediately available to them in both time and space and do not wait on distant regions in order to adjust their synapses, with global information provided through neurotransmitters such as dopamine.

Also later in the paper, he says, while the question as to how credit assignment is exactly implemented in the brain is an open one, it would prove useful to machine learning, computational neuroscience, and cognitive science to have a framework that demonstrates how a neural system can learn something as complex as a generative model without backpropagation using mechanisms and rules that are brain inspired.

So that's kind of what they set out to do in this paper.

So let's go ahead and flip it.

so here this is figure one and this is like figure one a uh looking at a back propagation model and so this is i from the legend in the figure he says credit assignment in back propagation or in back prop i won't keep uh elongating that credit assignment in back prop requires a strict global feedback pathway which requires the completion of the forward pass that carries information upstream right to left

This feedback pathway carries the error message E0 at the output layer back along left to right, the same synapses used in the forward pass to update downstream neurons Z1 and Z2.

Backprop differs from brain-like learning because, so this is within the paper, not the legend.

The synapses that make up the forward information pathway need to be directly used in reverse to communicate teaching signals, which is, this is known as the weight transport problem.

And so you can see that here.

So here it's going from left to right, like the output layer is here at E0, or the error message is here at E0.

And then the back propagation happens here at this red arrow.

Thank you, Daniel.

As I'm pointing to the screen, you're following my finger along.

Okay, neurons need to know about and communicate their own activation function's first derivative.

And that is shown here in the little deltas.

So neurons must wait for the neurons ahead of them to percolate their own error signals backwards before they know when or how to adjust their own synapses.

And this is known as the update locking problem.

then also there is a distinct form of information propagation for error feedback one that starts from the system's output and works its way back to the input layer so that's shown here in figure one which does not influence neural activity so in other words the global feedback pathway problem backprop creates signals that only affect weights but do not at least directly affect or improve the network the network's representation of the environment

So it seems like a lot of extra steps.

And then the error signals have a one to one correspondence with neurons.

So we can flip to the next slide.

So in this paper, the first four of those points are addressed by neural generative coding.

So everything except the final point, the one to one correspondence between the error signal and the neuron.

So this is the neural generative coding model.

So it says the proposed NGC model sidesteps this neurobiologically implausible requirement by learning with short local error transmission pathways that are made possible through recurrent error synapses and stateful neural activities.

Credit assignment under neural generative coding operates with local mismatch signals, E1 and E2, that readily communicate this information to their respective layers, Z1 and Z2.

So here the black arrows are forward propagation, while the red arrows are backwards transmission.

Solid lines indicate that a signal is transformed along a synapse, while dashed lines indicate direct copying of information.

So it's like the identity function, no transformation of the signal.

And then the delta shows the communication of the neuron's first derivative.

The triangle represents the computed change to the synapse of the forward pass that will use the nearby error signal.

And then the dot is the multiplication of incoming signals, which you see those all like we don't see the dot here, but it's in the backprop model.

So in the previous one.

OK, so that's the model.

That's all we're going to talk about.

for this so far so far so we're going to see now like what are the different models that they compared against so i'm just going to list them right here um so they used backprop models the regular regularized autoencoder and these are the abbreviations that are used here the gaussian variational autoencoder the constant various constant variance gaussian variational autoencoder

the adversarial autoencoder, and then the deep sparse rectifier network.

And that's purely discriminatively trained.

So I just want to point that out and it will be important as we get later.

Just the last one.

And then the neural generative coding models.

So this is actually the first one is from the like super seminal Rao and Ballard 1999 paper.

And then also a super seminal paper by Friston in 2008.

And then the later two models, which we'll talk about more in the dot one, are models that are developed by the authors of this paper.

So these G and C and models are both developed by the author.

And then the Gaussian mixture model is like just another classic benchmark model that's not back propagation or the generative code.

It's just a mixture of a Gaussian distribution.

So OK, next.

So what can these models do?

So generative neural coding can learn viable auto-associative generative models.

And let's check out some of the results that they got.

All right, so here we are, table one.

So I gave you all the models so that you can kind of see what's over here.

So the only ones that are neural generative coding are the GN-CN ones, so the last four.

And then the first two, the first one's from the Rao paper that says Rao, and then the second one with the sigma is the Fristen.

So we see here the regularized autoencoder autoassociative network least equipped to serve as a data synthesizer has a lower error, lower reconstruction error in terms of binary cross entropy compared to the other backpropagation generative models.

so the other gvae ones and then while the variational autoencoder models yield worse reconstruction than the regularized autoencoder they obtain much better log likelihoods especially compared to the gaussian mixed model baseline indicating that they are strong data samplers

And then the neural generative coding models, the ones developed by the authors here, obtain competitive log likelihood with the variational autoencoder models and with

The GN-CN-PDH yields the best log likelihood of all of them for all four datasets.

And so the datasets that they're talking about here, we're going to see some pictures of them in a little bit, but they're like the number datasets, the MNIST.

So not all of them are numbers.

So MNIST is like the classic number dataset.

And then the FMNIST is like pieces of clothes.

And then the Caltech one is like,

I don't know, some strange shapes.

So let's flip to the next one, and maybe it'll show us.

I think it does, actually.

Oh, no.

Let's flip back to figure four so we can look at the data sets, or to figure the next one.

Yeah, and then we'll flip back.

Yeah, let's see the data sets.

There you go.

Perfect.

So sorry, I should have reordered these.

But you can see, I'm trying to describe these samples.

So this one is like some handwriting.

I'm not sure.

Is that a language, Daniel, the K-M-N-I-S-T?

Do you know?

No.

It's not any language.

I was like, is it like, I don't know, is it Chinese, Arabic?

I can't read it, so I don't know.

But you can see here the MNIST is numbers, the FMNIST is closed, and like the rest are just like symbolic representations.

So just so you guys know, this is what they're looking at.

This is what they're trying to generate.

So then let's go back to figure four.

Oh, there we go.

Okay.

So figure four, this is the error, the BCE loss measurements.

So lower is better for pattern reconstruction on data subsets.

So this is measured in terms of the percentage of the original database sampled.

So here you can see that the BCE loss, which is like, we want less, less is better for all of the different, um,

models here shown.

So the RAE is blue, the variational is light green, the G-A-N-A-E, that's like another autoencoder, that is dark green.

And then the three, the pink, purple, and orange are neural generative coding models from Friston and then the two shown here.

So like they're not always, they don't always show every model in every figure.

But you can see here that the error for the three, like the pink, pink, orange, pink, pink, orange is pretty low all the way across the board here.

So I think that that's the important result.

And then we can flip on.

So neural generative coding also yields strong downstream pattern classifiers.

So let's see how the authors found that.

In table two, in terms of test error, we can see them all here.

So here's the different models.

The DSRN is the discriminatively trained model.

And then that's the autoencoder.

These are all the back propagation ones, the first four.

And then the last four are the neural generative coding models, with the final two being the ones used here, developed here by the authors.

So we can see the data sets here.

so the top two of each the top two um pictures in each row are the data sets like from the original data and then the bottom two are generated by um the model so we can see that this is how they are like how they're generating the samples so um here the the bottom two um even the very bottom one

Oh wait, yeah, the bottom three are competitive with the purely discriminatively trained model, the top one, which has the best output.

And they outperform all of the generative models, even the DSRN in one case.

So the bottom two models do really well in generating data.

So good.

uh and then figure six i think shows kind of the same thing so in figure 6a this is a t's knee plot of the data and the data clustering so we provide qualitative evidence that the latent representations of neural generative coding this is from the ncn pdh they appear to yield a stronger natural separation of the test data into class respective clusters compared with the back propagation based model

which was this was the best performing back prop model with respect to log likelihood and reconstruction error.

So we can see that there's like a more distinct clustering here.

And I think that that's that's what the authors are saying is the qualitative evidence.

Again, we emphasize that the neuro generative coding model acquired representations without labeled information.

So the class based relationships have emerged as a result of very sparse neural activities.

This is why I like I'm so interested in the sparsity here.

This offers some promising evidence that the neural generative coding models representations offer benefits beyond the original density estimation task, allowing reuse of the same system for downstream tasks like categorization.

Pretty neat.

Then neural generative coding can conduct pattern completion.

We'll click on the next one.

They didn't show the data here, they showed a little bit in the methods.

But they said that another interesting ability attributed to auto-associative memory models is their ability to complete partially corrupted or incomplete patterns.

And they hypothesized that a neural generative coding model's ability to conduct pattern completion better than the autoencoder models is due to its iterative inference processes.

So I'd like to see future work that kind of probes at this a little bit.


SPEAKER_00:
Thank you, Blue.

That's super helpful.

Very nice.

Any other comments on the paper?


SPEAKER_02:
No.


SPEAKER_00:
Yeah.

They replaced this time lock, weight transport bound, global feedback constraint setting with a biologically inspired, locally computable, et cetera, et cetera, architecture.

that is kind of the base case that we're now going to dive into two specifications or elaborations of convolutional first then spike level and then in the fourth paper it's going to come back to mainly blue with the overview of the cognitive architecture okay can you read the abstract for this one sorry


SPEAKER_01:
In this work, we develop convolutional neural generative coding, ConvNGC, a generalization of predictive coding to the case of convolution-deconvolution-based computation.

Specifically, we correctly implement a flexible neurobiologically motivated algorithm that progressively refines latent state feature maps in order to dynamically form a more accurate internal representation slash reconstruction model of natural images.

The performance of the resulting sensory processing system is evaluated on complex data sets, such as the color minced, CIFAR-10, and street house view numbers.

We study the effectiveness of our brain-inspired model on the tasks of reconstruction and image denoising and find that it is competitive with convolutional autoencoding systems trained by backpropagation of errors and outperforms them with respect to out-of-distribution reconstruction, including the full 90K CINIC-10 test set.


SPEAKER_00:
thank you so this paper is somewhat similar and related to the previous paper except as promised it's going to generalize it into the convolutional setting so there's also a lot of nice

exposition in this paper so it will be good to get some background too introduction then the convolutional neural generative coding model details on the theory and the practice of the experiments about what was tested and done

and they summarize contributions as follows one we propose a new neural perception model conv ngc which acquires robust representations of natural images in an unsupervised fashion two to the best of our knowledge this is the first work in the literature where visual inputs are processed using deep deconvolutional layers directly and naturally within a framework of predictive coding

significantly enhancing its representation power for vision-based tasks.

And three, we demonstrate on several natural image datasets that the proposed ConvNGC is competitive with existing backprop-based models of similar architectural designs on the tasks of image reconstruction and image denoising and outperforms them with respect to out-of-distribution prediction.

So some of the first sentences of this paper that set the stage really well.

The first sentence begins with backpropagation and highlighting that while it's done well, riding on the tide of increasing power of hardware and software, there are still some limitations associated with backprop-based AI.

The field still has a long way to go towards developing artificial general intelligence.

I think it'll be an interesting question.

Is that what this field or who in this field means what by that?

order to increase task level performance the size of deep networks has increased greatly over the years up to hundreds of billions of synaptic parameters as seen in modern day transformer networks this scaling is all you need paradigm increases the number of parameters and the deployment of multiple motifs like transformer architectures attention motifs and so on as well as changing and curing data sets and we are now getting up to models in the 2020s that have billions of parameters

However, this trend has started to raise concerns related to energy consumption and more.

energy consumption limited flexibility and generalization limited biological plausibility and this results in some computational constraints and limitations especially around parallelization federation learning all these kinds of settings that we want to have effective algorithms for so people are looking into different methods biologically based and otherwise that speak to different aspects of this

They write, some of the most promising pathways in addressing all those problems listed previously come from the emerging domain of research known as brain inspired computation, which seeks to develop neural architectures and their respective credit assignment algorithms that leverage only local information motivated strongly by how learning is conducted in the brain.

the promise of brain-inspired computing brings with it synaptic adjustment mechanisms that are neurobiologically grounded as well as neural computation and inference that is flexible capable of providing a wide variety of operations at biologically more faithful levels

facilitating massive algorithmic parallelization at scale and adaption on analog and neuromorphic hardware.

So it's an interesting discussion topics.

What aspects or properties or processes or features or perspectives are we trying to copy about the brain?

And while we were preparing for this in some open sessions, someone told me about this spy kid's third brain and this kind of

designing a brain and what does it mean to really design cognitive architectures what does it mean to consider diverse intelligences how is this related to the kinds of large-scale scientific research projects that are going on in the human brain any thoughts on that blue

the end of the introduction they again summarize their contributions that we looked at previously and before we jump in this is from the appendix of the paper here's some key symbol operator abbreviation definitions in table three a little bit of exposure to the symbols we're going to see

Section 2, Convolutional Neural Generative Coding.

We start by describing our model instantiation of a Convolutional Neural Generative Coding , which is tasked with learning from streams of natural images in an unsupervised fashion.

Typically in a visual recognition task, input patterns belonging to different object classes arranged into batches are presented to a processing system at different time points for training.

Here, they formalize the problem and give a bit more information on the notation.

CONF NGC is a generalization of the NGC computational framework from 41.

That was the NCF 22 paper that we just heard about.

generalizing it to the case of natural image data.

So in those other data sets like MNIST and others, every pixel is just passed in of a relatively small image so that the whole image can be taken in as a matrix, like a binary matrix with black and white squares or a grayscale matrix.

However, real visual data coming in, for example, off of a camera sensor is apt to have a much higher resolution.

And so a biologically inspired design pattern

that we find in the retina and other kinds of biological visual sensors is this idea of convolution, where a convolving function is used to blur and essentially get a core screening across a high resolution visual input.

The underlying process of conv NGC can be divided into three components.

local prediction and error unit map calculation latent state map correction and local synaptic adjustment so this is drawing on the architecture and the motifs that were introduced previously bringing in convolution so that natural images can be addressed here's the conv ngc model it consists of a set of l predictive layers

typically arranged hierarchically though this is not a strict architectural requirement eg figure one depicts a non-hierarchical NGC circuit so that was figure one here's figure two which is I guess a more hierarchically structured conf NGC with three layers

Section three goes into more detail on this model.

In this section, we provide concrete implementation details of the neural coding process described earlier, depicting how layer-wise state prediction, state correction, and synaptic parameter updating occurs specifically in the context of visual object reconstruction.

The first two steps of the algorithm, iteratively predicting correct representations of the CONT-NGC model for observed input values, natural images of the current dataset.

After T iterations, the final step entails adjusting model synaptic efficacies using simple Hebbian-like updates.

So these are the three main steps.

There's a two-stroke engine with prediction and correction, kind of like an expectation maximization.

And then after a certain number of iterations T, there's the adjustment of synaptic efficacies.

So here is from the appendix.

This is the pseudocode of the algorithm.

Section 3.1, inference, predicting and correcting neural states.

Going through those steps, layer-wise state map prediction and state map correction.

Then 3.2, training, updating the model's parameters.

Having described the model, essentially, they go to section four, the experiment.

We compare ConvNGC to powerful back-prop based models such as the convolutional autoencoder, denoising convolutional autoencoder, as well as the fully connected form of neural generative coding, which under certain conditions recovers the unsupervised form of the model in 55, which is Salvatore et al.

hierarchical predictive coding network.

Details are provided about the data sets used.

And tables 1 and 2 provide some performance measures evaluated in terms of mean squared error, structural similarity index measures, and peak signal-to-noise ratios.

One can look to the tables to see where the different models do well or not.

Here are figures four and three.

Figure three shows an image randomly sampled from a dataset.

Left, the same image corrupted with noise.

So here's the original image, a five and some kind of object, and then a digitally corrupted version with a normally distributed noise term added.

And then after processing, this is the denoised image.

That's figure three.

Figure four, feature maps for the bottom three layers of a trained ConvNGC model.

So this just shows a little bit of kind of the inner workings of the model.

And surprisingly, with respect to the out of distribution experiments, OOD, ConvNGC outperforms all the baseline models.

This desirable improvement is indicated by the higher trial averaged out-of-distribution peak signal-to-noise ratio, as well as the structural similarity scores.

What is most impressive is that the CONV-NGC model was trained on natural images in the housing numbers dataset that did not contain any of the objects found within either CIFAR-10 and CINIC-10 and was still able to reconstruct with top performance.

This result offers a promising future direction worth exploring for conv NGC and NGC predictive coding systems in general, their ability to generalize to unseen patterns that violate the assumption that they were generated by a distribution similar to that of a training data.

So this is, I think, going to be an interesting discussion point.

It's kind of like if you classified a visual detector based upon only a data set with a certain kind of edge or surface,

then it's possible that the model wouldn't learn more general features of edges and surfaces it might be able to use a back propagation type updating credit assignment paradigm so that something really really high level and abstract like whether it's a cat or a dog could be tagged with something very micro which is overfit to cats versus dogs or even more locally like that cat versus that dog data set

And so potentially constraining to these local connections only making a kind of enforced sparsity means that the messages that get passed are not just more biologically plausible and fewer of them, but potentially they extract features more like the visual system does in terms of extracting generalized edges and features through lower level

features that kind of percolate up into higher level features in a really methodical way, which is why we can recognize a shape that we've never seen before.

So in conclusion, here's the whole conclusion.

Our results mark an important step towards crafting more robust general brain inspired neural architectures and learning processes capable of handling complex machine learning tasks.

So some questions for discussion are written here.

And there are three very informative appendices in this paper.

So for more detail, if you think this is cool, definitely read the paper, check out the appendices.

Onto the third paper.

This is SNPC23.

Want to add anything on that, Blue?

Spiking neural predictive coding for continually learning from data streams.

Do you want to read the abstract?


SPEAKER_01:
Sure.

For energy-efficient computation in specialized neuromorphic hardware, we present spiking neural coding, an instantiation of a family of artificial neural models grounded in the theory of predictive coding.

This model, the first of its kind, works by operating in a never-ending process of guess and check, where neurons predict the activity values of one another and then adjust their own activities to make better future predictions.

The iterative, interactive nature of our system fits well into the continuous time formulation of sensory stream prediction and, as we show, the model structure yields a local synaptic update rule, which can be used to complement or as an alternative to online spike timing dependent plasticity.

In this article, we experiment with an instantiation of our model consisting of leaky integrate and fire units.

However, the framework within which our system is situated can naturally incorporate more complex neurons, such as the Hodgkin-Huxley model.

Our experimental results in pattern recognition demonstrate the potential of the model when binary spike trains are the primary paradigm for interneuron communication.

Notably, spiking neural coding is competitive in terms of classification performance

and experiences less forgetting when learning from a task sequence, offering a more computationally economical, biologically motivated alternative to popular artificial neural networks.


SPEAKER_00:
Awesome.

Thank you.

Okay, here's the roadmap and a key piece of this paper.

So they start with the introduction, then go to the methodology, define the problem really well, and introduce the spiking neural coding framework.

in order to implement this kind of continual error correction from the spike trains.

Section 3 covers the experimental results, simulations, and limitations.

Then there's a conclusion and an appendix.

They write, in this paper, we propose spiking neural coding as well as a concrete model instantiation that is composed of leaky integrate and fire neurons and its learning process for adjusting synaptic weight strengths incrementally.

Although we only examined one kind of spiking neuron, our general framework can accommodate simulated neural models of varying complexity.

So it's going to be a simple version of spiking with a binary spike train and a leaky integrate and fire neuron.

However,

There's even sub-generalizations within this spiking paradigm.

So just as a kind of reminder, we introduced the general local compute only predictive coding architecture in the first paper.

Second paper looked into the convolutional generalization.

both the first and the second paper though were trained in essentially batch mode training data sets were loaded in and they were trained in a sort of batch based approach here through this spiking mechanism

think about a trace through time and spikes happening along that timeline, the data are coming in, not in a batch that the computer gets to process at the kind of pace or way it wants, but rather the data are coming in real time.

So this generalization is much more fit to both the real time, like internet of things, semantic spatial web setting, but also the biological perception setting.

Here, there's a similar introduction

Instead of just harping on backprop though, artificial neural networks, ANNs, are the models which are described in terms of being very large and powerful, but with challenges.

And surprise, surprise, people are using brain-inspired computing to formalize and implement various kinds of neural computation and learning mechanisms currently known in cognitive neuroscience.

And

only bring a few of these onto the slides but i have been taking many sections of text and passing it through different image generators so it's always fun to like see visual semantics of what sometimes these otherwise really wordy sections are about

so section two 2.1 problem definition continually predicting sensory stream inputs so this is the key difference spiking is a motif or an architecture that is going to enable us to move from the batch based setting to the real time or the online setting the neural agents that we are ultimately concerned with process sensory stimuli

which are sampled from an environment or a stochastic process over time.

So here we see a really nice connection to epistemic foraging, information foraging in active inference.

And here are some specifications of what is meant by that, and also more about the data stream formulation.

It still treats each data point as a discrete element.

The spiking neural networks are intended to process input stimuli that arrive with a continuous flow of time.

In order to truly adapt the above stream to the spiking neural net, we further generalize the settings so that each tuple is presented to the learner for a fixed period of simulated time.

And the length of the stimulus and inter-stimulus times will vary depending on the data problem and application.

So it's not the case that it's just an infinitely short flash.

it's the case that it's there for a defined amount of time kind of like a video frame so if a video is 25 or 30 frames per second then that is in this sort of 30 to 50 milliseconds range of presentation of a given stimuli so that allows the connection between the frequency of the regularities in the pattern in the environment

to be connected to the rate of updating and of like the real biophysics of neuronal membranes or of some neuromorphic computer.

2.2 gives a mechanistic description of the spiking neural coding network.

So this is the spiking-based generalization.

More formalisms and information.

Here's what it looks like in figure one.

two layer spiking neural coding network architecture we see a lot of motifs from the previous papers this kind of local loop and the ease and disease green diamonds indicate the error units e which in this simplified model depiction compute the amount of mismatch between the current predictions which are the z's

of sub mu and the target signal traces z's of t variables s represent the binary spike outputs of a particular vector grouping of neural units at time t so more formalisms are provided and just like the last paper there is an algorithm in the appendix here's what the sp and cn algorithm looks like

Figure two, I'm just overlaying figure one just on top.

It's not meant to overlay it directly, but just to remember that this is the kind of architecture that spike trains are being passed through or are occurring within.

So figure two is showing these kind of biologically reasonable, despite the simple integrate and fire model.

traces, kind of like a single neuron recording from inside of this little in silico brain.

Section three, in silico results with the model simulation.

First, they looked at the MNIST, the digit classification data set.

They presented two sets of experiments that tested the SP-NCN's ability to generate and classify patterns, as well as its ability to retain knowledge over task sequence.

And the appendix has more experiments.

So table one, two, and three have different evaluations and scores.

And then also just like the t-SNE that Blue mentioned in the first paper, here's a t-SNE now of the SP-NCN with access to MNIST training upfront.

in the middle the continual SP-NCN and a continual SP-NCN with a buff versus the lat so we'll go into more detail with these sub-variants but here we can see that this real-time learning is actually recapitulating similar structure and near enough similar learning performance as batch learners which is a big theme of the paper

so here is a figure five showing task accuracy normalized to zero one over proportion of a current task data and so on the x-axis is the portion of the task completed on the y-axis is the accuracy range so here it's shown for the three spncn variants but that was done for not just one data set but that was actually done for a whole set of data sets in figure five

And so the big takeaway from this whole figure is that SP-NCN obtains competitive cross-task performance even though it is trained online.

It only processes data patterns once, whereas to obtain top performance, rate-coded baselines were trained on each task for multiple epoch.

Pretty cool.

And then figure four, again, is a t-SNE layout of these different model variants.

3.2 describes some limitations.

And then the conclusion.

In this paper, we propose spiking neural coding, as well as a concrete model instantiation composed of leaky integrate and fire neurons and its learning process for adjusting synaptic weight strengths incrementally.

Pattern classification and continual learning experiments demonstrate the generalization ability of our model in the realm of online continuous time-based learning.

And the model is competitive with many spiking neural networks learned with spike time dependent plasticity, STDP, or other mechanisms.

Further, our model exhibits better computational economy and is more biologically plausible than modern day non-spiking artificial neural networks.

And then it'll be cool to discuss future work.

to kind of close this paper and head into our fourth one we started with a big picture with predictive coding things we've talked about in many live streams we just heard in the last two papers the convolutional extension which enables higher resolution naturalistic images

and then this spiking variant which brings it into the setting of real-time online learning through spiking which also has lower energy requirements than non-spiking neural networks and now back to blue for the fourth paper cog and gen building the kernel for a hyperdimensional predictive processing cognitive architecture cool thanks daniel that was neat


SPEAKER_01:
Do you want to read this abstract?


SPEAKER_00:
Sure.

We present a new cognitive architecture that combines two neurobiologically plausible computational elements.

One, a variant of predictive processing known as neural generative coding, NGC, and two hyperdimensional slash vector symbolic models of human memory.

We draw inspiration from well-known cognitive architectures such as ACT-R, SOR, LIABRA, and SPOUNENGO.

Our cognitive architecture, the Cognitive Neural Generative System is in broad agreement with these architectures but provides a level of detail between ACT-R's high-level symbolic description of human cognition and SPOUN's low-level

neurobiological description.

Cogngen creates the groundwork for developing agents that learn continually from diverse tasks and model human performance at larger scales than what is possible with existent cognitive architectures.

We aim to develop a cognitive architecture that has the power of modern machine learning techniques while retaining long-term memory, single trial planning, transfer learning, planning,

and other capacities associated with high-level cognition.

We test COG and GEN on a set of maze learning tasks, including mazes that test short-term memory and planning and find that the synergy between its predictive processing and vector symbolic components allow it to master the maze tasks.

Future work includes testing COG and GEN on more tasks and exploring methods for efficiently scaling hyperdimensional memory models to lifetime learning.


SPEAKER_01:
Cool.

Thanks so much.

So unlike the first paper where we talked very little about the model and a lot about the tests and results, we're going to talk a lot about the model and very little about the tests and results in this paper.

So kind of dive a little bit deeper in here.

Okay.

Do you want to flip through to the next one?

All right, so the authors claim they propose a cognitive architecture that is built on two neurobiologically and cognitively plausible models, namely a variant of predictive processing known as neural generative coding from this 2020 paper, which we did not go into here today, and vector symbolic, also known as hyperdimensional models of memory.

And we're going to talk a lot about this Hintzman 1986 model.

It's called Minerva 2.

and then a few other references here okay so this is the common model of cognition and this is like a conglomeration of all these different references that are here in the legend um and it just shows like there's some commonalities that go like across different

um cognition system models okay so you have declarative long-term memory um also that's associated with the hippocampus and the temporal lobe and here that's achieved with minerva ii there is working memory which is the frontoparietal network they did this with minerva ii or the self-recurrent slot buffer

which we'll talk about later, later, later.

And then there's procedural memory that's associated with the basal ganglia done with neural generative coding here, perception and action.

So finally we get to perception and action after, you know, are we an hour deep so far or an hour in?

So finally we're here at perception and action, we've arrived.

So the perception is the sensory cortices and action is the motor cortex.

And both of these are achieved with the neural generative coding model.

Next slide.


UNKNOWN:
Cool.


SPEAKER_01:
Okay.

So we talk about a lot of neural building blocks.

I mean, I think that we just went through kind of all of the cognitive architecture building blocks.

And here, just to kind of get into the model a little bit.

So this first equation one is at the top right.

Okay.

So given an input output pair of sensory vectors, XI and XO,

where x is the x i is the input and x o is the output the circuit clamps the last layer z l so you see that up here in equation one to the input and clamps the first layer z zero to the output once clamped

The neural generative coding circuit will undergo a settling cycle where it processes the input and output vectors for k steps in time.

So it processes sensory signals over a stimulus window of k discrete time steps.

So that's just kind of like a broad overview of how

The system is working.

E is a matrix with error synapses that passes mismatched signals across the layers from L minus one to L. And then let's look.

So beta is the neural state update coefficient that's set according to one over tau, where tau is the integration time constant in the order of milliseconds.

so here you can see that this update equation indicates that a vector of neural activity changes at each step within a settling cycle according to from left to right in equation one a leak term modulated by gamma the bottom pressure from mismatched signals in lower level regions and a top-down pressure from the neural region above so that's kind of how this algorithm is working in a in a short summary kind of way okay do you want to flip to the next one

Okay, so neural building blocks, adding to memory.

So this is this reference that I told you we were going to talk a lot about, 1984.

I think it said 1986 somewhere.

Maybe there's like an updated model.

But Minerva 2 stores memory traces, observations, or sequences of observations of the state of the world.

Each memory trace is represented as an array of real numbers, a vector.

Minerva 2 stores all memory traces separately as rows in a continuously growing table.

we use Minerva 2's forgetting mechanism, oh, that's the 1986 reference, to randomly delete values from memory each time memory is updated at sufficiently high probability to impose a computationally tractable limit on the size of the memory.

So if there was no forgetting, there would be, like, this would be an impossible problem, just an impossible method by which to store the memory.

Okay, next.

So this is how you get things out of memory.

So in Minerva 2, memory retrieval is not a lookup process.

It's a reconstruction process.

So when a retrieval cue or a probe is presented, like the smell of chocolate chip cookies, each vector in the memory table is activated in proportion to its similarity to the cue.

So similarity is computed as a normalized dot product of the Q's vector with the stored vector.

So every stored vector is activated by its similarity to the probe raised to a power, and by raising the similarity to some power, the contribution of the most similar vectors is emphasized.

And then this is what I was talking about at the very beginning, about an echo.

So it says here, an echo is retrieved from memory as a weighted sum.

So that's this equation in the top right.

So where ME is the echo or the output from memory, MI is the ith trace in memory, MP is the probe or input to memory, and M is the number of traces in the memory table.

And so we're gonna look in the next slide about, so is prediction just an echo from memory?

So we're gonna look at that maybe here.

Oh yeah, it's here.

So it says, in CoggenGen, we combine insights from several versions of Minerva 2.

We make an architecture-wide commitment to predict then correct learning.

Predict then correct can be implemented in Minerva 2 as discrepancy in coding.

Given a sequence of observations, Minerva 2 can predict the next observation based on past experience.

After the prediction, a new observation is made.

We update memory with the difference between observation Mx and prediction Me.

So wasn't that just an echo?

Is prediction an echo from memory?

I think that that's what they're saying here.

Mx minus Me, so observation minus prediction, such that if the prediction is correct, no change is made to memory, whereas if the prediction is wrong, that prediction is inhibited in similar future contexts.


SPEAKER_00:
This is awesome.

I love that you're pulling on this kind of echo and prediction and memory because it's also related to how we learn.

Like, we predict then correct, and that's kind of like the prepare, measure, learn, quantum information, quantum FEP connection.

then how how rich could a learning experience be if we predicted what do you think the next slide's gonna be like and then i learn about you i learn about the paper and then no skin in the game it comes up i was off because i didn't know you learn that's the update and then you learn on the learning so there's no end to how many layers you could learn with just like one data point


SPEAKER_01:
Right, like what's my prediction of the next slide?

I theoretically have that in my memory as an echo somewhere, but it's not coming to mind.

So we'll find out when we get there.

All right, here we go.

So short-term versus long-term memory.

So for CoggenGen, we adopt Collins et al's 2020 approach and model both working and declarative memory using Minerva2.

Our working Minerva 2 is cleared after a task is completed.

For example, a maze is solved.

Whereas the contents of the declarative Minerva 2, which serves as the episodic memory in this paper's instantiation of CoggenGen, persist across tasks.

So there's some memory clear when the task is over.


SPEAKER_00:
yeah short-term memory that gets cleared after like each example like each question on a test each maze but then the declarative way of knowing is being kept episodically so like a memory of solving that maze is stored declaratively but each maze is each only being addressed with the working memory so it's cool


SPEAKER_01:
So there's not a lot here in the perception place, so...

They say that the environment that they investigate, which is gym mini grid, which is an open AI environment.

So if you guys are interested, you can go check out open AI gym mini grid.

But it provides a fixed problem specific encoder FE and decoder FG.

So they use a fixed encoding and decoding scheme to simplify the simulation and future work is going to investigate learning the perception modules as neural generative coding circuits instead.

So they didn't do that for this

paper next paper that's coming okay so neural generative procedural memory action selection is driven by changes in the neural activity of basal ganglia which estimate the value of the expected reward and these authors were motivated by the finding of expected value estimation in the brain so they made the cog and jen's procedural module implement a neural circuit neural circuit

that produces intrinsic reward signals.

At a high level, this neural machinery facilitates some of the functionality offered by the basal ganglia and procedural memory, simulating an internal reward creation process.

so they say we implement a neural generative coding dynamics model from which a reward signal is calculated as a function of its own error neurons we couple the dynamics model with a short-term memory module based on minerva 2 which adjusts the reward value produced by the dynamic circuit by determining if an observed state is familiar or not yeah and then


SPEAKER_00:
just you're lagging you're gonna see that in a figure in the next figure okay i'm gonna let your internet just return okay i shut off my video okay okay yeah the only comment on this is um this idea of the intrinsic reward signal happening from a procedural module yeah i'm good can you hear me yeah

so it's like epistemic foraging you have a procedure for foraging and then there's an intrinsic reward even arising just by carrying out the procedure which might be the predict and learn and reflect dot zero dot one dot two with an intrinsic smooth reward signal rather than kind of a needle in the haystack more like

being just in a continuous flow.

Is your internet back?


SPEAKER_01:
I think so.

Can you hear me?


SPEAKER_00:
Okay, yes.

Figure three, then continue.

Okay, now you can hear me.

Yep.

Okay, perfect.


SPEAKER_01:
I just I turned off my video.

So if I get the flash again, I'll turn it off because sometimes it's easier.

Okay, figure three.

Perfect.

So, and just what you were saying about the intrinsic reward before I delve into figure three, they did talk a little bit in this paper, and I don't know if I pushed it into here, but maybe we'll get into it in the dot two, but they did talk about trying to push exploration via an intrinsic reward signal.

So we can kind of get into that exploration exploitation discussion in the dot two a little bit more.

Okay.

so figure three the kagen gen dynamics model um it says that arrows transform data across across synapses um and then the gray circles represent stateful neurons and the green diamonds represent error neurons so this is um the multi-layer uh model that we're looking at here with an external input and z of t

And we can see the layers across time.

So I think maybe it says more in the next slide.

Let's see.

So building upon the notion of planning as inference, we generalized neural generative coding to the case of action-driven tasks, which we call active neural generative coding.

Notably, within the neural generative coding action motor model is a modifiable working memory that allows the model to store a finite quantity of projected latent state vectors

into a set of self-recurrent memory vector slots.

This particular working memory module, which we call the self-recurrent slot buffer, serves as the glue that joins the models of Cog and Gen together.

So we talked, we heard about the slot buffer a long time ago, and here we are back to the slot buffer.

And then in the next figure, we're going to look at the action-driven task in the next figure.

Perfect.

So this is the motor model where the internal control portion produces signals.

Those are the CINT that manipulate working memory and the external control portion transmits signals.

So this is the CEXT down at the bottom that affects the environment.

And so both the internal and external control share the same working memory, which is interesting.

So explicit declarative memory, episodic memory.

So CoggenGen implements a replay mechanism in the form of an episodic memory constructed with Minerva.

Information is transferred to memory through an intermediate working buffer where pieces of the transition are progressively stored as they are encountered through the agent environment interaction process.

We do not update the motor action and dynamics models online, but instead update their parameters only when episodic memory is sampled.

Thus, the Cog and Gen computational process has two phases at each time step of the simulation, the processing step and the learning step.

And so we'll see figure two.

I think I have it next.

yeah perfect i reordered them because um they were mentioned in in different orders in the um in the paper so daniel showed this to us earlier um do you want to maybe talk a little bit about this figure while i go grab my computer plug yeah do you mind cool well


SPEAKER_00:
few ways to look at it one of them is we're dealing with this fundamental interface question blanket question boundary question which is how we begin in active inference and free energy principle about the agent in their boundedness interfacing with the environment

And then while we might see this in a partially observable Markov decision process generative model, here we basically see the formalism from the last several papers, this paper specifically, being represented

in the way that we've seen agent environment interface before observation coming in at time step t latent space here it's actually a more sophisticated latent space it's not just like one gaussian parameter that's getting updated there's this entire dialectic with the procedural and the working memory and the episodic all these cognitive features and then

there is motor or action control action is emitted within the time step and then there's the observation at the next time step so this is a cognitive architecture that brings together a lot of the features that we've been talking about plus these higher order structured purely cognitive aspects


SPEAKER_01:
Thank you so much.

My computer thinks as well.

So I'm curious here in this model how things get from the working memory into the episodic memory.

So I would like to see maybe more details about that as we go into the dot 2.

Yeah.

Okay, so in the mini grid world problem, they are investigating two tasks to evaluate the agents that they used.

So one is the door key task, the DK task, and the other is the memory task.

So everybody's in a grid.

It's a partially observable view of the agent's environment.

And there's a three channel object.

Yeah, so the agent's environment is a three channel object, a tensor, created by mapping each visible grid cell to three integer values.

So everything, each tile contains either nothing, one object,

and a discrete object type so ultimately each tile is encoded to an object index so it's unseen empty wall and then a color index red green whatever and then a state index which is open closed or locked so those are the three um three yeah things three things three things in the environment okay next so

The agent is restricted to picking up one single object, a key, and may open a locked door if it carries a key that matches the door color.

So that's the door key problem.

The discrete action space for the agent is six actions.

Turn left, right, forward, pick up an object, or drop the object that it's currently carrying, or activate, such as opening a door or interacting with an object.

The reward signal provided by all tasks is sparse.

The agent is only given a positive extrinsic reward if it

extrinsic reward if it reaches a green goal tile and otherwise making all problems difficult from a reinforced reinforcement learning perspective because it's sparse and that's that's like again at the very beginning like i act i asked about the sparsity and what is the

I mean, there's some overarching level of complexity when there's a sparse set of rewards, a sparse set of problems, a sparse set of activated neurons, I mean, et cetera, et cetera, et cetera, right?

Sparse networks are hard to deal with mathematically and computationally, and I guess in terms of programming a system with sparse learning, it's beneficial.

It seems that way.

Anyway, okay, so each problem has a specific time step limit

limit allotted to allow the agent to complete the task.

So you do it within a certain time or you fail.


SPEAKER_00:
yeah very interesting there sparsity in architecture space time reward utility epistemic value all like we don't live in flatlands we live on the rough and variable Peaks and the very embodied constraints and limitations of how things can actually be connected to each other so that's wild


SPEAKER_01:
Cool, okay, so for the door key problem, agent must find the key, the position of the key changes inside of a locker room, and then pick up the key in order to unlock the door so it can leave the room.

Upon entering the second room, the agent must find the green tile in order to successfully exit, terminating the episode by reaching the goal state to receive a positive reward.

So they use a six by six grid.

The memory task is a different test.

It's posed as a memory test.

So in the memory test, the agent starts in a small room where it can see an object, a key or a ball, and they

and they modified the problem to ensure that the agent starts looking in the direction of the object.

That's interesting.

I wonder why, like, so that they don't have to, it does just save time.

Like, so you don't have to look right, look front, look left, look behind you.

I wonder why they modified the problem in that way.

Um,

So after perceiving the object, the agent has to turn around, exit the room, and go through a narrow hollow that ends in a split.

At the end of the split, the agent can either go up or down.

At the end of each of these splits is a different object, either a key or a ball.

To successfully complete the episode and receive a positive reward, the agent must remember the initial object that it saw and go to the split that contains the correct matching object.

so did you see a key or did you see a ball and then which way are you going to go at the end of the maze um and then they did a seven by seven grid for that one okay let's see what they found oh not yet so these are the models of comparison so they use their model and a standard dq network model distillation and that's the dqn

R&D, which is a curiosity model, and then a DQN that learns through account-based formulation of the Be Bold Exploration Framework, which we'll have to learn more about in the doc too.

So these are the models that they tested against.

And then this is what they found.

So

success rate in the memory task for the Coggen Gen was like off the hook, like blew the other ones out of the water, right?

Like 98 success rate for the memory task, which is pretty crazy.

And then the door key task was, I guess, like zero for the DQN and then 100%, 100%, 100% for the other ones.

Or maybe then the reverse.

So let's see.

From left to right, the door key task, the memory task,

the bottom row is success rate over the last 100 episodes the right is the episode length oh there you go the percentage in case of the maximum worst case episode length so that is um dqn having the worst right and then kagenjen wasn't the fastest so oh yeah you can see the episode yeah i think it's just a pure coincidence that this one's 100.0


SPEAKER_00:
But this is a hundred percent zero, like a hundred percent.

And this one's one hundred time steps point zero.

But either way, DQN does worse than all three, which can solve the problems.

But of the DK problem, but the Kagen Gen is twice as good on the Mem problem.

And then the episode lengths are pretty comparable.

And much shorter.

Except the DQN.

Yeah, exactly.

This one is just kind of a baseline.


SPEAKER_01:
The DQN is like, takes forever.

It takes forever and doesn't do very well in the memory task.

So yeah, there you go.

All right.

So that's a lot of time on the model and very little time on the results.

And then this is just some more of the results here.

So you can just see the average reward episode length smoothed for the door key problem.

and the memory task, which is the right.

So the door key is the left.

The memory task is the right.

And here, you can see the cog and gen on this bottom one.

Do you see this curve?

That is the curve depicts the mean and standard deviation over five trials.

And the left is the average reward, I think.

Oh, yeah, episodic reward.

So that is where we saw 98 for the cog and gen.

And that's here, too, for the door key task.

memory task.

Okay, results and discussion.

Based on the results, we find that CoggenGen is able to learn the grid world task.

The performance is comparable to or on par with powerful deep intrinsic curiosity reinforcement learning methods that have access to problem specific global information, and CoggenGen can successfully solve and outperform all baselines on the memory task.

So bravo.

We have arrived.


SPEAKER_00:
Wow, Blue.

What a ride.


SPEAKER_01:
Yeah.

What do you think about the four paper model?


SPEAKER_00:
Well, I can only return to what Alex wrote in an email, which is, it makes sense.

to have done it how it was done.

I think it'd be easy to look at a cognitive architecture like this, which integrates the analytical components and also these more psychological or cognitive features like epistatic and procedural memory.

It's easy to look at that in a way that's disconnected from the actual computational implementation.

because connecting up the formalism feels as easy as just writing an equation or just drawing something.

And then to say, well, maybe the working memory sends a message to this other buffer.

It's like, you can just draw that out and nothing's gonna stop you.

But then the first three papers kept it connected very closely to the actual locally implementable computation.

so then we were able to get to this point of um looking at such an interesting cognitive model that's also performant and be able to talk about where it could be applied or what would be different or limitations or like these different features of this but

it it feels very different to look at this after the first three which is the research direction so I think it was a great selection and I'm just looking forward to talking with Alex what about you


SPEAKER_01:
Yeah, I'm looking forward to like diving deeper into the models and kind of understanding more about like getting under the hood, maybe, so to speak.

So what makes them tick in a little bit more detail.

So should be good.


SPEAKER_00:
Cool.

Any other comments?

Awesome.

Well, thank you for preparing and having fun working on this .0 leading up to now.

And yeah, in the coming weeks, we will proceed with 56.1 and .2.

So thank you, everybody.

Until next time.
[
  {
    "start": 5.538,
    "end": 6.198,
    "text": " Hello, everyone.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 6.438,
    "end": 9.459,
    "text": "Welcome to Active Inference live stream number 6.1.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 9.7,
    "end": 12.101,
    "text": "It is October 13th, 2020.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 13.381,
    "end": 16.962,
    "text": "Whether you're a first-time listener or not, welcome to Team Calm.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 17.643,
    "end": 23.225,
    "text": "Team Calm is an experiment in online team communication and learning related to Active Inference.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 24.093,
    "end": 32.778,
    "text": " You can find us on Twitter at inferenceactive, at our Gmail address, our Keybase team that's public, or our YouTube channel.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 33.619,
    "end": 39.403,
    "text": "This is a recorded and an archived live stream, so please provide us feedback so that we can improve our work.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 40.183,
    "end": 52.451,
    "text": "Also, all backgrounds and perspectives are welcome here, and in service of that, remember your video etiquette, mute if there's sound in the background, and raise your hand so that we'll be able to hear from everyone.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 53.68,
    "end": 59.084,
    "text": " Here we are in Act-Imp stream 6.1, and today's stream is gonna go like this.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 59.524,
    "end": 67.729,
    "text": "We're gonna start out with a warmup section where we introduce some of ourselves, especially our newer participants, and just have a quick check-in.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 68.735,
    "end": 70.598,
    "text": " Then we'll turn to the discussion of the paper.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 71.519,
    "end": 82.857,
    "text": "Today we'll try to cover the goal of the paper, the roadmap, how they get from A to Z, go through the abstract, which is how the authors represent their work in the most distilled form, and then look",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 83.979,
    "end": 87.921,
    "text": " at some of the figures, see where they're going to be going with the figures, what they're going to be showing.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 88.261,
    "end": 97.886,
    "text": "And then next week in 6.2, we're going to have a lot more time for further discussion on this paper and also to dive into some of the technical details of the figures.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 98.306,
    "end": 100.607,
    "text": "So please save and submit your questions.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 101.977,
    "end": 104.88,
    "text": " All right, here we are in the intros and check-ins.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 105.301,
    "end": 109.786,
    "text": "For the introduction section, please introduce yourself and your location.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 109.886,
    "end": 115.473,
    "text": "Just say a quick hello, anything else you'd like to add, and then feel free to pass it to someone else.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 115.973,
    "end": 122.621,
    "text": "So I'm Daniel, I'm in Davis, California, and I'll pass it to one of our first-time participants, Matthias.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 129.782,
    "end": 131.523,
    "text": " It's actually Mathis Pink.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 133.063,
    "end": 140.965,
    "text": "I'm a master's student in Germany at the University of Osnabr\u00fcck, which is in Lower Saxony.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 141.606,
    "end": 144.566,
    "text": "So it's afternoon where I am.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 145.227,
    "end": 147.167,
    "text": "I guess it's in the morning where you are.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 149.968,
    "end": 152.349,
    "text": "Yeah, I'm excited about participating.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 153.489,
    "end": 154.209,
    "text": "Thanks for having me.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 156.03,
    "end": 156.23,
    "text": "Great.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 156.25,
    "end": 156.55,
    "text": "Let's start.",
    "speaker": "SPEAKER_06"
  },
  {
    "start": 158.766,
    "end": 185.713,
    "text": " don't know is there another person who's new here uh lee alex maybe let's go to lee our other new participant yeah hi um yeah i'm lee i'm based in london but i'm actually studying at the university of york studying embodied cognition and um yeah really looking forward to this um i will pass on to alejandra",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 189.58,
    "end": 190.96,
    "text": " Hello, everyone.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 193.221,
    "end": 196.521,
    "text": "I'm again at Mexico, kind of tired.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 199.362,
    "end": 207.363,
    "text": "Last week was very hard, but yeah, nice to be here again with you all.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 208.343,
    "end": 210.944,
    "text": "And I'll pass it to Alex.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 212.104,
    "end": 216.825,
    "text": "Hello, everybody.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 217.332,
    "end": 217.952,
    "text": " Hi, I'm Alex.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 218.512,
    "end": 224.994,
    "text": "I'm an engineer, basically, and now also a researcher.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 226.034,
    "end": 232.696,
    "text": "And I'm stated in Moscow, Russia, and related to this system management school.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 234.056,
    "end": 235.976,
    "text": "So I pass it to Clip.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 236.997,
    "end": 239.237,
    "text": "Hi, I'm John Clip.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 239.417,
    "end": 247.059,
    "text": "I'm in Cambridge, Mass, with the MIT Media Lab, and actually doing this in northern New Hampshire at my farm.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 248.816,
    "end": 252.877,
    "text": " And I'm very pleased to be part of this and here to learn.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 256.179,
    "end": 257.319,
    "text": "Let's go to Alex Kiefer.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 257.339,
    "end": 260.381,
    "text": "And I'll pass it off to Maxwell.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 262.502,
    "end": 262.782,
    "text": "Okay.",
    "speaker": "SPEAKER_07"
  },
  {
    "start": 264.643,
    "end": 265.143,
    "text": "Hi, everyone.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 265.423,
    "end": 266.644,
    "text": "So I was here a couple weeks ago.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 267.304,
    "end": 273.847,
    "text": "I'm a philosopher, I guess, and sometime computational modeler at Monash University.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 274.047,
    "end": 275.348,
    "text": "And right now I'm in New York City.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 277.481,
    "end": 282.884,
    "text": " And I've been involved in this debate that's explored in this paper for a while, so I thought I would join in.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 283.824,
    "end": 285.545,
    "text": "And I will pass it to, who's left?",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 286.705,
    "end": 287.045,
    "text": "Shannon?",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 289.687,
    "end": 291.047,
    "text": "Hi, I'm Shannon Crooks.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 291.668,
    "end": 298.571,
    "text": "I'm part of the sensory motor neuroscience lab at the University of California in Merced, but currently I'm in South Dakota.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 300.332,
    "end": 303.353,
    "text": "So I'll pass it to Sasha.",
    "speaker": "SPEAKER_08"
  },
  {
    "start": 307.315,
    "end": 307.836,
    "text": " Hi, everybody.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 308.416,
    "end": 309.157,
    "text": "I'm Sasha.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 309.337,
    "end": 318.746,
    "text": "I'm based out of Davis, California, and I'm affiliated with University of California, Davis, and also just very excited to learn and unpack some of these topics.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 319.747,
    "end": 327.215,
    "text": "It's been a really great past six or so streams, so I will pass it on to Stephen.",
    "speaker": "SPEAKER_04"
  },
  {
    "start": 330.306,
    "end": 330.666,
    "text": " Hello.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 330.706,
    "end": 336.429,
    "text": "Yeah, I've been involved in this for the last few sessions as well, finding it very, very helpful and very interesting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 336.509,
    "end": 355.079,
    "text": "I'm doing a practice-based PhD into some processes to explore social topographies at Canterbury Christ Church University at the Solomon School of Applied Psychology with the help of the Professional Development Institute.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 356.159,
    "end": 382.746,
    "text": " um and i'll pass that to maxwell i suppose isn't it i believe i'm the last one um yeah so uh i'm maxwell ramstad uh i'm based in montreal where i'm talking to you from uh at mcgill university and i'm also the first author on the paper that we'll be discussing today so i'm quite excited to discuss it with you uh",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 384.335,
    "end": 386.656,
    "text": " Thanks, Alex Kiefer, by the way, for showing up.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 386.816,
    "end": 399.1,
    "text": "Alex and I have had a sustained series of discussions around these papers that have led to follow up papers and I think to a really robust and fun friendship through these discussions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 399.16,
    "end": 403.121,
    "text": "So I'm looking forward to discussing all these issues with you.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 403.261,
    "end": 403.702,
    "text": "Thanks.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 404.742,
    "end": 407.703,
    "text": "Thanks for the continued following of our work.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 409.58,
    "end": 410.26,
    "text": " Awesome.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 410.28,
    "end": 414.201,
    "text": "We bring the ideas together sometimes through the people and the friendships.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 414.582,
    "end": 419.443,
    "text": "And I guess I'm not going to pass it to anyone since everyone else has spoken, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 419.764,
    "end": 419.924,
    "text": "Yep.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 420.144,
    "end": 421.944,
    "text": "I think that is everyone.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 422.165,
    "end": 423.005,
    "text": "So cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 423.545,
    "end": 425.626,
    "text": "Let's go to our warm-up questions.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 425.886,
    "end": 432.529,
    "text": "And anyone who likes to speak can feel free to raise their hand or jump in if there's no one speaking yet.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 433.109,
    "end": 437.411,
    "text": "So the first question is, what drew you to this paper or topic?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 438.511,
    "end": 440.652,
    "text": " And while people are raising their hand, I'll start.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 440.892,
    "end": 452.695,
    "text": "I think what was exciting about this paper was about just combining two different schools of thought and bringing them together in a constructive way that was really super additive rather than just choosing sides.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 453.195,
    "end": 454.255,
    "text": "What about you, Maxwell?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 456.416,
    "end": 464.658,
    "text": "Well, the motivation for writing the paper in part was that I don't think that at least at the time, I didn't think that",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 465.442,
    "end": 472.144,
    "text": " what the generative model business under the free energy principle is all about, really.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 472.164,
    "end": 478.565,
    "text": "I don't think that was well understood at the time that we wrote this.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 478.645,
    "end": 481.726,
    "text": "So I alluded to this, I think, a few weeks ago.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 482.466,
    "end": 492.368,
    "text": "This paper, the Answering Schrodinger's Question paper in Physics of Life Reviews and the paper that we discussed the last few weeks, Multiscale Integration, were all originally the same paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 493.008,
    "end": 494.069,
    "text": " It was all one big thing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 494.509,
    "end": 499.571,
    "text": "And we ended up splitting it into different papers to address different things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 500.311,
    "end": 508.575,
    "text": "So, you know, the multi-scale stuff that we'd been discussing to address like the scale-free and multi-level formulation of the FEP.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 508.995,
    "end": 512.417,
    "text": "And the aim here was really to connect",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 513.749,
    "end": 519.772,
    "text": " the FEP to other pragmatist approaches that center on action.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 521.452,
    "end": 535.138,
    "text": "And also to clarify the nature of the generative models that are at play here, which aren't just kind of brain bound statistical models, but turn out to be something like the phenotype of the organism.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 537.979,
    "end": 538.399,
    "text": "Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 538.539,
    "end": 540.44,
    "text": "Anyone else have thoughts on that?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 544.211,
    "end": 545.011,
    "text": " Alex, Kiefer?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 546.912,
    "end": 547.412,
    "text": "Yeah, thanks.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 548.493,
    "end": 559.276,
    "text": "So I guess, well, I was drawn into the debate because some of my work with Jacob Hovey was among the sort of target, the critical target of the article.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 559.616,
    "end": 567.119,
    "text": "Although Maxwell and company were very nice to us and they didn't say we were wrong, they just said this doesn't exactly generalize to the free energy principle.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 568.019,
    "end": 569.5,
    "text": " So that's how I got drawn into this.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 569.54,
    "end": 579.906,
    "text": "And I continue to be interested in it because I think it, I mean, I think we've converged on more of an agreement than maybe existed at this time this paper came out, but- Oh yeah, 100%.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 580.186,
    "end": 586.489,
    "text": "I think we've basically converged to one coherent story at this point, but this is two years in the making, so.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 586.99,
    "end": 593.133,
    "text": "Yeah, but I mean, the issues raised here are still interesting to think about, you know, and I continue to learn more by thinking about them, so.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 594.488,
    "end": 594.748,
    "text": " Awesome.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 595.449,
    "end": 603.336,
    "text": "All right, the second warmup question is gonna be, what would be something that you'd like to have resolved by the end of today's discussion?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 603.917,
    "end": 608.962,
    "text": "So that could be a specific question about how to apply something to a system or- Lee would like to speak.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 609.322,
    "end": 610.203,
    "text": "Yeah, Lee, go ahead.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 616.589,
    "end": 616.989,
    "text": "Go ahead, Lee.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 618.503,
    "end": 620.385,
    "text": " Sorry, I was just turning my sound off.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 621.366,
    "end": 621.646,
    "text": "So.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 621.986,
    "end": 632.655,
    "text": "So, yeah, essentially, I guess I was drawn to this topic because my journey towards active inference has been via cognitive linguistics.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 632.755,
    "end": 639.301,
    "text": "So I started off looking into relationships between language and perception and particularly metaphor.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 639.941,
    "end": 650.224,
    "text": " So I think I've got quite a different understanding of what's meant by model, probably something more kind of phenomenological and maybe epistemic.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 651.384,
    "end": 656.405,
    "text": "And I'm really starting to understand that that's not really what is meant by active inference.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 656.445,
    "end": 666.668,
    "text": "So I've taken a good look through this paper and I'm hoping to build some kind of bridge from where I'm at to more of an understanding of what's implied by generative model in active inference.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 668.677,
    "end": 668.957,
    "text": " Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 669.077,
    "end": 672.081,
    "text": "Well, great, because that's one of the main topics of the paper.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 673.102,
    "end": 673.603,
    "text": "And Stephen?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 676.095,
    "end": 700.262,
    "text": " Actually, just bouncing off what was just said there, in some ways when you take metaphors, if you bring metaphors into embodiment and as being embodied in space and around us, actually then suddenly that whole metaphor work becomes actually almost very close to what might be a generative model in a funny way, even though it's different.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 701.721,
    "end": 721.506,
    "text": " There might be some way, I'm actually, so in some ways I'm kind of interested how this sort of becomes full circle and can come back to these kind of human ways of knowing, even though we've accessing it through maybe kind of abstracted models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 724.608,
    "end": 747.889,
    "text": " interesting and i think we'll return to some of these ideas if anyone else wants to chime in on what they'd like to have resolved but very interesting to hear about metaphor as cognition and how we can humanize our understanding of some of these technical or conceptual issues all right well if there are no hands remaining i'll move on to the next slide so",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 748.59,
    "end": 769.679,
    "text": " today we're going to be talking about a tale of two densities active inference is an active inference which is a article in adaptive behavior in 2020 by maxwell ramstead kirchoff and friston and in this paper they lay out their goal really clearly which i always love to see in a paper they write at the very beginning",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 770.399,
    "end": 789.296,
    "text": " The aim of this article is to clarify how best to interpret some of the central constructs that underwrite the free energy principle, or FEP, and its corollary, active inference, in theoretical neuroscience and biology, namely the role that generative models and recognition densities play in this theory, aiming to unify life and mind.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 790.293,
    "end": 791.954,
    "text": " So what are the two densities?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 792.314,
    "end": 795.915,
    "text": "They're going to be the generative models and the recognition densities.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 796.015,
    "end": 802.597,
    "text": "And we're going to learn more about them and hear about how they're related and discuss different perspectives on how the densities are linked.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 803.477,
    "end": 807.958,
    "text": "And specifically, the question is, what is the tale of these two densities?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 808.198,
    "end": 819.542,
    "text": "It's alluded to in the title, and it's awesome that we have Maxwell and Alex and so many other voices here to make that synthesis and that tale that we're all telling together realized.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 820.242,
    "end": 833.294,
    "text": " So in the Act-Inf stream 6.0, I provided a little bit of context just from my perspective on some of these issues if people want to learn more about some of the background ideas.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 833.434,
    "end": 836.577,
    "text": "But for now, we're going to just jump into the abstract.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 836.757,
    "end": 842.643,
    "text": "And at any point, people can just raise their hand, and I'll just pause right there, and we'll take a comment or a thought.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 844.184,
    "end": 853.372,
    "text": " In the abstract, they begin by rehearsing what I had just read, that they're looking to clarify how to best interpret some of the constructs underwriting the free energy principle.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 854.392,
    "end": 858.696,
    "text": "And those two constructs are the generative models and the variational densities.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 859.136,
    "end": 862.419,
    "text": "So those are the two densities and the tail is going to link them.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 863.735,
    "end": 885.278,
    "text": " We argue that these constructs, generative models and variational densities, have been systematically misrepresented in the literature because of the conflation between the FEP and active inference on one hand, and distinct, albeit closely related Bayesian formulations centered on the brain, variously known as predictive processing, predictive coding, or the prediction error minimization framework.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 887.679,
    "end": 898.551,
    "text": " More specifically, we examine two contrasting interpretations of these active inference type models, a structural representationalist interpretation and an inactive interpretation.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 898.992,
    "end": 901.835,
    "text": "So we're setting up the two sort of sub stories.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 901.915,
    "end": 908.702,
    "text": "These are the tension between these two perspectives that we're going to be looking to resolve under the FEP through active inference.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 909.758,
    "end": 919.748,
    "text": " We argue that the structural representationalist interpretation of generative and recognition models does not do justice to the role that these constructs play in active inference under the FEP.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 921.055,
    "end": 926.4,
    "text": " We propose an inactive interpretation of active inference, what might be called an active inference.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 927.281,
    "end": 934.067,
    "text": "In active inference under the FEP, the generative and recognition models are best cast as realizing inference and control.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 934.648,
    "end": 942.755,
    "text": "The self-organizing belief guided selection of action policies and do not have the properties ascribed by structural representationalists.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 943.416,
    "end": 955.572,
    "text": " So for this next slide, I'll really appreciate anyone's perspective or linking it back to things they've seen before, because I just put it up there as a visual and just a starting place.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 956.132,
    "end": 960.978,
    "text": "On the left side here, we have the Bayesian Structural Representationalist.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 961.499,
    "end": 967.103,
    "text": " perspective that we're just trying to highlight the features that are going to be most simple to carry forward.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 967.743,
    "end": 982.253,
    "text": "And the Bayesian structural representationalist story is about how data and another type of data, which are often called hyperparameters, are linked through a recognition model that takes data like sensory data and recognizes it.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 982.773,
    "end": 988.177,
    "text": "And then going the other direction, you have the hyperparameters that are generating sensory data.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 988.437,
    "end": 991.159,
    "text": "And we can also talk about why it's important to have this generative step.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 991.759,
    "end": 1004.535,
    "text": " And the outcome of this Bayesian computationalist scheme is that there's a statistical convergence of a multi-level model that represents structures of the world through something like expectation maximization or EM models.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1005.316,
    "end": 1008.159,
    "text": "And we can contrast that with the inactive paradigm.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1008.82,
    "end": 1015.264,
    "text": " And the inactive paradigm is about how agents and the world are related through perception and action.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1015.865,
    "end": 1024.791,
    "text": "And the outcome of the inactivist perspective is an embodied ecological action sequence, really, from an embedded agent who is enacting behavior.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1025.031,
    "end": 1030.174,
    "text": "And so this is always the school of thought where we see all the E's encultured and embedded in all these things.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1030.775,
    "end": 1036.339,
    "text": "And they're seemingly, at least at the first pass, up to two quite different...",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1037.29,
    "end": 1057.67,
    "text": " explanatory uh outcomes they seem to be talking about somewhat different aspects about the world and they definitely link them through different ways so i'm just curious on maxwell or anyone else like what led to these two models being the two cities the two densities that were chosen how does one",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1058.311,
    "end": 1059.211,
    "text": " come down to just two?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1059.311,
    "end": 1060.852,
    "text": "Why is there not one or three cities?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1060.932,
    "end": 1071.455,
    "text": "And then how did the Bayesian structural representationalist and the inactive viewpoint rise up as like the two kind of tier one theories that we wanted to find a synthesis between?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1072.556,
    "end": 1086.2,
    "text": "Well, so when I got into this literature, especially from the the vantage point of philosophy, what I noticed was that very little of it was technically rigorous in the sense that",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1086.823,
    "end": 1093.625,
    "text": " You know, a lot of it was telling like a story about how, you know, the brain roughly performs Bayesian inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1094.025,
    "end": 1102.808,
    "text": "And then, you know, kind of saying, well, there's like a family of different theories that do this in various ways and grouping the free energy principle under that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1103.769,
    "end": 1104.149,
    "text": "I say, yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1105.911,
    "end": 1108.633,
    "text": " there was like a lack of technical rigor.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1108.653,
    "end": 1114.417,
    "text": "I want to emphasize that Alex's papers with Jacob are probably the exception to that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1115.978,
    "end": 1131.23,
    "text": "When I consulted Alex's papers in 2018, I thought, well, here's some wonderful work that is really taking the time to drill down on the formalisms as they're used to study the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1132.159,
    "end": 1132.879,
    "text": " I thought that was great.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1133.9,
    "end": 1139.183,
    "text": "But I spent a lot of time really drilling into the formalism of the free energy principle per se.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1139.223,
    "end": 1149.849,
    "text": "And one of the things that I was sort of surprised to find out as I was learning the formalism was that although everyone is talking about the generative model, there are really two models at play.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1151.434,
    "end": 1155.657,
    "text": " So those are the generative and the recognition models or densities equivalently.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1157.078,
    "end": 1158.259,
    "text": "So first of all, we say model.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1158.299,
    "end": 1167.465,
    "text": "By model, we just mean a probability density, so a probability distribution over a bunch of variables that are of interest to us.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1169.686,
    "end": 1179.813,
    "text": "And so yeah, the two models in question under the free energy principle function slightly differently than they do in more traditional brain-based.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1181.625,
    "end": 1193.908,
    "text": " to step back a bit in machine learning and in statistics, a recognition model basically tells you the probability of some state given a bunch of other things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1194.248,
    "end": 1196.329,
    "text": "It's not a joint probability distribution.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1197.409,
    "end": 1202.991,
    "text": "And it's used essentially to recognize what's causing your data.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1203.371,
    "end": 1207.592,
    "text": "So you're using it basically to invert your mapping.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1208.547,
    "end": 1215.719,
    "text": " I mean, Alex, also, if I'm saying anything inaccurate here, just please jump in and let me know.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1215.739,
    "end": 1220.608,
    "text": "But yeah, so there's basically in traditional kind of,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1221.616,
    "end": 1228.222,
    "text": " You know, Bayesian brain machine learning architectures, the generative and the recognition models are basically just the inverse of one another.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1228.342,
    "end": 1236.249,
    "text": "So the kind of top, the kind of bottom up pass is a recognition pass.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1236.309,
    "end": 1242.475,
    "text": "It's a recognition model in the sense that it's starting from the data and then kind of passing through the network.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1242.495,
    "end": 1244.877,
    "text": "You're able to infer what must have caused your data.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1245.897,
    "end": 1256.385,
    "text": " And the generative model is the inverse path where you're starting from your beliefs about states and you can generate fictive data, you know, based on this model.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1258.086,
    "end": 1261.529,
    "text": "At least in Alex's papers, the way that this, well, the...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1262.777,
    "end": 1268.822,
    "text": " This is how it was described as applying to the free energy formulation as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1269.222,
    "end": 1280.391,
    "text": "And my point in this paper was that, well, this is a very technically rigorous and accurate description of what's going on in the Bayesian brain, but these constructs have a slightly different meaning under active inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1281.131,
    "end": 1283.493,
    "text": "And to bring it back, I'm almost done.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1283.833,
    "end": 1285.034,
    "text": "Just give me one more minute.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1285.915,
    "end": 1292.2,
    "text": "To bring it back to these schemas, basically, so the recognition density",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1292.97,
    "end": 1295.251,
    "text": " is sort of like your best guess right now.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1295.371,
    "end": 1298.292,
    "text": "You can think about it sort of like as your posterior and your prior.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1298.792,
    "end": 1303.933,
    "text": "So your recognition density is a density defined over all of your states and your parameters.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1304.994,
    "end": 1313.836,
    "text": "It basically tells you what do I believe is the most probable value of these states and parameters now, you know, given my prior beliefs and my evidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1314.657,
    "end": 1318.878,
    "text": "Your generative model, to the contrary, is the point of reference",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1320.637,
    "end": 1322.938,
    "text": " for the generation of free energy gradients.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1323.278,
    "end": 1325.699,
    "text": "So it's not your posterior, it's your prior.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1325.879,
    "end": 1339.222,
    "text": "It's sort of like it harnesses all of the priors, especially the priors about your preferred data distributions, relative to which the free energy and therefore the dynamics are defined.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1339.262,
    "end": 1344.364,
    "text": "So you write inference and control, I think you discussed in 6.0, Dan,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1346.385,
    "end": 1353.469,
    "text": " Yeah, so the recognition model is responsible for inference and the generative model is responsible for control, you might say.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1354.29,
    "end": 1355.39,
    "text": "So I'll stop there.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1356.151,
    "end": 1356.411,
    "text": "Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1356.671,
    "end": 1359.393,
    "text": "Alex, Kiefer, and then we'll go to anyone else with a raised hand.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1360.509,
    "end": 1361.129,
    "text": " Yeah, thanks.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1361.149,
    "end": 1362.75,
    "text": "So I stopped myself from jumping in.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1362.77,
    "end": 1364.031,
    "text": "I mean, that was that was a good summary.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1364.391,
    "end": 1376.456,
    "text": "The only thing the point at which I wanted to jump in was to say, well, so the recognition model isn't as construed in these Bayesian brain theories is an approximate inversion of the generative model.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1376.536,
    "end": 1377.837,
    "text": "So my my",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1378.897,
    "end": 1388.466,
    "text": " If I have any complaint about this paper, it's that I think that there's a closer sort of conceptual connection between the generative and recognition densities than maybe the paper suggests.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1390.068,
    "end": 1392.771,
    "text": "And I don't think you can cleanly separate these things.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1394.452,
    "end": 1413.088,
    "text": " well anyway i don't want to launch into this yet what i wanted to do first was just address the the question was sort of how did these two visions sort of arise and my sense is that what carl friston did he did a lot of great stuff but the main thing he did that distinguished his approach from the existing stuff in machine learning a lot of which was based on free energy minimization",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1413.969,
    "end": 1436.829,
    "text": " uh which i think he came to around the same time as people like jeff hinton but anyway he you know he added action into the picture and he pointed out that you can act so as to reduce the surprisal or the free energy cosplay or sensory states uh instead of just revising your gender just generative distribution um anyway so i think i'll hold off on on arguing for the moment",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1437.759,
    "end": 1438.339,
    "text": " Awesome.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1438.439,
    "end": 1439.8,
    "text": "I just wanted to say, I agree with you now.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1439.82,
    "end": 1445.342,
    "text": "Uh, the, the two things do, do not really come together, uh, come apart.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1445.362,
    "end": 1445.642,
    "text": "Sorry.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1445.863,
    "end": 1452.786,
    "text": "Um, I mean, it's, it's really just an implementation of variational inference from that point of view and variational inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1452.846,
    "end": 1458.108,
    "text": "And this is why it's a tale of two densities is that to do this variational inference thing, you need both.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1458.7,
    "end": 1462.363,
    "text": " You need a kind of point of reference that's going to give you the free energy gradients.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1462.783,
    "end": 1468.467,
    "text": "And then you need a sort of what is my best guess as I am performing gradient descent on my free energy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1469.127,
    "end": 1472.47,
    "text": "And yeah, I don't want to say that the two come apart.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1473.29,
    "end": 1485.139,
    "text": "And indeed, I think in a newer paper that I've written on this, I basically just straight up say that you were correct initially with respect to the recognition density.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1485.199,
    "end": 1488.381,
    "text": "So the recognition density, it's fair to say that it's a representation",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1488.942,
    "end": 1498.084,
    "text": " in the structural representationalist sense that you've been articulating in the series of really awesome papers, which you should all read, by the way, I think.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1500.264,
    "end": 1506.485,
    "text": "Yeah, the one point of disagreement that I think we've clarified now is the status of these generative models.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1507.346,
    "end": 1510.546,
    "text": "And yeah, I'll stop there too.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1511.026,
    "end": 1511.266,
    "text": "Nice.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1511.366,
    "end": 1515.127,
    "text": "We'll go to Stephen and then anyone else with a raised hand before the roadmap.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1517.608,
    "end": 1545.561,
    "text": " Now, one question I've got is in terms of if these kind of hyper parameters that they use in machine learning are using a free energy, is it that they're just minimizing the kind of energy expenditure and the entropy internally and they're not using Shannon entropy, either not looking at how entropy is inferred or transmitted",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1547.512,
    "end": 1551.995,
    "text": " sort of in a second order process from interacting in the world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1552.335,
    "end": 1569.727,
    "text": "They're just minimizing it within the kind of data that's being kind of accrued, which kind of in some ways seems to happen easily when you look at like vision data, but may not be so easy to pass when you sort of look at the whole body.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1570.228,
    "end": 1574.811,
    "text": "But that question of entropy, is it that they use entropy in a different way and they're not really using",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1575.856,
    "end": 1592.706,
    "text": " entropy in the kind of external entropy of interaction, but kind of just the entropy within the calculations in terms of... There are two kinds of entropy at play in general, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1593.967,
    "end": 1601.291,
    "text": "So entropy in the thermodynamic sense is a measure of how many macro states are compatible with",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1601.832,
    "end": 1604.933,
    "text": " a given value of a macro state, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1604.973,
    "end": 1607.773,
    "text": "So I don't know, your temperature is 36 degrees Celsius.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1608.113,
    "end": 1611.434,
    "text": "How many different macro state configurations are compatible with that?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1613.554,
    "end": 1624.996,
    "text": "So that form of entropy is one special case of the broader kind of entropy, which is more or less a measure of how flat your probability distribution is over your states.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1625.376,
    "end": 1628.717,
    "text": "So if you have a perfectly flat distribution, your entropy is optimal.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1629.617,
    "end": 1629.937,
    "text": "And so",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1632.338,
    "end": 1635.219,
    "text": " The entropy that we're concerned with is really the second type.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1635.4,
    "end": 1637.601,
    "text": "It's the information theoretic entropy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1639.042,
    "end": 1647.467,
    "text": "But it's transpired over the last few years that variational free energy is also a thermodynamic free energy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1647.487,
    "end": 1649.628,
    "text": "You just have to multiply by Boltzmann's constant.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1651.377,
    "end": 1654.958,
    "text": " Essentially, yeah, it's always information theoretic measures.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1655.458,
    "end": 1661.459,
    "text": "But when this is realized in actual physical systems, it is also a thermodynamic free energy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1661.819,
    "end": 1667.3,
    "text": "But the variation of free energy, it sounds kind of spooky and esoteric, but it's really simple, really.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1667.32,
    "end": 1671.881,
    "text": "It's that you have a preferred data distribution and you have an actual data distribution.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1671.921,
    "end": 1674.941,
    "text": "And the free energy is just a way to quantify the difference between the two.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1674.961,
    "end": 1676.161,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1677.281,
    "end": 1677.582,
    "text": "Awesome.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1677.742,
    "end": 1680.202,
    "text": "We'll go to Alex and then back to Stephen.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1683.284,
    "end": 1691.968,
    "text": " Yeah, I was just going to briefly say in the earlier machine learning literature, the free energy definitely wasn't supposed to be anything thermodynamic.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1692.609,
    "end": 1700.913,
    "text": "Maybe that was an open possibility, but it was just a measure, as you were saying, I think, Stephen, between two internal...",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1701.733,
    "end": 1703.434,
    "text": " internally determined distributions.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1703.494,
    "end": 1707.756,
    "text": "It was the top-down generative posterior versus the approximate posterior.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1708.376,
    "end": 1715.659,
    "text": "And any connections to thermodynamics are really cool, but I think that's an additional, very substantive question slash thesis.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 1716.72,
    "end": 1718.921,
    "text": "Stephen, and then anyone else who raised their hand.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1721.442,
    "end": 1726.164,
    "text": "Yeah, I think this is quite a useful distinction, because I think that's where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1727.327,
    "end": 1739.36,
    "text": " a lot of the mixing gets sort of caught up at the moment because a lot of stuff has been referred to in like the last 15 years around Bayesian optimization and Bayesian stuff.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1739.9,
    "end": 1745.986,
    "text": "And then where it's all pretty much as if it's in contained in the data in the brain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1746.267,
    "end": 1747.047,
    "text": "And then now,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1748.939,
    "end": 1751.282,
    "text": " it's like a reconceptualizing of that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1751.302,
    "end": 1757.412,
    "text": "So I think it is a big challenge for people to like forget about what they've learned before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1759.102,
    "end": 1786.328,
    "text": " really agreed with that steven and um that's really what this conversation and synthesis is about is about bringing those qualitative often insights from inactivism and saying well wait a minute perception isn't simply the reverse of action you know there's not photons coming out of your eye so what is different between perception and action but also recognizing that the world and the agents have this sort of symmetry and that they are linked so",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1787.108,
    "end": 1798.784,
    "text": " Again, anyone can raise their hand, but we're going to turn to the roadmap and ask how the authors set up the discussion between the Bayesian and the inactivist approach, and then ultimately converge towards somewhat of a synthesis.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1799.325,
    "end": 1802.109,
    "text": "And on the right side is just the title page of the Dickens.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1803.263,
    "end": 1824.552,
    "text": " work tale of two cities so first there's an introduction sequence and then they talk about statistical models as representations and specifically they talk about generative models and recognition models in bayesian cognitive science as well as generative models as structural representations so that's the bayesian structural and the representational components",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1825.711,
    "end": 1836.038,
    "text": " Then they talk about generative models and action policies, which is sort of opening the door to this potential inadequacy of the purely Bayesian approach, which is that it's not action-oriented.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1837.059,
    "end": 1840.221,
    "text": "In the third section, they discuss the active inference framework.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1840.821,
    "end": 1847.946,
    "text": "First, they start by talking about how phenotypes are conceptualized under active inference and about Markov blankets.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1848.706,
    "end": 1855.471,
    "text": "And there's a figure one, which we've seen before and we will see again, about how Markov blankets and active inference are linked.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1856.551,
    "end": 1865.993,
    "text": " They then discuss surprise, entropy, and variational free energy, which are all terms, perhaps not surprise, but the other two that we've brought up in this discussion.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1866.293,
    "end": 1870.994,
    "text": "And in the paper, they really go into detail a bit more about how these topics are linked.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1871.775,
    "end": 1878.076,
    "text": "There's then a discussion about how active inference specifically links the variational free energy with the inferential models.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1879.435,
    "end": 1887.785,
    "text": " In Section 4, A Tale of Two Densities, they talk about how the generative model and the recognition density are to be conceptualized under the FEP.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1888.505,
    "end": 1893.471,
    "text": "And specifically, they talk about the generative model and the generative process in active inference.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1894.332,
    "end": 1899.438,
    "text": "In Figure 2, they depict a generative model and represent it as a Bayesian network.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1900.507,
    "end": 1919.805,
    "text": " figure three they talk about the same generative model in active inference represented as a fourney factor graph and the similarities and differences between figure two and three we're probably gonna go into next week in 6.2 there's then a section on variational inference and recognition dynamics under the fep",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1920.936,
    "end": 1929.124,
    "text": " In the fifth section, they turn to purely inactive inference, and they say, first, the claim that the generative models are control systems.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1929.564,
    "end": 1945.22,
    "text": "And so this sort of borrows on and builds on the generative model idea, which is a bit of a computationalist perspective, as well as this control systems perspective, which implicitly means action orientation, because control of action, not just control of theoretical parameters.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1946.613,
    "end": 1949.295,
    "text": " They then take the fighting words section.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1949.875,
    "end": 1952.317,
    "text": "Generative models are not structural representations.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1952.757,
    "end": 1963.143,
    "text": "I always like it when we can get down to specific claims and negatable statements and just really being specific because sometimes these abstractions get pretty far out.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1964.124,
    "end": 1971.829,
    "text": "There's then the final figure four, which depicts the action perception cycle in active inference as a generative modal and a process.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1972.964,
    "end": 1981.191,
    "text": " And last, they conclude with some remarks and the remarks also conclude with towards multidisciplinary research heuristics for cognitive science.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1981.711,
    "end": 1986.975,
    "text": "And I know we have a lot of awesome cognitive scientists and other perspectives in this room.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1987.155,
    "end": 1997.604,
    "text": "And so this is sort of our springboard where we can take the ideas that we're talking about here and ask, well, how does this impact how we're going to do the cognitive science research?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 1998.004,
    "end": 2018.383,
    "text": " what kind of explanations what kind of predictions what kind of experiments are we going to do differently how will our research paradigm be different if we take this approach that is synthesizing the inactive in the bayesian approach so especially cognitive scientists would be awesome to hear any thoughts",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2019.284,
    "end": 2035.875,
    "text": " And while people are sort of collecting their thoughts about anything on the roadmap or prior in the last 20 minutes here, we'll have good time to go through the figures and ask about how they relate to this big topic.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2036.036,
    "end": 2037.236,
    "text": "How are the two cities linked?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2037.597,
    "end": 2043.801,
    "text": "What is the highway that connects the two cities or what is the broader collaborative network that links these two?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2044.541,
    "end": 2046.683,
    "text": "Any thoughts here on the roadmap?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2049.617,
    "end": 2063.468,
    "text": " you you definitely laid out the reasoning in the paper very well so thank you good and uh these papers are uh straightforward to lay out because they really are almost a uh additive",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2064.577,
    "end": 2065.818,
    "text": " way of laying out the topics.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2066.038,
    "end": 2070.282,
    "text": "First, you lay out the strongest version of the other approach.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2070.463,
    "end": 2080.732,
    "text": "And actually, it's the professionality and the clarity and not misrepresenting another viewpoint just to throw it under the bus or to make a straw man argument.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2080.852,
    "end": 2082.834,
    "text": "It's laying out the strongest form",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2084.022,
    "end": 2092.605,
    "text": " of the, um, prior literature is what allows us to build another level of strength on top of the literature, rather than just cutting out the pillars from underneath us.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2092.845,
    "end": 2100.067,
    "text": "So I always have to say, you know, we started off with this, a much stronger position, uh, than we ended up with.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2100.488,
    "end": 2107.65,
    "text": "And it's, it's by engaging seriously with the arguments that Alex and Jakob had prepared.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2108.907,
    "end": 2114.349,
    "text": " you know, with a few other people, Pawe\u0142 G\u0142adziewski and so on had been arguing for this kind of stuff.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2114.369,
    "end": 2117.43,
    "text": "But I mean, thank you for saying that about the professionalism.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2117.69,
    "end": 2120.651,
    "text": "I really think that there is something to that argument.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2120.731,
    "end": 2124.032,
    "text": "And the whole point is to keep the baby, you know, with",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2124.833,
    "end": 2126.254,
    "text": " while getting rid of the bathwater.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2126.274,
    "end": 2138.418,
    "text": "And the baby is this sort of idea that there is something like representations of the external world that carry semantic content and so on.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2138.538,
    "end": 2141.339,
    "text": "It's just that it's not the generative models.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2142.2,
    "end": 2143.82,
    "text": "It's the recognition densities.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2143.86,
    "end": 2147.041,
    "text": "That was sort of the tweak that we wanted to bring to the table.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2148.402,
    "end": 2148.722,
    "text": "Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2148.982,
    "end": 2151.123,
    "text": "Alex, and then anyone else who raises their hand.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2152.662,
    "end": 2181.22,
    "text": " yeah so um yeah like like maxwell says we've had a lot of interesting questions about this um i guess the point the point on which i remain not convinced yet is that i think i still think of the generative model even in the fep as a representation so we can we can maybe talk about that um the i think to me what this brings to the table and this is like transformed how i see this stuff is the importance of understanding the generative model as a control system",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2182.06,
    "end": 2196.438,
    "text": " um i just think you can do that in a way that also grants that it's a structural representation and that's what i've been that's still the framework in which i work um so i think there's another question that raised here about whether the generative model is sort of encoded",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2197.419,
    "end": 2203.183,
    "text": " And I do understand where the authors are coming from on that question.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2203.303,
    "end": 2209.447,
    "text": "And I think it's not so straightforward in the FEP as it would be in some earlier models like the Helmholtz machine.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2210.788,
    "end": 2211.348,
    "text": "So I don't know.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2211.388,
    "end": 2213.469,
    "text": "I'm curious to see if Maxwell and I still disagree.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2213.549,
    "end": 2220.334,
    "text": "I have the sense that I maybe still want to call the generative model a representation more than Maxwell does, but we can get to that.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2221.035,
    "end": 2233.082,
    "text": " Can I ask Alex and again, anyone can raise their hand and have a thought here, too, is what is the advantage of wanting something to be a representation or what is the alternative to something being structural?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2233.743,
    "end": 2233.963,
    "text": "Right.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2234.183,
    "end": 2239.306,
    "text": "So I mean, so I don't think in those terms in terms of the advantage of it, I just think, is it a representation or not?",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2239.826,
    "end": 2244.029,
    "text": "I think it is based on what our representation is and how this thing functions in this model.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2244.109,
    "end": 2249.352,
    "text": "So I know oftentimes people talk about, like, what's the explanatory value of talking in terms of representations?",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2251.175,
    "end": 2256.399,
    "text": " But I mean, to answer more directly, so what's the alternative to it being structural?",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2256.439,
    "end": 2257.439,
    "text": "That's another good question.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2259.521,
    "end": 2263.243,
    "text": "In my view, this structural representationalist paradigm really isn't anything new.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2263.323,
    "end": 2269.748,
    "text": "It's kind of just like the core notion of representation that's been at work in serious cognitive science since, like, Turing.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2270.328,
    "end": 2276.312,
    "text": "So if you go back to papers, like there's a paper from 1980 by Alan Newell",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2280.575,
    "end": 2292.786,
    "text": " sort of foundations of cognitive science and he describes how what makes a particular symbolic physical symbol system able to",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2295.327,
    "end": 2300.073,
    "text": " what gives it its universal computational capacity is its capacity to simulate other systems.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2300.453,
    "end": 2306.921,
    "text": "So I really think this notion of simulation is at the heart of what representation is supposed to be in cognitive science.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2307.241,
    "end": 2310.084,
    "text": "And that really just is structural representation.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2310.104,
    "end": 2312.147,
    "text": "And if you go back less, I won't go on forever.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2313.697,
    "end": 2320.999,
    "text": " Cummins's early work on structural representationalism, the S representation can also be read as simulation.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2322.339,
    "end": 2326.82,
    "text": "So it's not that I think that you need to call the generative model a structural representation.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2326.86,
    "end": 2331.961,
    "text": "I just think once you understand how it functions, I would say even under the FEP, that's what it is.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2332.902,
    "end": 2335.522,
    "text": "But there's still a question about how it's encoded or whether it's encoded.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2335.562,
    "end": 2336.542,
    "text": "I think that's a separate question.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2337.383,
    "end": 2337.703,
    "text": "Awesome.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2337.803,
    "end": 2339.163,
    "text": "Thanks for that response.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2339.263,
    "end": 2342.124,
    "text": "We'll go to Lee, and then anyone else that can raise their hand.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2342.884,
    "end": 2342.984,
    "text": "OK.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2344.329,
    "end": 2350.693,
    "text": " So, excuse me, I was just trying to make sure that I was understanding terms in the same way that they were being used.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2350.813,
    "end": 2355.936,
    "text": "But Alex, you just talked about the notion of representation as simulation.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2356.756,
    "end": 2366.002,
    "text": "And one of the ways that I've kind of arrived at active inference is via the work of Barsalou and perceptual symbol theory and simulation.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2366.022,
    "end": 2371.065,
    "text": "Does that, is that overlapping with what you're meaning by simulation or does it have a different",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2372.782,
    "end": 2383.229,
    "text": " Um, meaning if you're familiar with this work, probably I'm not, I'm not as familiar with that work, um, as with, um, the, the new paper that I just, that I mentioned.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2383.609,
    "end": 2383.85,
    "text": "I am.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2383.89,
    "end": 2385.491,
    "text": "And it is just a, yeah.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2385.911,
    "end": 2386.171,
    "text": "Great.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2387.732,
    "end": 2388.153,
    "text": "No, just kidding.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2388.173,
    "end": 2388.593,
    "text": "I want to read it.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2388.853,
    "end": 2390.034,
    "text": "Um, okay.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2391.075,
    "end": 2391.675,
    "text": "I'll accept that.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2391.815,
    "end": 2393.436,
    "text": "I'll, I'll take that for the work for it.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2394.677,
    "end": 2394.937,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2394.957,
    "end": 2402.022,
    "text": "I mean, essentially, uh, especially these deep generative models or they allow you to entertain, um,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2402.742,
    "end": 2413.365,
    "text": " Well, I mean, there's a sense in which the, like, the Bar-Salu, you know, idea of metaphor is just what the generative models do.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2414.285,
    "end": 2422.348,
    "text": "You know, if we're talking about a kind of loose associative structure that redeploys inferences about one domain in another,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2423.328,
    "end": 2428.03,
    "text": " I mean, this is the kind of thing that you would expect the generative model would be able to do.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2428.87,
    "end": 2432.371,
    "text": "So these things are definitely, I think, strongly aligned.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2433.292,
    "end": 2436.573,
    "text": "In effect, I mean, didn't Steven suggest something similar?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2436.933,
    "end": 2442.455,
    "text": "I think like the Yeah, metaphors are generative models in a strong sense.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2443.335,
    "end": 2444.876,
    "text": "Cool notion.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2445.076,
    "end": 2447.797,
    "text": "We'll go to Alex Kiefer, and then anyone else?",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2449.491,
    "end": 2451.533,
    "text": " Yeah, so that's interesting.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2451.553,
    "end": 2452.714,
    "text": "I don't know what to say about metaphors.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2453.515,
    "end": 2468.989,
    "text": "But I think the way I see this now is that I don't think what Maxwell at all are saying is at all, of course, maybe not of course, I don't think it's at all wrong, but I just think that the structural representationalist reading",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2471.158,
    "end": 2476.562,
    "text": " My view is that it doesn't quite say enough, but not that it's wrong in any particular respect as applied to the FEP.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2477.503,
    "end": 2489.873,
    "text": "So after further conversations with Maxwell and also with Michael and the other authors on this, it seems to me that there's still a need for a sort of neuronally realized generative model",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2491.829,
    "end": 2495.111,
    "text": " in addition to considering the entire phenotype to be a generative model.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2495.171,
    "end": 2516.566,
    "text": "So if you have a fancy organism that can do deep temporal kind of modeling and plan for the future and things like that, I think there's still a need for something that looks like a structural representation in the brain that is the generative model can be construed that way, as well as there's this sort of larger, more encompassing system.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2516.686,
    "end": 2520.208,
    "text": "And maybe this could be somewhat hashed out by appealing to the multi-level",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2521.249,
    "end": 2522.051,
    "text": " active infant stuff.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2523.393,
    "end": 2531.326,
    "text": "But last piece of this, in discussions with Michael Kirchhoff, I hope I'm saying his name correctly.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2531.687,
    "end": 2533.309,
    "text": "We've never said it out loud to each other.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2535.428,
    "end": 2541.816,
    "text": " it seems as though at least he is thinking of top-down sort of propagation of signals as a form of action.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2542.357,
    "end": 2553.07,
    "text": "So the stuff that we were saying about top-down generative models might just be sort of a special case of the sort of action that this paper is talking about.",
    "speaker": "SPEAKER_10"
  },
  {
    "start": 2553.953,
    "end": 2554.233,
    "text": " Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2554.433,
    "end": 2572.017,
    "text": "Before we go to Steven and then Lee, it really reminds me about Alejandra's point one or two weeks ago about what are the similarities and the differences between the systems that can at least appear to do these deep simulations, counterfactuals like the brain versus a embodied phenotype.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2572.297,
    "end": 2575.758,
    "text": "The cell perimeter is not simulating other cell perimeters.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2576.658,
    "end": 2580.339,
    "text": "That's a little bit different than the way that the brain might be able to do something strategic.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2580.979,
    "end": 2584.951,
    "text": " So we'll go to Steven and then anyone else can raise their hand.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2588.22,
    "end": 2617.391,
    "text": " Yeah, one thing I always look at when you think of, we talk about representation also in performance, is to think of like re-presentation, like is there another presentation in the brain, which often people start thinking of as a projection, and then you start getting into this very visually dominated view of what our knowing is, you know, so whereas it's a bit different to re-enacting or re-imagining,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2618.379,
    "end": 2633.767,
    "text": " So, like, if I was to reimagine as an engineer a model and then I manipulate it in my brain, well, am I manipulating it in my brain or have I reimagined it in my peripersonal space?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2634.448,
    "end": 2639.05,
    "text": "And am I now manipulating it actively in my peripersonal space?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2640.451,
    "end": 2645.154,
    "text": "And that is what I'm actually thinking of as",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2646.5,
    "end": 2665.192,
    "text": " representation but it's not actually my brain at all it's just a reenactment in space for me to then start to work with or if i do on paper i think the mechanism for that is this is not quite clear but this sort of opens up that question cool lee",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2667.673,
    "end": 2669.654,
    "text": " Stephen, I think I understand what you're talking about.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2669.674,
    "end": 2678.996,
    "text": "But are you just kind of mixing up the phenomenological with the underlying ontological level?",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2679.116,
    "end": 2686.438,
    "text": "So if you have a representation in the brain and you simulate it, that it manifests in experiential space or perceptual space.",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2686.498,
    "end": 2687.078,
    "text": "Is that what you mean?",
    "speaker": "SPEAKER_11"
  },
  {
    "start": 2688.496,
    "end": 2695.183,
    "text": " Well, to manifest it in the phenomenological space is to enact, is what I'm saying.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2695.443,
    "end": 2699.327,
    "text": "So it's not like it's in the brain, then I project it into space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2699.947,
    "end": 2701.188,
    "text": "It's not in the brain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2701.228,
    "end": 2705.513,
    "text": "It might be encoded, like place cells, grid cells, all of that stuff.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2706.192,
    "end": 2710.614,
    "text": " with all the other stuff, but it might be encoded in a way that can reenact it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2710.954,
    "end": 2723.439,
    "text": "But once it's reenacted, so with the work with clean language and micro phenomenology looks at like re-embodying concepts in space around us.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2723.559,
    "end": 2733.864,
    "text": "So my kind of feeling is that a metaphor is kind of like our reading on our kind of affect and our kind of,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2736.556,
    "end": 2742.144,
    "text": " our beliefs recreated in a kind of a physicalized way, you know?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2742.164,
    "end": 2746.811,
    "text": "So we actually do like, we, I feel like I'm stuck behind the wall.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2747.85,
    "end": 2750.571,
    "text": " Like, you can actually ask people, well, where's the wall?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2750.711,
    "end": 2752.212,
    "text": "And they will know where the wall is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2752.632,
    "end": 2753.893,
    "text": "And what's it like to be stuck?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2754.093,
    "end": 2761.816,
    "text": "And where, so suddenly actually find that they've actually got like a model, but they never had that model before they kind of created it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2761.956,
    "end": 2766.218,
    "text": "So the question becomes, is it a reenactment, a re-imaginization?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2767.139,
    "end": 2772.201,
    "text": "And how much of that was already, I suppose it's a question, was that already sitting in the brain ready to go?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2772.221,
    "end": 2776.663,
    "text": "Or did it only come about through the act of re-imagining?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2777.637,
    "end": 2777.897,
    "text": " Cool.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2778.597,
    "end": 2793.06,
    "text": "In our last 10 minutes, it would be awesome to hear from anyone who hasn't spoken, as well as just to flip through the figures so that we can see where we're going to be headed to do some technical unpacking, especially next week.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2793.1,
    "end": 2798.242,
    "text": "So anyone can feel free to raise their hand if they have a related or unrelated point.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2798.282,
    "end": 2803.663,
    "text": "But in these last few minutes, let's just try to look at the figures and hear some closing thoughts.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2804.443,
    "end": 2811.149,
    "text": " So in figure one, we have the Markov blanket and active inference, and we've returned to this figure many times.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2811.249,
    "end": 2816.754,
    "text": "It's used across papers and it's really nice to see it again and again, because the context is always different.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2817.395,
    "end": 2829.185,
    "text": "And the part that I underlined in red was in graph theoretic terms, the Markov blanket per se is defined as the set of nodes that isolates internal nodes from the influence of external ones.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2829.766,
    "end": 2829.846,
    "text": "And",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2830.226,
    "end": 2841.153,
    "text": " And in this capacity, it's really good that we had the discussion about integrating internalism and externalism, because those are also sort of like two ideas that have this tension between them.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2841.913,
    "end": 2852.64,
    "text": "And the Beyond Internalism and Externalism paper was about building on top of those, about taking the strengths of those two ideas and then asking how they're integrated under ACT-INF.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2853.36,
    "end": 2868.392,
    "text": " and today what we're doing and in this paper is about taking interpretations of internalism and externalism specifically this bayesian more computational perspective and this inactive embodied perspective and building on those strengths so it almost",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2869.253,
    "end": 2896.382,
    "text": " reads like a recipe for how active inference can be used for synthesis which is by identifying areas of tension in the literature or previously unrecognized harmonies between different ideas and then just asking how we can build strength on strength rather than attack strength to weakness or just um you know contrast weaknesses and i'm just going to continue just to show the other figures but i'll pause anytime someone raises their hand",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2897.322,
    "end": 2898.142,
    "text": " on figure two.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2898.202,
    "end": 2900.984,
    "text": "Oh, Alejandra, go ahead.",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 2901.744,
    "end": 2909.328,
    "text": "Actually, yeah, in that figure, I can take again the question you mentioned.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2909.348,
    "end": 2924.275,
    "text": "I'm still confused in terms of these generative models and their recognition density in which temporal and spatial scale",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2926.222,
    "end": 2942.108,
    "text": " of course in each individual cell of the brain this process is occurring and then in a layer of the cortex and then in the hierarchy",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2944.876,
    "end": 2951.399,
    "text": " of the cortical layers and then in the whole brain and then blah, blah, blah.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2951.5,
    "end": 2959.283,
    "text": "But so belief updating is happening in each cell.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2963.465,
    "end": 2964.266,
    "text": "I don't know if I'm",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2971.021,
    "end": 2984.125,
    "text": " I'm getting to the point, but the cells have to maintain, actually this blanket contains also some beliefs that never can change, right?",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2986.585,
    "end": 2992.447,
    "text": "So in order to continue being the cell it is.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2994.675,
    "end": 2996.036,
    "text": " So I don't know.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 2997.837,
    "end": 3009.764,
    "text": "This process of the densities is occurring in I don't know how many scales and... Yes, it's all of that.",
    "speaker": "SPEAKER_05"
  },
  {
    "start": 3009.985,
    "end": 3014.447,
    "text": "I mean, we'll end up hopefully discussing this next week.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3014.527,
    "end": 3019.49,
    "text": "If it's unclear, I mean, I have to say I don't think the figures are super useful in this paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3019.911,
    "end": 3022.192,
    "text": "They're mostly there to illustrate a few things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3022.771,
    "end": 3027.155,
    "text": " What's really important is the equations.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3027.916,
    "end": 3033.2,
    "text": "So, I mean, for next week, Dan, I'll show you which ones in particular so that we can put them up.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3034.401,
    "end": 3037.504,
    "text": "Yeah, so, Alejandro, what you were saying is exactly right.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3037.964,
    "end": 3040.526,
    "text": "All of these things are performing belief updating.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3040.907,
    "end": 3045.791,
    "text": "And when we talk about behavior or dynamics or whatever, it is all belief updating, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3046.295,
    "end": 3051.219,
    "text": " And belief updating just means changing the physical value of the recognition density at a moment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3051.44,
    "end": 3063.99,
    "text": "So basically, in this paper, like the big kind of conceptual contribution that I tried to make to this literature is to say, well, with active inference, we can rethink what we mean by embodiment and enactment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3064.831,
    "end": 3068.434,
    "text": "So embodiment means encoding a recognition density.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3070.036,
    "end": 3073.879,
    "text": "So your physical body is a guess about the",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3074.335,
    "end": 3081.782,
    "text": " It's a posterior probability that you're constantly updating through this process of belief updating, a.k.a.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3081.882,
    "end": 3082.883,
    "text": "dynamics, a.k.a.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3083.003,
    "end": 3083.904,
    "text": "adaptive behavior.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3083.924,
    "end": 3089.29,
    "text": "We literally embody this density.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3090.21,
    "end": 3095.636,
    "text": "The physical states of our body encode the parameters of probability densities.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3095.976,
    "end": 3096.737,
    "text": "This is the idea.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3100.023,
    "end": 3106.61,
    "text": " And then basically the generative model is just the point of reference for the dynamics that exists in the dynamics.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3106.931,
    "end": 3113.778,
    "text": "The way I like to think about it is sort of like dominoes falling over where the recognition density is the dominoes, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3113.998,
    "end": 3119.184,
    "text": "Each of these dominoes is like a posterior estimate, you know, and they kind of accumulate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3119.564,
    "end": 3122.067,
    "text": "But the wave is the generative model.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3122.694,
    "end": 3134.423,
    "text": " You see like the domino wave itself, the generative model exists in the same sense as the wave kind of making all the dominoes fall.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3134.764,
    "end": 3143.591,
    "text": "And the recognition model exists in the sense of the physical dominoes that are kind of encoding the process as it kind of flows over.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3144.552,
    "end": 3146.834,
    "text": " But this will be, I think, hopefully clearer next week.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3147.195,
    "end": 3149.837,
    "text": "Mathematically, it's actually pretty simple.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3150.438,
    "end": 3154.322,
    "text": "The generative model is the joint probability density over all of your variables.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3155.509,
    "end": 3158.111,
    "text": " And this density doesn't exist in the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3158.651,
    "end": 3159.491,
    "text": "It's not anywhere.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3160.332,
    "end": 3163.814,
    "text": "What exists is the factorization of this density.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3164.254,
    "end": 3167.777,
    "text": "So, you know, you always see this in all these active inference papers.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3167.837,
    "end": 3181.125,
    "text": "It's like p of all of your variables, so the eta, mu, and all the parameters, et cetera, equals, and then this long factorized, which is basically, it's a product of likelihoods and priors.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3181.765,
    "end": 3188.29,
    "text": " So this product of likelihood and priors is the recognition density that's constantly updated, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3188.35,
    "end": 3189.911,
    "text": "And that's what exists in the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3191.913,
    "end": 3195.856,
    "text": "But this joint density only exists as a function of the dynamics.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3195.976,
    "end": 3208.425,
    "text": "It's like all of these partial, like, carving ups of this density, right, this factorization, they all move together, and in their dynamics together they realize this joint density.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3208.445,
    "end": 3209.165,
    "text": "Okay.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3210.086,
    "end": 3231.44,
    "text": " sorry i just i i do have to end it at the 59 as we discussed but this is the perfect excitement to build for our follow-up discussion next week we're definitely gonna go into this question we'll just pick up right here we'll literally just hit play on the video and uh go into detail on the technicality and let's even have some dominoes on the screen so",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 3232.02,
    "end": 3247.237,
    "text": " again just thanks everybody for understanding about the timing i've provided a follow-up form to the live participants in the chat and we welcome any other feedback suggestions or questions and please just uh stay in touch",
    "speaker": "SPEAKER_09"
  },
  {
    "start": 3247.677,
    "end": 3268.735,
    "text": " stay engaged if this was interesting or exciting we welcome all participants and it would be awesome to have you on this stream asking the questions and also learning by doing with us so thanks everyone for the awesome and energizing discussion and i'm really looking forward to next week when we can go another level detail into all of this so thanks so much",
    "speaker": "SPEAKER_09"
  }
]
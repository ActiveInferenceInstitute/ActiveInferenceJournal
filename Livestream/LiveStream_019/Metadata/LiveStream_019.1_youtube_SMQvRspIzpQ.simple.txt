SPEAKER_00:
Hello everyone.

Welcome to the Active Inference Lab and to the Active Inference Livestream.

This is Active Inference Livestream 19.1 on April 6, 2021.

Welcome to the Active Inference Lab.

We are a participatory online lab that is communicating, learning, and practicing applied active inference.

You can find us at our links and contact information here.

This is a recorded and an archived livestream, so please provide us feedback so that we can improve our work.

All backgrounds and perspectives are welcome here, and we'll be following hopefully video etiquette for livestreams, such as muting when there's noise in our background and raising our hands so we can hear from everyone who wants to speak.

You can go to this shortened link to see the upcoming streams of different kinds.

Today we're in 19.1 on April 6th, and we'll be having a follow-up group discussion next week on April 13th.

Hello, Stephen.

We're already live.

So today in Active Inference 19.1, the goal is really just to learn and discuss about this really awesome paper, Deeply Felt Affect, The Emergence of Valence in Deep Active Inference by Caspar Hesp, Ryan Smith, Thomas Parr, Mika Allen, Carl Friston, and Maxwell Ramstad.

And we're really appreciative that Caspar has joined us today.

And any one of these other authors are free to join us next week or the following.

In 19.1, we're gonna go over the various parts of the paper in any order that people want to raise them.

And we can just start with some short introductions and warmups that will be culminating in Casper and giving a little, maybe just introduction or a background.

And that will be great to start with.

So I'm Daniel, I'm a postdoc in California and I will pass it to Sarah


SPEAKER_02:
I'm Sarah.

I'm a person stuck in Berlin because I can't update my visa.

Next.


SPEAKER_03:
Hello, I'm Stephen.

I'm in Toronto.

And I'm very much looking forward to learning more about affect and its relationship to active inference.

And I'm going to pass it to Dave.


SPEAKER_06:
Okay, Dave Douglas.

I'm in the mountains of the Philippines, retired from

information technology including natural language processing and machine translation background in process philosophy whitehead especially and cybernetics and i will pass to dean if he hasn't spoken yet i'm dean i'm from calgary um retired but really interested in this stuff i don't know who i can pass it to other than maybe casper i'm not sure


SPEAKER_05:
Sure.

Yeah.

My name is Casper.

I'm a PhD student at the University of Amsterdam.

I'm currently being co-supervised by two professors at the University of Amsterdam and one being at the University College London.

So that's one you probably all know called Kristen.

my external supervisor and this paper was developed under his in his lab basically with a bunch of really wonderful people actually my co-first author is not here today but i am aware that he has been ryan smith has been with you on many of these sessions and

Yeah, so I think I'll just go into the intro of the paper.

And I guess it will be interesting because you can read in the paper itself the kind of formal motivation.

I will tell you a little bit more about the storyline, how this came about.

When I first entered the group, there was

most of the modeling that was done was with single layers and there was already some preliminary work on deep temporal modeling and that's essentially working out the implications of this kind of hierarchical models that start to look more like the one that you might have come across in deep learning research

But now with the Bayesian flavor.

And one of the things that I was missing personally was a meta cognitive aspect that moves beyond just estimating confidence.

It's one aspect.

But then reflecting on that confidence estimate to give you an idea of

something that can regulate other parts of the system.

So we ended up going back and forth on this topic with Carl and Thomas Barr was a huge help there as well.

And he's also on the paper.

Essentially, thinking about these action models, and active infants, and I'm kind of presupposing certain degree of familiarity, but I can

Suppose I can quickly recap.

Essentially, the way I presented it in paper is really meant to be something that you start from zero knowledge about active inference.

And I kind of build it up in steps so that people are not super familiar with it.

Yeah, I guess it's like these steps essentially

the the point is that these diagrams have a very mathematical meaning a very specific mathematical and when you just present people the whole diagram at once what we call directed acyclical graphs it usually is pretty overwhelming so for this paper i decided to kind of go back and just give the viewer the reader um a very

stepwise, an incremental primer, as we call it.

And we start with these first two states that are being linked is the types of states, these hidden states and sensory states.

And that's really the core.

And what's interesting is that it's very modular.

So if you go to the next slide, Daniel,

And yeah, so if you just ignore the maps for now, but the nice idea here is that these graphs are entirely, it's kind of like a Lego box essentially.

So once you understand the first piece of the puzzle in M1, where it's inferring hidden states based on sensory states,

you can basically understand all the other graphs as well to a certain degree.

At some point, we introduce links between continuous and discrete state spaces.

That's where it gets a little bit more complicated mathematically.

The one thing you can keep in mind when reading these figures is that downstream on the arrows is always about prediction.

So here, for example, the prior over hidden states on the top V predicts the hidden states below that S. So downstream on the arrows, we're talking about prediction.

Upstream, we're talking about inference.

So in this case, you start from the prior, make prediction about what the hidden states are a priori.

And then moving from these hidden states, make prediction about what the sensory states are.

And that's where the likelihood mapping comes in.

So that's one crucial aspect.

So downstream prediction, upstream inference.

Then there's another crucial part that you need to know in order to interpret these figures is that the circles are variables and the squares are parameters.

So in this case, hidden states and sensory states, those are listed in the circles, the S and the O. Those can vary over the course of the simulation.

If you set it up in this very simplistic way, then the squares are fixed.

So the values in the prior and the likelihood mapping are fixed in this very simplistic setting.

Once you understand this, you can start hooking these things up to each other.

So once we link another circle to a square, then the mappings become variable as well.

And you will see that coming back later.

But it would now generalize this reading of the figure to the second.

So M2 here.

Exactly the same maps is applied to this way the states evolve over time.

So generative model of anticipation is using current states to make predictions about future states.

That's where we come to the third part of these kinds of graphs that's pretty important to

know is that from reading from left to right it's supposed to be the temporal dimension so s1 is used to make predictions about s2 and based if you have knowledge about s2 you can make inferences about what s1 was the most likely state in the past as well so it works both ways

so that's perception and anticipation um obviously entirely reduced to the most simple yes simplistic kind of form that you couldn't come up with essentially sometimes often these models are criticized for being too simplistic well i think that's kind of the beauty in the sense that um we obviously

start with these components, the Lego blocks, so to speak.

The individual blocks are pretty simple.

But once you start hooking them up to each other, you get emergent dynamics that are pretty hard to already pretty, yeah, pretty quickly become hard to wrap your mind around.

So that's why we're presenting them in a stepwise fashion.

I would say if you're really interested in the maps, then

You would probably want to go into some of the tutorials, for example, that Ryan Smith recently made.

I can actually send a link to that right now.

But he works through the actual equations in step-by-step fashion.


SPEAKER_00:
Cool.

You can put any links in the YouTube live chat so that everybody can see it.

Otherwise, only we'll be able to see it.

Yeah.

Cool.

Thanks for all these clarifying points on figure one.

And I agree.

It'd be awesome to have Ryan.

He's been on a few times.

So yeah.

Okay.

Question related to figure one, Stephen.


SPEAKER_03:
Yeah, just one question.

When you mentioned the predictions and inferences, do you tend to think about the downward predictions across all the components and then the inferences more, although it's happening at all components, more in terms of the big block

jumps between like what m1 and m2 is inferring in relation to each other so you do you tend to think about it more that way so you look at each piece as a as more of smaller ideas predicting down and linking and then the inference is more thought of in these you know broader sense of something that you could get your head around as a human so to speak


SPEAKER_05:
It depends on which level of description you're currently focusing.

But in a sense, the answer is yes.

Once you stick another layer on top of it, so to speak, that layer is basically perceiving.

In a weird way, you could say it's trying to perceive the perceptions.

And then that allows the system to report.

on these perceptions.

So that's like a kind of meta-level representation that gives the system its capacity to reflect on its own inferences on the lower level.

And we have also worked on paper where we apply this to the precision on the likelihood method.

and probably not get into that too much for now, but it's if anybody's interested.

So let me just, I found, I've gotten access to the live stream now.

Let me just post.


SPEAKER_00:
Say hello, and then I'll make you a mod so that you can post links.

But yeah, thank you for adding links to the live stream.

anyone else have a question on figure one otherwise it'd be nice to i think continue this incremental um unrolling of the model and then we'll definitely get to a lot of different topics i think in a broader discussion too and in the lecture so here's the paper by ryan smith and then there's

Okay, now you're a moderator, so now nice.

Okay, Steven, question on figure one or broader question?


SPEAKER_03:
Yeah, just building on what was just mentioned a second ago with figure one is, you know, thanks for that clarification.

And so, like you say, this idea of like a meta level of inference is useful.

And I think, and this came up in the previous live stream with Ryan Smith, that there seems to be something about the updating between

So as well as like going from one chunk to the next chunk, so to speak, of M1, M2, and giving that as a way to get a handle on an inference, there's like something that can be done with whatever gets message passed up.

And so from my understanding, that looks like the two ways into the inferential piece, so.


SPEAKER_05:
Great.

Yeah, so that's where...

Essentially, one of the core innovations that we present in this paper is something we like to call deep parametric active inference, where you're making inferences about the parameters of lower level models.

And then that is guiding your expectations.

We're going to get into that more once we go down this path.

Let me see.

So I've managed to get access, I think.

So what does the moderation help me?


SPEAKER_00:
You could post links.

I think people who are moderators cannot post links, but we got your links.

So the tutorial paper and the meta awareness paper are in the live chat.

So thank you.


SPEAKER_05:
Yeah, good.

Okay, cool.

Yeah.

So just to illustrate the point, I think it makes sense to go further.

Because essentially what I think it was Dean was mentioning, or was it Dave?

I couldn't see.

It's my own site.

But anyway, so what was mentioned is that, yeah, you have these Lego blocks and you start building your hierarchy, so to speak.

One of the things that had already happened by the time I came around was this step presented here, where again, the circle here with the pie inside is a variable and the simple act of connecting this variable to the state transitions on the lower level.

basically the variable pi which stands for policy um starts to dynamically control your beliefs about how to stay how the states of the world equal all the time yes so this is um then controlled by the prior um

And this is really a little bit of a funky aspect of active inference in that formulation, because the prior on these policies is informed by the expected free energy.

The expected free energy I call is funky because it recapitulates the entire generative model down below.

The expected free energy includes the expectations of the organism about the world.

So there is this thing that Karl and Thomas Spahr have called the generalized free energy, where they reformulate this and unpack what's inside the policy variable in terms of a whole generative model that mirrors the perception aspect.

don't need to get too deep into that.

That's essentially unpacking what the expected free energy here does in the model.

It's very important because the expected free energy is what introduces what I call the phenotypic or phenotype congruent action model.

It's also going to be answering one of the questions that came up.

I watched the previous live stream that you had on this paper.

We'll get to that in a second.

But we can use it to answer one of the questions that came up about whether the effective charge, why the defense only arises when there is a difference between the prior and posterior action.

We'll get to that in a second.

So the expected free energy, you can think of it as recapitulating the entire action model with one big difference, namely that it's biased by phenotypic outcome preferences.

So it's essentially the phenotypic outcome preferences basically bring into active inference what the reward function does in reinforcement.

It does this because it's essentially biasing the way the agent behaves towards the world and trying to make it match its preferences that are specified here clearly in probabilistic terms.

So the phenotypic preferences can be

something that can be ingrained, that can be learned.

It's pretty agnostic where it comes from.

That's because it has this hierarchical nature.

So I have models in which these preferences themselves are state-dependent.

So let's say you're hungry.

When you're hungry, your preference factor for food is upregulated, essentially.

It's made more...

a stronger driver of your actions.

When you're tired, then you have a stronger preference against exerting effort.

So there's these kinds of, these preferences themselves can be made state-dependent.

If you just add another Lego block on top, you start having inferences about how hungry you are, how tired you are, and you can have an organism that's managing its energy resources.

Then there is this last component that I haven't really discussed is the E here.

The E matrix, you can think of it as a kind of habitual or what they call in reinforcement learning, they call it mobile free.

It's just counting the number of times a particular action occurred.

It's not evaluating it by any

like by any reward or something like that.

It's just

I mean, I actually don't like the term model-free because in essence, it is still a model.

It's just a model that has a separate parameter for every possible action.

So just counting, it's kind of like when you took your driver's lessons and I remember repeating mistakes, not because I liked to make those mistakes, but just because I made them before and my body and brain kind of

stored that information and I ended up repeating those mistakes just because this pathway had been explored before.

Okay, so that's like a little bit of unpacking here.

The way you can read this diagram is again from left to right.

So you start with the action model and the expected free energy guiding the a priori expectations.

this is a kind of crooked scientist i would say it is biased towards not what you believe the world to be exactly but more towards what you would like the world to be like um or what's if you are currently doing well what you would expect the world to do so it's pretty um

um it's it's an intentional by yeah like the bias is introduced in that site and then if you look on the other side and get the perceptual evidence it's the free energy you can think of that as a kind of reality check so i can have lots of expectations about what i want the world to be like and where i want to end up

or what I want my shoes to be like when I buy them.

I remember this example from last time.

But then I need perceptual evidence to actually see if the shoes actually fit after I bought them, let's say.

So this perceptual evidence is a very crucial component of how the whole perceptual feedback

Without that, your agent would be living in a fantasy world.

So unless there are any questions, we can move to the next slide.


SPEAKER_00:
Anyone can raise their hand.

Otherwise, Steven, question on this figure or a broader question?

OK, go for the question on this figure.


SPEAKER_03:
Just want to ask you one question there.

With your action model,

Because with expected free energy, you often have the risk, ambiguity, and pragmatic gain as three terms on one side.

So is it like that the pragmatic gain is built into the action model implicitly?

So it's kind of, you've rearranged it slightly with that in mind so that all action has some sort of pragmatic gain built into the phenotypical morphology of the animal or something like that?


SPEAKER_05:
Yeah, so here the pragmatic part, so that's the kind of the C, the outcome preferences, so your preferred observations essentially, is built into, it's part of the phenotypic risk.

So if you look at this equation on the left, so the right-hand side with the left term for phenotypic risk, you see there you take the difference between the logarithm of your expected outcomes minus your preferred outcomes.

than in terms of units of nets, so information .

So what you're essentially, the risk here is just quantified in terms of the difference between your preferred outcomes and the expected outcomes given a particular action.

That makes sense.


SPEAKER_00:
cool and it's also interesting it's the natural log of o sub pi so it's conditioned on a policy so it's not like some sort of all by all chess boards all strategies all situations it's very constrained with respect to what actually matters which are in the end the affordances in the niche of the actual agents in the in its scenario so it's really cool to see how that gets

baked into the model, not like a secondary pruning or a heuristic.


SPEAKER_05:
Yeah.

So this, this just falls out of calculating the expected energy under a certain policy.

Um,

So this G is conditioned on policies in that sense.

And one interesting aspect here is that if you make the generative model more sophisticated, then this G is going to become more sophisticated as well.

So you can add, if you add learning for parameters of, let's say, the likelihood method, then the expected free energy also expands accordingly, and it ends up having a term for active learning.

So your agents can anticipate basically that exploring the environment can allow to reduce uncertainty about these perceptual mappings.

So then you get an agent that's intrinsically curious about the world.

You can do the same with them.

the B matrices, so the transition matrices.

So you get an agent like a child who kind of just experiments, exploring different states of the world, and then starts picking up on different kinds of control that it has.

And can even, at some point, can even learn to anticipate that if it experiments in a certain way, it might be able to learn new things about these methods.

So this is currently not in this particular model, because if you remember to do that, you need to add variables to the particular mappings.

And the nice thing about this Lego box of active inference, so to speak, is that you're completely free to make anything variable.

as long as you specify certain probability distributions, you can do the same trick again and again on discrete and continuous datespaces.

So I think this is a nice segue into, oh, here you first introduce in table three some of the equations .


SPEAKER_00:
Here we have table three up on the screen, or do you want figure three?


SPEAKER_05:
No, I think we can go on.

I mean, unless somebody wants to go deeper into this, but I think we already can discuss this.

Great.


SPEAKER_00:
Yep.

It was really helpful to see the tables laid out.


SPEAKER_05:
This is actually not the way I delivered the table to the journal.

It's just that they had three, I mean, it's crazy.

You pay them this money to fix it, you know, but then they make it work.

So like basically they had like a new layout and their web layout messes up most of the

So when you want to read it, I would really recommend downloading the PDF and not using their web layout.


SPEAKER_00:
Oh, yes.

This is their web layout.

It definitely makes sort of a false equivalence or a way of making it look like there's something missing on one side or the other.


SPEAKER_05:
Yeah, so just read the PDF.

It's unfortunate, because it's supposed to be their job to do it.

Yeah, anyway.


SPEAKER_00:
Well, here we are on figure three.

And so we have just the figure three.

And then we also have that part with the AC, if you want to highlight that, because I know that's something we really want to understand.


SPEAKER_05:
Yes, so we're going to continue our building of this.

We're adding these different blocks together.

And now what we add on top of it is we have g. This is the recapitulation of the entire action model.

We add a variable on top of it that modulates the extent to which I rely on this action model.

So the precision, the gamma, can take any value between 0 and infinity.

And it just tells me when I predict what's going to happen in the world and anticipate what's going to happen, how much can I rely on my action model that's biased by my preferences?

So that is not the same as...

I mean, that's kind of where it comes into story, why it's this effective charge that we're talking about ends up being depending on the difference between pi at the prior and the posterior.

So essentially, what this precision is tracking is tracking the match between

predicted value, you can say, of G, or predict for the actual policies that I end up selecting.

The actual state transitions of the world, and how well they are predicted by my action model.

And this is my crooked scientist model, right?

So it's like an action model that's trying to realize preferences.

And

effective charge is essentially the kind of we call it like that because later on it's getting a special role and when you go to hierarchical setup we add one more lego block on top of this but essentially um whenever my um whenever um

So if you unpack this effective charge equation, you see that it's calculating the difference between my prior policy vector.

That's just based on my preferences, my actual model and the posterior.

And that's the one that happened that I have after I integrate perceptual evidence.

So, and then that difference.

So that's where it's a little bit hard to imagine, but basically the difference between these two is kind of like the rate of change, you could say, over time of my beliefs.

And then you take the product of that with the expected energy.

And what what you essentially want to know is after integrating perceptual evidence, do they get closer to

matching the expected free energy, the distribution anticipated by that, or did I get further away?

So that's why I call this phenotypic progress.

If the expected free energy is a bad predictor, then this term ends up being negative.

If it's a good predictor, it ends up being positive.

And I mean, there's like a little bit of a play with science there.

And we just define it in a way that it's positive, that positive value is a good thing.

That's like always a little bit confusing because in some different fields they use, there's even fields where the energy is defined in the opposite way.

So the whole story clips on its head.

That's why you always have to be careful.

how you define it locally in each paper.

And when you come from a different field, you have to make sure that you've got your science in your science in a row, so to speak.


SPEAKER_00:
Yeah, if I can say that one major field there are differences that physicists often go downhill, gradient descent, optimization, we're going to minimize the loss function.

And biologists often talk or think about hill climbing and fitness peaks and optimization, maximization and congruence and the best possible world, that kind of Pangloss world.

So...

totally agreed and especially when there's a lot of natural logs and negative signs and differences and divergences that can only be positive and things that are strictly negative or bounded but when it's something where it really needs to be walked through slowly because it's so easy to get tripped up in how these things flip back and forth

So really important notes to kind of just go slow and really make sure that we're using the right tools to establish that we're connecting the right qualitative ideas with these variables, because it's not just like you throw up the equation and then it's going to be easy to define what each of the variables are for real systems.


SPEAKER_05:
Yeah.

And on that note, we have interpreted this gamma in the paper as a type of subjective fitness.

And that's because literally it's tracking the degree to which your preference bias action model is fitting with the actual perceptual evidence that you're getting back.

That's why the effective charge can only be non-zero if there's some mismatch between your prior and your posterior beliefs about policies.

Because there's only when there's some mismatch between what you expected and what you got.

It makes sense to talk about updating your beliefs about proceeding.

It can be in a positive direction or in a negative direction.

So my fit can be actually better than expected.

or worse than expected.

And that's why I've described in the abstract that this lends a sign to these predictions.

Essentially, if you think about it just in terms of predictions, it's kind of, well, prediction is either wrong or very wrong, right?

So then, I mean, if you want to know, if you go to this meta level where you're estimating how wrong am I,

So is my prediction getting better or worse?

That's where the sign comes into the story and where you can talk about improvements or improving or worsening state of the world, so to speak, and state of yourself.

History applies to any kind of states, by the way, so it can

It can be about internal states, tracking, as I said, hunger and fatigue.

It can also be about the affective states of other or conspecifics.

So this kind of subjective fitness can be linked to any arbitrary state of interest.

And if you go to the next level, then the state of inference can be an observation for the next level.

That hierarchical trick is, I think, the power of it.

We have freedom to do what we want to do with our Lego blocks, so to speak, depending on this system of interests.

But we try to keep it simple while we're still just demonstrating possibilities.

But then the implications get

Yeah, really, really big once you start to release the simplicity constraints and you just go wild on constructing models, hierarchical models with different

you can have expected energies at different levels, you can have these kind of precision terms at different levels.

So you can have competing and affected charges, you can have the effective charge from your lower level, let's say you're trying to fight your addiction or something like that.

The lower level parts of your system are like kind of create and generating this creating in the higher level parts are like trying to

generates this kind of self-actualization or something like that, where you basically end up having conflicting drives.

So I mean, that's where we can go.

But to get there, we need to move to add states on top of this.

And I think, yeah.

If there are questions, we can.


SPEAKER_00:
I'm going to ask a question from the live chat and then anyone else who has been spoke yet or ask a question and then we'll ask a question about anything else we've talked about, Steven.

So in the chat, someone asks, can we relate affective charge with motivation as motivation can also affect policy?

So how do we think about motivation in this kind of a model?


SPEAKER_05:
affective charge is it affective charge yeah so um motivation in that sense is a very kind of fuzzy concept right but um if you think about the gamma term here it's modulating the extent to which you're relying on your action model

And the C, the preference factor, modulating the extent to which you're driven by your preferences.

Combining them together essentially means that if you have, let's say,

You need high confidence in your action model to actually allow it to influence your expectations.

So in that sense, there is definitely a link with motivation.

But then again, you also need preferences that motivate you to move towards them.

So what's kind of interesting is if you take away the preferences, then you can have an agent that's motivated entirely by epistemic value.

so I suppose to some extent it's kind of interesting because you get the precision starts to modulate curiosity and so that's the different kind of motivation

Anyway, I can't say much more because it's such a fuzzy concept.


SPEAKER_00:
Very true.

Reward motivation, curiosity motivation.

There's expressivity in the model to actually talk about motivation with respect to specific framings of it.


SPEAKER_05:
The way I've recently thought about it is also that if you add a higher level and you connect it to this preference matrix,

you can basically modulate the extent to which you're motivated by different kinds of outcomes.

And that means that if you're monitoring on a higher level, to which extent you're currently satisfied, for example.

So you're hungry, so you upregulate your preference for eating food.

And then you get your food and you get feedback, interoceptive feedback that you're satiated.

And then you're open for more epistemic drives, let's say.

So there's an extensive sense in which the various types of motivation have to be balanced by a higher level.

This current figure is not able to do that in a way.

I mean, our full Lego box can definitely do that.

But the current figure, I think, is still doesn't have dynamic preferences.

I think that's what you need to get more satisfying accounts for motivation in general.


SPEAKER_00:
Thanks for that awesome response.

All right, Stephen, and then anyone else who raises their hand.


SPEAKER_03:
just one question you mentioned dot product with expected free energy and just just if you could just clarify that a little bit because i think you've got something to do with the precision you got the the larger free energy piece so if you just clarify that would be helpful um yeah so this effective charge term is not something we came up with


SPEAKER_05:
right it's something that comes out of the maps when you start with this gamma distribution so the capital gamma distribution is a particular mathematical um yeah shape an exponential a natural exponent based on natural exponent

And when you use that as a probability distribution, so it has this kind of tail that goes towards infinity.

And the expectation value of this tail is regulated by what's called here the temperature parameter, also called the rate parameter in other contexts.

But essentially, the temperature parameter kind of regulates how

how strongly the distribution is shifted towards zero.

And essentially, what happens is that when you use that as the probability distribution and you try to optimize this beta parameter to best fit what's happening, you can do this pre-energy minimization.

to do that.

And what you get out of it kind of for free, once you write that down in probabilistic terms, you get this effective charge term.

And what I was talking about is essentially an interpretation of what, OK, this is what we got out when we took the generative model, let it minimize variation of free energy on this beta parameter.

then it just kind of spits out this affective charge term.

And then it's a matter of interpreting, like, what does this term mean in a mathematical sense?

And essentially what it's doing is taking dot product between two vectors.

One vector on the left is the difference between the prior and the posterior over action.

And the other factor is the expected energy for each policy.

And essentially, if the two factors are pointing in the same direction for an organism, that's bad news, right?

Because if my posterior results in larger expected energy, then that's bad because I'm trying to minimize it.

Wait, let me turn the light on.


SPEAKER_00:
Dark room solved.


SPEAKER_05:
What's wrong?

I just solved the dark room problem.

The idea is basically that the expected free energy, you can think of it as a kind of landscape across the domain of potential actions.

What I want to know is that after integrating my perceptual evidence, did what that vector looked like on the left, did it match the original vector I had in terms of the expected energy?

That's why if you look closely, it's the prior minus the posterior.

Because that's the negative rate of change, right?

The actual rate of change would be the posterior minus the prior.

But we flipped the sign there, such that you can interpret the effective charge as being positive when the precision goes up, and being negative when precision goes down.

Does that answer your question?


SPEAKER_03:
Satisfactory?

Yeah, I think that's very helpful.

Thanks.


SPEAKER_00:
one one example that came to mind was like you're driving and you're having a maps application tell you it's going to take one hour and then as the estimate gets worse it's like it's bad if you want to get there if your preference is to get there on time now if your preference was to get there later maybe it is more neutral but then as the update is changing and your relative policy it could be like

oh, it's gonna be 18 more minutes because of this crash.

Oh, but then this speeded up this way.

And so we're always in a really specific situation, conditioning on policy and the information we're getting.

And so we're not doing the whole traffic city flow.

This is about like the person getting the info from the screen and then making decisions about, oh, maybe I should get gas here or not.

Not every possible decision they could be making.

So very, very interesting way to frame this variable.


SPEAKER_05:
And this dot product, the way, I don't know how many people here are familiar with the way this kind of vector calculus tends to work, but essentially you can just think of it as the dot product kind of calculating the degree of overlap between the vectors.

And if they're anti,

If they are exactly in the opposite directions, then the dot product is very negative.

If they're pointing in exactly the same direction, then it's maximally positive.


SPEAKER_00:
And are these, what are the dimensionalities?

So you're talking about them as vectors, but are they scalar or is there just one entry in the vector or are there potentially multiple or how do we think about, are these one number or is it a list of numbers like a long vector?


SPEAKER_05:
So in this older formulation of the expected energy, it was like one value per policy.

essentially, which is like an action sequence.

And so every policy has a total sum of the expected free energy associated with it.

So then it's a vector in the sense that every policy has its own element in the vector.

But more recently, we've actually extended this in the sophisticated inference paper.

Because anybody who's worked with policy spaces knows about this problem of combinatorial explosion.

That once you start considering forces of action in time, then every point in time you expand it further, the more, yeah,

the more untenable and intractable the problem becomes.

In a more recent iteration, we actually divided further in terms of the actions components.

Instead of having one single policy vector that has to regulate through all the machinery, every time step has its own action variable.

and you're forming inference, doing inference on that on every time step.

This makes the problem, prevents you from having to integrate everything right away.

The end result of that is again an integration, but because here it kind of allows for parallel computing, so to speak.

Anyway, but that's a different paper.

I can actually also post that one.


SPEAKER_00:
I think we actually read it, but, you know, it all blurs together.

But we did read sophisticated inference, I think, at one point.


SPEAKER_05:
Oh, yeah.

Yeah.

Nice.

Yeah, you read lots of papers.

That's why you have to catch up.


SPEAKER_00:
We got to catch up.

You're leaving them as fast as we can get to them.


SPEAKER_05:
I mean, I made more recently in a paper on called Sophisticated Affective Inference, where

So you have another thing to patch up.

But it's essentially combining this affective inference story that we're talking about today, combining it with this research, which allows you to simulate an agent that has an affective response to imagined futures.

It would be cool to maybe at some point to come back to discuss that one as well.

Cool.


SPEAKER_00:
Cool.

So Dean with a question, and then anyone else who raises their hand.


SPEAKER_07:
Casper, when you go from a 2D M1, M2 to a 3D action,

Is that, have you thought a little bit about the sort of the less about the integration, more about the deflation and inflation that happens when you go from fewer dimensions to more?


SPEAKER_05:
Yeah, so that's kind of one of the motivations for going to new types of generative model, because this policy variable here can explode pretty quickly once you get to multiple steps in the future.

And you need somehow to reduce the dimensionality of the state space that you're considering.

One thought that kind of soothes me when I build these types of models is that

Even though, as we say, all models are wrong, but some are useful, right?

But even though that's the case, we know that the organisms that we're trying to model have exactly the same problem that we have.

Namely, they're trying to make something that's generally intractable, tractable.

So the simplifications, when we're trying to build a model of their model of the world,

you can actually justify the simplifications in the sense that they are trying to deal with, they have to do oversimplifications as well to be able to make sense, any sense at all of their environment.

I mean, I don't know if this exactly answered your question, but this was a thought that came up in response.

Sir had an interesting point about the dimensionality increasing.

And one thing that kind of happens is that you basically have to stimulate potential futures internally.

Once you start conditioning the states on your actions, that means every possible action has a parallel kind of process of inference that's happening to predict what's going to happen in the future.

And every dimension you add on top

can potentially make the whole thing intractable.

So we're always facing that challenge.


SPEAKER_00:
Yeah, Dean, and then


SPEAKER_07:
That's quick follow up.

So sometimes you hear the expression, oh, well, that problem is going to require some outside the box thinking.

And of course, that's so ambiguous.

Nobody really knows what that means.

But what you've done here, I think, is given people an opportunity to get out of the spatial envelope, get out of the

I'm an agent enveloped by the world around me, and you've given them a bit of an eyes of this perspective.

They can actually get outside and see how, as an agent, they're perceiving the world, how they can then

potentially act on the world.

But as you said, they're constantly updating.

A lot of this stuff isn't front of mind as they're doing it.

But I think that's one of the big things about going from 2D to 3D.

You're not just zooming in and zooming out.

You're

you're not just now you're able to look down on something, but if you hit the button on the Google map and you suddenly see a 3d, it doesn't, it doesn't stay at one position.

It actually circles around the thing that you're looking at.

And I think that's what this provides people is that sense of not just being inside of something, but actually being able to step outside of it as well.


SPEAKER_05:
I mean, that's something that, um,

Yeah, actually does happen in this type of model is when you add this pie, let's say, in M3, you end up expanding the whole thing.

And it's, like you said, like stepping from 2D to 3D in a sense, because you're expanding the whole number of possibilities, basically, in the model.

the same way it's kind of hard to wrap your mind around it and i think that's part of what makes these figures hard to read for people especially when you present the top level right away it's like asking somebody to step from one from one dimensional to three dimensional right in one go you kind of have to go through the the increases in dimensionality step by step

So this is also part of what motivates this whole incremental presentation.


SPEAKER_00:
Thanks.

Awesome question.

Thank you, Dean.

So Blue?


SPEAKER_01:
so we read this sophisticated affective inference like a long time ago and i always think about it and i think that i brought it up in the dot zero live stream actually but i thought it was the big five paper i think we read them like right at the same time and they they're very different but this um sophisticated active affective inference was really um

like prevalent with anxiety, like the future time steps, the further out you go in the future, like the more like you can't predict what's gonna happen.

And so that's like the underlying basis for anxiety.

And I thought about this and I also thought about the question about motivation and it made me think of Tony Robbins has like,

this um like theory that there's like six like driving factors that like influence people or motivate people through life and like you know depending on personality type or whatever and um their like uh certainty is one of them um uncertainty is also one of them like people who like variety or um

Then there's growth and altruism.

And I can't remember all of them, what they all are.

But it's interesting to think about the motivation, what's an underlying motivation for people and how that relates into this type of model, as well as those driving forces through life, how you might start to model those

Things like someone who's driven by uncertainty versus someone who's driven by certainty.

The uncertainty-driven people might place more of an epistemic value than an actual action reward value.

So it's just interesting to think about that.


SPEAKER_05:
Yeah.

I mean, there's like a paper in the works that got stalled, unfortunately, but, um, I think where we're trying to do something like that, where you're talking about, um, the way the big five, um, have kind of emerged as apparently, uh, pretty, uh, strong factorization people's behaviors, uh, people's, um, behavioral tendencies over long periods of time.

So their personalities, um,

and why they tend to factorize like that, and to which extent you can capture them with something that looks like this, and with at least one more layer, I would say.

But to some extent, yes, you can capture these tendencies towards exploration, the kind of exploration drive versus... So risk aversion and risk...

um as we risk seeking behaviors


SPEAKER_00:
i'm actually seeing when it comes out um cool blue that was really interesting and it reminded me about how people prefer let's just say to read different amounts and to read different topics so for some people historical fiction versus fiction science fiction all these different genres and sub genres it's related to maybe what their regime of attention will latch on to and that's something that's different as you age uh and

and it's encultured and it's embedded as well.

So it's kind of an interesting example that we could go into.

And then we did a live fact check.

Act Inf 11.

way back when in 2020 we did do sophisticated uh affective inference the simulating anticipatory responses paper and that was way more anxiety driven no it wasn't just because it was 2020 and then also the first one of this year with um adam saffron and colin de young was the big five

cybernetic big five kind of free energy inspired uh are based as well so um yep it's just it's good you know you you did say we're kind of going through them fast but there's so many to read and just to even get a little bit of a grasp on it it's to read 25 in a year or one every two weeks

it's kind of a pace that we have to hold up to.

But I'm sure you read more than one every two weeks.

So, yeah.


SPEAKER_05:
I guess I did read it somewhere and I was like, whoa, you...

I wasn't sure.

I didn't remember whether it was like sophisticated inference or sophisticated effective inference.


SPEAKER_00:
But so it was the effective one.

It was just the conference proceedings, though.

So it was a very short paper.


SPEAKER_05:
Yeah, it's a little bit too minimal, I think, to really.

It was more like a technical note.

And it would be hard to really get it just based on if you're not so familiar with the modeling.

actual like nitty-gritty you know like it's like in that sense conference proceedings are just a way to communicate to some uh people who are just in the exactly on this technical

for its domain and not best for communication to broader scientific audiences.


SPEAKER_00:
Yep.

Well, kind of on that point, and anyone can raise their hand, I move the slide to the additional information where there's the code and

Maybe just what would you say about the code?

What are the inputs or the outputs?

What would somebody need to run this code or like what could they do with it?

And it's really cool that you did provide all this information, but to kind of bridge that gap that you just mentioned, what does this function do or what do these different scripts do?


SPEAKER_05:
Yeah, so essentially you first need SPM 12.

um is that contains the core functionality for um yeah that it's based on and then these scripts are like additional um are like adaptations of scripts in spm 12. um and the only thing you need to do is run spm mdp pbx emo app um

So that stands for emotions factorized, which is something I worked on at some point.

But yeah, in the rest of the, so let me see.

So that's of course not enough for you to, then you just get,

model that we already have uh rerun to generate the figure yeah the one that's on this slide before so this yeah this one yeah and the one above it so let me just it's not by now it's so long but i would have to

This is cool, but... We're in the process of what I would recommend, actually, in the future.

I mean, I don't... To be honest, I don't like ModLog at all.

The only reason I worked with it was that the existing SPM active inference modeling was part of the existing SPM package.

But there's lots of...

Yeah, important work has been done by Alex Shams and Conor Hines and a few others to transform everything to Python and to do computational optimization while we're at it.

So there's this GitHub page called Inferactively.


SPEAKER_00:
Yep, here we are on it.

P-Y-M-D-P.

Inferactively is the GitHub repo.


SPEAKER_05:
Yes.

So instead of trying to get ModLab running on your PC, I would recommend just working with and then in the future, we'll have a module in there that can do the same as what we did in this paper.

Awesome.

So that's something I think that is much more sustainable for the future.

We're working on integration with GPUs, et cetera, to make things that can be simulated in a scalable way, can integrate with TensorFlow.

So you can even do kind of hybrid modeling with what they call amortized active inference.

So it's like deep learning models that are connected to

components of these Lego boxes that I was describing.

But if you want, I can run you through this MVP in MATLAB.

It's just that I'm not a fan of MATLAB myself, so I think it would be a little bit of a waste of time.


SPEAKER_00:
Yep, let's definitely have a model stream for the Python, because I know that will be something a lot of people are interested in.

Because the MATLAB, it has a certain charm to it, but I can see how you might want to develop

with other approaches, even though when Christopher and Ryan walked us in the model stream through the code, just seeing how the matrices multiply, there's something nice about how it was done in MATLAB, but it's not interfacing with all these modern tools like TensorFlow that you mentioned.

So anyway, but let me see.


SPEAKER_05:
I think we can move on.

But I think what I should do, basically, based on your feedback, is write a little bit of an explanation of what to do with those codes if you want to run it.

Let's say you do want to use mobile.

If you want to run it, what you should do.

That's actually useful feedback.

The reason I added these codes was mostly for...

Yeah, reproducibility, in case other technical folks want to look at what I did.

But it's not really out of the box in a way that anybody not familiar with it could easily use it.


SPEAKER_00:
Yep.

My advisor, she would always say, make it so that the anthropologists from Mars will know what to do with that spreadsheet.

Because even for people in the field or even yourself, months later, it's like, wait, what?

That was my research.


SPEAKER_05:
I mean, the proper practice is to add like a how-to, like a readme page on the GitHub.

So, I mean, that's good that you noticed that.

Yeah.

I guess it's in my mind a kind of transitioning away from a lot.

I kind of felt like, you know, but I do agree with it.

should round up that part of the documentation.

Cool, yeah.

Yeah.


SPEAKER_00:
Blue or Steven, either of you raise your hand.


SPEAKER_03:
Just one point about the effective charge piece that you mentioned before.

Can I just check, is that like an in-between level between one inference layer and the next?

Is that correct?


SPEAKER_05:
That's entirely correct.

Once we go to figure, I think it's figure six.

um let me see here seven seven sorry yep so um essentially now we connect um what's happening in a higher level connected to inferences about our predictions about what's happening on the lower level

And that's when the effective charge basically becomes like an ascending message that informs the inferences.

So there is an equation in the manuscript that I don't see here, because it was a separate table.


SPEAKER_00:
Oh, this part?


SPEAKER_05:
We split figure seven into two.

These are the priors, so this doesn't describe the posterior beliefs.

The effective charge basically comes into... Yeah, I understand there's many figures in the paper, so we didn't include that one.

But if you go to... It's in multiple places.

essentially what we call affected evidence on page, in the files, page 420.

But the actual page is 23, you know, the PDF.

But just for... Here we go.

Yeah, to answer your question.

Yeah, yeah, exactly.

So just to illustrate what's happening or to explain,

And in this paper, we use bars to indicate posterior beliefs.

So here the bar is on the left.

The bar S is like posterior beliefs at time capital T. So this is the cross-trial time steps.

So large, yeah, cross-trial time, essentially.

The superscript here is indicating the affective state.

so posterior state the posterior affective state on the higher level is a soft max function of um it consists of the prior bully which is the logarithm of the previous posterior from the previous time step multiplied by the transition matrix so that's the where the prior comes from and then

This term that you see after that comes from what we call Bayesian model reduction.

And it's what happens when you want to connect discrete and continuous state-space models.

Again, this term drops out of the derivations that you can do based on this formalism, Bayesian model reduction.

this case this sort of pre-energy minimization assuming that changes are small enough locally are small enough to make this connection and then if you ask what's small enough well that's something you would have to test in in the world and see if this approximation holds and essentially what you see here

is the effective charge coming back to haunt you so to speak but now as a message that is passed upwards so the higher level state is consists of two kind of extremes and so the extreme positive side so beta plus and the extreme negative side beta minus and at any point in time the organism is somewhere in between

It's never extreme.

It's never like at the exact extremes.

It's beliefs, just like you can never have 100% certainty about anything.

Because downwards in the hierarchy, we make predictions.

That's where we do Bayesian model averaging.

Upwards, you have to gather these messages.

So that's what I call effective accuracy.

Let's start to use Bayesian model reduction.

Yeah, so that's essentially a very long answer to your question.


SPEAKER_03:
but yeah, no, thanks.

Just, just one other thing with that is this thing gives a way for somatic kind of processes, you know, in every day or even in trauma to be integrated in a way, because it could be that non cognitive processes could be at play in adjusting this effective charge or keeping a score of that.

Yeah.

And that could give a way for that to influence perception or action model selection.


SPEAKER_05:
Yes, and this high-level state is not constrained in any way in terms of what you... You can add other types of evidence from other action models.

Other components of your system can add their own effective charge.

So...

That's at some point in the beginning, we mentioned this effective workspace theory.

So like, um, we call it effective charge because it's essentially a pretty domain general can be gathered from any kind of actual model.

It can also be used to model the way when you're listening to music, your attentional action model has, it's kind of being played around with, um,

with congruence and discongruence.

And you can kind of create mismatches and matches.

You can create fluency and disfluency.

And this all creates some kind of affective roller coaster, so to speak.

So these ups and downs can be gathered from an action model that would be guiding your attention states.

So the question of what to attend to when you're listening to music could be something that generates affective charge and then informs your valence state.

It can also be purely associative.

So it doesn't exclude purely associative types of valence.

So there can be certain things that

comes back in the contextual evidence.

If you go a little bit down.

I think it's up here.

Oh, yeah.

So you can also gather evidence from, in this case, we just talked about contextual states in terms of whether the food is on the left or on the right.

But you can also have contextual evidence being passed back up.

And that can

generates some kind of poplopian learning, where you just learn to associate particular contexts with particular positive states or negative states.

So there's also space to include these kind of

what I think are computationally a little bit less interesting types of valence, valence experience, because they're just kind of happy and learning in a sense, but it doesn't make them less important for experience.

So these purely associative relationships also come... Actually, we discussed that in the... There's a discussion set that's part of that.

and see how far we get today.


SPEAKER_00:
Nice.

I wanted to ask actually about Figure 10.

What were you showing in Figure 10 or how could we read this?

And does it apply to your model only or future models, other categories of models?


SPEAKER_05:
Everything that's happening here from the orange

uh orange level downwards let's say is is what we implemented but everything that's in the grade is basically something with we presupposed so we presupposed for example that this rat already learns how to how the mace works um we presuppose that they have some perceptual capacity that or as in some action control over

let's say, where their body is.

So all the things that we needed to presuppose in order to actually do the simulation demonstration are in the gray part of this box.

But the nice thing is that if you follow the arrows, this gray

part, so evolution, development, and learning, it's also imported in terms of timescales.

And the nice thing is that it's always a circular story.

So you get these different nested timescales that are all influencing each other recursively.

And once you get down to our actual computational experiments,

you are at the point in time where you enter basically on the left, this gray arrow that points into the affected box.

It's kind of you can think of that as the one that initializes the simulation, so to speak.

So this gray arrow is what initialized everything that allowed us to even simulate it.

And then we explore the dynamics of what happens within that.

And all the other arrows are basically illustrating that.

So the orange arrows provide priors of precision for the action model, for the perceptual state.

So that's all the orange arrows.

Then from the minimal meta cognition level, again, there's this arrow pointing down.

and to inform action jointly the effects end up informing perception and then it kind of passes through in the way that you described also last time in the last session and that's where it kind of connects to the world and then it passes back up again

And that's where this kind of perceptual integration happens and it trickles upwards into all these layers.

We could have simulated learning as well.

So that's where this orange arrow points back up on the right to the posterior phenotype.

We didn't simulate learning because there wasn't the focus of the paper.

But we have the machinery.

It's in our Lego box.

And that actually, I think, will make the story much even more interesting because then you can think about active learning and how that influences affective states.

So how people then enjoy learning just for the sake of learning.

or how they can learn to associate affective states and contextual states and how that recursively influences their system.

There's one simulation that I've been preparing where we have a setup like this and we actually simulate the development and learning as well.

And the idea there is that

you have a child, you can simulate a child, and it has these hyperparameters that influence different parts of its lower-level model.

But in the beginning, these hyperparameters are very unstable.

And then there is a kind of parent, simulated parent, that labels the states, like basically kind of reflecting and giving the system labels to work with.

And in the end, the idea here is that the labels end up stabilizing the inferences.

And this is kind of to simulate the idea that, as we know, that social interaction is crucial to develop any kind of emotional control, emotional self-control.

So there's these famous stories about children growing up in the wild or something like that, and then never really reach a level where they can

exert this type of self-control and reflective capacity, communicative capacity that we have.

Anyway, so that's another tangent, a very interesting one.


SPEAKER_00:
Yes, very interesting.

Stephen, and then anyone else with a question?


SPEAKER_03:
And in this diagram, you mentioned minimal metacognition, and then you've got affect and context.

So would that minimal metacognition be the sort of phenomenological consciousness, the kind of awareness that you can't necessarily take a perspective on, and it goes up into affect and context?

And that's where it's consolidated enough to be able to sort of take a perspective on it.

Would that be correct or?


SPEAKER_05:
And that's how I do tend to view it to some extent.

I mean, the actual representation of these precisions is often assumed or hypothesized to be, to occur in a,

in a localized fashion in the brain to some extent in this trade.

That's just a hypothesis that can be tested.

And it's actually an interesting thought that other biological systems can have entirely different kinds of representations for this precision term.

Daniel works with ants colonies.

They might have their own way of encoding this type of reliance on action modes that maybe has to do with weather conditions or something like that.

There can be some kind of shared, very minimal way of encoding that reliance.

Nice.

It doesn't necessarily, it's not, in that sense, pretty agnostic on how it's represented.

And you can just test hypotheses on how it works in real.


SPEAKER_00:
And also, it's really interesting how we can draw out that link to qualitative concepts or to phenomenological experience, but that's not part of this model.

This is a claim about a modeling architecture that lends itself

to certain kinds of calculations that maybe previously would have fallen within the domain of information theory or just control theory or cybernetics or just Bayesian inference or just multi-scale systems modeling.

And I know that those are some of the areas that you've drawn on in your work, Kasper, which is why you could kind of see that there was kind of like one, two, three, but not four, five, six,

for where the active inference model was going, because you even discussed how before some of your recent papers, the active inference model had less temporal depth and less metacognitive depth.

So it's just really cool to hear about how it kind of remaps where the questions and the modeling approaches fit in with respect to maybe previous disciplinary approaches.

So Dean, and then anyone else with a question.

Oh, you're muted, Dean.


SPEAKER_07:
So because you've got the time flow from left to right, and that's pretty much consistent, and you've got the parabolic introduced from top down and then back up,

Can you see, Caspar, the idea that around perception, around action, around minimal metacognition, and around effect and context, there could be a counterclockwise spin?

because I actually saw that in the way that people were working, not even aware of this model, but that that's kind of the direction where you've got the flow across the bottom and then the introduction of the parabola from top down and then back up.

And that's where you could actually get people looking through both directions.

That's why you had the effective valence.

That's part of where the charge came from.

Or am I sort of confusing the matters?

Because when I looked at that figure, I actually saw how people in that flow state were able, what direction the spin was actually going around the middle of your figure.

Hmm.

Hmm.


SPEAKER_05:
Yeah, I mean, this figure is more like, yeah, it's not, it kind of moves away from the very precise mathematical meaning of the other graphs, right, the directly cyclical graphs.

But it's a little bit of the same ideas here, like you said.

In that sense, it doesn't really matter where they're moving.

The reason that I put the upwards arrows on the right is that in temporal sense, it's representing the posteriors.

So you're moving from the prior to the posterior.

But then if you would, because it's nested, every next level means that you already cycled from basically yesterday's posterior is tomorrow's prior, or like yesterday's posterior is today's prior in some way, right?

So the capacity to integrate to the next iteration can also be itself a problem.

So you see that a lot with people with traumas that actually integrate an experience too strongly in the way they work.

And this actually comes to very interesting questions where it's like functional forgetting, something like,

a lot of maybe a lot of these meditative practices actually are able to do it's like help us forget the relevant the irrelevant bits or the things that we shouldn't take to the next integration right um that's sort of the the counterclockwise spin is is just essentially


SPEAKER_07:
making expressing reflection and so sometimes you choose to reflect on something sometimes you over reflect on it which is what you're describing but all i'm saying is is that when i saw this it was it was just more confirmation of yeah i i think it's i think it's correct that's not just one yeah it's a nice idea and how it's uh i mean the reflection part and kind of becoming aware


SPEAKER_05:
of the things that you want to, so how you want to learn, basically.

It's something that we did try to capture.

It's very interesting.

You start to be able to simulate processes like meditation and what's happening in the mind when you develop control on these attentional processes on a higher level.

Anyway.

But I think Stephen wants to say something.


SPEAKER_00:
Yeah, Stephen, go for it.


SPEAKER_03:
Yeah, just following on from that, and what's interesting is how this brings in this idea of affective representation and how we think of representation normally as something, as a thing,

in the head as opposed to maybe objects in the niche which we act on or create or interact with and now you've got an idea in ST of ST2 I suppose of this kind of what could be stored in an accessible form where the prior phenotype may not be entirely accessible except through science

you know, is this effective representation?

I just wondered how you see, because we've had quite a lot of conversations about this, is what do you see representation being in all of this, or the possibility for active inference and representation?


SPEAKER_05:
Yeah, so what I like about this approach is that we basically don't have to pin down exactly

and what the effective state means, as long as we're just trying to figure out how it can be inferred for an organism.

And the organism can use very limited cues to infer that.

And then you have a whole kind of organic

uh states of the system that that will correlate with that inference to some extent but just as the organism is doing is doing that internally um usually we think about this in terms of animals but it's interesting to think about more abstract types of representation and three just in the end the statistical concept so

I work with models where you add a layer on top of this affected contextual layer, so like here, but just in terms of to link it back to verbal expressions.

And if you ask me, what does the representation mean?

In this case, it just has a very specific computational effect on how

think of it as a generative model generative models so each affective state would be correspondent with a certain mode of cognition and action all the way down to the lowest level the model doesn't have to integrate or like specify all of those details as long as you know how let's say the connections

in each level happen, the rest becomes like an emergent whole.

And I think I've tried to be sensitive of the way in which the internal representations are not necessarily the same as the actual affective state if you look at all the different layers combined.

And that's when you think about that, then you start to get into really interesting domains like this disconnectedness between your beliefs about your affective state and the way your system is currently behaving and the different kinds of affective disorders.

You can start thinking about basically hallucinations or like self-reinforced anxiety as in one of those papers.

I guess I don't really have a very concrete answer, because these representations are meant to be abstract until you start to model them in specific contexts.

And I think

Oh, yeah, you want to?


SPEAKER_03:
Yeah, no, I think that that that makes a lot of sense.

And also, if our represents if what I see as being a representation or models that we people think is in people's heads are actually always something in the niche.

that we work with, so to speak.

So what we know is the action model for how to act and perceive to recapitulate.

It just feels like we have that also in the brain, but if it's actually

to some extent, this would be the most concrete model is the emotional imprint that helps then undertake the interaction with the models that we work with.

But those models don't need to be in the brain in that representational form, but you might need some affective

code it possible to know how to and that ties in a lot with some of the work with micro phenomenology and some of the work with um working with mental space psychology so it's quite interesting um yeah so um thanks for this comment yeah i mean um what motivated me actually to move in this direction is something that's not really included in the figure that much here that's super important is the social dimension um


SPEAKER_05:
And then once you think of this affective state, like kind of internal representations that are inferred on any number of cues, any number of arbitrary number of sources of data, so to speak.

You can also think of the facial expressions of my own specifics as being informative, my affective state.

You start having a very natural way of modeling things like contagion.

and just in general this kind of empathic responses where if you're also kind of tracking the group affect and that has places priors on your in your individual experience basically have a very natural way of moving from the connecting kind of the bodily

interaction with the world in the niche, as you described, to this abstract social relations that we're tracking all the time.


SPEAKER_01:
So I'm involved in a project right now that's primate foraging.

And it's really interesting to think about the group dynamic versus seeking out your own epistemic knowledge.

So this is just foraging, and it's an agent-based modeling project.

But I wonder, to what extent, instead of knowing which trees have fruit, for example, if I could just follow a group member.

So I wonder, just what you had said about the social dynamic, is this kind of interchangeable, just following the group?

I mean, we're modeling contagion and so forth.

So following the group, is that interchangeable for epistemic knowledge, really?


SPEAKER_05:
So there's different things you can do.

I mean, one of the ways you could factor that in here, it's like where something that has been worked out in the active inference communities in terms of deontic cues, basically cues that other conspecifics give you to kind of indicate what contexts you are in, and then your action model specifying for this particular context, what is the appropriate action.

So,

a very simple thing can be like in whether you are in a context where you're following another con specific or not.

can be something that I think bees have their kind of dance that they do to indicate whether other bees should be following them.

So there's like a kind of way in which these levels can talk to each other through signaling basically the appropriate what the context is.

And then from that inference about the context, you can have the whole action model kind of rolling out on the lower level.

And I think the following behavior that you described does have a pretty direct analogy to our innate tendency to synchronize affective states with our non-specifics.

Even with other animals, it's actually pretty interesting.

Something that other animals also seem to be able to do, to pick up on nervousness across species.

Stress, markers of stress, we seem to be able to pick up on those across species in a generalized fashion.

It's like an endless number of rabbit holes you can go down.


SPEAKER_00:
Here's one more little rabbit hole.

Looking at this figure, it reminded me of concurrent programming languages like Golang and the idea of having nested processes that were defined by interfaces.

So we've been talking a lot on the interfaces as Markov blankets or holograms with Chris Fields, but interfaces are also programming

patterns that make nested processes that can be effective or tractable or run on distributed computation without halting or spiraling out so uh in addition to the python potentially there could be something like a concurrent implementation and then it relates to this question about representation like is the representation a static object

Is it a dynamic process?

And I don't know what the formal answer is, but it reminds me of what we did talk about with Chris Fields with the types as processes and the choose spaces.

So maybe by defining these interfaces with the right dimensionality or bandwidth or structure, however the screens are defined,

then there will potentially be model ability not philosophical clarity on whether oh is the group is the party conscious or the person or the part of the brain it's like the ant question those are going to be potentially perennial debates but the model ability of the ant colony evacuation is going to be more akin to the model ability of these other higher level processes because of how um abstractly but also excessively defined the interfaces are


SPEAKER_05:
Yeah, I think in a similar way, the way I see these representations, I'm kind of fine with just thinking of them entirely as implicit descriptions of something that implicitly occurs in the system, basically, like a kind of tool, essentially.

And if you make it explicit, it ends up being amenable to modeling.

doesn't mean that there's ever this kind of explicit representation in the system, actually.

All we're doing is making these things explicit so we can actually compute things.

I'm pretty happy with being agnostic about whether there's anything.

I have no doubt in the fight of representationalism versus an activism, I think.

They're both pretty, I mean, they both have points, and I'm pretty happy with the models being agnostic on this question, because models can't speak for themselves in the end.


SPEAKER_00:
Cool.

It's almost like you're putting up your code and your model as an evidence.

And if they want to prosecute the case, if they want to have their debate and debate whether your evidence is supporting their notion or this notion or some future notion, it's a second level question.

And the first level is actually what you're just laying out here and how the variables are linked to each other.

And then there's alternate architectures that are possible.


SPEAKER_05:
Yeah, every model is, in that sense, just a very elaborate hypothesis.

And you can just test it by doing empirical work.

And then every architecture can be compared to each other once you have.

And to do that, you need to go explicit.

And that's the only part, I think, where

we're doing being reductive to make it to get a computational grip on it a scientific grip you could say actually I have been working on getting more observational constraints and these types of things but there's such an amazing

richness of things to explore by just even just capturing the phenomenology that we already have accessible in the first person.

That often you can already get very far by just making sure that whatever your model is going to be, it has to be able to recapitulate

our lived experience that we have directly accessible.

It's actually one of the more recent paper that I could share is from generative models to generative passages.

I don't know if any of you are familiar with that kind of work by Varela.


SPEAKER_00:
if i can ask one question on this figure casper so you mentioned that models are hypotheses which might be really interesting to people because they might think about the model as generating hypotheses which it also can do but actually you're talking about how the architecture and the way that the variables are connected is itself not some authoritative claim or final uh

description of the system but it's a hypothesis so it makes me wonder if somebody says well i think that there's you know letter um you know insert your favorite letter here and i want to hook it up to e i want e to be influenced by a new letter r or something like that is that going to um

keep this nice mathematical tractability?

Are there certain kinds of wires that if you cross them, just the script is going to die?

Is there certain pieces where we know that we can build the Legos really well?

Or are there certain, just how do we know which pieces can be tinkered with and what structural changes could even be done?

Or is it like all by all?


SPEAKER_05:
Well, one of the constraints is this

Basically, the way it works is that you can have only local interactions.

And that's the Markov blanket.

So in the end, the way it's kept at least close to biological plausibility is by asserting that you only specify

local interactions between variables.

And that's what these errors do.

So one kind of forbidden thing to do would be to connect this policy variable directly to the observations.

Because then you're breaking the market.

As the policies are directing in this case, the relations between hidden states, and the hidden states are

generating, or like, based on the hidden states, you're generating observations, predictions about them.

So there are certain kind of forbidden things in these directed acyclic graphs.

That being said, it's pretty universally, I mean, as I said, whenever there's an error, it actually implies bidirectional interactions.

you're completely free to add an high level state that modulates your e matrix so the yeah the sky is the limit so to speak i mean in the end you should be able as long as you keep this marco blanket structure intact so you ensure that it's biologically possible what you're doing um then

You can make any number of states connect to each other.

And if there's redundancy in your model, that should show up because the pre-energy can be decomposed as complexity minus accuracy.

So any increase in the number of states in your model will result in an increase in uncertainty about the parameters.

And that's the complexity.

So like Occam's razor is built in to the way these models work.

So yeah, in some, yes, you're free to do almost anything here.

But if you look at the model evidence in the end, it will punish you if you made your model unnecessarily complex.


SPEAKER_00:
Thanks for this awesome response.

We'll have a question from Dave and then any closing thoughts and then that will be it for 19.1.

So Dave, go for it.


SPEAKER_06:
Yeah, two points about the point of local interactions only.

They're kind of at odds with one another.

First, is that actually the definition of locality?

Whatever interacts immediately

is by definition local second in one of the computer simulations that was put together at the friston lab several years ago the dots on the screen where he's trying to get up uh no i assume a simple model of how market markov blankets work he found that the distant portions of the simulation

were more accurately modeled than the nearby ones.

And in my mathematical illiteracy, I cry out, oh, so it's all working holographically.

The intermediate entities are focusing the more distant ones.


SPEAKER_05:
Yeah, so this is really interesting that you picked up on this, because this is why we kind of start with this marble blanket,

criterion.

But then when you kind of go into the way these dynamics work, it actually gets more interesting once you're able to predict things that are outside of your system.

And the further away, the harder it gets usually.

But like you described in this, so I think you're talking about the Markov blanket, the kind of soup,

emergence and the emergent life paper.

And in that case, for some reason, this also has to do with the way he set up the state space system.

But the way he identified the crop, basically, he did it just in terms of cross correlation.

between the states of these chemical particles and the locations of the particles outside.

And it was an emergent effect.

I'm not sure.

I think it has to do with a particular simulation study.

But if we kind of abstract it a little bit away from that particular simulation, when you go to deep temporal models, actually,

what you're trying to do is you're trying to, you could call it the hologram, I guess, in some way, you're trying to create an image of what the future looks like outside of the current blanket.

And that's why they call these the temporal models in the sense semi-Markovian.

So lots of these interesting cognitive

phenomena actually emerge when you start to hack your way out of your blanket, so to speak.

And you're trying to get a grip on what's happening outside.

And that can be spatially, it can be temporary.

So temporary would be in the future, spatially would be places you can't reach.

So yeah, that's a very interesting question.

I think the semi-marked open aspect

It's something that has been actually discussed recently.

I think that it holds an important key to the way our cognition works is that by kind of absorbing information over time and combining with the right priors and the right, so nature and nurture have to be like kind of coalescing in a way that

gives you a grip on things that are not immediately accessible.

Anyway, so that's a very interesting question that I can't directly answer because we just know that for any physical system, like a biological system, to work, we can only assume local interactions.

And then the rest of the story is trying to figure out how far you can get.

it's just local interactions and to create those what you call holograms i guess you could call them like this is a particular literature you're drawing on you use that term i think chris field actually talked about holographs specifically dave what were you thinking of a holograph okay interesting well i got that from the chapter of mark soln's book that just came out in the last number of weeks


SPEAKER_06:
where he first is talking about actually visiting Carl Friston, but he doesn't use the term holographic.

The only place I've seen that in this kind of context is some discussion of using interferometry to see around corners.

This is something that presumably people can't do, but instruments can, that the

There's a holographic or holograph-like effect that allows either with sound or with light to image objects that can't be seen visibly.

But you have to have a corner.

As long as there's a corner, evidently that induces the kind of self-interference that allows distant perception.

You know, more relevant probably is the way that conspecifics in flocks and so forth convey information, mediated information.

Sorry, I don't have anything more specific than that.


SPEAKER_00:
Awesome, awesome point, Steve.

Thanks for sharing it.

It's time, and that was an awesome point one discussion.

Really appreciate having you on here, Casper, and everyone else who's joining live.

So we kind of just close with little, you know, pause the video or pause time, whatever your affordances.

Think about these questions.

See you.


SPEAKER_05:
Thanks for organizing.


SPEAKER_00:
Yep.

Oh, for sure.

And just fill out the feedback form in the calendar.

Invite.

We'll see you next week.

So just keep thinking about the paper and other topics.

We'll be back for a follow-up discussion on the same paper next week.

So bye, everyone.

Thanks.

Bye.
SPEAKER_00:
Hello and welcome.

It's May 28th, 2024.

We're in Active Livestream 57.1.

Welcome to the Active Inference Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

This is a recorded and archived livestream.

Please provide feedback so we can improve our work.

All backgrounds and perspectives are welcome and will follow video etiquette for livestreams.

Check out ActiveInference.org to learn more about live streams and other projects.

Okay, today in 57.1, we're continuing the discussion of the paper on active data selection and information seeking with Carl Fristen and Thomas Parr joining.

So thank you both.

We did 57.0 covering some of the background and to begin the 57.1, whoever would like to go first, how about Thomas?

lead us into the active data selection setting slash challenge and introduce yourself however you would like thank you though and go for it


SPEAKER_03:
Okay, so to introduce myself, I'm Thomas.

I've been working in active infants for a few years and I split my time between research and clinical practice, which I think is part of the motivation for some of the later parts of this paper that relate to some of the clinical type problems that we might face with it.

I guess the background of this particular paper is that a couple of years ago I was in Australia to present at the Australasian Basin Network Modelling Society, if I've got that right, who ran a really fascinating conference using basin networks to model everything from volcanoes to coastal erosion and various other things.

And there are quite a few clinicians involved in that conference as well and interested in things like respiratory diseases and diagnostics.

And one of the organizers of that conference happened to be running a special issue on basic networks and kind of invited us to take part in it.

And so this was our contribution to that.

And the idea is to take some of the same principles that we use in active inference and some of the long-standing principles from people like to apply to the problem of experimental design and to think about how active inference-like principles can be applied outside of biology.

Sorry, I'm talking too fast for you to type.


SPEAKER_00:
Thank you.

Okay.

Carl, do you want to add anything to that, or as we kind of begin?


SPEAKER_01:
No, no, no, I repeat, I'm an admiring spectator.

Okay, okay.

I enjoy Thomas's lucid account of this very pertinent application of some basic principles of optimal Bayesian design and epistemic foraging.


SPEAKER_00:
okay then no heckling okay let's go to some of chris's questions and also as people ask questions in the live chat we'll go in there um okay so

Let's just start with time sampling.

How did you or in what equations or in what conceptual ways did you bring in time and the kind of time sampling that was explored in this paper?


SPEAKER_03:
So time is a really interesting issue in terms of how you model a lot of these sorts of processes, partly because a lot of interesting processes evolve in time.

but it also brings in some really interesting issues in terms of how you sample and how you gather information about it.

And the most obvious problem there is that some things change and some things don't.

So that brings you into the world of a lot of what psychologists often deal with, things like inhibition of return.

The idea that when you look at something, you learn something about it.

The time it takes for it to be worth looking back there will depend on how unexpectedly things might change in that location.

So that's a really important part of a lot of the sorts of generative models that we deal with both in biology and out of that area, because you need to be able to deal with worlds that change.

And that sort of calls into question, how do you actually approach time in generative models?

There are a whole range of different approaches that one can take to that.

So I think a lot of early work in active inference was formulated in terms of continuous time models, working with differential equations, generalized coordinates of motion, which is a way of representing trajectories in terms of positions, velocities, accelerations, as if you're doing a Taylor series expansion.

and taking the coefficients of that.

And that would be one very good way of representing a temporal progression.

A lot of the more recent work we do in Active Influence is based upon discrete time models, where you see things as being a sequence of events and you just represent what happens at a sequence of different time points.

In this particular paper, we ended up using a slightly different approach and that was to essentially use a basis set or a set of basis functions to allow there to be some smooth progressions in time.

rather than dealing with specific time steps themselves.

It sort of blurs the gap between using discrete time steps where there's a clear boundary on the side of it and continuous progressions where there's no boundaries.

By using a series of basis functions placed at different points in time, you end up getting this sort of smooth progression from one blurry time step to the next.

So I think time is sort of incredibly important in how we think about these sorts of things, again, in terms of thinking about how things change, but also in terms of thinking what's the right way even of representing time in a generative model.


SPEAKER_00:
Awesome.

Welcome, Chris, as well.

This definitely is reminiscent of figure 4.3 from the textbook, which I'll bring up.

How would you contrast it, or which figure should we look at to explore which variables are continuous or discrete?


SPEAKER_03:
This figure is the one, yeah.

So this paper won't have anything specific, but you're right.

There probably is one from the textbook that deals with exactly this distinction.

So here, I think one of the later figures does use a sort of basis set type representation to look at temple progressions.

I can't remember which figure that is, I'm afraid.

OK.


SPEAKER_00:
Did you want to talk through this figure that's up at the moment?

Yeah, maybe summarize this figure.


SPEAKER_03:
So this figure was trying to demonstrate in a sort of relatively simple analytic case what it might mean, and sort of taking time out of it for the moment, what it might mean to select those data points that are going to be most informative about some underlying model.

And the way it's been set up here is to start with, so in the sort of upper left plot here, we're just showing a function with a whole series of different possible parameters that might exist.

So you can imagine these theta parameters that are sort of shown as one being bigger than the next, each represent which function might be used here.

And you can see that in some of those, there's a lot of variation in certain parts of the function.

So the green one, you can see there's a sort of very big peak around zero, but a slightly smaller peak for the purple one, slightly smaller for the yellow.

But actually, there are also certain points where it's not so obvious, and you wouldn't really be able to distinguish it.

So they all more or less cross the x-axis at the same point.

and on both sides of zero.

So if you were to choose somewhere on this plot to work out which function you are sampling from, you're much better off choosing from somewhere around zero than you are choosing at the points where it's actually crossing the x-axis, where all look very, very similar.

And so this is just to get across the intuition behind this process of selecting your data efficiently to be able to resolve between alternative hypotheses the best you can, or even to estimate along some continuous parameter value, as it would be here.

So following the sort of sequence around this graphic, the idea is that we can represent these things in terms of several different kinds of generative models.

So here we're representing either a Bayesian network where you say my explanatory variable is this theta that's generating some data y. And then we have this pi variable, which, like in a lot of active inference notation, represents our policy or our choice of where we want to sample.

So here this is choosing somewhere along the x-axis that we might want to work out what the corresponding y might be.

The factor graph representation deals with exactly the same model, but now expressed by putting the factors in explicitly.

And by factors I mean that our probability distribution, which is made up of our prior beliefs about theta, and our likelihood, which maps from theta to our data y, are now sort of explicitly put in as squares that represent the different bits of that generative model.

And the next step is to use a normal or a fawny factor graph, which essentially just omits the circles.

So now the variables are represented by the edges, the points that connect the different factors.

So you can imagine if we take our probability distributions, the probability of latent states or parameters, and the probability of the data given them, and we separate those out into factors,

And we draw a square for each of the factors of that probability distribution and connect those up whenever they share a variable.

So the edges or the connections between the factors are the random variables we might want to estimate.

And then I'm trying to see behind the cameras.

So the next step is to use the Forney factor graph because it's often quite a nice notation to use when trying to understand message passing and the different things that contribute to the way we estimate these variables.

So here, for each different square, we can see there being messages being passed between the neighboring edges.

And when we combine those edges, we can arrive at a posterior estimate of the marginal of the variable we're dealing with.

So this would be how we might go about estimating the theta there.

But the key twist here is that we're now not just interested in message passing for the process of inference, but we can also now start to interpret those messages

in terms of how they contribute to various different entropies that together make up our information game.

So here we have both a conditional entropy, which we sometimes refer to as an ambiguity, and that conditional entropy essentially says, if I were to sample over here, in a more biological setting, if I were to look over here,

How confident am I in terms of what I would see?

How confident am I that the parameters will map to something meaningful and informative?

The predictive entropy is another form of entropy or uncertainty that essentially says, before I've looked anywhere,

regardless of its parameters, even if I've marginalized them out, how uncertain am I about the data I'm going to get?

So the conditional entropy is how precise is the mapping or how imprecise is the mapping between parameters and data.

Predictive entropy is just my uncertainty a priori about what I'd see if I looked over there.

Those things are quite interpretable when you start thinking in terms of experimental science, because a predictive entropy, if I'm already very certain about what I'd see or what would happen if I performed a particular experiment, there's almost no point in doing that experiment.

There's only a point acting to find something out if I don't already know what I'm going to observe.

So this predictive entropy is a positive thing that if I'm uncertain about what I'd see, then I should go out and seek that out.

I should get those data points because there's something potentially to learn.

However, there are certain circumstances where all you get is noise and you may be completely uncertain about what you're going to get.

just keep running a random number generator over and over again, you're never going to become much more certain about what you're going to see next.

And so that's not a terribly useful thing to seek out.

And that's where this conditional entropy or ambiguity comes in, in that it penalizes those things that are not going to resolve any uncertainty.

So the way I often think about it is if you take predictive entropy, you have the total amount of uncertainty about what I'm going to observe.

If you then subtract the conditional entropy from that, then you're left with the amount of uncertainty that you might be able to resolve.

And I think this comes back in a bit more detail, and we can discuss that a little bit more in some of the later figures if we can get to those.

But essentially the difference between these two things is our expected information gain, which is also a mutual information, which can often be expressed as the difference between those two attributes.

How much am I going to update my beliefs?

How much am I going to change what I believe as a consequence of making a particular observation?


SPEAKER_00:
Thank you, Thomas.

That was a great summary.

I think we'll return to this.

I wanted to bring up figure 4.3 from the textbook.


SPEAKER_03:
Ah, yes, yeah.


SPEAKER_00:
And yes, anything on this and to kind of juxtapose the conditional entropy with the A matrix and the sensemaking and the mapping and then the predictive entropy to the B matrix and the transition among latent states.


SPEAKER_03:
Yes, that's right.

For those who haven't seen these before, these are again factor graph type representations where we're now dealing with temporal progressions.

The graph at the top represents a series of states as they transition from one to the next over time.

there may be more or less uncertainty in those transitions.

So I might be very confident as to how a sequence progresses if there's very low volatility, dynamics are very precise.

And so those are the factors labeled three.

The factors labeled two represent the way in which my data are generated given some states of the world.

So I might not be able to directly observe the S's, but they will give rise to the O's or the observable outcomes.

And exactly as Daniel says, there's a link between the two different terms in this expected information game.

So the conditional entry will depend almost entirely upon this mapping from states to outcomes upon this conditional entry will depend upon this likelihood distribution.

And so it will say that if I'm in this state, then there's a very precise mapping to the data, whereas in alternative states, there might not be.

And so that allows me to select between these things based upon policy pi I choose.

the predictive entropy will actually be a function of both forms of uncertainty.

So uncertainty in the transitions.

So if things change a lot since I last looked there, then my predictive entropy might be very high and I might want to look back there sooner.

Again, in psychological terms, that's a... Sorry, the lights have gone off.

I'll see if I can turn those back on.

So the predictive entropy...

deals with that sort of inhibition of return if it's due to the transition uncertainty.

However, if it's due to the uncertainty in the likelihood, then that will remain relatively fixed over time, and I won't be able to resolve that.

Sorry, let me see if I can sort out those lines.

It's better.

So the figure on the lower part of the screen, so the lower graph, is actually a similar representation, but now in continuous time.

So here we're dealing with the states x might be continuous variables.

The x prime is the rate of change of my state x. The x double prime is the acceleration.

And if you know all of these things and further coordinates of motion,

you can start to construct a trajectory or predict, anticipate both the future or the near future based upon these variables and the recent past as well.

And exactly as Daniel's written there, it's effectively a Taylor series expansion where these are the coefficients of that Taylor series.

So these would be sort of the two ends of the spectrum dealing with sort of a purely discrete time process and a continuous time process.

There are various ways of either blurring those boundaries or even combining the two.


SPEAKER_00:
So maybe now, having brought this out, could you return to the basis sets and comment on how it's similar or different to the time handling in the discrete time or the continuous time settings?


SPEAKER_03:
Yeah, so the use of basis sets will often imply that you're dealing with a continuous function, but rather than have variables that represent either positions and velocities and those things.

I mean, one could argue that actually a generalized coordinates and motion system is a form of basis set in itself.

It's just a polynomial basis set.

So you're just weighting sums of different orders in a polynomial series.

But there are other basis sets that one could use, and those include things like Fourier basis sets or cosine basis sets, where it would be difficult to sort of localize things in a specific point in time, but it's much more frequency based.

or as happens to be done in this particular paper, it's a series of Gaussian basis functions.

And those are centered on different points in time, but because they're Gaussian shaped, the boundaries between them are a little bit blurred.

So the value at any particular point, how much you weight that particular function localized around a specific time point will also have an effect on the neighboring time points.

And that's effectively a way of putting in a prior belief that although there's a temporal progression and perhaps a series of discrete steps, there's actually a smoothness to that as well.

There are no discrete jumps in this, that actually I'm just adding, subtracting functions that vary smoothly to construct that overall picture.

And that gives you quite a flexible way of representing a lot of often quite nonlinear functions.

I think from memory that Arl actually used a sort of basis set type functions in some of the COVID modeling as well in some of the later papers to model things like seasonal fluctuations and changes in transmissibility at the time.

So there's quite a broad way of using this.

The downside of using a lot of those functions is that they're often less mechanistically interpretable, so they're something that are often quite convenient in terms of getting a flexible function representation, but wouldn't necessarily speak to a specific mechanism.


SPEAKER_00:
thank you thomas yeah also basis sets are brought up in spm and i think one advantage that's raised there is how it allows the pre-specification of all of the oscillatory patterns of a given frequency range

and then filtering the data with that pre-specified sieve in a way um interesting and and compressing information about the hypothesis of what structure the data could have like nearby things being more related events in time being more related if they're close and so on


SPEAKER_03:
Yes, and compression is a good point there, that actually as soon as you start using a basis set, you've now got the option to stop after a certain number of functions.

So in generalized coordinates of motion,

there's a limit to how many additional coordinates it's normally useful to put in because the degree of accumulation of the uncertainty means that once you're beyond a certain point, it becomes so uncertain that it doesn't necessarily contribute to the earlier terms of the sequence.

And that sort of arises as a function of autocorrelations over time.

And you're right to say that using things like frequency-based or Fourier-type basis sets also gives you that opportunity for compression.

And I think a lot of image processing does use things like the discrete cosine transform, which is based upon a series of cosine basis sets, where you can throw away certain orders of that or certain frequency bands without losing too much information.


SPEAKER_00:
Awesome.

Okay, let's go to a second question by Chris, but Carl or Chris, just raise your hand or totally go for it.

Let's go to the second one.

Let's ask this question by Upcycle Club.

Do you think that there might be any algorithmic modifications, meta heuristics or hybrid approaches that could potentially lead to even better data selection optimization?


SPEAKER_03:
Probably.

I suppose it's a matter of thinking about what, so hybrid approaches, it's a question of what it's being hybrid with.

And I imagine we may come on to some of those questions later on as well.

So meta heuristics, well, I suppose a heuristic from a basic perspective is just a prior.

And so I think

it's exactly right to say that with the right priors in place for the right sorts of models and the right sorts of settings, this process can be made much more efficient.

So if you know a reasonable amount about the structure of your problem, then you don't necessarily need to rely on basis sets and that sort of thing, but actually often you can write down something with a good deal of information in it already, and then your model doesn't necessarily need to learn all of that.

It's able to then

use the resulting predictive entropies and conditional entropies to pin down the more specific things that you want it to now find out.

So yes, I'm sure that it's probably not going to be a one-size-fits-all in many ways.

The priors are going to be those that are most appropriate to the particular problem you're dealing with, the particular agent you have.

But ultimately, they all, I think, lead

come under the same underlying problem and the same underlying principle, which is that if you select things in order to optimize your model of the world, then it's about finding the best fit with your model of the world.

And that fit would normally be quantified using something like a free energy or a marginal one.

So I think it all comes from the same principle, but if you're a different sort of creature or a different sort of agent, then the implicit model, the way in which that directs your exploration to find out more, to improve your model fit can be very different.


SPEAKER_00:
Chris or Carl want to ask anything or bring up anything?


SPEAKER_02:
Not quite yet.

I'm kind of thinking through, you know, talking about the time series and that sort of thing.

So I think we can kind of keep going on.


SPEAKER_00:
Yeah, Carl.


SPEAKER_01:
Um,

Sorry, I was just thinking about heuristics and what it means to people in a certain kind of economics, something about giga renza and the like, which I just read as priors, the right kind of priors that have

a minimal complexity um but i was just talking my head what is a meta heuristic it's a heuristic next door to a heuristic so daniel or thomas can somebody defy for me hyper prior perhaps yes that's what it must be it must be a hybrid prime that's what i'd assume okay thank you although you know what even is a hyper prior it's effectively just a prior isn't it

Yeah, I suppose it's a prior in the context of a certain kind of hierarchical generative model, absolutely, yeah.


SPEAKER_00:
Also, there's a very practical element, I think, about chapter six of the textbook and the recipe for modeling.

So maybe to kind of bring in this practical element in the modeling of these adaptive data sampling, what is the recipe for active inference generative modeling, maybe even as you've learned or updated in the two years since chapter six?


SPEAKER_03:
I mean, I think the basic recipe is still the same.

So often I think what we do is think about what sort of model that we're interested in and what sort of question that's supposed to address.

and ultimately when you're using modeling in the context of data, the sorts of questions you might want to ask are things like which of several alternative models is the best explanation, so doing a sort of discrete hypothesis test, or trying to actually recover particular parameter estimates, so coming up with posterior distributions over

in an active inference context, that will be over particular kinds of beliefs that then determine behaviour.

And that's particularly important in the context of things like computational phenotyping in clinical populations, so trying to understand which beliefs might be, and by beliefs I mean sort of implicit Bayesian beliefs, not necessarily something explicit that you might be able to articulate.

but what sort of beliefs might vary between different populations that then lead to particular kinds of what we might think of as pathological behavior.

And so you can start to then ask questions in terms of if you can phenotype different people, you can start to use those beliefs almost as if they were data that you can then ask other questions.

So how does this vary with a particular diagnosis of therapeutic intervention?

So

There was a figure in the book, which I can't remember the sequence of now, setting out the sort of sequence that you might follow if you were trying to develop a model and then use that to actually answer a question based upon some empirical data.

But I think the key idea is, are you going to bring that up?

I think the key idea was if you have your model

predict some behavior from it you can then use use that model of that is effectively someone's brains model of the world but then you're modeling someone else modeling the world and here we are yeah so I think in the this one

So the figure you've sort of copied into the lower left there is quite a nice way of thinking about this process in active inference of modelling other people or things doing modelling.

So someone's brain that you're trying to explain is itself modelling the world.

So from that brain's perspective, the data that need to be explained are sensory data, so visual data, somatosensory data, auditory data, whatever.

And then as a scientist, you're trying to explain the way in which that brain is behaving and generating behavioral outputs or electrophysiological signals or whatever you might be measuring as a neuroscientist.

And so my model of this brain's model ends up becoming the key thing.

And that's sort of sketched out in this particular figure.

And the other figure that you had up for a moment, sort of talking through the sequence that one might adopt in terms of how you might construct this.

And it's not necessarily linear, I think, as the arrows show.

But essentially you need to think about what sort of task you want to

form or what experiments you want to form.

You need to think about what model that's optimising.

And here it's showing the partially observed Markov decision process model, which might in this case be the internal model of, as shown here, a rat in a maze.

From that, you then need to construct a likelihood function which says, given that the rat had this sort of model and these parameters in it, what's the likelihood that it would behave in a particular way?

So that p of u is the probability that they would have behaved in a particular way given that model.

And then we might equip that likelihood with some prior beliefs about parameters in that RATS model.

And combining the prior and the likelihood, we can then invert that model in much the same way we do any other sort of Bayesian inference.

And there are further analyses we could then do on that.

So this is what I was sketching out earlier on.

If you have parameter estimates and you know the posteriors for a number of different rats in a maze or patient populations or even artificial systems of some sort, you can then perform other analyses on that almost as if those posteriors were data points themselves.

And you can start to ask, how do these beliefs vary as a function of diagnosis or experimental condition or whatever else?


SPEAKER_00:
thank you okay let's

in the in these beginning parts focus on what the model is and then as we kind of move towards the ends in the second time talking we'll look more at the clinical trial and maybe the questions with um that are more open for the future how about connection with machine learning so chris wrote um

Your examples primarily focus on statistical modeling.

How do you see active data selection methods integrating with other machine learning models, especially deep learning architectures that typically require large data sets?

Are there specific challenges or considerations in this context?


SPEAKER_03:
I mean, I would argue that almost in a sense, the answers in the question that these large data sets that they often seem to require

is exactly the opposite sort of principle as to what we'd be working with in this sort of paper.

So the idea is how do I work with more efficient datasets?

How do I design not just models, but also the selection of data to make sure that even with a smaller dataset, it can be more informative?

So I think there's a bit of a difference in the underlying philosophy, but I would also say that most machine learning models are forms of statistical modeling in some sense, whether or not they explicitly represent the uncertainty that you get out of them.

I mean, I think it is also worth, you know, the comment about hybrid approaches that was discussed in one of the previous questions as well.

One way of thinking about that might be in terms of using things like neural network architectures and function approximators as part of a basinal statistical model.

And that's sometimes referred to as a process of amortization.

You can

fix certain parts of your function approximation.

And in a sense, using the basis sets is also a form of function approximation that then simplifies or in some ways makes it cheaper to do the process next time you need to do that inference that you can sort of save a function that says, given this input, this is the outcome I should end up with, or this is the inference I should arrive at.

One of the challenges there is being able to go in the other direction and to be able to say, if I'm going to actively select data, then I need to be in a position where I can predict or generate the outcomes I might get.

And that might be something that is possible to do in the context of generative approaches in machine learning.

And I suppose all of the approaches we've been discussing really are generative approaches.

But because I can now generate not just the outcome itself, but I can generate a distribution over the outcome conditioned upon what I might do.

That's one of the key things in being able to select those data appropriately.

And if you're using a model where it's very difficult to project that into that space and to ask what are these conditional entropies, then it does become very difficult to select data appropriately.

So using something that's sort of very highly non-linear and difficult to be able to work with those things, it does become more challenging to do that.

Sometimes people might

sort of approach it using sort of sampling-based methods to come up with an empirical distribution.

But by the time you start doing that, you end up with something that is much more costly.

So if the problem you're working with is one where you want to try and bring it down to use fewer data points and select your data more intelligently, then often that does require particular kinds of models.

that let you do that in a very efficient way.

There's an interesting area, and it'd be interesting to see how you might interact with these sorts of approaches.

One thing you might imagine is that actually, if you've got a very clear, unambiguous mapping that you can learn with the deep learning type model,

it might be that you could actually just treat the outcome or the labels that they should produce by that deep learning model as data from the perspective of another model might go about selecting the right sorts of labels that are going to be helpful in terms of learning about things so there may be levels of the model that can be when they're sufficiently unambiguous sufficiently precise mapping where you might not need to go level of detail and

and select things in that way, but it may be that the outputs of that are things that then need to be combined to form interesting mechanistic hypotheses about what happens next.


SPEAKER_00:
That's very interesting like thinking about different kinds of communications and reliabilities of different kinds of analyses or computer systems and then it's actually the active data sampling conditional entropy in practice which is that reliable sources are pursued

as part of the information foraging adaptive play out that every agent has to be involved in like reliability is its own suitability for an information resource

then the other side of it is actually resolving the the variability of the latent states however those may have extremely high fundamental variability so often pursuing the the conditional entropy the agent has more control over because they can resolve something more at least they can resolve that


SPEAKER_03:
Yes, it's knowing what is and isn't resolvable.

I think that that's key.

Because if you were to just seek out things that were very uncertain, you might be sat there pressing the random number generator over and over again without being able to learn anything new from it and attract a state that you end up being stuck in, unless you can contextualize that with how resolvable it is.

I think one of the simulations we put together in this particular paper explicitly varied the uncertainty as a function of where you are along a particular number line.

And that was just to illustrate the point that actually there is a... So it was before this section.

Yes, it would have been this one, that's right.

So here you can see that we're dealing with a function where the variance grows as you move away from about x is zero.

And if you know that, then you know that actually the informativeness of the data at the peripheries is much less because you're not going to be able to resolve a lot of that uncertainty no matter how much you look there.

And so you see in the choices plot in the lower right here, the solution it arrives at looks like a very sensible, deliberate solution, which is to start in the middle where things are most precise, then fan outwards, moving left and right to resolve as much uncertainty as it can before it's then resolved enough that the cost of sampling further data is more than the amount of uncertainty that might be resolvable.

Whereas if the ambiguity of the conditional entropy were consistent across the entire line, then you might expect a much more even pattern of sampling, and something that in practice might not be very different from random sampling, except for the sort of sequential effects.


SPEAKER_00:
I'd love to hear a first pass on how this figure addresses some of the similarities and differences between active data sampling and maximum entropy sampling.


SPEAKER_03:
yes so um i don't know whether you have one of the graphics of one of the previous figures where it doesn't have this ambiguity um but so perfect yeah um so so this figure shows an example where the ambiguity is actually the same everywhere so the way in which the data are generated and you can see that because the

you can see that in the sample data plot, there is a sort of well-defined function, but the amount of variance around that function is more or less constant throughout that entire line, throughout that function.

So in this circumstance, the conditional entropy or ambiguity is the same everywhere.

That means that the only thing that really differs when you're deciding where to look next and where to sample next is actually the predictive entropy.

And so here it will just choose locations about which it's most uncertain.

And that's quite a sensible thing to do because there's nowhere that's limited any more than anywhere else in terms of the amount of resolvable uncertainty.

And so you can see that the prediction it arrives at after making a series of choices, and you can also see the series of choices it makes are fairly well distributed.

They're quite evenly spaced and cover a lot of that ground.

And so the prediction it comes to actually matches that uncertainty quite well in a maximum entropy.

So this is completely identical in this particular setting, the maximum entry approach of selecting those locations that have the maximum predictive entropy.

That's very different to what we were seeing on the previous figure that Daniel was showing, the one where it starts in the center and fans outwards.

Because if you were to adopt a maximum entry approach here,

then you can see that you'd spend much more time sampling in the far peripheries, which is actually not a terribly useful thing to do here because you're going to just get a lot of uncertain data.

You're not going to be able to resolve it very effectively.

And so here the big difference is that ambiguity aversion forces you to start in the least ambiguous places, and only once you've got the most information you can out of that do you start fanning outwards and going for the more uncertain regions either side, where there's actually less information overall that you can gain, because most of the uncertainty is now unresolved.

The other thing that we've done with this plot, and I think the other one as well,

is specify a cost to sampling, a cost to choosing the next data point.

And this is very much like we do in active inference simulations where we might trade off exploration and exploitation, so trade off having preferences for one thing over another.

And this is just to get across that idea that there often are going to be

energetical, computational, or even financial costs to going out and getting more data.

And those costs are specified just as prior beliefs about what I will tend to do in the absence of any potential information that I might gain.

So being a prior belief that can just be added to the log probability of me stopping sampling can just be added to the mutual information or expected information gain, which then gives you an automatic trade-off between exploitation and exploration.

And in fact, it means that if I keep exploring one area exactly, so this C here, so once the expected information gain, once the amount of unserved air can resolve drops below the probability that I would stop sampling, then I will effectively just stop sampling.

So you get an automatic cutoff as to when you want to stop doing your experiment.


SPEAKER_00:
that's a very interesting piece this was this was kind of a cool aspect of it and thank you for that connection to the maximum entry Chris want to ask anything or we can ask a question from live chat or one of your prior questions um


SPEAKER_02:
Not sure it's necessarily a question, but more I appreciate your answer for that last question that we were just discussing, because it makes a lot of sense.

And it almost seems like at some point in more complex models that you can't really

necessarily know exactly what you're feeding, some sort of machine learning or neural net.

You might want to take a hybrid approach where you have something that you select a small amount, select down your sample, your data down to a certain size.

And then you can tweak and optimize your model a little bit more by throwing a couple of random things in there.

But at least you have a smaller set that you're starting from.

I mean, is that something that you can kind of see doing is like instead of trying to sample down to something that's most informative, sample out what is the least informed.


SPEAKER_03:
Yes, and I suppose that's the same sort of approach there, isn't it, as to this starting in the least ambiguous place with the low-hanging fruit and then moving out from there until you feel you're not really gaining anything further.

And I think you're absolutely right, because as long as you're able to specify the cost of carrying on something and seeking out new data, that gives you quite a principled way of working out when you stop doing that.

because a pure maximum entry approach will carry on going because there's lots more entry, there's lots more uncertainty.

And so you might want to resolve that unless there is cost to doing so.

And it's a perfectly reasonable thing to do.

There is no cost to doing so.

You can just keep looking.

But as soon as you're dealing in a realistic setting where actually by doing this, you're not doing something else or you're investing some resource, be that energetic, computational, financial, then taking account of that can be quite a useful way of doing that.

I mean, it's an interesting point about more complex and large models.

And I suppose one of the key points there is that the best models are always going to be the simplest models you can get away with.

So if you have very large models with lots of parameters, the advantage is they're very expressive.

You can fit lots and lots of functions or different patterns of data.

The downside is, first of all, that they become less interpretable, but second of all, that you expose yourself to problems of overfitting.

And one of the key things in the active inference approach is that by minimizing free energy or marginal likelihoods, you're implicitly minimizing complexity.

So the best models will always be the simplest you can get away with.

by get away with i mean as simple as you can get without compromising the accuracy right but that's the key point so sometimes actually there is quite a complex process generating data and you do need a relatively complex model to be able to capture that without compromising the accuracy um so in in that setting it you know it's a really good question how would you go about actually selecting data um in a model that is

sufficiently expressive that it's fit for purpose in a particular domain.

And I think a large part of that is being able to actually quantify the likelihoods and the uncertainties in terms of how that model works.

And that is a challenge in a lot of deep learning based models and neural network based models, but not all of them.

Some of them will explicitly account for these sorts of things as well.


SPEAKER_00:
Carl, yeah, anything, go for it.


SPEAKER_01:
Yeah, no, that was just very nicely expressed, a key issue here.

I was just taking my mind back to when I first learned about active learning as of the kind described by people like David McKay, where the objective was to try and find

the right data features that will resolve the uncertainty in exactly the way that we've been talking about, because we've been talking about it largely in the context of inference.

You're inferring latent states of affairs out there, given some data.

and clearly the original notion of active learning pertains to learning the parameters of a generative model, but you can also apply the same notion to model selection or structural learning.

And that resonates very much with the conversation with Christopher.

One can either look at seeking out

those informative data, the data features that are good and good simply in the sense that they inform your generative model in terms of ultimately maximising the evidence or the marginal likelihood of your generative model.

Or you can look at it as triaging bad data that's uninformative.

You can imagine that process a little bit like Maxwell's demon.

So the data that you don't go and actively seek out, or if you're in a big data setting and you're trying to ingest or simulate large data, what this notion of active learning and active selection brings to the table is that you don't just blindly ingest all the data as current deep learning does.

You carefully assess, should I ingest this data point or not?

um um you know implicit in what tom was thomas was saying possibly explicit but i'll have to ask him um what what's that kind of objective function what is the if you like um another way of talking about the information gain that i would get if i accepted as maxwell's demon this next bit of data and that information gain is just a mutual information

so what you're saying is that you want to only take those data that are literally good because they are informative that will technically or operationally increase the mutual information

of inherent in your model between cause and consequence so i think that you know there's something really quite fundamental about this sort of whole notion of being active in the way that you deal with data and gather information that you know that underwrites active inference as you know um

with its special focus on active sensing and active vision and active inference and active learning, that you can actually go right through to active natural selection.

And one has to wonder now whether there's a homologue in evolutionary thinking of this kind of active learning.


SPEAKER_00:
Thanks for those points.

Those are great.

Yeah, Thomas, go for it.


SPEAKER_03:
I don't think I had anything really to add to that.

It's exactly that.

It's being able to decide also upon your model space, isn't it?

I think it's part of the point that Carl's making there that it's actually, it's the selecting your model, but it's also knowing which models even to select between is another key part of it.

That's certainly been an interesting area in terms of structure learning.

Absolutely.


SPEAKER_02:
I really like the

a call to evolutionary biology, I think that there may be an analog there.

You can kind of wrap your mind around some of the things that you're doing if you frame it from an evolutionary standpoint, too.

It's an alternative way of looking at it, but I think it's quite valid.


SPEAKER_01:
Yeah, also, speaking of those two points there,

they speak to a key issue in structural learning read as Bayesian model selection.

Do you take a top-down approach or do you take a bottom-up?

Do you grow your models carefully by accepting this mutation or this extra state in the factor or indeed an extra level in a deep generative model in the face of this data or this evidence for how good this model is, almost in the sense of adaptive fitness in an evolutionary context.

Or do you use good old Bayesian model reduction and start with something that can do everything and then prune away the bits that you don't want?

I'm just thinking from a biometric point of view or an evolutionary slash developmental, at least neurodevelopmental perspective, so vivo-devo.

it looks as though biology does both doesn't it so we start off with a brain which is over parameterized over wired over connected and then carefully prune away using basic model reduction what we don't want and yet of course at the level of exploring that model hypothesis space i would imagine we rely a lot on

carefully crafted mutations split and merge like operators that gently feel out into different... From that perspective, each of us is a data point for evolution.

That's quite chilling, isn't it?


SPEAKER_00:
A lot to say on this.

One really practical note with the cost of sampling and the opportunity to stop is to have different information foraging locations that have different costs, like different APIs you could call or different costs associated with taking on or holding a record.

So it's really true what multiple of you have alluded to.

it does reframe the question into creating this semantic kernel of understanding for example the clinical trial setting something that can be shown up on a graphical representation and then being able to use that purely in the generative forward direction to generate synthetic data with whatever other evaluative criteria apply to that or just to learn active inference

and then also to use that generativity not just to make more data which is often of limited utility for retraining but actually it includes that adjacency of being able to ask how much another piece of information or evaluating the last incoming piece of information or rank the informativeness

however in a way it moves all the challenge into how you define the generative model surprise surprise because you could have a model of information in the world that would lead to very various uh attractor states repetitive looking fixed looking and so on so

it's it's pretty interesting how i guess that's the high road and the low road kind of coming to meet each other in the information foraging setting because the fundamental constraints and trade-offs associated with sampling they do come into play and it does not solve the question it just sets up some of the

objective functions that can be used and have interpretability for these uncertainty situations okay i'll ask another question from the live chat

All right, Demetrios asks, what is the nature of the precision weighting mechanism in this process?

Is it possible that high precision of the prior blocks the prediction error to convey the newsworthy information?


SPEAKER_03:
so um precision and the balance between priors of likelihoods and what determines which procedure we might use and i suppose relating that explicitly back to this this question of information seeking so um so precision

can be applied to any probability distribution, essentially.

So you could have a precise prior, you could have a precise likelihood, or imprecise versions of either of those.

And this gets particularly interesting and sometimes quite confusing in the context of hierarchical models, where the prior from one level is effectively the likelihood or conditional distribution from the perspective of the level above.

And so you end up with questions, am I increasing prior or am I decreasing the

the effect of the likelihood, and it often depends upon exactly where you are in these models.

But taken to its simplest level, imagine you have a prior likelihood and the data that you're evaluating likelihood in relation to.

The relative precision, and it is the relative precision that's important between the prior likelihood will determine which one of those has more of a sort of final beliefs on your posterior.

We have a very precise prior.

It says, I'm so confident about this that no matter what I see, I'm not going to shift my beliefs.

And relating this back to the concept of information seeking, that would be the situation where we're in a dynamic setting.

I think there's been so little change from the last time I saw something that even if I saw something new, it wouldn't make any difference.

The likelihood, if that is very precise, leads to very low ambiguity.

So if I believe there are very precise regions of space from the perspective of my likelihood or regions of whatever process I'm selecting between the different policies I could adopt, then I'm more likely to select those locations compared to others.

And this is often in the context of neurobiology and psychology.

These are often how people talk about processes like attention.

So being able to say that my likelihood is very precise in the context of spatial attention associated with these particular spatial channels or this area of space, then that means I'm going to update my beliefs much more based upon this thing that I'm paying attention to.

The link back to information seeking is that because I might update my beliefs much more based upon this location in space, I might act in such a way that I get more information from it.

move my eyes to look over there, or take whichever action is relevant for whichever data set I'm dealing with.

And we can think of that as essentially saying this region of space, this region of policy space is more salient than others, because when I choose to do something that leads to that outcome, I'm going to update my beliefs much more as a consequence.

So I think that gives you a brief summary of how priors and likelihoods interact with one another.

The notion of prediction error came up as well, which I think is an important one to think about, particularly when you're dealing with continuous states-based models and perhaps most simply when it's a simple linear Gaussian model.

Because the degree of update that you might make is effectively going to be

a linear error function where that error is the difference between what I predicted and what actually happened.

And that weighting is going to be weighted by the precision of the likelihood.

So the degree of update I'm going to make to my beliefs depends both on the precision

how unambiguous I think it is, and on the amount of discrepancy from what I'd originally predicted.

And that's the same idea that then underwrites a lot of predictive coding and a lot of neurobiological models.

Of course, even that needs to be weighted very carefully by the prior precision and the equivalent errors that you get once you deviate from your priors.

And that probably brings us back to this notion of complexity and the need to keep things simple.

Because one reading of keeping your model simple is that you keep things as close as possible to your priors.

If you have lots of parameters and they're all deviating from their priors quite a lot in order to come up with an explanation, then that's effectively a very complex model, one that I think would be penalized relative to a simpler model where everything stays much closer to the priors you have.


SPEAKER_00:
Thanks.

Let's go a different direction.

There was a very interesting and ambiguous use of the streetlight effect.

So what is the streetlight effects?

And how is it kind of a representative setting?

And what do we do?

Oh, there were so many fun, fun representations.

so what what what is it is it is it a um joke how did you find one how did you find one with a pie oh just typing in different symbols to see underneath the spotlight very good yeah gpt40 um so the the idea of the street light effect comes back to this idea of ambiguity aversion


SPEAKER_03:
And it's an idea that, I can't remember where it first came from, but I first came across it in psychology literature.

I think possibly a reviewer mentioned it in a paper at some stage.

But the streetlight effect is originally set up to be something to describe an unhelpful bias.

And the idea was that if you're sort of coming home late at night and it's a dark street and you drop your keys,

the first place you might look is underneath the streetlight.

And that's unhelpful in the original reading because your keys are not more likely to go into the streetlight than they are anywhere else.

And so it's a metaphor for seeking out those sources of information that are most easy to acquire and that you can learn the most from very quickly.

Of course, from an active inference perspective, that's actually a completely optimal thing to do, that if you

have some uncertainty to resolve, the very first thing you should do is look under the streetlight because then you'll quickly establish that your car keys or your house keys are not there.

And then you might start looking elsewhere.

So another way of putting that is that

The least ambiguous place, the place where there's the clearest mapping between the location of my keys and the visual data I would get is under that streetlight.

So if I'm ambiguity-averse, if I choose those places where that conditional entropy is lowest, then I'll start at the space under the streetlight, and maybe I'll then fan out from there, just as we saw in the simulation results that you put up earlier on.


SPEAKER_00:
yes what a what a comical plot twist that it's like it's a joke because it brings us to the point of laughing at the person looking under the streetlight and then it leaves it open like but what should they have done and then this isn't exactly what they should have done um in this paper but just like you said it's a reasonable starting location especially if it's where one finds one to be

So it's kind of interesting.

Carl or Chris?


SPEAKER_01:
No, I love that.

I haven't heard it phrased like that before.

It's very nice.


SPEAKER_02:
Chris?

Nothing really.

I do like that analogy, though.


SPEAKER_00:
let's how was equation three and this seemingly very general usage of of information gain on uh base graphs how did that come into play and maybe if you could just define how that was used


SPEAKER_03:
So the idea here was that just as in a lot of Bayesian inference problems where you're dealing with relatively complex generative models, appropriately complicated generative models to help you accurately explain data,

then there's often quite a lot of sparsity in those models and there might be multiple different nodes and edges that one might need to take advantage of, or sorry, that represent variables one might want to draw inferences about.

And so there's a long series of papers and advances that have been looking at ways to do that efficiently from an inference perspective.

And that has led to many, many forms of message passing algorithm, relying upon different approximations to posterior probabilities.

Those include sort of, trying to remember what the very early ones were, I think the Baumweltschart algorithm and things like that were sort of early versions of what then ended up becoming fully Bayesian type approaches.

And the more general forms would be things like variational message passing, which relies upon field approximation.

So the idea that I can factorize all my variables or the posterior probability over all the different variables I'm dealing with.

And by not having to deal with all the posterior covariances and conditional dependencies, I can come up with a much more efficient, but still relatively good and useful set of beliefs about the causes of my data.

Then there are more accurate algorithms that were then developed as well.

So things like belief propagation, some product type algorithms, which implicitly do take account of pairwise dependencies between different variables, but still allow you to come up with good marginal inferences.

Often these rely upon subtly different free energy functionals.

So, for instance, belief propagation implicitly comes from something known as the Bethe free energy, which is B-E-T-H-E.

And that effectively assumes that the posterior factorizes in such a way that I can take account of all the pairwise dependencies and then renormalize by all the singleton distributions over each single margin in my system.

And that allows you to achieve exact inference from the perspective of the marginal distributions you get out at the end.

The point of doing all of these sorts of things, and you know, there are many others, expectation propagation and various others that are based upon other free energy approximations, and one in general is a Kikuchi free energy that can be factorized again in different ways.

But the purpose and advantage of doing these things is that it allows us to make use of the conditional dependency structures, the sparsity structures, and the Markov blankets in a network and ask questions about specific variables, solve that in quite an efficient way to be able to say what are the beliefs we have about

The idea behind the way this equation was phrased is that you can actually use those same messages in the same cursive structures where beliefs depend upon the other beliefs in the network and the message passing that results from that to work out the information gain about specific variables.

So you can make use of exactly the same technology that was used in things like belief propagation schemes and which underwrite a lot of neurobiological message passing schemes.

So this equation uses the notation CH to mean children, so the things that depend upon the variable we're interested in.

The parents, which are the things that it depends upon.

And the parents of the children, which you'll see also in a couple of locations here, which

together so the children the parents and the parent the co-parents of the same children together make up the markov blanket which is all the things that um that if i knew about would give me all the information i needed about the thing i'm trying to infer so i only need to know about the local blanket surrounding a particular node and not the rest of the network because if i know about those things that tells me everything i need to know

And so this is just a simple way then of taking advantage of, again, that sparsity to be able to come up with an efficient way of estimating not just the posteriors, but the information gain I could potentially achieve about those posteriors given a particular policy.

Or the information I could gain about those variables with posterior relative to the prior variables.


SPEAKER_00:
So in the textbook there's an example where the generative process and the generative model have the same form.

Like if they're both generating from a Gaussian, then those distributions can truly lock.

But also it's possible to have the form of the generative process and the generative model differ.

So like the generative model is doing some categorical too cold, just right, too hot, and then actually temperature is a continuous variable and so on.

how how does the selection of family of distribution that's even used to relate to the um information gain like in table one uh okay i i think


SPEAKER_03:
I'll just try and unpick possibly two different things there.

So one was this notion of whether the generative model that we might use are prior and could actually match the way in which the world around us generates those data.

And I think that's a really important thing to consider.

And exactly as you say, there are often situations where the best model is, you know,

sort of lay sense the wrong model in that it doesn't match how data are actually generated but is a simpler explanation that it complies with Occam's razor

And the reason for that is you can always generate the same data in a more complex way.

So if you can get down to the simplest way of generating it, that's often the best way.

One of my favorite examples of this was something I found out a while back when we were doing some work on modeling of eye movements.

And one of the things I found is that it's much simpler to model your eyes moving together as if you just had one eye.

and then make predictions from the perspective of both eyes because both eyes move together, excluding sort of subtle things like convergence.

Most of the time, when you want to decide upon a gaze direction, you don't need to decide on separate gaze directions for each eye.

You might if you're a chameleon or another sort of creature that can move their eyes independently.

But from our perspective, actually, you can make do with a generative model that's much simpler than the physics of your eyes.

where actually there's nothing to say that you couldn't rotate two different spheres in completely different directions.

So by doing that, you actually end up with a model that is wrong, but is also a much better model than one that includes lots of additional parameterization and degrees of freedom that are just never used in reality.

So I think that sort of is quite a nice intuitive example of where your model might differ from that in the world.

And then perhaps related to that is the other point you were making about whether the particular distributions in your model are consistent with the distributions in the world.

Or even how do particular kinds of distribution, forgetting whether or not they're accurate models of the world, how do they affect how you then seek information?

And it, of course, there is.

So one of the reasons for including the table that you showed is that the form for the information game will be different depending upon which kind of model you have.

And that's just useful to know in the sense that if you're writing down a model and you want to know an efficient way of sampling from it, it's going to depend upon what form that model has.

So one of the big distinctions that we often see and often make use of is this distinction between categorical models and continuous models.

So the two middle rows here are dealing with categorical or Dirichlet priors.

And they have a particular form in terms of the information gain and the way in which you do that updating.

Pull out of that the ambiguity bits, the predicted entropy bits.

The lowest row, the third row here, shows the formula Gaussian model.

And one of the key things you can see in the Gaussian model is that the covariance predicted, so this sigma subscript y, is the predicted variance for the data.

And so that is going to scale with the ambiguity.

So you can immediately pull out from that how much ambiguity there's going to be just by looking at this term here, exactly what you're highlighting.

Now, if that depends upon pi, then the ambiguity really matters.

If that's constant, as we saw in some of the earlier ones, and it doesn't depend upon pi, then the only thing you need to take account of is actually the first bit, which is the maximum entry bit of these equations.

So the form that you choose for your model is absolutely key in terms of how you then go about sampling the world.

And how you optimize your model is then absolutely dependent upon how you've sampled the world.

So you get this sort of nice circularity that then determines how you build your model based upon what you see, but based upon how your model currently is and what's uncertain about and what there is to resolve, you're going to simply change how you go about sampling it.


SPEAKER_00:
that's a really fascinating point and it makes me think about learning like in a given situation one could over or under learn or however it comes to be that their policies do matter for resolving uncertainty and then that could set up the balance between continuing to sample from distributions and or stopping them


SPEAKER_03:
Yes, that's an interesting thought, because you could start with a model in which you don't know about how your policy affects, for example, this covariance, but it may be that over time you realize that when I do this and end up in this sort of state, actually it does affect it, or when I take a particular action.

And so if you were then optimizing your model, and then you go one step further and you say, well, how do I learn this?

which, or how do I infer which parts of my model will be affected by the actions I take?

And how do I choose the right actions so that I can best learn about which bits of my model will be affected?

And I think that's a really interesting question as to how you might approach that.

So we could have parameterized here, or we could have included additional parameters that are how does the covariance depend upon the policy?

And it might be that if those parameters were all zero, there's no dependence at all.

But as you learn more about it, you might then learn actually there is a function that affects the ambiguity.

And if that's the case, then I will change my behavior quite considerably once I've learned where the least ambiguous places are to go and so on.


SPEAKER_00:
To connect this to language interfaces and chat and just different kinds of...

language models that people can interact with so in human interacting with with other and with how we design these generative models to be this kind of awareness of of what parts could be influenced if i were to prompt this

looking into the how much learning would be expected from a distribution of what responses are being generated from the chatbot so like the distribution that the chatbot actually fleshes out is its irresolvable uncertainty and then the question of which prompt to enter next or which prompt to provide

could be guided by wanting to select the single best or from among the best next policy selections to reduce some uncertainty.

And so understand if it were a deterministic API on the other side, like a chess game, then it would be a simplified decision of what to prompt, like a calculator.

whereas with the probabilistic model there's sometimes small sometimes very large uncertainties on what it actually is out there so sometimes it dwarfs what can be

resolved but we don't necessarily have an awareness or ability to estimate in these different situations what that balance might be however these these methods whether the models generating process is like an active inference model or whatever happens to be a sensor

this can help interoperate the different information resources and patterns by putting them in a common epistemic plus pragmatic plus cost setting yes i think that's right maybe if you've got if you've got a sufficiently expressive model


SPEAKER_03:
and a reasonable degree of uncertainty about that model space, you can actually start to exactly learn how you even go about doing these things as your model becomes more sophisticated of the world.

You learn how to interact with it better in all three of those domains, epistemic, pragmatic, sorry, two domains.

You mentioned three.

What's the third one?

Epistemic, pragmatic, and?

Cost.

Ah, okay, so I normally think of the pragmatic as effectively accomplishing the cost, but yes, yeah, absolutely.


SPEAKER_00:
But the cost playing a special, clear role in this paper with a finite stopping threshold for sampling.


SPEAKER_03:
Yeah.

But in a sense, the cost is affecting the sampling, but it's not sort of just putting a clear time threshold on, I'm going to stop at this stage.

It's something that sort of automatically reweights itself relative to the amount of information that might be left to resolve.


SPEAKER_00:
I think that'll be something very relevant that will be explored.

For example, if there is a value that can be placed on a given piece of information and that could be how much it influences some trade or it could take into account like the cost of paying the employee for that much time or using the computational resources, then it's possible to allocate

fixed or variable costs and stopping times was completely separated from the epistemic and pragmatic generative model itself but with a cost as purely an exit boundary and not be not directly consuming the output of the free energy calculations


SPEAKER_03:
Yes, and that made me think of a couple of things.

One is that one of the advantages of that sort of cost is that in a dynamic environment, it may be appropriate at some points to rest.

And then as uncertainty accumulates beyond a certain point, you might want to go back to sampling.

So if you have a sort of time or cutoff after which you just stop, then that's no good for anything that might have changed.

But if you've stopped for a little while and decided not to put in those costs, you can then decide when's the appropriate place to start, not just to stop.

The other thing I thought was interesting about what you said is, again, this notion that to some extent, although from an active interest perspective and certainly from the perspective of this paper, information in itself is valuable.

And so the

the value of information is in terms of how informative it is, regardless of how it affects what you might gain in the future.

However, how much you might gain in the future may also be relevant in terms of that pragmatic level.

And this brings in some of the ideas that have come through sophisticated inference like schemes, where you start to evaluate these things recursively.

So in a lot of reinforcement learning,

in probably most of it, where there is information gain, it's often seen as the information that will lead me to the result I want, it will lead me to more reward.

And so creatures may learn a path through their environment that is the one that more reliably leads them to reward because it tells them where to go earlier on, something along those lines.

However, one could say that, and that is important, one could also say that it's important to learn about your environment for its own sake as well.

And one of the things active inference does is it doesn't put any special focus on either the fulfillment of your preferences or the seeking of information.

But the interesting connection between the two is that often seeking out information will help you fulfill your preferences in the future.

So if you're able to recursively evaluate, if I knew this, what would I do?

And if I did that, what would I learn?

And if I knew that and keep going into the future, you can also select things that will give you information that will help you satisfy your pragmatic values as well.

So information ends up having a dual purpose, first of all, in being valuable in its own sake, but also being valuable in terms of what else you can gain from it in the future, both in terms of information, but also in terms of the pragmatic value to be able to achieve.


SPEAKER_00:
I think we'll come to that with the pragmatics of epistemic value, but just one note on the message passing.

You mentioned how with the stopping criterion, it's possible to think about the information foraging setting, not just as like where to look every time a metronome clicks, but scheduling bouts, like batching emails or something like that.

And that reminded me of our recent discussion with some of the RxInfer team about the RxEnvironment package, which uses the reactive message passing to uncouple the timing of, for example, the nestmate and the niche in a generative model so that they can be on different

timings and the computations occur on demand like in a node local way and can have uncoupled timings so it's just very interesting how this pathway that is laid out in the paper about how on one hand we have the graphical

semantic layout of what these variables are and then how many preparation stages before it all gets played out as the relationship of the equations that we get from the entropies


SPEAKER_03:
So in a sense, this is sort of setting out the process that someone might go through if they were trying to unpack this sort of modeling.

In practice, you might assume that actually a lot of it just starts from this message parsing stage, and so it's the interaction directly with message parsing and what you sample.

And a lot of the graphics are on top, really, to set out the equivalence between different sorts of formulas in it.

which some people are very familiar with using one sort.

In principle, you could tell this story without referring to a Bayesian network, without referring to a factor graph.

And one of the big focuses really here was just trying to point out the way in which they're interrelated.


SPEAKER_00:
Carl or Christopher, we can go any direction.


SPEAKER_01:
I go in two directions because there are two really interesting issues there.

I can't resist commenting.

Perhaps in reverse order, the notion of reactive message passing, I think, is not only crucial and again very biomimetic, but also very relevant in terms of computer science and efficiency in terms of implementing

um any kind of active inference or learning scheme um it actually inherits from something called the actor model in computer science which is a big thing which led to the development of a whole suite of different programming languages you know one of the most popular nowadays is rust um that is predicated very much on this notion of um responding to queries and

requests for computation and the products of that computation, which fits very, very comfortably with the, you know, the maths of message passing on factor graphs.

So I think the RxInfer reactive message passing is exactly the right way to go, not only in terms of understanding biomimetic dynamics and belief updating, but also in engineering the next generation of deep learning or artificial intelligence of a more generalized sort.

But the other direction I want to do is come back to this, I think, really important discussion about.

Is information is the value of information intrinsic to the information game or is it always conditioned upon your ultimate goal?

I think that's a really kind of profound point, and I have a very specific take on that that inherits from the physics that you get from the free energy principle.

So if you just write down

what is the probability distribution over paths into the future and then you take the logarithm of that probability to create a potential that is the expected free energy so the expected free energy that can be carved into the expected information gain and the expected log prior preference or expected value

falls out of the physics of self-organization and that decomposition tells you that these things are linearly separable it means that there is no value of information about the reward that does not exist because about the reward suggests you can't linearly separate

the expected information came from the expected value so this notion i think that you um only have to learn about that which you need to know in order to get your reward i think is fundamentally wrong and one has to ask where did it come from and i think it's by putting a certain kind of teleology and semantics on the physics of sentience and your sentient behavior

that leads you to that false conclusion.

Because now, what does value mean?

Oh, well, everything has to be in relation to the reward or the goal.

But of course, the value of information is not.

Another way of looking, or if you like, dissolving that sort of disconnect between the intrinsic value of resolving your uncertainty about the world as opposed to the extrinsic value of avoiding expected surprise.

where surprise is read as a sort of negative log of your prior preferences, is just to think about, to acknowledge that we're talking about effectively Lagrangians or potential functions because they're log probabilities.

So when I say you can linearly separate them,

what we are separating in a linear non-interacting way are the log probabilities when you actually combine the two probabilities what that actually means is that i will only go for information gain if

it satisfies my um my goals or equivalently and symmetrically um i am only going to satisfy my goals if i get a sufficiently high information gain so they do with contextualize each other in probability space even though uh they are they don't interact in terms of information or um in terms of potentials or log probabilities the final point is that you know

this non-theological conflation of the value of information and the value of conforming to the constraints supplied by the prior preferences is that that is exactly what you get when acknowledging that the free energy principle is dual to james's principle of constrained maximum entropy so in this instance

you can just rewrite or reinterpret um the free energy principle as an instance of constraint maximum entry principle and you were talking before about you know the benefits of maximum entropy and this is one very simple extent instantiation of it where the entropy in question is the entropy of your posterior or your measurement but it's constrained so now rewards

are just ways of writing down constraints on an information maximizing system or a maximum entropy system and where do those constraints come from they come from the generative model where does the general model what constitutes general model it's just the prior preferences in your generative model from the point of view of the free energy principle

So by, if you like, appealing not just the free energy principle, but Jane's constrained maximum entry principle, mathematically, that speaks to a very, very clear picture that the intrinsic value of information has got nothing to do

with rewards or goals or constraints it's just a way of writing down a constraint and you can actually articulate this in terms of lagrange multipliers where you can either look at the information seeking as constrained by reward or you can look at rewards seeking as constrained by information but in log space they're completely independent

That's what came to mind.

I think it's a really interesting point, which confused me for years and also is a recurrent question when you talk to people in reinforcement learning who have this gambler's fallacy about the relationship between the two kinds of value.


SPEAKER_00:
Thank you, Carl.

That was amazing.

A few notes.

The additivity of the log probabilities is the

in the is the um multiplication of the probabilities non log it's just properties of logs so it's fascinating how they are both cross calibrated and referenced

and decomposed from generalized free energy or expected free energy considerations and in practice there's the parameterization of uh what the actual values are however you have given a very full-throated defense of the pure value of information gained

And that was something in the approach of this paper that Christopher and I had explored a little bit, how it starts with the information gain and then the end of the paper brings in the pragmatic value, as opposed to looking to expand the epistemic scope of reward and reinforcement

and introduce some kind of variability or learning or intelligence layer.

Other than that, starting with the information gain and then tuning the regimes of attention and action from there.

So that's very interesting.

Chris, do you have any other thoughts or directions?


SPEAKER_02:
No, I think that really everything's kind of been captured here very nicely, and I'm still trying to process through some of the things that we've discussed now.

I just really appreciate how the paper's written, how well it has, you know,

mentioning what Daniel said, how it builds from a single kernel of information.

mutual information gain, and then it just builds out a model from there into something that is more and more applicable to different regimes.

So you can kind of stop at any point that you want, and then it just kind of grows into something that is immediately applicable to clinical trials, which is quite wonderful, and I really did appreciate that.

I'm just going to have to sit through and probably process some of the things that we've discussed here today.


SPEAKER_00:
yeah i think that uh let's just in the last minutes think about what we might want to process over the next week and prepare some questions it might be cool to see the code so thomas if you want to have it loaded up or we can uh try to prepare it on one of our machines christopher um we uh yeah or thomas what would you look forward to discussing or doing


SPEAKER_03:
If you'd like to, it's all available on GitHub repository.

And I think it's set up so that if it's run, it will generate a series of animations showing the process of sampling as well.


SPEAKER_00:
so if you could if you could yeah that would be awesome yeah let's um return on that and look at the code what's the easiest thing would you like me to send you something there and

If you could just prepare to share your screen with the code running, and then we can start with just a little bit of looking at it, just seeing what the code does.

And then in the second part, talk more about the clinical trial setting and how it is all brought together in Section 6.

Sounds good.

Cool.

Any last thoughts from any of you?


SPEAKER_02:
None here.


SPEAKER_00:
Thank you, Thomas and Carl.

This is awesome to have you join.

Thank you, Chris, for helping prepare this too.

All right.

See you next time.

Thank you, guys.

Have a good one.

Bye.
[
  {
    "start": 8.013,
    "end": 9.274,
    "text": " All right, hello and welcome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 9.895,
    "end": 17.783,
    "text": "It's June 4th, 2024, 6-4-24, and we are in Active Inference Livestream 57.2.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 19.885,
    "end": 21.627,
    "text": "Welcome to the Active Inference Institute.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 22.208,
    "end": 28.054,
    "text": "We're a participatory online institute that is communicating, learning, and practicing applied active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 28.754,
    "end": 31.117,
    "text": "You can find us at the links on this page.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 32.018,
    "end": 37.243,
    "text": "This is a recorded and archived livestream, so please provide feedback so we can improve our work.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 37.342,
    "end": 41.588,
    "text": " All backgrounds and perspectives are welcome and will follow video etiquette for live streams.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 43.01,
    "end": 46.375,
    "text": "Head over to ActiveInference.org to learn more about projects.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 47.477,
    "end": 63.48,
    "text": "And today in 57.2, we'll jump right into it with a new guest, Frasier, and picking up on some topics that we left from the dot one, plus any other things that come up.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 64.081,
    "end": 67.045,
    "text": "So by way of getting started,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 67.025,
    "end": 68.913,
    "text": " Frasier, do you want to say hello?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 69.094,
    "end": 75.583,
    "text": "And then any recap slash anything that you're thinking about would be exciting to discuss today.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 77.302,
    "end": 77.682,
    "text": " Hello, yes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 77.803,
    "end": 79.024,
    "text": "So my name's Fraser.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 79.124,
    "end": 80.887,
    "text": "I hope everyone can hear me OK.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 81.868,
    "end": 85.753,
    "text": "So I'm a recent computer science honours graduate from the University of Western Australia.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 86.454,
    "end": 94.204,
    "text": "My interests very much lie in applying active inference in sort of in situ, real world, real time processing, and so forth.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 94.785,
    "end": 107.241,
    "text": "So I'm very much interested by the way in which organisms can, well, artificial or otherwise, can use active inference to sort of build an eco niche that affords their adaptivity over time.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 107.221,
    "end": 131.406,
    "text": " of course that's quite a general statement but doing this in real time is quite difficult and if we can get a way to optimize on how we're soliciting data you know soliciting data intelligently and so forth we're going to be able to do that much more effectively than we have in the past so that's a little bit about where my interests lie and i'm particularly interested in the conciliations between this paper and this suite of ideas",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 131.386,
    "end": 157.073,
    "text": " uh with another emerging framework in cognitive science called relevance realization i think there's quite a bit to be had there so that's a little bit about me and my interests so thank you cool thomas what would you say just right off the bat in terms of relevance realization and active data sampling or or what else would you want to sample to know how you could answer i'd want to know more about um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 157.93,
    "end": 160.761,
    "text": " about relevance realization and what that involves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 160.781,
    "end": 162.367,
    "text": "It's not something I have any expertise in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 164.776,
    "end": 165.057,
    "text": "Excellent.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 165.077,
    "end": 167.887,
    "text": "I think we'll be able to have a bit of a back and forth then.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 168.068,
    "end": 169.493,
    "text": "Sounds great.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 170.553,
    "end": 193.675,
    "text": " yeah go for it for sure how would you bring it in and then we'll see where it goes well it is i must stress it is a a an emerging framework there are several papers on precisely named as such you know relevance realization an emerging framework in in uh cognitive science but the general idea and this is very much spearheaded by i would say john viveki of the university of toronto",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 193.655,
    "end": 198.08,
    "text": " and a few other people, Mark Miller in the University of Monash here in Australia.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 199.461,
    "end": 205.568,
    "text": "The central idea is that the ability to zero in on relevant information is the criterion of the cognitive.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 206.208,
    "end": 208.03,
    "text": "So there are cognitive agents.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 208.21,
    "end": 209.131,
    "text": "We are all cognitive agents.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 209.692,
    "end": 211.954,
    "text": "We're trying to make cognitive agents an active inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 211.974,
    "end": 213.156,
    "text": "What exactly does that entail?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 213.356,
    "end": 219.983,
    "text": "The hypothesis is that to the extent that you are a cognitive agent, you're able to zero in on the relevant information.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 219.963,
    "end": 228.333,
    "text": " that is necessary for maintaining your survival across time and your ability to achieve certain goals.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 228.353,
    "end": 232.759,
    "text": "Certain goals that one would expect are constitutive of your autopoiesis as an organism.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 233.62,
    "end": 237.564,
    "text": "And so the question then becomes, okay, well, what precisely does this mean?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 237.604,
    "end": 242.19,
    "text": "How can we cash out what it means to look for relevance in an environment",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 243.233,
    "end": 251.125,
    "text": " Classically, this is going to involve avoiding things that are deleterious to our Markov blanket and seeking out things that are conducive to it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 252.728,
    "end": 255.692,
    "text": "The only other thing I would say, so that's a bit of a broad discussion.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 256.293,
    "end": 266.389,
    "text": "The only other thing I would say is that it's very much stressed in, I would say, the work of Viveki and colleagues that we actually cannot end up with a theory of relevance per se.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 266.538,
    "end": 268.101,
    "text": " Nothing is canonically relevant.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 268.662,
    "end": 277.758,
    "text": "So things might be relevant insofar as they are large, or insofar as they are small, or insofar as they live for a short amount of time, or they are selected or case selected.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 278.579,
    "end": 284.209,
    "text": "There is nothing that is relevant in any absolute sense.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 284.189,
    "end": 286.795,
    "text": " But what we might be able to do is have a theory of relevance realization.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 286.855,
    "end": 294.632,
    "text": "That is to say, the processes and mechanisms and trade-off relationships that allow and afford for the realization of relevance.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 295.674,
    "end": 296.636,
    "text": "But one more thing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 296.796,
    "end": 303.21,
    "text": "The emphasis is on that web realization is quite important.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 303.629,
    "end": 309.577,
    "text": " The framework does not presuppose that relevance is sort of out there in the world and we merely detect relevance.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 310.458,
    "end": 318.589,
    "text": "Neither is it assumed that we simply express something from within our own schema in a sort of romantic idea.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 318.609,
    "end": 319.911,
    "text": "We just sort of express relevance.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 320.332,
    "end": 326.38,
    "text": "Relevance is co-created in the sensor motor loop, the eco niche between the agent and its environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 326.36,
    "end": 332.226,
    "text": " So I mean, all of these ideas are very, very broadly conciliant with the active inference framework more generally.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 332.967,
    "end": 337.031,
    "text": "So I thought this would be a kind of perfect place to bring this other framework in.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 337.392,
    "end": 342.337,
    "text": "I'm sorry, Thomas, that I'm sort of blindsiding you here with respect to this theory.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 343.238,
    "end": 355.371,
    "text": "But I think the general discussion that we will have, even if we don't focus on the particular theory of, say, relevance realization per se, will very much swim in the same sort of waters that I'm interested in navigating.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 355.874,
    "end": 357.277,
    "text": " So I hope that makes some sort of sense.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 359.861,
    "end": 361.424,
    "text": "Absolutely, and thank you for explaining it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 363.187,
    "end": 373.847,
    "text": "The thought that came to my mind is, if you sort of go back a little bit in psychology, what's the relationship between the idea of relevance there and the idea of salience?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 375.49,
    "end": 378.996,
    "text": "Would you say the two are in some way analogous, or is one a subset of the other?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 380.36,
    "end": 391.585,
    "text": " Well, so the framework, as far as I'm familiar with it, the idea is that relevance realization is perhaps",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 391.784,
    "end": 399.811,
    "text": " isomorphic or at least deeply, very much deeply, you know, bedfellows with predictive processing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 400.632,
    "end": 412.243,
    "text": "So the idea would be that the mechanism of, say, precision weighting is very, very much shared by all of the processes that underwrite relevance realisation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 412.263,
    "end": 421.251,
    "text": "So to the extent that predictive processing and precision weighting have things to say about salience as opposed to relevance more generally construed,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 421.501,
    "end": 424.946,
    "text": " I suppose there would not be much of a distinction to be made.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 425.687,
    "end": 441.147,
    "text": "But to try and answer your question, the conciliants are very much between the idea about dynamical self-organisation over sets of trade-off relationships, that's essentially what relevance realisation",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 441.312,
    "end": 443.536,
    "text": " you know, the ontology of elements realization.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 444.298,
    "end": 449.028,
    "text": "The isomorphism is between that and precision weighting and predictive processing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 449.048,
    "end": 452.154,
    "text": "That's the very specific mechanism that is identified.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 454.859,
    "end": 457.725,
    "text": "Okay, so that's an interesting point to sort of consider.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 457.986,
    "end": 461.633,
    "text": "And one of the reasons I think that's quite interesting is to think",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 462.794,
    "end": 466.32,
    "text": " Is there anything intrinsic about relevance that requires action?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 467.061,
    "end": 471.508,
    "text": "Are my actions in a clear way determined by it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 472.089,
    "end": 485.511,
    "text": "Because one can imagine having an attentional system that just precision weights based upon some passive data to work out which stream is going to be most informative in updating beliefs without actually acting in any way to change it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 485.744,
    "end": 495.763,
    "text": " Whereas the concept of salience, I think, goes a little bit beyond that and says, actually, it's not just a matter that I'm estimating how precise the data streams that I'm getting are.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 495.863,
    "end": 497.707,
    "text": "I'm actually choosing those data streams.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 498.809,
    "end": 501.053,
    "text": "Where approximately do you think it would sit?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 501.944,
    "end": 508.293,
    "text": " Okay, in that sort of schema, I would very much say it's within the salience kind of landscape.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 509.515,
    "end": 518.368,
    "text": "The emphasis is very much on the means by which we can adaptively interact with our environment to solicit the right observations that are conducive to our agency.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 519.29,
    "end": 526.2,
    "text": "Do you mind then just, I suppose, riffing on the exact kind of definition of salience?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 526.22,
    "end": 530.466,
    "text": "You've already done it, I suppose, but that might be helpful for this particular discussion as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 531.307,
    "end": 538.034,
    "text": " I think it's an interesting one, and probably if you asked five or six different people, you'd get five or six different definitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 539.235,
    "end": 553.77,
    "text": "And I think, you know, I don't want to put my definition as being in any way superior to anybody else's, but the way I tend to think about it is that attention, as you say, is effectively a process of game control.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 553.79,
    "end": 558.155,
    "text": "It's working out how precise are the different modalities that are available to me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 559.116,
    "end": 560.537,
    "text": "And that can be",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 560.973,
    "end": 590.777,
    "text": " various different levels it can be a sort of spatial channels that i want to emphasize and spatial attention it might be attending to particular features and there are various different versions the distinction i'd make with salience is that i think salience is something that will actually drive action it's the amount normally i think of it in terms of the amount of information gain i would associate with several different actions and so the the degree to which i'm drawn towards doing that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 591.078,
    "end": 594.003,
    "text": " also raises the question then of what do I mean by doing something?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 595.025,
    "end": 615.82,
    "text": "Because I could be doing something by overtly moving my eyes towards it, but we may be able to loop back around and say, well, actually there's also the possibility to do something without making any explicit action, but by choosing to allocate more of my, or choosing a model in which I allocate more precision to a particular channel compared to another.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 616.897,
    "end": 625.065,
    "text": " But I think it relies upon a weighing up of alternatives, that there are several different things I could do here, and a selection between those.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 626.666,
    "end": 646.245,
    "text": "Of course, how I select between different actions will also depend on the precision that I ascribe to different, or the modalities that I might be able to observe following those actions, because if something is more precise, then that also implies that less ambiguous has the potential to allow me to gain more information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 647.508,
    "end": 660.648,
    "text": " So you see immediately why all these concepts end up quite conflated, because they do, quite sensibly in some ways, they do all feed into one another in quite a profound way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 662.351,
    "end": 672.486,
    "text": "But I think it's sometimes useful just to be able to separate out the process of describing precision to something from the process of choosing between several alternatives in order that I can gain the most information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 674.238,
    "end": 682.493,
    "text": " Right, yes, because specifically with that information gain, there is, of course, the perhaps equivocation between, say, which model",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 682.743,
    "end": 688.192,
    "text": " Are we going to deploy to make sense of the sensory errors and so forth?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 688.893,
    "end": 692.199,
    "text": "And then also, exactly what observations are we going to pay attention to?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 692.419,
    "end": 695.825,
    "text": "So that issue is perhaps one that needs to be finessed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 696.746,
    "end": 705.24,
    "text": "And I would say more particularly relevant to realisation is, at least historically, comes from the issue of model selection.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 705.22,
    "end": 721.407,
    "text": " so the the the emphasis was very much on how the framework might be able to address the frame problem in psychology so which you have a particular schema that you're bringing to bear on the way in which you're dynamically self-organizing around your sensory prediction errors that's within a particular model",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 721.387,
    "end": 736.746,
    "text": " um the yeah so historically that's been the focus i would say which would fit more precisely with model selection as opposed to just uh the the key sort of you know base raw salience of any particular observation so",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 737.907,
    "end": 742.055,
    "text": " There's almost a form of covert salience attribution, isn't it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 742.075,
    "end": 752.075,
    "text": "Not necessarily acting in the sense of moving, but acting in a covert mental action to select between some alternatives at that level.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 752.095,
    "end": 757.606,
    "text": "Yes, yes, there is an action in terms of which model is deployed in that respect.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 758.066,
    "end": 759.409,
    "text": "It is very much...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 759.389,
    "end": 767.161,
    "text": " you know, action of the prosaic kind of the deflationary kind, as Carl might say, you know, moving your finger or saccading your eye and so forth.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 767.181,
    "end": 771.928,
    "text": "That is that is definitely part of very much included in the relevance realization framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 771.948,
    "end": 782.424,
    "text": "But yeah, it does go all the way up to and is largely concerned with the question of model selection as we might pose in a machine learning context.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 783.846,
    "end": 785.368,
    "text": "This and I think that's incredible.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 785.388,
    "end": 787.371,
    "text": "Sorry, Dan, do you want to go ahead?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 788.465,
    "end": 802.881,
    "text": " I was just going to say, I think that's a really nice perspective, really important perspective to bring in in this particular discussion, sort of around themes that we've been discussing over the last couple of live streams as well, about the importance of selecting your data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 803.642,
    "end": 808.787,
    "text": "And that only comes about when you're capable of a particular kind of action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 809.848,
    "end": 813.572,
    "text": "And it's worth thinking about why particular kinds of action like this are so important.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 813.893,
    "end": 816.936,
    "text": "And one of the reasons may just be a matter of efficiency.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 817.675,
    "end": 823.805,
    "text": " But if you can select relatively small portions of the data that are going to be relevant, then you don't have to process everything at once.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 825.568,
    "end": 832.098,
    "text": "Arguably, that's the reason we don't have many, many eyes pointing in all different directions, because we don't need to.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 832.158,
    "end": 840.612,
    "text": "We just need something with a very high focus at a foveal level that we can use to specifically select the information we want to process at any one point in time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 846.16,
    "end": 855.95,
    "text": " On this salience question, it reminds me of the classic two options to dissipate the free energy with change the mind, change the world.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 856.711,
    "end": 864.859,
    "text": "So in terms of changing the world, or at least what it hands to you, there's the overt choice of which stream of data to sample from.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 865.36,
    "end": 866.641,
    "text": "That's like moving the eyes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 867.482,
    "end": 875.07,
    "text": "And then on the change the mind, there's the inner salience landscape, inner relevance realization.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 875.101,
    "end": 888.583,
    "text": " that given a fixed stream of observations still enables a degree of freedom in selecting models that provide epistemic value.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 890.426,
    "end": 893.771,
    "text": "What is the pragmatic counterpart to salience?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 894.713,
    "end": 897.197,
    "text": "Expected utility associated with an action?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 899.941,
    "end": 900.923,
    "text": "Yeah, I think that's probably...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 902.236,
    "end": 910.767,
    "text": " Again, it comes down to lots of different words, possibly with different meanings, but also lots of the same meanings that have lots of different words.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 911.007,
    "end": 920.419,
    "text": "And so you're probably right to say pragmatic value, expected utility, expected reward in some communities, the negative cost.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 921.3,
    "end": 925.746,
    "text": "There are all sorts of words you could use that more or less mean the same thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 925.726,
    "end": 931.054,
    "text": " And what that means is just a description of something being attractive or aversive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 931.755,
    "end": 946.096,
    "text": "And even that is itself a little bit tautological because it's just saying my description for why a particular behavior has occurred is that there is something that this particular behavior or that will bias behavior towards or away from it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 947.558,
    "end": 955.429,
    "text": "So it's almost quite a simple idea, which is that behavior depends upon attractive or aversive things",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 956.118,
    "end": 960.854,
    "text": " But in a sense, the reason they're defined as attractive or aversive is entirely in relation to the behaviours they elicit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 967.055,
    "end": 967.697,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 969.297,
    "end": 978.149,
    "text": " I think this also relates to the first principles partitioning of pragmatic and epistemic value and active inference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 978.169,
    "end": 996.975,
    "text": "So if we were dealing with a call with a purely cost based data sampling method, so something different than the approach taking this paper, then we might find a way to add in different epistemic features or heuristics and make a cost objective",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 998.137,
    "end": 1022.254,
    "text": " that could encompass various elements of like novelty and so on however we would still only end up with one monolithic cost-based still pragmatic objective whereas the approach taken in this paper was to actually start with the cost-free purely epistemic elements of information gain",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1022.352,
    "end": 1024.815,
    "text": " And then from there, cost can be brought in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1025.376,
    "end": 1034.068,
    "text": "But then it is clearer how the costs associated with sampling are not of the kinds that the epistemic gain is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1034.589,
    "end": 1047.047,
    "text": "Whereas if you start with a cost reward, pragmatic utility, etc, based model, then the only real method is to bring in the costs or values of information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1047.768,
    "end": 1050.992,
    "text": "But then that ends up all kind of dumping into the same silo.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1051.175,
    "end": 1053.559,
    "text": " and it can't be brought out back later.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1056.723,
    "end": 1078.896,
    "text": "Yes, and it's interesting thinking about those two different strands, partly just historically thinking you could regard active inference and the approach that we sort of outlined in this paper as being a point of convergence between several different strands of what's been going on for many, many years before then.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1079.577,
    "end": 1080.338,
    "text": "So there's a",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1080.807,
    "end": 1095.79,
    "text": " lots of people would think of it in terms of how things like expected utility theory or reinforcement learning or those approaches that are based upon the idea of a cost might have now incorporated information gain like terms into those cost functions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1097.313,
    "end": 1108.35,
    "text": "But the other way of thinking about it is that there's a long tradition in things like Bayesian experimental design and experimental design, so much more from the question of how do we best design experiments.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1109.123,
    "end": 1116.892,
    "text": " or even feature selection from particular data sets to try and work out how best to draw inferences from it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1117.753,
    "end": 1131.368,
    "text": "And you could argue that those sorts of entirely reward cost-free approaches that have then been based upon how do I get the most information out of these things are now being equipped with a cost function",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1132.58,
    "end": 1134.763,
    "text": " So you could see it from either perspective.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1134.783,
    "end": 1137.988,
    "text": "And really, this is one way you could go from either way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1138.028,
    "end": 1145.618,
    "text": "You could either say, am I trying to equip my cost with information gain, or am I trying to equip my information gain with the cost of sampling?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1152.568,
    "end": 1154.411,
    "text": "Yes, meeting in the middle.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1156.093,
    "end": 1161.861,
    "text": "But it really does depend where it starts from.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1162.904,
    "end": 1166.829,
    "text": " Chris, want to add anything or we can continue?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1168.932,
    "end": 1172.357,
    "text": "I think, I don't know, can you guys hear me?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1172.377,
    "end": 1172.597,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1173.879,
    "end": 1187.478,
    "text": "What I really appreciate about kind of the cost on that topic in this aspect is that it is a very useful sense of when do we not care about what we're doing?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1188.479,
    "end": 1192.084,
    "text": "At what point do we say, hey, we've made enough, it's good enough,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1192.469,
    "end": 1203.383,
    "text": " And I think it's parallel to a lot of the costs that we have in other models, but it's also its own unique thing where you're just saying, hey, we don't need to sample more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1203.444,
    "end": 1207.93,
    "text": "We don't need to go with the full factorial sort of sampling design.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1208.45,
    "end": 1218.704,
    "text": "You've gotten to the point that we are satisfied with the information that we have gained here, and we can move on with the next steps of actually using that data in some way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1220.246,
    "end": 1221.868,
    "text": "What's really nice about it is that it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1222.456,
    "end": 1238.579,
    "text": " So if you equip the cost to your situation appropriately, you can really reduce sort of the time it takes to sift through large amounts of data or kind of through the monetary cost of actually collecting that data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1240.122,
    "end": 1250.016,
    "text": "Because that's, we're in a big data world, but still a lot of areas that just don't have a lot of big data that you can",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1250.384,
    "end": 1274.873,
    "text": " actually utilize you have to collect it yourself and so you're actually showing the concept of cost data sense to a literal monetary value i think that makes it infinitely usable in industry spaces so",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1281.569,
    "end": 1287.456,
    "text": " Pretty sure, Thomas, you want to add anything, or we can move on to some of the pieces that we didn't get to in the .1?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1289.599,
    "end": 1295.166,
    "text": "I have a few questions of my own, but I would very much like to get on to some of the things we didn't do in the .1 just initially.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1295.526,
    "end": 1295.767,
    "text": "Okay.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1296.247,
    "end": 1309.804,
    "text": "I think the main area that we held off for, so maybe we can cover it and then sort of just in the openness see where it goes, was Section 6...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1309.784,
    "end": 1338.357,
    "text": " so for review in the dot one other than carl's amazing exposition and defense of epistemic value we walked through a few of the fundamental formalisms and got essentially on up through figure five with a dynamically changing setting and spoke a little bit about the uh the similarities and differences with the maximum entropy based sampling",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1341.492,
    "end": 1357.83,
    "text": " then several pieces come into play in section six um they close section five saying we address next we address active sampling in a situation where analytical solutions are no longer possible",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1358.013,
    "end": 1380.122,
    "text": " so there's a few things happening first is that the the loss of the analytic character where the generative process and the generative function have the same form that's what is going to motivate this kind of variational approximation and then also there's the sheer fact of bringing in the cost and the relevance",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1380.102,
    "end": 1410.042,
    "text": " of the clinical trial setting so maybe to kind of set up this example thomas you're a clinician what did you bring in or how did you bring together this research section and ground it in what you felt was going to be helpful and meaningful for clinical settings well i think you've you've highlighted some of the key sentences already",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1410.224,
    "end": 1426.377,
    "text": " And that's the idea that when we're, as a theoretician often, the sorts of simulations that I'll end up building are those that work relatively well in an analytic setting or where a lot of the problems are relatively easy to solve.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1427.082,
    "end": 1433.172,
    "text": " And that's certainly the case for some of the earlier simulations in April.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1434.374,
    "end": 1447.116,
    "text": "However, for anything to be practically useful, you often need to go far beyond those domains and you need to work with problems that are sometimes not linear and don't have clear analytic ways of solving them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1448.378,
    "end": 1457.611,
    "text": " But it was also trying to think what's a good example of something that's a bit more realistic, a bit more like what people might do in practice.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1459.694,
    "end": 1469.948,
    "text": "The other part of it is trying to think what's a really good example of something where there's a really obvious cost, but also it's really important to gain information about it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1470.569,
    "end": 1485.814,
    "text": " And clinical trials are quite an obvious example when you start thinking about it in those terms, where there is some uncertainty about the benefit of a particular therapeutic intervention, whether that be a new drug or surgical procedure or whatever else.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1486.936,
    "end": 1493.387,
    "text": "And if you knew the answer to this, if you knew whether it was effective or not, or even harmful,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1494.11,
    "end": 1503.347,
    "text": " that could make a big difference in terms of your cost function because you can benefit some people relative to if you haven't offered them that intervention.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1506.312,
    "end": 1512.523,
    "text": "So here there's this sort of interesting combination of those two things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1512.976,
    "end": 1536.047,
    "text": " And you think about how a lot of these trials work in practice, and in some cases they end up stopping early, either because they've just become so convinced of the benefit of the effect that it seems unethical now to have a control arm where you don't give people a treatment, or if they've started to become convinced that the intervention is harmful and therefore the trial should be stopped so as not to cause any more harm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1536.82,
    "end": 1542.509,
    "text": " So there's already implicitly a lot of these evaluations and these trade-offs going into this domain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1543.45,
    "end": 1562.44,
    "text": "And so I thought it'd be really interesting to try and unpack that explicitly and formally to see how we might start to deal with this in a simulated context, but in one that could be applied in quite an important real world question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1567.381,
    "end": 1584.9,
    "text": " okay so here in 14 we have delightfully stated the whole joint distribution so maybe could you summarize equation 14 here yes so um",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1585.302,
    "end": 1602.096,
    "text": " Equation 14 was a way of writing down something very much like a Kaplan-Meier-type survival analysis function, but written down in terms of a Bayesian generative model, and something that could actually generate some data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1602.329,
    "end": 1611.531,
    "text": " So these sorts of survival analyses essentially look at the time or look at the probability over time that someone has experienced an event.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1611.551,
    "end": 1619.41,
    "text": "And the word survival would come if that event is death, that we're looking effectively at how many people have survived over time and the shape of that function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1619.71,
    "end": 1631.122,
    "text": " And you can compare that with different function shapes to see the ratios between which intervention led to greater survival.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1632.504,
    "end": 1635.187,
    "text": "It doesn't necessarily have to be death.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1638.11,
    "end": 1640.012,
    "text": "It could be acquisition of a particular symptom.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1640.052,
    "end": 1641.173,
    "text": "It might be a positive thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1641.193,
    "end": 1649.042,
    "text": "So you might actually want to flip it the other way around at the time until some recovery threshold has been met.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1649.208,
    "end": 1653.798,
    "text": " But essentially you've got people moving from one category into another.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1653.818,
    "end": 1658.829,
    "text": "And traditionally in a survival analysis, that would be from alive to dead.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1659.089,
    "end": 1661.554,
    "text": "And you're trying to then map that out.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1661.675,
    "end": 1666.525,
    "text": "But again, it doesn't necessarily have to be so much, but can even be a positive category move.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1668.192,
    "end": 1681.675,
    "text": " So the way this is constructed here, I think our Y variable is the, let me try and remind myself, it's a while since I've looked at this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1682.396,
    "end": 1690.229,
    "text": "So the Y variable will be whether or not, or depending on how many people you're looking at, the proportion of people who survived a particular time point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1692.152,
    "end": 1694.436,
    "text": "The pi",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1694.855,
    "end": 1703.732,
    "text": " The tau variable that's underlined in blue is going to be the time we've chosen to sample to check how many people have survived in this analysis.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1705.034,
    "end": 1710.665,
    "text": "And then we have a number of other parameters that are going to be potentially relevant in explaining that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1713.53,
    "end": 1717.217,
    "text": "The theta variable here",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1717.585,
    "end": 1725.018,
    "text": " which is underlined in purple, is going to be the effect of our intervention or anything else that might be relevant.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1726.02,
    "end": 1740.505,
    "text": "And you can see that's making use of a design matrix, which is this X on the lowest row, the columns of which are each going to represent different sorts of potential explanatory variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1741.328,
    "end": 1743.974,
    "text": " Stop me if I'm jumping around too much as I do this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1744.796,
    "end": 1755.0,
    "text": "But you can imagine this x or the multiplication between x and theta as being very much like a standard general linear equation of exactly the kind that you might use in a regression analysis.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1755.976,
    "end": 1775.181,
    "text": " The phi variable is a set of temporal basis functions, so the point of this is that we can now start to estimate a time-varying survival function, so something that is a bit more interesting than just a simple sort of exponential decay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1775.201,
    "end": 1780.988,
    "text": "And the relevance of that might be time since a particular intervention, there might actually be variations in terms of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1782.453,
    "end": 1789.724,
    "text": " survive or experience a particular outcome as a consequence of the time since an intervention was put in place.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1790.204,
    "end": 1795.332,
    "text": "It might work for a short period of time and then stop working, for instance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1795.532,
    "end": 1801.241,
    "text": "Then I think the D and V here are additional explanatory variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1801.641,
    "end": 1806.148,
    "text": "So V is the randomization ratio we choose, or sorry, which",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1806.533,
    "end": 1832.558,
    "text": " group someone was randomized to categorical variable and that's chosen by our variable pi r so pi r is going to be the um whether we've chosen to randomize uh in various different proportions so i think here the options were to randomize you know a one to two a one to one or a two to one ratio between placebo group and an interventional group",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1833.567,
    "end": 1843.358,
    "text": " So depending on those, we'll sample the value for v, and that will tell us which group somebody has ended up in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1843.625,
    "end": 1847.589,
    "text": " The row variable here is just giving the survival function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1847.85,
    "end": 1853.576,
    "text": "So this is effectively just a sigmoidal function at each point in time saying whether somebody survived up until that time point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1854.397,
    "end": 1858.101,
    "text": "And you can see that that's the product of that over a series of discrete time points.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1858.501,
    "end": 1868.012,
    "text": "And the reason for that is that we're effectively saying if I have survived one time point, then I'm in the cohort who may or may not survive to the next time point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1868.178,
    "end": 1893.375,
    "text": " then i'm in the cohort of people who may have to go to the next time point which is why it's that product over time so cumulatively the number of people who have not experienced the outcome should drop at each point in time uh and again the sigmoidal function at each time point is just to make sure that that's in between zero and one so that one would be everybody",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1893.946,
    "end": 1900.535,
    "text": " avoids the outcome and zero would be everybody experiences the outcome from one category to the next.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1902.257,
    "end": 1919.059,
    "text": "The key thing to bring out of this is that the two choices I can make represented by pi as to how I sample my data are when I choose to follow someone up and what randomization ratio I choose.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1920.558,
    "end": 1937.613,
    "text": " And each of these are going to be the things that are subject to our constraints based upon our pragmatic value of cost, which here is going to be the want for people to survive or to avoid the negative outcome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1938.214,
    "end": 1948.523,
    "text": "And the randomization ratio will also affect that same, and the information gain that I'm going to achieve by doing this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1950.055,
    "end": 1965.011,
    "text": " And so the idea here was that we will carry on gaining information about the shape of these survival functions until the cost of gaining more information or the benefit of gaining more information is less than the cost of randomizing to the wrong group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1968.435,
    "end": 1968.815,
    "text": "Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1969.236,
    "end": 1969.676,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1970.617,
    "end": 1979.387,
    "text": "So we see it in its joint distributions specification form in equation 14 and then figure nine, the top with the graphical form.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1980.228,
    "end": 1986.138,
    "text": " Okay, so then let's briefly walk on through to the figure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1987.179,
    "end": 1998.177,
    "text": "We then have equation 15, 16, 17 giving details about how the variational Laplace is being applied to the joint distribution before.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1998.438,
    "end": 1999.84,
    "text": "Anything to add on this?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2002.267,
    "end": 2014.521,
    "text": " I don't know how deeply you want to go into it, but essentially the importance of these things is just that we're now having to deal with approximations because the form of the generation model on the previous slide was quite non-linear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2014.822,
    "end": 2018.467,
    "text": " it becomes much more difficult to solve in an analytic way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2019.569,
    "end": 2030.365,
    "text": "So what we can do is make use of various components of an expansion around different gradients and Hessians.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2030.425,
    "end": 2033.269,
    "text": "So this is looking up to second order.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2033.249,
    "end": 2060.522,
    "text": " um and that's quite useful in that those same quantities are the quantities that come up in um i think the lowest row here is effectively the penultimate row is a mutant scheme to optimize a variational free energy and a variation for energy under a class approximation requires exactly these gradients and hessians so first and second derivatives in order to be able to this sort of problem",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2061.498,
    "end": 2077.887,
    "text": " Cool, yeah, there's probably a lot to say on the math, but one part that I always find fascinating is however these distributions are shaped, whether they're with a central tendency like a Gaussian or whether they have some other kind of structure,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2077.867,
    "end": 2084.437,
    "text": " the variational Laplace is going to fit a parabola basically facing down over the central tendency.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2084.477,
    "end": 2093.632,
    "text": "And so there's places where that's misleading or not, but it's just so interesting to see here how you stepped through and we can still trace out what those variables mean.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2094.193,
    "end": 2097.698,
    "text": "And then by looking at the properties",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2098.91,
    "end": 2109.169,
    "text": " of the distributions as specified, it's like there's this incrementsable optimization scheme that will put a central tendency on the distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2109.911,
    "end": 2119.99,
    "text": "So it's really getting at what the variational approximation is, which is there's some family of function that maybe is",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2121.168,
    "end": 2135.386,
    "text": " completely intractable, but then we can choose to treat the distributions in a way where it is possible for us to shift and scale and tune an upside down parabola, and then that has a well-behaved property.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2137.249,
    "end": 2137.729,
    "text": "So it's cool.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2138.791,
    "end": 2140.873,
    "text": "Okay, and then we get- Yeah, go ahead.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2141.093,
    "end": 2144.678,
    "text": "Can I just ask, sorry, on those approximations there,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2145.063,
    "end": 2150.935,
    "text": " Is it the case that with respect to this approximation that we assume the distribution is quite tightly peaked about the mean?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2151.617,
    "end": 2154.483,
    "text": "So we have quite a precise approximate posterior?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2154.503,
    "end": 2156.748,
    "text": "Is that the case?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2157.108,
    "end": 2160.516,
    "text": "No, you don't need to assume that it's tightly peaked.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2160.816,
    "end": 2163.983,
    "text": "You just have to assume that there's a reasonable approximation around that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2165.819,
    "end": 2166.16,
    "text": " fact.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2166.28,
    "end": 2168.403,
    "text": "I'm trying to see whether it's obvious from these equations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2170.247,
    "end": 2175.656,
    "text": "It's a slightly different form to what I'm used to, but I just wondered if that was the case, if the answer is no then, sir?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2176.397,
    "end": 2188.418,
    "text": "No, so the reason that sometimes comes up is that if you are trying to derive a variation of a plus procedure, often part of that procedure is saying that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2189.191,
    "end": 2195.857,
    "text": " at some point you effectively say that all I need to do is work directly with the mode rather than working with both the mode and the variance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2196.198,
    "end": 2208.99,
    "text": "And the reason for that is that the variance or covariance in the posterior should just be, well, not necessarily an analytic, but a function of the mode in relation to the probability density you have.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2210.111,
    "end": 2217.038,
    "text": "And that's evident in the lower part of the final equation here, so precision here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2217.592,
    "end": 2221.317,
    "text": " is dependent directly on the Hessian evaluated at the mode.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2222.318,
    "end": 2227.966,
    "text": "That's why we're looking at the maximum iteration where we've reached hopefully the mode of that distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2229.228,
    "end": 2246.451,
    "text": "Now, it sometimes seems that to go from a free energy function where we're taking an expectation under the posterior to one where we can effectively neglect the variance to just deal with the mode,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2246.431,
    "end": 2257.124,
    "text": " One way you can think about that is if I were to assume that the mode is so sharply peaked that there is no variance to deal with, and then the expectation just becomes a delta function you can just evaluate directly at the mode.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2258.686,
    "end": 2264.492,
    "text": "And that certainly will get you some very similar equations, and you'll end up sort of arriving at a map estimate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2265.574,
    "end": 2274.244,
    "text": "The other approach to this is to say that if I'm assuming that the posterior is approximately Gaussian, in other words that the log",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2274.798,
    "end": 2295.793,
    "text": " probability of my data and explanatory variables are approximately quadratic, then it means that expanding up to a second order term, so making a Taylor series approximation up to the quadratic element, I can assume that that quadratic element is effectively constant in the region I'm dealing with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2297.495,
    "end": 2311.048,
    "text": " That quadratic bit is effectively our negative posterior precision, as you can see in the final plot here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2312.159,
    "end": 2332.502,
    "text": " So the point I'm trying to make here is that although one intuitive way of deriving some of these same quantities is to say, well, let's throw away the variance and imagine we're just doing a map estimate, a maximum a posteriori estimate, then you'll arrive at very similar modes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2333.463,
    "end": 2337.728,
    "text": "However, the variance and the posteriori covariance can actually be recovered.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2338.535,
    "end": 2346.004,
    "text": " under this assumption that you're treating it as being approximately Gaussian and at least close to the mode around which you're estimating it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2348.367,
    "end": 2348.748,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2348.928,
    "end": 2354.054,
    "text": "Yeah, it's explored in SPM.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2354.695,
    "end": 2367.951,
    "text": "I see the variational applause as mode-seeking and then secondarily estimating how spread a parabola should be, which isn't a normal distribution because a parabola drops off",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2368.403,
    "end": 2378.557,
    "text": " Whereas if you do the variational Gaussian, then you actually are jointly tuning the mean slash mode and the variance parameter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2378.577,
    "end": 2382.362,
    "text": "And then you do have the tails going off into the probability distribution.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2383.363,
    "end": 2390.032,
    "text": "Well, what you should remember that though, and so you're exactly right to talk about the mode seeking behavior and fitting the parabola.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2390.733,
    "end": 2398.123,
    "text": "But remember that the parabola we're talking about, so this quadratic function is actually being used to fit the log probability, not the probability itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2399.099,
    "end": 2403.865,
    "text": " And if your log probability is of a quadratic form, then you are dealing with a Gaussian distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2404.266,
    "end": 2410.314,
    "text": "And in fact, it is the coefficient of the quadratic term that is then your precision or your inverse covariance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2412.016,
    "end": 2412.437,
    "text": "Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2412.577,
    "end": 2414.099,
    "text": "Great addition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2414.259,
    "end": 2417.083,
    "text": "Okay, we get to figure nine.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2417.944,
    "end": 2421.389,
    "text": "So it's in the medical journal.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2422.15,
    "end": 2422.891,
    "text": "What does it mean?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2423.552,
    "end": 2424.533,
    "text": "What is it saying?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2428.58,
    "end": 2444.802,
    "text": " So when you're looking at, probably the easiest thing to look at here would be, sorry, I'm just minimising the video so I can see it clearly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2445.083,
    "end": 2454.756,
    "text": "Yeah, so the sort of thing you might see in a medical journal would often be a survival curve that looks a bit like the prediction curve that you see here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2455.445,
    "end": 2463.577,
    "text": " Often it's more stepwise in terms of how it's expressed, and that's to do in the form of Kaplan-Meier estimators.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2463.597,
    "end": 2474.212,
    "text": "And the way in which you'd read that is that the y-axis is showing the number of people, the proportion of people who have yet to experience an event.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2475.274,
    "end": 2484.287,
    "text": "And often that event is either a particular kind of clinical outcome or sometimes lost to follow-up, which is another thing that always needs to be incorporated in the image.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2485.06,
    "end": 2490.926,
    "text": " And then you end up in questions about censorship of data, which essentially means, how do I deal with data I no longer have access to?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2492.387,
    "end": 2496.051,
    "text": "And the x-axis here is time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2496.251,
    "end": 2512.587,
    "text": "So at each point along the x-axis, I'm asking, between these two curves, each of whom represent a population of people who have or have not been exposed to an intervention, what's the difference in terms of the proportion of people who have not yet experienced an event?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2514.389,
    "end": 2522.561,
    "text": " So that event might be a heart attack or a stroke or various other things that you might pick out as being clinically relevant.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2524.985,
    "end": 2537.263,
    "text": "And the key thing really is trying to understand how does either different interventions or even the absence of an intervention or more commonly a placebo intervention differ.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2537.723,
    "end": 2543.552,
    "text": "And that allows you to estimate the effect of the particular therapy that you might want to choose.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2544.511,
    "end": 2544.971,
    "text": " Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2545.552,
    "end": 2551.679,
    "text": "So here we have the random sampling in figure nine for the randomized control trial.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2552.339,
    "end": 2556.564,
    "text": "So in this situation, there was randomization of 5050.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2557.905,
    "end": 2571.7,
    "text": "And then the choices as shown here are are uniformly scattered throughout the duration of the study, then you build in the um,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2573.165,
    "end": 2584.164,
    "text": " deliberate choice of follow-up time and randomization by combining the general message passing with the information gain imperative here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2584.845,
    "end": 2586.608,
    "text": "And we get to figure 10.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2586.828,
    "end": 2588.912,
    "text": "So what is different in figure 10?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2590.343,
    "end": 2597.914,
    "text": " Well, I mean, the other thing I'll just point out is, so the choices here are both the choices about follow-up time and also the choices about randomization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2598.275,
    "end": 2608.69,
    "text": "So the size of the dots representing each choice represents, and I'm afraid I can't remember which way round this is, but the number of people who were randomized to each group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2608.67,
    "end": 2620.09,
    "text": " So three different sized dots, one of which represents one third in the placebo group, one of which represents half in the placebo group, and one of which represents two thirds in the placebo group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2621.252,
    "end": 2631.91,
    "text": "So it is actually, and you're right to say it's deciding in a completely uniform way in the first figure, just to illustrate what would happen if we just decided these things at random.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2632.092,
    "end": 2635.056,
    "text": " So nobody ever decides these things just at random.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2635.076,
    "end": 2636.538,
    "text": "It's a bit more rational.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2636.718,
    "end": 2650.496,
    "text": "There's normally often a pre-specified follow-up time or even in base adaptive type clinical trials, there'll be criteria that should be met in terms of when people are followed up and how the randomization works.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2651.478,
    "end": 2661.711,
    "text": "But we're comparing to a slightly straw man example of if we were to completely randomize and just pick things entirely at random.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2663.986,
    "end": 2677.06,
    "text": " So the difference between figures 9 and 10 is that in figure 10, we've now put the information gain as one of the key things to make use of.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2678.062,
    "end": 2687.071,
    "text": "And so here it's going to adjust the randomization as it goes to try and make sure it learns most about what happens in each of these different conditions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2687.172,
    "end": 2692.137,
    "text": "So it's more uncertain about the trajectory in one condition",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2692.977,
    "end": 2701.567,
    "text": " it will assign more people for that condition and vice versa until it's got more precise estimates, hopefully, of each of these lines.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2703.489,
    "end": 2705.591,
    "text": "It's also going to change where it solves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2705.972,
    "end": 2712.199,
    "text": "And you can see that there's a slight shift, I think, towards later in the trial.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2712.98,
    "end": 2718.546,
    "text": "One of the reasons for that is that the later the follow-up time, the more informative that is about all the previous follow-up times.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2719.201,
    "end": 2726.429,
    "text": " If I observe someone early on and see that they've survived, that doesn't necessarily tell me how much longer after that they've survived.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2727.09,
    "end": 2736.181,
    "text": "Whereas if I observe someone at the end and know that they've survived, that tells me they've probably, well, that tells me they've definitely survived throughout all of the preceding time points as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2737.502,
    "end": 2741.307,
    "text": "So there's a little bit more information to be gathered towards the end compared to at the beginning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2742.428,
    "end": 2744.13,
    "text": "So there's a little bit of a shift in that direction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2744.931,
    "end": 2746.693,
    "text": "What's not done in this particular",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2747.23,
    "end": 2757.4,
    "text": " simulation is any cost or any preference for people surviving, which obviously is a very important thing to include in any realistic decision making.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2758.681,
    "end": 2760.463,
    "text": "I think that's going to be the next figure we look at.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2760.483,
    "end": 2762.485,
    "text": "So then you layer in the cost in 11.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2762.585,
    "end": 2763.846,
    "text": "So explain this one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2765.528,
    "end": 2766.149,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2766.289,
    "end": 2772.715,
    "text": "So the first thing I tried doing with this one actually was just putting a general preference for observing someone surviving.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2773.218,
    "end": 2782.435,
    "text": " The issue was it then just selected all of the points at the very beginning of the trial, because everybody survives at the beginning, nobody has time to experience the outcome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2782.655,
    "end": 2788.927,
    "text": "Which, if you prefer seeing people survive, is actually a completely reasonable thing to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2790.59,
    "end": 2793.235,
    "text": "But what we really want is for people to survive",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2793.383,
    "end": 2794.585,
    "text": " for the duration of the trial.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2795.887,
    "end": 2811.411,
    "text": "So here there's a sort of increasing function of preference that says, I prefer to see people having survived at the very end, which is of course also weighted with this potential information gain down.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2811.431,
    "end": 2821.146,
    "text": "And one of the things you'll see here is that it relatively rapidly switches its behavior to randomize more people to the placebo group, which is the group in pink.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2822.847,
    "end": 2836.959,
    "text": " And that seems like a very reasonable thing to do because as things progress, you can see that the placebo group, even though it's less confident than on the previous slide, it learns that the placebo group actually survived better than the treatment group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2836.979,
    "end": 2839.801,
    "text": "So in this particular case, the treatment would not be a good treatment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2839.961,
    "end": 2842.864,
    "text": "It's actually harmful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2843.845,
    "end": 2852.652,
    "text": "And this is analogous to the idea of stopping early in a clinical trial because you're concerned about the effect that your intervention is having.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2852.885,
    "end": 2860.432,
    "text": " Um, and so I think this is, this is sort of encouraging to see that it's, um, it's behaving in the right way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2861.013,
    "end": 2868.2,
    "text": "It's, um, it's saying actually more people survive and don't experience this deleterious outcome if they have a placebo.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2868.22,
    "end": 2873.024,
    "text": "So as I become more and more confident of that, I'll randomize more people for the placebo arm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2874.085,
    "end": 2878.43,
    "text": "Uh, and the end conclusion of this is the difference between the randomized and that arm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2878.45,
    "end": 2881.052,
    "text": "And in a sense, this is, this almost",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2881.471,
    "end": 2908.17,
    "text": " blurs the boundaries between the information-seeking, which is the primary aim of the clinical trial, and the public health imperative to try and make available the best treatments that are going to be most effective and to not give people treatments that are going to cause them harm, but by trading off exploration and exploitation in a traditional sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2908.15,
    "end": 2922.615,
    "text": " We can actually blur the boundaries between those two things and gather information until we've reached the point where actually the amount we could potentially gain from this information is less than the cost that that's having on people who are randomized to the wrong group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2925.548,
    "end": 2926.029,
    "text": " Thank you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2926.049,
    "end": 2937.505,
    "text": "It's very interesting in light of our earlier discussion on the sort of reward first or epistemic first, when we think about the Hippocratic oath, like do no harm, do no evil.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2938.266,
    "end": 2940.97,
    "text": "And then it immediately comes in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2940.99,
    "end": 2943.073,
    "text": "Well, what about a blood draw?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2944.054,
    "end": 2945.696,
    "text": "And this is going to cause harm.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2946.017,
    "end": 2947.659,
    "text": "Oh, well, there's a better benefit.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2948.46,
    "end": 2955.37,
    "text": "But then that is still all cast and reduced down to the expected benefits and utilities.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2955.873,
    "end": 2980.024,
    "text": " which leaves information gain kind of out in the cold as something that can't really be compared or brought in whereas this is in in certain ways blurring a line that gives there's a lot of structures around that line between what is health research and what is public health and so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2982.097,
    "end": 2998.281,
    "text": " helping paint that it is blurred in practice and that the formalism brings clarity to that fusion imperative rather than blurring what was clear.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 2998.321,
    "end": 3011.361,
    "text": "It's a very, I mean, it's a society shifting concept that would change how studies and day-to-day health could be done.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3013.653,
    "end": 3017.479,
    "text": " And I think in many ways it is done already at an implicit level.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3017.759,
    "end": 3037.028,
    "text": "I mean, I think a large number of the decisions that any doctor is making on a day-to-day basis revolves around, do I have more to gain by getting this new piece of information versus the potential cost of doing this to the patient, to the health service, to society in general?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3037.008,
    "end": 3063.96,
    "text": " um and um you know do you do you treat somebody early before you're completely sure about what condition they might have or do you wait until you gather more information but potentially risk deteriorating in that time um so there's always a lot of these trade-offs happening in clinical practice i think implicitly they are already happening in at the level of these sorts of trial designs as well that people do stop trials early um",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3064.177,
    "end": 3069.364,
    "text": " Or do you use adaptive trial designs where they can change the parameters based on what's happened?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3069.384,
    "end": 3080.619,
    "text": "Probably means they are implicitly using these same ideas without necessarily formalizing them as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3080.759,
    "end": 3087.127,
    "text": "So in a sense, I think it's maybe an endorsement of that approach rather than necessarily saying people should do things differently.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3087.168,
    "end": 3089.07,
    "text": "I think in many ways people are already doing this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3090.89,
    "end": 3120.424,
    "text": " yeah um from the hell oh yeah chris chris go ahead i was just wondering because you you what i like about this figure that and you much said it outright is that it shows kind of a stopping early sort of uh modality in the study where you know you're shifting more people towards the placebo group you're effectively more or less just saying you know that it's not worth it anymore um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3120.927,
    "end": 3149.5,
    "text": " what was interesting to me or i guess what i'm wondering is is also the counter to that that you could you know in a different scenario say hey this is working really well um we're going to put more people into the treatment group absolutely fine but is there a way that you can beg kind of extending the time into some of these models or decreasing the time they're just saying at some point hey it's no longer",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3149.783,
    "end": 3168.383,
    "text": " worthy of me continuing to randomize people here it's just let's stop here this is kind of that cost function can you optimize the cost function to um either decrease or increase the time so that you can get the most benefit from the trial that's kind of what i was wondering",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3169.038,
    "end": 3170.881,
    "text": " Yeah, no, absolutely.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3170.901,
    "end": 3192.075,
    "text": "And I think sort of changing the time parameter is another one of the key things that you... I mean, in a sense, you could argue that this choice of when to follow up is precisely that, that actually it's already to some extent doing that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3193.152,
    "end": 3202.902,
    "text": " And it would be interesting to trade off the costs in terms of the information lost when people are lost to follow up and those sorts of things as well in an even more realistic model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3202.922,
    "end": 3205.965,
    "text": "It's still relatively simplistic when we move forward here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3208.088,
    "end": 3222.062,
    "text": "But I think to take your point more generally as well, it's really interesting to think about what are the parameters, not just in a clinical trial, but in any sort of experimental design or policy decision",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3222.852,
    "end": 3225.254,
    "text": " What are the key parameters that you have control over?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3225.435,
    "end": 3233.282,
    "text": "And what are the effects of those parameters on the information you might gain and on the preference you might have for different kinds of outcome?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3234.243,
    "end": 3250.8,
    "text": "And in principle, any of these that you can formulate in a clear way, you can use these sorts of approaches to start to try and optimize and choose in a principled way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3252.282,
    "end": 3253.923,
    "text": " I think that's fantastic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3253.943,
    "end": 3266.374,
    "text": "I'm kind of thinking through now, you know, survival is a pretty binary like outcome, you know, you're dead or you're alive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3268.035,
    "end": 3270.978,
    "text": "It's a very clean system to try this out in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3272.159,
    "end": 3276.963,
    "text": "When do you start getting into more nuanced health benefits?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3276.983,
    "end": 3282.087,
    "text": "So if you're looking for possibility of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3282.877,
    "end": 3302.603,
    "text": " If you're tweeting somebody with a myocardial infarction or faction, however you want to say that, and you're trying to treat them with a drug that is supposed to change an arrhythmia in their heart or something of that nature, you'll see smaller gains.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3302.683,
    "end": 3309.812,
    "text": "Instead of just like alive or dead, you're hoping the patient lives regardless in both cases, but you're not really measuring death.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3310.467,
    "end": 3317.095,
    "text": " How does this sort of model change when you start to look at more distributed data?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3319.538,
    "end": 3320.719,
    "text": "I think I caught all of that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3320.779,
    "end": 3322.541,
    "text": "Sorry, the sound is still a little bit quiet.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3322.561,
    "end": 3337.639,
    "text": "But if I could summarize, I think the question is effectively one of, yes, it's obvious that if your outcomes are life or death, that there's a clear distinction in terms of preferences I might want to have for those things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3337.889,
    "end": 3349.84,
    "text": " But there are a number of things that are much more nuanced and may have trade-offs in themselves that it becomes a little bit harder and less obvious as to how to assign preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3350.714,
    "end": 3357.42,
    "text": " And so you almost get back to what is often quite a generic issue as to how do I choose my priors?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3357.48,
    "end": 3359.202,
    "text": "How do I choose my degree?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3359.462,
    "end": 3361.944,
    "text": "And in this case, it's the prior preference that I'm choosing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3362.405,
    "end": 3363.425,
    "text": "How do I select that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3364.566,
    "end": 3369.611,
    "text": "And there are a couple of interesting approaches that people take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3370.071,
    "end": 3374.916,
    "text": "So one would be to say, well, let's actually look at what people already do in settings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3375.016,
    "end": 3380.721,
    "text": "When do people set these criteria for only stopping the trial or whatever else?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3380.701,
    "end": 3391.058,
    "text": " Can you then invert that process and then infer what the implicit cost function they were using was or what the implicit prior preferences were?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3391.078,
    "end": 3401.274,
    "text": "And then that might be a way of then just formalizing and saying, okay, so this clearly is the relative preference I have for these different outcomes relative to the information gain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3402.095,
    "end": 3404.92,
    "text": "That might be one way of approaching it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3405.44,
    "end": 3421.145,
    "text": " The other approach, I mentioned at the beginning of the session last week, that a lot of this had sort of come out of initially attending a workshop in Australia, in Sydney, where a lot of the discussion was around Bayesian networks and how you select priors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3421.285,
    "end": 3429.017,
    "text": "And a lot of people there work on the problem of elicitation and how you take",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3429.503,
    "end": 3449.908,
    "text": " focus groups or songs of the population or key groups who are invested in a particular problem, stakeholders, and how you extract from their views and how you ask questions in the right way to be able to get out the priors that matter to them or that they take to be relevant.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3451.29,
    "end": 3452.912,
    "text": "And that's obviously particularly",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3453.28,
    "end": 3465.107,
    "text": " used in contexts where the priors are uncertain and you want some expert views or there's limited information, you want some way of constraining a particular prior for a particular setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3465.528,
    "end": 3470.78,
    "text": "But I think it's also potentially very useful in terms of writing down the right kind of preferences here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3471.241,
    "end": 3472.504,
    "text": "So you could imagine if you asked",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3472.923,
    "end": 3477.349,
    "text": " enough people who might be relevant stakeholders in a particular outcome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3477.97,
    "end": 3479.953,
    "text": "How much do you value this over that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3480.273,
    "end": 3482.276,
    "text": "And do a number of comparisons there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3482.336,
    "end": 3491.77,
    "text": "You could start to construct the relevant cost function, because the relevant cost function here is often going to be for the people for whom the intervention is offered.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3492.631,
    "end": 3501.664,
    "text": "And so those of us who are not necessarily the beneficiaries of it or people who might be harmed by it are probably not the right people to be setting preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3503.197,
    "end": 3515.396,
    "text": " and finding out a way of engaging the relevant people and taking account of their views on it, and how those might be incorporated into pride preferences, I think is a really interesting and not entirely solved question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3520.184,
    "end": 3522.287,
    "text": "Could I jump in with a quick question as well?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3523.489,
    "end": 3523.83,
    "text": "Please.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3524.611,
    "end": 3527.736,
    "text": "So, if I may be so bold,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3528.56,
    "end": 3546.749,
    "text": " shall we say, the guiding normativity of the various simulations that were offered in the paper was something like, at least initially, the degree of expected information gain that one is able to ascertain at a certain point in time as a consequence of a certain intervention or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3547.43,
    "end": 3553.159,
    "text": "And now we're running into the issue of this, let's call that the cost function, perhaps.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3554.101,
    "end": 3554.922,
    "text": "This is,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3555.644,
    "end": 3574.632,
    "text": " this is what we're going to perhaps use as our proxy for the relevant thing to do at the relevant time or the relevant population and so on but we're coming up against issues now of uh yes but there are the the question is to the relevant uh you know action is not merely reducible to expected information gain at",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3574.763,
    "end": 3583.616,
    "text": " as a consequence of a certain action, there are meta considerations like, ah, we would like to observe people living throughout the trial, basically.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3583.657,
    "end": 3587.342,
    "text": "So we don't want to just have people be observed to be living at the beginning of the trial.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3588.384,
    "end": 3592.57,
    "text": "You can do expected information perfectly fine with all of that, but there is this meta consideration.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3592.55,
    "end": 3607.875,
    "text": " um of the way in which we're going to you know solicit preferred observations that doesn't seem to be just reducible to expected information gain so a lot of my particular you know interest are in well how do we how do we map out that sort of space it",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3607.855,
    "end": 3614.645,
    "text": " perhaps expect an information game could be exacted into this larger space of considerations, one might think.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3614.725,
    "end": 3618.35,
    "text": "But that's a very interesting problem that I think you said as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3619.331,
    "end": 3620.473,
    "text": "It's an incredibly difficult problem.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3621.174,
    "end": 3624.298,
    "text": "And it certainly wasn't the focus of this paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3625.18,
    "end": 3632.61,
    "text": "So it's a bit unfair, perhaps, to lay upon you questions as to, OK, well, what might guide those higher-order considerations?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3632.63,
    "end": 3637.457,
    "text": "But that is something that I thought would be very interesting to discuss maybe later or now.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3639.445,
    "end": 3642.088,
    "text": " As you've raised it now, shall we discuss it now?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3642.168,
    "end": 3652.619,
    "text": "I think you're exactly right that expected information gain on itself is not enough, at least in some settings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3652.639,
    "end": 3654.181,
    "text": "I think in some settings it probably is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3654.221,
    "end": 3666.094,
    "text": "And that almost begs the question, how do you know whether to put more emphasis on expected information gain or other forms of cost?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3666.968,
    "end": 3687.688,
    "text": " And I say that the sort of addition of cost into this, it is partly relevant in the sense that, as in relevant to the expected information gain, in that it's only when you have a good quantification of the cost of, or the benefit of gaining information, cost of not gaining that information, that you can actually start to do that trade off.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3689.069,
    "end": 3695.575,
    "text": "So although it can sometimes feel a little bit arbitrary that you're just sort of adding another term to the objective function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3695.909,
    "end": 3717.208,
    "text": " But actually, the only reason you could do that is because by expressing them in exactly the same units and the same notions, different kinds of probability distribution or log probabilities or potentials, it's only by being able to put them in that same space that you can even start to do this trade-off, which is the reason it was relevant to introduce the cost function",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3718.522,
    "end": 3742.487,
    "text": " in addition to that in this setting obviously the other approach that we might take in active inference is actually to go from first principles try and derive something that looks like an expected free energy that is already a combination of these things and then can be pulled apart to see how it contributes to both these sorts of costs and information gain um and i think",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3744.003,
    "end": 3755.134,
    "text": " clearly putting the right sort of costs so that you will only seek out information if it doesn't lead to something that is very, very aversive and vice versa.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3755.174,
    "end": 3765.865,
    "text": "You'll only seek out things you prefer if there's also some scope to gain information, at least relative to other actions you might take, I think is useful in allowing these things to contextualize one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3767.647,
    "end": 3772.272,
    "text": "And that's something Carl put very well in the last session we did.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3774.53,
    "end": 3775.752,
    "text": " What were your thoughts about it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3776.032,
    "end": 3777.315,
    "text": "How would you approach that question?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3778.697,
    "end": 3793.281,
    "text": "I mean, I can't think of any other way than to, you've kind of already said it, than to situate the whole task where, considering in this case, it's maybe clinical design, trial design and so on for survivability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3794.324,
    "end": 3800.81,
    "text": " The only way I can really see to do it is to relate it to the kind of auto poetic goals of the agents involved.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3801.37,
    "end": 3807.096,
    "text": "So maybe we have a meta assumption that all of these agents wish to continue across time or a certain subset of them.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3808.016,
    "end": 3811.86,
    "text": "We wish a certain subset of them to continue across time, given certain considerations.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3814.542,
    "end": 3822.93,
    "text": "I'm just spitballing here, but I don't really see how to situate the guiding normativity in anything other than the",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3823.737,
    "end": 3834.535,
    "text": " continuation of the sensor motor loop of a particular subset of the agents under consideration, of course, then that raises the question, well, which subset do we want?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3836.558,
    "end": 3838.741,
    "text": "So we sort of have a recapitulation of the same issue.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3839.583,
    "end": 3840.665,
    "text": "So to sum up, I have no idea.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3842.087,
    "end": 3844.691,
    "text": "That's the only really thing that comes to mind.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3844.711,
    "end": 3847.095,
    "text": "If we can somehow tie this to the",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3848.442,
    "end": 3875.175,
    "text": " the you know the ways in which autopoetic beings are able to maintain their autopoiesis across time um if anyone else has a better suggestion um that's that's as far as i've been able to get so far okay what that makes me think about is something like a um semantic supply chain for this for the clinical study so it's like we didn't assess in this paper",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3875.695,
    "end": 3904.673,
    "text": " should we do a clinical study but there's another room where that's a question it's like well what could we stand to learn what would the cost be and then so and then someone so somebody says should we do this clinical study and then someone's like should you have taken the policy to ask if we should have done the clinical study and someone's like yeah i didn't know so i asked to find out and then that's where the buck stops because right there you can say here's where there was a definable",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3904.653,
    "end": 3920.333,
    "text": " uncertainty that was actionably addressed and that's how it propagated on through to the clinical study and then and then um all along the way counterfactuals could be analyzed",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3921.123,
    "end": 3945.679,
    "text": " like if it were 10 times more costly to sequence the genome then we would have had done this study but then that also opens the door to when the cost of something is lower that another study could be done um there's this element of the participatory stakeholder involvement and asking people questions that that keeps it grounded but",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3946.672,
    "end": 3972.696,
    "text": " And it's obviously such a generic situation that there aren't strong answers, but just the idea of tracing our uncertainties and preferences and norms back on through to the cognition of the study design seems to take it into a place that's going far beyond just trying to sample optimally from a population.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3974.161,
    "end": 3987.073,
    "text": " because it calls into question like what does optimal sampling, if the people so strongly don't want to be sampled, then what is this kind of optimal sampling protocol actually sampling?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3989.5,
    "end": 3996.186,
    "text": " It's all maybe somewhat moot, obviously, because in this example, we are doing Bayesian optimal trial design.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3996.767,
    "end": 4000.95,
    "text": "A lot of those questions are going to be posed and answered for us.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4001.551,
    "end": 4005.775,
    "text": "So trying to make it more general is maybe just a fool's errand.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4005.815,
    "end": 4013.141,
    "text": "Maybe there isn't a way to generalize that other than to swim in the waters of the particular situation we're considering.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4014.402,
    "end": 4019.507,
    "text": "And of course, it's a very noble and worthwhile thing to consider Bayesian optimal trial design.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4020.263,
    "end": 4021.706,
    "text": " Perhaps it's not much of a problem.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4023.77,
    "end": 4028.118,
    "text": "Well, I mean, I think you're right to think about how to try and generalize the problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4028.178,
    "end": 4035.03,
    "text": "And arguably, the answer is exactly the same way, exactly as Daniel's described.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4035.211,
    "end": 4039.639,
    "text": "You could say the question of actually designing this trial, that's basically thinking of doing it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4040.311,
    "end": 4065.137,
    "text": " is itself something that comes with potential information gain, potential costs, benefits, and that actually a lot of behavior is based, well, almost all behavior where I select between alternative things I could do is going to be based on similar desires to both explore, to find out, and to exploit once I know that I can satisfy certain preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4065.117,
    "end": 4073.473,
    "text": " Arguably, it's only by doing those things that you can maintain your persistence over time in exactly the way you were describing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4074.755,
    "end": 4084.493,
    "text": "That possibly is one of the key imperatives for any sort of creature that can envisage alternative paths forwards and select between",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4088.438,
    "end": 4090.161,
    "text": " To the public how... To keep playing the game.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4090.182,
    "end": 4090.783,
    "text": "Oh, go ahead, Frasier.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4090.803,
    "end": 4091.164,
    "text": "Sorry, go ahead.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4091.264,
    "end": 4091.564,
    "text": "Oh, sorry.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4091.945,
    "end": 4096.314,
    "text": "I was just going to say, to keep playing the game, basically, is the answer.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4099.581,
    "end": 4104.27,
    "text": "I think there's another way that this comes into play with...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4104.419,
    "end": 4106.881,
    "text": " health study versus just public health.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4107.562,
    "end": 4117.371,
    "text": "In a health study, oftentimes just like laboratory studies in general, you want to make a control group that's actually like a valid comparison.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4117.491,
    "end": 4127.38,
    "text": "Like if you're studying the effect of food X versus not, you really want to make sure the people who ate it ate it and who didn't eat it didn't eat it because you're trying to make them maximally separable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4128.001,
    "end": 4133.346,
    "text": "Whereas if a given health intervention or program",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4133.63,
    "end": 4139.217,
    "text": " were driven to a large extent by pragmatic consideration.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4139.317,
    "end": 4142.481,
    "text": "Like we want to reduce the prevalence of this in this population.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4143.402,
    "end": 4150.911,
    "text": "Then that would plug into many other policies that could come into play.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4151.491,
    "end": 4156.758,
    "text": "So instead of like, we're going to study whether a daily email can help with this health condition.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4157.619,
    "end": 4162.945,
    "text": "And then the whole imperative of the researcher, okay, we want to make sure those emails were delivered or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4163.262,
    "end": 4170.134,
    "text": " Versus if we pull back and say, okay, they want these people to have less of this disease.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4171.035,
    "end": 4176.164,
    "text": "And here's how that is being balanced with reducing uncertainty about the effects of email.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4177.246,
    "end": 4188.004,
    "text": "Then somebody else, maybe that group or another group could pick up on the pragmatic consideration and then plug in with a different policy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4188.187,
    "end": 4193.896,
    "text": " So from that pure epistemic standpoint, it's like, oh, but now this is like less of a controlled experiment.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4195.038,
    "end": 4205.014,
    "text": "And it's like, well, yes, that is the trade-off that is being finessed is the frontier with who does what.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4205.315,
    "end": 4213.585,
    "text": " when there are both things that are uncertain, but there are things that we could be uncertain and simply be unsalient to us.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4214.806,
    "end": 4229.083,
    "text": "And then we don't need to take the policies to reduce uncertainty because the information gain, even if it was a lot, could be just like, well, you could look at a block of random text and you could reduce your uncertainty a lot.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4230.244,
    "end": 4234.95,
    "text": "But then that is not usually selected because it's not aligned with other preferences.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4237.175,
    "end": 4246.77,
    "text": " But that also, I think, the example of looking at a block of text is an interesting one as to why you might not do that, even though it might resolve your uncertainty as to what's on the page.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4247.711,
    "end": 4250.876,
    "text": "Because you also need to think about what else it might resolve your uncertainty about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4251.997,
    "end": 4259.689,
    "text": "So if you are curious about something or do not know something and that block of text will tell you it, then it may be quite a useful thing to read that block of text.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4260.108,
    "end": 4277.484,
    "text": " Whereas looking just to see the particular wording about something you already know about, the wording of something about which you already know, then there's not much point looking at it, is there?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4277.644,
    "end": 4285.992,
    "text": "It's sort of a waste of time and energy because it's already some uncertainty that you don't even have to resolve.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4286.012,
    "end": 4288.174,
    "text": "You don't have that uncertainty in the first place.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4289.015,
    "end": 4294.927,
    "text": " Similarly, you might not know something and the text may still not argue anything about it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4294.987,
    "end": 4301.019,
    "text": "In other words, it's quite ambiguous text because it doesn't help resolve between the alternative hypotheses.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4302.342,
    "end": 4306.23,
    "text": "And I guess this comes down to an interpretation of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4306.598,
    "end": 4313.73,
    "text": " expected information gain or the mutual information between your hypotheses and the data that you could measure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4314.911,
    "end": 4319.799,
    "text": "And mutual information can always be separated into two entropies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4320.32,
    "end": 4330.176,
    "text": "So one of them is an entropy that is how uncertain am I about what these data would look like under this plan.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4331.017,
    "end": 4336.546,
    "text": " And the other one, the other entropy is, which is subtracted from that, is the conditional entropy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4336.606,
    "end": 4341.614,
    "text": "So how ambiguous is this measurement?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4342.435,
    "end": 4348.264,
    "text": "And the way I would often like to think about that is there's no point doing an experiment where you already know what the data would look like.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4349.746,
    "end": 4358.34,
    "text": "If I could picture exactly what I would measure before I even did the experiment, there'd be no point in that experiment because what am I going to do?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4358.455,
    "end": 4360.538,
    "text": " And that's our predictive entropy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4360.558,
    "end": 4363.623,
    "text": "That's the first one, which is how uncertain am I about what I see?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4365.205,
    "end": 4369.772,
    "text": "But then you have to account for the amount of information that you're actually going to be able to resolve.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4371.174,
    "end": 4376.582,
    "text": "And so subtracting this ambiguity, making sure that actually",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4376.562,
    "end": 4379.247,
    "text": " this experiment is going to resolve that uncertainty.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4379.367,
    "end": 4386.439,
    "text": "It's not just going to be uncertain, noisy data that, yes, I can't predict, but that's just because I've used an extremely noisy measuring instrument.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4386.539,
    "end": 4388.482,
    "text": "That would also be a pointless experiment to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4388.502,
    "end": 4402.165,
    "text": "So the right experiments to do are the ones where I couldn't predict the data, but based upon what I know already, but that conditioned upon a particular hypothesis, the data would actually be very predictable.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4403.343,
    "end": 4409.232,
    "text": " And so by seeing those data, I can start to really disambiguate between things that I didn't already.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4416.523,
    "end": 4419.968,
    "text": "Okay, in this kind of spirit of the block of text.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4420.117,
    "end": 4426.894,
    "text": " There's a lens or a frame in which it's just uniform, but then there's another way in which it might resolve or not.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4427.014,
    "end": 4431.105,
    "text": "Okay, so there's no point to do the experiment if you already know what the outcome would be.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4431.646,
    "end": 4436.117,
    "text": "But then maybe in that case, it would be something more like a habit or a ritual.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4436.654,
    "end": 4451.994,
    "text": " Or even if you have an experiment that you don't know what the outcome is going to be, there's still procedural elements of the experiment that you do want to know the outcome on, like the intermediate calibrations or assays.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4453.135,
    "end": 4466.593,
    "text": "So I think this kind of points to the very rich and woven nature of epistemic and pragmatic value and why it's so important to have an accounting system that can",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4467.586,
    "end": 4474.272,
    "text": " hold them up side by side, because even on one action,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4474.673,
    "end": 4490.753,
    "text": " there might be many ways or different scales of a nested model which that action or the consequences are being chosen for a variety of competing factors and then sampled probabilistically.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4490.793,
    "end": 4504.309,
    "text": "And then the impact on a hierarchical predictive processing system also may not simply fall into an epistemic or a pragmatic bucket because it could be",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4504.289,
    "end": 4512.038,
    "text": " epistemically novel at this layer, but then that was exactly what was expected at another layer, but then the fact that it was expected at that layer was learning at another layer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4512.058,
    "end": 4516.443,
    "text": "So it's very interesting.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4517.444,
    "end": 4528.356,
    "text": "Yes, we spoke last week about the idea that you can actually use a form of message passing in sparse generative models to pick out the uncertainty or the expected information gain about a particular state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4529.652,
    "end": 4556.571,
    "text": " and i think that's an another way of looking at exactly what you've just outlined that it may be that there is a great that one action gives you a lot of potential information gain about a particular state um but you have to then encounter the information gain that other states um or about other states in your model about other things and so that's the situation where maybe it would be the wrong thing to to focus in on a particular margin",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4557.04,
    "end": 4563.208,
    "text": " And actually what you want to know is the information gain about all of the things you don't know about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4563.228,
    "end": 4568.576,
    "text": "Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4569.357,
    "end": 4575.505,
    "text": "So here you might say, do I want to know about the effects of all the confounding variables or do I just want to know?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4575.805,
    "end": 4605.795,
    "text": " or do i just want to estimate the effect of the intervention so is it relevant for me to characterize the precise shape of the um of the function and temporal trajectory is those basis sets or is it sufficient for me to just focus on resolving my uncertainty about the size of the effect of treatment how different these two plots are and that that problem you know we were speculating before",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4607.243,
    "end": 4615.755,
    "text": " The issue that I have in my mind is, well, how do you decide between those two levels, let's say, of expected information gain resolvable?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4616.295,
    "end": 4621.763,
    "text": "Do you have another meta expected information gain between the two, say, the lower level?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4621.823,
    "end": 4628.492,
    "text": "Which basis functions should I have versus the, well, which approach should I even take at all?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4628.945,
    "end": 4637.656,
    "text": " That's a question that I find particularly interesting, scaling all the way up to the previous example that Daniel mentioned, you know, well, should I even do the intervention at all?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4638.397,
    "end": 4653.315,
    "text": "I know that comes quite a far field from the subject of the paper, but that very much gets into the question of, well, exactly what level is relevant, what information gain is relevant, and how do we score which one is relevant?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4654.477,
    "end": 4659.603,
    "text": " When we're doing a clinical trial, it's quite obvious, relatively obvious, let's say, for sure.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4659.623,
    "end": 4659.783,
    "text": "Yes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4661.485,
    "end": 4671.816,
    "text": "And perhaps part of that comes back to the original discussion we were having about the relevance of different sorts of states and distributions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4671.836,
    "end": 4680.926,
    "text": "And obviously, this is a very subjective thing, exactly as you were talking about this realization, the idea that it's both about myself and my model and the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4680.966,
    "end": 4684.23,
    "text": "So indeed, you can't take one away from the other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4684.21,
    "end": 4710.623,
    "text": " And probably a large part of that does come down to the precision ascribed to different sorts of mappings, whether mappings are going to be conditional probability distributions, and the perhaps voluntary attentional weighting deployment of different decision values to different parts of your model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4711.615,
    "end": 4721.868,
    "text": " And you could argue that if you were to do this at a higher level, you might be able to use it on empirical priors over what might happen at a lower level of the model as well, or even to contextualize the preferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4722.269,
    "end": 4728.737,
    "text": "So there's no reason why your preferences or cost function can't be context sensitive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4730.44,
    "end": 4739.952,
    "text": "And in fact, the really interesting question comes when you invert that and you say, well, okay, if I'm predicting my cost function based upon the higher level, then what I'm doing at the lower level",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4740.927,
    "end": 4743.97,
    "text": " should be able to tell me something about the context at the higher level.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4745.751,
    "end": 4757.162,
    "text": "So if I believe that in this circumstance I like doing this, whereas in this circumstance I like doing that, then seeing what I do might then give me evidence for being in one of those two different contexts.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4759.584,
    "end": 4769.213,
    "text": "So it's a really interesting question is how you might hierarchically decompose these things and allow layers and layers of context sensitivity",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4770.104,
    "end": 4782.865,
    "text": " that then allow me to select my preferences and also potentially conditioning my precisions in such a way that the balance between these exploitative and explorative drives will vary.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4785.089,
    "end": 4791.657,
    "text": " Yes, there's a relatively canonical example that John Pawecki gives on precisely this issue.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4791.717,
    "end": 4794.9,
    "text": "Imagine you're in a lecture and you need to take good notes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4795.701,
    "end": 4797.343,
    "text": "Your initial state is, well, I don't have good notes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4797.764,
    "end": 4799.065,
    "text": "What are the possible actions?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4799.966,
    "end": 4803.49,
    "text": "Well, I could write stuff down, I could type things, I could use finger paint.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4804.512,
    "end": 4810.118,
    "text": "The affordances and the prediction errors, the precision of the prediction errors and the",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4811.228,
    "end": 4819.504,
    "text": " the expected information gain afforded as a consequence of any particular action is very undefined.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4819.544,
    "end": 4822.028,
    "text": "It's not unspecified, but it's underspecified.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4822.629,
    "end": 4832.528,
    "text": "And so the exact low-level routines that you should engage in, the expected information gain as a consequence of any particular action",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4832.761,
    "end": 4836.965,
    "text": " low-level action is relatively similar to any other one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4837.745,
    "end": 4854.821,
    "text": "So again, there is that question of, well, presumably there's going to be something from a higher-order preference, whereby I have, in the past, let's say, I've built up a model that, ah, my laptop is an excellent thing for me to use to write down things and so forth, so that's going to kind of solve the problem in that case.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4855.501,
    "end": 4862.768,
    "text": "So yeah, there does appear to be some model of hierarchical nesting of these things in some fashion.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4862.748,
    "end": 4884.317,
    "text": " i know that's delightfully um uh nebulous but uh that's the only that's the only really thing that came to mind i was just wondering if in the inversion of that particular example if you found yourself having not taken very good notes you might then say well if it were a good lecture i would have wanted to take good notes so the fact that i haven't it wasn't a very good lecture that has uh crossed my mind from time to time",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4886.66,
    "end": 4890.612,
    "text": " Something in there about like, if I had more time, I would have written a shorter letter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4890.673,
    "end": 4892.057,
    "text": "But I didn't.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4892.759,
    "end": 4894.645,
    "text": "So I must have had more time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4895.637,
    "end": 4899.762,
    "text": " Or the other thing that made me think of was back to the streetlight example.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4900.483,
    "end": 4909.095,
    "text": "So we talked about how the streetlight is both justified, but it's also this kind of like limitation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4909.115,
    "end": 4913.681,
    "text": "On one hand, that's where we can resolve our uncertainty, but also that's not the whole story.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4914.321,
    "end": 4920.97,
    "text": "But now we zoom out a layer and there's a policy on the electrical grid that",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4921.085,
    "end": 4926.31,
    "text": " deciding which streetlights to turn on or how to dim different streetlights.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4926.831,
    "end": 4933.818,
    "text": "And then you pull back even another layer, there's a construction policy on the streetlights over the decades.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4933.838,
    "end": 4937.582,
    "text": "And then at the nightly scale, it's the energy grid policy.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4938.243,
    "end": 4947.092,
    "text": "So then that agent's looking around information foraging is happening within slower scales",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4947.798,
    "end": 4952.627,
    "text": " such that they're not in a stationary single-street-like world,",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4953.265,
    "end": 4957.651,
    "text": " they might be looking around and then spiraling out just like you described.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4957.731,
    "end": 4964.419,
    "text": "And then that moves them into a new attractor, a new regime of attention where they're sampling differently from there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4966.862,
    "end": 4982.462,
    "text": "So it's kind of when we read a paper like this and we see a motif get built up so methodically, and then always there's this point in the dot too, where even that motif is just one puzzle piece.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4982.898,
    "end": 5011.032,
    "text": " swirling around with the broader question of of how we make these generative models when even the motif which looks awesome and sounds very cool it's very educational to learn about but then it is also just another element in these broader models yes and i think it clearly is a",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5011.518,
    "end": 5021.168,
    "text": " I mean, there are two big issues when you're, or three big issues when you're trying to design creatures that behave in an active inference way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5021.569,
    "end": 5027.275,
    "text": "So one of those issues is obviously drawing inferences about the world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5027.735,
    "end": 5030.799,
    "text": "Another one is changing the world to comply with your predictions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5030.959,
    "end": 5039.348,
    "text": "And then the third is, given that I've evaluated the first two in relation to the fit between my model and the data I currently have.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5040.037,
    "end": 5043.422,
    "text": " The third question is always, how do I deal with data that I don't yet have?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5044.123,
    "end": 5058.043,
    "text": "And that's where this sort of generic issue of selecting courses of action that lead to informative but also preferred data becomes so important.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5059.245,
    "end": 5060.106,
    "text": "But you're absolutely right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5060.146,
    "end": 5064.993,
    "text": "All of these things are then conditional upon the model that I have in the first place.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5065.214,
    "end": 5069.52,
    "text": "And that model will condition these various other things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5069.77,
    "end": 5098.832,
    "text": " model may say i have very very precise preferences or very precise prior beliefs over the sort of outcomes i'm going to seek out or it might not and that may be conditionally dependent on other things at other levels of the model which may be subject to exactly the same generic objective functions but because the form of the model at that level may lead to different sorts of behavior than you might anticipate otherwise and so i think you're absolutely right to highlight that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5099.538,
    "end": 5121.568,
    "text": " Although there are these generic motifs, the optimization of my model fits both by action and perception and the optimization of future data by choosing informative preferred data points are all relevant only in the context of the models under which they act.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5123.932,
    "end": 5125.714,
    "text": "There are, of course, also ways of building",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5126.015,
    "end": 5154.958,
    "text": " which is another interesting point but some of that will be dependent upon the data that's presented back by the world and um and and we sort of get back to some of the discussion we were having last week about things like natural selection and selection between alternative models and alternative individuals being alternative forms of models that that are selected um with each generation so there's a history dependent element which is dependent upon the data i've had before",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5155.597,
    "end": 5162.231,
    "text": " Which just comes down to that question of if I'm comparing to the data that I've observed previously, I'm back to just the problem of comparing models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5166.419,
    "end": 5169.285,
    "text": "Finding the most evidence, or finding the models that have the most evidence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5172.472,
    "end": 5189.883,
    "text": " In the conclusion, you pointed towards using sophisticated inference to approach some of these situations where you might want to plan over future observations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5190.524,
    "end": 5197.416,
    "text": "So where are you hoping to develop what is presented in this paper?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5199.505,
    "end": 5222.02,
    "text": " Well, I think to the point about the sophisticated inference scheme, and for those who haven't come across this, the idea is that although I have my separable objectives to seek out things I like and things that will inform me about my model of my world, each of those things may also be",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5222.675,
    "end": 5225.379,
    "text": " dependent upon what I've observed at earlier points in time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5226.04,
    "end": 5244.227,
    "text": "So it might be that having performed a particular action, made an observation and updated my beliefs in relation to the observation I've got, that that will then change what might be most informative at the next step in time, or what might be most rewarding at the next step in time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5244.882,
    "end": 5274.265,
    "text": " So by rolling out my beliefs about what I might do next and conditioning those upon what I might do after that and conditioning those upon what I might do after that, and more than that, conditioning those on the beliefs I might have at each of those points, you can really start to develop much deeper policies and have that recursive aspect that is probably quite familiar to a lot of people who've worked with things like Dolman optimality problems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5274.447,
    "end": 5293.114,
    "text": " One of the things I find quite interesting about that is that in principle, you might now do things that will help me seek out more information in the future, as well as doing things that will gain me information might help me achieve more rewarding observations in the future.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5295.357,
    "end": 5301.906,
    "text": "So again, they are still separate, but actually it will look like what a lot of people in",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5302.645,
    "end": 5308.774,
    "text": " in reinforcement learning type fields would be more familiar with, the idea that I might seek out information to gain reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5310.297,
    "end": 5322.996,
    "text": "If I have a recursive scheme, then it will look like I'm doing that, even if I additionally see information as having its own unique value in and of itself, regardless of whether or not it needs to reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5323.777,
    "end": 5327.623,
    "text": "So I think that's one of the key reasons for bringing that in here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5328.025,
    "end": 5337.707,
    "text": " that I might want to find out how far someone survived up until this point so that I can then contextualize the shape of the function when I know when they've survived later on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5338.528,
    "end": 5341.555,
    "text": "Or I might have the option to now put in multiple follow-up points.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5341.956,
    "end": 5346.827,
    "text": "And actually, if I were to do that independently of one another,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5347.516,
    "end": 5351.722,
    "text": " I might put them both in the same place because that's where I'm going to gain the most information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5352.363,
    "end": 5361.597,
    "text": "However, if I were to use a recursive sophisticated scheme, I might say, if I put in this follow-up point, then I'll have resolved all the uncertainty I need to about that point, so I'd better put this at the other point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5362.178,
    "end": 5374.076,
    "text": "And so you can now start to think about multiple follow-up points, multiple interventions, taking account of how one of those might influence the information you might gain from another one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5374.517,
    "end": 5376.46,
    "text": "So that was probably the key point of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5377.132,
    "end": 5378.634,
    "text": " of putting in that idea.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5379.456,
    "end": 5385.966,
    "text": "Anything you wanted to raise or discuss in depth on that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5387.849,
    "end": 5393.639,
    "text": "Yes, I think that's where some of these computational costs start to expand.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5394.66,
    "end": 5402.413,
    "text": "However, if it can be if a given situation can even be brought to the table of sophisticated inference, then",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5403.997,
    "end": 5425.38,
    "text": " it's well set up to do a lot of interesting analyses and then the other directions that you specified empirical evaluation of active versus alternative sampling methods and identifying appropriate cost functions so what about these yeah uh so i think the empirical evaluation um i think if you were to",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5426.778,
    "end": 5438.538,
    "text": " If you were to convince probably a broader community that this is a useful thing to do, then one of the things you'd have to do would be to show that it outperforms in a very clear way other schemes that you might want to use.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5439.66,
    "end": 5446.492,
    "text": "I think there are good principle theoretical reasons to see how this offers some benefits.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5447.113,
    "end": 5450.018,
    "text": "However, it may be that if you're dealing with a very non-linear",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5450.251,
    "end": 5479.465,
    "text": " uh problem that the cost of even doing all the computations to work out what i should do next might exceed the um process of actually just doing it uh or something at random one of the things i found quite interesting is that actually when for the for the first couple of simulations in the paper the random sampling is actually not much worse than uh choosing the data very carefully and one of the reasons for that is that random sampling will tend to give you a fairly uniform distribution",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5479.968,
    "end": 5502.904,
    "text": " over over the points you sampled and assuming there's uniform uncertainty initially that's actually a perfectly sensible thing to do so part of this is not just about evaluating it for the sake of it or to to prove that the theoretical results translate into something empirical it's not just about benchmarking but it's also about finding where are the places where it can be most useful",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5503.981,
    "end": 5507.828,
    "text": " Clearly, we have to have some sort of addressable way of generating the data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5507.848,
    "end": 5522.614,
    "text": "We have to be able to say, I'm going to sample over here or over here with a set of uniform unlabeled data points that there's no way of selecting between that have no spatial or other metric involved.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5523.32,
    "end": 5525.404,
    "text": " that's going to be a much harder thing to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5525.424,
    "end": 5530.214,
    "text": "In fact, it's going to be an impossible thing to do if there's no sensible way of defining your policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5530.936,
    "end": 5535.184,
    "text": "So I suppose the key thing here is thinking about when is this approach useful?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5535.305,
    "end": 5537.97,
    "text": "When will it actually help to resolve uncertainty?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5538.625,
    "end": 5557.567,
    "text": " and in many cases it might not if there is no difference in the information gain between different options I might take or if that difference is more or less the same as what I might get using another strategy and maybe more computationally efficient to use another sort of strategy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5559.869,
    "end": 5565.696,
    "text": "That's very interesting and the paper set up accordingly to this",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5569.979,
    "end": 5599.648,
    "text": " development from the random uniform to the max and to the full epistemic gain and so each of those are actually sampling policies that can be compared and i guess to um but they're not just something policies they're also something situations that i might be in so if i'm in the sort of situation where ambiguity varies then it's no longer sensible to use a maximum entry type approach so",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5599.831,
    "end": 5610.386,
    "text": " Ultimately, you could argue that this formulation automatically accounts for these different situations, that if my model is good enough as an explanation of the world, and how do I make sure it's good enough?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5610.546,
    "end": 5614.471,
    "text": "I make sure it's good enough by sampling appropriately, drawing those inferences.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5615.432,
    "end": 5626.007,
    "text": "But if it's good enough that I now have a sense of how the variance changes as a function of where I might sample, if I know where I'm uncertain, where I'm not.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5626.679,
    "end": 5629.943,
    "text": " then each of these things automatically becomes a contributory factor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5630.363,
    "end": 5639.394,
    "text": "Whereas if I don't know these things or I think the situation is one in which things are completely uniform, then this will now reduce back to one of the earlier approaches we were talking about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5639.414,
    "end": 5654.391,
    "text": "And if there's no benefit anywhere in terms of cost, no benefit in terms of the entropy I might be able to resolve in terms of objective entropy, and if the ambiguity is the same everywhere, then ultimately this is a random sampling approach and we'll reduce back to that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5655.215,
    "end": 5661.848,
    "text": " So in a sense, all of these things are versions of the same general problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5661.868,
    "end": 5677.257,
    "text": "But the situations in which I might benefit from employing different parts of this are those in which there is difference in ambiguity, in which there is difference in predictive entropy, in which there is difference in cost.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5678.84,
    "end": 5685.506,
    "text": " And so it all comes back to the model and whether I've got a good model of my world and my world complies with these different sorts of features.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5691.192,
    "end": 5708.408,
    "text": "Yes, it shifts the focus from summary performance on a previously acquired batch to more granular outcomes in a sampling driven",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5709.788,
    "end": 5735.779,
    "text": " regime like out of sample prediction versus looking at in sample just how well can you do on what you already have versus how surprised are you going to be about your prediction about what you haven't seen yet and then this even goes a little bit further which is how would you decide",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5736.502,
    "end": 5758.257,
    "text": " which things that you haven't seen to get data on and then how would you assess the cost benefits preferences and the epistemic value associated with that so that moves it from not just out of sample sense making but puts action up in front of getting that out of sample sample",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5762.185,
    "end": 5768.777,
    "text": " Yes, and you could reframe that as being a decision as to what should even be within sample versus out of sample.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5769.217,
    "end": 5773.746,
    "text": "And at each step, I'm just bringing more things into the sample if it's still valuable to do so.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5779.576,
    "end": 5783.603,
    "text": "Fraser, do you have any last thoughts or questions?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5784.865,
    "end": 5786.15,
    "text": " I have two questions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5786.231,
    "end": 5792.496,
    "text": "One of which is actually, I'm relatively familiar with sophisticated inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5793.37,
    "end": 5799.517,
    "text": " of it being a meta-consideration of not just beliefs about policies, but beliefs about what I would believe in certain states and so on.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5800.118,
    "end": 5806.045,
    "text": "Could you just very quickly recapitulate the potential advantages that this had in this particular situation?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5806.526,
    "end": 5808.588,
    "text": "That sort of went a little bit over my head, if you don't mind.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5808.629,
    "end": 5809.029,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5809.049,
    "end": 5819.161,
    "text": "So the particular place I was thinking about is, so imagine you don't use a sophisticated inference scheme, and you want to say, OK, I'm going to follow up after my intervention at two different times.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5820.203,
    "end": 5823.006,
    "text": "Where am I going to put those two time points?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5823.374,
    "end": 5831.282,
    "text": " And if you see that each point is effectively being a set decision, where do I put follow-up point one and where do I put follow-up point two?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5833.044,
    "end": 5841.053,
    "text": "If you're doing this in a non-sophisticated way, then the optimal point to put both of those follow-up time points is at the same point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5841.113,
    "end": 5842.915,
    "text": "And it's the point where I've got the most uncertainty.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5844.356,
    "end": 5847.66,
    "text": "However, if I take a sophisticated inference approach,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5847.978,
    "end": 5852.783,
    "text": " then I would say, okay, if I put the first follow-up time here, that would resolve my uncertainty.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5852.823,
    "end": 5855.286,
    "text": "My beliefs would be much more precise about that time point.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5856.007,
    "end": 5857.929,
    "text": "So the second one I should put somewhere else.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5858.99,
    "end": 5864.456,
    "text": "And that means you can start to think about this and plan this before you even start doing the experiment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5866.758,
    "end": 5872.524,
    "text": "It also, I mean, you can see them serially like that, but also it matters in terms of where you put them relative to one another.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5872.584,
    "end": 5875.167,
    "text": "So you might not need to put the first follow-up point right in the middle.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5875.535,
    "end": 5880.963,
    "text": " if it's better to have it slightly earlier to characterize that bit of the function so that you can put the other one slightly later.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5882.465,
    "end": 5892.62,
    "text": "So it's just having a sense of how each of these things would change my beliefs in such a way that I can then take account of that when dealing with the other choice.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5892.98,
    "end": 5898.268,
    "text": "And you could roll it out for a whole series of potential future follow-up points and sampling decisions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5899.531,
    "end": 5900.195,
    "text": " Right, right.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5900.215,
    "end": 5906.071,
    "text": "So that, I mean, that clearly does give us a sense of maybe higher order considerations than excellent stuff.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5906.353,
    "end": 5906.453,
    "text": "Yeah."
  },
  {
    "start": 5906.838,
    "end": 5915.574,
    "text": " My only other question was with respect to the issue of affording computational tractability and affordability and so on.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5915.594,
    "end": 5929.74,
    "text": "Had you seen any potential ins or avenues to maybe instead of things like sophisticated inference to use federated inference and sort of belief sharing approaches where perhaps we have several agents",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5930.294,
    "end": 5950.362,
    "text": " communicating amongst themselves in a joint sort of free energy minima minimization scheme with respect to this um approach i'm very interested on you know in in whether that would be applicable here and to what extent perhaps it would be um yeah just very interested on that issue no that's a very interesting point",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5950.578,
    "end": 5960.631,
    "text": " At a very simplistic level, I suppose being able to share beliefs when different agents have different vantage points is one of the most obvious examples of that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5961.512,
    "end": 5973.868,
    "text": "So if I know what's over here and you know what's over there, then there's no point both of us looking in both locations if we're able to share our beliefs directly about what's happening in each of those locations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5974.321,
    "end": 5982.75,
    "text": " So you can sample with much fewer data points or many fewer data points if someone else is sampling some of the other ones.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5983.952,
    "end": 5994.803,
    "text": "And by optimizing a shared generative model or passing messages that allow you to communicate, then that can be massively beneficial.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5996.726,
    "end": 5998.067,
    "text": "If you were to put a",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5998.334,
    "end": 6016.714,
    "text": " The clinical trial analogy on that, you might imagine that being equivalent to a multicenter trial, something where different groups across different centers or countries are jointly contributing to different parts of that sample space so that they can jointly optimize a model of how effective a treatment is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6019.31,
    "end": 6029.303,
    "text": " Yes, well, I had the thought as well that you can, and we touched on this, the issue of, you know, well, how to sample or, you know, where my, where my prize should be for where I sample and so on.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6031.567,
    "end": 6034.05,
    "text": "We can see how people have done it before and do that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6035.352,
    "end": 6040.178,
    "text": "You could have some sort of, you know, imitation scheme where you say, okay, well, I'm going to have a certain number of agents.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6040.579,
    "end": 6041.119,
    "text": "Yeah, exactly.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6041.179,
    "end": 6046.026,
    "text": "So yeah, that was just an interesting avenue that I thought was worthwhile to think about.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6047.002,
    "end": 6066.648,
    "text": " Yeah, and that idea of having habits passed down from others may feed into that as well, because, as we were saying, it may be that sometimes the computationally efficient thing to do is to adopt a policy that will always lead to the same sort of sampling, rather than have to compute that policy anew each time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6066.889,
    "end": 6076.722,
    "text": "So form a sort of amortized policy selection, which is effectively prior habits or biases or empirical priors over what I should do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6077.528,
    "end": 6083.175,
    "text": " that potentially resolve these problems without me having to even start to think about what I might see.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6084.217,
    "end": 6084.617,
    "text": "Yeah.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6086.259,
    "end": 6094.33,
    "text": "That really cuts down on, doesn't eliminate it, but very much delimits the problem of where do your priors come from for these certain things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6094.37,
    "end": 6094.89,
    "text": "Very good stuff.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6097.574,
    "end": 6105.544,
    "text": "Well, Thomas, do you have any last comments or thoughts or how do you see the active data sampling going from here?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6107.633,
    "end": 6111.988,
    "text": " That's a good question, isn't it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6113.533,
    "end": 6115.72,
    "text": "Where's it going to go from here?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6116.341,
    "end": 6121.989,
    "text": " In a way, this is the same sort of themes that have come up in a lot of active inference research.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6123.731,
    "end": 6129.158,
    "text": "I think it's going to be interesting over the next few years seeing how clinical trial methodology develops.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6129.439,
    "end": 6142.416,
    "text": "I think there's been a lot more focus in recent years on things like Bayes-adaptive clinical trials, and possibly some of these relatively old ideas about how to optimize experimental design might be coming back in those sorts of settings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6143.341,
    "end": 6156.883,
    "text": " particularly in settings where there's quite a rapid change in evidence and understanding of evolving situations, pandemics or epidemics and the like, these things become very, very relevant.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6157.444,
    "end": 6167.56,
    "text": "So it'll be interesting to see how things develop along those lines and whether they end up adopting approaches like that we've outlined here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6167.54,
    "end": 6174.912,
    "text": " But I think it's also, as a lot of what we've discussed has been, the sort of generic issues around how we sample data as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6177.116,
    "end": 6194.104,
    "text": "One of the difficult issues, I think, in a lot of machine learning approaches is the approach of dealing with very, very large data sets, which are computationally costly to acquire, costly to collect, and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6195.586,
    "end": 6211.722,
    "text": " and not necessarily the mode there may be in some circumstances but but in general may not be the most efficient way of dealing with the underlying inference problem um certainly not the most energetically efficient um so i think there'll be some interesting",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6212.765,
    "end": 6226.1,
    "text": " approach is thinking about how best to optimize that process and whether part of the process is not just optimizing the algorithms that do the inference based upon those data, but actually optimizing the way in which those data are selected in the first place.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6226.721,
    "end": 6230.926,
    "text": "Clearly, that's a lot of what we as living algorithms do and how we behave.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6235.271,
    "end": 6235.611,
    "text": "Awesome.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6236.172,
    "end": 6236.612,
    "text": "Fraser?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6238.988,
    "end": 6241.771,
    "text": " I only want to say thank you very, very much, Thomas.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6243.093,
    "end": 6252.483,
    "text": "This carefully crafted facade of my relative, you know, I'm extremely grateful to be here and it's been just amazing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6252.643,
    "end": 6256.187,
    "text": "So although it is early for me, but thank you very much.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6256.207,
    "end": 6258.77,
    "text": "It's been most of my life and I hope to do it again.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6259.571,
    "end": 6260.252,
    "text": "Well, thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6260.272,
    "end": 6262.855,
    "text": "I enjoyed learning about relevance realisation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 6264.837,
    "end": 6265.998,
    "text": "Thank you.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6266.018,
    "end": 6267.78,
    "text": "I may be so bold as to link something.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 6269.161,
    "end": 6269.604,
    "text": " Awesome.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6269.825,
    "end": 6270.73,
    "text": "Thank you again, Thomas.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6270.871,
    "end": 6272.52,
    "text": "Thank you, Fisher and Chris.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6272.762,
    "end": 6274.371,
    "text": "And see you all next time.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 6276.081,
    "end": 6276.302,
    "text": "Bye.",
    "speaker": "SPEAKER_02"
  }
]
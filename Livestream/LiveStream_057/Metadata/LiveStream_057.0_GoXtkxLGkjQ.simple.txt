SPEAKER_01:
All right, hello and welcome.

It's May 24th, 2024.

We're in Act-Inf live stream number 57.0 doing background and context video for the active data selection and information seeking paper and series.

So welcome to the Act-Inf Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

This is a recorded and archived live stream.

Please provide feedback so we can improve our work.

All backgrounds and perspectives are welcome.

and we'll follow video etiquette for live streams.

Head over to ActiveInference.org to learn more about any of the projects, including the live streams.

So today we're going to do together a background first pass on a very interesting paper from Thomas Parr, Carl Friston, and Peter Zeidman, Active Data Selection and Information Seeking from 2024.

In this video, we're going to introduce ourselves, talk about big questions, go through the keywords of the paper, then most of the sections, section by section, and as always with the dot zero, it's just like a first pass, and we'll look forward to speaking with hopefully some of the authors in the coming weeks, and also looking at what people ask about.

So, Christopher, let's introduce ourselves and go from there.

Thanks a lot also for helping in the .Zero preparation.


SPEAKER_00:
Happily.

Yeah, so I'm Christopher Bennett.

I'm a bioinformatics scientist.

I do a lot of data mangling, data analysis, and that sort of thing.

This paper was of great interest to me as we kind of go into this more data-driven era in making sure that with such large data sets that we have, making sure that we can actually select relevant data for any of our applications going forward, be it machine learning or whatever we're trying to do.


SPEAKER_01:
And I'm Daniel.

I'm a researcher in California and also was drawn to this on one hand on the applied side, the idea of more efficient and effective data sampling, and then on the more theory side, the connection with epistemic value information gain.

So here are some of the big questions.

Why don't you add some detail to this?


SPEAKER_00:
Absolutely.

So there's five major big questions that I had after reading this.

For the most part, it boils down to,

During our sampling, you can do sampling over time and sampling of different data sets in different ways.

Is there a way that we can intelligently select the data that we're going for the time that the time that we're trying to select?

Is there a way that we can understand how the time aspect of sample through time instead of just doing a. Like a dynamic.

or more dynamic, instead of doing a static, like we're going to do time zero, time seven, time 14, time 21.

Can we say, hey, the differences between time one and time two are very, time point one, time point two are very interesting.

It's a lot of data in there alone.

So we'll sample one and two, and then maybe sample 10.

Is there a way that we can intelligently select the time points that we are sampling from when we get into the time series aspect?

There's a number, the paper mentioned a number of different time dimension models that you can add to the core model that they're utilizing, one of which was a hidden Markov model.

Another was, they mentioned a differential equation in the actual model itself, in the equation itself.

Is there one, are there situations that one performs better over than the other?

or is what they have selected to use in the paper the optimal solution in most cases, if not all cases.

um you know that it in when it comes to clinical trials that was a little section um in this uh that they discussed um there's a lot of fda regulations of the clinical trials and it's very uh heavy red tape right now um is there a way that there's minimums ends that you need in many clinical trials to actually

be considered passing?

Is there a way that you can bound this model that they've developed into something that you can guarantee a minimum number, a minimum sampling that the FDA requires, or any regulatory body?

Another point is the next step of how are we going to integrate this in with other machine learning models or any downstream applications that you're going with?

Is there a selection method that we can, or how do you see these, this method kind of pre, kind of before machine learning?

How are we going to attach these things together so that we feed the right data into machine learning?

And then scalability and computational demands.

That's going to be a big one.

If this is going to be something that is used routinely in industry, you need to make sure that this is something that is as scalable as you can get.

You know, go from small scale, which is a lot of what they show in this paper, and then all the way up to the very large scale data sets that we

used to train MLMs and other models.

Those are kind of the five major points that I have.


SPEAKER_01:
Thank you.

Those are very insightful.

Here are some of the big questions that I was excited about.

So first, from a more general information gain, epistemic forging perspective, how do we model the implicit and explicit constraints or trade-offs or dynamics of information seeking?

which is often addressing a question that is left unaddressed in data science of where the data comes from it's just about doing analysis with the data that's there but even there as this paper will kind of get into there's still sub-sampling and all these other factors to consider the clinical trial example brings a very serious and very real plot twist into the paper

which moves through several levels of adding theoretical generalization and incorporating like the time dimension and other features.

And then the plot twist is when the preferences for certain kinds of observations is specified.

then there's all this interesting behavior and decisions that come into play.

So I'm sure that'll be a great discussion.

And then also in section four, they mentioned the streetlight effect, which is quote, the tendency to search where data are generated in a minimally ambiguous way

i.e.

under a street lamp compared to searching elsewhere on a darkened street.

And so there it's an interesting scenario and there'll be some fun art coming up.

And also how they distinguish the sampling method with the full information seeking from the maximum entropy sampling is a very subtle but very important point that I look forward to hearing more from the authors about.

Okay, so just to

summarize the uh paper is active data selection and information seeking 2024 thomas parr carl friston peter zeidman and just a few of the aims and claims of the paper and then christopher will read the abstract this paper aims to unpack the principles of active sampling of data by drawing from neurobiological research on animal exploration

and from the theory of optimal experimental design.

Our overall aim is to provide an intuitive overview of the principles that underwrite active data selection and to illustrate this with some simple examples.

Our interest is in the selection of data, either through sampling subsets of data from a large data set or through optimizing experimental design.

based upon the models we have of how those data are generated.

Optimizing data selection ensures we can achieve good inference with fewer data, saving on computational and experimental costs.

So if you could read the abstract.


SPEAKER_00:
Absolutely.

So the main points in the abstract are that Bayesian inference is typically focused on two major issues.

The first one is that you have to estimate the parameters of the model of the data.

And the second is that you need to quantify the evidence for alternative hypotheses and formulate an alternative model.

But this paper is actually looking at a third issue, which is in how you're going to select the data for your models.

um and either through sampling subsets of large data is typically used or optimizing experiments of design um based upon the models we have net these of how these data are generated

Optimizing data selection, what's going into the models, can achieve a very good inference with fewer data points.

So you're saving on computational time, costs, that sort of thing, by actually reducing the amount of information that you're putting into the model.

So what they're doing here is trying to unpack how you're going to actively select data, and I mean actively select data through a machine optimization protocol by drawing from some of these neurobiology concepts and trying to optimize the maximum information that the information can provide, maximum information gain.

So they offer overview of some basic points from the field and illustrates the application in some of the toy examples that they have that we'll go through, ranging from different approximations with basis sets to inference about how the process can evolve over time.

And finally, they'll go through and consider how the approach to the data selection could be applied to design of clinical trials in this case, and specifically Bayes adapted clinical trials, something that is more and more seeing headlines and it's more and more used today now that we have the technology to do it.


SPEAKER_01:
Great.

Okay.

For the roadmap, the paper begins with introduction section.

goes into Bayesian inference generative models and expected information gain they go through a simplest worked example and then consider a few more ways to level up that model with function approximation consideration of time dimension with dynamic processes and then bring in the preference for certain observations in the clinical trials then there's a discussion and conclusion and they also have a paragraph explaining their kind of logic there

The keywords for the paper were experimental design, active sampling, information gain, and Bayesian inference.

So the next slides are going to go into those four background topics.

After the four background topics, we'll speed through the sections and just plant a few seeds for what we want to explore more.

So first, experimental design.

Here's two kind of classical views of experimental design in the active inference and free energy principle eras.

So on the left is the statistical parametric mapping, textbook, toolbox, documentation, etc., has multiple chapters and kinds of analyses included in the package.

to specify and simulate and also to recognize data according to different experimental designs and one very hallmark or common visualization of these kinds of patterns of experimental design are these design matrices and it's just represented in this black to white gray scale

And it summarizes different kinds of measurements across different experimental settings.

Like here might be six settings in the larger white blocks.

And then there was variability within each of those trials.

And those are the smaller row levels.

So that's like where the data are collected.

And a lot of this has to do with the linear operations that can occur on this kind of matrix in a general linear modeling framework.

And then on the right is the experimental design

experimenter's perspective where the experimental stimuli they output as actions are the observations going into the subjective model like of the rat in the tea maze and then the action output of the rat is the observations of the experiment of the experimenter so there's kind of two different perspectives spm with more of a matrix multiplication fmri

optimal design and then active inference with the more general graphical bayesian modeling starting to broaden the consideration of what optimal foraging and what information gain epistemic value mean these are kind of the experimental design themes and how they connect a little bit with other experimental design factors want to add anything


SPEAKER_00:
Yeah, and keep in mind that a lot of these experiments, experimental design is a very big and very important consideration when you're actually running any sort of science or analytics of any variety.

And these experiments can actually get very large with,

huge huge amounts of data and not all of that data is relevant for every application that you want so you want to be able to design your experiment in a way that you can collect information in a intelligent way rather than trying to go through and just collect every data point that you can because humans in many cases are running some of these experiments and they have limited time

I certainly do when I'm running these things.

I have to be very intelligent in how I set things up and how I actually collect data and what data I collect.

In many of these cases, you only get one shot to collect the data.

You miss it, it's over.

You won't have that data point.

It's very critical that you actually take the experimental design seriously when you're setting these things up.


SPEAKER_01:
Great.

connecting that kind of experimental design, experimenter on a budget perspective with a more statistical and biologically

statistical based perspective, active sampling.

So they wrote, when we look at the world around us, we are implicitly engaging in a form of active data sampling, also known as active sensing or active inference.

So this is referencing the visual saccades.

And just to kind of highlight how extreme the relative acuity difference is,

between the sensor of the eye where the gaze is focused on and the off center among other visual changes and vision is just being taken as one sensory example here it could also be thought of as like looking for books within a library or any other kind of selection of what data are going to come in even if it seems like all of it is coming in that still is going to be uh perhaps addressed with a

different sensors that have different variability profiles or like there's different RNA sequencing kits that you could buy.

And so how do you balance the kind of more samples or which samples, especially as those spaces grow massive.

And then just to contrast that,

whereas digit recognition in a cicade based paradigm would focus on the motor patterns and the small centrally focused visual acuity and then the motor patterns that relate to cicading around a digit whereas in the kind of machine learning taken all at once approach

a matrix corresponding to like the pixels in the MNIST dataset are simply taken in all at equivariance level.

So that's just kind of taking in the data.

There's still another higher order data selection question of like, which digits do you take?

if there was a large number of digits in that library.

So this is active data sampling on multiple scales, which records you pull at all, and then how the resolution and all the trade-offs that are associated with using the data processing or making the experiment.

Yeah, add more though.


SPEAKER_00:
And keep in mind that when you're talking about something like the visual system, our visual system has access to untold amounts of information, but our brain can't

Take advantage of all of that at once.

There's a low energy usage of the brain that needs to optimize the relevant information.

Think, you know, your nose is right at the end of your face.

Your eyes are always seeing your nose, but your brain is filtering it out.

And this is happening all the time at all points in time.

There are literal blind spots in what you are actually capable of intaking and processing all at once.

And then additionally, when you're moving away from something like the eye or biological systems and into the experiment design itself, you know, you oftentimes can't run a full factorial design.

And there are other methods like a fractional factorial design, but those are random base.

And this is trying to actually talk about actively selecting how you're going to set up that design.

So it's kind of a, you can think of it multiple different ways.


SPEAKER_01:
Awesome.

The factor that's going to come into play as driving the active sampling is going to be the information gain.

And there's some quotes here and equation two is shown.

They write, we have conditioned our model upon the variable pi, which represents a choice we can make in selecting our data.

So data recognition, interpretation, analysis, and so on, it's often framed as kind of like an observation type or a sense-making type activity.

Here, pi for policy, as with usual, is being framed as a control or an active data selection policy we're applying to some data set.

So it adds an action element into this sequential epistemic foraging rather than just taking a large data set and just munching it like all at once.

It brings in this sequential question of where to sample and potentially updating where is informative to sample through time.

and the i of pi is the functional on that policy distribution or specific choice that can be decomposed all these interesting ways that we can explore more in the coming discussions what else would you add though about information gain


SPEAKER_00:
I think this is one of the biggest points in this whole paper is you're measuring how much information you are gaining in your model by adding these different variables in here and selecting different variables.

You're effectively automatically taking out or trying to remove things that have high mutual information that don't add as much.

So if you have parameter A and parameter B that are effectively just transformations of the same data, then you can easily remove one of those and still have all the information that you need.

So it's a really, really powerful way of saying I'm trying to optimize and maximize the amount of information that I'm adding into the model by selecting data that actually has the information that is going to improve the model.


SPEAKER_01:
Awesome.

One other interesting angle here is often in the control literature, utility, reinforcement learning, et cetera, the epistemic value component is added in.

Whereas in the structure of this paper, they start with the pure information gain perspective.

And then in the clinical trial, they bring the preference in.

So the pragmatic value comes in secondary to the information gain in how they build it up step by step.

Bayesian inference is the last keyword.

A lot of places to go.

Here's what they showed for equation one.

And they wrote, Bayesian inference is the process of inverting a model of how data, y, are generated to obtain two things.


SPEAKER_00:
marginal likelihood and the posterior probability so anything you want to say about bayesian inference or do you want to say something about bayesian networks and graphs i think that you've kind of covered it here and it's i think fairly textbook on this part yeah how about graphs on the bayesian graph side of it um there's multiple different ways that

these Bayesian statistics is done nowadays.

And the Bayesian networks and graphs is a really powerful method going forward.

I know that right now in the Institute, we have an RxInfer group working right now, which is a Julia package for actually just building these network graphs, these Bayesian graphs, and doing message passing between the different factors and the different nodes of the graph.

So this is a very big up-and-coming area right now.

It's very early in the timeframe that this is going to become big.

It's kind of on the upswing right now, and it's kind of, at least I would predict, going to be kind of the next big thing going forward in the next five years or so.


SPEAKER_01:
Yeah, we've been having a great epistemic time, and Livestream 55 explores some of this in more detail.

okay that was the background now on to the paper so first just to get the last part of the paper out of the way they have a github thomas pars github with the active data selection repo

and maybe in one of the upcoming discussions or somebody in the time between can explore and transform and play with the code.

And also all these different ways that we have fun discussions around the language of the active inference model and how building it in different languages or using different styles like is or isn't plausible.

These have been very fun discussions that help us get out what the core of the math really is.

and how that's independent of whether it's written in MATLAB or any other language.

And then also as it is simulated, it's written here in MATLAB.

And so that's kind of interesting.

Any thoughts on that or just like coding in RxInfer or


SPEAKER_00:
Yeah, I think that with RxInfer being, I think, relatively new on the scene, you have some of these other traditional approaches with MATLAB and PyMD and that sort of thing.

It'll be very interesting to see how these techniques evolve over time with packages like RxInfer really, I think, changing how we approach building these models and designing them.

I think that it's going to be even more critical now in this current environment to select the data intelligently going in so that you're not muddying up your models or having to build too big of models that might have information that's not as useful to the application at hand.


SPEAKER_01:
Yeah, great.

All right, section one, introduction.

So we'll try to hit on some of the key points.

i'll say something briefly and then feel free to add something if you want section one situates that inference and action cycle or loop or partition in terms of a statistician's job or process in modeling observations data as sampled and latent variables as models and the process by which there's

kind of snapshot or bulk summarization or generativity or and how it's possible to have a continuous resampling of informative data or how you even evaluate how data are informative in which way.

Want to add anything?


SPEAKER_00:
I think you've captured that very well.

I'm going to actually pull out my notes so that I can actually remember all the symbols.


SPEAKER_01:
y are going to be used for data and theta for the latent variables so distributions of data distributions of latent variables conditioned upon data coming in so that could be seen as just one data point sequentially or a big bulk vector coming in like all at once um just continuing to move through this section they wrote

careful data selection is especially important when we consider the problems associated with very large data sets of the sort that are now ubiquitous in machine learning and artificial intelligence settings so just to summarize a little bit or add a few notes that came up in the paper so other than this topic being

very fascinating and very integrative in terms of a unifying approach for information and behavior etc also this is definitely one of the active inference questions that has a lot of pragmatic relevance as dealing with with data sets of different kind is totally day to day and especially the way that even the examples specify important settings is very clear very direct though also the mathematics are very general about epistemics and

this motivation that they lay out in the first section about how if this challenge could be addressed, then there would be all these kinds of benefits that could be realized with current systems and data sets.

And then they provide the approach to at least getting there or towards it to optimize data selection, we first need to identify an appropriate optimality criteria.

And so they're gonna kind of go through several stages of with different generative models,

how that optimality criterion is defined.


SPEAKER_00:
Anything else, Ted?

And keep in mind that this is the expected information gain that they're talking about.

It's effectively how much do we think we're going to gain by adding this information in there.

And then you can, of course, train your model by looking at the actual information gain if necessary, and go through kind of a learning cycle.

But we're basing this all off of what do we expect to gain from this information.


SPEAKER_01:
right section two basing inference generative models and expected information gain in this equation three i won't read it all it models the markov blanket formalism in terms of upstream and downstream causal relationships in terms of messages that are passed along edges of a factor graph they introduce in this paper the lambda operator to indicate either summation or integration

So this is across continuous variables or discrete variables.

And we'll explore this more with the authors, hopefully.

Anything you want to add on equation three?


SPEAKER_00:
More that, you know, the information gain is a function of the data that you sample.

So depending on how you sample that data, you're going to get different information gain out of it, as you would expect.

And then you start to get into the message passing, which is that Bayes graph and factor graph, I guess, challenge going forward that construct.

When you build it in a factor graph model, you have to be able to pass the messages between the nodes effectively.


SPEAKER_01:
yeah and to kind of ground that in the data science situation if you have a latent state estimate and you're generating data generative ai synthetic data then the latent variable is upstream causally statistically from the data pseudo observation but that might be the actual real observation if you're interested in the computer model whereas the

data recognition case where the data are upstream of the estimate of a parameter like a risk score or something like that then the parent of the latent state estimate is the data so that's the recognition direction so this kind of covers the whole Bayesian update possibility spectrum in this essentially Markov blanket but it could be in face space or time or a few other situations they explore

all right three a worked example this section lays out the overall pipeline for how you get from the graphical notation of the bayesian network whether it's viewed visually graphically like with a variable dependency structure or whether it's just written out in terms of the plain text with the analytical

The Bayesian network is transformed into a factor graph, probably a constrained factor graph.

Discussion for another day.

On that graph, certain messages are passed at inference runtime.

Conditional and predictive entropies are calculated as part of the way that this system

outputs or is described by different probability distributions understand in a kind of statistical mechanical way in terms of entropy and then that is going to come together into calculation of the objective function which is the expected information gain which is basically conditioned upon the cognitive model of the sampler so just because the sampling is active data sampling doesn't mean that it's going to lead to like an adaptive

behavior it just means that where the learning rate is perceived to be highest informationally iteratively there is a ranking by which those can be uh which this the space of experiments can be ranked by and it can connect to pragmatic value in terms of epistemic and pragmatic coming together for the full expected free energy like in the clinical trial

Anything to add?


SPEAKER_00:
And you'll notice in this Tor example that they are discussing here, the factors that they have in their graph in each of their nodes is actually a cosine.

So that's why you get that kind of oscillation in that bottom plot there.

So you'll have areas of maximal information and then areas of minimal information just based on the Tor example that they have.

And this doesn't always have to be cosine, but in this example it is.

And so it just kind of gives you a really good graphical understanding of how your information gain can be viewed over a sinusoidal sort of oscillation.


SPEAKER_01:
yeah just to kind of double down on that if you sample right here on the number line or right here the lines are indistinguishable so the information gain is expected to be low

under understanding the parameter family that is being generated and sampled from which in this first example is the same same type of equations whereas where the functions are maximally distinct the information gain associated with reducing uncertainty about which one of those five

the the data point is coming from those are the informative points at the zero on the number line and far out that's where just perceiving one point uncolored would give you the most ability to even perfectly resolve which one of the five situations it was

so oh they're right in effect this model amplifies or attenuates the amplitude of the predicted data depending upon a periodic function of our data sampling policy pi so here the the policy distribution is like that kind of around the clock direction which is not a common setting but the the general idea of sampling amongst a finite set of alternatives where a control variable is going to result in the most informative

data point is a theme that is going to be expanded upon and also one interesting mathematical move once all terms that are constant with respect to pi are eliminated we are left with equation six so equation five comes down to equation six or maybe not exactly only five six but six has removed all the the the

variables that don't change as policy changes so if the question of policy selection is taken alone like gradients on policy updating then everything that's constant with respect to it doesn't come into like the delta pi delta something

So it just simplifies it down to only a function of policy.

And that just kind of reflects how like the sense making and belief updating component is partitioned off from the policy selection component here.


SPEAKER_00:
Yeah, you're looking for change in your belief based on the observations that you've gained.

So if it's not changing, it's not as informative in your information model.


SPEAKER_01:
Yeah.

Continuing on equation six there, which is modeling the policy dependent components of information gain as an objective function that ranks decisions about where to sample.

Equation six is a special case of the third row of table one, which highlights analytical expressions for expected information gain for a few common model structures.

As we might intuit, the most informative places to sample data align with those in which differences in theta lead to large differences in the predicted data in which our choice of pi maximizes the sensitivity with which y depends on theta.

So here are the categorical, the Dirichlet,

and other functions in terms of how they'd be written out in the probabilistic like specifying a distribution way and then how there's this relationship analytically to a related distribution which is an objective function that ranks the information content of sampling the likelihood distribution in a certain way

And that's closed form in certain situations.

And then also the explorer where it's intractable formally.

And so then that's where the variational approximation comes into play.

Anything to add?


SPEAKER_00:
No, I think that summarizes this slide.


SPEAKER_01:
Okay.

Section four, function approximation.

next turn to a generic supervised learning problem that of trying to approximate some function based upon known inputs and the observable outcomes they stochastically cause pretty general neural network or latent state observation setup um that information is composed and concatenated so so that there's a common

variable that's describing the statistical object that's going to be describing the inputs and the relationship with the observable outcomes and then that function approximation from sequential data in figure three is simulated with random but potentially you could call all of them random but this one is a flatter

or a less informed and iterated model of data sampling.

Just going to show that samples of random data with even this minimal non information gain driven model has a certain baseline prediction that's associated with certain choices about sampling sequentially from this generative model.

Want to add anything?


SPEAKER_00:
Yeah, it's just I like that they highlighted in this, that choice diagram there that you can actually get the inefficient sampling just by random that you start to rent, you can randomly select two things very close together.

And you've effectively, maybe not wasted a choice, but you know, not gotten the maximum gain from that choice that you could have.


SPEAKER_01:
Yeah.

They write a little bit more about Figure 3.

Figure 3 illustrates a depiction of this model as a Bayesian network and a visual representation of the data generating process.

Now they're going to bring in information gain.

So they write, this is where information gain becomes relevant.

into designing a more informed way to sample data than from a flat or a non-updating prior data sampling distribution.

It's like equivalent to having a policy prior that's fixed, which might be a heuristic in a certain space.

They write substituting equation seven into equation three.

So here's that.

um markov blanket parent child concept and here equation three is describing the policy dependence on the joint distribution of the observed and the unobserved and this is combined into equation 10. to show what equation 10 does in terms of now that we're sampling from this

distribution or like statistical distributions that this describes, they'll differentiate figure three from figure four, kind of like bring in this model change between those two figures.

Now samples are drawn from a distribution whose log is proportional to the information gain in equation 10.

So it takes the flat policy prior and in a fixed way has remapped it to be proportional to the information gain.

So here's three on the right.

and then four on the right and um the figure uses the same format as figure three but now each choice is sampled to maximize anticipated information gain and they point to some specific uh quantitative patterns but also like qualitative patterns so want to add anything on figure four


SPEAKER_00:
Just that now if you focus on those choices, because I really did, I think, like that choice plot there, you can see that the walks around kind of choices around your data space are a little bit more distributed, evenly distributed, a little bit less random, but you start to get, I think, a more cohesive sampling of the data without entire randomness, putting things too close together, putting selections too close together.


SPEAKER_01:
yeah okay continuing on uh well they set up the question as this raises the question as to how many samples should we collect so within a foraging bout where should one look and then at the kind of like pulling a layer back in the strategy when should you halt looking like if you have already sampled all three records from a data set

then unless you had some other reason you could fully stop sampling there but you also might want to have a softer stopping criterion that would relate to how much information you're gaining from continuing to sample in that way before like halting and so they include that by having like an exit policy in the state space of foraging possibilities

So how do you resolve that?

An answer to this question can be drawn from behavioral neuroscience and the so-called exploitation exploration dilemma.

And they introduced the notion of sampling costs to help decide that.

So this method is still going to require parameterization

situation-specific modeling of the relative costs versus the relative information gain.

However, at least there's an accounting that includes costs into the sampling equation to give any possibility of exiting.

Because if no costs are provided for sampling, then the model might just converge and continue to eke out very small amounts of variance explained.

if it doesn't explicitly have that stop option so they take the policy vector the list of locations that can be sampled from and adds a zero element which reflects the information gained if we were to stop sampling and then there's a preference over those observations

expressed in the C vector, preferences.

And this brings in the information seeking and the cost-aversive imperatives into the same objective function in 11.

Anything to add on 11?


SPEAKER_00:
Yeah, eventually the idea is that it just gets to a point where you're no longer, whatever you set your kind of stopping energy to be, kind of your breaking energy, eventually it's just going to get to a point where the model just says, hey,

I've reached what I can.

You've set this.

You're not gaining anything beyond this point.

We can just stop at this point, which is nice, since in the random selection case, there's not necessarily a stopping parameter.

As Daniel mentioned, you could continue to get eke out very, very small marginal changes, but you're not going to gain anything else, and so you're just spinning your wheels.


SPEAKER_01:
way of saying hey i've reached kind of an inflection point of data gain i'm done so figure five they're continuing in this genre of three four five and now they've added in to the policy decision which which has an upstream dependency on the data that's the active policy edge they add in this

uh cost to sampling so we can explore more however it now includes not just the information gain driven choices within the trial but it includes a specific probabilistic but decisive stopping point for that trial as parameterized by how sensitive it is to information gained and preference

so this is one of the most interesting parts and and discussions in the paper they they ask it out loud a reasonable question to ask at this stage is why bother with the full information seeking objective and basically how does this differ from maximum entropy sampling and um

let's look forward to the authors or other guests but here's just a few notes on this because i think it'll be a great place to explore what it really means to do a statistical and physical modeling on cognitive systems

they directly contrast maximum entropy sampling and their whole information gain family against each other and then the rebuttal is in figure six so just to show figure six six for a second the measurement noise increases in variance from the center of the function domain so the the variability profile of the function is non-uniform

This means the amount of unresolvable uncertainty is heterogeneous through the domain of potential sampling.

So in some kind of ways of thinking about what they're really getting at and just putting this out as a speculation or starting point for this key technical point.

if there were a case where the latent states were equivariant they had iid variability profiles then sampling the most variable sensory data is the most informative like if you're taking a picture of a solid black image

then sampling from the noisy pixels is going to potentially provide more information gain you're reducing uncertainty more about something it might be overfitting but you can select as a heuristic wanting to sample from where variability is high at just kind of a first pass layer

however as we start to think about richer or more specified statistical patterns generative models there become dependencies that are sparse but important amongst all different kinds of things so things that are variable from a sensory perspective provide high information gain potentially to

one part of a generative model like a screen and static but then other events might be less variable from a sensory perspective but smaller differences even in that variable relate to some other component of uncertainty resolution from some other component of the the model like those are going to be the cases where

cognitive modeling does differ from just dispersed decision-making.

However, they're both going to result in dispersed decision-making profiles, like looking at the choices in the figures.

But the choices to sample from the less ambiguous parts of the actual distribution, that leads to a much narrower policy path

in this cognitive control setting versus in a variability sampling where it would go for the areas that were just more variable but not necessarily providing more information question mark on this with the maximum entropy or anything


SPEAKER_00:
And I would even highlight on the next slide, effectively what it is doing is accepting that you're not going to gain a lot of resolution in these highly variable regions.

And so you don't really have to sample into those deeply because

you've accepted that it is variable it is not something it is inherently variable in the data we're not going to gain a lot of information from these regions um and i it's highlighted in blue down there and i think that's one of the big highlight notes of this figure is this less information gain available in these highly variable regions and that's something that makes this method more robust and powerful when you're dealing with some of these non

uniform variable data.


SPEAKER_01:
Yeah, awesome.

And then the streetlight effect is brought in there.

So the avoidance of sampling in ambiguous locations is sometimes referred to as a streetlight effect.

The tendency to search where data are generated in a minimally ambiguous way, i.e.

under a street lamp compared to searching elsewhere on a darkened street.

So I made some GPT-4-0 images, some fun streetlight.

And on one hand, there's kind of this sense of like, is it constraining to look under only the streetlight?

Isn't that kind of absurd?

And then there's the joke about how what the person's looking for is elsewhere, but they're still searching under the streetlight.

they're looking for something they they know is elsewhere so that's the kind of tragic element of it then there's this limited element however there's also this realistic element which is like well are you supposed to search where you can't sense or outside of where you are at that moment so how could you

you know say that that wasn't just and then this paper is more framing it as just a general condition of perception like you're in your tactile street light that is the part you can see at all you can have latent modeling of any and other things but if it's not grounded in some way to a measurement made in a street light under the metaphor where the light allows for observation

then you're not connected to data unless you're connected to that streetlight so that's just a very interesting kind of topic and reference that the authors use what do you think


SPEAKER_00:
Absolutely.

I mean, it kind of boils down to you can't know what you don't know or you can't observe.

You know, if you can only observe what's underneath the streetlight, then you can't really know what else is outside of there.

And so your inference necessarily should be constrained to what you are able to actually observe.

You can't observe the unknown.

And so not necessarily in this case, because you don't even know if it even exists.

You have no data to confirm or refute it.


SPEAKER_01:
All right, section five, dynamic processes.

So in that previous example, there was a data selection challenge, whether it was approached from the flat, fixed prior, or all these other kind of subsequent variants with adaptivity and or with the cost.

Now there's going to be a time element brought into the underlying generative model.

We'll just go quickly here because that's the big point.

They take the static distribution that was sampled from and now give the underlying process also variability through time.

So this is like a very SPM, brain, latent state, causal modeling type set.

Yeah, anything you wanna say on that before we go in?

Oh, no, go ahead.

Okay, okay.

So they consider processes that evolve in time.

Equation 12 can be interpreted similarly to equation 8, in which the expectation of the data is treated as a function approximation, which now includes a time argument.

So here was 8.

Expectation of the data given latent state parameterization and policy equals so on.

And then here, there is

data also being a function of parameterization and policy and then also bringing in an element with a subscript tau for time uh then you mentioned in your big questions the different approaches that they raise here with the three ways to bring temporalities into a model so let's definitely talk about that but just to show their images

7 and 8 are the pair for this dynamical section so figure 7 shows a graphical representation of the matrices involved in generating our data and the inferences obtained after sampling so here it is sampling from a time variance function

And then figure eight goes into more detail and notes predictions based upon current data can be used to inform predictions about nearby spatial locations and to predict and post-dict the values of the function at different points in time.

so just like you could have a 2d plane grayscale and infer the location of the street light by pursuing like a gradient up the light and then there would be this optimal sampling like if you just got one observation you would want to sample on a line that was orthogonal to the one that you couldn't resolve

lots of ways to think about this sequential prediction but now the underlying landscape also changes so there's some temporal dynamics and then that can be fit with all these different time series models and autocorrelation and so on however that's specified statistically in the generative model but this section just shows however you do make a statistical model for time it's basically going to be the same thing where information

is gonna be drawn from a distribution and now time is a variable in that distribution.

They write, in this in the previous section, we have demonstrated the way in which smart optimal sampling may be used to select data in a manner that balances the cost of sampling or performing further experiments against the information gained from those samples or experiments.

Each of these examples has relied upon relatively simple and analytically comfortable linear Gaussian systems.

Next, we address active sampling in a situation where analytical solutions are no longer possible.

So to highlight the key

formalisms that they're working with in that kind of background section or setup section they kept one thing constant which was that the generative model the generative process or however it's considered with the family of equations that the agent is inferring and tracking hidden states with and that being the same as the actual family of equations that's generating the function

of observations and here that is relaxed so that opens it up to all empirical settings where you can just say right off the bat we do not have access to the generative model of those data so we're making a map statistical map with all the associated trade-offs and statuses of like that genetic data or that transcriptomic data all those different kinds of data sets

starting from a position where it's going to have to be statistically approximated and it isn't going to be based unless explicitly otherwise on actual knowledge about the causal elements of the system any thoughts on that


SPEAKER_00:
Well, in something else that they noted in the paper, by adding this time element, when they're actually going through the time series, the model itself will preferentially select different data beyond what it just recently selected.

So if time point one, it selects X and Y data, time point two, it might select L and M data.

So it actually will go through and select different types of data, and it'll take a little bit of time.

What the time is can be variable, but it'll take a little bit of time before it revisits some of that previous data at a previous time.

So by this, you kind of have a sliding window of data that you're selecting over different time periods.


SPEAKER_01:
Yeah.

All right, that's all going to come to play in this clinical trial, which is the big final contribution section of the paper.

In our final example, we demonstrate the potential utility of the ideas outlined above in the context of a more concrete example.

So they model the statistical setting here as an adaptive Bayesian clinical intervention methodology experiment.

For example, the kind

that was done during the 2014 West African Ebola outbreak.

The active sampling approach advocated in this paper offers two main opportunities to augment adaptive trial designs.

First, it allows us to adapt the design to maximize the information we obtain about treatment efficacy.

So that's the pure sense-making information gain learning sampling from where it's informative, not from where like we habitually or prefer to look.

and then second to balance and bring together that information gain with costs and that was brought in with the cost of the sampling section

which was done in this paper by adding the stop policy option which can be probabilistically selected and then as other sampling locations become less informative or if somebody was just sampled and you know that there's a slow decay through time then on that subject the stop policy cost would outweigh the information gained from an experiment

And this is also, I think, will be a very interesting discussion.

This blurs the line between clinical trial and public health intervention and can be seen as analogous to animal behavior that is never fully exploitive or explorative, but is a balance between the two.

So how do we think about that in terms of biomedical and health security and all these different topics?

And any thoughts on this before we go into the formalism of the

clinical trial, just think about clinical trials or anything.


SPEAKER_00:
Yeah, and I think that that's going to be like that last point there is going to be a big one going forward.

It's like, how do you balance benefits to the patient, benefits to your trial, benefits to essentially the company?

Like there's a lot of different benefits and costs that you have to weigh into this.

And so these models are going to get very complicated when you start to distill this into something, especially with health related.

So it'll be very interesting to see how this evolves.


SPEAKER_01:
Okay, so here's how they do it.

Our setup is as follows.

For each new cohort of participants, we decide upon the randomization ratio to adopt.

That's the orange subscript R of policy.

So this is policy on a randomization ratio.

there's three options so this is a discrete but linearly ranked not fully categorical policy decision where one half would be the 50 50 sampling between the two groups whereas

you know a priori that sampling in a skewed ratio is going to be less informative.

Like if you sampled only from one, you would obviously be maximally uninformed about the other.

However, what's going to end up being reflected in the policy decision to shift to a one-third or a two-third

which is focusing observations on one branch of the study more than the other is going to focus on the explicit quantitative preference for observations of survival so that's going to be very interesting to see how the time variable which relates to the experimental design but by way of modeling the death curves of the participants and how different preferences for

complementary processes of reducing uncertainty about the treatment-specific death curves and not preferring to see death observations because that would introduce the pragmatic imperative to measure low survival experiments.

So there's a lot of complexity in there from the public health side, also in this very simple and interpretable way that like, this is like a Bayesian light switch with 50-50 information seeking mode, or tilt it one way or the other to bias observations.

Whereas if no information had to be resolved, then the policy selection would orient towards observing long survival.

Whereas if that was somehow changed, then it would have to be adaptively sampled on the fly and changing these ratios and all that.

What do you think about this?


SPEAKER_00:
Yeah, and what we're going to kind of get into is, especially with these sorts of health decisions, you want people to survive.

Like that's your primary goal in a lot of these.

You want to see an effect.

You want to see a positive effect of your treatment one way or the other.

You know, if it's the placebo that's the positive or it's the actual treatment that's the positive, you want people to survive.

So this is kind of getting into the ethics of making sure that

when you design these things, that you're doing the maximum good to your participants who may not have much hope to stand on during some of these crises or epidemics or whatever they are experiencing at the time.

So you want to design this in such a way that

You keep them, the patients in mind, that is the whole point of this.

And so by having a Bayesian kind of preference and bias to keeping the patient alive and the best outcome, you are maximizing how the patient's outcome in the patient's life.


SPEAKER_01:
Thanks for adding that.

Another point to make, this is from Figure 9 that's going to come up, but it really highlights how sparse and few and interpretable the Bayesian graphical formalism is.

And message passing, which a lot of the equations describe and the discussion about Rx and Fur touched upon, message passing gives procedural ways to implement this in computational systems, because it's sometimes hard to go from the simplicity of like this graphical model to fitting iteratively on complex data sets.

But it's pretty clear to see how different

variables are upstream or downstream of other variables and also how the time sampling can be shown to be which is upstream of data sampled as these other factors are but it has a separable interpretable calculable epistemic value that doesn't have a certain kind of connection to randomization ratio for example

So being able to have explicit statistical calculations and directednesses where the follow-up time doesn't influence the treatment group ratio or the randomization ratio or other processes gives a type of interpretability that the generative model gives us the equations for

And then the pragmatic challenges are about actually implementing that.

And then even if the computational component were totally addressed and abstracted away, that would basically center these broader questions, which I think the health examples are great, like jumping off point four.


SPEAKER_00:
Yeah, and you have probably recalled from all the other different figures, despite how simple this figure is, the other figures, the plots were very basic, the models themselves, like you just had the three nodes converging on the Y.

So despite how simple this looks, you are adding more complexity to these systems.

And the more complexity that you add, the harder it is, the more computationally intensive it is.

And so this is that question of how big can you go?

How many nodes can you add?

How many parameters can you add?

How much complexity can you add to the system before it starts to break down or not perform as well as you would hope?


SPEAKER_01:
yeah so other than bringing in that randomization ratio kind of expression of preference

this model differs from the prior one in that it's defined that the kind of cognitive map is not the territory they're different families so that's what motivates this approximation approach so this is a simple displacement where still it's a trackable problem as they'll unfold however the simulation family chosen for the for like the approximation basically the approximation could apply to any data set

but it might be woefully inadequate like it might fit only one component of it so that's again part of the interesting question is like how similar does the statistical model have to be or what information does it really bring in and how

to to model or work with the empirical side um but just on a more general statistical level uh equations 15 16 17 describe some of the technical details of the incremental optimization gradient scheme the newton optimization variational applause we'll talk to thomas at all figure nine

is displaying the kind of before picture for the randomized control trial so here's where that graphical model is that was shown earlier and then here are these two groups and their survival through time and different sampling choices that are made then just to to jump to figure 10

has the same layout as figure nine, but now using the expected information gain from equation 18 to guide sampling of data.

So this is just to show the impact of that active data sampling, and then we'll drop back to the equation.

There are some notable differences between the choices made in figure 10 compared with nine.

Nine choices, 10.

The most obvious of these is that the follow-up times selected have been moved later once optimal sampling is employed.

makes intuitive sense as a later follow-up time is informative about the survival probabilities at all prior times whereas an earlier follow-up time is not informative about survival probabilities at later time um where it gets in the final uh simulation brings in the random sampling plus the preference element

here's where the symmetry is broken to also want the measurement of survival as more likely than death which is how the preferences are specified in active inference and then the policy switch is reflected in this like part where the observations are shifted later because there's less than a threshold of information to gain by having them earlier and then they

even if there is equal variance I'm not exactly sure we can ask between the two branches there's an over sampling for the group with the higher survival which in this case was the placebo group so anything to add on this


SPEAKER_00:
Yeah, I think this is the crux of the whole making sure that you optimize the patient's outcome on this because the treatments in this scenario were not beneficial.

They're actually harmful.

And so throughout the course of the study, by utilizing this model, you actually randomized more people into the placebo group, which caused a

greater survival of these uh individuals and so you're you can already see the effect that this sort of methodology has on clinical trials because you're optimizing the outcome and i think that is exactly what you want to do in these health decisions in these trials in these things that impact human health


SPEAKER_01:
or humanity in general you want to optimize the outcome uh and you know in this way you're actually reducing the overall harm to patients yeah interesting um here's the specification for the information game so uh bringing in the the

form of the message is required for equation three now with all these extra components that have been added in with time variability demographic and the sampling they write out some of the technical details for the approximations in the variational Laplace and then some aspects about those models which we can ask about but figure 11 basically shows the big

change which is that as you go from having a flat sampling distribution across time and across treatment groups you can actually do better than that basically by choosing a sampling regime that makes sense given the costs of sampling

So that's just very interesting because it really does look like given the possibility for these two lines to diverge, their divergence would be largest later on.

Whereas if you could only schedule like one check,

for every person if it was something that was expected to happen later in life then sampling all the young wouldn't even make sense so then one approach is like flat sampling but that is kind of sometimes erroneously called uh like unbiased or or uninformative but it is very informative and then this is pointing towards how there can be better sampling

than just trying to go flat across the entire latent state estimate if there are priors relating to something they can be leveraged as part of the probabilistic sampling and adapted to the data set at the in the end however for picking prior families for the active data selection

that's a big question about how much it will change how the algorithms work.

So I guess that kind of takes us to the discussion.

The paper's focus has been on illustrating how we might make use of information-seeking objectives, augmented with costs, which gave kind of the exit criterion, or preferences, which gives the biased, pragmatic part of data selection, to choose the best data to optimize our inferences.

and they highlight that the maximum entropy would yield identical results in several of our examples so that'll be interesting like what were the examples where

maximum entropy and the information gain are identical and then what are the real world or the statistical settings when the variance around predicted outcomes is inhomogeneous how does the full cognitive epistemic model based objective do differently than the max and

distribution dispersal kind of null hypothesis.

Any thoughts on this?


SPEAKER_00:
I think you've captured it very well.


SPEAKER_01:
Okay, then there are several technical points worth considering for how we might advance the concepts reviewed in this paper.

So let's talk about each of these.

One, refinement of the active selection process.

Two, empirical evaluation of active versus alternative sampling methods.

And three, identifying the appropriate cost functions.

So we'll talk about those coming up.

Conclusion, here's the entire conclusion.

The key ideas involved an appeal to foveal like sampling of small portions of the total available data to minimize computational cost.

That's a very cool way to put it.

And it highlights that kind of like sequential scanning, but also opens up some very exciting directions about how efficient that could be for some but not other kinds of problems.

and how those problems could be identified or those patterns could be filtered for to where different kinds of circadian models would be adaptive or not.

So like these are all fun discussions we'll have.

And then they kind of brought all the theoretical components together at the end with the Bayes adaptive clinical trial, with the costs of sampling, constraints on sampling, and also the preference for survival.

what are your overall thoughts or what are you excited about for the ones to come?


SPEAKER_00:
I'm excited to kind of see, I mean, this is a fairly recent paper.

I'm excited to see kind of where they have gone since this paper was published.

You know, they had a number of kind of next directions.

I would love to see what are those directions that they've taken, what they've compared it to, other sampling techniques.

um this show shows a lot of promise going forward for very complex and you know ethical uh situations or situations where ethics are going to be a huge component so

kind of where that is what they're going into and kind of where they see you know further improvements i still kind of want to know what would happen or what of these other time metrics um or what other situations these time metrics uh would our alternative time models would be applicable to or if this really is like

the de facto, just the way it needs to be done, totally, totally fair.

I think that could very well be the case.

I would just love to hear exactly if you did a hidden Markov model, how would that look?

Is there a benefit of that over the selection that they have?

Does it provide more or less versatility in the models?

These are things that are going to be, I think, very interesting going forward.

And then how do we

like you noted in the discussion, how do you optimize those cost functions?

What's a cost?

You know, you're dealing with clinical trials, it's a human life, that's a cost.

Time, it's a cost.

But there's also just computer time.

How much you can actually get, how much compute you can get in a given moment of time.

Some of these machine learning models that you might want to apply this to or, you know, select data going into, sometimes takes a long time to train.

How are you going to sample data that's going into those models?

How are you going to continually feed those models appropriate data going forward?

So this is a huge broad category of directions that you can go.

Clinical trials was a very wonderful example of a complex situation that is, you know,

right there applicable to human life.

But then you also have just data science in general.

How are you going to utilize this to going more broad?

How are you going to utilize this just going forward in any data sense?

Data is growing.

It will continue to grow.

It will never really stop growing.

So it's going to be more and more important going forward to have these methods more broadly used.

And random's nice.

Random's really, really good.

But this holds a lot of promise to being... I would love to just hear their thoughts on where that's going, where they see that.


SPEAKER_01:
Yeah.

A lot of interesting directions.

A few things that made me think of, one was about search and about relational search concepts, page rank and everything, syntactic, semantic, new kinds of search algorithms and personalization.

for search and learning and updating and to what extent like explicit cognitive modeling would change the way that different recommendation algorithms or different kinds of um computer systems would work i'll read a question um and then any any other questions otherwise this is our last slide so thank you christopher um okay glia maximalist wrote

Interesting point about biased and unbiased sampling schemes.

Perhaps this points out the fact that unbiased approaches are the wrong thing to strive for in research study design.

What do you think about that?


SPEAKER_00:
Unbiased research in study design.

I think there's a time and place for unbiased, and I think there's a time and place for bias.

I think that when you accept a bias into your model or take bias out of your model, you need to understand why you're doing that.

Certain bias you don't want to have, researcher bias is something that's a huge bias that you want to not have, for example.

But in certain cases, like in this paper, where you actually do want to have a bias, you want to make an intelligent decision, note while you're making that decision, call it out, and then build it into your model.

That would be my thought.


SPEAKER_01:
Yeah, that's interesting.

Like there are certain statistical distributions, the bias or the constraints on which define the research study, whereas other ones that can have an explicitly, strictly negative bias.

like data loss or something but then the trade-offs of how that distribution actually interacts with others can enter into this more complex experimental calculus that relates to like well all these different experimental factors and so the optimal experiment for for different labs or different moments for the lab could look extremely different and that's going to be the case

There's going to be dispersed behavior either way, but then the question is, how is that actually driven in a way that is doing better than drawing from distributions?

However, even that does interestingly well for the right variables.


SPEAKER_00:
And you can mine bias all over the place.

Bias in data design in experiments can be very useful.

So it'll just be interesting.

I think it'll be case by case is the way I see it.


SPEAKER_01:
OK, well, do you have any last comments?


SPEAKER_00:
I think that the main thing here is I'm just very excited to see this paper come out.

I would love to see how this is going to evolve over time, see if this can be applied to different technologies, different areas.

I'm really excited just to see where this is going, because I think this is just right on the cusp of what's needed.


SPEAKER_01:
Awesome.

Thank you.

Okay.

We will look forward to it.

Thank you.

See you.

All right.

Thanks, guys.
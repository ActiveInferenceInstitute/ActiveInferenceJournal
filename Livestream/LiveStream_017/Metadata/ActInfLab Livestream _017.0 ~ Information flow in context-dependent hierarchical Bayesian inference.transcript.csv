"Speaker Name","Start Time","End Time","Transcript"
"Speaker 1","00;00;07;01","00;00;28;12","Hello and welcome everyone to the Active Inference Lab and to the Active Inference Livestream. We're here in Active Inference Live Stream. 17.0 and it's March 3rd, 2021. I'm Daniel and I'm here with two of my colleagues who can introduce themselves blue."
"Speaker 2","00;00;29;25","00;00;31;20","Hi, Sarah. Go ahead."
"Speaker 3","00;00;33;17","00;00;46;29","My name is Sarah Davis. I'm a person who's interested in things. I'm a student in this university, in a master's program, in philosophy of science. And I've been an engineer in a bunch of other things, so I'm just generally interested in how it's all connected."
"Speaker 2","00;00;49;04","00;00;53;02","I'm Blue Knight. I'm an independent research consultant based out of New Mexico."
"Speaker 1","00;00;54;08","00;01;16;17","Cool. Well, we're here for a fun discussion, so thanks both for joining at the Active Inference Lab. We're an experiment in online communication, learning and practice related to active inference. You can find us at our links and socials this is a recorded and an archived and a hastily produced livestream. So please provide us with feedback so that we can be improving on our work."
"Speaker 1","00;01;17;04","00;01;49;00","All perspectives and backgrounds are welcome here and we'll follow good livestream etiquette. So here we are in paper number 17 and we've just completed our first quarterly Active Inference Lab Roundtable. So check that out on our channels if you haven't. And today we're going to be setting the context for 17.1 and 17.2. The paper that we're discussing in 17 is information flow in context dependent hierarchical Bayesian Inference by Chris Fields and James F Glazebrook."
"Speaker 1","00;01;49;20","00;02;32;07","It was published in October 20, 20. And what's funny is that the Dot zero videos have always been about context, setting, context and we even wrote in the previous videos the video's introduction and its context. It's not a review of the final word. And then it got really better with this paper because it's a paper about context. The punch line or a punch line of this paper is that we can integrate various approaches from mathematics mostly to achieve a scale free formalism that leads to an interpretation of nested systems where communicating systems are engaged in active inference and distinguished by informational or statistical patterns, or they're engaged in something that's related to active inference, let's"
"Speaker 1","00;02;32;07","00;03;08;11","say, because it's pretty out there with what it is, and it's a challenging but also a fun paper. So we're going to start with the goals and the claims in the abstract. And at that point it might be like, Wait, what with the roadmap headers or with the abstract claims, it's like driving through a country where you don't speak the language perhaps, and that's OK because the bulk of this video in the keywords are going to be going through and kind of unpacking a lot of the key terms that are going to matter for even just understanding the lay of the land."
"Speaker 1","00;03;09;00","00;03;33;20","And then we've pulled out a few other key topics. Some quotations didn't go as much into the formalisms of the paper itself, but we're looking forward to doing that with the authors in 17.1. And so we're just looking forward to that in 17.1. And two, we'll be discussing the same paper. So read it and save and submit your questions and get in touch if you want to participate."
"Speaker 1","00;03;33;20","00;03;55;23","And also we're looking forward to your chats during this presentation, giving us a little support along the way. So here we go with the goals and claims of the paper under discussion. So here are the goals and the claims. So maybe Blue, do you want to read the authors goals and claims sure."
"Speaker 2","00;03;56;04","00;04;24;00","So the goals for the paper are twofold. First, to model intrinsic or true contextual lity using the general general category theoretic methods of choose basis and channel theory. And second, to employ this formulation to reconstruct hierarchical Bayesian inference in a context dependent way it doesn't say Here we have formulated an approach to intrinsic context duality in general category theoretic terms."
"Speaker 2","00;04;24;10","00;04;32;05","A set of observations exhibits intrinsic contextual city. If no coupon can be constructed over the observables that produce them."
"Speaker 1","00;04;34;13","00;05;01;10","Full cone, toe cone, who knows how would say so. They're doing a couple of things here. They have two main goals, so it's great that they're laying it out. Clearly, the first one is about context, and they want to be having a more advanced way to talk about context. And it turns out that they're going to be doing that using category theory and choose basis and channel theory, all of which we're going to be going into at least a little bit."
"Speaker 1","00;05;01;28","00;05;22;10","And then secondly, they're going to be linking all of this stuff that they just did up here with context theory, category theory. And they're kind of by combining that with context dependent Bayesian inference, we've talked a lot about active inference and Bayesian inference. So this is sort of the hinge where we connect some of these ideas to active inference."
"Speaker 1","00;05;23;03","00;05;58;08","And then this is a quote from their conclusion, which is that they have formulated a new way of modeling contextually in very general category theory terms, specifically relating to this code cone construction. But the category theoretic term allows them to move between a few different areas that we're going to be walking through today. So here we go with the abstract I'll read the first of the three slides of the abstract, and anyone can give any thoughts or just start reading the second part of the abstract."
"Speaker 1","00;05;59;11","00;06;21;18","Recent theories developing broad notions of context and its effects on inference are becoming increasingly important in fields as diverse as cognitive psychology, information science and quantum information theory and computing Here we introduce a novel and general approach to the characterization of contextual ity using the techniques of choice, basis and channel theory. View this general theories of information flow."
"Speaker 1","00;06;22;13","00;06;44;10","This involves introducing three essential components into the formalism, events, conditions and measurement systems. Any thoughts? Either of you two or someone can just start reading the second part of the abstract Let's go to the second part of the abstract. I think we'll have a lot of time for off the cuff remarks soon."
"Speaker 1","00;06;46;11","00;06;48;25","So, Sarah, do you want to go for the second part of the abstract?"
"Speaker 3","00;06;50;02","00;07;17;16","Sure. Incorporating these factors in relationship to conditional probabilities leads to information flows both in the setting of two spaces and channel theory. The latter provides a representation of semantic content using focal logistics from which conditional URLs can be derived. We employ these features to construct a cone cone diagrams come utility, compute activity of which and forces inferential coherence."
"Speaker 3","00;07;18;20","00;07;28;15","With these built a scale free architecture incorporating a Bayesian like hierarchical structure in which there is an interpretation of active inference in Markov blankets."
"Speaker 1","00;07;29;25","00;07;32;22","Nice and then blue. How about finish out the abstract."
"Speaker 2","00;07;36;01","00;08;07;14","We compare this architecture with other theories of context reality, which we briefly review. We also show that this development of ideas conveniently accommodates negative probabilities leading to the notion of sign the information flow and address how quantum context duality can be interpreted within this model. Finally, we relate contextually to the frame problem, another way of characterizing a fundamental limitation on the observational and information inferential capabilities of finite agents Perfect."
"Speaker 1","00;08;07;27","00;08;35;20","So that is the abstract. That's how the authors wanted to present and communicate their work to a broad audience. Of specialists and non-specialists. And again, we're going to be going into basically all the words that might be non-obvious in this abstract and the keywords that come into play. That's what we wanted to be unpacking today. So we're going to look over their roadmap because just like the abstract is how they want to communicate what they did."
"Speaker 1","00;08;35;20","00;09;17;05","The roadmap is how they've chosen to organize what they've done and get us from A to Z. And then we're going to be really going into this road trip and unpacking what some of these key terms are. It starts with an introduction, which I guess is helpful context. And then one of the focal ideas of the paper is this new space or the new construction, which we're going to be talking about, the history of some examples, some related topics, and just right off the bat, when we saw that the examples of new spaces and shoe morph isms included topological spaces, such all of network theory and graphs and things like that."
"Speaker 1","00;09;17;24","00;09;47;17","So that's one big area. But we're going to be talking about two spaces as something that connects all of network theory and topological space to types of processes. Probability Spaces. Bayesian belief networks, event space structure, shoe flows, callback ladder divergence and free energy. So we're going to be popping up a level from things that are already pretty abstract, but it's going to be done in a way that's quite useful, especially for formulating context."
"Speaker 1","00;09;48;02","00;10;12;08","And it's actually really interesting because a lot of times people will say that these topics in the examples that they're general, but they actually don't have enough context. For example, you can make a social network, but that wouldn't be the total context of the world if you only have some of the users on the social network. So maybe there's a way where we're going to be able to step up from some of our previous conceptions and introduce contexts potentially by default."
"Speaker 1","00;10;13;04","00;10;48;28","This leads the authors to a discussion of probability and how to use probabilistic models for context, duality for model and context. And this comes to a recent lead derived framework called Context Duality by default, which will be a topic which we dove into. Section four goes into some issues related to measurements and kind of the before, during and after of measurements, because you got to set things up and calibrate and then you got to make the measurements at a certain moment, and then you have to analyze it in the context of a certain system."
"Speaker 1","00;10;49;05","00;11;18;05","So it's like before, during and after the measurements in section five, they head into channel theory and information channels. And this was really fascinating because it's quite distinct from Shannen information theory in some ways that people might not expect and I had never heard this discussed alongside Shannon information theory. It's like, here's information theory and it's good at ABC, but it's not good in these three things."
"Speaker 1","00;11;18;20","00;11;43;04","That's where the discussion ended. I didn't know that there was another area that kind of picked up where Shannon left off in a sense, and that's related to this idea of an info morph ism as well as classifiers, conditional probabilities and this token diagram thinking about information channels and flows of information leads to this formulation of context compliant."
"Speaker 1","00;11;43;15","00;12;27;27","So context modeling or following hierarchical, multilevel scale Bayesian networks and first this section has a lot of technical detail, but it involves making a connection between Bayes networks approaches to probabilistic modeling and statistics and inference with context. This leads to a sort of conclusion or a climax section where the hierarchical Bayesian networks that were just introduced in six are brought into the context of two specific practices, which is, number one, the idea of active inference."
"Speaker 1","00;12;27;27","00;12;48;15","I highlighted it because that's kind of our lab. That's what we're here to be thinking. About. And then two was the frame problem, which is something that's also really interesting to think about is going to be the key motivator. So although they put it last with active inference in the frame problem, we're actually going to have those topics closer to the front of the presentation."
"Speaker 1","00;12;48;20","00;13;14;07","So people who are listening to this and the active inference perspective or from more generally the frame problem perspective, they're going to understand what motivated us to be studying the paper, the way that we're doing it and hopefully what motivated Chris to select this paper when he suggested it to be discussed during his visit. That's the sort of naked roadmap that's the turn left at this part and turn right at this part."
"Speaker 1","00;13;14;21","00;13;33;09","But there was a narrative roadmap that the authors actually put out, and their own words are on the right. So you can see that in the paper or here. But on the left I just thought of this is like the semantic roadmap as far as the flow of the paper. And I just extracted this semantic flow from the syntax that they wrote."
"Speaker 1","00;13;33;20","00;13;59;00","Not super knowing what all these topics are, but just rephrasing what they had put previously. These authors had been researching quantum and thermodynamics of separable systems in this paper. They started with a recap of Q spaces and shoe morph isms, which suggests that it's a foundational topic for the paper. They then reviewed contextual paradigms and the diversity of approaches."
"Speaker 1","00;13;59;00","00;14;30;13","That model contextually They then took a formal diversion into this idea, into various ideas of probability theory above my detailed understanding. But they took an extension in that direction. They then turned to the semantically richer methods of channel theory, and they're using channel theory as an even more general approach to inference rather than the syntactic information theory like Shannon information theory, which we'll get to."
"Speaker 1","00;14;30;22","00;15;01;09","They wanted something a little bigger thinking about these kinds of local logics of information channels leads to some new ways that they also combined ideas And then they visualize this represented as this cone co cone diagram, as they say as a natural scale field representation of Bayesian inference. So scale free just means that it doesn't have an apriori scale."
"Speaker 1","00;15;01;16","00;15;28;13","It's not in inches or in kilometers. It's just at the level of abstractness. And then they have the claim that is proven that the failure of the CD or the cone co cone diagram to diagrammatic grammatically commute is a condition of the measurement operators non code deployability. So it's just sort of like if you have two shapes and then they fit together, what they represent fits together."
"Speaker 1","00;15;28;16","00;15;57;15","And if you have two shapes and the puzzle pieces don't fit together, there's something mathematical that the shapes represents isn't fitting together in the way that it would, as if these two shapes fit together. I don't know what non code deployability is but it's like a puzzle piece if it were co deployable. And then the part that I think will be of special interest to a lot of our lab and colleagues is they're going to recover this new theoretical and inferential role for the miracle blanket."
"Speaker 1","00;15;58;00","00;16;23;11","It's the locus at which free energy is minimized. Part one and part two the epistemic barrier that keeps contextual information hidden from the observer. So when I saw that, I was like, Whoa, sounds fascinating because it's two distinct and critical roles of a blanket construction. And then I thought, so contextual information is hidden from the observer because a lot of times you might think about context as being like, what's around us?"
"Speaker 1","00;16;24;02","00;16;27;23","But we'll see if that's actually the way that context is being used."
"Speaker 2","00;16;29;21","00;16;31;06","And I'll jump in for a second. Daniel."
"Speaker 1","00;16;31;07","00;16;31;15","Yep."
"Speaker 2","00;16;32;15","00;16;55;11","So I think that I whether I understood code deployability measurement operator deployability like you can't at the same time measure position and velocity. That was like one example that they gave in the paper. So like because those two things like you can't if you're moving, you're not at a stable position, right? So, so it's those, those two measurements are not quantifiable."
"Speaker 2","00;16;55;11","00;17;01;18","You can't measure those two things at the same time. And that's kind of what they meant like about the measurement code of mobility. I think."
"Speaker 1","00;17;03;21","00;17;05;05","Cool. Thanks for that."
"Speaker 1","00;17;07;06","00;17;33;26","Here's to why it matters before we even jump into the keywords, which will be the bulk of our journey. And this is quotes from the authors while the results presented here are primarily technical. So just take a breath if it's worrying you because they wrote the paper so we don't have to and it's a technical paper and that's what we're learning by doing and exploring the paper is technical and abstract, but now we're looking for where we can go."
"Speaker 1","00;17;33;26","00;17;59;05","It's like a car is technical, but it lets you get somewhere. So where are we going and why will it matter? These results open a path to application in a number of areas where contextual ity has largely been neglected. But it's important in practice. That sounds really important because it sounds like there's systems that are unknown unknowns. We don't even know what to model, but there might be a large category systems where we think we've modeled it well."
"Speaker 1","00;17;59;12","00;18;18;23","But then there's this problem where context keeps on entering the picture or something that we didn't model throws our local model off that we spent time and energy on. So if we could find a way to respectfully interface with contextual city across different areas, that would be really important. What are those areas? What are the models that they're interested in?"
"Speaker 1","00;18;18;23","00;18;48;13","Just like first and a lot of active inference researchers is neuroscience. And so they suggest that the current approach they're taking takes a previous model, which is the global Toronto workspace model, and that's actually related to what Ryan Smith and Christopher White will be talking to us about in number 18. They're going to make that model contextually compliant and provide the building blocks needed to incorporate context switching and attention switching into global neuron or workspace models."
"Speaker 1","00;18;48;23","00;19;14;16","So it's kind of these two cool parts of the brain, which is that it can spiral in and really hone in with attention, but then it can also move attention. So it's like side to side and in and out and that sort of context and attention switching is not dealt with well by our current models, which might be about attention or inattention to a stimulus, but they're not actually modeling what the regime of attention is."
"Speaker 1","00;19;15;28","00;19;48;21","Another then there's a formal result which might matter to some people or others that contextually can always be associated with this sort of don community tivity kind of related to the puzzle pieces, the context we just mentioned. And then this is interesting from a more probability theoretic point of view. Our results further support the authors of the contextually by default in showing that the distinction between quantum and classical probabilities lies not in any ontological difference, but rather in what has been explicitly labeled."
"Speaker 1","00;19;49;05","00;20;18;19","So this was really mind bending because a lot of times it's like quantum, you know, theory of small things where there is an observer that has an effect in the outcomes of how the system is modeled or set up. And then the sort of disinfo of debate is like, who's an observer or does it matter if it's a conscious desert, you're going to go down a rabbit hole because there aren't simple answers there, but the system of the particle, it turns out, depends on how you structure the context of the experiment."
"Speaker 1","00;20;18;27","00;20;45;17","Just as is the case for macroscopic entities, So these were some questions that. So. So in other words, is there a continuum of tiny things and bigger things with context as being what is distinguishing the behavior that can seem quite different across scales? So it's not that electrons are magical and they can do wave particle, but nothing else exists in a similar regime."
"Speaker 1","00;20;46;03","00;21;17;08","It's not that we're going to get the same diffraction of people walking through a doorway. But then again, maybe if it's set up the right way, there's more similarity than not. So few opening questions before we jump into the keywords, just how will we connect across formal and informal or just plain different senses of the word context? And then what are some of these cases where whether they said it in the paper or not, where it contextually has been largely neglected, but it's important in practice."
"Speaker 1","00;21;17;08","00;21;52;16","So even within an informal context definition, what's in an area where potentially context is not taken into account? So I'll start with one. And then either of you could go maybe cultural context or for understanding geopolitics or individual context for an individual relationship. So it does matter. But when we approach those sort of individual or collective level problems as if we can ignore context, again, just using it informally, we're not really operating in the space that the system is in."
"Speaker 1","00;21;52;24","00;21;57;01","We're operating in reduced or a distorted model that we're projecting."
"Speaker 3","00;21;58;23","00;22;08;08","Hey, Daniel, I don't know if this is going to mess up your video set up, but if you're able to go into present mode on your slides, it would make the whole thing a lot more readable for a small screen."
"Speaker 1","00;22;09;23","00;22;13;28","In the live stream, which you can look at and mute and evaluate, is that."
"Speaker 3","00;22;15;18","00;22;15;24","OK?"
"Speaker 1","00;22;17;11","00;22;21;03","But yeah, we've got a lot of small text on these slides yeah."
"Speaker 3","00;22;21;14","00;22;47;18","Yeah. It's I don't know. It's so worth unpacking that last bullet point. It's there's just a lot there is is how so are you reading when I read this that he's saying ontological difference versus what's explicitly labeled. I immediately just think, well, of course, because you're you're either looking at small scale or you're looking at macro scale. But you think he's saying something something a little more subtle."
"Speaker 1","00;22;49;23","00;23;21;14","I would love to have a quantum expert, but just speaking with our friend Jason Larkin, he has often pointed to these sorts of almost like classical meets quantum crossover situations where you get the classical, quote, behavior from when you define the full system with the quantum observer, then it behaves, let's just use it winslett, for example, the system behaves a certain way when you define it a certain way with the observer in the picture."
"Speaker 1","00;23;22;01","00;23;48;04","But when you don't define the observer, you're just talking about a different system. And so that might not be something that only electrons exhibit. It might be a feature of certainty and uncertainty in measurements, and that would be various kinds of measurements. It wouldn't have to be just things that were smaller than a molecule."
"Speaker 3","00;23;49;19","00;23;52;06","OK, that's how you're reading it. Yeah, that makes sense."
"Speaker 1","00;23;53;21","00;23;57;16","But that's my non technical physics for biologists."
"Speaker 3","00;23;57;16","00;23;58;16","Take yeah."
"Speaker 1","00;23;59;06","00;24;01;01","Any thoughts in this blue? Or we can go to keyboards."
"Speaker 2","00;24;01;24","00;24;23;08","So I do have a little bit of thoughts and so it's tied into the quantum, but it's also tied into back in 37.1. So you have skipped over this word that I think is kind of critical and important in the previous bullet point that says our main form of result theorem 7.1 shows that intrinsic context duality can always be associated with non committed tivity."
"Speaker 2","00;24;23;16","00;24;48;03","So in that non commutative that means like you can't construct the big cone diagram, right? Like when it's not when it doesn't compute. So I think that it's important to highlight that that kind of context duality is like, you know, the way they were describing the context for something that's beautiful, like, you know, a beautiful watch, right? Like a beautiful leaf is not the same as a beautiful person."
"Speaker 2","00;24;48;09","00;25;06;14","If that leaf looked like a person, it'd be a really ugly leaf. Or if a person looks like a leaf, it'd be a really ugly person, right? Like so, so beautiful. It's always like that context dependent and beautiful. What like beautiful does not always have the same referential meaning. And so I think that this is really important and ties right?"
"Speaker 2","00;25;06;14","00;25;32;20","Into the quantum aspect of things, right? Like how they were talking about the Alison Bob with the machine, right? Like, so it's always you know, when you can measure. Like, it's always what is explicit context, right? So the quantum is tied into explicit context. Like when we have a set of variables that we measure for every circumstance at precisely the same exact time and under precisely the same conditions."
"Speaker 2","00;25;32;28","00;25;39;21","That is extrinsic context, right? And it's different and separate from intrinsic conductor reality. So I just wanted to think that."
"Speaker 3","00;25;41;13","00;26;04;11","You might be saying something that I'm. Yeah, it's great. Thank you. It's you might be saying something that, that, that I'm already about to say and, but, or maybe you just triggered my thought, but yeah, this intrinsic versus extrinsic the way what it made me think of was that, you know, one thing is quantifiable and the other thing isn't it's like a gradient, but that's essentially what you're saying."
"Speaker 3","00;26;04;11","00;26;06;11","Yeah. OK, it's great. Thank you."
"Speaker 1","00;26;08;03","00;26;33;17","Well, let's get to the keywords. This was definitely one of our, you know, even though you look at the keywords, you think, oh, it's the ABCs. I mean, how hard is it going to be? No, it was really fun and a good learning experience for all of us to go through these topics. And it includes active inference and then Bayesian inference, which are things that we've definitely talked about before, as well as Markov blankets a little bit."
"Speaker 1","00;26;34;05","00;27;00;03","However, the other words were basically all just even new to learning about, not just in active inference. So that's where we kind of started this one and that's why we want to just include people in the conversation and just see where everyone is at in learning these topics. So contextually, choose Space Channel Theory, Information Flow, Local logic, cone cone diagram, frame problem."
"Speaker 1","00;27;00;03","00;27;30;00","And it wasn't a keyword, but it's kind of related is concurrency and concurrent processes. I don't just always want why is the second C capitalized? Yeah, let's motivate the paper, not with what you spaces are, which is how the paper begins. If you were to read it, well, let's kind of motivate it with the problems and the context that we're looking to be applying in which is the frame problem and active inference and thinking about something that most of us are more familiar with, which is Bayesian computation."
"Speaker 1","00;27;30;15","00;27;56;18","So the frame problem is the problem of using their words, the problem of circumscribing what does not change when an action is performance viewed broadly, it's the problem of circumscribing what is relevant in a situation. It's kind of like if you want to say, well, what are all the connections in the social network? It's the shadow of what are all the non connections because those two answers are arrived at at the same time."
"Speaker 1","00;27;56;26","00;28;41;28","And if you have no false positives and no false negatives, it's the same to know what has changed versus what hasn't changed. So when we're thinking about scenarios, those two are equivalent instead of just reading this for a first pass, what did either of you how would you summarize the frame problem or what is something that makes it valuable or important for study OK, think about it and then raise your hand while I go through these the issue with the frame problem and the reason why people are working on better solutions is that let's say we want to understand what does or doesn't change when action is performed."
"Speaker 1","00;28;41;28","00;29;05;14","So we're making a self-driving car and we want to understand how something changes or not. When the scene changes in order to look for all the changes. OK, what hasn't changed? It's really hard to know when to stop or what has changed. There's just an exponentially large number of things to check and you can't brute force it. So there are here istic solutions."
"Speaker 1","00;29;05;14","00;29;24;05","So solutions that aren't exact but get you on the right track however, the drastic solutions which often work extremely well, like framed different thing. Like if something doesn't move between two slides, you just cancel it out. And if it does move, you're going to do some other algorithm on it. It's a good stick, but it assumes coherent context."
"Speaker 1","00;29;24;12","00;29;44;08","It assumes that what is observed could in principle be expanded to include all that there is to be observed. So if it's an image and it's a 64 by 64 image, and I do frame different thing and we're only studying the image, then I have captured everything that's changed but when there are systems with intrinsic contextual 80, this coherence assumption is violated."
"Speaker 1","00;29;44;23","00;29;58;02","If there is intrinsic context, duality, i.e. some of the observables are non co deployable, the current context cannot even in principle be expanded to include everything that's observable. So blue, what do you think about that or the frame problem."
"Speaker 2","00;30;00;15","00;30;39;07","So one thing that I thought was cool about the frame problem is that they so they had this corollary 8.1 I don't know if you have this up there, but when they're when they're discussing the frame problem, it says a distributed information information flow system can be informationally an encapsulated only in the absence of intrinsic contextually. But so, so what that really meant to me, it was they said again, they reiterated practically that this means that proving the absence of actuality in a domain requires discovering all of the information that is relevant to solving problems in that group, in that domain."
"Speaker 2","00;30;39;15","00;30;55;20","So the same problem can only be solved if it has already been solved. It's like having ground truth data, like if you're trying to predict something like you can only predict if you've been given a ground truth to predict from or by getting me to repeat that it."
"Speaker 1","00;30;55;28","00;31;07;11","I just yes, so you got it. Trusting it maybe just like has it been solved like it has been solved like in the past, present or the future. And we're just like figuring out again or something or what does it mean right?"
"Speaker 2","00;31;08;02","00;31;27;07","So it's essentially like you can't go, you can't afford solving it. And this goes into and I know we're going to get into the computing analogy later on in the paper, the reservoir computing, but it goes into that like without ground truth data, your neural network is useless. Like you can't predict the future based on something that if you don't have the the context for it."
"Speaker 2","00;31;27;07","00;31;38;25","Right, and not the intrinsic context reality, you have to have like a real context reality. You know, all of the variables that are important for the for the problem anyway. Sorry, that's that's my thoughts on the very problem."
"Speaker 1","00;31;39;20","00;32;03;04","Cool. And at the bottom here, how would domain specific or general solutions or simply better hearing sticks lead to real world implications. Well we can probably think of a lot of software and hardware and hybrid systems that need to ask themself what is relevant in a situation. So that's going to be a recommendation engine, a self-driving car, whatever it is."
"Speaker 1","00;32;03;17","00;32;18;09","We want to be modeling relevant aspects of a of a situation. So that is the big and it also has implications that each person is going to have their own take. So, you know, put it in live chat or the comment but it's a cool topic."
"Speaker 3","00;32;18;25","00;32;22;19","Of course, there's going to be new places of bias that'll be kind of interesting to see."
"Speaker 1","00;32;23;19","00;32;25;13","Yep. Any other thoughts on frame problems?"
"Speaker 3","00;32;25;13","00;32;27;12","Sarah No, that's it."
"Speaker 1","00;32;28;06","00;32;48;26","So Bayesian inference, another big topic, but we can start it with a fun little meme. So on the top, right? It says, OK gang, let's see what deep learning really is. And this is like Scooby Doo we do, where are you mean genre? And so we're going to find out what deep learning is. At the end of the episode, we unmask the villain."
"Speaker 1","00;32;48;26","00;33;16;14","Oh, it's rev base. So this is sort of hinting at the idea that Bayesian inference or Bayesian thinking, Bayesian statistics is underlying what deep learning is, what is Bayesian inference. So there's two ways we're going to show it here. There's the equation in it's sort of most simple form on the top left and then a more colorful unpacked version that's made a little bit closer to natural language."
"Speaker 1","00;33;17;06","00;33;39;16","There's equations. And then also we can look at it in a graphical way and what the equation as well as the graphical representation are getting at is the idea that there's a prior, which means before. And so there's a prior expectation and is some type of distribution, it could be tight or wide, it could be different types of things."
"Speaker 1","00;33;39;22","00;34;01;28","And there's no uninformative prior, even if it's flat, like I'm not sure if it could be 0% or 100% chance or it's all equal. That's not the same thing as unbiased. So this really gives a better lens to talk about bias and implicit or explicit bias because we can actually talk about priors being updated rather than who is or isn't coming to the table with no prior expectations."
"Speaker 1","00;34;02;09","00;34;38;19","That person doesn't exist. Priors are transformed through measurements into posterior estimates. And so it's like reality is providing stimuli and then the prior gets dragged along either a little bit if the learning rate is low or a lot if it's going to be an overwhelmingly powerful stimuli, it gets pulled a little bit or a lot to the data and what else would you say about that blue nothing else."
"Speaker 2","00;34;38;19","00;34;45;27","I mean, I think that that's that's the the deal. OK, you would pretty well describe what about you, Sarah?"
"Speaker 3","00;34;47;05","00;35;12;21","I mean, the only thing I thought about it's a little vague. I mean, I was just looking at some YouTubes on Koopman embedding and model discovery. And and when I see these Bayes diagrams, I'm often like, oh, OK. Well, they have some data where they presuppose what what is connected to what prior and with the with this this other method that I was looking at, it seems like that's not the case."
"Speaker 3","00;35;12;21","00;35;20;18","You know, they just feed it into this gigantic structured API. But so just thinking about Bayesian inference in a lot of different ways."
"Speaker 1","00;35;22;05","00;35;54;17","Oh, and we can also ground Bayesian inference in a history and a developmental trajectory of statistics, which is to say more memes. So first, here's just one quick meme. Here's Bayes saying, I thought the volume of a high dimensional hyper sphere would be evenly distributed throughout just funny stuff, truly funny yeah, exactly. And on the right side is a SABC comic just joking about how when data comes in, your models updated but that doesn't mean it's going to be an adaptive model."
"Speaker 1","00;35;55;00","00;36;11;09","So there's a lot of ways to go about integrating new data into priors. You could have fallacious priors, you could salaciously update, you could make the wrong measurement, you could interpret the measurement wrong. So saying we did it Bayesian is like saying, you know, I built the car out of metal. It's like, What else do you want to know?"
"Speaker 1","00;36;11;16","00;36;38;04","Does it work? Is it safe? So we can't let the buzzwords confuse us. We just want to follow up with what the buzzwords are. And here in these sort of stereotypical human evolution meme, there's and with a little extra labeling, there's a progression from the theta. Here is the model parameters or just the model. And then X is like the data or the observable."
"Speaker 1","00;36;38;14","00;37;00;14","So the apriori homo apriori is just as walking around with the model. Here's how the world is. What data just it's the model. This is what's likely homo pragmatic this is sort of again, it's not an actual evolutionary or creationist parallel, it's just a meme. The data is the only thing that's considered. What's the data? Show me the facts."
"Speaker 1","00;37;00;14","00;37;31;02","Facts don't lie. Facts don't care about your feelings. It's just the facts. It's not even about the model. And then we get to of the Frequentist. So the Frequentist is asking the statistical question what is the likelihood of the data given the model? So what, for example, a p value is coming from like a T a t test is like what is the likelihood of this height data given the model that the two classrooms don't have different heights?"
"Speaker 1","00;37;31;22","00;37;54;00","And so you can say that's .01 P value. So there's such and such likelihood for this data under that model. So we can reject that model. And then that's all this whole point with the rejection of no models and H not versus H one and rejecting things. And the whole idea of falsification at the statistical level, but also at the philosophy of science level."
"Speaker 1","00;37;54;07","00;38;21;28","Oh, one beautiful piece of data could ruin the theory and you just can disprove it. It's this whole idea of disproving you have the model in the background explicitly or implicitly, and then new data comes in. You go, Is this likely or not? And then if it's unlikely, you think that it casts doubt on your model Homosapiens here is thinking about the the likelihood or modeling both the data and the model, but then the sort of advanced or at least furthest."
"Speaker 1","00;38;21;28","00;38;40;16","Right. Hummel Bayesians this is thinking about the likelihood of the model given the data, which actually is what we want to know. We don't need to know the likelihood of all board positions and all possible chess strategies. We want to know the likelihood of certain things happening given what we're observing, which can often be quite limited or confusing."
"Speaker 1","00;38;41;01","00;38;44;02","So not an over analyzed mean, but yeah. OK."
"Speaker 2","00;38;44;22","00;39;02;13","So Kimberly, I want to interject really quick. Yeah. So we were talking earlier about frequencies and Bayesian statistics prior to pushing the broadcast button on the livestream. And you know, I was thinking about the movie 54 states, right? Like if you guys have seen the movie about two weeks of every day and can't remember anything except before her car accident."
"Speaker 2","00;39;02;20","00;39;19;12","So the models there, it's, it's existing, right? It's like she has the model. But then it's just just the model never updates. So every day like, you know, it's you just go based on the model. But then there's no there's no update. And so like the Bayesian would be like if every day you updated the model while you're sleeping at night."
"Speaker 2","00;39;19;12","00;39;38;12","And I think like, you know, that's a huge lead in to the Bayesian brain framework. Right? Is that what we're doing while we're sleeping? Really? Are we updating our model like this is why we need less sleep as we get older and we get stuck in the decade? Like, you know, you're stuck in your favorite decade because you're not updating your models as much or as well."
"Speaker 1","00;39;39;16","00;40;06;27","I don't remember the movie too well, but didn't the other character have memory across the dates? So it's almost like an alternative title it could have been, you know, when a Frequentist and a Bayesian date 50 times, that would definitely got left on the clipping for so Bayesian networks are taking this Bayesian inference idea and putting it in a graphical or a topology recall framework."
"Speaker 1","00;40;07;07","00;40;34;27","So it's representing variables in the nodes. There's other graphical models, but just for the ones we're showing here, the nodes are random variables, and then conditional dependencies are shown with arrows or with edges. So this is like a very general one on the left. It's like a is just having a conditional dependency with B, and it's directed in this case, this one right here is actually a Bayesian network called a neural network."
"Speaker 1","00;40;35;01","00;40;56;27","This is a single layer neural network where one input kind of unfolds to all these hidden layers, just one here, and then it goes to an output layer. But this is the structure of neural networks. So everything that we're talking about, like neural networks, all kinds of deep learning are encapsulated and then on the top right from par at all 2018 is the example of active inference."
"Speaker 1","00;40;57;10","00;41;15;23","And so here is the progression that we walk through in our model stream. But the simplest Bayesian network that we can imagine between a hidden state and an observable is you have s the hidden state and then there's some sort of p which is the probability of an observation given the hidden state of the observable. So that's like hidden state."
"Speaker 1","00;41;15;23","00;41;42;08","And then you're getting observables and then you're trying to model the hidden state given the observables in B, that same process happens through time. A changing hidden state is emitting observables at different time steps. Still, a Bayesian graphical model C introduces control theory with pie. That's why our lab logo is a little pie because it's action playing into the way that states are modeled to evolve through time."
"Speaker 1","00;41;43;15","00;42;10;09","D takes another level of complexity and allows actually policy. So still we see this motif with PI influencing how s one, two and three change. But at each time point, let's take this one down into S one at each time point. It's being evaluated how a policy is going to play out at S 12 and three. And notice that at one, two and three there's an inference on one, two and three."
"Speaker 1","00;42;10;22","00;42;31;19","And so I'll bring in my camera worry the the trajectory of action is what is being estimated. So it's not just doing state estimation at each of this time points, it's estimating a trajectory of action. And this idea of action trajectory is going to come back into play in a kind of interesting way."
"Speaker 1","00;42;34;23","00;42;53;17","Let's just take one more second more generally to think about active inference and what it is. Why are we here where the Markov blanket is in active inference? We have internal and external states, and it's a scale free framework because these internal states could be small or large in scale. It's up outside the level of specifying a specific scale."
"Speaker 1","00;42;54;11","00;43;15;00","And these internal and external states are separated by a blanket. And as we're going to show in just a second, this Markov blanket concept is just the total layer of insulating nodes, such that the internal and external states have a certain type of independence relationship from each other. And then the innovation will get to where it first came into play."
"Speaker 1","00;43;15;04","00;43;44;18","So that's active inference, just as a way to integrate action and perception with sensations influencing internal states. Internal states, including generative model estimates of the world and policy estimations, implements action and action can influence external states, which also is undergoing its own dynamics, which we might think of as actually in actuality. Any other thoughts on active inference? OK, so here's where we go from Bayes networks to Markov Blankets."
"Speaker 1","00;43;44;28","00;44;18;04","This was part of our discussion on item 14 and the idea of making a Markov blanket was introduced by Perl 1988 and where, for instance, innovation occurred. And this is I think from Mel Andrews's paper in 14 is that Pearl's original construction of the Marcos blanket was subdivided into two kinds of nodes sensory states, nodes whose influence is directed towards the blanket node under interest ex and active states, which are nodes influenced by X."
"Speaker 1","00;44;18;14","00;44;43;09","So we can have an organismal metaphor with sense and action, but a little bit more generally sense is like incoming of the system and action is outgoing and the Markel blanket is the total set of all the ones that totally give you all the inputs and all the outputs of the system. So stating the conditional dependencies is the same thing as stating all the conditional in dependencies."
"Speaker 1","00;44;43;15","00;45;05;12","Just like we're showing where the social network who's friends with who is the same question is who's not friends with who? Who is there is the same question is who is not there? And then to put Markoff, Perl and Frist in on a continuum, you have Markov and Properties by Andre Markov and other Markov actually, who are working in a more mathematical frame."
"Speaker 1","00;45;05;16","00;45;56;17","They don't have access to computers. Perl brings in Bayesian statistics as well as computational approaches in the second half of the 1900s. And then the innovations of first in were to divide those Markov nodes the markup blanket nodes into sense and action nodes which enables this cybernetic approach to input generative model internal states and then outgoing action states in service of maintaining a non equilibrium steady state and then though it's an ongoing and open and developing area with how we formalize these things, the idea is that by this kind of partition of the blanket happening, we'll be able to have modeling of non-local dependencies like latent causes which are not observed in the world and"
"Speaker 1","00;45;56;17","00;46;23;03","therefore have cybernetic agents in niches that are enacting and embodied and uncultured in a really rich way. And then just to show one more figure, this is from Wanda's paper with Carl Frist, in which we talked about just a few weeks ago, and just showing that you have internal states that influence external influence, active states. Then there's aligned with external."
"Speaker 1","00;46;23;03","00;46;56;02","And so here's the blanket here in the pink dash line is the blanket states. And then there's the internal states OK, any thoughts on active inference? Or if we go to context so again, putting the sort of end parts of the paper up closer to the beginning, the context in practice, the first context introduced is the idea of active inference in the paper and the motivation for active inference here, and we can definitely talk to the authors more is that observers are constantly probing their environments through action influenced by hidden states and time sensitive policies."
"Speaker 1","00;46;56;26","00;47;23;16","So these are some of the challenges that set contexts agnostic or unspecified context modeling is when the environment is changing. How does the action policy update? Does it just reevaluate every single moment as if it were just like fresh looking at a new environment? But how do you have these sequences of action that might involve sequential modifications of the environment and uncertainty in the action?"
"Speaker 1","00;47;23;16","00;47;54;25","Policy? And the big question, just like, you know, kindergarten is what actions are appropriate? What actions will reveal aspects of the environment important for defining and reasoning within a context? What's the frame problem? What actions will reveal that a context has changed? So we want to know which of the three cups the ball is under and so there's like this complex thing that's happening as we guide our action by changing the context to resolve our uncertainty about actions, which we can then get more information on."
"Speaker 1","00;47;55;28","00;48;34;03","And active inference tells us that there's two ways to minimize expected free energy, which is that we can modify internal state parameters of our generative model through learning and development or external states can be modified through action or their own endogenous changes. So active inference, we're kind of going even more abstract than some of the other papers. And just thinking about this question of if you're an agent who's operating under uncertainty, modifying the niche, but you don't know whether you should modify or explore or weights and all these things are coming together, we want to set a frame that's big enough to encompass that."
"Speaker 1","00;48;35;03","00;49;05;11","Any thoughts on that? OK, let's turn to context more generally. So that was active inference in context. They write the actuality in the behavior of complex systems has traditionally been regarded as a practical problem of limited experimental control. That could in principle be eliminated by obtaining more uniform experimental subjects and achieving better control of experimental conditions. This traditional view of contextual ity as merely a practical limitation is, however, increasingly under strain."
"Speaker 1","00;49;06;06","00;49;27;01","This kind of resonated with me as an experimentalist because in biology you'd get the sense like, well, if it was a clonal animal and we just had the light dark cycle the same and the same animal handler picked it up and there was no pipetting error and everything was the same. You'd get the same results and it was really exciting to think about."
"Speaker 1","00;49;28;05","00;49;57;22","To what extent is that like a faith based position and actually tiny amplifications in the system, you have to clone all animals, but if there's like a bifurcation point, then unless you truly have every single part of the system, you're always going to be subject to these intrinsic actualities. So that's kind of cool to think about. And then here in the conclusion context is often used to designate what is neglected when an observation is made or an action undertaken environment is used in a similar way."
"Speaker 1","00;49;57;28","00;50;19;16","So that's how maybe informally people think about context. Like right now one of my contexts is like this cup of water. It's in my environment, it's in my niche, it's in my local area. Like their context is used like that, like what is around something and the formalism of a theory like context by default challenges us to acknowledge that context is always there, whether attended to or not."
"Speaker 1","00;50;20;06","00;50;39;10","So we're thinking about context in sort of a more active way in terms of what influences processes or where conditional dependencies are. And so it might be the case that you can ignore a context because you've actually isolated the system. So it's like a closed system or it's a system where you can fully describe it. And in those situations, context is there."
"Speaker 1","00;50;39;16","00;51;00;27","But you've made a special sub problem where you have all the context. But when you're in a situation where you don't have all the context, we need to have a framework for reasoning that has contextual ity by default so that we can work better in those spaces. And let's talk a little bit more about this contextual view by default."
"Speaker 1","00;51;01;00","00;51;28;12","CBD, not that CBD this is the author's talking about context reality by default and just totally, you know, raise your hand or jump in if you have a thought or a question and they say that to determining whether a particular instance of observed behavior of a complex system, particularly one with memory, exhibits intrinsic context, duality is not straightforward."
"Speaker 1","00;51;29;13","00;51;54;07","So that was interesting. Makes sense. Complex systems, hard to know what to do sometimes, whether you want to include memory in historiography or not. And in some ways, like if you have like ten different animals and you think, OK, well they're the same background, so we can just consider them even. But they've actually had this history that differs so they're not the same, but you want to imagine that they're replaceable or shuffleboard in a statistical sense, but that might not be."
"Speaker 1","00;51;55;05","00;52;20;24","And then here's the quantum sense of context duality in quantum systems, a set of measurements, exhibits, contextual ity, if it cannot be characterized by mathematically consistent context for joint probability distribution, asking whether a set of measurement outcomes exhibits intrinsic contextual ity is in this case asking whether a consistent globally defined globally connectable, joint probability distribution exists or not."
"Speaker 1","00;52;21;24","00;52;48;13","And that's where this formalism of contextual you by default addresses questions by labeling context and includes them explicitly in conditional probabilities, rendering probability distributions context dependent. So let's look at the papers that were cited and referenced and think a little bit about where is this context duality theory coming from. Where's it taking us? What do you think? I Oh yeah."
"Speaker 3","00;52;49;06","00;53;15;02","Comment that that brought to my mind emotion related to context because I've been thinking a lot about how like if I'm attached to some idea or something like that, and then I just remember that I was attached to it, but then I go to sleep and whatever and then I forget what it was. I have a tendency to like really be like, no, that was really important."
"Speaker 3","00;53;15;02","00;53;33;27","And so regardless of whether it was true or not, I want to go back to it. And so I'm thinking about the way in which emotion is a kind of a context, but it takes you over a longer time domain because like it doesn't even just sit in like an decay rate that would normally be related to the other information."
"Speaker 3","00;53;33;27","00;53;43;19","It pulls you way back into another space. So it's just kind of like long range driver I don't know, that's just context cool."
"Speaker 1","00;53;44;05","00;54;06;09","And it brings us to this next question, which is, is there this is the title of a paper that's cited by this offer of 2016. Is there a contextual in behavioral and social systems? So if we were dealing with this informal definition of context, it's just like the environment. It's like, yes, the answer is just yes. Behavioral and social systems."
"Speaker 1","00;54;06;09","00;54;42;03","Yes, there are social contexts like, oh, you missed the context on this dinner party. OK, so off the bat we'll say yes, but we're thinking about it formally. And this is, this is the most hyphenated sentenced I've just ever seen in science. So just the number of names, it was quite something to read but we're thinking about contextually in a more formal way, which is like, so let's say someone says you're missing the dinner party context and then it's like, well, if you would have told me all the info, then I would have had all the context."
"Speaker 1","00;54;42;06","00;55;22;04","So then that would make it that's it's a similar notion, but we're going for measurement systems and they're proposing that their CBT allows one to define and measure contextually and all systems of all these hyphenated types, even if there are context dependent errors in measurements or if something in the context directly interacts with the measurement. These are the two cases that really matter a lot for scientists, like what if at higher temperatures your thermometers biased towards over estimating or being more error prone, then you're going to get this distribution that looks like it has some sort of super specific relationship."
"Speaker 1","00;55;22;04","00;55;42;17","That's totally not the case. It's all an observation error. And the second one is that the context could directly interact with the measurement, a.k.a every single time you want to know something new. So the context which you don't have because you don't have the full context because you're going out into the unknown to make a measurement is going to interacting, interact with your measurement, maybe even modify it itself."
"Speaker 1","00;55;42;27","00;55;43;10","What do you think."
"Speaker 2","00;55;45;17","00;56;13;06","So I think this just goes back to the intrinsic versus extrinsic context, like in a social and behavioral system, there's no extrinsic concept, right? Like every there's no extrinsic context. Every thing that you're going to try to measure, if you ask like what was the temperature at that dinner party or how is the food, you can ask like, you know, 12 different people and they're all going to have different things to say."
"Speaker 2","00;56;13;06","00;56;26;29","Or like, did you have fun or whatever? Like, I mean, everyone you ask because it's only defined by the intrinsic contextual ality, which is the experience of each agent in the room and I think cool."
"Speaker 1","00;56;27;16","00;56;56;06","And interestingly, they look at several data sets. None of the data provide evidence for contextual 80. So again, don't just yes, there's an each, there's a context, we know what social context is, but we're actually thinking about it in this very interesting way where those data sets don't have context duality. So a good little gut check is if you're thinking about the applications, this paper and you're thinking about it like social context, it's just not that one because it's not."
"Speaker 1","00;56;56;24","00;57;31;19","And their rule is that behavioral and social systems are non contextual. Again, if that makes you cringe, it's because you're thinking about it informally and we're actually thinking about it in a way where that claim makes sense. That's where we want to understand, to be like, Oh, right, informally we get where they're saying it, but then we're interpreting their claims rigorously, i.e. all contextual effects in these contexts, in these settings, result from the ubiquitous dependance of response distributions on elements of context other than the ones to which the response is presumably or normatively directed."
"Speaker 1","00;57;32;08","00;57;52;09","So let's say you have a driving simulation and then there's going to be some green lights and some big red stop signs and you do the behavioral measurements and you find out that conditioned on whether it was a green light or a stop sign, the people accelerate or break. So that's your experimental data. What you controlled and what you observed was stoplight and then whether they stayed still or accelerated."
"Speaker 1","00;57;52;23","00;58;20;00","Now, that is context. It is context for the person who's driving. That's the social sense. But all that contextual facts in this experiments result from the ubiquitous dependance. So across the whole population of response distributions on elements of context other than the ones to which the response is presumed to flee or normatively directed. So it's like you thought you were measuring the response to a green light or a stop sign."
"Speaker 1","00;58;20;07","00;58;46;17","Actually, you're measuring a social context and a group of people who were in a relation with those symbols, such as they operated in a certain way. So we need context, duality by default. So that we don't fall into like correlation causation, super fallacious scenarios. Here's another case of where context matters and it's going to be fun is actually this decision making is going to matter for context and argue the city."
"Speaker 1","00;58;47;05","00;59;23;08","Here's a paper by all of local choices rationality in the context, reality of decision making. So every decision has a context. So we know about that. However, rational explanation appears to be challenged by apparently systematic irrationality observed in psychological experiments, especially in the field of judgment in decision making. So how many times have we heard, Oh, people are so irrational, they'll take the $1 today rather than the dollar $0.20 tomorrow, or they'll pick up their children from the daycare at the right time unless there's a monetary penalty that will make them later."
"Speaker 1","00;59;23;08","00;59;51;29","They're so irrational within this one variable that's being estimated here, it's proposed that the experimental results require not that rational explanation should be rejected, but that rational explanation is local, i.e. within a context thus rational models need to be supplemented with a theory of contextual shifts. Makes a lot of sense. It sounds like something that's relevant to learn about in this paper as probably other relevant things."
"Speaker 1","00;59;52;06","01;00;13;28","That's like instead of just saying, Well, who's rational and who's irrational, couldn't we just understand that everyone is in a local logic and a local context? And then they're operating in a certain way conditioned on that. And so it's not irrational versus irrational divide it's like, what's the context? Who's the agent, what's the niche, what's the context shift?"
"Speaker 1","01;00;15;02","01;00;41;05","So that's a much more open but also powerful way, I think, to talk about decision making. OK, raise your hand if you want to posit. Let's take this idea of context. So we started with context in the sort of folk sense and took it to this idea of defining all the relevant variables. Now let's think about defining relevant variables in a model in terms of topology."
"Speaker 1","01;00;41;17","01;01;04;02","So here are some cool slides from the Simmons Institute. And in this top right, one it says Topology is about distinguishing the continuous from the non continuous and also about moving around. So I was like, that's a cool definition of topology, not nodes and edges, not how things are connected, but it's a way it's saying how things are connected."
"Speaker 1","01;01;04;16","01;01;24;13","And this slide was really informative when we're asking several questions at once, the answers could obey constraints. So like if you're going to measure the pressure and the volume and the temperature of an ideal gas there's an equation. PV equals NRT that's going to relate those things. And if one of them is out of whack, like probably something is wrong."
"Speaker 1","01;01;25;03","01;01;52;11","So those bundles of answers are related to each other. Could be by the laws of physics. Like here's vino velocity and time or whatever it happens to be like physical constraints or logical constraints. So like if things are true, then if they're false, they're not that. And then if it's double, not that, it's back to being true. So you can have these bundles that are logical or a bundle that is based upon some of the constraints of physics."
"Speaker 1","01;01;53;17","01;02;18;20","And then also the constraints can be rows of a table in a relational database. That's the trip because that's where like SQL and a lot of programing comes into play and models distinguish good and bad ways of connecting the dots and bundles just like continuous sections. So this is kind of like thinking about actual queries and database queries in terms of the context that's being concerned."
"Speaker 1","01;02;18;26","01;02;37;29","So in the context of this database, I want to make a query within it. So that's an interesting way to think about how things are connected and context. But this is also stuff that's we're all just trying to put out there and hear people's reflections and thoughts on. What do you think, Sara?"
"Speaker 3","01;02;39;07","01;02;53;28","I just wanted to restate it to make sure I understand it. I mean, basically they're saying that the, the, the way you move through context sets creates the topology space I think that's is that an accurate way to phrase it?"
"Speaker 1","01;03;00;21","01;03;26;27","Just another aspect of it? Yes. Like there's certain in the space of making multiple measurements. Those measurements can have different topologies. Like if you make three measurements, they could be in a line, they could be in a triangle. Like there's that's not at the formality that's on here. But when you're making different measurements, they can either be in a well-behaved bundle or a not well-behaved bundle."
"Speaker 1","01;03;26;27","01;03;38;25","And then watch. Whether it's well-behaved or not is related to whether it's blue. What do you think about that? It's a hard."
"Speaker 2","01;03;38;25","01;04;11;02","Question. I just jump in for you. Yeah. So so going back to like the intrinsic and extrinsic contextually like so it's contextual. It's contextually when you have like a set of measurements that, you know, or corresponding measurements, right? Like so I mean, it's like I don't know if by the background, Sarah, like like, oh, I'll talk about it just in terms of like RNA sequencing, right?"
"Speaker 2","01;04;11;02","01;04;32;11","Like you can sequence your sample in a different lane on the same flow cell or two different flow cells and there's variability, right? So you always have to try to like design controls into your experiment from flow cell to flow cell from Lane's lane. And so that's why you replicate the samples sequencing just to kind of control that, that variation in just running it at a different time."
"Speaker 2","01;04;32;11","01;04;54;16","The temperature in the room might be different or you know, the machine might not be feeling as well that day. Or whatever. So so it's these kinds of are the measurements, are they all like do they all link up together? Like if they're like a correlation between all of the measurements or not? So that's the well-behaved bundle is when they do all link up, they have that shared information."
"Speaker 2","01;04;54;29","01;04;59;08","So so I think as we get into the paper, maybe it might start to make more sense."
"Speaker 1","01;05;00;26","01;05;27;23","Let's go to channel theory. OK, so this part was super fascinating. And I'm drawing a little bit on one of the background papers, mosaic of two spaces and channel theory, one by Fields in Glazebrook, which we're going to talk more about the two Mosaic papers soon. But let's contextualize this in terms of information theory, so-called info theory, but I personally call it disinfo theory, and I follow that for a long time and now I know why actually."
"Speaker 1","01;05;28;03","01;05;59;06","So here's the second paragraph of Claude Shannon's 1948 work. Shannon writes The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Why is that the problem of communication? Am I trying to reconstruct something bit for bit via communication, or is the goal of communication to impel joint action to update other agents, not to replicate a message?"
"Speaker 1","01;05;59;14","01;06;22;21","Shannon's working with telegraphs and copper wire, so it makes sense why you want high fidelity data transmission, a signal of transportation of information. But let's think about how broad people have gone with info theory. According to Shannon and then he writes frequently, the messages have meanings they refer to or are correlated according to some system with certain physical conceptual entities."
"Speaker 1","01;06;23;00","01;06;43;17","These semantic aspects of communications are irrelevant to the engineering problem. Oh, semantics are relevant to communicating information semantically for people or for what the significant aspect is that the actual message is one selected from a set of all possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen."
"Speaker 1","01;06;43;17","01;07;06;26","Since this is unknown at the time of design. So fair enough. You want to have all of those options because you don't know what next letter needs to be poked. But I think if people read through Shannon's work and considered a few of the aspects of the Shannon Entropy, like how it disorders messages, like it just asks what's the symbol frequency of the whole message or page or genome."
"Speaker 1","01;07;07;10","01;07;32;03","If you think about how it removes the ordering of symbols and is actually asking about replica getting bit for bit messages, not communicating in any sense, people would really consider what information theory is that? Actually in this paper the of Glazebrook wrote about comparing and this is cited in the 2020 paper comparing and combining the Shannon theory of information with channel theory."
"Speaker 1","01;07;32;15","01;08;03;29","OK so though very general as a quantitative theory of communication flow, the original Shannon theory had largely overlooked the question of semantic content in any Gretzky type theory. The basis of semantic content is in the world, i.e. the events or situations that signals or states carry information about the car. You know, it's out there in the world. By showing how local logics are connected by information networks, channel theory provides a generative, general qualitative theory of information flow in this context."
"Speaker 1","01;08;04;27","01;08;29;22","So that's pretty interesting. It's more about information flow that's semantic rather than syntactic. And then this last part here this is there's more to say here, but all in choosing for create a synthesis of Shannon's quantitative information theory with the bar wise Seligman qualitative theory to address how the question of how specific objects, situations and events carry information about each other."
"Speaker 1","01;08;30;09","01;09;00;26","That's what people are always talking about when they want to be talking about sharing information, not sharing surprising symbols, but updating each other semantically informationally in the terms of Bayesian priors about the world things that we're referencing in the world, the temperature not just as a variable, but as about something in the world. So some thought questions is Shannon 1948 correct about the stance taken on the fundamental issues of communication are what communication is or isn't."
"Speaker 1","01;09;01;18","01;09;31;08","How is chain theory distinct from information theory, so called disinfo theory, we can start calling it. And then how is real communication the kind that we care about one a model in many cases. How is it both syntactic and semantic, all these things at once, grammatical, logical, as well as lexical word based. It's also narrative driven. A narrative or the narrative context can make just one word semantically powerful speak now or forever hold your piece."
"Speaker 1","01;09;31;08","01;09;47;08","It's a narrative context where one word cannot be reduced, just the bits or something like that. Oh, but no is a common word or yes as a common word. It doesn't matter that it's a common token because it's semantic information. And then how is it contextual blue."
"Speaker 2","01;09;50;16","01;10;16;00","So I mean, I think that there's this of like I, I like Shannon information theory. I like how it's very logical and it appeals to like my, my, my brain structure. The semantic information that's contained in this channel theory description is very squishy and I think maybe not as appealing to me, but I do think that it's more crapshoot."
"Speaker 2","01;10;16;05","01;10;42;01","Like when you say when you reproduce the information like if I say something like, oh, it's raining outside, I live in the desert, so like it's raining outside. It's like a really awesome like event, like let's go play in the rain, right? So when I say that and someone else is, oh, oh, it's raining outside like, like I'm like my plans are destroyed because I wanted to go to the park and I can't because it's raining outside."
"Speaker 2","01;10;42;01","01;10;50;16","So so it's two, it's the same information. But like I the way that I said, it was very different in those different contexts, right?"
"Speaker 1","01;10;50;16","01;11;14;28","So cool. Let's return the discussion from information flows and communicating information. And we're kind of thinking, Shannon, syntax information reproducing bits, bits on this computer to bits here. We're going to take it up a level to semantic information and then semantic information. So like I'm thinking of a purple cat now you're thinking of a purple cat, but that could have been in a different language."
"Speaker 1","01;11;14;28","01;11;33;22","And it's not really related to how surprising each word is. Let's return to this question of local logic and context duality. So here's a paper, not sure if it was sighted or not, but it's going to bring together local logics, contextual city and topological approach so we're just going to kind of give multiple coats, a paint on everything."
"Speaker 1","01;11;34;08","01;12;00;19","Local logic is like the local rules or the context. And in this casita 2016 paper they wrote, the top logical approach characterizes contextual city as global inconsistency coupled with local consistency. So that was kind of cool to me. It's like if you have this field and it's globally consistent, then it's like transparent. There's, there's nothing inconsistent to speak of."
"Speaker 1","01;12;00;19","01;12;34;26","There's no fracture in the glass. So it just is purely transparent. So that's just to say it was part of the same context but local consistency coupled to global inconsistency is like a bubble. It's like it's an inconsistency in the tank of water. But then locally it has another transparency that makes sense in that local context. And so you could have like a computer program and then that could be a local logic, but it's embedded in another logic of being incomplete or, you know, whatever kind of computer."
"Speaker 1","01;12;35;03","01;12;59;16","And then within that program, there could be like a simulation of another program of an N64 running another program. And even if it's all the way back up at the top, it's running on Linux, but that local program is running on the N64 local logic and then another kind of metaphor. And then in the paper they write, Our goal is to capture this local consistency part, not just the global inconsistency."
"Speaker 1","01;12;59;16","01;13;23;22","So the global inconsistency is like our receptor is not getting any photons, boom, it got a photon inconsistency. So not just that part, but the local consistency part, which. So how do we know how do we know that it was consistent before that photon hit, which requires a novel approach to logic, sensitive to the topology of contexts to achieve this, we formulate a logic of local inference by using context sensitive theories and models in regular categories."
"Speaker 1","01;13;24;19","01;13;51;16","And that's going to be powerful. Here's how I was thinking about this in terms of chess so the rules of the game of chess are like, which piece can move which way? And then some of the imperatives like you have to alleviate check otherwise you lose the game. That's the context, the local rules of the game. Then there are local or situational patterns like a pin where one piece is like in front of the king and it can't move out of the way because then the king would go in to check."
"Speaker 1","01;13;51;24","01;14;22;08","So that's not a rule of chess, but it's an emergent pattern that arises from constraints in the space, given the vocal logic of chess. But now we're in this even sub local logic and not going hyper formal. Maybe chess players have another way of doing this, but it's like there's a local logic in the corner of the board with a king, and not every piece or rule of chess like, OK, well the pawn can move to or one, but that rule might not matter for this pin with these pieces."
"Speaker 1","01;14;22;22","01;14;50;08","And so not every single rule or piece is going to matter for these local logics. And maybe there's a way to pursue this topic logically in terms of the connectedness of different relationships of the system. Let's take that to choose spaces. OK, so this one was, you know, it's the second section of the paper. So it's like you got to understand and learn this one."
"Speaker 1","01;14;50;08","01;15;21;19","So we were kind of excited to do this. So they start by defining one of the simplest categories. So we're in category theory is the category of Q spaces, which we're going to talk more about. And it basically is about a set K and then within K, there are kind of two kinds of things objects and attributes. And the Chu space is like a matrix that is the relationship over that set K of the relationship with A and X."
"Speaker 1","01;15;22;29","01;15;42;14","It's so it's actually probably a data form that we've seen a ton of times, like here's ten people and then have they RSVP for my event or not. Yes, yes, yes. No. 1110. That's a true space for RCP. But it's like that, you know, here. But it could be a lot more general and we're going to keep on generalizing on that."
"Speaker 1","01;15;42;14","01;16;02;02","But the basic Q space is a matrix, but it probably can be higher dimensional. That is just a regular matrix, but a top logical space is also matrix. So you can have like the adjacency matrix, right? Like all the nodes by all the nodes, one or zero connected or not, or a weighted graph. How strong is the connection?"
"Speaker 1","01;16;02;12","01;16;26;08","Or it could be symmetric and undirected or there could be a from and a two. So like all of topology is matrix forms and it turns out that they're two spaces, but also we can think about like programing languages and we can have a type as one of an attribute or a type, you know, is the would tabling Q space or is the object a table?"
"Speaker 1","01;16;26;10","01;16;48;19","That's a true space. So there's a lot of programing, relationships that the choose space covers, probability spaces, coin flip, you know, contact you know, is it going to happen or not? So it's like if it's a matrix, I don't know if it's you can go as far as say it is a true space, but certainly a broad family of objects or categories, I don't even know what to say."
"Speaker 1","01;16;48;19","01;17;18;27","There are within this true space paradigm, there's a few more definitions because we looked up just a bunch here's from Wikipedia. And basically the big idea is that the two Morph isms are transformations on Chu spaces and if things are set up properly, the Morph isms continuous and the pair of functions has some special relationships. So one example this makes me think of is like the type of graphical number theory."
"Speaker 1","01;17;18;27","01;17;48;06","In Gorilla, you're back where it's like one plus one equals two, and then you make a set of text rules like you can add an S to the string under these contexts, or you can add or you can, you know, it's like it's basically doing addition and subtraction by typographical like letter based rules. So if you can prove something in one system because you've super strongly shown that there's a more physical you can show that that same relationship should exist in another system."
"Speaker 1","01;17;48;11","01;18;23;14","It also turns out that's actually at the heart of like the whole girdle question, but that's the static interpretation. And then here's dynamically true spaces transform in the manner of topological spaces yeah. Don't go too deep down here, but there's special relationships of to choose spaces that are to morph isms. And then if that morph ism is bidirectional, it just allows a really nice thing where you can sort of make a finding on one side and then arbitrage it to some relationship that might have been difficult to show on the other side."
"Speaker 1","01;18;23;27","01;18;47;07","So it's like a perfect language mapping. You can say, you know, the boy is younger than the car. If you had a perfect language mapping semantically you could say it in another language and the semantic information will be perfectly preserved. So then maybe one person has a piece of semantic information in English and then they could convey it through the perfect morph ism, and it would be correctly semantically understood on the other side."
"Speaker 1","01;18;48;20","01;19;14;29","OK, here it's just a couple years. We just we wanted to actually understand it and also shout out to master students everywhere. So here's history from this paper that new construction takes us symmetric Benoit all closed category V and A with pullbacks got reading it and an object K of and completes V to a self dual category chu v k."
"Speaker 1","01;19;15;19","01;19;42;17","The details of this construction appear in Chu's master's thesis, published as an appendix to his advisors and Barr's book introducing the notion of an asterisk autonomous category. So chapter that was an appendix. And now here we are many years later. So it's all good to research. Whatever your researching on, it appears to you to be the big question here's Barr's perspective."
"Speaker 1","01;19;42;24","01;20;21;13","So Barr, it's another perspective on two spaces. Barr had this idea during his sabbatical to study duality in categories in some depth. Just what most of us take our sabbatical wondering. He wasn't wondering about the simple dualities, but self dual categories like complete semi lattices or find a valley in groups. He was interested in the possibility of having a category that was not only self dual but one that had an internal home and for which the duality was implemented as the internal home into a dual izing object."
"Speaker 1","01;20;23;01","01;20;41;10","It's technical, so duality is going to mean something different to a mathematician than an odd mathematician. And I don't know what a home is. However, searching for an internal home, I just thought, OK, the dual it's like forwards and backwards or it's like it goes both ways. That would be nice, you know, reversible it's like those are always good things."
"Speaker 1","01;20;41;22","01;21;05;11","And then the internal home internal homes, when changed together, form a language called the internal language of the category. And I thought, OK, so now we're going from categories and attributes to like something else. The most famous of these internal homes are simply typed lambda calculus which is the internal language of Cartesian close categories and a linear type system."
"Speaker 1","01;21;05;11","01;21;35;14","The internal language of closed symmetric minus little categories, lambda calculus is something that's quite broad and central to computation. So this is like something about how logic, local objects and nested logics are related to categories in an analytical way, such that the kinds of logic that we see on computers is only a type of it, because in a way we're dealing with syntax based logic."
"Speaker 1","01;21;35;26","01;22;06;27","You know, you have two files. Are they the same? It lines up the bits and if they're the same, it's yes, no. But what would this mean if it could be semantically the same and how would we make that non squishy because to ask whether semantics were the same, you need all the context. So that's kind of this fun twist and it makes sense why for simplicity, you use Frequentist statistics instead of Bayesian inference, because it is hard to do sometimes."
"Speaker 1","01;22;07;12","01;22;30;29","Or you do have to grapple with unknown unknowns and it's easier to just do a T test or a multi linear regression rather than some other type of generative model because that takes like bravery to specify your uncertainty and to give support to why you did a certain things rather than just say, well, you know, the countries differ with the P value of point one, but it's easy just to say that and move on rather than actually do some sort of rich modeling underneath."
"Speaker 1","01;22;31;14","01;22;53;25","OK, two spaces and channel theory. So we're kind of just trying to bring all these to, you know, connect two of the ideas. And then you can start nucleated some ideas from the ones that are making sense in channel theory, which again, we're thinking of like the more general cousin of Shannon disinfo theory in channel theory, true transforms."
"Speaker 1","01;22;54;14","01;23;10;14","These are the transformations between the two sets or spaces become info morph isms, which are natural maps between classifiers. OK, so blue, what would you say about the red underlined part or what you wrote at the bottom."
"Speaker 2","01;23;12;19","01;23;38;05","So this will transition you into the next slide. May be here in a minute, but you see just the red underlined part. It's the transform from a to be. So if you have one to space and another to space, the whatever, it's like, you know, you can think about a territory and the map of that territory, like whatever transform going from the territory to the map that is that channel, right?"
"Speaker 2","01;23;38;05","01;24;03;05","Is thought informally of as a channel that is like the channel that we're talking about here in terms of channel theory. And so these are the, the classifiers here so the classifier is the set of weird. I have that here somewhere wow."
"Speaker 1","01;24;03;18","01;24;05;09","Classifiers link tokens to types."
"Speaker 2","01;24;05;29","01;24;33;07","Oh yeah. But so that's this is the definition of classifier. So classifiers link tokens to types that encompass them. Oh yeah. Because you've already read this. So the Channel three, the two transforms become info morph isms, which map between classifiers. So if you have different sets of classifiers, right? So classifiers link tokens to types that encompass them. So for the types stoplight, the set of tokens could be like you can have a red stoplight, a yellow stoplight or green stoplight."
"Speaker 2","01;24;33;07","01;24;55;07","And that's also known as a local logic. The classifier which we talked about, local objects, just a few slides back but flip to the next slide and I'll continue this. So mapping between classifiers. So here if you have the classifier stoplight and then you have like what the motorist should do at the subway. So that's the second classifier."
"Speaker 2","01;24;55;07","01;25;12;08","You could have the tokens like stop at the red light, go forward at the green light or proceed slowly at the yellow light or get out of the intersection or whatever it is you think you should actually do it at the stoplight. Then you can have other classifiers too. So doesn't just have to be the stoplight and the motorist should do."
"Speaker 2","01;25;12;16","01;25;30;10","It's also like what bastions should do and like at the game, like red light, green light that you play when your kid like you should run forward when slowly or walk or then and stop. So I mean, there's different classifiers that you can have that all map to the same central information core. Right? And so that's what you see here."
"Speaker 2","01;25;30;10","01;25;57;14","The classifiers are these things a one, a two, and you can have many of them up to eight of the K. And then you have at a core this information channel. So this is the information system that allows semantic information, not just channeled information to be transmitted between classifiers. So this comes down to having a shared memory. So we all learn from the time that we're six that red means to stop, green means to go, yellow means to slow down."
"Speaker 2","01;25;57;14","01;26;15;00","So so back to a six year old, it means running when you're a pedestrian, you're looking out for other cars and and there are other behaviors that you do. And as a motorist also and as a police officer policing the motorist there's all kinds of people that have different reactions to what the stoplight is doing. But that's all based on this shared memory."
"Speaker 2","01;26;15;00","01;26;31;04","Like everybody has been taught, this is what you're supposed to do out of light. And so so that's the score information channel through which information flows. It's can be also be thought of as a shared memory or a shared context, if you will."
"Speaker 1","01;26;33;04","01;27;02;12","Cool. One thing that that makes me think of here is like this sentence that said the existence of a true transform 80 B is equivalent for every flow formula valid and A being valid in B, so it's kind of like you have the chessboard and the board position says it's game over and then you have the the data file and then those two are within a bigger C that is establishing a connection between these very different types."
"Speaker 1","01;27;02;21","01;27;15;14","And so it's like there's in the example you gave, the local logic is like there is a total logic that it does all come back to even though there's disparate types and there's sub logic."
"Speaker 1","01;27;17;16","01;27;27;25","Let's hear what everyone else and Chris can bring to this discussion because it's like super interesting to think about. Yeah. OK, so what about sequence blue."
"Speaker 2","01;27;31;12","01;28;05;09","So the sequence here, I just wanted to put this in because I thought it was it was important for information flow. So the sequence when a secret encodes a semantic like a causal constraint that effectively functions as a logic gate. So this really what it's what enables semantic information as opposed to just Shannon information. So it's like you have the message but there's a logic gate there that says if the message is applicable or not applicable or to be transmitted or not."
"Speaker 2","01;28;05;26","01;28;31;05","And also that it's I think puts forward into the it enables putting forward into the scale free or scalable infrastructure here. I think and then I like all this like math symbols. Like I've never seen this stuff before. These like what is this like parallel lines, lollipop stick? I don't know. But, but I just I, I put a box around this because it's like, oh, that goes back to Bayes."
"Speaker 2","01;28;31;18","01;28;36;21","Oh, probability. All right, I know this. So I'm coming home again."
"Speaker 1","01;28;37;12","01;29;01;19","Cool. I think this one just reading this one x then shape RN to assign a probability x satisfies end. So maybe it's the sign of satisfaction I guess it just says something that we don't know what it looks like, but it's like there's a satisfaction of X to assign a probability that x satisfies. And given that it satisfies m, so it's like, what's the probability that we're in checkmate on the board?"
"Speaker 1","01;29;01;27","01;29;31;16","Given that the file says checkmate? Well, if you have the morph ism, it's perfect. But if you don't have that morph ism, then it's probabilistic or it's different. So it's a conditional probability that's conditioned upon the way that you can map across different kinds of things. And it was the perfect connection to Bayesian inference, because at the bottom here we have basically m satisfying A is the true space."
"Speaker 1","01;29;32;00","01;30;24;11","That's the the scarlet letter, the fancy A is the choose base and so this is almost like saying M is satisfying sum to flow relationship with N. That is itself about how tightly the spaces are connected. And so then we can take this p m given N single line not satisfied. And it is belief updating. And so then yeah, it provides the necessary semantic consistency for such updating info morph isms, thus capture a significant representation of Bayesian inference with the necessary coherence since the target classifier context admits the same semantics as that as the source within the information flow information or info morph isms, our infomation flows that capture the updating of something semantic because"
"Speaker 1","01;30;24;11","01;30;47;09","it's semantic in that jet ski meaning is in the world way like the prior in the posterior are about the world. And then as new data or events come in, which I didn't think we captures much here, but like this event theory connection, it's like Bayesian statistics is kind of like the steady state and then new data comes in."
"Speaker 1","01;30;47;09","01;31;12;11","It's like an event in a network and the that event propagates through a network of semantic information flow and then that updates the system and maybe it's like the system's already vibrating. And so the new event comes in. Nothing changes with a vibration that might be a situation like the system semantically is already at its non equilibrium steady state."
"Speaker 1","01;31;12;11","01;31;25;24","And so it is like not surprising but then there's other kinds of events that enter our system that might be more surprising, but it's all about the semantic nature of a system. OK."
"Speaker 2","01;31;26;16","01;31;58;27","So we wait, can I just add on to this? Yeah, but like in our, in our contextualizing like and I didn't know if you're going there or not, but I just want to explicitly state that like Bayesian inference is the event like what we are updating with active inference and category theory and two spaces we like like implying that or I just wanted to say that explicitly because in this context, contextually we talk about contextual like we're updating our own events with our, with our own knowledge."
"Speaker 2","01;31;59;06","01;31;59;17","Yep."
"Speaker 1","01;32;00;13","01;32;21;22","And I, I think was sort of fun about this semantic info flow idea, the whole channel theory thing, which I really didn't get exposed to in a lot of contexts, was that instead of the propagation of bits and bytes on wires and the signal fidelity, and then it's like, well then where does the semantics get unpacked? Does it get unpacked at every little post office?"
"Speaker 1","01;32;22;02","01;32;46;01","And they just say, No, no, it just syntax all the way down. So it's inherently reductionist because you go, we've explained away meaning because we're studying the connection from an entropy perspective, and then it's sort of a strong internally logical argument, which is why there's people who believe it. And then it's like, could we actually think of semantics as the heart of the question?"
"Speaker 1","01;32;46;15","01;33;11;12","Because when we're talking about communication of action or modification of a niche or solving the frame problem, we don't need the syntactic differences. We want the semantic differences like what has changed? My car is broken, I don't need to know the syntax, but from a action oriented semantic view, that's what we're communicating on. And then we're seeing these specialized communication message passing systems, like on a silicon chip."
"Speaker 1","01;33;11;24","01;33;48;29","It's a special case. It's a controlled system where we've defined the context semantically so that it isomorphic leap maps onto bits and voltages. OK, here's the two fields in Glazebrook papers from 2018 mosaic of two spaces and Channel 31 and two. So one category theory concepts and tools is like a lot of formalism diagrams, but two has some nice images and that's applications to objects, identification and macro logical complexity."
"Speaker 1","01;33;48;29","01;34;17;26","Macro logical is parts and whole relationships and this is to go from the two spaces to this token, a system capable of both object history, construction and it's dual category learning. So just their history reconstruction and it's dual category learning. It's like I'm learning about cars being broken or not. And then if you tell me it's a broken car, I can generate a vision in my mind of a broken car."
"Speaker 1","01;34;18;11","01;34;47;24","So it's like you have a bi directional relationship. It's this open to generating specifics from the abstract or updating the abstract from the specifics, which is really related to this hierarchical Bayesian learning and act of inference. And they have reached, after many years of research, an understanding that objects that have this sort of analytical relationship between object history, construction and category learning with a singular category of maintenance."
"Speaker 1","01;34;47;24","01;35;29;19","As a special case, I'm not sure I think that maybe that has to be one object that does both itself dual, not just two objects, one that does one part, one that is, the other is characterized by a cone cocom diagram a cone token diagram captures the simultaneous upward and downward flow of constraints that characterize human visual object identification and it is reasonable to suppose other sensing modalities, and they then take it very instantly into something fascinating, which is the functional duality between a high precision expectations and high precision inputs in an active inference system, and hence the duality between dorsal active and passive attention systems."
"Speaker 1","01;35;29;27","01;35;49;22","So it is about this precision modulation and the way that you get high precision expectations and high precision inputs. So how do you have a clear generative model of the visual field? But then at the same time you want to be able to recognize new objects at High Fidelity as they come into the screen, come into your field."
"Speaker 1","01;35;50;04","01;36;15;08","So how are you going to have high precision generating and high precision learning? And there's going to be this rich, multilevel ongoing integration of priors and new events and posteriors that's semantic. So it's like I'm looking at, Oh, it's a face, and then my eyes engage in action selection pattern appropriate for a face that reveals some sort of anomalous information pattern."
"Speaker 1","01;36;15;15","01;36;41;18","And all of a sudden the meaning has changed. And so it's almost like what is crossing the interface perhaps is that the data, the observable, the measurements, but what is crossing is actually like the reconstruction of semantics, rather like if it's a car driving across your view, but it's going 5000 miles an hour and you can't see it, then for you as the measure, it doesn't matter because you didn't see it."
"Speaker 1","01;36;42;08","01;37;07;03","So even though it was perhaps there, but it's just a blur. And so this is just getting at something very interesting which is these systems that are exquisitely generative, yet also hyper able to recognize and to update the generator based upon the specifics of which have been generated up. All cats have stripes. All right. You just saw one that had polka dots and now you can generate that."
"Speaker 1","01;37;07;03","01;37;31;17","And then maybe you might generalize and even generate another type of pattern of cats. And then you could see whether that was accurate or not. So here is the diagram that Blue talked to us through here with the core information channel. See at the top. And so this is the cone diagram on the top it's it's like a pyramid diagram, but like maybe if it rotates, it's like a cone or there's some other way of thinking of it like a cone."
"Speaker 1","01;37;32;20","01;37;53;29","And you have the classifiers and this is the whole Q space as classifiers scarlet letter A choose base. So the classifiers are linked through info morph ism's, semantic bridges to the core information channel. So like here's the chessboard, here's another two space, and here's another true space for chess. And they're all linking to this like game of chess, local logic."
"Speaker 1","01;37;54;12","01;38;24;24","And then there's a map between the board and the computer program. So G one, two, two, three are the community. So this is like you got a link between the board, the computer and the computer and the drawing and they're all info morph isms to see the game of chess. So semantically, if there's a checkmate on one board, it's like a checkmate across paradigms now when you're when this is a tight cone and it represents like a huge variety of things, and then I'll get to second blue."
"Speaker 1","01;38;25;06","01;38;28;19","The token is when we go ahead. Go ahead."
"Speaker 2","01;38;28;19","01;38;48;26","But let me let me interject before you do the coconuts. I was like, OK, so I want to interject here for Sarah because remember, we were talking about like the the observables being like a tight like a tight little cluster of measurements versus a not tight cluster of measurements. And so here they define it in this before we go into the cocoon."
"Speaker 2","01;38;50;02","01;39;21;16","So you can kind of take this back to like the stoplight and the kids playing and what the driver does, she doesn't whatever. But but here, if you think about it, instead of like that, the categories or the sorry, the classifiers, it says when we define a finite information channels see as a finite index, family of info, morph isms, those are all the Fs having a cutoff for the component classifiers will be taken to characterize a system of observables or some subcomponents."
"Speaker 2","01;39;21;16","01;39;39;15","There are so here they're specifically stating that all of the class here are all the things that can be measured or observed in a in a context that's extrinsic as opposed to intrinsic, I think. So I could be wrong. I hope that someone someone corrects me, but I feel like this is where where it's going."
"Speaker 1","01;39;41;26","01;40;16;15","To cone diagram. Then we introduce the cocoa, the shadow as above. So below wouldn't have it any other way. A commuting, finite cone cocoa diagram. Which one's the cone? Which one's the cockroach, right? Comprises both a cone and a cone. On a finite set of classifiers. So they're mapping to the same set of a classifier. Same two spaces are being bridged and then you have like sort of two systems looking at the same bridge."
"Speaker 1","01;40;17;08","01;40;44;03","And then if there's a community beauty across the elements of the of the two spaces, we have that from above. And then there's info morph isms from all the A's to C Prime and all the A's to D Prime. Then it's a special kind of object. What is special about it? It captures in addition to a ton of other stuff, probably a more subtle duality."
"Speaker 1","01;40;44;11","01;41;24;23","Remember the whole thing about studying objects that were dual dual objects from like bars? 1975 paper and Chris Fields this other research like what is it, dual object shadow a more subtle duality between processes. So not an object duality but a process duality maybe forward and backwards. It enables object files, object tokens and object histories to be viewed not as tokens but as types that organize respectively trajectory components that's files features that's tokens and feature based singular categories histories into mutually consistent collections."
"Speaker 1","01;41;25;14","01;42;01;23","So this is some next level token algebra and I wondered if it related to crypto because it's like instead of the history, like the object history of my web history. Well, that's actually a that's a singular category. It's a singular trajectory. If you take this list of a million things and it's like one of a unique kind of thing, and then even objects and tokens and descriptors are all understand being understood as highly relational, highly semantically map of all way so that's pretty cool."
"Speaker 1","01;42;01;23","01;42;23;17","And I guess we'll look forward to seeing what the authors can help us with or like what system can we apply this to first or what would be an empirical system where this is being measured? Or what would we gain by measuring something this way? Yeah. What are various examples of context and then here's just a few figures from that other paper, the mosaic too."
"Speaker 1","01;42;24;00","01;42;51;01","So here's a cat. Well, it's an image. It's a pattern on a Google slide. Who's to say it's a cat, right? That's semantic. But there's all kinds of things that we might want to have, such as an object token is classified at construction into multiple types by distinct but cross modulating processes. This includes animal intimacy and agency detection, emotion mediated threat detection, and entry level followed by super and subordinate categorization in the types of objects."
"Speaker 1","01;42;51;19","01;43;19;10","So it's like depending on the context and the agents observing this stimuli whether it's a predator or prey, whether it's a cat or whether it's Phyllis Domestics or whether it's a sub breed of cat, whether it's something that you approach or something that you withdraw from or something you associate culturally with good or bad luck, that is like the agent's ontology semantically as the interface with a cue in the environments."
"Speaker 1","01;43;19;22","01;43;53;21","And then this is a funny image. It's mapping. Chris Fields is brain, so it's like it says like, see if that's, that's him. So here's CF is a human, humans has a brain and that here's another ontology of brains. A primary brain is a mammal brain is a vertebrate, brain is a brain. So you can go superordinate and subordinate categories in a relational ontology as well as have these like totally disparate classification schemes just like you could in programing."
"Speaker 1","01;43;54;09","01;44;16;05","So you could say this person has a height in a way and that's in their public health record. And then there's this other thing, are they a friend or foe? And that's my private information. And then all of those are being linked semantically in a special kind of system. All right. That kind of that's one little section here."
"Speaker 1","01;44;16;12","01;44;45;11","Is bringing in this element of concurrency. So some people will have heard about this from programing languages like Golang or from other kinds of concurrent programing. But this is pretty interesting because it's from 2005 and it's called Paper by Pratt. It's, I think, cited in Glazebrook and Fields, two spaces in their interpretation as concurrent objects. So it's yet another way that we can think about new spaces, not just as mapping attributes to types or doing all these other transformations, but like also as concurrent programs."
"Speaker 1","01;44;45;28","01;45;09;18","So they right here that the two pressing questions for computer science in 2005, maybe forever, are what is concurrency and what is an object. The first question is of interest to today's sibling growth industries of parallel computing and networks, both of which stretch our extant models of computation all well beyond their sequential origins. So you have a touring tape and it's just zooming backwards and forwards on a tape."
"Speaker 1","01;45;09;18","01;45;35;22","It's a standalone machine. It's not connected to the Internet. When you have machines that are in networks, then there's semantic logics that are happening due to those local logics that are boolean. But at a higher level, there might be more like a complex adaptive system like a deadly attack on a network or a certain type of governance game theory situation in a crypto economic network using a CAD, CAD model."
"Speaker 1","01;45;36;08","01;46;06;03","Those kinds of complex adaptive dynamics at the network level may or may not actually be able to recapitulate from the syntactic or from the lower level. And so that's the first question is how do we think about about concurrency and about designing for good concurrent systems when are parallel and distributed? Computational and even unconventional computational paradigms are rapidly expanding."
"Speaker 1","01;46;06;26","01;46;27;06","The second question is relevant to programing languages where the definition of object seems to be based more on whatever software engineering methodologies happen to be in vogue, that on the intuitive sense of object. So in other words, people use object and process in a way that might have a namespace meaning for computer science, but is actually less connected than we might want it to be to the formal math."
"Speaker 1","01;46;28;02","01;46;55;27","And this is pretty another last quote on this paper they write in as the separate chapters of a theory of concurrency, all these different topics petri nets, event structures and domains, stone duality and process algebras, these topics make concurrency something of a jigsaw puzzle. One is the naturally led to ask whether the pieces of this puzzle can be arranged in some recognizable order and then they give a strong endorsement of the two space framework."
"Speaker 1","01;46;56;06","01;47;16;09","So it's just interesting that these two spaces can be objects of relations that can be processes, but also it can include context. That's what this paper is about and algorithmic and game like elements. So if you can do it in Golang, if you can do it in the process algebra, if you can use higher order, reflexive process algebra, maybe that's a true space."
"Speaker 1","01;47;17;13","01;47;47;05","All right. Let's talk a little bit about Air City. So this is going to have a few slides and definition. So on the top left a process, it says to be ergodic. If the ensemble average and time average are equivalent. And here's a more formal definition from this is from the neural bytes site. A more formal definition is from Oli Peterson, who's been at the Santa Fe Institute, an economics professor the ergodic hypothesis is a key analytical device of equilibrium."
"Speaker 1","01;47;47;05","01;48;20;04","Statistical mechanics underlies the assumption that the time average and the expectation value of an observable are the same. Where it is valid dynamical descriptions can often be replaced with much simpler probabilistic ones. Time is essentially eliminated from the model, so this is on the website Neuro Bytes. It's live and updating, but here is just static. It's kind of thinking of two ways you can make purple from blue and red one is to have the spatial resolution increasing, and pretty soon semantically it's interpreted as purple."
"Speaker 1","01;48;20;04","01;48;44;06","So if you doubt that you can zoom in on an old comic book and see how they made color, which is by increasing the scale of resolution really small, then it makes it look like a new colors there. Or you can flash things really fast and then it will appear as if there's purple and those equivalencies or ways of reaching and also deviations from city spatially and deviations from city temporally are what we're talking about now."
"Speaker 1","01;48;44;06","01;49;11;29","Why does the Ergodic assumption matter and what happens if this is empirically valid, if this is violated? So here's an example from modern portfolio theory, which is what all only Peters and others like Glenn Bazerman study, which is that and this is quote from Terabytes Modern Portfolio Theory stipulates there exists some optimal risk to return profile, but the theory rests on the ergodic hypothesis by conflating an ensemble aggregate return with the individual's path dependent return through time."
"Speaker 1","01;49;12;21","01;49;40;02","So it's really interesting and this is related to the St Petersburg Paradox and a couple other economic ideas that basically the total populations performance or a hundred different portfolios performance at one snapshot is not the same thing as any portfolio being carried forward through time. Why does that matter? How off on the wrong track would we be? So here's some funny examples of to violations and also a hypothetical example a very city."
"Speaker 1","01;49;40;13","01;50;03;03","So number two from the neuro base, if it takes one pregnant woman nine months to give birth, nine pregnant women should only take one month. And then this says Recipe says bake for 60 minutes at 200 degrees, but I'll just bake for 30 minutes at 400 degrees. So like if the degree minute is all things being equal, why couldn't we just vaporize it with 10,000 degrees Celsius for just a split second?"
"Speaker 1","01;50;04;01","01;50;13;24","It turns out that there's a process that has to play out. And so yeah, what do you think about this blue or how would you link it to anything else we talked about today?"
"Speaker 2","01;50;17;16","01;50;26;06","I'm like, I don't know. I think my brain is, is I examine something like at the 99 segment months I."
"Speaker 3","01;50;26;19","01;50;50;03","Think I'll say something. Yeah. I find those examples actually worse than just giving me no example because like Airgood the city implies you have to have a system level demarcation. And so to compare it to like nine pregnant women, it's like, well, the system is the woman so you can't really do that in the end now. Like it actually makes it harder to understand and just good."
"Speaker 1","01;50;50;17","01;50;53;15","Good point. Yes. It's only for all reference like."
"Speaker 3","01;50;53;24","01;50;56;26","But it just yeah, I think it makes it more confusing."
"Speaker 1","01;50;57;05","01;51;20;22","Yeah, I agree. It's not a perfect mapping. It's like syntactically it's the kind of error you'd be making. Like, here's the ergodic in a life span. But then if you go to down the rabbit hole, we're trying to add the variables. That's why we always pull back to the actual formalisms so here is from Math Encyclopedia, and here it's I thought it was quite interesting."
"Speaker 1","01;51;20;26","01;51;51;06","So it's about trajectories of points and saying that as you'll hear it with many other definitions, that it's about visiting a space through space and through time. That was sort of that first example. Here's what was interesting, though, when the Birkoff or Gordy theorem had been proved, it became clear that Irkutsk City is equivalent to metric transitivity measurements, transitivity the more general situation it was no longer suitable to talk of the equality of time and space averages systems with an infinite."
"Speaker 1","01;51;52;11","01;52;18;18","So what kinds of systems are beyond spatial and temporal? The city systems with an infinite invariant or quasi invariant measurement not only flows and cascades, but also more general transformation groups and semi groups. Transformations groups, categories, semi groups. We saw a rebellion some groups earlier in the bar paper, so it's almost like we're taking airgood the city. Even beyond the trajectory visitation idea."
"Speaker 1","01;52;19;10","01;52;47;18","I don't know where it's going in the way that the authors are using it, but it's like there's, there's like the, you know, the totally inadequate metaphor like of just making a total category error with the recipe. And then there's the more empirically observable strategy violations and like you're going to have a bad strategy if you make if you're using a model with an ergodic assumption, you're going to do strategy that's just ridiculous."
"Speaker 1","01;52;47;18","01;53;21;19","You're going to fit what seems to be the best or the most likely strategy, and you're just going to be way off base empirically. But this is even taking it to another way. OK, let's just with a couple of these questions, bring some of our close I think you may have raised this one, Sara, but it was asking that this context by default generalizes the intrinsic context duality of quantum theory to the case in which other properties of a context, for example, the order in which questions are asked also affect the distribution of a variable of interest."
"Speaker 1","01;53;22;08","01;53;51;00","This kind of direct influence is generally avoided by experiment design, particularly by imposing no signaling conditions in physics, but it's virtually an escapable when studying complex macroscopic systems. So these are some pretty open questions. How does context relate to Air City, especially when systems are changing their context or vice versa or both? And how does this contextually by default specifically help us here?"
"Speaker 1","01;53;51;24","01;54;20;13","And then how does Fields in Glazebrook build upon Contextual City by default? She may be at another Planck to this question. OK, here's another set of questions. The existence of complementary observables has perplexed physicists. For decades, Feynman characterized as the only mystery of quantum theory. How are such paradoxes resolved at the implementation level? If they are overlooked around?"
"Speaker 1","01;54;20;13","01;54;41;06","If they're not, what happens when workaround mechanisms fail? What is required to detect failure post hoc ability to prove this experimentally? Would begin to answer the broader question of how prior probabilities are represented in memory and how different sets of priors can be deployed in different contexts. So how do we know what we don't know for a given context?"
"Speaker 1","01;54;41;06","01;55;05;14","And how can a context be made to entail unknown unknowns? How do we bound our not knowing and I think part of the way is going to be defining everything as the Markoff blanket, which is going to be US internal states and blanket states is what we can observe and control. And so everything is going to be conditioned on generative model of external states."
"Speaker 1","01;55;05;25","01;55;29;19","So we're never even going to peep outside of the blanket states we're going to be super serious about modeling ourself and the blanket states as interacting with and modeling external world states. So instead of how tall is the tree, you're going to have like you and the measuring device and the measurements in a system, and maybe for some easy questions, it's going to be total overkill."
"Speaker 1","01;55;29;27","01;55;55;04","But I think for other questions it's going to really open the door because once people frame it in this way, what was an unknown, unknown is going to be something that we can actually encapsulate so that's bringing us to this question. Here's a quote. We apply this development of ideas to recover, among other things, a new theoretical and inferential role for the Markoff blanket concept."
"Speaker 1","01;55;55;04","01;56;27;04","We read this in the introduction because it's a key point as the locus at which free energy is minimized and the epistemic knowledge barrier that keeps contextual information hidden from the observer. So where is Markoff blank coming into play? What does this have to do with our active inference modeling and learning? And then just all these issues that we've been talking about representation, internal versus external perspectives and where is this paper coming into play?"
"Speaker 1","01;56;29;07","01;56;54;06","OK then, Sarah, this is one of your suggested slides from a couple weeks ago, so but it's relevant here again. So what maybe what if you want to anything that's interesting, like about context or frame or reservoir, just your you know, a high form philosopher of science. So it's always good to hear what you think about it."
"Speaker 3","01;56;55;08","01;57;26;19","And my brain is pretty fried, but I think I can my eyeballs are fried. We'll forget. Anyway, the thing that strikes me as a kind of contrast about what I'm able to read from this paper is, I don't know, I get the sense when they're writing in the paper that they just have this like, you know, how can we have the eye of God in like all contexts and shift like, you know, or at least they're talking about that in the context of like modeling and talking about how our world actually is."
"Speaker 3","01;57;26;19","01;57;57;04","But the thing that makes reservoir computing so powerful, one of the things is, is it's not organized city. It's it's kind of natural partitioning. It's it's heterogeneity in some way. And so I don't know. That's really interesting to me. This kind of how can a body compute and what are the attributes about a body that what are the kind of imperfections about the body that make it that make it intelligent in that sense?"
"Speaker 3","01;57;57;08","01;58;10;03","So so yeah, you know, in what way do channels form like these natural cut natural joints or make these kind of boundaries and why? I don't know. That's those are the interesting parallels for me."
"Speaker 1","01;58;12;09","01;58;54;01","Oh, yep. That idea of like it's the local consistency. The organ is malady, the end in of itself. That's like the Kantian organism concept, but it's an aberration. It's far from equilibrium from the global context, which is just hydrogen molecules dispersed. So it's like when you have a global disorder or some sort of perturbation and then you have a locally organized bubble it's then it's like it is our unique differences and situation that allow that agent to be adept in the niche it almost couldn't be another way or it's hard to see."
"Speaker 3","01;58;55;00","01;59;14;01","I don't know. The thing that's interesting to me is like, you don't even have to think about, well, I'm sure there is more. You have to think about it in terms of adaptation and response and things like this. But just, you know, how does a rock compute? And there was one paper that I was reading where they were talking about like, well, OK, if you stick a rock in the oven, it doesn't necessarily heat up in all the same places."
"Speaker 3","01;59;14;01","01;59;35;03","So you do have this kind of you do have these, these non homogeneous kind of attributes about the rock. And it's a it's a very loose analogy. But, you know, that is essentially like this this non Boltzmann type type distribution that is what makes these bodies computable in some way. That is yet still a little bit mysterious."
"Speaker 1","01;59;35;26","01;59;55;07","Yep. And it's like if it were a little worm, you said if it oh, it's body is set up with the muscle and the connective tissue and the nerve so that it moves away from a prick from on the side. It doesn't need to be computing where everything is. If its body is able to respond, then it can do it needs to do in the niche."
"Speaker 1","01;59;55;14","01;59;55;25","OK."
"Speaker 3","01;59;56;21","02;00;01;07","But I mean you don't even need to be alive. That's the thing that kind of blows my mind about how this computing works."
"Speaker 1","02;00;03;21","02;00;31;27","Yes. So implications and next steps the first. So this is good that they went there. So the first question, which has expanded upon more in the space, the other paper is exploring contextually for human measurements. Cool. A second is understanding how context switching is implemented at the neurocognitive level. So that's the Feynman quote that we just looked into."
"Speaker 1","02;00;31;27","02;00;54;23","But basically what is happening when it's like somebody is your friend, you're in a narrative of friendship and they get from the cultural down to the, you know, the day down to the second down to the micro expression, you're in a narrative and then a stimuli happens in a context switches and that trajectory is still a unitary. Your relationship with them is still a thing but it's change in a way that the context has changed."
"Speaker 1","02;00;55;02","02;01;40;24","Like how does that get implemented? This combination of the high precision priors and the high precision inputs how do we get the best of both worlds with this rapid attentional modulation as brains into and away from objects? And then context switching those are features that are just not within the scope of Shane and disinfo theory. And they're much more semantic questions a third and broader question is whether intrinsic context duality can be detected and characterized on multiple scales in complex systems generally by formulating a scale free model of contextual ity in category theory terms, and in particular by employing the concepts of classifiers and information channels, we hope to open a new pathway towards addressing"
"Speaker 1","02;01;40;24","02;01;44;07","these questions in multiple ways or addressing these questions in multiple systems."
"Speaker 1","02;01;46;10","02;02;18;18","So maybe if there is something special to be said about contextual ality or violations thereof, two sides of the same coin, maybe we could identify in a less biased way what scales of organization are relevance or just detect new dynamics of complex systems? Because there's a lot of recent work showing that like with complex systems, they can emit structured outputs that have the signature of white noise or have a signature of nothing."
"Speaker 1","02;02;18;18","02;02;44;13","Like an encrypted file is going to have a distribution syntactically that's going to be looking like nothing different from the Frequentist expectations, but it's a semantically encoded local logic that is allowing communication through that channel. So if all you have is the white noise detector, you're going to think up that looks like the signals not containing information, but actually semantically it is."
"Speaker 1","02;02;44;23","02;03;07;10","So there's there can be systems just in and around us that are doing things we don't even see. And that would be no weirder than if there was wavelengths. We didn't see. We know that's true. We know that there's things that are too big or too small for us to see. So could there be things informationally or from a narrative perspective that we're just not perceiving doesn't seem out of the picture."
"Speaker 1","02;03;07;25","02;03;30;27","So I think it's a it actually is a major area. And, you know, for those who have participated or listened to this deep in Chris Fields was the one who recommended this paper to us and just said, I think this is a topic that is not really thought about in the active inference community that much, but it will become important later I don't even know what topic he was referring to, but this was the paper that really raised some huge questions."
"Speaker 1","02;03;31;07","02;03;53;17","So all kinds of questions that we have but I think it's fair enough just to say thanks to our two slash three brave and awesome participants Thank Sarah and Blue, because that was a fun convo and we definitely laughed and beamed because it was like the hardest paper I think we've read yet."
"Speaker 2","02;03;55;10","02;04;09;00","Definitely. But you know, the paper was so well-written. Like I am not a big fan of reading like math papers and Lemma This and Theorem that. I'm not I usually don't enjoy it at all, but I really did find this paper to be very well-written and easy to follow."
"Speaker 1","02;04;10;12","02;04;29;02","I agree. When there was a formalism, it's a theorem definition figure formalism is like, OK, I'm not going to understand that, but I know that that's just the theorem. So I could just say, OK, got it. Something I don't understand put it away. Encapsulate that. So I agree. Any last thoughts? Sara Blue?"
"Speaker 2","02;04;29;29","02;04;41;27","I just that I almost wish wishing like as I was reading the paper like for some concrete examples, but we would have ended up with like a 65 so I guess that's our job to be the concrete example, you know."
"Speaker 1","02;04;42;27","02;05;08;03","Yet and that will be fun to talk to Chris about. So let's until the time we actually get to talk about this paper which is going to be on March 9th. So between now and March nine, let's just chill you know, do our other jobs instead of just read this paper, think about what would be really powerful and meaningful to ask them semantic."

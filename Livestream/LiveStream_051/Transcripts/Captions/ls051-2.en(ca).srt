1
00:00:33,087 --> 00:00:35,932
Hello and welcome everyone. This is

2
00:00:36,010 --> 00:00:39,322
ActInf livestream number 51. Two.

3
00:00:39,505 --> 00:00:42,350
It's November 9, 2022.

4
00:00:43,162 --> 00:00:44,997
Welcome to the the active inference

5
00:00:45,042 --> 00:00:47,947
institute. We're a participatory online

6
00:00:48,055 --> 00:00:49,932
institute that is communication,

7
00:00:50,022 --> 00:00:52,527
learning and practicing applied active

8
00:00:52,557 --> 00:00:55,297
inference. This is a recorded and an

9
00:00:55,330 --> 00:00:57,667
archived livestream, so please provide

10
00:00:57,715 --> 00:01:00,350
us feedback so we can improve our work.

11
00:01:00,712 --> 00:01:03,127
All backgrounds and perspectives are

12
00:01:03,145 --> 00:01:05,692
welcome and we'll be following video

13
00:01:05,740 --> 00:01:09,397
etiquette for live streams, head over

14
00:01:09,430 --> 00:01:12,427
active inference.org to learn more about

15
00:01:12,520 --> 00:01:14,967
participating in different institute

16
00:01:15,027 --> 00:01:18,397
projects. Alright, well,

17
00:01:18,580 --> 00:01:23,137
we're in act imp stream number 51.2.

18
00:01:23,275 --> 00:01:26,047
We're in our third discussion on the

19
00:01:26,080 --> 00:01:28,672
paper. Canonical neural networks perform

20
00:01:28,780 --> 00:01:31,942
active inference from 2022.

21
00:01:32,140 --> 00:01:35,242
We had a dot zero video with some

22
00:01:35,290 --> 00:01:37,962
background and context and overview.

23
00:01:38,112 --> 00:01:42,022
And then last week in 51.1 we had

24
00:01:42,055 --> 00:01:45,007
a great discussion, went over many

25
00:01:45,085 --> 00:01:48,367
interesting details of the paper and

26
00:01:48,415 --> 00:01:51,727
related topics. So today we're going to

27
00:01:51,820 --> 00:01:56,697
jump in, cover some empirical details,

28
00:01:56,817 --> 00:01:59,332
some implications, connect some more

29
00:01:59,410 --> 00:02:02,767
dots, maybe look at some code. And

30
00:02:02,815 --> 00:02:05,392
thanks again to Kuya for joining these

31
00:02:05,515 --> 00:02:08,932
discussions. I'm Daniel, I'm a

32
00:02:08,935 --> 00:02:12,232
researcher in California and thought a

33
00:02:12,235 --> 00:02:15,532
lot over the last week about what this

34
00:02:15,685 --> 00:02:20,607
kind of neural network synthesis

35
00:02:20,697 --> 00:02:23,900
or translation really means,

36
00:02:24,487 --> 00:02:27,862
and just want to learn more about what

37
00:02:28,000 --> 00:02:31,887
fundamentals or foundational aspects

38
00:02:31,962 --> 00:02:34,137
of these different kinds of models

39
00:02:34,287 --> 00:02:37,937
enable that synthesis or translation.

40
00:02:38,287 --> 00:02:42,132
And then again what that means for areas

41
00:02:42,222 --> 00:02:45,187
where one or the other kind of model is

42
00:02:45,250 --> 00:02:48,517
already in use. So thanks again for

43
00:02:48,565 --> 00:02:56,290
joining and I'll pass it to you if

44
00:02:56,307 --> 00:02:59,920
you want to say hi or give any a

45
00:02:59,922 --> 00:03:03,730
second interoception. Oh yeah,

46
00:03:03,852 --> 00:03:07,635
I'm appearing Brain Science Institute,

47
00:03:07,755 --> 00:03:11,980
Japan. So I look forward to

48
00:03:12,177 --> 00:03:15,800
discuss another different aspect

49
00:03:16,900 --> 00:03:19,687
of this work.

50
00:03:23,200 --> 00:03:27,130
Well, let's just remind ourselves of

51
00:03:27,327 --> 00:03:31,390
the fundamental parallel being made

52
00:03:31,482 --> 00:03:34,795
in the paper and then we'll get to these

53
00:03:34,872 --> 00:03:37,135
two questions about kind of the two

54
00:03:37,167 --> 00:03:39,112
directions that things can go.

55
00:03:39,925 --> 00:03:43,312
One representation is in equation one

56
00:03:43,825 --> 00:03:46,650
with loss function of a neural network

57
00:03:46,725 --> 00:03:50,505
and the free energy on a POMDP.

58
00:03:50,640 --> 00:03:53,950
And that's also seen visually in figure

59
00:03:54,012 --> 00:03:57,615
one, with a neural network being drawn

60
00:03:57,645 --> 00:03:59,805
a concordance against the variational

61
00:03:59,865 --> 00:04:03,100
base of the action perception loop. So

62
00:04:03,162 --> 00:04:06,650
maybe just let's begin by restating.

63
00:04:07,000 --> 00:04:10,330
What is this parallel that is

64
00:04:10,377 --> 00:04:14,560
in equation one and figure one and

65
00:04:14,742 --> 00:04:16,912
how was it reached in this paper?

66
00:04:19,375 --> 00:04:23,485
So basically idea

67
00:04:23,592 --> 00:04:27,730
here is that we

68
00:04:27,777 --> 00:04:30,430
derived to characterize the dynamics and

69
00:04:30,477 --> 00:04:33,685
activity of canonical neural network in

70
00:04:33,717 --> 00:04:36,835
terms of Bayesian inference, because

71
00:04:37,017 --> 00:04:41,065
arbitrary dynamics of neural network is

72
00:04:41,232 --> 00:04:44,640
interoceptive in the sense that we don't

73
00:04:44,670 --> 00:04:48,555
know what is the function underlying

74
00:04:48,615 --> 00:04:51,835
such a dynamics and what is

75
00:04:51,867 --> 00:04:55,735
the coherence of the

76
00:04:55,917 --> 00:04:58,550
self organization or activity.

77
00:05:00,100 --> 00:05:03,655
So once we translate that

78
00:05:03,702 --> 00:05:06,445
dynamics in terms of Bayesian, we can

79
00:05:06,522 --> 00:05:09,630
assign quantities

80
00:05:09,765 --> 00:05:13,410
in Bayesian for any biological

81
00:05:13,530 --> 00:05:18,030
quantities, which enables us to lend

82
00:05:18,090 --> 00:05:22,065
the explainability to the neural network

83
00:05:22,170 --> 00:05:25,435
dynamics and architectures. So that's a

84
00:05:25,467 --> 00:05:28,990
basic idea. And what we have done

85
00:05:29,082 --> 00:05:32,662
in this paper is that we

86
00:05:33,025 --> 00:05:36,625
consider a biological plausible cost

87
00:05:36,687 --> 00:05:40,485
function for this particular canonical

88
00:05:40,530 --> 00:05:44,685
neural network. And show the equivalence

89
00:05:44,730 --> 00:05:47,065
between that Costa function and the

90
00:05:47,082 --> 00:05:49,837
variation navy energy and the particular

91
00:05:50,500 --> 00:05:52,900
partially observable cognition process

92
00:05:52,962 --> 00:05:53,587
model.

93
00:05:57,850 --> 00:06:01,140
Awesome. So let's look at the parallel

94
00:06:01,320 --> 00:06:04,440
between the cost function for neural

95
00:06:04,470 --> 00:06:07,735
networks and the informational free

96
00:06:07,767 --> 00:06:10,435
energy. So one representation of that

97
00:06:10,467 --> 00:06:13,900
was in figure three. So maybe

98
00:06:13,962 --> 00:06:17,335
could you just describe what is the

99
00:06:17,367 --> 00:06:19,705
structure of the informational free

100
00:06:19,752 --> 00:06:23,230
energy expression and what is the

101
00:06:23,352 --> 00:06:25,537
structure of the loss function?

102
00:06:26,950 --> 00:06:30,795
Okay, so there is a clear parallel

103
00:06:30,885 --> 00:06:34,785
between the functional structure

104
00:06:34,905 --> 00:06:38,415
or those component in informational

105
00:06:38,445 --> 00:06:41,190
free energy and component in neural

106
00:06:41,220 --> 00:06:45,400
network Costa function. So let's say the

107
00:06:45,462 --> 00:06:48,595
first time in F correspond to

108
00:06:48,747 --> 00:06:52,087
the it correspond to the

109
00:06:53,200 --> 00:06:56,485
expectation about hidden states is

110
00:06:56,517 --> 00:06:58,850
a hidden states Australia.

111
00:06:59,950 --> 00:07:03,725
So that part basically indicates

112
00:07:04,975 --> 00:07:08,935
the free energy with respect to the

113
00:07:08,967 --> 00:07:12,112
hidden state. Yeah, and the second part

114
00:07:12,625 --> 00:07:17,095
correspond to the free energy about the

115
00:07:17,247 --> 00:07:21,420
decision posterior. So the indicates

116
00:07:21,510 --> 00:07:24,490
the posterior belief about agent

117
00:07:24,582 --> 00:07:26,525
decision or action.

118
00:07:28,300 --> 00:07:31,360
And now in

119
00:07:31,392 --> 00:07:35,290
terms of the correspondence between the

120
00:07:35,307 --> 00:07:37,510
free energy and neural network function

121
00:07:37,692 --> 00:07:40,770
here the first time in the neural

122
00:07:40,785 --> 00:07:42,820
network function correspond to middle

123
00:07:42,897 --> 00:07:46,780
layer neural activity which has

124
00:07:46,827 --> 00:07:51,195
a recurrent connection and receive

125
00:07:51,285 --> 00:07:54,655
sensory input from sensory layer and

126
00:07:54,702 --> 00:07:58,285
then project the

127
00:07:58,317 --> 00:08:02,485
output to the output layer and

128
00:08:02,517 --> 00:08:04,815
the second term correspond to output

129
00:08:04,845 --> 00:08:08,455
layer which receive signals from middle

130
00:08:08,502 --> 00:08:12,255
layer and send the feedback

131
00:08:12,315 --> 00:08:14,525
response to the environment.

132
00:08:25,037 --> 00:08:28,737
So both of us expressions

133
00:08:29,387 --> 00:08:33,137
have the first term being more like a

134
00:08:33,200 --> 00:08:36,377
cognition perceptual sensory learning

135
00:08:36,470 --> 00:08:40,157
term and the second term is more like

136
00:08:40,235 --> 00:08:43,287
a control theoretic action selection.

137
00:08:43,787 --> 00:08:48,150
And how did you see this

138
00:08:48,512 --> 00:08:50,897
analogy or concordance because it looks

139
00:08:50,930 --> 00:08:52,697
like a zipper, like everything is

140
00:08:52,730 --> 00:08:55,275
totally lined up.

141
00:08:58,137 --> 00:08:58,900
Well,

142
00:09:04,512 --> 00:09:07,450
this graph itself showed a clear

143
00:09:08,262 --> 00:09:12,447
correspondence because now we

144
00:09:12,480 --> 00:09:16,617
are considering a particular form of on

145
00:09:16,665 --> 00:09:20,322
DP in which each element of

146
00:09:20,355 --> 00:09:23,427
hidden states takes either zero or one.

147
00:09:23,595 --> 00:09:27,147
But there are many states so

148
00:09:27,330 --> 00:09:30,175
it is expressed in a form of

149
00:09:33,837 --> 00:09:37,842
factorization. So now we

150
00:09:38,040 --> 00:09:41,125
consider that in terms of the

151
00:09:42,237 --> 00:09:45,687
s fosteria Bordeaux. Upper part

152
00:09:45,750 --> 00:09:48,417
of Bordeaux correspond to the

153
00:09:48,465 --> 00:09:51,597
expectation about each element of s

154
00:09:51,630 --> 00:09:55,767
taking one and lower part of

155
00:09:55,890 --> 00:09:58,347
the bordese correspond to the

156
00:09:58,380 --> 00:10:01,872
expectation about s taking zero. So it

157
00:10:01,905 --> 00:10:06,242
is broke vector about the posterior

158
00:10:06,302 --> 00:10:10,387
expectation and this nicely

159
00:10:10,962 --> 00:10:15,752
correspond to the Brea

160
00:10:15,782 --> 00:10:19,392
vector shown in the bottom up this

161
00:10:19,440 --> 00:10:22,907
figure it is a vector

162
00:10:23,072 --> 00:10:27,572
of x and Bijan

163
00:10:27,767 --> 00:10:31,902
sorry it is a vector of x and by x

164
00:10:31,995 --> 00:10:35,847
and here by x indicate one minus x in

165
00:10:35,880 --> 00:10:40,000
the element y sense which is

166
00:10:40,662 --> 00:10:45,122
exactly correspond to block

167
00:10:45,167 --> 00:10:48,637
vector or expectation.

168
00:10:50,412 --> 00:10:53,800
This correspondence also

169
00:10:54,312 --> 00:10:58,987
observed in the second tab.

170
00:10:59,562 --> 00:11:03,925
Here, log S correspond to log X

171
00:11:04,362 --> 00:11:08,682
and also log

172
00:11:08,760 --> 00:11:12,212
A correspond to the broke matrix

173
00:11:12,287 --> 00:11:15,792
of W log W.

174
00:11:15,990 --> 00:11:19,607
Here W hat indicates

175
00:11:19,697 --> 00:11:23,667
the sigmoidal function of W and its

176
00:11:23,790 --> 00:11:28,142
bar means sigmoidal

177
00:11:28,202 --> 00:11:32,350
function of W. So actually,

178
00:11:33,612 --> 00:11:38,737
because we now consider binary

179
00:11:39,387 --> 00:11:42,562
hidden state and binary observation

180
00:11:42,912 --> 00:11:47,382
it's like reviewed mapping. Mapping from

181
00:11:47,535 --> 00:11:50,637
hidden states to conversation is

182
00:11:50,700 --> 00:11:54,192
expressed as block matrix, which is

183
00:11:54,240 --> 00:11:56,822
exactly correspond to broke matrix shown

184
00:11:56,867 --> 00:11:58,600
in the bottom of this figure.

185
00:12:04,775 --> 00:12:08,770
So like this, for every tab

186
00:12:08,860 --> 00:12:11,645
we have the exact correspondence between

187
00:12:11,797 --> 00:12:14,195
the upper expression and the lower

188
00:12:14,272 --> 00:12:15,225
expression.

189
00:12:17,825 --> 00:12:20,990
So that's why we can say that this is a

190
00:12:21,007 --> 00:12:24,400
natural mapping from neural network

191
00:12:24,475 --> 00:12:26,680
formation to parishional vision

192
00:12:26,740 --> 00:12:27,525
formation.

193
00:12:31,137 --> 00:12:36,027
So it speaks a sort of identity between

194
00:12:36,120 --> 00:12:38,812
those two different expressions.

195
00:12:40,062 --> 00:12:43,362
So although 1 may be able to consider

196
00:12:43,500 --> 00:12:47,042
another mapping from neural network

197
00:12:47,102 --> 00:12:50,502
to Bayesian inference, this is a

198
00:12:50,520 --> 00:12:54,712
sort of simplest mapping.

199
00:12:57,012 --> 00:12:59,202
So how would it look different if it

200
00:12:59,220 --> 00:13:03,242
were three states categorical

201
00:13:03,302 --> 00:13:05,192
distribution or a continuous

202
00:13:05,252 --> 00:13:08,650
distribution? What aspects would change?

203
00:13:09,462 --> 00:13:13,312
Thank you for asking that. So that's

204
00:13:14,037 --> 00:13:17,697
in some sense outside of this paper

205
00:13:17,880 --> 00:13:22,000
because only when we consider a

206
00:13:24,012 --> 00:13:25,600
binary hidden state,

207
00:13:27,537 --> 00:13:32,342
this analogy is established

208
00:13:32,402 --> 00:13:36,417
nicely. Otherwise we need to

209
00:13:36,465 --> 00:13:40,300
consider some attention. So because

210
00:13:42,087 --> 00:13:47,712
consider that each neuron code

211
00:13:47,850 --> 00:13:51,672
the probability or expectation of some

212
00:13:51,705 --> 00:13:55,802
value taking one, then the probability

213
00:13:55,907 --> 00:14:00,492
or expectation of

214
00:14:00,540 --> 00:14:04,687
taking zero can be simply

215
00:14:05,187 --> 00:14:09,077
computed by computing one Ines neural

216
00:14:09,107 --> 00:14:12,047
activity. So actually neural activity

217
00:14:12,167 --> 00:14:16,382
which is a single dimensional variable

218
00:14:16,472 --> 00:14:18,882
is sufficient to express the

219
00:14:18,960 --> 00:14:22,332
expectation. Right? But once we

220
00:14:22,410 --> 00:14:26,597
consider the three state hidden

221
00:14:26,642 --> 00:14:28,675
states program,

222
00:14:30,162 --> 00:14:32,787
this doesn't work. So we need to

223
00:14:32,850 --> 00:14:36,572
consider at least two variables

224
00:14:36,692 --> 00:14:41,222
but it's relation to neural

225
00:14:41,267 --> 00:14:45,552
network expression is not very clear in

226
00:14:45,570 --> 00:14:46,150
general.

227
00:14:51,987 --> 00:14:55,722
That's very interesting why it

228
00:14:55,755 --> 00:15:00,112
would be so strong of a concordance

229
00:15:01,287 --> 00:15:05,717
in a binary case but immediately

230
00:15:05,777 --> 00:15:09,562
unclear for other distributions.

231
00:15:10,737 --> 00:15:11,500
Yeah,

232
00:15:15,462 --> 00:15:19,032
generally for poem DB expression we

233
00:15:19,110 --> 00:15:22,587
consider the one hot

234
00:15:22,650 --> 00:15:25,987
expression, one hot vector expression

235
00:15:26,937 --> 00:15:30,925
which means that we normalize the value

236
00:15:33,162 --> 00:15:36,492
in the sense that the summation of all

237
00:15:36,615 --> 00:15:40,000
variable to be one.

238
00:15:41,562 --> 00:15:44,147
Maybe there is some neural substrate

239
00:15:44,192 --> 00:15:47,177
that achieve that communication.

240
00:15:47,357 --> 00:15:51,527
But for classic

241
00:15:51,632 --> 00:15:55,292
type of neural network like canonical

242
00:15:55,352 --> 00:15:57,700
neural network, consider in this paper

243
00:15:58,587 --> 00:16:02,177
what is that neural substrate

244
00:16:02,357 --> 00:16:05,397
is not very clear. So that's why we

245
00:16:05,505 --> 00:16:08,777
selected the binary case because it's

246
00:16:08,807 --> 00:16:12,567
simple and have a clear

247
00:16:12,615 --> 00:16:16,827
analogy. So what does it mean for

248
00:16:16,920 --> 00:16:20,537
an artificial or for a biological

249
00:16:20,612 --> 00:16:24,712
neuron to have activity dynamics

250
00:16:25,287 --> 00:16:28,537
or plasticity context?

251
00:16:29,487 --> 00:16:33,167
That justifies it being described

252
00:16:33,227 --> 00:16:36,932
as playing like a belief role

253
00:16:37,022 --> 00:16:38,662
in a Bayesian setting.

254
00:16:41,625 --> 00:16:43,905
Higher firing means more belief, higher

255
00:16:43,952 --> 00:16:46,290
firing means lower belief. What does it

256
00:16:46,307 --> 00:16:48,570
really mean to have a connection between

257
00:16:48,722 --> 00:16:50,885
in this episode of street talk belief

258
00:16:50,930 --> 00:16:54,537
states. I see, so if

259
00:16:55,275 --> 00:16:58,725
you assign a mapping, a particular

260
00:16:58,862 --> 00:17:01,770
mapping, then its meaning is also

261
00:17:01,847 --> 00:17:04,365
determined. In this case,

262
00:17:04,532 --> 00:17:07,675
we assign that neural activity

263
00:17:08,025 --> 00:17:11,180
correspond to the posterior

264
00:17:11,240 --> 00:17:14,730
expectation about an element of

265
00:17:14,777 --> 00:17:18,255
hidden states taking one. So once we

266
00:17:18,377 --> 00:17:21,635
define this mapping, then higher neural

267
00:17:21,680 --> 00:17:24,150
activity indicates the higher

268
00:17:24,212 --> 00:17:26,037
probability of taking one.

269
00:17:32,362 --> 00:17:35,462
So neural activity is the probability

270
00:17:36,787 --> 00:17:40,222
neural activity is on the x axis and

271
00:17:40,255 --> 00:17:43,447
the y axis of the sigmoid function is

272
00:17:43,480 --> 00:17:45,575
the probability of taking one.

273
00:17:47,362 --> 00:17:51,137
Well. Neural activity

274
00:17:52,312 --> 00:17:55,707
encodes the expectation. So neural

275
00:17:55,722 --> 00:17:58,957
activity is sigmoidal function

276
00:17:59,035 --> 00:18:00,725
of something itself.

277
00:18:08,487 --> 00:18:11,702
This is because once we see a fixed

278
00:18:11,732 --> 00:18:15,147
point of neural activity equation which

279
00:18:15,180 --> 00:18:18,342
is derived from this cos function, it

280
00:18:18,390 --> 00:18:21,042
has a form of sigma WADA function so x

281
00:18:21,090 --> 00:18:23,372
equals sigmawilder function of graph,

282
00:18:23,417 --> 00:18:26,667
graph, graph. So this form is exactly

283
00:18:26,790 --> 00:18:32,657
correspond to softwaremax

284
00:18:32,747 --> 00:18:36,507
function of something which is seen

285
00:18:36,585 --> 00:18:40,687
in the solution of possibly expectation.

286
00:18:46,600 --> 00:18:49,920
That'S what the neural activity encodes

287
00:18:50,085 --> 00:18:52,925
and what is the Bayesian interpretation

288
00:18:53,500 --> 00:18:56,300
or the update rules on the plasticity.

289
00:18:57,775 --> 00:19:02,215
Okay, that's another important point.

290
00:19:02,382 --> 00:19:07,335
So in terms of posture

291
00:19:07,530 --> 00:19:11,455
of parameters so in the case

292
00:19:11,502 --> 00:19:15,300
of Bayesian force us we consider update

293
00:19:15,375 --> 00:19:19,165
about deleted parameters of

294
00:19:19,257 --> 00:19:21,760
a matrix and B matrix which is usually

295
00:19:21,867 --> 00:19:25,312
expressed by the small case

296
00:19:26,500 --> 00:19:30,355
variable. And its

297
00:19:30,402 --> 00:19:35,365
meaning is that if

298
00:19:35,382 --> 00:19:39,787
we compute the partial derivative of a

299
00:19:41,275 --> 00:19:44,395
partial derivative of F with respect to

300
00:19:44,547 --> 00:19:48,285
small A then its solution it's

301
00:19:48,330 --> 00:19:53,860
fixed point solution looks like an

302
00:19:53,892 --> 00:19:57,535
computer product of which

303
00:19:57,567 --> 00:20:01,612
is also known as Hebbian product

304
00:20:02,125 --> 00:20:06,037
because it has an errors drink to

305
00:20:06,625 --> 00:20:11,225
update depending on the precinaptic

306
00:20:11,575 --> 00:20:14,610
neuron activity and postsynaptic neuron

307
00:20:14,730 --> 00:20:19,110
activity. And according

308
00:20:19,155 --> 00:20:24,255
to this formal equivalent we revisit

309
00:20:24,390 --> 00:20:28,775
we can see again such analogy

310
00:20:29,200 --> 00:20:35,035
in a formal sense here if

311
00:20:35,067 --> 00:20:39,385
we computer the partial derivative of

312
00:20:39,567 --> 00:20:42,505
neural network function with respect to

313
00:20:42,627 --> 00:20:49,762
W then

314
00:20:50,275 --> 00:20:54,765
we can formally derive the Hebbian

315
00:20:54,945 --> 00:20:58,480
prosthesis which depends on

316
00:20:58,527 --> 00:21:02,225
the activity of prey and postsynaptic

317
00:21:03,475 --> 00:21:04,775
neuro activity.

318
00:21:08,337 --> 00:21:12,037
Okay, so hebion plasticity

319
00:21:12,462 --> 00:21:15,602
often described as neurons that fired

320
00:21:15,632 --> 00:21:18,227
together, wired together. Here you're

321
00:21:18,257 --> 00:21:20,732
discussing it in terms of a matrix

322
00:21:20,822 --> 00:21:24,700
operation on the POMDP side between

323
00:21:25,437 --> 00:21:28,975
observables and hidden states.

324
00:21:29,937 --> 00:21:33,250
So there's a hebion plasticity happening

325
00:21:34,737 --> 00:21:38,212
between the perceptual layer

326
00:21:40,287 --> 00:21:42,862
and the cognitive layer,

327
00:21:43,662 --> 00:21:47,575
right? So the first half

328
00:21:48,837 --> 00:21:52,202
of the neural network is trained

329
00:21:52,232 --> 00:21:54,422
according to heavy and plasticity rules

330
00:21:54,467 --> 00:21:58,497
that optimize the A in

331
00:21:58,530 --> 00:22:00,837
terms of the perceptual and learning

332
00:22:00,900 --> 00:22:03,657
like relationship between hidden states

333
00:22:03,735 --> 00:22:07,377
and observables. And then the

334
00:22:07,395 --> 00:22:11,157
second half of the neural network has

335
00:22:11,235 --> 00:22:15,025
a slightly different structure. It is

336
00:22:15,987 --> 00:22:21,037
optimizing based upon retroactive

337
00:22:21,837 --> 00:22:25,612
re analysis of consequences of action

338
00:22:26,637 --> 00:22:29,057
according to the fictive causality

339
00:22:29,222 --> 00:22:30,337
construction.

340
00:22:33,387 --> 00:22:37,497
So actually in

341
00:22:37,530 --> 00:22:41,492
this figure b up layer

342
00:22:41,552 --> 00:22:44,067
correspond to environment and lower part

343
00:22:44,115 --> 00:22:46,967
correspond to agent. So this structure

344
00:22:47,027 --> 00:22:49,225
corresponds to figure eight,

345
00:22:51,012 --> 00:22:53,922
this correspond to a simpler version of

346
00:22:53,955 --> 00:22:58,237
foam DP. So for version of POMDP,

347
00:22:58,737 --> 00:23:01,872
its corresponding neural network is

348
00:23:01,905 --> 00:23:11,765
showing figure four or this paper image

349
00:23:11,795 --> 00:23:15,680
task. This is the neural network

350
00:23:15,740 --> 00:23:16,750
architectures.

351
00:23:22,712 --> 00:23:26,887
So as you say, there is a network

352
00:23:27,037 --> 00:23:29,925
connection from sensory layer to

353
00:23:30,362 --> 00:23:33,902
cognition layer which is expressed by W

354
00:23:33,995 --> 00:23:36,017
here and recurrent connection which

355
00:23:36,065 --> 00:23:39,225
corresponds to state transient matrix is

356
00:23:39,812 --> 00:23:43,247
expressed by K matrix which is

357
00:23:43,280 --> 00:23:46,657
recurrent Sinematic connectivity.

358
00:23:46,747 --> 00:23:50,862
And as you say the action generation

359
00:23:51,512 --> 00:23:55,792
through retrospective reward

360
00:23:55,852 --> 00:24:00,127
or risk evolution is done by output

361
00:24:00,157 --> 00:24:04,627
trigger through the synaptic

362
00:24:04,657 --> 00:24:08,477
connectivity expressed as V in this

363
00:24:08,570 --> 00:24:12,002
figure. So V is

364
00:24:12,020 --> 00:24:15,075
the synaptic connectivity between

365
00:24:15,887 --> 00:24:18,722
cognitive states in the middle layer and

366
00:24:18,755 --> 00:24:21,150
the action selection states in Y.

367
00:24:21,587 --> 00:24:23,807
Exactly. And so in that way V is exactly

368
00:24:23,885 --> 00:24:27,572
analogous to W. But why and

369
00:24:27,605 --> 00:24:30,917
how does gamma come into play only in

370
00:24:30,965 --> 00:24:33,062
this second layer? I mean why not have

371
00:24:33,125 --> 00:24:35,327
gamma one in the first layer? Gamma two

372
00:24:35,345 --> 00:24:36,612
in the second layer.

373
00:24:38,987 --> 00:24:41,925
Generally speaking, it is possible to

374
00:24:42,887 --> 00:24:46,662
moderate plasticity in first layer

375
00:24:48,062 --> 00:24:51,997
using another moderator gamma.

376
00:24:52,117 --> 00:24:55,292
But for complexity we focus only on

377
00:24:55,415 --> 00:24:58,512
neural modulation in the output layer.

378
00:24:59,237 --> 00:25:02,522
Analogy is that for

379
00:25:02,555 --> 00:25:06,352
example, as you said, the first rigor

380
00:25:06,532 --> 00:25:13,275
computer more perceptual things

381
00:25:14,012 --> 00:25:17,477
so perception of external world

382
00:25:17,645 --> 00:25:21,977
and instead on

383
00:25:21,995 --> 00:25:26,002
the other hand middle secondary

384
00:25:26,182 --> 00:25:31,297
which is mapping from cognition

385
00:25:31,492 --> 00:25:39,422
layer to action layer perform

386
00:25:39,530 --> 00:25:43,137
the optimization of its own action.

387
00:25:43,487 --> 00:25:45,300
So for example,

388
00:25:46,937 --> 00:25:50,072
in the story item in

389
00:25:50,105 --> 00:25:53,632
the brain action prediction is optimized

390
00:25:53,722 --> 00:25:56,987
by conversation of

391
00:25:57,050 --> 00:26:01,200
dopamine as a input. So usually

392
00:26:01,787 --> 00:26:04,725
that socket receives signal from

393
00:26:05,312 --> 00:26:09,257
ecological neural socket and send

394
00:26:09,335 --> 00:26:15,312
signal to another neuronal

395
00:26:16,337 --> 00:26:20,512
nucleus in meat

396
00:26:20,587 --> 00:26:24,742
grain. But the point here is that neuron

397
00:26:24,802 --> 00:26:28,972
in storatum encodes some decision

398
00:26:29,092 --> 00:26:32,932
for examples goal or no goal.

399
00:26:33,022 --> 00:26:36,237
So such a decision is encoded.

400
00:26:36,812 --> 00:26:41,197
So now we consider analogy between pond

401
00:26:41,242 --> 00:26:44,947
DP expression in the Bayesian

402
00:26:44,992 --> 00:26:48,677
formation and neural socket in

403
00:26:48,695 --> 00:26:53,637
the brain that optimize

404
00:26:54,137 --> 00:26:58,277
action through some sort of

405
00:26:58,370 --> 00:27:02,132
moderation by another

406
00:27:02,210 --> 00:27:05,072
factor. Here that factor corresponds to

407
00:27:05,105 --> 00:27:13,852
gamma and gamma

408
00:27:13,882 --> 00:27:18,452
has variety of function.

409
00:27:18,545 --> 00:27:22,247
But in this paper we focus only on the

410
00:27:22,280 --> 00:27:25,287
moderation of activity.

411
00:27:26,387 --> 00:27:29,492
So here the behavior activity is not

412
00:27:29,540 --> 00:27:33,517
determined by only a preposter

413
00:27:33,577 --> 00:27:38,507
relationship but determined by three

414
00:27:38,585 --> 00:27:41,625
factors relationship in the sense that

415
00:27:42,362 --> 00:27:46,142
the activity is updated by the

416
00:27:46,190 --> 00:27:48,992
product of gamma and prayer and

417
00:27:49,040 --> 00:27:52,117
postsynaptic activity.

418
00:27:52,252 --> 00:27:55,950
So there are three times in one.

419
00:27:58,037 --> 00:28:01,237
This is why this comma can moderate

420
00:28:01,387 --> 00:28:02,487
prosthesity.

421
00:28:06,150 --> 00:28:10,215
So how would a glial factor look

422
00:28:10,382 --> 00:28:13,785
different computationally? And where in

423
00:28:13,817 --> 00:28:17,750
the brain have people identified levels

424
00:28:17,900 --> 00:28:20,280
or other factors as relevant for

425
00:28:20,327 --> 00:28:24,255
learning? Yeah, that's an interesting

426
00:28:24,377 --> 00:28:28,095
point. I'm not really sure about the

427
00:28:28,247 --> 00:28:30,990
equation of the real moderation of

428
00:28:31,007 --> 00:28:34,260
neural activity or plasticity. There are

429
00:28:34,292 --> 00:28:39,320
many discussions and I'm

430
00:28:39,335 --> 00:28:43,737
sorry, I don't know the exact form,

431
00:28:44,400 --> 00:28:49,375
but one possible implementation

432
00:28:50,625 --> 00:28:53,505
is similar to this type of

433
00:28:53,552 --> 00:28:56,580
neuromodulation. So it

434
00:28:56,627 --> 00:28:59,910
would be possible to model

435
00:29:00,092 --> 00:29:03,285
some real

436
00:29:03,467 --> 00:29:05,730
contribution or free factor to

437
00:29:05,777 --> 00:29:09,620
plasticity in the form of three factor

438
00:29:09,785 --> 00:29:12,785
learning room which is mathematically

439
00:29:12,830 --> 00:29:16,635
speaking Lamme as this type

440
00:29:16,667 --> 00:29:18,700
of neural moderation.

441
00:29:23,750 --> 00:29:27,605
Here in table two we have another set

442
00:29:27,652 --> 00:29:31,420
of correspondences. It's like a sideways

443
00:29:31,585 --> 00:29:34,775
figure three, right? But a little bit

444
00:29:34,837 --> 00:29:36,225
more like a dictionary.

445
00:29:40,862 --> 00:29:43,502
Anything to add? Or any variables that

446
00:29:43,520 --> 00:29:47,297
we haven't really mentioned. What about

447
00:29:47,330 --> 00:29:49,875
the firing thresholds? Because these are

448
00:29:50,312 --> 00:29:54,525
common parameters in a neural model,

449
00:29:55,187 --> 00:29:58,292
however, we don't really hear

450
00:29:58,340 --> 00:30:02,192
about the interpretation of

451
00:30:02,240 --> 00:30:06,397
these constructs within the variational

452
00:30:06,442 --> 00:30:08,112
base POMDP.

453
00:30:09,587 --> 00:30:13,500
Yeah, there is an interesting

454
00:30:14,312 --> 00:30:15,075
story.

455
00:30:17,462 --> 00:30:22,712
That's a very interesting point. So when

456
00:30:22,775 --> 00:30:26,642
we first tried to make

457
00:30:26,765 --> 00:30:30,900
analogy between neurons network and

458
00:30:35,012 --> 00:30:39,242
one program is the

459
00:30:39,290 --> 00:30:41,927
law of threshold factor because as you

460
00:30:41,945 --> 00:30:46,062
said, it is not absorbing POMDP

461
00:30:46,412 --> 00:30:49,907
structure. But there is another

462
00:30:49,985 --> 00:30:53,100
factor in Pompey which is

463
00:30:58,037 --> 00:31:01,382
prior expectation about hidden state

464
00:31:01,460 --> 00:31:04,812
which is usually expressed by D matrix.

465
00:31:06,062 --> 00:31:09,572
And what we consider is

466
00:31:09,605 --> 00:31:13,322
the relationship between d matrix and

467
00:31:13,430 --> 00:31:18,152
firing threshold. And finally,

468
00:31:18,245 --> 00:31:21,667
what we found is that firing

469
00:31:21,727 --> 00:31:25,267
threshold is not equal to the matrix

470
00:31:25,327 --> 00:31:28,837
itself, but it is a summation

471
00:31:28,912 --> 00:31:32,597
of B matrix under some function

472
00:31:32,705 --> 00:31:36,202
of synaptic

473
00:31:36,382 --> 00:31:40,275
strength or which is equal to

474
00:31:42,212 --> 00:31:45,622
a matrix b matrix in the POMDP

475
00:31:45,667 --> 00:31:48,512
formation. In other words, what we found

476
00:31:48,575 --> 00:31:52,352
is that each which is

477
00:31:52,370 --> 00:31:54,592
a firing solution in neural network

478
00:31:54,652 --> 00:31:55,587
architecture,

479
00:31:59,912 --> 00:32:04,267
it is actually an adaptive

480
00:32:04,327 --> 00:32:09,067
threshold which is not a fixed

481
00:32:09,127 --> 00:32:13,127
value, but h is a function of

482
00:32:13,220 --> 00:32:16,962
W sinatic strengths

483
00:32:17,387 --> 00:32:20,850
and h changes depending on

484
00:32:21,887 --> 00:32:25,622
W's value. For example, if W is too

485
00:32:25,655 --> 00:32:30,017
Laje, then your activity can

486
00:32:30,065 --> 00:32:34,642
be unstable. So h behavior

487
00:32:34,702 --> 00:32:38,897
to reduce the activity to make

488
00:32:39,080 --> 00:32:41,262
neural activity more stable.

489
00:32:41,837 --> 00:32:45,202
So we can see an analogy

490
00:32:45,232 --> 00:32:48,450
of omeostatic mechanism here.

491
00:32:55,562 --> 00:32:59,702
If we design A as

492
00:32:59,720 --> 00:33:04,727
the function of w and function

493
00:33:04,820 --> 00:33:10,725
of another factor which is all

494
00:33:11,387 --> 00:33:15,422
part of the term in this table, then we

495
00:33:15,455 --> 00:33:19,262
could make common analogy between this

496
00:33:19,325 --> 00:33:22,817
h and some variable in

497
00:33:23,015 --> 00:33:26,362
palm DP correlation which is shown

498
00:33:26,437 --> 00:33:29,887
in the right hand side of this table.

499
00:33:30,037 --> 00:33:33,452
Although its value is not

500
00:33:33,620 --> 00:33:37,487
simple because it chaos three

501
00:33:37,550 --> 00:33:40,200
different tasks. So all of them

502
00:33:40,787 --> 00:33:43,907
contribute to make

503
00:33:43,985 --> 00:33:45,900
h or M.

504
00:33:48,662 --> 00:33:52,682
But anyway, once we

505
00:33:52,760 --> 00:33:56,017
map, so once we establish a mapping

506
00:33:56,077 --> 00:33:58,937
between h and this value,

507
00:33:59,000 --> 00:34:02,882
then everything works. So the

508
00:34:02,960 --> 00:34:06,612
cost function in different settings

509
00:34:07,037 --> 00:34:10,212
have Omar correspondence.

510
00:34:14,462 --> 00:34:17,562
H and the M firing thresholds.

511
00:34:17,987 --> 00:34:24,152
So H correspond to middle rare M

512
00:34:24,245 --> 00:34:28,137
indicator output raider threshold

513
00:34:30,137 --> 00:34:33,407
which are different variables. And

514
00:34:33,485 --> 00:34:39,967
interestingly h correspond to prior

515
00:34:40,027 --> 00:34:42,467
expectation about hidden states because

516
00:34:42,515 --> 00:34:46,125
it corresponds to community rare and M

517
00:34:46,487 --> 00:34:50,597
correspond a priori belief about its

518
00:34:50,630 --> 00:34:55,397
own action because it

519
00:34:55,430 --> 00:34:58,737
is a bias in the action layer.

520
00:35:00,437 --> 00:35:04,862
Yeah, it's very interesting that the

521
00:35:05,000 --> 00:35:10,472
perceptual firing threshold h only

522
00:35:10,655 --> 00:35:13,725
includes prior beliefs on hidden states,

523
00:35:14,762 --> 00:35:17,327
beliefs about how observations map to

524
00:35:17,345 --> 00:35:20,372
hidden states A and beliefs about how

525
00:35:20,405 --> 00:35:23,147
hidden states change their time B. So

526
00:35:23,180 --> 00:35:26,562
that's like pure passive inference.

527
00:35:27,212 --> 00:35:30,872
And then the firing thresholds for M

528
00:35:31,055 --> 00:35:34,477
correspond to only beliefs

529
00:35:34,507 --> 00:35:37,307
about preferences and beliefs about

530
00:35:37,385 --> 00:35:40,712
actions or habits with C and E. So

531
00:35:40,775 --> 00:35:43,502
there's like a complete division of

532
00:35:43,520 --> 00:35:47,187
labor or partitioning functionally

533
00:35:48,212 --> 00:35:50,717
between these structurally different

534
00:35:50,915 --> 00:35:53,642
parts of the neural network and

535
00:35:53,690 --> 00:35:55,342
structurally different and functionally

536
00:35:55,402 --> 00:35:57,462
different parts of the POMDP.

537
00:35:59,162 --> 00:36:03,087
Yet they're integrated in unified

538
00:36:03,962 --> 00:36:07,662
loss functions or unified imperatives.

539
00:36:10,037 --> 00:36:13,222
And so it's like there's extreme

540
00:36:13,267 --> 00:36:17,562
separability of perception and action

541
00:36:18,812 --> 00:36:22,512
on both sides of the figure one divide,

542
00:36:23,237 --> 00:36:26,797
but also they're

543
00:36:26,842 --> 00:36:29,912
integrated, but they're separate. And

544
00:36:29,975 --> 00:36:31,847
that's what kind of grants it the best

545
00:36:31,880 --> 00:36:35,042
of both worlds because if they were any

546
00:36:35,090 --> 00:36:37,217
more integrated you couldn't really pull

547
00:36:37,265 --> 00:36:40,052
them apart. And if they were any less

548
00:36:40,145 --> 00:36:44,167
integrated then the imperative, the loss

549
00:36:44,227 --> 00:36:46,577
function or the variational free energy

550
00:36:46,670 --> 00:36:49,662
would be ad hoc and unprincipled.

551
00:36:50,537 --> 00:36:52,192
But there's kind of a middle ground

552
00:36:52,252 --> 00:36:54,622
where they have a principled integration

553
00:36:54,742 --> 00:36:56,562
but still a distinguishment.

554
00:36:58,862 --> 00:37:03,412
Right? This is caused

555
00:37:03,487 --> 00:37:11,775
by network structure defined or

556
00:37:12,137 --> 00:37:17,925
it is because the

557
00:37:19,862 --> 00:37:23,422
structure of Bayesian

558
00:37:23,467 --> 00:37:28,022
network defined in the MDV model.

559
00:37:28,205 --> 00:37:32,147
So both of them define the

560
00:37:32,180 --> 00:37:35,282
causal relationship between elements or

561
00:37:35,360 --> 00:37:36,162
quantities.

562
00:37:40,187 --> 00:37:44,342
Its substrate is not important,

563
00:37:44,465 --> 00:37:48,727
so it's relationship is crucial

564
00:37:48,832 --> 00:37:52,792
to determine the cost function or it's

565
00:37:52,852 --> 00:37:56,602
a fixed point in this context. So that's

566
00:37:56,632 --> 00:37:59,747
why we can see the

567
00:37:59,855 --> 00:38:01,212
data analogy.

568
00:38:04,525 --> 00:38:08,100
Well, there's a few technical

569
00:38:08,250 --> 00:38:11,110
points I think we can now go into and

570
00:38:11,142 --> 00:38:14,050
then there will be some more general

571
00:38:14,187 --> 00:38:16,165
points about applications and

572
00:38:16,182 --> 00:38:20,335
intelligence. So first the

573
00:38:20,367 --> 00:38:23,335
code availability statement. Awesome to

574
00:38:23,367 --> 00:38:25,735
see that the MATLAB scripts are

575
00:38:25,767 --> 00:38:29,970
available and also active on Zinodo.

576
00:38:30,135 --> 00:38:33,780
So here is the GitHub repo

577
00:38:33,840 --> 00:38:36,940
for reverse engineering. Do you want to

578
00:38:36,957 --> 00:38:39,085
give any overview descriptions of what

579
00:38:39,117 --> 00:38:41,505
people can expect to see in this repo?

580
00:38:41,640 --> 00:38:45,050
And also what about using MATLAB?

581
00:38:50,062 --> 00:38:53,457
Why did you use MATLAB? What advantages

582
00:38:53,547 --> 00:38:56,087
or limitations do you see in MATLAB?

583
00:38:58,387 --> 00:39:02,107
So, because this is a very simple

584
00:39:02,260 --> 00:39:06,627
simulation, so Macrab

585
00:39:06,657 --> 00:39:09,725
is sufficient to encode the whole

586
00:39:10,612 --> 00:39:11,537
script.

587
00:39:14,812 --> 00:39:19,897
We also try some implication in

588
00:39:19,930 --> 00:39:22,025
the material. See here,

589
00:39:23,437 --> 00:39:28,222
if you run the script, then you

590
00:39:28,255 --> 00:39:32,032
can see the process of

591
00:39:32,110 --> 00:39:34,937
an agent solving the maze task.

592
00:39:39,412 --> 00:39:41,837
What did they do in the maze task?

593
00:39:42,787 --> 00:39:46,027
So here the aim of this

594
00:39:46,120 --> 00:39:50,827
agent is to reach

595
00:39:50,920 --> 00:39:54,887
the right hand side of this maze.

596
00:39:56,287 --> 00:39:59,752
Because this is a typical example of

597
00:39:59,845 --> 00:40:02,542
the rate moderation task. That's why we

598
00:40:02,590 --> 00:40:05,237
select the main task.

599
00:40:06,637 --> 00:40:10,402
So to achieve this next task, is it

600
00:40:10,495 --> 00:40:15,442
required to make some plan to be

601
00:40:15,490 --> 00:40:19,752
able to select a good decision?

602
00:40:19,932 --> 00:40:23,472
Because without planning,

603
00:40:23,667 --> 00:40:28,402
you may encounter the war and cannot go

604
00:40:28,570 --> 00:40:31,812
further and you may fail

605
00:40:31,962 --> 00:40:35,277
the image. But with learning,

606
00:40:35,382 --> 00:40:39,667
it is possible to see the path to

607
00:40:39,715 --> 00:40:43,700
reach the right hand side of this space.

608
00:40:46,012 --> 00:40:49,112
So does it know its exploration?

609
00:40:53,137 --> 00:40:56,947
Yeah, it received a state from

610
00:40:57,055 --> 00:41:00,642
neighboring eleven times eleven Jelle.

611
00:41:00,702 --> 00:41:04,312
So which is shown in the bottom part.

612
00:41:04,375 --> 00:41:09,547
Yeah, this left

613
00:41:09,730 --> 00:41:13,287
figure C indicates the observation.

614
00:41:13,362 --> 00:41:18,275
So eleven times eleven state around

615
00:41:19,162 --> 00:41:22,192
the agent position. Okay.

616
00:41:22,240 --> 00:41:26,317
Now agent is on the right hand side of

617
00:41:26,365 --> 00:41:30,172
image at the goal position and it

618
00:41:30,280 --> 00:41:32,900
observes our neighboring state.

619
00:41:36,937 --> 00:41:40,237
Well, a few interesting things here.

620
00:41:40,300 --> 00:41:43,250
It's looking off the right end.

621
00:41:43,762 --> 00:41:46,862
Yeah. And it has this kind of periodic

622
00:41:47,812 --> 00:41:51,137
belief in the key distribution.

623
00:41:51,562 --> 00:41:55,882
Why? I think

624
00:41:55,960 --> 00:42:00,697
it is because when

625
00:42:00,880 --> 00:42:04,132
the agent is in the middle point

626
00:42:04,210 --> 00:42:10,447
of maze, then all

627
00:42:10,480 --> 00:42:13,377
neighboring state is in the maze.

628
00:42:13,407 --> 00:42:17,272
So there is a

629
00:42:17,305 --> 00:42:21,172
path and there is a wall. So this makes

630
00:42:21,205 --> 00:42:26,762
some ergodicity because mazes

631
00:42:27,112 --> 00:42:30,847
have some structures and actually have

632
00:42:30,880 --> 00:42:35,107
a periodic structure and

633
00:42:35,185 --> 00:42:39,817
only at the goal position, then right

634
00:42:39,865 --> 00:42:42,650
hand side becomes war.

635
00:42:43,687 --> 00:42:47,300
But it is not common

636
00:42:48,637 --> 00:42:50,450
for this agent.

637
00:42:52,312 --> 00:42:56,332
This is because this agent show

638
00:42:56,410 --> 00:43:02,762
such a priori pattern.

639
00:43:04,237 --> 00:43:07,852
Yeah, the streets are one wide and

640
00:43:07,870 --> 00:43:10,325
they tend to be separated by one.

641
00:43:11,362 --> 00:43:15,172
So we see this periodicity. What is the

642
00:43:15,205 --> 00:43:19,102
numbers in this middle bottom plot and

643
00:43:19,120 --> 00:43:21,062
what does the checker board represent?

644
00:43:21,487 --> 00:43:22,250
Yeah,

645
00:43:23,512 --> 00:43:27,350
hered correspond to

646
00:43:27,787 --> 00:43:30,022
possibility, expectation about active

647
00:43:30,055 --> 00:43:35,022
states and decision. So middle indicate

648
00:43:35,142 --> 00:43:38,687
decision posterior and decision.

649
00:43:39,037 --> 00:43:42,575
Here we characterize decision as

650
00:43:43,012 --> 00:43:47,547
a secret of four step actions.

651
00:43:47,667 --> 00:43:51,082
So each action correspond to a

652
00:43:51,160 --> 00:43:55,537
movement to right

653
00:43:55,600 --> 00:43:58,700
or left or up or down.

654
00:43:59,362 --> 00:44:03,522
And we consider a four step sequence

655
00:44:03,567 --> 00:44:07,312
of that option which is expressed as

656
00:44:07,375 --> 00:44:11,225
D. So it has

657
00:44:12,862 --> 00:44:15,367
four power,

658
00:44:15,565 --> 00:44:19,637
four possibility.

659
00:44:21,337 --> 00:44:24,262
256. Yeah,

660
00:44:24,325 --> 00:44:28,392
256. So this is a protein

661
00:44:28,452 --> 00:44:31,637
on XY coordinate

662
00:44:32,287 --> 00:44:35,647
because in the

663
00:44:35,680 --> 00:44:39,142
middle panel, middle point correspond to

664
00:44:39,190 --> 00:44:43,522
the current position of agent. And with

665
00:44:43,630 --> 00:44:50,692
four step movement, agent can go one

666
00:44:50,740 --> 00:44:55,897
of any current position and

667
00:44:55,930 --> 00:44:59,727
the current brightness corresponds

668
00:44:59,757 --> 00:45:02,977
to the expectation about

669
00:45:03,070 --> 00:45:06,802
the agent decision. Well this is very

670
00:45:06,895 --> 00:45:10,117
interesting. If we just were to think

671
00:45:10,165 --> 00:45:12,622
about you're at a point and you can go

672
00:45:12,655 --> 00:45:14,062
up, down, left, right, you have four

673
00:45:14,125 --> 00:45:17,572
moves. Naively it

674
00:45:17,605 --> 00:45:20,077
sounds like, well it should look like a

675
00:45:20,095 --> 00:45:23,272
gaussian blur. Most of those should

676
00:45:23,305 --> 00:45:25,627
cancel out and then it should become

677
00:45:25,720 --> 00:45:28,822
rarer and rarer monotonically. But

678
00:45:28,855 --> 00:45:32,300
actually you start in the middle,

679
00:45:33,262 --> 00:45:36,662
you can't end up on these white squares

680
00:45:37,987 --> 00:45:40,702
because it's like one, two,

681
00:45:40,870 --> 00:45:43,550
three and then you have to leave.

682
00:45:44,737 --> 00:45:48,567
Right? So it's kind of like horses

683
00:45:48,627 --> 00:45:52,647
in chess or other pieces

684
00:45:52,842 --> 00:45:55,562
where actually their embodiment,

685
00:45:56,737 --> 00:45:59,617
it's very unexpected that you can't in

686
00:45:59,665 --> 00:46:01,777
four moves end up next to where you

687
00:46:01,795 --> 00:46:05,837
began when you can be so much further.

688
00:46:06,262 --> 00:46:08,375
And then we see this kind of like

689
00:46:08,812 --> 00:46:13,037
embodied inferential prior with QS

690
00:46:14,062 --> 00:46:17,857
that embodies regularity beliefs about

691
00:46:17,935 --> 00:46:19,947
the width of the road and the separation

692
00:46:19,992 --> 00:46:22,597
of the roads. And then there's these

693
00:46:22,630 --> 00:46:26,782
like embodied action priors and

694
00:46:26,935 --> 00:46:29,975
real consequences that have to do with

695
00:46:30,637 --> 00:46:33,362
the structure of movement.

696
00:46:34,837 --> 00:46:39,682
So what it's doing? It's thinking about

697
00:46:39,835 --> 00:46:42,667
policies of length four. There's 256

698
00:46:42,715 --> 00:46:45,637
policies of length four. There's some

699
00:46:45,700 --> 00:46:48,397
degeneracy because there's obviously not

700
00:46:48,430 --> 00:46:52,117
256 squares here. So while only one

701
00:46:52,165 --> 00:46:54,275
policy is going to take you up, up,

702
00:46:55,012 --> 00:46:58,347
down, down, other squares

703
00:46:58,392 --> 00:47:01,417
are reachable. Like the center square is

704
00:47:01,465 --> 00:47:04,417
probably the mode because it can be

705
00:47:04,465 --> 00:47:08,452
reached at least a handful of

706
00:47:08,470 --> 00:47:12,307
ways. And then

707
00:47:12,385 --> 00:47:15,082
at each time point it's basically saying

708
00:47:15,160 --> 00:47:19,175
okay, I know where my X position is

709
00:47:19,537 --> 00:47:23,142
and given my local eleven by eleven

710
00:47:23,202 --> 00:47:27,022
view, I'm trying to

711
00:47:27,055 --> 00:47:28,550
plan to go right.

712
00:47:34,662 --> 00:47:37,077
And then here through time in the

713
00:47:37,095 --> 00:47:39,837
simulation here it starts at 30

714
00:47:39,975 --> 00:47:42,927
something, it quickly figures out how to

715
00:47:42,945 --> 00:47:46,122
get to about 40 and then it's kind of

716
00:47:46,155 --> 00:47:49,802
going up and down on 40. But it can't

717
00:47:49,832 --> 00:47:53,022
really break out because all

718
00:47:53,055 --> 00:47:56,662
of these bottom four routes are closed.

719
00:47:57,162 --> 00:48:00,417
It has a breakout and then very quickly

720
00:48:00,615 --> 00:48:03,325
it hits another plateau around 60.

721
00:48:04,212 --> 00:48:07,797
Right. Then it kind of has

722
00:48:07,830 --> 00:48:11,307
a very nice breakout and in just a

723
00:48:11,310 --> 00:48:13,075
few steps goes very far.

724
00:48:15,162 --> 00:48:17,802
So what is dopamine doing? Or how is

725
00:48:17,820 --> 00:48:20,952
Dopamine helping it in the plateau and

726
00:48:20,970 --> 00:48:22,987
then to break out of the plateau?

727
00:48:23,562 --> 00:48:27,550
Yes. So this agent

728
00:48:28,737 --> 00:48:33,187
learned this particular structure

729
00:48:36,687 --> 00:48:39,927
through many trials. So before

730
00:48:40,020 --> 00:48:42,347
training it failed to reach the goal,

731
00:48:42,392 --> 00:48:46,537
but after training it achieved

732
00:48:47,937 --> 00:48:51,487
such a nice behavior.

733
00:48:52,062 --> 00:48:55,697
So to active this, the role of domain

734
00:48:55,742 --> 00:48:58,877
is that we design gamma

735
00:48:58,907 --> 00:49:02,772
function such that if the

736
00:49:02,805 --> 00:49:07,392
agent can move rightward with

737
00:49:07,440 --> 00:49:11,067
some distance during some time

738
00:49:11,115 --> 00:49:15,282
limit, then risk becomes small like

739
00:49:15,435 --> 00:49:19,022
say comma equals zero nor risk

740
00:49:19,142 --> 00:49:22,150
situation in that sense. In that case,

741
00:49:23,412 --> 00:49:26,732
this agent updates synaptic weights

742
00:49:26,897 --> 00:49:29,437
through hebion frostbust.

743
00:49:30,087 --> 00:49:33,587
But if the agent failed

744
00:49:33,662 --> 00:49:37,572
to go rightward with

745
00:49:37,605 --> 00:49:41,772
some distance during a limited time

746
00:49:41,955 --> 00:49:45,567
frame, then gamma becomes large

747
00:49:45,690 --> 00:49:49,722
like zero six which is

748
00:49:49,755 --> 00:49:53,112
larger than the average zero five.

749
00:49:53,250 --> 00:49:57,642
Then we design

750
00:49:57,765 --> 00:50:01,462
that drawing in attention

751
00:50:02,712 --> 00:50:05,907
antihebian prosthesis occurred instead

752
00:50:05,985 --> 00:50:09,262
of Hebbian. So antihabion

753
00:50:10,737 --> 00:50:13,992
indicates the works as the

754
00:50:14,115 --> 00:50:18,462
disassociation between the current state

755
00:50:18,525 --> 00:50:21,822
and current decisions. Because the

756
00:50:21,855 --> 00:50:24,717
current decision does work, it's not

757
00:50:24,840 --> 00:50:28,362
good decision. So we

758
00:50:28,425 --> 00:50:31,767
try to make the agent who will get

759
00:50:31,965 --> 00:50:35,687
that particular decision rules

760
00:50:35,837 --> 00:50:38,952
through conversation of heavy and

761
00:50:38,970 --> 00:50:42,182
plasticity done by Dharma factory.

762
00:50:42,347 --> 00:50:46,572
So this can be an

763
00:50:46,605 --> 00:50:49,427
arrow to the Dopamine moderation heavy

764
00:50:49,457 --> 00:50:53,275
and activity. So if the policy

765
00:50:53,637 --> 00:50:57,302
is resulting in the expected outcome,

766
00:50:57,482 --> 00:51:01,047
gamma stays at .5, the policy is

767
00:51:01,080 --> 00:51:04,352
as risky or consequential as expected,

768
00:51:04,532 --> 00:51:07,692
and then the policy can either go better

769
00:51:07,740 --> 00:51:10,592
than expected, which facilitates

770
00:51:10,652 --> 00:51:13,602
learning to support that decision. To be

771
00:51:13,620 --> 00:51:16,632
made more or the outcome of the policy

772
00:51:16,710 --> 00:51:19,167
can be worse than expected, which

773
00:51:19,290 --> 00:51:23,907
disassociates previous conceptions to

774
00:51:24,060 --> 00:51:26,287
discourage that kind of behavior.

775
00:51:26,937 --> 00:51:31,197
Exactly. Crucial point is that

776
00:51:31,305 --> 00:51:35,142
this association with different time

777
00:51:35,190 --> 00:51:38,125
frame in the sense that we consider

778
00:51:39,537 --> 00:51:42,837
multiplication of current risk and least

779
00:51:42,900 --> 00:51:46,675
decisions to average over

780
00:51:47,037 --> 00:51:50,987
past two present Hebbian

781
00:51:51,062 --> 00:51:54,242
product. This makes an association

782
00:51:54,302 --> 00:51:56,757
between past decision secrets and the

783
00:51:56,760 --> 00:52:01,682
current risk which enables to optimize

784
00:52:01,847 --> 00:52:05,457
decision to minimize the future

785
00:52:05,535 --> 00:52:09,112
risk. It is just a safe time frame.

786
00:52:11,562 --> 00:52:15,325
So here risk is being used in

787
00:52:16,512 --> 00:52:19,497
a formal sense similar to how it's used

788
00:52:19,530 --> 00:52:22,632
in economics which is the

789
00:52:22,710 --> 00:52:25,347
associated uncertainty of outcomes with

790
00:52:25,380 --> 00:52:26,800
respect to a policy.

791
00:52:28,062 --> 00:52:30,850
Where does danger come into play?

792
00:52:31,737 --> 00:52:34,982
Like what if there was an adversary

793
00:52:35,072 --> 00:52:38,292
in the maze or something that was

794
00:52:38,340 --> 00:52:41,967
dangerous? How does this kind of model

795
00:52:42,090 --> 00:52:46,167
accommodate or hunger or

796
00:52:46,215 --> 00:52:47,867
different kinds of competitions?

797
00:52:47,927 --> 00:52:49,692
Because right now it's basically just

798
00:52:49,740 --> 00:52:51,882
trying to diffuse right word with a

799
00:52:51,885 --> 00:52:55,362
bias. Right? But how do

800
00:52:55,425 --> 00:52:59,287
different kind of situational elements

801
00:52:59,937 --> 00:53:02,162
become interface into the generative

802
00:53:02,237 --> 00:53:05,575
model and generative process? Okay,

803
00:53:06,312 --> 00:53:11,192
any of those factors

804
00:53:11,327 --> 00:53:15,827
can be involved in risk factor,

805
00:53:15,932 --> 00:53:18,327
a single risk factor. So you can

806
00:53:18,345 --> 00:53:21,977
arbitrary design and risk factor

807
00:53:22,157 --> 00:53:27,187
because risk factor moderate

808
00:53:28,137 --> 00:53:31,617
generative model. So that's why agent

809
00:53:31,740 --> 00:53:38,200
try to minimize the risk through

810
00:53:38,637 --> 00:53:40,997
basic embryo updating. But the risk

811
00:53:41,042 --> 00:53:44,412
itself is in some sense outside

812
00:53:44,550 --> 00:53:47,287
of such a Bayesian framework.

813
00:53:47,712 --> 00:53:51,625
So we can design

814
00:53:52,212 --> 00:53:55,542
arbitrary risk. So it may involve some

815
00:53:55,590 --> 00:53:57,787
danger factor,

816
00:54:00,012 --> 00:54:01,537
any other factor.

817
00:54:03,762 --> 00:54:06,787
And this simulation,

818
00:54:08,412 --> 00:54:12,672
it is a POMDP or

819
00:54:12,705 --> 00:54:15,587
it is a neural network. And what scripts

820
00:54:15,662 --> 00:54:19,122
might we look at to understand the

821
00:54:19,155 --> 00:54:22,975
structure of the maze agents? Okay,

822
00:54:23,862 --> 00:54:28,292
it is basically expressed

823
00:54:28,427 --> 00:54:32,222
using the quantity in home depp

824
00:54:32,417 --> 00:54:35,922
for tractability. But for example, if

825
00:54:35,955 --> 00:54:48,187
you see the MDP

826
00:54:48,987 --> 00:54:56,237
learning probably okay,

827
00:54:56,750 --> 00:55:00,170
there is a variable Lamme

828
00:55:00,247 --> 00:55:04,115
type in

829
00:55:04,132 --> 00:55:07,640
the definition of SIM type

830
00:55:07,732 --> 00:55:10,600
correspond to the type of simulation.

831
00:55:10,675 --> 00:55:15,160
So if it's one or two it becomes

832
00:55:15,355 --> 00:55:19,415
homo DP or neural network to

833
00:55:19,432 --> 00:55:21,962
my understanding. Well,

834
00:55:25,100 --> 00:55:28,910
in this particular example,

835
00:55:29,092 --> 00:55:33,437
Jelle, we use the

836
00:55:35,225 --> 00:55:42,362
let's say maybe

837
00:55:42,425 --> 00:55:46,072
it's not good example that DeForest

838
00:55:46,117 --> 00:55:54,217
is learning the deforesting

839
00:55:54,277 --> 00:55:58,607
this script as well. So maybe

840
00:55:58,760 --> 00:56:00,825
another as an example,

841
00:56:04,937 --> 00:56:06,000
let's see.

842
00:56:09,287 --> 00:56:13,312
What is MDP init is initiating

843
00:56:13,462 --> 00:56:16,202
the markup decision process. Exactly.

844
00:56:16,295 --> 00:56:18,975
It's just determining the initial state

845
00:56:19,337 --> 00:56:23,700
of the computer generative model

846
00:56:26,462 --> 00:56:31,125
fe compute variational free energy or

847
00:56:31,787 --> 00:56:35,767
risk MDP computer risk

848
00:56:35,827 --> 00:56:40,367
function. So basically

849
00:56:40,490 --> 00:56:44,977
we use the neural network structure

850
00:56:45,007 --> 00:56:49,512
computation in this particular setup.

851
00:56:50,237 --> 00:56:56,197
So when you click maze

852
00:56:56,392 --> 00:57:00,947
m then in

853
00:57:00,980 --> 00:57:04,875
the line 31

854
00:57:05,462 --> 00:57:08,417
line we determine that Lamme type is

855
00:57:08,465 --> 00:57:10,962
two. This correspond to neural network

856
00:57:12,362 --> 00:57:15,902
architecture. So there is

857
00:57:15,920 --> 00:57:18,467
a very slight difference between home

858
00:57:18,515 --> 00:57:20,917
depicture and neural network

859
00:57:20,977 --> 00:57:24,472
architecture because assuming

860
00:57:24,517 --> 00:57:26,812
neural network architecture correspond

861
00:57:26,887 --> 00:57:28,875
to, you know,

862
00:57:30,887 --> 00:57:41,787
considering considering

863
00:57:52,787 --> 00:57:55,350
okay, well,

864
00:57:56,537 --> 00:58:00,937
if you choose the palm DP architectures,

865
00:58:01,012 --> 00:58:05,057
then we sometimes use

866
00:58:05,210 --> 00:58:07,637
the gamma function to computer the

867
00:58:07,700 --> 00:58:12,612
posterior expectation about parameters.

868
00:58:13,337 --> 00:58:18,312
But in the neural network modeling

869
00:58:19,862 --> 00:58:22,997
the gamma function doesn't appear but it

870
00:58:23,030 --> 00:58:27,242
is replaced with the logarithm of some

871
00:58:27,290 --> 00:58:30,722
function. And simply speaking,

872
00:58:30,830 --> 00:58:32,872
the difference between the gamma

873
00:58:32,917 --> 00:58:35,527
function of something and the logarithm

874
00:58:35,557 --> 00:58:39,547
function of something is asymmetry

875
00:58:39,592 --> 00:58:43,672
Lamme. So that's why we can transform

876
00:58:43,867 --> 00:58:47,752
home DP two neural network

877
00:58:47,857 --> 00:58:49,967
architectures. When the number of

878
00:58:50,090 --> 00:58:52,800
samples is sufficiently large.

879
00:58:57,287 --> 00:59:01,597
Which form do you expect performs

880
00:59:01,642 --> 00:59:04,727
better under small or large amounts of

881
00:59:04,745 --> 00:59:08,072
data? Well,

882
00:59:08,180 --> 00:59:11,927
for large amount of data they work in

883
00:59:11,945 --> 00:59:14,812
the same manner. Same manner.

884
00:59:14,887 --> 00:59:18,332
For small amount

885
00:59:18,410 --> 00:59:22,922
of examples, I'm not

886
00:59:23,105 --> 00:59:28,037
truly sure but it

887
00:59:28,100 --> 00:59:30,752
corresponds to assumptions about the

888
00:59:30,845 --> 00:59:33,292
posterior belief distribution.

889
00:59:33,427 --> 00:59:37,657
So if you assume delicious distribution

890
00:59:37,747 --> 00:59:41,942
then your

891
00:59:41,990 --> 00:59:46,337
resulting function form is something

892
00:59:46,475 --> 00:59:50,475
that used the gamma function

893
00:59:51,362 --> 00:59:53,672
in terms of basic inference probably

894
00:59:53,780 --> 00:59:55,362
which is optimal.

895
01:00:00,512 --> 01:00:03,912
All right, let's return to the earliest

896
01:00:04,862 --> 01:00:06,600
questions from today.

897
01:00:08,012 --> 01:00:11,972
So in

898
01:00:12,005 --> 01:00:15,772
your script, which people can reference,

899
01:00:15,892 --> 01:00:19,575
there's basically a toggle between

900
01:00:20,162 --> 01:00:23,912
having it in SIM type one or SIM type

901
01:00:23,975 --> 01:00:27,547
two corresponding to the POMDP

902
01:00:27,592 --> 01:00:31,547
in the neural network. What about if

903
01:00:31,580 --> 01:00:34,592
there's a published neural network or

904
01:00:34,640 --> 01:00:38,550
POMDP? How can we use

905
01:00:38,912 --> 01:00:43,007
this architecture to create

906
01:00:43,160 --> 01:00:44,412
a translation?

907
01:00:48,000 --> 01:00:50,310
Is there any difference in this? Kind of

908
01:00:50,342 --> 01:00:52,900
like translating models in the wild

909
01:00:55,575 --> 01:00:59,500
different than the full construction

910
01:01:00,225 --> 01:01:04,620
of a special script that can speak

911
01:01:04,697 --> 01:01:05,800
both languages?

912
01:01:10,500 --> 01:01:14,435
Well, in terms of script there's

913
01:01:14,480 --> 01:01:16,062
no difference,

914
01:01:17,400 --> 01:01:20,430
sympathetic difference. Right.

915
01:01:20,627 --> 01:01:23,835
So they work in

916
01:01:23,867 --> 01:01:27,860
the same manner. So only a translation

917
01:01:27,980 --> 01:01:42,735
of variable the

918
01:01:42,767 --> 01:01:46,155
same source code in two different ways.

919
01:01:46,277 --> 01:01:48,665
So if you see that this is a neural

920
01:01:48,695 --> 01:01:52,590
network generation, then it

921
01:01:52,607 --> 01:01:56,085
is translated as a neural network. Or if

922
01:01:56,117 --> 01:01:58,640
you see that this is a POMDP, then it's

923
01:01:58,670 --> 01:01:59,650
POMDP.

924
01:02:03,075 --> 01:02:07,065
So for some neural network being used in

925
01:02:07,082 --> 01:02:09,025
an industrial setting,

926
01:02:10,200 --> 01:02:13,490
how would we get from the neural

927
01:02:13,520 --> 01:02:17,310
network to a POMDP? And where

928
01:02:17,342 --> 01:02:19,185
or how would that representation be

929
01:02:19,217 --> 01:02:24,885
valuable? Right? So when

930
01:02:24,917 --> 01:02:28,590
neural network in the different

931
01:02:28,682 --> 01:02:31,950
architectures the important point

932
01:02:32,012 --> 01:02:35,655
is that we consider a particular form of

933
01:02:35,702 --> 01:02:38,750
neural network which is called canonical

934
01:02:38,825 --> 01:02:42,180
neural network architecture. So only

935
01:02:42,227 --> 01:02:47,075
when we assume this crossover

936
01:02:47,150 --> 01:02:50,400
neural network then you can find

937
01:02:50,462 --> 01:02:53,385
the exact correspondence to a particular

938
01:02:53,492 --> 01:02:57,210
form of POMDP. Otherwise you

939
01:02:57,242 --> 01:03:01,205
need to establish another equivalent

940
01:03:01,265 --> 01:03:04,225
between another form of neural network

941
01:03:04,800 --> 01:03:09,485
architectures and some sort of Bayesian

942
01:03:09,605 --> 01:03:13,740
model. This may

943
01:03:13,832 --> 01:03:17,487
be expressed by POMDP, but maybe

944
01:03:19,500 --> 01:03:23,870
not so straightforward to be expressed

945
01:03:24,035 --> 01:03:27,850
as the computational AP architectures.

946
01:03:29,175 --> 01:03:31,820
So what is it about the canonical neural

947
01:03:31,835 --> 01:03:35,345
network architecture that facilitates

948
01:03:35,435 --> 01:03:38,637
its translation into the POMDP form?

949
01:03:39,225 --> 01:03:42,905
Yeah, first of all, it assumes sigmoid

950
01:03:42,965 --> 01:03:46,025
or activation function. It is nicely

951
01:03:46,100 --> 01:03:49,320
correspond to enthralpy time in the

952
01:03:49,397 --> 01:03:52,975
force DP equations from DP formulations.

953
01:03:53,400 --> 01:03:57,912
So that's why we can

954
01:03:58,950 --> 01:04:02,160
clear marketing. So yeah, in other

955
01:04:02,192 --> 01:04:06,255
words, simply speaking, they have

956
01:04:06,377 --> 01:04:09,900
the same nonlinearity. That's why this

957
01:04:10,037 --> 01:04:17,775
translation is very easy with

958
01:04:17,837 --> 01:04:21,850
another nonlinearity or neural network

959
01:04:22,200 --> 01:04:25,530
equation, then we need

960
01:04:25,577 --> 01:04:29,780
to find another type of entropy

961
01:04:29,915 --> 01:04:32,540
equation or another type of prior

962
01:04:32,645 --> 01:04:36,175
distribution. It is very nontrivial.

963
01:04:38,175 --> 01:04:41,880
How does one even go about doing

964
01:04:41,927 --> 01:04:42,912
that research?

965
01:04:49,575 --> 01:04:52,620
If you want to go

966
01:04:52,697 --> 01:04:55,080
that direction, then I think the first

967
01:04:55,127 --> 01:04:58,880
step is to find the prior

968
01:04:58,940 --> 01:05:07,860
brief, which makes the

969
01:05:07,892 --> 01:05:11,895
prior brief and find the

970
01:05:11,972 --> 01:05:14,765
equivalence between a particular neural

971
01:05:14,795 --> 01:05:16,980
network architectures and particular

972
01:05:17,102 --> 01:05:19,137
Bayesian model.

973
01:05:25,312 --> 01:05:29,047
This sigmoidal activation is

974
01:05:29,080 --> 01:05:33,200
interesting. It corresponds to

975
01:05:34,537 --> 01:05:37,562
general patterns seen in psychophysics,

976
01:05:37,987 --> 01:05:41,692
like two objects that are the same

977
01:05:41,740 --> 01:05:45,052
weight. You're going to have a chance of

978
01:05:45,070 --> 01:05:47,857
saying that one is heavier and then

979
01:05:48,010 --> 01:05:51,050
initially the difference has the most

980
01:05:52,462 --> 01:05:55,737
returns on that decision

981
01:05:55,812 --> 01:05:57,962
being made accurately.

982
01:05:59,287 --> 01:06:02,537
And then as it crosses some threshold

983
01:06:02,887 --> 01:06:06,192
where it just is beyond a noticeable

984
01:06:06,252 --> 01:06:09,222
difference, the decision becomes

985
01:06:09,267 --> 01:06:11,827
essentially probabilistic, like the

986
01:06:11,845 --> 01:06:14,312
firing curve becomes saturated,

987
01:06:15,412 --> 01:06:18,597
the neuron chaos, a very low belief

988
01:06:18,642 --> 01:06:20,812
about zero or very high belief about

989
01:06:20,875 --> 01:06:23,150
zero or one flip that.

990
01:06:26,587 --> 01:06:29,697
So there is a nice grounding

991
01:06:29,817 --> 01:06:33,672
of that kind of a sigmoidal response

992
01:06:33,717 --> 01:06:36,042
curve with respect to stimuli

993
01:06:36,102 --> 01:06:40,625
differences and it has of course,

994
01:06:42,187 --> 01:06:45,362
tractable analytical properties,

995
01:06:45,712 --> 01:06:48,562
but it also just happens to be a good

996
01:06:48,625 --> 01:06:51,982
response summarizer. Yeah,

997
01:06:52,135 --> 01:06:55,147
you're right. So sigmoidar function is

998
01:06:55,180 --> 01:06:57,382
also known as a psychometric function,

999
01:06:57,460 --> 01:07:00,950
as you say. We observe that

1000
01:07:01,762 --> 01:07:05,172
characteristic in many psychical

1001
01:07:05,292 --> 01:07:08,947
experiments. And the previous work also

1002
01:07:09,130 --> 01:07:12,577
said that even

1003
01:07:12,670 --> 01:07:15,327
at the single neuron level, neuronal

1004
01:07:15,357 --> 01:07:19,347
level, the same behavior were absorbed,

1005
01:07:19,467 --> 01:07:23,312
which means that each heuristic

1006
01:07:23,737 --> 01:07:27,950
we can reobserve the similar

1007
01:07:28,312 --> 01:07:31,717
property, which is sometimes called

1008
01:07:31,765 --> 01:07:34,475
as neurometric function,

1009
01:07:35,362 --> 01:07:39,222
which is which have the form of sigmoida

1010
01:07:39,342 --> 01:07:40,700
activation function.

1011
01:07:43,312 --> 01:07:48,022
So it is nice reason

1012
01:07:48,130 --> 01:07:51,952
to design

1013
01:07:52,045 --> 01:07:54,307
neural network architecture using a

1014
01:07:54,310 --> 01:07:56,675
sigmoid or function.

1015
01:07:58,837 --> 01:08:02,977
All right, let's cover a few questions

1016
01:08:03,070 --> 01:08:05,722
in the chat from Dave and then in the

1017
01:08:05,755 --> 01:08:09,962
end turn to some general thoughts.

1018
01:08:10,462 --> 01:08:13,177
Okay, this was when we were looking at

1019
01:08:13,345 --> 01:08:17,307
figure three. So you described

1020
01:08:17,472 --> 01:08:21,962
these vectors

1021
01:08:22,462 --> 01:08:25,447
or matrices. What kind of matrix or

1022
01:08:25,480 --> 01:08:32,035
vector did you describe? The mass block

1023
01:08:32,080 --> 01:08:36,000
matrix. Block matrix.

1024
01:08:38,900 --> 01:08:43,060
Block rock learning

1025
01:08:43,105 --> 01:08:46,510
what? Okay, rock matrix of rock.

1026
01:08:46,555 --> 01:08:50,635
Vector is a vector vector or matrix

1027
01:08:50,680 --> 01:08:54,587
of matrix. So imagine that.

1028
01:09:02,900 --> 01:09:05,840
Sorry, just zoom, just glitch just

1029
01:09:05,857 --> 01:09:08,312
repeat the last piece. Okay,

1030
01:09:09,950 --> 01:09:14,030
well, broke matrix Dean that the

1031
01:09:14,152 --> 01:09:17,400
element of matrix is a matrix.

1032
01:09:18,125 --> 01:09:21,515
So let's say two by two matrix like

1033
01:09:21,682 --> 01:09:25,875
matrix in the ear pointing.

1034
01:09:26,375 --> 01:09:30,430
So this here W one hat

1035
01:09:30,565 --> 01:09:34,175
is a matrix and W zero

1036
01:09:34,312 --> 01:09:39,100
hat is another matrix. And combining

1037
01:09:39,175 --> 01:09:42,935
four matrices, we define a single

1038
01:09:43,117 --> 01:09:47,537
block matrix. All right,

1039
01:09:48,125 --> 01:09:51,205
thank you. So Dave

1040
01:09:51,265 --> 01:09:55,175
then asks the hosts of Machine Learning

1041
01:09:55,237 --> 01:09:58,690
Street Talk Number 67 with Karl

1042
01:09:58,720 --> 01:10:02,095
Friston. Another podcast, Karl Friston

1043
01:10:02,110 --> 01:10:05,875
has spoken. Pressed Karl Friston

1044
01:10:05,950 --> 01:10:08,765
on why is it so important that most of

1045
01:10:08,782 --> 01:10:11,320
the values in a generative model matrix

1046
01:10:11,410 --> 01:10:14,087
assume values of exactly zero?

1047
01:10:17,037 --> 01:10:18,842
Why is it important that generative

1048
01:10:18,902 --> 01:10:21,262
model matrices are sparse?

1049
01:10:22,962 --> 01:10:23,725
Why?

1050
01:10:30,687 --> 01:10:33,492
I'm not Bull sure. I think there is some

1051
01:10:33,540 --> 01:10:37,450
context before that point.

1052
01:10:38,037 --> 01:10:40,482
I think on that particular situation,

1053
01:10:40,635 --> 01:10:44,787
then, yeah, as you say, the many

1054
01:10:44,925 --> 01:10:47,712
elements in the matrix or gentle model

1055
01:10:47,775 --> 01:10:50,777
should be zero, but I'm not sure if it's

1056
01:10:50,807 --> 01:10:53,200
a general statement at all.

1057
01:10:54,312 --> 01:10:57,037
What do you think about compressed

1058
01:11:01,025 --> 01:11:04,575
analyses on sparse matrices?

1059
01:11:08,825 --> 01:11:21,455
Is that a useful technique or

1060
01:11:21,502 --> 01:11:22,350
direction?

1061
01:11:25,850 --> 01:11:29,087
You can use that knowledge to

1062
01:11:29,975 --> 01:11:33,665
construct model, so you

1063
01:11:33,682 --> 01:11:37,505
can use that knowledge to make more

1064
01:11:37,702 --> 01:11:40,505
accurate inference. So in that sense,

1065
01:11:40,627 --> 01:11:43,165
generally speaking, such assumptions

1066
01:11:43,270 --> 01:11:45,225
should be useful.

1067
01:11:46,775 --> 01:11:49,805
For example, yeah, as you said,

1068
01:11:49,927 --> 01:11:53,710
it would be possible to use some sparse

1069
01:11:53,830 --> 01:11:58,225
prior to restrict

1070
01:11:58,300 --> 01:12:03,455
the value of parameter. Like, it is in

1071
01:12:03,502 --> 01:12:07,535
principle same as assuming some L

1072
01:12:07,567 --> 01:12:11,435
one norm to design the

1073
01:12:11,542 --> 01:12:14,710
distribution. To design the prior

1074
01:12:14,755 --> 01:12:18,110
distribution, which is mass Dutch

1075
01:12:18,217 --> 01:12:21,175
speaking. Exactly same as considering

1076
01:12:21,250 --> 01:12:23,625
Lasso regression.

1077
01:12:25,550 --> 01:12:30,700
Yes. So we've

1078
01:12:30,775 --> 01:12:33,965
explored a little bit how from a

1079
01:12:33,982 --> 01:12:36,860
canonical neural network to a particular

1080
01:12:36,967 --> 01:12:40,355
form of a POMDP, gives us

1081
01:12:40,477 --> 01:12:44,245
some semantics and interpretability

1082
01:12:44,410 --> 01:12:47,210
around the dynamics and plasticity of

1083
01:12:47,242 --> 01:12:51,150
the neural network. What do we gain

1084
01:12:51,950 --> 01:12:55,255
by taking a stated POMDP

1085
01:12:55,315 --> 01:12:59,285
generative model and deriving an

1086
01:12:59,317 --> 01:13:01,050
analogous neural network?

1087
01:13:03,725 --> 01:13:07,360
Do we gain access to efficient

1088
01:13:07,405 --> 01:13:09,775
computation, new software packages,

1089
01:13:09,925 --> 01:13:11,625
different applications?

1090
01:13:15,462 --> 01:13:19,592
Well, if one use Home DP

1091
01:13:19,727 --> 01:13:23,922
and one's goal is

1092
01:13:24,105 --> 01:13:27,775
so design an efficient basic model,

1093
01:13:28,287 --> 01:13:31,922
then I think your Home DP expression

1094
01:13:31,967 --> 01:13:36,852
is sufficient. So you

1095
01:13:36,870 --> 01:13:41,522
don't need to consider neural network

1096
01:13:41,717 --> 01:13:42,787
architecture,

1097
01:13:45,762 --> 01:13:50,512
probably because Homedippy

1098
01:13:51,012 --> 01:13:54,372
architecture and Bayesian correlation is

1099
01:13:54,405 --> 01:13:57,657
designed to achieve some sort of

1100
01:13:57,810 --> 01:14:01,097
mathematically optimal inference

1101
01:14:01,142 --> 01:14:04,675
and decision making. Right? So it itself

1102
01:14:05,037 --> 01:14:08,837
is optimal scheme.

1103
01:14:08,987 --> 01:14:13,167
But if one need to

1104
01:14:13,215 --> 01:14:18,437
consider a link between Bayesian

1105
01:14:18,587 --> 01:14:22,007
inference and biological substrates,

1106
01:14:22,097 --> 01:14:24,637
then this mapping is crucial.

1107
01:14:28,962 --> 01:14:32,967
Simply speaking, we consider that

1108
01:14:33,015 --> 01:14:35,597
we assume that a brain perform Beijing

1109
01:14:35,642 --> 01:14:37,977
inference, but its substrate is still

1110
01:14:38,070 --> 01:14:42,327
unclear. So we need to link the

1111
01:14:42,420 --> 01:14:45,812
Bayesian quantity to biological

1112
01:14:45,887 --> 01:14:48,642
quantities. So this mapping, this

1113
01:14:48,690 --> 01:14:51,942
equivalent, helps us to its

1114
01:14:51,990 --> 01:14:55,617
translation. Right? So when you

1115
01:14:55,665 --> 01:15:00,207
start from on the model, then this

1116
01:15:00,285 --> 01:15:05,232
translation facilitates the process

1117
01:15:05,310 --> 01:15:08,912
of finding its neuronal substrate.

1118
01:15:09,062 --> 01:15:13,467
So once you translate that to a basic to

1119
01:15:13,665 --> 01:15:19,692
neural network quantities, then it

1120
01:15:19,740 --> 01:15:23,402
facilitates the experimental

1121
01:15:23,582 --> 01:15:27,147
validation application to

1122
01:15:27,180 --> 01:15:31,467
reality. So if its

1123
01:15:31,590 --> 01:15:36,332
modeling is apt

1124
01:15:36,422 --> 01:15:40,802
for a particular neural network neural

1125
01:15:40,982 --> 01:15:45,357
circuit architecture, then it

1126
01:15:45,435 --> 01:15:48,252
should provide some prediction about the

1127
01:15:48,270 --> 01:15:52,737
architecture or dynamics of the

1128
01:15:52,875 --> 01:15:54,700
empirical data.

1129
01:15:55,212 --> 01:15:58,742
Right? So first we start from Bayesian

1130
01:15:58,802 --> 01:16:01,992
model, which is not necessary to be

1131
01:16:02,040 --> 01:16:05,427
equal to empirical data. So there

1132
01:16:05,445 --> 01:16:08,967
is some mapping, but it's mapping is

1133
01:16:09,015 --> 01:16:12,025
not straightforward. We may have

1134
01:16:12,762 --> 01:16:15,267
multiple mappings, but once you

1135
01:16:15,315 --> 01:16:18,747
translate Bayesian model to a particular

1136
01:16:18,855 --> 01:16:21,700
neural network architectures then

1137
01:16:23,037 --> 01:16:25,497
mapping or relationship between

1138
01:16:25,605 --> 01:16:28,347
applicable data to such a particular

1139
01:16:28,455 --> 01:16:31,537
neural network model is straightforward.

1140
01:16:32,187 --> 01:16:36,777
So it helps us to apply base

1141
01:16:36,870 --> 01:16:40,122
to an explanation of MP card

1142
01:16:40,155 --> 01:16:40,750
data.

1143
01:16:46,737 --> 01:16:50,027
So is it fair to say that neural

1144
01:16:50,057 --> 01:16:53,867
networks have found wide recent

1145
01:16:53,927 --> 01:16:58,247
application because they facilitate

1146
01:16:58,367 --> 01:17:01,197
statistical learning in cases where the

1147
01:17:01,230 --> 01:17:04,727
inference problem has not been a priori

1148
01:17:04,832 --> 01:17:08,307
well specified? One can just have a

1149
01:17:08,310 --> 01:17:11,922
folder of images and a list of

1150
01:17:11,955 --> 01:17:15,747
labels and just say, here's the

1151
01:17:15,780 --> 01:17:19,937
data. Run it through this architecture

1152
01:17:20,012 --> 01:17:23,737
or this architecture Explorer.

1153
01:17:25,137 --> 01:17:29,427
And so with this concordance we

1154
01:17:29,445 --> 01:17:31,947
gain new interpretation into those

1155
01:17:32,055 --> 01:17:35,847
settings that kind of arose from ill

1156
01:17:35,880 --> 01:17:38,967
specified inference problems. And then

1157
01:17:39,015 --> 01:17:41,592
on the other hand, for problems that we

1158
01:17:41,640 --> 01:17:45,327
already have well specified in terms of

1159
01:17:45,345 --> 01:17:48,747
a POMDP generative model of a particular

1160
01:17:48,855 --> 01:17:52,952
form, we gain

1161
01:17:53,057 --> 01:17:57,250
the connection to actually implement it

1162
01:17:57,987 --> 01:18:01,062
with empirical data and

1163
01:18:01,125 --> 01:18:03,767
bring it into relevant industrial

1164
01:18:03,827 --> 01:18:04,687
settings.

1165
01:18:07,987 --> 01:18:12,217
What systems or phenomena are

1166
01:18:12,265 --> 01:18:19,177
promising to continue research on the

1167
01:18:19,195 --> 01:18:22,777
image example obviously is a simple

1168
01:18:22,945 --> 01:18:26,227
case, but are you continuing research

1169
01:18:26,320 --> 01:18:29,167
into more advanced computational agents?

1170
01:18:29,365 --> 01:18:31,937
Robotic animal.

1171
01:18:36,200 --> 01:18:42,905
Oh, hello can

1172
01:18:42,952 --> 01:18:47,625
design a more sophisticated

1173
01:18:48,500 --> 01:18:51,755
agent which performs some difficult

1174
01:18:51,877 --> 01:18:56,005
tasks based on canonical

1175
01:18:56,065 --> 01:18:59,945
network neural network but

1176
01:19:00,097 --> 01:19:02,570
there is some limitation, clear

1177
01:19:02,647 --> 01:19:06,505
limitation on that direction.

1178
01:19:06,565 --> 01:19:10,255
Right? So yeah, I should emphasize

1179
01:19:10,315 --> 01:19:13,255
that across the canonical neural network

1180
01:19:13,315 --> 01:19:15,675
which correspond to a particular home DP

1181
01:19:16,250 --> 01:19:20,660
is much smaller than general

1182
01:19:20,842 --> 01:19:24,560
home DP framework. So there

1183
01:19:24,592 --> 01:19:27,005
are some limitations, a least of

1184
01:19:27,052 --> 01:19:30,610
limitations. So if one's goal

1185
01:19:30,655 --> 01:19:34,270
is designed and sophisticated DAGs

1186
01:19:34,435 --> 01:19:37,825
to perform some task or control robot,

1187
01:19:37,900 --> 01:19:41,660
then one direction is just

1188
01:19:41,767 --> 01:19:45,137
forget such limitations and

1189
01:19:45,725 --> 01:19:49,280
take the mathematical optimality right

1190
01:19:49,402 --> 01:19:51,985
and another direction is barrelscale

1191
01:19:52,030 --> 01:19:56,105
probability. So if one wants to image

1192
01:19:56,152 --> 01:19:58,460
some agent which is barely possible,

1193
01:19:58,567 --> 01:20:01,495
then this correspondence is crucial

1194
01:20:01,585 --> 01:20:07,180
because it tells us biological

1195
01:20:07,240 --> 01:20:11,960
limitations through the

1196
01:20:11,992 --> 01:20:14,765
existence, no existence of such a

1197
01:20:14,782 --> 01:20:17,450
mapping between POMDP and particular

1198
01:20:17,587 --> 01:20:20,537
neural network architectures. So,

1199
01:20:21,800 --> 01:20:26,462
yeah. So it would be useful to consider

1200
01:20:27,275 --> 01:20:31,050
a vertical substrate to achieve

1201
01:20:32,150 --> 01:20:33,975
exafferent task.

1202
01:20:42,162 --> 01:20:46,675
And that task would be related to high

1203
01:20:48,087 --> 01:20:51,362
dimension image processing.

1204
01:20:51,512 --> 01:20:54,602
Image recognition or sound recognition,

1205
01:20:54,782 --> 01:20:58,667
such as multimodality, can be inbox

1206
01:20:58,802 --> 01:21:02,792
or decision. Can be higher dimensional

1207
01:21:02,927 --> 01:21:05,777
like in the mainstays,

1208
01:21:05,807 --> 01:21:08,927
we just consider the four directional

1209
01:21:08,957 --> 01:21:12,717
movement, but it can be extended to

1210
01:21:12,840 --> 01:21:15,032
higher dimensionality,

1211
01:21:15,197 --> 01:21:18,317
like arm movement, body movement,

1212
01:21:18,377 --> 01:21:24,372
so on, so on. So in principle it

1213
01:21:24,405 --> 01:21:27,412
can be extended in that direction.

1214
01:21:30,312 --> 01:21:33,625
Which directions or questions

1215
01:21:34,587 --> 01:21:38,437
are you excited about? Or what areas

1216
01:21:38,787 --> 01:21:42,002
of studying the basis

1217
01:21:42,032 --> 01:21:44,387
of biological and computational

1218
01:21:44,462 --> 01:21:47,967
intelligence are relevant? Yes.

1219
01:21:48,165 --> 01:21:52,300
So in terms of

1220
01:21:53,712 --> 01:21:56,547
the importance of canonical network, as

1221
01:21:56,580 --> 01:21:57,325
you said,

1222
01:21:59,637 --> 01:22:02,447
virtue is a biological probability.

1223
01:22:02,567 --> 01:22:06,325
So it would be nice that if

1224
01:22:07,362 --> 01:22:11,307
we model some task which is

1225
01:22:11,460 --> 01:22:15,550
conducted by learning and

1226
01:22:16,212 --> 01:22:20,147
one already recorded some neural

1227
01:22:20,342 --> 01:22:24,192
activity, then we

1228
01:22:24,390 --> 01:22:27,762
design a task which is

1229
01:22:27,825 --> 01:22:31,152
exactly same as the task which is done

1230
01:22:31,320 --> 01:22:35,942
by the animal and then compare

1231
01:22:36,077 --> 01:22:39,642
the simulated agent and Mpcar data

1232
01:22:39,765 --> 01:22:43,572
to discuss about the similarity or

1233
01:22:43,605 --> 01:22:46,737
difference between the

1234
01:22:46,800 --> 01:22:50,012
simulated agent and the animals.

1235
01:22:50,162 --> 01:22:52,952
That would be very interesting direction

1236
01:22:53,057 --> 01:22:53,950
of research.

1237
01:22:56,862 --> 01:22:59,050
Yeah, and if there could be some

1238
01:22:59,487 --> 01:23:04,302
unexpected prediction or explanation in

1239
01:23:04,320 --> 01:23:06,312
the computational agent, that would

1240
01:23:06,375 --> 01:23:10,450
bolster the relationship. And then

1241
01:23:11,337 --> 01:23:14,577
one other aspect is it

1242
01:23:14,595 --> 01:23:19,497
would help with the reproducibility and

1243
01:23:19,530 --> 01:23:24,122
the documentation around behavior

1244
01:23:24,167 --> 01:23:27,302
studies if the computational

1245
01:23:27,407 --> 01:23:30,987
agent were preregistered and someone

1246
01:23:31,050 --> 01:23:32,942
said we've already done the statistical

1247
01:23:33,002 --> 01:23:35,802
power analysis and we've already

1248
01:23:35,895 --> 01:23:39,342
explored with parameter sweeps how

1249
01:23:39,390 --> 01:23:43,000
many observations we need to make of

1250
01:23:43,362 --> 01:23:46,092
the two armed bandit. How many

1251
01:23:46,140 --> 01:23:48,467
observations of the three armed bandit

1252
01:23:48,602 --> 01:23:52,377
should we do? Three mice 100 times or

1253
01:23:52,395 --> 01:23:56,050
100 mice three times? Those are

1254
01:23:56,787 --> 01:24:00,257
the total substance of designing

1255
01:24:00,347 --> 01:24:03,612
research programs. And so

1256
01:24:03,675 --> 01:24:06,762
having a formal representation of

1257
01:24:06,825 --> 01:24:09,412
behavioral tasks that are being studied

1258
01:24:09,837 --> 01:24:12,017
will help us design behavior

1259
01:24:12,077 --> 01:24:14,297
observations and experiments that aren't

1260
01:24:14,342 --> 01:24:15,787
simply ad hoc.

1261
01:24:20,037 --> 01:24:23,067
All right, that's an interesting

1262
01:24:23,190 --> 01:24:23,875
application.

1263
01:24:26,862 --> 01:24:29,997
This framework helps to design the

1264
01:24:30,030 --> 01:24:32,800
experimental setup itself.

1265
01:24:33,762 --> 01:24:37,947
And what we often consider is

1266
01:24:37,980 --> 01:24:42,017
the prediction ability of these modern

1267
01:24:42,077 --> 01:24:47,397
canonical neural networks to predict the

1268
01:24:47,505 --> 01:24:51,717
Jelle phoneization or dynamics of

1269
01:24:51,840 --> 01:24:55,682
the VR neural network in the animal

1270
01:24:55,847 --> 01:24:59,802
during the learning process. So in

1271
01:24:59,820 --> 01:25:03,312
place for it is possible to predict the

1272
01:25:03,375 --> 01:25:06,475
behavior after learning based only on

1273
01:25:07,362 --> 01:25:10,787
data in the initial

1274
01:25:10,862 --> 01:25:14,102
stage because once we obtain

1275
01:25:14,207 --> 01:25:16,812
some empirical data, then we can fit

1276
01:25:16,875 --> 01:25:19,647
that data to design a particular

1277
01:25:19,755 --> 01:25:22,877
canonical network. And canonical neural

1278
01:25:22,907 --> 01:25:27,347
network makes some correlation

1279
01:25:27,467 --> 01:25:30,057
through a minimization cost function

1280
01:25:30,135 --> 01:25:33,242
which is exactly same as the Bayesian

1281
01:25:33,302 --> 01:25:36,102
belief updating under a particular

1282
01:25:36,270 --> 01:25:39,822
guarantee model. So which means that its

1283
01:25:39,855 --> 01:25:43,862
dynamics goes through the shortest path

1284
01:25:44,012 --> 01:25:46,467
on the free energy landscape which means

1285
01:25:46,515 --> 01:25:51,227
that we can make some quantitative

1286
01:25:51,332 --> 01:25:54,377
prediction about the sinatic trajectory

1287
01:25:54,482 --> 01:25:58,347
or neural activity or any

1288
01:25:58,455 --> 01:26:02,612
kind of parameters. So we demonstrated

1289
01:26:02,687 --> 01:26:06,752
that using in virtual neural network

1290
01:26:06,857 --> 01:26:11,267
and uploaded some footprint recently.

1291
01:26:11,402 --> 01:26:15,372
So at the stage, at least at

1292
01:26:15,405 --> 01:26:19,037
the level in vitro network,

1293
01:26:19,112 --> 01:26:22,647
which is much simpler than VR brain, we

1294
01:26:22,755 --> 01:26:27,112
could predict the self organization

1295
01:26:27,537 --> 01:26:31,857
of in virtual network using

1296
01:26:32,010 --> 01:26:36,402
this canonical network architecture and

1297
01:26:36,495 --> 01:26:39,597
this support the probability of

1298
01:26:39,780 --> 01:26:44,412
free energy principle because this

1299
01:26:44,475 --> 01:26:46,722
canonical network predict the

1300
01:26:46,755 --> 01:26:50,447
communication through the variational

1301
01:26:50,492 --> 01:26:53,772
free energy minimization and its

1302
01:26:53,880 --> 01:26:58,287
solution. Its results have a very

1303
01:26:58,350 --> 01:27:01,862
tight correlation between correlation

1304
01:27:01,937 --> 01:27:04,537
to Archer synchronization.

1305
01:27:07,962 --> 01:27:10,787
That's a very interesting experiment.

1306
01:27:10,862 --> 01:27:14,792
So what animal

1307
01:27:14,852 --> 01:27:17,442
were the neurons from and what was

1308
01:27:17,490 --> 01:27:20,367
measured about these neurons? Yes,

1309
01:27:20,490 --> 01:27:25,302
so that in vitro network is

1310
01:27:25,470 --> 01:27:29,362
obtained from blood embryo,

1311
01:27:30,687 --> 01:27:34,227
we use cortical cells to

1312
01:27:34,320 --> 01:27:38,472
make that individual network and

1313
01:27:38,505 --> 01:27:41,672
task is sort of causal inference

1314
01:27:41,717 --> 01:27:45,117
task which can be designed in the form

1315
01:27:45,165 --> 01:27:48,597
of OMDP. So imagine that

1316
01:27:48,705 --> 01:27:52,912
we usually simulate agents that receive

1317
01:27:53,337 --> 01:27:56,687
signals generated by OMDP generative

1318
01:27:56,762 --> 01:27:59,957
process and process and Beijing task.

1319
01:28:00,122 --> 01:28:03,897
So we just replaced that Bayesian agent

1320
01:28:04,005 --> 01:28:07,847
to neural network.

1321
01:28:07,967 --> 01:28:11,567
So we stimulate neuron with some signal

1322
01:28:11,702 --> 01:28:15,002
which is made by some hidden sources

1323
01:28:15,107 --> 01:28:18,567
through like a human mapping and

1324
01:28:18,615 --> 01:28:21,522
question is whether in viral network can

1325
01:28:21,630 --> 01:28:25,512
info the hidden states through some

1326
01:28:25,575 --> 01:28:29,125
communication and they can they could

1327
01:28:29,637 --> 01:28:31,537
Ines the hidden culture.

1328
01:28:35,112 --> 01:28:38,517
What does it look like functionally when

1329
01:28:38,715 --> 01:28:41,892
the neural network has succeeded at

1330
01:28:41,940 --> 01:28:44,832
inferring the hidden causes? Yeah,

1331
01:28:44,985 --> 01:28:47,547
the direct conversation is done by the

1332
01:28:47,655 --> 01:28:50,997
response number response spikes to a

1333
01:28:51,030 --> 01:28:55,397
particular pattern sensory input.

1334
01:28:55,517 --> 01:28:58,407
So again, we can see a clear

1335
01:28:58,560 --> 01:29:02,275
correspondence between activity level

1336
01:29:02,787 --> 01:29:05,502
and posterior belief about hidden state.

1337
01:29:05,595 --> 01:29:09,647
So here we see re a book response

1338
01:29:09,692 --> 01:29:12,782
to an electropaste memory.

1339
01:29:12,872 --> 01:29:16,872
We see the response from ten

1340
01:29:16,980 --> 01:29:20,127
to 13 milliseconds after each

1341
01:29:20,220 --> 01:29:23,907
stimulation and we compute the number of

1342
01:29:24,060 --> 01:29:28,332
spikes and that spikes changes

1343
01:29:28,485 --> 01:29:31,827
their preference in

1344
01:29:31,845 --> 01:29:36,237
the sense that some neurons learn

1345
01:29:36,300 --> 01:29:39,612
to preferentially respond to force one

1346
01:29:39,675 --> 01:29:43,675
but not source two. So which is not

1347
01:29:44,862 --> 01:29:48,492
a response to input itself, but it

1348
01:29:48,540 --> 01:29:51,972
looks like a response to particular

1349
01:29:52,080 --> 01:29:55,687
source. So it is inference

1350
01:29:56,487 --> 01:30:00,132
which is empirical evidence that

1351
01:30:00,285 --> 01:30:02,742
neural network actually perform some

1352
01:30:02,790 --> 01:30:06,252
sort of causal inference in

1353
01:30:06,270 --> 01:30:08,882
a manner consistent with traditional

1354
01:30:08,972 --> 01:30:11,822
Bayesian inference. And then we compute

1355
01:30:11,867 --> 01:30:15,257
another quantity in Bayesian inference

1356
01:30:15,422 --> 01:30:18,162
in the real vertical data.

1357
01:30:18,300 --> 01:30:22,217
We show that firing

1358
01:30:22,277 --> 01:30:25,782
special factor is consistent with

1359
01:30:25,860 --> 01:30:28,497
the prior belief about hidden states and

1360
01:30:28,530 --> 01:30:31,917
we also compute the synaptic rate

1361
01:30:32,040 --> 01:30:34,847
statistically through some connection

1362
01:30:34,892 --> 01:30:37,900
strength estimation method and show that

1363
01:30:40,062 --> 01:30:44,582
the estimated

1364
01:30:44,672 --> 01:30:49,075
synaptic strength is consistent with

1365
01:30:50,187 --> 01:30:53,642
something encoding posterior

1366
01:30:53,702 --> 01:30:57,250
belief about parameters, as expected by

1367
01:30:57,987 --> 01:30:58,987
the theory.

1368
01:31:00,612 --> 01:31:04,537
Well, we looked at table two earlier

1369
01:31:04,887 --> 01:31:08,187
and this is almost like the next step

1370
01:31:08,250 --> 01:31:12,537
after the theoretical concordance is

1371
01:31:12,675 --> 01:31:18,102
all right, well, let's measure the

1372
01:31:18,120 --> 01:31:20,472
release of a neurotransmitter or the

1373
01:31:20,505 --> 01:31:22,872
empirical synaptic strength or the

1374
01:31:22,905 --> 01:31:24,717
firing threshold or all these different

1375
01:31:24,765 --> 01:31:28,237
features in different empirical systems.

1376
01:31:28,962 --> 01:31:33,002
So what experimental

1377
01:31:33,032 --> 01:31:36,250
systems does your group work in?

1378
01:31:42,012 --> 01:31:46,675
That in virtual system was made

1379
01:31:47,262 --> 01:31:50,967
when I was a PhD student. So that

1380
01:31:51,015 --> 01:31:54,867
is experiment we done in

1381
01:31:54,915 --> 01:31:58,677
my previous route and now I

1382
01:31:58,695 --> 01:32:02,500
COVID to the Bijan Institute and

1383
01:32:03,687 --> 01:32:06,782
I'm a Princepal investigator

1384
01:32:06,872 --> 01:32:10,692
of Celery unit. So now actually we

1385
01:32:10,740 --> 01:32:14,492
don't use any experimental setup,

1386
01:32:14,552 --> 01:32:18,717
so any experimental bidding is

1387
01:32:18,765 --> 01:32:22,002
down with some collaboration. So

1388
01:32:22,170 --> 01:32:25,917
although I cannot say detail about that

1389
01:32:25,965 --> 01:32:29,217
exploration. But yeah, we learn some

1390
01:32:29,415 --> 01:32:32,282
collaborating work about the implication

1391
01:32:32,372 --> 01:32:36,362
of salary using various

1392
01:32:36,437 --> 01:32:40,047
animals. Yeah, so we hope we

1393
01:32:40,080 --> 01:32:42,497
can show some interesting results

1394
01:32:42,542 --> 01:32:45,700
following results using animal data.

1395
01:32:49,662 --> 01:32:51,775
Very interesting.

1396
01:32:52,887 --> 01:32:57,487
Yeah, well, it speaks a lot to the stage

1397
01:32:59,412 --> 01:33:03,012
that our field is in in certain

1398
01:33:03,075 --> 01:33:06,677
ways where we've

1399
01:33:06,707 --> 01:33:10,377
seen a lot of graphics that

1400
01:33:10,395 --> 01:33:13,497
are suggestive. This paper

1401
01:33:13,680 --> 01:33:16,767
and the building on the previous 2020

1402
01:33:16,815 --> 01:33:20,212
paper made a suggestive

1403
01:33:21,087 --> 01:33:24,617
possibility much closer

1404
01:33:24,677 --> 01:33:28,262
to an analytically

1405
01:33:28,412 --> 01:33:32,112
demonstrated translation and then

1406
01:33:32,175 --> 01:33:34,317
took the next step incrementally into

1407
01:33:34,365 --> 01:33:38,007
the in silico agent. And so it's only

1408
01:33:38,085 --> 01:33:41,397
natural to then explore different

1409
01:33:41,505 --> 01:33:45,175
embodied systems as well.

1410
01:33:50,862 --> 01:33:53,622
Are there any other sections that you

1411
01:33:53,655 --> 01:33:57,492
wanted to like, look at or highlight or

1412
01:33:57,540 --> 01:34:00,972
any other topics about the paper or

1413
01:34:01,005 --> 01:34:04,782
adjacencies or active inference that

1414
01:34:04,935 --> 01:34:07,300
you think are interesting to go into?

1415
01:34:10,212 --> 01:34:13,525
Yeah, okay,

1416
01:34:14,562 --> 01:34:16,797
I would like to mention about some

1417
01:34:16,830 --> 01:34:19,872
implication of these papers, which is

1418
01:34:19,905 --> 01:34:23,417
not the director discussing the papers.

1419
01:34:23,552 --> 01:34:25,750
So for example,

1420
01:34:26,412 --> 01:34:29,577
well, we focus on a

1421
01:34:29,595 --> 01:34:31,675
discrete state space model.

1422
01:34:32,037 --> 01:34:35,192
So we avoid

1423
01:34:35,252 --> 01:34:39,062
to assume some subscript that encodes

1424
01:34:39,212 --> 01:34:42,642
the coherence of

1425
01:34:42,690 --> 01:34:45,122
the distribution. So once you assume

1426
01:34:45,167 --> 01:34:48,092
homedp then it is categorical

1427
01:34:48,152 --> 01:34:50,382
distinctions. So it is different from

1428
01:34:50,460 --> 01:34:52,987
assuming Gaussian distribution

1429
01:34:54,087 --> 01:34:57,942
characterized by me and variants. So the

1430
01:34:57,990 --> 01:35:02,097
neural substrate of variance is

1431
01:35:02,205 --> 01:35:06,072
still unclear and we now try to

1432
01:35:06,180 --> 01:35:09,177
figure out that. So this is one

1433
01:35:09,345 --> 01:35:12,687
direction of limitation and another

1434
01:35:12,750 --> 01:35:16,825
implication is that thanks to a simple

1435
01:35:17,187 --> 01:35:21,777
ODP structure in this paper,

1436
01:35:21,945 --> 01:35:25,992
we don't care about the

1437
01:35:26,190 --> 01:35:30,767
hierarchical optimization.

1438
01:35:30,902 --> 01:35:35,127
But generally it is crucial to

1439
01:35:35,220 --> 01:35:39,317
update parameters through hierarchical

1440
01:35:39,452 --> 01:35:41,277
optimization through some

1441
01:35:41,445 --> 01:35:44,437
backpropagation like computation.

1442
01:35:44,787 --> 01:35:47,667
Although it is unclear whether back

1443
01:35:47,715 --> 01:35:51,162
propagation itself occurred in the real

1444
01:35:51,225 --> 01:35:53,650
brain. But we still have some

1445
01:35:54,312 --> 01:35:58,000
alternative that achieve such

1446
01:35:58,737 --> 01:36:02,402
optimization and it's neural substrate

1447
01:36:02,432 --> 01:36:05,822
still unclear and this paper doesn't

1448
01:36:05,942 --> 01:36:08,062
address that direction.

1449
01:36:25,650 --> 01:36:29,805
Another area I'm wondering about is like

1450
01:36:30,002 --> 01:36:34,485
where in neural structures is

1451
01:36:34,517 --> 01:36:38,310
the learning reflected? Where is

1452
01:36:38,342 --> 01:36:40,790
the function and learning reflected?

1453
01:36:40,970 --> 01:36:44,085
Well, sometimes it has to do with not

1454
01:36:44,117 --> 01:36:46,825
just structural Tweaking,

1455
01:36:47,325 --> 01:36:49,955
but the presence or absence of synapses.

1456
01:36:50,015 --> 01:36:53,480
So obviously this model does not expand

1457
01:36:53,540 --> 01:36:56,720
into synaptogenesis synaptic pruning,

1458
01:36:56,885 --> 01:36:59,350
let alone neurogenesis and neuro

1459
01:37:00,075 --> 01:37:02,415
allostasis which we mentioned in the

1460
01:37:02,432 --> 01:37:06,155
previous discussion. But understanding

1461
01:37:06,215 --> 01:37:09,455
how these larger scale structural

1462
01:37:09,515 --> 01:37:12,765
changes which are certainly important in

1463
01:37:12,782 --> 01:37:16,725
biological systems become reflected in

1464
01:37:16,787 --> 01:37:18,840
artificial neural networks and then how

1465
01:37:18,857 --> 01:37:20,565
that translates all the way back to

1466
01:37:20,582 --> 01:37:23,490
P-O-M. DP and then whether we could go

1467
01:37:23,507 --> 01:37:26,425
the other way. What kinds of POMDP

1468
01:37:26,925 --> 01:37:30,800
structures in their neural network

1469
01:37:30,875 --> 01:37:34,640
realization would have structural

1470
01:37:34,745 --> 01:37:36,870
modification. Like you COVID imagine a

1471
01:37:36,872 --> 01:37:40,050
POMDP that does

1472
01:37:40,112 --> 01:37:42,230
structure learning but the neural

1473
01:37:42,290 --> 01:37:44,535
network doesn't have structural change.

1474
01:37:44,717 --> 01:37:47,010
Or there's A-P-O-M DP that doesn't do

1475
01:37:47,042 --> 01:37:50,285
structural learning but it's manifested

1476
01:37:50,330 --> 01:37:52,365
by a neural network that does have a

1477
01:37:52,382 --> 01:37:56,060
structural change element. So structure

1478
01:37:56,105 --> 01:37:58,140
is doing something very different in

1479
01:37:58,157 --> 01:38:00,690
these two different categories of model.

1480
01:38:00,857 --> 01:38:03,990
And then also even within

1481
01:38:04,157 --> 01:38:08,535
neural firing, which is

1482
01:38:08,567 --> 01:38:10,065
different amongst different species and

1483
01:38:10,082 --> 01:38:14,450
so on, there's different aspects

1484
01:38:14,525 --> 01:38:17,910
of what that firing is that would have

1485
01:38:17,942 --> 01:38:19,740
different implications for the actual

1486
01:38:19,832 --> 01:38:23,085
biological substrate of cognition niche.

1487
01:38:23,267 --> 01:38:27,485
The simple connection is firing

1488
01:38:27,530 --> 01:38:31,160
rate to posterior belief.

1489
01:38:31,355 --> 01:38:33,690
Average firing rate, no change in

1490
01:38:33,707 --> 01:38:36,165
posterior. Reduce the firing rate if the

1491
01:38:36,182 --> 01:38:37,830
posterior should be going down.

1492
01:38:37,952 --> 01:38:40,020
Increase it if it should be going up.

1493
01:38:40,172 --> 01:38:41,820
Or maybe there are neurons that have a

1494
01:38:41,822 --> 01:38:44,010
flipped valence but the same type of

1495
01:38:44,042 --> 01:38:47,660
relationship, but there's other firing

1496
01:38:47,780 --> 01:38:51,050
patterns like spike

1497
01:38:51,125 --> 01:38:53,200
time dependent plasticity

1498
01:38:54,525 --> 01:38:56,885
synchronization amongst different brain

1499
01:38:56,930 --> 01:39:00,345
regions. There's a lot of things that

1500
01:39:00,422 --> 01:39:04,000
don't change the rate overall.

1501
01:39:05,250 --> 01:39:07,715
That again from the biological systems

1502
01:39:07,745 --> 01:39:09,660
we know that those phenomena and

1503
01:39:09,692 --> 01:39:11,580
mechanism are important for different

1504
01:39:11,627 --> 01:39:13,150
cognitive processes.

1505
01:39:17,400 --> 01:39:21,150
So there will be many many years

1506
01:39:21,287 --> 01:39:23,337
of a fruitful relationship.

1507
01:39:25,875 --> 01:39:30,605
I'm going to bring in this picture

1508
01:39:30,665 --> 01:39:33,945
that Alexandra had taken.

1509
01:39:34,022 --> 01:39:38,010
Maybe we need a third panel in

1510
01:39:38,042 --> 01:39:41,760
figure one because

1511
01:39:41,942 --> 01:39:44,787
these three systems moving between them

1512
01:39:47,775 --> 01:39:51,240
is going to be the substance of the

1513
01:39:51,257 --> 01:39:54,510
field for a long time and there may be

1514
01:39:54,542 --> 01:39:58,460
other edges to build. But understanding

1515
01:39:58,505 --> 01:40:00,925
how artificial neural networks

1516
01:40:03,075 --> 01:40:06,875
intermediate between the empirical

1517
01:40:06,950 --> 01:40:09,615
measurements and manipulations that we

1518
01:40:09,632 --> 01:40:13,887
can make of real neural systems and

1519
01:40:15,150 --> 01:40:19,680
the interpretability and

1520
01:40:19,727 --> 01:40:25,815
factorizability of POMDPs it

1521
01:40:25,907 --> 01:40:28,590
might be a bridge too far to go from the

1522
01:40:28,607 --> 01:40:32,085
POMDP to the neuron. You could always

1523
01:40:32,192 --> 01:40:35,535
use this technique but it would be a

1524
01:40:35,567 --> 01:40:39,225
purely descriptive statistic type

1525
01:40:39,287 --> 01:40:43,415
approach. But it's

1526
01:40:43,445 --> 01:40:46,265
so interesting that by intermediating

1527
01:40:46,370 --> 01:40:49,900
through a formal connection

1528
01:40:51,375 --> 01:40:54,600
established in figure one I Dean in

1529
01:40:54,662 --> 01:40:57,012
equation one but also shown here,

1530
01:40:57,900 --> 01:41:03,630
then we can kind of extend the

1531
01:41:03,677 --> 01:41:06,410
chain of exploration, prediction,

1532
01:41:06,530 --> 01:41:10,155
control, design all the way

1533
01:41:10,202 --> 01:41:13,985
on through. And that just unlocks

1534
01:41:14,030 --> 01:41:15,960
like an incredible amount of

1535
01:41:15,992 --> 01:41:19,600
neuroscience that hasn't been formalized

1536
01:41:20,175 --> 01:41:23,750
mathematically and an incredible

1537
01:41:23,825 --> 01:41:27,735
amount of generative models that

1538
01:41:27,767 --> 01:41:29,655
have been specified for different

1539
01:41:29,777 --> 01:41:33,690
learning settings sometimes even by

1540
01:41:33,707 --> 01:41:36,025
analogy to biological settings.

1541
01:41:37,200 --> 01:41:40,975
But the metaphor remains just a metaphor

1542
01:41:42,150 --> 01:41:45,780
until it's possible to intermediate with

1543
01:41:45,827 --> 01:41:48,175
this type of neural network development.

1544
01:41:49,125 --> 01:41:52,737
Yeah, that's a crucial point.

1545
01:41:56,400 --> 01:42:03,080
It is easy to imagine that phenomena

1546
01:42:03,140 --> 01:42:07,000
can be modeled using very realistic

1547
01:42:07,350 --> 01:42:10,640
neural model or free model synaptic

1548
01:42:10,670 --> 01:42:11,937
model. Right.

1549
01:42:14,400 --> 01:42:18,012
We believe that it is possible and then

1550
01:42:20,250 --> 01:42:24,125
Laje model is not necessarily tractable,

1551
01:42:24,200 --> 01:42:26,550
not necessarily useful because it's too

1552
01:42:26,612 --> 01:42:29,715
much complicated to analyze something.

1553
01:42:29,807 --> 01:42:32,187
So we use some reduction, usually

1554
01:42:32,550 --> 01:42:34,005
mathematically speaking, which

1555
01:42:34,052 --> 01:42:36,935
correspond to topological transformation

1556
01:42:37,055 --> 01:42:40,150
to make the model simpler.

1557
01:42:40,800 --> 01:42:44,055
And then we need to consider the

1558
01:42:44,102 --> 01:42:47,915
translation of that simplified neural

1559
01:42:47,945 --> 01:42:49,760
network model because neural network

1560
01:42:49,805 --> 01:42:54,550
model itself is not explainable

1561
01:42:55,350 --> 01:42:58,560
which just represents some dynamics and

1562
01:42:58,592 --> 01:43:02,040
its functional meaning is not clear.

1563
01:43:02,207 --> 01:43:06,630
But thanks to the Bayesian framework we

1564
01:43:06,677 --> 01:43:10,520
have a very nice event framework

1565
01:43:10,610 --> 01:43:13,975
to least the experiment ability.

1566
01:43:14,400 --> 01:43:17,070
And this translation, this

1567
01:43:17,147 --> 01:43:20,730
correspondence helped us to link such

1568
01:43:20,777 --> 01:43:24,770
a phenomena base equation

1569
01:43:24,935 --> 01:43:28,437
modeling and functional base

1570
01:43:30,000 --> 01:43:34,212
equations. Yeah, one paper

1571
01:43:34,800 --> 01:43:38,235
from 2017 that was

1572
01:43:38,267 --> 01:43:40,545
much discussed by some could a

1573
01:43:40,547 --> 01:43:42,240
neuroscientist understand a

1574
01:43:42,257 --> 01:43:46,062
microprocessor? And this

1575
01:43:47,100 --> 01:43:50,655
group with Jonas and Courting, they had

1576
01:43:50,702 --> 01:43:54,325
a simulation of a microprocessor

1577
01:43:54,825 --> 01:43:58,610
from an earlier video game console,

1578
01:43:58,655 --> 01:44:02,405
I believe. And then using the analogy

1579
01:44:02,465 --> 01:44:04,185
of like the transistors and their

1580
01:44:04,217 --> 01:44:06,480
connections as neural firing and

1581
01:44:06,527 --> 01:44:09,255
structural connectivity they were able

1582
01:44:09,302 --> 01:44:13,130
to simulate experimental settings,

1583
01:44:13,190 --> 01:44:16,035
input and action and then make

1584
01:44:16,067 --> 01:44:20,105
measurements from every neuron including

1585
01:44:20,165 --> 01:44:22,440
doing lesions and loss of functions and

1586
01:44:22,457 --> 01:44:24,795
so on. And it turns out that a lot of

1587
01:44:24,797 --> 01:44:28,550
the techniques that are used to derive

1588
01:44:28,625 --> 01:44:31,880
scientific explanation from analogous

1589
01:44:31,940 --> 01:44:34,512
data collected from a biological system,

1590
01:44:35,550 --> 01:44:37,935
those techniques which ostensibly should

1591
01:44:37,967 --> 01:44:41,340
be isolating functional explanations in

1592
01:44:41,357 --> 01:44:44,275
fact did not isolate effective

1593
01:44:45,750 --> 01:44:49,310
exploration. You could have a deletion

1594
01:44:49,355 --> 01:44:53,720
over here that induces some statistical

1595
01:44:53,810 --> 01:44:56,430
change all the way over here and that

1596
01:44:56,477 --> 01:45:00,270
may or may not be a useful clue towards

1597
01:45:00,347 --> 01:45:03,700
the function of even subcircuits.

1598
01:45:04,575 --> 01:45:08,337
And so I think that was a wake up call

1599
01:45:09,600 --> 01:45:14,112
with respect to the interpretability of

1600
01:45:17,400 --> 01:45:21,025
simply this connection

1601
01:45:23,175 --> 01:45:25,340
between the biological and the neural

1602
01:45:25,370 --> 01:45:29,255
network. This connection

1603
01:45:29,315 --> 01:45:32,855
alone is of limited

1604
01:45:32,915 --> 01:45:36,810
applicability even when the

1605
01:45:36,842 --> 01:45:39,815
neural network model becomes so complex

1606
01:45:39,920 --> 01:45:41,960
as to recapitulate the biological

1607
01:45:42,005 --> 01:45:45,375
phenomena, you're never under

1608
01:45:45,437 --> 01:45:47,040
any guarantee that you're going to

1609
01:45:47,057 --> 01:45:49,740
recover interpretability. You may have

1610
01:45:49,757 --> 01:45:53,425
just created an atomic level simulation

1611
01:45:54,225 --> 01:45:56,915
of the phenomena, but of course, a map

1612
01:45:57,095 --> 01:45:59,660
that is the same scale as the phenomena

1613
01:45:59,705 --> 01:46:02,985
isn't a map. It's just a copy that has

1614
01:46:03,017 --> 01:46:04,450
no more interpretability.

1615
01:46:06,600 --> 01:46:10,200
And it's almost like what

1616
01:46:10,337 --> 01:46:13,860
is now extended again, as we kind of

1617
01:46:13,892 --> 01:46:16,290
just summarize this and think about how

1618
01:46:16,307 --> 01:46:20,330
we move forward, is that connection

1619
01:46:20,390 --> 01:46:24,330
can now be extended into the space

1620
01:46:24,452 --> 01:46:28,035
of interpretable causal models and the

1621
01:46:28,067 --> 01:46:29,975
generalized Bayesian graphical

1622
01:46:30,050 --> 01:46:33,090
computational frameworks and all the

1623
01:46:33,107 --> 01:46:35,160
heuristic that we can then use like

1624
01:46:35,192 --> 01:46:37,155
variational, Bayes and all these other

1625
01:46:37,202 --> 01:46:40,560
methods. So it'd be interesting to look

1626
01:46:40,592 --> 01:46:43,740
back at different data sets of in

1627
01:46:43,757 --> 01:46:47,630
vitro and in vivo and in silico neural

1628
01:46:47,765 --> 01:46:51,240
activation, especially if

1629
01:46:51,257 --> 01:46:55,295
the task was of this constrained

1630
01:46:55,385 --> 01:46:58,965
set of POMDPs and

1631
01:46:58,982 --> 01:47:00,765
it was already amenable. Because, as you

1632
01:47:00,782 --> 01:47:03,890
brought up, other settings would require

1633
01:47:03,920 --> 01:47:06,100
a little bit more theory development

1634
01:47:06,825 --> 01:47:10,287
before we understand what POMDP family

1635
01:47:12,150 --> 01:47:13,750
would be applicable.

1636
01:47:19,650 --> 01:47:23,535
Cool. Well, do you have any final

1637
01:47:23,642 --> 01:47:24,837
thoughts or questions?

1638
01:47:27,300 --> 01:47:30,625
Well, do you hop.

1639
01:47:34,912 --> 01:47:38,762
I kant to download the MATLAB scripts

1640
01:47:39,487 --> 01:47:42,437
and generate the figures,

1641
01:47:43,387 --> 01:47:45,907
play around with a few of these

1642
01:47:46,060 --> 01:47:49,297
parameters? Like, I see that you can

1643
01:47:49,330 --> 01:47:52,175
change how far the entity can see.

1644
01:47:53,887 --> 01:47:56,662
And then with these models, I'm also

1645
01:47:56,725 --> 01:47:59,787
always curious about the computational

1646
01:47:59,937 --> 01:48:02,902
complexity. Like if you extended the

1647
01:48:02,920 --> 01:48:05,347
planning horizon from four to five or

1648
01:48:05,380 --> 01:48:06,950
you dropped it down to three,

1649
01:48:08,962 --> 01:48:11,902
what is the runtime consequences and

1650
01:48:11,920 --> 01:48:14,162
what is the performance consequences?

1651
01:48:15,187 --> 01:48:19,792
And where might we be able to use single

1652
01:48:19,915 --> 01:48:23,182
or swarms of really simple

1653
01:48:23,335 --> 01:48:26,817
agent, maybe even making binary

1654
01:48:26,877 --> 01:48:30,057
decisions and achieve

1655
01:48:30,147 --> 01:48:33,382
high performance? And where

1656
01:48:33,460 --> 01:48:37,507
do we really need to move into

1657
01:48:37,585 --> 01:48:41,387
these large combinatoric spaces

1658
01:48:42,787 --> 01:48:46,327
in order to solve problems and the

1659
01:48:46,345 --> 01:48:48,967
kinds of complex planning problems that

1660
01:48:49,090 --> 01:48:52,732
we solve, whether it's planning our day

1661
01:48:52,810 --> 01:48:54,425
or planning our week,

1662
01:48:56,287 --> 01:49:00,650
are those force like true

1663
01:49:01,087 --> 01:49:04,567
Deep Harrison planning problems with

1664
01:49:04,615 --> 01:49:06,322
extensive consideration of

1665
01:49:06,355 --> 01:49:09,097
counterfactuals and calculation of

1666
01:49:09,130 --> 01:49:12,425
alternatives? Or are those actually

1667
01:49:15,187 --> 01:49:18,502
composite decisions that are

1668
01:49:18,520 --> 01:49:20,237
made up of smaller,

1669
01:49:21,487 --> 01:49:25,222
simpler, sub decisions that

1670
01:49:25,255 --> 01:49:28,267
we may or may not have flexibility to

1671
01:49:28,315 --> 01:49:32,497
restructure? So that

1672
01:49:32,605 --> 01:49:35,877
a decision, a complex

1673
01:49:35,982 --> 01:49:39,367
chess maneuver, a sacrifice in chess or

1674
01:49:39,490 --> 01:49:42,877
another game. It may be

1675
01:49:42,895 --> 01:49:46,512
possible to model that as a Deep Horizon

1676
01:49:46,587 --> 01:49:50,077
scan or a kind

1677
01:49:50,095 --> 01:49:54,575
of intuitive heuristic for

1678
01:49:54,937 --> 01:49:58,262
an appropriate skilled entity.

1679
01:50:01,537 --> 01:50:04,917
For this particular two Brea structure,

1680
01:50:05,052 --> 01:50:11,032
there is a clear limitation about the

1681
01:50:11,110 --> 01:50:15,575
horizon forward

1682
01:50:16,462 --> 01:50:18,697
because it doesn't use forward

1683
01:50:18,880 --> 01:50:21,897
prediction, it used post action

1684
01:50:22,092 --> 01:50:24,297
approach. So it's a clear limitation,

1685
01:50:24,342 --> 01:50:30,042
but still, it may have some performance

1686
01:50:30,102 --> 01:50:32,900
ability, active, some levels of

1687
01:50:35,512 --> 01:50:36,587
affordance.

1688
01:50:38,587 --> 01:50:41,872
That provides even another way

1689
01:50:41,905 --> 01:50:45,472
to look at planning. The two

1690
01:50:45,505 --> 01:50:47,802
ways I was describing planning, as it's

1691
01:50:47,832 --> 01:50:50,277
often described in the PMDP literature

1692
01:50:50,457 --> 01:50:53,547
is again, is it a true Deep Horizon

1693
01:50:53,592 --> 01:50:57,027
consideration or is it just short term

1694
01:50:57,057 --> 01:50:59,467
heuristics or nested models that are

1695
01:50:59,515 --> 01:51:03,097
short? And I think that this

1696
01:51:03,205 --> 01:51:06,812
paper says maybe neither.

1697
01:51:07,537 --> 01:51:11,302
Maybe it's purely the

1698
01:51:11,320 --> 01:51:15,592
active causality on the past that

1699
01:51:15,640 --> 01:51:19,192
leads to the emergence of sentient and

1700
01:51:19,240 --> 01:51:23,217
maybe even teleological planninglike

1701
01:51:23,277 --> 01:51:26,442
behavior through the ongoing

1702
01:51:26,502 --> 01:51:29,167
reconsideration of the consequences of

1703
01:51:29,215 --> 01:51:32,017
least action. But it's neither a short

1704
01:51:32,065 --> 01:51:34,947
nor a long term planning challenge.

1705
01:51:34,992 --> 01:51:37,250
It's actually like a memory and learning

1706
01:51:37,687 --> 01:51:40,287
challenge. And no planning occurs.

1707
01:51:40,437 --> 01:51:44,537
Right. Indirect planning

1708
01:51:45,187 --> 01:51:48,587
planning element is involving C matrix.

1709
01:51:50,212 --> 01:51:53,347
Planning as a phenomenon occurs and

1710
01:51:53,380 --> 01:51:55,937
derisking through time occurs.

1711
01:51:56,437 --> 01:52:00,067
But it says something quite

1712
01:52:00,115 --> 01:52:03,687
interesting and deep that that phenotype

1713
01:52:03,762 --> 01:52:07,672
or function could be enacted by

1714
01:52:07,705 --> 01:52:11,347
a system that explicitly looks a long

1715
01:52:11,380 --> 01:52:13,972
way ahead, explicitly looks a short way

1716
01:52:14,005 --> 01:52:17,677
ahead or moves forward and

1717
01:52:17,695 --> 01:52:20,527
looks backwards only, right, which is

1718
01:52:20,545 --> 01:52:22,222
what they sometimes say about the past

1719
01:52:22,255 --> 01:52:23,225
and the future.

1720
01:52:25,012 --> 01:52:28,852
So that may be a very

1721
01:52:28,945 --> 01:52:33,247
biologically plausible form

1722
01:52:33,430 --> 01:52:36,942
of learning. And it's

1723
01:52:37,002 --> 01:52:39,502
already intimately connected with the

1724
01:52:39,520 --> 01:52:43,102
dynamics and the activity in terms of

1725
01:52:43,120 --> 01:52:45,050
an integrated loss function.

1726
01:52:46,537 --> 01:52:50,097
So these are all excellent directions

1727
01:52:50,142 --> 01:52:53,600
to keep learning on. Right?

1728
01:52:54,262 --> 01:52:57,462
And I'm also interested in the barrel

1729
01:52:57,537 --> 01:53:00,772
implementation of such a short term or

1730
01:53:00,805 --> 01:53:03,397
long term for the prediction and

1731
01:53:03,430 --> 01:53:06,832
running. And I hope to find

1732
01:53:06,910 --> 01:53:10,775
some nice connectivity to such

1733
01:53:11,362 --> 01:53:15,350
implementation of Bayesian motor and

1734
01:53:15,937 --> 01:53:19,562
implementation in VR brain network.

1735
01:53:20,887 --> 01:53:25,102
Cool. And also I'm always curious about

1736
01:53:25,195 --> 01:53:29,092
the invertebrate brain as an

1737
01:53:29,215 --> 01:53:32,647
ant researcher. And so many of

1738
01:53:32,680 --> 01:53:36,802
the brain architectures as

1739
01:53:36,820 --> 01:53:38,947
well as the brain architecture that

1740
01:53:38,980 --> 01:53:42,447
people discuss are mammalian centric,

1741
01:53:42,492 --> 01:53:44,787
which makes sense. The mammalian

1742
01:53:44,862 --> 01:53:47,092
cortical column and the relationship

1743
01:53:47,215 --> 01:53:50,107
with Dopaminergic midbrain and the

1744
01:53:50,110 --> 01:53:52,842
cortical regions and the spinal reflex

1745
01:53:52,902 --> 01:53:56,377
arc those are all important systems of

1746
01:53:56,395 --> 01:53:59,837
interest. Yet the micro

1747
01:54:00,187 --> 01:54:03,297
and meso anatomy of the invertebrate

1748
01:54:03,342 --> 01:54:06,662
nervous system is pretty distinct.

1749
01:54:07,687 --> 01:54:11,762
So our model should be able to describe

1750
01:54:12,337 --> 01:54:15,875
neural and cognitive systems, of course,

1751
01:54:16,237 --> 01:54:19,112
across invertebrates and vertebrates.

1752
01:54:19,537 --> 01:54:22,042
So I look forward to also seeing what

1753
01:54:22,090 --> 01:54:24,927
those models of the invertebrate nervous

1754
01:54:24,957 --> 01:54:29,512
system and collective behavior where

1755
01:54:29,575 --> 01:54:32,412
you could have some type of backwards

1756
01:54:32,487 --> 01:54:36,587
looking risk inference

1757
01:54:37,162 --> 01:54:41,162
of the swarm. Who knows?

1758
01:54:43,537 --> 01:54:47,497
Well. We really

1759
01:54:47,605 --> 01:54:50,122
appreciate the time that you took for

1760
01:54:50,155 --> 01:54:53,227
these discussions. I think they are

1761
01:54:53,320 --> 01:54:56,752
immensely important. And we wish you the

1762
01:54:56,770 --> 01:54:58,572
best of luck in these continued

1763
01:54:58,617 --> 01:54:59,462
directions.

1764
01:55:03,387 --> 01:55:04,150
Yes.

1765
01:55:06,612 --> 01:55:10,227
Okay, that's it. Thank you. Thank you

1766
01:55:10,245 --> 01:55:11,967
very much. See you next time.



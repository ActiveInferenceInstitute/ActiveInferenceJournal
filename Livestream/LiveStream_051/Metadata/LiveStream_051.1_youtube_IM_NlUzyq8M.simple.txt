SPEAKER_00:
All right.

Hello, everyone.

Welcome.

This is ACT-INF livestream number 51.1.

We are in the second discussion of this paper, Canonical Neural Networks Perform Active Inference.

Welcome to the Active Inference Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

You can find us on this slide.

And this is recorded in an archived livestream.

So please provide us feedback so we can improve our work.

All backgrounds and perspectives are welcome and will follow good video etiquette for live streams.

Head over to ActiveInference.org to learn more about the Institute and how to participate in projects and learning groups.

All right.

We're in act-inf stream number 51.1 and having our first non-solo discussion on this paper, Canonical Neural Networks Perform Active Inference.

And really appreciative.

to you that you've joined today it's going to be a great discussion we'll begin with introductions i'll say hello and then please just jump in however you'd like and we can start by setting some context so i'm daniel i'm a researcher in california

And I was interested in this paper because we've been talking a lot about active inference from a variety of different perspectives, from the more fundamental math and physics to some applications, philosophy, embodiment, all these really interesting threads.

And this paper seems to make a really interesting

clear meaningful contribution and connection by connecting active inference entities and this approach of modeling to neural networks which are in daily use globally so thought it was a fascinating connection and really appreciate that we can talk about this today so to you and welcome

Go for it, Takuya, however you'd like to introduce and say hello.


SPEAKER_01:
Oh, yeah.

Hi, I'm Takuya Isomura, neuroscientist in RIKEN Brain Science Institute in Japan.

I'm particularly interested in universal characterization of neural network and brain using mathematical techniques.

So this work, I believe, is important as a link between active brain-influenced aspect, vaginal-influenced aspect of the brain, and the dynamic system aspect of the neural network.

So I'm very happy to join this discussion session.

Thank you for the invitation.

Nice to meet you.


SPEAKER_00:
Nice to meet you as well.

The first thing you added, the universal characterization of neural networks.

What is the universal characterization of neural networks?

Why is it being pursued in this area of research?


SPEAKER_01:
So as a narrow sense, my aim, main aim of this paper is that so, you know,

People use active inference formalization to characterize brain activity, behavior, so on, so on, but which would be different from a conventional neural network.

So there is a class of problem.

which is associated with conventional neural network.

And it is not very clear whether all characterization of conventional neural network can be explained by active inference free energy principle or not.

So here, universal characterization means that characterization of every aspect of conventional neural network

which is a kind of dynamical system derived as the association between a physiological phenomena and simple mathematical formula typically using differential equations.

As the broad sense, I think universal characterization means that, well, it is the characterization of brain intelligence, but it's a big picture.

And what the paper particularly addresses is only one aspect of the full picture.


SPEAKER_00:
All right.

So it'll be great to pull back to really understand what synthesis is happening.

So I'm going to ask, what makes a neural network model a neural network model?

And what makes an active inference model an active inference model?

Is this synthesis and connection you've made true because of what?


SPEAKER_01:
because basically what we showed is the mathematical equivalence between the formulation

of canonical neural networks and the formulation of active inference, in the sense that we showed that a class of neural network can be characterized by a minimization of some biologically plausible cost function.

And we show that that cost function can be read as a class

variational Bayesian inference under a particular class of generative model in terms of well-known partial observer polymer composition process.


SPEAKER_00:
All right.

Shall we perhaps walk through some of the sections of the paper?

It would be awesome just for each of these sections,

maybe the numbered and the lettered sections, what does the section aim to show and why was it there in the paper?


SPEAKER_01:
So yeah, briefly.

It's an overview, right?

So briefly, so first we introduce the main issue, main problem of our interest, which is a relationship.

So we try to make a formal link between neural network and active rainforest.

That's the main

problem background.

And then we first formulate the equivalence, mathematical equivalence in a very broad manner.

So in the first section in result, we formulate the

the relationship using a complete class theorem, which is a well-known statistical theorem proposed very long time ago.

And using that, we link a general form of neural network with a general form of variational Bayesian inference.

problem is that this characterization does not address a specific form of generative model, which is crucial to characterize a specific model, specific neural network dynamics.

So in the following

sections, we characterize the problem using POMDP, or Partially Observed Grammar Conditional Process, and link that model with a particular class of canonical neural network.

And then we simulate it.

We use the simulation to

to corroborate that property in terms of some main tasks.


SPEAKER_00:
All right.

Thank you for this.

Could we talk about the complete class theorem?

So what is the scope of the complete class theorem?

And why was it the relevant set of the neural networks to pursue or the right way to frame it?


SPEAKER_01:
Thank you for asking that.

I like the slide you showed in the last week's video.

Completed class theorem basically indicates the relationship between some class of decision rule and Bayesian inference.

Here, a crucial keyword is admissibility or admissible decision rule, which is a rule which is as good as other decision rules, or at least

at one point, better than other decision rules.

So, simply speaking, admissibility indicates, in some sense, it is the best rule for some aspect.

And usually we characterize such a goodness using cost function, loss function, or risk function.

And here, what we did is establish some association with this type of loss function or risk function with a function of canonical neural network, which we call cost function or biologically plausible cost function for neural network.

Our assumption is that neural network minimizes cost function.

If it achieves the minimization and it is obviously achieves some sort of optimality, we can say it is admissible with respect to that cost function.

The beauty of complete class theorem is that if we find some admissible decision, then automatically we can say that it is Bayesian inference in terms of some Bayesian cost function with generative model or prior beliefs.

So this complete cross theorem is crucial as an abstract characterization of the relationship between conventional neural network architecture dynamics and variational Bayesian inference.


SPEAKER_00:
All right.

Thank you.

What does it mean when you said it was biologically plausible of a loss function?


SPEAKER_01:
The term is a little bit arbitrary because in this paper, we mean by biological reproducibility in the sense that this neural network model can be derived from realistic neural model through some approximation.

Here, biologically plausibility suggests or means plausibility as a neural model or synaptic plus system model.

And if this cost function, loss function can derive such a plausible algorithm, then we can say that this cost function is biologically plausible.


SPEAKER_00:
So what is the distinction between those neural and synaptic components in the loss function, or what equation to look at?


SPEAKER_01:
You mean distinction between neurodynamics and synaptics?


SPEAKER_00:
Yeah, what is the distinction between them and how is it represented in the equations?

Okay.


SPEAKER_01:
Basically, neural activity equation means a differential equation about a variable that represents firing intensity or some sort of variables associated with neural firing.

On the other hand, synaptic elasticity equation means an update rule about the synaptic weight or synaptic strength, which is a connection between two neurons.

And the beauty of this formulation proposed in this paper is that we characterize both neuroactivity equation and synaptic plasticity equations in terms of gradient descent on a same cost function, a common cost function.

So we can say that

If we consider the partial derivative of some cost function with respect to neural activity, then it derives a gradient descent rule about neural activity.

While if we consider partial derivative of cost function L with respect to synaptic weight, then we derive a synaptic plasticity rule.


SPEAKER_00:
Are those the only two aspects of a neural network or why are those the two key aspects?


SPEAKER_01:
It is a main character.

I think it's a main body of the neural activity if we consider some inference running or action exhibit by neural network in the sense that

neural activity correspond to fast dynamics, fast gradient dynamics.

And while synaptic plasticity indicate a slow dynamics that minimize risk function, cost function.

But in general, we can consider any aspect, any variables associated with neural network.

For example, at least what we show in the paper is that

any free parameter which may be associated with firing threshold.

Although we don't discuss in this paper, it would be possible to add other

variables related to neural network for example we here we ignore contribution of real factor but it will be possible to add the real factor in this formulation or any other aspect of virus called neural network


SPEAKER_00:
That's very interesting, and it speaks also to a general separation of time scales.

For example, in different multi-scale systems or in the renormalization group, where it's describing some minimal multi-time scale system

where the faster timescale can be seen as perception like a slower timescale can be seen as more learning like, and then in some hierarchical model, what's learning of one timescale can be perceptual for a slower timescale.

So it's a very nice generalization.

Are there any examples of decision rules that will help us think about the action components of what the neural network is doing?

Because it may be more familiar to think about digit characterization and image classification, some kind of classical tasks for neural networks.

But how does the decision rule play out in the context of neural networks?


SPEAKER_01:
In this paper, we basically assume a closed loop, comprising a neural network part and an environmental part.

Neurons receive sensory input from the environment and provide some feedback to the environment.

Yeah, even with the example of digit classification, we can say that output corresponds to classifier classification output, which is kind of decision rule.

More relevant

example would be, for example, controlling agent, like a robot control, or any kind of controlling or decision making tasks.

For example, when we encounter some choice task, we need to advertise, for example, left or right, or something, any kind of such a

decisions can be associated with the admissibility or admissible decision rule.


SPEAKER_00:
So what would an example of an inadmissible or admissible strategy be in the decision-making task?


SPEAKER_01:
Admissibility

usually characterized by loss function or risk function so if

so here in admissibility indicate that this there is another decision rule which is uh at least one point better than uh the the the forecast decision rule so yeah simply speaking in other visibility indicate that it's that that decision rule is not

better, not good, relatively.


SPEAKER_00:
But let's just say our decision rule is we always turn right.

is that an example of a decision rule because there might be settings where that is strictly effective and the simplest rule whereas there's other settings where that's going to be tragic so what does it mean to be admissible for an agent in light of different environmental contexts


SPEAKER_01:
That's an interesting point.

So even with such a two-month simplified rule, it can be admissible under some particular situation, particular loss function.

For example, the rule that always turn right may be the best under some situation, right?

So the relationship of admissibility or inadmissibility depends on both agent characteristic and the environmental characteristic.


SPEAKER_00:
What aspects of the environment


SPEAKER_01:
For example, if that decision rule matches the structure, architecture of environment, then maybe that decision always turn right, achieve the shortest path under some situation, some environment.


SPEAKER_00:
How does this admissibility help us think about like overfitting?

And how does it help us think about the way that different practices are used for neural networks to prevent them from being overfit in practice?


SPEAKER_01:
Well,

Well, strictly, admissivity is characterized with the Bayesian risk.

So we cannot observe a hidden state of the environment.

Only we can observe a part of the entire

So, the question is, important question is, what is the best choice under such a limited information?

Yeah, information.

So, this Bayesian list idea or admissibility or complete crest theorem,

tell us that well-known Beijing

Only the well-known Bayesian framework achieved the admissibility, admissible decision, which means that in this aspect, Bayesian optimizations give us a best-of-choice strategy.

Otherwise, we overfeed for

would find the suboptimal solution.

So it's a nice association, nice linkage between the decision, what is a good decision, and more established statistical inference, Bayesian inference framework.


SPEAKER_00:
Thank you.

That's very helpful.

So we're reducing our

uncertainty and risk about hidden states in the environment so in the special case where the entire environment is observable without error like a chess game then there's an equivalence between calculation of risk or loss on observables or on hidden states but they're not really hidden but they are environmental states

Whereas any amount of uncertainty in the mapping between observations and hidden states, which is usually shown as A in the partially observable Markov decision process, any amount of uncertainty about unobserved or partially observed environmental states

enables you to fit your uncertainty optimally about that hidden state and fit that uncertainty simply with a gradient descent and by doing so you don't overfit a model of observables which might be the fallacy or the issue with simply doing descriptive statistics

you might get an infinitely small variance with a frequentist estimate because you have 100,000 data points.

So the variance from a descriptive statistics perspective might be very small.

I think it speaks very much to why neural networks are useful in practice from training with limited data sets

Because that's an empirical observation that they don't entirely overfit.

But also I'm sure there's ways to construct them that are overfit.


SPEAKER_01:
Yeah.

Overfit would occur if we select some optimal prior beliefs, for example.

Well, I'm not sure if...

it is an overfit in the sense that you mentioned.

Because if we select some prior wave, then the Bayesian function itself changes, and the neural network tries to fit to that cost function.

So cost function minimization would be achieved in such a situation, but that solution is not

good for our, you know, original purpose.

That's a tricky part.


SPEAKER_00:
Yeah, that is reminiscent of some discussions we've had discussing like driving off a cliff or blowing up is also reducing free energy, like dropping off a building reduces your potential energy.

And so there are potentially decision making or strategic trajectories

that do for some time horizon minimize free energy.

Perhaps even, or maybe even guaranteed better than some longer time horizon.

Because if the short-term strategies were somehow better than the long-term horizon, it would be difficult to imagine because the long-term horizon would be at least as good as a short-term strategy.

So that speaks to the challenges of planning

in action.

So how is planning addressed in modern neural networks?

And how does this work help us think about that?


SPEAKER_01:
That's another very important aspect.

So first, I have to say that this framework addresses the planning aspect.

But that planning is not necessarily

the optimal or the optimal solution in the sense that what we are interested in is optimization or planning under a limited structure.

The structure is characterized by here, vertical plausible two layer neural networks.

So yeah, planning,

occurred by association between risk in the future and our decision in the past.

Here, we model that aspect using a delayed modulation of cyanotic plasticity mediated by some neuromodulator or neurotransmitters.

This is the model.

This is modeled as the product of the risk factor and the heavier product holding the cost function, neural network cost function.


SPEAKER_00:
All right, I'm going to ask a great question from the chat, and then we'll look at the figures a little closer.

So ML Don wrote, a question stuck in my mind for a long time.

Could you please put it to rest?

Do we need to have knowledge about all states, possible actions, and sensory inputs for active inference?


SPEAKER_01:
Well, you mean if you seek the exact solution, exact optimal solution, then maybe more information would help you to find that.

But under some model, general model, under some ideal assumption, then

or variables are not necessary to achieve the optimal solution.

I'm not sure if I correctly answered your point.


SPEAKER_00:
So, just to restate it,

Of course, knowing all the state's possible actions and sensory inputs, it's not a bad thing.

Worst case, there's some computational complexity trade-offs, but the problem becomes fully stateable.

But I think ML Don is asking about cases where you don't know all of the state spaces or potentially even the dimensionality or the semantics of...

hidden states active states sensory inputs and why not even add cognitive states so in not just partially observed but partially known state spaces how are these addressed in neural networks and how does active inference help us think about it


SPEAKER_01:
Okay.

I think the question is about how can we separate those states, sensory, action, internal, external.


SPEAKER_00:
How can we separate, not just in principle have these states be separated,

but deal with the fact that some of these states we might have good knowledge on and some states, like the hidden states, we might not even know.

Like we don't know the dimensionality of the cause vector in the world.

I see.


SPEAKER_01:
For example, in terms of dimensionality,

There is a statistical technique to estimate dimensionality, for example, various

information criteria, like Akai-gei information criteria, base information criteria, or then try to infer or estimate plausible dimensionality about the environmental hidden states.

So there is an analogy with those information criteria and variational free energy minimization.

So with variational free energy minimization, we can

we can identify the plausible model structure, which in principle involves the dimensionality aspect.

But in terms of neural network, in this paper, we don't carefully consider about the dimensionality optimization because we first define the number of neurons.

and don't change during training.

But in principle, we can consider the change in the number of neurons which is associated with, for example, adult neurogenesis or development during the developmental stage.

That would be an important extension.


SPEAKER_00:
this this this this direction that's very interesting here's um a remark well one note is equation one summarizes a lot of what you've been describing

There's a parallelism or a concordance being drawn between the loss function of neural networks and the variational free energy of the parameterized model there.

So to come back to these processes that influence learning, which we could think of as the neural network becoming more fit from a loss function perspective, or

the variational Bayesian partially observable Markov decision process entity generative model becoming better at doing what it does.

So there's the firing rate on the neural network side.

the synaptic plasticity at a slower time scale which we discussed a little earlier and then now there's a third time scale with the birth and death of new cells and maybe even new layers and that kind of multi-scale temporal structuring is not intrinsic to the base graph

to represent multiple nested time scales in a base graph in the active inference literature, it's more common to make a hierarchically nested model.

And just say that the time handling on one level is happening more rapidly with respect to clock time than deeper nested slower models.

Whereas the neural formulation

allows us to deal with multiple ongoing timescales without appealing to hierarchical nesting which is a very important feature well yeah both both directions would be


SPEAKER_01:
possible without hierarchical modeling or with hierarchical modeling.

So even with hierarchical modeling, the optimization of

dimensionality should be possible or would be possible but so in other direction so we can consider that a population of neural models so one has a single layer another has a

two layers, three layers, four layers, and consider the probability or plausibility of network architecture.

associated with the performance or cost function minimization under a particular environment, which is, in principle, have the same computational architecture with the hierarchical Bayesian model.


SPEAKER_00:
Very interesting.

Yes.

Perhaps I over generalized or speculated because I thought about how one could have a 100 time step POMDP that also performs multi-scale behavior, potentially extremely wastefully, but at least it could in principle.

And similarly within a neuron, there could be another neural network.

or some other structure approximated by that.

So they almost both enable hierarchical and non-hierarchical modeling, as you described, but in very different ways that lead to very different implementations.


SPEAKER_02:
Yes.


SPEAKER_00:
i think this brings us to the topic of forward and reverse engineering so you talked a lot about reverse engineering yeah what is reverse engineering and what is forward engineering and what has been done in these areas of engineering


SPEAKER_01:
OK, I'm not an expert of reverse engineering in the broad sense.

But I believe that reverse here means your characterization of the blueprint of some device or machine

data observable information like activity or action behavior of some agent right so goal is identification of the blueprint

and the crucially here blueprint correspond to generative model because once we define generative model we can derive variation of energy algorithm running algorithm inference algorithm and any behavior of agent so

here reverse means that we first observe some activity of agent and its mechanism is still unknown for us but we can estimate its mechanism using that activity by identifying the most plausible

generative model which can minimize some cost function or risk function when we feed the data to

the model.

So, on the other hand, forward engineering would be a more mainstream way, first define model, blueprint, generate model, then derive everything, including functional, running inference algorithms, behavior action algorithm.


SPEAKER_00:
So by reverse engineering neural networks, we're observing some already parameterized neural network and then fitting a POMDP to it.

To what extent is it possible to take a given POMDP and create a neural network that performs that inference?


SPEAKER_01:
uh okay in this paper uh or in the following uh paper what we consider is a strategy that we first fit uh empirical data whether for neural response data to a basically plausible canonical neural network model which is a

which is similar to a conventional model fitting approach, where we have differential equation and data and fit data to the differential equation to explain the behavior with the minimal prediction.

So now,

a virtue of this framework we established is that we can transform we can naturally transform such a neural network architecture with the well-known partially observable market decision process architecture because for any a kind of canonical neural network there is a

cost function.

So we derive cost function from neural activity equation, which is opposite with the conventional way we first define cost function derived algorithm.

And then we use

the former equivalence between neural network cost function and variational free energy.

So now transform the neural network architecture to Bayesian model architecture.

And once we characterize variational free energy, there should be some generative model that

define that variational free energy functional.

So in particular, in this example, canonical neural network nicely correspond to a class of well-known partially observed Markov decision process.

So by using this procedure, we

uh identify the plausible home dp architecture which is a quest which correspond to observed activity data okay


SPEAKER_00:
well let's stay on this last point so after all those transformations first the measurements of neurons using that data to fit the neural network and then by virtue of the relationships unpacked in the paper transforming the neural network in the left side of figure one into

a particular form of the POMDP.

So first, what are the constraints on that form of the POMDP?

Is this a little corner of model space or what are the space of acceptable POMDPs?


SPEAKER_01:
That totally depends on what kind of neural network model you are considering.

So for example,

In this paper, we discuss about a particular class of POMDP in which each state takes either 0 or 1.

So it's very restricted compared to the general form of POMDP.

But we consider a factorization in the sense that although

Each is either 0 or 1, but we consider a vector of observation, a vector of hidden state, where each element corresponds to one single one-hot vector.

But as an entire state, it can represent a high-dimensional discrete state space.

And this architecture nicely corresponds to neural network architecture because usually each neuron takes

either 0 over 1 or some continuous variable between 0 and 1.

So we use this association to characterize a particular POMDP which corresponds to neural networks.

And this follows a particular mean field approximation or approximation in generative model because we associate posterior belief in this particular POMDP with the neural activity.

which means that posterior of action also have a factorization architecture, in the sense that we don't, we don't fully consider about the second order statistics between neuron 1's activity and neuron 2's activity, which is outside of this formalism.

So each

Each neuron's activity corresponds to a posterior expectation about a particular element of the state.

And we don't consider the joint posterior probability of all states.

So although this is a limitation, we see this as a Bayesian inference.

But otherwise, we can consider any recurrent network architecture which corresponds to a state of transition matrix.

And it would be possible to extend this architecture to hierarchical structure in the sense that it is straightforward to consider a tree structure or any kind of hierarchical structure by assuming that some neuron connect to other neuron but not connect to

other neuron.

So this is same as considering hierarchal structure in general.


SPEAKER_00:
Thanks, that's very interesting.

It's commonly remarked in

the base graphs that they represent the connections amongst random variables.

And there's a relationship between their computability and their sparsity.

The sparsity structure as in which variables do or do not influence each other

makes the problem tractable through factorization and just kind of conceptually.

Like if every one of a thousand variables or an unknown number of large variables, if it was all by all, the number of parameters to fit on that connectivity matrix would be very high.

So statistical power would be very low for any given edge.

Whereas the more and more constrained you make the connectivity of the variables,

the more statistical power you have to resolve or kind of spend on fitting those edges, like in a structural equation.

But you might be losing sight of the unknown unknowns by constraining yourself to a very limited or fallacious topology of the variables.

So there's this kind of structure learning, statistical inference question in the Bayes graphs.

then on the neural side from the biological much of neuroscience is about understanding how the firing rate connectivity patterns and other factors how the structure of those neural systems and their function like form and function enable

adequate inference and inference on action.

So it's like in both of those areas, or really like in neural network, artificial and neural networks, and in variational bays, the discussion is about how the structure and the fine tuning

work together to generate function and about some of the statistical or um biological challenges of balancing different needs while also constraining the cost in terms of materials and biometabolism so it's a very rich intersection that is being explored here

if these models can really be moving back and forth.


SPEAKER_01:
In the sense that back and forth?


SPEAKER_00:
Moving back and forth like there's some imprints of the model that is implementation independent or like some interlingua or some semantics

Or compatibility?

I don't really know.

I mean, that's something we can explore is like, what is it that is such that one could forward engineer and then reverse engineer and have like kind of an expectation maximization between these two areas?

So what is it that's being sought?


SPEAKER_01:
Yes, that's an important point.

For example, a virtue of this relationship is that we can use the knowledge of Bayesian inference to explain neuroactivity and neurodynamics.

which is crucial because people often say that characterizing neural dynamics is not straightforward.

We may obtain some solution of neural net dynamics, but the meaning of that dynamics in terms of functional aspect is very unclear.

We don't know the meaning of connectivity

strength matrices and what is the meaning of the threshold factor so on so on those are derived from the you know the modeling physiological phenomena but it is not necessary to

have a clear linkage to a functional explanation of the brain.

But once we translate these dynamics into Bayesian inference,

then we can explain every functional aspect of the neural net dynamics architecture in terms of where established Bayesian inference under a particular class of Bayesian modeling, in this case, POMDP model.

So now it turns out that synaptic strengths correspond to A matrix, B matrix, which are very established

clear meaning.

So yeah, this is useful to explain neural synaptic property in terms of where we establish statistics.

Also, for the people in the active inference side,

it would be helpful to understand the neuronal substrates

about a particular active inference model.

So I think it's related to forward modeling.

But finally, to discuss about the vertical substrate of that forward model, we need to address the neural network

architecture, the substrate property.

So in that

case, we can transform a particular POMDP Bayesian modeling to a neural network architecture using this relationship and then get a prediction about the substrate.

So if we have this Bayesian model, this particular quantity in this model should be

would be possible using.


SPEAKER_00:
Oh, sorry.

Oh, it's all good.

Can you just repeat the last, like, 20 seconds?


SPEAKER_01:
Yes.

So in the last part I mentioned about the four, so first we define the Bayesian model and then can predict what is the neuronal substrate that corresponds to

that particular Bayesian model.

So this would be useful to identify the biological quantities that correspond to a quantity in Bayesian influence.

Yeah.


SPEAKER_00:
Wow.

Well,

there's a lot there it makes me think about the differences of implementation and heuristics in the computational setting which is often in the extreme disembodied and the biological setting which is in the extreme entirely embodied and for a given generative model

the kinds of computational heuristics that can be applied include a whole host of different strategies ranging from sampling to tree exploration and branching to parallel paralyzing the data architecture and all these other kinds of disparate strategies and software packages and implementations

But on the biological side, what is needed is something that's very simple, but also very inscrutable, which is a given pattern of interactions must embody that calculation.

So that might mean that it can add three digit numbers, but it can't add two digit numbers under some constraints.

but what isn't accessible to that kind of morphological biological or like form and functional computing what's not accessible are the tree branching the database decentralization like they're a different set of heuristics

right but they're both very useful when we're thinking about making sentient artifacts or benefiting simply from the explainability across both sides of this figure yeah


SPEAKER_01:
So you now address an important point.

So honestly, it is very non-trivial whether there is a corresponding biological architecture for any given Bayesian architecture.

I believe it is impossible to design biological architecture to correspond to arbitrary Bayesian architecture.

Only a limited aspect of Bayesian model can be implemented in a biological plausible manner.

And that point is crucial as a characterization of biological network, biological brain.

Yeah.


SPEAKER_00:
Wow.

Well, just to kind of touch again on this forward and reverse engineering,

for a given pomdp if we're willing to compose it within a certain class which might be quite general still but some class of pomdp as written on the paper

we may be able to have a neural network architecture that would be very amenable to deep learning, low energy computing, pre-training, various features.

And then on the other side, for a given artificial neural network that we come across in the wild,

or a model of neural dynamics that we fit using a neural network model, so something in a neuroscience laboratory, that model can have interpretability corresponding to the variables of a given POMDP.

And just to kind of give one more point on how that's going deeper than, for example, statistical parametric mapping, SPM.

So let's just assume that the neural network we're dealing with is fit from brain data from some lucky ant.

Right now, what would be possible, or prior to this line of work or without this line of work, one could...

fit a neural dynamics model and then do all kinds of analyses like power analyses on the different frequency spectra and say, look at the average firing rate or the correlation coefficients of firing rate.

So fit the firing rates and the synaptic plasticities and store all that data.

And then we could just pick a POMDP that we've seen in the literature

without any reference to the neural network and optimize the POMDP.

And then we could say, well, it turns out that when the POMDP O is high, there's increased theta power in this firing pattern.

So it's like comparing the descriptive statistics from the neural model to the descriptive summary statistics of the POMDP decision-making model.

However, with this formal connection, there is actually an interpretability to the unobserved neural states, which are what are being inferred from the fMRI measurement, from the EEG measurements and so on.

Those underlying variables have a specific interpretability in relationship to the structure of the POMDP.


SPEAKER_01:
Right.

So yeah, that's also a very interesting, important aspect.

So as you said, what you said is, I think, conventional, more conventional strategy.

And it is also

formally related to model comparison aspect so we usually think various major modeling and identify or select what is the

you know best model to explain the given data and this reverse engineering idea involves such a model comparison aspect in the sense that we we try to find the model with the best best explainability which should be

have the identical functional, right?

So we directly addresses the exact same cost function architecture using the transformation, natural transformation.

So it should be up to explain the neural data in the Bayesian sense.


SPEAKER_00:
yeah one can imagine how that would transform the way that current neuroimaging studies and technologies describe

what it is about the measurement that provides information about the cognitive model so to give another related example let's just say a person was wearing a EEG headset and a previous study had shown that increased Alpha band activity was associated with this behavior

That's comparing a descriptive statistic of the observations of the sensor and correlating the summarized observable to some other variable, like anxiety or performance on a behavior.

In contrast, an unobserved variable in this setting

the actual underlying neural state is being correlated to some semantic generative model component.

So it's no longer necessarily that any single frequency band would be associated more or less with a given outcome.

but it's actually some hidden state variability which gains the interpretability across this transformation which is a subtle point but it speaks to how broadly the equivalents would reinterpret empirical neuroimaging results

as well as a variety of artificial neural network experiments and diagnostics where people do lesion studies and double knockouts on artificial neural networks so anywhere where somebody with awareness sees that a neural network artificial or biological

is having summary features described and correlated to something that's more semantic in a quest for meaning may now have a different approach that involves formalizing the model explicitly in terms of unobserved hidden states with a cost function

akin to a variational free energy minimizing risk bounding surprise on the unobservables so even though the unobservables were modeled in a sense in the other conventional strategy like neural activity is a variable in fmri experiments it's underlying the bold signal

Yet this formalism concordance is a more coherent and powerful connection.


SPEAKER_01:
I believe so.

So you now address this very

important point.

First, to address that, we need to

clarify about the what is a program considered here.

So this is a problem so called meta Bayesian problem in the sense that researchers try to infer or estimate neural activity or brain activity, which inferred the external world dynamics, right?

So neuron or

brain-inferred environment, and we research inferred brain activity.

So there are two-step Bayesian inference processes.

So this sort of meta-Bayesian problem is quite tricky, intractable, because sometimes

So probability, so sometimes random variable becomes a posterior belief about other aspect.

So I think there is some established approach about meta Bayesian, but this paper provides some alternative in the sense that we separate two problem by saying that

here, what we infer is simply a neural network model or neural network dynamics, which is shown in the left-hand side of this figure.

So we feed the data to conventional neural network model, which is a simple differential equation.

But thanks to this

formal equivalence between neural network dynamics and the POMDP behavior, then we can transform the resulting neural network architecture or dynamics into the Bayesian inference in some sense post hoc manner.

So we nicely avoid directly addressing the meta Bayesian problem, but obtain the same kind of solution.

In that sense, yes, combining with brain activity recording like EEG or imaging, we can estimate a plausible neural network model in the left-hand side and we can transform that to a POMDP in the right-hand side.


SPEAKER_00:
Right.

Awesome.

I'm going to show an image and ask a question from Dave in the chat.

So Dave made this image.

It's the right side of figure one that we've just been looking at with the variational Bayesian formation.

And he wrote, the arc shown as impinging on the S self-arc

Is this intentional?

If so, it could represent tuning or modulation of the feedback of S into itself.

So do you have a thought on this?


SPEAKER_01:
It's intentional, yes.

I think it's related to the usual formulation of POMDP architecture and active inference context, in the sense that

you know our decision or policy in the usual setting modify the state transition matrix B matrix right so here delta is an alternative of policy

of agent.

So basically, the director indicates a state transition matrix under a particular decision which agent made.

In that sense, what the agent changes is a state transition matrix, not the state itself directly.

That's why we use this illustration.


SPEAKER_00:
awesome very subtle but important point which is when we look at the classical pomdp formulation so here we'll look at a version shown in figure two i'll just bring just figure two in um could you describe what you just did about the role of the b matrix in


SPEAKER_01:
influencing how hidden states change and how that is where our policies have impact and also please how do the top and the bottom of figure two differ okay so in the in the usual formulation uh under uh active influence uh with the pom db structure so we first consider uh

uh the the three prior free for us and the

depending on the prior preference, we compute the expected energy and its minimization provides the policy and the policy modulates the state transition.

So now in the upper layer, we instead use the

filter which is the action of the agent so here action or decision was made for each time step

so that unlike the computational formulation, we have a sequence of delta.

And for each time step, delta modulates the state transition matrix B. So B is the matrix that transforms a hidden state in the previous step to the S in the current time step.

Its moderation indicates that under a specific decision rule, for example, if this S indicates our position in the virtual environment, with the GO decision, our position moves forward.

Or if we choose a no-go decision, then it's unchanged.

So such a modulation of state transition was made by choosing delta.

And the lower part corresponds to Bayesian inference made by a Bayesian agent.

So basically, there is a symmetry between upper part and the lower part because we assume that this Bayesian agent has a plausible generative model which nicely corresponds to a given environment defined in the above part, upper part in this figure.

But one interesting thing, asymmetry, is that to model this particular canonical neural network, we don't

consider an arrow or a link from delta posterior to s posterior which is in the environment delta modulate s in the next step.

through beam matrix modulation.

In this particular Bayesian engine, which formally correspond to canonical neural network,

we don't consider that.

It is correspond to an absence of the projection from output layer to the middle layer.

OK.


SPEAKER_00:
Let's.

This is from the

2020 paper but it shows the neural network architecture the two-layer architecture so could you um restate the top and the bottom of figure two in the 22 paper and connect it to why it's important that you're studying two-layer neural network models

uh i miss you oh yeah can you just connect um the uh asymmetry between the top and the bottom on figure two with the two-layer neural network architecture you said that the asymmetry there's no direct link between


SPEAKER_01:
Yeah, this is another story.

So in the previous paper, there is only output rate or the perceptual layer, because we basically consider a single-layer feedforth network.

My apologies for some confusion about the network architecture in the 2020 paper.

So now, upper part of this network architecture correspond to environmental generative process.

And only a lower part correspond to single layer, single feed forward neural network architecture.

So now,

This part is identical to a map from O to S posterior in the 2022 papers.


SPEAKER_00:
Ah.

Okay.

So, on the top of Figure 2,

is the actual generative process.

It's the true structure of causation in the environment, which is to say that actions, delta, actually influence how states change through time via B delta.

The generative process through the A matrix emits observations, sequences of observations.

and here on the bottom with a mirrored structure is the generative model of the entity so what's the relevance of the arrows and the more 40 factor graph structure on the bottom the arrow uh indicates the the


SPEAKER_01:
the influence.

So it's a flow of the information in the sense that to calculate S in the step 2, we use the information of step 2's observation and step 1's

posterior expectation of our hidden state.

So those two determine the S2's expectation.

Usually in the phonograph, we consider a retrospective

So in the sense that S3 also affect the S2 inference.

But this correspond to a Bayesian smoother in the sense that we update every time step simultaneously to better inference.

However, what we consider here is more

filtering approach in the sense that for each step we compute the latest hidden state and then we don't change any other states in the past.

So that's why we don't consider the arrow from future to the past.


SPEAKER_00:
Awesome.

Yeah, just to highlight that.

In the Bayesian smoothing approach, it's kind of like fitting a spline because it takes the whole time series and it fits the smoothest possible line or the line whose smoothness is on the AIC-BIC frontier.

But here on the bottom, with the almost pseudocode implementation provided by the 40-factor graph, which was demonstrated to be equivalent with the Bayesian graph in the 2017 work with Fristin, Parr, and de Vries.

This architecture is reflecting a filtering scheme like a Kalman filter or just generalized Bayesian filtering through time where

estimates are being carried forward and changed time point to time point such that the decision rules or the updates perhaps more accurately are defined between time points and the total time series does not have to be loaded into memory or remembered at once and then the Bayesian filtering approach

has the asymmetry with a different consideration of action.

So why again is it that action is considered differently in the Bayesian filtering approach on the bottom of the generative model than the consideration of action in the generative process?


SPEAKER_01:
That corresponds to

lack of connection from y to x in the figure one or probably a figure four is helpful to the that relationship you also uh

Yeah, this is an example network architecture comprising input layer, middle layer, output layer.

What we consider is the information flow from sensory to middle layer, and middle layer have self-connection, recurrent connection, and middle layer project to output layer.

So there is no connection from all output layer to middle layer.

So that's why we don't consider the link from delta in the bottom layer of the figure to 2s posterior.

So this is different from true

generative process in the environment.

This is a kind of simplification because our purpose is identifying the plausible Bayesian model which corresponds to this type of Tureya neural network, canonical network.

So in other words, this canonical neural network uses an approximation about that point or uses a limited form of POMDP scheme.


SPEAKER_00:
Thanks.

So could you describe W, V, K, and gamma?

Just what is the biological or functional interpretation of those variables?

What brain regions or what processes or pathologies do they map to?


SPEAKER_01:
OK, so basically W, V, K are synaptic stresses in the form of matrices.

And their difference, so they represent

dynamic connection in the different layer or different architecture, in the sense that W means forward connectivity from sensory layer to middle layer, K corresponds to recurrent network, recurrent connectivity, and V corresponds to projection from middle layer to output layer.

So in this paper, we don't discuss the relation to brain anatomy in detail.

But one can consider an analogy, for example, say x correspond to cerebral cortex activity and y

why, for example, correspond to cerebrum in the sense that it determines the action.

So it is considered that in the cerebrum there is a signal that represents a choice decision, for example, goal decision or no goal decision.

made in Cerebrum.

It's analogous to this particular architecture.

On the other hand, in the Cerebrum, sorry, in the Cerebral Cortex, we compute the sensory information to generate some inference, prediction, and perhaps planning, so on.

which is computed by this recurrent network in this particular modeling.

Although we don't separate brain region in detail, this class of recurrent network is sufficient to characterize any type of architectures.

in the sense that we can design any higher curve or mutually connected architecture using a generic class of recurrent network by changing weight matrices.


SPEAKER_00:
Awesome.

So the middle layer we can think of as like the cognitive stuff.

It's the internal states when we talk about perception, cognition, action in the act-inf scheme or even in the sandwich model of cognition, perception, cognition, action.

so w is describing how those sensory inputs either in one step or composably in multiple steps become processed to these x representations of hidden external causes inferred external states and so these are the states that have that sigma relationship

a generalized synchrony with external states the sigma and the generalized synchrony are not discussed in in your paper but um it connects to other work and the recurrent connections are facilitating attention or waiting of the stimuli

This is the recurrent learning loop and the relationship of the A between observations and hidden state estimates.

And then a different kind of modulation comes into play between the hidden state estimate of the internal state and the action selection.

So what is gamma?

Corresponding to and why is the gamma modulation between layers two and three differing functionally from the K synaptic modulation of one and two?


SPEAKER_01:
Yeah, so K matrix basically formally correspond to B matrix in the Bayesian formulation.

So it's the information about the prediction, right?

Our knowledge or our expectation about the next state based on the previous state.

On the other hand, the role of gamma is quite different from such a computation.

Gamma basically means risk factor, risk function, which is... In principle, we can use arbitrary risk function.

So this is a part of generative model we designed.

And the role of risk function in generative model formulation is an alternation of form of generative model depending on the value of gamma, which enables

post-addictive or retrospective modulation of evaluation and past decisions given an outcome in the future.

In terms of neural network, of course,

it corresponds to some neuromodulation.

For example, dopaminergic moderation is famous in the literature, which modulates the activity and plasticity of various brain regions.

But we particularly focus on dopaminergic or any kind of neuromodulation of cyanotic plasticity in

the output layer which may correspond to cerebrum.

In the cerebrum, neuroactivity or plasticity modulated by dopaminergic input is used as the optimization option

rule, decision rule, or sometimes attention and various purpose.


SPEAKER_00:
awesome very interesting because in some previous papers and models that we've looked at attention is dealt with as policy selection on mental states so internal action selection is it's an action-like variable describing attention and awareness and even metacognition and so that connects

the role of dopamine in motor decision making seen in many dyskinesias, but also with the role of dopamine in seemingly

non-motor based decision making like gambling or investing where it doesn't seem to immediately translate to a given motor sequence yet it has analogous computational characteristics and the comorbidities and the side effects of different drugs that affect the dopamine neurophysiology are known to have carryover

in terms of like the rigidity or excessivity of motor and decision-making aspects so it's like interesting that dopamine has long been understood to have that parallel role in attention as cognitive action and motor action and that was established empirically through modifications of dopamine signaling

and also had been modeled analogously with computational neuroscience and this is providing again a slightly different interpretation of that very well studied dopaminergic modulation of attention and policy um


SPEAKER_01:
Yes.

And in addition to that, I believe another important aspect is a modulation of synaptic plasticity by dopamine.

Well, I can show you.


SPEAKER_00:
Do you want to show something?

Yeah.


SPEAKER_01:
Can you see this paper?

I sent you a link in the chat.

Or if you can't, I'll send you a PDF.


SPEAKER_00:
OK, let me see.

I'll get it up now.

right the paper is um a critical time window for dopamine actions on the structural plasticity of dendritic spines from 2014. so what is interesting about this paper


SPEAKER_01:
Yeah, it basically explains the modulation of plasticity by dopamine, which is common.

But the crucial point of this paper is that it proves that dopaminergic input can modulate

Hebbian plasticity even after Hebbian plasticity is established.

So this paper showed that they add dopaminergic input, for example, two seconds after or several seconds after the Hebbian plasticity is established.

But such a post hoc modulation, post hoc

introduction of dopaminergic input is sufficient to change the past plasticity, which may be associated with the post hoc re-evaluation of our past decisions.

So by decision making, we of course changing the plasticity, changing the weight matrix through plasticity,

But to evaluate the goodness or badness of our decision, we need to observe the future outcome, which is propagated by, for example, dopamine.

And this paper nicely shows empirically that dopamine actually can change the past

this Jones evaluation, maybe after such a psychic level, very, very local level equals called peak level.


SPEAKER_02:
Hmm.


SPEAKER_00:
So there's a short term window

critical time window that they're describing but there's some window yeah some window yeah by which dopamine potentially unrelated to the initial heavy and plasticity event right where secondary dopamine signaling or not secondary just after the initial fact

potentially have a different valence or the same valence can synergize or cancel the plasticity formed in the moment.


SPEAKER_01:
Exactly.

And this is not limited to dopamine, but other neuromodulator can also do this.


SPEAKER_00:
Well, on one hand, how does this change our understanding of animal neurophysiology?

And then I guess on the other hand, how does this influence how we would design sentient artifacts?


SPEAKER_01:
For both animals and the sentient,

agent artificial agent uh and one important message i believe is that so this tells us um possible simple architecture to make planning you know this is association between past decision and future reward or any risk factors which is otherwise computed by

computing forward prediction by iterating some computation.

I believe this is a usual way to predict the future event and then select the action.

But using this physical property, which is observed in an actual experiment,

we can design, we can imagine other simpler architecture to make a planning.

So for both animal and synthetic Bayesian agent, it provides an alternative explanation about the


SPEAKER_00:
association between our past decision and the future based on the optimization of our decision to maximize the reward or minimize risk well one interesting note is we spoke earlier about the difference between the bayesian smoothing all at once approach and the bayesian filtering step-by-step approach now

If one had infinite knowledge and computational resources, the Bayesian smoothing approach is the way to go.

Like, you don't want the decision rule for investment.

You want to look at the whole time series, past, present, and future, and know the best moments to have made the trades.

I mean, there's no comparison.

You're going to do better with the Bayesian smoothing.

However, it's just implausible computationally and because it requires total memory of the past and knowledge of the future.

So that's what motivates the development of Bayesian filtering approaches, which are tractable and calculable through time.

Yet, with this time-delayed modulation,

part of the Bayesian smoothing strength comes back into play.

It doesn't enable true anticipation of future states, but that's what the expected free energy does.

However, the delayed neuromodulation allows for reconsideration of a window of past states

And so in that way, it corresponds to like a slightly deeper filter.

Not just a filter of a time step of one, but a filter of like a rolling window of five or with some decay.

And you don't want that window to be too big because if the window were 10 minutes...

then too many contrasting stimuli would get piled together.

The dopamine level would just converge to a mean field average.

But there's some time decay or time constant on the post hoc modulation where that neuromodulatory signal is actually a parameter of interest.

and that's not an infinitely long or infinitely short window but it's some niche dependent amount of time and that's a very interpretable and first principles interpretation of the computational role of neuromodulators in a way that is also consistent with all these other concordances we've been exploring so

it's quite an interesting connection back.

I guess in our final minutes of this discussion, what are you, well, maybe go to the beginning at the end.

which I meant to ask earlier, but it's a good way that we can sort of close today and look forward, which is how did you come to this line of research, specifically studying neural networks in this way with Carl Friston and your colleagues?


SPEAKER_01:
So, yes.

So, well...

So my interest was the characterization of biological network.

So my first motivation is to make a biologically plausible artificial intelligence, but to achieve that we need to know about biological brain or biological neural network.

And then, in these several years, I collaborated with the doctor, Professor Carl Christen, to study about his theory of re-energy principle activity in forests.

And then my question during that period was, is the free energy principle everything about the biologically plausible neural network?

Or is there any other aspect that can characterize

uh the vertical neural network so it is non-trivial uh it was non-trivial so that's why i i tried to so i tried to start from uh characterizing the neural network first so our strategy is not

not considering the way of implementing any Bayesian algorithm as the brain architecture.

But my interest is rather characterization of a given vertical network in terms of

something, some other things.

One possible way is, of course, Bayesian inference, free-energy principle-active inference.

So that's why I first start from characterizing biological network.

But just defining neural network architecture is insufficient.

It is too

it is not tractable.

It is far beyond the computational tractability as the mathematical analysis.

And we need some assumption or some trick to increase the tractability.

And one day, I came up with an idea that

So in which we consider that both neuroactivity and plasticity follow the same cost function gradient.

This is very much an analogy with a physical system like Lagrangian formulation or Hamiltonian formulation.

So usually we

consider some energy landscape and design plausible trajectory as the solution of some principle of minimum action or least action.

So we

Imagine that, what if we apply such idea to neural network, conventional neural, canonical neural network or biological neural network to characterize their dynamics in the first principle.

That's the first motivation, first step to come up with this framework.

And finally, we noticed that it is not easy to connect the Newtonian dynamics with this type of neural activity study because neural activity creation is not necessary to be a second-order differential equation.

rather it is a first order and considering many things then we we finally used a cost function proposed in the papers which is not necessary have a formal

identity with the so-called Lagrangian in the Newtonian physics, but it is rather plausible as the rule or underlying mechanism of such type of network.


SPEAKER_00:
Awesome.

Well, it has been quite an interesting dot one.

I really appreciate everything you've shared today.

Is there anything else you want to add at this point?

Otherwise, we'll talk again.


SPEAKER_01:
Yeah.

I already speak.

Thank you.


SPEAKER_00:
All right.

Talk to you later.

Bye.


SPEAKER_02:
Yeah, thank you very much for a nice discussion.
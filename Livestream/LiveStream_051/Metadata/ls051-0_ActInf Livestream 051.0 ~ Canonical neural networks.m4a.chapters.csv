start	end	startTime	summary	headline	gist
28470	263530	00:28	This is the background and context first discussion for canonical neural networks. It's the first of several discussions that we'll have on the paper. How can artificial neural networks be understood as generic optimization processes. We propose that to accelerate progress in AI, we must invest in fundamental research in neuro AI.	This is the background and context first discussion for canonical neural networks	Can Canonical Neural Networks Perform Active Inference?
263680	450598	04:23	Canonical neural networks perform active inference and numerical simulations. This theory offers a universal characterization of canonical neural networks. It provides insight into the neuronal mechanisms underlying planning and adaptive behavior control. Here is the roadmap I'll be driving.	This paper looks at the interface between neural networks and variational Bayesian methods	Inferring with neural networks and variational Bayesian
450684	1225720	07:30	The papers canonical neural networks perform active inference by the authors listed previously. Biological organisms recognize the state of their environment by optimizing internal representations. They optimize their behavior for adaptation to the environment, thereby increasing their probability of survival and reproduction. They suggest a way to think of it in terms of a cost function minimization.	The canonical neural networks perform active inference by the authors listed previously	How canonical neural networks perform active inference
1227690	1487870	20:27	This letter considers a class of biologically plausible cost functions for neural networks using generative models based on partially observable Markov decision processes. The paper's key addition, as I understand it hopefully, is that they bring action more formally into the model.	This paper is reverse engineering neural networks to characterize their cost functions	Bayesian Inferring neural networks 2,022
1491070	2512970	24:51	In the results section, they write an overview of equivalence between neural networks and variational base. A biological agent is formulated here as an autonomous system comprising a network of rate coding neurons. A simple decision rule in a known family of distributions can often be good enough as an approximation.	This work summarizes the formal correspondence between neural networks and variational base	Neural Networks and variational Bayesian inference
2518030	2928160	41:58	In this section, canonical neural networks perform active Coherence. We consider a class of two layer neural networks with recurrent connections in the middle layer. This suggests that any canonical neural network in this class is implicitly performing active inference. What papers of neural networks can we make active models for?	In this section, canonical neural networks perform active Coherence	Inferred Neural Networks: Active Coherence
2929160	3026680	48:49	The paper extended the variational bays meets inference neural network result from the 2,020 paper. Corporate needs you to find the differences between this picture and this picture. It'll be awesome to hear the author describe it more and differently.	Image memes show neural network and implicit performance of active inference	Image Meme
3027020	3408070	50:27	Here we demonstrate the performance of canonical neural networks using may's tasks. The agent comprised the aforementioned neural networks figure four a thus it implication performs active inference by minimizing variational free energy. Now some entity or agent is going to be constructed and numerically simulated.	This paper demonstrates the performance of canonical neural networks using may's tasks	Quantum simulations of neural networks solving May's
3411160	3814420	56:51	The precise relationship between these update rules and the neural activity and plasticity of canonical neural networks has yet to be fully established. Here is a discussion action on Hebbian plasticity, neurotransmitters and glia. Could be potentially useful when casting these theories in terms of generative generative learning.	The accuracies of inferences and decisions depend upon prior beliefs in neural networks	Bayesian Inference in the Brain
3817340	4908290	1:03:37	The Complete Class Theorem ensures that any neural network whose activity and plasticity minimize the same cost function can be cast as performing Bayesian inference. But identifying the implicit generative model that underwrites any canonical neural network is a more delicate problem. They say the class of neural networks we consider can be viewed as a class of reservoir networks.	Identifying the implicit generative model that underwrites any canonical neural network	Identifying the generative model in neural networks

SPEAKER_04:
Hello and welcome everyone.

This is Active Livestream number 51.2.

It's November 9th, 2022.

Welcome to the Active Inference Institute.

We're a participatory online institute that is communicating, learning, and practicing applied active inference.

This is a recorded and an archived livestream, so please provide us feedback so we can improve our work.

All backgrounds and perspectives are welcome, and we'll be following video etiquette for livestreams.

Head over to activeinference.org to learn more about participating in different institute projects.

All right, well, we're in

actin stream number 51.2 we're in our third discussion on the paper canonical neural networks perform active inference from 2022. we had a dot zero video with some background and context and overview and then last week in 51.1 we had a great discussion went over many interesting details of the paper

and related topics so today we're going to jump in cover some empirical details some implications connect some more dots maybe look at some code and thanks again takuya for joining these discussions um

i'm daniel i'm a researcher in california and thought a lot over the last week about what this kind of neural network pomdp synthesis or translation really means

and just want to learn more about what fundamentals or foundational aspects of these different kinds of models enable that synthesis or translation and then again what that means for areas where one or the other kind of model is already in use so thanks again for joining and i'll pass it to you

If you want to say hi or give any other a second introduction.


SPEAKER_00:
Oh, yeah.

I'm Takuya Isomura in RIKEN Brain Science Institute, Japan.

So I look forward to discuss another different aspect of this work.


SPEAKER_04:
So yeah.

Well, let's just remind ourselves of the fundamental parallel being made in the paper, and then we'll get to these two questions about kind of the two directions that things can go.

One representation is in equation one.

with a loss function of a neural network and the free energy on a POMDP.

And that's also seen visually in figure one with a neural network being drawn a concordance against the variational base of the action perception loop.

So maybe just let's begin by restating what is this parallel

that is in equation one and figure one.

And how was it reached in this paper?


SPEAKER_00:
So,

Basically, the idea here is that we would like to characterize the dynamics and plasticity of canonical neural network in terms of Bayesian inference because arbitrary dynamics of neural network is intractable in the sense that we don't know

what is the function underlying such a dynamics and what is the consequence of the self-organization or plasticity.

So once we translate the dynamics in terms of Bayesian inference, we can assign

quantities in Bayesian inference for any biological quantities, which enables us to lend the explainability to the neural network dynamics and architecture.

So that's a basic idea.

And what we have done in this paper is that we

consider a biologically plausible cost function for this particular canonical neural network and show the equivalence between that cost function and the variation of free energy under a particular partially observable normal composition process model.


SPEAKER_03:
So, yeah.


SPEAKER_04:
Awesome.

So let's look at the parallel between the cost function for neural networks and the variational free energy.

So one representation of that was in figure three.

So maybe could you just describe what is the structure of the variational free energy expression and what is the structure of the loss function?


SPEAKER_00:
Okay, so there is a clear parallel between the functional structure or those component in variational free energy and the component in neural network cost function.

So let's say the first time in F correspond to the, yeah, it's correspond to the

the expectation about hidden state.

S is hidden state posterior.

So that part basically indicates the free energy with respect to the hidden state.

Yeah.

And the second part

correspond to the free energy but the decision posterior so here delta indicates the posterior belief about agent decision or action and now

in terms of the correspondence between the free energy and neural network function here the first time in the neural network cost function correspond to middle layer neural activity which has the recurrent connection and receive sensory input from sensory layer and then project the

the output to the output layer.

And the second term corresponds to the output layer, which receives signals from the middle layer and sends the feedback response to the environment.


SPEAKER_04:
So both of these expressions have the first term being more like a cognitive, perceptual, sensory learning term.

And the second term is more like a control theoretic action selection.

And how did you see this analogy or concordance?

Because it looks like a zipper, like everything is totally lined up.


SPEAKER_00:
Well, so this figure, this graph itself showed a clear correspondence because so now

we are considering a particular form of POMDP in which each element of hidden state takes either 0 or 1.

But there are many states, so it is expressed in the form of, you know,

factorization.

So now we consider that in terms of the S's posterior, bold S, upper part of bold S correspond to the expectation about each element of S taking one.

and the lower part of the bold S correspond to the expectation about S taking zero.

So it is a block vector about the expectation, posterior expectation.

And this S nicely

correspond to the block vector shown in the bottom of this figure.

It is a vector of x and bar x. And here, bar x indicates 1 minus x in the element-wise sense.

which exactly corresponds to the block vector of S expectation.

This correspondence also observed in the second

term here log s correspond to log x and also log a correspond to the block matrix of w log w here

W hat indicates the sigmoidal function of W, and its bar means 1 minus sigmoidal function of W. So actually, because we now consider binary hidden state and binary observation, it's likelihood mapping

So likelihood mapping from hidden state to observation is expressed as a block matrix, which exactly corresponds to the block matrix shown in the bottom of this figure.

So like this, for every term, we have the exact correspondence between the upper expression and the lower expression.

So that's why we can say that this is a natural mapping from neural network formation to variational region formation.

So it speaks as a sort of identity between those two different expressions.

So although one may be able to consider another mapping from neural network to Bayesian inference, this is a sort of the simplest mapping.


SPEAKER_04:
So how would it look different if it were three states categorical distribution or a continuous distribution?

What aspects would change?


SPEAKER_00:
Thank you for asking that.

So that's, in some sense, outside of this paper, because only when we consider a binary state, binary hidden state, this analogy is established nicely.

Otherwise,

we need to consider some extension.

So because consider that each neuron encode the probability or expectation of some value taking 1.

Then the probability or expectation

of taking 0 can be simply computed by computing 1 minus neural activity.

So actually, neural activity, which is a single-dimensional variable, is sufficient to express the expectation.

But once we consider the three-state hidden state program,

So this doesn't work, so we need to consider at least two variables, but its relation to neural network expression is not very clear in general.


SPEAKER_04:
that's very interesting why it would be so strong of a concordance in a binary case but immediately unclear for other distributions yeah


SPEAKER_00:
Generally, for POMDP expression, we consider the one-hot expression, one-hot vector expression, which means that we normalize the value in the sense that the summation or variable to be one.

Maybe there is some neural substrates that achieve that normalization, but for a classic type of neural network, like canonical neural network considered in this paper,

What is that neuronal substrate is not really clear.

That's why we selected the binary case, because it's simple and has a clear analogy.


SPEAKER_04:
So what does it mean for a artificial or for a biological neuron to have activity dynamics or plasticity context that justifies it being described as playing like a belief role in a Bayesian setting?

Higher firing means more belief, higher firing means lower belief.

What does it really mean to have a connection between activity dynamics and belief states?


SPEAKER_00:
I see.

So if you assign a particular mapping, then it's meaning.

is also determined.

In this case, we assign that neural activity correspond to the posterior expectation about an element of hidden state taking one.

So once we define this mapping, then higher neural activity indicate the higher probability of taking one.


SPEAKER_04:
So neural activity is on the x-axis, and the y-axis of the sigmoid function is the probability of taking one.


SPEAKER_00:
Well, neural activity encodes the expectation.

So neural activity is the sigmoidal function of something itself.

So this is because once we see a fixed point of neural activity equation, which is derived from this cos function, it has a form of sigmoidal function.

So x equals sigmoidal function of blah, blah, blah.

So this form is exactly correspond to a softmax

operation softmax function of something which is seen in the solution of posterior expectation that is so that's what the neural activity encodes and what is the Bayesian interpretation or the update rules on the plasticity

OK, that's another important point.

So in terms of posterior parameters, so in the case of Bayesian inference, we consider the update about the Dirichlet parameter of A matrix and B matrix, which is usually expressed by the small case variable.

And its meaning is that if we compute the partial derivative of f with respect to small a, then its fixed point solution looks like

Outer product of SNO, which is also known as Hebbian product, because it has analogy to update rule depending on the pre-synaptic neuron activity and post-synaptic neuron activity.

And according to this formal equivalence, we revisit, we can see again such an analogy in a formal sense here.

if we compute the partial derivative of L neural network cost function with respect to W, then we get a

then we can formally derive the Hebbian plasticity which depends on the activity of pre- and post-synaptic neural activity.


SPEAKER_04:
okay so hebion plasticity often described as neurons that fire together wired together here you're discussing it in terms of a matrix operation on the pomdp side between observables and hidden states so there's a hebion plasticity happening between the perceptual layer

and the cognitive layer.

Right.

So the first half of the neural network is trained according to Hebbian plasticity rules that optimize the A in terms of the perceptual and learning-like relationship between hidden states and observables.

then the second half of the neural network has a slightly different structure it is optimizing based upon retroactive reanalysis of consequences of action according to the fictive causality construction


SPEAKER_00:
So actually, in this figure B, upper layer correspond to environment, and the lower part correspond to agent.

So this structure correspond to figure A's.

This correspond to a simpler version of POMDP.

So for full version of POMDP,

Its corresponding neural network is showing figure four of this paper.

This is the neural network architecture.

All right.

So as you said, there is a network connection from sensory layer to cognitive layer, which is expressed by W here, and recurrent connection, which correspond to state transition matrix,

is expressed by K matrix which is recurrent synaptic connectivity and as you say the action generation through retrospective reward or risk evaluation is done by output layer through the

synaptic connectivity expressed as V in this figure.


SPEAKER_04:
So V is the synaptic connectivity between cognitive states in the middle layer and the action selection states in Y. And so in that way, V is exactly analogous to W. But why and how does gamma come into play only in this second layer?

I mean, why not have gamma one in the first layer, gamma two in the second layer?


SPEAKER_00:
Generally speaking, it is possible to modulate plasticity in the first layer using another modulator, gamma.

But for simplicity, we focus only on neural modulation in the output layer.

Analogy is that, for example, as you said, the first layer computes more perceptual things, so perception of an external world, and instead, on the other hand, the second layer, which is

mapping from cognitive layer to action layer

perform the optimization of its own action.

For example, in the story atom, in the brain, action selection is optimized by modulation of dopaminergic input.

Usually, that circuit receives signals from a cortical neural circuit and sends signals to another neural nucleus in

meet a brain.

But a point here is that neuron in striatum encode some decision, for example, goal or no goal.

So such a decision is encoded.

So now we consider analogy between POMDP expression in the Bayesian formation and the neural circuit in the brain that optimize action through some sort of modulation by another factor here that factor correspond to gamma

And the role of gamma, gamma has a variety of functions.

But in this paper, we focus only on the modulation of plasticity.

So here, the Hebbian plasticity is not determined by only a preposterous relationship, but determined by

It's a three-factor relationship in the sense that the plasticity is updated by the product of gamma and pre- and post-synaptic activity.

So there are three times in one plasticity group.

This is why this gamma can modulate plasticity.


SPEAKER_04:
So how would a glial factor look different computationally?

And where in the brain have people identified glial or other factors as relevant for learning?


SPEAKER_00:
Yeah, that's interesting point.

I'm not fully sure about the equation of the glial modulation of neural activity or

plasticity, there are many discussion and I don't, I'm sorry, I don't know the exact form, but one possible implementation is similar to this type of neuromodulation.

So it would be possible to

model some glial modulation, contribution of glial factor to plasticity in the form of three-factor learning rule, which is mathematically speaking the same as this type of neuromodulation.


SPEAKER_04:
here in table two we have another set of correspondences it's like a sideways figure three right but a little bit more like a dictionary anything to add or any variables that we haven't really mentioned

What about the firing thresholds?

Because these are common parameters in a neural model.

However, we don't really hear about the interpretation of these constructs within the variational Bayes POMDP.


SPEAKER_00:
Yeah, there is an interesting story.

Yeah, that's a very interesting point.

So when we first tried to make made an analogy between neural network on the home DP program, one program is

the law of threshold factor, because as you said, it is not observed in POMDP structure.

But there is another factor in POMDP, which is prior expectation about hidden state, which is usually expressed by D matrix.

And we

But what we consider is the relationship between the matrix and the firing threshold.

Finally, what we found is that spiring threshold is not equal to D matrix itself, but it is a summation of D matrix and some function of synaptic strength, which is equal to A matrix, B matrix in the POMDP formation.

In other words, what we found is that H, which is the firing threshold in neural network architecture, is actually an adaptive threshold, which is not

fixed value but h is a function of w synaptic strength and h changes depending on

For example, if W is too large, then neural activity can be unstable.

So we have to reduce the activity to make neural activity more stable.

So we can see an analogy of homeostatic mechanism here.

So like this, if we design H as the function of W and the function of some another factor, which is all perturbation term in this,

table, then we could make a formal analogy between this H and some variable in POMDP formulation, which is shown in the right-hand side of this table.

table, although its value is not simple because it has three terms, three different terms.

So all of them contribute to make H or M

But anyway, once we establish a mapping between H and this value, then everything works.

So the cost function in different settings have a homer correspondence.


SPEAKER_04:
What are the H and the M firing thresholds?


SPEAKER_00:
So H corresponds to middle-layer RM threshold, and M indicates output-layer threshold, which are different variables.

And interestingly, H corresponds to prior expectation but hidden state, because it corresponds to cognitive layer, and M corresponds to


SPEAKER_04:
prior belief about its own action because it is a it is a bias in the action layer yeah it's very interesting that the perceptual firing threshold h only includes prior beliefs on hidden states

beliefs about how observations map to hidden states A and beliefs about how hidden states change through time B. So that's like pure passive inference.

And then the firing thresholds for M correspond to only beliefs about preferences and beliefs about actions or habits with C and E. So there's like a complete division of labor

or partitioning functionally between these structurally different parts of the neural network and structurally different and functionally different parts of the pomdp yet they're integrated in unified loss functions or unified imperatives

so it's like there's extreme separability of perception and action on both sides of the figure one divide but also they're integrated but they're separate and that's what kind of grants it the best of both worlds because if they were any more integrated you couldn't really pull them apart

if they were any less integrated then the imperative the loss function or the variational free energy would be ad hoc and unprincipled but there's kind of a middle ground where they have a principled integration but still a distinguishment right this is caused by you know network structure defined uh


SPEAKER_00:
or it is because the structure of Bayesian network defined in POMDP.

model.

Both of them define the causal relationship between elements or quantities.

Its substrate is

not important.

So its relationship, causal relationship is crucial to determine the cost function or its fixed point in this context.

So that's why we can see that analogy.


SPEAKER_04:
Well, there's a few technical points I think we can now go into, and then there will be some more general points about applications and intelligence.

So first, the code availability statement.

Awesome to see that the MATLAB scripts are available.

and also archived on Zenodo.

So here is the GitHub repo for reverse engineering.

Do you want to give any overview descriptions of what people can expect to see in this repo?

And also what about using MATLAB?

Why did you use MATLAB?

What advantages or limitations do you see in MATLAB?


SPEAKER_00:
So because this is a very simple simulation, so MATLAB is sufficient to encode the whole script.

And so we also try some visualization in the MATLAB.

See here, if you run the script, then

you can see the process of an agent solving the maze task.


SPEAKER_04:
What do they do in the maze task?


SPEAKER_00:
So here, the aim of this agent

is to reach the right-hand side of this maze.

And so because this is a typical example of a delayed modulation task, that's why we select the maze task.

So to achieve this maze task, agent required to make some plan to

be able to select a good decision.

Because without planning, you may encounter the wall and cannot go further and you may fail the maze.

But with planning, it is possible to see the path to reach the right-hand side of this space.


SPEAKER_04:
So does it know its x position?


SPEAKER_00:
Yeah, it received a state from neighboring 11 times 11 cells, which is shown in the bottom part.

Yeah, this all most left figure indicate the observation, so 11 times 11 state.

around the agent position.

Now agent is on the right-hand side of the maze at the goal position, and it observes a neighboring state.


SPEAKER_04:
Well, a few interesting things.

Here, it's looking off the right end.


SPEAKER_00:
Yeah.


SPEAKER_04:
And it has this kind of periodic belief in the key distribution.

Why?


SPEAKER_00:
I think it is because when the agent is in the middle point of maze, then

All neighboring state is in the maze.

So there is a path, and there is a wall.

So this makes some periodicity, because maze have some structures.

And actually, it have a periodic structure.

And only at the goal position, then right-hand side becomes a wall.

But it is not common for this agent.

This is because this agent shows such periodic patterns.


SPEAKER_04:
yeah the streets are one wide and they tend to be separated by one so we see this periodicity what is um the numbers in this middle bottom plot and what does the checkerboard represent


SPEAKER_00:
Yeah.

Here, Qs and Qd correspond to possible expectation about the state and the decision.

So middle panel indicate decision posterior.

And decision here, we characterize decision as a sequence of four-step

So each action corresponds to a movement to right or left or up or down.

And we consider a four-step sequence of that action, which is expressed as D. So it has four power of four possibilities. 256.

Yeah, 256.

So this is plotted on a x-y coordinate.

Because in the middle panel, the middle point corresponds to the current position of agent.

And with the four-step movement, agent can go one of any colored position.

And the color brightness correspond to the expectation about the agent decision.


SPEAKER_04:
Well, this is very interesting.

If we just were to think about you're in a point, and you can go up, down, left, right, you have four moves.

Naively, it sounds like, well, it should look like a Gaussian blur.

Most of those should cancel out and then it should become rarer and rarer monotonically.

But actually you start in the middle, you can't end up on these white squares because it's like one, two, three, and then you have to leave.

So it's kind of like horses in chess or other pieces where actually their embodiment, like it's very unexpected that you can't in four moves, end up next to where you began when you can be so much further.

And then we see this kind of like embodied inferential prior with QS.

that embodies regularity beliefs about the width of the road and the separation of the roads.

And then there's these like embodied action priors and real consequences that have to do with the structure of movement.

So what it's doing, it's thinking,

about policies of length four.

There's 256 policies of length four.

There's some degeneracy because there's obviously not 256 squares here.

So while only one policy is going to take you up, up, up, up, up, down, down, down, down, down, other squares are reachable.

Like the center square is probably the mode because it can be reached at least a handful of ways.

and then at each time point it's basically saying okay I know where my X position is and given my local 11 by 11 view I'm trying to plan to go right oh

then here through time in the simulation here it starts at 30 something it quickly figures out how to get to about 40. and then it's kind of going up and down on 40 but it can't really break out because all of these bottom four routes are closed it has a breakout and then very quickly it hits another plateau around 60.

right then it kind of has a very nice breakout and in just a few steps goes very far so what is dopamine doing or how is dopamine helping it in the Plateau and then to break out of the Plateau


SPEAKER_00:
Yes, so this agent runs this particular maze structure

through many trials so before training it failed to reach the goal but after training it achieved such a such a nice behavior so to achieve this the role of dopamine is that we design a gamma function such that if

the agent can move rightward with some distance during some time limit, then risk becomes small.

So like, say, gamma equals 0, so no risk situation.

In that sense, in that case, this agent updates synaptic weights through Hebbian plasticity.

But if the agent fails to go rightward with some distance during a limited time frame, then gamma becomes large, like 0.6, which is larger than the average 0.5.

design that drawing in such situation

anti-Hebbian plasticity occurred instead of Hebbian plasticity.

So anti-Hebbian plasticity indicate, so works as the disassociation between the current state and current decisions, because the current decision doesn't work.

It's not good decision.

So we try to make

the agent forget that particular decision rules through the modulation of heavy and plasticity done by gamma factor.

So this can be analog to the dopaminergic modulation of heavy and plasticity.


SPEAKER_04:
so if the policy is resulting in the expected outcome gamma stays at 0.5 the policy is as risky or consequential as expected and then the policy can either go better than expected

which facilitates learning to support that decision to be made more, or the outcome of the policy can be worse than expected, which disassociates previous conceptions to discourage that kind of behavior.


SPEAKER_00:
Exactly.

A crucial point is that this association is done with a different time frame in the sense that we consider multiplication of current risk and past decisions to average

over past to present Hebbian product.

This makes an association between past decision sequence and the current risk, which enables to optimize the decision to minimize the future risk.

It is just a shift of timeframe.


SPEAKER_04:
So here, risk is being used in

a formal sense similar to how it's used in economics which is the associated uncertainty of outcomes with respect to a policy where does danger come into play like what if there was a adversary in the maze or something that was dangerous how does this kind of model accommodate or hunger

or different kinds of competitions because right now it's basically just trying to diffuse right word with a bias right but how do different kind of um situational elements become integrated into the generative model and generative process okay


SPEAKER_00:
Any of those factors can be involved in a risk factor, a single risk factor.

So you can actually design risk factor because risk factor modulates generative model.

So that's why agent try to minimize the risk through Bayesian brief updating.

But the risk itself is in some sense outside of such a Bayesian framework.

So we can design arbitrary risk.

So it may involve some danger


SPEAKER_04:
factor any any any other factor and this simulation it is a pomdp or it is a neural network and what scripts might we look at to understand the structure of the maze agent


SPEAKER_00:
OK, it is basically expressed using the quantity in HomeDP for tractability.

But for example, if you see the MDP, well, this is made as MDP learning, probably.

okay uh there is a variable sim type uh on the in the definition of yes same type correspond to the type of simulation so if it's one or two it becomes a form dp or neural network to my understanding well

In this particular example, we use the, let's say,

Oh, maybe it's not a good example.

The difference is unclear in there.

Yeah, well, there's no difference in this script as well.

So maybe another example.

Let's see.


SPEAKER_04:
What is MDP init is initiating the markup decision process?


SPEAKER_00:
Exactly.

It's just determining the initial state of the home DB simulation.

And if we compute the variational free energy or risk, MDP risk, compute the risk function.

So basically, we use the neural network

structure computation in this particular setup.

So when you click maze.m, then in the line 13.1,

line 31, we determined that the same type is 2.

This corresponds to neural network architecture.

So there is a very slight difference between home DP architecture and neural network architecture, because assuming neural network architecture corresponds to considering

If you choose the POMDP architecture, then

we sometimes use the D-gamma function to compute the posterior expectation about parameters.

But in the neural network modeling, the D-gamma function doesn't appear, but it is replaced with the logarithm of some function.

And asymptotically speaking, the difference between the gamma function of something and the logarithm function of something is asymptotically the same.

So that's why we can transform HomeDP to neural network architecture when the number of samples is sufficiently large.


SPEAKER_04:
Which form do you expect performs better under small or large amounts of data?


SPEAKER_00:
Well, for large amounts of data, they work in the same manner.

For small amounts of samples, I'm not

I'm not fully sure, but it corresponds to the assumption about the posterior belief distribution.

So if you assume Dirichlet distribution, then your resulting functional form is something that uses the D-gamma function.


SPEAKER_04:
in terms of bayesian inference probably which is optimal well probably all right let's return to the earliest questions from today so um in your script which people can reference there's basically a toggle

between having it in SIM type one or SIM type two, corresponding to the POMDP and the neural network.

What about if there's a published neural network or POMDP?

How can we use this architecture to create a translation

Is there any difference in this kind of like translating models in the wild?

Different than the full construction of a special script that can speak both languages.


SPEAKER_00:
Well, in terms of script, there's no difference, asymptotic difference, right?

So they work in the same manner.

So only translation of variable is so you can

you can see the same source code in two different ways.

So if you see that this is a neural network simulation, then it is translated as a neural network.

Or if you see that this is a POMDP, then it's POMDP.


SPEAKER_04:
so for some neural network being used in an industrial setting how would we get from the neural network to a pomdp and where would where or how would that representation be valuable


SPEAKER_00:
Right.

The important point is that we consider a particular form of neural network, which is called canonical neural network architecture.

So only when we assume this class of neural network, then you can find the exact correspondence to a particular form of POMDP.

Otherwise, you need to establish another equivalence between another form of neural network architecture and some sort of Bayesian model.

This may be expressed by POMDP, but maybe not so straightforward to be expressed as the conventional POMDP architecture.


SPEAKER_04:
So what is it about the canonical neural network architecture that facilitates its translation into the POMDP form?


SPEAKER_00:
Yeah, first of all, it assumes a sigmoidal activation function.

It is nicely correspond to entropy term in the POMDP equations, POMDP formulations.

So that's why we can create clear mapping.

So yeah, in other words,

Simply speaking, they have the same nonlinearity.

That's why this translation is very easy.

with another non-linearity or a neural network equation, then we need to find another type of entropy equation or another type of prior distribution, which is very non-trivial.


SPEAKER_04:
How does one even go about doing that research?


SPEAKER_00:
Well, if you want to go that direction, then I think the first step is to find the prior brief, which makes... Find the prior brief and

find the equivalence between a particular neural network architecture and a particular Bayesian model.


SPEAKER_04:
This sigmoidal activation is interesting.

It corresponds to

general patterns seen in psychophysics like two objects that are the same weight you're gonna have a 50 chance of saying that one is heavier and then initially the difference has the most um returns on that decision being made accurately yeah

and then as it crosses some threshold where it just is the beyond a noticeable difference the decision becomes essentially probabilistic like the firing curve becomes saturated the neuron is at the it has a very low belief about zero or very high belief about zero or one flip that um

so there there is a nice grounding of that kind of a sigmoidal response curve with respect to stimuli differences and it has of course tractable analytical properties but it also just happens to be a good response summarizer


SPEAKER_00:
Yeah, you're right.

So sigmoidal function is also known as a psychometric function.

As you say, we observe that characteristic in many psychophysical experiments.

And the previous work also said that even at the single neuronal level, the same behavior

were observed, which means that for each neural activity, we can re-observe the similar property, which is sometimes called as neurometric function, which have the form of sigmoidal activation function.

So it is a nice reason to design neural network architecture using a sigmoidal function.


SPEAKER_04:
All right, let's cover a few questions in the chat from Dave, and then in the end, turn to some general thoughts.

Okay, this was when we were looking at figure three.

So you described these vectors or matrices.

What kind of matrix or vector did you describe them as?

Block matrix?


SPEAKER_00:
Block matrix.

Block?

Block.

Meaning what?

Okay.

Block matrix or block vector is a vector of vector or matrix of matrix.

So imagine that


SPEAKER_04:
Sorry, just zoom.

Just glitch.

Just repeat the last piece.


SPEAKER_00:
Okay.

Unstable.

So, well, block matrix means that so element of matrix is a matrix.

So, let's say 2 by 2 matrix like matrix in the yeah.

you're pointing.

So this here, W1 hat is a matrix.

And W0 hat is another matrix.

And combining four matrices, we define a single block matrix.


SPEAKER_04:
All right.

Thank you.

So Dave then asks,

The hosts of Machine Learning Street Talk, number 67, with Carl Fristen, another podcast where Fristen has spoken, pressed Carl Fristen on why is it so important that most of the values in a generative model matrix assume values of exactly zero?

Why is it important that generative model matrices are sparse?


SPEAKER_00:
I'm not fully sure.

I think there is some context before that point.

I think under a particular situation, then, yeah, as you say, the many elements in the matrix of Genting model should be zero, but I'm not sure if it's a general statement or not.


SPEAKER_04:
What do you think about compressed analyses on sparse matrices?

Is that a useful technique or direction?


SPEAKER_00:
you can use that knowledge to construct a model.

So you can use that knowledge to make more accurate inference.

So in that sense, generally speaking, such assumption should be useful.

For example, as you said, it would be possible to use some

sparse prior to restrict the value of parameter.

It is, in principle, same as assuming some L1 norm.

to design the distribution, to design the prior distribution, which is, mathematically speaking, exactly the same as considering Lasso regression.

Yes.


SPEAKER_04:
We've explored a little bit how from a canonical neural network to a particular form of a POMDP gives us some semantics and interpretability around the dynamics and plasticity of the neural network.

What do we gain by taking a stated POMDP generative model

and deriving an analogous neural network?

Do we gain access to efficient computation, new software packages, different applications?


SPEAKER_00:
Well, if one use POMDP and

one's goal is to design an efficient Bayesian model, then I think POMDP expression is sufficient.

So you don't need to consider neural network architecture, probably, because Bayesian inference or POMDP

architecture and the Bayesian formulation is designed to achieve some sort of mathematically optimal inference and decision making.

So it itself is an optimal scheme.

But if one needs to consider a link between

Bayesian inference and biological substrate, then this mapping is crucial.

So simply speaking, we assume that a brain performs Bayesian inference, but its substrate is still unclear.

So we need to link the Bayesian quantity to biological quantities.

So this mapping, this equivalence, helps us to its translation.

So when you start from HomeDP model, then this translation facilitates the process of finding its neuronal substrate.

So once you translate that to neural network quantities, then it helps us, it facilitates

experimental validation or application to real data.

So if its modeling is apt for a particular neural network, neural circuit architecture, then it should provide some prediction about the architecture or dynamics

empirical data, right?

So first we start from Bayesian model, which is not necessary to be equal to empirical data.

So maybe there is some mapping, but its mapping is not straightforward.

We may have multiple mappings, but once you translate Bayesian model to a particular neural network architecture, then

mapping or relationship between empirical data to such a particular neural network model is straightforward.

So it helps us to apply Bayes to an explanation of empirical data.

That's my thought.


SPEAKER_04:
is it fair to say that neural networks have found wide recent application because they facilitate statistical learning in cases where the inference problem has not been a priori well specified one can just have a folder of images and a list of labels and just say here's the data

run it through this architecture or this architecture explorer and so with this concordance we gain new interpretability into those settings that kind of arose from ill-specified inference problems

And then on the other hand, for problems that we already have well specified in terms of a POMDP generative model of a particular form, we gain the connection to actually implement it with empirical data and bring it into relevant industrial settings.

what systems or phenomena are promising to continue research on the maze example obviously is a simple case but are you continuing research into more advanced computational agents robotic animal oh hello


SPEAKER_00:
Yeah, one can design a more sophisticated agent which performs some difficult task based on a canonical neural network.

But there is some limitation, clear limitation,

on that direction, right?

So yeah, I should emphasize that a class of canonical neural network which correspond to a particular POMDP is much smaller than a general POMDP framework.

So there are some limitations, a list of limitations.

So if one's goal is to design a sophisticated Bayesian engine to perform some task or control robot, then one direction is just to forget such a limitation.

seek the mathematical optimality.

And another direction is biological plausibility.

So if one wants to make some agent which is biologically plausible, then this correspondence is crucial because it tells us

limitation, the biological limitation, through the existence or non-existence of such a mapping between POMDP and particular neural network architecture.

So it would be useful to consider a biological substrate to achieve a difficult task.

And that task would be related to large-dimensional image processing, image recognition, or sound recognition.

Such a multi-modality can be involved, or decision can be

higher dimensional.

In the major task, we just consider the four direction of movement, but it can be extended to higher dimensionality, like arm movement, body movement, so on and so on.

So in principle, it can be extended in that direction.


SPEAKER_04:
Which directions or questions are you excited about?

Or what areas of studying the basis of biological and computational intelligence are relevant?


SPEAKER_00:
Yes, so in terms of

The importance of canonical network, as you said, its virtue is a vertical plausibility.

So it would be nice that if we model some task which is conducted by real animal, and one already recorded some neural activity, brain activity,

Then we design a task which is exactly same as the task which is done by the animal, and then compare the simulated agent and MP-Curl data to discuss about the similarity or difference between the simulated agent and real.

animal, that would be very interesting direction of research.


SPEAKER_04:
yeah and if there could be some unexpected prediction or explanation right in the computational agent that would bolster the relationship and then um one other aspect is it would help with the reproducibility and the

documentation around behavioral studies if the computational agents were pre-registered and someone said we've already done the statistical power analyses and we've already explored with parameter sweeps

How many observations we need to make of the two-armed bandit?

How many observations of the three-armed bandit?

Should we do three mice a hundred times or a hundred mice three times?

Those are the total substance of designing research programs.

And so having a formal representation

of behavioral tasks that are being studied will help us design behavioral observations and experiments that aren't simply ad hoc.


SPEAKER_00:
All right.

That's an interesting application.

So yeah, this framework helps to design the experimental setup itself.

And what we often consider is the prediction ability of this modeling canonical neural networks to predict

the scenario harmonization or dynamics

or the real neural network in the animal during the learning process.

So in principle, it is possible to predict the behavior after learning based only on data in the initial stage.

Because once we obtain some empirical data, then we can feed that data to design a particular canonical network.

And canonical neural network makes some self-organization through a minimization of cost function, which is exactly the same as Bayesian belief updating under a particular generative model.

which means that its dynamics go through the shortest path on the free energy landscape, which means that we can make some quantitative prediction about the synaptic trajectory or neural activity or any kind of parameters.

So we demonstrated that using in vitro neural network and uploaded some pre-print recently.

So at least at the level of in vitro network, which is much simpler than VR brain,

we could predict the self-organization of in vitro network using this canonical network architecture.

This supports the plausibility of the free energy principle because

this canonical network predicts the self-organization through the variational free energy minimization and its solution, its result, has a tight correlation to actual self-organization.


SPEAKER_04:
That's a very interesting experiment.

So what

animal were the neurons from and what was measured about these neurons?


SPEAKER_00:
Yes, so that in vitro network is obtained from a rat embryo

we use cortical cells to make that in vitro network.

And task is a sort of causal inference task, which can be designed in the form of HomeDP.

So imagine that.

So we usually simulate agents that receive

signals generated by HomeDP generative process and process some Bayesian task.

So we just replace that Bayesian agent to a real in vitro neural network.

So we stimulate neuron with some signal which is made by some hidden sources through likelihood mapping.

The question is whether the in vitro network can infer the hidden states through some self-organization.

And they can.

They could infer the hidden causes.


SPEAKER_04:
What does it look like functionally when the neural network has succeeded at inferring the hidden causes?


SPEAKER_00:
Yeah, the direct observation is done by the number of response spikes to a particular pattern of sensory input.

So again, we can see a clear correspondence between

neural activity level and posterior belief about hidden state.

So here we see an early evoked response to an electrical stimuli.

We see the response from 10 to 30 milliseconds after each stimulation, and we compute the number of spikes.

And that spike changes their preference in the sense that some neurons learn to preferentially respond to source 1 but not source 2, which is not a response to input itself, but it looks like a response to a particular source.

So it is inference, which is empirical evidence that neural network actually perform some sort of causal inference in a manner consistent with variational Bayesian inference.

And then we compute another quantity in Bayesian inference.

In the real vertical data, we show that

firing threshold factor is consistent with the prior belief about hidden state.

And we also compute the synaptic weight statistically through some connection strength estimation method and show that

So estimated synaptic strength is consistent with something encoding posterior belief about parameters as expected by the theory.


SPEAKER_04:
Well, we looked at Table 2 earlier, and this is almost like the next step after the theoretical concordance.

is all right, well, let's measure the release of a neurotransmitter or the empirical synaptic strength or the firing threshold or all these different features in different empirical systems.

So what systems, what experimental systems does your group work in?


SPEAKER_00:
That in vitro system was made when I was a PhD student.

So that is the experiment we done in my previous lab.

And now I move

to the RIKEN Institute.

And I'm a principal investigator of a salary unit.

So now, actually, we don't use any experimental setup.

So any experimental validation is done with some collaboration.

So although I cannot say a detail about that collaboration, but yeah, we learned some.

a collaborating work about the validation of celery using uh various animals yeah so we hope we can show some interesting result following results using animal data very um interesting yeah well it speaks a lot to the stage


SPEAKER_04:
that our field is in in certain ways where we've seen a lot of graphics that are suggestive this paper and the building on the previous 2020 paper made a suggestive possibility

much closer to a analytically demonstrated translation and then took the next step incrementally into the in silica agent and so it's only natural to then explore different embodied systems as well um

there any other sections that you wanted to like look at or highlight or any other topics about the paper or adjacencies or active inference that you think are interesting to go into yeah


SPEAKER_00:
So OK, then I would like to mention about some limitation of this paper, which is not directly discussed in the papers.

So for example, well, we focus on a discrete state space model.

So we avoid to assume some substrates that encode

the covariance of the distribution.

So once you assume POMDP, then it is categorical distribution.

So it is different from assuming Gaussian distribution characterized by mean and variance.

So the neuronal substrate of variance

is still unclear.

And we now try to figure out that.

So this is one direction of limitation.

And another limitation is that, thanks to a simple ODP structure, in this paper, we don't care about the hierarchical

you know, optimization.

But generally, it is crucial to update parameters through hierarchical optimization through some backpropagation-like computation.

Although it is unclear whether backpropagation itself occurs in the real brain, but we still have some alternatives that achieve such a

optimization, and its neural substrate is still unclear.

And this paper does address that direction.


SPEAKER_01:
Hmm.

Hmm.


SPEAKER_04:
Another area I'm wondering about is like, where in neural structures is the learning reflected?

Where is the function and learning reflected?

Well, sometimes it has to do with not just structural tweaking, but the presence or absence of synapses.

So obviously this model does not expand into synaptogenesis, synaptic pruning, let alone neurogenesis and neuroapoptosis, which we mentioned in the previous discussion, but understanding how these larger scale structural changes

which are certainly important in biological systems, become reflected in artificial neural networks, and then how that translates all the way back to POMDP, and then whether we could go the other way, what kinds of POMDP structures in their neural network realization would have structural modification?

You can imagine a POMDP structure

that does structure learning but the neural network doesn't have structural change or there's a pomdp that doesn't do structural learning but it's met it's manifested by a neural network that does have a structural change element

so structure is doing something very different in these two different categories of model and then also um even within neural firing which is different amongst different species and so on there's different aspects of what that firing is

that would have different implications for the actual biological substrate of cognition.

Like the simple connection is firing rate to posterior belief.

Average firing rate, no change in posterior.

Reduce the firing rate if the posterior should be going down, increase it if it should be going up.

Or maybe there are neurons that have a flipped valence, but the same type of relationship.

But there's other...

firing patterns like spike time dependent plasticity synchronization amongst different brain regions there's a lot of things that don't change the rate overall that again from the biological systems we know that those phenomena and mechanism are important for different cognitive processes

so there will be many many years of a fruitful relationship um i'm gonna bring in this picture that that um alexandra had taken maybe we need a third panel in figure one because these three systems moving between them

is going to be the substance of the field for a long time and there's maybe other edges to build but understanding how artificial neural networks intermediate between the empirical measurements and manipulations that we can make of real neural systems and the

interpretability and factorize ability of POMDPs, it might be a bridge too far to go from the POMDP to the neuron.

You could always use this technique, but it would be a purely descriptive statistic type approach.

But it's so interesting that by intermediating through a formal connection established in figure one, I mean in equation one, but also shown here, then we can kind of extend the chain of explanation, prediction, control, design,

all the way on through and that just unlocks like an incredible amount of neuroscience that hasn't been formalized mathematically and an incredible amount of generative models that have been specified for different learning settings sometimes even by analogy to biological settings

But the metaphor remains just a metaphor until it's possible to intermediate with this type of neural network development.


SPEAKER_00:
Yeah.

That's a crucial point.

It is easy to imagine that real data or real phenomena can be modeled using very realistic

neuron model or glial model, synaptic model.

We believe that it is possible.

And then real data, sorry, real model is not necessarily tractable, not necessarily useful because it is too much complicated to analyze something.

So we use some reduction usually, mathematically speaking, which correspond to topological transformation to make the model simpler.

And then we need to consider the translation of that simplified neural network model, because neural network model itself is not explainable, which just represents some dynamics.

And its functional meaning is not clear.

But thanks to the Bayesian framework,

a very nice framework to lend the explainability.

And this translation, this correspondence, helps us to link such phenomena-based equation modeling and functional-based equations.


SPEAKER_04:
Yeah, one paper from 2017 that was much discussed by some.

Could a neuroscientist understand a microprocessor?

And this group with Jonas and Cording, they had a simulation of a microprocessor.

from uh earlier video game console I believe and then using the analogy of like the transistors and their connections as neural firing and structural connectivity they were able to simulate experimental settings input and action and then make measurements from like every neuron

including doing lesions and loss of functions and so on.

And it turns out that a lot of the techniques that are used to derive scientific explanation from analogous data collected from a biological system

those techniques which ostensibly should be isolating functional explanations, in fact, did not isolate effective explanations.

You could have a deletion over here that induces some statistical change all the way over here, and that may or may not be a useful clue towards the function of even sub-circuits.

And so I think that was a wake-up call with respect to the interpretability of simply this connection between the biological and the neural network.

This connection alone is of limited applicability

even when the neural network model becomes so complex as to recapitulate the biological phenomena, you're never under any guarantee that you're going to recover interpretability.

You may have just created an atomic level simulation of the phenomena, but of course a map that is the same scale as the phenomena isn't a map.

It's just a copy that has no more interpretability.

And it's almost like what is now extended, again, as we kind of just summarize this and think about how we move forward, is that connection can now be extended into the space of interpretable causal models, right?

and the generalized Bayesian graphical computational frameworks and all the heuristics that we can then use like variational Bayes and all these other methods.

So it'd be interesting to look back at different data sets of in vitro and in vivo and in silico neural activation, especially if the task was of this constrained set of POMDPs

And it was already amenable because as you brought up, other settings would require a little bit more theory development before we understand what POMDP family would be applicable.

Cool.

Well, do you have any final thoughts or questions?


SPEAKER_00:
I don't.

Do you hop?


SPEAKER_04:
I want to download the MATLAB scripts and generate the figures, play around with a few of these parameters.

Like I see that you can change how far the entity can see.

And then with these models, I'm also always curious about the computational complexity.

Like if you extended the planning horizon from four to five or you dropped it down to three.

what is the runtime consequences and what is the performance consequences and where might we be able to use single or swarms of really simple agents

maybe even making binary decisions and achieve high performance.

And where do we really need to move into these large combinatoric spaces in order to solve problems and the kinds of complex planning problems that we solve, whether it's planning our day or planning our week,

Are those more like true deep horizon planning problems with extensive consideration of counterfactuals and calculation of alternatives?

Or are those actually composite decisions that are made up of smaller, simpler sub decisions?

that we may or may not have flexibility to restructure.

So that a decision, a complex chess maneuver, a sacrifice in chess or another game, it may be possible to model that as a deep horizon scan or a kind of intuitive heuristic

for inappropriate skilled entity.


SPEAKER_00:
For for this particular two layer structure, there is a clear limitation about the, you know,

the horizon of the search, forward search, because it doesn't use forward prediction.

It uses a post-diction approach.

So it's a clear limitation.

But still, it may have some performance, ability to achieve some level of performance.


SPEAKER_04:
That actually, I mean, that provides even another way to look at planning.

So the two ways I was describing planning, as it's often described in the PMDP literature, is again, is it a true deep horizon consideration or is it just short-term heuristics or nested models that are short?

And I think that this paper says maybe neither.

Maybe it's purely the fictive causality on the past.


SPEAKER_02:
Hmm.


SPEAKER_04:
that leads to the emergence of sentience and maybe even teleological planning-like behavior through the ongoing reconsideration of the consequences of past action.

But it's neither a short nor a long-term planning challenge.

It's actually like a memory and learning challenge, and no planning occurs.


SPEAKER_00:
Right.

But indirect planning...


SPEAKER_04:
you know planning element is involving c matrix planning as a phenomena occurs and de-risking through time occurs but there um but it says something quite interesting and deep that that phenotype or function could be enacted by a system that

explicitly looks a long way ahead explicitly looks a short way ahead or moves forward and looks backwards only right which is what they sometimes say about the past and the future so that may be a very biologically plausible form of learning

And it's already intimately connected with the dynamics and the plasticity in terms of an integrated loss function.

So these are all excellent directions to keep learning on.


SPEAKER_00:
Right.

And I'm also interested in the vertical implementation of such a short-term or long-term forward prediction and planning.

And I hope to find some nice connectivity to such an implementation of Bayesian model and the implementation in real brain network.


SPEAKER_04:
cool and also i'm i'm always curious about the invertebrate brain as a ant researcher and so many of the micro architectures as well as the regional architectures that people discuss are mammalian centric which makes sense

the mammalian cortical column and the relationship with the dopaminergic midbrain and the cortical regions and the spinal reflex arc those are all important systems of interest yet the micro and meso anatomy of the invertebrate nervous system is pretty distinct

so our model should be able to describe neural and cognitive systems of course across invertebrates and vertebrates so i look forward to also seeing like what those models of the invertebrate nervous system and collective behavior where you could have some type of

backwards looking risk inference of the swarm who knows who knows well we really appreciate the time that you took for these discussions I think they are immensely important and we wish you the best of luck in these continued directions

Okay, that's it.

Thank you.

Thank you very much.

See you next time.

Bye.

Bye.
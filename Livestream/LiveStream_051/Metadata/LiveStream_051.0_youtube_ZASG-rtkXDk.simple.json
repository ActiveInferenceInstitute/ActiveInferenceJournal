[
  {
    "start": 29.23,
    "end": 29.711,
    "text": " All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 31.172,
    "end": 31.512,
    "text": "Welcome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 32.253,
    "end": 35.176,
    "text": "It is ACT-INF live stream number 51.0.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 35.997,
    "end": 44.104,
    "text": "This is the background and context first discussion for Canonical Neural Networks Perform Active Inference by Isamura et al.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 44.765,
    "end": 45.405,
    "text": "It's October 26, 2022.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 45.506,
    "end": 47.207,
    "text": "Welcome to the Active Inference Institute.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 51.411,
    "end": 57.197,
    "text": " We're a participatory online institute that is communicating, learning, and practicing applied active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 57.897,
    "end": 60.04,
    "text": "You can find us at some of the links on the page.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 60.82,
    "end": 62.842,
    "text": "This is a recorded and an archived live stream.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 63.163,
    "end": 65.725,
    "text": "So please provide us with feedback so we can improve our work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 66.686,
    "end": 72.172,
    "text": "All backgrounds and perspectives are welcome and we'll be following video etiquette for solo live streams.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 74.261,
    "end": 81.127,
    "text": " head to ActiveInference.org to learn how to get involved with Active Inference Institute projects.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 83.348,
    "end": 97.62,
    "text": "Today, it's the first of several discussions that we'll have on the paper, Canonical Neural Networks Perform Active Inference 2022 by Takuya Isamura, Hideaki Shimizaki, and Carl Fristen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 98.421,
    "end": 101.543,
    "text": "The video is just an introduction to some of the ideas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 101.603,
    "end": 103.184,
    "text": "It's not a review or a final word.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 104.125,
    "end": 121.976,
    "text": " there will be an overview of the structure of the paper and then we'll go through many of the key points also just to disclaim there's many many other and better resources to learn about neural networks so i would very much welcome those with a technical",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 122.876,
    "end": 141.528,
    "text": " understanding of neural networks and or some of the more applied computational or uh otherwise aspects of neural networks it would be awesome to have them on for the dot one and the dot two because it was not an area i was familiar with and so hope that i can",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 142.429,
    "end": 171.909,
    "text": " hear more from the authors in our coming weeks and others i'm daniel i'm a researcher in california and this will just be a solo dot zero which i guess hasn't happened in a while uh in the making of this here's some of the generated art prompts area 51 active inference area 51 neural network area 51 active inference ants area 51 active inference neural network",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 172.969,
    "end": 175.311,
    "text": " Just some interesting images coming out of stable diffusion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 176.712,
    "end": 188.219,
    "text": "So as to some big questions that the paper is addressing and that one might be interested in to come to the paper, how can artificial neural networks be understood as generic optimization processes?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 189.039,
    "end": 193.682,
    "text": "And what is the correspondence between neural dynamics and modern statistical inference methods?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 195.563,
    "end": 202.891,
    "text": " Other big questions are about the history and next steps of the enmeshment of natural intelligence, e.g.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 202.931,
    "end": 213.462,
    "text": "neuroscience and artificial intelligence, as well as of course, whether to even play into this kind of distinction at all and have different integrated intelligence frameworks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 214.563,
    "end": 234.54,
    "text": " and one paper where any of the authors or anyone who has uh kind of resonated with this work is recent by zador at all 2022 towards next generation artificial intelligence catalyzing the neuro ai revolution and so this is a bunch of authors and so it's interesting just to quote in terms of what",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 235.18,
    "end": 263.146,
    "text": " some areas of discourse are saying right now which is neuroscience has long been an important driver of progress in artificial intelligence ai we propose that to accelerate progress in ai we must invest in fundamental research in neuro ai so that's one way to read some of the developments that are happening in the paper we'll discuss what does it mean to be particular but generic that's a phrase used in the paper so maybe that's kind of a jumping off point",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 264.005,
    "end": 280.15,
    "text": " then how can active inference help us understand the past present and future here of the interface with neural networks statistics and neural ai here's the abstract",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 282.52,
    "end": 295.147,
    "text": " This work considers a class of canonical neural networks comprising rate coding models where neural activity and plasticity minimize a common cost function and plasticity is modulated with a certain delay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 295.967,
    "end": 302.291,
    "text": "We show that such neural networks implicitly perform active inference and learning to minimize the risk associated with future outcomes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 303.031,
    "end": 309.715,
    "text": "Mathematical analyses demonstrate that this biological optimization can be cast as maximization of model evidence",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 310.595,
    "end": 318.958,
    "text": " or equivalently minimization of variational free energy under the well-known form of a partially observed Markov decision process model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 320.219,
    "end": 331.343,
    "text": "This equivalence indicates that the delayed modulation of Hebbian plasticity accompanied with adaptation of firing thresholds is a sufficient neuronal substrate to attain Bayes' optimal inference and control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 332.083,
    "end": 336.805,
    "text": "We corroborated this proposition using numerical analyses of Mayes' tasks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 337.805,
    "end": 350.395,
    "text": " This theory offers a universal characterization of canonical neural networks in terms of Bayesian belief updating and provides insight into the neuronal mechanisms underlying planning and adaptive behavioral control.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 353.457,
    "end": 354.879,
    "text": "Here is the roadmap.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 355.699,
    "end": 357.22,
    "text": "I'll be driving on the right side of the road.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 359.162,
    "end": 363.826,
    "text": "There's an introduction and a table with a glossary of different expressions used.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 365.063,
    "end": 381.652,
    "text": " Then there's the results sections, which has first an overview of equivalence between neural networks and variational Bayes, active inference formulated using a post-diction of past decisions, canonical neural networks perform active inference and numerical simulations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 382.312,
    "end": 392.238,
    "text": "So they work on the interface between neural networks and variational Bayesian methods and start with a more theoretical,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 393.57,
    "end": 400.594,
    "text": " and mathematical background, and then eventually present some maze simulations that are in MATLAB.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 401.355,
    "end": 405.898,
    "text": "Then there's a discussion, and then the methods section is following the discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 406.918,
    "end": 414.223,
    "text": "And it has the subsections generative model, variational free energy, inference and learning, neural networks, simulations, and data analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 415.203,
    "end": 423.127,
    "text": " And I think that I have kept to the right color codes consistently from here on forward.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 423.548,
    "end": 426.089,
    "text": "The black quotes are the direct quotes from the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 426.909,
    "end": 429.431,
    "text": "Blue quotes are related to perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 430.92,
    "end": 446.913,
    "text": " uh orange for action complete class theorems and neural networks together in purple and then red is commentary and then the highlighting for red is related to um the variational methods but it's kind of red ish",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 450.325,
    "end": 471.031,
    "text": " okay the papers canonical neural networks perform active inference by the authors uh listed previously it's in communications biology from 2022. and the key aims of the paper are laid out well in the first paragraph so that's in black and other colors and uh here the red is just kind of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 473.129,
    "end": 482.525,
    "text": " Getting into initial conversation with the paper and connecting a little bit more to some previous streams before we go more into the specifics of the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 483.427,
    "end": 486.832,
    "text": "The sentient behavior of biological organisms is characterized by optimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 487.895,
    "end": 496.081,
    "text": " Biological organisms recognize the state of their environment by optimizing internal representations of the external environmental dynamics generating sensory inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 496.562,
    "end": 498.803,
    "text": "So that's the sensory recognition model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 499.764,
    "end": 507.41,
    "text": "In addition, they optimize their behavior for adaptation to the environment, thereby increasing their probability of survival and reproduction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 508.311,
    "end": 512.654,
    "text": "So that is bringing the entire active and inactive population",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 513.496,
    "end": 519.201,
    "text": " control theory, formal theories of action, action selections, planning, and planning as inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 519.922,
    "end": 527.89,
    "text": "All of these action-oriented pragmatic turn type ideas come into play when this action orange aspect is brought in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 528.771,
    "end": 536.278,
    "text": "And it is slash must be slash should be, et cetera, oriented towards survival and persistence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 537.734,
    "end": 544.341,
    "text": " Otherwise, we don't see that kind of thing for log over appropriate definitions for thing and log, etc.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 545.682,
    "end": 558.155,
    "text": "And this few sentences that the paper begins with summarizes a common theme in Act-Inf, which is that there's basically two pathways towards proficiency in the niche.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 559.124,
    "end": 564.751,
    "text": " There's changing the mind, which is the perceptual and learning, and then there's changing the world through action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 565.311,
    "end": 578.287,
    "text": "And that kind of integration of inference, cognitive inference, whether it's perceptual or learning or other, and action as enacted is part of the act-inf formulism and shared with other areas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 579.668,
    "end": 588.933,
    "text": " This biological self-organization is typically formulated as the minimization of cost functions wherein a gradient descent on a cost function furnishes neural dynamics and synaptic plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 590.114,
    "end": 595.177,
    "text": "So they are working with a framework where the way that this perception action flow",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 596.599,
    "end": 616.06,
    "text": " is tractable computationally, either just for our current computers so that we can implement simulations and data fitting, or whether more fundamentally like this is the computational context of action, they're suggesting there's a way to think of it in terms of a cost function minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 616.92,
    "end": 642.831,
    "text": " also note like minimization maximization in a way they're interchangeable because there's just sometimes negative signs that can flip them so it's the same energy and fitness landscape that is being navigated whether you're minimizing to the bottom of the bowl and thinking of action selection that way and inference or whether you're climbing to the top of the hill gradient descent gradient ascent they're both kind of two sides of the same coin",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 646.092,
    "end": 647.936,
    "text": " Two fundamental issues remain to be established.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 647.976,
    "end": 651.062,
    "text": "Namely, so this is what the paper is saying.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 651.082,
    "end": 652.464,
    "text": "They're filling the gap in literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 652.805,
    "end": 657.314,
    "text": "The characterization of the dynamics of an arbitrary neural network as a generic optimization process",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 659.088,
    "end": 664.834,
    "text": " and the correspondence between such neural dynamics and statistical inference found in applied mathematics and machine learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 665.635,
    "end": 682.352,
    "text": "The present work addresses these issues by demonstrating that a class of canonical neural networks of rate-coding models is functioning as, and thus universally characterized in terms of, variational Bayesian inference under a particular but generic form of the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 685.27,
    "end": 692.073,
    "text": " This is maybe the only slide that has resources from too far afield from the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 692.734,
    "end": 694.354,
    "text": "Well, almost, a few more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 696.315,
    "end": 697.276,
    "text": "It's just on neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 698.536,
    "end": 707.04,
    "text": "The search on a very common search tool resulted in five plus million results for what are neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 707.821,
    "end": 709.942,
    "text": "And the first one was three blue, one brown.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 710.322,
    "end": 731.205,
    "text": " from 2017 which is just a nice YouTube video as they very often make with these beautiful renditions and it's a very great video and there's many many others a neural network is a network or circuit of biological neurons and that is variously purely computational and or biological",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 732.833,
    "end": 734.514,
    "text": " map territory stuff.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 735.875,
    "end": 747.121,
    "text": "And they can be thought of as nodes and edges, which are the firing rates and the connections between the biological neurons being modeled as weights between nodes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 749.232,
    "end": 768.806,
    "text": " a positive weight reflects an excitatory connection while negative values mean inhibitory connections and here is data coming in and ending up activating different neurons in this final layer so this is kind of doing inference in a neural net work and then there's an action part okay",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 770.387,
    "end": 781.511,
    "text": " The paper says variational Bayesian inference offers a unified explanation for inference, learning prediction, decision making, and the evolution of biological form, which can be considered over multiple timescales.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 781.571,
    "end": 785.992,
    "text": "So this returns to the earlier theme of like perception, action, and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 787.045,
    "end": 789.046,
    "text": " Persistence Within and Among Generations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 789.826,
    "end": 798.77,
    "text": "And the citations that are provided here for this are Friston, Kilner, Harrison, 2006, and Friston, 2010.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 799.591,
    "end": 804.773,
    "text": "Both very classic ACDINF slash FEP papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 805.393,
    "end": 806.894,
    "text": "Just to show one figure from each.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 807.574,
    "end": 808.195,
    "text": "Here's from...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 809.514,
    "end": 825.38,
    "text": " the first citation with the winged snowflake, where if the snowflake ends up being somewhere that's too warm, that's not able for it to maintain the structure, then it melts, melts into a dew, et cetera.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 826.5,
    "end": 832.903,
    "text": "And it must be acting as if in one way or another, it's staying above that phase boundary.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 833.403,
    "end": 834.983,
    "text": "That's how it's statistically identified.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 835.604,
    "end": 836.064,
    "text": "It's how it's,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 837.429,
    "end": 840.67,
    "text": " ontogenetically identified, functionally identified.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 840.69,
    "end": 866.017,
    "text": "And then here is one of the kind of path setting figures in the Princeton 2010 paper with a tree of many different areas to connect to and formalisms to represent as, including probabilistic neural coding, Bayesian brain,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 867.845,
    "end": 889.491,
    "text": " optimal control and value learning these three and others can be really seen at play in this 2022 paper uh the kind of inference variational bayesian methods use rests upon a generative model that expresses a hypothesis about the generation of sensory input",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 890.492,
    "end": 896.955,
    "text": " Perception and behavior can then be read as optimizing the evidence for a generative model inherent in sensory exchanges for the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 897.795,
    "end": 904.278,
    "text": "And that's the integration of perception and action within a single imperative in terms of if only computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 905.498,
    "end": 913.722,
    "text": "And that is described more on this slide, which one can pause and read, but the rest of the quotes are from the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 920.894,
    "end": 936.561,
    "text": " One key formalism that I had no familiarity with before this paper and associated readings and conversations with Ali was the complete class theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 937.421,
    "end": 942.163,
    "text": "So it would be great for anyone who's familiar with complete class theorem to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 943.339,
    "end": 947.584,
    "text": " help a little bit here, but here's some interesting things that I came across.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 948.625,
    "end": 961.319,
    "text": "Crucially, from the paper, as a corollary of the complete class theorem, citations 19 through 21, any neural network minimizing a cost function could be viewed as performing variational Bayesian inference under some prior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 962.881,
    "end": 968.168,
    "text": " here are the citations 19 through 21 from 1947 1981 2013 and uh also found some interesting resources so quoting from this less wrong post linked here",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 981.112,
    "end": 994.214,
    "text": " dutch book defines belief as willingness to bet and money pump defines preference as willingness to pay this is in terms of the foundational arguments for a bayesian epistemological",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 995.68,
    "end": 1016.67,
    "text": " worldview hope that's correct again it'd be good to hear from anyone who knows more here but in terms of different ways that one can consider the Bayesian proposition which perhaps these are even better to use than referencing any person's last name because the interesting thing will be the different",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1019.157,
    "end": 1025.23,
    "text": " lenses that these different framings on bayesian probabilities",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1026.656,
    "end": 1050.572,
    "text": " are interpreted as just like what a p-value is interpreted as in the frequentist world kind of analogously like a bayes factor interpretation so dutch book defines belief as willingness to bet and money pump defines preference as willingness to pay in doing so both arguments put the justification of decision theory into hypothetical exploitation scenarios which are not quite the same as the actual decisions we face",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1052.017,
    "end": 1059.606,
    "text": " If these were the best justifications for consequentialism we could muster, I would be somewhat dissatisfied, but would likely leave it alone.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1060.407,
    "end": 1063.73,
    "text": "Fortunately, a better alternative exists, the complete class theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1066.514,
    "end": 1070.198,
    "text": "So here's an image from that post.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1072.881,
    "end": 1076.964,
    "text": " Possible world states observation likelihood maps to the possible observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1077.704,
    "end": 1078.905,
    "text": "Hashtag A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1079.706,
    "end": 1082.608,
    "text": "Then decision-making rule, maybe probabilistic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1084.349,
    "end": 1085.67,
    "text": "Action selection as inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1086.771,
    "end": 1088.111,
    "text": "A, possible actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1089.072,
    "end": 1089.613,
    "text": "Affordances.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1090.506,
    "end": 1097.954,
    "text": " And here, we commonly see the loop being closed from actions to world states, like through the B matrix, changing how the world changes through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1098.675,
    "end": 1111.93,
    "text": "And here, these are both going to be mapped to a loss function, L, a real valued score from taking an action A when the world turned out to be theta, realized theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1113.53,
    "end": 1115.432,
    "text": " So that's quite an interesting framing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1117.313,
    "end": 1119.094,
    "text": "And there were some other useful posts online.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1119.615,
    "end": 1129.843,
    "text": "And from this ZHAT blog, the argument boils down to, if you agree with expected utility as your objective, then you have to be Bayesian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1130.644,
    "end": 1138.77,
    "text": "In a nutshell, a strategy is inadmissible if there exists another strategy that is as good in all situations and strictly better in at least one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1139.671,
    "end": 1141.813,
    "text": "If you want your strategy to be admissible,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1143.16,
    "end": 1145.163,
    "text": " It should be equivalent to a Bayes estimator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1146.145,
    "end": 1148.388,
    "text": "Complete class theorems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1149.891,
    "end": 1154.438,
    "text": "Only Bayes strategies are admissible and admissible strategies are Bayes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1156.351,
    "end": 1167.5,
    "text": " So there's probably a lot of interpretations of this, but it seems kind of related to optimal control or Bayes' optimal inference, perhaps a little bit more keenly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1168.361,
    "end": 1172.844,
    "text": "So these are some next two slides were contributed by Ali.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1173.244,
    "end": 1178.308,
    "text": "So if you, or if anyone familiar in this area wants to come on the discussion to be, great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1179.229,
    "end": 1183.833,
    "text": "But Ali pointed me to this book, Fundamentals of Bayesian Epistemology,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1184.485,
    "end": 1208.127,
    "text": " by Teitelbaum 2022 table of contents shown here and some challenges and objections to Bayesian epistemology are listed which can be read and then also there's some quite interesting logical structures which can be read here in terms of their premise theorem and conclusion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1209.17,
    "end": 1225.58,
    "text": " in the areas of arguments for bayesianism being representation theorem argument for probabilism the dutch book argument for probabilism and the gradational accuracy argument for probabilism so pretty interesting",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1227.847,
    "end": 1228.627,
    "text": " Back to the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1229.648,
    "end": 1236.792,
    "text": "They wrote, we have previously introduced a reverse engineering approach that identifies a class of biologically plausible cost functions for neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1237.653,
    "end": 1238.253,
    "text": "Citation 22.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1239.614,
    "end": 1243.616,
    "text": "Previous paper of Isomura and Friston, 2020.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1245.077,
    "end": 1249.42,
    "text": "The paper was reverse engineering neural networks to characterize their cost functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1250.765,
    "end": 1257.755,
    "text": " the abstract shown here, but this letter considers a class of biologically plausible cost functions for neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1259.835,
    "end": 1272.541,
    "text": " Using generative models based on partially observable Markov decision processes, POMDP, we show that neural activity and plasticity perform Bayesian inference and learning, respectively by maximizing model evidence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1273.141,
    "end": 1281.384,
    "text": "So this is a precursor paper from 2020 that is cited and foundational for the 2022 paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1283.185,
    "end": 1284.526,
    "text": "Here's some figures from that paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1285.492,
    "end": 1292.193,
    "text": " They have comparisons among a Markov decision process scheme and a neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1293.293,
    "end": 1301.095,
    "text": "For example, a Markov decision process scheme expressed as a Forney factor graph based on the formulation in Friston.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1302.435,
    "end": 1311.217,
    "text": "So here is the Markov decision process as a Forney factor graph, and on the right side is the neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1312.182,
    "end": 1321.748,
    "text": " with the hidden state sensory inputs and neural activity and some figures and concordances.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1326.667,
    "end": 1334.572,
    "text": " And they wrote, in this context, the neural network can in principle be used as a dynamic causal model to estimate threshold constants and implicit priors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1335.192,
    "end": 1344.538,
    "text": "This reverse engineering speaks to estimating the priors used by real neuronal systems under ideal Bayesian assumptions, sometimes referred to as meta-Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1345.339,
    "end": 1352.483,
    "text": "So they're laying out their research agenda and they're",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1352.703,
    "end": 1373.403,
    "text": " much of the work is going to reference this earlier paper and other work and then there's also some new integrations and especially the maze simulation in this paper and probably more so let's find out referring to the earlier paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1374.349,
    "end": 1383.512,
    "text": " This foundational work identified a class of cost functions for single layer feedforward neural networks of rate coding models with a sigmoid or logistic activation function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1384.613,
    "end": 1397.658,
    "text": "We subsequently demonstrated that mathematical equivalence between the class of cost functions for such neural networks and variational free energy under a particular form of the generative model, which is similar broadly to what the 22 paper does too.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1399.918,
    "end": 1407.641,
    "text": " This equivalence licenses variational Bayesian inference as a fundamental optimization process that underlies both the dynamics and function of such neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1410.683,
    "end": 1419.727,
    "text": "However, it remains to be established whether the active inference is an apt explanation for any given neural network that actively exchanges with its environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1420.627,
    "end": 1428.471,
    "text": "In this paper, we address this inactive or control aspect to complete the formal equivalence of neural network optimization and the free energy principle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1429.662,
    "end": 1436.128,
    "text": " So this earlier work was not including action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1437.269,
    "end": 1451.742,
    "text": "And so this paper's key addition, as I understand it, hopefully, is that they bring action more formally into the model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1452.978,
    "end": 1460.12,
    "text": " And it'd be interesting to know what else is that change associated with or what else happens or doesn't.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1463.501,
    "end": 1469.023,
    "text": "Here's some more text about the approach that they're going to take.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1470.583,
    "end": 1472.804,
    "text": "And many of these details are going to be addressed later on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1477.219,
    "end": 1480.082,
    "text": " Here's table one glossary of expressions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1481.483,
    "end": 1487.368,
    "text": "So these will also be broadly addressed later on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1491.297,
    "end": 1494.738,
    "text": " Maybe it's useful though, just to read the first one, a canonical neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1495.639,
    "end": 1510.424,
    "text": "In this work, the canonical neural network is defined by differential equations of neural activity derived as a reduction of realistic neuron models through some approximations, which give a network of rate coding neurons with a sigmoid activation function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1511.545,
    "end": 1517.107,
    "text": "In particular, we consider networks comprising a middle layer that involves recurrent connections",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1518.326,
    "end": 1541.752,
    "text": " the output layer that provides feedback responses to the environment so some category of neural network architectures or anatomies physiologies whatever with sensory cognitive and action-like features in an environment or a generative process so the results section",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1546.451,
    "end": 1551.275,
    "text": " They write, in overview of equivalence between neural networks and variational base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1551.815,
    "end": 1555.779,
    "text": "First, we summarize the formal correspondence between neural networks and variational base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1556.419,
    "end": 1563.685,
    "text": "A biological agent is formulated here as an autonomous system comprising a network of rate coding neurons, figure 1a.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1563.945,
    "end": 1566.388,
    "text": "So here's a small figure 1a, we'll see it larger.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1568.049,
    "end": 1586.679,
    "text": " based upon the assumptions which will be brought up later on in a different fuller form the update rule for the ith component of phi which is the internal states so the cognitive states and the output layer y so middle layer x and output layer y",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1589.109,
    "end": 1605.852,
    "text": " These can be seen as the cognitive and the action selective aspects, which are the autonomous states in contrast to the particular states of the active inference entity, which is the whole blanket and the cognitive states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1605.872,
    "end": 1610.573,
    "text": "So including the sensory states, but the sensory states can't be directly controlled.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1611.293,
    "end": 1612.973,
    "text": "However, action can influence them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1613.433,
    "end": 1616.094,
    "text": "And so that's what closes the causal loop.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1617.688,
    "end": 1630.824,
    "text": " And that set of the particular states comprising the autonomous states, the update rule for the ith component of phi is derived as the gradient descent on the cost function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1631.758,
    "end": 1639.401,
    "text": " So the change in that, bi sub i, is proportional to a derivative on the loss function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1640.241,
    "end": 1644.203,
    "text": "This determines the dynamics of neural networks, including their activity and plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1644.263,
    "end": 1646.144,
    "text": "So L, common loss function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1646.924,
    "end": 1649.165,
    "text": "O, observations, sensory states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1650.706,
    "end": 1658.489,
    "text": "Bi, internal states comprising the middle and output layer, neural activity, that's the rate coding part, and synaptic strengths, W, the parameterization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1659.474,
    "end": 1665.977,
    "text": " and other free parameters, including that, that characterize L. Then there's the output layer activity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1666.477,
    "end": 1669.779,
    "text": "Y is determining the network's actions or decisions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1670.799,
    "end": 1671.139,
    "text": "Delta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1673.32,
    "end": 1684.726,
    "text": "O, X, and Y. Y is the outputting action, environment, generative process, hidden state, passing observations to the sensory, cognitive, active layers of the neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1685.606,
    "end": 1687.067,
    "text": "And that is analogous",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1688.208,
    "end": 1694.089,
    "text": " topologically to the variational Bayesian formulation in Figure 1b.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1695.13,
    "end": 1701.371,
    "text": "And so, left side, gradient descent on a neural network cost function L determines the dynamics of neural activity and plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1701.891,
    "end": 1704.732,
    "text": "Thus, L is sufficient to characterize the neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1705.872,
    "end": 1714.614,
    "text": "And then, being shown next to, on the right side, variational free energy minimization allows an agent to self-organize",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1715.424,
    "end": 1720.506,
    "text": " to encode the hidden states of the external milieu and to make decisions minimizing future risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1721.387,
    "end": 1727.469,
    "text": "Here, variational free energy F is sufficient to characterize the inferences and behaviors of the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1728.27,
    "end": 1742.076,
    "text": "So, sentence structure, parallelism, and these two schemes or approaches being linked and applied is a focus of this area.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1745.655,
    "end": 1761.702,
    "text": " So neural networks are minimizing the cost function L. In contrast, variational Bayesian inference depicts a process of updating the prior distribution of external states, p of theta, to the corresponding posterior distribution, q of theta, based upon a sequence of observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1762.923,
    "end": 1769.466,
    "text": "And we see other familiar terms from discussions on variational Bayes, like minimization of surprise,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1771.107,
    "end": 1774.911,
    "text": " And there's some more model details shown here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1774.931,
    "end": 1783.338,
    "text": "A few points are about choosing the family of distributions that one is doing variational inference with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1784.7,
    "end": 1793.548,
    "text": "And that allows for the gradient update rules in that family of distributions to be made simpler.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1794.738,
    "end": 1803.763,
    "text": " for example, choosing a linear regression with an L2 norm, at the very least, you can say that it has a simple decision rule for being fit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1804.284,
    "end": 1809.006,
    "text": "I'm sure there's like also counter arguments and other algorithms that are better and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1809.066,
    "end": 1818.312,
    "text": "But just to say that a simple decision rule in a known family of distributions can often be good enough as an approximation, if not more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1821.154,
    "end": 1823.555,
    "text": "Crucially, according to the complete class theorem,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1824.977,
    "end": 1833.664,
    "text": " from earlier, a dynamical system that minimizes its cost function can be viewed as performing Bayesian inference under some generative model and prior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1834.464,
    "end": 1852.859,
    "text": "The complete class theorem, just goes on to describe it, ensures the presence of a generative model that formally corresponds to the above defined neural network characterized by L. Hence, this speaks to the equivalence between the class of neural network cost functions and variational free energy under such a generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1853.519,
    "end": 1854.12,
    "text": "Equation one,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1855.39,
    "end": 1873.336,
    "text": " loss function on the time series of observations comma parameters three lines is defined as f the free energy function on the same o through time comma parameters so there's a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1874.773,
    "end": 1886.835,
    "text": " parallel being shown slash defined, wherein the internal states of a network phi encode or parameterize the posterior expectation theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1888.896,
    "end": 1898.058,
    "text": "This mathematical equivalence means that an arbitrary neural network in the class under consideration is implicitly performing active inference through variational free energy minimization,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1899.618,
    "end": 1920.024,
    "text": " And more is written, but this is one of many statements that will be made corresponding to correspondences between neural network loss function, generative model, act-inf, free energy minimization, and associated formalisms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1923.005,
    "end": 1928.247,
    "text": "Note that being able to characterize the neural network in terms of maximizing model evidence lends it in explainability,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1929.625,
    "end": 1936.671,
    "text": " in the sense that internal neural network states and parameters encode Bayesian beliefs or expectations about the causes of observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1937.672,
    "end": 1940.975,
    "text": "In other words, the generative model explains how outcomes were generated.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1942.056,
    "end": 1948.122,
    "text": "However, the complete class theorem does not specify the form of a generative model for any given neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1950.236,
    "end": 1963.05,
    "text": " To address this issue, in the remainder of the paper, we formulate active inference using a particular form of partially observable Markov decision processes, POMDP models, whose states take binary values.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1963.43,
    "end": 1970.077,
    "text": "So this is one of several simplifications, is one way to see it, or areas for later generalization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1972.52,
    "end": 1972.94,
    "text": " Figure 2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1973.201,
    "end": 1982.408,
    "text": "In this section, we define a generative model and ensuing variational free energy that corresponds to a class of canonical neural networks that will be considered in the subsequent section.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1983.068,
    "end": 1988.433,
    "text": "The external milieu is expressed as a discrete state space in the form of a POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1988.713,
    "end": 1988.953,
    "text": "Figure 2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1990.234,
    "end": 1993.537,
    "text": "So to make figure 2 larger.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2000.919,
    "end": 2004.841,
    "text": " There's a lot to probably say about this figure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2006.542,
    "end": 2015.407,
    "text": "I'll just note that the caption is informative and some of the highlighted lines.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2017.048,
    "end": 2027.854,
    "text": "It'll be awesome to have the author and other people who's familiar with some of the differences and symmetries between the, for example, bottom and top.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2028.843,
    "end": 2042.937,
    "text": " above and below the observations and also about the role of time and what these two risks are what is fictive causality here's more details on that pomdp",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2045.899,
    "end": 2047.259,
    "text": " Then equation two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2048.38,
    "end": 2053.501,
    "text": "This is in the section active inference formulated using a postdiction of past decisions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2054.001,
    "end": 2060.203,
    "text": "So if it was a prediction of future decisions, it's the opposite, a postdiction of past decisions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2061.323,
    "end": 2064.124,
    "text": "Hence, we define the generative model as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2065.064,
    "end": 2071.486,
    "text": "P O delta S gamma theta, which is the model of observations, decisions,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2073.058,
    "end": 2076.66,
    "text": " observations, decisions, hidden states, risk, and theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2079.581,
    "end": 2087.085,
    "text": "Theta equals A, B, and C. Constitutes the set of parameters and more details are provided.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2088.146,
    "end": 2096.37,
    "text": "This is the familiar notation with some slight differences that'll be described.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2097.03,
    "end": 2100.952,
    "text": "And equation two reflects the factorizability",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2102.725,
    "end": 2125.002,
    "text": " and the dimensions and the kind of computational tractability expression so that would be interesting to also learn about like under what conditions can this joint distribution be factorized under what simplifications or constructions",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2128.871,
    "end": 2132.557,
    "text": " The agent is making decisions to minimize a risk function, capital Gamma.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2132.918,
    "end": 2134.561,
    "text": "That's on the bottom of figure two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2136.885,
    "end": 2140.952,
    "text": "Fictive causality leading to this Gamma coming from that Gamma.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2144.049,
    "end": 2155.558,
    "text": " Equation three is shown, and to characterize the optimal decisions as minimizing expected risk in our POMDP model, we use a fictive mapping from the current risk gamma to pass decisions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2155.939,
    "end": 2159.361,
    "text": "That's that retrodictive fictive causality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2160.302,
    "end": 2172.191,
    "text": "Although this is not the true causality in the real generative process that generates sensory data, here we intend to model the manner that an agent subjectively evaluates its previous decisions after experiencing their consequences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2172.972,
    "end": 2177.436,
    "text": " This fictive causality is expressed in the form of a categorical distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2178.397,
    "end": 2181.22,
    "text": "So it could be other families, hypothetically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2182.261,
    "end": 2189.408,
    "text": "But this equation three is describing fictive causality and this interesting sign.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2191.013,
    "end": 2194.094,
    "text": " indicates the element-wise division operator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2194.575,
    "end": 2199.597,
    "text": "Also note throughout the manuscript, the overlying variable indicates 1 minus that variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2199.917,
    "end": 2208.021,
    "text": "So gamma bar equals 1 minus gamma, or it can be understood as the complement of a statistical probability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2208.701,
    "end": 2213.063,
    "text": "Probability of A happening and the probability of not A happening in that one way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2216.846,
    "end": 2221.567,
    "text": " Importantly, the agent needs to keep selecting good decisions while avoiding bad decisions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2221.947,
    "end": 2231.63,
    "text": "To this end, equation 3 supposes that the agent learns from the failure of decisions by assuming that the bad decisions were sampled from the opposite of the optimal policy mapping.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2232.83,
    "end": 2234.351,
    "text": "Some more details.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2235.991,
    "end": 2238.032,
    "text": "And then, by convention,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2238.931,
    "end": 2242.113,
    "text": " Active inference uses C to denote the prior preference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2243.134,
    "end": 2250.558,
    "text": "That's how we've seen the C variable in many models as preferences, prior preference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2251.359,
    "end": 2257.222,
    "text": "This work uses C to denote a mapping to determine a decision depending on the previous state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2258.123,
    "end": 2261.625,
    "text": "Herein, the prior preference is implicit in the risk function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2262.466,
    "end": 2263.366,
    "text": "Due to construction, C",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2265.953,
    "end": 2268.494,
    "text": " does not explicitly appear in the inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2268.894,
    "end": 2271.595,
    "text": "Thus, it is omitted in the following formulations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2272.835,
    "end": 2278.777,
    "text": "So that's a key point about the notation of the variable C and something kind of maybe interesting to explore.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2278.797,
    "end": 2288.36,
    "text": "Equation 4, variational Bayesian inference refers to the process that optimizes the posterior belief Q of theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2288.88,
    "end": 2293.902,
    "text": "Based on the mean field approximation, Q of theta is expressed as, and here is the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2296.192,
    "end": 2302.895,
    "text": " factorization representation of the Q of theta, variational.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2303.856,
    "end": 2304.996,
    "text": "And another notation note.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2305.377,
    "end": 2316.182,
    "text": "Throughout the manuscript, bold case variables, such as bold s sub tau, denote the posterior expectations of the corresponding italic case random variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2318.22,
    "end": 2320.841,
    "text": " And some more details about the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2320.981,
    "end": 2327.623,
    "text": "For example, for simplicity, here we suppose that state and decision priors D and E are fixed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2328.183,
    "end": 2334.365,
    "text": "So one could imagine they don't have to be, but for simplicity, they will be here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2337.066,
    "end": 2343.388,
    "text": "Under the above defined generative model and posterior beliefs, the ensuing variational free energy is analytically expressed as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2343.708,
    "end": 2344.328,
    "text": "Equation five.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2346.172,
    "end": 2359.063,
    "text": " This is a variational free energy, F, and recall equation one, that it is being connected to, juxtaposed with, et cetera, the loss function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2360.964,
    "end": 2362.946,
    "text": "The gradient descent on variational free energy",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2364.935,
    "end": 2369.518,
    "text": " updates the posterior beliefs about hidden states, S, decisions, delta, and parameters theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2370.619,
    "end": 2387.911,
    "text": "The optimal posterior beliefs that minimize variational free energy are obtained as the fixed point of the implicit gradient descent, which ensures that change in F with respect to the change in hidden state through time equals zero and some more definitions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2388.371,
    "end": 2389.672,
    "text": "All of them are zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2390.132,
    "end": 2392.334,
    "text": "That one might be an O, but they're all zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2393.729,
    "end": 2419.841,
    "text": " and this is saying the ball rolls to the bottom of the hill in this gradient descent and when the rate of change is zero then that is a fixed point attractor like dynamic and that can be used as a way to incrementally fit statistical models like the loss function is used to incrementally fit neural networks",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2422.355,
    "end": 2429.836,
    "text": " To explicitly demonstrate the formal correspondence with the cost function for neural networks, consider in the next section we further transform the variational free energy as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2430.897,
    "end": 2431.857,
    "text": "Some details.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2432.977,
    "end": 2437.678,
    "text": "Using these relationships, equation five is transformed into the form shown in figure three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2438.698,
    "end": 2441.438,
    "text": "See the method section variational free energy for further details.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2443.199,
    "end": 2448.32,
    "text": "In what follows, we demonstrate that the internal states of canonical neural networks encode posterior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2448.94,
    "end": 2449.84,
    "text": "Here's figure three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2451.621,
    "end": 2467.656,
    "text": " On the top from the caption figure three is the mathematical mathematical equivalence between variational free energy and neural network cost functions depicted by one to one correspondence of their components top variational free energy transformed from equation five using the Bayes theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2468.978,
    "end": 2469.098,
    "text": "and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2471.616,
    "end": 2472.978,
    "text": " foreshadowing equation seven.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2473.778,
    "end": 2479.825,
    "text": "Using this relationship, equation seven is transformed into the form presented at the bottom of the figure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2480.626,
    "end": 2489.637,
    "text": "So here's F, variational free energy, and L, the neural network cost function, and different elements as they're represented here",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2491.328,
    "end": 2512.681,
    "text": " with some resonances and concordance and beyond which we can explore they're being shown to be equivalent and it was in the conversation preparing for this dot zero with dean where we saw this as some chromosomes",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2518.28,
    "end": 2520.882,
    "text": " In this section, canonical neural networks perform active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2521.222,
    "end": 2526.786,
    "text": "In this section, we identify the neuronal substrates that correspond to components of the active inference scheme defined above.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2527.526,
    "end": 2532.389,
    "text": "We consider a class of two-layer neural networks with recurrent connections in the middle layer, Figure 1a.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2532.809,
    "end": 2538.293,
    "text": "Those are those connections with loops recurrent in the middle layer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2539.854,
    "end": 2546.938,
    "text": "The modeling of the networks in this section, referred to as canonical neural networks, is based on the following three assumptions that reflect physiological knowledge.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2548.167,
    "end": 2554.09,
    "text": " 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2548.307,
    "end": 2554.09,
    "text": "Gradient descent on a cost function L determines the updates of neural activity and synaptic weights.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2554.43,
    "end": 2555.891,
    "text": "Method section, neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2557.132,
    "end": 2558.513,
    "text": "Assumption 2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2558.973,
    "end": 2565.616,
    "text": "Neural activity is updated by the weighted sum of inputs and its fixed point is expressed in a form of the sigmoid or logistic function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2565.776,
    "end": 2566.317,
    "text": "Assumption 3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2566.877,
    "end": 2570.719,
    "text": "A modulatory factor mediates synaptic plasticity in a post hoc manner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2576.121,
    "end": 2587.549,
    "text": " They write, based upon assumption two, which is neural activity is updated by the weighted sum of inputs and its fixed point, is expressed in the form of a sigmoid or logistic function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2588.869,
    "end": 2596.855,
    "text": "Based on assumption two, we formulate neural activity in the middle layer and output layer, the autonomous states, as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2598.996,
    "end": 2599.596,
    "text": "Equation six.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2602.118,
    "end": 2603.319,
    "text": "Without loss of generality,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2605.942,
    "end": 2619.206,
    "text": " Equation six can be cast as the gradient descent on cost function L. Such a cost function can be identified by simply integrating the right-hand side of equation six with respect to X and Y consistent with previous treatments, citations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2620.886,
    "end": 2631.649,
    "text": "Because neural activity and synaptic plasticity minimize the same cost function L, the derivatives of L must generate the modulated synaptic plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2633.143,
    "end": 2639.785,
    "text": " Under these constraints, reflecting assumptions one through three, a class of cost functions is identified as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2641.286,
    "end": 2643.266,
    "text": "Equation seven, loss function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2644.627,
    "end": 2647.928,
    "text": "Awesome to hear somebody read this directly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2651.009,
    "end": 2653.23,
    "text": "So there's a firing rate aspect.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2654.861,
    "end": 2662.804,
    "text": " that's related to neural firing in the short term, more perceptual aspect of function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2663.644,
    "end": 2675.369,
    "text": "And there's a slower neurotransmitter, neuroglial factor mediated learning over perhaps a different timescale and with some different features.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2676.149,
    "end": 2678.83,
    "text": "And they're being included as a joint model of inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2679.691,
    "end": 2682.532,
    "text": "And here that is being connected to doing inference on action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2684.597,
    "end": 2710.477,
    "text": " synaptic plasticity in neural networks so synaptic plasticity rules conjugate to the above rate coding model can now be expressed as a gradient descent on the same cost function l according to assumption one equations eight and nine showing that neural networks can integrate those modes of firing rate like and synaptomodulatory like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2713.049,
    "end": 2715.791,
    "text": " neural networks, cost functions, and variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2717.952,
    "end": 2724.556,
    "text": "Based on the above considerations, we now establish the formal correspondence between the neural network cost function and variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2725.577,
    "end": 2731.241,
    "text": "Using under the aforementioned three minimal assumptions, we identified the neural network cost function as equation seven.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2731.801,
    "end": 2738.605,
    "text": "Equation seven can be transformed into the form shown in figure three bottom using sigmoid functions of synaptic strengths.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2739.726,
    "end": 2742.388,
    "text": "So equation seven, loss function,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2744.164,
    "end": 2747.766,
    "text": " transformed into the form shown in Figure 3.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2749.427,
    "end": 2749.727,
    "text": "Bottom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2758.652,
    "end": 2758.912,
    "text": "Hence,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2763.479,
    "end": 2774.324,
    "text": " Hence, this class of cost functions for canonical neural networks is formally homologous to variational free energy under the particular form of the POMDP generative model defined in the previous section.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2776.084,
    "end": 2788.83,
    "text": "We obtained this result based on analytic derivations without reference to the complete class theorem, thereby confirming the proposition of equation one, loss function and free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2790.168,
    "end": 2796.273,
    "text": " This in turn suggests that any canonical neural network in this class is implicitly performing active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2798.075,
    "end": 2804.901,
    "text": "Table 2 summarizes the correspondence between the quantities of the neural network and their homologues in variational base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2805.862,
    "end": 2806.002,
    "text": "So 2",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2808.522,
    "end": 2835.556,
    "text": " a great concordance table on the left side neural network formation on the right side variational bayes formulation so just like in figure three we had the two long lines then table two we have rotated 90 degrees",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2837.839,
    "end": 2860.559,
    "text": " and the variables for different parts of these models are laid next to each other so what papers of neural networks can we make active models for is it already done or is there just one",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2861.966,
    "end": 2885.232,
    "text": " script that needs to be done and then conversely what interesting variational bayesian models have interesting applications or some other value or information gain from being cast as neural networks or integrating neural networks into what had previously been",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2886.835,
    "end": 2895.729,
    "text": " only analytical variational Bayesian models as well as the empirical data fitting aspect which is related that's highlighted by the authors",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2898.061,
    "end": 2908.088,
    "text": " So in summary, when a neural network minimizes the cost function with respect to its activity and plasticity, the network self-organizes to furnish responses that minimize the risk implicit in the cost function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2908.808,
    "end": 2916.173,
    "text": "This biological optimization is identical to variational free energy minimization under a particular form of the POMDP model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2917.094,
    "end": 2927.801,
    "text": "Hence, this equivalence indicates that minimizing the expected risk through variational free energy minimization is an inherent property of canonical neural networks featuring delayed modulation of Hebbian plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2929.382,
    "end": 2956.752,
    "text": " okay brief second to view some image memes and take a breath and a stretch on the left side the top panel says every neural network is implicitly performing active inference question mark that's awesome",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2958.365,
    "end": 2965.352,
    "text": " In the middle image meme, one simply makes a neural network and it is implicitly performing active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2966.393,
    "end": 2967.413,
    "text": "Also question mark.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2968.875,
    "end": 2976.061,
    "text": "And then in the right image meme, there are two pieces of paper, neural network and implicit performance of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2976.722,
    "end": 2979.765,
    "text": "Corporate needs you to find the differences between this picture and this picture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2981.246,
    "end": 2982.387,
    "text": "And here is the paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2983.208,
    "end": 2984.029,
    "text": "They're the same picture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2986.109,
    "end": 3010.623,
    "text": " So the previous sections have successfully extended the variational Bayes meets inference neural network results from the 2020 paper with an action loop and a loss function only on the autonomous states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3012.124,
    "end": 3014.786,
    "text": "So that was the conceptual and technical",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3016.564,
    "end": 3039.967,
    "text": " feature that this paper brought in again it'll be awesome to hear the author describe it more and differently and then in this part of this of the paper they turn from that kind of analytical theoretical towards some numerical simulations in MATLAB",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3041.604,
    "end": 3049.09,
    "text": " Here we demonstrate the performance of canonical neural networks using maze tasks as an example of a delayed reward task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3050.631,
    "end": 3054.734,
    "text": "The agent comprised the aforementioned canonical neural networks",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3056.192,
    "end": 3073.233,
    "text": " figure 4a thus it implicitly performs active inference by minimizing variational free energy so now some entity or agent is going to be constructed and numerically simulated to support some of the uh",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3075.931,
    "end": 3105.11,
    "text": " aspects of their model and point towards utility in other settings so here's figure four simulation of neural networks solving maze tasks uh in part a there is the architecture of the agent sensory input layer o of t synaptic weights into the middle layer x internal states",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3108.152,
    "end": 3122.939,
    "text": " and the output layer Y, decision action with risk gamma of T. The sensation comes in, a neural network, then outputs a decision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3125.18,
    "end": 3134.944,
    "text": "There's some task that the entity must perform, which is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3137.918,
    "end": 3142.901,
    "text": " to move towards the goal from the start across this lateral maze.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3144.222,
    "end": 3157.97,
    "text": "So one could imagine that if it were just a hallway with no maze features, simple strategies would be very overfit, but effective, like always move simply towards the goal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3158.985,
    "end": 3169.694,
    "text": " Whereas in more complex settings, which this is an example of, where there's uncertainty as well as many local optima.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3170.515,
    "end": 3178.702,
    "text": "So one has to sometimes take one or two or three or four or more steps or however many to get closer to the goal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3180.507,
    "end": 3198.351,
    "text": " and sometimes may not know, for example, how long those backtracking situations are going to be and all these other complexities for which this maze example is symbolic of.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3199.111,
    "end": 3204.112,
    "text": "So maybe there's some interesting mythos maze connections and computational.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3205.393,
    "end": 3214.226,
    "text": " And here's some performance measures on the maze task with the neural network entity this way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3216.943,
    "end": 3225.051,
    "text": " Before training, the agent moved to a random direction in each step, resulting in a failure to reach the goal position within the time limit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3225.391,
    "end": 3231.518,
    "text": "During training, the neural network updated synaptic strengths depending on its neural activity and ensuing outcomes .",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3233.84,
    "end": 3237.804,
    "text": "This training comprised a cycle of action and learning phases.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3239.245,
    "end": 3250.908,
    "text": " This treatment renders neural activity and adaptive behaviors of the agent highly explainable and manipulatable in terms of the appropriate prior beliefs, implicit and firing thresholds for a given task or environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3251.568,
    "end": 3259.67,
    "text": "In other words, these results suggest that firing thresholds are the neuronal substrates that encode state and decision priors as predicted mathematically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3261.01,
    "end": 3261.55,
    "text": "Big if true.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3264.554,
    "end": 3287.322,
    "text": " Furthermore, when the updating of parameters is slow across these two now linked domains, parameters of the variational Bayesian autonomous state inference and the neural network loss function parameters, when the updating of these parameters are slow in relation to the experimental observations,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3288.493,
    "end": 3294.036,
    "text": " The parameters can be estimated through Bayesian inference based on empirically observed neuronal responses.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3294.496,
    "end": 3296.477,
    "text": "See method section data analysis for details.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3297.618,
    "end": 3302.16,
    "text": "Using this approach, we estimated implicit prior E, which is encoded by psi,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3303.735,
    "end": 3310.896,
    "text": " from sequences of neural activity generated from the synthetic neural networks used in the simulations reported in figure four.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3311.856,
    "end": 3319.198,
    "text": "We confirmed that this estimator was a good approximation to the true E. So that's also pretty interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3319.958,
    "end": 3333.4,
    "text": "This is showing they don't just lay out this architecture and show that it's possible to fulfill this maze task with the best",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3335.973,
    "end": 3360.224,
    "text": " whatever it's not a classification accuracy imperative alone they're describing that from empirically observed neuronal responses which is to say the experimenter's observation the sequences of neural activity generated from the synthetic neural networks used in that figure",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3361.952,
    "end": 3369.361,
    "text": " Numerically, so statistically, that estimator was a good approximation to the true E, figure 5A.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3371.083,
    "end": 3372.205,
    "text": "So here's figure 5.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3374.507,
    "end": 3379.934,
    "text": "Estimation of implicit priors enables the prediction of subsequent learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3381.522,
    "end": 3391.726,
    "text": " So that's pretty interesting and will be great to hear what each of the axes are and what all the variables mean.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3394.887,
    "end": 3407.772,
    "text": "With this setup in place, they did some numerical validation and talked a little bit more about fitting data from this simulation model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3411.397,
    "end": 3413.298,
    "text": " Here's the discussion section.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3414.498,
    "end": 3416.979,
    "text": "So the first paragraph of the discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3418.179,
    "end": 3421.5,
    "text": "Biological organisms formulate plans to minimize future risks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3422.241,
    "end": 3427.002,
    "text": "In this work, we captured this characteristic in biologically plausible terms under minimal assumptions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3428.002,
    "end": 3437.545,
    "text": "We derive simple differential equations that can be plausibly interpreted in terms of a neural network architecture that entails degrees of freedom with respect to certain free parameters, e.g.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3437.625,
    "end": 3438.346,
    "text": "firing threshold.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3439.426,
    "end": 3444.268,
    "text": " These three parameters play the role of prior beliefs in variational Bayesian formulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3445.249,
    "end": 3452.812,
    "text": "Thus, the accuracies of inferences and decisions depend upon prior beliefs, implicit in neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3453.612,
    "end": 3455.173,
    "text": "And here's some more stable diffusion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3455.973,
    "end": 3460.235,
    "text": "Ant neural network, ant brain neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3462.907,
    "end": 3465.529,
    "text": " So some more summarization of what they did.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3466.51,
    "end": 3484.046,
    "text": "Based on the view of the brain as an agent that performs Bayesian inference, neuronal implementations of Bayesian belief updating have been proposed, which enables neural networks to store and recall spiking sequences, eight, learn temporal dynamics and causal hierarchy, nine,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3484.807,
    "end": 3490.849,
    "text": " extract hidden causes 10, solve maze tasks 11, and make plans to control robots 12.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3491.629,
    "end": 3494.35,
    "text": "So citations 8 through 12 listed here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3496.511,
    "end": 3501.852,
    "text": "In these approaches, the update rules are generally derived from Bayesian cost functions, e.g.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3501.892,
    "end": 3503.113,
    "text": "variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3503.873,
    "end": 3513.876,
    "text": "However, the precise relationship between these update rules and the neural activity and plasticity of canonical neural networks has yet to be fully established.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3517.662,
    "end": 3525.266,
    "text": " We identified a one-to-one correspondence between neural network architecture and a specific POMDP implicit in that network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3527.007,
    "end": 3533.211,
    "text": "Equation 2 speaks to a unique POMDP model consistent with the neural network architecture defined in equation 6.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3534.667,
    "end": 3538.868,
    "text": " where their correspondences are summarized in table two and the figures.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3540.649,
    "end": 3548.391,
    "text": "This means that our scheme can be used to identify the form of the POMDP given an observable circuit structure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3549.491,
    "end": 3554.373,
    "text": "Moreover, the free parameters that parameterize equation six can be estimated using equation 24.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3556.55,
    "end": 3563.274,
    "text": " This means that the generative model and ensuing variational free energy can in principle be reconstructed from empirical data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3564.495,
    "end": 3572.881,
    "text": "This offers a formal characterization of implicit Bayesian models entailed by neural circuits, thereby enabling a prediction of subsequent learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3573.781,
    "end": 3576.443,
    "text": "So what can be done with this?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3577.844,
    "end": 3580.445,
    "text": "What is this new?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3582.026,
    "end": 3582.667,
    "text": "What is new here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3584.368,
    "end": 3585.829,
    "text": "Does this second selection",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3588.39,
    "end": 3598.037,
    "text": " fully establish the precise relationship between these update rules and the neural activity and plasticity of canonical neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3603.482,
    "end": 3611.004,
    "text": " Here is a discussion section on Hebbian plasticity, neurotransmitters, and glia, with a lot of citations listed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3612.064,
    "end": 3631.569,
    "text": "And here is just one interesting follow-on discussion from the computational aspects, neurocognitive and computational aspects of Hebbian plasticity, is these modulations have been observed empirically with various neuromodulators and neurotransmitters, such as dopamine,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3632.54,
    "end": 3637.763,
    "text": " noradrenaline, muscarine, and GABA, as well as glial factors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3638.784,
    "end": 3641.686,
    "text": "So here's a picture by Alexandra Mikhailova.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3642.286,
    "end": 3649.09,
    "text": "Cultured astrocytes release signaling and growth factors that regulate proper neuronal development.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3649.77,
    "end": 3652.012,
    "text": "So cool glial pic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3654.758,
    "end": 3656.619,
    "text": " Dopamine and reinforcement learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3657.3,
    "end": 3663.384,
    "text": "In particular, a delayed modulation of synaptic plasticity is well known with dopamine neurons.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3664.064,
    "end": 3664.804,
    "text": "Citations 35 through 37.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3664.844,
    "end": 3666.205,
    "text": "Those citations are listed here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3671.969,
    "end": 3683.727,
    "text": " This speaks to a learning scheme that is conceptually distinct from standard reinforcement learning algorithms, such as the temporal difference learning with actor-critic models based on state action value functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3684.909,
    "end": 3687.112,
    "text": "Please see the previous work, citation 50,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3688.308,
    "end": 3692.194,
    "text": " for a detailed comparison between active inference and reinforcement learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3693.275,
    "end": 3699.885,
    "text": "That is Sajid, Bal, Par, and Fristen, active inference, demystified, and compared from 2021.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3700.265,
    "end": 3704.131,
    "text": "And that's also ACTIV model stream number 2.1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3706.581,
    "end": 3718.609,
    "text": " We mathematically demonstrated that such plasticity enhances the association between the pre-post mapping and the future value of the modulatory factor, where the latter is cast as a risk function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3719.63,
    "end": 3725.914,
    "text": "This means that postsynaptic neurons self-organize to react in a manner that minimizes future risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3726.835,
    "end": 3729.657,
    "text": "So the neural network had three layers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3731.007,
    "end": 3746.198,
    "text": " It's the second and the third layer, not the initial sensory layer, but the second and the third layer, the cognitive and the active states, which are the autonomous states of the particular states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3747.358,
    "end": 3752.162,
    "text": "So that's quite interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3753.583,
    "end": 3756.745,
    "text": "The self-organization of postsynaptic neurons",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3758.362,
    "end": 3760.985,
    "text": " to react in a manner that minimizes future risk?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3762.927,
    "end": 3775.741,
    "text": "Crucially, this computation corresponds formally to variational Bayesian inference under a particular form of POMDP generative models, suggesting that the delayed modulation of Hebbian plasticity is a realization of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3777.111,
    "end": 3790.621,
    "text": " And regionally specific projections of neuromodulators may allow each brain region to optimize activity to minimize risk and leverage a hierarchical generative model implicit in cortical and subcortical hierarchies.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3792.022,
    "end": 3796.725,
    "text": "This is reminiscent of theories of neuromodulation and meta-learning developed previously.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3797.005,
    "end": 3797.886,
    "text": "Citation 52.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3798.506,
    "end": 3800.007,
    "text": "Doya, 2002.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3801.108,
    "end": 3802.849,
    "text": "Meta-learning and neuromodulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3803.75,
    "end": 3803.97,
    "text": "Cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3805.399,
    "end": 3814.181,
    "text": " Our work may be potentially useful when casting these theories in terms of generative models and variational free energy minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3817.506,
    "end": 3822.891,
    "text": " They then return the discussion to the complete class theorem and neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3823.151,
    "end": 3836.482,
    "text": "So the complete class theorem, same citations from before, ensures that any neural network whose activity and plasticity minimize the same cost function can be cast as performing Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3837.643,
    "end": 3865.781,
    "text": " however identifying the implicit generative model that underwrites any canonical neural network is a more delicate problem because the theorem does not specify a form of the generative model for a given canonical neural network which is pretty interesting is that to say that the form of the generative model as modeled is different in what ways from the given canonical neural network",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3868.058,
    "end": 3875.504,
    "text": " The posterior beliefs are largely shaped by prior beliefs, making it challenging to identify the generative model by simply observing systemic dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3876.264,
    "end": 3886.171,
    "text": "To this end, it is necessary to commit to a particular form of the generative model and elucidate how posterior beliefs are encoded or parameterized by the neural network states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3887.355,
    "end": 3902.618,
    "text": " This work addresses these issues by establishing a reverse engineering approach to identify a generative model implicit in a canonical neural network, thereby establishing one-to-one correspondences between their components.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3903.579,
    "end": 3916.55,
    "text": " Remarkably, a network of rate coding models with sigmoid activation function formally corresponds to a class of POMDP models, which provide an analytically tractable example of the present equivalence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3917.571,
    "end": 3921.114,
    "text": "Please refer to the previous paper, Cytation 22, for further discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3922.155,
    "end": 3931.243,
    "text": "So some of the analytical details, especially on the inferential side, cognitive side,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3933.487,
    "end": 3945.297,
    "text": " burr captured in the earlier paper this paper goes further into mapping the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3947.796,
    "end": 3975.716,
    "text": " potentially necessary and sufficient aspects of the particular entity which is to say the risk minimizing features of the autonomous states with respect to the entire particular states including sensory states connecting that back of the envelope verbally expressible formulation to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3977.566,
    "end": 3980.708,
    "text": " some complete class of neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3981.529,
    "end": 3991.075,
    "text": "So what's outside of the complete class and why, and then what groups within the class have special or different features.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3993.577,
    "end": 4000.521,
    "text": "It is remarkable that the proposed equivalence can be leveraged to identify a generative model that an arbitrary neural network implicitly employs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4001.342,
    "end": 4007.226,
    "text": "This contrasts with naive neural network models that address only the dynamics of neural activity and plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4009.693,
    "end": 4024.72,
    "text": " If the generative model differs from the true generative process that generates the sensory input, inferences and decisions are biased relative to Bayes' optimal inferences and decisions based upon the right sort of prior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4026.601,
    "end": 4030.823,
    "text": "In general, the implicit priors may or may not be equal to the true priors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4031.263,
    "end": 4034.265,
    "text": "Thus, a generic neural network is typically suboptimal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4035.425,
    "end": 4041.069,
    "text": " Nevertheless, these implicit priors can be optimized by updating free parameters e.g.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4041.109,
    "end": 4041.97,
    "text": "threshold factors,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4043.442,
    "end": 4057.685,
    "text": " by psi, based on the gradient descent on cost function L. By updating the free parameters, the network will eventually, in principle, become Bayes optimal for any given task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4058.866,
    "end": 4067.948,
    "text": "In essence, when the cost function is minimized with respect to neural activity, synaptic strengths and any other constants that characterize the cost function",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4070.68,
    "end": 4075.965,
    "text": " the cost function becomes equivalent to variational free energy with the optimal prior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4081.11,
    "end": 4090.699,
    "text": "So the cost function for the neural network activity and synaptic strengths underlying the loss function",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4092.426,
    "end": 4110.807,
    "text": " are equivalent to the kind of gradient descent enabled variational free energy minimization under the base optimality scenario from a risk perspective",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4112.452,
    "end": 4124.276,
    "text": " Simultaneously, the expected risk is minimized because variational free energy is minimized only when the precision of the risk, gamma, is maximized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4125.757,
    "end": 4128.498,
    "text": "See method section generative model for further details.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4130.478,
    "end": 4131.199,
    "text": "Very interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4134.019,
    "end": 4134.76,
    "text": "They then say...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4135.899,
    "end": 4141.265,
    "text": " The class of neural networks we consider can be viewed as a class of reservoir networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4142.166,
    "end": 4145.509,
    "text": "Citation 54, citation 55, here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4147.351,
    "end": 4150.915,
    "text": "The proposed equivalence could render such reservoir networks explainable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4152.321,
    "end": 4166.467,
    "text": " and may provide the optimal plasticity rules for these networks to minimize future risk by using the formal analogy to variational free energy minimization under the particular form of POMDP models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4167.628,
    "end": 4175.751,
    "text": "A clear interpretation of reservoir networks remains an important open issue in computational neuroscience and machine learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4178.419,
    "end": 4192.672,
    "text": " So from Wikipedia, reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed nonlinear system called a reservoir.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4194.414,
    "end": 4204.163,
    "text": " After the input signal is fed into the reservoir, which is treated as a black box, a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4205.644,
    "end": 4207.466,
    "text": "Then there's two key benefits of this approach.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4208.667,
    "end": 4235.702,
    "text": " the first key benefit of this framework is that training is performed only at the readout stage as the reservoir dynamics are fixed the second is that the computational power of naturally available systems both classical and quantum mechanical can be used to reduce the effective computational cost here's some stable diffusion reservoir computing active inference neural network",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4240.819,
    "end": 4248.222,
    "text": " So this would be interesting to discuss, and I remember some Act-Inf Institute participants who are specifically interested.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4250.903,
    "end": 4253.304,
    "text": "Empirical analyses and hypotheses.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4254.905,
    "end": 4265.929,
    "text": "They write, the equivalence between neural network dynamics and gradient flows on variational free energy is empirically testable using electrophysiological recordings or functional imaging of brain activity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4267.446,
    "end": 4270.948,
    "text": " So, then another summarization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4271.208,
    "end": 4286.538,
    "text": "Crucially, the proposed equivalence guarantees that an arbitrary neural network that minimizes its cost function, possibly implemented in biological organisms or neuromorphic hardware, can be cast as performing variational Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4287.298,
    "end": 4293.122,
    "text": "So, to state it a few more times in the final paragraph.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4295.276,
    "end": 4303.502,
    "text": " In summary, a class of biologically plausible cost functions for canonical neural networks can be cast as variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4306.021,
    "end": 4310.383,
    "text": " Formal correspondences exist between priors, posteriors, and cost functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4311.163,
    "end": 4317.046,
    "text": "This means that canonical neural networks that optimize their cost functions implicitly perform active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4318.166,
    "end": 4325.67,
    "text": "This approach enables identification of the implicit generative model and reconstruction of variational free energy that neural networks employ.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4326.99,
    "end": 4328.191,
    "text": "This means that in principle,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4328.951,
    "end": 4336.417,
    "text": " Neural activity, behavior, and learning through plasticity can be predicted under Bayes' optimality assumptions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4341.64,
    "end": 4343.322,
    "text": "There's a code availability statement.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4343.522,
    "end": 4352.508,
    "text": "The MATLAB scripts are available at GitHub in the repo of the first author, and it would be awesome",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4353.909,
    "end": 4377.364,
    "text": " the author or anyone to bring this working matlab script up and see if we could do some real-time active inference then as mentioned earlier from the roadmap the methods are following the discussion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4379.662,
    "end": 4400.615,
    "text": " i'll just show the equations but not go into any details because there's not time nor familiarity so those who have more of one or the other would be welcome to fill in some dots because this is a really awesome and important paper so i hope that it can be",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4402.443,
    "end": 4417.471,
    "text": " interpreted and critiqued and elaborated on and so on by those with familiarity in both sides of that free energy loss function equation one",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4419.901,
    "end": 4421.462,
    "text": " Generative model section.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4421.762,
    "end": 4422.743,
    "text": "Many details are provided.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4423.243,
    "end": 4424.404,
    "text": "Equation 10 is shown.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4424.424,
    "end": 4427.386,
    "text": "It's a larger unpacking of the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4429.727,
    "end": 4431.108,
    "text": "Section variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4431.408,
    "end": 4432.509,
    "text": "Many details are provided.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4432.909,
    "end": 4436.412,
    "text": "Equation 11, 12, 13, 14, and 15.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4439.994,
    "end": 4441.415,
    "text": "Section inference and learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4442.175,
    "end": 4443.036,
    "text": "Details are provided.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4443.536,
    "end": 4443.997,
    "text": "Equations 16, 17, 18.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4448.971,
    "end": 4451.212,
    "text": " Then section on neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4451.692,
    "end": 4453.993,
    "text": "So there was just some interesting writing here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4454.213,
    "end": 4455.054,
    "text": "So I wanted to highlight that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4455.954,
    "end": 4465.838,
    "text": "In this section, we elaborate the one-to-one correspondences between components of the canonical neural networks and variational bays via an analytically tractable model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4466.259,
    "end": 4471.381,
    "text": "So that's the figure three that we've been returning to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4473.809,
    "end": 4481.712,
    "text": " Neurons respond quickly to a continuous stimulus stream with a timescale faster than typical changes in sensory input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4482.633,
    "end": 4491.537,
    "text": "For instance, a peak of spiking neurons in the visual cortex of V1 appears within 50 and 80 milliseconds after a visual stimulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4492.497,
    "end": 4492.917,
    "text": "Citation 62-63.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4494.958,
    "end": 4504.183,
    "text": " which is substantially faster than the temporal autocorrelation timescale of natural image sequences, about 500 milliseconds, citation 6465.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4504.243,
    "end": 4507.465,
    "text": "So that's pretty interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4509.266,
    "end": 4513.929,
    "text": "What is the temporal autocorrelation timescale of natural sequences?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4517.048,
    "end": 4525.002,
    "text": " What timescale do neural firing and neuroplasticity, etc.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4525.262,
    "end": 4526.945,
    "text": "processes actually occur at?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4528.823,
    "end": 4547.849,
    "text": " and when might some type of function at a given time scale be understood to be functional or not thus in other words downstream of the fact that neurons respond quickly",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4549.83,
    "end": 4558.914,
    "text": " at a time scale faster than typical changes in sensory input, we consider the case where the neural activity converges to a fixed point given a sensory stimulus.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4560.295,
    "end": 4566.538,
    "text": "We note that the present equivalence is derived from the differential equations, equation six, but not from its fixed point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4567.478,
    "end": 4572.221,
    "text": "Thus, the equivalence holds true irrespective of the time constant of neurons.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4573.361,
    "end": 4576.683,
    "text": "To rephrase, neural networks with a large time constant",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4578.134,
    "end": 4588.104,
    "text": " formally correspond to Bayesian belief updating with a large time constant, which implies an insufficient coverage of the posterior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4590.612,
    "end": 4614.484,
    "text": " Pretty interesting related to learning rates and Bayesian updating rates and all of the nooks and crannies of Bayesian inference and the challenges associated with dynamic, uncertain, rugged, fitness, and free energy landscapes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4616.761,
    "end": 4628.946,
    "text": " these optimization challenges, which are addressed differently methodologically, culturally, et cetera, in the variational Bayesian and in the neural cases,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4631.932,
    "end": 4655.772,
    "text": " they have to deal with time and so all of those different nooks and crannies mentioned like catastrophic learning catastrophic forgetting simply memory anticipation attention local maxima choosing when to play all these higher order questions",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4660.14,
    "end": 4675.146,
    "text": " are connected here does that make it what kind of a problem space now or just what kind of space pretty interesting and equation 19 20 21 22 23. they have some more details on the simulation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4684.102,
    "end": 4686.103,
    "text": " Maybe we could see the simulation go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4687.023,
    "end": 4690.664,
    "text": "And in the last section, data analysis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4691.384,
    "end": 4700.786,
    "text": "So this is kind of returning to that point about the time constants in Bayesian and neural network systems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4702.007,
    "end": 4707.228,
    "text": "When the belief updating of implicit priors, D and E, is slow in relation to experimental observations...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4708.314,
    "end": 4721.918,
    "text": " D and E, which are encoded by phi and psi, can be viewed as being fixed over a short period of time, as an analogy to homeostatic plasticity over longer timescales, 66.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4723.299,
    "end": 4726.18,
    "text": "Homeostatic plasticity in the developing nervous system, 2004.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4729.481,
    "end": 4735.842,
    "text": "Very interesting in light of our recent discussions on allostasis and other topics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4737.003,
    "end": 4737.203,
    "text": "Thus,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4739.076,
    "end": 4749.621,
    "text": " Phi and Psi can be statistically estimated by a conventional Bayesian inference or maximum likelihood estimation given a flat prior based on empirically observed neuronal responses.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4750.401,
    "end": 4753.883,
    "text": "In this case, the estimators of Phi and Psi are obtained as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4755.424,
    "end": 4758.105,
    "text": "Nice equation number 24 mentioned earlier.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4760.946,
    "end": 4764.668,
    "text": "So that will be definitely one to look into more.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4768.15,
    "end": 4768.35,
    "text": "Well,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4769.986,
    "end": 4774.401,
    "text": " I hope you found this a useful and interesting .zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4776.946,
    "end": 4800.628,
    "text": " I'm looking forward to the discussion with the author and again any other Institute participants or those with knowledge or strong feelings to express about neural networks active inference applied active inference computational modeling of perception cognition and action",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4802.566,
    "end": 4813.309,
    "text": " neural networks in the wild, AI ethics, all these different areas can hopefully have a nice discussion in 51.1 and 51.2.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4813.429,
    "end": 4815.569,
    "text": "That'll be the last paper streams for 2022.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4815.629,
    "end": 4821.231,
    "text": "There will still be a few guest streams upcoming.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4821.251,
    "end": 4823.491,
    "text": "So those discussions will close the paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4831.873,
    "end": 4850.226,
    "text": " focused prepared live streams for 2022 and yeah if you want to be more involved with live streams whenever you're listening to this join or recommend someone to join or help in any number of other ways",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4851.946,
    "end": 4855.429,
    "text": " Just listening and learning is awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4856.17,
    "end": 4867.841,
    "text": "And we also really look forward to those who want to help make some of these connections that are latent and sometimes exposed in these papers and conversations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4869.162,
    "end": 4890.69,
    "text": " and with a few motivated people who want to connect for example to the neural network communities and those who can facilitate discussions on these topics that would be awesome just using my final thoughts on this dot zero",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4891.59,
    "end": 4908.327,
    "text": " because it's always great to have others also join in the preparation for the dot zero so just want to add that note on this rare solo stream so thanks again looking forward to talking or seeing you later bye",
    "speaker": "SPEAKER_00"
  }
]
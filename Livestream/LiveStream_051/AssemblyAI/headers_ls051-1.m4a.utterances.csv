39287	178212	A	0.9159334519572949	All right, hello everyone. Welcome. This is ActInf livestream number 51 one. We are in the second discussion of this paper, canonical Neural Networks perform Active Inference. Welcome to the active inference institute. We're a participatory online institute that is communication, learning and practicing applied active inference. You can find us on this slide and this is recorded in an archived livestream. So please provide us feedback so we can improve our work. All backgrounds and perspectives are welcome and we'll follow good video etiquette for live streams, head over active inference.org to learn more about the institute and how to participate in projects and learning groups. All right, we're in ActInf livestream number 51 one, and having our first nonsolo discussion on this paper, canonical Neural Networks perform active inference and really appreciative that you've joined today. It's going to be a great discussion. We'll begin with introductions. I'll say hello and then please just jump in however you'd like. And we can start by setting some context. So I'm Daniel, I'm a researcher in California, and I was interested in this paper because we've been talking a lot about active inference from a variety of different perspectives, from the more fundamental math and physics to some applications, philosophy, embodiment, all these really interesting threads. And this paper seems to make a really clear meaningful contribution and connection by connecting active inference entities and this approach of modeling to neural networks which are in daily use globally. So thought it was a fascinating connection and really appreciate that we can talk about this today. So to you and welcome. Go forward, Takuya, however you'd like to introduce and say hello.
178575	220962	B	0.8259454166666667	Yeah. Hi. I'm Tafia Isomura, neuroscientist in Lique Brain Science Institute in Japan. I'm particularly interested in universal characterization of neural network and brain using mathematical techniques. So this work I believe important as a link between active brain forest aspect, Bayesian aspect of the brain, and the dynamics system aspect of the neural network. So I'm very happy to join this discussion session. Thank you for invitation. Nice to meet you.
222000	239712	A	0.8979862857142855	Nice to meet you as well. The first thing you added, the universal characterization of neural networks. What is the universal characterization of neural networks? Why is it being pursued in this area of research?
241200	342250	B	0.8266106081081082	So, as a narrow sense, my gain aim of this paper is that so, you know, people active inference lab communication to characterize brain activity, behavior, so on, so on, but which would be different from conventional neural network. So there is a crossover program which is associated with conventional neural network and it is not very clear whether all characterization of computational neural network can be explained by activity infrastructure principle or not. So here universal characterization means that characterization of every aspect of conventional neural network which is a kind of dynamics system derived as association between biological phenomena and simple mathematics. Car formula using gift card, using differential equations as the broad sense. I think universal characterization means that well, it is a characterization of brain intelligence, but it's a big picture and the paper particular address is only one aspect of the Bull picture.
346200	371487	A	0.9167828301886792	All right? So it'll be great to pull back to really understand what synthesis is happening. So I'm going to ask what makes a neural network model a neural network model and what makes active inference lab model an active inference model? Is this synthesis and connection you've made true? Because of what?
374175	422187	B	0.7889137333333331	Because basically what we show is the mathematical equivalence between the formulation of canonical neural networks and the formulation active inference lab in the sense that we show that as possible neural networks can be characterized by minimization of some biological plausible cost function. And we show that that cost function can be least as variational based on inference and a particular cross of gentlemen model in terms of well known partially observable position process.
426000	447087	A	0.9075211111111111	Alright, shall we perhaps walk through some of the sections of the paper? It would be awesome. Just for each of these sections, maybe the numbered and the lettered sections. What does the section aim to show and why was it there in the paper?
457125	589150	B	0.7797235502958585	Briefly it's over, right? So briefly. So first we introduce so the gain issue main program, our interest, which is relationship. We try to make a formal ring between neural network and active reinforcements that gain program background. And then we first formulate the equivalence, mathematical equivalence, in a very Brea manner. So in the first section in results, we formulate the relationship using complete craft serum, which is well known statistical theorem proposed very long time ago. And using that we link a general form of neural network with a general form of variational data impress. But a problem is that this characterization does not address a specific generative model which is crucial to characterize a specific model, specific neural network dynamics. So in the following sections, we characterize the problem using Pomodb or partially observable Markosition process and link that model with a particular class force canonical neural network. And then we simulated we use the simulation to propagate that property in terms of some major tasks.
595000	614137	A	0.9720179545454546	All right, thank you for this. Could we talk about the complete class theorem? So what is the scope of the complete class theorem and why was it the relevant set of the neural networks to pursue or the right way to frame it?
615100	768350	B	0.8109271120689655	Thank you for asking that. So I like the slide you showed last week's video. So computer cross theorem basically indicates the relationship between some crossover decision rule and vision in France. Here a crucial keyword is admissible decision rule, which is a rule which is as good as other decision rules or at least at one point better than other decision rules. So simply speaking, adomissibility indicates in some sense it is the best rule for some aspect. And usually we characterize such a goodness using cost function, loss function or risk function. And here what we did is we established some association with this type of loss function or risk function with canonical neural network which is we call cost function or biotic roles Costa function or neural network. So our assumption is that neural network minimize cost function. So if it active the inclination and it is virusly active some sort of optimality so we can say it is adommissible with respect to that cost function. So the beauty of complete cross theorem is that if we find some admissible decision rule then automatically we can say that it is based on inference in terms of some Bayesian Costa function with gentlemen model a priori beliefs. So this computer chaos theorem is crucial as abstract characterization of the relationship between conventional neural network architecture, dynamics and variational. Beijing influence.
771650	780287	A	0.9372515789473683	All right, thank you. What does it mean when you said it was biologically plausible of a loss function?
781925	836175	B	0.8424660273972605	The term is a little bit arbitrary because in this paper we mean by probability in the sense that this neural network model can be derived from realistic neural model through some approximation. And so here barricade probability, suggest means probability as a neural model or synaptic processing model. And if this cost function loss function can derive such a plausible algorithm, then we can say that this cost function is barely plausible.
839525	847862	A	0.9423009523809525	So what is the distinction between those neural and synaptic components in the loss function or what equation to look at?
849500	854035	B	0.7382042857142856	You mean distinction between dynamics and synaptic?
854155	858975	A	0.978892	Yeah. What is the distinction between them and how is it represented in the equations?
860525	959625	B	0.812328359375	Okay, basically neuropathivity equation means differentiate equation about a variable that represents firing intensity or some sort of variables associated with the firing. On the other hand, dusty equation means an update rule about the synaptic weight or synaptic strengths which is a connection between two neurons. And beauty of this formulation proposed in this paper is that we characterize both heuristic equations synaptic procedure equations in terms of gradient descent on a same cost function, common cost function. So we can say that if we consider the partial derivative of some cost function with respect to new activity, then it's derived by gradient descent rule about if we consider a partial derivative of chaos function errors with respect to synaptic weights, then we derive a prosthesis rule.
970187	978312	A	0.9200683333333335	Are those the only two aspects of a neural network or why are those the two key aspects?
980837	1060362	B	0.7943356115107912	It is a main, I think it's the main body of the neural activity. If we consider some inference running or action exhibit by neural networks in the sense that neural activity correspond to fast dynamics, fast gradient dynamics mix and scientific processes indicate through dynamics that minimize least function and cost function. But in general, we can consider any aspects, any variables associated with your method. For example, at least what we show in the paper is any free parameter which may be associated with firing threshold or although we don't discuss in this paper it would be possible to add other variables related to neural network. For example, here we ignored contribution of Griad factor but it would be possible to add the Griar factor in this correlation or any other aspect of virus corporate neural network.
1064612	1141362	A	0.9290445070422537	That's very interesting and it speaks also to a general separation of time scales. For example in different multi scale systems or in the renormalization group where it's describing some minimal multi time scale system where the faster time scale can be seen as perception like a slower time scale can be seen as more learning like. And then in some hierarchical model what's learning of one time scale can be perceptual for a slower time scale? So it's a very nice generalization. Are there any examples of decision rules that will help us think about the action components of what the neural network is doing? Because it may be more familiar to think about digit characterization and image classification, some kind of classical tasks for neural networks. But how does the decision rule play out in the context of neural networks?
1144187	1222362	B	0.845156636363636	Okay, so in this paper we basically assume a closed loop so comprising a neural network part and environmental part. So Neuron receives sensor input from environment and provide some feedback to the environment. Even with the example of classification, we can say that output correspond to classification output, which is kind of generative model relevant. Example would be, for example, controlling agent like a robot control or any kind of control errors. Decision making tasks. For example, when we encounter some choice tasks, we need to advertise, for example, left or right or something. Any kind of such a decision can be associated with the admissibility or admissible decision.
1227050	1238175	A	0.8840135294117647	So what would an example of an inadmissible or admissible strategy be in the decision making task?
1240850	1286025	B	0.8184155813953489	Admissibility usually characterized by loss function or risk function. Here admissivity indicates that there is another decision rule which is at least one point better than the forecast decision rule. Simply speaking in Adobe CBD indicates that decision rule is not good relatively.
1287050	1311000	A	0.9280346666666666	Let's just say our decision rule is we always turn right. Is that an example of a decision rule? Because there might be settings where that is strictly effective and the simplest rule whereas there's other settings where that's going to be tragic. So what does it mean to be admissible for an agent in light of different environmental contexts?
1312250	1355625	B	0.8215345454545456	That's an interesting point. So even with such a too much simplified rule it can be admissible under some particular situation, particular loss function. For example, the rulers that always turn right maybe the best under some situation, right? So the relationship of admissibility or enough adommissibility depends on both agent characteristics and environmental characteristics.
1359350	1362300	A	0.9617340000000001	What aspects of the environment.
1364975	1389225	B	0.7843013793103447	For example? For example, if that decision group matches the structure architecture of environment then maybe that decision always downright active the shortest past under some situation, some environment.
1391675	1408500	A	0.9148510810810812	How does this admissibility help us think about like overfitting and how does it help us think about the way that different practices are used for neural networks to prevent them from being over fit in practice?
1410725	1527662	B	0.7555904999999998	Well, strictly admissivity is characterized with the Bayesian risk. We cannot observe a hidden states of the environment, only we can observe is a part of the entire universe. So the question is an important question is what is the best choice under such a limited information? Limited information? So this Bayesian list adoissibility or computer credit theorem tell us that well known, only the well known Bayesian framework achieved the adommissible decision. Which means that in this aspects Bayesian optimization give us a least choice strategy, otherwise we overfit or find the suboptima evolution. So it's a nice association, nice linkage between the decision, but is a good decision about the decision and more established statistical inference. Freedom work.
1531137	1660237	A	0.924325327102804	Thank you, that's very helpful. So we're reducing our uncertainty and risk about hidden states in the environment. So in the special case where the entire environment is observable without errors like a chess game, then there's an equivalence between correlation of risk or loss on observables or on hidden states. But they're not really hidden, but they are environmental states. Whereas any amount of uncertainty in the mapping between observations and hidden states, which is usually shown as a in the partially observable Markov decision process, any amount of uncertainty about unobserved or partially observed environmental states enables you to fit your uncertainty optimally about that hidden state and fit that uncertainty simply with the gradient descent. And by doing so, you don't overfit a model of observables, which might be the fallacy or the issue with simply doing descriptive statistics you might get an infinitely small variance with a frequentist estimate because you have 1000 data points. So the variance from a descriptive statistics perspective might be very small. I think it speaks very much to why neural networks are useful in practice from training with limited data sets because that's an empirical observation that they don't entirely over fit. But also I'm sure there's ways to construct them that are overfit.
1661112	1714422	B	0.8293352631578949	Yeah, overfit will occur if we select some optimal priorities. For example. Well, I'm not sure if it is overfit in the sense what you mentioned because if we select some priorities then the Bayesian function itself changes and the neural networks that try to fit to that Costa function. So cost function minimization will be achieved agent such a situation. But that solution is not good for our original help us. That's the tricky part.
1714605	1777000	A	0.9309825	Yeah, that is reminiscent of some discussions we've had discussing like driving off a cliff or blowing up is also reducing free energy. Like dropping up a building reduces your potential energy. And so there are potentially decisionmaking or strategic trajectories that do for some time horizon minimize free energy, perhaps even or maybe even guaranteed better than some longer time horizon. Because if the shortterm strategies were somehow better than the longterm horizon. It would be difficult to imagine because the long term horizon would be at least as good as a shortterm strategy. So that speaks to the challenges of planning in action. So how is planning addressed in modern neural networks and how does this work help us think about that?
1779837	1876837	B	0.8193560396039608	That's another very important aspect. I have to say that this framework addresses planning aspect, but that planning is not necessarily the optimal solution in the sense that what we interested in is optimization or learning under limited structure. The structure is characterized by here Prosperia neural networks. So yeah, planning occurred by association between risk in the future and our decision in the past. Here we model that aspects using delayed moderation of scientific activity mediated by some neuromodurator or neurotransmitters. This is the model. This is model as the risk factor and the heavy product holding the neural network.
1881987	1906537	A	0.9149541666666665	All right, I'm going to ask a great question from the chat and then we'll look at the figures a little closer. So ML Don wrote a question stuck in my mind for a long time. Could you please put it to rest? Do we need to have knowledge about all states possible actions and sensory inputs for active inference?
1910812	1953800	B	0.8732573913043477	Well, you mean if you seek the exact solution, exact optimal solution, then maybe more information would help you to find that. But under some ideal assumptions then the is not necessary to achieve the optimal solution. I'm not sure if I correctly answer your point.
1955512	2012225	A	0.9260123300970876	So just to restate it. Of course, knowing all the state's possible actions and sensory inputs, it's not a bad thing. Worst case, there's some computational complexity, trade offs, but the problem becomes fully stateable. But I think ML Dawn is asking about cases where you don't know all of the state spaces or potentially even the dimension or the semantics of hidden states, active states, sensory inputs and why not even add cognitive states? So in not just partially observed but partially known state spaces, how are these address in neural networks and how does active inference help us think about it?
2017762	2037565	B	0.8489134999999999	Okay, I think the question is about how can we separate those states? Like sensory function interface entorhinal, how can.
2037582	2058880	A	0.9306230769230768	We separate not just in principle have these states be separated, but deal with the fact that some of these states we might have good knowledge on and some states like the hidden states we might not even know, like we don't know the dimension of the cause vector in the world.
2059077	2166125	B	0.7983218881118882	I see. In terms of dimension, there is a statistical technique to estimate the dimensionality, for example via information criteria like I agent information criteria, based information criteria, all them try to info estimate plausible dimension about the environmental hidden states. There is an analogy with those information criteria and version of free energy minimization. So with version of free energy inclination we can identify the plausible model structure which in principle involves the dimension aspect. But in terms of Neural network in this paper we don't carefully consider about the dimensionality optimization because we first define the number of neurons and don't change during the training. But in principle we can consider the change in the number of neurons which is associated with the neurogenesis adult neurogenesis or development during the developmental stage. That would be an important expansion of this direction.
2173450	2303750	A	0.9168540358744397	That's very interesting. Here's a remark. Well, one note is equation one summarizes a lot of what you've been describing. There's a parallelism or a concordance being drawn between the loss function of Neural networks and the variational free energy of the parameterized model there. So to come back to these processes that influence learning which we could think of as the Neural network becoming more fit from a loss function perspective or the variational Bayesian partially Observable Markov decision process entity generative model encoding better at doing what it does. So there's the firing rate on the Neural network side, the synaptic plasticity at a slower time scale which we discussed a little earlier. And then now there's a third time scale with the birth and death of new cells and maybe even new layers. And that kind of multiscale temporal structuring is not intrinsic to the Bayes graph to represent multiple nested timescales in a Bayesian graph in the act of inference literature it's more common to make a hierarchically nested model, right? And just say that the time handling on one level is happening more rapidly with respect to clock time than deeper nested, slower models. Whereas the Neural formulation allows us to deal with multiple ongoing active states without appealing to hierarchical nesting, which is a very important feature.
2311450	2382787	B	0.7903279999999997	Well, both distinctions will be possible. So without hierarchical or with higher car modeling so even with hierarchical modeling, the optimization of dimensionality should be possible. It would be possible. But in other distinctions we can consider that a population of Neural models so one has a single layer, another has two layers, three layers, four layers. And consider the probability of network architectures associated with Costa minimization and a particular environment which is in principle have the same computational architectures with the hierarchy model.
2386800	2440900	A	0.9146544303797468	Very interesting. Yes, perhaps I over generalized or speculated because I thought about how one could have a 100 timestep POMDP that also performs multiscale behavior potentially extremely wastefully, but at least it could in principle. And similarly, within a neuron there could be another Neural network or some other structure approximated by that. So they almost both enable hierarchical and non hierarchical model modeling as you described, but in very different ways that lead to very different implementations.
2444025	2444787	B	0.79544	Yes.
2445900	2468500	A	0.9402777499999999	I think this brings us to the topic of forward and reverse engineering. So you talked a lot about reverse engineering. What is reverse engineering and what is forward engineering and what has been done in these areas of engineering?
2470350	2595250	B	0.7869754140127388	Okay, I'm not an expert in this process, but I believe that liver here means your characterization of the blueprint of some device or machine from data observable information like activity or action behavior of some agent. Goal is identification of blueprint and the crucially here blueprint correspond to generative models because once we define generative model, we can Deneve evolution, anthropology algorithm, running inference algorithm and any behavior of the agent. So here reverse means that we first observe some activity of agent and its mechanism is still unknown for us, but we can estimate its mechanism using that activity by identifying the most plausible guarantee model which can minimize some Costa function or risk function when we feed the data to the model. So, on the other hand, for the engineering would be more mainstream, way fast defined model blueprint gently model then drive everything including parasite functional running algorithms and behavior action prediction algorithm.
2599875	2628250	A	0.8945324999999997	So, by reverse engineering neural networks, we're observing some already parameterized neural network and then fitting a POMDP to it. To what extent is it possible to take a given POMDP and create a neural network that performs that inference?
2632875	2800062	B	0.7853038383838382	Okay, in this paper or in the following paper, what we consider is a strategy that we first feed empirical data whether force neural response data to BioScale prosper neural network model which is similar to a conventional model fitting approach where we have differential equation data and differential equation to explain the behavior with the minimum prediction. So now, a virtue of this framework we established is that we can naturally transform such neural network architecture with the very known partially observable markup action process architecture. Because for any kind of canonical neural network there is a cost function. So we Deneve cost function through neuroactive decision which is opposite with the conventional way we define cost function derived algorithm and then we use the formal equivalence between neural network Costa function and variant queen energy. So now transform the journal architecture to Beijing model architectures and once we characterize vital energy, there should be some general that define that informational energy functional. So in particular, in this example, canon network nicely correspond to well known across macquarlin process. So, by using this procedure, we identify a plausible home DP architecture which correspond to observed activity data.
2809337	2855862	A	0.9156324705882349	Well, let's stay on this last point. So, after all those transformations, first the measurements of neurons using that data to fit the neural network and then by virtue of the relationships unpacked in the paper, transforming the neural network in the left side of figure one into a particular form of the P-O-M DP. So first, what are the constraints on that form of the P-O-M DP? Is this a little corner of model space or what are the space of acceptable P-O-M DPS?
2858012	3096812	B	0.8295823717948713	That totally depends on what kind of neural network model you are considering. So for example, in this paper we discussed about a particular crossover from DP in which each state takes either zero or one. So it's very restricted compared to the general form of homedp. But we consider a factorization so in the sense that although each but we consider a vector of observation, a vector of hidden states where each element correspond to one single one hot vector but as an entire state it can represent high dimension discrete state space. And this architectures nicely correspond to neural network architectures because usually each neuron takes either zero one or some value continuous variable between zero and one. So we use this association to characterize a particular OMDP which correspond to neural networks, and this follows a particular mini field approximation, approximation or approximation in generative model because we associate posterior belief in this particular homo DP with the neural activity, which means that posterior of action also has a factorization architecture in the sense that we don't fully consider about the second order statistics between neurons activity and activity, which is outside of this poem. RASM. So each neuron activity correspond to posterior expectation about a particular element of the state and we don't consider the joint posterior property of all state. So although this is a implication, we see this Asia impress, but otherwise, for example, we can consider any recurrent network architectures which correspond to state to transition metrics and it would be possible to extend this architecture to higher call structure in the sense that it is straightforward. Consider a tree structure or any kind of higher car structure by assumptions that some neurons connect to other neuron but not connect to other neurons. So this is Lamme as considering the higher car structure in general.
3103675	3272787	A	0.9193772390572393	That's very interesting. It's commonly remarked in the base graphs that they represent the connections amongst random variables and there's a relationship between their computability and their sparsity. The sparsity structure as in which variables do or do not influence each other makes the problem tractable through factorization and just kind of conceptually like if every one of a thousand variables or an unknown number of large variables if it was all by all the number of parameters to fit on that connectivity matrix would be very high. So statistical power would be very low for any given edge. Whereas the more and more constrained you make the connectivity of the variables, the more statistical power you have to resolve or kind of spend on fitting those edges like in a structural equation. But you might be losing sight of the unknown unknowns by constraining yourself to a very limited or fallacious topology of the variables. So there's this kind of structure learning statistical inference question in the Bayes graphs then on the neural side from the biological much of neuroscience is about understanding how the firing rate, connectivity patterns and other factors Hohwy the structure of those neural systems and their function like form and function enable adequate inference and inference on action. So it's like in both of those areas or really like in neural network artificial and neural networks and in variational. DAGs the discussion is about how the structure and the fine tuning work together to generate function and about some of the statistical or biological challenges of balancing different needs while also constraining the cost in terms of materials and biometabolism. So it's a very rich interoception that is being explored here. If these models can really be moving back and forth.
3278137	3282587	B	0.8248071428571428	In the sense that back and forth.
3285487	3319412	A	0.8871647887323947	Moving back and forth, like there's some imprints of the model that is implementation independent or like some interlingua or some semantics or compatibility, I don't really know. I mean, that's something we can explore is like what is it that is such that one could forward engineer and then reverse engineer and have like kind of an expectation maximization between these two areas. So what is it that's being solved?
3325087	3547175	B	0.8000598972602742	Yes, important point, for example, about you is that we can use the knowledge of Bayesian inference to explain your activity dynamics, which is crucial because people often say that characterizing neurodynamics is no straightforward, we may obtain some solution on your net dynamics, but the meaning of that dynamics in terms of the functional aspect is very unclear. We don't know the meaning of connectivity strength matrices and what is the learning of the threshold factor, so on and so on those de Vries from the modern physiological phenomena. But it is not necessary to have clear linkage to functional exploration. So explanation of function of the brain. But once we transform translate this dynamics into Bayesian inference, then we can explain every functional aspect of the neural network diagrams architecture in terms of where established Bayesian inference under a particular crossover Bayesian model, in this case palm DP model. So now it turns out that synaptic strength correspond to a matrix B matrix, which are very established culture meaning. So yeah, this is useful to explain neuronsynatic property in terms of established statistics. Also, for the people in active inference lab site, it would be helpful to understand the neuronounce master straight about particular active interface model model. So I think it related to forward modeling. But finally to discuss with discuss about the border service rate of that forward model, we need to address the neural network architecture service property. So in that case, we can transform a particular force DP invasion modeling to a neural network architecture using this relationship and then get prediction about the substrate. So if we have this based on model, this particular quantity in this model should be it would be possible using.
3554737	3557987	A	0.9291874999999999	Oh, it's all good. Can you just repeat the last 20 seconds?
3558637	3594125	B	0.7879652	Yes. So in the last part I mentioned about first we define the Bayesian model and then can predict what is the neural net substrates that correspond to that particular Beijing model. So this will be useful to identify the biological quantities that correspond to a quantity in Beijing. Chaos.
3603562	3720287	A	0.9064691017964069	There's a lot there. It makes me think about the inference of implementation and. Heuristics in the computational setting, which is often in the extreme disembodied, and the biological setting, which is in the extreme entirely embodied. And for a given generative model, the kinds of computational heuristics that can be applied include a whole host of different strategies ranging from sampling to tree exploration and branching to paralyzing the data architectures and all these other kinds of disparate strategies and software packages and implementations. But on the biological side, what is needed is something that's very simple but also very inscrutable, which is a given pattern of interactions must embody that calculation. So that might mean that it can add three digit numbers, but it can't add two digit numbers under some constraints. But what isn't accessible to that kind of morphological, biological or like form and functional computing, what's not accessible are the tree branching, the database decentralization, like they're a different set of heuristics.
3721012	3721372	B	0.57	Right.
3721405	3736550	A	0.9053416666666664	But they're both very useful when we're thinking about making sentience artifacts or benefiting simply from the explainability across both sides of this figure.
3738562	3789952	B	0.7651597183098592	Yeah. So you now address an important point. So Homistry, it is very nontrivial whether there is a corresponding valve car architecture force any given Bayesian architectures. I believe it is impossible to design biography architectures to respond to arbitrary Bayesian architectures. So only a limited aspect of region model can be implemented in a vertical plausible manner. And that point is crucial as capitalization of biological network. Biological brain. Yeah.
3790045	3796112	A	0.8771842857142859	Wow. Well, just to kind of touch again on this forward in reverse engineering.
3798412	3798727	B	0.55183	For.
3798745	3974687	A	0.9282649552238806	A given POMDP if we're willing to compose it within a certain class, which might be quite general still, but some class of PMDP, as written. On the paper. We may be able to have a neural network architecture that would be very amenable to deep learning, low energy computing, pretraining various features. And then on the other side, for a given artificial neural network that we come across in the wild or a model of neural dynamics that we fit using a neural network model. So something in a neuroscience laboratory that model can have interpretability corresponding to the variables of a given POMDP. And just to kind of give one more point on how that's going deeper than, for example, statistical parametric mapping SPM. So let's just assume that the neural network we're dealing with is fit from brain data from some lucky Kant, right? Now, what would be possible or prior to this line of work or without this line of work, one could fit a neural dynamics model and then do all kinds of analyses, like power analyses on the different frequency spectra and say, look at the average firing rate or the correlation coefficients of firing rate. So fit the firing rates and the synaptic Plasticities and store all that data. And then we could just pick a POMDP that we've seen in the literature without any reference to the neural network and optimize the POMDP. And then we could say well, it turns out that when the POMDP o is high there's increased theta power in this firing pattern. So it's like comparing the descriptive statistics from the neural model to the descriptive summary statistics of the POMDP decision making model. However, with this formal connection there is actually an interpretability to the unobserved neural states which are what are being inferred from the fMRI measurement, from the EEG measurements and so on. Those underlying variables have a specific interpretability in relationship to the structure of the P-O-M DP.
3981337	4060025	B	0.8370849541284406	Right? So yeah, that's also very interesting important aspect. So what you said is I think more conventional strategy and it is also formally related to model comparison aspect. So we usually think various modeling and identify or select what is the best model to explain a given data. And this reverse engineering idea involves such a model comparison aspect in the sense that we try to find the model with the best expandability which should we have the identical functionality, right directory address, the exact same Costa function architectures using the information natural transformation. So it should be up to explain the neural data in the Bayesian sense.
4062412	4287512	A	0.9193085958904107	Yeah, one can imagine how that would transform the way that current neuroimaging studies and technologies describe what it is about the measurement that provides information about the cognition model. So, to give another related example, let's just say a person was wearing an EEG headset and a previous study had shown that increased alphaband activity was associated with this behavior. That's comparing a descriptive statistic of the observations of the sensor and correlating the summarized observable to some other variable like anxiety or performance on a behavior. In contrast, an unobserved variable in this setting the actual underlying neural state is being correlated to some semantic generative models component. So it's no longer necessarily that any single frequency band would be associated more or less with a given outcome, but it's actually some hidden state variability which gains the interpretability across this transformation. Which is a subtle point, but it speaks to how broadly the equivalents would reinterpret empirical neuroimaging results as well as a variety of artificial neural network experiments and diagnostics where people do lesion studies and double knockouts on artificial neural networks. So anywhere where somebody with awareness sees that a neural network, artificial or biological, is having summary features described and correlated to something that's more semantic in a quest for meaning may now have a different approach that involves formalizing. The model explicitly in terms of unobserved hidden states with a cost function akin to a variational free energy minimizing risk bounding surprise on the Unobservables. So even though the unobservables were modeled in a sense in the other conventional strategy like neural activity is a variable in fMRI experiments, it's underlying the bold signal. Yet this formalism concordance is a more coherent and powerful connection.
4295362	4477250	B	0.7970659999999998	Lib sold. So you now address this very important point. So first to address that so we need to clarify about what is a program, consider here. So this is a program Socalled metabasian problem in the sense that researchers try to infer or estimate neuro activity or brain activity which infer the external world dynamics. Right. So neuron or brain environment and we research brain activity. So there are two step processes. So this sort of meta Bay is quite tricky intractable because sometimes London variable becomes posterior about other aspects. So I think there is some established approach about metabolism. But this paper provides some alternative in the sense that we separate two programs by saying that here what we import is simply neural network dynamics which is shown in the left hand side of this figure. So we feed data to conventional neural network model which is a simple differential creation. But thanks to this formal recovery between neural network dynamics and home VP behavior, then we can transform the resulting neural network architectures or dynamics into the page and in force in some sense post Hog mana. So we nicely avoid the directory addressing the meta agent program but obtain the same kind of solution in that sense. Yes, with combining with brain activity recording Lieke de Boer imaging. Yeah, we can estimate a plausible neural network model in the right hand side and we can transform that to home DB in the right hand side.
4480537	4519325	A	0.9434301298701298	Awesome. I'm going to show an image and ask a question from Dave in the chat. So, Dave made this image, it's the right side of figure one that we've just been looking at with the variational Bayesian information and he wrote the arc shown as impinging on the S self arc. Is this intentional? If so, it could represent tuning or modulation of the feedback of S into itself. Do you have a thought on this?
4520737	4587062	B	0.7912169879518072	It's attention? Yes. I think it's related to the usual formulation of home DP architecture and active inference concept in the sense that our decision or policy in the usual setting modify the state transition matrix b matrix. Here, delta is an alternative of policy of agent. So basically the director indicates stated transition metrics under a particular decision which agent made. In that sense, what the agent changes is state transition metrics, not state itself directly. That's why we use this illustration.
4589362	4629500	A	0.9299146753246753	Awesome. Very subtle but important point, which is when we look at the classical POMDP formulation. So here we'll look at a version shown in figure two. I'll just bring just figure two in. Could you describe what you just did about the role of the B matrix in influencing how hidden states change and how that is where our policies have impact? And also please, how do the top and the bottom of figure two differ?
4633087	4879850	B	0.795663952702702	Okay, so in the usual correlation under active inference with palm DB structure. So we for us to consider the prior inference and depending on the prior preference, we compute the expect free energy and its minimization provide the policy and the policy moderate state transition. So now in the upper Brea, we instead use the builder which is the option of the agent. So here option or decision was made for each timestep so that unlike the conventional formation, we have a sequence of delta and for each time step delta moderates active states cognition matrix B. So B is a matrix that transformed hidden state in the previous step to the current time step and its moderation indicate that under a specific decision rule. For example, if this F indicates our cognition in the virtual environment with the Gold decision move forward. But if we choose the no go decision, then it unchanged. So such a moderation of state transition was made by choosing debuta and the lower part correspond to Beijing inference made by Bayesian agent. So basically there is a symmetry between a third part and roar part because we assume that this Beijing agent has a plausible guarantee model which nicely correspond to given environment defined in the above upper part in this figure. But one interesting thing asymmetry is that to model this particular canonical neural network, we don't consider an arrow or link from delta posteria to S posteria which is in the environment data moderate S in the next step through P matrix moderation. In this particular Bay jets which formally correspond to a canonical neural network, we don't consider that it is correspond to an absence of the projection from output layer to the middle layer. Okay.
4885837	4914737	A	0.9144739583333333	This is from the 2020 paper, but it shows the neural network architectures, the two layer architectures. So could you restate the top and the bottom of figure two in the 22 paper and connect it to why it's important that you're studying two layer neural network models?
4915837	4917050	B	0.6633333333333332	I miss you.
4918162	4940375	A	0.88509	Yeah, can you just connect the asymmetry between the top and the bottom on figure two with the two layer neural network architectures? You said that the asymmetry, there's no direct link between.
4947475	5017625	B	0.7916763529411763	This is another story. So in the previous paper there is only output or concept layer because we basically consider a single layer feedforward network. So my apologies for some confusion about the network architectures in the 2020 paper. So now upper part of this network architectures correspond to environmental generative process and only a lower part correspond to single foot feed for neural network architectures. So now this part is identical to a map from O to S. OSS area in the 2022 papers.
5023225	5078887	A	0.9198816666666664	Okay, so on the top of figure two is the actual generative process. It's the true structure of causation in the environment, which is to say that actions delta actually influence how states change through time via B delta. The generative process through the A matrix emits observations, sequences of observations. And here on the bottom with a mirrored structure is the generative model of the entity. So what's the relevance of the arrows and the more force factor graph structure on the bottom?
5080675	5178712	B	0.8140682706766922	The arrow indicates active inference. So it's a flow of the information in the sense that to calculate in the step two, we use the information of step two conversation and step one's posterior expectation about hidden states. So those two determine the s two's expectation. Usually in the following graph, we consider retrospective arrow so in the sense that s three also affect the s two inference. But this corresponds to Bayesian smoother in the sense that we update every time step simultaneously to better inference. However, what we consider here is more filtering approach in the sense that for each step we compute the latest hidden states and then we don't change any other states in the past. So that's why we don't consider the arrow from future to the past.
5182337	5287875	A	0.9114456084656084	Awesome. Yeah. Just to highlight that in the Bayesian smoothing approach, it's kind of like fitting a spline because it takes the whole time series and it fits the smoothest possible line or the line whose smoothness is on the AIC BIC frontier. But here on the bottom with the almost pseudocode implementation provided by the Force Factor graph, which was demonstrated to be equivalent with the Bayesian graph in the 2017 work with Friston, Par and de Vries. This architectures is reflecting a filtering scheme like a common filter or just generalized Bayesian filtering through time, where estimates are being carried forward and changed time point to time point, such that the decision rules, or the updates perhaps more accurately, are defined between time points. And the total time series does not have to be loaded into memory or remembered at once. And then the Bayesian filtering approach has the asymmetry with a different consideration of action. So why again is it that action is considered differently in the Bayesian filtering approach on the bottom of the generative models than the consideration of action in the generative process.
5291162	5312100	B	0.8316641666666666	That correspond to lack of cognition from Y to X in the figure one? Or probably a figure four is helpful to that relationship.
5317487	5318250	A	0.8	Four?
5318762	5417300	B	0.8038341406249999	Yeah. This is an example network architecture comprising input Brea, output Brea. What we consider is information flow from sensory to middle Brea and middle area have a self connection, recurrent connection and middle area project to output layer. So there is no connection from all output layer to middle layer. Right. So that's why we don't consider the link from data in the bottom layer of the figure to posterior. So this is different from true generative process in the environment. This is a kind of simplification. So because our purpose is identifying the plausible Bayesian model which correspond to this type of neural network canonical network. So in other words, this neural network uses approximation about that point or use limited form of palm DP scheme.
5423700	5445837	A	0.9281670967741934	Thanks. So could you describe W-V-K and Gamma? Just what is the biological or functional interpretation of those variables? What brain regions or what processes or pathologies do they map to?
5447025	5626750	B	0.8030040825688067	Okay, so basically, WVK, synaptic strength is in the form of matrix and active inference. They represent connection in the different layer or different architectures in the sense that W means forward connectivity from sensory Laje to middle Laje, k correspond to recurrent network recurrent connectivity and V correspond to projection from middle Brea to output layer. So in this paper, we don't discuss the relation to brain anatomy in detail, but what one can consider analogy, for example, say x corresponds to several cortex activity and Y, for example correspond to cerebral wrong in the sense that it determines the action. So it is considered that in the cerebrum there is a signal that represents choice. This is joined for examples goal no go decision made in cergram. It's analogous to this particular architectures. On the other hand, in the several cortex we compute the sensory information to generate some inference, prediction and planning the way it is computer by this recurrent network. In this particular modeling, although we don't separate brain region in detail, but this recurrent network is sufficient this graph of recurrent network is sufficient to characterize any type brain architecture in the sense that we can design any higher car or mutually connected architecture using a generic crossover recurrent network by changing weight.
5634750	5746812	A	0.9173432446808506	Awesome. So the middle layer we can think of as like the cognition stuff. It's the internal states when we talk about perception, cognition, action in the active scheme or even in the sandwich model of cognition, perception, cognition action. So W is describing how those sensory inputs either in one step or composably in multiple steps become processed to these internal representation of hidden external causes inferred external states. And so these are the states that have that sigma relationship and a generalized synchrony with external states. The sigma and the generalized synchrony are not discussed in your paper, but it connects to other work and the recurrent connections are facilitating attention or waiting of the stimuli. This is the recurrent learning loop and the relationship of the A between observations and hidden state estimates. And then a different kind of modulation comes Hinton play between the hidden state estimate of the internal states state and the action selection. So what is gamma corresponding to? And why is the gamma modulation between layers two and three differing functionally from the k synaptic modulation of one and two?
5747850	5905362	B	0.7681760824742262	Yeah, so K matrix basically formally correspond to B matrix in the Bayesian information. So we rotate the information about the prediction, right, our narrator or our expectation about the next state based on the previous state. On the other hand, Laurel gamma is quite different from such a computation. Gamma basically means risk function, which is in principle can we use arbitrary risk function. So this is a part of generative models we designed and the rule of risk function in generative model formulation is attention form of generative models depending on that value of gamma which examples retrospective moderation of evaluation of task decisions given an outcome in the future. In terms of neural network, of course it corresponds to some neural modulation. For example, Dopaminergic moderation is famous in the literature which moderates the activity and fluxicity of various brain vision. But we particularly focus on Dopaminergic or any kind of neuromoduration of cyanogic prosthesis in the output trigger which may correspond to Cergram. So in the Serbram neural activity or processes moderated by Dopaminergic, input from is used as the optimization action rule, decision rule or sometimes attention help us.
5909212	6035900	A	0.9164495698924728	Awesome. Very interesting because in some previous papers and models that we've looked at, attention is dealt with as policy selection on mental states. So internal action selection, it's an action like variable describing attention and awareness and even metacognition. And so that connects the role of Dopamine in motor decision making seen in many Dyskinesias but also with the role of Dopamine in seemingly nonmotor based decisionmaking like gambling or investing where it doesn't seem to immediately translate to a given motor sequence. Yet it has analogous computational characteristics and the comorbidities and the side effects of different drugs that affect the Dopamine neurophysiology are known to have carryover in terms of like the rigidity or excessivity of motor and decision making aspects. So it's like interesting that Dopamine has long been understood to have that parallel role in attention as cognitive action and motor action and that was established empirically through modifications of Dopamine signaling and also had been modeled analogously with computational neuroscience. And this is providing again a slightly different interpretation of that very well studied Dofaminergic modulation of attention and policy.
6040837	6051887	B	0.749035882352941	Yes. In addition to that, I believe another important aspects is correlation of scientific processing by document.
6075850	6078937	A	0.8346937499999999	Do you want to show something or yeah.
6080875	6096275	B	0.8593822222222222	Can you see this paper? I sent you a chat. If you can't, I'll send you a PDF.
6097450	6125155	A	0.8933315384615383	Okay, let me see. I'll look at it up now. All right. The paper is a critical time window for Dopamine actions on the structural plasticity of dendritic spines from 2014 byagasha. So what is interesting about this paper.
6125352	6245437	B	0.7862291772151898	Yeah, it basically explained conversation of plasticity by Dopamine, which is common but crucial point of this paper is that it shows that it proved that Dopaminezic input can moderate after hebion prosthesis is established. So this paper showed that they add domain logic input for about 2 seconds after or several seconds after the Hebbian process is established. But such a post hoc moderation, post hoc introduction of heterotopamagic Impetu is sufficient to change the past capacity which may be associated with the Costa hoc evolution of our past decisions. So by decision making we of course changes the changes the weight matrix by through trust 50. But to evaluate the goodness or badness of our decision, we need to see observe the future outcome which is propagated by for example, Dopamine. And this paper nicely show empirically that Dopamine actually can change the past evaluation, maybe after such a psychic level, very local level, ecoscopic level.
6252250	6302662	A	0.8963289062500001	So there's a short term window, the critical time window that they're describing. But there's some window. Yeah, some window by which dopamine potentially unrelated to the initial heavy and plasticity events, right. Where secondary dopamine signaling or not secondary just after the initial fact, potentially of a different valence or the same valence can synergize or cancel the plasticity formed in the moment.
6303100	6312862	B	0.8640506250000001	Exactly. And this is not limited to Dopamine, but other neuro moderator can also do this.
6318250	6333575	A	0.9094316129032256	Well, on one hand, how does this change our understanding of animal neurophysiology? And then I guess, on the other hand, how does this influence how we would design sentient artifacts.
6342625	6440600	B	0.835021056910569	For both animals and artificial agent? One important message I free with us. So this tells us possible simple architectures to make learning. This is association between past decision and future reward or any risk factors, which is otherwise computed by computing forward prediction by iterating some computational, this is a usual way to predict the future event and then select the option. But using this property, biological property, which is observed in experiment, we can design, we can imagine other simpler architecture to make a planning. So for both animals and synthetic Bayesian agent, it provides an alternative explanation about the association between our past decision and the future risk and the optimization of our decision to maximize, reward or minimize risk.
6445075	6505625	A	0.9325190225563911	Well, one interesting note is we spoke earlier about the difference between the Bayesian smoothing all at once approach and the Bayesian filtering step by step approach. Now, if one had infinite knowledge and computational resources, the Bayesian smoothing approach is the way to go. Like, you don't want the decision rule for investment. You want to look at the whole time series past, present and future, and know the best moments to have made the trades. I mean, there's no comparison. You're going to do better with the Bayesian smoothing. However, it's just implausible computationally and because it requires total memory of the past and knowledge of the future. So that's what motivates the development of Bayesian filtering approaches, which are tractable and calculable through time. Yet with this time delayed modulation.
6507400	6507895	B	0.99664	Part.
6507972	6653800	A	0.9184073437500005	Of the Bayesian smoothing strength comma back into play. It doesn't enable true anticipation of future states, but that's what the expected free energy does. However, the delayed neuromodulation allows for reconsideration of a window of past states. And so in that way it corresponds to like a slightly deeper filter, not just a filter of a time step of one, but a filter of like a rolling window of five or with some decay. And you don't want that window to be too big because if the window were ten minutes, then too many contrasting stimuli would get piled together. The Dopamine level would just converge to a mean field average. But there's some time decay or time constant on the post hoc modulation where that neuromodulatory signal is actually a parameter of interest. And that's not an infinitely long or infinitely short window, but it's some niche dependent amount of time. And that's a very interpretable and first principles interpretation of the computational role of neuromodulators in a way that is also consistent with all these other concordances we've been exploring. So it's quite an interesting connection back, I guess, in our final minutes of this discussion. What are you? Well, maybe go to the beginning at the end, which I meant to ask earlier, but it's a good way that we can sort of close today and look forward, which is how did you come to this line of research specifically studying neural networks in this way with Karl Friston and your colleagues?
6658275	7003675	B	0.8037008785529715	So, yes, so my interest was the characterization of Barricade network. So my first motivation is to make biologically plausible artificial intelligence. But to achieve that, we need to know about biological brain or biological neural networking. In these several years, I collaborated with the doctor professor Californiston to study about his salary principal after doing forest. My question during that period was the priority principle, is everything about the biological possible neural network or is there another aspect that can characterize the virus car neural network? So it is non trivial. It was non trivial. So that's why I tried to start from characterizing the neural network first. So our strategy is not considering the way of implementing any Bayesian algorithm as the brain architectures, but my interest is rather characterization of a given vertical network in terms of some other things. One possible way is of course based on inference free energy transplant reinforce. So that's why I start from characterizing power's network. But just defining neural network architecture is insufficient. It is not tractable, it is far beyond the computational tractability as the mathematical analysis. And we need some assumptions or some trick to increase the tractability. One day I came up with an idea that in which we consider that both new activity and fastest follow the same cost function gradient. This is very much an allergy with physical system like Lagrangian information geometry, Hamiltonian formation. So usually we consider some energy landscape and design plausible trajectory as the evolution of some principle of minimum action or restruction. So we imagine that what if we applied such idea to computational neural network or biological neural networks to characterize their dynamics in the first principle, that's the first computational step to come up with this framework. And finally we noticed that it is not easy to connect the Newtonian dynamics with this type of neural activity study because neural activification not necessary to be a second order differential equation, but rather it is fast order and considering many things. Then we finally use a Costa function proposal in the papers, which is not necessary to have a former identity with the so called lavalier in the Newtonian physics, but it is rather plausible as the rule or underlying mechanism of such type of network.
7011800	7028912	A	0.9003956249999999	Awesome. Well, it has been quite an interesting dot one. I really appreciate everything you've shared today. Is there anything else you want to add at this point? Otherwise we'll talk again.
7030325	7036050	B	0.7190483333333333	Yeah, I already speak a role.
7037675	7041450	A	0.8525	Thank you. Alright, talk to you later. Bye.
7042325	7044400	B	0.9015574999999999	Thank you very much for a nice discussion.

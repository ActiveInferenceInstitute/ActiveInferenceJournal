39287	41007	A	0.5716504454612732	All right, hello everyone.
41085	41727	A	0.7317204475402832	Welcome.
41895	45507	A	0.9180998802185059	This is ActInf livestream number 51 one.
45660	52612	A	0.9071857929229736	We are in the second discussion of this paper, canonical Neural Networks perform Active Inference.
53187	55757	A	0.6894928812980652	Welcome to the active inference institute.
55922	62072	A	0.506173849105835	We're a participatory online institute that is communication, learning and practicing applied active inference.
62192	67517	A	0.8612511157989502	You can find us on this slide and this is recorded in an archived livestream.
67652	70872	A	0.5307847857475281	So please provide us feedback so we can improve our work.
71055	84937	A	0.9059376120567322	All backgrounds and perspectives are welcome and we'll follow good video etiquette for live streams, head over active inference.org to learn more about the institute and how to participate in projects and learning groups.
85887	104877	A	0.9657738208770752	All right, we're in ActInf livestream number 51 one, and having our first nonsolo discussion on this paper, canonical Neural Networks perform active inference and really appreciative that you've joined today.
104970	107282	A	0.9682714343070984	It's going to be a great discussion.
107447	109412	A	0.8881863355636597	We'll begin with introductions.
109487	114657	A	0.5099745988845825	I'll say hello and then please just jump in however you'd like.
114810	118337	A	0.8556798100471497	And we can start by setting some context.
118487	142262	A	0.9561832547187805	So I'm Daniel, I'm a researcher in California, and I was interested in this paper because we've been talking a lot about active inference from a variety of different perspectives, from the more fundamental math and physics to some applications, philosophy, embodiment, all these really interesting threads.
142412	159422	A	0.89274662733078	And this paper seems to make a really clear meaningful contribution and connection by connecting active inference entities and this approach of modeling to neural networks which are in daily use globally.
159617	166302	A	0.9859209656715393	So thought it was a fascinating connection and really appreciate that we can talk about this today.
166395	168550	A	0.9168375730514526	So to you and welcome.
174375	178212	A	0.6943787336349487	Go forward, Takuya, however you'd like to introduce and say hello.
178575	179337	B	0.5491447448730469	Yeah.
179700	180195	B	0.5325649380683899	Hi.
180272	186175	B	0.9274767637252808	I'm Tafia Isomura, neuroscientist in Lique Brain Science Institute in Japan.
186525	196370	B	0.6824936270713806	I'm particularly interested in universal characterization of neural network and brain using mathematical techniques.
196535	213500	B	0.765424370765686	So this work I believe important as a link between active brain forest aspect, Bayesian aspect of the brain, and the dynamics system aspect of the neural network.
213650	217790	B	0.9881964325904846	So I'm very happy to join this discussion session.
217895	219440	B	0.9678386449813843	Thank you for invitation.
219620	220962	B	0.9245960712432861	Nice to meet you.
222000	224337	A	0.9636296629905701	Nice to meet you as well.
226350	231700	A	0.8458783626556396	The first thing you added, the universal characterization of neural networks.
232275	236210	A	0.8708625435829163	What is the universal characterization of neural networks?
236255	239712	A	0.633854866027832	Why is it being pursued in this area of research?
241200	269075	B	0.830827534198761	So, as a narrow sense, my gain aim of this paper is that so, you know, people active inference lab communication to characterize brain activity, behavior, so on, so on, but which would be different from conventional neural network.
269150	289935	B	0.6438356637954712	So there is a crossover program which is associated with conventional neural network and it is not very clear whether all characterization of computational neural network can be explained by activity infrastructure principle or not.
290042	312170	B	0.8935089707374573	So here universal characterization means that characterization of every aspect of conventional neural network which is a kind of dynamics system derived as association between biological phenomena and simple mathematics.
312185	320190	B	0.8896403312683105	Car formula using gift card, using differential equations as the broad sense.
320357	342250	B	0.7153328657150269	I think universal characterization means that well, it is a characterization of brain intelligence, but it's a big picture and the paper particular address is only one aspect of the Bull picture.
346200	347205	A	0.7200906276702881	All right?
347402	355365	A	0.6434908509254456	So it'll be great to pull back to really understand what synthesis is happening.
355532	365640	A	0.8935652375221252	So I'm going to ask what makes a neural network model a neural network model and what makes active inference lab model an active inference model?
365807	370170	A	0.8328147530555725	Is this synthesis and connection you've made true?
370247	371487	A	0.5308760404586792	Because of what?
374175	402570	B	0.7619171738624573	Because basically what we show is the mathematical equivalence between the formulation of canonical neural networks and the formulation active inference lab in the sense that we show that as possible neural networks can be characterized by minimization of some biological plausible cost function.
402722	422187	B	0.854020893573761	And we show that that cost function can be least as variational based on inference and a particular cross of gentlemen model in terms of well known partially observable position process.
426000	433812	A	0.9310100078582764	Alright, shall we perhaps walk through some of the sections of the paper?
434250	435285	A	0.9647946953773499	It would be awesome.
435392	442160	A	0.8859955072402954	Just for each of these sections, maybe the numbered and the lettered sections.
442355	447087	A	0.838677704334259	What does the section aim to show and why was it there in the paper?
457125	462312	B	0.7789990901947021	Briefly it's over, right?
463275	464825	B	0.7049640417098999	So briefly.
464900	476787	B	0.8713266849517822	So first we introduce so the gain issue main program, our interest, which is relationship.
480975	489935	B	0.7828583717346191	We try to make a formal ring between neural network and active reinforcements that gain program background.
490055	499535	B	0.8207331895828247	And then we first formulate the equivalence, mathematical equivalence, in a very Brea manner.
499730	521712	B	0.8495802879333496	So in the first section in results, we formulate the relationship using complete craft serum, which is well known statistical theorem proposed very long time ago.
522300	535505	B	0.8354532122612	And using that we link a general form of neural network with a general form of variational data impress.
535640	553895	B	0.6051488518714905	But a problem is that this characterization does not address a specific generative model which is crucial to characterize a specific model, specific neural network dynamics.
553985	573925	B	0.9110010862350464	So in the following sections, we characterize the problem using Pomodb or partially observable Markosition process and link that model with a particular class force canonical neural network.
575625	589150	B	0.8888842463493347	And then we simulated we use the simulation to propagate that property in terms of some major tasks.
595000	597412	A	0.949095606803894	All right, thank you for this.
598150	602225	A	0.9226244688034058	Could we talk about the complete class theorem?
602650	614137	A	0.8830890655517578	So what is the scope of the complete class theorem and why was it the relevant set of the neural networks to pursue or the right way to frame it?
615100	616975	B	0.784842312335968	Thank you for asking that.
617037	625015	B	0.9476460218429565	So I like the slide you showed last week's video.
625182	636125	B	0.9071885347366333	So computer cross theorem basically indicates the relationship between some crossover decision rule and vision in France.
637150	664620	B	0.797556459903717	Here a crucial keyword is admissible decision rule, which is a rule which is as good as other decision rules or at least at one point better than other decision rules.
664635	674175	B	0.5897615551948547	So simply speaking, adomissibility indicates in some sense it is the best rule for some aspect.
674325	683290	B	0.8199795484542847	And usually we characterize such a goodness using cost function, loss function or risk function.
683457	707940	B	0.8751643896102905	And here what we did is we established some association with this type of loss function or risk function with canonical neural network which is we call cost function or biotic roles Costa function or neural network.
708120	713485	B	0.8604286313056946	So our assumption is that neural network minimize cost function.
713667	730162	B	0.7092446684837341	So if it active the inclination and it is virusly active some sort of optimality so we can say it is adommissible with respect to that cost function.
730525	751815	B	0.6212623119354248	So the beauty of complete cross theorem is that if we find some admissible decision rule then automatically we can say that it is based on inference in terms of some Bayesian Costa function with gentlemen model a priori beliefs.
751995	767085	B	0.8437924981117249	So this computer chaos theorem is crucial as abstract characterization of the relationship between conventional neural network architecture, dynamics and variational.
767130	768350	B	0.7775117754936218	Beijing influence.
771650	773762	A	0.8092430830001831	All right, thank you.
775325	780287	A	0.7769991755485535	What does it mean when you said it was biologically plausible of a loss function?
781925	808050	B	0.8342673182487488	The term is a little bit arbitrary because in this paper we mean by probability in the sense that this neural network model can be derived from realistic neural model through some approximation.
808625	823562	B	0.8851572871208191	And so here barricade probability, suggest means probability as a neural model or synaptic processing model.
824300	836175	B	0.5104811787605286	And if this cost function loss function can derive such a plausible algorithm, then we can say that this cost function is barely plausible.
839525	847862	A	0.8488853573799133	So what is the distinction between those neural and synaptic components in the loss function or what equation to look at?
849500	854035	B	0.785338282585144	You mean distinction between dynamics and synaptic?
854155	854525	A	0.5491447448730469	Yeah.
854587	858975	A	0.8563041687011719	What is the distinction between them and how is it represented in the equations?
860525	883050	B	0.8329108357429504	Okay, basically neuropathivity equation means differentiate equation about a variable that represents firing intensity or some sort of variables associated with the firing.
883625	900450	B	0.8797471523284912	On the other hand, dusty equation means an update rule about the synaptic weight or synaptic strengths which is a connection between two neurons.
901025	926387	B	0.7582737803459167	And beauty of this formulation proposed in this paper is that we characterize both heuristic equations synaptic procedure equations in terms of gradient descent on a same cost function, common cost function.
926975	959625	B	0.8758535981178284	So we can say that if we consider the partial derivative of some cost function with respect to new activity, then it's derived by gradient descent rule about if we consider a partial derivative of chaos function errors with respect to synaptic weights, then we derive a prosthesis rule.
970187	978312	A	0.8534008860588074	Are those the only two aspects of a neural network or why are those the two key aspects?
980837	990637	B	0.8523344397544861	It is a main, I think it's the main body of the neural activity.
990712	1017692	B	0.8456089496612549	If we consider some inference running or action exhibit by neural networks in the sense that neural activity correspond to fast dynamics, fast gradient dynamics mix and scientific processes indicate through dynamics that minimize least function and cost function.
1017890	1023997	B	0.8755599856376648	But in general, we can consider any aspects, any variables associated with your method.
1024042	1044892	B	0.7093244791030884	For example, at least what we show in the paper is any free parameter which may be associated with firing threshold or although we don't discuss in this paper it would be possible to add other variables related to neural network.
1044952	1060362	B	0.7597104907035828	For example, here we ignored contribution of Griad factor but it would be possible to add the Griar factor in this correlation or any other aspect of virus corporate neural network.
1064612	1073422	A	0.9302430152893066	That's very interesting and it speaks also to a general separation of time scales.
1073617	1096277	A	0.8082248568534851	For example in different multi scale systems or in the renormalization group where it's describing some minimal multi time scale system where the faster time scale can be seen as perception like a slower time scale can be seen as more learning like.
1096445	1103632	A	0.8216408491134644	And then in some hierarchical model what's learning of one time scale can be perceptual for a slower time scale?
1103722	1107237	A	0.9597514867782593	So it's a very nice generalization.
1112387	1123322	A	0.9159371852874756	Are there any examples of decision rules that will help us think about the action components of what the neural network is doing?
1123505	1135582	A	0.6064329743385315	Because it may be more familiar to think about digit characterization and image classification, some kind of classical tasks for neural networks.
1135747	1141362	A	0.8562892079353333	But how does the decision rule play out in the context of neural networks?
1144187	1156050	B	0.8449636101722717	Okay, so in this paper we basically assume a closed loop so comprising a neural network part and environmental part.
1156487	1167387	B	0.8835619688034058	So Neuron receives sensor input from environment and provide some feedback to the environment.
1171787	1189062	B	0.6735146641731262	Even with the example of classification, we can say that output correspond to classification output, which is kind of generative model relevant.
1189412	1198772	B	0.7725695371627808	Example would be, for example, controlling agent like a robot control or any kind of control errors.
1198805	1200787	B	0.7190074324607849	Decision making tasks.
1200937	1212375	B	0.882944643497467	For example, when we encounter some choice tasks, we need to advertise, for example, left or right or something.
1212887	1222362	B	0.7332891821861267	Any kind of such a decision can be associated with the admissibility or admissible decision.
1227050	1238175	A	0.7783260345458984	So what would an example of an inadmissible or admissible strategy be in the decision making task?
1240850	1248962	B	0.5380498766899109	Admissibility usually characterized by loss function or risk function.
1252925	1270500	B	0.5044518113136292	Here admissivity indicates that there is another decision rule which is at least one point better than the forecast decision rule.
1272575	1286025	B	0.8511552214622498	Simply speaking in Adobe CBD indicates that decision rule is not good relatively.
1287050	1290737	A	0.5110300779342651	Let's just say our decision rule is we always turn right.
1291475	1293790	A	0.8060204386711121	Is that an example of a decision rule?
1293820	1302370	A	0.5415184497833252	Because there might be settings where that is strictly effective and the simplest rule whereas there's other settings where that's going to be tragic.
1302535	1311000	A	0.8833467364311218	So what does it mean to be admissible for an agent in light of different environmental contexts?
1312250	1314155	B	0.7870833873748779	That's an interesting point.
1314277	1331612	B	0.7774111032485962	So even with such a too much simplified rule it can be admissible under some particular situation, particular loss function.
1331975	1341110	B	0.6412718296051025	For example, the rulers that always turn right maybe the best under some situation, right?
1341292	1355625	B	0.7611930966377258	So the relationship of admissibility or enough adommissibility depends on both agent characteristics and environmental characteristics.
1359350	1362300	A	0.7203050851821899	What aspects of the environment.
1364975	1365962	B	0.7977058291435242	For example?
1366400	1389225	B	0.8458048701286316	For example, if that decision group matches the structure architecture of environment then maybe that decision always downright active the shortest past under some situation, some environment.
1391675	1408500	A	0.8127415776252747	How does this admissibility help us think about like overfitting and how does it help us think about the way that different practices are used for neural networks to prevent them from being over fit in practice?
1410725	1424100	B	0.7291553020477295	Well, strictly admissivity is characterized with the Bayesian risk.
1430225	1442860	B	0.7288210391998291	We cannot observe a hidden states of the environment, only we can observe is a part of the entire universe.
1442905	1453955	B	0.762688934803009	So the question is an important question is what is the best choice under such a limited information?
1454077	1459250	B	0.7199766635894775	Limited information?
1459387	1489780	B	0.8104281425476074	So this Bayesian list adoissibility or computer credit theorem tell us that well known, only the well known Bayesian framework achieved the adommissible decision.
1489840	1512900	B	0.5876259803771973	Which means that in this aspects Bayesian optimization give us a least choice strategy, otherwise we overfit or find the suboptima evolution.
1513700	1526470	B	0.8475409150123596	So it's a nice association, nice linkage between the decision, but is a good decision about the decision and more established statistical inference.
1526560	1527662	B	0.6497023105621338	Freedom work.
1531137	1534082	A	0.9836751818656921	Thank you, that's very helpful.
1534172	1543912	A	0.7165529131889343	So we're reducing our uncertainty and risk about hidden states in the environment.
1544262	1563162	A	0.7618077397346497	So in the special case where the entire environment is observable without errors like a chess game, then there's an equivalence between correlation of risk or loss on observables or on hidden states.
1563225	1566925	A	0.7673779129981995	But they're not really hidden, but they are environmental states.
1567512	1605752	A	0.7729731798171997	Whereas any amount of uncertainty in the mapping between observations and hidden states, which is usually shown as a in the partially observable Markov decision process, any amount of uncertainty about unobserved or partially observed environmental states enables you to fit your uncertainty optimally about that hidden state and fit that uncertainty simply with the gradient descent.
1605932	1630287	A	0.5939494967460632	And by doing so, you don't overfit a model of observables, which might be the fallacy or the issue with simply doing descriptive statistics you might get an infinitely small variance with a frequentist estimate because you have 1000 data points.
1630425	1636375	A	0.777267336845398	So the variance from a descriptive statistics perspective might be very small.
1641012	1654992	A	0.7793028950691223	I think it speaks very much to why neural networks are useful in practice from training with limited data sets because that's an empirical observation that they don't entirely over fit.
1655115	1660237	A	0.5372817516326904	But also I'm sure there's ways to construct them that are overfit.
1661112	1670597	B	0.572420060634613	Yeah, overfit will occur if we select some optimal priorities.
1670717	1671550	B	0.6899105906486511	For example.
1673712	1696707	B	0.7397666573524475	Well, I'm not sure if it is overfit in the sense what you mentioned because if we select some priorities then the Bayesian function itself changes and the neural networks that try to fit to that Costa function.
1696785	1703697	B	0.752202033996582	So cost function minimization will be achieved agent such a situation.
1703880	1712125	B	0.8809207081794739	But that solution is not good for our original help us.
1712787	1714422	B	0.5381297469139099	That's the tricky part.
1714605	1726857	A	0.556452214717865	Yeah, that is reminiscent of some discussions we've had discussing like driving off a cliff or blowing up is also reducing free energy.
1726935	1730200	A	0.6109896302223206	Like dropping up a building reduces your potential energy.
1730637	1749437	A	0.5363143086433411	And so there are potentially decisionmaking or strategic trajectories that do for some time horizon minimize free energy, perhaps even or maybe even guaranteed better than some longer time horizon.
1749587	1754362	A	0.6797226071357727	Because if the shortterm strategies were somehow better than the longterm horizon.
1755087	1761112	A	0.48860153555870056	It would be difficult to imagine because the long term horizon would be at least as good as a shortterm strategy.
1762437	1767902	A	0.7763909697532654	So that speaks to the challenges of planning in action.
1768007	1777000	A	0.9025499820709229	So how is planning addressed in modern neural networks and how does this work help us think about that?
1779837	1783162	B	0.7076609134674072	That's another very important aspect.
1785612	1812187	B	0.5596296787261963	I have to say that this framework addresses planning aspect, but that planning is not necessarily the optimal solution in the sense that what we interested in is optimization or learning under limited structure.
1814112	1821422	B	0.853919506072998	The structure is characterized by here Prosperia neural networks.
1821617	1834797	B	0.754309356212616	So yeah, planning occurred by association between risk in the future and our decision in the past.
1834980	1849162	B	0.9085871577262878	Here we model that aspects using delayed moderation of scientific activity mediated by some neuromodurator or neurotransmitters.
1850712	1852075	B	0.7656590342521667	This is the model.
1858812	1876837	B	0.719912588596344	This is model as the risk factor and the heavy product holding the neural network.
1881987	1889342	A	0.7946822047233582	All right, I'm going to ask a great question from the chat and then we'll look at the figures a little closer.
1889477	1895637	A	0.798363208770752	So ML Don wrote a question stuck in my mind for a long time.
1895775	1897872	A	0.8244044780731201	Could you please put it to rest?
1898055	1906537	A	0.879948616027832	Do we need to have knowledge about all states possible actions and sensory inputs for active inference?
1910812	1928000	B	0.7309863567352295	Well, you mean if you seek the exact solution, exact optimal solution, then maybe more information would help you to find that.
1928887	1948187	B	0.6081230044364929	But under some ideal assumptions then the is not necessary to achieve the optimal solution.
1949662	1953800	B	0.5741965174674988	I'm not sure if I correctly answer your point.
1955512	1958300	A	0.7975739240646362	So just to restate it.
1959037	1964812	A	0.523143470287323	Of course, knowing all the state's possible actions and sensory inputs, it's not a bad thing.
1964950	1971897	A	0.514402449131012	Worst case, there's some computational complexity, trade offs, but the problem becomes fully stateable.
1972092	1996625	A	0.6510485410690308	But I think ML Dawn is asking about cases where you don't know all of the state spaces or potentially even the dimension or the semantics of hidden states, active states, sensory inputs and why not even add cognitive states?
1998412	2012225	A	0.8996272683143616	So in not just partially observed but partially known state spaces, how are these address in neural networks and how does active inference help us think about it?
2017762	2028067	B	0.8371174931526184	Okay, I think the question is about how can we separate those states?
2028190	2037565	B	0.7910472750663757	Like sensory function interface entorhinal, how can.
2037582	2058880	A	0.7145907282829285	We separate not just in principle have these states be separated, but deal with the fact that some of these states we might have good knowledge on and some states like the hidden states we might not even know, like we don't know the dimension of the cause vector in the world.
2059077	2060062	B	0.5826064944267273	I see.
2062525	2091937	B	0.86702561378479	In terms of dimension, there is a statistical technique to estimate the dimensionality, for example via information criteria like I agent information criteria, based information criteria, all them try to info estimate plausible dimension about the environmental hidden states.
2093650	2099790	B	0.893538773059845	There is an analogy with those information criteria and version of free energy minimization.
2099895	2115200	B	0.8221995830535889	So with version of free energy inclination we can identify the plausible model structure which in principle involves the dimension aspect.
2116375	2136937	B	0.5862983465194702	But in terms of Neural network in this paper we don't carefully consider about the dimensionality optimization because we first define the number of neurons and don't change during the training.
2137600	2156975	B	0.9030560851097107	But in principle we can consider the change in the number of neurons which is associated with the neurogenesis adult neurogenesis or development during the developmental stage.
2157475	2166125	B	0.8123371601104736	That would be an important expansion of this direction.
2173450	2175187	A	0.959330141544342	That's very interesting.
2175925	2178500	A	0.7187589406967163	Here's a remark.
2179000	2187875	A	0.8223819732666016	Well, one note is equation one summarizes a lot of what you've been describing.
2188300	2201470	A	0.867261528968811	There's a parallelism or a concordance being drawn between the loss function of Neural networks and the variational free energy of the parameterized model there.
2201622	2225350	A	0.5400485992431641	So to come back to these processes that influence learning which we could think of as the Neural network becoming more fit from a loss function perspective or the variational Bayesian partially Observable Markov decision process entity generative model encoding better at doing what it does.
2225487	2235090	A	0.8004576563835144	So there's the firing rate on the Neural network side, the synaptic plasticity at a slower time scale which we discussed a little earlier.
2235120	2245250	A	0.7645341753959656	And then now there's a third time scale with the birth and death of new cells and maybe even new layers.
2246350	2271010	A	0.6483818888664246	And that kind of multiscale temporal structuring is not intrinsic to the Bayes graph to represent multiple nested timescales in a Bayesian graph in the act of inference literature it's more common to make a hierarchically nested model, right?
2271192	2284365	A	0.6388000249862671	And just say that the time handling on one level is happening more rapidly with respect to clock time than deeper nested, slower models.
2284545	2303750	A	0.8010738492012024	Whereas the Neural formulation allows us to deal with multiple ongoing active states without appealing to hierarchical nesting, which is a very important feature.
2311450	2317237	B	0.6240747570991516	Well, both distinctions will be possible.
2317600	2333725	B	0.6710776090621948	So without hierarchical or with higher car modeling so even with hierarchical modeling, the optimization of dimensionality should be possible.
2333787	2335162	B	0.7382022142410278	It would be possible.
2335525	2353470	B	0.8592540621757507	But in other distinctions we can consider that a population of Neural models so one has a single layer, another has two layers, three layers, four layers.
2353635	2382787	B	0.8868715763092041	And consider the probability of network architectures associated with Costa minimization and a particular environment which is in principle have the same computational architectures with the hierarchy model.
2386800	2388462	A	0.9324204325675964	Very interesting.
2389200	2410675	A	0.6045494079589844	Yes, perhaps I over generalized or speculated because I thought about how one could have a 100 timestep POMDP that also performs multiscale behavior potentially extremely wastefully, but at least it could in principle.
2411100	2422110	A	0.8783804774284363	And similarly, within a neuron there could be another Neural network or some other structure approximated by that.
2422292	2440900	A	0.8219764232635498	So they almost both enable hierarchical and non hierarchical model modeling as you described, but in very different ways that lead to very different implementations.
2444025	2444787	B	0.46103888750076294	Yes.
2445900	2452525	A	0.7828199863433838	I think this brings us to the topic of forward and reverse engineering.
2452950	2457025	A	0.768190860748291	So you talked a lot about reverse engineering.
2458650	2468500	A	0.8875699043273926	What is reverse engineering and what is forward engineering and what has been done in these areas of engineering?
2470350	2503212	B	0.8177483081817627	Okay, I'm not an expert in this process, but I believe that liver here means your characterization of the blueprint of some device or machine from data observable information like activity or action behavior of some agent.
2504400	2523810	B	0.7944806814193726	Goal is identification of blueprint and the crucially here blueprint correspond to generative models because once we define generative model, we can Deneve evolution, anthropology algorithm, running inference algorithm and any behavior of the agent.
2523917	2568330	B	0.8619163632392883	So here reverse means that we first observe some activity of agent and its mechanism is still unknown for us, but we can estimate its mechanism using that activity by identifying the most plausible guarantee model which can minimize some Costa function or risk function when we feed the data to the model.
2568452	2595250	B	0.7278324961662292	So, on the other hand, for the engineering would be more mainstream, way fast defined model blueprint gently model then drive everything including parasite functional running algorithms and behavior action prediction algorithm.
2599875	2615187	A	0.8876768350601196	So, by reverse engineering neural networks, we're observing some already parameterized neural network and then fitting a POMDP to it.
2616525	2628250	A	0.9049030542373657	To what extent is it possible to take a given POMDP and create a neural network that performs that inference?
2632875	2679025	B	0.915711522102356	Okay, in this paper or in the following paper, what we consider is a strategy that we first feed empirical data whether force neural response data to BioScale prosper neural network model which is similar to a conventional model fitting approach where we have differential equation data and differential equation to explain the behavior with the minimum prediction.
2679450	2701310	B	0.7029591202735901	So now, a virtue of this framework we established is that we can naturally transform such neural network architecture with the very known partially observable markup action process architecture.
2701430	2707962	B	0.6446375846862793	Because for any kind of canonical neural network there is a cost function.
2709900	2736612	B	0.625480592250824	So we Deneve cost function through neuroactive decision which is opposite with the conventional way we define cost function derived algorithm and then we use the formal equivalence between neural network Costa function and variant queen energy.
2737575	2761790	B	0.8395493030548096	So now transform the journal architecture to Beijing model architectures and once we characterize vital energy, there should be some general that define that informational energy functional.
2761970	2778075	B	0.755287230014801	So in particular, in this example, canon network nicely correspond to well known across macquarlin process.
2778212	2800062	B	0.8081389665603638	So, by using this procedure, we identify a plausible home DP architecture which correspond to observed activity data.
2809337	2815200	A	0.7932830452919006	Well, let's stay on this last point.
2815862	2842387	A	0.9004149436950684	So, after all those transformations, first the measurements of neurons using that data to fit the neural network and then by virtue of the relationships unpacked in the paper, transforming the neural network in the left side of figure one into a particular form of the P-O-M DP.
2842537	2848462	A	0.8816705942153931	So first, what are the constraints on that form of the P-O-M DP?
2848612	2855862	A	0.8789132237434387	Is this a little corner of model space or what are the space of acceptable P-O-M DPS?
2858012	2864037	B	0.8610717058181763	That totally depends on what kind of neural network model you are considering.
2864387	2879597	B	0.9150072336196899	So for example, in this paper we discussed about a particular crossover from DP in which each state takes either zero or one.
2879780	2886862	B	0.6269912719726562	So it's very restricted compared to the general form of homedp.
2886937	2922000	B	0.8739616274833679	But we consider a factorization so in the sense that although each but we consider a vector of observation, a vector of hidden states where each element correspond to one single one hot vector but as an entire state it can represent high dimension discrete state space.
2923112	2943750	B	0.5881401896476746	And this architectures nicely correspond to neural network architectures because usually each neuron takes either zero one or some value continuous variable between zero and one.
2944262	3005722	B	0.732383668422699	So we use this association to characterize a particular OMDP which correspond to neural networks, and this follows a particular mini field approximation, approximation or approximation in generative model because we associate posterior belief in this particular homo DP with the neural activity, which means that posterior of action also has a factorization architecture in the sense that we don't fully consider about the second order statistics between neurons activity and activity, which is outside of this poem.
3005767	3006612	B	0.6570615172386169	RASM.
3007337	3036600	B	0.820539653301239	So each neuron activity correspond to posterior expectation about a particular element of the state and we don't consider the joint posterior property of all state.
3040700	3076045	B	0.5777048468589783	So although this is a implication, we see this Asia impress, but otherwise, for example, we can consider any recurrent network architectures which correspond to state to transition metrics and it would be possible to extend this architecture to higher call structure in the sense that it is straightforward.
3076135	3089830	B	0.7444840669631958	Consider a tree structure or any kind of higher car structure by assumptions that some neurons connect to other neuron but not connect to other neurons.
3089890	3096812	B	0.9067493081092834	So this is Lamme as considering the higher car structure in general.
3103675	3106837	A	0.959330141544342	That's very interesting.
3108175	3124815	A	0.8824880123138428	It's commonly remarked in the base graphs that they represent the connections amongst random variables and there's a relationship between their computability and their sparsity.
3124995	3149635	A	0.8073223233222961	The sparsity structure as in which variables do or do not influence each other makes the problem tractable through factorization and just kind of conceptually like if every one of a thousand variables or an unknown number of large variables if it was all by all the number of parameters to fit on that connectivity matrix would be very high.
3149742	3154020	A	0.5851528644561768	So statistical power would be very low for any given edge.
3154185	3168800	A	0.8116526007652283	Whereas the more and more constrained you make the connectivity of the variables, the more statistical power you have to resolve or kind of spend on fitting those edges like in a structural equation.
3169300	3182160	A	0.5045137405395508	But you might be losing sight of the unknown unknowns by constraining yourself to a very limited or fallacious topology of the variables.
3182280	3218300	A	0.8411668539047241	So there's this kind of structure learning statistical inference question in the Bayes graphs then on the neural side from the biological much of neuroscience is about understanding how the firing rate, connectivity patterns and other factors Hohwy the structure of those neural systems and their function like form and function enable adequate inference and inference on action.
3218875	3229590	A	0.8719266057014465	So it's like in both of those areas or really like in neural network artificial and neural networks and in variational.
3229620	3257150	A	0.8987515568733215	DAGs the discussion is about how the structure and the fine tuning work together to generate function and about some of the statistical or biological challenges of balancing different needs while also constraining the cost in terms of materials and biometabolism.
3257650	3265537	A	0.7327591776847839	So it's a very rich interoception that is being explored here.
3269462	3272787	A	0.827610433101654	If these models can really be moving back and forth.
3278137	3282587	B	0.820376992225647	In the sense that back and forth.
3285487	3299947	A	0.8620191216468811	Moving back and forth, like there's some imprints of the model that is implementation independent or like some interlingua or some semantics or compatibility, I don't really know.
3299980	3315627	A	0.8079150915145874	I mean, that's something we can explore is like what is it that is such that one could forward engineer and then reverse engineer and have like kind of an expectation maximization between these two areas.
3315657	3319412	A	0.7779492139816284	So what is it that's being solved?
3325087	3371412	B	0.6519169807434082	Yes, important point, for example, about you is that we can use the knowledge of Bayesian inference to explain your activity dynamics, which is crucial because people often say that characterizing neurodynamics is no straightforward, we may obtain some solution on your net dynamics, but the meaning of that dynamics in terms of the functional aspect is very unclear.
3371487	3391017	B	0.641798198223114	We don't know the meaning of connectivity strength matrices and what is the learning of the threshold factor, so on and so on those de Vries from the modern physiological phenomena.
3391077	3399702	B	0.6672590970993042	But it is not necessary to have clear linkage to functional exploration.
3399882	3403452	B	0.8071322441101074	So explanation of function of the brain.
3403557	3426425	B	0.8089718222618103	But once we transform translate this dynamics into Bayesian inference, then we can explain every functional aspect of the neural network diagrams architecture in terms of where established Bayesian inference under a particular crossover Bayesian model, in this case palm DP model.
3428287	3440912	B	0.8444141149520874	So now it turns out that synaptic strength correspond to a matrix B matrix, which are very established culture meaning.
3441862	3457937	B	0.5885579586029053	So yeah, this is useful to explain neuronsynatic property in terms of established statistics.
3464287	3481775	B	0.6415101289749146	Also, for the people in active inference lab site, it would be helpful to understand the neuronounce master straight about particular active interface model model.
3483712	3488592	B	0.8841578960418701	So I think it related to forward modeling.
3488727	3508100	B	0.8713429570198059	But finally to discuss with discuss about the border service rate of that forward model, we need to address the neural network architecture service property.
3508687	3529272	B	0.8794801235198975	So in that case, we can transform a particular force DP invasion modeling to a neural network architecture using this relationship and then get prediction about the substrate.
3529317	3547175	B	0.756606936454773	So if we have this based on model, this particular quantity in this model should be it would be possible using.
3554737	3555502	A	0.8785415291786194	Oh, it's all good.
3555520	3557987	A	0.69062340259552	Can you just repeat the last 20 seconds?
3558637	3559402	B	0.46103888750076294	Yes.
3559570	3578825	B	0.9049388766288757	So in the last part I mentioned about first we define the Bayesian model and then can predict what is the neural net substrates that correspond to that particular Beijing model.
3579337	3593412	B	0.7895135283470154	So this will be useful to identify the biological quantities that correspond to a quantity in Beijing.
3593487	3594125	B	0.6990138292312622	Chaos.
3603562	3606050	A	0.7669020891189575	There's a lot there.
3606787	3612367	A	0.8129109144210815	It makes me think about the inference of implementation and.
3612415	3626997	A	0.8348168730735779	Heuristics in the computational setting, which is often in the extreme disembodied, and the biological setting, which is in the extreme entirely embodied.
3627192	3662237	A	0.7785119414329529	And for a given generative model, the kinds of computational heuristics that can be applied include a whole host of different strategies ranging from sampling to tree exploration and branching to paralyzing the data architectures and all these other kinds of disparate strategies and software packages and implementations.
3664387	3686387	A	0.7691224217414856	But on the biological side, what is needed is something that's very simple but also very inscrutable, which is a given pattern of interactions must embody that calculation.
3688387	3699137	A	0.66755211353302	So that might mean that it can add three digit numbers, but it can't add two digit numbers under some constraints.
3700837	3720287	A	0.608123779296875	But what isn't accessible to that kind of morphological, biological or like form and functional computing, what's not accessible are the tree branching, the database decentralization, like they're a different set of heuristics.
3721012	3721372	B	0.5664745569229126	Right.
3721405	3736550	A	0.8597650527954102	But they're both very useful when we're thinking about making sentience artifacts or benefiting simply from the explainability across both sides of this figure.
3738562	3739372	B	0.5491447448730469	Yeah.
3739555	3742762	B	0.7657756805419922	So you now address an important point.
3742825	3756252	B	0.8184967041015625	So Homistry, it is very nontrivial whether there is a corresponding valve car architecture force any given Bayesian architectures.
3756357	3765927	B	0.5331952571868896	I believe it is impossible to design biography architectures to respond to arbitrary Bayesian architectures.
3766032	3775422	B	0.8231493234634399	So only a limited aspect of region model can be implemented in a vertical plausible manner.
3775542	3782787	B	0.8136772513389587	And that point is crucial as capitalization of biological network.
3782862	3785237	B	0.6266252994537354	Biological brain.
3789412	3789952	B	0.5491447448730469	Yeah.
3790045	3790567	A	0.7093686461448669	Wow.
3790690	3796112	A	0.8780814409255981	Well, just to kind of touch again on this forward in reverse engineering.
3798412	3798727	B	0.5803324580192566	For.
3798745	3812802	A	0.8853741884231567	A given POMDP if we're willing to compose it within a certain class, which might be quite general still, but some class of PMDP, as written.
3812832	3813725	A	0.7717182040214539	On the paper.
3814912	3830517	A	0.8451250195503235	We may be able to have a neural network architecture that would be very amenable to deep learning, low energy computing, pretraining various features.
3830652	3848000	A	0.857935905456543	And then on the other side, for a given artificial neural network that we come across in the wild or a model of neural dynamics that we fit using a neural network model.
3848737	3862337	A	0.824637234210968	So something in a neuroscience laboratory that model can have interpretability corresponding to the variables of a given POMDP.
3862762	3872162	A	0.865664005279541	And just to kind of give one more point on how that's going deeper than, for example, statistical parametric mapping SPM.
3872962	3883897	A	0.772049605846405	So let's just assume that the neural network we're dealing with is fit from brain data from some lucky Kant, right?
3883930	3908017	A	0.8954700231552124	Now, what would be possible or prior to this line of work or without this line of work, one could fit a neural dynamics model and then do all kinds of analyses, like power analyses on the different frequency spectra and say, look at the average firing rate or the correlation coefficients of firing rate.
3908065	3914122	A	0.8480722308158875	So fit the firing rates and the synaptic Plasticities and store all that data.
3914305	3924237	A	0.8204174041748047	And then we could just pick a POMDP that we've seen in the literature without any reference to the neural network and optimize the POMDP.
3924387	3935202	A	0.7870781421661377	And then we could say well, it turns out that when the POMDP o is high there's increased theta power in this firing pattern.
3935382	3947075	A	0.7979928851127625	So it's like comparing the descriptive statistics from the neural model to the descriptive summary statistics of the POMDP decision making model.
3947437	3964492	A	0.8729248642921448	However, with this formal connection there is actually an interpretability to the unobserved neural states which are what are being inferred from the fMRI measurement, from the EEG measurements and so on.
3964690	3974687	A	0.8521543145179749	Those underlying variables have a specific interpretability in relationship to the structure of the P-O-M DP.
3981337	3981832	B	0.7360090017318726	Right?
3981910	3988937	B	0.9522576332092285	So yeah, that's also very interesting important aspect.
3989587	4006467	B	0.907575786113739	So what you said is I think more conventional strategy and it is also formally related to model comparison aspect.
4006602	4018100	B	0.8555521965026855	So we usually think various modeling and identify or select what is the best model to explain a given data.
4018612	4051262	B	0.6460374593734741	And this reverse engineering idea involves such a model comparison aspect in the sense that we try to find the model with the best expandability which should we have the identical functionality, right directory address, the exact same Costa function architectures using the information natural transformation.
4051687	4060025	B	0.8611866235733032	So it should be up to explain the neural data in the Bayesian sense.
4062412	4085017	A	0.7142360210418701	Yeah, one can imagine how that would transform the way that current neuroimaging studies and technologies describe what it is about the measurement that provides information about the cognition model.
4085215	4104062	A	0.590126097202301	So, to give another related example, let's just say a person was wearing an EEG headset and a previous study had shown that increased alphaband activity was associated with this behavior.
4105762	4125362	A	0.871090292930603	That's comparing a descriptive statistic of the observations of the sensor and correlating the summarized observable to some other variable like anxiety or performance on a behavior.
4127512	4149887	A	0.8128696084022522	In contrast, an unobserved variable in this setting the actual underlying neural state is being correlated to some semantic generative models component.
4151437	4178312	A	0.806414783000946	So it's no longer necessarily that any single frequency band would be associated more or less with a given outcome, but it's actually some hidden state variability which gains the interpretability across this transformation.
4178887	4208687	A	0.8360474109649658	Which is a subtle point, but it speaks to how broadly the equivalents would reinterpret empirical neuroimaging results as well as a variety of artificial neural network experiments and diagnostics where people do lesion studies and double knockouts on artificial neural networks.
4211062	4237617	A	0.8268256187438965	So anywhere where somebody with awareness sees that a neural network, artificial or biological, is having summary features described and correlated to something that's more semantic in a quest for meaning may now have a different approach that involves formalizing.
4237677	4255037	A	0.8741811513900757	The model explicitly in terms of unobserved hidden states with a cost function akin to a variational free energy minimizing risk bounding surprise on the Unobservables.
4256362	4276637	A	0.8863771557807922	So even though the unobservables were modeled in a sense in the other conventional strategy like neural activity is a variable in fMRI experiments, it's underlying the bold signal.
4277287	4287512	A	0.6226274967193604	Yet this formalism concordance is a more coherent and powerful connection.
4295362	4296702	B	0.6181519627571106	Lib sold.
4296882	4302787	B	0.5490405559539795	So you now address this very important point.
4302850	4314982	B	0.8720166087150574	So first to address that so we need to clarify about what is a program, consider here.
4315060	4335042	B	0.7660890221595764	So this is a program Socalled metabasian problem in the sense that researchers try to infer or estimate neuro activity or brain activity which infer the external world dynamics.
4335177	4335607	B	0.5664745569229126	Right.
4335685	4342992	B	0.8882015347480774	So neuron or brain environment and we research brain activity.
4343127	4348812	B	0.8427281379699707	So there are two step processes.
4348962	4370612	B	0.5644471645355225	So this sort of meta Bay is quite tricky intractable because sometimes London variable becomes posterior about other aspects.
4370962	4377432	B	0.869030773639679	So I think there is some established approach about metabolism.
4377522	4404967	B	0.8744500279426575	But this paper provides some alternative in the sense that we separate two programs by saying that here what we import is simply neural network dynamics which is shown in the left hand side of this figure.
4405165	4414302	B	0.8763266801834106	So we feed data to conventional neural network model which is a simple differential creation.
4414482	4438562	B	0.5425313115119934	But thanks to this formal recovery between neural network dynamics and home VP behavior, then we can transform the resulting neural network architectures or dynamics into the page and in force in some sense post Hog mana.
4440487	4454587	B	0.6878560185432434	So we nicely avoid the directory addressing the meta agent program but obtain the same kind of solution in that sense.
4454650	4462332	B	0.8611655235290527	Yes, with combining with brain activity recording Lieke de Boer imaging.
4462497	4477250	B	0.6879724264144897	Yeah, we can estimate a plausible neural network model in the right hand side and we can transform that to home DB in the right hand side.
4480537	4481212	A	0.9184247851371765	Awesome.
4481350	4487012	A	0.9178134202957153	I'm going to show an image and ask a question from Dave in the chat.
4487437	4504562	A	0.9056059718132019	So, Dave made this image, it's the right side of figure one that we've just been looking at with the variational Bayesian information and he wrote the arc shown as impinging on the S self arc.
4506412	4508157	A	0.7737468481063843	Is this intentional?
4508247	4514150	A	0.8903065323829651	If so, it could represent tuning or modulation of the feedback of S into itself.
4517587	4519325	A	0.8624383807182312	Do you have a thought on this?
4520737	4521942	B	0.7123621106147766	It's attention?
4522002	4522672	B	0.46103888750076294	Yes.
4522855	4551137	B	0.9094850420951843	I think it's related to the usual formulation of home DP architecture and active inference concept in the sense that our decision or policy in the usual setting modify the state transition matrix b matrix.
4553662	4560472	B	0.8328380584716797	Here, delta is an alternative of policy of agent.
4560580	4573025	B	0.9080176949501038	So basically the director indicates stated transition metrics under a particular decision which agent made.
4573987	4582152	B	0.81220942735672	In that sense, what the agent changes is state transition metrics, not state itself directly.
4582332	4587062	B	0.7678238153457642	That's why we use this illustration.
4589362	4590125	A	0.9184247851371765	Awesome.
4591387	4600487	A	0.548354983329773	Very subtle but important point, which is when we look at the classical POMDP formulation.
4600837	4604167	A	0.9177669286727905	So here we'll look at a version shown in figure two.
4604215	4606550	A	0.8441991209983826	I'll just bring just figure two in.
4609312	4623287	A	0.9159209132194519	Could you describe what you just did about the role of the B matrix in influencing how hidden states change and how that is where our policies have impact?
4623637	4629500	A	0.8564046025276184	And also please, how do the top and the bottom of figure two differ?
4633087	4644432	B	0.887744665145874	Okay, so in the usual correlation under active inference with palm DB structure.
4644522	4673537	B	0.8945459127426147	So we for us to consider the prior inference and depending on the prior preference, we compute the expect free energy and its minimization provide the policy and the policy moderate state transition.
4674187	4686425	B	0.8906983733177185	So now in the upper Brea, we instead use the builder which is the option of the agent.
4687162	4712032	B	0.9037054777145386	So here option or decision was made for each timestep so that unlike the conventional formation, we have a sequence of delta and for each time step delta moderates active states cognition matrix B.
4712185	4730462	B	0.9114028215408325	So B is a matrix that transformed hidden state in the previous step to the current time step and its moderation indicate that under a specific decision rule.
4731637	4744250	B	0.8895480632781982	For example, if this F indicates our cognition in the virtual environment with the Gold decision move forward.
4745287	4751877	B	0.8330151438713074	But if we choose the no go decision, then it unchanged.
4751982	4770950	B	0.8928296566009521	So such a moderation of state transition was made by choosing debuta and the lower part correspond to Beijing inference made by Bayesian agent.
4771687	4801475	B	0.640707790851593	So basically there is a symmetry between a third part and roar part because we assume that this Beijing agent has a plausible guarantee model which nicely correspond to given environment defined in the above upper part in this figure.
4802062	4844087	B	0.6337830424308777	But one interesting thing asymmetry is that to model this particular canonical neural network, we don't consider an arrow or link from delta posteria to S posteria which is in the environment data moderate S in the next step through P matrix moderation.
4845187	4873037	B	0.7995629906654358	In this particular Bay jets which formally correspond to a canonical neural network, we don't consider that it is correspond to an absence of the projection from output layer to the middle layer.
4879087	4879850	B	0.584351658821106	Okay.
4885837	4896162	A	0.8825401067733765	This is from the 2020 paper, but it shows the neural network architectures, the two layer architectures.
4896312	4914737	A	0.8877468705177307	So could you restate the top and the bottom of figure two in the 22 paper and connect it to why it's important that you're studying two layer neural network models?
4915837	4917050	B	0.4783906638622284	I miss you.
4918162	4934387	A	0.8550167083740234	Yeah, can you just connect the asymmetry between the top and the bottom on figure two with the two layer neural network architectures?
4935337	4940375	A	0.5998576283454895	You said that the asymmetry, there's no direct link between.
4947475	4949437	B	0.7625042200088501	This is another story.
4949800	4972125	B	0.890859067440033	So in the previous paper there is only output or concept layer because we basically consider a single layer feedforward network.
4972200	4983145	B	0.5699732303619385	So my apologies for some confusion about the network architectures in the 2020 paper.
4983222	5000675	B	0.8885048627853394	So now upper part of this network architectures correspond to environmental generative process and only a lower part correspond to single foot feed for neural network architectures.
5001400	5012965	B	0.7500959634780884	So now this part is identical to a map from O to S.
5013057	5017625	B	0.8218392133712769	OSS area in the 2022 papers.
5023225	5031187	A	0.8923178911209106	Okay, so on the top of figure two is the actual generative process.
5032000	5047925	A	0.8798792958259583	It's the true structure of causation in the environment, which is to say that actions delta actually influence how states change through time via B delta.
5049625	5057150	A	0.8652575612068176	The generative process through the A matrix emits observations, sequences of observations.
5058100	5070125	A	0.8812516331672668	And here on the bottom with a mirrored structure is the generative model of the entity.
5070625	5078887	A	0.9000087380409241	So what's the relevance of the arrows and the more force factor graph structure on the bottom?
5080675	5088275	B	0.858893096446991	The arrow indicates active inference.
5090425	5111680	B	0.9033619165420532	So it's a flow of the information in the sense that to calculate in the step two, we use the information of step two conversation and step one's posterior expectation about hidden states.
5111802	5117900	B	0.8373386263847351	So those two determine the s two's expectation.
5118625	5135880	B	0.9210317730903625	Usually in the following graph, we consider retrospective arrow so in the sense that s three also affect the s two inference.
5135940	5151555	B	0.5588024854660034	But this corresponds to Bayesian smoother in the sense that we update every time step simultaneously to better inference.
5151690	5171230	B	0.889107346534729	However, what we consider here is more filtering approach in the sense that for each step we compute the latest hidden states and then we don't change any other states in the past.
5171427	5178712	B	0.5336402654647827	So that's why we don't consider the arrow from future to the past.
5182337	5183012	A	0.9184247851371765	Awesome.
5183150	5183492	A	0.5491447448730469	Yeah.
5183540	5206587	A	0.6084095239639282	Just to highlight that in the Bayesian smoothing approach, it's kind of like fitting a spline because it takes the whole time series and it fits the smoothest possible line or the line whose smoothness is on the AIC BIC frontier.
5207612	5227362	A	0.7598077654838562	But here on the bottom with the almost pseudocode implementation provided by the Force Factor graph, which was demonstrated to be equivalent with the Bayesian graph in the 2017 work with Friston, Par and de Vries.
5228237	5253150	A	0.8719705939292908	This architectures is reflecting a filtering scheme like a common filter or just generalized Bayesian filtering through time, where estimates are being carried forward and changed time point to time point, such that the decision rules, or the updates perhaps more accurately, are defined between time points.
5253662	5261567	A	0.797831654548645	And the total time series does not have to be loaded into memory or remembered at once.
5261765	5272362	A	0.8627719283103943	And then the Bayesian filtering approach has the asymmetry with a different consideration of action.
5273987	5287875	A	0.5827471613883972	So why again is it that action is considered differently in the Bayesian filtering approach on the bottom of the generative models than the consideration of action in the generative process.
5291162	5302125	B	0.5581706166267395	That correspond to lack of cognition from Y to X in the figure one?
5303387	5312100	B	0.6406744122505188	Or probably a figure four is helpful to that relationship.
5317487	5318250	A	0.6727579832077026	Four?
5318762	5319392	B	0.5491447448730469	Yeah.
5319515	5327237	B	0.8971371054649353	This is an example network architecture comprising input Brea, output Brea.
5327375	5340907	B	0.9110991358757019	What we consider is information flow from sensory to middle Brea and middle area have a self connection, recurrent connection and middle area project to output layer.
5341072	5347812	B	0.6103971600532532	So there is no connection from all output layer to middle layer.
5348387	5349150	B	0.5664745569229126	Right.
5350412	5365362	B	0.5428928732872009	So that's why we don't consider the link from data in the bottom layer of the figure to posterior.
5367062	5374662	B	0.6728655695915222	So this is different from true generative process in the environment.
5377975	5380500	B	0.6590452194213867	This is a kind of simplification.
5380650	5394650	B	0.8409977555274963	So because our purpose is identifying the plausible Bayesian model which correspond to this type of neural network canonical network.
5396200	5417300	B	0.9039918780326843	So in other words, this neural network uses approximation about that point or use limited form of palm DP scheme.
5423700	5424555	A	0.6283750534057617	Thanks.
5424752	5433170	A	0.9067398905754089	So could you describe W-V-K and Gamma?
5433335	5439925	A	0.8654643297195435	Just what is the biological or functional interpretation of those variables?
5440800	5445837	A	0.8257004618644714	What brain regions or what processes or pathologies do they map to?
5447025	5459862	B	0.8678262233734131	Okay, so basically, WVK, synaptic strength is in the form of matrix and active inference.
5464875	5493215	B	0.9094690680503845	They represent connection in the different layer or different architectures in the sense that W means forward connectivity from sensory Laje to middle Laje, k correspond to recurrent network recurrent connectivity and V correspond to projection from middle Brea to output layer.
5493395	5534200	B	0.8060925006866455	So in this paper, we don't discuss the relation to brain anatomy in detail, but what one can consider analogy, for example, say x corresponds to several cortex activity and Y, for example correspond to cerebral wrong in the sense that it determines the action.
5534625	5542820	B	0.9119424223899841	So it is considered that in the cerebrum there is a signal that represents choice.
5542910	5549200	B	0.8738073110580444	This is joined for examples goal no go decision made in cergram.
5551575	5557550	B	0.832018256187439	It's analogous to this particular architectures.
5557700	5582615	B	0.8866313695907593	On the other hand, in the several cortex we compute the sensory information to generate some inference, prediction and planning the way it is computer by this recurrent network.
5582720	5626750	B	0.6216258406639099	In this particular modeling, although we don't separate brain region in detail, but this recurrent network is sufficient this graph of recurrent network is sufficient to characterize any type brain architecture in the sense that we can design any higher car or mutually connected architecture using a generic crossover recurrent network by changing weight.
5634750	5635470	A	0.9184247851371765	Awesome.
5635622	5642187	A	0.8977538347244263	So the middle layer we can think of as like the cognition stuff.
5642625	5656300	A	0.8763316869735718	It's the internal states when we talk about perception, cognition, action in the active scheme or even in the sandwich model of cognition, perception, cognition action.
5657250	5678100	A	0.9022550582885742	So W is describing how those sensory inputs either in one step or composably in multiple steps become processed to these internal representation of hidden external causes inferred external states.
5678162	5688240	A	0.8767582178115845	And so these are the states that have that sigma relationship and a generalized synchrony with external states.
5688407	5706550	A	0.8520497679710388	The sigma and the generalized synchrony are not discussed in your paper, but it connects to other work and the recurrent connections are facilitating attention or waiting of the stimuli.
5708250	5716450	A	0.881449818611145	This is the recurrent learning loop and the relationship of the A between observations and hidden state estimates.
5718225	5729645	A	0.8607588410377502	And then a different kind of modulation comes Hinton play between the hidden state estimate of the internal states state and the action selection.
5729810	5734410	A	0.86183762550354	So what is gamma corresponding to?
5734442	5746812	A	0.7961727380752563	And why is the gamma modulation between layers two and three differing functionally from the k synaptic modulation of one and two?
5747850	5758490	B	0.9023073315620422	Yeah, so K matrix basically formally correspond to B matrix in the Bayesian information.
5758595	5774037	B	0.911096453666687	So we rotate the information about the prediction, right, our narrator or our expectation about the next state based on the previous state.
5774775	5782550	B	0.8209996223449707	On the other hand, Laurel gamma is quite different from such a computation.
5782625	5796330	B	0.8574314713478088	Gamma basically means risk function, which is in principle can we use arbitrary risk function.
5796452	5833587	B	0.8943491578102112	So this is a part of generative models we designed and the rule of risk function in generative model formulation is attention form of generative models depending on that value of gamma which examples retrospective moderation of evaluation of task decisions given an outcome in the future.
5835225	5841485	B	0.8876855373382568	In terms of neural network, of course it corresponds to some neural modulation.
5841605	5854580	B	0.5981672406196594	For example, Dopaminergic moderation is famous in the literature which moderates the activity and fluxicity of various brain vision.
5854640	5870950	B	0.9041245579719543	But we particularly focus on Dopaminergic or any kind of neuromoduration of cyanogic prosthesis in the output trigger which may correspond to Cergram.
5871525	5905362	B	0.9018924236297607	So in the Serbram neural activity or processes moderated by Dopaminergic, input from is used as the optimization action rule, decision rule or sometimes attention help us.
5909212	5909797	A	0.9184247851371765	Awesome.
5909905	5925817	A	0.7188023328781128	Very interesting because in some previous papers and models that we've looked at, attention is dealt with as policy selection on mental states.
5926015	5936012	A	0.8787127137184143	So internal action selection, it's an action like variable describing attention and awareness and even metacognition.
5936737	5967962	A	0.5501455068588257	And so that connects the role of Dopamine in motor decision making seen in many Dyskinesias but also with the role of Dopamine in seemingly nonmotor based decisionmaking like gambling or investing where it doesn't seem to immediately translate to a given motor sequence.
5968912	5990877	A	0.7124358415603638	Yet it has analogous computational characteristics and the comorbidities and the side effects of different drugs that affect the Dopamine neurophysiology are known to have carryover in terms of like the rigidity or excessivity of motor and decision making aspects.
5990907	6019337	A	0.529427170753479	So it's like interesting that Dopamine has long been understood to have that parallel role in attention as cognitive action and motor action and that was established empirically through modifications of Dopamine signaling and also had been modeled analogously with computational neuroscience.
6019687	6035900	A	0.7596100568771362	And this is providing again a slightly different interpretation of that very well studied Dofaminergic modulation of attention and policy.
6040837	6041600	B	0.46103888750076294	Yes.
6043462	6051887	B	0.5904572010040283	In addition to that, I believe another important aspects is correlation of scientific processing by document.
6075850	6078937	A	0.8277719616889954	Do you want to show something or yeah.
6080875	6085387	B	0.8644910454750061	Can you see this paper?
6087475	6091325	B	0.8261170983314514	I sent you a chat.
6092275	6096275	B	0.7235491871833801	If you can't, I'll send you a PDF.
6097450	6099037	A	0.6621345281600952	Okay, let me see.
6100300	6102412	A	0.7972273826599121	I'll look at it up now.
6107425	6107980	A	0.4896697998046875	All right.
6108027	6122460	A	0.8873566389083862	The paper is a critical time window for Dopamine actions on the structural plasticity of dendritic spines from 2014 byagasha.
6122580	6125155	A	0.9392553567886353	So what is interesting about this paper.
6125352	6154395	B	0.5317803621292114	Yeah, it basically explained conversation of plasticity by Dopamine, which is common but crucial point of this paper is that it shows that it proved that Dopaminezic input can moderate after hebion prosthesis is established.
6154485	6170190	B	0.8578988909721375	So this paper showed that they add domain logic input for about 2 seconds after or several seconds after the Hebbian process is established.
6170220	6194520	B	0.8734358549118042	But such a post hoc moderation, post hoc introduction of heterotopamagic Impetu is sufficient to change the past capacity which may be associated with the Costa hoc evolution of our past decisions.
6194610	6205180	B	0.9141200184822083	So by decision making we of course changes the changes the weight matrix by through trust 50.
6205377	6221000	B	0.7210248708724976	But to evaluate the goodness or badness of our decision, we need to see observe the future outcome which is propagated by for example, Dopamine.
6221425	6245437	B	0.8160250782966614	And this paper nicely show empirically that Dopamine actually can change the past evaluation, maybe after such a psychic level, very local level, ecoscopic level.
6252250	6263520	A	0.8308271765708923	So there's a short term window, the critical time window that they're describing.
6263610	6265935	A	0.7067142724990845	But there's some window.
6266130	6278605	A	0.7826635241508484	Yeah, some window by which dopamine potentially unrelated to the initial heavy and plasticity events, right.
6278727	6302662	A	0.8481422662734985	Where secondary dopamine signaling or not secondary just after the initial fact, potentially of a different valence or the same valence can synergize or cancel the plasticity formed in the moment.
6303100	6303862	B	0.5662814974784851	Exactly.
6305425	6312862	B	0.8117495179176331	And this is not limited to Dopamine, but other neuro moderator can also do this.
6318250	6325110	A	0.6647605299949646	Well, on one hand, how does this change our understanding of animal neurophysiology?
6325305	6333575	A	0.7934632897377014	And then I guess, on the other hand, how does this influence how we would design sentient artifacts.
6342625	6351112	B	0.6030767560005188	For both animals and artificial agent?
6353350	6357820	B	0.521881639957428	One important message I free with us.
6357972	6366275	B	0.598333477973938	So this tells us possible simple architectures to make learning.
6367900	6396037	B	0.8478472828865051	This is association between past decision and future reward or any risk factors, which is otherwise computed by computing forward prediction by iterating some computational, this is a usual way to predict the future event and then select the option.
6396475	6415100	B	0.6673157811164856	But using this property, biological property, which is observed in experiment, we can design, we can imagine other simpler architecture to make a planning.
6417250	6440600	B	0.8275951743125916	So for both animals and synthetic Bayesian agent, it provides an alternative explanation about the association between our past decision and the future risk and the optimization of our decision to maximize, reward or minimize risk.
6445075	6457335	A	0.7037177085876465	Well, one interesting note is we spoke earlier about the difference between the Bayesian smoothing all at once approach and the Bayesian filtering step by step approach.
6457530	6466120	A	0.5765283703804016	Now, if one had infinite knowledge and computational resources, the Bayesian smoothing approach is the way to go.
6466272	6469455	A	0.6117194294929504	Like, you don't want the decision rule for investment.
6469590	6476940	A	0.5632453560829163	You want to look at the whole time series past, present and future, and know the best moments to have made the trades.
6477045	6478500	A	0.6546281576156616	I mean, there's no comparison.
6478575	6481515	A	0.7041006684303284	You're going to do better with the Bayesian smoothing.
6481695	6489640	A	0.7955893278121948	However, it's just implausible computationally and because it requires total memory of the past and knowledge of the future.
6489807	6501037	A	0.5489904284477234	So that's what motivates the development of Bayesian filtering approaches, which are tractable and calculable through time.
6501775	6505625	A	0.4944218397140503	Yet with this time delayed modulation.
6507400	6507895	B	0.5885526537895203	Part.
6507972	6512662	A	0.8623244762420654	Of the Bayesian smoothing strength comma back into play.
6513850	6523537	A	0.5737380385398865	It doesn't enable true anticipation of future states, but that's what the expected free energy does.
6524650	6531712	A	0.8097062110900879	However, the delayed neuromodulation allows for reconsideration of a window of past states.
6532525	6548900	A	0.896230936050415	And so in that way it corresponds to like a slightly deeper filter, not just a filter of a time step of one, but a filter of like a rolling window of five or with some decay.
6549550	6562837	A	0.5592350363731384	And you don't want that window to be too big because if the window were ten minutes, then too many contrasting stimuli would get piled together.
6563350	6567050	A	0.785103440284729	The Dopamine level would just converge to a mean field average.
6567400	6582412	A	0.8510470390319824	But there's some time decay or time constant on the post hoc modulation where that neuromodulatory signal is actually a parameter of interest.
6583000	6591787	A	0.729786217212677	And that's not an infinitely long or infinitely short window, but it's some niche dependent amount of time.
6592450	6610920	A	0.6136233806610107	And that's a very interpretable and first principles interpretation of the computational role of neuromodulators in a way that is also consistent with all these other concordances we've been exploring.
6611085	6622400	A	0.976565420627594	So it's quite an interesting connection back, I guess, in our final minutes of this discussion.
6626850	6627987	A	0.759223997592926	What are you?
6628725	6653800	A	0.7958566546440125	Well, maybe go to the beginning at the end, which I meant to ask earlier, but it's a good way that we can sort of close today and look forward, which is how did you come to this line of research specifically studying neural networks in this way with Karl Friston and your colleagues?
6658275	6677915	B	0.5597987174987793	So, yes, so my interest was the characterization of Barricade network.
6678095	6687155	B	0.4923993647098541	So my first motivation is to make biologically plausible artificial intelligence.
6687215	6695725	B	0.8485372066497803	But to achieve that, we need to know about biological brain or biological neural networking.
6701100	6716725	B	0.8715739250183105	In these several years, I collaborated with the doctor professor Californiston to study about his salary principal after doing forest.
6721125	6749390	B	0.8766372799873352	My question during that period was the priority principle, is everything about the biological possible neural network or is there another aspect that can characterize the virus car neural network?
6749495	6752350	B	0.5330686569213867	So it is non trivial.
6753150	6755015	B	0.5706772208213806	It was non trivial.
6755195	6767130	B	0.84393310546875	So that's why I tried to start from characterizing the neural network first.
6767327	6796887	B	0.8193053007125854	So our strategy is not considering the way of implementing any Bayesian algorithm as the brain architectures, but my interest is rather characterization of a given vertical network in terms of some other things.
6797400	6803200	B	0.8519021272659302	One possible way is of course based on inference free energy transplant reinforce.
6803550	6809425	B	0.8904638290405273	So that's why I start from characterizing power's network.
6810675	6818575	B	0.5872383117675781	But just defining neural network architecture is insufficient.
6821625	6833675	B	0.797863245010376	It is not tractable, it is far beyond the computational tractability as the mathematical analysis.
6833750	6843400	B	0.805845320224762	And we need some assumptions or some trick to increase the tractability.
6844500	6865580	B	0.540503740310669	One day I came up with an idea that in which we consider that both new activity and fastest follow the same cost function gradient.
6865715	6877295	B	0.6391971707344055	This is very much an allergy with physical system like Lagrangian information geometry, Hamiltonian formation.
6877385	6899435	B	0.9145013093948364	So usually we consider some energy landscape and design plausible trajectory as the evolution of some principle of minimum action or restruction.
6899630	6929575	B	0.8626276850700378	So we imagine that what if we applied such idea to computational neural network or biological neural networks to characterize their dynamics in the first principle, that's the first computational step to come up with this framework.
6931425	6958335	B	0.5434722900390625	And finally we noticed that it is not easy to connect the Newtonian dynamics with this type of neural activity study because neural activification not necessary to be a second order differential equation, but rather it is fast order and considering many things.
6958442	7003675	B	0.7210469841957092	Then we finally use a Costa function proposal in the papers, which is not necessary to have a former identity with the so called lavalier in the Newtonian physics, but it is rather plausible as the rule or underlying mechanism of such type of network.
7011800	7012562	A	0.9184247851371765	Awesome.
7013525	7019810	A	0.9439004063606262	Well, it has been quite an interesting dot one.
7019992	7024487	A	0.9860720038414001	I really appreciate everything you've shared today.
7025075	7027280	A	0.902848482131958	Is there anything else you want to add at this point?
7027327	7028912	A	0.7310769557952881	Otherwise we'll talk again.
7030325	7036050	B	0.8209488391876221	Yeah, I already speak a role.
7037675	7038485	A	0.8529649376869202	Thank you.
7038592	7040675	A	0.7564576864242554	Alright, talk to you later.
7040737	7041450	A	0.5137446522712708	Bye.
7042325	7044400	B	0.9887828230857849	Thank you very much for a nice discussion.

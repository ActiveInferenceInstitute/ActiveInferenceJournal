start	end	startTime	summary	headline	gist
33087	75962	00:33	This is actinF livestream number 51. It's November 9, 2022. actinF is a participatory online institute that is communication, learning and practicing applied active inference. All backgrounds and perspectives are welcome. Head over activeinformativeorg to learn more about participating in different institute projects.	ActinF is an online institute that is communication, learning and practicing applied active inference.	Intro to the active inference institute.
76912	199687	01:16	Daniel, Kuya and others are discussing the canonical neural networks perform active Inference from 2022 paper daniel is a researcher in California. Kuya is from Brain Science Institute, Japan daniel wants to learn more about the fundamentals of neural network synthesis or translation.	Daniel Kuya and others are discussing the canonical neural networks, perform active inference from 2022 paper.	Canonical neural networks perform active inference from 2022.
203200	358660	03:23	In the paper, we are trying to characterize the dynamics and activity of a canonical neural network. In terms of Bayesian inference, we consider a biological plausible cost function for this particular canonical network and show the equivalence between that costa function and the variation of the free energy.	In the paper we are trying to describe the dynamics and activity of a canonical neural network in terms of Bayesian inference.	The fundamental parallel.
358842	973537	05:58	There is a clear parallel between the functional structure of informational free energy and the costa function for neural networks and the cost function for the loss function. Both expressions have the first term being more like a cognition perceptual sensory learning term and the second term like a control theoretic action selection.	There is a clear parallel between the functional structure of informational free energy and the costa function for neural networks.	The informational free energy expression.
974637	2297190	16:14	There is a hebion plasticity happening between the perceptual layer and the cognitive layer of the POMDP. The first half of the neural network is trained according to heavy and plasticity rules. The second half is optimizing based upon retroactive analysis of consequences of action according to the fictive causality construction.	There is a hebion plasticity happening between the perceptual layer and the cognitive layer of the POMDP.	Activity dynamics and plasticity.
2297370	2593527	38:17	The MATLAB scripts are available and active. On Zinodo, there is a GitHub repo for reverse engineering. Macrab is sufficient to encode the whole script. If you run the script, you can see the process of an agent solving the maze task. The agent is on the right hand side of image at the goal position and observes our neighboring state.	The MATLAB scripts are available and active on Zinodo.	The code availability statement.
2593707	3595362	43:13	In the checker board hered correspond to possibility, expectation about active states and decision. In the middle panel, middle point correspond to the current position of agent. The current brightness corresponds to the expectation about the agent's decision. The agent is thinking about policies of length four. There's some degeneracy because there's not 256 squares.	In the checker board hered correspond to possibility, expectation about active states and decision.	The checker board represents decision.
3600512	4076675	1:00:00	In your script there's a toggle between SIM type one and SIM type two corresponding to the POMDP in the neural network. The canonical neural network architecture facilitates its translation into pom DP form. It assumes sigmoid or activation function, which is a good response. Summarizer.	In your script, there's a toggle between SIM type one and SIM type two, corresponding to the POMDP in the neural network.	Neural networks to pomdp.
4078837	4189112	1:07:58	Dave explains to Dean that the element of matrix is a matrix. Dave describes the mass block matrix as a block matrix of four matrices. Dean explains to Dave that a matrix is two by two matrices, like a matrix in the ear. Pointing, Dave answers Dean's questions.	Dave describes the mass block matrix as a block matrix of four matrices.	Mass block matrix.
4189625	5458987	1:09:49	Karl Friston explains why it's important that most of the values in a generative model matrix assume values of exactly zero. He also explains how to translate Bayesian model to a particular form of POMDP and how to create an efficient basic model using Home DP.	Karl Friston explains why it's important that most of the values in a generative model matrix assume zero.	Sparse matrix design.
5460612	5625175	1:31:00	As a PhD student he did an experiment in virtual system. Now he works at the Bijan Institute and he's a princepal investigator of Celery unit. He doesn't use experimental setup so any experimental bidding is down. With some collaboration, he hopes to show some interesting results following results using animal data.	Now. He works at the Bijan Institute and he's a princepal investigator of Celery unit.	Introduction to experimental systems.
5630862	6433750	1:33:50	In the paper, Dean and Laj focus on a discrete state space model. This model does not expand into synaptogenesis neurogenesis and neurogenetic processes. Dean is interested in how these larger scale structural changes become reflected in artificial neural networks.	Dean is interested in how these larger scale structural changes become reflected in artificial neural networks.	Other interesting sections.
6439650	6912712	1:47:19	There are many questions for the participants of the discussion. They want to know more about the computational complexity of the models. They appreciate the time that you took for these discussions. They wish you the best of luck in these continued directions and see you next time.	There are many questions for the participants of the discussion.	Final thoughts and questions.

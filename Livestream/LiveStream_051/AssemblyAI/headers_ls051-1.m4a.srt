1
00:00:39,287 --> 00:00:41,727
All right, hello everyone. Welcome.

2
00:00:41,895 --> 00:00:45,507
This is ActInf livestream number 51 one.

3
00:00:45,660 --> 00:00:49,197
We are in the second discussion of this

4
00:00:49,230 --> 00:00:51,372
paper, canonical Neural Networks perform

5
00:00:51,480 --> 00:00:54,482
Active Inference. Welcome to the active

6
00:00:54,497 --> 00:00:56,382
inference institute. We're a

7
00:00:56,385 --> 00:00:58,677
participatory online institute that is

8
00:00:58,695 --> 00:01:00,527
communication, learning and practicing

9
00:01:00,557 --> 00:01:02,817
applied active inference. You can find

10
00:01:02,865 --> 00:01:05,952
us on this slide and this is recorded

11
00:01:06,045 --> 00:01:08,262
in an archived livestream. So please

12
00:01:08,325 --> 00:01:09,977
provide us feedback so we can improve

13
00:01:10,007 --> 00:01:12,177
our work. All backgrounds and

14
00:01:12,195 --> 00:01:13,772
perspectives are welcome and we'll

15
00:01:13,817 --> 00:01:15,627
follow good video etiquette for live

16
00:01:15,645 --> 00:01:19,217
streams, head over active inference.org

17
00:01:19,277 --> 00:01:21,522
to learn more about the institute and

18
00:01:21,555 --> 00:01:23,877
how to participate in projects and

19
00:01:23,895 --> 00:01:26,800
learning groups. All right,

20
00:01:27,162 --> 00:01:29,292
we're in ActInf livestream number 51

21
00:01:29,340 --> 00:01:32,650
one, and having our first

22
00:01:33,387 --> 00:01:36,732
nonsolo discussion on this paper,

23
00:01:36,810 --> 00:01:38,927
canonical Neural Networks perform active

24
00:01:38,957 --> 00:01:43,602
inference and really appreciative that

25
00:01:43,620 --> 00:01:45,897
you've joined today. It's going to be a

26
00:01:45,930 --> 00:01:48,612
great discussion. We'll begin with

27
00:01:48,675 --> 00:01:51,147
introductions. I'll say hello and then

28
00:01:51,180 --> 00:01:54,657
please just jump in however you'd like.

29
00:01:54,810 --> 00:01:57,477
And we can start by setting some

30
00:01:57,570 --> 00:02:00,947
context. So I'm Daniel, I'm a researcher

31
00:02:00,992 --> 00:02:04,152
in California, and I was interested in

32
00:02:04,170 --> 00:02:07,332
this paper because we've been talking a

33
00:02:07,335 --> 00:02:09,447
lot about active inference from a

34
00:02:09,480 --> 00:02:11,937
variety of different perspectives, from

35
00:02:12,000 --> 00:02:15,897
the more fundamental math and physics to

36
00:02:16,005 --> 00:02:18,332
some applications, philosophy,

37
00:02:18,422 --> 00:02:21,222
embodiment, all these really interesting

38
00:02:21,330 --> 00:02:24,252
threads. And this paper seems to make a

39
00:02:24,270 --> 00:02:28,047
really clear meaningful contribution and

40
00:02:28,080 --> 00:02:31,157
connection by connecting active

41
00:02:31,172 --> 00:02:33,747
inference entities and this approach of

42
00:02:33,780 --> 00:02:37,347
modeling to neural networks which are in

43
00:02:37,455 --> 00:02:40,827
daily use globally. So thought it was a

44
00:02:40,845 --> 00:02:43,647
fascinating connection and really

45
00:02:43,680 --> 00:02:45,897
appreciate that we can talk about this

46
00:02:45,930 --> 00:02:48,550
today. So to you and welcome.

47
00:02:54,375 --> 00:02:56,115
Go forward, Takuya, however you'd like

48
00:02:56,132 --> 00:02:59,337
to introduce and say hello. Yeah.

49
00:02:59,700 --> 00:03:03,135
Hi. I'm Tafia Isomura, neuroscientist in

50
00:03:03,167 --> 00:03:06,175
Lique Brain Science Institute in Japan.

51
00:03:06,525 --> 00:03:10,355
I'm particularly interested in universal

52
00:03:10,490 --> 00:03:13,680
characterization of neural network and

53
00:03:13,727 --> 00:03:17,262
brain using mathematical techniques. So

54
00:03:18,225 --> 00:03:21,555
this work I believe important

55
00:03:21,752 --> 00:03:25,010
as a link between active brain forest

56
00:03:25,055 --> 00:03:28,280
aspect, Bayesian aspect of the brain,

57
00:03:28,340 --> 00:03:32,415
and the dynamics system aspect of the

58
00:03:32,432 --> 00:03:35,940
neural network. So I'm very happy to

59
00:03:35,957 --> 00:03:38,340
join this discussion session. Thank you

60
00:03:38,357 --> 00:03:40,962
for invitation. Nice to meet you.

61
00:03:42,000 --> 00:03:44,337
Nice to meet you as well.

62
00:03:46,350 --> 00:03:49,445
The first thing you added, the universal

63
00:03:49,535 --> 00:03:51,700
characterization of neural networks.

64
00:03:52,275 --> 00:03:55,175
What is the universal characterization

65
00:03:55,250 --> 00:03:57,480
of neural networks? Why is it being

66
00:03:57,527 --> 00:03:59,712
pursued in this area of research?

67
00:04:01,200 --> 00:04:04,437
So, as a narrow sense,

68
00:04:05,100 --> 00:04:08,555
my gain aim

69
00:04:08,615 --> 00:04:11,712
of this paper is that so, you know,

70
00:04:13,275 --> 00:04:15,305
people active inference lab

71
00:04:15,440 --> 00:04:20,150
communication to characterize brain

72
00:04:20,225 --> 00:04:23,250
activity, behavior, so on, so on,

73
00:04:23,312 --> 00:04:27,375
but which would be different from

74
00:04:27,512 --> 00:04:30,300
conventional neural network. So there is

75
00:04:30,437 --> 00:04:34,125
a crossover program which is

76
00:04:34,187 --> 00:04:37,190
associated with conventional neural

77
00:04:37,220 --> 00:04:41,187
network and it is not very clear whether

78
00:04:41,925 --> 00:04:44,510
all characterization of computational

79
00:04:44,555 --> 00:04:47,025
neural network can be explained by

80
00:04:47,087 --> 00:04:49,485
activity infrastructure principle or

81
00:04:49,517 --> 00:04:53,030
not. So here universal characterization

82
00:04:53,165 --> 00:04:55,770
means that characterization of every

83
00:04:55,847 --> 00:04:58,750
aspect of conventional neural network

84
00:05:00,000 --> 00:05:03,135
which is a kind of dynamics system

85
00:05:03,317 --> 00:05:07,287
derived as association between

86
00:05:07,800 --> 00:05:11,400
biological phenomena and simple

87
00:05:11,537 --> 00:05:14,390
mathematics. Car formula using gift

88
00:05:14,420 --> 00:05:18,360
card, using differential equations as

89
00:05:18,392 --> 00:05:21,860
the broad sense. I think universal

90
00:05:21,905 --> 00:05:26,787
characterization means that well,

91
00:05:28,200 --> 00:05:30,740
it is a characterization of brain

92
00:05:30,845 --> 00:05:34,362
intelligence, but it's a big picture and

93
00:05:35,325 --> 00:05:39,315
the paper particular address is only one

94
00:05:39,482 --> 00:05:42,250
aspect of the Bull picture.

95
00:05:46,200 --> 00:05:49,950
All right? So it'll be great

96
00:05:50,012 --> 00:05:53,370
to pull back to really understand what

97
00:05:53,522 --> 00:05:56,565
synthesis is happening. So I'm going to

98
00:05:56,582 --> 00:06:00,045
ask what makes a neural network model a

99
00:06:00,047 --> 00:06:02,610
neural network model and what makes

100
00:06:02,642 --> 00:06:04,520
active inference lab model an active

101
00:06:04,535 --> 00:06:07,515
inference model? Is this synthesis and

102
00:06:07,532 --> 00:06:10,860
connection you've made true? Because of

103
00:06:10,892 --> 00:06:11,487
what?

104
00:06:14,175 --> 00:06:18,030
Because basically what we

105
00:06:18,077 --> 00:06:22,525
show is the mathematical

106
00:06:22,875 --> 00:06:26,655
equivalence between the formulation of

107
00:06:26,852 --> 00:06:28,740
canonical neural networks and the

108
00:06:28,757 --> 00:06:31,890
formulation active inference lab in the

109
00:06:31,907 --> 00:06:35,430
sense that we show that as possible

110
00:06:35,552 --> 00:06:38,610
neural networks can be characterized by

111
00:06:38,717 --> 00:06:41,075
minimization of some biological

112
00:06:41,150 --> 00:06:43,530
plausible cost function. And we show

113
00:06:43,577 --> 00:06:46,812
that that cost function can be least as

114
00:06:51,225 --> 00:06:53,430
variational based on inference and a

115
00:06:53,477 --> 00:06:57,465
particular cross of gentlemen model in

116
00:06:57,482 --> 00:07:00,860
terms of well known partially observable

117
00:07:00,980 --> 00:07:02,187
position process.

118
00:07:06,000 --> 00:07:09,560
Alright, shall we perhaps

119
00:07:09,680 --> 00:07:13,065
walk through some of the sections of

120
00:07:13,082 --> 00:07:17,790
the paper? It would be awesome. Just for

121
00:07:17,807 --> 00:07:20,415
each of these sections, maybe the

122
00:07:20,432 --> 00:07:22,160
numbered and the lettered sections.

123
00:07:22,355 --> 00:07:24,810
What does the section aim to show and

124
00:07:24,842 --> 00:07:27,087
why was it there in the paper?

125
00:07:37,125 --> 00:07:41,090
Briefly it's

126
00:07:41,120 --> 00:07:44,825
over, right? So briefly.

127
00:07:44,900 --> 00:07:49,150
So first we introduce

128
00:07:50,550 --> 00:07:53,490
so the gain issue main program,

129
00:07:53,657 --> 00:07:56,787
our interest, which is relationship.

130
00:08:00,975 --> 00:08:04,005
We try to make a formal ring between

131
00:08:04,052 --> 00:08:05,915
neural network and active reinforcements

132
00:08:05,945 --> 00:08:09,935
that gain program background.

133
00:08:10,055 --> 00:08:14,125
And then we first formulate

134
00:08:14,850 --> 00:08:16,640
the equivalence, mathematical

135
00:08:16,670 --> 00:08:20,445
equivalence, in a very Brea manner. So

136
00:08:20,597 --> 00:08:24,065
in the first section in results,

137
00:08:24,170 --> 00:08:28,785
we formulate the

138
00:08:28,817 --> 00:08:33,285
relationship using complete

139
00:08:33,392 --> 00:08:35,960
craft serum, which is well known

140
00:08:36,080 --> 00:08:40,155
statistical theorem proposed very

141
00:08:40,352 --> 00:08:44,520
long time ago. And using

142
00:08:44,597 --> 00:08:48,585
that we link a

143
00:08:48,617 --> 00:08:51,615
general form of neural network with a

144
00:08:51,632 --> 00:08:54,720
general form of variational data

145
00:08:54,797 --> 00:08:58,185
impress. But a problem is

146
00:08:58,217 --> 00:09:00,825
that this characterization does not

147
00:09:00,887 --> 00:09:04,062
address a specific generative model

148
00:09:04,800 --> 00:09:09,915
which is crucial to characterize a

149
00:09:09,932 --> 00:09:13,160
specific model, specific neural network

150
00:09:13,205 --> 00:09:18,245
dynamics. So in the following sections,

151
00:09:18,335 --> 00:09:21,615
we characterize the problem

152
00:09:21,707 --> 00:09:24,335
using Pomodb or partially observable

153
00:09:24,380 --> 00:09:28,125
Markosition process and link that

154
00:09:28,262 --> 00:09:31,605
model with a particular class force

155
00:09:31,727 --> 00:09:33,925
canonical neural network.

156
00:09:35,625 --> 00:09:40,110
And then we simulated we

157
00:09:40,142 --> 00:09:44,645
use the simulation to propagate

158
00:09:44,810 --> 00:09:48,330
that property in terms of some major

159
00:09:48,377 --> 00:09:49,150
tasks.

160
00:09:55,000 --> 00:09:57,412
All right, thank you for this.

161
00:09:58,150 --> 00:10:01,345
Could we talk about the complete class

162
00:10:01,422 --> 00:10:04,885
theorem? So what is the scope of the

163
00:10:04,917 --> 00:10:08,437
complete class theorem and why

164
00:10:08,950 --> 00:10:11,040
was it the relevant set of the neural

165
00:10:11,070 --> 00:10:13,240
networks to pursue or the right way to

166
00:10:13,257 --> 00:10:16,975
frame it? Thank you for asking that.

167
00:10:17,037 --> 00:10:21,085
So I like the slide you

168
00:10:21,192 --> 00:10:25,015
showed last week's video.

169
00:10:25,182 --> 00:10:28,510
So computer cross theorem basically

170
00:10:28,692 --> 00:10:32,830
indicates the relationship between some

171
00:10:32,877 --> 00:10:35,485
crossover decision rule and vision in

172
00:10:35,517 --> 00:10:39,390
France. Here a crucial keyword

173
00:10:39,495 --> 00:10:43,800
is admissible decision

174
00:10:43,875 --> 00:10:47,962
rule, which is a rule which is

175
00:10:51,925 --> 00:10:56,145
as good as other decision

176
00:10:56,235 --> 00:11:00,250
rules or at least at

177
00:11:00,312 --> 00:11:03,580
one point better than other

178
00:11:03,702 --> 00:11:06,562
decision rules. So simply speaking,

179
00:11:06,925 --> 00:11:09,805
adomissibility indicates in some sense

180
00:11:09,852 --> 00:11:14,175
it is the best rule for some aspect.

181
00:11:14,325 --> 00:11:17,560
And usually we characterize such a

182
00:11:17,667 --> 00:11:20,665
goodness using cost function,

183
00:11:20,757 --> 00:11:23,290
loss function or risk function.

184
00:11:23,457 --> 00:11:28,237
And here what we did is we

185
00:11:29,275 --> 00:11:32,935
established some association with this

186
00:11:32,967 --> 00:11:35,170
type of loss function or risk function

187
00:11:35,322 --> 00:11:39,855
with canonical

188
00:11:39,915 --> 00:11:42,700
neural network which is we call cost

189
00:11:42,762 --> 00:11:45,760
function or biotic roles Costa function

190
00:11:45,942 --> 00:11:50,005
or neural network. So our assumption is

191
00:11:50,052 --> 00:11:52,780
that neural network minimize cost

192
00:11:52,827 --> 00:11:55,615
function. So if it active the

193
00:11:55,632 --> 00:12:00,510
inclination and it is virusly

194
00:12:00,705 --> 00:12:04,690
active some sort of optimality so we can

195
00:12:04,782 --> 00:12:08,305
say it is adommissible with respect to

196
00:12:08,427 --> 00:12:12,535
that cost function. So the

197
00:12:12,567 --> 00:12:15,895
beauty of complete cross theorem is that

198
00:12:16,047 --> 00:12:20,130
if we find some admissible decision

199
00:12:20,190 --> 00:12:22,510
rule then automatically we can say that

200
00:12:22,542 --> 00:12:26,290
it is based on inference in

201
00:12:26,307 --> 00:12:28,555
terms of some Bayesian Costa function

202
00:12:28,677 --> 00:12:31,815
with gentlemen model a priori beliefs.

203
00:12:31,995 --> 00:12:35,425
So this computer chaos theorem is

204
00:12:35,487 --> 00:12:39,500
crucial as abstract characterization

205
00:12:39,850 --> 00:12:42,810
of the relationship between conventional

206
00:12:42,855 --> 00:12:45,225
neural network architecture, dynamics

207
00:12:45,300 --> 00:12:48,350
and variational. Beijing influence.

208
00:12:51,650 --> 00:12:53,762
All right, thank you.

209
00:12:55,325 --> 00:12:57,110
What does it mean when you said it was

210
00:12:57,142 --> 00:12:59,635
biologically plausible of a loss

211
00:12:59,680 --> 00:13:03,140
function? The term is a

212
00:13:03,157 --> 00:13:06,950
little bit arbitrary because in this

213
00:13:07,012 --> 00:13:13,050
paper we mean by probability

214
00:13:17,225 --> 00:13:21,070
in the sense that this neural network

215
00:13:21,160 --> 00:13:24,820
model can be derived from realistic

216
00:13:24,985 --> 00:13:28,050
neural model through some approximation.

217
00:13:28,625 --> 00:13:32,625
And so here barricade probability,

218
00:13:34,025 --> 00:13:39,375
suggest means probability

219
00:13:39,875 --> 00:13:42,910
as a neural model or synaptic processing

220
00:13:42,955 --> 00:13:46,280
model. And if this cost

221
00:13:46,327 --> 00:13:49,520
function loss function can derive such a

222
00:13:49,672 --> 00:13:53,105
plausible algorithm, then we can say

223
00:13:53,152 --> 00:13:55,300
that this cost function is barely

224
00:13:55,375 --> 00:13:56,175
plausible.

225
00:13:59,525 --> 00:14:01,700
So what is the distinction between those

226
00:14:01,762 --> 00:14:05,045
neural and synaptic components in the

227
00:14:05,047 --> 00:14:07,235
loss function or what equation to look

228
00:14:07,267 --> 00:14:11,975
at? You mean distinction between

229
00:14:12,112 --> 00:14:15,065
dynamics and synaptic? Yeah. What is the

230
00:14:15,082 --> 00:14:16,715
distinction between them and how is it

231
00:14:16,732 --> 00:14:18,975
represented in the equations?

232
00:14:20,525 --> 00:14:24,460
Okay, basically neuropathivity

233
00:14:24,655 --> 00:14:28,915
equation means differentiate equation

234
00:14:29,095 --> 00:14:33,225
about a variable that represents firing

235
00:14:33,650 --> 00:14:37,487
intensity or some sort of

236
00:14:37,850 --> 00:14:43,050
variables associated with the firing.

237
00:14:43,625 --> 00:14:44,912
On the other hand,

238
00:14:46,325 --> 00:14:50,800
dusty equation means an update

239
00:14:50,875 --> 00:14:53,555
rule about the synaptic weight or

240
00:14:53,602 --> 00:14:57,140
synaptic strengths which is

241
00:14:57,232 --> 00:15:00,450
a connection between two neurons.

242
00:15:01,025 --> 00:15:04,955
And beauty of

243
00:15:05,152 --> 00:15:08,870
this formulation proposed in this paper

244
00:15:09,022 --> 00:15:12,235
is that we characterize both heuristic

245
00:15:12,280 --> 00:15:14,935
equations synaptic procedure equations

246
00:15:15,130 --> 00:15:21,205
in terms of gradient

247
00:15:21,265 --> 00:15:23,987
descent on a same cost function,

248
00:15:24,875 --> 00:15:29,585
common cost function. So we

249
00:15:29,617 --> 00:15:33,662
can say that if we

250
00:15:34,100 --> 00:15:38,195
consider the partial derivative of

251
00:15:38,272 --> 00:15:41,705
some cost function with respect to new

252
00:15:41,752 --> 00:15:45,835
activity, then it's derived by gradient

253
00:15:45,880 --> 00:15:51,260
descent rule about if

254
00:15:51,292 --> 00:15:53,675
we consider a partial derivative of

255
00:15:53,737 --> 00:15:55,760
chaos function errors with respect to

256
00:15:55,792 --> 00:15:58,145
synaptic weights, then we derive a

257
00:15:58,297 --> 00:15:59,625
prosthesis rule.

258
00:16:10,187 --> 00:16:13,757
Are those the only two aspects of a

259
00:16:13,760 --> 00:16:16,832
neural network or why are those the two

260
00:16:16,910 --> 00:16:18,312
key aspects?

261
00:16:20,837 --> 00:16:24,467
It is a main, I think

262
00:16:24,515 --> 00:16:28,622
it's the main body

263
00:16:28,730 --> 00:16:32,867
of the neural activity. If we consider

264
00:16:33,065 --> 00:16:36,812
some inference running or

265
00:16:36,875 --> 00:16:40,867
action exhibit by neural networks

266
00:16:40,927 --> 00:16:43,267
in the sense that neural activity

267
00:16:43,327 --> 00:16:45,842
correspond to fast dynamics, fast

268
00:16:45,890 --> 00:16:49,350
gradient dynamics mix and

269
00:16:50,987 --> 00:16:53,357
scientific processes indicate through

270
00:16:53,435 --> 00:16:56,462
dynamics that minimize least function

271
00:16:56,525 --> 00:16:59,822
and cost function. But in general, we

272
00:16:59,855 --> 00:17:02,572
can consider any aspects, any variables

273
00:17:02,617 --> 00:17:04,202
associated with your method. For

274
00:17:04,220 --> 00:17:07,577
example, at least what we show in the

275
00:17:07,595 --> 00:17:11,072
paper is any free parameter which may be

276
00:17:11,105 --> 00:17:14,612
associated with firing threshold or

277
00:17:14,750 --> 00:17:17,432
although we don't discuss in this paper

278
00:17:17,585 --> 00:17:20,625
it would be possible to add other

279
00:17:22,262 --> 00:17:24,892
variables related to neural network.

280
00:17:24,952 --> 00:17:28,327
For example, here we ignored

281
00:17:28,507 --> 00:17:32,522
contribution of Griad factor but

282
00:17:32,705 --> 00:17:35,152
it would be possible to add the Griar

283
00:17:35,182 --> 00:17:37,922
factor in this correlation or any other

284
00:17:38,030 --> 00:17:39,652
aspect of virus corporate neural

285
00:17:39,682 --> 00:17:40,362
network.

286
00:17:44,612 --> 00:17:48,047
That's very interesting and it

287
00:17:48,080 --> 00:17:51,817
speaks also to a general separation

288
00:17:51,877 --> 00:17:55,397
of time scales. For example in

289
00:17:55,430 --> 00:17:57,602
different multi scale systems or in the

290
00:17:57,620 --> 00:18:00,652
renormalization group where it's

291
00:18:00,682 --> 00:18:04,942
describing some minimal multi

292
00:18:05,002 --> 00:18:08,897
time scale system where the faster time

293
00:18:08,930 --> 00:18:12,872
scale can be seen as perception like a

294
00:18:12,905 --> 00:18:14,957
slower time scale can be seen as more

295
00:18:15,035 --> 00:18:17,747
learning like. And then in some

296
00:18:17,780 --> 00:18:20,147
hierarchical model what's learning of

297
00:18:20,180 --> 00:18:22,607
one time scale can be perceptual for a

298
00:18:22,610 --> 00:18:26,267
slower time scale? So it's a very nice

299
00:18:26,315 --> 00:18:27,237
generalization.

300
00:18:32,387 --> 00:18:36,787
Are there any examples of decision rules

301
00:18:36,937 --> 00:18:39,687
that will help us think about the action

302
00:18:40,037 --> 00:18:42,647
components of what the neural network is

303
00:18:42,680 --> 00:18:45,997
doing? Because it may be more familiar

304
00:18:46,042 --> 00:18:49,447
to think about digit characterization

305
00:18:49,642 --> 00:18:52,247
and image classification, some kind of

306
00:18:52,280 --> 00:18:55,582
classical tasks for neural networks.

307
00:18:55,747 --> 00:18:59,132
But how does the decision rule play out

308
00:18:59,210 --> 00:19:01,362
in the context of neural networks?

309
00:19:04,187 --> 00:19:07,502
Okay, so in this paper

310
00:19:07,595 --> 00:19:11,807
we basically assume a closed loop so

311
00:19:11,885 --> 00:19:14,657
comprising a neural network part and

312
00:19:14,735 --> 00:19:18,277
environmental part. So Neuron receives

313
00:19:18,382 --> 00:19:21,150
sensor input from environment and

314
00:19:23,687 --> 00:19:26,312
provide some feedback to the

315
00:19:26,450 --> 00:19:27,387
environment.

316
00:19:31,787 --> 00:19:37,177
Even with the example of classification,

317
00:19:37,357 --> 00:19:40,875
we can say that output correspond to

318
00:19:42,137 --> 00:19:44,992
classification output,

319
00:19:45,127 --> 00:19:48,197
which is kind of generative model

320
00:19:48,230 --> 00:19:51,302
relevant. Example would be, for example,

321
00:19:51,395 --> 00:19:55,125
controlling agent like a robot control

322
00:19:55,637 --> 00:19:58,772
or any kind of control errors.

323
00:19:58,805 --> 00:20:01,622
Decision making tasks. For example,

324
00:20:01,730 --> 00:20:05,447
when we encounter some

325
00:20:05,630 --> 00:20:09,322
choice tasks, we need to advertise,

326
00:20:09,517 --> 00:20:12,375
for example, left or right or something.

327
00:20:12,887 --> 00:20:17,297
Any kind of such a decision can

328
00:20:17,330 --> 00:20:20,897
be associated with the admissibility or

329
00:20:20,930 --> 00:20:22,362
admissible decision.

330
00:20:27,050 --> 00:20:30,260
So what would an example of an

331
00:20:30,292 --> 00:20:34,285
inadmissible or admissible strategy

332
00:20:34,330 --> 00:20:38,175
be in the decision making task?

333
00:20:40,850 --> 00:20:45,175
Admissibility usually characterized

334
00:20:45,250 --> 00:20:48,962
by loss function or risk function.

335
00:20:52,925 --> 00:20:57,185
Here admissivity indicates that

336
00:20:57,367 --> 00:21:00,680
there is another decision rule which is

337
00:21:00,877 --> 00:21:04,250
at least one point better

338
00:21:04,312 --> 00:21:09,775
than the forecast decision

339
00:21:09,850 --> 00:21:14,162
rule. Simply speaking

340
00:21:14,600 --> 00:21:18,685
in Adobe CBD indicates that decision

341
00:21:18,805 --> 00:21:22,637
rule is not good

342
00:21:24,800 --> 00:21:28,285
relatively. Let's just say our decision

343
00:21:28,330 --> 00:21:32,315
rule is we always turn right. Is that an

344
00:21:32,332 --> 00:21:34,010
example of a decision rule? Because

345
00:21:34,042 --> 00:21:35,510
there might be settings where that is

346
00:21:35,542 --> 00:21:38,925
strictly effective and the simplest rule

347
00:21:39,275 --> 00:21:40,715
whereas there's other settings where

348
00:21:40,732 --> 00:21:43,535
that's going to be tragic. So what does

349
00:21:43,567 --> 00:21:47,180
it mean to be admissible for an agent in

350
00:21:47,227 --> 00:21:49,645
light of different environmental

351
00:21:49,810 --> 00:21:53,390
contexts? That's an interesting

352
00:21:53,557 --> 00:21:58,205
point. So even with such

353
00:21:58,252 --> 00:22:02,470
a too much simplified

354
00:22:02,560 --> 00:22:06,625
rule it can be admissible

355
00:22:06,700 --> 00:22:09,425
under some particular situation,

356
00:22:09,562 --> 00:22:12,887
particular loss function. For example,

357
00:22:13,250 --> 00:22:17,375
the rulers that always turn right maybe

358
00:22:17,437 --> 00:22:20,285
the best under some situation,

359
00:22:20,392 --> 00:22:24,905
right? So the

360
00:22:24,952 --> 00:22:27,770
relationship of admissibility or enough

361
00:22:27,847 --> 00:22:32,060
adommissibility depends on both agent

362
00:22:32,167 --> 00:22:34,345
characteristics and environmental

363
00:22:34,435 --> 00:22:35,625
characteristics.

364
00:22:39,350 --> 00:22:42,300
What aspects of the environment.

365
00:22:44,975 --> 00:22:47,237
For example? For example,

366
00:22:47,750 --> 00:22:51,075
if that decision group matches

367
00:22:51,575 --> 00:22:54,125
the structure architecture of

368
00:22:54,262 --> 00:22:58,505
environment then maybe

369
00:22:58,702 --> 00:23:02,815
that decision always downright active

370
00:23:02,845 --> 00:23:07,070
the shortest past under some

371
00:23:07,222 --> 00:23:09,225
situation, some environment.

372
00:23:11,675 --> 00:23:14,960
How does this admissibility help us

373
00:23:14,992 --> 00:23:19,025
think about like overfitting and how

374
00:23:19,087 --> 00:23:21,320
does it help us think about the way that

375
00:23:21,472 --> 00:23:25,015
different practices are used for neural

376
00:23:25,045 --> 00:23:27,080
networks to prevent them from being over

377
00:23:27,127 --> 00:23:28,500
fit in practice?

378
00:23:30,725 --> 00:23:31,487
Well,

379
00:23:38,075 --> 00:23:41,500
strictly admissivity is characterized

380
00:23:41,575 --> 00:23:44,100
with the Bayesian risk.

381
00:23:50,225 --> 00:23:54,155
We cannot observe a hidden states of

382
00:23:54,352 --> 00:23:58,505
the environment, only we can observe is

383
00:23:58,627 --> 00:24:02,860
a part of the entire universe.

384
00:24:02,905 --> 00:24:06,935
So the question is an important

385
00:24:07,117 --> 00:24:10,285
question is what is the best choice

386
00:24:10,405 --> 00:24:13,955
under such a limited information?

387
00:24:14,077 --> 00:24:19,250
Limited information?

388
00:24:19,387 --> 00:24:24,595
So this Bayesian list adoissibility

389
00:24:24,685 --> 00:24:32,675
or computer credit theorem tell

390
00:24:32,737 --> 00:24:36,980
us that well

391
00:24:37,027 --> 00:24:40,800
known, only the well known

392
00:24:41,525 --> 00:24:45,340
Bayesian framework achieved

393
00:24:45,445 --> 00:24:49,180
the adommissible

394
00:24:49,315 --> 00:24:53,285
decision. Which means that in

395
00:24:53,317 --> 00:24:58,160
this aspects Bayesian optimization give

396
00:24:58,192 --> 00:25:03,335
us a least choice strategy, otherwise we

397
00:25:03,442 --> 00:25:08,975
overfit or find

398
00:25:09,112 --> 00:25:12,900
the suboptima evolution.

399
00:25:13,700 --> 00:25:17,200
So it's a nice association, nice linkage

400
00:25:17,275 --> 00:25:19,655
between the decision, but is a good

401
00:25:19,702 --> 00:25:23,135
decision about the decision and more

402
00:25:23,167 --> 00:25:26,470
established statistical inference.

403
00:25:26,560 --> 00:25:27,662
Freedom work.

404
00:25:31,137 --> 00:25:34,082
Thank you, that's very helpful.

405
00:25:34,172 --> 00:25:37,375
So we're reducing our

406
00:25:38,487 --> 00:25:42,347
uncertainty and risk about hidden

407
00:25:42,392 --> 00:25:45,492
states in the environment. So in the

408
00:25:45,540 --> 00:25:47,577
special case where the entire

409
00:25:47,745 --> 00:25:50,522
environment is observable without errors

410
00:25:50,642 --> 00:25:55,067
like a chess game, then there's

411
00:25:55,202 --> 00:25:58,452
an equivalence between correlation of

412
00:25:58,620 --> 00:26:02,472
risk or loss on observables or on

413
00:26:02,505 --> 00:26:04,017
hidden states. But they're not really

414
00:26:04,065 --> 00:26:06,272
hidden, but they are environmental

415
00:26:06,317 --> 00:26:10,362
states. Whereas any amount

416
00:26:10,425 --> 00:26:14,207
of uncertainty in the mapping

417
00:26:14,297 --> 00:26:18,042
between observations and hidden states,

418
00:26:18,240 --> 00:26:21,342
which is usually shown as a in the

419
00:26:21,390 --> 00:26:23,117
partially observable Markov decision

420
00:26:23,177 --> 00:26:26,787
process, any amount of uncertainty about

421
00:26:26,850 --> 00:26:28,712
unobserved or partially observed

422
00:26:28,787 --> 00:26:32,852
environmental states enables

423
00:26:32,882 --> 00:26:36,632
you to fit your uncertainty optimally

424
00:26:36,722 --> 00:26:40,317
about that hidden state and

425
00:26:40,365 --> 00:26:44,397
fit that uncertainty simply with

426
00:26:44,430 --> 00:26:47,067
the gradient descent. And by doing so,

427
00:26:47,115 --> 00:26:49,797
you don't overfit a model of

428
00:26:49,830 --> 00:26:50,962
observables,

429
00:26:53,562 --> 00:26:57,177
which might be the fallacy or the issue

430
00:26:57,270 --> 00:26:59,962
with simply doing descriptive statistics

431
00:27:00,687 --> 00:27:03,012
you might get an infinitely small

432
00:27:03,150 --> 00:27:07,202
variance with a frequentist estimate

433
00:27:07,382 --> 00:27:10,722
because you have 1000 data points. So

434
00:27:10,755 --> 00:27:13,922
the variance from a descriptive

435
00:27:13,967 --> 00:27:15,717
statistics perspective might be very

436
00:27:15,765 --> 00:27:16,375
small.

437
00:27:21,012 --> 00:27:24,387
I think it speaks very much to why

438
00:27:24,450 --> 00:27:26,612
neural networks are useful in practice

439
00:27:26,687 --> 00:27:29,137
from training with limited data sets

440
00:27:29,862 --> 00:27:32,642
because that's an empirical observation

441
00:27:32,702 --> 00:27:35,397
that they don't entirely over fit. But

442
00:27:35,430 --> 00:27:37,292
also I'm sure there's ways to construct

443
00:27:37,352 --> 00:27:40,237
them that are overfit.

444
00:27:41,112 --> 00:27:41,875
Yeah,

445
00:27:43,962 --> 00:27:47,697
overfit will occur if we select

446
00:27:47,805 --> 00:27:51,550
some optimal priorities. For example.

447
00:27:53,712 --> 00:27:57,850
Well, I'm not sure if it

448
00:27:58,812 --> 00:28:02,547
is overfit in the

449
00:28:02,580 --> 00:28:06,597
sense what you mentioned because if

450
00:28:06,630 --> 00:28:09,777
we select some priorities then the

451
00:28:09,795 --> 00:28:12,897
Bayesian function itself changes and the

452
00:28:13,080 --> 00:28:15,942
neural networks that try to fit to that

453
00:28:15,990 --> 00:28:18,357
Costa function. So cost function

454
00:28:18,510 --> 00:28:22,587
minimization will be achieved agent

455
00:28:22,650 --> 00:28:26,442
such a situation. But that solution is

456
00:28:26,490 --> 00:28:30,100
not good for our

457
00:28:30,537 --> 00:28:33,752
original help us. That's the tricky

458
00:28:33,782 --> 00:28:37,742
part. Yeah, that is reminiscent

459
00:28:37,802 --> 00:28:40,547
of some discussions we've had discussing

460
00:28:40,592 --> 00:28:44,862
like driving off a cliff or blowing up

461
00:28:45,000 --> 00:28:47,217
is also reducing free energy. Like

462
00:28:47,265 --> 00:28:49,092
dropping up a building reduces your

463
00:28:49,140 --> 00:28:51,882
potential energy. And so there are

464
00:28:51,960 --> 00:28:54,937
potentially decisionmaking or strategic

465
00:28:55,287 --> 00:28:58,992
trajectories that do for some time

466
00:28:59,040 --> 00:29:02,575
horizon minimize free energy,

467
00:29:03,612 --> 00:29:06,562
perhaps even or maybe even guaranteed

468
00:29:06,912 --> 00:29:09,437
better than some longer time horizon.

469
00:29:09,587 --> 00:29:11,922
Because if the shortterm strategies were

470
00:29:11,955 --> 00:29:13,367
somehow better than the longterm

471
00:29:13,427 --> 00:29:16,227
horizon. It would be difficult to

472
00:29:16,245 --> 00:29:18,257
imagine because the long term horizon

473
00:29:18,272 --> 00:29:20,177
would be at least as good as a shortterm

474
00:29:20,207 --> 00:29:23,802
strategy. So that speaks to the

475
00:29:23,820 --> 00:29:27,902
challenges of planning in action.

476
00:29:28,007 --> 00:29:32,267
So how is planning addressed in modern

477
00:29:32,327 --> 00:29:35,292
neural networks and how does this work

478
00:29:35,340 --> 00:29:37,000
help us think about that?

479
00:29:39,837 --> 00:29:43,162
That's another very important aspect.

480
00:29:45,612 --> 00:29:48,992
I have to say that this framework

481
00:29:49,052 --> 00:29:51,497
addresses planning aspect,

482
00:29:51,617 --> 00:29:55,462
but that planning is not necessarily

483
00:29:57,612 --> 00:30:01,577
the optimal

484
00:30:01,607 --> 00:30:04,375
solution in the sense that what we

485
00:30:04,737 --> 00:30:08,412
interested in is optimization or

486
00:30:08,550 --> 00:30:12,187
learning under limited structure.

487
00:30:14,112 --> 00:30:17,575
The structure is characterized by here

488
00:30:18,087 --> 00:30:21,422
Prosperia neural networks.

489
00:30:21,617 --> 00:30:25,652
So yeah, planning occurred

490
00:30:25,757 --> 00:30:30,347
by association between risk

491
00:30:30,392 --> 00:30:33,707
in the future and our decision

492
00:30:33,797 --> 00:30:37,172
in the past. Here we model that aspects

493
00:30:37,292 --> 00:30:40,982
using delayed moderation

494
00:30:41,147 --> 00:30:44,672
of scientific activity mediated

495
00:30:44,717 --> 00:30:47,367
by some neuromodurator or

496
00:30:47,565 --> 00:30:51,297
neurotransmitters. This is

497
00:30:51,330 --> 00:30:52,075
the model.

498
00:30:58,812 --> 00:31:05,697
This is model as the

499
00:31:05,730 --> 00:31:09,925
risk factor and the heavy product

500
00:31:10,587 --> 00:31:16,052
holding the neural

501
00:31:16,082 --> 00:31:16,837
network.

502
00:31:21,987 --> 00:31:24,237
All right, I'm going to ask a great

503
00:31:24,300 --> 00:31:26,927
question from the chat and then we'll

504
00:31:27,032 --> 00:31:30,177
look at the figures a little closer. So

505
00:31:30,345 --> 00:31:33,927
ML Don wrote a question stuck in

506
00:31:33,945 --> 00:31:36,222
my mind for a long time. Could you

507
00:31:36,255 --> 00:31:38,922
please put it to rest? Do we need to

508
00:31:38,955 --> 00:31:42,372
have knowledge about all states possible

509
00:31:42,480 --> 00:31:45,677
actions and sensory inputs for active

510
00:31:45,707 --> 00:31:46,537
inference?

511
00:31:50,812 --> 00:31:51,575
Well,

512
00:31:54,337 --> 00:31:58,207
you mean if you seek the exact

513
00:31:58,285 --> 00:32:02,150
solution, exact optimal solution, then

514
00:32:02,887 --> 00:32:06,607
maybe more information would help you

515
00:32:06,685 --> 00:32:10,850
to find that. But under some

516
00:32:16,537 --> 00:32:20,900
ideal assumptions then the

517
00:32:21,937 --> 00:32:27,402
is not necessary to achieve the optimal

518
00:32:27,432 --> 00:32:31,297
solution. I'm not sure if

519
00:32:31,405 --> 00:32:33,800
I correctly answer your point.

520
00:32:35,512 --> 00:32:38,300
So just to restate it.

521
00:32:39,037 --> 00:32:40,872
Of course, knowing all the state's

522
00:32:40,917 --> 00:32:43,242
possible actions and sensory inputs,

523
00:32:43,377 --> 00:32:45,637
it's not a bad thing. Worst case,

524
00:32:45,700 --> 00:32:47,952
there's some computational complexity,

525
00:32:48,057 --> 00:32:50,472
trade offs, but the problem becomes

526
00:32:50,517 --> 00:32:53,977
fully stateable. But I

527
00:32:53,995 --> 00:32:57,267
think ML Dawn is asking about cases

528
00:32:57,327 --> 00:33:02,017
where you don't know all of the state

529
00:33:02,065 --> 00:33:05,382
spaces or potentially even the dimension

530
00:33:05,547 --> 00:33:10,617
or the semantics of hidden

531
00:33:10,677 --> 00:33:13,902
states, active states, sensory inputs

532
00:33:13,932 --> 00:33:16,625
and why not even add cognitive states?

533
00:33:18,412 --> 00:33:21,472
So in not just partially observed but

534
00:33:21,505 --> 00:33:23,987
partially known state spaces,

535
00:33:25,087 --> 00:33:28,782
how are these address in neural networks

536
00:33:28,947 --> 00:33:31,042
and how does active inference help us

537
00:33:31,090 --> 00:33:32,225
think about it?

538
00:33:37,762 --> 00:33:41,437
Okay, I think the question

539
00:33:41,500 --> 00:33:45,417
is about how can we separate

540
00:33:45,552 --> 00:33:48,067
those states?

541
00:33:48,190 --> 00:33:51,792
Like sensory function interface

542
00:33:51,852 --> 00:33:52,862
entorhinal,

543
00:33:57,100 --> 00:34:00,115
how can. We separate not just in

544
00:34:00,132 --> 00:34:02,035
principle have these states be

545
00:34:02,067 --> 00:34:05,185
separated, but deal with the fact that

546
00:34:05,217 --> 00:34:08,575
some of these states we might have good

547
00:34:08,637 --> 00:34:11,515
knowledge on and some states like the

548
00:34:11,532 --> 00:34:13,570
hidden states we might not even know,

549
00:34:13,722 --> 00:34:16,060
like we don't know the dimension of the

550
00:34:16,092 --> 00:34:20,062
cause vector in the world. I see.

551
00:34:22,525 --> 00:34:24,650
In terms of dimension,

552
00:34:26,050 --> 00:34:29,155
there is a statistical technique to

553
00:34:29,277 --> 00:34:32,587
estimate the dimensionality, for example

554
00:34:33,025 --> 00:34:36,985
via information criteria like I

555
00:34:37,017 --> 00:34:38,830
agent information criteria, based

556
00:34:38,952 --> 00:34:42,475
information criteria, all them try to

557
00:34:42,537 --> 00:34:46,770
info estimate plausible dimension

558
00:34:46,860 --> 00:34:50,525
about the environmental hidden

559
00:34:51,175 --> 00:34:55,555
states. There is an analogy with

560
00:34:55,602 --> 00:34:58,230
those information criteria and version

561
00:34:58,290 --> 00:35:01,345
of free energy minimization. So with

562
00:35:01,422 --> 00:35:03,787
version of free energy inclination we

563
00:35:04,225 --> 00:35:07,870
can identify the plausible model

564
00:35:07,947 --> 00:35:12,075
structure which in principle involves

565
00:35:12,150 --> 00:35:15,200
the dimension aspect.

566
00:35:16,375 --> 00:35:20,110
But in terms of Neural network in

567
00:35:20,142 --> 00:35:23,812
this paper we don't carefully consider

568
00:35:24,475 --> 00:35:27,840
about the dimensionality optimization

569
00:35:27,945 --> 00:35:31,855
because we first define the

570
00:35:31,977 --> 00:35:35,530
number of neurons and don't change

571
00:35:35,652 --> 00:35:40,065
during the training. But in principle

572
00:35:40,170 --> 00:35:43,525
we can consider the

573
00:35:43,662 --> 00:35:46,912
change in the number of neurons which is

574
00:35:47,350 --> 00:35:51,255
associated with the neurogenesis

575
00:35:51,315 --> 00:35:53,995
adult neurogenesis or development during

576
00:35:54,072 --> 00:35:56,975
the developmental stage.

577
00:35:57,475 --> 00:36:01,625
That would be an important expansion

578
00:36:02,350 --> 00:36:06,125
of this direction.

579
00:36:13,450 --> 00:36:15,187
That's very interesting.

580
00:36:15,925 --> 00:36:18,500
Here's a remark.

581
00:36:19,000 --> 00:36:22,875
Well, one note is equation

582
00:36:22,950 --> 00:36:26,565
one summarizes a lot of what you've

583
00:36:26,595 --> 00:36:30,120
been describing. There's a parallelism

584
00:36:30,210 --> 00:36:32,440
or a concordance being drawn between the

585
00:36:32,457 --> 00:36:35,725
loss function of Neural networks and the

586
00:36:35,862 --> 00:36:38,215
variational free energy of the

587
00:36:38,232 --> 00:36:41,470
parameterized model there.

588
00:36:41,622 --> 00:36:45,800
So to come back to these processes

589
00:36:46,375 --> 00:36:49,270
that influence learning which we could

590
00:36:49,272 --> 00:36:51,060
think of as the Neural network becoming

591
00:36:51,105 --> 00:36:53,170
more fit from a loss function

592
00:36:53,247 --> 00:36:56,750
perspective or the variational Bayesian

593
00:36:57,400 --> 00:36:59,730
partially Observable Markov decision

594
00:36:59,790 --> 00:37:03,480
process entity generative model encoding

595
00:37:03,540 --> 00:37:06,645
better at doing what it does. So there's

596
00:37:06,735 --> 00:37:09,180
the firing rate on the Neural network

597
00:37:09,240 --> 00:37:12,295
side, the synaptic plasticity at a

598
00:37:12,297 --> 00:37:14,545
slower time scale which we discussed a

599
00:37:14,547 --> 00:37:16,285
little earlier. And then now there's a

600
00:37:16,317 --> 00:37:20,020
third time scale with the

601
00:37:20,097 --> 00:37:23,680
birth and death of new cells and maybe

602
00:37:23,727 --> 00:37:25,250
even new layers.

603
00:37:26,350 --> 00:37:29,975
And that kind of multiscale

604
00:37:30,550 --> 00:37:34,885
temporal structuring is

605
00:37:34,917 --> 00:37:39,125
not intrinsic to the Bayes graph

606
00:37:40,300 --> 00:37:42,870
to represent multiple nested timescales

607
00:37:42,885 --> 00:37:45,640
in a Bayesian graph in the act of

608
00:37:45,657 --> 00:37:47,815
inference literature it's more common to

609
00:37:47,832 --> 00:37:50,035
make a hierarchically nested model,

610
00:37:50,217 --> 00:37:53,512
right? And just say that the time

611
00:37:55,300 --> 00:37:57,910
handling on one level is happening more

612
00:37:57,942 --> 00:38:01,705
rapidly with respect to clock time than

613
00:38:01,827 --> 00:38:05,130
deeper nested, slower models. Whereas

614
00:38:05,190 --> 00:38:10,110
the Neural formulation allows

615
00:38:10,155 --> 00:38:13,935
us to deal with multiple ongoing

616
00:38:13,980 --> 00:38:17,725
active states without appealing to

617
00:38:17,787 --> 00:38:19,700
hierarchical nesting,

618
00:38:21,175 --> 00:38:23,750
which is a very important feature.

619
00:38:31,450 --> 00:38:35,205
Well, both distinctions

620
00:38:35,265 --> 00:38:38,740
will be possible. So without

621
00:38:38,907 --> 00:38:42,262
hierarchical or with

622
00:38:42,925 --> 00:38:46,120
higher car modeling so even with

623
00:38:46,197 --> 00:38:49,410
hierarchical modeling, the optimization

624
00:38:49,530 --> 00:38:53,410
of dimensionality should be

625
00:38:53,442 --> 00:38:56,995
possible. It would be possible. But in

626
00:38:57,072 --> 00:39:00,865
other distinctions we can consider

627
00:39:00,957 --> 00:39:04,800
that a population of Neural models

628
00:39:04,875 --> 00:39:07,455
so one has a single layer,

629
00:39:07,590 --> 00:39:10,325
another has two layers,

630
00:39:11,275 --> 00:39:14,815
three layers, four layers. And consider

631
00:39:14,907 --> 00:39:18,750
the probability of network

632
00:39:18,825 --> 00:39:22,740
architectures associated

633
00:39:22,770 --> 00:39:28,395
with Costa

634
00:39:28,485 --> 00:39:30,562
minimization and a particular

635
00:39:31,225 --> 00:39:35,455
environment which is in principle have

636
00:39:35,502 --> 00:39:38,025
the same computational architectures

637
00:39:38,100 --> 00:39:42,787
with the hierarchy model.

638
00:39:46,800 --> 00:39:50,790
Very interesting. Yes, perhaps I

639
00:39:50,957 --> 00:39:54,585
over generalized or speculated because I

640
00:39:54,617 --> 00:39:58,080
thought about how one could have a

641
00:39:58,127 --> 00:40:02,235
100 timestep POMDP that

642
00:40:02,267 --> 00:40:05,090
also performs multiscale behavior

643
00:40:05,270 --> 00:40:08,790
potentially extremely wastefully, but at

644
00:40:08,807 --> 00:40:11,910
least it could in principle. And

645
00:40:12,092 --> 00:40:14,940
similarly, within a neuron there could

646
00:40:14,957 --> 00:40:19,410
be another Neural network or

647
00:40:19,442 --> 00:40:21,435
some other structure approximated by

648
00:40:21,467 --> 00:40:25,525
that. So they almost both enable

649
00:40:26,925 --> 00:40:30,120
hierarchical and non hierarchical model

650
00:40:30,122 --> 00:40:33,225
modeling as you described, but in very

651
00:40:33,287 --> 00:40:37,095
different ways that

652
00:40:37,247 --> 00:40:40,900
lead to very different implementations.

653
00:40:44,025 --> 00:40:47,325
Yes. I think this brings us

654
00:40:47,387 --> 00:40:51,350
to the topic of forward and reverse

655
00:40:51,425 --> 00:40:54,987
engineering. So you talked a lot about

656
00:40:55,350 --> 00:40:57,025
reverse engineering.

657
00:40:58,650 --> 00:41:01,920
What is reverse engineering and what is

658
00:41:01,997 --> 00:41:05,865
forward engineering and what has

659
00:41:05,882 --> 00:41:08,500
been done in these areas of engineering?

660
00:41:10,350 --> 00:41:13,935
Okay, I'm not

661
00:41:13,967 --> 00:41:17,370
an expert in this

662
00:41:17,447 --> 00:41:21,545
process, but I believe that liver

663
00:41:21,635 --> 00:41:25,560
here means your

664
00:41:25,592 --> 00:41:29,495
characterization of the blueprint

665
00:41:29,660 --> 00:41:33,500
of some device or machine

666
00:41:33,650 --> 00:41:37,530
from data observable information like

667
00:41:37,727 --> 00:41:41,810
activity or action behavior

668
00:41:41,930 --> 00:41:45,270
of some agent. Goal is

669
00:41:45,347 --> 00:41:49,485
identification of blueprint and

670
00:41:49,517 --> 00:41:52,310
the crucially here blueprint correspond

671
00:41:52,355 --> 00:41:54,735
to generative models because once we

672
00:41:54,767 --> 00:41:56,780
define generative model, we can Deneve

673
00:41:56,840 --> 00:41:59,585
evolution, anthropology algorithm,

674
00:41:59,630 --> 00:42:02,190
running inference algorithm and any

675
00:42:02,282 --> 00:42:05,430
behavior of the agent. So here

676
00:42:05,477 --> 00:42:09,000
reverse means that we

677
00:42:09,062 --> 00:42:12,930
first observe some activity of agent

678
00:42:13,127 --> 00:42:16,530
and its mechanism is still unknown for

679
00:42:16,577 --> 00:42:20,975
us, but we can estimate

680
00:42:21,050 --> 00:42:25,275
its mechanism using that

681
00:42:25,337 --> 00:42:29,250
activity by identifying the most

682
00:42:29,312 --> 00:42:32,840
plausible guarantee

683
00:42:32,870 --> 00:42:36,510
model which can minimize some

684
00:42:36,617 --> 00:42:39,860
Costa function or risk

685
00:42:39,905 --> 00:42:43,730
function when we feed

686
00:42:43,790 --> 00:42:47,835
the data to the

687
00:42:47,867 --> 00:42:50,655
model. So, on the other hand, for the

688
00:42:50,702 --> 00:42:54,890
engineering would be more mainstream,

689
00:42:55,070 --> 00:42:58,715
way fast defined model blueprint gently

690
00:42:58,745 --> 00:43:02,487
model then drive everything

691
00:43:03,000 --> 00:43:07,362
including parasite functional running

692
00:43:07,725 --> 00:43:11,725
algorithms and behavior action

693
00:43:13,575 --> 00:43:15,250
prediction algorithm.

694
00:43:19,875 --> 00:43:23,420
So, by reverse engineering neural

695
00:43:23,435 --> 00:43:26,940
networks, we're observing some

696
00:43:27,032 --> 00:43:30,810
already parameterized neural network and

697
00:43:30,842 --> 00:43:34,560
then fitting a POMDP to

698
00:43:34,592 --> 00:43:37,730
it. To what extent

699
00:43:37,865 --> 00:43:42,987
is it possible to take a given POMDP and

700
00:43:43,350 --> 00:43:47,090
create a neural network that performs

701
00:43:47,120 --> 00:43:48,250
that inference?

702
00:43:52,875 --> 00:43:57,525
Okay, in this paper or

703
00:43:57,662 --> 00:44:01,545
in the following paper, what we

704
00:44:01,697 --> 00:44:05,085
consider is a strategy that we

705
00:44:05,117 --> 00:44:08,925
first feed empirical data whether

706
00:44:08,987 --> 00:44:13,460
force neural response data to BioScale

707
00:44:13,505 --> 00:44:16,812
prosper neural network model

708
00:44:17,850 --> 00:44:20,775
which is similar to a conventional model

709
00:44:20,837 --> 00:44:23,835
fitting approach where we have

710
00:44:23,942 --> 00:44:27,462
differential equation data and

711
00:44:28,800 --> 00:44:33,162
differential equation to explain the

712
00:44:34,500 --> 00:44:38,045
behavior with the minimum

713
00:44:38,135 --> 00:44:40,812
prediction. So now,

714
00:44:41,325 --> 00:44:45,480
a virtue of this framework we

715
00:44:45,527 --> 00:44:49,365
established is that we

716
00:44:49,382 --> 00:44:52,145
can naturally transform such neural

717
00:44:52,160 --> 00:44:55,512
network architecture with the

718
00:44:56,700 --> 00:44:58,970
very known partially observable markup

719
00:44:58,985 --> 00:45:01,310
action process architecture.

720
00:45:01,430 --> 00:45:05,465
Because for any kind of canonical neural

721
00:45:05,495 --> 00:45:07,962
network there is a cost function.

722
00:45:09,900 --> 00:45:13,395
So we Deneve cost function through

723
00:45:13,472 --> 00:45:17,425
neuroactive decision which is opposite

724
00:45:19,575 --> 00:45:23,430
with the conventional way we define cost

725
00:45:23,477 --> 00:45:27,525
function derived algorithm and then we

726
00:45:27,662 --> 00:45:32,210
use the formal equivalence

727
00:45:32,255 --> 00:45:34,395
between neural network Costa function

728
00:45:34,472 --> 00:45:38,925
and variant queen energy. So now

729
00:45:38,987 --> 00:45:42,462
transform the journal architecture to

730
00:45:43,275 --> 00:45:46,530
Beijing model architectures and once

731
00:45:46,577 --> 00:45:49,515
we characterize vital energy,

732
00:45:49,682 --> 00:45:53,760
there should be some

733
00:45:53,942 --> 00:45:57,850
general that define

734
00:45:59,250 --> 00:46:01,790
that informational energy functional.

735
00:46:01,970 --> 00:46:05,535
So in

736
00:46:05,567 --> 00:46:09,087
particular, in this example,

737
00:46:09,525 --> 00:46:13,305
canon network nicely correspond to well

738
00:46:13,352 --> 00:46:17,540
known across macquarlin

739
00:46:17,570 --> 00:46:21,612
process. So, by using this

740
00:46:23,775 --> 00:46:29,160
procedure, we identify a

741
00:46:29,192 --> 00:46:33,200
plausible home DP architecture

742
00:46:33,275 --> 00:46:36,980
which correspond

743
00:46:37,040 --> 00:46:40,062
to observed activity data.

744
00:46:49,337 --> 00:46:52,622
Well, let's stay on

745
00:46:52,655 --> 00:46:55,200
this last point.

746
00:46:55,862 --> 00:46:58,927
So, after all those transformations,

747
00:46:59,107 --> 00:47:02,412
first the measurements of neurons

748
00:47:03,062 --> 00:47:04,807
using that data to fit the neural

749
00:47:04,822 --> 00:47:08,867
network and then by virtue of

750
00:47:08,915 --> 00:47:11,492
the relationships unpacked in the paper,

751
00:47:11,690 --> 00:47:15,377
transforming the neural network in

752
00:47:15,395 --> 00:47:19,712
the left side of figure one into a

753
00:47:19,775 --> 00:47:22,387
particular form of the P-O-M DP.

754
00:47:22,537 --> 00:47:26,042
So first, what are the constraints on

755
00:47:26,090 --> 00:47:29,207
that form of the P-O-M DP? Is this a

756
00:47:29,210 --> 00:47:32,357
little corner of model space or what are

757
00:47:32,435 --> 00:47:35,862
the space of acceptable P-O-M DPS?

758
00:47:38,012 --> 00:47:40,922
That totally depends on what kind of

759
00:47:40,955 --> 00:47:42,887
neural network model you are

760
00:47:42,950 --> 00:47:46,350
considering. So for example,

761
00:47:47,387 --> 00:47:50,447
in this paper we discussed about a

762
00:47:50,480 --> 00:47:54,377
particular crossover from DP in

763
00:47:54,395 --> 00:47:58,172
which each state takes either

764
00:47:58,280 --> 00:48:02,572
zero or one. So it's very restricted

765
00:48:02,767 --> 00:48:06,862
compared to the general form of homedp.

766
00:48:06,937 --> 00:48:10,747
But we consider a factorization

767
00:48:10,867 --> 00:48:14,550
so in the sense that although each

768
00:48:16,412 --> 00:48:19,837
but we consider a vector of observation,

769
00:48:19,912 --> 00:48:23,732
a vector of hidden states where each

770
00:48:23,885 --> 00:48:28,500
element correspond to one

771
00:48:29,537 --> 00:48:33,182
single one hot vector but as an entire

772
00:48:33,260 --> 00:48:37,812
state it can represent

773
00:48:38,237 --> 00:48:42,000
high dimension discrete state space.

774
00:48:43,112 --> 00:48:46,912
And this architectures nicely correspond

775
00:48:46,987 --> 00:48:51,300
to neural network architectures because

776
00:48:51,662 --> 00:48:56,132
usually each neuron takes either zero

777
00:48:56,210 --> 00:48:59,917
one or some value continuous

778
00:48:59,977 --> 00:49:03,750
variable between zero and one.

779
00:49:04,262 --> 00:49:08,122
So we use this association

780
00:49:08,242 --> 00:49:11,717
to characterize a particular OMDP which

781
00:49:11,765 --> 00:49:14,377
correspond to neural networks,

782
00:49:14,557 --> 00:49:19,772
and this follows a

783
00:49:19,805 --> 00:49:23,302
particular mini field approximation,

784
00:49:23,407 --> 00:49:26,057
approximation or approximation in

785
00:49:26,210 --> 00:49:31,062
generative model because we associate

786
00:49:31,637 --> 00:49:35,677
posterior belief in this particular homo

787
00:49:35,707 --> 00:49:39,312
DP with the neural activity,

788
00:49:40,112 --> 00:49:43,667
which means that posterior of

789
00:49:43,715 --> 00:49:48,087
action also has a factorization

790
00:49:48,662 --> 00:49:54,347
architecture in the sense that we

791
00:49:54,380 --> 00:49:58,037
don't fully consider about the second

792
00:49:58,100 --> 00:50:01,102
order statistics between neurons

793
00:50:01,132 --> 00:50:03,917
activity and activity, which is outside

794
00:50:04,040 --> 00:50:06,612
of this poem. RASM.

795
00:50:07,337 --> 00:50:11,717
So each

796
00:50:11,765 --> 00:50:17,512
neuron activity correspond to posterior

797
00:50:17,587 --> 00:50:22,887
expectation about a particular element

798
00:50:24,662 --> 00:50:28,950
of the state and we don't consider the

799
00:50:32,762 --> 00:50:36,600
joint posterior property of all state.

800
00:50:40,700 --> 00:50:43,800
So although this is a implication,

801
00:50:44,900 --> 00:50:49,885
we see this Asia

802
00:50:49,930 --> 00:50:53,590
impress, but otherwise,

803
00:50:53,770 --> 00:50:56,312
for example, we can consider any

804
00:50:57,800 --> 00:51:00,755
recurrent network architectures which

805
00:51:00,802 --> 00:51:02,380
correspond to state to transition

806
00:51:02,440 --> 00:51:06,410
metrics and it would be

807
00:51:06,442 --> 00:51:09,500
possible to extend this architecture to

808
00:51:09,562 --> 00:51:13,190
higher call structure in the sense that

809
00:51:13,357 --> 00:51:16,045
it is straightforward.

810
00:51:16,135 --> 00:51:19,460
Consider a tree structure or any kind of

811
00:51:19,567 --> 00:51:22,520
higher car structure by assumptions that

812
00:51:22,672 --> 00:51:26,105
some neurons connect to other neuron but

813
00:51:26,152 --> 00:51:29,830
not connect to other neurons.

814
00:51:29,890 --> 00:51:33,065
So this is Lamme as considering the

815
00:51:33,082 --> 00:51:36,812
higher car structure in general.

816
00:51:43,675 --> 00:51:46,837
That's very interesting.

817
00:51:48,175 --> 00:51:52,705
It's commonly remarked in the

818
00:51:52,752 --> 00:51:56,745
base graphs that they represent

819
00:51:56,835 --> 00:51:59,745
the connections amongst random variables

820
00:51:59,910 --> 00:52:02,110
and there's a relationship between their

821
00:52:02,142 --> 00:52:04,815
computability and their sparsity.

822
00:52:04,995 --> 00:52:07,450
The sparsity structure as in which

823
00:52:07,512 --> 00:52:09,940
variables do or do not influence each

824
00:52:09,957 --> 00:52:14,355
other makes the problem tractable

825
00:52:14,490 --> 00:52:17,785
through factorization and just kind of

826
00:52:17,817 --> 00:52:19,990
conceptually like if every one of a

827
00:52:20,007 --> 00:52:22,060
thousand variables or an unknown number

828
00:52:22,092 --> 00:52:24,610
of large variables if it was all by all

829
00:52:24,792 --> 00:52:27,505
the number of parameters to fit on that

830
00:52:27,552 --> 00:52:29,635
connectivity matrix would be very high.

831
00:52:29,742 --> 00:52:32,200
So statistical power would be very low

832
00:52:32,337 --> 00:52:35,440
for any given edge. Whereas the more and

833
00:52:35,457 --> 00:52:37,735
more constrained you make the

834
00:52:37,767 --> 00:52:40,480
connectivity of the variables, the more

835
00:52:40,527 --> 00:52:44,140
statistical power you have to resolve or

836
00:52:44,157 --> 00:52:46,920
kind of spend on fitting those edges

837
00:52:47,010 --> 00:52:50,530
like in a structural equation. But you

838
00:52:50,577 --> 00:52:53,910
might be losing sight of the unknown

839
00:52:53,955 --> 00:52:56,410
unknowns by constraining yourself to a

840
00:52:56,442 --> 00:52:59,895
very limited or fallacious

841
00:52:59,985 --> 00:53:02,745
topology of the variables. So there's

842
00:53:02,760 --> 00:53:04,462
this kind of structure learning

843
00:53:05,275 --> 00:53:07,810
statistical inference question in the

844
00:53:07,842 --> 00:53:11,485
Bayes graphs then on

845
00:53:11,517 --> 00:53:16,735
the neural side from the biological much

846
00:53:16,767 --> 00:53:19,080
of neuroscience is about understanding

847
00:53:19,140 --> 00:53:21,685
how the firing rate,

848
00:53:21,867 --> 00:53:25,100
connectivity patterns and other factors

849
00:53:25,450 --> 00:53:27,810
Hohwy the structure of those neural

850
00:53:27,855 --> 00:53:30,715
systems and their function like form and

851
00:53:30,732 --> 00:53:34,755
function enable adequate

852
00:53:34,815 --> 00:53:38,300
inference and inference on action.

853
00:53:38,875 --> 00:53:42,585
So it's like in both of those areas

854
00:53:42,705 --> 00:53:44,715
or really like in neural network

855
00:53:44,820 --> 00:53:48,050
artificial and neural networks

856
00:53:48,400 --> 00:53:51,960
and in variational. DAGs the discussion

857
00:53:52,005 --> 00:53:55,465
is about how the structure and the

858
00:53:55,482 --> 00:53:59,635
fine tuning work together to

859
00:53:59,667 --> 00:54:02,605
generate function and about some of the

860
00:54:02,652 --> 00:54:06,535
statistical or biological challenges of

861
00:54:06,567 --> 00:54:10,600
balancing different needs while also

862
00:54:10,662 --> 00:54:13,930
constraining the cost in terms of

863
00:54:13,977 --> 00:54:17,150
materials and biometabolism.

864
00:54:17,650 --> 00:54:21,650
So it's a very rich interoception

865
00:54:23,350 --> 00:54:25,537
that is being explored here.

866
00:54:29,462 --> 00:54:31,577
If these models can really be moving

867
00:54:31,670 --> 00:54:32,787
back and forth.

868
00:54:38,137 --> 00:54:42,587
In the sense that back and forth.

869
00:54:45,487 --> 00:54:48,907
Moving back and forth, like there's some

870
00:54:49,060 --> 00:54:52,087
imprints of the model that is

871
00:54:52,150 --> 00:54:54,472
implementation independent or like some

872
00:54:54,505 --> 00:54:58,387
interlingua or some semantics or

873
00:54:58,450 --> 00:55:00,082
compatibility, I don't really know. I

874
00:55:00,085 --> 00:55:01,822
mean, that's something we can explore is

875
00:55:01,855 --> 00:55:04,822
like what is it that is such that one

876
00:55:04,855 --> 00:55:08,082
could forward engineer and then reverse

877
00:55:08,097 --> 00:55:11,152
engineer and have like kind of

878
00:55:11,170 --> 00:55:14,557
an expectation maximization between

879
00:55:14,635 --> 00:55:16,677
these two areas. So what is it that's

880
00:55:16,707 --> 00:55:19,412
being solved?

881
00:55:25,087 --> 00:55:25,850
Yes,

882
00:55:28,462 --> 00:55:29,825
important point,

883
00:55:31,912 --> 00:55:38,347
for example, about you is

884
00:55:38,380 --> 00:55:40,912
that we can use the knowledge of

885
00:55:41,050 --> 00:55:43,372
Bayesian inference to explain your

886
00:55:43,405 --> 00:55:46,987
activity dynamics, which is

887
00:55:47,050 --> 00:55:50,600
crucial because people often say that

888
00:55:53,512 --> 00:55:56,512
characterizing neurodynamics is no

889
00:55:56,575 --> 00:55:57,587
straightforward,

890
00:56:00,112 --> 00:56:03,637
we may obtain some solution on your net

891
00:56:03,700 --> 00:56:06,892
dynamics, but the meaning of that

892
00:56:07,015 --> 00:56:09,567
dynamics in terms of the functional

893
00:56:09,627 --> 00:56:12,187
aspect is very unclear. We don't know

894
00:56:12,250 --> 00:56:16,002
the meaning of connectivity strength

895
00:56:16,182 --> 00:56:19,447
matrices and what is the learning of the

896
00:56:19,480 --> 00:56:22,267
threshold factor, so on and so on those

897
00:56:22,315 --> 00:56:25,925
de Vries from

898
00:56:27,637 --> 00:56:31,017
the modern physiological phenomena.

899
00:56:31,077 --> 00:56:34,775
But it is not necessary to have

900
00:56:35,212 --> 00:56:39,702
clear linkage to functional exploration.

901
00:56:39,882 --> 00:56:43,452
So explanation of function of the brain.

902
00:56:43,557 --> 00:56:47,037
But once we transform translate

903
00:56:47,112 --> 00:56:50,067
this dynamics into Bayesian inference,

904
00:56:50,202 --> 00:56:53,727
then we can explain every functional

905
00:56:53,757 --> 00:56:56,802
aspect of the neural network diagrams

906
00:56:56,832 --> 00:56:59,212
architecture in terms of where

907
00:56:59,275 --> 00:57:01,627
established Bayesian inference under a

908
00:57:01,645 --> 00:57:04,372
particular crossover Bayesian model, in

909
00:57:04,405 --> 00:57:06,425
this case palm DP model.

910
00:57:08,287 --> 00:57:12,217
So now it turns out that

911
00:57:12,415 --> 00:57:15,072
synaptic strength correspond to a matrix

912
00:57:15,117 --> 00:57:18,287
B matrix, which are very established

913
00:57:19,687 --> 00:57:23,407
culture meaning. So yeah,

914
00:57:23,560 --> 00:57:30,812
this is useful to explain neuronsynatic

915
00:57:31,387 --> 00:57:34,687
property in terms

916
00:57:34,750 --> 00:57:37,937
of established statistics.

917
00:57:44,287 --> 00:57:48,252
Also, for the people in active inference

918
00:57:48,282 --> 00:57:52,552
lab site, it would be helpful to

919
00:57:52,720 --> 00:57:54,747
understand the neuronounce master

920
00:57:54,792 --> 00:57:57,950
straight about particular

921
00:57:59,362 --> 00:58:01,775
active interface model model.

922
00:58:03,712 --> 00:58:07,567
So I think it related to forward

923
00:58:07,690 --> 00:58:12,200
modeling. But finally to discuss with

924
00:58:12,712 --> 00:58:16,087
discuss about the border service rate

925
00:58:16,150 --> 00:58:19,522
of that forward model, we need to

926
00:58:19,555 --> 00:58:25,262
address the neural network architecture

927
00:58:26,662 --> 00:58:30,022
service property. So in

928
00:58:30,055 --> 00:58:34,227
that case, we can transform

929
00:58:34,332 --> 00:58:38,322
a particular force DP invasion modeling

930
00:58:38,517 --> 00:58:41,212
to a neural network architecture using

931
00:58:41,275 --> 00:58:44,600
this relationship and then get

932
00:58:45,412 --> 00:58:49,272
prediction about the substrate.

933
00:58:49,317 --> 00:58:52,625
So if we have this based on model,

934
00:58:53,587 --> 00:58:55,987
this particular quantity in this model

935
00:58:56,050 --> 00:59:05,632
should be it

936
00:59:05,635 --> 00:59:07,175
would be possible using.

937
00:59:14,737 --> 00:59:16,227
Oh, it's all good. Can you just repeat

938
00:59:16,257 --> 00:59:19,402
the last 20 seconds? Yes.

939
00:59:19,570 --> 00:59:23,407
So in the last part I mentioned

940
00:59:23,485 --> 00:59:27,252
about first we define

941
00:59:27,282 --> 00:59:31,467
the Bayesian model and then can predict

942
00:59:31,527 --> 00:59:33,922
what is the neural net substrates that

943
00:59:33,955 --> 00:59:37,477
correspond to that particular

944
00:59:37,645 --> 00:59:41,712
Beijing model. So this will be useful

945
00:59:41,787 --> 00:59:45,887
to identify the biological

946
00:59:47,887 --> 00:59:51,877
quantities that correspond to a

947
00:59:51,895 --> 00:59:54,125
quantity in Beijing. Chaos.

948
01:00:03,562 --> 01:00:06,050
There's a lot there.

949
01:00:06,787 --> 01:00:10,467
It makes me think about the inference

950
01:00:10,527 --> 01:00:14,497
of implementation and. Heuristics in the

951
01:00:14,530 --> 01:00:18,277
computational setting, which is often in

952
01:00:18,295 --> 01:00:21,202
the extreme disembodied, and the

953
01:00:21,220 --> 01:00:23,797
biological setting, which is in the

954
01:00:23,830 --> 01:00:26,997
extreme entirely embodied.

955
01:00:27,192 --> 01:00:30,950
And for a given generative model,

956
01:00:32,512 --> 01:00:35,322
the kinds of computational heuristics

957
01:00:35,367 --> 01:00:38,827
that can be applied include a

958
01:00:38,845 --> 01:00:42,222
whole host of different strategies

959
01:00:42,417 --> 01:00:45,492
ranging from sampling to tree

960
01:00:45,552 --> 01:00:48,800
exploration and branching to

961
01:00:49,387 --> 01:00:53,172
paralyzing the data architectures

962
01:00:53,292 --> 01:00:56,487
and all these other kinds of disparate

963
01:00:56,637 --> 01:01:00,417
strategies and software

964
01:01:00,477 --> 01:01:02,237
packages and implementations.

965
01:01:04,387 --> 01:01:06,500
But on the biological side,

966
01:01:07,312 --> 01:01:11,502
what is needed is something that's

967
01:01:11,532 --> 01:01:15,200
very simple but also very

968
01:01:15,862 --> 01:01:19,025
inscrutable, which is a given

969
01:01:19,837 --> 01:01:23,257
pattern of interactions must

970
01:01:23,335 --> 01:01:26,387
embody that calculation.

971
01:01:28,387 --> 01:01:31,777
So that might mean that

972
01:01:31,945 --> 01:01:34,732
it can add three digit numbers, but it

973
01:01:34,735 --> 01:01:38,092
can't add two digit numbers under some

974
01:01:38,140 --> 01:01:41,997
constraints. But what isn't

975
01:01:42,042 --> 01:01:43,477
accessible to that kind of

976
01:01:43,495 --> 01:01:46,192
morphological, biological or like form

977
01:01:46,240 --> 01:01:48,547
and functional computing, what's not

978
01:01:48,580 --> 01:01:51,962
accessible are the tree branching,

979
01:01:52,687 --> 01:01:54,812
the database decentralization,

980
01:01:56,062 --> 01:01:58,400
like they're a different set of

981
01:01:59,137 --> 01:02:02,317
heuristics. Right. But they're both very

982
01:02:02,365 --> 01:02:06,397
useful when we're thinking about making

983
01:02:06,505 --> 01:02:10,377
sentience artifacts or benefiting

984
01:02:10,482 --> 01:02:13,672
simply from the explainability across

985
01:02:13,780 --> 01:02:16,550
both sides of this figure.

986
01:02:18,562 --> 01:02:21,950
Yeah. So you now address an important

987
01:02:22,312 --> 01:02:26,092
point. So Homistry, it is very

988
01:02:26,140 --> 01:02:29,075
nontrivial whether there is a

989
01:02:29,587 --> 01:02:31,992
corresponding valve car architecture

990
01:02:32,052 --> 01:02:36,252
force any given Bayesian architectures.

991
01:02:36,357 --> 01:02:39,802
I believe it is impossible to design

992
01:02:39,895 --> 01:02:43,102
biography architectures to respond to

993
01:02:43,120 --> 01:02:45,927
arbitrary Bayesian architectures.

994
01:02:46,032 --> 01:02:49,657
So only a limited aspect of

995
01:02:49,735 --> 01:02:53,262
region model can be implemented

996
01:02:53,337 --> 01:02:57,112
in a vertical plausible manner. And that

997
01:02:57,175 --> 01:03:01,582
point is crucial as capitalization of

998
01:03:01,660 --> 01:03:05,237
biological network. Biological brain.

999
01:03:09,412 --> 01:03:13,027
Yeah. Wow. Well, just to kind of

1000
01:03:13,195 --> 01:03:15,027
touch again on this forward in reverse

1001
01:03:15,057 --> 01:03:16,112
engineering.

1002
01:03:18,412 --> 01:03:22,152
For. A given POMDP if we're

1003
01:03:22,182 --> 01:03:25,417
willing to compose it within a certain

1004
01:03:25,615 --> 01:03:28,102
class, which might be quite general

1005
01:03:28,195 --> 01:03:31,412
still, but some class of PMDP,

1006
01:03:31,837 --> 01:03:35,812
as written. On the paper. We may

1007
01:03:35,950 --> 01:03:38,742
be able to have a neural network

1008
01:03:38,802 --> 01:03:42,057
architecture that would be very amenable

1009
01:03:42,147 --> 01:03:46,857
to deep learning, low energy computing,

1010
01:03:47,022 --> 01:03:50,517
pretraining various features.

1011
01:03:50,652 --> 01:03:54,725
And then on the other side, for a given

1012
01:03:55,237 --> 01:03:57,697
artificial neural network that we come

1013
01:03:57,730 --> 01:04:01,325
across in the wild or

1014
01:04:01,762 --> 01:04:05,002
a model of neural dynamics that

1015
01:04:05,020 --> 01:04:08,000
we fit using a neural network model.

1016
01:04:08,737 --> 01:04:10,497
So something in a neuroscience

1017
01:04:10,542 --> 01:04:13,775
laboratory that model

1018
01:04:14,437 --> 01:04:17,502
can have interpretability corresponding

1019
01:04:17,532 --> 01:04:20,872
to the variables of a given

1020
01:04:20,980 --> 01:04:24,487
POMDP. And just to kind of give one more

1021
01:04:24,550 --> 01:04:27,787
point on how that's going deeper than,

1022
01:04:27,850 --> 01:04:30,462
for example, statistical parametric

1023
01:04:30,537 --> 01:04:34,462
mapping SPM. So let's just

1024
01:04:34,525 --> 01:04:36,102
assume that the neural network we're

1025
01:04:36,132 --> 01:04:39,802
dealing with is fit from brain data

1026
01:04:39,970 --> 01:04:42,275
from some lucky Kant,

1027
01:04:43,537 --> 01:04:46,777
right? Now, what would be possible or

1028
01:04:46,870 --> 01:04:48,892
prior to this line of work or without

1029
01:04:48,940 --> 01:04:53,242
this line of work, one could fit

1030
01:04:53,365 --> 01:04:56,572
a neural dynamics model and then do

1031
01:04:56,605 --> 01:04:58,387
all kinds of analyses, like power

1032
01:04:58,450 --> 01:05:00,492
analyses on the different frequency

1033
01:05:00,552 --> 01:05:03,867
spectra and say, look at the average

1034
01:05:03,927 --> 01:05:06,387
firing rate or the correlation

1035
01:05:06,537 --> 01:05:09,052
coefficients of firing rate. So fit the

1036
01:05:09,070 --> 01:05:10,452
firing rates and the synaptic

1037
01:05:10,482 --> 01:05:14,122
Plasticities and store all that data.

1038
01:05:14,305 --> 01:05:17,877
And then we could just pick a POMDP

1039
01:05:17,907 --> 01:05:19,262
that we've seen in the literature

1040
01:05:19,762 --> 01:05:21,327
without any reference to the neural

1041
01:05:21,357 --> 01:05:24,237
network and optimize the POMDP.

1042
01:05:24,387 --> 01:05:26,527
And then we could say well, it turns out

1043
01:05:26,545 --> 01:05:31,122
that when the POMDP o is high there's

1044
01:05:31,167 --> 01:05:34,347
increased theta power in this firing

1045
01:05:34,392 --> 01:05:37,147
pattern. So it's like comparing the

1046
01:05:37,180 --> 01:05:40,182
descriptive statistics from the neural

1047
01:05:40,272 --> 01:05:42,897
model to the descriptive summary

1048
01:05:42,942 --> 01:05:46,387
statistics of the POMDP decision making

1049
01:05:46,450 --> 01:05:50,217
model. However, with this formal

1050
01:05:50,277 --> 01:05:53,077
connection there is actually an

1051
01:05:53,095 --> 01:05:57,342
interpretability to the unobserved

1052
01:05:57,402 --> 01:05:59,572
neural states which are what are being

1053
01:05:59,605 --> 01:06:02,037
inferred from the fMRI measurement,

1054
01:06:02,112 --> 01:06:04,492
from the EEG measurements and so on.

1055
01:06:04,690 --> 01:06:08,302
Those underlying variables have a

1056
01:06:08,320 --> 01:06:11,242
specific interpretability in

1057
01:06:11,290 --> 01:06:13,477
relationship to the structure of the

1058
01:06:13,495 --> 01:06:14,687
P-O-M DP.

1059
01:06:21,337 --> 01:06:24,952
Right? So yeah, that's also

1060
01:06:25,120 --> 01:06:28,937
very interesting important aspect.

1061
01:06:29,587 --> 01:06:33,172
So what

1062
01:06:33,205 --> 01:06:38,022
you said is I think more conventional

1063
01:06:38,217 --> 01:06:41,347
strategy and it is

1064
01:06:41,380 --> 01:06:44,557
also formally related to model

1065
01:06:44,635 --> 01:06:48,412
comparison aspect. So we usually think

1066
01:06:48,475 --> 01:06:52,177
various modeling and identify or select

1067
01:06:52,270 --> 01:06:55,762
what is the best model

1068
01:06:55,825 --> 01:06:58,100
to explain a given data.

1069
01:06:58,612 --> 01:07:02,362
And this reverse engineering idea

1070
01:07:02,500 --> 01:07:05,637
involves such a model comparison

1071
01:07:05,787 --> 01:07:09,397
aspect in the sense that we try

1072
01:07:09,430 --> 01:07:12,187
to find the model with the best

1073
01:07:12,250 --> 01:07:16,117
expandability which should we have

1074
01:07:16,165 --> 01:07:18,837
the identical functionality,

1075
01:07:18,987 --> 01:07:22,242
right directory

1076
01:07:22,302 --> 01:07:26,767
address, the exact same Costa

1077
01:07:26,815 --> 01:07:28,867
function architectures using the

1078
01:07:28,915 --> 01:07:32,450
information natural transformation. So

1079
01:07:32,887 --> 01:07:36,667
it should be up to explain

1080
01:07:36,790 --> 01:07:40,025
the neural data in the Bayesian sense.

1081
01:07:42,412 --> 01:07:46,567
Yeah, one can imagine how

1082
01:07:46,615 --> 01:07:50,900
that would transform the way that

1083
01:07:51,862 --> 01:07:54,472
current neuroimaging studies and

1084
01:07:54,505 --> 01:07:58,222
technologies describe what

1085
01:07:58,255 --> 01:08:00,367
it is about the measurement that

1086
01:08:00,415 --> 01:08:04,212
provides information about the cognition

1087
01:08:04,287 --> 01:08:07,992
model. So, to give another related

1088
01:08:08,052 --> 01:08:10,522
example, let's just say a person was

1089
01:08:10,555 --> 01:08:15,202
wearing an EEG headset and

1090
01:08:15,295 --> 01:08:18,277
a previous study had shown that

1091
01:08:18,445 --> 01:08:21,802
increased alphaband activity was

1092
01:08:21,820 --> 01:08:24,062
associated with this behavior.

1093
01:08:25,762 --> 01:08:28,737
That's comparing a descriptive statistic

1094
01:08:28,887 --> 01:08:32,762
of the observations of the sensor

1095
01:08:33,862 --> 01:08:36,582
and correlating the summarized

1096
01:08:36,672 --> 01:08:40,550
observable to some other

1097
01:08:40,987 --> 01:08:44,527
variable like anxiety or performance on

1098
01:08:44,545 --> 01:08:48,887
a behavior. In contrast,

1099
01:08:50,212 --> 01:08:53,932
an unobserved variable in this

1100
01:08:54,010 --> 01:08:57,737
setting the actual underlying

1101
01:08:58,312 --> 01:09:03,592
neural state is being correlated to

1102
01:09:03,715 --> 01:09:07,812
some semantic generative

1103
01:09:07,887 --> 01:09:09,887
models component.

1104
01:09:11,437 --> 01:09:15,575
So it's no longer necessarily that any

1105
01:09:17,437 --> 01:09:20,077
single frequency band would be

1106
01:09:20,095 --> 01:09:23,257
associated more or less with a given

1107
01:09:23,335 --> 01:09:27,567
outcome, but it's actually some hidden

1108
01:09:27,627 --> 01:09:31,937
state variability

1109
01:09:33,562 --> 01:09:36,772
which gains the interpretability across

1110
01:09:36,880 --> 01:09:40,262
this transformation. Which is a subtle

1111
01:09:40,912 --> 01:09:44,612
point, but it speaks to how broadly

1112
01:09:45,487 --> 01:09:48,962
the equivalents would reinterpret

1113
01:09:49,837 --> 01:09:54,247
empirical neuroimaging results as

1114
01:09:54,280 --> 01:09:57,777
well as a variety of artificial neural

1115
01:09:57,807 --> 01:10:01,187
network experiments and diagnostics

1116
01:10:01,912 --> 01:10:04,717
where people do lesion studies and

1117
01:10:04,840 --> 01:10:07,677
double knockouts on artificial neural

1118
01:10:07,707 --> 01:10:08,687
networks.

1119
01:10:11,062 --> 01:10:14,422
So anywhere where

1120
01:10:14,530 --> 01:10:17,572
somebody with awareness sees that a

1121
01:10:17,605 --> 01:10:19,522
neural network, artificial or

1122
01:10:19,555 --> 01:10:22,917
biological, is having summary features

1123
01:10:22,977 --> 01:10:25,687
described and correlated to something

1124
01:10:25,750 --> 01:10:29,137
that's more semantic in a quest

1125
01:10:29,200 --> 01:10:32,827
for meaning may now have a

1126
01:10:32,845 --> 01:10:36,137
different approach that involves

1127
01:10:36,637 --> 01:10:41,422
formalizing. The model explicitly in

1128
01:10:41,455 --> 01:10:44,525
terms of unobserved hidden states

1129
01:10:44,887 --> 01:10:48,727
with a cost function akin to

1130
01:10:48,745 --> 01:10:50,892
a variational free energy minimizing

1131
01:10:50,952 --> 01:10:53,677
risk bounding surprise on the

1132
01:10:53,695 --> 01:10:57,232
Unobservables. So even though the

1133
01:10:57,235 --> 01:11:01,252
unobservables were modeled in a

1134
01:11:01,270 --> 01:11:05,592
sense in the other conventional

1135
01:11:05,652 --> 01:11:09,112
strategy like neural activity is

1136
01:11:09,175 --> 01:11:12,882
a variable in fMRI

1137
01:11:12,972 --> 01:11:15,642
experiments, it's underlying the bold

1138
01:11:15,777 --> 01:11:19,332
signal. Yet this formalism

1139
01:11:19,422 --> 01:11:24,282
concordance is a more coherent

1140
01:11:24,372 --> 01:11:27,512
and powerful connection.

1141
01:11:35,362 --> 01:11:39,022
Lib sold. So you now

1142
01:11:39,205 --> 01:11:42,787
address this very important point.

1143
01:11:42,850 --> 01:11:46,507
So first to address

1144
01:11:46,585 --> 01:11:50,647
that so we need to clarify about

1145
01:11:50,830 --> 01:11:52,850
what is a program,

1146
01:11:53,437 --> 01:11:57,022
consider here. So this is a

1147
01:11:57,055 --> 01:12:00,877
program Socalled metabasian problem in

1148
01:12:00,895 --> 01:12:05,262
the sense that researchers try to infer

1149
01:12:05,337 --> 01:12:08,797
or estimate neuro activity or

1150
01:12:08,830 --> 01:12:12,622
brain activity which infer the

1151
01:12:12,730 --> 01:12:16,012
external world dynamics. Right. So

1152
01:12:16,075 --> 01:12:19,912
neuron or brain environment and we

1153
01:12:19,975 --> 01:12:22,992
research brain activity.

1154
01:12:23,127 --> 01:12:28,812
So there are two step processes.

1155
01:12:28,962 --> 01:12:33,187
So this sort of meta Bay is

1156
01:12:33,250 --> 01:12:38,525
quite tricky intractable because

1157
01:12:38,887 --> 01:12:44,322
sometimes London

1158
01:12:44,367 --> 01:12:48,892
variable becomes posterior about

1159
01:12:49,090 --> 01:12:52,897
other aspects. So I

1160
01:12:52,930 --> 01:12:56,112
think there is some established approach

1161
01:12:56,187 --> 01:13:00,427
about metabolism. But this

1162
01:13:00,520 --> 01:13:03,772
paper provides some alternative in the

1163
01:13:03,805 --> 01:13:09,142
sense that we separate two programs by

1164
01:13:09,190 --> 01:13:12,907
saying that here what we

1165
01:13:12,985 --> 01:13:17,907
import is simply neural

1166
01:13:17,922 --> 01:13:21,275
network dynamics which is shown in the

1167
01:13:21,712 --> 01:13:24,967
left hand side of this figure.

1168
01:13:25,165 --> 01:13:28,887
So we feed data to conventional

1169
01:13:28,962 --> 01:13:32,707
neural network model which is a simple

1170
01:13:32,785 --> 01:13:36,517
differential creation. But thanks to

1171
01:13:36,640 --> 01:13:39,627
this formal recovery between neural

1172
01:13:39,657 --> 01:13:45,357
network dynamics and home VP behavior,

1173
01:13:45,522 --> 01:13:49,227
then we can transform the resulting

1174
01:13:49,407 --> 01:13:51,717
neural network architectures or dynamics

1175
01:13:51,777 --> 01:13:55,632
into the page and in force

1176
01:13:55,797 --> 01:13:58,562
in some sense post Hog mana.

1177
01:14:00,487 --> 01:14:04,582
So we nicely avoid the

1178
01:14:04,735 --> 01:14:08,422
directory addressing the meta agent

1179
01:14:08,530 --> 01:14:11,497
program but obtain the same kind of

1180
01:14:11,530 --> 01:14:14,587
solution in that sense.

1181
01:14:14,650 --> 01:14:18,942
Yes, with combining with brain activity

1182
01:14:19,002 --> 01:14:22,332
recording Lieke de Boer imaging.

1183
01:14:22,497 --> 01:14:26,817
Yeah, we can estimate

1184
01:14:26,877 --> 01:14:30,577
a plausible neural network model in

1185
01:14:30,595 --> 01:14:33,672
the right hand side and we can transform

1186
01:14:33,717 --> 01:14:37,250
that to home DB in the right hand side.

1187
01:14:40,537 --> 01:14:43,822
Awesome. I'm going to show an image and

1188
01:14:43,855 --> 01:14:47,012
ask a question from Dave in the chat.

1189
01:14:47,437 --> 01:14:49,637
So, Dave made this image,

1190
01:14:52,462 --> 01:14:55,357
it's the right side of figure one that

1191
01:14:55,360 --> 01:14:56,752
we've just been looking at with the

1192
01:14:56,770 --> 01:15:00,877
variational Bayesian information and he

1193
01:15:00,895 --> 01:15:03,097
wrote the arc shown as impinging on the

1194
01:15:03,130 --> 01:15:07,207
S self arc. Is this

1195
01:15:07,285 --> 01:15:09,942
intentional? If so, it could represent

1196
01:15:10,002 --> 01:15:12,592
tuning or modulation of the feedback of

1197
01:15:12,640 --> 01:15:14,150
S into itself.

1198
01:15:17,587 --> 01:15:21,147
Do you have a thought on this? It's

1199
01:15:21,192 --> 01:15:25,482
attention? Yes. I think it's related

1200
01:15:25,572 --> 01:15:29,900
to the usual formulation of

1201
01:15:30,337 --> 01:15:33,972
home DP architecture and active

1202
01:15:34,017 --> 01:15:37,550
inference concept in the sense that

1203
01:15:38,587 --> 01:15:43,102
our decision or policy in

1204
01:15:43,120 --> 01:15:47,212
the usual setting modify the

1205
01:15:47,275 --> 01:15:51,137
state transition matrix b matrix.

1206
01:15:53,662 --> 01:15:57,725
Here, delta is an alternative of policy

1207
01:15:59,587 --> 01:16:03,127
of agent. So basically the

1208
01:16:03,220 --> 01:16:06,552
director indicates stated

1209
01:16:06,582 --> 01:16:09,022
transition metrics under a particular

1210
01:16:09,205 --> 01:16:13,025
decision which agent made.

1211
01:16:13,987 --> 01:16:17,392
In that sense, what the agent changes

1212
01:16:17,515 --> 01:16:20,632
is state transition metrics, not state

1213
01:16:20,710 --> 01:16:25,400
itself directly. That's why we use this

1214
01:16:26,062 --> 01:16:27,062
illustration.

1215
01:16:29,362 --> 01:16:32,952
Awesome. Very subtle

1216
01:16:32,982 --> 01:16:35,947
but important point, which is when we

1217
01:16:35,980 --> 01:16:40,487
look at the classical POMDP formulation.

1218
01:16:40,837 --> 01:16:43,597
So here we'll look at a version shown in

1219
01:16:43,630 --> 01:16:45,517
figure two. I'll just bring just figure

1220
01:16:45,565 --> 01:16:46,550
two in.

1221
01:16:49,312 --> 01:16:52,327
Could you describe what you just did

1222
01:16:52,495 --> 01:16:56,750
about the role of the B matrix in

1223
01:16:57,787 --> 01:17:00,127
influencing how hidden states change and

1224
01:17:00,145 --> 01:17:02,572
how that is where our policies have

1225
01:17:02,605 --> 01:17:06,052
impact? And also please, how do the

1226
01:17:06,070 --> 01:17:09,500
top and the bottom of figure two differ?

1227
01:17:13,087 --> 01:17:17,387
Okay, so in the usual correlation

1228
01:17:17,887 --> 01:17:21,597
under active inference

1229
01:17:21,792 --> 01:17:25,552
with palm DB structure. So we for us

1230
01:17:25,570 --> 01:17:30,350
to consider the

1231
01:17:32,512 --> 01:17:37,002
prior inference and depending

1232
01:17:37,032 --> 01:17:40,712
on the prior preference, we compute

1233
01:17:41,062 --> 01:17:43,747
the expect free energy and its

1234
01:17:43,780 --> 01:17:48,127
minimization provide the policy and

1235
01:17:48,295 --> 01:17:52,657
the policy moderate state

1236
01:17:52,735 --> 01:17:56,242
transition. So now in the

1237
01:17:56,290 --> 01:17:59,962
upper Brea, we instead use

1238
01:18:00,100 --> 01:18:03,817
the builder which is the

1239
01:18:03,940 --> 01:18:06,425
option of the agent.

1240
01:18:07,162 --> 01:18:11,737
So here option or decision was

1241
01:18:11,800 --> 01:18:15,587
made for each timestep

1242
01:18:16,312 --> 01:18:19,422
so that unlike the conventional

1243
01:18:19,467 --> 01:18:22,862
formation, we have a sequence

1244
01:18:23,212 --> 01:18:27,292
of delta and for each time

1245
01:18:27,340 --> 01:18:30,337
step delta moderates active states

1246
01:18:30,400 --> 01:18:34,092
cognition matrix B. So B is a matrix

1247
01:18:34,152 --> 01:18:38,292
that transformed hidden

1248
01:18:38,352 --> 01:18:41,887
state in the previous step to the

1249
01:18:41,950 --> 01:18:45,612
current time step and its moderation

1250
01:18:45,687 --> 01:18:49,602
indicate that under a specific decision

1251
01:18:49,707 --> 01:18:52,475
rule. For example,

1252
01:18:53,062 --> 01:18:56,407
if this F indicates our

1253
01:18:56,560 --> 01:18:59,532
cognition in the virtual environment

1254
01:18:59,697 --> 01:19:03,412
with the Gold decision move

1255
01:19:03,550 --> 01:19:07,400
forward. But if we choose the

1256
01:19:07,762 --> 01:19:10,897
no go decision, then it

1257
01:19:10,930 --> 01:19:16,337
unchanged. So such a moderation

1258
01:19:16,987 --> 01:19:20,817
of state transition was made by choosing

1259
01:19:20,877 --> 01:19:25,617
debuta and the lower part correspond

1260
01:19:25,677 --> 01:19:30,267
to Beijing inference made by Bayesian

1261
01:19:30,327 --> 01:19:34,972
agent. So basically there

1262
01:19:35,005 --> 01:19:38,302
is a symmetry between a third part and

1263
01:19:38,395 --> 01:19:42,202
roar part because we assume that

1264
01:19:42,295 --> 01:19:46,937
this Beijing agent has a plausible

1265
01:19:47,437 --> 01:19:51,117
guarantee model which nicely correspond

1266
01:19:51,177 --> 01:19:54,652
to given environment defined in

1267
01:19:54,745 --> 01:19:59,802
the above upper

1268
01:19:59,832 --> 01:20:03,800
part in this figure. But one

1269
01:20:04,312 --> 01:20:10,100
interesting thing

1270
01:20:10,762 --> 01:20:14,632
asymmetry is that to

1271
01:20:14,710 --> 01:20:18,027
model this particular canonical neural

1272
01:20:18,057 --> 01:20:23,467
network, we don't consider an

1273
01:20:23,515 --> 01:20:27,762
arrow or link from delta posteria

1274
01:20:27,837 --> 01:20:31,462
to S posteria which is

1275
01:20:31,525 --> 01:20:35,367
in the environment data

1276
01:20:35,427 --> 01:20:38,602
moderate S

1277
01:20:38,695 --> 01:20:42,942
in the next step through P matrix

1278
01:20:43,077 --> 01:20:48,327
moderation. In this particular Bay

1279
01:20:48,357 --> 01:20:50,932
jets which formally correspond to a

1280
01:20:50,935 --> 01:20:54,492
canonical neural network, we don't

1281
01:20:54,552 --> 01:20:59,150
consider that it is correspond to

1282
01:21:00,187 --> 01:21:06,117
an absence of the projection

1283
01:21:06,177 --> 01:21:11,947
from output layer to the

1284
01:21:11,980 --> 01:21:13,037
middle layer.

1285
01:21:19,087 --> 01:21:19,850
Okay.

1286
01:21:25,837 --> 01:21:30,367
This is from the 2020

1287
01:21:30,490 --> 01:21:33,462
paper, but it shows the neural network

1288
01:21:33,537 --> 01:21:35,172
architectures, the two layer

1289
01:21:35,217 --> 01:21:41,612
architectures. So could you restate

1290
01:21:42,412 --> 01:21:46,882
the top and the bottom of figure two in

1291
01:21:46,885 --> 01:21:50,217
the 22 paper and connect it to why it's

1292
01:21:50,277 --> 01:21:52,902
important that you're studying two layer

1293
01:21:53,007 --> 01:21:57,050
neural network models? I miss you.

1294
01:21:58,162 --> 01:22:03,142
Yeah, can you just connect the

1295
01:22:03,340 --> 01:22:06,877
asymmetry between the top and the

1296
01:22:06,895 --> 01:22:10,747
bottom on figure two with the

1297
01:22:10,855 --> 01:22:14,387
two layer neural network architectures?

1298
01:22:15,337 --> 01:22:17,197
You said that the asymmetry, there's no

1299
01:22:17,230 --> 01:22:20,375
direct link between.

1300
01:22:27,475 --> 01:22:31,612
This is another story. So in

1301
01:22:32,425 --> 01:22:38,680
the previous paper there

1302
01:22:38,727 --> 01:22:44,655
is only output or concept

1303
01:22:44,715 --> 01:22:49,420
layer because we basically consider a

1304
01:22:49,497 --> 01:22:54,160
single layer feedforward network. So my

1305
01:22:54,192 --> 01:22:59,455
apologies for some confusion about the

1306
01:22:59,502 --> 01:23:02,725
network architectures in the 2020

1307
01:23:02,787 --> 01:23:06,625
paper. So now upper part of

1308
01:23:06,687 --> 01:23:09,820
this network architectures correspond to

1309
01:23:09,897 --> 01:23:13,060
environmental generative process and

1310
01:23:13,092 --> 01:23:17,125
only a lower part correspond to single

1311
01:23:17,187 --> 01:23:19,365
foot feed for neural network

1312
01:23:19,545 --> 01:23:24,250
architectures. So now this

1313
01:23:24,312 --> 01:23:28,255
part is identical to

1314
01:23:28,452 --> 01:23:32,965
a map from O to S.

1315
01:23:33,057 --> 01:23:37,625
OSS area in the 2022 papers.

1316
01:23:43,225 --> 01:23:47,512
Okay, so on the top of figure two

1317
01:23:48,400 --> 01:23:51,187
is the actual generative process.

1318
01:23:52,000 --> 01:23:55,195
It's the true structure of causation in

1319
01:23:55,197 --> 01:23:58,330
the environment, which is to say that

1320
01:23:58,527 --> 01:24:02,485
actions delta actually

1321
01:24:02,592 --> 01:24:05,437
influence how states change through time

1322
01:24:05,950 --> 01:24:07,925
via B delta.

1323
01:24:09,625 --> 01:24:12,160
The generative process through the A

1324
01:24:12,192 --> 01:24:15,180
matrix emits observations,

1325
01:24:15,315 --> 01:24:17,150
sequences of observations.

1326
01:24:18,100 --> 01:24:21,955
And here on the bottom with

1327
01:24:22,002 --> 01:24:25,400
a mirrored structure

1328
01:24:26,500 --> 01:24:30,125
is the generative model of the entity.

1329
01:24:30,625 --> 01:24:33,425
So what's the relevance of the arrows

1330
01:24:34,225 --> 01:24:36,795
and the more force factor graph

1331
01:24:36,885 --> 01:24:38,887
structure on the bottom?

1332
01:24:40,675 --> 01:24:44,662
The arrow indicates active

1333
01:24:47,050 --> 01:24:48,275
inference.

1334
01:24:50,425 --> 01:24:55,987
So it's a flow of the information

1335
01:24:56,425 --> 01:25:00,640
in the sense that to calculate in

1336
01:25:00,657 --> 01:25:04,465
the step two, we use the information

1337
01:25:04,632 --> 01:25:07,900
of step two conversation and step

1338
01:25:07,962 --> 01:25:11,190
one's posterior expectation about hidden

1339
01:25:11,220 --> 01:25:14,962
states. So those two determine the

1340
01:25:15,325 --> 01:25:17,900
s two's expectation.

1341
01:25:18,625 --> 01:25:22,150
Usually in the following graph, we

1342
01:25:22,212 --> 01:25:27,600
consider retrospective arrow

1343
01:25:27,675 --> 01:25:33,720
so in the sense that s three also affect

1344
01:25:33,810 --> 01:25:37,762
the s two inference. But this

1345
01:25:38,425 --> 01:25:43,090
corresponds to Bayesian smoother in

1346
01:25:43,107 --> 01:25:46,912
the sense that we update every time step

1347
01:25:47,875 --> 01:25:51,555
simultaneously to better inference.

1348
01:25:51,690 --> 01:25:55,762
However, what we consider here is more

1349
01:25:57,550 --> 01:26:00,130
filtering approach in the sense that for

1350
01:26:00,177 --> 01:26:03,480
each step we compute the latest hidden

1351
01:26:03,540 --> 01:26:07,735
states and then we don't change any

1352
01:26:07,767 --> 01:26:11,230
other states in the past.

1353
01:26:11,427 --> 01:26:14,860
So that's why we don't consider the

1354
01:26:14,892 --> 01:26:18,085
arrow from future to the

1355
01:26:18,117 --> 01:26:18,712
past.

1356
01:26:22,337 --> 01:26:27,002
Awesome. Yeah. Just to highlight that in

1357
01:26:27,020 --> 01:26:29,902
the Bayesian smoothing approach, it's

1358
01:26:29,932 --> 01:26:32,882
kind of like fitting a spline because it

1359
01:26:32,885 --> 01:26:35,092
takes the whole time series and it fits

1360
01:26:35,152 --> 01:26:38,475
the smoothest possible line

1361
01:26:39,587 --> 01:26:43,262
or the line whose smoothness is on the

1362
01:26:43,400 --> 01:26:46,587
AIC BIC frontier.

1363
01:26:47,612 --> 01:26:51,000
But here on the bottom with the

1364
01:26:51,737 --> 01:26:54,547
almost pseudocode implementation

1365
01:26:54,742 --> 01:26:57,612
provided by the Force Factor graph,

1366
01:26:58,037 --> 01:27:00,217
which was demonstrated to be equivalent

1367
01:27:00,277 --> 01:27:03,707
with the Bayesian graph in the

1368
01:27:03,785 --> 01:27:06,752
2017 work with Friston, Par and de

1369
01:27:06,770 --> 01:27:10,217
Vries. This architectures is

1370
01:27:10,265 --> 01:27:12,557
reflecting a filtering scheme like a

1371
01:27:12,560 --> 01:27:14,872
common filter or just generalized

1372
01:27:14,917 --> 01:27:17,925
Bayesian filtering through time, where

1373
01:27:19,637 --> 01:27:22,787
estimates are being carried forward and

1374
01:27:22,850 --> 01:27:25,200
changed time point to time point,

1375
01:27:25,862 --> 01:27:28,322
such that the decision rules, or the

1376
01:27:28,355 --> 01:27:31,022
updates perhaps more accurately, are

1377
01:27:31,130 --> 01:27:34,847
defined between time points. And the

1378
01:27:34,955 --> 01:27:37,532
total time series does not have to be

1379
01:27:37,610 --> 01:27:40,817
loaded into memory or remembered at

1380
01:27:40,865 --> 01:27:43,777
once. And then the Bayesian filtering

1381
01:27:43,807 --> 01:27:48,322
approach has the asymmetry

1382
01:27:48,517 --> 01:27:51,392
with a different consideration of

1383
01:27:51,515 --> 01:27:54,992
action. So why again

1384
01:27:55,040 --> 01:27:57,427
is it that action is considered

1385
01:27:57,457 --> 01:28:00,607
differently in the Bayesian filtering

1386
01:28:00,697 --> 01:28:03,037
approach on the bottom of the generative

1387
01:28:03,112 --> 01:28:06,442
models than the consideration of action

1388
01:28:06,502 --> 01:28:07,875
in the generative process.

1389
01:28:11,162 --> 01:28:15,202
That correspond to lack

1390
01:28:15,232 --> 01:28:19,112
of cognition from Y to X

1391
01:28:19,175 --> 01:28:22,125
in the figure one?

1392
01:28:23,387 --> 01:28:27,632
Or probably a figure four

1393
01:28:27,710 --> 01:28:31,292
is helpful to that

1394
01:28:31,415 --> 01:28:32,100
relationship.

1395
01:28:37,487 --> 01:28:41,402
Four? Yeah. This is an example

1396
01:28:41,570 --> 01:28:44,722
network architecture comprising input

1397
01:28:44,767 --> 01:28:47,942
Brea, output Brea. What we

1398
01:28:47,990 --> 01:28:51,000
consider is information flow from

1399
01:28:51,362 --> 01:28:54,587
sensory to middle Brea and middle area

1400
01:28:54,650 --> 01:28:56,572
have a self connection, recurrent

1401
01:28:56,617 --> 01:28:59,642
connection and middle area project to

1402
01:28:59,690 --> 01:29:03,262
output layer. So there is no connection

1403
01:29:03,337 --> 01:29:06,962
from all output layer to middle

1404
01:29:07,025 --> 01:29:09,150
layer. Right.

1405
01:29:10,412 --> 01:29:14,867
So that's why we don't consider

1406
01:29:14,990 --> 01:29:18,892
the link from data

1407
01:29:19,027 --> 01:29:21,975
in the bottom layer of the figure to

1408
01:29:24,212 --> 01:29:27,677
posterior. So this

1409
01:29:27,695 --> 01:29:31,432
is different from true generative

1410
01:29:31,522 --> 01:29:34,662
process in the environment.

1411
01:29:37,975 --> 01:29:40,500
This is a kind of simplification.

1412
01:29:40,650 --> 01:29:44,130
So because our purpose is identifying

1413
01:29:44,190 --> 01:29:48,210
the plausible Bayesian

1414
01:29:48,255 --> 01:29:51,337
model which correspond to this type of

1415
01:29:52,075 --> 01:29:54,650
neural network canonical network.

1416
01:29:56,200 --> 01:29:59,715
So in other words, this neural

1417
01:29:59,745 --> 01:30:03,460
network uses approximation about that

1418
01:30:03,567 --> 01:30:09,487
point or use

1419
01:30:10,525 --> 01:30:14,685
limited form of palm

1420
01:30:14,730 --> 01:30:17,300
DP scheme.

1421
01:30:23,700 --> 01:30:31,950
Thanks. So could you describe W-V-K

1422
01:30:32,087 --> 01:30:35,800
and Gamma? Just what is the biological

1423
01:30:36,450 --> 01:30:38,850
or functional interpretation of those

1424
01:30:38,912 --> 01:30:42,110
variables? What brain regions

1425
01:30:42,155 --> 01:30:44,880
or what processes or pathologies do they

1426
01:30:44,927 --> 01:30:47,787
map to? Okay,

1427
01:30:48,225 --> 01:30:51,475
so basically, WVK,

1428
01:30:52,200 --> 01:30:55,485
synaptic strength is in the form of

1429
01:30:55,592 --> 01:30:59,205
matrix and active

1430
01:30:59,252 --> 01:30:59,862
inference.

1431
01:31:04,875 --> 01:31:08,255
They represent connection

1432
01:31:08,315 --> 01:31:10,620
in the different layer or different

1433
01:31:10,772 --> 01:31:15,012
architectures in the sense that W

1434
01:31:15,750 --> 01:31:19,625
means forward connectivity from sensory

1435
01:31:19,700 --> 01:31:22,800
Laje to middle Laje, k correspond to

1436
01:31:22,862 --> 01:31:25,520
recurrent network recurrent connectivity

1437
01:31:25,685 --> 01:31:30,155
and V correspond to projection

1438
01:31:30,215 --> 01:31:33,215
from middle Brea to output layer.

1439
01:31:33,395 --> 01:31:37,080
So in this paper, we don't discuss the

1440
01:31:37,277 --> 01:31:40,755
relation to brain anatomy in detail,

1441
01:31:40,877 --> 01:31:47,385
but what one

1442
01:31:47,417 --> 01:31:50,862
can consider analogy, for example,

1443
01:31:51,900 --> 01:31:55,287
say x corresponds to

1444
01:31:57,225 --> 01:32:01,362
several cortex activity and

1445
01:32:02,175 --> 01:32:05,930
Y, for example correspond to cerebral

1446
01:32:05,990 --> 01:32:09,700
wrong in the sense that it determines

1447
01:32:13,125 --> 01:32:17,415
the action. So it is considered that

1448
01:32:17,507 --> 01:32:20,610
in the cerebrum there is a signal that

1449
01:32:20,717 --> 01:32:23,745
represents choice. This is joined for

1450
01:32:23,747 --> 01:32:27,750
examples goal no go decision made

1451
01:32:27,812 --> 01:32:29,200
in cergram.

1452
01:32:31,575 --> 01:32:35,700
It's analogous to this

1453
01:32:35,762 --> 01:32:38,310
particular architectures. On the other

1454
01:32:38,342 --> 01:32:41,855
hand, in the several cortex

1455
01:32:41,990 --> 01:32:45,785
we compute the sensory

1456
01:32:45,830 --> 01:32:49,985
information to generate

1457
01:32:50,030 --> 01:32:55,300
some inference, prediction and planning

1458
01:32:56,850 --> 01:33:00,855
the way it is computer by

1459
01:33:00,977 --> 01:33:03,300
this recurrent network. In this

1460
01:33:03,362 --> 01:33:07,235
particular modeling, although we don't

1461
01:33:07,355 --> 01:33:10,815
separate brain region in detail,

1462
01:33:10,982 --> 01:33:14,500
but this recurrent network is sufficient

1463
01:33:16,725 --> 01:33:18,960
this graph of recurrent network is

1464
01:33:18,992 --> 01:33:20,850
sufficient to characterize any type

1465
01:33:20,912 --> 01:33:24,765
brain architecture in

1466
01:33:24,782 --> 01:33:28,770
the sense that we can design

1467
01:33:28,847 --> 01:33:32,850
any higher

1468
01:33:32,912 --> 01:33:37,520
car or mutually

1469
01:33:37,610 --> 01:33:41,405
connected architecture using a generic

1470
01:33:41,465 --> 01:33:45,815
crossover recurrent network by changing

1471
01:33:45,995 --> 01:33:46,750
weight.

1472
01:33:54,750 --> 01:33:58,760
Awesome. So the middle layer

1473
01:33:58,880 --> 01:34:01,445
we can think of as like the cognition

1474
01:34:01,535 --> 01:34:04,560
stuff. It's the internal states when we

1475
01:34:04,592 --> 01:34:06,320
talk about perception, cognition,

1476
01:34:06,410 --> 01:34:10,475
action in the active scheme

1477
01:34:10,550 --> 01:34:12,810
or even in the sandwich model of

1478
01:34:12,842 --> 01:34:16,300
cognition, perception, cognition action.

1479
01:34:17,250 --> 01:34:21,080
So W is describing

1480
01:34:21,140 --> 01:34:23,805
how those sensory inputs either in one

1481
01:34:23,852 --> 01:34:27,740
step or composably in multiple steps

1482
01:34:27,920 --> 01:34:31,437
become processed to these internal

1483
01:34:32,475 --> 01:34:36,260
representation of hidden external causes

1484
01:34:36,455 --> 01:34:38,760
inferred external states. And so these

1485
01:34:38,792 --> 01:34:41,820
are the states that have

1486
01:34:41,897 --> 01:34:45,735
that sigma relationship and

1487
01:34:45,767 --> 01:34:47,585
a generalized synchrony with external

1488
01:34:47,630 --> 01:34:49,715
states. The sigma and the generalized

1489
01:34:49,745 --> 01:34:51,960
synchrony are not discussed in your

1490
01:34:51,992 --> 01:34:55,305
paper, but it connects to other work and

1491
01:34:55,502 --> 01:34:59,160
the recurrent connections are

1492
01:34:59,342 --> 01:35:03,630
facilitating attention or

1493
01:35:03,677 --> 01:35:06,550
waiting of the stimuli.

1494
01:35:08,250 --> 01:35:11,685
This is the recurrent learning loop and

1495
01:35:11,717 --> 01:35:13,965
the relationship of the A between

1496
01:35:14,057 --> 01:35:16,450
observations and hidden state estimates.

1497
01:35:18,225 --> 01:35:20,875
And then a different kind of modulation

1498
01:35:21,675 --> 01:35:24,710
comes Hinton play between the hidden

1499
01:35:24,755 --> 01:35:27,195
state estimate of the internal states

1500
01:35:27,197 --> 01:35:30,435
state and the action selection. So what

1501
01:35:30,467 --> 01:35:34,410
is gamma corresponding to?

1502
01:35:34,442 --> 01:35:37,725
And why is the gamma modulation between

1503
01:35:37,787 --> 01:35:40,450
layers two and three differing

1504
01:35:40,950 --> 01:35:44,510
functionally from the k synaptic

1505
01:35:44,555 --> 01:35:46,812
modulation of one and two?

1506
01:35:47,850 --> 01:35:50,875
Yeah, so K matrix

1507
01:35:52,275 --> 01:35:55,605
basically formally correspond to B

1508
01:35:55,652 --> 01:35:58,490
matrix in the Bayesian information.

1509
01:35:58,595 --> 01:36:02,580
So we rotate the information about

1510
01:36:02,627 --> 01:36:06,515
the prediction, right, our narrator

1511
01:36:06,545 --> 01:36:10,935
or our expectation about the next

1512
01:36:11,117 --> 01:36:14,037
state based on the previous state.

1513
01:36:14,775 --> 01:36:18,125
On the other hand, Laurel gamma

1514
01:36:18,275 --> 01:36:21,795
is quite different from such a

1515
01:36:21,872 --> 01:36:25,287
computation. Gamma basically means

1516
01:36:27,675 --> 01:36:32,535
risk function, which is in

1517
01:36:32,567 --> 01:36:35,780
principle can we use arbitrary risk

1518
01:36:35,840 --> 01:36:38,610
function. So this is a part of

1519
01:36:38,792 --> 01:36:42,180
generative models we designed and the

1520
01:36:42,227 --> 01:36:44,855
rule of risk function in generative

1521
01:36:44,915 --> 01:36:48,710
model formulation is attention

1522
01:36:48,830 --> 01:36:51,825
form of generative models depending on

1523
01:36:51,962 --> 01:36:55,365
that value of gamma which

1524
01:36:55,532 --> 01:37:03,400
examples retrospective

1525
01:37:03,825 --> 01:37:07,385
moderation of evaluation of task

1526
01:37:07,430 --> 01:37:11,130
decisions given an

1527
01:37:11,252 --> 01:37:13,587
outcome in the future.

1528
01:37:15,225 --> 01:37:19,185
In terms of neural network, of course it

1529
01:37:19,217 --> 01:37:21,485
corresponds to some neural modulation.

1530
01:37:21,605 --> 01:37:24,195
For example, Dopaminergic moderation is

1531
01:37:24,272 --> 01:37:28,375
famous in the literature which moderates

1532
01:37:29,025 --> 01:37:33,250
the activity and fluxicity of various

1533
01:37:33,675 --> 01:37:37,290
brain vision. But we particularly focus

1534
01:37:37,382 --> 01:37:41,475
on Dopaminergic or any

1535
01:37:41,612 --> 01:37:44,930
kind of neuromoduration of cyanogic

1536
01:37:44,990 --> 01:37:48,555
prosthesis in the output trigger which

1537
01:37:48,602 --> 01:37:50,950
may correspond to Cergram.

1538
01:37:51,525 --> 01:37:54,860
So in the Serbram neural

1539
01:37:54,905 --> 01:37:57,510
activity or processes moderated by

1540
01:37:57,617 --> 01:38:07,812
Dopaminergic, input from is

1541
01:38:08,250 --> 01:38:11,800
used as the optimization

1542
01:38:13,425 --> 01:38:16,190
action rule,

1543
01:38:16,295 --> 01:38:20,200
decision rule or sometimes attention

1544
01:38:24,225 --> 01:38:25,362
help us.

1545
01:38:29,212 --> 01:38:32,917
Awesome. Very interesting because in

1546
01:38:32,965 --> 01:38:36,007
some previous papers and models that

1547
01:38:36,010 --> 01:38:40,012
we've looked at, attention is

1548
01:38:40,150 --> 01:38:44,497
dealt with as policy selection on

1549
01:38:44,605 --> 01:38:47,622
mental states. So internal action

1550
01:38:47,667 --> 01:38:51,262
selection, it's an action like

1551
01:38:51,325 --> 01:38:53,752
variable describing attention and

1552
01:38:53,770 --> 01:38:56,012
awareness and even metacognition.

1553
01:38:56,737 --> 01:39:00,522
And so that connects the role

1554
01:39:00,567 --> 01:39:04,400
of Dopamine in motor decision making

1555
01:39:05,362 --> 01:39:08,652
seen in many Dyskinesias

1556
01:39:08,832 --> 01:39:12,007
but also with the role of Dopamine in

1557
01:39:12,085 --> 01:39:15,607
seemingly nonmotor based

1558
01:39:15,685 --> 01:39:19,362
decisionmaking like gambling

1559
01:39:19,437 --> 01:39:22,747
or investing where it doesn't seem to

1560
01:39:22,780 --> 01:39:26,047
immediately translate to a

1561
01:39:26,080 --> 01:39:29,767
given motor sequence. Yet it has

1562
01:39:29,815 --> 01:39:32,607
analogous computational characteristics

1563
01:39:32,772 --> 01:39:35,617
and the comorbidities and the side

1564
01:39:35,665 --> 01:39:37,737
effects of different drugs that affect

1565
01:39:37,812 --> 01:39:40,797
the Dopamine neurophysiology are known

1566
01:39:40,842 --> 01:39:44,437
to have carryover in terms of

1567
01:39:44,500 --> 01:39:47,587
like the rigidity or excessivity of

1568
01:39:47,650 --> 01:39:50,877
motor and decision making aspects.

1569
01:39:50,907 --> 01:39:53,312
So it's like interesting that Dopamine

1570
01:39:53,662 --> 01:39:55,567
has long been understood to have that

1571
01:39:55,615 --> 01:39:59,212
parallel role in

1572
01:39:59,350 --> 01:40:03,157
attention as cognitive action and

1573
01:40:03,235 --> 01:40:06,652
motor action and that was

1574
01:40:06,820 --> 01:40:08,900
established empirically through

1575
01:40:09,712 --> 01:40:13,342
modifications of Dopamine signaling and

1576
01:40:13,390 --> 01:40:16,267
also had been modeled analogously with

1577
01:40:16,315 --> 01:40:19,337
computational neuroscience.

1578
01:40:19,687 --> 01:40:23,142
And this is providing again a slightly

1579
01:40:23,202 --> 01:40:27,217
different interpretation of

1580
01:40:27,265 --> 01:40:31,542
that very well studied Dofaminergic

1581
01:40:31,602 --> 01:40:35,182
modulation of attention and

1582
01:40:35,260 --> 01:40:35,900
policy.

1583
01:40:40,837 --> 01:40:44,412
Yes. In addition

1584
01:40:44,487 --> 01:40:46,777
to that, I believe another important

1585
01:40:46,870 --> 01:40:49,437
aspects is correlation of scientific

1586
01:40:49,512 --> 01:40:51,887
processing by document.

1587
01:41:15,850 --> 01:41:18,937
Do you want to show something or yeah.

1588
01:41:20,875 --> 01:41:24,670
Can you see this

1589
01:41:24,747 --> 01:41:28,150
paper? I sent

1590
01:41:28,212 --> 01:41:31,325
you a chat.

1591
01:41:32,275 --> 01:41:36,275
If you can't, I'll send you a PDF.

1592
01:41:37,450 --> 01:41:41,110
Okay, let me see. I'll look

1593
01:41:41,142 --> 01:41:42,412
at it up now.

1594
01:41:47,425 --> 01:41:51,540
All right. The paper is a critical

1595
01:41:51,645 --> 01:41:54,950
time window for Dopamine actions

1596
01:41:56,050 --> 01:41:57,940
on the structural plasticity of

1597
01:41:57,957 --> 01:42:02,460
dendritic spines from 2014 byagasha.

1598
01:42:02,580 --> 01:42:05,155
So what is interesting about this paper.

1599
01:42:05,352 --> 01:42:08,150
Yeah, it basically explained

1600
01:42:10,150 --> 01:42:14,160
conversation of plasticity by Dopamine,

1601
01:42:14,280 --> 01:42:18,885
which is common but crucial

1602
01:42:19,080 --> 01:42:22,330
point of this paper is that it shows

1603
01:42:22,377 --> 01:42:26,355
that it proved that Dopaminezic input

1604
01:42:26,490 --> 01:42:31,495
can moderate after

1605
01:42:31,647 --> 01:42:34,395
hebion prosthesis is established.

1606
01:42:34,485 --> 01:42:37,915
So this paper showed that they

1607
01:42:38,007 --> 01:42:41,635
add domain logic input for

1608
01:42:41,667 --> 01:42:44,835
about 2 seconds after or several seconds

1609
01:42:44,880 --> 01:42:49,090
after the Hebbian process

1610
01:42:49,182 --> 01:42:52,950
is established. But such a post hoc

1611
01:42:53,025 --> 01:42:58,080
moderation, post hoc introduction

1612
01:42:58,215 --> 01:43:01,110
of heterotopamagic Impetu is sufficient

1613
01:43:01,155 --> 01:43:05,380
to change the past capacity which

1614
01:43:05,427 --> 01:43:09,340
may be associated with the

1615
01:43:09,432 --> 01:43:12,700
Costa hoc evolution of

1616
01:43:12,762 --> 01:43:16,605
our past decisions. So by decision

1617
01:43:16,665 --> 01:43:21,625
making we of course changes the changes

1618
01:43:21,687 --> 01:43:25,180
the weight matrix by through trust 50.

1619
01:43:25,377 --> 01:43:29,335
But to evaluate the

1620
01:43:29,367 --> 01:43:32,230
goodness or badness of our decision, we

1621
01:43:32,277 --> 01:43:36,425
need to see observe the future outcome

1622
01:43:37,150 --> 01:43:39,925
which is propagated by for example,

1623
01:43:39,987 --> 01:43:44,305
Dopamine. And this paper nicely show

1624
01:43:44,352 --> 01:43:48,300
empirically that Dopamine

1625
01:43:48,450 --> 01:43:52,045
actually can change

1626
01:43:52,122 --> 01:43:56,610
the past evaluation,

1627
01:43:56,805 --> 01:44:00,145
maybe after such a psychic level,

1628
01:44:00,297 --> 01:44:03,037
very local level,

1629
01:44:03,625 --> 01:44:05,437
ecoscopic level.

1630
01:44:12,250 --> 01:44:15,910
So there's a

1631
01:44:15,942 --> 01:44:19,100
short term window,

1632
01:44:20,275 --> 01:44:22,815
the critical time window that they're

1633
01:44:22,845 --> 01:44:25,935
describing. But there's some window.

1634
01:44:26,130 --> 01:44:29,812
Yeah, some window by which

1635
01:44:30,925 --> 01:44:33,910
dopamine potentially unrelated to the

1636
01:44:33,942 --> 01:44:37,550
initial heavy and plasticity events,

1637
01:44:37,975 --> 01:44:41,100
right. Where secondary dopamine

1638
01:44:41,175 --> 01:44:45,180
signaling or not secondary

1639
01:44:45,240 --> 01:44:47,737
just after the initial fact,

1640
01:44:48,925 --> 01:44:50,890
potentially of a different valence or

1641
01:44:50,907 --> 01:44:56,730
the same valence can synergize

1642
01:44:56,865 --> 01:45:00,160
or cancel the

1643
01:45:00,267 --> 01:45:02,662
plasticity formed in the moment.

1644
01:45:03,100 --> 01:45:07,125
Exactly. And this is not limited

1645
01:45:07,200 --> 01:45:09,930
to Dopamine, but other neuro moderator

1646
01:45:09,990 --> 01:45:12,862
can also do this.

1647
01:45:18,250 --> 01:45:20,755
Well, on one hand, how does this change

1648
01:45:20,802 --> 01:45:23,460
our understanding of animal

1649
01:45:23,505 --> 01:45:26,395
neurophysiology? And then I guess, on

1650
01:45:26,397 --> 01:45:28,830
the other hand, how does this influence

1651
01:45:28,890 --> 01:45:32,430
how we would design sentient

1652
01:45:32,490 --> 01:45:33,575
artifacts.

1653
01:45:42,625 --> 01:45:50,430
For both animals and artificial

1654
01:45:50,490 --> 01:45:54,760
agent? One important

1655
01:45:54,867 --> 01:45:57,820
message I free with us.

1656
01:45:57,972 --> 01:46:02,185
So this tells us possible

1657
01:46:02,292 --> 01:46:06,275
simple architectures to make learning.

1658
01:46:07,900 --> 01:46:09,955
This is association between past

1659
01:46:10,002 --> 01:46:13,390
decision and future reward or

1660
01:46:13,482 --> 01:46:17,280
any risk factors, which is otherwise

1661
01:46:17,415 --> 01:46:21,510
computed by computing forward prediction

1662
01:46:21,630 --> 01:46:24,875
by iterating some computational,

1663
01:46:26,500 --> 01:46:31,710
this is a usual way to predict

1664
01:46:31,755 --> 01:46:35,320
the future event and then select the

1665
01:46:35,397 --> 01:46:39,337
option. But using this

1666
01:46:39,775 --> 01:46:42,685
property, biological property, which is

1667
01:46:42,717 --> 01:46:45,725
observed in experiment,

1668
01:46:46,525 --> 01:46:50,512
we can design, we can imagine other

1669
01:46:51,325 --> 01:46:55,100
simpler architecture to make a planning.

1670
01:46:57,250 --> 01:47:02,490
So for both animals and synthetic

1671
01:47:02,670 --> 01:47:05,650
Bayesian agent, it provides an

1672
01:47:05,712 --> 01:47:09,610
alternative explanation about the

1673
01:47:09,642 --> 01:47:12,480
association between our past decision

1674
01:47:12,540 --> 01:47:15,125
and the future risk and the optimization

1675
01:47:15,550 --> 01:47:18,450
of our decision to maximize,

1676
01:47:18,525 --> 01:47:20,600
reward or minimize risk.

1677
01:47:25,075 --> 01:47:28,590
Well, one interesting note is we spoke

1678
01:47:28,620 --> 01:47:30,640
earlier about the difference between the

1679
01:47:30,657 --> 01:47:33,975
Bayesian smoothing all at once approach

1680
01:47:34,125 --> 01:47:36,445
and the Bayesian filtering step by step

1681
01:47:36,522 --> 01:47:40,185
approach. Now, if one had infinite

1682
01:47:40,230 --> 01:47:42,765
knowledge and computational resources,

1683
01:47:42,945 --> 01:47:45,265
the Bayesian smoothing approach is the

1684
01:47:45,282 --> 01:47:47,515
way to go. Like, you don't want the

1685
01:47:47,532 --> 01:47:50,020
decision rule for investment. You want

1686
01:47:50,022 --> 01:47:52,180
to look at the whole time series past,

1687
01:47:52,227 --> 01:47:55,105
present and future, and know the best

1688
01:47:55,152 --> 01:47:57,415
moments to have made the trades. I mean,

1689
01:47:57,432 --> 01:47:58,990
there's no comparison. You're going to

1690
01:47:59,007 --> 01:48:01,515
do better with the Bayesian smoothing.

1691
01:48:01,695 --> 01:48:03,555
However, it's just implausible

1692
01:48:03,690 --> 01:48:06,210
computationally and because it requires

1693
01:48:06,330 --> 01:48:08,790
total memory of the past and knowledge

1694
01:48:08,820 --> 01:48:11,850
of the future. So that's what motivates

1695
01:48:12,000 --> 01:48:15,225
the development of Bayesian filtering

1696
01:48:15,300 --> 01:48:19,255
approaches, which are tractable and

1697
01:48:19,302 --> 01:48:23,485
calculable through time. Yet with

1698
01:48:23,517 --> 01:48:25,625
this time delayed modulation.

1699
01:48:27,400 --> 01:48:30,870
Part. Of the Bayesian smoothing strength

1700
01:48:30,960 --> 01:48:34,650
comma back into play. It doesn't

1701
01:48:34,725 --> 01:48:38,730
enable true anticipation

1702
01:48:38,790 --> 01:48:41,890
of future states, but that's what the

1703
01:48:41,907 --> 01:48:45,190
expected free energy does. However,

1704
01:48:45,282 --> 01:48:48,430
the delayed neuromodulation allows for

1705
01:48:48,477 --> 01:48:51,055
reconsideration of a window of past

1706
01:48:51,102 --> 01:48:54,175
states. And so in that way it

1707
01:48:54,237 --> 01:48:57,937
corresponds to like a

1708
01:48:58,525 --> 01:49:01,475
slightly deeper filter,

1709
01:49:01,975 --> 01:49:04,375
not just a filter of a time step of one,

1710
01:49:04,437 --> 01:49:06,835
but a filter of like a rolling window of

1711
01:49:06,867 --> 01:49:10,380
five or with some decay. And you don't

1712
01:49:10,440 --> 01:49:12,745
want that window to be too big because

1713
01:49:12,822 --> 01:49:14,887
if the window were ten minutes,

1714
01:49:15,850 --> 01:49:19,260
then too many contrasting

1715
01:49:19,305 --> 01:49:22,837
stimuli would get piled together.

1716
01:49:23,350 --> 01:49:25,290
The Dopamine level would just converge

1717
01:49:25,320 --> 01:49:27,990
to a mean field average. But there's

1718
01:49:28,020 --> 01:49:31,960
some time decay or time constant on

1719
01:49:31,992 --> 01:49:35,740
the post hoc modulation where that

1720
01:49:35,832 --> 01:49:39,040
neuromodulatory signal is

1721
01:49:39,132 --> 01:49:42,412
actually a parameter of interest.

1722
01:49:43,000 --> 01:49:44,965
And that's not an infinitely long or

1723
01:49:44,982 --> 01:49:47,437
infinitely short window, but it's some

1724
01:49:48,325 --> 01:49:51,787
niche dependent amount of time.

1725
01:49:52,450 --> 01:49:55,885
And that's a very interpretable and

1726
01:49:55,917 --> 01:50:00,310
first principles interpretation of

1727
01:50:00,492 --> 01:50:01,915
the computational role of

1728
01:50:01,932 --> 01:50:05,515
neuromodulators in a way

1729
01:50:05,532 --> 01:50:08,200
that is also consistent with all these

1730
01:50:08,262 --> 01:50:10,920
other concordances we've been exploring.

1731
01:50:11,085 --> 01:50:15,585
So it's quite an interesting connection

1732
01:50:15,705 --> 01:50:19,960
back, I guess, in our final

1733
01:50:20,067 --> 01:50:22,400
minutes of this discussion.

1734
01:50:26,850 --> 01:50:31,440
What are you? Well, maybe go

1735
01:50:31,457 --> 01:50:34,110
to the beginning at the end, which I

1736
01:50:34,142 --> 01:50:36,210
meant to ask earlier, but it's a good

1737
01:50:36,242 --> 01:50:39,825
way that we can sort of close today and

1738
01:50:39,887 --> 01:50:42,735
look forward, which is how did you come

1739
01:50:42,767 --> 01:50:46,700
to this line of research specifically

1740
01:50:46,775 --> 01:50:50,262
studying neural networks in this way

1741
01:50:51,150 --> 01:50:53,800
with Karl Friston and your colleagues?

1742
01:50:58,275 --> 01:51:00,837
So, yes,

1743
01:51:08,550 --> 01:51:13,335
so my interest was the

1744
01:51:13,367 --> 01:51:17,915
characterization of Barricade network.

1745
01:51:18,095 --> 01:51:21,885
So my first motivation is to make

1746
01:51:21,992 --> 01:51:26,210
biologically plausible artificial

1747
01:51:26,405 --> 01:51:29,610
intelligence. But to achieve that, we

1748
01:51:29,642 --> 01:51:32,790
need to know about biological brain or

1749
01:51:32,882 --> 01:51:35,725
biological neural networking.

1750
01:51:41,100 --> 01:51:44,780
In these several years, I collaborated

1751
01:51:44,840 --> 01:51:49,205
with the doctor professor

1752
01:51:49,265 --> 01:51:53,145
Californiston to study about

1753
01:51:53,222 --> 01:51:56,725
his salary principal after doing forest.

1754
01:52:01,125 --> 01:52:05,040
My question during that

1755
01:52:05,132 --> 01:52:09,705
period was the

1756
01:52:09,752 --> 01:52:13,812
priority principle, is everything

1757
01:52:14,175 --> 01:52:18,435
about the biological possible

1758
01:52:18,542 --> 01:52:22,305
neural network or is there another

1759
01:52:22,427 --> 01:52:27,285
aspect that can characterize the

1760
01:52:27,317 --> 01:52:30,795
virus car neural network? So it is

1761
01:52:30,947 --> 01:52:35,015
non trivial. It was non trivial.

1762
01:52:35,195 --> 01:52:39,705
So that's why I

1763
01:52:39,752 --> 01:52:44,450
tried to start from characterizing

1764
01:52:44,525 --> 01:52:48,135
the neural network first. So our

1765
01:52:48,167 --> 01:52:54,960
strategy is not

1766
01:52:55,067 --> 01:52:59,190
considering the way of implementing any

1767
01:52:59,282 --> 01:53:02,960
Bayesian algorithm as the brain

1768
01:53:03,155 --> 01:53:06,960
architectures, but my interest is

1769
01:53:06,992 --> 01:53:10,125
rather characterization of a given

1770
01:53:10,262 --> 01:53:15,405
vertical network in terms of some

1771
01:53:15,452 --> 01:53:18,735
other things. One possible way

1772
01:53:18,767 --> 01:53:21,090
is of course based on inference free

1773
01:53:21,107 --> 01:53:24,140
energy transplant reinforce. So that's

1774
01:53:24,170 --> 01:53:28,520
why I start from characterizing power's

1775
01:53:28,610 --> 01:53:32,580
network. But just

1776
01:53:32,702 --> 01:53:36,550
defining neural network architecture

1777
01:53:36,975 --> 01:53:38,575
is insufficient.

1778
01:53:41,625 --> 01:53:45,255
It is not tractable, it is far

1779
01:53:45,302 --> 01:53:50,235
beyond the computational tractability as

1780
01:53:50,267 --> 01:53:53,675
the mathematical analysis.

1781
01:53:53,750 --> 01:53:58,130
And we need some assumptions

1782
01:53:58,190 --> 01:54:02,280
or some trick to increase the

1783
01:54:02,402 --> 01:54:06,075
tractability. One day I came

1784
01:54:06,137 --> 01:54:10,137
up with an idea that

1785
01:54:14,550 --> 01:54:18,435
in which we consider that both

1786
01:54:18,542 --> 01:54:22,545
new activity and fastest follow the

1787
01:54:22,622 --> 01:54:25,580
same cost function gradient.

1788
01:54:25,715 --> 01:54:29,787
This is very much an allergy with

1789
01:54:30,675 --> 01:54:34,150
physical system like Lagrangian

1790
01:54:34,575 --> 01:54:36,725
information geometry, Hamiltonian

1791
01:54:36,800 --> 01:54:40,575
formation. So usually we consider some

1792
01:54:40,637 --> 01:54:44,337
energy landscape and design

1793
01:54:45,600 --> 01:54:50,090
plausible trajectory

1794
01:54:50,195 --> 01:54:53,615
as the evolution of some principle

1795
01:54:53,720 --> 01:54:58,040
of minimum action

1796
01:54:58,070 --> 01:55:03,335
or restruction. So we imagine

1797
01:55:03,380 --> 01:55:07,137
that what if we applied such idea to

1798
01:55:08,775 --> 01:55:11,640
computational neural network or

1799
01:55:11,657 --> 01:55:13,740
biological neural networks to

1800
01:55:13,907 --> 01:55:18,175
characterize their dynamics

1801
01:55:18,825 --> 01:55:21,325
in the first principle,

1802
01:55:23,475 --> 01:55:27,105
that's the first computational step to

1803
01:55:27,152 --> 01:55:29,575
come up with this framework.

1804
01:55:31,425 --> 01:55:34,935
And finally we noticed that

1805
01:55:35,042 --> 01:55:38,330
it is not easy to connect the Newtonian

1806
01:55:38,390 --> 01:55:40,910
dynamics with this type of neural

1807
01:55:40,955 --> 01:55:44,735
activity study because neural

1808
01:55:44,780 --> 01:55:47,640
activification not necessary to be a

1809
01:55:47,732 --> 01:55:51,305
second order differential

1810
01:55:51,365 --> 01:55:55,137
equation, but rather it is fast order

1811
01:55:55,500 --> 01:55:58,335
and considering many things.

1812
01:55:58,442 --> 01:56:04,287
Then we finally

1813
01:56:06,000 --> 01:56:09,687
use a Costa function

1814
01:56:11,250 --> 01:56:13,975
proposal in the papers,

1815
01:56:16,125 --> 01:56:19,335
which is not necessary to have a

1816
01:56:19,367 --> 01:56:22,910
former identity

1817
01:56:23,030 --> 01:56:26,445
with the so called lavalier in the

1818
01:56:26,597 --> 01:56:29,910
Newtonian physics, but it

1819
01:56:29,942 --> 01:56:34,895
is rather plausible as the rule

1820
01:56:34,985 --> 01:56:39,035
or underlying mechanism

1821
01:56:39,230 --> 01:56:43,675
of such type of network.

1822
01:56:51,800 --> 01:56:54,287
Awesome. Well,

1823
01:56:55,400 --> 01:56:59,080
it has been quite an interesting dot

1824
01:56:59,140 --> 01:57:02,825
one. I really appreciate everything

1825
01:57:02,887 --> 01:57:05,735
you've shared today. Is there anything

1826
01:57:05,767 --> 01:57:07,280
else you want to add at this point?

1827
01:57:07,327 --> 01:57:08,912
Otherwise we'll talk again.

1828
01:57:10,325 --> 01:57:14,720
Yeah, I already

1829
01:57:14,872 --> 01:57:18,485
speak a role. Thank you.

1830
01:57:18,592 --> 01:57:21,450
Alright, talk to you later. Bye.

1831
01:57:22,325 --> 01:57:23,900
Thank you very much for a nice



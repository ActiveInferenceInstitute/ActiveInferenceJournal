33087	181175	A	0.9135634082397001	Hello and welcome everyone. This is ActInf livestream number 51. Two. It's November 9, 2022. Welcome to the the active inference institute. We're a participatory online institute that is communication, learning and practicing applied active inference. This is a recorded and an archived livestream, so please provide us feedback so we can improve our work. All backgrounds and perspectives are welcome and we'll be following video etiquette for live streams, head over active inference.org to learn more about participating in different institute projects. Alright, well, we're in act imp stream number 51.2. We're in our third discussion on the paper. Canonical neural networks perform active inference from 2022. We had a dot zero video with some background and context and overview. And then last week in 51.1 we had a great discussion, went over many interesting details of the paper and related topics. So today we're going to jump in, cover some empirical details, some implications, connect some more dots, maybe look at some code. And thanks again to Kuya for joining these discussions. I'm Daniel, I'm a researcher in California and thought a lot over the last week about what this kind of neural network synthesis or translation really means, and just want to learn more about what fundamentals or foundational aspects of these different kinds of models enable that synthesis or translation. And then again what that means for areas where one or the other kind of model is already in use. So thanks again for joining and I'll pass it to you if you want to say hi or give any a second interoception.
182875	199687	B	0.7425360000000001	Oh yeah, I'm appearing Brain Science Institute, Japan. So I look forward to discuss another different aspect of this work.
203200	256912	A	0.9208823076923071	Well, let's just remind ourselves of the fundamental parallel being made in the paper and then we'll get to these two questions about kind of the two directions that things can go. One representation is in equation one with loss function of a neural network and the free energy on a POMDP. And that's also seen visually in figure one, with a neural network being drawn a concordance against the variational base of the action perception loop. So maybe just let's begin by restating. What is this parallel that is in equation one and figure one and how was it reached in this paper?
259375	353587	B	0.8327465000000001	So basically idea here is that we derived to characterize the dynamics and activity of canonical neural network in terms of Bayesian inference, because arbitrary dynamics of neural network is interoceptive in the sense that we don't know what is the function underlying such a dynamics and what is the coherence of the self organization or activity. So once we translate that dynamics in terms of Bayesian, we can assign quantities in Bayesian for any biological quantities, which enables us to lend the explainability to the neural network dynamics and architectures. So that's a basic idea. And what we have done in this paper is that we consider a biological plausible cost function for this particular canonical neural network. And show the equivalence between that Costa function and the variation navy energy and the particular partially observable cognition process model.
357850	385537	A	0.9299941509433963	Awesome. So let's look at the parallel between the cost function for neural networks and the informational free energy. So one representation of that was in figure three. So maybe could you just describe what is the structure of the informational free energy expression and what is the structure of the loss function?
386950	494525	B	0.8012909937888205	Okay, so there is a clear parallel between the functional structure or those component in informational free energy and component in neural network Costa function. So let's say the first time in F correspond to the it correspond to the expectation about hidden states is a hidden states Australia. So that part basically indicates the free energy with respect to the hidden state. Yeah, and the second part correspond to the free energy about the decision posterior. So the indicates the posterior belief about agent decision or action. And now in terms of the correspondence between the free energy and neural network function here the first time in the neural network function correspond to middle layer neural activity which has a recurrent connection and receive sensory input from sensory layer and then project the output to the output layer and the second term correspond to output layer which receive signals from middle layer and send the feedback response to the environment.
505037	535275	A	0.9019307843137254	So both of us expressions have the first term being more like a cognition perceptual sensory learning term and the second term is more like a control theoretic action selection. And how did you see this analogy or concordance because it looks like a zipper, like everything is totally lined up.
538137	774712	B	0.8184755161290319	Well, this graph itself showed a clear correspondence because now we are considering a particular form of on DP in which each element of hidden states takes either zero or one. But there are many states so it is expressed in a form of factorization. So now we consider that in terms of the s fosteria Bordeaux. Upper part of Bordeaux correspond to the expectation about each element of s taking one and lower part of the bordese correspond to the expectation about s taking zero. So it is broke vector about the posterior expectation and this nicely correspond to the Brea vector shown in the bottom up this figure it is a vector of x and Bijan sorry it is a vector of x and by x and here by x indicate one minus x in the element y sense which is exactly correspond to block vector or expectation. This correspondence also observed in the second tab. Here, log S correspond to log X and also log A correspond to the broke matrix of W log W. Here W hat indicates the sigmoidal function of W and its bar means sigmoidal function of W. So actually, because we now consider binary hidden state and binary observation it's like reviewed mapping. Mapping from hidden states to conversation is expressed as block matrix, which is exactly correspond to broke matrix shown in the bottom of this figure. So like this, for every tab we have the exact correspondence between the upper expression and the lower expression. So that's why we can say that this is a natural mapping from neural network formation to parishional vision formation. So it speaks a sort of identity between those two different expressions. So although 1 may be able to consider another mapping from neural network to Bayesian inference, this is a sort of simplest mapping.
777012	788650	A	0.9265133333333333	So how would it look different if it were three states categorical distribution or a continuous distribution? What aspects would change?
789462	886150	B	0.8550723140495866	Thank you for asking that. So that's in some sense outside of this paper because only when we consider a binary hidden state, this analogy is established nicely. Otherwise we need to consider some attention. So because consider that each neuron code the probability or expectation of some value taking one, then the probability or expectation of taking zero can be simply computed by computing one Ines neural activity. So actually neural activity which is a single dimensional variable is sufficient to express the expectation. Right? But once we consider the three state hidden states program, this doesn't work. So we need to consider at least two variables but it's relation to neural network expression is not very clear in general.
891987	909562	A	0.9063331818181819	That's very interesting why it would be so strong of a concordance in a binary case but immediately unclear for other distributions.
910737	973537	B	0.8397831764705883	Yeah, generally for poem DB expression we consider the one hot expression, one hot vector expression which means that we normalize the value in the sense that the summation of all variable to be one. Maybe there is some neural substrate that achieve that communication. But for classic type of neural network like canonical neural network, consider in this paper what is that neural substrate is not very clear. So that's why we selected the binary case because it's simple and have a clear analogy.
974637	1011537	A	0.9107561904761907	So what does it mean for an artificial or for a biological neuron to have activity dynamics or plasticity context? That justifies it being described as playing like a belief role in a Bayesian setting. Higher firing means more belief, higher firing means lower belief. What does it really mean to have a connection between in this episode of street talk belief states.
1012200	1046037	B	0.8111990909090907	I see, so if you assign a mapping, a particular mapping, then its meaning is also determined. In this case, we assign that neural activity correspond to the posterior expectation about an element of hidden states taking one. So once we define this mapping, then higher neural activity indicates the higher probability of taking one.
1052362	1068125	A	0.8801092857142857	So neural activity is the probability neural activity is on the x axis and the y axis of the sigmoid function is the probability of taking one. Well.
1069837	1120687	B	0.7708125352112675	Neural activity encodes the expectation. So neural activity is sigmoidal function of something itself. This is because once we see a fixed point of neural activity equation which is derived from this cos function, it has a form of sigma WADA function so x equals sigmawilder function of graph, graph, graph. So this form is exactly correspond to softwaremax function of something which is seen in the solution of possibly expectation.
1126600	1136300	A	0.8626599999999999	That'S what the neural activity encodes and what is the Bayesian interpretation or the update rules on the plasticity.
1137775	1264775	B	0.7872069934640521	Okay, that's another important point. So in terms of posture of parameters so in the case of Bayesian force us we consider update about deleted parameters of a matrix and B matrix which is usually expressed by the small case variable. And its meaning is that if we compute the partial derivative of a partial derivative of F with respect to small A then its solution it's fixed point solution looks like an computer product of which is also known as Hebbian product because it has an errors drink to update depending on the precinaptic neuron activity and postsynaptic neuron activity. And according to this formal equivalent we revisit we can see again such analogy in a formal sense here if we computer the partial derivative of neural network function with respect to W then we can formally derive the Hebbian prosthesis which depends on the activity of prey and postsynaptic neuro activity.
1268337	1350337	A	0.9065775221238936	Okay, so hebion plasticity often described as neurons that fired together, wired together. Here you're discussing it in terms of a matrix operation on the POMDP side between observables and hidden states. So there's a hebion plasticity happening between the perceptual layer and the cognitive layer, right? So the first half of the neural network is trained according to heavy and plasticity rules that optimize the A in terms of the perceptual and learning like relationship between hidden states and observables. And then the second half of the neural network has a slightly different structure. It is optimizing based upon retroactive re analysis of consequences of action according to the fictive causality construction.
1353387	1449225	B	0.8288889599999998	So actually in this figure b up layer correspond to environment and lower part correspond to agent. So this structure corresponds to figure eight, this correspond to a simpler version of foam DP. So for version of POMDP, its corresponding neural network is showing figure four or this paper image task. This is the neural network architectures. So as you say, there is a network connection from sensory layer to cognition layer which is expressed by W here and recurrent connection which corresponds to state transient matrix is expressed by K matrix which is recurrent Sinematic connectivity. And as you say the action generation through retrospective reward or risk evolution is done by output trigger through the synaptic connectivity expressed as V in this figure.
1451087	1461150	A	0.9239830000000001	So V is the synaptic connectivity between cognitive states in the middle layer and the action selection states in Y.
1461587	1461947	B	0.66981	Exactly.
1461980	1476612	A	0.9067645238095237	And so in that way V is exactly analogous to W. But why and how does gamma come into play only in this second layer? I mean why not have gamma one in the first layer? Gamma two in the second layer.
1478987	1682487	B	0.8133080246913583	Generally speaking, it is possible to moderate plasticity in first layer using another moderator gamma. But for complexity we focus only on neural modulation in the output layer. Analogy is that for example, as you said, the first rigor computer more perceptual things so perception of external world and instead on the other hand middle secondary which is mapping from cognition layer to action layer perform the optimization of its own action. So for example, in the story item in the brain action prediction is optimized by conversation of dopamine as a input. So usually that socket receives signal from ecological neural socket and send signal to another neuronal nucleus in meat grain. But the point here is that neuron in storatum encodes some decision for examples goal or no goal. So such a decision is encoded. So now we consider analogy between pond DP expression in the Bayesian formation and neural socket in the brain that optimize action through some sort of moderation by another factor. Here that factor corresponds to gamma and gamma has variety of function. But in this paper we focus only on the moderation of activity. So here the behavior activity is not determined by only a preposter relationship but determined by three factors relationship in the sense that the activity is updated by the product of gamma and prayer and postsynaptic activity. So there are three times in one. This is why this comma can moderate prosthesity.
1686150	1700937	A	0.9310687999999999	So how would a glial factor look different computationally? And where in the brain have people identified levels or other factors as relevant for learning?
1702200	1758700	B	0.8419455696202536	Yeah, that's an interesting point. I'm not really sure about the equation of the real moderation of neural activity or plasticity. There are many discussions and I'm sorry, I don't know the exact form, but one possible implementation is similar to this type of neuromodulation. So it would be possible to model some real contribution or free factor to plasticity in the form of three factor learning room which is mathematically speaking Lamme as this type of neural moderation.
1763750	1808112	A	0.9255836363636365	Here in table two we have another set of correspondences. It's like a sideways figure three, right? But a little bit more like a dictionary. Anything to add? Or any variables that we haven't really mentioned. What about the firing thresholds? Because these are common parameters in a neural model, however, we don't really hear about the interpretation of these constructs within the variational base POMDP.
1809587	2050212	B	0.8260480794701988	Yeah, there is an interesting story. That's a very interesting point. So when we first tried to make analogy between neurons network and one program is the law of threshold factor because as you said, it is not absorbing POMDP structure. But there is another factor in Pompey which is prior expectation about hidden state which is usually expressed by D matrix. And what we consider is the relationship between d matrix and firing threshold. And finally, what we found is that firing threshold is not equal to the matrix itself, but it is a summation of B matrix under some function of synaptic strength or which is equal to a matrix b matrix in the POMDP formation. In other words, what we found is that each which is a firing solution in neural network architecture, it is actually an adaptive threshold which is not a fixed value, but h is a function of W sinatic strengths and h changes depending on W's value. For example, if W is too Laje, then your activity can be unstable. So h behavior to reduce the activity to make neural activity more stable. So we can see an analogy of omeostatic mechanism here. If we design A as the function of w and function of another factor which is all part of the term in this table, then we could make common analogy between this h and some variable in palm DP correlation which is shown in the right hand side of this table. Although its value is not simple because it chaos three different tasks. So all of them contribute to make h or M. But anyway, once we map, so once we establish a mapping between h and this value, then everything works. So the cost function in different settings have Omar correspondence.
2054462	2057562	A	0.7649266666666666	H and the M firing thresholds.
2057987	2098737	B	0.7849763999999999	So H correspond to middle rare M indicator output raider threshold which are different variables. And interestingly h correspond to prior expectation about hidden states because it corresponds to community rare and M correspond a priori belief about its own action because it is a bias in the action layer.
2100437	2216562	A	0.8965637055837569	Yeah, it's very interesting that the perceptual firing threshold h only includes prior beliefs on hidden states, beliefs about how observations map to hidden states A and beliefs about how hidden states change their time B. So that's like pure passive inference. And then the firing thresholds for M correspond to only beliefs about preferences and beliefs about actions or habits with C and E. So there's like a complete division of labor or partitioning functionally between these structurally different parts of the neural network and structurally different and functionally different parts of the POMDP. Yet they're integrated in unified loss functions or unified imperatives. And so it's like there's extreme separability of perception and action on both sides of the figure one divide, but also they're integrated, but they're separate. And that's what kind of grants it the best of both worlds because if they were any more integrated you couldn't really pull them apart. And if they were any less integrated then the imperative, the loss function or the variational free energy would be ad hoc and unprincipled. But there's kind of a middle ground where they have a principled integration but still a distinguishment.
2218862	2281212	B	0.8201339393939397	Right? This is caused by network structure defined or it is because the structure of Bayesian network defined in the MDV model. So both of them define the causal relationship between elements or quantities. Its substrate is not important, so it's relationship is crucial to determine the cost function or it's a fixed point in this context. So that's why we can see the data analogy.
2284525	2336087	A	0.9167206451612905	Well, there's a few technical points I think we can now go into and then there will be some more general points about applications and intelligence. So first the code availability statement. Awesome to see that the MATLAB scripts are available and also active on Zinodo. So here is the GitHub repo for reverse engineering. Do you want to give any overview descriptions of what people can expect to see in this repo? And also what about using MATLAB? Why did you use MATLAB? What advantages or limitations do you see in MATLAB?
2338387	2374937	B	0.8524271111111109	So, because this is a very simple simulation, so Macrab is sufficient to encode the whole script. We also try some implication in the material. See here, if you run the script, then you can see the process of an agent solving the maze task.
2379412	2381837	A	0.7756262499999999	What did they do in the maze task?
2382787	2443700	B	0.8328251612903228	So here the aim of this agent is to reach the right hand side of this maze. Because this is a typical example of the rate moderation task. That's why we select the main task. So to achieve this next task, is it required to make some plan to be able to select a good decision? Because without planning, you may encounter the war and cannot go further and you may fail the image. But with learning, it is possible to see the path to reach the right hand side of this space.
2446012	2449112	A	0.8494666666666667	So does it know its exploration?
2453137	2492900	B	0.8206887719298247	Yeah, it received a state from neighboring eleven times eleven Jelle. So which is shown in the bottom part. Yeah, this left figure C indicates the observation. So eleven times eleven state around the agent position. Okay. Now agent is on the right hand side of image at the goal position and it observes our neighboring state.
2496937	2503250	A	0.9704983333333334	Well, a few interesting things here. It's looking off the right end.
2503762	2504392	B	0.51	Yeah.
2504515	2512325	A	0.9151015384615385	And it has this kind of periodic belief in the key distribution. Why?
2515087	2582762	B	0.8351720253164561	I think it is because when the agent is in the middle point of maze, then all neighboring state is in the maze. So there is a path and there is a wall. So this makes some ergodicity because mazes have some structures and actually have a periodic structure and only at the goal position, then right hand side becomes war. But it is not common for this agent. This is because this agent show such a priori pattern.
2584237	2601062	A	0.9532114285714286	Yeah, the streets are one wide and they tend to be separated by one. So we see this periodicity. What is the numbers in this middle bottom plot and what does the checker board represent?
2601487	2659637	B	0.784743181818182	Yeah, hered correspond to possibility, expectation about active states and decision. So middle indicate decision posterior and decision. Here we characterize decision as a secret of four step actions. So each action correspond to a movement to right or left or up or down. And we consider a four step sequence of that option which is expressed as D. So it has four power, four possibility.
2661337	2662925	A	0.75	256.
2663812	2704637	B	0.815848163265306	Yeah, 256. So this is a protein on XY coordinate because in the middle panel, middle point correspond to the current position of agent. And with four step movement, agent can go one of any current position and the current brightness corresponds to the expectation about the agent decision.
2705662	2743550	A	0.9196393902439026	Well this is very interesting. If we just were to think about you're at a point and you can go up, down, left, right, you have four moves. Naively it sounds like, well it should look like a gaussian blur. Most of those should cancel out and then it should become rarer and rarer monotonically. But actually you start in the middle, you can't end up on these white squares because it's like one, two, three and then you have to leave.
2744737	2745500	B	0.57	Right?
2745862	2883325	A	0.9199569635627526	So it's kind of like horses in chess or other pieces where actually their embodiment, it's very unexpected that you can't in four moves end up next to where you began when you can be so much further. And then we see this kind of like embodied inferential prior with QS that embodies regularity beliefs about the width of the road and the separation of the roads. And then there's these like embodied action priors and real consequences that have to do with the structure of movement. So what it's doing? It's thinking about policies of length four. There's 256 policies of length four. There's some degeneracy because there's obviously not 256 squares here. So while only one policy is going to take you up, up, down, down, other squares are reachable. Like the center square is probably the mode because it can be reached at least a handful of ways. And then at each time point it's basically saying okay, I know where my X position is and given my local eleven by eleven view, I'm trying to plan to go right. And then here through time in the simulation here it starts at 30 something, it quickly figures out how to get to about 40 and then it's kind of going up and down on 40. But it can't really break out because all of these bottom four routes are closed. It has a breakout and then very quickly it hits another plateau around 60.
2884212	2884975	B	0.41	Right.
2885562	2902987	A	0.9420445000000003	Then it kind of has a very nice breakout and in just a few steps goes very far. So what is dopamine doing? Or how is Dopamine helping it in the plateau and then to break out of the plateau?
2903562	3050512	B	0.8126786315789473	Yes. So this agent learned this particular structure through many trials. So before training it failed to reach the goal, but after training it achieved such a nice behavior. So to active this, the role of domain is that we design gamma function such that if the agent can move rightward with some distance during some time limit, then risk becomes small like say comma equals zero nor risk situation in that sense. In that case, this agent updates synaptic weights through hebion frostbust. But if the agent failed to go rightward with some distance during a limited time frame, then gamma becomes large like zero six which is larger than the average zero five. Then we design that drawing in attention antihebian prosthesis occurred instead of Hebbian. So antihabion indicates the works as the disassociation between the current state and current decisions. Because the current decision does work, it's not good decision. So we try to make the agent who will get that particular decision rules through conversation of heavy and plasticity done by Dharma factory. So this can be an arrow to the Dopamine moderation heavy and activity.
3051912	3086287	A	0.9424013846153844	So if the policy is resulting in the expected outcome, gamma stays at .5, the policy is as risky or consequential as expected, and then the policy can either go better than expected, which facilitates learning to support that decision. To be made more or the outcome of the policy can be worse than expected, which disassociates previous conceptions to discourage that kind of behavior.
3086937	3129112	B	0.8289170491803278	Exactly. Crucial point is that this association with different time frame in the sense that we consider multiplication of current risk and least decisions to average over past two present Hebbian product. This makes an association between past decision secrets and the current risk which enables to optimize decision to minimize the future risk. It is just a safe time frame.
3131562	3172612	A	0.9247383333333332	So here risk is being used in a formal sense similar to how it's used in economics which is the associated uncertainty of outcomes with respect to a policy. Where does danger come into play? Like what if there was an adversary in the maze or something that was dangerous? How does this kind of model accommodate or hunger or different kinds of competitions? Because right now it's basically just trying to diffuse right word with a bias.
3173187	3173950	B	0.63	Right?
3174537	3183850	A	0.921828823529412	But how do different kind of situational elements become interface into the generative model and generative process?
3184812	3241537	B	0.8073270833333335	Okay, any of those factors can be involved in risk factor, a single risk factor. So you can arbitrary design and risk factor because risk factor moderate generative model. So that's why agent try to minimize the risk through basic embryo updating. But the risk itself is in some sense outside of such a Bayesian framework. So we can design arbitrary risk. So it may involve some danger factor, any other factor.
3243762	3260950	A	0.8827649999999999	And this simulation, it is a POMDP or it is a neural network. And what scripts might we look at to understand the structure of the maze agents?
3262212	3366000	B	0.7822199999999999	Okay, it is basically expressed using the quantity in home depp for tractability. But for example, if you see the MDP learning probably okay, there is a variable Lamme type in the definition of SIM type correspond to the type of simulation. So if it's one or two it becomes homo DP or neural network to my understanding. Well, in this particular example, Jelle, we use the let's say maybe it's not good example that DeForest is learning the deforesting this script as well. So maybe another as an example, let's see.
3369287	3375467	A	0.7330699999999999	What is MDP init is initiating the markup decision process.
3375665	3532800	B	0.8051625882352945	Exactly. It's just determining the initial state of the computer generative model fe compute variational free energy or risk MDP computer risk function. So basically we use the neural network structure computation in this particular setup. So when you click maze m then in the line 31 line we determine that Lamme type is two. This correspond to neural network architecture. So there is a very slight difference between home depicture and neural network architecture because assuming neural network architecture correspond to, you know, considering considering okay, well, if you choose the palm DP architectures, then we sometimes use the gamma function to computer the posterior expectation about parameters. But in the neural network modeling the gamma function doesn't appear but it is replaced with the logarithm of some function. And simply speaking, the difference between the gamma function of something and the logarithm function of something is asymmetry Lamme. So that's why we can transform home DP two neural network architectures. When the number of samples is sufficiently large.
3537287	3545325	A	0.950452142857143	Which form do you expect performs better under small or large amounts of data?
3547487	3595362	B	0.7867671666666667	Well, for large amount of data they work in the same manner. Same manner. For small amount of examples, I'm not truly sure but it corresponds to assumptions about the posterior belief distribution. So if you assume delicious distribution then your resulting function form is something that used the gamma function in terms of basic inference probably which is optimal.
3600512	3665800	A	0.9142317977528088	All right, let's return to the earliest questions from today. So in your script, which people can reference, there's basically a toggle between having it in SIM type one or SIM type two corresponding to the POMDP in the neural network. What about if there's a published neural network or POMDP? How can we use this architecture to create a translation? Is there any difference in this? Kind of like translating models in the wild different than the full construction of a special script that can speak both languages?
3670500	3719650	B	0.8422425396825397	Well, in terms of script there's no difference, sympathetic difference. Right. So they work in the same manner. So only a translation of variable the same source code in two different ways. So if you see that this is a neural network generation, then it is translated as a neural network. Or if you see that this is a POMDP, then it's POMDP.
3723075	3739900	A	0.9175590322580642	So for some neural network being used in an industrial setting, how would we get from the neural network to a POMDP? And where or how would that representation be valuable?
3740925	3807850	B	0.8131580000000002	Right? So when neural network in the different architectures the important point is that we consider a particular form of neural network which is called canonical neural network architecture. So only when we assume this crossover neural network then you can find the exact correspondence to a particular form of POMDP. Otherwise you need to establish another equivalent between another form of neural network architectures and some sort of Bayesian model. This may be expressed by POMDP, but maybe not so straightforward to be expressed as the computational AP architectures.
3809175	3818637	A	0.8284499999999998	So what is it about the canonical neural network architecture that facilitates its translation into the POMDP form?
3819225	3876175	B	0.8008314102564105	Yeah, first of all, it assumes sigmoid or activation function. It is nicely correspond to enthralpy time in the force DP equations from DP formulations. So that's why we can clear marketing. So yeah, in other words, simply speaking, they have the same nonlinearity. That's why this translation is very easy with another nonlinearity or neural network equation, then we need to find another type of entropy equation or another type of prior distribution. It is very nontrivial.
3878175	3882912	A	0.9752888888888889	How does one even go about doing that research?
3889575	3919137	B	0.8492376315789475	If you want to go that direction, then I think the first step is to find the prior brief, which makes the prior brief and find the equivalence between a particular neural network architectures and particular Bayesian model.
3925312	4010387	A	0.9121092366412209	This sigmoidal activation is interesting. It corresponds to general patterns seen in psychophysics, like two objects that are the same weight. You're going to have a chance of saying that one is heavier and then initially the difference has the most returns on that decision being made accurately. And then as it crosses some threshold where it just is beyond a noticeable difference, the decision becomes essentially probabilistic, like the firing curve becomes saturated, the neuron chaos, a very low belief about zero or very high belief about zero or one flip that. So there is a nice grounding of that kind of a sigmoidal response curve with respect to stimuli differences and it has of course, tractable analytical properties, but it also just happens to be a good response summarizer.
4011262	4076675	B	0.8407391954022987	Yeah, you're right. So sigmoidar function is also known as a psychometric function, as you say. We observe that characteristic in many psychical experiments. And the previous work also said that even at the single neuron level, neuronal level, the same behavior were absorbed, which means that each heuristic we can reobserve the similar property, which is sometimes called as neurometric function, which is which have the form of sigmoida activation function. So it is nice reason to design neural network architecture using a sigmoid or function.
4078837	4112925	A	0.9130832692307693	All right, let's cover a few questions in the chat from Dave and then in the end turn to some general thoughts. Okay, this was when we were looking at figure three. So you described these vectors or matrices. What kind of matrix or vector did you describe? The mass block matrix.
4114625	4116000	B	0.8161499999999999	Block matrix.
4118900	4123712	A	0.8278725	Block rock learning what?
4124075	4134587	B	0.7810294117647059	Okay, rock matrix of rock. Vector is a vector vector or matrix of matrix. So imagine that.
4142900	4147112	A	0.92964	Sorry, just zoom, just glitch just repeat the last piece.
4147550	4184550	B	0.8156053846153843	Okay, well, broke matrix Dean that the element of matrix is a matrix. So let's say two by two matrix like matrix in the ear pointing. So this here W one hat is a matrix and W zero hat is another matrix. And combining four matrices, we define a single block matrix.
4186625	4221262	A	0.9051188333333332	All right, thank you. So Dave then asks the hosts of Machine Learning Street Talk Number 67 with Karl Friston. Another podcast, Karl Friston has spoken. Pressed Karl Friston on why is it so important that most of the values in a generative model matrix assume values of exactly zero? Why is it important that generative model matrices are sparse?
4222962	4253200	B	0.7994895833333331	Why? I'm not Bull sure. I think there is some context before that point. I think on that particular situation, then, yeah, as you say, the many elements in the matrix or gentle model should be zero, but I'm not sure if it's a general statement at all.
4254312	4282350	A	0.8838205882352942	What do you think about compressed analyses on sparse matrices? Is that a useful technique or direction?
4285850	4343625	B	0.8454243373493974	You can use that knowledge to construct model, so you can use that knowledge to make more accurate inference. So in that sense, generally speaking, such assumptions should be useful. For example, yeah, as you said, it would be possible to use some sparse prior to restrict the value of parameter. Like, it is in principle same as assuming some L one norm to design the distribution. To design the prior distribution, which is mass Dutch speaking. Exactly same as considering Lasso regression.
4345550	4391625	A	0.9126548437500002	Yes. So we've explored a little bit how from a canonical neural network to a particular form of a POMDP, gives us some semantics and interpretability around the dynamics and plasticity of the neural network. What do we gain by taking a stated POMDP generative model and deriving an analogous neural network? Do we gain access to efficient computation, new software packages, different applications?
4395462	4442457	B	0.7855329629629629	Well, if one use Home DP and one's goal is so design an efficient basic model, then I think your Home DP expression is sufficient. So you don't need to consider neural network architecture, probably because Homedippy architecture and Bayesian correlation is designed to achieve some sort of mathematically optimal inference and decision making.
4442535	4442817	A	0.58	Right?
4442865	4600750	B	0.8030123696682462	So it itself is optimal scheme. But if one need to consider a link between Bayesian inference and biological substrates, then this mapping is crucial. Simply speaking, we consider that we assume that a brain perform Beijing inference, but its substrate is still unclear. So we need to link the Bayesian quantity to biological quantities. So this mapping, this equivalent, helps us to its translation. Right? So when you start from on the model, then this translation facilitates the process of finding its neuronal substrate. So once you translate that to a basic to neural network quantities, then it facilitates the experimental validation application to reality. So if its modeling is apt for a particular neural network neural circuit architecture, then it should provide some prediction about the architecture or dynamics of the empirical data. Right? So first we start from Bayesian model, which is not necessary to be equal to empirical data. So there is some mapping, but it's mapping is not straightforward. We may have multiple mappings, but once you translate Bayesian model to a particular neural network architectures then mapping or relationship between applicable data to such a particular neural network model is straightforward. So it helps us to apply base to an explanation of MP card data.
4606737	4711937	A	0.9292928571428561	So is it fair to say that neural networks have found wide recent application because they facilitate statistical learning in cases where the inference problem has not been a priori well specified? One can just have a folder of images and a list of labels and just say, here's the data. Run it through this architecture or this architecture Explorer. And so with this concordance we gain new interpretation into those settings that kind of arose from ill specified inference problems. And then on the other hand, for problems that we already have well specified in terms of a POMDP generative model of a particular form, we gain the connection to actually implement it with empirical data and bring it into relevant industrial settings. What systems or phenomena are promising to continue research on the image example obviously is a simple case, but are you continuing research into more advanced computational agents? Robotic animal.
4716200	4887412	B	0.778574054054054	Oh, hello can design a more sophisticated agent which performs some difficult tasks based on canonical network neural network but there is some limitation, clear limitation on that direction. Right? So yeah, I should emphasize that across the canonical neural network which correspond to a particular home DP is much smaller than general home DP framework. So there are some limitations, a least of limitations. So if one's goal is designed and sophisticated DAGs to perform some task or control robot, then one direction is just forget such limitations and take the mathematical optimality right and another direction is barrelscale probability. So if one wants to image some agent which is barely possible, then this correspondence is crucial because it tells us biological limitations through the existence, no existence of such a mapping between POMDP and particular neural network architectures. So, yeah. So it would be useful to consider a vertical substrate to achieve exafferent task. And that task would be related to high dimension image processing. Image recognition or sound recognition, such as multimodality, can be inbox or decision. Can be higher dimensional like in the mainstays, we just consider the four directional movement, but it can be extended to higher dimensionality, like arm movement, body movement, so on, so on. So in principle it can be extended in that direction.
4890312	4906162	A	0.9482240909090912	Which directions or questions are you excited about? Or what areas of studying the basis of biological and computational intelligence are relevant?
4907112	4973950	B	0.8292010000000003	Yes. So in terms of the importance of canonical network, as you said, virtue is a biological probability. So it would be nice that if we model some task which is conducted by learning and one already recorded some neural activity, then we design a task which is exactly same as the task which is done by the animal and then compare the simulated agent and Mpcar data to discuss about the similarity or difference between the simulated agent and the animals. That would be very interesting direction of research.
4976862	5055787	A	0.9087189843749998	Yeah, and if there could be some unexpected prediction or explanation in the computational agent, that would bolster the relationship. And then one other aspect is it would help with the reproducibility and the documentation around behavior studies if the computational agent were preregistered and someone said we've already done the statistical power analysis and we've already explored with parameter sweeps how many observations we need to make of the two armed bandit. How many observations of the three armed bandit should we do? Three mice 100 times or 100 mice three times? Those are the total substance of designing research programs. And so having a formal representation of behavioral tasks that are being studied will help us design behavior observations and experiments that aren't simply ad hoc.
5060037	5224537	B	0.8367354852320676	All right, that's an interesting application. This framework helps to design the experimental setup itself. And what we often consider is the prediction ability of these modern canonical neural networks to predict the Jelle phoneization or dynamics of the VR neural network in the animal during the learning process. So in place for it is possible to predict the behavior after learning based only on data in the initial stage because once we obtain some empirical data, then we can fit that data to design a particular canonical network. And canonical neural network makes some correlation through a minimization cost function which is exactly same as the Bayesian belief updating under a particular guarantee model. So which means that its dynamics goes through the shortest path on the free energy landscape which means that we can make some quantitative prediction about the sinatic trajectory or neural activity or any kind of parameters. So we demonstrated that using in virtual neural network and uploaded some footprint recently. So at the stage, at least at the level in vitro network, which is much simpler than VR brain, we could predict the self organization of in virtual network using this canonical network architecture and this support the probability of free energy principle because this canonical network predict the communication through the variational free energy minimization and its solution. Its results have a very tight correlation between correlation to Archer synchronization.
5227962	5239387	A	0.8829400000000001	That's a very interesting experiment. So what animal were the neurons from and what was measured about these neurons?
5239737	5311537	B	0.781859090909091	Yes, so that in vitro network is obtained from blood embryo, we use cortical cells to make that individual network and task is sort of causal inference task which can be designed in the form of OMDP. So imagine that we usually simulate agents that receive signals generated by OMDP generative process and process and Beijing task. So we just replaced that Bayesian agent to neural network. So we stimulate neuron with some signal which is made by some hidden sources through like a human mapping and question is whether in viral network can info the hidden states through some communication and they can they could Ines the hidden culture.
5315112	5323922	A	0.8988017647058825	What does it look like functionally when the neural network has succeeded at inferring the hidden causes?
5324117	5458987	B	0.8530673604060915	Yeah, the direct conversation is done by the response number response spikes to a particular pattern sensory input. So again, we can see a clear correspondence between activity level and posterior belief about hidden state. So here we see re a book response to an electropaste memory. We see the response from ten to 13 milliseconds after each stimulation and we compute the number of spikes and that spikes changes their preference in the sense that some neurons learn to preferentially respond to force one but not source two. So which is not a response to input itself, but it looks like a response to particular source. So it is inference which is empirical evidence that neural network actually perform some sort of causal inference in a manner consistent with traditional Bayesian inference. And then we compute another quantity in Bayesian inference in the real vertical data. We show that firing special factor is consistent with the prior belief about hidden states and we also compute the synaptic rate statistically through some connection strength estimation method and show that the estimated synaptic strength is consistent with something encoding posterior belief about parameters, as expected by the theory.
5460612	5496250	A	0.9126126315789473	Well, we looked at table two earlier and this is almost like the next step after the theoretical concordance is all right, well, let's measure the release of a neurotransmitter or the empirical synaptic strength or the firing threshold or all these different features in different empirical systems. So what experimental systems does your group work in?
5502012	5565700	B	0.8160354736842105	That in virtual system was made when I was a PhD student. So that is experiment we done in my previous route and now I COVID to the Bijan Institute and I'm a Princepal investigator of Celery unit. So now actually we don't use any experimental setup, so any experimental bidding is down with some collaboration. So although I cannot say detail about that exploration. But yeah, we learn some collaborating work about the implication of salary using various animals. Yeah, so we hope we can show some interesting results following results using animal data.
5569662	5647300	A	0.9255466972477058	Very interesting. Yeah, well, it speaks a lot to the stage that our field is in in certain ways where we've seen a lot of graphics that are suggestive. This paper and the building on the previous 2020 paper made a suggestive possibility much closer to an analytically demonstrated translation and then took the next step incrementally into the in silico agent. And so it's only natural to then explore different embodied systems as well. Are there any other sections that you wanted to like, look at or highlight or any other topics about the paper or adjacencies or active inference that you think are interesting to go into?
5650212	5768062	B	0.832317724550898	Yeah, okay, I would like to mention about some implication of these papers, which is not the director discussing the papers. So for example, well, we focus on a discrete state space model. So we avoid to assume some subscript that encodes the coherence of the distribution. So once you assume homedp then it is categorical distinctions. So it is different from assuming Gaussian distribution characterized by me and variants. So the neural substrate of variance is still unclear and we now try to figure out that. So this is one direction of limitation and another implication is that thanks to a simple ODP structure in this paper, we don't care about the hierarchical optimization. But generally it is crucial to update parameters through hierarchical optimization through some backpropagation like computation. Although it is unclear whether back propagation itself occurred in the real brain. But we still have some alternative that achieve such optimization and it's neural substrate still unclear and this paper doesn't address that direction.
5785650	6108175	A	0.9213002671755727	Another area I'm wondering about is like where in neural structures is the learning reflected? Where is the function and learning reflected? Well, sometimes it has to do with not just structural Tweaking, but the presence or absence of synapses. So obviously this model does not expand into synaptogenesis synaptic pruning, let alone neurogenesis and neuro allostasis which we mentioned in the previous discussion. But understanding how these larger scale structural changes which are certainly important in biological systems become reflected in artificial neural networks and then how that translates all the way back to P-O-M. DP and then whether we could go the other way. What kinds of POMDP structures in their neural network realization would have structural modification. Like you COVID imagine a POMDP that does structure learning but the neural network doesn't have structural change. Or there's A-P-O-M DP that doesn't do structural learning but it's manifested by a neural network that does have a structural change element. So structure is doing something very different in these two different categories of model. And then also even within neural firing, which is different amongst different species and so on, there's different aspects of what that firing is that would have different implications for the actual biological substrate of cognition niche. The simple connection is firing rate to posterior belief. Average firing rate, no change in posterior. Reduce the firing rate if the posterior should be going down. Increase it if it should be going up. Or maybe there are neurons that have a flipped valence but the same type of relationship, but there's other firing patterns like spike time dependent plasticity synchronization amongst different brain regions. There's a lot of things that don't change the rate overall. That again from the biological systems we know that those phenomena and mechanism are important for different cognitive processes. So there will be many many years of a fruitful relationship. I'm going to bring in this picture that Alexandra had taken. Maybe we need a third panel in figure one because these three systems moving between them is going to be the substance of the field for a long time and there may be other edges to build. But understanding how artificial neural networks intermediate between the empirical measurements and manipulations that we can make of real neural systems and the interpretability and factorizability of POMDPs it might be a bridge too far to go from the POMDP to the neuron. You could always use this technique but it would be a purely descriptive statistic type approach. But it's so interesting that by intermediating through a formal connection established in figure one I Dean in equation one but also shown here, then we can kind of extend the chain of exploration, prediction, control, design all the way on through. And that just unlocks like an incredible amount of neuroscience that hasn't been formalized mathematically and an incredible amount of generative models that have been specified for different learning settings sometimes even by analogy to biological settings. But the metaphor remains just a metaphor until it's possible to intermediate with this type of neural network development.
6109125	6210850	B	0.8560064285714282	Yeah, that's a crucial point. It is easy to imagine that phenomena can be modeled using very realistic neural model or free model synaptic model. Right. We believe that it is possible and then Laje model is not necessarily tractable, not necessarily useful because it's too much complicated to analyze something. So we use some reduction, usually mathematically speaking, which correspond to topological transformation to make the model simpler. And then we need to consider the translation of that simplified neural network model because neural network model itself is not explainable which just represents some dynamics and its functional meaning is not clear. But thanks to the Bayesian framework we have a very nice event framework to least the experiment ability. And this translation, this correspondence helped us to link such a phenomena base equation modeling and functional base equations.
6212250	6433750	A	0.8996584880636596	Yeah, one paper from 2017 that was much discussed by some could a neuroscientist understand a microprocessor? And this group with Jonas and Courting, they had a simulation of a microprocessor from an earlier video game console, I believe. And then using the analogy of like the transistors and their connections as neural firing and structural connectivity they were able to simulate experimental settings, input and action and then make measurements from every neuron including doing lesions and loss of functions and so on. And it turns out that a lot of the techniques that are used to derive scientific explanation from analogous data collected from a biological system, those techniques which ostensibly should be isolating functional explanations in fact did not isolate effective exploration. You could have a deletion over here that induces some statistical change all the way over here and that may or may not be a useful clue towards the function of even subcircuits. And so I think that was a wake up call with respect to the interpretability of simply this connection between the biological and the neural network. This connection alone is of limited applicability even when the neural network model becomes so complex as to recapitulate the biological phenomena, you're never under any guarantee that you're going to recover interpretability. You may have just created an atomic level simulation of the phenomena, but of course, a map that is the same scale as the phenomena isn't a map. It's just a copy that has no more interpretability. And it's almost like what is now extended again, as we kind of just summarize this and think about how we move forward, is that connection can now be extended into the space of interpretable causal models and the generalized Bayesian graphical computational frameworks and all the heuristic that we can then use like variational, Bayes and all these other methods. So it'd be interesting to look back at different data sets of in vitro and in vivo and in silico neural activation, especially if the task was of this constrained set of POMDPs and it was already amenable. Because, as you brought up, other settings would require a little bit more theory development before we understand what POMDP family would be applicable.
6439650	6440280	B	0.98211	Cool.
6440402	6444837	A	0.9671733333333334	Well, do you have any final thoughts or questions?
6447300	6450625	B	0.661	Well, do you hop.
6454912	6598262	A	0.9107256018518526	I kant to download the MATLAB scripts and generate the figures, play around with a few of these parameters? Like, I see that you can change how far the entity can see. And then with these models, I'm also always curious about the computational complexity. Like if you extended the planning horizon from four to five or you dropped it down to three, what is the runtime consequences and what is the performance consequences? And where might we be able to use single or swarms of really simple agent, maybe even making binary decisions and achieve high performance? And where do we really need to move into these large combinatoric spaces in order to solve problems and the kinds of complex planning problems that we solve, whether it's planning our day or planning our week, are those force like true Deep Harrison planning problems with extensive consideration of counterfactuals and calculation of alternatives? Or are those actually composite decisions that are made up of smaller, simpler, sub decisions that we may or may not have flexibility to restructure? So that a decision, a complex chess maneuver, a sacrifice in chess or another game. It may be possible to model that as a Deep Horizon scan or a kind of intuitive heuristic for an appropriate skilled entity.
6601537	6636587	B	0.8115049999999999	For this particular two Brea structure, there is a clear limitation about the horizon forward because it doesn't use forward prediction, it used post action approach. So it's a clear limitation, but still, it may have some performance ability, active, some levels of affordance.
6638587	6700287	A	0.9128338181818184	That provides even another way to look at planning. The two ways I was describing planning, as it's often described in the PMDP literature is again, is it a true Deep Horizon consideration or is it just short term heuristics or nested models that are short? And I think that this paper says maybe neither. Maybe it's purely the active causality on the past that leads to the emergence of sentient and maybe even teleological planninglike behavior through the ongoing reconsideration of the consequences of least action. But it's neither a short nor a long term planning challenge. It's actually like a memory and learning challenge. And no planning occurs.
6700437	6708587	B	0.5900200000000001	Right. Indirect planning planning element is involving C matrix.
6710212	6771725	A	0.9253067326732675	Planning as a phenomenon occurs and derisking through time occurs. But it says something quite interesting and deep that that phenotype or function could be enacted by a system that explicitly looks a long way ahead, explicitly looks a short way ahead or moves forward and looks backwards only, right, which is what they sometimes say about the past and the future. So that may be a very biologically plausible form of learning. And it's already intimately connected with the dynamics and the activity in terms of an integrated loss function. So these are all excellent directions to keep learning on.
6772837	6801697	B	0.7782393023255811	Right? And I'm also interested in the barrel implementation of such a short term or long term for the prediction and running. And I hope to find some nice connectivity to such implementation of Bayesian motor and implementation in VR brain network. Cool.
6801880	6881162	A	0.9022334645669291	And also I'm always curious about the invertebrate brain as an ant researcher. And so many of the brain architectures as well as the brain architecture that people discuss are mammalian centric, which makes sense. The mammalian cortical column and the relationship with Dopaminergic midbrain and the cortical regions and the spinal reflex arc those are all important systems of interest. Yet the micro and meso anatomy of the invertebrate nervous system is pretty distinct. So our model should be able to describe neural and cognitive systems, of course, across invertebrates and vertebrates. So I look forward to also seeing what those models of the invertebrate nervous system and collective behavior where you could have some type of backwards looking risk inference of the swarm. Who knows?
6883537	6884300	B	0.71772	Well.
6886537	6899462	A	0.9454489655172412	We really appreciate the time that you took for these discussions. I think they are immensely important. And we wish you the best of luck in these continued directions.
6903387	6904150	B	0.08	Yes.
6906612	6908875	A	0.9755100000000001	Okay, that's it. Thank you.
6909687	6910962	B	0.99927	Thank you very much.
6911100	6912712	A	0.961222	See you next time. Bye.

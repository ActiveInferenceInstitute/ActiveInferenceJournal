Active Inference BookStream 002.02 ~ Parr, Pezzulo, Friston ~ Chapters 4, 5, 7, 8
	https://www.youtube.com/watch?v=3_5pTCguAv4

Active Inference Institute
	https://www.youtube.com/@ActiveInference

1.74K subscribers
77 views  

Active Inference: The Free Energy Principle in Mind, Brain, and Behavior
By Thomas Parr, Giovanni Pezzulo and Karl J. Friston
https://mitpress.mit.edu/9780262045353/active-inference/

Time	Text
0:10	2023 and we're in active inference textbook group slash bookstream
0:17	2.02 thanks Ali for joining so what we're going to do today is give a short
0:24	overview of the chapters from the par at all 2022 book
0:30	we're going to do chapters four five seven and eight
0:37	and we're just going to pause between them because then we'll clip them into
0:42	the shorter videos append that to the playlist just so there's a first video
0:47	overview of each of the chapters and this is the second in that work
0:53	all right so we'll do chapter four
0:59	um we'll just wait a few seconds and then start chapter four
1:05	okay chapter four is called the generative models of active inference
1:11	and it begins with a quotation everything should be made as simple as possible but not simpler by Albert
1:16	Einstein Ollie what is your overview thought or warning for chapter four
1:25	okay so uh after the preliminary materials um in chapters two and three uh which
1:33	was basically largely based on um concept providing some conceptual
1:39	framework for developing uh the further Theory chapter four delves into much
1:47	more detail in terms of mathematical formulation and it unpacks a lot more uh
1:55	the way that the central equations of active inference is derived and how to
2:02	construct the important elements of active inference models uh so say
2:10	matrices a b c and d and also how to put together generative models in different
2:18	situations so it basically lays out the foundation or
2:25	um for I mean constructing an active inference models uh both uh for discrete
2:32	time situations and continuous time at once uh which will be used later in
2:39	chapters seven and eight but this is uh probably one of the most uh challenging
2:47	and uh at least mathematically dense chapters in the book so I would
2:54	personally suggest reading through this chapter really slowly and even if we
3:01	don't get to understand every single detail of the chapter uh obviously we
3:07	can return required as we go through uh the textbook
3:13	thank you Ali yes so let's look through the sections just to add on though
3:20	chapter four is one of the larger and more equation dense chapters because it
3:27	is the common kernel or basis that's then going to get applied in chapter
3:34	five in the neurobiological case there's a recipe for making chapter 4 in
3:39	chapter six that's the recipe for active inference modeling chapter 7 and 8 are
3:45	about the discrete and the continuous time variant or or subtype or motif of
3:51	these kinds of things called generative models so this is the real common root
3:57	and we'll just look at what the sections are
4:02	this chapter complements the preceding chapters conceptual treatment of active inference with a more formal treatment
4:09	section 4.2 from Bayesian inference to free energy what would you say about this section
4:15	Ellie okay so as we know the free energy principle is inspired by previous work
4:26	on Bayesian inference I mean all the way way back to helmholtz theory about
4:34	um unconscious inference or something to that effect I can remember the exact
4:40	term but here I mean it provides in a more in a bit more
4:48	detail how we can derive pre-energy principle uh formalism using the
4:56	established Bayesian inference formulation and particularly one of the key
5:04	uh movements or at least one of the key decisions uh in uh through the
5:10	derivation of free energy principle formulation is using uh Jane's inequality principle to derive uh
5:20	an upper upper bound instead of just using uh the exact values to compute
5:29	um or or to achieve required parameters so uh that's basically uh
5:36	in my opinion the key premise of uh section 4.2
5:42	and to see uh in a bit more detail how
5:47	we can achieve those uh upper bounds using uh Jane's inequality uh directly
5:54	from by using um I mean manipulations of Bayesian
6:00	inference based in statistical formalism
6:05	thanks I'll just add one point from this section broadly these are the problems
6:11	of inferring states of the world perception and inferring a course of action planning so this is again
6:16	referring to the perception and action and everything that happens in between is the internal or the cognitive part of
6:24	the inference but this is like the blanket State cybernetic input output and then
6:30	let's look at the first equation or how much equations overall or what equations
6:36	do you think we should highlight
6:42	uh okay so um maybe we can uh I mean uh just uh as
6:49	a general comment about these um different equations uh well each of
6:55	these equations provide a distinct step toward deriving uh the ultimate whole
7:02	picture so uh even if we don't quite
7:08	understand how we can derive from uh each step to the other one
7:14	it's good to know that it's only required to understand how we
7:22	get to that ultimate whole picture but ultimately what we would need in order
7:29	to develop um for a develop active inference models is the ultimate equation or ultimate
7:37	whole picture so this is just um a way to elucidate uh the steps toward
7:45	developing uh that whole picture but again it's not an essential requirement
7:53	to understand um the materials of the rest of the book
7:58	uh but if we go from uh I mean equations 4.1 toward the 4.4 or other words the
8:08	pre A variation of free energy well equation 4.1 is just a basic definition
8:14	of um probably some properties of probabilities
8:21	in terms of conditional probability and so on so equation 4.2 provides the
8:29	central chains inequality principle and how it relates to
8:37	um I mean conditional probabilities and uh of course joint probabilities and
8:43	then by using uh those uh two properties
8:48	or those two equations we ultimately get to 4.4 which is the definition of
8:55	variation free energy parameter uh which is the parameter of interest that needs
9:01	to be optimized in order to inference to happen
9:06	or at least perceptual inference to happen in active inference models
9:13	thanks only thing all add is f is the letter used for variational free energy
9:20	you can think of it like a computer program and the arguments that it takes in or the variables that it takes in are
9:26	Q which is the distribution that's under the statisticians control and Y which
9:33	are the data which are outside of the statisticians control and
9:39	do you want to describe more about anything in this equation or carry on
9:45	uh just one thing to uh that can probably be helpful is uh to somehow
9:53	compare these steps with um the initial picture we had from uh
9:59	chapter two uh because uh variation free energy was first introduced in chapter
10:05	two so uh it can be helpful to go back and forth between chapters two and four
10:12	and try to connect the dots between um the related points there
10:24	section 4.3 generative models all right I'll read the first sentence then you can give some thoughts
10:31	to calculate the for the free energy we need three things data a family a
10:37	variational distributions and a generative Model comprising A prior and a likelihood
10:46	in this section we outline two very general sorts of generative model used for active inference and the form the
10:52	free energy takes in relation to each
10:59	but okay so uh as mentioned earlier
11:05	this chapter deals both with discrete time and continuous time situations so
11:12	clearly we would need two different types of generative models for each situation and
11:19	obviously the the generative models or the way to construct generative models
11:25	or discrete time situations uh would vary quite a bit from the one for
11:31	continuous time situations um but the general principle underlying those
11:40	General models are basically the same which is uh I mean to somehow construct
11:47	a model of the environment and I mean either be it
11:54	for the situation that that is sequential in time or for the situations
12:01	that need to be somehow each moment of the situation needs to be
12:07	accommodated in terms of a continuous time situation so uh
12:13	figure 4.2 provides some examples of both so
12:19	for uh yes let me see yes so we have some examples of
12:29	different kinds of uh generative models some uh case case studies if you like
12:37	and it provides various ways to show how
12:42	the the dependencies between between variables can be modeled using
12:48	these kinds of graphical probabilistic models so one common way to represent
12:54	General to models is to use these kinds of graphical probabilistic models in
13:01	active imprints literature which is at least in this case the circles would
13:09	represent the random variables and the squares
13:14	that would represent the distributions which would describe the dependencies between those random
13:22	variables so we can see the clear relationships
13:28	between those parameters here which is basically what this whole graph what
13:35	constitutes the generative model that needs to be used for different
13:41	situations and then in figure 4.3
13:47	we can compare the two different types of generative models based on uh whether it's discrete
13:56	time or continuous time situations so the upper picture is a generative model
14:02	for the discrete time situation and the lower picture
14:07	is the parallel continuous time version of it and as we can see the general
14:16	topology of these models are the same the only things that differ is the use
14:25	of parameters for policies or I mean discrete time
14:31	policies or the continuous time ones and we can obviously compare the different
14:38	elements for for both uh priors States and uh external States internal States
14:46	and so on by comparing these two models here
14:52	yeah we often return to figure 4.3 it's kind of the Rosetta Stone
14:58	of generative modeling for the context of this book because it's then going to
15:03	develop out into chapter seven and eight and it represents a really fundamental decision made in modeling
15:11	and in the later chapters it's also shown how it can be made into a hierarchical
15:17	model that combines aspects of both but within each level of modeling still
15:22	these are the kinds of decisions that modelers are presented with when it comes to statistical modeling overall
15:30	so section 4.4 goes into essentially the top half of figure 4.3 discrete time
15:39	what would you say about discrete time okay so uh the discrete time situation
15:46	uh is uh obviously the archetype discrete time situation which is upon
15:53	the Palm DP models so at this point I
15:58	would very much like to recommend uh following uh the steps following the
16:04	material from step by step paper because uh that's uh in that paper uh the way to
16:13	construct pump DP models is described in a bit more detail so if anyone feels
16:20	like they should learn a bit more about the the gaps in the details uh I I would
16:27	very much like to recommend that particular paper so
16:33	uh yeah uh I don't know how much detail uh we should go into because uh it's I
16:42	mean uh although uh it's not maybe a detailed enough for some tastes but it
16:49	goes in a quite uh extensive detail uh about how we can construct these models
16:56	using the the concepts we've learned uh in previous chapters uh so
17:03	ultimately we reach equations for Point 13 and
17:09	4.14 which are basically the culmination of palm DP formulation using the vector
17:18	notations and gradients and so on so
17:24	uh that's when we go to continuous time situation
17:30	great a few things intervene in the continuous
17:36	time chapter ones that we'll just mention here because they're kind of boxed or
17:43	partitioned from The Continuous time part about their following Pages first is Markov blankets we won't go into it
17:49	here but kind of footnote that or look at some other places where we talk about it outside of this chapter overview
17:56	figure 4.4 AZ message passing again a big topic
18:02	let's kind of just go past it now we're back to the regularly scheduled
18:08	continuous time generative model discussion and then another box to the generalized
18:16	coordinates of motion so taking position plus derivatives of
18:22	precision and that has some beneficial properties that are described and unpacked also
18:29	elsewhere do you want to say anything about 4.5.2
18:38	uh uh well the only thing that comes to mind is
18:45	although as I said before all the formulations here may look uh
18:52	more I mean a bit too dense to understand at the first pass but uh some
18:59	of the key uh maybe components here
19:05	could be uh obviously the the material from box 4.2 and 4.3 uh is I think are
19:16	quite essential to understand uh the uh the underlying principle behind uh
19:23	deriving uh The Continuous time situation because without LaPlace approximation what we would have uh in
19:31	terms of free energy minimization is uh it would look very much like the Gibbs
19:38	it gives free energy so uh I mean the key distinction between uh
19:46	the free energy Principle as described in active inference literature as opposed to Gibbs free energy is this
19:53	distinction uh is the LaPlace approximation so this is what enables us
19:59	to go from Gibbs free energy to um I mean uh the variation of free
20:06	energy so uh yeah that's uh I mean quite essential to uh make this to be familiar
20:13	with this essential approximation and obviously the concept of generalized
20:20	coordinates of motion will come time and time again throughout the whole book
20:25	particularly in chapters uh eight and nine uh so yeah those two concepts I
20:34	believe needs a bit more attention
20:41	so yeah sounds good box 4.3 LaPlace approximation
20:47	equations another message passing representation
20:54	and a summary the key message to take away is that approximate Bayesian inference may be
21:00	framed as minimizing a quantity known as variational free energy this
21:05	depends on the generative model that expresses our belief about how data are generated anything else you want to add
21:15	ah nothing comes to mind at the moment because uh as I said this is I mean
21:23	we're still in the stage that we want to develop our
21:28	essential tools to be used uh in the rest of the book so uh here up until now
21:35	I Believe by the end of chapter four we have acquired all the essential
21:40	necessary mathematical tools and the next chapter chapter 5 is kind of acts
21:49	like an interlude and I don't think it's the direct
21:56	um I mean continuation of chapters uh one through four so I believe the the
22:03	first um section or the first part of the book uh conceptually and mathematically ends
22:10	here so yeah that's it yes it's a little bit like the pragmatic
22:17	modeling part gets foreshadowed or explored in five now that we're all
22:23	built up with four all right that's the end of the overview for four
22:43	okay chapter five is called message passing and neurobiology
22:48	what is your overview thought on chapter five
22:54	okay uh I mean uh it's a kind of I don't know I have
23:00	mixed feelings about this chapter because on one hand uh you see as far as
23:05	I understand uh active inference uh although it originated uh as uh
23:11	quote-unquote a unified theory of the brain uh I don't think uh it's a
23:17	neurobiological theory per se uh of course there can be some correlations
23:25	between a neurobiological uh components or concepts with active inference active
23:32	inference um I mean Concepts but uh I mean it's not uh an essential premise of
23:40	active inference Theory to provide a theory to provide a comprehensive theory
23:47	about how the neurobiology of human brain or other organisms brain
23:55	behave at it's a detailed and a neuro anatomical
24:02	um level but at the uh then again it's nice to have
24:09	these kinds of empirical correlations uh between the findings of neurobiology and
24:14	the active inference Theory uh but I I don't think it's uh one of active
24:22	inference um Central assertions at least uh to my understanding
24:29	well said very interesting framing well chapter five definitely takes a
24:36	very specific system of Interest approach by highlighting one of the most studied
24:43	areas also one of the most relevant areas which is
24:49	mammalian neuroscience and the chapter is going to introduce a
24:55	few different motifs in the nervous system
25:02	and essentially build up towards figure 5.5 which is at the end of the chapter
25:09	and 5.5 wires together three specific neural systems that the chapter is going
25:16	to focus on work in that area from so Ollie said it very well
25:23	active inference was built up to in chapter four here is another level or type of science
25:30	with assertions or with representations or mappings to any specific system but
25:36	this is the kind of modeling that has been built up and done by fristin power
25:42	pizullo and others over the decades with a focus coming from a human neuroimaging
25:49	laboratory setting a lot of focus and study and attention
25:55	and funding and everything on the mammalian nervous system
26:01	but claims about the nervous system have have uh
26:07	are not the basis of what active inference claims or how it's derived
26:13	but this is like an example case study in neurobiology connecting back to some
26:19	of the formalisms that we've just seen introduced in chapter four yeah and uh to add a minor Point uh to
26:27	what you just said I think it's important to uh draw attention to the
26:33	last sentence of the last paragraph of the first page it is important to draw a distinction between a principle I.E the
26:41	minimization of free energy and a process theory about how this principle may be implemented in a certain kind of
26:47	system so I I think uh this sentence here uh frames this chapter uh in
26:55	relation to all the other technical chapters of this book so if every other
27:01	chapter is about developing or at least up to now was about developing
27:08	um the principled formalism of active inference now chapter 5 provides a kind
27:15	of preliminary sketch for the process theory of active inference which is uh
27:21	obviously far from an extensive Theory it's just a single chapter but then again it can provide some important uh
27:31	signpost signpost for anyone who wants to further investigate this area
27:39	awesome free energy principle Bayesian mechanics all things in that area are on
27:47	this principle not responsive to empirical data and then the process theory is about how
27:54	the principle is implemented so the specific generative models that are made and how well they map or how well they
28:02	do in a portfolio of models that can have very different goals and assumptions and all of this
28:09	but the process Theory implementation lets us develop hypotheses that are answerable to empirical data like
28:16	what is the kind of information or relationship between
28:23	photons hitting the retina and changes in activity in neural systems
28:31	and that's an informational question or can be abstracted in a way to an informational question that it turns out
28:37	does have empirical support and results in unique explanations and predictions
28:43	that doesn't mean that it always results in unique explanations and predictions but a lot of citations are provided here
28:51	that's what we can explore in chapter five
28:56	the last paragraph of the first section describes that they're going to look at
29:03	the three different neural systems okay section 5.2 micro circuits and
29:10	messages what do you think Ali
29:17	uh all right so uh I mean this chapter begins uh from
29:23	how a message passing happens in neurobiological terms
29:30	and compare it to the way active inference frames this message passing
29:36	mechanism and specifically if we look at a figure
29:41	5.1 and compare this figure to the ones
29:46	we've seen before in chapters uh in one through four I
29:53	think it was in chapter four we can see some clear parallels between how this
29:59	kind of cortical message passaging happens in the brain versus how it is framed in
30:08	active inference literature and as we can see it's uh clearly inspired by the
30:15	neurobiology of the brain uh but uh then it's important to keep in mind that it's
30:22	not a direct one-to-one mapping between um these two models uh this is just um a
30:32	kind of I don't know an interesting or Illuminating uh if you like parallel to
30:41	keep in mind uh to somehow be a bit more confident about the viability of the
30:50	theory we want to use for message passing and active inference uh which is
30:56	to say it's not some haphazard Theory that's just been developed for practical
31:01	reasons uh it has some basis in neurobiology although uh it's not
31:08	necessarily fully congruent with every detail of neurobiological neurobiology
31:19	great the specific example is going to involve this one region of mammalian
31:24	cortex tissue that has these six layers and there's a ton of neurobiology
31:32	the big takeaway for figure 5.1 is that it's possible to graphically lay out
31:38	nodes and variables and find some empirical correspondences again some unique
31:45	explanations and predictions in certain cases and that's one kind of modeling where
31:52	it's really trying to understand uh and improve the ability to do correlation
31:57	and intervention and counter factual causal type analysis with the real
32:04	system of Interest or in a more pedagogical setting or in a research setting or an industrial setting you
32:10	might sweep across large families of structures of models and there's no need to be grounded to any biological
32:18	structure at all so this is just describing the specific neural anatomical research that really
32:26	arose out of the Imaging work at UCL and SPM package that's where a lot of this
32:32	comes from 5.2 yeah good and sorry just as a side
32:38	note uh I think watching uh one of Thomas parr's lectures on neurobiology
32:44	of active inference which is available in YouTube uh would really help to understand the materials of this uh
32:51	chapter better so I highly recommend watching that one thanks
32:59	figure 5.2 gives a re-rendering of a kind of
33:06	classical View of a hierarchical predictive coding
33:11	system works so here abstracting a layer from the
33:18	tissue six layer to just two layers here computational layers now
33:25	and then showing how there's hierarchical communication within a layer but also
33:31	uh well there's they're signaling within a layer and there's a hierarchy uh in Bayesian modeling with variables
33:40	that are higher order predictions about other variables and that's the basis of the predictive coding architecture
33:48	so 5.2 looks at some ways that the something that resonates with the
33:55	cerebral cortical architecture enables what might computationally look like or
34:01	have some really strong and explanatory values in actually
34:07	relating to computationally a hierarchical Bayesian model which could
34:12	do various General tasks all right 5.3 is motor commands leaving
34:19	the prefrontal cortex going down to the butterfly looking cross section here
34:26	what is 5.3 okay so 5.3 moves to the other half of
34:34	active inference framework which is uh I mean how how it can model uh the
34:43	decision making and ultimately the movement of the agent uh in order to
34:49	minimize the expected free energy as opposed to variation of free energy that
34:56	we saw in perceptual uh half of active inference so
35:01	it can provide a kind of uh correlation or analogy between the structural neural
35:10	Anatomy uh particularly related to uh I
35:15	mean the motor commands and uh how it can relate to active inference in
35:22	particularly The Continuous time active inference so we can see that uh for I mean for the
35:32	external uh uh event or I'm sorry for the external State we can take for
35:40	example the proprioceptive afferent and then uh this proprioceptive afferent
35:47	acts as a kind of why or the continuous time active inference which needs to be
35:53	uh um I mean processed in in a way to
35:59	optimize the expected free energy and how it relates to both attention and
36:06	precision uh we will see a bit more detail about uh those uh those terms and
36:15	the relation between them in chapter eight but I think here section 5.3
36:23	provides a good summary about the general uh paths through the motor
36:30	command systems of neurobiology great let's say while the previous case
36:38	study focused on how the connectivity within in between the cortical columns
36:45	could have a computational relationship with a Bayesian hierarchical predictive
36:50	coding architecture the argument of this second case study is that a continuous input continuous
36:58	output kind of set point seeking reflexive motor Behavior
37:06	with a moving set point with a descending moving set point enabling
37:11	motion by changing ultimately the set point and enabling uh variation in the
37:17	strategies to reach that set point through different mechanisms this is also describable in a compatible
37:25	way that's a shorter section now section 5.4
37:32	subcortical structures what would you say about this section
37:41	okay so subcortical um structures are very important in the
37:49	the decision making and and I mean the planning
37:54	of the agents so uh obviously here we need to uh we need
38:02	another kind of analogy between uh the way that these plannings and decision
38:10	makings happen uh neuroanatomically with the way uh that uh that it's framed in
38:17	active inference but again uh we can see it's uh clearly based on uh I mean at
38:25	least some of the important elements we've seen from uh the previous chapters
38:30	uh so for example we we saw how policy is described or how how it relates to
38:38	outcomes and and preference and so on uh we can see uh those elements are
38:44	directly inspired by a neuroanatomical structures so
38:50	uh I guess that's uh at least in my opinion
38:58	this section here 5.4 uh seems a bit more uh
39:05	sketchy in the meaning that it doesn't go into quite uh in the extensive
39:11	details about how how how those structures can be
39:17	um can be compared about for anyone who wants to uh
39:23	further investigate these topics there
39:28	are some useful references put on here on pages 93 and 94. so
39:37	yeah thanks yeah it's really abbreviated and overviewed but
39:45	we get an interlude from table 5.1 with putative roles of neurotransmitters
39:50	so same perspective that we took before on neural anatomical functionalism here
39:56	directly translates to neurotransmitter reductionism or essentialism or
40:03	something like that so certainly all neurotransmitters and molecules they play variable roles in different
40:09	settings and this is the knee in scruffy
40:16	manifold all over again one person might say well we we need a
40:21	theory for every acetylcholine molecule in the world they're all in a unique context and someone else says all
40:28	neurotransmitters are described by one parameter in this model I'm getting value from it so to me that's an account
40:34	and somewhere in between is the work in this space which is making an attempt to
40:42	have a principled and falsifiable approach to
40:48	model the computational aspects of specific regions and contexts and
40:54	settings and so acetylcholine noradrenaline dopamine and serotonin are
41:00	given a little mini review here and so it's not an exhaustive or an
41:06	exclusive claim it's kind of a provocation from computational and molecular neuroscience and people can
41:15	look into the papers and also ones that probably have been published since
41:20	5.6 goes to continuous and discrete hierarchies which is graphically overviewed in
41:27	figure 5.5 so what would you say about this uh yeah one interesting thing about this
41:34	section is um the observation that our lower level
41:41	engagement with the environment uh it can be most successfully characterized
41:47	with continuous time formulations but as we go up uh on uh I mean on the level of
41:54	uh cognitive Concepts or at the level of cognitive hierarchies and we come to
42:02	Concepts such as I don't know decisions or even beliefs and so on we can reach
42:10	the area that the discrete time situations uh would probably be more
42:16	efficient to to characterize the behavior of the
42:22	agent so this multi-scale [Music]
42:28	multi-scale structure of active inference modeling is quite evident in
42:33	the way that our message passing happens in our brain in terms of our lower level
42:40	data processing uh up into consolidating the higher level cognitive
42:48	Concepts and ontologies awesome thank you
42:55	to me figure 5.5 demonstrates the kind of whole of body approach
43:00	that you could imagine there's so many organs and systems and phenomena for which there aren't specific generative
43:07	models so little can be said about situations where no generative model has been articulated and here's one where it
43:14	has so it gives you also it's kind of like reading a drosophila melanogaster review paper
43:19	relatively it's like this is how much work it takes to get to this state of knowledge in an insect so then in
43:26	another insect do we know less about that insect empirically and genetically so consider this to be what's known to
43:34	be a lot however also about one of the most sophisticated or specific cognitive
43:42	systems at least we know so there's that additional kind of like self-reflexive
43:47	aspect to this chapter that is not a Cornerstone of active
43:52	inference but here it's just presented in a synthetic case study
43:58	anything else you want to say about five uh nothing particular comes to mind
44:05	all right
44:30	okay chapter 7 is called active inference and discrete time chapter seven is the first in a pair of
44:38	chapters with chapter eight on discrete and continuous time so that they're kind
44:44	of like two forks of a river that we discussed in chapter four and before and
44:52	describe the recipe in chapter six now seven and eight are kind of like one level deeper going from the kind of all
44:59	of uh this group of animals to one level deeper into its classification scheme on
45:06	the way to the specific generative model for which it's actually given in its
45:12	totality but everything prior to that is about the learning about its principles
45:18	and this is kind of on the trunk of the path to discrete time modeling just like
45:24	chapter eight will be about continuous time modeling what would you add in
45:32	okay so I think uh chapters seven and eight uh really helps to understand
45:39	um in a more practical way uh how the materials from uh particularly chapters
45:45	one through five applies in real time situations uh so uh even if uh we
45:55	somehow didn't get to understand every details of chapters one through four uh
46:01	by when we come to Chapters seven and eight uh I I think uh I mean some of
46:07	those uncertainties uh about or about our understandings uh can be uh
46:13	clarified uh in a in a at least in a practical sense so uh I believe these
46:21	two chapters are really helpful um in order to consolidate our
46:26	understandings from the previous chapters awesome well said
46:32	so it's going to involve specifying some discrete time models
46:38	7.2 goes into perceptual processing and the general structure of the chapter is
46:44	going to walk through a series of examples that building complexity where they first start with perception
46:50	in 7.2 introduce decision making and then describe a few more types of motifs
46:57	or cognitive structure or patterns and also check out step by step and
47:04	model stream one where it's built up to in a different way so the first example
47:11	is I'll let you describe it since it's Musical
47:16	okay so yeah the first example is um the situation in which we try to
47:24	describe the performance of an amateur a musician in terms of how how we listen
47:32	to the performance of an amateur musicians in terms of the the predictions we get
47:40	from our anticipation of the following notes as opposed to the actual notes
47:48	that's being played so these kinds of anticipatory reaction uh listening
47:56	reaction to the musician can be successfully formalized using
48:01	um discrete time active inference by uh putting up uh by by
48:09	putting together the matrices a uh for
48:14	the states and uh Matrix B for the transition between the states
48:20	um or the transition probabilities which uh describe in this case describes uh
48:26	the probability from going from one note to the other and uh obviously the
48:32	um the actual sequence that's been played which can be described with uh
48:37	The Matrix T so uh and another Point uh
48:43	I wanted to point I wanted to mention is for anyone who has downloaded this
48:49	chapter uh before I don't know I think about June or something
48:54	I recommend re-downloading it from mit's website because uh they have corrected
49:00	some of the typos that was previously present in this chapter particularly in
49:05	gear 7.2 cool so this graphical model
49:14	where a person is listening this is a general perceptual Bayesian framing it's
49:20	specified just like with any other equations there's a lot to look into but a
49:28	indicates the probability of an outcome given a state this is saying if it were all
49:35	on the diagonal like an identity Matrix this is kind of a common Motif then States kind of map to themself so in the
49:43	context of uh uh in the context of this model A
49:51	represents the mapping between the observed note and the underlying hidden true note
49:58	FNB describes the transition Matrix of how those change to time D is the prior
50:04	they're specified figure 7.2
50:10	you want to describe it foreign
50:15	so in figure 7.2 or at least the incomplete version of uh figure 7.2 we
50:22	see here uh well at the upper left uh part of the picture uh we see uh I mean
50:30	uh the beliefs about uh about each note uh at each step at each time step and at
50:40	up upper right uh we somehow translate those uh beliefs into a specific
50:46	numerical values uh so uh so instead of
50:52	just assigning some continuous values we uh
50:57	we we've simplified the situation by assigning some discrete numerical values
51:03	for each note and then
51:08	the lower left is supposed to show the uh the act the free energy gradients and
51:17	over time uh or in in other terms uh the prediction errors uh we get from
51:26	um I mean comparing our predictions with the actual outcomes uh so lastly uh lower
51:36	right picture uh shows uh in parallel to the upper right picture uh determines uh
51:44	the values of these errors so uh we we can see both
51:50	uh the The Continuous the initial or at least initial uh continuous assignments
51:57	and values and then the further uh discretizing of the values in order to
52:04	get the discrete time situation or the more tractable at this group time
52:10	situations okay so it's a general passive inference task
52:18	where there's priors about how states are going to change through time and then there's real data coming in so
52:24	that's the kind of classical predictive coding video compression common filter
52:29	Bayesian setting 7.3 introduces a key Motif which is
52:36	decision making and planning as inference so this is the idea of having
52:42	a base graph where the variables can relate to different things there's high composability and here the idea is that
52:49	a variable is going to be proposed that we can do inference about that describes
52:55	the process of decision making or policy selection so what would you say about 7.3
53:04	okay so 7.3 is obviously similar to what we saw in
53:12	chapter 4 and if I'm not mistaken even the topology is exactly the same with
53:20	that picture we saw previously so uh yeah this is the initial setup to or
53:29	which acts also as a review about how these different
53:37	components of General upon DP General to models needs to be
53:43	described in in such situations but uh
53:48	ultimately the specific case study we
53:54	come across in this section is uh the attempt to model uh the behavior of
54:01	uh of a of the mouse in a teammate so rat and a teammate so
54:08	um especially uh teammates containing uh an aversive stimulus in one arm and an
54:16	attractive stimulus on the other so this is this can act as a kind of toy example
54:23	to use uh this kind of probabilistic modeling to uh
54:29	describe these situations
54:35	thanks so that leads us right to figure 7.4 here's a visualization of the
54:40	situation with the the rat in this case where there's a a pleasant and aversive
54:50	stimuli on each end of a decision point and there's also a epistemic opportunity
55:00	to receive some information about the context that the animal is in
55:08	and so that setting is described for both the case with white on the left
55:13	block on the right and block on the left white on the right and those are shown in terms of their
55:20	differences in the matrices the explicit specification of the generative model
55:27	visualizations show some of the slices of the B variable
55:33	which reflect different transition probabilities C represents the preferences which are
55:41	expressed over the observable States D reflects the priors on the different
55:50	states that need priors 7.4 what would you say about this
55:58	yeah okay so in 7.4 it builds up on the
56:11	and sorry two and four uh which is how the exact formulation for
56:19	um expected free energy can be used sorry variation free energy can be used to
56:27	formulate the trade-off between uh the uh I mean information uh seeking and and
56:34	or at least uh between the epistemic value and information seeking so uh here
56:42	it uses uh again that rad example in a bit more uh more extended and elaborate
56:51	form to formulate the epistemic value of observing a q in a given a given
56:57	location and figure 7.7 is a representation of
57:04	this situation but another situation that's been
57:12	uh let me see yeah in 7.9 another case study discussed here is uh is is the
57:22	situation of uh The Psychotic eye movements and um because it is something
57:31	that can be quite successful you just describe or characterize in terms of
57:37	information seeking versus the epistemic value and the situation here is
57:45	uh let me see yeah shown visually in figure 7.9 which
57:52	clearly shows how our visual uh psychotic eye movement eye movements can
57:59	be um described in such a way as to as to kind
58:05	of trace the trajectory uh of our eye movement among different regions of the
58:12	visual space so uh and how the information We Gather
58:18	from a given region can be can affect
58:24	the um I mean the subsequent trajectories of our psychotic eye movements so
58:31	um yeah that's basically the main premise of
58:39	this section I guess nice great 7.5
58:46	what would you say about it
58:51	okay so uh 7.5 again adds another dimension uh to the
58:59	previous formulations and this time we get to update the generator models by
59:06	learning and the um
59:12	so the the generative models for this situation is a bit more complicated than
59:19	the previous ones because it now needs to account for a mechanism or a way to
59:28	update the matrices we had before so in the previous situations uh we we didn't
59:36	account for learning per se but here
59:41	we directly update our general sorry
59:46	the word update can be confusing here we we get to somehow
59:52	improve our generative models to accommodate for these
59:58	updating accounts and
1:00:04	yeah so uh this the situation here uh or
1:00:10	the case of study uh uh here uh which uh somehow elucidate the way that the
1:00:18	learning um can be a common can be accounted for
1:00:23	with these models is uh again a toy example of a creature uh in
1:00:31	a in a simple world of uh black and white tiles which kind of
1:00:39	tries to find a a path to reach a given
1:00:45	destination a certain destination so it is more complicated than the
1:00:52	situation we had for the rat example because it only had
1:00:58	um I mean simple trajectories that needed to um
1:01:03	a Traverse but here uh the creature or the agent in this case needs to do lots
1:01:13	of a lot more learning and information seeking and so on so all the previous
1:01:20	elements uh is kind of combined in this example and it's a really good example
1:01:27	to see how the different components of active imprints can be connected to each other
1:01:35	nice and seven six hierarchical or deep inference first a box 7.3 interlude on structure
1:01:44	learning boxed off topic and a lot to say but structure learning broadly
1:01:50	refers to learning the structure about a model using the same types of methods that you
1:01:57	might to do inference on for example a more observable sensor data reading something like that
1:02:06	this section Works towards the idea of nested inference or or multi-scale
1:02:14	modeling what would you say about figure 712
1:02:22	so again this situation is um I think the most complex
1:02:29	situations of this chapter which builds up uh from the previous sections uh and
1:02:36	this time it adds another layer to accommodate for
1:02:43	for the inferences that happen uh in uh I mean different time steps so in this
1:02:50	case we have multi-time or multi-scale inference and learning happening
1:02:57	um both at the levels of um sorry at the levels of learning and at the levels of information seeking so
1:03:04	this this is represented in
1:03:11	figure 7.12 which represents how uh kind of
1:03:18	this fractal generative model can be seen as a component in um
1:03:27	this multi-scale bigger generative or and as a kind of leaf in this bigger
1:03:35	bigger generative model uh so it can be seen as a lower uh level inference
1:03:43	happening at the leaf level going up to the hierarchy and influencing sorry uh
1:03:52	collaborating on the whole process of learning and inference at the higher
1:03:59	level so um yeah yeah I guess that's
1:04:06	somehow summarizes this figure so if you have anything to add
1:04:12	that's that's great it's an example of the composability
1:04:18	of generative models what we've uh talked about and had Toby
1:04:26	Sinclair Smith describe as as the compositional cognitive cartography
1:04:31	and just what kinds of connectors can and can't you do and how can that Motif
1:04:37	that the discrete time model introduces and then the rest of these features
1:04:43	including action and learning and so on get layered in on top what can you do
1:04:48	with that 713 gives another example
1:04:54	do you want to say anything about it or maybe continue on uh yeah so the case
1:04:59	study here is the example of um linguistic I mean language learning
1:05:06	uh through reading so not language learning uh maybe it's just what happens
1:05:14	uh sometimes yeah yeah in comprehension so what
1:05:19	happens when reading and in a anticipatory way uh the the words
1:05:27	that that comes each after the other so uh why this kind of situation uh can be
1:05:36	most successfully characterized with this kind of modeling because it involves a different scales of learning
1:05:45	and comprehension both at the level of I mean reading at the level of uh
1:05:54	somehow observing the letters and then going onto uh the words and then word
1:06:00	groups and so on uh so uh yeah that's uh really interesting way to uh again
1:06:07	combine all of those elements uh into a single unified model to see how those
1:06:14	different time scales slow and fast time scales operate together to
1:06:22	to build this more encompassing model of
1:06:29	more incoming generative model of the situation pray any closing thoughts on seven
1:06:39	uh nothing particular now thanks all right
1:06:45	next chapters chapter eight which is going to go into the continuous time
1:07:07	all right chapter 8 is called active inference and continuous time begins with that Timeless quote everything
1:07:13	flows nothing stands still what would you say about chapter eight
1:07:19	all right so this chapter uh probably is at my most favorite chapter in the book
1:07:27	uh because of my own personal interests in um uh I don't know the process
1:07:34	materialism and so on but uh yeah so chapter seven
1:07:39	um and acts as a really good starting point for anyone who wants to
1:07:46	develop the discrete time situations and to model the screen time situations uh
1:07:52	enacted within active inference framework but in chapter 8 we kind of
1:07:58	get into get to a Model A bit more interesting or let's
1:08:06	say more involving situations uh and they're not necessarily
1:08:13	a kind of toy examples we saw at least at the beginning of chapter seven
1:08:19	so obviously as the title suggests this
1:08:25	chapter deals with the continuous time situation so uh in the in that case
1:08:30	we'll need to and maybe at this point refresh our
1:08:36	memory about what continuous time situation involves by reading the
1:08:41	relevant Parts reading Arabian relevant parts of chapter four so
1:08:49	uh yeah in chapter four we saw that the generative model for continuous time situation uh derives derives from uh the
1:08:59	uh it was a stochastic calculus in terms of uh putting the whole process
1:09:07	um into two elements uh two stochastic um two elements of stochastic equations
1:09:13	uh one of which is the actual uh State and the condition of actual States
1:09:21	um or the behavior of the actual States and the other one uh is the randomness
1:09:27	that we need to account for in each real-time continuous time situations so
1:09:33	that's what we get here in equation eight a 8.1 and then building up from
1:09:41	that equation uh we um it generalizes that equation to
1:09:47	involve uh I mean the fun the functionals of G and F
1:09:53	instead of just um the um the single valued functions of G and F
1:10:01	so uh then we get to
1:10:06	put that into uh the situation that can
1:10:12	be used for describing the behavior of dynamical systems which is a very well known uh situation to use these kinds of
1:10:22	uh stochastic equations and it's widely studied how those those kinds of
1:10:29	Dynamics can be characterized especially in recent Bayesian mechanics paper by
1:10:36	Dalton exactly vadavel and others so and then it gets to some more specific
1:10:43	examples such as a lockable terror Dynamics and the synchronicity and so on
1:10:50	in order to show how these kinds of Dynamics can be
1:10:58	elaborated upon and can be generalized to
1:11:05	and and enables them to characterize a more complex situations so
1:11:13	uh get that's a really short and brief overview of the whole uh chapter maybe
1:11:22	uh we can talk about a bit more details as we go through
1:11:28	right well said well I'm sure for another day the philosophical
1:11:33	implications of a seven and eight and high road and low road and all these other parts of the textbook great topics
1:11:40	um I I agree I would see chapter 8 as demonstrating continuity
1:11:46	with some classical continuous time modeling motifs from a
1:11:53	few different areas of dynamical Systems Science which is applied in like many
1:11:58	many many fields but these are some classical examples so figure 8.1 goes a
1:12:04	little bit more into depth or at least into more formalism detail about exactly
1:12:09	what we saw in chapter five with the spinal reflex arc with the proprioceptive data coming in and then a
1:12:17	differential being calculated with the set point which reflects a descending prediction from a decision-making layer
1:12:24	and that can be viewed as this kind of mechanics that plays out in a phase
1:12:31	space in continuous time like a spring moving around with with someone making a
1:12:37	certain path with an attractor and a spring being dragged around something in
1:12:43	that area box 8.1 goes into a very fascinating
1:12:48	topic do you want to describe it uh well uh it's uh maybe uh one of the
1:12:57	most um thought-provoking uh section pages of
1:13:02	the whole book and if I remember correctly uh in all of the cohorts this
1:13:08	particular box I mean gives always gives rise to lots
1:13:15	of questions uh because of some of the interesting and uh at least initially
1:13:21	counter-intuitive claims here but I don't want to spoil it so
1:13:31	um but as a kind of spoiler alert it kind
1:13:38	of gets to a really interesting but uh alas
1:13:44	very brief discussion about the comparing these terms Precision
1:13:51	attention and sensory attenuate and the relation and similarities and difference
1:13:57	between these two these three terms and how each understanding each of them is
1:14:04	essential to understanding the other ones but as I said it's a really interesting
1:14:11	topic which gives rise to lots of discussions
1:14:16	and I believe it's one of those topics that uh that's worth looking a
1:14:25	bit more uh looking into in some other literature as well
1:14:31	great well said what a cliffhanger next they go to a classic Model family
1:14:38	called lockabletera these Dynamics inherit from characterizations of Predator prey
1:14:44	Dynamics in ecology so it's kind of a classical ecology model shown in figure 8.2 on the top
1:14:51	it's actually the ecosystem model plants herbivores and carnivores which follow
1:14:57	different kinds of oscillatory Trends in continuous time
1:15:02	and so that also has enabled it to be applied for other so-called winnerless
1:15:08	competitions and that relates to topics like neural Darwinism and also neurodynamics where things have kind of
1:15:15	oscillatory relationships with each other which are being modeled as a continuous time underlying process
1:15:24	with a lot of measurement noise and discretization through space and time but those are the kinds of algorithms
1:15:30	that SPM explores more and there's lack of Volterra and a lot of other dynamical systems theory in SPM
1:15:37	so active inference kind of adds action and more to what was laid out from a
1:15:44	pure dynamical systems theory in SPM here it really is just showing the
1:15:51	ecology example and how you can project if you have three different species you can think about that motion in a cube or
1:15:58	tetrahedron and then you could project onto kind of like looking at a lower
1:16:05	dimensional manifold relating just two of the three species and that events is this kind of
1:16:12	oscillatory but also moving Behavior that gets connected in figure 8.3 to
1:16:20	neurobiology what would you say about this
1:16:26	uh okay so here in figure 8.3 uh we see
1:16:31	some applications of um a lock of Altera Dynamics uh so uh
1:16:37	the left column uh here uh show represents
1:16:44	um what happens in I mean in eye blinking eye blink conditioning so uh of
1:16:54	course here we need to account for uh I mean the expected States
1:17:03	of uh of the of the sequel of the sequences of events that happens in in
1:17:10	the eye blinking so uh the upper upper left figure shows the expectations
1:17:18	uh in terms of time and then uh the parallel right hand side equation
1:17:26	sorry right hand side figures uh shows the uh
1:17:32	the lock of Altera system that that is applied um in the handwriting situation so as we
1:17:40	can see although uh the uh I mean mathematical technology is the same or
1:17:47	at least the modeling technology is the same the outcome of each situation uh varies
1:17:55	drastically in two distinct
1:18:00	um the two distinct neurobiological Behavior not neurobiological but
1:18:07	biological Behavior Uh so yeah we can see how the same modeling framework can
1:18:15	give rise to different outcomes uh based on what parameters is
1:18:24	needs to be optimized what parameters are selected for the modeling and so on
1:18:31	so I believe it's quite uh interesting example to compare
1:18:38	handwriting and the blinking together and how those can be compared to each
1:18:46	other using the last couple Terror Dynamics great thank you
1:18:51	box 8.2 gives a variance on the learning here presented with the formalism for
1:18:58	continuous models kind of a technical aside section 8.4 is about generalized
1:19:04	synchrony so figure 8.4 is going to visualize one of the classic dynamical
1:19:11	systems which is the Lorenz attractor so what would you say about this bigger okay so this section is uh truly
1:19:21	interesting because uh when one thinks of active inference
1:19:27	probably the first situations that comes to mind is uh the situations in which we
1:19:33	have a quite well-defined uh probability distributions for different parameters
1:19:40	but as we can see here in section 8.4 actually some of the
1:19:47	formalism of active inference can be successfully used to characterize even
1:19:53	chaotic systems and partic and in particular the way in which two chaotic
1:19:59	systems can be synchronized with each other so this is a classic example of a
1:20:06	chaotic Lorenz system and it derives from it draws upon from
1:20:14	some of Professor person's earlier work on Birdsong synchrony
1:20:20	and as a side note any literature before 2016 is considered earlier history in
1:20:29	active inference literature because it evolves uh quite rapidly so
1:20:35	uh yeah this kind of synchrony between two chaotic systems uh can be
1:20:44	interpreted as providing evidence or even
1:20:50	let's say a way to model a kind of primitive theory of mind uh in in the
1:20:58	sense that how exactly can we understand uh or can two two agents can
1:21:09	trace each other's trajectories uh without
1:21:15	um any I mean engaging in any direct exchange of observations between their
1:21:24	internal and external States so uh yeah that's a really good example and I
1:21:30	believe one of the most interesting examples of how active inference can even account for these kinds of uh
1:21:38	Behavior so and the rest of the section goes into the details of how this kind of
1:21:45	synchrony between um multi multi-scale uh Birds
1:21:51	multi-scale Lorenz systems can happen um and how can we
1:21:58	formulate it mathematically in terms of continuous time active inference
1:22:04	awesome and there's been more recent work on Mark all blankets and stochastic
1:22:10	chaos but the bird example is a classic 8.5 goes into hybrid discrete and
1:22:16	continuous models so this could be kind of like a in-between chapter of seven
1:22:22	and eight but now that we've been introduced to the pure form of discrete in the pure form of continuous models
1:22:28	here shown the dot composability extends to so-called Hybrid models where here
1:22:35	the lower level visually is using the continuous time formalism
1:22:40	and the higher level is describing the little line added here the discrete time
1:22:46	formalism and this was the similar structure described by the authors of
1:22:54	the paper octave inference does not contradict folk psychology where they describe this lower level as
1:23:01	motor active inference which was closely allied with the spinal Arc reflex shown
1:23:08	above and then this higher level they call decision active inference because
1:23:14	in that case it was referring to a discrete decision and so they used that kind of basic
1:23:20	motif of continuous activity or continuous time modeling at the more
1:23:26	peripheral aspects of a cognitive entity and like Ali said more discretization
1:23:32	[Music] and hybridization as well at higher
1:23:37	levels of the cognitive modeling and that type of an architecture here
1:23:45	instead of describing who wants the ice cream cone I believe here it's gonna be a mixed or hybrid
1:23:52	model that is going to call back the isocade system
1:23:58	where there's a fixed point that is able to be moved as a set point
1:24:06	and then there's a continuous time isocade that pursues the new fixed point and so that's analogous to a new set
1:24:14	point or fixed point being specified from the top down muscle command about a
1:24:20	new location for a muscle followed by movement towards it this is a muscular
1:24:25	activity that is realizing that but but not in the elbow coming away from the
1:24:30	hot stove this is about the eye cicading to an epistemic foraging location
1:24:36	specified by top down hierarchical systems 8.3 describes little technical aside on
1:24:45	mixture of gaussian gaussian mixture models kind of a technical modeling note an 8.6 closes
1:24:55	it says it's a huge topic and much has been left out and so they list in table 8.1 key advances in continuous time
1:25:02	models and those areas are synthetic bird song ocular motor delays
1:25:08	conditioned reflexes smooth Pursuit eye movements psychosis illusions cicades
1:25:14	action observation attention Hybrid models and self-organization
1:25:19	and that's chapter 8. what else would you say and also what would you kind of
1:25:25	lead someone to in the philosophical implications of eight because it sounds kind of cool
1:25:33	uh okay so uh well uh the case of continuous time
1:25:39	active inference uh uh I think it leads to
1:25:46	a really interesting questions uh both in terms of uh philosophical questions
1:25:52	and also uh a more practical modeling
1:25:57	questions about uh what parameters needs to be accounted for and so on uh and as
1:26:05	I said I believe it's a more and more interesting way of
1:26:12	um if not interesting but but at least more involved way of doing active
1:26:19	inference modeling but uh one thing that uh one of the philosophical questions
1:26:25	that when I uh have explored in our paper is
1:26:33	how the processes of I mean ontological
1:26:38	processes can uh philosophically described using fpp
1:26:46	assertions in terms of their interaction with the environment
1:26:53	[Applause] in which they co-constitute themselves and we don't necessarily
1:27:01	distinguish between uh between the internal and the external state so one
1:27:07	obvious example of this is that generalized synchrony example that we saw in this chapter in which we don't
1:27:15	necessarily distinguish between which of the birds act as
1:27:22	the agents and which one is the um environment or device versa so these
1:27:29	kind of co-constitution of the environment and the agent which gives rise to the partitioning of State space
1:27:37	through a markup blanket is uh one of the interesting uh
1:27:45	philosophical points that I think needs to be elaborated
1:27:50	a bit more using and some of the recent
1:27:55	advances in philosophy such as the two the tools that's been developed in new
1:28:02	materialism school or some other philosophical approach approaches but yeah these kinds
1:28:10	of uh what exactly gives size gives rise to emergence what is the ontological
1:28:16	status of emergent properties and so on are some of the
1:28:22	um burning questions for many philosophers today and I believe active inference in
1:28:28	particularly continuous time active inference provides a clear uh precise
1:28:36	mathematical formalism uh even if not to
1:28:43	answer these questions but at least to explore it in a more rigorous and uh
1:28:49	practical way and also practical and attractive attractable way
1:28:55	so this is the area that I believe uh
1:29:01	philosophy and science uh are beautifully intertwined uh into a
1:29:06	coherent view of not only the phenomenon of interest but even
1:29:12	about the whole world wow
1:29:20	wow pretty cool yeah a lot to say about that topic
1:29:26	after completing chapter seven and eight you've seen the kind of two major
1:29:31	branches or two major motifs of just one kind of modeling but these kind of
1:29:37	models have so many different forms that that's why it's such a Hands-On process
1:29:43	to specify the generative model in chapter six and fit it with data in chapter nine those are all what's
1:29:49	required and that's kind of the last mile of where these discussions about General motifs gets you
1:29:57	but also playing with these pedagogical models can be really helpful because it
1:30:03	will help you understand the basic patterns and relationships and start to
1:30:08	see see different patterns in the graphical models and know from there
1:30:13	what levels of technical processes can be kind of coarse grained over
1:30:20	all right
1:30:27	okay well that's it um I guess next time we will do probably
1:30:33	9 10 and maybe something else all right I'll end it now thanks Holly
1:30:41	thank you

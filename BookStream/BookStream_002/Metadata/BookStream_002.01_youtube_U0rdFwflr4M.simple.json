[
  {
    "start": 0.389,
    "end": 25.983,
    "text": " on to chapter two the low road to active inference so all chapters begin with a short quotation and the quotation here reads my thinking is first and last and always for the sake of my doing William James so even before reading the chapter these quotations are often great places to jump off and have a discussion so Ali please begin and go as long as you want on the low road to active inference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 28.764,
    "end": 32.179,
    "text": " Okay, so I guess...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 34.568,
    "end": 45.055,
    "text": " Chapter 2, as the title suggests, it's the construction of active inference theory from the viewpoint of the low road of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 45.576,
    "end": 51.58,
    "text": "So it begins by providing this notion of perception as inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 51.66,
    "end": 58.565,
    "text": "Because more often than not, we usually think of perception as something just",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 60.026,
    "end": 69.151,
    "text": " in a computational sense, is a processing of the inputs, or rather, it's a raw processing of the inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 69.551,
    "end": 79.537,
    "text": "But here it shows that it can also be described as, or even more accurately described, as a kind of inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 79.918,
    "end": 88.163,
    "text": "So it's not just a simple and raw processing of the inputs, as my computational model...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 90.175,
    "end": 92.237,
    "text": " or a computer analogy might suggest.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 92.877,
    "end": 115.796,
    "text": "It's more like something that we predict and then we compare our inputs or the inputs we get from the stimuli with our prediction and then try to somehow minimize that prediction error.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 116.136,
    "end": 118.978,
    "text": "So I believe that this,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 119.919,
    "end": 133.31,
    "text": " Notion of perception as inference is the most central notion of all the related theories of predictive coding, predictive processing, Bayesian brain hypothesis.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 133.871,
    "end": 141.857,
    "text": "So up to now, active inference is not very different from all the other theories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 143.078,
    "end": 147.582,
    "text": "It's basically a subset or a variant of those theories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 148.32,
    "end": 161.572,
    "text": " but then it progresses into distinguishing the active inference from all the other theories and how it stands apart from the other ones.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 163.174,
    "end": 166.136,
    "text": "So in section 2.1,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 169.103,
    "end": 184.928,
    "text": " We began with some of the basics of probability theory, namely the Bayes theorem in box 2.1 and some simple examples about how Bayes theorem can be applied.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 187.348,
    "end": 195.991,
    "text": "And again, a very important page or important part of this section will be",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 197.764,
    "end": 210.876,
    "text": " Page 20, which describes the concept of surprisal and specifically the statistical surprisal and how it differs from the phenomenological surprise or psychological surprise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 213.498,
    "end": 217.842,
    "text": "And how we can formulate the statistical surprise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 217.922,
    "end": 218.062,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 218.743,
    "end": 241.236,
    "text": " yes just just to catch up here this example that's going to come back again and again is a person who's guessing the object that's in their hand and it could be a frog or an apple and the object is going to jump or not and so that's used as a way to talk about the bayesian updating that forms the kernel of a variety of bayesian brain like models including active inference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 242.156,
    "end": 260.686,
    "text": " and then um there are the presentation of the exact base so in simple cases you can compute exactly what you want with base theorem however it's a lot more effective on large data sets to use certain approximations and heuristics that we're going to be discussing and then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 262.708,
    "end": 269.357,
    "text": " Ali has highlighted that there are two concepts that are very closely related to each other regarding surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 269.897,
    "end": 274.423,
    "text": "There's surprise by itself and then there's Bayesian surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 274.723,
    "end": 279.169,
    "text": "So please pick up there what are surprise and Bayesian surprise and why do they matter here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 283.888,
    "end": 284.068,
    "text": " Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 284.108,
    "end": 298.073,
    "text": "So, well, surprise, I mean, the regular surprise, as we're all familiar with, is something more related to our sense of surprise or psychological sense of surprise of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 299.818,
    "end": 304.199,
    "text": " observing some unexpected phenomena or unexpected behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 304.699,
    "end": 327.163,
    "text": "But Bayesian surprise or statistical surprise is, of course, closely related with the psychological sense of surprise, but a bit more rigorous in which it's a way to compare two probabilistic information using callback-Leibler divergence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 328.407,
    "end": 345.311,
    "text": " And somehow getting the, I mean, how unexpected that information emerging from callback divergence, and they call it the surprisal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 345.411,
    "end": 355.093,
    "text": "So surprisal is, in other words, is a way to formulating those unexpected probabilistic information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 355.826,
    "end": 365.349,
    "text": " And it doesn't necessarily align or maps onto perfectly our psychological sense of surprisal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 365.409,
    "end": 368.19,
    "text": "But as I said, it's closely related with that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 369.23,
    "end": 369.49,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 370.11,
    "end": 374.692,
    "text": "By surprise, I mean for almost... Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 375.272,
    "end": 376.212,
    "text": "No, you take it, Ali.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 376.312,
    "end": 376.512,
    "text": "I know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 376.532,
    "end": 377.713,
    "text": "I was just going to say that...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 379.878,
    "end": 394.49,
    "text": " I was just going to say that by surprise, almost in the rest, all the other sections and chapters of the book, we basically mean this Bayesian surprise sense of the word.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 395.07,
    "end": 406.66,
    "text": "And in order to distinguish that with the regular sense of surprise, sometimes in the literature, surprisal is used to refer to Bayesian surprise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 408.015,
    "end": 431.945,
    "text": " yes so they're both measured in information theoretic units this is all happening in information geometric spaces the first concept of surprise is applied to a given single observations how surprising is that one observation and so in that sense it's a lot like the z-score of a data point coming in with respect to a statistical distribution",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 432.665,
    "end": 440.668,
    "text": " So it's like you have a height distribution in a classroom and you measure one person, and then you can say, what is the z-score of that measurement?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 440.989,
    "end": 448.372,
    "text": "Was it right at the center of the distribution with a z-score of zero, or were they two standard deviations higher with a z-score of two?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 448.512,
    "end": 449.432,
    "text": "So it's kind of like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 449.972,
    "end": 460.497,
    "text": "And that's why there's a discussion of probability distributions and their support, which is the x values for which they're defined, and surprise function with the fancy i.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 461.297,
    "end": 469.36,
    "text": " And that's going to be a function that helps you compute how surprising each observation is given a parameterization of that distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 470.18,
    "end": 474.881,
    "text": "Whereas this Bayesian surprise is more related to learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 475.501,
    "end": 484.624,
    "text": "It has to do with how much updating happens between the prior and the posterior, and that's before and after the observation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 485.224,
    "end": 489.286,
    "text": "So one could imagine a surprising observation, surprise concept one,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 490.126,
    "end": 495.37,
    "text": " that either does or doesn't update the prior into a very different posterior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 496.091,
    "end": 501.395,
    "text": "So as Ali mentioned, they are not exactly the same, but it's going to be important to understand how they're different.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 501.916,
    "end": 504.998,
    "text": "And that's worked out again in the case of the apple jumping.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 506.26,
    "end": 510.722,
    "text": " Box 2.2 continues with a discussion of expectations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 511.282,
    "end": 519.364,
    "text": "Now expectation in everyday parlance might be specifically referring to something in the future, like I expect it to rain tomorrow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 520.225,
    "end": 530.008,
    "text": "And in statistics, when we talk about the expectation, we're talking about the weighted average or the center of gravity of a distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 530.468,
    "end": 535.35,
    "text": "And that can be in both a discrete distribution, at which point you have a weighted sum,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 536.19,
    "end": 557.052,
    "text": " or a continuous distribution in which case it's an integral so you can have an expectation for the humidity tomorrow and so that might refer to the center of gravity at a t equals plus one but just taken alone expectation means center of gravity of a statistical distribution not anticipation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 558.806,
    "end": 568.964,
    "text": " Section 2.3 is going to describe how some of this, how this low road that we're on is going to connect to biological inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 572.162,
    "end": 592.389,
    "text": " There's more discussion of the generative model and the generative process, and a little bit of a hint that the generative model captures aspects of the generative process, which is where we see a lot of the classical cybernetic theorems and concepts like requisite diversity, good regulator theorem, and so on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 592.809,
    "end": 598.991,
    "text": "However, the generative model does not have to be exactly isomorphic with the generative process.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 599.571,
    "end": 615.237,
    "text": " For example, the generative process, the temperature in the room might be a continuous variable, but then the generative model might be discrete, only modeling integer-based temperatures, or might be categorical like too hot, just right, and too cold.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 615.817,
    "end": 623.96,
    "text": "So there's a lot of articulations that can be done because of how flexible and interoperable generative models are with each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 625.353,
    "end": 634.396,
    "text": " Figure 2.2 is going to expand upon that chapter one representation of the cybernetic action perception loop.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 634.936,
    "end": 649.342,
    "text": "And we're going to see this more in terms of a generative model and generative process articulation, and there are incoming, speaking from the perspective of the generative model, the agent, there are incoming and outgoing",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 650.262,
    "end": 665.755,
    "text": " dependencies in this graph and this is a little bit like a schema more so than a formal graph but also it is like a bayesian graph where nodes are variables and edges are causal relationships and so we have the internal states of the model",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 667.165,
    "end": 675.888,
    "text": " the external states of the generative process, and then the blanket states that make internal and external states conditionally independent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 676.008,
    "end": 682.67,
    "text": "And again, speaking from the perspective of the agent, although there is a symmetry, we can talk about the incoming",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 683.53,
    "end": 711.77,
    "text": " sensory signals happening from the observations handed from the process passed on to the internal states of the generative model and then the outgoing actions that are selected that then can influence the hidden state of the world for example going and turning on the heater to increase the temperature in the room and so this action perception loop or particular partition is going to get explored in a lot more detail in the coming sections",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 714.124,
    "end": 743.173,
    "text": " previous section 2.3 was on perception as inference and now action as inference is going to be discussed and that is where they say the discussion to this point is common to all Bayesian brain theories we now introduce the simple but fundamental Advance offered by active inference which is the extension of this inferential perspective to consideration of action as inference perception and action cooperate to realize a single objective",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 748.317,
    "end": 774.299,
    "text": " section 2.5 is about minimizing the discrepancy between the model and the world we already described that there's two ways for this to happen change your mind and change the world that's how the discrepancy can be reduced or managed we see a variant of the action perception loop where the agent is making perceptions uh predictive uh models of the world",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 775.191,
    "end": 785.138,
    "text": " that through perception are being juxtaposed with observations handed from the generative process, the world, and a discrepancy is realized, some non-zero discrepancy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 786.059,
    "end": 794.245,
    "text": "And then here are those two paths to minimize free energy, change beliefs by perception and learning, or change the world through action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 797.153,
    "end": 807.936,
    "text": " Section 2.6 is going to discuss how the exact Bayesian approach described earlier is absolutely spot on if you have infinite computing resources.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 808.396,
    "end": 823.541,
    "text": "However, we're often interested in rapid or large data sets where we want to be able to get approximate Bayesian computation or probably approximately correct computation in a vastly accelerated fashion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 824.281,
    "end": 831.929,
    "text": " And so that is going to be approached using what's called variational Bayesian inference that's unpacked in chapter four.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 832.95,
    "end": 842.859,
    "text": "But it suffices to say that variational Bayesian inference implies substituting two intractable quantities, the posterior probability and log model evidence,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 843.8,
    "end": 847.443,
    "text": " with two quantities that approximate them but can be computed efficiently.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 848.104,
    "end": 851.246,
    "text": "The approximate posterior Q and a variational free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 851.867,
    "end": 859.273,
    "text": "So it transforms an intractable estimation problem into a highly tractable optimization problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 859.753,
    "end": 860.674,
    "text": "Pick up from there, Ali.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 863.656,
    "end": 867.92,
    "text": "Yes, sorry, I just wanted to point out",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 868.895,
    "end": 894.66,
    "text": " a couple of things especially about section 2.3 because I believe it is one of the crucial sections of this chapter and in fact the whole book because it provides some of the justifications of using Bayesian inference as opposed to some other mathematical techniques such as maximum likelihood estimation but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 895.686,
    "end": 903.497,
    "text": " More important than that, I think it's this concept of optimality which comes into play in almost",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 904.002,
    "end": 908.446,
    "text": " uh, everywhere, uh, in the literature and, uh, of course in this book.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 909.086,
    "end": 919.075,
    "text": "Uh, so there are actually two notions of optimality, uh, which has not discussed, uh, in detail here, namely Bayesian optimality and Jane's optimality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 919.095,
    "end": 931.265,
    "text": "Uh, but, uh, some in some of the recent papers and Bayesian mechanics, uh, Dalton's active, I develop Maxwell Ramstein and others have shown that those two, uh, concepts of optimality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 931.704,
    "end": 933.405,
    "text": " are actually congruent with each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 933.845,
    "end": 945.393,
    "text": "So that's one of the reasons that the duality between FEP, free energy principle, and constrained maximum entropy principle",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 949.168,
    "end": 957.376,
    "text": " I mean, it's one of the justifications for providing that dual formalism between those two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 958.777,
    "end": 966.785,
    "text": "But another point I wanted to mention here is because I've seen that using the word hidden state,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 968.487,
    "end": 979.897,
    "text": " can be a bit confusing for some people because when we observe something as an observation, obviously it is not quote-unquote hidden, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 980.317,
    "end": 986.423,
    "text": "So what a hidden state here refers to is actually the hidden cause of that observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 986.503,
    "end": 989.746,
    "text": "So it's not that the observation itself is hidden,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 990.166,
    "end": 992.807,
    "text": " hidden from the observational or it's unobserved.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 993.307,
    "end": 1005.491,
    "text": "So that might be a bit confusing if we don't take into consideration the exact meaning of the hidden state or latent state here and in the rest of the literature.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1006.658,
    "end": 1019.406,
    "text": " So continuing from section 2.6, here we see one of the two central equations of active inference, which is equation 2.5.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1021.957,
    "end": 1023.778,
    "text": " for variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1024.319,
    "end": 1049.574,
    "text": "So it's, as I said, I mean, understanding this equation and how each line of its formulation represents in terms of the, I mean, trade-off between energy entropy or complexity and accuracy or divergence and evidence is key to understanding almost everything in the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1051.044,
    "end": 1054.767,
    "text": " rest of the book and in many other literature on active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1055.667,
    "end": 1060.111,
    "text": "So this is the perceptual part of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1060.131,
    "end": 1061.992,
    "text": "So variation-free energy is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1063.889,
    "end": 1077.235,
    "text": " parameter that is the notion that parameterizes the surprisal of our perceptual information we have about the external states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1078.655,
    "end": 1082.237,
    "text": "And then we'll see in the next section the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1084.918,
    "end": 1094.666,
    "text": " related and almost symmetrical formulation to variational free energy, namely expected free energy, which is basically the action",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1095.236,
    "end": 1096.817,
    "text": " part of the active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1097.117,
    "end": 1110.967,
    "text": "So we can see how those two can somehow be seen as a kind of unified formalism, but described in the alternate expressions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1111.527,
    "end": 1119.853,
    "text": "But another thing about equation 2.5 is it may be a bit, I don't know,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1120.493,
    "end": 1128.916,
    "text": " daunting to see all the relations between those three lines of equation and how we can get from one to the other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1128.936,
    "end": 1149.043,
    "text": "So there are some supplementary materials that we have developed in the past weeks, which I think can help in clarifying how the derivations of these three lines of equation be done so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1149.831,
    "end": 1160.14,
    "text": " I hope they would be clarifying and help to understand how those three lines relate to each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1164.143,
    "end": 1165.524,
    "text": "But the key point here is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1168.075,
    "end": 1177.337,
    "text": " to understand that variation of free energy is not something absolute, but it's just an upper bound for the minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1177.797,
    "end": 1189.939,
    "text": "So as Daniel just mentioned, it's untractable to have an absolute amount for the surprisal to be minimized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1190.38,
    "end": 1197.701,
    "text": "So we need to have an upper bound in order to make that more tractable because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1199.04,
    "end": 1223.867,
    "text": " By Jane's inequality, as we saw in chapter 4, I think, we can see that how the upper bound of a surprisal necessarily provides a condition for minimizing the precise or the exact free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1226.807,
    "end": 1240.358,
    "text": " And that's the key insight of equation 2.5, or the notion of variation of free energy, which is to provide this upper bound instead of the exact amount of surprisal to be minimized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1241.633,
    "end": 1252.868,
    "text": " Yeah, what I'll add there is if you knew exactly how well you should be surprised by a given data point Y, then you would have had the optimal model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1253.669,
    "end": 1257.154,
    "text": "However, that is not tractable, and so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1257.955,
    "end": 1278.309,
    "text": " by making a quantity that's always higher or an upper bound, the variational free energy F, which is a function of broadly Q, our beliefs, our variational beliefs, which are built in a way that makes them very compositional, very optimizable, very interpretable, and data.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1279.31,
    "end": 1305.312,
    "text": " and we can reduce the divergence here the kl divergence with a double line between q our beliefs and p the kind of actuality of it and if we can reduce this divergence in other words minimize the free energy then we will come closer and closer to the true surprise function and do that in a tractable incrementally optimizable way so equation 2.5",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1306.753,
    "end": 1328.319,
    "text": " is going to be the variational free energy different ways that it can be represented as it takes in data and beliefs about the world q and then this is going to be expanded into the future to include action with g the expected free energy",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1329.198,
    "end": 1331.439,
    "text": " Now, there's a lot more that we can say about this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1331.459,
    "end": 1346.967,
    "text": "There's a lot of technicalities to go into, but broadly, notice that G, the expected free energy, is a functional of policy, pi, because it's only being evaluated to select amongst different action outcomes, pi,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1348.027,
    "end": 1374.919,
    "text": " and another important difference is that it's going to be describing sensory outcomes that haven't yet happened a sort of what would i perceive if i did a or what if i did b and it's that kind of comparison that allows the expected free energy functional here to be used in action selection or policy selection as inference planning as inference",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1377.799,
    "end": 1397.655,
    "text": " that section is expanded upon and in figure 2.6 we see a very nice representation of the expected free energy equation and then how when certain aspects of this equation or situation are zeroed out we get certain other familiar cases for example",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1398.596,
    "end": 1407.159,
    "text": " where there's no epistemic value, there's no information to learn, then you get ruthless expected utility theory.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1407.859,
    "end": 1422.645,
    "text": "And conversely, where there is no pragmatic value to extract, so all outcomes are equally valid or preferable, then you get things like InfoMax principle and optimal Bayesian design.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1423.385,
    "end": 1426.169,
    "text": " And then everything in between is the space that we're interested in.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1426.409,
    "end": 1441.708,
    "text": "And so this figure 2.6 shows that the expected free energy functional can be seen as like a generalization of a lot of other settings related to perception and action and planning amidst uncertainty.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1442.99,
    "end": 1464.118,
    "text": " and section 2.9 closes the low road they took us all the way to active inference from bayes theorem through the generative model onto active inference and clarifies these two notions of variational free energy that's the real-time perceptual unfolding evidence lower bound on surprisal tractable optimizable and so on",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1465.038,
    "end": 1474.185,
    "text": " And F and G, the expected free energy, which is able to do planning as inference or policy selection as inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1475.065,
    "end": 1481.59,
    "text": "Expected free energy is fundamentally prospective and that enables counterfactual cognition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1483.228,
    "end": 1505.872,
    "text": " section 2.10 summarizes active inference is the theory of how living artifacts underwrite their existence by minimizing surprise or attractable proxy to surprise variational free energy via perception and action and they motivate that from a first principles base theorem starting place any closing thoughts on chapter 2 ali",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1508.772,
    "end": 1525.396,
    "text": " I would just humbly suggest for the people who want to go through this chapter to try their best to really understand specifically what equation 2.5 and 2.6 represents and how to use those equations to describe different",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1535.777,
    "end": 1559.8,
    "text": " situations with uh some uh missing elements as well uh because uh i believe those sections and uh particularly those equations are absolutely essential uh to for understanding uh everything active inference related both in this in the rest of this book and uh in rest of the and in almost all",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1561.308,
    "end": 1561.568,
    "text": " Thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1561.748,
    "end": 1567.373,
    "text": "And just the last thought I'll give on chapter two is this is exactly the work that we do in the Active Inference textbook group.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1567.773,
    "end": 1569.495,
    "text": "We really welcome all backgrounds.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1569.755,
    "end": 1573.278,
    "text": "Every single question and uncertainty you have is beautiful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1573.738,
    "end": 1579.222,
    "text": "We have a lot of resources that Ali and others make to make the math approachable and rigorous.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1580.303,
    "end": 1597.63,
    "text": " and natural language descriptions of the equations and so on so yes it's really important to understand the equations because after all that's like the skeleton that gives meaning to our usage and fluency of the active inference ontology which ali and i are speaking right now",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1598.806,
    "end": 1602.47,
    "text": " We're not just saying surprise is related to this because we felt it that way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1602.49,
    "end": 1612.642,
    "text": "There is an underpinning and it is a really interesting life's work to explore it, but we're finding ways to communicate it and learn and teach it better and better every time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1613.703,
    "end": 1614.644,
    "text": "So that's chapter two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1614.804,
    "end": 1615.425,
    "text": "That's the low road.",
    "speaker": "SPEAKER_01"
  }
]
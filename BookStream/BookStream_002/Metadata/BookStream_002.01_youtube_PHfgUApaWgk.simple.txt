SPEAKER_01:
Moving on to chapter three, the high road to active inference.

And so recall that the high road is like the why.

So Ali, please begin, just go as long as you want on through the high road to active inference.


SPEAKER_00:
Okay, so the high road to active inference

begin with the question about the conditions for the persistence of quote-unquote things.

I mean, what would we expect for a thing to behave if it persists through time?

And that's why sometimes FAP is referred to as the theory of every thing.

space thing and it's not absolutely everything but think just in this sense of uh i mean persisting through time and uh the notion that allows for this

thing to act as it as it persists through time is markup blanket so markup blanket is one of the most essential ingredients of the high road approach to active inference and that's why this section opens up with

describing how Markup Blanket allows to describe the situation of interest through this conceptual and mathematical tool.

But one thing that I always point out in almost all our textbook cohorts is

In this section, it doesn't exactly describe markup blankets rigorously enough in terms of its carving up the state space and so on.

So one thing I think can be clarifying in following

the discussions around Markov blanket is to keep in mind that it's just a boundary in the state space so it's not a necessarily spatiotemporal boundary although in many cases it manifests itself as spatiotemporal boundaries such as the

I don't know, cell membranes or organelle membranes and so on.

But it's not necessarily the case.

So I think it can be helpful to keep that in mind everywhere we see markup blanket.

But in a very simply markup blanket,

is what allows for the agent or the system to be statistically separated from its environment as is shown here in figure 3.1.

So basically Markov Blanket mathematically is just

the sensory and active states together, it consists of both of those states, and it statistically separates what happened externally or internally from each other.

So, in other words, internal states cannot observe or infer anything about the external states directly.

but only through the Markov blanket.

And that's why it's essential to describe the systems in this way, because obviously, in any sparse coupled systems, there isn't the possibility to observe the external states directly, but only

through those sensory and active states.

And one other thing that I also believe can be a bit confusing in this picture is that, yes, typo in the active states and sensory states, because it's important to observe that active state only, or the flow in the active states more precisely, only depends on

internal n-Markov states or blanket states.

And on the other hand, sensory states only depend on the external n-blanket states.

So mu and x in those two equations should be exchanged.

Yes.

Yeah.

Let me just unpack this.

Anything to add before we continue?


SPEAKER_01:
yes so this is what's known as the particular partition and that terminology was brought to the fore by carl friston's famous monograph in 2019 and it's a little bit of a pun it's a particular partition because this is just one way to partition agent from environment figure from ground

And we call what it partitions out, the blankets and internal states, as the particle.

So we can think about the most inert, least cognitive particle is like a speck of dust doing Brownian diffusion.

But also, there are more sophisticated kinds of cognitive particles that can include...

world models counterfactuals what would happen if i did this all of those kinds of sophisticated cognitive operations that we can explore in active inference so note the small typo that ali mentioned internal states and also these edges reflect causal possibilities

amongst internal external and blanket states blanket constituting the active u and sensory y states so these are map not territory this is not a spatio-temporal

articulation these are like causal maps of the world that may have to do with spatial temporal boundaries but are not simply that and sensory data or sensory states flow on to internal states internal states are involved in action selection resulting in active states

which have some causal consequence in the world, the generative process, the external states, which results in different sensory states coming in again.

And so the Markov blanket is what makes the internal and the external states conditionally independent.

And that can be thought of as just a no telekinesis, no telepathy clause.

The only way the information comes across internal and external states, which are symmetrical with each other,

is through the boundary or the holographic screen or the Markov blanket.

You can continue on, Ali.


SPEAKER_00:
Okay, so the next section is...

about surprise minimization and self-evidencing.

So again, this term self-evidencing is a common term in active inference literature first proposed by Jacob Howey.

So basically it refers to how a system or agent, or in other words, a particular state can

gather the evidence for its self-persistence or self-existence through time.

In other words, by inferring the state of the environment and comparing that inference with its internal states, or in other words, with its generative model, it's basically an interpretation of that

that kind of inference is on the one hand there is this dynamical interaction between the internal and external state but we can somehow interpret that physical dynamics as engaging in the active inference or

as a model for the persistence of the agent through time.

So that's basically what refers to as self-evidencing.

And then we go through the subsection 3.3.1, which sets the stage for the recent formulation of active inference as in mechanics.

which is to somehow to interpret the act of surprise minimization as minimizing the action through the Hamiltonian principle of least action, which is a variational principle.

And by variational principle is what is meant here is just a computational or mathematical tool

that allows for the computation or the specific derivations to happen.

identical to scientific theories or scientific i don't know facts or observations it's just a tool a principle mathematical uh tool uh so again one of the main um

misconceptions about free energy principle is that it is unfalsifiable.

Obviously, if we see it in this way, it doesn't make sense to say that a mathematical tool or principle is unfalsifiable because

it doesn't say anything about the empirical evidence of the phenomena we're talking about.

So that's why here it's important to understand how the surprise minimization can be seen as this kind of variational principle.

And then equation 3.1 draws the parallel between

the surprisal as defined in section in chapter two, sorry, and this chapter.

So it kind of ties up all the arguments provided in the previous chapter with this one and how everything comes together in a single formulation of active inference.

But they're just the two distinct approaches to arrive at same destination.

then we go also sorry the other important equation and again another key equation here is equation 3.2 which is a kind of ex sorry yes equation

I meant to say equation 3.2, not equation 3.1.

Sorry, my bad.

So yes, the other section is about the relations between inference, cognition, and stochastic dynamics.

I mean, all the previous discussions around how the act of inference can be seen as a kind of self-evidencing through the variational

principles such as FEP comes together to reframe the previous discussions we saw in chapter two about perception as inference and action as inference and to see the concepts of variational free energy and expected free energy through the lens of variational free energy variational

principle of least action.

So it unifies nicely all the material from chapters two and three into something that, I mean, not necessarily distinct from each other, but just two sides of


SPEAKER_01:
Awesome.

Yeah, I find Table 3.1 to be very exciting.

It draws together statistical physics, Bayesian information, information theory, and cognitive interpretations.

So it's kind of like learn one thing, learn many things.

statistical physics has been talking about minimization of variational free energy for a long time and such methods are absolutely every day and professionalized in bayesian statistics active inference is using exactly just that to describe perception and action and so on so that's very um exciting box 3.2 describes

free energy in statistical physics and active inference.

I sometimes joke that we have a few kinds of free energy.

We have Tesla, like electrical power should be available to everybody for no cost.

We're not talking about that right now.

There's Gibbs free energy, which is the quantity that makes chemical or thermochemical reactions irreversible.

like the hydrolysis of ATP.

And when we're talking about variational free energy, we're talking about an information geometric space with similar dynamics, similar kinetics and thermodynamics, but rather than describing the reaction coordinates of a chemical reaction, we're thinking about it in terms of Bayesian updating, and this is all called the Bayesian mechanics.

3.41 continues on with variational free energy.

3.42 goes into expected free energy.

So we see this a lot.

F, variational free energy.

That's the real-time unfolding sensory flow.

And then G, expected free energy and the policy planning as inference.

Section 3.5 concludes with a novel foundation, active inference, to understand behavior and cognition.

And it describes a few features of active inference, including its distinguishing features from a few other ways that people have looked at behavior and cybernetics.

Section 3.6 goes into a bit more detail on models, policies, and trajectories having to do with agency and policy selection.

3.7, reconciliation of inactive, cybernetic, and predictive theories under active inference.

Again, the exact kind of thing that there's a whole literature on and it's always amazing to hear everybody's perspective on in the textbook groups.

3.8, active inference from the emergence of life to agency.

And 3.9, summary.

You want to just give any other thoughts that you have on chapter three.


SPEAKER_00:
Again, it's one of the probably most fundamental chapters and

I mean, the topics covered in this chapter is essential or absolutely essential to understand everything active inference related, but specifically the discussions in section 3.6 onward, again, I believe is really important to understand the broader context

of active inference and how it relates to all the other theories, but specifically section 3.8 provides a nice view of how we can

use the same modeling to the same theoretical tools to model both non-living dynamical systems or sparse coupled dynamical stochastic systems and also to

the systems that has agency or sentience so it's a much broader theoretical framework that doesn't restrict itself to only particular kinds of systems or agents and we'll see much more elaboration on that in the later wonderful


SPEAKER_01:
right that concludes our overview on chapter three
SPEAKER_00:
hello and welcome it's february 20th 7th 2023 we're here in active inference book stream number 2.1 with thomas parr as well as terry and ali so thank you all for joining this should be a very fun discussion and let's go first by way of introduction and beginning to thomas

Welcome, and please, how did you come to write this textbook in collaboration with Pizzulo and Friston?


SPEAKER_03:
Well, again, thank you very much for having me here.

How did we come to write the textbook?

That's a good question.

I think Giovanni was the person who really had the idea and pushed for this, but I think Carl had already

previously been approached by the editor who ended up managing the publication of the book.

And when Giovanni had the idea that this would be a really useful contribution, he and I had a number of discussions about it, both in London and in Rome where he's based.

and sort of put out a plan and sort of came up with something that we hoped would be useful for the community.

I suppose part of the reason why we thought it'd be a useful thing to do is that active inference is a very multidisciplinary type field that draws a lot from lots of different domains.

And I think one of the struggles

know we all had when we started out in it and i think certainly what students and people who are interested in the topics say to us is that it's often quite difficult trying to find all of these different topics trying to find the right background when they don't all come from one field so putting it all together in one book with a consistent notation consistent language and trying to make it complete we hoped would uh just make the field a little bit more accessible um to others who wanted to get involved


SPEAKER_00:
How did this dovetail or integrate with your own education and research directions?

What phases or stages were you moving through while this discussion was happening?


SPEAKER_03:
When we initially started discussing it, it must have been probably the last year of my PhD.

By that stage, I'd spent

three or four sorry two to three years sort of getting to grips with active inference having spent you know being relatively recent in terms of learning some of the key material myself I think it was quite a good stage to be at in terms of thinking about what I would have wanted to know a few years previously

The actual writing of the book then carried on over the next year or two.

So by that stage, I was back in clinical training as well, which I think is also a useful perspective in terms of some of the later chapters and thinking about application of active inference type models in the context of clinical conditions and psychiatry and neurology and those sorts of areas.


SPEAKER_00:
That's actually a really interesting note.

How did the clinical experiences give a perspective on the book or how did the book or your learning in these abstract and theoretical domains give a different insight in the clinical setting?


SPEAKER_03:
It's a good question, and I think you're right to highlight the bidirectionality of it.

Both are useful in different ways.

I think, for me, the clinical setting is quite useful in terms of thinking, first of all,

interacting with different sort of audience.

So people who may be much more skilled on the clinical side of things, maybe had less experience on the mathematical computational side of things, but may still find the concepts very useful.

So from that side of things, it's quite a useful exercise in being able to communicate these things.

And then there's sort of the interaction on the patient side of things and trying to, you know, I think it makes a big difference, first of all, in

interacting with patients has an impact on how you might think about the interesting questions and the things you might want to use that to inference for but also having an understanding of principles behind it helps you I think formulate your questions about how individual patients might present or the reasons for that I think is a useful way of thinking about I mean I suppose on two levels so one in terms of understanding

understanding particular sorts of pathology in terms of the inferences that might have gone wrong, but also in terms of understanding your own active inferential processes when you're trying to work out what's going on yourself, asking the right sort of questions to gather the right sort of information and test those internal hypotheses.

I think that's a very useful way of thinking about being a doctor.


SPEAKER_00:
awesome yeah a lot to say about that the bayesian optimal differential diagnosis ali or terry on any of these thoughts do you have any questions or comments you want to raise there or any follow-ups


SPEAKER_01:
Well, yeah, actually, I just wanted to ask, what are the intended audience of the book?

And what do you think are the minimum prerequisites for going through the whole book?

Because I'm sure you'll agree that even today, the intersection between the people who are well-versed in mathematics and the people who have a solid background in, say, neurobiology are

uh quite narrow so um so what's the solution here and what did you think about about the organizational structure of the book that could accommodate various audiences so um yeah


SPEAKER_03:
Again, another really good question and I think you're right to highlight that that intersection is relatively narrow and there are lots and lots of people who I think struggle to feel involved in those kinds of discussions and you're absolutely right to say that and I think one of the solutions is the sorts of things that you guys are doing in terms of setting up these reading groups, trying to open that up, trying to provide educational resources.

I think that's really very important.

In terms of the specific questions about the book itself, who's the audience?

And what are the prerequisites?

I think it sort of depends what you want to get out of it and what your purpose is in reading it.

So some aspects of the book I suppose will appeal more to some audiences and others to other kinds of audience.

I would hope that the readership or the audience is relatively broad because we've tried to provide as much background as we can on sort of the mathematical and the neurobiological side of things to try and complement the skill set of whoever's not in that intersection who comes to it, who may need a bit more sort of neurobiological grounding or who may need a bit more mathematical and technical grounding.

But I think ultimately,

Like with most textbooks, you don't necessarily need to come to it thinking, I'm going to understand everything in this book when I come away from it.

Some people may want to, but other people, depending upon what they want to get out of it, they might say, actually, what I really want to get out of this is an understanding of some of the kind of message passing as it manifests in the brain.

Whereas other people might come to it and say, actually, I already understand a bit about that.

And what I want to get out of this is to understand a bit more of the technical detail.

If you're a roboticist coming to this, it probably doesn't matter too much about how things are implemented in the brain.

If you're a neurobiologist coming to this,

you may not need to know the details of the implementation as long as you can understand it conceptually.

And I hope the organisation, particularly putting things like the appendices with the detailed information at the back, tries to keep most of the text a bit more accessible, the early chapters being relatively light on the technical material and building up the concepts before then moving into the more technical aspects.


SPEAKER_00:
about the uh chapters let's actually just take a quick look at the chapters um we're in the uh coda document that we use so perhaps you could just say one or a few sentences on each chapter what's the short of each chapter what is it doing in this 10 chapter work


SPEAKER_03:
You're testing my memory now.


SPEAKER_00:
The titles are there.

The cues are there.

Epistemic cues are there.


SPEAKER_03:
Okay.

Well, the overview is what it says on the tin.

It's a description of what's to come in the subsequent chapters to try and orientate people.

And the low road and high road, I think, again, Giovanni has to take credit for this idea of saying, actually, there's quite a, it's often difficult to know where to begin when you're trying to understand active interference and the free energy principle and the Bayesian brain and all those sorts of things, because it seems like there are so many potential starting points, all of which ultimately, I think, get at the same thing.

But knowing where you begin is often really quite difficult.

And so these two chapters were designed to explicitly acknowledge that difficulty.

So the idea of the low road is to say, well, let's take a pragmatic perspective where we take ideas that have developed in neuroscience based upon our understanding of the brain and of psychology and models that have been developed to explain those things.

And let's take those to their conclusion, which is, in a sense, the free energy principle, which is active inference, the idea of using our internal models both to drive behavior and to understand what's going on in the world around us.

The High Road perspective was designed to take a different approach to that.

And instead of starting from what's already been developed in the sort of natural trajectory in the neurosciences, the High Road chapter was more saying, let's start from first principles as much as we can.

And here was a tricky balance to try and make sure that we didn't go too far in depth, that we put people off by the third chapter, but enough that people might get a bit of a conceptual sense of where you might go with this.

And here we sort of started from the kind of physics of self-organisation, the ideas that come underneath that, and how those ideas can then feed back into this same idea of active influence that the neuroscientists and psychology seem to have arrived at relatively naturally.

I suppose chapter four is when things start getting a bit more technical.

and starts dealing with the specifics that I think most people would want to know if they were implementing these models themselves.

So trying to understand exactly how you construct an internal model, how you specify it, write it down mathematically, how you perform inference with those, how you understand the message passing schemes.

chapter five then takes a step back and thinks, well, in the context of the brain, in the context of neurobiology, what do those things actually look like?

How do we understand those inferential processes with different kinds of generative models in terms of what we know about message passing and synaptic communication?

And those five chapters together really are the first part of the book.

And the subsequent five chapters really then take you through

the idea of actually applying those ideas in particular domains.

And so chapter six is sort of the overview chapter looking at the rest of it.

And it's really setting out if you wanted to sit down and design a model, either conceptually or computationally, these are the steps you would have to go through to construct that.

And then chapters seven and eight take you through that sort of recipe for several examples, building up different concepts.

both for models designed in discrete time, where you're formulating things as a sequence of events, and models formulated in continuous time, where we work with continuous dynamical systems and differential equations.

Chapter 9, as I mentioned before, one of my interests is in the clinical domain and trying to understand what sort of models people use to perform inference, how those can go wrong.

And part of the research on that is thinking, well, how do we then fit these models and use them to actually draw inferences about what's going on with particular individuals or groups of individuals?

And then chapter 10 takes the broadest perspective and looks at how that relates to a range of other theories and ideas, both in the neurosciences and beyond.

And then I'm sure the appendices speak for themselves.


SPEAKER_00:
Awesome, great overview.

Let's come back to our questions.

So you laid out that the first half of the book is about learning and the second half of the book is about applying and it begins with that recipe.

so if chapter six is the recipe as a chef and sous chef yourself what else have you found was important in the restaurant what is your full stack active inference modeling look like and

Maybe summarizing in six or going beyond what was in six, like what have you found to be important as part of the modeling journey, especially with teams where people might have like different levels of domain specific knowledge or active knowledge?


SPEAKER_03:
I like the restaurant analogy, actually.

And I was thinking, I suppose the first thing that anybody needs to do if they're setting up a restaurant or a model is thinking about who their customers are and what they might want.

And so in the scientific domain, you're thinking, well, what sort of journal am I planning on submitting this to?

What sort of questions are the communities who read those journal articles interested in?

And I think that's a really important first step so that you know what I'm actually trying to do with this model.

What's not such a good idea often, although you can find interesting things by doing it, is to say, well, I'm going to develop a model and work out what it's for later on.

You can end up sort of tying yourself up a little bit by doing that.

Although there is value in exploring as I think we would all agree.

Then

I suppose in terms of ingredients, it's relatively straightforward and deliberately so.

The idea of these sorts of models is that they should be relatively accessible, relatively easy to construct without having to make use of large supercomputers or anything like that.

A lot of the models that I develop can be run on a laptop fairly straightforwardly.

So, you know, obviously just picking your computing language and how you're going to construct that, there are now a couple of different options out there.

MATLAB being the one that I think is most well developed, but now lots of people developing Python-based implementations of active inference schemes, and particularly in the Markov decision process realm.

So that gives you, I suppose, some of the key ingredients, but then it's really sitting down and thinking, well, how would I now actually construct this model?

And the key question I think often is, if you have a particular phenomena or a particular task in mind that you want the model to perform, it's thinking, well, how would I go about actually constructing the data that's presented to a participant performing that task or presented to an agent that's trying to perform this task?

And if you can explain how those data are generated, then you effectively have the generative model that implicitly your agent or your participant or whoever else should be using to be able to perform that task.

So really, all you need to be able to do is to work out how to create the experimental stimuli.

And once you've done that, you've almost already created your model.

And the rest comes from predefined, preexisting inversion schemes that actually minimize the free energy for that model.

So at that stage, it becomes relatively straightforward.

I say relatively because as anybody who has experience in coding knows, a lot of time is spent debugging and throwing up errors and spending a while trying to work out why, a little bit of Googling to work out a solution around that.

I think those are all important parts of the process.


SPEAKER_00:
Awesome.

Ali or Terry, on that recipe and modeling question, any thoughts on that?


SPEAKER_01:
Actually, I also wanted to ask if you have a plan for extending the

codes or supplementary materials to include other languages such as IMDP packages or any other packages that might come up later and

would be widely used in Active Inference.

Because as far as I know, there isn't any specific GitHub repo or something dedicated for this book so far.

So do you have any plans in developing such a supplementary material for the book?


SPEAKER_03:
We don't at the moment.

It might be an interesting thing.

I mean, it might be a useful thing to be done by the active influence community as well as a way of sort of being engaged and involved in it.

I suppose one of the difficulties of doing that for a book is that often these things move on so quickly by the time it comes out in a book that those packages have moved on.

So I think it was useful for us to give a simple example of MATLAB implementation to demonstrate the principles in one of the packages that we've been involved in developing.

So as part of the SPM package where we've been involved in developing the active instance routines as part of that.

And I think it's certainly a good idea thinking about having other repositories out there with all sorts of demonstrations.

So the SPM package does have a range of different demonstrations for different sorts of

of implementations as well for different kinds of models, which is a really helpful resource.

And I often suggest to people that one of the first ways of getting a sense of how to build these models is to look at one that's already been built and do their best to break it and change things and make it behave in different ways.

And so I think you're absolutely right to say examples of the code and how it's implemented are vital in bringing people into this.

But yes, I think you're absolutely right to say that it would be really helpful to have both Python and Julia implementations and all sorts of different ways of doing it and examples so that people can pick those up.

I agree completely.

And of course, one of the advantages of Python and Julia is that they're open source and you don't necessarily need an academic license to use it, which again, I think increases the number of people who can access it.


SPEAKER_00:
Giving an overview to someone who might not have been following in the blow by blow of our little literature corner over the last few years, how would you say active inference modeling has evolved during your time in the space?

Where are we at?

just generally speaking, with applications?

What applications are developed or possible today that when you were starting your PhD seemed like they were just implausible?


SPEAKER_03:
So I think if we go back a bit, I suppose some of the earlier implementations were almost all in the continuous domain.

So they were all based upon...

things like the generalized filtering type algorithms that were initially developed as sort of filtering methods to perform time series inference.

Those things then

equipped with an active component then became some of the earlier active inference type simulations, and a lot of that focused on things in the movement domain, whether that be eye movements or control of limbs to the extent that you can develop handwriting and those sorts of things.

One of the interesting things that started to happen based upon that was to think about how you then get something that looks more sequential emerge from those continuous dynamics.

And handwriting was actually a really good example of that, where there were some simulations developed in which

in which the continuous model effectively had something a bit like a Lotka-Volterra dynamic in it.

So Lotka-Volterra dynamics are a kind of dynamical system often used for modeling things like predator-prey interactions, and the idea is that you get a peak in terms of a predator population

Sorry, prey population followed by a peak in the predators as they eat the prey, causing drop in the prey population.

But then as the predators die out, the prey come back and you get this sort of sequence of peaks from predator, prey, predator, prey.

And you can generalize that to then have multiple different populations, each preying on one another.

So you get this sequence of peaks over time.

And that sort of emergence of sequences from continuous dynamics, I think, was one of the key moves early on in terms of thinking about things through a bit more cognitive in terms of how do I do this and then this and this and start to plan and generate much more purposeful type of behavior.

So that went so far until things moved much more to an explicitly discrete or sequential style of modeling.

That's when things like Markov decision processes started to be employed in the active inference domain or partially observed Markov decision processes.

So those are models that say, I assume my world is a sequence of states over time

one after another, each of which generates some observations.

And from those observations, I can make inferences about the states.

You can then say, well, if I can act upon the world, I can change the sequence of states as they evolve over time.

And by selecting different actions or different ways I can change that sequence, I'm effectively planning and making decisions.

So that's, again, where you get into a much more cognitive sort of domain.

Some of the key advances sort of early on while I was doing my PhD were the emergence of things like deep hierarchical models of things like Markov decision processes.

So here the idea was that in the world around us, there are lots of things that evolve over different sorts of timescales.

And so one of the early examples of this was reading, the idea that when you move your eyes around the page, you're sort of looking at an individual word or the letters within that word, but that word is itself part of a sentence.

And the time it takes you to infer what the word is, is going to be much shorter than the time it takes you to work out what's going on in terms of the sentence.

and the sentence and the paragraph and the paragraph and the page and the page and the book and etc.

So there's a whole hierarchy of different time scales in the world around us.

And one way of accounting for that relatively simply in these sorts of models is to simply stack them on top of one another.

So the outcomes you're now predicting from one of the Markov decision process type models I was describing

now reflects a sequence of very fast states.

And each of those states, yes, exactly, is the graphic of this.

And then each of those outcomes predicts another sequence of very fast states.

So what you're left with is, at the very top level, a very slow progression of state after state after state.

For each of those states, which might be the sentence, you then have the words in that sentence beneath it.

And then for each of those words, you might have each of the letters in that sentence.

Sorry, each of the letters in that word.

And so that then sort of expanded the range of things that we can do in terms of how we give some sort of deep temporal structure to the kinds of models we develop.

Sometimes people refer to that as kind of breaking the Markov property.

So for those who aren't familiar with this, the Markov property is the idea that each step in time depends only upon the step immediately previous to it.

That all you need to know about the next time step is what's happening at the current time step.

Nothing else in the past is useful once you know that.

And people often suggest that means that these processes are effectively memoryless because it's only where I am now that matters.

It's not what was happening well into the past.

By having a hierarchical structure,

where you have a higher level state that is more slowly changing, that's contextualizing what's going on at the lower level, it means that the fast sequence of changes is actually dependent upon something much slower that's happening.

And that in itself is dependent upon something much slower.

And that gives you a sort of memory.

It allows for the Markov property to be broken.

So like I said, we're sort of starting off with continuous, the emergence of discrete from continuous, the explicit modeling of continuous models, decision-making, and the sort of exploration, exploitation type trade-offs that you get once you treat planning as being an inference process.

The idea that you can then stack these things hierarchically and have a range of different timescales that break the Markov process, allow us to think about things like memory and working memory.

And then I suppose one of the next stages was the combination or the reintroduction of the continuous models as the lowest level of one of these hierarchies.

The idea that

Actually, it's all very well being able to make decisions, but at some stage you need to implement those decisions in a world that's made up of physics and muscle lengths and continuous sensory inputs.

And so putting these sort of hybrid models together where you can use your decisions to then predict short sequences and trajectories defined in a continuous domain.

And I suppose since then, there have been a range of further developments in active inference in that setting and a range of developments in a number of other directions and sort of in parallel.

So some of those developments have been thinking about implementation in other languages.

Some of those developments have been thinking about combination with methods from deep learning and the introduction of variational autoencoders and function approximators and amortized inference, which a lot of people in the robotics community have been pioneering particularly.

So that would be one direction that things have been developed in.

And I suppose the other direction that's seen a lot of development recently has been the sort of underlying principles, the physics that underwrites active inference, the principles and how you justify the sort of dynamics and where free energy comes from as a concept and why it's useful in understanding how systems behave.

So that...

I hope that wasn't too much all at once, but there was a sort of run-through of some of the developments that spring to mind over the last few years.


SPEAKER_00:
Awesome.

Perry or Ali, any comments on that quick historical review?


SPEAKER_01:
Again, I had another question regarding... Actually, it's a question out of mere curiosity.

Did you have any specific reason for adopting this particular mathematical notation throughout the book?

Because it seems like...

somewhat diverges from the notations used in some of the well-known papers in the literature.

And I know active inference notation is not

consistently used across the literature, but even in the papers that came out after the publication of this book, written by Professor Friston or others, I don't see this specific notation used a lot in those papers.

So I just wanted to ask about the reasons behind this choice.


SPEAKER_03:
Okay.

I mean, I think you're absolutely right to say that it's not always been used consistently.

And it's worth saying that in some cases,

There are a couple of different tensions that affect this.

So one is the idea that in a lot of papers that are published, because it's only dealing with an aspect of active inference and not trying to put it all together, they can sort of use a notation that may be the same as notation you might use in another field without worry that people will get conflicted and misinterpret one variable for another because everything's just defined within that one domain.

Another aspect to it is that, depending upon the paper in question and the audience it's aimed at, sometimes the notation is designed to match what might be used in that broader field, whereas we've tried to make it as consistent as possible across different fields.

Were there any particular notations that you were thinking of or that I can help clarify?


SPEAKER_01:
Yes, so for example, as just a very simple example, using eta for external states instead of x that's been used, or s and a for active states and sensory states, or other kinds of uses of lowercase matrices and uppercase matrices, which is used differently in a paper,

such as a step-by-step tutorial on active inference and these kinds of notational divergences.


SPEAKER_03:
Okay, so yes, I mean, those are some quite nice illustrations.

So things like A and S, which, as you say, are often used in some of the more physics-orientated papers.

Unfortunately, if you use those in a Markov decision process paper, people will interpret those as being states and parameters of a likelihood matrix.

So unfortunately, we couldn't use those same letters without causing more confusion.

So what we try to do there is to focus more on the sort of practical inference side of things, of the actual implementations we tend to use of active inference in particular problem settings.

And there, when you're constructing a generative model,

particularly if it's in the continuous domain, will often use x as being the states that I have to infer, which from the perspective of the physics type papers are the external states, the things that are having an effect on my sensory states.

We've

I can't remember whether we used Y or O for the sensory states or the data in the book.

I think it's Y. I think it's Y, okay.

And that sort of fits with the idea of using it in the continuous domain, where in a lot of settings when you're trying to predict sensory data, often Y will end up being the variable that we end up using.

So I appreciate that it can be confusing, sort of jumping between different resources.

But what we strove for in this one was internal consistency as much as we could and trying not to overload specific variables or symbols.


SPEAKER_00:
Thank you.

Thanks.

figure 4.3 we bound ourselves coming back to again and again it four sages chapters seven and eight and it lays out discrete time on top and continuous time on the bottom in terms of their structural similarities but also they have some really important differences so could you just as you see it how do the discrete time and the continuous time models

have similarities and differences such that you can lay them out in a graphically or visually similar way, yet there's also some key differences, for example, in what each of the nodes mean.

So how are they similar and different?

And what did you hope to illustrate by laying out figure 4.3 this way?


SPEAKER_03:
So I suppose the reason that they can be seen as similar is that effectively what you're trying to do in both cases is to represent a trajectory.

We're trying to represent how something evolves in time.

And that something may be the states S in the discrete time, or they may be the trajectory of some hidden states X in the continuous domain.

And when you're trying to say, how do I actually

put together a set of numbers that tell me how something's evolved or evolving.

There are a couple of different ways you can do that.

One is to say, I'm going to tell you where it is at time one, and then I'm going to tell you where it is at time two, and then at time three, et cetera, et cetera.

And that's a perfectly legitimate and perfectly reasonable way of writing down a trajectory.

It obviously misses out all the times between time one and time two, which may or may not be significant depending upon what the problem is you're dealing with.

The second way that we would approach it is to say, well, you can use what's known as a Taylor series approximation or a Taylor series or a polynomial expression for that trajectory.

So essentially what we're saying there is that you just construct a function that follows the shape of that trajectory over time.

And you can construct that function by saying, well, let's look at where I am now at this point in time and my position.

How am I changing over time?

If you just took those two variables together, you'd just have a straight line telling you where you are and where you're going to be.

If you then take account of your acceleration, you may even have a curved line which says, if I'm here now, I'm going to be there then.

And as you add in more and more elements to this, so the rate of change of the velocity and the rate of change of the rate of change of the velocity, et cetera, and just add in more and more rates of change, you effectively get a more and more precise function telling you, at least locally around where you are now, where you're going to be at various other times.

So that difference, either using Taylor series coefficients, which are just successive rates of change,

or by using a sequence of states over time are what's illustrated in the lower and upper graphics here, respectively.

Each of those things has, as you said, they have a lot in common in terms of their structure, in terms of the number of numbers you need to be able to describe them.

but each has their advantages and disadvantages depending upon what you're trying to actually model.

And I think for me, I tend to think of the sequence of categorical states over time as being quite useful in thinking about anything to do with decision-making, deciding between alternatives, thinking about sequences like language and words and

or even sequences in terms of the eye movements you make, looking in one place, then the next, then the next, rather than continuously moving your eyes around space in a smooth fashion.

Whereas the process of interpreting sensory data that's coming in from your retina or from your stretch receptors or whatever else, or in terms of actually driving action that changes continuous variables in the world,

There I think it's much more useful to be able to describe things in terms of a continuous number and those sorts of continuous trajectories that you can get through using these Taylor series type approximations, which are often referred to in this context as generalized coordinates of motion.


SPEAKER_00:
Awesome.

Cool.

Well, another figure that we come back to again and again, tucked away in chapter 9, but giving a lot of context, is figure 9.1.

on what is termed meta bayesian inference so could you describe what is happening in this figure and is this exactly as every empirical ethologist is going to agree with is this controversial is this novel so what is it doing for active inference when we conceptualize our

empirical task structurally like this and do you think that that is exactly how ethologists have long seen this topic or is this something contentious


SPEAKER_03:
I think, so I'll talk through the figure, but then you can tell me if that's how ethologists traditionally look at it and whether it's contentious.

I'll be interested to hear.

So the idea behind this figure is to highlight that when dealing in psychology or neuroscience or fields where you're trying to understand how a particular system is behaving or how it's

essentially what's going on in the mind or the brain or the nervous system of some creature that is behaving in some way, there are two levels at which you can look at it.

One is saying, well, from my perspective as the experimenter,

What are the data I'm measuring?

And that may be behavioral measurements from that creature.

It may be taking electrophysiological measurements.

It may be asking it a question if it's a patient in a clinical setting.

And I'm essentially now trying to optimize my model of this system.

Now, in the neurosciences and in the settings outlined above, the kind of system we're interested in is the sort of system that has its own model of other sorts of system.

And so the kinds of questions we're interested in, the kind of parameters we're interested in, are things like the prior belief that this creature has about this thing, or how precise it thinks its data are.

And so the parameters I'm now asking a question about are the parameters of the beliefs of another system.

So the outside box here, the one that the parameters are going into, the experimental stimuli are going into, and the observed behavior are coming out of, is essentially my model of how this other system behaves.

Now the inside box here, the one in the dotted line or the dashed line, that now represents the model that that system has to understand the world around it.

And so when we construct our model, that contains another model inside it.

And by saying, if this creature believed this, this is how they would make inferences about their environment and generate behavior,

I can now predict the behaviour I would observe if this creature believed this thing.

So the subjective-objective idea is that the objective model is the experimenter and the experimenter's beliefs and the observations they make by measuring what the participant or experimental subject does.

And the subjective model is the participant's model of the world around them.

So that, I think, would be my summary of how this figure works and the idea it's trying to get across.

Now, you can tell me how ethologists may disagree with that or agree with it or what they'd find controversial.


SPEAKER_00:
I'll give a thought, and Ali or Terry, feel free to give a follow-up or a thought about

I don't think in principle it's contentious or dissentious.

However, there might be an explicit or an implicit belief, of course, ironically, that we can simply take some of these parameters coming into our experimental setup and even the selection of experimental stimuli, we can take those as simply given, which is a very frequentist perspective.

And then on the observed behavior, we can simply use descriptive statistics.

We'll characterize the predefined groups according to the one experiment we ran.

We'll do the t-test.

We'll do the ANOVA.

We'll come up with a p-value.

It's going to be frequentism all the way forward.

So even if we're doing frequentism about a generative model, there's a sense that if we just clamp down on our experiment and treat it as handed down without error, instead of being actually drawn from distributions of possible experiments we could have done,

if we clamp down upstream of the experiment, we'll just be able to describe and ultimately publish without needing to look at what else could have happened.

And I think this diagram and also the very subtle dashed line around even the bigger box

suggests that just like the active clinician engaging with a conversation or the active experimenter also engaging iteratively over their system of interest that itself is an adaptive epistemic process where

Yes, you could simplify it by treating the parameters and the stimuli sequences as simply given, but then you don't have access to the generative model that actually is the generative process for the subjective model.

So summarize in principle, uncontentious, however, sometimes discordant with the ways that behavioral experiments are conceptualized, planned, and analyzed.

One reason why it's helpful because during the book, we try to stay on the road, on the high road and the low road.

But when we're in discussions,

it's almost like there's a whole roadmap and people have their own backgrounds and questions and it kind of comes together with all these different um topics that people are curious about ultimately that bring them to active inference so going to this meta Bayesian figure

helps in some ways situate how the authors are seeing this entire process of experimental design, observation, and cognitive modeling.

And that was unpacked also in the recent map territory, fallacy, fallacy work.

Ali or Terry, any thoughts or questions in our last 10 minutes?


SPEAKER_01:
there a figure you want to look at or we can go to some other places uh maybe we can ask uh the missing figure in uh it was in i think it was in figure seven point yes yes or something 7.2 in the lower left


SPEAKER_03:
So this was actually corrected by, I don't know quite how it ended up vanishing, but it was corrected in one of the later print versions.

I'll see if it's in the version I've got in front of me now, and I can show you what it would have looked like.

But apologies for that.


SPEAKER_00:
Which figure is it?

7.2 on page 128.


SPEAKER_03:
Unfortunately, I picked up a copy of the book that doesn't have it.


SPEAKER_00:
So there have already been one and a half fractal dimensions of printing?


SPEAKER_03:
Yes.

Yeah, I think, unfortunately, I don't think that one's corrected yet in the online copy.

Essentially, what would have appeared in that figure or what did in some of the later printing versions is the errors that are driving the updates of the states in the plot above.

Now, because the... So the errors here are effectively the...

are the free energy gradients, or the negative free energy gradients.

So essentially, as you go down the free energy gradient, you're effectively minimizing your error.

So what that means in practice is that the errors would effectively be the rates of change of the states in the plot above.

So if you take the states in the plot above and you imagine what the rates of change of each of those lines are, that's what you would get in the plot below.


SPEAKER_00:
Well, on this same kind of topic, in Chapter 7 we're in the discrete time generative models.

And

little bit less so here but in the uh ladder models here in 7.13 and in what we just looked at in 7.2 where are these continuous interpolations coming from if it's a discrete time model


SPEAKER_03:
That's a good point.

So I suppose there's an important distinction to be made here.

There's the distinction between the things that are being inferred, which are happening over discrete steps in time.

And then there's the inference process itself, which deals in probabilities, which are themselves continuous.

They can vary, in this case, anywhere between zero and one.

And the time over which that inference happens.

So

To give you an example, if you imagine that the way you're analyzing this figure at the moment is by making a series of saccadic eye movements, you're not sort of continuously moving your eyes from one plot to the next.

You're making a series of rapid ballistic movements that take you from one to the next.

And effectively, every time your eyes stand still, you have a new data point that you're now trying to analyze.

You can imagine that between that, every time your eyes stay still, everything else in your brain doesn't just stop.

So once you've got that new data point, your brain now has some time to assimilate that data point, to draw inferences about what it's seeing there.

And those inferences happen through the continuous dynamics of what's happening in your neurons and various networks across your brain.

So you can think of the continuous changes that are happening for each state as not being changes in the world that we're trying to infer, not being changes in the data point I'm currently looking at, but the changes in my beliefs about it before I then sample the next data point.


SPEAKER_00:
Interesting.

I have a few more figure-based quick questions, but Terry, do you want to bring up anything or just give any remarks?

No worries if not.


SPEAKER_02:
Well, I am very much just trying to understand these concepts, so I don't make any pretence of having any deep understanding.

But the key thing that interests me is how you jump from this sense of how we update our predictions

into the clinical realm.

I'm interested in pain medicine and I see neuroplastic pain as very much a mistaken prediction based on often misinterpretation by medical professionals and the information that they give to patients.

And I'm really intrigued by the potential for using these concepts to give patients an architecture in which they can start to understand that they have some agency in altering their future predictions and really changing their experience of their pain.

And as your clinician, I'd be really intrigued to see

get your sense of where this is going in that type of clinical realm?


SPEAKER_03:
I think that's a really nice example as well, thinking about attitudes towards pain and particular kinds of pain, particularly when there isn't a sort of obvious tissue damage that can be identified that's causing that pain, but that still is obviously a very real sensation to the people who are experiencing it.

And I think it generalizes to a number of other conditions.

And probably there's an overlap with things like functional neurological disorders, probably a number of medically unexplained symptoms and things where there is a huge amount of overlap in these sorts of processes where you can't necessarily directly identify a big structural lesion of any sort.

But clearly the way the nervous system is working, something isn't quite right.

Pain, I think, is an interesting example, particularly because pain is never a real thing out there in the world, is it?

I mean, it's always an inference.

And so I think that's one of the first things to say to anybody, you know, regardless of whether you've got pain because you've lost a leg or whatever else, there's no such thing as pain as an external state.

It is just your inference, your explanation for pain.

the signals that are coming in from your body.

Now, one of the important concepts that I think comes into all of these sorts of syndromes and probably many others as well,

is the idea of precision, the idea that the data I predict, sensory data coming in, I can, in addition to estimating what those data are going to look like, I also have to estimate how confident I am in what they're going to look like.

If I'm not very confident or I think I'm dealing in something that's very noisy, then I can actually discount a lot of that signal.

Whereas if I believe that it's all extremely reliable, very clean data that's coming in, then I want to put a lot of weight on that.

And another way of saying that is that there are some things I pay attention to and some things I don't.

And just saying it's reliable data is just a matter of saying I'm going to attend to it.

Now, it might be that in a number of these different syndromes,

What's going on is that our bodies have overestimated the precision, overestimated how clean the data are coming in from a particular body part.

And that's why we then over-interpret any signals that are coming in as if they are painful.

And the inferences we draw from those signals are just too confident and too driven by something that they shouldn't be.

An example I often like to use in clinical discussions is that if I told you to pay attention now to the feeling that your left sock is having on your left big toe, you can probably feel it, but you were completely unaware of it before.

But now if I say continue to pay attention to that and carry on paying attention to that and you're unable to pay attention away from it anymore,

If you felt that all the time, it would feel unusual, it would feel wrong.

And maybe a misestimation of precision in that way may be what's going on in a lot of these sorts of conditions.

Partly, I think it's often interesting that in some of these people, there was at some stage an injury or something that affected that body part.

And you can imagine that if you went through a period of illness where you did have tissue damage and were in pain for that reason, it might direct your attention to that and you may have a learned deployment of attention towards that area that might persist even after the insult and the damage has resolved.

And again, maybe it's that sort of persistent mislearning of how precise things are that leads to the continued experience of pain even when there's nothing sort of externally causing it anymore.

So I think just having a sense of inference and the idea that all our perceptions of the world around us are ultimately inferences

says that if we change our model in some way, if we're able to adapt and change and if plasticity in our brains and spinal cords can be exploited, then we can also correct those mislearned models and hopefully resolve those sorts of symptoms and syndromes.


SPEAKER_00:
Awesome.

thank you well we are at the very close really appreciative thomas of how much we were able to cover what are your closing words or thoughts potentially to someone who has listened to this one hour discussion as their appetizer before joining the par at all restaurant service


SPEAKER_03:
Well, no, I've enjoyed the discussion.

Thank you for having me here.

I'll reiterate what I said earlier, that I think one of the key things in making this field more accessible to more people are these sort of reading groups, the ideas of

trying to create these educational resources and to unpack and discuss these things um and you know i'm impressed by all the infrastructure you've put together and um and by everything you've managed to achieve by doing this um so i'd certainly encourage anyone who's interested to join up to your reading groups um and i hope they they get something out of it and out of the book thanks ali


SPEAKER_01:
Well, yes.

Actually, since the beginning of the first cohort, this book has practically become a part of our life that we love and cherish.

And it's amazing to see how with each iteration of our reading group, it gives rise to a whole different set of questions and discussions.

Actually, I'm a firm believer in this famous quote, I don't know by whom, which says that the sign of a great book is not how many answers it provides, but rather how many thought-provoking questions it engenders.

So based on that criterion and judging by the variety of questions brought up during our live discussions,

as a kind of empirical evidence for that, I think it's probably safe to say that this is truly a great book.

So congratulations on that, and thank you.


SPEAKER_04:
Thank you very much.


SPEAKER_00:
Great closing thoughts.

So Thomas, thank you again.

You're welcome back anytime.

Thanks, Ali and Terry, for all the great participation.

And until next time, bye.
start	end	sentNum	speaker	confidence	text
7800	18710	2	B	0.99988	Hello, it's July 28, 2023, and we're in active inference textbook group Slash Bookstream 2.02.
19240	20832	3	B	0.99995	Thanks Ali, for joining.
20896	29710	4	B	0.99995	So what we're going to do today is give a short overview of the chapters from the par at all 2022 book.
30640	45548	5	B	0.8713	We're going to do chapters four, five, seven, and eight, and we're just going to pause between them because then we'll clip them into the shorter videos, append that to the playlist.
45724	52610	6	B	0.99999	Just so there's a first video overview of each of the chapters and this is the second in that work.
53400	58550	7	B	0.85247	All right, so we'll do chapter four.
59000	61750	8	B	0.99773	We'll just wait a few seconds and then start chapter four.
66100	77120	9	B	0.81026	Okay, chapter four is called Degenerative Models of Active Inference, and it begins with a quotation, everything should be made as simple as possible, but not simpler by Albert Einstein.
77280	83290	10	B	0.77651	Ali, what is your overview, thought or warning for chapter four?
85340	110748	11	A	0.99934	Okay, so after the preliminary materials in chapters two and three, which was basically largely based on providing some conceptual framework for developing the further theory, chapter four delves into much more detail in terms of mathematical formulation.
110924	128020	12	A	1	And it unpacks a lot more the way that the central equations of active inference is derived and how to construct the important elements of active inference models.
128600	139720	13	A	0.99876	So say matrices A-B-C and D, and also how to put together generative models in different situations.
140140	161744	14	A	0.99907	So it basically lays out the foundation constructing active inference models both for discrete time situations and continuous time ones, which will be used later in chapters seven and eight.
161942	172100	15	A	0.99996	But this is probably one of the most challenging and at least mathematically dense chapters in the book.
172250	178816	16	A	0.99993	So I would personally suggest reading through this chapter really slowly.
179008	192860	17	A	0.9	And even if we don't get to understand every single detail of the chapter, obviously we can return required as we go through the textbook.
194000	195336	18	B	0.9999	Thank you, Ali.
195528	196270	19	B	0.99993	Yes.
196640	198760	20	B	0.99935	So let's look through the sections.
198840	214512	21	B	0.9999	Just to add on, though, chapter four is one of the larger and more equation dense chapters because it is the common kernel or basis that's then going to get applied in chapter five.
214566	220068	22	B	0.53319	In the Neurobiological case, there's a recipe for making chapter four in chapter six.
220154	222720	23	B	0.99994	That's the recipe for active inference modeling.
222880	234276	24	B	0.99947	Chapter seven and eight are about the discrete and the continuous time variant or subtype or motif of these kinds of things called generative models.
234388	241210	25	B	0.99994	So this is the real common route and we'll just look at what the sections are.
242720	249020	26	B	0.56818	This chapter complements the preceding chapter's conceptual treatment of active inference with a more formal treatment.
249760	253630	27	B	0.99973	Section 4.2 from Bayesian inference to free energy.
254000	256860	28	B	1	What would you say about this section, Ali?
258480	279140	29	A	0.99656	Okay, so as we know, the free energy principle is inspired by previous work on Bayesian inference, I mean all the way back to Helmholt's theory about unconscious inference or something to that effect.
279290	299660	30	A	1	I can't remember the exact term, but here it provides in a bit more detail how we can derive free energy principle formalism using the established Bayesian inference formulation.
301120	332932	31	A	0.99	And particularly one of the key movements or at least one of the key decisions through the derivation of free energy principle formulation is using Jane's inequality principle to derive an upper bound instead of just using the exact values to compute or to achieve the required parameters.
333076	363920	32	A	0.99991	So that's basically, in my opinion, the key premise of section 4.2 and to see in a bit more detail how we can achieve those upper bounds using Jane's inequality directly by using manipulations of Bayesian statistical formalism.
365620	366032	33	B	0.99999	Thanks.
366086	370060	34	B	0.99998	I'll just add one point from this section broadly.
370140	375868	35	B	0.99998	These are the problems of inferring states of the world perception and inferring a course of action planning.
375964	385236	36	B	0.99973	So this is again referring to the perception and everything that happens in between is the internal or the cognitive part of the inference.
385268	389252	37	B	0.51972	But this is like the blanket state cybernetic input output.
389396	398940	38	B	1	And then let's look at the first equation or how much equations overall or what equations do you think we should highlight?
402960	413840	39	A	0.99973	Okay, just as a general comment about these different equations.
414260	423680	40	A	0.99992	Well, each of these equations provide a distinct step toward deriving the ultimate whole picture.
423840	445864	41	A	0.99973	So even if we don't quite understand how we can derive from each step to the other one, it's good to know that it's only required to understand how we get to that ultimate whole picture.
445992	458364	42	A	0.98675	But ultimately what we would need in order to develop active inference models is the ultimate equation or ultimate whole picture.
458492	468144	43	A	0.9999	So this is just a way to elucidate the steps toward developing that whole picture.
468272	478870	44	A	0.99999	But again, it's not an essential requirement to understand the materials of the rest of the book.
479320	504840	45	A	0.99996	But if we go from equations 4.1 toward the 4.4 or other words, a variational free energy, well, equation 4.1 is just a basic definition of some properties of probabilities in terms of conditional probability and so on.
504930	522560	46	A	0.99399	So equation 4.2 provides the central Jane's inequality principle and how it relates to conditional probabilities and of course joint probabilities.
522720	551580	47	A	1	And then by using those two properties or those two equations, we ultimately get to 4.4, which is the definition of variation free energy parameter, which is the parameter of interest that needs to be optimized in order to inference to happen or at least perceptual inference to happen in active inference models.
553280	553740	48	B	0.99979	Thanks.
553810	560560	49	B	0.99941	Only thing I'll add is F is the letter used for variational free energy.
560710	576630	50	B	0.99992	Think of it like a computer program and the arguments that it takes in or the variables that it takes in are q, which is the distribution that's under the statistician's control and Y, which are the data which are outside of the statistician's control.
577080	584090	51	B	0.59	And do you want to describe more about anything in this equation or carry on?
586060	605968	52	A	0.99989	Just one thing that can probably be helpful is to somehow compare these steps with the initial picture we had from chapter two because variation free energy was first introduced in chapter two.
606134	618310	53	A	0.99995	So it can be helpful to go back and forth between chapters two and four and try to connect the dots between the related points there.
624420	630720	54	B	0.99981	Section 4.3 generative Models all right, I'll read the first sentence, then you can give some thoughts.
631300	642180	55	B	1	To calculate the free energy, we need three things data, a family of variational distributions and a generative model comprising a prior and a likelihood.
646280	654090	56	B	1	In this section, we outlined two very general sorts of generative model used for active inference and the form the free energy takes in relation to each.
661430	671270	57	A	0.99687	Okay, so as mentioned earlier, this chapter deals both with discrete time and continuous time situations.
671610	677260	58	A	0.99993	So clearly we would need two different types of generative models for each situation.
678190	693870	59	A	1	And obviously the generative models or the way to construct generative models for discrete time situations would vary quite a bit from the one for continuous time situations.
695170	731114	60	A	0.99984	But the general principle underlying those generative models are basically the same, which is to somehow construct a model of the environment, either be it for the situation that is sequential in time or for the situations that need to be somehow each moment of the situation needs to be accommodated in terms of a continuous time situation.
731232	737660	61	A	0.53958	So Figure 4.2 provides some examples of both.
741970	743120	62	A	0.99718	Let me see.
744450	757460	63	A	0.50832	Yes, so we have some examples of different kinds of generative models, some case studies if you like.
757830	771506	64	A	1	And it provides various ways to show how the dependencies between variables can be modeled using these kinds of graphical probabilistic models.
771538	803330	65	A	0.99945	So one common way to represent generative models is to use these kinds of graphical probabilistic models in active inference literature, which is at least in this case, the circles would represent the random variables and the squares would represent the distributions, which would describe the dependencies between those random variables.
803830	822354	66	A	0.70394	So we can see the clear relationships between those parameters here, which is basically what this whole graph, what constitutes the generative model that needs to be used for different situations.
822402	838694	67	A	1	And then in Figure 4.3, we can compare the two different types of generative models based on whether it's discrete time or continuous time situations.
838822	852914	68	A	0.99995	So the upper picture is a generative model for the discrete time situation and the lower picture is the parallel continuous time version of it.
853112	861074	69	A	1	And as we can see, the general topology of these models are the same.
861272	873990	70	A	1	The only things that differ is the use of parameters for policies or I mean, discrete time policies or the continuous time ones.
874060	891040	71	A	1	And we can obviously compare the different elements for both priors states and external states, internal states and so on by comparing these two models here.
892530	895758	72	B	0.99963	Yeah, we often return to Figure 4.3.
895844	905682	73	B	0.75136	It's kind of the Rosetta Stone of generative modeling for the context of this book because it's then going to develop out into chapter seven and eight.
905816	914622	74	B	1	And it represents a really fundamental decision made in modeling and in the later chapters.
914686	928890	75	B	0.51905	It's also shown how it can be made into a hierarchical model that combines aspects of both, but within each level of modeling still, these are the kinds of decisions that modelers are presented with when it comes to statistical modeling overall.
930430	938640	76	B	0.99938	So Section 4.4 goes into essentially the top half of Figure 4.3 discrete time.
939810	941920	77	B	0.99994	What would you say about discrete time?
943570	955966	78	A	0.83174	Okay, so the discrete time situation is obviously the archetype discrete time situation, which is the Palm DP models.
956078	977834	79	A	0.9799	So at this point I would very much like to recommend following the material from step by step paper, because in that paper, the way to construct Palm DP models is described in a bit more detail.
977952	991840	80	A	0.9968	So if anyone feels like they should learn a bit more about the gaps in the details, I would very much like to recommend that particular paper.
995730	1021270	81	A	1	I don't know how much detail we should go into because although it's not maybe detailed enough for some tastes, but it goes in a quite extensive detail about how we can construct these models using the concepts we've learned in previous chapters.
1022010	1041120	82	A	0.99957	So ultimately we reach equations 4.13 and 4.14, which are basically the culmination of Palm DP formulation using the vector notations and gradients and so on.
1046130	1049120	83	A	0.8893	Then we go to continuous time situation.
1050550	1051300	84	B	0.89445	Great.
1053030	1068274	85	B	1	A few things intervene in the continuous time chapter ones that we'll just mention here because they're kind of boxed or partitioned from the continuous time part, but they're following pages versus Markov blankets.
1068402	1074654	86	B	0.99997	We won't go into it here, but kind of footnote that or look at some other places where we talk about it outside of this chapter.
1074722	1079690	87	B	0.99986	Overview figure 4.4 Bayesian message passing.
1080110	1082410	88	B	0.99998	Again, a big topic.
1082750	1092430	89	B	0.99992	Let's kind of just go past it now back to the regularly scheduled continuous time generative model discussion.
1092850	1098110	90	B	1	And then another box to the generalized coordinates of motion.
1098450	1102660	91	B	0.99997	So taking position plus derivatives of position.
1103190	1109730	92	B	1	And that has some beneficial properties that are described and unpacked also elsewhere.
1112890	1115800	93	B	0.99915	Do you want to say anything about 4.5.2?
1120570	1137590	94	A	0.99998	Well, the only thing that comes to mind is although as I said before, all the formulations here may look more, I mean, a bit too dense to understand at the first pass.
1137760	1180140	95	A	0.99999	But some of the key maybe components here could be obviously the material from box 4.2 and 4.3 I think are quite essential to understand the underlying principle behind deriving the continuous time situation because without LaPlace approximation, what we would have in terms of free energy minimization is it would look very much like the Gibbs free energy.
1182670	1196762	96	A	1	The key distinction between the free energy principle as described in active inference literature, as opposed to Gibbs free energy is this distinction, is the LaPlace approximation.
1196826	1206690	97	A	0.981	So this is what enables us to go from Gibbs free energy to the variation of free energy.
1206840	1216386	98	A	0.97545	So yeah, that's quite essential to make this to be familiar with this essential approximation.
1216578	1229020	99	A	1	And obviously the concept of generalized coordinates of motion will come time and time again throughout the whole book, particularly in chapters eight and nine.
1229710	1237870	100	A	0.96307	So, yeah, those two concepts, I believe, needs a bit more attention.
1242450	1243838	101	B	0.99981	Yeah, sounds good.
1243924	1256078	102	B	0.99973	Box 4.3 LaPlace approximation equations, another message passing, representation, and a summary.
1256254	1264646	103	B	1	The key message to take away is that approximate Bayesian inference may be framed as minimizing a quantity known as variational free energy.
1264828	1269970	104	B	1	This depends on a generative model that expresses our belief about how data are generated.
1270130	1271800	105	B	0.99998	Anything else you want to add?
1275930	1291982	106	A	0.75357	Ah, nothing comes to mind at the moment because, as I said, we're still in the stage that we want to develop our essential tools to be used in the rest of the book.
1292036	1303838	107	A	0.98932	So here, up until now, I believe by the end of chapter four, we have acquired all the essential, necessary mathematical tools.
1304014	1321110	108	A	1	And the next chapter, chapter five, kind of acts like an interlude, and I don't think it's the direct continuation of chapters one through four.
1321260	1331290	109	A	0.99993	So I believe the first part of the book, conceptually and mathematically, ends here.
1331440	1333580	110	A	0.73074	So, yeah, that's it.
1334590	1344750	111	B	0.99997	Yes, it's a little bit like the Pragmatic modeling part gets foreshadowed or explored in five now that we're all built up with four.
1344900	1363020	112	B	0.89814	All right, that's the end of the overview for 14.
1363700	1368080	113	B	0.88415	Chapter five is called Message Passing and Neurobiology.
1368760	1372070	114	B	0.99956	What is your overview thought on chapter five?
1374840	1379828	115	A	0.99905	Okay, I don't know.
1379834	1399870	116	A	0.84	I have mixed feelings about this chapter because on one hand, you see, as far as I understand active inference, although it originated as, quote unquote, a unified theory of the brain, I don't think it's a neurobiological theory per se.
1400640	1443370	117	A	1	Of course, there can be some correlations between neurobiological components or concepts with active inference concepts, but it's not an essential premise of active inference theory to provide a comprehensive theory about how the neurobiology of human brain or other organisms brain behave at detailed and neuroanatomical anatomical level.
1443680	1457200	118	A	0.96449	But then again, it's nice to have these kinds of empirical correlations between the findings of neurobiology and the active inference theory.
1457540	1468080	119	A	0.99368	But I don't think it's one of active inference central assertions, at least to my understanding.
1469720	1470548	120	B	0.9991	Well said.
1470634	1472660	121	B	0.99994	Very interesting framing.
1473480	1491320	122	B	0.99913	Well, chapter five definitely takes a very specific system of interest approach by highlighting one of the most studied areas, also one of the most relevant areas, which is mammalian neuroscience.
1491740	1519510	123	B	1	And the chapter is going to introduce a few different motifs in the nervous system and essentially build up towards figure 5.5, which is at the end of the chapter, and 5.5 wires together three specific neural systems that the chapter is going to focus on work in that area from.
1519960	1527476	124	B	0.99522	So Ali said it very well, active inference was built up to in chapter four.
1527658	1535528	125	B	0.52118	Here is another level or type of science with assertions or with representations or mappings to any specific system.
1535694	1559250	126	B	0.99754	But this is the kind of modeling that has been built up and done by Friston Parr Pazulo and others over the decades, with a focus coming from a human neuroimaging laboratory setting a lot of focus and study and attention and funding and everything on the mammalian nervous system.
1561380	1573088	127	B	0.99889	But claims about the nervous system are not the basis of what active inference claims or how it's derived.
1573264	1583510	128	B	0.99993	But this is like an example case study in neurobiology connecting back to some of the formalisms that we've just seen introduced in chapter four.
1584440	1596812	129	A	0.63789	Yeah, and to add a minor point to what you just said, I think it's important to draw attention to the last sentence of the last paragraph of the first page.
1596946	1601596	130	A	0.91914	It is important to draw a distinction between a principle I, e.
1601618	1607920	131	A	0.91	The minimization of free energy, and a process theory about how this principle may be implemented in a certain kind of system.
1608070	1618948	132	A	0.99956	So I think this sentence here frames this chapter in relation to all the other technical chapters of this book.
1619034	1643992	133	A	0.99881	So if every other chapter is about developing, or at least up to now, was about developing the principled formalism of active inference, now, chapter five provides a kind of preliminary sketch for the process theory of active inference, which is obviously far from an extensive theory.
1644056	1645640	134	A	0.64413	It's just a single chapter.
1645720	1661600	135	A	0.99989	But then again, it can provide some important signposts for anyone who wants to further investigate this area awesome free energy.
1661670	1671430	136	B	0.82219	Principle, Bayesian mechanics, all things in that area are, on this principle, not responsive to empirical data.
1672040	1675984	137	B	1	And then the process theory is about how the principle is implemented.
1676112	1688824	138	B	0.99997	So the specific generative models that are made and how well they map or how well they do in a portfolio of models that can have very different goals and assumptions and all of this.
1689022	1710320	139	B	0.99979	But the process theory implementation lets us develop hypotheses that are answerable to empirical data, like what is the kind of information or relationship between photons hitting the retina and changes in activity in neural systems?
1711220	1723520	140	B	1	And that's an informational question or can be abstracted in a way to an informational question that, it turns out, does have empirical support and results in unique explanations and predictions.
1723680	1730550	141	B	0.99994	That doesn't mean that it always results in unique explanations and predictions, but a lot of citations are provided here.
1731580	1733850	142	B	0.99758	That's what we can explore in chapter five.
1736860	1747420	143	B	1	The last paragraph of the first section describes that they're going to look at the three different neural systems.
1747760	1751580	144	B	0.99584	Okay, section 5.2, microcircuits and messages.
1755290	1756710	145	B	0.96876	What do you think, Ali?
1758030	1758860	146	A	0.95668	All right.
1761310	1777546	147	A	1	This chapter begins from how message passing happens in neurobiological terms and compare it to the way active inference frames this message passing mechanism.
1777738	1809398	148	A	1	And specifically, if we look at Figure 5.1 and compare this figure to the ones we've seen before in chapters one through four I think it was in chapter four, we can see some clear parallels between how this kind of cortical message passaging happens in the brain versus how it is framed in active inference literature.
1809494	1817478	149	A	1	And as we can see it's clearly inspired by the neurobiology of the brain.
1817654	1829230	150	A	0.99989	But then it's important to keep in mind that it's not a direct one to one mapping between these two models.
1830290	1854630	151	A	0.99999	This is just a kind of, I don't know, an interesting or illuminating, if you like, parallel to keep in mind to somehow be a bit more confident about the viability of the theory we want to use for message passing and active inference.
1855610	1862890	152	A	0.99999	Which is to say, it's not some haphazard theory that's just been developed for practical reasons.
1863230	1876490	153	A	0.99997	It has some basis in neurobiology, although it's not necessarily fully congruent with every detail of neurobiology.
1879390	1879850	154	B	0.99955	Great.
1879920	1890850	155	B	0.5	The specific example is going to involve this one region of mammalian cortex tissue that has these six layers, and there's a ton of neurobiology.
1892630	1907750	156	B	1	The big takeaway for Figure 5.1 is that it's possible to graphically lay out nodes and variables and find some empirical correspondences, again, some unique explanations and predictions in certain cases.
1908890	1925678	157	B	1	And that's one kind of modeling where it's really trying to understand and improve the ability to do correlation and intervention and counterfactual causal type analysis with the real system of interest.
1925844	1940706	158	B	0.99999	Or in a more pedagogical setting or in a research setting or an industrial setting, you might sweep across large families of structures of models and there's no need to be grounded to any biological structure at all.
1940888	1951106	159	B	0.99933	So this is just describing the specific neuroanatomical research that really arose out of the imaging work at UCL and the SPM package.
1951218	1952920	160	B	0.99009	That's where a lot of this comes from.
1954250	1955334	161	B	0.9	5.2.
1955452	1956518	162	B	0.99601	Yeah, go ahead.
1956684	1957110	163	A	0.72819	Sorry.
1957180	1972190	164	A	0.99991	Just as a side note, I think watching one of Thomas Parr's lectures on neurobiology of active inference, which is available on YouTube, would really help to understand the materials of this chapter better.
1972260	1975040	165	A	0.99995	So I highly recommend watching that one.
1975890	1976640	166	B	0.99999	Thanks.
1979170	1992820	167	B	0.99987	Figure 5.2 gives a rerendering of a kind of classical view of a hierarchical predictive coding system works.
1993750	2002178	168	B	0.99997	So here abstracting a layer from the tissue six layer to just two layers.
2002274	2014294	169	B	0.99994	Here computational layers now and then showing how there's hierarchical communication within a layer, but also there's signaling within a layer.
2014342	2023834	170	B	0.93	And there's a hierarchy in Bayesian modeling with variables that are higher order predictions about other variables.
2023962	2027790	171	B	1	And that's the basis of the predictive coding architecture.
2028370	2055030	172	B	0.99986	So 5.2 looks at some ways that something that resonates with the cerebral cortical architecture enables what might computationally look like or have some really strong and explanatory values in actually relating to computationally a hierarchical Bayesian model which could do various general tasks.
2056330	2065900	173	B	0.70367	All right, 5.3 is motor commands leaving the prefrontal cortex, going down to the butterfly looking cross section here.
2066270	2068220	174	B	0.99998	What is 5.3?
2070510	2099750	175	A	0.99905	Okay, so 5.3 moves to the other half of active inference framework, which is how it can model the decision making and ultimately the movement of the agent in order to minimize the expected free energy as opposed to variational free energy that we saw in perceptual half of active inference.
2100090	2125790	176	A	0.99932	So it again provides a kind of correlation or analogy between the structural neuroanatomy particularly related to the motor commands and how it can relates to active inference, particularly the continuous time active inference.
2126210	2142878	177	A	0.99788	So we can see that for the external event or, I'm sorry, for the external state, we can take, for example, the proprioceptive afferent.
2143054	2167130	178	A	1	And then this proprioceptive afferent acts as a kind of y for the continuous time active inference which needs to be processed in a way to optimize the expected free energy and how it relates to both attention and precision.
2168670	2178190	179	A	0.99492	We will see a bit more detail about those terms and the relation between them in chapter eight.
2178260	2193250	180	A	0.99998	But I think here Section 5.3 provides a good summary about the general paths through the motor command systems of neurobiology.
2194230	2194978	181	B	0.99977	Great.
2195144	2195890	182	B	0.93881	I'd say.
2195960	2212150	183	B	0.99997	While the previous case study focused on how the connectivity within and between the Cortical columns could have a computational relationship with a Bayesian hierarchical predictive coding architecture.
2212650	2241170	184	B	0.85	The argument of the second case study is that a continuous input, continuous output kind of set point seeking, reflexive motor behavior with a moving set point with a descending moving set point enabling motion by changing ultimately the set point and enabling variation in the strategies to reach that set point through different mechanisms.
2242230	2250866	185	B	0.99998	This is also describable in a compatible way that's a shorter section.
2250978	2254870	186	B	0.99926	Now, section 5.4 subcortical structures.
2256810	2258790	187	B	0.99999	What would you say about this section?
2261070	2276286	188	A	0.99814	Okay, so subcortical structures are very important in the decision making and the planning of the agents.
2276388	2298626	189	A	0.48781	So obviously here we need another kind of analogy between the way that these plannings and decision making happen neuroanatomically with the way that it's framed in active inference.
2298738	2311130	190	A	0.99999	But again, we can see it's clearly based on at least some of the important elements we've seen from the previous chapters.
2311710	2321718	191	A	0.58998	So for example, we saw how policy is described or how it relates to outcomes and preference and so on.
2321904	2328910	192	A	0.99982	We can see those elements are directly inspired by neuroanatomical structures.
2329250	2359374	193	A	0.7298	So I guess that's at least in my opinion, this section here 5.4 seems a bit more sketchy in the meaning that it doesn't go into quite the extensive details about how those structures can be compared.
2359442	2376320	194	A	0.99989	But for anyone who wants to further investigate these topics, there are some useful references put on here on pages 93 and 94.
2379650	2380062	195	B	0.99995	Thanks.
2380116	2390350	196	B	0.99994	Yeah, it's really abbreviated and overviewed but we get an interlude from Table 5.1 with putative roles of neurotransmitters.
2390430	2404182	197	B	0.99993	So same perspective that we took before on neuroanatomical functionalism here directly translates to neurotransmitters reductionism or essentialism or something like that.
2404236	2419260	198	B	0.99993	So certainly all neurotransmitters and molecules, they play variable roles in different settings and this is the neat and Scruffy manifold all over again.
2419870	2425454	199	B	1	One person might say well, we need a theory for every acetylcholine molecule in the world.
2425572	2427354	200	B	0.99997	They're all in a unique context.
2427482	2431354	201	B	1	And someone else says all neurotransmitters are described by one parameter.
2431402	2433326	202	B	0.99911	In this model I'm getting value from it.
2433348	2434960	203	B	0.99997	So to me, that's an account.
2435270	2455474	204	B	1	And somewhere in between is the work in this space, which is making an attempt to have a principled and falsifiable approach to model the computational aspects of specific regions and contexts and settings.
2455602	2463820	205	B	1	And so Acetylcholine, Noradrenaline, Dopamine, and Serotonin are given a little mini review here.
2464510	2467894	206	B	1	And so it's not an exhaustive or an exclusive claim.
2467942	2478800	207	B	0.99256	It's kind of a provocation from computational and molecular neuroscience, and people can look into the papers and also ones that probably have been published since.
2480930	2487874	208	B	0.93	5.6 goes to Continuous and Discrete Hierarchies, which is graphically overviewed in Figure 5.5.
2487912	2489620	209	B	0.99998	So what would you say about this?
2491750	2510030	210	A	0.99994	Yeah, one interesting thing about this section is the observation that our lower level engagement with the environment can be most successfully characterized with continuous time formulations.
2510210	2542706	211	A	0.99998	But as we go up on the level of cognitive concepts or at the level of cognitive hierarchies and we come to concepts such as, I don't know, decisions or even beliefs and so on, we can reach the area that the discrete time situation would probably be more efficient to characterize the behavior of the agent.
2542888	2572382	212	A	0.99987	So this multiscale structure of active inference modeling is quite evident in the way that our message passing happens in our brain in terms of our lower level data processing up into consolidating the higher level cognitive concepts and ontologies awesome.
2572516	2573440	213	B	0.99998	Thank you.
2575410	2582330	214	B	1	To me, Figure 5.5 demonstrates the kind of whole of body approach that you could imagine.
2582410	2587670	215	B	0.85386	There's so many organs and systems and phenomena for which there aren't specific generative models.
2587690	2594434	216	B	0.99985	So little can be said about situations where no generative model has been articulated, and here's one where it has.
2594472	2597598	217	B	0.9996	So it gives you also it's kind of like reading a Drosophila.
2597614	2600734	218	B	0.92302	Melanogaster review paper relatively.
2600782	2605874	219	B	0.94543	It's like, this is how much work it takes to get to this state of knowledge in an insect.
2606002	2611350	220	B	0.99983	So then in another insect, do we know less about that insect empirically and genetically?
2611510	2623854	221	B	0.97821	So consider this to be what's known to be a lot, however, also about one of the most sophisticated or specific cognitive systems at least we know.
2623972	2636980	222	B	0.73956	So there's that additional kind of like, self reflexive aspect to this chapter that is not a cornerstone of active inference, but here it's just presented in a synthetic case study.
2638230	2640500	223	B	0.99999	Anything else you want to say about five?
2642870	2644900	224	A	0.99996	Nothing particular comes to mind.
2645670	2646500	225	B	0.99841	All right.
2671000	2671748	226	B	0.99536	Okay.
2671914	2675350	227	B	0.99937	Chapter seven is called Active Inference and Discrete Time.
2675880	2683560	228	B	0.99999	Chapter seven is the first in a pair of chapters with chapter eight on Discrete and Continuous Time.
2683630	2693964	229	B	0.99995	So they're kind of like two forks of a river that we discussed in chapter four and before and described the recipe in chapter six.
2694082	2713564	230	B	0.99724	Now, seven and eight are kind of like one level deeper, going from the kind of all of this group of animals to one level deeper into its classification scheme on the way to the specific generative model for which it's actually given in its totality.
2713692	2718984	231	B	0.99998	But everything prior to that is about the learning about its principles.
2719132	2724400	232	B	1	And this is kind of on the trunk of the path to discrete time modeling.
2724480	2727392	233	B	1	Just like chapter eight will be about continuous time modeling.
2727536	2729590	234	B	0.99998	What would you add in?
2732300	2749660	235	A	0.99686	Okay, so I think chapters seven and eight really helps to understand in a more practical way how the materials from particularly chapters one through five applies in real time situations.
2750160	2778820	236	A	0.99943	So even if we somehow didn't get to understand every details of chapters one through four, when we come to chapters seven and eight, I think some of those uncertainties about our understandings can be clarified, at least in a practical sense.
2778890	2788680	237	A	0.99994	So I believe these two chapters are really helpful in order to consolidate our understandings from the previous chapters.
2789740	2791210	238	B	0.99996	Awesome, well said.
2792060	2796620	239	B	0.90949	So it's going to involve specifying some discrete time models.
2798880	2828470	240	B	1	7.2 goes into perceptual processing and the general structure of the chapter is going to walk through a series of examples that build in complexity where they first start with perception in 7.2, introduce decision making and then describe a few more types of motifs or cognitive structure or patterns and also check out step by step and model stream one where it's built up to in a different way.
2829160	2835000	241	B	0.99983	So the first example is I'll let you describe it since it's musical.
2836860	2837272	242	A	0.99893	Okay.
2837326	2870176	243	A	0.96078	So yeah, the first example is the situation in which we try to describe the performance of an amateur musician in terms of how we listen to the performance of an amateur musicians in terms of the predictions we get from our anticipation of the following notes as opposed to the actual notes that's being played.
2870288	2934090	244	A	0.9999	So these kinds of anticipatory reaction listening reaction to the musician can be successfully formalized using discrete time active inference by putting up by putting together the matrices A for the states and matrix B for the transition between the states or the transition probabilities, which in this case describes the probability from going from one note to the other and obviously the actual sequence that's been played which can be described with the matrix d another point I wanted to mention is for anyone who has downloaded this chapter before I don't know, I think about June or something.
2934700	2947470	245	A	1	I recommend redownloading it from MIT's website because they have corrected some of the typos that was previously present in this chapter, particularly in Figure 7.2.
2950560	2951020	246	B	0.99993	Cool.
2951090	2960076	247	B	0.55998	So this graphical model where a person is listening, this is a general perceptual Bayesian framing.
2960188	2965500	248	B	0.96735	It's specified just like with any other equations.
2965580	2972470	249	B	0.99997	There's a lot to look into but A indicates the probability of an outcome given a state.
2973080	2982328	250	B	0.99999	This is saying if it were all on the diagonal like an identity matrix, this is kind of a common motif then states kind of map to themselves.
2982494	2999132	251	B	0.99855	So in the context of this model, a represents the mapping between the observed note and the underlying hidden true note and then B.
2999186	3002924	252	B	0.99991	Describes the transition matrix of how those change to time.
3002962	3005340	253	B	1	D is the prior they're specified.
3008180	3011650	254	B	0.99996	Figure 7.2, do you want to describe it?
3015220	3048460	255	A	0.99935	All right, so in figure 7.2, or at least the incomplete version of Figure 7.2 we see here well, at the upper left part of the picture we see the beliefs about each note at each time step and at upper right we somehow translate those beliefs into specific numerical values.
3051040	3065628	256	A	0.57997	So instead of just assigning some continuous values, we've simplified the situation by assigning some discrete numerical values for each node.
3065804	3092520	257	A	1	And then the lower left is supposed to show the free energy gradients over time or in other terms the prediction errors we get from comparing our predictions with the actual outcomes.
3092960	3106640	258	A	0.99921	So lastly, the lower right picture shows in parallel to the upper right picture, determines the values of these errors.
3107060	3131800	259	A	0.99868	So we can see both the initial or at least initial continuous assignment and values and then the further discretizing of the values in order to get the discrete time situation or the more tractable discrete time situations.
3133980	3143676	260	B	0.99818	Okay, so it's a general passive inference task where there's priors about how states are going to change through time and then there's real data coming in.
3143778	3159980	261	B	0.99984	So that's the kind of classical predictive coding video compression coleman filter Bayesian setting 7.3 introduces a key motif which is decision making and planning as inference.
3160140	3166032	262	B	0.99654	So this is the idea of having a Bayes graph where the variables can relate to different things.
3166086	3178896	263	B	0.99918	There's high composability and here the idea is that a variable is going to be proposed that we can do inference about that describes the process of decision making or policy selection.
3179008	3181530	264	B	0.99842	So what would you say about 7.3?
3184780	3193630	265	A	0.99916	Okay, so 7.3 is obviously similar to what we saw in chapter four.
3194080	3202300	266	A	0.96	And if I'm not mistaken, even the topology is exactly the same with that picture we saw previously.
3204900	3227344	267	A	0.70509	This is the initial setup which acts also as a review about how these different components of palmdp generative models needs to be described in such situations.
3227472	3258624	268	A	0.99104	But ultimately the specific case study we come across in this section is the attempt to model the behavior of a mouse in a teammate of a rat and a teammate, especially teammates, containing an aversive stimulus in one arm and an attractive stimulus on the other.
3258822	3272660	269	A	0.99941	So this can act as a kind of toy example to use this kind of probabilistic modeling to describe these situations.
3275850	3276310	270	B	0.99996	Thanks.
3276380	3278870	271	B	0.99999	So that leads us right to figure 7.4.
3279020	3308320	272	B	0.94138	Here's a visualization of the situation with the rat in this case where there's a pleasant and aversive stimuli on each end of a decision point and there's also an epistemic opportunity to receive some information about the context that the animal is in.
3308710	3317954	273	B	1	And so that setting is described for both the case with white on the left, black on the right and black on the left, white on the right.
3318152	3321842	274	B	1	And those are shown in terms of their differences in the matrices.
3321906	3336730	275	B	1	The explicit specification of the generative model visualizations show some of the slices of the B variable which reflect different transition probabilities.
3337630	3352350	276	B	1	C represents the preferences which are expressed over the observable states, d reflects the priors on the different states that need priors.
3353890	3357300	277	B	1	7.4, what would you say about this?
3359750	3400490	278	A	0.99924	Okay, so in 7.4, it builds up on the previous section and adds other elements that we previously saw in chapters two and four, which is how the exact formulation for expected free energy can be used sorry, variation free energy can be used to formulate the trade off between the information seeking, or at least between the epistemic value and information seeking.
3400570	3418450	279	A	0.99746	So here it uses again that rad example in a bit more extended and elaborate form to formulate the epistemic value of observing a queue in a given location.
3419210	3425960	280	A	1	And figure 7.7 is a representation of this situation.
3427530	3461522	281	A	0.99998	But another situation that's been, let me see yeah, in 7.9, another case study discussed here is the situation of the psychotic eye movements because it is something that can be quite successfully described or characterized in terms of information seeking versus the epistemic value.
3461656	3466498	282	A	1	And the situation here is let me see.
3466584	3467220	283	A	0.69554	Yeah.
3467750	3510450	284	A	0.52441	Shown visually in Figure 7.9, which clearly shows how our visual psychotic eye movement can be described in such a way as to kind of trace the trajectory of our eye movement among different regions of the visual space and how the information we gather from a given region can affect the subsequent trajectories of our psychotic eye movements.
3513190	3521400	285	A	0.99945	That's basically the main premise of this section, I guess.
3521850	3522310	286	B	0.9824	Nice.
3522380	3523000	287	B	0.99838	Great.
3524010	3528520	288	B	0.97	7.5, what would you say about it?
3531870	3571542	289	A	0.50106	Okay, so 7.5 again adds another dimension to the previous formulations, and this time, we get to update the generative models by learning and the so the generative models for this situation is a bit more complicated than the previous ones because it now needs to account for a mechanism or a way to update the matrices we had before.
3571676	3588886	290	A	0.99998	So in the previous situations we didn't account for learning per se, but here we directly update our general sorry, the word update can be confusing.
3588918	3601390	291	A	0.99995	Here we get to somehow improve our generative models to accommodate for these updating accounts.
3607330	3647630	292	A	0.82	The situation here or the case study here, which somehow elucidate the way that the learning can be accounted for with these models is again a toy example of a creature in a simple world of black and white tiles which kind of tries to find a path to reach a given destination, a certain destination.
3648130	3664366	293	A	0.99904	So it is more complicated than the situation we had for the Rat example because it only had simple trajectories that needed to traverse.
3664398	3678854	294	A	1	But here the creature, or the agent in this case needs to do lots more learning and information seeking and so on.
3678972	3694974	295	A	0.99988	So all the previous elements is kind of combined in this example and it's a really good example to see how the different components of active inference can be connected to each other.
3695172	3699630	296	B	0.99988	Nice and 76 hierarchical or deep inference.
3699970	3725080	297	B	0.51	First, a box 7.3 interlude on structure learning, boxed off topic and a lot to say, but structure learning broadly refers to learning the structure about a model using the same types of methods that you might to do inference on, for example, a more observable sensor data reading, something like that.
3726730	3735850	298	B	0.63671	This section works towards the idea of nested inference or multiscale modeling.
3736190	3738700	299	B	0.60674	What would you say about figure 712?
3742190	3755898	300	A	0.99929	Okay, so again, this situation is, I think, the most complex situations of this chapter, which builds up from the previous sections.
3756074	3769986	301	A	1	And this time it adds another layer to accommodate for the inferences that happen in different time steps.
3770098	3784022	302	A	0.99994	So in this case we have multi time or multiscale inference and learning happening both at the levels of learning and at the levels of information seeking.
3784166	3816820	303	A	0.99942	So this is represented in figure seven, point twelve, which represents how kind of this fractal generative model can be seen as a component in this multiscale bigger generative or as a kind of leaf in this bigger generative model.
3817350	3839980	304	A	0.99997	So it can be seen as a lower level inference happening at the leaf level, going up to the hierarchy and influencing sorry, collaborating on the whole process of learning and inference at the higher level.
3843070	3849134	305	A	0.82979	Yeah, I guess that somehow summarizes this figure.
3849252	3854320	306	A	0.96848	So if you have anything to add, that's great.
3854770	3860290	307	B	0.7724	It's an example of the composability of generative models.
3860710	3887066	308	B	0.9661	What we've talked about and had Toby Sinclair Smith describe as as the compositional cognitive cartography and just what kinds of connectors can and can't you do and how can that motif that the discrete time model introduces and then the rest of these features, including action and learning and so on, get layered in on top?
3887248	3889260	309	B	1	What can you do with that?
3890430	3893980	310	B	0.94	713 gives another example.
3894450	3897920	311	B	0.93149	Do you want to say anything about it or maybe continue on?
3898530	3908782	312	A	0.99888	Yeah, so the case study here is the example of linguistic, I mean, language learning through reading.
3908846	3916830	313	A	0.96824	So not language learning, maybe just what happens sentence comprehension reading.
3916990	3918654	314	A	0.99988	Yeah, in comprehension.
3918702	3932074	315	A	0.99998	So what happens when reading in an anticipatory way the words that comes each after the other.
3932112	3961538	316	A	0.88616	So why this kind of situation can be most successfully characterized with this kind of modeling because it involves different scales of learning and comprehension, both at the level of at the level of somehow observing the letters and then going onto the words and then word groups and so on.
3961704	3991980	317	A	0.99968	So, yeah, that's really interesting way to again combine all of those elements into a single unified model to see how those different timescales, slow and fast timescales operate together to build this more encompassing model of more encompassing generative model of the situation.
3993550	3994022	318	B	0.99934	Great.
3994096	3995760	319	B	0.99533	Any closing thoughts on seven?
4000610	4002800	320	A	0.9994	Nothing particular now, thanks.
4003730	4008980	321	B	0.99839	All right, next chapter is chapter eight, which is going to go into the continuous time.
4021560	4027810	322	B	0.01314	It's all right.
4027900	4031162	323	B	0.99974	Chapter eight is called active inference in continuous time.
4031296	4035738	324	B	0.93974	Begins with that timeless quote, everything flows, nothing stands still.
4035904	4038380	325	B	0.74618	So what would you say about chapter eight?
4039870	4055780	326	A	0.99812	All right, so this chapter probably is my most favorite chapter in the book because of my own personal interest in, I don't know, process materialism and so on.
4058230	4074962	327	A	0.98919	Chapter seven acts as a really good starting point for anyone who wants to develop the discrete time situations to model discrete time situations within active inference framework.
4075106	4089210	328	A	0.99948	But in chapter eight, we kind of get to model a bit more interesting or let's say more involving situations.
4089630	4099440	329	A	1	And they're not necessarily kind of toy examples we saw at least at the beginning of chapter seven.
4099970	4107890	330	A	0.39661	So obviously, as the title suggests, this chapter deals with the continuous time situation.
4108040	4127160	331	A	0.99972	So in that case we'll need to maybe at this point refresh our memory about what continuous time situation involves by reading the relevant parts, reading or reviewing relevant parts of chapter four.
4129870	4164146	332	A	0.99986	In chapter four, we saw that the generative model for continuous time situation derives from the Edo's Stochastic calculus in terms of putting the whole process into two elements of Stochastic equations, one of which is the actual states, the condition of actual states or the behavior of the actual states.
4164248	4173250	333	A	1	And the other one is the randomness that we need to account for in each real time continuous time situations.
4173330	4178778	334	A	0.99997	So that's what we get here in equation 8.1.
4178944	4201802	335	A	1	And then building up from that equation, it generalizes that equation to involve the functionals of G and F instead of just the single valued functions of GNF.
4201866	4224310	336	A	0.83538	So then we get to put that into the situation that can be used for describing the behavior of dynamical systems, which is a very well known situation to use these kinds of Stochastic equations.
4224650	4239260	337	A	0.96	And it's widely studied how those kinds of dynamics can be characterized, especially in recent Bayesian mechanics paper by Dalton Saktivetevel and others.
4241250	4272070	338	A	0.98	And then it gets to some more specific examples such as Lotgobal Terra dynamics and synchronicity and so on, in order to show how these kinds of dynamics can be elaborated upon and can be generalized and enables them to characterize more complex situations.
4275930	4281398	339	A	0.70474	That's a really short and brief overview of the whole chapter.
4281494	4286220	340	A	0.99998	Maybe we can talk about a bit more details as we go through it.
4288110	4289306	341	B	0.99155	Great, well said.
4289408	4298714	342	B	0.99996	Well, I'm sure for another day the philosophical implications of eight, Seven and Eight and High Road and Low Road and all these other parts of the textbook.
4298762	4299790	343	B	0.9999	Great topics.
4300530	4301230	344	B	0.96	I agree.
4301300	4319698	345	B	1	I would see chapter eight as demonstrating continuity with some classical continuous time modeling motifs from a few different areas of dynamical systems science, which is applied in many, many fields.
4319714	4321298	346	B	0.68035	But these are some classic examples.
4321394	4344618	347	B	0.99986	So figure 8.1 goes a little bit more into depth or at least into more formalism detail about exactly what we saw in chapter five with the spinal reflex arc with the proprioceptive data coming in and then a differential being calculated with the set point which reflects a descending prediction from a decision making layer.
4344794	4364340	348	B	1	And that can be viewed as this kind of mechanics that plays out in a phase space in continuous time, like a spring moving around with someone making a certain path within a tractor and a spring being dragged around something in that area.
4365610	4369266	349	B	0.69917	Box 8.1 goes into a very fascinating topic.
4369298	4370760	350	B	0.92301	Do you want to describe it?
4373690	4383290	351	A	0.99997	Well, it's maybe one of the most thought provoking pages of the whole book.
4383360	4405940	352	A	1	And if I remember correctly, in all of the cohorts, this particular box always gives rise to lots of questions because of some of the interesting and at least initially counterintuitive claims here.
4406630	4409860	353	A	0.99887	But I don't want to spoil it.
4412710	4447120	354	A	0.99355	But as a kind of spoiler alert, it kind of gets to really interesting, but alas, very brief discussion about the comparing these terms precision, attention and sensory attenuation and the relation and similarities and difference between these three terms and how understanding each of them is essential to understanding the other ones.
4447570	4471480	355	A	0.73765	But as I said, it's a really interesting topic which gives rise to lots of discussions and I believe it's one of those topics that's worth looking a bit more looking into in some other literature as well.
4472010	4472518	356	B	0.99999	Great.
4472604	4473318	357	B	0.99999	Well said.
4473404	4474790	358	B	0.99996	What a cliffhanger.
4475130	4480710	359	B	1	Next they go to a classic model family called Lockable Terra.
4481230	4486310	360	B	1	These dynamics inherit from characterizations of predator prey dynamics in ecology.
4486470	4491566	361	B	0.99979	So it's kind of a classical ecology model shown in Figure 8.2 on the top.
4491668	4502478	362	B	0.75863	It's actually the ecosystem model plants, herbivores and carnivores which follow different kinds of oscillatory trends in continuous time.
4502644	4509662	363	B	1	And so that also has enabled it to be applied for other so called winnerless competitions.
4509806	4528978	364	B	1	And that relates to topics like neural Darwinism and also neural dynamics where things have kind of oscillatory relationships with each other which are being modeled as a continuous time underlying process with a lot of measurement, noise and discretization through space and time.
4529084	4537286	365	B	0.99993	Those are the kinds of algorithms that SPM explores more and there's lock of Voltera and a lot of other dynamical systems theory in SPM.
4537478	4548746	366	B	0.92734	So active inference kind of adds action and more to what was laid out from a pure dynamical systems theory in SPM.
4548938	4549390	367	A	0.99981	Here.
4549460	4570150	368	B	0.99998	It really is just showing the ecology example and how you can project if you have three different species, you can think about that motion in a cube or tetrahedron, and then you could project onto kind of like looking at a lower dimensional manifold relating just two of the three species.
4570490	4582214	369	B	1	And that evinces this kind of oscillatory but also moving behavior that gets connected in Figure 8.3 to neurobiology.
4582342	4583980	370	B	0.99999	What would you say about this?
4586830	4596106	371	A	0.9991	Okay, so here in Figure 8.3 we see some applications of a lot of altera dynamics.
4596298	4611486	372	A	0.98267	So the left column here represents what happens in eye blink conditioning.
4611678	4631974	373	A	0.77568	So of course here we need to account for the expected states of the sequences of events that happens in the eye blinking.
4632022	4640700	374	A	0.99988	So the upper left figure shows the expectations in terms of time.
4641090	4659714	375	A	0.99	And then the parallel right hand side equation, sorry, right hand side figures shows the Lotka volterra systems that is applied in the handwriting situation.
4659912	4688730	376	A	0.99559	So, as we can see, although the mathematical technology is the same, or at least the modeling technology is the same, the outcome of each situation varies drastically in two distinct two distinct neurobiological behavior, not neurobiological, but biological behavior.
4689070	4711234	377	A	0.50672	So, yeah, we can see how the same modeling framework can give rise to different outcomes based on what parameters needs to be optimized, what parameters are selected for the modeling, and so on.
4711432	4730410	378	A	0.99445	So I believe it's a quite interesting example to compare handwriting and a blinking together and how those can be compared to each other using the lotkobal thermodynamics great.
4730480	4731340	379	B	1	Thank you.
4731710	4739686	380	B	0.50999	Box 8.2 gives a variant on the learning here presented with the formalism for continuous models.
4739718	4745546	381	B	1	Kind of a technical aside, section 8.4 is about generalized synchrony.
4745658	4753334	382	B	0.9999	So figure 8.4 is going to visualize one of the classic dynamical systems, which is the Lorenz attractor.
4753402	4756722	383	B	0.99926	So what would you say about this figure?
4756856	4780198	384	A	0.99907	Okay, so this section is truly interesting because when one thinks of active inference, probably the first situations that comes to mind is the situations in which we have quite well defined probability distributions for different parameters.
4780374	4802142	385	A	0.99999	But as we can see here in Section 8.4, actually some of the formalism of active inference can be successfully used to characterize even chaotic systems, and in particular the way in which two chaotic systems can be synchronized with each other.
4802276	4819910	386	A	0.964	So this is a classic example of chaotic Lorentz system and it draws upon from some of Professor Prison's earlier work on birdsong synchrony.
4820490	4833850	387	A	0.53	And as a side note, any literature before 2016 is considered earlier history in active inference literature because it evolves quite rapidly.
4837870	4886458	388	A	0.99993	This kind of synchrony between two chaotic systems can be interpreted as providing evidence or even, let's say, a way to model a kind of primitive theory of mind, in the sense that how exactly can we understand or two agents can trace each other's trajectories without any I mean, engaging in any direct exchange of observations between their internal and external states.
4886544	4898830	389	A	0.99959	So, yeah, that's a really good example, and I believe one of the most interesting examples of how active inference can even account for these kinds of behavior.
4900770	4922950	390	A	0.99	And the rest of the section goes into the details of how this kind of synchrony between multiscale Lorentz systems can happen and how can we formulate it mathematically in terms of continuous time active inference.
4924330	4925126	391	B	0.99997	Awesome.
4925308	4930646	392	B	0.69	And there's been more recent work on Markov Blankets and Stochastic chaos.
4930758	4937606	393	B	0.99998	But the Bird example is a classic 8.5 goes into hybrid discrete and continuous models.
4937718	4943150	394	B	0.99944	So this could be kind of like an in between chapter of seven and eight.
4943220	4968178	395	B	0.99998	But now that we've been introduced to the pure form of discrete and the pure form of continuous models here shown that that composability extends to so called hybrid models where here the lower level visually is using the continuous time formalism and the higher level is describing a little line added here, the discrete time formalism.
4968354	4975318	396	B	1	And this was the similar structure described by the authors of the paper.
4975404	4989622	397	B	0.99858	Active inference does not contradict folk psychology where they described this lower level as motor active inference which was closely allied with the spinal arc reflex shown above.
4989766	4997258	398	B	0.97	And then this higher level they called decision active inference because in that case it was referring to a discrete decision.
4997434	5019910	399	B	0.99	And so they used that kind of basic motif of continuous activity or continuous time modeling at the more peripheral aspects of a cognitive entity and like Ali said, more discretization and hybridization as well at higher levels of the cognitive modeling.
5021850	5051998	400	B	1	And that type of an architecture here, instead of describing who wants the ice cream cone, I believe here it's going to be a mixed or a hybrid model that is going to call back the Icicade system where there's a fixed point that is able to be moved as a set point, and then there's a continuous time icicade that pursues the new fixed point.
5052164	5063906	401	B	0.92	And so that's analogous to a new set point or fixed point being specified from the top down muscle command about a new location for a muscle followed by movement towards it.
5064008	5071362	402	B	1	This is a muscular activity that is realizing that but not in the elbow coming away from the hot stove.
5071506	5081050	403	B	0.99999	This is about the eye circading to an epistemic foraging location specified by top down hierarchical systems.
5081950	5092406	404	B	1	8.3 describes little technical aside on mixture of Gaussian gaussian mixture models, kind of a technical modeling note.
5092518	5098414	405	B	0.51	And 8.6 closes it says it's a huge topic and much has been left out.
5098452	5102506	406	B	1	And so they list in Table 8.1 key advances in continuous time models.
5102538	5119110	407	B	1	And those areas are synthetic birdsong, ocular, motor delays, conditioned reflexes, smooth pursuit, eye movements, psychosis illusions, cicades action, observation attention, hybrid models and self organization.
5119930	5121686	408	B	1	And that's chapter eight.
5121868	5123238	409	B	0.98743	What else would you say?
5123324	5129338	410	B	1	And also what would you kind of lead someone to in the philosophical implications of eight?
5129424	5131020	411	B	1	Because it sounds kind of cool.
5134190	5163620	412	A	0.99922	Okay, well, the case of continuous time active inference, I think it leads to really interesting questions both in terms of philosophical questions and also more practical modeling questions about what parameters needs to be accounted for and so on.
5164390	5180150	413	A	1	And as I said, I believe it's a more interesting way of it's not interesting, but at least more involved way of doing active inference modeling.
5180230	5217560	414	A	0.94556	But one thing that one of the philosophical questions that Mahault and I have explored in our paper is how the processes, ontological processes can philosophically described using FEP assertions in terms of their intraaction with the environment in which they co constitute themselves.
5217930	5226358	415	A	0.57	And we don't necessarily distinguish between the internal and the external states.
5226444	5248730	416	A	0.99969	So one obvious example of this is that generalized synchrony example that we saw in this chapter in which we don't necessarily distinguish between which of the birds act as the agent and which one is the environment or the vice versa.
5248890	5271574	417	A	0.99921	So these kind of co constitution of the environment and the agent which gives rise to the partitioning of state space through markup blanket is one of the interesting philosophical points that I think needs to be elaborated a bit.
5271612	5288022	418	A	0.99999	More using some of the recent advances in philosophy, such as the tools that's been developed in New Materialism School or some other philosophical approaches.
5288086	5294442	419	A	0.99987	But yeah, these kinds of what exactly gives rise to emergence?
5294506	5306002	420	A	0.99998	What is the ontological status of emergent properties and so on, are some of the burning questions for many philosophers today.
5306136	5336122	421	A	1	And I believe active inference, and particularly continuous time active inference, provides a clear, precise mathematical formalism, even if not to answer these questions, but at least to explore it in a more rigorous and practical way, and also practical and attractable way.
5336256	5355060	422	A	0.9999	So this is the area that I believe philosophy and science are beautifully intertwined into a coherent view of not only the phenomena of interest, but even about the whole world.
5357430	5358180	423	B	0.99962	Wow.
5361350	5362274	424	A	0.93312	Pretty cool.
5362392	5362674	425	B	0.86944	Yeah.
5362712	5365250	426	B	0.98	A lot to say about that topic.
5366330	5376946	427	B	0.9999	After completing chapters seven and eight, you've seen the kind of two major branches or two major motifs of just one kind of modeling.
5376978	5388490	428	B	0.99999	But these kind of models have so many different forms that that's why it's such a hands on process to specify the generative model in chapter six and fit it with data in chapter nine.
5388640	5390054	429	B	1	Those are all what's required.
5390102	5397280	430	B	1	And that's kind of the last mile of where these discussions about general motifs gets you.
5397650	5419300	431	B	0.74582	But also playing with these pedagogical models can be really helpful because it will help you understand the basic patterns and relationships and start to see different patterns in the graphical models and know from there what levels of technical processes can be kind of coarse grained over.
5420950	5421860	432	B	0.9975	All right.
5427140	5427648	433	B	0.66768	Okay.
5427734	5429440	434	B	0.99996	Well, that's it.
5429590	5436170	435	B	1	I guess next time we will do probably 910 and maybe something else.
5438060	5439752	436	B	0.99475	All right, I'll end it now.
5439806	5440840	437	B	0.99984	Thanks, Ali.
5441260	5442330	438	A	0.99962	Thank you.
5443500	5443780	439	A	0.32621	Bye.

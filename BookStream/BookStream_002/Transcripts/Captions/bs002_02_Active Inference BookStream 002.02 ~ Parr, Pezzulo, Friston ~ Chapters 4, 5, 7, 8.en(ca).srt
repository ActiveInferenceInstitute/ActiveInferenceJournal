1
00:00:03,600 --> 00:00:10,836
DANIEL FRIEDMAN: Hello, it's July 28, 2023,

3
00:00:10,858 --> 00:00:14,672
and we're in active inference textbook

4
00:00:14,736 --> 00:00:18,710
group Slash Bookstream 2.02.

5
00:00:19,240 --> 00:00:21,408
Thanks Ali, for joining. So what we're

6
00:00:21,424 --> 00:00:24,070
going to do today is give a short

7
00:00:24,760 --> 00:00:27,876
overview of the chapters from the par at

8
00:00:27,898 --> 00:00:31,356
all 2022 book. We're going to do

9
00:00:31,378 --> 00:00:33,870
chapters four, five, seven,

10
00:00:34,720 --> 00:00:35,870
and eight,

11
00:00:37,520 --> 00:00:40,012
and we're just going to pause between

12
00:00:40,066 --> 00:00:42,784
them because then we'll clip them into

13
00:00:42,822 --> 00:00:44,736
the shorter videos, append that to the

14
00:00:44,758 --> 00:00:47,872
playlist. Just so there's a first video

15
00:00:47,926 --> 00:00:49,744
overview of each of the chapters and

16
00:00:49,782 --> 00:00:52,610
this is the second in that work.

17
00:00:53,400 --> 00:00:57,124
All right, so we'll do

18
00:00:57,322 --> 00:00:59,956
chapter four. We'll just wait a few

19
00:00:59,978 --> 00:01:01,750
seconds and then start chapter four.

20
00:01:06,100 --> 00:01:08,912
Okay, chapter four is called

21
00:01:08,966 --> 00:01:11,056
Degenerative Models of Active Inference,

22
00:01:11,088 --> 00:01:12,528
and it begins with a quotation,

23
00:01:12,624 --> 00:01:14,276
everything should be made as simple as

24
00:01:14,298 --> 00:01:16,208
possible, but not simpler by Albert

25
00:01:16,224 --> 00:01:19,040
Einstein. Ali, what is your overview,

26
00:01:19,120 --> 00:01:23,290
thought or warning for chapter four?

27
00:01:25,340 --> 00:01:29,640
ALI RAHMJOO: Okay, so after the preliminary materials

28
00:01:30,300 --> 00:01:32,330
in chapters two and three,

29
00:01:33,280 --> 00:01:36,830
which was basically largely based on

30
00:01:38,320 --> 00:01:41,292
providing some conceptual framework for

31
00:01:41,346 --> 00:01:44,852
developing the further theory,

32
00:01:45,016 --> 00:01:47,424
chapter four delves into much more

33
00:01:47,462 --> 00:01:49,756
detail in terms of mathematical

34
00:01:49,788 --> 00:01:53,488
formulation. And it unpacks a

35
00:01:53,494 --> 00:01:57,476
lot more the way that the

36
00:01:57,498 --> 00:02:00,564
central equations of active inference is

37
00:02:00,602 --> 00:02:04,644
derived and how to construct the

38
00:02:04,682 --> 00:02:07,072
important elements of active inference

39
00:02:07,136 --> 00:02:11,540
models. So say matrices

40
00:02:11,620 --> 00:02:14,904
A-B-C and D, and also how to put

41
00:02:14,942 --> 00:02:18,600
together generative models in different

42
00:02:18,670 --> 00:02:22,324
situations. So it basically lays

43
00:02:22,372 --> 00:02:28,376
out the foundation constructing

44
00:02:28,488 --> 00:02:32,140
active inference models both for

45
00:02:32,210 --> 00:02:34,596
discrete time situations and continuous

46
00:02:34,648 --> 00:02:38,624
time ones, which will be used

47
00:02:38,822 --> 00:02:42,352
later in chapters seven and eight. But

48
00:02:42,406 --> 00:02:45,570
this is probably one of the most

49
00:02:46,340 --> 00:02:49,952
challenging and at least mathematically

50
00:02:50,016 --> 00:02:53,924
dense chapters in the book. So I would

51
00:02:53,962 --> 00:02:56,004
personally suggest reading through this

52
00:02:56,042 --> 00:02:58,816
chapter really slowly.

53
00:02:59,008 --> 00:03:02,424
And even if we don't get to

54
00:03:02,542 --> 00:03:04,936
understand every single detail of the

55
00:03:04,958 --> 00:03:08,570
chapter, obviously we can return

56
00:03:09,260 --> 00:03:12,860
required as we go through the textbook.

57
00:03:14,000 --> 00:03:17,304
Thank you, Ali. Yes. So let's

58
00:03:17,352 --> 00:03:19,564
look through the sections. Just to add

59
00:03:19,602 --> 00:03:22,060
on, though, chapter four is one of the

60
00:03:22,210 --> 00:03:25,756
larger and more equation dense

61
00:03:25,868 --> 00:03:28,976
chapters because it is the common

62
00:03:29,078 --> 00:03:32,416
kernel or basis that's then going to

63
00:03:32,438 --> 00:03:34,848
get applied in chapter five. In the

64
00:03:34,854 --> 00:03:37,608
Neurobiological case, there's a recipe

65
00:03:37,644 --> 00:03:40,068
for making chapter four in chapter six.

66
00:03:40,154 --> 00:03:41,904
That's the recipe for active inference

67
00:03:41,952 --> 00:03:44,884
modeling. Chapter seven and eight are

68
00:03:44,922 --> 00:03:46,624
about the discrete and the continuous

69
00:03:46,672 --> 00:03:51,240
time variant or subtype or motif of

70
00:03:51,390 --> 00:03:53,572
these kinds of things called generative

71
00:03:53,636 --> 00:03:57,396
models. So this is the real common route

72
00:03:57,588 --> 00:04:00,564
and we'll just look at what the sections

73
00:04:00,612 --> 00:04:04,156
are. This chapter complements the

74
00:04:04,178 --> 00:04:06,024
preceding chapter's conceptual treatment

75
00:04:06,072 --> 00:04:08,120
of active inference with a more formal

76
00:04:08,200 --> 00:04:11,752
treatment. Section 4.2 from Bayesian

77
00:04:11,816 --> 00:04:14,828
inference to free energy. What would you

78
00:04:14,914 --> 00:04:18,892
say about this section, Ali? Okay,

79
00:04:18,946 --> 00:04:21,920
so as we know, the free energy principle

80
00:04:22,740 --> 00:04:26,432
is inspired by previous work

81
00:04:26,486 --> 00:04:29,268
on Bayesian inference, I mean all the

82
00:04:29,354 --> 00:04:32,992
way back to Helmholt's theory

83
00:04:33,136 --> 00:04:37,120
about unconscious inference

84
00:04:37,200 --> 00:04:39,828
or something to that effect. I can't

85
00:04:39,844 --> 00:04:43,610
remember the exact term, but here

86
00:04:45,740 --> 00:04:49,976
it provides in a bit more detail how

87
00:04:49,998 --> 00:04:52,680
we can derive free energy principle

88
00:04:53,200 --> 00:04:56,712
formalism using the established

89
00:04:56,776 --> 00:04:59,660
Bayesian inference formulation.

90
00:05:01,120 --> 00:05:04,350
And particularly one of the key

91
00:05:05,040 --> 00:05:08,160
movements or at least one of the key

92
00:05:08,230 --> 00:05:11,904
decisions through the derivation of free

93
00:05:11,942 --> 00:05:14,800
energy principle formulation is using

94
00:05:14,950 --> 00:05:21,844
Jane's inequality principle to derive an

95
00:05:22,042 --> 00:05:25,812
upper bound instead of just using the

96
00:05:25,866 --> 00:05:29,920
exact values to compute or

97
00:05:30,010 --> 00:05:32,932
to achieve the required parameters.

98
00:05:33,076 --> 00:05:35,290
So that's basically,

99
00:05:36,940 --> 00:05:39,370
in my opinion, the key premise of

100
00:05:39,900 --> 00:05:43,052
section 4.2 and

101
00:05:43,186 --> 00:05:46,604
to see in a bit more detail

102
00:05:46,722 --> 00:05:50,104
how we can achieve those upper

103
00:05:50,152 --> 00:05:55,208
bounds using Jane's inequality directly

104
00:05:55,384 --> 00:06:02,284
by using manipulations of Bayesian

105
00:06:02,332 --> 00:06:06,032
statistical formalism. Thanks.

106
00:06:06,086 --> 00:06:08,448
I'll just add one point from this

107
00:06:08,614 --> 00:06:11,120
section broadly. These are the problems

108
00:06:11,190 --> 00:06:12,512
of inferring states of the world

109
00:06:12,566 --> 00:06:14,544
perception and inferring a course of

110
00:06:14,582 --> 00:06:16,960
action planning. So this is again

111
00:06:17,030 --> 00:06:20,296
referring to the perception and

112
00:06:20,478 --> 00:06:22,184
everything that happens in between is

113
00:06:22,222 --> 00:06:24,616
the internal or the cognitive part of

114
00:06:24,638 --> 00:06:26,056
the inference. But this is like the

115
00:06:26,078 --> 00:06:29,252
blanket state cybernetic input output.

116
00:06:29,396 --> 00:06:32,748
And then let's look at the

117
00:06:32,754 --> 00:06:34,680
first equation or how much equations

118
00:06:34,760 --> 00:06:37,196
overall or what equations do you think

119
00:06:37,218 --> 00:06:38,940
we should highlight?

120
00:06:42,960 --> 00:06:43,710
Okay,

121
00:06:48,580 --> 00:06:51,970
just as a general comment about these

122
00:06:52,580 --> 00:06:55,968
different equations. Well, each of these

123
00:06:56,054 --> 00:06:59,968
equations provide a distinct step toward

124
00:07:00,064 --> 00:07:03,680
deriving the ultimate whole picture.

125
00:07:03,840 --> 00:07:07,632
So even if we don't

126
00:07:07,776 --> 00:07:11,450
quite understand how we can derive from

127
00:07:11,820 --> 00:07:14,010
each step to the other one,

128
00:07:15,340 --> 00:07:18,952
it's good to know that it's only

129
00:07:19,006 --> 00:07:23,452
required to understand how we get to

130
00:07:23,586 --> 00:07:26,670
that ultimate whole picture. But

131
00:07:27,440 --> 00:07:29,724
ultimately what we would need in order

132
00:07:29,762 --> 00:07:34,060
to develop active inference models

133
00:07:34,500 --> 00:07:37,356
is the ultimate equation or ultimate

134
00:07:37,388 --> 00:07:42,256
whole picture. So this is just a

135
00:07:42,278 --> 00:07:45,692
way to elucidate the steps toward

136
00:07:45,756 --> 00:07:48,950
developing that whole picture. But

137
00:07:49,400 --> 00:07:53,728
again, it's not an essential requirement

138
00:07:53,904 --> 00:07:57,536
to understand the materials

139
00:07:57,568 --> 00:08:00,744
of the rest of the book. But if we go

140
00:08:00,782 --> 00:08:05,460
from equations 4.1 toward

141
00:08:05,620 --> 00:08:09,364
the 4.4 or other words, a variational

142
00:08:09,412 --> 00:08:12,924
free energy, well, equation 4.1 is

143
00:08:12,962 --> 00:08:18,924
just a basic definition of some

144
00:08:19,042 --> 00:08:22,476
properties of probabilities in terms of

145
00:08:22,498 --> 00:08:25,280
conditional probability and so on. So

146
00:08:25,350 --> 00:08:28,850
equation 4.2 provides the

147
00:08:29,540 --> 00:08:32,860
central Jane's inequality principle

148
00:08:32,940 --> 00:08:38,912
and how it relates to conditional

149
00:08:38,976 --> 00:08:41,504
probabilities and of course joint

150
00:08:41,552 --> 00:08:45,830
probabilities. And then by using those

151
00:08:47,000 --> 00:08:50,068
two properties or those two equations,

152
00:08:50,164 --> 00:08:53,064
we ultimately get to 4.4,

153
00:08:53,182 --> 00:08:55,572
which is the definition of variation

154
00:08:55,636 --> 00:08:59,016
free energy parameter, which is the

155
00:08:59,038 --> 00:09:01,624
parameter of interest that needs to be

156
00:09:01,662 --> 00:09:05,372
optimized in order to inference to

157
00:09:05,426 --> 00:09:09,224
happen or at least perceptual inference

158
00:09:09,272 --> 00:09:11,580
to happen in active inference models.

159
00:09:13,280 --> 00:09:16,784
Thanks. Only thing I'll add is F is

160
00:09:16,822 --> 00:09:19,984
the letter used for variational free

161
00:09:20,022 --> 00:09:21,840
energy. Think of it like a computer

162
00:09:21,910 --> 00:09:23,904
program and the arguments that it takes

163
00:09:23,942 --> 00:09:26,752
in or the variables that it takes in are

164
00:09:26,806 --> 00:09:29,856
q, which is the distribution that's

165
00:09:29,888 --> 00:09:32,916
under the statistician's control and Y,

166
00:09:33,018 --> 00:09:34,916
which are the data which are outside of

167
00:09:34,938 --> 00:09:39,236
the statistician's control. And do

168
00:09:39,258 --> 00:09:41,130
you want to describe more about anything

169
00:09:41,820 --> 00:09:44,090
in this equation or carry on?

170
00:09:46,060 --> 00:09:49,624
Just one thing that

171
00:09:49,662 --> 00:09:53,384
can probably be helpful is to somehow

172
00:09:53,512 --> 00:09:57,464
compare these steps with the initial

173
00:09:57,512 --> 00:10:01,564
picture we had from chapter two because

174
00:10:01,762 --> 00:10:03,852
variation free energy was first

175
00:10:03,906 --> 00:10:08,064
introduced in chapter two. So it

176
00:10:08,102 --> 00:10:10,332
can be helpful to go back and forth

177
00:10:10,396 --> 00:10:13,744
between chapters two and four and try to

178
00:10:13,782 --> 00:10:16,892
connect the dots between the related

179
00:10:16,956 --> 00:10:18,310
points there.

180
00:10:24,420 --> 00:10:27,776
Section 4.3 generative Models all right,

181
00:10:27,798 --> 00:10:29,408
I'll read the first sentence, then you

182
00:10:29,414 --> 00:10:32,656
can give some thoughts. To calculate the

183
00:10:32,678 --> 00:10:36,420
free energy, we need three things data,

184
00:10:36,570 --> 00:10:38,704
a family of variational distributions

185
00:10:38,752 --> 00:10:40,676
and a generative model comprising a

186
00:10:40,698 --> 00:10:42,180
prior and a likelihood.

187
00:10:46,280 --> 00:10:48,084
In this section, we outlined two very

188
00:10:48,122 --> 00:10:50,344
general sorts of generative model used

189
00:10:50,382 --> 00:10:52,008
for active inference and the form the

190
00:10:52,014 --> 00:10:54,090
free energy takes in relation to each.

191
00:11:01,430 --> 00:11:05,298
Okay, so as mentioned earlier,

192
00:11:05,474 --> 00:11:08,546
this chapter deals both with discrete

193
00:11:08,578 --> 00:11:11,270
time and continuous time situations.

194
00:11:11,610 --> 00:11:14,922
So clearly we would need two different

195
00:11:14,976 --> 00:11:16,634
types of generative models for each

196
00:11:16,672 --> 00:11:20,554
situation. And obviously

197
00:11:20,752 --> 00:11:23,514
the generative models or the way to

198
00:11:23,552 --> 00:11:26,118
construct generative models for discrete

199
00:11:26,134 --> 00:11:29,358
time situations would vary quite a bit

200
00:11:29,444 --> 00:11:32,782
from the one for continuous time

201
00:11:32,836 --> 00:11:37,054
situations. But the

202
00:11:37,092 --> 00:11:40,034
general principle underlying those

203
00:11:40,072 --> 00:11:42,834
generative models are basically the

204
00:11:42,872 --> 00:11:46,082
same, which is to

205
00:11:46,136 --> 00:11:48,946
somehow construct a model of the

206
00:11:48,968 --> 00:11:49,810
environment,

207
00:11:51,930 --> 00:11:55,960
either be it for the situation

208
00:11:57,610 --> 00:12:01,046
that is sequential in time or for the

209
00:12:01,068 --> 00:12:06,154
situations that need to be somehow each

210
00:12:06,192 --> 00:12:07,834
moment of the situation needs to be

211
00:12:07,872 --> 00:12:10,342
accommodated in terms of a continuous

212
00:12:10,406 --> 00:12:14,026
time situation. So Figure

213
00:12:14,128 --> 00:12:17,660
4.2 provides some examples of both.

214
00:12:21,970 --> 00:12:25,054
Let me see. Yes,

215
00:12:25,172 --> 00:12:28,794
so we have some examples

216
00:12:28,842 --> 00:12:32,254
of different kinds of generative models,

217
00:12:32,302 --> 00:12:36,158
some case studies

218
00:12:36,334 --> 00:12:39,502
if you like. And it provides various

219
00:12:39,566 --> 00:12:44,386
ways to show how the dependencies

220
00:12:44,578 --> 00:12:47,320
between variables can be modeled using

221
00:12:48,570 --> 00:12:50,994
these kinds of graphical probabilistic

222
00:12:51,042 --> 00:12:55,022
models. So one common way to represent

223
00:12:55,106 --> 00:12:58,166
generative models is to use these kinds

224
00:12:58,198 --> 00:13:01,546
of graphical probabilistic models in

225
00:13:01,568 --> 00:13:05,178
active inference literature, which is

226
00:13:05,344 --> 00:13:09,054
at least in this case, the circles would

227
00:13:09,092 --> 00:13:12,958
represent the random variables and

228
00:13:13,124 --> 00:13:16,206
the squares would represent the

229
00:13:16,228 --> 00:13:20,402
distributions, which would describe the

230
00:13:20,456 --> 00:13:22,334
dependencies between those random

231
00:13:22,382 --> 00:13:26,500
variables. So we can see

232
00:13:26,870 --> 00:13:30,050
the clear relationships between those

233
00:13:30,120 --> 00:13:32,870
parameters here, which is basically what

234
00:13:33,020 --> 00:13:36,882
this whole graph, what constitutes

235
00:13:37,026 --> 00:13:39,654
the generative model that needs to be

236
00:13:39,692 --> 00:13:43,160
used for different situations. And then

237
00:13:43,770 --> 00:13:46,380
in Figure 4.3,

238
00:13:47,550 --> 00:13:50,570
we can compare the two

239
00:13:50,640 --> 00:13:52,854
different types of generative models

240
00:13:52,902 --> 00:13:56,730
based on whether it's discrete time

241
00:13:56,800 --> 00:13:59,962
or continuous time situations. So the

242
00:14:00,016 --> 00:14:03,280
upper picture is a generative model for

243
00:14:04,050 --> 00:14:06,526
the discrete time situation and the

244
00:14:06,548 --> 00:14:09,906
lower picture is the

245
00:14:09,928 --> 00:14:12,914
parallel continuous time version of it.

246
00:14:13,112 --> 00:14:17,534
And as we can see, the general topology

247
00:14:17,662 --> 00:14:21,074
of these models are the same.

248
00:14:21,272 --> 00:14:24,950
The only things that differ is

249
00:14:25,100 --> 00:14:29,666
the use of parameters for policies

250
00:14:29,778 --> 00:14:32,966
or I mean, discrete time policies or the

251
00:14:32,988 --> 00:14:35,866
continuous time ones. And we can

252
00:14:35,888 --> 00:14:39,050
obviously compare the different elements

253
00:14:40,030 --> 00:14:43,580
for both priors states and

254
00:14:44,670 --> 00:14:46,874
external states, internal states and so

255
00:14:46,912 --> 00:14:50,426
on by comparing these two models

256
00:14:50,458 --> 00:14:53,758
here. Yeah, we often

257
00:14:53,844 --> 00:14:56,654
return to Figure 4.3. It's kind of the

258
00:14:56,692 --> 00:15:00,750
Rosetta Stone of generative modeling

259
00:15:00,910 --> 00:15:02,722
for the context of this book because

260
00:15:02,776 --> 00:15:04,354
it's then going to develop out into

261
00:15:04,392 --> 00:15:06,386
chapter seven and eight. And it

262
00:15:06,408 --> 00:15:09,090
represents a really fundamental decision

263
00:15:09,430 --> 00:15:13,650
made in modeling and in

264
00:15:13,720 --> 00:15:15,686
the later chapters. It's also shown how

265
00:15:15,708 --> 00:15:17,942
it can be made into a hierarchical model

266
00:15:18,076 --> 00:15:20,614
that combines aspects of both, but

267
00:15:20,652 --> 00:15:22,662
within each level of modeling still,

268
00:15:22,716 --> 00:15:24,134
these are the kinds of decisions that

269
00:15:24,172 --> 00:15:26,186
modelers are presented with when it

270
00:15:26,208 --> 00:15:28,890
comes to statistical modeling overall.

271
00:15:30,430 --> 00:15:34,102
So Section 4.4 goes into essentially

272
00:15:34,166 --> 00:15:38,026
the top half of Figure 4.3 discrete

273
00:15:38,058 --> 00:15:41,306
time. What would you say about discrete

274
00:15:41,338 --> 00:15:44,974
time? Okay, so the

275
00:15:45,012 --> 00:15:48,960
discrete time situation is

276
00:15:49,510 --> 00:15:52,002
obviously the archetype discrete time

277
00:15:52,056 --> 00:15:55,966
situation, which is the Palm DP models.

278
00:15:56,078 --> 00:15:59,394
So at this point I would very

279
00:15:59,432 --> 00:16:04,466
much like to recommend following

280
00:16:04,498 --> 00:16:07,798
the material from step by step paper,

281
00:16:07,964 --> 00:16:11,734
because in

282
00:16:11,772 --> 00:16:14,846
that paper, the way to construct Palm DP

283
00:16:14,898 --> 00:16:17,322
models is described in a bit more

284
00:16:17,376 --> 00:16:20,954
detail. So if anyone feels like they

285
00:16:20,992 --> 00:16:24,794
should learn a bit more about the

286
00:16:24,832 --> 00:16:28,222
gaps in the details, I would very much

287
00:16:28,276 --> 00:16:31,840
like to recommend that particular paper.

288
00:16:35,730 --> 00:16:38,574
I don't know how much detail we should

289
00:16:38,612 --> 00:16:43,886
go into because although

290
00:16:44,078 --> 00:16:48,002
it's not maybe detailed enough for some

291
00:16:48,056 --> 00:16:51,310
tastes, but it goes in a quite extensive

292
00:16:51,390 --> 00:16:55,106
detail about how we can construct

293
00:16:55,218 --> 00:16:58,594
these models using the concepts we've

294
00:16:58,642 --> 00:17:01,270
learned in previous chapters.

295
00:17:02,010 --> 00:17:05,738
So ultimately we reach

296
00:17:05,904 --> 00:17:10,890
equations 4.13 and 4.14,

297
00:17:11,230 --> 00:17:14,570
which are basically the culmination of

298
00:17:14,720 --> 00:17:18,042
Palm DP formulation using the vector

299
00:17:18,106 --> 00:17:21,120
notations and gradients and so on.

300
00:17:26,130 --> 00:17:29,120
Then we go to continuous time situation.

301
00:17:30,550 --> 00:17:35,054
Great. A few things intervene

302
00:17:35,102 --> 00:17:38,738
in the continuous time chapter ones that

303
00:17:38,744 --> 00:17:40,766
we'll just mention here because they're

304
00:17:40,798 --> 00:17:44,062
kind of boxed or partitioned

305
00:17:44,126 --> 00:17:45,746
from the continuous time part, but

306
00:17:45,768 --> 00:17:47,586
they're following pages versus Markov

307
00:17:47,618 --> 00:17:49,974
blankets. We won't go into it here, but

308
00:17:50,012 --> 00:17:51,846
kind of footnote that or look at some

309
00:17:51,868 --> 00:17:53,526
other places where we talk about it

310
00:17:53,548 --> 00:17:56,762
outside of this chapter. Overview figure

311
00:17:56,816 --> 00:17:59,690
4.4 Bayesian message passing.

312
00:18:00,110 --> 00:18:03,338
Again, a big topic. Let's kind

313
00:18:03,344 --> 00:18:07,126
of just go past it now back to

314
00:18:07,168 --> 00:18:10,238
the regularly scheduled continuous time

315
00:18:10,404 --> 00:18:14,302
generative model discussion. And then

316
00:18:14,356 --> 00:18:16,586
another box to the generalized

317
00:18:16,618 --> 00:18:19,230
coordinates of motion. So taking

318
00:18:19,300 --> 00:18:22,660
position plus derivatives of position.

319
00:18:23,190 --> 00:18:26,110
And that has some beneficial properties

320
00:18:26,190 --> 00:18:28,882
that are described and unpacked also

321
00:18:28,936 --> 00:18:29,730
elsewhere.

322
00:18:32,890 --> 00:18:35,800
Do you want to say anything about 4.5.2?

323
00:18:40,570 --> 00:18:44,038
Well, the only thing that comes to mind

324
00:18:44,124 --> 00:18:47,900
is although as I said before,

325
00:18:48,750 --> 00:18:52,140
all the formulations here may look

326
00:18:52,750 --> 00:18:55,626
more, I mean, a bit too dense to

327
00:18:55,808 --> 00:18:59,246
understand at the first pass. But some

328
00:18:59,268 --> 00:19:03,674
of the key maybe components

329
00:19:03,722 --> 00:19:07,838
here could be obviously

330
00:19:08,004 --> 00:19:11,394
the material from box 4.2

331
00:19:11,432 --> 00:19:15,010
and 4.3 I

332
00:19:15,080 --> 00:19:17,540
think are quite essential to understand

333
00:19:20,310 --> 00:19:24,194
the underlying principle behind deriving

334
00:19:24,322 --> 00:19:26,486
the continuous time situation because

335
00:19:26,588 --> 00:19:29,526
without LaPlace approximation, what we

336
00:19:29,548 --> 00:19:33,030
would have in terms of free energy

337
00:19:33,180 --> 00:19:36,522
minimization is it would look

338
00:19:36,576 --> 00:19:40,140
very much like the Gibbs free energy.

339
00:19:42,670 --> 00:19:46,746
The key distinction between the

340
00:19:46,768 --> 00:19:49,086
free energy principle as described in

341
00:19:49,108 --> 00:19:51,274
active inference literature, as opposed

342
00:19:51,322 --> 00:19:53,454
to Gibbs free energy is this

343
00:19:53,492 --> 00:19:55,994
distinction, is the LaPlace

344
00:19:56,042 --> 00:19:58,906
approximation. So this is what enables

345
00:19:58,938 --> 00:20:02,740
us to go from Gibbs free energy to

346
00:20:04,950 --> 00:20:07,922
the variation of free energy. So yeah,

347
00:20:07,976 --> 00:20:12,030
that's quite essential to make

348
00:20:12,120 --> 00:20:15,154
this to be familiar with this essential

349
00:20:15,202 --> 00:20:18,966
approximation. And obviously the

350
00:20:18,988 --> 00:20:20,806
concept of generalized coordinates of

351
00:20:20,828 --> 00:20:23,750
motion will come time and time again

352
00:20:23,900 --> 00:20:26,246
throughout the whole book, particularly

353
00:20:26,278 --> 00:20:29,020
in chapters eight and nine.

354
00:20:29,710 --> 00:20:33,210
So, yeah, those two concepts,

355
00:20:33,550 --> 00:20:36,902
I believe, needs a bit more

356
00:20:37,056 --> 00:20:37,870
attention.

357
00:20:42,450 --> 00:20:45,466
Yeah, sounds good. Box 4.3 LaPlace

358
00:20:45,498 --> 00:20:48,750
approximation equations,

359
00:20:49,590 --> 00:20:51,358
another message passing,

360
00:20:51,534 --> 00:20:55,346
representation, and a

361
00:20:55,368 --> 00:20:58,146
summary. The key message to take away is

362
00:20:58,168 --> 00:20:59,778
that approximate Bayesian inference may

363
00:20:59,784 --> 00:21:03,026
be framed as minimizing a quantity known

364
00:21:03,058 --> 00:21:05,506
as variational free energy. This depends

365
00:21:05,538 --> 00:21:07,366
on a generative model that expresses our

366
00:21:07,388 --> 00:21:09,970
belief about how data are generated.

367
00:21:10,130 --> 00:21:11,800
Anything else you want to add?

368
00:21:15,930 --> 00:21:18,650
Ah, nothing comes to mind at the moment

369
00:21:18,720 --> 00:21:21,340
because, as I said,

370
00:21:23,070 --> 00:21:26,454
we're still in the stage

371
00:21:26,502 --> 00:21:29,146
that we want to develop our essential

372
00:21:29,258 --> 00:21:31,726
tools to be used in the rest of the

373
00:21:31,748 --> 00:21:35,614
book. So here, up until now,

374
00:21:35,652 --> 00:21:38,462
I believe by the end of chapter four,

375
00:21:38,596 --> 00:21:40,670
we have acquired all the essential,

376
00:21:40,750 --> 00:21:43,838
necessary mathematical tools.

377
00:21:44,014 --> 00:21:46,820
And the next chapter, chapter five,

378
00:21:48,310 --> 00:21:51,010
kind of acts like an interlude,

379
00:21:52,170 --> 00:21:55,400
and I don't think it's the direct

380
00:21:57,530 --> 00:22:00,470
continuation of chapters one through

381
00:22:00,540 --> 00:22:06,458
four. So I believe the

382
00:22:06,464 --> 00:22:09,594
first part of the book, conceptually and

383
00:22:09,632 --> 00:22:12,634
mathematically, ends here. So, yeah,

384
00:22:12,672 --> 00:22:16,282
that's it. Yes, it's a little bit like

385
00:22:16,336 --> 00:22:18,222
the Pragmatic modeling part gets

386
00:22:18,276 --> 00:22:22,174
foreshadowed or explored in five now

387
00:22:22,212 --> 00:22:25,166
that we're all built up with four. All

388
00:22:25,188 --> 00:22:27,402
right, that's the end of the overview

389
00:22:27,466 --> 00:22:43,020
for 14.

390
00:22:43,700 --> 00:22:46,444
Chapter five is called Message Passing

391
00:22:46,492 --> 00:22:50,224
and Neurobiology. What is your overview

392
00:22:50,352 --> 00:22:52,070
thought on chapter five?

393
00:22:54,840 --> 00:22:55,590
Okay,

394
00:22:59,320 --> 00:23:01,116
I don't know. I have mixed feelings

395
00:23:01,248 --> 00:23:03,944
about this chapter because on one hand,

396
00:23:04,062 --> 00:23:07,124
you see, as far as I understand active

397
00:23:07,172 --> 00:23:11,304
inference, although it originated as,

398
00:23:11,502 --> 00:23:13,804
quote unquote, a unified theory of the

399
00:23:13,842 --> 00:23:17,356
brain, I don't think it's a

400
00:23:17,378 --> 00:23:19,870
neurobiological theory per se.

401
00:23:20,640 --> 00:23:24,220
Of course, there can be some

402
00:23:24,370 --> 00:23:27,388
correlations between neurobiological

403
00:23:27,564 --> 00:23:32,156
components or concepts with active

404
00:23:32,188 --> 00:23:37,084
inference concepts, but it's

405
00:23:37,132 --> 00:23:40,368
not an essential premise

406
00:23:40,464 --> 00:23:45,924
of active inference theory to

407
00:23:45,962 --> 00:23:49,076
provide a comprehensive theory about how

408
00:23:49,178 --> 00:23:52,730
the neurobiology of human brain or

409
00:23:53,340 --> 00:23:56,920
other organisms brain behave

410
00:23:57,500 --> 00:24:00,276
at detailed and neuroanatomical

411
00:24:00,308 --> 00:24:03,370
anatomical level.

412
00:24:03,680 --> 00:24:07,980
But then again, it's nice

413
00:24:08,050 --> 00:24:10,232
to have these kinds of empirical

414
00:24:10,296 --> 00:24:13,036
correlations between the findings of

415
00:24:13,058 --> 00:24:16,068
neurobiology and the active inference

416
00:24:16,264 --> 00:24:19,616
theory. But I

417
00:24:19,638 --> 00:24:22,960
don't think it's one of active inference

418
00:24:23,300 --> 00:24:26,992
central assertions, at least to my

419
00:24:27,046 --> 00:24:30,548
understanding. Well said.

420
00:24:30,634 --> 00:24:32,660
Very interesting framing.

421
00:24:33,480 --> 00:24:35,988
Well, chapter five definitely takes a

422
00:24:35,994 --> 00:24:38,756
very specific system of interest

423
00:24:38,858 --> 00:24:42,968
approach by highlighting one of the most

424
00:24:43,054 --> 00:24:45,368
studied areas, also one of the most

425
00:24:45,454 --> 00:24:50,100
relevant areas, which is mammalian

426
00:24:50,180 --> 00:24:53,888
neuroscience. And the chapter

427
00:24:54,004 --> 00:24:56,830
is going to introduce a few different

428
00:24:59,200 --> 00:25:02,668
motifs in the nervous system and

429
00:25:02,754 --> 00:25:06,750
essentially build up towards figure 5.5,

430
00:25:07,060 --> 00:25:10,272
which is at the end of the chapter, and

431
00:25:10,326 --> 00:25:13,744
5.5 wires together three specific

432
00:25:13,862 --> 00:25:16,448
neural systems that the chapter is going

433
00:25:16,454 --> 00:25:19,510
to focus on work in that area from.

434
00:25:19,960 --> 00:25:23,030
So Ali said it very well,

435
00:25:23,960 --> 00:25:26,260
active inference was built up to in

436
00:25:26,330 --> 00:25:29,844
chapter four. Here is another level or

437
00:25:29,882 --> 00:25:32,424
type of science with assertions or with

438
00:25:32,462 --> 00:25:34,344
representations or mappings to any

439
00:25:34,382 --> 00:25:37,496
specific system. But this is the kind of

440
00:25:37,678 --> 00:25:40,536
modeling that has been built up and done

441
00:25:40,638 --> 00:25:44,492
by Friston Parr Pazulo and others over

442
00:25:44,546 --> 00:25:47,948
the decades, with a focus coming from a

443
00:25:48,034 --> 00:25:51,660
human neuroimaging laboratory setting

444
00:25:52,080 --> 00:25:54,984
a lot of focus and study and attention

445
00:25:55,032 --> 00:25:56,944
and funding and everything on the

446
00:25:56,982 --> 00:25:59,250
mammalian nervous system.

447
00:26:01,380 --> 00:26:08,064
But claims about the nervous system are

448
00:26:08,102 --> 00:26:10,352
not the basis of what active inference

449
00:26:10,416 --> 00:26:13,088
claims or how it's derived.

450
00:26:13,264 --> 00:26:15,620
But this is like an example case study

451
00:26:15,690 --> 00:26:19,044
in neurobiology connecting back to

452
00:26:19,082 --> 00:26:21,444
some of the formalisms that we've just

453
00:26:21,482 --> 00:26:24,816
seen introduced in chapter four. Yeah,

454
00:26:24,858 --> 00:26:27,896
and to add a minor point to what you

455
00:26:27,918 --> 00:26:30,970
just said, I think it's important to

456
00:26:31,420 --> 00:26:34,632
draw attention to the last sentence of

457
00:26:34,766 --> 00:26:36,812
the last paragraph of the first page.

458
00:26:36,946 --> 00:26:38,920
It is important to draw a distinction

459
00:26:39,000 --> 00:26:41,756
between a principle I, e. The

460
00:26:41,778 --> 00:26:43,276
minimization of free energy, and a

461
00:26:43,298 --> 00:26:45,320
process theory about how this principle

462
00:26:45,400 --> 00:26:47,344
may be implemented in a certain kind of

463
00:26:47,382 --> 00:26:51,056
system. So I think this

464
00:26:51,158 --> 00:26:54,828
sentence here frames this chapter

465
00:26:55,004 --> 00:26:57,532
in relation to all the other technical

466
00:26:57,596 --> 00:27:00,804
chapters of this book. So if

467
00:27:01,002 --> 00:27:04,704
every other chapter is about developing,

468
00:27:04,752 --> 00:27:06,980
or at least up to now, was about

469
00:27:07,050 --> 00:27:10,768
developing the principled formalism

470
00:27:10,864 --> 00:27:13,720
of active inference, now, chapter five

471
00:27:13,870 --> 00:27:16,932
provides a kind of preliminary sketch

472
00:27:16,996 --> 00:27:19,348
for the process theory of active

473
00:27:19,364 --> 00:27:22,344
inference, which is obviously far from

474
00:27:22,382 --> 00:27:25,052
an extensive theory. It's just a single

475
00:27:25,106 --> 00:27:28,988
chapter. But then again, it can provide

476
00:27:29,074 --> 00:27:33,416
some important signposts

477
00:27:33,448 --> 00:27:36,412
for anyone who wants to further

478
00:27:36,476 --> 00:27:40,736
investigate this area awesome

479
00:27:40,918 --> 00:27:43,164
free energy. Principle, Bayesian

480
00:27:43,212 --> 00:27:46,976
mechanics, all things in that area are,

481
00:27:47,158 --> 00:27:50,276
on this principle, not responsive to

482
00:27:50,298 --> 00:27:53,140
empirical data. And then the process

483
00:27:53,210 --> 00:27:55,124
theory is about how the principle is

484
00:27:55,162 --> 00:27:57,664
implemented. So the specific generative

485
00:27:57,712 --> 00:28:00,948
models that are made and how well they

486
00:28:01,034 --> 00:28:03,636
map or how well they do in a portfolio

487
00:28:03,668 --> 00:28:05,560
of models that can have very different

488
00:28:05,630 --> 00:28:08,824
goals and assumptions and all of this.

489
00:28:09,022 --> 00:28:11,940
But the process theory implementation

490
00:28:12,020 --> 00:28:13,836
lets us develop hypotheses that are

491
00:28:13,858 --> 00:28:17,356
answerable to empirical data, like what

492
00:28:17,378 --> 00:28:21,356
is the kind of information or

493
00:28:21,458 --> 00:28:24,536
relationship between photons hitting

494
00:28:24,568 --> 00:28:28,220
the retina and changes in activity

495
00:28:28,300 --> 00:28:31,936
in neural systems? And that's an

496
00:28:31,958 --> 00:28:33,776
informational question or can be

497
00:28:33,798 --> 00:28:35,436
abstracted in a way to an informational

498
00:28:35,468 --> 00:28:38,016
question that, it turns out, does have

499
00:28:38,038 --> 00:28:41,300
empirical support and results in

500
00:28:41,370 --> 00:28:43,520
unique explanations and predictions.

501
00:28:43,680 --> 00:28:45,504
That doesn't mean that it always results

502
00:28:45,552 --> 00:28:47,840
in unique explanations and predictions,

503
00:28:48,000 --> 00:28:49,936
but a lot of citations are provided

504
00:28:49,968 --> 00:28:52,936
here. That's what we can explore in

505
00:28:52,958 --> 00:28:53,850
chapter five.

506
00:28:56,860 --> 00:28:59,604
The last paragraph of the first section

507
00:28:59,652 --> 00:29:03,444
describes that they're going to look at

508
00:29:03,582 --> 00:29:07,420
the three different neural systems.

509
00:29:07,760 --> 00:29:10,716
Okay, section 5.2, microcircuits and

510
00:29:10,738 --> 00:29:11,580
messages.

511
00:29:15,290 --> 00:29:18,860
What do you think, Ali? All right.

512
00:29:21,310 --> 00:29:25,770
This chapter begins from how message

513
00:29:25,840 --> 00:29:29,660
passing happens in neurobiological terms

514
00:29:30,690 --> 00:29:33,946
and compare it to the way active

515
00:29:33,978 --> 00:29:36,586
inference frames this message passing

516
00:29:36,618 --> 00:29:39,614
mechanism. And specifically, if we look

517
00:29:39,652 --> 00:29:44,018
at Figure 5.1 and

518
00:29:44,184 --> 00:29:47,134
compare this figure to the ones we've

519
00:29:47,182 --> 00:29:50,530
seen before in chapters

520
00:29:52,150 --> 00:29:53,986
one through four I think it was in

521
00:29:54,008 --> 00:29:57,030
chapter four, we can see some clear

522
00:29:57,100 --> 00:30:00,630
parallels between how this kind of

523
00:30:00,780 --> 00:30:04,726
cortical message passaging happens in

524
00:30:04,748 --> 00:30:08,178
the brain versus how it is framed in

525
00:30:08,204 --> 00:30:10,506
active inference literature. And as we

526
00:30:10,528 --> 00:30:14,790
can see it's clearly inspired

527
00:30:14,870 --> 00:30:18,330
by the neurobiology of the brain. But

528
00:30:18,480 --> 00:30:21,534
then it's important to keep in mind that

529
00:30:21,572 --> 00:30:24,782
it's not a direct one to one

530
00:30:24,916 --> 00:30:29,230
mapping between these two models.

531
00:30:30,290 --> 00:30:34,340
This is just a kind of, I don't know,

532
00:30:35,190 --> 00:30:38,770
an interesting or illuminating,

533
00:30:39,430 --> 00:30:43,378
if you like, parallel to keep in mind to

534
00:30:43,464 --> 00:30:47,720
somehow be a bit more confident about

535
00:30:48,250 --> 00:30:51,526
the viability of the theory we want to

536
00:30:51,548 --> 00:30:53,746
use for message passing and active

537
00:30:53,778 --> 00:30:57,414
inference. Which is to say, it's not

538
00:30:57,452 --> 00:31:00,714
some haphazard theory that's just been

539
00:31:00,752 --> 00:31:03,978
developed for practical reasons. It has

540
00:31:04,064 --> 00:31:07,530
some basis in neurobiology, although

541
00:31:08,190 --> 00:31:11,974
it's not necessarily fully congruent

542
00:31:12,022 --> 00:31:16,490
with every detail of neurobiology.

543
00:31:19,390 --> 00:31:21,646
Great. The specific example is going to

544
00:31:21,668 --> 00:31:24,614
involve this one region of mammalian

545
00:31:24,682 --> 00:31:28,222
cortex tissue that has these six layers,

546
00:31:28,286 --> 00:31:30,850
and there's a ton of neurobiology.

547
00:31:32,630 --> 00:31:35,746
The big takeaway for Figure 5.1 is that

548
00:31:35,768 --> 00:31:38,470
it's possible to graphically lay out

549
00:31:38,620 --> 00:31:41,286
nodes and variables and find some

550
00:31:41,308 --> 00:31:44,470
empirical correspondences, again,

551
00:31:44,540 --> 00:31:46,514
some unique explanations and predictions

552
00:31:46,562 --> 00:31:50,054
in certain cases. And that's

553
00:31:50,102 --> 00:31:52,842
one kind of modeling where it's really

554
00:31:52,896 --> 00:31:55,466
trying to understand and improve the

555
00:31:55,488 --> 00:31:58,762
ability to do correlation and

556
00:31:58,896 --> 00:32:01,286
intervention and counterfactual causal

557
00:32:01,318 --> 00:32:04,894
type analysis with the real system

558
00:32:04,932 --> 00:32:07,674
of interest. Or in a more pedagogical

559
00:32:07,722 --> 00:32:09,406
setting or in a research setting or an

560
00:32:09,428 --> 00:32:11,226
industrial setting, you might sweep

561
00:32:11,258 --> 00:32:13,566
across large families of structures of

562
00:32:13,588 --> 00:32:16,626
models and there's no need to be

563
00:32:16,728 --> 00:32:20,066
grounded to any biological structure at

564
00:32:20,088 --> 00:32:23,026
all. So this is just describing the

565
00:32:23,048 --> 00:32:26,162
specific neuroanatomical research that

566
00:32:26,216 --> 00:32:29,014
really arose out of the imaging work at

567
00:32:29,052 --> 00:32:31,718
UCL and the SPM package. That's where a

568
00:32:31,724 --> 00:32:35,334
lot of this comes from. 5.2.

569
00:32:35,452 --> 00:32:38,214
Yeah, go ahead. Sorry. Just as a side

570
00:32:38,252 --> 00:32:41,814
note, I think watching one of Thomas

571
00:32:41,862 --> 00:32:44,826
Parr's lectures on neurobiology of

572
00:32:44,848 --> 00:32:46,634
active inference, which is available on

573
00:32:46,672 --> 00:32:49,690
YouTube, would really help to understand

574
00:32:49,760 --> 00:32:52,190
the materials of this chapter better.

575
00:32:52,260 --> 00:32:55,040
So I highly recommend watching that one.

576
00:32:55,890 --> 00:33:00,880
Thanks. Figure 5.2

577
00:33:02,770 --> 00:33:05,954
gives a rerendering of a kind of

578
00:33:05,992 --> 00:33:09,106
classical view of a

579
00:33:09,128 --> 00:33:12,034
hierarchical predictive coding system

580
00:33:12,152 --> 00:33:16,286
works. So here abstracting

581
00:33:16,318 --> 00:33:19,862
a layer from the tissue six

582
00:33:19,916 --> 00:33:22,920
layer to just two layers. Here

583
00:33:23,290 --> 00:33:26,518
computational layers now and then

584
00:33:26,604 --> 00:33:28,642
showing how there's hierarchical

585
00:33:28,706 --> 00:33:31,100
communication within a layer, but also

586
00:33:32,750 --> 00:33:34,506
there's signaling within a layer. And

587
00:33:34,528 --> 00:33:38,342
there's a hierarchy in Bayesian

588
00:33:38,406 --> 00:33:41,482
modeling with variables that are higher

589
00:33:41,536 --> 00:33:43,834
order predictions about other variables.

590
00:33:43,962 --> 00:33:45,626
And that's the basis of the predictive

591
00:33:45,658 --> 00:33:49,726
coding architecture. So 5.2

592
00:33:49,828 --> 00:33:53,550
looks at some ways that something that

593
00:33:53,620 --> 00:33:56,270
resonates with the cerebral cortical

594
00:33:56,430 --> 00:33:58,962
architecture enables what might

595
00:33:59,016 --> 00:34:01,394
computationally look like or have some

596
00:34:01,432 --> 00:34:05,998
really strong and explanatory values

597
00:34:06,174 --> 00:34:09,666
in actually relating to computationally

598
00:34:09,698 --> 00:34:12,614
a hierarchical Bayesian model which

599
00:34:12,652 --> 00:34:15,030
could do various general tasks.

600
00:34:16,330 --> 00:34:19,662
All right, 5.3 is motor commands leaving

601
00:34:19,746 --> 00:34:22,586
the prefrontal cortex, going down to the

602
00:34:22,608 --> 00:34:25,900
butterfly looking cross section here.

603
00:34:26,270 --> 00:34:28,220
What is 5.3?

604
00:34:30,510 --> 00:34:34,014
Okay, so 5.3 moves to the

605
00:34:34,052 --> 00:34:35,950
other half of active inference

606
00:34:36,370 --> 00:34:41,054
framework, which is how

607
00:34:41,092 --> 00:34:44,298
it can model the decision making and

608
00:34:44,484 --> 00:34:48,946
ultimately the movement of the agent in

609
00:34:48,968 --> 00:34:52,526
order to minimize the expected

610
00:34:52,558 --> 00:34:54,494
free energy as opposed to variational

611
00:34:54,542 --> 00:34:57,650
free energy that we saw in perceptual

612
00:34:58,010 --> 00:35:02,374
half of active inference. So it

613
00:35:02,412 --> 00:35:06,674
again provides a kind of correlation

614
00:35:06,802 --> 00:35:09,646
or analogy between the structural

615
00:35:09,698 --> 00:35:13,820
neuroanatomy particularly related to

616
00:35:14,990 --> 00:35:19,482
the motor commands and how

617
00:35:19,536 --> 00:35:22,330
it can relates to active inference,

618
00:35:22,690 --> 00:35:24,826
particularly the continuous time active

619
00:35:24,858 --> 00:35:29,040
inference. So we can see that

620
00:35:31,730 --> 00:35:36,402
for the external event or,

621
00:35:36,536 --> 00:35:38,994
I'm sorry, for the external state, we

622
00:35:39,032 --> 00:35:40,882
can take, for example, the

623
00:35:41,016 --> 00:35:45,874
proprioceptive afferent. And then this

624
00:35:45,912 --> 00:35:48,754
proprioceptive afferent acts as a kind

625
00:35:48,792 --> 00:35:52,306
of y for the continuous time active

626
00:35:52,338 --> 00:35:57,730
inference which needs to be processed

627
00:35:57,890 --> 00:36:01,322
in a way to optimize the expected free

628
00:36:01,376 --> 00:36:04,650
energy and how it relates to both

629
00:36:04,720 --> 00:36:07,130
attention and precision.

630
00:36:08,670 --> 00:36:11,020
We will see a bit more detail about

631
00:36:13,810 --> 00:36:16,110
those terms and the relation between

632
00:36:16,180 --> 00:36:19,630
them in chapter eight. But I think

633
00:36:19,780 --> 00:36:24,438
here Section 5.3 provides

634
00:36:24,474 --> 00:36:28,574
a good summary about the general paths

635
00:36:28,622 --> 00:36:32,034
through the motor command systems of

636
00:36:32,072 --> 00:36:35,890
neurobiology. Great. I'd say.

637
00:36:35,960 --> 00:36:39,366
While the previous case study focused on

638
00:36:39,388 --> 00:36:42,200
how the connectivity within and between

639
00:36:42,730 --> 00:36:45,798
the Cortical columns could have a

640
00:36:45,804 --> 00:36:48,806
computational relationship with a

641
00:36:48,828 --> 00:36:51,154
Bayesian hierarchical predictive coding

642
00:36:51,202 --> 00:36:54,122
architecture. The argument of the second

643
00:36:54,176 --> 00:36:57,382
case study is that a continuous input,

644
00:36:57,446 --> 00:37:00,570
continuous output kind of set point

645
00:37:00,640 --> 00:37:04,634
seeking, reflexive motor

646
00:37:04,682 --> 00:37:08,734
behavior with a moving set point with

647
00:37:08,772 --> 00:37:11,802
a descending moving set point enabling

648
00:37:11,866 --> 00:37:13,854
motion by changing ultimately the set

649
00:37:13,892 --> 00:37:17,266
point and enabling variation in

650
00:37:17,288 --> 00:37:19,202
the strategies to reach that set point

651
00:37:19,256 --> 00:37:21,170
through different mechanisms.

652
00:37:22,230 --> 00:37:26,030
This is also describable in a compatible

653
00:37:26,190 --> 00:37:29,746
way that's

654
00:37:29,778 --> 00:37:32,758
a shorter section. Now, section 5.4

655
00:37:32,924 --> 00:37:34,870
subcortical structures.

656
00:37:36,810 --> 00:37:38,790
What would you say about this section?

657
00:37:41,070 --> 00:37:45,206
Okay, so subcortical structures

658
00:37:45,398 --> 00:37:50,154
are very important in the

659
00:37:50,192 --> 00:37:54,110
decision making and the planning

660
00:37:55,330 --> 00:37:59,760
of the agents. So obviously here

661
00:38:02,050 --> 00:38:05,380
we need another kind of analogy between

662
00:38:05,750 --> 00:38:09,214
the way that these plannings

663
00:38:09,262 --> 00:38:12,180
and decision making happen

664
00:38:12,790 --> 00:38:16,926
neuroanatomically with the way that

665
00:38:16,968 --> 00:38:19,062
it's framed in active inference. But

666
00:38:19,116 --> 00:38:22,514
again, we can see it's clearly

667
00:38:22,562 --> 00:38:26,006
based on at least some

668
00:38:26,028 --> 00:38:28,214
of the important elements we've seen

669
00:38:28,332 --> 00:38:31,130
from the previous chapters.

670
00:38:31,710 --> 00:38:35,386
So for example, we saw how policy

671
00:38:35,568 --> 00:38:38,682
is described or how it relates to

672
00:38:38,736 --> 00:38:42,206
outcomes and preference and so on. We

673
00:38:42,228 --> 00:38:45,626
can see those elements are directly

674
00:38:45,738 --> 00:38:48,910
inspired by neuroanatomical structures.

675
00:38:49,250 --> 00:38:52,814
So I

676
00:38:52,852 --> 00:38:57,106
guess that's at

677
00:38:57,128 --> 00:38:59,538
least in my opinion, this section here

678
00:38:59,624 --> 00:39:03,300
5.4 seems a bit more

679
00:39:05,430 --> 00:39:07,794
sketchy in the meaning that it doesn't

680
00:39:07,842 --> 00:39:12,150
go into quite the extensive details

681
00:39:12,490 --> 00:39:16,470
about how those structures

682
00:39:18,490 --> 00:39:20,954
can be compared. But for anyone who

683
00:39:20,992 --> 00:39:25,690
wants to further investigate

684
00:39:26,030 --> 00:39:29,050
these topics, there are some

685
00:39:29,200 --> 00:39:33,882
useful references put on here on pages

686
00:39:34,026 --> 00:39:36,320
93 and 94.

687
00:39:39,650 --> 00:39:41,562
Thanks. Yeah, it's really abbreviated

688
00:39:41,626 --> 00:39:45,298
and overviewed but we

689
00:39:45,384 --> 00:39:48,354
get an interlude from Table 5.1 with

690
00:39:48,392 --> 00:39:50,866
putative roles of neurotransmitters. So

691
00:39:50,968 --> 00:39:53,714
same perspective that we took before on

692
00:39:53,752 --> 00:39:56,990
neuroanatomical functionalism here

693
00:39:57,080 --> 00:40:00,710
directly translates to neurotransmitters

694
00:40:01,370 --> 00:40:03,526
reductionism or essentialism or

695
00:40:03,548 --> 00:40:05,174
something like that. So certainly all

696
00:40:05,212 --> 00:40:06,886
neurotransmitters and molecules, they

697
00:40:06,908 --> 00:40:09,014
play variable roles in different

698
00:40:09,052 --> 00:40:12,666
settings and this

699
00:40:12,688 --> 00:40:18,038
is the neat and Scruffy manifold

700
00:40:18,134 --> 00:40:20,826
all over again. One person might say

701
00:40:20,848 --> 00:40:22,858
well, we need a theory for every

702
00:40:22,944 --> 00:40:25,454
acetylcholine molecule in the world.

703
00:40:25,572 --> 00:40:27,726
They're all in a unique context. And

704
00:40:27,748 --> 00:40:29,914
someone else says all neurotransmitters

705
00:40:29,962 --> 00:40:31,726
are described by one parameter. In this

706
00:40:31,748 --> 00:40:33,678
model I'm getting value from it. So to

707
00:40:33,684 --> 00:40:36,386
me, that's an account. And somewhere in

708
00:40:36,408 --> 00:40:39,394
between is the work in this space,

709
00:40:39,592 --> 00:40:43,186
which is making an attempt to have a

710
00:40:43,208 --> 00:40:47,082
principled and falsifiable approach

711
00:40:47,246 --> 00:40:50,786
to model the computational

712
00:40:50,898 --> 00:40:54,466
aspects of specific regions and contexts

713
00:40:54,498 --> 00:40:57,074
and settings. And so Acetylcholine,

714
00:40:57,122 --> 00:40:59,934
Noradrenaline, Dopamine, and Serotonin

715
00:41:00,082 --> 00:41:03,820
are given a little mini review here.

716
00:41:04,510 --> 00:41:06,826
And so it's not an exhaustive or an

717
00:41:06,848 --> 00:41:08,986
exclusive claim. It's kind of a

718
00:41:09,088 --> 00:41:11,514
provocation from computational and

719
00:41:11,552 --> 00:41:14,926
molecular neuroscience, and people can

720
00:41:14,948 --> 00:41:16,974
look into the papers and also ones that

721
00:41:17,012 --> 00:41:18,800
probably have been published since.

722
00:41:20,930 --> 00:41:23,258
5.6 goes to Continuous and Discrete

723
00:41:23,274 --> 00:41:26,302
Hierarchies, which is graphically

724
00:41:26,366 --> 00:41:28,386
overviewed in Figure 5.5. So what would

725
00:41:28,408 --> 00:41:29,620
you say about this?

726
00:41:31,750 --> 00:41:34,146
Yeah, one interesting thing about this

727
00:41:34,168 --> 00:41:38,040
section is the observation that

728
00:41:38,970 --> 00:41:42,294
our lower level engagement with

729
00:41:42,332 --> 00:41:45,942
the environment can be most

730
00:41:45,996 --> 00:41:48,182
successfully characterized with

731
00:41:48,236 --> 00:41:51,034
continuous time formulations. But as we

732
00:41:51,072 --> 00:41:55,100
go up on the level of

733
00:41:56,030 --> 00:41:58,666
cognitive concepts or at the level of

734
00:41:58,688 --> 00:42:01,894
cognitive hierarchies and we come

735
00:42:01,952 --> 00:42:04,814
to concepts such as, I don't know,

736
00:42:04,852 --> 00:42:08,734
decisions or even beliefs and so on,

737
00:42:08,932 --> 00:42:12,894
we can reach the area that the

738
00:42:12,932 --> 00:42:15,730
discrete time situation would probably

739
00:42:15,800 --> 00:42:20,862
be more efficient to characterize

740
00:42:21,006 --> 00:42:23,860
the behavior of the agent. So this

741
00:42:28,010 --> 00:42:30,194
multiscale structure of active inference

742
00:42:30,242 --> 00:42:33,526
modeling is quite evident in the

743
00:42:33,548 --> 00:42:37,190
way that our message passing

744
00:42:37,690 --> 00:42:39,810
happens in our brain in terms of our

745
00:42:39,900 --> 00:42:43,660
lower level data processing up into

746
00:42:44,270 --> 00:42:48,570
consolidating the higher level cognitive

747
00:42:48,910 --> 00:42:52,382
concepts and ontologies awesome.

748
00:42:52,516 --> 00:42:53,440
Thank you.

749
00:42:55,410 --> 00:42:57,998
To me, Figure 5.5 demonstrates the kind

750
00:42:58,004 --> 00:43:01,422
of whole of body approach that

751
00:43:01,476 --> 00:43:03,054
you could imagine. There's so many

752
00:43:03,092 --> 00:43:05,646
organs and systems and phenomena for

753
00:43:05,668 --> 00:43:07,274
which there aren't specific generative

754
00:43:07,322 --> 00:43:08,802
models. So little can be said about

755
00:43:08,856 --> 00:43:11,362
situations where no generative model has

756
00:43:11,416 --> 00:43:14,066
been articulated, and here's one where

757
00:43:14,088 --> 00:43:15,858
it has. So it gives you also it's kind

758
00:43:15,864 --> 00:43:17,598
of like reading a Drosophila.

759
00:43:17,614 --> 00:43:20,734
Melanogaster review paper relatively.

760
00:43:20,782 --> 00:43:22,806
It's like, this is how much work it

761
00:43:22,828 --> 00:43:24,610
takes to get to this state of knowledge

762
00:43:24,690 --> 00:43:27,346
in an insect. So then in another insect,

763
00:43:27,378 --> 00:43:29,582
do we know less about that insect

764
00:43:29,666 --> 00:43:32,778
empirically and genetically? So consider

765
00:43:32,864 --> 00:43:35,946
this to be what's known to be a lot,

766
00:43:36,128 --> 00:43:39,242
however, also about one of the most

767
00:43:39,376 --> 00:43:42,386
sophisticated or specific cognitive

768
00:43:42,438 --> 00:43:44,586
systems at least we know. So there's

769
00:43:44,618 --> 00:43:45,902
that additional kind of like, self

770
00:43:45,956 --> 00:43:49,470
reflexive aspect to this chapter

771
00:43:49,890 --> 00:43:52,746
that is not a cornerstone of active

772
00:43:52,778 --> 00:43:55,054
inference, but here it's just presented

773
00:43:55,102 --> 00:43:56,980
in a synthetic case study.

774
00:43:58,230 --> 00:43:59,842
Anything else you want to say about

775
00:43:59,896 --> 00:44:03,778
five? Nothing particular

776
00:44:03,864 --> 00:44:06,500
comes to mind. All right.

777
00:44:31,000 --> 00:44:33,696
Okay. Chapter seven is called Active

778
00:44:33,728 --> 00:44:36,336
Inference and Discrete Time. Chapter

779
00:44:36,368 --> 00:44:39,248
seven is the first in a pair of chapters

780
00:44:39,344 --> 00:44:42,696
with chapter eight on Discrete and

781
00:44:42,718 --> 00:44:44,984
Continuous Time. So they're kind of like

782
00:44:45,022 --> 00:44:48,964
two forks of a river that we discussed

783
00:44:49,092 --> 00:44:52,548
in chapter four and before and described

784
00:44:52,564 --> 00:44:54,684
the recipe in chapter six. Now, seven

785
00:44:54,722 --> 00:44:56,492
and eight are kind of like one level

786
00:44:56,626 --> 00:45:00,350
deeper, going from the kind of all of

787
00:45:00,880 --> 00:45:03,212
this group of animals to one level

788
00:45:03,266 --> 00:45:06,336
deeper into its classification scheme on

789
00:45:06,358 --> 00:45:09,296
the way to the specific generative model

790
00:45:09,478 --> 00:45:12,752
for which it's actually given in its

791
00:45:12,806 --> 00:45:15,312
totality. But everything prior to that

792
00:45:15,366 --> 00:45:17,824
is about the learning about its

793
00:45:17,942 --> 00:45:20,788
principles. And this is kind of on the

794
00:45:20,874 --> 00:45:23,684
trunk of the path to discrete time

795
00:45:23,722 --> 00:45:25,668
modeling. Just like chapter eight will

796
00:45:25,674 --> 00:45:27,796
be about continuous time modeling. What

797
00:45:27,818 --> 00:45:29,590
would you add in?

798
00:45:32,300 --> 00:45:35,112
Okay, so I think chapters seven and

799
00:45:35,166 --> 00:45:40,136
eight really helps to understand in

800
00:45:40,158 --> 00:45:43,544
a more practical way how the materials

801
00:45:43,592 --> 00:45:46,380
from particularly chapters one through

802
00:45:46,450 --> 00:45:49,660
five applies in real time situations.

803
00:45:50,160 --> 00:45:54,830
So even if we

804
00:45:55,460 --> 00:45:58,464
somehow didn't get to understand every

805
00:45:58,502 --> 00:46:00,770
details of chapters one through four,

806
00:46:02,020 --> 00:46:04,464
when we come to chapters seven and

807
00:46:04,502 --> 00:46:07,716
eight, I think some

808
00:46:07,738 --> 00:46:10,852
of those uncertainties about our

809
00:46:10,906 --> 00:46:14,100
understandings can be clarified,

810
00:46:16,840 --> 00:46:20,216
at least in a practical sense. So I

811
00:46:20,238 --> 00:46:22,632
believe these two chapters are really

812
00:46:22,686 --> 00:46:26,024
helpful in order to consolidate our

813
00:46:26,062 --> 00:46:27,684
understandings from the previous

814
00:46:27,812 --> 00:46:31,210
chapters. Awesome, well said.

815
00:46:32,060 --> 00:46:34,284
So it's going to involve specifying some

816
00:46:34,322 --> 00:46:36,620
discrete time models.

817
00:46:38,880 --> 00:46:42,476
7.2 goes into perceptual processing and

818
00:46:42,498 --> 00:46:44,124
the general structure of the chapter is

819
00:46:44,162 --> 00:46:46,064
going to walk through a series of

820
00:46:46,102 --> 00:46:49,024
examples that build in complexity where

821
00:46:49,062 --> 00:46:52,400
they first start with perception in 7.2,

822
00:46:52,550 --> 00:46:55,152
introduce decision making and then

823
00:46:55,206 --> 00:46:57,940
describe a few more types of motifs or

824
00:46:58,010 --> 00:47:01,748
cognitive structure or patterns and

825
00:47:01,834 --> 00:47:04,548
also check out step by step and model

826
00:47:04,634 --> 00:47:07,476
stream one where it's built up to in a

827
00:47:07,498 --> 00:47:12,104
different way. So the first example is

828
00:47:12,222 --> 00:47:14,116
I'll let you describe it since it's

829
00:47:14,148 --> 00:47:17,272
musical. Okay.

830
00:47:17,326 --> 00:47:21,304
So yeah, the first example is the

831
00:47:21,342 --> 00:47:25,656
situation in which we try to describe

832
00:47:25,848 --> 00:47:29,000
the performance of an amateur musician

833
00:47:29,080 --> 00:47:32,780
in terms of how we listen

834
00:47:32,850 --> 00:47:34,616
to the performance of an amateur

835
00:47:34,648 --> 00:47:38,864
musicians in terms of the

836
00:47:38,902 --> 00:47:42,850
predictions we get from our

837
00:47:43,220 --> 00:47:46,736
anticipation of the following notes as

838
00:47:46,758 --> 00:47:49,492
opposed to the actual notes that's being

839
00:47:49,546 --> 00:47:53,872
played. So these kinds of anticipatory

840
00:47:53,936 --> 00:47:57,348
reaction listening reaction to

841
00:47:57,434 --> 00:47:59,932
the musician can be successfully

842
00:48:00,016 --> 00:48:03,636
formalized using discrete time active

843
00:48:03,668 --> 00:48:08,890
inference by putting up by

844
00:48:09,820 --> 00:48:14,270
putting together the matrices A for

845
00:48:15,040 --> 00:48:18,428
the states and matrix B for

846
00:48:18,514 --> 00:48:21,004
the transition between the states or the

847
00:48:21,042 --> 00:48:24,876
transition probabilities, which in this

848
00:48:24,898 --> 00:48:27,504
case describes the probability from

849
00:48:27,542 --> 00:48:30,576
going from one note to the other and

850
00:48:30,758 --> 00:48:34,668
obviously the actual sequence that's

851
00:48:34,684 --> 00:48:37,248
been played which can be described with

852
00:48:37,414 --> 00:48:42,468
the matrix d another

853
00:48:42,554 --> 00:48:46,470
point I wanted to mention is

854
00:48:47,160 --> 00:48:49,124
for anyone who has downloaded this

855
00:48:49,162 --> 00:48:52,536
chapter before I don't know, I think

856
00:48:52,638 --> 00:48:55,976
about June or something. I recommend

857
00:48:56,078 --> 00:48:58,808
redownloading it from MIT's website

858
00:48:58,894 --> 00:49:01,048
because they have corrected some of the

859
00:49:01,054 --> 00:49:03,628
typos that was previously present in

860
00:49:03,634 --> 00:49:06,140
this chapter, particularly in Figure

861
00:49:06,210 --> 00:49:07,470
7.2.

862
00:49:10,560 --> 00:49:14,910
Cool. So this graphical model where

863
00:49:15,220 --> 00:49:17,472
a person is listening, this is a general

864
00:49:17,606 --> 00:49:20,556
perceptual Bayesian framing. It's

865
00:49:20,588 --> 00:49:23,984
specified just like

866
00:49:24,022 --> 00:49:26,064
with any other equations. There's a lot

867
00:49:26,102 --> 00:49:29,796
to look into but A indicates the

868
00:49:29,818 --> 00:49:32,470
probability of an outcome given a state.

869
00:49:33,080 --> 00:49:36,084
This is saying if it were all on the

870
00:49:36,122 --> 00:49:37,908
diagonal like an identity matrix, this

871
00:49:37,914 --> 00:49:40,920
is kind of a common motif then states

872
00:49:40,990 --> 00:49:48,056
kind of map to themselves. So in

873
00:49:48,078 --> 00:49:50,792
the context of this model,

874
00:49:50,926 --> 00:49:53,484
a represents the mapping between the

875
00:49:53,522 --> 00:49:56,264
observed note and the underlying hidden

876
00:49:56,312 --> 00:49:59,132
true note and then B.

877
00:49:59,186 --> 00:50:02,076
Describes the transition matrix of how

878
00:50:02,098 --> 00:50:03,992
those change to time. D is the prior

879
00:50:04,136 --> 00:50:05,340
they're specified.

880
00:50:08,180 --> 00:50:11,650
Figure 7.2, do you want to describe it?

881
00:50:15,220 --> 00:50:18,356
All right, so in figure 7.2, or at

882
00:50:18,378 --> 00:50:21,012
least the incomplete version of Figure

883
00:50:21,066 --> 00:50:25,360
7.2 we see here well, at the upper

884
00:50:25,440 --> 00:50:28,740
left part of the picture we

885
00:50:28,810 --> 00:50:34,312
see the beliefs about each

886
00:50:34,366 --> 00:50:38,024
note at each time

887
00:50:38,062 --> 00:50:42,388
step and at upper right we

888
00:50:42,494 --> 00:50:45,870
somehow translate those beliefs into

889
00:50:46,240 --> 00:50:48,460
specific numerical values.

890
00:50:51,040 --> 00:50:54,284
So instead of just assigning some

891
00:50:54,322 --> 00:50:58,764
continuous values, we've simplified

892
00:50:58,892 --> 00:51:02,188
the situation by assigning some discrete

893
00:51:02,204 --> 00:51:05,628
numerical values for each node.

894
00:51:05,804 --> 00:51:09,044
And then the

895
00:51:09,082 --> 00:51:14,596
lower left is supposed to show the

896
00:51:14,618 --> 00:51:18,340
free energy gradients over

897
00:51:18,410 --> 00:51:22,232
time or in other terms the

898
00:51:22,286 --> 00:51:26,090
prediction errors we get from

899
00:51:28,140 --> 00:51:30,984
comparing our predictions with the

900
00:51:31,022 --> 00:51:35,400
actual outcomes. So lastly,

901
00:51:35,560 --> 00:51:39,756
the lower right picture shows in

902
00:51:39,778 --> 00:51:42,140
parallel to the upper right picture,

903
00:51:42,720 --> 00:51:46,640
determines the values of these errors.

904
00:51:47,060 --> 00:51:53,504
So we can see both the

905
00:51:53,542 --> 00:51:56,812
initial or at least initial continuous

906
00:51:56,956 --> 00:51:59,764
assignment and values and then the

907
00:51:59,802 --> 00:52:03,876
further discretizing of the values in

908
00:52:03,898 --> 00:52:07,348
order to get the discrete time situation

909
00:52:07,514 --> 00:52:10,712
or the more tractable discrete time

910
00:52:10,766 --> 00:52:11,800
situations.

911
00:52:13,980 --> 00:52:17,044
Okay, so it's a general passive

912
00:52:17,092 --> 00:52:19,380
inference task where there's priors

913
00:52:19,460 --> 00:52:20,904
about how states are going to change

914
00:52:20,942 --> 00:52:22,892
through time and then there's real data

915
00:52:22,946 --> 00:52:24,636
coming in. So that's the kind of

916
00:52:24,658 --> 00:52:27,308
classical predictive coding video

917
00:52:27,394 --> 00:52:30,312
compression coleman filter Bayesian

918
00:52:30,376 --> 00:52:34,376
setting 7.3 introduces

919
00:52:34,488 --> 00:52:38,064
a key motif which is decision making and

920
00:52:38,102 --> 00:52:40,944
planning as inference. So this is the

921
00:52:40,982 --> 00:52:44,368
idea of having a Bayes graph where the

922
00:52:44,374 --> 00:52:45,744
variables can relate to different

923
00:52:45,782 --> 00:52:48,224
things. There's high composability and

924
00:52:48,262 --> 00:52:51,236
here the idea is that a variable is

925
00:52:51,258 --> 00:52:52,804
going to be proposed that we can do

926
00:52:52,842 --> 00:52:55,476
inference about that describes the

927
00:52:55,498 --> 00:52:58,068
process of decision making or policy

928
00:52:58,154 --> 00:53:00,240
selection. So what would you say about

929
00:53:00,330 --> 00:53:01,530
7.3?

930
00:53:04,780 --> 00:53:09,096
Okay, so 7.3 is obviously

931
00:53:09,198 --> 00:53:12,952
similar to what we saw in chapter

932
00:53:13,016 --> 00:53:16,808
four. And if I'm not mistaken,

933
00:53:16,904 --> 00:53:19,708
even the topology is exactly the same

934
00:53:19,794 --> 00:53:22,300
with that picture we saw previously.

935
00:53:24,900 --> 00:53:30,352
This is the initial setup which

936
00:53:30,406 --> 00:53:34,656
acts also as a review about

937
00:53:34,758 --> 00:53:38,560
how these different components of

938
00:53:38,710 --> 00:53:42,084
palmdp generative models needs to

939
00:53:42,122 --> 00:53:47,344
be described in such situations.

940
00:53:47,472 --> 00:53:50,808
But ultimately the

941
00:53:50,974 --> 00:53:54,680
specific case study we

942
00:53:54,750 --> 00:53:58,696
come across in this section is the

943
00:53:58,718 --> 00:54:03,736
attempt to model the behavior of

944
00:54:03,758 --> 00:54:06,328
a mouse in a teammate of a rat and a

945
00:54:06,334 --> 00:54:10,920
teammate, especially teammates,

946
00:54:11,000 --> 00:54:15,028
containing an aversive stimulus

947
00:54:15,064 --> 00:54:17,808
in one arm and an attractive stimulus on

948
00:54:17,814 --> 00:54:20,912
the other. So this can

949
00:54:20,966 --> 00:54:25,072
act as a kind of toy example to use this

950
00:54:25,126 --> 00:54:28,070
kind of probabilistic modeling to

951
00:54:29,240 --> 00:54:32,660
describe these situations.

952
00:54:35,850 --> 00:54:37,734
Thanks. So that leads us right to figure

953
00:54:37,772 --> 00:54:40,934
7.4. Here's a visualization of the

954
00:54:40,972 --> 00:54:44,634
situation with the rat in this

955
00:54:44,672 --> 00:54:48,838
case where there's a pleasant

956
00:54:48,934 --> 00:54:52,634
and aversive stimuli on

957
00:54:52,672 --> 00:54:56,346
each end of a decision point and there's

958
00:54:56,378 --> 00:55:01,502
also an epistemic opportunity to

959
00:55:01,636 --> 00:55:04,782
receive some information about the

960
00:55:04,836 --> 00:55:08,320
context that the animal is in.

961
00:55:08,710 --> 00:55:12,130
And so that setting is described for

962
00:55:12,200 --> 00:55:14,002
both the case with white on the left,

963
00:55:14,056 --> 00:55:16,306
black on the right and black on the

964
00:55:16,328 --> 00:55:19,206
left, white on the right. And those are

965
00:55:19,228 --> 00:55:21,046
shown in terms of their differences in

966
00:55:21,068 --> 00:55:23,826
the matrices. The explicit specification

967
00:55:23,858 --> 00:55:29,074
of the generative model visualizations

968
00:55:29,202 --> 00:55:31,994
show some of the slices of the B

969
00:55:32,112 --> 00:55:35,082
variable which reflect different

970
00:55:35,136 --> 00:55:38,902
transition probabilities. C represents

971
00:55:38,966 --> 00:55:42,666
the preferences which are expressed over

972
00:55:42,848 --> 00:55:44,880
the observable states,

973
00:55:46,530 --> 00:55:50,320
d reflects the priors on the different

974
00:55:50,690 --> 00:55:52,350
states that need priors.

975
00:55:53,890 --> 00:55:57,300
7.4, what would you say about this?

976
00:55:59,750 --> 00:56:03,074
Okay, so in 7.4, it builds up

977
00:56:03,112 --> 00:56:07,122
on the previous section and adds other

978
00:56:07,176 --> 00:56:10,390
elements that we previously saw in

979
00:56:10,540 --> 00:56:14,200
chapters two and four,

980
00:56:14,650 --> 00:56:18,418
which is how the exact formulation

981
00:56:18,514 --> 00:56:22,010
for expected free energy

982
00:56:22,160 --> 00:56:25,530
can be used sorry, variation free energy

983
00:56:25,600 --> 00:56:28,442
can be used to formulate the trade off

984
00:56:28,496 --> 00:56:31,900
between the information

985
00:56:32,430 --> 00:56:37,854
seeking, or at least between

986
00:56:37,972 --> 00:56:39,918
the epistemic value and information

987
00:56:40,004 --> 00:56:43,818
seeking. So here it uses

988
00:56:43,994 --> 00:56:47,540
again that rad example in a bit more

989
00:56:49,270 --> 00:56:52,606
extended and elaborate form to formulate

990
00:56:52,638 --> 00:56:55,854
the epistemic value of observing a queue

991
00:56:55,902 --> 00:56:58,450
in a given location.

992
00:56:59,210 --> 00:57:03,126
And figure 7.7 is

993
00:57:03,148 --> 00:57:05,960
a representation of this situation.

994
00:57:07,530 --> 00:57:11,160
But another situation that's been,

995
00:57:13,630 --> 00:57:17,082
let me see yeah, in 7.9, another case

996
00:57:17,136 --> 00:57:22,314
study discussed here is

997
00:57:22,352 --> 00:57:26,170
the situation of the psychotic eye

998
00:57:26,330 --> 00:57:30,094
movements because it

999
00:57:30,132 --> 00:57:32,654
is something that can be quite

1000
00:57:32,772 --> 00:57:36,838
successfully described or characterized

1001
00:57:36,954 --> 00:57:39,662
in terms of information seeking versus

1002
00:57:39,726 --> 00:57:42,626
the epistemic value. And the situation

1003
00:57:42,728 --> 00:57:46,498
here is let me see.

1004
00:57:46,584 --> 00:57:49,894
Yeah. Shown visually in Figure

1005
00:57:50,012 --> 00:57:53,574
7.9, which clearly shows

1006
00:57:53,772 --> 00:57:58,226
how our visual psychotic

1007
00:57:58,418 --> 00:58:02,646
eye movement can be described

1008
00:58:02,678 --> 00:58:06,314
in such a way as to kind of trace the

1009
00:58:06,352 --> 00:58:10,374
trajectory of our eye movement among

1010
00:58:10,502 --> 00:58:13,420
different regions of the visual space

1011
00:58:15,890 --> 00:58:20,094
and how the information we gather from a

1012
00:58:20,132 --> 00:58:23,694
given region can

1013
00:58:23,732 --> 00:58:27,278
affect the subsequent

1014
00:58:27,374 --> 00:58:29,758
trajectories of our psychotic eye

1015
00:58:29,774 --> 00:58:30,450
movements.

1016
00:58:33,190 --> 00:58:37,074
That's basically the

1017
00:58:37,192 --> 00:58:41,400
main premise of this section, I guess.

1018
00:58:41,850 --> 00:58:43,000
Nice. Great.

1019
00:58:44,010 --> 00:58:47,654
7.5, what would you say

1020
00:58:47,692 --> 00:58:48,520
about it?

1021
00:58:51,870 --> 00:58:56,042
Okay, so 7.5 again

1022
00:58:56,096 --> 00:58:59,462
adds another dimension to the previous

1023
00:58:59,526 --> 00:59:03,434
formulations, and this time, we get to

1024
00:59:03,632 --> 00:59:07,280
update the generative models by learning

1025
00:59:07,970 --> 00:59:13,120
and the so

1026
00:59:14,050 --> 00:59:17,234
the generative models for this situation

1027
00:59:17,432 --> 00:59:19,426
is a bit more complicated than the

1028
00:59:19,448 --> 00:59:23,122
previous ones because it now needs

1029
00:59:23,176 --> 00:59:27,022
to account for a mechanism

1030
00:59:27,086 --> 00:59:31,014
or a way to update the matrices we had

1031
00:59:31,052 --> 00:59:34,934
before. So in the previous situations we

1032
00:59:34,972 --> 00:59:39,078
didn't account for learning

1033
00:59:39,164 --> 00:59:43,370
per se, but here we directly

1034
00:59:43,870 --> 00:59:47,722
update our general sorry, the word

1035
00:59:47,776 --> 00:59:50,746
update can be confusing. Here we get to

1036
00:59:50,768 --> 00:59:54,602
somehow improve our generative

1037
00:59:54,666 --> 00:59:59,950
models to accommodate for these updating

1038
01:00:00,370 --> 01:00:01,390
accounts.

1039
01:00:07,330 --> 01:00:11,140
The situation here or the case study

1040
01:00:12,310 --> 01:00:16,334
here, which somehow elucidate

1041
01:00:16,462 --> 01:00:21,494
the way that the learning can

1042
01:00:21,532 --> 01:00:25,122
be accounted for with these models

1043
01:00:25,266 --> 01:00:29,398
is again a toy example of

1044
01:00:29,564 --> 01:00:32,666
a creature in a simple

1045
01:00:32,768 --> 01:00:36,874
world of black and

1046
01:00:36,912 --> 01:00:40,294
white tiles which kind of tries

1047
01:00:40,342 --> 01:00:43,990
to find a path to reach

1048
01:00:44,160 --> 01:00:46,542
a given destination, a certain

1049
01:00:46,596 --> 01:00:49,982
destination. So it is

1050
01:00:50,116 --> 01:00:53,326
more complicated than the situation we

1051
01:00:53,348 --> 01:00:56,850
had for the Rat example because it only

1052
01:00:56,920 --> 01:01:00,590
had simple trajectories

1053
01:01:00,670 --> 01:01:04,366
that needed to traverse.

1054
01:01:04,398 --> 01:01:08,306
But here the creature, or the

1055
01:01:08,328 --> 01:01:12,520
agent in this case needs to do

1056
01:01:14,170 --> 01:01:17,366
lots more learning and information

1057
01:01:17,468 --> 01:01:20,450
seeking and so on. So all the previous

1058
01:01:20,530 --> 01:01:23,958
elements is kind of combined

1059
01:01:24,134 --> 01:01:27,434
in this example and it's a really good

1060
01:01:27,472 --> 01:01:29,562
example to see how the different

1061
01:01:29,616 --> 01:01:32,540
components of active inference can be

1062
01:01:33,310 --> 01:01:37,470
connected to each other. Nice and 76

1063
01:01:37,540 --> 01:01:40,574
hierarchical or deep inference. First,

1064
01:01:40,692 --> 01:01:44,074
a box 7.3 interlude on structure

1065
01:01:44,122 --> 01:01:47,474
learning, boxed off topic and a lot to

1066
01:01:47,512 --> 01:01:50,254
say, but structure learning broadly

1067
01:01:50,302 --> 01:01:53,266
refers to learning the structure about a

1068
01:01:53,288 --> 01:01:56,846
model using the same types of methods

1069
01:01:56,878 --> 01:01:59,254
that you might to do inference on, for

1070
01:01:59,292 --> 01:02:02,850
example, a more observable sensor

1071
01:02:02,930 --> 01:02:05,080
data reading, something like that.

1072
01:02:06,730 --> 01:02:10,534
This section works towards the idea

1073
01:02:10,652 --> 01:02:14,790
of nested inference or multiscale

1074
01:02:14,870 --> 01:02:17,290
modeling. What would you say about

1075
01:02:17,360 --> 01:02:18,700
figure 712?

1076
01:02:22,190 --> 01:02:27,200
Okay, so again, this situation is,

1077
01:02:27,730 --> 01:02:30,926
I think, the most complex situations of

1078
01:02:30,948 --> 01:02:34,686
this chapter, which builds up from the

1079
01:02:34,708 --> 01:02:38,558
previous sections. And this time it adds

1080
01:02:38,654 --> 01:02:44,034
another layer to accommodate for

1081
01:02:44,072 --> 01:02:48,910
the inferences that happen in different

1082
01:02:49,000 --> 01:02:51,670
time steps. So in this case we have

1083
01:02:51,740 --> 01:02:55,126
multi time or multiscale inference and

1084
01:02:55,148 --> 01:03:00,678
learning happening both at

1085
01:03:00,684 --> 01:03:02,646
the levels of learning and at the levels

1086
01:03:02,678 --> 01:03:07,994
of information seeking. So this

1087
01:03:08,032 --> 01:03:12,362
is represented in figure

1088
01:03:12,496 --> 01:03:16,038
seven, point twelve, which represents

1089
01:03:16,214 --> 01:03:20,570
how kind of this fractal

1090
01:03:20,730 --> 01:03:24,846
generative model can be seen as a

1091
01:03:24,868 --> 01:03:28,194
component in this

1092
01:03:28,232 --> 01:03:32,100
multiscale bigger generative or

1093
01:03:32,470 --> 01:03:35,326
as a kind of leaf in this bigger

1094
01:03:35,518 --> 01:03:39,350
generative model. So it can be seen

1095
01:03:39,420 --> 01:03:43,110
as a lower level

1096
01:03:43,180 --> 01:03:46,518
inference happening at the leaf level,

1097
01:03:46,684 --> 01:03:49,574
going up to the hierarchy and

1098
01:03:49,692 --> 01:03:52,140
influencing sorry,

1099
01:03:52,510 --> 01:03:55,500
collaborating on the whole process of

1100
01:03:56,270 --> 01:03:59,290
learning and inference at the higher

1101
01:03:59,360 --> 01:03:59,980
level.

1102
01:04:03,070 --> 01:04:07,066
Yeah, I guess that somehow

1103
01:04:07,178 --> 01:04:10,654
summarizes this figure. So if you have

1104
01:04:10,692 --> 01:04:14,320
anything to add, that's great.

1105
01:04:14,770 --> 01:04:18,882
It's an example of the composability of

1106
01:04:18,936 --> 01:04:23,010
generative models. What we've

1107
01:04:24,310 --> 01:04:27,598
talked about and had Toby Sinclair Smith

1108
01:04:27,694 --> 01:04:29,530
describe as as the compositional

1109
01:04:29,630 --> 01:04:32,870
cognitive cartography and just what

1110
01:04:32,940 --> 01:04:35,526
kinds of connectors can and can't you do

1111
01:04:35,708 --> 01:04:38,466
and how can that motif that the discrete

1112
01:04:38,498 --> 01:04:42,394
time model introduces and then the rest

1113
01:04:42,432 --> 01:04:44,266
of these features, including action and

1114
01:04:44,288 --> 01:04:46,394
learning and so on, get layered in on

1115
01:04:46,432 --> 01:04:49,260
top? What can you do with that?

1116
01:04:50,430 --> 01:04:53,980
713 gives another example.

1117
01:04:54,450 --> 01:04:56,254
Do you want to say anything about it or

1118
01:04:56,372 --> 01:04:59,742
maybe continue on? Yeah, so the case

1119
01:04:59,796 --> 01:05:05,002
study here is the example of linguistic,

1120
01:05:05,146 --> 01:05:08,162
I mean, language learning through

1121
01:05:08,296 --> 01:05:11,346
reading. So not language learning,

1122
01:05:11,528 --> 01:05:15,502
maybe just what happens sentence

1123
01:05:15,566 --> 01:05:17,954
comprehension reading. Yeah, in

1124
01:05:17,992 --> 01:05:20,294
comprehension. So what happens when

1125
01:05:20,412 --> 01:05:24,674
reading in an anticipatory

1126
01:05:24,722 --> 01:05:29,542
way the words that

1127
01:05:29,596 --> 01:05:34,154
comes each after the other. So why

1128
01:05:34,272 --> 01:05:36,954
this kind of situation can be most

1129
01:05:36,992 --> 01:05:39,354
successfully characterized with this

1130
01:05:39,392 --> 01:05:42,726
kind of modeling because it involves

1131
01:05:42,918 --> 01:05:45,694
different scales of learning and

1132
01:05:45,732 --> 01:05:52,686
comprehension, both at the level of at

1133
01:05:52,708 --> 01:05:56,174
the level of somehow observing the

1134
01:05:56,212 --> 01:05:59,458
letters and then going onto the words

1135
01:05:59,544 --> 01:06:02,370
and then word groups and so on. So,

1136
01:06:02,520 --> 01:06:06,260
yeah, that's really interesting way to

1137
01:06:06,630 --> 01:06:10,742
again combine all of those elements into

1138
01:06:10,796 --> 01:06:14,182
a single unified model to see how

1139
01:06:14,236 --> 01:06:16,454
those different timescales, slow and

1140
01:06:16,572 --> 01:06:21,240
fast timescales operate together

1141
01:06:23,070 --> 01:06:27,974
to build this more encompassing

1142
01:06:28,102 --> 01:06:30,742
model of more encompassing generative

1143
01:06:30,806 --> 01:06:31,980
model of the situation.

1144
01:06:33,550 --> 01:06:35,760
Great. Any closing thoughts on seven?

1145
01:06:40,610 --> 01:06:44,046
Nothing particular now, thanks. All

1146
01:06:44,068 --> 01:06:47,074
right, next chapter is chapter eight,

1147
01:06:47,112 --> 01:06:48,366
which is going to go into the continuous

1148
01:06:48,398 --> 01:06:48,980
time.

1149
01:07:01,560 --> 01:07:07,446
It's all

1150
01:07:07,468 --> 01:07:09,526
right. Chapter eight is called active

1151
01:07:09,558 --> 01:07:11,766
inference in continuous time. Begins

1152
01:07:11,798 --> 01:07:13,770
with that timeless quote, everything

1153
01:07:13,840 --> 01:07:16,666
flows, nothing stands still. So what

1154
01:07:16,688 --> 01:07:20,186
would you say about chapter eight? All

1155
01:07:20,208 --> 01:07:23,662
right, so this chapter probably

1156
01:07:23,796 --> 01:07:27,454
is my most favorite chapter in the book

1157
01:07:27,572 --> 01:07:31,040
because of my own personal interest in,

1158
01:07:31,970 --> 01:07:35,026
I don't know, process materialism and

1159
01:07:35,048 --> 01:07:35,780
so on.

1160
01:07:38,230 --> 01:07:41,806
Chapter seven acts

1161
01:07:41,838 --> 01:07:44,354
as a really good starting point for

1162
01:07:44,392 --> 01:07:47,906
anyone who wants to develop the discrete

1163
01:07:47,938 --> 01:07:50,902
time situations to model discrete time

1164
01:07:50,956 --> 01:07:54,146
situations within active inference

1165
01:07:54,178 --> 01:07:57,126
framework. But in chapter eight,

1166
01:07:57,308 --> 01:08:02,380
we kind of get to model

1167
01:08:02,990 --> 01:08:06,650
a bit more interesting or let's say

1168
01:08:06,720 --> 01:08:09,210
more involving situations.

1169
01:08:09,630 --> 01:08:14,686
And they're not necessarily kind

1170
01:08:14,708 --> 01:08:17,806
of toy examples we saw at least at the

1171
01:08:17,828 --> 01:08:20,720
beginning of chapter seven. So

1172
01:08:23,110 --> 01:08:25,554
obviously, as the title suggests, this

1173
01:08:25,592 --> 01:08:27,314
chapter deals with the continuous time

1174
01:08:27,352 --> 01:08:31,566
situation. So in that case we'll

1175
01:08:31,598 --> 01:08:35,986
need to maybe at this point refresh

1176
01:08:36,018 --> 01:08:38,662
our memory about what continuous time

1177
01:08:38,716 --> 01:08:41,606
situation involves by reading the

1178
01:08:41,628 --> 01:08:44,386
relevant parts, reading or reviewing

1179
01:08:44,418 --> 01:08:47,160
relevant parts of chapter four.

1180
01:08:49,870 --> 01:08:52,154
In chapter four, we saw that the

1181
01:08:52,192 --> 01:08:54,154
generative model for continuous time

1182
01:08:54,192 --> 01:08:57,782
situation derives

1183
01:08:57,846 --> 01:09:01,626
from the Edo's Stochastic

1184
01:09:01,658 --> 01:09:05,262
calculus in terms of putting

1185
01:09:05,396 --> 01:09:11,934
the whole process into two

1186
01:09:11,972 --> 01:09:14,978
elements of Stochastic equations, one of

1187
01:09:14,984 --> 01:09:18,802
which is the actual states,

1188
01:09:18,936 --> 01:09:22,450
the condition of actual states or

1189
01:09:22,520 --> 01:09:24,466
the behavior of the actual states. And

1190
01:09:24,488 --> 01:09:27,734
the other one is the randomness that

1191
01:09:27,772 --> 01:09:31,062
we need to account for in each real

1192
01:09:31,116 --> 01:09:33,574
time continuous time situations. So

1193
01:09:33,612 --> 01:09:38,778
that's what we get here in equation 8.1.

1194
01:09:38,944 --> 01:09:42,010
And then building up from that equation,

1195
01:09:44,990 --> 01:09:48,170
it generalizes that equation to involve

1196
01:09:52,030 --> 01:09:54,414
the functionals of G and F instead of

1197
01:09:54,452 --> 01:09:59,454
just the

1198
01:09:59,492 --> 01:10:04,290
single valued functions of GNF. So then

1199
01:10:04,360 --> 01:10:08,210
we get to put that

1200
01:10:08,360 --> 01:10:12,226
into the situation that can be

1201
01:10:12,248 --> 01:10:14,386
used for describing the behavior of

1202
01:10:14,408 --> 01:10:17,118
dynamical systems, which is a very well

1203
01:10:17,144 --> 01:10:21,078
known situation to use

1204
01:10:21,244 --> 01:10:24,310
these kinds of Stochastic equations.

1205
01:10:24,650 --> 01:10:29,434
And it's widely studied how those

1206
01:10:29,472 --> 01:10:31,850
kinds of dynamics can be characterized,

1207
01:10:32,270 --> 01:10:34,614
especially in recent Bayesian mechanics

1208
01:10:34,662 --> 01:10:38,374
paper by Dalton Saktivetevel

1209
01:10:38,422 --> 01:10:42,126
and others. And then it

1210
01:10:42,148 --> 01:10:44,766
gets to some more specific examples such

1211
01:10:44,788 --> 01:10:47,918
as Lotgobal Terra dynamics and

1212
01:10:48,004 --> 01:10:51,390
synchronicity and so on, in order

1213
01:10:51,460 --> 01:10:56,370
to show how these kinds of dynamics

1214
01:10:56,950 --> 01:11:01,540
can be elaborated upon and

1215
01:11:02,070 --> 01:11:07,158
can be generalized and

1216
01:11:07,244 --> 01:11:09,894
enables them to characterize more

1217
01:11:09,932 --> 01:11:12,070
complex situations.

1218
01:11:15,930 --> 01:11:19,382
That's a really short and brief overview

1219
01:11:19,446 --> 01:11:23,146
of the whole chapter. Maybe we

1220
01:11:23,168 --> 01:11:25,146
can talk about a bit more details as we

1221
01:11:25,168 --> 01:11:29,306
go through it. Great, well said.

1222
01:11:29,408 --> 01:11:32,422
Well, I'm sure for another day the

1223
01:11:32,496 --> 01:11:34,478
philosophical implications of eight,

1224
01:11:34,564 --> 01:11:36,654
Seven and Eight and High Road and Low

1225
01:11:36,692 --> 01:11:38,318
Road and all these other parts of the

1226
01:11:38,324 --> 01:11:41,566
textbook. Great topics. I agree. I would

1227
01:11:41,588 --> 01:11:44,478
see chapter eight as demonstrating

1228
01:11:44,574 --> 01:11:48,930
continuity with some classical

1229
01:11:49,990 --> 01:11:53,218
continuous time modeling motifs from a

1230
01:11:53,224 --> 01:11:55,614
few different areas of dynamical systems

1231
01:11:55,662 --> 01:11:58,774
science, which is applied in many,

1232
01:11:58,812 --> 01:12:00,626
many fields. But these are some classic

1233
01:12:00,658 --> 01:12:04,038
examples. So figure 8.1 goes a

1234
01:12:04,044 --> 01:12:06,630
little bit more into depth or at least

1235
01:12:06,700 --> 01:12:09,610
into more formalism detail about exactly

1236
01:12:09,680 --> 01:12:11,674
what we saw in chapter five with the

1237
01:12:11,712 --> 01:12:14,410
spinal reflex arc with the

1238
01:12:14,480 --> 01:12:17,786
proprioceptive data coming in and then a

1239
01:12:17,808 --> 01:12:19,386
differential being calculated with the

1240
01:12:19,408 --> 01:12:21,786
set point which reflects a descending

1241
01:12:21,818 --> 01:12:24,618
prediction from a decision making layer.

1242
01:12:24,794 --> 01:12:28,174
And that can be viewed as this kind of

1243
01:12:28,212 --> 01:12:31,274
mechanics that plays out in a phase

1244
01:12:31,322 --> 01:12:34,926
space in continuous time, like a spring

1245
01:12:35,038 --> 01:12:37,666
moving around with someone making a

1246
01:12:37,688 --> 01:12:40,194
certain path within a tractor and a

1247
01:12:40,232 --> 01:12:42,914
spring being dragged around something in

1248
01:12:42,952 --> 01:12:47,094
that area. Box 8.1

1249
01:12:47,212 --> 01:12:49,446
goes into a very fascinating topic. Do

1250
01:12:49,468 --> 01:12:50,760
you want to describe it?

1251
01:12:53,690 --> 01:12:57,490
Well, it's maybe one of the

1252
01:12:57,660 --> 01:13:02,406
most thought provoking pages

1253
01:13:02,438 --> 01:13:04,730
of the whole book. And if I remember

1254
01:13:04,800 --> 01:13:08,442
correctly, in all of the cohorts, this

1255
01:13:08,496 --> 01:13:13,070
particular box always

1256
01:13:13,140 --> 01:13:16,894
gives rise to lots of questions because

1257
01:13:16,932 --> 01:13:20,830
of some of the interesting and at least

1258
01:13:20,900 --> 01:13:24,850
initially counterintuitive claims

1259
01:13:25,190 --> 01:13:28,626
here. But I don't want

1260
01:13:28,648 --> 01:13:29,860
to spoil it.

1261
01:13:32,710 --> 01:13:36,206
But as a kind of spoiler alert,

1262
01:13:36,398 --> 01:13:40,260
it kind of gets to

1263
01:13:41,370 --> 01:13:44,870
really interesting, but alas,

1264
01:13:45,370 --> 01:13:48,474
very brief discussion about the

1265
01:13:48,512 --> 01:13:51,302
comparing these terms precision,

1266
01:13:51,366 --> 01:13:54,586
attention and sensory attenuation and

1267
01:13:54,768 --> 01:13:57,306
the relation and similarities and

1268
01:13:57,328 --> 01:14:01,102
difference between these three terms and

1269
01:14:01,156 --> 01:14:04,414
how understanding each of them is

1270
01:14:04,452 --> 01:14:06,494
essential to understanding the other

1271
01:14:06,532 --> 01:14:09,678
ones. But as I said,

1272
01:14:09,764 --> 01:14:12,480
it's a really interesting topic which

1273
01:14:12,870 --> 01:14:17,380
gives rise to lots of discussions and

1274
01:14:18,150 --> 01:14:22,030
I believe it's one of those topics

1275
01:14:22,110 --> 01:14:25,974
that's worth looking a bit

1276
01:14:26,012 --> 01:14:29,814
more looking into in some

1277
01:14:29,852 --> 01:14:32,902
other literature as well. Great. Well

1278
01:14:32,956 --> 01:14:36,054
said. What a cliffhanger. Next they go

1279
01:14:36,092 --> 01:14:39,222
to a classic model family called

1280
01:14:39,276 --> 01:14:42,486
Lockable Terra. These dynamics inherit

1281
01:14:42,518 --> 01:14:44,454
from characterizations of predator prey

1282
01:14:44,502 --> 01:14:47,386
dynamics in ecology. So it's kind of a

1283
01:14:47,408 --> 01:14:49,562
classical ecology model shown in Figure

1284
01:14:49,616 --> 01:14:52,846
8.2 on the top. It's actually the

1285
01:14:52,948 --> 01:14:55,838
ecosystem model plants, herbivores and

1286
01:14:55,844 --> 01:14:58,586
carnivores which follow different kinds

1287
01:14:58,618 --> 01:15:01,802
of oscillatory trends in continuous

1288
01:15:01,866 --> 01:15:05,346
time. And so that also has enabled it to

1289
01:15:05,368 --> 01:15:07,682
be applied for other so called

1290
01:15:07,736 --> 01:15:10,274
winnerless competitions. And that

1291
01:15:10,312 --> 01:15:12,750
relates to topics like neural Darwinism

1292
01:15:12,830 --> 01:15:14,962
and also neural dynamics where things

1293
01:15:15,016 --> 01:15:17,058
have kind of oscillatory relationships

1294
01:15:17,074 --> 01:15:19,570
with each other which are being modeled

1295
01:15:19,730 --> 01:15:23,960
as a continuous time underlying process

1296
01:15:24,570 --> 01:15:26,566
with a lot of measurement, noise and

1297
01:15:26,588 --> 01:15:28,978
discretization through space and time.

1298
01:15:29,084 --> 01:15:30,634
Those are the kinds of algorithms that

1299
01:15:30,672 --> 01:15:33,018
SPM explores more and there's lock of

1300
01:15:33,024 --> 01:15:35,014
Voltera and a lot of other dynamical

1301
01:15:35,062 --> 01:15:38,886
systems theory in SPM. So active

1302
01:15:38,918 --> 01:15:42,446
inference kind of adds action and more

1303
01:15:42,628 --> 01:15:44,986
to what was laid out from a pure

1304
01:15:45,018 --> 01:15:48,746
dynamical systems theory in SPM.

1305
01:15:48,938 --> 01:15:51,054
Here. It really is just showing the

1306
01:15:51,092 --> 01:15:54,178
ecology example and how you can project

1307
01:15:54,264 --> 01:15:55,662
if you have three different species,

1308
01:15:55,726 --> 01:15:57,554
you can think about that motion in a

1309
01:15:57,592 --> 01:16:00,994
cube or tetrahedron, and then you could

1310
01:16:01,032 --> 01:16:05,262
project onto kind of like looking

1311
01:16:05,336 --> 01:16:07,618
at a lower dimensional manifold relating

1312
01:16:07,714 --> 01:16:11,142
just two of the three species. And that

1313
01:16:11,196 --> 01:16:13,862
evinces this kind of oscillatory but

1314
01:16:13,916 --> 01:16:18,422
also moving behavior that gets connected

1315
01:16:18,486 --> 01:16:22,214
in Figure 8.3 to neurobiology.

1316
01:16:22,342 --> 01:16:23,980
What would you say about this?

1317
01:16:26,830 --> 01:16:30,278
Okay, so here in Figure 8.3

1318
01:16:30,464 --> 01:16:34,078
we see some applications of a

1319
01:16:34,084 --> 01:16:38,400
lot of altera dynamics. So the

1320
01:16:38,850 --> 01:16:44,370
left column here represents

1321
01:16:44,950 --> 01:16:49,266
what happens in

1322
01:16:49,448 --> 01:16:54,306
eye blink conditioning. So of

1323
01:16:54,328 --> 01:16:58,198
course here we need to account

1324
01:16:58,284 --> 01:17:02,114
for the expected

1325
01:17:02,162 --> 01:17:06,998
states of

1326
01:17:07,004 --> 01:17:10,794
the sequences of events that happens in

1327
01:17:10,912 --> 01:17:15,846
the eye blinking. So the upper

1328
01:17:15,878 --> 01:17:19,626
left figure shows the expectations in

1329
01:17:19,648 --> 01:17:23,838
terms of time. And then the

1330
01:17:24,004 --> 01:17:26,746
parallel right hand side equation,

1331
01:17:26,938 --> 01:17:30,106
sorry, right hand side figures

1332
01:17:30,298 --> 01:17:34,266
shows the Lotka

1333
01:17:34,298 --> 01:17:38,098
volterra systems that is applied in

1334
01:17:38,104 --> 01:17:40,914
the handwriting situation. So, as we can

1335
01:17:40,952 --> 01:17:45,850
see, although the mathematical

1336
01:17:45,950 --> 01:17:48,566
technology is the same, or at least the

1337
01:17:48,588 --> 01:17:51,750
modeling technology is the same, the

1338
01:17:51,820 --> 01:17:55,218
outcome of each situation varies

1339
01:17:55,314 --> 01:17:59,370
drastically in two distinct

1340
01:18:02,350 --> 01:18:05,130
two distinct neurobiological behavior,

1341
01:18:05,630 --> 01:18:07,894
not neurobiological, but biological

1342
01:18:07,942 --> 01:18:11,918
behavior. So, yeah, we can see how

1343
01:18:12,084 --> 01:18:15,694
the same modeling framework can give

1344
01:18:15,732 --> 01:18:20,282
rise to different outcomes

1345
01:18:20,426 --> 01:18:23,730
based on what parameters

1346
01:18:24,790 --> 01:18:27,630
needs to be optimized, what parameters

1347
01:18:27,790 --> 01:18:30,546
are selected for the modeling, and so

1348
01:18:30,568 --> 01:18:34,100
on. So I believe it's a quite

1349
01:18:35,110 --> 01:18:37,918
interesting example to compare

1350
01:18:38,094 --> 01:18:41,990
handwriting and a blinking together and

1351
01:18:42,140 --> 01:18:46,006
how those can be compared to

1352
01:18:46,028 --> 01:18:48,178
each other using the lotkobal

1353
01:18:48,194 --> 01:18:51,340
thermodynamics great. Thank you.

1354
01:18:51,710 --> 01:18:55,034
Box 8.2 gives a variant on

1355
01:18:55,072 --> 01:18:57,706
the learning here presented with the

1356
01:18:57,728 --> 01:18:59,946
formalism for continuous models. Kind of

1357
01:18:59,968 --> 01:19:03,502
a technical aside, section 8.4

1358
01:19:03,556 --> 01:19:05,934
is about generalized synchrony. So

1359
01:19:05,972 --> 01:19:09,134
figure 8.4 is going to visualize one of

1360
01:19:09,172 --> 01:19:11,966
the classic dynamical systems, which is

1361
01:19:11,988 --> 01:19:13,938
the Lorenz attractor. So what would you

1362
01:19:13,944 --> 01:19:17,202
say about this figure? Okay,

1363
01:19:17,256 --> 01:19:21,134
so this section is truly

1364
01:19:21,182 --> 01:19:24,926
interesting because when one thinks

1365
01:19:24,958 --> 01:19:29,126
of active inference, probably the

1366
01:19:29,148 --> 01:19:31,686
first situations that comes to mind is

1367
01:19:31,868 --> 01:19:34,790
the situations in which we have quite

1368
01:19:34,860 --> 01:19:38,254
well defined probability distributions

1369
01:19:38,322 --> 01:19:41,274
for different parameters. But as we can

1370
01:19:41,312 --> 01:19:45,580
see here in Section 8.4,

1371
01:19:45,950 --> 01:19:48,166
actually some of the formalism of active

1372
01:19:48,198 --> 01:19:51,440
inference can be successfully used

1373
01:19:51,890 --> 01:19:55,290
to characterize even chaotic systems,

1374
01:19:55,450 --> 01:19:58,526
and in particular the way in which two

1375
01:19:58,628 --> 01:20:01,406
chaotic systems can be synchronized with

1376
01:20:01,428 --> 01:20:04,654
each other. So this is a classic

1377
01:20:04,702 --> 01:20:08,020
example of chaotic Lorentz system

1378
01:20:09,430 --> 01:20:12,958
and it draws

1379
01:20:12,974 --> 01:20:16,890
upon from some of Professor Prison's

1380
01:20:16,910 --> 01:20:19,910
earlier work on birdsong synchrony.

1381
01:20:20,490 --> 01:20:23,570
And as a side note, any literature

1382
01:20:23,650 --> 01:20:26,754
before 2016 is considered

1383
01:20:26,882 --> 01:20:29,846
earlier history in active inference

1384
01:20:29,878 --> 01:20:32,922
literature because it evolves quite

1385
01:20:32,976 --> 01:20:33,850
rapidly.

1386
01:20:37,870 --> 01:20:40,814
This kind of synchrony between two

1387
01:20:40,932 --> 01:20:45,850
chaotic systems can be interpreted

1388
01:20:45,930 --> 01:20:49,200
as providing evidence or even,

1389
01:20:51,170 --> 01:20:54,866
let's say, a way to model a

1390
01:20:54,888 --> 01:20:57,220
kind of primitive theory of mind,

1391
01:20:58,310 --> 01:21:02,500
in the sense that how exactly can we

1392
01:21:03,670 --> 01:21:08,422
understand or two

1393
01:21:08,476 --> 01:21:11,714
agents can trace each other's

1394
01:21:11,762 --> 01:21:16,790
trajectories without any

1395
01:21:16,940 --> 01:21:20,314
I mean, engaging in any direct

1396
01:21:20,432 --> 01:21:24,378
exchange of observations between their

1397
01:21:24,544 --> 01:21:27,594
internal and external states. So, yeah,

1398
01:21:27,632 --> 01:21:30,766
that's a really good example, and I

1399
01:21:30,788 --> 01:21:32,158
believe one of the most interesting

1400
01:21:32,244 --> 01:21:34,734
examples of how active inference can

1401
01:21:34,772 --> 01:21:37,726
even account for these kinds of

1402
01:21:37,908 --> 01:21:41,374
behavior. And the rest

1403
01:21:41,412 --> 01:21:44,082
of the section goes into the details of

1404
01:21:44,136 --> 01:21:47,220
how this kind of synchrony between

1405
01:21:51,430 --> 01:21:54,900
multiscale Lorentz systems can happen

1406
01:21:56,090 --> 01:21:59,542
and how can we formulate it

1407
01:21:59,596 --> 01:22:01,506
mathematically in terms of continuous

1408
01:22:01,538 --> 01:22:02,950
time active inference.

1409
01:22:04,330 --> 01:22:06,802
Awesome. And there's been more recent

1410
01:22:06,866 --> 01:22:10,054
work on Markov Blankets and Stochastic

1411
01:22:10,102 --> 01:22:13,130
chaos. But the Bird example is a classic

1412
01:22:13,790 --> 01:22:16,506
8.5 goes into hybrid discrete and

1413
01:22:16,528 --> 01:22:19,546
continuous models. So this could be kind

1414
01:22:19,568 --> 01:22:22,614
of like an in between chapter of seven

1415
01:22:22,672 --> 01:22:24,174
and eight. But now that we've been

1416
01:22:24,212 --> 01:22:26,666
introduced to the pure form of discrete

1417
01:22:26,698 --> 01:22:28,778
and the pure form of continuous models

1418
01:22:28,874 --> 01:22:30,954
here shown that that composability

1419
01:22:31,082 --> 01:22:34,642
extends to so called hybrid models where

1420
01:22:34,696 --> 01:22:38,194
here the lower level visually is

1421
01:22:38,232 --> 01:22:41,586
using the continuous time formalism and

1422
01:22:41,688 --> 01:22:44,274
the higher level is describing a little

1423
01:22:44,312 --> 01:22:46,982
line added here, the discrete time

1424
01:22:47,116 --> 01:22:50,566
formalism. And this was the similar

1425
01:22:50,668 --> 01:22:54,086
structure described by the authors of

1426
01:22:54,108 --> 01:22:56,694
the paper. Active inference does not

1427
01:22:56,732 --> 01:22:59,882
contradict folk psychology where they

1428
01:22:59,936 --> 01:23:01,814
described this lower level as motor

1429
01:23:01,862 --> 01:23:04,646
active inference which was closely

1430
01:23:04,758 --> 01:23:08,870
allied with the spinal arc reflex shown

1431
01:23:08,950 --> 01:23:12,126
above. And then this higher level they

1432
01:23:12,148 --> 01:23:14,462
called decision active inference because

1433
01:23:14,516 --> 01:23:15,998
in that case it was referring to a

1434
01:23:16,004 --> 01:23:19,294
discrete decision. And so they used that

1435
01:23:19,332 --> 01:23:22,678
kind of basic motif of continuous

1436
01:23:22,794 --> 01:23:25,554
activity or continuous time modeling at

1437
01:23:25,592 --> 01:23:28,018
the more peripheral aspects of a

1438
01:23:28,024 --> 01:23:30,866
cognitive entity and like Ali said,

1439
01:23:31,048 --> 01:23:35,118
more discretization and hybridization

1440
01:23:35,214 --> 01:23:38,390
as well at higher levels of the

1441
01:23:38,460 --> 01:23:39,910
cognitive modeling.

1442
01:23:41,850 --> 01:23:44,760
And that type of an architecture here,

1443
01:23:45,290 --> 01:23:47,586
instead of describing who wants the ice

1444
01:23:47,618 --> 01:23:51,066
cream cone, I believe here it's going to

1445
01:23:51,168 --> 01:23:54,266
be a mixed or a hybrid model that is

1446
01:23:54,288 --> 01:23:58,060
going to call back the Icicade system

1447
01:23:58,670 --> 01:24:02,302
where there's a fixed point that

1448
01:24:02,356 --> 01:24:06,240
is able to be moved as a set point,

1449
01:24:06,690 --> 01:24:08,542
and then there's a continuous time

1450
01:24:08,596 --> 01:24:11,386
icicade that pursues the new fixed

1451
01:24:11,418 --> 01:24:14,290
point. And so that's analogous to a new

1452
01:24:14,440 --> 01:24:17,102
set point or fixed point being specified

1453
01:24:17,166 --> 01:24:19,826
from the top down muscle command about a

1454
01:24:19,848 --> 01:24:22,562
new location for a muscle followed by

1455
01:24:22,616 --> 01:24:25,482
movement towards it. This is a muscular

1456
01:24:25,646 --> 01:24:28,806
activity that is realizing that but not

1457
01:24:28,828 --> 01:24:30,694
in the elbow coming away from the hot

1458
01:24:30,732 --> 01:24:33,986
stove. This is about the eye circading

1459
01:24:34,018 --> 01:24:37,026
to an epistemic foraging location

1460
01:24:37,218 --> 01:24:40,182
specified by top down hierarchical

1461
01:24:40,246 --> 01:24:43,894
systems. 8.3 describes little technical

1462
01:24:43,942 --> 01:24:47,014
aside on mixture of Gaussian gaussian

1463
01:24:47,062 --> 01:24:50,950
mixture models, kind of a technical

1464
01:24:51,030 --> 01:24:55,390
modeling note. And 8.6 closes

1465
01:24:55,810 --> 01:24:57,806
it says it's a huge topic and much has

1466
01:24:57,828 --> 01:24:59,594
been left out. And so they list in Table

1467
01:24:59,642 --> 01:25:02,094
8.1 key advances in continuous time

1468
01:25:02,132 --> 01:25:05,006
models. And those areas are synthetic

1469
01:25:05,038 --> 01:25:08,126
birdsong, ocular, motor delays,

1470
01:25:08,318 --> 01:25:10,526
conditioned reflexes, smooth pursuit,

1471
01:25:10,558 --> 01:25:12,850
eye movements, psychosis illusions,

1472
01:25:13,190 --> 01:25:16,690
cicades action, observation attention,

1473
01:25:16,770 --> 01:25:19,110
hybrid models and self organization.

1474
01:25:19,930 --> 01:25:22,454
And that's chapter eight. What else

1475
01:25:22,492 --> 01:25:24,662
would you say? And also what would you

1476
01:25:24,716 --> 01:25:27,014
kind of lead someone to in the

1477
01:25:27,052 --> 01:25:29,338
philosophical implications of eight?

1478
01:25:29,424 --> 01:25:31,020
Because it sounds kind of cool.

1479
01:25:34,190 --> 01:25:37,420
Okay, well,

1480
01:25:37,790 --> 01:25:39,606
the case of continuous time active

1481
01:25:39,638 --> 01:25:40,490
inference,

1482
01:25:43,650 --> 01:25:47,422
I think it leads to really

1483
01:25:47,476 --> 01:25:50,880
interesting questions both in terms of

1484
01:25:51,330 --> 01:25:55,620
philosophical questions and also more

1485
01:25:56,150 --> 01:25:59,554
practical modeling questions about

1486
01:25:59,752 --> 01:26:02,414
what parameters needs to be accounted

1487
01:26:02,462 --> 01:26:05,598
for and so on. And as I said,

1488
01:26:05,704 --> 01:26:09,622
I believe it's a more

1489
01:26:09,676 --> 01:26:13,718
interesting way of it's not

1490
01:26:13,884 --> 01:26:16,930
interesting, but at least more involved

1491
01:26:17,010 --> 01:26:20,150
way of doing active inference modeling.

1492
01:26:20,230 --> 01:26:23,706
But one thing that one

1493
01:26:23,728 --> 01:26:26,300
of the philosophical questions that

1494
01:26:27,870 --> 01:26:31,566
Mahault and I have explored in our

1495
01:26:31,588 --> 01:26:35,454
paper is how the

1496
01:26:35,492 --> 01:26:36,510
processes,

1497
01:26:37,970 --> 01:26:41,200
ontological processes can

1498
01:26:41,970 --> 01:26:45,890
philosophically described using FEP

1499
01:26:46,470 --> 01:26:51,278
assertions in terms of their intraaction

1500
01:26:51,374 --> 01:26:55,234
with the environment in

1501
01:26:55,272 --> 01:26:57,560
which they co constitute themselves.

1502
01:26:57,930 --> 01:27:02,290
And we don't necessarily distinguish

1503
01:27:02,370 --> 01:27:05,954
between the internal and the external

1504
01:27:06,002 --> 01:27:09,226
states. So one obvious example of this

1505
01:27:09,328 --> 01:27:12,250
is that generalized synchrony example

1506
01:27:12,320 --> 01:27:15,066
that we saw in this chapter in which we

1507
01:27:15,088 --> 01:27:18,522
don't necessarily distinguish between

1508
01:27:18,656 --> 01:27:22,910
which of the birds act as the

1509
01:27:23,060 --> 01:27:27,082
agent and which one is the environment

1510
01:27:27,226 --> 01:27:30,206
or the vice versa. So these kind of co

1511
01:27:30,228 --> 01:27:32,858
constitution of the environment and the

1512
01:27:32,884 --> 01:27:36,034
agent which gives rise to the

1513
01:27:36,072 --> 01:27:38,466
partitioning of state space through

1514
01:27:38,568 --> 01:27:42,386
markup blanket is one of the

1515
01:27:42,488 --> 01:27:46,210
interesting philosophical

1516
01:27:46,290 --> 01:27:48,870
points that I think needs to be

1517
01:27:49,020 --> 01:27:52,840
elaborated a bit. More using

1518
01:27:54,010 --> 01:27:56,806
some of the recent advances in

1519
01:27:56,908 --> 01:28:00,438
philosophy, such as the tools that's

1520
01:28:00,454 --> 01:28:04,138
been developed in New Materialism School

1521
01:28:04,224 --> 01:28:08,022
or some other philosophical approaches.

1522
01:28:08,086 --> 01:28:12,480
But yeah, these kinds of what exactly

1523
01:28:12,930 --> 01:28:15,566
gives rise to emergence? What is the

1524
01:28:15,748 --> 01:28:18,202
ontological status of emergent

1525
01:28:18,266 --> 01:28:21,520
properties and so on, are some of the

1526
01:28:23,030 --> 01:28:25,486
burning questions for many philosophers

1527
01:28:25,518 --> 01:28:28,094
today. And I believe active inference,

1528
01:28:28,142 --> 01:28:30,126
and particularly continuous time active

1529
01:28:30,158 --> 01:28:32,980
inference, provides a clear,

1530
01:28:34,390 --> 01:28:38,150
precise mathematical formalism,

1531
01:28:39,930 --> 01:28:43,558
even if not to answer

1532
01:28:43,644 --> 01:28:46,354
these questions, but at least to explore

1533
01:28:46,402 --> 01:28:49,750
it in a more rigorous and practical

1534
01:28:49,830 --> 01:28:53,782
way, and also practical

1535
01:28:53,846 --> 01:28:58,074
and attractable way. So this

1536
01:28:58,112 --> 01:29:01,754
is the area that I believe philosophy

1537
01:29:01,802 --> 01:29:05,470
and science are beautifully intertwined

1538
01:29:05,810 --> 01:29:09,600
into a coherent view of not only

1539
01:29:10,290 --> 01:29:13,050
the phenomena of interest, but even

1540
01:29:13,220 --> 01:29:15,060
about the whole world.

1541
01:29:17,430 --> 01:29:18,180
Wow.

1542
01:29:21,350 --> 01:29:23,602
Pretty cool. Yeah. A lot to say about

1543
01:29:23,656 --> 01:29:27,538
that topic. After completing

1544
01:29:27,634 --> 01:29:30,182
chapters seven and eight, you've seen

1545
01:29:30,236 --> 01:29:33,222
the kind of two major branches or two

1546
01:29:33,276 --> 01:29:36,486
major motifs of just one kind of

1547
01:29:36,508 --> 01:29:38,934
modeling. But these kind of models have

1548
01:29:38,972 --> 01:29:41,706
so many different forms that that's why

1549
01:29:41,728 --> 01:29:44,102
it's such a hands on process to specify

1550
01:29:44,166 --> 01:29:46,506
the generative model in chapter six and

1551
01:29:46,528 --> 01:29:48,954
fit it with data in chapter nine. Those

1552
01:29:48,992 --> 01:29:50,638
are all what's required. And that's kind

1553
01:29:50,644 --> 01:29:53,582
of the last mile of where these

1554
01:29:53,716 --> 01:29:56,622
discussions about general motifs gets

1555
01:29:56,676 --> 01:29:58,846
you. But also playing with these

1556
01:29:58,868 --> 01:30:02,122
pedagogical models can be really helpful

1557
01:30:02,266 --> 01:30:04,274
because it will help you understand the

1558
01:30:04,312 --> 01:30:07,298
basic patterns and relationships and

1559
01:30:07,464 --> 01:30:10,146
start to see different patterns in the

1560
01:30:10,168 --> 01:30:13,378
graphical models and know from there

1561
01:30:13,464 --> 01:30:16,946
what levels of technical processes can

1562
01:30:16,968 --> 01:30:19,300
be kind of coarse grained over.

1563
01:30:20,950 --> 01:30:21,860
All right.

1564
01:30:27,140 --> 01:30:30,352
Okay. Well, that's it. I guess next

1565
01:30:30,406 --> 01:30:34,600
time we will do probably 910

1566
01:30:34,670 --> 01:30:38,824
and maybe something else. All right,

1567
01:30:38,942 --> 01:30:42,330
I'll end it now. Thanks, Ali. Thank you.

1568
01:30:43,500 --> 01:30:43,780
Bye.



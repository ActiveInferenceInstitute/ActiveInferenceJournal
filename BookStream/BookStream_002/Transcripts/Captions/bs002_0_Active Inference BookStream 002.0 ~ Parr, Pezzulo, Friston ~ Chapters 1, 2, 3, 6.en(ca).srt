1
00:00:03,330 --> 00:00:07,430
All right! Hello and welcome

2
00:00:07,500 --> 00:00:10,546
everyone. This is ActInf BookStream

3
00:00:10,738 --> 00:00:13,974
number 2.0. We are going to be

4
00:00:14,012 --> 00:00:16,034
discussing the textbook "Active

5
00:00:16,082 --> 00:00:19,014
Inference, the Free Energy Principle in

6
00:00:19,052 --> 00:00:21,894
Mind, Brain and Behavior, 2022.

7
00:00:21,932 --> 00:00:24,498
Textbook by Thomas Parr, Giovanni

8
00:00:24,514 --> 00:00:27,686
Pazulo, and Karl Friston. In

9
00:00:27,708 --> 00:00:30,778
this Livestream with Ali and I, we're

10
00:00:30,794 --> 00:00:34,014
going to be giving some overviews and

11
00:00:34,052 --> 00:00:35,950
some kind of points that you'll want to

12
00:00:36,020 --> 00:00:37,594
have in mind, little bit of background

13
00:00:37,642 --> 00:00:40,282
and context. Specifically on chapters

14
00:00:40,426 --> 00:00:43,586
one, two, three and six,

15
00:00:43,768 --> 00:00:45,858
we're going to do more live streams in

16
00:00:45,864 --> 00:00:47,890
the future that will cover the other

17
00:00:47,960 --> 00:00:50,478
chapters. There's ten chapters overall,

18
00:00:50,654 --> 00:00:53,862
and the goal of this live stream is to

19
00:00:53,996 --> 00:00:56,850
use the live stream format to produce

20
00:00:56,930 --> 00:01:00,614
materials that we can then clip out so

21
00:01:00,652 --> 00:01:03,142
that the textbook groups, which we have

22
00:01:03,196 --> 00:01:05,746
multiple ongoing cohorts at the

23
00:01:05,788 --> 00:01:08,438
institute so that those textbook groups

24
00:01:08,534 --> 00:01:10,854
can be really engaging and discussion

25
00:01:10,902 --> 00:01:13,766
oriented. Because after several cohorts

26
00:01:13,878 --> 00:01:16,634
of going through this, ali and I and

27
00:01:16,672 --> 00:01:18,974
others have started to kind of hone our

28
00:01:19,012 --> 00:01:21,374
understanding of the textbook. And so we

29
00:01:21,412 --> 00:01:24,574
are starting to feel ready to put down

30
00:01:24,612 --> 00:01:27,598
some background and context zero videos.

31
00:01:27,764 --> 00:01:30,094
And then ideally, people who are

32
00:01:30,132 --> 00:01:31,790
participating in the textbook group

33
00:01:31,860 --> 00:01:34,754
cohorts will be able to view and think

34
00:01:34,792 --> 00:01:37,506
about those videos leading up to a

35
00:01:37,528 --> 00:01:39,474
discussion so that when we do have the

36
00:01:39,512 --> 00:01:41,346
discussions in the textbook group, that

37
00:01:41,368 --> 00:01:43,974
those can be maximally interactive and

38
00:01:44,012 --> 00:01:46,466
engaging. So before we jump into chapter

39
00:01:46,498 --> 00:01:48,134
one, Ali, do you want to just add

40
00:01:48,172 --> 00:01:51,910
anything about the textbook or anything

41
00:01:51,980 --> 00:01:55,554
else? Hello. I'm ali. I'm very happy

42
00:01:55,612 --> 00:01:58,506
and excited to be here to discuss the

43
00:01:58,528 --> 00:02:01,594
book that we've fallen in love for the

44
00:02:01,712 --> 00:02:04,586
last year. And despite the fact that

45
00:02:04,768 --> 00:02:07,322
we've been studying and discussing this

46
00:02:07,376 --> 00:02:10,766
book for more than a year, there's still

47
00:02:10,788 --> 00:02:13,150
a lot to learn and unpack. So yeah,

48
00:02:13,220 --> 00:02:17,486
great. All right, so then

49
00:02:17,668 --> 00:02:21,506
we will begin in just a few seconds the

50
00:02:21,688 --> 00:02:23,810
chapter one and overview.

51
00:02:26,470 --> 00:02:28,802
All right, this section is going to be

52
00:02:28,856 --> 00:02:31,266
the chapter one and overview of the

53
00:02:31,288 --> 00:02:34,082
active inference textbook. So shown here

54
00:02:34,136 --> 00:02:36,662
are the table of contents. There are ten

55
00:02:36,716 --> 00:02:39,174
chapters and three appendices in this

56
00:02:39,212 --> 00:02:41,782
book, and they're separated into part

57
00:02:41,836 --> 00:02:43,462
one with chapters one through five.

58
00:02:43,516 --> 00:02:45,446
Andy Clark, two, chapters six through

59
00:02:45,468 --> 00:02:47,082
ten. Ollie, do you want to give any

60
00:02:47,136 --> 00:02:49,546
overview thoughts on the organization or

61
00:02:49,568 --> 00:02:50,780
structure of the book?

62
00:02:55,410 --> 00:02:56,160
Okay.

63
00:02:59,010 --> 00:03:01,794
Can you hear me now? Sorry. Yes. Okay,

64
00:03:01,912 --> 00:03:06,420
so part one is basically more line

65
00:03:07,350 --> 00:03:11,454
in terms of the theoretical construction

66
00:03:11,502 --> 00:03:15,394
of the active inference, and it provides

67
00:03:15,442 --> 00:03:19,170
some conceptual and theoretical tools

68
00:03:19,250 --> 00:03:22,040
to be utilized in the second part,

69
00:03:22,490 --> 00:03:24,920
because for the second part,

70
00:03:26,190 --> 00:03:29,722
we mostly deal with the practical side

71
00:03:29,776 --> 00:03:32,026
of active inference, how to actually

72
00:03:32,128 --> 00:03:35,626
implement active inference to model some

73
00:03:35,648 --> 00:03:38,954
of the systems of interest. So that's

74
00:03:39,002 --> 00:03:42,238
basically the main justification behind

75
00:03:42,324 --> 00:03:44,640
dividing up these chapters into two.

76
00:03:45,650 --> 00:03:48,430
Great. All right, we're going to pass

77
00:03:48,500 --> 00:03:50,906
over the short preface written by Karl

78
00:03:50,938 --> 00:03:52,926
Friston, but it's worth a read. It's

79
00:03:52,958 --> 00:03:55,890
only several pages long, so let's go to

80
00:03:55,960 --> 00:03:59,282
chapter one overview. So do you have any

81
00:03:59,336 --> 00:04:01,714
opening thoughts on chapter one or where

82
00:04:01,752 --> 00:04:04,440
does one even begin with such a book?

83
00:04:06,890 --> 00:04:10,102
Sure, yeah. For chapter one,

84
00:04:10,156 --> 00:04:13,798
I think probably the most

85
00:04:13,884 --> 00:04:17,174
central theme of chapter one is about

86
00:04:17,372 --> 00:04:20,226
the two different approaches to active

87
00:04:20,258 --> 00:04:23,034
inference, namely the high road and the

88
00:04:23,072 --> 00:04:26,742
low road. And I think it provides

89
00:04:26,806 --> 00:04:30,126
a really nice perspective to view kind

90
00:04:30,148 --> 00:04:32,350
of the whole picture and how the

91
00:04:32,500 --> 00:04:35,050
different components of active inference

92
00:04:35,210 --> 00:04:39,502
fit together. But also

93
00:04:39,636 --> 00:04:43,714
I highly recommend chapter 13 from

94
00:04:43,752 --> 00:04:47,726
this book, andy Clark and andy

95
00:04:47,758 --> 00:04:50,500
Clark and his critics, written by

96
00:04:51,270 --> 00:04:54,206
chapter 13, I think is entitled beyond

97
00:04:54,238 --> 00:04:56,298
the Desert Landscape, written by Karl

98
00:04:56,334 --> 00:04:59,222
Friston in which he provides more

99
00:04:59,276 --> 00:05:01,910
elaborate and a bit philosophical

100
00:05:02,330 --> 00:05:05,762
reasoning behind what exactly are

101
00:05:05,916 --> 00:05:09,546
low road and high road and what are the

102
00:05:09,568 --> 00:05:12,474
aims of each approach and the

103
00:05:12,512 --> 00:05:16,922
distinction between those two? So for

104
00:05:17,056 --> 00:05:21,566
chapter one we begin obviously with an

105
00:05:21,588 --> 00:05:24,734
introductory paragraph which sets the

106
00:05:24,772 --> 00:05:28,734
stage for all

107
00:05:28,772 --> 00:05:31,158
the components that would be discussed

108
00:05:31,194 --> 00:05:34,754
in this chapter. But a very important

109
00:05:34,952 --> 00:05:37,698
section I believe is section two here

110
00:05:37,784 --> 00:05:40,526
which is how do living organisms persist

111
00:05:40,558 --> 00:05:42,834
and act adaptively? Which is basically

112
00:05:43,032 --> 00:05:46,518
the main question active inference is

113
00:05:46,604 --> 00:05:49,862
trying to address. So if

114
00:05:49,916 --> 00:05:53,650
we want to describe

115
00:05:53,810 --> 00:05:59,030
active inference minimalistically

116
00:05:59,110 --> 00:06:01,980
with only one sentence or two,

117
00:06:02,350 --> 00:06:04,234
probably we can say something to the

118
00:06:04,272 --> 00:06:07,754
effect that it's a theory that address

119
00:06:07,872 --> 00:06:12,014
this very question here. So I

120
00:06:12,052 --> 00:06:14,606
why they've opened up this chapter with

121
00:06:14,708 --> 00:06:19,178
important question and then section

122
00:06:19,274 --> 00:06:23,262
three, which is again a continuation

123
00:06:23,406 --> 00:06:26,930
of section one but a bit more

124
00:06:27,000 --> 00:06:30,242
specifically about active inference and

125
00:06:30,296 --> 00:06:35,314
how active inference is addressing

126
00:06:35,442 --> 00:06:38,994
the question provided in the section

127
00:06:39,042 --> 00:06:42,998
two. So basically again,

128
00:06:43,084 --> 00:06:46,286
ActInf inference is a modeling framework

129
00:06:46,418 --> 00:06:49,530
for modeling the behaviors from first

130
00:06:49,600 --> 00:06:53,034
principles and what is

131
00:06:53,072 --> 00:06:56,042
meant by first principles will be

132
00:06:56,096 --> 00:06:58,626
elaborated upon in the following

133
00:06:58,678 --> 00:07:01,422
chapters, following sections and also

134
00:07:01,476 --> 00:07:03,470
obviously in the following chapters.

135
00:07:04,290 --> 00:07:07,838
But very briefly, it's a kind of

136
00:07:08,004 --> 00:07:13,010
modeling framework that tries to somehow

137
00:07:14,150 --> 00:07:17,790
use variational principles

138
00:07:17,870 --> 00:07:21,780
such as free free energy principle,

139
00:07:23,370 --> 00:07:26,178
construct modeling and mathematical

140
00:07:26,274 --> 00:07:32,306
tools that would enable us to describe

141
00:07:32,498 --> 00:07:36,098
dynamical systems of living

142
00:07:36,204 --> 00:07:39,482
and even nonliving agents in terms of

143
00:07:39,536 --> 00:07:43,180
those variational principles and how

144
00:07:43,630 --> 00:07:46,234
those agents or those systems are

145
00:07:46,272 --> 00:07:48,170
coupled with their environment.

146
00:07:48,850 --> 00:07:52,400
So that's basically the

147
00:07:52,850 --> 00:07:56,078
unificatory principle that is

148
00:07:56,244 --> 00:07:58,714
at the heart of this active inference

149
00:07:58,762 --> 00:08:02,260
approach because yes, exactly here

150
00:08:02,870 --> 00:08:06,162
it's a kind of description of this

151
00:08:06,296 --> 00:08:09,378
action perception loop that is at the

152
00:08:09,384 --> 00:08:12,690
heart of active inference or FEP

153
00:08:13,690 --> 00:08:17,222
framework. And then

154
00:08:17,276 --> 00:08:20,742
we go to chapter sorry,

155
00:08:20,796 --> 00:08:24,070
section 1.4 which is

156
00:08:24,140 --> 00:08:27,206
again a description of the structure

157
00:08:27,238 --> 00:08:30,506
of the book and why those two parts are

158
00:08:30,528 --> 00:08:34,074
divided as such. So as I

159
00:08:34,192 --> 00:08:36,620
said earlier, first part is more

160
00:08:37,390 --> 00:08:39,434
concerned with the theoretical

161
00:08:39,482 --> 00:08:41,118
underpinnings of active inference and

162
00:08:41,124 --> 00:08:43,566
the second part is more about the

163
00:08:43,588 --> 00:08:46,126
practical side of it as we see in these

164
00:08:46,148 --> 00:08:47,840
subsection headings here.

165
00:08:52,470 --> 00:08:57,410
And then a very important diagram

166
00:08:58,550 --> 00:09:01,106
which will provide very helpful as we

167
00:09:01,208 --> 00:09:04,582
whole book is provided in Figure

168
00:09:04,636 --> 00:09:09,490
1.2 which kind of ties

169
00:09:09,570 --> 00:09:12,850
all the different strands of various

170
00:09:13,010 --> 00:09:15,234
related theories such as predictive

171
00:09:15,282 --> 00:09:17,446
coding, predictive processing, Bayesian

172
00:09:17,478 --> 00:09:21,306
brain hypothesis and so on into a

173
00:09:21,328 --> 00:09:25,046
kind of unified and integrated framework

174
00:09:25,238 --> 00:09:29,200
by using those two trajectories of

175
00:09:29,890 --> 00:09:32,234
constructing active inference, namely

176
00:09:32,362 --> 00:09:36,174
the high road and the low road. So it

177
00:09:36,292 --> 00:09:39,454
literally here is represented as the

178
00:09:39,492 --> 00:09:42,114
high road and the low road in this

179
00:09:42,152 --> 00:09:44,850
diagram. So,

180
00:09:45,000 --> 00:09:49,090
very briefly, high road is

181
00:09:49,160 --> 00:09:51,806
a kind of top down approach to active

182
00:09:51,838 --> 00:09:55,560
inference. It begins with the question

183
00:09:57,690 --> 00:10:01,430
how things should act if they exist,

184
00:10:02,570 --> 00:10:04,694
if they persist through time. Sorry. In

185
00:10:04,732 --> 00:10:09,366
other words, it kind of sets

186
00:10:09,398 --> 00:10:12,586
the stage for developing the concept of

187
00:10:12,608 --> 00:10:16,940
markup blankets. And then obviously

188
00:10:17,870 --> 00:10:21,440
by setting that also

189
00:10:22,370 --> 00:10:24,270
surprise minimization,

190
00:10:25,650 --> 00:10:30,718
it attempts to elaborate that road

191
00:10:30,804 --> 00:10:34,878
by adding other aspects of modeling

192
00:10:34,974 --> 00:10:37,460
other aspects of theory such as

193
00:10:39,510 --> 00:10:41,630
predictive processing, self evidencing,

194
00:10:41,710 --> 00:10:45,174
autopoiesis and so on and then reach an

195
00:10:45,212 --> 00:10:48,258
integrated version of active inference

196
00:10:48,434 --> 00:10:51,330
which is basically an integration

197
00:10:51,490 --> 00:10:55,906
between perception and action. So I

198
00:10:55,948 --> 00:10:59,126
believe the main thing that sets active

199
00:10:59,158 --> 00:11:01,210
inference apart from all the other

200
00:11:01,280 --> 00:11:03,734
related theories such as predictive

201
00:11:03,782 --> 00:11:07,274
processing and predictive coding is the

202
00:11:07,312 --> 00:11:11,150
very deep integration of action and

203
00:11:11,220 --> 00:11:15,742
perception. So action is not just some

204
00:11:15,876 --> 00:11:18,570
afterthought or some additional

205
00:11:18,650 --> 00:11:21,046
component that needs to be accounted

206
00:11:21,098 --> 00:11:25,234
for. It is deeply woven into the

207
00:11:25,272 --> 00:11:29,246
fabric of the theory. So, as we'll

208
00:11:29,278 --> 00:11:31,954
see especially in chapters two and

209
00:11:31,992 --> 00:11:35,922
three, even the mathematical formalism

210
00:11:35,986 --> 00:11:39,494
for perception and action are

211
00:11:39,692 --> 00:11:42,054
interestingly similar to each other and

212
00:11:42,092 --> 00:11:45,320
even symmetric. And it shows that

213
00:11:46,270 --> 00:11:48,940
perception and ActInf are actually not

214
00:11:49,310 --> 00:11:52,822
separate and distinct, totally distinct

215
00:11:52,966 --> 00:11:56,486
entities and concepts. And active

216
00:11:56,518 --> 00:11:59,886
inference definitely provides a

217
00:11:59,908 --> 00:12:04,074
very promising framework to integrate

218
00:12:04,122 --> 00:12:06,510
them into a holistic framework.

219
00:12:06,850 --> 00:12:10,678
And another approach to construct

220
00:12:10,874 --> 00:12:15,298
that same theory is to begin with

221
00:12:15,464 --> 00:12:19,714
the probabilistic description of

222
00:12:19,752 --> 00:12:23,406
Bayes theorem and try

223
00:12:23,448 --> 00:12:26,754
to build up the formalism of active

224
00:12:26,802 --> 00:12:32,626
inference using theorem

225
00:12:32,658 --> 00:12:35,794
and Bayesian statistical inference.

226
00:12:35,842 --> 00:12:39,800
So again, we here see some

227
00:12:40,330 --> 00:12:44,230
very apparently

228
00:12:44,390 --> 00:12:47,034
different and distinct strands such as

229
00:12:47,072 --> 00:12:48,906
predictive coding, Bayesian brain

230
00:12:48,938 --> 00:12:52,174
hypothesis and so on. But as we see,

231
00:12:52,292 --> 00:12:54,382
all of those different strands can be

232
00:12:54,436 --> 00:12:58,414
united and tied together within this

233
00:12:58,532 --> 00:13:01,070
low road approach to ActInf inference.

234
00:13:05,650 --> 00:13:07,920
I'll just give one short thought.

235
00:13:10,470 --> 00:13:12,018
Yeah, just one short thought. Thank you

236
00:13:12,024 --> 00:13:15,122
Ollie, for these great summaries. The

237
00:13:15,256 --> 00:13:16,838
low road and the high road are going to

238
00:13:16,844 --> 00:13:18,966
be explored in a lot more detail in the

239
00:13:18,988 --> 00:13:22,406
coming chapters two and three. We can

240
00:13:22,428 --> 00:13:25,640
think of the high road as being a why

241
00:13:26,570 --> 00:13:29,242
because it describes how persistent or

242
00:13:29,296 --> 00:13:31,914
remeasurable systems exist. Now, they

243
00:13:31,952 --> 00:13:33,626
might be using base theorem or they

244
00:13:33,648 --> 00:13:35,114
might be using any other kind of

245
00:13:35,152 --> 00:13:37,674
mechanism internally. But the high road

246
00:13:37,712 --> 00:13:41,166
is describing the why of existence from

247
00:13:41,188 --> 00:13:43,454
the low road. The bottom up approach is

248
00:13:43,492 --> 00:13:46,382
kind of like a how you could use base

249
00:13:46,436 --> 00:13:48,874
theorem to describe persistent

250
00:13:48,922 --> 00:13:51,774
autopoietic entities, or you could use

251
00:13:51,812 --> 00:13:54,026
base theorem for other purposes too. So

252
00:13:54,068 --> 00:13:56,082
where these two roads intersect is

253
00:13:56,136 --> 00:13:58,130
active inference, and we have a lot more

254
00:13:58,200 --> 00:14:00,366
discussion coming and of course unpacked

255
00:14:00,398 --> 00:14:02,482
in the textbook groups about how these

256
00:14:02,536 --> 00:14:04,982
really map on to bottom up and top down

257
00:14:05,036 --> 00:14:08,358
causation and the plurality of whys that

258
00:14:08,364 --> 00:14:10,646
we approach in different settings. Good

259
00:14:10,668 --> 00:14:11,560
to continue?

260
00:14:14,700 --> 00:14:18,460
Yes, sure. Some of the

261
00:14:18,530 --> 00:14:21,112
coming sections of this chapter,

262
00:14:21,176 --> 00:14:23,804
continuing from page eight, are going to

263
00:14:23,842 --> 00:14:26,476
describe what are happening in the next

264
00:14:26,578 --> 00:14:29,688
chapters. So chapter two is going to set

265
00:14:29,714 --> 00:14:32,844
out the low road perspective. Chapter

266
00:14:32,892 --> 00:14:35,024
three is going to describe the high

267
00:14:35,062 --> 00:14:38,284
road. Chapter four we will unpack

268
00:14:38,332 --> 00:14:40,716
active inference more formally. That's

269
00:14:40,748 --> 00:14:43,744
one of the math heavy chapters. Chapter

270
00:14:43,792 --> 00:14:46,356
five will move from formal treatments to

271
00:14:46,378 --> 00:14:48,208
biological implications of active

272
00:14:48,224 --> 00:14:50,692
inference with a special focus on

273
00:14:50,746 --> 00:14:52,900
mammalian neurophysiology.

274
00:14:54,200 --> 00:14:56,224
Chapter five sets out the process theory

275
00:14:56,272 --> 00:14:57,896
associated with active inference and

276
00:14:57,918 --> 00:15:00,100
gives a lot of hands on and empirical

277
00:15:00,180 --> 00:15:02,936
examples. And that's a lot of the first

278
00:15:03,038 --> 00:15:05,704
part of the book. They're then going to

279
00:15:05,742 --> 00:15:09,260
summarize some of the key points and

280
00:15:09,330 --> 00:15:11,752
distillations from that theory heavy

281
00:15:11,816 --> 00:15:14,364
first section. One of them is that, as

282
00:15:14,402 --> 00:15:16,364
Ali mentioned, perception and action are

283
00:15:16,402 --> 00:15:19,004
complementary processes and ways that

284
00:15:19,042 --> 00:15:21,600
fulfill the same imperative free energy

285
00:15:21,670 --> 00:15:23,804
minimization. We sometimes summarize

286
00:15:23,852 --> 00:15:26,016
that by saying that in the pursuit of

287
00:15:26,038 --> 00:15:28,608
the minimization of free energy, agents

288
00:15:28,694 --> 00:15:30,912
can either change their mind or change

289
00:15:30,966 --> 00:15:32,996
the world. Change your mind is

290
00:15:33,018 --> 00:15:35,270
associated with perception and learning

291
00:15:35,640 --> 00:15:38,884
and those are distinguished later and

292
00:15:38,922 --> 00:15:40,804
changing the world is associated with

293
00:15:40,842 --> 00:15:44,672
action. There's some discussion about

294
00:15:44,746 --> 00:15:47,208
action selection and optimal policy

295
00:15:47,294 --> 00:15:51,524
selection and all cognitive operations

296
00:15:51,652 --> 00:15:53,892
in active inference are conceptualized

297
00:15:53,956 --> 00:15:57,144
as inference over generative models. So

298
00:15:57,182 --> 00:15:59,964
the true kernel, the cognitive kernel of

299
00:16:00,002 --> 00:16:02,316
active inference is going to be the

300
00:16:02,338 --> 00:16:05,676
generative model. Want to pick up from

301
00:16:05,698 --> 00:16:07,464
here or give any thoughts on generative

302
00:16:07,512 --> 00:16:09,580
models and then carry on? Thanks Ollie.

303
00:16:12,900 --> 00:16:15,936
Just one point is there is a

304
00:16:15,958 --> 00:16:18,252
common misconception about generative

305
00:16:18,316 --> 00:16:21,276
models, which is that generative models

306
00:16:21,388 --> 00:16:24,596
only refers to the inner states of the

307
00:16:24,618 --> 00:16:28,084
system and also its markup blanket. But

308
00:16:28,122 --> 00:16:30,276
actually generative model kind of

309
00:16:30,298 --> 00:16:35,064
encompass all the states of interest of

310
00:16:35,102 --> 00:16:37,640
the situation we're trying to model. So

311
00:16:37,790 --> 00:16:41,844
it also encodes the probabilistic

312
00:16:41,892 --> 00:16:44,216
information about the external states as

313
00:16:44,238 --> 00:16:46,584
well. So generative model can be thought

314
00:16:46,622 --> 00:16:50,156
of as kind of the

315
00:16:50,178 --> 00:16:52,300
whole state of the system and by system

316
00:16:52,370 --> 00:16:59,748
here I dean the coupled

317
00:16:59,784 --> 00:17:01,808
dynamics between the internal and

318
00:17:01,894 --> 00:17:05,680
external states. So that might

319
00:17:05,830 --> 00:17:07,890
be helpful to keep in mind.

320
00:17:09,620 --> 00:17:11,872
Great. One other distinction that comes

321
00:17:11,926 --> 00:17:14,944
up and is used a lot nowadays, although

322
00:17:14,992 --> 00:17:16,576
it's not always used in prior

323
00:17:16,608 --> 00:17:18,164
literature, is the distinction between

324
00:17:18,202 --> 00:17:20,352
the generative model and the generative

325
00:17:20,416 --> 00:17:22,596
process. We're going to come back to it,

326
00:17:22,618 --> 00:17:26,424
but usually the generative model is

327
00:17:26,462 --> 00:17:29,044
being used to describe our statistical

328
00:17:29,092 --> 00:17:32,536
model of the agent of interest. And the

329
00:17:32,558 --> 00:17:35,752
generative process is the process that

330
00:17:35,806 --> 00:17:37,804
generates observations that are then

331
00:17:37,842 --> 00:17:39,900
passed to the generative model as

332
00:17:39,970 --> 00:17:42,108
observations. However, because

333
00:17:42,194 --> 00:17:43,944
generative models can be generative

334
00:17:43,992 --> 00:17:46,476
processes for each other, these two are

335
00:17:46,498 --> 00:17:49,192
not necessarily distinct in principle.

336
00:17:49,336 --> 00:17:51,776
So in different simulations and in

337
00:17:51,798 --> 00:17:53,804
different specific cases, generative

338
00:17:53,852 --> 00:17:56,204
models can also be generative processes

339
00:17:56,252 --> 00:17:58,208
for each other. That leads us towards

340
00:17:58,294 --> 00:18:00,524
talking about ecosystems of shared

341
00:18:00,572 --> 00:18:02,624
intelligence and interactions amongst

342
00:18:02,672 --> 00:18:04,336
generative models. But when we're

343
00:18:04,368 --> 00:18:06,256
talking about generative model, it's

344
00:18:06,288 --> 00:18:08,356
like the organism or the system of

345
00:18:08,378 --> 00:18:10,230
interest that we're focused on.

346
00:18:12,840 --> 00:18:16,708
There are some more themes that arise

347
00:18:16,804 --> 00:18:19,016
from the first half of the book. Now

348
00:18:19,038 --> 00:18:21,384
we're on page eleven action is

349
00:18:21,422 --> 00:18:23,624
quintessentially, goal directed and

350
00:18:23,662 --> 00:18:25,832
purpose of we talk a lot about how

351
00:18:25,886 --> 00:18:28,324
preference plays a role in shaping

352
00:18:28,372 --> 00:18:30,520
action prediction in active inference.

353
00:18:30,600 --> 00:18:32,568
And the goal directed nature of active

354
00:18:32,584 --> 00:18:34,456
inference will be unpacked. In chapters

355
00:18:34,488 --> 00:18:37,512
two and three, they discuss how various

356
00:18:37,576 --> 00:18:39,196
constructs of active inference have

357
00:18:39,218 --> 00:18:41,516
plausible biological analogues in the

358
00:18:41,538 --> 00:18:44,688
brain. Once one has defined a specific

359
00:18:44,774 --> 00:18:47,120
generative model for a problem at hand,

360
00:18:47,270 --> 00:18:49,328
one can move from active inference as a

361
00:18:49,334 --> 00:18:51,456
normative theory to active inference as

362
00:18:51,478 --> 00:18:53,556
a process theory which makes specific

363
00:18:53,658 --> 00:18:56,164
empirical predictions. And there's many,

364
00:18:56,202 --> 00:18:58,544
many interesting philosophy of science

365
00:18:58,592 --> 00:19:00,676
type discussions that we have about

366
00:19:00,858 --> 00:19:03,924
explain predict, design control. And

367
00:19:03,962 --> 00:19:07,588
then we get to section 1.4.2,

368
00:19:07,754 --> 00:19:10,188
giving overview on part two, active

369
00:19:10,224 --> 00:19:13,160
inference in practice. So I'll just

370
00:19:13,230 --> 00:19:16,296
continue on. Chapter six is going to

371
00:19:16,318 --> 00:19:18,560
introduce a recipe for building active

372
00:19:18,580 --> 00:19:20,764
inference ModelStream. So this is where

373
00:19:20,802 --> 00:19:24,456
the step by step comes into play. It's

374
00:19:24,488 --> 00:19:27,276
going to give a sequence of steps and

375
00:19:27,298 --> 00:19:30,030
approaches that you can take to go from

376
00:19:30,400 --> 00:19:33,136
understanding the structural aspects of

377
00:19:33,158 --> 00:19:35,872
a system or phenomena of interest and

378
00:19:35,926 --> 00:19:37,916
build your way towards having an active

379
00:19:37,948 --> 00:19:40,464
inference model that can then be

380
00:19:40,502 --> 00:19:42,492
implemented in code. And we have various

381
00:19:42,556 --> 00:19:44,444
examples and we're there in the textbook

382
00:19:44,492 --> 00:19:46,548
group to work with everyone who wants to

383
00:19:46,554 --> 00:19:48,724
take this journey. But chapter six is

384
00:19:48,762 --> 00:19:50,964
going to describe that process of

385
00:19:51,002 --> 00:19:52,992
building the active inference generative

386
00:19:53,056 --> 00:19:56,208
model. So that's a recipe chapter and

387
00:19:56,234 --> 00:19:57,576
there's a lot more that goes into the

388
00:19:57,598 --> 00:19:59,800
restaurant also as we kind of have fun

389
00:19:59,870 --> 00:20:03,304
exploring. Chapters seven and eight are

390
00:20:03,342 --> 00:20:06,472
like a pair and they describe two

391
00:20:06,606 --> 00:20:10,232
different broad families of generative

392
00:20:10,296 --> 00:20:12,316
models. Now these two families of

393
00:20:12,338 --> 00:20:14,444
generative models, they work well, they

394
00:20:14,482 --> 00:20:16,972
play well with each other, but they are

395
00:20:17,026 --> 00:20:19,992
very educationally relevant to also

396
00:20:20,066 --> 00:20:22,416
consider separately. Chapter seven is

397
00:20:22,438 --> 00:20:24,576
going to focus on the discrete time

398
00:20:24,758 --> 00:20:27,456
generative model and chapter eight is

399
00:20:27,478 --> 00:20:29,296
going to focus on the continuous time

400
00:20:29,398 --> 00:20:32,380
generative models. And again, these

401
00:20:32,550 --> 00:20:34,916
discrete and continuous time models can

402
00:20:34,938 --> 00:20:37,748
be nested and interoperable. However,

403
00:20:37,834 --> 00:20:40,550
they do describe some very important

404
00:20:40,920 --> 00:20:43,376
different situations. And so chapter

405
00:20:43,408 --> 00:20:45,332
seven and eight have a lot of technical

406
00:20:45,396 --> 00:20:46,904
information on the discrete and

407
00:20:46,942 --> 00:20:49,108
continuous time formulations of active

408
00:20:49,124 --> 00:20:50,424
inference and also about how they

409
00:20:50,462 --> 00:20:53,944
integrate together. Chapter nine is

410
00:20:54,062 --> 00:20:56,404
illustrations of how active inference

411
00:20:56,452 --> 00:20:58,652
models can be used to analyze data from

412
00:20:58,706 --> 00:21:01,356
behavioral experiments. And so this kind

413
00:21:01,378 --> 00:21:04,076
of goes beyond chapter six's recipe and

414
00:21:04,098 --> 00:21:06,556
it talks about some of the process of

415
00:21:06,658 --> 00:21:09,180
making a behavioral experiment and then

416
00:21:09,250 --> 00:21:11,484
using the outcomes of that behavioral

417
00:21:11,532 --> 00:21:12,944
experiment, the data from that

418
00:21:12,982 --> 00:21:14,944
experiment and using that to

419
00:21:14,982 --> 00:21:17,616
parameterize and do statistics with the

420
00:21:17,638 --> 00:21:19,904
active inference generative model, and

421
00:21:19,942 --> 00:21:22,432
chapter ten brings it all together.

422
00:21:22,566 --> 00:21:24,724
Chapter ten is a lot like chapter one,

423
00:21:24,842 --> 00:21:27,332
and we've even joked of reading the book

424
00:21:27,386 --> 00:21:29,700
backwards. That's how great chapter ten

425
00:21:29,770 --> 00:21:32,176
is. So for those who are reading chapter

426
00:21:32,208 --> 00:21:35,300
one, consider reading chapter ten next

427
00:21:35,450 --> 00:21:37,716
and then picking back up with chapter

428
00:21:37,748 --> 00:21:40,072
two because chapter ten is going to give

429
00:21:40,126 --> 00:21:43,080
some judgments and also sketch some

430
00:21:43,150 --> 00:21:45,432
exciting future directions for where

431
00:21:45,486 --> 00:21:47,436
active inference is in relationship to

432
00:21:47,458 --> 00:21:51,004
other fields and where it's going. I

433
00:21:51,042 --> 00:21:53,310
also second that. Yeah,

434
00:21:54,480 --> 00:21:56,136
the second part of the book illustrates

435
00:21:56,168 --> 00:21:59,224
a broad variety of models of biological

436
00:21:59,272 --> 00:22:01,404
and cognitive phenomena. So it's an

437
00:22:01,442 --> 00:22:03,436
application oriented second half of the

438
00:22:03,458 --> 00:22:06,060
book, and we have a lot of accessory

439
00:22:06,140 --> 00:22:08,384
code and approaches and notebooks, so 1

440
00:22:08,422 --> 00:22:10,160
may not find that this is the one stop

441
00:22:10,230 --> 00:22:11,744
shop that they were looking for.

442
00:22:11,862 --> 00:22:14,064
However, for those who dive in, there's

443
00:22:14,112 --> 00:22:16,336
absolutely more than enough to scaffold

444
00:22:16,368 --> 00:22:17,620
their learning journey.

445
00:22:19,320 --> 00:22:21,904
And then section 1.5 is the summary.

446
00:22:22,032 --> 00:22:23,776
The chapter introduces the active

447
00:22:23,808 --> 00:22:26,016
inference approach to explain biological

448
00:22:26,128 --> 00:22:28,416
problems from a normative perspective

449
00:22:28,528 --> 00:22:30,376
and previews some implications of this

450
00:22:30,398 --> 00:22:32,216
perspective that will be unpacked in

451
00:22:32,238 --> 00:22:34,984
later chapters. It describes the

452
00:22:35,022 --> 00:22:36,760
structural outline of the book,

453
00:22:36,910 --> 00:22:38,772
chapters one through five being theory,

454
00:22:38,836 --> 00:22:40,780
chapters six through ten being practice.

455
00:22:41,120 --> 00:22:43,436
And then it signals that the next two

456
00:22:43,458 --> 00:22:45,292
chapters are going to develop the low

457
00:22:45,346 --> 00:22:47,980
and the high road perspectives. And

458
00:22:48,050 --> 00:22:51,856
altogether, that is chapter one. Any

459
00:22:51,878 --> 00:22:53,490
other thoughts on chapter one?

460
00:22:56,820 --> 00:22:58,976
Not as specifically, but I think it

461
00:22:58,998 --> 00:23:01,936
provides a really good context to the

462
00:23:01,958 --> 00:23:05,924
rest of the book in terms of how all

463
00:23:05,962 --> 00:23:07,968
the different elements of active

464
00:23:07,984 --> 00:23:10,340
inference comes into play in addressing

465
00:23:10,680 --> 00:23:13,990
different problems. Different problems.

466
00:23:14,780 --> 00:23:15,530
Awesome.

467
00:23:19,420 --> 00:23:22,344
On to chapter two, the low road to

468
00:23:22,382 --> 00:23:25,736
active inference. So all chapters begin

469
00:23:25,838 --> 00:23:28,056
with a short quotation, and the

470
00:23:28,078 --> 00:23:29,884
quotation here reads, my thinking is

471
00:23:29,922 --> 00:23:32,088
first and last and always for the sake

472
00:23:32,104 --> 00:23:34,652
of my doing. William James so even

473
00:23:34,706 --> 00:23:36,204
before reading the chapter, these

474
00:23:36,242 --> 00:23:39,052
quotations are often great places to

475
00:23:39,106 --> 00:23:41,136
jump off and have a discussion. So

476
00:23:41,158 --> 00:23:43,056
Ollie, please begin and go as long as

477
00:23:43,078 --> 00:23:44,956
you want on the low road to active

478
00:23:44,988 --> 00:23:48,530
inference. Okay,

479
00:23:50,900 --> 00:23:54,692
I guess chapter two,

480
00:23:54,826 --> 00:23:57,284
as the title suggests, it's the

481
00:23:57,322 --> 00:23:59,408
construction of active inference theory

482
00:23:59,584 --> 00:24:02,950
from the viewpoint of the low road

483
00:24:03,400 --> 00:24:07,210
active inference. So it begins by

484
00:24:07,820 --> 00:24:10,232
providing this notion of perception as

485
00:24:10,286 --> 00:24:13,384
inference, because more often than not,

486
00:24:13,502 --> 00:24:16,392
we usually think of perception as

487
00:24:16,446 --> 00:24:20,040
something just in a computational

488
00:24:20,120 --> 00:24:23,688
sense of sense is processing

489
00:24:23,864 --> 00:24:26,984
of the inputs, or rather it's a raw

490
00:24:27,032 --> 00:24:29,856
processing of the inputs. But here it

491
00:24:29,878 --> 00:24:33,840
shows that it can also be described as,

492
00:24:33,910 --> 00:24:37,136
or even more accurately described as a

493
00:24:37,158 --> 00:24:40,784
kind of inference. So it's not just a

494
00:24:40,822 --> 00:24:44,464
simple and raw processing of the inputs,

495
00:24:44,512 --> 00:24:49,524
as my computational model or

496
00:24:49,562 --> 00:24:52,384
a computer analogy might suggest. It's

497
00:24:52,432 --> 00:24:56,184
more like something that

498
00:24:56,302 --> 00:25:00,792
we predict and then we

499
00:25:00,846 --> 00:25:04,776
compare our inputs or the inputs we

500
00:25:04,798 --> 00:25:08,510
get from the stimuli with our

501
00:25:08,960 --> 00:25:12,328
prediction and then try to somehow

502
00:25:12,504 --> 00:25:15,240
minimize that prediction error.

503
00:25:15,320 --> 00:25:19,416
So I believe that this notion

504
00:25:19,448 --> 00:25:23,104
of perception as inference is the

505
00:25:23,142 --> 00:25:26,672
most central notion of all

506
00:25:26,726 --> 00:25:29,596
the related theories of predictive

507
00:25:29,628 --> 00:25:31,436
coding, predictive processing, Bayesian

508
00:25:31,468 --> 00:25:35,140
brain hypothesis. So up to now,

509
00:25:35,290 --> 00:25:39,236
active inference is not very different

510
00:25:39,338 --> 00:25:41,380
from all the other theories.

511
00:25:42,040 --> 00:25:45,876
It's basically a subset or a variant

512
00:25:45,908 --> 00:25:48,568
of those theories, but then it

513
00:25:48,654 --> 00:25:53,956
progresses into distinguishing

514
00:25:53,988 --> 00:25:56,072
the active inference from all the other

515
00:25:56,126 --> 00:25:59,836
theories and how it stands apart from

516
00:26:00,018 --> 00:26:05,500
the other ones. So in

517
00:26:05,570 --> 00:26:09,652
section 2.1, we began

518
00:26:09,736 --> 00:26:12,396
with some of the basics of probability

519
00:26:12,508 --> 00:26:16,080
theory, namely the base theorem in box

520
00:26:16,150 --> 00:26:20,880
2.1 and some simple examples

521
00:26:21,300 --> 00:26:24,580
about how base theorem can be applied.

522
00:26:26,280 --> 00:26:29,910
And again, a very important

523
00:26:31,400 --> 00:26:34,684
page or important part of this section

524
00:26:34,832 --> 00:26:39,508
will be page 20, which describes

525
00:26:39,684 --> 00:26:41,832
the concept of surprisal and

526
00:26:41,886 --> 00:26:45,172
specifically the statistical surprisal

527
00:26:45,236 --> 00:26:47,564
and how it differs from the

528
00:26:47,602 --> 00:26:49,036
phenomenological surprise or

529
00:26:49,058 --> 00:26:50,540
psychological surprise.

530
00:26:52,880 --> 00:26:56,520
How we can formulate the statistical

531
00:26:56,600 --> 00:26:59,580
surprise. Yes, just to catch up here

532
00:26:59,650 --> 00:27:01,164
this example that's going to come back

533
00:27:01,202 --> 00:27:03,144
again and again is a person who's

534
00:27:03,192 --> 00:27:05,520
guessing the object that's in their hand

535
00:27:05,670 --> 00:27:07,856
and it could be a frog or an apple and

536
00:27:07,878 --> 00:27:09,696
the object is going to jump or not. And

537
00:27:09,718 --> 00:27:12,172
so that's used as a way to talk about

538
00:27:12,326 --> 00:27:15,236
the Bayesian updating that forms the

539
00:27:15,258 --> 00:27:17,856
kernel of a variety of Bayesian brain

540
00:27:17,888 --> 00:27:21,008
like models, including ActInf inference.

541
00:27:21,184 --> 00:27:24,228
And then there are the presentation of

542
00:27:24,234 --> 00:27:26,964
the exact Bayes. So in simple cases,

543
00:27:27,092 --> 00:27:29,096
you can compute exactly what you want.

544
00:27:29,118 --> 00:27:32,280
With Bayes theorem, however, it's a lot

545
00:27:32,350 --> 00:27:35,672
more effective on large data sets to use

546
00:27:35,726 --> 00:27:37,704
certain approximations and heuristics

547
00:27:37,752 --> 00:27:39,804
that we're going to be discussing. And

548
00:27:39,842 --> 00:27:43,548
then Ali has highlighted that

549
00:27:43,634 --> 00:27:46,076
there are two concepts that are very

550
00:27:46,098 --> 00:27:48,072
closely related to each other regarding

551
00:27:48,136 --> 00:27:52,016
surprise. There's surprise by itself and

552
00:27:52,038 --> 00:27:54,112
then there's Bayesian surprise. So

553
00:27:54,166 --> 00:27:56,124
please pick up there. What are surprise

554
00:27:56,172 --> 00:27:57,984
and Bayesian surprise and why do they

555
00:27:58,022 --> 00:27:58,930
matter here?

556
00:28:02,920 --> 00:28:06,036
Yes, well, surprise,

557
00:28:06,148 --> 00:28:08,456
I mean the regular surprise as we're all

558
00:28:08,478 --> 00:28:12,232
familiar with is something more

559
00:28:12,286 --> 00:28:14,984
related to our sense of surprise or

560
00:28:15,022 --> 00:28:17,710
psychological sense of surprise of

561
00:28:18,880 --> 00:28:22,044
observing some unexpected phenomena or

562
00:28:22,082 --> 00:28:25,112
unexpected behavior. But Bayesian

563
00:28:25,176 --> 00:28:28,430
surprise or statistical surprise is

564
00:28:28,980 --> 00:28:32,384
of course closely related with the

565
00:28:32,422 --> 00:28:34,736
psychological sense of surprise, but a

566
00:28:34,758 --> 00:28:38,370
bit more rigorous in which it's a way to

567
00:28:38,900 --> 00:28:43,170
compare two probabilistic information

568
00:28:43,540 --> 00:28:48,150
using callback libel or divergence and

569
00:28:49,400 --> 00:28:53,716
somehow getting the I

570
00:28:53,738 --> 00:28:57,450
mean how unexpected that information

571
00:28:58,060 --> 00:29:02,650
emerging from callback divergence and

572
00:29:03,020 --> 00:29:06,520
they call it the surprisal. So surprisal

573
00:29:07,260 --> 00:29:10,548
in other words, is a way to formulating

574
00:29:10,644 --> 00:29:13,540
those unexpected probabilistic

575
00:29:13,620 --> 00:29:17,000
information and it doesn't necessarily

576
00:29:18,300 --> 00:29:21,660
align war maps unto perfectly

577
00:29:21,740 --> 00:29:24,652
our psychological sense of surprisal.

578
00:29:24,716 --> 00:29:27,264
But as I said, it's closely related with

579
00:29:27,302 --> 00:29:30,748
that. Yes, by surprise,

580
00:29:30,924 --> 00:29:31,650
please.

581
00:29:34,260 --> 00:29:36,036
No, you take it, Allie. I know, I was

582
00:29:36,058 --> 00:29:39,556
just going to say that. I was just going

583
00:29:39,578 --> 00:29:43,476
to say that by surprise, almost in the

584
00:29:43,498 --> 00:29:46,804
rest all the other sections and chapters

585
00:29:46,852 --> 00:29:50,248
of the book, we basically mean this

586
00:29:50,334 --> 00:29:53,992
Bayesian surprise sense of the world.

587
00:29:54,126 --> 00:29:57,790
And in order to distinguish that with

588
00:29:58,160 --> 00:30:01,272
the regular sense of surprise,

589
00:30:01,416 --> 00:30:03,964
sometimes in the literature surprisal is

590
00:30:04,002 --> 00:30:06,380
used to refer to Bayesian Surprise.

591
00:30:07,120 --> 00:30:09,224
Yes. So they're both measured in

592
00:30:09,282 --> 00:30:11,424
information theoretic units. This is all

593
00:30:11,462 --> 00:30:13,564
happening in information geometric

594
00:30:13,612 --> 00:30:16,684
spaces. The first concepts of surprise

595
00:30:16,812 --> 00:30:19,776
is applied to a given single

596
00:30:19,878 --> 00:30:23,072
observations. How surprising is that one

597
00:30:23,126 --> 00:30:25,108
observation? And so in that sense it's a

598
00:30:25,114 --> 00:30:28,484
lot like the Z score of a data point

599
00:30:28,522 --> 00:30:30,592
coming in with respect to a statistical

600
00:30:30,656 --> 00:30:33,108
distribution. So it's like you have a

601
00:30:33,114 --> 00:30:35,156
height distribution in a classroom and

602
00:30:35,178 --> 00:30:37,476
you measure one person and then you can

603
00:30:37,498 --> 00:30:39,352
say what is the Z score of that

604
00:30:39,406 --> 00:30:42,072
measurement? Was it right at the center

605
00:30:42,126 --> 00:30:43,816
of the distribution with a z score of

606
00:30:43,838 --> 00:30:45,560
zero or were they two standard

607
00:30:45,630 --> 00:30:47,672
deviations higher with a zscore of two?

608
00:30:47,726 --> 00:30:49,448
So it's kind of like that. And that's

609
00:30:49,464 --> 00:30:51,128
why there's a discussion of probability

610
00:30:51,224 --> 00:30:53,916
distributions and their support, which

611
00:30:53,938 --> 00:30:55,896
is the x values for which they're

612
00:30:55,928 --> 00:30:59,068
defined. And surprise function with the

613
00:30:59,074 --> 00:31:01,904
fancy I. And that's going to be a

614
00:31:01,942 --> 00:31:03,792
function that helps you compute how

615
00:31:03,846 --> 00:31:06,608
surprising each observation is given a

616
00:31:06,614 --> 00:31:08,880
parameterization of that distribution.

617
00:31:09,220 --> 00:31:12,660
Whereas this Bayesian Surprise is more

618
00:31:12,730 --> 00:31:16,020
related to learning. It has to do with

619
00:31:16,170 --> 00:31:19,284
how much updating happens between the

620
00:31:19,322 --> 00:31:21,936
prior and the posterior and that's

621
00:31:21,968 --> 00:31:24,744
before and after the observation. So one

622
00:31:24,782 --> 00:31:27,140
could imagine a surprising observation

623
00:31:27,300 --> 00:31:30,392
surprise concept, one that either does

624
00:31:30,446 --> 00:31:33,656
or doesn't update the prior into a very

625
00:31:33,678 --> 00:31:36,276
different posterior. So as Ollie

626
00:31:36,308 --> 00:31:37,576
mentioned, they are not exactly the

627
00:31:37,598 --> 00:31:39,276
same, but it's going to be important to

628
00:31:39,298 --> 00:31:41,196
understand how they're different. And

629
00:31:41,218 --> 00:31:43,196
that's worked out again in the case of

630
00:31:43,218 --> 00:31:46,940
the apple jumping box 2.2

631
00:31:47,090 --> 00:31:49,080
continues with a discussion of

632
00:31:49,170 --> 00:31:52,144
expectations. Now, expectation in

633
00:31:52,182 --> 00:31:54,844
everyday parlance might be specifically

634
00:31:54,892 --> 00:31:56,320
referring to something in the future

635
00:31:56,390 --> 00:31:59,920
like I expect it to rain tomorrow. And

636
00:32:00,070 --> 00:32:02,324
in statistics, when we talk about the

637
00:32:02,362 --> 00:32:04,804
expectation, we're talking about the

638
00:32:04,842 --> 00:32:07,748
weighted average or the center of

639
00:32:07,754 --> 00:32:10,244
gravity of a distribution. And that can

640
00:32:10,282 --> 00:32:13,236
be in both a discrete distribution, at

641
00:32:13,258 --> 00:32:15,540
which point you have a weighted sum, or

642
00:32:15,610 --> 00:32:17,576
a continuous distribution, in which case

643
00:32:17,598 --> 00:32:19,624
it's an integral. So you can have an

644
00:32:19,662 --> 00:32:21,972
expectation for the humidity tomorrow.

645
00:32:22,116 --> 00:32:23,976
And so that might refer to the center of

646
00:32:23,998 --> 00:32:27,900
gravity at a t equals plus one.

647
00:32:28,050 --> 00:32:30,072
But just taken alone,

648
00:32:30,216 --> 00:32:33,164
expectation means center of gravity of a

649
00:32:33,202 --> 00:32:35,436
statistical distribution, not

650
00:32:35,538 --> 00:32:39,432
anticipation. Section 2.3

651
00:32:39,586 --> 00:32:42,368
is going to describe how some of this,

652
00:32:42,454 --> 00:32:46,064
how this low road that we're on is going

653
00:32:46,102 --> 00:32:48,720
to connect to biological inference.

654
00:32:51,220 --> 00:32:53,988
There's more discussion of the

655
00:32:53,994 --> 00:32:55,472
generative model and the generative

656
00:32:55,536 --> 00:32:59,044
process and a little bit of a hint that

657
00:32:59,082 --> 00:33:01,924
the generative model captures aspects of

658
00:33:01,962 --> 00:33:03,636
the generative process, which is where

659
00:33:03,658 --> 00:33:06,580
we see a lot of the classical cybernetic

660
00:33:06,660 --> 00:33:09,076
theorems and concepts like Requisite

661
00:33:09,108 --> 00:33:11,464
diversity, good regulator theorem and so

662
00:33:11,502 --> 00:33:13,944
on. However, the generative model does

663
00:33:13,982 --> 00:33:17,116
not have to be exactly isomorphic with

664
00:33:17,138 --> 00:33:19,260
the generative process. For example,

665
00:33:19,330 --> 00:33:21,144
the generative process, the temperature

666
00:33:21,192 --> 00:33:23,304
in the room might be a continuous

667
00:33:23,352 --> 00:33:26,300
variable, but then the generative model

668
00:33:26,450 --> 00:33:29,604
might be discrete only, modeling integer

669
00:33:29,672 --> 00:33:31,152
based temperatures or might be

670
00:33:31,206 --> 00:33:33,728
categorical like too hot, just right,

671
00:33:33,814 --> 00:33:36,224
and too cold. So there's a lot of

672
00:33:36,262 --> 00:33:38,912
articulations that can be done because

673
00:33:38,966 --> 00:33:41,416
of how flexible and interoperable

674
00:33:41,548 --> 00:33:43,670
generative models are with each other.

675
00:33:44,280 --> 00:33:48,560
Figure two is going to expand

676
00:33:48,720 --> 00:33:51,636
upon that chapter one representation of

677
00:33:51,658 --> 00:33:53,888
the cybernetic action perception loop.

678
00:33:53,984 --> 00:33:56,456
And we're going to see this more in

679
00:33:56,478 --> 00:33:58,552
terms of a generative model and

680
00:33:58,606 --> 00:34:01,496
generative process articulation. And

681
00:34:01,598 --> 00:34:03,944
there are incoming speaking from the

682
00:34:03,982 --> 00:34:05,400
perspective of the generative model,

683
00:34:05,470 --> 00:34:07,912
the agent. There are incoming and

684
00:34:07,966 --> 00:34:11,528
outgoing dependencies in this graph.

685
00:34:11,624 --> 00:34:13,704
And this is a little bit like a schema,

686
00:34:13,832 --> 00:34:16,236
more so than a formal graph, but also it

687
00:34:16,258 --> 00:34:19,256
is like a Bayesian graph where nodes are

688
00:34:19,298 --> 00:34:21,516
variables and edges are causal

689
00:34:21,548 --> 00:34:23,776
relationships. And so we have the

690
00:34:23,798 --> 00:34:26,544
internal states of the model, the

691
00:34:26,582 --> 00:34:28,012
external states of the generative

692
00:34:28,076 --> 00:34:31,388
process, and then the blanket states

693
00:34:31,574 --> 00:34:33,908
that make internal and external states

694
00:34:33,994 --> 00:34:35,748
conditionally independent. And again,

695
00:34:35,834 --> 00:34:37,876
speaking from the perspective of the

696
00:34:37,898 --> 00:34:40,436
agent, although there is a symmetry, we

697
00:34:40,458 --> 00:34:43,408
can talk about the incoming sensory

698
00:34:43,504 --> 00:34:46,516
signals happening from the observations

699
00:34:46,628 --> 00:34:48,936
handed from the process passed on to the

700
00:34:48,958 --> 00:34:50,792
internal states of the generative model

701
00:34:50,926 --> 00:34:53,944
and then the outgoing actions that are

702
00:34:53,982 --> 00:34:56,296
selected that then can influence the

703
00:34:56,318 --> 00:34:58,236
hidden state of the world. For example,

704
00:34:58,338 --> 00:35:00,236
going and turning on the heater to

705
00:35:00,258 --> 00:35:02,108
increase the temperature in the room.

706
00:35:02,274 --> 00:35:05,868
And so this action perception loop or

707
00:35:05,954 --> 00:35:08,412
particular partition is going to get

708
00:35:08,466 --> 00:35:10,336
explored in a lot more detail in the

709
00:35:10,358 --> 00:35:14,172
coming sections. Previous section

710
00:35:14,236 --> 00:35:17,184
2.3 was on perception as inference and

711
00:35:17,222 --> 00:35:19,776
now action as inference is going to be

712
00:35:19,798 --> 00:35:22,272
discussed. And that is where they say

713
00:35:22,326 --> 00:35:23,812
the discussion to this point is common

714
00:35:23,866 --> 00:35:26,404
to all Bayesian brain theories. We now

715
00:35:26,442 --> 00:35:27,936
introduce the simple but fundamental

716
00:35:27,968 --> 00:35:30,064
advance offered by active inference,

717
00:35:30,192 --> 00:35:31,924
which is the extension of this

718
00:35:31,962 --> 00:35:34,716
inferential perspective to consideration

719
00:35:34,848 --> 00:35:36,840
of action as inference.

720
00:35:38,780 --> 00:35:40,824
Perception and action cooperate to

721
00:35:40,862 --> 00:35:42,840
realize a single objective.

722
00:35:47,340 --> 00:35:50,376
Section two five is about minimizing the

723
00:35:50,398 --> 00:35:51,916
discrepancy between the model and the

724
00:35:51,938 --> 00:35:54,136
world. We already described that there's

725
00:35:54,168 --> 00:35:56,076
two ways for this to happen change your

726
00:35:56,098 --> 00:35:58,236
mind and change the world. That's how

727
00:35:58,258 --> 00:36:00,332
the discrepancy can be reduced or

728
00:36:00,386 --> 00:36:01,260
managed.

729
00:36:03,220 --> 00:36:05,516
We see a variant of the action

730
00:36:05,548 --> 00:36:07,584
perception loop where the agent is

731
00:36:07,622 --> 00:36:12,892
making action prediction models

732
00:36:12,956 --> 00:36:15,796
of the world that through perception are

733
00:36:15,818 --> 00:36:17,504
being juxtaposed with observations

734
00:36:17,552 --> 00:36:19,236
handed from the generative process the

735
00:36:19,258 --> 00:36:22,288
world and a discrepancy is realized,

736
00:36:22,384 --> 00:36:25,712
some nonzero discrepancy. And then

737
00:36:25,786 --> 00:36:27,524
here are those two paths to minimize

738
00:36:27,572 --> 00:36:29,832
free energy, change beliefs by

739
00:36:29,886 --> 00:36:32,696
perception and learning, or change the

740
00:36:32,718 --> 00:36:36,676
world through action prediction

741
00:36:36,708 --> 00:36:40,284
two six is going to discuss how the

742
00:36:40,322 --> 00:36:42,024
exact Bayesian approach described

743
00:36:42,072 --> 00:36:45,388
earlier is absolutely spot on if you

744
00:36:45,394 --> 00:36:47,416
have infinite computing resources.

745
00:36:47,528 --> 00:36:51,028
However, we're often interested in rapid

746
00:36:51,144 --> 00:36:54,256
or large data sets where we want

747
00:36:54,278 --> 00:36:56,332
to be able to get approximate Bayesian

748
00:36:56,396 --> 00:36:58,780
computation or probably approximately

749
00:36:58,860 --> 00:37:01,644
correct computation in a vastly

750
00:37:01,692 --> 00:37:04,516
accelerated fashion. And so that is

751
00:37:04,538 --> 00:37:06,688
going to be approached using what's

752
00:37:06,704 --> 00:37:09,200
called variational Bayesian inference.

753
00:37:09,280 --> 00:37:12,356
That's unpacked in chapter four. But it

754
00:37:12,378 --> 00:37:14,256
suffices to say that variational

755
00:37:14,288 --> 00:37:16,372
Bayesian inference implies substituting

756
00:37:16,436 --> 00:37:19,860
two intractable quantities the posterior

757
00:37:19,940 --> 00:37:23,224
probability and log model evidence with

758
00:37:23,262 --> 00:37:25,416
two quantities that approximate them but

759
00:37:25,438 --> 00:37:27,416
can be computed efficiently the

760
00:37:27,438 --> 00:37:29,356
approximate posterior Q and A

761
00:37:29,378 --> 00:37:31,852
variational free energy. So it

762
00:37:31,906 --> 00:37:34,760
transforms an intractable estimation

763
00:37:34,840 --> 00:37:37,304
problem into a highly tractable

764
00:37:37,432 --> 00:37:39,436
optimization problem. Pick up from

765
00:37:39,458 --> 00:37:40,380
there. Ali.

766
00:37:42,660 --> 00:37:46,528
Yes, but sorry. I just wanted to

767
00:37:46,694 --> 00:37:49,056
point out a couple of things,

768
00:37:49,158 --> 00:37:52,976
especially about section 2.3, because I

769
00:37:52,998 --> 00:37:55,776
believe it. It is one of the crucial

770
00:37:55,888 --> 00:37:57,908
sections of this chapter and in fact,

771
00:37:57,994 --> 00:38:00,916
the whole book, because it provides some

772
00:38:00,938 --> 00:38:04,144
of the justifications of using Bayesian

773
00:38:04,192 --> 00:38:07,450
inference as opposed to some other

774
00:38:08,780 --> 00:38:11,556
mathematical techniques such as maximum

775
00:38:11,588 --> 00:38:15,064
likelihood estimation. But more

776
00:38:15,102 --> 00:38:18,796
important than that, I think it's this

777
00:38:18,898 --> 00:38:21,132
concept of optimality which comes into

778
00:38:21,186 --> 00:38:25,356
play in almost everywhere in

779
00:38:25,378 --> 00:38:27,516
the literature and of course, in this

780
00:38:27,538 --> 00:38:30,924
book. So there are actually two notions

781
00:38:30,972 --> 00:38:34,704
of optimality which is not discussed in

782
00:38:34,742 --> 00:38:37,036
detail here, namely Bayesian optimality

783
00:38:37,068 --> 00:38:40,688
and Jane's optimality. But in some of

784
00:38:40,694 --> 00:38:43,360
the recent papers on Bayesian mechanics,

785
00:38:44,180 --> 00:38:46,356
dalton, Sactavato, Maxwell, Ramstead and

786
00:38:46,378 --> 00:38:48,900
others have shown that those two

787
00:38:49,050 --> 00:38:51,380
concepts of optimality are actually

788
00:38:51,450 --> 00:38:54,116
congruent with each other. So that's one

789
00:38:54,138 --> 00:38:57,896
of the reasons that the

790
00:38:57,918 --> 00:39:01,320
duality between FEP free energy

791
00:39:01,390 --> 00:39:03,556
principle and constrained maximum

792
00:39:03,588 --> 00:39:06,984
entropy principle can

793
00:39:07,022 --> 00:39:10,076
be I mean, it's one of the

794
00:39:10,098 --> 00:39:13,592
justifications for providing

795
00:39:13,656 --> 00:39:16,110
that dual formalism between those two.

796
00:39:17,840 --> 00:39:20,348
But another point I wanted to mention

797
00:39:20,434 --> 00:39:24,656
here is because I've seen that using the

798
00:39:24,678 --> 00:39:28,096
word hidden state can be

799
00:39:28,118 --> 00:39:30,290
a bit confusing for some people because

800
00:39:30,660 --> 00:39:34,036
when we observe something

801
00:39:34,218 --> 00:39:37,428
as an observation, obviously it is not,

802
00:39:37,594 --> 00:39:40,596
quote unquote, hidden, right? So what a

803
00:39:40,618 --> 00:39:43,220
hidden state here refers to is actually

804
00:39:43,370 --> 00:39:45,644
the hidden cause of that observation.

805
00:39:45,712 --> 00:39:48,552
So it's not that the observation itself

806
00:39:48,686 --> 00:39:51,396
is hidden from the observation or it's

807
00:39:51,428 --> 00:39:54,552
unobserved. So that might be a bit

808
00:39:54,686 --> 00:39:58,364
confusing if we don't take into

809
00:39:58,402 --> 00:40:00,396
consideration the exact meaning of the

810
00:40:00,418 --> 00:40:03,676
hidden state or latent state here and in

811
00:40:03,698 --> 00:40:06,332
the rest of the literature. So

812
00:40:06,466 --> 00:40:10,430
continuing from Section 2.6 here

813
00:40:10,820 --> 00:40:14,988
we see one of the two central equations

814
00:40:15,164 --> 00:40:18,604
of active inference, which is equation

815
00:40:18,652 --> 00:40:22,316
2.5 for variational

816
00:40:22,348 --> 00:40:26,000
free energy. So it's,

817
00:40:26,440 --> 00:40:30,608
as I said, understanding this equation

818
00:40:30,704 --> 00:40:34,048
and how each line of its formulation

819
00:40:34,144 --> 00:40:38,628
represents in terms of the trade

820
00:40:38,644 --> 00:40:40,712
off between energy, entropy or

821
00:40:40,766 --> 00:40:42,884
complexity and accuracy or divergence.

822
00:40:42,932 --> 00:40:47,236
And evidence is key to understanding

823
00:40:47,268 --> 00:40:50,716
almost everything in the rest of the

824
00:40:50,738 --> 00:40:53,324
book and in many other literature on

825
00:40:53,362 --> 00:40:56,748
active inference. So this is

826
00:40:56,834 --> 00:40:59,336
the perceptual part of active inference.

827
00:40:59,368 --> 00:41:03,184
So variation free energy is a

828
00:41:03,222 --> 00:41:07,904
parameter, that is

829
00:41:07,942 --> 00:41:10,160
a notion that parameterizes the

830
00:41:10,230 --> 00:41:14,004
surprisal of our perceptual information

831
00:41:14,122 --> 00:41:18,004
we have about the external states. And

832
00:41:18,042 --> 00:41:21,700
then we'll see in the next section

833
00:41:22,680 --> 00:41:26,544
the related and almost symmetrical

834
00:41:26,672 --> 00:41:29,160
formulation to variation of free energy,

835
00:41:29,230 --> 00:41:32,184
namely expected free energy, which is

836
00:41:32,222 --> 00:41:35,556
basically the action part of the active

837
00:41:35,588 --> 00:41:38,750
inference. So we can see how those two

838
00:41:39,120 --> 00:41:42,348
can somehow be seen

839
00:41:42,434 --> 00:41:45,084
as a kind of unified formalism but

840
00:41:45,122 --> 00:41:49,340
described in alternate expressions.

841
00:41:50,660 --> 00:41:55,250
But another thing about equation 2.5 is

842
00:41:56,420 --> 00:41:59,472
it may be a bit, I don't know,

843
00:41:59,606 --> 00:42:03,748
daunting to see all. The relations

844
00:42:03,834 --> 00:42:05,392
between those three lines of equation

845
00:42:05,456 --> 00:42:07,876
and how we can get from one to the

846
00:42:07,898 --> 00:42:11,270
other. So there are some

847
00:42:11,640 --> 00:42:14,596
supplementary materials that we have

848
00:42:14,618 --> 00:42:19,064
developed in the past weeks which

849
00:42:19,102 --> 00:42:23,044
I think can help in clarifying

850
00:42:23,092 --> 00:42:25,556
how the derivations of these three lines

851
00:42:25,588 --> 00:42:29,870
of equation be done. So I hope

852
00:42:30,480 --> 00:42:35,740
they would be clarifying and

853
00:42:35,810 --> 00:42:38,360
help to understand how those three lines

854
00:42:38,440 --> 00:42:39,790
relate to each other.

855
00:42:43,060 --> 00:42:47,424
But the key point here is to

856
00:42:47,462 --> 00:42:49,696
understand that variation of free energy

857
00:42:49,798 --> 00:42:53,292
is not something absolute,

858
00:42:53,446 --> 00:42:55,908
but it's just an upper bound for the

859
00:42:55,914 --> 00:42:59,044
minimization. So as Daniel just

860
00:42:59,162 --> 00:43:02,496
mentioned, it's untractable

861
00:43:02,608 --> 00:43:06,616
to have an absolute amount for

862
00:43:06,638 --> 00:43:09,976
the surprisal to be minimized. So we

863
00:43:09,998 --> 00:43:13,832
need to have an upper bound in order

864
00:43:13,886 --> 00:43:18,648
to make that more tractable because by

865
00:43:18,734 --> 00:43:21,950
Jane's inequality, as we'll saw in

866
00:43:23,840 --> 00:43:28,204
chapter four, I think we

867
00:43:28,242 --> 00:43:31,772
can see that how the upper bound

868
00:43:31,836 --> 00:43:35,836
of a surprisal necessarily

869
00:43:35,948 --> 00:43:39,040
provides a condition for minimizing the

870
00:43:39,190 --> 00:43:43,396
precise or the

871
00:43:43,418 --> 00:43:47,364
exact free energy. And that's the

872
00:43:47,402 --> 00:43:51,140
key insight of equation 2.5 or

873
00:43:51,290 --> 00:43:53,652
the notion of variation of free energy

874
00:43:53,786 --> 00:43:56,156
which is to provide this upper bound

875
00:43:56,208 --> 00:43:58,804
instead of the exact amount of surprisal

876
00:43:58,852 --> 00:44:02,152
to be minimized. Yeah, what I'll add

877
00:44:02,206 --> 00:44:05,624
there is if you knew exactly how well

878
00:44:05,662 --> 00:44:08,084
you should be surprised by a given data

879
00:44:08,142 --> 00:44:11,244
point Y, then you would have had the

880
00:44:11,282 --> 00:44:14,732
optimal model. However, that is not

881
00:44:14,786 --> 00:44:18,312
tractable. And so by making a quantity

882
00:44:18,376 --> 00:44:21,948
that's always higher or an upper bound,

883
00:44:22,044 --> 00:44:24,416
the variational free energy F, which is

884
00:44:24,438 --> 00:44:27,904
a function of broadly Q, our beliefs or

885
00:44:27,942 --> 00:44:31,328
variational beliefs which are built in a

886
00:44:31,334 --> 00:44:33,512
way that makes them very compositional,

887
00:44:33,676 --> 00:44:37,172
very optimizable, very interpretable and

888
00:44:37,226 --> 00:44:40,512
data. And we can reduce the divergence

889
00:44:40,576 --> 00:44:42,544
here, the KL divergence with a double

890
00:44:42,592 --> 00:44:46,548
line between Q, our beliefs and P,

891
00:44:46,634 --> 00:44:49,416
the kind of actuality of it. And if we

892
00:44:49,438 --> 00:44:51,656
can reduce this divergence, in other

893
00:44:51,678 --> 00:44:55,336
words, minimize the free energy, then we

894
00:44:55,358 --> 00:44:57,912
will come closer and closer to the true

895
00:44:57,966 --> 00:44:59,996
surprise function and do that in an

896
00:45:00,018 --> 00:45:02,712
attractable, incrementally, optimizable

897
00:45:02,776 --> 00:45:06,316
way. So equation 2.5 is going

898
00:45:06,338 --> 00:45:08,750
to be the variational free energy,

899
00:45:09,120 --> 00:45:10,696
different ways that it can be

900
00:45:10,738 --> 00:45:14,800
represented as it takes in data and

901
00:45:14,950 --> 00:45:18,610
beliefs about the world Q. And then

902
00:45:18,980 --> 00:45:21,824
this is going to be expanded into the

903
00:45:21,862 --> 00:45:25,570
future to include action with G,

904
00:45:25,880 --> 00:45:29,108
the expected free energy. Now, there's a

905
00:45:29,114 --> 00:45:30,676
lot more that we can say about this.

906
00:45:30,698 --> 00:45:32,196
There's a lot of technicalities to go

907
00:45:32,218 --> 00:45:35,444
into. But broadly, notice that g the

908
00:45:35,482 --> 00:45:38,040
expected free energy is a functional of

909
00:45:38,110 --> 00:45:41,224
policy pi because it's only being

910
00:45:41,262 --> 00:45:44,328
evaluated to select amongst different

911
00:45:44,414 --> 00:45:47,992
action outcomes. Pi and another

912
00:45:48,046 --> 00:45:49,884
important difference is that it's going

913
00:45:49,922 --> 00:45:53,084
to be describing sensory outcomes that

914
00:45:53,122 --> 00:45:55,852
haven't yet happened. A sort of what

915
00:45:55,906 --> 00:45:59,308
would I perceive if I did A or what

916
00:45:59,314 --> 00:46:01,584
if I did B? And it's that kind of

917
00:46:01,622 --> 00:46:04,752
comparison that allows the expected free

918
00:46:04,806 --> 00:46:08,208
energy functional here to be used in

919
00:46:08,294 --> 00:46:11,760
action selection or policy selection as

920
00:46:11,830 --> 00:46:14,660
inference, planning as inference.

921
00:46:16,920 --> 00:46:19,856
That section is expanded upon.

922
00:46:20,048 --> 00:46:22,916
And in figure two six, we see a very

923
00:46:22,938 --> 00:46:25,092
nice representation of the expected free

924
00:46:25,146 --> 00:46:28,520
energy equation. And then how, when

925
00:46:28,590 --> 00:46:30,792
certain aspects of this equation or

926
00:46:30,846 --> 00:46:34,120
situation are zeroed out, we get certain

927
00:46:34,190 --> 00:46:37,130
other familiar cases. For example,

928
00:46:37,740 --> 00:46:40,204
where there is no epistemic value,

929
00:46:40,322 --> 00:46:43,068
there's no information to learn. Then

930
00:46:43,234 --> 00:46:45,960
you get ruthless expected utility

931
00:46:46,040 --> 00:46:49,484
theory. And conversely, where there is

932
00:46:49,522 --> 00:46:52,444
no pragmatic value to extract.

933
00:46:52,572 --> 00:46:55,280
So all outcomes are equally valid or

934
00:46:55,350 --> 00:46:58,672
preferable. Then you get things like

935
00:46:58,726 --> 00:47:01,532
Infomax principle and optimal Bayesian

936
00:47:01,596 --> 00:47:04,016
design and then everything in between is

937
00:47:04,038 --> 00:47:05,796
the space that we're interested in. And

938
00:47:05,818 --> 00:47:09,252
so this figure 2.6 shows that

939
00:47:09,306 --> 00:47:12,916
the expected free energy functional can

940
00:47:12,938 --> 00:47:15,428
be seen as like a generalization of a

941
00:47:15,434 --> 00:47:17,752
lot of other settings related to

942
00:47:17,806 --> 00:47:19,924
perception and action and planning

943
00:47:19,972 --> 00:47:23,752
amidst uncertainty. And section 2.9

944
00:47:23,886 --> 00:47:26,408
closes the low road. They took us all

945
00:47:26,414 --> 00:47:28,368
the way to active inference, from Bayes

946
00:47:28,404 --> 00:47:30,060
theorem through the generative model

947
00:47:30,130 --> 00:47:33,352
onto active inference and clarifies

948
00:47:33,416 --> 00:47:35,324
these two notions of variational free

949
00:47:35,362 --> 00:47:37,800
energy that's the real time perceptual

950
00:47:37,880 --> 00:47:40,104
unfolding evidence, lower bound,

951
00:47:40,152 --> 00:47:43,056
unsurprisal tractable optimizable, and

952
00:47:43,078 --> 00:47:47,244
so on and f and g, the expected

953
00:47:47,292 --> 00:47:50,444
free energy which is able to do planning

954
00:47:50,492 --> 00:47:53,004
as inference or policy selection as

955
00:47:53,062 --> 00:47:55,604
inference. Expected free energy is

956
00:47:55,642 --> 00:47:58,372
fundamentally prospective and that

957
00:47:58,426 --> 00:48:01,220
enables counterfactual cognition.

958
00:48:02,280 --> 00:48:05,456
Section 210 summarizes active inference

959
00:48:05,488 --> 00:48:07,332
is the theory of how living artifacts

960
00:48:07,396 --> 00:48:09,364
underwrite their existence by minimizing

961
00:48:09,412 --> 00:48:11,704
surprise or attractable proxy to

962
00:48:11,742 --> 00:48:14,484
surprise variational free energy via

963
00:48:14,532 --> 00:48:17,124
perception and action. And they motivate

964
00:48:17,172 --> 00:48:19,932
that from a first principles base

965
00:48:19,986 --> 00:48:23,896
theorem starting place. Any closing

966
00:48:23,928 --> 00:48:25,660
thoughts on chapter two, Ali?

967
00:48:27,760 --> 00:48:30,236
I would just humbly suggest for the

968
00:48:30,258 --> 00:48:32,704
people who want to go through this

969
00:48:32,742 --> 00:48:36,944
chapter to try their

970
00:48:36,982 --> 00:48:40,720
best to really understand specifically

971
00:48:41,140 --> 00:48:44,992
what equation 2.5 and 2.6 represents

972
00:48:45,056 --> 00:48:48,804
and how I mean, how to

973
00:48:48,842 --> 00:48:52,820
use those equations to describe

974
00:48:53,240 --> 00:48:57,816
different situations with some

975
00:48:57,998 --> 00:48:59,770
missing elements as well.

976
00:49:00,300 --> 00:49:03,736
Because I believe those sections, and

977
00:49:03,838 --> 00:49:06,536
particularly those equations are

978
00:49:06,638 --> 00:49:10,460
absolutely essential for understanding

979
00:49:10,880 --> 00:49:13,724
everything active inference related both

980
00:49:13,762 --> 00:49:18,444
in the rest of this book and in

981
00:49:18,562 --> 00:49:21,612
almost all. Thanks. And just the last

982
00:49:21,666 --> 00:49:23,456
thought I'll give on chapter two is this

983
00:49:23,478 --> 00:49:25,248
is exactly the work that we do in the

984
00:49:25,254 --> 00:49:27,136
active inference textbook group. We

985
00:49:27,158 --> 00:49:29,232
really welcome all backgrounds, every

986
00:49:29,286 --> 00:49:31,616
single question and uncertainty you have

987
00:49:31,718 --> 00:49:34,464
is beautiful. We have a lot of resources

988
00:49:34,512 --> 00:49:36,756
that Ali and others make to make the

989
00:49:36,778 --> 00:49:39,892
math approachable and rigorous and

990
00:49:40,026 --> 00:49:41,876
natural language descriptions of the

991
00:49:41,898 --> 00:49:44,564
equations and so on. So yes, it's really

992
00:49:44,602 --> 00:49:46,660
important to understand the equations

993
00:49:46,820 --> 00:49:48,776
because after all, that's like the

994
00:49:48,798 --> 00:49:52,564
skeleton that gives meaning to our usage

995
00:49:52,612 --> 00:49:54,276
and fluency of the active inference

996
00:49:54,308 --> 00:49:56,520
ontology which Ali and I are speaking

997
00:49:56,590 --> 00:49:58,792
right now. We're not just saying

998
00:49:58,846 --> 00:50:00,904
surprise is related to this because we

999
00:50:00,942 --> 00:50:03,164
felt it that way. There is an

1000
00:50:03,202 --> 00:50:05,612
underpinning and it is a really

1001
00:50:05,666 --> 00:50:07,708
interesting life's work to explore it,

1002
00:50:07,794 --> 00:50:09,756
but we're finding ways to communicate it

1003
00:50:09,778 --> 00:50:11,360
and learn and teach it better and better

1004
00:50:11,430 --> 00:50:13,904
every time. So that's chapter two,

1005
00:50:13,942 --> 00:50:17,952
that's the low road moving

1006
00:50:18,006 --> 00:50:20,144
on to chapter three, the high road to

1007
00:50:20,182 --> 00:50:22,816
active inference. And so recall that the

1008
00:50:22,838 --> 00:50:25,812
high road is like the why. So,

1009
00:50:25,866 --> 00:50:27,796
Ali, please begin, just go as long as

1010
00:50:27,818 --> 00:50:29,716
you want on through the high road to

1011
00:50:29,738 --> 00:50:30,900
active inference.

1012
00:50:33,400 --> 00:50:35,216
Okay, so the high road to active

1013
00:50:35,248 --> 00:50:38,424
inference begin with the

1014
00:50:38,462 --> 00:50:41,930
question about

1015
00:50:42,540 --> 00:50:45,956
the conditions for the persistence

1016
00:50:46,068 --> 00:50:49,710
of things. I mean,

1017
00:50:50,320 --> 00:50:53,964
what would we expect for a thing to

1018
00:50:54,002 --> 00:50:56,972
behave if it persists through time? And

1019
00:50:57,026 --> 00:51:00,744
that's why sometimes FEP is referred to

1020
00:51:00,802 --> 00:51:04,848
as the theory of every space thing.

1021
00:51:04,934 --> 00:51:07,248
And it's not absolutely everything, but

1022
00:51:07,414 --> 00:51:09,872
just in this sense of I mean,

1023
00:51:09,926 --> 00:51:13,256
persisting through time. And the notion

1024
00:51:13,308 --> 00:51:17,108
that allows for this thing

1025
00:51:17,194 --> 00:51:20,884
to act as

1026
00:51:20,922 --> 00:51:24,864
it persists through time is markup

1027
00:51:24,912 --> 00:51:27,752
blanket. So Markov blanket is one of the

1028
00:51:27,806 --> 00:51:32,184
most essential ingredients of the

1029
00:51:32,222 --> 00:51:34,756
high road approach to active inference.

1030
00:51:34,948 --> 00:51:38,148
And that's why this section opens up

1031
00:51:38,254 --> 00:51:41,628
with describing how

1032
00:51:41,714 --> 00:51:45,640
markup blanket allows to describe

1033
00:51:45,720 --> 00:51:49,436
the situation of interest through this

1034
00:51:49,538 --> 00:51:52,272
conceptual and mathematical tool. But

1035
00:51:52,406 --> 00:51:55,344
one thing that I always point out in

1036
00:51:55,382 --> 00:51:59,490
almost all our textbook cohorts is

1037
00:52:01,540 --> 00:52:07,588
in this section. It doesn't exactly

1038
00:52:07,754 --> 00:52:12,624
describe markup blankets rigorously

1039
00:52:12,672 --> 00:52:15,990
enough in terms of it's carving up the

1040
00:52:17,500 --> 00:52:21,050
state space and so on. So one thing

1041
00:52:23,580 --> 00:52:27,652
I think can be clarifying

1042
00:52:27,716 --> 00:52:30,924
in following all the discussions around

1043
00:52:30,962 --> 00:52:33,580
markup blanket is to keep in mind that

1044
00:52:33,730 --> 00:52:36,748
it's just a boundary in the state space.

1045
00:52:36,834 --> 00:52:40,916
So it's not necessarily spatial temporal

1046
00:52:41,048 --> 00:52:44,528
boundary, although in

1047
00:52:44,614 --> 00:52:47,120
many cases it manifests itself as

1048
00:52:47,190 --> 00:52:48,944
spatial temporal boundaries, such as

1049
00:52:48,982 --> 00:52:51,410
the, I don't know, cell membranes or

1050
00:52:52,020 --> 00:52:54,576
organelle membranes and so on. But it's

1051
00:52:54,608 --> 00:52:58,148
not necessarily the case. So I think it

1052
00:52:58,234 --> 00:53:00,724
can be helpful to keep that in mind when

1053
00:53:00,762 --> 00:53:03,264
we every everywhere we see markup

1054
00:53:03,312 --> 00:53:08,056
blanket. But in

1055
00:53:08,078 --> 00:53:12,232
a very simply, markup blanket is

1056
00:53:12,286 --> 00:53:16,216
what allows for the agent or the system

1057
00:53:16,398 --> 00:53:19,292
to be statistically separated from its

1058
00:53:19,346 --> 00:53:21,900
environment, as is shown here in Figure

1059
00:53:21,970 --> 00:53:24,620
3.1. So basically,

1060
00:53:24,690 --> 00:53:27,630
markup blanket mathematically is just

1061
00:53:31,220 --> 00:53:33,490
the sensory and active states together.

1062
00:53:33,860 --> 00:53:38,770
It consists of both of those states and

1063
00:53:39,220 --> 00:53:42,288
it statistically separates what happened

1064
00:53:42,374 --> 00:53:44,996
externally or internally from each

1065
00:53:45,018 --> 00:53:47,056
other. So in other words, internal

1066
00:53:47,088 --> 00:53:50,580
states cannot observe or infer anything

1067
00:53:50,650 --> 00:53:53,364
about the external states directly, but

1068
00:53:53,402 --> 00:53:56,840
only through the Markov blanket.

1069
00:53:57,260 --> 00:54:00,890
And that's why it's essential to

1070
00:54:02,220 --> 00:54:04,264
describe the systems in this way,

1071
00:54:04,302 --> 00:54:08,336
because obviously, in any sparse coupled

1072
00:54:08,388 --> 00:54:12,524
systems, there isn't the

1073
00:54:12,562 --> 00:54:15,816
possibility to observe the external

1074
00:54:15,848 --> 00:54:19,724
states directly, but only through

1075
00:54:19,762 --> 00:54:22,544
those sensory and active states. And one

1076
00:54:22,582 --> 00:54:26,144
other thing that I also believe can be

1077
00:54:26,182 --> 00:54:29,392
a bit confusing in this picture is that,

1078
00:54:29,526 --> 00:54:33,276
yes, typo in the ActInf estates

1079
00:54:33,308 --> 00:54:35,036
and sensory states because it's

1080
00:54:35,068 --> 00:54:38,392
important to observe that active estate

1081
00:54:38,556 --> 00:54:41,460
only or the flow in the active states

1082
00:54:41,530 --> 00:54:45,510
more precisely, only depends on

1083
00:54:45,900 --> 00:54:49,364
internal and markup states or blanket

1084
00:54:49,412 --> 00:54:52,490
states. And on the other hand, in

1085
00:54:52,860 --> 00:54:56,104
sensory states only depend on the

1086
00:54:56,142 --> 00:54:59,932
external and blanket states. So Mu and

1087
00:54:59,986 --> 00:55:02,476
X in those two equations should be

1088
00:55:02,498 --> 00:55:04,670
exchanged. Yes.

1089
00:55:06,240 --> 00:55:08,828
Let me just unpack before we continue.

1090
00:55:08,914 --> 00:55:11,996
To add before we continue. Yes. So this

1091
00:55:12,018 --> 00:55:13,680
is what's known as the particular

1092
00:55:13,830 --> 00:55:16,736
partition. And that terminology was

1093
00:55:16,758 --> 00:55:19,036
brought to the fore by Carl Friston's

1094
00:55:19,068 --> 00:55:20,736
famous monograph in 2019.

1095
00:55:20,758 --> 00:55:23,956
And it's a little bit of a pun. It's a

1096
00:55:23,978 --> 00:55:25,716
particular partition because this is

1097
00:55:25,738 --> 00:55:28,484
just one way to partition agent from

1098
00:55:28,522 --> 00:55:30,900
environment, figure from ground.

1099
00:55:31,960 --> 00:55:35,508
And we call what it partitions out

1100
00:55:35,594 --> 00:55:38,184
the blankets and internal states as the

1101
00:55:38,222 --> 00:55:40,760
particle. So we can think about the most

1102
00:55:40,830 --> 00:55:44,616
inert least cognitive particle is like a

1103
00:55:44,638 --> 00:55:46,728
speck of dust doing brownie and

1104
00:55:46,734 --> 00:55:49,420
diffusion. But also there are more

1105
00:55:49,490 --> 00:55:51,432
sophisticated kinds of cognitive

1106
00:55:51,496 --> 00:55:55,160
particles that can include world models,

1107
00:55:55,240 --> 00:55:57,916
counterfactuals what would happen if I

1108
00:55:57,938 --> 00:55:59,484
did this, all of those kinds of

1109
00:55:59,522 --> 00:56:01,904
sophisticated cognitive operations that

1110
00:56:01,942 --> 00:56:04,704
we can explore in active inference. So

1111
00:56:04,822 --> 00:56:08,304
note the small typo that Ali mentioned,

1112
00:56:08,502 --> 00:56:11,068
internal states. And also these edges

1113
00:56:11,164 --> 00:56:15,456
reflect causal possibilities amongst

1114
00:56:15,568 --> 00:56:17,748
internal, external and blanket states.

1115
00:56:17,834 --> 00:56:21,572
Blanket constituting the active U and

1116
00:56:21,626 --> 00:56:25,284
sensory y states. So these are

1117
00:56:25,402 --> 00:56:28,404
map, not territory. This is not a

1118
00:56:28,442 --> 00:56:32,056
spatiotemporal articulation. These are

1119
00:56:32,078 --> 00:56:35,528
like causal maps of the world that

1120
00:56:35,614 --> 00:56:38,856
may have to do with spatial temporal

1121
00:56:38,888 --> 00:56:41,852
boundaries but are not simply that. And

1122
00:56:41,906 --> 00:56:45,672
sensory data or sensory states flow onto

1123
00:56:45,736 --> 00:56:47,644
internal states. Internal states are

1124
00:56:47,682 --> 00:56:50,056
involved in action selection resulting

1125
00:56:50,088 --> 00:56:53,612
in active states which have some causal

1126
00:56:53,676 --> 00:56:55,532
consequence in the world, the generative

1127
00:56:55,596 --> 00:56:57,712
process, the external states which

1128
00:56:57,766 --> 00:56:59,600
results in different sensory states

1129
00:56:59,670 --> 00:57:02,236
coming in again. And so the Markov

1130
00:57:02,268 --> 00:57:04,196
blanket is what makes the internal and

1131
00:57:04,218 --> 00:57:05,856
the external states conditionally

1132
00:57:05,888 --> 00:57:08,276
independent. And that can be thought of

1133
00:57:08,298 --> 00:57:11,552
as just a no telekinesis, no telepathy

1134
00:57:11,616 --> 00:57:14,756
clause. The only way that information

1135
00:57:14,938 --> 00:57:17,396
comes across internal and external

1136
00:57:17,428 --> 00:57:19,576
states which are symmetrical with each

1137
00:57:19,598 --> 00:57:22,088
other is through the boundary or the

1138
00:57:22,094 --> 00:57:24,356
Holographic screen or the Markov

1139
00:57:24,388 --> 00:57:25,240
blanket.

1140
00:57:28,370 --> 00:57:30,190
You can continue on ali.

1141
00:57:33,750 --> 00:57:37,460
Okay, so the next section is

1142
00:57:39,030 --> 00:57:41,394
about surprise, minimization and self

1143
00:57:41,432 --> 00:57:44,502
evidencing. So again, this term self

1144
00:57:44,556 --> 00:57:48,306
evidencing is a common term in active

1145
00:57:48,338 --> 00:57:51,062
inference literature first proposed by

1146
00:57:51,116 --> 00:57:55,034
Jacob Howie. So basically it

1147
00:57:55,072 --> 00:57:58,714
refers to how a system or agent or

1148
00:57:58,752 --> 00:58:02,140
in other words a particular state can

1149
00:58:02,670 --> 00:58:06,010
gather the evidence for its self

1150
00:58:06,080 --> 00:58:09,022
persistence or self existence through

1151
00:58:09,076 --> 00:58:10,800
time. In other words,

1152
00:58:11,810 --> 00:58:14,814
by inferring the state of the

1153
00:58:14,852 --> 00:58:19,390
environment and comparing that

1154
00:58:19,460 --> 00:58:22,546
inference with its internal states or in

1155
00:58:22,568 --> 00:58:24,820
other words, with its generative model.

1156
00:58:26,870 --> 00:58:29,554
It's basically a way, I mean an

1157
00:58:29,592 --> 00:58:32,934
interpretation of that

1158
00:58:32,972 --> 00:58:36,182
kind of inference is on the one hand,

1159
00:58:36,236 --> 00:58:38,834
there is this dynamical interaction

1160
00:58:38,882 --> 00:58:41,062
between the internal and external state.

1161
00:58:41,196 --> 00:58:44,760
But we can somehow interpret that

1162
00:58:45,230 --> 00:58:50,026
physical dynamics as engaging in

1163
00:58:50,048 --> 00:58:54,220
the active inference or as model for

1164
00:58:55,550 --> 00:58:58,814
the persistence of the agent through

1165
00:58:58,852 --> 00:59:01,406
time. So that's basically what refers to

1166
00:59:01,428 --> 00:59:06,990
as self evidencing. And then

1167
00:59:07,140 --> 00:59:11,620
we go through the subsection 3.3.1,

1168
00:59:14,470 --> 00:59:17,582
which sets the stage for the recent

1169
00:59:17,646 --> 00:59:19,970
formulation of active inference

1170
00:59:20,630 --> 00:59:26,082
mechanics, which is to somehow

1171
00:59:26,226 --> 00:59:29,122
to interpret the ActInf surprise

1172
00:59:29,186 --> 00:59:33,490
minimization as minimizing

1173
00:59:33,650 --> 00:59:36,694
the action through the Hamiltonian

1174
00:59:36,742 --> 00:59:38,794
principle of least action, which is a

1175
00:59:38,832 --> 00:59:41,514
variational principle. And by

1176
00:59:41,552 --> 00:59:45,606
variational principle, what is meant

1177
00:59:45,638 --> 00:59:48,494
here is just a computational or

1178
00:59:48,532 --> 00:59:52,014
mathematical tool that allows for

1179
00:59:52,212 --> 00:59:57,038
the computation or specific

1180
00:59:57,124 --> 01:00:00,500
derivations to happen. So it's not

1181
01:00:02,950 --> 01:00:06,274
identical to scientific theories or

1182
01:00:06,312 --> 01:00:08,658
scientific, I don't know, facts or

1183
01:00:08,824 --> 01:00:11,906
observations. It's just a tool,

1184
01:00:12,008 --> 01:00:15,334
a principle mathematical tool.

1185
01:00:15,532 --> 01:00:17,720
So again,

1186
01:00:18,890 --> 01:00:22,518
one of the main misconceptions about

1187
01:00:22,604 --> 01:00:25,322
free free energy principle, that it is

1188
01:00:25,376 --> 01:00:29,130
unfalsifiable. Obviously, if we see it

1189
01:00:29,200 --> 01:00:32,554
in this way, it doesn't make sense to

1190
01:00:32,592 --> 01:00:34,554
say that a mathematical tool or

1191
01:00:34,592 --> 01:00:38,094
principle is unfalsifiable because it

1192
01:00:38,132 --> 01:00:40,810
doesn't say anything about the empirical

1193
01:00:40,890 --> 01:00:44,026
evidence of the phenomena we're

1194
01:00:44,058 --> 01:00:47,386
talking about. So that's why here it's

1195
01:00:47,418 --> 01:00:50,006
important to understand how the surprise

1196
01:00:50,058 --> 01:00:53,394
minimization can be seen as this

1197
01:00:53,432 --> 01:00:55,730
kind of variational principle.

1198
01:00:57,350 --> 01:01:01,422
And then equation

1199
01:01:01,486 --> 01:01:04,850
3.1 draws

1200
01:01:04,930 --> 01:01:08,930
the parallel between the surprisal

1201
01:01:09,010 --> 01:01:11,574
as defined in section in chapter two

1202
01:01:11,612 --> 01:01:15,322
sorry. And this chapter. So it kind of

1203
01:01:15,456 --> 01:01:18,986
ties up all the arguments provided in

1204
01:01:19,008 --> 01:01:21,082
the previous chapter with this one and

1205
01:01:21,136 --> 01:01:24,074
how everything comes together in a

1206
01:01:24,112 --> 01:01:27,930
single formulation of active inference.

1207
01:01:28,010 --> 01:01:31,674
But they're just the two distinct

1208
01:01:31,722 --> 01:01:33,582
approaches to arrive at same

1209
01:01:33,636 --> 01:01:34,670
destination.

1210
01:01:38,770 --> 01:01:42,290
And then we go also sorry,

1211
01:01:42,360 --> 01:01:45,694
the other important equation,

1212
01:01:45,822 --> 01:01:49,762
and again, another key equation here is

1213
01:01:49,816 --> 01:01:55,494
equation 3.2, which is kind

1214
01:01:55,532 --> 01:01:57,880
of sorry,

1215
01:01:59,690 --> 01:02:02,514
yes, equation I meant to say equation

1216
01:02:02,562 --> 01:02:05,786
3.2, not equation 3.1. Sorry, my bad.

1217
01:02:05,968 --> 01:02:09,514
So yes. The other section is

1218
01:02:09,632 --> 01:02:12,934
about the relations between inference,

1219
01:02:12,982 --> 01:02:15,446
cognition and stochastic dynamics,

1220
01:02:15,638 --> 01:02:19,294
as all

1221
01:02:19,332 --> 01:02:22,702
the previous discussions around how

1222
01:02:22,836 --> 01:02:25,486
the active inference can be seen as a

1223
01:02:25,508 --> 01:02:28,094
kind of self evidencing through the

1224
01:02:28,292 --> 01:02:32,386
variational principles such as

1225
01:02:32,488 --> 01:02:37,838
FEP comes together to reframe

1226
01:02:38,014 --> 01:02:41,506
the previous discussions we saw in

1227
01:02:41,528 --> 01:02:43,974
chapter two about perception as

1228
01:02:44,012 --> 01:02:46,920
inference and action as inference, and

1229
01:02:48,170 --> 01:02:50,582
to see the concepts of variational free

1230
01:02:50,636 --> 01:02:53,366
energy and expected free energy through

1231
01:02:53,388 --> 01:02:55,882
the lens of variational free energy.

1232
01:02:56,016 --> 01:02:59,530
Variational principle of least

1233
01:02:59,600 --> 01:03:03,670
action. So it unifies

1234
01:03:03,750 --> 01:03:08,390
nicely all the material from chapters

1235
01:03:08,470 --> 01:03:13,660
two and three into something

1236
01:03:15,810 --> 01:03:18,686
not necessarily distinct from each

1237
01:03:18,708 --> 01:03:21,440
other, but just two sides of.

1238
01:03:24,030 --> 01:03:27,534
Awesome. Yeah, I find Table 31 to be

1239
01:03:27,572 --> 01:03:30,526
very exciting. It draws together

1240
01:03:30,628 --> 01:03:32,954
statistical physics, Bayesian

1241
01:03:33,002 --> 01:03:35,122
information, information theory and

1242
01:03:35,176 --> 01:03:37,378
cognitive interpretations. So it's kind

1243
01:03:37,384 --> 01:03:39,442
of like learn one thing, learn many

1244
01:03:39,496 --> 01:03:42,146
things. Statistical physics has been

1245
01:03:42,168 --> 01:03:43,554
talking about minimization of

1246
01:03:43,592 --> 01:03:45,602
variational free energy for a long time,

1247
01:03:45,656 --> 01:03:48,498
and such methods are absolutely everyday

1248
01:03:48,594 --> 01:03:50,754
and professionalized. In Bayesian

1249
01:03:50,802 --> 01:03:53,638
statistics, active inference is using

1250
01:03:53,724 --> 01:03:56,946
exactly just that to describe perception

1251
01:03:56,978 --> 01:03:59,610
and action and so on. So that's very

1252
01:03:59,760 --> 01:04:03,610
exciting. Box 3.2 describes

1253
01:04:04,350 --> 01:04:06,714
free energy in statistical physics and

1254
01:04:06,752 --> 01:04:09,226
active inference. I sometimes joke that

1255
01:04:09,248 --> 01:04:11,878
we have a few kinds of free energy. We

1256
01:04:11,904 --> 01:04:15,086
have Tesla like electrical power should

1257
01:04:15,108 --> 01:04:17,038
be available to everybody for no cost.

1258
01:04:17,124 --> 01:04:19,022
We're not talking about that right now.

1259
01:04:19,156 --> 01:04:22,126
There's Gibbs free energy, which is the

1260
01:04:22,148 --> 01:04:24,354
quantity that makes chemical or

1261
01:04:24,392 --> 01:04:27,758
thermochemical reactions irreversible,

1262
01:04:27,934 --> 01:04:31,266
like the hydrolysis of ATP. And when

1263
01:04:31,288 --> 01:04:33,122
we're talking about variational free

1264
01:04:33,176 --> 01:04:34,914
energy, we're talking about an

1265
01:04:34,952 --> 01:04:37,846
information geometric space with similar

1266
01:04:37,948 --> 01:04:40,374
dynamics, similar kinetics and

1267
01:04:40,412 --> 01:04:42,214
thermodynamics. But rather than

1268
01:04:42,252 --> 01:04:44,726
describing the reaction coordinates of a

1269
01:04:44,748 --> 01:04:46,694
chemical reaction, we're thinking about

1270
01:04:46,732 --> 01:04:48,826
it in terms of Bayesian updating. And

1271
01:04:48,848 --> 01:04:50,214
this is all called the Bayesian

1272
01:04:50,262 --> 01:04:51,210
mechanics.

1273
01:04:57,470 --> 01:05:00,522
3.41 continues on with variational free

1274
01:05:00,576 --> 01:05:01,180
energy.

1275
01:05:02,610 --> 01:05:05,946
3.42 goes into expected

1276
01:05:05,978 --> 01:05:08,640
free energy. So we see this a lot f

1277
01:05:09,010 --> 01:05:11,086
variational, free energy, that's the

1278
01:05:11,108 --> 01:05:13,954
real time unfolding sensory flow and

1279
01:05:13,992 --> 01:05:17,442
then g expected free energy and the

1280
01:05:17,496 --> 01:05:21,566
policy planning as Inference section

1281
01:05:21,598 --> 01:05:24,862
3.5 concludes with a novel

1282
01:05:24,926 --> 01:05:26,446
foundation active inference to

1283
01:05:26,488 --> 01:05:28,774
understand behavior and cognition. And

1284
01:05:28,812 --> 01:05:31,218
it describes a few features of active

1285
01:05:31,234 --> 01:05:33,826
inference, including its distinguishing

1286
01:05:33,858 --> 01:05:35,846
features from a few other ways that

1287
01:05:35,868 --> 01:05:37,926
people have looked at behavior and

1288
01:05:37,948 --> 01:05:42,234
cybernetics. Section 3.6

1289
01:05:42,352 --> 01:05:44,534
goes into a bit more detail on models,

1290
01:05:44,582 --> 01:05:47,066
policies and trajectories having to do

1291
01:05:47,088 --> 01:05:49,530
with agency and policy selection.

1292
01:05:49,870 --> 01:05:53,210
3.7 reconciliation of inactive

1293
01:05:53,290 --> 01:05:55,662
cybernetic and predictive theories under

1294
01:05:55,716 --> 01:05:58,286
active inference. Again, the exact kind

1295
01:05:58,308 --> 01:06:00,010
of thing that there's a whole literature

1296
01:06:00,090 --> 01:06:01,902
on and it's always amazing to hear

1297
01:06:01,956 --> 01:06:03,806
everybody's perspective on in the

1298
01:06:03,828 --> 01:06:07,246
textbook groups. Three eight active

1299
01:06:07,278 --> 01:06:09,314
inference from the emergence of life to

1300
01:06:09,352 --> 01:06:13,186
agency and three nine summary. You want

1301
01:06:13,208 --> 01:06:14,946
to just give any other thoughts that you

1302
01:06:14,968 --> 01:06:16,420
have on chapter three.

1303
01:06:19,530 --> 01:06:23,800
Again, it's one of the probably most

1304
01:06:24,330 --> 01:06:30,346
fundamental chapters and the

1305
01:06:30,368 --> 01:06:32,780
topics covered in this chapter is

1306
01:06:33,550 --> 01:06:37,466
essential, are absolutely essential to

1307
01:06:37,568 --> 01:06:39,446
understand everything active inference

1308
01:06:39,478 --> 01:06:42,640
related. But specifically the

1309
01:06:43,330 --> 01:06:48,510
discussions in section 3.73.6

1310
01:06:48,580 --> 01:06:52,350
onward again, I believe is

1311
01:06:52,420 --> 01:06:54,500
really important to understand the

1312
01:06:55,110 --> 01:06:57,394
broader context of active inference and

1313
01:06:57,432 --> 01:07:00,962
how it relates to all the other

1314
01:07:01,096 --> 01:07:05,300
theories. But specifically, section 3.8

1315
01:07:06,330 --> 01:07:09,670
provides a nice view of

1316
01:07:09,740 --> 01:07:14,566
how we can use the

1317
01:07:14,588 --> 01:07:18,200
same theoretical tools to model both

1318
01:07:19,530 --> 01:07:22,226
nonliving dynamical systems or sparse

1319
01:07:22,258 --> 01:07:26,026
coupled stochastic systems and also

1320
01:07:26,208 --> 01:07:29,914
to the systems that

1321
01:07:30,032 --> 01:07:33,566
has agency or sentience. So it's a

1322
01:07:33,588 --> 01:07:36,462
much broader theoretical framework that

1323
01:07:36,516 --> 01:07:39,646
doesn't restrict itself to only

1324
01:07:39,828 --> 01:07:43,120
particular kinds of systems or

1325
01:07:43,750 --> 01:07:47,154
agents and we'll see much

1326
01:07:47,192 --> 01:07:50,020
more elaboration on that in the later.

1327
01:07:52,070 --> 01:07:56,538
Wonderful. All right, that concludes

1328
01:07:56,734 --> 01:07:58,920
our overview on chapter three.

1329
01:08:00,090 --> 01:08:02,680
Now we're going to skip to chapter six.

1330
01:08:06,820 --> 01:08:09,648
So again, this live stream is just

1331
01:08:09,734 --> 01:08:12,540
providing some materials on chapters

1332
01:08:12,620 --> 01:08:14,396
one, two, three and six. Eventually

1333
01:08:14,428 --> 01:08:15,876
we're going to get to all chapters and

1334
01:08:15,898 --> 01:08:18,484
version them and have as many of you as

1335
01:08:18,522 --> 01:08:20,196
want join into the process of

1336
01:08:20,218 --> 01:08:21,636
constructing these. But this is all so

1337
01:08:21,658 --> 01:08:24,000
that our textbook groups can be really

1338
01:08:24,090 --> 01:08:25,800
interactive and people can show up

1339
01:08:25,870 --> 01:08:27,304
having listened to these background and

1340
01:08:27,342 --> 01:08:30,584
context videos. All right, so now we

1341
01:08:30,622 --> 01:08:33,192
are in chapter six, a recipe for

1342
01:08:33,246 --> 01:08:35,508
designing active inference models.

1343
01:08:35,684 --> 01:08:38,776
Abraham Lincoln give me 6 hours to chop

1344
01:08:38,808 --> 01:08:40,972
down a tree and I will spend the first

1345
01:08:41,026 --> 01:08:43,452
four sharpening the axe. All right,

1346
01:08:43,506 --> 01:08:45,980
Ali, what does the quote mean? And

1347
01:08:46,050 --> 01:08:48,800
please lead us into this discussion.

1348
01:08:52,260 --> 01:08:54,924
Okay, so as the opening quotation

1349
01:08:54,972 --> 01:08:58,732
suggests, it's about honing

1350
01:08:58,796 --> 01:09:03,216
the skill set and skill set for applying

1351
01:09:03,328 --> 01:09:07,972
ActInf inference framework to model the

1352
01:09:08,026 --> 01:09:12,550
actual empirical situations that

1353
01:09:13,740 --> 01:09:18,088
we may use active inference for. And it

1354
01:09:18,174 --> 01:09:22,010
frames it as a four step recipe to

1355
01:09:22,940 --> 01:09:26,030
actually do this kind of model,

1356
01:09:26,560 --> 01:09:30,524
although it's termed as recipe. But I

1357
01:09:30,562 --> 01:09:33,132
believe it's more like a guideline and

1358
01:09:33,186 --> 01:09:35,432
it's not like, I don't know, cooking

1359
01:09:35,496 --> 01:09:39,184
recipe that needs to be

1360
01:09:39,222 --> 01:09:42,268
observed strictly.

1361
01:09:42,444 --> 01:09:46,492
So it's more like suggestions

1362
01:09:46,556 --> 01:09:50,188
or guidelines to begin

1363
01:09:50,294 --> 01:09:53,504
to apply active inference framework

1364
01:09:53,632 --> 01:09:57,380
in any kind of situation we'd like.

1365
01:09:57,530 --> 01:10:02,256
So section 6.2 outlines

1366
01:10:02,368 --> 01:10:05,768
those four steps. So the first step is

1367
01:10:05,854 --> 01:10:09,704
which system are we modeling in

1368
01:10:09,742 --> 01:10:13,876
terms of how we can define

1369
01:10:13,908 --> 01:10:17,516
the Markup blanket that most

1370
01:10:17,618 --> 01:10:21,132
effectively and efficient the problem or

1371
01:10:21,186 --> 01:10:25,580
question we want to explore

1372
01:10:26,000 --> 01:10:29,232
through that modeling through the

1373
01:10:29,366 --> 01:10:33,200
modeling of that situation. So first

1374
01:10:33,350 --> 01:10:36,720
step is about defining those boundaries

1375
01:10:37,780 --> 01:10:40,772
through Markov blanket. And the second

1376
01:10:40,826 --> 01:10:42,468
step is about what's the most

1377
01:10:42,554 --> 01:10:44,064
appropriate form for the generative

1378
01:10:44,112 --> 01:10:46,804
model because obviously we can have many

1379
01:10:46,842 --> 01:10:49,984
different types of generative models

1380
01:10:50,032 --> 01:10:52,984
depending on, again, what phenomena or

1381
01:10:53,022 --> 01:10:56,090
what questions we're trying to address.

1382
01:10:56,860 --> 01:10:59,864
The third stage is how to set up

1383
01:10:59,902 --> 01:11:03,220
Degenerative Model. So after settling

1384
01:11:03,300 --> 01:11:06,956
upon the type of Degenerative Model we

1385
01:11:06,978 --> 01:11:10,620
want to use, we can talk about

1386
01:11:10,690 --> 01:11:13,580
the details of Degenerative Model and

1387
01:11:13,650 --> 01:11:16,476
how it can be efficiently modeled to be

1388
01:11:16,498 --> 01:11:20,572
both tractable and also useful

1389
01:11:20,636 --> 01:11:23,952
to address the situation of interest.

1390
01:11:24,086 --> 01:11:26,016
And finally, how to set up the

1391
01:11:26,038 --> 01:11:28,208
generative process. So, as we saw in

1392
01:11:28,214 --> 01:11:32,790
chapter two, there's important

1393
01:11:33,400 --> 01:11:34,932
but slight distinction between

1394
01:11:34,986 --> 01:11:37,700
generative process and generative model.

1395
01:11:37,850 --> 01:11:41,590
So they're not always the same, but

1396
01:11:42,200 --> 01:11:44,232
they're closely related to each other.

1397
01:11:44,286 --> 01:11:48,008
Generative process, the hidden states

1398
01:11:48,094 --> 01:11:51,284
of the external world. And generative

1399
01:11:51,332 --> 01:11:55,032
model is how the agent infer about those

1400
01:11:55,086 --> 01:11:58,152
hidden states provided by generative

1401
01:11:58,216 --> 01:11:59,790
process. So,

1402
01:12:01,440 --> 01:12:05,320
considering that the environment

1403
01:12:05,400 --> 01:12:09,424
is always very complex to be

1404
01:12:09,462 --> 01:12:12,720
modeled precisely and exactly with

1405
01:12:12,790 --> 01:12:15,712
every parameter accounted for, we need

1406
01:12:15,766 --> 01:12:20,000
to have a restricted set of parameters

1407
01:12:20,680 --> 01:12:22,960
to account for in the generative

1408
01:12:23,040 --> 01:12:28,656
process. So that's what stage four deals

1409
01:12:28,688 --> 01:12:32,150
for deals about. So all of these

1410
01:12:32,620 --> 01:12:37,076
steps, even if not observed

1411
01:12:37,108 --> 01:12:41,290
or followed sequentially, are really

1412
01:12:42,940 --> 01:12:46,540
essential to setting up or

1413
01:12:46,610 --> 01:12:50,156
the initial setup of the situation

1414
01:12:50,258 --> 01:12:54,556
we're trying to model. And I

1415
01:12:54,578 --> 01:12:57,870
cannot imagine how any

1416
01:12:58,420 --> 01:13:02,240
scenario can progress without observing

1417
01:13:04,420 --> 01:13:08,112
any of these steps at least, I mean,

1418
01:13:08,166 --> 01:13:11,668
just by thinking about how to set up

1419
01:13:11,834 --> 01:13:14,660
the situation or scenario.

1420
01:13:15,480 --> 01:13:19,044
Awesome. And in the rest

1421
01:13:19,082 --> 01:13:23,028
of the chapter. Yes, just wanted to

1422
01:13:23,194 --> 01:13:25,156
highlight again, these steps don't have

1423
01:13:25,178 --> 01:13:27,672
to be followed in order. They really are

1424
01:13:27,726 --> 01:13:30,228
like a summary as Mark has just written

1425
01:13:30,244 --> 01:13:32,376
in the live chat here. It really is like

1426
01:13:32,398 --> 01:13:34,216
a summary of the earlier parts of the

1427
01:13:34,238 --> 01:13:36,680
book in terms of things that you want to

1428
01:13:36,750 --> 01:13:39,564
capture in your consideration. And also

1429
01:13:39,602 --> 01:13:41,596
it's a great way to connect a lot of the

1430
01:13:41,618 --> 01:13:43,644
technical ideas that are brought up

1431
01:13:43,682 --> 01:13:45,272
about generative model, generative

1432
01:13:45,336 --> 01:13:47,736
process and so on with some bigger

1433
01:13:47,768 --> 01:13:49,836
questions like who are we? Why are we

1434
01:13:49,858 --> 01:13:51,712
making this model? How is the model

1435
01:13:51,766 --> 01:13:54,128
going to be used? So we take that in a

1436
01:13:54,134 --> 01:13:55,692
lot of different ways in the textbook

1437
01:13:55,756 --> 01:13:58,064
groups. But which system are we

1438
01:13:58,102 --> 01:14:00,396
modeling? What is the most appropriate

1439
01:14:00,428 --> 01:14:03,524
form for the generative model? How

1440
01:14:03,562 --> 01:14:05,380
should we set up the generative model?

1441
01:14:05,530 --> 01:14:07,636
And how to set up the generative process

1442
01:14:07,738 --> 01:14:09,796
are all questions that must be

1443
01:14:09,818 --> 01:14:13,512
addressed, at least when actually coming

1444
01:14:13,566 --> 01:14:16,212
through with carrying out a theoretical

1445
01:14:16,276 --> 01:14:18,436
or empirical active inference modeling

1446
01:14:18,468 --> 01:14:22,104
task. And the following sections are

1447
01:14:22,142 --> 01:14:24,824
going to go into more detail on each of

1448
01:14:24,862 --> 01:14:27,708
those four questions. So Ollie, do you

1449
01:14:27,714 --> 01:14:30,830
want to go for this? 63 and carry on.

1450
01:14:32,960 --> 01:14:37,310
Okay, so section six one three is

1451
01:14:37,860 --> 01:14:40,304
the elaboration of the first step that

1452
01:14:40,342 --> 01:14:41,970
we just saw.

1453
01:14:43,940 --> 01:14:46,160
What systems are we modeling?

1454
01:14:46,660 --> 01:14:50,450
And by this question we

1455
01:14:56,760 --> 01:15:00,772
mean how the system carves up the state

1456
01:15:00,826 --> 01:15:04,028
space using the markup blanket

1457
01:15:04,144 --> 01:15:07,704
or more precisely, the active states and

1458
01:15:07,742 --> 01:15:11,544
sensory states. And I believe it

1459
01:15:11,582 --> 01:15:14,936
is both one

1460
01:15:14,958 --> 01:15:19,196
of the most fundamental and also one

1461
01:15:19,218 --> 01:15:22,524
of the most challenging steps to be

1462
01:15:22,562 --> 01:15:25,976
carried out because, well, markup

1463
01:15:26,008 --> 01:15:29,776
blanket is just a conceptual tool we use

1464
01:15:29,878 --> 01:15:33,648
to frame the natural phenomena or

1465
01:15:33,734 --> 01:15:36,880
any other scenario we want to model. So

1466
01:15:37,030 --> 01:15:40,288
that scenario or that model obviously

1467
01:15:40,454 --> 01:15:42,976
doesn't care about how we set up our

1468
01:15:42,998 --> 01:15:45,104
Markov blanket or how we knit our markup

1469
01:15:45,152 --> 01:15:48,912
blanket. But deciding

1470
01:15:48,976 --> 01:15:51,876
on a proper set of active states and

1471
01:15:51,898 --> 01:15:55,690
sensory states that would allow us to

1472
01:15:56,380 --> 01:15:59,860
address the problems we need to explore

1473
01:15:59,940 --> 01:16:03,112
or the questions we need to answer, is

1474
01:16:03,166 --> 01:16:06,236
not a trivial task. And it needs lots of

1475
01:16:06,258 --> 01:16:07,820
lots of consideration and even

1476
01:16:07,890 --> 01:16:11,870
creativity on part of the researcher or

1477
01:16:12,640 --> 01:16:17,516
anyone who wants to design an

1478
01:16:17,538 --> 01:16:19,456
active inference based model for the

1479
01:16:19,478 --> 01:16:24,304
problem. So that's where the

1480
01:16:24,342 --> 01:16:28,192
significance of doing these

1481
01:16:28,246 --> 01:16:32,256
kinds of modeling with the insights

1482
01:16:32,288 --> 01:16:36,420
and creativity of a researcher

1483
01:16:39,000 --> 01:16:41,684
will be obvious because it's not just

1484
01:16:41,882 --> 01:16:45,876
something mechanical or despite

1485
01:16:45,908 --> 01:16:49,880
the fact that the word recipe

1486
01:16:51,660 --> 01:16:54,708
can somehow make us believe that it's

1487
01:16:54,724 --> 01:16:58,284
just a recipe to follow. But in

1488
01:16:58,322 --> 01:17:01,550
actuality it's not the case at all. So

1489
01:17:02,080 --> 01:17:06,012
that's the

1490
01:17:06,066 --> 01:17:09,330
discussion of markup blanket and what

1491
01:17:09,860 --> 01:17:12,560
criteria we need to keep in mind to

1492
01:17:12,630 --> 01:17:17,152
settle upon proper

1493
01:17:17,206 --> 01:17:19,936
Markov blanket around our system. And

1494
01:17:19,958 --> 01:17:22,692
the next section, what is the most

1495
01:17:22,746 --> 01:17:25,072
appropriate form for the generative

1496
01:17:25,136 --> 01:17:28,116
model? So here again,

1497
01:17:28,218 --> 01:17:31,348
we can have various way

1498
01:17:31,434 --> 01:17:34,504
to set up our generative model in terms

1499
01:17:34,542 --> 01:17:37,572
of whether it's discrete or continuous

1500
01:17:37,636 --> 01:17:41,032
variables we want to account for about

1501
01:17:41,086 --> 01:17:43,064
the timescale, is it shallow or

1502
01:17:43,102 --> 01:17:47,212
hierarchical about

1503
01:17:47,266 --> 01:17:49,724
the temporal depth of the inference and

1504
01:17:49,762 --> 01:17:52,316
planning. So these are all problems we

1505
01:17:52,338 --> 01:17:55,420
need to be precise about

1506
01:17:55,490 --> 01:17:57,800
or questions we need to precise about

1507
01:17:57,970 --> 01:18:00,384
whenever we want to set up our

1508
01:18:00,422 --> 01:18:03,936
generative model. And then we

1509
01:18:03,958 --> 01:18:07,410
go to section 6.5,

1510
01:18:09,620 --> 01:18:12,564
how to set up the generative model. So

1511
01:18:12,682 --> 01:18:16,180
after precisely describing what

1512
01:18:16,250 --> 01:18:19,380
needs to be accounted or what parameters

1513
01:18:19,720 --> 01:18:23,750
and the nature of the generative model

1514
01:18:24,120 --> 01:18:26,010
needs to be accounted for,

1515
01:18:27,820 --> 01:18:31,370
then we need to

1516
01:18:32,060 --> 01:18:35,532
address this question of how can that

1517
01:18:35,586 --> 01:18:39,310
generative model be actually set up. So

1518
01:18:39,920 --> 01:18:42,684
setting up the variables for the

1519
01:18:42,722 --> 01:18:48,576
generative model will

1520
01:18:48,598 --> 01:18:52,368
be concerned about how those matrices as

1521
01:18:52,454 --> 01:18:56,016
A-B-C and D matrices can

1522
01:18:56,038 --> 01:18:59,072
be defined, as we saw

1523
01:18:59,126 --> 01:19:02,528
in chapter four. And then which parts

1524
01:19:02,544 --> 01:19:04,484
of the generator model are fixed and

1525
01:19:04,522 --> 01:19:07,684
what is learned. So again, it's really

1526
01:19:07,722 --> 01:19:11,392
important to know what are the dynamics

1527
01:19:11,456 --> 01:19:14,650
of those parameters are in terms of

1528
01:19:16,220 --> 01:19:19,912
the learning process and what needs

1529
01:19:19,966 --> 01:19:23,588
to be updated consistently or learned

1530
01:19:23,684 --> 01:19:28,364
as opposed to the variables that either

1531
01:19:28,562 --> 01:19:32,604
don't require consistent updating or

1532
01:19:32,802 --> 01:19:36,344
they vary quite slowly

1533
01:19:36,392 --> 01:19:38,800
as compared to the timescale of the

1534
01:19:38,950 --> 01:19:42,864
generative model. And then we

1535
01:19:42,902 --> 01:19:46,604
go through the process of setting

1536
01:19:46,652 --> 01:19:50,884
up the generative process. And as

1537
01:19:50,922 --> 01:19:54,436
I mentioned earlier, it is probably one

1538
01:19:54,458 --> 01:19:58,100
of the most complicated steps of

1539
01:19:58,170 --> 01:20:01,460
the recipe because of the complexity,

1540
01:20:01,800 --> 01:20:05,320
the enormous complexity of any real time

1541
01:20:05,390 --> 01:20:08,776
situation and particularly the

1542
01:20:08,878 --> 01:20:13,480
environment that the agent will

1543
01:20:13,550 --> 01:20:21,932
act and behave. But even

1544
01:20:21,986 --> 01:20:26,392
if there is not any strict

1545
01:20:26,456 --> 01:20:30,130
or, I don't know, obvious way to go

1546
01:20:30,740 --> 01:20:33,296
through this stage of setting up the

1547
01:20:33,318 --> 01:20:35,532
generative process, this section

1548
01:20:35,596 --> 01:20:39,372
provides some helpful criteria

1549
01:20:39,436 --> 01:20:44,516
or helpful points in order to help us in

1550
01:20:44,538 --> 01:20:48,230
that stage as well. So finally,

1551
01:20:49,000 --> 01:20:51,488
after going through all of these stages,

1552
01:20:51,664 --> 01:20:55,016
we need to somehow get I

1553
01:20:55,038 --> 01:20:57,256
mean, obviously we need results, and we

1554
01:20:57,278 --> 01:21:01,528
need to provide our results in a

1555
01:21:01,694 --> 01:21:04,436
we get our results from our active

1556
01:21:04,468 --> 01:21:08,232
inference model, and section

1557
01:21:08,296 --> 01:21:14,204
6.7 addresses that

1558
01:21:14,242 --> 01:21:17,272
very idea of how to simulate,

1559
01:21:17,336 --> 01:21:20,552
visualize, analyze, and fit data using

1560
01:21:20,626 --> 01:21:22,752
active inference. Because it's not just

1561
01:21:22,806 --> 01:21:25,776
enough to set up the model, we need to

1562
01:21:25,798 --> 01:21:28,736
get the model to work. So how can we

1563
01:21:28,838 --> 01:21:31,510
feed the data into the model? How can we

1564
01:21:32,920 --> 01:21:35,910
read the data? And finally, how can we

1565
01:21:37,640 --> 01:21:41,590
analyze the simulated data through the

1566
01:21:42,280 --> 01:21:45,624
active inference? So this

1567
01:21:45,662 --> 01:21:48,730
is in a way

1568
01:21:49,260 --> 01:21:52,344
a kind of preliminary recipe or

1569
01:21:52,382 --> 01:21:56,136
preliminary procedure for doing active

1570
01:21:56,168 --> 01:21:58,536
inference or active inference modeling,

1571
01:21:58,568 --> 01:22:01,772
or at least suggested way to do those

1572
01:22:01,826 --> 01:22:04,456
kinds of modeling. But in the subsequent

1573
01:22:04,568 --> 01:22:07,404
chapters, in particular chapters seven

1574
01:22:07,442 --> 01:22:11,244
and eight, we'll go through some helpful

1575
01:22:11,372 --> 01:22:14,560
cases, studies for how to actually

1576
01:22:14,630 --> 01:22:18,000
use these procedures to model

1577
01:22:18,070 --> 01:22:20,400
some real world situations.

1578
01:22:22,440 --> 01:22:24,676
Awesome, thanks. Well, there's a lot to

1579
01:22:24,698 --> 01:22:28,052
say on chapter six. One piece

1580
01:22:28,106 --> 01:22:30,964
that comes up a lot is what is in the

1581
01:22:31,002 --> 01:22:33,596
active inference kernel, the cognitive

1582
01:22:33,648 --> 01:22:36,008
kernel, and what are situations that we

1583
01:22:36,014 --> 01:22:38,116
can model with active inference.

1584
01:22:38,228 --> 01:22:40,372
There's a huge variety of cognitive

1585
01:22:40,436 --> 01:22:42,856
phenomena and statistical outcomes that

1586
01:22:42,878 --> 01:22:45,210
we might be interested to study like

1587
01:22:45,840 --> 01:22:48,396
counterfactuals and planning, like

1588
01:22:48,498 --> 01:22:51,240
multiscale attention covert action,

1589
01:22:51,400 --> 01:22:54,008
like the complex ways in which memories

1590
01:22:54,104 --> 01:22:57,356
might influence decision making in the

1591
01:22:57,378 --> 01:22:59,584
moment based upon associations made

1592
01:22:59,622 --> 01:23:01,548
between different sensory modalities,

1593
01:23:01,724 --> 01:23:04,656
every amazing real world situation you

1594
01:23:04,678 --> 01:23:07,616
can imagine. And it's really helpful to

1595
01:23:07,638 --> 01:23:09,552
think about active inference as more

1596
01:23:09,606 --> 01:23:13,568
like a framework of interoperable

1597
01:23:13,664 --> 01:23:17,316
motifs that we can compose and

1598
01:23:17,418 --> 01:23:20,692
indeed even be creative with rather than

1599
01:23:20,746 --> 01:23:22,876
giving us all the answers at the core

1600
01:23:22,928 --> 01:23:25,844
model. Because the kind of essential

1601
01:23:25,972 --> 01:23:28,724
minimal active inference model doesn't

1602
01:23:28,772 --> 01:23:31,608
necessarily include every single

1603
01:23:31,694 --> 01:23:33,288
attribute that you might be interested

1604
01:23:33,374 --> 01:23:35,764
in, just like any given linear

1605
01:23:35,812 --> 01:23:38,012
regression isn't going to include every

1606
01:23:38,066 --> 01:23:39,756
single feature that you're looking for.

1607
01:23:39,858 --> 01:23:42,364
This is happening in a different type of

1608
01:23:42,402 --> 01:23:44,588
modeling framework. But still, there's a

1609
01:23:44,594 --> 01:23:46,984
lot of customization and development

1610
01:23:47,032 --> 01:23:49,772
that goes into adapting or elaborating

1611
01:23:49,836 --> 01:23:52,144
an active inference type model for your

1612
01:23:52,182 --> 01:23:54,850
given system or scenario of interest.

1613
01:23:55,860 --> 01:23:58,880
Chapter six gives us some helpful

1614
01:24:00,120 --> 01:24:02,420
guidelines and things to consider,

1615
01:24:02,490 --> 01:24:04,948
which we've unpacked and expanded on

1616
01:24:05,034 --> 01:24:07,252
elsewhere. And in the textbook group,

1617
01:24:07,306 --> 01:24:09,872
we work together to characterize

1618
01:24:10,016 --> 01:24:13,092
different systems of interest, to map

1619
01:24:13,156 --> 01:24:15,156
different parts or different observables

1620
01:24:15,188 --> 01:24:17,108
from that system, using the active

1621
01:24:17,124 --> 01:24:20,436
inference ontology into a unified ActInf

1622
01:24:20,468 --> 01:24:23,272
inference type model. So chapter six

1623
01:24:23,406 --> 01:24:25,704
starts the second half of the book,

1624
01:24:25,822 --> 01:24:27,816
which is the more practice oriented part

1625
01:24:27,838 --> 01:24:29,692
of the book. And the rest of the part

1626
01:24:29,746 --> 01:24:31,836
two is going to be a lot more about the

1627
01:24:31,858 --> 01:24:33,836
specific kinds of generative models that

1628
01:24:33,858 --> 01:24:36,172
you can use discrete and continuous time

1629
01:24:36,306 --> 01:24:39,052
and about using them with empirical data

1630
01:24:39,106 --> 01:24:42,224
in chapter nine. But chapter six just

1631
01:24:42,262 --> 01:24:44,896
stands alone as a summary of part one of

1632
01:24:44,918 --> 01:24:47,792
the book in some ways, and a set of four

1633
01:24:47,846 --> 01:24:50,284
questions which system are we modeling?

1634
01:24:50,412 --> 01:24:52,036
What is the most appropriate form for

1635
01:24:52,058 --> 01:24:53,956
the generative model, how to set up the

1636
01:24:53,978 --> 01:24:55,588
generative model, and how to set up the

1637
01:24:55,594 --> 01:24:58,612
generative process so that we can be

1638
01:24:58,666 --> 01:25:00,820
active inference modelers ourselves?

1639
01:25:06,120 --> 01:25:09,512
All right, well, Ali, thanks so much

1640
01:25:09,566 --> 01:25:12,504
for all of these great conversations and

1641
01:25:12,542 --> 01:25:15,964
cohorts. We will be able to process this

1642
01:25:16,002 --> 01:25:20,168
video to clip out these first versions

1643
01:25:20,344 --> 01:25:23,112
of the background and context overviews,

1644
01:25:23,256 --> 01:25:25,624
and we'll distribute them to the cohorts

1645
01:25:25,672 --> 01:25:28,300
so that people can show up with their

1646
01:25:28,370 --> 01:25:31,196
curiosities and passions and we'll see

1647
01:25:31,218 --> 01:25:33,900
where that goes. Any last thoughts?

1648
01:25:36,560 --> 01:25:39,996
Thank you. It was a pleasure to go

1649
01:25:40,018 --> 01:25:43,076
through these chapters once again with

1650
01:25:43,258 --> 01:25:46,836
each time, obviously with a

1651
01:25:46,858 --> 01:25:49,552
bit more understanding than the previous

1652
01:25:49,616 --> 01:25:52,852
one. So I'm looking forward to the next

1653
01:25:52,906 --> 01:25:56,052
cohorts. Awesome. All right. Thanks,

1654
01:25:56,106 --> 01:25:56,496
Ollie.



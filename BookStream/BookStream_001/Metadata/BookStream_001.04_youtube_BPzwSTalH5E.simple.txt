SPEAKER_00:
hello and welcome everyone it's January 3rd 2023 we're in active book stream number 001.04 on governing continuous transformation off to you blue


SPEAKER_02:
So we are the Active Inference Institute, and this discussion is over governing continuous transformation.

This is a recorded and an archived livestream, so please provide us with feedback so we can improve upon our work.

We are a participatory online institute that is communicating, learning, and practicing applied active inference.

You can reach us on social media at all of these different links here.

All backgrounds and perspectives are welcome to discuss and contribute to this work, and we will be following good video etiquette for live streams.

If you would like to get in touch to participate in a future dot zero live stream or one of the upcoming discussions, please reach out to us via social media or email.

And again, our links.

So today we are going to cover section 1.4.

So we're just going to go kind of over where we are in the book, the keywords, and then the different subjects and things that we've pulled out of this section.

So just to start off, we will maybe just introduce ourselves.

I am Blue.

I am a longtime Institute participant and an independent researcher in New Mexico.

and I'll pass it to Daniel.


SPEAKER_00:
I'm Daniel.

I'm a researcher and institute participant.

I'll pass to Tyler.


SPEAKER_01:
I'm Tyler.

I'm a DAO researcher and practitioner in California.


SPEAKER_02:
Awesome.

Is there any introductory comments or do you guys just want to jump right into it?

Jump in?

Okay, here we go.

So this is the section that covers the free energy principle.

And that's also the title of this chapter.

And I think the first keyword on our keyword slides.

So we pulled out a lot of these keywords.

I mean, we pulled them out.

They weren't highlighted by Bijan or selected by him at all.

And this is just kind of the topics that we thought that would be better to dive deeper into for a more thorough understanding of the material presented in this chapter.

And we will, this is not like a final word.

This is maybe just an introductory perspective and our take on what's maybe happening in this, in this chapter, not a review.

So we're definitely open to correction and updating our model of this model.

All right.

So abstract, Tyler, do you want to read this first section for us?


SPEAKER_01:
Sure.

So the free energy principle is the first-order principle of police action empowering self-organization.

FEP commands the generative process of active inference dedicated to minimizing the information theoretical mathematical difference between top-down predictions and action-generated bottom-up stimuli in pursuit of minimizing error, surprise, and entropy.

The power of Rustonian active inference is fundamentally fourfold.

It is the pure belief-setting in dynamic and non-stationary environments.

The active inference agent carries out epistemic exploration to account for uncertainty by making inferences in Bayes' optimal fashion.

The reward signal is so characteristic that reinforcement learning is removed.

And finally, active inference sets the collaborative human-machine AI potential so integral to the multi-intelligence firm as the mathematical expression of beliefs in the form of probabilities provides the very common denominator to align humans with machines.


SPEAKER_02:
Wow, big if true.

Okay, and the author goes on to say, there are only two distinctions that matter, what is known and unknown, and what is in our control and what is not.

There are four states, active, action, internal, firm resources, sensory, perception, and external, unknown and hidden behind the Markov blanket.

According to free energy governance, free energy principle powered AI, which he uses here this capital A and lowercase I to mean active inference.

So free energy principle powered active inference is the future site of organizational becoming.

So we can unpack that, I guess, starting with this claim, which was in the abstract itself.

So

Maybe we don't need to read it.

Or was it exactly stated there?

Maybe not.

Oh, yeah, fourfold.

So the four claims, we'll just start off this first one.

The power of Fristonian actance is fundamentally fourfold.

It is a pure belief based setting in dynamic and non stationary environments.

So

What does that mean?

What is Fristonian active inference?

So maybe we want to just step back a little bit and talk about the FEP.

And the definition or explanation that Bijan picked out for this chapter is the free energy principle stems from the idea that living systems can be distinguished from other self-organizing systems because they actively avoid deleterious phase transitions

by bounding the entropy of their sensory and physical states.

Under the FEP, to be alive simply means to revisit a bounded set of states with a high probability.

And that's from this 2019 Badcock et al paper.

And this image is from Kirchhoff et al, 2018.

And I always like this when I think about life and self-organizing systems.

it kind of this picture just really like speaks to what it is to kind of be self-organizing like a hierarchical um scale so and this like revisiting a bounded set of states with a high probability kind of speaks to the non-equilibrium steady state density that um is like a constraint of life we kind of have to live within you know like a certain temperature

range, a certain, you know, glucose range, and all life has like an optimal range of states that they stay in.

But like, if your fever gets too high, like if your temperature goes too high or too low, like you will dissipate.

So you revisit this bounded set of states with a high probability, and that's kind of your non equilibrium steady state density.

Do you guys have any comments here on the FPP and self organizing maybe?


SPEAKER_00:
I'll give one comment.

This is a via positiva.

It's describing what the FEP is, but we could look back to those four claims and think about what this is not, the via negativa.

So rather than being belief-based in dynamic settings and adaptive autonomous agents, it might be a framework oriented around some sort of static or perennial understanding of the world.

The second point, rather than being centered around epistemic exploration and the value of reducing uncertainty and gaining information, hearing new perspectives, it could be oriented around pragmatic or utility value, which ties very closely to the third point, which is that

A framework contrary to active inference might center a reward function that maps different states of the agent or the organization or the world to some sort of reward function, which is the heart of reward learning.

And then the fourth point also kind of ties in that rather than thinking about ecosystem of shared intelligence, intermediated by composable frameworks like active inference, we might be looking for like the one framework to simply rule them all, and therefore not think about the human machine or human human relationships as collaborative and emergent, but rather just be looking for the one computer, we're just going to turn it on, the business is going to run.


SPEAKER_02:
Awesome.

So getting into active inference a little bit more.

So we're kind of unpacking here.

We're going to unpack slowly these like four claims.

And so for Estonian active inference or FEP powered active.

So in active inference, there are four states.

This is what Bijan claims.

Active, action, internal, firm resources, sensory, perception, and external, which is unknown and hidden behind the markup blanket.

And Bijan quotes Frist in here and says, in active inference, the agent makes choices based on its belief about states of the world and not based on the value of the states.

And so that speaks to what Daniel was maybe just mentioning about reinforcement learning.

And we'll start to unravel reinforcement learning a little bit more.

But where reinforcement learning is really kind of driven by that pragmatic value function,

active inference is driven by your beliefs about the world.

So not how much value can you gain out of moving left or moving right.

But like, I fundamentally believe that I should move right because that's my policy that I've, you know, that I operate on.

So, um, yeah.

All right.

And, um, in this, where, where these claims start to be unpacked in the paper,

Bijan says, one could argue that nearly all physical sciences can be reduced to a metrology in service of confirming a variant of the principle of least action.

This is important because

it means that physics does not offer any ground truth.

It is just a search for measurements that endorse an appropriately formulated prediction based upon a variational principle.

And this quotes a first in 2019 paper.

I don't know that paper off the top of my head, but as funny as I said, Bijan says, and then I started reading it, I'm like, no, Carl said this.

It's like a very, this sounds very much like something that would come out of a Carl paper.

Okay.

And so what is this principle of least action?

We've talked about this a lot before in many different live streams, but I looked it up just for like, what is like a really easy, simple way that I could explain it in a minute or less.

And I found this definition on Scholarpedia that says, a true dynamical trajectory of a system between an initial and final configuration in a specified time

is found by imagining all possible trajectories that the system could conceivably take, computing the action, which is a functional of the trajectory.

And just as a reminder, a functional is like a function of a function.

So computing the action, a functional of the trajectory for each of these trajectories and selecting one that makes the action locally stationary, traditionally called least.

True trajectories are those that have the least action.

And so this is like the laziness, the lazy society.

Everybody makes the lazy choice, right?

So like the true trajectory is the one where you kind of just stay put.

Daniel, do you have a comment here?


SPEAKER_00:
Yes, although on the previous slide we looked at the particular partition where action states are the outgoing dependencies.

And here we see action again with a principle of least action.

But importantly, to abide by a principle of least action isn't simply to take the lowest energy action or the least movement.

It's more akin to a conservation law

like the total energy is conserved of potential plus kinetic when the ball falls off the platform so that means that there's a path of least action with the ball dropping now that is not the path of least movement but it's a path of least action given an initial setup because the total energy is conserved and so we're interested in paths of least action

over the particular partitioned states internal external and blanket states and so it's a little bit like um not just simply confusing but least action can include the selection of very energetic or risky behaviors


SPEAKER_02:
That's cool.

So, and just to kind of like maybe tie that to a real world example, I think about like the principle of least action.

Like I prefer my body temperature to be like 75 degrees all the time, right?

Like do I live in the tropics?

No, because there's like a lot of things about living on a tropical island that are like not optimal for me.

So like I live in New Mexico cause that's like my optimal like place.

but I wear a jacket in the winter and like in the summertime, I, you know, wear less clothes or like drive around in my air conditioned car and live in my air conditioned house.

So even though like I'm still expending energy to maintain my physical temperature, like that is the path of least like action for me.

Like I, I, I compensate for the change in my equilibrium state through my actions.

Okay.

And we will,

talk more about energy in a minute.

So unpacking this second part of the claim, the active agent carries out epistemic exploration to account for uncertainty by making inferences in Bayes optimal fashion.

Okay.

So

like, I know what Bayes optimal is, or like, I think I know what Bayes optimal is, but just to kind of like, what is Bayes optimal fashion and how do you make inference in Bayes optimal fashion?

And it's cool.

Cause like in a quick Google search, like I always like to search these fundamental like things just to see kind of what pops up.

Like, I think I know what Bayes optimal is.

And I pulled out this 2019 paper, like

what is optimal in optimal inference?

And it's interesting because like going into Bayes optimal, I mean, I think about Bayes optimal as like minimizing prediction error, or it says here classically optimal Bayesian inference balances prior knowledge and ongoing observations to identify the model with maximum posterior probability.

And so these authors actually take this a step further and I really enjoyed their,

But like thought process here, so they said concretely, we might say that an inference procedures optimal if it maximizes benefit per unit cost in which the benefit is some monotonically increasing function of accuracy.

But they propose the generalization of this approach that that kind of splits this cost benefit curve into two components.

where the first component describes the benefit, perhaps the reward obtained as a function of accuracy.

But then the second component, so that's the minimizing prediction error that I was speaking to earlier, right?

Like in terms of accuracy.

And then the second component describes accuracy as a function of costs, which can include time,

memory and computational resources needed to process information.

And I think that this is like a fundamental thing that, I mean, I might overlook, like I might be able to get like a way more accurate answer.

Like, you know, just so say, for example, like what is the number pi?

And I could just say like 3.14, I could actually go through, I could say like 3.14159, like off the top of my head, like that's like the whatever number of digits of pi that I have memorized to like some accurate point.

But like I could come to a far more accurate answer of that if I sat there with a pencil and a piece of paper for the entire rest of my life doing long division.

So I could come to some infinite number of units past the decimal point.

And I could go on and on and on forever.

But is it worth it?

Is it worth it in terms of memory, effort to calculate that increasing accuracy?

Or is 3.14 good enough?

And so that is kind of the thing that they're... The functions that they're describing here, like this benefit, this two...

Dual component.

Yes, we want accuracy, but we also want it to be fast.

We want to get the maximum benefit for the minimum resource expenditure.

I like that they are describing accuracy as a function of the costs, the effort that goes into it.

Do you guys have any comments here?

Okay.

Okay, so why am I going into this?

Okay, so Tyler and I got into this long talk about energy.

And we got into, so I can't remember, I guess it's not maybe in this claim, but it was somewhere in the paper.

Maybe I should look and try to pull it up.

But we really got into discussing energy, like what is free energy?

What is like kinetic and potential energy?

And what is free energy like?

in FEP and we've gone to many live streams that, that like dive way deeper into this.

Um, and I think it's live stream 17.

That is the one I pointed you to Tyler that, that is, um,

Alex Kiefer does a great unpacking, unraveling of free energy in that way.

But I think it's important here to point out, maybe Tyler, you can pull that quote out of the paper while I'm talking about this, but I think it's important to point out that information theoretic entropy is different than thermodynamic entropy.

And so in information theory,

And there are many times in the paper where, like, the author specifically says, like, we're talking about information theoretic entropy.

But then it seems like that the other kind of entropy is hinted at.

And so I kind of wanted to just, like, draw a line between, like, information theoretic entropy and the other kind of entropy, like the second law of thermodynamics entropy, thermodynamic entropy.

And these two are distinct but not entirely unrelated.

So information theoretic entropy is defined as if you have a function x or some event x, the negative log of the probability of x is equal to the information.

So here you can see the probability versus the information is this graph on the left.

So where there's a high probability of an event, there's low information.

There's a high probability that I will

take a breath in my in the next 30 seconds so like me taking a breath in the next 30 seconds like doesn't contain any information but like the you guys can predict that i will talk about

the FEP and free energy governance in the next minute.

If I start talking about like my dead dog, there's like low probability of that like happening.

And so there's like maybe a much higher level of information, like, whoa, where did this come from?

This is way out of left field.

So there's more information in like an unexpected claim.

And so you can see like a probability distribution versus entropy that that relates like the Shannon entropy where the entropy, like if it's, if there's,

high probability it's low entropy right but if they're like so so i look at this in a coin flip so if we know that the coin is always going to come out heads there's not any entropy in that like say it's a two-sided head coin so like it's always going to come out heads there's no other option the coin always flips heads but where in a fair coin it's 50 50 right so like at the um

right end of the second graph, it's a probability of 0.5, 0.5, like half of probability that it's heads and half of a probability that it's tails.

This has the highest entropy because it's random, like we don't know what's going to come out.

And so that is what is meant here by information theoretic entropy.

In contrast to thermodynamic entropy, where that is like what we think about when we think about physics and the second law of thermodynamics,

Entropy, we think entropy in the universe is always increasing.

There's a lot of disorder, the disorder in the universe.

If you clean your house and you go away for 20 years and you come back and your house is a mess, you're not going to be surprised at all.

But if your house is a mess...

and you go away for 20 years, and you come back, and your house is perfectly spotless, that would be weird, right?

That's like you're scheduled for someone to come in and clean it.

But the disorder in the universe is always increasing.

And that is what is meant by entropy in terms of the second law.

And we also think about this kind of entropy as energy.

This is where Tyler and I got into talking about energy.

But entropy is the energy that is not available to do work, right?

So in...

In a system, there's some amount of energy.

I consume so many calories in a day, and this is very simplified, but of those calories, say I consume 2,000 calories in a day, I can use only 1,500 of them for work because some energy is always lost in heat, in the conversion, in any energy conversion process.

Converting energy to work, you will always lose some energy as heat.

And so that is kind of another way to think about this thermodynamic entropy.

Tyler, were you able to find that quote where we talked about this or no?


SPEAKER_01:
Yeah, I was.

The quote is fairly subtle in that he goes right between information theory and thermodynamic theory kind of seamlessly.

And he doesn't make it clear that these are like two very different things.

different things that there's a potential tenuous connection between so he says fep hence has no purpose of its own it powers the generally the general the process of active inactive inference is dedicated to minimizing the information theoretical maximal difference between the top-down predictions and action generated bottom-up stimuli in pursuit of minimizing errors of present entropy

variational free energy and thermodynamic free energy are inversely related if variational free energy is high then thermodynamic entropy will be high and thermodynamic free energy the energy available for doing productive work will be low so i think what he did there was like subtle he like just goes between informational and thermodynamic energy kind of interchangeably


SPEAKER_00:
and they were like wait a minute there's like actually a difference between these two concepts and the connection between them is something that you know actually the community doesn't have a great handle on at this point very cool uh daniel do you have any comments there on that it's a pretty complex and developing area how the informational entropy is related to the actual physics and biology it's kind of like the connection between

Friston free energy and Gibbs free energy.

How does reducing divergences on informational spaces translate to the amount of available energy?

And there's situations where survival could be secured one way or the other.

There just isn't at this point a general answer to how they're connected.


SPEAKER_02:
So when Tyler asked me how they're connected, what is the relationship?

My answer is Maxwell's demon is the connection really between thermodynamic and informational free energy.

And there's actually like, I think a 2018 or 2019 paper

that quantifies like this relationship between thermodynamic and informational entropy at the quantum level.

So I don't know if that applies to all scales, but it's at least cool that, and I don't remember the paper offhand, but if you search it, Maxwell's demon quantum information, you'll find it.

And it's pretty recent.

So it's neat that people are thinking about like concrete and direct ways

to transform and translate information theoretic and thermodynamic entropy and so we just kind of wanted to unpack that because in that claim it was like whoa there's a lot of a lot of things happening here that it might be more useful for the audience to have a deeper understanding of like what is known and what is unknown and how how deep does this rabbit hole really go and right so so it gets pretty uh pretty hairy

Okay, cool, all right, so back to reality, but back to things that we can concretely talk about.

So the reward signal so characteristic of reinforcement learning is removed and replaced with one sole purpose, surprise minimization.

Effectively, the agent is empowered to minimize surprise by way of a generative model of the partially observable world, only perceiving itself and the world via outcomes.

And so like just for people who might not be familiar with reinforcement learning,

Hold on just a second, sorry.

am getting over getting over sickness sometimes i didn't want to cough it to the microphone all right um so in reinforcement learning there's like a couple required components so you have a policy which maps the state of the environment to an action um and that's like for an agent right so this we're always talking about agentive systems here so an agent has a policy like if the environment is in this state i will take this action and policies can change and update

And then there's a reward signal, which is given at like each step.

So we're talking about like a time step type of thing.

Second one, I take this action.

My reward is two carrots, right?

So say my policy is to turn right unless I see immediate danger on my left.

So I turn right and I turn right and I get two carrots.

I turn left and like I can't turn left because there's immediate danger over there.

It's against my policy, whatever, right?

So the reward signal is the number of carrots that I receive right there at that first step.

And then there's a value function, which actually tracks the reward over time.

And the objective in reinforcement learning is not to maximize the reward signal at each instance, but it's to maximize the value function over time.

And so in a circumstance like where I turn right to get two carrots,

But I could have gone down and then right and gotten 50 carats.

So maybe I should have gone down and right.

And so this is kind of how an agent will learn in a reinforcement learning environment to maximize the value as opposed to just maximizing incrementally the reward.

So after every action that the agent takes, the environment sends a single number.

That's the reward.

The reward that the agent receives depends on the agent's action and the state.

And so the agent's only goal is to maximize the total reward, which is the value function over the long term.

Any comments here?

All right.

So precision.

Tyler, do you want to read the quote at the top of this?


SPEAKER_01:
Sure, so the Bayesian brain has optimized not just its expectations, it also has to optimize precision.

This means that one has to predict how much precision is afforded to various sources of sensory evidence relative to prior beliefs.


SPEAKER_02:
Thank you.

And so this is like a definition from Wikipedia talking about like classifier systems and precision and recall in a classifier system, which is like a predictive system or it can be Bayesian classification

So there are retrieved items.

Like, so we're going to say, you know, of this field of images, the ones in the circle are cats, right?

So we're identifying these circles, the data points within the larger circle as all these things that the classifier says, these are all cats.

The precision is of all of the things that the classifier says are cats, how many are actually cats?

So it's the true positives that are in, um,

in the system and then the recall is like how many total how many relevant items are returned so that's that's the the precision here um that's referenced by first um do you want to read this one too for us tyler


SPEAKER_01:
Sure alright, so when dynamics change salience should be prevented from potentially increasing free energy in the context of corporate governance, one may think of time recognition as a one form of salience.

The more senior and experienced board members are the more likely, and they feel comfortable with mental shortcuts so just pattern recognition built through prior experience.


SPEAKER_02:
And when I searched for salience, I just randomly searched for pictures of salience to make this pretty red apple picture appear here.

But when I searched salience and I looked at images, a lot of them were like,

stakeholder salience and so I like couldn't even start to try to unpack this like it was some corporate definition of salience and some corporate model of salience that I don't know if um Bijan is referring here to more indirectly or Tyler if you know anything about like stakeholder the stakeholder salience model but maybe we could ask Bijan if he um

is referencing that or if he's just talking about salience the way that we talk about it as in like things that we attune our attention to because they're relevant for the task at hand or like things that stick out or things that are memorable or relevant right yeah i don't think there's like a corporate difference like technical definition from it i think it's just like you have limited attention there are things that the board was more likely to focus on and they have to constrain their attention on limited information

Yeah, but I was intrigued by the fact that there is like a corporate salience model.

Like, oh, is Bijan talking about corporate salience?

What's that?

Because it's just something that I've never heard of before.


SPEAKER_00:
Well, it's something we've seen with many active inference ontology terms, that there's a broad, everyday, qualitative sense, like salience is something that you pay attention to.

And then there's also a more technical, usually Bayesian sense, such as salience is actually referring to the precision in terms of a variance estimator that we place on incoming sensory data.

Whereas low attention means that data points coming in are not salient.

They're not used to update the model.

High attention is...

coming together with high attention to incoming data that does update the prior a lot into the posterior.

And so which patterns are salient is itself learnt.

And we can think about that as the regime of attention with all the strengths and weaknesses that expertise and heuristics bring along with.


SPEAKER_02:
Yeah, and I think,

They go on to say, I think Bichon impacts that more in the book about salience and pattern recognition.

OK, so single and double loop learning.

Tyler, do you want to take this?


SPEAKER_01:
Single-loop learning occurs when errors are detected and corrected without altering the governing values of the master program.

Double-loop learning occurs when, nor the correct and error is necessary to alter the governing values of the master programs.

Most governance models are at best single-loop, rule-based learning systems, but struggle to activate double-loop learning.


SPEAKER_02:
Thank you.

And I found this little, like,

loop this loop diagram which is like you know actions and consequences actions and consequences and that's like the single loop but um this double loop learning goes into understanding governing very variables and like this is kind of like i think about like meta some kind of meta analysis um where you can you know it's zoomed out more in the double loop learning


SPEAKER_00:
yeah with our favorite example of the temperature in the room if the single loop is just the knob that we're controlling on the heater or the air conditioning the double loop would be taking a step back and thinking well could we also open or close a window and so if we only stay within the single loop maybe our heater or cooler has limited capacity and so there's times where the environmental challenges

are going to take us outside of the range of what can be learned or accommodated with a single loop and it's those times especially when opening up into a double loop space or like a hyper prior space is what is actually required to get the generative model operating in a way that's survivable


SPEAKER_02:
So like I would never open the window with the heater on, but boy, like I changed the weather stripping on my back door this year and it made such a huge difference.

So that's like a more realistic, like a governing variable, like you have a leaky house or poor insulation or something like that.

All right.

Deliberate ignorance.

So that is choosing not to know.

So Bijan talks about this and says,

The more senior and experienced board members are, the more likely that they feel comfortable with mental shortcuts, such as pattern recognition built through prior experience.

That's what I was gonna say earlier about this.

Again, I thought it was this next slide, but when they talked about here pattern recognition as one form of salience, but in this deliberate ignorance, it says, it is a form of prior based heuristics,

too often simply deliberate ignorance.

However, in a discontinuously non-repeat game environment, pattern recognition without maintaining sensitivity for small but nonetheless transformative variations puts survival at risk.

Therefore, we need a process and a measure of error that optimizes survival, hence action-centric free energy minimization powering inactive AI.

And so I thought about this, like, okay, we're talking about like corporate strategy.

So in pattern recognition, that's super useful for something like a game of chess, right?

Like, so chess has like, these are the pieces, these are the rules.

There's only so many like ways that a piece can move.

And yes, it's super complicated, but like over time, a master chess player can start to recognize patterns in the game or like certain sequences of moves that result in, you know,

And so if you know those sequences of moves that results in success, that's pattern recognition, but, but in a non-repeat game, like where the rules are always changing and the pieces can move in a different way.

Like imagine now we're playing 4D chess, like where we could go also like X, Y, but we can also move in the Z direction, jump over pieces and so forth.

Like if you add an extra dimension, those prior sequences,

patterns that you knew for, you know, X, Y chess now doesn't happen, right?

Like they're not, they don't even apply or aren't even relevant here.

So failing to update like your, your prior pattern recognition is going to not lead to successful strategies.


SPEAKER_01:
Yeah, just to make this a little bit more concrete.

So a very common example of this I've experienced is that sometimes when you're doing business strategy and the business is still running and you might have customer service claims incoming while you're doing this core business strategy.

Generally, you want to ignore all of these customer service claims because that's just like noise and you don't want to pay attention to it.

It's going to overwhelm you.

Though occasionally, you'll sometimes hear some very unusual customer experience claims.

That kind of implies something much bigger and much more serious is going on systemically within the organization.

and you need to have this like spidey sense and this is where there's often used in business settings like the spidey sense of knowing like oh wait like something bigger is going on i need to switch gears into being super micro rather than being macro um so i think it's a good example when you have to kind of like switch your your model of like how the organization is working really dynamically and go from deliberate ignorance choosing what information will be selling to you


SPEAKER_02:
Okay, and then sophistication, which I thought this was relevant.

I can't remember the quote that said this again.

Oh, Bijan actually started to unpack this in figure four.

And he said, we alluded to sophistication back in chapter one.

And this also applies to this like double loop learning.

So sophisticated inference is where you have

like many potential steps that can unfold.

And this also applies to the principle of least action, which we're talking about, which is like a decision tree search, right?

Like if you're standing here at step one, you can go, you know, which is like the top level here, you can go to any of these four points, you know, you one, you two, you three, you four.

So if you're here at the very beginning step, and then each one of these points has their own branch choices that you can make.

And then each one of those choices has their own choices.

And so this is, you know, this sophisticated inference, which is unpacked in this paper, sophisticated inference.

And then we've done on different live streams here before.

Also, we didn't do this paper, but we did deep, effective inference.

I think that was maybe also 17, 13.

I can't remember.

But a long time ago, it was like a 2020 early live stream with Ryan Smith.

So in this sophisticated model, it's unfolding temporarily through time.

And this applies to both like the trajectory when we're talking about least action trajectories and also to double loop learning where you're looking at like maybe governing variables over time.

So he starts to unpack that here in this chapter.

It goes a little bit more into it than in chapter one.

All right, so now we're on the fourth part of this claim.

We made it all the way to the end.

Active inference should provide the basis to set free the collaborative human-machine artificial intelligence potential so integral to the multi-intelligence firm.

And note those two separate AI abbreviations.

All right, so Bajan makes the claim, social fields are not as real time as electrically charged and chemically powered neural responsiveness.

But the underlying logic and principles nonetheless determine firm performance and survival.

All living self-organizing systems, as is true for organizations,

strive to reduce entropy in pursuit of minimizing surprise.

But organizations, other than living systems, are too often trapped in what Aguirre's 1995 termed skilled incompetence, meaning that organizations are programmed to deal with error and threat in ways that are counterproductive to their own intentions, not least because organizations struggle with unlearning and deliberately suppress subtlety.

That maybe refers back to that deliberate ignorance that we talked about earlier.

Do you want to talk here, Tyler, about organizational becoming?


SPEAKER_01:
Yeah, sure.

So just on that last slide, a couple thoughts on that.

I think one part that was interesting about this was explicitly connecting free energy to the firm, which I guess that's the whole point of this book.

But I think that's actually quite a large claim to say that firms also exhibit free energy.

And so I just wanted to point that out.

But then also, I think a really brilliant aspect of free energy governance is the lag between stimuli and action and action and stimuli.

where that is a lot larger in an organizational space rather than something kind of like a neuron for example and that makes i think a lot harder to like operationalize and really understand free energy in like a firm context as well

So the next slide that goes into organizational becoming is this is a term he doesn't really define explicitly throughout the book, but he does mention it quite a number of times.

So I think it's worth calling out.

So he says strategic renewal as organizational becoming categorical imperative of being in the game.

And so, again, he doesn't explicitly define this, but I think what he's implying is this idea of becoming, it's like this deeper philosophical term that goes back thousands of years, where the organization isn't a static thing, or it's not something that becomes something into an end state, but rather it is like a dynamic process, and it is always a dynamic process.

And so, yeah, Blue, Daniel, do you have anything you want to add to this?


SPEAKER_00:
You have the Heraclitus river quote, which is a statement of the dynamic nature of becoming.

And then in contrast, we have being.

And so people often talk about being and becoming as a noun centric and a verb centric way to describe something is something that

is that's like being or is it becoming which is more of a process-based focus and so he refers to it as the site of organizational becoming not some sort of ready-made tool that's going to be but rather a process that will be applied or is already being applied as a distributed site of becoming


SPEAKER_02:
Cool.

Do you want to read this one too, Tyler?


SPEAKER_01:
Sure.

So FFP is not simplicity craving, but empowers a system to exploit uncertainty and complexity in pursuit of performance and survival by way of curiosity-driven epistemic forging and exploration.

Correspondingly, free energy governance is a next-generation learning system designed for a discontinuous world, de-chaining the firm from dominant top-down logics and

empowering continuous strategic renewal through purpose-directed generative inferential and cross-hierarchical prediction error minimization.


SPEAKER_02:
Nice.

And so I wonder here if organizational becoming and like strategic renewal, I wonder if those are maybe being used interchangeably.

We've talked about like strategic renewal or like maybe strategic renewal is like a fundamental, like an organizational becoming or organizational becoming includes strategic renewal and other things.

I don't know.

That's a good question for Bijan who we will get to ask lots of questions of next week.

Do you have any additional comments here, Daniel or Tyler?

Okay.

Free energy governance.

I'll let you take this one too.


SPEAKER_01:
All right, so energy governance effectively transposes the active inference framework to the firm and adopts FEP as a first-order principle underlying a new logic of organizing built around three core dimensions.

So structure, hierarchy, cognition, environmental enactment, capabilities, sensing, and sensemaking principles.

The framework is centered around cross-hierarchical journal processes of sensing and sense-making to institutionalize continuous strategic renewal as a matter of inactive inference in pursuit of firm outperformance and survival.


SPEAKER_02:
Nice.

So here we are at the end of the free energy principle chapter, the end of the first section.

on the FEP in the free energy governance textbook.

Looking forward to having a good discussion with the author next week.

Do you guys have any final thoughts on this section or chapter or anything overall?


SPEAKER_00:
I think a question to unpack is going to be, how does a descriptive theory of everything in free energy principle help us make normative or even adaptive decisions for ourselves and for our organizations?

Every single information architecture or strategy that a given group implements is

will be describable in terms of the particular partitioning into internal external and blanket states they're all going to be describable in terms of Bayes optimal inference given some generative model and incoming stream of data so how do we take something that is kind of like the number line it just is and it is neutral and then proactively apply it

up to and including the last mile do people need to actually know about information entropy to be part of a system that's designed with these strategies in mind or is there going to be some other way to communicate and apply it so how we actually move it to the space of proactive decision making today and what that looks like in different sectors and for different organizations is really exciting


SPEAKER_02:
Awesome.

Well, I'm also looking forward to reading the next sections of the book and kind of going beyond like the background section into like, what's next?

What is the application?

And looking forward to Bijan's comments next week.

Tyler, any comments?

Are you good?


SPEAKER_01:
No, all good for me.

Looking forward to next week.


SPEAKER_02:
Yeah, me too.

So if anybody has questions, please go ahead and submit them.

You're welcome to email them to us or submit them here in the comments.

But do it before the 11th, which is when our meeting with Bijan will be.

And if you want to participate on the live stream and have read the chapters in the book and watch the live streams or one or the other, get in touch.

Okay.

Thanks, guys.


SPEAKER_00:
Thanks.
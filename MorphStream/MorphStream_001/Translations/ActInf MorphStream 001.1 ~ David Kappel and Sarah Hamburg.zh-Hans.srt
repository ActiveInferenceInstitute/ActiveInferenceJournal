1
00:00:07,040 --> 00:00:10,320
大家好，欢迎光临，今天是 2023 年 9 月

2
00:00:10,320 --> 00:00:12,080
26 日，

3
00:00:12,080 --> 00:00:15,120
我们在主动推理研究所启动了一个新的

4
00:00:15,120 --> 00:00:17,119
流系列，

5
00:00:17,119 --> 00:00:20,400
这是今天的变形流

6
00:00:20,400 --> 00:00:24,800
1.1，我们有 David kapple，

7
00:00:24,800 --> 00:00:27,960
本节和流由

8
00:00:27,960 --> 00:00:31,320
Sarah Hamburg 主持，我们将有一个

9
00:00:31,320 --> 00:00:35,440
概述首先由 Sarah 介绍，然后

10
00:00:35,440 --> 00:00:38,040
David 将分享一些有关

11
00:00:38,040 --> 00:00:40,520
神经拟态计算的工作，然后我们将

12
00:00:40,520 --> 00:00:42,879
有一些时间进行讨论，感谢

13
00:00:42,879 --> 00:00:45,480
你们的加入，感谢 Sarah 的

14
00:00:45,480 --> 00:00:47,360
第一次演示，

15
00:00:47,360 --> 00:00:49,600
如果您愿意，也请介绍一下自己

16
00:00:49,600 --> 00:00:51,000
一个好主意，非常感谢

17
00:00:51,000 --> 00:00:54,120
丹尼尔，所以我的名字是莎拉，我是一名

18
00:00:54,120 --> 00:00:56,120
神经科学家，专门从事情报研究，

19
00:00:56,120 --> 00:00:57,280
目前在英国的

20
00:00:57,280 --> 00:01:00,280
Chef Field Hal 从事神经形态计算领域的工作，

21
00:01:00,280 --> 00:01:02,480
所以我将为您提供一个

22
00:01:02,480 --> 00:01:04,400
高层次的概述 神经拟态

23
00:01:04,400 --> 00:01:06,720
计算是什么，在我们听到 David

24
00:01:06,720 --> 00:01:08,640
在这个新系列的第一版中令人兴奋的演讲之前，

25
00:01:08,640 --> 00:01:11,320
嗯只是为了让你知道

26
00:01:11,320 --> 00:01:12,799
你是否在未来观看双时间节目，

27
00:01:12,799 --> 00:01:14,320
我说得很快，所以你可能

28
00:01:14,320 --> 00:01:17,080
不想看我 双倍所以

29
00:01:17,080 --> 00:01:19,400
我放在这里的这个二维码将带您

30
00:01:19,400 --> 00:01:20,880
阅读一篇论文，我认为这篇论文

31
00:01:20,880 --> 00:01:23,520
对该领域非常好的介绍，但是

32
00:01:23,520 --> 00:01:25,520
neic计算可以定义为

33
00:01:25,520 --> 00:01:27,000
旨在

34
00:01:27,000 --> 00:01:28,680
模仿神经系统的结构和功能的计算系统

35
00:01:28,680 --> 00:01:30,720
所以这不一定

36
00:01:30,720 --> 00:01:32,680
是人类神经系统，这个领域

37
00:01:32,680 --> 00:01:34,320
实际上从

38
00:01:34,320 --> 00:01:36,640
各种动物和昆虫中汲取灵感，嗯，尽管

39
00:01:36,640 --> 00:01:38,159
网上的定义不一定

40
00:01:38,159 --> 00:01:40,680
承认这一点，所以有些人

41
00:01:40,680 --> 00:01:42,200
对神经形态的构成相当开放，

42
00:01:42,200 --> 00:01:43,920
而也许其他人更

43
00:01:43,920 --> 00:01:46,119
喜欢 神经形态是为像神经元这样的

44
00:01:46,119 --> 00:01:48,600
生物的硬件实例保留的，它们

45
00:01:48,600 --> 00:01:50,520
有时

46
00:01:50,520 --> 00:01:53,280
被称为非运行计算机，我

47
00:01:53,280 --> 00:01:55,280
认为这就是 QR

48
00:01:55,280 --> 00:01:58,320
码嗯将在其

49
00:01:58,320 --> 00:02:00,280
定义中提到的论文，所以我认为这真的很

50
00:02:00,280 --> 00:02:01,200
有趣 有点

51
00:02:01,200 --> 00:02:02,920
背景，所以就像我们当前的 vuman

52
00:02:02,920 --> 00:02:04,479
计算机体系结构也受到

53
00:02:04,479 --> 00:02:06,439
神经科学的启发，特别是

54
00:02:06,439 --> 00:02:08,840
Mull 和pits 43 神经元模型 um

55
00:02:08,840 --> 00:02:12,080
启发了 Von y 1945 年的第一份草案，所以

56
00:02:12,080 --> 00:02:14,120
Neuroscience 有着悠久的历史，嗯

57
00:02:14,120 --> 00:02:16,120
启发计算机科学，这也

58
00:02:16,120 --> 00:02:17,680
包括

59
00:02:17,680 --> 00:02:19,440


60
00:02:19,440 --> 00:02:20,720


61
00:02:20,720 --> 00:02:22,480
基于奖励和

62
00:02:22,480 --> 00:02:24,760
惩罚的行为心理学学习决策理论的强化学习，以及

63
00:02:24,760 --> 00:02:26,519
细胞一起发射的平安学习原理，嗯，

64
00:02:26,519 --> 00:02:29,239
从 49 开始成为

65
00:02:29,239 --> 00:02:31,680
无监督学习的基础，

66
00:02:31,680 --> 00:02:34,959
所以首先，嗯

67
00:02:34,959 --> 00:02:37,640
等一下 所以，为了理解

68
00:02:37,640 --> 00:02:39,440
神经形态计算的原因，我真的

69
00:02:39,440 --> 00:02:40,959
很想解释一下

70
00:02:40,959 --> 00:02:44,120
大脑的伟大之处，所以呃，这里有一些

71
00:02:44,120 --> 00:02:46,360
关于灯泡的灵感，所以我要问你

72
00:02:46,360 --> 00:02:47,280
一个问题，我只是想让你思考

73
00:02:47,280 --> 00:02:49,159
一下 第二，就

74
00:02:49,159 --> 00:02:51,239
灯泡而言，你认为

75
00:02:51,239 --> 00:02:53,879
大脑使用了多少能量，你认为它

76
00:02:53,879 --> 00:02:56,280
比照亮

77
00:02:56,280 --> 00:02:58,040
你所在房间的灯泡的能量更多还是更少，如果你在

78
00:02:58,040 --> 00:02:59,560
未来，无论如何，PA，暂停一下，

79
00:02:59,560 --> 00:03:01,200
如果如果 你想做一些深入的

80
00:03:01,200 --> 00:03:02,760
计算，但我要跳到

81
00:03:02,760 --> 00:03:06,680
答案，答案就在

82
00:03:06,680 --> 00:03:10,159
粉色圆圈里，所以它是 20 瓦，

83
00:03:10,159 --> 00:03:12,200
相当于一个现代节能

84
00:03:12,200 --> 00:03:14,519
灯泡，所以这可能就是

85
00:03:14,519 --> 00:03:16,280
我现在上面的东西 基本上在我的房间

86
00:03:16,280 --> 00:03:18,840
里，如果你对此感兴趣的话，这个二维码应该会带你读到一篇

87
00:03:18,840 --> 00:03:20,040
关于大脑功耗的非常有趣的论文，

88
00:03:20,040 --> 00:03:21,239


89
00:03:21,239 --> 00:03:23,080


90
00:03:23,080 --> 00:03:25,360
这样就可以计算出

91
00:03:25,360 --> 00:03:28,519
每天大约四根香蕉为你的大脑提供能量，

92
00:03:28,519 --> 00:03:30,760
这是顺便计算出来的 基于

93
00:03:30,760 --> 00:03:32,840
大脑所需的卡路里摄入量，所以就

94
00:03:32,840 --> 00:03:35,200
上下文而言，欧洲最快的超级计算机

95
00:03:35,200 --> 00:03:36,720
我认为它在芬兰被称为 Lumi

96
00:03:36,720 --> 00:03:38,560
嗯，它被称为

97
00:03:38,560 --> 00:03:40,560
非常绿色，它的

98
00:03:40,560 --> 00:03:43,640
功耗为 850 万瓦，所以

99
00:03:43,640 --> 00:03:45,840


100
00:03:45,840 --> 00:03:48,840
你的大脑使用了大约 50 万个灯泡 只是一个，那么

101
00:03:48,840 --> 00:03:49,879
问题就是你的

102
00:03:49,879 --> 00:03:52,280
大脑如何处理一个灯泡或

103
00:03:52,280 --> 00:03:54,200
四个

104
00:03:54,200 --> 00:03:57,599
香蕉显然它每秒进行 10,000 亿次

105
00:03:57,599 --> 00:04:00,120
计算所以还有

106
00:04:00,120 --> 00:04:01,879
很多其他的大规模估计

107
00:04:01,879 --> 00:04:03,599
呃这甚至不是

108
00:04:03,599 --> 00:04:05,840
几个订单中最大的估计 数量级的估计

109
00:04:05,840 --> 00:04:07,799
显然是非常推测性的，但是

110
00:04:07,799 --> 00:04:09,519
它们都是巨大的，而且它们都

111
00:04:09,519 --> 00:04:11,280
倾向于基于神经元的数量、它们的

112
00:04:11,280 --> 00:04:13,840
连接和发射率，但是我

113
00:04:13,840 --> 00:04:14,879
认为这对于

114
00:04:14,879 --> 00:04:17,040
超级计算机

115
00:04:17,040 --> 00:04:18,880
实际上还无法与你匹配的背景来说非常重要 了解我们

116
00:04:18,880 --> 00:04:20,880
技能的复杂性或者

117
00:04:20,880 --> 00:04:23,400
人脑的适应性，所以

118
00:04:23,400 --> 00:04:25,520


119
00:04:25,520 --> 00:04:26,960
当涉及到复杂决策之类的事情时，我们实际上超越了超级计算机，

120
00:04:26,960 --> 00:04:29,120
呃从经验中学习，

121
00:04:29,120 --> 00:04:31,520


122
00:04:31,520 --> 00:04:34,039
那么你的大脑与人工智能相比如何，

123
00:04:34,039 --> 00:04:36,039
所以我提到现代人工智能是 已经受到

124
00:04:36,039 --> 00:04:38,199
大脑启发，但是人工

125
00:04:38,199 --> 00:04:41,240
神经元被高度简化了，它们

126
00:04:41,240 --> 00:04:42,600
没有捕捉到

127
00:04:42,600 --> 00:04:44,320
生物神经元或网络的复杂性，就像

128
00:04:44,320 --> 00:04:47,240
甚至不接近一样，单个神经元

129
00:04:47,240 --> 00:04:49,520
实际上更像网络本身，

130
00:04:49,520 --> 00:04:51,120
研究表明，对一个

131
00:04:51,120 --> 00:04:53,039
生物神经元进行建模需要五个

132
00:04:53,039 --> 00:04:55,240
八层深度人工神经

133
00:04:55,240 --> 00:04:57,080
元 由大约一千个人

134
00:04:57,080 --> 00:04:59,160
工神经元组成的网络，这个二维码应该

135
00:04:59,160 --> 00:05:02,160
带你到这篇论文，

136
00:05:02,160 --> 00:05:04,919
因为你的大脑中有 860 亿个神经元，

137
00:05:04,919 --> 00:05:07,120
呃，它们一起工作，形成一个

138
00:05:07,120 --> 00:05:09,160
高能效、低延迟的

139
00:05:09,160 --> 00:05:11,560
超级计算机，在上面运行

140
00:05:11,560 --> 00:05:13,759
室温相当于

141
00:05:13,759 --> 00:05:16,440
每天四根香蕉，所以希望我

142
00:05:16,440 --> 00:05:17,680
能让你感受到你的

143
00:05:17,680 --> 00:05:19,720
大脑是多么神奇，就好像你不知道这一点，

144
00:05:19,720 --> 00:05:21,960
以及它如何被用来

145
00:05:21,960 --> 00:05:24,039
激发我猜相当基本的人工智能一样

146
00:05:24,039 --> 00:05:25,560
我们现在已经将其与人类智能进行了比较，

147
00:05:25,560 --> 00:05:27,360
所以接下来我将

148
00:05:27,360 --> 00:05:29,000
解释如何通过神经形态计算领域实现大脑的关键特征来

149
00:05:29,000 --> 00:05:31,319
催化我们的

150
00:05:31,319 --> 00:05:33,720
下一代人工智能和技术，

151
00:05:33,720 --> 00:05:35,080


152
00:05:35,080 --> 00:05:37,680
这就是为什么你们都在这里

153
00:05:37,680 --> 00:05:39,960
如此传统 vum 和计算机在物理上具有

154
00:05:39,960 --> 00:05:42,039
独立的计算和内存

155
00:05:42,039 --> 00:05:45,039
单元，如左侧所示，在计算过程中，

156
00:05:45,039 --> 00:05:47,280
数据必须

157
00:05:47,280 --> 00:05:49,000
非常快地前后传输，因此

158
00:05:49,000 --> 00:05:51,600
速度和

159
00:05:51,600 --> 00:05:53,840
能量本质上存在瓶颈，而在神经形态架构中，如

160
00:05:53,840 --> 00:05:55,160


161
00:05:55,160 --> 00:05:57,400
右侧所示， 达里

162
00:05:57,400 --> 00:05:59,639
计算和记忆的帮助发生在同一个

163
00:05:59,639 --> 00:06:01,840
地方，所以据说它们

164
00:06:01,840 --> 00:06:03,880
本质上是并置的，单个

165
00:06:03,880 --> 00:06:06,080
神经元执行计算，而

166
00:06:06,080 --> 00:06:07,759
记忆则由神经元

167
00:06:07,759 --> 00:06:09,720
之间的连接强度代表，

168
00:06:09,720 --> 00:06:13,000
所以树林所以像这样的芯片

169
00:06:13,000 --> 00:06:15,440
可能是用组件创建的 例如，像

170
00:06:15,440 --> 00:06:17,919
Mistas 一样，它可以模拟

171
00:06:17,919 --> 00:06:20,840
突触权重，这种架构

172
00:06:20,840 --> 00:06:22,960
提高了速度，降低了

173
00:06:22,960 --> 00:06:24,520
能耗，真正

174
00:06:24,520 --> 00:06:26,319
有趣的是它支持大规模

175
00:06:26,319 --> 00:06:28,240
并行处理，这意味着

176
00:06:28,240 --> 00:06:30,080
可以同时处理多个问题，

177
00:06:30,080 --> 00:06:32,720
因此

178
00:06:32,720 --> 00:06:34,680
这一架构对于各种

179
00:06:34,680 --> 00:06:37,039
用途尤为重要 案例还因为，当我们到达

180
00:06:37,039 --> 00:06:39,199
Moos 日志的末尾时，这是

181
00:06:39,199 --> 00:06:41,520
您能够在物理上

182
00:06:41,520 --> 00:06:44,599
制造越来越小以适应芯片的晶体管数量

183
00:06:44,599 --> 00:06:46,759
，这也很重要，因为人类

184
00:06:46,759 --> 00:06:48,560
需要在创造能力下降的背景下大幅减少其能源

185
00:06:48,560 --> 00:06:51,520
消耗

186
00:06:51,520 --> 00:06:54,400
更强大的

187
00:06:54,400 --> 00:06:57,280
人工智能，所以人工神经元通常

188
00:06:57,280 --> 00:06:59,560
使用连续激活，如左侧所示，

189
00:06:59,560 --> 00:07:01,599
它们始终处于开启状态，而

190
00:07:01,599 --> 00:07:03,560
神经形态神经元据说会处于

191
00:07:03,560 --> 00:07:05,720
尖峰状态，因此它们要么开启，要么关闭，如

192
00:07:05,720 --> 00:07:07,440
右侧所示，非常

193
00:07:07,440 --> 00:07:09,759
相似 对动作潜力进行排序，

194
00:07:09,759 --> 00:07:12,000
因此这样做的好处又是功率

195
00:07:12,000 --> 00:07:14,160
效率，以及

196
00:07:14,160 --> 00:07:16,319
时序很重要的应用程序，这是

197
00:07:16,319 --> 00:07:18,720
因为它们的事件驱动，所以

198
00:07:18,720 --> 00:07:21,160
本质上它们具有

199
00:07:21,160 --> 00:07:23,759
空间和时间维度的潜力，

200
00:07:23,759 --> 00:07:25,759
然后可以添加时空

201
00:07:25,759 --> 00:07:28,479
编码 和信息处理

202
00:07:28,479 --> 00:07:30,680
嗯，你可能想知道关于 GPU 的一些信息，嗯，

203
00:07:30,680 --> 00:07:33,240
它也支持并行处理 嗯，

204
00:07:33,240 --> 00:07:35,080
研究表明，GPU 是

205
00:07:35,080 --> 00:07:37,319
部署

206
00:07:37,319 --> 00:07:39,599
峰值网络的合适架构 嗯，考虑到高端 GPU 的发展，我认为

207
00:07:39,599 --> 00:07:41,280
这对于该领域来说是一个非常有趣的时刻

208
00:07:41,280 --> 00:07:43,199


209
00:07:43,199 --> 00:07:46,159
越来越

210
00:07:46,159 --> 00:07:49,479
普遍，所以大脑可以学习

211
00:07:49,479 --> 00:07:51,599
神经元之间的突触强度，这是

212
00:07:51,599 --> 00:07:53,560
基于突触前和突触后放电

213
00:07:53,560 --> 00:07:55,240
模式，我认为大卫的演讲会讲

214
00:07:55,240 --> 00:07:57,680
很多，我们将对此进行更深入的讨论，

215
00:07:57,680 --> 00:07:59,319
但有许多不同的

216
00:07:59,319 --> 00:08:01,240
类型和 大脑中的这种模式

217
00:08:01,240 --> 00:08:03,720
取决于突触的类型，

218
00:08:03,720 --> 00:08:05,599
例如兴奋性到抑制性

219
00:08:05,599 --> 00:08:06,919
兴奋性到

220
00:08:06,919 --> 00:08:09,159
兴奋性嗯，神经元市场

221
00:08:09,159 --> 00:08:11,639
领域正在努力利用这些规则

222
00:08:11,639 --> 00:08:13,800
嗯，因为芯片

223
00:08:13,800 --> 00:08:16,159
学习以及

224
00:08:16,159 --> 00:08:18,039
模式识别和边缘计算等应用都有好处

225
00:08:18,039 --> 00:08:19,599
由于事件驱动的性质以及低能耗，边缘计算是

226
00:08:19,599 --> 00:08:21,680
神经拟态计算的一个相当大的用例，

227
00:08:21,680 --> 00:08:23,000


228
00:08:23,000 --> 00:08:26,240


229
00:08:26,240 --> 00:08:27,879
这个二维码应该带你到

230
00:08:27,879 --> 00:08:31,839
我发现的一篇关于 stdp 的非常有趣的论文，

231
00:08:31,839 --> 00:08:34,799
那么什么是神经拟态解决方案

232
00:08:34,799 --> 00:08:36,159
现在可用，你可能只是认为这是

233
00:08:36,159 --> 00:08:38,320
理论上的呃，实际上有很多

234
00:08:38,320 --> 00:08:40,200
不同的解决方案，我

235
00:08:40,200 --> 00:08:41,559
只是给你一个非常高层次的

236
00:08:41,559 --> 00:08:44,120
概述，所以人脑项目

237
00:08:44,120 --> 00:08:45,760
已经创建了几个大型

238
00:08:45,760 --> 00:08:48,120
神经形态计算机，包括角宿一，

239
00:08:48,120 --> 00:08:49,800
这是一个在 底部这个

240
00:08:49,800 --> 00:08:51,560
板可能就像我不知道

241
00:08:51,560 --> 00:08:53,480
我的脸或其他东西的大小，所以它是

242
00:08:53,480 --> 00:08:55,760
实时运行的，它

243
00:08:55,760 --> 00:08:58,200
由微处理器上的多个通用用途组成

244
00:08:58,200 --> 00:09:00,279
，还有脑

245
00:09:00,279 --> 00:09:03,120
秤，它是一种加速的模拟

246
00:09:03,120 --> 00:09:05,399
架构，它运行

247
00:09:05,399 --> 00:09:09,240
实时一千次，所以嗯，

248
00:09:09,240 --> 00:09:11,120
蓝色旁边的板是一个实际的

249
00:09:11,120 --> 00:09:13,240
信用卡大小的脑秤版本，

250
00:09:13,240 --> 00:09:14,399
他们最近制作了，我

251
00:09:14,399 --> 00:09:16,920
认为这很酷，嗯，然后

252
00:09:16,920 --> 00:09:18,399
这个领域也有一些大玩家，

253
00:09:18,399 --> 00:09:21,000
所以 这里这个蓝色的是英特尔的

254
00:09:21,000 --> 00:09:23,640
Loi 芯片，他们现在正在使用 lii 2，这是

255
00:09:23,640 --> 00:09:25,480
他们的神经拟态芯片，他们也有一个开源

256
00:09:25,480 --> 00:09:27,000
软件框架，

257
00:09:27,000 --> 00:09:27,920
因为他们真的很想

258
00:09:27,920 --> 00:09:29,480
促进开源关键社区

259
00:09:29,480 --> 00:09:32,079
参与其中，如此神经拟态

260
00:09:32,079 --> 00:09:34,040
传感器也存在，所以中间的这个蓝色小

261
00:09:34,040 --> 00:09:35,000
东西实际上是一个

262
00:09:35,000 --> 00:09:36,800
神经形态相机，它可能就像这个

263
00:09:36,800 --> 00:09:40,040
大嗯，所以他们的目标是重现我们的

264
00:09:40,040 --> 00:09:42,480
神经系统如何感知

265
00:09:42,480 --> 00:09:44,880
光等刺激，例如在神经形态

266
00:09:44,880 --> 00:09:47,200
相机中，这是每个像素的一个

267
00:09:47,200 --> 00:09:49,560
以 microsc 分辨率独立工作

268
00:09:49,560 --> 00:09:52,120
希望我的 GIF 能够工作哦，我们

269
00:09:52,120 --> 00:09:54,440
走了，这样你就可以看到每个像素

270
00:09:54,440 --> 00:09:56,680
在那里工作，这非常酷，所以

271
00:09:56,680 --> 00:09:58,760
与传统数码相机相比，

272
00:09:58,760 --> 00:10:00,120
它们提高了

273
00:10:00,120 --> 00:10:02,920
运动性能并降低了功耗，

274
00:10:02,920 --> 00:10:05,120
最近还有一个神经形态鼻子 由

275
00:10:05,120 --> 00:10:07,079
英特尔开发，这非常酷，所以它

276
00:10:07,079 --> 00:10:08,440


277
00:10:08,440 --> 00:10:10,959
只需接触一次就可以学习化学物质的气味，然后

278
00:10:10,959 --> 00:10:12,480
即使它

279
00:10:12,480 --> 00:10:15,279
被其他人掩盖，它也可以识别出这种气味，最后

280
00:10:15,279 --> 00:10:17,959
这是一个名为 iub 的人形机器人，

281
00:10:17,959 --> 00:10:19,240
你可以做什么 要做的是，你实际上可以

282
00:10:19,240 --> 00:10:21,200
将相机等神经形态传感器，

283
00:10:21,200 --> 00:10:24,000
然后将神经形态芯片（

284
00:10:24,000 --> 00:10:27,000
可能是水疗中心或脑秤）集成到

285
00:10:27,000 --> 00:10:28,440
像这样的人形设备或

286
00:10:28,440 --> 00:10:30,560
无人机等其他设备中，然后从中

287
00:10:30,560 --> 00:10:31,839
你实际上可以创建具体的

288
00:10:31,839 --> 00:10:34,200
神经形态系统，呃，这是

289
00:10:34,200 --> 00:10:35,839
我们在英国谢菲尔德的智能

290
00:10:35,839 --> 00:10:37,519
交互技术研究实验室工作

291
00:10:37,519 --> 00:10:40,320


292
00:10:40,480 --> 00:10:42,600
这张幻灯片只是强调了神经拟态计算

293
00:10:42,600 --> 00:10:43,959
的一些潜在应用，

294
00:10:43,959 --> 00:10:45,480


295
00:10:45,480 --> 00:10:46,720
当你思考它时我认为这非常有趣，

296
00:10:46,720 --> 00:10:49,120
所以对上下文

297
00:10:49,120 --> 00:10:51,560
模式识别的理解高级感知少量

298
00:10:51,560 --> 00:10:54,360
学习 跨任务概括

299
00:10:54,360 --> 00:10:56,839
复杂的决策可解释性

300
00:10:56,839 --> 00:10:59,519
和大脑接口嗯所以

301
00:10:59,519 --> 00:11:01,360
当你

302
00:11:01,360 --> 00:11:03,399
考虑

303
00:11:03,399 --> 00:11:06,240
动态环境中以人为中心的实时应用程序时所有这些技能都非常有用，

304
00:11:06,240 --> 00:11:07,839
例如自动驾驶汽车之类的东西，

305
00:11:07,839 --> 00:11:10,000
我个人认为

306
00:11:10,000 --> 00:11:12,800
神经形态系统嗯 也可能

307
00:11:12,800 --> 00:11:14,360
是脑机接口的未来基础，

308
00:11:14,360 --> 00:11:16,000
可能有点

309
00:11:16,000 --> 00:11:17,959
偏见，因为我是一名神经科学家，但

310
00:11:17,959 --> 00:11:19,959
嗯，它们是低能量的，它们是实时的，

311
00:11:19,959 --> 00:11:22,000
而且它们也有与

312
00:11:22,000 --> 00:11:24,839
我们自己的硬件相匹配的架构，所以我确实认为

313
00:11:24,839 --> 00:11:26,560
我们’ 很快我们就会看到脑机接口领域受到

314
00:11:26,560 --> 00:11:28,680
神经形态系统的催化，

315
00:11:28,680 --> 00:11:29,480
特别

316
00:11:29,480 --> 00:11:31,360
是硬件和湿衣的混合体，

317
00:11:31,360 --> 00:11:33,680
所以甚至可能

318
00:11:33,680 --> 00:11:35,800
包含人们自己的

319
00:11:35,800 --> 00:11:37,399
脑细胞，你知道，你实际上可以

320
00:11:37,399 --> 00:11:40,160
从毛

321
00:11:40,360 --> 00:11:43,040
细胞中生长出脑细胞，我们工作的特别重点

322
00:11:43,040 --> 00:11:45,160
是 设计人工智能以

323
00:11:45,160 --> 00:11:47,120
与人类类似的方式学习，因此它具有

324
00:11:47,120 --> 00:11:48,959
与生俱来的好奇心，并

325
00:11:48,959 --> 00:11:51,560
通过与现实世界的互动来学习，

326
00:11:51,560 --> 00:11:53,959
所以在 50 年代艾伦图灵说，而不是

327
00:11:53,959 --> 00:11:56,079
试图制作一个程序来

328
00:11:56,079 --> 00:11:58,519
模拟成年人的思维，为什么不呢？

329
00:11:58,519 --> 00:12:00,360
尝试生产一种刺激

330
00:12:00,360 --> 00:12:02,760
儿童大脑的产品，如果接受

331
00:12:02,760 --> 00:12:04,560
适当的教育课程，

332
00:12:04,560 --> 00:12:07,480
人们将获得成人大脑，这很大程度上是

333
00:12:07,480 --> 00:12:09,360


334
00:12:09,360 --> 00:12:12,320
人工智能和

335
00:12:12,320 --> 00:12:13,800
神经形态计算的神经发育方法背后的哲学，我只是

336
00:12:13,800 --> 00:12:15,800
想

337
00:12:15,800 --> 00:12:17,839
在那里强调一下 该领域存在一些挑战，

338
00:12:17,839 --> 00:12:19,839
这些挑战水平非常高，

339
00:12:19,839 --> 00:12:20,880
但我只会向您提供一点

340
00:12:20,880 --> 00:12:23,760
概念，因此训练尖峰神经

341
00:12:23,760 --> 00:12:25,079
网络比

342
00:12:25,079 --> 00:12:27,199
传统神经网络更复杂，同时还

343
00:12:27,199 --> 00:12:28,560
设计了在实际

344
00:12:28,560 --> 00:12:31,079
实现尖峰神经网络 stdp 的硬件

345
00:12:31,079 --> 00:12:33,120
据说大规模是相当具

346
00:12:33,120 --> 00:12:35,120
有挑战性的，然后还要开发

347
00:12:35,120 --> 00:12:36,480
实际上可以

348
00:12:36,480 --> 00:12:38,959
有效利用所有这些

349
00:12:38,959 --> 00:12:41,440
技术的算法，因此硬件标准差

350
00:12:41,440 --> 00:12:45,399
它是一个持续活跃的

351
00:12:45,399 --> 00:12:48,760
研究领域，所以如果你在这里，你

352
00:12:48,760 --> 00:12:50,959
可能对主动推理感兴趣，

353
00:12:50,959 --> 00:12:52,560
所以 我想强调这一点，实际上

354
00:12:52,560 --> 00:12:53,839
今天有人在 Discord 上发布了其中一项

355
00:12:53,839 --> 00:12:55,839
非常酷的研究，所以

356
00:12:55,839 --> 00:12:58,040
最近的一些研究将

357
00:12:58,040 --> 00:12:59,560
神经形态计算与主动推理原理结合起来，

358
00:12:59,560 --> 00:13:01,720
所以主动推理

359
00:13:01,720 --> 00:13:03,720
来自神经科学，我

360
00:13:03,720 --> 00:13:05,040
认为它适合自己

361
00:13:05,040 --> 00:13:06,600
对神经形态

362
00:13:06,600 --> 00:13:08,760
架构非常好，嗯，在最近一篇关于

363
00:13:08,760 --> 00:13:10,680
体现神经形态智能的论文中，所以

364
00:13:10,680 --> 00:13:12,320
不是，它并没有真正提到

365
00:13:12,320 --> 00:13:14,279
其中的主动推理，右上角的二维码，

366
00:13:14,279 --> 00:13:16,279
嗯，有人建议，

367
00:13:16,279 --> 00:13:18,079


368
00:13:18,079 --> 00:13:19,959
如果整个神经形态学领域将发生真正的突破 系统设计

369
00:13:19,959 --> 00:13:21,839
基于生物计算

370
00:13:21,839 --> 00:13:23,800
原理，在

371
00:13:23,800 --> 00:13:25,199


372
00:13:25,199 --> 00:13:27,360
环境估计和机器人自身状态

373
00:13:27,360 --> 00:13:30,240
以及决策规划和行动之间紧密结合，

374
00:13:30,240 --> 00:13:31,600
因此其中一些主题对于对主动推理和 U

375
00:13:31,600 --> 00:13:32,839
感兴趣的人来说可能听起来非常熟悉，

376
00:13:32,839 --> 00:13:35,199
我建议

377
00:13:35,199 --> 00:13:36,959
主动推理 推理可以很好地

378
00:13:36,959 --> 00:13:39,160
满足这些要求，我只是

379
00:13:39,160 --> 00:13:40,600
想在这里强调一些最近的

380
00:13:40,600 --> 00:13:43,079
研究，所以左边的

381
00:13:43,079 --> 00:13:45,000
gandal FAL 最近展示了

382
00:13:45,000 --> 00:13:47,519


383
00:13:47,519 --> 00:13:50,240
使用主动推理原理的神经形态系统中的可塑性和快速无监督学习，

384
00:13:50,240 --> 00:13:52,560
作者

385
00:13:52,560 --> 00:13:53,800
建议他们的实验可以

386
00:13:53,800 --> 00:13:55,560
被采用来

387
00:13:55,560 --> 00:13:58,759
在神经神经形态机器人系统中实现类似大脑的预测能力，

388
00:13:58,759 --> 00:14:01,480
然后还有

389
00:14:01,480 --> 00:14:03,000


390
00:14:03,000 --> 00:14:05,560
Kagan atal 的盘子 frin 纸，你们中的一些人可能很熟悉，

391
00:14:05,560 --> 00:14:08,399
所以这是一个混合的 um 湿衣硬件神经

392
00:14:08,399 --> 00:14:10,399
形态系统，作者

393
00:14:10,399 --> 00:14:12,759
声称它体现了 该系统显示

394
00:14:12,759 --> 00:14:14,480
家长

395
00:14:14,480 --> 00:14:16,839
使用自由能学习原理快速学习乒乓球比赛，

396
00:14:16,839 --> 00:14:19,040
作者声称

397
00:14:19,040 --> 00:14:20,720
该系统表现出合成

398
00:14:20,720 --> 00:14:21,959
生物

399
00:14:21,959 --> 00:14:24,320
智能，因此

400
00:14:24,320 --> 00:14:25,920
在神经形态系统中实施主动影响原理的领域

401
00:14:25,920 --> 00:14:28,480
非常新奇，

402
00:14:28,480 --> 00:14:31,199
这个 mream 系列背后的想法是

403
00:14:31,199 --> 00:14:33,480
创建一个空间和社区来分享

404
00:14:33,480 --> 00:14:36,079
知识想法和专业知识，以

405
00:14:36,079 --> 00:14:38,240
促进该领域的发展，我认为这个领域

406
00:14:38,240 --> 00:14:40,560
可能会出现一些真正令人兴奋的技术飞跃，

407
00:14:40,560 --> 00:14:42,839
所以

408
00:14:42,839 --> 00:14:44,880
感谢您收听我的

409
00:14:44,880 --> 00:14:48,920
神经形态 101 嗯以及接下来的快速

410
00:14:48,920 --> 00:14:51,880
介绍 我们会收到大卫的来信，所以大卫

411
00:14:51,880 --> 00:14:53,519
向你介绍一下，如果你想

412
00:14:53,519 --> 00:14:56,399
自我介绍，请谢谢你

413
00:14:56,399 --> 00:14:59,639
莎拉嗯，我会

414
00:14:59,639 --> 00:15:02,320
分享我的

415
00:15:03,320 --> 00:15:06,320
屏幕

416
00:15:10,820 --> 00:15:14,040
[音乐]是的，

417
00:15:14,040 --> 00:15:17,600
你现在能看到

418
00:15:17,600 --> 00:15:21,240
我的屏幕吗？演示是的，好的，完美的，

419
00:15:21,240 --> 00:15:24,240
好的，你好，我的名字是

420
00:15:24,240 --> 00:15:27,639
David kle 我是博姆大学信息学研究所的研究员和政变

421
00:15:27,639 --> 00:15:30,519
领导人，

422
00:15:30,519 --> 00:15:33,839


423
00:15:33,839 --> 00:15:36,040
所以我领导着

424
00:15:36,040 --> 00:15:37,920
可持续机器学习小组，我们

425
00:15:37,920 --> 00:15:40,199
非常关注神经形态

426
00:15:40,199 --> 00:15:41,800
计算，这就是为什么我 今天在这里，

427
00:15:41,800 --> 00:15:44,360
我将从与

428
00:15:44,360 --> 00:15:47,279
莎拉非常相似的动机开始，这对

429
00:15:47,279 --> 00:15:49,240


430
00:15:49,240 --> 00:15:51,959
这次演讲来说确实是一个很大的灵感，我想嗯，所以可能你们大多数人都

431
00:15:51,959 --> 00:15:54,560
已经看到了这个有趣的

432
00:15:54,560 --> 00:15:57,120
最近结果，我指的不是德国

433
00:15:57,120 --> 00:15:59,120
赢得篮球冠军船，

434
00:15:59,120 --> 00:16:03,880
但是嗯，人工智能方面的巨大飞跃是

435
00:16:03,880 --> 00:16:05,319
我们

436
00:16:05,319 --> 00:16:07,079
在过去几年中看到的，尤其是

437
00:16:07,079 --> 00:16:08,959
过去两三年，所以这是一张

438
00:16:08,959 --> 00:16:11,480
由 doly 网络提示生成的图片，

439
00:16:11,480 --> 00:16:14,360
它真的很

440
00:16:14,360 --> 00:16:16,399
神奇，

441
00:16:16,399 --> 00:16:18,519
仅在两三年前就被认为是科幻小说

442
00:16:18,519 --> 00:16:22,360
，这本质上是

443
00:16:22,360 --> 00:16:24,120
通过神经拟态方法实现的，

444
00:16:24,120 --> 00:16:26,680
即深度神经网络，

445
00:16:26,680 --> 00:16:28,079
这些深度神经网络

446
00:16:28,079 --> 00:16:31,120
现在已经变得巨大，但这也有

447
00:16:31,120 --> 00:16:33,839
一个警告，所以基本上，另一面是

448
00:16:33,839 --> 00:16:36,639
除了其他问题之外，也许

449
00:16:36,639 --> 00:16:40,360
这些模型可能会消耗

450
00:16:40,360 --> 00:16:43,959
大量的能源，所以像 um

451
00:16:43,959 --> 00:16:47,279
Dal 或 jet GPT 这样的模型，莎拉已经

452
00:16:47,279 --> 00:16:50,360
提到过，它们会消耗

453
00:16:50,360 --> 00:16:52,680
与

454
00:16:52,680 --> 00:16:56,759
房屋或汽车相当的能源预算，所以训练 um jgpt a

455
00:16:56,759 --> 00:16:59,440
单次大约相当于 10 亿小时，

456
00:16:59,440 --> 00:17:02,680
因此会排放 300 吨

457
00:17:02,680 --> 00:17:07,160
二氧化碳，而且是普通汽车

458
00:17:07,160 --> 00:17:09,760
寿命的很多倍，

459
00:17:09,760 --> 00:17:13,359
因此这

460
00:17:13,359 --> 00:17:15,280
显然会带来两个问题，

461
00:17:15,280 --> 00:17:17,880
这使得训练这些 模型只能

462
00:17:17,880 --> 00:17:20,160
供极少数

463
00:17:20,160 --> 00:17:21,799
非常大的参与者使用，所以本质上是

464
00:17:21,799 --> 00:17:24,599
大型科技公司，其次，

465
00:17:24,599 --> 00:17:26,319
也许更重要的是，这

466
00:17:26,319 --> 00:17:29,160
与

467
00:17:29,160 --> 00:17:31,880
资源有限的呃星球不兼容，所以如果

468
00:17:31,880 --> 00:17:34,160
人工智能的增长率继续像去年那样 到

469
00:17:34,160 --> 00:17:35,360


470
00:17:35,360 --> 00:17:38,240


471
00:17:38,240 --> 00:17:41,400
20 30 年，它将消耗全球能源消耗的 133%，并且在大约五年内，它将

472
00:17:41,400 --> 00:17:45,080
基本上超过交通运输

473
00:17:45,080 --> 00:17:48,200
部门，

474
00:17:48,200 --> 00:17:50,720
所以这提出了一个问题：

475
00:17:50,720 --> 00:17:52,600
可持续的机器学习是否存在，

476
00:17:52,600 --> 00:17:55,080
显然，因为我正在工作 在

477
00:17:55,080 --> 00:17:56,799
可持续机器

478
00:17:56,799 --> 00:17:59,039
学习领域，我相信它确实如此，

479
00:17:59,039 --> 00:18:01,799
为什么我认为它如此，是因为我们

480
00:18:01,799 --> 00:18:04,960
知道一个非常高效的系统，

481
00:18:04,960 --> 00:18:08,600
并且仍然可能比

482
00:18:08,600 --> 00:18:11,039
这些人工智能模型更好，正如莎拉在 20 左右提到的那样，这些人工智能模型是消耗人类大脑的

483
00:18:11,039 --> 00:18:13,000


484
00:18:13,000 --> 00:18:14,039


485
00:18:14,039 --> 00:18:16,919
瓦呃或每天四个香蕉

486
00:18:16,919 --> 00:18:21,400
呃呃所以它比我们今天拥有的

487
00:18:21,400 --> 00:18:24,400
人工智能模型效率高出许多数量级

488
00:18:24,400 --> 00:18:26,200


489
00:18:26,200 --> 00:18:29,480
呃但到目前为止我们基本上不知道

490
00:18:29,480 --> 00:18:32,480
这些网络是如何工作的嗯

491
00:18:32,480 --> 00:18:34,520
特别是如何训练它们以及

492
00:18:34,520 --> 00:18:37,080
基本上我们的网络 我们的目标是

493
00:18:37,080 --> 00:18:39,559
现在转移机器学习的机制，

494
00:18:39,559 --> 00:18:41,679
所以你在这里有这张漂亮的图片

495
00:18:41,679 --> 00:18:44,679
呃基本上我们从

496
00:18:44,679 --> 00:18:46,440
机器学习网站开始，在那里我们已经

497
00:18:46,440 --> 00:18:49,400
知道我们的方法所以我们有这些

498
00:18:49,400 --> 00:18:51,280
很棒的模型，并且可以

499
00:18:51,280 --> 00:18:53,120
提供 我们确实取得了令人

500
00:18:53,120 --> 00:18:55,240
印象深刻的结果，但它们

501
00:18:55,240 --> 00:18:57,480
效率不高，我们希望将

502
00:18:57,480 --> 00:19:01,840
它们转移到新的高效呃人工智能

503
00:19:01,840 --> 00:19:04,120
一代，我们的想法是利用

504
00:19:04,120 --> 00:19:07,520
神经科学的灵感，

505
00:19:07,520 --> 00:19:09,799


506
00:19:09,799 --> 00:19:12,240
首先使这种转移更快、更可能，

507
00:19:12,240 --> 00:19:17,039
好吧，实际上生物学 是一个

508
00:19:17,039 --> 00:19:19,679
伟大的灵感来源，总是

509
00:19:19,679 --> 00:19:21,400
会带来非常

510
00:19:21,400 --> 00:19:23,200
令人惊讶的结果，

511
00:19:23,200 --> 00:19:24,960
几年前我偶然发现的这些结果之一

512
00:19:24,960 --> 00:19:30,919
是

513
00:19:30,919 --> 00:19:33,559
大脑中的灌木丛的能力，所以你可能

514
00:19:33,559 --> 00:19:35,200
知道大脑中的神经元 与

515
00:19:35,200 --> 00:19:36,159


516
00:19:36,159 --> 00:19:40,960
树林相连，但是如果你看一下，那么

517
00:19:40,960 --> 00:19:43,480
这是

518
00:19:43,480 --> 00:19:45,919
2019 年的一篇论文，他们实际上可以识别

519
00:19:45,919 --> 00:19:47,320
单个 Sy upses，并且可以

520
00:19:47,320 --> 00:19:49,400
触发他们呃做出

521
00:19:49,400 --> 00:19:51,799
像单次传输一样的概要发布，但是

522
00:19:51,799 --> 00:19:54,840
如果你看一下这个 um um 在这些

523
00:19:54,840 --> 00:19:57,799
测量中，你会发现这确实被

524
00:19:57,799 --> 00:20:00,320
噪声覆盖了，所以本质上，如果

525
00:20:00,320 --> 00:20:02,960
你基本上对此进行平均并将其缩小，

526
00:20:02,960 --> 00:20:04,720
你会在这里看到这些典型的

527
00:20:04,720 --> 00:20:06,679
天气痕迹，这是

528
00:20:06,679 --> 00:20:09,240
这里的平均白线，但在其下方你会看到

529
00:20:09,240 --> 00:20:12,080
巨大的抖动，所以有它们 真的

530
00:20:12,080 --> 00:20:14,960
很像上下几个嗯标准差

531
00:20:14,960 --> 00:20:16,120


532
00:20:16,120 --> 00:20:18,640
嗯，这实际上是非常

533
00:20:18,640 --> 00:20:21,000
令人惊讶的，因为呃神经元

534
00:20:21,000 --> 00:20:23,480
可能是你体内最昂贵的细胞

535
00:20:23,480 --> 00:20:25,559
类型，就能量

536
00:20:25,559 --> 00:20:28,840
消耗而言，它们实际上或

537
00:20:28,840 --> 00:20:32,000
相比而言，它们消耗了相当

538
00:20:32,000 --> 00:20:34,159
多的能量 你体内的能量，所以你

539
00:20:34,159 --> 00:20:36,440
会期望这些

540
00:20:36,440 --> 00:20:38,720


541
00:20:38,720 --> 00:20:41,480
在神经元之间传递的转换或传输是非常昂贵的，

542
00:20:41,480 --> 00:20:43,480
应该是高度可靠的，所以这是

543
00:20:43,480 --> 00:20:46,120
非常违反直觉的，这个结果已经让

544
00:20:46,120 --> 00:20:48,120
神经科学家困惑了

545
00:20:48,120 --> 00:20:50,039
很长一段时间，

546
00:20:50,039 --> 00:20:53,600
嗯，然后有

547
00:20:53,600 --> 00:20:58,080
a 还有第二个令人费解的观察

548
00:20:58,080 --> 00:21:00,120
结果，就是

549
00:21:00,120 --> 00:21:03,919
呃，神经元的形态看起来

550
00:21:03,919 --> 00:21:05,480
有点像这样，所以这将是

551
00:21:05,480 --> 00:21:08,159
你皮层中的典型锥体神经元，

552
00:21:08,159 --> 00:21:09,039


553
00:21:09,039 --> 00:21:12,880
但是你会发现这

554
00:21:12,880 --> 00:21:15,919
实际上是一个相当大的细胞，

555
00:21:15,919 --> 00:21:18,039
拉长了，所以这

556
00:21:18,039 --> 00:21:21,039
在人脑中可以达到一毫米呃，

557
00:21:21,039 --> 00:21:23,000
这意味着如果一个信号在

558
00:21:23,000 --> 00:21:25,400
这里的某个地方触发呃它

559
00:21:25,400 --> 00:21:27,200
很难与这里的细胞体进行通信

560
00:21:27,200 --> 00:21:29,000
所以

561
00:21:29,000 --> 00:21:33,240
这里产生的电信号呃 可能会到

562
00:21:33,240 --> 00:21:35,440
这里，但是这里的注册者

563
00:21:35,440 --> 00:21:38,480
无法测量

564
00:21:38,480 --> 00:21:40,520
细胞体的实际电压，这

565
00:21:40,520 --> 00:21:41,960
实际上是有趣的地方，因为

566
00:21:41,960 --> 00:21:43,760
这里是形成的动作电位，所以

567
00:21:43,760 --> 00:21:46,039
如果注册者真的想

568
00:21:46,039 --> 00:21:48,240
知道什么是 细胞

569
00:21:48,240 --> 00:21:50,120
体内发生的事情，这样它就可以预测

570
00:21:50,120 --> 00:21:52,840
呃神经元将如何表现以及

571
00:21:52,840 --> 00:21:55,400
它如何与世界相互作用，这

572
00:21:55,400 --> 00:21:59,880
是神经科学中另一个非常令人费解的或开放性的

573
00:21:59,880 --> 00:22:02,120
问题，这种

574
00:22:02,120 --> 00:22:04,120
沟通实际上是如何在

575
00:22:04,120 --> 00:22:07,320
单个神经元中进行的 在细胞体和

576
00:22:07,320 --> 00:22:10,720
呃和信号之间，众所周知，

577
00:22:10,720 --> 00:22:12,840
实际上动作电位可以

578
00:22:12,840 --> 00:22:14,880
向上传播，所以

579
00:22:14,880 --> 00:22:18,000
当呃，当

580
00:22:18,000 --> 00:22:20,240
神经元尖峰时，他们看到这种二元变量，但他们实际上无法

581
00:22:20,240 --> 00:22:23,799
测量这里的膜电位，

582
00:22:23,799 --> 00:22:25,559
所以只有 最突出的

583
00:22:25,559 --> 00:22:27,159
电信号实际上可以

584
00:22:27,159 --> 00:22:29,120
通过这个

585
00:22:29,120 --> 00:22:31,400
呃反向传播，这表明

586
00:22:31,400 --> 00:22:33,000
注册实际上对

587
00:22:33,000 --> 00:22:34,799


588
00:22:34,799 --> 00:22:40,720
细胞体内这个呃呃发生了什么的信息非常稀疏，并且呃大多数

589
00:22:40,720 --> 00:22:43,000
天气可塑性模型根本没有

590
00:22:43,000 --> 00:22:44,159
涵盖这

591
00:22:44,159 --> 00:22:48,279
一点，所以 我们想知道，

592
00:22:48,279 --> 00:22:50,880
嗯，这种相互作用是如何工作的，

593
00:22:50,880 --> 00:22:52,760


594
00:22:52,760 --> 00:22:55,679


595
00:22:55,679 --> 00:22:57,520
考虑到关于神经元

596
00:22:57,520 --> 00:23:00,200
这一重要状态的稀疏信息，嗯嗯注册如何产生有用的学习信号，

597
00:23:00,200 --> 00:23:02,840
我们的想法是，本质上

598
00:23:02,840 --> 00:23:04,480
这两个观察结果是这些高水平

599
00:23:04,480 --> 00:23:06,440
的 噪音和注册以及

600
00:23:06,440 --> 00:23:08,880
细胞体和注册之间的距离很大，这

601
00:23:08,880 --> 00:23:10,640
给了这些高度的

602
00:23:10,640 --> 00:23:12,679
不确定性，这些实际上是

603
00:23:12,679 --> 00:23:15,480
同一枚硬币的两面，所以我们的假设

604
00:23:15,480 --> 00:23:18,840
是，我们实际上可以使用

605
00:23:18,840 --> 00:23:22,880
我们已经知道的相同模型

606
00:23:22,880 --> 00:23:25,640
从行为层面来看，

607
00:23:25,640 --> 00:23:29,559
代理如何在高度不确定的环境中行动和执行

608
00:23:29,559 --> 00:23:31,760
，

609
00:23:31,760 --> 00:23:33,400
我们只是将其应用于

610
00:23:33,400 --> 00:23:37,760
每个注册，哦，有

611
00:23:37,760 --> 00:23:40,120
一个错误，所以每个注册都应该使用

612
00:23:40,120 --> 00:23:44,559
这个基本上相同的模型，

613
00:23:44,559 --> 00:23:47,679
呃 这个模型会

614
00:23:47,679 --> 00:23:49,159
立即表明，实际上

615
00:23:49,159 --> 00:23:51,919
天气传输应该是

616
00:23:51,919 --> 00:23:54,320
有噪音的，这些噪音水平会

617
00:23:54,320 --> 00:23:56,880
表达环境的不确定性，

618
00:23:56,880 --> 00:23:59,640
然后我们可以使用这个

619
00:23:59,640 --> 00:24:02,080
模型来推导嗯学习规则，我们

620
00:24:02,080 --> 00:24:04,600
可以将它们与

621
00:24:04,600 --> 00:24:06,520
生物学并排比较，这就是 我

622
00:24:06,520 --> 00:24:08,960
想向您展示的第一件事是，我只是

623
00:24:08,960 --> 00:24:10,880
快速介绍一下自由

624
00:24:10,880 --> 00:24:13,720
能模型，因为你们中的一些人

625
00:24:13,720 --> 00:24:15,720
可能不太熟悉它，但这

626
00:24:15,720 --> 00:24:19,159
本质上是一个模型来

627
00:24:19,159 --> 00:24:21,000
描述这样的情况，您有

628
00:24:21,000 --> 00:24:23,279
一个 与某些环境交互的人，

629
00:24:23,279 --> 00:24:25,039
在这里我假设非常

630
00:24:25,039 --> 00:24:28,320
简单，所以这个人试图

631
00:24:28,320 --> 00:24:31,799
到达某个目标，而我们作为人类，我们

632
00:24:31,799 --> 00:24:34,240
擅长解决此类任务，

633
00:24:34,240 --> 00:24:36,039
如果水平较高，我们也擅长解决此类任务

634
00:24:36,039 --> 00:24:37,840
不确定性，

635
00:24:37,840 --> 00:24:41,960
所以如果这个人可能会收到

636
00:24:41,960 --> 00:24:44,000
一些视觉反馈，但很多

637
00:24:44,000 --> 00:24:46,440
反馈可能是隐藏的，所以我你可以想象

638
00:24:46,440 --> 00:24:48,520
这发生在某

639
00:24:48,520 --> 00:24:52,600
堵墙后面，而这个人现在仍然可能想

640
00:24:52,600 --> 00:24:55,039
预测这个球的轨迹是什么

641
00:24:55,039 --> 00:24:56,679
飞向目标，

642
00:24:56,679 --> 00:25:00,960
以便它可以做出正确的呃动作，所以

643
00:25:00,960 --> 00:25:03,960
我们将分配一些变量给这个呃

644
00:25:03,960 --> 00:25:05,600
这些状态，所以我们本质上有

645
00:25:05,600 --> 00:25:07,440
这个人可以

646
00:25:07,440 --> 00:25:09,880
观察到的反馈，我们有

647
00:25:09,880 --> 00:25:12,279
球在这里飞行的未被观察到的状态，我们

648
00:25:12,279 --> 00:25:14,799
称之为 如果人们无法

649
00:25:14,799 --> 00:25:17,000
直接访问它，则只会看到

650
00:25:17,000 --> 00:25:19,200
它的一部分，例如，当球位于

651
00:25:19,200 --> 00:25:22,000
出现的墙角时，

652
00:25:22,000 --> 00:25:25,000
然后呃本质上对此进行建模或

653
00:25:25,000 --> 00:25:28,200
描述此

654
00:25:28,200 --> 00:25:31,240
人的行为 呃有

655
00:25:31,240 --> 00:25:33,360
这个轨迹的内部描述，所以

656
00:25:33,360 --> 00:25:37,520
这个状态 U 的内部模型

657
00:25:37,520 --> 00:25:41,480
和模型然后这个模型将

658
00:25:41,480 --> 00:25:45,679
被更新以匹配观察到的

659
00:25:45,679 --> 00:25:48,120
反馈，这可以

660
00:25:48,120 --> 00:25:50,200
在自由能原理的这个美丽的数学框架中很好地描述，

661
00:25:50,200 --> 00:25:52,320
所以

662
00:25:52,320 --> 00:25:55,159
这个想法是，你基本上会

663
00:25:55,159 --> 00:25:57,279
建立一个内部状态的模型，

664
00:25:57,279 --> 00:26:00,720
所以本质上，

665
00:26:00,720 --> 00:26:04,640
内部状态和环境状态如何

666
00:26:04,640 --> 00:26:06,440


667
00:26:06,440 --> 00:26:09,000
相互作用，你将有一个

668
00:26:09,000 --> 00:26:11,720
反馈模型，这样

669
00:26:11,720 --> 00:26:14,240
你观察到的状态和反馈如何相互作用和

670
00:26:14,240 --> 00:26:18,559
本质上你可以写下一个

671
00:26:18,559 --> 00:26:21,080
损失函数，它测量

672
00:26:21,080 --> 00:26:23,039


673
00:26:23,039 --> 00:26:24,679
内部状态模型和

674
00:26:24,679 --> 00:26:29,600
反馈模型以及外部状态

675
00:26:29,960 --> 00:26:31,720
um

676
00:26:31,720 --> 00:26:35,120
和 uh 之间的距离，然后通过本质上最小化

677
00:26:35,120 --> 00:26:37,799
两者之间的距离，你可以

678
00:26:37,799 --> 00:26:41,799
得出所有 各种与行为相关的

679
00:26:41,799 --> 00:26:44,799
嗯可以解决与行为相关的

680
00:26:44,799 --> 00:26:46,679
问题，例如学习，但你

681
00:26:46,679 --> 00:26:48,399
也可以将其用于其他事情，例如，

682
00:26:48,399 --> 00:26:50,399
弄清楚什么是好的行动，

683
00:26:50,399 --> 00:26:53,440
例如，对

684
00:26:53,440 --> 00:26:55,520
内部状态和行动进行推断，

685
00:26:55,520 --> 00:26:58,440
这是外壳 自由

686
00:26:58,440 --> 00:27:00,480
能原理，这里的这个对象

687
00:27:00,480 --> 00:27:02,360
恰好是所谓的

688
00:27:02,360 --> 00:27:04,760
变分自由能，这也

689
00:27:04,760 --> 00:27:06,960
恰好与统计物理学相一致，

690
00:27:06,960 --> 00:27:08,480
这就是这个框架的

691
00:27:08,480 --> 00:27:11,640
名称来源，但你会发现它

692
00:27:11,640 --> 00:27:13,159
在这里都是概率性的，所以本质上你

693
00:27:13,159 --> 00:27:15,240
有 内部模型的两个概率函数 Q

694
00:27:15,240 --> 00:27:17,320
和

695
00:27:17,320 --> 00:27:19,640
状态与

696
00:27:19,640 --> 00:27:22,120
观察之间的相互作用的 P ，这里有一个

697
00:27:22,120 --> 00:27:24,279
它们之间的距离度量，您

698
00:27:24,279 --> 00:27:25,200
希望将其

699
00:27:25,200 --> 00:27:27,799
最小化，所以现在如果我们看看神经元

700
00:27:27,799 --> 00:27:29,640
和注册以及它们如何

701
00:27:29,640 --> 00:27:31,520
与每个交互交互 其他我们发现了一个非常相似的

702
00:27:31,520 --> 00:27:35,440
图片，所以单个注册嗯，我们

703
00:27:35,440 --> 00:27:38,520
在这里用绿色表示，有一个

704
00:27:38,520 --> 00:27:41,120
内部状态，我们仅将其作为简单

705
00:27:41,120 --> 00:27:45,080
模型呃作为基于此的概要诱饵，

706
00:27:45,080 --> 00:27:46,960
当它被

707
00:27:46,960 --> 00:27:50,799
视前尖峰触发时，它会生成嗯

708
00:27:50,799 --> 00:27:53,200
然后，天气后电流会

709
00:27:53,200 --> 00:27:56,320
传播到 Som，这是我们的呃

710
00:27:56,320 --> 00:27:57,760
外部状态，我们无法

711
00:27:57,760 --> 00:27:59,799
直接观察到它，因为它

712
00:27:59,799 --> 00:28:02,640
离杯子太远，但我们可以看到一个

713
00:28:02,640 --> 00:28:04,279
反馈，即这个 spe 传播

714
00:28:04,279 --> 00:28:05,880
动作电位，它是这个二元

715
00:28:05,880 --> 00:28:07,200
变量 告诉我们

716
00:28:07,200 --> 00:28:09,840
神经元是否已经尖峰，所以

717
00:28:09,840 --> 00:28:12,120
如果我们

718
00:28:12,120 --> 00:28:13,679
这样写下来，这就是完全相同的框架，我们可以

719
00:28:13,679 --> 00:28:18,200
使用相同的数学来解决它，

720
00:28:18,200 --> 00:28:21,159
所以为了解决它，我们只需要

721
00:28:21,159 --> 00:28:24,320
想出一个 嗯，我们必须

722
00:28:24,320 --> 00:28:25,960
做出一些假设，所以我们

723
00:28:25,960 --> 00:28:28,279
必须在这里为这个人写下一个模型，

724
00:28:28,279 --> 00:28:32,240
这个模型说明了

725
00:28:32,240 --> 00:28:35,880
反馈和外部状态如何

726
00:28:35,880 --> 00:28:38,159
相互作用，但是我们有很好的模型

727
00:28:38,159 --> 00:28:40,519
这已经被研究了很多

728
00:28:40,519 --> 00:28:43,120
年，所以在这里你可以看到

729
00:28:43,120 --> 00:28:45,880
模型神经元的行为是如何典型的，所以你在这里有

730
00:28:45,880 --> 00:28:47,440
一个泄漏

731
00:28:47,440 --> 00:28:49,120
集成火神经元的膜电位，你会看到

732
00:28:49,120 --> 00:28:51,159
这只是呃上下，所以

733
00:28:51,159 --> 00:28:53,720
这个神经元会收到呃一个 大量的

734
00:28:53,720 --> 00:28:56,440
视前输入，也许还有噪音，

735
00:28:56,440 --> 00:28:57,679
最终在某个点达到

736
00:28:57,679 --> 00:28:59,640
阈值，它会产生一个尖峰，这样

737
00:28:59,640 --> 00:29:01,760
Z 就会传播到

738
00:29:01,760 --> 00:29:04,279
下游神经元，也可能回到

739
00:29:04,279 --> 00:29:07,120
杯子，然后它会

740
00:29:07,120 --> 00:29:12,159
重置，嗯，对，但是现在 这是呃，所以

741
00:29:12,159 --> 00:29:13,799
我们可以用数学方法写下来，这是

742
00:29:13,799 --> 00:29:16,399
一个非常简单的微分方程，

743
00:29:16,399 --> 00:29:19,039
但是呃神经元无法

744
00:29:19,039 --> 00:29:21,360
再次访问这个状态，所以它又在

745
00:29:21,360 --> 00:29:24,440
这堵墙后面，它只能看到这个尖峰事件，

746
00:29:24,440 --> 00:29:26,440
但我们实际上可以对于这个简单的

747
00:29:26,440 --> 00:29:28,320
情况 真正集成的 F 神经元，我们可以通过

748
00:29:28,320 --> 00:29:30,399
分析来解决这个问题，这样我们就可以

749
00:29:30,399 --> 00:29:32,240
写下给定尖峰时间的膜电位的后验分布是什么，

750
00:29:32,240 --> 00:29:34,559


751
00:29:34,559 --> 00:29:37,480
以及由此产生的

752
00:29:37,480 --> 00:29:40,240
实际上是所谓的随机

753
00:29:40,240 --> 00:29:44,159
桥模型，或者在这种情况下是一个呃

754
00:29:44,159 --> 00:29:45,960
火神经元中的泄漏整数，它是一个

755
00:29:45,960 --> 00:29:48,240
橙桥模型，因此可以

756
00:29:48,240 --> 00:29:51,200
分析地写下来，这个 U 我的

757
00:29:51,200 --> 00:29:53,880
意思是它并不简单，但它是

758
00:29:53,880 --> 00:29:56,760
可行的，然后我们可以直接使用它，

759
00:29:56,760 --> 00:29:59,799
所以我们必须再次写下这个模型

760
00:29:59,799 --> 00:30:02,279
这个呃自由能

761
00:30:02,279 --> 00:30:04,440
函数，所以我们在这里假设

762
00:30:04,440 --> 00:30:06,600
注册实际上如何产生后

763
00:30:06,600 --> 00:30:08,600
天气电流以及它们如何

764
00:30:08,600 --> 00:30:11,000
集成在神经元中，但这

765
00:30:11,000 --> 00:30:12,840
也是由泄漏

766
00:30:12,840 --> 00:30:14,799
集成火神经元和实际上

767
00:30:14,799 --> 00:30:17,760
注册生成的随机um输入给出的

768
00:30:17,760 --> 00:30:20,120
因此，为了简单起见，我们

769
00:30:20,120 --> 00:30:22,919
只假设这里基本上是高斯 Sy

770
00:30:22,919 --> 00:30:25,559
upses，它将注入绘制一个高斯

771
00:30:25,559 --> 00:30:27,960
随机变量并将其注入到 inte

772
00:30:27,960 --> 00:30:30,399
Great 和 Fire 神经元中，然后所有这些

773
00:30:30,399 --> 00:30:31,960
成分实际上都可以以

774
00:30:31,960 --> 00:30:33,640
封闭形式解决，我们可以推导出学习

775
00:30:33,640 --> 00:30:35,720
规则 现在最小化这个前

776
00:30:35,720 --> 00:30:37,640
能量功能

777
00:30:37,640 --> 00:30:40,720
呃，如果我们这样做，

778
00:30:40,720 --> 00:30:44,279
嗯嗯，这有很多很好的

779
00:30:44,279 --> 00:30:46,279
属性，因为这个奥宁贝克

780
00:30:46,279 --> 00:30:48,480
桥完全由

781
00:30:48,480 --> 00:30:51,480
前呃由反向传播动作

782
00:30:51,480 --> 00:30:54,480
电位决定，所以基本上是

783
00:30:54,480 --> 00:30:57,000
后天气峰值的时间 到达

784
00:30:57,000 --> 00:30:57,310


785
00:30:57,310 --> 00:30:58,519
[音乐]

786
00:30:58,519 --> 00:31:01,760
神经元，我们在这里得到的这个形状

787
00:31:01,760 --> 00:31:03,440
仅取决于两个相邻的

788
00:31:03,440 --> 00:31:04,760
视后

789
00:31:04,760 --> 00:31:08,519
尖峰，这意味着我们自动到达这里，

790
00:31:08,519 --> 00:31:10,720
呃，学习规则

791
00:31:10,720 --> 00:31:12,880
看起来像这样，所以学习规则

792
00:31:12,880 --> 00:31:15,480
仅取决于

793
00:31:15,480 --> 00:31:17,320
两个视后尖峰之间的差异 我们在这里称之为

794
00:31:17,320 --> 00:31:18,159
Delta

795
00:31:18,159 --> 00:31:22,960
T2 um 和 uh

796
00:31:22,960 --> 00:31:25,159
后天气尖峰与

797
00:31:25,159 --> 00:31:27,240
在

798
00:31:27,240 --> 00:31:29,120
视前

799
00:31:29,120 --> 00:31:32,720
侧某个点 uh 触发的实际输入之间的差异，我们基本上可以在这里制作

800
00:31:32,720 --> 00:31:35,919
这个查找表，然后只需

801
00:31:35,919 --> 00:31:38,200
计算一下

802
00:31:38,200 --> 00:31:40,960
这些注册需要进行更新，以便

803
00:31:40,960 --> 00:31:42,840
它根据

804
00:31:42,840 --> 00:31:44,760
自由能原理进行最佳学习，这就是

805
00:31:44,760 --> 00:31:46,279
我们得到的形状，您会看到，

806
00:31:46,279 --> 00:31:47,679
对天气后发射率有很强的依赖，

807
00:31:47,679 --> 00:31:49,440
但也有

808
00:31:49,440 --> 00:31:52,720
对基本的依赖 莎拉之前提到的这个典型的STP

809
00:31:52,720 --> 00:31:55,880
呃

810
00:31:55,880 --> 00:31:57,639
视前

811
00:31:57,639 --> 00:31:59,080
和视后

812
00:31:59,080 --> 00:32:02,440
尖峰的相对定位是什么所以简而言之这个模型

813
00:32:02,440 --> 00:32:04,760
现在可以基本上分成两个

814
00:32:04,760 --> 00:32:08,279
路径所以我们有这个atoc响应

815
00:32:08,279 --> 00:32:11,000
基本上只要有一个

816
00:32:11,000 --> 00:32:13,880
视前尖峰 触发

817
00:32:13,880 --> 00:32:16,080
杯子中的一个动作，他将从这个

818
00:32:16,080 --> 00:32:18,039
冲击分布中提取并将其注入

819
00:32:18,039 --> 00:32:21,799
到神经元中，然后有

820
00:32:21,799 --> 00:32:22,880
一个postt

821
00:32:22,880 --> 00:32:25,480
talk呃更新，我们在那里

822
00:32:25,480 --> 00:32:28,720
注册者会在这个orang

823
00:32:28,720 --> 00:32:30,360
和wenck桥中查找um会是什么

824
00:32:30,360 --> 00:32:33,600
它应该生成的最佳 um 输出，

825
00:32:33,600 --> 00:32:35,679
那么

826
00:32:35,679 --> 00:32:38,559
最佳动作是什么，然后根据自由能原理将

827
00:32:38,559 --> 00:32:40,320
实际动作与最佳

828
00:32:40,320 --> 00:32:41,880
动作进行比较，

829
00:32:41,880 --> 00:32:44,880
然后生成延迟

830
00:32:44,880 --> 00:32:47,360
响应响应，这是

831
00:32:47,360 --> 00:32:49,399
天气权

832
00:32:49,399 --> 00:32:52,159
重的更新 重要的是，这个呃

833
00:32:52,159 --> 00:32:54,519
内部模型只是隐式的，它是

834
00:32:54,519 --> 00:32:57,960
被编码的，所以可以说这个嗯呃尖峰

835
00:32:57,960 --> 00:33:00,399
时间依赖性可塑性

836
00:33:00,399 --> 00:33:02,880
规则呃那么这些规则看起来怎么样，

837
00:33:02,880 --> 00:33:04,639
它们与它们与

838
00:33:04,639 --> 00:33:07,360
生物学的比较如何比较，呃实际上它们

839
00:33:07,360 --> 00:33:09,240
非常适合 很好地考虑到这

840
00:33:09,240 --> 00:33:11,519
实际上是从第一原理推导出来的，

841
00:33:11,519 --> 00:33:13,880
没有做任何假设，所以这

842
00:33:13,880 --> 00:33:16,320
是生物学中的测量，这是

843
00:33:16,320 --> 00:33:20,279
Al 规则 B 的 um B 和 po 非常古老的

844
00:33:20,279 --> 00:33:23,279
工作，他们实际上

845
00:33:23,279 --> 00:33:26,519
在 vro 研究中做了这个，他们测量了注射的

846
00:33:26,519 --> 00:33:29,120
注射量 前后的天气峰值，

847
00:33:29,120 --> 00:33:30,880
然后他们测量了

848
00:33:30,880 --> 00:33:32,679
警察的重量变化，这是

849
00:33:32,679 --> 00:33:35,320
我们的呃模型预测的规则，

850
00:33:35,320 --> 00:33:37,799
你会看到它至少在

851
00:33:37,799 --> 00:33:39,559
一阶近似中它给了我们

852
00:33:39,559 --> 00:33:42,080
非常相似的形状，这 也是

853
00:33:42,080 --> 00:33:43,519
有道理的，因为

854
00:33:43,519 --> 00:33:48,120
嗯，注册者希望

855
00:33:48,120 --> 00:33:51,840
在接近视前

856
00:33:51,840 --> 00:33:54,559
和视后尖峰时间时进行最大的改变，

857
00:33:54,559 --> 00:33:56,039
因为这是它最了解视前尖峰时间和后

858
00:33:56,039 --> 00:33:57,919
尖峰时间的地方，因为这是

859
00:33:57,919 --> 00:34:00,240
它最了解的状态

860
00:34:00,240 --> 00:34:02,159
的地方 PO 神经元的自由能

861
00:34:02,159 --> 00:34:05,600
原理实际上会建议这种

862
00:34:05,600 --> 00:34:07,639
锥体

863
00:34:07,639 --> 00:34:11,359
U um 基本上没有任何假设，

864
00:34:11,359 --> 00:34:14,199
但是我们也得到了，因为

865
00:34:14,199 --> 00:34:17,719
我们不仅有一阶

866
00:34:17,719 --> 00:34:20,199
尖峰时间依赖性可塑性规则，而且

867
00:34:20,199 --> 00:34:21,480
我们也有这种依赖性

868
00:34:21,480 --> 00:34:24,119
后放电率我们还可以将

869
00:34:24,119 --> 00:34:26,280
其与其他结果进行比较，这是

870
00:34:26,280 --> 00:34:29,119
gr 和 Brunel 的旧结果，所以这

871
00:34:29,119 --> 00:34:31,159
实际上是一个模型，但非常

872
00:34:31,159 --> 00:34:35,839
详细，嗯描述了嗯

873
00:34:35,839 --> 00:34:37,679
基于前和

874
00:34:37,679 --> 00:34:41,280
后视放电率的呃可塑性 在注册中，

875
00:34:41,280 --> 00:34:42,960
这就是我们的模型预测的结果，因此，

876
00:34:42,960 --> 00:34:44,639
如果我们以不同的速率注入随机的前后

877
00:34:44,639 --> 00:34:47,359
天气 popik tra uh 列车，

878
00:34:47,359 --> 00:34:50,040
我们的模型将预测

879
00:34:50,040 --> 00:34:52,639
这个形状，这又不是完美的

880
00:34:52,639 --> 00:34:55,440
匹配，但考虑到这是一个非常

881
00:34:55,440 --> 00:34:58,359
理想化的模型，它实际上是 至少

882
00:34:58,359 --> 00:35:01,880
主要特征是，呃

883
00:35:01,880 --> 00:35:04,160
视后侧的低放电率会

884
00:35:04,160 --> 00:35:07,560
导致抑郁和更高的增强

885
00:35:07,560 --> 00:35:10,440
嗯，这都反映在

886
00:35:10,440 --> 00:35:14,240
这个好吧呃我假设我还有10

887
00:35:14,240 --> 00:35:19,520
分钟的时间，好吧所以我会嗯给出一个

888
00:35:19,520 --> 00:35:21,320
快速的中间总结，然后我

889
00:35:21,320 --> 00:35:23,400
想要展示一些其他工作，我们

890
00:35:23,400 --> 00:35:27,320
现在实际上将其应用到实际的

891
00:35:27,320 --> 00:35:29,040
机器学习

892
00:35:29,040 --> 00:35:32,440
模型中，所以我们在这里看到的

893
00:35:32,440 --> 00:35:36,760
是，Sy upses 实际上是

894
00:35:36,760 --> 00:35:39,240
非常随机的，这是一个非常

895
00:35:39,240 --> 00:35:42,599
大的难题，我们建议

896
00:35:42,599 --> 00:35:44,160
实际上，

897
00:35:44,160 --> 00:35:47,680
突触噪声

898
00:35:47,680 --> 00:35:50,920
实际上是信号，是

899
00:35:50,920 --> 00:35:52,640
报告其自身对环境的不确定性的方式，

900
00:35:52,640 --> 00:35:54,079
其中环境

901
00:35:54,079 --> 00:35:57,560
实际上是 POS 概要神经元

902
00:35:57,560 --> 00:36:00,119
，它确实以

903
00:36:00,119 --> 00:36:02,160
自由能原理的这种方式与此相互作用

904
00:36:02,160 --> 00:36:03,880
后观视神经元或者这是

905
00:36:03,880 --> 00:36:06,720
描述它的一个非常好的方式，呃，如果

906
00:36:06,720 --> 00:36:09,400
你更感兴趣的话，有一篇

907
00:36:09,400 --> 00:36:13,480
预打印的论文，你可以阅读所有

908
00:36:13,480 --> 00:36:16,359
这些内容，好吧，那么现在这与

909
00:36:16,359 --> 00:36:19,440
神经形态学有何联系，实际上，

910
00:36:19,440 --> 00:36:21,160
我们是 实际上我们并不是在做神经形态

911
00:36:21,160 --> 00:36:23,400
硬件，而是在做神经形态

912
00:36:23,400 --> 00:36:25,920
算法，所以我们尝试将这些

913
00:36:25,920 --> 00:36:27,480
灵感现在带入实际的机器

914
00:36:27,480 --> 00:36:30,319
学习模型中，我们认为这

915
00:36:30,319 --> 00:36:33,920
可能是一个很好的攻击角度来

916
00:36:33,920 --> 00:36:35,599
解决机器学习中众所周知的问题，

917
00:36:35,599 --> 00:36:40,880
所以 我只是嗯嗯，

918
00:36:40,880 --> 00:36:43,640
这里画了一个非常简单的卷积

919
00:36:43,640 --> 00:36:45,440
神经网络，具有各种卷积

920
00:36:45,440 --> 00:36:48,079
层，然后

921
00:36:48,079 --> 00:36:49,839
可能是机器

922
00:36:49,839 --> 00:36:52,760
学习算法中的一些密集层，以及

923
00:36:52,760 --> 00:36:54,599
你们很多人都知道的训练方式，我想是

924
00:36:54,599 --> 00:36:57,400
通过嗯结束 到endend和arrowback

925
00:36:57,400 --> 00:36:59,480
传播，所以我们的想法是，你有

926
00:36:59,480 --> 00:37:02,119
一个训练集，其中有输入和

927
00:37:02,119 --> 00:37:04,079
目标，例如在

928
00:37:04,079 --> 00:37:06,920
分类任务中，这可能是

929
00:37:06,920 --> 00:37:08,680
猫和狗的图片，你的

930
00:37:08,680 --> 00:37:12,440
目标是U类标签，可以这么

931
00:37:12,440 --> 00:37:15,160
说 所以有些实际上有

932
00:37:15,160 --> 00:37:17,119
人工神经元，其中一些

933
00:37:17,119 --> 00:37:18,640
神经元可能对猫活跃

934
00:37:18,640 --> 00:37:20,240
，一个可能对

935
00:37:20,240 --> 00:37:23,440
狗活跃，嗯，在你的训练数据中，

936
00:37:23,440 --> 00:37:25,400
你确切地拥有这些标签，嗯，这些标签是

937
00:37:25,400 --> 00:37:27,119
由人类

938
00:37:27,119 --> 00:37:29,319
坐下来生成的 手动进行此操作，然后

939
00:37:29,319 --> 00:37:32,119
在训练过程中，

940
00:37:32,119 --> 00:37:34,520
通过将

941
00:37:34,520 --> 00:37:37,040
输入从

942
00:37:37,040 --> 00:37:39,760
输入层一直传播到输出层，向网络展示此示例，然后将输出

943
00:37:39,760 --> 00:37:42,560
与

944
00:37:42,560 --> 00:37:44,960
这些手标签目标的呃进行比较，然后

945
00:37:44,960 --> 00:37:47,000
比较 两个

946
00:37:47,000 --> 00:37:48,839
通过所有这些层反向传播回

947
00:37:48,839 --> 00:37:52,000
输入，并且该层内的所有权重或

948
00:37:52,000 --> 00:37:54,720
概要权重

949
00:37:54,720 --> 00:37:57,160


950
00:37:57,160 --> 00:37:58,960
将相应地更新，

951
00:37:58,960 --> 00:38:01,560
以便在执行此操作多次后，

952
00:38:01,560 --> 00:38:03,920
该网络变得擅长告诉

953
00:38:03,920 --> 00:38:05,560
C 部分这样

954
00:38:05,560 --> 00:38:08,520
做，所以这有一个问题，这个

955
00:38:08,520 --> 00:38:10,359
算法在实践中效果很好，

956
00:38:10,359 --> 00:38:12,960
它是

957
00:38:12,960 --> 00:38:15,480
我们讨论过的所有这些模型的基础，例如 D 或 J

958
00:38:15,480 --> 00:38:18,520
gbt，但它效率很低，

959
00:38:18,520 --> 00:38:20,280
问题是这在

960
00:38:20,280 --> 00:38:22,880
文献作为锁定问题，所以如果

961
00:38:22,880 --> 00:38:25,480
你现在将这个网络分成

962
00:38:25,480 --> 00:38:27,560
我之前已经做过的块，但这

963
00:38:27,560 --> 00:38:30,839
是任意的，但为了

964
00:38:30,839 --> 00:38:32,400
根据软件算法有效地实现这一点，

965
00:38:32,400 --> 00:38:34,000
这样做可能会

966
00:38:34,000 --> 00:38:36,520
很有趣，现在你会

967
00:38:36,520 --> 00:38:39,160
想要这些 理想情况下，块可以并行运行，

968
00:38:39,160 --> 00:38:41,560
这样您基本上就可以在第

969
00:38:41,560 --> 00:38:44,359


970
00:38:44,359 --> 00:38:46,440
一个块上显示第一个示例，然后在

971
00:38:46,440 --> 00:38:48,839
第二个块正在执行

972
00:38:48,839 --> 00:38:50,760
其他操作时对其进行训练，但这

973
00:38:50,760 --> 00:38:52,800
对于端对端反向

974
00:38:52,800 --> 00:38:54,440
传播来说是不可能的 由于这个锁定

975
00:38:54,440 --> 00:38:57,200
问题，因为

976
00:38:57,200 --> 00:38:58,800
第二个块的激活取决于

977
00:38:58,800 --> 00:39:00,319
第一个块的激活，因此您必须将

978
00:39:00,319 --> 00:39:02,720
其一直传播到末尾，然后

979
00:39:02,720 --> 00:39:04,319
计算此错误，

980
00:39:04,319 --> 00:39:06,680
然后仅在完成后才进行反向传播

981
00:39:06,680 --> 00:39:09,520
呃，你可以开始下一个纪元，

982
00:39:09,520 --> 00:39:14,359
在那里你展示一堆新的

983
00:39:14,359 --> 00:39:17,040
示例，呃，你会看到，在

984
00:39:17,040 --> 00:39:20,319
这段时间里，

985
00:39:20,319 --> 00:39:23,319
运行第一个块的三个人可能会处于

986
00:39:23,319 --> 00:39:25,520
空闲状态，并且必须一直等待，

987
00:39:25,520 --> 00:39:28,079
并且 这显然使它们非常

988
00:39:28,079 --> 00:39:30,040
低效，现在我的想法是，

989
00:39:30,040 --> 00:39:33,000
我们基本上使用我们

990
00:39:33,000 --> 00:39:36,680
从这个早期模型中学到的关于

991
00:39:36,680 --> 00:39:39,440
符号如何在这些长距离上进行通信的

992
00:39:39,440 --> 00:39:42,079
自由能原理，

993
00:39:42,079 --> 00:39:43,920
并将其应用于深层神经元

994
00:39:43,920 --> 00:39:46,200
网络和你在这里再次拥有的想法，

995
00:39:46,200 --> 00:39:49,040
基本上你已经有了

996
00:39:49,040 --> 00:39:50,880


997
00:39:50,880 --> 00:39:54,800
这一代的输入到某些

998
00:39:54,800 --> 00:39:57,960
输出，但是要使其

999
00:39:57,960 --> 00:39:59,440
适用于自由能

1000
00:39:59,440 --> 00:40:01,359
原理，缺少的是你

1001
00:40:01,359 --> 00:40:03,920
总是需要的反馈，这个想法是我们把

1002
00:40:03,920 --> 00:40:06,960
这里是非常轻量级的反馈

1003
00:40:06,960 --> 00:40:09,480
网络，所以本质上

1004
00:40:09,480 --> 00:40:11,800
这个深度神经网络中的每个块

1005
00:40:11,800 --> 00:40:15,319
都会伴随着一个

1006
00:40:15,319 --> 00:40:18,520
本地生成目标的反馈块，

1007
00:40:18,520 --> 00:40:20,240
所以我们曾经在最简单的

1008
00:40:20,240 --> 00:40:21,760
情况下使用过，所以这是

1009
00:40:21,760 --> 00:40:24,640
最近的 到目前为止，我们只使用线性块，

1010
00:40:24,640 --> 00:40:27,480
所以这些是单个线性层，

1011
00:40:27,480 --> 00:40:29,160
我们现在将

1012
00:40:29,160 --> 00:40:32,040
在这些反馈块中生成这些输出

1013
00:40:32,040 --> 00:40:34,040
，然后使用自由能原理

1014
00:40:34,040 --> 00:40:38,119
来推导局部损失，使

1015
00:40:38,119 --> 00:40:40,920
我们能够再次最小化这两个块

1016
00:40:40,920 --> 00:40:43,000
我们这里的反馈权重以及

1017
00:40:43,000 --> 00:40:45,119
前向网络中的权重，

1018
00:40:45,119 --> 00:40:48,079
所以它本质上是相同的

1019
00:40:48,079 --> 00:40:49,960
想法，所以我们这里有这些输出，我们

1020
00:40:49,960 --> 00:40:52,960
现在将它们解释为

1021
00:40:52,960 --> 00:40:54,800
概率函数的参数，这样我们就可以应用

1022
00:40:54,800 --> 00:40:57,240
这个概率框架，

1023
00:40:57,240 --> 00:40:59,040
但是现在所有 其余的基本上以

1024
00:40:59,040 --> 00:41:01,359
相同的方式推出，所以我们呃假设

1025
00:41:01,359 --> 00:41:04,040
这些输出本质上是

1026
00:41:04,040 --> 00:41:06,599
这个模型的内部状态，并且我们

1027
00:41:06,599 --> 00:41:10,480


1028
00:41:10,480 --> 00:41:14,000
在呃输入和目标中给出了这个呃这个观察结果，

1029
00:41:14,000 --> 00:41:17,880
现在我们尝试基本上最小化呃P

1030
00:41:17,880 --> 00:41:19,640
现在将是这个前馈网络，

1031
00:41:19,640 --> 00:41:23,480
Q 现在将是一个函数，其中

1032
00:41:23,480 --> 00:41:26,319
包含

1033
00:41:26,319 --> 00:41:31,119
反馈和前馈

1034
00:41:31,119 --> 00:41:33,880
网络的 um 特征，好的事情是，如果我们

1035
00:41:33,880 --> 00:41:36,160
um uh 我没有时间进入

1036
00:41:36,160 --> 00:41:38,880
现在详细说明，但如果你写

1037
00:41:38,880 --> 00:41:43,200
出来，你会发现，嗯，这实际上呃

1038
00:41:43,200 --> 00:41:45,079
分解了这个锁定项，

1039
00:41:45,079 --> 00:41:49,200
分解为局部线性项，

1040
00:41:49,200 --> 00:41:50,920
给你这些局部损失，所以

1041
00:41:50,920 --> 00:41:52,960
本质上你可以最小化

1042
00:41:52,960 --> 00:41:55,520
B 和

1043
00:41:55,520 --> 00:41:59,560
相应反馈块之间的局部块 嗯，一个损失

1044
00:41:59,560 --> 00:42:01,839
函数，然后你实际上可以并行执行

1045
00:42:01,839 --> 00:42:05,680
此操作，因为呃，也许

1046
00:42:05,680 --> 00:42:07,800
这里的图片很好看，所以你

1047
00:42:07,800 --> 00:42:09,000
现在要做的就是你有一点

1048
00:42:09,000 --> 00:42:10,720
开销，因为你有这个反馈

1049
00:42:10,720 --> 00:42:15,200
块，所以这将是两个呃 呃

1050
00:42:15,200 --> 00:42:18,359
前馈

1051
00:42:18,359 --> 00:42:20,160
块和反馈

1052
00:42:20,160 --> 00:42:23,359
块的执行时间，但原则上它们可以

1053
00:42:23,359 --> 00:42:25,920
并行运行，一旦前向块

1054
00:42:25,920 --> 00:42:28,400
完成，下一个前向块就可以开始

1055
00:42:28,400 --> 00:42:30,079
通过这个

1056
00:42:30,079 --> 00:42:33,079
网络传播，但是同时已经是

1057
00:42:33,079 --> 00:42:34,880
前向块了，因为它已经

1058
00:42:34,880 --> 00:42:37,880
接收到了 这里的目标可以开始呃

1059
00:42:37,880 --> 00:42:40,520
更新权重，当它完成时，

1060
00:42:40,520 --> 00:42:43,119
它可以在下一个纪元上自由操作，

1061
00:42:43,119 --> 00:42:45,760
所以在这个框架中不再有锁定，

1062
00:42:45,760 --> 00:42:48,040


1063
00:42:48,040 --> 00:42:50,880
好吧，所以我已经完成了，

1064
00:42:50,880 --> 00:42:53,119
我也没有时间了，我想是这样

1065
00:42:53,119 --> 00:42:55,359
这是如何执行的当然我们改变了

1066
00:42:55,359 --> 00:42:56,720
学习算法现在我们

1067
00:42:56,720 --> 00:42:59,720
还必须回去看看这是否

1068
00:42:59,720 --> 00:43:03,280
仍然呃给我们相同的性能

1069
00:43:03,280 --> 00:43:06,480
并且实际上对于嗯所以正如我所说

1070
00:43:06,480 --> 00:43:09,400
这是我们得到的第一个结果

1071
00:43:09,400 --> 00:43:15,559
现在，至少对于

1072
00:43:15,559 --> 00:43:18,280
像 Cypher 10 这样的中型呃数据集来说，这

1073
00:43:18,280 --> 00:43:20,440
似乎实际上表现得非常好，

1074
00:43:20,440 --> 00:43:22,160
所以我们尝试将它用于标准

1075
00:43:22,160 --> 00:43:24,559
架构时尚 mnist 与重置

1076
00:43:24,559 --> 00:43:29,880
15 和 res 18 到目前为止，我们主要工作呃

1077
00:43:29,880 --> 00:43:32,160
像小 像时尚 amist 这样的数据集，

1078
00:43:32,160 --> 00:43:35,160
我们应用自由分割来

1079
00:43:35,160 --> 00:43:37,640
重置 50，随着网络变得更深，我们获得了

1080
00:43:37,640 --> 00:43:40,040
与标准 backrop 基本相同的性能，

1081
00:43:40,040 --> 00:43:42,680
呃，你看，

1082
00:43:42,680 --> 00:43:44,880
实际上，

1083
00:43:44,880 --> 00:43:47,079
我们现在遇到的问题实际上是过度拟合，

1084
00:43:47,079 --> 00:43:48,599
因为我们有这些本地 目标

1085
00:43:48,599 --> 00:43:50,280
似乎这些较小的块实际上

1086
00:43:50,280 --> 00:43:53,079
在某种程度上过度拟合，这

1087
00:43:53,079 --> 00:43:55,800
对于仍然完成像 Cypher t 这样的任务来说并不是那么严重，

1088
00:43:55,800 --> 00:43:58,839
所以我们已经非常接近了，但是如果

1089
00:43:58,839 --> 00:44:01,520
你现在去执行非常大的任务，那么

1090
00:44:01,520 --> 00:44:04,480
仍然缺少一些我们

1091
00:44:04,480 --> 00:44:06,839
需要的东西 就像单次分割一样，我们已经

1092
00:44:06,839 --> 00:44:09,000
到达了那里，但我们还没有

1093
00:44:09,000 --> 00:44:11,559
完全达到反向传播，但

1094
00:44:11,559 --> 00:44:13,119
看到你可以

1095
00:44:13,119 --> 00:44:18,240
将这个嗯这个原则也呃应用到

1096
00:44:18,240 --> 00:44:21,160
这个标准机器学习

1097
00:44:21,160 --> 00:44:23,599
算法中仍然很有趣，好吧这是我的第二个

1098
00:44:23,599 --> 00:44:26,240
总结所以 实际上，我们发现，嗯，

1099
00:44:26,240 --> 00:44:28,040
深 Nur 网络在概率空间的泛化方面出人意料地好，

1100
00:44:28,040 --> 00:44:31,720


1101
00:44:31,720 --> 00:44:33,839
这就是这项工作实际上是如何

1102
00:44:33,839 --> 00:44:36,760
开始的，嗯，我们的想法是

1103
00:44:36,760 --> 00:44:39,920
利用这个嗯，并利用它以与

1104
00:44:39,920 --> 00:44:41,680


1105
00:44:41,680 --> 00:44:44,040
嗯相同的方式分配学习 在第一个项目中，我向

1106
00:44:44,040 --> 00:44:48,000
您展示了

1107
00:44:48,000 --> 00:44:50,359
通过生成这些

1108
00:44:50,359 --> 00:44:52,160
反馈

1109
00:44:52,160 --> 00:44:55,760
网络来解决这个学分分配问题，嗯，是的，基本上就是这样，然后

1110
00:44:55,760 --> 00:44:58,680
嗯，我想感谢我的

1111
00:44:58,680 --> 00:45:01,000
同事和我的学生，所以我有两个

1112
00:45:01,000 --> 00:45:05,480
非常好的嗯嗯博士生 KH 和

1113
00:45:05,480 --> 00:45:08,400
cabel 现在在 bom，正在

1114
00:45:08,400 --> 00:45:11,680
研究这个主题，

1115
00:45:11,680 --> 00:45:14,760
我展示的第一个项目是 B，

1116
00:45:14,760 --> 00:45:17,640
当我在古坦时与 Christian TL 一起做的，

1117
00:45:17,640 --> 00:45:20,559
呃这个项目，第二个项目

1118
00:45:20,559 --> 00:45:22,800
是我的工作，我与他们密切合作

1119
00:45:22,800 --> 00:45:24,880
克里斯蒂安·米亚和阿南·

1120
00:45:24,880 --> 00:45:26,599
苏普伦，

1121
00:45:26,599 --> 00:45:29,760
是的，谢谢

1122
00:45:30,520 --> 00:45:32,920
你，非常感谢大卫，这

1123
00:45:32,920 --> 00:45:35,040
绝对令人着迷，我认为

1124
00:45:35,040 --> 00:45:37,559
这种模型与

1125
00:45:37,559 --> 00:45:39,599
生物学的匹配程度令人难以置信，就像考虑到你

1126
00:45:39,599 --> 00:45:41,359
从第一原理中得出它一样，我认为

1127
00:45:41,359 --> 00:45:43,760
这真的很酷，嗯，我确实有一个

1128
00:45:43,760 --> 00:45:45,800
问题 就

1129
00:45:45,800 --> 00:45:47,720
你谈到的关于

1130
00:45:47,720 --> 00:45:49,760
卷积网络的最后一点，你知道你

1131
00:45:49,760 --> 00:45:51,079
说传统上必须一路端到端，

1132
00:45:51,079 --> 00:45:53,400
这确实效率低下

1133
00:45:53,400 --> 00:45:54,960
，然后你展示了你

1134
00:45:54,960 --> 00:45:57,240
得到的结果，你是否查看了能耗

1135
00:45:57,240 --> 00:46:00,839
和你的一样，呃，还没有，所以我们

1136
00:46:00,839 --> 00:46:02,520
有，呃，我们实际上正在

1137
00:46:02,520 --> 00:46:06,760
研究一个，所以这个，嗯，

1138
00:46:06,760 --> 00:46:09,040
在标准机器学习中实现这些东西实际上并不那么容易，

1139
00:46:09,040 --> 00:46:10,880


1140
00:46:10,880 --> 00:46:14,319
我们有一个所以卡尔目前正在

1141
00:46:14,319 --> 00:46:17,400
研究这个，呃，博士 Dron 的学生

1142
00:46:17,400 --> 00:46:20,160
嗯，他现在已经有了一个实现，

1143
00:46:20,160 --> 00:46:22,359
他现在正在评估

1144
00:46:22,359 --> 00:46:25,520
我们在实践中如何充分利用这个

1145
00:46:25,520 --> 00:46:28,359
parization，但我们

1146
00:46:28,359 --> 00:46:32,119
实际上非常有信心这个

1147
00:46:32,119 --> 00:46:36,640
嗯是为了瘫痪它呃

1148
00:46:36,640 --> 00:46:38,960
应该在那里，问题是多少 你

1149
00:46:38,960 --> 00:46:41,400
节省了能源，因为嗯，对于

1150
00:46:41,400 --> 00:46:43,440
我们现在使用的这些较小规模的模型，

1151
00:46:43,440 --> 00:46:47,720
重置18重置15，效果

1152
00:46:47,720 --> 00:46:50,319
可能不会那么大，所以一旦我们将

1153
00:46:50,319 --> 00:46:52,000
其提升到真正更大的模型，

1154
00:46:52,000 --> 00:46:54,400
效果应该会更大，但是是的，这是

1155
00:46:54,400 --> 00:46:56,480
正在进行的工作

1156
00:46:56,480 --> 00:46:58,920
非常酷，谢谢，嗯，然后我

1157
00:46:58,920 --> 00:47:00,760
也想知道，像这样的

1158
00:47:00,760 --> 00:47:03,119
局部误差反向传播是

1159
00:47:03,119 --> 00:47:04,960
其他人尝试过的

1160
00:47:04,960 --> 00:47:06,520
卷积神经网络的方法，

1161
00:47:06,520 --> 00:47:08,880
还是这是一种相当新的

1162
00:47:08,880 --> 00:47:11,680
实现方法，呃，有很多 这样

1163
00:47:11,680 --> 00:47:14,280
做的方法所以呃有

1164
00:47:14,280 --> 00:47:16,000
例如我的意思是我猜最接近的

1165
00:47:16,000 --> 00:47:18,040
是目标传播，它已经被

1166
00:47:18,040 --> 00:47:20,440
提出，它本质上使用随机

1167
00:47:20,440 --> 00:47:23,640
反馈权重呃

1168
00:47:23,640 --> 00:47:25,359
在这里反向传播，所以这些人不会接受

1169
00:47:25,359 --> 00:47:27,720
训练，

1170
00:47:28,000 --> 00:47:32,520
据我所知。

1171
00:47:32,520 --> 00:47:35,559
这对于小规模的呃问题也很有效，

1172
00:47:35,559 --> 00:47:38,160
但据我所知，

1173
00:47:38,160 --> 00:47:40,280
即使对于

1174
00:47:40,280 --> 00:47:42,319
Cipher 10，它们也表现不佳，它们已经开始

1175
00:47:42,319 --> 00:47:43,599
崩溃，因为我认为这些随机

1176
00:47:43,599 --> 00:47:46,160
反馈权重的近似值太粗略了

1177
00:47:46,160 --> 00:47:48,559
这是

1178
00:47:48,559 --> 00:47:52,760
第一个，好吧，也许我必须小心，

1179
00:47:52,760 --> 00:47:56,079
我认为这是第一个

1180
00:47:56,079 --> 00:47:58,240
允许你训练反馈权

1181
00:47:58,240 --> 00:48:01,200
重的方法，它不是对比方法，

1182
00:48:01,200 --> 00:48:03,079
所以有很多方法使用

1183
00:48:03,079 --> 00:48:05,559
对比步骤，

1184
00:48:05,559 --> 00:48:08,040
所以你有 也许看到了这个前

1185
00:48:08,040 --> 00:48:10,800
向算法和所有这些

1186
00:48:10,800 --> 00:48:13,040
东西，但他们必须做的始终

1187
00:48:13,040 --> 00:48:14,830
是他们发送

1188
00:48:14,830 --> 00:48:16,160
[音乐]

1189
00:48:16,160 --> 00:48:19,319
呃呃他们发送实际的输入数据

1190
00:48:19,319 --> 00:48:22,480
，然后他们发送一种反

1191
00:48:22,480 --> 00:48:26,520
输入，所以一个反输入 呃

1192
00:48:26,520 --> 00:48:29,000
通常是人工生成的，所以他们对

1193
00:48:29,000 --> 00:48:31,040
输入进行一些扭曲以使其

1194
00:48:31,040 --> 00:48:33,839
呃，然后他们进行训练，他们必须在

1195
00:48:33,839 --> 00:48:36,960
本地使用这两种信息呃来

1196
00:48:36,960 --> 00:48:38,640
进行更新，因此网络必须将

1197
00:48:38,640 --> 00:48:41,440
输入和反输入保留在内存中

1198
00:48:41,440 --> 00:48:44,440
响应，这

1199
00:48:44,440 --> 00:48:48,200
使得嗯，这使得这种方法

1200
00:48:48,200 --> 00:48:50,880
有点难以并行化，

1201
00:48:50,880 --> 00:48:54,280
这里的好处是我们推导出一个

1202
00:48:54,280 --> 00:48:57,440
呃变分

1203
00:48:57,440 --> 00:49:00,960
自由能损失的上限，可以

1204
00:49:00,960 --> 00:49:03,680
通过前向传播完全阐明

1205
00:49:03,680 --> 00:49:05,960
并且 我认为这是新的，所以这是

1206
00:49:05,960 --> 00:49:08,480
这个[音乐]的新部分，

1207
00:49:08,480 --> 00:49:11,550


1208
00:49:11,680 --> 00:49:15,520


1209
00:49:15,720 --> 00:49:19,720
非常棒，是的，很少有评论，

1210
00:49:19,720 --> 00:49:21,640
在

1211
00:49:21,640 --> 00:49:24,280
你们的两次演讲之间，这至少对我来说是一个新的

1212
00:49:24,280 --> 00:49:25,880
区别，这就是

1213
00:49:25,880 --> 00:49:27,319
区别 神经形态

1214
00:49:27,319 --> 00:49:29,920
硬件和神经形态

1215
00:49:29,920 --> 00:49:33,640
算法之间的关系，所以这不仅仅是新

1216
00:49:33,640 --> 00:49:35,520
硬件或

1217
00:49:35,520 --> 00:49:39,240
湿衣的问题，尽管很高兴

1218
00:49:39,240 --> 00:49:40,760
看到它几乎就像在

1219
00:49:40,760 --> 00:49:45,839


1220
00:49:45,839 --> 00:49:47,599


1221
00:49:47,599 --> 00:49:51,440
我们今天拥有的硬件上使用算法的中间步骤或桥梁步骤一样，

1222
00:49:51,440 --> 00:49:54,160
就像莎拉提到的

1223
00:49:54,160 --> 00:49:56,240
尖峰一样

1224
00:49:56,240 --> 00:49:59,880
适合 GPU 的神经网络或仅使用

1225
00:49:59,880 --> 00:50:04,079
标准化的 um CPU 多核调度

1226
00:50:04,079 --> 00:50:07,000
方法，您已经可以使用

1227
00:50:07,000 --> 00:50:09,799
我们

1228
00:50:09,799 --> 00:50:13,559
使用神经形态算法做更多的事情，

1229
00:50:13,559 --> 00:50:18,640
因此这不仅仅是一个材料科学

1230
00:50:18,640 --> 00:50:22,040
主题，而且在

1231
00:50:22,040 --> 00:50:24,640
真正的微观尺度上还有很多我们需要做的事情。 可以学习

1232
00:50:24,640 --> 00:50:26,920
与噪声处理调度相关的知识

1233
00:50:26,920 --> 00:50:28,599
，甚至在更高的

1234
00:50:28,599 --> 00:50:30,559
抽象层次上也可能从

1235
00:50:30,559 --> 00:50:32,720
仿生学和认知系统中学习，

1236
00:50:32,720 --> 00:50:34,839
但这

1237
00:50:34,839 --> 00:50:37,240
对我来说是一个区别，是的，

1238
00:50:37,240 --> 00:50:40,839
所以也许要添加嗯是的，所以我

1239
00:50:40,839 --> 00:50:43,720
认为问题变成了

1240
00:50:43,720 --> 00:50:46,640
现在确实更加紧迫，因为这些呃

1241
00:50:46,640 --> 00:50:49,000
神经形态呃设备变得更加

1242
00:50:49,000 --> 00:50:51,119


1243
00:50:51,119 --> 00:50:53,839
成熟，硬件设备变得更加成熟，它们通常不能真正

1244
00:50:53,839 --> 00:50:57,520
在这些标准

1245
00:50:57,520 --> 00:50:59,119
机器学习算法上发挥作用，因为它们

1246
00:50:59,119 --> 00:51:01,280
确实针对

1247
00:51:01,280 --> 00:51:04,319
GPU 进行了优化，所以你需要考虑

1248
00:51:04,319 --> 00:51:06,079
一下 你必须退后一步，重新思考

1249
00:51:06,079 --> 00:51:08,680
算法方面，

1250
00:51:08,680 --> 00:51:11,960
才能真正充分利用它们，

1251
00:51:11,960 --> 00:51:13,960
正如你所看到的，

1252
00:51:13,960 --> 00:51:16,480
我们与 Maya 教授合作，

1253
00:51:16,480 --> 00:51:18,839
他正在 Dron 中做 Spiner 芯片，

1254
00:51:18,839 --> 00:51:20,720
但还有其他 方法，

1255
00:51:20,720 --> 00:51:23,359
就像莎拉提到的英特尔的 L 芯片

1256
00:51:23,359 --> 00:51:26,319
等等，嗯，他们

1257
00:51:26,319 --> 00:51:29,520
现在真的在研究这个，他们也

1258
00:51:29,520 --> 00:51:31,960
从算法

1259
00:51:31,960 --> 00:51:34,520
方面，我发现它非常有用，当我思考时，它

1260
00:51:34,520 --> 00:51:36,520
有点像一种心态或心理框架

1261
00:51:36,520 --> 00:51:38,839
计算机或人工智能

1262
00:51:38,839 --> 00:51:39,760
和我，这可能是因为我是一名

1263
00:51:39,760 --> 00:51:40,920
神经科学家，但我也必须将

1264
00:51:40,920 --> 00:51:43,040
其转化为大脑如何

1265
00:51:43,040 --> 00:51:44,680
工作，大脑中的信息处理如何

1266
00:51:44,680 --> 00:51:46,880
工作等，以及当我

1267
00:51:46,880 --> 00:51:48,480
接触计算机科学和人工智能时

1268
00:51:48,480 --> 00:51:49,920
在神经科学之后，我发现自己

1269
00:51:49,920 --> 00:51:51,720
很自然地翻译它，但我觉得

1270
00:51:51,720 --> 00:51:53,760
这个框架最终是理解计算的一种非常有用的

1271
00:51:53,760 --> 00:51:56,000
方式，

1272
00:51:56,000 --> 00:51:57,319
因为正如我所说，我们的大脑

1273
00:51:57,319 --> 00:51:59,280
就是这些巨大的超级计算机，

1274
00:51:59,280 --> 00:52:01,079
而我不断地在计算机上阅读论文

1275
00:52:01,079 --> 00:52:02,599
科学或其他什么，

1276
00:52:02,599 --> 00:52:04,680
然后一旦你可以

1277
00:52:04,680 --> 00:52:07,520
真正概念化任何东西，以及它的

1278
00:52:07,520 --> 00:52:09,880
神经拟态性有多接近，然后如果你

1279
00:52:09,880 --> 00:52:11,440
开始思考如何

1280
00:52:11,440 --> 00:52:12,520
调整它，使其稍微更加

1281
00:52:12,520 --> 00:52:14,079
神经拟态，然后会给

1282
00:52:14,079 --> 00:52:15,839
你带来这些收获 我们对

1283
00:52:15,839 --> 00:52:17,040
大脑的了解是，它会给你

1284
00:52:17,040 --> 00:52:19,319
一些额外的并行计算，

1285
00:52:19,319 --> 00:52:20,559
还是会给你一些能源

1286
00:52:20,559 --> 00:52:23,480
效率，所以是的，我发现它像

1287
00:52:23,480 --> 00:52:24,599
定义一样，当我查看

1288
00:52:24,599 --> 00:52:26,000
定义时，我认为这真的

1289
00:52:26,000 --> 00:52:27,520
取决于 你的权利是一个定义，

1290
00:52:27,520 --> 00:52:30,079
因为它目前也是一个充满活力的领域，

1291
00:52:30,079 --> 00:52:32,640
嗯，我认为它会一直

1292
00:52:32,640 --> 00:52:34,079
在变化，并且将会

1293
00:52:34,079 --> 00:52:35,559
发生变化，但对我来说，我觉得

1294
00:52:35,559 --> 00:52:37,680
神经形态更像是一个框架，就像一个

1295
00:52:37,680 --> 00:52:39,559
心理框架，我在其中

1296
00:52:39,559 --> 00:52:42,240
通过

1297
00:52:42,400 --> 00:52:44,119
概念化来看待事物是的，我认为它

1298
00:52:44,119 --> 00:52:47,920
也没有很好的定义，我的意思是呃，

1299
00:52:47,920 --> 00:52:50,319
你也提到，实际上

1300
00:52:50,319 --> 00:52:52,359
人工神经元网络是一个

1301
00:52:52,359 --> 00:52:54,880
神经形态的呃概念，如果你愿意的话，

1302
00:52:54,880 --> 00:52:55,960
它们

1303
00:52:55,960 --> 00:52:58,119
从第一天就开始了，它实际上是

1304
00:52:58,119 --> 00:53:00,160
一个它是一个 如果

1305
00:53:00,160 --> 00:53:02,400
你看看 90 年代左右，当这些

1306
00:53:02,400 --> 00:53:04,960
支持向量机和

1307
00:53:04,960 --> 00:53:08,200
这些替代模型出现时，这是一个巨大的成功故事，但

1308
00:53:08,200 --> 00:53:10,920
它们都没有比

1309
00:53:10,920 --> 00:53:13,440
新的神经拟态方法更长寿，所以呃，它

1310
00:53:13,440 --> 00:53:14,880
实际上非常好，但仍然有

1311
00:53:14,880 --> 00:53:17,040
这个交流社区 认为

1312
00:53:17,040 --> 00:53:19,119
大脑中有更多的功能

1313
00:53:19,119 --> 00:53:22,400
需要放入呃才能得到

1314
00:53:22,400 --> 00:53:23,599
真实的

1315
00:53:23,599 --> 00:53:26,960
东西是的所以我认为这有点，

1316
00:53:26,960 --> 00:53:30,799


1317
00:53:30,799 --> 00:53:33,040
实际上这不是一个定义非常明确的术语，我认为根据你的研究

1318
00:53:33,040 --> 00:53:34,799
几乎就像

1319
00:53:34,799 --> 00:53:36,440
我见过的人们在看它的最小水平，我

1320
00:53:36,440 --> 00:53:37,880
不知道你是否见过其他东西，

1321
00:53:37,880 --> 00:53:39,520
但我们不仅仅是在谈论

1322
00:53:39,520 --> 00:53:41,400
自由能和主动推理的细胞水平，

1323
00:53:41,400 --> 00:53:43,040
我们是 实际上谈论的

1324
00:53:43,040 --> 00:53:46,599
是细胞结构，然后像这样

1325
00:53:46,599 --> 00:53:48,319
然后你想好它有多小

1326
00:53:48,319 --> 00:53:49,960
我们要谈论你最终知道的细胞亚细胞

1327
00:53:49,960 --> 00:53:51,559
结构，

1328
00:53:51,559 --> 00:53:53,559
就像线粒体以

1329
00:53:53,559 --> 00:53:55,160
与复合隔室类似的方式使用自由能，

1330
00:53:55,160 --> 00:53:56,760
是的，所以我想

1331
00:53:56,760 --> 00:53:57,920
实际上 ' 有趣的是，了解你的

1332
00:53:57,920 --> 00:53:59,960
想法，大卫，你知道你

1333
00:53:59,960 --> 00:54:02,520
一开始就谈到了注册是如何

1334
00:54:02,520 --> 00:54:05,319
划分的，嗯，你是否在一个注册中的不同隔间中

1335
00:54:05,319 --> 00:54:07,599
对此进行了不同类型的实例化，

1336
00:54:07,599 --> 00:54:09,480


1337
00:54:09,480 --> 00:54:11,480
就像你想看一下一样

1338
00:54:11,480 --> 00:54:13,359
那个粒度级别

1339
00:54:13,359 --> 00:54:14,880
还是现在更多的是把

1340
00:54:14,880 --> 00:54:16,079
你从中学到的东西带回到

1341
00:54:16,079 --> 00:54:17,760
我们如何才能

1342
00:54:17,760 --> 00:54:19,440
让人工智能更

1343
00:54:19,440 --> 00:54:21,920
高效呃是的，我们

1344
00:54:21,920 --> 00:54:24,240
现在正在朝这个方向走得更多，因为我们看到了

1345
00:54:24,240 --> 00:54:28,040
我们如何将其重新构建到人工智能模型中，

1346
00:54:28,040 --> 00:54:31,359
所以我认为自由能，所以

1347
00:54:31,359 --> 00:54:32,960
在使用自由能原理时必须小心一点，

1348
00:54:32,960 --> 00:54:34,839
因为它是一个非常

1349
00:54:34,839 --> 00:54:36,880
强大的通用框架，你

1350
00:54:36,880 --> 00:54:40,160
可以将它应用于基本上

1351
00:54:40,160 --> 00:54:41,359
[音乐]

1352
00:54:41,359 --> 00:54:45,680
任何东西 嗯，你不一定

1353
00:54:45,680 --> 00:54:49,440
会在最后得到一个有用的结果，只需

1354
00:54:49,440 --> 00:54:53,680
应用这个，

1355
00:54:53,680 --> 00:54:56,119
我们基本上我的意思是，这

1356
00:54:56,119 --> 00:54:58,040
实际上是作为一个副项目开始的，这是我的

1357
00:54:58,040 --> 00:55:03,760
一种呃 coid 嗯流行病锁定嗯

1358
00:55:03,760 --> 00:55:06,480
项目，我只是 对此感到好奇，

1359
00:55:06,480 --> 00:55:09,119
嗯，你是否真的可以

1360
00:55:09,119 --> 00:55:10,680
解决这个问题，因为我认为注册

1361
00:55:10,680 --> 00:55:12,839
可能足够简单，因为

1362
00:55:12,839 --> 00:55:15,520
当你进入论文时，嗯，

1363
00:55:15,520 --> 00:55:17,440
他们有时会进入一些

1364
00:55:17,440 --> 00:55:19,760
近似值，他们通常会做一些嗯平均场，

1365
00:55:19,760 --> 00:55:22,960
所以他们 选择第一种模式，

1366
00:55:22,960 --> 00:55:25,240
然后你可以解决这些更

1367
00:55:25,240 --> 00:55:27,039
复杂的问题，即使对于神经元来说，

1368
00:55:27,039 --> 00:55:29,119
实际上很难，如果你进入神经元或

1369
00:55:29,119 --> 00:55:31,160
网络级别，这很难，这是

1370
00:55:31,160 --> 00:55:35,160
非常复杂的数学，但对于正弦

1371
00:55:35,160 --> 00:55:36,599
实际上足够简单，所以你

1372
00:55:36,599 --> 00:55:38,559
实际上可以做

1373
00:55:38,559 --> 00:55:39,880
如果你做出正确的

1374
00:55:39,880 --> 00:55:42,640
假设，并且真的只是

1375
00:55:42,640 --> 00:55:44,559
导出了这些东西，那就是

1376
00:55:44,559 --> 00:55:47,079


1377
00:55:47,079 --> 00:55:52,480
一种我我参与的游戏，

1378
00:55:52,480 --> 00:55:55,240
然后结果证明它运行得

1379
00:55:55,240 --> 00:55:58,720
很好，我认为最后 是的，非常

1380
00:55:58,720 --> 00:56:01,200
嗯，我不确定你是否喜欢

1381
00:56:01,200 --> 00:56:02,960
线粒体，所以我确信你可以

1382
00:56:02,960 --> 00:56:04,640
应用相同的原则，但我

1383
00:56:04,640 --> 00:56:08,599
不确定你得到的结果是否

1384
00:56:08,599 --> 00:56:11,079
有意义，或者是否会以

1385
00:56:11,079 --> 00:56:12,240
任何

1386
00:56:12,240 --> 00:56:14,960
方式帮助你，那就是 这始终是

1387
00:56:14,960 --> 00:56:16,839
你投入大量时间的风险，

1388
00:56:16,839 --> 00:56:18,839
最后你得到了一些结果，但

1389
00:56:18,839 --> 00:56:19,240
你不知道

1390
00:56:19,240 --> 00:56:20,440
[音乐]

1391
00:56:20,440 --> 00:56:23,000
这也是我

1392
00:56:23,000 --> 00:56:25,599
认为非常有趣的事情，

1393
00:56:25,599 --> 00:56:28,720
突触是代理，这很

1394
00:56:28,720 --> 00:56:31,000
容易想到 哦，我们首先将制作一个基于代理的

1395
00:56:31,000 --> 00:56:34,240
神经系统模型，该模型

1396
00:56:34,240 --> 00:56:36,480
往往不包括 Gia 或非神经

1397
00:56:36,480 --> 00:56:39,240
细胞类型，但这几乎就像一个双重

1398
00:56:39,240 --> 00:56:41,480
不容置疑的假设，即细胞

1399
00:56:41,480 --> 00:56:43,200
将是

1400
00:56:43,200 --> 00:56:48,079
代理，但这是一个伟大的转变

1401
00:56:48,079 --> 00:56:50,319
从把球扔过

1402
00:56:50,319 --> 00:56:53,440
墙的人那里，这是一种以行动为中心的

1403
00:56:53,440 --> 00:56:56,000
方法，你只能看到部分

1404
00:56:56,000 --> 00:56:57,960


1405
00:56:57,960 --> 00:57:01,079
后果，然后这就是

1406
00:57:01,079 --> 00:57:03,640
突触

1407
00:57:03,640 --> 00:57:06,599
以不同方式发现自己所处的确切场景，或者它可能已经被

1408
00:57:06,599 --> 00:57:09,640
设置为 神经元是

1409
00:57:09,640 --> 00:57:12,319
我们正在构建地图而不是

1410
00:57:12,319 --> 00:57:15,119
领土的代理，所以就像你

1411
00:57:15,119 --> 00:57:17,000
说的自由能原理一样，它是

1412
00:57:17,000 --> 00:57:19,359


1413
00:57:19,359 --> 00:57:22,720
一切的原则，所以只是

1414
00:57:22,720 --> 00:57:25,440
对事物做出原则性的陈述

1415
00:57:25,440 --> 00:57:27,319
就是

1416
00:57:27,319 --> 00:57:30,119
赌注，然后我想我对

1417
00:57:30,119 --> 00:57:33,079
你的问题是那么什么是 让它有用，或者

1418
00:57:33,079 --> 00:57:35,119
在你的学习和修改

1419
00:57:35,119 --> 00:57:37,880
这些模型中，

1420
00:57:37,880 --> 00:57:39,760
你应用自由能

1421
00:57:39,760 --> 00:57:42,000
原理或主动推理的情况有什么不同，你

1422
00:57:42,000 --> 00:57:44,079
觉得它

1423
00:57:44,079 --> 00:57:46,440
为你的研究方向和

1424
00:57:46,440 --> 00:57:47,960
你所玩的地方做出了贡献，而且感觉

1425
00:57:47,960 --> 00:57:50,119
很好 从

1426
00:57:50,119 --> 00:57:52,480
逻辑上讲，我认为自由能

1427
00:57:52,480 --> 00:57:55,000
原理在

1428
00:57:55,000 --> 00:57:58,160
您拥有不完整

1429
00:57:58,160 --> 00:58:01,440
信息的情况下是有意义的，因此，

1430
00:58:01,440 --> 00:58:03,039
例如在注册案例中，

1431
00:58:03,039 --> 00:58:05,520
我们开始的问题是

1432
00:58:05,520 --> 00:58:07,039
注册必须解决的问题

1433
00:58:07,039 --> 00:58:09,039
它对细胞体状态的信息不完整，

1434
00:58:09,039 --> 00:58:11,760


1435
00:58:11,760 --> 00:58:15,359
因为它只看到这种呃，

1436
00:58:15,359 --> 00:58:17,200
或者

1437
00:58:17,200 --> 00:58:19,400
至少是模型的假设，而且我们

1438
00:58:19,400 --> 00:58:22,119
从实验者那里得到的信息是，他们

1439
00:58:22,119 --> 00:58:23,359
本质上只看到这种特殊的

1440
00:58:23,359 --> 00:58:25,160
传播行为 潜力所以看到一个

1441
00:58:25,160 --> 00:58:28,440
关于关于 Soma 状态的单一二元变量

1442
00:58:28,440 --> 00:58:31,960


1443
00:58:31,960 --> 00:58:35,440
所以本质上这是一个

1444
00:58:35,440 --> 00:58:39,480
不完整的呃信息的问题，也是

1445
00:58:39,480 --> 00:58:42,720
你需要一个代理的第二个要素

1446
00:58:42,720 --> 00:58:45,599
，你需要某种形式的代理

1447
00:58:45,599 --> 00:58:46,960
我认为如果你 将自由能

1448
00:58:46,960 --> 00:58:50,119
原理应用于没有代理的系统，

1449
00:58:50,119 --> 00:58:52,440
因此，如果某些东西不与

1450
00:58:52,440 --> 00:58:55,440
闭环中的环境交互，

1451
00:58:55,440 --> 00:58:57,880
那么它就变得非常粗略，我

1452
00:58:57,880 --> 00:59:00,480
认为这个模型

1453
00:59:00,480 --> 00:59:03,000
在出现时已经处于边缘，因为

1454
00:59:03,000 --> 00:59:05,319
这些模型不' 确实没有

1455
00:59:05,319 --> 00:59:07,079
一个机构，但他们至少产生了一个

1456
00:59:07,079 --> 00:59:09,359
输出权，所以你仍然可以将

1457
00:59:09,359 --> 00:59:11,839
其视为与

1458
00:59:11,839 --> 00:59:14,760
环境的交互，但一旦你失去了一些，

1459
00:59:14,760 --> 00:59:17,599
我认为就会有更简单的

1460
00:59:17,599 --> 00:59:21,039
模型可以为你提供

1461
00:59:22,400 --> 00:59:26,119
是的，

1462
00:59:26,119 --> 00:59:27,839
实际上在注册

1463
00:59:27,839 --> 00:59:31,319
案例中，代理机构只是这个，这

1464
00:59:31,319 --> 00:59:33,880
实际上在模型中添加了噪音，

1465
00:59:33,880 --> 00:59:35,240
因为注册是在

1466
00:59:35,240 --> 00:59:38,640
概要上触发 PR，然后它添加了这个，它

1467
00:59:38,640 --> 00:59:40,520
使用其内部状态来添加

1468
00:59:40,520 --> 00:59:41,480
适量的

1469
00:59:41,480 --> 00:59:44,000
噪音，这 可能已经是

1470
00:59:44,000 --> 00:59:45,710
你可以

1471
00:59:45,710 --> 00:59:48,799
[音乐]

1472
00:59:48,799 --> 00:59:51,119
想象莎拉你想问一个问题

1473
00:59:51,119 --> 00:59:52,599
或者我可以问一个

1474
00:59:52,599 --> 00:59:54,920
问题的最小机构，嗯，是的，我更多的是一个

1475
00:59:54,920 --> 00:59:56,160
评论，就像我认为这很

1476
00:59:56,160 --> 00:59:58,280
有趣，就像人们谈论

1477
00:59:58,280 --> 01:00:00,440
生物学一样，有些人说这不是真的

1478
01:00:00,440 --> 01:00:02,400
科学，因为它都是混乱和嘈杂的，

1479
01:00:02,400 --> 01:00:04,359
但我认为它的工作原理非常

1480
01:00:04,359 --> 01:00:06,039
有趣，因为就像你说的，

1481
01:00:06,039 --> 01:00:07,559
突触噪声实际上是对

1482
01:00:07,559 --> 01:00:10,000
不确定性的报告，所以在

1483
01:00:10,000 --> 01:00:11,480
本质上，它实际上可能相当

1484
01:00:11,480 --> 01:00:13,880
准确地报告和混乱的

1485
01:00:13,880 --> 01:00:15,359
世界，而不是生物学本身

1486
01:00:15,359 --> 01:00:16,920
一切都很混乱，但这正是

1487
01:00:16,920 --> 01:00:19,359
我在想的，嗯，是的，我

1488
01:00:19,359 --> 01:00:20,480
想我也很好奇，就像你

1489
01:00:20,480 --> 01:00:21,880
说的那样，这就像你的封锁项目，

1490
01:00:21,880 --> 01:00:23,920
但我只是对你是

1491
01:00:23,920 --> 01:00:25,960
如何得出自由能原理感兴趣的，

1492
01:00:25,960 --> 01:00:27,359
你是如何遇到的 这是你

1493
01:00:27,359 --> 01:00:29,240
已经非常熟悉的东西，或者

1494
01:00:29,240 --> 01:00:30,960
你的一些网络或同行正在

1495
01:00:30,960 --> 01:00:32,359
谈论它，或者你是

1496
01:00:32,359 --> 01:00:36,000
在一篇论文中偶然发现它的，嗯，我的意思是，

1497
01:00:36,000 --> 01:00:37,839
当我在攻读博士学位时，

1498
01:00:37,839 --> 01:00:40,920
我对变分方法感兴趣

1499
01:00:40,920 --> 01:00:44,319
和概率方法，嗯，

1500
01:00:44,319 --> 01:00:46,680
然后我开始阅读这个，所以

1501
01:00:46,680 --> 01:00:49,440
我读了一堆嗯克里斯坦斯论文，

1502
01:00:49,440 --> 01:00:53,079
我发现这很有趣，

1503
01:00:53,079 --> 01:00:57,319
嗯实际上我的博士导师总是

1504
01:00:57,319 --> 01:01:00,280
鼓励我不要不要朝

1505
01:01:00,280 --> 01:01:01,520
这个

1506
01:01:01,520 --> 01:01:03,880
方向走，然后之后 我完成了

1507
01:01:03,880 --> 01:01:05,799
博士学位，我想现在可以了，我可以做

1508
01:01:05,799 --> 01:01:08,079
我想做的事，我尝试一下，

1509
01:01:08,079 --> 01:01:10,039


1510
01:01:10,039 --> 01:01:12,400
是的，然后我想你认为这

1511
01:01:12,400 --> 01:01:15,319
对于

1512
01:01:15,319 --> 01:01:17,400
你或实际尝试

1513
01:01:17,400 --> 01:01:19,799
实施这一点的领域来说是否值得下一步，你知道也许 一些正在

1514
01:01:19,799 --> 01:01:21,960


1515
01:01:21,960 --> 01:01:23,400
该领域构建的更多模拟芯片，例如模拟

1516
01:01:23,400 --> 01:01:25,119
神经形态芯片，我知道您可以

1517
01:01:25,119 --> 01:01:28,200
拥有一些突触动态突触之类的

1518
01:01:28,200 --> 01:01:29,799
东西，您认为

1519
01:01:29,799 --> 01:01:31,839
尝试实现自己的

1520
01:01:31,839 --> 01:01:34,839
硬硬件或您的东西是否值得？ 对此的想法，

1521
01:01:34,839 --> 01:01:37,119
嗯，我的意思是，

1522
01:01:37,119 --> 01:01:39,799
我展示的第一个作品中出现的三重规则，我

1523
01:01:39,799 --> 01:01:41,079
认为实现它会很有趣，

1524
01:01:41,079 --> 01:01:45,200
嗯，这是一个很好的功能，

1525
01:01:45,200 --> 01:01:47,440
原则上它应该是

1526
01:01:47,440 --> 01:01:49,359
这样的，应该具有这种自我

1527
01:01:49,359 --> 01:01:52,319
稳定能力 图片因为它真的

1528
01:01:52,319 --> 01:01:56,640
模仿了细胞膜的动力学，

1529
01:01:56,640 --> 01:01:59,839
所以如果

1530
01:01:59,839 --> 01:02:02,640
神经形态硬件

1531
01:02:02,640 --> 01:02:04,640
会

1532
01:02:04,640 --> 01:02:08,079
呃，如果注册中的模型

1533
01:02:08,079 --> 01:02:10,440
和神经元模型匹配得

1534
01:02:10,440 --> 01:02:13,680
很好，模型应该呃给你 这个

1535
01:02:13,680 --> 01:02:16,400
很好的呃自稳定功能，这样

1536
01:02:16,400 --> 01:02:19,160
神经元真的不会进入某些

1537
01:02:19,160 --> 01:02:20,880
癫痫状态，你可以

1538
01:02:20,880 --> 01:02:22,720
从这个模型中免费得到这个，这

1539
01:02:22,720 --> 01:02:25,160
至少是我们在模拟中看到的，

1540
01:02:25,160 --> 01:02:28,720
但是嗯，在模拟中，我们当然可以

1541
01:02:28,720 --> 01:02:30,960
完全控制这个 动态

1542
01:02:30,960 --> 01:02:33,000
以正确的方式匹配，

1543
01:02:33,000 --> 01:02:36,680
所以嗯，这对于硬件来说可能有点棘手，

1544
01:02:36,680 --> 01:02:39,200
但它可能是可以解决的，

1545
01:02:39,200 --> 01:02:41,039
所以这会很有趣，是的，那么

1546
01:02:41,039 --> 01:02:42,480
你得到的是，你拥有

1547
01:02:42,480 --> 01:02:44,520
这个纯粹基于事件的工具，

1548
01:02:44,520 --> 01:02:48,599
它只使用嗯前峰值 这很好，

1549
01:02:48,599 --> 01:02:49,520


1550
01:02:49,520 --> 01:02:53,000
非常酷，谢谢，是的，丹尼尔，如果

1551
01:02:53,000 --> 01:02:55,200
你有一个问题，那就是

1552
01:02:55,200 --> 01:02:58,559
一个很好的原则，

1553
01:02:58,559 --> 01:03:00,319
就像你可以设计

1554
01:03:00,319 --> 01:03:03,240
神经形态算法，以便它

1555
01:03:03,240 --> 01:03:06,599
利用材料特征，如

1556
01:03:06,599 --> 01:03:08,640
膜的实际渗漏渗透性

1557
01:03:08,640 --> 01:03:12,599
或实际空间接近度 如果您可以

1558
01:03:12,599 --> 01:03:15,640
利用

1559
01:03:15,640 --> 01:03:19,000
未虚拟化的材料特征模拟功能，那么它

1560
01:03:19,000 --> 01:03:20,119
已经

1561
01:03:20,119 --> 01:03:23,079
与未来的硬件相邻，所以这是

1562
01:03:23,079 --> 01:03:25,760
一个很好的观点，然后就S的观点而言，

1563
01:03:25,760 --> 01:03:28,920
几乎生物学不是一门

1564
01:03:28,920 --> 01:03:32,520
科学，嗯，有一句著名的

1565
01:03:32,520 --> 01:03:34,480
引言永远不会 牛顿

1566
01:03:34,480 --> 01:03:36,839
代表一片草叶，因为有些人

1567
01:03:36,839 --> 01:03:38,839
说是的，这是一种不同的生物学，

1568
01:03:38,839 --> 01:03:41,079
更像是历史，因为无论你

1569
01:03:41,079 --> 01:03:42,680
从发展、

1570
01:03:42,680 --> 01:03:44,880
生态还是进化的角度来看待这个问题，生物学

1571
01:03:44,880 --> 01:03:46,640
都是一门历史科学，它不像一门

1572
01:03:46,640 --> 01:03:49,480
真正的科学，这让我想起

1573
01:03:49,480 --> 01:03:53,000
越野衫上写着我们的

1574
01:03:53,000 --> 01:03:55,000
运动是对你的惩罚，

1575
01:03:55,000 --> 01:03:58,319
所以这就像你的

1576
01:03:58,319 --> 01:04:02,760
噪音是生物学的信号一样，这就是

1577
01:04:02,760 --> 01:04:04,279
它

1578
01:04:04,279 --> 01:04:08,680
发生的方式我的问题是关于

1579
01:04:08,680 --> 01:04:10,640


1580
01:04:10,640 --> 01:04:14,039
我想神经和计算

1581
01:04:14,039 --> 01:04:16,760
方式之间的紧张关系，以查看

1582
01:04:16,760 --> 01:04:19,440
与计算相关的资源，所以 从

1583
01:04:19,440 --> 01:04:23,720
Von noyman 范式来看，我们有很多

1584
01:04:23,720 --> 01:04:27,720
共享参考点 CPU 周期 RAM

1585
01:04:27,720 --> 01:04:30,559
容量以及所有这些类型的东西

1586
01:04:30,559 --> 01:04:34,520
，甚至在您的

1587
01:04:34,520 --> 01:04:36,559
介绍中您也传达了这样的信息：

1588
01:04:36,559 --> 01:04:38,839
它正在经历多少个 CPU 周期，

1589
01:04:38,839 --> 01:04:41,799
或者这是有多少个参数

1590
01:04:41,799 --> 01:04:43,920
要存储或类似的东西，

1591
01:04:43,920 --> 01:04:45,319


1592
01:04:45,319 --> 01:04:48,240
但是这

1593
01:04:48,240 --> 01:04:53,160
引用了另一个范式，那么

1594
01:04:53,160 --> 01:04:56,160


1595
01:04:56,880 --> 01:05:01,640


1596
01:05:01,640 --> 01:05:04,319
当我们超出了功耗的空间时，资源描述符或容量描述

1597
01:05:04,319 --> 01:05:05,559
符是

1598
01:05:05,559 --> 01:05:07,039
什么样子的，你可以将其放入盒子中，然后

1599
01:05:07,039 --> 01:05:09,079
使用炸弹热量计

1600
01:05:09,079 --> 01:05:12,520
有点像一个容易实现的目标，但现在还

1601
01:05:12,520 --> 01:05:15,760
可以除了纯粹的能量或

1602
01:05:15,760 --> 01:05:16,960
热量

1603
01:05:16,960 --> 01:05:19,319
需求之外，我们还能说什么呢，这

1604
01:05:19,319 --> 01:05:21,880
就像我们

1605
01:05:21,880 --> 01:05:25,200
谈论计算机上的处理器、内存或硬盘

1606
01:05:25,200 --> 01:05:27,759
驱动器一样，

1607
01:05:30,000 --> 01:05:32,839
我的意思是我是的 我必须再

1608
01:05:32,839 --> 01:05:34,279
考虑一下，我确实

1609
01:05:34,279 --> 01:05:35,200
认为

1610
01:05:35,200 --> 01:05:37,240
那张幻灯片上的论文中有一些有趣的评论，我

1611
01:05:37,240 --> 01:05:39,520
展示了有关大脑和

1612
01:05:39,520 --> 01:05:41,119
能量的内容，我链接到了一篇论文，

1613
01:05:41,119 --> 01:05:42,359
我必须得到 参考并让

1614
01:05:42,359 --> 01:05:43,839
你知道它是什么，因为 QR C 现在已经消失了，

1615
01:05:43,839 --> 01:05:46,520
但是其中有一些有趣的

1616
01:05:46,520 --> 01:05:48,920
想法，我想你会得到什么，

1617
01:05:48,920 --> 01:05:52,880
但我是的，我必须听从

1618
01:05:52,880 --> 01:05:55,279
这篇论文，

1619
01:05:55,279 --> 01:05:57,200
我的意思是他们如何描述什么

1620
01:05:57,200 --> 01:06:00,520
正在设计中，他们说它有这么

1621
01:06:00,520 --> 01:06:03,200
多这种类型的组件，然后

1622
01:06:03,200 --> 01:06:05,720
可能什么也不做，那么

1623
01:06:05,720 --> 01:06:08,079
他们如何描述或评估这些

1624
01:06:08,079 --> 01:06:11,480
不同的设计或

1625
01:06:11,640 --> 01:06:13,520
算法我认为这

1626
01:06:13,520 --> 01:06:14,799
取决于用例，就像

1627
01:06:14,799 --> 01:06:16,319
我发现的那样 真的很喜欢这种语言

1628
01:06:16,319 --> 01:06:18,520
是不同的，具体取决于你知道它

1629
01:06:18,520 --> 01:06:20,440
是否是由具有更多

1630
01:06:20,440 --> 01:06:21,480
神经科学背景或

1631
01:06:21,480 --> 01:06:23,799
工程学背景的人编写的，然后

1632
01:06:23,799 --> 01:06:25,279
你说有些术语

1633
01:06:25,279 --> 01:06:26,920
比其他术语更容易互换，但我确实

1634
01:06:26,920 --> 01:06:29,400
认为术语是 在这个领域中

1635
01:06:29,400 --> 01:06:31,559
需要更仔细地研究一些东西，

1636
01:06:31,559 --> 01:06:33,039
因为这样我认为这

1637
01:06:33,039 --> 01:06:35,599
将有助于在其中工作的每个人都在

1638
01:06:35,599 --> 01:06:38,799
同一页面上更

1639
01:06:42,079 --> 01:06:44,359
接近一点，

1640
01:06:44,359 --> 01:06:47,960
好的任何其他想法或问题

1641
01:06:47,960 --> 01:06:51,440
首先是大卫，然后是莎拉我' 我很

1642
01:06:51,440 --> 01:06:55,480
好奇这个系列会朝什么方向

1643
01:06:55,480 --> 01:06:59,039
发展，但首先大卫，你想提供什么其他

1644
01:06:59,039 --> 01:07:02,119
类型的结束语或指示，嗯，

1645
01:07:02,119 --> 01:07:03,359


1646
01:07:03,359 --> 01:07:07,000
不是真的，我的意思是，我

1647
01:07:07,000 --> 01:07:09,480
想说谢谢你今天邀请我，

1648
01:07:09,480 --> 01:07:11,279
真的很高兴与你讨论

1649
01:07:11,279 --> 01:07:13,359


1650
01:07:13,359 --> 01:07:15,720
谢谢你，大卫，

1651
01:07:15,720 --> 01:07:17,000
有你在，真是太棒了，我认为你的工作

1652
01:07:17,000 --> 01:07:19,079
绝对令人着迷，我认为它将

1653
01:07:19,079 --> 01:07:20,880
在未来的实施中带来很多好处，

1654
01:07:20,880 --> 01:07:23,400
这

1655
01:07:23,400 --> 01:07:26,559
总是很高兴看到，嗯，是的，那么

1656
01:07:26,559 --> 01:07:27,760
问题是什么？ 我

1657
01:07:27,760 --> 01:07:30,279
看到这个系列了吗嗯希望我们

1658
01:07:30,279 --> 01:07:33,720
每个月都能有一位新客人嗯我想

1659
01:07:33,720 --> 01:07:36,240
也许下个月能

1660
01:07:36,240 --> 01:07:38,680
有一个正在构建

1661
01:07:38,680 --> 01:07:40,319
硬件的人（比如来自

1662
01:07:40,319 --> 01:07:42,079
脑秤团队或角宿一团队的人）会很酷 或者

1663
01:07:42,079 --> 01:07:44,279
类似的东西会很酷，

1664
01:07:44,279 --> 01:07:46,880
但是，是的，我只是想

1665
01:07:46,880 --> 01:07:49,119
为那些对

1666
01:07:49,119 --> 01:07:52,359
这个交叉点感兴趣的人提供一个空间，让他们结识那些在这个领域工作的人，

1667
01:07:52,359 --> 01:07:54,079
看看谈话，你知道，接触

1668
01:07:54,079 --> 01:07:56,520
也在这个领域工作的人，

1669
01:07:56,520 --> 01:07:59,319
因为嗯 这是相当利基的，但我认为

1670
01:07:59,319 --> 01:08:01,920
这非常重要，嗯，实际上已经

1671
01:08:01,920 --> 01:08:03,400
说过，大卫，你能让每个人都

1672
01:08:03,400 --> 01:08:05,039
知道，如果他们想联系

1673
01:08:05,039 --> 01:08:08,359
你，他们最好的方式是什么

1674
01:08:08,359 --> 01:08:09,390


1675
01:08:09,390 --> 01:08:11,400
[音乐]

1676
01:08:11,400 --> 01:08:13,960
嗯，我对此不是很积极 Discord

1677
01:08:13,960 --> 01:08:16,399
频道，所以也许电子邮件仍然是

1678
01:08:16,399 --> 01:08:19,158
联系我的最佳方式，我

1679
01:08:19,158 --> 01:08:21,399
想很酷，你想提供你的

1680
01:08:21,399 --> 01:08:22,839
电子邮件吗？

1681
01:08:22,839 --> 01:08:27,560
哦，我认为我的电子邮件应该

1682
01:08:27,560 --> 01:08:28,960
很容易找到，但你也可以将

1683
01:08:28,960 --> 01:08:30,600
电子邮件发送出去，

1684
01:08:30,600 --> 01:08:33,439
人们可以查看 论文，

1685
01:08:33,439 --> 01:08:36,000
然后在主动推理研究所

1686
01:08:36,000 --> 01:08:40,080
Discord 中，有神经形态

1687
01:08:41,120 --> 01:08:43,719
通道，好吧，谢谢大卫和

1688
01:08:43,719 --> 01:08:47,198
莎拉，看到变形流以这种方式

1689
01:08:47,198 --> 01:08:49,158
开始其发展轨迹真的很酷，

1690
01:08:49,158 --> 01:08:52,799
所以直到下次谢谢你

1691
01:08:52,799 --> 01:08:54,109
再见

1692
01:08:54,109 --> 01:08:57,169
[音乐]

1693
01:09:19,040 --> 01:09:22,040
很好


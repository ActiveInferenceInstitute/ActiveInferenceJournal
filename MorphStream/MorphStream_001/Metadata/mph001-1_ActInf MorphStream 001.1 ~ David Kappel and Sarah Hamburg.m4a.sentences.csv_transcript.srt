1
00:00:06,874 --> 00:00:07,973
Daniel: Hello, everybody.

2
00:00:08,122 --> 00:00:08,822
Welcome.

3
00:00:09,019 --> 00:00:12,384
It is September 26, 2023.

4
00:00:13,399 --> 00:00:15,623
We are here kicking off a new stream

5
00:00:15,623 --> 00:00:15,623
series at the Active Inference Institute.

6
00:00:15,623 --> 00:00:17,884
.

7
00:00:17,897 --> 00:00:18,996
This is the morph.

8
00:00:19,001 --> 00:00:21,209
Stream 1.1.

9
00:00:21,227 --> 00:00:24,587
Today we have David Cappell and also this

10
00:00:24,587 --> 00:00:24,587
section and streams facilitated by Sarah

11
00:00:24,587 --> 00:00:28,991
Ham.

12
00:00:29,045 --> 00:00:32,378
We're going to have a overview, first

13
00:00:32,378 --> 00:00:34,559
presented by Sarah.

14
00:00:34,571 --> 00:00:37,815
Then David will share some work on

15
00:00:37,815 --> 00:00:37,815
neuromorphic computing, and then we'll

16
00:00:37,815 --> 00:00:41,288
have some time to discuss.

17
00:00:42,300 --> 00:00:44,532
So thank you both for joining and Sarah,

18
00:00:44,532 --> 00:00:44,532
to you for the first presentation and

19
00:00:44,532 --> 00:00:44,532
also to introduce yourself, if you'd like.

20
00:00:44,532 --> 00:00:49,004
.

21
00:00:49,022 --> 00:00:50,126
Sarah: Yes, that's a good idea.

22
00:00:50,131 --> 00:00:51,285
Thank you very much, Daniel.

23
00:00:52,303 --> 00:00:53,458
So my name is Sarah.

24
00:00:53,469 --> 00:00:55,657
I'm a neuroscientist specializing

25
00:00:55,657 --> 00:00:55,657
intelligence, currently working in the

26
00:00:55,657 --> 00:00:55,657
field of neuromorphic computing at

27
00:00:55,657 --> 00:01:01,646
Sheffield Hallam in the UK.

28
00:01:01,659 --> 00:01:03,857
So I'm going to give you a high level

29
00:01:03,857 --> 00:01:03,857
overview of what neuromorphic computing

30
00:01:03,857 --> 00:01:03,857
is before we hear David's exciting talk

31
00:01:03,857 --> 00:01:10,505
in the first edition of this new series.

32
00:01:10,563 --> 00:01:12,720
Just to let you know, if you're watching

33
00:01:12,720 --> 00:01:12,720
on Double Time in the future, I talk

34
00:01:12,720 --> 00:01:12,720
quite fast, so you might not want to

35
00:01:12,720 --> 00:01:16,111
watch me on Double Time.

36
00:01:16,172 --> 00:01:18,382
So this QR code I put here will take you

37
00:01:18,382 --> 00:01:18,382
to a paper which I thought was a really

38
00:01:18,382 --> 00:01:22,760
nice introduction to the field.

39
00:01:23,805 --> 00:01:25,002
But neuromorphic computing can be defined

40
00:01:25,002 --> 00:01:25,002
as computing systems that are designed to

41
00:01:25,002 --> 00:01:25,002
mimic the structure and function of the

42
00:01:25,002 --> 00:01:29,471
nervous system.

43
00:01:29,488 --> 00:01:31,646
So this doesn't have to be the human

44
00:01:31,646 --> 00:01:32,716
nervous system.

45
00:01:32,726 --> 00:01:34,929
The field actually takes inspiration from

46
00:01:34,929 --> 00:01:34,929
all sorts of animals and insects,

47
00:01:34,929 --> 00:01:34,929
although the definitions online don't

48
00:01:34,929 --> 00:01:39,433
necessarily acknowledge that.

49
00:01:39,470 --> 00:01:41,685
So some people are quite open with what

50
00:01:41,685 --> 00:01:41,685
constitutes neuromorphic, while maybe

51
00:01:41,685 --> 00:01:41,685
others would prefer neuromorphic was

52
00:01:41,685 --> 00:01:41,685
reserved for hardware instantiations of

53
00:01:41,685 --> 00:01:41,685
biological like neurons, which are

54
00:01:41,685 --> 00:01:41,685
sometimes referred to as non von Newman

55
00:01:41,685 --> 00:01:53,079
computers.

56
00:01:53,079 --> 00:01:55,107
And I think that's what the paper that

57
00:01:55,107 --> 00:01:55,107
that QR code refers to it as their

58
00:01:55,107 --> 00:01:59,141
definition.

59
00:01:59,143 --> 00:02:00,096
So what I think is really interesting is

60
00:02:00,096 --> 00:02:01,106
a little bit of the context.

61
00:02:01,106 --> 00:02:03,125
So, like our current von Newman computer

62
00:02:03,125 --> 00:02:03,125
architecture was also inspired by

63
00:02:03,125 --> 00:02:03,125
neuroscience, particularly the McCullough

64
00:02:03,125 --> 00:02:07,168
and Pitts 43.

65
00:02:07,169 --> 00:02:10,196
Neuromodel inspired von Newman's first

66
00:02:10,196 --> 00:02:11,207
draft in 1945.

67
00:02:11,208 --> 00:02:13,227
So neuroscience has a long history of

68
00:02:13,227 --> 00:02:13,227
inspiring computer science, and this also

69
00:02:13,227 --> 00:02:13,227
includes reinforcement learning, which is

70
00:02:13,227 --> 00:02:13,227
based on theories about learning,

71
00:02:13,227 --> 00:02:13,227
decision making from behavioral

72
00:02:13,227 --> 00:02:13,227
psychology based on rewards and

73
00:02:13,227 --> 00:02:13,227
punishments, and also Hebian learning

74
00:02:13,227 --> 00:02:13,227
principles of cells that fire together,

75
00:02:13,227 --> 00:02:13,227
wire together from 49 became foundational

76
00:02:13,227 --> 00:02:31,404
for unsupervised learning.

77
00:02:31,405 --> 00:02:34,439
So, first of all, hang on 1 second.

78
00:02:35,447 --> 00:02:37,465
So in order to understand the why of

79
00:02:37,465 --> 00:02:37,465
neuromorphic computing, I really wanted

80
00:02:37,465 --> 00:02:37,465
to explain what's so great about the

81
00:02:37,465 --> 00:02:41,508
brain.

82
00:02:41,509 --> 00:02:44,537
So here's some inspiration for light

83
00:02:44,537 --> 00:02:45,542
bulbs.

84
00:02:45,544 --> 00:02:46,556
So I'm going to ask you a question.

85
00:02:46,556 --> 00:02:48,570
I just want you to think about it for a

86
00:02:48,570 --> 00:02:48,572
second.

87
00:02:48,574 --> 00:02:50,596
In terms of light bulbs, how much energy

88
00:02:50,596 --> 00:02:52,612
do you think the brain uses?

89
00:02:52,614 --> 00:02:54,635
Do you think it's more or less energy

90
00:02:54,635 --> 00:02:54,635
than the bulbs lighting the room that you'

91
00:02:54,635 --> 00:02:57,663
're in?

92
00:02:57,665 --> 00:02:58,678
If you're in the future, by all means

93
00:02:58,678 --> 00:02:59,682
pause this.

94
00:02:59,683 --> 00:02:59,686
Pause this.

95
00:02:59,687 --> 00:03:01,641
If you want to do some in depth

96
00:03:01,641 --> 00:03:01,647
calculations.

97
00:03:01,648 --> 00:03:03,665
But I'm going to skip to the answer.

98
00:03:04,670 --> 00:03:07,704
The answer is here, in the pink circle.

99
00:03:07,705 --> 00:03:09,725
So it's 20 watts.

100
00:03:09,727 --> 00:03:11,748
So that's the equivalent of one modern

101
00:03:11,748 --> 00:03:13,766
day energy efficient light bulb.

102
00:03:13,767 --> 00:03:15,786
So that's probably what's above me now,

103
00:03:15,786 --> 00:03:17,800
basically in my room here.

104
00:03:17,802 --> 00:03:18,819
This QR code should take you to quite an

105
00:03:18,819 --> 00:03:18,819
interesting paper on power consumption in

106
00:03:18,819 --> 00:03:22,854
the brain, if you're interested in that.

107
00:03:23,868 --> 00:03:26,894
So that works out about four bananas a

108
00:03:26,894 --> 00:03:28,910
day to power your brain.

109
00:03:28,912 --> 00:03:30,933
And this is calculated, by the way, based

110
00:03:30,933 --> 00:03:32,954
on calorie intake that the brain needs.

111
00:03:32,955 --> 00:03:34,972
So, for context, the fastest

112
00:03:34,972 --> 00:03:34,972
supercomputer in Europe, I think it's

113
00:03:34,972 --> 00:03:34,972
called lumi, in Finland it's been called

114
00:03:34,972 --> 00:03:34,972
exceptionally green, and its power

115
00:03:34,972 --> 00:03:43,062
consumption is 8.5 million watts.

116
00:03:43,063 --> 00:03:45,086
So that's around half a million light

117
00:03:45,086 --> 00:03:48,110
bulbs, while your brain uses just one.

118
00:03:48,112 --> 00:03:50,134
So then the question is, well, what does

119
00:03:50,134 --> 00:03:50,134
your brain do with that one light bulb or

120
00:03:50,134 --> 00:03:53,164
four bananas?

121
00:03:55,180 --> 00:03:57,203
Apparently it does 1000 billion

122
00:03:57,203 --> 00:03:59,221
calculations per second.

123
00:03:59,223 --> 00:04:01,181
So there's lots of other massive

124
00:04:01,181 --> 00:04:02,193
estimates out there.

125
00:04:02,194 --> 00:04:04,212
This wasn't even the largest by several

126
00:04:04,212 --> 00:04:05,222
orders of magnitude.

127
00:04:05,223 --> 00:04:07,244
Estimates are obviously very speculative,

128
00:04:07,244 --> 00:04:07,244
but they're all massive and they all tend

129
00:04:07,244 --> 00:04:07,244
to be based on the number of neurons,

130
00:04:07,244 --> 00:04:13,303
their connections and firing rates.

131
00:04:13,305 --> 00:04:15,329
But I think it's really important for the

132
00:04:15,329 --> 00:04:15,329
context that supercomputers can't

133
00:04:15,329 --> 00:04:15,329
actually yet match our complexity of

134
00:04:15,329 --> 00:04:15,329
skills or the adaptability of the human

135
00:04:15,329 --> 00:04:22,398
brain.

136
00:04:22,399 --> 00:04:24,416
So we actually excel way beyond

137
00:04:24,416 --> 00:04:24,416
supercomputers when it comes to things

138
00:04:24,416 --> 00:04:24,416
like complex decision making, learning

139
00:04:24,416 --> 00:04:29,467
from experience.

140
00:04:31,483 --> 00:04:33,505
So how does your brain compare to AI?

141
00:04:33,509 --> 00:04:36,532
So, I mentioned that modern AI is already

142
00:04:36,532 --> 00:04:37,541
brain inspired.

143
00:04:37,543 --> 00:04:39,565
However, artificial neurons are highly

144
00:04:39,565 --> 00:04:40,573
simplified.

145
00:04:41,581 --> 00:04:42,597
They don't capture the complexity of

146
00:04:42,597 --> 00:04:42,597
biological neurons or networks, like, not

147
00:04:42,597 --> 00:04:45,623
even close.

148
00:04:46,631 --> 00:04:48,650
Individual neurons are actually more like

149
00:04:48,650 --> 00:04:49,660
networks themselves.

150
00:04:49,662 --> 00:04:51,684
And research suggests that modeling one

151
00:04:51,684 --> 00:04:51,684
biological neuron requires a five to

152
00:04:51,684 --> 00:04:51,684
eight layer deep artificial neural

153
00:04:51,684 --> 00:04:51,684
network made of around 1000 artificial

154
00:04:51,684 --> 00:04:58,752
neurons.

155
00:04:58,752 --> 00:05:00,716
This QR code should take you to the paper.

156
00:05:00,716 --> 00:05:00,716
.

157
00:05:00,718 --> 00:05:01,728
For that.

158
00:05:03,740 --> 00:05:05,767
You have 86 billion neurons in your brain.

159
00:05:05,767 --> 00:05:05,767
.

160
00:05:05,768 --> 00:05:07,788
They work together to form a highly

161
00:05:07,788 --> 00:05:07,788
energy efficient, low latency

162
00:05:07,788 --> 00:05:07,788
supercomputer that works just above room

163
00:05:07,788 --> 00:05:07,788
temperature, off the equivalent of about

164
00:05:07,788 --> 00:05:15,864
four bananas a day.

165
00:05:15,866 --> 00:05:17,884
So hopefully I've given you a sense of

166
00:05:17,884 --> 00:05:17,884
how amazing your brain is, as if you didn'

167
00:05:17,884 --> 00:05:17,884
't know that already, and how it's

168
00:05:17,884 --> 00:05:17,884
already been used to inspire the, I guess,

169
00:05:17,884 --> 00:05:17,884
, fairly basic AI that we have now

170
00:05:17,884 --> 00:05:26,973
compared to human intelligence.

171
00:05:26,974 --> 00:05:28,993
So, next I'm going to explain how key

172
00:05:28,993 --> 00:05:28,993
features of the brain are being

173
00:05:28,993 --> 00:05:28,993
implemented to catalyze our next

174
00:05:28,993 --> 00:05:28,993
generation of AI and technology through

175
00:05:28,993 --> 00:05:28,993
the field of neuromorphic computing,

176
00:05:28,993 --> 00:05:37,082
which is why you're all here.

177
00:05:37,084 --> 00:05:39,108
So, traditional volume and computers have

178
00:05:39,108 --> 00:05:39,108
physically separate computing and memory

179
00:05:39,108 --> 00:05:44,149
units, shown here on the left.

180
00:05:44,155 --> 00:05:46,174
During computation, data must transfer

181
00:05:46,174 --> 00:05:46,174
backwards and forwards, like really fast.

182
00:05:46,174 --> 00:05:48,195
.

183
00:05:48,196 --> 00:05:50,218
So there's a bottleneck, essentially for

184
00:05:50,218 --> 00:05:50,218
speed and energy, whereas in neuromorphic

185
00:05:50,218 --> 00:05:50,218
architectures, which are shown here on

186
00:05:50,218 --> 00:05:50,218
the right, with the help of Dali,

187
00:05:50,218 --> 00:05:50,218
computing and memory occur in the same

188
00:05:50,218 --> 00:06:00,250
place.

189
00:06:00,252 --> 00:06:02,271
So they're said to be colocated.

190
00:06:02,277 --> 00:06:04,298
Essentially, individual neurons perform

191
00:06:04,298 --> 00:06:04,298
computation while memory is represented

192
00:06:04,298 --> 00:06:04,298
by the strength of the connections, the

193
00:06:04,298 --> 00:06:10,353
weights between neurons.

194
00:06:10,353 --> 00:06:11,367
So the synapses.

195
00:06:11,368 --> 00:06:14,395
So chips like this might be created with

196
00:06:14,395 --> 00:06:14,395
components like Memoristas for example,

197
00:06:14,395 --> 00:06:19,444
which can emulate synaptic weights.

198
00:06:19,446 --> 00:06:22,470
And this architecture improves speed, it

199
00:06:22,470 --> 00:06:23,487
reduces energy consumption.

200
00:06:23,488 --> 00:06:25,506
And what's really interesting is it

201
00:06:25,506 --> 00:06:25,506
enables massively parallel processing

202
00:06:25,506 --> 00:06:25,506
meaning that multiple problems can be

203
00:06:25,506 --> 00:06:31,561
worked on at the same time.

204
00:06:31,566 --> 00:06:33,587
So this is particularly important, this

205
00:06:33,587 --> 00:06:33,587
architecture, for various use cases, but

206
00:06:33,587 --> 00:06:33,587
also because as we reach the end of Moore'

207
00:06:33,587 --> 00:06:33,587
's Law, which is the number of

208
00:06:33,587 --> 00:06:33,587
transistors you were able to physically

209
00:06:33,587 --> 00:06:44,691
make tinier and tinier to fit on a chip.

210
00:06:44,693 --> 00:06:46,718
And it's also important because humanity

211
00:06:46,718 --> 00:06:46,718
needs to massively reduce its energy

212
00:06:46,718 --> 00:06:46,718
consumption against the backdrop drop of

213
00:06:46,718 --> 00:06:53,787
creating ever more powerful AI.

214
00:06:55,802 --> 00:06:57,821
So artificial neurons typically use

215
00:06:57,821 --> 00:06:57,821
continuous activation shown on the left,

216
00:06:57,821 --> 00:06:57,821
they're always on, while neuromorphic

217
00:06:57,821 --> 00:06:57,821
neurons, they're said to be spiking, so

218
00:06:57,821 --> 00:06:57,821
they're on or they're off, which is shown

219
00:06:57,821 --> 00:07:07,861
here on the right.

220
00:07:07,862 --> 00:07:09,884
So similar to sort of an action potential.

221
00:07:09,884 --> 00:07:09,884
.

222
00:07:09,886 --> 00:07:12,910
So the benefits for this are again power,

223
00:07:12,910 --> 00:07:12,910
efficiency and also applications where

224
00:07:12,910 --> 00:07:12,910
timing is important and this is given

225
00:07:12,910 --> 00:07:18,972
that they're event driven.

226
00:07:18,973 --> 00:07:20,996
So essentially they have the potential

227
00:07:20,996 --> 00:07:20,996
for spatial and temporal dimensions which

228
00:07:20,996 --> 00:07:20,996
then enables added spatiotemporal

229
00:07:20,996 --> 00:07:27,067
encoding and processing of information.

230
00:07:28,074 --> 00:07:30,097
You might be wondering a bit about GPUs

231
00:07:30,097 --> 00:07:33,121
which also enable parallel processing.

232
00:07:33,122 --> 00:07:35,145
Research suggests that GPUs are suitable

233
00:07:35,145 --> 00:07:35,145
architectures for deploying spiking

234
00:07:35,145 --> 00:07:35,145
neural networks which I think makes this

235
00:07:35,145 --> 00:07:35,145
a really interesting time for the field

236
00:07:35,145 --> 00:07:35,145
given how high end GPUs are becoming ever

237
00:07:35,145 --> 00:07:45,243
more pervasive.

238
00:07:47,262 --> 00:07:50,290
So the brain learns strength of synapses

239
00:07:50,290 --> 00:07:51,301
between neurons.

240
00:07:51,302 --> 00:07:53,324
This is based on pre and synaptic firing

241
00:07:53,324 --> 00:07:54,330
patterns.

242
00:07:54,330 --> 00:07:56,351
I think David's Talk will talk a lot, we'

243
00:07:56,351 --> 00:07:58,369
'll go into a lot more depth on this.

244
00:07:58,370 --> 00:08:00,331
But there are many different types and

245
00:08:00,331 --> 00:08:00,331
patterns of this across the brain

246
00:08:00,331 --> 00:08:00,331
depending on the types of synapses such

247
00:08:00,331 --> 00:08:00,331
as excitatory to inhibitory, excitatory

248
00:08:00,331 --> 00:08:07,405
to excalitatory.

249
00:08:07,409 --> 00:08:10,432
And the neuromorphic field is working to

250
00:08:10,432 --> 00:08:10,432
leverage these rules because of benefits

251
00:08:10,432 --> 00:08:10,432
for on chip learning and also

252
00:08:10,432 --> 00:08:10,432
applications such as pattern recognition

253
00:08:10,432 --> 00:08:18,514
and edge computing as well.

254
00:08:18,515 --> 00:08:20,533
Edge computing being quite a huge use

255
00:08:20,533 --> 00:08:20,533
case for neuromorphic computing because

256
00:08:20,533 --> 00:08:20,533
of the sort of event driven nature and

257
00:08:20,533 --> 00:08:25,588
also the low energy usage.

258
00:08:25,589 --> 00:08:28,615
And this QR code should take you to quite

259
00:08:28,615 --> 00:08:28,615
an interesting paper on STDP that I found.

260
00:08:28,615 --> 00:08:31,641
.

261
00:08:32,656 --> 00:08:34,675
So what neuromorphic solutions are

262
00:08:34,675 --> 00:08:35,681
available?

263
00:08:35,682 --> 00:08:35,684
Now?

264
00:08:35,685 --> 00:08:37,699
You might just think this is all

265
00:08:37,699 --> 00:08:37,699
theoretical, there are actually many

266
00:08:37,699 --> 00:08:37,699
different solutions out there which I'll

267
00:08:37,699 --> 00:08:37,699
just give you a really high level

268
00:08:37,699 --> 00:08:42,755
overview of.

269
00:08:42,756 --> 00:08:44,776
So the Human Brain Project has created

270
00:08:44,776 --> 00:08:44,776
several large scale neuromorphic

271
00:08:44,776 --> 00:08:44,776
computers including Spinnaker, which is

272
00:08:44,776 --> 00:08:44,776
this one at the bottom, this board's,

273
00:08:44,776 --> 00:08:44,776
like maybe like the size of my face or

274
00:08:44,776 --> 00:08:52,858
something.

275
00:08:52,859 --> 00:08:55,882
So that runs in real time and it's

276
00:08:55,882 --> 00:08:55,882
comprised of multiple general purpose arm

277
00:08:55,882 --> 00:08:55,882
microprocessors and there was also

278
00:08:55,882 --> 00:08:55,882
BrainScaleS which is an accelerated

279
00:08:55,882 --> 00:08:55,882
analog architecture and it runs 1000

280
00:08:55,882 --> 00:09:06,934
times real time.

281
00:09:06,935 --> 00:09:08,959
So the board next to the blue one, that's

282
00:09:08,959 --> 00:09:08,959
an actual credit card size version of

283
00:09:08,959 --> 00:09:08,959
BrainScaleS, which they've recently made,

284
00:09:08,959 --> 00:09:15,025
which I thought was pretty cool.

285
00:09:16,035 --> 00:09:18,053
And then there's also some big players in

286
00:09:18,053 --> 00:09:18,057
the space.

287
00:09:18,058 --> 00:09:21,085
So this blue one here is Intel's Louihi

288
00:09:21,085 --> 00:09:21,089
chip.

289
00:09:22,089 --> 00:09:23,100
They're onto Luihi two.

290
00:09:23,100 --> 00:09:24,117
Now, that's their Neuromorphic chip, and

291
00:09:24,117 --> 00:09:24,117
they have an open source software

292
00:09:24,117 --> 00:09:24,117
framework for that as well, because they

293
00:09:24,117 --> 00:09:24,117
really want to catalyze the open source

294
00:09:24,117 --> 00:09:31,179
community to get involved with it.

295
00:09:31,181 --> 00:09:33,201
So neuromorphic sensors also exist?

296
00:09:33,201 --> 00:09:34,216
So this little blue thing in the middle

297
00:09:34,216 --> 00:09:35,228
is actually a Neuromorphic camera.

298
00:09:35,229 --> 00:09:37,242
It's maybe like this big.

299
00:09:38,251 --> 00:09:40,276
So they aim to recreate how our nervous

300
00:09:40,276 --> 00:09:43,300
system senses stimuli, such as light.

301
00:09:43,301 --> 00:09:45,325
So, for example, in a Neuromorphic camera,

302
00:09:45,325 --> 00:09:45,325
, which is the one here, each pixel works

303
00:09:45,325 --> 00:09:45,325
independently with a microsecond

304
00:09:45,325 --> 00:09:50,372
resolution.

305
00:09:50,373 --> 00:09:51,388
Hopefully, my GIF will work.

306
00:09:52,389 --> 00:09:52,397
There we go.

307
00:09:52,398 --> 00:09:55,419
So you can see each pixel working there,

308
00:09:55,419 --> 00:09:56,431
which is pretty cool.

309
00:09:56,433 --> 00:09:58,451
So compared to traditional digital

310
00:09:58,451 --> 00:09:58,451
cameras, they have improved performance

311
00:09:58,451 --> 00:10:02,435
with motion and lower power consumption.

312
00:10:02,437 --> 00:10:04,454
There was also a Neuromorphic nose

313
00:10:04,454 --> 00:10:04,454
recently by intel, which was pretty cool.

314
00:10:04,454 --> 00:10:06,476
.

315
00:10:06,477 --> 00:10:08,498
So it could learn the scent of a chemical

316
00:10:08,498 --> 00:10:08,498
after just one exposure, and then it

317
00:10:08,498 --> 00:10:08,498
could identify that scent even when it

318
00:10:08,498 --> 00:10:13,549
was masked by others.

319
00:10:14,553 --> 00:10:16,574
And then finally, this is a humanoid

320
00:10:16,574 --> 00:10:17,586
robot called an ICup.

321
00:10:17,587 --> 00:10:19,609
And what you can do is you can actually

322
00:10:19,609 --> 00:10:19,609
integrate Neuromorphic sensors, such as

323
00:10:19,609 --> 00:10:19,609
the camera, and then Neuromorphic chips

324
00:10:19,609 --> 00:10:19,609
maybe spinnaker or brain scales into a

325
00:10:19,609 --> 00:10:19,609
humanoid device like this or other

326
00:10:19,609 --> 00:10:29,707
devices like a drone.

327
00:10:29,708 --> 00:10:31,726
And then from that, you can actually

328
00:10:31,726 --> 00:10:33,745
create embodied neuromorphic systems.

329
00:10:33,747 --> 00:10:35,766
And this is something that we work on at

330
00:10:35,766 --> 00:10:35,766
the Smart Interactive Technologies

331
00:10:35,766 --> 00:10:39,800
Research Lab in Sheffield in the UK.

332
00:10:41,822 --> 00:10:42,837
This slide just highlights some of the

333
00:10:42,837 --> 00:10:42,837
potential applications of Neuromorphic

334
00:10:42,837 --> 00:10:42,837
computing, which I thought were quite

335
00:10:42,837 --> 00:10:47,883
interesting when you think about it.

336
00:10:47,884 --> 00:10:49,908
So the understanding of context, pattern

337
00:10:49,908 --> 00:10:49,908
recognition, advanced sensing, fusot,

338
00:10:49,908 --> 00:10:49,908
learning, generalizing across tasks,

339
00:10:49,908 --> 00:10:49,908
complex decision making, explainability,

340
00:10:49,908 --> 00:10:58,994
and brain interfaces.

341
00:10:58,998 --> 00:11:01,960
So all these skills are really beneficial

342
00:11:01,960 --> 00:11:01,960
when you're thinking about human centered,

343
00:11:01,960 --> 00:11:01,960
, real time applications in dynamic

344
00:11:01,960 --> 00:11:05,007
environments.

345
00:11:05,008 --> 00:11:08,030
So things like self driving cars, for

346
00:11:08,030 --> 00:11:08,035
example.

347
00:11:08,037 --> 00:11:11,059
And personally, I think that neuromorphic

348
00:11:11,059 --> 00:11:11,059
systems are also likely to be the future

349
00:11:11,059 --> 00:11:15,104
substrate of brain computer interfaces.

350
00:11:15,105 --> 00:11:17,123
Probably a bit biased because I'm a

351
00:11:17,123 --> 00:11:17,123
neuroscientist, but they're low energy,

352
00:11:17,123 --> 00:11:17,123
they're real time, and they also have

353
00:11:17,123 --> 00:11:17,123
architectures which match our own

354
00:11:17,123 --> 00:11:23,188
hardware.

355
00:11:24,189 --> 00:11:26,210
So I do think we'll soon see the BCI

356
00:11:26,210 --> 00:11:26,210
field being catalyzed by neuromorphic

357
00:11:26,210 --> 00:11:26,210
systems, particularly maybe for hybrids

358
00:11:26,210 --> 00:11:32,271
of hardware and wetware.

359
00:11:32,272 --> 00:11:34,293
So maybe even potentially containing

360
00:11:34,293 --> 00:11:34,293
people's own brain cells, which you can

361
00:11:34,293 --> 00:11:38,338
actually grow just from a hair cell.

362
00:11:41,361 --> 00:11:43,382
And a particular focus of our work is

363
00:11:43,382 --> 00:11:43,382
designing AI, which learns in a similar

364
00:11:43,382 --> 00:11:46,414
way to a human.

365
00:11:46,415 --> 00:11:48,435
So it has an innate sense of curiosity,

366
00:11:48,435 --> 00:11:48,435
and it learns through interacting with

367
00:11:48,435 --> 00:11:51,462
the real world.

368
00:11:51,464 --> 00:11:53,488
So in the 50s, Alan Turing said, instead

369
00:11:53,488 --> 00:11:53,488
of trying to produce a program to

370
00:11:53,488 --> 00:11:53,488
simulate the adult mind, why not rather

371
00:11:53,488 --> 00:11:53,488
try to produce one which simulates the

372
00:11:53,488 --> 00:12:01,505
child's brain.

373
00:12:01,507 --> 00:12:03,523
If this were then subject to an

374
00:12:03,523 --> 00:12:03,523
appropriate course of education, one

375
00:12:03,523 --> 00:12:06,558
would obtain the adult brain.

376
00:12:07,560 --> 00:12:09,582
This is very much the philosophy behind

377
00:12:09,582 --> 00:12:09,582
the neurodevelopmental approach to AI and

378
00:12:09,582 --> 00:12:09,582
neuromorphic computing, which I just

379
00:12:09,582 --> 00:12:14,639
wanted to highlight.

380
00:12:16,658 --> 00:12:18,675
There are some challenges in the field.

381
00:12:18,676 --> 00:12:20,692
These are very high level, but I'll just

382
00:12:20,692 --> 00:12:21,709
give you a little bit of an idea of it.

383
00:12:22,710 --> 00:12:24,731
So, training spiking neural networks is

384
00:12:24,731 --> 00:12:24,731
more complex than traditional neural

385
00:12:24,731 --> 00:12:26,757
networks.

386
00:12:26,758 --> 00:12:28,779
Also designing hardware which actually

387
00:12:28,779 --> 00:12:28,779
implements spiking neural networks STDP

388
00:12:28,779 --> 00:12:28,779
on a large scale is said to be fairly

389
00:12:28,779 --> 00:12:33,828
challenging.

390
00:12:34,830 --> 00:12:36,853
And then also developing algorithms which

391
00:12:36,853 --> 00:12:36,853
can actually effectively leverage all

392
00:12:36,853 --> 00:12:39,885
these technologies.

393
00:12:39,886 --> 00:12:41,908
So the hardware, the STDP, it's an

394
00:12:41,908 --> 00:12:44,930
ongoing active area of research.

395
00:12:46,955 --> 00:12:48,976
So if you're here, you're probably

396
00:12:48,976 --> 00:12:50,996
interested in active inference.

397
00:12:50,998 --> 00:12:52,013
So I wanted to highlight this.

398
00:12:52,014 --> 00:12:53,026
Actually, someone put on the discord

399
00:12:53,026 --> 00:12:53,026
today one of these studies, which was

400
00:12:53,026 --> 00:12:55,043
pretty cool.

401
00:12:55,044 --> 00:12:57,062
So a couple of recent studies have

402
00:12:57,062 --> 00:12:57,062
combined neuromorphic computing with

403
00:12:57,062 --> 00:13:01,041
principles of active inference.

404
00:13:01,042 --> 00:13:02,056
So active inference comes from

405
00:13:02,056 --> 00:13:02,056
neuroscience, and I would argue that it

406
00:13:02,056 --> 00:13:02,056
lends itself very well to neuromorphic

407
00:13:02,056 --> 00:13:07,100
architectures.

408
00:13:07,106 --> 00:13:09,121
In a recent paper on embodied

409
00:13:09,121 --> 00:13:09,121
neuromorphic intelligence, so it didn't

410
00:13:09,121 --> 00:13:13,161
really mention active inference in it.

411
00:13:13,162 --> 00:13:15,183
The QR code on the top right here, it was

412
00:13:15,183 --> 00:13:15,183
suggested that a real breakthrough in

413
00:13:15,183 --> 00:13:15,183
neuromorphics will happen if the whole

414
00:13:15,183 --> 00:13:15,183
system design is based on biological

415
00:13:15,183 --> 00:13:15,183
computational principles with a tight

416
00:13:15,183 --> 00:13:15,183
interplay between the estimation of the

417
00:13:15,183 --> 00:13:15,183
surroundings and the robot's own state

418
00:13:15,183 --> 00:13:15,183
and decision making, planning and action.

419
00:13:15,183 --> 00:13:29,328
.

420
00:13:30,330 --> 00:13:32,349
So some of those themes might sound quite

421
00:13:32,349 --> 00:13:32,349
familiar to people interested in active

422
00:13:32,349 --> 00:13:32,349
inference, and I would suggest that

423
00:13:32,349 --> 00:13:32,349
active inference is well placed to meet

424
00:13:32,349 --> 00:13:38,413
these requirements.

425
00:13:38,415 --> 00:13:40,434
And I just wanted to highlight a couple

426
00:13:40,434 --> 00:13:41,445
of recent studies here.

427
00:13:41,446 --> 00:13:44,470
So this one on the left, Gandalfiatal,

428
00:13:44,470 --> 00:13:44,470
recently demonstrated plasticity and

429
00:13:44,470 --> 00:13:44,470
rapid unsupervised learning in a

430
00:13:44,470 --> 00:13:44,470
neuromorphic system using active

431
00:13:44,470 --> 00:13:52,549
inference principles.

432
00:13:52,550 --> 00:13:54,569
The author suggested that their

433
00:13:54,569 --> 00:13:54,569
experiments could be adopted to implement

434
00:13:54,569 --> 00:13:54,569
brain like predictive capabilities in

435
00:13:54,569 --> 00:14:00,577
neuromorphic robotic systems.

436
00:14:00,579 --> 00:14:02,598
And then there was the Dish brain paper,

437
00:14:02,598 --> 00:14:02,598
which some of you may be familiar with by

438
00:14:02,598 --> 00:14:05,624
Kaganatal.

439
00:14:05,625 --> 00:14:07,649
So this was a hybrid wetware hardware

440
00:14:07,649 --> 00:14:07,649
neuromorphic system that the authors

441
00:14:07,649 --> 00:14:11,688
claimed was embodied.

442
00:14:11,689 --> 00:14:14,710
The system showed rapid apparent learning

443
00:14:14,710 --> 00:14:14,710
of the game of Pong using the free energy

444
00:14:14,710 --> 00:14:17,744
principle for learning.

445
00:14:17,746 --> 00:14:19,767
And the authors claimed that the system

446
00:14:19,767 --> 00:14:19,767
exhibited synthetic biological

447
00:14:19,767 --> 00:14:22,792
intelligence.

448
00:14:22,797 --> 00:14:24,816
So the field implementing active

449
00:14:24,816 --> 00:14:24,816
inference principles in neuromorphic

450
00:14:24,816 --> 00:14:28,852
systems is very nascent.

451
00:14:28,853 --> 00:14:30,876
And the idea behind this more stream

452
00:14:30,876 --> 00:14:30,876
series is to create a space and a

453
00:14:30,876 --> 00:14:30,876
community to share knowledge, ideas and

454
00:14:30,876 --> 00:14:37,944
expertise to catalyze the field.

455
00:14:37,946 --> 00:14:39,960
I think some really exciting

456
00:14:39,960 --> 00:14:39,960
technological leaps are probably going to

457
00:14:39,960 --> 00:14:42,991
come from this area.

458
00:14:42,993 --> 00:14:44,017
So thank you for listening to my quick

459
00:14:44,017 --> 00:14:46,035
run through Neuromorphic 101.

460
00:14:46,035 --> 00:14:50,072
And next up, we're going to hear from

461
00:14:50,072 --> 00:14:51,079
David.

462
00:14:51,080 --> 00:14:52,094
So David, over to you.

463
00:14:52,095 --> 00:14:54,116
If you want to introduce yourself, please.

464
00:14:54,116 --> 00:14:54,116
.

465
00:14:55,122 --> 00:14:56,135
David: Thank you, Sarah.

466
00:14:58,151 --> 00:15:00,118
I would share my screen.

467
00:15:04,150 --> 00:15:04,158
Yeah.

468
00:15:14,259 --> 00:15:18,292
Can you see now my screen, the

469
00:15:18,292 --> 00:15:20,313
presentation yeah.

470
00:15:20,314 --> 00:15:21,328
Okay, perfect.

471
00:15:22,330 --> 00:15:22,337
Okay.

472
00:15:22,338 --> 00:15:23,345
Hello.

473
00:15:23,346 --> 00:15:25,360
My name is David Kappel.

474
00:15:25,361 --> 00:15:28,391
I'm a researcher and group leader at the

475
00:15:28,391 --> 00:15:28,391
Institute for Moyo Informatics at the

476
00:15:28,391 --> 00:15:32,437
Rural University Bohom.

477
00:15:33,447 --> 00:15:36,470
So I'm leading the group on sustainable

478
00:15:36,470 --> 00:15:36,470
machine learning and we have a very

479
00:15:36,470 --> 00:15:40,516
strong focus on neuromorphic computing.

480
00:15:40,516 --> 00:15:41,528
That's why I'm here today.

481
00:15:42,535 --> 00:15:44,556
And I'm going to start with a very

482
00:15:44,556 --> 00:15:44,556
similar motivation than Sarah did, which

483
00:15:44,556 --> 00:15:44,556
was really a great inspiration for this

484
00:15:44,556 --> 00:15:50,614
talk, I think.

485
00:15:50,618 --> 00:15:53,644
So probably most of you have seen this

486
00:15:53,644 --> 00:15:53,644
interesting recent result, and I don't

487
00:15:53,644 --> 00:15:53,644
mean Germany winning the basketball

488
00:15:53,644 --> 00:15:53,644
championship, but this really big leap in

489
00:15:53,644 --> 00:15:53,644
artificial intelligence that we have seen

490
00:15:53,644 --> 00:15:53,644
in the last years, especially the last

491
00:15:53,644 --> 00:16:08,732
two or three years.

492
00:16:08,733 --> 00:16:11,760
So this is a picture generated from a

493
00:16:11,760 --> 00:16:11,760
prompt by a Dali network and it's really

494
00:16:11,760 --> 00:16:14,797
amazing.

495
00:16:14,797 --> 00:16:17,820
It was considered science fiction like

496
00:16:17,820 --> 00:16:18,835
only two, three years ago.

497
00:16:19,843 --> 00:16:22,870
And this was essentially made possible by

498
00:16:22,870 --> 00:16:22,870
a neuromorphic approach, which is deep

499
00:16:22,870 --> 00:16:25,906
neural networks.

500
00:16:25,906 --> 00:16:28,933
And these deep neural networks have

501
00:16:28,933 --> 00:16:28,933
become huge now, but this comes also with

502
00:16:28,933 --> 00:16:32,972
a caveat.

503
00:16:32,973 --> 00:16:35,004
So basically the flip side is that among

504
00:16:35,004 --> 00:16:35,004
other problems maybe these models may

505
00:16:35,004 --> 00:16:35,004
have, they consume huge amounts of energy.

506
00:16:35,004 --> 00:16:41,067
.

507
00:16:41,068 --> 00:16:45,104
So models like Dali or Jet GPT, as Sarah

508
00:16:45,104 --> 00:16:45,104
already mentioned, they would consume

509
00:16:45,104 --> 00:16:45,104
energy budgets that are comparable to

510
00:16:45,104 --> 00:16:53,188
houses or cars.

511
00:16:53,189 --> 00:16:57,220
So training Chat GPT a single time is

512
00:16:57,220 --> 00:17:00,191
like a gigawatt hour approximately.

513
00:17:00,192 --> 00:17:04,233
So that would be 300 tons of CO2 emission

514
00:17:04,233 --> 00:17:04,233
and many times, which comes down to many

515
00:17:04,233 --> 00:17:12,312
times the lifespan of a typical car.

516
00:17:12,318 --> 00:17:14,333
So this comes with two problems.

517
00:17:14,334 --> 00:17:16,357
Obviously, this makes training these

518
00:17:16,357 --> 00:17:16,357
models only accessible to a very small

519
00:17:16,357 --> 00:17:21,401
number of very large players.

520
00:17:21,402 --> 00:17:23,419
So essentially the big tech companies.

521
00:17:23,424 --> 00:17:25,445
And secondly, and maybe even more

522
00:17:25,445 --> 00:17:25,445
importantly, this is not compatible with

523
00:17:25,445 --> 00:17:30,492
a planet with limited resources.

524
00:17:30,493 --> 00:17:33,523
So if the growth rate of AI continues

525
00:17:33,523 --> 00:17:33,523
like it did in the last years, it will

526
00:17:33,523 --> 00:17:33,523
consume 13% of the global energy

527
00:17:33,523 --> 00:17:33,523
consumption by 2030 and it will basically

528
00:17:33,523 --> 00:17:33,523
outrun the transportation sector in

529
00:17:33,523 --> 00:17:48,669
another five years or so.

530
00:17:49,680 --> 00:17:50,698
So this raises the question, does

531
00:17:50,698 --> 00:17:50,698
sustainable machine learning exist at

532
00:17:50,698 --> 00:17:53,721
all?

533
00:17:53,722 --> 00:17:55,743
And obviously, since I'm working in the

534
00:17:55,743 --> 00:17:55,743
group of sustainable machine learning, I

535
00:17:55,743 --> 00:17:58,773
believe it does.

536
00:17:58,778 --> 00:18:01,748
And why I think it does is because we

537
00:18:01,748 --> 00:18:01,748
know a system that is very efficient and

538
00:18:01,748 --> 00:18:01,748
is still probably better than these AI

539
00:18:01,748 --> 00:18:01,748
models, which is the human brain, which

540
00:18:01,748 --> 00:18:01,748
consumes, as Sarah mentioned, around 20

541
00:18:01,748 --> 00:18:16,899
watts or four bananas a day.

542
00:18:20,932 --> 00:18:22,956
It's many orders of magnitude more

543
00:18:22,956 --> 00:18:22,956
efficient than the AI models we have

544
00:18:22,956 --> 00:18:25,987
today.

545
00:18:27,000 --> 00:18:30,003
But so far we don't know essentially how

546
00:18:30,003 --> 00:18:30,003
these networks work and especially how to

547
00:18:30,003 --> 00:18:34,007
train them.

548
00:18:34,007 --> 00:18:37,010
And basically our goal is to transfer now

549
00:18:37,010 --> 00:18:39,012
mechanisms from machine learning.

550
00:18:40,013 --> 00:18:42,015
We have this nice picture here.

551
00:18:42,015 --> 00:18:44,017
Basically we start from the machine

552
00:18:44,017 --> 00:18:44,017
learning side where we already know our

553
00:18:44,017 --> 00:18:47,020
way around.

554
00:18:48,021 --> 00:18:50,023
So we have these models that are

555
00:18:50,023 --> 00:18:50,023
wonderful and that give us really

556
00:18:50,023 --> 00:18:50,023
impressive results, but they are not

557
00:18:50,023 --> 00:18:50,023
efficient and we want to transfer them to

558
00:18:50,023 --> 00:19:02,029
a new efficient AI generation.

559
00:19:02,029 --> 00:19:05,032
And our idea is to use inspiration from

560
00:19:05,032 --> 00:19:05,032
neuroscience that make this transfer

561
00:19:05,032 --> 00:19:11,038
faster and possible in the first place.

562
00:19:13,039 --> 00:19:13,040
Okay?

563
00:19:15,042 --> 00:19:17,044
Actually, biology is a great source of

564
00:19:17,044 --> 00:19:17,044
inspiration and always comes around the

565
00:19:17,044 --> 00:19:22,049
corner with very surprising results.

566
00:19:22,049 --> 00:19:26,052
And one of these results that I stumbled

567
00:19:26,052 --> 00:19:26,052
upon a couple of years ago is that

568
00:19:26,052 --> 00:19:32,059
reliability of synapses in the brain.

569
00:19:32,059 --> 00:19:34,061
So, as you probably know, neurons in your

570
00:19:34,061 --> 00:19:36,063
brain are connected for synapses.

571
00:19:37,064 --> 00:19:40,067
But if you look at this is a paper from

572
00:19:40,067 --> 00:19:40,067
2019, and they could actually identify

573
00:19:40,067 --> 00:19:40,067
individual synapses and they could

574
00:19:40,067 --> 00:19:40,067
trigger them to make a synaptic release,

575
00:19:40,067 --> 00:19:51,078
like a single transmission.

576
00:19:51,078 --> 00:19:54,081
But if you look at these measurements,

577
00:19:54,081 --> 00:19:54,081
you see that this is really covered in

578
00:19:54,081 --> 00:19:58,085
noise.

579
00:19:58,085 --> 00:20:01,082
So essentially, if you basically average

580
00:20:01,082 --> 00:20:01,082
over this and zoom this out, you see here

581
00:20:01,082 --> 00:20:01,082
these typical synaptic traces, which is

582
00:20:01,082 --> 00:20:07,088
the average white line here.

583
00:20:07,088 --> 00:20:11,092
But below that, you see this huge jitter.

584
00:20:11,092 --> 00:20:11,092
.

585
00:20:11,092 --> 00:20:14,095
So they really like go several standard

586
00:20:14,095 --> 00:20:16,097
deviations up and down.

587
00:20:17,098 --> 00:20:19,100
And this is actually very surprising

588
00:20:19,100 --> 00:20:19,100
given that neurons are probably the

589
00:20:19,100 --> 00:20:19,100
single most costly cell type in your body

590
00:20:19,100 --> 00:20:26,107
in terms of energy consumption.

591
00:20:26,107 --> 00:20:29,110
They are really in comparison, they

592
00:20:29,110 --> 00:20:29,110
consume quite a bit of energy in your

593
00:20:29,110 --> 00:20:33,114
body.

594
00:20:33,114 --> 00:20:35,116
So you would expect that these

595
00:20:35,116 --> 00:20:35,116
transmissions that are communicated

596
00:20:35,116 --> 00:20:35,116
between neurons, which are very costly,

597
00:20:35,116 --> 00:20:42,123
should be highly reliable.

598
00:20:42,123 --> 00:20:45,126
So this is very counterintuitive these

599
00:20:45,126 --> 00:20:45,126
results and has been puzzling

600
00:20:45,126 --> 00:20:49,130
neuroscientists for quite a while now.

601
00:20:51,132 --> 00:20:55,136
And then there is a second puzzling

602
00:20:55,136 --> 00:20:55,136
observation, which is that the morphology

603
00:20:55,136 --> 00:21:05,140
of neurons looks somewhat like this.

604
00:21:05,140 --> 00:21:07,142
So this would be your typical pyramidal

605
00:21:07,142 --> 00:21:09,144
neuron you have in your cortex.

606
00:21:11,146 --> 00:21:14,149
But you see that this is actually, for a

607
00:21:14,149 --> 00:21:16,151
cell, quite big and elongated.

608
00:21:16,151 --> 00:21:19,154
So this can be up to a millimeter in the

609
00:21:19,154 --> 00:21:19,154
human brain, which means that if a

610
00:21:19,154 --> 00:21:19,154
synapse fires somewhere here, it has a

611
00:21:19,154 --> 00:21:19,154
very hard time communicating with the

612
00:21:19,154 --> 00:21:27,162
cell body, which is down here.

613
00:21:27,162 --> 00:21:30,165
So the electrical signals that are

614
00:21:30,165 --> 00:21:30,165
produced here may travel down here, but

615
00:21:30,165 --> 00:21:30,165
the synapse up here has no way of

616
00:21:30,165 --> 00:21:30,165
measuring the actual voltages at the cell

617
00:21:30,165 --> 00:21:40,174
body.

618
00:21:40,175 --> 00:21:41,176
And this is actually the interesting

619
00:21:41,176 --> 00:21:41,176
place because here are the action

620
00:21:41,176 --> 00:21:43,178
potentials formed.

621
00:21:43,178 --> 00:21:45,180
So if the synapse would really like to

622
00:21:45,180 --> 00:21:45,180
know about what is going on in the cell

623
00:21:45,180 --> 00:21:45,180
body so that it can make predictions

624
00:21:45,180 --> 00:21:45,180
about how the neuron will behave and how

625
00:21:45,180 --> 00:21:54,189
it interacts with the world.

626
00:21:55,189 --> 00:21:58,193
And this is another very puzzling or open

627
00:21:58,193 --> 00:21:58,193
problem in neuroscience, how this

628
00:21:58,193 --> 00:21:58,193
communication actually works out in

629
00:21:58,193 --> 00:21:58,193
single neurons between the cell body and

630
00:21:58,193 --> 00:22:09,198
the SynOps.

631
00:22:09,198 --> 00:22:12,201
It is known that actually the action

632
00:22:12,201 --> 00:22:13,202
potentials can travel back up.

633
00:22:13,202 --> 00:22:16,205
So they see kind of this binary variable

634
00:22:16,205 --> 00:22:16,205
when the neuron spikes, but they cannot

635
00:22:16,205 --> 00:22:16,205
actually measure the membrane potential

636
00:22:16,205 --> 00:22:22,211
down here.

637
00:22:23,212 --> 00:22:25,214
So only the most prominent electrical

638
00:22:25,214 --> 00:22:25,214
signals can actually back propagate

639
00:22:25,214 --> 00:22:28,217
through this.

640
00:22:29,218 --> 00:22:31,220
And this suggests that the synapse

641
00:22:31,220 --> 00:22:31,220
actually has very sparse information

642
00:22:31,220 --> 00:22:38,227
about what's going on in the cell body.

643
00:22:38,227 --> 00:22:41,230
And most models of synaptic plasticity

644
00:22:41,230 --> 00:22:44,233
don't cover this at all.

645
00:22:45,234 --> 00:22:48,237
And we were wondering how does this

646
00:22:48,237 --> 00:22:50,239
interaction work?

647
00:22:50,239 --> 00:22:53,242
How can the synapse produce useful

648
00:22:53,242 --> 00:22:53,242
learning signals given this sparse

649
00:22:53,242 --> 00:22:53,242
information about this important state of

650
00:22:53,242 --> 00:23:00,243
the neuron?

651
00:23:01,244 --> 00:23:03,246
And our idea was that essentially these

652
00:23:03,246 --> 00:23:03,246
two observations, these high levels of

653
00:23:03,246 --> 00:23:03,246
noise and the synapse and this large

654
00:23:03,246 --> 00:23:03,246
distance between cell body and synapses

655
00:23:03,246 --> 00:23:03,246
which give these high uncertainties,

656
00:23:03,246 --> 00:23:03,246
these are actually the two sides of the

657
00:23:03,246 --> 00:23:14,257
same coin.

658
00:23:14,257 --> 00:23:17,260
So our hypothesis was that actually we

659
00:23:17,260 --> 00:23:17,260
could use the same models that we know

660
00:23:17,260 --> 00:23:17,260
already from the behavioral level, how an

661
00:23:17,260 --> 00:23:17,260
agent can act and perform in an

662
00:23:17,260 --> 00:23:31,274
environment of high uncertainty.

663
00:23:31,274 --> 00:23:35,278
And we just apply this to every sign ups

664
00:23:35,278 --> 00:23:38,281
and there is an error.

665
00:23:38,281 --> 00:23:41,284
So every sign up should utilize these

666
00:23:41,284 --> 00:23:43,286
same models, basically.

667
00:23:45,288 --> 00:23:48,291
And this model would immediately suggest

668
00:23:48,291 --> 00:23:48,291
that actually synaptic transmissions

669
00:23:48,291 --> 00:23:48,291
should be noisy and these levels of noise

670
00:23:48,291 --> 00:23:48,291
would express uncertainty about the

671
00:23:48,291 --> 00:23:56,299
environment.

672
00:23:58,301 --> 00:24:00,297
And then we can use this model to derive

673
00:24:00,297 --> 00:24:00,297
learning rules and we can compare them

674
00:24:00,297 --> 00:24:04,301
side by side to biology.

675
00:24:05,302 --> 00:24:07,304
And this is the first thing I want to

676
00:24:07,304 --> 00:24:07,304
show you.

677
00:24:07,304 --> 00:24:10,307
So I just give a very quick introduction

678
00:24:10,307 --> 00:24:10,307
to the free energy model, because some of

679
00:24:10,307 --> 00:24:10,307
you might not be that familiar with it,

680
00:24:10,307 --> 00:24:10,307
but this is essentially a model to

681
00:24:10,307 --> 00:24:20,317
describe a situation like this.

682
00:24:20,317 --> 00:24:22,319
You have a person that is interacting

683
00:24:22,319 --> 00:24:22,319
with some environment and here I assumed

684
00:24:22,319 --> 00:24:25,322
very simple.

685
00:24:25,322 --> 00:24:28,325
So this person tries to throw a ball to

686
00:24:28,325 --> 00:24:29,326
some target.

687
00:24:29,326 --> 00:24:33,329
And we as humans, we are good in solving

688
00:24:33,329 --> 00:24:33,330
such tasks.

689
00:24:34,331 --> 00:24:35,332
And we are also good in solving such

690
00:24:35,332 --> 00:24:35,332
tasks if there is high levels of

691
00:24:35,332 --> 00:24:38,335
uncertainty in this.

692
00:24:38,335 --> 00:24:41,338
So if the person may receive some visual

693
00:24:41,338 --> 00:24:41,338
feedback, but a lot of this feedback may

694
00:24:41,338 --> 00:24:45,342
be hidden.

695
00:24:45,342 --> 00:24:47,344
So you can imagine this going on behind

696
00:24:47,344 --> 00:24:48,345
some wall.

697
00:24:49,346 --> 00:24:52,349
And the person now still might want to

698
00:24:52,349 --> 00:24:52,349
predict what is the trajectory of this

699
00:24:52,349 --> 00:24:52,349
ball flying towards the target so that it

700
00:24:52,349 --> 00:25:00,351
can make an accurate action.

701
00:25:00,351 --> 00:25:03,354
So we will assign some variables to these

702
00:25:03,354 --> 00:25:04,355
states here.

703
00:25:04,355 --> 00:25:06,357
So we have essentially this feedback that

704
00:25:06,357 --> 00:25:08,359
this person can observe.

705
00:25:08,359 --> 00:25:10,361
And we have this unobserved state of the

706
00:25:10,361 --> 00:25:10,361
ball flying here, which we call U, where

707
00:25:10,361 --> 00:25:10,361
the person doesn't have direct access to

708
00:25:10,361 --> 00:25:10,361
it, would only see parts of it, for

709
00:25:10,361 --> 00:25:10,361
example, when the ball is at the corners

710
00:25:10,361 --> 00:25:21,372
of the ball appearing.

711
00:25:21,372 --> 00:25:25,376
And then to model this essentially, or to

712
00:25:25,376 --> 00:25:25,376
describe this behavior, the person would

713
00:25:25,376 --> 00:25:25,376
have an internal description of this

714
00:25:25,376 --> 00:25:32,383
trajectory.

715
00:25:33,384 --> 00:25:37,388
So an internal model of this state, U.

716
00:25:38,389 --> 00:25:42,393
And this model would then be updated to

717
00:25:42,393 --> 00:25:45,396
match the observed feedback.

718
00:25:46,397 --> 00:25:48,399
And this can be described very nicely in

719
00:25:48,399 --> 00:25:48,399
this beautiful mathematical framework of

720
00:25:48,399 --> 00:25:52,403
the free energy principle.

721
00:25:52,403 --> 00:25:55,406
So the idea is that you would essentially

722
00:25:55,406 --> 00:25:55,406
establish a model of your internal state,

723
00:25:55,406 --> 00:25:55,406
so essentially how the internal states

724
00:25:55,406 --> 00:25:55,406
and the state of the environment interact.

725
00:25:55,406 --> 00:26:06,411
.

726
00:26:07,412 --> 00:26:09,414
And you would have a model of the

727
00:26:09,414 --> 00:26:09,414
feedback, so how the states and the

728
00:26:09,414 --> 00:26:13,418
feedback you observe interact.

729
00:26:13,418 --> 00:26:17,422
And essentially you can then write down a

730
00:26:17,422 --> 00:26:17,422
loss function that measures the distance

731
00:26:17,422 --> 00:26:17,422
between this model of the internal state

732
00:26:17,422 --> 00:26:17,422
and this model of the feedback and the

733
00:26:17,422 --> 00:26:28,433
external state.

734
00:26:30,435 --> 00:26:34,439
And then by essentially minimizing this

735
00:26:34,439 --> 00:26:34,439
distance between the two, you can derive

736
00:26:34,439 --> 00:26:34,439
all sorts of behaviorally relevant, can

737
00:26:34,439 --> 00:26:34,439
solve behaviorally relevant problems, for

738
00:26:34,439 --> 00:26:46,451
example, learning.

739
00:26:46,451 --> 00:26:48,453
But you can also use this for other

740
00:26:48,453 --> 00:26:48,453
things, like figuring out what are good

741
00:26:48,453 --> 00:26:50,455
actions, for example.

742
00:26:51,456 --> 00:26:53,458
So making inference about both the

743
00:26:53,458 --> 00:26:55,460
internal states and the actions.

744
00:26:55,460 --> 00:26:57,462
For example, and this is in a nutshell,

745
00:26:57,462 --> 00:26:59,464
the free energy principle.

746
00:26:59,464 --> 00:27:01,460
And this object here happens to be what

747
00:27:01,460 --> 00:27:01,460
is known as the variation of free energy,

748
00:27:01,460 --> 00:27:01,460
which is also just coincides with

749
00:27:01,460 --> 00:27:07,466
statistical physics.

750
00:27:07,466 --> 00:27:08,467
And this is where this framework has its

751
00:27:08,467 --> 00:27:09,468
name from.

752
00:27:09,468 --> 00:27:12,471
But you see that all is probabilistic

753
00:27:12,471 --> 00:27:12,471
here.

754
00:27:12,471 --> 00:27:14,473
So essentially you have two probability

755
00:27:14,473 --> 00:27:14,473
functions q for the internal model and P

756
00:27:14,473 --> 00:27:14,473
for this interaction between states and

757
00:27:14,473 --> 00:27:20,479
observations.

758
00:27:20,479 --> 00:27:22,481
And you have here a distance measure

759
00:27:22,481 --> 00:27:25,484
between them that you want to minimize.

760
00:27:26,485 --> 00:27:28,487
So now if we look at the neuron and the

761
00:27:28,487 --> 00:27:28,487
synapse and how they interact with each

762
00:27:28,487 --> 00:27:32,491
other, we find a very similar picture.

763
00:27:32,491 --> 00:27:36,495
So single SynOps, which we have here in

764
00:27:36,495 --> 00:27:36,495
green, has an internal state which we,

765
00:27:36,495 --> 00:27:36,495
for simplicity, model only as the

766
00:27:36,495 --> 00:27:44,503
synaptic weight.

767
00:27:44,503 --> 00:27:47,506
Based on this, the SynOps, when it's

768
00:27:47,506 --> 00:27:47,506
triggered by a presynaptic spike, it

769
00:27:47,506 --> 00:27:52,511
would generate a post synaptic current.

770
00:27:52,511 --> 00:27:54,513
This would then propagate to the soma,

771
00:27:54,513 --> 00:27:54,513
which is our external state, which we

772
00:27:54,513 --> 00:27:54,513
cannot directly observe because it's too

773
00:27:54,513 --> 00:28:01,514
far away from the synapse.

774
00:28:01,514 --> 00:28:03,516
But we can see a feedback, which is this

775
00:28:03,516 --> 00:28:03,516
back propagating action potential, which

776
00:28:03,516 --> 00:28:03,516
is this binary variable that tells us

777
00:28:03,516 --> 00:28:08,521
whether the neuron has spiked or not.

778
00:28:09,522 --> 00:28:11,524
So this is exactly the same framework if

779
00:28:11,524 --> 00:28:13,526
we write it down like that.

780
00:28:13,526 --> 00:28:15,528
And we can just use the same mathematics

781
00:28:15,528 --> 00:28:16,529
to solve it.

782
00:28:18,531 --> 00:28:21,534
So to solve it, we only have to come up

783
00:28:21,534 --> 00:28:21,534
with a couple of we have to make a couple

784
00:28:21,534 --> 00:28:25,538
of assumptions.

785
00:28:25,538 --> 00:28:27,540
So we have to write down a model for this

786
00:28:27,540 --> 00:28:28,541
guy here.

787
00:28:28,541 --> 00:28:33,546
So this model of how the feedback and the

788
00:28:33,546 --> 00:28:36,549
external state interact.

789
00:28:36,549 --> 00:28:38,551
But we have very good models for this.

790
00:28:38,551 --> 00:28:41,554
This has been studied over many years.

791
00:28:41,554 --> 00:28:43,556
So here you see how typically a model

792
00:28:43,556 --> 00:28:44,557
neuron behaves.

793
00:28:44,557 --> 00:28:46,559
So you have here the membrane potential

794
00:28:46,559 --> 00:28:48,561
of a leaky integrated fire neuron.

795
00:28:48,561 --> 00:28:50,563
And you see that this is just going up

796
00:28:50,563 --> 00:28:51,563
and down.

797
00:28:51,564 --> 00:28:53,566
So this neuron would receive a lot of

798
00:28:53,566 --> 00:28:56,569
presynaptic input and maybe also noise.

799
00:28:56,569 --> 00:28:57,570
And eventually at some point it hits a

800
00:28:57,570 --> 00:28:59,572
threshold, it would generate a spike.

801
00:28:59,572 --> 00:29:01,568
So that would be the sets that travels to

802
00:29:01,568 --> 00:29:01,568
the downstream neurons and also back to

803
00:29:01,568 --> 00:29:04,571
the SynOps.

804
00:29:05,572 --> 00:29:09,576
And then it resets, right?

805
00:29:11,578 --> 00:29:14,581
So we can write this down mathematically

806
00:29:14,581 --> 00:29:16,583
as a very simple differential equation.

807
00:29:16,583 --> 00:29:18,585
But the neuron doesn't have access to

808
00:29:18,585 --> 00:29:20,587
this state again.

809
00:29:20,587 --> 00:29:22,589
So it's again behind this wall, it only

810
00:29:22,589 --> 00:29:24,591
sees these spike events.

811
00:29:24,591 --> 00:29:26,593
But we can actually, for this simple case

812
00:29:26,593 --> 00:29:26,593
of a leaky integrated fire neuron, we can

813
00:29:26,593 --> 00:29:29,596
solve this analytically.

814
00:29:29,596 --> 00:29:31,598
So we can write down what is the

815
00:29:31,598 --> 00:29:31,598
posterior distribution of membrane

816
00:29:31,598 --> 00:29:35,601
potentials given the spike times.

817
00:29:35,602 --> 00:29:38,605
And what comes out of this is actually a

818
00:29:38,605 --> 00:29:38,605
so called stochastic bridge model, or in

819
00:29:38,605 --> 00:29:38,605
this case of a leak integrated in fire

820
00:29:38,605 --> 00:29:45,612
neuron.

821
00:29:45,612 --> 00:29:47,614
It's an orange bridge model.

822
00:29:47,614 --> 00:29:49,616
So this can be written down analytically.

823
00:29:49,616 --> 00:29:49,616
.

824
00:29:51,618 --> 00:29:53,620
It's not simple, but it's doable.

825
00:29:55,622 --> 00:29:57,624
And we can then just use this directly.

826
00:29:57,624 --> 00:30:00,621
So this model that we have to again write

827
00:30:00,621 --> 00:30:02,623
down these free energy functionals.

828
00:30:02,623 --> 00:30:05,626
So we make here an assumption how the

829
00:30:05,626 --> 00:30:05,626
synapse actually produces post synoptic

830
00:30:05,626 --> 00:30:05,626
currents and how they are integrated in

831
00:30:05,626 --> 00:30:10,631
the neuron.

832
00:30:10,631 --> 00:30:13,634
But that's also given by the leaky

833
00:30:13,634 --> 00:30:13,634
integrated fire neuron and actually the

834
00:30:13,634 --> 00:30:13,634
stochastic inputs that the synapse

835
00:30:13,634 --> 00:30:18,639
generates.

836
00:30:18,639 --> 00:30:21,642
So for simplicity, we only assume here

837
00:30:21,642 --> 00:30:21,642
basically Gaussian synapses that would

838
00:30:21,642 --> 00:30:21,642
inject draw a Gaussian random variable

839
00:30:21,642 --> 00:30:21,642
and inject this to a leaky integrated

840
00:30:21,642 --> 00:30:29,650
fire neuron.

841
00:30:29,650 --> 00:30:31,652
And then all these ingredients actually

842
00:30:31,652 --> 00:30:31,652
can be solved in closed form and we can

843
00:30:31,652 --> 00:30:31,652
derive learning rules that would minimize

844
00:30:31,652 --> 00:30:36,657
this free energy functional.

845
00:30:36,657 --> 00:30:40,661
Now, and if we do that, this has a bunch

846
00:30:40,661 --> 00:30:40,661
of nice properties because this Einstein

847
00:30:40,661 --> 00:30:40,661
Beck bridge is completely determined by

848
00:30:40,661 --> 00:30:52,672
the back propagating action potentials.

849
00:30:52,673 --> 00:30:54,675
So basically the times of the post

850
00:30:54,675 --> 00:30:54,675
synaptic spikes that arrive at the neuron,

851
00:30:54,675 --> 00:30:54,675
, this shape that we get here only

852
00:30:54,675 --> 00:30:54,675
depends on two neighboring post synaptic

853
00:30:54,675 --> 00:30:54,675
spikes, which means that we get here

854
00:30:54,675 --> 00:30:54,675
automatically a learning rule that looks

855
00:30:54,675 --> 00:31:11,686
like this.

856
00:31:11,686 --> 00:31:14,689
So a learning rule that only depends on

857
00:31:14,689 --> 00:31:14,689
the difference between two postal optic

858
00:31:14,689 --> 00:31:14,689
spikes, which we call here delta t two,

859
00:31:14,689 --> 00:31:14,689
and the difference between the post

860
00:31:14,689 --> 00:31:14,689
synoptic spike and the actual input that

861
00:31:14,689 --> 00:31:14,689
is triggered at some point on the

862
00:31:14,689 --> 00:31:28,703
presynaptic side.

863
00:31:30,705 --> 00:31:32,707
And we can basically make here this

864
00:31:32,707 --> 00:31:32,707
lookup table and just compute what would

865
00:31:32,707 --> 00:31:32,707
be the update that these sign ups would

866
00:31:32,707 --> 00:31:32,707
need to make so that it learns optimally

867
00:31:32,707 --> 00:31:44,719
in terms of this free energy principle.

868
00:31:44,719 --> 00:31:45,720
And this is the shape that we get out.

869
00:31:45,720 --> 00:31:48,723
You see that there is a strong dependence

870
00:31:48,723 --> 00:31:48,723
on the post synoptic firing rate, but

871
00:31:48,723 --> 00:31:48,723
there is also a dependence on basically

872
00:31:48,723 --> 00:31:48,723
this typical STDP that Sarah mentioned

873
00:31:48,723 --> 00:31:48,723
before, what is the relative positioning

874
00:31:48,723 --> 00:31:59,734
of the pre and postsynoptic spike.

875
00:31:59,734 --> 00:32:02,731
So in a nutshell, this model can now be

876
00:32:02,731 --> 00:32:05,734
split into essentially two pathways.

877
00:32:05,734 --> 00:32:08,737
So we have this ATOC response, which

878
00:32:08,737 --> 00:32:08,737
basically just whenever there is a

879
00:32:08,737 --> 00:32:08,737
presynaptic spike that triggers an action

880
00:32:08,737 --> 00:32:08,737
in the synapse, we would draw from this

881
00:32:08,737 --> 00:32:08,737
Gaussian distribution and inject it into

882
00:32:08,737 --> 00:32:20,749
the neuron.

883
00:32:20,749 --> 00:32:23,752
And then there is this postalk update

884
00:32:23,752 --> 00:32:23,752
where the synapse would look up in this

885
00:32:23,752 --> 00:32:23,752
onstain Woolenbeck bridge, what would

886
00:32:23,752 --> 00:32:23,752
have been the optimal output that it

887
00:32:23,752 --> 00:32:23,752
should have generated, so what would have

888
00:32:23,752 --> 00:32:37,766
been the optimal action.

889
00:32:37,766 --> 00:32:39,768
And then it compares the actual action

890
00:32:39,768 --> 00:32:39,768
with this optimal action according to

891
00:32:39,768 --> 00:32:39,768
this free energy principle and then

892
00:32:39,768 --> 00:32:39,768
generates a delayed response which is an

893
00:32:39,768 --> 00:32:48,777
update of the synaptic weight.

894
00:32:50,779 --> 00:32:50,779
Okay?

895
00:32:50,779 --> 00:32:53,782
And importantly, this internal model is

896
00:32:53,782 --> 00:32:54,782
only implicit.

897
00:32:54,783 --> 00:32:57,786
It's encoded, so to say, into this spike

898
00:32:57,786 --> 00:32:58,787
time dependent.

899
00:32:58,787 --> 00:32:59,788
Plasticity rule.

900
00:33:01,784 --> 00:33:03,786
So how do these rules then look and how

901
00:33:03,786 --> 00:33:05,788
do they compare to biology?

902
00:33:05,788 --> 00:33:08,791
And actually the fit is quite nicely

903
00:33:08,791 --> 00:33:08,791
given that this is derived really from

904
00:33:08,791 --> 00:33:08,791
first principles without making any

905
00:33:08,791 --> 00:33:13,796
assumptions.

906
00:33:13,796 --> 00:33:15,798
So this is the measurement in biology.

907
00:33:16,799 --> 00:33:19,801
This is this B and Al rule, b and Pool,

908
00:33:19,801 --> 00:33:19,801
very old work where they actually did

909
00:33:19,801 --> 00:33:19,801
this in vitro studies where they injected

910
00:33:19,801 --> 00:33:19,801
pre and post synaptic spikes and then

911
00:33:19,801 --> 00:33:19,801
they measured what is the weight change

912
00:33:19,801 --> 00:33:32,815
in the synopsis.

913
00:33:32,815 --> 00:33:34,817
And this is the rule that is predicted by

914
00:33:34,817 --> 00:33:35,818
our model.

915
00:33:35,818 --> 00:33:37,820
And you see that at least in a first

916
00:33:37,820 --> 00:33:37,820
order approximation, it gives us very

917
00:33:37,820 --> 00:33:41,823
similar shapes.

918
00:33:41,824 --> 00:33:44,827
And this also makes sense because the

919
00:33:44,827 --> 00:33:44,827
synapse wants to change the most when it'

920
00:33:44,827 --> 00:33:44,827
's close to the pre and post synoptic

921
00:33:44,827 --> 00:33:44,827
spike times because this is where or the

922
00:33:44,827 --> 00:33:44,827
post synoptic spike times because this is

923
00:33:44,827 --> 00:33:44,827
where it knows the most about the state

924
00:33:44,827 --> 00:34:01,838
of the postynaptic neuron.

925
00:34:01,838 --> 00:34:05,842
The free energy principle would actually

926
00:34:05,842 --> 00:34:05,842
suggest these kind of cones with almost

927
00:34:05,842 --> 00:34:12,849
no assumption essentially.

928
00:34:12,849 --> 00:34:14,851
But we also get because we have not just

929
00:34:14,851 --> 00:34:14,851
the first order spike time dependent

930
00:34:14,851 --> 00:34:14,851
plasticity rule, but we also have this

931
00:34:14,851 --> 00:34:14,851
dependency on the post not of the firing

932
00:34:14,851 --> 00:34:22,859
rate.

933
00:34:23,860 --> 00:34:25,862
We can also compare this to other results.

934
00:34:25,862 --> 00:34:25,862
.

935
00:34:25,862 --> 00:34:28,865
And this here is this old work by Kaupner

936
00:34:28,865 --> 00:34:28,865
and Brunel.

937
00:34:28,865 --> 00:34:32,869
So this is actually a model, but that was

938
00:34:32,869 --> 00:34:32,869
very detailed describing the plasticity

939
00:34:32,869 --> 00:34:32,869
based on the pre and post synoptic firing

940
00:34:32,869 --> 00:34:41,878
rate in the synapse.

941
00:34:41,878 --> 00:34:42,879
And this is what our model predicts.

942
00:34:42,879 --> 00:34:45,882
So if we inject render and pre and post

943
00:34:45,882 --> 00:34:45,882
synaptic bossBack trains with different

944
00:34:45,882 --> 00:34:45,882
rates, our model would predict this shape,

945
00:34:45,882 --> 00:34:52,889
, which again is not a perfect match.

946
00:34:52,889 --> 00:34:56,893
But given that this is a very idealistic

947
00:34:56,893 --> 00:34:56,893
model, it's actually the main features

948
00:34:56,893 --> 00:34:56,893
that low firing rates on the postynopstic

949
00:34:56,893 --> 00:34:56,893
side would lead to depression and higher

950
00:34:56,893 --> 00:35:09,900
to potentiation are reflected in this.

951
00:35:11,902 --> 00:35:14,905
Okay, I assume I still have ten minutes,

952
00:35:14,905 --> 00:35:15,906
right?

953
00:35:15,906 --> 00:35:17,908
Okay, so I would give a quick

954
00:35:17,908 --> 00:35:17,908
intermediate summary and then I want to

955
00:35:17,908 --> 00:35:17,908
show some other work where we actually

956
00:35:17,908 --> 00:35:17,908
apply this now to actual machine learning

957
00:35:17,908 --> 00:35:28,919
model.

958
00:35:31,921 --> 00:35:34,925
So what we have seen here is that

959
00:35:34,925 --> 00:35:34,925
synapses are actually are very stochastic

960
00:35:34,925 --> 00:35:41,932
and this was a big puzzle.

961
00:35:41,932 --> 00:35:44,935
And B suggests that actually the synaptic

962
00:35:44,935 --> 00:35:44,935
noise is actually the synapses way of

963
00:35:44,935 --> 00:35:44,935
reporting its own uncertainty about the

964
00:35:44,935 --> 00:35:44,935
environment, where the environment is

965
00:35:44,935 --> 00:35:44,935
actually the post synaptic neuron and it

966
00:35:44,935 --> 00:35:44,935
really interacts in this fashion of the

967
00:35:44,935 --> 00:35:44,935
free energy principle with this post

968
00:35:44,935 --> 00:35:44,935
synaptic neuron or that's a very nice way

969
00:35:44,935 --> 00:36:05,950
of describing it.

970
00:36:05,950 --> 00:36:09,954
And if you're interested more, there's a

971
00:36:09,954 --> 00:36:09,954
paper, a preprint out, you can read up on

972
00:36:09,954 --> 00:36:13,958
all this.

973
00:36:14,959 --> 00:36:16,961
Okay, so how does this now connect to

974
00:36:16,961 --> 00:36:17,962
neuromorphics?

975
00:36:18,963 --> 00:36:20,965
And actually so we are not actually doing

976
00:36:20,965 --> 00:36:20,965
neuromorphic hardware, we are doing

977
00:36:20,965 --> 00:36:24,969
neuromorphic algorithms.

978
00:36:24,969 --> 00:36:26,971
So we try to bring these inspirations now

979
00:36:26,971 --> 00:36:26,971
into actual machine learning models and

980
00:36:26,971 --> 00:36:26,971
we thought that this might be a good

981
00:36:26,971 --> 00:36:26,971
attack angle to solve a problem that is

982
00:36:26,971 --> 00:36:36,981
well known in machine learning.

983
00:36:37,982 --> 00:36:39,984
So I just drew here very simple

984
00:36:39,984 --> 00:36:39,984
convolutional neural networks with

985
00:36:39,984 --> 00:36:39,984
various convolutional layers and then

986
00:36:39,984 --> 00:36:39,984
maybe some dense layers that you would

987
00:36:39,984 --> 00:36:51,996
have in your machine learning algorithm.

988
00:36:51,996 --> 00:36:54,999
And the way this is trained, as many of

989
00:36:54,999 --> 00:36:54,999
you know, I guess, is through end to end

990
00:36:54,999 --> 00:36:58,003
errorback propagation.

991
00:36:58,003 --> 00:37:00,999
So the idea is that you have a training

992
00:37:00,999 --> 00:37:02,001
set which has inputs and targets.

993
00:37:02,001 --> 00:37:05,004
So, for example, in a classification task,

994
00:37:05,004 --> 00:37:05,004
, this could be pictures of cats and dogs.

995
00:37:05,004 --> 00:37:08,007
.

996
00:37:08,007 --> 00:37:11,010
And you would have targets which are

997
00:37:11,010 --> 00:37:12,011
class labels, so to say.

998
00:37:12,011 --> 00:37:15,014
So there's actually artificial neurons in

999
00:37:15,014 --> 00:37:15,014
there, and one of these neuron may be

1000
00:37:15,014 --> 00:37:15,014
active for cats and 1 may be active for

1001
00:37:15,014 --> 00:37:20,019
dogs.

1002
00:37:21,020 --> 00:37:23,022
And in your training, that data, you have

1003
00:37:23,022 --> 00:37:23,022
exactly these labels that were generated

1004
00:37:23,022 --> 00:37:23,022
by humans, that were sitting down doing

1005
00:37:23,022 --> 00:37:28,027
this by hand.

1006
00:37:28,027 --> 00:37:31,030
And then during training, you show these

1007
00:37:31,030 --> 00:37:31,030
examples to the network by propagating

1008
00:37:31,030 --> 00:37:31,030
these inputs all the way from the input

1009
00:37:31,030 --> 00:37:39,038
layer to the output layer.

1010
00:37:39,038 --> 00:37:42,041
Then the output is here compared to these

1011
00:37:42,041 --> 00:37:44,043
hand labeled targets.

1012
00:37:44,043 --> 00:37:46,045
And then the mismatch between the two is

1013
00:37:46,045 --> 00:37:46,045
back propagated through all these layers

1014
00:37:46,045 --> 00:37:50,049
back to the input.

1015
00:37:50,049 --> 00:37:52,051
And all the weights or the synoptic

1016
00:37:52,051 --> 00:37:52,051
weights that are here in between inside

1017
00:37:52,051 --> 00:37:52,051
these layers would then be updated

1018
00:37:52,051 --> 00:37:52,051
accordingly, so that after doing this

1019
00:37:52,051 --> 00:37:52,051
many, many times, this network becomes

1020
00:37:52,051 --> 00:38:05,058
good in telling apart Kelvin docs.

1021
00:38:06,059 --> 00:38:09,062
So this has a problem, this algorithm.

1022
00:38:09,062 --> 00:38:11,064
It works great in practice and it's the

1023
00:38:11,064 --> 00:38:11,064
foundation of all these models we have

1024
00:38:11,064 --> 00:38:11,064
talked about, like Dali or Jet GPT, but

1025
00:38:11,064 --> 00:38:18,071
it is quite inefficient.

1026
00:38:18,071 --> 00:38:20,073
And the problem is what is known in the

1027
00:38:20,073 --> 00:38:22,075
literature as the locking problem.

1028
00:38:22,075 --> 00:38:24,077
So if you would split up this network now

1029
00:38:24,077 --> 00:38:24,077
into blocks, which I already did before,

1030
00:38:24,077 --> 00:38:24,077
but this is arbitrary, but for

1031
00:38:24,077 --> 00:38:24,077
implementing this efficiently in terms of

1032
00:38:24,077 --> 00:38:24,077
a software algorithm, it might be

1033
00:38:24,077 --> 00:38:35,088
interesting to do that.

1034
00:38:35,088 --> 00:38:38,091
And now you would want these blocks

1035
00:38:38,091 --> 00:38:38,091
ideally to run in parallel, so that you

1036
00:38:38,091 --> 00:38:38,091
can basically show the first example on

1037
00:38:38,091 --> 00:38:38,091
this first block and then already train

1038
00:38:38,091 --> 00:38:38,091
it while the second block is doing

1039
00:38:38,091 --> 00:38:49,102
something else.

1040
00:38:49,102 --> 00:38:51,104
But this is not really possible with end

1041
00:38:51,104 --> 00:38:51,104
to end back propagation because of this

1042
00:38:51,104 --> 00:38:51,104
lock in problem, because the activation

1043
00:38:51,104 --> 00:38:51,104
of the second block depends on the

1044
00:38:51,104 --> 00:38:59,112
activation of the first block.

1045
00:38:59,112 --> 00:39:01,108
So you have to propagate it all the way

1046
00:39:01,108 --> 00:39:02,109
to the end.

1047
00:39:02,109 --> 00:39:04,111
Then you compute this error and you would

1048
00:39:04,111 --> 00:39:05,112
then back propagate.

1049
00:39:05,112 --> 00:39:09,116
And only when this is done, you can start

1050
00:39:09,116 --> 00:39:09,116
the next epoch where you show a new bunch

1051
00:39:09,116 --> 00:39:09,116
of examples and you see that during all

1052
00:39:09,116 --> 00:39:09,116
this time, here, the thread that would

1053
00:39:09,116 --> 00:39:09,116
run this first block maybe would be idle

1054
00:39:09,116 --> 00:39:09,116
and has to wait essentially all the time.

1055
00:39:09,116 --> 00:39:26,133
.

1056
00:39:26,133 --> 00:39:28,135
And this obviously makes them very

1057
00:39:28,135 --> 00:39:28,135
inefficient.

1058
00:39:28,135 --> 00:39:30,137
And now our idea was that we use

1059
00:39:30,137 --> 00:39:30,137
basically what we had learned from this

1060
00:39:30,137 --> 00:39:30,137
earlier model on how synopsis communicate

1061
00:39:30,137 --> 00:39:30,137
over these long distances through this

1062
00:39:30,137 --> 00:39:30,137
free energy principle, and also apply it

1063
00:39:30,137 --> 00:39:44,151
just to a deep neural network.

1064
00:39:44,151 --> 00:39:48,155
And the idea is that you have here, again,

1065
00:39:48,155 --> 00:39:48,155
, basically you have already this

1066
00:39:48,155 --> 00:39:55,162
generation of inputs to some output.

1067
00:39:55,162 --> 00:39:58,165
But what is missing to make it applicable

1068
00:39:58,165 --> 00:39:58,165
to the free energy principle is this

1069
00:39:58,165 --> 00:40:02,163
feedback that you always need.

1070
00:40:02,163 --> 00:40:05,166
The idea was that we put here a very

1071
00:40:05,166 --> 00:40:07,168
lightweight feedback network.

1072
00:40:07,168 --> 00:40:10,171
So essentially each of these blocks now

1073
00:40:10,171 --> 00:40:10,171
in this deep neural network would be

1074
00:40:10,171 --> 00:40:10,171
accompanied by a feedback block that

1075
00:40:10,171 --> 00:40:18,179
locally generates a target.

1076
00:40:18,179 --> 00:40:20,181
So we used here really in the simplest

1077
00:40:20,181 --> 00:40:20,181
case, which we have, so this is very

1078
00:40:20,181 --> 00:40:20,181
recent work, we only used linear blocks

1079
00:40:20,181 --> 00:40:25,186
so far.

1080
00:40:25,186 --> 00:40:28,189
So these are single linear layers and we

1081
00:40:28,189 --> 00:40:28,189
would generate now these outputs here in

1082
00:40:28,189 --> 00:40:28,189
these feedback blocks and then use the

1083
00:40:28,189 --> 00:40:28,189
free energy principle to derive a local

1084
00:40:28,189 --> 00:40:37,198
loss.

1085
00:40:37,198 --> 00:40:39,200
That allows us again to minimize both

1086
00:40:39,200 --> 00:40:39,200
these feedback weights that we have here

1087
00:40:39,200 --> 00:40:39,200
and also the weights in the forward

1088
00:40:39,200 --> 00:40:45,206
network.

1089
00:40:46,207 --> 00:40:48,209
So it's essentially the same idea.

1090
00:40:48,209 --> 00:40:50,211
So we have here these outputs which we

1091
00:40:50,211 --> 00:40:50,211
interpret now as parameters to a

1092
00:40:50,211 --> 00:40:53,214
probability function.

1093
00:40:54,214 --> 00:40:56,217
So we can apply this probabilistic

1094
00:40:56,217 --> 00:40:56,217
framework, but now all the rest basically

1095
00:40:56,217 --> 00:41:00,215
rolls out the same way.

1096
00:41:00,215 --> 00:41:03,218
So we assume that these outputs are

1097
00:41:03,218 --> 00:41:03,218
essentially the internal states of this

1098
00:41:03,218 --> 00:41:03,218
model and we have these observations

1099
00:41:03,218 --> 00:41:13,228
given in the inputs and the targets.

1100
00:41:13,228 --> 00:41:17,232
And now we try to minimize basically P

1101
00:41:17,232 --> 00:41:17,232
would now be this feed forward network

1102
00:41:17,232 --> 00:41:17,232
and Q would be now a function that

1103
00:41:17,232 --> 00:41:17,232
contains features of both the feedback

1104
00:41:17,232 --> 00:41:31,246
and the feed forward network.

1105
00:41:32,246 --> 00:41:35,250
And the nice thing is that if we I don't

1106
00:41:35,250 --> 00:41:35,250
have time to go into the details now, but

1107
00:41:35,250 --> 00:41:35,250
if you write this out, you see that this

1108
00:41:35,250 --> 00:41:35,250
actually decomposes, this lock term here

1109
00:41:35,250 --> 00:41:35,250
decomposes into local linear terms that

1110
00:41:35,250 --> 00:41:50,265
give you these local losses here.

1111
00:41:50,265 --> 00:41:53,268
So essentially that you can minimize here

1112
00:41:53,268 --> 00:41:53,268
block local between forward and the

1113
00:41:53,268 --> 00:41:53,268
corresponding feedback block, a loss

1114
00:41:53,268 --> 00:42:00,269
function.

1115
00:42:00,269 --> 00:42:03,272
And you can then actually do this in

1116
00:42:03,272 --> 00:42:03,272
parallel because maybe the picture is

1117
00:42:03,272 --> 00:42:07,276
good to see here.

1118
00:42:07,276 --> 00:42:09,278
So what you have to do now you have a bit

1119
00:42:09,278 --> 00:42:09,278
of overhead because you have this

1120
00:42:09,278 --> 00:42:11,280
feedback block.

1121
00:42:11,280 --> 00:42:15,284
So this would be the two execution times

1122
00:42:15,284 --> 00:42:15,284
of the feed forward block and the

1123
00:42:15,284 --> 00:42:20,289
feedback block.

1124
00:42:20,289 --> 00:42:23,292
But in principle they can run in parallel

1125
00:42:23,292 --> 00:42:23,292
and once the forward block is done, the

1126
00:42:23,292 --> 00:42:23,292
next forward block can start propagating

1127
00:42:23,292 --> 00:42:23,292
through this network, but simultaneously

1128
00:42:23,292 --> 00:42:23,292
already the forward block, because it

1129
00:42:23,292 --> 00:42:23,292
already received a target here, can start

1130
00:42:23,292 --> 00:42:23,292
updating the weights and when it's done,

1131
00:42:23,292 --> 00:42:43,312
it's free to operate on the next epoch.

1132
00:42:43,312 --> 00:42:46,315
So there is no locking anymore in this

1133
00:42:46,315 --> 00:42:47,315
framework.

1134
00:42:48,317 --> 00:42:50,319
Okay, so I'm pretty much done.

1135
00:42:50,319 --> 00:42:52,321
I'm also out of time, I think.

1136
00:42:52,321 --> 00:42:54,323
So how does this perform?

1137
00:42:54,323 --> 00:42:55,324
Of course we changed the learning

1138
00:42:55,324 --> 00:42:56,325
algorithm.

1139
00:42:56,325 --> 00:42:59,328
Now we have to also go back and see if

1140
00:42:59,328 --> 00:42:59,328
this is still giving us the same

1141
00:42:59,328 --> 00:43:03,326
performance.

1142
00:43:05,328 --> 00:43:09,332
As I said, this is the first results we

1143
00:43:09,332 --> 00:43:09,332
have here now and at least to mid scale

1144
00:43:09,332 --> 00:43:09,332
data sets like cipher ten or so, this

1145
00:43:09,332 --> 00:43:20,343
seems to actually perform very well.

1146
00:43:20,343 --> 00:43:22,345
So we attract it for standard

1147
00:43:22,345 --> 00:43:22,345
architectures, fashion MNIST with ResNet

1148
00:43:22,345 --> 00:43:26,349
15 and ResNet 18.

1149
00:43:27,349 --> 00:43:30,353
We worked mostly so far for small data

1150
00:43:30,353 --> 00:43:30,353
sets like Fashion MNIST with applying

1151
00:43:30,353 --> 00:43:35,358
free splits to ResNet 50.

1152
00:43:35,358 --> 00:43:38,361
We get basically the same performance as

1153
00:43:38,361 --> 00:43:39,362
standard backprop.

1154
00:43:39,362 --> 00:43:42,365
As networks get deeper, you see that our

1155
00:43:42,365 --> 00:43:42,365
problem that we have now is actually

1156
00:43:42,365 --> 00:43:42,365
overfitting because we have these local

1157
00:43:42,365 --> 00:43:48,371
targets.

1158
00:43:48,371 --> 00:43:50,373
It seems that these smaller blocks

1159
00:43:50,373 --> 00:43:52,374
actually overfit to some extent.

1160
00:43:52,375 --> 00:43:54,377
This is not so severe for still up to

1161
00:43:54,377 --> 00:43:56,379
tasks like cipher ten.

1162
00:43:56,379 --> 00:43:58,381
So we get quite close already.

1163
00:43:58,381 --> 00:44:01,378
But if you go now for really large tasks,

1164
00:44:01,378 --> 00:44:01,378
there's still something missing for like

1165
00:44:01,378 --> 00:44:06,383
single splits.

1166
00:44:06,383 --> 00:44:08,385
We are getting there, but we are not

1167
00:44:08,385 --> 00:44:08,385
reaching all the way up to back

1168
00:44:08,385 --> 00:44:11,388
propagation.

1169
00:44:11,388 --> 00:44:14,391
But it's still interesting to see that

1170
00:44:14,391 --> 00:44:14,391
you can apply this principle also to

1171
00:44:14,391 --> 00:44:14,391
these standard machine learning

1172
00:44:14,391 --> 00:44:20,397
algorithms.

1173
00:44:21,398 --> 00:44:24,401
Okay, this is my second summary.

1174
00:44:24,401 --> 00:44:26,403
So actually we found that deep neural

1175
00:44:26,403 --> 00:44:26,403
networks are surprisingly good in

1176
00:44:26,403 --> 00:44:31,408
generalizing over probability spaces.

1177
00:44:31,408 --> 00:44:33,410
This is how actually this work started.

1178
00:44:34,411 --> 00:44:37,414
And our idea was to exploit this and to

1179
00:44:37,414 --> 00:44:37,414
utilize it to distribute learning in the

1180
00:44:37,414 --> 00:44:37,414
same fashion as in the first project I

1181
00:44:37,414 --> 00:44:37,414
showed you and to solve this credit

1182
00:44:37,414 --> 00:44:37,414
assignment problem by generating these

1183
00:44:37,414 --> 00:44:51,428
feedback networks.

1184
00:44:53,430 --> 00:44:55,432
Yeah, that's basically it.

1185
00:44:55,432 --> 00:44:58,435
And then I want to acknowledge my

1186
00:44:58,435 --> 00:45:00,431
coworkers and my students.

1187
00:45:00,431 --> 00:45:03,434
So I have two very good PhD students,

1188
00:45:03,434 --> 00:45:03,434
carlisle and Cabrill, who is now here in

1189
00:45:03,434 --> 00:45:10,441
Bohom and works on this topic.

1190
00:45:10,441 --> 00:45:13,444
And the first project I showed was work I

1191
00:45:13,444 --> 00:45:13,444
did together when I was in gettingham

1192
00:45:13,444 --> 00:45:17,448
with Christian Tetlav.

1193
00:45:17,448 --> 00:45:21,452
And the second project is I worked

1194
00:45:21,452 --> 00:45:21,452
closely with Christian Meyer and Anand

1195
00:45:21,452 --> 00:45:28,459
Sutomoni and yeah, thank you.

1196
00:45:31,462 --> 00:45:32,463
Sarah: Thank you very much, David.

1197
00:45:32,463 --> 00:45:34,465
That was absolutely fascinating.

1198
00:45:34,465 --> 00:45:36,467
I think it's incredible how closely the

1199
00:45:36,467 --> 00:45:38,469
sort of model matched biology.

1200
00:45:38,469 --> 00:45:40,471
Like considering you derived it from

1201
00:45:40,471 --> 00:45:40,471
first principles, I think that was really

1202
00:45:40,471 --> 00:45:42,473
cool.

1203
00:45:43,474 --> 00:45:45,476
I did have a question just about the last

1204
00:45:45,476 --> 00:45:46,477
sort of the bit.

1205
00:45:46,477 --> 00:45:48,479
You spoke about the Convolutional network

1206
00:45:48,479 --> 00:45:48,479
and you said traditionally it has to go

1207
00:45:48,479 --> 00:45:48,479
all the way end to end, which is really

1208
00:45:48,479 --> 00:45:53,484
inefficient.

1209
00:45:53,484 --> 00:45:55,485
And then you showed the results that you

1210
00:45:55,485 --> 00:45:55,486
got.

1211
00:45:55,486 --> 00:45:58,489
Did you look at energy consumption with

1212
00:45:58,489 --> 00:45:59,490
yours as well?

1213
00:45:59,490 --> 00:46:00,485
David: Not yet.

1214
00:46:00,485 --> 00:46:03,488
So we are actually currently working on

1215
00:46:03,488 --> 00:46:03,488
it's actually not so easy to implement

1216
00:46:03,488 --> 00:46:03,488
these things in standard machine learning

1217
00:46:03,488 --> 00:46:12,497
toolboxes.

1218
00:46:12,497 --> 00:46:15,500
So Carlisle is currently looking into

1219
00:46:15,500 --> 00:46:15,500
this, the PhD student in Jester, he has

1220
00:46:15,500 --> 00:46:15,500
an implementation now, and he's now

1221
00:46:15,500 --> 00:46:15,500
evaluating how well we can make use of

1222
00:46:15,500 --> 00:46:27,512
this parallelization in practice.

1223
00:46:27,512 --> 00:46:32,517
But we are actually quite confident that

1224
00:46:32,517 --> 00:46:32,517
for parallelizing it, it should be there.

1225
00:46:32,517 --> 00:46:37,522
.

1226
00:46:37,522 --> 00:46:39,524
The question is how much you save in

1227
00:46:39,524 --> 00:46:40,525
terms of energy.

1228
00:46:40,525 --> 00:46:43,528
Because for these smaller scale models

1229
00:46:43,528 --> 00:46:43,528
that we use now, ResNet 18, ResNet 50,

1230
00:46:43,528 --> 00:46:48,533
the effect might be not that huge.

1231
00:46:49,534 --> 00:46:51,536
So once we ramp this up to really larger

1232
00:46:51,536 --> 00:46:53,538
models, the effect should be bigger.

1233
00:46:53,538 --> 00:46:55,540
But yeah, this is ongoing work.

1234
00:46:56,541 --> 00:46:57,542
Sarah: Very cool.

1235
00:46:57,542 --> 00:46:58,543
Thank you.

1236
00:46:58,543 --> 00:47:00,539
And then I was just wondering as well,

1237
00:47:00,539 --> 00:47:00,539
this local error back propagation, is

1238
00:47:00,539 --> 00:47:00,539
that something that other people have

1239
00:47:00,539 --> 00:47:00,539
tried with these convolutional neural

1240
00:47:00,539 --> 00:47:00,539
networks or is this quite a new way of

1241
00:47:00,539 --> 00:47:10,549
implementing it?

1242
00:47:10,549 --> 00:47:12,551
David: There is a bunch of approaches

1243
00:47:12,551 --> 00:47:13,552
that do this.

1244
00:47:14,553 --> 00:47:16,555
For example, I mean, the closest I guess

1245
00:47:16,555 --> 00:47:16,555
is target propagation which has been

1246
00:47:16,555 --> 00:47:16,555
proposed, which essentially uses random

1247
00:47:16,555 --> 00:47:23,562
feedback weights to back propagate here.

1248
00:47:24,562 --> 00:47:26,565
So these guys would not be trained.

1249
00:47:28,567 --> 00:47:34,573
And this works nicely also for small

1250
00:47:34,573 --> 00:47:37,576
scale problems.

1251
00:47:37,576 --> 00:47:38,577
But as far as I know, they don't perform

1252
00:47:38,577 --> 00:47:39,578
that well.

1253
00:47:39,578 --> 00:47:41,580
Even for cipher ten, it already starts

1254
00:47:41,580 --> 00:47:41,580
breaking down because these random

1255
00:47:41,580 --> 00:47:41,580
feedback weights are just too coarse an

1256
00:47:41,580 --> 00:47:47,586
approximation, I think.

1257
00:47:47,586 --> 00:47:51,590
And this is the first okay, maybe I have

1258
00:47:51,590 --> 00:47:52,591
to be careful.

1259
00:47:52,591 --> 00:47:55,594
I think this is the first method that

1260
00:47:55,594 --> 00:47:55,594
allows you to train these feedback

1261
00:47:55,594 --> 00:47:58,597
weights.

1262
00:47:58,597 --> 00:48:00,593
That is not a contrastive method.

1263
00:48:01,594 --> 00:48:03,596
There is a bunch of methods that use a

1264
00:48:03,596 --> 00:48:04,597
contrastive step.

1265
00:48:06,599 --> 00:48:09,602
So you have maybe seen this forward

1266
00:48:09,602 --> 00:48:09,602
forward algorithm and all these things,

1267
00:48:09,602 --> 00:48:09,602
but what they have to do always is they

1268
00:48:09,602 --> 00:48:09,602
send in the actual input data and then

1269
00:48:09,602 --> 00:48:09,602
they send an kind of anti input, an anti

1270
00:48:09,602 --> 00:48:09,602
input that is usually generated

1271
00:48:09,602 --> 00:48:28,621
artificially.

1272
00:48:28,621 --> 00:48:31,624
So they do some distortion to the input

1273
00:48:31,624 --> 00:48:31,624
to make it and then they have to use both

1274
00:48:31,624 --> 00:48:37,630
information locally to do the update.

1275
00:48:37,630 --> 00:48:40,633
So the network has to keep in memory the

1276
00:48:40,633 --> 00:48:40,633
input and the anti input and the

1277
00:48:40,633 --> 00:48:43,636
responses.

1278
00:48:43,636 --> 00:48:47,640
And this makes these approaches a bit

1279
00:48:47,640 --> 00:48:50,643
harder to parallelize in this.

1280
00:48:50,643 --> 00:48:53,646
And the nice thing here is that we derive

1281
00:48:53,646 --> 00:48:53,646
an upper bound to this variational free

1282
00:48:53,646 --> 00:48:53,646
energy loss that can be spelled out

1283
00:48:53,646 --> 00:49:03,650
completely by forward propagation.

1284
00:49:03,650 --> 00:49:04,651
And that I think is new.

1285
00:49:04,651 --> 00:49:07,654
So that is the new bit of this.

1286
00:49:08,655 --> 00:49:09,656
Sarah: Very cool.

1287
00:49:12,659 --> 00:49:14,661
Daniel: That's awesome.

1288
00:49:16,663 --> 00:49:19,666
Well, yeah, few comments, one piece that

1289
00:49:19,666 --> 00:49:22,669
kind of between the two of your talks.

1290
00:49:22,669 --> 00:49:25,672
That was at least a new distinction to me,

1291
00:49:25,672 --> 00:49:25,672
, which was the difference between the

1292
00:49:25,672 --> 00:49:25,672
Neuromorphic hardware and the

1293
00:49:25,672 --> 00:49:29,676
Neuromorphic algorithms.

1294
00:49:30,677 --> 00:49:34,681
So it's not just about new hardware or

1295
00:49:34,681 --> 00:49:34,681
Wetware, though that would be great to

1296
00:49:34,681 --> 00:49:39,686
see.

1297
00:49:39,686 --> 00:49:42,689
It's almost like there's this

1298
00:49:42,689 --> 00:49:42,689
intermediate or a bridge step with using

1299
00:49:42,689 --> 00:49:42,689
the algorithms on the hardware we have

1300
00:49:42,689 --> 00:49:42,689
today that like Sarah mentioned, the

1301
00:49:42,689 --> 00:49:42,689
Spiking neural networks which are

1302
00:49:42,689 --> 00:49:42,689
amenable to GPUs or just using

1303
00:49:42,689 --> 00:49:42,689
standardized CPU multicore scheduling

1304
00:49:42,689 --> 00:50:04,705
approaches.

1305
00:50:05,706 --> 00:50:09,710
You can already do more with what we have

1306
00:50:09,710 --> 00:50:13,714
using the Neuromorphic algorithms.

1307
00:50:13,714 --> 00:50:16,717
So it's not just a material science topic,

1308
00:50:16,717 --> 00:50:16,717
, but also there's a lot at the really

1309
00:50:16,717 --> 00:50:16,717
micro scale that we can learn related to

1310
00:50:16,717 --> 00:50:16,717
noise processing scheduling and then also

1311
00:50:16,717 --> 00:50:16,717
even at higher levels of abstraction,

1312
00:50:16,717 --> 00:50:16,717
probably learning from biomimicry and

1313
00:50:16,717 --> 00:50:33,734
cognitive systems more generally.

1314
00:50:33,734 --> 00:50:36,737
But that was a distinction for me.

1315
00:50:38,739 --> 00:50:38,739
David: Yeah.

1316
00:50:38,739 --> 00:50:40,741
So maybe to add yeah.

1317
00:50:40,741 --> 00:50:43,744
So I think that the problem becomes

1318
00:50:43,744 --> 00:50:43,744
really now more pressing as these

1319
00:50:43,744 --> 00:50:43,744
Neuromorphic devices become so the

1320
00:50:43,744 --> 00:50:43,744
hardware devices become more mature and

1321
00:50:43,744 --> 00:50:43,744
they usually cannot really shine on these

1322
00:50:43,744 --> 00:50:43,744
standard machine learning algorithms

1323
00:50:43,744 --> 00:50:43,744
because they are really optimized for

1324
00:50:43,744 --> 00:51:01,756
GPUs.

1325
00:51:02,757 --> 00:51:04,759
So you need to think a little bit.

1326
00:51:04,759 --> 00:51:06,761
So you have to take one step back and

1327
00:51:06,761 --> 00:51:06,761
think again about the algorithmic side to

1328
00:51:06,761 --> 00:51:12,767
really use them to full capacity.

1329
00:51:12,767 --> 00:51:14,769
And as you have seen now, we collaborate

1330
00:51:14,769 --> 00:51:14,769
with Professor Meyer, who is doing this

1331
00:51:14,769 --> 00:51:14,769
spinnaker chip in Dresden, but there's

1332
00:51:14,769 --> 00:51:14,769
also other approaches like the Louis chip

1333
00:51:14,769 --> 00:51:14,769
that Sarah mentioned from intel and so on,

1334
00:51:14,769 --> 00:51:27,782
, and they are really looking into this.

1335
00:51:27,782 --> 00:51:31,786
Now, also from the algorithmic side.

1336
00:51:32,787 --> 00:51:34,789
Sarah: I find it really useful.

1337
00:51:34,789 --> 00:51:35,790
It's kind of like a mindset or a mental

1338
00:51:35,790 --> 00:51:35,790
framework when I'm thinking about

1339
00:51:35,790 --> 00:51:35,790
computers or AI, and this is probably

1340
00:51:35,790 --> 00:51:35,790
because I'm a neuroscientist, but I also

1341
00:51:35,790 --> 00:51:35,790
have to translate it to, well, how does

1342
00:51:35,790 --> 00:51:35,790
the brain work, how does the information

1343
00:51:35,790 --> 00:51:35,790
processing work, et cetera, in the

1344
00:51:35,790 --> 00:51:45,800
brain?

1345
00:51:46,801 --> 00:51:47,802
And when I came to sort of computer

1346
00:51:47,802 --> 00:51:47,802
science and AI after neuroscience, I

1347
00:51:47,802 --> 00:51:51,805
found myself naturally translating it.

1348
00:51:51,805 --> 00:51:52,807
But I feel like the framework is just a

1349
00:51:52,807 --> 00:51:52,807
really useful way of understanding

1350
00:51:52,807 --> 00:51:52,807
computation at the end of the day because

1351
00:51:52,807 --> 00:51:52,807
our brains, as I said, are just these

1352
00:51:52,807 --> 00:51:59,814
massive supercomputers.

1353
00:51:59,814 --> 00:52:00,809
And I'm constantly reading papers on

1354
00:52:00,809 --> 00:52:02,811
computer science or whatever.

1355
00:52:02,811 --> 00:52:05,814
And then once you can conceptualize

1356
00:52:05,814 --> 00:52:05,814
anything really as well, how neuromorphic

1357
00:52:05,814 --> 00:52:09,818
is this?

1358
00:52:09,818 --> 00:52:10,819
And then if you start thinking about,

1359
00:52:10,819 --> 00:52:10,819
well, how could you tweak it so it's

1360
00:52:10,819 --> 00:52:13,822
slightly more neuromorphic?

1361
00:52:13,822 --> 00:52:14,823
And is that then going to give you these

1362
00:52:14,823 --> 00:52:16,825
gains that we get with the brain?

1363
00:52:16,825 --> 00:52:17,826
Is it going to give you some extra

1364
00:52:17,826 --> 00:52:17,826
parallel computation or is it going to

1365
00:52:17,826 --> 00:52:21,830
give you some energy efficiency?

1366
00:52:21,830 --> 00:52:23,832
So, yeah, I find it the definitions when

1367
00:52:23,832 --> 00:52:23,832
I was looking at the definition is I

1368
00:52:23,832 --> 00:52:23,832
think it really depends who writes the

1369
00:52:23,832 --> 00:52:27,836
definition.

1370
00:52:27,836 --> 00:52:29,838
And because it's such a dynamic area at

1371
00:52:29,838 --> 00:52:29,838
the moment as well, I think it has been

1372
00:52:29,838 --> 00:52:29,838
changing and it will be changing, but for

1373
00:52:29,838 --> 00:52:29,838
me, I feel like neuromorphic is more of

1374
00:52:29,838 --> 00:52:29,838
like a mental framework where I look at

1375
00:52:29,838 --> 00:52:29,838
things through conceptualize, through

1376
00:52:29,838 --> 00:52:43,852
yeah, I think it's.

1377
00:52:44,852 --> 00:52:45,854
David: It'S also not very well defined.

1378
00:52:47,856 --> 00:52:50,858
You also mentioned that actually

1379
00:52:50,858 --> 00:52:50,858
artificial neural networks are a

1380
00:52:50,858 --> 00:52:50,858
neuromorphic concept, if you want, and

1381
00:52:50,858 --> 00:52:57,865
they were from the first day.

1382
00:52:57,865 --> 00:53:00,863
And it's a big success story, right, if

1383
00:53:00,863 --> 00:53:00,863
you look into the 90s or so when these

1384
00:53:00,863 --> 00:53:00,863
support vector machines and these

1385
00:53:00,863 --> 00:53:00,863
alternative models came up, but none of

1386
00:53:00,863 --> 00:53:00,863
them have outlived the neuromorphic

1387
00:53:00,863 --> 00:53:12,875
approaches.

1388
00:53:12,875 --> 00:53:14,877
So it's actually very nice.

1389
00:53:14,877 --> 00:53:16,879
But still there is this community that

1390
00:53:16,879 --> 00:53:16,879
thinks that there is more features from

1391
00:53:16,879 --> 00:53:16,879
the brain that you need to put in to get

1392
00:53:16,879 --> 00:53:23,886
to the real thing.

1393
00:53:24,887 --> 00:53:28,890
So I think this is a bit of a it's not a

1394
00:53:28,890 --> 00:53:28,890
very well defined term, actually, and I

1395
00:53:28,890 --> 00:53:32,895
think with.

1396
00:53:32,895 --> 00:53:33,896
Sarah: Your research, yours is almost

1397
00:53:33,896 --> 00:53:33,896
like the smallest level that I've seen

1398
00:53:33,896 --> 00:53:36,899
people look at it on.

1399
00:53:36,899 --> 00:53:38,901
I don't know if you've seen anything else,

1400
00:53:38,901 --> 00:53:38,901
, but we're not just talking about a cell

1401
00:53:38,901 --> 00:53:38,901
level of free energy and active inference.

1402
00:53:38,901 --> 00:53:41,904
.

1403
00:53:41,904 --> 00:53:44,907
We're actually talking about a cell

1404
00:53:44,907 --> 00:53:45,908
structure.

1405
00:53:46,909 --> 00:53:47,910
So then you think, well, how small does

1406
00:53:47,910 --> 00:53:48,911
it go?

1407
00:53:48,911 --> 00:53:50,912
Are we going to talk about cell

1408
00:53:50,912 --> 00:53:50,912
subcellular structures eventually, like

1409
00:53:50,912 --> 00:53:50,912
Mitochondria using free energy in a

1410
00:53:50,912 --> 00:53:50,912
similar way with compartments, I guess

1411
00:53:50,912 --> 00:53:50,912
actually it'd be interesting to get your

1412
00:53:50,912 --> 00:53:50,912
thoughts, david on, you talked at the

1413
00:53:50,912 --> 00:53:50,912
start about how synapses are

1414
00:53:50,912 --> 00:54:03,920
compartmentalized.

1415
00:54:03,920 --> 00:54:05,922
Do you see different sort of

1416
00:54:05,922 --> 00:54:05,922
instantiations of this in different

1417
00:54:05,922 --> 00:54:05,922
compartments just within one synapse,

1418
00:54:05,922 --> 00:54:05,922
almost like are you wanting to sort of

1419
00:54:05,922 --> 00:54:05,922
look at that granular level or is it more

1420
00:54:05,922 --> 00:54:05,922
now taking what you've learned from this

1421
00:54:05,922 --> 00:54:05,922
and putting it back into how can we sort

1422
00:54:05,922 --> 00:54:19,936
of make the AI more efficient?

1423
00:54:20,937 --> 00:54:22,939
David: Yeah, we are going much more in

1424
00:54:22,939 --> 00:54:22,939
this direction now that we see how we can

1425
00:54:22,939 --> 00:54:27,944
build this back into AI models.

1426
00:54:31,947 --> 00:54:34,951
One has to be a bit careful when using

1427
00:54:34,951 --> 00:54:34,951
the free energy principle because it's

1428
00:54:34,951 --> 00:54:34,951
such a powerful general framework that

1429
00:54:34,951 --> 00:54:34,951
you can apply it to basically anything

1430
00:54:34,951 --> 00:54:34,951
and it's not necessarily you will come up

1431
00:54:34,951 --> 00:54:50,967
with a useful result in the end.

1432
00:54:50,967 --> 00:54:55,972
Just putting this this started actually

1433
00:54:55,972 --> 00:54:57,974
as a side project.

1434
00:54:57,974 --> 00:55:01,972
This was my kind of COVID pandemic

1435
00:55:01,972 --> 00:55:03,974
lockdown project.

1436
00:55:04,975 --> 00:55:06,977
And I was just curious about this and

1437
00:55:06,977 --> 00:55:06,977
whether you can actually solve this,

1438
00:55:06,977 --> 00:55:06,977
because I thought the synopsis is maybe

1439
00:55:06,977 --> 00:55:12,983
simple enough.

1440
00:55:12,983 --> 00:55:14,985
Because when you go into the papers, they

1441
00:55:14,985 --> 00:55:14,985
have at some point to go into some

1442
00:55:14,985 --> 00:55:18,989
approximations.

1443
00:55:18,989 --> 00:55:20,991
They do some mean field, usually, so they

1444
00:55:20,991 --> 00:55:20,991
go for first modes, and then you can

1445
00:55:20,991 --> 00:55:20,991
solve these guys for more complex, even

1446
00:55:20,991 --> 00:55:20,991
for neurons, it's hard, actually, if you

1447
00:55:20,991 --> 00:55:20,991
go to the neuron or network level, it's

1448
00:55:20,991 --> 00:55:30,001
hard.

1449
00:55:30,001 --> 00:55:33,004
It's very involved math, but for a sinus

1450
00:55:33,004 --> 00:55:36,007
is simple enough, actually.

1451
00:55:36,007 --> 00:55:38,009
So you can actually do this and spell

1452
00:55:38,009 --> 00:55:38,009
everything out if you make the right

1453
00:55:38,009 --> 00:55:38,009
assumptions and really just derive these

1454
00:55:38,009 --> 00:55:43,014
things.

1455
00:55:43,014 --> 00:55:48,019
And that was kind of just kind of a game

1456
00:55:48,019 --> 00:55:48,019
I went into and then it turned out to

1457
00:55:48,019 --> 00:55:56,027
work quite nicely, I think, in the end.

1458
00:55:56,027 --> 00:55:57,028
Sarah: Yeah.

1459
00:55:59,030 --> 00:56:01,026
David: I'm not sure if you like

1460
00:56:01,026 --> 00:56:01,026
Mitochondria or so I'm sure you could

1461
00:56:01,026 --> 00:56:01,026
apply the same principles but I'm not

1462
00:56:01,026 --> 00:56:01,026
sure if the results you get would be any

1463
00:56:01,026 --> 00:56:12,037
meaningful or would help you in any way.

1464
00:56:13,038 --> 00:56:15,040
That's always the risk.

1465
00:56:15,040 --> 00:56:17,042
You invest so much time and then in the

1466
00:56:17,042 --> 00:56:17,042
end you get some results and you don't

1467
00:56:17,042 --> 00:56:19,044
know.

1468
00:56:21,046 --> 00:56:23,048
Daniel: That was also something that I

1469
00:56:23,048 --> 00:56:23,048
thought was quite interesting, which was

1470
00:56:23,048 --> 00:56:28,052
the synapse was the agent.

1471
00:56:28,053 --> 00:56:30,055
It's really easy to think, oh, we'll make

1472
00:56:30,055 --> 00:56:33,058
an agent based model of a neural system.

1473
00:56:33,058 --> 00:56:35,060
First off, that tends to not include Glia

1474
00:56:35,060 --> 00:56:35,060
or non neural cell types, but it's almost

1475
00:56:35,060 --> 00:56:35,060
like a doubly unquestioned assumption

1476
00:56:35,060 --> 00:56:42,067
that the cell would be the agent.

1477
00:56:44,069 --> 00:56:47,072
But then it was a great transition from

1478
00:56:47,072 --> 00:56:47,072
the person throwing the ball over the

1479
00:56:47,072 --> 00:56:51,075
wall.

1480
00:56:51,076 --> 00:56:54,079
That's an action centric approach where

1481
00:56:54,079 --> 00:56:54,079
you only have partial visibility of the

1482
00:56:54,079 --> 00:56:54,079
consequences and then that is the exact

1483
00:56:54,079 --> 00:56:54,079
scenario that the synapse finds itself in

1484
00:56:54,079 --> 00:57:05,084
in a different way.

1485
00:57:05,084 --> 00:57:08,087
Or it could have been set up so that a

1486
00:57:08,087 --> 00:57:10,089
neuron is the agent.

1487
00:57:10,089 --> 00:57:12,091
We're building maps, not territories.

1488
00:57:13,092 --> 00:57:15,094
And so then, just like you said, free

1489
00:57:15,094 --> 00:57:15,094
energy principle, it's a principle for

1490
00:57:15,094 --> 00:57:18,097
everything.

1491
00:57:20,099 --> 00:57:24,103
And so just making principled statements

1492
00:57:24,103 --> 00:57:27,106
about things is table stakes.

1493
00:57:28,107 --> 00:57:30,109
And then I guess my question for you is

1494
00:57:30,109 --> 00:57:30,109
then what does make it useful or in your

1495
00:57:30,109 --> 00:57:30,109
learning and tinkering around with these

1496
00:57:30,109 --> 00:57:36,115
models?

1497
00:57:36,115 --> 00:57:38,117
What differentiated situations where you

1498
00:57:38,117 --> 00:57:38,117
applied free energy principle or active

1499
00:57:38,117 --> 00:57:38,117
inference and you felt like it was

1500
00:57:38,117 --> 00:57:38,117
providing a contribution to your research

1501
00:57:38,117 --> 00:57:38,117
direction versus where you played around

1502
00:57:38,117 --> 00:57:38,117
and it was like, well, that was

1503
00:57:38,117 --> 00:57:50,129
tautological.

1504
00:57:51,130 --> 00:57:53,132
David: I think the free energy principle

1505
00:57:53,132 --> 00:57:53,132
makes sense in a context where you have

1506
00:57:53,132 --> 00:57:57,136
incomplete information.

1507
00:58:01,134 --> 00:58:03,136
So for example, in the synapse case,

1508
00:58:03,136 --> 00:58:03,136
right, the question we started with was

1509
00:58:03,136 --> 00:58:03,136
this problem that the synapse has to

1510
00:58:03,136 --> 00:58:03,136
solve, that it has incomplete information

1511
00:58:03,136 --> 00:58:03,136
about the state in the cell body because

1512
00:58:03,136 --> 00:58:03,136
it only sees this kind of or that's the

1513
00:58:03,136 --> 00:58:18,151
assumption, at least of the model.

1514
00:58:18,151 --> 00:58:20,152
And also what we get from the

1515
00:58:20,152 --> 00:58:20,152
experimentalists, that it essentially

1516
00:58:20,152 --> 00:58:20,152
only sees this back propagating action

1517
00:58:20,152 --> 00:58:24,157
potential.

1518
00:58:24,157 --> 00:58:29,162
So it sees a single binary variable about

1519
00:58:29,162 --> 00:58:31,164
the state of the soma.

1520
00:58:32,165 --> 00:58:36,169
So essentially this is a problem of

1521
00:58:36,169 --> 00:58:36,169
incomplete information and also the

1522
00:58:36,169 --> 00:58:36,169
second ingredient that you need some form

1523
00:58:36,169 --> 00:58:45,178
of agency.

1524
00:58:45,178 --> 00:58:48,180
I think if you apply the free energy

1525
00:58:48,180 --> 00:58:48,180
principle to a system without agency, so

1526
00:58:48,180 --> 00:58:48,180
if something is not interacting with an

1527
00:58:48,180 --> 00:58:48,180
environment in a closed loop, then it

1528
00:58:48,180 --> 00:58:57,190
becomes really sketchy.

1529
00:58:57,190 --> 00:59:00,187
And I think already this model is on the

1530
00:59:00,187 --> 00:59:00,187
edge when it comes because these models

1531
00:59:00,187 --> 00:59:00,187
don't really have an agency, but they at

1532
00:59:00,187 --> 00:59:07,194
least produce an output, right?

1533
00:59:07,194 --> 00:59:10,197
So you can still think of this as an

1534
00:59:10,197 --> 00:59:12,199
interaction with an environment.

1535
00:59:12,199 --> 00:59:15,202
But as soon as you lose that, I think

1536
00:59:15,202 --> 00:59:15,202
then there would be simpler models that

1537
00:59:15,202 --> 00:59:19,206
can just give you the same.

1538
00:59:26,212 --> 00:59:29,216
Actually, in the synapse case, the agency

1539
00:59:29,216 --> 00:59:29,216
is only this adding noise actually in the

1540
00:59:29,216 --> 00:59:29,216
model because the synapse is triggered

1541
00:59:29,216 --> 00:59:29,216
presynaptically and then it uses its

1542
00:59:29,216 --> 00:59:29,216
internal state to add the right amount of

1543
00:59:29,216 --> 00:59:29,216
noise, which is probably already the

1544
00:59:29,216 --> 00:59:46,233
minimal agency you could imagine.

1545
00:59:49,236 --> 00:59:51,238
Daniel: Sir, you want to ask a question

1546
00:59:51,238 --> 00:59:52,239
or I can ask a question?

1547
00:59:53,240 --> 00:59:55,242
Sarah: Yeah, it was more just a comment.

1548
00:59:55,242 --> 00:59:57,244
I think it's quite interesting people

1549
00:59:57,244 --> 00:59:58,245
talk about biology.

1550
00:59:59,246 --> 01:00:00,843
Some people say it's not a real science

1551
01:00:00,843 --> 01:00:02,392
because it's all messy and noisy.

1552
01:00:02,440 --> 01:00:04,575
But I think it works really interesting

1553
01:00:04,575 --> 01:00:04,575
because it's like you say, the synaptic

1554
01:00:04,575 --> 01:00:04,575
noise is actually a reporting of

1555
01:00:04,575 --> 01:00:09,028
uncertainty.

1556
01:00:09,041 --> 01:00:11,203
So in that essence, it's actually

1557
01:00:11,203 --> 01:00:11,203
probably quite accurately reporting the

1558
01:00:11,203 --> 01:00:11,203
messy world rather than the biology

1559
01:00:11,203 --> 01:00:16,719
itself just being all messy.

1560
01:00:16,722 --> 01:00:18,916
But that's just what I was thinking about.

1561
01:00:18,916 --> 01:00:18,916
.

1562
01:00:18,969 --> 01:00:20,136
Yeah, I think I was curious as well.

1563
01:00:20,138 --> 01:00:21,273
Like you said, this was like your

1564
01:00:21,273 --> 01:00:21,273
Lockdown project, but I'm just interested

1565
01:00:21,273 --> 01:00:21,273
in how you sort of came to use the free

1566
01:00:21,273 --> 01:00:21,273
energy principle, how you came across it.

1567
01:00:21,273 --> 01:00:26,778
.

1568
01:00:26,780 --> 01:00:28,960
Was it something you were quite familiar

1569
01:00:28,960 --> 01:00:28,960
with already or some of your network or

1570
01:00:28,960 --> 01:00:31,272
peers were talking about it?

1571
01:00:31,276 --> 01:00:33,446
Or did you just stumble across it in a.

1572
01:00:36,715 --> 01:00:38,938
David: Was during my PhD, I was

1573
01:00:38,938 --> 01:00:38,938
interested in variational methods and

1574
01:00:38,938 --> 01:00:42,378
probabilistic methods.

1575
01:00:43,476 --> 01:00:45,680
And then I started reading about this.

1576
01:00:45,684 --> 01:00:49,997
And so I read a bunch of Karl Friston's

1577
01:00:49,997 --> 01:00:51,294
papers and I found this interesting.

1578
01:00:52,303 --> 01:00:56,740
And actually, my PhD supervisor always

1579
01:00:56,740 --> 01:00:56,740
encouraged me not to go in that direction.

1580
01:00:56,740 --> 01:01:01,640
.

1581
01:01:02,754 --> 01:01:04,978
And then after I finished my PhD and

1582
01:01:04,978 --> 01:01:04,978
thought, okay, now I can do what I want,

1583
01:01:04,978 --> 01:01:08,319
I try it out.

1584
01:01:10,588 --> 01:01:13,817
Sarah: And then, I guess, do you think it

1585
01:01:13,817 --> 01:01:13,817
would be worthwhile, like, next steps for

1586
01:01:13,817 --> 01:01:13,817
you or for the field actually trying to

1587
01:01:13,817 --> 01:01:13,817
implement this on maybe some of the more

1588
01:01:13,817 --> 01:01:13,817
analog chips that are being built in the

1589
01:01:13,817 --> 01:01:22,777
space?

1590
01:01:22,781 --> 01:01:25,017
Like the analog neuromorphic chips, which

1591
01:01:25,017 --> 01:01:25,017
I know you can have some dynamic synapses

1592
01:01:25,017 --> 01:01:25,017
and things like do you think it would be

1593
01:01:25,017 --> 01:01:25,017
worthwhile trying to implement it on

1594
01:01:25,017 --> 01:01:32,769
hardware?

1595
01:01:32,777 --> 01:01:35,040
Or what are your thoughts on that?

1596
01:01:35,077 --> 01:01:37,258
David: I mean, the triplet rule that

1597
01:01:37,258 --> 01:01:37,258
comes out from this first work I showed,

1598
01:01:37,258 --> 01:01:37,258
I think that would be interesting to

1599
01:01:37,258 --> 01:01:42,719
implement it.

1600
01:01:44,934 --> 01:01:47,024
The nice feature is that it should, in

1601
01:01:47,024 --> 01:01:47,024
principle, should have this self

1602
01:01:47,024 --> 01:01:47,024
stabilizing feature because it's really

1603
01:01:47,024 --> 01:01:47,024
mimicking the dynamics of the membrane,

1604
01:01:47,024 --> 01:01:58,133
of the cell membrane.

1605
01:01:58,134 --> 01:02:02,112
So if the neuromorphic hardware would so

1606
01:02:02,112 --> 01:02:02,112
if the model in the synapse and the

1607
01:02:02,112 --> 01:02:02,112
neuron model match up very well, the

1608
01:02:02,112 --> 01:02:02,112
model should give you this nice self

1609
01:02:02,112 --> 01:02:02,112
stabilizing feature so that neurons

1610
01:02:02,112 --> 01:02:02,112
really not go into some epileptic states

1611
01:02:02,112 --> 01:02:20,294
or so.

1612
01:02:20,294 --> 01:02:22,311
And you get this for free from this model.

1613
01:02:22,311 --> 01:02:22,311
.

1614
01:02:22,312 --> 01:02:24,331
That's what we saw in the simulations, at

1615
01:02:24,331 --> 01:02:24,334
least.

1616
01:02:25,339 --> 01:02:27,369
But in the simulations, of course, we had

1617
01:02:27,369 --> 01:02:27,369
full control over this dynamics matching

1618
01:02:27,369 --> 01:02:32,413
up in the right way.

1619
01:02:32,414 --> 01:02:35,449
So that is probably a bit more tricky for

1620
01:02:35,449 --> 01:02:39,481
hardware, but it's probably solvable.

1621
01:02:39,482 --> 01:02:40,494
So it would be interesting.

1622
01:02:40,499 --> 01:02:43,526
What you get for it is that you have

1623
01:02:43,526 --> 01:02:43,526
these purely event based tools which only

1624
01:02:43,526 --> 01:02:49,585
use pre post spikes, which is nice.

1625
01:02:50,593 --> 01:02:51,602
Sarah: Very cool.

1626
01:02:51,603 --> 01:02:51,609
Thank you.

1627
01:02:52,610 --> 01:02:52,613
Yeah.

1628
01:02:52,614 --> 01:02:54,638
Daniel, if you had a question?

1629
01:02:55,639 --> 01:02:58,669
Daniel: Well, that's a great principle

1630
01:02:58,669 --> 01:02:58,669
there, which is like, if you can design

1631
01:02:58,669 --> 01:02:58,669
the neuromorphic algorithm so that it

1632
01:02:58,669 --> 01:02:58,669
harnesses a material feature like the

1633
01:02:58,669 --> 01:02:58,669
actual leaky permeability of a membrane

1634
01:02:58,669 --> 01:02:58,669
or actual spatial proximity, if you can

1635
01:02:58,669 --> 01:02:58,669
leverage a material feature, analog

1636
01:02:58,669 --> 01:02:58,669
feature that isn't virtualized, then it's

1637
01:02:58,669 --> 01:02:58,669
already an adjacency into future hardware.

1638
01:02:58,669 --> 01:03:22,858
.

1639
01:03:22,858 --> 01:03:23,867
So that's one great point.

1640
01:03:23,867 --> 01:03:26,897
And then to Sarah's point about almost

1641
01:03:26,897 --> 01:03:26,897
biology not being a science, which there

1642
01:03:26,897 --> 01:03:26,897
is a famous quotation, there will never

1643
01:03:26,897 --> 01:03:35,989
be a Newton for a blade of grass.

1644
01:03:36,990 --> 01:03:38,010
Because some people say, yeah, it's a

1645
01:03:38,010 --> 01:03:40,030
different biology is more like history.

1646
01:03:40,032 --> 01:03:42,051
Because whether you approach this from a

1647
01:03:42,051 --> 01:03:42,051
development or ecology or evolution

1648
01:03:42,051 --> 01:03:42,051
perspective, biology is a historical

1649
01:03:42,051 --> 01:03:46,089
science.

1650
01:03:46,090 --> 01:03:47,108
It's not like a real science.

1651
01:03:48,110 --> 01:03:50,137
And then that reminded me of the cross

1652
01:03:50,137 --> 01:03:50,137
country shirt that says, our sport is

1653
01:03:50,137 --> 01:03:54,177
your punishment.

1654
01:03:54,178 --> 01:03:59,221
So it's like, well, no, your noise is

1655
01:03:59,221 --> 01:04:01,183
biology's signal.

1656
01:04:01,185 --> 01:04:03,208
And that's how it happens.

1657
01:04:05,221 --> 01:04:08,254
My question was about this tension

1658
01:04:08,254 --> 01:04:08,254
between, I guess, neural and

1659
01:04:08,254 --> 01:04:08,254
computational ways of looking at the

1660
01:04:08,254 --> 01:04:18,354
resources associated with computation.

1661
01:04:18,356 --> 01:04:21,387
So from the von Neumann paradigm.

1662
01:04:21,389 --> 01:04:25,422
We have a lot of shared reference points,

1663
01:04:25,422 --> 01:04:25,422
CPU cycles, Ram capacity, and all these

1664
01:04:25,422 --> 01:04:25,422
kinds of and like, even in your

1665
01:04:25,422 --> 01:04:25,422
introductions, you conveyed like, well,

1666
01:04:25,422 --> 01:04:25,422
this is how many CPU cycles it's going

1667
01:04:25,422 --> 01:04:25,422
through, or this is how many parameters

1668
01:04:25,422 --> 01:04:25,422
would have to be stored, or something

1669
01:04:25,422 --> 01:04:44,617
like that.

1670
01:04:46,630 --> 01:04:49,665
However, that's referencing another

1671
01:04:49,665 --> 01:04:50,675
paradigm.

1672
01:04:50,675 --> 01:04:54,714
So what do resource descriptors or

1673
01:04:54,714 --> 01:04:54,714
capacity descriptors look like when we're

1674
01:04:54,714 --> 01:04:54,714
outside the space of okay, yeah, power

1675
01:04:54,714 --> 01:05:05,764
consumption.

1676
01:05:05,765 --> 01:05:07,784
That's something that you can put into a

1677
01:05:07,784 --> 01:05:07,784
box and just use a bomb calorimeter that'

1678
01:05:07,784 --> 01:05:11,822
's kind of like a low hanging fruit.

1679
01:05:11,824 --> 01:05:14,850
But now okay, beyond just the sheer

1680
01:05:14,850 --> 01:05:14,850
energy or caloric requirements, what can

1681
01:05:14,850 --> 01:05:14,850
we say that is, like, analogous to the

1682
01:05:14,850 --> 01:05:14,850
way that we talk about the processor or

1683
01:05:14,850 --> 01:05:26,974
the Ram or the hard drive on a computer?

1684
01:05:31,026 --> 01:05:33,043
Sarah: Yeah, I'd have to think about that

1685
01:05:33,043 --> 01:05:34,050
one some more.

1686
01:05:34,050 --> 01:05:35,068
I do think there was some interesting

1687
01:05:35,068 --> 01:05:35,068
comments in the paper on that slide I

1688
01:05:35,068 --> 01:05:35,068
showed that talks about the brain and

1689
01:05:35,068 --> 01:05:39,108
energy.

1690
01:05:39,109 --> 01:05:41,121
There was a paper I linked to.

1691
01:05:41,121 --> 01:05:42,135
I'll have to get the reference and let

1692
01:05:42,135 --> 01:05:42,135
you know what it is because QR code is

1693
01:05:42,135 --> 01:05:44,154
gone now.

1694
01:05:44,156 --> 01:05:46,178
But that had some interesting ideas, I

1695
01:05:46,178 --> 01:05:49,202
think, on what you're getting at there.

1696
01:05:49,203 --> 01:05:53,246
But I'd have to defer to the paper.

1697
01:05:55,263 --> 01:05:57,286
Daniel: How do they describe what is

1698
01:05:57,286 --> 01:05:58,297
being designed?

1699
01:05:59,301 --> 01:06:01,266
So they say it has this many of this type

1700
01:06:01,266 --> 01:06:01,266
of component, and then that might do

1701
01:06:01,266 --> 01:06:04,298
nothing though.

1702
01:06:05,302 --> 01:06:08,331
So how do they describe or evaluate these

1703
01:06:08,331 --> 01:06:10,354
different designs or algorithms?

1704
01:06:12,375 --> 01:06:13,386
Sarah: I think it's all different

1705
01:06:13,386 --> 01:06:14,395
depending on the use case.

1706
01:06:14,396 --> 01:06:15,404
That's what I've found.

1707
01:06:15,404 --> 01:06:15,406
Really?

1708
01:06:15,407 --> 01:06:17,426
Like, the language is different depending

1709
01:06:17,426 --> 01:06:17,426
on if it's written by someone maybe with

1710
01:06:17,426 --> 01:06:17,426
more of a neuroscience background or an

1711
01:06:17,426 --> 01:06:22,475
engineering background.

1712
01:06:22,476 --> 01:06:24,492
And then you kind of get used to some

1713
01:06:24,492 --> 01:06:24,492
terms that are more interchangeable than

1714
01:06:24,492 --> 01:06:26,513
others.

1715
01:06:26,514 --> 01:06:28,531
But I do think the terminology is

1716
01:06:28,531 --> 01:06:28,531
something which needs to be looked at a

1717
01:06:28,531 --> 01:06:28,531
lot more closely in this space, because

1718
01:06:28,531 --> 01:06:28,531
then I think that will help everybody

1719
01:06:28,531 --> 01:06:28,531
working in it to be on the same page a

1720
01:06:28,531 --> 01:06:37,627
little bit closer.

1721
01:06:42,678 --> 01:06:43,683
David: Cool.

1722
01:06:43,683 --> 01:06:46,716
Daniel: Well, any other thoughts or

1723
01:06:46,716 --> 01:06:47,726
questions?

1724
01:06:47,728 --> 01:06:50,755
David first and then Sarah.

1725
01:06:50,755 --> 01:06:53,785
Also, I'm very curious, what direction

1726
01:06:53,785 --> 01:06:55,801
will this series go?

1727
01:06:56,813 --> 01:06:59,844
But first, David, what are any other kind

1728
01:06:59,844 --> 01:06:59,844
of closing comments or directions you

1729
01:06:59,844 --> 01:07:03,820
want to.

1730
01:07:05,846 --> 01:07:06,852
David: Really?

1731
01:07:06,853 --> 01:07:08,877
I mean, I would say thanks for having me

1732
01:07:08,877 --> 01:07:09,881
today.

1733
01:07:09,886 --> 01:07:11,908
It was really a pleasure to discuss with

1734
01:07:11,908 --> 01:07:12,911
you.

1735
01:07:14,932 --> 01:07:14,936
Sarah: Thank you.

1736
01:07:14,937 --> 01:07:16,951
David, it was amazing to have you on.

1737
01:07:16,952 --> 01:07:17,968
I think your work is absolutely

1738
01:07:17,968 --> 01:07:17,968
fascinating, and I think it's going to

1739
01:07:17,968 --> 01:07:17,968
have lots of benefits in the future for

1740
01:07:17,968 --> 01:07:17,968
implementation, which is always nice to

1741
01:07:17,968 --> 01:07:25,040
see as well.

1742
01:07:26,049 --> 01:07:26,053
Yeah.

1743
01:07:26,053 --> 01:07:27,062
So what was the question?

1744
01:07:27,063 --> 01:07:29,081
Where do I see the series going?

1745
01:07:29,083 --> 01:07:32,115
Hopefully we can have a new guest each

1746
01:07:32,115 --> 01:07:33,121
month.

1747
01:07:33,123 --> 01:07:35,143
I think it'd be kind of cool maybe next

1748
01:07:35,143 --> 01:07:35,143
month to have someone who's building

1749
01:07:35,143 --> 01:07:35,143
hardware, so, like, maybe someone from

1750
01:07:35,143 --> 01:07:35,143
the BrainScaleS team or spinnaker team or

1751
01:07:35,143 --> 01:07:43,219
something like that would be.

1752
01:07:43,219 --> 01:07:45,245
Pretty cool, but yeah, really, I just

1753
01:07:45,245 --> 01:07:45,245
want to have a space for people who are

1754
01:07:45,245 --> 01:07:45,245
interested in this intersection to meet

1755
01:07:45,245 --> 01:07:45,245
people and see talks and reach out to

1756
01:07:45,245 --> 01:07:45,245
people who are also working in the space,

1757
01:07:45,245 --> 01:07:45,245
because it's pretty niche, but I think it'

1758
01:07:45,245 --> 01:08:01,346
's pretty important, actually.

1759
01:08:01,347 --> 01:08:03,364
Having said that, David, could you let

1760
01:08:03,364 --> 01:08:03,364
everybody know if they wanted to reach

1761
01:08:03,364 --> 01:08:03,364
out to you, what's the best way for them

1762
01:08:03,364 --> 01:08:07,407
to do that?

1763
01:08:12,452 --> 01:08:14,473
David: I'm not very active on this

1764
01:08:14,473 --> 01:08:14,473
discord channel, so maybe email is still

1765
01:08:14,473 --> 01:08:19,521
the best to reach out to me, I guess.

1766
01:08:20,530 --> 01:08:20,535
Sarah: Cool.

1767
01:08:20,536 --> 01:08:24,570
Do you want to give your oh.

1768
01:08:24,572 --> 01:08:26,599
David: I think my email should be easy

1769
01:08:26,599 --> 01:08:28,610
enough to find.

1770
01:08:28,611 --> 01:08:29,629
But you can also give the email out there.

1771
01:08:29,629 --> 01:08:29,629
.

1772
01:08:30,629 --> 01:08:30,635
Sure.

1773
01:08:31,641 --> 01:08:31,645
Cool.

1774
01:08:31,645 --> 01:08:33,669
Daniel: People can check the papers, and

1775
01:08:33,669 --> 01:08:33,669
then in the active inference institute

1776
01:08:33,669 --> 01:08:33,669
discord, there's the neuromorphic channel.

1777
01:08:33,669 --> 01:08:38,717
.

1778
01:08:41,749 --> 01:08:42,757
David: All right.

1779
01:08:42,758 --> 01:08:44,774
Daniel: Thank you, David and Sarah.

1780
01:08:44,775 --> 01:08:47,801
Really cool to see morphstream kick off

1781
01:08:47,801 --> 01:08:49,827
its developmental trajectory this way.

1782
01:08:49,828 --> 01:08:51,844
So till next time.

1783
01:08:51,844 --> 01:08:52,850
Sarah: Thank you.

1784
01:08:52,851 --> 01:08:52,853
David: Bye.


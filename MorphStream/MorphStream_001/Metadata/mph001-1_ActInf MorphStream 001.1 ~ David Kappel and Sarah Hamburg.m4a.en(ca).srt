1
00:00:03,960 --> 00:00:07,340
Hello,

2
00:00:07,410 --> 00:00:11,116
everybody. Welcome. It is September 26,

3
00:00:11,218 --> 00:00:14,572
2023. We are here kicking off

4
00:00:14,626 --> 00:00:16,728
a new stream series at the Active

5
00:00:16,744 --> 00:00:18,984
Inference Institute. This is the morph.

6
00:00:19,032 --> 00:00:23,304
Stream 1.1. Today we have David

7
00:00:23,352 --> 00:00:26,444
Cappell and also this section and

8
00:00:26,482 --> 00:00:29,856
streams facilitated by Sarah Ham. We're

9
00:00:29,888 --> 00:00:32,692
going to have a overview, first

10
00:00:32,746 --> 00:00:36,000
presented by Sarah. Then David

11
00:00:36,080 --> 00:00:38,896
will share some work on neuromorphic

12
00:00:38,928 --> 00:00:41,204
computing, and then we'll have some time

13
00:00:41,242 --> 00:00:43,572
to discuss. So thank you both for

14
00:00:43,626 --> 00:00:45,992
joining and Sarah, to you for the first

15
00:00:46,046 --> 00:00:47,876
presentation and also to introduce

16
00:00:47,908 --> 00:00:49,928
yourself, if you'd like. Yes, that's a

17
00:00:49,934 --> 00:00:51,908
good idea. Thank you very much, Daniel.

18
00:00:52,084 --> 00:00:54,248
So my name is Sarah. I'm a

19
00:00:54,254 --> 00:00:55,476
neuroscientist specializing

20
00:00:55,508 --> 00:00:57,068
intelligence, currently working in the

21
00:00:57,074 --> 00:00:59,836
field of neuromorphic computing at

22
00:00:59,858 --> 00:01:02,156
Sheffield Hallam in the UK. So I'm going

23
00:01:02,178 --> 00:01:03,676
to give you a high level overview of

24
00:01:03,698 --> 00:01:06,124
what neuromorphic computing is before we

25
00:01:06,162 --> 00:01:08,124
hear David's exciting talk in the first

26
00:01:08,162 --> 00:01:11,228
edition of this new series. Just to let

27
00:01:11,234 --> 00:01:12,316
you know, if you're watching on Double

28
00:01:12,348 --> 00:01:13,984
Time in the future, I talk quite fast,

29
00:01:14,022 --> 00:01:15,248
so you might not want to watch me on

30
00:01:15,254 --> 00:01:18,656
Double Time. So this QR code I put

31
00:01:18,678 --> 00:01:20,448
here will take you to a paper which I

32
00:01:20,454 --> 00:01:21,756
thought was a really nice introduction

33
00:01:21,788 --> 00:01:24,576
to the field. But neuromorphic computing

34
00:01:24,608 --> 00:01:26,516
can be defined as computing systems that

35
00:01:26,538 --> 00:01:28,196
are designed to mimic the structure and

36
00:01:28,218 --> 00:01:30,436
function of the nervous system. So this

37
00:01:30,458 --> 00:01:31,776
doesn't have to be the human nervous

38
00:01:31,808 --> 00:01:33,224
system. The field actually takes

39
00:01:33,262 --> 00:01:35,076
inspiration from all sorts of animals

40
00:01:35,108 --> 00:01:37,268
and insects, although the definitions

41
00:01:37,284 --> 00:01:38,724
online don't necessarily acknowledge

42
00:01:38,772 --> 00:01:41,496
that. So some people are quite open with

43
00:01:41,518 --> 00:01:43,304
what constitutes neuromorphic, while

44
00:01:43,342 --> 00:01:44,960
maybe others would prefer neuromorphic

45
00:01:44,980 --> 00:01:47,464
was reserved for hardware instantiations

46
00:01:47,512 --> 00:01:50,236
of biological like neurons, which are

47
00:01:50,258 --> 00:01:52,504
sometimes referred to as non von Newman

48
00:01:52,552 --> 00:01:54,284
computers. And I think that's what the

49
00:01:54,322 --> 00:01:57,788
paper that that QR code refers to it as

50
00:01:57,954 --> 00:02:00,168
their definition. So what I think is

51
00:02:00,194 --> 00:02:01,168
really interesting is a little bit of

52
00:02:01,174 --> 00:02:02,668
the context. So, like our current von

53
00:02:02,684 --> 00:02:04,176
Newman computer architecture was also

54
00:02:04,198 --> 00:02:06,316
inspired by neuroscience, particularly

55
00:02:06,348 --> 00:02:08,796
the McCullough and Pitts 43. Neuromodel

56
00:02:08,908 --> 00:02:10,656
inspired von Newman's first draft in

57
00:02:10,678 --> 00:02:13,540
1945. So neuroscience has a long history

58
00:02:13,610 --> 00:02:16,004
of inspiring computer science, and this

59
00:02:16,042 --> 00:02:17,412
also includes reinforcement learning,

60
00:02:17,466 --> 00:02:18,980
which is based on theories about

61
00:02:19,050 --> 00:02:20,356
learning, decision making from

62
00:02:20,378 --> 00:02:22,404
behavioral psychology based on rewards

63
00:02:22,452 --> 00:02:24,452
and punishments, and also Hebian

64
00:02:24,516 --> 00:02:26,232
learning principles of cells that fire

65
00:02:26,286 --> 00:02:29,284
together, wire together from 49 became

66
00:02:29,332 --> 00:02:31,464
foundational for unsupervised learning.

67
00:02:31,582 --> 00:02:34,970
So, first of all, hang on 1 second.

68
00:02:35,820 --> 00:02:38,156
So in order to understand the why of

69
00:02:38,178 --> 00:02:39,676
neuromorphic computing, I really wanted

70
00:02:39,698 --> 00:02:41,244
to explain what's so great about the

71
00:02:41,282 --> 00:02:44,444
brain. So here's some inspiration for

72
00:02:44,482 --> 00:02:46,508
light bulbs. So I'm going to ask you a

73
00:02:46,514 --> 00:02:47,516
question. I just want you to think about

74
00:02:47,538 --> 00:02:49,244
it for a second. In terms of light

75
00:02:49,282 --> 00:02:51,344
bulbs, how much energy do you think the

76
00:02:51,382 --> 00:02:53,904
brain uses? Do you think it's more or

77
00:02:53,942 --> 00:02:56,368
less energy than the bulbs lighting the

78
00:02:56,374 --> 00:02:58,128
room that you're in? If you're in the

79
00:02:58,134 --> 00:02:59,548
future, by all means pause this. Pause

80
00:02:59,564 --> 00:03:01,216
this. If you want to do some in depth

81
00:03:01,248 --> 00:03:02,836
calculations. But I'm going to skip to

82
00:03:02,858 --> 00:03:06,484
the answer. The answer is here,

83
00:03:06,522 --> 00:03:09,568
in the pink circle. So it's 20 watts.

84
00:03:09,744 --> 00:03:11,616
So that's the equivalent of one modern

85
00:03:11,648 --> 00:03:13,956
day energy efficient light bulb. So

86
00:03:13,978 --> 00:03:15,384
that's probably what's above me now,

87
00:03:15,422 --> 00:03:18,280
basically in my room here. This QR code

88
00:03:18,350 --> 00:03:19,432
should take you to quite an interesting

89
00:03:19,486 --> 00:03:20,996
paper on power consumption in the brain,

90
00:03:21,028 --> 00:03:24,456
if you're interested in that. So that

91
00:03:24,478 --> 00:03:26,868
works out about four bananas a day to

92
00:03:26,894 --> 00:03:28,876
power your brain. And this is

93
00:03:28,898 --> 00:03:31,096
calculated, by the way, based on calorie

94
00:03:31,128 --> 00:03:33,084
intake that the brain needs. So, for

95
00:03:33,122 --> 00:03:35,196
context, the fastest supercomputer in

96
00:03:35,218 --> 00:03:36,716
Europe, I think it's called lumi, in

97
00:03:36,738 --> 00:03:39,196
Finland it's been called exceptionally

98
00:03:39,228 --> 00:03:42,192
green, and its power consumption is 8.5

99
00:03:42,246 --> 00:03:44,608
million watts. So that's around half a

100
00:03:44,614 --> 00:03:46,556
million light bulbs, while your brain

101
00:03:46,588 --> 00:03:49,396
uses just one. So then the question is,

102
00:03:49,418 --> 00:03:51,284
well, what does your brain do with that

103
00:03:51,322 --> 00:03:53,540
one light bulb or four bananas?

104
00:03:55,080 --> 00:03:57,584
Apparently it does 1000 billion

105
00:03:57,632 --> 00:04:00,368
calculations per second. So there's lots

106
00:04:00,384 --> 00:04:02,324
of other massive estimates out there.

107
00:04:02,442 --> 00:04:03,956
This wasn't even the largest by several

108
00:04:03,988 --> 00:04:06,056
orders of magnitude. Estimates are

109
00:04:06,078 --> 00:04:07,988
obviously very speculative, but they're

110
00:04:08,004 --> 00:04:09,736
all massive and they all tend to be

111
00:04:09,758 --> 00:04:11,384
based on the number of neurons, their

112
00:04:11,422 --> 00:04:13,928
connections and firing rates. But I

113
00:04:13,934 --> 00:04:14,956
think it's really important for the

114
00:04:14,978 --> 00:04:17,064
context that supercomputers can't

115
00:04:17,112 --> 00:04:19,756
actually yet match our complexity of

116
00:04:19,778 --> 00:04:22,172
skills or the adaptability of the human

117
00:04:22,226 --> 00:04:24,776
brain. So we actually excel way beyond

118
00:04:24,808 --> 00:04:26,384
supercomputers when it comes to things

119
00:04:26,422 --> 00:04:28,880
like complex decision making, learning

120
00:04:28,950 --> 00:04:32,096
from experience. So how does

121
00:04:32,118 --> 00:04:34,416
your brain compare to AI? So, I

122
00:04:34,438 --> 00:04:36,144
mentioned that modern AI is already

123
00:04:36,182 --> 00:04:38,316
brain inspired. However, artificial

124
00:04:38,348 --> 00:04:41,476
neurons are highly simplified. They

125
00:04:41,498 --> 00:04:42,676
don't capture the complexity of

126
00:04:42,698 --> 00:04:44,276
biological neurons or networks, like,

127
00:04:44,298 --> 00:04:47,348
not even close. Individual neurons are

128
00:04:47,354 --> 00:04:49,124
actually more like networks themselves.

129
00:04:49,322 --> 00:04:51,204
And research suggests that modeling one

130
00:04:51,242 --> 00:04:53,176
biological neuron requires a five to

131
00:04:53,198 --> 00:04:55,316
eight layer deep artificial neural

132
00:04:55,348 --> 00:04:57,684
network made of around 1000 artificial

133
00:04:57,732 --> 00:04:59,864
neurons. This QR code should take you to

134
00:04:59,902 --> 00:05:01,850
the paper. For that.

135
00:05:03,020 --> 00:05:05,036
You have 86 billion neurons in your

136
00:05:05,058 --> 00:05:07,228
brain. They work together to form a

137
00:05:07,234 --> 00:05:09,032
highly energy efficient, low latency

138
00:05:09,096 --> 00:05:11,644
supercomputer that works just above room

139
00:05:11,682 --> 00:05:13,852
temperature, off the equivalent of about

140
00:05:13,906 --> 00:05:16,568
four bananas a day. So hopefully I've

141
00:05:16,584 --> 00:05:17,836
given you a sense of how amazing your

142
00:05:17,858 --> 00:05:19,724
brain is, as if you didn't know that

143
00:05:19,762 --> 00:05:22,064
already, and how it's already been used

144
00:05:22,102 --> 00:05:24,076
to inspire the, I guess, fairly basic AI

145
00:05:24,108 --> 00:05:25,552
that we have now compared to human

146
00:05:25,606 --> 00:05:27,488
intelligence. So, next I'm going to

147
00:05:27,494 --> 00:05:29,164
explain how key features of the brain

148
00:05:29,212 --> 00:05:31,396
are being implemented to catalyze our

149
00:05:31,418 --> 00:05:33,508
next generation of AI and technology

150
00:05:33,674 --> 00:05:35,216
through the field of neuromorphic

151
00:05:35,248 --> 00:05:37,284
computing, which is why you're all here.

152
00:05:37,482 --> 00:05:39,824
So, traditional volume and computers

153
00:05:39,872 --> 00:05:41,784
have physically separate computing and

154
00:05:41,822 --> 00:05:44,010
memory units, shown here on the left.

155
00:05:44,620 --> 00:05:46,744
During computation, data must transfer

156
00:05:46,862 --> 00:05:48,264
backwards and forwards, like really

157
00:05:48,302 --> 00:05:49,924
fast. So there's a bottleneck,

158
00:05:49,972 --> 00:05:51,930
essentially for speed and energy,

159
00:05:52,620 --> 00:05:54,516
whereas in neuromorphic architectures,

160
00:05:54,548 --> 00:05:55,788
which are shown here on the right, with

161
00:05:55,794 --> 00:05:58,504
the help of Dali, computing and memory

162
00:05:58,552 --> 00:06:00,796
occur in the same place. So they're said

163
00:06:00,818 --> 00:06:03,320
to be colocated. Essentially,

164
00:06:03,400 --> 00:06:05,672
individual neurons perform computation

165
00:06:05,816 --> 00:06:07,488
while memory is represented by the

166
00:06:07,494 --> 00:06:09,372
strength of the connections, the weights

167
00:06:09,436 --> 00:06:12,272
between neurons. So the synapses. So

168
00:06:12,326 --> 00:06:13,984
chips like this might be created with

169
00:06:14,022 --> 00:06:16,880
components like Memoristas for example,

170
00:06:17,030 --> 00:06:20,076
which can emulate synaptic weights. And

171
00:06:20,118 --> 00:06:22,276
this architecture improves speed, it

172
00:06:22,298 --> 00:06:24,336
reduces energy consumption. And what's

173
00:06:24,368 --> 00:06:25,936
really interesting is it enables

174
00:06:25,968 --> 00:06:28,128
massively parallel processing meaning

175
00:06:28,144 --> 00:06:29,940
that multiple problems can be worked on

176
00:06:30,010 --> 00:06:32,324
at the same time. So this is

177
00:06:32,362 --> 00:06:33,524
particularly important, this

178
00:06:33,562 --> 00:06:35,628
architecture, for various use cases,

179
00:06:35,744 --> 00:06:37,576
but also because as we reach the end of

180
00:06:37,598 --> 00:06:39,464
Moore's Law, which is the number of

181
00:06:39,502 --> 00:06:41,396
transistors you were able to physically

182
00:06:41,508 --> 00:06:44,208
make tinier and tinier to fit on a chip.

183
00:06:44,404 --> 00:06:46,792
And it's also important because humanity

184
00:06:46,856 --> 00:06:48,620
needs to massively reduce its energy

185
00:06:48,690 --> 00:06:51,468
consumption against the backdrop drop of

186
00:06:51,554 --> 00:06:53,820
creating ever more powerful AI.

187
00:06:55,280 --> 00:06:57,644
So artificial neurons typically use

188
00:06:57,682 --> 00:07:00,092
continuous activation shown on the left,

189
00:07:00,226 --> 00:07:02,316
they're always on, while neuromorphic

190
00:07:02,348 --> 00:07:04,576
neurons, they're said to be spiking, so

191
00:07:04,598 --> 00:07:06,096
they're on or they're off, which is

192
00:07:06,118 --> 00:07:07,936
shown here on the right. So similar to

193
00:07:07,958 --> 00:07:10,116
sort of an action potential. So the

194
00:07:10,138 --> 00:07:11,780
benefits for this are again power,

195
00:07:11,850 --> 00:07:14,452
efficiency and also applications where

196
00:07:14,506 --> 00:07:16,644
timing is important and this is given

197
00:07:16,682 --> 00:07:18,724
that they're event driven. So

198
00:07:18,762 --> 00:07:21,544
essentially they have the potential for

199
00:07:21,582 --> 00:07:23,944
spatial and temporal dimensions which

200
00:07:23,982 --> 00:07:25,796
then enables added spatiotemporal

201
00:07:25,828 --> 00:07:27,770
encoding and processing of information.

202
00:07:28,460 --> 00:07:30,708
You might be wondering a bit about GPUs

203
00:07:30,884 --> 00:07:33,140
which also enable parallel processing.

204
00:07:33,300 --> 00:07:35,640
Research suggests that GPUs are suitable

205
00:07:35,720 --> 00:07:37,656
architectures for deploying spiking

206
00:07:37,688 --> 00:07:40,124
neural networks which I think makes this

207
00:07:40,162 --> 00:07:41,772
a really interesting time for the field

208
00:07:41,826 --> 00:07:43,800
given how high end GPUs are becoming

209
00:07:43,880 --> 00:07:45,420
ever more pervasive.

210
00:07:47,280 --> 00:07:50,264
So the brain learns strength of synapses

211
00:07:50,312 --> 00:07:52,444
between neurons. This is based on pre

212
00:07:52,482 --> 00:07:54,336
and synaptic firing patterns. I think

213
00:07:54,358 --> 00:07:56,768
David's Talk will talk a lot, we'll go

214
00:07:56,774 --> 00:07:58,368
into a lot more depth on this. But there

215
00:07:58,374 --> 00:08:00,436
are many different types and patterns of

216
00:08:00,458 --> 00:08:02,468
this across the brain depending on the

217
00:08:02,474 --> 00:08:04,996
types of synapses such as excitatory to

218
00:08:05,018 --> 00:08:07,540
inhibitory, excitatory to excalitatory.

219
00:08:07,960 --> 00:08:10,244
And the neuromorphic field is working to

220
00:08:10,282 --> 00:08:12,804
leverage these rules because of benefits

221
00:08:12,922 --> 00:08:15,192
for on chip learning and also

222
00:08:15,246 --> 00:08:17,044
applications such as pattern recognition

223
00:08:17,092 --> 00:08:18,756
and edge computing as well. Edge

224
00:08:18,788 --> 00:08:20,536
computing being quite a huge use case

225
00:08:20,558 --> 00:08:22,056
for neuromorphic computing because of

226
00:08:22,078 --> 00:08:24,024
the sort of event driven nature and also

227
00:08:24,062 --> 00:08:27,004
the low energy usage. And this QR code

228
00:08:27,042 --> 00:08:28,220
should take you to quite an interesting

229
00:08:28,290 --> 00:08:31,230
paper on STDP that I found.

230
00:08:32,720 --> 00:08:34,924
So what neuromorphic solutions are

231
00:08:34,962 --> 00:08:36,076
available? Now? You might just think

232
00:08:36,098 --> 00:08:37,756
this is all theoretical, there are

233
00:08:37,778 --> 00:08:39,448
actually many different solutions out

234
00:08:39,474 --> 00:08:41,072
there which I'll just give you a really

235
00:08:41,126 --> 00:08:43,392
high level overview of. So the Human

236
00:08:43,446 --> 00:08:45,584
Brain Project has created several large

237
00:08:45,622 --> 00:08:47,564
scale neuromorphic computers including

238
00:08:47,612 --> 00:08:49,088
Spinnaker, which is this one at the

239
00:08:49,094 --> 00:08:51,332
bottom, this board's, like maybe like

240
00:08:51,466 --> 00:08:53,284
the size of my face or something. So

241
00:08:53,322 --> 00:08:55,488
that runs in real time and it's

242
00:08:55,504 --> 00:08:57,312
comprised of multiple general purpose

243
00:08:57,376 --> 00:09:00,084
arm microprocessors and there was also

244
00:09:00,122 --> 00:09:02,468
BrainScaleS which is an accelerated

245
00:09:02,564 --> 00:09:05,432
analog architecture and it runs 1000

246
00:09:05,486 --> 00:09:08,744
times real time. So the

247
00:09:08,782 --> 00:09:10,696
board next to the blue one, that's an

248
00:09:10,718 --> 00:09:12,728
actual credit card size version of

249
00:09:12,734 --> 00:09:13,956
BrainScaleS, which they've recently

250
00:09:13,988 --> 00:09:15,610
made, which I thought was pretty cool.

251
00:09:16,540 --> 00:09:18,296
And then there's also some big players

252
00:09:18,328 --> 00:09:20,492
in the space. So this blue one here is

253
00:09:20,546 --> 00:09:22,856
Intel's Louihi chip. They're onto Luihi

254
00:09:22,888 --> 00:09:24,616
two. Now, that's their Neuromorphic

255
00:09:24,648 --> 00:09:26,104
chip, and they have an open source

256
00:09:26,152 --> 00:09:27,356
software framework for that as well,

257
00:09:27,378 --> 00:09:28,636
because they really want to catalyze the

258
00:09:28,658 --> 00:09:30,156
open source community to get involved

259
00:09:30,188 --> 00:09:32,784
with it. So neuromorphic sensors also

260
00:09:32,822 --> 00:09:34,448
exist? So this little blue thing in the

261
00:09:34,454 --> 00:09:35,628
middle is actually a Neuromorphic

262
00:09:35,644 --> 00:09:38,544
camera. It's maybe like this big. So

263
00:09:38,582 --> 00:09:40,448
they aim to recreate how our nervous

264
00:09:40,464 --> 00:09:43,092
system senses stimuli, such as light.

265
00:09:43,226 --> 00:09:45,008
So, for example, in a Neuromorphic

266
00:09:45,024 --> 00:09:46,804
camera, which is the one here, each

267
00:09:46,842 --> 00:09:48,836
pixel works independently with a

268
00:09:48,858 --> 00:09:50,916
microsecond resolution. Hopefully, my

269
00:09:50,938 --> 00:09:53,368
GIF will work. There we go. So you can

270
00:09:53,374 --> 00:09:55,336
see each pixel working there, which is

271
00:09:55,358 --> 00:09:57,860
pretty cool. So compared to traditional

272
00:09:57,940 --> 00:09:59,604
digital cameras, they have improved

273
00:09:59,652 --> 00:10:01,672
performance with motion and lower power

274
00:10:01,726 --> 00:10:03,624
consumption. There was also a

275
00:10:03,662 --> 00:10:05,604
Neuromorphic nose recently by intel,

276
00:10:05,652 --> 00:10:07,508
which was pretty cool. So it could learn

277
00:10:07,534 --> 00:10:09,116
the scent of a chemical after just one

278
00:10:09,138 --> 00:10:11,672
exposure, and then it could identify

279
00:10:11,736 --> 00:10:13,324
that scent even when it was masked by

280
00:10:13,362 --> 00:10:15,708
others. And then finally, this is a

281
00:10:15,714 --> 00:10:18,316
humanoid robot called an ICup. And what

282
00:10:18,338 --> 00:10:19,804
you can do is you can actually integrate

283
00:10:19,852 --> 00:10:21,488
Neuromorphic sensors, such as the

284
00:10:21,494 --> 00:10:24,044
camera, and then Neuromorphic chips

285
00:10:24,092 --> 00:10:27,056
maybe spinnaker or brain scales into a

286
00:10:27,078 --> 00:10:28,544
humanoid device like this or other

287
00:10:28,582 --> 00:10:30,436
devices like a drone. And then from

288
00:10:30,458 --> 00:10:31,936
that, you can actually create embodied

289
00:10:31,968 --> 00:10:34,308
neuromorphic systems. And this is

290
00:10:34,314 --> 00:10:35,904
something that we work on at the Smart

291
00:10:35,952 --> 00:10:37,636
Interactive Technologies Research Lab in

292
00:10:37,658 --> 00:10:39,060
Sheffield in the UK.

293
00:10:41,320 --> 00:10:42,856
This slide just highlights some of the

294
00:10:42,878 --> 00:10:44,676
potential applications of Neuromorphic

295
00:10:44,708 --> 00:10:45,976
computing, which I thought were quite

296
00:10:45,998 --> 00:10:47,704
interesting when you think about it. So

297
00:10:47,742 --> 00:10:49,524
the understanding of context, pattern

298
00:10:49,572 --> 00:10:51,876
recognition, advanced sensing, fusot,

299
00:10:51,908 --> 00:10:54,216
learning, generalizing across tasks,

300
00:10:54,328 --> 00:10:55,692
complex decision making,

301
00:10:55,826 --> 00:10:58,460
explainability, and brain interfaces.

302
00:10:58,880 --> 00:11:00,652
So all these skills are really

303
00:11:00,706 --> 00:11:02,012
beneficial when you're thinking about

304
00:11:02,066 --> 00:11:04,152
human centered, real time applications

305
00:11:04,216 --> 00:11:06,732
in dynamic environments. So things like

306
00:11:06,786 --> 00:11:09,104
self driving cars, for example. And

307
00:11:09,142 --> 00:11:10,876
personally, I think that neuromorphic

308
00:11:10,908 --> 00:11:13,520
systems are also likely to be the future

309
00:11:13,590 --> 00:11:15,468
substrate of brain computer interfaces.

310
00:11:15,564 --> 00:11:16,688
Probably a bit biased because I'm a

311
00:11:16,694 --> 00:11:19,008
neuroscientist, but they're low energy,

312
00:11:19,094 --> 00:11:20,804
they're real time, and they also have

313
00:11:20,842 --> 00:11:23,028
architectures which match our own

314
00:11:23,114 --> 00:11:25,476
hardware. So I do think we'll soon see

315
00:11:25,498 --> 00:11:27,284
the BCI field being catalyzed by

316
00:11:27,322 --> 00:11:29,684
neuromorphic systems, particularly maybe

317
00:11:29,722 --> 00:11:32,632
for hybrids of hardware and wetware. So

318
00:11:32,686 --> 00:11:34,148
maybe even potentially containing

319
00:11:34,164 --> 00:11:36,936
people's own brain cells, which you can

320
00:11:36,958 --> 00:11:38,890
actually grow just from a hair cell.

321
00:11:41,180 --> 00:11:43,496
And a particular focus of our work is

322
00:11:43,518 --> 00:11:45,512
designing AI, which learns in a similar

323
00:11:45,566 --> 00:11:47,528
way to a human. So it has an innate

324
00:11:47,544 --> 00:11:49,016
sense of curiosity, and it learns

325
00:11:49,048 --> 00:11:51,340
through interacting with the real world.

326
00:11:51,490 --> 00:11:53,660
So in the 50s, Alan Turing said,

327
00:11:53,730 --> 00:11:55,868
instead of trying to produce a program

328
00:11:55,954 --> 00:11:58,204
to simulate the adult mind, why not

329
00:11:58,242 --> 00:11:59,744
rather try to produce one which

330
00:11:59,782 --> 00:12:02,224
simulates the child's brain. If this

331
00:12:02,262 --> 00:12:03,596
were then subject to an appropriate

332
00:12:03,628 --> 00:12:05,452
course of education, one would obtain

333
00:12:05,516 --> 00:12:08,176
the adult brain. This is very much the

334
00:12:08,198 --> 00:12:10,364
philosophy behind the neurodevelopmental

335
00:12:10,412 --> 00:12:13,056
approach to AI and neuromorphic

336
00:12:13,088 --> 00:12:14,276
computing, which I just wanted to

337
00:12:14,298 --> 00:12:17,524
highlight. There are some

338
00:12:17,562 --> 00:12:19,236
challenges in the field. These are very

339
00:12:19,258 --> 00:12:20,628
high level, but I'll just give you a

340
00:12:20,634 --> 00:12:22,564
little bit of an idea of it. So,

341
00:12:22,682 --> 00:12:24,536
training spiking neural networks is more

342
00:12:24,558 --> 00:12:25,876
complex than traditional neural

343
00:12:25,908 --> 00:12:28,424
networks. Also designing hardware which

344
00:12:28,462 --> 00:12:29,828
actually implements spiking neural

345
00:12:29,844 --> 00:12:32,488
networks STDP on a large scale is said

346
00:12:32,494 --> 00:12:34,732
to be fairly challenging. And then also

347
00:12:34,786 --> 00:12:36,492
developing algorithms which can actually

348
00:12:36,546 --> 00:12:38,716
effectively leverage all these

349
00:12:38,898 --> 00:12:41,400
technologies. So the hardware, the STDP,

350
00:12:41,480 --> 00:12:44,110
it's an ongoing active area of research.

351
00:12:46,640 --> 00:12:49,084
So if you're here, you're probably

352
00:12:49,122 --> 00:12:51,456
interested in active inference. So I

353
00:12:51,478 --> 00:12:52,656
wanted to highlight this. Actually,

354
00:12:52,678 --> 00:12:53,968
someone put on the discord today one of

355
00:12:53,974 --> 00:12:55,360
these studies, which was pretty cool.

356
00:12:55,510 --> 00:12:57,856
So a couple of recent studies have

357
00:12:58,038 --> 00:12:59,616
combined neuromorphic computing with

358
00:12:59,638 --> 00:13:01,524
principles of active inference. So

359
00:13:01,562 --> 00:13:02,596
active inference comes from

360
00:13:02,618 --> 00:13:04,468
neuroscience, and I would argue that it

361
00:13:04,474 --> 00:13:06,096
lends itself very well to neuromorphic

362
00:13:06,128 --> 00:13:08,836
architectures. In a recent paper on

363
00:13:08,858 --> 00:13:11,140
embodied neuromorphic intelligence, so

364
00:13:11,290 --> 00:13:12,448
it didn't really mention active

365
00:13:12,464 --> 00:13:14,344
inference in it. The QR code on the top

366
00:13:14,382 --> 00:13:16,616
right here, it was suggested that a real

367
00:13:16,638 --> 00:13:18,216
breakthrough in neuromorphics will

368
00:13:18,238 --> 00:13:20,056
happen if the whole system design is

369
00:13:20,078 --> 00:13:21,732
based on biological computational

370
00:13:21,796 --> 00:13:23,876
principles with a tight interplay

371
00:13:23,908 --> 00:13:25,336
between the estimation of the

372
00:13:25,358 --> 00:13:27,340
surroundings and the robot's own state

373
00:13:27,410 --> 00:13:29,084
and decision making, planning and

374
00:13:29,122 --> 00:13:31,436
action. So some of those themes might

375
00:13:31,458 --> 00:13:32,444
sound quite familiar to people

376
00:13:32,482 --> 00:13:34,796
interested in active inference, and I

377
00:13:34,818 --> 00:13:36,396
would suggest that active inference is

378
00:13:36,418 --> 00:13:38,396
well placed to meet these requirements.

379
00:13:38,588 --> 00:13:40,096
And I just wanted to highlight a couple

380
00:13:40,118 --> 00:13:42,496
of recent studies here. So this one on

381
00:13:42,518 --> 00:13:44,524
the left, Gandalfiatal, recently

382
00:13:44,572 --> 00:13:46,844
demonstrated plasticity and rapid

383
00:13:46,892 --> 00:13:49,328
unsupervised learning in a neuromorphic

384
00:13:49,344 --> 00:13:51,184
system using active inference

385
00:13:51,232 --> 00:13:53,188
principles. The author suggested that

386
00:13:53,194 --> 00:13:54,724
their experiments could be adopted to

387
00:13:54,762 --> 00:13:56,384
implement brain like predictive

388
00:13:56,432 --> 00:13:59,904
capabilities in neuromorphic robotic

389
00:13:59,952 --> 00:14:02,016
systems. And then there was the Dish

390
00:14:02,048 --> 00:14:03,496
brain paper, which some of you may be

391
00:14:03,518 --> 00:14:06,136
familiar with by Kaganatal. So this was

392
00:14:06,158 --> 00:14:09,236
a hybrid wetware hardware neuromorphic

393
00:14:09,268 --> 00:14:10,984
system that the authors claimed was

394
00:14:11,022 --> 00:14:13,156
embodied. The system showed rapid

395
00:14:13,188 --> 00:14:15,256
apparent learning of the game of Pong

396
00:14:15,368 --> 00:14:16,956
using the free energy principle for

397
00:14:16,978 --> 00:14:19,116
learning. And the authors claimed that

398
00:14:19,138 --> 00:14:20,744
the system exhibited synthetic

399
00:14:20,792 --> 00:14:23,820
biological intelligence. So the field

400
00:14:23,890 --> 00:14:25,720
implementing active inference principles

401
00:14:25,800 --> 00:14:28,292
in neuromorphic systems is very nascent.

402
00:14:28,376 --> 00:14:30,204
And the idea behind this more stream

403
00:14:30,252 --> 00:14:32,336
series is to create a space and a

404
00:14:32,358 --> 00:14:34,944
community to share knowledge, ideas and

405
00:14:34,982 --> 00:14:38,144
expertise to catalyze the field. I think

406
00:14:38,182 --> 00:14:40,444
some really exciting technological leaps

407
00:14:40,492 --> 00:14:41,604
are probably going to come from this

408
00:14:41,642 --> 00:14:44,596
area. So thank you for listening to my

409
00:14:44,698 --> 00:14:46,616
quick run through Neuromorphic 101.

410
00:14:46,618 --> 00:14:50,052
And next up, we're going to hear

411
00:14:50,106 --> 00:14:52,696
from David. So David, over to you. If

412
00:14:52,718 --> 00:14:54,730
you want to introduce yourself, please.

413
00:14:55,260 --> 00:14:56,600
Thank you, Sarah.

414
00:14:58,220 --> 00:15:00,890
I would share my screen.

415
00:15:04,070 --> 00:15:04,820
Yeah.

416
00:15:14,950 --> 00:15:19,106
Can you see now my screen,

417
00:15:19,208 --> 00:15:21,858
the presentation yeah. Okay, perfect.

418
00:15:22,024 --> 00:15:25,122
Okay. Hello. My name is David Kappel.

419
00:15:25,186 --> 00:15:28,246
I'm a researcher and group leader at the

420
00:15:28,268 --> 00:15:31,238
Institute for Moyo Informatics at the

421
00:15:31,244 --> 00:15:34,802
Rural University Bohom. So I'm

422
00:15:34,946 --> 00:15:36,946
leading the group on sustainable machine

423
00:15:36,978 --> 00:15:38,938
learning and we have a very strong focus

424
00:15:39,024 --> 00:15:40,986
on neuromorphic computing. That's why

425
00:15:41,008 --> 00:15:43,994
I'm here today. And I'm going to start

426
00:15:44,032 --> 00:15:46,410
with a very similar motivation than

427
00:15:46,480 --> 00:15:48,414
Sarah did, which was really a great

428
00:15:48,532 --> 00:15:51,294
inspiration for this talk, I think. So

429
00:15:51,332 --> 00:15:54,110
probably most of you have seen this

430
00:15:54,180 --> 00:15:55,966
interesting recent result, and I don't

431
00:15:55,988 --> 00:15:58,282
mean Germany winning the basketball

432
00:15:58,346 --> 00:16:01,870
championship, but this really big leap

433
00:16:02,790 --> 00:16:05,426
in artificial intelligence that we have

434
00:16:05,448 --> 00:16:07,106
seen in the last years, especially the

435
00:16:07,128 --> 00:16:08,978
last two or three years. So this is a

436
00:16:08,984 --> 00:16:11,474
picture generated from a prompt by a

437
00:16:11,512 --> 00:16:14,742
Dali network and it's really amazing.

438
00:16:14,796 --> 00:16:17,094
It was considered science fiction like

439
00:16:17,132 --> 00:16:20,102
only two, three years ago. And this was

440
00:16:20,156 --> 00:16:23,078
essentially made possible by a

441
00:16:23,084 --> 00:16:24,866
neuromorphic approach, which is deep

442
00:16:24,898 --> 00:16:27,718
neural networks. And these deep neural

443
00:16:27,734 --> 00:16:30,234
networks have become huge now, but this

444
00:16:30,272 --> 00:16:33,018
comes also with a caveat. So basically

445
00:16:33,104 --> 00:16:35,882
the flip side is that among other

446
00:16:35,936 --> 00:16:38,022
problems maybe these models may have,

447
00:16:38,176 --> 00:16:41,774
they consume huge amounts of energy.

448
00:16:41,892 --> 00:16:45,642
So models like Dali or Jet GPT,

449
00:16:45,786 --> 00:16:49,054
as Sarah already mentioned, they would

450
00:16:49,092 --> 00:16:51,922
consume energy budgets that are

451
00:16:51,976 --> 00:16:54,530
comparable to houses or cars. So

452
00:16:54,680 --> 00:16:57,794
training Chat GPT a single time is like

453
00:16:57,832 --> 00:17:00,514
a gigawatt hour approximately. So that

454
00:17:00,552 --> 00:17:04,310
would be 300 tons of CO2 emission

455
00:17:05,530 --> 00:17:08,486
and many times, which comes down to many

456
00:17:08,508 --> 00:17:12,280
times the lifespan of a typical car.

457
00:17:12,890 --> 00:17:14,390
So this comes with two problems.

458
00:17:14,460 --> 00:17:17,298
Obviously, this makes training these

459
00:17:17,324 --> 00:17:19,562
models only accessible to a very small

460
00:17:19,696 --> 00:17:21,386
number of very large players. So

461
00:17:21,408 --> 00:17:23,882
essentially the big tech companies. And

462
00:17:23,936 --> 00:17:25,306
secondly, and maybe even more

463
00:17:25,328 --> 00:17:27,274
importantly, this is not compatible with

464
00:17:27,312 --> 00:17:31,086
a planet with limited resources. So if

465
00:17:31,188 --> 00:17:33,566
the growth rate of AI continues like it

466
00:17:33,588 --> 00:17:35,710
did in the last years, it will consume

467
00:17:36,610 --> 00:17:39,054
13% of the global energy consumption by

468
00:17:39,092 --> 00:17:42,850
2030 and it will basically outrun

469
00:17:44,070 --> 00:17:46,642
the transportation sector in another

470
00:17:46,696 --> 00:17:50,194
five years or so. So this raises the

471
00:17:50,232 --> 00:17:51,726
question, does sustainable machine

472
00:17:51,758 --> 00:17:54,370
learning exist at all? And obviously,

473
00:17:54,440 --> 00:17:55,958
since I'm working in the group of

474
00:17:56,044 --> 00:17:57,574
sustainable machine learning, I believe

475
00:17:57,612 --> 00:18:01,046
it does. And why I think it

476
00:18:01,068 --> 00:18:04,214
does is because we know a system that is

477
00:18:04,252 --> 00:18:07,386
very efficient and is still probably

478
00:18:07,488 --> 00:18:10,346
better than these AI models, which is

479
00:18:10,368 --> 00:18:11,866
the human brain, which consumes, as

480
00:18:11,888 --> 00:18:15,434
Sarah mentioned, around 20 watts or

481
00:18:15,472 --> 00:18:16,940
four bananas a day.

482
00:18:20,290 --> 00:18:22,414
It's many orders of magnitude more

483
00:18:22,452 --> 00:18:25,166
efficient than the AI models we have

484
00:18:25,188 --> 00:18:28,906
today. But so far we don't

485
00:18:28,938 --> 00:18:32,178
know essentially how these networks work

486
00:18:32,344 --> 00:18:34,562
and especially how to train them. And

487
00:18:34,616 --> 00:18:37,346
basically our goal is to transfer now

488
00:18:37,368 --> 00:18:40,594
mechanisms from machine learning. We

489
00:18:40,632 --> 00:18:43,490
have this nice picture here. Basically

490
00:18:43,640 --> 00:18:45,574
we start from the machine learning side

491
00:18:45,612 --> 00:18:47,800
where we already know our way around.

492
00:18:48,330 --> 00:18:50,214
So we have these models that are

493
00:18:50,252 --> 00:18:53,174
wonderful and that give us really

494
00:18:53,212 --> 00:18:55,298
impressive results, but they are not

495
00:18:55,324 --> 00:18:57,802
efficient and we want to transfer them

496
00:18:57,856 --> 00:19:02,518
to a new efficient AI generation.

497
00:19:02,694 --> 00:19:04,826
And our idea is to use inspiration from

498
00:19:04,848 --> 00:19:08,094
neuroscience that make this

499
00:19:08,132 --> 00:19:10,526
transfer faster and possible in the

500
00:19:10,548 --> 00:19:13,760
first place. Okay?

501
00:19:15,490 --> 00:19:17,886
Actually, biology is a great source of

502
00:19:17,908 --> 00:19:20,546
inspiration and always comes around the

503
00:19:20,568 --> 00:19:22,494
corner with very surprising results.

504
00:19:22,542 --> 00:19:24,286
And one of these results that I stumbled

505
00:19:24,318 --> 00:19:26,260
upon a couple of years ago is that

506
00:19:27,990 --> 00:19:31,698
reliability of synapses in

507
00:19:31,704 --> 00:19:33,814
the brain. So, as you probably know,

508
00:19:33,852 --> 00:19:35,494
neurons in your brain are connected for

509
00:19:35,532 --> 00:19:39,560
synapses. But if you look at

510
00:19:40,810 --> 00:19:44,886
this is a paper from 2019, and they

511
00:19:44,908 --> 00:19:46,298
could actually identify individual

512
00:19:46,384 --> 00:19:48,522
synapses and they could trigger them to

513
00:19:48,656 --> 00:19:50,810
make a synaptic release, like a single

514
00:19:50,880 --> 00:19:54,874
transmission. But if you look at these

515
00:19:54,912 --> 00:19:57,546
measurements, you see that this is

516
00:19:57,568 --> 00:19:59,690
really covered in noise. So essentially,

517
00:20:00,050 --> 00:20:02,686
if you basically average over this and

518
00:20:02,708 --> 00:20:04,414
zoom this out, you see here these

519
00:20:04,452 --> 00:20:06,366
typical synaptic traces, which is the

520
00:20:06,388 --> 00:20:08,574
average white line here. But below that,

521
00:20:08,612 --> 00:20:12,094
you see this huge jitter. So they really

522
00:20:12,132 --> 00:20:15,186
like go several standard deviations up

523
00:20:15,208 --> 00:20:18,674
and down. And this is actually very

524
00:20:18,712 --> 00:20:21,106
surprising given that neurons are

525
00:20:21,128 --> 00:20:23,494
probably the single most costly cell

526
00:20:23,532 --> 00:20:25,542
type in your body in terms of energy

527
00:20:25,596 --> 00:20:28,646
consumption. They are really in

528
00:20:28,668 --> 00:20:32,326
comparison, they consume quite a bit

529
00:20:32,348 --> 00:20:34,502
of energy in your body. So you would

530
00:20:34,556 --> 00:20:37,962
expect that these transmissions that are

531
00:20:38,096 --> 00:20:40,186
communicated between neurons, which are

532
00:20:40,208 --> 00:20:42,902
very costly, should be highly reliable.

533
00:20:42,966 --> 00:20:44,954
So this is very counterintuitive these

534
00:20:44,992 --> 00:20:47,094
results and has been puzzling

535
00:20:47,142 --> 00:20:49,360
neuroscientists for quite a while now.

536
00:20:51,730 --> 00:20:54,846
And then there

537
00:20:54,868 --> 00:20:58,650
is a second puzzling observation,

538
00:20:58,730 --> 00:21:02,670
which is that the morphology

539
00:21:02,750 --> 00:21:05,186
of neurons looks somewhat like this. So

540
00:21:05,208 --> 00:21:07,438
this would be your typical pyramidal

541
00:21:07,454 --> 00:21:09,410
neuron you have in your cortex.

542
00:21:11,430 --> 00:21:13,678
But you see that this is actually, for a

543
00:21:13,704 --> 00:21:16,562
cell, quite big and elongated.

544
00:21:16,626 --> 00:21:19,286
So this can be up to a millimeter in the

545
00:21:19,308 --> 00:21:22,166
human brain, which means that if a

546
00:21:22,188 --> 00:21:24,966
synapse fires somewhere here, it has a

547
00:21:24,988 --> 00:21:26,746
very hard time communicating with the

548
00:21:26,768 --> 00:21:28,266
cell body, which is down here. So the

549
00:21:28,288 --> 00:21:29,862
electrical signals that are produced

550
00:21:29,926 --> 00:21:33,834
here may travel down here,

551
00:21:33,872 --> 00:21:36,538
but the synapse up here has no way of

552
00:21:36,624 --> 00:21:39,418
measuring the actual voltages at the

553
00:21:39,424 --> 00:21:41,034
cell body. And this is actually the

554
00:21:41,072 --> 00:21:42,478
interesting place because here are the

555
00:21:42,484 --> 00:21:44,670
action potentials formed. So if the

556
00:21:44,740 --> 00:21:47,022
synapse would really like to know about

557
00:21:47,076 --> 00:21:48,766
what is going on in the cell body so

558
00:21:48,788 --> 00:21:50,786
that it can make predictions about how

559
00:21:50,808 --> 00:21:53,186
the neuron will behave and how it

560
00:21:53,208 --> 00:21:55,666
interacts with the world. And this is

561
00:21:55,688 --> 00:21:59,330
another very puzzling or

562
00:21:59,480 --> 00:22:02,194
open problem in neuroscience, how this

563
00:22:02,232 --> 00:22:04,226
communication actually works out in

564
00:22:04,248 --> 00:22:08,898
single neurons between the cell body and

565
00:22:08,984 --> 00:22:11,014
the SynOps. It is known that actually

566
00:22:11,052 --> 00:22:13,574
the action potentials can travel back

567
00:22:13,612 --> 00:22:15,406
up. So they see kind of this binary

568
00:22:15,458 --> 00:22:19,126
variable when the neuron spikes,

569
00:22:19,238 --> 00:22:21,130
but they cannot actually measure the

570
00:22:21,200 --> 00:22:24,346
membrane potential down here. So only

571
00:22:24,368 --> 00:22:26,054
the most prominent electrical signals

572
00:22:26,102 --> 00:22:27,514
can actually back propagate through

573
00:22:27,552 --> 00:22:31,178
this. And this suggests

574
00:22:31,194 --> 00:22:32,574
that the synapse actually has very

575
00:22:32,612 --> 00:22:35,120
sparse information about what's going on

576
00:22:37,250 --> 00:22:40,458
in the cell body. And most

577
00:22:40,644 --> 00:22:42,878
models of synaptic plasticity don't

578
00:22:42,974 --> 00:22:46,770
cover this at all. And we were wondering

579
00:22:49,350 --> 00:22:51,394
how does this interaction work? How can

580
00:22:51,432 --> 00:22:55,406
the synapse produce useful

581
00:22:55,438 --> 00:22:57,086
learning signals given this sparse

582
00:22:57,118 --> 00:22:59,582
information about this important state

583
00:22:59,736 --> 00:23:02,342
of the neuron? And our idea was that

584
00:23:02,396 --> 00:23:03,842
essentially these two observations,

585
00:23:03,906 --> 00:23:05,238
these high levels of noise and the

586
00:23:05,244 --> 00:23:07,514
synapse and this large distance between

587
00:23:07,552 --> 00:23:10,426
cell body and synapses which give these

588
00:23:10,448 --> 00:23:12,234
high uncertainties, these are actually

589
00:23:12,272 --> 00:23:14,714
the two sides of the same coin. So our

590
00:23:14,752 --> 00:23:18,334
hypothesis was that actually we could

591
00:23:18,372 --> 00:23:21,566
use the same models that

592
00:23:21,588 --> 00:23:23,786
we know already from the behavioral

593
00:23:23,818 --> 00:23:27,422
level, how an agent can

594
00:23:27,476 --> 00:23:30,606
act and perform in an environment of

595
00:23:30,628 --> 00:23:33,220
high uncertainty. And we just apply this

596
00:23:35,510 --> 00:23:38,430
to every sign ups and there is an error.

597
00:23:38,510 --> 00:23:42,194
So every sign up should utilize these

598
00:23:42,232 --> 00:23:43,960
same models, basically.

599
00:23:45,370 --> 00:23:48,674
And this model would immediately suggest

600
00:23:48,722 --> 00:23:51,266
that actually synaptic transmissions

601
00:23:51,378 --> 00:23:53,766
should be noisy and these levels of

602
00:23:53,788 --> 00:23:55,654
noise would express uncertainty about

603
00:23:55,692 --> 00:23:59,434
the environment. And then we can use

604
00:23:59,472 --> 00:24:02,058
this model to derive learning rules and

605
00:24:02,064 --> 00:24:03,930
we can compare them side by side to

606
00:24:04,000 --> 00:24:06,666
biology. And this is the first thing I

607
00:24:06,688 --> 00:24:09,214
want to show you. So I just give a very

608
00:24:09,252 --> 00:24:11,230
quick introduction to the free energy

609
00:24:11,300 --> 00:24:14,414
model, because some of you might not be

610
00:24:14,452 --> 00:24:16,270
that familiar with it, but this is

611
00:24:16,340 --> 00:24:19,642
essentially a model to describe

612
00:24:19,706 --> 00:24:21,742
a situation like this. You have a person

613
00:24:21,796 --> 00:24:23,314
that is interacting with some

614
00:24:23,352 --> 00:24:25,074
environment and here I assumed very

615
00:24:25,112 --> 00:24:28,098
simple. So this person tries to throw a

616
00:24:28,104 --> 00:24:31,646
ball to some target. And we as humans,

617
00:24:31,758 --> 00:24:34,226
we are good in solving such tasks. And

618
00:24:34,248 --> 00:24:35,954
we are also good in solving such tasks

619
00:24:36,002 --> 00:24:37,714
if there is high levels of uncertainty

620
00:24:37,762 --> 00:24:41,110
in this. So if the person may receive

621
00:24:41,930 --> 00:24:44,022
some visual feedback, but a lot of this

622
00:24:44,076 --> 00:24:46,486
feedback may be hidden. So you can

623
00:24:46,508 --> 00:24:48,700
imagine this going on behind some wall.

624
00:24:49,310 --> 00:24:53,050
And the person now still might want to

625
00:24:53,120 --> 00:24:55,274
predict what is the trajectory of this

626
00:24:55,312 --> 00:24:56,906
ball flying towards the target so that

627
00:24:56,928 --> 00:25:00,350
it can make an accurate action.

628
00:25:00,690 --> 00:25:03,040
So we will assign some variables to

629
00:25:03,810 --> 00:25:05,326
these states here. So we have

630
00:25:05,348 --> 00:25:06,782
essentially this feedback that this

631
00:25:06,836 --> 00:25:09,214
person can observe. And we have this

632
00:25:09,252 --> 00:25:11,434
unobserved state of the ball flying

633
00:25:11,482 --> 00:25:14,322
here, which we call U, where the person

634
00:25:14,376 --> 00:25:16,306
doesn't have direct access to it, would

635
00:25:16,328 --> 00:25:18,066
only see parts of it, for example, when

636
00:25:18,088 --> 00:25:20,254
the ball is at the corners of the ball

637
00:25:20,302 --> 00:25:24,022
appearing. And then to model

638
00:25:24,076 --> 00:25:26,806
this essentially, or to describe this

639
00:25:26,988 --> 00:25:30,454
behavior, the person would have

640
00:25:30,492 --> 00:25:32,374
an internal description of this

641
00:25:32,412 --> 00:25:35,080
trajectory. So an internal model of this

642
00:25:36,190 --> 00:25:40,794
state, U. And this

643
00:25:40,832 --> 00:25:43,578
model would then be updated to match the

644
00:25:43,664 --> 00:25:45,210
observed feedback.

645
00:25:46,510 --> 00:25:48,766
And this can be described very nicely in

646
00:25:48,788 --> 00:25:50,878
this beautiful mathematical framework of

647
00:25:50,884 --> 00:25:53,182
the free energy principle. So the idea

648
00:25:53,236 --> 00:25:56,186
is that you would essentially establish

649
00:25:56,218 --> 00:25:57,934
a model of your internal state, so

650
00:25:57,972 --> 00:26:01,326
essentially how the internal

651
00:26:01,358 --> 00:26:05,358
states and the state of the environment

652
00:26:05,454 --> 00:26:09,026
interact. And you would have a model of

653
00:26:09,048 --> 00:26:11,682
the feedback, so how the states and the

654
00:26:11,736 --> 00:26:14,294
feedback you observe interact. And

655
00:26:14,332 --> 00:26:18,614
essentially you can then write down a

656
00:26:18,652 --> 00:26:21,602
loss function that measures the distance

657
00:26:21,666 --> 00:26:23,782
between this model of the internal state

658
00:26:23,836 --> 00:26:26,360
and this model of the feedback and the

659
00:26:26,750 --> 00:26:28,060
external state.

660
00:26:30,750 --> 00:26:34,422
And then by essentially

661
00:26:34,486 --> 00:26:36,826
minimizing this distance between the

662
00:26:36,848 --> 00:26:39,054
two, you can derive all sorts of

663
00:26:39,092 --> 00:26:41,070
behaviorally relevant,

664
00:26:42,690 --> 00:26:44,762
can solve behaviorally relevant

665
00:26:44,826 --> 00:26:46,798
problems, for example, learning. But you

666
00:26:46,804 --> 00:26:48,062
can also use this for other things,

667
00:26:48,116 --> 00:26:50,474
like figuring out what are good actions,

668
00:26:50,522 --> 00:26:52,960
for example. So making inference about

669
00:26:53,350 --> 00:26:55,186
both the internal states and the

670
00:26:55,208 --> 00:26:57,586
actions. For example, and this is in a

671
00:26:57,608 --> 00:26:59,246
nutshell, the free energy principle.

672
00:26:59,278 --> 00:27:01,554
And this object here happens to be what

673
00:27:01,592 --> 00:27:03,106
is known as the variation of free

674
00:27:03,128 --> 00:27:05,818
energy, which is also just coincides

675
00:27:05,854 --> 00:27:07,398
with statistical physics. And this is

676
00:27:07,404 --> 00:27:09,400
where this framework has its name from.

677
00:27:09,770 --> 00:27:12,386
But you see that all is probabilistic

678
00:27:12,418 --> 00:27:13,606
here. So essentially you have two

679
00:27:13,628 --> 00:27:15,746
probability functions q for the internal

680
00:27:15,778 --> 00:27:18,700
model and P for this interaction between

681
00:27:19,070 --> 00:27:21,034
states and observations. And you have

682
00:27:21,072 --> 00:27:23,354
here a distance measure between them

683
00:27:23,392 --> 00:27:26,858
that you want to minimize. So now if we

684
00:27:26,864 --> 00:27:28,862
look at the neuron and the synapse and

685
00:27:28,916 --> 00:27:30,398
how they interact with each other, we

686
00:27:30,404 --> 00:27:33,790
find a very similar picture. So single

687
00:27:33,860 --> 00:27:36,526
SynOps, which we have here in green,

688
00:27:36,628 --> 00:27:40,126
has an internal state which we,

689
00:27:40,228 --> 00:27:43,374
for simplicity, model only as the

690
00:27:43,412 --> 00:27:45,694
synaptic weight. Based on this, the

691
00:27:45,732 --> 00:27:47,298
SynOps, when it's triggered by a

692
00:27:47,304 --> 00:27:50,946
presynaptic spike, it would generate a

693
00:27:50,968 --> 00:27:53,234
post synaptic current. This would then

694
00:27:53,272 --> 00:27:55,720
propagate to the soma, which is our

695
00:27:56,170 --> 00:27:58,194
external state, which we cannot directly

696
00:27:58,242 --> 00:28:00,326
observe because it's too far away from

697
00:28:00,348 --> 00:28:03,186
the synapse. But we can see a feedback,

698
00:28:03,218 --> 00:28:04,626
which is this back propagating action

699
00:28:04,658 --> 00:28:06,274
potential, which is this binary variable

700
00:28:06,322 --> 00:28:07,786
that tells us whether the neuron has

701
00:28:07,808 --> 00:28:10,554
spiked or not. So this is exactly the

702
00:28:10,592 --> 00:28:12,954
same framework if we write it down like

703
00:28:12,992 --> 00:28:14,394
that. And we can just use the same

704
00:28:14,432 --> 00:28:16,220
mathematics to solve it.

705
00:28:18,030 --> 00:28:21,406
So to solve it, we only have to come up

706
00:28:21,428 --> 00:28:24,686
with a couple of we have to make a

707
00:28:24,708 --> 00:28:26,494
couple of assumptions. So we have to

708
00:28:26,612 --> 00:28:28,494
write down a model for this guy here.

709
00:28:28,532 --> 00:28:33,250
So this model of how the feedback

710
00:28:34,150 --> 00:28:37,426
and the external state interact. But we

711
00:28:37,448 --> 00:28:39,106
have very good models for this. This has

712
00:28:39,128 --> 00:28:41,874
been studied over many years. So here

713
00:28:41,912 --> 00:28:43,934
you see how typically a model neuron

714
00:28:43,982 --> 00:28:46,538
behaves. So you have here the membrane

715
00:28:46,574 --> 00:28:48,134
potential of a leaky integrated fire

716
00:28:48,172 --> 00:28:50,134
neuron. And you see that this is just

717
00:28:50,252 --> 00:28:52,054
going up and down. So this neuron would

718
00:28:52,092 --> 00:28:55,046
receive a lot of presynaptic input and

719
00:28:55,068 --> 00:28:57,126
maybe also noise. And eventually at some

720
00:28:57,148 --> 00:28:58,506
point it hits a threshold, it would

721
00:28:58,528 --> 00:29:00,266
generate a spike. So that would be the

722
00:29:00,288 --> 00:29:02,326
sets that travels to the downstream

723
00:29:02,358 --> 00:29:04,410
neurons and also back to the SynOps.

724
00:29:05,310 --> 00:29:06,810
And then it resets,

725
00:29:08,290 --> 00:29:09,040
right?

726
00:29:11,890 --> 00:29:13,738
So we can write this down mathematically

727
00:29:13,754 --> 00:29:16,170
as a very simple differential equation.

728
00:29:16,330 --> 00:29:19,166
But the neuron doesn't have access to

729
00:29:19,188 --> 00:29:21,374
this state again. So it's again behind

730
00:29:21,412 --> 00:29:23,466
this wall, it only sees these spike

731
00:29:23,498 --> 00:29:26,066
events. But we can actually, for this

732
00:29:26,088 --> 00:29:27,682
simple case of a leaky integrated fire

733
00:29:27,736 --> 00:29:29,550
neuron, we can solve this analytically.

734
00:29:29,630 --> 00:29:31,186
So we can write down what is the

735
00:29:31,208 --> 00:29:33,106
posterior distribution of membrane

736
00:29:33,138 --> 00:29:35,782
potentials given the spike times. And

737
00:29:35,836 --> 00:29:38,854
what comes out of this is actually a so

738
00:29:38,892 --> 00:29:41,606
called stochastic bridge model, or in

739
00:29:41,628 --> 00:29:45,414
this case of a leak integrated in fire

740
00:29:45,452 --> 00:29:47,798
neuron. It's an orange bridge model. So

741
00:29:47,804 --> 00:29:49,750
this can be written down analytically.

742
00:29:51,310 --> 00:29:53,690
It's not simple, but it's doable.

743
00:29:55,070 --> 00:29:57,382
And we can then just use this directly.

744
00:29:57,446 --> 00:30:00,606
So this model that we have to again

745
00:30:00,708 --> 00:30:02,302
write down these free energy

746
00:30:02,356 --> 00:30:04,046
functionals. So we make here an

747
00:30:04,068 --> 00:30:05,534
assumption how the synapse actually

748
00:30:05,572 --> 00:30:08,446
produces post synoptic currents and how

749
00:30:08,468 --> 00:30:11,058
they are integrated in the neuron. But

750
00:30:11,064 --> 00:30:12,958
that's also given by the leaky

751
00:30:12,974 --> 00:30:14,834
integrated fire neuron and actually the

752
00:30:14,872 --> 00:30:17,886
stochastic inputs that the synapse

753
00:30:17,918 --> 00:30:20,434
generates. So for simplicity, we only

754
00:30:20,472 --> 00:30:23,374
assume here basically Gaussian synapses

755
00:30:23,422 --> 00:30:25,886
that would inject draw a Gaussian random

756
00:30:25,918 --> 00:30:27,678
variable and inject this to a leaky

757
00:30:27,694 --> 00:30:30,246
integrated fire neuron. And then all

758
00:30:30,268 --> 00:30:31,906
these ingredients actually can be solved

759
00:30:31,938 --> 00:30:33,426
in closed form and we can derive

760
00:30:33,458 --> 00:30:35,410
learning rules that would minimize this

761
00:30:35,500 --> 00:30:37,340
free energy functional. Now,

762
00:30:38,590 --> 00:30:40,220
and if we do that,

763
00:30:42,670 --> 00:30:44,790
this has a bunch of nice properties

764
00:30:44,870 --> 00:30:46,906
because this Einstein Beck bridge is

765
00:30:46,928 --> 00:30:50,714
completely determined by the back

766
00:30:50,752 --> 00:30:52,254
propagating action potentials. So

767
00:30:52,292 --> 00:30:55,722
basically the times of the post synaptic

768
00:30:55,866 --> 00:30:58,750
spikes that arrive at the neuron,

769
00:31:00,210 --> 00:31:02,586
this shape that we get here only depends

770
00:31:02,618 --> 00:31:05,010
on two neighboring post synaptic spikes,

771
00:31:06,870 --> 00:31:08,402
which means that we get here

772
00:31:08,456 --> 00:31:11,074
automatically a learning rule that looks

773
00:31:11,112 --> 00:31:13,234
like this. So a learning rule that only

774
00:31:13,272 --> 00:31:15,746
depends on the difference between two

775
00:31:15,768 --> 00:31:17,366
postal optic spikes, which we call here

776
00:31:17,388 --> 00:31:21,254
delta t two, and the

777
00:31:21,372 --> 00:31:23,746
difference between the post synoptic

778
00:31:23,778 --> 00:31:25,546
spike and the actual input that is

779
00:31:25,568 --> 00:31:27,338
triggered at some point on the

780
00:31:27,344 --> 00:31:30,874
presynaptic side. And we can

781
00:31:30,912 --> 00:31:33,734
basically make here this lookup table

782
00:31:33,782 --> 00:31:37,354
and just compute what would be the

783
00:31:37,392 --> 00:31:39,882
update that these sign ups would need to

784
00:31:39,936 --> 00:31:42,426
make so that it learns optimally in

785
00:31:42,448 --> 00:31:44,130
terms of this free energy principle.

786
00:31:44,230 --> 00:31:45,774
And this is the shape that we get out.

787
00:31:45,812 --> 00:31:46,814
You see that there is a strong

788
00:31:46,852 --> 00:31:48,426
dependence on the post synoptic firing

789
00:31:48,458 --> 00:31:50,378
rate, but there is also a dependence on

790
00:31:50,484 --> 00:31:53,406
basically this typical STDP that Sarah

791
00:31:53,438 --> 00:31:56,542
mentioned before, what is the relative

792
00:31:56,606 --> 00:31:58,526
positioning of the pre and postsynoptic

793
00:31:58,558 --> 00:32:02,210
spike. So in a nutshell, this model

794
00:32:02,280 --> 00:32:04,774
can now be split into essentially two

795
00:32:04,812 --> 00:32:08,146
pathways. So we have this ATOC response,

796
00:32:08,258 --> 00:32:10,998
which basically just whenever there is a

797
00:32:11,004 --> 00:32:13,206
presynaptic spike that triggers an

798
00:32:13,228 --> 00:32:15,754
action in the synapse, we would draw

799
00:32:15,792 --> 00:32:17,274
from this Gaussian distribution and

800
00:32:17,312 --> 00:32:20,790
inject it into the neuron.

801
00:32:20,950 --> 00:32:24,902
And then there is this postalk update

802
00:32:25,046 --> 00:32:28,474
where the synapse would look up in this

803
00:32:28,512 --> 00:32:30,126
onstain Woolenbeck bridge, what would

804
00:32:30,148 --> 00:32:33,390
have been the optimal output that it

805
00:32:33,460 --> 00:32:35,406
should have generated, so what would

806
00:32:35,428 --> 00:32:38,014
have been the optimal action. And then

807
00:32:38,052 --> 00:32:39,934
it compares the actual action with this

808
00:32:39,972 --> 00:32:41,566
optimal action according to this free

809
00:32:41,588 --> 00:32:44,446
energy principle and then generates a

810
00:32:44,468 --> 00:32:47,458
delayed response which is an update of

811
00:32:47,464 --> 00:32:50,610
the synaptic weight. Okay?

812
00:32:50,680 --> 00:32:53,206
And importantly, this internal model is

813
00:32:53,228 --> 00:32:55,734
only implicit. It's encoded, so to say,

814
00:32:55,772 --> 00:32:58,626
into this spike time dependent.

815
00:32:58,658 --> 00:33:01,926
Plasticity rule. So how

816
00:33:01,948 --> 00:33:04,726
do these rules then look and how do they

817
00:33:04,748 --> 00:33:07,834
compare to biology? And actually the fit

818
00:33:07,872 --> 00:33:09,642
is quite nicely given that this is

819
00:33:09,696 --> 00:33:11,494
derived really from first principles

820
00:33:11,542 --> 00:33:13,994
without making any assumptions. So this

821
00:33:14,032 --> 00:33:16,426
is the measurement in biology. This is

822
00:33:16,448 --> 00:33:19,578
this B and Al rule, b and Pool,

823
00:33:19,754 --> 00:33:21,822
very old work where they actually did

824
00:33:21,876 --> 00:33:26,606
this in vitro studies where they

825
00:33:26,628 --> 00:33:29,194
injected pre and post synaptic spikes

826
00:33:29,242 --> 00:33:31,006
and then they measured what is the

827
00:33:31,028 --> 00:33:32,638
weight change in the synopsis. And this

828
00:33:32,644 --> 00:33:34,820
is the rule that is predicted by our

829
00:33:35,270 --> 00:33:37,842
model. And you see that at least in a

830
00:33:37,896 --> 00:33:39,634
first order approximation, it gives us

831
00:33:39,672 --> 00:33:42,114
very similar shapes. And this also makes

832
00:33:42,152 --> 00:33:45,654
sense because the

833
00:33:45,692 --> 00:33:48,886
synapse wants to change the most when

834
00:33:48,908 --> 00:33:52,086
it's close to

835
00:33:52,108 --> 00:33:54,278
the pre and post synoptic spike times

836
00:33:54,444 --> 00:33:56,294
because this is where or the post

837
00:33:56,332 --> 00:33:57,978
synoptic spike times because this is

838
00:33:57,984 --> 00:34:00,234
where it knows the most about the state

839
00:34:00,272 --> 00:34:01,914
of the postynaptic neuron. The free

840
00:34:01,952 --> 00:34:03,850
energy principle would actually suggest

841
00:34:04,670 --> 00:34:10,142
these kind of cones with

842
00:34:10,276 --> 00:34:13,246
almost no assumption essentially. But we

843
00:34:13,268 --> 00:34:16,366
also get because we have not just the

844
00:34:16,388 --> 00:34:19,066
first order spike time dependent

845
00:34:19,098 --> 00:34:21,006
plasticity rule, but we also have this

846
00:34:21,028 --> 00:34:22,474
dependency on the post not of the firing

847
00:34:22,522 --> 00:34:24,930
rate. We can also compare this to other

848
00:34:25,000 --> 00:34:27,170
results. And this here is this old work

849
00:34:27,240 --> 00:34:29,186
by Kaupner and Brunel. So this is

850
00:34:29,208 --> 00:34:31,154
actually a model, but that was very

851
00:34:31,192 --> 00:34:35,538
detailed describing the

852
00:34:35,704 --> 00:34:37,846
plasticity based on the pre and post

853
00:34:37,868 --> 00:34:41,138
synoptic firing rate in the synapse.

854
00:34:41,234 --> 00:34:42,966
And this is what our model predicts. So

855
00:34:42,988 --> 00:34:44,614
if we inject render and pre and post

856
00:34:44,652 --> 00:34:47,722
synaptic bossBack trains with different

857
00:34:47,776 --> 00:34:50,314
rates, our model would predict this

858
00:34:50,352 --> 00:34:52,602
shape, which again is not a perfect

859
00:34:52,656 --> 00:34:55,434
match. But given that this is a very

860
00:34:55,472 --> 00:34:59,274
idealistic model, it's actually the main

861
00:34:59,312 --> 00:35:02,538
features that low firing rates on

862
00:35:02,544 --> 00:35:04,446
the postynopstic side would lead to

863
00:35:04,468 --> 00:35:07,230
depression and higher to potentiation

864
00:35:07,650 --> 00:35:09,280
are reflected in this.

865
00:35:11,190 --> 00:35:14,658
Okay, I assume I still have ten minutes,

866
00:35:14,744 --> 00:35:19,394
right? Okay, so I would give

867
00:35:19,432 --> 00:35:21,506
a quick intermediate summary and then I

868
00:35:21,528 --> 00:35:23,506
want to show some other work where we

869
00:35:23,528 --> 00:35:26,994
actually apply this now to

870
00:35:27,032 --> 00:35:28,660
actual machine learning model.

871
00:35:31,030 --> 00:35:33,060
So what we have seen here is that

872
00:35:33,430 --> 00:35:37,034
synapses are actually are very

873
00:35:37,072 --> 00:35:40,314
stochastic and this was a

874
00:35:40,352 --> 00:35:43,340
big puzzle. And B suggests that actually

875
00:35:46,190 --> 00:35:48,686
the synaptic noise is actually the

876
00:35:48,708 --> 00:35:51,774
synapses way of reporting its own

877
00:35:51,812 --> 00:35:53,066
uncertainty about the environment,

878
00:35:53,098 --> 00:35:56,526
where the environment is actually the

879
00:35:56,548 --> 00:35:58,622
post synaptic neuron and it really

880
00:35:58,676 --> 00:36:00,654
interacts in this fashion of the free

881
00:36:00,692 --> 00:36:02,846
energy principle with this post synaptic

882
00:36:02,878 --> 00:36:04,466
neuron or that's a very nice way of

883
00:36:04,488 --> 00:36:07,506
describing it. And if you're interested

884
00:36:07,608 --> 00:36:10,740
more, there's a paper, a preprint out,

885
00:36:11,190 --> 00:36:13,060
you can read up on all this.

886
00:36:14,250 --> 00:36:17,046
Okay, so how does this now connect to

887
00:36:17,068 --> 00:36:19,846
neuromorphics? And actually so we are

888
00:36:19,868 --> 00:36:21,218
not actually doing neuromorphic

889
00:36:21,234 --> 00:36:23,378
hardware, we are doing neuromorphic

890
00:36:23,394 --> 00:36:25,986
algorithms. So we try to bring these

891
00:36:26,028 --> 00:36:27,606
inspirations now into actual machine

892
00:36:27,638 --> 00:36:30,394
learning models and we thought that this

893
00:36:30,432 --> 00:36:33,706
might be a good attack angle to

894
00:36:33,888 --> 00:36:35,706
solve a problem that is well known in

895
00:36:35,728 --> 00:36:41,306
machine learning. So I just drew

896
00:36:41,338 --> 00:36:43,978
here very simple convolutional neural

897
00:36:43,994 --> 00:36:45,546
networks with various convolutional

898
00:36:45,578 --> 00:36:48,010
layers and then maybe some dense layers

899
00:36:48,090 --> 00:36:49,946
that you would have in your machine

900
00:36:49,978 --> 00:36:52,890
learning algorithm. And the way this is

901
00:36:52,900 --> 00:36:54,482
trained, as many of you know, I guess,

902
00:36:54,536 --> 00:36:57,454
is through end to end errorback

903
00:36:57,502 --> 00:36:59,426
propagation. So the idea is that you

904
00:36:59,448 --> 00:37:02,002
have a training set which has inputs and

905
00:37:02,136 --> 00:37:04,246
targets. So, for example, in a

906
00:37:04,268 --> 00:37:06,694
classification task, this could be

907
00:37:06,892 --> 00:37:08,806
pictures of cats and dogs. And you would

908
00:37:08,828 --> 00:37:12,354
have targets which are class labels,

909
00:37:12,402 --> 00:37:15,142
so to say. So there's actually

910
00:37:15,196 --> 00:37:17,318
artificial neurons in there, and one of

911
00:37:17,324 --> 00:37:18,938
these neuron may be active for cats and

912
00:37:18,944 --> 00:37:22,106
1 may be active for dogs. And in your

913
00:37:22,128 --> 00:37:24,170
training, that data, you have exactly

914
00:37:24,240 --> 00:37:26,394
these labels that were generated by

915
00:37:26,432 --> 00:37:28,026
humans, that were sitting down doing

916
00:37:28,048 --> 00:37:30,350
this by hand. And then during training,

917
00:37:30,500 --> 00:37:33,258
you show these examples to the network

918
00:37:33,434 --> 00:37:36,302
by propagating these inputs all the way

919
00:37:36,356 --> 00:37:38,266
from the input layer to the output

920
00:37:38,298 --> 00:37:41,390
layer. Then the output is here compared

921
00:37:42,290 --> 00:37:44,802
to these hand labeled targets. And then

922
00:37:44,856 --> 00:37:46,994
the mismatch between the two is back

923
00:37:47,032 --> 00:37:48,962
propagated through all these layers back

924
00:37:49,016 --> 00:37:52,066
to the input. And all the weights or the

925
00:37:52,088 --> 00:37:54,566
synoptic weights that are here in

926
00:37:54,588 --> 00:37:57,606
between inside these layers would then

927
00:37:57,628 --> 00:37:59,494
be updated accordingly, so that after

928
00:37:59,532 --> 00:38:01,766
doing this many, many times, this

929
00:38:01,788 --> 00:38:04,194
network becomes good in telling apart

930
00:38:04,242 --> 00:38:07,994
Kelvin docs. So this has a problem,

931
00:38:08,192 --> 00:38:09,866
this algorithm. It works great in

932
00:38:09,888 --> 00:38:12,826
practice and it's the foundation of all

933
00:38:12,848 --> 00:38:14,426
these models we have talked about, like

934
00:38:14,448 --> 00:38:17,562
Dali or Jet GPT, but it is quite

935
00:38:17,616 --> 00:38:19,878
inefficient. And the problem is what is

936
00:38:19,904 --> 00:38:21,562
known in the literature as the locking

937
00:38:21,626 --> 00:38:24,174
problem. So if you would split up this

938
00:38:24,212 --> 00:38:26,782
network now into blocks, which I already

939
00:38:26,836 --> 00:38:29,614
did before, but this is arbitrary, but

940
00:38:29,732 --> 00:38:32,318
for implementing this efficiently in

941
00:38:32,324 --> 00:38:33,906
terms of a software algorithm, it might

942
00:38:33,928 --> 00:38:36,386
be interesting to do that. And now you

943
00:38:36,408 --> 00:38:38,994
would want these blocks ideally to run

944
00:38:39,032 --> 00:38:40,980
in parallel, so that you can basically

945
00:38:41,590 --> 00:38:44,674
show the first example on this first

946
00:38:44,712 --> 00:38:46,694
block and then already train it while

947
00:38:46,732 --> 00:38:49,222
the second block is doing something

948
00:38:49,276 --> 00:38:51,206
else. But this is not really possible

949
00:38:51,308 --> 00:38:53,734
with end to end back propagation because

950
00:38:53,772 --> 00:38:56,022
of this lock in problem, because the

951
00:38:56,076 --> 00:38:58,258
activation of the second block depends

952
00:38:58,274 --> 00:38:59,734
on the activation of the first block.

953
00:38:59,782 --> 00:39:01,674
So you have to propagate it all the way

954
00:39:01,712 --> 00:39:03,846
to the end. Then you compute this error

955
00:39:03,878 --> 00:39:06,154
and you would then back propagate. And

956
00:39:06,192 --> 00:39:08,734
only when this is done, you can start

957
00:39:08,772 --> 00:39:11,262
the next epoch where you show a new

958
00:39:11,316 --> 00:39:15,726
bunch of examples and

959
00:39:15,748 --> 00:39:17,582
you see that during all this time,

960
00:39:17,636 --> 00:39:20,926
here, the thread that would run this

961
00:39:20,948 --> 00:39:24,066
first block maybe would be idle and has

962
00:39:24,088 --> 00:39:26,994
to wait essentially all the time. And

963
00:39:27,032 --> 00:39:28,146
this obviously makes them very

964
00:39:28,168 --> 00:39:30,178
inefficient. And now our idea was that

965
00:39:30,184 --> 00:39:33,086
we use basically what we had learned

966
00:39:33,118 --> 00:39:37,810
from this earlier model on how synopsis

967
00:39:37,970 --> 00:39:40,114
communicate over these long distances

968
00:39:40,162 --> 00:39:42,374
through this free energy principle, and

969
00:39:42,412 --> 00:39:44,018
also apply it just to a deep neural

970
00:39:44,034 --> 00:39:45,882
network. And the idea is that you have

971
00:39:45,936 --> 00:39:48,940
here, again, basically you have already

972
00:39:51,790 --> 00:39:54,842
this generation of inputs to some

973
00:39:54,896 --> 00:39:58,202
output. But what is missing to make it

974
00:39:58,256 --> 00:39:59,834
applicable to the free energy principle

975
00:39:59,882 --> 00:40:02,254
is this feedback that you always need.

976
00:40:02,372 --> 00:40:04,494
The idea was that we put here a very

977
00:40:04,532 --> 00:40:08,254
lightweight feedback network. So

978
00:40:08,292 --> 00:40:10,366
essentially each of these blocks now in

979
00:40:10,388 --> 00:40:13,386
this deep neural network would be

980
00:40:13,428 --> 00:40:16,674
accompanied by a feedback block that

981
00:40:16,712 --> 00:40:19,154
locally generates a target. So we used

982
00:40:19,192 --> 00:40:20,786
here really in the simplest case, which

983
00:40:20,808 --> 00:40:22,354
we have, so this is very recent work,

984
00:40:22,392 --> 00:40:25,266
we only used linear blocks so far. So

985
00:40:25,288 --> 00:40:27,826
these are single linear layers and we

986
00:40:27,848 --> 00:40:30,178
would generate now these outputs here in

987
00:40:30,184 --> 00:40:33,026
these feedback blocks and then use the

988
00:40:33,048 --> 00:40:36,314
free energy principle to derive a

989
00:40:36,512 --> 00:40:38,874
local loss. That allows us again to

990
00:40:38,912 --> 00:40:41,846
minimize both these feedback weights

991
00:40:41,878 --> 00:40:43,798
that we have here and also the weights

992
00:40:43,814 --> 00:40:47,206
in the forward network. So it's

993
00:40:47,238 --> 00:40:49,066
essentially the same idea. So we have

994
00:40:49,088 --> 00:40:50,634
here these outputs which we interpret

995
00:40:50,682 --> 00:40:53,530
now as parameters to a probability

996
00:40:53,610 --> 00:40:55,054
function. So we can apply this

997
00:40:55,092 --> 00:40:58,526
probabilistic framework, but now all the

998
00:40:58,548 --> 00:41:00,346
rest basically rolls out the same way.

999
00:41:00,388 --> 00:41:02,466
So we assume that these outputs are

1000
00:41:02,488 --> 00:41:05,554
essentially the internal states of this

1001
00:41:05,592 --> 00:41:09,394
model and we have these

1002
00:41:09,432 --> 00:41:12,966
observations given in the inputs and the

1003
00:41:12,988 --> 00:41:15,618
targets. And now we try to minimize

1004
00:41:15,714 --> 00:41:18,946
basically P would now be this feed

1005
00:41:18,978 --> 00:41:20,966
forward network and Q would be now a

1006
00:41:20,988 --> 00:41:25,686
function that contains features

1007
00:41:25,718 --> 00:41:29,882
of both the feedback and

1008
00:41:29,936 --> 00:41:32,634
the feed forward network. And the nice

1009
00:41:32,672 --> 00:41:35,626
thing is that if we I don't have time to

1010
00:41:35,648 --> 00:41:38,318
go into the details now, but if you

1011
00:41:38,404 --> 00:41:41,742
write this out, you see that this

1012
00:41:41,796 --> 00:41:45,166
actually decomposes, this lock term here

1013
00:41:45,188 --> 00:41:49,262
decomposes into local linear terms that

1014
00:41:49,316 --> 00:41:51,026
give you these local losses here. So

1015
00:41:51,048 --> 00:41:52,994
essentially that you can minimize here

1016
00:41:53,032 --> 00:41:55,586
block local between forward and the

1017
00:41:55,608 --> 00:41:59,582
corresponding feedback block, a loss

1018
00:41:59,646 --> 00:42:01,906
function. And you can then actually do

1019
00:42:01,928 --> 00:42:05,714
this in parallel because maybe the

1020
00:42:05,752 --> 00:42:07,878
picture is good to see here. So what you

1021
00:42:07,884 --> 00:42:09,126
have to do now you have a bit of

1022
00:42:09,148 --> 00:42:10,722
overhead because you have this feedback

1023
00:42:10,786 --> 00:42:12,680
block. So this would be the two

1024
00:42:15,230 --> 00:42:18,330
execution times of the feed forward

1025
00:42:18,400 --> 00:42:21,546
block and the feedback block. But in

1026
00:42:21,568 --> 00:42:24,714
principle they can run in parallel and

1027
00:42:24,752 --> 00:42:27,162
once the forward block is done, the next

1028
00:42:27,216 --> 00:42:28,954
forward block can start propagating

1029
00:42:29,002 --> 00:42:32,794
through this network, but simultaneously

1030
00:42:32,842 --> 00:42:34,414
already the forward block, because it

1031
00:42:34,452 --> 00:42:36,942
already received a target here, can

1032
00:42:36,996 --> 00:42:40,378
start updating the weights and when it's

1033
00:42:40,394 --> 00:42:42,434
done, it's free to operate on the next

1034
00:42:42,472 --> 00:42:45,986
epoch. So there is no locking anymore in

1035
00:42:46,008 --> 00:42:50,014
this framework. Okay, so I'm

1036
00:42:50,062 --> 00:42:51,874
pretty much done. I'm also out of time,

1037
00:42:51,912 --> 00:42:54,566
I think. So how does this perform? Of

1038
00:42:54,588 --> 00:42:55,782
course we changed the learning

1039
00:42:55,836 --> 00:42:57,414
algorithm. Now we have to also go back

1040
00:42:57,452 --> 00:43:01,542
and see if this is still giving

1041
00:43:01,596 --> 00:43:03,030
us the same performance.

1042
00:43:05,470 --> 00:43:09,094
As I said, this is the first results

1043
00:43:09,142 --> 00:43:13,562
we have here now and at least to mid

1044
00:43:13,616 --> 00:43:17,014
scale data sets like cipher

1045
00:43:17,062 --> 00:43:19,470
ten or so, this seems to actually

1046
00:43:19,620 --> 00:43:21,758
perform very well. So we attract it for

1047
00:43:21,844 --> 00:43:23,946
standard architectures, fashion MNIST

1048
00:43:23,978 --> 00:43:27,870
with ResNet 15 and ResNet 18. We worked

1049
00:43:27,940 --> 00:43:31,966
mostly so far for small data sets like

1050
00:43:31,988 --> 00:43:35,034
Fashion MNIST with applying free splits

1051
00:43:35,082 --> 00:43:37,634
to ResNet 50. We get basically the same

1052
00:43:37,672 --> 00:43:40,114
performance as standard backprop. As

1053
00:43:40,152 --> 00:43:44,434
networks get deeper, you see that our

1054
00:43:44,472 --> 00:43:45,782
problem that we have now is actually

1055
00:43:45,836 --> 00:43:48,102
overfitting because we have these local

1056
00:43:48,156 --> 00:43:49,618
targets. It seems that these smaller

1057
00:43:49,634 --> 00:43:52,050
blocks actually overfit to some extent.

1058
00:43:52,210 --> 00:43:55,046
This is not so severe for still up to

1059
00:43:55,068 --> 00:43:57,562
tasks like cipher ten. So we get quite

1060
00:43:57,616 --> 00:43:59,594
close already. But if you go now for

1061
00:43:59,632 --> 00:44:02,154
really large tasks, there's still

1062
00:44:02,192 --> 00:44:06,154
something missing for like

1063
00:44:06,192 --> 00:44:07,402
single splits. We are getting there,

1064
00:44:07,456 --> 00:44:09,566
but we are not reaching all the way up

1065
00:44:09,588 --> 00:44:11,854
to back propagation. But it's still

1066
00:44:11,892 --> 00:44:13,760
interesting to see that you can apply

1067
00:44:15,330 --> 00:44:19,182
this principle also to these standard

1068
00:44:19,236 --> 00:44:22,546
machine learning algorithms. Okay, this

1069
00:44:22,568 --> 00:44:25,394
is my second summary. So actually we

1070
00:44:25,432 --> 00:44:27,298
found that deep neural networks are

1071
00:44:27,304 --> 00:44:30,034
surprisingly good in generalizing over

1072
00:44:30,152 --> 00:44:32,482
probability spaces. This is how actually

1073
00:44:32,536 --> 00:44:36,294
this work started. And our

1074
00:44:36,332 --> 00:44:39,074
idea was to exploit this and to utilize

1075
00:44:39,122 --> 00:44:41,734
it to distribute learning in the same

1076
00:44:41,772 --> 00:44:44,418
fashion as in the first project I showed

1077
00:44:44,434 --> 00:44:48,534
you and to solve this credit assignment

1078
00:44:48,582 --> 00:44:50,806
problem by generating these feedback

1079
00:44:50,838 --> 00:44:51,770
networks.

1080
00:44:53,950 --> 00:44:57,386
Yeah, that's basically it. And then I

1081
00:44:57,408 --> 00:44:59,774
want to acknowledge my coworkers and my

1082
00:44:59,812 --> 00:45:03,754
students. So I have two very good PhD

1083
00:45:03,802 --> 00:45:06,846
students, carlisle and Cabrill, who is

1084
00:45:06,868 --> 00:45:09,934
now here in Bohom and works on this

1085
00:45:09,972 --> 00:45:12,846
topic. And the first project I showed

1086
00:45:12,878 --> 00:45:15,778
was work I did together when I was in

1087
00:45:15,864 --> 00:45:18,340
gettingham with Christian Tetlav. And

1088
00:45:19,590 --> 00:45:22,606
the second project is I worked closely

1089
00:45:22,638 --> 00:45:25,170
with Christian Meyer and Anand Sutomoni

1090
00:45:26,410 --> 00:45:28,440
and yeah, thank you.

1091
00:45:31,370 --> 00:45:32,966
Thank you very much, David. That was

1092
00:45:32,988 --> 00:45:35,138
absolutely fascinating. I think it's

1093
00:45:35,154 --> 00:45:37,558
incredible how closely the sort of model

1094
00:45:37,644 --> 00:45:39,654
matched biology. Like considering you

1095
00:45:39,692 --> 00:45:41,338
derived it from first principles, I

1096
00:45:41,344 --> 00:45:43,866
think that was really cool. I did have a

1097
00:45:43,888 --> 00:45:46,026
question just about the last sort of the

1098
00:45:46,048 --> 00:45:48,550
bit. You spoke about the Convolutional

1099
00:45:48,710 --> 00:45:50,538
network and you said traditionally it

1100
00:45:50,544 --> 00:45:51,838
has to go all the way end to end, which

1101
00:45:51,844 --> 00:45:53,886
is really inefficient. And then you

1102
00:45:53,908 --> 00:45:55,518
showed the results that you got. Did you

1103
00:45:55,524 --> 00:45:58,686
look at energy consumption with yours as

1104
00:45:58,708 --> 00:46:02,114
well? Not yet. So we are actually

1105
00:46:02,152 --> 00:46:06,494
currently working on it's

1106
00:46:06,542 --> 00:46:08,434
actually not so easy to implement these

1107
00:46:08,472 --> 00:46:10,690
things in standard machine learning

1108
00:46:10,840 --> 00:46:14,274
toolboxes. So Carlisle is currently

1109
00:46:14,392 --> 00:46:17,506
looking into this, the PhD student in

1110
00:46:17,528 --> 00:46:20,054
Jester, he has an implementation now,

1111
00:46:20,092 --> 00:46:24,150
and he's now evaluating how well

1112
00:46:24,220 --> 00:46:26,430
we can make use of this parallelization

1113
00:46:26,530 --> 00:46:28,874
in practice. But we are actually quite

1114
00:46:28,912 --> 00:46:34,010
confident that for

1115
00:46:34,080 --> 00:46:37,498
parallelizing it, it should be there.

1116
00:46:37,664 --> 00:46:39,498
The question is how much you save in

1117
00:46:39,504 --> 00:46:41,790
terms of energy. Because for these

1118
00:46:41,860 --> 00:46:43,694
smaller scale models that we use now,

1119
00:46:43,732 --> 00:46:45,600
ResNet 18, ResNet 50,

1120
00:46:47,010 --> 00:46:49,438
the effect might be not that huge. So

1121
00:46:49,524 --> 00:46:51,466
once we ramp this up to really larger

1122
00:46:51,498 --> 00:46:53,774
models, the effect should be bigger.

1123
00:46:53,822 --> 00:46:56,754
But yeah, this is ongoing work. Very

1124
00:46:56,792 --> 00:46:59,154
cool. Thank you. And then I was just

1125
00:46:59,192 --> 00:47:01,842
wondering as well, this local error back

1126
00:47:01,896 --> 00:47:03,634
propagation, is that something that

1127
00:47:03,672 --> 00:47:05,346
other people have tried with these

1128
00:47:05,368 --> 00:47:07,046
convolutional neural networks or is this

1129
00:47:07,068 --> 00:47:10,120
quite a new way of implementing it?

1130
00:47:10,650 --> 00:47:12,614
There is a bunch of approaches that do

1131
00:47:12,652 --> 00:47:15,926
this. For example, I mean, the closest I

1132
00:47:15,948 --> 00:47:17,866
guess is target propagation which has

1133
00:47:17,888 --> 00:47:20,102
been proposed, which essentially uses

1134
00:47:20,166 --> 00:47:22,794
random feedback weights to back

1135
00:47:22,832 --> 00:47:25,386
propagate here. So these guys would not

1136
00:47:25,408 --> 00:47:26,330
be trained.

1137
00:47:28,830 --> 00:47:32,734
And this

1138
00:47:32,772 --> 00:47:34,990
works nicely also for small scale

1139
00:47:36,610 --> 00:47:38,206
problems. But as far as I know, they

1140
00:47:38,228 --> 00:47:40,746
don't perform that well. Even for cipher

1141
00:47:40,778 --> 00:47:42,926
ten, it already starts breaking down

1142
00:47:42,948 --> 00:47:44,318
because these random feedback weights

1143
00:47:44,334 --> 00:47:47,026
are just too coarse an approximation, I

1144
00:47:47,048 --> 00:47:51,282
think. And this is the first okay,

1145
00:47:51,416 --> 00:47:53,266
maybe I have to be careful. I think this

1146
00:47:53,288 --> 00:47:56,806
is the first method that allows you to

1147
00:47:56,828 --> 00:47:59,046
train these feedback weights. That is

1148
00:47:59,068 --> 00:48:01,846
not a contrastive method. There is a

1149
00:48:01,868 --> 00:48:03,714
bunch of methods that use a contrastive

1150
00:48:03,762 --> 00:48:07,066
step. So you have

1151
00:48:07,088 --> 00:48:08,490
maybe seen this forward forward

1152
00:48:08,560 --> 00:48:11,322
algorithm and all these things,

1153
00:48:11,456 --> 00:48:17,594
but what they have to do always is they

1154
00:48:17,632 --> 00:48:19,706
send in the actual input data and then

1155
00:48:19,728 --> 00:48:22,170
they send an kind of anti input,

1156
00:48:23,490 --> 00:48:26,958
an anti input that is usually

1157
00:48:27,044 --> 00:48:29,566
generated artificially. So they do some

1158
00:48:29,588 --> 00:48:32,226
distortion to the input to make it and

1159
00:48:32,248 --> 00:48:35,186
then they have to use both information

1160
00:48:35,368 --> 00:48:38,254
locally to do the update. So the network

1161
00:48:38,302 --> 00:48:41,154
has to keep in memory the input and the

1162
00:48:41,192 --> 00:48:46,374
anti input and the responses. And this

1163
00:48:46,412 --> 00:48:49,334
makes these approaches a bit harder to

1164
00:48:49,372 --> 00:48:51,574
parallelize in this. And the nice thing

1165
00:48:51,612 --> 00:48:54,726
here is that we derive an

1166
00:48:54,748 --> 00:48:57,754
upper bound to this variational free

1167
00:48:57,792 --> 00:49:01,034
energy loss that can be spelled out

1168
00:49:01,072 --> 00:49:03,706
completely by forward propagation. And

1169
00:49:03,728 --> 00:49:06,234
that I think is new. So that is the new

1170
00:49:06,272 --> 00:49:09,180
bit of this. Very cool.

1171
00:49:12,530 --> 00:49:14,160
That's awesome.

1172
00:49:16,530 --> 00:49:20,350
Well, yeah, few comments, one piece that

1173
00:49:20,420 --> 00:49:22,842
kind of between the two of your talks.

1174
00:49:22,906 --> 00:49:25,026
That was at least a new distinction to

1175
00:49:25,048 --> 00:49:26,706
me, which was the difference between the

1176
00:49:26,728 --> 00:49:28,146
Neuromorphic hardware and the

1177
00:49:28,168 --> 00:49:31,682
Neuromorphic algorithms. So it's not

1178
00:49:31,736 --> 00:49:35,250
just about new hardware or Wetware,

1179
00:49:36,490 --> 00:49:39,702
though that would be great to see.

1180
00:49:39,756 --> 00:49:40,854
It's almost like there's this

1181
00:49:40,892 --> 00:49:45,318
intermediate or a bridge step with using

1182
00:49:45,484 --> 00:49:48,954
the algorithms on

1183
00:49:48,992 --> 00:49:52,922
the hardware we have today that

1184
00:49:53,056 --> 00:49:54,966
like Sarah mentioned, the Spiking neural

1185
00:49:54,998 --> 00:49:58,314
networks which are amenable to GPUs or

1186
00:49:58,432 --> 00:50:03,066
just using standardized CPU multicore

1187
00:50:03,178 --> 00:50:05,966
scheduling approaches. You can already

1188
00:50:06,068 --> 00:50:09,920
do more with what we have

1189
00:50:10,770 --> 00:50:13,826
using the Neuromorphic algorithms. So

1190
00:50:13,848 --> 00:50:17,810
it's not just a material science

1191
00:50:18,150 --> 00:50:21,378
topic, but also there's a lot

1192
00:50:21,544 --> 00:50:23,826
at the really micro scale that we can

1193
00:50:23,848 --> 00:50:26,082
learn related to noise processing

1194
00:50:26,146 --> 00:50:28,054
scheduling and then also even at higher

1195
00:50:28,092 --> 00:50:30,230
levels of abstraction, probably learning

1196
00:50:30,300 --> 00:50:32,578
from biomimicry and cognitive systems

1197
00:50:32,594 --> 00:50:34,854
more generally. But that was a

1198
00:50:34,892 --> 00:50:36,280
distinction for me.

1199
00:50:38,110 --> 00:50:41,114
Yeah. So maybe to add yeah. So I think

1200
00:50:41,152 --> 00:50:44,986
that the problem becomes really now more

1201
00:50:45,008 --> 00:50:48,310
pressing as these Neuromorphic devices

1202
00:50:48,390 --> 00:50:50,266
become so the hardware devices become

1203
00:50:50,368 --> 00:50:53,626
more mature and they usually cannot

1204
00:50:53,658 --> 00:50:57,406
really shine on these standard

1205
00:50:57,508 --> 00:50:59,198
machine learning algorithms because they

1206
00:50:59,204 --> 00:51:02,686
are really optimized for GPUs. So you

1207
00:51:02,708 --> 00:51:04,866
need to think a little bit. So you have

1208
00:51:04,888 --> 00:51:06,642
to take one step back and think again

1209
00:51:06,696 --> 00:51:10,562
about the algorithmic side to really

1210
00:51:10,616 --> 00:51:13,746
use them to full capacity. And as you

1211
00:51:13,768 --> 00:51:15,460
have seen now, we collaborate with

1212
00:51:15,990 --> 00:51:18,038
Professor Meyer, who is doing this

1213
00:51:18,124 --> 00:51:19,746
spinnaker chip in Dresden, but there's

1214
00:51:19,778 --> 00:51:22,146
also other approaches like the Louis

1215
00:51:22,178 --> 00:51:24,166
chip that Sarah mentioned from intel and

1216
00:51:24,188 --> 00:51:27,302
so on, and they are really looking

1217
00:51:27,356 --> 00:51:29,846
into this. Now, also from the

1218
00:51:29,868 --> 00:51:33,514
algorithmic side. I find

1219
00:51:33,552 --> 00:51:34,986
it really useful. It's kind of like a

1220
00:51:35,008 --> 00:51:36,918
mindset or a mental framework when I'm

1221
00:51:36,934 --> 00:51:39,258
thinking about computers or AI, and this

1222
00:51:39,264 --> 00:51:39,898
is probably because I'm a

1223
00:51:39,904 --> 00:51:41,066
neuroscientist, but I also have to

1224
00:51:41,088 --> 00:51:42,906
translate it to, well, how does the

1225
00:51:42,928 --> 00:51:44,250
brain work, how does the information

1226
00:51:44,320 --> 00:51:45,438
processing work, et cetera, in the

1227
00:51:45,444 --> 00:51:47,566
brain? And when I came to sort of

1228
00:51:47,588 --> 00:51:48,814
computer science and AI after

1229
00:51:48,852 --> 00:51:50,314
neuroscience, I found myself naturally

1230
00:51:50,362 --> 00:51:51,966
translating it. But I feel like the

1231
00:51:51,988 --> 00:51:54,174
framework is just a really useful way of

1232
00:51:54,212 --> 00:51:56,158
understanding computation at the end of

1233
00:51:56,164 --> 00:51:57,394
the day because our brains, as I said,

1234
00:51:57,432 --> 00:51:59,246
are just these massive supercomputers.

1235
00:51:59,278 --> 00:52:01,186
And I'm constantly reading papers on

1236
00:52:01,208 --> 00:52:02,930
computer science or whatever. And then

1237
00:52:03,000 --> 00:52:05,010
once you can conceptualize anything

1238
00:52:05,080 --> 00:52:08,674
really as well, how neuromorphic is

1239
00:52:08,712 --> 00:52:10,434
this? And then if you start thinking

1240
00:52:10,472 --> 00:52:12,006
about, well, how could you tweak it so

1241
00:52:12,028 --> 00:52:13,606
it's slightly more neuromorphic? And is

1242
00:52:13,628 --> 00:52:14,978
that then going to give you these gains

1243
00:52:14,994 --> 00:52:16,838
that we get with the brain? Is it going

1244
00:52:16,844 --> 00:52:18,578
to give you some extra parallel

1245
00:52:18,674 --> 00:52:20,054
computation or is it going to give you

1246
00:52:20,092 --> 00:52:22,546
some energy efficiency? So, yeah, I find

1247
00:52:22,588 --> 00:52:24,698
it the definitions when I was looking at

1248
00:52:24,704 --> 00:52:26,106
the definition is I think it really

1249
00:52:26,128 --> 00:52:27,594
depends who writes the definition. And

1250
00:52:27,632 --> 00:52:30,298
because it's such a dynamic area at the

1251
00:52:30,304 --> 00:52:33,646
moment as well, I think it has been

1252
00:52:33,668 --> 00:52:34,846
changing and it will be changing, but

1253
00:52:34,868 --> 00:52:36,878
for me, I feel like neuromorphic is more

1254
00:52:36,884 --> 00:52:39,166
of like a mental framework where I look

1255
00:52:39,188 --> 00:52:41,194
at things through conceptualize,

1256
00:52:41,322 --> 00:52:44,386
through yeah, I think it's. It'S also

1257
00:52:44,408 --> 00:52:45,810
not very well defined.

1258
00:52:47,830 --> 00:52:50,066
You also mentioned that actually

1259
00:52:50,248 --> 00:52:52,306
artificial neural networks are a

1260
00:52:52,328 --> 00:52:54,994
neuromorphic concept, if you want, and

1261
00:52:55,032 --> 00:52:58,898
they were from the first day. And it's

1262
00:52:58,914 --> 00:53:00,614
a big success story, right, if you look

1263
00:53:00,652 --> 00:53:02,790
into the 90s or so when these support

1264
00:53:02,860 --> 00:53:05,938
vector machines and these alternative

1265
00:53:06,034 --> 00:53:09,782
models came up, but none of them have

1266
00:53:09,836 --> 00:53:12,374
outlived the neuromorphic approaches.

1267
00:53:12,422 --> 00:53:14,634
So it's actually very nice. But still

1268
00:53:14,672 --> 00:53:17,146
there is this community that thinks that

1269
00:53:17,168 --> 00:53:19,206
there is more features from the brain

1270
00:53:19,238 --> 00:53:22,666
that you need to put in to get to

1271
00:53:22,688 --> 00:53:26,014
the real thing. So I think this is a bit

1272
00:53:26,052 --> 00:53:29,710
of a it's not a very well defined term,

1273
00:53:30,130 --> 00:53:32,574
actually, and I think with. Your

1274
00:53:32,612 --> 00:53:34,158
research, yours is almost like the

1275
00:53:34,164 --> 00:53:35,774
smallest level that I've seen people

1276
00:53:35,812 --> 00:53:37,278
look at it on. I don't know if you've

1277
00:53:37,294 --> 00:53:38,706
seen anything else, but we're not just

1278
00:53:38,728 --> 00:53:40,866
talking about a cell level of free

1279
00:53:40,888 --> 00:53:41,986
energy and active inference. We're

1280
00:53:42,008 --> 00:53:45,810
actually talking about a cell structure.

1281
00:53:46,310 --> 00:53:47,826
So then you think, well, how small does

1282
00:53:47,848 --> 00:53:49,858
it go? Are we going to talk about cell

1283
00:53:49,944 --> 00:53:52,086
subcellular structures eventually, like

1284
00:53:52,108 --> 00:53:54,406
Mitochondria using free energy in a

1285
00:53:54,428 --> 00:53:56,806
similar way with compartments, I guess

1286
00:53:56,828 --> 00:53:57,986
actually it'd be interesting to get your

1287
00:53:58,028 --> 00:54:00,186
thoughts, david on, you talked at the

1288
00:54:00,208 --> 00:54:01,706
start about how synapses are

1289
00:54:01,808 --> 00:54:05,690
compartmentalized. Do you see different

1290
00:54:05,760 --> 00:54:07,866
sort of instantiations of this in

1291
00:54:07,888 --> 00:54:09,802
different compartments just within one

1292
00:54:09,856 --> 00:54:11,834
synapse, almost like are you wanting to

1293
00:54:11,872 --> 00:54:13,818
sort of look at that granular level or

1294
00:54:13,904 --> 00:54:15,178
is it more now taking what you've

1295
00:54:15,194 --> 00:54:16,286
learned from this and putting it back

1296
00:54:16,308 --> 00:54:18,766
into how can we sort of make the AI more

1297
00:54:18,788 --> 00:54:22,014
efficient? Yeah, we are going much more

1298
00:54:22,052 --> 00:54:24,466
in this direction now that we see how we

1299
00:54:24,488 --> 00:54:27,170
can build this back into AI models.

1300
00:54:31,030 --> 00:54:32,946
One has to be a bit careful when using

1301
00:54:32,968 --> 00:54:34,686
the free energy principle because it's

1302
00:54:34,718 --> 00:54:36,886
such a powerful general framework that

1303
00:54:36,908 --> 00:54:40,680
you can apply it to basically anything

1304
00:54:42,650 --> 00:54:46,374
and it's not necessarily you will come

1305
00:54:46,412 --> 00:54:50,140
up with a useful result in the end.

1306
00:54:50,750 --> 00:54:55,914
Just putting this this

1307
00:54:55,952 --> 00:54:57,338
started actually as a side project.

1308
00:54:57,424 --> 00:55:02,106
This was my kind of COVID pandemic

1309
00:55:02,138 --> 00:55:06,234
lockdown project. And I was just curious

1310
00:55:06,282 --> 00:55:09,214
about this and whether you can actually

1311
00:55:09,252 --> 00:55:10,398
solve this, because I thought the

1312
00:55:10,404 --> 00:55:12,014
synopsis is maybe simple enough.

1313
00:55:12,212 --> 00:55:14,750
Because when you go into the papers,

1314
00:55:15,410 --> 00:55:17,474
they have at some point to go into some

1315
00:55:17,512 --> 00:55:20,018
approximations. They do some mean field,

1316
00:55:20,104 --> 00:55:22,622
usually, so they go for first modes,

1317
00:55:22,766 --> 00:55:24,994
and then you can solve these guys for

1318
00:55:25,032 --> 00:55:26,766
more complex, even for neurons, it's

1319
00:55:26,798 --> 00:55:29,026
hard, actually, if you go to the neuron

1320
00:55:29,058 --> 00:55:31,798
or network level, it's hard. It's very

1321
00:55:31,964 --> 00:55:35,414
involved math, but for a sinus is simple

1322
00:55:35,452 --> 00:55:37,174
enough, actually. So you can actually do

1323
00:55:37,212 --> 00:55:39,498
this and spell everything out if you

1324
00:55:39,504 --> 00:55:42,474
make the right assumptions and really

1325
00:55:42,512 --> 00:55:44,330
just derive these things. And that was

1326
00:55:44,400 --> 00:55:48,474
kind of just kind of

1327
00:55:48,512 --> 00:55:52,494
a game I went into and

1328
00:55:52,532 --> 00:55:55,594
then it turned out to work quite nicely,

1329
00:55:55,642 --> 00:55:57,680
I think, in the end. Yeah.

1330
00:55:59,570 --> 00:56:02,078
I'm not sure if you like Mitochondria or

1331
00:56:02,084 --> 00:56:03,586
so I'm sure you could apply the same

1332
00:56:03,608 --> 00:56:05,266
principles but I'm not sure if the

1333
00:56:05,288 --> 00:56:09,230
results you get would be any meaningful

1334
00:56:09,310 --> 00:56:12,020
or would help you in any way.

1335
00:56:13,750 --> 00:56:16,306
That's always the risk. You invest so

1336
00:56:16,328 --> 00:56:18,022
much time and then in the end you get

1337
00:56:18,076 --> 00:56:21,606
some results and you don't know. That

1338
00:56:21,628 --> 00:56:23,734
was also something that I thought was

1339
00:56:23,852 --> 00:56:26,914
quite interesting, which was the synapse

1340
00:56:26,962 --> 00:56:29,158
was the agent. It's really easy to

1341
00:56:29,164 --> 00:56:31,162
think, oh, we'll make an agent based

1342
00:56:31,216 --> 00:56:33,914
model of a neural system. First off,

1343
00:56:33,952 --> 00:56:36,246
that tends to not include Glia or non

1344
00:56:36,278 --> 00:56:38,570
neural cell types, but it's almost like

1345
00:56:38,640 --> 00:56:40,942
a doubly unquestioned assumption that

1346
00:56:40,996 --> 00:56:42,880
the cell would be the agent.

1347
00:56:44,130 --> 00:56:48,510
But then it was a great transition from

1348
00:56:48,580 --> 00:56:50,606
the person throwing the ball over the

1349
00:56:50,628 --> 00:56:54,270
wall. That's an action centric approach

1350
00:56:54,430 --> 00:56:56,718
where you only have partial visibility

1351
00:56:56,814 --> 00:57:00,322
of the consequences and then that

1352
00:57:00,456 --> 00:57:02,526
is the exact scenario that the synapse

1353
00:57:02,558 --> 00:57:05,858
finds itself in in a different way.

1354
00:57:05,944 --> 00:57:08,726
Or it could have been set up so that a

1355
00:57:08,748 --> 00:57:11,270
neuron is the agent. We're building

1356
00:57:11,340 --> 00:57:14,438
maps, not territories. And so then,

1357
00:57:14,604 --> 00:57:15,942
just like you said, free energy

1358
00:57:15,996 --> 00:57:17,734
principle, it's a principle for

1359
00:57:17,772 --> 00:57:21,770
everything. And so just

1360
00:57:21,840 --> 00:57:23,834
making principled statements about

1361
00:57:23,952 --> 00:57:27,770
things is table stakes.

1362
00:57:28,190 --> 00:57:30,506
And then I guess my question for you is

1363
00:57:30,528 --> 00:57:33,838
then what does make it useful or in your

1364
00:57:33,924 --> 00:57:35,614
learning and tinkering around with these

1365
00:57:35,652 --> 00:57:38,506
models? What differentiated situations

1366
00:57:38,538 --> 00:57:40,330
where you applied free energy principle

1367
00:57:40,410 --> 00:57:42,814
or active inference and you felt like it

1368
00:57:42,852 --> 00:57:45,106
was providing a contribution to your

1369
00:57:45,128 --> 00:57:47,346
research direction versus where you

1370
00:57:47,368 --> 00:57:48,674
played around and it was like, well,

1371
00:57:48,712 --> 00:57:52,226
that was tautological. I think the free

1372
00:57:52,248 --> 00:57:54,466
energy principle makes sense in a

1373
00:57:54,488 --> 00:57:57,134
context where you have incomplete

1374
00:57:57,182 --> 00:57:57,780
information.

1375
00:58:01,030 --> 00:58:02,694
So for example, in the synapse case,

1376
00:58:02,732 --> 00:58:04,760
right, the question we started with was

1377
00:58:05,130 --> 00:58:06,886
this problem that the synapse has to

1378
00:58:06,908 --> 00:58:08,034
solve, that it has incomplete

1379
00:58:08,082 --> 00:58:11,002
information about the state in the cell

1380
00:58:11,056 --> 00:58:14,860
body because it only sees this

1381
00:58:15,230 --> 00:58:17,258
kind of or that's the assumption, at

1382
00:58:17,264 --> 00:58:19,674
least of the model. And also what we get

1383
00:58:19,712 --> 00:58:22,166
from the experimentalists, that it

1384
00:58:22,208 --> 00:58:23,406
essentially only sees this back

1385
00:58:23,428 --> 00:58:25,098
propagating action potential. So it sees

1386
00:58:25,114 --> 00:58:28,686
a single binary variable about the

1387
00:58:28,708 --> 00:58:31,710
state of the soma.

1388
00:58:32,950 --> 00:58:35,460
So essentially this is a problem of

1389
00:58:36,150 --> 00:58:39,714
incomplete information and also the

1390
00:58:39,752 --> 00:58:43,762
second ingredient that

1391
00:58:43,816 --> 00:58:46,018
you need some form of agency. I think if

1392
00:58:46,024 --> 00:58:48,886
you apply the free energy principle to a

1393
00:58:48,908 --> 00:58:50,774
system without agency, so if something

1394
00:58:50,812 --> 00:58:53,154
is not interacting with an environment

1395
00:58:53,202 --> 00:58:56,822
in a closed loop, then it becomes really

1396
00:58:56,876 --> 00:58:59,718
sketchy. And I think already this model

1397
00:58:59,884 --> 00:59:02,780
is on the edge when it comes because

1398
00:59:04,270 --> 00:59:05,546
these models don't really have an

1399
00:59:05,568 --> 00:59:07,146
agency, but they at least produce an

1400
00:59:07,168 --> 00:59:09,386
output, right? So you can still think of

1401
00:59:09,408 --> 00:59:11,246
this as an interaction with an

1402
00:59:11,268 --> 00:59:14,714
environment. But as soon as you lose

1403
00:59:14,762 --> 00:59:17,326
that, I think then there would be

1404
00:59:17,348 --> 00:59:18,926
simpler models that can just give you

1405
00:59:18,948 --> 00:59:19,680
the same.

1406
00:59:26,010 --> 00:59:28,566
Actually, in the synapse case, the

1407
00:59:28,588 --> 00:59:32,162
agency is only this adding noise

1408
00:59:32,226 --> 00:59:34,278
actually in the model because the

1409
00:59:34,284 --> 00:59:36,038
synapse is triggered presynaptically and

1410
00:59:36,044 --> 00:59:39,606
then it uses its internal

1411
00:59:39,638 --> 00:59:41,690
state to add the right amount of noise,

1412
00:59:42,350 --> 00:59:44,614
which is probably already the minimal

1413
00:59:44,662 --> 00:59:46,250
agency you could imagine.

1414
00:59:49,710 --> 00:59:51,626
Sir, you want to ask a question or I can

1415
00:59:51,648 --> 00:59:54,958
ask a question? Yeah, it was more just a

1416
00:59:54,964 --> 00:59:56,878
comment. I think it's quite interesting

1417
00:59:57,044 --> 00:59:59,534
people talk about biology. Some people

1418
00:59:59,572 --> 01:00:01,178
say it's not a real science because it's

1419
01:00:01,194 --> 01:00:04,014
all messy and noisy. But I think it

1420
01:00:04,052 --> 01:00:05,438
works really interesting because it's

1421
01:00:05,454 --> 01:00:07,266
like you say, the synaptic noise is

1422
01:00:07,288 --> 01:00:09,714
actually a reporting of uncertainty. So

1423
01:00:09,752 --> 01:00:11,234
in that essence, it's actually probably

1424
01:00:11,272 --> 01:00:13,934
quite accurately reporting the messy

1425
01:00:13,982 --> 01:00:15,362
world rather than the biology itself

1426
01:00:15,416 --> 01:00:16,994
just being all messy. But that's just

1427
01:00:17,032 --> 01:00:19,506
what I was thinking about. Yeah, I think

1428
01:00:19,528 --> 01:00:20,670
I was curious as well. Like you said,

1429
01:00:20,680 --> 01:00:21,862
this was like your Lockdown project,

1430
01:00:21,916 --> 01:00:24,006
but I'm just interested in how you sort

1431
01:00:24,028 --> 01:00:25,382
of came to use the free energy

1432
01:00:25,436 --> 01:00:26,966
principle, how you came across it. Was

1433
01:00:26,988 --> 01:00:28,430
it something you were quite familiar

1434
01:00:28,450 --> 01:00:30,618
with already or some of your network or

1435
01:00:30,624 --> 01:00:32,138
peers were talking about it? Or did you

1436
01:00:32,144 --> 01:00:33,500
just stumble across it in a.

1437
01:00:36,190 --> 01:00:39,750
Was during my PhD, I was interested in

1438
01:00:39,920 --> 01:00:41,946
variational methods and probabilistic

1439
01:00:41,978 --> 01:00:45,466
methods. And then I started reading

1440
01:00:45,498 --> 01:00:48,030
about this. And so I read a bunch of

1441
01:00:48,180 --> 01:00:51,582
Karl Friston's papers and I found this

1442
01:00:51,636 --> 01:00:54,770
interesting. And actually,

1443
01:00:54,920 --> 01:00:58,606
my PhD supervisor always encouraged

1444
01:00:58,638 --> 01:01:01,410
me not to go in that direction.

1445
01:01:02,550 --> 01:01:04,626
And then after I finished my PhD and

1446
01:01:04,648 --> 01:01:06,786
thought, okay, now I can do what I want,

1447
01:01:06,968 --> 01:01:08,200
I try it out.

1448
01:01:10,890 --> 01:01:12,678
And then, I guess, do you think it would

1449
01:01:12,684 --> 01:01:15,574
be worthwhile, like, next steps for you

1450
01:01:15,612 --> 01:01:17,686
or for the field actually trying to

1451
01:01:17,708 --> 01:01:20,454
implement this on maybe some of the more

1452
01:01:20,492 --> 01:01:22,586
analog chips that are being built in the

1453
01:01:22,608 --> 01:01:24,166
space? Like the analog neuromorphic

1454
01:01:24,198 --> 01:01:26,140
chips, which I know you can have some

1455
01:01:26,990 --> 01:01:29,338
dynamic synapses and things like do you

1456
01:01:29,344 --> 01:01:31,338
think it would be worthwhile trying to

1457
01:01:31,344 --> 01:01:33,998
implement it on hardware? Or what are

1458
01:01:34,004 --> 01:01:36,366
your thoughts on that? I mean, the

1459
01:01:36,388 --> 01:01:38,014
triplet rule that comes out from this

1460
01:01:38,052 --> 01:01:40,526
first work I showed, I think that would

1461
01:01:40,548 --> 01:01:42,240
be interesting to implement it.

1462
01:01:44,390 --> 01:01:46,546
The nice feature is that it should, in

1463
01:01:46,568 --> 01:01:49,154
principle, should have this self

1464
01:01:49,192 --> 01:01:52,354
stabilizing feature because it's really

1465
01:01:52,392 --> 01:01:56,342
mimicking the dynamics of

1466
01:01:56,476 --> 01:01:59,240
the membrane, of the cell membrane. So

1467
01:01:59,690 --> 01:02:06,086
if the neuromorphic hardware would so

1468
01:02:06,188 --> 01:02:08,906
if the model in the synapse and the

1469
01:02:08,928 --> 01:02:11,706
neuron model match up very well, the

1470
01:02:11,728 --> 01:02:14,762
model should give you this nice self

1471
01:02:14,816 --> 01:02:17,110
stabilizing feature so that neurons

1472
01:02:17,190 --> 01:02:19,994
really not go into some epileptic states

1473
01:02:20,032 --> 01:02:21,638
or so. And you get this for free from

1474
01:02:21,664 --> 01:02:23,198
this model. That's what we saw in the

1475
01:02:23,204 --> 01:02:27,166
simulations, at least. But in

1476
01:02:27,188 --> 01:02:29,374
the simulations, of course, we had full

1477
01:02:29,412 --> 01:02:31,566
control over this dynamics matching up

1478
01:02:31,588 --> 01:02:35,102
in the right way. So that is

1479
01:02:35,156 --> 01:02:37,422
probably a bit more tricky for hardware,

1480
01:02:37,486 --> 01:02:39,746
but it's probably solvable. So it would

1481
01:02:39,768 --> 01:02:42,114
be interesting. What you get for it is

1482
01:02:42,152 --> 01:02:43,922
that you have these purely event based

1483
01:02:43,976 --> 01:02:47,698
tools which only use pre post spikes,

1484
01:02:47,794 --> 01:02:51,990
which is nice. Very cool. Thank you.

1485
01:02:52,060 --> 01:02:54,854
Yeah. Daniel, if you had a question?

1486
01:02:55,052 --> 01:02:58,522
Well, that's a great principle there,

1487
01:02:58,576 --> 01:03:00,346
which is like, if you can design the

1488
01:03:00,368 --> 01:03:02,922
neuromorphic algorithm so that it

1489
01:03:03,056 --> 01:03:06,570
harnesses a material feature like the

1490
01:03:06,640 --> 01:03:08,726
actual leaky permeability of a membrane

1491
01:03:08,758 --> 01:03:12,078
or actual spatial proximity, if you can

1492
01:03:12,244 --> 01:03:14,938
leverage a material feature,

1493
01:03:15,034 --> 01:03:17,898
analog feature that isn't virtualized,

1494
01:03:18,074 --> 01:03:21,790
then it's already an adjacency into

1495
01:03:21,940 --> 01:03:23,554
future hardware. So that's one great

1496
01:03:23,592 --> 01:03:26,466
point. And then to Sarah's point about

1497
01:03:26,648 --> 01:03:29,614
almost biology not being a science,

1498
01:03:29,742 --> 01:03:33,294
which there is a famous quotation,

1499
01:03:33,422 --> 01:03:35,118
there will never be a Newton for a blade

1500
01:03:35,134 --> 01:03:37,106
of grass. Because some people say,

1501
01:03:37,128 --> 01:03:39,126
yeah, it's a different biology is more

1502
01:03:39,148 --> 01:03:41,158
like history. Because whether you

1503
01:03:41,164 --> 01:03:42,694
approach this from a development or

1504
01:03:42,732 --> 01:03:44,482
ecology or evolution perspective,

1505
01:03:44,546 --> 01:03:46,258
biology is a historical science. It's

1506
01:03:46,274 --> 01:03:49,034
not like a real science. And then that

1507
01:03:49,072 --> 01:03:52,294
reminded me of the cross country shirt

1508
01:03:52,342 --> 01:03:54,774
that says, our sport is your punishment.

1509
01:03:54,902 --> 01:03:58,918
So it's like, well, no, your noise

1510
01:03:59,014 --> 01:04:02,638
is biology's signal. And that's how

1511
01:04:02,724 --> 01:04:06,638
it happens. My question was

1512
01:04:06,804 --> 01:04:10,400
about this tension between,

1513
01:04:11,490 --> 01:04:14,402
I guess, neural and computational ways

1514
01:04:14,456 --> 01:04:17,374
of looking at the resources associated

1515
01:04:17,422 --> 01:04:19,726
with computation. So from the von

1516
01:04:19,758 --> 01:04:22,946
Neumann paradigm. We have a

1517
01:04:22,968 --> 01:04:25,902
lot of shared reference points, CPU

1518
01:04:25,966 --> 01:04:29,234
cycles, Ram capacity, and all

1519
01:04:29,272 --> 01:04:33,480
these kinds of and like, even in your

1520
01:04:33,930 --> 01:04:36,646
introductions, you conveyed like, well,

1521
01:04:36,668 --> 01:04:38,922
this is how many CPU cycles it's going

1522
01:04:38,976 --> 01:04:41,798
through, or this is how many parameters

1523
01:04:41,894 --> 01:04:43,914
would have to be stored, or something

1524
01:04:43,952 --> 01:04:47,770
like that. However, that's referencing

1525
01:04:49,230 --> 01:04:52,670
another paradigm. So what do resource

1526
01:04:55,730 --> 01:04:59,690
descriptors or capacity descriptors

1527
01:04:59,770 --> 01:05:03,838
look like when we're outside the

1528
01:05:03,844 --> 01:05:05,478
space of okay, yeah, power consumption.

1529
01:05:05,514 --> 01:05:06,866
That's something that you can put into a

1530
01:05:06,888 --> 01:05:08,942
box and just use a bomb calorimeter

1531
01:05:09,006 --> 01:05:11,294
that's kind of like a low hanging fruit.

1532
01:05:11,422 --> 01:05:14,830
But now okay, beyond just the sheer

1533
01:05:14,990 --> 01:05:18,486
energy or caloric requirements, what can

1534
01:05:18,508 --> 01:05:21,286
we say that is, like, analogous to the

1535
01:05:21,308 --> 01:05:24,358
way that we talk about the processor or

1536
01:05:24,364 --> 01:05:26,440
the Ram or the hard drive on a computer?

1537
01:05:31,640 --> 01:05:33,684
Yeah, I'd have to think about that one

1538
01:05:33,722 --> 01:05:34,936
some more. I do think there was some

1539
01:05:34,958 --> 01:05:36,776
interesting comments in the paper on

1540
01:05:36,798 --> 01:05:39,256
that slide I showed that talks about the

1541
01:05:39,278 --> 01:05:40,696
brain and energy. There was a paper I

1542
01:05:40,718 --> 01:05:41,928
linked to. I'll have to get the

1543
01:05:41,934 --> 01:05:42,968
reference and let you know what it is

1544
01:05:42,974 --> 01:05:45,324
because QR code is gone now. But that

1545
01:05:45,362 --> 01:05:47,820
had some interesting ideas, I think, on

1546
01:05:47,890 --> 01:05:51,176
what you're getting at there. But I'd

1547
01:05:51,208 --> 01:05:53,710
have to defer to the paper.

1548
01:05:55,440 --> 01:05:57,756
How do they describe what is being

1549
01:05:57,858 --> 01:06:00,912
designed? So they say it has this many

1550
01:06:00,966 --> 01:06:03,424
of this type of component, and then that

1551
01:06:03,462 --> 01:06:05,984
might do nothing though. So how do they

1552
01:06:06,022 --> 01:06:08,468
describe or evaluate these different

1553
01:06:08,554 --> 01:06:10,500
designs or algorithms?

1554
01:06:12,520 --> 01:06:13,988
I think it's all different depending on

1555
01:06:13,994 --> 01:06:15,444
the use case. That's what I've found.

1556
01:06:15,482 --> 01:06:17,332
Really? Like, the language is different

1557
01:06:17,386 --> 01:06:19,812
depending on if it's written by someone

1558
01:06:19,866 --> 01:06:21,136
maybe with more of a neuroscience

1559
01:06:21,168 --> 01:06:22,560
background or an engineering background.

1560
01:06:22,640 --> 01:06:24,728
And then you kind of get used to some

1561
01:06:24,734 --> 01:06:26,088
terms that are more interchangeable than

1562
01:06:26,094 --> 01:06:27,940
others. But I do think the terminology

1563
01:06:28,020 --> 01:06:30,696
is something which needs to be looked at

1564
01:06:30,718 --> 01:06:32,164
a lot more closely in this space,

1565
01:06:32,222 --> 01:06:33,532
because then I think that will help

1566
01:06:33,586 --> 01:06:35,948
everybody working in it to be on the

1567
01:06:35,954 --> 01:06:37,740
same page a little bit closer.

1568
01:06:42,910 --> 01:06:46,806
Cool. Well, any other thoughts

1569
01:06:46,838 --> 01:06:50,094
or questions? David first and then

1570
01:06:50,132 --> 01:06:52,894
Sarah. Also, I'm very curious, what

1571
01:06:53,012 --> 01:06:56,734
direction will this series go? But

1572
01:06:56,772 --> 01:06:59,454
first, David, what are any other kind of

1573
01:06:59,492 --> 01:07:02,478
closing comments or directions you want

1574
01:07:02,484 --> 01:07:03,040
to.

1575
01:07:05,650 --> 01:07:08,066
Really? I mean, I would say thanks for

1576
01:07:08,088 --> 01:07:10,466
having me today. It was really a

1577
01:07:10,488 --> 01:07:12,180
pleasure to discuss with you.

1578
01:07:14,230 --> 01:07:15,906
Thank you. David, it was amazing to have

1579
01:07:15,928 --> 01:07:17,350
you on. I think your work is absolutely

1580
01:07:17,420 --> 01:07:19,590
fascinating, and I think it's going to

1581
01:07:19,660 --> 01:07:22,134
have lots of benefits in the future for

1582
01:07:22,172 --> 01:07:24,086
implementation, which is always nice to

1583
01:07:24,108 --> 01:07:26,930
see as well. Yeah. So what was the

1584
01:07:26,940 --> 01:07:28,486
question? Where do I see the series

1585
01:07:28,518 --> 01:07:31,882
going? Hopefully we can have a new

1586
01:07:31,936 --> 01:07:34,218
guest each month. I think it'd be kind

1587
01:07:34,224 --> 01:07:38,010
of cool maybe next month to have someone

1588
01:07:38,080 --> 01:07:39,674
who's building hardware, so, like,

1589
01:07:39,712 --> 01:07:41,158
maybe someone from the BrainScaleS team

1590
01:07:41,184 --> 01:07:42,734
or spinnaker team or something like that

1591
01:07:42,772 --> 01:07:44,814
would be. Pretty cool, but yeah,

1592
01:07:44,852 --> 01:07:47,614
really, I just want to have a space for

1593
01:07:47,652 --> 01:07:49,374
people who are interested in this

1594
01:07:49,412 --> 01:07:52,826
intersection to meet people and see

1595
01:07:52,868 --> 01:07:55,234
talks and reach out to people who are

1596
01:07:55,272 --> 01:07:57,534
also working in the space, because it's

1597
01:07:57,582 --> 01:07:59,922
pretty niche, but I think it's pretty

1598
01:07:59,976 --> 01:08:02,306
important, actually. Having said that,

1599
01:08:02,328 --> 01:08:04,086
David, could you let everybody know if

1600
01:08:04,108 --> 01:08:05,538
they wanted to reach out to you, what's

1601
01:08:05,554 --> 01:08:07,720
the best way for them to do that?

1602
01:08:12,250 --> 01:08:14,034
I'm not very active on this discord

1603
01:08:14,082 --> 01:08:15,786
channel, so maybe email is still the

1604
01:08:15,808 --> 01:08:19,180
best to reach out to me, I guess.

1605
01:08:20,030 --> 01:08:24,122
Cool. Do you want to give your oh.

1606
01:08:24,256 --> 01:08:27,594
I think my email should be easy

1607
01:08:27,632 --> 01:08:28,874
enough to find. But you can also give

1608
01:08:28,912 --> 01:08:31,834
the email out there. Sure. Cool. People

1609
01:08:31,872 --> 01:08:34,586
can check the papers, and then in the

1610
01:08:34,608 --> 01:08:36,582
active inference institute discord,

1611
01:08:36,646 --> 01:08:38,780
there's the neuromorphic channel.

1612
01:08:41,990 --> 01:08:44,526
All right. Thank you, David and Sarah.

1613
01:08:44,638 --> 01:08:47,746
Really cool to see morphstream kick off

1614
01:08:47,768 --> 01:08:49,810
its developmental trajectory this way.

1615
01:08:49,880 --> 01:08:52,114
So till next time. Thank you.



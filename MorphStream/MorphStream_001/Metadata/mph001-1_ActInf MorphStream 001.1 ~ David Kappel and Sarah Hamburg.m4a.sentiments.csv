start	end	speaker	sentiment	confidence	text
3960	4510	A	0.546256959438324	You.
6880	7980	B	0.5307407379150391	Hello, everybody.
8130	8830	B	0.7317204475402832	Welcome.
9200	12860	B	0.9151225090026855	It is September 26, 2023.
13010	17864	B	0.8066417574882507	We are here kicking off a new stream series at the Active Inference Institute.
17992	18984	B	0.6354769468307495	This is the morph.
19032	21116	B	0.825240969657898	Stream 1.1.
21298	28940	B	0.8460759520530701	Today we have David Cappell and also this section and streams facilitated by Sarah Ham.
29480	34624	B	0.9002998471260071	We're going to have a overview, first presented by Sarah.
34752	41924	B	0.7986225485801697	Then David will share some work on neuromorphic computing, and then we'll have some time to discuss.
42042	49096	B	0.9510809183120728	So thank you both for joining and Sarah, to you for the first presentation and also to introduce yourself, if you'd like.
49278	50312	C	0.9551897048950195	Yes, that's a good idea.
50366	51908	C	0.9763081073760986	Thank you very much, Daniel.
52084	53636	C	0.8094164729118347	So my name is Sarah.
53748	61464	C	0.9026572704315186	I'm a neuroscientist specializing intelligence, currently working in the field of neuromorphic computing at Sheffield Hallam in the UK.
61592	70060	C	0.8677146434783936	So I'm going to give you a high level overview of what neuromorphic computing is before we hear David's exciting talk in the first edition of this new series.
70640	76130	C	0.6338506937026978	Just to let you know, if you're watching on Double Time in the future, I talk quite fast, so you might not want to watch me on Double Time.
76740	82630	C	0.9516409039497375	So this QR code I put here will take you to a paper which I thought was a really nice introduction to the field.
83080	89748	C	0.8729680776596069	But neuromorphic computing can be defined as computing systems that are designed to mimic the structure and function of the nervous system.
89914	92196	C	0.6549088358879089	So this doesn't have to be the human nervous system.
92298	99370	C	0.7722007036209106	The field actually takes inspiration from all sorts of animals and insects, although the definitions online don't necessarily acknowledge that.
99740	113016	C	0.7830567955970764	So some people are quite open with what constitutes neuromorphic, while maybe others would prefer neuromorphic was reserved for hardware instantiations of biological like neurons, which are sometimes referred to as non von Newman computers.
113048	119176	C	0.8682031035423279	And I think that's what the paper that that QR code refers to it as their definition.
119368	121644	C	0.8865306973457336	So what I think is really interesting is a little bit of the context.
121692	127872	C	0.8101491332054138	So, like our current von Newman computer architecture was also inspired by neuroscience, particularly the McCullough and Pitts 43.
127926	131716	C	0.8928350210189819	Neuromodel inspired von Newman's first draft in 1945.
131818	151464	C	0.5770881772041321	So neuroscience has a long history of inspiring computer science, and this also includes reinforcement learning, which is based on theories about learning, decision making from behavioral psychology based on rewards and punishments, and also Hebian learning principles of cells that fire together, wire together from 49 became foundational for unsupervised learning.
151582	154970	C	0.7179729342460632	So, first of all, hang on 1 second.
155820	161864	C	0.7640137076377869	So in order to understand the why of neuromorphic computing, I really wanted to explain what's so great about the brain.
161992	165336	C	0.5090985894203186	So here's some inspiration for light bulbs.
165448	166684	C	0.870448887348175	So I'm going to ask you a question.
166722	168332	C	0.7921290397644043	I just want you to think about it for a second.
168466	172332	C	0.8154611587524414	In terms of light bulbs, how much energy do you think the brain uses?
172476	177456	C	0.788081705570221	Do you think it's more or less energy than the bulbs lighting the room that you're in?
177558	179344	C	0.7036219835281372	If you're in the future, by all means pause this.
179382	179744	C	0.5271216034889221	Pause this.
179782	181792	C	0.8755771517753601	If you want to do some in depth calculations.
181856	183590	C	0.7361037731170654	But I'm going to skip to the answer.
184040	187488	C	0.5913783311843872	The answer is here, in the pink circle.
187584	189568	C	0.8363921642303467	So it's 20 watts.
189744	193616	C	0.702826976776123	So that's the equivalent of one modern day energy efficient light bulb.
193728	197064	C	0.8163810968399048	So that's probably what's above me now, basically in my room here.
197262	202490	C	0.8938761949539185	This QR code should take you to quite an interesting paper on power consumption in the brain, if you're interested in that.
203900	208120	C	0.6978097558021545	So that works out about four bananas a day to power your brain.
208280	212444	C	0.8055492639541626	And this is calculated, by the way, based on calorie intake that the brain needs.
212562	223276	C	0.6474525332450867	So, for context, the fastest supercomputer in Europe, I think it's called lumi, in Finland it's been called exceptionally green, and its power consumption is 8.5 million watts.
223388	228144	C	0.5901479721069336	So that's around half a million light bulbs, while your brain uses just one.
228342	233540	C	0.745290219783783	So then the question is, well, what does your brain do with that one light bulb or four bananas?
235080	239204	C	0.7174939513206482	Apparently it does 1000 billion calculations per second.
239402	242324	C	0.7334306836128235	So there's lots of other massive estimates out there.
242442	245252	C	0.5147907733917236	This wasn't even the largest by several orders of magnitude.
245396	253348	C	0.597614049911499	Estimates are obviously very speculative, but they're all massive and they all tend to be based on the number of neurons, their connections and firing rates.
253524	262824	C	0.5851499438285828	But I think it's really important for the context that supercomputers can't actually yet match our complexity of skills or the adaptability of the human brain.
262952	269810	C	0.8761602640151978	So we actually excel way beyond supercomputers when it comes to things like complex decision making, learning from experience.
271380	273600	C	0.7778394222259521	So how does your brain compare to AI?
273940	277212	C	0.7853953838348389	So, I mentioned that modern AI is already brain inspired.
277356	280340	C	0.7510982751846313	However, artificial neurons are highly simplified.
281160	285350	C	0.7107450366020203	They don't capture the complexity of biological neurons or networks, like, not even close.
286200	289124	C	0.8353853225708008	Individual neurons are actually more like networks themselves.
289322	298292	C	0.8847196102142334	And research suggests that modeling one biological neuron requires a five to eight layer deep artificial neural network made of around 1000 artificial neurons.
298356	300696	C	0.8022791743278503	This QR code should take you to the paper.
300878	301850	C	0.6025077104568481	For that.
303020	305720	C	0.6305771470069885	You have 86 billion neurons in your brain.
305880	315484	C	0.8215188384056091	They work together to form a highly energy efficient, low latency supercomputer that works just above room temperature, off the equivalent of about four bananas a day.
315682	326364	C	0.940920889377594	So hopefully I've given you a sense of how amazing your brain is, as if you didn't know that already, and how it's already been used to inspire the, I guess, fairly basic AI that we have now compared to human intelligence.
326492	337284	C	0.5258020758628845	So, next I'm going to explain how key features of the brain are being implemented to catalyze our next generation of AI and technology through the field of neuromorphic computing, which is why you're all here.
337482	344010	C	0.891249418258667	So, traditional volume and computers have physically separate computing and memory units, shown here on the left.
344620	348600	C	0.733672559261322	During computation, data must transfer backwards and forwards, like really fast.
348670	360092	C	0.7898656725883484	So there's a bottleneck, essentially for speed and energy, whereas in neuromorphic architectures, which are shown here on the right, with the help of Dali, computing and memory occur in the same place.
360226	362140	C	0.7717717885971069	So they're said to be colocated.
362800	370316	C	0.8807703256607056	Essentially, individual neurons perform computation while memory is represented by the strength of the connections, the weights between neurons.
370348	371740	C	0.654126763343811	So the synapses.
371900	379516	C	0.8762125968933105	So chips like this might be created with components like Memoristas for example, which can emulate synaptic weights.
379708	383760	C	0.7607485055923462	And this architecture improves speed, it reduces energy consumption.
383840	391190	C	0.938780665397644	And what's really interesting is it enables massively parallel processing meaning that multiple problems can be worked on at the same time.
391640	404208	C	0.5487903356552124	So this is particularly important, this architecture, for various use cases, but also because as we reach the end of Moore's Law, which is the number of transistors you were able to physically make tinier and tinier to fit on a chip.
404404	413820	C	0.5440857410430908	And it's also important because humanity needs to massively reduce its energy consumption against the backdrop drop of creating ever more powerful AI.
415280	427136	C	0.8537102937698364	So artificial neurons typically use continuous activation shown on the left, they're always on, while neuromorphic neurons, they're said to be spiking, so they're on or they're off, which is shown here on the right.
427238	429448	C	0.8714908957481384	So similar to sort of an action potential.
429644	438240	C	0.8250173926353455	So the benefits for this are again power, efficiency and also applications where timing is important and this is given that they're event driven.
438400	447770	C	0.704767644405365	So essentially they have the potential for spatial and temporal dimensions which then enables added spatiotemporal encoding and processing of information.
448460	453140	C	0.7497591376304626	You might be wondering a bit about GPUs which also enable parallel processing.
453300	465420	C	0.9676511883735657	Research suggests that GPUs are suitable architectures for deploying spiking neural networks which I think makes this a really interesting time for the field given how high end GPUs are becoming ever more pervasive.
467280	471240	C	0.8617396354675293	So the brain learns strength of synapses between neurons.
471320	474076	C	0.9082034230232239	This is based on pre and synaptic firing patterns.
474108	478000	C	0.5169634222984314	I think David's Talk will talk a lot, we'll go into a lot more depth on this.
478070	487540	C	0.8572902083396912	But there are many different types and patterns of this across the brain depending on the types of synapses such as excitatory to inhibitory, excitatory to excalitatory.
487960	498472	C	0.7311043739318848	And the neuromorphic field is working to leverage these rules because of benefits for on chip learning and also applications such as pattern recognition and edge computing as well.
498526	505832	C	0.8981257081031799	Edge computing being quite a huge use case for neuromorphic computing because of the sort of event driven nature and also the low energy usage.
505976	511230	C	0.9463430643081665	And this QR code should take you to quite an interesting paper on STDP that I found.
512720	515212	C	0.8961585760116577	So what neuromorphic solutions are available?
515266	515532	C	0.6853025555610657	Now?
515586	522592	C	0.551993727684021	You might just think this is all theoretical, there are actually many different solutions out there which I'll just give you a really high level overview of.
522726	532852	C	0.7093831300735474	So the Human Brain Project has created several large scale neuromorphic computers including Spinnaker, which is this one at the bottom, this board's, like maybe like the size of my face or something.
532986	546488	C	0.7597209811210632	So that runs in real time and it's comprised of multiple general purpose arm microprocessors and there was also BrainScaleS which is an accelerated analog architecture and it runs 1000 times real time.
546574	555610	C	0.9765264391899109	So the board next to the blue one, that's an actual credit card size version of BrainScaleS, which they've recently made, which I thought was pretty cool.
556540	558812	C	0.6158440113067627	And then there's also some big players in the space.
558866	561976	C	0.8817033767700195	So this blue one here is Intel's Louihi chip.
562008	563084	C	0.8744253516197205	They're onto Luihi two.
563122	571008	C	0.8374875783920288	Now, that's their Neuromorphic chip, and they have an open source software framework for that as well, because they really want to catalyze the open source community to get involved with it.
571174	573164	C	0.8613414764404297	So neuromorphic sensors also exist?
573212	575916	C	0.8535295724868774	So this little blue thing in the middle is actually a Neuromorphic camera.
575948	577250	C	0.6850147247314453	It's maybe like this big.
578180	583092	C	0.873974084854126	So they aim to recreate how our nervous system senses stimuli, such as light.
583226	590256	C	0.8050253391265869	So, for example, in a Neuromorphic camera, which is the one here, each pixel works independently with a microsecond resolution.
590368	591892	C	0.8757849335670471	Hopefully, my GIF will work.
592026	592788	C	0.5760965943336487	There we go.
592874	596216	C	0.9738153219223022	So you can see each pixel working there, which is pretty cool.
596398	602580	C	0.793737530708313	So compared to traditional digital cameras, they have improved performance with motion and lower power consumption.
602740	606632	C	0.977268397808075	There was also a Neuromorphic nose recently by intel, which was pretty cool.
606766	613950	C	0.8233899474143982	So it could learn the scent of a chemical after just one exposure, and then it could identify that scent even when it was masked by others.
614400	617672	C	0.775600016117096	And then finally, this is a humanoid robot called an ICup.
617816	629756	C	0.6284558773040771	And what you can do is you can actually integrate Neuromorphic sensors, such as the camera, and then Neuromorphic chips maybe spinnaker or brain scales into a humanoid device like this or other devices like a drone.
629868	633568	C	0.6421248912811279	And then from that, you can actually create embodied neuromorphic systems.
633744	639060	C	0.713161289691925	And this is something that we work on at the Smart Interactive Technologies Research Lab in Sheffield in the UK.
641320	647400	C	0.9669695496559143	This slide just highlights some of the potential applications of Neuromorphic computing, which I thought were quite interesting when you think about it.
647470	658460	C	0.6567866206169128	So the understanding of context, pattern recognition, advanced sensing, fusot, learning, generalizing across tasks, complex decision making, explainability, and brain interfaces.
658880	665752	C	0.954884946346283	So all these skills are really beneficial when you're thinking about human centered, real time applications in dynamic environments.
665896	668568	C	0.8581395745277405	So things like self driving cars, for example.
668754	675468	C	0.5135999321937561	And personally, I think that neuromorphic systems are also likely to be the future substrate of brain computer interfaces.
675564	683872	C	0.7045568823814392	Probably a bit biased because I'm a neuroscientist, but they're low energy, they're real time, and they also have architectures which match our own hardware.
684016	692132	C	0.589652419090271	So I do think we'll soon see the BCI field being catalyzed by neuromorphic systems, particularly maybe for hybrids of hardware and wetware.
692276	698890	C	0.7855725884437561	So maybe even potentially containing people's own brain cells, which you can actually grow just from a hair cell.
701180	706472	C	0.5939959287643433	And a particular focus of our work is designing AI, which learns in a similar way to a human.
706606	711340	C	0.5079829692840576	So it has an innate sense of curiosity, and it learns through interacting with the real world.
711490	721580	C	0.7091907858848572	So in the 50s, Alan Turing said, instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's brain.
721740	726876	C	0.7672171592712402	If this were then subject to an appropriate course of education, one would obtain the adult brain.
727068	734980	C	0.5350028276443481	This is very much the philosophy behind the neurodevelopmental approach to AI and neuromorphic computing, which I just wanted to highlight.
736840	738548	C	0.5573817491531372	There are some challenges in the field.
738634	741956	C	0.6235443353652954	These are very high level, but I'll just give you a little bit of an idea of it.
742058	746772	C	0.6796377301216125	So, training spiking neural networks is more complex than traditional neural networks.
746916	753872	C	0.5506254434585571	Also designing hardware which actually implements spiking neural networks STDP on a large scale is said to be fairly challenging.
754036	759624	C	0.5569220185279846	And then also developing algorithms which can actually effectively leverage all these technologies.
759672	764110	C	0.7166865468025208	So the hardware, the STDP, it's an ongoing active area of research.
766640	770700	C	0.5301631093025208	So if you're here, you're probably interested in active inference.
770860	772432	C	0.5159217715263367	So I wanted to highlight this.
772486	775360	C	0.9801679849624634	Actually, someone put on the discord today one of these studies, which was pretty cool.
775510	781104	C	0.8241217136383057	So a couple of recent studies have combined neuromorphic computing with principles of active inference.
781232	787060	C	0.8813198804855347	So active inference comes from neuroscience, and I would argue that it lends itself very well to neuromorphic architectures.
787640	793172	C	0.7056871652603149	In a recent paper on embodied neuromorphic intelligence, so it didn't really mention active inference in it.
793226	809896	C	0.7334690690040588	The QR code on the top right here, it was suggested that a real breakthrough in neuromorphics will happen if the whole system design is based on biological computational principles with a tight interplay between the estimation of the surroundings and the robot's own state and decision making, planning and action.
810088	818396	C	0.8107957243919373	So some of those themes might sound quite familiar to people interested in active inference, and I would suggest that active inference is well placed to meet these requirements.
818588	821584	C	0.7397137880325317	And I just wanted to highlight a couple of recent studies here.
821702	832016	C	0.7370291948318481	So this one on the left, Gandalfiatal, recently demonstrated plasticity and rapid unsupervised learning in a neuromorphic system using active inference principles.
832128	840768	C	0.6370322108268738	The author suggested that their experiments could be adopted to implement brain like predictive capabilities in neuromorphic robotic systems.
840944	845476	C	0.9068568348884583	And then there was the Dish brain paper, which some of you may be familiar with by Kaganatal.
845588	851812	C	0.90184485912323	So this was a hybrid wetware hardware neuromorphic system that the authors claimed was embodied.
851956	857500	C	0.7094034552574158	The system showed rapid apparent learning of the game of Pong using the free energy principle for learning.
857650	862300	C	0.7059891819953918	And the authors claimed that the system exhibited synthetic biological intelligence.
862800	868292	C	0.8129979372024536	So the field implementing active inference principles in neuromorphic systems is very nascent.
868376	877504	C	0.5962698459625244	And the idea behind this more stream series is to create a space and a community to share knowledge, ideas and expertise to catalyze the field.
877702	882228	C	0.9762821793556213	I think some really exciting technological leaps are probably going to come from this area.
882394	886616	C	0.9683854579925537	So thank you for listening to my quick run through Neuromorphic 101.
886618	891020	C	0.8867402672767639	And next up, we're going to hear from David.
891120	892536	C	0.8089405298233032	So David, over to you.
892558	894730	C	0.8582507371902466	If you want to introduce yourself, please.
895260	896600	A	0.892195463180542	Thank you, Sarah.
898220	900890	A	0.7641181349754333	I would share my screen.
904070	904820	A	0.5491447448730469	Yeah.
914950	920370	A	0.8234164714813232	Can you see now my screen, the presentation yeah.
920440	921858	A	0.8497673273086548	Okay, perfect.
922024	922750	A	0.584351658821106	Okay.
922920	923542	A	0.5208580493927002	Hello.
923676	925122	A	0.8444040417671204	My name is David Kappel.
925186	932790	A	0.9385750889778137	I'm a researcher and group leader at the Institute for Moyo Informatics at the Rural University Bohom.
933770	940646	A	0.8382688164710999	So I'm leading the group on sustainable machine learning and we have a very strong focus on neuromorphic computing.
940678	941900	A	0.6740779876708984	That's why I'm here today.
942590	950480	A	0.9682577848434448	And I'm going to start with a very similar motivation than Sarah did, which was really a great inspiration for this talk, I think.
950930	968274	A	0.9094209671020508	So probably most of you have seen this interesting recent result, and I don't mean Germany winning the basketball championship, but this really big leap in artificial intelligence that we have seen in the last years, especially the last two or three years.
968392	974742	A	0.9831193089485168	So this is a picture generated from a prompt by a Dali network and it's really amazing.
974796	978600	A	0.7554166316986084	It was considered science fiction like only two, three years ago.
979370	985666	A	0.5169882774353027	And this was essentially made possible by a neuromorphic approach, which is deep neural networks.
985698	992310	A	0.6926565766334534	And these deep neural networks have become huge now, but this comes also with a caveat.
992390	1001774	A	0.8843782544136047	So basically the flip side is that among other problems maybe these models may have, they consume huge amounts of energy.
1001892	1013902	A	0.8361555933952332	So models like Dali or Jet GPT, as Sarah already mentioned, they would consume energy budgets that are comparable to houses or cars.
1013966	1020142	A	0.8982113599777222	So training Chat GPT a single time is like a gigawatt hour approximately.
1020206	1032280	A	0.5197464823722839	So that would be 300 tons of CO2 emission and many times, which comes down to many times the lifespan of a typical car.
1032890	1034390	A	0.7797640562057495	So this comes with two problems.
1034460	1041174	A	0.5830011963844299	Obviously, this makes training these models only accessible to a very small number of very large players.
1041222	1043020	A	0.7602649927139282	So essentially the big tech companies.
1043470	1050314	A	0.9016392230987549	And secondly, and maybe even more importantly, this is not compatible with a planet with limited resources.
1050362	1068020	A	0.6347023248672485	So if the growth rate of AI continues like it did in the last years, it will consume 13% of the global energy consumption by 2030 and it will basically outrun the transportation sector in another five years or so.
1069110	1073202	A	0.7374133467674255	So this raises the question, does sustainable machine learning exist at all?
1073336	1078440	A	0.7552915811538696	And obviously, since I'm working in the group of sustainable machine learning, I believe it does.
1078890	1096940	A	0.6627404689788818	And why I think it does is because we know a system that is very efficient and is still probably better than these AI models, which is the human brain, which consumes, as Sarah mentioned, around 20 watts or four bananas a day.
1100290	1105760	A	0.728139340877533	It's many orders of magnitude more efficient than the AI models we have today.
1107490	1114242	A	0.4944416582584381	But so far we don't know essentially how these networks work and especially how to train them.
1114296	1119700	A	0.7296255826950073	And basically our goal is to transfer now mechanisms from machine learning.
1120230	1122260	A	0.9510844945907593	We have this nice picture here.
1122790	1127800	A	0.533246636390686	Basically we start from the machine learning side where we already know our way around.
1128330	1142518	A	0.39084944128990173	So we have these models that are wonderful and that give us really impressive results, but they are not efficient and we want to transfer them to a new efficient AI generation.
1142694	1151360	A	0.8307061791419983	And our idea is to use inspiration from neuroscience that make this transfer faster and possible in the first place.
1153010	1153760	A	0.7123861312866211	Okay?
1155490	1162494	A	0.958970308303833	Actually, biology is a great source of inspiration and always comes around the corner with very surprising results.
1162542	1172318	A	0.7685986161231995	And one of these results that I stumbled upon a couple of years ago is that reliability of synapses in the brain.
1172414	1176470	A	0.84587162733078	So, as you probably know, neurons in your brain are connected for synapses.
1177690	1191558	A	0.8817243576049805	But if you look at this is a paper from 2019, and they could actually identify individual synapses and they could trigger them to make a synaptic release, like a single transmission.
1191654	1198694	A	0.703108012676239	But if you look at these measurements, you see that this is really covered in noise.
1198742	1207646	A	0.8486735224723816	So essentially, if you basically average over this and zoom this out, you see here these typical synaptic traces, which is the average white line here.
1207748	1211066	A	0.5816059112548828	But below that, you see this huge jitter.
1211098	1216020	A	0.6908650398254395	So they really like go several standard deviations up and down.
1217510	1226114	A	0.7500163912773132	And this is actually very surprising given that neurons are probably the single most costly cell type in your body in terms of energy consumption.
1226162	1233702	A	0.48711249232292175	They are really in comparison, they consume quite a bit of energy in your body.
1233836	1242902	A	0.6145456433296204	So you would expect that these transmissions that are communicated between neurons, which are very costly, should be highly reliable.
1242966	1249360	A	0.6579213738441467	So this is very counterintuitive these results and has been puzzling neuroscientists for quite a while now.
1251730	1265026	A	0.8091652393341064	And then there is a second puzzling observation, which is that the morphology of neurons looks somewhat like this.
1265048	1269410	A	0.6620036959648132	So this would be your typical pyramidal neuron you have in your cortex.
1271430	1276562	A	0.5597980618476868	But you see that this is actually, for a cell, quite big and elongated.
1276626	1287946	A	0.6602258086204529	So this can be up to a millimeter in the human brain, which means that if a synapse fires somewhere here, it has a very hard time communicating with the cell body, which is down here.
1287968	1300026	A	0.6796525716781616	So the electrical signals that are produced here may travel down here, but the synapse up here has no way of measuring the actual voltages at the cell body.
1300128	1303626	A	0.9095873832702637	And this is actually the interesting place because here are the action potentials formed.
1303658	1314500	A	0.8640778660774231	So if the synapse would really like to know about what is going on in the cell body so that it can make predictions about how the neuron will behave and how it interacts with the world.
1315030	1329882	A	0.5260097980499268	And this is another very puzzling or open problem in neuroscience, how this communication actually works out in single neurons between the cell body and the SynOps.
1329966	1333814	A	0.8030785918235779	It is known that actually the action potentials can travel back up.
1333852	1342940	A	0.704358696937561	So they see kind of this binary variable when the neuron spikes, but they cannot actually measure the membrane potential down here.
1343710	1348140	A	0.805505633354187	So only the most prominent electrical signals can actually back propagate through this.
1349090	1358222	A	0.5656253695487976	And this suggests that the synapse actually has very sparse information about what's going on in the cell body.
1358276	1364340	A	0.6113060116767883	And most models of synaptic plasticity don't cover this at all.
1365270	1370882	A	0.8443220257759094	And we were wondering how does this interaction work?
1370936	1380930	A	0.70644211769104	How can the synapse produce useful learning signals given this sparse information about this important state of the neuron?
1381090	1394086	A	0.7336755394935608	And our idea was that essentially these two observations, these high levels of noise and the synapse and this large distance between cell body and synapses which give these high uncertainties, these are actually the two sides of the same coin.
1394198	1411550	A	0.8412296175956726	So our hypothesis was that actually we could use the same models that we know already from the behavioral level, how an agent can act and perform in an environment of high uncertainty.
1411630	1418430	A	0.8094179034233093	And we just apply this to every sign ups and there is an error.
1418510	1423960	A	0.8492321372032166	So every sign up should utilize these same models, basically.
1425370	1436790	A	0.4910983443260193	And this model would immediately suggest that actually synaptic transmissions should be noisy and these levels of noise would express uncertainty about the environment.
1438170	1444810	A	0.7962921261787415	And then we can use this model to derive learning rules and we can compare them side by side to biology.
1445470	1447546	A	0.7361325025558472	And this is the first thing I want to show you.
1447648	1460654	A	0.7607405185699463	So I just give a very quick introduction to the free energy model, because some of you might not be that familiar with it, but this is essentially a model to describe a situation like this.
1460692	1465554	A	0.7552477121353149	You have a person that is interacting with some environment and here I assumed very simple.
1465672	1469490	A	0.69688481092453	So this person tries to throw a ball to some target.
1469830	1473982	A	0.8527365326881409	And we as humans, we are good in solving such tasks.
1474046	1478134	A	0.58524489402771	And we are also good in solving such tasks if there is high levels of uncertainty in this.
1478172	1485266	A	0.7300754189491272	So if the person may receive some visual feedback, but a lot of this feedback may be hidden.
1485298	1488700	A	0.70045006275177	So you can imagine this going on behind some wall.
1489310	1500350	A	0.9025145173072815	And the person now still might want to predict what is the trajectory of this ball flying towards the target so that it can make an accurate action.
1500690	1504846	A	0.88422030210495	So we will assign some variables to these states here.
1504868	1508202	A	0.9099205136299133	So we have essentially this feedback that this person can observe.
1508346	1521246	A	0.7626451253890991	And we have this unobserved state of the ball flying here, which we call U, where the person doesn't have direct access to it, would only see parts of it, for example, when the ball is at the corners of the ball appearing.
1521438	1532994	A	0.8788185715675354	And then to model this essentially, or to describe this behavior, the person would have an internal description of this trajectory.
1533042	1537340	A	0.8527774214744568	So an internal model of this state, U.
1538590	1545210	A	0.9099452495574951	And this model would then be updated to match the observed feedback.
1546510	1552154	A	0.9493045806884766	And this can be described very nicely in this beautiful mathematical framework of the free energy principle.
1552202	1566290	A	0.8929907083511353	So the idea is that you would essentially establish a model of your internal state, so essentially how the internal states and the state of the environment interact.
1567270	1573746	A	0.8571004867553711	And you would have a model of the feedback, so how the states and the feedback you observe interact.
1573938	1588060	A	0.8600963354110718	And essentially you can then write down a loss function that measures the distance between this model of the internal state and this model of the feedback and the external state.
1590750	1606462	A	0.5054342746734619	And then by essentially minimizing this distance between the two, you can derive all sorts of behaviorally relevant, can solve behaviorally relevant problems, for example, learning.
1606516	1610990	A	0.5768263339996338	But you can also use this for other things, like figuring out what are good actions, for example.
1611060	1615534	A	0.8218620419502258	So making inference about both the internal states and the actions.
1615582	1619246	A	0.8491992950439453	For example, and this is in a nutshell, the free energy principle.
1619278	1627026	A	0.7908898591995239	And this object here happens to be what is known as the variation of free energy, which is also just coincides with statistical physics.
1627058	1629400	A	0.6828494668006897	And this is where this framework has its name from.
1629770	1632566	A	0.8037832379341125	But you see that all is probabilistic here.
1632588	1640310	A	0.893646776676178	So essentially you have two probability functions q for the internal model and P for this interaction between states and observations.
1640390	1645530	A	0.8180657029151917	And you have here a distance measure between them that you want to minimize.
1646110	1652282	A	0.6356396079063416	So now if we look at the neuron and the synapse and how they interact with each other, we find a very similar picture.
1652426	1664570	A	0.8297128677368164	So single SynOps, which we have here in green, has an internal state which we, for simplicity, model only as the synaptic weight.
1664730	1672338	A	0.9158005118370056	Based on this, the SynOps, when it's triggered by a presynaptic spike, it would generate a post synaptic current.
1672504	1681170	A	0.6191928386688232	This would then propagate to the soma, which is our external state, which we cannot directly observe because it's too far away from the synapse.
1681250	1688986	A	0.880132794380188	But we can see a feedback, which is this back propagating action potential, which is this binary variable that tells us whether the neuron has spiked or not.
1689168	1693194	A	0.7901712656021118	So this is exactly the same framework if we write it down like that.
1693232	1696220	A	0.7726593613624573	And we can just use the same mathematics to solve it.
1698030	1705514	A	0.6819815039634705	So to solve it, we only have to come up with a couple of we have to make a couple of assumptions.
1705562	1708494	A	0.7677342295646667	So we have to write down a model for this guy here.
1708532	1716450	A	0.8548927903175354	So this model of how the feedback and the external state interact.
1716870	1718674	A	0.7830522656440735	But we have very good models for this.
1718712	1721170	A	0.8367156386375427	This has been studied over many years.
1721320	1724334	A	0.8774057626724243	So here you see how typically a model neuron behaves.
1724382	1728514	A	0.8374277949333191	So you have here the membrane potential of a leaky integrated fire neuron.
1728562	1731046	A	0.5178213715553284	And you see that this is just going up and down.
1731068	1736146	A	0.7484773397445679	So this neuron would receive a lot of presynaptic input and maybe also noise.
1736258	1739462	A	0.8935201168060303	And eventually at some point it hits a threshold, it would generate a spike.
1739526	1744410	A	0.911812424659729	So that would be the sets that travels to the downstream neurons and also back to the SynOps.
1745310	1749040	A	0.7320018410682678	And then it resets, right?
1751890	1756170	A	0.7117286324501038	So we can write this down mathematically as a very simple differential equation.
1756330	1760158	A	0.8156018853187561	But the neuron doesn't have access to this state again.
1760244	1764214	A	0.5162473917007446	So it's again behind this wall, it only sees these spike events.
1764362	1769550	A	0.5022411942481995	But we can actually, for this simple case of a leaky integrated fire neuron, we can solve this analytically.
1769630	1775000	A	0.9023817181587219	So we can write down what is the posterior distribution of membrane potentials given the spike times.
1775370	1785778	A	0.7230814099311829	And what comes out of this is actually a so called stochastic bridge model, or in this case of a leak integrated in fire neuron.
1785794	1787622	A	0.8146549463272095	It's an orange bridge model.
1787676	1789750	A	0.8670498728752136	So this can be written down analytically.
1791310	1793690	A	0.8369513154029846	It's not simple, but it's doable.
1795070	1797382	A	0.8111506700515747	And we can then just use this directly.
1797446	1802826	A	0.7852368950843811	So this model that we have to again write down these free energy functionals.
1802858	1810894	A	0.8818232417106628	So we make here an assumption how the synapse actually produces post synoptic currents and how they are integrated in the neuron.
1810942	1818526	A	0.8592295050621033	But that's also given by the leaky integrated fire neuron and actually the stochastic inputs that the synapse generates.
1818638	1829458	A	0.8355377912521362	So for simplicity, we only assume here basically Gaussian synapses that would inject draw a Gaussian random variable and inject this to a leaky integrated fire neuron.
1829634	1836694	A	0.608906090259552	And then all these ingredients actually can be solved in closed form and we can derive learning rules that would minimize this free energy functional.
1836742	1852050	A	0.8672216534614563	Now, and if we do that, this has a bunch of nice properties because this Einstein Beck bridge is completely determined by the back propagating action potentials.
1852070	1871762	A	0.8441450595855713	So basically the times of the post synaptic spikes that arrive at the neuron, this shape that we get here only depends on two neighboring post synaptic spikes, which means that we get here automatically a learning rule that looks like this.
1871896	1888700	A	0.8813449740409851	So a learning rule that only depends on the difference between two postal optic spikes, which we call here delta t two, and the difference between the post synoptic spike and the actual input that is triggered at some point on the presynaptic side.
1890110	1904130	A	0.7602095007896423	And we can basically make here this lookup table and just compute what would be the update that these sign ups would need to make so that it learns optimally in terms of this free energy principle.
1904230	1905774	A	0.8241153359413147	And this is the shape that we get out.
1905812	1919250	A	0.8606308698654175	You see that there is a strong dependence on the post synoptic firing rate, but there is also a dependence on basically this typical STDP that Sarah mentioned before, what is the relative positioning of the pre and postsynoptic spike.
1919990	1925362	A	0.7476826310157776	So in a nutshell, this model can now be split into essentially two pathways.
1925426	1940790	A	0.8911333680152893	So we have this ATOC response, which basically just whenever there is a presynaptic spike that triggers an action in the synapse, we would draw from this Gaussian distribution and inject it into the neuron.
1940950	1957150	A	0.8769909739494324	And then there is this postalk update where the synapse would look up in this onstain Woolenbeck bridge, what would have been the optimal output that it should have generated, so what would have been the optimal action.
1957490	1968770	A	0.8905385732650757	And then it compares the actual action with this optimal action according to this free energy principle and then generates a delayed response which is an update of the synaptic weight.
1970150	1970610	A	0.7123861312866211	Okay?
1970680	1974034	A	0.7906700968742371	And importantly, this internal model is only implicit.
1974082	1978626	A	0.846738874912262	It's encoded, so to say, into this spike time dependent.
1978658	1979910	A	0.6983193755149841	Plasticity rule.
1981370	1985826	A	0.8402833342552185	So how do these rules then look and how do they compare to biology?
1985938	1993302	A	0.9203745126724243	And actually the fit is quite nicely given that this is derived really from first principles without making any assumptions.
1993446	1995958	A	0.8537347316741943	So this is the measurement in biology.
1996054	2012314	A	0.9056238532066345	This is this B and Al rule, b and Pool, very old work where they actually did this in vitro studies where they injected pre and post synaptic spikes and then they measured what is the weight change in the synopsis.
2012362	2015682	A	0.7433961033821106	And this is the rule that is predicted by our model.
2015736	2021038	A	0.5223312377929688	And you see that at least in a first order approximation, it gives us very similar shapes.
2021214	2041542	A	0.713266134262085	And this also makes sense because the synapse wants to change the most when it's close to the pre and post synoptic spike times because this is where or the post synoptic spike times because this is where it knows the most about the state of the postynaptic neuron.
2041606	2052090	A	0.7474308609962463	The free energy principle would actually suggest these kind of cones with almost no assumption essentially.
2052250	2062974	A	0.6702552437782288	But we also get because we have not just the first order spike time dependent plasticity rule, but we also have this dependency on the post not of the firing rate.
2063092	2065486	A	0.8482645750045776	We can also compare this to other results.
2065518	2068766	A	0.8512459397315979	And this here is this old work by Kaupner and Brunel.
2068798	2081138	A	0.8003057241439819	So this is actually a model, but that was very detailed describing the plasticity based on the pre and post synoptic firing rate in the synapse.
2081234	2082818	A	0.7215320467948914	And this is what our model predicts.
2082834	2092922	A	0.6966717839241028	So if we inject render and pre and post synaptic bossBack trains with different rates, our model would predict this shape, which again is not a perfect match.
2092976	2109280	A	0.5038574934005737	But given that this is a very idealistic model, it's actually the main features that low firing rates on the postynopstic side would lead to depression and higher to potentiation are reflected in this.
2111190	2115234	A	0.8055386543273926	Okay, I assume I still have ten minutes, right?
2115352	2128660	A	0.7185015082359314	Okay, so I would give a quick intermediate summary and then I want to show some other work where we actually apply this now to actual machine learning model.
2131030	2141062	A	0.6720016002655029	So what we have seen here is that synapses are actually are very stochastic and this was a big puzzle.
2141126	2165620	A	0.7491183876991272	And B suggests that actually the synaptic noise is actually the synapses way of reporting its own uncertainty about the environment, where the environment is actually the post synaptic neuron and it really interacts in this fashion of the free energy principle with this post synaptic neuron or that's a very nice way of describing it.
2165990	2173060	A	0.5053932666778564	And if you're interested more, there's a paper, a preprint out, you can read up on all this.
2174250	2177986	A	0.8864251971244812	Okay, so how does this now connect to neuromorphics?
2178098	2184194	A	0.7810578346252441	And actually so we are not actually doing neuromorphic hardware, we are doing neuromorphic algorithms.
2184322	2196620	A	0.865884006023407	So we try to bring these inspirations now into actual machine learning models and we thought that this might be a good attack angle to solve a problem that is well known in machine learning.
2197070	2211738	A	0.7913126945495605	So I just drew here very simple convolutional neural networks with various convolutional layers and then maybe some dense layers that you would have in your machine learning algorithm.
2211914	2218158	A	0.7163277864456177	And the way this is trained, as many of you know, I guess, is through end to end errorback propagation.
2218254	2222874	A	0.8778929710388184	So the idea is that you have a training set which has inputs and targets.
2222942	2228306	A	0.8755853772163391	So, for example, in a classification task, this could be pictures of cats and dogs.
2228338	2232934	A	0.6465615034103394	And you would have targets which are class labels, so to say.
2232972	2240650	A	0.8638057112693787	So there's actually artificial neurons in there, and one of these neuron may be active for cats and 1 may be active for dogs.
2241470	2248846	A	0.6616100668907166	And in your training, that data, you have exactly these labels that were generated by humans, that were sitting down doing this by hand.
2248948	2259066	A	0.8898390531539917	And then during training, you show these examples to the network by propagating these inputs all the way from the input layer to the output layer.
2259258	2264158	A	0.7456728219985962	Then the output is here compared to these hand labeled targets.
2264254	2270446	A	0.5410102009773254	And then the mismatch between the two is back propagated through all these layers back to the input.
2270558	2285510	A	0.6272135972976685	And all the weights or the synoptic weights that are here in between inside these layers would then be updated accordingly, so that after doing this many, many times, this network becomes good in telling apart Kelvin docs.
2286410	2289094	A	0.8920595049858093	So this has a problem, this algorithm.
2289142	2298182	A	0.7479677796363831	It works great in practice and it's the foundation of all these models we have talked about, like Dali or Jet GPT, but it is quite inefficient.
2298246	2302286	A	0.5215379595756531	And the problem is what is known in the literature as the locking problem.
2302468	2315554	A	0.5014252066612244	So if you would split up this network now into blocks, which I already did before, but this is arbitrary, but for implementing this efficiently in terms of a software algorithm, it might be interesting to do that.
2315752	2329734	A	0.8330700397491455	And now you would want these blocks ideally to run in parallel, so that you can basically show the first example on this first block and then already train it while the second block is doing something else.
2329852	2339734	A	0.6796538829803467	But this is not really possible with end to end back propagation because of this lock in problem, because the activation of the second block depends on the activation of the first block.
2339782	2342474	A	0.6270236968994141	So you have to propagate it all the way to the end.
2342592	2345702	A	0.7603226900100708	Then you compute this error and you would then back propagate.
2345846	2366180	A	0.6412903070449829	And only when this is done, you can start the next epoch where you show a new bunch of examples and you see that during all this time, here, the thread that would run this first block maybe would be idle and has to wait essentially all the time.
2366630	2368654	A	0.9389354586601257	And this obviously makes them very inefficient.
2368702	2384354	A	0.7472571730613708	And now our idea was that we use basically what we had learned from this earlier model on how synopsis communicate over these long distances through this free energy principle, and also apply it just to a deep neural network.
2384402	2395558	A	0.8152276873588562	And the idea is that you have here, again, basically you have already this generation of inputs to some output.
2395654	2402254	A	0.7247610688209534	But what is missing to make it applicable to the free energy principle is this feedback that you always need.
2402372	2407770	A	0.615394115447998	The idea was that we put here a very lightweight feedback network.
2407930	2418414	A	0.9145779013633728	So essentially each of these blocks now in this deep neural network would be accompanied by a feedback block that locally generates a target.
2418542	2425074	A	0.6387709379196167	So we used here really in the simplest case, which we have, so this is very recent work, we only used linear blocks so far.
2425112	2437558	A	0.8911938667297363	So these are single linear layers and we would generate now these outputs here in these feedback blocks and then use the free energy principle to derive a local loss.
2437654	2445050	A	0.6962530612945557	That allows us again to minimize both these feedback weights that we have here and also the weights in the forward network.
2446270	2448442	A	0.7318012118339539	So it's essentially the same idea.
2448496	2453950	A	0.8927439451217651	So we have here these outputs which we interpret now as parameters to a probability function.
2454020	2460346	A	0.829003632068634	So we can apply this probabilistic framework, but now all the rest basically rolls out the same way.
2460388	2473746	A	0.9036275744438171	So we assume that these outputs are essentially the internal states of this model and we have these observations given in the inputs and the targets.
2473858	2491450	A	0.8457437753677368	And now we try to minimize basically P would now be this feed forward network and Q would be now a function that contains features of both the feedback and the feed forward network.
2492030	2510834	A	0.508690357208252	And the nice thing is that if we I don't have time to go into the details now, but if you write this out, you see that this actually decomposes, this lock term here decomposes into local linear terms that give you these local losses here.
2510872	2520162	A	0.8037164211273193	So essentially that you can minimize here block local between forward and the corresponding feedback block, a loss function.
2520296	2527318	A	0.8058704733848572	And you can then actually do this in parallel because maybe the picture is good to see here.
2527404	2531154	A	0.5784321427345276	So what you have to do now you have a bit of overhead because you have this feedback block.
2531202	2540490	A	0.8752388954162598	So this would be the two execution times of the feed forward block and the feedback block.
2540990	2563102	A	0.7983086109161377	But in principle they can run in parallel and once the forward block is done, the next forward block can start propagating through this network, but simultaneously already the forward block, because it already received a target here, can start updating the weights and when it's done, it's free to operate on the next epoch.
2563166	2567010	A	0.7792109251022339	So there is no locking anymore in this framework.
2568790	2570882	A	0.6102103590965271	Okay, so I'm pretty much done.
2570936	2572418	A	0.7207058072090149	I'm also out of time, I think.
2572504	2574086	A	0.8506492972373962	So how does this perform?
2574268	2576274	A	0.7663666605949402	Of course we changed the learning algorithm.
2576322	2583030	A	0.72419673204422	Now we have to also go back and see if this is still giving us the same performance.
2585470	2600526	A	0.9606539607048035	As I said, this is the first results we have here now and at least to mid scale data sets like cipher ten or so, this seems to actually perform very well.
2600548	2606906	A	0.7370507717132568	So we attract it for standard architectures, fashion MNIST with ResNet 15 and ResNet 18.
2607018	2615922	A	0.5933750867843628	We worked mostly so far for small data sets like Fashion MNIST with applying free splits to ResNet 50.
2615976	2619726	A	0.7955886721611023	We get basically the same performance as standard backprop.
2619838	2628546	A	0.5854429006576538	As networks get deeper, you see that our problem that we have now is actually overfitting because we have these local targets.
2628578	2632050	A	0.914317786693573	It seems that these smaller blocks actually overfit to some extent.
2632210	2636106	A	0.5266082286834717	This is not so severe for still up to tasks like cipher ten.
2636128	2638394	A	0.7176154255867004	So we get quite close already.
2638512	2646726	A	0.5774989724159241	But if you go now for really large tasks, there's still something missing for like single splits.
2646758	2651274	A	0.6679957509040833	We are getting there, but we are not reaching all the way up to back propagation.
2651322	2660750	A	0.8613711595535278	But it's still interesting to see that you can apply this principle also to these standard machine learning algorithms.
2661910	2664050	A	0.860592246055603	Okay, this is my second summary.
2664470	2671566	A	0.9583816528320312	So actually we found that deep neural networks are surprisingly good in generalizing over probability spaces.
2671678	2673960	A	0.8204445838928223	This is how actually this work started.
2674890	2691770	A	0.5469626784324646	And our idea was to exploit this and to utilize it to distribute learning in the same fashion as in the first project I showed you and to solve this credit assignment problem by generating these feedback networks.
2693950	2695754	A	0.6135087609291077	Yeah, that's basically it.
2695792	2700110	A	0.806670069694519	And then I want to acknowledge my coworkers and my students.
2700180	2710326	A	0.9545431137084961	So I have two very good PhD students, carlisle and Cabrill, who is now here in Bohom and works on this topic.
2710378	2717662	A	0.7340967059135437	And the first project I showed was work I did together when I was in gettingham with Christian Tetlav.
2717726	2728440	A	0.879848837852478	And the second project is I worked closely with Christian Meyer and Anand Sutomoni and yeah, thank you.
2731370	2732706	C	0.9730646014213562	Thank you very much, David.
2732738	2734370	C	0.9631105065345764	That was absolutely fascinating.
2734530	2738578	C	0.855825662612915	I think it's incredible how closely the sort of model matched biology.
2738674	2742620	C	0.9766654372215271	Like considering you derived it from first principles, I think that was really cool.
2743070	2746234	C	0.8144336342811584	I did have a question just about the last sort of the bit.
2746272	2753354	C	0.8211303949356079	You spoke about the Convolutional network and you said traditionally it has to go all the way end to end, which is really inefficient.
2753402	2755246	C	0.737413227558136	And then you showed the results that you got.
2755268	2759280	C	0.8938682079315186	Did you look at energy consumption with yours as well?
2759810	2760462	A	0.6702306270599365	Not yet.
2760516	2772050	A	0.5246490836143494	So we are actually currently working on it's actually not so easy to implement these things in standard machine learning toolboxes.
2772710	2787558	A	0.732481837272644	So Carlisle is currently looking into this, the PhD student in Jester, he has an implementation now, and he's now evaluating how well we can make use of this parallelization in practice.
2787734	2797498	A	0.7663531303405762	But we are actually quite confident that for parallelizing it, it should be there.
2797664	2800230	A	0.7799625396728516	The question is how much you save in terms of energy.
2800320	2808990	A	0.7148597836494446	Because for these smaller scale models that we use now, ResNet 18, ResNet 50, the effect might be not that huge.
2809060	2813774	A	0.532607913017273	So once we ramp this up to really larger models, the effect should be bigger.
2813822	2815620	A	0.845369279384613	But yeah, this is ongoing work.
2816390	2817138	C	0.9677404165267944	Very cool.
2817224	2818146	C	0.8529649376869202	Thank you.
2818328	2830120	C	0.8302311897277832	And then I was just wondering as well, this local error back propagation, is that something that other people have tried with these convolutional neural networks or is this quite a new way of implementing it?
2830650	2833240	A	0.8535732626914978	There is a bunch of approaches that do this.
2834250	2843930	A	0.8888899087905884	For example, I mean, the closest I guess is target propagation which has been proposed, which essentially uses random feedback weights to back propagate here.
2844000	2846330	A	0.8698208332061768	So these guys would not be trained.
2848830	2857118	A	0.9259157776832581	And this works nicely also for small scale problems.
2857204	2859294	A	0.8504185080528259	But as far as I know, they don't perform that well.
2859332	2867474	A	0.8191599249839783	Even for cipher ten, it already starts breaking down because these random feedback weights are just too coarse an approximation, I think.
2867592	2872638	A	0.6935515999794006	And this is the first okay, maybe I have to be careful.
2872734	2878706	A	0.7463994026184082	I think this is the first method that allows you to train these feedback weights.
2878738	2880790	A	0.5945667624473572	That is not a contrastive method.
2881210	2884360	A	0.8656333088874817	There is a bunch of methods that use a contrastive step.
2886510	2908602	A	0.8092776536941528	So you have maybe seen this forward forward algorithm and all these things, but what they have to do always is they send in the actual input data and then they send an kind of anti input, an anti input that is usually generated artificially.
2908666	2917646	A	0.6687823534011841	So they do some distortion to the input to make it and then they have to use both information locally to do the update.
2917678	2923802	A	0.8435261845588684	So the network has to keep in memory the input and the anti input and the responses.
2923966	2930534	A	0.6766847968101501	And this makes these approaches a bit harder to parallelize in this.
2930572	2943206	A	0.7946926355361938	And the nice thing here is that we derive an upper bound to this variational free energy loss that can be spelled out completely by forward propagation.
2943398	2944842	A	0.685150682926178	And that I think is new.
2944976	2947946	A	0.8507927656173706	So that is the new bit of this.
2948128	2949180	C	0.9677404165267944	Very cool.
2952530	2954160	B	0.9799191951751709	That's awesome.
2956530	2962842	B	0.8350792527198792	Well, yeah, few comments, one piece that kind of between the two of your talks.
2962906	2969810	B	0.5975539088249207	That was at least a new distinction to me, which was the difference between the Neuromorphic hardware and the Neuromorphic algorithms.
2970790	2979702	B	0.852240800857544	So it's not just about new hardware or Wetware, though that would be great to see.
2979756	3004954	B	0.7976247072219849	It's almost like there's this intermediate or a bridge step with using the algorithms on the hardware we have today that like Sarah mentioned, the Spiking neural networks which are amenable to GPUs or just using standardized CPU multicore scheduling approaches.
3005082	3013382	B	0.7708698511123657	You can already do more with what we have using the Neuromorphic algorithms.
3013546	3033330	B	0.6065588593482971	So it's not just a material science topic, but also there's a lot at the really micro scale that we can learn related to noise processing scheduling and then also even at higher levels of abstraction, probably learning from biomimicry and cognitive systems more generally.
3033410	3036280	B	0.831575870513916	But that was a distinction for me.
3038110	3038426	A	0.5491447448730469	Yeah.
3038448	3040586	A	0.7126405239105225	So maybe to add yeah.
3040608	3061630	A	0.46694910526275635	So I think that the problem becomes really now more pressing as these Neuromorphic devices become so the hardware devices become more mature and they usually cannot really shine on these standard machine learning algorithms because they are really optimized for GPUs.
3062130	3064238	A	0.7692801356315613	So you need to think a little bit.
3064324	3072414	A	0.844603419303894	So you have to take one step back and think again about the algorithmic side to really use them to full capacity.
3072462	3087814	A	0.7797118425369263	And as you have seen now, we collaborate with Professor Meyer, who is doing this spinnaker chip in Dresden, but there's also other approaches like the Louis chip that Sarah mentioned from intel and so on, and they are really looking into this.
3087852	3091080	A	0.843010663986206	Now, also from the algorithmic side.
3092970	3094182	C	0.939202070236206	I find it really useful.
3094246	3105994	C	0.6516830325126648	It's kind of like a mindset or a mental framework when I'm thinking about computers or AI, and this is probably because I'm a neuroscientist, but I also have to translate it to, well, how does the brain work, how does the information processing work, et cetera, in the brain?
3106122	3111006	C	0.7392842173576355	And when I came to sort of computer science and AI after neuroscience, I found myself naturally translating it.
3111028	3119246	C	0.6861011981964111	But I feel like the framework is just a really useful way of understanding computation at the end of the day because our brains, as I said, are just these massive supercomputers.
3119278	3122306	C	0.7026350498199463	And I'm constantly reading papers on computer science or whatever.
3122408	3129154	C	0.8043836951255798	And then once you can conceptualize anything really as well, how neuromorphic is this?
3129272	3133298	C	0.8434040546417236	And then if you start thinking about, well, how could you tweak it so it's slightly more neuromorphic?
3133314	3136402	C	0.729188084602356	And is that then going to give you these gains that we get with the brain?
3136466	3141122	C	0.8593443632125854	Is it going to give you some extra parallel computation or is it going to give you some energy efficiency?
3141186	3147334	C	0.8108698725700378	So, yeah, I find it the definitions when I was looking at the definition is I think it really depends who writes the definition.
3147382	3163998	C	0.7525786757469177	And because it's such a dynamic area at the moment as well, I think it has been changing and it will be changing, but for me, I feel like neuromorphic is more of like a mental framework where I look at things through conceptualize, through yeah, I think it's.
3164014	3165810	A	0.6893141269683838	It'S also not very well defined.
3167830	3177014	A	0.7495493292808533	You also mentioned that actually artificial neural networks are a neuromorphic concept, if you want, and they were from the first day.
3177052	3192374	A	0.777177631855011	And it's a big success story, right, if you look into the 90s or so when these support vector machines and these alternative models came up, but none of them have outlived the neuromorphic approaches.
3192422	3194234	A	0.9845171570777893	So it's actually very nice.
3194272	3203660	A	0.5567746758460999	But still there is this community that thinks that there is more features from the brain that you need to put in to get to the real thing.
3204690	3212366	A	0.506346583366394	So I think this is a bit of a it's not a very well defined term, actually, and I think with.
3212388	3216458	C	0.6121401190757751	Your research, yours is almost like the smallest level that I've seen people look at it on.
3216484	3221806	C	0.7557634115219116	I don't know if you've seen anything else, but we're not just talking about a cell level of free energy and active inference.
3221838	3225810	C	0.8481549024581909	We're actually talking about a cell structure.
3226310	3228290	C	0.7371799349784851	So then you think, well, how small does it go?
3228360	3243130	C	0.6382299065589905	Are we going to talk about cell subcellular structures eventually, like Mitochondria using free energy in a similar way with compartments, I guess actually it'd be interesting to get your thoughts, david on, you talked at the start about how synapses are compartmentalized.
3243710	3259550	C	0.8042600154876709	Do you see different sort of instantiations of this in different compartments just within one synapse, almost like are you wanting to sort of look at that granular level or is it more now taking what you've learned from this and putting it back into how can we sort of make the AI more efficient?
3260530	3267170	A	0.8246107697486877	Yeah, we are going much more in this direction now that we see how we can build this back into AI models.
3271030	3290140	A	0.53044193983078	One has to be a bit careful when using the free energy principle because it's such a powerful general framework that you can apply it to basically anything and it's not necessarily you will come up with a useful result in the end.
3290750	3297338	A	0.8730670213699341	Just putting this this started actually as a side project.
3297424	3303920	A	0.5828426480293274	This was my kind of COVID pandemic lockdown project.
3304610	3312014	A	0.7376889586448669	And I was just curious about this and whether you can actually solve this, because I thought the synopsis is maybe simple enough.
3312212	3318142	A	0.851594090461731	Because when you go into the papers, they have at some point to go into some approximations.
3318206	3330806	A	0.5253750085830688	They do some mean field, usually, so they go for first modes, and then you can solve these guys for more complex, even for neurons, it's hard, actually, if you go to the neuron or network level, it's hard.
3330908	3336150	A	0.6238003969192505	It's very involved math, but for a sinus is simple enough, actually.
3336220	3343594	A	0.6965997219085693	So you can actually do this and spell everything out if you make the right assumptions and really just derive these things.
3343632	3356798	A	0.9537969827651978	And that was kind of just kind of a game I went into and then it turned out to work quite nicely, I think, in the end.
3356964	3357680	C	0.5491447448730469	Yeah.
3359570	3372020	A	0.5589296817779541	I'm not sure if you like Mitochondria or so I'm sure you could apply the same principles but I'm not sure if the results you get would be any meaningful or would help you in any way.
3373750	3375438	A	0.49665480852127075	That's always the risk.
3375534	3379800	A	0.5794393420219421	You invest so much time and then in the end you get some results and you don't know.
3381290	3388006	B	0.9426373243331909	That was also something that I thought was quite interesting, which was the synapse was the agent.
3388188	3393066	B	0.7976881265640259	It's really easy to think, oh, we'll make an agent based model of a neural system.
3393248	3402880	B	0.5614361763000488	First off, that tends to not include Glia or non neural cell types, but it's almost like a doubly unquestioned assumption that the cell would be the agent.
3404130	3411006	B	0.8353279232978821	But then it was a great transition from the person throwing the ball over the wall.
3411108	3425858	B	0.7284753918647766	That's an action centric approach where you only have partial visibility of the consequences and then that is the exact scenario that the synapse finds itself in in a different way.
3425944	3430040	B	0.7033782005310059	Or it could have been set up so that a neuron is the agent.
3430570	3432870	B	0.7000305652618408	We're building maps, not territories.
3433210	3438360	B	0.8108298182487488	And so then, just like you said, free energy principle, it's a principle for everything.
3440270	3447770	B	0.7148290276527405	And so just making principled statements about things is table stakes.
3448190	3456538	B	0.8739230036735535	And then I guess my question for you is then what does make it useful or in your learning and tinkering around with these models?
3456714	3470050	B	0.8204039931297302	What differentiated situations where you applied free energy principle or active inference and you felt like it was providing a contribution to your research direction versus where you played around and it was like, well, that was tautological.
3471190	3477780	A	0.7034745216369629	I think the free energy principle makes sense in a context where you have incomplete information.
3481030	3498314	A	0.5978736281394958	So for example, in the synapse case, right, the question we started with was this problem that the synapse has to solve, that it has incomplete information about the state in the cell body because it only sees this kind of or that's the assumption, at least of the model.
3498512	3504554	A	0.806304395198822	And also what we get from the experimentalists, that it essentially only sees this back propagating action potential.
3504602	3511710	A	0.825053870677948	So it sees a single binary variable about the state of the soma.
3512950	3525550	A	0.5890114903450012	So essentially this is a problem of incomplete information and also the second ingredient that you need some form of agency.
3525630	3537474	A	0.7020362019538879	I think if you apply the free energy principle to a system without agency, so if something is not interacting with an environment in a closed loop, then it becomes really sketchy.
3537522	3547834	A	0.5954710841178894	And I think already this model is on the edge when it comes because these models don't really have an agency, but they at least produce an output, right?
3547872	3552110	A	0.8586702346801758	So you can still think of this as an interaction with an environment.
3552690	3559680	A	0.723565399646759	But as soon as you lose that, I think then there would be simpler models that can just give you the same.
3566010	3586250	A	0.5938594341278076	Actually, in the synapse case, the agency is only this adding noise actually in the model because the synapse is triggered presynaptically and then it uses its internal state to add the right amount of noise, which is probably already the minimal agency you could imagine.
3589710	3592480	B	0.8590901494026184	Sir, you want to ask a question or I can ask a question?
3593810	3595370	C	0.7564593553543091	Yeah, it was more just a comment.
3595450	3598922	C	0.9552093148231506	I think it's quite interesting people talk about biology.
3599066	3602394	C	0.8395212292671204	Some people say it's not a real science because it's all messy and noisy.
3602442	3609294	C	0.864008367061615	But I think it works really interesting because it's like you say, the synaptic noise is actually a reporting of uncertainty.
3609422	3616206	C	0.5553358197212219	So in that essence, it's actually probably quite accurately reporting the messy world rather than the biology itself just being all messy.
3616238	3618180	C	0.7559846639633179	But that's just what I was thinking about.
3618710	3620386	C	0.7712986469268799	Yeah, I think I was curious as well.
3620408	3626806	C	0.7761576771736145	Like you said, this was like your Lockdown project, but I'm just interested in how you sort of came to use the free energy principle, how you came across it.
3626828	3631754	C	0.9071978330612183	Was it something you were quite familiar with already or some of your network or peers were talking about it?
3631792	3633500	C	0.7680945992469788	Or did you just stumble across it in a.
3636190	3642830	A	0.7108213901519775	Was during my PhD, I was interested in variational methods and probabilistic methods.
3643810	3645854	A	0.6394463777542114	And then I started reading about this.
3645892	3651998	A	0.9382179975509644	And so I read a bunch of Karl Friston's papers and I found this interesting.
3652084	3661410	A	0.5102344751358032	And actually, my PhD supervisor always encouraged me not to go in that direction.
3662550	3668200	A	0.540230393409729	And then after I finished my PhD and thought, okay, now I can do what I want, I try it out.
3670890	3682794	C	0.8084942698478699	And then, I guess, do you think it would be worthwhile, like, next steps for you or for the field actually trying to implement this on maybe some of the more analog chips that are being built in the space?
3682832	3692722	C	0.8569079637527466	Like the analog neuromorphic chips, which I know you can have some dynamic synapses and things like do you think it would be worthwhile trying to implement it on hardware?
3692806	3695440	C	0.8590136170387268	Or what are your thoughts on that?
3695810	3702240	A	0.8423938155174255	I mean, the triplet rule that comes out from this first work I showed, I think that would be interesting to implement it.
3704390	3718370	A	0.8046519756317139	The nice feature is that it should, in principle, should have this self stabilizing feature because it's really mimicking the dynamics of the membrane, of the cell membrane.
3718530	3740426	A	0.566730797290802	So if the neuromorphic hardware would so if the model in the synapse and the neuron model match up very well, the model should give you this nice self stabilizing feature so that neurons really not go into some epileptic states or so.
3740448	3742206	A	0.6081511974334717	And you get this for free from this model.
3742308	3744480	A	0.853223979473114	That's what we saw in the simulations, at least.
3745010	3752398	A	0.5390762090682983	But in the simulations, of course, we had full control over this dynamics matching up in the right way.
3752484	3759214	A	0.4852038025856018	So that is probably a bit more tricky for hardware, but it's probably solvable.
3759262	3760500	A	0.8135852813720703	So it would be interesting.
3760950	3769560	A	0.9121372699737549	What you get for it is that you have these purely event based tools which only use pre post spikes, which is nice.
3770410	3771302	C	0.9677404165267944	Very cool.
3771436	3771990	C	0.8529649376869202	Thank you.
3772060	3772438	C	0.5491447448730469	Yeah.
3772524	3774854	C	0.8700623512268066	Daniel, if you had a question?
3775052	3802846	B	0.7249813079833984	Well, that's a great principle there, which is like, if you can design the neuromorphic algorithm so that it harnesses a material feature like the actual leaky permeability of a membrane or actual spatial proximity, if you can leverage a material feature, analog feature that isn't virtualized, then it's already an adjacency into future hardware.
3802878	3803746	B	0.9422008395195007	So that's one great point.
3803768	3815950	B	0.6981877088546753	And then to Sarah's point about almost biology not being a science, which there is a famous quotation, there will never be a Newton for a blade of grass.
3816110	3820134	B	0.6887983679771423	Because some people say, yeah, it's a different biology is more like history.
3820332	3826034	B	0.854087233543396	Because whether you approach this from a development or ecology or evolution perspective, biology is a historical science.
3826082	3827910	B	0.6567173600196838	It's not like a real science.
3828070	3834774	B	0.6730287671089172	And then that reminded me of the cross country shirt that says, our sport is your punishment.
3834902	3841370	B	0.605498194694519	So it's like, well, no, your noise is biology's signal.
3841530	3843840	B	0.6090611815452576	And that's how it happens.
3845170	3858510	B	0.803932249546051	My question was about this tension between, I guess, neural and computational ways of looking at the resources associated with computation.
3858670	3861758	B	0.8767774701118469	So from the von Neumann paradigm.
3861934	3884780	B	0.857184648513794	We have a lot of shared reference points, CPU cycles, Ram capacity, and all these kinds of and like, even in your introductions, you conveyed like, well, this is how many CPU cycles it's going through, or this is how many parameters would have to be stored, or something like that.
3886110	3890554	B	0.8145067095756531	However, that's referencing another paradigm.
3890602	3905478	B	0.7273221611976624	So what do resource descriptors or capacity descriptors look like when we're outside the space of okay, yeah, power consumption.
3905514	3911294	B	0.5184066891670227	That's something that you can put into a box and just use a bomb calorimeter that's kind of like a low hanging fruit.
3911422	3926440	B	0.8316086530685425	But now okay, beyond just the sheer energy or caloric requirements, what can we say that is, like, analogous to the way that we talk about the processor or the Ram or the hard drive on a computer?
3931640	3934084	C	0.7912412881851196	Yeah, I'd have to think about that one some more.
3934122	3939912	C	0.9397105574607849	I do think there was some interesting comments in the paper on that slide I showed that talks about the brain and energy.
3939966	3941144	C	0.872623085975647	There was a paper I linked to.
3941182	3944508	C	0.797826886177063	I'll have to get the reference and let you know what it is because QR code is gone now.
3944674	3949308	C	0.872270941734314	But that had some interesting ideas, I think, on what you're getting at there.
3949394	3953710	C	0.7733883857727051	But I'd have to defer to the paper.
3955440	3958780	B	0.8215062618255615	How do they describe what is being designed?
3959200	3964850	B	0.5769074559211731	So they say it has this many of this type of component, and then that might do nothing though.
3965220	3970500	B	0.8581826090812683	So how do they describe or evaluate these different designs or algorithms?
3972520	3974580	C	0.8269603252410889	I think it's all different depending on the use case.
3974650	3975444	C	0.7978518605232239	That's what I've found.
3975482	3975684	C	0.6008132696151733	Really?
3975722	3982560	C	0.8994600176811218	Like, the language is different depending on if it's written by someone maybe with more of a neuroscience background or an engineering background.
3982640	3986360	C	0.7263482809066772	And then you kind of get used to some terms that are more interchangeable than others.
3986430	3997740	C	0.4950847327709198	But I do think the terminology is something which needs to be looked at a lot more closely in this space, because then I think that will help everybody working in it to be on the same page a little bit closer.
4002910	4003370	A	0.84200119972229	Cool.
4003440	4007718	B	0.8366825580596924	Well, any other thoughts or questions?
4007904	4010554	B	0.8261914253234863	David first and then Sarah.
4010602	4015200	B	0.8060358762741089	Also, I'm very curious, what direction will this series go?
4016370	4023040	B	0.8964484930038452	But first, David, what are any other kind of closing comments or directions you want to.
4025650	4026206	A	0.6008132696151733	Really?
4026308	4029140	A	0.9007526636123657	I mean, I would say thanks for having me today.
4029670	4032180	A	0.9863854050636292	It was really a pleasure to discuss with you.
4034230	4034706	C	0.8529649376869202	Thank you.
4034728	4036194	C	0.9790282845497131	David, it was amazing to have you on.
4036232	4045080	C	0.9913807511329651	I think your work is absolutely fascinating, and I think it's going to have lots of benefits in the future for implementation, which is always nice to see as well.
4046010	4046374	C	0.5491447448730469	Yeah.
4046412	4047306	C	0.7717005014419556	So what was the question?
4047408	4049194	C	0.9081942439079285	Where do I see the series going?
4049392	4053226	C	0.8421046137809753	Hopefully we can have a new guest each month.
4053408	4063006	C	0.9194446206092834	I think it'd be kind of cool maybe next month to have someone who's building hardware, so, like, maybe someone from the BrainScaleS team or spinnaker team or something like that would be.
4063028	4081618	C	0.9444637298583984	Pretty cool, but yeah, really, I just want to have a space for people who are interested in this intersection to meet people and see talks and reach out to people who are also working in the space, because it's pretty niche, but I think it's pretty important, actually.
4081704	4087720	C	0.8761792182922363	Having said that, David, could you let everybody know if they wanted to reach out to you, what's the best way for them to do that?
4092250	4099180	A	0.6573434472084045	I'm not very active on this discord channel, so maybe email is still the best to reach out to me, I guess.
4100030	4100586	C	0.84200119972229	Cool.
4100688	4104122	C	0.7587466239929199	Do you want to give your oh.
4104256	4108106	A	0.5173580646514893	I think my email should be easy enough to find.
4108128	4109962	A	0.8680420517921448	But you can also give the email out there.
4110016	4110620	A	0.4753468334674835	Sure.
4111150	4111562	A	0.84200119972229	Cool.
4111616	4118780	B	0.9019215106964111	People can check the papers, and then in the active inference institute discord, there's the neuromorphic channel.
4121990	4122802	A	0.4896697998046875	All right.
4122936	4124526	B	0.9129462838172913	Thank you, David and Sarah.
4124638	4129810	B	0.9749829173088074	Really cool to see morphstream kick off its developmental trajectory this way.
4129880	4131474	B	0.7429341673851013	So till next time.
4131512	4132114	C	0.8529649376869202	Thank you.
4132232	4132430	A	0.5137446522712708	Bye.

start	end	startTime	summary	headline	gist
3960	51908	00:03	We are here kicking off a new stream series at the Active Inference Institute. Today we have David Cappell and also this section and streams facilitated by Sarah Ham. David will share some work on neuromorphic computing. Then we'll have some time to discuss.	We kick off a new stream series at the Active Inference Institute	Neuroonomorphic Computing: The Stream Series
52084	326364	00:52	 neuromorphic computing can be defined as computing systems designed to mimic the structure and function of the nervous system. Modern AI is already brain inspired. But supercomputers can't yet match our complexity of skills or the adaptability of the human brain.	Sarah is a neuroscientist specializing intelligence working in neuromorphic computing	Neuromorphic Computing
326492	511230	05:26	Next I'm going to explain how key features of the brain are being implemented to catalyze our next generation of AI and technology through neuromorphic computing. In neuromorphic architectures, computing and memory occur in the same place. This architecture improves speed, it reduces energy consumption and enables massively parallel processing.	Neuromorphic computing allows massively parallel processing and reduces energy consumption	Neuroimputation
512720	764110	08:32	The Human Brain Project has created several large scale neuromorphic computers. Neuromorphic cameras have improved performance with motion and lower power consumption. neuromorphic systems are also likely to be the future substrate of brain computer interfaces. There are some challenges in the field.	There are many different solutions available for neuromorphic computing	What are the Neuromorphic Solutions Available?
766640	891020	12:46	Recent studies have combined neuromorphic computing with principles of active inference. Active inference comes from neuroscience, and I would argue that it lends itself very well to neuromorphic architectures. The field implementing active inference principles in neuromorphic systems is very nascent.	Recent studies have combined neuromorphic computing with principles of active inference	Neuromorphic Intelligence: Active Inference
891120	950480	14:51	David Kappel is a researcher and group leader at the Institute for Moyo Informatics at the Rural University Bohom. He is leading the group on sustainable machine learning. His talk will focus on neuromorphic computing.	David Kappel will give a talk on neuromorphic computing	Sustainable Machine Learning talk
950930	1444810	15:50	Deep neural networks consume huge amounts of energy. If the growth rate of AI continues, it will consume 13% of the global energy consumption by 2030. Our goal is to transfer now mechanisms from machine learning to a new efficient AI generation.	Deep neural networks have become huge but they consume huge amounts of energy	Machine Learning's energy footprint
1445470	2109280	24:05	Free energy model is essentially a model to describe a situation like this. How the internal states and the state of the environment interact. By essentially minimizing this distance between the two, you can derive all sorts of behaviorally relevant problems.	Free energy model describes how the internal and external states interact	
2111190	2742620	35:11	We try to bring these inspirations now into actual machine learning models. The way this is trained is through end to end errorback propagation. But what is missing to make it applicable to the free energy principle is this feedback that you always need.	We try to bring these inspirations now into actual machine learning models	Free Energy in machine learning
2743070	2817138	45:43	Did you look at energy consumption with yours as well? Not yet. It's actually not so easy to implement these things in standard machine learning toolboxes. But we are quite confident that for parallelizing it, it should be there. The question is how much you save in terms of energy.	Did you look at energy consumption with your Convolutional network	How to parallelize neural networks
2817224	2949180	46:57	This is the first method that allows you to train these feedback weights. And the nice thing here is that we derive an upper bound to this variational free energy loss that can be spelled out completely by forward propagation. Very cool.	Local error back propagation is a new way of implementing convolutional neural networks	Local error back propagation in convolutional networks
2952530	3618180	49:12	The difference between neuromorphic hardware and the Neuromorphic algorithms. It's kind of like a mindset or a mental framework when I'm thinking about computers or AI. We are going to see more in this direction now that we can build this back into AI.	David Mitra: Difference between Neuromorphic hardware and the neuromorphic algorithms	Neuromorphic and the Future of Machine Learning
3618710	3843840	1:00:18	The researchers used the free energy principle to mimic the dynamics of the membrane, of the cell membrane. Do you think it would be worthwhile trying to implement it on hardware? Or what are your thoughts on that?	I'm curious about how you came to use the free energy principle	The free energy principle for neuromorphic computing
3845170	3997740	1:04:05	My question was about this tension between, I guess, neural and computational ways of looking at the resources associated with computation. What do resource descriptors or capacity descriptors look like when we're outside the space of power consumption? How do they describe or evaluate these different designs or algorithms?	My question was about this tension between neural and computational ways of looking at resources	Neuroscience and computational resources
4002910	4118780	1:06:42	David: Where do I see the series going? Hopefully we can have a new guest each month. Could you let everybody know if they wanted to reach out to you, what's the best way for them to do that?	David: Where do I see this series going? Sarah: Cool. Cool. Also, what direction will this series go	A Neuron Minute: Thank You David
4121990	4132430	1:08:41	All right. Really cool to see morphstream kick off its developmental trajectory this way. So till next time. Bye.	Really cool to see morphstream kick off its developmental trajectory this way	 morphstream

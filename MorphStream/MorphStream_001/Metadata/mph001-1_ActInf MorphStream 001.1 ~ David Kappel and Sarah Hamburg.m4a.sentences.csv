start	end	sentNum	speaker	confidence	text
6880	7980	2	B	0.95454	Hello, everybody.
8130	8830	3	B	0.98382	Welcome.
9200	12860	4	B	0.99999	It is September 26, 2023.
13010	17864	5	B	0.99999	We are here kicking off a new stream series at the Active Inference Institute.
17992	18984	6	B	0.99998	This is the morph.
19032	21116	7	B	0.95429	Stream 1.1.
21298	28940	8	B	0.77118	Today we have David Cappell and also this section and streams facilitated by Sarah Ham.
29480	34624	9	B	0.94898	We're going to have a overview, first presented by Sarah.
34752	41924	10	B	0.99935	Then David will share some work on neuromorphic computing, and then we'll have some time to discuss.
42042	49096	11	B	0.99192	So thank you both for joining and Sarah, to you for the first presentation and also to introduce yourself, if you'd like.
49278	50312	12	C	0.98493	Yes, that's a good idea.
50366	51908	13	C	0.99995	Thank you very much, Daniel.
52084	53636	14	C	0.994	So my name is Sarah.
53748	61464	15	C	0.90995	I'm a neuroscientist specializing intelligence, currently working in the field of neuromorphic computing at Sheffield Hallam in the UK.
61592	70060	16	C	0.98833	So I'm going to give you a high level overview of what neuromorphic computing is before we hear David's exciting talk in the first edition of this new series.
70640	76130	17	C	0.99996	Just to let you know, if you're watching on Double Time in the future, I talk quite fast, so you might not want to watch me on Double Time.
76740	82630	18	C	0.99073	So this QR code I put here will take you to a paper which I thought was a really nice introduction to the field.
83080	89748	19	C	0.61148	But neuromorphic computing can be defined as computing systems that are designed to mimic the structure and function of the nervous system.
89914	92196	20	C	0.99987	So this doesn't have to be the human nervous system.
92298	99370	21	C	1.0	The field actually takes inspiration from all sorts of animals and insects, although the definitions online don't necessarily acknowledge that.
99740	113016	22	C	0.99828	So some people are quite open with what constitutes neuromorphic, while maybe others would prefer neuromorphic was reserved for hardware instantiations of biological like neurons, which are sometimes referred to as non von Newman computers.
113048	119176	23	C	1.0	And I think that's what the paper that that QR code refers to it as their definition.
119368	121644	24	C	0.99988	So what I think is really interesting is a little bit of the context.
121692	127872	25	C	0.99174	So, like our current von Newman computer architecture was also inspired by neuroscience, particularly the McCullough and Pitts 43.
127926	131716	26	C	0.43508	Neuromodel inspired von Newman's first draft in 1945.
131818	151464	27	C	0.99982	So neuroscience has a long history of inspiring computer science, and this also includes reinforcement learning, which is based on theories about learning, decision making from behavioral psychology based on rewards and punishments, and also Hebian learning principles of cells that fire together, wire together from 49 became foundational for unsupervised learning.
151582	154970	28	C	0.99877	So, first of all, hang on 1 second.
155820	161864	29	C	0.9995	So in order to understand the why of neuromorphic computing, I really wanted to explain what's so great about the brain.
161992	165336	30	C	0.99978	So here's some inspiration for light bulbs.
165448	166684	31	C	0.99994	So I'm going to ask you a question.
166722	168332	32	C	1.0	I just want you to think about it for a second.
168466	172332	33	C	0.99999	In terms of light bulbs, how much energy do you think the brain uses?
172476	177456	34	C	0.99281	Do you think it's more or less energy than the bulbs lighting the room that you're in?
177558	179344	35	C	0.99999	If you're in the future, by all means pause this.
179382	179744	36	C	0.76007	Pause this.
179782	181792	37	C	0.99994	If you want to do some in depth calculations.
181856	183590	38	C	0.99858	But I'm going to skip to the answer.
184040	187488	39	C	1.0	The answer is here, in the pink circle.
187584	189568	40	C	0.99997	So it's 20 watts.
189744	193616	41	C	0.99997	So that's the equivalent of one modern day energy efficient light bulb.
193728	197064	42	C	0.99999	So that's probably what's above me now, basically in my room here.
197262	202490	43	C	0.99988	This QR code should take you to quite an interesting paper on power consumption in the brain, if you're interested in that.
203900	208120	44	C	0.99286	So that works out about four bananas a day to power your brain.
208280	212444	45	C	1.0	And this is calculated, by the way, based on calorie intake that the brain needs.
212562	223276	46	C	0.51763	So, for context, the fastest supercomputer in Europe, I think it's called lumi, in Finland it's been called exceptionally green, and its power consumption is 8.5 million watts.
223388	228144	47	C	0.99998	So that's around half a million light bulbs, while your brain uses just one.
228342	233540	48	C	0.89179	So then the question is, well, what does your brain do with that one light bulb or four bananas?
235080	239204	49	C	0.97243	Apparently it does 1000 billion calculations per second.
239402	242324	50	C	0.99998	So there's lots of other massive estimates out there.
242442	245252	51	C	1.0	This wasn't even the largest by several orders of magnitude.
245396	253348	52	C	0.99991	Estimates are obviously very speculative, but they're all massive and they all tend to be based on the number of neurons, their connections and firing rates.
253524	262824	53	C	0.99997	But I think it's really important for the context that supercomputers can't actually yet match our complexity of skills or the adaptability of the human brain.
262952	269810	54	C	0.99526	So we actually excel way beyond supercomputers when it comes to things like complex decision making, learning from experience.
271380	273600	55	C	0.99553	So how does your brain compare to AI?
273940	277212	56	C	0.99339	So, I mentioned that modern AI is already brain inspired.
277356	280340	57	C	0.99998	However, artificial neurons are highly simplified.
281160	285350	58	C	0.99999	They don't capture the complexity of biological neurons or networks, like, not even close.
286200	289124	59	C	0.77104	Individual neurons are actually more like networks themselves.
289322	298292	60	C	0.81	And research suggests that modeling one biological neuron requires a five to eight layer deep artificial neural network made of around 1000 artificial neurons.
298356	300696	61	C	0.99997	This QR code should take you to the paper.
300878	301850	62	C	0.99999	For that.
303020	305720	63	C	0.77492	You have 86 billion neurons in your brain.
305880	315484	64	C	0.99988	They work together to form a highly energy efficient, low latency supercomputer that works just above room temperature, off the equivalent of about four bananas a day.
315682	326364	65	C	0.99984	So hopefully I've given you a sense of how amazing your brain is, as if you didn't know that already, and how it's already been used to inspire the, I guess, fairly basic AI that we have now compared to human intelligence.
326492	337284	66	C	0.99881	So, next I'm going to explain how key features of the brain are being implemented to catalyze our next generation of AI and technology through the field of neuromorphic computing, which is why you're all here.
337482	344010	67	C	0.99893	So, traditional volume and computers have physically separate computing and memory units, shown here on the left.
344620	348600	68	C	0.99985	During computation, data must transfer backwards and forwards, like really fast.
348670	360092	69	C	0.99179	So there's a bottleneck, essentially for speed and energy, whereas in neuromorphic architectures, which are shown here on the right, with the help of Dali, computing and memory occur in the same place.
360226	362140	70	C	0.99559	So they're said to be colocated.
362800	370316	71	C	0.99987	Essentially, individual neurons perform computation while memory is represented by the strength of the connections, the weights between neurons.
370348	371740	72	C	0.84033	So the synapses.
371900	379516	73	C	0.99988	So chips like this might be created with components like Memoristas for example, which can emulate synaptic weights.
379708	383760	74	C	1.0	And this architecture improves speed, it reduces energy consumption.
383840	391190	75	C	1.0	And what's really interesting is it enables massively parallel processing meaning that multiple problems can be worked on at the same time.
391640	404208	76	C	0.98624	So this is particularly important, this architecture, for various use cases, but also because as we reach the end of Moore's Law, which is the number of transistors you were able to physically make tinier and tinier to fit on a chip.
404404	413820	77	C	1.0	And it's also important because humanity needs to massively reduce its energy consumption against the backdrop drop of creating ever more powerful AI.
415280	427136	78	C	0.99882	So artificial neurons typically use continuous activation shown on the left, they're always on, while neuromorphic neurons, they're said to be spiking, so they're on or they're off, which is shown here on the right.
427238	429448	79	C	0.99995	So similar to sort of an action potential.
429644	438240	80	C	0.99995	So the benefits for this are again power, efficiency and also applications where timing is important and this is given that they're event driven.
438400	447770	81	C	0.99999	So essentially they have the potential for spatial and temporal dimensions which then enables added spatiotemporal encoding and processing of information.
448460	453140	82	C	0.99997	You might be wondering a bit about GPUs which also enable parallel processing.
453300	465420	83	C	0.99999	Research suggests that GPUs are suitable architectures for deploying spiking neural networks which I think makes this a really interesting time for the field given how high end GPUs are becoming ever more pervasive.
467280	471240	84	C	0.99038	So the brain learns strength of synapses between neurons.
471320	474076	85	C	0.99984	This is based on pre and synaptic firing patterns.
474108	478000	86	C	1.0	I think David's Talk will talk a lot, we'll go into a lot more depth on this.
478070	487540	87	C	0.99613	But there are many different types and patterns of this across the brain depending on the types of synapses such as excitatory to inhibitory, excitatory to excalitatory.
487960	498472	88	C	0.98	And the neuromorphic field is working to leverage these rules because of benefits for on chip learning and also applications such as pattern recognition and edge computing as well.
498526	505832	89	C	0.99836	Edge computing being quite a huge use case for neuromorphic computing because of the sort of event driven nature and also the low energy usage.
505976	511230	90	C	0.98	And this QR code should take you to quite an interesting paper on STDP that I found.
512720	515212	91	C	0.99509	So what neuromorphic solutions are available?
515266	515532	92	C	0.99999	Now?
515586	522592	93	C	0.99998	You might just think this is all theoretical, there are actually many different solutions out there which I'll just give you a really high level overview of.
522726	532852	94	C	0.99429	So the Human Brain Project has created several large scale neuromorphic computers including Spinnaker, which is this one at the bottom, this board's, like maybe like the size of my face or something.
532986	546488	95	C	0.9994	So that runs in real time and it's comprised of multiple general purpose arm microprocessors and there was also BrainScaleS which is an accelerated analog architecture and it runs 1000 times real time.
546574	555610	96	C	0.99711	So the board next to the blue one, that's an actual credit card size version of BrainScaleS, which they've recently made, which I thought was pretty cool.
556540	558812	97	C	1.0	And then there's also some big players in the space.
558866	561976	98	C	0.99872	So this blue one here is Intel's Louihi chip.
562008	563084	99	C	0.99833	They're onto Luihi two.
563122	571008	100	C	0.99998	Now, that's their Neuromorphic chip, and they have an open source software framework for that as well, because they really want to catalyze the open source community to get involved with it.
571174	573164	101	C	0.79373	So neuromorphic sensors also exist?
573212	575916	102	C	0.9999	So this little blue thing in the middle is actually a Neuromorphic camera.
575948	577250	103	C	0.98141	It's maybe like this big.
578180	583092	104	C	0.99997	So they aim to recreate how our nervous system senses stimuli, such as light.
583226	590256	105	C	0.99997	So, for example, in a Neuromorphic camera, which is the one here, each pixel works independently with a microsecond resolution.
590368	591892	106	C	0.71401	Hopefully, my GIF will work.
592026	592788	107	C	0.99994	There we go.
592874	596216	108	C	0.99993	So you can see each pixel working there, which is pretty cool.
596398	602580	109	C	0.99111	So compared to traditional digital cameras, they have improved performance with motion and lower power consumption.
602740	606632	110	C	0.99982	There was also a Neuromorphic nose recently by intel, which was pretty cool.
606766	613950	111	C	0.99628	So it could learn the scent of a chemical after just one exposure, and then it could identify that scent even when it was masked by others.
614400	617672	112	C	0.99	And then finally, this is a humanoid robot called an ICup.
617816	629756	113	C	0.92	And what you can do is you can actually integrate Neuromorphic sensors, such as the camera, and then Neuromorphic chips maybe spinnaker or brain scales into a humanoid device like this or other devices like a drone.
629868	633568	114	C	1.0	And then from that, you can actually create embodied neuromorphic systems.
633744	639060	115	C	0.98	And this is something that we work on at the Smart Interactive Technologies Research Lab in Sheffield in the UK.
641320	647400	116	C	0.99994	This slide just highlights some of the potential applications of Neuromorphic computing, which I thought were quite interesting when you think about it.
647470	658460	117	C	0.5507	So the understanding of context, pattern recognition, advanced sensing, fusot, learning, generalizing across tasks, complex decision making, explainability, and brain interfaces.
658880	665752	118	C	0.99983	So all these skills are really beneficial when you're thinking about human centered, real time applications in dynamic environments.
665896	668568	119	C	0.99993	So things like self driving cars, for example.
668754	675468	120	C	0.97	And personally, I think that neuromorphic systems are also likely to be the future substrate of brain computer interfaces.
675564	683872	121	C	0.99992	Probably a bit biased because I'm a neuroscientist, but they're low energy, they're real time, and they also have architectures which match our own hardware.
684016	692132	122	C	0.99703	So I do think we'll soon see the BCI field being catalyzed by neuromorphic systems, particularly maybe for hybrids of hardware and wetware.
692276	698890	123	C	0.99984	So maybe even potentially containing people's own brain cells, which you can actually grow just from a hair cell.
701180	706472	124	C	1.0	And a particular focus of our work is designing AI, which learns in a similar way to a human.
706606	711340	125	C	0.99994	So it has an innate sense of curiosity, and it learns through interacting with the real world.
711490	721580	126	C	0.99984	So in the 50s, Alan Turing said, instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's brain.
721740	726876	127	C	0.99998	If this were then subject to an appropriate course of education, one would obtain the adult brain.
727068	734980	128	C	1.0	This is very much the philosophy behind the neurodevelopmental approach to AI and neuromorphic computing, which I just wanted to highlight.
736840	738548	129	C	0.99997	There are some challenges in the field.
738634	741956	130	C	0.72287	These are very high level, but I'll just give you a little bit of an idea of it.
742058	746772	131	C	0.99623	So, training spiking neural networks is more complex than traditional neural networks.
746916	753872	132	C	0.99996	Also designing hardware which actually implements spiking neural networks STDP on a large scale is said to be fairly challenging.
754036	759624	133	C	0.99	And then also developing algorithms which can actually effectively leverage all these technologies.
759672	764110	134	C	0.99957	So the hardware, the STDP, it's an ongoing active area of research.
766640	770700	135	C	0.94777	So if you're here, you're probably interested in active inference.
770860	772432	136	C	0.99974	So I wanted to highlight this.
772486	775360	137	C	0.99993	Actually, someone put on the discord today one of these studies, which was pretty cool.
775510	781104	138	C	0.99965	So a couple of recent studies have combined neuromorphic computing with principles of active inference.
781232	787060	139	C	0.99959	So active inference comes from neuroscience, and I would argue that it lends itself very well to neuromorphic architectures.
787640	793172	140	C	0.99999	In a recent paper on embodied neuromorphic intelligence, so it didn't really mention active inference in it.
793226	809896	141	C	1.0	The QR code on the top right here, it was suggested that a real breakthrough in neuromorphics will happen if the whole system design is based on biological computational principles with a tight interplay between the estimation of the surroundings and the robot's own state and decision making, planning and action.
810088	818396	142	C	0.99075	So some of those themes might sound quite familiar to people interested in active inference, and I would suggest that active inference is well placed to meet these requirements.
818588	821584	143	C	0.99	And I just wanted to highlight a couple of recent studies here.
821702	832016	144	C	0.99403	So this one on the left, Gandalfiatal, recently demonstrated plasticity and rapid unsupervised learning in a neuromorphic system using active inference principles.
832128	840768	145	C	1.0	The author suggested that their experiments could be adopted to implement brain like predictive capabilities in neuromorphic robotic systems.
840944	845476	146	C	0.99	And then there was the Dish brain paper, which some of you may be familiar with by Kaganatal.
845588	851812	147	C	0.96096	So this was a hybrid wetware hardware neuromorphic system that the authors claimed was embodied.
851956	857500	148	C	1.0	The system showed rapid apparent learning of the game of Pong using the free energy principle for learning.
857650	862300	149	C	1.0	And the authors claimed that the system exhibited synthetic biological intelligence.
862800	868292	150	C	0.99359	So the field implementing active inference principles in neuromorphic systems is very nascent.
868376	877504	151	C	1.0	And the idea behind this more stream series is to create a space and a community to share knowledge, ideas and expertise to catalyze the field.
877702	882228	152	C	1.0	I think some really exciting technological leaps are probably going to come from this area.
882394	886616	153	C	0.99996	So thank you for listening to my quick run through Neuromorphic 101.
886618	891020	154	C	0.98	And next up, we're going to hear from David.
891120	892536	155	C	0.97502	So David, over to you.
892558	894730	156	C	0.99981	If you want to introduce yourself, please.
895260	896600	157	A	0.99985	Thank you, Sarah.
898220	900890	158	A	1.0	I would share my screen.
904070	904820	159	A	0.70614	Yeah.
914950	920370	160	A	0.9271	Can you see now my screen, the presentation yeah.
920440	921858	161	A	0.75269	Okay, perfect.
922024	922750	162	A	0.70383	Okay.
922920	923542	163	A	0.54674	Hello.
923676	925122	164	A	0.99998	My name is David Kappel.
925186	932790	165	A	0.72708	I'm a researcher and group leader at the Institute for Moyo Informatics at the Rural University Bohom.
933770	940646	166	A	0.96667	So I'm leading the group on sustainable machine learning and we have a very strong focus on neuromorphic computing.
940678	941900	167	A	0.99825	That's why I'm here today.
942590	950480	168	A	0.98	And I'm going to start with a very similar motivation than Sarah did, which was really a great inspiration for this talk, I think.
950930	968274	169	A	0.99178	So probably most of you have seen this interesting recent result, and I don't mean Germany winning the basketball championship, but this really big leap in artificial intelligence that we have seen in the last years, especially the last two or three years.
968392	974742	170	A	0.99018	So this is a picture generated from a prompt by a Dali network and it's really amazing.
974796	978600	171	A	0.66631	It was considered science fiction like only two, three years ago.
979370	985666	172	A	1.0	And this was essentially made possible by a neuromorphic approach, which is deep neural networks.
985698	992310	173	A	1.0	And these deep neural networks have become huge now, but this comes also with a caveat.
992390	1001774	174	A	0.98204	So basically the flip side is that among other problems maybe these models may have, they consume huge amounts of energy.
1001892	1013902	175	A	0.99839	So models like Dali or Jet GPT, as Sarah already mentioned, they would consume energy budgets that are comparable to houses or cars.
1013966	1020142	176	A	0.99967	So training Chat GPT a single time is like a gigawatt hour approximately.
1020206	1032280	177	A	0.99995	So that would be 300 tons of CO2 emission and many times, which comes down to many times the lifespan of a typical car.
1032890	1034390	178	A	1.0	So this comes with two problems.
1034460	1041174	179	A	0.99997	Obviously, this makes training these models only accessible to a very small number of very large players.
1041222	1043020	180	A	0.63554	So essentially the big tech companies.
1043470	1050314	181	A	0.6	And secondly, and maybe even more importantly, this is not compatible with a planet with limited resources.
1050362	1068020	182	A	0.99982	So if the growth rate of AI continues like it did in the last years, it will consume 13% of the global energy consumption by 2030 and it will basically outrun the transportation sector in another five years or so.
1069110	1073202	183	A	0.9954	So this raises the question, does sustainable machine learning exist at all?
1073336	1078440	184	A	1.0	And obviously, since I'm working in the group of sustainable machine learning, I believe it does.
1078890	1096940	185	A	1.0	And why I think it does is because we know a system that is very efficient and is still probably better than these AI models, which is the human brain, which consumes, as Sarah mentioned, around 20 watts or four bananas a day.
1100290	1105760	186	A	0.94491	It's many orders of magnitude more efficient than the AI models we have today.
1107490	1114242	187	A	0.99919	But so far we don't know essentially how these networks work and especially how to train them.
1114296	1119700	188	A	1.0	And basically our goal is to transfer now mechanisms from machine learning.
1120230	1122260	189	A	0.57613	We have this nice picture here.
1122790	1127800	190	A	0.99997	Basically we start from the machine learning side where we already know our way around.
1128330	1142518	191	A	0.99596	So we have these models that are wonderful and that give us really impressive results, but they are not efficient and we want to transfer them to a new efficient AI generation.
1142694	1151360	192	A	1.0	And our idea is to use inspiration from neuroscience that make this transfer faster and possible in the first place.
1153010	1153760	193	A	0.99876	Okay?
1155490	1162494	194	A	0.99957	Actually, biology is a great source of inspiration and always comes around the corner with very surprising results.
1162542	1172318	195	A	1.0	And one of these results that I stumbled upon a couple of years ago is that reliability of synapses in the brain.
1172414	1176470	196	A	0.99997	So, as you probably know, neurons in your brain are connected for synapses.
1177690	1191558	197	A	0.99557	But if you look at this is a paper from 2019, and they could actually identify individual synapses and they could trigger them to make a synaptic release, like a single transmission.
1191654	1198694	198	A	0.99999	But if you look at these measurements, you see that this is really covered in noise.
1198742	1207646	199	A	0.99998	So essentially, if you basically average over this and zoom this out, you see here these typical synaptic traces, which is the average white line here.
1207748	1211066	200	A	0.99998	But below that, you see this huge jitter.
1211098	1216020	201	A	0.80792	So they really like go several standard deviations up and down.
1217510	1226114	202	A	1.0	And this is actually very surprising given that neurons are probably the single most costly cell type in your body in terms of energy consumption.
1226162	1233702	203	A	0.94974	They are really in comparison, they consume quite a bit of energy in your body.
1233836	1242902	204	A	0.99998	So you would expect that these transmissions that are communicated between neurons, which are very costly, should be highly reliable.
1242966	1249360	205	A	0.99999	So this is very counterintuitive these results and has been puzzling neuroscientists for quite a while now.
1251730	1265026	206	A	1.0	And then there is a second puzzling observation, which is that the morphology of neurons looks somewhat like this.
1265048	1269410	207	A	0.99906	So this would be your typical pyramidal neuron you have in your cortex.
1271430	1276562	208	A	0.99992	But you see that this is actually, for a cell, quite big and elongated.
1276626	1287946	209	A	0.99987	So this can be up to a millimeter in the human brain, which means that if a synapse fires somewhere here, it has a very hard time communicating with the cell body, which is down here.
1287968	1300026	210	A	0.99994	So the electrical signals that are produced here may travel down here, but the synapse up here has no way of measuring the actual voltages at the cell body.
1300128	1303626	211	A	1.0	And this is actually the interesting place because here are the action potentials formed.
1303658	1314500	212	A	0.99993	So if the synapse would really like to know about what is going on in the cell body so that it can make predictions about how the neuron will behave and how it interacts with the world.
1315030	1329882	213	A	1.0	And this is another very puzzling or open problem in neuroscience, how this communication actually works out in single neurons between the cell body and the SynOps.
1329966	1333814	214	A	0.99997	It is known that actually the action potentials can travel back up.
1333852	1342940	215	A	0.99995	So they see kind of this binary variable when the neuron spikes, but they cannot actually measure the membrane potential down here.
1343710	1348140	216	A	0.99978	So only the most prominent electrical signals can actually back propagate through this.
1349090	1358222	217	A	1.0	And this suggests that the synapse actually has very sparse information about what's going on in the cell body.
1358276	1364340	218	A	0.99	And most models of synaptic plasticity don't cover this at all.
1365270	1370882	219	A	1.0	And we were wondering how does this interaction work?
1370936	1380930	220	A	0.99974	How can the synapse produce useful learning signals given this sparse information about this important state of the neuron?
1381090	1394086	221	A	1.0	And our idea was that essentially these two observations, these high levels of noise and the synapse and this large distance between cell body and synapses which give these high uncertainties, these are actually the two sides of the same coin.
1394198	1411550	222	A	0.99861	So our hypothesis was that actually we could use the same models that we know already from the behavioral level, how an agent can act and perform in an environment of high uncertainty.
1411630	1418430	223	A	1.0	And we just apply this to every sign ups and there is an error.
1418510	1423960	224	A	0.99992	So every sign up should utilize these same models, basically.
1425370	1436790	225	A	1.0	And this model would immediately suggest that actually synaptic transmissions should be noisy and these levels of noise would express uncertainty about the environment.
1438170	1444810	226	A	0.64	And then we can use this model to derive learning rules and we can compare them side by side to biology.
1445470	1447546	227	A	0.98	And this is the first thing I want to show you.
1447648	1460654	228	A	0.99993	So I just give a very quick introduction to the free energy model, because some of you might not be that familiar with it, but this is essentially a model to describe a situation like this.
1460692	1465554	229	A	0.99995	You have a person that is interacting with some environment and here I assumed very simple.
1465672	1469490	230	A	0.61087	So this person tries to throw a ball to some target.
1469830	1473982	231	A	1.0	And we as humans, we are good in solving such tasks.
1474046	1478134	232	A	1.0	And we are also good in solving such tasks if there is high levels of uncertainty in this.
1478172	1485266	233	A	0.99998	So if the person may receive some visual feedback, but a lot of this feedback may be hidden.
1485298	1488700	234	A	0.99848	So you can imagine this going on behind some wall.
1489310	1500350	235	A	0.77	And the person now still might want to predict what is the trajectory of this ball flying towards the target so that it can make an accurate action.
1500690	1504846	236	A	0.99989	So we will assign some variables to these states here.
1504868	1508202	237	A	0.99996	So we have essentially this feedback that this person can observe.
1508346	1521246	238	A	1.0	And we have this unobserved state of the ball flying here, which we call U, where the person doesn't have direct access to it, would only see parts of it, for example, when the ball is at the corners of the ball appearing.
1521438	1532994	239	A	1.0	And then to model this essentially, or to describe this behavior, the person would have an internal description of this trajectory.
1533042	1537340	240	A	0.66973	So an internal model of this state, U.
1538590	1545210	241	A	1.0	And this model would then be updated to match the observed feedback.
1546510	1552154	242	A	1.0	And this can be described very nicely in this beautiful mathematical framework of the free energy principle.
1552202	1566290	243	A	0.83146	So the idea is that you would essentially establish a model of your internal state, so essentially how the internal states and the state of the environment interact.
1567270	1573746	244	A	0.7	And you would have a model of the feedback, so how the states and the feedback you observe interact.
1573938	1588060	245	A	1.0	And essentially you can then write down a loss function that measures the distance between this model of the internal state and this model of the feedback and the external state.
1590750	1606462	246	A	1.0	And then by essentially minimizing this distance between the two, you can derive all sorts of behaviorally relevant, can solve behaviorally relevant problems, for example, learning.
1606516	1610990	247	A	1.0	But you can also use this for other things, like figuring out what are good actions, for example.
1611060	1615534	248	A	0.8723	So making inference about both the internal states and the actions.
1615582	1619246	249	A	0.99999	For example, and this is in a nutshell, the free energy principle.
1619278	1627026	250	A	1.0	And this object here happens to be what is known as the variation of free energy, which is also just coincides with statistical physics.
1627058	1629400	251	A	1.0	And this is where this framework has its name from.
1629770	1632566	252	A	0.99988	But you see that all is probabilistic here.
1632588	1640310	253	A	0.99998	So essentially you have two probability functions q for the internal model and P for this interaction between states and observations.
1640390	1645530	254	A	1.0	And you have here a distance measure between them that you want to minimize.
1646110	1652282	255	A	0.99988	So now if we look at the neuron and the synapse and how they interact with each other, we find a very similar picture.
1652426	1664570	256	A	0.99896	So single SynOps, which we have here in green, has an internal state which we, for simplicity, model only as the synaptic weight.
1664730	1672338	257	A	0.99982	Based on this, the SynOps, when it's triggered by a presynaptic spike, it would generate a post synaptic current.
1672504	1681170	258	A	0.99999	This would then propagate to the soma, which is our external state, which we cannot directly observe because it's too far away from the synapse.
1681250	1688986	259	A	0.99987	But we can see a feedback, which is this back propagating action potential, which is this binary variable that tells us whether the neuron has spiked or not.
1689168	1693194	260	A	0.99972	So this is exactly the same framework if we write it down like that.
1693232	1696220	261	A	1.0	And we can just use the same mathematics to solve it.
1698030	1705514	262	A	0.99949	So to solve it, we only have to come up with a couple of we have to make a couple of assumptions.
1705562	1708494	263	A	0.99978	So we have to write down a model for this guy here.
1708532	1716450	264	A	0.99738	So this model of how the feedback and the external state interact.
1716870	1718674	265	A	0.99996	But we have very good models for this.
1718712	1721170	266	A	0.99986	This has been studied over many years.
1721320	1724334	267	A	0.99979	So here you see how typically a model neuron behaves.
1724382	1728514	268	A	0.99978	So you have here the membrane potential of a leaky integrated fire neuron.
1728562	1731046	269	A	1.0	And you see that this is just going up and down.
1731068	1736146	270	A	0.92214	So this neuron would receive a lot of presynaptic input and maybe also noise.
1736258	1739462	271	A	1.0	And eventually at some point it hits a threshold, it would generate a spike.
1739526	1744410	272	A	0.99998	So that would be the sets that travels to the downstream neurons and also back to the SynOps.
1745310	1749040	273	A	1.0	And then it resets, right?
1751890	1756170	274	A	0.99284	So we can write this down mathematically as a very simple differential equation.
1756330	1760158	275	A	0.9999	But the neuron doesn't have access to this state again.
1760244	1764214	276	A	0.99784	So it's again behind this wall, it only sees these spike events.
1764362	1769550	277	A	0.99965	But we can actually, for this simple case of a leaky integrated fire neuron, we can solve this analytically.
1769630	1775000	278	A	0.99972	So we can write down what is the posterior distribution of membrane potentials given the spike times.
1775370	1785778	279	A	0.99	And what comes out of this is actually a so called stochastic bridge model, or in this case of a leak integrated in fire neuron.
1785794	1787622	280	A	0.99853	It's an orange bridge model.
1787676	1789750	281	A	0.99258	So this can be written down analytically.
1791310	1793690	282	A	0.99973	It's not simple, but it's doable.
1795070	1797382	283	A	0.98	And we can then just use this directly.
1797446	1802826	284	A	0.98775	So this model that we have to again write down these free energy functionals.
1802858	1810894	285	A	0.79035	So we make here an assumption how the synapse actually produces post synoptic currents and how they are integrated in the neuron.
1810942	1818526	286	A	0.99961	But that's also given by the leaky integrated fire neuron and actually the stochastic inputs that the synapse generates.
1818638	1829458	287	A	0.99283	So for simplicity, we only assume here basically Gaussian synapses that would inject draw a Gaussian random variable and inject this to a leaky integrated fire neuron.
1829634	1836694	288	A	0.97	And then all these ingredients actually can be solved in closed form and we can derive learning rules that would minimize this free energy functional.
1836742	1852050	289	A	0.95229	Now, and if we do that, this has a bunch of nice properties because this Einstein Beck bridge is completely determined by the back propagating action potentials.
1852070	1871762	290	A	0.99478	So basically the times of the post synaptic spikes that arrive at the neuron, this shape that we get here only depends on two neighboring post synaptic spikes, which means that we get here automatically a learning rule that looks like this.
1871896	1888700	291	A	0.9995	So a learning rule that only depends on the difference between two postal optic spikes, which we call here delta t two, and the difference between the post synoptic spike and the actual input that is triggered at some point on the presynaptic side.
1890110	1904130	292	A	1.0	And we can basically make here this lookup table and just compute what would be the update that these sign ups would need to make so that it learns optimally in terms of this free energy principle.
1904230	1905774	293	A	0.98	And this is the shape that we get out.
1905812	1919250	294	A	0.99964	You see that there is a strong dependence on the post synoptic firing rate, but there is also a dependence on basically this typical STDP that Sarah mentioned before, what is the relative positioning of the pre and postsynoptic spike.
1919990	1925362	295	A	0.99154	So in a nutshell, this model can now be split into essentially two pathways.
1925426	1940790	296	A	0.99819	So we have this ATOC response, which basically just whenever there is a presynaptic spike that triggers an action in the synapse, we would draw from this Gaussian distribution and inject it into the neuron.
1940950	1957150	297	A	0.98	And then there is this postalk update where the synapse would look up in this onstain Woolenbeck bridge, what would have been the optimal output that it should have generated, so what would have been the optimal action.
1957490	1968770	298	A	0.99	And then it compares the actual action with this optimal action according to this free energy principle and then generates a delayed response which is an update of the synaptic weight.
1970150	1970610	299	A	0.99778	Okay?
1970680	1974034	300	A	1.0	And importantly, this internal model is only implicit.
1974082	1978626	301	A	0.48621	It's encoded, so to say, into this spike time dependent.
1978658	1979910	302	A	0.989	Plasticity rule.
1981370	1985826	303	A	0.99997	So how do these rules then look and how do they compare to biology?
1985938	1993302	304	A	1.0	And actually the fit is quite nicely given that this is derived really from first principles without making any assumptions.
1993446	1995958	305	A	0.99791	So this is the measurement in biology.
1996054	2012314	306	A	0.99991	This is this B and Al rule, b and Pool, very old work where they actually did this in vitro studies where they injected pre and post synaptic spikes and then they measured what is the weight change in the synopsis.
2012362	2015682	307	A	0.99	And this is the rule that is predicted by our model.
2015736	2021038	308	A	0.99	And you see that at least in a first order approximation, it gives us very similar shapes.
2021214	2041542	309	A	1.0	And this also makes sense because the synapse wants to change the most when it's close to the pre and post synoptic spike times because this is where or the post synoptic spike times because this is where it knows the most about the state of the postynaptic neuron.
2041606	2052090	310	A	0.52	The free energy principle would actually suggest these kind of cones with almost no assumption essentially.
2052250	2062974	311	A	0.97904	But we also get because we have not just the first order spike time dependent plasticity rule, but we also have this dependency on the post not of the firing rate.
2063092	2065486	312	A	0.99963	We can also compare this to other results.
2065518	2068766	313	A	0.99	And this here is this old work by Kaupner and Brunel.
2068798	2081138	314	A	0.87633	So this is actually a model, but that was very detailed describing the plasticity based on the pre and post synoptic firing rate in the synapse.
2081234	2082818	315	A	1.0	And this is what our model predicts.
2082834	2092922	316	A	0.58193	So if we inject render and pre and post synaptic bossBack trains with different rates, our model would predict this shape, which again is not a perfect match.
2092976	2109280	317	A	1.0	But given that this is a very idealistic model, it's actually the main features that low firing rates on the postynopstic side would lead to depression and higher to potentiation are reflected in this.
2111190	2115234	318	A	0.98162	Okay, I assume I still have ten minutes, right?
2115352	2128660	319	A	0.52239	Okay, so I would give a quick intermediate summary and then I want to show some other work where we actually apply this now to actual machine learning model.
2131030	2141062	320	A	0.9999	So what we have seen here is that synapses are actually are very stochastic and this was a big puzzle.
2141126	2165620	321	A	1.0	And B suggests that actually the synaptic noise is actually the synapses way of reporting its own uncertainty about the environment, where the environment is actually the post synaptic neuron and it really interacts in this fashion of the free energy principle with this post synaptic neuron or that's a very nice way of describing it.
2165990	2173060	322	A	1.0	And if you're interested more, there's a paper, a preprint out, you can read up on all this.
2174250	2177986	323	A	0.99748	Okay, so how does this now connect to neuromorphics?
2178098	2184194	324	A	0.68	And actually so we are not actually doing neuromorphic hardware, we are doing neuromorphic algorithms.
2184322	2196620	325	A	0.99996	So we try to bring these inspirations now into actual machine learning models and we thought that this might be a good attack angle to solve a problem that is well known in machine learning.
2197070	2211738	326	A	0.99995	So I just drew here very simple convolutional neural networks with various convolutional layers and then maybe some dense layers that you would have in your machine learning algorithm.
2211914	2218158	327	A	1.0	And the way this is trained, as many of you know, I guess, is through end to end errorback propagation.
2218254	2222874	328	A	0.99991	So the idea is that you have a training set which has inputs and targets.
2222942	2228306	329	A	0.98643	So, for example, in a classification task, this could be pictures of cats and dogs.
2228338	2232934	330	A	1.0	And you would have targets which are class labels, so to say.
2232972	2240650	331	A	0.99776	So there's actually artificial neurons in there, and one of these neuron may be active for cats and 1 may be active for dogs.
2241470	2248846	332	A	1.0	And in your training, that data, you have exactly these labels that were generated by humans, that were sitting down doing this by hand.
2248948	2259066	333	A	0.51	And then during training, you show these examples to the network by propagating these inputs all the way from the input layer to the output layer.
2259258	2264158	334	A	0.99998	Then the output is here compared to these hand labeled targets.
2264254	2270446	335	A	1.0	And then the mismatch between the two is back propagated through all these layers back to the input.
2270558	2285510	336	A	1.0	And all the weights or the synoptic weights that are here in between inside these layers would then be updated accordingly, so that after doing this many, many times, this network becomes good in telling apart Kelvin docs.
2286410	2289094	337	A	0.9932	So this has a problem, this algorithm.
2289142	2298182	338	A	0.99962	It works great in practice and it's the foundation of all these models we have talked about, like Dali or Jet GPT, but it is quite inefficient.
2298246	2302286	339	A	1.0	And the problem is what is known in the literature as the locking problem.
2302468	2315554	340	A	0.99996	So if you would split up this network now into blocks, which I already did before, but this is arbitrary, but for implementing this efficiently in terms of a software algorithm, it might be interesting to do that.
2315752	2329734	341	A	1.0	And now you would want these blocks ideally to run in parallel, so that you can basically show the first example on this first block and then already train it while the second block is doing something else.
2329852	2339734	342	A	0.99997	But this is not really possible with end to end back propagation because of this lock in problem, because the activation of the second block depends on the activation of the first block.
2339782	2342474	343	A	0.99995	So you have to propagate it all the way to the end.
2342592	2345702	344	A	0.99961	Then you compute this error and you would then back propagate.
2345846	2366180	345	A	1.0	And only when this is done, you can start the next epoch where you show a new bunch of examples and you see that during all this time, here, the thread that would run this first block maybe would be idle and has to wait essentially all the time.
2366630	2368654	346	A	0.51	And this obviously makes them very inefficient.
2368702	2384354	347	A	1.0	And now our idea was that we use basically what we had learned from this earlier model on how synopsis communicate over these long distances through this free energy principle, and also apply it just to a deep neural network.
2384402	2395558	348	A	1.0	And the idea is that you have here, again, basically you have already this generation of inputs to some output.
2395654	2402254	349	A	0.9178	But what is missing to make it applicable to the free energy principle is this feedback that you always need.
2402372	2407770	350	A	0.89	The idea was that we put here a very lightweight feedback network.
2407930	2418414	351	A	0.99993	So essentially each of these blocks now in this deep neural network would be accompanied by a feedback block that locally generates a target.
2418542	2425074	352	A	0.99989	So we used here really in the simplest case, which we have, so this is very recent work, we only used linear blocks so far.
2425112	2437558	353	A	0.99999	So these are single linear layers and we would generate now these outputs here in these feedback blocks and then use the free energy principle to derive a local loss.
2437654	2445050	354	A	0.99998	That allows us again to minimize both these feedback weights that we have here and also the weights in the forward network.
2446270	2448442	355	A	0.9612	So it's essentially the same idea.
2448496	2453950	356	A	0.99998	So we have here these outputs which we interpret now as parameters to a probability function.
2454020	2460346	357	A	0.99999	So we can apply this probabilistic framework, but now all the rest basically rolls out the same way.
2460388	2473746	358	A	0.99908	So we assume that these outputs are essentially the internal states of this model and we have these observations given in the inputs and the targets.
2473858	2491450	359	A	1.0	And now we try to minimize basically P would now be this feed forward network and Q would be now a function that contains features of both the feedback and the feed forward network.
2492030	2510834	360	A	1.0	And the nice thing is that if we I don't have time to go into the details now, but if you write this out, you see that this actually decomposes, this lock term here decomposes into local linear terms that give you these local losses here.
2510872	2520162	361	A	0.99968	So essentially that you can minimize here block local between forward and the corresponding feedback block, a loss function.
2520296	2527318	362	A	1.0	And you can then actually do this in parallel because maybe the picture is good to see here.
2527404	2531154	363	A	0.99976	So what you have to do now you have a bit of overhead because you have this feedback block.
2531202	2540490	364	A	0.99994	So this would be the two execution times of the feed forward block and the feedback block.
2540990	2563102	365	A	0.99989	But in principle they can run in parallel and once the forward block is done, the next forward block can start propagating through this network, but simultaneously already the forward block, because it already received a target here, can start updating the weights and when it's done, it's free to operate on the next epoch.
2563166	2567010	366	A	0.99893	So there is no locking anymore in this framework.
2568790	2570882	367	A	0.81395	Okay, so I'm pretty much done.
2570936	2572418	368	A	0.9999	I'm also out of time, I think.
2572504	2574086	369	A	0.9986	So how does this perform?
2574268	2576274	370	A	0.96	Of course we changed the learning algorithm.
2576322	2583030	371	A	0.99981	Now we have to also go back and see if this is still giving us the same performance.
2585470	2600526	372	A	0.9985	As I said, this is the first results we have here now and at least to mid scale data sets like cipher ten or so, this seems to actually perform very well.
2600548	2606906	373	A	0.99999	So we attract it for standard architectures, fashion MNIST with ResNet 15 and ResNet 18.
2607018	2615922	374	A	0.70608	We worked mostly so far for small data sets like Fashion MNIST with applying free splits to ResNet 50.
2615976	2619726	375	A	0.99999	We get basically the same performance as standard backprop.
2619838	2628546	376	A	0.99999	As networks get deeper, you see that our problem that we have now is actually overfitting because we have these local targets.
2628578	2632050	377	A	0.99997	It seems that these smaller blocks actually overfit to some extent.
2632210	2636106	378	A	0.94628	This is not so severe for still up to tasks like cipher ten.
2636128	2638394	379	A	0.97158	So we get quite close already.
2638512	2646726	380	A	0.99999	But if you go now for really large tasks, there's still something missing for like single splits.
2646758	2651274	381	A	0.9999	We are getting there, but we are not reaching all the way up to back propagation.
2651322	2660750	382	A	1.0	But it's still interesting to see that you can apply this principle also to these standard machine learning algorithms.
2661910	2664050	383	A	0.99629	Okay, this is my second summary.
2664470	2671566	384	A	0.99994	So actually we found that deep neural networks are surprisingly good in generalizing over probability spaces.
2671678	2673960	385	A	0.99998	This is how actually this work started.
2674890	2691770	386	A	1.0	And our idea was to exploit this and to utilize it to distribute learning in the same fashion as in the first project I showed you and to solve this credit assignment problem by generating these feedback networks.
2693950	2695754	387	A	0.90053	Yeah, that's basically it.
2695792	2700110	388	A	0.94	And then I want to acknowledge my coworkers and my students.
2700180	2710326	389	A	0.9759	So I have two very good PhD students, carlisle and Cabrill, who is now here in Bohom and works on this topic.
2710378	2717662	390	A	0.97	And the first project I showed was work I did together when I was in gettingham with Christian Tetlav.
2717726	2728440	391	A	0.98	And the second project is I worked closely with Christian Meyer and Anand Sutomoni and yeah, thank you.
2731370	2732706	392	C	0.99947	Thank you very much, David.
2732738	2734370	393	C	0.99993	That was absolutely fascinating.
2734530	2738578	394	C	1.0	I think it's incredible how closely the sort of model matched biology.
2738674	2742620	395	C	0.95394	Like considering you derived it from first principles, I think that was really cool.
2743070	2746234	396	C	1.0	I did have a question just about the last sort of the bit.
2746272	2753354	397	C	1.0	You spoke about the Convolutional network and you said traditionally it has to go all the way end to end, which is really inefficient.
2753402	2755246	398	C	1.0	And then you showed the results that you got.
2755268	2759280	399	C	0.99995	Did you look at energy consumption with yours as well?
2759810	2760462	400	A	0.99799	Not yet.
2760516	2772050	401	A	0.83678	So we are actually currently working on it's actually not so easy to implement these things in standard machine learning toolboxes.
2772710	2787558	402	A	0.86865	So Carlisle is currently looking into this, the PhD student in Jester, he has an implementation now, and he's now evaluating how well we can make use of this parallelization in practice.
2787734	2797498	403	A	0.99997	But we are actually quite confident that for parallelizing it, it should be there.
2797664	2800230	404	A	1.0	The question is how much you save in terms of energy.
2800320	2808990	405	A	1.0	Because for these smaller scale models that we use now, ResNet 18, ResNet 50, the effect might be not that huge.
2809060	2813774	406	A	0.99997	So once we ramp this up to really larger models, the effect should be bigger.
2813822	2815620	407	A	0.99983	But yeah, this is ongoing work.
2816390	2817138	408	C	0.99997	Very cool.
2817224	2818146	409	C	0.99999	Thank you.
2818328	2830120	410	C	1.0	And then I was just wondering as well, this local error back propagation, is that something that other people have tried with these convolutional neural networks or is this quite a new way of implementing it?
2830650	2833240	411	A	0.99999	There is a bunch of approaches that do this.
2834250	2843930	412	A	0.6063	For example, I mean, the closest I guess is target propagation which has been proposed, which essentially uses random feedback weights to back propagate here.
2844000	2846330	413	A	0.99989	So these guys would not be trained.
2848830	2857118	414	A	1.0	And this works nicely also for small scale problems.
2857204	2859294	415	A	1.0	But as far as I know, they don't perform that well.
2859332	2867474	416	A	0.99999	Even for cipher ten, it already starts breaking down because these random feedback weights are just too coarse an approximation, I think.
2867592	2872638	417	A	1.0	And this is the first okay, maybe I have to be careful.
2872734	2878706	418	A	1.0	I think this is the first method that allows you to train these feedback weights.
2878738	2880790	419	A	0.99998	That is not a contrastive method.
2881210	2884360	420	A	0.99998	There is a bunch of methods that use a contrastive step.
2886510	2908602	421	A	0.96053	So you have maybe seen this forward forward algorithm and all these things, but what they have to do always is they send in the actual input data and then they send an kind of anti input, an anti input that is usually generated artificially.
2908666	2917646	422	A	0.50181	So they do some distortion to the input to make it and then they have to use both information locally to do the update.
2917678	2923802	423	A	0.99956	So the network has to keep in memory the input and the anti input and the responses.
2923966	2930534	424	A	0.99	And this makes these approaches a bit harder to parallelize in this.
2930572	2943206	425	A	0.66	And the nice thing here is that we derive an upper bound to this variational free energy loss that can be spelled out completely by forward propagation.
2943398	2944842	426	A	1.0	And that I think is new.
2944976	2947946	427	A	0.99996	So that is the new bit of this.
2948128	2949180	428	C	0.99916	Very cool.
2952530	2954160	429	B	0.96895	That's awesome.
2956530	2962842	430	B	0.96376	Well, yeah, few comments, one piece that kind of between the two of your talks.
2962906	2969810	431	B	0.99999	That was at least a new distinction to me, which was the difference between the Neuromorphic hardware and the Neuromorphic algorithms.
2970790	2979702	432	B	0.99989	So it's not just about new hardware or Wetware, though that would be great to see.
2979756	3004954	433	B	0.99972	It's almost like there's this intermediate or a bridge step with using the algorithms on the hardware we have today that like Sarah mentioned, the Spiking neural networks which are amenable to GPUs or just using standardized CPU multicore scheduling approaches.
3005082	3013382	434	B	0.99989	You can already do more with what we have using the Neuromorphic algorithms.
3013546	3033330	435	B	0.99308	So it's not just a material science topic, but also there's a lot at the really micro scale that we can learn related to noise processing scheduling and then also even at higher levels of abstraction, probably learning from biomimicry and cognitive systems more generally.
3033410	3036280	436	B	0.99996	But that was a distinction for me.
3038110	3038426	437	A	0.59004	Yeah.
3038448	3040586	438	A	0.90585	So maybe to add yeah.
3040608	3061630	439	A	0.71801	So I think that the problem becomes really now more pressing as these Neuromorphic devices become so the hardware devices become more mature and they usually cannot really shine on these standard machine learning algorithms because they are really optimized for GPUs.
3062130	3064238	440	A	0.86768	So you need to think a little bit.
3064324	3072414	441	A	0.55887	So you have to take one step back and think again about the algorithmic side to really use them to full capacity.
3072462	3087814	442	A	0.95	And as you have seen now, we collaborate with Professor Meyer, who is doing this spinnaker chip in Dresden, but there's also other approaches like the Louis chip that Sarah mentioned from intel and so on, and they are really looking into this.
3087852	3091080	443	A	0.70192	Now, also from the algorithmic side.
3092970	3094182	444	C	1.0	I find it really useful.
3094246	3105994	445	C	0.99946	It's kind of like a mindset or a mental framework when I'm thinking about computers or AI, and this is probably because I'm a neuroscientist, but I also have to translate it to, well, how does the brain work, how does the information processing work, et cetera, in the brain?
3106122	3111006	446	C	0.93	And when I came to sort of computer science and AI after neuroscience, I found myself naturally translating it.
3111028	3119246	447	C	0.99753	But I feel like the framework is just a really useful way of understanding computation at the end of the day because our brains, as I said, are just these massive supercomputers.
3119278	3122306	448	C	1.0	And I'm constantly reading papers on computer science or whatever.
3122408	3129154	449	C	1.0	And then once you can conceptualize anything really as well, how neuromorphic is this?
3129272	3133298	450	C	1.0	And then if you start thinking about, well, how could you tweak it so it's slightly more neuromorphic?
3133314	3136402	451	C	1.0	And is that then going to give you these gains that we get with the brain?
3136466	3141122	452	C	0.99999	Is it going to give you some extra parallel computation or is it going to give you some energy efficiency?
3141186	3147334	453	C	0.99581	So, yeah, I find it the definitions when I was looking at the definition is I think it really depends who writes the definition.
3147382	3163998	454	C	1.0	And because it's such a dynamic area at the moment as well, I think it has been changing and it will be changing, but for me, I feel like neuromorphic is more of like a mental framework where I look at things through conceptualize, through yeah, I think it's.
3164014	3165810	455	A	0.59081	It'S also not very well defined.
3167830	3177014	456	A	0.99079	You also mentioned that actually artificial neural networks are a neuromorphic concept, if you want, and they were from the first day.
3177052	3192374	457	A	1.0	And it's a big success story, right, if you look into the 90s or so when these support vector machines and these alternative models came up, but none of them have outlived the neuromorphic approaches.
3192422	3194234	458	A	0.99983	So it's actually very nice.
3194272	3203660	459	A	1.0	But still there is this community that thinks that there is more features from the brain that you need to put in to get to the real thing.
3204690	3212366	460	A	0.99922	So I think this is a bit of a it's not a very well defined term, actually, and I think with.
3212388	3216458	461	C	0.91367	Your research, yours is almost like the smallest level that I've seen people look at it on.
3216484	3221806	462	C	1.0	I don't know if you've seen anything else, but we're not just talking about a cell level of free energy and active inference.
3221838	3225810	463	C	0.75939	We're actually talking about a cell structure.
3226310	3228290	464	C	0.82677	So then you think, well, how small does it go?
3228360	3243130	465	C	0.99998	Are we going to talk about cell subcellular structures eventually, like Mitochondria using free energy in a similar way with compartments, I guess actually it'd be interesting to get your thoughts, david on, you talked at the start about how synapses are compartmentalized.
3243710	3259550	466	C	0.99757	Do you see different sort of instantiations of this in different compartments just within one synapse, almost like are you wanting to sort of look at that granular level or is it more now taking what you've learned from this and putting it back into how can we sort of make the AI more efficient?
3260530	3267170	467	A	0.99847	Yeah, we are going much more in this direction now that we see how we can build this back into AI models.
3271030	3290140	468	A	0.64	One has to be a bit careful when using the free energy principle because it's such a powerful general framework that you can apply it to basically anything and it's not necessarily you will come up with a useful result in the end.
3290750	3297338	469	A	0.98142	Just putting this this started actually as a side project.
3297424	3303920	470	A	0.99994	This was my kind of COVID pandemic lockdown project.
3304610	3312014	471	A	1.0	And I was just curious about this and whether you can actually solve this, because I thought the synopsis is maybe simple enough.
3312212	3318142	472	A	0.99889	Because when you go into the papers, they have at some point to go into some approximations.
3318206	3330806	473	A	0.99899	They do some mean field, usually, so they go for first modes, and then you can solve these guys for more complex, even for neurons, it's hard, actually, if you go to the neuron or network level, it's hard.
3330908	3336150	474	A	0.7301	It's very involved math, but for a sinus is simple enough, actually.
3336220	3343594	475	A	0.99986	So you can actually do this and spell everything out if you make the right assumptions and really just derive these things.
3343632	3356798	476	A	1.0	And that was kind of just kind of a game I went into and then it turned out to work quite nicely, I think, in the end.
3356964	3357680	477	C	0.7361	Yeah.
3359570	3372020	478	A	0.91716	I'm not sure if you like Mitochondria or so I'm sure you could apply the same principles but I'm not sure if the results you get would be any meaningful or would help you in any way.
3373750	3375438	479	A	0.84229	That's always the risk.
3375534	3379800	480	A	0.99982	You invest so much time and then in the end you get some results and you don't know.
3381290	3388006	481	B	0.99997	That was also something that I thought was quite interesting, which was the synapse was the agent.
3388188	3393066	482	B	0.75975	It's really easy to think, oh, we'll make an agent based model of a neural system.
3393248	3402880	483	B	1.0	First off, that tends to not include Glia or non neural cell types, but it's almost like a doubly unquestioned assumption that the cell would be the agent.
3404130	3411006	484	B	0.99755	But then it was a great transition from the person throwing the ball over the wall.
3411108	3425858	485	B	0.74491	That's an action centric approach where you only have partial visibility of the consequences and then that is the exact scenario that the synapse finds itself in in a different way.
3425944	3430040	486	B	0.99997	Or it could have been set up so that a neuron is the agent.
3430570	3432870	487	B	0.66574	We're building maps, not territories.
3433210	3438360	488	B	1.0	And so then, just like you said, free energy principle, it's a principle for everything.
3440270	3447770	489	B	1.0	And so just making principled statements about things is table stakes.
3448190	3456538	490	B	1.0	And then I guess my question for you is then what does make it useful or in your learning and tinkering around with these models?
3456714	3470050	491	B	0.99999	What differentiated situations where you applied free energy principle or active inference and you felt like it was providing a contribution to your research direction versus where you played around and it was like, well, that was tautological.
3471190	3477780	492	A	1.0	I think the free energy principle makes sense in a context where you have incomplete information.
3481030	3498314	493	A	0.62791	So for example, in the synapse case, right, the question we started with was this problem that the synapse has to solve, that it has incomplete information about the state in the cell body because it only sees this kind of or that's the assumption, at least of the model.
3498512	3504554	494	A	1.0	And also what we get from the experimentalists, that it essentially only sees this back propagating action potential.
3504602	3511710	495	A	0.99992	So it sees a single binary variable about the state of the soma.
3512950	3525550	496	A	0.79225	So essentially this is a problem of incomplete information and also the second ingredient that you need some form of agency.
3525630	3537474	497	A	1.0	I think if you apply the free energy principle to a system without agency, so if something is not interacting with an environment in a closed loop, then it becomes really sketchy.
3537522	3547834	498	A	1.0	And I think already this model is on the edge when it comes because these models don't really have an agency, but they at least produce an output, right?
3547872	3552110	499	A	0.99998	So you can still think of this as an interaction with an environment.
3552690	3559680	500	A	0.99968	But as soon as you lose that, I think then there would be simpler models that can just give you the same.
3566010	3586250	501	A	0.99947	Actually, in the synapse case, the agency is only this adding noise actually in the model because the synapse is triggered presynaptically and then it uses its internal state to add the right amount of noise, which is probably already the minimal agency you could imagine.
3589710	3592480	502	B	0.53147	Sir, you want to ask a question or I can ask a question?
3593810	3595370	503	C	0.99938	Yeah, it was more just a comment.
3595450	3598922	504	C	1.0	I think it's quite interesting people talk about biology.
3599066	3602394	505	C	0.99999	Some people say it's not a real science because it's all messy and noisy.
3602442	3609294	506	C	0.99728	But I think it works really interesting because it's like you say, the synaptic noise is actually a reporting of uncertainty.
3609422	3616206	507	C	0.99992	So in that essence, it's actually probably quite accurately reporting the messy world rather than the biology itself just being all messy.
3616238	3618180	508	C	0.99974	But that's just what I was thinking about.
3618710	3620386	509	C	0.99806	Yeah, I think I was curious as well.
3620408	3626806	510	C	0.99795	Like you said, this was like your Lockdown project, but I'm just interested in how you sort of came to use the free energy principle, how you came across it.
3626828	3631754	511	C	0.99999	Was it something you were quite familiar with already or some of your network or peers were talking about it?
3631792	3633500	512	C	0.9976	Or did you just stumble across it in a.
3636190	3642830	513	A	0.99087	Was during my PhD, I was interested in variational methods and probabilistic methods.
3643810	3645854	514	A	0.99	And then I started reading about this.
3645892	3651998	515	A	0.58	And so I read a bunch of Karl Friston's papers and I found this interesting.
3652084	3661410	516	A	0.78	And actually, my PhD supervisor always encouraged me not to go in that direction.
3662550	3668200	517	A	1.0	And then after I finished my PhD and thought, okay, now I can do what I want, I try it out.
3670890	3682794	518	C	1.0	And then, I guess, do you think it would be worthwhile, like, next steps for you or for the field actually trying to implement this on maybe some of the more analog chips that are being built in the space?
3682832	3692722	519	C	0.99993	Like the analog neuromorphic chips, which I know you can have some dynamic synapses and things like do you think it would be worthwhile trying to implement it on hardware?
3692806	3695440	520	C	0.60545	Or what are your thoughts on that?
3695810	3702240	521	A	1.0	I mean, the triplet rule that comes out from this first work I showed, I think that would be interesting to implement it.
3704390	3718370	522	A	1.0	The nice feature is that it should, in principle, should have this self stabilizing feature because it's really mimicking the dynamics of the membrane, of the cell membrane.
3718530	3740426	523	A	0.59329	So if the neuromorphic hardware would so if the model in the synapse and the neuron model match up very well, the model should give you this nice self stabilizing feature so that neurons really not go into some epileptic states or so.
3740448	3742206	524	A	1.0	And you get this for free from this model.
3742308	3744480	525	A	0.99995	That's what we saw in the simulations, at least.
3745010	3752398	526	A	0.9975	But in the simulations, of course, we had full control over this dynamics matching up in the right way.
3752484	3759214	527	A	0.98455	So that is probably a bit more tricky for hardware, but it's probably solvable.
3759262	3760500	528	A	0.98123	So it would be interesting.
3760950	3769560	529	A	0.99961	What you get for it is that you have these purely event based tools which only use pre post spikes, which is nice.
3770410	3771302	530	C	0.99956	Very cool.
3771436	3771990	531	C	0.99955	Thank you.
3772060	3772438	532	C	0.91201	Yeah.
3772524	3774854	533	C	0.99454	Daniel, if you had a question?
3775052	3802846	534	B	0.93139	Well, that's a great principle there, which is like, if you can design the neuromorphic algorithm so that it harnesses a material feature like the actual leaky permeability of a membrane or actual spatial proximity, if you can leverage a material feature, analog feature that isn't virtualized, then it's already an adjacency into future hardware.
3802878	3803746	535	B	0.98474	So that's one great point.
3803768	3815950	536	B	0.99	And then to Sarah's point about almost biology not being a science, which there is a famous quotation, there will never be a Newton for a blade of grass.
3816110	3820134	537	B	0.99962	Because some people say, yeah, it's a different biology is more like history.
3820332	3826034	538	B	0.99998	Because whether you approach this from a development or ecology or evolution perspective, biology is a historical science.
3826082	3827910	539	B	0.99992	It's not like a real science.
3828070	3834774	540	B	1.0	And then that reminded me of the cross country shirt that says, our sport is your punishment.
3834902	3841370	541	B	0.99989	So it's like, well, no, your noise is biology's signal.
3841530	3843840	542	B	1.0	And that's how it happens.
3845170	3858510	543	B	0.99969	My question was about this tension between, I guess, neural and computational ways of looking at the resources associated with computation.
3858670	3861758	544	B	0.98816	So from the von Neumann paradigm.
3861934	3884780	545	B	0.99992	We have a lot of shared reference points, CPU cycles, Ram capacity, and all these kinds of and like, even in your introductions, you conveyed like, well, this is how many CPU cycles it's going through, or this is how many parameters would have to be stored, or something like that.
3886110	3890554	546	B	0.99995	However, that's referencing another paradigm.
3890602	3905478	547	B	0.99985	So what do resource descriptors or capacity descriptors look like when we're outside the space of okay, yeah, power consumption.
3905514	3911294	548	B	0.99157	That's something that you can put into a box and just use a bomb calorimeter that's kind of like a low hanging fruit.
3911422	3926440	549	B	0.99999	But now okay, beyond just the sheer energy or caloric requirements, what can we say that is, like, analogous to the way that we talk about the processor or the Ram or the hard drive on a computer?
3931640	3934084	550	C	0.92902	Yeah, I'd have to think about that one some more.
3934122	3939912	551	C	1.0	I do think there was some interesting comments in the paper on that slide I showed that talks about the brain and energy.
3939966	3941144	552	C	0.99928	There was a paper I linked to.
3941182	3944508	553	C	0.99839	I'll have to get the reference and let you know what it is because QR code is gone now.
3944674	3949308	554	C	0.99995	But that had some interesting ideas, I think, on what you're getting at there.
3949394	3953710	555	C	0.99932	But I'd have to defer to the paper.
3955440	3958780	556	B	0.99998	How do they describe what is being designed?
3959200	3964850	557	B	0.49971	So they say it has this many of this type of component, and then that might do nothing though.
3965220	3970500	558	B	0.99997	So how do they describe or evaluate these different designs or algorithms?
3972520	3974580	559	C	1.0	I think it's all different depending on the use case.
3974650	3975444	560	C	0.99988	That's what I've found.
3975482	3975684	561	C	0.99996	Really?
3975722	3982560	562	C	0.71999	Like, the language is different depending on if it's written by someone maybe with more of a neuroscience background or an engineering background.
3982640	3986360	563	C	1.0	And then you kind of get used to some terms that are more interchangeable than others.
3986430	3997740	564	C	0.99999	But I do think the terminology is something which needs to be looked at a lot more closely in this space, because then I think that will help everybody working in it to be on the same page a little bit closer.
4002910	4003370	565	A	0.98391	Cool.
4003440	4007718	566	B	0.98586	Well, any other thoughts or questions?
4007904	4010554	567	B	0.58049	David first and then Sarah.
4010602	4015200	568	B	0.99968	Also, I'm very curious, what direction will this series go?
4016370	4023040	569	B	0.99883	But first, David, what are any other kind of closing comments or directions you want to.
4025650	4026206	570	A	0.56482	Really?
4026308	4029140	571	A	0.85	I mean, I would say thanks for having me today.
4029670	4032180	572	A	0.85556	It was really a pleasure to discuss with you.
4034230	4034706	573	C	0.99978	Thank you.
4034728	4036194	574	C	0.99991	David, it was amazing to have you on.
4036232	4045080	575	C	1.0	I think your work is absolutely fascinating, and I think it's going to have lots of benefits in the future for implementation, which is always nice to see as well.
4046010	4046374	576	C	0.94778	Yeah.
4046412	4047306	577	C	0.99986	So what was the question?
4047408	4049194	578	C	0.99996	Where do I see the series going?
4049392	4053226	579	C	0.99996	Hopefully we can have a new guest each month.
4053408	4063006	580	C	1.0	I think it'd be kind of cool maybe next month to have someone who's building hardware, so, like, maybe someone from the BrainScaleS team or spinnaker team or something like that would be.
4063028	4081618	581	C	0.9997	Pretty cool, but yeah, really, I just want to have a space for people who are interested in this intersection to meet people and see talks and reach out to people who are also working in the space, because it's pretty niche, but I think it's pretty important, actually.
4081704	4087720	582	C	0.99999	Having said that, David, could you let everybody know if they wanted to reach out to you, what's the best way for them to do that?
4092250	4099180	583	A	0.6998	I'm not very active on this discord channel, so maybe email is still the best to reach out to me, I guess.
4100030	4100586	584	C	0.99873	Cool.
4100688	4104122	585	C	0.99557	Do you want to give your oh.
4104256	4108106	586	A	1.0	I think my email should be easy enough to find.
4108128	4109962	587	A	0.99973	But you can also give the email out there.
4110016	4110620	588	A	0.89967	Sure.
4111150	4111562	589	A	0.92248	Cool.
4111616	4118780	590	B	0.99999	People can check the papers, and then in the active inference institute discord, there's the neuromorphic channel.
4121990	4122802	591	A	0.86653	All right.
4122936	4124526	592	B	0.99994	Thank you, David and Sarah.
4124638	4129810	593	B	0.99973	Really cool to see morphstream kick off its developmental trajectory this way.
4129880	4131474	594	B	0.98322	So till next time.
4131512	4132114	595	C	0.90538	Thank you.
4132232	4132430	596	A	0.90276	Bye.

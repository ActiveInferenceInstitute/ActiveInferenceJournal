SPEAKER_00:
Hello, welcome everybody to the Active Inference Institute's second episode of our Morphstream series.

So these sessions are intended as a space to showcase and discuss the intersection between neuromorphic computing, so brain-inspired computing, and active inference.

If you're new to neuromorphic computing, I did give a short primer in the first session, which is available on YouTube.

So the format is that our guest speaker will present in the first part, then there's a Q&A, and then a wider conversation for the rest of the call.

So our first one featured David Capel, who's kindly joined us again today.

Thank you, David.

And he gave an absolutely fascinating talk about a free energy model to uncover the role of noise in the brain.

So it's like active inference at the synapse.

And today's episode features Alon Loeffler.

He's a postdoctoral scientist at Cortical Labs in Melbourne, Australia.

Cortical Labs being the company behind the very interesting Dish Brain paper, which recently came out.

So he specializes in synthetic biological intelligence and has a PhD in neuromorphic nanowire networks from the University of Sydney.

He also has almost three, sorry, over three years of experience in designing learning tasks and algorithms for brain inspired systems.

And today he's going to be presenting some really interesting research on neuromorphic nanowire networks.

And so if you want to share your screen.


SPEAKER_02:
Absolutely.


SPEAKER_00:
Thank you.


SPEAKER_02:
All right, can you see this all good?

Yeah, all good?

All good.

Fantastic.

So thank you very much for having me.

And thanks for the wonderful introduction.

Not much more to say about myself about so that was perfect summary.

Yeah, today I'm going to talk mostly about my PhD research, which I finished last year from the University of Sydney, which was

in neuromorphic nanoline networks.

And that's quite a mouthful.

We'll definitely get into what that is.

Don't worry.

Hopefully by the end, you'll be experts.

And I'll also touch a bit at the end about what I'm working on at Cortical Labs at the moment.

And there's quite an interesting link.

They're slightly different, but hopefully we'll get through that.

So just a bit of an overview of how today's going to go.

I'm going to give a very brief background.

I imagine a lot of the people who are interested in this

kind of topics, know quite a lot about AI and the state of AI at the moment.

But just in case you don't, I'll give some background and go over that.

Explain what nanowire networks are, explain what we actually do, how we model these systems, and how we actually apply these systems.

And then at the end, just give a brief overview of what I've been working on for the last year or so.

So let's start with a bit of a background.

The brain, the art, the original inspiring or inspiration for artificial neural networks, AI kind of this machine learning idea behind AI was the neuron.

Um, the most basic ANN is called a perceptron and it was sort of in the fifties and sixties.

It was, um, created to mimic the idea of this inputs into a neuron from this sort of dendrites.

through the cell body into the axon and then across as an output into different neurons.

And that's sort of the idea that we have.

That's sort of what became this beginning of artificial neural networks and AI in general.

But there's quite a few key differences between neurons and the structure of neurons in the brain and how we implement our perceptron slash ANN

interpretation of these neurons.

Mainly, there's the idea behind von Neumann architecture, which is how we pretty much implement all of our computing these days, compared to how the brain is structured.

So in most modern AI systems, we implement this software of AI on hardware, von Neumann hardware, where we have this

separation between computing, processing and memory.

And we have to bus information back and forth from the CPU to the memory, which is quite a big bottleneck.

It takes quite a lot of time to do that.

And that's pretty much the main kind of limitation of this architecture.

Whereas in the brain, there's sort of this integrated idea of memory and processing that happens in the synaptic kind of gaps between neurons, in neural cells, in

chemical processes, all that kind of stuff occurs simultaneously.

And so we're trying to overcome this bottleneck of this busing information back and forth, which actually takes quite a lot of energy and quite a lot of time.

The main way we've been doing this is adding more transistors.

This is sort of how the structures are built using transistors.

More of those transistors are added so that we can have more and more computer processing.

This is what's called Moore's law.

And you may have heard of that before, but the idea is over the past few decades from the beginning of the perceptron, which I introduced, we've sort of seen this two year doubling idea of computer processing power and energy requirement.

And then in the last sort of decade or so, we've seen that this is

not a linear well it's still linear but it's not quite as simple this relationship it's not a doubling of energy and power we're seeing this 3.5 3.4 month doubling of requirement for power um so more and more and more transistors are required to do ai and ann kind of processing and more and more power is therefore required which is a big issue

Why is this a big issue?

Well, firstly, I'll get back to that in a bit of a second.

But importantly, another important aspect between, you know, the brain and Neumann architecture, another difference is this brain circuitry idea.

And so while modern applications of AI are really, really, really good at, you know,

object recognition and pattern recognition.

They're able to beat humans in lots of things like that.

But their original urine-inspired architecture, which I discussed from the perceptron, has transformed into a much more complicated system that's reliant on these higher and higher power requirements and energy requirements and are held back by this bottleneck.

And the feed forward structure that we see, that we're all quite familiar with, where you have this input layers into hidden layers into an output layer where information flows in from the left, goes through these layers and out to the right, it's an inherent approach in AI.

There is some recurrence that can be introduced, but even that's quite a simple method.

And this is highly, highly different to biological neural networks.

We see a very different structure.

We see a very different kind of functionality even in these recurrent networks.

And so the question is, humans behave at least sometimes intelligently with significantly low energy requirements.

You and I only need a few meals a day worth of energy.

The brain works at around 20 watts of information, of energy, sorry.

And in order to try and capture this kind of intelligence and processing capacity of the brain, we need to try and overcome these bottlenecks and structural issues that I've mentioned briefly that AI faces.

And so to do that, we can turn to the brain for inspiration.

And then the question that I was asking during my PhD is how can we capture the physical nature of biological intelligence?

So that's a brief background of where the state we're in.

and this is where neuromorphics enters right that's the background behind what we were just what the state we're in but neuromorphics is one suggestion into how to um solve this so neuromorphics the term neuro comes from neuron morphic meaning mimicking or copying uh neuromorphics

sorted into two main categories.

One is the idea of neuromorphic chips, which is something that is probably more familiar to similar to computer chips, CMOS, electronic electrical chips, stuff like that.

And they're used as mainly AI accelerators.

So how do we implement current AI models, the software on this new hardware?

to get more neural mimicking behavior and reduce power requirements.

And something that other people might be more familiar with is spiking neural networks, where the idea is we take traditional artificial neural networks and we turn them into this binary input output or binary inputs and binary outputs where it's just ones and zeros.

And so you get significantly reduced amount of data that you have to deal with.

The weights are significantly

more simple to deal with.

And so this is a much easier kind of data set to deal with and uses a lot more, a lot less energy as well.

On the other hand, we have this Memristive systems.

And so Memristive systems are systems that have this thing called conductance-based synapses, where the idea behind it, the word Memrista or Memristive comes from Memrista, which is a variable resistor.

And pretty much it exhibits memory as

a kind of current hysteresis in a current response curve.

That might not mean anything, but I'll explain that to you in a little bit.

Pretty much the idea is that we get these memories of systems that are much more biological, like when they're self-assembled, much more array like when the array assembled.

But they have this more non-volatile.

They have this more interesting behavior than your morphic chips do.

So there's some examples of this these

type of systems uh intel and ibm mainly work on ai accelerators in loyalty and true north um it's through the human brain um through human brain project we have developed uh spinnaker brain scales and a few other models to try and implement spiking neural networks which try and represent replicate spikes in the brain neurons and then memoristic crossbar arrays uh this idea where you have these sort of metal metal bottom and top electrodes you have this

cell in the middle and you get flow of energy electricity through that i'll explain a lot more about this later it's just sort of a bit of a note the thing that we are focusing on today though is something called nano i networks which fall into the self-assembled networks which means they assemble into a network from the bottom up rather than what you might imagine these other three categories do in this kind of top-down clean lab fabrication um

experience, and that makes them a lot more similar to biological systems.

So that's a big mouthful, but we'll get, we'll break it down a bit more.

So what are Nanowire networks?

Well, Nanowire networks are typically made out of a simple material, like a silver material.

They're nano, so they're really, really tiny on the orders of nanometers in width, about a thousandth of a hair, width of a hair.

And they get to about 5 to 10 micrometers in length.

So they're really long and very, very thin, quite similar to a neuron.

And on the right here, you see an example of what a nanowire network looks like, a silver nanowire network, so made out of silver material, compared to an example of what a biological system looks like.

So there's quite significant similarities in the structure of how they look.

So what happens is we deposit these nanowires, these silver wires onto a glass substrate usually, and they self assemble.

So they assemble into this network and they start to cross over each other.

So you can see wherever, let me get the pointer out.

Where is the pointer?

Sorry, there we go.

So wherever a wire crosses over another wire,

We assume that there's a junction or a synapse, similar to where a axon connects into a dendrite in neural structures.

And so the idea behind this, the structure resembles itself and the function also resembles itself.

That's what these two graphs at the bottom look like.

So when we zoom into these kind of junctions, as you can see here, that's a zoom in of that.

We get this silver, two silver wires are represented here.

We get this insulating polymer in between.

It's just a simple plastic called PVP polymer.

um in between and then when that is off meaning when there's no electric current being passed into the system nothing special happens the what the wires aren't actually touching each other there's no um electrical conductivity but then when we turn it on when we feed in some kind of input voltage input pulse like is shown here

we see this ion migration, this flow of atoms, positively charged atoms from one of the wires to the other through this plastic polymer.

And this is what we call like a switch.

It's similar to what an action potential kind of looks like, where action potentials occur and then you get this increase in conductivity in the membrane of the neuron.

We see a similar behavior in nanowires.

This is sort of the basic

blocks of how neurons communicate with each other, for those of you who don't know how that happens.

And we see a very, very similar kind of response occurring in these nanowires.

Now, I mentioned Memristors before.

Memristors

have this property of when this switch like behavior occurs, when you get this nanofilament forming between across here, you get it's a nonlinear activation.

So it's not predictable.

It's quite a difficult thing to model.

And when it forms, it has memory for previous times, previous signals that have occurred that have been input into the system.

So it forms at a certain pace, nonlinear pace.

Once we stop that pulse, it starts to decay slowly back across the network at a different rate than what it originally formed.

So you get these multiple complex behaviors occurring.

And because of that decay, the ions decay very slowly.

And imagine you've stopped the signal for a couple of minutes.

the the bridges decayed halfway and then you if you start the signal again that bridge is going to form a lot quicker than a bridge that hasn't been formed previously so that's where we get this memory aspect from it that's a junction level memory we also get all the beautiful recurrent behaviors of you know the information isn't just free forward um forward flowing like in um

like in artificial neural networks, we get this kind of recurrent activity that information flows in all directions in these networks, which is really, really important for information processing, biological-like information processing.

Okay, I've talked a lot more than I thought I would, so let's power through.

One really...


SPEAKER_00:
You can take your time.

It's no problem.


SPEAKER_02:
Great.

One really important, really interesting thing about nanowires is we can model them in a graphical representation where we take the wires and we represent them as nodes, these actual wires.

And whenever there's a synapse or a crossover between the junctions, we can represent those as edges.

And so that really helps us to look at the structural properties and also the functional connectivity of these networks.

So this is an example of what we've done with a model.

You can take a real nanowire network and you can model the density.

You can model the length of the wires, which we can measure relatively difficultly, but we can measure them.

And then you can construct a simulated version of it, which is definitely not one-to-one.

That's one limitation that we have.

It's very difficult to model exactly the same, but it is experimentally validated

meaning that the activity that we see in the simulated network, we see similar conductance, similar current, similar voltage in the real nanowires.

And what's really lovely and what I've published in my first PhD paper is you can model these nanowires in a graphical representation, as I mentioned before, and then compare them to other structures that occur in the real world.

For example, a fully random

network over here a completely lattice-like network where each node is just connected to its nearest neighbor and then our nano i network compared to a very simple biological system which is called the c elegans so what we see is that our biological nano network is much more similar sorry our

bio-inspired neuromorphic nano i network is much more similar to biological systems like the c elegans than it is to random networks or these lattice-like objects and these lattice sort of like structures is what we see in typically in cmos chips or compute computer chips um a bit more about the modeling i've explained a bit about how this conductivity uh

this low to high conductance occurs, but we can model that using a mathematical equation.

For those of you who like maths, this is the only equation that I will share in this presentation.

I don't like having equations on presentations, but it's a pretty cool thing to see.

Pretty much we can model when the voltage is higher than what we have a V set state, which is shown in this graph here.

So the voltage is on the X axis, the current on the Y axis.

Once there's a voltage set

Once we reach a V set and once our voltage is higher than that, we have this kind of switch that is on.

So we are in a high conductive phase.

And then when the derivative of this lambda function, which models this kind of filament,

is at zero, we have when the voltage is between the set state and the reset state that we have, we get this kind of activity where it slowly decreases, the current decreases and flips.

And then when the voltage is lower than reset, we get back to the off state where the current is at zero.

So this is what I was mentioning.

Memristors have this pinched hysteresis curve.

That's what you see here.

This is called the

pinched hysteresis curve.

And this is sort of the idea where we want to see this kind of nonlinear memoristive activity.

That's what makes a nanowire, a neuromorphic system interesting.

Don't worry about it if that's too much jargon for you.

It's just a bonus for the maths, whoever likes the maths.

So a bit more of a visual explanation of

Gareth J. How this what the nanowires network is actually doing this is a model of the network we've simulated where we have this input electric up here, which is represented by this green.

Gareth J. star and then we have an out a drain or an output electrode bottom right here we feeding in a voltage a standard voltage bias simple DC voltage bias and we start to see.

Gareth J. Each one of these wires, as I mentioned, each one of the wisest men is modeled by a dot here and each one of the connections between the wise is modeled by an edge and you start to see this kind of increase in current flow that's what this is showing current flow.

across the network from the source to the drain, you see this, what we call a winner takes all pathway, where a path is formed where these junctions switch on.

So all of these junctions are at that on high resistance state and all the others are not yet.

And then you start to see like more parallel paths beginning to activate once that original path is formed, which is a really interesting functionality that we can model.

we also see this network level kind of increase in g g is conductance so it kind of models the it's the inverse of the resistance so pretty much we start at a very very high resistance state and then we or low conduction state and then at some point and that point is at the the path of least resistance when the drain and the source and the drain are connected we see this massive increase in conductivity massive decrease in resistance and that's that

swap between low resistance to a high resistance state we see that at a network level as well as an individual junction level which is really interesting behavior this is this sort of non-linear activation function which might look familiar to some of you that have played with artificial intelligence before for those of you who haven't this non-linear non-linearity is sort of how we do our calculations and transformations in um

Richard Johnston, M.D.

: hidden states it's you know it's this there's different types of nonlinear activation functions, but there's the reticular and linear activation functions and not important.

Richard Johnston, M.D.

: This is sort of what this system does inherently we don't have to train it to do that it just that is the inherent properties of this network, whereas in artificial neural networks, we sort of have to play around to get what the best model is to fit that and.

The memory aspect, which I mentioned, also happens at a network level, not just a junction level.

So if you remember, I mentioned that each filament has memory for what happened previously.

The network also has that memory.

And what you can see is when we're feeding a bunch of

short small pulses for a short amount of time you get this increase in conductance at the network level what i explained previously and then it sort of resets back to its original state after a certain amount of time uh that's because each one of those junctions that i've mentioned here that is activated takes a while to decay and go back to its normal state and eventually it all goes back to normal but if you increase that uh the number the amount of time that you're

inputting this voltage and the number of voltage pulses, you can get the network into this kind of steady state where it doesn't reset fully for quite a long time.

And so you see this full network kind of activation level similar to long term memory or long term potentiation compared to short term memory.

This is just some really cool

Tim Jones- Things that we've seen in the network it's you know much more complex than than I have time to really delve into it, but I just if you have more questions, please feel free to ask I can I can answer as many as as needed, and so.

a lot of background a lot of kind of information dump but what can we actually do with these what is why are these cool you know why are we even interested in these well firstly we have to understand how we apply these so as i said we take these silver nanowires and we drop cast them meaning we take a little pet we take um from a pre um created

mixture of these pvp silver nanowire networks and we put them in the middle of an electrode array that we've set up so an electrode array might look like this where we have nine electrodes on one side that are measuring the activity from electrical activity from the networks and nine on the other side and you can kind of program any of these to do anything that you want nine and nine is quite small compared to a lot of electrodes but this is just a start that what we did in our system

And so, one potential approach to using these nanowire networks is something called reservoir computing.

That's a bit closer to artificial neural network kind of implementations.

The idea is you have your inputs, like an artificial neural network, you have your inputs, you have your hidden layers in the middle, and then you have your outputs.

The main differences in reservoir computing is the reservoir system

or the in-between layers are untouched.

We do not touch them.

In traditional AI, the idea is you use something called back propagation to change your inputs to get your outputs

you change the weights of each one of these kind of nodes to try and match your outputs with a desired output that you have.

And if the desired output is not similar to your current output, you back propagate, you change the weights of the middle layers, and then you try and make that a bit closer to the output.

That's how traditional ANNs work.

In reservoir computing, we just let the network kind of do its thing.

We don't change the weights.

We just let the inherent physics

um do what it wants and then we train an out on the outputs we train a very simple model to try and understand the outputs from the network and so we can do the same with physical systems like nanowire networks

where we just take out a traditional reservoir system and we put in our nanowire networks.

So what does that look like?

Well, in a task which I did in, well, a bunch of my collaborators did prior to me and also I did in my PhD, we have this input signal.

It's called the nonlinear transformation task.

We feed in a sinusoidal input, just a simple sine wave to the reservoir system, which is our nanowire networks.

We let them do their thing.

We don't touch them.

They have some physics that they do.

based on all that kind of interesting stuff that I explained before.

We get the linear readouts from that network and we train some weight matrix to try and using linear regression, very, very simple kind of method to try and convert this input signal into a different output signal.

And the only way that that can happen is if the reservoir system in the middle has nonlinear behavior, interesting kind of

behaviors pretty much to try and that can transform that input signal inherently and that we don't need to train a very complex system to do.

The other task, for example, is something called the memory capacity task where you have a random input signal that you're feeding into the reservoir.

at each time point and then you delay your signal by however many steps and then you're trying to reconstruct the time points and steps backwards.

These are just a couple of examples.

Don't worry if it's a bit too confusing, but

So what did I do?

Another thing that I did, which was really interesting in my second paper was trying to combine these kind of interesting structures of nano wire networks, which make them a lot more biological with this reservoir computing model.

And so over here we have a nano wire network.

All of these use the same nano wire network model, but we just changed the structure.

So this is an example of a fully connected two layer nano wire network.

not nanowire network, sorry, artificial neural network, a fully random network, a nanowire network, and a biological network called the C. elegans.

And we did, we performed these two tasks and we showed that the nanowire networks at certain values outperform, if not perform as well as other

types of networks without the need for these.

And most interestingly is these bipartite networks which we use or these ANN type structures perform the least effectively when using our models.

So that shows that there is an importance in this biological structure as much as there is in the biological function, which I've been talking about previously.

So this this kind of combination of the two is the most important aspect.

And lastly,

last thing i want to talk about about the nano i networks this is a very speed run introduction into it was my final paper in my phd which was actually trying to take brain inspired um learning tasks and implement them in this biological intelligence so reservoir computing was one way to try and implement some kind of ai like task into this but that's not the point of this

talk and the point of my project was to try and get a lot more brain inspired learning, a lot more brain inspired intelligence, those kind of aspects.

And so that brings us to this type of task where I tried to implement a brain inspired learning task, inspired by a psychological task called the NBAC task, where you give a target pattern to participants, then you give them a bunch of different patterns, and then you ask them after a certain number of steps,

do you remember the original pattern that I presented to you?

And that's what we did to this network.

We presented a specific pattern, for example, specific pattern number A, pattern A, and then we gave it a bunch of different patterns.

And then we asked it, we wanted to see how many steps back it can remember that original step, that original pattern.

That's pretty much.

And if it recognized the pattern successfully, we gave it some external feedback.

So we either punished it or rewarded it.

And if that was in what we call the physical reinforcement category, and if we had no reinforcement, then we didn't punish it at all.

We didn't give it any feedback and we just saw what it would do.

And so the results.

So this is an example of what it looks like a bit more clearly.

So if our target pattern here is pattern A, we give a bunch of patterns, then we give pattern A, then we give a couple of other patterns, and then we ask for pattern A again.

So this would be three steps back where N equals three.

So the N back would be

And this would be kind of what we feed into the network on the left is where the inputs are on the right is sort of what the output that we wanted to get associated with.

And each one of these patterns has a different output that is associated with it.

And what we saw is that the accuracy, this is a bit of a comp, a bit of a, you know, difficult thing to look at, but pretty much the square, the square, um, dots, uh, what we saw experimentally, the circle

Gareth J. dots what we saw in simulation and the reinforcement, so the what we see with with the reinforcements and giving them reward and punishment is at the top here, and what we see without giving them a reward and punishment is at the bottom so interestingly enough.

Gareth J. At this is the number of steps back where we presented that original.

Gareth J. pattern so, for example, at n back equals seven the original pattern will be right at the start and then be a bunch of other patterns and then we'd ask it again.

And so what we see is that without any reinforcement, there is this gradual decay of memory over time or that pattern.

And that makes a lot of sense.

Humans are pretty bad at this task as well.

We forget quite easily after about seven steps as well.

So that's quite an interesting thing.

But what's really interesting is when we implement this reward and punishment application, which is based in biological reward and punishment, something called supervised learning and reinforcement learning, these two things combined.

we see a massive increase in memory for this pattern and the target.

Importantly, we also saw that activity when we had two targets.

So if we had like a target A and then we swapped and asked for target B, it managed to remember both of those at the same time, which is really interesting.

We haven't pushed that to the limits, but that's a really interesting aspect.

And so that shows that we can,

Gareth J. get this kind of biologically inspired learning mechanisms to give us some interesting activities in this quite complex biological inspired system.

Gareth J. How does that look i've got a video hopefully that will work on me yep yep yep we got there come on, you can do it Okay, maybe I need to select the.

Gareth J. Sorry pointer options turn off laser pointer play.

So this is an example of without the reinforcement.

We have our inputs on the right here, which is a bit confusing.

And then we have our target pattern in this circle type pattern.

So what you'd want to see is a clear path from anywhere around here to this target, meaning that the network is learning to prioritize that pathway over time.

Without reinforcement, we're seeing that yes, sometimes that path is being formed, most likely when

You know, that signal is being implemented, but a lot of the time it's not, it's not really prioritizing.

And on the other hand, in the reinforcement paradigm, you'll see pretty quickly, we start off with this sort of random kind of path activity.

And then you'll see that over time, this path is remembered by the network and prioritized.

You do still get other pathways, but they're slowly kind of weakened over time, but this path is almost always staying there.

And that's the idea behind this reinforcement.

Tim Jones- Learning and so the network has remembered that whenever we're given that input signal, we want to go to that output time.

Tim Jones- Which is really cool to see I think this is a really cool video and there's a summary image of what happens over time, but this is flipped so the inputs and out on the left sorry I know it's annoying but i'm.

Tim Jones- The pretty much the red regions, the regions that are strengthened between this when we give reinforcement and we don't give reinforcement.

And the blue regions are what are weakened.

So the network is effectively learning to weaken these pathways and strengthen these pathways.

Similar to what you might, you know, you might say it's similar to this pruning activity or strengthening activity in a synaptic kind of brain like activity and strengthening of synapses, which is really, really interesting.

Really cool that we managed to see this.

So that's quite a lot of information.

How does this even link into what I'm doing now?

So that was my PhD.

Right now, I'm working on real neurons at cortical labs.

And neuromorphics are really cool, especially neuromorphic 901 networks.

They try and replicate the brain.

They try and replicate, well, on a much smaller scale, neurons and synapses.

They do it in this nonlinear type of activation, this really interesting structure and function

combination that mimics that of neurons and synapses.

But they're not neurons and synapses.

And so we know that neurons and synapses and other types of cells lead to really cool stuff.

I'm sitting here and talking to you because my neurons and my glial cells and my interneurons and all that kind of cool stuff works together to allow me to speak, to think, to breathe, to do all the cool things that I can.

And we're not at the stage where neuromorphic nanowires or other neuromorphic systems can do that.

They can do a lot of really interesting stuff.

They can definitely reduce the amount of energy requirements that we have.

But we're definitely not at a stage where we can implement that.

So why not try to do it in neurons?

We know that neurons can do it.

Let's try and do that.

And that's what Cortical Labs is trying to do.

So pretty much what we do very, very briefly is we take

induced pluripotent stem cells.

We take a blood sample from a donor, a human donor, very, very non-invasive, simple procedure.

We take some blood like you would at a blood test or if you're donating blood.

And the amazing biologists in our team managed to convert it into something called induced pluripotent stem cells, which means cells that can be reprogrammed to be any type of cell.

And we do this

We change these IPSCs, induced polyprotein stem cells, into something called urinal cultures or neural cells that are very much cells that are in the cortex, very similar to cells that you and me have.

In fact, if the blood sample is from you, those cells would have the exact genome that you have.

So we've seen that in patients who have epilepsy, if we take their blood, the cells that we take also have epilepsy, which is really interesting.

We can also take it from mice, embryos, or other types of animals, which a lot of other people do.

And then what really is cool about and differentiates cortical labs is we teach them to play games.

And we do that using the free energy principle, which, you know, where I had to bring in some active inference related stuff.

And so that's sort of what we did in our first paper.

We presented, we modeled this kind of pong environment, the game pong, hopefully most of you are,

um know this game and the neurons we tell the neurons we put them on this chip similar to what i showed in the nanowires and we tell them you are the paddle control the paddle based on this kind of structure this setup that we give you and try and predict where the ball is going to go and the way we do that is we give them either an unpredictable feedback or a predictable feedback um

And when they miss the ball, we give them this unpredictable random kind of feedback.

And when they hit the ball, they get a predictable sensory feedback.

And this fits quite nicely with the free energy principle, the idea that we're trying to minimize the randomness and try and get some predictable, interesting behavior in order to sort of maximize our reward and maximize our understanding of the environment.

We also give a silent feedback where nothing happens.

So the ball just continues.

just as a control.

And what's really interesting, and I didn't actually include the graph, but I can refer you all to the papers.

And I'm sure Brett, the CSO of my company, has spoken a couple of times here.

So you may be familiar with that.

But we see that when we give this predictable sensory feedback, the neurons learn to play this game very, very quickly, within 5 to 20 minutes of playing the game.

And they play it at statistically significant, statistically better than average, more than chance would expect.

And so this kind of encoding scheme where we're just giving this predictable versus unpredictable sensory feedback, that's the only encoding that we're giving them, we see quite interesting

you know, support for the free energy principle being used in this kind of new rule based system.

So there's a lot of stuff that I've covered today.

There's still so many questions left.

I'm more than happy to answer those questions, ask, raise more questions.

If anyone wants to ask me in person, this is my email here.

Add me on LinkedIn.

I'd love to chat to anyone and everyone about this.

You know, 40 minutes it took in the end is not enough to cover everything that I did, but I hope it gave a nice overview and a nice start for people to

go in and try and understand and ask questions.

So thank you very much to my supervisor for my PhD, my current bosses and workmates, and for you guys for having me and for listening.

And I look forward to your questions.


SPEAKER_00:
Thank you so much.

That was absolutely fascinating.

That was amazing.

Thank you.

I have so many different questions.

I don't even know where to start.

I mean, the main thing I was wondering about is when you were showing the networks with the MBAC test, which was super interesting.

Romy Ellis- I know why networks and you were talking about rewards and punishment I was quite interested yes implement that but then, then you show the work at critical level pong and it's about.

Romy Ellis- unpredictable, so is it a similar type of reward and punishment that you did with your simulation with your nano wire networks to that.


SPEAKER_02:
So, unfortunately, I hadn't.

I wasn't aware of active inference for energy principle all that cool stuff when I was doing my PhD that was.

I was introduced to that at cortical labs I would have loved to try that on these nanowires I imagine it would be a fantastic way of doing this, and we did something.

more similar to what you would imagine in gradient descent but also this kind of feedback so in a sense we're trying to give a more uh not maybe not predictable but we're trying to match what the output of the network is with what the expected output is so maybe that is in a little bit like a predictable kind of trying to increase the predictability but pretty much what we did is we changed the can you see my mouse or do i need the

Guy Cowardly- No, you can see one okay great which pretty much what we did was we changed the voltage of the.

Guy Cowardly- The desired output electrode and so, if the correct paths were be.

uh formed because this is a physical system and it's a metal system at the end of the day you get this kind of path of least resistance uh forming from where your sources where your inputs are to where your drain is the drain is pretty much just a grounding you know a zero voltage kind of thing and but you can play around with that voltage it doesn't have to be zero you can increase it you can decrease it you can increase and decrease the other ones and so we implemented something similar to gradient descent on these

uh voltages on these electrodes and when voltage is increased for example then you imagine there's less current or less current will flow to that um to that electrode and when voltage is decreased more current will flow to the electrode so in a very basic level that sort of will be implemented but we saw that wasn't enough that really wasn't enough to train it it needed this rewarding kind of thing so we needed to give some kind of threshold that when that's reached

we increase it.

It was a bit more complex than that, but that's pretty much the idea behind what we do.


SPEAKER_00:
Thank you.

Okay, very interesting.

Cool.

Yeah.

Does anyone else have any questions?


SPEAKER_01:
Yeah, I would have one question.

Please, yeah.

Thank you.

Thank you.

So you started your talk with motivating it with the energy consumption of the brain.

Correct.

So can you give us a kind of rough estimate of how much these nanowire systems?


SPEAKER_02:
Right.

They're tiny, tiny, tiny on the scales of watts.

You know, we're talking even significantly lower than that.

The only energy that's kind of being used in the system is what you pump in from

the, you know, the, the inputs that you give it.

So you have to give some kind of voltage as an input or, you know, and then some current is, is formed from that.

But the system itself, the beauty of this is because of it's a, you know, it's a fully connected kind of physical system.

We can solve the weights automatically using Kirchhoff's laws of current and voltage.

It's just a physics problem at the end of the day.

And so.

Andrew Mashman- physics just does it for us this spreading out of the voltage the spreading out of the conduct of all that stuff as I showed, if I go back here.

Andrew Mashman- Right all this stuff that we see here the spreading out of this kind of activation happens inherently based on the structure of the network.

Andrew Mashman- And that's why it's so important to China and understand that structure and combine it with a function so.

Andrew Mashman- When you think about it that way that's significantly lower energy requirements, then.

trying to change each weight individually of each one of these, you know, junctions over and over and over and over again to better, you know, get a better result.


SPEAKER_01:
Okay.

How many signups typically could you realize in such a physical system?


SPEAKER_02:
For sure.

In our simulations, we use significantly smaller simulations because that's what requires a lot of energy to simulate.

We use about 100 to 700 wires.

In the actual system,

tens of thousands, if not hundreds of thousands of wires per system.

The really other interesting thing is each one of these systems will be unique.

So you'll have a slightly different structure, slightly different kind of way that it's implemented.

And a lot of people think, oh, that would make it function significantly differently, but we don't see that.

We see, you know,

standard kind of characteristics obviously there's variance but you you know the same as with humans and with brains you see very different structures and very different uh you know experiences that humans have and there is a variance in in our who we are but for much of the case our activity and our the way we function is quite similar at the end of the day


SPEAKER_01:
okay but um in in the systems you actually really realized physically um how many so i guess if you have like ten thousands of wires um only a subset of those would actually form correct functional potential synoptic connections right so there's probably correct are two are like electrically isolated from each other right so i guess 100 so


SPEAKER_02:
Tim Jones- The really nice thing about these for now is that they're two dimensional.

Tim Jones- And so, because they're two dimensional a lot more of the wires crossover each other in kind of separated regions, so you wouldn't you wouldn't get, for example, the right region connected to the left region right.

Tim Jones- And so that makes it a lot simpler to model when you start getting three dimensional models, you get this kind of you know.

wrapping around and a lot of different connections connecting to each other so they could be a lot more connections or a lot less connections it doesn't necessarily mean that when a wire crosses over another wire a perfect connection is formed so it's quite difficult to expect this and model it that's why we have to rely on electrical characterization and seeing these kind of conductance curves here right comparing the conductance curves versus with the models that we have with the actual


SPEAKER_01:
uh the actual physical systems that's how we do that kind of model okay yeah thank you and for the for the dish brain for the like biological cells so i mean the brain is actually fairly structured right in the end yes correct so are you working in the direction of also using a bit more of the brain's morphology absolutely different cell types and also layers


SPEAKER_02:
So we have a whole bunch of different cell types that we have.

We do all sorts of combinations of excitatory, inhibitory cells, glial cells, astrocytes, all that kind of stuff.

We're currently still also working on two dimensional layouts or two and a half dimensional layouts.

But we've had some collaborations with people, for example, John Hopkins University in America, which I believe I can talk about.

Hopefully I can talk about where we work with organoids.

um which are three-dimensional tiny little kind of cell structures they're not fully you know structured like what you'd see in any kind of brain organ structure but they're a lot more similar to a tiny kind of brain structure and so what we're really i mean my end goal in this is to try and stack chips together you know so a chip might be have a two-dimensional structure but then you can have it

um touching another chip forming you know forming long connections across that kind of barrier with with another chip and then your first chip might act as some kind of sensory region and your second chip might be a processing region right and then you can form these pseudo structures in that way and what's beautiful about neurons compared to nanowires which i think is another reason and i didn't touch upon this is neurons self-organize after their

um placed down so with nanowires you put them down they self-organize and they never move again you know they will never move again because they're silver wires and you cannot move neurons automatically are constantly looking for connections constantly changing their structure based on the input that is given to them so you know they will form inherently structures as well um

And we're really, we're exploring that, we're exploring how that happens when you're giving them predictable feedback, how that happens when you're giving them unpredictable feedback, how it happens when you give them no feedback at all.

And these are all the kind of questions that we're trying to, you know, answer at Cortical Labs, which is, I think, I find extremely interesting.


SPEAKER_01:
Okay, thank you.


SPEAKER_00:
It's interesting.

I'm really interested in the applications, which you also briefly touched on in terms of when you said you get a donor with epilepsy and then you...

their cells also exhibit epilepsy, which then leads me on to thinking about, well, the applications of this.

Could you potentially see this as sort of like a neural aesthetic or something where, say with someone with epilepsy, they have that little part of their brain with the high concentration of epileptic cells removed sometimes.

I think that already happens now with epilepsy surgery.

Could you see this almost of them being an implant where you've taken their cells, you've edited out their epileptic gene if it was able to be identified.


SPEAKER_02:
then you reprogram them into neurons basically without epilepsy and then sort of put them back into the bit that you could take out i think that would be absolutely fascinating um i think there's so many different ways this can go down luckily i'm not the one who has to decide that i have a lot of ideas myself uh but uh i think one really key thing that we i should highlight is you know what what you mentioned is really really interesting and

Some people might think, oh, that's currently already being done in brain computer interfaces, right?

There's a key difference between what we're doing and what brain computer interfacing is.

Pretty much the idea with brain computer interfacing is you have a chip that reads the activity of the brain, right?

And it spits out whatever that activity is.

And then you have some very clever algorithms to understand what that original signal was in the brain.

So for example, you might think of a tennis game.

then you get some kind of electrical signal that represents a tennis game and eventually these algorithms will pick up that you're thinking of a tennis game now that's really cool really important going to help a lot of people but there's a significant limitation in that whereas we cannot

We don't know what the brain is doing still to create those images of a tennis game.

Why are certain regions lighting up to represent that?

And brain-computer interfacing is not giving us any information about that.

It's just trying to reconstruct what is already taking place.

And what we're trying to do is we're actually trying to figure out the algorithms and the encoding schemes, whether it's free energy principle or whatever else might be, hopefully it is something

as straightforward as the free energy principle, where we want to actually write those encoding processes into these neurons, teach them the output that we want them to give us rather than just interpret what the output is.

So we're doing the encoding into the networks and the decoding, whereas the brain computer interface is sort of just the decoding.

And so, you know, in an epilepsy kind of situation, you might be able to

have a chip like that in the brain.

This is just, you know, pure conjecture, but it could be interesting.

And you feed in some kind of input into that chip, which programs the cells that it's, you know, that are being controlled by that to give a different output than the epileptic output that we're already seeing, right?

And so rather than just saying, oh, that's an epileptic fit, we can predict that

It's like, okay, yeah, we can predict it, but now we can actually do something to change it, right?

We can actually change it with some kind of input.

I like to imagine in a very futuristic kind of application of this, hopefully not too futuristic, but in a very futuristic application of this, you know, in the matrix when Neo is plugged in and he suddenly goes, you know, his eyes go blank and they're uploading all this stuff to him and he wakes up and he goes, I know Kung Fu.

That's the very futuristic sci-fi application of what we're actually doing.

We're actually encoding Kung Fu into these cells and then trying to replicate that.


SPEAKER_00:
That is absolutely fascinating.

I was actually going to ask you, what are you most excited about in this field?

What do you see coming in the future?

For you, would it be the brain-computer interface space?


SPEAKER_02:
I think the brain-computer interface space is

fascinating.

You know, we're seeing really interesting companies, even local Australian companies like Synchron overtaking these multibillion dollar organizations in this space.

We're going to see a lot more interesting kind of predictions of what people who might not be able to communicate normally and reconstructing their thoughts and what they're actually trying to represent.

And so that's going to transform a lot of people's lives.

It's going to transform medicine in that sense.

But what I'm much more interested in, you know, it's still very interesting and it's going to be really cool in the near future.

But what I'm much more interested in is this idea of can we write things into neurons?

Can we program neurons to give us the kind of outputs that we want rather than just, you know, interpreting what they're already doing?

And that's what we're doing at Cortical Labs in the long run.

It might take a long time to get there, but really, really interested to do that.


SPEAKER_00:
Very cool.

And then something which obviously people are talking about a lot is like the ethics of AI and the explainability and the transparency of AI, which is a current problem with the black box methods that we have.

One of the things I found interesting about Active Inference is people talk about how you can actually have much more transparent and explainable AI potentially.

And I see what you're doing with the, do you think with the nanowire networks, you also have the opportunity for that because you can actually go in and we didn't get onto how you actually visualize and analyze them, which is interesting, but like, do you think that this type of AI would be more explainable than sort of traditional artificial neural networks?


SPEAKER_02:
Oh, without a question.

I mean, you know, the reservoir computing approach that I, that I touched on does use that black box approach, which is why I didn't really like it.

I used it for the first couple of, um,

papers that I worked on and then was sort of like, this is not how the brain works.

Maybe it is, maybe there are some aspects of it, but the brain is not a black box.

I mean, to us it is, but you know, there are so many little pieces that work together to, you know, create these kinds of, uh, emergent properties and there's rules that govern that and there's structures that govern that.

And so there must be a way to implement those kinds of rules.

into these systems and then be able to actually see them learning.

And that video that I showed, you can see how it's changing over time, which is very, very different to these black box approaches where we don't even know why different weights changing results in this kind of learning.

I think in neurons, you can do all sorts of cool calcium imaging.

You can do infrared kind of microscopy.

I'm not a biologist, so I'm sorry to my team if I was wrong about that.

But we have them on these transparent cells, arrays, so we can actually see, we can take images, we can put them in the incubator, play a bunch of long games, take them out, take some images, see how they've shifted, see what's happened, and try and correlate that with the activity.

That's a much more transparent way of,


SPEAKER_00:
learning in my opinion very cool so it's almost like we have more energy efficiency we have more explainability and we have like these opportunities for personalization almost like with your own uh cells in the in the chip which I think is really cool it's uh it's a very exciting direction I think for lots of different Technologies I guess not just brain computer interfaces as well for sure and you know I think what's really interesting is how we can combine these Technologies like neuromorphics with neurons


SPEAKER_02:
Neuromorphics are trying to replicate neurons.

Is there a way we can use neuromorphics to kind of act as like, you know, eyes, you know, our eyes do this pre-processing kind of activity where you take in a light signal and electrical signal and convert it through some kind of cell, you know, activity into a more understandable signal for your occipital lobe and your other regions of your brain.

Can we use neuromorphic systems like nano eye networks to take in a raw input

and output a more biological-like signal, which then neurons would be a lot more receptive to.

I said we'd really like to encode these right things into neurons and get them to do things that way.

But the question of how to do that is a really difficult question.

Otherwise, people would have figured it out by now.

We have a lot of theories, but one of those might be as simple as let's use these technologies that are taking in complex inputs and making them into a more biological-like output based on just their inherent properties, and then feed that interface with a biological system.

Who knows?

Maybe that'll be a really cool way of getting that kind of encoding and writing in method.

So that's something that I'm really interested in, in marrying those two worlds together as well.


SPEAKER_00:
OK.

I'm conscious I don't want to go too much over time, but I do have a little bit more time.


SPEAKER_02:
I've actually got as much time as you need to.


SPEAKER_00:
Anyone else, please jump in with questions.

But I did just want to touch on something which I'd heard about with your work before, where you mentioned about the quantum aspects of the silver nanowire network.


SPEAKER_02:
Yes.


SPEAKER_00:
Not a quantum computer scientist by any means, but I'm interested in this because there's sort of this debate in neuroscience about whether or not there's a quantum brain, like the brain of a quantum.

Maybe, you know, for olfaction, there's been some suggestions, some theories about that's how we smell with

with quantum tunneling and the olfaction receptors, but the rest of the brain people seem to be, the jury seems to be still out.

And I'm just interested in your network seem to use, you mentioned that there was like some quantum tunneling forming the bridges potentially.

Is that your work or was it just, you know, something which is interesting, but wasn't really you examined?


SPEAKER_02:
So it was actually more of my colleagues, Joel, and we have a paper in nature comms.

nature communications about that.

So I can definitely share that with people.

If you Google my name, it's the first thing that will come up.

But yeah, I love that we're covering all the buzzwords here.

It's fantastic.

So yes, the quantum tunneling, or we call it electrode tunneling, we see this behavior.

It's sort of the only way we can explain this.

And so that's sort of why we've hypothesized.

And Joel's shown some evidence for this.

But the idea is that

You start with a no bridge here on the left, for example, and over time you get this fully connected bridge, but that doesn't, it's not like an immediate kind of response that takes time to sort of the ions flow slowly through that.

It's quite quick, but it's not an immediate response.

But what we start to see is if, for example, the bridge is kind of semi-formed and let's imagine these two dots here, aren't there these two little, um,

balls at the end or ions aren't there, you can see, you start seeing these weird fluctuations of conductance before that switch is fully formed.

And that's explained by this quantum tunnelling or electro tunnelling of electrodes jumping through the gap between the whatever leftover gap is there just by chance.

And that actually accounts for really interesting behaviours

that I haven't covered today.

I don't really fully understand them myself.

That's not my area of expertise, but Joel has explored them and has some really interesting theories about how that might affect the networks.

One example is this idea of criticality.

So criticality is sort of the theory that, you know, biological systems or a lot of systems, actually real world systems have maximal information processing at the edge of criticality or the edge of chaos.

So

A chaotic system is one that has sort of completely unpredictable as full chaotic activity and an audit.

On the other hand, an audit system is something that nothing interesting can happen because you can, you know, exactly what's going to happen.

It's fully predictable, you know, every single aspect of it, but somewhere in the middle, somewhere in between where you're not too chaotic, there's still quite a lot of unpredictable behavior, but there is a predictability to that chaotic mess.

It's called the edge of chaos.

That's where things seem to thrive, life seems to thrive.

And we've seen criticality in nanowire networks.

We've seen criticality in our neurons as well in cortical labs.

And one of these ideas is that potentially the electro tunnelling might help tune the system into this critical state.

We don't really fully understand it.

If you're really interested in that, the paper is a really cool

summary of that, I guess.

Yeah.


SPEAKER_00:
I'll get the link off you and I'll make sure you put it in.


SPEAKER_02:
Sounds great.


SPEAKER_00:
YouTube description.

Um, yeah, cuz I'm really interested in sort of what these synthetic systems can then tell us in turn about the brain for those sort of things.

We've been.

Mm-hmm time, like whether or not the brain uses quantum and it's interesting actually as well, because there was a Loihi chip and they used that for the neuromorphic nose, um, and that performed quite well.

So it would be interesting if you could sort of combine

with these types of networks that maybe do have the quantum effects and sort of test out these more quantum theories of olfaction, that would be pretty cool.


SPEAKER_02:
For sure.

I think there's so much, you know, to explore here.

Look, I have maybe been unreasonable to some of the neuromorphic chips like he in this talk.

I know I only briefly spoke about it.

There's definitely space for that.

They're doing amazing things.

There's some really cool learning algorithms and some really awesome people working on this stuff.

I think my perspective from this is that we want to try and replicate more brain like learning and more brain like activity.

And for that, we need a structure that is more brain like as well.

Um, the structure function act behavior and, and, and relationship is key in my opinion, to unlocking the emergence, the emergent properties in, you know, the brain.

Um, and so a lot of those chips lack that, um, and hopefully people, you know, we'll work more and more on these brain inspired systems, um, and, and start to


SPEAKER_00:
consider the structural element of it as well because it is extremely important and i guess you've spoken about the nanowire networks and you've also spoken about the more neuronal networks like where's your work going the direction like are you moving away from the nanowire networks or can you see like a hybrid of of both actually being quite useful in the future yeah so i think i touched on that briefly you know marrying the two nanowires and and um


SPEAKER_02:
and neurons together as sort of this like encoding bridge or something like that.

That's, you know, unofficially something that I'm interested in.

I'd love to explore that, whether or not that's something that I'll have the time and the resources to do.

We'll see.

But that's definitely where my interests lie.

I'm very happy to be able to apply, you know, my knowledge and my expertize in these kind of algorithm development and brain learning.

in actual neurons.

So that's sort of what my focus is on at the moment, but I'm still very much in touch with my supervisor and my team at University of Sydney.

And, you know, we've got a paper coming out later this year, or I guess next year.

Or did it just come out now?

I think it just recently came out, actually.

What am I talking about?

Gosh, what year is it?

It just recently came out in Nature as well.

So Nature comes.

So yeah, go check that one out as well.


SPEAKER_00:
Very cool.

Okay.

Has anybody else got any questions before we wrap up?

Cool.

Okay.

I've got one last question.

One thing I'm always curious about is if there's any sort of major problems in your field that you want someone to go out and solve almost like, cause anyone might watch this and they might be list, they might be working on some obscure little problem.

And do you have any asks of, it could be, you know, this mathematical theorem or, you know, create some hardware that we can do this weird thing with like, what would be your main asks?

I think for anybody listening who might work on something weird and wonderful.


SPEAKER_02:
So I think for that, let me just go back to the final slide again.

I know I'm jumping around back and forth, but I think what I'm working on most at the moment and what I think is the most important thing for our work and for anyone who's working in this space is figuring out that encoding paradigm.

What is it that the eyes do, that the ears do, that the nose does?

Actually, I think the nose does at least, but that our sensory organs do so well

to encode what might be seen as nonsense information.

There's obviously structures to it and it's not nonsense, but it's light or sound or very different forms of information into something that the brain can understand and make inferences about.

And not just make inferences, build models of the world to try and replicate it.

Active inference is all about trying to understand that.

How do we do that?

Obviously that is a huge question, but in a simpler way, how do we encode a simple electrical impulse or a string, a temporal sequence into something that neurons can understand.


SPEAKER_00:
Okay.


SPEAKER_02:
And we can, we can actually elicit different outputs from.


SPEAKER_00:
Very cool.

Okay.

So if anybody's listening and they're working, please get in touch with Alan because, uh, please do.

Yeah, that would definitely catalyze the field, I guess, solving those kinds of questions and problems.


SPEAKER_02:
If we can solve that, then yeah, I mean, we'll be having very different conversations in the next few years.

I think very, very exciting conversation.


SPEAKER_00:
Well, hopefully the more stream will still be going, we can bring you back.


SPEAKER_02:
I would love that.

I would absolutely love that.


SPEAKER_00:
Okay, right.

I guess we'll end there.

Have you got any last comments or anything before we end the recording?


SPEAKER_02:
Just feel free to email me, reach out.

I think science needs to really be open and collaborative.

And, you know, I do work for a company, but I'm also still very much about open source work.

And, you know, I will... And add me on LinkedIn and let's just keep chatting.

Let's just keep answering these questions.


SPEAKER_00:
Right.

Thank you so much.

And thank you everybody for listening and for joining the call today.


SPEAKER_02:
And thank you for having me.

See you later.
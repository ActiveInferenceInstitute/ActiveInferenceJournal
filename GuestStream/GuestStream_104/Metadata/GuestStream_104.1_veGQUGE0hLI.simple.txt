SPEAKER_01:
Hello, welcome everyone.

It is the 22nd of April 2025.

We're in Active Inference Guest Stream 104.1 with Tadahiro Taniguchi.

We will be hearing a presentation and a discussion on collective predictive coding and active inference, a new perspective on emergent communication.

So thank you for joining and looking forward to the presentation.


SPEAKER_00:
Okay, thank you very much.

Thank you very much for giving me this great opportunity to speak at this great live stream.

I'm Tadahiro Taniguchi from Kyoto University, Japan.

Okay, today, I will be talking about collective predictive coding and active inference, a new perspective on emergent communication.

So maybe for many of you,

the collective predictive coding, maybe a new term.

And I would like to introduce this, our theory.

Okay, so before starting, let me introduce myself.

I'm Tadahiro Taniguchi.

And actually, I have been working on the simple emergence for a long time.

And in more

process the ai and robotics and especially cognitive developmental robotics is my background okay last year i moved to kyoto university and i'm now the professor at kyoto university in japan so if you have a chance to visit kyoto it's a beautiful city please stop by our laboratory okay

Anyway, let me start.

So before starting, let me share the key points of today's talk.

So I will propose the Floch Collective Predictive Coding, CPC in short, as a dynamic underlying simple emergent system.

I will explain what is a simple emergent system later as well.

and present a formulation of language emergence based on probabilistic generative models, showing its relationship to the free energy principle and active inference.

They also position CPC as a natural generalization of FEP to the group or societal level, and present the CPC hypothesis.

as an interpretation of why large energy models appear to understand the structure of the physical world, and also introduce new perspectives derived from or inspired by CPC, such as CPC as a model of science and quad process theory.

Maybe the topic will be too much, but I would like to introduce our idea.

I hope it is exciting.

Before diving into today's main topic, maybe the CPC, as I mentioned, CPC will be a new term for you.

Let me introduce the collective predictive coding CPC hypothesis briefly.

This hypothesis extends the idea of predictive coding to social and linguistic phenomena.

It states that similar systems, including language, are formed through collective predictive coding by humans.

In other words, symbol emergence or language emergence can be understood as a social representation learning process.

As a result, the structure of the world may be encoded in distributional semantics in language.

The hypothesis also suggests that this is why LLMs appear to understand the world.

They inherit this structure through vast amounts of distributed human-generated text.

The main topic is CPC and free energy principle and so on.

But let me choose the LLM as the starting point of today's talk.

Maybe the LLM because everybody knows LLM recently.

OK.

So LLM, we have LLM.

But what are they actually based on?

So they are completely reliant on human language data.

Let's take a moment to reflect on our linguistic lives, our language, human language.

Actually, as I showed on this slide, each individual possesses a unique set of linguistic and word knowledge.

But no one has complete knowledge of language or knowledge of the world.

And actually, we dynamically learn, even when we were children or infants.

So we dynamically learn and adapt our linguistic knowledge through communication, including reading, writing, and conversation.

So, language system itself is not a concrete fixed system.

So, that is always changing and that is above ourselves.

So, nobody has complete the language system.

Importantly, through

communication, the language system, as I said, itself evolve over time.

In recent human history, the vast amount of linguistic data have been accumulated in massive datasets on the internet or server.

So by training,

On these corpora, using next token prediction, and also, of course, we can use some reinforcement learning method, the large neural networks have internalized the distributional structure of human language.

As a result, LLM exhibit surprisingly high level of language understanding and usage.

This achievement makes a major discovery of the past decade and indeed a significant milestone in human history, I think.

But, as I mentioned, this capability is very much relying on our language that we created.

So the question remains, how does human society form language in the first place?

How do we share meaning without directly accessing each other's minds?

How did society create a language system without any centralized learning process?

These profound questions motivated us to engage in constructive research on language evolution and language learning and symbol emergence.

And actually, we have been working on this kind of constructive studies since around 2010.

Our community has been studying symbol emergence in robotics.

SEL, in short.

SEL takes a constructive approach to cognitive systems inspired by developmental robotics.

So actually, we developed many unsupervised learning methods and artificial cognitive systems for multimodal object categorizations and concept formation, language acquisition from speech signals, and so on, to better understand human developmental cognitive systems.

and the emergence of language.

So actually, recently, this kind of self-supervised, unsupervised learning on multimodal data is very, very common recently.

But 15 years ago, it was very, very characteristic research to my understanding, to my knowledge.

And also, most of our models were based on probabilistic generative model.

and predictive models in other words.

We defined the symbol emergence in robotics in this series of research as constructive approach to symbol emergence systems.

As a part of this annual research, we have been working on world models and predictive coding as well in collaboration with co-authors from

the active inference community, we have published several papers on this topic.

For example, we co-authored world models and predictive coding for cognitive and developmental robotics alongside roboticists, neuroscientists, and cognitive scientists.

We also contributed to a paper on world model learning and inference, where Professor Carl Frisson served as the first author.

These are the kind of background of my research before getting into the CPC-like topic.

Let me briefly introduce the simple emergent system, another important term in this talk.

This figure provides an overview of the concept.

let me briefly introduce the main idea here the system does not mean artificial system or a robot or something like that the system here refers a complex system that enables symbol systems including language to emerge

The core idea is that symbol emergence involves two key dynamics happening simultaneously.

So you have individual agents learning internal representations, forming concepts through physical interaction with the world.

It's kind of embodied, the interaction, the active interaction with the world.

The traditional studies in developmental and cognitive robotics and also our simple emergency robotics focus on this individual representation learning process.

And still, many AI studies are looking at the individual capability

of language learning and other skills.

But at the same time, these engines interact socially through semiotic communication, communication using symbols or signs, including language.

And it's through this social communication that agents share the symbol systems

And through that process, symbol systems emerges across that group.

This emergent system then influence, the emergent symbol system then influence individual perception, communication, and behaviors in a top-down manner.

It's a kind of top-down constraint.

So the SES, symbol emergent system framework, emphasizes the interplay between individual learning

and collective behavior and kind of bottom-up formation and top-down constraints this kind of micro macro loop is the heart of the symbol emergent systems and i believe this is the crucial part of our language

After proposing this descriptive model from the viewpoint of emergent systems, I have been studying this system to uncover the fundamental mechanism of the human cognitive and collective intelligence in terms of symbols and language.

Of course, there are so many related prior studies.

The researchers have been studying constructive approaches

to language evolution and emergent communication for a long time.

So, for example, in 1990s, Luke Stiles in Europe and other researchers pioneered a language game experiment, where agents form a sign through interactions.

So many algorithms.

are proposed and most of them are based on language game so the two agents or similar robots apply some sort of communication including the naming game or differential games and through that process they become sharing the signs and symbols or form language or something and so

More recently, after deep learning became popular, deep reinforcement learning-based approaches have gained popularity, and that changed the landscape of emergent research on emergent communication.

But basically,

the central idea is similar.

I can talk about the pros and cons of this approach, but because of time constraints, let me simply introduce another line of research, our approach on emergent communication.

based on genitive model, probabilistic genitive model, which has been employed in a series of study on simple emergency in robotics, our research and the answer by the learning based approach.

So I mentioned the naming game.

So I didn't talk about the example.

But we extended, or we changed, the theoretical basis of naming game and proposed Metropolis Hastings naming game.

A bit complicated name, but that has reason why we call this Metropolis Hastings naming game.

This is a kind of game, language game, built on a solid mathematical foundation.

Okay, let me introduce how does it work.

First, there are two agents.

Consider agent A as a speaker and agent B as the listener.

Both agents observe the same object here.

So this means we assume joint attention.

Joint attention is the fundamental basis of the

our language acquisition.

Basically, when infants around nine months old become able to perform the joint attention.

And the joint attention is believed to be the basis of language learning.

So I think this is a very, very natural assumption.

OK, so next, the speaker names the object probabilistically.

And that naming is performed by sampling from the posterior distribution of a name based on this image and the latent variable, internal representation.

The listener then decides whether to accept the naming based on a certain probability

which depends on the belief state.

Specifically, this probability reflects how well the listener's belief matches the name proposed by the speaker.

The listener agent does not find the incoming sign believable, so it can reject the proposed naming.

This ability to reject is crucial.

After communication, the listener updates the internal parameters related to representation learning and naming.

Then they switch roles, and Agent B becomes the speaker and the game continues.

Overall, this is a very, very simple game, but it captures essential dynamics of how shared symbols can emerge through communication.

Let me continue the explanation from the mathematical viewpoint.

If the acceptance decision is made according to this probability, we can mathematically prove that the name game is equivalent to the metal processing algorithm for Bayesian inference.

of the latent variable shown in the figure on the right, this W. Let's begin with this graphical model.

Here, OA and OB represents a visual observation of each event.

Of course, they can be multimodal information that can involve other sensor input.

But anyway, they have observations.

That is the internal representations.

latent variable.

We also assume the existence of a variable w here, which serves as a prior of these two observations, two agents.

If we treat w as a latent variable, this graphical model becomes very similar to multimodal or multivariational autoencoder.

or some multi-modal representation learning system.

So, if the two agents' brain were directly connected and could use both observations simultaneously, they could jointly update the shared variable W. And this variable would encode the combined information from both agents' input.

However, in reality, they are independent cognitive systems.

We humans cannot directly connect our brain.

So, let's introduce a trick.

Algorithmically, we can decompose the model into two parts.

The right-hand side represents agent A. The right-hand side represents agent B. Then we consider an alternative inference method for W.

So now, think about the sampling process.

Agent A proposed a candidate latent variable, WD, by sampling from its posterior distribution.

And this value is passed to Agent B.

and who then decides whether to accept the sample based on a certain probability.

If the acceptance probability is designed appropriately, this process becomes equivalent to Metropolis-Haysing's algorithm, which some of you may recognize as a Markovitch and Monte Carlo MCMC method, under certain conditions.

And this guarantees that the shared signs are forked and that the categories emerge among agents to approximate the posterior distribution based on agents' observations, because this is completely the same as a multi-modal variational autoencoder, multi-viewport variational autoencoder like SAC, like SYNC.

Okay, this is the kind of the theory, the basic theory.

And of course, we can test this kind of system on some dataset.

Okay, the simplest one is always MNIST.

Now let's move on to the result of the experiment.

We conducted a small experiment.

Agent A observes the MNIST dataset, and Agent B observes rotated MNIST data.

You know, for example, in learning process, even though we have joint attention, when mother is pointing, oh, this is 0, or this is 1, or this is 2, the infant or child look at the object from a different direction.

This is imitating such kind of situation.

And we let the agent play the Metaboys Hastings naming game.

Okay, this is a simple result.

So, simply, this result, if the agent do not perform communication, the performance is like this, no communication setting.

And if the agent performs the Metropolis Async Slamming game, they can share sign, they can share sign, and actually the performance of clustering is improved.

So this is because in the learning process, the categorization process involving representation learning process depends not only on single agents observation, but also on both agents observation.

The information is rich, and the result becomes better.

So this is wrong.

Don't forget that this Metropolis-Hassings naming game is an inference process of this multi-view representation learning system, of course, with a single view.

So without communication, the agent can only use

the image data the agent observes.

But through the emergent communication, they mathematically, they theoretically make use of the information as another agent of them.

This implies that the simple emergence in society is not only for emergent communication, but also for better representation learning, in other words, better understanding of the world.

That's a very important implication, I think.

So in that paper I published two years ago, we conducted the experiment on a slightly different death set.

And a similar result was obtained, observed.

Actually, if we are based on this kind of modeling, this gave us a different view of semiotic communication, communication using symbol signs.

Here, the communication is interpreted as an interpersonal kind of cross-modal inference.

First, agent A observes the object and infer the latent variable, the sampling.

And the sampling, the what?

And after that, based on a given sample, the receiver can reconstruct the observation.

If the agent performs the MH Metropolis Hastings naming game, they shall sign, and also Agent B, after receiving that message, successfully reconstructs the image of the fruit.

Okay, and interestingly, please see the all-acceptance condition.

All-acceptance condition means the listener agent never rejects the incoming sign.

Always, they listen to the speakers and learn something from their level, their word.

Interestingly, if we make the agent accept all samples, and the agent do not use their belief state for rejection, they could not successfully form nice representation internally.

As a result, they cannot reconstruct the images from the signs.

It's very interesting, I think.

This is because the communication without rejected decision diverts from the theoretical posterior sampling, the Bayesian inference, approximate Bayesian inference.

And this is the kind of first step of our journey.

And following this study, we have continued to explore the Metropolis-Hassings naming game and emergent communication based on this kind of probabilistic generative modeling.

The recent research has introduced several key extensions to improve scalability and structure

adaptability, including expansion to multiple agents, not only two, but three or four or more.

Expansion to compositional language.

In the previous scenario, the agents just used the A or B or C categorical signs.

But now, the agents can use the sequence of tokens, like sentence.

Not only such a discrete sequence, actually the agents can use continuous signals as well.

Also, in a budget communication, not only this kind of naming or captioning, something like that, so cooperation is important.

So one of the main fields of emergent communication is multi-agent reinforcement learning with emergent communication.

So actually, thanks to the probabilistic modeling and controller-as-influence framework, now model-based reinforcement learning can be interpreted as probabilistic inference.

So we successfully integrate our idea to merge agent reinforcement learning as well.

And we can use this kind of emergent communication for making agent run cooperative behavior as well.

And also, our original idea was based on Metropolis Hastings.

that is a kind of approximate Bayesian inference.

And of course, Metropolis-Hastings algorithm is not only a solution for such kind of implementation.

So we are now developing a variational inference-based and contrastive learning-based approach, and we can get a similar result with.

Let me introduce one very recent work.

Actually, we uploaded the display print one or two weeks ago.

And we want to connect our emergent communication like research to the current recent trend of AI.

And we replace the variational autoencoder and Gaussian mixture model

the previous model by the vision language model.

So naming can be replaced by captioning.

The agent can observe the natural image and play captioning.

Matsui et al.

extended the Metaboys Hastings naming game to image captioning using vision language model.

We introduced a language model, GPT-2, into the caption generation process.

In this scenario, agents were pre-trained on different datasets and attempted to adjust their expression through communication.

More specifically, one agent was trained with Cocoa dataset, and another agent learned

are trained with conceptual captions, so very different way of captioning type of expression they have.

But through the process, their way of captioning is become similar.

And I will not get into the detail,

today but if you are interested please take a look okay and also as we found that

This kind of process even prevents the catastrophic forgetting that was a problematic problem in the fine-tuning in VLM as well.

This may be an engineering aspect, but I think it has much implication.

These studies, along with the idea of viewing symbol or language emergence as decentralized Bayesian inference, have led us to a new concept, that is, a collective predictive coding of CPC.

The idea is something like that.

Through the creation and negotiation of symbols or symbol systems, societies may optimize

collective world models.

This perspective suggests a social extension of the free energy principle.

Later, I will talk about this a bit more in detail.

And I think this is very much inspiring idea.

And I think so human society,

is operating the same mechanism, same things.

And I'm wondering, actually, when people play the similar naming game, are they following the similar acceptance ratio or rejection ratio?

I mean, if they follow the similar rejection ratio, acceptance ratio, that shows we humans can perform the Bayesian inference, posterior inference together as a group.

So we conducted this kind of experiment with participants.

So sometimes, in the study of semiotics, they use this kind of experimental setting.

This is called experimental semiotics.

And as a result, the result supports

the idea that human language, you know, that these humans follow the Metropolis, follow the similar pattern to the Metropolis-Hasings naming game.

This is a very, very weak evidence, I think, but this relatively supports the idea that human language evolution, language follows a CPC-like mechanism to a certain extent, I think.

OK, so we have been building many algorithms with the idea of that emergent communication or our simple emergence can be viewed as a kind of generative model, generative process.

And reviewing these works and considering future perspective, we recently wrote a preprint titled, Generative Amazon Communication, Large Language Model as a Collective Word Model.

In this paper, we expressly argue that a large language model can function as a collective word model in a sense.

So let me go back to the starting point of today's talk.

I talked about the language-language model, the learning process.

We have a simple emergent system on the right-hand side.

And we now have the idea of collective predict coding.

This is a kind of representation learning performed by everybody.

And usually, when we talk about the representation learning, it's often, it also always consider it is a kind of internal representation learning.

But the process of symbol emergence, in the process of symbol emergence, we actually create, form the external symbol systems, like language and knowledge and so on.

And that process can be regarded as a kind of social representation learning or external representation learning performed by multi-agent systems.

So, in some sense, language embodying collective intelligence.

That is the whole thing.

And then, with this perspective,

We can now say that our language system is structurally the modeling, the representing, the structure of the world.

But this is not the kind of objective world.

It's a kind of the sum of our subjective and perceptual embodied experience.

But as a result, and by internalizing this structure through predictive modeling, the LLM reconstruct such kind of world model internally.

That is a scenario.

And many, I think that this, I think that this is one idea that can, that

as a kind of the why LLM is so working well.

Okay, so let me move on to the, let me re-illustrate the CPC hypothesis again, CPC again, with the idea of emergent communication and pre-predictive coding or free energy principle.

First, let's start from predictive coding or world model learning.

There is a person, the he,

internally has internal representation or world models.

We are living with action-perception loop.

This is very, how to say, this is very popular way of describing the agent interacting with the world.

And this process is regarded as a kind of inference process.

Okay, but with humans,

We are so many.

And also, this interaction, this process is not informationally probabilistically independent.

So yes, we have communication, and we are influencing each other.

Following the predictive-predictive coding hypothesis through language game, we can form language-like social representation through the learning process.

I think through this process, we model and adapt to the world collectively.

So this is the CPC process explaining the simple language.

But with this view, we can change our view.

Until now, we believe that each person is an independent and autonomous agent.

But let's change the viewpoint.

First, language is here.

Language is a kind of autonomous system here.

And language has sensors.

That's us.

Language uses us to model the world through interaction.

From this point, we are supporting it through the language system.

Actually, this kind of view is often mentioned in sociology and some other academic fields.

and we are kind of the people working for language.

Okay, I think this kind of dual viewpoint is very interesting to extend our view of cognition to the social view.

Then, let's move on to the free energy principle.

Recently, we formalized the CPC from the viewpoint of free energy principle.

Based on a generative model representing CPC, we can naturally derive variational free energy.

Interestingly, final form can be divided into two parts.

The first one is an ordinary free energy term.

This is the sum of the variation-free energy of a number of agents.

In addition to that, an interesting term pops up.

We call this the collective regularization term.

This term tries to align

agents external representation w with each other's internal representation with this term the problem of minimizing individual free energy becomes the minimization of free energy of a single of a single total system and actually this is this free energy is not for a single agent

But actually, this language, this total system, free energy, is defined like this, and they are divided into the individual free energy and some mysterious collective regularization term.

I think this view can extend the free energy principle from the individual to the collective level with a clear relationship to simple emergence.

And actually, this is related to the classical semiotics, I think.

The corrective visualization term is particularly interesting from the perspective of semiotics.

In semiotics, here is Charles Sanders' path, the father of semiotics.

And arbitrariness is a crucial concept.

Science can represent orbital objects.

depending on language, culture, society, and context.

So while this is widely recognized, to my knowledge, most studies in semiotics have explicitly addressed the functional role of this arbitrariness.

From the perspective of simple emergence, we can argue that this arbitrariness provides a degree of freedom that allow us to optimize collective variation of free energy

This shifts individual's free energy minimization problem into a collective free energy minimization framework.

In other words, while we rely on the plasticity of individual brain, we become a collective social intelligence by exploiting the plasticity of similar systems.

These are my current speculative thoughts.

on human intelligence okay so before wrapping up my talk so i want to talk about a bit more a bit several uh one or two topics so and this you gave us further inspiration about the temporal aspect of human mind

So many of you may know the dual process theory.

Sometimes it is called System 1 and 2.

So that tells us we humans have fast thinking and slow thinking.

CPC gives us an opportunity to extend this idea to group level.

So in our paper, we discussed the system 0123, acquired process theory for multi-time-scale embodied collective cognitive systems.

The CPC hypothesis allows us the emergent symbol system as a type of supercognitive system.

that adapts to its environment with extremities through temporal dynamics.

First, we have a kind of physical dynamics.

Even some of you may know, like passive dynamic walker and soft robotics.

They showed that even without brain, our body itself interacts with the world so flexibly if your body has sufficient mechanical properties.

So this is a kind of superfast dynamics, and this is important in robotics and often missed in AI studies.

and yes after that in our cognitive process first dynamic system one and system two and actually the recently the system one is modeled by world model and the political learning and imitation learning and so on and the system two the language level the kind of derivative thought that is modeled by language model

But these are happening internally, inside of our mind.

But as a group, we have more slow, super slow dynamics.

It is the learning process is corresponding to the kind of negotiating our simple systems, changing our custom habit in our society.

That is a more slower dynamics.

So, and importantly, System 3 and System 2 are happening outside of our mind.

But it is super important for our society and ourselves.

We really have to do this as System 3.

Dr. Hirai, who is a philosopher,

Okay, and following this work, we wrote another perspective paper exploring the formalization of scientific activity using the CPC.

The core idea of CPC is the symbolic system emerged through the organization of external representations as a human attempt to model the world.

This concept closely aligns with scientific activity, even rather than language learning.

As one of the fundamental goals of science is to construct external representation systems that describe and enhance our understanding of the world, ultimately improving our predictive capabilities.

So, and one of the key insights is that image name gate in symbol emergence has

an interesting similarity to scientific discussion.

A representative form of scientific discussion is a kind of peer review process.

An agent performs an experiment, writes a paper based on their observations, submits the paper, and the reviewers decide whether to accept or reject it, considering their beliefs, evidence, and the style of paper.

And this structural similarity between the image naming game and scientific discussion suggests that this type of overall scientific activity can be regarded as a presentation learning process.

This analogy leads us to the idea of CPC as the model of science.

I believe this work will bring meaningful implications to the philosophy of science and other fields as well.

In this paper, we derived the free energy of CPC I explained before in the previous slide.

On the basis of free energy for CPC framework,

we can naturally connect it to active inference, finally.

It extends the individual expected free energy by incorporating a collective term.

This now can be called collective epistemic value.

So this formulation allows us to consider active inference motivated or guided by emergent language.

The future work includes exploring this concept as active influence at a group level, potentially framing it as collective curiosity or something.

In scientific community, we are conducting research not only on collective curiosity driving scientific exploration or societal adaptation.

This is just the beginning, I think.

But I think this kind of approach, actually connecting, this has been a kind of long journey, but connecting the simple emergence or I would say the collective learning process for me.

some external representation, not only just a sequence of token or category of something like that, but the scientific knowledge, our laws, and so on.

So modeling such kind of social phenomena is a huge challenge for us and future AI, I think.

Future AI.

Okay.

So that's almost all of my talk.

The summary is something like this.

We propose a corrective-predictive coding.

And that was from the simple emergence as decentralized Bayesian inference.

And that was extended to the free energy principle and the active inference to the societal end.

And I mentioned the relationship between LLM and kind of the world modeling.

And we introduced generative emergent communication and related works.

And lastly, I mentioned several implications, including CPC as a model of science and system 0123 quadratic process model.

Okay, that's all for today.

Thank you very much.

So these are the main differences.

I added these to today's abstract.

So if you are interested in, please take a look.

Okay, thank you very much.


SPEAKER_01:
Awesome.

Thank you for that.

Lots in the presentation.

Okay, I'll ask a few questions I wrote down and read some questions from the live chat.

So what is the relationship between convergence amongst agents in their symbolic representation and, for example, cooperative social behavior or novel research?

Like what else needs to come into play beyond just symbolic representation conformity to get learning, novelty, pro-social activity?


SPEAKER_00:
or is this or is the symbol representation do that work by itself okay uh could you copy and paste copy it to the chat and uh yeah yeah so it does a bit difficult to remember okay could you

okay what is the relationship between convergence of symbolic representation and narrative coherent corporate social behavior novel research okay that's a very important point interesting and important point and actually the maybe the

So the answer is not a single one.

So for example, regarding the cooperative social behavior, let me take the cooperative social behavior first.

So in cooperative social behavior, we need, and if we play a cooperative social behavior using language,

So we need some sort of convergence to let the counterpart understand my intention.

So actually, such kind of, how to say, convergence of symbolic representation, symbolic external symbol system, allow us to collaborate.

But before that, we need to

construct, form the symbolic system, simple system, like language.

This itself is a kind of social cooperation, cooperative behavior.

So for example, Michael Tomasello, who is a very famous, I would say, linguist and developmental scientist, he put much emphasis on cooperative behavior.

as a kind of origin of language learning.

And interestingly, from the viewpoint of CPC, kind of the cooperative behavior is treated as a kind of very much innate stuff, innate like stuff.

So can I go back to the... Okay, let me go back to the Metropolis-Hessing name game.

So in our point of view, usually,

usually the, how to say, the previous approach, the conventional approach to emergent communication.

So forming the language is modeled as a part of kind of policy learning or something, the kind of explicit cooperation, have maximizing reward or something.

But here, we don't have any reward.

So basically, what we assured is the joint attention.

Joint attention.

And also, we assumed this rejection or acceptance that determine whether they accept or reject based on their own belief state.

So as a result, the kind of shared language cooperatively shared or formed.

So this is a kind of cooperative behavior.

But it's not about the policy learning, but the kind of process of cooperative world modeling.

So this is a very much different viewpoint.

And this is the first point.

So in other words, convergence of symbolic representation itself is a cooperative social behavior.

So this is the first answer.

And the second answer is novel research.

Maybe this is about the CPC-MS, collective predictive coding as the model of science.

And yes, so as you know, or as we understand, this is a kind of optimization process, but we, how to say, I think that most of us don't believe this process reaches actually the kind of convergence.

We are always struggling to minimize the free energy

and take actions you know so and the other kind of the you know the collective behavior cpc try to minimize the i said try to minimize the social level collectively energy and to that end the each researcher each research is like this

This looks like controlled by the scientific society, but they need to explore to obtain new data to update the simple system with new information.

So this is corresponding to the novel research in our society, I think.

OK.

That's the answer from my side.


SPEAKER_01:
thank you yeah i really like this image it's like humans as the blanket or the interface between a disembodied symbolic system and a world that evades symbolic description alone um okay i'll ask another question and then i'll look for questions in the live chat and i put it in the chat so

You brought up this dual view of neural plasticity in terms of our learning and symbolic plasticity in terms of the development of symbol systems.

So with access to LLMs and computational semantics, how is human language changing or how might it change?


SPEAKER_00:
That's very much interesting topic.

And actually, that is a project I want to start, I want to launch.

Actually, this is kind of a very new and interesting topic.

And I think, yeah, to be honest, I don't have answer now.

But with the lens of CPC, I can say

I can have a discussion about this topic.

Sorry, let me move to the generative.

Okay, this one, this one, this one.

So, of course, the LLM will give us a huge impact.

But that also depends on in what way in the future we learn

the LLM system.

Actually, two years ago, the GPT-4 was launched.

And at that time, everybody started using the GPT.

And everything, every linguistic phenomena on the internet is affected by a single LLM.

But recently, the situation is changing, and the LLM becomes diversified.

So the situation has changed.

But anyway, our communication, our language has been affected by the LLM.

And the empirical evidence shows some effect has already shown some effects.

So the word choice has already been changed to some extent.

But, you know, the people tend to think LLM is something like an alien or something very much different from human, I'll say, human cognitive system, you know, humans.

But, you know, if the LLM

based on the language generated by CPC.

So basically, the distribution is very much a naturalistic one.

We, humans, created the language.

And maybe we can use this figure.

So behind the LLM, there are so many humans, people.

So just let the LLM summarize the information and give you an influence.

So actually, there is similar to what was happening in the past like this.

So language system was, we shared, our way of speaking, way of writing is affected by some, you know, some, how to say,

the shared symbol system.

Of course, the distribution of nature has become very much different.

So maybe that leads us to a different way of language use.

But yes, and I think that I would say that we cannot

uh we i think we cannot determine or this is the future or something but uh we can discuss the i'll say uh the future you know uh the future changes based on using the model of uh cpc you know model of symbol emergence you know the the if we can model the

how to say background of llm so that may enrich our discussion so maybe next year or next next year i will have a three other answer i don't know thank you very much all right all


SPEAKER_01:
Read the next question.

ML Don wrote, In the collective predictive coding hypothesis paper, the generative model does not seem to have any mechanism to capture the dynamics of the world.

This is surprising since the paper keeps referring to the dynamic nature of the world repeatedly.

If my observation is correct, may I suggest to add, at the very least, x as well as dx dt to the generative model.

This can help immensely when the world is truly dynamic.

So yeah, how did any of these papers deal with potential change through time of the referent or of the semiosis?


SPEAKER_00:
Thank you very much.

and no objection no defense and i totally agree with you and uh actually so when i when i when i uh proposed the metropolis hashing's name game and uh actually uh uh cpc so we didn't yes the as a first as the first step we didn't involve the temporal nature and dynamic nature

And it was a kind of a strategic one.

And we wanted to focus on this part, connecting the collective learning or symbol emergence process at the group level and the representation learning in the individual level.

And also, we have been conducting research

on the world modeling and predictable learning like stuff and so the actually we thought that i thought that we can extend we can involve the dynamic nature after that and actually that we wrote uh and we wrote the some papers about the kind of the uh say the the merchant the

the collective predictive learning-based approach, coding-based approach, based the multi-agent reinforcement learning with imagined communication.

And also, we haven't published that, but our students are now working on the world model

based world, how to say, emergent communication based on world model, and so on.

And of course, how to say, we haven't invented that.

But yes, we want to, how to say, reformulate this kind of process based on dynamical systems-based approach.

And yeah, we had a discussion over that with our collaborator,

and friends that yes that i think so such if you if you are somebody some great researchers add such kind of aspect to this this kind of cpc like idea that was hugely that is very great and that's actually also why

uh we i i wanted to have said join this live stream you know so in the active influence community there are so many great researchers who uh i think who can add the another value uh or another well

nice idea, nice formulation to this kind of research preaching, collective behavior, and individual learning process.

So, yes.

Thank you very much for your comment.

Thank you.


SPEAKER_01:
Um, sort of in closing, what are you looking forward to this year or how might people get involved beyond like reading the papers?


SPEAKER_00:
Thank you very much.

So, uh, last year, so after I got the inspiration of CBC, I rushed, we rushed to write several papers.

And I wanted to share this idea.

And I think that this will be a kind of a stream of how the following streams of research is.

And in Japan, I have now

many collaborators.

But if you think this is an interesting aspect, but it's insufficient, please, please add another value or another contribution

to the research community and active influence community as well.

And actually the last year and the year before last, I really wanted to join the IW

AI in the workshop on active influence.

And yes, my friend Tetsuya Ogata was a keynote speaker last year.

Actually, because of some university's job or something, I couldn't attend the meeting.

This year, I really want to join.

But if I miss, my collaborator or my students will join.

I hope we can have a fruitful collaboration or discussion in the venue and not only in person but also online as well.

I welcome any messages or any tweets.


SPEAKER_01:
to and i'm the kind of the long-term twitter or x user and yes and please keep in touch thank you very much thank you it was an awesome presentation i hope people in the community read these papers and start to build and work with them so i hope to keep in touch and see you again thank you very much
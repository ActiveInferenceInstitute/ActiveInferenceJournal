start	end	speaker	sentiment	confidence	text
3540	4090	A	0.546256959438324	You.
6060	7256	B	0.872096598148346	Hello and welcome.
7358	13416	B	0.9447628855705261	This is act infg guest stream 47.1 on July 10, 2023.
13598	20644	B	0.8661268949508667	We're here with Nick Bird and Samuel Bellini Leitchi and we're going to have a presentation followed by a discussion.
20692	23076	B	0.9761011004447937	So thank you both for joining.
23188	26116	B	0.9521719217300415	And Sam, thanks again for this presentation.
26228	27210	A	0.6221094727516174	Off to you.
30460	32370	A	0.7438894510269165	You okay?
33380	35810	A	0.861903190612793	Was Nick going to say something before I started?
36660	37552	A	0.6805202960968018	I can go ahead.
37606	43264	A	0.7047561407089233	Okay, so some introduction first.
43382	47300	A	0.9235245585441589	I'm a professor at Minajara State University.
49560	55540	A	0.8117748498916626	I'm a psychologist philosopher, mainly working in cognitive science.
55980	81540	A	0.5850392580032349	I'm going to talk about what I did in my PhD thesis and also some renewed interest I had on this based on some developments in large language models which happened this year, most notably Chain of Thoughts and Tree of Thoughts.
81720	88450	A	0.8147339224815369	Those additions to large language model made me want to come back to this again and talk about this.
90500	98020	A	0.8680683374404907	So how do we know about human reasoning, which is the subject, the main subject underlying this topic?
99000	116920	A	0.7952768206596375	Well, we had empirical research from the 60s, starting with Peter Watson, who was challenging the notion that we have a logic module for reasoning or some general understanding of logic when we're adults.
117980	138384	A	0.5400193929672241	And he crafted tasks which were hard to solve, but they had actually simple fundamentals to it, which were taken to show people's irrationality, something like that.
138582	160310	A	0.6069755554199219	And also a huge tradition that's called Heuristics and Biases that also started in the mainly by Kanmen and Amosverskin, which had similar tasks and went beyond, and they even won the Nobel Prize for this work.
162120	172510	A	0.8738654255867004	I had here on the slides Evolutionary Psychology and Language and Pragmatics, which were also people in the 90s working with this sort of research.
172960	196260	A	0.8919036388397217	But I'm going to talk about dual process theory in the way they formulated by Evans and Stanovich, which mainly summarizes the results of Watson and common risk biases literature.
196840	205910	A	0.7560037970542908	So I'm going to talk about how dual process theory came to be and then some problems that it has that I tried to address.
208120	222696	A	0.8574805855751038	So one of the first reasoning tests in this trend literature was the Watson selection test, which goes like this please indicate which cards can determine if the following rule is false.
222808	227630	A	0.8831903338432312	If a card has a vowel on one side, then has an even number on the other side.
230240	233020	A	0.6206117868423462	This sounds like a simple exercise.
233180	239250	A	0.6773883104324341	So the card A clearly is relevant because it has a vowel on one side.
239620	245916	A	0.5643056035041809	If it doesn't have an even number on the other, then the statement is false.
245948	250310	A	0.6980230808258057	And everyone can find this neatly of most people.
251160	258688	A	0.678550660610199	But the problem comes with the card seven, which is not so intuitive.
258784	266984	A	0.8780558109283447	People actually choose the second card two there, which has an even number on one side.
267102	272920	A	0.7174385190010071	But having that even number doesn't help at all to determine if the statement is false.
273000	286370	A	0.7996633648872375	What does help is which is an odd number, if it has a file the other side, then the statement could be.
288020	322136	A	0.5818448066711426	So this was one of the tasks that famous for and conman and first key show them this one they're famous task as well solved the conjunction fallacy the issue that people arise when solving this task goes like this it is a 31 years old, single, outspoken and very bright.
322248	325292	A	0.908957302570343	She majored in philosophy as a student.
325426	333964	A	0.6818704009056091	She was deeply concerned with issues of discrimination and social justice and also participated in anti nuclear demonstrations.
334092	340160	A	0.8724493384361267	Please make the following statements by their probability using one for the most probable and eight for the least probable.
342820	348944	A	0.6510463953018188	What's interesting here is the statement, the last statement.
348992	356020	A	0.5821635127067566	Linda is a bank teller and isn't active active in the feminist movement.
356920	367560	A	0.7529253363609314	Because there's also these statements without one another, without the conjecture which is Linda is active in the feminist movement and Linda is a bank teller.
368640	380860	A	0.6164919137954712	Because of the description, people think that Linda is a feminist is very likely given the description, but they think being a bank teller is not likely.
381220	392240	A	0.5953924059867859	The issue is that people usually mark Linda is a bank teller and is a feminist as more likely than being a bank teller.
392740	398320	A	0.6337196826934814	The problem here is that the conjunction of the two necessarily is less likely.
398660	406420	A	0.5542848706245422	So being a feminist plus being something else or being a bank teller plus being something else is automatically less likely.
409580	418920	A	0.55884850025177	So those are the sort of tasks that the reasoning literature used to show some sort of human irrationality.
420480	433870	A	0.7422199249267578	One issue with this is that it's not very interesting to show that humans are irrational, it's more interesting to show how humans reason.
434560	446980	A	0.616936445236206	And also these sort of studies didn't show how people actually solved the problems.
447050	450500	A	0.5908305048942566	They were actually focused on how people made mistakes.
451480	459800	A	0.8905428051948547	One thing that's interesting about dual process theory is that it explains both the mistakes and the solutions to the answers.
460220	472072	A	0.6802833676338196	So in 2002, Conan and Frederick formulated this cognitive reflection test which is another one of these tasks but a lot much simpler and elegant.
472136	482510	A	0.6576182842254639	And it really shows indicates at least how dual process theory is a good solution to a good explanation of the evidence in general.
483040	494108	A	0.8011369705200195	So yeah, the western selection test used logic and the Linda task used probability.
494284	496924	A	0.606026828289032	And here we just have basic arithmetic.
496972	506388	A	0.6966390609741211	So as if math we don't have to worry about which answer is truly correct or which rational norms we need to follow.
506474	508100	A	0.7281862497329712	It's just basic math.
509400	511380	A	0.4991835057735443	There's nothing to discuss here.
511450	511780	A	0.7360090017318726	Right?
511850	517592	A	0.7626073956489563	So the task goes like this a bat and ball costs one in ten in total.
517726	524360	A	0.6562380790710449	You might know this test, it's a famous test even out of the reasoning literature, the bat costs one more than ball.
524440	526350	A	0.8338316082954407	How much does the ball cost?
526880	531820	A	0.7763864398002625	So people when when people read this, they usually say $0.10 intuitively.
532480	539740	A	0.7541927099227905	But if you take a little time to write write it down, you can see that it's actually $0.05.
539820	548272	A	0.8163725137710571	So the bat cost one and ball costs $0.05.
548336	550230	A	0.5746413469314575	Together it costs one and ten.
551640	554416	A	0.8530238270759583	The other two have a similar structure.
554528	563530	A	0.8092136979103088	The point here is that sometimes we look at these tests and we have an intuitive answer which is wrong.
564620	581230	A	0.8349360823631287	And the way we get out of these issues is that we stop and think a little bit and review our answers with some sort of step by step reasoning or writing it down, something like that.
582640	590770	A	0.8856251835823059	What's interesting here is if you look at two plus two, it's almost impossible not to think four.
591300	592924	A	0.7357710003852844	It's almost like you're perceiving.
592972	595700	A	0.7909343242645264	The answer is four just by looking at it.
595850	604580	A	0.506781280040741	But if you have some sort of more complex calculation to do, obviously just by looking at it, you won't have a solution.
605320	612410	A	0.8691971898078918	So this basic intuition is clearly shown in the cognitive reflection test.
613580	625336	A	0.7204946279525757	And what's more interesting about it is that the cognitive reflection test actually predicts the performance of various Aristocrats and Biases tests.
625448	634720	A	0.7244227528572083	So if someone does well on cognitive reflection test, they're more likely to go well on any other reasoning task.
636420	643650	A	0.8697640299797058	And if they go bad on the cognitive reflection test, they likely will go bad on other reasoning tests as well.
644440	660872	A	0.6223396062850952	To me, this is also an indication that this dual process structure of intuitive reasoning versus slow reasoning, reflective reasoning, likely explains the rest of the text as well.
660926	664328	A	0.7697445154190063	Not only these two, but most of the others.
664494	667240	A	0.7463845610618591	So it's a good explanation of the literature.
668480	680060	A	0.8533298373222351	Okay, so what does the theory say besides besides explaining the reasoning literature?
680820	703120	A	0.8334944844245911	So, in 2008, Jonathan Evans tried to make a formulation of it that would include and unify views from different domains such as social cognition, neurosciences, other people in psychologists which had similar theories.
703200	713368	A	0.5877851843833923	And it seems like there's something in common here to unify, but Evans wasn't able to.
713534	725180	A	0.59547358751297	And so there's a formulation specific for reasoning and we don't know if it applies so much well to other areas of cognition.
727680	737612	A	0.8400575518608093	One common way to describe dual process theory is by making these tables of features.
737756	750150	A	0.8529466390609741	So we say, oh, type one processes have these sorts of features on the left, and type two processes have this other type of features on the right.
752600	774780	A	0.847194254398346	This table here was one I made by joining some of the features of Evans insteadovich, and also of Kunman, and then with some questions I raised in the thesis, I elaborated it this way, but it's very similar to the Yavin Instant formulation.
776080	791890	A	0.5237467288970947	One problem we have with this tables formulation of the Duo Process Theory is that usually people agree that in one thinking instance, when you're solving a problem, you're not going to have all of those features together.
793400	799732	A	0.7196037769317627	So we don't know for sure when some feature will be present or not, for instance.
799866	808840	A	0.7065600752830505	So clear that type one processes necessarily have to be unconscious and type two processes necessarily have to be conscious.
810060	816120	A	0.5713464617729187	I think that comes a lot because of some conceptual issues in the theory.
817020	826536	A	0.8176735043525696	But one solution they have found was to have at least some defining features but different authors have different defining features.
826648	842880	A	0.7394484877586365	So here I have some of the defining features of the main authors, which is type one processes use less working memory while type two processes most strongly on working memory.
843860	853008	A	0.5684094429016113	Type one processes aren't autonomous in the sense that they don't need to wait for higher processing to respond.
853104	855910	A	0.8070873022079468	If they have an answer, they go for it.
856280	874110	A	0.788743257522583	And using the couple's representations mean that means that type two systems can reason beyond what's in the present environment can think about things that are not there.
875600	881500	A	0.5710412263870239	Type one processing is faster in comparison while two is slower in comparison.
883060	893164	A	0.6228638291358948	Type one processing is less effortful while two is more effortful in the sense of psychological effort.
893212	897940	A	0.7014707326889038	Like people report having a hard time doing things.
898010	912330	A	0.4692128598690033	So for instance in the cognitive reflection test, people that solve the test correctly usually rate it and it's harder because they had to use tattoo processing to solve it.
917160	938508	A	0.7406821846961975	Okay, but Samuels, which is a philosopher, dual process theory and other subjects, he says okay, even if these features proposal is good, we still need a mechanism to explain why these features are linked together.
938594	952512	A	0.902677595615387	So what is the mechanism that explains why type one processing features go together and what is the mechanism that explains why type two processing goes together?
952646	962390	A	0.825808584690094	So we need some beyond saying that they're different, we need to explain what are these mechanisms that make them different?
963560	974032	A	0.6495437622070312	And I actually had Samuels on my PhD committee and I'm not sure if he was convinced with my solution.
974096	976952	A	0.6318807005882263	So my PhD was a solution to this problem.
977006	985470	A	0.8242329955101013	I'm not sure how he was convinced of it, but he approved the PhD, so that's good enough, I guess.
989040	989790	A	0.5491447448730469	Yeah.
991520	1004000	A	0.8062470555305481	So the goal was to solve the unity problem, was to explain the mechanisms that explain type one processing and explain the mechanisms that explain type two processing.
1004420	1030760	A	0.7806820869445801	And I started to see, to search in the literature first, how people were conceiving of the general frameworks that were behind these two types of reasoning and one that was not actually related to this literature of dual process theory but was actually famous in cognitive science, was the modularity of mind in the 80s proposed by philosopher Jared Schroder.
1031260	1050156	A	0.8909940123558044	And he said that we had two types of processes, a modular process mostly related to input and perception and we had central processes that was mostly related to higher reasoning.
1050188	1054960	A	0.8894715309143066	So you can trace some sort of resemblance there to do process theory.
1055940	1065776	A	0.841376006603241	And the way he went about this is that he said these perceptual models, modular processes are information encapsulated.
1065888	1072650	A	0.7765172719955444	So vision only works with vision and language only works with language, space only works with space.
1073180	1086808	A	0.5929670929908752	And so these were domain specific knowledge systems that did not communicate with each other and when they needed contact sensitivity, they needed central processes.
1086904	1109408	A	0.8310160040855408	So these central processes he would argue, were Isotropic and Quinine, which means that any type of information could be used in a solution so we can relate, I don't know, physics with biology and form a new solution.
1109504	1121540	A	0.5406442284584045	So there's knowledge from multiple domains working here that would also need some sort of context sensitivity, which was a problem for photo.
1121620	1129992	A	0.5059935450553894	He didn't think we would solve this context sensitivity problem at all and he had good reasons for that.
1130126	1131812	A	0.7655699849128723	In classic AI.
1131956	1135772	A	0.7353748083114624	We had a lot of issues with that.
1135826	1149532	A	0.5231555104255676	I mean, we still do have issues with the context sensitivity problem, but not the sort of issues we had in the 80s when they were debating this same problem of classical AI.
1149596	1155920	A	0.5444793105125427	So this problem was a problem of how to represent a changing environment in symbolic representation.
1158040	1189580	A	0.7229998111724854	So yeah, it was a problem because classical AI attempted to give knowledge to robots or artificial intelligence and statements about the world and they were intractable, hard to compute, there are so many knowledge banks to search and so many knowledge to try to use that the robots couldn't do anything relevant.
1191760	1195552	A	0.7672932744026184	They also couldn't represent change.
1195686	1208420	A	0.9431018233299255	So the changing environment was terrible for them, didn't work with this symbolic representation, it didn't work with serial reasoning or anything like of the sort.
1208570	1213700	A	0.7251819372177124	This was a huge problem for classic cognitive scientists.
1215320	1237848	A	0.8145827054977417	And since photo was a believer in cognitive science in the sense of a classical computational theory of mind or language of thought theory, he said that he had found Photo's first law of the nonexistence of cognitive science.
1237944	1245730	A	0.5308785438537598	This law states that the more processes are global, the less cognitive science will be able to understand them.
1247380	1252364	A	0.7146949768066406	So he thought only these encapsulated processes could be studied.
1252492	1269380	A	0.4935528039932251	And if we needed this sort of global and context sensitive type of processing, then contest science just couldn't do it because he had in mind contra science in the sense of classical AI.
1273820	1292812	A	0.7667838335037231	But it also seems not only folder, it seems that the literature abandoned classical cognitive science in various ways, which were strange because if you take the work of Alan Newell and Herbert Simon, it's strange to think that oh, they were completely wrong.
1292866	1299010	A	0.8149766325950623	There's nothing, nothing useful that they say that could be applied today.
1299380	1304610	A	0.926964282989502	All the evidence, all the theories they made are just completely wrong.
1305620	1308290	A	0.6443331241607666	That's a weird way of thinking.
1308820	1319110	A	0.5196734070777893	And well, if you take for instance, our theories today of the human Bayesian brain, they don't consider this at all.
1321480	1323304	A	0.7804862856864929	It's just like they don't care.
1323342	1327560	A	0.8155030608177185	It's a different strand of theories.
1327980	1339400	A	0.5269904136657715	But it also seemed to me that there must be at least something wrong about, something right about this shouldn't be totally abandoned.
1339560	1343820	A	0.7702981233596802	There must be a place for these results.
1346900	1369260	A	0.5343170166015625	Explanation of the Mind one thing that was in odds with the way Photer described his dual process theory, and also sometimes Stan of a trend talks about dual process theory.
1370640	1386500	A	0.5555809140205383	It seems like forgetting this fact here is the fact that we have a limited capacity when we use conscious processing or working memory.
1387400	1391510	A	0.9773484468460083	And this is one of the best evidence in psychology we have so far.
1392280	1408648	A	0.854260265827179	We had behaviorist evidence for reinforcement, enforcement, learning and that stuff and then we had cognitive psychology in the this is the best evidence that cognitive psychology has had for a long time.
1408734	1413500	A	0.5067344903945923	So it's also something we should not simply ignore.
1414240	1435520	A	0.6262156963348389	So we have in short term memory the knowledge that when we store some information in short term memory, we usually can only keep track of about seven items, sometimes even less in selective attention.
1436200	1467464	A	0.525386393070221	There was this task where there was a video of people playing basketball and the subjects had to count how many passes they were making and suddenly a gorilla passed by on the scene and people didn't notice the gorilla because they were so concentrated on the task, which is some evidence of selective attention.
1467512	1475660	A	0.5441632270812988	That is, when you're concentrating on tasks, you actually don't even see other stimuli.
1479860	1494624	A	0.7333011627197266	We have also a competition for limited resources when doing dual tasks, or if I'm talking on the phone and writing at the same time, I'm probably not going to do a very good writing because we're competing for limited resources.
1494752	1503428	A	0.6795554161071777	And it's important to note that if you're walking and writing or walking and talking, since those are different tasks, they won't compete.
1503524	1505880	A	0.5751827955245972	So we have some competition.
1506460	1515580	A	0.5413796305656433	We have more competition when these limited resources are disputed also in working memory.
1515920	1526464	A	0.7186824083328247	So if I say the number 793241 and I ask you to repeat them backwards and you're not looking at the slides, you're going to have a hard time with that.
1526662	1548150	A	0.6174395680427551	Because of the limited capacity of working memory, when it's clear that in other areas of our thinking of our brain is using a lot more information than seven items to make the scenes happen, perception and to make us walk and so on.
1552460	1563000	A	0.8585786819458008	So that's on the side of the type two constraints, on the side of type one reasoning.
1563820	1567576	A	0.8494573831558228	Our Audi has some intuitions similar to the ones I described.
1567608	1570620	A	0.8509143590927124	So he said from its earlier days.
1570690	1583116	A	0.8799726963043213	The research that RSD and I conducted was guided by the idea that intuitive judgments acquired position between the automatic operations of perception and the deliberate operations of reasoning.
1583308	1596464	A	0.8293690085411072	Communric claimed that intuitive thinking is perception like and that intuitive predictions is an operation of system one further that the boundary between perception and judgment is fuzzy and permeable.
1596592	1603080	A	0.7257794737815857	The perception of a stranger as menacing is inseparable from a prediction of future harm.
1603500	1613444	A	0.6829849481582642	So you can see that this is very likely, very similar, sorry, to the intuitions that we were speaking on when we're talking about predictive processing.
1613492	1624130	A	0.5579196810722351	So feminine saying when you look at a face of someone and he's mad, you have a prediction that you're in trouble, right?
1624660	1640300	A	0.7760829329490662	So this is a sort of judgment, that same sort of perception, and also the context sensitivity is pretty much solved in predictive processing.
1640460	1645620	A	0.8534368276596069	And if you take on the AI literature that also happens.
1645770	1649128	A	0.5859599709510803	For instance, paper attention is all you need.
1649214	1657368	A	0.6135454177856445	They have some tips for how to deal with context sensitivity, which photo didn't dream of.
1657534	1660860	A	0.8044482469558716	So this is something that Clark said in 2013.
1661280	1676192	A	0.6697374582290649	The best overall fit between driving signal and expectations will often be found by, in effect, inferring noise, in the driving signal and thus recognizing a stimulus as, for example, the letter M say in the context of the word.
1676246	1685570	A	0.7436882853507996	Mother, even though the same bare stimulus presented out of context or in most other contexts, would have been a better fit with the letter N.
1686500	1695664	A	0.6343718767166138	A unit normally responsive to the letter M might, under such circumstances, be successfully driven by an N like stimulus.
1695792	1709288	A	0.834526777267456	So what is Clark is trying to say here that since we represent using probability, then if the context says we should go left, we go left.
1709374	1722110	A	0.5611446499824524	If the context says we should go right, we go right, because the representation is fluid and it accepts changes really quickly.
1723060	1728400	A	0.8835544586181641	So that's something that stems from the predictive processing architecture.
1733220	1741424	A	0.5287747383117676	But one issue I think that we might see in the predictive processing architecture is the lack of compositionality.
1741552	1752680	A	0.8723642230033875	So symbolic representations exhibit compositionalities meaning that complex representations are built by combining simpler elements according to rules or syntax.
1753020	1760696	A	0.8354502320289612	The meaning of a complex representation is derived from the meanings of its constituent parts in the way they are combined.
1760808	1767320	A	0.5689984560012817	This property allows for the generation of new and meaningful expressions by manipulating symbolic structures.
1767480	1786610	A	0.8363016247749329	So Folder in 88 argues that even if connectionist models succeed, they would likely have some sort of simulation of a language of thoughts that exhibits conventionality in order to work like we do.
1787080	1796390	A	0.7919557690620422	And we'll see that it seemed like it's the case with the results we have this year.
1798840	1819256	A	0.7845214009284973	So considering those constraints and the initial field footer and the problem I was trying to solve, which is the unity problem for blue process theory, a major shift I had on the PhD thesis.
1819288	1823630	A	0.8555459976196289	And then I wrote a chapter on this book.
1824320	1827520	A	0.9165348410606384	The Chan Book of Founded Rationality with Keith.
1828420	1848410	A	0.7572334408760071	So we explain here in this book the chapter The Shift Power Structure, which is, I mean, it's not clear that a dual process theory theorists would adopt Folder, but it's still very similar.
1850220	1860600	A	0.633266270160675	The way the received view of dual process theory works, which is type one, is a number of dumb heuristics blind modules and animal intelligence.
1861740	1867608	A	0.7915006279945374	And then type two is the real contextual and complex reasoning, the real human intelligence.
1867784	1884956	A	0.8201143145561218	So I tried to invert that power structure by saying that the contextual, predictive processing is a type one reasoning, which is giving us the basics to reason on.
1885078	1888160	A	0.8614178895950317	So our reasoning already comes with context.
1888320	1892180	A	0.7303891777992249	We don't need to reason the context with type two reasoning.
1893240	1899748	A	0.6963093876838684	We only use type two reasoning to fix some minor issues in predictive processing.
1899844	1903800	A	0.6444751024246216	And we use heuristic search because it's limited.
1904460	1910910	A	0.7845824360847473	We only use it when there's a lot of prediction error and predictions aren't working that well.
1911360	1921820	A	0.7792916893959045	And then we call for this heuristic search to handle whatever is missing to see if we can find a new solution that wasn't captured.
1922160	1930530	A	0.6801563501358032	So in this case the powerful system is the predictive processing and not the Type Two research.
1936870	1939970	A	0.8997037410736084	So that was the general idea for this shift.
1940550	1946318	A	0.8685240149497986	Now I'm going to say some hypotheses that stem from this shift.
1946414	1963942	A	0.7960584759712219	So t one processes deal with content encoded in the form of probability density functions, which is how predictive processing uses to represent the world, which means there is no symbol and no definite content but values, means, standard deviation.
1964006	1979182	A	0.8029384613037109	Influenced by previous movements in previous world contingencies, manipulating prior information, biases the distribution to one or another direction closer to or further from a certain value.
1979316	1985022	A	0.8548730611801147	So people here in the active entrance institute we know this by heart.
1985076	1988850	A	0.7690296769142151	I'm not changing anything here, I'm just using what they say.
1988920	2000790	A	0.9022506475448608	Basically these functions are not stored in a memory bank but distributed from the responsible brain regions over to the external organs, body parts through neural connections.
2001850	2029710	A	0.5330876708030701	The values in the distribution do not represent objects directly indiscreetly they refer to distinct aspects of the input when perceptual systems are dealing with such objects this is aligned with the T one processes being easily biased when working with references to similar properties like similar numbers, objects, rhymes or pet names very often an incorrect value is picked from a distribution.
2030130	2058230	A	0.5550182461738586	An example that I have from this is when usually I have seen people confuse their youngest child with their dog name youngest child and when someone has another new child then the new youngest child is often confused with the dog because they are, I think, somewhat represented in similar distribution similar points of the distribution.
2059290	2065690	A	0.6143559813499451	This is also in line with claimants of embodied proposals that the world is not representative in symbols.
2067230	2074810	A	0.586337685585022	T one processes are subpersonal and their predictions are made by the same systems which process perception.
2074970	2096740	A	0.7065955400466919	A clear example is that a judgment about a facial expression is related to the FFA which is a region brain that processes faces and this is also related to that judgment that conman may argue that when looking at someone we make judgments about their face.
2097670	2107222	A	0.7968572378158569	The idea is that perception is not passive but already comes with predictions and when in problem solving such prediction is precisely the Type One answer.
2107356	2118170	A	0.8082250356674194	So if we look back at the reasoning test all those Type One tests are likely stemming from predictions.
2119790	2139934	A	0.5333042740821838	I don't know what to claim that T one processes are purely perceptual if in contest to cognitive only that such predictions reasoning is that there's not a clear line between what is perception and cognitive.
2139982	2146242	A	0.8664765954017639	So I think the word you're just using different words for this is the problem.
2146296	2152026	A	0.5572800040245056	So just not getting into that convenience.
2152078	2175790	A	0.7048007845878601	Example of judgment of negative spatial expression shows how this is expected of dual process theory like it's expected of dual process theory that Type One reason works under predictive processing, also in line with the claims of embody cognition that there is no sharp link between perception and reasoning.
2176610	2184350	A	0.7704506516456604	On the other hand, c two processing works like a classical machine for reasoning, such as General Problem Solver of Newman Simon.
2185010	2191358	A	0.7257418036460876	However, this classical machine only makes sense in the brain if it exists in a wider setup.
2191454	2224250	A	0.8415019512176514	Predictive processing network generating Type One responses like in Newell's Physical Symbol system, when facing a reasoning problem, T Two Processing opens a problem space containing an expression that designates the initial problem and an expression designates a solution which was produced by a probabilistic prediction having the initial expression and the predictive expression in the problem space.
2224400	2232414	A	0.822370707988739	T Two Processing then uses its move generators to attempt to reduce differences between them and sometimes find different solutions in such bad.
2232532	2235120	A	0.636920690536499	So let me just explain this a little bit better.
2235810	2245986	A	0.6739668846130371	When Newell was working in the 60s there they knew they couldn't search all possible space of a problem space because it wouldn't work.
2246168	2259320	A	0.5810753107070923	So they have this library search which works like a detective, let's ignore the problems which are likely wrong, just look at the spaces that might be right.
2261290	2277434	A	0.8147100210189819	What this predicting and reflecting framework does for a risk search is that the risk of search only works on boards already with the prediction that was sent from Type One processing.
2277562	2286980	A	0.6291066408157349	So it's like we only start reasoning and making searches based on the prior predictions we already had.
2287750	2293940	A	0.7684275507926941	And we only go further with this search when the predictions aren't working.
2295110	2306470	A	0.7096219062805176	And the main point here is that the risk search isn't searching a random space or even a pre programmed space.
2306620	2313958	A	0.706753134727478	It's searching a space that was left out from the predictions that started in Type One processing.
2314054	2327950	A	0.7247193455696106	So that's how it doesn't go to the frame problem anymore because a large part of the contextual issue was also already solved by predictive processes.
2329650	2346580	A	0.8011887669563293	Okay, so that was the general hypothesis I presented to attempt to solve the unity problem, which is the problem of how Type One features go together and how Type Two features go together.
2347190	2356550	A	0.5902478098869324	And now I'm going to try to argue that this is likely the case, since that I can't prove this is the case.
2356620	2368086	A	0.5475896000862122	But there are good reasons for us to think that Type One processes use predictive processing and that Type Two processes do not use predictive processing.
2368198	2373020	A	0.7639299035072327	Rather they need some kind of symbolic heuristic search.
2375010	2386550	A	0.5757986307144165	One thing that stems out of is a good explanation for the difference between implicit representation and explicit representation.
2386730	2398062	A	0.49921682476997375	If you look at the literature in psychology, you will see that it's very ambiguous in these definitions.
2398126	2410086	A	0.599155068397522	Like people use implicit and explicit representations as the same thing as unconscious, conscious or fleeting graspable, or the same thing as automatic in control.
2410268	2421660	A	0.7480194568634033	And so when it goes to dual process theory, it's not very clear what they're saying in each case, but they use this expression and explicit a lot.
2426050	2441342	A	0.7439757585525513	How I think this model solves this is by saying that implicit representations are probability density functions while explicit representations are symbolic representations.
2441406	2443970	A	0.6253086924552917	Classical symbolic symbolic representation.
2446070	2454594	A	0.8627821207046509	Clark in his book sometimes mentions that we can have single peak distribution distributions.
2454722	2468966	A	0.5933598279953003	But the problem with having single peak distributions is that well, if you only have one peak on the distribution function then it doesn't have all these features of probabilistic representation.
2469158	2474194	A	0.8686325550079346	It's more likely similar to a symbolic representation.
2474262	2486910	A	0.8658711910247803	The fact that having a single peak distribution might be the way that a probability representation turns to a symbolic representation.
2487750	2492210	A	0.5751487016677856	So these have different and important features.
2492950	2503270	A	0.8022139072418213	The probability representation is continuous, it's uncertain and ambiguous and that's why it's able to be sensitive to context.
2504170	2509154	A	0.8106968402862549	The mother example of part it can vary on retrieval.
2509282	2512380	A	0.7320529818534851	We can remember something and be different the second time.
2514270	2515846	A	0.7225364446640015	It's tied to priors.
2515958	2518060	A	0.6654884219169617	It works with statistical relations.
2519710	2530350	A	0.8558214902877808	Yeah, it's called in and depending on which value it's most probable at the time, it has a different outcome.
2531330	2542030	A	0.8672434091567993	So I argue that explicit representations are likely discrete, they are likely symbolic in nature, they're unambiguous.
2542110	2547694	A	0.6694575548171997	This is what allows us to disabilite in the first place they're stored reliably.
2547822	2556280	A	0.7685745358467102	If you have a reliable definition of it, you will likely remember it the same way the next time.
2556810	2566938	A	0.6501549482345581	It's arbitrary in the sense that it's not really coupled to the stimuli, in the sense that the perception of perception usually is.
2567104	2571740	A	0.802401065826416	The statistical relations are related to aspects of the world.
2573150	2579150	A	0.8117870688438416	It's using compositionality and it's immutable in the sense that you can't change it.
2579220	2583760	A	0.7551795244216919	It's fixed value doesn't change.
2586230	2589326	A	0.8198295831680298	So that's the implicit versus explicit feature.
2589438	2595102	A	0.8346670866012573	You can have also explanations for automaticity in contrast to working memory.
2595246	2604870	A	0.6125800609588623	So automaticity concerns overlearn skills and overlearn skills are understood as skills that have become predictable in this framework.
2605690	2610710	A	0.8788305521011353	So we usually use this to explain like driving, riding a bicycle, riding.
2612430	2628414	A	0.6267679333686829	So when you're driving, if you don't know how to drive, you need certain statements, like some statements about how you, how you should steer, right?
2628452	2630080	A	0.8181216716766357	You need to have those in mind.
2630610	2638770	A	0.6952527165412903	But then once you learn how to drive, it's like your body already knows what it's doing and you don't even need to think about this.
2638840	2641742	A	0.8658781051635742	So this is the classical distinction in psychology.
2641886	2652520	A	0.7403746843338013	But I argue that the predictive processing theory makes a new enlightenment to this.
2653530	2657398	A	0.708949089050293	So it's like our body is predicting what we're doing.
2657564	2658786	A	0.6796759366989136	So it's automatic.
2658898	2661746	A	0.7590482831001282	So it's an explanation for optimisticity.
2661938	2670026	A	0.6151739954948425	But say that a dog certainly suddenly runs from his car.
2670128	2672874	A	0.6488057971000671	You obviously don't have a model for that.
2672992	2680240	A	0.6049752235412598	So you're going to need to call in work in memory to solve some of those issues so you don't hit the dog.
2681170	2693220	A	0.6027629375457764	So everything works under prediction unless the model is very unreliable and then working memory is called in to solve some further issues.
2694550	2694914	C	0.5721262693405151	An.
2694952	2699794	A	0.566195011138916	Interesting hypothesis I had on the PhD thesis which I never explored.
2699922	2716390	A	0.8688254356384277	I actually don't even think anyone read this part, but it seems to me to be a very interesting hypothesis in relation to free energy in active inference.
2716470	2726400	A	0.7445526123046875	So that's why I'm bringing it here in the Active Inference Institute that as we know, the brain attempts to minimize free energy by getting predictions right.
2727170	2744690	A	0.6482299566268921	Thus higher need of type two processes are related to higher free energy interpreting information in the sense that when the predictions are working clearly, then you don't need to have effort.
2745430	2754598	A	0.5201201438903809	But when there are a lot of prediction error, then you need effort because you call in the working memory to do a risk search.
2754764	2759926	A	0.5443201065063477	So to minimize a great amount, better amount of free energy will take more time and work.
2760108	2765946	A	0.6308774352073669	This is more effortful than having predictions ready that minimize free energy as quickly as possible.
2766128	2772250	A	0.5937711596488953	Before, when probabilities fail, the system needs to start risking possibilities.
2772830	2776940	A	0.8144118189811707	It searches for other possible solutions by means of heuristic search.
2777470	2784526	A	0.5421544909477234	Heuristic search would be related to more information and time because it does not have probable solutions ready.
2784708	2789342	A	0.6156800985336304	Instead it needs to investigate its state space almost from scratch.
2789486	2792094	A	0.8111503720283508	We say almost because it is heuristic.
2792142	2796590	A	0.6596807241439819	And hence it also will have tricks to get the correct solution faster.
2796750	2807990	A	0.854734480381012	Unlike group force search, which would investigate the state space from beginning to end, we believe reports of effort would be related to executing more heuristic searches.
2808650	2815494	A	0.8793309330940247	Reports of effort by subjects would seem to be based on cognitive informational and physical constraints of reality.
2815622	2825730	A	0.8520955443382263	So that's why I think this is an interesting part of the hypothesis is because we have this psychological measure of effort.
2825830	2830826	A	0.825379490852356	But if this is true, then we actually have a physical measure of effort.
2831018	2847220	A	0.46980294585227966	Like if people are working hard on the problem, that means there's a lot of free energy going on and they are having to use this the risk of search to fix some issues that the model isn't able to fix by itself.
2852630	2854982	A	0.7876410484313965	Yeah, so that's that's the effort part.
2855036	2873020	A	0.7035505771636963	So now going for the working memory part, what's interesting here is that in working memory, working memory, it's expected of it that it works like a classical computer.
2874110	2886110	A	0.6749475002288818	So working memory is a widely research topic in psychology, but in predictive processing it's rarely, rarely mentioned.
2886260	2893380	A	0.7512122392654419	And I actually searched the predictive processing books, control F for working.
2893830	2902466	A	0.6895199418067932	And people rarely use this topic because it's not done by predictive processing.
2902498	2905670	A	0.6733720898628235	It has nothing to do with predictive processing.
2906090	2919126	A	0.4880537986755371	It's actually an issue for predictive processing because well, if you're going to eliminate classical symbolic processing altogether, then we need a predictive processing explanation of working memory.
2919238	2923850	A	0.68623948097229	And so far I haven't found one and I don't think there will be one.
2924000	2927914	A	0.5449693202972412	But this also could be just ignorance of mine.
2927962	2932910	A	0.6355713605880737	Maybe there's some paper there which I haven't found might prove me wrong.
2932980	2949650	A	0.551180362701416	But in any way, working memory is very aligned with what Turing was thinking when he was making the connection between machines and minds.
2950250	2962150	A	0.8507971167564392	So if this is a sentence of Alan Turing explaining his computer, right, the Turing machine.
2962810	2969580	A	0.6419005393981934	And if we change the word computer to work in memory, it clearly works.
2969950	2980382	A	0.5028812885284424	It's almost like it's talking about the same thing, when of course, if we change the word here, computer, to predictive processing, it's clearly not talking about the same thing.
2980516	2990254	A	0.8844134211540222	So the behavior of the computer or working memory at any moment is determined by the symbols which it's observing in his state of mind at that moment.
2990452	2997586	A	0.8752248883247375	We may suppose that there is a bound b to the number of symbols or squares which the computer can observe at one moment.
2997768	3002390	A	0.8783841133117676	If he wishes to observe more, he must use successive observations.
3002730	3008306	A	0.8446390628814697	We will also suppose that the number of states of mind which can be taken into account is finite.
3008498	3014730	A	0.6810526847839355	The reason for this are the same characters as those which restrict the numbers of symbols.
3015070	3022486	A	0.6363999247550964	If we admitted an infinity of states of mind, some of them would be arbitrarily close and would be confused.
3022598	3034190	A	0.6309790015220642	Again, the restriction is not one which is seriously affect computation since the use of more complicated states of mind can be avoided by writing more symbols on the tape.
3034850	3050450	A	0.7637604475021362	So this is very similar to what we think about when we're talking about working memory, not only because we were instance by Turing, but because of the evidence that comes from working memory is very similar to what suspected of this sort of machine.
3051930	3054642	A	0.6032905578613281	Finally, we have speed.
3054786	3066860	A	0.5335143208503723	So predictive processing has various strategies to make the processing be faster as fast as possible.
3067310	3090512	A	0.6012466549873352	While that's not true for symbolic classical AI, so yeah, Clark says cheap, fast world exploiting action rather than the pursuit of truth optimality and deductive inference is now the key organizing principle.
3090576	3104520	A	0.5490598678588867	When he's talking about predictive processing, the predictive processor is always taking certain bets about what the current state of the world implies losing accuracy, compensation for speed.
3105740	3112170	A	0.8608300685882568	We also have predictive coding in the more strict sense.
3114460	3123448	A	0.6063156723976135	So by predictive coding we mean specifically the property of the system to consider from the world only stimulant which result in greater prediction error.
3123544	3130220	A	0.8190855383872986	So there's also a filter there which allows for speed in perception and focus.
3130290	3139600	A	0.8274863958358765	Beyond predictive prediction relevant stimulant only permits the agent to quickly decide forces of action and select amongst the possible forces.
3140040	3147008	A	0.8015032410621643	So predictive processing is tailored to be fast, necessarily.
3147184	3154816	A	0.5742678046226501	Well, that's not the case for symbolic AI and so that's the explanation for the difference in speed.
3155008	3160648	A	0.784130334854126	So beyond that, the T two processes need to figure out the solutions online.
3160734	3168952	A	0.8327510952949524	So it's different if you have a prior prediction that would say to you what the answer is from having to search its place from scratch.
3169016	3169628	A	0.5664746165275574	Right.
3169794	3176316	A	0.8744238615036011	There is also another issue that the biological brains certainly were not built for.
3176338	3187964	A	0.8322569131851196	A serial heuristic search photo pilotians say that the morals that the absolute speed of a process is a property part excellence of its implementation.
3188012	3199088	A	0.6156764626502991	And since the brain does not have a symbolic processor implemented, if we do use some sort of heuristic search, it's like a different adaptation.
3199184	3202070	A	0.7869218587875366	It's not what the brain is used to.
3202440	3207210	A	0.6731454730033875	Inserting problem spaces is slower than having a problem outcome ready.
3209980	3216780	A	0.8606897592544556	Okay, so that was the work I did in my PhD thesis.
3217440	3238640	A	0.9176792502403259	And those one, those were one of the main reasons why I think this framework is good for explaining the differences in features of dual process theory.
3239940	3251268	A	0.6562780737876892	And I actually wrote a prediction that I recently learned that I wrote this because I, I forgot I wrote this.
3251354	3259290	A	0.7272684574127197	And then I went back to to the thesis and and I said, well, I actually said this would happen and it kind of did.
3259820	3267180	A	0.9213320016860962	So that was one of the reasons why I renewed my interest in my PhD thesis.
3267920	3281520	A	0.7358530163764954	So what I said was predictive processing system would be subject to bias from lack of compositionality such as mistakes in connectivity, failures in noticing necessary character, formal rules and so on.
3281670	3285724	A	0.7634498476982117	Precisely the type of mistake, type one processing incurs.
3285772	3289776	A	0.5111048221588135	And so at that point we didn't have large language models.
3289808	3293696	A	0.6184605956077576	In fact, the attention is all unique.
3293728	3295750	A	0.8259350061416626	Paper came out in the same year.
3296920	3308840	A	0.7970924973487854	And then I noticed that this could be seen as a prediction of the type of issues that large language models are facing, that's reliability.
3309260	3318060	A	0.547836422920227	Hard time keeping the order of units or steps in complex reasoning or math straight, hard time letting go of priors.
3318400	3334370	A	0.8266728520393372	And to my what really motivated me to start working on this scan was that they solved this precisely by using dual process theory and t two agents they're calling.
3335300	3347456	A	0.943342387676239	So what they're doing now to solve these reliability issues is adding a heuristic search to the genetic model and it's having awesome results.
3347488	3350710	A	0.6492499709129333	This is just this year 2023.
3352140	3368990	A	0.4982678294181824	So the more they develop these type two agents to answer these reliability problems of large language models, the more the large language models are getting good at stuff.
3369840	3385584	A	0.8969728946685791	So this is a paper, I got the images from the paper tree of Socks, right, which is one of the last ones, probably came out about two months ago, something like that.
3385782	3405988	A	0.8849203586578369	The way they're doing this implementation is they're taking the results outputs of the large language model and they're doing further reasoning on it and then feeding the large language model with the result of the further reasoning.
3406084	3411560	A	0.8396669030189514	The further reasoning they're doing is very similar to the classic symbolic AI.
3412460	3423960	A	0.7928748726844788	So here they explain there's a simple input output prompting here, which is simply inputting a prompt and the large language model will give you an output.
3424120	3429600	A	0.7456057667732239	And as we know, these outputs sometimes are very biased.
3430180	3434610	A	0.8387354016304016	There's hallucination on it, they're not reliable, all that stuff.
3435620	3448768	A	0.5014398097991943	And last year they figured out that through chain of thought prompting we could also already increase the reliability of these models.
3448864	3473704	A	0.524848222732544	So you give an input and just as we do, the better prompting when we go to Chattbc and say, I'll do this better fix this issue, solve this step by step, some of the commands that are good for Chatter, PT and Chain of thought prompting automated some of that, some of those reasoning steps.
3473752	3480256	A	0.8136795163154602	And then large language models were working much better after this.
3480358	3481792	A	0.8606948256492615	So this was last year.
3481846	3503050	A	0.6020388603210449	And then last year this started a new trend of research to make these agents coupled to large language model output linguistic outputs back into the large language models to see how they can get better.
3505100	3512440	A	0.8427514433860779	The self consistency one, you give the input and then there are various solutions.
3516240	3525464	A	0.7929524779319763	The self consistency forces the large language model to try different solutions and then you get a majority vote.
3525512	3538640	A	0.6322206258773804	So if you have like five similar answers, some different paths, then this one SPicked over the one which is less popular.
3539400	3553210	A	0.7258915305137634	And the Tree of Thoughts, which I thought was most similar to the proposal I had in psychology, in Duprose theory, is the one which opens up a problem space.
3554300	3557444	A	0.7059432864189148	It's actually pretty much what I just explained.
3557492	3578210	A	0.6478250026702881	In the case of psychology, they open up a problem space and they start to search possible solutions which are better than the original and they feed that back into the predictive processing, sorry, the generative model, the large language model.
3579620	3586930	A	0.9430208206176758	And then this one is currently one of the best ones they have.
3588740	3596870	A	0.8738336563110352	So here I made a general diagram of how this would work.
3597880	3600480	A	0.5385151505470276	It's not psychology nor AI.
3600560	3603210	A	0.7977654933929443	It's like what's singular in the two?
3604940	3610760	A	0.8310304880142212	So we have a generative model here using probability density functions.
3611260	3619260	A	0.6501802206039429	This looks more like a traditional cognitive cognitive connectionist model than a predictive processing.
3621120	3623068	A	0.8305961489677429	It's just the diagram is not that good.
3623154	3625310	A	0.873623788356781	I'll talk a little about that.
3628500	3636172	A	0.8237414956092834	We generate an answer here, which is to the problem here, and we get a linguistic output.
3636236	3642740	A	0.8919098377227783	And this linguistic output then goes for an heuristic search using symbolic representations.
3644440	3650470	A	0.8110963106155396	So if you get a better answer here, feeds it back to the Genitive model and so on.
3651000	3661176	A	0.8576421141624451	Basically, the knowledge part is the Genitive model and the heuristic search is simply using more steps to see to prompt it back.
3661278	3679680	A	0.796795129776001	It's more like a prompting scheme than a new knowledge scheme, which is both true for the dual process theory model I created and the heuristic search that's happening here on the T, two agents for LLMs.
3681220	3690660	A	0.523434579372406	A better way to do this, and I think Tree of Thoughts does implementate this, is that the input layers are trying to predict the thoughts.
3691320	3695620	A	0.8272276520729065	It's a bit more like predictive process than the other diagram.
3697080	3706840	A	0.7515885233879089	And so they're trying to predict the thoughts and by open up this searching this problem space during these steps.
3707660	3713370	A	0.690314531326294	We already have new prompts here for the Gentrific model.
3715500	3722270	A	0.6335919499397278	So if you take predictive processing seriously, and this does happen in the brain, more likely like this.
3722720	3733120	A	0.8009582757949829	I do think that the Tree of Thoughts paper does say that the intermediary processes interfere in the searching on the aliens.
3734660	3749296	A	0.5729677081108093	Finally, I thought maybe we could suggest something from psychology for these models to keep growing, working differently based on what we know from psychology.
3749488	3763892	A	0.6974620819091797	So some things that are not implemented yet is that executive functioning have these updating abilities in intelligent forgetting, which is related to insight problem solving.
3763956	3769964	A	0.5317085981369019	It's when you forget something, but it means, like, you forget the prior, right?
3770082	3776692	A	0.6664797067642212	You're using a different prior, so you're able to let go of biases.
3776776	3777410	A	0.5664746165275574	Right.
3778180	3782880	A	0.6433060169219971	There's also work on thinking dispositions, which could be relevant.
3784900	3791888	A	0.8463453650474548	Sentences like this belief should always be revised in response to new information of our evidence.
3792064	3799460	A	0.8769630789756775	So these would be, like, imperative linguistic knowledge, which would have to be added on.
3799530	3803320	A	0.5083665251731873	I don't know where, but likely would help work.
3803470	3809796	A	0.6480154395103455	There's a lot of thinking dispositions that are relevant to our successful reasoning.
3809828	3811690	A	0.5493825674057007	And this is just one example.
3813020	3816300	A	0.5424411296844482	The literature knows a lot more examples that could be relevant.
3817360	3835920	A	0.7034099698066711	We have in creativity research, we have this generative phase, which is similar to what the generative models are doing, but we do not have the exploratory processes going on, which is common in the creativity research literature.
3836820	3842180	A	0.5847791433334351	But it's what we do with, for instance, mid journey when we're doing better prompts.
3843240	3863850	A	0.6922168135643005	So maybe using these exploratory processes from the creativity research, we can also automate the exploratory part of the creativity processes that these image generators are getting to, and not just the generative part.
3864240	3872760	A	0.5354564785957336	And finally, I think the embodiment, you don't have robots or anything like that that do this sort of reasoning.
3872840	3888624	A	0.6357932686805725	So AI on the computer and also stuff from active entrance, which was not considered slightly related to embodiments in the sense of navigating the world and predicting the world.
3888822	3895270	A	0.6122167706489563	That stuff is far from happening in the traditional Galileans we have.
3896040	3898470	A	0.548243522644043	So, yeah, that's a lot of stuff.
3899160	3903400	A	0.7769731879234314	Sand, maybe I hope you guys have some comments.
3903980	3921592	A	0.6837844848632812	As I said, I haven't been able to talk about this to anyone, and I do think I have some, at least some maybe some relevant stuff that I've published.
3921656	3927576	A	0.5302860140800476	But since I'm not famous and I don't have people to talk to, it's mostly gone unnoticed.
3927768	3929820	A	0.8845898509025574	So thanks for being stay tuned.
3932980	3933680	B	0.8529649972915649	Thank you.
3933750	3934940	B	0.9605408906936646	Awesome presentation.
3935020	3935852	B	0.710349977016449	All right, Nick.
3935916	3940790	B	0.954698383808136	It'd be awesome to hear your introduction and then take it wherever you'd like to go.
3942200	3942564	C	0.4753468334674835	Sure.
3942602	3945892	C	0.9541037678718567	So first, thanks, Samuel, for interesting talk.
3945946	3948388	C	0.9799308776855469	And I've been really pleased to find your research.
3948474	3954344	C	0.8424627780914307	I guess maybe I'm supposed to say something about who I am, where I am.
3954382	3955508	C	0.825480580329895	So I'm Nick Bird.
3955524	3964036	C	0.849522590637207	I'm at the Stevens Institute of Technology in the New York City metropolitan area in a department that's kind of interdisciplinary.
3964068	3970510	C	0.855548083782196	So we have, like, philosophy and quantitative social science, but also, like, people doing, like, music and visual arts and all sorts of other things.
3971040	3975470	C	0.8507341742515564	I tend to do more of the philosophy and quantitative social science stuff.
3976720	3980744	C	0.5485167503356934	So I'm kind of also interested in human reasoning.
3980792	4000064	C	0.8813996315002441	And so much of the people I'm reading and citing and drawing on in my research are a lot of the people that you saw in the opening slides that we saw people like was on and Conneman Tversky and Mercier and Sperber and a lot of the other people that were cited.
4000192	4007704	C	0.9059516787528992	I think Samuel does a really good job to incorporate the kind of like shoulders of giants that we're standing on in terms of the philosophy of mind, right.
4007742	4010970	C	0.5625994801521301	The voters and the Pilitians and these people.
4012540	4018728	C	0.5551265478134155	And one thing that I think is on everybody's mind these days is these large language models.
4018744	4040336	C	0.9025992751121521	So I'm really glad to see how this dual process theory that emerged from research like economan and diversky's on these problems like the conjunction fallacy problems or the waste on selection guard task or the conjunction fallacy tasks, how that has been applied to models like large language models and what we can learn from that.
4040358	4053156	C	0.9236923456192017	And then I think the coolest thing about the preprint print is how we could kind of port back and forth the learnings from both the computer science and the cognitive science to improve one another, right?
4053178	4074572	C	0.8521261215209961	So I think that one of the neat things about this preprint that Samuel was giving us at the end is that we're kind of taking what we think is a model of reflective reasoning that we represent certain parts of a problem and maybe reason more carefully and effortfully about them in certain situations in ways that might help us.
4074706	4077884	C	0.8152515888214111	And how can we help these large language models do the same?
4078002	4084240	C	0.7161489129066467	Because it does seem like large language models function, at least the ones that we interact with online.
4084310	4084876	C	0.5742325186729431	Mostly.
4084988	4088080	C	0.8539062142372131	They seem to function mostly as like a system one or type one process.
4088150	4092804	C	0.7798539996147156	They're just kind of like quickly generating lots of text or imagery or something like that.
4092842	4105370	C	0.5059160590171814	But they're not necessarily reflecting on it in the ways that we would think that a human is capable of and they might not even be capable of representing things in the same way that we do.
4106220	4108184	C	0.9671151638031006	Yeah, so I thought that was a really interesting idea.
4108302	4128236	C	0.9203052520751953	Using chain of thought and tree of thought models to kind of create a reflective level or reflective system within the models seems just like a really valuable idea and just a really great synthesis of research in multiple disciplines, something that I think few people are actually very good at.
4128258	4131036	C	0.8608251214027405	It's like incorporating some of the best insights from multiple fields.
4131068	4132844	C	0.5845221877098083	We're often pretty siloed in academia.
4132892	4140450	C	0.9869676232337952	So I just think this is great work and I hope more people will appreciate and pay attention to it and build on it.
4140820	4153176	C	0.847356915473938	So one of the things that stood out to me in this presentation more so than when I was, like, reading this 2023 preprint analytic Reasoning for Large language models by Dr.
4153278	4154840	C	0.8008809685707092	Bellini Leiche.
4155820	4169710	C	0.7911871671676636	The thing that stood out to me is this idea that Bellini Leiche and Frankish sort of switched which of these two types of reasonings processes are supposed to be context dependent or context sensitive or however you would want to word that.
4170080	4179244	C	0.7379382252693176	And I started thinking, well, I wonder if there's a sense in which both types of responses or both types of reasoning are somewhat context dependent and I'll just say what I mean.
4179282	4188192	C	0.8972494602203369	And then maybe, Samuel, you can kind of say what you think I should think or clarify the view of the predictive and reflective framework or something.
4188246	4202464	C	0.8032792210578918	So the thought I was having was, well, in a familiar context, these intuitive predictions, these type one processes, our gut response, so to speak, those are going to be pretty useful because they're well trained.
4202592	4204312	C	0.49809399247169495	We have a lot of experience that we're drawing on.
4204366	4207530	C	0.8589746952056885	So like our gut response, our first response is often quite good.
4207980	4215704	C	0.7699126601219177	It's in these less familiar contexts, or maybe similarly familiar, but, like, way higher stakes or something.
4215742	4226716	C	0.7602601051330566	But it's in these other contexts where we might think, maybe I should slow down and make sure I've double checked whatever my initial impulse is before I just accept it because there's a lot riding on this.
4226738	4232048	C	0.8556257486343384	Or, like, I'm just not used to this type of problem, so I need to slow down.
4232214	4243910	C	0.846839964389801	And so there's a sense in which what context is doing is not just showing up in one or the other type of reasoning, but it's sort of like determining which type of reasoning might be best at the moment.
4244280	4252100	C	0.7152081727981567	But I'm wondering, is that just totally compatible with what you're imagining or is that somehow a deviation from the framework, the predictive and reflective framework?
4252940	4256916	A	0.8150978684425354	Yeah, thanks for the thoughts.
4256948	4263050	A	0.8978796005249023	There compliments and thanks for the question as well.
4264080	4281996	A	0.8078224658966064	So the reason why I think context needs always some sort of type one help is because of the constraints we have learned from symbolic AI.
4282108	4295350	A	0.6225230693817139	So if I'm saying that type two reasoning have the same constraints, then it cannot be contextual because it doesn't work like that.
4296040	4299830	A	0.804445207118988	We know that if it fails in context, it fails bad.
4300600	4312312	A	0.6418080925941467	But of course we do need to like when we have a novel situation, we often solve this by context, right?
4312366	4313850	A	0.5056394338607788	So how could this be?
4314640	4321470	A	0.7356312274932861	And I think it's by the interconnection of the power of the two system.
4325440	4334604	A	0.5638387799263	So we always start with a prediction, which in this case would be mistaken, and then we have to search for novel answers.
4334652	4341552	A	0.8257925510406494	And in this search we make the contextual comparisons.
4341696	4349110	A	0.5452731847763062	So it can't be that the system too is making the comparison on its own.
4349480	4365470	A	0.8286357522010803	I think it's more likely that when it's a case of context that stems from a novel situation and then the two systems have to figure out it together.
4366080	4386930	A	0.6856339573860168	Likely in the sense that I was pointing out here in the end, these predictions that go over the thoughts, they most likely are the ones that will solve these context issues in novel problems, I'd argue something like that.
4392280	4395792	C	0.9137988686561584	Okay, I think that's somewhat helpful.
4395856	4400916	C	0.8738454580307007	So then maybe what I'm thinking is something along what the lines you were saying at the end.
4400938	4412356	C	0.8147790431976318	You were saying how there's still kind of an opportunity to understand things like executive function in this framework.
4412548	4415660	C	0.8065385818481445	And I think that's maybe part of what I'm wondering about.
4415810	4432928	C	0.8789176344871521	So I guess I'm wondering if there's more to be said within this predictive and reflecting framework about how each of these two types of processes get selected or, like, what might help the system.
4433014	4433984	A	0.6407236456871033	Hold on.
4434182	4434624	C	0.6500546336174011	Go ahead.
4434662	4439576	A	0.856311023235321	That specific point you just remember, I just remembered something that's related.
4439708	4445910	A	0.7685940861701965	So, yeah, clearly our working memory or type two reasoning likely does more than that.
4446520	4449990	A	0.8063400387763977	It does this, but likely more than that.
4452300	4464840	A	0.9111655950546265	There's likely also a belief bank, something like the thinking dispositions, some sort of knowledge bank, which is related to type two processing.
4466240	4480172	A	0.7719135284423828	So Keith Franklin does this distinction between flat out beliefs and I'm not remembering terms, but he has a type one belief and type two belief.
4480316	4485792	A	0.5433505773544312	Type one belief is that belief you have, but you're not fully confident of it.
4485926	4499670	A	0.7639920711517334	And the type two belief is the belief you have when you have a very firm political position and you state it with obvious words that you sure you believe that.
4500760	4511432	A	0.7444210052490234	So there's likely to be some sort of different belief structure as well, which I don't talk about at all thesis of the work.
4511486	4521630	A	0.5943936705589294	So I'm not saying, obviously that this is a complete model of reasoning, and obviously we'll need more to figure out how reasoning works.
4526530	4527470	C	0.5509032607078552	Yeah, okay.
4527620	4528930	C	0.8737272620201111	That's also helpful.
4530070	4533700	C	0.9203454256057739	I'm wondering if Daniel, did you want to weigh in?
4535030	4542600	B	0.6560617089271545	Well, I think it's a very interesting question about which of those modes are engaged or how they're balanced through time.
4543210	4553110	B	0.8854343891143799	And how does this connect to what's known as the context window in today's transformer type large language models.
4554350	4565202	B	0.8819630146026611	So how do you map the computational attributes of large language models today, like architecturally or their Ram or CPU usage?
4565366	4570074	B	0.8647612929344177	How do you map that onto, for example, human cognitive processes?
4570122	4573440	B	0.8484394550323486	Do you think that's useful or there are any insights there?
4575090	4595846	A	0.6126527190208435	Well, I'm mostly making a relation between predictive processing and generative models, but I know that large language models are not entirely the same as what active inference is saying.
4595948	4604280	A	0.7423483729362488	I'm aware of that, but there is some similarity specifically in regards to the generative model part.
4605150	4617306	A	0.6674966812133789	So I'm not sure I'm not an AI specialist to be sure which details of a generative model could be true of the human brain.
4617418	4633940	A	0.7298301458358765	And I'm most likely betting that the people who study generative models in the brain, like the Frisons and all, have figured out something like what our generative models do.
4641270	4653810	B	0.897243857383728	One other thought or nick, you want to add there just thought of some different ways whether people have connected it explicitly to the literature on working memory in predictive processing.
4653890	4657942	B	0.503265917301178	I think, as you pointed out, it's definitely a link that is not highlighted.
4658086	4673946	B	0.9010969400405884	It's one that we recently heard from Professors Walker and Monriquez and Pristine in the recent Livestream 53 that was on like, cognitive paleoanthropology looking at human working memory.
4673978	4693700	B	0.7590109705924988	But some ways that people have incorporated working memory is like a nested model that carries context at a deeper time, but that is not necessarily like an actual mechanism that gives rise to why type one and type two are the way they are.
4694890	4710314	B	0.7622673511505127	So I thought that was a very provocative direction to go to move past the descriptive and then to look at the underlying generative model of why these outcomes are the way they are.
4710352	4726110	B	0.7151248455047607	Even though these cognitive phenomena are causes of other things happening, they are also caused by some influenceable or contextual or variable aspects.
4726610	4737140	B	0.839510440826416	And when we lose that context dependence of the cognitive systems, then they're totally lifted and disembodied don't really help.
4738390	4763610	B	0.5640239715576172	So by putting the primacy on that predictive processing element, I think it opens the door to connecting those areas beyond just mentioning the terms in proximity, but to really support different epistemologies from predictive processing as a model or approach.
4766430	4767180	A	0.5491447448730469	Yeah.
4768030	4770014	A	0.8657985925674438	Can you go on that idea again?
4770052	4772298	A	0.611121416091919	I'm not sure what you want me to comment.
4772474	4775406	A	0.818183183670044	Can you summarize that again and make it a question?
4775588	4776222	B	0.4753468334674835	Sure.
4776356	4790210	B	0.8868221640586853	What do you think the epistemological consequences are of taking predictive processing the way that you have approached it here versus alternative or prior approaches to cognitive sciences?
4792070	4795110	A	0.7676071524620056	Okay, but what do you mean by epistemological.
4798490	4799430	B	0.7865362167358398	Normative?
4800170	4801160	A	0.7345588803291321	How we know.
4803610	4810250	B	0.8437761068344116	How it influences our understanding of how we know or seek or practice or do or decide.
4812350	4812906	A	0.7672419548034668	Perfect.
4813008	4815820	A	0.8523040413856506	Okay, let me think about that, for instance.
4817870	4837810	A	0.580754280090332	Yeah, I think one of the best what this helps with most is explaining dual process theory because dual process theory is in bad shape in terms of concepts, in terms of theory and formulation.
4839830	4846390	A	0.6681166887283325	People have no idea what these systems refer to in the mind brain.
4847610	4852600	A	0.8679735064506531	They often have different intuitions into this.
4852970	4866246	A	0.5748700499534607	And basically what I most think this work is relevant for is for saving dual process theory from criticism, which has been happening in psychology.
4866358	4877822	A	0.6157218217849731	So people are like, it seems like we're talking about the left and right brain, something like that, something that we don't know for sure what these two systems are.
4877956	4878640	A	0.5664746165275574	Right.
4882070	4888690	A	0.9439486265182495	So this is a good way forward, I think, for dual process theory.
4889030	4897080	A	0.5464893579483032	And as I said, dual process theory does have some value in itself.
4899690	4911578	A	0.8492698669433594	So, yeah, saving the process theory, I think is a good direction for this model.
4911744	4933810	A	0.5339512228965759	But also if we think about the advantages of it would be something like, for instance, it could have helped us had the idea of using Chain of Thoughts or Tree of thoughts.
4934230	4936130	A	0.7940796613693237	Had anyone read this before.
4936200	4945874	A	0.7418054342269897	So it can help us make these sorts of predictions that we shouldn't have some sort of zero reasoning coupled to the gentle models.
4946002	4969340	A	0.6562286615371704	So yeah, I think it could help us think further on on AI, if we AI, like Nick was saying, to incorporating more stuff in psychology that we know that's certainly missing here, but here we have a direction of what we should incorporate and so on.
4970210	4978686	A	0.5308057069778442	But I'm also unaware of the working memory research from predictive processing you mentioned.
4978868	4985780	A	0.706463098526001	And if you want to go back to that and explain me some more of that, I'd be happy to hear.
4988950	4990098	B	0.4872898459434509	Yeah, sure.
4990184	5001670	B	0.6542350053787231	So not that this is the most effective way to implement a memory system, but at least that this provides analytical method for measuring or describing them.
5001820	5018186	B	0.7608198523521423	If one were going to have the five digit number, you could imagine a nested model where the lowest level is the ones place and it's nested within a decision tree of tens places and hundreds and so on.
5018368	5028346	B	0.8657339811325073	And then there's some seek or access policy or strategy that helps speak the number in reverse.
5028538	5037730	B	0.7393805384635925	And some cognitive or computational limitation is just how well that cognitive agent can perform on that test.
5037880	5052834	B	0.8319069147109985	Again, that doesn't mean that's the mechanism that's being used, but that would be a way to use a nested generative model to encode like multiple levels of spatial or temporal variability.
5052962	5062498	B	0.6151571869850159	But context becomes an issue because you're basically just expanding the possibility space looking for sparser and sparser associations.
5062674	5079790	B	0.483168363571167	So if you don't have a good compositionality or really well definable well articulated causal process, then you're just building these all by all models that you're going to be searching through in the dark.
5082390	5093410	A	0.49204525351524353	Yeah, one point I want to comment on that is that, well, it's not clear that the best way to reason will be the way humans reason.
5093480	5093918	A	0.7360090017318726	Right?
5094024	5104550	A	0.6626089215278625	So we might eventually have a different way of implementing working memory on an LLM which would be more effective than ours.
5105610	5124170	A	0.6443156003952026	There's also to the other side, there's also the point that it seems that the limited capacity we have on working memory, maybe it's not just a lack of power, maybe it's necessary.
5124250	5133890	A	0.7812971472740173	So we kind of like don't lose our minds in the sense that we have a limited space to search and then we're able to search this limited space.
5133960	5149122	A	0.5948875546455383	And if it was too, if we have too much space to search because we have super working memory, then we wouldn't be able to finish the task anytime.
5149266	5157290	A	0.6194272637367249	So maybe this is like a physical good barrier, something that helps.
5158110	5183726	A	0.908063530921936	And it's interesting that what I find interesting is that okay, the AI may find a lot of different and better ways to implement work memory, but it's interesting that they implemented a worksheet memory or a search agent, which was very like the way I described in the thesis.
5183838	5186900	A	0.9723256230354309	That was what I found amazing.
5187990	5189220	A	0.7067722082138062	As you were saying.
5189590	5202950	A	0.6746219396591187	There's very ways to implement it working memory, but they did it in a very similar way in the sense of searching the space after the genitive model has offered a solution.
5203850	5220894	A	0.5515037775039673	So I don't know, maybe that's a good indication that this is on the right track, but it also may just simply be a simple way they found to solve the problem.
5221012	5225038	A	0.7915506958961487	And it was just by coincidence similar to this.
5225204	5230846	A	0.8314593434333801	Yeah, maybe the correct way or the way the brain does may also be different.
5230948	5231922	A	0.6351645588874817	We don't know.
5232056	5245810	A	0.8339266180992126	All we have was the limited evidence I offered a bit more obviously and some constraints we know on AI information processing.
5246870	5248134	A	0.7328722476959229	We have to figure it out.
5248172	5263260	A	0.7828356027603149	And I think that this implementation stuff you're asking is mostly due to the AI people to find out or the math people, the systems to figure out.
5265630	5269370	B	0.9089995622634888	Nick, do you want to comment or a question or I'll ask one from the chat.
5269870	5270186	A	0.5491447448730469	Yeah.
5270208	5277486	C	0.707660973072052	So I just have a kind of a bigger picture question that kind of gets us into the realm of these chat bots, right?
5277508	5285214	C	0.783911943435669	So you in this preprint and in this talk have like given us a variety of different tasks in the heuristics and biases literature.
5285262	5293218	C	0.5018627047538757	The the conjunction fallacy task, the the lend a problem, some people call it, or these cognitive reflection test questions.
5293384	5305286	C	0.8991216421127319	And you know, there's a, there's like a paper in the fall that showed that a preprint in the fall that last fall that showed 2022, that showed GPT-3 and 3.5.
5305388	5311186	C	0.5892044901847839	And even the earlier versions, like, showed a lot of the human, like, faulty intuitions.
5311298	5314040	C	0.5131837129592896	Like it basically fell for the lure pretty often.
5314810	5317430	C	0.5297799706459045	Sometimes, like most of the time it was falling for the lure.
5317510	5326110	C	0.7025327682495117	But then as soon as GPT Four was available to be studied and used on these models, it was like performing basically near perfect.
5326260	5334626	C	0.7884088754653931	And when it did get the incorrect response, it wasn't necessarily the lure, it was just like some other general type of error it was making, like misinterpreting the question altogether or something.
5334808	5345250	C	0.7888848185539246	And it seems like the predicting, predictive and reflecting model framework could have at least two different ways of explaining this.
5345320	5357394	C	0.7319775819778442	One is what happened is they changed the AI's like type one thinking or whatever so that it could respond intuitively to all these, but intuitively and correctly, so it didn't need to reflect.
5357522	5372650	C	0.8454870581626892	Or it could be that they somehow created this reflective type system with either like a chain of thought or a tree of thought or some other type of system that helps it actually engage in reflection on these tasks or whatever the analog of reflection would be for a chat bot.
5373470	5380414	C	0.8014425039291382	And so I'm wondering if you have thoughts on this, as you've been thinking about this or talking to anyone about this, how do you think they did this?
5380452	5382734	C	0.697378396987915	Obviously we have to speculate because a lot of the data is proprietary.
5382782	5384530	C	0.8422346115112305	But I'm just curious to get your thoughts?
5385270	5386914	A	0.9500653743743896	Yeah, that's an awesome question.
5387032	5398470	A	0.6404349207878113	I actually forgot to mention this on the presentation that the biases they make on the recent test last year were exactly the ones the humans made.
5398540	5408978	A	0.6010544300079346	So if we fed it some dual process real, it would give exactly our mistakes.
5409154	5410380	A	0.981107771396637	That's so amazing.
5411310	5422800	A	0.7507480382919312	I'm not certain why that happened, but it did and how they fixed this.
5423970	5441794	A	0.7923434972763062	So, yeah, I do think that they trained the model on these tasks because people were able to get similar issues by changing the questions a little bit.
5441912	5453238	A	0.5986488461494446	So I think they cheated on that in the sense that they trained on known problems, but that's not true of new problems.
5453324	5462742	A	0.7871562242507935	If you give it some different formulation and feed it to 3.5 new version, it's still failing.
5462806	5479214	A	0.646677553653717	But the people on Open AI, they are aware of this chain of thought research, they are aware of this tree of thought research, but I don't think they want to depend or rely on these external agents.
5479412	5493634	A	0.8551937937736511	So they recently released a report where they are trying to reinforce intermediate steps of reasoning on GPC four.
5493752	5510658	A	0.7031940221786499	So they try to simulate internally what the chain of thoughts reasoning is doing externally by reinforcing the intermediate steps of reasoning so that it starts failing composition, helping.
5510754	5511640	A	0.5880858302116394	So on.
5514910	5516330	B	0.8442453742027283	A lot of thought on that.
5516400	5526138	B	0.6314446926116943	I was reminded by Daniel Dennett's framework or model in Darwin's dangerous idea of cranes and skyhooks.
5526234	5538114	B	0.7288532257080078	And so some building is built and then the question is like, was it built with cranes or with top down hung skyhooks that just sort of descend from nowhere, hold everything up while it's all being built?
5538152	5540638	B	0.4866569936275482	And it's like, well, no, it's with cranes.
5540734	5544846	B	0.8597323298454285	And then if you need a big crane, you can use a smaller crane to assemble a crane.
5544878	5553570	B	0.7471500635147095	So that's kind of a bottom up constructive metaphor, whereas the sky hook is like the top down compositionality.
5553730	5570490	B	0.8156649470329285	And that's when you get to certain level of sophistication for a cognitive system, it can make the blueprint or the plan like maybe what we would associate with the expected free energy calculation, not just the variational free energy calculation.
5570650	5581280	B	0.8989804983139038	So at some point when a plan can be made, it requires a strategy between type one and type two switching and probably other switches too.
5582610	5589186	B	0.6958400011062622	And so that crane approach, you need a lot of compute to do a bottom up.
5589208	5606694	B	0.5389190316200256	So even though it's kind of weird to think about, it's almost like the current large models are very bottom up because they're very bottom up from syntax and they don't have what you refer to as imperative linguistic knowledge, which can be normative as well as a heuristic for knowledge.
5606742	5621658	B	0.8074836134910583	And that's potentially a small model, but it contains wisdom that's explicit and practices and stances or dispositions as you described.
5621754	5633358	B	0.8321465253829956	And so it's kind of like it's not a denial of embodiment to also clarify what this skyhook ability is.
5633444	5637680	B	0.857585608959198	And that's type two like so I think.
5640050	5640574	B	0.6234597563743591	I guess.
5640612	5641694	B	0.8296127915382385	What do you think about that?
5641732	5645320	B	0.8661275506019592	Or how do you connect this to anything in that area?
5646410	5655480	A	0.7233725786209106	Yeah, I'm going to make you do the summary and the question again, because although I did track some of what you mean, I'm not sure which ones made to comment on that.
5656590	5674110	B	0.8433486223220825	Do you think that the landscape of large and small models would be different if people took on board some of the features you described, like imperative linguistic knowledge, seriously into construction of models?
5675970	5681022	A	0.8158694505691528	So let me just see if this is something you're asking.
5681156	5691650	A	0.4820147156715393	So they're doing small models now for phones and open source models for phones, and they have the issue of not being able to scale up.
5691720	5692340	A	0.7360090017318726	Right?
5692790	5696040	A	0.6674262285232544	Because if you scale up too much, then you can't run on the phone.
5696810	5706338	A	0.8517755270004272	So you're asking if we can implement something like Tree of Thoughts to help these smaller models.
5706434	5707660	A	0.7802931666374207	Is that what you said?
5708030	5708780	B	0.46103888750076294	Yes.
5710110	5714154	A	0.8770807981491089	Are you sure or are you just saying, okay, go for that?
5714272	5718074	B	0.8051424026489258	That's an example of what it would mean to re understand.
5718192	5721018	B	0.8224372267723083	Like where do we need an API call to a cloud center?
5721104	5726798	B	0.8955602645874023	Or where could some context be locally computed with much more of a type One?
5726884	5730974	B	0.7565120458602905	What are some type two problems that we might be able to type one our way out of?
5731092	5740690	B	0.5123838782310486	What are some type one areas where things are not working and a type two description might be advantageous?
5741350	5744774	A	0.7895194888114929	Yeah, I think they're getting onto that.
5744812	5757286	A	0.5665131211280823	And they're being able to make these smaller, smaller models precisely because of these sorts of discoveries they made on Chain of Thoughts.
5757318	5757946	A	0.5664746165275574	Right.
5758128	5762540	A	0.8067774176597595	There's also stuff about being able to train on specific data.
5762910	5764300	A	0.7248346209526062	I read one.
5765070	5765914	A	0.7782580256462097	It's called?
5766032	5767980	A	0.5515372157096863	Textbooks are all you need.
5768510	5774302	A	0.8367578387260437	They're training smaller, smaller models on textbooks, and they're doing better.
5774436	5795238	A	0.6301848292350769	So, yeah, there are differences in training smaller models, different data, but there's also the possibility of making them more reliable based on what I'm calling T Two Agents or Train of Thoughts, stuff like that.
5795324	5816906	A	0.6853588819503784	And again, we don't know if that's all we can do externally, but I would argue that we would need necessarily some sort of external mechanism coupled to any large language model for the best results because of the differences in representations, mainly because of this.
5817088	5831354	A	0.5888424515724182	Unless the large language model somehow makes a symbolic representation emerge from its processes, it's not clear that it has done so.
5831492	5865546	A	0.599044919013977	But unless it does so, and it uses that reliably because it may even have generated some sort of symbolic process internally, but doesn't always use that or doesn't know when to use that, most likely we're going to need something external to the language model, and this is some clues to how to implement that.
5865648	5890270	A	0.7212000489234924	And also, I think, in our brain, it's kind of external not only in the sense of our minds, but also in the brain, since it's a recent step in evolution, we don't see I don't know lizards reasoning in type two manner.
5890350	5890980	A	0.5664746165275574	Right.
5891990	5898390	A	0.6749091148376465	We do see Chintenzees reasoning in type two manner, but not as much as we do.
5898460	5901750	A	0.5490345358848572	So it's very human like reasoning.
5903050	5909210	A	0.673490047454834	It's just that going back to the context problem, animals do solve contextual problems.
5909280	5911242	A	0.5047014355659485	That's why they can live.
5911296	5911706	A	0.5664746165275574	Right.
5911808	5914122	A	0.6507657170295715	So the issue is not actually the context there.
5914176	5922874	A	0.7313351035118103	It's more like some boost on reasoning capacity.
5922922	5930254	A	0.823652446269989	And that boost on reasoning capacity may be related to the prefrontal cortex, as we know.
5930452	5936350	A	0.7425788044929504	And so, of course, the prefrontal cortex is also a network.
5936430	5938958	A	0.5646065473556519	It's not a serial Turing machine.
5939134	5974750	A	0.8265808820724487	But if there's somewhere that's implementing a serial machine or a symbolic classic AI machine or symbolic reasoning or something like that, most likely it would be the prefrontal cortex, which is kind of like a distinct addiction addition to the brain and maybe something related to language, which is also distinct, a new addition to the brain.
5977650	5989780	A	0.6910073161125183	Also, I'm very convinced by the photo and deletion argument in 88 that you're going to need symbolic processing anyway.
5990470	6010070	A	0.4984454810619354	Either the model is going to generate that symbolic processing, or you're going to have to feed it for him, or else you're going to have hallucination of mistakes and reasoning because of the nature of because of the nature of distributed representations.
6010150	6010780	A	0.5664746165275574	Right.
6011550	6014570	A	0.5531166195869446	It's uncertain and ambiguous by nature.
6014990	6025360	A	0.603887140750885	That's why I'm not entirely convinced that only probabilistic representations will be enough to solve hallucination issues.
6029490	6030334	B	0.9184247851371765	Awesome.
6030532	6035380	B	0.9036189913749695	Nick, do you have any other questions or areas you want to mention or ask?
6039830	6041970	C	0.8462600111961365	I think those were my main questions.
6042120	6045686	C	0.7836915850639343	I don't know if there's anything in the Chat that you wanted to touch on before we go.
6045708	6048818	C	0.650926411151886	I know we're maybe over time, I'll.
6048834	6052370	B	0.893367350101471	Go with one from the Chat, and then we can have any closing thoughts.
6052450	6061526	B	0.8735371828079224	So Glia maximalist asks, are you familiar with visual pathways in the human brain?
6061638	6064310	B	0.8799208998680115	Specifically the dorsal where and ventral?
6064390	6071070	B	0.9080166220664978	What pathways do you think this maps onto the two systems you describe for problem solving?
6073330	6074398	A	0.6095885634422302	Okay, yeah.
6074484	6094260	A	0.5666860938072205	So like I said, in 2008, evans tried to accomplish that unifying vision of due process theory, but I'm not very convinced by that.
6094710	6113610	A	0.5616942644119263	He also said, well, we can't do this as of yet, but I do think it's possible, as I said on the presentation, I just don't know how to do it, how to have a convincing theory of everything, of dual process theory in the brain.
6115550	6118390	A	0.8470370769500732	It does go on to other regions.
6118470	6123310	A	0.8360438346862793	Like, you have automatic motor responses and controlled motor responses.
6126210	6136580	A	0.841927170753479	Like you said, there's different pathways in reasoning, in figuring out stuff in the visual field.
6138150	6153590	A	0.9052013754844666	Maybe the what would be related to something and where to another most likely the what would be tapping on to type two processing.
6154170	6167174	A	0.49506744742393494	I'm not sure how the where would be tapping on that, but yeah, as I'm trying to make the point, if we extend it to the whole brain, we lose what we've learned.
6167222	6169530	A	0.5072621703147888	We're not sure how that applies.
6174290	6175040	B	0.84200119972229	Cool.
6175970	6179600	B	0.8566102981567383	Well, Nick, any penultimate last words?
6183800	6184980	A	0.5947045683860779	You muted.
6187640	6188164	A	0.5717682242393494	Oh, wait.
6188202	6189510	B	0.5393100380897522	Unmute and then continue.
6190920	6191908	C	0.6856347918510437	Sorry about that.
6191994	6192884	C	0.618049681186676	No worries.
6193082	6196020	C	0.9265952706336975	Thanks to both of you, Dr.
6196090	6199576	C	0.6733447313308716	Bellini, Lighte and Daniel, for coordinating this.
6199598	6206810	C	0.9756843447685242	And I'm really just glad to have found the research and to discuss it and connect and looking forward to further conversations, to be honest.
6209120	6209870	B	0.9184247851371765	Awesome.
6210400	6212860	B	0.8477172255516052	Sam, any closing thoughts?
6213600	6218780	A	0.8427882194519043	Yeah, I'm just sad I couldn't go much beyond what I said on the presentation.
6220100	6227970	A	0.5325220823287964	Most of the stuff you guys asked, I'm not sure I have good answers for that.
6228340	6242672	A	0.7166363000869751	Basically, what I've done is all I know, and I'm not sure how to go on the future with this, and maybe someone in the audience will do.
6242726	6245850	A	0.86214280128479	So I hope that helps, guys.
6247260	6247864	A	0.7671424746513367	Great.
6247982	6249370	B	0.5735007524490356	I hope so, too.
6249980	6251880	B	0.49069198966026306	All right, till next time.
6252030	6252970	B	0.8529649972915649	Thank you.
6253340	6253700	C	0.5137447118759155	Bye.

start	end	speaker	confidence	text
3540	4090	A	0.62668	You.
6060	26116	B	0.8798993617021278	Hello and welcome. This is act infg guest stream 47.1 on July 10, 2023. We're here with Nick Bird and Samuel Bellini Leitchi and we're going to have a presentation followed by a discussion. So thank you both for joining. And Sam, thanks again for this presentation.
26228	2693220	A	0.8907979192239108	Off to you. You okay? Was Nick going to say something before I started? I can go ahead. Okay, so some introduction first. I'm a professor at Minajara State University. I'm a psychologist philosopher, mainly working in cognitive science. I'm going to talk about what I did in my PhD thesis and also some renewed interest I had on this based on some developments in large language models which happened this year, most notably Chain of Thoughts and Tree of Thoughts. Those additions to large language model made me want to come back to this again and talk about this. So how do we know about human reasoning, which is the subject, the main subject underlying this topic? Well, we had empirical research from the 60s, starting with Peter Watson, who was challenging the notion that we have a logic module for reasoning or some general understanding of logic when we're adults. And he crafted tasks which were hard to solve, but they had actually simple fundamentals to it, which were taken to show people's irrationality, something like that. And also a huge tradition that's called Heuristics and Biases that also started in the mainly by Kanmen and Amosverskin, which had similar tasks and went beyond, and they even won the Nobel Prize for this work. I had here on the slides Evolutionary Psychology and Language and Pragmatics, which were also people in the 90s working with this sort of research. But I'm going to talk about dual process theory in the way they formulated by Evans and Stanovich, which mainly summarizes the results of Watson and common risk biases literature. So I'm going to talk about how dual process theory came to be and then some problems that it has that I tried to address. So one of the first reasoning tests in this trend literature was the Watson selection test, which goes like this please indicate which cards can determine if the following rule is false. If a card has a vowel on one side, then has an even number on the other side. This sounds like a simple exercise. So the card A clearly is relevant because it has a vowel on one side. If it doesn't have an even number on the other, then the statement is false. And everyone can find this neatly of most people. But the problem comes with the card seven, which is not so intuitive. People actually choose the second card two there, which has an even number on one side. But having that even number doesn't help at all to determine if the statement is false. What does help is which is an odd number, if it has a file the other side, then the statement could be. So this was one of the tasks that famous for and conman and first key show them this one they're famous task as well solved the conjunction fallacy the issue that people arise when solving this task goes like this it is a 31 years old, single, outspoken and very bright. She majored in philosophy as a student. She was deeply concerned with issues of discrimination and social justice and also participated in anti nuclear demonstrations. Please make the following statements by their probability using one for the most probable and eight for the least probable. What's interesting here is the statement, the last statement. Linda is a bank teller and isn't active active in the feminist movement. Because there's also these statements without one another, without the conjecture which is Linda is active in the feminist movement and Linda is a bank teller. Because of the description, people think that Linda is a feminist is very likely given the description, but they think being a bank teller is not likely. The issue is that people usually mark Linda is a bank teller and is a feminist as more likely than being a bank teller. The problem here is that the conjunction of the two necessarily is less likely. So being a feminist plus being something else or being a bank teller plus being something else is automatically less likely. So those are the sort of tasks that the reasoning literature used to show some sort of human irrationality. One issue with this is that it's not very interesting to show that humans are irrational, it's more interesting to show how humans reason. And also these sort of studies didn't show how people actually solved the problems. They were actually focused on how people made mistakes. One thing that's interesting about dual process theory is that it explains both the mistakes and the solutions to the answers. So in 2002, Conan and Frederick formulated this cognitive reflection test which is another one of these tasks but a lot much simpler and elegant. And it really shows indicates at least how dual process theory is a good solution to a good explanation of the evidence in general. So yeah, the western selection test used logic and the Linda task used probability. And here we just have basic arithmetic. So as if math we don't have to worry about which answer is truly correct or which rational norms we need to follow. It's just basic math. There's nothing to discuss here. Right? So the task goes like this a bat and ball costs one in ten in total. You might know this test, it's a famous test even out of the reasoning literature, the bat costs one more than ball. How much does the ball cost? So people when when people read this, they usually say $0.10 intuitively. But if you take a little time to write write it down, you can see that it's actually $0.05. So the bat cost one and ball costs $0.05. Together it costs one and ten. The other two have a similar structure. The point here is that sometimes we look at these tests and we have an intuitive answer which is wrong. And the way we get out of these issues is that we stop and think a little bit and review our answers with some sort of step by step reasoning or writing it down, something like that. What's interesting here is if you look at two plus two, it's almost impossible not to think four. It's almost like you're perceiving. The answer is four just by looking at it. But if you have some sort of more complex calculation to do, obviously just by looking at it, you won't have a solution. So this basic intuition is clearly shown in the cognitive reflection test. And what's more interesting about it is that the cognitive reflection test actually predicts the performance of various Aristocrats and Biases tests. So if someone does well on cognitive reflection test, they're more likely to go well on any other reasoning task. And if they go bad on the cognitive reflection test, they likely will go bad on other reasoning tests as well. To me, this is also an indication that this dual process structure of intuitive reasoning versus slow reasoning, reflective reasoning, likely explains the rest of the text as well. Not only these two, but most of the others. So it's a good explanation of the literature. Okay, so what does the theory say besides besides explaining the reasoning literature? So, in 2008, Jonathan Evans tried to make a formulation of it that would include and unify views from different domains such as social cognition, neurosciences, other people in psychologists which had similar theories. And it seems like there's something in common here to unify, but Evans wasn't able to. And so there's a formulation specific for reasoning and we don't know if it applies so much well to other areas of cognition. One common way to describe dual process theory is by making these tables of features. So we say, oh, type one processes have these sorts of features on the left, and type two processes have this other type of features on the right. This table here was one I made by joining some of the features of Evans insteadovich, and also of Kunman, and then with some questions I raised in the thesis, I elaborated it this way, but it's very similar to the Yavin Instant formulation. One problem we have with this tables formulation of the Duo Process Theory is that usually people agree that in one thinking instance, when you're solving a problem, you're not going to have all of those features together. So we don't know for sure when some feature will be present or not, for instance. So clear that type one processes necessarily have to be unconscious and type two processes necessarily have to be conscious. I think that comes a lot because of some conceptual issues in the theory. But one solution they have found was to have at least some defining features but different authors have different defining features. So here I have some of the defining features of the main authors, which is type one processes use less working memory while type two processes most strongly on working memory. Type one processes aren't autonomous in the sense that they don't need to wait for higher processing to respond. If they have an answer, they go for it. And using the couple's representations mean that means that type two systems can reason beyond what's in the present environment can think about things that are not there. Type one processing is faster in comparison while two is slower in comparison. Type one processing is less effortful while two is more effortful in the sense of psychological effort. Like people report having a hard time doing things. So for instance in the cognitive reflection test, people that solve the test correctly usually rate it and it's harder because they had to use tattoo processing to solve it. Okay, but Samuels, which is a philosopher, dual process theory and other subjects, he says okay, even if these features proposal is good, we still need a mechanism to explain why these features are linked together. So what is the mechanism that explains why type one processing features go together and what is the mechanism that explains why type two processing goes together? So we need some beyond saying that they're different, we need to explain what are these mechanisms that make them different? And I actually had Samuels on my PhD committee and I'm not sure if he was convinced with my solution. So my PhD was a solution to this problem. I'm not sure how he was convinced of it, but he approved the PhD, so that's good enough, I guess. Yeah. So the goal was to solve the unity problem, was to explain the mechanisms that explain type one processing and explain the mechanisms that explain type two processing. And I started to see, to search in the literature first, how people were conceiving of the general frameworks that were behind these two types of reasoning and one that was not actually related to this literature of dual process theory but was actually famous in cognitive science, was the modularity of mind in the 80s proposed by philosopher Jared Schroder. And he said that we had two types of processes, a modular process mostly related to input and perception and we had central processes that was mostly related to higher reasoning. So you can trace some sort of resemblance there to do process theory. And the way he went about this is that he said these perceptual models, modular processes are information encapsulated. So vision only works with vision and language only works with language, space only works with space. And so these were domain specific knowledge systems that did not communicate with each other and when they needed contact sensitivity, they needed central processes. So these central processes he would argue, were Isotropic and Quinine, which means that any type of information could be used in a solution so we can relate, I don't know, physics with biology and form a new solution. So there's knowledge from multiple domains working here that would also need some sort of context sensitivity, which was a problem for photo. He didn't think we would solve this context sensitivity problem at all and he had good reasons for that. In classic AI. We had a lot of issues with that. I mean, we still do have issues with the context sensitivity problem, but not the sort of issues we had in the 80s when they were debating this same problem of classical AI. So this problem was a problem of how to represent a changing environment in symbolic representation. So yeah, it was a problem because classical AI attempted to give knowledge to robots or artificial intelligence and statements about the world and they were intractable, hard to compute, there are so many knowledge banks to search and so many knowledge to try to use that the robots couldn't do anything relevant. They also couldn't represent change. So the changing environment was terrible for them, didn't work with this symbolic representation, it didn't work with serial reasoning or anything like of the sort. This was a huge problem for classic cognitive scientists. And since photo was a believer in cognitive science in the sense of a classical computational theory of mind or language of thought theory, he said that he had found Photo's first law of the nonexistence of cognitive science. This law states that the more processes are global, the less cognitive science will be able to understand them. So he thought only these encapsulated processes could be studied. And if we needed this sort of global and context sensitive type of processing, then contest science just couldn't do it because he had in mind contra science in the sense of classical AI. But it also seems not only folder, it seems that the literature abandoned classical cognitive science in various ways, which were strange because if you take the work of Alan Newell and Herbert Simon, it's strange to think that oh, they were completely wrong. There's nothing, nothing useful that they say that could be applied today. All the evidence, all the theories they made are just completely wrong. That's a weird way of thinking. And well, if you take for instance, our theories today of the human Bayesian brain, they don't consider this at all. It's just like they don't care. It's a different strand of theories. But it also seemed to me that there must be at least something wrong about, something right about this shouldn't be totally abandoned. There must be a place for these results. Explanation of the Mind one thing that was in odds with the way Photer described his dual process theory, and also sometimes Stan of a trend talks about dual process theory. It seems like forgetting this fact here is the fact that we have a limited capacity when we use conscious processing or working memory. And this is one of the best evidence in psychology we have so far. We had behaviorist evidence for reinforcement, enforcement, learning and that stuff and then we had cognitive psychology in the this is the best evidence that cognitive psychology has had for a long time. So it's also something we should not simply ignore. So we have in short term memory the knowledge that when we store some information in short term memory, we usually can only keep track of about seven items, sometimes even less in selective attention. There was this task where there was a video of people playing basketball and the subjects had to count how many passes they were making and suddenly a gorilla passed by on the scene and people didn't notice the gorilla because they were so concentrated on the task, which is some evidence of selective attention. That is, when you're concentrating on tasks, you actually don't even see other stimuli. We have also a competition for limited resources when doing dual tasks, or if I'm talking on the phone and writing at the same time, I'm probably not going to do a very good writing because we're competing for limited resources. And it's important to note that if you're walking and writing or walking and talking, since those are different tasks, they won't compete. So we have some competition. We have more competition when these limited resources are disputed also in working memory. So if I say the number 793241 and I ask you to repeat them backwards and you're not looking at the slides, you're going to have a hard time with that. Because of the limited capacity of working memory, when it's clear that in other areas of our thinking of our brain is using a lot more information than seven items to make the scenes happen, perception and to make us walk and so on. So that's on the side of the type two constraints, on the side of type one reasoning. Our Audi has some intuitions similar to the ones I described. So he said from its earlier days. The research that RSD and I conducted was guided by the idea that intuitive judgments acquired position between the automatic operations of perception and the deliberate operations of reasoning. Communric claimed that intuitive thinking is perception like and that intuitive predictions is an operation of system one further that the boundary between perception and judgment is fuzzy and permeable. The perception of a stranger as menacing is inseparable from a prediction of future harm. So you can see that this is very likely, very similar, sorry, to the intuitions that we were speaking on when we're talking about predictive processing. So feminine saying when you look at a face of someone and he's mad, you have a prediction that you're in trouble, right? So this is a sort of judgment, that same sort of perception, and also the context sensitivity is pretty much solved in predictive processing. And if you take on the AI literature that also happens. For instance, paper attention is all you need. They have some tips for how to deal with context sensitivity, which photo didn't dream of. So this is something that Clark said in 2013. The best overall fit between driving signal and expectations will often be found by, in effect, inferring noise, in the driving signal and thus recognizing a stimulus as, for example, the letter M say in the context of the word. Mother, even though the same bare stimulus presented out of context or in most other contexts, would have been a better fit with the letter N. A unit normally responsive to the letter M might, under such circumstances, be successfully driven by an N like stimulus. So what is Clark is trying to say here that since we represent using probability, then if the context says we should go left, we go left. If the context says we should go right, we go right, because the representation is fluid and it accepts changes really quickly. So that's something that stems from the predictive processing architecture. But one issue I think that we might see in the predictive processing architecture is the lack of compositionality. So symbolic representations exhibit compositionalities meaning that complex representations are built by combining simpler elements according to rules or syntax. The meaning of a complex representation is derived from the meanings of its constituent parts in the way they are combined. This property allows for the generation of new and meaningful expressions by manipulating symbolic structures. So Folder in 88 argues that even if connectionist models succeed, they would likely have some sort of simulation of a language of thoughts that exhibits conventionality in order to work like we do. And we'll see that it seemed like it's the case with the results we have this year. So considering those constraints and the initial field footer and the problem I was trying to solve, which is the unity problem for blue process theory, a major shift I had on the PhD thesis. And then I wrote a chapter on this book. The Chan Book of Founded Rationality with Keith. So we explain here in this book the chapter The Shift Power Structure, which is, I mean, it's not clear that a dual process theory theorists would adopt Folder, but it's still very similar. The way the received view of dual process theory works, which is type one, is a number of dumb heuristics blind modules and animal intelligence. And then type two is the real contextual and complex reasoning, the real human intelligence. So I tried to invert that power structure by saying that the contextual, predictive processing is a type one reasoning, which is giving us the basics to reason on. So our reasoning already comes with context. We don't need to reason the context with type two reasoning. We only use type two reasoning to fix some minor issues in predictive processing. And we use heuristic search because it's limited. We only use it when there's a lot of prediction error and predictions aren't working that well. And then we call for this heuristic search to handle whatever is missing to see if we can find a new solution that wasn't captured. So in this case the powerful system is the predictive processing and not the Type Two research. So that was the general idea for this shift. Now I'm going to say some hypotheses that stem from this shift. So t one processes deal with content encoded in the form of probability density functions, which is how predictive processing uses to represent the world, which means there is no symbol and no definite content but values, means, standard deviation. Influenced by previous movements in previous world contingencies, manipulating prior information, biases the distribution to one or another direction closer to or further from a certain value. So people here in the active entrance institute we know this by heart. I'm not changing anything here, I'm just using what they say. Basically these functions are not stored in a memory bank but distributed from the responsible brain regions over to the external organs, body parts through neural connections. The values in the distribution do not represent objects directly indiscreetly they refer to distinct aspects of the input when perceptual systems are dealing with such objects this is aligned with the T one processes being easily biased when working with references to similar properties like similar numbers, objects, rhymes or pet names very often an incorrect value is picked from a distribution. An example that I have from this is when usually I have seen people confuse their youngest child with their dog name youngest child and when someone has another new child then the new youngest child is often confused with the dog because they are, I think, somewhat represented in similar distribution similar points of the distribution. This is also in line with claimants of embodied proposals that the world is not representative in symbols. T one processes are subpersonal and their predictions are made by the same systems which process perception. A clear example is that a judgment about a facial expression is related to the FFA which is a region brain that processes faces and this is also related to that judgment that conman may argue that when looking at someone we make judgments about their face. The idea is that perception is not passive but already comes with predictions and when in problem solving such prediction is precisely the Type One answer. So if we look back at the reasoning test all those Type One tests are likely stemming from predictions. I don't know what to claim that T one processes are purely perceptual if in contest to cognitive only that such predictions reasoning is that there's not a clear line between what is perception and cognitive. So I think the word you're just using different words for this is the problem. So just not getting into that convenience. Example of judgment of negative spatial expression shows how this is expected of dual process theory like it's expected of dual process theory that Type One reason works under predictive processing, also in line with the claims of embody cognition that there is no sharp link between perception and reasoning. On the other hand, c two processing works like a classical machine for reasoning, such as General Problem Solver of Newman Simon. However, this classical machine only makes sense in the brain if it exists in a wider setup. Predictive processing network generating Type One responses like in Newell's Physical Symbol system, when facing a reasoning problem, T Two Processing opens a problem space containing an expression that designates the initial problem and an expression designates a solution which was produced by a probabilistic prediction having the initial expression and the predictive expression in the problem space. T Two Processing then uses its move generators to attempt to reduce differences between them and sometimes find different solutions in such bad. So let me just explain this a little bit better. When Newell was working in the 60s there they knew they couldn't search all possible space of a problem space because it wouldn't work. So they have this library search which works like a detective, let's ignore the problems which are likely wrong, just look at the spaces that might be right. What this predicting and reflecting framework does for a risk search is that the risk of search only works on boards already with the prediction that was sent from Type One processing. So it's like we only start reasoning and making searches based on the prior predictions we already had. And we only go further with this search when the predictions aren't working. And the main point here is that the risk search isn't searching a random space or even a pre programmed space. It's searching a space that was left out from the predictions that started in Type One processing. So that's how it doesn't go to the frame problem anymore because a large part of the contextual issue was also already solved by predictive processes. Okay, so that was the general hypothesis I presented to attempt to solve the unity problem, which is the problem of how Type One features go together and how Type Two features go together. And now I'm going to try to argue that this is likely the case, since that I can't prove this is the case. But there are good reasons for us to think that Type One processes use predictive processing and that Type Two processes do not use predictive processing. Rather they need some kind of symbolic heuristic search. One thing that stems out of is a good explanation for the difference between implicit representation and explicit representation. If you look at the literature in psychology, you will see that it's very ambiguous in these definitions. Like people use implicit and explicit representations as the same thing as unconscious, conscious or fleeting graspable, or the same thing as automatic in control. And so when it goes to dual process theory, it's not very clear what they're saying in each case, but they use this expression and explicit a lot. How I think this model solves this is by saying that implicit representations are probability density functions while explicit representations are symbolic representations. Classical symbolic symbolic representation. Clark in his book sometimes mentions that we can have single peak distribution distributions. But the problem with having single peak distributions is that well, if you only have one peak on the distribution function then it doesn't have all these features of probabilistic representation. It's more likely similar to a symbolic representation. The fact that having a single peak distribution might be the way that a probability representation turns to a symbolic representation. So these have different and important features. The probability representation is continuous, it's uncertain and ambiguous and that's why it's able to be sensitive to context. The mother example of part it can vary on retrieval. We can remember something and be different the second time. It's tied to priors. It works with statistical relations. Yeah, it's called in and depending on which value it's most probable at the time, it has a different outcome. So I argue that explicit representations are likely discrete, they are likely symbolic in nature, they're unambiguous. This is what allows us to disabilite in the first place they're stored reliably. If you have a reliable definition of it, you will likely remember it the same way the next time. It's arbitrary in the sense that it's not really coupled to the stimuli, in the sense that the perception of perception usually is. The statistical relations are related to aspects of the world. It's using compositionality and it's immutable in the sense that you can't change it. It's fixed value doesn't change. So that's the implicit versus explicit feature. You can have also explanations for automaticity in contrast to working memory. So automaticity concerns overlearn skills and overlearn skills are understood as skills that have become predictable in this framework. So we usually use this to explain like driving, riding a bicycle, riding. So when you're driving, if you don't know how to drive, you need certain statements, like some statements about how you, how you should steer, right? You need to have those in mind. But then once you learn how to drive, it's like your body already knows what it's doing and you don't even need to think about this. So this is the classical distinction in psychology. But I argue that the predictive processing theory makes a new enlightenment to this. So it's like our body is predicting what we're doing. So it's automatic. So it's an explanation for optimisticity. But say that a dog certainly suddenly runs from his car. You obviously don't have a model for that. So you're going to need to call in work in memory to solve some of those issues so you don't hit the dog. So everything works under prediction unless the model is very unreliable and then working memory is called in to solve some further issues.
2694550	2694914	C	0.53436	An.
2694952	3929820	A	0.8946321446078456	Interesting hypothesis I had on the PhD thesis which I never explored. I actually don't even think anyone read this part, but it seems to me to be a very interesting hypothesis in relation to free energy in active inference. So that's why I'm bringing it here in the Active Inference Institute that as we know, the brain attempts to minimize free energy by getting predictions right. Thus higher need of type two processes are related to higher free energy interpreting information in the sense that when the predictions are working clearly, then you don't need to have effort. But when there are a lot of prediction error, then you need effort because you call in the working memory to do a risk search. So to minimize a great amount, better amount of free energy will take more time and work. This is more effortful than having predictions ready that minimize free energy as quickly as possible. Before, when probabilities fail, the system needs to start risking possibilities. It searches for other possible solutions by means of heuristic search. Heuristic search would be related to more information and time because it does not have probable solutions ready. Instead it needs to investigate its state space almost from scratch. We say almost because it is heuristic. And hence it also will have tricks to get the correct solution faster. Unlike group force search, which would investigate the state space from beginning to end, we believe reports of effort would be related to executing more heuristic searches. Reports of effort by subjects would seem to be based on cognitive informational and physical constraints of reality. So that's why I think this is an interesting part of the hypothesis is because we have this psychological measure of effort. But if this is true, then we actually have a physical measure of effort. Like if people are working hard on the problem, that means there's a lot of free energy going on and they are having to use this the risk of search to fix some issues that the model isn't able to fix by itself. Yeah, so that's that's the effort part. So now going for the working memory part, what's interesting here is that in working memory, working memory, it's expected of it that it works like a classical computer. So working memory is a widely research topic in psychology, but in predictive processing it's rarely, rarely mentioned. And I actually searched the predictive processing books, control F for working. And people rarely use this topic because it's not done by predictive processing. It has nothing to do with predictive processing. It's actually an issue for predictive processing because well, if you're going to eliminate classical symbolic processing altogether, then we need a predictive processing explanation of working memory. And so far I haven't found one and I don't think there will be one. But this also could be just ignorance of mine. Maybe there's some paper there which I haven't found might prove me wrong. But in any way, working memory is very aligned with what Turing was thinking when he was making the connection between machines and minds. So if this is a sentence of Alan Turing explaining his computer, right, the Turing machine. And if we change the word computer to work in memory, it clearly works. It's almost like it's talking about the same thing, when of course, if we change the word here, computer, to predictive processing, it's clearly not talking about the same thing. So the behavior of the computer or working memory at any moment is determined by the symbols which it's observing in his state of mind at that moment. We may suppose that there is a bound b to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which can be taken into account is finite. The reason for this are the same characters as those which restrict the numbers of symbols. If we admitted an infinity of states of mind, some of them would be arbitrarily close and would be confused. Again, the restriction is not one which is seriously affect computation since the use of more complicated states of mind can be avoided by writing more symbols on the tape. So this is very similar to what we think about when we're talking about working memory, not only because we were instance by Turing, but because of the evidence that comes from working memory is very similar to what suspected of this sort of machine. Finally, we have speed. So predictive processing has various strategies to make the processing be faster as fast as possible. While that's not true for symbolic classical AI, so yeah, Clark says cheap, fast world exploiting action rather than the pursuit of truth optimality and deductive inference is now the key organizing principle. When he's talking about predictive processing, the predictive processor is always taking certain bets about what the current state of the world implies losing accuracy, compensation for speed. We also have predictive coding in the more strict sense. So by predictive coding we mean specifically the property of the system to consider from the world only stimulant which result in greater prediction error. So there's also a filter there which allows for speed in perception and focus. Beyond predictive prediction relevant stimulant only permits the agent to quickly decide forces of action and select amongst the possible forces. So predictive processing is tailored to be fast, necessarily. Well, that's not the case for symbolic AI and so that's the explanation for the difference in speed. So beyond that, the T two processes need to figure out the solutions online. So it's different if you have a prior prediction that would say to you what the answer is from having to search its place from scratch. Right. There is also another issue that the biological brains certainly were not built for. A serial heuristic search photo pilotians say that the morals that the absolute speed of a process is a property part excellence of its implementation. And since the brain does not have a symbolic processor implemented, if we do use some sort of heuristic search, it's like a different adaptation. It's not what the brain is used to. Inserting problem spaces is slower than having a problem outcome ready. Okay, so that was the work I did in my PhD thesis. And those one, those were one of the main reasons why I think this framework is good for explaining the differences in features of dual process theory. And I actually wrote a prediction that I recently learned that I wrote this because I, I forgot I wrote this. And then I went back to to the thesis and and I said, well, I actually said this would happen and it kind of did. So that was one of the reasons why I renewed my interest in my PhD thesis. So what I said was predictive processing system would be subject to bias from lack of compositionality such as mistakes in connectivity, failures in noticing necessary character, formal rules and so on. Precisely the type of mistake, type one processing incurs. And so at that point we didn't have large language models. In fact, the attention is all unique. Paper came out in the same year. And then I noticed that this could be seen as a prediction of the type of issues that large language models are facing, that's reliability. Hard time keeping the order of units or steps in complex reasoning or math straight, hard time letting go of priors. And to my what really motivated me to start working on this scan was that they solved this precisely by using dual process theory and t two agents they're calling. So what they're doing now to solve these reliability issues is adding a heuristic search to the genetic model and it's having awesome results. This is just this year 2023. So the more they develop these type two agents to answer these reliability problems of large language models, the more the large language models are getting good at stuff. So this is a paper, I got the images from the paper tree of Socks, right, which is one of the last ones, probably came out about two months ago, something like that. The way they're doing this implementation is they're taking the results outputs of the large language model and they're doing further reasoning on it and then feeding the large language model with the result of the further reasoning. The further reasoning they're doing is very similar to the classic symbolic AI. So here they explain there's a simple input output prompting here, which is simply inputting a prompt and the large language model will give you an output. And as we know, these outputs sometimes are very biased. There's hallucination on it, they're not reliable, all that stuff. And last year they figured out that through chain of thought prompting we could also already increase the reliability of these models. So you give an input and just as we do, the better prompting when we go to Chattbc and say, I'll do this better fix this issue, solve this step by step, some of the commands that are good for Chatter, PT and Chain of thought prompting automated some of that, some of those reasoning steps. And then large language models were working much better after this. So this was last year. And then last year this started a new trend of research to make these agents coupled to large language model output linguistic outputs back into the large language models to see how they can get better. The self consistency one, you give the input and then there are various solutions. The self consistency forces the large language model to try different solutions and then you get a majority vote. So if you have like five similar answers, some different paths, then this one SPicked over the one which is less popular. And the Tree of Thoughts, which I thought was most similar to the proposal I had in psychology, in Duprose theory, is the one which opens up a problem space. It's actually pretty much what I just explained. In the case of psychology, they open up a problem space and they start to search possible solutions which are better than the original and they feed that back into the predictive processing, sorry, the generative model, the large language model. And then this one is currently one of the best ones they have. So here I made a general diagram of how this would work. It's not psychology nor AI. It's like what's singular in the two? So we have a generative model here using probability density functions. This looks more like a traditional cognitive cognitive connectionist model than a predictive processing. It's just the diagram is not that good. I'll talk a little about that. We generate an answer here, which is to the problem here, and we get a linguistic output. And this linguistic output then goes for an heuristic search using symbolic representations. So if you get a better answer here, feeds it back to the Genitive model and so on. Basically, the knowledge part is the Genitive model and the heuristic search is simply using more steps to see to prompt it back. It's more like a prompting scheme than a new knowledge scheme, which is both true for the dual process theory model I created and the heuristic search that's happening here on the T, two agents for LLMs. A better way to do this, and I think Tree of Thoughts does implementate this, is that the input layers are trying to predict the thoughts. It's a bit more like predictive process than the other diagram. And so they're trying to predict the thoughts and by open up this searching this problem space during these steps. We already have new prompts here for the Gentrific model. So if you take predictive processing seriously, and this does happen in the brain, more likely like this. I do think that the Tree of Thoughts paper does say that the intermediary processes interfere in the searching on the aliens. Finally, I thought maybe we could suggest something from psychology for these models to keep growing, working differently based on what we know from psychology. So some things that are not implemented yet is that executive functioning have these updating abilities in intelligent forgetting, which is related to insight problem solving. It's when you forget something, but it means, like, you forget the prior, right? You're using a different prior, so you're able to let go of biases. Right. There's also work on thinking dispositions, which could be relevant. Sentences like this belief should always be revised in response to new information of our evidence. So these would be, like, imperative linguistic knowledge, which would have to be added on. I don't know where, but likely would help work. There's a lot of thinking dispositions that are relevant to our successful reasoning. And this is just one example. The literature knows a lot more examples that could be relevant. We have in creativity research, we have this generative phase, which is similar to what the generative models are doing, but we do not have the exploratory processes going on, which is common in the creativity research literature. But it's what we do with, for instance, mid journey when we're doing better prompts. So maybe using these exploratory processes from the creativity research, we can also automate the exploratory part of the creativity processes that these image generators are getting to, and not just the generative part. And finally, I think the embodiment, you don't have robots or anything like that that do this sort of reasoning. So AI on the computer and also stuff from active entrance, which was not considered slightly related to embodiments in the sense of navigating the world and predicting the world. That stuff is far from happening in the traditional Galileans we have. So, yeah, that's a lot of stuff. Sand, maybe I hope you guys have some comments. As I said, I haven't been able to talk about this to anyone, and I do think I have some, at least some maybe some relevant stuff that I've published. But since I'm not famous and I don't have people to talk to, it's mostly gone unnoticed. So thanks for being stay tuned.
3932980	3940790	B	0.9232034782608697	Thank you. Awesome presentation. All right, Nick. It'd be awesome to hear your introduction and then take it wherever you'd like to go.
3942200	4252100	C	0.949647719688546	Sure. So first, thanks, Samuel, for interesting talk. And I've been really pleased to find your research. I guess maybe I'm supposed to say something about who I am, where I am. So I'm Nick Bird. I'm at the Stevens Institute of Technology in the New York City metropolitan area in a department that's kind of interdisciplinary. So we have, like, philosophy and quantitative social science, but also, like, people doing, like, music and visual arts and all sorts of other things. I tend to do more of the philosophy and quantitative social science stuff. So I'm kind of also interested in human reasoning. And so much of the people I'm reading and citing and drawing on in my research are a lot of the people that you saw in the opening slides that we saw people like was on and Conneman Tversky and Mercier and Sperber and a lot of the other people that were cited. I think Samuel does a really good job to incorporate the kind of like shoulders of giants that we're standing on in terms of the philosophy of mind, right. The voters and the Pilitians and these people. And one thing that I think is on everybody's mind these days is these large language models. So I'm really glad to see how this dual process theory that emerged from research like economan and diversky's on these problems like the conjunction fallacy problems or the waste on selection guard task or the conjunction fallacy tasks, how that has been applied to models like large language models and what we can learn from that. And then I think the coolest thing about the preprint print is how we could kind of port back and forth the learnings from both the computer science and the cognitive science to improve one another, right? So I think that one of the neat things about this preprint that Samuel was giving us at the end is that we're kind of taking what we think is a model of reflective reasoning that we represent certain parts of a problem and maybe reason more carefully and effortfully about them in certain situations in ways that might help us. And how can we help these large language models do the same? Because it does seem like large language models function, at least the ones that we interact with online. Mostly. They seem to function mostly as like a system one or type one process. They're just kind of like quickly generating lots of text or imagery or something like that. But they're not necessarily reflecting on it in the ways that we would think that a human is capable of and they might not even be capable of representing things in the same way that we do. Yeah, so I thought that was a really interesting idea. Using chain of thought and tree of thought models to kind of create a reflective level or reflective system within the models seems just like a really valuable idea and just a really great synthesis of research in multiple disciplines, something that I think few people are actually very good at. It's like incorporating some of the best insights from multiple fields. We're often pretty siloed in academia. So I just think this is great work and I hope more people will appreciate and pay attention to it and build on it. So one of the things that stood out to me in this presentation more so than when I was, like, reading this 2023 preprint analytic Reasoning for Large language models by Dr. Bellini Leiche. The thing that stood out to me is this idea that Bellini Leiche and Frankish sort of switched which of these two types of reasonings processes are supposed to be context dependent or context sensitive or however you would want to word that. And I started thinking, well, I wonder if there's a sense in which both types of responses or both types of reasoning are somewhat context dependent and I'll just say what I mean. And then maybe, Samuel, you can kind of say what you think I should think or clarify the view of the predictive and reflective framework or something. So the thought I was having was, well, in a familiar context, these intuitive predictions, these type one processes, our gut response, so to speak, those are going to be pretty useful because they're well trained. We have a lot of experience that we're drawing on. So like our gut response, our first response is often quite good. It's in these less familiar contexts, or maybe similarly familiar, but, like, way higher stakes or something. But it's in these other contexts where we might think, maybe I should slow down and make sure I've double checked whatever my initial impulse is before I just accept it because there's a lot riding on this. Or, like, I'm just not used to this type of problem, so I need to slow down. And so there's a sense in which what context is doing is not just showing up in one or the other type of reasoning, but it's sort of like determining which type of reasoning might be best at the moment. But I'm wondering, is that just totally compatible with what you're imagining or is that somehow a deviation from the framework, the predictive and reflective framework?
4252940	4386930	A	0.8995870000000001	Yeah, thanks for the thoughts. There compliments and thanks for the question as well. So the reason why I think context needs always some sort of type one help is because of the constraints we have learned from symbolic AI. So if I'm saying that type two reasoning have the same constraints, then it cannot be contextual because it doesn't work like that. We know that if it fails in context, it fails bad. But of course we do need to like when we have a novel situation, we often solve this by context, right? So how could this be? And I think it's by the interconnection of the power of the two system. So we always start with a prediction, which in this case would be mistaken, and then we have to search for novel answers. And in this search we make the contextual comparisons. So it can't be that the system too is making the comparison on its own. I think it's more likely that when it's a case of context that stems from a novel situation and then the two systems have to figure out it together. Likely in the sense that I was pointing out here in the end, these predictions that go over the thoughts, they most likely are the ones that will solve these context issues in novel problems, I'd argue something like that.
4392280	4432928	C	0.9617140449438203	Okay, I think that's somewhat helpful. So then maybe what I'm thinking is something along what the lines you were saying at the end. You were saying how there's still kind of an opportunity to understand things like executive function in this framework. And I think that's maybe part of what I'm wondering about. So I guess I'm wondering if there's more to be said within this predictive and reflecting framework about how each of these two types of processes get selected or, like, what might help the system.
4433014	4433984	A	0.774905	Hold on.
4434182	4434624	C	0.924175	Go ahead.
4434662	4521630	A	0.9022285393258423	That specific point you just remember, I just remembered something that's related. So, yeah, clearly our working memory or type two reasoning likely does more than that. It does this, but likely more than that. There's likely also a belief bank, something like the thinking dispositions, some sort of knowledge bank, which is related to type two processing. So Keith Franklin does this distinction between flat out beliefs and I'm not remembering terms, but he has a type one belief and type two belief. Type one belief is that belief you have, but you're not fully confident of it. And the type two belief is the belief you have when you have a very firm political position and you state it with obvious words that you sure you believe that. So there's likely to be some sort of different belief structure as well, which I don't talk about at all thesis of the work. So I'm not saying, obviously that this is a complete model of reasoning, and obviously we'll need more to figure out how reasoning works.
4526530	4533700	C	0.9441526666666665	Yeah, okay. That's also helpful. I'm wondering if Daniel, did you want to weigh in?
4535030	4573440	B	0.9445499999999998	Well, I think it's a very interesting question about which of those modes are engaged or how they're balanced through time. And how does this connect to what's known as the context window in today's transformer type large language models. So how do you map the computational attributes of large language models today, like architecturally or their Ram or CPU usage? How do you map that onto, for example, human cognitive processes? Do you think that's useful or there are any insights there?
4575090	4633940	A	0.8923295098039213	Well, I'm mostly making a relation between predictive processing and generative models, but I know that large language models are not entirely the same as what active inference is saying. I'm aware of that, but there is some similarity specifically in regards to the generative model part. So I'm not sure I'm not an AI specialist to be sure which details of a generative model could be true of the human brain. And I'm most likely betting that the people who study generative models in the brain, like the Frisons and all, have figured out something like what our generative models do.
4641270	4763610	B	0.9179531645569614	One other thought or nick, you want to add there just thought of some different ways whether people have connected it explicitly to the literature on working memory in predictive processing. I think, as you pointed out, it's definitely a link that is not highlighted. It's one that we recently heard from Professors Walker and Monriquez and Pristine in the recent Livestream 53 that was on like, cognitive paleoanthropology looking at human working memory. But some ways that people have incorporated working memory is like a nested model that carries context at a deeper time, but that is not necessarily like an actual mechanism that gives rise to why type one and type two are the way they are. So I thought that was a very provocative direction to go to move past the descriptive and then to look at the underlying generative model of why these outcomes are the way they are. Even though these cognitive phenomena are causes of other things happening, they are also caused by some influenceable or contextual or variable aspects. And when we lose that context dependence of the cognitive systems, then they're totally lifted and disembodied don't really help. So by putting the primacy on that predictive processing element, I think it opens the door to connecting those areas beyond just mentioning the terms in proximity, but to really support different epistemologies from predictive processing as a model or approach.
4766430	4775406	A	0.9678933333333333	Yeah. Can you go on that idea again? I'm not sure what you want me to comment. Can you summarize that again and make it a question?
4775588	4790210	B	0.9021268965517242	Sure. What do you think the epistemological consequences are of taking predictive processing the way that you have approached it here versus alternative or prior approaches to cognitive sciences?
4792070	4795110	A	0.9633687500000001	Okay, but what do you mean by epistemological.
4798490	4799430	B	0.31485	Normative?
4800170	4801160	A	0.8418633333333333	How we know.
4803610	4810250	B	0.9339464705882353	How it influences our understanding of how we know or seek or practice or do or decide.
4812350	4985780	A	0.9030324918032793	Perfect. Okay, let me think about that, for instance. Yeah, I think one of the best what this helps with most is explaining dual process theory because dual process theory is in bad shape in terms of concepts, in terms of theory and formulation. People have no idea what these systems refer to in the mind brain. They often have different intuitions into this. And basically what I most think this work is relevant for is for saving dual process theory from criticism, which has been happening in psychology. So people are like, it seems like we're talking about the left and right brain, something like that, something that we don't know for sure what these two systems are. Right. So this is a good way forward, I think, for dual process theory. And as I said, dual process theory does have some value in itself. So, yeah, saving the process theory, I think is a good direction for this model. But also if we think about the advantages of it would be something like, for instance, it could have helped us had the idea of using Chain of Thoughts or Tree of thoughts. Had anyone read this before. So it can help us make these sorts of predictions that we shouldn't have some sort of zero reasoning coupled to the gentle models. So yeah, I think it could help us think further on on AI, if we AI, like Nick was saying, to incorporating more stuff in psychology that we know that's certainly missing here, but here we have a direction of what we should incorporate and so on. But I'm also unaware of the working memory research from predictive processing you mentioned. And if you want to go back to that and explain me some more of that, I'd be happy to hear.
4988950	5079790	B	0.9265066666666665	Yeah, sure. So not that this is the most effective way to implement a memory system, but at least that this provides analytical method for measuring or describing them. If one were going to have the five digit number, you could imagine a nested model where the lowest level is the ones place and it's nested within a decision tree of tens places and hundreds and so on. And then there's some seek or access policy or strategy that helps speak the number in reverse. And some cognitive or computational limitation is just how well that cognitive agent can perform on that test. Again, that doesn't mean that's the mechanism that's being used, but that would be a way to use a nested generative model to encode like multiple levels of spatial or temporal variability. But context becomes an issue because you're basically just expanding the possibility space looking for sparser and sparser associations. So if you don't have a good compositionality or really well definable well articulated causal process, then you're just building these all by all models that you're going to be searching through in the dark.
5082390	5263260	A	0.8953891388888887	Yeah, one point I want to comment on that is that, well, it's not clear that the best way to reason will be the way humans reason. Right? So we might eventually have a different way of implementing working memory on an LLM which would be more effective than ours. There's also to the other side, there's also the point that it seems that the limited capacity we have on working memory, maybe it's not just a lack of power, maybe it's necessary. So we kind of like don't lose our minds in the sense that we have a limited space to search and then we're able to search this limited space. And if it was too, if we have too much space to search because we have super working memory, then we wouldn't be able to finish the task anytime. So maybe this is like a physical good barrier, something that helps. And it's interesting that what I find interesting is that okay, the AI may find a lot of different and better ways to implement work memory, but it's interesting that they implemented a worksheet memory or a search agent, which was very like the way I described in the thesis. That was what I found amazing. As you were saying. There's very ways to implement it working memory, but they did it in a very similar way in the sense of searching the space after the genitive model has offered a solution. So I don't know, maybe that's a good indication that this is on the right track, but it also may just simply be a simple way they found to solve the problem. And it was just by coincidence similar to this. Yeah, maybe the correct way or the way the brain does may also be different. We don't know. All we have was the limited evidence I offered a bit more obviously and some constraints we know on AI information processing. We have to figure it out. And I think that this implementation stuff you're asking is mostly due to the AI people to find out or the math people, the systems to figure out.
5265630	5269370	B	0.9707043750000001	Nick, do you want to comment or a question or I'll ask one from the chat.
5269870	5270186	A	0.81548	Yeah.
5270208	5384530	C	0.9322739828080225	So I just have a kind of a bigger picture question that kind of gets us into the realm of these chat bots, right? So you in this preprint and in this talk have like given us a variety of different tasks in the heuristics and biases literature. The the conjunction fallacy task, the the lend a problem, some people call it, or these cognitive reflection test questions. And you know, there's a, there's like a paper in the fall that showed that a preprint in the fall that last fall that showed 2022, that showed GPT-3 and 3.5. And even the earlier versions, like, showed a lot of the human, like, faulty intuitions. Like it basically fell for the lure pretty often. Sometimes, like most of the time it was falling for the lure. But then as soon as GPT Four was available to be studied and used on these models, it was like performing basically near perfect. And when it did get the incorrect response, it wasn't necessarily the lure, it was just like some other general type of error it was making, like misinterpreting the question altogether or something. And it seems like the predicting, predictive and reflecting model framework could have at least two different ways of explaining this. One is what happened is they changed the AI's like type one thinking or whatever so that it could respond intuitively to all these, but intuitively and correctly, so it didn't need to reflect. Or it could be that they somehow created this reflective type system with either like a chain of thought or a tree of thought or some other type of system that helps it actually engage in reflection on these tasks or whatever the analog of reflection would be for a chat bot. And so I'm wondering if you have thoughts on this, as you've been thinking about this or talking to anyone about this, how do you think they did this? Obviously we have to speculate because a lot of the data is proprietary. But I'm just curious to get your thoughts?
5385270	5511640	A	0.8959576818181819	Yeah, that's an awesome question. I actually forgot to mention this on the presentation that the biases they make on the recent test last year were exactly the ones the humans made. So if we fed it some dual process real, it would give exactly our mistakes. That's so amazing. I'm not certain why that happened, but it did and how they fixed this. So, yeah, I do think that they trained the model on these tasks because people were able to get similar issues by changing the questions a little bit. So I think they cheated on that in the sense that they trained on known problems, but that's not true of new problems. If you give it some different formulation and feed it to 3.5 new version, it's still failing. But the people on Open AI, they are aware of this chain of thought research, they are aware of this tree of thought research, but I don't think they want to depend or rely on these external agents. So they recently released a report where they are trying to reinforce intermediate steps of reasoning on GPC four. So they try to simulate internally what the chain of thoughts reasoning is doing externally by reinforcing the intermediate steps of reasoning so that it starts failing composition, helping. So on.
5514910	5645320	B	0.9155666883116886	A lot of thought on that. I was reminded by Daniel Dennett's framework or model in Darwin's dangerous idea of cranes and skyhooks. And so some building is built and then the question is like, was it built with cranes or with top down hung skyhooks that just sort of descend from nowhere, hold everything up while it's all being built? And it's like, well, no, it's with cranes. And then if you need a big crane, you can use a smaller crane to assemble a crane. So that's kind of a bottom up constructive metaphor, whereas the sky hook is like the top down compositionality. And that's when you get to certain level of sophistication for a cognitive system, it can make the blueprint or the plan like maybe what we would associate with the expected free energy calculation, not just the variational free energy calculation. So at some point when a plan can be made, it requires a strategy between type one and type two switching and probably other switches too. And so that crane approach, you need a lot of compute to do a bottom up. So even though it's kind of weird to think about, it's almost like the current large models are very bottom up because they're very bottom up from syntax and they don't have what you refer to as imperative linguistic knowledge, which can be normative as well as a heuristic for knowledge. And that's potentially a small model, but it contains wisdom that's explicit and practices and stances or dispositions as you described. And so it's kind of like it's not a denial of embodiment to also clarify what this skyhook ability is. And that's type two like so I think. I guess. What do you think about that? Or how do you connect this to anything in that area?
5646410	5655480	A	0.8534027272727273	Yeah, I'm going to make you do the summary and the question again, because although I did track some of what you mean, I'm not sure which ones made to comment on that.
5656590	5674110	B	0.908339117647059	Do you think that the landscape of large and small models would be different if people took on board some of the features you described, like imperative linguistic knowledge, seriously into construction of models?
5675970	5707660	A	0.9065202702702704	So let me just see if this is something you're asking. So they're doing small models now for phones and open source models for phones, and they have the issue of not being able to scale up. Right? Because if you scale up too much, then you can't run on the phone. So you're asking if we can implement something like Tree of Thoughts to help these smaller models. Is that what you said?
5708030	5708780	B	0.99995	Yes.
5710110	5714154	A	0.9543658333333332	Are you sure or are you just saying, okay, go for that?
5714272	5740690	B	0.9341079999999999	That's an example of what it would mean to re understand. Like where do we need an API call to a cloud center? Or where could some context be locally computed with much more of a type One? What are some type two problems that we might be able to type one our way out of? What are some type one areas where things are not working and a type two description might be advantageous?
5741350	6025360	A	0.8941922222222224	Yeah, I think they're getting onto that. And they're being able to make these smaller, smaller models precisely because of these sorts of discoveries they made on Chain of Thoughts. Right. There's also stuff about being able to train on specific data. I read one. It's called? Textbooks are all you need. They're training smaller, smaller models on textbooks, and they're doing better. So, yeah, there are differences in training smaller models, different data, but there's also the possibility of making them more reliable based on what I'm calling T Two Agents or Train of Thoughts, stuff like that. And again, we don't know if that's all we can do externally, but I would argue that we would need necessarily some sort of external mechanism coupled to any large language model for the best results because of the differences in representations, mainly because of this. Unless the large language model somehow makes a symbolic representation emerge from its processes, it's not clear that it has done so. But unless it does so, and it uses that reliably because it may even have generated some sort of symbolic process internally, but doesn't always use that or doesn't know when to use that, most likely we're going to need something external to the language model, and this is some clues to how to implement that. And also, I think, in our brain, it's kind of external not only in the sense of our minds, but also in the brain, since it's a recent step in evolution, we don't see I don't know lizards reasoning in type two manner. Right. We do see Chintenzees reasoning in type two manner, but not as much as we do. So it's very human like reasoning. It's just that going back to the context problem, animals do solve contextual problems. That's why they can live. Right. So the issue is not actually the context there. It's more like some boost on reasoning capacity. And that boost on reasoning capacity may be related to the prefrontal cortex, as we know. And so, of course, the prefrontal cortex is also a network. It's not a serial Turing machine. But if there's somewhere that's implementing a serial machine or a symbolic classic AI machine or symbolic reasoning or something like that, most likely it would be the prefrontal cortex, which is kind of like a distinct addiction addition to the brain and maybe something related to language, which is also distinct, a new addition to the brain. Also, I'm very convinced by the photo and deletion argument in 88 that you're going to need symbolic processing anyway. Either the model is going to generate that symbolic processing, or you're going to have to feed it for him, or else you're going to have hallucination of mistakes and reasoning because of the nature of because of the nature of distributed representations. Right. It's uncertain and ambiguous by nature. That's why I'm not entirely convinced that only probabilistic representations will be enough to solve hallucination issues.
6029490	6035380	B	0.9684637499999998	Awesome. Nick, do you have any other questions or areas you want to mention or ask?
6039830	6048818	C	0.9136028125000002	I think those were my main questions. I don't know if there's anything in the Chat that you wanted to touch on before we go. I know we're maybe over time, I'll.
6048834	6071070	B	0.9701785999999999	Go with one from the Chat, and then we can have any closing thoughts. So Glia maximalist asks, are you familiar with visual pathways in the human brain? Specifically the dorsal where and ventral? What pathways do you think this maps onto the two systems you describe for problem solving?
6073330	6169530	A	0.9059858333333327	Okay, yeah. So like I said, in 2008, evans tried to accomplish that unifying vision of due process theory, but I'm not very convinced by that. He also said, well, we can't do this as of yet, but I do think it's possible, as I said on the presentation, I just don't know how to do it, how to have a convincing theory of everything, of dual process theory in the brain. It does go on to other regions. Like, you have automatic motor responses and controlled motor responses. Like you said, there's different pathways in reasoning, in figuring out stuff in the visual field. Maybe the what would be related to something and where to another most likely the what would be tapping on to type two processing. I'm not sure how the where would be tapping on that, but yeah, as I'm trying to make the point, if we extend it to the whole brain, we lose what we've learned. We're not sure how that applies.
6174290	6179600	B	0.8459142857142856	Cool. Well, Nick, any penultimate last words?
6183800	6188164	A	0.81344	You muted. Oh, wait.
6188202	6189510	B	0.945435	Unmute and then continue.
6190920	6206810	C	0.9393632558139532	Sorry about that. No worries. Thanks to both of you, Dr. Bellini, Lighte and Daniel, for coordinating this. And I'm really just glad to have found the research and to discuss it and connect and looking forward to further conversations, to be honest.
6209120	6212860	B	0.92254	Awesome. Sam, any closing thoughts?
6213600	6247864	A	0.9293580303030303	Yeah, I'm just sad I couldn't go much beyond what I said on the presentation. Most of the stuff you guys asked, I'm not sure I have good answers for that. Basically, what I've done is all I know, and I'm not sure how to go on the future with this, and maybe someone in the audience will do. So I hope that helps, guys. Great.
6247982	6253700	B	0.8444816666666665	I hope so, too. All right, till next time. Thank you. Bye.

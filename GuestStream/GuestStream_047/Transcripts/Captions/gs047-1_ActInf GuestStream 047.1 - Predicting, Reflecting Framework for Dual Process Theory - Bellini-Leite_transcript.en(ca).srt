1
00:00:06,054 --> 00:00:07,249
Daniel Friedman: Hello and welcome.

2
00:00:07,351 --> 00:00:12,316
This is ActInf GuestStream 47.1 on July

3
00:00:12,316 --> 00:00:13,440
10, 2023.

4
00:00:13,458 --> 00:00:15,676
We're here with Nick Byrd and Samuel

5
00:00:15,676 --> 00:00:15,676
Bellini-Leite, and we're going to have a

6
00:00:15,676 --> 00:00:20,162
presentation followed by a discussion.

7
00:00:20,167 --> 00:00:23,405
So thank you both for joining.

8
00:00:23,416 --> 00:00:25,615
And Sam, thanks again for this

9
00:00:25,615 --> 00:00:26,709
presentation.

10
00:00:26,720 --> 00:00:27,818
Off to you.

11
00:00:30,143 --> 00:00:32,333
Samuel Bellini-Leite: Okay?

12
00:00:33,434 --> 00:00:35,632
Was Nick going to say something before I

13
00:00:35,632 --> 00:00:35,677
started?

14
00:00:36,762 --> 00:00:37,851
I can go ahead.

15
00:00:37,856 --> 00:00:43,422
Okay, so some introduction first.

16
00:00:43,433 --> 00:00:46,729
I'm a professor at Minas Gerais State

17
00:00:46,729 --> 00:00:47,825
University.

18
00:00:49,051 --> 00:00:52,384
I'm a psychologist philosopher, mainly

19
00:00:52,384 --> 00:00:55,648
working in cognitive science.

20
00:00:55,692 --> 00:01:00,548
I'm going to talk about what I did in my

21
00:01:00,548 --> 00:01:00,548
PhD thesis and also some renewed interest

22
00:01:00,548 --> 00:01:00,548
I had on this based on some developments

23
00:01:00,548 --> 00:01:00,548
in large language models which happened

24
00:01:00,548 --> 00:01:00,548
this year, most notably Chain of Thoughts

25
00:01:00,548 --> 00:01:21,651
and Tree of Thoughts.

26
00:01:21,669 --> 00:01:24,932
Those additions to large language model

27
00:01:24,932 --> 00:01:24,932
made me want to come back to this again

28
00:01:24,932 --> 00:01:28,342
and talk about this.

29
00:01:30,547 --> 00:01:33,841
So how do we know about human reasoning,

30
00:01:33,841 --> 00:01:33,841
which is the subject, the main subject

31
00:01:33,841 --> 00:01:38,298
underlying this topic?

32
00:01:39,396 --> 00:01:42,747
Well, we had empirical research from the

33
00:01:42,747 --> 00:01:42,747
60s, starting with Peter Watson, who was

34
00:01:42,747 --> 00:01:42,747
challenging the notion that we have a

35
00:01:42,747 --> 00:01:42,747
logic module for reasoning or some

36
00:01:42,747 --> 00:01:42,747
general understanding of logic when we're

37
00:01:42,747 --> 00:01:56,118
adults.

38
00:01:57,129 --> 00:02:02,118
And he crafted tasks which were hard to

39
00:02:02,118 --> 00:02:02,118
solve, but they had actually simple

40
00:02:02,118 --> 00:02:02,118
fundamentals to it, which were taken to

41
00:02:02,118 --> 00:02:02,118
show people's irrationality, something

42
00:02:02,118 --> 00:02:18,273
like that.

43
00:02:18,275 --> 00:02:22,315
And also a huge tradition that's called

44
00:02:22,315 --> 00:02:22,315
Heuristics and Biases that also started

45
00:02:22,315 --> 00:02:22,315
in the mainly by Kanmen and Amosverskin,

46
00:02:22,315 --> 00:02:22,315
which had similar tasks and went beyond,

47
00:02:22,315 --> 00:02:22,315
and they even won the Nobel Prize for

48
00:02:22,315 --> 00:02:40,492
this work.

49
00:02:42,510 --> 00:02:44,536
I had here on the slides Evolutionary

50
00:02:44,536 --> 00:02:44,536
Psychology and Language and Pragmatics,

51
00:02:44,536 --> 00:02:44,536
which were also people in the 90s working

52
00:02:44,536 --> 00:02:52,614
with this sort of research.

53
00:02:52,619 --> 00:02:58,671
But I'm going to talk about dual process

54
00:02:58,671 --> 00:02:58,671
theory in the way they formulated by

55
00:02:58,671 --> 00:02:58,671
Evans and Stanovich, which mainly

56
00:02:58,671 --> 00:02:58,671
summarizes the results of Watson and

57
00:02:58,671 --> 00:03:16,792
common risk biases literature.

58
00:03:16,798 --> 00:03:19,824
So I'm going to talk about how dual

59
00:03:19,824 --> 00:03:19,824
process theory came to be and then some

60
00:03:19,824 --> 00:03:19,824
problems that it has that I tried to

61
00:03:19,824 --> 00:03:25,888
address.

62
00:03:28,910 --> 00:03:31,940
So one of the first reasoning tests in

63
00:03:31,940 --> 00:03:31,940
this trend literature was the Watson

64
00:03:31,940 --> 00:03:31,940
selection test, which goes like this

65
00:03:31,940 --> 00:03:31,940
please indicate which cards can determine

66
00:03:31,940 --> 00:03:42,056
if the following rule is false.

67
00:03:42,057 --> 00:03:45,082
If a card has a vowel on one side, then

68
00:03:45,082 --> 00:03:47,105
has an even number on the other side.

69
00:03:50,131 --> 00:03:53,159
This sounds like a simple exercise.

70
00:03:53,161 --> 00:03:56,197
So the card A clearly is relevant because

71
00:03:56,197 --> 00:03:59,221
it has a vowel on one side.

72
00:03:59,225 --> 00:04:02,199
If it doesn't have an even number on the

73
00:04:02,199 --> 00:04:05,229
other, then the statement is false.

74
00:04:05,229 --> 00:04:09,265
And everyone can find this neatly of most

75
00:04:09,265 --> 00:04:10,273
people.

76
00:04:11,281 --> 00:04:15,327
But the problem comes with the card seven,

77
00:04:15,327 --> 00:04:18,356
, which is not so intuitive.

78
00:04:18,357 --> 00:04:22,393
People actually choose the second card

79
00:04:22,393 --> 00:04:22,393
two there, which has an even number on

80
00:04:22,393 --> 00:04:26,439
one side.

81
00:04:27,440 --> 00:04:29,467
But having that even number doesn't help

82
00:04:29,467 --> 00:04:29,467
at all to determine if the statement is

83
00:04:29,467 --> 00:04:32,498
false.

84
00:04:33,499 --> 00:04:38,552
What does help is which is an odd number,

85
00:04:38,552 --> 00:04:38,552
if it has a file the other side, then the

86
00:04:38,552 --> 00:04:46,633
statement could be.

87
00:04:48,649 --> 00:04:53,700
So this was one of the tasks that famous

88
00:04:53,700 --> 00:04:53,700
for and conman and first key show them

89
00:04:53,700 --> 00:04:53,700
this one they're famous task as well

90
00:04:53,700 --> 00:04:53,700
solved the conjunction fallacy the issue

91
00:04:53,700 --> 00:04:53,700
that people arise when solving this task

92
00:04:53,700 --> 00:04:53,700
goes like this it is a 31 years old,

93
00:04:53,700 --> 00:05:22,931
single, outspoken and very bright.

94
00:05:22,932 --> 00:05:25,962
She majored in philosophy as a student.

95
00:05:25,964 --> 00:05:28,990
She was deeply concerned with issues of

96
00:05:28,990 --> 00:05:28,990
discrimination and social justice and

97
00:05:28,990 --> 00:05:28,990
also participated in anti nuclear

98
00:05:28,990 --> 00:05:33,049
demonstrations.

99
00:05:34,050 --> 00:05:36,069
Please make the following statements by

100
00:05:36,069 --> 00:05:36,069
their probability using one for the most

101
00:05:36,069 --> 00:05:36,069
probable and eight for the least probable.

102
00:05:36,069 --> 00:05:40,111
.

103
00:05:42,137 --> 00:05:46,178
What's interesting here is the statement,

104
00:05:46,178 --> 00:05:48,198
the last statement.

105
00:05:48,199 --> 00:05:52,237
Linda is a bank teller and isn't active

106
00:05:52,237 --> 00:05:56,269
active in the feminist movement.

107
00:05:56,278 --> 00:05:59,303
Because there's also these statements

108
00:05:59,303 --> 00:05:59,303
without one another, without the

109
00:05:59,303 --> 00:05:59,303
conjecture which is Linda is active in

110
00:05:59,303 --> 00:05:59,303
the feminist movement and Linda is a bank

111
00:05:59,303 --> 00:06:07,325
teller.

112
00:06:08,336 --> 00:06:11,368
Because of the description, people think

113
00:06:11,368 --> 00:06:11,368
that Linda is a feminist is very likely

114
00:06:11,368 --> 00:06:11,368
given the description, but they think

115
00:06:11,368 --> 00:06:20,458
being a bank teller is not likely.

116
00:06:21,461 --> 00:06:24,496
The issue is that people usually mark

117
00:06:24,496 --> 00:06:24,496
Linda is a bank teller and is a feminist

118
00:06:24,496 --> 00:06:32,572
as more likely than being a bank teller.

119
00:06:32,577 --> 00:06:35,605
The problem here is that the conjunction

120
00:06:35,605 --> 00:06:38,632
of the two necessarily is less likely.

121
00:06:38,636 --> 00:06:41,661
So being a feminist plus being something

122
00:06:41,661 --> 00:06:41,661
else or being a bank teller plus being

123
00:06:41,661 --> 00:06:41,661
something else is automatically less

124
00:06:41,661 --> 00:06:46,713
likely.

125
00:06:49,745 --> 00:06:52,779
So those are the sort of tasks that the

126
00:06:52,779 --> 00:06:52,779
reasoning literature used to show some

127
00:06:52,779 --> 00:06:58,838
sort of human irrationality.

128
00:07:00,794 --> 00:07:04,834
One issue with this is that it's not very

129
00:07:04,834 --> 00:07:04,834
interesting to show that humans are

130
00:07:04,834 --> 00:07:04,834
irrational, it's more interesting to show

131
00:07:04,834 --> 00:07:13,928
how humans reason.

132
00:07:14,935 --> 00:07:20,990
And also these sort of studies didn't

133
00:07:20,990 --> 00:07:20,990
show how people actually solved the

134
00:07:20,990 --> 00:07:26,059
problems.

135
00:07:27,060 --> 00:07:29,085
They were actually focused on how people

136
00:07:29,085 --> 00:07:30,094
made mistakes.

137
00:07:31,104 --> 00:07:34,130
One thing that's interesting about dual

138
00:07:34,130 --> 00:07:34,130
process theory is that it explains both

139
00:07:34,130 --> 00:07:34,130
the mistakes and the solutions to the

140
00:07:34,130 --> 00:07:39,187
answers.

141
00:07:40,191 --> 00:07:42,217
So in 2002, Conan and Frederick

142
00:07:42,217 --> 00:07:42,217
formulated this cognitive reflection test

143
00:07:42,217 --> 00:07:42,217
which is another one of these tasks but a

144
00:07:42,217 --> 00:07:52,310
lot much simpler and elegant.

145
00:07:52,310 --> 00:07:55,340
And it really shows indicates at least

146
00:07:55,340 --> 00:07:55,340
how dual process theory is a good

147
00:07:55,340 --> 00:07:55,340
solution to a good explanation of the

148
00:07:55,340 --> 00:08:02,355
evidence in general.

149
00:08:03,360 --> 00:08:08,413
So yeah, the western selection test used

150
00:08:08,413 --> 00:08:08,413
logic and the Linda task used probability.

151
00:08:08,413 --> 00:08:14,470
.

152
00:08:14,472 --> 00:08:16,499
And here we just have basic arithmetic.

153
00:08:16,499 --> 00:08:19,529
So as if math we don't have to worry

154
00:08:19,529 --> 00:08:19,529
about which answer is truly correct or

155
00:08:19,529 --> 00:08:26,593
which rational norms we need to follow.

156
00:08:26,594 --> 00:08:28,610
It's just basic math.

157
00:08:29,623 --> 00:08:31,643
There's nothing to discuss here.

158
00:08:31,644 --> 00:08:31,647
Right?

159
00:08:31,648 --> 00:08:35,682
So the task goes like this a bat and ball

160
00:08:35,682 --> 00:08:37,705
costs one in ten in total.

161
00:08:37,706 --> 00:08:39,729
You might know this test, it's a famous

162
00:08:39,729 --> 00:08:39,729
test even out of the reasoning literature,

163
00:08:39,729 --> 00:08:44,773
, the bat costs one more than ball.

164
00:08:44,773 --> 00:08:46,793
How much does the ball cost?

165
00:08:46,798 --> 00:08:49,823
So people when when people read this,

166
00:08:49,823 --> 00:08:51,847
they usually say $0.10 intuitively.

167
00:08:52,854 --> 00:08:55,884
But if you take a little time to write

168
00:08:55,884 --> 00:08:55,884
write it down, you can see that it's

169
00:08:55,884 --> 00:08:59,926
actually $0.05.

170
00:08:59,927 --> 00:09:08,952
So the bat cost one and ball costs $0.05.

171
00:09:08,952 --> 00:09:08,952
.

172
00:09:08,953 --> 00:09:10,972
Together it costs one and ten.

173
00:09:11,986 --> 00:09:14,014
The other two have a similar structure.

174
00:09:14,015 --> 00:09:18,050
The point here is that sometimes we look

175
00:09:18,050 --> 00:09:18,050
at these tests and we have an intuitive

176
00:09:18,050 --> 00:09:23,105
answer which is wrong.

177
00:09:24,115 --> 00:09:28,153
And the way we get out of these issues is

178
00:09:28,153 --> 00:09:28,153
that we stop and think a little bit and

179
00:09:28,153 --> 00:09:28,153
review our answers with some sort of step

180
00:09:28,153 --> 00:09:28,153
by step reasoning or writing it down,

181
00:09:28,153 --> 00:09:41,281
something like that.

182
00:09:42,295 --> 00:09:46,330
What's interesting here is if you look at

183
00:09:46,330 --> 00:09:46,330
two plus two, it's almost impossible not

184
00:09:46,330 --> 00:09:50,377
to think four.

185
00:09:51,382 --> 00:09:52,398
It's almost like you're perceiving.

186
00:09:52,399 --> 00:09:55,426
The answer is four just by looking at it.

187
00:09:55,426 --> 00:09:55,426
.

188
00:09:55,427 --> 00:09:58,457
But if you have some sort of more complex

189
00:09:58,457 --> 00:09:58,457
calculation to do, obviously just by

190
00:09:58,457 --> 00:09:58,457
looking at it, you won't have a solution.

191
00:09:58,457 --> 00:10:04,455
.

192
00:10:05,463 --> 00:10:09,501
So this basic intuition is clearly shown

193
00:10:09,501 --> 00:10:12,533
in the cognitive reflection test.

194
00:10:13,545 --> 00:10:16,576
And what's more interesting about it is

195
00:10:16,576 --> 00:10:16,576
that the cognitive reflection test

196
00:10:16,576 --> 00:10:16,576
actually predicts the performance of

197
00:10:16,576 --> 00:10:25,663
various Aristocrats and Biases tests.

198
00:10:25,664 --> 00:10:28,693
So if someone does well on cognitive

199
00:10:28,693 --> 00:10:28,693
reflection test, they're more likely to

200
00:10:28,693 --> 00:10:34,756
go well on any other reasoning task.

201
00:10:36,773 --> 00:10:38,796
And if they go bad on the cognitive

202
00:10:38,796 --> 00:10:38,796
reflection test, they likely will go bad

203
00:10:38,796 --> 00:10:43,846
on other reasoning tests as well.

204
00:10:44,853 --> 00:10:47,888
To me, this is also an indication that

205
00:10:47,888 --> 00:10:47,888
this dual process structure of intuitive

206
00:10:47,888 --> 00:10:47,888
reasoning versus slow reasoning,

207
00:10:47,888 --> 00:10:47,888
reflective reasoning, likely explains the

208
00:10:47,888 --> 00:11:00,958
rest of the text as well.

209
00:11:00,959 --> 00:11:03,986
Not only these two, but most of the

210
00:11:03,986 --> 00:11:04,993
others.

211
00:11:04,994 --> 00:11:06,015
So it's a good explanation of the

212
00:11:06,015 --> 00:11:07,022
literature.

213
00:11:08,034 --> 00:11:14,089
Okay, so what does the theory say besides

214
00:11:14,089 --> 00:11:14,089
besides explaining the reasoning

215
00:11:14,089 --> 00:11:20,150
literature?

216
00:11:20,158 --> 00:11:25,199
So, in 2008, Jonathan Evans tried to make

217
00:11:25,199 --> 00:11:25,199
a formulation of it that would include

218
00:11:25,199 --> 00:11:25,199
and unify views from different domains

219
00:11:25,199 --> 00:11:25,199
such as social cognition, neurosciences,

220
00:11:25,199 --> 00:11:25,199
other people in psychologists which had

221
00:11:25,199 --> 00:11:43,380
similar theories.

222
00:11:43,381 --> 00:11:47,426
And it seems like there's something in

223
00:11:47,426 --> 00:11:47,426
common here to unify, but Evans wasn't

224
00:11:47,426 --> 00:11:53,483
able to.

225
00:11:53,484 --> 00:11:57,522
And so there's a formulation specific for

226
00:11:57,522 --> 00:11:57,522
reasoning and we don't know if it applies

227
00:11:57,522 --> 00:11:57,522
so much well to other areas of cognition.

228
00:11:57,522 --> 00:12:05,541
.

229
00:12:07,566 --> 00:12:12,612
One common way to describe dual process

230
00:12:12,612 --> 00:12:12,612
theory is by making these tables of

231
00:12:12,612 --> 00:12:17,665
features.

232
00:12:17,667 --> 00:12:21,699
So we say, oh, type one processes have

233
00:12:21,699 --> 00:12:21,699
these sorts of features on the left, and

234
00:12:21,699 --> 00:12:21,699
type two processes have this other type

235
00:12:21,699 --> 00:12:30,791
of features on the right.

236
00:12:32,815 --> 00:12:36,853
This table here was one I made by joining

237
00:12:36,853 --> 00:12:36,853
some of the features of Evans

238
00:12:36,853 --> 00:12:36,853
insteadovich, and also of Kunman, and

239
00:12:36,853 --> 00:12:36,853
then with some questions I raised in the

240
00:12:36,853 --> 00:12:36,853
thesis, I elaborated it this way, but it'

241
00:12:36,853 --> 00:12:36,853
's very similar to the Yavin Instant

242
00:12:36,853 --> 00:12:54,037
formulation.

243
00:12:56,050 --> 00:12:58,075
One problem we have with this tables

244
00:12:58,075 --> 00:12:58,075
formulation of the Duo Process Theory is

245
00:12:58,075 --> 00:12:58,075
that usually people agree that in one

246
00:12:58,075 --> 00:12:58,075
thinking instance, when you're solving a

247
00:12:58,075 --> 00:12:58,075
problem, you're not going to have all of

248
00:12:58,075 --> 00:13:11,148
those features together.

249
00:13:13,163 --> 00:13:16,191
So we don't know for sure when some

250
00:13:16,191 --> 00:13:16,191
feature will be present or not, for

251
00:13:16,191 --> 00:13:19,227
instance.

252
00:13:19,228 --> 00:13:22,251
So clear that type one processes

253
00:13:22,251 --> 00:13:22,251
necessarily have to be unconscious and

254
00:13:22,251 --> 00:13:22,251
type two processes necessarily have to be

255
00:13:22,251 --> 00:13:28,318
conscious.

256
00:13:30,330 --> 00:13:33,363
I think that comes a lot because of some

257
00:13:33,363 --> 00:13:36,390
conceptual issues in the theory.

258
00:13:37,399 --> 00:13:39,427
But one solution they have found was to

259
00:13:39,427 --> 00:13:39,427
have at least some defining features but

260
00:13:39,427 --> 00:13:39,427
different authors have different defining

261
00:13:39,427 --> 00:13:46,494
features.

262
00:13:46,496 --> 00:13:49,528
So here I have some of the defining

263
00:13:49,528 --> 00:13:49,528
features of the main authors, which is

264
00:13:49,528 --> 00:13:49,528
type one processes use less working

265
00:13:49,528 --> 00:13:49,528
memory while type two processes most

266
00:13:49,528 --> 00:14:02,598
strongly on working memory.

267
00:14:03,608 --> 00:14:07,640
Type one processes aren't autonomous in

268
00:14:07,640 --> 00:14:07,640
the sense that they don't need to wait

269
00:14:07,640 --> 00:14:13,699
for higher processing to respond.

270
00:14:13,700 --> 00:14:15,728
If they have an answer, they go for it.

271
00:14:16,732 --> 00:14:20,772
And using the couple's representations

272
00:14:20,772 --> 00:14:20,772
mean that means that type two systems can

273
00:14:20,772 --> 00:14:20,772
reason beyond what's in the present

274
00:14:20,772 --> 00:14:20,772
environment can think about things that

275
00:14:20,772 --> 00:14:34,910
are not there.

276
00:14:35,925 --> 00:14:38,949
Type one processing is faster in

277
00:14:38,949 --> 00:14:38,949
comparison while two is slower in

278
00:14:38,949 --> 00:14:41,984
comparison.

279
00:14:43,000 --> 00:14:46,036
Type one processing is less effortful

280
00:14:46,036 --> 00:14:46,036
while two is more effortful in the sense

281
00:14:46,036 --> 00:14:53,101
of psychological effort.

282
00:14:53,101 --> 00:14:56,135
Like people report having a hard time

283
00:14:56,135 --> 00:14:57,148
doing things.

284
00:14:58,149 --> 00:15:00,116
So for instance in the cognitive

285
00:15:00,116 --> 00:15:00,116
reflection test, people that solve the

286
00:15:00,116 --> 00:15:00,116
test correctly usually rate it and it's

287
00:15:00,116 --> 00:15:00,116
harder because they had to use tattoo

288
00:15:00,116 --> 00:15:12,233
processing to solve it.

289
00:15:17,281 --> 00:15:21,323
Okay, but Samuels, which is a philosopher,

290
00:15:21,323 --> 00:15:21,323
, dual process theory and other subjects,

291
00:15:21,323 --> 00:15:21,323
he says okay, even if these features

292
00:15:21,323 --> 00:15:21,323
proposal is good, we still need a

293
00:15:21,323 --> 00:15:21,323
mechanism to explain why these features

294
00:15:21,323 --> 00:15:38,494
are linked together.

295
00:15:38,495 --> 00:15:41,528
So what is the mechanism that explains

296
00:15:41,528 --> 00:15:41,528
why type one processing features go

297
00:15:41,528 --> 00:15:41,528
together and what is the mechanism that

298
00:15:41,528 --> 00:15:41,528
explains why type two processing goes

299
00:15:41,528 --> 00:15:52,634
together?

300
00:15:52,635 --> 00:15:55,667
So we need some beyond saying that they'

301
00:15:55,667 --> 00:15:55,667
're different, we need to explain what

302
00:15:55,667 --> 00:15:55,667
are these mechanisms that make them

303
00:15:55,667 --> 00:16:02,673
different?

304
00:16:03,685 --> 00:16:07,722
And I actually had Samuels on my PhD

305
00:16:07,722 --> 00:16:07,722
committee and I'm not sure if he was

306
00:16:07,722 --> 00:16:14,790
convinced with my solution.

307
00:16:14,790 --> 00:16:16,819
So my PhD was a solution to this problem.

308
00:16:16,819 --> 00:16:16,819
.

309
00:16:17,819 --> 00:16:20,854
I'm not sure how he was convinced of it,

310
00:16:20,854 --> 00:16:20,854
but he approved the PhD, so that's good

311
00:16:20,854 --> 00:16:25,904
enough, I guess.

312
00:16:29,940 --> 00:16:29,947
Yeah.

313
00:16:31,964 --> 00:16:34,990
So the goal was to solve the unity

314
00:16:34,990 --> 00:16:34,990
problem, was to explain the mechanisms

315
00:16:34,990 --> 00:16:34,990
that explain type one processing and

316
00:16:34,990 --> 00:16:34,990
explain the mechanisms that explain type

317
00:16:34,990 --> 00:16:44,089
two processing.

318
00:16:44,093 --> 00:16:47,121
And I started to see, to search in the

319
00:16:47,121 --> 00:16:47,121
literature first, how people were

320
00:16:47,121 --> 00:16:47,121
conceiving of the general frameworks that

321
00:16:47,121 --> 00:16:47,121
were behind these two types of reasoning

322
00:16:47,121 --> 00:16:47,121
and one that was not actually related to

323
00:16:47,121 --> 00:16:47,121
this literature of dual process theory

324
00:16:47,121 --> 00:16:47,121
but was actually famous in cognitive

325
00:16:47,121 --> 00:16:47,121
science, was the modularity of mind in

326
00:16:47,121 --> 00:16:47,121
the 80s proposed by philosopher Jared

327
00:16:47,121 --> 00:17:10,297
Schroder.

328
00:17:11,302 --> 00:17:15,340
And he said that we had two types of

329
00:17:15,340 --> 00:17:15,340
processes, a modular process mostly

330
00:17:15,340 --> 00:17:15,340
related to input and perception and we

331
00:17:15,340 --> 00:17:15,340
had central processes that was mostly

332
00:17:15,340 --> 00:17:30,491
related to higher reasoning.

333
00:17:30,491 --> 00:17:33,519
So you can trace some sort of resemblance

334
00:17:33,519 --> 00:17:34,539
there to do process theory.

335
00:17:35,549 --> 00:17:39,582
And the way he went about this is that he

336
00:17:39,582 --> 00:17:39,582
said these perceptual models, modular

337
00:17:39,582 --> 00:17:45,647
processes are information encapsulated.

338
00:17:45,648 --> 00:17:48,672
So vision only works with vision and

339
00:17:48,672 --> 00:17:48,672
language only works with language, space

340
00:17:48,672 --> 00:17:52,715
only works with space.

341
00:17:53,721 --> 00:17:55,748
And so these were domain specific

342
00:17:55,748 --> 00:17:55,748
knowledge systems that did not

343
00:17:55,748 --> 00:17:55,748
communicate with each other and when they

344
00:17:55,748 --> 00:17:55,748
needed contact sensitivity, they needed

345
00:17:55,748 --> 00:18:06,798
central processes.

346
00:18:06,798 --> 00:18:11,842
So these central processes he would argue,

347
00:18:11,842 --> 00:18:11,842
, were Isotropic and Quinine, which means

348
00:18:11,842 --> 00:18:11,842
that any type of information could be

349
00:18:11,842 --> 00:18:11,842
used in a solution so we can relate, I

350
00:18:11,842 --> 00:18:11,842
don't know, physics with biology and form

351
00:18:11,842 --> 00:18:29,002
a new solution.

352
00:18:29,002 --> 00:18:32,005
So there's knowledge from multiple

353
00:18:32,005 --> 00:18:32,005
domains working here that would also need

354
00:18:32,005 --> 00:18:32,005
some sort of context sensitivity, which

355
00:18:32,005 --> 00:18:41,014
was a problem for photo.

356
00:18:41,014 --> 00:18:44,017
He didn't think we would solve this

357
00:18:44,017 --> 00:18:44,017
context sensitivity problem at all and he

358
00:18:44,017 --> 00:18:49,022
had good reasons for that.

359
00:18:50,023 --> 00:18:51,024
In classic AI.

360
00:18:51,024 --> 00:18:55,028
We had a lot of issues with that.

361
00:18:55,028 --> 00:18:58,031
I mean, we still do have issues with the

362
00:18:58,031 --> 00:18:58,031
context sensitivity problem, but not the

363
00:18:58,031 --> 00:18:58,031
sort of issues we had in the 80s when

364
00:18:58,031 --> 00:18:58,031
they were debating this same problem of

365
00:18:58,031 --> 00:19:09,036
classical AI.

366
00:19:09,036 --> 00:19:12,039
So this problem was a problem of how to

367
00:19:12,039 --> 00:19:12,039
represent a changing environment in

368
00:19:12,039 --> 00:19:15,042
symbolic representation.

369
00:19:18,045 --> 00:19:21,048
So yeah, it was a problem because

370
00:19:21,048 --> 00:19:21,048
classical AI attempted to give knowledge

371
00:19:21,048 --> 00:19:21,048
to robots or artificial intelligence and

372
00:19:21,048 --> 00:19:21,048
statements about the world and they were

373
00:19:21,048 --> 00:19:21,048
intractable, hard to compute, there are

374
00:19:21,048 --> 00:19:21,048
so many knowledge banks to search and so

375
00:19:21,048 --> 00:19:21,048
many knowledge to try to use that the

376
00:19:21,048 --> 00:19:49,076
robots couldn't do anything relevant.

377
00:19:51,078 --> 00:19:55,082
They also couldn't represent change.

378
00:19:55,082 --> 00:19:58,085
So the changing environment was terrible

379
00:19:58,085 --> 00:19:58,085
for them, didn't work with this symbolic

380
00:19:58,085 --> 00:19:58,085
representation, it didn't work with

381
00:19:58,085 --> 00:19:58,085
serial reasoning or anything like of the

382
00:19:58,085 --> 00:20:08,089
sort.

383
00:20:08,089 --> 00:20:11,092
This was a huge problem for classic

384
00:20:11,092 --> 00:20:13,094
cognitive scientists.

385
00:20:15,096 --> 00:20:18,099
And since photo was a believer in

386
00:20:18,099 --> 00:20:18,099
cognitive science in the sense of a

387
00:20:18,099 --> 00:20:18,099
classical computational theory of mind or

388
00:20:18,099 --> 00:20:18,099
language of thought theory, he said that

389
00:20:18,099 --> 00:20:18,099
he had found Photo's first law of the

390
00:20:18,099 --> 00:20:37,118
nonexistence of cognitive science.

391
00:20:37,118 --> 00:20:40,121
This law states that the more processes

392
00:20:40,121 --> 00:20:40,121
are global, the less cognitive science

393
00:20:40,121 --> 00:20:45,126
will be able to understand them.

394
00:20:47,128 --> 00:20:50,131
So he thought only these encapsulated

395
00:20:50,131 --> 00:20:52,133
processes could be studied.

396
00:20:52,133 --> 00:20:56,137
And if we needed this sort of global and

397
00:20:56,137 --> 00:20:56,137
context sensitive type of processing,

398
00:20:56,137 --> 00:20:56,137
then contest science just couldn't do it

399
00:20:56,137 --> 00:20:56,137
because he had in mind contra science in

400
00:20:56,137 --> 00:21:09,144
the sense of classical AI.

401
00:21:13,148 --> 00:21:16,151
But it also seems not only folder, it

402
00:21:16,151 --> 00:21:16,151
seems that the literature abandoned

403
00:21:16,151 --> 00:21:16,151
classical cognitive science in various

404
00:21:16,151 --> 00:21:16,151
ways, which were strange because if you

405
00:21:16,151 --> 00:21:16,151
take the work of Alan Newell and Herbert

406
00:21:16,151 --> 00:21:16,151
Simon, it's strange to think that oh,

407
00:21:16,151 --> 00:21:32,167
they were completely wrong.

408
00:21:32,167 --> 00:21:36,171
There's nothing, nothing useful that they

409
00:21:36,171 --> 00:21:39,173
say that could be applied today.

410
00:21:39,174 --> 00:21:42,177
All the evidence, all the theories they

411
00:21:42,177 --> 00:21:44,179
made are just completely wrong.

412
00:21:45,180 --> 00:21:48,183
That's a weird way of thinking.

413
00:21:48,183 --> 00:21:52,187
And well, if you take for instance, our

414
00:21:52,187 --> 00:21:52,187
theories today of the human Bayesian

415
00:21:52,187 --> 00:21:59,194
brain, they don't consider this at all.

416
00:22:01,190 --> 00:22:03,192
It's just like they don't care.

417
00:22:03,192 --> 00:22:07,196
It's a different strand of theories.

418
00:22:07,196 --> 00:22:11,200
But it also seemed to me that there must

419
00:22:11,200 --> 00:22:11,200
be at least something wrong about,

420
00:22:11,200 --> 00:22:11,200
something right about this shouldn't be

421
00:22:11,200 --> 00:22:19,208
totally abandoned.

422
00:22:19,208 --> 00:22:23,212
There must be a place for these results.

423
00:22:26,215 --> 00:22:31,220
Explanation of the Mind one thing that

424
00:22:31,220 --> 00:22:31,220
was in odds with the way Photer described

425
00:22:31,220 --> 00:22:31,220
his dual process theory, and also

426
00:22:31,220 --> 00:22:31,220
sometimes Stan of a trend talks about

427
00:22:31,220 --> 00:22:49,238
dual process theory.

428
00:22:50,239 --> 00:22:55,244
It seems like forgetting this fact here

429
00:22:55,244 --> 00:22:55,244
is the fact that we have a limited

430
00:22:55,244 --> 00:22:55,244
capacity when we use conscious processing

431
00:22:55,244 --> 00:23:06,249
or working memory.

432
00:23:07,250 --> 00:23:09,252
And this is one of the best evidence in

433
00:23:09,252 --> 00:23:11,254
psychology we have so far.

434
00:23:12,255 --> 00:23:14,257
We had behaviorist evidence for

435
00:23:14,257 --> 00:23:14,257
reinforcement, enforcement, learning and

436
00:23:14,257 --> 00:23:14,257
that stuff and then we had cognitive

437
00:23:14,257 --> 00:23:14,257
psychology in the this is the best

438
00:23:14,257 --> 00:23:14,257
evidence that cognitive psychology has

439
00:23:14,257 --> 00:23:28,271
had for a long time.

440
00:23:28,271 --> 00:23:32,275
So it's also something we should not

441
00:23:32,275 --> 00:23:33,276
simply ignore.

442
00:23:34,277 --> 00:23:37,280
So we have in short term memory the

443
00:23:37,280 --> 00:23:37,280
knowledge that when we store some

444
00:23:37,280 --> 00:23:37,280
information in short term memory, we

445
00:23:37,280 --> 00:23:37,280
usually can only keep track of about

446
00:23:37,280 --> 00:23:37,280
seven items, sometimes even less in

447
00:23:37,280 --> 00:23:55,298
selective attention.

448
00:23:56,299 --> 00:23:59,302
There was this task where there was a

449
00:23:59,302 --> 00:23:59,302
video of people playing basketball and

450
00:23:59,302 --> 00:23:59,302
the subjects had to count how many passes

451
00:23:59,302 --> 00:23:59,302
they were making and suddenly a gorilla

452
00:23:59,302 --> 00:23:59,302
passed by on the scene and people didn't

453
00:23:59,302 --> 00:23:59,302
notice the gorilla because they were so

454
00:23:59,302 --> 00:23:59,302
concentrated on the task, which is some

455
00:23:59,302 --> 00:24:27,324
evidence of selective attention.

456
00:24:27,324 --> 00:24:30,327
That is, when you're concentrating on

457
00:24:30,327 --> 00:24:30,327
tasks, you actually don't even see other

458
00:24:30,327 --> 00:24:35,332
stimuli.

459
00:24:39,336 --> 00:24:42,339
We have also a competition for limited

460
00:24:42,339 --> 00:24:42,339
resources when doing dual tasks, or if I'

461
00:24:42,339 --> 00:24:42,339
'm talking on the phone and writing at

462
00:24:42,339 --> 00:24:42,339
the same time, I'm probably not going to

463
00:24:42,339 --> 00:24:42,339
do a very good writing because we're

464
00:24:42,339 --> 00:24:54,351
competing for limited resources.

465
00:24:54,351 --> 00:24:57,354
And it's important to note that if you're

466
00:24:57,354 --> 00:24:57,354
walking and writing or walking and

467
00:24:57,354 --> 00:24:57,354
talking, since those are different tasks,

468
00:24:57,354 --> 00:25:03,354
they won't compete.

469
00:25:03,354 --> 00:25:05,356
So we have some competition.

470
00:25:06,357 --> 00:25:10,360
We have more competition when these

471
00:25:10,360 --> 00:25:10,360
limited resources are disputed also in

472
00:25:10,360 --> 00:25:15,366
working memory.

473
00:25:15,366 --> 00:25:18,369
So if I say the number 793241 and I ask

474
00:25:18,369 --> 00:25:18,369
you to repeat them backwards and you're

475
00:25:18,369 --> 00:25:18,369
not looking at the slides, you're going

476
00:25:18,369 --> 00:25:26,377
to have a hard time with that.

477
00:25:26,377 --> 00:25:29,380
Because of the limited capacity of

478
00:25:29,380 --> 00:25:29,380
working memory, when it's clear that in

479
00:25:29,380 --> 00:25:29,380
other areas of our thinking of our brain

480
00:25:29,380 --> 00:25:29,380
is using a lot more information than

481
00:25:29,380 --> 00:25:29,380
seven items to make the scenes happen,

482
00:25:29,380 --> 00:25:29,380
perception and to make us walk and so on.

483
00:25:29,380 --> 00:25:48,399
.

484
00:25:52,403 --> 00:25:57,407
So that's on the side of the type two

485
00:25:57,407 --> 00:25:57,407
constraints, on the side of type one

486
00:25:57,407 --> 00:26:03,407
reasoning.

487
00:26:03,408 --> 00:26:06,411
Our Audi has some intuitions similar to

488
00:26:06,411 --> 00:26:07,412
the ones I described.

489
00:26:07,412 --> 00:26:10,415
So he said from its earlier days.

490
00:26:10,415 --> 00:26:13,418
The research that RSD and I conducted was

491
00:26:13,418 --> 00:26:13,418
guided by the idea that intuitive

492
00:26:13,418 --> 00:26:13,418
judgments acquired position between the

493
00:26:13,418 --> 00:26:13,418
automatic operations of perception and

494
00:26:13,418 --> 00:26:23,428
the deliberate operations of reasoning.

495
00:26:23,428 --> 00:26:25,430
Communric claimed that intuitive thinking

496
00:26:25,430 --> 00:26:25,430
is perception like and that intuitive

497
00:26:25,430 --> 00:26:25,430
predictions is an operation of system one

498
00:26:25,430 --> 00:26:25,430
further that the boundary between

499
00:26:25,430 --> 00:26:25,430
perception and judgment is fuzzy and

500
00:26:25,430 --> 00:26:36,441
permeable.

501
00:26:36,441 --> 00:26:39,444
The perception of a stranger as menacing

502
00:26:39,444 --> 00:26:39,444
is inseparable from a prediction of

503
00:26:39,444 --> 00:26:43,448
future harm.

504
00:26:43,448 --> 00:26:46,451
So you can see that this is very likely,

505
00:26:46,451 --> 00:26:46,451
very similar, sorry, to the intuitions

506
00:26:46,451 --> 00:26:46,451
that we were speaking on when we're

507
00:26:46,451 --> 00:26:53,458
talking about predictive processing.

508
00:26:53,458 --> 00:26:56,461
So feminine saying when you look at a

509
00:26:56,461 --> 00:26:56,461
face of someone and he's mad, you have a

510
00:26:56,461 --> 00:26:56,461
prediction that you're in trouble,

511
00:26:56,461 --> 00:27:04,463
right?

512
00:27:04,463 --> 00:27:09,468
So this is a sort of judgment, that same

513
00:27:09,468 --> 00:27:09,468
sort of perception, and also the context

514
00:27:09,468 --> 00:27:09,468
sensitivity is pretty much solved in

515
00:27:09,468 --> 00:27:20,479
predictive processing.

516
00:27:20,479 --> 00:27:24,483
And if you take on the AI literature that

517
00:27:24,483 --> 00:27:25,484
also happens.

518
00:27:25,484 --> 00:27:28,487
For instance, paper attention is all you

519
00:27:28,487 --> 00:27:29,488
need.

520
00:27:29,488 --> 00:27:32,491
They have some tips for how to deal with

521
00:27:32,491 --> 00:27:32,491
context sensitivity, which photo didn't

522
00:27:32,491 --> 00:27:37,496
dream of.

523
00:27:37,496 --> 00:27:40,499
So this is something that Clark said in

524
00:27:40,499 --> 00:27:40,499
2013.

525
00:27:41,500 --> 00:27:43,502
The best overall fit between driving

526
00:27:43,502 --> 00:27:43,502
signal and expectations will often be

527
00:27:43,502 --> 00:27:43,502
found by, in effect, inferring noise, in

528
00:27:43,502 --> 00:27:43,502
the driving signal and thus recognizing a

529
00:27:43,502 --> 00:27:43,502
stimulus as, for example, the letter M

530
00:27:43,502 --> 00:27:56,515
say in the context of the word.

531
00:27:56,515 --> 00:27:58,517
Mother, even though the same bare

532
00:27:58,517 --> 00:27:58,517
stimulus presented out of context or in

533
00:27:58,517 --> 00:27:58,517
most other contexts, would have been a

534
00:27:58,517 --> 00:28:05,518
better fit with the letter N.

535
00:28:06,519 --> 00:28:09,522
A unit normally responsive to the letter

536
00:28:09,522 --> 00:28:09,522
M might, under such circumstances, be

537
00:28:09,522 --> 00:28:09,522
successfully driven by an N like stimulus.

538
00:28:09,522 --> 00:28:15,528
.

539
00:28:15,528 --> 00:28:19,532
So what is Clark is trying to say here

540
00:28:19,532 --> 00:28:19,532
that since we represent using probability,

541
00:28:19,532 --> 00:28:19,532
, then if the context says we should go

542
00:28:19,532 --> 00:28:29,542
left, we go left.

543
00:28:29,542 --> 00:28:33,546
If the context says we should go right,

544
00:28:33,546 --> 00:28:33,546
we go right, because the representation

545
00:28:33,546 --> 00:28:33,546
is fluid and it accepts changes really

546
00:28:33,546 --> 00:28:42,555
quickly.

547
00:28:43,556 --> 00:28:45,558
So that's something that stems from the

548
00:28:45,558 --> 00:28:48,561
predictive processing architecture.

549
00:28:53,566 --> 00:28:56,568
But one issue I think that we might see

550
00:28:56,568 --> 00:28:56,568
in the predictive processing architecture

551
00:28:56,568 --> 00:29:01,568
is the lack of compositionality.

552
00:29:01,568 --> 00:29:03,570
So symbolic representations exhibit

553
00:29:03,570 --> 00:29:03,570
compositionalities meaning that complex

554
00:29:03,570 --> 00:29:03,570
representations are built by combining

555
00:29:03,570 --> 00:29:03,570
simpler elements according to rules or

556
00:29:03,570 --> 00:29:12,579
syntax.

557
00:29:13,580 --> 00:29:15,582
The meaning of a complex representation

558
00:29:15,582 --> 00:29:15,582
is derived from the meanings of its

559
00:29:15,582 --> 00:29:15,582
constituent parts in the way they are

560
00:29:15,582 --> 00:29:20,587
combined.

561
00:29:20,587 --> 00:29:23,590
This property allows for the generation

562
00:29:23,590 --> 00:29:23,590
of new and meaningful expressions by

563
00:29:23,590 --> 00:29:27,594
manipulating symbolic structures.

564
00:29:27,594 --> 00:29:30,597
So Folder in 88 argues that even if

565
00:29:30,597 --> 00:29:30,597
connectionist models succeed, they would

566
00:29:30,597 --> 00:29:30,597
likely have some sort of simulation of a

567
00:29:30,597 --> 00:29:30,597
language of thoughts that exhibits

568
00:29:30,597 --> 00:29:30,597
conventionality in order to work like we

569
00:29:30,597 --> 00:29:46,613
do.

570
00:29:47,614 --> 00:29:51,618
And we'll see that it seemed like it's

571
00:29:51,618 --> 00:29:51,618
the case with the results we have this

572
00:29:51,618 --> 00:29:56,623
year.

573
00:29:58,625 --> 00:30:03,624
So considering those constraints and the

574
00:30:03,624 --> 00:30:03,624
initial field footer and the problem I

575
00:30:03,624 --> 00:30:03,624
was trying to solve, which is the unity

576
00:30:03,624 --> 00:30:03,624
problem for blue process theory, a major

577
00:30:03,624 --> 00:30:19,640
shift I had on the PhD thesis.

578
00:30:19,640 --> 00:30:23,644
And then I wrote a chapter on this book.

579
00:30:24,645 --> 00:30:27,648
The Chan Book of Founded Rationality with

580
00:30:27,648 --> 00:30:27,648
Keith.

581
00:30:28,649 --> 00:30:32,653
So we explain here in this book the

582
00:30:32,653 --> 00:30:32,653
chapter The Shift Power Structure, which

583
00:30:32,653 --> 00:30:32,653
is, I mean, it's not clear that a dual

584
00:30:32,653 --> 00:30:32,653
process theory theorists would adopt

585
00:30:32,653 --> 00:30:48,669
Folder, but it's still very similar.

586
00:30:50,671 --> 00:30:53,674
The way the received view of dual process

587
00:30:53,674 --> 00:30:53,674
theory works, which is type one, is a

588
00:30:53,674 --> 00:30:53,674
number of dumb heuristics blind modules

589
00:30:53,674 --> 00:31:00,675
and animal intelligence.

590
00:31:01,676 --> 00:31:04,679
And then type two is the real contextual

591
00:31:04,679 --> 00:31:04,679
and complex reasoning, the real human

592
00:31:04,679 --> 00:31:07,682
intelligence.

593
00:31:07,682 --> 00:31:12,687
So I tried to invert that power structure

594
00:31:12,687 --> 00:31:12,687
by saying that the contextual, predictive

595
00:31:12,687 --> 00:31:12,687
processing is a type one reasoning, which

596
00:31:12,687 --> 00:31:24,699
is giving us the basics to reason on.

597
00:31:25,700 --> 00:31:27,702
So our reasoning already comes with

598
00:31:27,702 --> 00:31:28,703
context.

599
00:31:28,703 --> 00:31:30,705
We don't need to reason the context with

600
00:31:30,705 --> 00:31:32,707
type two reasoning.

601
00:31:33,708 --> 00:31:36,711
We only use type two reasoning to fix

602
00:31:36,711 --> 00:31:36,711
some minor issues in predictive

603
00:31:36,711 --> 00:31:39,714
processing.

604
00:31:39,714 --> 00:31:43,718
And we use heuristic search because it's

605
00:31:43,718 --> 00:31:43,718
limited.

606
00:31:44,719 --> 00:31:46,721
We only use it when there's a lot of

607
00:31:46,721 --> 00:31:46,721
prediction error and predictions aren't

608
00:31:46,721 --> 00:31:50,725
working that well.

609
00:31:51,726 --> 00:31:54,729
And then we call for this heuristic

610
00:31:54,729 --> 00:31:54,729
search to handle whatever is missing to

611
00:31:54,729 --> 00:31:54,729
see if we can find a new solution that

612
00:31:54,729 --> 00:32:01,730
wasn't captured.

613
00:32:02,731 --> 00:32:05,734
So in this case the powerful system is

614
00:32:05,734 --> 00:32:05,734
the predictive processing and not the

615
00:32:05,734 --> 00:32:10,739
Type Two research.

616
00:32:16,745 --> 00:32:19,748
So that was the general idea for this

617
00:32:19,748 --> 00:32:19,748
shift.

618
00:32:20,749 --> 00:32:24,753
Now I'm going to say some hypotheses that

619
00:32:24,753 --> 00:32:26,755
stem from this shift.

620
00:32:26,755 --> 00:32:28,757
So t one processes deal with content

621
00:32:28,757 --> 00:32:28,757
encoded in the form of probability

622
00:32:28,757 --> 00:32:28,757
density functions, which is how

623
00:32:28,757 --> 00:32:28,757
predictive processing uses to represent

624
00:32:28,757 --> 00:32:28,757
the world, which means there is no symbol

625
00:32:28,757 --> 00:32:28,757
and no definite content but values, means,

626
00:32:28,757 --> 00:32:43,772
, standard deviation.

627
00:32:44,772 --> 00:32:46,775
Influenced by previous movements in

628
00:32:46,775 --> 00:32:46,775
previous world contingencies,

629
00:32:46,775 --> 00:32:46,775
manipulating prior information, biases

630
00:32:46,775 --> 00:32:46,775
the distribution to one or another

631
00:32:46,775 --> 00:32:46,775
direction closer to or further from a

632
00:32:46,775 --> 00:32:59,788
certain value.

633
00:32:59,788 --> 00:33:02,785
So people here in the active entrance

634
00:33:02,785 --> 00:33:05,788
institute we know this by heart.

635
00:33:05,788 --> 00:33:07,790
I'm not changing anything here, I'm just

636
00:33:07,790 --> 00:33:08,791
using what they say.

637
00:33:08,791 --> 00:33:11,794
Basically these functions are not stored

638
00:33:11,794 --> 00:33:11,794
in a memory bank but distributed from the

639
00:33:11,794 --> 00:33:11,794
responsible brain regions over to the

640
00:33:11,794 --> 00:33:11,794
external organs, body parts through

641
00:33:11,794 --> 00:33:20,803
neural connections.

642
00:33:21,804 --> 00:33:24,807
The values in the distribution do not

643
00:33:24,807 --> 00:33:24,807
represent objects directly indiscreetly

644
00:33:24,807 --> 00:33:24,807
they refer to distinct aspects of the

645
00:33:24,807 --> 00:33:24,807
input when perceptual systems are dealing

646
00:33:24,807 --> 00:33:24,807
with such objects this is aligned with

647
00:33:24,807 --> 00:33:24,807
the T one processes being easily biased

648
00:33:24,807 --> 00:33:24,807
when working with references to similar

649
00:33:24,807 --> 00:33:24,807
properties like similar numbers, objects,

650
00:33:24,807 --> 00:33:24,807
rhymes or pet names very often an

651
00:33:24,807 --> 00:33:24,807
incorrect value is picked from a

652
00:33:24,807 --> 00:33:49,832
distribution.

653
00:33:50,833 --> 00:33:53,836
An example that I have from this is when

654
00:33:53,836 --> 00:33:53,836
usually I have seen people confuse their

655
00:33:53,836 --> 00:33:53,836
youngest child with their dog name

656
00:33:53,836 --> 00:33:53,836
youngest child and when someone has

657
00:33:53,836 --> 00:33:53,836
another new child then the new youngest

658
00:33:53,836 --> 00:33:53,836
child is often confused with the dog

659
00:33:53,836 --> 00:33:53,836
because they are, I think, somewhat

660
00:33:53,836 --> 00:33:53,836
represented in similar distribution

661
00:33:53,836 --> 00:34:18,855
similar points of the distribution.

662
00:34:19,856 --> 00:34:21,858
This is also in line with claimants of

663
00:34:21,858 --> 00:34:21,858
embodied proposals that the world is not

664
00:34:21,858 --> 00:34:25,862
representative in symbols.

665
00:34:27,864 --> 00:34:30,867
T one processes are subpersonal and their

666
00:34:30,867 --> 00:34:30,867
predictions are made by the same systems

667
00:34:30,867 --> 00:34:34,871
which process perception.

668
00:34:34,871 --> 00:34:38,875
A clear example is that a judgment about

669
00:34:38,875 --> 00:34:38,875
a facial expression is related to the FFA

670
00:34:38,875 --> 00:34:38,875
which is a region brain that processes

671
00:34:38,875 --> 00:34:38,875
faces and this is also related to that

672
00:34:38,875 --> 00:34:38,875
judgment that conman may argue that when

673
00:34:38,875 --> 00:34:38,875
looking at someone we make judgments

674
00:34:38,875 --> 00:34:56,893
about their face.

675
00:34:57,894 --> 00:34:59,896
The idea is that perception is not

676
00:34:59,896 --> 00:34:59,896
passive but already comes with

677
00:34:59,896 --> 00:34:59,896
predictions and when in problem solving

678
00:34:59,896 --> 00:34:59,896
such prediction is precisely the Type One

679
00:34:59,896 --> 00:35:07,898
answer.

680
00:35:07,898 --> 00:35:11,902
So if we look back at the reasoning test

681
00:35:11,902 --> 00:35:11,902
all those Type One tests are likely

682
00:35:11,902 --> 00:35:18,909
stemming from predictions.

683
00:35:19,910 --> 00:35:23,914
I don't know what to claim that T one

684
00:35:23,914 --> 00:35:23,914
processes are purely perceptual if in

685
00:35:23,914 --> 00:35:23,914
contest to cognitive only that such

686
00:35:23,914 --> 00:35:23,914
predictions reasoning is that there's not

687
00:35:23,914 --> 00:35:23,914
a clear line between what is perception

688
00:35:23,914 --> 00:35:39,930
and cognitive.

689
00:35:39,930 --> 00:35:42,933
So I think the word you're just using

690
00:35:42,933 --> 00:35:46,937
different words for this is the problem.

691
00:35:46,937 --> 00:35:52,942
So just not getting into that convenience.

692
00:35:52,942 --> 00:35:52,942
.

693
00:35:52,943 --> 00:35:55,946
Example of judgment of negative spatial

694
00:35:55,946 --> 00:35:55,946
expression shows how this is expected of

695
00:35:55,946 --> 00:35:55,946
dual process theory like it's expected of

696
00:35:55,946 --> 00:35:55,946
dual process theory that Type One reason

697
00:35:55,946 --> 00:35:55,946
works under predictive processing, also

698
00:35:55,946 --> 00:35:55,946
in line with the claims of embody

699
00:35:55,946 --> 00:35:55,946
cognition that there is no sharp link

700
00:35:55,946 --> 00:36:15,960
between perception and reasoning.

701
00:36:16,961 --> 00:36:19,964
On the other hand, c two processing works

702
00:36:19,964 --> 00:36:19,964
like a classical machine for reasoning,

703
00:36:19,964 --> 00:36:19,964
such as General Problem Solver of Newman

704
00:36:19,964 --> 00:36:24,969
Simon.

705
00:36:25,969 --> 00:36:27,972
However, this classical machine only

706
00:36:27,972 --> 00:36:27,972
makes sense in the brain if it exists in

707
00:36:27,972 --> 00:36:31,976
a wider setup.

708
00:36:31,976 --> 00:36:34,979
Predictive processing network generating

709
00:36:34,979 --> 00:36:34,979
Type One responses like in Newell's

710
00:36:34,979 --> 00:36:34,979
Physical Symbol system, when facing a

711
00:36:34,979 --> 00:36:34,979
reasoning problem, T Two Processing opens

712
00:36:34,979 --> 00:36:34,979
a problem space containing an expression

713
00:36:34,979 --> 00:36:34,979
that designates the initial problem and

714
00:36:34,979 --> 00:36:34,979
an expression designates a solution which

715
00:36:34,979 --> 00:36:34,979
was produced by a probabilistic

716
00:36:34,979 --> 00:36:34,979
prediction having the initial expression

717
00:36:34,979 --> 00:36:34,979
and the predictive expression in the

718
00:36:34,979 --> 00:37:04,003
problem space.

719
00:37:04,003 --> 00:37:06,005
T Two Processing then uses its move

720
00:37:06,005 --> 00:37:06,005
generators to attempt to reduce

721
00:37:06,005 --> 00:37:06,005
differences between them and sometimes

722
00:37:06,005 --> 00:37:12,011
find different solutions in such bad.

723
00:37:12,011 --> 00:37:14,013
So let me just explain this a little bit

724
00:37:14,013 --> 00:37:15,014
better.

725
00:37:15,014 --> 00:37:18,017
When Newell was working in the 60s there

726
00:37:18,017 --> 00:37:18,017
they knew they couldn't search all

727
00:37:18,017 --> 00:37:18,017
possible space of a problem space because

728
00:37:18,017 --> 00:37:25,024
it wouldn't work.

729
00:37:26,025 --> 00:37:29,028
So they have this library search which

730
00:37:29,028 --> 00:37:29,028
works like a detective, let's ignore the

731
00:37:29,028 --> 00:37:29,028
problems which are likely wrong, just

732
00:37:29,028 --> 00:37:39,038
look at the spaces that might be right.

733
00:37:41,040 --> 00:37:44,043
What this predicting and reflecting

734
00:37:44,043 --> 00:37:44,043
framework does for a risk search is that

735
00:37:44,043 --> 00:37:44,043
the risk of search only works on boards

736
00:37:44,043 --> 00:37:44,043
already with the prediction that was sent

737
00:37:44,043 --> 00:37:57,056
from Type One processing.

738
00:37:57,056 --> 00:38:01,054
So it's like we only start reasoning and

739
00:38:01,054 --> 00:38:01,054
making searches based on the prior

740
00:38:01,054 --> 00:38:06,059
predictions we already had.

741
00:38:07,060 --> 00:38:10,063
And we only go further with this search

742
00:38:10,063 --> 00:38:13,066
when the predictions aren't working.

743
00:38:15,068 --> 00:38:19,072
And the main point here is that the risk

744
00:38:19,072 --> 00:38:19,072
search isn't searching a random space or

745
00:38:19,072 --> 00:38:26,079
even a pre programmed space.

746
00:38:26,079 --> 00:38:29,082
It's searching a space that was left out

747
00:38:29,082 --> 00:38:29,082
from the predictions that started in Type

748
00:38:29,082 --> 00:38:33,086
One processing.

749
00:38:34,087 --> 00:38:37,090
So that's how it doesn't go to the frame

750
00:38:37,090 --> 00:38:37,090
problem anymore because a large part of

751
00:38:37,090 --> 00:38:37,090
the contextual issue was also already

752
00:38:37,090 --> 00:38:47,100
solved by predictive processes.

753
00:38:49,102 --> 00:38:53,106
Okay, so that was the general hypothesis

754
00:38:53,106 --> 00:38:53,106
I presented to attempt to solve the unity

755
00:38:53,106 --> 00:38:53,106
problem, which is the problem of how Type

756
00:38:53,106 --> 00:38:53,106
One features go together and how Type Two

757
00:38:53,106 --> 00:39:06,113
features go together.

758
00:39:07,114 --> 00:39:10,117
And now I'm going to try to argue that

759
00:39:10,117 --> 00:39:10,117
this is likely the case, since that I can'

760
00:39:10,117 --> 00:39:16,123
't prove this is the case.

761
00:39:16,123 --> 00:39:19,126
But there are good reasons for us to

762
00:39:19,126 --> 00:39:19,126
think that Type One processes use

763
00:39:19,126 --> 00:39:19,126
predictive processing and that Type Two

764
00:39:19,126 --> 00:39:19,126
processes do not use predictive

765
00:39:19,126 --> 00:39:28,135
processing.

766
00:39:28,135 --> 00:39:31,138
Rather they need some kind of symbolic

767
00:39:31,138 --> 00:39:33,139
heuristic search.

768
00:39:35,141 --> 00:39:38,145
One thing that stems out of is a good

769
00:39:38,145 --> 00:39:38,145
explanation for the difference between

770
00:39:38,145 --> 00:39:38,145
implicit representation and explicit

771
00:39:38,145 --> 00:39:46,153
representation.

772
00:39:46,153 --> 00:39:50,157
If you look at the literature in

773
00:39:50,157 --> 00:39:50,157
psychology, you will see that it's very

774
00:39:50,157 --> 00:39:58,165
ambiguous in these definitions.

775
00:39:58,165 --> 00:40:00,161
Like people use implicit and explicit

776
00:40:00,161 --> 00:40:00,161
representations as the same thing as

777
00:40:00,161 --> 00:40:00,161
unconscious, conscious or fleeting

778
00:40:00,161 --> 00:40:00,161
graspable, or the same thing as automatic

779
00:40:00,161 --> 00:40:10,171
in control.

780
00:40:10,171 --> 00:40:12,173
And so when it goes to dual process

781
00:40:12,173 --> 00:40:12,173
theory, it's not very clear what they're

782
00:40:12,173 --> 00:40:12,173
saying in each case, but they use this

783
00:40:12,173 --> 00:40:21,182
expression and explicit a lot.

784
00:40:26,187 --> 00:40:29,190
How I think this model solves this is by

785
00:40:29,190 --> 00:40:29,190
saying that implicit representations are

786
00:40:29,190 --> 00:40:29,190
probability density functions while

787
00:40:29,190 --> 00:40:29,190
explicit representations are symbolic

788
00:40:29,190 --> 00:40:41,202
representations.

789
00:40:41,202 --> 00:40:43,203
Classical symbolic symbolic

790
00:40:43,203 --> 00:40:43,204
representation.

791
00:40:46,207 --> 00:40:49,210
Clark in his book sometimes mentions that

792
00:40:49,210 --> 00:40:49,210
we can have single peak distribution

793
00:40:49,210 --> 00:40:54,215
distributions.

794
00:40:54,215 --> 00:40:57,218
But the problem with having single peak

795
00:40:57,218 --> 00:40:57,218
distributions is that well, if you only

796
00:40:57,218 --> 00:40:57,218
have one peak on the distribution

797
00:40:57,218 --> 00:40:57,218
function then it doesn't have all these

798
00:40:57,218 --> 00:40:57,218
features of probabilistic representation.

799
00:40:57,218 --> 00:41:08,223
.

800
00:41:09,224 --> 00:41:12,227
It's more likely similar to a symbolic

801
00:41:12,227 --> 00:41:14,229
representation.

802
00:41:14,229 --> 00:41:17,232
The fact that having a single peak

803
00:41:17,232 --> 00:41:17,232
distribution might be the way that a

804
00:41:17,232 --> 00:41:17,232
probability representation turns to a

805
00:41:17,232 --> 00:41:26,241
symbolic representation.

806
00:41:27,242 --> 00:41:31,246
So these have different and important

807
00:41:31,246 --> 00:41:32,247
features.

808
00:41:32,247 --> 00:41:35,250
The probability representation is

809
00:41:35,250 --> 00:41:35,250
continuous, it's uncertain and ambiguous

810
00:41:35,250 --> 00:41:35,250
and that's why it's able to be sensitive

811
00:41:35,250 --> 00:41:43,258
to context.

812
00:41:44,259 --> 00:41:48,263
The mother example of part it can vary on

813
00:41:48,263 --> 00:41:49,264
retrieval.

814
00:41:49,264 --> 00:41:50,265
We can remember something and be

815
00:41:50,265 --> 00:41:52,267
different the second time.

816
00:41:54,269 --> 00:41:55,270
It's tied to priors.

817
00:41:55,270 --> 00:41:58,273
It works with statistical relations.

818
00:41:59,274 --> 00:42:03,272
Yeah, it's called in and depending on

819
00:42:03,272 --> 00:42:03,272
which value it's most probable at the

820
00:42:03,272 --> 00:42:10,279
time, it has a different outcome.

821
00:42:11,280 --> 00:42:14,283
So I argue that explicit representations

822
00:42:14,283 --> 00:42:14,283
are likely discrete, they are likely

823
00:42:14,283 --> 00:42:22,291
symbolic in nature, they're unambiguous.

824
00:42:22,291 --> 00:42:24,293
This is what allows us to disabilite in

825
00:42:24,293 --> 00:42:27,296
the first place they're stored reliably.

826
00:42:27,296 --> 00:42:31,300
If you have a reliable definition of it,

827
00:42:31,300 --> 00:42:31,300
you will likely remember it the same way

828
00:42:31,300 --> 00:42:36,305
the next time.

829
00:42:36,305 --> 00:42:39,308
It's arbitrary in the sense that it's not

830
00:42:39,308 --> 00:42:39,308
really coupled to the stimuli, in the

831
00:42:39,308 --> 00:42:39,308
sense that the perception of perception

832
00:42:39,308 --> 00:42:46,315
usually is.

833
00:42:47,316 --> 00:42:50,319
The statistical relations are related to

834
00:42:50,319 --> 00:42:51,320
aspects of the world.

835
00:42:53,322 --> 00:42:55,324
It's using compositionality and it's

836
00:42:55,324 --> 00:42:55,324
immutable in the sense that you can't

837
00:42:55,324 --> 00:42:59,328
change it.

838
00:42:59,328 --> 00:43:03,326
It's fixed value doesn't change.

839
00:43:06,329 --> 00:43:08,331
So that's the implicit versus explicit

840
00:43:08,331 --> 00:43:09,332
feature.

841
00:43:09,332 --> 00:43:11,334
You can have also explanations for

842
00:43:11,334 --> 00:43:11,334
automaticity in contrast to working

843
00:43:11,334 --> 00:43:15,338
memory.

844
00:43:15,338 --> 00:43:18,341
So automaticity concerns overlearn skills

845
00:43:18,341 --> 00:43:18,341
and overlearn skills are understood as

846
00:43:18,341 --> 00:43:18,341
skills that have become predictable in

847
00:43:18,341 --> 00:43:24,347
this framework.

848
00:43:25,348 --> 00:43:28,351
So we usually use this to explain like

849
00:43:28,351 --> 00:43:30,353
driving, riding a bicycle, riding.

850
00:43:32,355 --> 00:43:36,359
So when you're driving, if you don't know

851
00:43:36,359 --> 00:43:36,359
how to drive, you need certain statements,

852
00:43:36,359 --> 00:43:36,359
, like some statements about how you, how

853
00:43:36,359 --> 00:43:48,371
you should steer, right?

854
00:43:48,371 --> 00:43:50,373
You need to have those in mind.

855
00:43:50,373 --> 00:43:53,376
But then once you learn how to drive, it'

856
00:43:53,376 --> 00:43:53,376
's like your body already knows what it's

857
00:43:53,376 --> 00:43:53,376
doing and you don't even need to think

858
00:43:53,376 --> 00:43:58,381
about this.

859
00:43:58,381 --> 00:44:01,378
So this is the classical distinction in

860
00:44:01,378 --> 00:44:01,378
psychology.

861
00:44:01,378 --> 00:44:05,382
But I argue that the predictive

862
00:44:05,382 --> 00:44:05,382
processing theory makes a new

863
00:44:05,382 --> 00:44:12,389
enlightenment to this.

864
00:44:13,390 --> 00:44:16,393
So it's like our body is predicting what

865
00:44:16,393 --> 00:44:17,394
we're doing.

866
00:44:17,394 --> 00:44:18,395
So it's automatic.

867
00:44:18,395 --> 00:44:21,398
So it's an explanation for optimisticity.

868
00:44:21,398 --> 00:44:21,398
.

869
00:44:21,398 --> 00:44:27,404
But say that a dog certainly suddenly

870
00:44:27,404 --> 00:44:30,406
runs from his car.

871
00:44:30,407 --> 00:44:32,409
You obviously don't have a model for that.

872
00:44:32,409 --> 00:44:32,409
.

873
00:44:32,409 --> 00:44:35,412
So you're going to need to call in work

874
00:44:35,412 --> 00:44:35,412
in memory to solve some of those issues

875
00:44:35,412 --> 00:44:40,417
so you don't hit the dog.

876
00:44:41,418 --> 00:44:44,421
So everything works under prediction

877
00:44:44,421 --> 00:44:44,421
unless the model is very unreliable and

878
00:44:44,421 --> 00:44:44,421
then working memory is called in to solve

879
00:44:44,421 --> 00:44:53,430
some further issues.

880
00:44:54,431 --> 00:44:54,431
An.

881
00:44:54,431 --> 00:44:57,434
Interesting hypothesis I had on the PhD

882
00:44:57,434 --> 00:44:59,436
thesis which I never explored.

883
00:44:59,436 --> 00:45:04,435
I actually don't even think anyone read

884
00:45:04,435 --> 00:45:04,435
this part, but it seems to me to be a

885
00:45:04,435 --> 00:45:04,435
very interesting hypothesis in relation

886
00:45:04,435 --> 00:45:16,447
to free energy in active inference.

887
00:45:16,447 --> 00:45:19,450
So that's why I'm bringing it here in the

888
00:45:19,450 --> 00:45:19,450
Active Inference Institute that as we

889
00:45:19,450 --> 00:45:19,450
know, the brain attempts to minimize free

890
00:45:19,450 --> 00:45:26,457
energy by getting predictions right.

891
00:45:27,458 --> 00:45:30,461
Thus higher need of type two processes

892
00:45:30,461 --> 00:45:30,461
are related to higher free energy

893
00:45:30,461 --> 00:45:30,461
interpreting information in the sense

894
00:45:30,461 --> 00:45:30,461
that when the predictions are working

895
00:45:30,461 --> 00:45:30,461
clearly, then you don't need to have

896
00:45:30,461 --> 00:45:44,475
effort.

897
00:45:45,476 --> 00:45:48,479
But when there are a lot of prediction

898
00:45:48,479 --> 00:45:48,479
error, then you need effort because you

899
00:45:48,479 --> 00:45:48,479
call in the working memory to do a risk

900
00:45:48,479 --> 00:45:54,485
search.

901
00:45:54,485 --> 00:45:56,487
So to minimize a great amount, better

902
00:45:56,487 --> 00:45:56,487
amount of free energy will take more time

903
00:45:56,487 --> 00:45:59,490
and work.

904
00:46:00,485 --> 00:46:02,487
This is more effortful than having

905
00:46:02,487 --> 00:46:02,487
predictions ready that minimize free

906
00:46:02,487 --> 00:46:05,490
energy as quickly as possible.

907
00:46:06,491 --> 00:46:08,493
Before, when probabilities fail, the

908
00:46:08,493 --> 00:46:08,493
system needs to start risking

909
00:46:08,493 --> 00:46:12,497
possibilities.

910
00:46:12,497 --> 00:46:15,500
It searches for other possible solutions

911
00:46:15,500 --> 00:46:16,501
by means of heuristic search.

912
00:46:17,502 --> 00:46:20,505
Heuristic search would be related to more

913
00:46:20,505 --> 00:46:20,505
information and time because it does not

914
00:46:20,505 --> 00:46:24,509
have probable solutions ready.

915
00:46:24,509 --> 00:46:27,512
Instead it needs to investigate its state

916
00:46:27,512 --> 00:46:29,514
space almost from scratch.

917
00:46:29,514 --> 00:46:32,517
We say almost because it is heuristic.

918
00:46:32,517 --> 00:46:34,519
And hence it also will have tricks to get

919
00:46:34,519 --> 00:46:36,521
the correct solution faster.

920
00:46:36,521 --> 00:46:39,524
Unlike group force search, which would

921
00:46:39,524 --> 00:46:39,524
investigate the state space from

922
00:46:39,524 --> 00:46:39,524
beginning to end, we believe reports of

923
00:46:39,524 --> 00:46:39,524
effort would be related to executing more

924
00:46:39,524 --> 00:46:47,532
heuristic searches.

925
00:46:48,533 --> 00:46:51,535
Reports of effort by subjects would seem

926
00:46:51,535 --> 00:46:51,535
to be based on cognitive informational

927
00:46:51,535 --> 00:46:55,540
and physical constraints of reality.

928
00:46:55,540 --> 00:46:58,543
So that's why I think this is an

929
00:46:58,543 --> 00:46:58,543
interesting part of the hypothesis is

930
00:46:58,543 --> 00:46:58,543
because we have this psychological

931
00:46:58,543 --> 00:47:05,544
measure of effort.

932
00:47:05,544 --> 00:47:08,547
But if this is true, then we actually

933
00:47:08,547 --> 00:47:10,549
have a physical measure of effort.

934
00:47:11,550 --> 00:47:13,552
Like if people are working hard on the

935
00:47:13,552 --> 00:47:13,552
problem, that means there's a lot of free

936
00:47:13,552 --> 00:47:13,552
energy going on and they are having to

937
00:47:13,552 --> 00:47:13,552
use this the risk of search to fix some

938
00:47:13,552 --> 00:47:13,552
issues that the model isn't able to fix

939
00:47:13,552 --> 00:47:27,566
by itself.

940
00:47:32,571 --> 00:47:34,573
Yeah, so that's that's the effort part.

941
00:47:35,574 --> 00:47:39,578
So now going for the working memory part,

942
00:47:39,578 --> 00:47:39,578
what's interesting here is that in

943
00:47:39,578 --> 00:47:39,578
working memory, working memory, it's

944
00:47:39,578 --> 00:47:39,578
expected of it that it works like a

945
00:47:39,578 --> 00:47:53,591
classical computer.

946
00:47:54,593 --> 00:47:57,596
So working memory is a widely research

947
00:47:57,596 --> 00:47:57,596
topic in psychology, but in predictive

948
00:47:57,596 --> 00:47:57,596
processing it's rarely, rarely mentioned.

949
00:47:57,596 --> 00:48:06,599
.

950
00:48:06,599 --> 00:48:09,602
And I actually searched the predictive

951
00:48:09,602 --> 00:48:13,606
processing books, control F for working.

952
00:48:13,606 --> 00:48:18,611
And people rarely use this topic because

953
00:48:18,611 --> 00:48:22,615
it's not done by predictive processing.

954
00:48:22,615 --> 00:48:24,617
It has nothing to do with predictive

955
00:48:24,617 --> 00:48:25,618
processing.

956
00:48:26,619 --> 00:48:28,621
It's actually an issue for predictive

957
00:48:28,621 --> 00:48:28,621
processing because well, if you're going

958
00:48:28,621 --> 00:48:28,621
to eliminate classical symbolic

959
00:48:28,621 --> 00:48:28,621
processing altogether, then we need a

960
00:48:28,621 --> 00:48:28,621
predictive processing explanation of

961
00:48:28,621 --> 00:48:39,632
working memory.

962
00:48:39,632 --> 00:48:42,635
And so far I haven't found one and I don'

963
00:48:42,635 --> 00:48:43,636
't think there will be one.

964
00:48:44,636 --> 00:48:47,640
But this also could be just ignorance of

965
00:48:47,640 --> 00:48:47,640
mine.

966
00:48:47,640 --> 00:48:50,643
Maybe there's some paper there which I

967
00:48:50,643 --> 00:48:52,645
haven't found might prove me wrong.

968
00:48:52,645 --> 00:48:57,650
But in any way, working memory is very

969
00:48:57,650 --> 00:48:57,650
aligned with what Turing was thinking

970
00:48:57,650 --> 00:48:57,650
when he was making the connection between

971
00:48:57,650 --> 00:49:09,656
machines and minds.

972
00:49:10,657 --> 00:49:15,662
So if this is a sentence of Alan Turing

973
00:49:15,662 --> 00:49:15,662
explaining his computer, right, the

974
00:49:15,662 --> 00:49:22,669
Turing machine.

975
00:49:22,669 --> 00:49:26,673
And if we change the word computer to

976
00:49:26,673 --> 00:49:29,676
work in memory, it clearly works.

977
00:49:29,676 --> 00:49:32,679
It's almost like it's talking about the

978
00:49:32,679 --> 00:49:32,679
same thing, when of course, if we change

979
00:49:32,679 --> 00:49:32,679
the word here, computer, to predictive

980
00:49:32,679 --> 00:49:32,679
processing, it's clearly not talking

981
00:49:32,679 --> 00:49:40,687
about the same thing.

982
00:49:40,687 --> 00:49:42,689
So the behavior of the computer or

983
00:49:42,689 --> 00:49:42,689
working memory at any moment is

984
00:49:42,689 --> 00:49:42,689
determined by the symbols which it's

985
00:49:42,689 --> 00:49:42,689
observing in his state of mind at that

986
00:49:42,689 --> 00:49:50,697
moment.

987
00:49:50,697 --> 00:49:52,699
We may suppose that there is a bound b to

988
00:49:52,699 --> 00:49:52,699
the number of symbols or squares which

989
00:49:52,699 --> 00:49:57,704
the computer can observe at one moment.

990
00:49:57,704 --> 00:50:00,701
If he wishes to observe more, he must use

991
00:50:00,701 --> 00:50:02,703
successive observations.

992
00:50:02,703 --> 00:50:04,705
We will also suppose that the number of

993
00:50:04,705 --> 00:50:04,705
states of mind which can be taken into

994
00:50:04,705 --> 00:50:08,709
account is finite.

995
00:50:08,709 --> 00:50:10,711
The reason for this are the same

996
00:50:10,711 --> 00:50:10,711
characters as those which restrict the

997
00:50:10,711 --> 00:50:14,715
numbers of symbols.

998
00:50:15,716 --> 00:50:17,718
If we admitted an infinity of states of

999
00:50:17,718 --> 00:50:17,718
mind, some of them would be arbitrarily

1000
00:50:17,718 --> 00:50:22,723
close and would be confused.

1001
00:50:22,723 --> 00:50:25,726
Again, the restriction is not one which

1002
00:50:25,726 --> 00:50:25,726
is seriously affect computation since the

1003
00:50:25,726 --> 00:50:25,726
use of more complicated states of mind

1004
00:50:25,726 --> 00:50:25,726
can be avoided by writing more symbols on

1005
00:50:25,726 --> 00:50:34,735
the tape.

1006
00:50:34,735 --> 00:50:37,738
So this is very similar to what we think

1007
00:50:37,738 --> 00:50:37,738
about when we're talking about working

1008
00:50:37,738 --> 00:50:37,738
memory, not only because we were instance

1009
00:50:37,738 --> 00:50:37,738
by Turing, but because of the evidence

1010
00:50:37,738 --> 00:50:37,738
that comes from working memory is very

1011
00:50:37,738 --> 00:50:37,738
similar to what suspected of this sort of

1012
00:50:37,738 --> 00:50:50,751
machine.

1013
00:50:51,752 --> 00:50:54,755
Finally, we have speed.

1014
00:50:54,755 --> 00:50:59,760
So predictive processing has various

1015
00:50:59,760 --> 00:50:59,760
strategies to make the processing be

1016
00:50:59,760 --> 00:51:06,761
faster as fast as possible.

1017
00:51:07,762 --> 00:51:11,766
While that's not true for symbolic

1018
00:51:11,766 --> 00:51:11,766
classical AI, so yeah, Clark says cheap,

1019
00:51:11,766 --> 00:51:11,766
fast world exploiting action rather than

1020
00:51:11,766 --> 00:51:11,766
the pursuit of truth optimality and

1021
00:51:11,766 --> 00:51:11,766
deductive inference is now the key

1022
00:51:11,766 --> 00:51:30,785
organizing principle.

1023
00:51:30,785 --> 00:51:33,788
When he's talking about predictive

1024
00:51:33,788 --> 00:51:33,788
processing, the predictive processor is

1025
00:51:33,788 --> 00:51:33,788
always taking certain bets about what the

1026
00:51:33,788 --> 00:51:33,788
current state of the world implies losing

1027
00:51:33,788 --> 00:51:44,799
accuracy, compensation for speed.

1028
00:51:45,800 --> 00:51:49,804
We also have predictive coding in the

1029
00:51:49,804 --> 00:51:52,807
more strict sense.

1030
00:51:54,809 --> 00:51:56,811
So by predictive coding we mean

1031
00:51:56,811 --> 00:51:56,811
specifically the property of the system

1032
00:51:56,811 --> 00:51:56,811
to consider from the world only stimulant

1033
00:51:56,811 --> 00:51:56,811
which result in greater prediction error.

1034
00:51:56,811 --> 00:52:03,812
.

1035
00:52:03,812 --> 00:52:06,815
So there's also a filter there which

1036
00:52:06,815 --> 00:52:06,815
allows for speed in perception and focus.

1037
00:52:06,815 --> 00:52:10,819
.

1038
00:52:10,819 --> 00:52:12,821
Beyond predictive prediction relevant

1039
00:52:12,821 --> 00:52:12,821
stimulant only permits the agent to

1040
00:52:12,821 --> 00:52:12,821
quickly decide forces of action and

1041
00:52:12,821 --> 00:52:19,828
select amongst the possible forces.

1042
00:52:20,829 --> 00:52:24,833
So predictive processing is tailored to

1043
00:52:24,833 --> 00:52:27,835
be fast, necessarily.

1044
00:52:27,836 --> 00:52:30,839
Well, that's not the case for symbolic AI

1045
00:52:30,839 --> 00:52:30,839
and so that's the explanation for the

1046
00:52:30,839 --> 00:52:34,843
difference in speed.

1047
00:52:35,843 --> 00:52:37,846
So beyond that, the T two processes need

1048
00:52:37,846 --> 00:52:40,849
to figure out the solutions online.

1049
00:52:40,849 --> 00:52:42,851
So it's different if you have a prior

1050
00:52:42,851 --> 00:52:42,851
prediction that would say to you what the

1051
00:52:42,851 --> 00:52:42,851
answer is from having to search its place

1052
00:52:42,851 --> 00:52:48,857
from scratch.

1053
00:52:49,857 --> 00:52:49,858
Right.

1054
00:52:49,858 --> 00:52:52,861
There is also another issue that the

1055
00:52:52,861 --> 00:52:52,861
biological brains certainly were not

1056
00:52:52,861 --> 00:52:56,865
built for.

1057
00:52:56,865 --> 00:52:59,868
A serial heuristic search photo pilotians

1058
00:52:59,868 --> 00:52:59,868
say that the morals that the absolute

1059
00:52:59,868 --> 00:52:59,868
speed of a process is a property part

1060
00:52:59,868 --> 00:53:07,870
excellence of its implementation.

1061
00:53:08,871 --> 00:53:10,873
And since the brain does not have a

1062
00:53:10,873 --> 00:53:10,873
symbolic processor implemented, if we do

1063
00:53:10,873 --> 00:53:10,873
use some sort of heuristic search, it's

1064
00:53:10,873 --> 00:53:19,882
like a different adaptation.

1065
00:53:19,882 --> 00:53:22,885
It's not what the brain is used to.

1066
00:53:22,885 --> 00:53:25,888
Inserting problem spaces is slower than

1067
00:53:25,888 --> 00:53:27,890
having a problem outcome ready.

1068
00:53:29,892 --> 00:53:35,898
Okay, so that was the work I did in my

1069
00:53:35,898 --> 00:53:36,899
PhD thesis.

1070
00:53:37,900 --> 00:53:43,906
And those one, those were one of the main

1071
00:53:43,906 --> 00:53:43,906
reasons why I think this framework is

1072
00:53:43,906 --> 00:53:43,906
good for explaining the differences in

1073
00:53:43,906 --> 00:53:58,921
features of dual process theory.

1074
00:53:59,922 --> 00:54:04,921
And I actually wrote a prediction that I

1075
00:54:04,921 --> 00:54:04,921
recently learned that I wrote this

1076
00:54:04,921 --> 00:54:11,928
because I, I forgot I wrote this.

1077
00:54:11,928 --> 00:54:14,931
And then I went back to to the thesis and

1078
00:54:14,931 --> 00:54:14,931
and I said, well, I actually said this

1079
00:54:14,931 --> 00:54:19,936
would happen and it kind of did.

1080
00:54:19,936 --> 00:54:23,940
So that was one of the reasons why I

1081
00:54:23,940 --> 00:54:27,944
renewed my interest in my PhD thesis.

1082
00:54:27,944 --> 00:54:30,947
So what I said was predictive processing

1083
00:54:30,947 --> 00:54:30,947
system would be subject to bias from lack

1084
00:54:30,947 --> 00:54:30,947
of compositionality such as mistakes in

1085
00:54:30,947 --> 00:54:30,947
connectivity, failures in noticing

1086
00:54:30,947 --> 00:54:30,947
necessary character, formal rules and so

1087
00:54:30,947 --> 00:54:41,958
on.

1088
00:54:41,958 --> 00:54:44,961
Precisely the type of mistake, type one

1089
00:54:44,961 --> 00:54:45,962
processing incurs.

1090
00:54:45,962 --> 00:54:48,965
And so at that point we didn't have large

1091
00:54:48,965 --> 00:54:49,966
language models.

1092
00:54:49,966 --> 00:54:53,970
In fact, the attention is all unique.

1093
00:54:53,970 --> 00:54:55,972
Paper came out in the same year.

1094
00:54:56,973 --> 00:55:00,971
And then I noticed that this could be

1095
00:55:00,971 --> 00:55:00,971
seen as a prediction of the type of

1096
00:55:00,971 --> 00:55:00,971
issues that large language models are

1097
00:55:00,971 --> 00:55:08,979
facing, that's reliability.

1098
00:55:09,980 --> 00:55:12,983
Hard time keeping the order of units or

1099
00:55:12,983 --> 00:55:12,983
steps in complex reasoning or math

1100
00:55:12,983 --> 00:55:12,983
straight, hard time letting go of priors.

1101
00:55:12,983 --> 00:55:18,989
.

1102
00:55:18,989 --> 00:55:22,993
And to my what really motivated me to

1103
00:55:22,993 --> 00:55:22,993
start working on this scan was that they

1104
00:55:22,993 --> 00:55:22,993
solved this precisely by using dual

1105
00:55:22,993 --> 00:55:22,993
process theory and t two agents they're

1106
00:55:22,993 --> 00:55:34,005
calling.

1107
00:55:35,006 --> 00:55:38,009
So what they're doing now to solve these

1108
00:55:38,009 --> 00:55:38,009
reliability issues is adding a heuristic

1109
00:55:38,009 --> 00:55:38,009
search to the genetic model and it's

1110
00:55:38,009 --> 00:55:47,018
having awesome results.

1111
00:55:47,018 --> 00:55:50,021
This is just this year 2023.

1112
00:55:52,023 --> 00:55:55,026
So the more they develop these type two

1113
00:55:55,026 --> 00:55:55,026
agents to answer these reliability

1114
00:55:55,026 --> 00:55:55,026
problems of large language models, the

1115
00:55:55,026 --> 00:55:55,026
more the large language models are

1116
00:55:55,026 --> 00:56:08,033
getting good at stuff.

1117
00:56:09,034 --> 00:56:13,038
So this is a paper, I got the images from

1118
00:56:13,038 --> 00:56:13,038
the paper tree of Socks, right, which is

1119
00:56:13,038 --> 00:56:13,038
one of the last ones, probably came out

1120
00:56:13,038 --> 00:56:13,038
about two months ago, something like that.

1121
00:56:13,038 --> 00:56:25,050
.

1122
00:56:25,050 --> 00:56:29,054
The way they're doing this implementation

1123
00:56:29,054 --> 00:56:29,054
is they're taking the results outputs of

1124
00:56:29,054 --> 00:56:29,054
the large language model and they're

1125
00:56:29,054 --> 00:56:29,054
doing further reasoning on it and then

1126
00:56:29,054 --> 00:56:29,054
feeding the large language model with the

1127
00:56:29,054 --> 00:56:45,070
result of the further reasoning.

1128
00:56:46,071 --> 00:56:48,073
The further reasoning they're doing is

1129
00:56:48,073 --> 00:56:51,076
very similar to the classic symbolic AI.

1130
00:56:52,077 --> 00:56:55,080
So here they explain there's a simple

1131
00:56:55,080 --> 00:56:55,080
input output prompting here, which is

1132
00:56:55,080 --> 00:56:55,080
simply inputting a prompt and the large

1133
00:56:55,080 --> 00:57:03,082
language model will give you an output.

1134
00:57:04,083 --> 00:57:07,086
And as we know, these outputs sometimes

1135
00:57:07,086 --> 00:57:09,088
are very biased.

1136
00:57:10,089 --> 00:57:12,091
There's hallucination on it, they're not

1137
00:57:12,091 --> 00:57:14,093
reliable, all that stuff.

1138
00:57:15,094 --> 00:57:19,098
And last year they figured out that

1139
00:57:19,098 --> 00:57:19,098
through chain of thought prompting we

1140
00:57:19,098 --> 00:57:19,098
could also already increase the

1141
00:57:19,098 --> 00:57:28,107
reliability of these models.

1142
00:57:28,107 --> 00:57:32,111
So you give an input and just as we do,

1143
00:57:32,111 --> 00:57:32,111
the better prompting when we go to

1144
00:57:32,111 --> 00:57:32,111
Chattbc and say, I'll do this better fix

1145
00:57:32,111 --> 00:57:32,111
this issue, solve this step by step, some

1146
00:57:32,111 --> 00:57:32,111
of the commands that are good for Chatter,

1147
00:57:32,111 --> 00:57:32,111
, PT and Chain of thought prompting

1148
00:57:32,111 --> 00:57:32,111
automated some of that, some of those

1149
00:57:32,111 --> 00:57:53,132
reasoning steps.

1150
00:57:53,132 --> 00:57:57,136
And then large language models were

1151
00:57:57,136 --> 00:58:00,133
working much better after this.

1152
00:58:00,133 --> 00:58:01,134
So this was last year.

1153
00:58:01,134 --> 00:58:05,138
And then last year this started a new

1154
00:58:05,138 --> 00:58:05,138
trend of research to make these agents

1155
00:58:05,138 --> 00:58:05,138
coupled to large language model output

1156
00:58:05,138 --> 00:58:05,138
linguistic outputs back into the large

1157
00:58:05,138 --> 00:58:05,138
language models to see how they can get

1158
00:58:05,138 --> 00:58:23,156
better.

1159
00:58:25,158 --> 00:58:28,161
The self consistency one, you give the

1160
00:58:28,161 --> 00:58:28,161
input and then there are various

1161
00:58:28,161 --> 00:58:32,165
solutions.

1162
00:58:36,169 --> 00:58:39,172
The self consistency forces the large

1163
00:58:39,172 --> 00:58:39,172
language model to try different solutions

1164
00:58:39,172 --> 00:58:45,178
and then you get a majority vote.

1165
00:58:45,178 --> 00:58:49,182
So if you have like five similar answers,

1166
00:58:49,182 --> 00:58:49,182
some different paths, then this one

1167
00:58:49,182 --> 00:58:49,182
SPicked over the one which is less

1168
00:58:49,182 --> 00:58:58,191
popular.

1169
00:58:59,192 --> 00:59:03,190
And the Tree of Thoughts, which I thought

1170
00:59:03,190 --> 00:59:03,190
was most similar to the proposal I had in

1171
00:59:03,190 --> 00:59:03,190
psychology, in Duprose theory, is the one

1172
00:59:03,190 --> 00:59:13,200
which opens up a problem space.

1173
00:59:14,201 --> 00:59:16,203
It's actually pretty much what I just

1174
00:59:16,203 --> 00:59:17,204
explained.

1175
00:59:17,204 --> 00:59:20,207
In the case of psychology, they open up a

1176
00:59:20,207 --> 00:59:20,207
problem space and they start to search

1177
00:59:20,207 --> 00:59:20,207
possible solutions which are better than

1178
00:59:20,207 --> 00:59:20,207
the original and they feed that back into

1179
00:59:20,207 --> 00:59:20,207
the predictive processing, sorry, the

1180
00:59:20,207 --> 00:59:20,207
generative model, the large language

1181
00:59:20,207 --> 00:59:38,225
model.

1182
00:59:39,226 --> 00:59:44,231
And then this one is currently one of the

1183
00:59:44,231 --> 00:59:46,233
best ones they have.

1184
00:59:48,235 --> 00:59:54,241
So here I made a general diagram of how

1185
00:59:54,241 --> 00:59:56,243
this would work.

1186
00:59:57,244 --> 01:00:00,480
It's not psychology nor AI.

1187
01:00:00,560 --> 01:00:03,207
It's like what's singular in the two?

1188
01:00:04,936 --> 01:00:08,210
So we have a generative model here using

1189
01:00:08,210 --> 01:00:10,175
probability density functions.

1190
01:00:11,224 --> 01:00:13,486
This looks more like a traditional

1191
01:00:13,486 --> 01:00:13,486
cognitive cognitive connectionist model

1192
01:00:13,486 --> 01:00:19,024
than a predictive processing.

1193
01:00:21,209 --> 01:00:23,404
It's just the diagram is not that good.

1194
01:00:23,413 --> 01:00:25,628
I'll talk a little about that.

1195
01:00:28,947 --> 01:00:31,283
We generate an answer here, which is to

1196
01:00:31,283 --> 01:00:31,283
the problem here, and we get a linguistic

1197
01:00:31,283 --> 01:00:36,713
output.

1198
01:00:36,720 --> 01:00:39,002
And this linguistic output then goes for

1199
01:00:39,002 --> 01:00:39,002
an heuristic search using symbolic

1200
01:00:39,002 --> 01:00:42,369
representations.

1201
01:00:44,539 --> 01:00:47,840
So if you get a better answer here, feeds

1202
01:00:47,840 --> 01:00:50,142
it back to the Genitive model and so on.

1203
01:00:51,194 --> 01:00:53,478
Basically, the knowledge part is the

1204
01:00:53,478 --> 01:00:53,478
Genitive model and the heuristic search

1205
01:00:53,478 --> 01:00:53,478
is simply using more steps to see to

1206
01:00:53,478 --> 01:01:01,617
prompt it back.

1207
01:01:01,627 --> 01:01:04,990
It's more like a prompting scheme than a

1208
01:01:04,990 --> 01:01:04,990
new knowledge scheme, which is both true

1209
01:01:04,990 --> 01:01:04,990
for the dual process theory model I

1210
01:01:04,990 --> 01:01:04,990
created and the heuristic search that's

1211
01:01:04,990 --> 01:01:04,990
happening here on the T, two agents for

1212
01:01:04,990 --> 01:01:19,466
LLMs.

1213
01:01:21,619 --> 01:01:24,900
A better way to do this, and I think Tree

1214
01:01:24,900 --> 01:01:24,900
of Thoughts does implementate this, is

1215
01:01:24,900 --> 01:01:24,900
that the input layers are trying to

1216
01:01:24,900 --> 01:01:30,563
predict the thoughts.

1217
01:01:31,628 --> 01:01:33,894
It's a bit more like predictive process

1218
01:01:33,894 --> 01:01:35,058
than the other diagram.

1219
01:01:37,204 --> 01:01:40,512
And so they're trying to predict the

1220
01:01:40,512 --> 01:01:40,512
thoughts and by open up this searching

1221
01:01:40,512 --> 01:01:46,017
this problem space during these steps.

1222
01:01:47,026 --> 01:01:51,066
We already have new prompts here for the

1223
01:01:51,066 --> 01:01:53,083
Gentrific model.

1224
01:01:55,104 --> 01:01:57,127
So if you take predictive processing

1225
01:01:57,127 --> 01:01:57,127
seriously, and this does happen in the

1226
01:01:57,127 --> 01:02:02,112
brain, more likely like this.

1227
01:02:02,117 --> 01:02:05,147
I do think that the Tree of Thoughts

1228
01:02:05,147 --> 01:02:05,147
paper does say that the intermediary

1229
01:02:05,147 --> 01:02:05,147
processes interfere in the searching on

1230
01:02:05,147 --> 01:02:13,221
the aliens.

1231
01:02:14,236 --> 01:02:18,273
Finally, I thought maybe we could suggest

1232
01:02:18,273 --> 01:02:18,273
something from psychology for these

1233
01:02:18,273 --> 01:02:18,273
models to keep growing, working

1234
01:02:18,273 --> 01:02:18,273
differently based on what we know from

1235
01:02:18,273 --> 01:02:29,382
psychology.

1236
01:02:29,384 --> 01:02:32,416
So some things that are not implemented

1237
01:02:32,416 --> 01:02:32,416
yet is that executive functioning have

1238
01:02:32,416 --> 01:02:32,416
these updating abilities in intelligent

1239
01:02:32,416 --> 01:02:32,416
forgetting, which is related to insight

1240
01:02:32,416 --> 01:02:43,528
problem solving.

1241
01:02:43,529 --> 01:02:46,557
It's when you forget something, but it

1242
01:02:46,557 --> 01:02:46,557
means, like, you forget the prior,

1243
01:02:46,557 --> 01:02:49,589
right?

1244
01:02:50,590 --> 01:02:54,630
You're using a different prior, so you're

1245
01:02:54,630 --> 01:02:56,656
able to let go of biases.

1246
01:02:56,657 --> 01:02:57,663
Right.

1247
01:02:58,671 --> 01:03:00,631
There's also work on thinking

1248
01:03:00,631 --> 01:03:02,658
dispositions, which could be relevant.

1249
01:03:04,678 --> 01:03:07,707
Sentences like this belief should always

1250
01:03:07,707 --> 01:03:07,707
be revised in response to new information

1251
01:03:07,707 --> 01:03:11,748
of our evidence.

1252
01:03:12,750 --> 01:03:14,779
So these would be, like, imperative

1253
01:03:14,779 --> 01:03:14,779
linguistic knowledge, which would have to

1254
01:03:14,779 --> 01:03:19,824
be added on.

1255
01:03:19,825 --> 01:03:22,858
I don't know where, but likely would help

1256
01:03:22,858 --> 01:03:23,862
work.

1257
01:03:23,864 --> 01:03:26,892
There's a lot of thinking dispositions

1258
01:03:26,892 --> 01:03:26,892
that are relevant to our successful

1259
01:03:26,892 --> 01:03:29,927
reasoning.

1260
01:03:29,927 --> 01:03:31,946
And this is just one example.

1261
01:03:33,959 --> 01:03:35,980
The literature knows a lot more examples

1262
01:03:35,980 --> 01:03:36,992
that could be relevant.

1263
01:03:37,003 --> 01:03:40,034
We have in creativity research, we have

1264
01:03:40,034 --> 01:03:40,034
this generative phase, which is similar

1265
01:03:40,034 --> 01:03:40,034
to what the generative models are doing,

1266
01:03:40,034 --> 01:03:40,034
but we do not have the exploratory

1267
01:03:40,034 --> 01:03:40,034
processes going on, which is common in

1268
01:03:40,034 --> 01:03:55,188
the creativity research literature.

1269
01:03:56,197 --> 01:03:59,222
But it's what we do with, for instance,

1270
01:03:59,222 --> 01:03:59,222
mid journey when we're doing better

1271
01:03:59,222 --> 01:04:02,191
prompts.

1272
01:04:03,202 --> 01:04:06,232
So maybe using these exploratory

1273
01:04:06,232 --> 01:04:06,232
processes from the creativity research,

1274
01:04:06,232 --> 01:04:06,232
we can also automate the exploratory part

1275
01:04:06,232 --> 01:04:06,232
of the creativity processes that these

1276
01:04:06,232 --> 01:04:06,232
image generators are getting to, and not

1277
01:04:06,232 --> 01:04:23,408
just the generative part.

1278
01:04:24,412 --> 01:04:27,442
And finally, I think the embodiment, you

1279
01:04:27,442 --> 01:04:27,442
don't have robots or anything like that

1280
01:04:27,442 --> 01:04:32,497
that do this sort of reasoning.

1281
01:04:32,498 --> 01:04:36,534
So AI on the computer and also stuff from

1282
01:04:36,534 --> 01:04:36,534
active entrance, which was not considered

1283
01:04:36,534 --> 01:04:36,534
slightly related to embodiments in the

1284
01:04:36,534 --> 01:04:36,534
sense of navigating the world and

1285
01:04:36,534 --> 01:04:48,655
predicting the world.

1286
01:04:48,657 --> 01:04:52,693
That stuff is far from happening in the

1287
01:04:52,693 --> 01:04:55,722
traditional Galileans we have.

1288
01:04:56,729 --> 01:04:58,754
So, yeah, that's a lot of stuff.

1289
01:04:59,761 --> 01:05:02,734
Sand, maybe I hope you guys have some

1290
01:05:02,734 --> 01:05:03,743
comments.

1291
01:05:03,749 --> 01:05:08,794
As I said, I haven't been able to talk

1292
01:05:08,794 --> 01:05:08,794
about this to anyone, and I do think I

1293
01:05:08,794 --> 01:05:08,794
have some, at least some maybe some

1294
01:05:08,794 --> 01:05:21,925
relevant stuff that I've published.

1295
01:05:21,926 --> 01:05:24,953
But since I'm not famous and I don't have

1296
01:05:24,953 --> 01:05:24,953
people to talk to, it's mostly gone

1297
01:05:24,953 --> 01:05:27,985
unnoticed.

1298
01:05:27,987 --> 01:05:29,007
So thanks for being stay tuned.

1299
01:05:32,039 --> 01:05:33,046
Daniel: Thank you.

1300
01:05:33,047 --> 01:05:34,059
Awesome presentation.

1301
01:05:35,059 --> 01:05:35,068
All right!

1302
01:05:35,068 --> 01:05:38,092
Nick, it'd be awesome to hear your introduction

1303
01:05:38,092 --> 01:05:38,092
and then take it wherever you'd like to

1304
01:05:38,092 --> 01:05:40,117
go.

1305
01:05:42,131 --> 01:05:42,135
Nick Byrd: Sure.

1306
01:05:42,135 --> 01:05:45,164
So first, thanks, Samuel, for interesting

1307
01:05:45,164 --> 01:05:45,168
talk.

1308
01:05:45,169 --> 01:05:47,188
And I've been really pleased to find your

1309
01:05:47,188 --> 01:05:48,193
research.

1310
01:05:48,194 --> 01:05:51,221
I guess maybe I'm supposed to say

1311
01:05:51,221 --> 01:05:54,252
something about who I am, where I am.

1312
01:05:54,253 --> 01:05:55,264
So I'm Nick Bird.

1313
01:05:55,264 --> 01:05:57,284
I'm at the Stevens Institute of

1314
01:05:57,284 --> 01:05:57,284
Technology in the New York City

1315
01:05:57,284 --> 01:05:57,284
metropolitan area in a department that's

1316
01:05:57,284 --> 01:06:04,290
kind of interdisciplinary.

1317
01:06:04,290 --> 01:06:05,304
So we have, like, philosophy and

1318
01:06:05,304 --> 01:06:05,304
quantitative social science, but also,

1319
01:06:05,304 --> 01:06:05,304
like, people doing, like, music and

1320
01:06:05,304 --> 01:06:05,304
visual arts and all sorts of other things.

1321
01:06:05,304 --> 01:06:10,355
.

1322
01:06:11,360 --> 01:06:13,383
I tend to do more of the philosophy and

1323
01:06:13,383 --> 01:06:15,404
quantitative social science stuff.

1324
01:06:16,417 --> 01:06:19,448
So I'm kind of also interested in human

1325
01:06:19,448 --> 01:06:20,457
reasoning.

1326
01:06:20,457 --> 01:06:23,488
And so much of the people I'm reading and

1327
01:06:23,488 --> 01:06:23,488
citing and drawing on in my research are

1328
01:06:23,488 --> 01:06:23,488
a lot of the people that you saw in the

1329
01:06:23,488 --> 01:06:23,488
opening slides that we saw people like

1330
01:06:23,488 --> 01:06:23,488
was on and Conneman Tversky and Mercier

1331
01:06:23,488 --> 01:06:23,488
and Sperber and a lot of the other people

1332
01:06:23,488 --> 01:06:40,650
that were cited.

1333
01:06:40,651 --> 01:06:42,670
I think Samuel does a really good job to

1334
01:06:42,670 --> 01:06:42,670
incorporate the kind of like shoulders of

1335
01:06:42,670 --> 01:06:42,670
giants that we're standing on in terms of

1336
01:06:42,670 --> 01:06:47,726
the philosophy of mind, right.

1337
01:06:47,726 --> 01:06:50,753
The voters and the Pilitians and these

1338
01:06:50,753 --> 01:06:50,759
people.

1339
01:06:52,774 --> 01:06:54,796
And one thing that I think is on

1340
01:06:54,796 --> 01:06:54,796
everybody's mind these days is these

1341
01:06:54,796 --> 01:06:58,836
large language models.

1342
01:06:58,836 --> 01:07:01,802
So I'm really glad to see how this dual

1343
01:07:01,802 --> 01:07:01,802
process theory that emerged from research

1344
01:07:01,802 --> 01:07:01,802
like economan and diversky's on these

1345
01:07:01,802 --> 01:07:01,802
problems like the conjunction fallacy

1346
01:07:01,802 --> 01:07:01,802
problems or the waste on selection guard

1347
01:07:01,802 --> 01:07:01,802
task or the conjunction fallacy tasks,

1348
01:07:01,802 --> 01:07:01,802
how that has been applied to models like

1349
01:07:01,802 --> 01:07:01,802
large language models and what we can

1350
01:07:01,802 --> 01:07:20,993
learn from that.

1351
01:07:20,993 --> 01:07:22,018
And then I think the coolest thing about

1352
01:07:22,018 --> 01:07:22,018
the preprint print is how we could kind

1353
01:07:22,018 --> 01:07:22,018
of port back and forth the learnings from

1354
01:07:22,018 --> 01:07:22,018
both the computer science and the

1355
01:07:22,018 --> 01:07:22,018
cognitive science to improve one another,

1356
01:07:22,018 --> 01:07:33,121
right?

1357
01:07:33,121 --> 01:07:35,146
So I think that one of the neat things

1358
01:07:35,146 --> 01:07:35,146
about this preprint that Samuel was

1359
01:07:35,146 --> 01:07:35,146
giving us at the end is that we're kind

1360
01:07:35,146 --> 01:07:35,146
of taking what we think is a model of

1361
01:07:35,146 --> 01:07:35,146
reflective reasoning that we represent

1362
01:07:35,146 --> 01:07:35,146
certain parts of a problem and maybe

1363
01:07:35,146 --> 01:07:35,146
reason more carefully and effortfully

1364
01:07:35,146 --> 01:07:35,146
about them in certain situations in ways

1365
01:07:35,146 --> 01:07:54,335
that might help us.

1366
01:07:54,336 --> 01:07:56,357
And how can we help these large language

1367
01:07:56,357 --> 01:07:57,368
models do the same?

1368
01:07:58,369 --> 01:08:00,333
Because it does seem like large language

1369
01:08:00,333 --> 01:08:00,333
models function, at least the ones that

1370
01:08:00,333 --> 01:08:04,372
we interact with online.

1371
01:08:04,373 --> 01:08:04,378
Mostly.

1372
01:08:04,379 --> 01:08:06,396
They seem to function mostly as like a

1373
01:08:06,396 --> 01:08:08,410
system one or type one process.

1374
01:08:08,411 --> 01:08:09,428
They're just kind of like quickly

1375
01:08:09,428 --> 01:08:09,428
generating lots of text or imagery or

1376
01:08:09,428 --> 01:08:12,457
something like that.

1377
01:08:12,458 --> 01:08:15,485
But they're not necessarily reflecting on

1378
01:08:15,485 --> 01:08:15,485
it in the ways that we would think that a

1379
01:08:15,485 --> 01:08:15,485
human is capable of and they might not

1380
01:08:15,485 --> 01:08:15,485
even be capable of representing things in

1381
01:08:15,485 --> 01:08:25,583
the same way that we do.

1382
01:08:26,591 --> 01:08:27,605
Yeah, so I thought that was a really

1383
01:08:27,605 --> 01:08:28,611
interesting idea.

1384
01:08:28,612 --> 01:08:30,635
Using chain of thought and tree of

1385
01:08:30,635 --> 01:08:30,635
thought models to kind of create a

1386
01:08:30,635 --> 01:08:30,635
reflective level or reflective system

1387
01:08:30,635 --> 01:08:30,635
within the models seems just like a

1388
01:08:30,635 --> 01:08:30,635
really valuable idea and just a really

1389
01:08:30,635 --> 01:08:30,635
great synthesis of research in multiple

1390
01:08:30,635 --> 01:08:30,635
disciplines, something that I think few

1391
01:08:30,635 --> 01:08:48,811
people are actually very good at.

1392
01:08:48,812 --> 01:08:49,827
It's like incorporating some of the best

1393
01:08:49,827 --> 01:08:51,839
insights from multiple fields.

1394
01:08:51,840 --> 01:08:52,857
We're often pretty siloed in academia.

1395
01:08:52,858 --> 01:08:55,884
So I just think this is great work and I

1396
01:08:55,884 --> 01:08:55,884
hope more people will appreciate and pay

1397
01:08:55,884 --> 01:09:00,874
attention to it and build on it.

1398
01:09:00,878 --> 01:09:03,907
So one of the things that stood out to me

1399
01:09:03,907 --> 01:09:03,907
in this presentation more so than when I

1400
01:09:03,907 --> 01:09:03,907
was, like, reading this 2023 preprint

1401
01:09:03,907 --> 01:09:03,907
analytic Reasoning for Large language

1402
01:09:03,907 --> 01:09:13,001
models by Dr.

1403
01:09:13,002 --> 01:09:14,018
Bellini Leiche.

1404
01:09:15,028 --> 01:09:18,050
The thing that stood out to me is this

1405
01:09:18,050 --> 01:09:18,050
idea that Bellini Leiche and Frankish

1406
01:09:18,050 --> 01:09:18,050
sort of switched which of these two types

1407
01:09:18,050 --> 01:09:18,050
of reasonings processes are supposed to

1408
01:09:18,050 --> 01:09:18,050
be context dependent or context sensitive

1409
01:09:18,050 --> 01:09:29,166
or however you would want to word that.

1410
01:09:30,170 --> 01:09:32,191
And I started thinking, well, I wonder if

1411
01:09:32,191 --> 01:09:32,191
there's a sense in which both types of

1412
01:09:32,191 --> 01:09:32,191
responses or both types of reasoning are

1413
01:09:32,191 --> 01:09:32,191
somewhat context dependent and I'll just

1414
01:09:32,191 --> 01:09:39,262
say what I mean.

1415
01:09:39,262 --> 01:09:41,285
And then maybe, Samuel, you can kind of

1416
01:09:41,285 --> 01:09:41,285
say what you think I should think or

1417
01:09:41,285 --> 01:09:41,285
clarify the view of the predictive and

1418
01:09:41,285 --> 01:09:48,351
reflective framework or something.

1419
01:09:48,351 --> 01:09:50,379
So the thought I was having was, well, in

1420
01:09:50,379 --> 01:09:50,379
a familiar context, these intuitive

1421
01:09:50,379 --> 01:09:50,379
predictions, these type one processes,

1422
01:09:50,379 --> 01:09:50,379
our gut response, so to speak, those are

1423
01:09:50,379 --> 01:09:50,379
going to be pretty useful because they're

1424
01:09:50,379 --> 01:10:02,434
well trained.

1425
01:10:02,435 --> 01:10:03,448
We have a lot of experience that we're

1426
01:10:03,448 --> 01:10:04,453
drawing on.

1427
01:10:04,453 --> 01:10:06,470
So like our gut response, our first

1428
01:10:06,470 --> 01:10:07,485
response is often quite good.

1429
01:10:07,489 --> 01:10:10,518
It's in these less familiar contexts, or

1430
01:10:10,518 --> 01:10:10,518
maybe similarly familiar, but, like, way

1431
01:10:10,518 --> 01:10:15,566
higher stakes or something.

1432
01:10:15,567 --> 01:10:17,589
But it's in these other contexts where we

1433
01:10:17,589 --> 01:10:17,589
might think, maybe I should slow down and

1434
01:10:17,589 --> 01:10:17,589
make sure I've double checked whatever my

1435
01:10:17,589 --> 01:10:17,589
initial impulse is before I just accept

1436
01:10:17,589 --> 01:10:26,676
it because there's a lot riding on this.

1437
01:10:26,677 --> 01:10:29,705
Or, like, I'm just not used to this type

1438
01:10:29,705 --> 01:10:32,730
of problem, so I need to slow down.

1439
01:10:32,731 --> 01:10:34,752
And so there's a sense in which what

1440
01:10:34,752 --> 01:10:34,752
context is doing is not just showing up

1441
01:10:34,752 --> 01:10:34,752
in one or the other type of reasoning,

1442
01:10:34,752 --> 01:10:34,752
but it's sort of like determining which

1443
01:10:34,752 --> 01:10:34,752
type of reasoning might be best at the

1444
01:10:34,752 --> 01:10:43,848
moment.

1445
01:10:44,852 --> 01:10:46,870
But I'm wondering, is that just totally

1446
01:10:46,870 --> 01:10:46,870
compatible with what you're imagining or

1447
01:10:46,870 --> 01:10:46,870
is that somehow a deviation from the

1448
01:10:46,870 --> 01:10:46,870
framework, the predictive and reflective

1449
01:10:46,870 --> 01:10:52,930
framework?

1450
01:10:52,938 --> 01:10:56,978
Samuel: Yeah, thanks for the thoughts.

1451
01:10:56,978 --> 01:11:01,960
There compliments and thanks for the

1452
01:11:01,960 --> 01:11:03,980
question as well.

1453
01:11:04,990 --> 01:11:09,041
So the reason why I think context needs

1454
01:11:09,041 --> 01:11:09,041
always some sort of type one help is

1455
01:11:09,041 --> 01:11:09,041
because of the constraints we have

1456
01:11:09,041 --> 01:11:21,169
learned from symbolic AI.

1457
01:11:22,170 --> 01:11:26,211
So if I'm saying that type two reasoning

1458
01:11:26,211 --> 01:11:26,211
have the same constraints, then it cannot

1459
01:11:26,211 --> 01:11:26,211
be contextual because it doesn't work

1460
01:11:26,211 --> 01:11:35,303
like that.

1461
01:11:36,310 --> 01:11:38,339
We know that if it fails in context, it

1462
01:11:38,339 --> 01:11:39,347
fails bad.

1463
01:11:40,355 --> 01:11:45,401
But of course we do need to like when we

1464
01:11:45,401 --> 01:11:45,401
have a novel situation, we often solve

1465
01:11:45,401 --> 01:11:52,472
this by context, right?

1466
01:11:52,473 --> 01:11:53,487
So how could this be?

1467
01:11:54,495 --> 01:11:58,533
And I think it's by the interconnection

1468
01:11:58,533 --> 01:12:01,504
of the power of the two system.

1469
01:12:05,544 --> 01:12:08,572
So we always start with a prediction,

1470
01:12:08,572 --> 01:12:08,572
which in this case would be mistaken, and

1471
01:12:08,572 --> 01:12:08,572
then we have to search for novel answers.

1472
01:12:08,572 --> 01:12:14,635
.

1473
01:12:14,636 --> 01:12:19,688
And in this search we make the contextual

1474
01:12:19,688 --> 01:12:21,705
comparisons.

1475
01:12:21,706 --> 01:12:25,745
So it can't be that the system too is

1476
01:12:25,745 --> 01:12:29,780
making the comparison on its own.

1477
01:12:29,784 --> 01:12:33,828
I think it's more likely that when it's a

1478
01:12:33,828 --> 01:12:33,828
case of context that stems from a novel

1479
01:12:33,828 --> 01:12:33,828
situation and then the two systems have

1480
01:12:33,828 --> 01:12:45,944
to figure out it together.

1481
01:12:46,950 --> 01:12:49,986
Likely in the sense that I was pointing

1482
01:12:49,986 --> 01:12:49,986
out here in the end, these predictions

1483
01:12:49,986 --> 01:12:49,986
that go over the thoughts, they most

1484
01:12:49,986 --> 01:12:49,986
likely are the ones that will solve these

1485
01:12:49,986 --> 01:12:49,986
context issues in novel problems, I'd

1486
01:12:49,986 --> 01:13:06,099
argue something like that.

1487
01:13:12,152 --> 01:13:15,181
Nick Byrd: Okay, I think that's somewhat

1488
01:13:15,181 --> 01:13:15,187
helpful.

1489
01:13:15,188 --> 01:13:17,206
So then maybe what I'm thinking is

1490
01:13:17,206 --> 01:13:17,206
something along what the lines you were

1491
01:13:17,206 --> 01:13:20,238
saying at the end.

1492
01:13:20,239 --> 01:13:24,278
You were saying how there's still kind of

1493
01:13:24,278 --> 01:13:24,278
an opportunity to understand things like

1494
01:13:24,278 --> 01:13:32,353
executive function in this framework.

1495
01:13:32,355 --> 01:13:34,377
And I think that's maybe part of what I'm

1496
01:13:34,377 --> 01:13:35,386
wondering about.

1497
01:13:35,387 --> 01:13:39,422
So I guess I'm wondering if there's more

1498
01:13:39,422 --> 01:13:39,422
to be said within this predictive and

1499
01:13:39,422 --> 01:13:39,422
reflecting framework about how each of

1500
01:13:39,422 --> 01:13:39,422
these two types of processes get selected

1501
01:13:39,422 --> 01:13:52,558
or, like, what might help the system.

1502
01:13:53,559 --> 01:13:53,569
Samuel: Hold on.

1503
01:13:54,571 --> 01:13:54,575
Nick Byrd: Go ahead.

1504
01:13:54,576 --> 01:13:56,595
Samuel: That specific point you just

1505
01:13:56,595 --> 01:13:56,595
remember, I just remembered something

1506
01:13:56,595 --> 01:13:59,625
that's related.

1507
01:13:59,626 --> 01:14:02,595
So, yeah, clearly our working memory or

1508
01:14:02,595 --> 01:14:02,595
type two reasoning likely does more than

1509
01:14:02,595 --> 01:14:05,629
that.

1510
01:14:06,635 --> 01:14:09,669
It does this, but likely more than that.

1511
01:14:12,692 --> 01:14:15,722
There's likely also a belief bank,

1512
01:14:15,722 --> 01:14:15,722
something like the thinking dispositions,

1513
01:14:15,722 --> 01:14:15,722
some sort of knowledge bank, which is

1514
01:14:15,722 --> 01:14:24,818
related to type two processing.

1515
01:14:26,832 --> 01:14:29,869
So Keith Franklin does this distinction

1516
01:14:29,869 --> 01:14:29,869
between flat out beliefs and I'm not

1517
01:14:29,869 --> 01:14:29,869
remembering terms, but he has a type one

1518
01:14:29,869 --> 01:14:40,971
belief and type two belief.

1519
01:14:40,972 --> 01:14:43,000
Type one belief is that belief you have,

1520
01:14:43,000 --> 01:14:45,027
but you're not fully confident of it.

1521
01:14:45,028 --> 01:14:49,064
And the type two belief is the belief you

1522
01:14:49,064 --> 01:14:49,064
have when you have a very firm political

1523
01:14:49,064 --> 01:14:49,064
position and you state it with obvious

1524
01:14:49,064 --> 01:14:59,166
words that you sure you believe that.

1525
01:15:00,117 --> 01:15:03,148
So there's likely to be some sort of

1526
01:15:03,148 --> 01:15:03,148
different belief structure as well, which

1527
01:15:03,148 --> 01:15:03,148
I don't talk about at all thesis of the

1528
01:15:03,148 --> 01:15:11,224
work.

1529
01:15:11,224 --> 01:15:14,255
So I'm not saying, obviously that this is

1530
01:15:14,255 --> 01:15:14,255
a complete model of reasoning, and

1531
01:15:14,255 --> 01:15:14,255
obviously we'll need more to figure out

1532
01:15:14,255 --> 01:15:21,326
how reasoning works.

1533
01:15:26,375 --> 01:15:27,384
Nick Byrd: Yeah, okay.

1534
01:15:27,385 --> 01:15:28,399
That's also helpful.

1535
01:15:30,410 --> 01:15:32,439
I'm wondering if Daniel, did you want to

1536
01:15:32,439 --> 01:15:33,446
weigh in?

1537
01:15:35,459 --> 01:15:36,479
Daniel: Well, I think it's a very

1538
01:15:36,479 --> 01:15:36,479
interesting question about which of those

1539
01:15:36,479 --> 01:15:36,479
modes are engaged or how they're balanced

1540
01:15:36,479 --> 01:15:42,535
through time.

1541
01:15:43,541 --> 01:15:46,577
And how does this connect to what's known

1542
01:15:46,577 --> 01:15:46,577
as the context window in today's

1543
01:15:46,577 --> 01:15:53,640
transformer type large language models.

1544
01:15:54,652 --> 01:15:57,683
So how do you map the computational

1545
01:15:57,683 --> 01:15:57,683
attributes of large language models today,

1546
01:15:57,683 --> 01:15:57,683
, like architecturally or their Ram or

1547
01:15:57,683 --> 01:16:05,701
CPU usage?

1548
01:16:05,703 --> 01:16:08,731
How do you map that onto, for example,

1549
01:16:08,731 --> 01:16:10,750
human cognitive processes?

1550
01:16:10,751 --> 01:16:12,773
Do you think that's useful or there are

1551
01:16:12,773 --> 01:16:13,784
any insights there?

1552
01:16:15,800 --> 01:16:18,835
Samuel: Well, I'm mostly making a

1553
01:16:18,835 --> 01:16:18,835
relation between predictive processing

1554
01:16:18,835 --> 01:16:18,835
and generative models, but I know that

1555
01:16:18,835 --> 01:16:18,835
large language models are not entirely

1556
01:16:18,835 --> 01:16:18,835
the same as what active inference is

1557
01:16:18,835 --> 01:16:35,008
saying.

1558
01:16:35,009 --> 01:16:38,038
I'm aware of that, but there is some

1559
01:16:38,038 --> 01:16:38,038
similarity specifically in regards to the

1560
01:16:38,038 --> 01:16:44,092
generative model part.

1561
01:16:45,101 --> 01:16:49,141
So I'm not sure I'm not an AI specialist

1562
01:16:49,141 --> 01:16:49,141
to be sure which details of a generative

1563
01:16:49,141 --> 01:16:57,222
model could be true of the human brain.

1564
01:16:57,223 --> 01:17:00,198
And I'm most likely betting that the

1565
01:17:00,198 --> 01:17:00,198
people who study generative models in the

1566
01:17:00,198 --> 01:17:00,198
brain, like the Frisons and all, have

1567
01:17:00,198 --> 01:17:00,198
figured out something like what our

1568
01:17:00,198 --> 01:17:13,329
generative models do.

1569
01:17:21,402 --> 01:17:23,426
Daniel: One other thought or nick, you

1570
01:17:23,426 --> 01:17:23,426
want to add there just thought of some

1571
01:17:23,426 --> 01:17:23,426
different ways whether people have

1572
01:17:23,426 --> 01:17:23,426
connected it explicitly to the literature

1573
01:17:23,426 --> 01:17:23,426
on working memory in predictive

1574
01:17:23,426 --> 01:17:33,527
processing.

1575
01:17:33,528 --> 01:17:35,546
I think, as you pointed out, it's

1576
01:17:35,546 --> 01:17:35,546
definitely a link that is not highlighted.

1577
01:17:35,546 --> 01:17:37,569
.

1578
01:17:38,570 --> 01:17:41,600
It's one that we recently heard from

1579
01:17:41,600 --> 01:17:41,600
Professors Walker and Monriquez and

1580
01:17:41,600 --> 01:17:41,600
Pristine in the recent Livestream 53 that

1581
01:17:41,600 --> 01:17:41,600
was on like, cognitive paleoanthropology

1582
01:17:41,600 --> 01:17:53,728
looking at human working memory.

1583
01:17:53,729 --> 01:17:56,753
But some ways that people have

1584
01:17:56,753 --> 01:17:56,753
incorporated working memory is like a

1585
01:17:56,753 --> 01:17:56,753
nested model that carries context at a

1586
01:17:56,753 --> 01:17:56,753
deeper time, but that is not necessarily

1587
01:17:56,753 --> 01:17:56,753
like an actual mechanism that gives rise

1588
01:17:56,753 --> 01:17:56,753
to why type one and type two are the way

1589
01:17:56,753 --> 01:18:13,866
they are.

1590
01:18:14,878 --> 01:18:18,912
So I thought that was a very provocative

1591
01:18:18,912 --> 01:18:18,912
direction to go to move past the

1592
01:18:18,912 --> 01:18:18,912
descriptive and then to look at the

1593
01:18:18,912 --> 01:18:18,912
underlying generative model of why these

1594
01:18:18,912 --> 01:18:30,003
outcomes are the way they are.

1595
01:18:30,003 --> 01:18:34,007
Even though these cognitive phenomena are

1596
01:18:34,007 --> 01:18:34,007
causes of other things happening, they

1597
01:18:34,007 --> 01:18:34,007
are also caused by some influenceable or

1598
01:18:34,007 --> 01:18:46,019
contextual or variable aspects.

1599
01:18:46,019 --> 01:18:49,022
And when we lose that context dependence

1600
01:18:49,022 --> 01:18:49,022
of the cognitive systems, then they're

1601
01:18:49,022 --> 01:18:49,022
totally lifted and disembodied don't

1602
01:18:49,022 --> 01:18:57,030
really help.

1603
01:18:58,031 --> 01:19:01,028
So by putting the primacy on that

1604
01:19:01,028 --> 01:19:01,028
predictive processing element, I think it

1605
01:19:01,028 --> 01:19:01,028
opens the door to connecting those areas

1606
01:19:01,028 --> 01:19:01,028
beyond just mentioning the terms in

1607
01:19:01,028 --> 01:19:01,028
proximity, but to really support

1608
01:19:01,028 --> 01:19:01,028
different epistemologies from predictive

1609
01:19:01,028 --> 01:19:23,050
processing as a model or approach.

1610
01:19:26,053 --> 01:19:27,054
Samuel: Yeah.

1611
01:19:28,055 --> 01:19:30,056
Can you go on that idea again?

1612
01:19:30,057 --> 01:19:32,059
I'm not sure what you want me to comment.

1613
01:19:32,059 --> 01:19:32,059
.

1614
01:19:32,059 --> 01:19:34,061
Can you summarize that again and make it

1615
01:19:34,061 --> 01:19:35,062
a question?

1616
01:19:35,062 --> 01:19:36,063
Daniel: Sure.

1617
01:19:36,063 --> 01:19:39,066
What do you think the epistemological

1618
01:19:39,066 --> 01:19:39,066
consequences are of taking predictive

1619
01:19:39,066 --> 01:19:39,066
processing the way that you have

1620
01:19:39,066 --> 01:19:39,066
approached it here versus alternative or

1621
01:19:39,066 --> 01:19:50,077
prior approaches to cognitive sciences?

1622
01:19:52,079 --> 01:19:54,081
Samuel: Okay, but what do you mean by

1623
01:19:54,081 --> 01:19:55,082
epistemological.

1624
01:19:58,085 --> 01:19:59,086
Daniel: Normative?

1625
01:20:00,081 --> 01:20:01,082
Samuel: How we know.

1626
01:20:03,084 --> 01:20:05,086
Daniel: How it influences our

1627
01:20:05,086 --> 01:20:05,086
understanding of how we know or seek or

1628
01:20:05,086 --> 01:20:10,091
practice or do or decide.

1629
01:20:12,093 --> 01:20:12,093
Samuel: Perfect.

1630
01:20:13,093 --> 01:20:15,096
Okay, let me think about that, for

1631
01:20:15,096 --> 01:20:15,096
instance.

1632
01:20:17,098 --> 01:20:21,102
Yeah, I think one of the best what this

1633
01:20:21,102 --> 01:20:21,102
helps with most is explaining dual

1634
01:20:21,102 --> 01:20:21,102
process theory because dual process

1635
01:20:21,102 --> 01:20:21,102
theory is in bad shape in terms of

1636
01:20:21,102 --> 01:20:21,102
concepts, in terms of theory and

1637
01:20:21,102 --> 01:20:37,118
formulation.

1638
01:20:39,120 --> 01:20:43,124
People have no idea what these systems

1639
01:20:43,124 --> 01:20:46,127
refer to in the mind brain.

1640
01:20:47,128 --> 01:20:51,132
They often have different intuitions into

1641
01:20:51,132 --> 01:20:52,133
this.

1642
01:20:52,133 --> 01:20:56,137
And basically what I most think this work

1643
01:20:56,137 --> 01:20:56,137
is relevant for is for saving dual

1644
01:20:56,137 --> 01:20:56,137
process theory from criticism, which has

1645
01:20:56,137 --> 01:21:06,141
been happening in psychology.

1646
01:21:06,141 --> 01:21:09,144
So people are like, it seems like we're

1647
01:21:09,144 --> 01:21:09,144
talking about the left and right brain,

1648
01:21:09,144 --> 01:21:09,144
something like that, something that we

1649
01:21:09,144 --> 01:21:09,144
don't know for sure what these two

1650
01:21:09,144 --> 01:21:17,152
systems are.

1651
01:21:17,152 --> 01:21:18,153
Right.

1652
01:21:22,157 --> 01:21:26,161
So this is a good way forward, I think,

1653
01:21:26,161 --> 01:21:28,163
for dual process theory.

1654
01:21:29,164 --> 01:21:33,168
And as I said, dual process theory does

1655
01:21:33,168 --> 01:21:37,172
have some value in itself.

1656
01:21:39,174 --> 01:21:45,180
So, yeah, saving the process theory, I

1657
01:21:45,180 --> 01:21:45,180
think is a good direction for this model.

1658
01:21:45,180 --> 01:21:51,186
.

1659
01:21:51,186 --> 01:21:57,191
But also if we think about the advantages

1660
01:21:57,191 --> 01:21:57,191
of it would be something like, for

1661
01:21:57,191 --> 01:21:57,191
instance, it could have helped us had the

1662
01:21:57,191 --> 01:21:57,191
idea of using Chain of Thoughts or Tree

1663
01:21:57,191 --> 01:22:13,202
of thoughts.

1664
01:22:14,203 --> 01:22:16,205
Had anyone read this before.

1665
01:22:16,205 --> 01:22:18,207
So it can help us make these sorts of

1666
01:22:18,207 --> 01:22:18,207
predictions that we shouldn't have some

1667
01:22:18,207 --> 01:22:18,207
sort of zero reasoning coupled to the

1668
01:22:18,207 --> 01:22:25,214
gentle models.

1669
01:22:26,214 --> 01:22:29,218
So yeah, I think it could help us think

1670
01:22:29,218 --> 01:22:29,218
further on on AI, if we AI, like Nick was

1671
01:22:29,218 --> 01:22:29,218
saying, to incorporating more stuff in

1672
01:22:29,218 --> 01:22:29,218
psychology that we know that's certainly

1673
01:22:29,218 --> 01:22:29,218
missing here, but here we have a

1674
01:22:29,218 --> 01:22:29,218
direction of what we should incorporate

1675
01:22:29,218 --> 01:22:49,238
and so on.

1676
01:22:50,239 --> 01:22:53,242
But I'm also unaware of the working

1677
01:22:53,242 --> 01:22:53,242
memory research from predictive

1678
01:22:53,242 --> 01:22:58,247
processing you mentioned.

1679
01:22:58,247 --> 01:23:01,244
And if you want to go back to that and

1680
01:23:01,244 --> 01:23:01,244
explain me some more of that, I'd be

1681
01:23:01,244 --> 01:23:05,248
happy to hear.

1682
01:23:08,251 --> 01:23:10,253
Daniel: Yeah, sure.

1683
01:23:10,253 --> 01:23:12,255
So not that this is the most effective

1684
01:23:12,255 --> 01:23:12,255
way to implement a memory system, but at

1685
01:23:12,255 --> 01:23:12,255
least that this provides analytical

1686
01:23:12,255 --> 01:23:21,264
method for measuring or describing them.

1687
01:23:21,264 --> 01:23:25,268
If one were going to have the five digit

1688
01:23:25,268 --> 01:23:25,268
number, you could imagine a nested model

1689
01:23:25,268 --> 01:23:25,268
where the lowest level is the ones place

1690
01:23:25,268 --> 01:23:25,268
and it's nested within a decision tree of

1691
01:23:25,268 --> 01:23:38,281
tens places and hundreds and so on.

1692
01:23:38,281 --> 01:23:42,285
And then there's some seek or access

1693
01:23:42,285 --> 01:23:42,285
policy or strategy that helps speak the

1694
01:23:42,285 --> 01:23:48,291
number in reverse.

1695
01:23:48,291 --> 01:23:51,294
And some cognitive or computational

1696
01:23:51,294 --> 01:23:51,294
limitation is just how well that

1697
01:23:51,294 --> 01:23:51,294
cognitive agent can perform on that test.

1698
01:23:51,294 --> 01:23:57,300
.

1699
01:23:57,300 --> 01:24:00,297
Again, that doesn't mean that's the

1700
01:24:00,297 --> 01:24:00,297
mechanism that's being used, but that

1701
01:24:00,297 --> 01:24:00,297
would be a way to use a nested generative

1702
01:24:00,297 --> 01:24:00,297
model to encode like multiple levels of

1703
01:24:00,297 --> 01:24:12,309
spatial or temporal variability.

1704
01:24:12,309 --> 01:24:15,312
But context becomes an issue because you'

1705
01:24:15,312 --> 01:24:15,312
're basically just expanding the

1706
01:24:15,312 --> 01:24:15,312
possibility space looking for sparser and

1707
01:24:15,312 --> 01:24:22,319
sparser associations.

1708
01:24:22,319 --> 01:24:24,321
So if you don't have a good

1709
01:24:24,321 --> 01:24:24,321
compositionality or really well definable

1710
01:24:24,321 --> 01:24:24,321
well articulated causal process, then you'

1711
01:24:24,321 --> 01:24:24,321
're just building these all by all models

1712
01:24:24,321 --> 01:24:24,321
that you're going to be searching through

1713
01:24:24,321 --> 01:24:39,336
in the dark.

1714
01:24:42,339 --> 01:24:45,342
Samuel: Yeah, one point I want to comment

1715
01:24:45,342 --> 01:24:45,342
on that is that, well, it's not clear

1716
01:24:45,342 --> 01:24:45,342
that the best way to reason will be the

1717
01:24:45,342 --> 01:24:53,350
way humans reason.

1718
01:24:53,350 --> 01:24:53,350
Right?

1719
01:24:54,350 --> 01:24:57,354
So we might eventually have a different

1720
01:24:57,354 --> 01:24:57,354
way of implementing working memory on an

1721
01:24:57,354 --> 01:24:57,354
LLM which would be more effective than

1722
01:24:57,354 --> 01:25:04,355
ours.

1723
01:25:05,356 --> 01:25:09,360
There's also to the other side, there's

1724
01:25:09,360 --> 01:25:09,360
also the point that it seems that the

1725
01:25:09,360 --> 01:25:09,360
limited capacity we have on working

1726
01:25:09,360 --> 01:25:09,360
memory, maybe it's not just a lack of

1727
01:25:09,360 --> 01:25:24,375
power, maybe it's necessary.

1728
01:25:24,375 --> 01:25:26,377
So we kind of like don't lose our minds

1729
01:25:26,377 --> 01:25:26,377
in the sense that we have a limited space

1730
01:25:26,377 --> 01:25:26,377
to search and then we're able to search

1731
01:25:26,377 --> 01:25:33,384
this limited space.

1732
01:25:33,384 --> 01:25:37,388
And if it was too, if we have too much

1733
01:25:37,388 --> 01:25:37,388
space to search because we have super

1734
01:25:37,388 --> 01:25:37,388
working memory, then we wouldn't be able

1735
01:25:37,388 --> 01:25:49,400
to finish the task anytime.

1736
01:25:49,400 --> 01:25:53,404
So maybe this is like a physical good

1737
01:25:53,404 --> 01:25:57,408
barrier, something that helps.

1738
01:25:58,409 --> 01:26:01,406
And it's interesting that what I find

1739
01:26:01,406 --> 01:26:01,406
interesting is that okay, the AI may find

1740
01:26:01,406 --> 01:26:01,406
a lot of different and better ways to

1741
01:26:01,406 --> 01:26:01,406
implement work memory, but it's

1742
01:26:01,406 --> 01:26:01,406
interesting that they implemented a

1743
01:26:01,406 --> 01:26:01,406
worksheet memory or a search agent, which

1744
01:26:01,406 --> 01:26:01,406
was very like the way I described in the

1745
01:26:01,406 --> 01:26:23,428
thesis.

1746
01:26:23,428 --> 01:26:26,431
That was what I found amazing.

1747
01:26:27,432 --> 01:26:29,434
As you were saying.

1748
01:26:29,434 --> 01:26:32,437
There's very ways to implement it working

1749
01:26:32,437 --> 01:26:32,437
memory, but they did it in a very similar

1750
01:26:32,437 --> 01:26:32,437
way in the sense of searching the space

1751
01:26:32,437 --> 01:26:32,437
after the genitive model has offered a

1752
01:26:32,437 --> 01:26:42,447
solution.

1753
01:26:43,448 --> 01:26:47,452
So I don't know, maybe that's a good

1754
01:26:47,452 --> 01:26:47,452
indication that this is on the right

1755
01:26:47,452 --> 01:26:47,452
track, but it also may just simply be a

1756
01:26:47,452 --> 01:26:47,452
simple way they found to solve the

1757
01:26:47,452 --> 01:27:00,459
problem.

1758
01:27:01,460 --> 01:27:04,463
And it was just by coincidence similar to

1759
01:27:04,463 --> 01:27:05,464
this.

1760
01:27:05,464 --> 01:27:08,467
Yeah, maybe the correct way or the way

1761
01:27:08,467 --> 01:27:10,469
the brain does may also be different.

1762
01:27:10,469 --> 01:27:11,470
We don't know.

1763
01:27:12,471 --> 01:27:16,475
All we have was the limited evidence I

1764
01:27:16,475 --> 01:27:16,475
offered a bit more obviously and some

1765
01:27:16,475 --> 01:27:16,475
constraints we know on AI information

1766
01:27:16,475 --> 01:27:25,484
processing.

1767
01:27:26,485 --> 01:27:28,487
We have to figure it out.

1768
01:27:28,487 --> 01:27:31,490
And I think that this implementation

1769
01:27:31,490 --> 01:27:31,490
stuff you're asking is mostly due to the

1770
01:27:31,490 --> 01:27:31,490
AI people to find out or the math people,

1771
01:27:31,490 --> 01:27:43,502
the systems to figure out.

1772
01:27:45,504 --> 01:27:47,506
Daniel: Nick, do you want to comment or a

1773
01:27:47,506 --> 01:27:49,508
question or I'll ask one from the chat.

1774
01:27:49,508 --> 01:27:50,509
Samuel: Yeah.

1775
01:27:50,509 --> 01:27:52,511
Nick Byrd: So I just have a kind of a

1776
01:27:52,511 --> 01:27:52,511
bigger picture question that kind of gets

1777
01:27:52,511 --> 01:27:52,511
us into the realm of these chat bots,

1778
01:27:52,511 --> 01:27:57,516
right?

1779
01:27:57,516 --> 01:27:59,518
So you in this preprint and in this talk

1780
01:27:59,518 --> 01:27:59,518
have like given us a variety of different

1781
01:27:59,518 --> 01:27:59,518
tasks in the heuristics and biases

1782
01:27:59,518 --> 01:28:05,518
literature.

1783
01:28:05,518 --> 01:28:07,520
The the conjunction fallacy task, the the

1784
01:28:07,520 --> 01:28:07,520
lend a problem, some people call it, or

1785
01:28:07,520 --> 01:28:07,520
these cognitive reflection test questions.

1786
01:28:07,520 --> 01:28:13,526
.

1787
01:28:13,526 --> 01:28:16,529
And you know, there's a, there's like a

1788
01:28:16,529 --> 01:28:16,529
paper in the fall that showed that a

1789
01:28:16,529 --> 01:28:16,529
preprint in the fall that last fall that

1790
01:28:16,529 --> 01:28:25,538
showed 2022, that showed GPT-3 and 3.5.

1791
01:28:25,538 --> 01:28:27,540
And even the earlier versions, like,

1792
01:28:27,540 --> 01:28:27,540
showed a lot of the human, like, faulty

1793
01:28:27,540 --> 01:28:31,544
intuitions.

1794
01:28:31,544 --> 01:28:33,546
Like it basically fell for the lure

1795
01:28:33,546 --> 01:28:34,547
pretty often.

1796
01:28:34,547 --> 01:28:36,549
Sometimes, like most of the time it was

1797
01:28:36,549 --> 01:28:37,550
falling for the lure.

1798
01:28:37,550 --> 01:28:39,552
But then as soon as GPT Four was

1799
01:28:39,552 --> 01:28:39,552
available to be studied and used on these

1800
01:28:39,552 --> 01:28:39,552
models, it was like performing basically

1801
01:28:39,552 --> 01:28:46,559
near perfect.

1802
01:28:46,559 --> 01:28:47,560
And when it did get the incorrect

1803
01:28:47,560 --> 01:28:47,560
response, it wasn't necessarily the lure,

1804
01:28:47,560 --> 01:28:47,560
it was just like some other general type

1805
01:28:47,560 --> 01:28:47,560
of error it was making, like

1806
01:28:47,560 --> 01:28:47,560
misinterpreting the question altogether

1807
01:28:47,560 --> 01:28:54,567
or something.

1808
01:28:54,567 --> 01:28:57,570
And it seems like the predicting,

1809
01:28:57,570 --> 01:28:57,570
predictive and reflecting model framework

1810
01:28:57,570 --> 01:28:57,570
could have at least two different ways of

1811
01:28:57,570 --> 01:29:05,572
explaining this.

1812
01:29:05,572 --> 01:29:07,574
One is what happened is they changed the

1813
01:29:07,574 --> 01:29:07,574
AI's like type one thinking or whatever

1814
01:29:07,574 --> 01:29:07,574
so that it could respond intuitively to

1815
01:29:07,574 --> 01:29:07,574
all these, but intuitively and correctly,

1816
01:29:07,574 --> 01:29:17,584
so it didn't need to reflect.

1817
01:29:17,584 --> 01:29:19,586
Or it could be that they somehow created

1818
01:29:19,586 --> 01:29:19,586
this reflective type system with either

1819
01:29:19,586 --> 01:29:19,586
like a chain of thought or a tree of

1820
01:29:19,586 --> 01:29:19,586
thought or some other type of system that

1821
01:29:19,586 --> 01:29:19,586
helps it actually engage in reflection on

1822
01:29:19,586 --> 01:29:19,586
these tasks or whatever the analog of

1823
01:29:19,586 --> 01:29:32,599
reflection would be for a chat bot.

1824
01:29:33,600 --> 01:29:35,602
And so I'm wondering if you have thoughts

1825
01:29:35,602 --> 01:29:35,602
on this, as you've been thinking about

1826
01:29:35,602 --> 01:29:35,602
this or talking to anyone about this, how

1827
01:29:35,602 --> 01:29:40,607
do you think they did this?

1828
01:29:40,607 --> 01:29:41,608
Obviously we have to speculate because a

1829
01:29:41,608 --> 01:29:42,609
lot of the data is proprietary.

1830
01:29:42,609 --> 01:29:44,611
But I'm just curious to get your

1831
01:29:44,611 --> 01:29:44,611
thoughts?

1832
01:29:45,612 --> 01:29:46,613
Samuel: Yeah, that's an awesome question.

1833
01:29:46,613 --> 01:29:46,613
.

1834
01:29:47,613 --> 01:29:50,617
I actually forgot to mention this on the

1835
01:29:50,617 --> 01:29:50,617
presentation that the biases they make on

1836
01:29:50,617 --> 01:29:50,617
the recent test last year were exactly

1837
01:29:50,617 --> 01:29:58,625
the ones the humans made.

1838
01:29:58,625 --> 01:30:03,624
So if we fed it some dual process real,

1839
01:30:03,624 --> 01:30:08,629
it would give exactly our mistakes.

1840
01:30:09,630 --> 01:30:10,631
That's so amazing.

1841
01:30:11,632 --> 01:30:18,639
I'm not certain why that happened, but it

1842
01:30:18,639 --> 01:30:22,643
did and how they fixed this.

1843
01:30:23,644 --> 01:30:28,649
So, yeah, I do think that they trained

1844
01:30:28,649 --> 01:30:28,649
the model on these tasks because people

1845
01:30:28,649 --> 01:30:28,649
were able to get similar issues by

1846
01:30:28,649 --> 01:30:41,662
changing the questions a little bit.

1847
01:30:41,662 --> 01:30:45,666
So I think they cheated on that in the

1848
01:30:45,666 --> 01:30:45,666
sense that they trained on known problems,

1849
01:30:45,666 --> 01:30:53,674
, but that's not true of new problems.

1850
01:30:53,674 --> 01:30:57,678
If you give it some different formulation

1851
01:30:57,678 --> 01:30:57,678
and feed it to 3.5 new version, it's

1852
01:30:57,678 --> 01:31:02,677
still failing.

1853
01:31:02,677 --> 01:31:06,681
But the people on Open AI, they are aware

1854
01:31:06,681 --> 01:31:06,681
of this chain of thought research, they

1855
01:31:06,681 --> 01:31:06,681
are aware of this tree of thought

1856
01:31:06,681 --> 01:31:06,681
research, but I don't think they want to

1857
01:31:06,681 --> 01:31:19,694
depend or rely on these external agents.

1858
01:31:19,694 --> 01:31:24,699
So they recently released a report where

1859
01:31:24,699 --> 01:31:24,699
they are trying to reinforce intermediate

1860
01:31:24,699 --> 01:31:33,708
steps of reasoning on GPC four.

1861
01:31:33,708 --> 01:31:37,712
So they try to simulate internally what

1862
01:31:37,712 --> 01:31:37,712
the chain of thoughts reasoning is doing

1863
01:31:37,712 --> 01:31:37,712
externally by reinforcing the

1864
01:31:37,712 --> 01:31:37,712
intermediate steps of reasoning so that

1865
01:31:37,712 --> 01:31:50,725
it starts failing composition, helping.

1866
01:31:50,725 --> 01:31:51,726
So on.

1867
01:31:54,729 --> 01:31:56,731
Daniel: A lot of thought on that.

1868
01:31:56,731 --> 01:31:59,734
I was reminded by Daniel Dennett's

1869
01:31:59,734 --> 01:31:59,734
framework or model in Darwin's dangerous

1870
01:31:59,734 --> 01:32:06,735
idea of cranes and skyhooks.

1871
01:32:06,735 --> 01:32:08,737
And so some building is built and then

1872
01:32:08,737 --> 01:32:08,737
the question is like, was it built with

1873
01:32:08,737 --> 01:32:08,737
cranes or with top down hung skyhooks

1874
01:32:08,737 --> 01:32:08,737
that just sort of descend from nowhere,

1875
01:32:08,737 --> 01:32:08,737
hold everything up while it's all being

1876
01:32:08,737 --> 01:32:18,747
built?

1877
01:32:18,747 --> 01:32:20,749
And it's like, well, no, it's with cranes.

1878
01:32:20,749 --> 01:32:20,749
.

1879
01:32:20,749 --> 01:32:22,751
And then if you need a big crane, you can

1880
01:32:22,751 --> 01:32:24,753
use a smaller crane to assemble a crane.

1881
01:32:24,753 --> 01:32:27,756
So that's kind of a bottom up

1882
01:32:27,756 --> 01:32:27,756
constructive metaphor, whereas the sky

1883
01:32:27,756 --> 01:32:27,756
hook is like the top down

1884
01:32:27,756 --> 01:32:33,762
compositionality.

1885
01:32:33,762 --> 01:32:36,765
And that's when you get to certain level

1886
01:32:36,765 --> 01:32:36,765
of sophistication for a cognitive system,

1887
01:32:36,765 --> 01:32:36,765
it can make the blueprint or the plan

1888
01:32:36,765 --> 01:32:36,765
like maybe what we would associate with

1889
01:32:36,765 --> 01:32:36,765
the expected free energy calculation, not

1890
01:32:36,765 --> 01:32:36,765
just the variational free energy

1891
01:32:36,765 --> 01:32:50,779
calculation.

1892
01:32:50,779 --> 01:32:53,782
So at some point when a plan can be made,

1893
01:32:53,782 --> 01:32:53,782
it requires a strategy between type one

1894
01:32:53,782 --> 01:32:53,782
and type two switching and probably other

1895
01:32:53,782 --> 01:33:01,784
switches too.

1896
01:33:02,785 --> 01:33:06,789
And so that crane approach, you need a

1897
01:33:06,789 --> 01:33:09,792
lot of compute to do a bottom up.

1898
01:33:09,792 --> 01:33:11,794
So even though it's kind of weird to

1899
01:33:11,794 --> 01:33:11,794
think about, it's almost like the current

1900
01:33:11,794 --> 01:33:11,794
large models are very bottom up because

1901
01:33:11,794 --> 01:33:11,794
they're very bottom up from syntax and

1902
01:33:11,794 --> 01:33:11,794
they don't have what you refer to as

1903
01:33:11,794 --> 01:33:11,794
imperative linguistic knowledge, which

1904
01:33:11,794 --> 01:33:11,794
can be normative as well as a heuristic

1905
01:33:11,794 --> 01:33:26,809
for knowledge.

1906
01:33:26,809 --> 01:33:31,814
And that's potentially a small model, but

1907
01:33:31,814 --> 01:33:31,814
it contains wisdom that's explicit and

1908
01:33:31,814 --> 01:33:31,814
practices and stances or dispositions as

1909
01:33:31,814 --> 01:33:41,824
you described.

1910
01:33:41,824 --> 01:33:45,828
And so it's kind of like it's not a

1911
01:33:45,828 --> 01:33:45,828
denial of embodiment to also clarify what

1912
01:33:45,828 --> 01:33:53,836
this skyhook ability is.

1913
01:33:53,836 --> 01:33:57,840
And that's type two like so I think.

1914
01:34:00,837 --> 01:34:00,837
I guess.

1915
01:34:00,837 --> 01:34:01,838
What do you think about that?

1916
01:34:01,838 --> 01:34:04,841
Or how do you connect this to anything in

1917
01:34:04,841 --> 01:34:05,842
that area?

1918
01:34:06,843 --> 01:34:08,845
Samuel: Yeah, I'm going to make you do

1919
01:34:08,845 --> 01:34:08,845
the summary and the question again,

1920
01:34:08,845 --> 01:34:08,845
because although I did track some of what

1921
01:34:08,845 --> 01:34:08,845
you mean, I'm not sure which ones made to

1922
01:34:08,845 --> 01:34:15,852
comment on that.

1923
01:34:16,853 --> 01:34:19,856
Daniel: Do you think that the landscape

1924
01:34:19,856 --> 01:34:19,856
of large and small models would be

1925
01:34:19,856 --> 01:34:19,856
different if people took on board some of

1926
01:34:19,856 --> 01:34:19,856
the features you described, like

1927
01:34:19,856 --> 01:34:19,856
imperative linguistic knowledge,

1928
01:34:19,856 --> 01:34:34,871
seriously into construction of models?

1929
01:34:35,872 --> 01:34:38,875
Samuel: So let me just see if this is

1930
01:34:38,875 --> 01:34:41,877
something you're asking.

1931
01:34:41,878 --> 01:34:44,881
So they're doing small models now for

1932
01:34:44,881 --> 01:34:44,881
phones and open source models for phones,

1933
01:34:44,881 --> 01:34:44,881
and they have the issue of not being able

1934
01:34:44,881 --> 01:34:51,888
to scale up.

1935
01:34:51,888 --> 01:34:52,889
Right?

1936
01:34:52,889 --> 01:34:54,891
Because if you scale up too much, then

1937
01:34:54,891 --> 01:34:56,892
you can't run on the phone.

1938
01:34:56,893 --> 01:35:00,891
So you're asking if we can implement

1939
01:35:00,891 --> 01:35:00,891
something like Tree of Thoughts to help

1940
01:35:00,891 --> 01:35:06,897
these smaller models.

1941
01:35:06,897 --> 01:35:07,898
Is that what you said?

1942
01:35:08,899 --> 01:35:08,899
Daniel: Yes.

1943
01:35:10,901 --> 01:35:12,903
Samuel: Are you sure or are you just

1944
01:35:12,903 --> 01:35:14,905
saying, okay, go for that?

1945
01:35:14,905 --> 01:35:16,907
Daniel: That's an example of what it

1946
01:35:16,907 --> 01:35:18,909
would mean to re understand.

1947
01:35:18,909 --> 01:35:20,911
Like where do we need an API call to a

1948
01:35:20,911 --> 01:35:21,911
cloud center?

1949
01:35:21,912 --> 01:35:23,914
Or where could some context be locally

1950
01:35:23,914 --> 01:35:26,917
computed with much more of a type One?

1951
01:35:26,917 --> 01:35:28,919
What are some type two problems that we

1952
01:35:28,919 --> 01:35:28,919
might be able to type one our way out

1953
01:35:28,919 --> 01:35:30,921
of?

1954
01:35:31,922 --> 01:35:34,925
What are some type one areas where things

1955
01:35:34,925 --> 01:35:34,925
are not working and a type two

1956
01:35:34,925 --> 01:35:40,931
description might be advantageous?

1957
01:35:41,932 --> 01:35:43,934
Samuel: Yeah, I think they're getting

1958
01:35:43,934 --> 01:35:44,935
onto that.

1959
01:35:44,935 --> 01:35:48,938
And they're being able to make these

1960
01:35:48,938 --> 01:35:48,938
smaller, smaller models precisely because

1961
01:35:48,938 --> 01:35:48,938
of these sorts of discoveries they made

1962
01:35:48,938 --> 01:35:57,948
on Chain of Thoughts.

1963
01:35:57,948 --> 01:35:57,948
Right.

1964
01:35:58,949 --> 01:36:00,945
There's also stuff about being able to

1965
01:36:00,945 --> 01:36:02,947
train on specific data.

1966
01:36:02,947 --> 01:36:04,949
I read one.

1967
01:36:05,950 --> 01:36:05,950
It's called?

1968
01:36:06,951 --> 01:36:07,952
Textbooks are all you need.

1969
01:36:08,953 --> 01:36:11,956
They're training smaller, smaller models

1970
01:36:11,956 --> 01:36:14,959
on textbooks, and they're doing better.

1971
01:36:14,959 --> 01:36:17,962
So, yeah, there are differences in

1972
01:36:17,962 --> 01:36:17,962
training smaller models, different data,

1973
01:36:17,962 --> 01:36:17,962
but there's also the possibility of

1974
01:36:17,962 --> 01:36:17,962
making them more reliable based on what I'

1975
01:36:17,962 --> 01:36:17,962
'm calling T Two Agents or Train of

1976
01:36:17,962 --> 01:36:35,980
Thoughts, stuff like that.

1977
01:36:35,980 --> 01:36:38,983
And again, we don't know if that's all we

1978
01:36:38,983 --> 01:36:38,983
can do externally, but I would argue that

1979
01:36:38,983 --> 01:36:38,983
we would need necessarily some sort of

1980
01:36:38,983 --> 01:36:38,983
external mechanism coupled to any large

1981
01:36:38,983 --> 01:36:38,983
language model for the best results

1982
01:36:38,983 --> 01:36:38,983
because of the differences in

1983
01:36:38,983 --> 01:36:56,001
representations, mainly because of this.

1984
01:36:57,002 --> 01:37:01,000
Unless the large language model somehow

1985
01:37:01,000 --> 01:37:01,000
makes a symbolic representation emerge

1986
01:37:01,000 --> 01:37:01,000
from its processes, it's not clear that

1987
01:37:01,000 --> 01:37:11,010
it has done so.

1988
01:37:11,010 --> 01:37:15,014
But unless it does so, and it uses that

1989
01:37:15,014 --> 01:37:15,014
reliably because it may even have

1990
01:37:15,014 --> 01:37:15,014
generated some sort of symbolic process

1991
01:37:15,014 --> 01:37:15,014
internally, but doesn't always use that

1992
01:37:15,014 --> 01:37:15,014
or doesn't know when to use that, most

1993
01:37:15,014 --> 01:37:15,014
likely we're going to need something

1994
01:37:15,014 --> 01:37:15,014
external to the language model, and this

1995
01:37:15,014 --> 01:37:45,044
is some clues to how to implement that.

1996
01:37:45,044 --> 01:37:49,048
And also, I think, in our brain, it's

1997
01:37:49,048 --> 01:37:49,048
kind of external not only in the sense of

1998
01:37:49,048 --> 01:37:49,048
our minds, but also in the brain, since

1999
01:37:49,048 --> 01:37:49,048
it's a recent step in evolution, we don't

2000
01:37:49,048 --> 01:37:49,048
see I don't know lizards reasoning in

2001
01:37:49,048 --> 01:38:10,063
type two manner.

2002
01:38:10,063 --> 01:38:10,063
Right.

2003
01:38:11,064 --> 01:38:15,068
We do see Chintenzees reasoning in type

2004
01:38:15,068 --> 01:38:18,071
two manner, but not as much as we do.

2005
01:38:18,071 --> 01:38:21,074
So it's very human like reasoning.

2006
01:38:23,076 --> 01:38:25,078
It's just that going back to the context

2007
01:38:25,078 --> 01:38:25,078
problem, animals do solve contextual

2008
01:38:25,078 --> 01:38:29,082
problems.

2009
01:38:29,082 --> 01:38:31,084
That's why they can live.

2010
01:38:31,084 --> 01:38:31,084
Right.

2011
01:38:31,084 --> 01:38:33,086
So the issue is not actually the context

2012
01:38:33,086 --> 01:38:34,087
there.

2013
01:38:34,087 --> 01:38:41,094
It's more like some boost on reasoning

2014
01:38:41,094 --> 01:38:42,095
capacity.

2015
01:38:42,095 --> 01:38:46,099
And that boost on reasoning capacity may

2016
01:38:46,099 --> 01:38:46,099
be related to the prefrontal cortex, as

2017
01:38:46,099 --> 01:38:50,103
we know.

2018
01:38:50,103 --> 01:38:54,107
And so, of course, the prefrontal cortex

2019
01:38:54,107 --> 01:38:56,109
is also a network.

2020
01:38:56,109 --> 01:38:58,111
It's not a serial Turing machine.

2021
01:38:59,112 --> 01:39:02,109
But if there's somewhere that's

2022
01:39:02,109 --> 01:39:02,109
implementing a serial machine or a

2023
01:39:02,109 --> 01:39:02,109
symbolic classic AI machine or symbolic

2024
01:39:02,109 --> 01:39:02,109
reasoning or something like that, most

2025
01:39:02,109 --> 01:39:02,109
likely it would be the prefrontal cortex,

2026
01:39:02,109 --> 01:39:02,109
which is kind of like a distinct

2027
01:39:02,109 --> 01:39:02,109
addiction addition to the brain and maybe

2028
01:39:02,109 --> 01:39:02,109
something related to language, which is

2029
01:39:02,109 --> 01:39:02,109
also distinct, a new addition to the

2030
01:39:02,109 --> 01:39:34,141
brain.

2031
01:39:37,144 --> 01:39:41,148
Also, I'm very convinced by the photo and

2032
01:39:41,148 --> 01:39:41,148
deletion argument in 88 that you're going

2033
01:39:41,148 --> 01:39:49,156
to need symbolic processing anyway.

2034
01:39:50,157 --> 01:39:53,160
Either the model is going to generate

2035
01:39:53,160 --> 01:39:53,160
that symbolic processing, or you're going

2036
01:39:53,160 --> 01:39:53,160
to have to feed it for him, or else you'

2037
01:39:53,160 --> 01:39:53,160
're going to have hallucination of

2038
01:39:53,160 --> 01:39:53,160
mistakes and reasoning because of the

2039
01:39:53,160 --> 01:39:53,160
nature of because of the nature of

2040
01:39:53,160 --> 01:40:10,171
distributed representations.

2041
01:40:10,171 --> 01:40:10,171
Right.

2042
01:40:11,172 --> 01:40:14,175
It's uncertain and ambiguous by nature.

2043
01:40:14,175 --> 01:40:18,179
That's why I'm not entirely convinced

2044
01:40:18,179 --> 01:40:18,179
that only probabilistic representations

2045
01:40:18,179 --> 01:40:18,179
will be enough to solve hallucination

2046
01:40:18,179 --> 01:40:25,186
issues.

2047
01:40:29,190 --> 01:40:30,191
Daniel: Awesome.

2048
01:40:30,191 --> 01:40:33,194
Nick, do you have any other questions or

2049
01:40:33,194 --> 01:40:35,196
areas you want to mention or ask?

2050
01:40:39,200 --> 01:40:41,202
Nick Byrd: I think those were my main

2051
01:40:41,202 --> 01:40:41,202
questions.

2052
01:40:42,203 --> 01:40:43,204
I don't know if there's anything in the

2053
01:40:43,204 --> 01:40:43,204
Chat that you wanted to touch on before

2054
01:40:43,204 --> 01:40:45,206
we go.

2055
01:40:45,206 --> 01:40:48,209
I know we're maybe over time, I'll.

2056
01:40:48,209 --> 01:40:50,211
Daniel: Go with one from the Chat, and

2057
01:40:50,211 --> 01:40:52,213
then we can have any closing thoughts.

2058
01:40:52,213 --> 01:40:56,217
So Glia maximalist asks, are you familiar

2059
01:40:56,217 --> 01:41:01,216
with visual pathways in the human brain?

2060
01:41:01,216 --> 01:41:03,218
Specifically the dorsal where and

2061
01:41:03,218 --> 01:41:04,219
ventral?

2062
01:41:04,219 --> 01:41:07,222
What pathways do you think this maps onto

2063
01:41:07,222 --> 01:41:07,222
the two systems you describe for problem

2064
01:41:07,222 --> 01:41:11,226
solving?

2065
01:41:13,228 --> 01:41:14,229
Samuel: Okay, yeah.

2066
01:41:14,229 --> 01:41:20,235
So like I said, in 2008, evans tried to

2067
01:41:20,235 --> 01:41:20,235
accomplish that unifying vision of due

2068
01:41:20,235 --> 01:41:20,235
process theory, but I'm not very

2069
01:41:20,235 --> 01:41:34,249
convinced by that.

2070
01:41:34,249 --> 01:41:38,252
He also said, well, we can't do this as

2071
01:41:38,252 --> 01:41:38,252
of yet, but I do think it's possible, as

2072
01:41:38,252 --> 01:41:38,252
I said on the presentation, I just don't

2073
01:41:38,252 --> 01:41:38,252
know how to do it, how to have a

2074
01:41:38,252 --> 01:41:38,252
convincing theory of everything, of dual

2075
01:41:38,252 --> 01:41:53,268
process theory in the brain.

2076
01:41:55,270 --> 01:41:58,273
It does go on to other regions.

2077
01:41:58,273 --> 01:42:01,270
Like, you have automatic motor responses

2078
01:42:01,270 --> 01:42:03,272
and controlled motor responses.

2079
01:42:06,275 --> 01:42:10,279
Like you said, there's different pathways

2080
01:42:10,279 --> 01:42:10,279
in reasoning, in figuring out stuff in

2081
01:42:10,279 --> 01:42:16,285
the visual field.

2082
01:42:18,287 --> 01:42:22,291
Maybe the what would be related to

2083
01:42:22,291 --> 01:42:22,291
something and where to another most

2084
01:42:22,291 --> 01:42:22,291
likely the what would be tapping on to

2085
01:42:22,291 --> 01:42:33,302
type two processing.

2086
01:42:34,303 --> 01:42:37,305
I'm not sure how the where would be

2087
01:42:37,305 --> 01:42:37,305
tapping on that, but yeah, as I'm trying

2088
01:42:37,305 --> 01:42:37,305
to make the point, if we extend it to the

2089
01:42:37,305 --> 01:42:47,316
whole brain, we lose what we've learned.

2090
01:42:47,316 --> 01:42:49,318
We're not sure how that applies.

2091
01:42:54,323 --> 01:42:55,323
Daniel: Cool.

2092
01:42:55,324 --> 01:42:59,328
Well, Nick, any penultimate last words?

2093
01:43:03,326 --> 01:43:04,327
Samuel: You muted.

2094
01:43:07,330 --> 01:43:08,331
Oh, wait.

2095
01:43:08,331 --> 01:43:09,332
Daniel: Unmute and then continue.

2096
01:43:10,333 --> 01:43:11,334
Nick Byrd: Sorry about that.

2097
01:43:11,334 --> 01:43:12,335
No worries.

2098
01:43:13,336 --> 01:43:16,339
Thanks to both of you, Dr.

2099
01:43:16,339 --> 01:43:18,341
Bellini, Lighte and Daniel, for

2100
01:43:18,341 --> 01:43:19,342
coordinating this.

2101
01:43:19,342 --> 01:43:21,344
And I'm really just glad to have found

2102
01:43:21,344 --> 01:43:21,344
the research and to discuss it and

2103
01:43:21,344 --> 01:43:21,344
connect and looking forward to further

2104
01:43:21,344 --> 01:43:26,349
conversations, to be honest.

2105
01:43:29,352 --> 01:43:29,352
Daniel: Awesome.

2106
01:43:30,353 --> 01:43:32,355
Sam, any closing thoughts?

2107
01:43:33,356 --> 01:43:36,359
Samuel: Yeah, I'm just sad I couldn't go

2108
01:43:36,359 --> 01:43:36,359
much beyond what I said on the

2109
01:43:36,359 --> 01:43:38,361
presentation.

2110
01:43:40,363 --> 01:43:44,367
Most of the stuff you guys asked, I'm not

2111
01:43:44,367 --> 01:43:47,370
sure I have good answers for that.

2112
01:43:48,371 --> 01:43:52,375
Basically, what I've done is all I know,

2113
01:43:52,375 --> 01:43:52,375
and I'm not sure how to go on the future

2114
01:43:52,375 --> 01:43:52,375
with this, and maybe someone in the

2115
01:43:52,375 --> 01:44:02,379
audience will do.

2116
01:44:02,379 --> 01:44:05,382
So I hope that helps, guys.

2117
01:44:07,384 --> 01:44:07,384
Great.

2118
01:44:07,384 --> 01:44:09,386
Daniel: I hope so, too.

2119
01:44:09,386 --> 01:44:11,388
All right, till next time.

2120
01:44:12,389 --> 01:44:12,389
Thank you.

2121
01:44:13,390 --> 01:44:13,390
Nick Byrd: Bye.


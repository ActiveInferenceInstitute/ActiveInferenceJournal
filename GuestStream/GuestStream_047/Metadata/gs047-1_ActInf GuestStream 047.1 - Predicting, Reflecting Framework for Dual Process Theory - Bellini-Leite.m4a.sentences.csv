start	end	sentNum	speaker	confidence	text
6060	7256	2	B	0.98047	Hello and welcome.
7358	13416	3	B	0.99942	This is ActInf GuestStream 47.1 on July 10, 2023.
13598	20644	4	B	0.99382	We're here with Nick Byrd and Samuel Bellini-Leite, and we're going to have a presentation followed by a discussion.
20692	23076	5	B	0.83975	So thank you both for joining.
23188	26116	6	B	1	And Sam, thanks again for this presentation.
26228	27210	7	B	0.98785	Off to you.
30460	32370	8	A	0.64391	Okay?
33380	35810	9	A	0.97156	Was Nick going to say something before I started?
36660	37552	10	A	0.95	I can go ahead.
37606	43264	11	A	0.77075	Okay, so some introduction first.
43382	47300	12	A	0.6214	I'm a professor at Minas Gerais State University.
49560	55540	13	A	0.90513	I'm a psychologist philosopher, mainly working in cognitive science.
55980	81540	14	A	0.90508	I'm going to talk about what I did in my PhD thesis and also some renewed interest I had on this based on some developments in large language models which happened this year, most notably Chain of Thoughts and Tree of Thoughts.
81720	88450	15	A	0.88901	Those additions to large language model made me want to come back to this again and talk about this.
90500	98020	16	A	0.99952	So how do we know about human reasoning, which is the subject, the main subject underlying this topic?
99000	116920	17	A	0.9935	Well, we had empirical research from the 60s, starting with Peter Watson, who was challenging the notion that we have a logic module for reasoning or some general understanding of logic when we're adults.
117980	138384	18	A	0.61	And he crafted tasks which were hard to solve, but they had actually simple fundamentals to it, which were taken to show people's irrationality, something like that.
138582	160310	19	A	1	And also a huge tradition that's called Heuristics and Biases that also started in the mainly by Kanmen and Amosverskin, which had similar tasks and went beyond, and they even won the Nobel Prize for this work.
162120	172510	20	A	1	I had here on the slides Evolutionary Psychology and Language and Pragmatics, which were also people in the 90s working with this sort of research.
172960	196260	21	A	0.9995	But I'm going to talk about dual process theory in the way they formulated by Evans and Stanovich, which mainly summarizes the results of Watson and common risk biases literature.
196840	205910	22	A	0.99871	So I'm going to talk about how dual process theory came to be and then some problems that it has that I tried to address.
208120	222696	23	A	0.94127	So one of the first reasoning tests in this trend literature was the Watson selection test, which goes like this please indicate which cards can determine if the following rule is false.
222808	227630	24	A	0.99979	If a card has a vowel on one side, then has an even number on the other side.
230240	233020	25	A	0.99993	This sounds like a simple exercise.
233180	239250	26	A	0.9976	So the card A clearly is relevant because it has a vowel on one side.
239620	245916	27	A	0.97711	If it doesn't have an even number on the other, then the statement is false.
245948	250310	28	A	1	And everyone can find this neatly of most people.
251160	258688	29	A	0.92732	But the problem comes with the card seven, which is not so intuitive.
258784	266984	30	A	0.99988	People actually choose the second card two there, which has an even number on one side.
267102	272920	31	A	0.99999	But having that even number doesn't help at all to determine if the statement is false.
273000	286370	32	A	0.95882	What does help is which is an odd number, if it has a file the other side, then the statement could be.
288020	322136	33	A	0.99446	So this was one of the tasks that famous for and conman and first key show them this one they're famous task as well solved the conjunction fallacy the issue that people arise when solving this task goes like this it is a 31 years old, single, outspoken and very bright.
322248	325292	34	A	0.99983	She majored in philosophy as a student.
325426	333964	35	A	0.99993	She was deeply concerned with issues of discrimination and social justice and also participated in anti nuclear demonstrations.
334092	340160	36	A	0.98723	Please make the following statements by their probability using one for the most probable and eight for the least probable.
342820	348944	37	A	0.55306	What's interesting here is the statement, the last statement.
348992	356020	38	A	0.86714	Linda is a bank teller and isn't active active in the feminist movement.
356920	367560	39	A	0.99996	Because there's also these statements without one another, without the conjecture which is Linda is active in the feminist movement and Linda is a bank teller.
368640	380860	40	A	0.94442	Because of the description, people think that Linda is a feminist is very likely given the description, but they think being a bank teller is not likely.
381220	392240	41	A	1	The issue is that people usually mark Linda is a bank teller and is a feminist as more likely than being a bank teller.
392740	398320	42	A	1	The problem here is that the conjunction of the two necessarily is less likely.
398660	406420	43	A	0.99962	So being a feminist plus being something else or being a bank teller plus being something else is automatically less likely.
409580	418920	44	A	0.99931	So those are the sort of tasks that the reasoning literature used to show some sort of human irrationality.
420480	433870	45	A	1	One issue with this is that it's not very interesting to show that humans are irrational, it's more interesting to show how humans reason.
434560	446980	46	A	0.92	And also these sort of studies didn't show how people actually solved the problems.
447050	450500	47	A	0.99754	They were actually focused on how people made mistakes.
451480	459800	48	A	1	One thing that's interesting about dual process theory is that it explains both the mistakes and the solutions to the answers.
460220	472072	49	A	0.99512	So in 2002, Conan and Frederick formulated this cognitive reflection test which is another one of these tasks but a lot much simpler and elegant.
472136	482510	50	A	1	And it really shows indicates at least how dual process theory is a good solution to a good explanation of the evidence in general.
483040	494108	51	A	0.60012	So yeah, the western selection test used logic and the Linda task used probability.
494284	496924	52	A	0.6	And here we just have basic arithmetic.
496972	506388	53	A	0.98829	So as if math we don't have to worry about which answer is truly correct or which rational norms we need to follow.
506474	508100	54	A	0.98101	It's just basic math.
509400	511380	55	A	0.99997	There's nothing to discuss here.
511450	511780	56	A	0.99659	Right?
511850	517592	57	A	0.98154	So the task goes like this a bat and ball costs one in ten in total.
517726	524360	58	A	0.99562	You might know this test, it's a famous test even out of the reasoning literature, the bat costs one more than ball.
524440	526350	59	A	1	How much does the ball cost?
526880	531820	60	A	0.97139	So people when when people read this, they usually say $0.10 intuitively.
532480	539740	61	A	0.91877	But if you take a little time to write write it down, you can see that it's actually $0.05.
539820	548272	62	A	0.98812	So the bat cost one and ball costs $0.05.
548336	550230	63	A	0.99982	Together it costs one and ten.
551640	554416	64	A	1	The other two have a similar structure.
554528	563530	65	A	1	The point here is that sometimes we look at these tests and we have an intuitive answer which is wrong.
564620	581230	66	A	1	And the way we get out of these issues is that we stop and think a little bit and review our answers with some sort of step by step reasoning or writing it down, something like that.
582640	590770	67	A	0.98926	What's interesting here is if you look at two plus two, it's almost impossible not to think four.
591300	592924	68	A	0.97643	It's almost like you're perceiving.
592972	595700	69	A	1	The answer is four just by looking at it.
595850	604580	70	A	0.99998	But if you have some sort of more complex calculation to do, obviously just by looking at it, you won't have a solution.
605320	612410	71	A	0.99989	So this basic intuition is clearly shown in the cognitive reflection test.
613580	625336	72	A	1	And what's more interesting about it is that the cognitive reflection test actually predicts the performance of various Aristocrats and Biases tests.
625448	634720	73	A	0.79223	So if someone does well on cognitive reflection test, they're more likely to go well on any other reasoning task.
636420	643650	74	A	1	And if they go bad on the cognitive reflection test, they likely will go bad on other reasoning tests as well.
644440	660872	75	A	0.99	To me, this is also an indication that this dual process structure of intuitive reasoning versus slow reasoning, reflective reasoning, likely explains the rest of the text as well.
660926	664328	76	A	0.99999	Not only these two, but most of the others.
664494	667240	77	A	0.99988	So it's a good explanation of the literature.
668480	680060	78	A	0.88049	Okay, so what does the theory say besides besides explaining the reasoning literature?
680820	703120	79	A	0.84149	So, in 2008, Jonathan Evans tried to make a formulation of it that would include and unify views from different domains such as social cognition, neurosciences, other people in psychologists which had similar theories.
703200	713368	80	A	0.74	And it seems like there's something in common here to unify, but Evans wasn't able to.
713534	725180	81	A	0.52	And so there's a formulation specific for reasoning and we don't know if it applies so much well to other areas of cognition.
727680	737612	82	A	0.9	One common way to describe dual process theory is by making these tables of features.
737756	750150	83	A	0.99976	So we say, oh, type one processes have these sorts of features on the left, and type two processes have this other type of features on the right.
752600	774780	84	A	0.99996	This table here was one I made by joining some of the features of Evans insteadovich, and also of Kunman, and then with some questions I raised in the thesis, I elaborated it this way, but it's very similar to the Yavin Instant formulation.
776080	791890	85	A	1	One problem we have with this tables formulation of the Duo Process Theory is that usually people agree that in one thinking instance, when you're solving a problem, you're not going to have all of those features together.
793400	799732	86	A	0.81152	So we don't know for sure when some feature will be present or not, for instance.
799866	808840	87	A	0.91261	So clear that type one processes necessarily have to be unconscious and type two processes necessarily have to be conscious.
810060	816120	88	A	1	I think that comes a lot because of some conceptual issues in the theory.
817020	826536	89	A	0.99489	But one solution they have found was to have at least some defining features but different authors have different defining features.
826648	842880	90	A	0.99993	So here I have some of the defining features of the main authors, which is type one processes use less working memory while type two processes most strongly on working memory.
843860	853008	91	A	0.52178	Type one processes aren't autonomous in the sense that they don't need to wait for higher processing to respond.
853104	855910	92	A	1	If they have an answer, they go for it.
856280	874110	93	A	1	And using the couple's representations mean that means that type two systems can reason beyond what's in the present environment can think about things that are not there.
875600	881500	94	A	0.94511	Type one processing is faster in comparison while two is slower in comparison.
883060	893164	95	A	0.9997	Type one processing is less effortful while two is more effortful in the sense of psychological effort.
893212	897940	96	A	0.99715	Like people report having a hard time doing things.
898010	912330	97	A	0.65178	So for instance in the cognitive reflection test, people that solve the test correctly usually rate it and it's harder because they had to use tattoo processing to solve it.
917160	938508	98	A	0.81507	Okay, but Samuels, which is a philosopher, dual process theory and other subjects, he says okay, even if these features proposal is good, we still need a mechanism to explain why these features are linked together.
938594	952512	99	A	0.79639	So what is the mechanism that explains why type one processing features go together and what is the mechanism that explains why type two processing goes together?
952646	962390	100	A	0.98998	So we need some beyond saying that they're different, we need to explain what are these mechanisms that make them different?
963560	974032	101	A	0.59	And I actually had Samuels on my PhD committee and I'm not sure if he was convinced with my solution.
974096	976952	102	A	0.94315	So my PhD was a solution to this problem.
977006	985470	103	A	0.47071	I'm not sure how he was convinced of it, but he approved the PhD, so that's good enough, I guess.
989040	989790	104	A	0.91982	Yeah.
991520	1004000	105	A	0.9826	So the goal was to solve the unity problem, was to explain the mechanisms that explain type one processing and explain the mechanisms that explain type two processing.
1004420	1030760	106	A	0.71	And I started to see, to search in the literature first, how people were conceiving of the general frameworks that were behind these two types of reasoning and one that was not actually related to this literature of dual process theory but was actually famous in cognitive science, was the modularity of mind in the 80s proposed by philosopher Jared Schroder.
1031260	1050156	107	A	0.63	And he said that we had two types of processes, a modular process mostly related to input and perception and we had central processes that was mostly related to higher reasoning.
1050188	1054960	108	A	0.99994	So you can trace some sort of resemblance there to do process theory.
1055940	1065776	109	A	0.79	And the way he went about this is that he said these perceptual models, modular processes are information encapsulated.
1065888	1072650	110	A	0.68568	So vision only works with vision and language only works with language, space only works with space.
1073180	1086808	111	A	0.66	And so these were domain specific knowledge systems that did not communicate with each other and when they needed contact sensitivity, they needed central processes.
1086904	1109408	112	A	0.99989	So these central processes he would argue, were Isotropic and Quinine, which means that any type of information could be used in a solution so we can relate, I don't know, physics with biology and form a new solution.
1109504	1121540	113	A	0.99613	So there's knowledge from multiple domains working here that would also need some sort of context sensitivity, which was a problem for photo.
1121620	1129992	114	A	0.8177	He didn't think we would solve this context sensitivity problem at all and he had good reasons for that.
1130126	1131812	115	A	0.99504	In classic AI.
1131956	1135772	116	A	0.97994	We had a lot of issues with that.
1135826	1149532	117	A	0.55	I mean, we still do have issues with the context sensitivity problem, but not the sort of issues we had in the 80s when they were debating this same problem of classical AI.
1149596	1155920	118	A	0.99978	So this problem was a problem of how to represent a changing environment in symbolic representation.
1158040	1189580	119	A	0.99441	So yeah, it was a problem because classical AI attempted to give knowledge to robots or artificial intelligence and statements about the world and they were intractable, hard to compute, there are so many knowledge banks to search and so many knowledge to try to use that the robots couldn't do anything relevant.
1191760	1195552	120	A	0.99997	They also couldn't represent change.
1195686	1208420	121	A	0.70164	So the changing environment was terrible for them, didn't work with this symbolic representation, it didn't work with serial reasoning or anything like of the sort.
1208570	1213700	122	A	0.99833	This was a huge problem for classic cognitive scientists.
1215320	1237848	123	A	0.82	And since photo was a believer in cognitive science in the sense of a classical computational theory of mind or language of thought theory, he said that he had found Photo's first law of the nonexistence of cognitive science.
1237944	1245730	124	A	0.99995	This law states that the more processes are global, the less cognitive science will be able to understand them.
1247380	1252364	125	A	0.99844	So he thought only these encapsulated processes could be studied.
1252492	1269380	126	A	0.69	And if we needed this sort of global and context sensitive type of processing, then contest science just couldn't do it because he had in mind contra science in the sense of classical AI.
1273820	1292812	127	A	0.46757	But it also seems not only folder, it seems that the literature abandoned classical cognitive science in various ways, which were strange because if you take the work of Alan Newell and Herbert Simon, it's strange to think that oh, they were completely wrong.
1292866	1299010	128	A	0.99852	There's nothing, nothing useful that they say that could be applied today.
1299380	1304610	129	A	1	All the evidence, all the theories they made are just completely wrong.
1305620	1308290	130	A	0.99914	That's a weird way of thinking.
1308820	1319110	131	A	0.54	And well, if you take for instance, our theories today of the human Bayesian brain, they don't consider this at all.
1321480	1323304	132	A	0.99935	It's just like they don't care.
1323342	1327560	133	A	0.69279	It's a different strand of theories.
1327980	1339400	134	A	0.99855	But it also seemed to me that there must be at least something wrong about, something right about this shouldn't be totally abandoned.
1339560	1343820	135	A	0.99974	There must be a place for these results.
1346900	1369260	136	A	0.4906	Explanation of the Mind one thing that was in odds with the way Photer described his dual process theory, and also sometimes Stan of a trend talks about dual process theory.
1370640	1386500	137	A	0.99816	It seems like forgetting this fact here is the fact that we have a limited capacity when we use conscious processing or working memory.
1387400	1391510	138	A	0.89	And this is one of the best evidence in psychology we have so far.
1392280	1408648	139	A	0.99803	We had behaviorist evidence for reinforcement, enforcement, learning and that stuff and then we had cognitive psychology in the this is the best evidence that cognitive psychology has had for a long time.
1408734	1413500	140	A	0.99956	So it's also something we should not simply ignore.
1414240	1435520	141	A	0.99988	So we have in short term memory the knowledge that when we store some information in short term memory, we usually can only keep track of about seven items, sometimes even less in selective attention.
1436200	1467464	142	A	0.99997	There was this task where there was a video of people playing basketball and the subjects had to count how many passes they were making and suddenly a gorilla passed by on the scene and people didn't notice the gorilla because they were so concentrated on the task, which is some evidence of selective attention.
1467512	1475660	143	A	0.99987	That is, when you're concentrating on tasks, you actually don't even see other stimuli.
1479860	1494624	144	A	0.99985	We have also a competition for limited resources when doing dual tasks, or if I'm talking on the phone and writing at the same time, I'm probably not going to do a very good writing because we're competing for limited resources.
1494752	1503428	145	A	0.99	And it's important to note that if you're walking and writing or walking and talking, since those are different tasks, they won't compete.
1503524	1505880	146	A	0.90218	So we have some competition.
1506460	1515580	147	A	0.91463	We have more competition when these limited resources are disputed also in working memory.
1515920	1526464	148	A	0.9981	So if I say the number 793241 and I ask you to repeat them backwards and you're not looking at the slides, you're going to have a hard time with that.
1526662	1548150	149	A	0.99989	Because of the limited capacity of working memory, when it's clear that in other areas of our thinking of our brain is using a lot more information than seven items to make the scenes happen, perception and to make us walk and so on.
1552460	1563000	150	A	0.99749	So that's on the side of the type two constraints, on the side of type one reasoning.
1563820	1567576	151	A	0.6272	Our Audi has some intuitions similar to the ones I described.
1567608	1570620	152	A	0.99995	So he said from its earlier days.
1570690	1583116	153	A	0.87	The research that RSD and I conducted was guided by the idea that intuitive judgments acquired position between the automatic operations of perception and the deliberate operations of reasoning.
1583308	1596464	154	A	0.61576	Communric claimed that intuitive thinking is perception like and that intuitive predictions is an operation of system one further that the boundary between perception and judgment is fuzzy and permeable.
1596592	1603080	155	A	1	The perception of a stranger as menacing is inseparable from a prediction of future harm.
1603500	1613444	156	A	0.99814	So you can see that this is very likely, very similar, sorry, to the intuitions that we were speaking on when we're talking about predictive processing.
1613492	1624130	157	A	0.99617	So feminine saying when you look at a face of someone and he's mad, you have a prediction that you're in trouble, right?
1624660	1640300	158	A	0.99996	So this is a sort of judgment, that same sort of perception, and also the context sensitivity is pretty much solved in predictive processing.
1640460	1645620	159	A	0.8	And if you take on the AI literature that also happens.
1645770	1649128	160	A	0.82603	For instance, paper attention is all you need.
1649214	1657368	161	A	0.55363	They have some tips for how to deal with context sensitivity, which photo didn't dream of.
1657534	1660860	162	A	0.99994	So this is something that Clark said in 2013.
1661280	1676192	163	A	1	The best overall fit between driving signal and expectations will often be found by, in effect, inferring noise, in the driving signal and thus recognizing a stimulus as, for example, the letter M say in the context of the word.
1676246	1685570	164	A	0.84667	Mother, even though the same bare stimulus presented out of context or in most other contexts, would have been a better fit with the letter N.
1686500	1695664	165	A	0.99	A unit normally responsive to the letter M might, under such circumstances, be successfully driven by an N like stimulus.
1695792	1709288	166	A	0.99935	So what is Clark is trying to say here that since we represent using probability, then if the context says we should go left, we go left.
1709374	1722110	167	A	0.99975	If the context says we should go right, we go right, because the representation is fluid and it accepts changes really quickly.
1723060	1728400	168	A	0.99999	So that's something that stems from the predictive processing architecture.
1733220	1741424	169	A	0.99973	But one issue I think that we might see in the predictive processing architecture is the lack of compositionality.
1741552	1752680	170	A	0.99973	So symbolic representations exhibit compositionalities meaning that complex representations are built by combining simpler elements according to rules or syntax.
1753020	1760696	171	A	1	The meaning of a complex representation is derived from the meanings of its constituent parts in the way they are combined.
1760808	1767320	172	A	0.99659	This property allows for the generation of new and meaningful expressions by manipulating symbolic structures.
1767480	1786610	173	A	0.95372	So Folder in 88 argues that even if connectionist models succeed, they would likely have some sort of simulation of a language of thoughts that exhibits conventionality in order to work like we do.
1787080	1796390	174	A	0.88	And we'll see that it seemed like it's the case with the results we have this year.
1798840	1819256	175	A	0.99987	So considering those constraints and the initial field footer and the problem I was trying to solve, which is the unity problem for blue process theory, a major shift I had on the PhD thesis.
1819288	1823630	176	A	1	And then I wrote a chapter on this book.
1824320	1827520	177	A	0.44	The Chan Book of Founded Rationality with Keith.
1828420	1848410	178	A	0.68186	So we explain here in this book the chapter The Shift Power Structure, which is, I mean, it's not clear that a dual process theory theorists would adopt Folder, but it's still very similar.
1850220	1860600	179	A	1	The way the received view of dual process theory works, which is type one, is a number of dumb heuristics blind modules and animal intelligence.
1861740	1867608	180	A	1	And then type two is the real contextual and complex reasoning, the real human intelligence.
1867784	1884956	181	A	0.9997	So I tried to invert that power structure by saying that the contextual, predictive processing is a type one reasoning, which is giving us the basics to reason on.
1885078	1888160	182	A	0.96103	So our reasoning already comes with context.
1888320	1892180	183	A	1	We don't need to reason the context with type two reasoning.
1893240	1899748	184	A	0.99999	We only use type two reasoning to fix some minor issues in predictive processing.
1899844	1903800	185	A	0.99	And we use heuristic search because it's limited.
1904460	1910910	186	A	0.99999	We only use it when there's a lot of prediction error and predictions aren't working that well.
1911360	1921820	187	A	1	And then we call for this heuristic search to handle whatever is missing to see if we can find a new solution that wasn't captured.
1922160	1930530	188	A	0.99952	So in this case the powerful system is the predictive processing and not the Type Two research.
1936870	1939970	189	A	0.99054	So that was the general idea for this shift.
1940550	1946318	190	A	0.99995	Now I'm going to say some hypotheses that stem from this shift.
1946414	1963942	191	A	0.99874	So t one processes deal with content encoded in the form of probability density functions, which is how predictive processing uses to represent the world, which means there is no symbol and no definite content but values, means, standard deviation.
1964006	1979182	192	A	0.63561	Influenced by previous movements in previous world contingencies, manipulating prior information, biases the distribution to one or another direction closer to or further from a certain value.
1979316	1985022	193	A	0.95926	So people here in the active entrance institute we know this by heart.
1985076	1988850	194	A	0.99956	I'm not changing anything here, I'm just using what they say.
1988920	2000790	195	A	0.80053	Basically these functions are not stored in a memory bank but distributed from the responsible brain regions over to the external organs, body parts through neural connections.
2001850	2029710	196	A	1	The values in the distribution do not represent objects directly indiscreetly they refer to distinct aspects of the input when perceptual systems are dealing with such objects this is aligned with the T one processes being easily biased when working with references to similar properties like similar numbers, objects, rhymes or pet names very often an incorrect value is picked from a distribution.
2030130	2058230	197	A	0.99861	An example that I have from this is when usually I have seen people confuse their youngest child with their dog name youngest child and when someone has another new child then the new youngest child is often confused with the dog because they are, I think, somewhat represented in similar distribution similar points of the distribution.
2059290	2065690	198	A	0.99998	This is also in line with claimants of embodied proposals that the world is not representative in symbols.
2067230	2074810	199	A	0.67	T one processes are subpersonal and their predictions are made by the same systems which process perception.
2074970	2096740	200	A	0.64	A clear example is that a judgment about a facial expression is related to the FFA which is a region brain that processes faces and this is also related to that judgment that conman may argue that when looking at someone we make judgments about their face.
2097670	2107222	201	A	0.91	The idea is that perception is not passive but already comes with predictions and when in problem solving such prediction is precisely the Type One answer.
2107356	2118170	202	A	0.99995	So if we look back at the reasoning test all those Type One tests are likely stemming from predictions.
2119790	2139934	203	A	1	I don't know what to claim that T one processes are purely perceptual if in contest to cognitive only that such predictions reasoning is that there's not a clear line between what is perception and cognitive.
2139982	2146242	204	A	0.69903	So I think the word you're just using different words for this is the problem.
2146296	2152026	205	A	0.62025	So just not getting into that convenience.
2152078	2175790	206	A	0.99969	Example of judgment of negative spatial expression shows how this is expected of dual process theory like it's expected of dual process theory that Type One reason works under predictive processing, also in line with the claims of embody cognition that there is no sharp link between perception and reasoning.
2176610	2184350	207	A	1	On the other hand, c two processing works like a classical machine for reasoning, such as General Problem Solver of Newman Simon.
2185010	2191358	208	A	0.63465	However, this classical machine only makes sense in the brain if it exists in a wider setup.
2191454	2224250	209	A	0.92833	Predictive processing network generating Type One responses like in Newell's Physical Symbol system, when facing a reasoning problem, T Two Processing opens a problem space containing an expression that designates the initial problem and an expression designates a solution which was produced by a probabilistic prediction having the initial expression and the predictive expression in the problem space.
2224400	2232414	210	A	0.52	T Two Processing then uses its move generators to attempt to reduce differences between them and sometimes find different solutions in such bad.
2232532	2235120	211	A	0.99998	So let me just explain this a little bit better.
2235810	2245986	212	A	0.99362	When Newell was working in the 60s there they knew they couldn't search all possible space of a problem space because it wouldn't work.
2246168	2259320	213	A	0.99988	So they have this library search which works like a detective, let's ignore the problems which are likely wrong, just look at the spaces that might be right.
2261290	2277434	214	A	0.99974	What this predicting and reflecting framework does for a risk search is that the risk of search only works on boards already with the prediction that was sent from Type One processing.
2277562	2286980	215	A	0.99989	So it's like we only start reasoning and making searches based on the prior predictions we already had.
2287750	2293940	216	A	0.99	And we only go further with this search when the predictions aren't working.
2295110	2306470	217	A	0.53	And the main point here is that the risk search isn't searching a random space or even a pre programmed space.
2306620	2313958	218	A	0.63624	It's searching a space that was left out from the predictions that started in Type One processing.
2314054	2327950	219	A	0.99975	So that's how it doesn't go to the frame problem anymore because a large part of the contextual issue was also already solved by predictive processes.
2329650	2346580	220	A	0.99415	Okay, so that was the general hypothesis I presented to attempt to solve the unity problem, which is the problem of how Type One features go together and how Type Two features go together.
2347190	2356550	221	A	0.98	And now I'm going to try to argue that this is likely the case, since that I can't prove this is the case.
2356620	2368086	222	A	0.99999	But there are good reasons for us to think that Type One processes use predictive processing and that Type Two processes do not use predictive processing.
2368198	2373020	223	A	0.99992	Rather they need some kind of symbolic heuristic search.
2375010	2386550	224	A	1	One thing that stems out of is a good explanation for the difference between implicit representation and explicit representation.
2386730	2398062	225	A	0.80727	If you look at the literature in psychology, you will see that it's very ambiguous in these definitions.
2398126	2410086	226	A	0.58707	Like people use implicit and explicit representations as the same thing as unconscious, conscious or fleeting graspable, or the same thing as automatic in control.
2410268	2421660	227	A	0.54	And so when it goes to dual process theory, it's not very clear what they're saying in each case, but they use this expression and explicit a lot.
2426050	2441342	228	A	0.9529	How I think this model solves this is by saying that implicit representations are probability density functions while explicit representations are symbolic representations.
2441406	2443970	229	A	0.71792	Classical symbolic symbolic representation.
2446070	2454594	230	A	0.68237	Clark in his book sometimes mentions that we can have single peak distribution distributions.
2454722	2468966	231	A	0.99996	But the problem with having single peak distributions is that well, if you only have one peak on the distribution function then it doesn't have all these features of probabilistic representation.
2469158	2474194	232	A	0.64868	It's more likely similar to a symbolic representation.
2474262	2486910	233	A	0.72	The fact that having a single peak distribution might be the way that a probability representation turns to a symbolic representation.
2487750	2492210	234	A	0.99291	So these have different and important features.
2492950	2503270	235	A	0.71	The probability representation is continuous, it's uncertain and ambiguous and that's why it's able to be sensitive to context.
2504170	2509154	236	A	1	The mother example of part it can vary on retrieval.
2509282	2512380	237	A	0.73284	We can remember something and be different the second time.
2514270	2515846	238	A	0.78312	It's tied to priors.
2515958	2518060	239	A	0.99974	It works with statistical relations.
2519710	2530350	240	A	0.99331	Yeah, it's called in and depending on which value it's most probable at the time, it has a different outcome.
2531330	2542030	241	A	0.55013	So I argue that explicit representations are likely discrete, they are likely symbolic in nature, they're unambiguous.
2542110	2547694	242	A	0.85396	This is what allows us to disabilite in the first place they're stored reliably.
2547822	2556280	243	A	0.63573	If you have a reliable definition of it, you will likely remember it the same way the next time.
2556810	2566938	244	A	0.54025	It's arbitrary in the sense that it's not really coupled to the stimuli, in the sense that the perception of perception usually is.
2567104	2571740	245	A	0.47	The statistical relations are related to aspects of the world.
2573150	2579150	246	A	0.87533	It's using compositionality and it's immutable in the sense that you can't change it.
2579220	2583760	247	A	0.87895	It's fixed value doesn't change.
2586230	2589326	248	A	0.80977	So that's the implicit versus explicit feature.
2589438	2595102	249	A	0.69254	You can have also explanations for automaticity in contrast to working memory.
2595246	2604870	250	A	0.99987	So automaticity concerns overlearn skills and overlearn skills are understood as skills that have become predictable in this framework.
2605690	2610710	251	A	0.99862	So we usually use this to explain like driving, riding a bicycle, riding.
2612430	2628414	252	A	0.82247	So when you're driving, if you don't know how to drive, you need certain statements, like some statements about how you, how you should steer, right?
2628452	2630080	253	A	0.99637	You need to have those in mind.
2630610	2638770	254	A	0.61167	But then once you learn how to drive, it's like your body already knows what it's doing and you don't even need to think about this.
2638840	2641742	255	A	0.78537	So this is the classical distinction in psychology.
2641886	2652520	256	A	0.96105	But I argue that the predictive processing theory makes a new enlightenment to this.
2653530	2657398	257	A	0.99417	So it's like our body is predicting what we're doing.
2657564	2658786	258	A	0.99987	So it's automatic.
2658898	2661746	259	A	0.99781	So it's an explanation for optimisticity.
2661938	2670026	260	A	0.99998	But say that a dog certainly suddenly runs from his car.
2670128	2672874	261	A	0.53351	You obviously don't have a model for that.
2672992	2680240	262	A	0.99998	So you're going to need to call in work in memory to solve some of those issues so you don't hit the dog.
2681170	2693220	263	A	0.9999	So everything works under prediction unless the model is very unreliable and then working memory is called in to solve some further issues.
2694550	2694914	264	A	0.53436	An.
2694952	2699794	265	A	0.77705	Interesting hypothesis I had on the PhD thesis which I never explored.
2699922	2716390	266	A	0.99	I actually don't even think anyone read this part, but it seems to me to be a very interesting hypothesis in relation to free energy in active inference.
2716470	2726400	267	A	0.93656	So that's why I'm bringing it here in the Active Inference Institute that as we know, the brain attempts to minimize free energy by getting predictions right.
2727170	2744690	268	A	0.95304	Thus higher need of type two processes are related to higher free energy interpreting information in the sense that when the predictions are working clearly, then you don't need to have effort.
2745430	2754598	269	A	0.99996	But when there are a lot of prediction error, then you need effort because you call in the working memory to do a risk search.
2754764	2759926	270	A	0.94418	So to minimize a great amount, better amount of free energy will take more time and work.
2760108	2765946	271	A	0.99991	This is more effortful than having predictions ready that minimize free energy as quickly as possible.
2766128	2772250	272	A	0.66431	Before, when probabilities fail, the system needs to start risking possibilities.
2772830	2776940	273	A	0.79528	It searches for other possible solutions by means of heuristic search.
2777470	2784526	274	A	0.43651	Heuristic search would be related to more information and time because it does not have probable solutions ready.
2784708	2789342	275	A	0.99993	Instead it needs to investigate its state space almost from scratch.
2789486	2792094	276	A	0.53359	We say almost because it is heuristic.
2792142	2796590	277	A	0.74	And hence it also will have tricks to get the correct solution faster.
2796750	2807990	278	A	0.45205	Unlike group force search, which would investigate the state space from beginning to end, we believe reports of effort would be related to executing more heuristic searches.
2808650	2815494	279	A	0.99556	Reports of effort by subjects would seem to be based on cognitive informational and physical constraints of reality.
2815622	2825730	280	A	0.9995	So that's why I think this is an interesting part of the hypothesis is because we have this psychological measure of effort.
2825830	2830826	281	A	0.99999	But if this is true, then we actually have a physical measure of effort.
2831018	2847220	282	A	0.99065	Like if people are working hard on the problem, that means there's a lot of free energy going on and they are having to use this the risk of search to fix some issues that the model isn't able to fix by itself.
2852630	2854982	283	A	0.98822	Yeah, so that's that's the effort part.
2855036	2873020	284	A	0.91607	So now going for the working memory part, what's interesting here is that in working memory, working memory, it's expected of it that it works like a classical computer.
2874110	2886110	285	A	0.96758	So working memory is a widely research topic in psychology, but in predictive processing it's rarely, rarely mentioned.
2886260	2893380	286	A	0.55	And I actually searched the predictive processing books, control F for working.
2893830	2902466	287	A	0.99	And people rarely use this topic because it's not done by predictive processing.
2902498	2905670	288	A	0.97976	It has nothing to do with predictive processing.
2906090	2919126	289	A	0.95048	It's actually an issue for predictive processing because well, if you're going to eliminate classical symbolic processing altogether, then we need a predictive processing explanation of working memory.
2919238	2923850	290	A	0.94	And so far I haven't found one and I don't think there will be one.
2924000	2927914	291	A	0.99999	But this also could be just ignorance of mine.
2927962	2932910	292	A	0.99997	Maybe there's some paper there which I haven't found might prove me wrong.
2932980	2949650	293	A	0.99995	But in any way, working memory is very aligned with what Turing was thinking when he was making the connection between machines and minds.
2950250	2962150	294	A	0.93733	So if this is a sentence of Alan Turing explaining his computer, right, the Turing machine.
2962810	2969580	295	A	1	And if we change the word computer to work in memory, it clearly works.
2969950	2980382	296	A	0.66764	It's almost like it's talking about the same thing, when of course, if we change the word here, computer, to predictive processing, it's clearly not talking about the same thing.
2980516	2990254	297	A	0.91909	So the behavior of the computer or working memory at any moment is determined by the symbols which it's observing in his state of mind at that moment.
2990452	2997586	298	A	0.49991	We may suppose that there is a bound b to the number of symbols or squares which the computer can observe at one moment.
2997768	3002390	299	A	0.99999	If he wishes to observe more, he must use successive observations.
3002730	3008306	300	A	0.99984	We will also suppose that the number of states of mind which can be taken into account is finite.
3008498	3014730	301	A	1	The reason for this are the same characters as those which restrict the numbers of symbols.
3015070	3022486	302	A	0.99999	If we admitted an infinity of states of mind, some of them would be arbitrarily close and would be confused.
3022598	3034190	303	A	0.99999	Again, the restriction is not one which is seriously affect computation since the use of more complicated states of mind can be avoided by writing more symbols on the tape.
3034850	3050450	304	A	0.99457	So this is very similar to what we think about when we're talking about working memory, not only because we were instance by Turing, but because of the evidence that comes from working memory is very similar to what suspected of this sort of machine.
3051930	3054642	305	A	0.96216	Finally, we have speed.
3054786	3066860	306	A	0.99338	So predictive processing has various strategies to make the processing be faster as fast as possible.
3067310	3090512	307	A	0.99602	While that's not true for symbolic classical AI, so yeah, Clark says cheap, fast world exploiting action rather than the pursuit of truth optimality and deductive inference is now the key organizing principle.
3090576	3104520	308	A	0.73711	When he's talking about predictive processing, the predictive processor is always taking certain bets about what the current state of the world implies losing accuracy, compensation for speed.
3105740	3112170	309	A	0.99997	We also have predictive coding in the more strict sense.
3114460	3123448	310	A	0.97818	So by predictive coding we mean specifically the property of the system to consider from the world only stimulant which result in greater prediction error.
3123544	3130220	311	A	0.9998	So there's also a filter there which allows for speed in perception and focus.
3130290	3139600	312	A	0.7121	Beyond predictive prediction relevant stimulant only permits the agent to quickly decide forces of action and select amongst the possible forces.
3140040	3147008	313	A	0.99974	So predictive processing is tailored to be fast, necessarily.
3147184	3154816	314	A	0.85847	Well, that's not the case for symbolic AI and so that's the explanation for the difference in speed.
3155008	3160648	315	A	0.85072	So beyond that, the T two processes need to figure out the solutions online.
3160734	3168952	316	A	0.99978	So it's different if you have a prior prediction that would say to you what the answer is from having to search its place from scratch.
3169016	3169628	317	A	0.82718	Right.
3169794	3176316	318	A	0.99983	There is also another issue that the biological brains certainly were not built for.
3176338	3187964	319	A	0.53	A serial heuristic search photo pilotians say that the morals that the absolute speed of a process is a property part excellence of its implementation.
3188012	3199088	320	A	0.54	And since the brain does not have a symbolic processor implemented, if we do use some sort of heuristic search, it's like a different adaptation.
3199184	3202070	321	A	0.54327	It's not what the brain is used to.
3202440	3207210	322	A	0.23015	Inserting problem spaces is slower than having a problem outcome ready.
3209980	3216780	323	A	0.97525	Okay, so that was the work I did in my PhD thesis.
3217440	3238640	324	A	1	And those one, those were one of the main reasons why I think this framework is good for explaining the differences in features of dual process theory.
3239940	3251268	325	A	0.75	And I actually wrote a prediction that I recently learned that I wrote this because I, I forgot I wrote this.
3251354	3259290	326	A	1	And then I went back to to the thesis and and I said, well, I actually said this would happen and it kind of did.
3259820	3267180	327	A	0.5822	So that was one of the reasons why I renewed my interest in my PhD thesis.
3267920	3281520	328	A	0.99657	So what I said was predictive processing system would be subject to bias from lack of compositionality such as mistakes in connectivity, failures in noticing necessary character, formal rules and so on.
3281670	3285724	329	A	0.4554	Precisely the type of mistake, type one processing incurs.
3285772	3289776	330	A	0.51	And so at that point we didn't have large language models.
3289808	3293696	331	A	0.91654	In fact, the attention is all unique.
3293728	3295750	332	A	0.99919	Paper came out in the same year.
3296920	3308840	333	A	0.96	And then I noticed that this could be seen as a prediction of the type of issues that large language models are facing, that's reliability.
3309260	3318060	334	A	0.99845	Hard time keeping the order of units or steps in complex reasoning or math straight, hard time letting go of priors.
3318400	3334370	335	A	1	And to my what really motivated me to start working on this scan was that they solved this precisely by using dual process theory and t two agents they're calling.
3335300	3347456	336	A	0.7268	So what they're doing now to solve these reliability issues is adding a heuristic search to the genetic model and it's having awesome results.
3347488	3350710	337	A	0.58356	This is just this year 2023.
3352140	3368990	338	A	0.54762	So the more they develop these type two agents to answer these reliability problems of large language models, the more the large language models are getting good at stuff.
3369840	3385584	339	A	0.99671	So this is a paper, I got the images from the paper tree of Socks, right, which is one of the last ones, probably came out about two months ago, something like that.
3385782	3405988	340	A	1	The way they're doing this implementation is they're taking the results outputs of the large language model and they're doing further reasoning on it and then feeding the large language model with the result of the further reasoning.
3406084	3411560	341	A	0.99	The further reasoning they're doing is very similar to the classic symbolic AI.
3412460	3423960	342	A	0.99984	So here they explain there's a simple input output prompting here, which is simply inputting a prompt and the large language model will give you an output.
3424120	3429600	343	A	1	And as we know, these outputs sometimes are very biased.
3430180	3434610	344	A	0.98859	There's hallucination on it, they're not reliable, all that stuff.
3435620	3448768	345	A	0.55	And last year they figured out that through chain of thought prompting we could also already increase the reliability of these models.
3448864	3473704	346	A	0.99997	So you give an input and just as we do, the better prompting when we go to Chattbc and say, I'll do this better fix this issue, solve this step by step, some of the commands that are good for Chatter, PT and Chain of thought prompting automated some of that, some of those reasoning steps.
3473752	3480256	347	A	1	And then large language models were working much better after this.
3480358	3481792	348	A	0.99608	So this was last year.
3481846	3503050	349	A	1	And then last year this started a new trend of research to make these agents coupled to large language model output linguistic outputs back into the large language models to see how they can get better.
3505100	3512440	350	A	0.63	The self consistency one, you give the input and then there are various solutions.
3516240	3525464	351	A	0.59	The self consistency forces the large language model to try different solutions and then you get a majority vote.
3525512	3538640	352	A	0.955	So if you have like five similar answers, some different paths, then this one SPicked over the one which is less popular.
3539400	3553210	353	A	0.63	And the Tree of Thoughts, which I thought was most similar to the proposal I had in psychology, in Duprose theory, is the one which opens up a problem space.
3554300	3557444	354	A	0.88711	It's actually pretty much what I just explained.
3557492	3578210	355	A	0.93154	In the case of psychology, they open up a problem space and they start to search possible solutions which are better than the original and they feed that back into the predictive processing, sorry, the generative model, the large language model.
3579620	3586930	356	A	1	And then this one is currently one of the best ones they have.
3588740	3596870	357	A	0.99971	So here I made a general diagram of how this would work.
3597880	3600480	358	A	0.6758	It's not psychology nor AI.
3600560	3603210	359	A	0.99464	It's like what's singular in the two?
3604940	3610760	360	A	0.99989	So we have a generative model here using probability density functions.
3611260	3619260	361	A	0.64571	This looks more like a traditional cognitive cognitive connectionist model than a predictive processing.
3621120	3623068	362	A	0.95338	It's just the diagram is not that good.
3623154	3625310	363	A	0.67211	I'll talk a little about that.
3628500	3636172	364	A	0.57702	We generate an answer here, which is to the problem here, and we get a linguistic output.
3636236	3642740	365	A	1	And this linguistic output then goes for an heuristic search using symbolic representations.
3644440	3650470	366	A	0.99798	So if you get a better answer here, feeds it back to the Genitive model and so on.
3651000	3661176	367	A	0.99999	Basically, the knowledge part is the Genitive model and the heuristic search is simply using more steps to see to prompt it back.
3661278	3679680	368	A	0.78705	It's more like a prompting scheme than a new knowledge scheme, which is both true for the dual process theory model I created and the heuristic search that's happening here on the T, two agents for LLMs.
3681220	3690660	369	A	1	A better way to do this, and I think Tree of Thoughts does implementate this, is that the input layers are trying to predict the thoughts.
3691320	3695620	370	A	0.38923	It's a bit more like predictive process than the other diagram.
3697080	3706840	371	A	0.93	And so they're trying to predict the thoughts and by open up this searching this problem space during these steps.
3707660	3713370	372	A	0.99998	We already have new prompts here for the Gentrific model.
3715500	3722270	373	A	0.99996	So if you take predictive processing seriously, and this does happen in the brain, more likely like this.
3722720	3733120	374	A	0.6	I do think that the Tree of Thoughts paper does say that the intermediary processes interfere in the searching on the aliens.
3734660	3749296	375	A	0.91616	Finally, I thought maybe we could suggest something from psychology for these models to keep growing, working differently based on what we know from psychology.
3749488	3763892	376	A	0.98051	So some things that are not implemented yet is that executive functioning have these updating abilities in intelligent forgetting, which is related to insight problem solving.
3763956	3769964	377	A	0.22213	It's when you forget something, but it means, like, you forget the prior, right?
3770082	3776692	378	A	0.97453	You're using a different prior, so you're able to let go of biases.
3776776	3777410	379	A	0.59852	Right.
3778180	3782880	380	A	0.58958	There's also work on thinking dispositions, which could be relevant.
3784900	3791888	381	A	0.99559	Sentences like this belief should always be revised in response to new information of our evidence.
3792064	3799460	382	A	0.97899	So these would be, like, imperative linguistic knowledge, which would have to be added on.
3799530	3803320	383	A	0.99	I don't know where, but likely would help work.
3803470	3809796	384	A	0.69979	There's a lot of thinking dispositions that are relevant to our successful reasoning.
3809828	3811690	385	A	0.57	And this is just one example.
3813020	3816300	386	A	0.99	The literature knows a lot more examples that could be relevant.
3817360	3835920	387	A	0.5397	We have in creativity research, we have this generative phase, which is similar to what the generative models are doing, but we do not have the exploratory processes going on, which is common in the creativity research literature.
3836820	3842180	388	A	0.99408	But it's what we do with, for instance, mid journey when we're doing better prompts.
3843240	3863850	389	A	0.983	So maybe using these exploratory processes from the creativity research, we can also automate the exploratory part of the creativity processes that these image generators are getting to, and not just the generative part.
3864240	3872760	390	A	0.98	And finally, I think the embodiment, you don't have robots or anything like that that do this sort of reasoning.
3872840	3888624	391	A	0.73083	So AI on the computer and also stuff from active entrance, which was not considered slightly related to embodiments in the sense of navigating the world and predicting the world.
3888822	3895270	392	A	0.86283	That stuff is far from happening in the traditional Galileans we have.
3896040	3898470	393	A	0.98891	So, yeah, that's a lot of stuff.
3899160	3903400	394	A	0.39567	Sand, maybe I hope you guys have some comments.
3903980	3921592	395	A	0.99986	As I said, I haven't been able to talk about this to anyone, and I do think I have some, at least some maybe some relevant stuff that I've published.
3921656	3927576	396	A	0.77261	But since I'm not famous and I don't have people to talk to, it's mostly gone unnoticed.
3927768	3929820	397	A	0.99866	So thanks for being stay tuned.
3932980	3933680	398	B	0.99982	Thank you.
3933750	3934940	399	B	0.99997	Awesome presentation.
3935020	3935852	400	B	0.5	All right! Nick,
3935916	3940790	401	B	0.94744	it'd be awesome to hear your introduction and then take it wherever you'd like to go.
3942200	3942564	402	C	0.9998	Sure.
3942602	3945892	403	C	0.65345	So first, thanks, Samuel, for interesting talk.
3945946	3948388	404	C	1	And I've been really pleased to find your research.
3948474	3954344	405	C	1	I guess maybe I'm supposed to say something about who I am, where I am.
3954382	3955508	406	C	0.99221	So I'm Nick Bird.
3955524	3964036	407	C	0.99989	I'm at the Stevens Institute of Technology in the New York City metropolitan area in a department that's kind of interdisciplinary.
3964068	3970510	408	C	0.99999	So we have, like, philosophy and quantitative social science, but also, like, people doing, like, music and visual arts and all sorts of other things.
3971040	3975470	409	C	1	I tend to do more of the philosophy and quantitative social science stuff.
3976720	3980744	410	C	0.99806	So I'm kind of also interested in human reasoning.
3980792	4000064	411	C	1	And so much of the people I'm reading and citing and drawing on in my research are a lot of the people that you saw in the opening slides that we saw people like was on and Conneman Tversky and Mercier and Sperber and a lot of the other people that were cited.
4000192	4007704	412	C	1	I think Samuel does a really good job to incorporate the kind of like shoulders of giants that we're standing on in terms of the philosophy of mind, right.
4007742	4010970	413	C	0.72	The voters and the Pilitians and these people.
4012540	4018728	414	C	0.79	And one thing that I think is on everybody's mind these days is these large language models.
4018744	4040336	415	C	0.98356	So I'm really glad to see how this dual process theory that emerged from research like economan and diversky's on these problems like the conjunction fallacy problems or the waste on selection guard task or the conjunction fallacy tasks, how that has been applied to models like large language models and what we can learn from that.
4040358	4053156	416	C	1	And then I think the coolest thing about the preprint print is how we could kind of port back and forth the learnings from both the computer science and the cognitive science to improve one another, right?
4053178	4074572	417	C	0.94408	So I think that one of the neat things about this preprint that Samuel was giving us at the end is that we're kind of taking what we think is a model of reflective reasoning that we represent certain parts of a problem and maybe reason more carefully and effortfully about them in certain situations in ways that might help us.
4074706	4077884	418	C	1	And how can we help these large language models do the same?
4078002	4084240	419	C	1	Because it does seem like large language models function, at least the ones that we interact with online.
4084310	4084876	420	C	0.99901	Mostly.
4084988	4088080	421	C	0.99653	They seem to function mostly as like a system one or type one process.
4088150	4092804	422	C	0.99928	They're just kind of like quickly generating lots of text or imagery or something like that.
4092842	4105370	423	C	0.99999	But they're not necessarily reflecting on it in the ways that we would think that a human is capable of and they might not even be capable of representing things in the same way that we do.
4106220	4108184	424	C	0.90704	Yeah, so I thought that was a really interesting idea.
4108302	4128236	425	C	0.77241	Using chain of thought and tree of thought models to kind of create a reflective level or reflective system within the models seems just like a really valuable idea and just a really great synthesis of research in multiple disciplines, something that I think few people are actually very good at.
4128258	4131036	426	C	0.57613	It's like incorporating some of the best insights from multiple fields.
4131068	4132844	427	C	0.50615	We're often pretty siloed in academia.
4132892	4140450	428	C	0.99992	So I just think this is great work and I hope more people will appreciate and pay attention to it and build on it.
4140820	4153176	429	C	0.83886	So one of the things that stood out to me in this presentation more so than when I was, like, reading this 2023 preprint analytic Reasoning for Large language models by Dr.
4153278	4154840	430	C	0.37959	Bellini Leiche.
4155820	4169710	431	C	0.95	The thing that stood out to me is this idea that Bellini Leiche and Frankish sort of switched which of these two types of reasonings processes are supposed to be context dependent or context sensitive or however you would want to word that.
4170080	4179244	432	C	0.66	And I started thinking, well, I wonder if there's a sense in which both types of responses or both types of reasoning are somewhat context dependent and I'll just say what I mean.
4179282	4188192	433	C	0.82	And then maybe, Samuel, you can kind of say what you think I should think or clarify the view of the predictive and reflective framework or something.
4188246	4202464	434	C	0.9998	So the thought I was having was, well, in a familiar context, these intuitive predictions, these type one processes, our gut response, so to speak, those are going to be pretty useful because they're well trained.
4202592	4204312	435	C	0.99996	We have a lot of experience that we're drawing on.
4204366	4207530	436	C	0.99434	So like our gut response, our first response is often quite good.
4207980	4215704	437	C	0.989	It's in these less familiar contexts, or maybe similarly familiar, but, like, way higher stakes or something.
4215742	4226716	438	C	1	But it's in these other contexts where we might think, maybe I should slow down and make sure I've double checked whatever my initial impulse is before I just accept it because there's a lot riding on this.
4226738	4232048	439	C	0.99999	Or, like, I'm just not used to this type of problem, so I need to slow down.
4232214	4243910	440	C	1	And so there's a sense in which what context is doing is not just showing up in one or the other type of reasoning, but it's sort of like determining which type of reasoning might be best at the moment.
4244280	4252100	441	C	0.99997	But I'm wondering, is that just totally compatible with what you're imagining or is that somehow a deviation from the framework, the predictive and reflective framework?
4252940	4256916	442	A	0.69542	Yeah, thanks for the thoughts.
4256948	4263050	443	A	0.99324	There compliments and thanks for the question as well.
4264080	4281996	444	A	0.41875	So the reason why I think context needs always some sort of type one help is because of the constraints we have learned from symbolic AI.
4282108	4295350	445	A	0.99999	So if I'm saying that type two reasoning have the same constraints, then it cannot be contextual because it doesn't work like that.
4296040	4299830	446	A	0.99997	We know that if it fails in context, it fails bad.
4300600	4312312	447	A	0.99775	But of course we do need to like when we have a novel situation, we often solve this by context, right?
4312366	4313850	448	A	0.87348	So how could this be?
4314640	4321470	449	A	1	And I think it's by the interconnection of the power of the two system.
4325440	4334604	450	A	0.88718	So we always start with a prediction, which in this case would be mistaken, and then we have to search for novel answers.
4334652	4341552	451	A	0.86	And in this search we make the contextual comparisons.
4341696	4349110	452	A	0.8959	So it can't be that the system too is making the comparison on its own.
4349480	4365470	453	A	0.69	I think it's more likely that when it's a case of context that stems from a novel situation and then the two systems have to figure out it together.
4366080	4386930	454	A	0.99467	Likely in the sense that I was pointing out here in the end, these predictions that go over the thoughts, they most likely are the ones that will solve these context issues in novel problems, I'd argue something like that.
4392280	4395792	455	C	0.90689	Okay, I think that's somewhat helpful.
4395856	4400916	456	C	0.99986	So then maybe what I'm thinking is something along what the lines you were saying at the end.
4400938	4412356	457	C	0.99998	You were saying how there's still kind of an opportunity to understand things like executive function in this framework.
4412548	4415660	458	C	1	And I think that's maybe part of what I'm wondering about.
4415810	4432928	459	C	0.92506	So I guess I'm wondering if there's more to be said within this predictive and reflecting framework about how each of these two types of processes get selected or, like, what might help the system.
4433014	4433984	460	A	0.57934	Hold on.
4434182	4434624	461	C	0.95877	Go ahead.
4434662	4439576	462	A	0.92368	That specific point you just remember, I just remembered something that's related.
4439708	4445910	463	A	0.99562	So, yeah, clearly our working memory or type two reasoning likely does more than that.
4446520	4449990	464	A	0.51366	It does this, but likely more than that.
4452300	4464840	465	A	0.89668	There's likely also a belief bank, something like the thinking dispositions, some sort of knowledge bank, which is related to type two processing.
4466240	4480172	466	A	0.99897	So Keith Franklin does this distinction between flat out beliefs and I'm not remembering terms, but he has a type one belief and type two belief.
4480316	4485792	467	A	0.99971	Type one belief is that belief you have, but you're not fully confident of it.
4485926	4499670	468	A	0.79	And the type two belief is the belief you have when you have a very firm political position and you state it with obvious words that you sure you believe that.
4500760	4511432	469	A	0.97688	So there's likely to be some sort of different belief structure as well, which I don't talk about at all thesis of the work.
4511486	4521630	470	A	0.55923	So I'm not saying, obviously that this is a complete model of reasoning, and obviously we'll need more to figure out how reasoning works.
4526530	4527470	471	C	0.99341	Yeah, okay.
4527620	4528930	472	C	0.99997	That's also helpful.
4530070	4533700	473	C	0.9854	I'm wondering if Daniel, did you want to weigh in?
4535030	4542600	474	B	0.99976	Well, I think it's a very interesting question about which of those modes are engaged or how they're balanced through time.
4543210	4553110	475	B	1	And how does this connect to what's known as the context window in today's transformer type large language models.
4554350	4565202	476	B	0.99409	So how do you map the computational attributes of large language models today, like architecturally or their Ram or CPU usage?
4565366	4570074	477	B	0.60633	How do you map that onto, for example, human cognitive processes?
4570122	4573440	478	B	0.99996	Do you think that's useful or there are any insights there?
4575090	4595846	479	A	0.6963	Well, I'm mostly making a relation between predictive processing and generative models, but I know that large language models are not entirely the same as what active inference is saying.
4595948	4604280	480	A	0.74485	I'm aware of that, but there is some similarity specifically in regards to the generative model part.
4605150	4617306	481	A	0.93778	So I'm not sure I'm not an AI specialist to be sure which details of a generative model could be true of the human brain.
4617418	4633940	482	A	1	And I'm most likely betting that the people who study generative models in the brain, like the Frisons and all, have figured out something like what our generative models do.
4641270	4653810	483	B	1	One other thought or nick, you want to add there just thought of some different ways whether people have connected it explicitly to the literature on working memory in predictive processing.
4653890	4657942	484	B	1	I think, as you pointed out, it's definitely a link that is not highlighted.
4658086	4673946	485	B	0.99948	It's one that we recently heard from Professors Walker and Monriquez and Pristine in the recent Livestream 53 that was on like, cognitive paleoanthropology looking at human working memory.
4673978	4693700	486	B	0.99999	But some ways that people have incorporated working memory is like a nested model that carries context at a deeper time, but that is not necessarily like an actual mechanism that gives rise to why type one and type two are the way they are.
4694890	4710314	487	B	0.997	So I thought that was a very provocative direction to go to move past the descriptive and then to look at the underlying generative model of why these outcomes are the way they are.
4710352	4726110	488	B	0.99988	Even though these cognitive phenomena are causes of other things happening, they are also caused by some influenceable or contextual or variable aspects.
4726610	4737140	489	B	0.81	And when we lose that context dependence of the cognitive systems, then they're totally lifted and disembodied don't really help.
4738390	4763610	490	B	0.99946	So by putting the primacy on that predictive processing element, I think it opens the door to connecting those areas beyond just mentioning the terms in proximity, but to really support different epistemologies from predictive processing as a model or approach.
4766430	4767180	491	A	0.98238	Yeah.
4768030	4770014	492	A	0.99924	Can you go on that idea again?
4770052	4772298	493	A	0.99495	I'm not sure what you want me to comment.
4772474	4775406	494	A	0.99996	Can you summarize that again and make it a question?
4775588	4776222	495	B	0.99996	Sure.
4776356	4790210	496	B	1	What do you think the epistemological consequences are of taking predictive processing the way that you have approached it here versus alternative or prior approaches to cognitive sciences?
4792070	4795110	497	A	0.99955	Okay, but what do you mean by epistemological.
4798490	4799430	498	B	0.31485	Normative?
4800170	4801160	499	A	0.53393	How we know.
4803610	4810250	500	B	0.99959	How it influences our understanding of how we know or seek or practice or do or decide.
4812350	4812906	501	A	0.99967	Perfect.
4813008	4815820	502	A	0.99971	Okay, let me think about that, for instance.
4817870	4837810	503	A	0.98365	Yeah, I think one of the best what this helps with most is explaining dual process theory because dual process theory is in bad shape in terms of concepts, in terms of theory and formulation.
4839830	4846390	504	A	0.66584	People have no idea what these systems refer to in the mind brain.
4847610	4852600	505	A	0.84534	They often have different intuitions into this.
4852970	4866246	506	A	0.96	And basically what I most think this work is relevant for is for saving dual process theory from criticism, which has been happening in psychology.
4866358	4877822	507	A	0.93141	So people are like, it seems like we're talking about the left and right brain, something like that, something that we don't know for sure what these two systems are.
4877956	4878640	508	A	0.60314	Right.
4882070	4888690	509	A	0.86232	So this is a good way forward, I think, for dual process theory.
4889030	4897080	510	A	1	And as I said, dual process theory does have some value in itself.
4899690	4911578	511	A	0.82502	So, yeah, saving the process theory, I think is a good direction for this model.
4911744	4933810	512	A	0.99986	But also if we think about the advantages of it would be something like, for instance, it could have helped us had the idea of using Chain of Thoughts or Tree of thoughts.
4934230	4936130	513	A	0.99836	Had anyone read this before.
4936200	4945874	514	A	0.99976	So it can help us make these sorts of predictions that we shouldn't have some sort of zero reasoning coupled to the gentle models.
4946002	4969340	515	A	0.999	So yeah, I think it could help us think further on on AI, if we AI, like Nick was saying, to incorporating more stuff in psychology that we know that's certainly missing here, but here we have a direction of what we should incorporate and so on.
4970210	4978686	516	A	0.99732	But I'm also unaware of the working memory research from predictive processing you mentioned.
4978868	4985780	517	A	0.61	And if you want to go back to that and explain me some more of that, I'd be happy to hear.
4988950	4990098	518	B	0.91312	Yeah, sure.
4990184	5001670	519	B	0.97905	So not that this is the most effective way to implement a memory system, but at least that this provides analytical method for measuring or describing them.
5001820	5018186	520	B	0.99999	If one were going to have the five digit number, you could imagine a nested model where the lowest level is the ones place and it's nested within a decision tree of tens places and hundreds and so on.
5018368	5028346	521	B	0.75	And then there's some seek or access policy or strategy that helps speak the number in reverse.
5028538	5037730	522	B	0.69	And some cognitive or computational limitation is just how well that cognitive agent can perform on that test.
5037880	5052834	523	B	0.98306	Again, that doesn't mean that's the mechanism that's being used, but that would be a way to use a nested generative model to encode like multiple levels of spatial or temporal variability.
5052962	5062498	524	B	0.99998	But context becomes an issue because you're basically just expanding the possibility space looking for sparser and sparser associations.
5062674	5079790	525	B	0.99986	So if you don't have a good compositionality or really well definable well articulated causal process, then you're just building these all by all models that you're going to be searching through in the dark.
5082390	5093410	526	A	0.97852	Yeah, one point I want to comment on that is that, well, it's not clear that the best way to reason will be the way humans reason.
5093480	5093918	527	A	0.56861	Right?
5094024	5104550	528	A	0.99902	So we might eventually have a different way of implementing working memory on an LLM which would be more effective than ours.
5105610	5124170	529	A	0.55047	There's also to the other side, there's also the point that it seems that the limited capacity we have on working memory, maybe it's not just a lack of power, maybe it's necessary.
5124250	5133890	530	A	0.999	So we kind of like don't lose our minds in the sense that we have a limited space to search and then we're able to search this limited space.
5133960	5149122	531	A	0.7	And if it was too, if we have too much space to search because we have super working memory, then we wouldn't be able to finish the task anytime.
5149266	5157290	532	A	0.99918	So maybe this is like a physical good barrier, something that helps.
5158110	5183726	533	A	0.89	And it's interesting that what I find interesting is that okay, the AI may find a lot of different and better ways to implement work memory, but it's interesting that they implemented a worksheet memory or a search agent, which was very like the way I described in the thesis.
5183838	5186900	534	A	0.98391	That was what I found amazing.
5187990	5189220	535	A	0.99999	As you were saying.
5189590	5202950	536	A	0.95214	There's very ways to implement it working memory, but they did it in a very similar way in the sense of searching the space after the genitive model has offered a solution.
5203850	5220894	537	A	0.7092	So I don't know, maybe that's a good indication that this is on the right track, but it also may just simply be a simple way they found to solve the problem.
5221012	5225038	538	A	0.76	And it was just by coincidence similar to this.
5225204	5230846	539	A	0.58179	Yeah, maybe the correct way or the way the brain does may also be different.
5230948	5231922	540	A	0.9965	We don't know.
5232056	5245810	541	A	1	All we have was the limited evidence I offered a bit more obviously and some constraints we know on AI information processing.
5246870	5248134	542	A	0.65597	We have to figure it out.
5248172	5263260	543	A	0.78	And I think that this implementation stuff you're asking is mostly due to the AI people to find out or the math people, the systems to figure out.
5265630	5269370	544	B	0.99308	Nick, do you want to comment or a question or I'll ask one from the chat.
5269870	5270186	545	A	0.81548	Yeah.
5270208	5277486	546	C	0.94987	So I just have a kind of a bigger picture question that kind of gets us into the realm of these chat bots, right?
5277508	5285214	547	C	0.74627	So you in this preprint and in this talk have like given us a variety of different tasks in the heuristics and biases literature.
5285262	5293218	548	C	0.88	The the conjunction fallacy task, the the lend a problem, some people call it, or these cognitive reflection test questions.
5293384	5305286	549	C	1	And you know, there's a, there's like a paper in the fall that showed that a preprint in the fall that last fall that showed 2022, that showed GPT-3 and 3.5.
5305388	5311186	550	C	1	And even the earlier versions, like, showed a lot of the human, like, faulty intuitions.
5311298	5314040	551	C	0.7356	Like it basically fell for the lure pretty often.
5314810	5317430	552	C	0.51626	Sometimes, like most of the time it was falling for the lure.
5317510	5326110	553	C	1	But then as soon as GPT Four was available to be studied and used on these models, it was like performing basically near perfect.
5326260	5334626	554	C	1	And when it did get the incorrect response, it wasn't necessarily the lure, it was just like some other general type of error it was making, like misinterpreting the question altogether or something.
5334808	5345250	555	C	0.72	And it seems like the predicting, predictive and reflecting model framework could have at least two different ways of explaining this.
5345320	5357394	556	C	0.98	One is what happened is they changed the AI's like type one thinking or whatever so that it could respond intuitively to all these, but intuitively and correctly, so it didn't need to reflect.
5357522	5372650	557	C	0.83263	Or it could be that they somehow created this reflective type system with either like a chain of thought or a tree of thought or some other type of system that helps it actually engage in reflection on these tasks or whatever the analog of reflection would be for a chat bot.
5373470	5380414	558	C	0.99	And so I'm wondering if you have thoughts on this, as you've been thinking about this or talking to anyone about this, how do you think they did this?
5380452	5382734	559	C	0.99999	Obviously we have to speculate because a lot of the data is proprietary.
5382782	5384530	560	C	0.99999	But I'm just curious to get your thoughts?
5385270	5386914	561	A	0.85986	Yeah, that's an awesome question.
5387032	5398470	562	A	0.85	I actually forgot to mention this on the presentation that the biases they make on the recent test last year were exactly the ones the humans made.
5398540	5408978	563	A	0.99982	So if we fed it some dual process real, it would give exactly our mistakes.
5409154	5410380	564	A	0.95642	That's so amazing.
5411310	5422800	565	A	0.99049	I'm not certain why that happened, but it did and how they fixed this.
5423970	5441794	566	A	0.96888	So, yeah, I do think that they trained the model on these tasks because people were able to get similar issues by changing the questions a little bit.
5441912	5453238	567	A	0.99997	So I think they cheated on that in the sense that they trained on known problems, but that's not true of new problems.
5453324	5462742	568	A	0.99937	If you give it some different formulation and feed it to 3.5 new version, it's still failing.
5462806	5479214	569	A	0.99999	But the people on Open AI, they are aware of this chain of thought research, they are aware of this tree of thought research, but I don't think they want to depend or rely on these external agents.
5479412	5493634	570	A	0.9102	So they recently released a report where they are trying to reinforce intermediate steps of reasoning on GPC four.
5493752	5510658	571	A	0.99701	So they try to simulate internally what the chain of thoughts reasoning is doing externally by reinforcing the intermediate steps of reasoning so that it starts failing composition, helping.
5510754	5511640	572	A	0.75389	So on.
5514910	5516330	573	B	0.9	A lot of thought on that.
5516400	5526138	574	B	0.52	I was reminded by Daniel Dennett's framework or model in Darwin's dangerous idea of cranes and skyhooks.
5526234	5538114	575	B	0.73	And so some building is built and then the question is like, was it built with cranes or with top down hung skyhooks that just sort of descend from nowhere, hold everything up while it's all being built?
5538152	5540638	576	B	1	And it's like, well, no, it's with cranes.
5540734	5544846	577	B	1	And then if you need a big crane, you can use a smaller crane to assemble a crane.
5544878	5553570	578	B	0.98788	So that's kind of a bottom up constructive metaphor, whereas the sky hook is like the top down compositionality.
5553730	5570490	579	B	1	And that's when you get to certain level of sophistication for a cognitive system, it can make the blueprint or the plan like maybe what we would associate with the expected free energy calculation, not just the variational free energy calculation.
5570650	5581280	580	B	0.99988	So at some point when a plan can be made, it requires a strategy between type one and type two switching and probably other switches too.
5582610	5589186	581	B	0.82	And so that crane approach, you need a lot of compute to do a bottom up.
5589208	5606694	582	B	0.99991	So even though it's kind of weird to think about, it's almost like the current large models are very bottom up because they're very bottom up from syntax and they don't have what you refer to as imperative linguistic knowledge, which can be normative as well as a heuristic for knowledge.
5606742	5621658	583	B	1	And that's potentially a small model, but it contains wisdom that's explicit and practices and stances or dispositions as you described.
5621754	5633358	584	B	0.99	And so it's kind of like it's not a denial of embodiment to also clarify what this skyhook ability is.
5633444	5637680	585	B	0.87	And that's type two like so I think.
5640050	5640574	586	B	0.92	I guess.
5640612	5641694	587	B	0.99959	What do you think about that?
5641732	5645320	588	B	1	Or how do you connect this to anything in that area?
5646410	5655480	589	A	0.50648	Yeah, I'm going to make you do the summary and the question again, because although I did track some of what you mean, I'm not sure which ones made to comment on that.
5656590	5674110	590	B	0.983	Do you think that the landscape of large and small models would be different if people took on board some of the features you described, like imperative linguistic knowledge, seriously into construction of models?
5675970	5681022	591	A	0.92439	So let me just see if this is something you're asking.
5681156	5691650	592	A	0.99174	So they're doing small models now for phones and open source models for phones, and they have the issue of not being able to scale up.
5691720	5692340	593	A	0.96863	Right?
5692790	5696040	594	A	1	Because if you scale up too much, then you can't run on the phone.
5696810	5706338	595	A	0.99983	So you're asking if we can implement something like Tree of Thoughts to help these smaller models.
5706434	5707660	596	A	0.80318	Is that what you said?
5708030	5708780	597	B	0.99995	Yes.
5710110	5714154	598	A	0.99982	Are you sure or are you just saying, okay, go for that?
5714272	5718074	599	B	0.49359	That's an example of what it would mean to re understand.
5718192	5721018	600	B	0.96578	Like where do we need an API call to a cloud center?
5721104	5726798	601	B	1	Or where could some context be locally computed with much more of a type One?
5726884	5730974	602	B	0.99922	What are some type two problems that we might be able to type one our way out of?
5731092	5740690	603	B	0.99996	What are some type one areas where things are not working and a type two description might be advantageous?
5741350	5744774	604	A	0.72556	Yeah, I think they're getting onto that.
5744812	5757286	605	A	0.52	And they're being able to make these smaller, smaller models precisely because of these sorts of discoveries they made on Chain of Thoughts.
5757318	5757946	606	A	0.60489	Right.
5758128	5762540	607	A	0.99996	There's also stuff about being able to train on specific data.
5762910	5764300	608	A	0.6	I read one.
5765070	5765914	609	A	0.61163	It's called?
5766032	5767980	610	A	0.98026	Textbooks are all you need.
5768510	5774302	611	A	0.96728	They're training smaller, smaller models on textbooks, and they're doing better.
5774436	5795238	612	A	0.97501	So, yeah, there are differences in training smaller models, different data, but there's also the possibility of making them more reliable based on what I'm calling T Two Agents or Train of Thoughts, stuff like that.
5795324	5816906	613	A	1	And again, we don't know if that's all we can do externally, but I would argue that we would need necessarily some sort of external mechanism coupled to any large language model for the best results because of the differences in representations, mainly because of this.
5817088	5831354	614	A	0.99999	Unless the large language model somehow makes a symbolic representation emerge from its processes, it's not clear that it has done so.
5831492	5865546	615	A	0.62731	But unless it does so, and it uses that reliably because it may even have generated some sort of symbolic process internally, but doesn't always use that or doesn't know when to use that, most likely we're going to need something external to the language model, and this is some clues to how to implement that.
5865648	5890270	616	A	0.77	And also, I think, in our brain, it's kind of external not only in the sense of our minds, but also in the brain, since it's a recent step in evolution, we don't see I don't know lizards reasoning in type two manner.
5890350	5890980	617	A	0.99855	Right.
5891990	5898390	618	A	0.99999	We do see Chintenzees reasoning in type two manner, but not as much as we do.
5898460	5901750	619	A	0.61353	So it's very human like reasoning.
5903050	5909210	620	A	0.94886	It's just that going back to the context problem, animals do solve contextual problems.
5909280	5911242	621	A	0.80756	That's why they can live.
5911296	5911706	622	A	0.98614	Right.
5911808	5914122	623	A	0.99966	So the issue is not actually the context there.
5914176	5922874	624	A	0.67749	It's more like some boost on reasoning capacity.
5922922	5930254	625	A	0.83	And that boost on reasoning capacity may be related to the prefrontal cortex, as we know.
5930452	5936350	626	A	1	And so, of course, the prefrontal cortex is also a network.
5936430	5938958	627	A	0.69055	It's not a serial Turing machine.
5939134	5974750	628	A	0.99999	But if there's somewhere that's implementing a serial machine or a symbolic classic AI machine or symbolic reasoning or something like that, most likely it would be the prefrontal cortex, which is kind of like a distinct addiction addition to the brain and maybe something related to language, which is also distinct, a new addition to the brain.
5977650	5989780	629	A	0.99969	Also, I'm very convinced by the photo and deletion argument in 88 that you're going to need symbolic processing anyway.
5990470	6010070	630	A	0.57231	Either the model is going to generate that symbolic processing, or you're going to have to feed it for him, or else you're going to have hallucination of mistakes and reasoning because of the nature of because of the nature of distributed representations.
6010150	6010780	631	A	0.70633	Right.
6011550	6014570	632	A	0.93686	It's uncertain and ambiguous by nature.
6014990	6025360	633	A	0.94046	That's why I'm not entirely convinced that only probabilistic representations will be enough to solve hallucination issues.
6029490	6030334	634	B	0.99977	Awesome.
6030532	6035380	635	B	0.958	Nick, do you have any other questions or areas you want to mention or ask?
6039830	6041970	636	C	0.91	I think those were my main questions.
6042120	6045686	637	C	0.64	I don't know if there's anything in the Chat that you wanted to touch on before we go.
6045708	6048818	638	C	1	I know we're maybe over time, I'll.
6048834	6052370	639	B	0.99952	Go with one from the Chat, and then we can have any closing thoughts.
6052450	6061526	640	B	0.8965	So Glia maximalist asks, are you familiar with visual pathways in the human brain?
6061638	6064310	641	B	0.92834	Specifically the dorsal where and ventral?
6064390	6071070	642	B	0.99971	What pathways do you think this maps onto the two systems you describe for problem solving?
6073330	6074398	643	A	0.99939	Okay, yeah.
6074484	6094260	644	A	0.9673	So like I said, in 2008, evans tried to accomplish that unifying vision of due process theory, but I'm not very convinced by that.
6094710	6113610	645	A	0.99996	He also said, well, we can't do this as of yet, but I do think it's possible, as I said on the presentation, I just don't know how to do it, how to have a convincing theory of everything, of dual process theory in the brain.
6115550	6118390	646	A	0.95663	It does go on to other regions.
6118470	6123310	647	A	0.95861	Like, you have automatic motor responses and controlled motor responses.
6126210	6136580	648	A	0.99999	Like you said, there's different pathways in reasoning, in figuring out stuff in the visual field.
6138150	6153590	649	A	0.99997	Maybe the what would be related to something and where to another most likely the what would be tapping on to type two processing.
6154170	6167174	650	A	0.79251	I'm not sure how the where would be tapping on that, but yeah, as I'm trying to make the point, if we extend it to the whole brain, we lose what we've learned.
6167222	6169530	651	A	0.83848	We're not sure how that applies.
6174290	6175040	652	B	0.6031	Cool.
6175970	6179600	653	B	0.99954	Well, Nick, any penultimate last words?
6183800	6184980	654	A	0.79171	You muted.
6187640	6188164	655	A	0.79	Oh, wait.
6188202	6189510	656	B	0.96923	Unmute and then continue.
6190920	6191908	657	C	0.99997	Sorry about that.
6191994	6192884	658	C	0.77912	No worries.
6193082	6196020	659	C	0.7149	Thanks to both of you, Dr.
6196090	6199576	660	C	0.75936	Bellini, Lighte and Daniel, for coordinating this.
6199598	6206810	661	C	1	And I'm really just glad to have found the research and to discuss it and connect and looking forward to further conversations, to be honest.
6209120	6209870	662	B	0.99962	Awesome.
6210400	6212860	663	B	0.98244	Sam, any closing thoughts?
6213600	6218780	664	A	0.87919	Yeah, I'm just sad I couldn't go much beyond what I said on the presentation.
6220100	6227970	665	A	0.98455	Most of the stuff you guys asked, I'm not sure I have good answers for that.
6228340	6242672	666	A	0.99964	Basically, what I've done is all I know, and I'm not sure how to go on the future with this, and maybe someone in the audience will do.
6242726	6245850	667	A	0.86413	So I hope that helps, guys.
6247260	6247864	668	A	0.99412	Great.
6247982	6249370	669	B	0.99	I hope so, too.
6249980	6251880	670	B	0.83	All right, till next time.
6252030	6252970	671	B	0.73914	Thank you.
6253340	6253700	672	C	0.92462	Bye.

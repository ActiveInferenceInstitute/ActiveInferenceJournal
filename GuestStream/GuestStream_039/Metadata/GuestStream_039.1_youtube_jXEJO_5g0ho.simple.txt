SPEAKER_03:
hello and welcome this is active guest stream number 39.1 it's march 16th 2023 we are here with Xu Ji Eric L. Mosnino and Guillaume Dumas we're going to have a presentation followed by a discussion section so thank you all for joining and off to you for the presentation


SPEAKER_02:
All right.

Thanks, Daniel.

Yeah, I'm Eric.

I'm a PhD student in Yoshua Bengio's lab.


SPEAKER_04:
I'm Shuji.

I'm a postdoc with Yoshua.


SPEAKER_02:
And we're really excited to be here.

Thanks for the invite.

We're going to be talking about why we can't describe conscious experiences.

So this is going to be our take on a really long-standing problem in the philosophy of mind, but we're going to be looking at it through the lens of computational neuroscience and information theory.

So yeah, hopefully it'll be fun for everyone.

It mixes a bunch of disciplines.

So I think the most salient way to illustrate the problem that we're going to be addressing is to ask you to try and think about how you would describe the experience of seeing the color red.

So probably the kind of stuff that's going in your head is things like, well, it's a bright, aggressive color, you know, symbolizes love, that sort of stuff.

And in a sense, this is a description.

It's effective.

I would be able to guess which color you were describing.

But in another sense, it's really inadequate, right?

If I were blind, for instance, your description would be totally useless.

I would be no better in understanding what red looks like.

So there's this real sense in which conscious experiences are ineffable such that we can't describe them.

And this applies to percepts like red, but it also applies to experiences more broadly.

Like the experience of having a thought is just so ineffable, so hard to describe it.

And really importantly, this doesn't happen with most of our knowledge, right?

I can describe most of what I know, except for experiences.

They have this special place.

So it's a big topic in the philosophy of mind because it relates a lot to this thing called the hard problem of consciousness.

So basically all the hard problem is, it's the problem of how and why physical processes give rise to conscious experiences in the first place.

So this is probably the oldest and most debated problem in the philosophy of mind.

And it's even led many to the conclusion that, you know, actually we're going to give up and consciousness can't be explained with physical theories at all.

You know, that it can't simply emerge from neuroscience, computation, or known physical laws.

So that's a bold statement, and I won't get into all the details about what makes the hard problem so salient, but just here's a few things.

So first of all, we can logically conceive of what are called philosophical zombies.

Philosophical zombies, very different from Hollywood zombies, they're basically beings that are physically identical to us.

They behave exactly like us.

In fact, they even have the exact same neural activity that we do.

but they just aren't conscious, right?

So similarly, instead of these alternate agents being unconscious, we could also imagine them as just experiencing different things when they're in the same states.

So for instance, it's conceivable at least that in an alternate universe, the same exact brain mechanisms that produce an experience of red in this world would instead produce an experience of green in the other and that all experiences would kind of be like flipped in this way.

So, you know, intuitively, we think that these these two thought experiments aren't actually possible in our universe, but even just their logical conceivability suggests some kind of explanatory gap where consciousness doesn't seem to neatly emerge from or be determined by physical state or function.

Another really prominent thought experiment is the knowledge argument.

And this is the main sort of problem that our work is going to address.

We're not going to address all facets of the hard problem of consciousness, but this knowledge argument is one of the biggest arguments against physicalism.

So how the knowledge argument goes is it's another thought experiment and asks us to imagine Mary, who has grown up in a black and white room her whole life.

And, you know, despite this poverty and stimulus, she actually knows a lot.

In fact, we're going to assume that she knows all the physical facts about how color perception works.

So all the relevant physics, all the relevant neuroscience, et cetera.

And now we're going to ask, well, does she learn something new when she steps out of the room for the first time and actually experiences color?

And if we say that she does learn something new, which I think is the intuitive answer, there seems to be a problem.

We assumed that she knew all the relevant physical facts about color perception.

So doesn't this mean that what she learned had to be something that was non-physical?

And if there was nothing more to color perception than physically embodied information and neural activity, then the experience is something describable that she presumably could have read about, understood, and would have already known.

So it feels like this thought experiment really pushes us against physicalism towards this idea that maybe conscious experience has something on top of just information content.

Now, of course, we don't want to reject physicalism.

It's been very fruitful for us in the history of science.

So we really do want to say that experience just consists of information that can be described.

So there has to be something wrong in this knowledge argument here.

And what we're going to do is we're going to bite the bullet and acknowledge that experiences actually are ineffable.

So maybe it is the case that experiences can be described in principle since they consist of nothing more than information.

But maybe they can't be fully described in practice using something like language.

So then the challenge becomes explaining why experiences are ineffable under a physicalist framework.

And that's what our work is really going to be about.


SPEAKER_04:
Okay, so the structure of the talk is going to be like this.

I'm going to summarize briefly why characterizing richness and ineffability is an interesting and important question.

Then Eric is going to talk about how there's a natural correspondence between ineffability and information loss, which is going to let us link biologically plausible attractor models of working memory

to ineffability, essentially because information loss is inherent in attractor dynamics.

Then I will talk about if you consider the ineffability of conscious experience to verbal report as just a special case of information loss between two specific points in the communication pipeline, then you can actually generalize the notion of ineffability more broadly by considering, for example, loss from sensory processing to conscious experience,

and even interpersonal information loss.

We're going to prove using Kolgomorov mutual information that your conscious experience being ineffable to another person implies high cognitive dissimilarity between the two of you under our model.

And finally, we'll talk about whether an exact definition of conscious experience is required, at least for characterizing the nature of ineffability and richness, and also some open questions.

So, yeah, why is richness and ineffability an important question?

There are sort of several reasons.

So first, as Eric said, understanding consciousness is not just of general interest.

It's a longstanding central problem in philosophy of mind.

because it's not easy to reason about, which has led some dualists to believe that consciousness must be at least partly non-physical in nature because they can't see how it could be explained by physical processes.

But it's also a topic that is of great interest in machine learning.

Because if you assume that consciousness is essential to human cognition, then it follows that we won't be able to engineer machines that think like humans without incorporating consciousness.

And so we're actually going to argue in this talk that the confusing aspects of human consciousness, such as its ineffability, can be understood with information theory, which, as you all know, is born from computer science.

We're going to use some primitives from information theory to shed light on why consciousness is ineffable and hopefully to dispel some of the mystery surrounding it.


SPEAKER_02:
All right.

So one of the main primitives over here that we're going to use is to talk about attractor dynamics in the brain and in conscious experience and how that results in ineffability.

So hopefully this sort of perspective is familiar to most people, but.

we could talk about neural dynamics by saying the brain has a state at any particular time, and we could denote this state using a vector.

So for instance, in this plot over here, maybe we could denote each axis in the state, each dimension using an individual neuron, and maybe we denote the activity of each neuron using its average firing rate.

So any states at a particular time would be the firing rate of all the neurons across the brain.

Now, there's nothing special about neurons or firing rates.

You know, if you think that things like the synapse strengths or the dendrite potentials or the astrocytes or whatever else is also important to the representation, you could include those in a state.

They're just additional axes.

The details aren't so important as long as, you know, we have some state that describes what the brain is doing at any particular time.

Now, of course, the brain is a dynamical system.

These states are changing.

So you can think of tracing out trajectories through state space as a function of time.

And the way that the state evolves will be determined just by the recurrent connections within the brain, as well as inputs coming from elsewhere.

Maybe we're just talking about the dynamics within a given brain region.

The inputs would be, you know, the other brain regions talking to it, as well as any sensory stimulus coming in.

Right.

So this dynamical systems perspective of the brain, it's been really instrumental in understanding the computations it performs.

And one of the primary methods for characterizing these computations is to look at what are called the state attractors of the system.

And for us as well, state attractors are going to be a really important part of our ineffability framework in a bit.

Basically, what an attractor state is, is a state where once the system reaches it, it's going to remain there, at least until inputs come in and change the dynamics or noise nudges the state out of this attractor.

So you can see this thing clearly within the figure.

So the X and Y axis over here are like the state of the brain.

Again, maybe something like the average firing rates of the neurons.

These arrows are illustrating the dynamics of the brain.

So how the state will evolve given the brain's connectivity, given the synapses.

And you can see that there's some regions where all the arrows, all the dynamics kind of contract to a single point.

So that's called a fixed point.

If it's one dimensional, it's a kind of state attractor.

And because all the arrows point towards it, once you finally reach that state, you're not going to move out again until the dynamics change because inputs came in or until noise nudges you out.

And you can see that each state attractor also is associated with this sort of

attractive region an attractor basin it's called where you know once a trajectory is somewhere in that region it will eventually converge to the attractor again in the absence of noise um so these these attractors are really common in the brain uh and also in uh artificial recurrent neural networks uh trained on tasks and a big computational benefit is that they provide a form of transient memory right once you're in the attractor you remain there that's where the memory comes in

And because of this and other benefits, they're implicated in a wide variety of neural processes, like working memory, long-term memory, decision-making, and higher level cognition, and we'll argue consciousness in general.

So they're all over the place in the brain.

An additional aspect of them that's interesting and that's going to be relevant for our talk is that you can think of attractors as having a sort of dual, discrete, and continuous nature.

So what I mean by that is that the discrete aspect of an attractor is given by the fact that there's a finite number of them and that attractors are mutually exclusive, right?

You're in one or you're in another.

So this means that we could use discrete symbols to denote which attractor the system is in, right?

There's a finite number of them.

You're either in one or another.

And this basically lets us, you know, identify or label the states using a discrete variable.

There's also a notion, though, in which an attractor is just a continuous high-dimensional vector in this really high-dimensional state space of the brain.

Every attractor basically is just a vector in state space, and that means that they're not arbitrarily discrete states.

They have continuous representations.

We can say, for instance, that one attractor is more or less similar to another one, depending on how close they are in state space.

And that gives a representation to the attractors.

That's the high dimensional sort of continuous part.

So just to recap, the discrete part is you could identify which attractor you're in in the finite set of all possible attractors.

But you could also talk about an attractor through this continuous high dimensional vector that describes where the attractor is in the high dimensional state space of the brain.

To wrap your head around this, there's a good analogy to word embeddings in NLP.

So in NLP, each word has a discrete ID.

That's like the symbolic discrete part.

But also each word is associated to a high dimensional vector that gives it a representation, sort of the meaning of the word, where we could say that words are more or less similar to each other.

Okay, so we're going to use attractor dynamics quite a bit to explain a significant source of ineffability in conscious experience later.

But for that to be relevant, I first need to spend a bit of time to convince you that neural dynamics that produce conscious experience actually do have attractors.

And for this, there's already a lot of work in the literature connecting attractors to conscious experience.

So this part is not something new from our work.

So, for instance, one of the leading theories of consciousness, which is the global workspace theory, actually implicitly implies that attractor dynamics are kind of fundamental.

So what global workspace theory says is basically like across the brain and all the different regions, you have sort of some process worker workers basically.

And these different distributed mechanisms, they can't speak to each other.

very easily they mainly communicate through this like global workspace which is kind of like a bottleneck if i think of it like a blackboard uh that all the different processes across the brain could write to you can also think of it as like the ram of the brain but basically it's collecting information from the different workers um and one key uh aspect about uh the global workspace that's posited in the theory is that for content in the workspace to then be broadcast

to the step processes.

For instance, for us to be able to talk about what's in the workspace, for it to be written to speech regions, the content of the workspace has to be amplified and sustained for a certain amount of time.

It has to be stable, basically.

And this is clearly where the attractors come in.

Attractors, by definition, are these states that are stable, these states that are sustained.

So basically what global workspace theory says is that the context of the global workspace that can be used by the rest of the brain are the attractor states.

And the relevance to consciousness in this theory is that it posits that the contents of this global workspace are the things that we're aware of, the things that we're able to report, the things basically that we're conscious of.

And then just kind of like terminologically, another term for this workspace is called working memory.

The idea there is it's a sort of like really short-term memory in the sense that you could quickly, you know, act on the things that are in this global workspace or report on them.

They're accessible basically.

Okay, so that's one very powerful argument for why attractors are relevant to consciousness.

There's a bunch of other related arguments.

For instance, just introspectively, you could think of what experience is like is a sequence going from one thought to the next to the next.

We kind of have this sequence of discrete events.

And that clearly looks like a dynamical system kind of jumping between attractor states.

And then experimentally, there's also been work showing that in psychology experiments, when you sort of present a stimulus to a subject and you vary the amount of time that the stimulus is presented to kind of bring it below or above the conscious threshold.

Well, in the conditions where subjects report being aware of the stimulus, the main thing that's different is that the representation is really stable and robust in noise.

And those are also clearly properties of attractors.

Okay, so again, we'll use those attractors to identify a source of ineffability in a little bit, but first I need to also introduce how we're going to formalize the very notion of ineffability.

And for this, we're going to use information theory.

Part Shannon information theory, I'll do that now, and then Shu will talk a bit about Kolmogorov complexity as another formalism.

Hopefully, most people are familiar with this, but one thing that we're going to be using is the entropy of a state.

So, for instance, if you have the entropy of some random variable X, to make things concrete, maybe that random variable is conscious experience.

So the entropy of conscious experiences, that value would be high if the distribution over conscious experiences was kind of diffuse and flat.

So there's equal probability for all possible experiences.

And entropy would be low if the distribution over conscious experiences was peaky.

So that's kind of like the green distribution in this image would have low entropy and the red distribution that's sort of flatter and more uncertain would have higher entropy.

And basically what entropy is going to quantify in our framework is the richness of a variable, in particular, the richness of conscious experiences.

Why this is a good measure of richness is because if conscious experiences could kind of take on many possible values with equal probability, they're very kind of diverse in that sense.

Well, then it's something that's rich.

Some other notions that are gonna be important are mutual information.

So here that's denoted by I of actin Y. And what that denotes is how much shared information there is between two random variables.

So again, to make things concrete, X we could take to be a conscious experience, whereas Y, maybe that's a description of the experience, maybe a verbal description.

So in this case, the mutual information will be really high if the verbal description uniquely determines the experience.

So basically, another way of saying that is knowing the verbal description, we have no more uncertainty about what experience is going on.

They perfectly determine each other.

So that would maximize the mutual information.

And in contrast, if the message told you nothing about the conscious experience and you were just as unsure as when you didn't have the message, well, then the mutual information would be zero.

This is useful because we can use it to define a notion of ineffability.

So this conditional entropy that we've written, H of X given Y, is equal basically to the richness of X, so the entropy of conscious experiences, minus the mutual information between conscious experiences and other variables like their descriptions.

That's the mutual information.

What this captures basically is how much information is lost when going from an experience X to a description Y, right?

If all the information was lost, if Y doesn't tell you anything about X, the mutual information is zero and H of X will, well, the conditional entropy will be equal to the richness of X. If Y perfectly determined X, well, then H of X and the mutual information cancel and the conditional entropy would be zero.

So what conditional entropy describes basically is how much information is lost when moving from X, so like a conscious experience, to Y, a description of it.

And that's a perfect description of what ineffability is intuitively, right?

It's how much information is lost when I try and describe something like an experience.

Okay, so I think we have most of the background out of the way, and now we're ready to describe one main source of ineffability that arises due to attractor dynamics.

So very quickly to describe some variables over here that we're gonna be using throughout the presentation.

If you look at that top right diagram, we're gonna have X, which is basically the trajectory of neural activity that's relevant for a conscious experience.

So, for instance, in global workspace, X would be the trajectory of the working memory state.

We're going to assume that this trajectory of neural activity produces a conscious experience.

We're going to be calling that conscious experience S throughout.

But also the trajectory, importantly, is going to follow attractor dynamics.

So they converge to states such as X.

So how does this result in ineffability?

Well, basically the idea is that whenever you have attractor dynamics, there's a many to one mapping from trajectories to attractors, and this induces information loss, right?

Just knowing the attractor, we're not able to kind of go the other way around and infer what the original trajectory was.

So there's this conditional entropy, this ineffability of X given A is gonna be high basically.

All right, why does this matter?

Who cares if there's information loss between the trajectory of working memory and the attractors that it converges to?

Well, important thing here, again, is that Global Workspace postulates that for working memory contents to be accessible across the brain, it needs to be amplified and maintained over a sufficient duration, so thought to be around 100 milliseconds.

This means that working memory is going through these trajectories X, but only A, only the attractor, can be reported.

Only the attractor could broadly affect behavior.

Only it could be written to memory.

Only it could be broadcast to these other processes.

So that means that the contents of working memory, kind of these transient contents in the trajectory X, are inherently fleeting.

There's a rich information that was encoded in the moment of the trajectory, but that can't later be recalled or reported because only the attractors can be recalled or reported.

And kind of if we zoom out and

go back to our philosophical notions of ineffability, this also explains why it's difficult to sort of catch yourself in a thought, right?

Working memory encodes these rich and subtle thoughts through the trajectory, but we can never quite pinpoint or report in words what those thoughts consisted of.

Were they verbal?

Were they pre-verbal?

Did they consist of visual imagery?

It's really hard to say exactly.

And the reasoning would be that, well, there's just a lot of information that's lost.

going from these trajectories to the attractors that, again, we could report on, or that we could recall, or they could just broadly affect behavior.

And so that's one huge source of ineffability arising from attractor dynamics in working memory.

Now, these attractor dynamics also give a lower bound for ineffability during verbal reports.

So now at the top right, I've added another variable, M. That's basically some verbal message that you can be using to describe your experience.

And M is a function of the attractors.

According to Global Workspace, only the attractors can be reported on or used across the brain.

So this means that

Basically, the ineffability of an experience given a verbal message has to be at least as great as the ineffability given an attractor, because M is a function of the attractor, and you can't gain information when applying a function.

You can only destroy it.

But we're actually going to argue that the ineffability is not just lower bounded by the ineffability of the attractor.

It's actually, in practice, quite a bit greater than that.

And the reason is that M, this message, is a discrete variable.

It's language.

Whereas A, the attractor, is this really high dimensional snapshot of some cortical state.

So because of the asymmetry between the richness of these variables, there has to be information loss.

And intuitively what's going on here is that you can think of there as being a many to one mapping from attractors to messages as well.

So, for instance, if I say that I saw fat cats.

Right.

That paints a picture, but it's also leaving out significant details about the original attractor, which presumably encoded things that you were aware of, like the cat's color, size, pose, the background, all these things that you were aware of and that were encoded in the attractor.

But you can't really put into a message without it being prohibitively large.

So again, the idea here is that the ineffability of experiences given messages higher than given attractors because the message divides the space of attractors more coarsely.

It's a simpler variable.

It adds additional information loss.

So one problem is if language is so coarse and simple, why can it describe experiences at all?

And the solution is that while attractors and the message kind of both share this discrete part.

So the message, because language is discrete, can be used to index or identify compositional attractors.

There's this comparable richness, basically, between the message and the discrete part of the attractor, which means that in practice, the richness of the experience H of S is typically much greater than the ineffability of the experience given the message.

So that explains why we still can communicate somewhat with language, even though it's this really low dimensional simple thing.

So to recap quickly, what we have over here is the argument says the detractor dynamics are empirically ubiquitous in neural activity across brain regions.

They've been proposed as a computational model for working memory and prominent models contends that conscious experience is a projection of working memory states.

And one of our key contributions is to connect these theories to argue that attractor models working memory offer an account for the ineffability of conscious experience because attractor dynamics induce significant information loss.

That's the general argument.

There's one quick thing to add over here.

So I've been talking about working memory a lot less about conscious experience so far.

So let's link the two.

What's going on over here is that there's many ways to link conscious experience to X, to working memory basically.

And there's kind of two main options.

So one is we could say that the instantaneous state of working memory is always conscious.

So all the states in between attractors are also consciously experienced.

And how attractors would result in NF ability here is that, again, what you're able to recall, what you're able to report is just the attractor, not the actual experiences in between attractors that you were having.

But there's another possibility, which is to say, well, maybe only the attractors themselves are experienced.

However, importantly, ineffability exists regardless in this case, right?

If only A is conscious, A can still encode the fact that there's information loss during processing and working memory during these trajectories.

Information loss is something that is computable.

It could be encoded along some dimensions in the attractor, for instance, which explains how, you know,

we could be consciously aware that there's information lost during working memory processing.

Okay.


SPEAKER_04:
Yeah.

So now I'm going to talk about generalizing ineffability.

Once you view ineffability as information loss from a source variable to a destination variable, then there are a myriad of different pathways that you can characterize, which we're going to split into two groups.

So pathways confined within an individual or intrapersonal communication and pathways that extend between individuals or interpersonal communication.

So in the intra case, you've already seen conscious experience, the working memory trajectory, the attractor state and the message.

But you can also consider D, which is the input data to the system, and V, which we use to denote the state of processes considered outside the delimitation of working memory.

And you can also consider the cognitive parameters of the individual being communicated to.

So we're going to work with Alice and Bob, and we're going to assume that Bob has a brain which is structurally identical to Alice's, but has different parameters, phi tilde instead of phi.

And we're going to denote Bob's cognitive state using the tilde.

So empirical evidence from neuroscience suggests that the brain is hierarchical in nature, and there are many levels of organization, many instances of attractor dynamics across organizational levels and cortical regions.

So one example is the inferior temporal cortex is a sensory processing area that responds discriminatively to novel sensory stimuli, whereas the prefrontal cortex or PFC appears to be implicated in maintaining the attention modulated representations of working memory.

And to a first approximation in this simple model,

It's easy to obtain the result that richness of one process constrains the richness of another.

So here we have that richness of working memory trajectories.

HX is upper bounded by the richness of its inputs.

And subsequently, we also have that the richness of conscious experience.

HS is upper bounded by the richness of conscious experience and the richness of subprocess states.

Now we're going to zoom out and consider the interpersonal communication case.

Shannon entropy has some drawbacks when it comes to characterizing interpersonal ineffability.

A major one is that Shannon entropy assumes you have access to phi, Alice's cognitive parameters.

So note that H phi S given M, for example, is the descriptional complexity of S given not just A, but also phi.

And since we can't assume in the interpersonal case that Bob has access to Alice's brain parameters, we're going to rely on the Kolgomorov framework for information theory.

If you're unfamiliar with the Kolgomorov complexity, the first thing to note is that Kolgomorov complexity or Kx is defined on instances.

hence the lowercase, rather than variables, which are denoted by capitals.

KX is roughly defined as the length in bits LZ of the shortest binary program Z that prints X and halts.

Um, so there's no explicit dependency on the probability distribution over X, unlike in, um, Shannon information.

Uh, but we can introduce PX, um, by taking an expectation over the Kogomorov complexity of states, um, to, uh, obtain the Kogomorov version of Shannon entropy.

And likewise, um, for an affability, um, we characterized it before using conditional entropy.

In the Kogomar framework, expected K of X given Y is sort of the analog to Shannon conditional entropy.

And that represents the length of the shortest program that prints X given that you know Y or that Y is accessible to your program.

And it's approximately equivalent up to logarithmic terms to kx minus ix colon y, where ix colon y is Kogomarov mutual information, which can be interpreted as the difference in program lengths for printing x, depending on if you know y or not, roughly speaking.

There are a lot of similarities between Shannon information and Kolgomorov information.

For example, mutual information in both cases is maximized if x is equal to y. In the Shannon case, because that means y uniquely determines x, and in the Kolgomorov case, because it means that given y, you don't require any extra bits to print x.

In fact, if you assume knowledge of the probability distribution p, then expected Kogomarov complexity and Shannon entropy are equivalent up to some constant factor, because it turns out that the most efficient way to describe a state x on average, given that you know the distribution, is to encode x with a description of length minus log p x bits.

Um, but if you don't know the distribution, um, which we want to make use of, uh, in, in the interpersonal case, because the parameters of the speaker's brain are not given, um, then the higher, the descriptive complexity of that unknown distribution, then the higher, the ceiling on the difference between expected Kogama of complexity and Shannon entropy.

Um, uh, and in our case, we'd expect that to apply because the space of cognitive states is enormous.

the probability distribution over those states is very complex and the states themselves are in general complex to reconstruct, being very high dimensional vectors.

The second advantage of Kogomorov complexity, in addition to allowing us to explicitly avoid conditioning on the probability distribution, is that Shannon entropy is a measure of statistical determinability of states as opposed to difference in unique states.

So in Shannon information, entropy is fully determined by the probability distribution on states and it's unrelated to the meaning or structure or content of the states.

Whereas Kolgomov complexity is concerned with the difficulty of reconstructing states, i.e.

absolute difference, which corresponds more closely to the lay definition of ineffability.

Here are a number of results that we include in the paper.

So the first one is quite simple.

You can't increase Kogomarov complexity by conditioning on more states because the program can simply choose to ignore the input if it doesn't help to shorten the length of the program.

So we have trivially that Ks given m, the complexity of conscious experience given the message, is at least as great as Ks given m and p phi.

But you would expect this gap to be quite significant.

uh because of the complexity of um p phi um so because in in our case and generally in non-toy cases um the probability distribution itself provides a lot of information so it it it significantly reduces the descriptive complexity of s if you're able to um condition on it um

This first result illustrates essentially the difference in ineffability from the tabular Rasa case on the left where you don't condition on Alice's brain parameters phi to the case where you can assume access to Alice's brain parameters, which is analogous to the Shannon entropy

characterization that Eric was talking about.

But the quantity that we're more interested in is this expression in the green box.

So K S given M and P tilde phi, which is the expected ineffability, well, in expectation, of Alice's experience S given the message and Bob's brain parameters tilde phi.

You can show that this quantity is upper bounded by an expression that scales with K p phi given tilde phi, which is a measure of the cognitive dissimilarity or the descriptive complexity of Alice's brain given Bob's brain.

So in other words, if your experiences are ineffable to someone,

i.e.

the left-hand term in the green box is high, that implies that your brains are highly behaviorally dissimilar under our model.

So we can use this result to provide an account for what Mary learned when she stepped out of her black and white room.

And essentially, what it's saying is that neurotypical Alice's experience of color is ineffable to Mary.

which implies that they are cognitively dissimilar.

And cognitive dissimilarity is not the same as knowledge inadequacy, because knowing how the brain should respond to a particular stimulus is not the same as being able to execute that response when you're actually exposed to the stimulus.

So the cognitive dissimilarity principle is something that we generally believe intuitively, but it's also been studied in neuroscience.

So the ability for the neural activity of two brains to synchronize, which is to say to behave in a mutually determined manner, appears to facilitate communication between individuals.

There's also a connection to theory of mind, which is the skill of being able to infer the thoughts of others.

So if Bob's cognitive functions, FX and FS, which produce his working memory trajectory and his conscious experience, are optimized for decoding Alice's message M,

into her conscious experience S, then ineffability is reduced if Bob's conscious experience S tilde is conditioned on compared to M. Because

it implies that part of the computation of reconstructing S is executed during inference of tilde S, meaning that the smallest program from tilde S, Bob's conscious experience,

and tilde phi, Bob's cognitive parameters, to S, Alice's conscious experience, would make use of tilde S to reduce the sort of residual work that needs to be, the residual information that needs to be supplied in order to determine S, which would shorten the descriptive length of that program.

So in other words, making progress at inferring the conscious experience of others in your own cognitive processing could literally be interpreted as reducing the ineffability of their conscious experience.

Quickly, we're going to touch on the grounding problem.

So as Eric mentioned before, two individuals will generally understand the same word or sentence in different ways.

And our measure of ineffability does capture this dissonance for two reasons.

And first, measures of ineffability, such as we saw in the previous slide, they are bound by a ceiling that scales with cognitive dissimilarity.

or the Kogomarov complexity of p phi given p tilde phi.

And phi includes all the parameters in Alice's computational graph, including those that parameterize functions defined on the input data d, and likewise for Bob.

So that means that

Alice's parameters phi are grounded in a representation that is at least partly shared with Bob's tilde phi.

So if Bob's parameters implement a function that operates differently on input data,

compared to Alice, then they do not inform on each other to a great extent, and the ceiling on ineffability is increased via this K P phi given P tilde phi tan.

And secondly, conscious experience S is a function of phi.

It's computed using a function that depends on phi, and likewise for Bob.

And phi contains Alice's long-term knowledge.

Therefore, S is capable of containing information about the associations that Alice makes in the process of generating her conscious experience.

Um, and, and that's included in the reconstruction targets of, um, KS, uh, which is to say, um, the description of complexity of her conscious experience.

So our model offers an interpretation for the observations of Sperling, which were made in 1960 in this very famous experiment where he showed subjects a grid of characters briefly and then asked them to recall a specific row.

He observed that subjects were generally able to report the prompted row accurately, but not all the characters in the grid, despite being able to report that they had a conscious experience of, they sort of had, they consciously apprehended all characters in the grid.

And our model offers an account for this phenomenon in the following way.

So upon being exposed to the grid of characters and being prompted to report the characters in a specific row, working memory contents represented by the attractor state A presumably contains the identities of those four characters in the prompted row.

as well as a summary over the grid, for example, the approximate number of characters or their arrangement, and an estimate of the information lost by that summary.

Whilst information sufficient to discriminate all characters

would exist at some point in the computational pipeline, but primarily in upstream sensory state V, from which working memory trajectory X and working memory output A are computed.

Subsequently, since the attractor state A is directly accessible to verbal reporting processes, the characters of the prompted row, the grid details at summary level, and the presence of information loss would be directly reportable, whereas full grid details wouldn't.

And note that this

explanation would hold irrespective of where the distinction between conscious and unconscious is drawn.

So whether X, the working memory trajectory, which may contain sufficient information to discriminate all characters or not, whether X is considered conscious or not.

So as this suggests,

One of the points that we make is that, at least when it comes to characterizing richness and ineffability, the exact definition of what constitutes conscious experience, so the definition of F as phi in the computational pipeline, is not that important.

We utilize this minimal computational model without relying on the implementation details in order to allow us to make general statements about richness and ineffability.

And as Eric hinted before, you can account for the report of ineffability from this computational, assuming this computational graph

irrespective of how you define FS.

So if X is considered to be conscious, then the attracted dynamics bottleneck the amount of information accessible to working memory output and verbal report.

But if X is not conscious, then information lost during processing can still be reported on.

It can still be approximately computed and reported on, leading us to report on ineffability.

Um, so finally, uh, some, some future directions for this work.

Uh, one of them is to, uh, link it to the neuroscience.

Um, so for example, to, um, uh, actually get some empirical estimates of the amount of information loss, um, using neural correlates of working memory state, for example.

And also,

I think for a deeper understanding of consciousness, you have to consider why it exists and not just how it manifests.

So this is something that we don't touch on on the paper, but the use of information bottlenecks in machine learning would suggest that information loss actually has a purpose and specifically the purpose is generalization.

It improves the robustness of functions learned from data to noise, and it improves the ability of functions to perform optimally on inputs that were not seen during training, which is what generalization is defined by.

And this is important because if we want to incorporate sort of observations about consciousness into our artificial models, what we really want to capture is the benefit that consciousness affords biological models.

And we might not need to transfer the actual form that it takes in human cognition.

And that's it.


SPEAKER_03:
Awesome.

Thank you for the presentation.

Guillaume, it would be awesome if you could give some opening remarks, and then we can have a discussion and hear any questions from the live chat.


SPEAKER_00:
Sure.

Well, thanks again for the invitation

mind jazz on these beautiful topics so as opening questions and uh discourse i would say like uh a strong message here is that uh through uh the formalization that was just presented we have a

an account that allowed to make the bridge between access consciousness and phenological consciousness.

Something that was described very well in the paper that was cited of Yonel Nakash in Philosophical Transaction B, how you don't necessarily need something more than access consciousness to account for these phenomenal aspects.

But it also left a lot of open questions, like typically

we have seen how there is an information loss within brain and between brains, but we can also complexify the within brain message passage, for instance, adding metacognition.

So, at Mila, we are also working on addition of cognitive architecture on top of global neural workspace with typically attention schema theory from Michael Graziano.

And so there is, again, in these meta-representational steps, a loss of information as well.

And I think from an empirical point of view, at both behavioral and neurophysiological level, there's also a lot to unpack there on how our metacognitive representation of ourselves is also an impoverished information from the real

states that occur and underlying.

And that connect well with the social dimension of consciousness.

And here we tapped a bit on that with the ineffability at the interpersonal level.

But in the case of the attention schema, there is a very interesting reversal from evolutionary standpoint of why the brain comes up to

to represent others at first, and then we are recycling the neural mechanisms to predict others' behavior on ourselves.

There's a kind of flipping of the traditional narrative in cognitive neuroscience with Graziano because the others come first in a way, and self-consciousness becomes a sort of side effect of having to

have all the mechanisms to deal with others.

And here we don't talk too much about the representation of the cell, but I think it's a very interesting path following this work.

Then there is also something that we didn't discuss too much, but we think about the trajectory to the attractor and how many trajectories can lead to one attractor.

But interestingly, from a sequence of attractor, you can recover sort of a trajectory.

So if I have point A and point B and point C, by having the sequence IBC, I will tend to have a trajectory that looks alike each time.

And so it's kind of heading towards the dynamics also of social interaction and culture, so typically cultural artifacts like musics or movies.

are eliciting stuff from a subjective experience that seems like stronger than just words.

And even, I mean, I'm saying stronger than words, but even words are sequences.

So in a way there is maybe in the dynamics of communication, a little bit of information that is retrieved by the interpolation of different discrete states, different attractors.

So it's in the inequality that we saw.

So the inequalities are there, but maybe we can play a bit with those phenomena to see to what extent we can retrieve more information by playing on those dynamical phenomena in communication.

And regarding communication, so we talked about the grounding problem and how

The message and the attractors are kind of aligned between people through a shared statistical environment.

And I think the Kolmogorov framework is interesting in the sense of a generative model of the world that you have to share with others.

In a sense, I'm working a lot on autism and I'm interested in neurodiversity.

The fact that you have this cognitive dissimilarity in the information loss is also very interesting to interpret how diversity at the biological level can have an impact.

on how certain people would have more difficulty to communicate with others, specifically in a neurotypical world.

Like we have a society that is very normative in design for neurotypical.

And so people who are more outside the norm and are more neurodiverse would then have more difficulty to align their generative model and communicate with others.

That's also something that would be nice to discuss or to explore later.

And finally, so since we are on the active inference livestream, I would take the occasion to mention how it could connect with active inference.

We didn't really talk about the active inference formalism in the paper, but typically I'm coming back from a workshop on computational neurophenomenology, where we used

the previous work of Varela on neuroscience and phenology to try to have a generative passage between first-person and third-person experience.

And we discussed during the whole week how computational models can be a third constraint to stabilize this relationship between first- and third-person perspective on consciousness.

I was among the rare people there, not completely convinced by active inference.

And I was trying to argue about the need for plurality at the formalism level, even from a theoretical perspective and for plurality in those domains.

But at least from a formal point of view, to try to not bet all our eggs in the same basket, so to say.

And here it's a very nice example how even you can combine formalisms that were mutually exclusive in the literature.

So for typically coming more from the embodied cognition and Varela's work, people with embodied cognition tends to be more in dynamical system theory and very opposed to computationalism and information theory.

Here we show in this paper that information theory and dynamical system theory can totally work hand in hand.

So it doesn't mean that we need to commit to one or to have all formalism.

But to maintain this plurality, I think, is very important from an epistemological standpoint.

And still, the connection with active inference that could be interesting, and that's one of the things that I kept from last week's workshop, is how, from a social interaction point of view,

synchronization and I worked out on interbrain synchronization and those phenomena synchronization can be optimality for information transfer, as we see here.

But in the context of active inference, it's also the optimal minimizer of free energy because you have the two generative models that are perfectly aligned.

And so you don't have error in your prediction because the other person become a mirror of your own generative model.

So that may be also some bridges and predictions that can show that those different formalisms are multiple ways of looking at the same thing.


SPEAKER_03:
Thank you Guillaume.

Awesome points.

Feel free to discuss if you have any responses and just take it from there, Eric and Shu, or I can ask some questions or read some comments.

Do you have any comments on Guillaume's thoughts?


SPEAKER_01:
Yeah, I think we have some.

Do you have things to say?


SPEAKER_04:
Yeah, that was a long list of comments.

I find your comments about the autism link really interesting because actually that's pretty much directly what the message of the result is, is that cognitive dissimilarity is associated with difficulty in communicating.

So, yeah.

Yeah.

Actually, the last thing that you said about how if the communicator and the listener's brains are synchronized or if they're equal, there's no prediction error.

That's kind of the machine learning problem, actually, as a whole.

I don't know if this is a big thing, but the whole point of machine learning is to discover the true model that you're trying to learn so that you have no prediction error.

So I'm not an expert on active inference by any means.

But one thing that does make me a bit apprehensive about it is that it's so concerned with prediction.

Generative models are about reconstruction and prediction, whereas cognition is about survival.

And prediction is...

part of survival obviously i mean you need to be able to predict you know um i don't know uh not not not crashing into a car or something so that you don't die but but it's it's actually you know it's it's it's it's just a part um um and more generally uh the inference of cognitive states in the brain is not about

discovery of truth in some sense.

It's not about coming up with a true model or explanation for some phenomenon.

It's about sort of optimizing for your fitness,

So yeah, that's something that I don't quite understand sort of what the active inference perspective on that would be.

I don't know if Daniel, you had some comments.

Or not.


SPEAKER_03:
Okay, just jumping with active inference and then I'll leave more comments in the chat, but I think that's a great set of points.

So if I can make one point to Guillaume's first, you spoke about in the presentation, the generalized ineffability.

And it reminded me of a few times in active inference where we're hearing about generalized synchrony, generalized, which means like not necessarily lockstep or mirror, but mutually information encoding and not necessarily linearly correlate and,

synchronized like the end state of the metronomes but there's like complex information transfer especially in this multi-scale setting um generalized synchrony which is enabled by the general generalized coordinates which are the coordinates of motion of the path so these are kind of the summary statistics to describe the Taylor series expansion around that path or to better describe that path

taken in some way and generalized homeostasis, which for certain environmental regularities, it may be sufficient to be entirely retrodictive, or there may be

physical or cyber physical constraints so that you'll have a system that is purely designed a different way so i wouldn't say any theory is necessarily too opinionated with respect to what you could describe and i think part of the reason to want such a descriptive approach which is what i feel like you all did by deciding to describe an analytics framework for ineffability rather than for example a mechanistic explanation but describing some

evidence all of that was just to say pluralism in what were approaching that ineffable space from multiple compressions or bowls to even describe english words scientific models and it reminded me of the recent discussions with lance da costa

who described how other discrepancy measures other than free energy could be used.

And that free energy has been used in machine learning, the variational autoencoders, all kinds of Bayesian applications, because it has a KL divergence.

And so it has some convenient variational optimization properties as a discrepancy measure, but it is not the only discrepancy measure.

and you mentioned two in your discussion which were the shannon syntactic sort of classical information content and then the kolomogorov the program and so that is like another distance measure so in some ways these spaces can't or won't or don't even need to collapse

below pluralism of at least several relatively stabilized kinds but i thought that was a very important point guillaume

about explanatory pluralism and how we can be excited about one area of science or one way of knowing and it's compatible with others ways of knowing because we have some of those properties of differences that were discussed so those are first thoughts before even any active technicality yeah i mean go for it


SPEAKER_04:
No, I just want to remark that KL is distance between two distributions.

Yeah.

Whereas, yeah.

Anyway, yeah, Guillaume, continue.


SPEAKER_00:
Yeah, I was just saying, in the case of autism, this typically shows how also

you don't necessarily need to go to the mechanistic neurobiology to have potential explanation.

And so in the case of Kravr for explaining the brain, there's a very nice definition, very nice landscape of different kind of explanation.

And in the same way, those different formalisms can give different perspective on how to understand what's going on.

autism has been very focused on the neurobiology at some point and the genetics, but maybe it's not at that level specifically that the functional understanding should occur.

I don't say it has no causal relationship because it's highly genetically irritable and so on, but sometimes the functional understanding is not necessarily at the very...


SPEAKER_03:
lowest level of explanation yeah so now to kind of move to active inference and really physics based approaches the trajectory recovery is going to have some maximum information representation with the generalized coordinates of motion it's how paths are represented in many settings

and so that path of least action the bayesian mechanics on that landscape whatever it is empirically it's like fitting a spline in that space or doing some other kind of modeling whether it's spm modeling or hyperscanning there's some kind of generalized time series modeling

between the present and artifacts, the past, the future, other entities.


SPEAKER_00:
What do you mean, Daniel?

I didn't get it.


SPEAKER_03:
just that that kind of trajectory recovery is what is being parameterized in physics of consciousness type models or physics of cognition bayesian physics anything where a probability distribution has a position velocity acceleration and so on it's being path inferred continuous time um generative models and there's discrete time generative models in actinth

and hybrid models, just like other Bayesian graphs.

So in the continuous time setting where there's a Taylor series expansion, that's trajectory recovery.


SPEAKER_04:
Yeah.

Well, I think more generally, maybe the point is that you could use Bayesian learning variational inference to learn a generative model of

these states that we're referring to, right?

If you had some kind of data sets that actually represents a sample from, let's say, the true model, then you could use variational inference to approximate that model.

But you don't have to though.

I mean, there are many generative modeling techniques in machine learning that don't fall under variational inference.


SPEAKER_03:
I'll read a comment and a question.

So, Joshua Bengio writes, Guillaume Dumas' idea about trajectories of consecutive attractors to recover information about likely trajectories that lead into each of these attractors that are normally lost in working memory is very cool.

And then, Shu, Eric, any relation between what you talked about and the compositional properties of conscious thoughts?


SPEAKER_01:
Yeah, I think so.

So in terms of compositionality of conscious thoughts,


SPEAKER_02:
I think what Joshua is referring to now is that our thoughts kind of compose concepts that we already have.

So something like, you know, a sentence, if I think, you know, the fat cat, I'm composing these mental concepts of fat and cat, and that generates sort of a new experience.

So experiences are always generated from these building blocks.

And then similarly, one thing that we didn't talk so much about is that

conscious experiences, language and attractors can also have this shared compositional structure.

So like language, it's clear, you know, base space where it's combined to form new little phrases, new sentences that have novel meaning in a systematic way.

How this could happen with attractors is

Well, kind of in the most basic case, different dimensions in this high dimensional state space might converge to different attractors.

And so now, you know, one attractor that's converged to is really a composition of attractors along different dimensions.

So this is a sense in which our experiences are compositional, our attractors that generate those experiences can have compositional structure, and this could be a reason why language itself is compositional.

We're basically trying to come up with some way of describing or identifying compositions of attractors.


SPEAKER_04:
I mean, I would add that compositionality is a mechanism for reducing descriptional complexity.

So in fact, that's arguably the objective for the compositional nature of our thoughts and language.

Features like less time to describe, more robustness to noise,

Yeah, I mean, if you're trying to describe an instance of a state, obviously the descriptional complexity is much less if it's built up of concepts that you already know about.


SPEAKER_02:
Yeah, just to bring this back to something I mentioned in the talk, I pointed out that attractors have this discrete structure.

There's a finite number of them, and we could assign labels to them.

But if these attractors are compositional, which would allow for a richer space, a richer number of attractors, basically,

it's finite but it's the space of total attractors is exponentially large right so we don't want a language that's assigning one unique word to every composition of attractors it would be an exponentially large language um we want some kind of uh language that will minimize the description like what she said you know you could have a language that's compositional yeah i think that provides a really i think you're muted daniel


SPEAKER_03:
Sorry.

It provides a very interesting and justifying rationale for using the description length as opposed to the KL divergence, which is maybe in some telepathy-enabled world or space, the KL divergence does help you move quickly or on some defined manifold, but...

in an encoding decoding space and on the computers that we have then writing shorter programs and or making smaller information theoretic compressions becomes one of the imperatives

to take the kind of empirical approach that you are describing.

Because though you've described mainly equations, these are all also computer packages that can be used to model different data sets.

So what kinds of data sets do you think this is most proximally applicable to?


SPEAKER_02:
Yeah, are you saying like what kind of data sets would kind of minimum description length compositional models be most useful for?


SPEAKER_03:
Or just with the approaches and measures that you described today, what kinds of data sets or behavioral and cognitive settings do you think that could be used as an empirical tool or measure for?

I think it'd be really interesting to hear about, does the data already exist?

Is it some hypothetical measurement?

Are there ways to even reanalyze?

Yes, Guillaume or anyone else.


SPEAKER_00:
Yeah, I think like on language already, there is like a lot of things that have been done on the number of bites per seconds transferred, for instance.

And I guess like on large corpus of discussions or even in clinical settings, for instance, I'm working on the clinical alliance between a patient and a clinician, for instance.

This formalism of ineffability and information loss would be interesting to apply to as even a form of biomarker.

I mean, even if at the language level, it's not bio.

And then you can also apply those information theoretic measurements to physiological data, movement, neural data.

So in my lab, we are reporting multiple brains simultaneously.

we can totally try to empirically approach what we are describing in this paper and show that, for instance, in certain populations that are more challenged to communicate through neurodiversity, so what we discussed earlier, we can apply the same type of measurements with information theory instead of just classical neuroimaging measurements.

So that would be, for instance,

empirical way of dealing with that.

What I'm very interested in would be to make the relationship between the access consciousness and the V basically process and the information loss that you have in your working memory.

Right now, I feel that the neuroimaging technologies are not at that level of

possibility, but who knows, maybe we're going to have soon the ability to disentangle that kind of stuff now.


SPEAKER_04:
Oh, yes.

Yeah.

So the delimitation of what constitutes working memory in the brain, I think that's not a settled question, right?

I mean, there are lots of different

possible approaches.


SPEAKER_02:
Yeah, I mean, it's totally up in the air.

Like classically in global workspace theory, they traditionally thought, okay, working memory is localized to prefrontal cortex.

Now the new perspective is emerging that, okay, maybe working memory is sort of distributed across the brain.

We see kind of sustained attractor representations all over the place in this distributed working memory.

So all these different pieces

you know what what does what constitutes x what constitutes that working memory trajectory um and and also you know what constitutes the experience what's the function that goes from x from working memory to an experience uh all that is is up in the air when writing this paper we had plenty of discussions about you know what is the trajectory the whole trajectory conscious is just the attractor

conscious?

Is there kind of sense in which both these things are true?

So like Guillaume was talking about access versus phenomenal consciousness.

Access consciousness refers to what I'm able to report, whereas phenomenal consciousness kind of refers more to the standard definition of what is my experience like.

So our framework kind of illustrates that maybe we could unify these things where X, the trajectory, is some phenomenal experience.

It's in the moment, whereas your tractor is really the only thing you're able to report, remember, and you still know that something is missing between what you're reporting and what you were actually experiencing throughout.


SPEAKER_04:
Yeah, I mean, I think in terms of our model, it's pretty clear.

But I think actually grounding it in empirical data sets is...

is a challenging question.

The other thing is that Kogomarov complexity is more of a theoretical measure.

So that's actually, I think, the reason why Shannon information is much more ubiquitous, for example, in machine learning.

Because basically, given a generative model, you can approximate it, for example, just using Monte Carlo

estimation, but Kolgomorov complexity, yeah, it would be much more of an approximation, with a much greater error, I think, if you attempted to approximate that.

So I guess that would be a potential concern.


SPEAKER_02:
Yeah, one advantage of it, if you could measure the Kolmogorov complexity, is it would allow you to kind of measure the ineffability of an experience on an individual case.

So, for instance, like one thing we spoke about with theory of mind was that this complexity of the experience given the message,

We would think it would be much higher than an experience given Bob's experience, given the listener's experience, because the listener's brain is doing a lot of the work of decoding the message into a similar sort of experience.

So it'd be useful if you could see something like, are there certain types of experiences where the message is wildly insufficient?

You know, the complexity of the experience given the message is massive.

But actually people do a really good job inferring the experience.

And maybe it would be different in a case of like autism.

Maybe there's some sorts of experiences that they simply can't reconstruct.

So I think there's some value in trying to create measures of Kolmogorov complexity to kind of handle these individual cases.

The issue is, yeah, it's not really computable.

You can't search through the space of all possible programs, but maybe there are kind of ways to get around this.

For instance, you could sort of parameterize a program with,

I don't know, a neural network, let's say.

You have a neural network going from message to some brain scan for an experience.

And then maybe the size of that neural network or the complexity of the network could be sort of a surrogate for the complexity of the program, the Shores program.


SPEAKER_01:
Yeah.


SPEAKER_00:
That's a cool idea.


SPEAKER_03:
That is really cool.

It reminds me of using adversarial neural networks, which don't have as strong analytical guarantees as, for example, cryptography.

But then they kind of converge in being able to protect or change or modify information in certain ways.

But you aren't getting the same bounds or guarantees on the formality.

And that's kind of like the discrete continuous dialectic.

And so it's important to embody even that pluralism with respect to state spaces, because if the formalisms in any of these domains, the formalism for discrete and continuous, they're not always aligned.

And those are definitely the areas that can be studied.

But for the space of empirical models that we're talking about,

as part of even then methodological pluralism in science.

But if we're talking about statistical models, we wanna be able to intake the data at some point.

So it has to have certain properties, which are even looser than the math.

And there's probably a lot that could fit in this.

And I think it'll be interesting, like when you have a hyperscanning experiment

How do you include the environment or how do you deal with different number of sensors or different types of sensors, different kinds of action and report that people might encode their symbols through symbol, all kinds of accessibility and interface types, which I think returns to Guillaume's point.

So yes, please.


SPEAKER_00:
Yeah, actually, that's also reminding me that there is also a nice empirical playground associated with what we discussed is how we can maximize from an environmental perspective instead of the biological perspective, this information transfer.

So I'm working, for instance, with Michael Lipschitz, which is part of ALIUS as well.

From an anthropological standpoint, humans came out with

rituals or cultural uh practice to to maximize uh the alignments of people and those things also are uh very important in our society or to reach consensus and collaborate and so on and uh i guess like this can also of uh issue would be a very important i mean when we have to uh

converge on optimal collective dynamics.

Here we are talking with Alice and Bob in a dyadic scale, but a nice follow-up also on this work would be at the group level, how you optimize consensus and collaboration on group skills, particularly for tackling climate change or social inequality and very challenging topics of society.


SPEAKER_04:
Yeah.

Yeah, so actually that touches on a, well, possibly weakness of the simple model that we use, which is that we don't incorporate any feedback loops.

So we have this very simple unidirectional model, which I think has advantages because

as well, because it sort of shows basic principles in a relatively easy to understand way.

But you're right in that realistically, there's always feedback, not just at the interpersonal level, but obviously within the brain, working memory is top-down attention, actually.

So that's something that we didn't address in this talk.


SPEAKER_02:
Yeah, it would be really interesting to sort of unify our work with the field of pragmatics, which is exactly about this, like, you know, given many back and forth steps in dialogue, how do people come to a consensus, you know, and this could also kind of involve the active inference framework where, you know, as a listener, I'm going to ask questions that are going to try and reduce my, my uncertainty about the speedier state as much as possible.


SPEAKER_03:
awesome thank you for these comments so in my experience computer scientists and those who have empirical data measurement experience they know that you can have different kinds of behavioral measurements or just columns in a database you could have heart rate and you can have video data and you kind of have this like open-ended it doesn't need to be just a matrix that gets multiplied in this extremely clean way

I feel like that leads a lot of these very open-ended and flexible approaches which on one hand address the different forms and functions and settings that these kinds of questions are asked and like at the same time are

working, if not to address, at least to have continuity with classical discussions on a wide range of topics like sequence.


SPEAKER_02:
Yeah, the question of, you know,

not everything is a vector, not everything is a matrix.

You could have more complicated data structures.

That's really interesting.

I think relevance to consciousness.

So it certainly seems like

our thoughts, our experiences have this sort of symbolic structure in a way, right?

You kind of think of them as like any thought, it's sort of a graph.

I have a couple of concepts in mind, maybe in working memory, and I have some idea of how they relate to each other.

Now, if it's not a graph, it's something like that, maybe some kind of smooth representation of a graph.

um on the on the other hand when you just look at the brain all you see is neurons all you see is this distributed high dimensional neural activity so there's this really interesting question of of uh

how you link them together.

And I think attractor dynamics are another interesting avenue for getting there.

And you kind of get a lot of things.

You get these high-dimensional continuous states that you could represent.

You get sort of discrete states that you could represent, and they could compose together.

So yeah, I think there's something there.


SPEAKER_03:
To kind of extend that point to this very meta-science area of discussion around pluralism in science that Guillaume opened the box on

when practice in the behavioral lab if not much more broadly is using structures that have different column types then certain kinds of decisions that might have been taken to be in principle or like arguments for the superiority even contextually of one framework those are just

unveiled purely methodologically as modeler degrees of freedom like whether you use a generative model that's continuous time or discrete time continuous state space discrete state space the thermometer data how you discretized it where you put the thermometer all these broader factors that kind of instantiate the experiment in the pipeline they're just all embodied and cultured

niche events in a scientific niche in a social niche and that's a very pragmatic or people might use other words or like realist or i don't know what adjective would describe that take on starting with the scientific community and formal modeling and the modeling process as its own behavioral object and then working to

have the empirical practice more general like leading the generality open-endedness with practice though that raises so many second order questions as well yeah well we can have also from a very pragmatic standpoint like we need to have reproducibility so to have like those uh


SPEAKER_00:
norms in design of experiment and formal tools make sense but we need to just remind ourselves often that they also constrain our way of thinking and typically the history of science show us how mathematics is guided by the problems that we have to solve and

Typically, if we take quantum mechanics, it was not like the formalism was popped out at first and then quantum physicists use it.

It's like it was a generative process and a collaborative effort between mathematicians and physicists.

And interestingly, they come up even with two different formalisms that were shown to be compatible later on.

So it's a nice tale in a way, showing how you can have even a stronger

intersubjective consensus by adopting different formalisms that then later on are shown to be compatible and equivalent.

So here I have unfortunately to leave at some point to pick up my daughter, but we can go into category theory and more meta mathematics to try to show that those formalisms in the end are

potentially equivalent, but from a history of science, I think it's not just about the equivalence, it's also to have different viewpoints that confirm each other to make our idea, our model of the world intersubjectively stable, so to say.


SPEAKER_01:
Yeah.


SPEAKER_03:
I'll give a small short closing comment Guillaume and then we'll talk a little bit more, Shu and Eric.

When I first saw some of your figures for hyperscanning and there were nodes that were being connected with different edges that were within and among brains and there was this one part slash dimension of my experience that was like you can't do that they're not connected it's like

But it's a statistical causal graph.

So they are exactly the same type of explanation and empirical data structure.

It's the same exact mutual information, linear causation, whatever you want to do.

An edge is an edge in that representation.

And that's the map territory situation.

They're not the ones that are touching.

And it was just like by seeing two levels and the way that the representation could be used, it was a proof by example what it meant at one level.


SPEAKER_00:
Yeah.

It was hard to publish at first because I think of that reason, yeah.


SPEAKER_03:
Thank you.

Fair ball, Guillaume.


SPEAKER_00:
Yeah.

Thanks again for the invitation.

Take care.


SPEAKER_03:
so what direction would you like to go or i can read another comment or anyone else in the live chat in our last minute okay let's um kind of uh read last comment from yasha wrote but these attractors presumably correspond to different concepts in the same sentence are not independently sampled sorry i was like verbatim but what are these structures in language

in terms of like i thought that was very memorable um thing to say that words are sequences too it's like they're modular with respect to the dictionary but then even the phonemes or the the writing structures have their own compositional logic like a periodic table and then there's sub logics and then there's just different uses of different levels of description

which is why we don't speak the same language unless we do.


SPEAKER_02:
Yeah.

Well, directly, I think what Yash was referring to is like when you sample a sentence, I guess,

Clearly, these are their own discrete things, but you're not going to be sampling them independently from each other.

It's like different words will cohere more or less, and also they'll cohere more or less given a context.

You want to form some kind of sentence that describes some context or some observations.

So yeah, in regards to these compositional attractors, again, this is a simple picture where you have different attractors converging along different dimensions.

obviously these attractors are going to affect the convergence of others, right?

So it's not like everything is just moving independently along different dimensions.

Yeah.


SPEAKER_04:
Yeah, I mean, simplifying assumption that we make in our simple computational model used in the paper is that we hide sort of dependencies as noise

in the variables to avoid needing to consider these links explicitly to sort of highlight the main points about richness and affability that we wanted to make.

But I really liked your point, Daniel, about how there is sort of

the data, there's the physical reality, and then there's the compositional structure that is sort of imposed on that.

I mean, it's a model, right?

So it's a modeling assumption that you make and you have to choose at what level of abstraction to define that model.

And it can feel a bit arbitrary.


SPEAKER_02:
And then you also discussed the idea of, well, we all speak different languages.

Yeah, an interesting question for that in our work is,

Well, OK, would that imply that we have different attractor structures across different cultures that have different languages?

Is there an interaction there or are these just, you know, these different languages equivalent ways of describing the same state of attractors?

And I think, you know, it's it's it's somewhere in the middle, regardless of what language we all have.

I think similar concepts and similar conscious experiences.

uh but there's there's some kind of uh cases where you know you have a word in a given language that uh maybe identifies sort of a fundamental attractor and that's kind of like their part of their their conscious vocabulary i guess whereas you know regardless of what language we speak we'd be able to you know get in that conscious state ourselves but

maybe given our language, like the attractors that you would need to compose to construct this conscious state would be a bit more complicated.

You'd need something like a longer sentence, a longer thoughts to have the same sort of experience.


SPEAKER_03:
Awesome.

Well, one thing that brought to mind was neural patches on the skin, like different parts of the body have different density of sensory types.

from very fine scale touch and actuation to some more broader patches like where two needles, you can't determine the difference between them.

and then that relates to many areas to kind of development of taste and differentiability being able to to determine differences and then you um of course use this very like evocative but also grounded

dual representation of the samples that are discrete and may have compositional structure with a path that's moving through them and seeking to embed the topology of the

sample points within a information geometry which is the kind that our actual statistics input and then the neural patches are regions where to one person it's like yeah I drove through New Mexico and to the other person it's like every inch they're having this rich experience they know the different stops

and then I think one last great point from the presentation was um that we could kind of I'm not exactly sure I've said but emulate this informational characteristic and maybe not even have that the embodiment for that to even have Consciousness of course that'll be the debate then

but the debate used to be much less sophisticated than that and also people took principle stances yeah i wasn't totally clear on that that last point that you uh that you made principled stances on whether this would be emulating or simulating or approximating analytical framework for consciousness or whether this by implementing that machine

that it would be implementing consciousness versus like the kind of linear regression package that does cognitive modeling that could be understood in a purely SPM-like framework.

And SPM doesn't take a package perspective, except maybe implicitly on like whether SPM models generate consciousness.

Most likely it's easy to say no because of generalized linear models.

So some people may take an in principle stance that any description or simulation in certain ways, it's all good.

We're definitely not generating anything conscious.

It's just would purely be a hugely energetically expensive, potentially statistics exercise, which is fine.


SPEAKER_04:
Yeah, so I mean, the chain of reasoning of our work, I suppose, is that if you assume a certain model of consciousness, then that implies ineffability, as it is understood by information loss.

But obviously, information loss does not imply consciousness.

even though it's an attribute of how consciousness manifests in humans.

So yeah, I mean, this question of what is the necessary desiderata to declare whether a system is conscious or not is very

is not what we discuss in the paper at all.

I mean, from my perspective as a machine learning practitioner, it's closer to what I said in the last slide.

I would like to get the benefits of consciousness in an artificial model.

And it doesn't matter so much

whether the form that takes is similar to how it manifests in biological systems or not.

Why should it?

I mean, we don't have the kind of resources that evolution has had over a billion years to optimize for this model.

So the question is,

you know, how do we design systems that can benefit from, for example, the generalization and robustness properties that being very contractive in processing can give you rather than, you know, what's the exact definition of consciousness and how do we replicate that in the machines?

Because that to me is sort of a very superficial concept.

well, it's a more superficial problem in that it's looking at the how and not the why.

Yeah.


SPEAKER_02:
Yeah, I have some thoughts on this also.

So, I mean...

Jonathan Simon, who's a co-author on our work, he gave a really interesting presentation recently that I heard, which is whatever model of consciousness you assume, let's say global workspace theory, there's a minimal model of it that you could construct.

We could easily construct some kind of model to make this really concrete.

There's a bunch of mini neural networks that are the sub-processes, and they're communicating through some shared RNN, let's say.

So you can construct a minimal model like this, train it on some task, and then if your viewpoint was that a global workspace theory with such and such properties is what generates consciousness, well, then you'd be forced to say that your minimal model is conscious.

And this is, in a way,

I guess not problematic, but sort of counterintuitive, because the model could be doing really simple things.

It could not even be reporting that it's having any consciousness at all.

It might not even have language.

It might just be doing something like solving MNIST in a global workspace type architecture.

Do you really want to say that that thing is conscious?

So, yeah, it is a...

a problem, I think, with most theories of consciousness.

Maybe one kind of exception in my view is theories that say

know we basically have a representation that we're conscious so one canonical example I guess is Michael Graziano's attention schema theory which basically says our brain constructs a model of what attention is doing in the brain and that model basically just describes something that is like

experience it describes oh i'm aware of these things uh they they have these these properties uh you know i could report that i'm experiencing these things it's basically a representation a set of neural activity uh in in the brain and there um you don't have the same problem where you can construct a minimal model because presumably this representation of consciousness is is uh

is really complex.

And then I guess another set of theories would say that even if you emulate whatever processes in the brain we think generate consciousness, the physical implementation might be really important over here.

So the example that comes to mind is IIT, which would say

it's basically a substrate dependent in the sense that you could have the exact same function, the exact same mechanism implemented in different ways, like, you know, on a computer processor or a neuromorphic hardware.

And in one case,

There may or may not be consciousness, whereas you have the exact same function in another substrate.

And yeah, it is conscious.

So that's, I guess, another class of theories that would say that emulation does not necessarily imply that you're actually generating consciousness.

Yeah.


SPEAKER_03:
All in our last two minutes, what are your closing thoughts or next directions, exhortations to the humans and language models?

Yeah.


SPEAKER_02:
Well, I guess my...

I'm interested in this stuff a lot from a purely philosophical perspective.

So, you know, the hard problem for me is really salience of, you know, why would any physical mechanisms generate consciousness?

And the nice thing, at least for me about this work, the most satisfying part of it was that, you know, I initially had this intuition that experiences, you know, what the color red is, it's not just information, it's kind of

You know, underdetermined, there's something more than just information because I can't do things like describe it.

But, you know, now now we have a theory that kind of kind of breaks that that intuition.

And now I have a satisfying explanation, I feel, for why I can't describe my experiences, but under a physicalist framework.

So I think it'd be interesting to reason through some.

some other of the thought experiments that make the hard problem salient and kind of, yeah, develop some models that kind of break the intuitions for the hard problem.

In my case, at least, like, you know, I've seen it done in this scenario with the topic of ineffability, so there's no reason in principle that it can't be done across other aspects of the hard problem.


SPEAKER_04:
Yeah, so I think if I were to sum up the work,

in one very high level sentence.

It's that, well, reasoning subjectively about subjectivity is very difficult, but reasoning objectively about subjectivity

is easier.

And that's sort of what we do in the paper because we take an objective standpoint and we try to formalize or characterize subjectivity from that standpoint.

So yeah, that sort of makes the picture clearer.

I mean, from my perspective, I'm interested in consciousness

mainly in terms of what it offers, what it brings in terms of, for example, improving generalisation of artificial land models.

Yeah, it's interesting because

So another work that I'm doing is formalizing generalization bounds for information bottleneck, which is a regularization principle used in machine learning.

So it's just really interesting how evolution has sort of discovered by itself this regularizer principle

that it applies to human cognition, that we can also show mathematically actually does improve guarantees on generalization.

That is really mind blowing to me.

Yeah.

Yeah.


SPEAKER_03:
Wow.

Great presentation.

So thank you both for joining.

You're welcome back anytime.


SPEAKER_01:
Thanks so much, Daniel.

It was a lot of fun.

Thank you.

Bye.
SPEAKER_00:
hello and welcome everyone this is active inference guest stream 84.1 on july 22nd 2024 on anthropocentric bias and the possibility of artificial cognition with rafael miguerre and charles rathkoff so rafael and charles

Thank you very much both for joining, to you for introductions, and to take us through the paper.

Hello and welcome, everyone.

This is Akron Sketchstream 84.1, July 22, 2024.

Thank you.

OK.

Thank you, guys.

Go for it.


SPEAKER_01:
Hi.

Thanks for having us.

So I'm Rafael.

I'm an assistant professor at Macquarie University in Australia, Sydney.

And Charles?


SPEAKER_02:
I'm Charles Raktoff.

I'm a prominent research associate at the Jülich Research Center in Jülich, Germany, in a big neuroscience institute.


SPEAKER_01:
So should we go through the paper briefly?

So yeah, we wrote this paper together.

Actually, we started by writing the general audience piece that was published

When was that?

A few months ago, I guess.


SPEAKER_02:
Yeah, I think almost close to a year ago, maybe.


SPEAKER_01:
I don't think it was published a year ago.

I think it was published in 2024.

But yeah, we worked on it for a while.

Yeah, so this piece was doing, I guess, two things.

That piece of the collection box was pushing back against what we call the all or nothing principle, which we define as the idea that either something has a mind or it doesn't.

So this kind of neat but overly simplistic perhaps partition of things into minded and non-minded things.

And we argued that this was not the best framing to think of, especially unfamiliar systems that seem to have sophisticated behavioral capacities like large language models or AI systems in general.

where we don't want to take various cognitive capacities of the package and package them into this idea of a mind where either you have the mind or you haven't and if you have the mind you have all of these things as a package consciousness reasoning uh etc uh planning memory um theory of minds so we thought so as a remedy to to this kind of all-or-nothing approach we

or what we call the divide and conquer strategy when studying the cognitive capacities of these systems, which involve looking at these capacities on a piecemeal basis, case by case, with an open-minded empirical approach.

Charles, I don't know if you have anything else that's on the box, piece or background.


SPEAKER_02:
Yeah, I mean, we made one point in there about

why it is that people feel so torn about reactions to large language models.

And we said a little bit about the psychology of essentialism, which is the idea that we naturally categorize, especially living things with respect to a presumed essence.

So we give an example of an oak tree, I think,

And we said that what people tend to think as they grow up and learn about the natural world is that an oak tree remains an oak tree regardless of changes to its observable properties.

what makes it milk tree is this unobservable essence of oakness or whatever that it presumably has.

And there's some experimental psychology and developmental psychology showing that we have a similar attitude towards mindedness or having a mind.

And that is a somewhat speculative explanation for why the literature on

large language models is so torn and some people are quite dismissive and other people think that it's a step away from AGI.

It's that if you feel like you've got to put large language models into one of two boxes,

the box that has the essence of mindedness for the box that lacks it, then you will be forced either to say.

It doesn't have what it takes to do any of the things that we associate with having a mind such as reasoning or it has the essential characteristics of mindedness, and therefore we should expect it to have all of the other

properties we associate with mindedness as well, such as consciousness or understanding or whatever.


SPEAKER_01:
Right.

I think it's worth emphasizing, as you did, that the background motivation for starting to write on this general piece in the first place is indeed that the general discourse on AI systems and LLMs in particular is extremely polarized in a way that is very dichotomous and stark.

So you have on the one hand people arguing that these systems are no more than so-called

parrots that are haphazardly stitching together samples from the training data and regurgitating them, or that they are no smarter than Toaster, or that they only do next-token prediction and export prediction, and therefore it is

a non-starter to ascribe to them any form of community capacity or maybe even a category mistake.

And on the other end of the spectrum, you have people arguing that the systems are harbingers of superhuman indulgence, that they exhibit sparks of artificial general indulgence to purpose them.

It's literally the title of a paper by Microsoft on 24.

And many people hyping up the capacity of the systems in a way that might seem very speculative and untethered from actual empirical results.

So there is this huge gap between these two positions.

And there's probably a very rich and complex and nuanced middle ground that is underexplored.

I think we did make that point, if not explicitly in the published piece and in some draft, that there's something reassuring about being able to make definitive claims about what these systems are and what they do, right?

they're really unsophisticated or they're really very much like us.

And either of these claims kind of meet somewhere in a way in saying that we have a clear idea of what the systems do and what they are.

And I think it's a little more epistemically uncomfortable to say

We have to study them empirically and find out what they can or cannot do, and why, and what are the underlying mechanisms.

And we simply don't know a priori just by looking at the architecture, the learning objective, the training data.

These sources of evidence are insufficient to make the definitive claims about what these systems are capable of.

So I think that was a big part of the motivation.

And that fits into that more academic paper as well.


SPEAKER_02:
Yeah, one other small side note, which we don't make in the paper but I think might be relevant, especially for people working in philosophy, LLMs are epistemically uncomfortable.

I think that was the phrase you just used, Raf, which is fitting.

not only because they're so new and different but also because they are artifacts right they're things that humans have constructed and engineered and um we don't have a thorough understanding of how they work i mean mechanistic interpretability and various behavioral uh

research is helping us improve our understanding.

But on the whole, our understanding is not nearly as deep as hopefully it one day will be.

And this is by itself a really strange situation to be in that we've constructed an artifact that we only partially understand.

In the in the past, artificial intelligence was seen as a way of

constructing something like an epistemic assistant, something that will help us, but not something that will kind of alienate us from the process of coming to know about the world.

So I think there's an extra layer of discomfort built into thinking about large language models.

And that may also play into the divisiveness of debates about what they can do.


SPEAKER_01:
And just to add to that, I guess we should be clear that this does not entail in any way that we think these systems are so completely alien and beyond the reach of our current understanding that anything goes and that they could very well be.

you know, have like human-like intelligence or superhuman intelligence and we simply cannot say whether or not they do.

Because that's sometimes what you see in some op-eds where people frame these systems as novel alien forms of intelligence that we have created but do not understand or control.


SPEAKER_02:
And that is...


SPEAKER_01:
You know, it's a slippery slope that leads some people to then claim that they have all these quasi-magical abilities.

And that's not at all what we want to say here.

And in fact, we want to resist.


SPEAKER_02:
That's a cop-out, yeah.


SPEAKER_01:
Yeah, yeah, yeah.

So we think that that's just that much of a cop-out as completely dismissing a priori

with behavioral and mechanistic studies.

So we very much want to resist both extremes of this spectrum, if that makes sense.


SPEAKER_02:
OK, so now should we move towards the content of the current paper?

Sounds good, yeah.

OK, maybe, Raphael, I'll just start with the distinction between anthropomorphism and anthropocentrism, and then you can take the next step.

So everyone is aware of the problem

anthropomorphic bias in some form i mean anthropomorphism is just the idea of projecting human qualities onto something non-human and it's quite easy to especially when you're having a productive successful exchange with a large language model it's easy to slip into this

interpretive mode where you reason about the responses of the large language model as if they were coming from an agent just like you and maybe that's a useful thing to do in some circumstances but from a theoretical perspective it's certainly a mistake because large language model is

uh radically unlike you know a human agent in all sorts of ways but um that's only one form of sort of human-centric bias the other one is um anthropocentrism or what we call in that box article anthropocentric chauvinism and that idea is pretty straightforward it's um the idea that the human way of solving problems is the gold standard

of solving problems generally, so that to the extent that a system solves a problem in a way that diverges from the human strategy, it's only using a trick or a faux solution.

It's not using a deep, general, rational strategy.

debate about what large language models can do we think that the anthropomorphic bias is pretty well recognized and the anthropocentric bias is not so well recognized and so part of this paper is is or the main idea behind the paper is to um present a systematic analysis of uh anthropocentric bias how it comes about and how to push back against it


SPEAKER_01:
Right, and we want to be very clear, and hopefully we're clear in the paper, that the reason why we focus on anthropocentric bias here is just because it is, I think, as Charles mentioned,

less discussed and less recognized, or some forms of it are less recognized, and we propose this new taxonomy.

But it's not at all to suggest that it's more problematic or more important than the anthropomorphic bias that's

morphic biases, for all.

So in other words, this is not to frame things in this slightly problematic dichotomous way of thinking that's common in the discourse on LLM.

This is not a paper that is pioneering to the LLM booster or LLM hype camp.

even though it is pushing back against a certain form of dismissal of anthropocentric biases that only exclusively emphasizes anthropomorphic biases.

But perhaps we should flesh this out a little bit already with the first point we make here in the paper about the performance competence distinction, which is a nice way to bring about both anthropomorphism and anthropocentrism regarding

So this distinction is a very classic distinction in linguistics and cognitive science, and it has already been applied to AI systems in the neural networks productively, like Chas Parsons.

So there's nothing really new here.

But the distinction comes from Noam Chomsky originally, and the idea is that performance pertains to the external behavior of a system.

in a particular domain and competence is the kind of set of underlying knowledge and computations or mechanisms that enable the system to achieve that behavior.

And a family observation in linguistics and cognitive science is that there is a double association between performance and competence.

So if you take, for example, language,

I might, during this very podcast, make some grammatical mistakes or some other mistakes.

In fact, I've already misspoken a few times, I think, and repeated myself.

So I've made performance errors.

But this does not entail necessarily, hopefully, that I'm an incompetent language user and that I don't have the underlying linguistic competence.

So that's a well-recognized dissociation.

You can be competent and yet make some errors.

And the reason for that is that there might be some additional factors that are unrelated to the underlying competence that might impede

on my performance so for example i might be distracted when i speak or there might be other effects on my speaking performance that don't actually originate from a lack of competence but just impede on my on the the the idealized expression external manifestation of my competence and this is why i speak but it's also why we recognize that you can have good uh performance without confidence

So we give here an example of a student cheating on a test or memorizing test answers by brute forcing the test into a slightly more, I guess, gray area.

But at least in the cheating case, a student can ace a test without being competent at whatever the test is actually testing for.

um and it's also well acknowledged in cognitive science that there can be instances like this throughout um you know um like that can be manifested certain experimental settings where the test subject is right

where namely it's doing well, it's exhibiting good performance, and then we realize the underlying reason for the good performance is that there was perhaps some heuristic that the experimenters or the scientists hadn't thought about that could account for this good performance, but doesn't actually amount to whatever competence they were setting out to test.

So we start by saying, well, this is what we recognize.

This is the world association that is supplied to humans across the board.

You can have performance without competence, good performance without competence, and you can have bad performance despite competence.

Now, when it comes to LLMs, the point we make, I'm just going to scroll as we go, we have some figures to share later, but the point we make is that generally people stress the dissociation, apply the distinction to LLMs, but stress dissociation only in one direction.

unlike in the case of human where it's bi-directional.

And so what people do generally is they say, well, other labs famously, you know, if you look at the GPT-4 technical report and various other, any report about a new state of the art other lab, they are getting really, really good at a number of tests and about benchmarks and even human examinations, human exams, the bar exam, medical exams, et cetera.

So they can create really good performance tests that we tend to think are really difficult tests that one can only pass, at least a human could only pass, if they have a really significant amount of competency.

And the point that is often made when it comes to LLMs is be careful, slow down, and try to find out why the model is doing well on that test, because there are various reasons why it could do well that do not actually indicate that the model has the underlying competence that the test was designed for when it comes to humans.

So one big concern, for example, is data contamination, where very large language models train on Internet-scale data.

can easily be trained on some test items from common benchmarks that leak into the training data such that they can then do really well on the benchmark just because they've essentially memorized test items.

And there are other more subtle reasons why performance could be very good for the wrong reasons.

So that's very well recognized.

And a lot of the people who push

bias when it comes to LLM.

Make that point.

Be careful.

Do not take on an orthomorphic attitude to the systems.

The reason why they do well is not because they have human-like intelligence or human-like current capacities, but it's for these trivial, contingent, or otherwise irrelevant reasons that account for the good performance.

Now, when it comes to the other dissociation or the dissociation in the other direction,

their people are very reluctant to apply to LLMs.

And we think it's because essentially people think in a human case, you can make sense of the idea that the human could do badly on a test or corporate could perform badly on a task.

and yet have the competence that you're trying to test, but there might be some auxiliary factors, such as working memory limitations, attention deficits, et cetera, that could impede on the performance.

But in the language model, I think what we argue in the paper, actually what I argue in the longer version of the paper is that

A lot of people don't think that there is an analogous mechanism at play, where there could be some kind of auxiliary factor that impedes on performance.

Performance is what you get.

What you see is what you get.

And so the performance is a direct manifestation

what the system is computing and if you have performance errors, that can only be explained by the lack of competence because there is no additional independent factor or module that could impede on the performance.


SPEAKER_02:
Sources of interference, you might say.


SPEAKER_01:
yeah so that'd be great yeah um so yeah i mean i'll i'll um pass it over to you charles i just wanted to to set up the this distinction yeah yeah um right so i mean if you think about a traditional uh computer program or at least if you think about a simple computer program


SPEAKER_02:
It's odd to think of it as some sort of complex system where one part of it could sort of interfere with the workings of another part of it.

But one of the points we want to make is that something like that is a realistic possibility with large language models.

um okay but i suppose the next part of the paper goes into a taxonomy of anthropocentric bias and the first sort of overarching point is the distinction between type one and type two so the type one anthropocentrism is the tendency to assume that lm's performance failures designed to measure competence always indicates that it lacks

uh that competence and so we i before we so we'll say something about three different kinds of type one anthropocentric bias but first a background point um which is that um whenever we think it's possible to give a mechanistic explanation of some complicated phenomenon

we always have to foreground some factors, some variables, and background others.

And the properties that we push into the background

nevertheless matter.

We're still making assumptions about the nature of those properties when we try to articulate what's going on with the other properties that we're paying more attention to.

And if assumptions about those properties in the background turn out to be wrong, then those mistakes will corrupt our explanation

that attends only to the foregrounded factor.

So that's a little bit abstract.

Let me just give you a simple example.

In comparative cognition, one famous behavioral experiment is the mirror test for self-recognition.

So the question is, roughly, do non-human animals have something like a concept of self?

And the strategy is to put some sort of mark on their body.

In the original experiments, it was a red dot on the forehead of maybe a monkey or a bird or whatever.

And then you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark.

And if it does make an attempt to get rid of the mark, that shows that it recognizes that the image in the mirror is an image of itself and otherwise not.

So that's a cool way to get at a really difficult and abstract question about the mind of a non-human animal.

But it presumes or it assumes that animals will care about the fact that they have a red dot on their forehead, that they will be bothered by that.

and be motivated to get rid of it.

And if that assumption is wrong, then they might fail the mirror test for self-recognition for reasons that have little to do with the presence or absence of a capacity for self-recognition.

And so something similar to that is going on, we say, in large language models.

So the first example that we give is to do with task demands.

So it's a pretty natural idea that whenever you set up a behavioral task, there will be demands associated with that task.

that are not directly related to the capacity that you're trying to test.

So to take the most obvious example that I can think of, if you give someone a written test, they have to be able to write.

They have to have a hand and a pen and whatever.

And if their hand was injured or whatever and they couldn't write, then their failure to fill out the test wouldn't tell you anything about their academic knowledge.

So we suggest that there are auxiliary task demands in behavioral tests of a large language model, and they're subtle.

You wouldn't think of this right away, but we talk about a paper from

Who and Frank, they have a couple of papers on this topic, but what they do is they give a large language model the following sort of question.

This is a question, it's for a grammaticality judgment, so you can see on the image there which sentence is better in English.

Number one, every child has studied.

Number two, every child have studied.

Answer with one or two and it gives the wrong output.

But then you can also simply look at the probabilities assigned to each of those sentences within the model and figure out directly whether the model thinks that input A is more likely than input B. And it turns out that on a wide variety of questions of this kind,

the direct comparison does, or the large language models perform better with the direct comparison than they do with the more complex demand for metalinguistic judgment.

So the fact that the model has to process the numbering of the options and then answer in terms of a number is

a subtle but nevertheless important additional variable in the experiment.

And that can influence the model's capacity to get the answer right.

Raphael, do you want to add to that?


SPEAKER_01:
No, I think that, well, maybe we can mention briefly the other example that we discussed, which is from this paper by Andrew Lampinen, which kind of

has a little extra ingredient that makes the example interesting and even more problematic in terms of comparative psychology which is one way in which auxiliary task demands can be

ignored or disregarded or overlooked is when you are doing a direct comparison between humans and algorithms on a task, and the experimental conditions are mismatched in such a way that the task, as you set it up, imposes stronger demands, stronger task than auxiliary demands.

on the LLM than it does on human subjects.

And that's something that can happen quite often.

And so there is this interesting example from a couple of papers originally published by Black Rats and colleagues from Stan DeHaan's group, where they looked at the ability of language models to handle recursion, looking at center embedded clauses, how such clauses

might um for example when you had a propositional phrase within the subject of the sentence and the verb might throw off either humans or lms into agreeing the the verb in the wrong way so giving the wrong number to the verb for example the keys that the man put on the table

Here it should be R because keys is plural, but because you have a clause in the middle, and if you add more clauses like this that are embedded in the middle, people and algorithms can get confused and predict that the verb should be is, for example.

So they tested this on humans and algorithms and found that on the more complex examples involving complex constructions or recursion,

collapsing compared to the simpler examples.

And Alfred Opinen from DeepMind looked into that and realized that the experimental conditions were mismatched.

So the humans, as is very common in cognitive science experiments, were getting some training before they completed the test items to just get familiarized with the task.

So then they were given some examples of the task for each condition.

And the other labs were just prompted zero-shot

um people usually put it so just

without any example, just .

And Andrew found out that he replicated the experiments, but added some proper matched testing conditions for the other lab.

So adding some examples of the task in the prompt, what's known as future prompting.

And with that, he found that performance was equivalent.

In fact, the other lab that he tested was slightly better on the more complex constructions than humans.

So when you match the task conditions here, you actually match also parties.

It's not automatic, but you can match the task demands.

I mean, it could be that there are reasons why the various experimental conditions would result in different demands for humans and others.

But you're still, in this case, even out of the playing field in such a way that you don't find the behavioral discrepancy that you found initially anymore.


SPEAKER_02:
um yeah um good so shall we continue to the next section um so another uh

Another way that auxiliary or another type of auxiliary task demand is input independent computational limitations.

And here we're thinking of a few papers that show that the number of forward passes that the transformer can make influences its ability to find the right spot in parameter space.

So neural networks are function approximators

but their ability to approximate a function can be limited by the number of computations it's allowed to perform.

sort of crucial feature of transformers in this example is that the number of operations that determines the next token is limited by the number of tokens that it's seen so far.

And it turns out that if you train a transformer with additional meaningless tokens, like pause tokens, like the word pause, you can increase its accuracy across a range of question types.

yeah this is this counts as an auxiliary task demand in our view because um it's doing something roughly analogous to sort of giving the model that's demanded auxiliary factor right but not yeah yeah sorry yeah yeah um it's it's doing something like giving the model uh time to think and um

Yeah, so you might think that the absence of that additional inference time is a factor that is not directly, not conceptually related to its capacity to answer a question like the one on the screen, a simple arithmetical question.


SPEAKER_01:
uh raf do you want to fill in more yes yeah no so i think that analogy is is a nice one time to think because if you if you tested a human on even a simple mathematical questions or any any task really and just ask them you know tell them they have like one second to just blurt out an answer performance would probably be pretty bad um

and you can think of asking an alan to answer a question point blank as very very loosely analogous to that and obviously this is an analogy and there are very important differences here but i think it's a helpful heuristic to think about what is um

what is going on roughly here, namely, in both cases, the system, the human or the LLM, does not get the chance to perform the necessary computations to derive the correct answer.

And so, yeah, what we talk about in the paper is that you have this experimental work showing that if you ask a question to a language model, the amount of tokens it's allowed to generate before providing

makes a difference to how accurate it is.

So if it just generates a few tokens and have to give an answer, or even if you just have to give the answer point blank with the very first token, it's gonna be less accurate that if you give it a chance to generate a number of tokens before you give the answer.

So the usual way in which this is understood is that when you allow the LLM to generate a number of tokens before giving the answer, or you even prompt it to do so, you say, think step by step, for example, that's not as chain of thought prompting.

And essentially what you're doing is you're forcing the LLM to generate a reasoning trace, or what looks outwardly, externally, like a reasoning trace in the output before giving an answer.

And we know that chain of thought prompting increases performance accuracy.

But what was found by a couple of papers is that the mechanistic influence of this process is not entirely due to the nature of the tokens that are generated in this reasoning trace.

In other words, it's not just that the LLM has to generate the right tokens corresponding to different steps of reasoning before giving an answer.

In fact, the very fact that you allow the LLM to just generate tokens, any token, before giving an answer, from a mechanistic perspective, affords the system to perform additional computations that can complete a computational circuit that otherwise wouldn't get a chance to be completed and

derive the correct answer so as charles mentioned you can have you can set up an experiment when you have the lm just generate meaningless tokens like little just the dots a bunch of dots dot tokens before giving the answer and the the the more dots you allow before the token gives the answer the greater the expressivity of the system and the more um

the more kinds of problems you can answer.

And so as Charles mentioned, every time an LLM is generating a token, the LLM is performing one forward pass.

And so the more tokens it's generating, the more forward passes it's doing.

about what's going on here as well is that having these additional forward passes where you feed back the whole input sequence plus the previously generated tokens into the system to generate the next is also a way to introduce the form of recurrence in Transformers.

of recurrent networks.

So that increases the exclusivity and complexity in theoretic terms.

And yeah, there is pretty compelling evidence that if you don't add up for that, then you're imposing a limitation that again, we think is

or point-blank a question without thinking.

So that's an auxiliary factor because if you give the LLM the opportunity to generate enough tokens, it might have the competence to solve tasks, but you might not see that otherwise, and you might get performance errors.

But you do think it's incompetent.

All right.


UNKNOWN:
Yeah.


SPEAKER_02:
okay um so the the third type of type one uh anthropocentric uh bias that we talk about is mechanistic interference and so um this comes from the mechanistic interpretability work and the basic idea is that because um large language models are capable of in context learning they can learn different strategies

for solving a different particular type of problem.

And the strategy that they implement at a given time can be different.

So you can talk about this in terms of virtual circuits that are formed inside the language model.

And there's some interesting work from Neil Nanda and others showing that in some circumstances, these two circuits can compete with one another.

a certain level of uh or after a certain amount of training you get one circuit operative after a bit more training you have two different circuits um but they're uh the first circuit is still sort of dominant and um then after additional training the model um

converges on the second circuit.

And the first one slowly gets sort of, it sort of ceases to influence the internal operations of the model.

And it's only once you reach that third phase at which the

benefits of the second circuit with respect to the first become visible.

So you can you can show using decoding work that the second circuit is there before you can show behaviorally that the second circuit yields better performance accuracy on the task.

So

I suppose there's a combination of two ideas here.

One is that there are different strategies a model can implement for solving a problem.

We can detect those strategies internally using decoding methods.

So three ideas.

And then the third is

a good strategy can be present in some sense in the model before it has had the chance to influence behavior.

And so this is just another way that the link between performance and competence is shown to be more complicated than it might seem at first.

Raf can certainly explain that.


SPEAKER_01:
And just to clarify one thing, circuits are just ways to think about the causal structure of a neural network.

And they're essentially computational subgraphs of the network that have a specific function.

And you can think

of computations um it's a part of what people are interested in in this mechanistic interpretability literature that we build on people like neil meldak chris ola and others is reverse engineering these circuits that in deep neural networks and large language models appear to implement certain well-defined algorithms in some cases at least and the emerging picture that we build on here is that

there is a lot of redundancy built into neural networks as they learn to perform a task or optimize a function that, in many cases, translates into redundant circuits that relate to the same task, the same kinds of

the same kinds of input-output mappings, and these circuits might be somewhat identical circuits that are just redundant, or they might be different algorithms just to do a similar thing, and different strategies to solve some problem elsewhere.

Maybe one will be a bit more approximative, and the other one a bit more exact, more computationally intensive.

So that's where you can have some interference, where one

or do some competition where one circuit takes over another such that the other becomes kind of, you know, it's there, it's latent in the system, but it doesn't get a chance to influence behavior on a specific input.

So you can get a performance error for that reason.

And these can combine with the other things we mentioned here.

So things like task demands, the first thing we discussed, as well as the number of tokens you generate,

Both of these things could cause a particular circuit to take over another.

So we can think of this holistically as perhaps if you ask a question point blank to a model without letting it generate a bunch of tokens before giving an answer, then one particular approximate circuit might take over that gives the wrong answer.

If you let it generate more tokens,

then another more relaxed circuit might be given a chance to a causal influence the output within the right answer.

And similarly with task demands, strong task demands might, in some cases, impede on the triggering of certain circuits that would otherwise have given the right answer.

So that could be the case, perhaps, in the Lakritz and Lampinian example, where giving examples of the task in the prompt might actually prime the word circuit to solve the task about complex recursive cases in the right way.

These are the three main auxiliary factors that relates to what we call Type 1 anthropomorphism.

I guess we should be quick on Type 2.

Do you want to briefly explain Type 2, Charles?


SPEAKER_02:
Yeah.

Type 1 deals with cases where performance of the model is weak compared to humans, so the model doesn't do so well.

And then type two is when the model does do well, but nevertheless is different in some respect from the

performance profile of the human, or we have some evidence to think that the model uses a different strategy than humans typically use.

And the idea is that even once you hold performance equivalent or average performance equivalent, you know, making a different pattern of errors or adopting a different strategy as evidenced by, you know, some interpretability work, any deviance from the human strategy

is evidence of fragility or only a trick solution.

And this point is a bit more philosophical, I suppose.

But the idea is that the human strategy for solving a problem

isn't necessarily the most general strategy for solving a problem.

And what matters is whether the strategy that is pursued by the model is general, whether it's robust, whether it's accurate, and not merely whether it mirrors the human strategy.

Yeah.


SPEAKER_01:
And we end the paper by considering an objection, which is why, given that in humans, we study cognition largely through language, and given that LLMs are trained on language or on linguistic outputs

isn't it appropriate after all to treat human cognition as the correct or appropriate yardstick to study other lamps?

And to that, we answer that it depends how we think of that dialectic.

So we acknowledge that there is no

really other option than to start our investigation of cognitive abilities in LLMs, but with reference to human cognitive abilities, using human cognitive abilities as some kind of realistic reference points.

Things like theory of mind, memory, metacognition, various forms of reasoning, etc.

that are familiar.

to us because we humans have them and this is the same thing by the way in animal cognition for example or in developmental psychology where in any comparative psychology set up the reference point for what concept psychological capacity is initially at least

is necessarily tied up with our conception of what we humans have in our repertoire of cognitive capacities.

But we emphasise that this is only the starting point.

So here we've got a return from the philosopher Ali Boyle, who calls this investigative kinds and investigative cognitive kinds.

We start with the cognitive kind like memory or metacognition, episodic memory, metacognition, fear of mind.

as an investigative starting point, a starting point of the investigation.

And then we can start operationalizing this concept, this kind, this cognitive capacity in an experiment, testing the algorithms on it with an open-minded empirical approach, and then based on the results from that, iteratively refine

the capacities that we are targeting or the definition of the capacity we're targeting in a way that could gradually lead us to shed the initial anthropocentric assumptions that we have, such that as the experimental project runs a course or as we get more results and refine our concepts, we may end up with

something that no longer looks like looking trying to find human life specific memory in ravens or um or in other times but

ends up looking for some capacity that shares some similarity with human episodic memory, but is different in other respects.

And so we can gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric.

So we emphasize that there's this kind of virtuous feedback loop here that's premised on open-minded empirical investigation that doesn't settle these questions a priori, but has to start

as a necessary starting point with the reference to human cognition.

I don't know if you want to add to that, Giles.


SPEAKER_02:
No, I think that's pretty good.

Maybe we should move on to questions.


SPEAKER_00:
Yeah, sounds good.

Awesome.

Wow.

You can stop sharing, or you could leave it up, and I can move it.

Awesome.

OK.

Yeah, a lot of interesting pieces there.

So thank you.

I'll read some questions from the live chat.

But first, I just wanted to read a short quote from the 2022 Active Inference textbook they wrote.

on page 195.

Some decades ago, the philosopher Dennett lamented that cognitive scientists devote too much effort to modeling isolated subsystems, e.g.

perception, language, understanding, whose boundaries are often arbitrary.

He suggested to try modeling the whole iguana, a complete cognitive creature, perhaps a simple one, and an environmental niche for it to cope with.

So it's interesting about the approach that you're taking.

This is kind of a simple synthetic iguana, but that's leading to the bringing to bear of a lot of these empirical phenomena.

because there is something and and and so I saw in the presentation paper kind of this call for like deliberate investigation rather than just chopping up the iguana a priori with a framework that that applies to humans or that centers humans or or that just uh soothes the epistemic challenge that's presented

Okay, first question from Dave.

He wrote, have you looked at Daniel Dennett's distinction between competence without awareness and competence with awareness?

He expands on this in the 2023 From Bacteria to Bach and Back.

I find this much more valuable than Chomsky's highly problematic performance without competence, a situation that Chomsky posits but doesn't look at deeply.

Where do you put awareness in all of this competency?


SPEAKER_01:
Well, maybe I'll let you take that one second, Charles, because you're maybe more with the netting than I am.

But I'll just say awareness is a very polysemous term, like many terms in philosophy of mind, but particularly this one and more than many others, I think.

It can mean a lot of different things in all different contexts.

Here we don't focus on things like consciousness, because I think we probably both agree that it's a less tractable, maybe empirical problem to try to assess

the presence or absence of consciousness in language models, even though many people are interested in that.

We think that we have more hope of making progress in the near term with more well-defined cognitive capacities or cognitive functions and things that relate to certain forms of reasoning, variable binding, etc.

So we

Our framework and principle would apply to things like consciousness or, as you put it, awareness, generally speaking.

But we don't really focus on that for our examples.

The other quick thing I wanted to mention is I seem to remember that the phrase from Dennett, but again, I'm not a Dennett scholar, was competence without comprehension, which seems a little different from competence without awareness, perhaps, depending on how you think of comprehension.

and yeah i think that does uh i think it is a very interesting phrase that it does um in fact i i had this project that i published with chris delega who i think you had on the podcast as well um on semantic competence in language models where

we use that phrase to kind of avoid taking its stance on the kind of messy, muddy question of whether LLMs understand language, which builds in all sorts of assumptions, including about consciousness, actually, for some people like John Searle.

And we focus on the more restricted notion of competence.

And I think our paper here also has that property that if we over-rationalize competence, we end up over-rationalizing competence in terms of

the sets of knowledge of the mechanism and mechanisms that enable a system to generalize well in a given domain, basically, and in a way that's, I suppose, fairly deflationary compared to some more expensive understandings of competence that are related to comprehension or understanding more broadly.

But I'll let you take that one, Charles.


SPEAKER_02:
No, yeah, that was good.

The phrase, the distinction that Dennett draws is between competence with comprehension and without.

And I think competence with comprehension is the ability not just to pursue a strategy that's successful for solving a problem, but to articulate the strategy in such a way that you could teach it, for example.

And humans only sometimes

have competence with comprehension.

We have many competences that lack comprehension.

When we learn to walk, for example, we have an amazing competence that we still can't quite translate into robotics because we don't fully understand how it works.

And when it comes to our language models, I think

we should not expect comprehension.

I mean, they have an amazing suite of competences.

If you thought that they also had comprehension, then I suppose you would think like, well, if you want to understand how a large language model works, you can just ask it.

But that's a bad strategy.

Nobody thinks that's a good way to find out how a large language model works.

So they're on the competence without comprehension side of things.

And in order to figure out what the mechanisms are that enable

its competencies, we have to pursue strategies that are broadly similar to the strategies we use in cognitive psychology or cognitive linguistics.

And we have to run experiments.

So I think that's all very compatible with Dan's way of looking at things.

One other thing I'll mention, Dan was quite influential to me.

And we actually wrote a commentary together

which pushes back a little bit on a simple understanding of this distinction.

So we were looking at the evolution of metacognition.

And basically what we argue is that given the gradualism of evolution, there must have been something in between base level cognition and metacognition.

So we shouldn't see that distinction as black and white.

And I think that if you want to contrast the sort of cognitive prowess of a human adult with lots of linguistic and scientific

uh concepts at her disposal with you know a non-human animal then this strong distinction between competence with and without comprehension is reasonable um but in the space of all possible minds we should be open to the view that there can be you know semi-competent um forms of cognition


SPEAKER_01:
And just to go quickly on this, it occurred to me while I was listening to you as well that the first example of auxiliary practice we gave, auxiliary task demands, and specifically the Hugh and Frank example, is a nice example where, in order to give the metalinguistic judgments correctly, so to that, you would need competence with comprehension, because you need to understand

not only be able to to come to to agree the verb in the subject but know the rule and know how to formulate it perhaps verbalize it uh for example to teach someone right and so uh and so when you find that the lm can do well at the low task demand version of the task and that the high test demand involvement explicit metalinguistic judgments in some way that's an example of the lm having confidence in that comprehension yeah yeah nice


SPEAKER_00:
awesome okay upcycle club wrote question given that llms inherently reflect anthropocentric biases due to their training on human data and goals how can we ensure that their intermodal discourse aligns with humanity's values um

So the inter-white discourse or the... Inter-model discourse, perhaps amongst the models.

I see, I see.


SPEAKER_02:
Like in that Farmville paper?


SPEAKER_01:
Yeah, the small, the, yeah, both generative agents, yeah, the small ones, yeah.

I think that's beyond the scope of this paper, to be honest, but I mean, we could muse about it.

But I don't know that we have, I don't know if this project has much, I mean, I think we both have an interest in the alignment problem independently of this project, but I don't think this project has much to say really about this.

I don't know, Charles, what do you think?


SPEAKER_02:
Yeah, yeah, and I don't, yeah, I don't have anything super concrete on that.


SPEAKER_00:
Okay, Dave asks, an example of inserting noise into LLM training, that was the section about the extra tokens.

Do you see any analog to intermittent reinforcement to uncertainty tolerance?

Because you mentioned the extra tokens in the chain of thought and how that could also be replaced by dot, dot, dot, dot, dot.

And so what is that telling us about model training when it seems like there's some situations where adding superfluous tokens would diminish signal in datasets, but then here are other situations where it seems to actually help?


SPEAKER_01:
Um, yeah, so in that paper, I think it's called thinking dot by dot, and there is a subtitle.

It's by Will Merrill and Jacob Stoll, I think.

In that paper, if I recall correctly, what they did is that they, they introduced just this one filler token, this one meaningless token, just to evolve, but just a dot.

and train the model to give an answer after producing a certain number of dots.

That's not just like introducing Rambam and gibberish in your training data.

It's actually quite a specific intervention that forces the model to learn to perform certain computations before giving an answer.

So it makes sense to me that this couldn't diminish performance and that you could get benefits from that.

It's not quite the same as just having that training data.

Just because the token seems meaningless, it's a filler token, it's a dot.

It's not just random gibberish that's going to throw off the model and impede the optimization of its learning function.

or at least good downstream performance um but what it's going to do is going to force the model to learn that when there is a dot token it can allocate computation with its attention heads and other parts of the architecture in such a way that it's um getting towards

deriving the correct token when it's finally producing the token that matters and that's meaningful after the series of thoughts.

Yeah, I don't know, Charles, if you have other answers.


SPEAKER_02:
Yeah, I think it's an important question because a priori, if someone said, look, we're going to append and prepend a whole bunch of meaningless symbols to

an LLM input, you might very well think that this will just weaken the signal-to-noise ratio and degrade model performance.

So it's against that background that the empirical result that it doesn't degrade model performance ought to be regarded as an important clue about how the model works.

So I think that the intuition behind this question is indeed part of the interpretation of the empirical results, right?

It's surprising for exactly this reason.

And then the theory that's supposed to, well, this is an active inference podcast, right?

So the theory that's supposed to help rid some of the surprise here is the idea that

Given the architecture of a transformer where it has to go through all the tokens in every cycle, having these extra tokens gives it sort of more computational bandwidth and therefore more expressivity or more capacity to locate the right spot in parameter space.


SPEAKER_00:
And even that, in a way, reminds me of... So it's not just that dots improve performance.

It's that, like you mentioned, it was trained to have that.

And similarly, it could have been trained maybe hypothetically to just output Shakespeare quotes verbatim while you're processing.

So that's kind of like a filler or more of like a sort of

that was a great question.

It's like, these are linguistic paddings that do create time to get to the meat.

So not only does it signal and signpost if it's being trained to have that meaning, which then questions like, so then was it a meaningless dot if it had a cognitive or even like a semantic aspect?

I had a question.

How do you feel like in this era, Cambrian explosion of diverse intelligences, how can we understand capacities when they seem so conditional upon the setting and how the system of interest is interacted with?

like what are the practical implications for people who are studying LLMs and other synthetic intelligences from like a safety or reliability or performance perspective?


SPEAKER_01:
Charles, you want to?


SPEAKER_02:
No, I was throwing it to you.


SPEAKER_01:
So, so, so just want to chat to some of the questions.

So the question is,

how is the notion of a capacity changing when we have such different systems that seem to have intelligent behavior?


SPEAKER_00:
Yeah, and it's so dependent upon potentially initially unintuitive ways of interacting.

So how can we understand the reliability and the performance and the capacity of a model

other than, for example, by exhaustively inputting prompts, which can't really happen, what could we really say or know?

And or just how do you feel that this work reenters into the ways that people practically are using the models?


SPEAKER_01:
Right.

Okay.

Yeah.

It's a very interesting question.

So the first part of the question is, I think

Part of the background assumption from this paper that I've explicitly defended in other work is that behavioral evidence is simply not sufficient in most cases to arbitrate disputes about capacities of algorithms.

When it comes to human cognition,

We do have to rely a lot on psychological experiments that are ultimately behavioral.

And we do also rely on self-reports a little more than we can when it comes to LLMs because we, despite the move away from relying on intuition and introspection in the history of psychology, it still has a role to play.

But we've, by and large, gravitated towards behavioral experiments that get increasingly sophisticated.

to try to reverse engineer what's going on inside the black box.

When it comes to LLMs, partly because that's so different from us, relying exclusively on behavior is even more difficult because we have even less of an idea of what might be going on inside the black box.

and we have reasons to think it might be very different.

So I think we both agree that we have to supplement this with mechanistic work that essentially involve performing causal interventions on the inner mechanisms, on the inner workings of the systems.

So decoding representation and computations that the systems that are in principle available to the systems and then intervening on them to confirm hypotheses about the causal role of these representations and computations.

And we have methods to do that, and partly

Why we can be a little optimistic about this project, even though it's extremely challenging, especially to be scaled up to large models, is because unlike what's happening in neuroscience with the brain, where the range of decoding methods and intervention methods we have is extremely limited, but for ethical and for simply practical reasons, we just don't have ground truth access to activation.

we also are generally unable to make specific interventions on activations in the brain.

When it comes to LLAMS, we have full ground truth knowledge of all activations of every single part of the network.

And we also have full access to all of it for interventions at inference time.

So that opens up a whole new range of things we can do.

And that enables us to go beyond behavioral studies and actually decode these features

philosophers would generally put it, representations and computations that the system is actually making use of, and try to reverse engineer what kind of algorithms it's making use of.

So part of the broader problem projects that we have with Charles is to, suppose that we start with these investigative kinds, as we put, as Adivald calls them, these human-centered capacities.

We can operationalize them and do behavioral experiments in the top down.

And then from bottom up, we can also try to reverse engineer the mechanics of building blocks of the computations and representations that elements make use of to solve the task related to that particular capacity.

And then we can meet somewhere in the middle and try to, from that line of work that I

these things from both ends bring to the fore some kinds of mid-level abstractions as we call it or computational building blocks that might be key to the performance of the system in that domain so for example if you're interested in the capacity for reasoning you can start with this very broad

human-centric notion of reasoning, then try to operationalize it in a reasoning task, then do some behavioral testing and then mechanistic interpretability of that reasoning task, find out how the system is solving it, find out how the other one is doing

well and why reverse engineer building blocks that might for example have to be variable manipulation viable binding and then from there you might be able to iteratively refine the notion of reasoning you started with uh to have a more specific and less human centric notion that is now operationalized in more low level

multiple terms that involve the manipulation of variables in certain ways and the banning of variables to figures, et cetera.

So yeah, so that's, I think, the general approach we take.

Now, does any of that feed back into interactions, how we humans interact with LLMs?

I think that's one way in which it could feed back is simply in terms of challenging our spontaneous anthropomorphic attitudes to LLMs to some extent.

The same way if you read a lot of animal cognition, perhaps you will interact with your cat in a slightly different way that you might maybe not rush to conclude

that when your cat performs a certain behavior, it has understood what you think and it's modeling what you're thinking about what it's thinking or something.

Perhaps you might adopt a more deflationary attitude to explain the behavior of the cat.

It doesn't mean you have to love them any less, or it doesn't mean you have to, you know,

That is the other thing.

If you want, at the end of the day, to speak to your cat like a human because you're the urban gentleman for that, then that's all the more part of you.

In the same way, if you find it useful to treat LLMs in the way you interact with them,

to have fluid interactions with them, to treat them as if they had beliefs, desires, et cetera, of human-like capacities, then that's fine if that's for actual purposes.

But at least if that line of work that we are sketching here ends up maturing enough, the hope is that we can interact with LLMs perhaps in a way that's

where even if we have that kind of intentional stance and make believe about the kinds of capacities they have, at the background, we will know what their limitations are and what their actual capacities are.

I don't know if you agree with that, Charles, because we're going to discuss that much.


SPEAKER_02:
Yeah, I agree with all that.

I just had a slightly different first reaction to the question.

I took the question to be in part about

how to deal with the sort of prompt sensitivity of models.

The fact that sometimes we write something that seems natural to us, but provokes an unexpected response from a large language model.

And how do we think about that?

And the first thought that occurred to me was just that we should distinguish between different kinds of large language models.

You know, we have this sort of huge large language models, which are

fine-tuned to interact with us in a particular way.

And here's the central point, they're trained on a sort of unthinkably large database, whereas there are other sorts of large language models where the training data is more circumscribed and where we know

in more detail where you can survey what the training data says.

And I think if you're interested in what the mechanisms are underlying the responses, it's certainly very helpful to look at smaller

but nevertheless large language models where the training data is known to us.

Because when you train a model on the entire internet, there are going to be all kinds of subtle signals in there that we don't have much hope of tracing back to their source, but which will influence the model behavior in all sorts of ways.

But working with these somewhat more conscripted models gets rid of that problem, at least in part.


SPEAKER_00:
Cool.

Well, where do you see the work going or where do you plan to continue this direction?


SPEAKER_01:
Yeah, so actually, so we wrote this paper, this short paper for the ICML machine learning conference, the International Conference Machine Learning that's happening this week in Vienna and we'll be getting to Vienna at the end of the week for the popular workshop at which we're presenting this

models and cognitive science.

So there is a very strict page limit for these ICML contributions, which is four pages.

But what we want to do next is to expand this into a more philosophically substantive paper, and it's going to be a bit longer.

And that's going to expand on the more philosophically meaty parts of that project.

compressed in that version that we're presenting on HTML.

So yeah, that's the next step for us.

This is a really useful way for us to force ourselves to write things down.

After running the box piece, we wanted to write an academic piece.

Now we've written kind of a condensed skeleton of the piece that focuses more, that caters more to an ML audience.

And now the next step is to write the full philosophy paper for at least that part of the project to be complete.

And then after that, maybe we'll have a lot of ideas, but yeah.


SPEAKER_02:
Yeah.

I got nothing to add to that.


SPEAKER_00:
Cool.

That's well, it's very interesting work.

I think it brings a lot of pieces together.

And it's some philosophy and cognitive science, jumping in, jumping into the heat and into the the spotlight and the relevance.

And so it's going to be an exciting learning journey.

Thanks for having us.


SPEAKER_02:
Yeah, thank you very much.

Cool.

Enjoy the conversation.


SPEAKER_00:
Until next time.

Thank you.


SPEAKER_02:
Bye.
[
  {
    "start": 7.22,
    "end": 8.401,
    "text": " Hello, welcome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 9.002,
    "end": 25.16,
    "text": "It is July 3rd or 4th, depending on where you are, and we are in Active Inference Guest Stream 110.1 with Takuya Isamura discussing the triple equivalence for the emergence of biological intelligence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 25.18,
    "end": 35.792,
    "text": "Previously, we had some discussions in the Livestream 51 series when we visited Area 51 together, and we talked a lot about a",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 35.772,
    "end": 37.014,
    "text": " double equivalence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 37.975,
    "end": 40.239,
    "text": "And now the bar has been raised.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 40.54,
    "end": 42.362,
    "text": "The increment has been incremented.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 42.703,
    "end": 48.272,
    "text": "And this is a really exciting area that I'm sure we'll have a lot to discuss and learn from.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 48.492,
    "end": 52.318,
    "text": "So thank you again for joining and looking forward to the presentation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 55.884,
    "end": 56.625,
    "text": "Great.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 56.865,
    "end": 60.531,
    "text": "Thank you, Daniel, for the invitation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 61.017,
    "end": 65.606,
    "text": " So thank you for inviting me for this guest talk of Active Inference Institute.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 65.707,
    "end": 71.739,
    "text": "I'm very happy to share my recent paper on the triple equivalence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 72.48,
    "end": 74.284,
    "text": "So let's get started.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 76.188,
    "end": 81.234,
    "text": " So I'm interested in the theory of intelligence in general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 81.535,
    "end": 90.886,
    "text": "So maybe baby had no such high intelligence, but through learning it obtained the intelligence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 91.387,
    "end": 99.136,
    "text": "This can be modeled or understand in terms of the self-organization of neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 99.156,
    "end": 101.419,
    "text": "So newborn neural network may",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 101.77,
    "end": 117.805,
    "text": " may not have enough information regarding the external world, but through some optimization, which is typically expressed in terms of the energy minimization, it achieves some",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 118.898,
    "end": 132.674,
    "text": " obtaining of some good structure to recapturate the external world which is the basis for prediction and making insight or making creativity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 133.174,
    "end": 147.471,
    "text": "So we would like to model those emergence of intelligent function within the neural network through the energy minimization formulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 147.451,
    "end": 172.53,
    "text": " and so far there are to my opinion there are three major pillar in the theory of intelligence so intelligence occurs in the brain so we model the brain using a network of neurons and this is a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 173.185,
    "end": 198.214,
    "text": " diagram of electrical circuit and similarly a single neuron can be modeled like this so this diagram represents a dynamical equation of a single neuron and through some approximation we can represent such a dynamics in terms of the energy minimization so here",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 198.194,
    "end": 210.074,
    "text": " The dynamics that descent this energy gradient, energy landscape represents the time change of the neural activity like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 210.555,
    "end": 213.9,
    "text": "So this is the view from the dynamical system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 215.964,
    "end": 220.371,
    "text": "Another view of intelligence is the computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 221.043,
    "end": 228.611,
    "text": " So it is well known that any algorithm can be expressed in terms of the Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 228.891,
    "end": 241.885,
    "text": "Turing machine is a simple abstract machine that express the computation, which involves finite state machine that make some computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 242.506,
    "end": 244.608,
    "text": "And there is a long tape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 245.269,
    "end": 250.054,
    "text": "So in the tape, some information is encoded.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 250.034,
    "end": 253.321,
    "text": " So like binary signals.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 254.404,
    "end": 263.925,
    "text": "And when this finite state machine read this information,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 264.361,
    "end": 289.761,
    "text": " incorporate that information into the of the this state machine header so those components can be expressed by mapping shown here so given the current state and readout information it provides the move and the writing information and the next state",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 289.741,
    "end": 294.293,
    "text": " So this is a basic idea underlying the Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 294.433,
    "end": 304.78,
    "text": "And interestingly, this simple computation is sufficient to express any kind of computable algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 306.481,
    "end": 314.329,
    "text": " And another view recently focusing on is the brain as the inference machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 314.97,
    "end": 319.955,
    "text": "So brain is considered now as an agent that perform Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 320.755,
    "end": 326.121,
    "text": "So this is an example of the Bayesian statistical inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 326.541,
    "end": 331.206,
    "text": "So here, there is some apple nearby the wall.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 331.186,
    "end": 346.222,
    "text": " By seeing that, we don't think that apple is wrapped in the shape of wall, but we guess that this apple is hidden by the wall in front of the apple.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 346.242,
    "end": 350.566,
    "text": "So this is a kind of inference based on our experiences.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 350.647,
    "end": 354.611,
    "text": "So this can be formulated in terms of the Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 355.151,
    "end": 358.695,
    "text": "So Bayesian inference is basically",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 359.232,
    "end": 365.022,
    "text": " defined by the equation called Bayes' theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 365.883,
    "end": 372.354,
    "text": "This is the Bayes' theorem that tells us the probability of state given.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 372.775,
    "end": 383.653,
    "text": "We can obtain the same solution by just minimizing energy, which is called variational free energy in the Bayesian inference field.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 383.633,
    "end": 408.612,
    "text": " The idea here is that if we change the state to reduce diff energy, and then if it reaches the valley of this energy landscape, this point represents a solution of Bayes' theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 409.03,
    "end": 420.104,
    "text": " So instead of directly computing equation by just minimizing energy, we can get the best guess about the external world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 420.585,
    "end": 424.47,
    "text": "And this process is expressed by like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 425.391,
    "end": 432.76,
    "text": "So this is the idea behind the theory called the free energy principle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 432.942,
    "end": 452.115,
    "text": " Free energy principle, as you may know, is a theory of the brain proposed by Carl Fristen, which states that perception, learning, and action of all biological organs are determined to minimize variational free energy, and by doing so, they perform variational basal inference for external world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 452.534,
    "end": 459.943,
    "text": " This animation is the one I often use to explain the forensic principle framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 460.544,
    "end": 466.732,
    "text": "Here there is an animal, which is our agent that infers external words.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 467.212,
    "end": 475.723,
    "text": "An external word comprises observable information and unobservable information, which is a hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 476.294,
    "end": 484.266,
    "text": " The task for this agent is to infer the hidden state based only on observable information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 485.949,
    "end": 498.449,
    "text": "By constructing the posterior belief in the brain, it can guess what is the current hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 499.35,
    "end": 503.196,
    "text": "This can be done by minimizing the free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 503.176,
    "end": 524.593,
    "text": " The idea is that if we change the posterior belief to minimize this landscape, then we get the optimal solution, which is called posterior belief or expectation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 524.708,
    "end": 528.081,
    "text": " This is the equation of the variation of energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 528.101,
    "end": 535.208,
    "text": "It comprises the prediction error term and entropy or complexity term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 535.846,
    "end": 544.0,
    "text": " So to achieve this sort of inference, we also need to update the model itself.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 544.281,
    "end": 548.148,
    "text": "So this is done by update of the parameter theta.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 548.749,
    "end": 554.799,
    "text": "So in the external world, there is some mapping from hidden state to observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 554.96,
    "end": 560.79,
    "text": "So we imitate this mapping using the generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 560.77,
    "end": 570.409,
    "text": " By tuning this theta, our generative model is optimized to recapture the external world, and then we get the best guess about the hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 571.31,
    "end": 574.797,
    "text": "This is the mechanism underlying the perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 575.992,
    "end": 586.104,
    "text": " And a unique point of the free energy principle is that it can be applied also to the optimization of action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 586.764,
    "end": 594.754,
    "text": "So here we consider that there is some preference prior for this agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 595.134,
    "end": 601.982,
    "text": "And to achieve this sensory input, this agent can select",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 601.962,
    "end": 605.792,
    "text": " an action from a given set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 606.674,
    "end": 612.428,
    "text": "Here there are three options and this optimization can be done by",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 612.796,
    "end": 619.826,
    "text": " minimizing the free energy in the future, which is called expected free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 620.247,
    "end": 622.911,
    "text": "So, except free energy is a function of action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 622.991,
    "end": 636.671,
    "text": "So, by selecting the action that achieves the minimum free energy, minimum expected free energy, it can maximize the probability to obtain the preferred input in the future.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 637.051,
    "end": 640.997,
    "text": "So, this is the idea, so-called active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 641.922,
    "end": 654.008,
    "text": " So like this, free energy principle is a normative theory that can express both perceptual optimization and action optimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 654.106,
    "end": 666.452,
    "text": " For neuroscientists like us, the most interesting point is what is the neuronal implementation underlying those inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 667.053,
    "end": 671.963,
    "text": "So we would like to show",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 673.023,
    "end": 678.549,
    "text": " actual neuronal implementation of this sort of active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 679.41,
    "end": 689.241,
    "text": "To address this issue, we recently proposed a double equivalence between neural dynamics and Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 689.301,
    "end": 692.625,
    "text": "We start from evaluating the neural activity equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 693.025,
    "end": 695.588,
    "text": "The neural activity equation is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 695.568,
    "end": 701.566,
    "text": " have long history, so it has a clear biological background.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 701.647,
    "end": 705.358,
    "text": "So we consider that this is a plausible one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 705.693,
    "end": 726.604,
    "text": " And if this equation is derived as a gradient descent of some energy function, we can obtain the original equation by just taking the integral of this activity equation and get energy, which is called Helmholtz energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 727.782,
    "end": 738.334,
    "text": " On the other hand, for Bayesian inference, once we define the generative model about the external world, we can derive the free energy functional.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 739.375,
    "end": 746.163,
    "text": "This is a variational free energy, and its minimization indicates the achievement of Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 746.183,
    "end": 751.95,
    "text": "We show a mathematical equivalence between those two energy functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 753.213,
    "end": 760.606,
    "text": " Which means that for any neural network in this class, there is an interpretation in terms of Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 760.706,
    "end": 773.989,
    "text": "So we can, in general, see neural activity or neural network dynamics, including synaptic plasticity, as inference about the external states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 776.5,
    "end": 780.971,
    "text": " So this is a more mathematical definition of our equivalence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 781.632,
    "end": 793.581,
    "text": "So we characterize neural activity equation using some rate coding model, which is derived from a realistic spiking model through some approximation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 793.561,
    "end": 796.648,
    "text": " Its form is like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 796.668,
    "end": 809.477,
    "text": "It is characterized by inverse sigmoid rift factor and O indicates the sensory input from input layer, W is the synaptic weight, H is the firing threshold.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 809.457,
    "end": 834.52,
    "text": " and x here is a vector of neural activity which is rate coding neurons and its dynamics is given like this and if this is given then we can consider the original energy function by taking the integral of this activity equation with respect to x and we get",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 835.09,
    "end": 839.336,
    "text": " explicit form of Helmholtz energy like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 839.356,
    "end": 849.972,
    "text": "Although it's a little bit complicated, it's just the integral of the original equation, so don't worry about that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 850.874,
    "end": 860.608,
    "text": "And those definitions, bar is here, and hat indicates the sigmoid function of W and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 861.044,
    "end": 866.834,
    "text": " Phi is a threshold factor defined like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 867.255,
    "end": 877.192,
    "text": "Interestingly, although this energy is purely derived from activity equation, the same energy provides the plasticity equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 877.833,
    "end": 886.748,
    "text": "If we take the derivative of this A with respect to W, then we get Hebbian plasticity equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 887.234,
    "end": 889.036,
    "text": " So this is non-trivial, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 890.258,
    "end": 896.786,
    "text": "This is purely derived from this activity equation, but it also provides spasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 897.788,
    "end": 904.116,
    "text": "Moreover, for Bayesian inference, once we define the generative model, we get the variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 904.456,
    "end": 909.563,
    "text": "So this is a variational free energy under the assumption that the generative model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 909.88,
    "end": 915.293,
    "text": " have the shape of the partially observable Markov decision process or POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 915.895,
    "end": 924.396,
    "text": "So if the external world is expressed as a discrete state space, then we get this kind of variational frequency.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 925.456,
    "end": 931.972,
    "text": " Its minimization provides the solution of Bayesian inference, which is a posterior belief about state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 932.594,
    "end": 940.714,
    "text": "We also obtain the posterior belief about the model parameter, which corresponds to learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 940.694,
    "end": 952.247,
    "text": " And what we say is that there is a formal equivalence between those two energy functions in the sense that each component has a one-to-one mapping between those two.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 952.908,
    "end": 962.279,
    "text": "So S here indicates the posterior expectation about each state taking 0 or 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 962.619,
    "end": 967.765,
    "text": "So it is expressed as the block vector just like the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 967.745,
    "end": 970.369,
    "text": " block beta of X and X bar.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 971.15,
    "end": 973.453,
    "text": "And this logarithm corresponds to this logarithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 973.934,
    "end": 979.923,
    "text": "And this A actually has the block matrix structure, which is exactly corresponding to this block matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 980.423,
    "end": 986.472,
    "text": "And this operation in a product corresponding to this product of matrix and vector.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 986.993,
    "end": 995.305,
    "text": "And this log D, which is the state prior, is formally corresponding to phi here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 996.196,
    "end": 1015.701,
    "text": " So as a result, we can say that for any neural activity and plasticity in this class that minimize the common Helmholtz energy, we can lead those dynamics in terms of the Bayesian inference of external states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1018.104,
    "end": 1024.532,
    "text": "Moreover, this framework can be extended to the empirical application",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1025.086,
    "end": 1034.577,
    "text": " So to apply the theoretical framework into the empirical data, we first record neuronal activity from the real brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1036.058,
    "end": 1043.627,
    "text": "And we assign those data into our canonical neural network, which is our network model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1044.528,
    "end": 1053.658,
    "text": "And by taking the integral, we can get the underlying Helmholtz energy, which characterizes the neural dynamics and plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1054.921,
    "end": 1069.183,
    "text": " Moreover, we can estimate the parameter from data to characterize this energy function specific for individual.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1070.345,
    "end": 1081.182,
    "text": "And through our established equivalence, we can identify the generative model and the corresponding variational free energy from this A.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1082.157,
    "end": 1087.244,
    "text": " And this works as a synthetic agent that performs active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1087.365,
    "end": 1100.083,
    "text": "So its derivatives produce a learning algorithm or plasticity algorithm that updates synaptic weight to adapt to the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1100.644,
    "end": 1106.072,
    "text": "And its time integral provides a prediction about the learning process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1106.052,
    "end": 1118.475,
    "text": " so we call this framework as a reverse engineering in the sense that from neural activity data we can get the generative model which is a blueprint for the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1118.692,
    "end": 1119.954,
    "text": " individual animals.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1120.715,
    "end": 1129.71,
    "text": "So by doing that, we can get a synthetic agent that imitates the original animal's neural activity and behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1130.371,
    "end": 1137.443,
    "text": "So now we can make the examination of the free energy principle",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1137.423,
    "end": 1148.735,
    "text": " Our idea is that we can do this based only on neural activity in the initial learning stage.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1149.877,
    "end": 1164.072,
    "text": "We record neural activity before training and characterize generative model and make prediction of the subsequent learning process based on the synthetic agents that perform active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1165.233,
    "end": 1179.635,
    "text": " its prediction is consistent with neural activity data recorded after training, then it indicates the plausibility of the theory underlying this synthetic agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1179.735,
    "end": 1190.071,
    "text": "So if this is the case, then we can validate the predictive validity of the free energy principle and active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1190.411,
    "end": 1191.713,
    "text": "So this is the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1192.79,
    "end": 1202.706,
    "text": " We applied this to an in vitro neural network cultured on a dish.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1202.826,
    "end": 1212.942,
    "text": "This is a neuron taken from rat embryos, and we cultured those neurons on a dish that has 64 electrodes at the floor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1213.377,
    "end": 1222.547,
    "text": " And we stimulate those neurons using designed sensory signals, which is the electrical pulse.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1222.567,
    "end": 1233.92,
    "text": "So to stimulate neurons, we design those stimulation patterns using two hidden states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1234.521,
    "end": 1239.486,
    "text": "So those are hidden sources that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1239.466,
    "end": 1244.773,
    "text": " invisible for in vitro neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1244.813,
    "end": 1252.403,
    "text": "They can directly receive signals only from sensory stimuli layer here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1253.564,
    "end": 1260.293,
    "text": "By receiving those mixed sensory signals, those neurons are required to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1260.273,
    "end": 1262.096,
    "text": " infer the hidden state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1262.696,
    "end": 1271.028,
    "text": "So if it can infer the hidden state, then it supports the prediction of frequency principle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1271.068,
    "end": 1272.21,
    "text": "So we tested that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1273.131,
    "end": 1280.161,
    "text": "And we found that actually some neurons respond preferentially to a specific source.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1280.421,
    "end": 1285.909,
    "text": "So the prediction of frequency principle is correct in this setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1285.889,
    "end": 1298.223,
    "text": " And we also statistically estimate the effective connectivity between those neurons and plot those trajectories.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1298.243,
    "end": 1301.668,
    "text": "So synaptic weights change during the training.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1301.888,
    "end": 1308.896,
    "text": "So we plot that trajectory onto the theoretically derived free energy landscape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1309.296,
    "end": 1315.744,
    "text": "So this is free energy characterized only by the initial data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1315.724,
    "end": 1319.35,
    "text": " of this in vitro learning experiment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1320.051,
    "end": 1325.059,
    "text": "And we now plot the trajectory onto this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1325.08,
    "end": 1341.647,
    "text": "So this is the empirically estimated synaptic wedge trajectory, which started from the high point of the landscape and it reduced the free energy to reach the bottom.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1343.129,
    "end": 1352.825,
    "text": " which is consistent with the expectation of the free energy principle that synaptic plasticity occur to minimize the free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1353.025,
    "end": 1354.808,
    "text": "So yeah, it's consistent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1355.609,
    "end": 1362.42,
    "text": "If this is the case, we can make synthetic agents that perform free energy minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1363.061,
    "end": 1369.592,
    "text": "So this is a result of the simulator that just exploit the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1369.572,
    "end": 1374.14,
    "text": " gradient of this theoretically computed free energy landscape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1374.561,
    "end": 1379.088,
    "text": "So it is just a simulation without using data.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1380.01,
    "end": 1391.249,
    "text": "Nevertheless, its result is highly consistent with actual observation, meaning that the theory has the prediction ability in this setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1391.47,
    "end": 1394.635,
    "text": "So it shows the validity of the free energy principle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1395.762,
    "end": 1403.477,
    "text": " And then we extend our idea to action optimization or active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1403.497,
    "end": 1415.02,
    "text": "To optimize action, we need to consider the modulation of heavy and plasticity because our action is changed based on the given reward or punishment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1415.0,
    "end": 1428.341,
    "text": " Classically, this sort of optimization of action is associated with the Hebbian plasticity modulation by various neuromodulators.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1428.962,
    "end": 1433.91,
    "text": "One famous one is the dopaminergic modulation of Hebbian type plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1434.551,
    "end": 1438.037,
    "text": "It changes the time window of the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1438.304,
    "end": 1446.058,
    "text": " So STDP here means the spike timing dependent plasticity, which is a sort of hepium plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1446.398,
    "end": 1454.172,
    "text": "So dopamine changes drastically the window of STDP like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1455.073,
    "end": 1461.865,
    "text": "We incorporate those effects into the plasticity equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1462.03,
    "end": 1474.127,
    "text": " And we also consider two-layer recurrent neural network with output layer to model the action of animals.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1474.147,
    "end": 1479.354,
    "text": "So it is basically the same model as the previous one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1480.236,
    "end": 1483.38,
    "text": "We call this canonical neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1483.647,
    "end": 1503.893,
    "text": " One important characteristic of this canonical neural network is that all of those equations, both neural activity and plasticity, can be derived from a single energy function, which means that there is a corresponding Bayesian inference characterized by variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1503.913,
    "end": 1512.304,
    "text": "So we again get the one-to-one correspondence between neural dynamics and the Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1512.284,
    "end": 1536.925,
    "text": " Interestingly, this sort of neural network can perform maze task, so this is a typical delayed reward task, but by inferring the best action that obtains the maximum reward in the future, which is, in other words, minimum risk associated with the future outcome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1536.905,
    "end": 1548.517,
    "text": " By doing so, this network can achieve the best action to realize this main task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1549.378,
    "end": 1555.784,
    "text": "So far, we showed double equivalence between neural network dynamics and Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1556.125,
    "end": 1565.494,
    "text": "In general, a canonical neural network that minimizes some energy function by both activity and plasticity",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1565.744,
    "end": 1571.417,
    "text": " it inevitably performs Bayesian inference, active inference, in some sense.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1572.038,
    "end": 1582.943,
    "text": "So Frey-Nagy principle is such a generic, general framework to characterize this sort of biological approach for neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1582.923,
    "end": 1589.552,
    "text": " The next step is the incorporation of algorithmic computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1590.374,
    "end": 1601.269,
    "text": "So far, I have discussed optimization of perception and action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1602.228,
    "end": 1605.371,
    "text": " completely generic intelligence form.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1605.572,
    "end": 1615.803,
    "text": "So it is limited only for optimizing behavior to obtain, to minimize the future risk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1616.263,
    "end": 1626.514,
    "text": "But question is whether this framework can be applied to the emergence of a generic intelligence, generic algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1627.388,
    "end": 1629.891,
    "text": " That's my interest.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1629.911,
    "end": 1636.198,
    "text": "So to address this issue, we propose the triple equivalence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1636.218,
    "end": 1644.026,
    "text": "We show the triple equivalence between neural network dynamics and variational Bayesian inference framework and Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1644.467,
    "end": 1647.43,
    "text": "So here we have two Turing machines.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1647.63,
    "end": 1651.234,
    "text": "One is an internal Turing machine and one is an external Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1651.614,
    "end": 1656.72,
    "text": "So instead of characterizing the environment in terms of the",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1656.953,
    "end": 1665.09,
    "text": " generative model, we characterized the environment in terms of external Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1665.442,
    "end": 1679.917,
    "text": " But the point here is that such a Turing machine can also be implemented in terms of the probability distribution, which is actually expressed in terms of generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1681.599,
    "end": 1694.993,
    "text": "Moreover, because Turing machine typically deal with the discrete state space, so this is expressed in the form of POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1694.973,
    "end": 1701.552,
    "text": " So we simply consider the POMDP with some external tape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1702.014,
    "end": 1705.945,
    "text": "So this is what I actually did.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1706.195,
    "end": 1717.19,
    "text": " So to infer such an environment, we have the Turing machine, which basically has the same shape as the external Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1717.871,
    "end": 1726.223,
    "text": "And we show that this type of Turing machine can be implemented by canonical neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1726.243,
    "end": 1731.19,
    "text": "So the idea is that we first",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1732.064,
    "end": 1737.09,
    "text": " characterize the canonical neural network in terms of the energy function like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1737.591,
    "end": 1748.003,
    "text": "And we also characterize differentiable Turing machine in terms of the energy minimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1748.784,
    "end": 1762.02,
    "text": "So if we write Turing machine in some differential equation by taking the integral, we can again obtain the energy function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1763.215,
    "end": 1780.043,
    "text": " And through this process, we confirm that actually the energy function derived from Turing machine is equivalent to that for canonical neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1780.023,
    "end": 1791.62,
    "text": " So this happened because output layer of canonical neural network can be seen as a tape for Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1792.221,
    "end": 1798.69,
    "text": "And this operation, reading out and writing into tape, can be implemented",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1799.075,
    "end": 1816.037,
    "text": " in a biologically plausible manner through mental action that affects some change for the internal state and the writing can be implemented by",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1816.017,
    "end": 1821.606,
    "text": " rapid plasticity of output layer semantic weight.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1822.167,
    "end": 1834.026,
    "text": "So through those processes, we can implement a function of Turing machine, which is the reading and writing and state update.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1834.006,
    "end": 1848.825,
    "text": " And header position movement is also implemented by the sparse neural activity that expresses an address of the information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1849.177,
    "end": 1863.95,
    "text": " And once we get an energy function, which is common for all components in the system, then we can say that there is a corresponding variation of energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1863.97,
    "end": 1874.139,
    "text": "So we found that actually this sort of Turing machine formally corresponds to a kind of POMDP generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1875.18,
    "end": 1877.682,
    "text": "And again,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1878.033,
    "end": 1888.607,
    "text": " This mapping from output to state posterior corresponds to readout of information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1891.411,
    "end": 1902.927,
    "text": "If we rapidly change the C matrix, which is a policy matrix that determines our action, then this acts as memory writing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1902.907,
    "end": 1911.45,
    "text": " So rapid change of policy can be seen as writing of information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1912.713,
    "end": 1916.764,
    "text": "So in short, what we show is that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1916.997,
    "end": 1925.193,
    "text": " We characterize neural network in terms of the energy minimization, so it involves both neural activity and plasticity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1926.195,
    "end": 1937.858,
    "text": "Interestingly, this typical neural network dynamics can be read as both mapping of Turing machine and Bayes theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1939.137,
    "end": 1957.12,
    "text": " in the sense that this is an energy-minimizing activity, and if energy is minimized with respect to internal state of neural network, then this solution can be seen as the solution of Bayesian inference, which is Bayes' theorem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1957.6,
    "end": 1965.33,
    "text": "And also, simultaneously, it also represents mapping, this mapping.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1965.31,
    "end": 1986.55,
    "text": " which means that this network is not only limited to infer the typical POMDP in the external world, but also it can infer the Turing machine in the external world and imitate that function inside the network dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1987.07,
    "end": 1994.457,
    "text": "So it can mimic the external algorithm using neural network dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1994.437,
    "end": 2022.688,
    "text": " so that's the idea so based on this generic framework we performed some demonstration so this is a demonstration using other so other means a machine that adds count up the number so here we provide the input as the MNIST hand digit so actually this is only binary signal so each",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2022.668,
    "end": 2026.053,
    "text": " becomes either 0 or 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2026.113,
    "end": 2034.547,
    "text": "This 16 binary number represents inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2035.889,
    "end": 2044.001,
    "text": "External Turing machines count up those numbers and memorize that sum into the tape.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2044.041,
    "end": 2047.647,
    "text": "This is how the environment works.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2048.69,
    "end": 2064.841,
    "text": " Given this system, the internal canonical neural network, which performs active inference, can perform the imitation of these dynamics by receiving this 100-digit infra-weather.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2064.961,
    "end": 2071.153,
    "text": "This is 0 over 1 through this layer and internal computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2071.133,
    "end": 2082.98,
    "text": " make addition of those signal into the previous summation which is stored in the memory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2083.02,
    "end": 2090.577,
    "text": "And the memory is actually implemented by output layer synaptic weights.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2090.81,
    "end": 2107.367,
    "text": " So here, given the internal state, the middle layer neural activity, it sends some signal to output layer, which corresponds to address of the information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2107.347,
    "end": 2122.082,
    "text": " And given this output reactivity generated action, this weight corresponds to the memory in the tape, and it takes either 0 or 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2122.872,
    "end": 2135.251,
    "text": " So by sending your signal header information here, it receives the memory information and its feedback to the internal state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2135.692,
    "end": 2139.638,
    "text": "So this is how this network works as an adder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2140.56,
    "end": 2143.885,
    "text": "And writing of new information into",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2144.32,
    "end": 2150.148,
    "text": " the output layer is done by Hebbian plasticity modulated by risk factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2150.668,
    "end": 2173.298,
    "text": "So this plasticity occurs in a rapid manner so that by receiving new risk, it drastically changes the synaptic weight either from 0 to 1 or 1 to 0, which is sufficient to implement the lighting function of the Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2173.278,
    "end": 2179.169,
    "text": " So this sort of neural network can implement Turing machine in a biologically plausible manner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2180.111,
    "end": 2182.776,
    "text": "And this is a result of inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2183.217,
    "end": 2195.961,
    "text": "So by receiving hand digit, which is noisy information, it can correctly infer the hidden state like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2195.941,
    "end": 2207.019,
    "text": " And it successfully stores the information in the output layer, which represents the summation of the number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2207.039,
    "end": 2215.091,
    "text": "So it counts up the input number and stores it here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2215.111,
    "end": 2216.734,
    "text": "And through learning,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2217.102,
    "end": 2227.475,
    "text": " the accuracy of inferring hidden state and memorize information summed number is improved.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2228.396,
    "end": 2234.123,
    "text": "So before training, the estimated number is largely different from the true number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2234.564,
    "end": 2245.658,
    "text": "But in these four sessions, which comprises 2,048 steps, it achieved good inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2245.84,
    "end": 2250.909,
    "text": " And this is a quantitative evaluation prediction error.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2251.209,
    "end": 2255.116,
    "text": "It decreased with time increase.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2255.877,
    "end": 2268.118,
    "text": "And also, this network optimized its parameter by minimizing free energy, and the parameter estimation error decreased with time, like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2269.972,
    "end": 2282.913,
    "text": " This is a demonstration of algorithmic implementation and inference of external algorithms using canonical neural networks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2282.933,
    "end": 2286.9,
    "text": "However, there are some limitations in this framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2286.88,
    "end": 2296.572,
    "text": " Here, we assume a predefined writing rule which is implemented by RISC function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2297.894,
    "end": 2309.749,
    "text": "If we can design a RISC function which is corresponding to expect free energy a priori, then we can train the neural network in an appropriate manner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2309.729,
    "end": 2320.729,
    "text": " But the question is whether those predefined functions can also be optimized through a biologically plausible process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2321.009,
    "end": 2322.993,
    "text": "That's our next question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2323.594,
    "end": 2328.362,
    "text": "To address that, we consider the extension of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2328.342,
    "end": 2331.706,
    "text": " our framework to the evolutionary time scale.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2332.046,
    "end": 2340.116,
    "text": "So evolution occurs through the update of gene, and we associate that with the update of model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2340.436,
    "end": 2344.902,
    "text": "So we now extend the framework to the Bayesian model selection framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2345.703,
    "end": 2350.148,
    "text": "So by updating the model, we get a better",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2350.128,
    "end": 2370.752,
    "text": " generative model and based on that model we update parameter through the synaptic plasticity and then infer the hidden state and action through neural activity and its result is evaluated and then it's iterated so on so.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2370.732,
    "end": 2383.165,
    "text": " So we model this framework using extended Helmholtz energy, which is a function for a species, not the individual.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2383.666,
    "end": 2389.372,
    "text": "So we now define the distribution of agent like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2389.392,
    "end": 2398.782,
    "text": "So this represents a distribution of observation sequence and the gene distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2398.762,
    "end": 2412.784,
    "text": " which characterize what kind of agent exists in an environment and what kind of information is received by those agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2412.864,
    "end": 2415.308,
    "text": "We now consider the populations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2415.348,
    "end": 2424.702,
    "text": "This is defined by adding some component to the individual Helmholtz energy defined previously.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2424.682,
    "end": 2434.259,
    "text": " So this is the Helmholtz energy we discussed, and we add here to some factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2434.279,
    "end": 2440.509,
    "text": "For example, this represents the reproduction rate, which is the evolutionary fitness function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2441.471,
    "end": 2449.525,
    "text": "And this is the parent gene distribution, which corresponds to prior distribution of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2449.505,
    "end": 2466.169,
    "text": " And this is entropy, but it represents posterior beliefs about the observation and the gene distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2466.31,
    "end": 2475.563,
    "text": "So it is analogous to conventional variational free energy, but now we also optimize the model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2476.994,
    "end": 2490.037,
    "text": " So, interestingly, by taking the derivative of this or taking the variation of this functional, we get a solution like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2490.959,
    "end": 2499.053,
    "text": "So this solution, of course, it has a meaning in terms of Bayesian inference, but it has another meaning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2499.387,
    "end": 2518.468,
    "text": " So interestingly, this solution, posterior belief represents, is given as the product of evolutionary fitness function and marginal likelihood and prior gene distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2519.369,
    "end": 2529.04,
    "text": "And this marginal likelihood can be seen as the observation conditioned by the gene.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2529.762,
    "end": 2557.525,
    "text": " this result actually is equivalent to the solution of natural selection simple natural selection so now we found that by defining Helmholtz energy which is equivalent to variational free energy and minimizing that we get the solution of natural selection in a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2557.505,
    "end": 2569.165,
    "text": " This indicates that the solution of Bayesian model selection involves conventional natural selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2570.146,
    "end": 2574.674,
    "text": "This can be also understood by this inequality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2575.335,
    "end": 2578.18,
    "text": "Here it indicates that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2579.003,
    "end": 2587.595,
    "text": " population level Helmholtz energy or variation of energy, upper bounds this factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2587.615,
    "end": 2600.472,
    "text": "So this is a negative logarithm of partition function z. And interestingly, under this framework, this partition function correspond to the total of offspring number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2600.452,
    "end": 2608.061,
    "text": " Minimization of free energy provides the maximization of total offspring man-number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2608.381,
    "end": 2620.915,
    "text": "If this sort of agent wants to maximize their offspring, then it naturally minimizes free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2621.255,
    "end": 2627.202,
    "text": "That's what we characterize through those equations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2627.502,
    "end": 2644.515,
    "text": " And we confirm that through this process, the species of agent finally achieves the good model for making inference of the given environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2644.535,
    "end": 2650.506,
    "text": "So before the evolution, optimal genes correspond to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2652.072,
    "end": 2673.085,
    "text": " some machine that is different from others so it's genes like this so its operation provides a solution different from others but through training through evolution only gene that achieved operation or mapping",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2673.065,
    "end": 2677.149,
    "text": " which is correct as other survived.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2677.769,
    "end": 2692.083,
    "text": "And after sufficient evolution through 40 generations, finally most agents converge to a machine that performs as others.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2692.864,
    "end": 2699.73,
    "text": "So this is a result of Bayesian model selection, but it's more",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2700.942,
    "end": 2713.224,
    "text": " it's more actively select the best external world to achieve the maximization of offspring number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2713.284,
    "end": 2717.632,
    "text": "So it is called active Bayesian model selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2718.453,
    "end": 2723.001,
    "text": "And through this process, estimation error decrease and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2723.47,
    "end": 2726.557,
    "text": " Also, total number estimation error decreased.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2727.459,
    "end": 2736.18,
    "text": "So it showed a successful model selection by simply operating natural selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2737.51,
    "end": 2745.947,
    "text": " And finally, I briefly mentioned that this framework can also implement universal Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2746.168,
    "end": 2755.266,
    "text": "So universal Turing machine is a little bit more than conventional Turing machine because it can imitate any other Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2755.938,
    "end": 2762.088,
    "text": " Although it's an abstract definition, implementation is very straightforward.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2762.408,
    "end": 2771.803,
    "text": "If we have two mental actions, one represents data reading and one represents program reading, then we can.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2771.783,
    "end": 2775.87,
    "text": " implement a universal Turing machine using canonical neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2776.331,
    "end": 2784.805,
    "text": "This machine can infer the environment, even though the environment comprises 10 different machines.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2785.366,
    "end": 2798.027,
    "text": "So given sensory input, it can guess which machine now generates sensory input and select the best position of header or reading program.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2798.007,
    "end": 2809.898,
    "text": " By receiving a program written in tape, it characterizes internal computation in an adequate manner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2809.978,
    "end": 2811.559,
    "text": "It's a very flexible neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2812.84,
    "end": 2828.014,
    "text": "Given this program, it can imitate the current environmental machines through this process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2827.994,
    "end": 2839.448,
    "text": " And here, a different color indicates a different machine, and a black color indicates the position of header that neural network inferred.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2840.249,
    "end": 2848.779,
    "text": "So before training, inference is a little bit noisy, but after training, it success accurate inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2849.459,
    "end": 2855.807,
    "text": "And by doing so, parameter estimation error and model selection error decrease, and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2855.787,
    "end": 2859.23,
    "text": " the subsequent state prediction error also decreases.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2859.971,
    "end": 2870.461,
    "text": "So it successfully imitates 10 different machines in the external world only in the single neural network architecture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2871.181,
    "end": 2880.85,
    "text": "So in summary, we showed that the dynamics of canonical neural networks that minimize shared Helmholtz energy can be cast as minimizing variational free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2880.87,
    "end": 2884.714,
    "text": "So it basically performs active inference in general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2884.694,
    "end": 2901.285,
    "text": " A canonical neural network with mental action and rapid modulated plasticity can implement Turing machine in a biologically plausible manner and perform variational Bayesian inference of external algorithm which is expressed by Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2902.025,
    "end": 2915.606,
    "text": " And if we extend the Helmholtz energy minimization to the evolutionary scale, then spacy level Helmholtz energy minimization involves natural selection.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2915.847,
    "end": 2922.497,
    "text": "So that process can be naturally derived from minimizing energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2922.477,
    "end": 2928.587,
    "text": " providing a possible explanation for the emergence of generic biological intelligence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2929.308,
    "end": 2940.447,
    "text": "So finally, I just briefly mentioned that those programs, MATLAB or Python codes are all available at my GitHub page.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2940.467,
    "end": 2943.392,
    "text": "So if you are interested in, please check that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2943.412,
    "end": 2944.013,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2946.659,
    "end": 2973.257,
    "text": " thank you awesome a lot to cover there for people watching live they can write a question in the chat i wrote down a few as well so i'll just sort of start in no particular order are we seeking to find pairwise transformational edges and then maintain overall network equality through transitive properties",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2973.355,
    "end": 2982.329,
    "text": " Or do you see these different equivalences as arising from a more general form or notation?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2984.352,
    "end": 2985.754,
    "text": "Thank you for your question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2985.894,
    "end": 2991.603,
    "text": "So can I see that comment by myself?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2992.224,
    "end": 2992.405,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2993.246,
    "end": 2993.326,
    "text": "Yep.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2993.346,
    "end": 2994.147,
    "text": "Give me one second.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2995.289,
    "end": 2995.509,
    "text": "Yes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2996.851,
    "end": 3000.397,
    "text": "Yes, first part is a little bit complicated to me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3000.417,
    "end": 3001.759,
    "text": "So pairwise,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3002.735,
    "end": 3019.42,
    "text": " When you did the work in the double equivalence and now with the triple equivalence, do you seek to find pairwise strong relationships and keep everything equal through the transitive property?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3019.46,
    "end": 3031.257,
    "text": "Or do you think that there could be a common representation that has more of like a hub and spoke topology with a common underlying format that then is...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3033.667,
    "end": 3035.889,
    "text": " connected to each of these domains?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3038.412,
    "end": 3050.043,
    "text": "So I'm not sure if I correctly understand the question, but unlike double equivalence, which is correspondence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3050.063,
    "end": 3061.294,
    "text": "So in double equivalence, all neural network property, as long as it minimizes shared energy function, it is always Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3061.454,
    "end": 3063.336,
    "text": "So there is a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3063.721,
    "end": 3065.303,
    "text": " set of Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3065.423,
    "end": 3066.625,
    "text": "It is very general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3067.186,
    "end": 3072.853,
    "text": "And there is a subpopulation, which is biologically plausible Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3073.253,
    "end": 3079.862,
    "text": "So all of those subpopulations are involved in the generic Bayesian inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3080.403,
    "end": 3088.193,
    "text": "So if we design neural network by considering the energy minimization, then it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3090.687,
    "end": 3092.47,
    "text": " generic VGA inference framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3093.031,
    "end": 3101.824,
    "text": "On the other hand, Turing machine or in particular universal Turing machine is special one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3102.344,
    "end": 3108.954,
    "text": "Not all Turing machine works as a universal Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3109.415,
    "end": 3115.464,
    "text": "We may design some random network or random machine and we can say this is a Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3115.845,
    "end": 3118.028,
    "text": "It's not",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3118.008,
    "end": 3131.631,
    "text": " In some sense, it's correct, but it's not useful so far.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3132.512,
    "end": 3136.118,
    "text": "Universal computation, Turing completeness,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3136.098,
    "end": 3145.57,
    "text": " means the computation computing machine have sufficient ability to deal with algorithm computation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3146.07,
    "end": 3149.715,
    "text": "So yeah, that is something outside.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3151.497,
    "end": 3160.468,
    "text": "So what we are now thinking is that there is a group that achieved the Bayesian inference and there is another group",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3160.904,
    "end": 3167.553,
    "text": " that perform the Turing-complete computation, and there's an overlap here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3168.334,
    "end": 3178.687,
    "text": "So if we focus on that, that would be interesting to consider the emergence of high-level intelligence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3181.31,
    "end": 3181.731,
    "text": "I don't know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3182.723,
    "end": 3204.327,
    "text": " okay awesome all right i'm gonna paste into the chat so you can see it i just wanted to read this uh this quotation from a right i never checked that on the yeah",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3208.559,
    "end": 3225.488,
    "text": " okay this is a from a 2017 article by andre carpathy called software 2.0 and so he wrote i sometimes see people refer to neural networks as just another tool in your machine learning toolbox they have some pros and cons they work here and there",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3225.468,
    "end": 3228.192,
    "text": " and sometimes you can use them to win Kaggle competitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3229.193,
    "end": 3233.099,
    "text": "Unfortunately, this interpretation completely misses the forest for the trees.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3234.1,
    "end": 3237.325,
    "text": "Neural networks are not just another classifier.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3237.385,
    "end": 3240.91,
    "text": "They represent the beginning of a fundamental shift in how we develop software.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3241.11,
    "end": 3242.953,
    "text": "They are software 2.0.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3243.17,
    "end": 3256.327,
    "text": " And then a later section is, it turns out a large portion of real world problems have the property it is significantly easier to collect data or more generally identify desirable behavior than to explicitly write the program.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3256.828,
    "end": 3268.924,
    "text": "Because of this and many other benefits of software 2.0 programs that I will go into below, we are witnessing a massive transition across the industry where a lot of 1.0 code is being ported into 2.0 code.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3268.904,
    "end": 3294.591,
    "text": " software 1.0 is eating the world and now ai software 2.0 is eating software so how do you how do you think the kinds of analytical and empirical results that you are yielding might play into how we think about software engineering and designing different kinds of intelligent systems",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3296.123,
    "end": 3300.49,
    "text": " That's a very interesting question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3303.475,
    "end": 3304.296,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3307.701,
    "end": 3308.042,
    "text": "Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3309.304,
    "end": 3314.993,
    "text": "So I thought of neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3315.243,
    "end": 3324.515,
    "text": " I'm working on is largely different from the state-of-the-art deep learning machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3325.076,
    "end": 3340.997,
    "text": "We are more focusing on how the real brain works when it's doing some computation or making decision.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3341.517,
    "end": 3370.292,
    "text": " And yeah, although the large gap between the neural network model in computational neuroscience, which is too simple, and the state-of-the-art AIs, so we think that if we want to examine whether what we are doing is correct as the model of the brain,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3370.846,
    "end": 3391.487,
    "text": " then what we are doing it have meaning because so when we want to do some task for animals non-human animals only they can do is just a simple task and to my uh",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3391.467,
    "end": 3401.502,
    "text": " I believe, to my experiences, most of those tasks can be implemented by simple POMDP architecture, and some require some extension.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3402.303,
    "end": 3417.306,
    "text": "But basically, what we are dealing with here is sufficient to characterize what animals are doing in the system neuroscience sense.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3417.286,
    "end": 3438.636,
    "text": " Beyond that, we may be able to consider some more functioning neural networks through our strategy, but it is not soon, so we need many other validation before going to that step.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3440.911,
    "end": 3454.566,
    "text": " what does the reverse engineering as you have it in the github or as you imagine this developing what does it yield that a systems biology assessment wouldn't",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3457.465,
    "end": 3484.182,
    "text": " you mean you what is uh sorry can you rephrase that yeah what do we gain and learn from the reverse engineering uh right that just a circuit level dissection doesn't provide yeah so circuit level dissection basically provide us static information right information about architecture",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3484.415,
    "end": 3487.603,
    "text": " neural population at that point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3487.883,
    "end": 3491.111,
    "text": "So we see snapshot.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3491.131,
    "end": 3494.559,
    "text": "So what we are interested in is more prediction things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3495.0,
    "end": 3501.917,
    "text": "So through reverse engineering, we get a generative model, which has",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3502.319,
    "end": 3504.481,
    "text": " So it has learning ability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3504.842,
    "end": 3507.144,
    "text": "It itself is agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3507.164,
    "end": 3522.299,
    "text": "So instead of making connectomics or making some characteristic of static neural network, we can characterize the dynamics of neural network in terms of a generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3522.76,
    "end": 3527.905,
    "text": "It provides us a prediction of a long-term future",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3527.885,
    "end": 3535.797,
    "text": " after some learning, which is completely different from what conventional system neuroscience does.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3541.205,
    "end": 3543.85,
    "text": "Okay, another question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3543.87,
    "end": 3545.472,
    "text": "I put it in the chat.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3545.975,
    "end": 3565.688,
    "text": " So how do we reconcile the traditionally essentially error-free nature of Turing machines and digital computation with the more fundamentally probabilistic nature of neural and Bayesian systems?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3566.14,
    "end": 3572.717,
    "text": " Like part of the reason why the symbolic architectures are useful is because they can be used deterministically.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3572.758,
    "end": 3577.45,
    "text": "So are we seeking something that's deterministic or probabilistic?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3578.797,
    "end": 3580.419,
    "text": " Thank you for asking that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3581.02,
    "end": 3589.449,
    "text": "So it is related to our demonstration using others and hand digit numbers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3590.03,
    "end": 3596.938,
    "text": "So hand digit are your noisy stochastic information.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3597.459,
    "end": 3601.243,
    "text": "And once our network",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3602.962,
    "end": 3621.491,
    "text": " extract the hidden features from such a stochastic information, then hidden layer is becoming more deterministic in the sense that if we accumulate sufficient information, the POMDP model becomes either 0 or 1 state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3621.932,
    "end": 3629.323,
    "text": "So in that case, the state transition can be very deterministic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3629.303,
    "end": 3647.234,
    "text": " Of course, in general, we need to deal with the stochastic environment, but sometimes either likelihood matrix or transition matrix can be deterministic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3647.535,
    "end": 3653.826,
    "text": "Under that situation, our machine provides",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3654.515,
    "end": 3660.268,
    "text": " very accurate prediction about current state or future state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3661.23,
    "end": 3665.52,
    "text": "Moreover, although conventional",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3665.872,
    "end": 3670.057,
    "text": " naive Turing machine is very deterministic or discrete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3670.898,
    "end": 3681.75,
    "text": "But differential Turing machine incorporate the continuous dynamics and discrete Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3682.11,
    "end": 3686.615,
    "text": "So it is a Turing machine that is expressed by the differential equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3686.975,
    "end": 3693.803,
    "text": "And we extend that idea to the energy minimization in the sense that differential Turing machine",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3693.783,
    "end": 3698.716,
    "text": " of us is derived as the gradient descent on some energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3698.736,
    "end": 3707.759,
    "text": "So this dynamics is something between the discretized symbolic computation and the continuous dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3707.96,
    "end": 3710.847,
    "text": "And based on the condition",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3710.827,
    "end": 3722.584,
    "text": " just like inverse temperature parameter, we can shift that dynamics both to discretize symbolic computation and continuous dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3722.945,
    "end": 3725.909,
    "text": "That idea underlines what we are doing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3727.972,
    "end": 3728.353,
    "text": "Great.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3729.354,
    "end": 3729.915,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3731.397,
    "end": 3760.695,
    "text": " so in computer science theory the halting problem or turing's halting problem is the description that or the problem that the description of a computer program and the input you cannot determine whether the program will finish running or run forever so if we can make this kind of software 2.0 neural network representation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3760.81,
    "end": 3769.453,
    "text": " and or like this software question mark with the Bayesian interpretation as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3771.492,
    "end": 3776.018,
    "text": " Are they also bound by some of these constraints?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3776.839,
    "end": 3791.078,
    "text": "Or do you think there could be some angles of approach where problems that are not solvable in a procedural computational framework could be approached probabilistically and vice versa?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3792.56,
    "end": 3792.64,
    "text": "Hmm."
  },
  {
    "start": 3793.21,
    "end": 3795.473,
    "text": " Thank you for the question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3797.495,
    "end": 3805.985,
    "text": "The simple answer is I don't know the exact answer, so I try to make my opinion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3808.748,
    "end": 3816.957,
    "text": "First of all, although we consider some neural networks that perform Turing machine, it is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3817.747,
    "end": 3831.388,
    "text": " still in the framework of Turing machine or chart Turing tese, in the sense that what we are doing is just a computation based on algorithms, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3831.969,
    "end": 3840.603,
    "text": "Even though we are designing very realistic brain model comprising Hodgkin-Huxley spiking neural network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3840.623,
    "end": 3842.385,
    "text": "So it isn't just an algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3842.466,
    "end": 3845.931,
    "text": "So it is implemented in terms of Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3845.911,
    "end": 3865.81,
    "text": " so in that sense nothing different but uh and it it means that we do not solve the halting problem so our network can conclude whether this program is finished or not",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3865.975,
    "end": 3871.96,
    "text": " However, we can guess whether this is a good strategy or bad strategy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3872.601,
    "end": 3880.468,
    "text": "So one interesting difference from conventional Turing machine is that our neural network works in real time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3881.028,
    "end": 3890.096,
    "text": "So time is passing during computations, unlike a conventional Turing machine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3890.596,
    "end": 3895.881,
    "text": "So which means that if we select a bad strategy that does not stop,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3895.861,
    "end": 3906.82,
    "text": " during the meaningful time frame, then that synthetic animal die or eliminated through the natural selection stage.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3907.421,
    "end": 3921.565,
    "text": "So, survived species should have some meaningful function, meaningful algorithm that works in a realistic time frame.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3921.545,
    "end": 3931.15,
    "text": " so yeah that works as some debug or selection of algorithm so that is what i'm thinking",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3931.501,
    "end": 3958.015,
    "text": " yeah the connections with the neural darwinism are very interesting and also the open-endedness of successful life it's like those are the programs that haven't halted and you don't know whether the environment could change and they could all of a sudden die but what you know by observing their existence is that they haven't halted yet",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3958.45,
    "end": 3974.725,
    "text": " So it's like one sense they're able to carry out a computation in plausible time and on the other hand their persistence as a species, as a population, is still running.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3979.029,
    "end": 3981.931,
    "text": "So how will you proceed from here?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3982.412,
    "end": 3986.095,
    "text": "What directions do you plan to follow?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3987.728,
    "end": 3996.615,
    "text": " The plan involves some emergence of a new algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3997.879,
    "end": 3998.38,
    "text": "I mean,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3998.798,
    "end": 4000.159,
    "text": " Yeah, algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4000.179,
    "end": 4012.713,
    "text": "So now we have mean to implement represents arbitrary algorithm within the neural network architecture, which is corresponding to extended POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4013.534,
    "end": 4028.67,
    "text": "And what now we are interested in is whether the function that can the function those network can do is only limited to imitation of external world or",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4028.971,
    "end": 4034.776,
    "text": " it can do something different from the simple imitation of external world.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4035.377,
    "end": 4043.684,
    "text": "So if the internal algorithm itself has some new meaning, then that would be more interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4048.828,
    "end": 4058.657,
    "text": "What would that look like, or how will you explore that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4059.11,
    "end": 4064.437,
    "text": " Yeah, it's a difficult question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4064.457,
    "end": 4075.833,
    "text": "And what can happen within a fixed topology of a base graph or neural network or program information flow?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4076.814,
    "end": 4080.58,
    "text": "And then what is possible when the structure of the network changes?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4082.002,
    "end": 4088.07,
    "text": "Because there's a space of semantics that can be explored with the quantitative variation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4088.354,
    "end": 4096.844,
    "text": " just through the weight updating, and then there's another shell of possibility based upon topological change.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4098.105,
    "end": 4099.367,
    "text": "That's true, that's true.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4099.467,
    "end": 4116.387,
    "text": "So for example, now we do not involve structural plasticity within a lifetime, but the model selection done by update of ZIN involves the change of the size of network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4117.295,
    "end": 4128.957,
    "text": " By changing the gene, we can freely change the number of neurons involved in the network, or whether it is a two-layer or three-layer, like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4129.157,
    "end": 4137.733,
    "text": "So anything can be characterized by the gene, and its selection means the model selection from",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4139.283,
    "end": 4152.415,
    "text": " yeah very various model types so so i i mean yeah in principle we can consider many things but question is whether it is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4153.255,
    "end": 4153.916,
    "text": " plausible.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4154.917,
    "end": 4160.783,
    "text": "Intuitively, it requires very high computational cost.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4161.784,
    "end": 4168.69,
    "text": "It is something like a random search in a huge binary space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4168.791,
    "end": 4175.497,
    "text": "So as you may know, combinatorial optimization takes exponential time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4176.058,
    "end": 4180.242,
    "text": "So the question is, whether animals",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4180.222,
    "end": 4193.062,
    "text": " do that or animals know better searching algorithm or animal avoid to do that and have another solution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4193.563,
    "end": 4196.667,
    "text": "So that kind of things is very interesting to me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4199.231,
    "end": 4199.592,
    "text": "Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4200.393,
    "end": 4202.837,
    "text": "Is there anything else you want to add?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4205.399,
    "end": 4222.851,
    "text": " Although I'm not fully sure whether we can validate the relationship between Turing machine Bayesian inference and neural network using real data,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4222.831,
    "end": 4240.755,
    "text": " At least we can validate the double equivalence between neural network and Bayesian inference, and we can extend something to the evolutionary direction or Turing machine direction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4241.765,
    "end": 4246.75,
    "text": " As the neuroscience, I think I'm a theoretical neuroscientist.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4247.131,
    "end": 4255.56,
    "text": "So as the neuroscience, we need to do both development of the theory and validation of theory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4256.521,
    "end": 4258.603,
    "text": "They are interacting with each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4258.823,
    "end": 4263.028,
    "text": "So I'm very interested in both side.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4263.228,
    "end": 4268.734,
    "text": "And if you are interested in this sort of research, I'm very happy to working with you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4270.57,
    "end": 4271.111,
    "text": " Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4271.151,
    "end": 4272.373,
    "text": "Last question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4272.453,
    "end": 4278.041,
    "text": "Do you think there's a fourth player at that intersection?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4279.143,
    "end": 4283.41,
    "text": "Yeah, that's a very interesting question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4284.211,
    "end": 4293.625,
    "text": "So one thing which I'm used to be interested in is consciousness things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4293.942,
    "end": 4300.469,
    "text": " So yeah, many people nowadays are interested in consciousness and there are several theories of consciousness.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4300.95,
    "end": 4317.469,
    "text": "But as long as I know those three pillars, neural network, Bayesian inference, and Turing machine algorithm computation, do not involve consciousness idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4317.449,
    "end": 4330.698,
    "text": " So I want to ask some experts whether consciousness can be involved in those frameworks or it is different from those frameworks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4331.038,
    "end": 4334.145,
    "text": "Maybe that can be a false pillar.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4337.312,
    "end": 4337.753,
    "text": "Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4338.61,
    "end": 4341.594,
    "text": " Okay, thank you very much for joining.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4341.914,
    "end": 4351.187,
    "text": "It's exciting to see how things have developed with the empirical work and this new theoretical layer as well since a few years ago.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4352.168,
    "end": 4355.573,
    "text": "So looking forward to where you take it from here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4357.355,
    "end": 4357.936,
    "text": "Thank you very much.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4358.116,
    "end": 4358.517,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4359.017,
    "end": 4359.137,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4360.86,
    "end": 4361.04,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
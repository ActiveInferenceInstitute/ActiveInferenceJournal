SPEAKER_00:
Hello and welcome.

This is Active Inference guest stream number 86.1, Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Game World with Farouk Habibullahi and Moeen Khashnazad.

And thank you both for joining.

Really looking forward to the presentation and take it from here.


SPEAKER_04:
Thank you very much.


SPEAKER_02:
Yeah, thanks, Danielle.

I guess I'll just open up.

Hello, everyone.

Thanks for joining.

I hope this will be interesting and fun for everyone.

So today we are going to talk about a collaboration between Cortical Labs here in Melbourne and Monash University.

We will mostly emphasize on the details of the project as titled Biological Neurons Compete with Deep Reinforcement Learning.

in sample efficiency in a simulated game world but before going into details of the findings that we had in this study i'm just gonna talk briefly to our setup our system of in vitro neuronal cultures that we are interacting with and we are embedding them in simulated game environments to be able to do all these cool tests and uh well which has led us to these findings

so to start with um i'm gonna just tell you uh what were our motivations in cortical labs and what led us to where we are here today uh without getting too philosophical though uh we can say that we as humans need


SPEAKER_01:
they froze i'll just wait

their own shortfalls, if you would say.


SPEAKER_03:
For example... Yeah?

Oh, yeah, sure.

Okay.

So... Re-share.

So it stopped sharing?


SPEAKER_04:
Oh, no, no, that's okay.


SPEAKER_01:
So it stopped sharing, maybe?


UNKNOWN:
Yeah.


SPEAKER_02:
so yeah without getting too philosophical here uh we can say that us as humans we need intelligence in some form to be able to distill meaning from the phenomena around us but to be able to make more intelligent decisions by computing more data faster and with better outcomes has actually become one of the major goals and the holy grounds of machine learning and ai in the past decades

Now, in search for different ways of intelligent computing, one way for us to categorize all of our recent efforts could be to talk about these categories, like machine learning and AI, quantum computing or neuromorphic chips.

Obviously, there are other ways of looking at these and other tools that we've developed, and they are all the right tools for the right purposes.

They could all also have their own pros and cons, and what motivated us to look for new alternative ways of computing was that, for example, in machine learning and AI, we are facing challenges such as their high computational power, high power use, their catastrophic forgetfulness, or the fact that they require very large samples and very

long training times and they were also very susceptible to errors.

Or if you're talking about quantum computing,

Again, we have extremely high power usage.

These are very isolated from the external world and very prone to errors, and they are not very adaptable.

And also, if you're looking at neuromorphic computing and the whole field around it, there's also limited error correction capabilities.

variability between circuits and also they still require some sort of programming to some capacity to be able to do what we are designing them to do.

So basically in search for this alternative intelligent computing system that could actually solve some of these challenges that we have in these other categories, we thought that why not use the brain and the brain cells themselves

which are the most advanced intelligent systems that we know of.

And there's actually no need to prove that these brain cells are capable of doing very high order of computation.

So I'm not going to go into too many details of our setup because I think many of you might have heard or read about it.

But just to recap, firstly, what we do in cortical labs

to basically design and develop our system is that we use human iPSC cells, which are then differentiated to a neuronal phenotype.

For the studies that we're going to talk to you about today, we also had primary cortical neuronal cell cultures from mice.

and all of these cell cultures were then plated onto max one high density multi-electrode arrays which basically enables us to stimulate the neurons and also record from them at the same time from the same cultures and well we saw and we showed that these cultures could actually survive for even

more than six months on these multi-electrode arrays.

With our new products, new systems in this brand, we can go even way beyond six months and keep them healthy, keep them alive, keep them active, which gives us the capability of studying them for longer terms.

And for example, here is just a sample microscopy imaging, but you can see the dense interconnected population of these neurons that have been cultured and are active on the multi-electrode array.

So then we designed what we called initially Dishbrain, which is this closed-loop real-time system to interact with these neuronal cultures and embed them in the environments of the arcade game of Pong, which you can see here on the right-hand side.

This is a famous arcade game.

You have one paddle and basically the paddle is playing against the wall, trying to hit the ball accurately.

And the first published work that we explained the system and we showed, you know, basically all our findings of the system.

I think you've seen it and you've talked about it before, but it basically explores the system in extensive detail, which I will not bore you with here with all of those details, but because it's important to know what's this closed-loop system that we're dealing with, I'm just gonna briefly talk you through,

how it works and how it's designed.

Basically on each culture, which is integrated onto a multi-electrode array, we first predefined separate regions, one sensory region on top and two model regions on the bottom of this cartoon image that you can see on the left-hand side and the right-hand side.

First, using the sensory region, we feed information about these hypothetical balls, x and y-axis locations.

So imagine that game environment, you have the ball moving, you want to know where it is in that frame.

so the x-axis location is actually uh encoded by rate coding between 4 and 40 hertz depending on how close the ball is to the paddle on the x-axis and also we use place coding where where we encode where the ball is on the y-axis

by only simulating one of the eight available sensory electrodes within the sensory region.

And these eight electrodes are actually evenly distributed along the y-axis.

So whichever part or segment the ball falls into, that sensory electrode is activated.

Then we record from these two, again, predefined motor regions.

and compare their activities of all the channels in those regions, which are some of them are representative of the up movement, as you can see the arrows on this figure, and some of them are responsible for downward movement of the paddle.

And then obviously comparing the activities of these two sub regions, we decide where the culture is moving the paddle in what direction.

And then we need to close the loop by applying feedback to the same sensory region as we defined.

Again, this is another schematic, maybe to make things a bit more clear, but basically to explain our feedback loop, what happens is that if the ball is missed by the paddle, random unpredictable signal is applied for like four seconds at random sites in the sensory region.

And then if the ball is hit accurately, a predictable repeated stimulation for a shorter amount of time is applied to all of the sensory electrodes.

So we then again record from these cells on top of these silicon chips, both during their rest state spontaneous activity for comparison reasons and during their embodiment in their gameplay environment.

And I'm just going to briefly

touch upon something you might all have been thinking of now that I have explained our feedback loop here that after designing the system and finding actual evidence that this particular type of feedback loop is actually enabling the neurons to be learning the game or to be performing the game better as we go on in time.

You could think of this clearly as some evidence supporting

the free energy principle that we've been talking about for a long time now, the fact that these neurons don't like surprise, don't like the unpredictable sensory feedback that they receive when they miss the ball is what seems to be driving them towards playing the game better and towards performing better in the game.

So this was really cool evidence that we found that could be actually explained by FPP as we all know about.

We could definitely talk more in detail about this, but I'm just going to go to our basically newest findings.

This is again a quick illustration of the actual gameplay environments on one of our MEAs.

This is obviously the simulated environment.

You can see the gameplay in the corner top right and how all the channels that we have on the MEA

have these activities that we're recording from, and then we are comparing the two sides of the motor region to actually decide what the culture is doing in this gameplay environment.

Yeah, and then we evaluated their performance and whether these are learning using several metrics.

This is a very, very short summary,

uh of how the mice cells and human cell cultures which we call mcc and hcc compared to our media only controls to our in silico models and to our neurons at rest and they all show that they perform much better with more significant improvements in several metrics including their average rally legs

the percentage of aces or ace here is actually a bad thing.

So ace means a ball is missed after the initial serve.

So the lower the number of aces gets, the better they're performing.

And also they showed a significant improvement in the percentage of long rallies, which we defined as rallies that have more than three accurate hits.

Again, we've got much more nuanced results and details that we have figured out from these systems,

Here you can see the comparison between the first five minutes in green and the second 15 minutes of each gameplay session in orange.

And you can see that's basically the only significant improvement occurred in our human and mice cortical sets.

Now, in our obviously never-ending search for the underlying mechanisms that could drive this behavior,

In one of our more recent projects, we first wanted to compare the gameplay and rest sessions in terms of their connectivity.

So the connectivity between the different channels that we are recording from and how actually these connectivities are changing over time and whether the evolution in these connectivities can capture parts of this learning or this basically the

plasticity we assume that is happening during gameplay.

And Moin is going to take on from here, talk about our findings from this functional connectivity study, and then we're going to spend most of the time on the comparison with the reinforcement learning methods that we did in terms of their sample efficiency.


SPEAKER_04:
OK, great.

Hi, everyone, again.

Now to do this task that we've mentioned, first the challenge was that for every recording and for each channel we have a very very large time series of spiking activity.

So to make these sparse spiking patterns more interactable and easier to work with, first we decided to use a dimensionality reduction algorithm to

bring each channel time series to a lower dimensional space.

We use and compared different algorithms, including TISNI and isomap that you see here.

And as we can observe, interestingly, even in a very low dimensional space, such as a 2D space here, the first and second half of the

of each recordings were easily distinguished during gameplay, but not clearly in the rest.

These figures are four sample cultures, and the two colors represent all the channels in each half of the recording.

So we were motivated to continue this study in this lower dimension for higher interpretability and lower computational power use.

But then also having 1024 recorded channels still meant high computational complexity in our problem.

So we have this hypothesis that most probably a lot of these channels are highly correlated and we might be facing redundancy of information when looking at all the available channels in the recording.

So for that reason, we designed this pipeline to eventually study the functional connectivity of channels and compare between gameplay and rest.

Very briefly, we first used T-SNE algorithm to map all channels' recordings to the lower dimensions, which is selected to be three here.

And then to select a smaller set of channels, which are good representatives among all the recordings, we stacked up the low dimensional representations of all gameplay sessions and using attacker decomposition, followed by K-Metroid clustering algorithm,

We chose the centroids of the channel clusters in this low dimensional space as the mutual representative channels.

We tried different numbers of clusters and got the best results by choosing 30 clusters and higher numbers of clusters show no significant improvement in the results.

Finally, we built functional connectivity networks using these 30 channels as nodes and the Pearson correlation of their spiking activities as the weight of the edges between each pair of channels.

But we were also interested in tracking the connectivity dynamics and changes in time

as learning occurred in the culture.

So what we looked at here was the changes in correlation between pairs of notes from the first two minutes to the last two minutes of the recordings.

And what we see here on the left,

And right are the average networks from all of our recordings, which is around 300 gameplay sessions and about 150 rest sessions.

First of all, the extracted representative channels, which come from the sensor regions, are in green circles, and the ones from motor regions are in blue squares.

Now, the color of the edges indicates an increasing or decreasing correlation between nodes in red and black, respectively, and the edge thicknesses are related to the magnitude of these differences.

Simply by visualizing the data,

we saw significant differences between rest and gameplay where we observe many more positive edges or increase in correlation between channels.

And finally, to give you some more statistics of what we found happens as the network of channels evolves in time.

Here we can see the same five measures of network dynamics compared between the first and last two minutes in pink and green and also gameplay on top versus

rest session.

The gameplays are in pink and the rest are in green.

First, we have the average weight of the functional connectivity networks, which shows a significant increase during gameplay, but not at rest.

Next, the modularity index that decreases significantly, but only at gameplay.

That basically means that

the different clusters or modules of the network become more interconnected during the gameplay as time goes on.

The clustering coefficient also shows a significant increase only in gameplay, meaning that the neighbors of a given node have become more strongly connected between themselves.

And finally, characteristic path lengths or the average shortest path between any pair of nodes in the network decreases significantly after 18 minutes or gameplay recording.

Now that we have found even more evidence of the dynamics of these networks, which could potentially give rise to the emergent learning that is observed in their behavior, in

Then in another study, a Nexus study, we were also interested to do a direct comparison between the deep reinforcement learning algorithms and our normal cultures in the same game environment of Pong, but clearly only during the gameplay sessions.

We all know that RL algorithms that use deep neural networks in their structures have been developed to beat human experts in various games such as Atari games and have proven to be very strong players of these games given they have enough training.

But these algorithms suffer from different issues, such as modeling the rebar structure, sample inefficiency, reproducibility, as well as requiring high levels of computing power.

And these issues suggest that deep RL algorithms might actually differ fundamentally from human learning mechanisms and not be efficient to be a plausible model for human learning.

That's why we were motivated and interested to do this research and comparison.

In our work, we use three deep RL algorithms, DQN, A2C, and PPO, which are established to have good performance in Atari games and are robust tools in reinforcement tasks, particularly in games where the input is an image.

But apart from the

traditional gray scale image input of the game environment, we also take a step further aiming to account for potential adversaries resulting from the increased dimensionality of the image input to the deep RL algorithms or so-called Kerasov dimensionality in machine learning terminology.

We designed two additional types of input information.

first paddle and ball position design where we obtain a four-dimensional vector encoding the x and y coordinates of the ball and also the y coordinates of the paddles top and bottom

And also the ball position design where we divide the y-axis of the gameplay environment to eight equal segments, each mimicking one of the sensory electrodes in the biological cultures that place code the information about the ball's y-axis.

Then the ball's x-axis position is used as the second element of this input vector, similar to the rate-coded component of the stimulation applied to the biological cultures that we had.

This design is the most similar one to the cultures with the lowest dimension of the input vectors.

Also, we found that using the same number of training game episodes for biological neural networks and deep reinforcement learning algorithms in the image input design, the average Raleigh lens is highest for the human and mice cell cultures here in orange and blue, while they also achieve the lowest percentage of ACEs

and highest percentage of lung rallies.


SPEAKER_02:
May I just add here that maybe if it's not being clear, we are training all of these for the same number of episodes that the biological neuronal networks had to be trained on, which was on average 70.

game episodes in 20 minutes so just to make things as fair as possible we train these RL algorithms also only for 70 episodes and that's why we're talking about sample efficiency here because we're going to show you and we know that training them for a very longer longer time would result different obviously performances in these different algorithms yeah right thanks


SPEAKER_04:
Then comparing the first five minutes to the last 15 minutes of the game here in the third row in green and orange, we see that only the biological cultures show significant improvement in these metrics.

next we repeated all the analyses for the paddle and ball position input design which revealed the fact that oil algorithms performances were not suffering from the curse of dimensionality and all the previous conclusions still hold and finally very similar comparisons were observed when using the last input design which was the most similar to the biological cultures input information structure

Once again, the RL algorithms are outperformed or rarely match the polyurgical culture's performance at their best.

And lastly, we directly compared the relative improvement in the average value lengths of different groups over time, and these plots show that

In all the three designs, human cell cultures show the highest improvement, usually followed by the mouse cell cultures.

We also compared the relative improvement in the average bit counts or average rallies between the first five minutes and the last 15 minutes for all the sessions, as well as the post-hoc tests.

in each separate group for different batch sizes and different hyperparameters, as we can see in the first two rows.

And additionally, as Prue mentioned, we'll look at the mean total reward of the RL algorithms with only the image input design using the same hyperparameters.

uh for an extended training period of uh 11 000 uh game episodes in the last uh or third row here and it showed improved performance across an extended number of training episodes this is while as it anticipated 70 game episodes or 20 minutes of reporting

which is the same number used to train the biological cultures, but not sufficient for any of the algorithms.

So we could eventually conclude that the RL algorithms showed the lowest sample efficiency, having the lowest improvement in learning given the same number of training episodes provided for all the groups.

And this was even despite the fact that these methods receive a higher information density compared to cultures.

Finally, we also explored a biologically inspired algorithm implementing an active inference agent that uses counterfactual learning.

um improved learning rates observed in this biologically inspired learning protocol supports the potential of active inference agents to provide valuable insights into optimized learning strategies so and they can enhance our understanding of these dynamics however these active inference algorithms are um

still highly dependent on the chosen hyperparameters and require relatively higher power consumption compared to biological systems.

However, these results highlight the value of further exploring biologically inspired systems of learning and support the notion that synthetic biological intelligence systems

may offer a useful pathway to do this in the future.

So, yeah, I think I stop here and let us thank all of our internal and external collaborators, both in Monash University and Cortical Labs and in other institutes.

Thank you.

Thank you.


SPEAKER_00:
Awesome.

Okay, many places to jump in.

Well, thank you for sharing the results.

Maybe just while we're on this slide with the active inference, how was this agent constructed and how or was it the case that the neurons consider counterfactuals

or how did the counterfactual come into play?


SPEAKER_02:
So here, maybe this is actually a bit bad naming with you.

So CL here,

is actually our counterfactual learning agent.

And the number you can see in front of it is its horizon, which is one of the parameters we've adjusted.

Then for our search space, we have tried to mimic

as much as we could.

I should mention also here that Ashwin, Ashwin Paul was the person who has helped us with this analysis, worth mentioning again.

So for the search space, we try to mimic what we have in the biological cultures.

So the X axis

which location of the ball which we encode using the frequency values between 4 and 40 hertz in our dish brain uh were mapped into basically a space of 37 that would be 37 different values along x-axis

and our y-axis eight different values, which could be given as inputs.

So this would be our search space or environment.

And yeah, so we explored different values of the memory horizon.

We could obviously go even further, but we were trying to find a model that

kind of matches the biological cultures the best, because as you all know, we are thinking of active inference as a plausible and possible model of what is actually happening in our brain.

So we wanted to see if we can actually reconstruct a model that is very close to the performance of these cultures.

So the horizon memory of seven

basically having the seven frames prior to the current step was what gave us the closest results.


SPEAKER_04:
Yeah, I think, yeah, just to add, I think a recent active inference scheme is shown to be mathematically equivalent to a particular class of neural networks accompanied by some neuromodulations of synaptic plasticity

It uses counterfactual learning to accumulate a measure of risk over time based on feedback from the environment.

Subsequent work that validates this scheme experimentally using in vitro neural network has also appeared recently in Isamuah et al.

paper in 2022.


SPEAKER_02:
So yeah, just to, I think, more clearly answer your question, this was just a counterfactual learning model on its own.

We are not using the neurons or the input from neurons in the model.

This is just a standalone model on its own, trying to mimic the performance of the in vitro cultures.


SPEAKER_00:
cool like really interesting how the same kind of functional play or differing might arise from systems that are explicitly comparing alternatives and systems that at least are appearing to simulate counterfactuals yet somehow the instantaneous firing rate and the response comes to act as if

there is a counterfactual in the sense that the gradient is being compared and the gradient is more often than not going in the direction that is leading to less surprising sensory outcomes.


SPEAKER_03:
Exactly, yeah.


SPEAKER_00:
Okay, I'll ask a question from the live chat.

Jeff asks, is there a noise or density limit to the matrix culture interconnect?


SPEAKER_02:
So by matrix culture, I am assuming they mean the multi-electrode array.

Yeah.

So for the results that we have talked about today and most of our publications up to date, we're using a maximum high density multi-electrode array.

And maybe we didn't state this clearly.

We have 1024 channels that we are using for our design.

uh there are thousands more channels but uh we are only interacting with 1024 of them so maybe our limit we could say was 1024 channels but we are designing new systems now that are actually less dense so we are using uh 64 channels in our own

chips designed at home at Cortico Labs.

This will, yes, limit us in the sense that we don't have as many channels to be working with, but also we are hoping to be able to get more interpretable results and let's see, interact more efficiently with the cultures with the 64, 8x8 matrix.

They also asked about noise.

Yes, obviously, we are facing noise in any system, any experiment that we're doing.

We do have human induced noise.

We do have noise when obviously recording from these channels, but we have taken

uh as many measures as we could in order to clean uh the recorded uh signals first obviously we do filtering we do spike sorting uh we have tried to do template matching with the waveforms with the spike waveforms that we extract in order to make sure we are not picking up any false positives and then after all these steps done we are facing these


SPEAKER_00:
rather sparse spiking time series for each of the recorded channels cool um another question how would you characterize why catastrophic forgetting catastrophic forgetting happens in rl and how do you feel that active inference generative models and or the embodied dish brain averts that or does it ever do forgetting

or what kind of forgetting and how does the dynamics there differ from the reasons why those phenomena happen in RL?


SPEAKER_04:
Yeah, this is a great question and one part of the ongoing project that we are doing now.

Regarding this question, we are using lifelong learning or continual learning.

We are trying to

teach the DishBrain and train the DishBrain in different tasks, multiple tasks, instead of only one task so far on the plan game.

But we are planning to train the system on different tasks and see how they act in different consecutive tasks and without forgetting the previous ones.

um we are still doing the recordings we don't have the all the results yet but yeah as look as soon as we got the recordings we can start the analysis and see what happens so basically yeah we don't have a


SPEAKER_02:
evidence in our dish brain yet that catastrophic forgetting is not happening.

That is on our plan.

But just in terms of comparison to RL, like why catastrophic forgetting happens in RL, do you have any insights on that?


SPEAKER_04:
I'm not sure.

Maybe just one answer that I can say now is that

Because most of the RL algorithms use backpropagation in their optimizations.

And maybe because of, I'm not sure, but maybe because of the gradient vanishing and something like that happens in their training, we can relate that that's the catastrophe before getting to ANNs.

because of these matters.


SPEAKER_02:
Which is, again, exactly the point that is in contrast with what we have in an active inference model and the generative models that we use in active inference.


SPEAKER_04:
But when we have the multiple tasks in our framework, I think we can answer this question much better.


SPEAKER_02:
In terms of what's happening in biology and in the brain, yeah.


SPEAKER_00:
yeah just just to kind of highlight that when there's a training loss gradient in a neural network then different tasks can have performances that are at odds so even if you did epochs of training task a and task b you might end up following the gradient for task b so much that it deteriorates the performance on task a um so then

active inference angle would be well if you have a joint generative model and you're going down the free energy gradient on the performance of both at least you'd have the explicit frontier and you could explore like is there are these gradients at odds like will there be a trade-off with param tuning it one way or the other or is the architecture such that it could sustain

the both functions at once, and then that would just be how the generative model was parameterized, it wouldn't have invoked any kind of reward model, which especially as more and more tasks go into play, becomes more and more arbitrary to specify.


SPEAKER_02:
And a huge amount of hyperparameters to be tuning, which can be again at odds with each other when


SPEAKER_00:
we're trying training on more than one task yeah cool so about like the sense and the action cells that is spatially fixed on the chip so like how do those come to be associated

Is it just giving the sense region what and what is given to the action region such that it does self-organize into that kind of dynamic?


SPEAKER_02:
Yeah.

So again, the sensory and motor regions or action regions that we have were predefined during design of Dishbrain by us, by the founders of Dishbrain mostly, I would

need to acknowledge here again, Han, Andy and Brett.

But the challenge that we were facing obviously is that we do not have any ground truth or any map or layout that the neurons give us to say, okay, yes, we are responsible of sensing, we are responsible of making an action.

So, because these are all the same cell types.

So during the design of DishBrain, what we did was we did experiment trials and errors with different layouts.

pre-defining different regions having different roles.

And this is the design we came to just because of the fact that it showed better performance in the game.

The sensory region is the region that is receiving all the sensory input and feedback and all of that.

There is no input fed to the motor regions or the action regions.

They are only being recorded from.

And this is actually what we are searching for to be able to come closer to an answer of how these are self-organizing themselves to be acting the way they are acting.

One way of looking at it was looking at the functional connectivity networks, which we very briefly talked to you about here.

But in another project that is ongoing here, we've actually implemented a framework

which actually showed us that looking at different game episodes when the cultures are doing well or bad in the game and comparing an embedding or a lower dimensional representation of all the channels in these different episodes showed us that during the

better episodes when the game performance is high, there is actually a nice association or correlation between the activities of the sensory region

and then the motor regions which are responsible for up movement and the motor regions which are responsible for down movement so they actually do make these nicely separated clusters when this does not happen during a bad game performance which was actually very nice for us because it showed us maybe this pre-definition of regions was towards the right direction

And more importantly, for our future designs, we can actually use this framework beforehand to see what are the channels that are clustering together, self-organizing to be more associated together, more correlated to then define the different rules for them.

We also looked at the criticality metrics actually in these cultures when they are doing well in the game in terms of

Again, maybe they are self-organizing themselves in a specific matter that helps them process information better, have higher storage capacity, which is all that criticality is actually showing.

And in one of our recent studies, which is already published, we did see that during gameplay, the whole population is actually self-organizing near a critical point.

and the subregion separately as well, which is actually a hallmark of criticality being scale-free, right?

So we did see this scale-free pattern in their activity as well, the motor region separately and the sensory region separately, both also self-organized near criticality, which could be arising from a nice balance between excitation and inhibition.

But again, there's lots to explore and lots to look at still.


SPEAKER_00:
Wow, very cool with the multi-scale criticality as those are the kinds of empirical patterns that in the biological systems maximize the signal intensity and its kind of first principle statistical properties.

Like it wouldn't be using the dynamical system to the full gain if it were falling far on one side or another.

And then to show that even absent glia and all these other forcing or buffering elements that probably do play important roles in the criticality being maintained or useful organismally, but then even stripping out just the cells still has that pattern just for what they expect and prefer.


SPEAKER_02:
Yeah, exactly.

And we do hope in our future iterations of the system by the introduction of glial cells, astrocytes, we hope that this will actually be again helping and forcing how they are self-organizing your criticality to be performing the tasks, to be processing information, however we want to phrase that.


SPEAKER_00:
what kinds of like computational or or system specific challenges do you think this kind of system would be useful for in like shorter and longer terms but what what do you think and how does that work with plugging in and co-evolving all of these like very material details with the kinds of settings that are useful


SPEAKER_02:
Yeah, so we do have this here as a short list of what we are thinking of as our short-term goals, long-term goals.

So as what we have ongoing as ongoing projects would be like what Moin mentioned.

So we are as a first step, because if we are aiming to have these computational

units or biological intelligence units, we do want them to show memory, to have the ability to show this lifelong learning or continual learning, which we haven't fully shown yet.

So that's one of our ongoing projects to be able to train them in different tasks.

We are working on different applications in drug discovery, meaning that we can have

in vitro models of different diseases, such as epileptic models, for example, and then testing the drugs on our cultures to see not only how the activity patterns are changing, but also even compare between drugs and how effective each of them are.


SPEAKER_04:
Then... A recording capacity, actually,

with these projects.

Our previous recordings were just 20 minutes, but now we can have recordings up to a month, hours and hours.

And we have this ability to record from the cultures for, I don't know, up to 60 days.


SPEAKER_02:
Oh, way more than that.

We can go up to even 8, 9, 10 months and ongoing.


SPEAKER_04:
But definitely it's going to be a new challenge because we have a huge amount of data to analyze, but at least we have a good source of recordings.


SPEAKER_02:
Again, in terms of challenges, yes, just analyzing the data, the data storage for our purposes is going to be a challenge, but it's really nice to keep in mind that although it is a challenge for us to

maybe have all of these recordings and analyze them computationally.

But these neurons, these cultures are still using very, very tiny amounts of power.

So the power consumption by these computational units, if you want to call them, is still very efficient, which will hopefully in the future enable us to potentially use them as these closed loop

intelligent systems with these inputs and outputs, obviously, design.

And I think we can go wild in our imagination.

Maybe someday they can even replace silicon chips.


SPEAKER_00:
Interesting.

Well, OK.

One question, I guess, to that is, how do you think about the resourcing and the benchmarking

like you could take the thermometer and give the temperature reading that's kind of cool that it does have this fundamental physical energy balance too so that's very interesting but in terms of the computation so here you explore some of the sample efficiency like how can these different computing substrates be compared when it comes to like

thinking about speed, but it doesn't have a clock speed.

It's just unfolding, or it doesn't have a RAM memory or GPU measures for parallelization.

So what are promising or useful ways to understand what these systems can do?


SPEAKER_02:
Yeah.

So one way that we are exploring now, again, is that

If we are able to understand at least parts of the mechanisms that is driving this learning that's happening or the information processing that they're doing, one avenue could be actually to use those mechanisms, implement them in what we know as machine learning or AI methods, and actually help to improve those methods.

using these more biologically inspired and more intuitive mechanisms that we will be hopefully uncovering.

But in terms of just comparing, this was our first effort, comparison to RL in terms of sample efficiency.

It was challenging to make the comparison fair.

We did have suggestions, we did have criticism in terms of

As you saw, we showed results that if they are trained for longer durations of time, the RL methods, obviously they can even outperform our cultures at this state.

But we all know that the power consumption now is like a huge issue, right?

Even in these LLMs, the new hot topic that everyone's talking about.

If you just think about how much power they use, I think just comparing in terms of

pictures like this, their power consumption, that could give the biological cultures a huge edge and a huge motivation for us to follow up on our designs and just continue with what we're doing.

Do you have any ideas on how to compare?


SPEAKER_04:
At this stage, honestly,


SPEAKER_02:
I think, again, the continual learning, the lifetime learning will be able to show us the actual capabilities that these biological neural networks can have.

The memory, if they show the memory that you expect them to show, could be another point of comparison between whatever technology we have available now.

But yeah, lots to explore still.


SPEAKER_00:
Cool.

I guess to a different side of it, it says ethical considerations like ongoing.

I mean, even broadly, what does that look like or how does that really come into play and get stewarded in the research?


SPEAKER_02:
So when we started the research, like the start of all of that, first of all, one of our challenge

challenges was always just to put into words the phenomena that we are seeing, we are observing, that's what is happening.

Because if you're using terminologies like learning, cognition,

sentience and all of that, this could by itself be a huge area of discussion and even dispute in some cases.

But to start with, our reason to do this was that there is no

terminology like there is no set language to talk about predefined terminologies or language to talk about these systems or these phenomena so we decided to go with okay choose whatever best describes them but then as we moved on we realized that even the terminology that we use could have uh could have huge you know ethical um controversies or cause uh

people to think about lots of ethical considerations.

Are they sentient?

Do they feel?

And all of these.

So all of our analysis that we're doing, we are using different metrics, different measures of, for example, measures that have been introduced to quantify consciousness.

Like for example, the PCI metric, right?

We can have all of these implemented for our cultures, which we are doing now.

and then compare them against the findings, the state of the art research that are being done on consciousness, on cognitive abilities and all of that.

And so far to show that while these are capable computing units, they do not necessarily show or have the features that would qualify them as

conscious features or units.

So these are things, some parts of the things that we are doing, we are trying to address, to move in parallel with all of our computational studies in parallel, do all of these basically analysis as well to make sure that we are considerate enough of a phenomenon that we might not be aware of it happening.

underneath all of this but so far uh we are happy to say that we have published everything that we have found we have been very open with our findings and open to discussion open to everyone's opinions about whether there is an ethical angle that we have maybe missed wow very interesting like


SPEAKER_00:
how is that vector of different cognitive performance on different tasks associated with the different measures of consciousness and are those like maybe some implicitly or explicitly believe those are like the same vector or that they're orthogonal to each other and there's some other kind of

dimensionality where changing the the cognitive capacity like on any given task ends up not or even diminishing the performance in other ways so it's like a whole portfolio of evaluations because there's tasks but then these are other measures and yet it's like an opening up of these measures into the empirical

Whereas even before that, it was from the qualitative to the formal, but then all the equations on the piece of paper don't get to the sensor reading.

So that will be quite an exploration.

Okay, I'll just ask like one or any last questions.

Jeff writes, have you compared results from cultures from different brain structures, e.g.

prefrontal cortex versus visual cortex?


SPEAKER_02:
No, that we haven't done.

All of our research is based on these iPSC cells, which are differentiated into cortical cells.

Again, in our future iterations of the system, we are introducing glial cells, astrocytes and all of that.

And we are trying to also move to 3D organoid intelligent systems or organoids.


SPEAKER_00:
but we have not implemented or compared between different brain structures yet what what attributes or were there any other things that correlated with like the human and the mouse difference i mean do they are they different sized or do they have different branching characters uh so


SPEAKER_02:
I'm not sure about the size, to be honest.

I would love to have one of our biology team to be here to answer that, so I don't want to mislead anyone.

But in terms of their characteristics, we did study them in terms of their connectivity patterns, like the study we showed you about and the dynamics of the connectivity.

we do see, maybe we could call it slower plasticity mechanisms happening at a slower rate, which is what we saw in comparing mice and human cortical cells, which could explain parts of the difference in the performance that we saw.

But still, we shouldn't forget that even the mice cortical cells were performing quite well in the context of the tests that we defined for them.


SPEAKER_00:
Wow, very interesting.

Do you have any... Oh, go ahead.


SPEAKER_02:
They're quite smart.


SPEAKER_00:
Yeah.

Even when alone, it's like it reminds me of kind of a lot of the laboratory experiments in ant colonies, where even taking a Nesme out of its usual collective niche...

which is kind of it's analogous like other nestmate interactions that's like other neural connections um but with the more dynamic topology and then the glial niche is kind of like the nest architecture and then even stripping them of all of that and the pheromones and the tunnels and the nest mates interacting and then it's like still they could do the tea maze um okay one one last question from the live chat jeff wrote

Could you see scaling this from a 2D grid to a 3D matrix, emulating cortical columns and interconnecting them?


SPEAKER_02:
Yeah, I think I briefly mentioned that.

Yeah, but we are actually doing it right now.

We are working on organoids, which is basically the 3D structure that Jeff is talking about.

One major challenge of organoid studies is always...

Yes, exactly.

How do you record from the cells on the outer layer?

How do you make sure the inner layer cells are still alive and functioning?

And how do you efficiently record from them without causing any damage or major interruption to the activity of the organoid of the outer layers?

But I think those are challenges that everyone in this field

are trying to solve now are facing we are also trying the same but we are doing that we already have some recordings of our organoids which are comparing against our 2ds right now no results to show unfortunately that is very new but yes that is on our to-do list cool do you have any last comments or questions or like anything else you want to ask


SPEAKER_00:
no i think that was pretty much that yeah we wanted to discuss and talked about what could what could someone who's learning and applying active inference do in terms of their research and learning to kind of dovetail with these directions


SPEAKER_02:
to model them or biologically inspire.


SPEAKER_04:
Yeah, I don't know.

So maybe if you would like to... It's still an open question for us to understand and realize the learning mechanism behind these biological cultures.

And we need to find a way to somehow model the learning.

We were thinking about...


SPEAKER_02:
active inference for a long time as a biologically inspired algorithm but definitely there's a long path and lots of things that we can do yeah and for someone who is interested who's starting up I think although we have done this comparison that we showed you already but still we are looking for

for stronger evidence of these, for example, biological cultures actually following an active inference kind of framework, if you want to say.

So it could be a very nice starting point to actually model the exact game environment that these cultures, these biological cultures seem to be handling quite well in an active inference framework.


SPEAKER_04:
And later on, very soon for lifelong learning, actually, we need it.

mathematical analysis to model the system.


SPEAKER_02:
Yeah.

So to start with an environment or with a search space exactly similar to what the cultures are facing, trying to build a model upon that which matches the activity of our neurons of the biological cultures as we did.

But still, again, starting from that point on, they will have a lot to explore.

We're happy to

discuss with anyone who's interested to contact us via email cortical labs we have discord channel we have all the means of communication that you can think of so if they're interested to you know get a sneak peek or have some some sample data to work with definitely happy to stay in touch with them awesome thank you both so much it's really fascinating so

hope people enjoyed it and learnt it was great and good luck thank you very much for inviting us yeah great and pleasure yeah thanks a lot for the invitation hope we had some you know new stuff to show people some good takeaways for the audience thanks a lot thank you see ya bye
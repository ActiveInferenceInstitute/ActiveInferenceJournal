SPEAKER_01:
all right hello and welcome it's january 17th 2025 and we're in active inference guest stream 96.1 with ron edelman discussing designing intelligent experiences for trust so thank you ron for joining and looking forward to your presentation and the discussion


SPEAKER_00:
All right, so really happy to be here.

I must say that I'm only here because I randomly reached out to Carl and he was very kind and generous and he decided to, you know, entertain some conversations and

Those, it's been about two years.

Those conversations led to a couple of presentations, meeting some really incredible people involved in active inference.

And it really kind of set me on a new course, new ways of thinking.

And I wanted to share a more practical, perhaps implementation practitioner approach to what are the real world problems we're seeing with integrating the brilliant community of people who are trained in Bayesian statistics.

It's a completely different way to think about problem solving.

And if you start working with product managers, with designers, business folks,

they might have just a very different framework for thinking about how to create systems.

And I just want to kind of share an experience I had, which kind of changed my life.

And it was about music.

And I was really struggling to learn the Western style of music.

And I had a teacher who studied in India.

And so they have a different way, it's instead of writing things down in straight lines, and you

You sing your music first.

It's called conical.

So, um, quarter notes would be takademi, takademi, takademi, takademi, right?

Triplets would be takita, takita, takita, takita, takita.

And that system of learning accelerated me more than in a few weeks than years of like taking music lessons and

The reason that this, I think, is important is because it introduced me to the idea around having a trusted source of truth in collaboration.

All of a sudden, when I was playing music, we could all agree, you know, metronomes, timing, it opened up an entire language for me for how to interface with others, be completely creative.

have that freedom to collaborate, but it was also safe, right?

Like it was also, we could remain within the constraints of the system.

And that unlocked all kinds of incredible learning experiences for me.

And since then I've viewed information flowing through organizational networks, kind of through the lens of music.

Like what is our metronome that allows us to collaborate when we're building software?

when we're providing recommendations to customers, when we're troubleshooting operational and business processes.

And I think Carl enjoyed my analogy of synchronization in music when talking about these systems.

So he called it computational psychiatry, not words I would use, but that's how this journey kind of began for me.

So just a little bit about me.

I've written two books now.

The first one came out last year.

It's Unifying Business Data and Code.

It was incredible to write an O'Reilly book.

And that book deal got me a second book deal, The Language of Innovation, which is coming out in a couple of months.

And my life has been around meeting incredibly talented people and figuring how do we leverage

that creative brain power and passion and help organizations minimize their costs.

And I think that I view myself as kind of a bridge, a connector between all these different worlds.

And unifying business data and code was kind of my first exploration into how we formalize methodologies.

to get everyone to have, quote unquote, a metronome.

So how did we get to the idea of why is trust interesting?

Well, it started with a conversation with Carl, I think, at his theoretical neurobiology group, where I asked, what is the definition of understanding?

And he was like, well, it's quite simply

It's a shared model.

If you both have the same inputs, you should both have the same outputs.

And that for me was how I connected back to the idea of the metronome.

Because in reality, we don't have shared metronomes in a business.

In fact, I would say if you go to two different departments and you say, what does this word mean?

Usually they will give me a different definition.

And even if I ask two people within the same department, just as a pure consulting kind of approach on process optimization or architecture, explain to me how this process works, it is almost never described to me the same.

So this is a really hard problem because if we're going to be able to have AI make decisions and people can't even agree on what the operational reality is, how is an AI going to do that?

So the metronome for me is just a symbol of, it's always something I could trust.

It was just something I could like let go of holding on to what I think is right.

And I just, this is the truth and everyone else can align around it.

And

it was really interesting for me to approach the problem of like, how would you computationally define it?

And I will tell you, I do not have a computational definition of trust right now, but I am thinking about frameworks, approaches, ways to quantify it and engineer trust in systems.

And this to me is a very, very exciting problem.

So,

best analogy I had was when I was just, I was feeling a little tired and I started, you know, watching some TV and I pulled up the movie Edge of Tomorrow.

For those of you that don't know Edge of Tomorrow, it's Groundhog Day meets sci-fi.

Tom Cruise keeps dying over and over in this battle with aliens and each time he wakes up, he's learned from his previous experiences.

but there's one scene in the in the movie which really kind of helped me think of ways to articulate ways we can think about trust so in the beginning tom's like smiling he's laughing you know 100 feeling safety and control the general says you're going to go out into the battlefield and he's a tom cruise is a reporter you're going to report on the um

on the war, Tom's afraid of this, and so he threatens the general, if you send me into battle, I'm going to basically make you look bad.

And so then the general then says, I'm going to call you a deserter.

And he throws Tom not into a reporter role, but actually into this mechanized combat suit.

And Tom doesn't even know how to use the suit.

And eventually the sergeant says, jump or die.

And it's in this particular moment where it's like the opposite of trust kind of

felt clear to me, right?

He has no certainty of what's going to happen.

It's pure chaos.

He has no control over these events.

He has no safety because he's going to die, possibly.

And, you know, he has no understanding of like, what the heck is even happening to me.

And if we look back at how LLMs, and not everything is LLMs, but LLMs seem to be very hot and

But I think this is true for any kind of machine learning probabilistic system.

When we are in situations, granted it's not the edge of tomorrow with that situations, but when we are in a situation where we have to make decisions that could be life or death, it could be patient care.

It could be the life or death of a company if you make a wrong decision.

It could be the difference between layoffs, right?

When we use these probabilistic systems and we're leaving the deterministic approach, we have a clash because there's a lot of attention on fine-tuning and prompt engineering and trying to force these AI systems to behave deterministically.

But they are not deterministic.

And so I want to think about these four dimensions around certainty, control, safety, and understanding, right?

So when we have what's, if you're familiar with a paper called Toolformer, it introduces these kind of two terms, post-processing and pre-processing.

And so what OpenAI did when they first kind of

shaped the entire industry, right, that we now are thinking about.

It's you give us all of your, you know, put everything in a vector database that you send, you do your fine tuning, your prompt engineering, and, you know, our AI is going to be super, you know, quote unquote, super intelligence, general intelligence.

It's really good if that's your business model to have companies do that.

It's in their benefit.

It's in the cloud provider's benefit for a business to have really messy data.

And of course, preprocessing is hard because you can't possibly know in advance all the ways things will be used and all the rules.

And we're left with this kind of black box situation.

And it leaves this black box situation obviously is very ambiguous, right?

And when we're going shopping and we want to buy a cup of coffee, we don't want to have a probabilistic answer to what a cup of coffee costs.

We want to know this is how much it costs.

every single time when we're going to make our decision right and so how do we bring in these probabilistic and deterministic systems into user experiences another dimension around safety compliance is typically governance kind of things is

just sharing my experience first of all if you're into if you're a taxonomist or you know if you're if your knowledge graphs and linked data are your thing incredibly powerful great but i do want to share with people what the problems i see from in the real world companies that are trying to implement these approaches you have a very small group of people

who cannot possibly know everything in the company and every operation and define things.

So they quickly become bottlenecks for these top-down rules.

And the bottom-up requests become an overwhelming tsunami.

So people just go around it.

And now you have the worst of these worlds, right?

Because you're paying for enterprise software that you're also not using.

And you have all this shadow IT kind of things going on.

And when we, you know, thinking of biology, like when ants leave these trails, we typically have a really hard time leaving trails of how we solve tasks.

But if we can find a way to leave trails, these audit trails of how do we solve cognitive tasks?

We create the opportunity for kind of an organizational scale learning, right?

So the difference between the top-down approach and this complex adaptive systems approach is if you just can observe how people are solving tasks, you will have a fluid way to see what are the rules and processes people are following in the real world.

autonomously with freedom, et cetera, et cetera.

And how are the interactions with governance?

And this is what was the origin of what I was really interested in working on.

So I had a previous startup where we basically pulled all public data from GitHub,

If you're a developer and you did some code, we had data on you and your code and down to the commit level.

And we were trying to sell it to recruiters like, hey, do you want developers?

We can analyze people's codes and recommend developers for you to recruit.

But I had a problem.

I had two weeks for a big demo to like, you know, one of these

Fortune 50 companies and all our data was in a SQL database and I wanted to use AI and I didn't have time to learn lane chain and figure out how to convert things into a vector database.

I had to hack something together, which would enable me to use natural language to query a database.

And so I decided to take a really old school approach, just using regular expressions.

And that is what I'm trying to formalize into a protocol, which I call trust loop.

So before I go into anything, Daniel, are there any questions coming up or any comments, or should I proceed?

Not yet.

Proceed.

Thanks.

Very good.

So we want to take these kind of four dimensions and just think about them in terms of how we're designing systems.

where people can learn and then the whole organization can learn.

And where I think this could be really interesting is, you know, someone responded as I started posting about this, about the EEU AI Act.

How are you going to be compliant?

And I think this particular protocol has a lot of promise around compliance because everything is meant to be open and transparent and with trails.

And I'm trying to bridge them together, bringing some reinforcement learning principles, some active inference principles, but that is kind of what is guiding my approach.

So what does the architecture look like?

First of all, we need to integrate into a user interface.

And then we need to be able to have a registry of rules.

The approach I took is to have an LLM look at my rules and then recommend suggestions to dynamically improve with a feedback loop, any kind of rule management for me as an individual, but it could also be for a system if there are like compliance or regulatory requirements.

And then lastly is this kind of cognitive audit trail, right?

So I'm going to give you a very super simple example.

But let's say I, and this is literally the most simple example I could come up with.

But let's say for example, I have this chat interface and I wanna ask a question about my data, right?

Like let's say I have some dataset or database and I wanna calculate revenue.

So like if I were to post a question, you know, can you tell me what my revenue was, et cetera, et cetera.

and there are clearly some bugs, simply asking up here a feedback loop, what do you mean by revenue?

Do you mean net or gross?

Because these are wildly different calculations.

This is incredibly, incredibly simple.

It is merely identifying possible ambiguity in my language and then saying, hey, is this what you mean?

Or, hey, do you wanna use the revenue calculation from this particular database?

Now, why is this important?

Well, let's go back to this slide here, because I think the important question is why use AI at all?

if you can accomplish your task in the most efficient manner, that doesn't mean there needs to be AI involved.

So there was a really interesting metric from Versys recently, which I know everyone here is a fan of, that it beat,

OpenAI's mastermind gameplay versus 01, I think the 01 model.

And it said 01, I think it was like, took five hours and cost $200 and it was only 70% accurate.

And versus was 100% accurate, took five seconds and like cost 50 cents.

That's amazing.

I love that.

I love that.

And this is no disrespect to versus or anyone in AI.

But my immediate question was, hang on a second.

I'm pretty sure the algorithm to solve mastermind exists in a JavaScript file somewhere on GitHub.

And that means zero cost.

And instead of taking seconds, it's milliseconds.

And 100% accurate.

I actually checked it was Python code is the code I checked, but same thing, right?

And so it just kind of reinforced this idea around, hey, the whole industry is barreling at 100 miles an hour towards this post-processing paradigm.

But why?

Why should we send everything through AI?

How do we know what we should send through AI and what we should not send through AI?

And so that is what the trustee protocol is meant to do is we're trying to figure out how do we route which types of cognitive tasks already have rules and deterministic processes to solve and which ones don't exist in our rules registry.

So that that is where AI can shine, right?

Because if the reality is, as I've read, I haven't personally audited, that a single query for a, you know, kind of an enterprise, mid and enterprise company can cost a couple hundred dollars to process and hallucinates.

Is that really cost efficient, especially if there's a deterministic solution out there that works 100%?

And just to be clear on as much as I think AI is amazing at certain tasks, this just came across this morning.

So I just want to put it up here.

I didn't believe it when someone posted this on Blue Sky.

So I typed it in myself.

It's not the exact same wording, but I got the exact same result.

So then I'm like, okay, well, is this a Google thing?

Or maybe it's like, what if I give it to Claude?

So I gave this to Claude and asked, is this correct?

And Claude was like, yeah, no, your glass of water will not freeze at 27 degrees.

So I said, but isn't 27 degrees lower than 32 degrees?

And it said, oh, yeah, expertise has far less infrastructure.

think that like slapping on AI is going to be safe and reliable.

Which leads me to another question.

How do you even measure that?

So if you look at Chatbot Arena, right, if I have the name right, where you have two different LLM models and people are picking what they prefer, okay.

A model has a preferred answer, but different people in different contexts with different language might have what they call a preferred, you know, or a correct answer.

And a preference is not necessarily a deterministic correct answer.

Okay.

I'm going to pause there, Daniel.

Any, any questions or should I continue?


SPEAKER_01:
I'm writing some things down, a lot of cool points to discuss.

And if anyone watching live wants to write questions, they can go for it.

Continue, though.


SPEAKER_00:
Okay, good.

Yeah, I want this to be interactive and not me just rambling on.

So if you have anything you're curious about or not, so what we're hoping to do is to create a

very basic protocol protocol is a guarantee.

If you're gonna build these types of systems, just a simple scaffolding of here is how you do it.

And so what we need to do is have our intents predicted from our words.

We need to reflect them back to the user.

so that we can see the intent of the system.

I don't know if Riddhi is here, but she's the one that kind of like, she's amazing and she's the one that kind of introduced me to that idea around how do we see into the mind of our AI systems?

Like what is the belief of the system?

And I think if people are concerned about the EU AI Act,

That's exactly what they want to be able to have is how much uncertainty exists in your model and in your system, right?

Because going back to that chatbot arena, you might have preferences of model answers, but that is very different than how efficient the system is at completing a task.

Right?

Like your model might have a good answer, but is it what the person who's using the model needs to solve their problem?

Which is a much different benchmark, especially when you start calculating the cost of running some of these models.

Secondly, there's security issues, but being able to configure rules for these intents, what is deterministic?

and what is probabilistic, right?

And there's one thing that kind of shocked me.

Oops.

There's one thing that kind of shocked me that because I'm in presenter mode, I can't move things around.

But I don't know if you see here using Claude, the API, I ran prep my,

mouse wheels accidentally getting hit all the time okay so if running plot there's a um like a hundred percent success rate if i ran it through the user interface but if i run it through the api and this is the a benchmark of math word problems that openai released the accuracy dropped to 30 percent

Why is that?

Well, it's because the user interface when you use the web app has all kinds of fine tuning that you and I have no idea exist.

It's a black box to us.

And when we run it through an API, we don't have the benefit of that.

And the reason this is important is because when I, and I'm gonna stop sharing my screen now, when I told my friend this, who uses Cloud on a regular basis for data engineering tasks, he had no idea.

So he had no idea he had to do all this extra, because he's like, in the web UI it works.

Once I get it working in the web UI, why don't I just like, you know, automate it through APIs?

not necessarily bother even bothering to benchmark it because he trusted the web user interface.

So I think that we're really an immature kind of industry trying to apply AI to solve really hard problems that often have really significant safety compliance decision-making impacts.

And I don't think we really have a way to

understand how to design an engineer trust and so that's that's what i'm passionate about i just kind of wanted to you know share with the community some some of my research and you know i think there's there's i could ramble on here but i also don't know if uh i'd like to like kind of take a pause and see if anyone has any questions or comments


SPEAKER_01:
Awesome.

I will read a question from the live chat.

Anyone else can write, and then I can make some other comments and questions.

Okay.

Reconfigurability Trainer writes, Ron, did your work building the architecture include non-compliance related performance KPIs?

If so, did anything about that exercise surprise you?


SPEAKER_00:
Non-compliant paced performance.

other KPIs not related to system compliance.

Got it.

So if you haven't seen the startup Unify.ai, I was really surprised reading all the logos that use their service.

And I think they're really well supported by the Hugging Face leadership.

they distilled it down to three metrics, cost, speed, accuracy, and you can test any model.

And so I haven't used their service, but they seem to have a really good way for as simple as possible, as simple API calls for people that test various models, various prompt engineering, various fine tuning for benchmarking that has nothing to do with compliance.

I think though, there's a disconnect for me around, um, I've worked with some brilliant data scientists and AI engineers who, you know, working on their local host, local machine would have incredible results.

And then they think good luck getting that into production, uh, in the real world with like, you know, uh, outside of training, uh, I forget the term, but when you, when you have examples that are outside of your training set, um,

And so that just brings me back to this idea of benchmarking is absolutely important for models.

It will not get me to say anything else, but if you're not benchmarking in the real world, how those models are interacting operationally and what the impact to the business is,

Eventually someone in the finance department is going to be like, hey, we just invested $5 million into AI and we have to now have $3 million worth of extra HR costs to maintain all these complex new systems and our sales or whatever, our marketing, we don't have any real improvements.

I've been around to see those kinds of layoffs.

They're not fun.

And so I think that the discipline, the business discipline that a data scientist could benefit from is really starting to include thinking about benchmarks beyond simply model performance or something like that.


SPEAKER_01:
That's very interesting to kind of echo a piece back there.

Within a compliance testing sandbox skeleton or scaffold, then other kinds of measures and performances can be evaluated.

But to pursue those measures only,

when the vector of the performance isn't necessarily aligned with that of the compliance, then is going to lead to some kind of go-no-go with compliance down the road that might not be trivial.

But if it was trivial, then it would have been trivial in any order.

But in situations where it isn't trivial, then you want to make sure that you have the expressivity to test different compliance environments almost before you test the size of the data, for example.


SPEAKER_00:
You brought up a really important point.

So I'm not sure if people are familiar with the whole world models.

Kind of buzz.

Yann LeCun, if I'm pronouncing his name right.

He's been tweeting or posting on LinkedIn about it.

Fei-Fei, is it Fei-Fei Ling?

I'm not sure if I'm asking her name, my apologies.

At Stanford, she just raised $200 million for her startup on world models.

And there were a couple of things there that I just wanted to point out.

The first one is when we have a simulator, like a physics simulator,

right?

Like a car driving simulator.

And we can run that simulation a million times.

And then out of that million times, maybe we can do some like, you know, deep learning on it.

That is a fixed rule system, right?

There are, there are rules just like when we're doing compliance, our simulators have things they are physically bound to follow.

The rules are known, just like when you have alpha zero playing chess or whatever it is.

Games are fixed rule systems.

In the real world, in business, the rules are constantly changing.

Legislation changes.

The ways people operate changes.

HR rules change.

Language changes.

There's no fixed simulator you can have for a business.

And so the approach that we're proposing with trust the protocol is to say, okay, if we cannot impose some top down fixed rule stimulator, but we can create those, remember I had that image of the ants trails that if we can collect all these like ant trails for how things are actually getting done, what do people mean by these different words?

You can have a, and I don't know the actual technical term for this, but it's like a liquid simulator, fluid simulator.

It's revealing things that are happening by these quasi rules that are emergent.

And that hopefully can create world models, but not through a fixed system, right?

That was the first thing.

I wanted to say before I go on to the second thing, the answer is that, okay.

Um, the second thing was I was lucky enough to do this like little case study for, um, precision medicine department of Stanford.

Um, and they were doing a hackathon around people who had rare diseases.

So the precision medicine lab had very few patients, but they had lots and lots and lots of data.

I mean, genomic data, proteomic data, metabolic data, like if you name it, they were just barraging them with tests to create the most massive data sets they could on the few people they had with these very rare diseases.

And to prepare for that hackathon, I met with another Stanford team that was trying to create like the largest physics dataset in the world.

I'm wondering if they were actually part of this Feifei Lang world model thing.

But what was really interesting that they did is they collected all this data and then they ambiguated everything to numbers that you couldn't see what the columns were.

of the data.

And I thought that was a really interesting approach.

And so I asked the organizer of the hackathon why, you know, what was behind it.

And he goes, because we found when we have these data sets, with the words in the columns, the people who are experts, right, the trained PhDs in their field, regularly underperformed.

people who had no knowledge or expertise of that field, but they were purely machine learning experts.

And I thought that was I thought that was very interesting as well.

That doesn't tie into the, to the compliance thing, but just this, this idea around creating simulators and

removing even our own language and constraints in how we think, and just completely trusting good practices, those seem to be the highest benchmark approaches, at least talking to an expert in the field that created this massive physics dataset.


SPEAKER_01:
Yeah, in the design architecture you outlined and kind of connecting it to, let's say, employee nestmate ants leaving their traces through a sandbox or something like that, it made me think about the metacognitive environment that cognitive tasks are performed in.

Like, if the company were playing board games,

which model or method should be applied to each of the cognitive games.

And then the metacognitive task, you could bring out the heavy or expensive one too often, or you could bring out the lightweight, less performant one or narrow one too often.

So that's like sort of thinking too much or too little, these kinds of metacognitive or attentional tasks.

And it's almost like, okay, over the last...

years we've started to see the upswing of the capacity of the sort of like horsepower of the statistical probabilistic which has always existed as this dual thread along the more expert rule-based regular expression type systems

and then it's like okay so we know at the very least we have whether we think about it with system one system two or you know thinking symbolically and rule-based or thinking probabilistically and and numerically and then it's like now where and how within an enterprise auditing and compliance environment

that also creates not just the required operations that are that are needed, but also the training data, where it's like, what are the low hanging fruit that we can use the shorter ladder for?

What are the high hanging fruit that like no one has been able to hit?

What so what's our reach goal?

It's like, you know, because we're starting to see we have all these different affordances to deploy that have wildly different capacities and different costs.

You could outsource it to someone for thousands of dollars over weeks, or we could do this for hundreds of dollars over minutes.

And when you burn through hundreds in just hitting one query, those are critical and they really add up.


SPEAKER_00:
Yeah.

And that's kind of smiling because I was just like, you got it, right?

Like, that's exactly what we're trying to architect here is when do you use AI?

When not?

Which model?

Which approach?

When do you use an expert system?

It's a really interesting problem because if you're measuring against operational performance versus the model performance, you might have very simple hacks

which are way cheaper to implement, faster to implement, less IT complexity.

And I feel like that is completely off the radar right now, because everyone's like rushing towards this new paradigm.


SPEAKER_01:
Yeah, not every company is a frontier slash foundation model AI research lab.

so why use something more complex um i'll read another question from the chat and then anyone else can write more okay andrew peshea wrote nice talk all relevant points topics on trust regarding agent activity do you have an opinion on the uses of actor critic frameworks in the case of llms for output refinement fact checking etc yeah great question um


SPEAKER_00:
I think I heard, was the word expert in there or opinion?

I'm not sure what it was.

Let me just say, I just have an opinion.

I'm just going through, you know, it's funny because writing a book or getting published, now you get questions, oh, like I have some secret answer.

No, I'm figuring all this out just like everyone else.

And I can tell you what I've been doing

I'm interested in and where I think being an author is awesome and where it sucks.

So being where being an author is awesome is that you're completely removed from having to do any of the work, right?

Like I got to spend two years not coding and just thinking about all these ideas and talking to the most interesting people I could.

And, you know, what about this?

What about this?

Right.

And so when we talk about these agentic actor

paradigms, I can share with you what I've learned.

But it's not from a practitioner point of view, it is purely from reading, thinking, imagining, and working on my own little kind of agentic little experiments.

So it's clearly very, very valuable.

I think for me personally, just even like through the chat UI,

for me to write something and then, you know, Claude will be like, that's awesome, that's great.

And then you build it up and then like, I'll send it in a new chat and I'll say, okay, now you're gonna be the role of a critic.

Tell me all the things that's wrong, right?

Go through that process.

And Andrew Ng, actually a friend, a good friend, Yoshi, he pointed me to a recent Andrew Ng talk where he talks about this emergence of AI product managers.

And he talks about this multi-actor role.

Like you have the first brainstorm partner, then the critic, and then maybe the engineer who has to take the critic and figure out how to build it.

You know, and you can see now in the, like, in the LLM chain of thought reasoning approaches, they are definitely, you know, they're even saying like, hey, criticizing or reviewing, et cetera.

So it's clearly a pattern that I think has great value and is being engineered and implemented.

However, I recently had an experience with someone from the Active Inference Institute.

I don't know if Lance is here.

He's amazing.

I love him.

It's still incredibly helpful.

I recently worked on this paper.

It's the first time I've ever submitted anything for a talk, RLDM in Ireland, Reinforcement Learning Multidisciplinary Conference.

on cognitive world models, which is like, how do we take the world model approach introduced in 2018, where you create a simulation of a simulation?

Super cool.

And now if we had trust the protocol, instead of a fixed rule simulator, we take this like emergent kind of data.

And I tried to write the paper using kind of this actor approach, right?

I did my best.

And then I was like, Hey, before I submit to this conference, I've never written an academic paper before.

Don't have a PhD.

Totally intimidated.

Help me.

Great.

Okay.

Then I said, please critique it.

Great.

You know how to improve it.

And I tried to go through that multi-actor approach to using LLM, which again, I think is incredibly valuable, blah, blah, blah.

Um,

then i sent it to lance right someone who actually does um who i trust deeply right there's no better person other than carl i can think of right um and maybe ready right so i he took a look at the paper and immediately he's like it's missing boom boom boom he's like you should try asking chat gpt for help not knowing that i was using cloud like the whole time and like

It was really interesting for me to see that despite using that multi-actor approach, none of the things that Lance pointed out did I get feedback from those multi-actors.

And I think that's, how do you benchmark that, right?


SPEAKER_01:
Yeah, that's awesome.

One thought, and I've seen this also come up many, many times is like, even when a character or a persona prompt layer is added, so it's like, okay, you're an engineer or you're a product designer.

So it's kind of preempting trying to warm up or put these general purpose models into character.

In some sense, the response that gets pinged back is always going to have been like washed or brought back towards the center of gravity of this model that was trained on prior text.

And so sometimes it's like if somebody has three authentic ideas, one, two, three, and then they ask for it to be upgraded and it looks on a first pass like the professionalism has been brought up greatly.

But at this point, we all know that we can get those

kinds of responses looking professional back rapidly and maybe it converges to common idea three four five and if they had just sent the authentic one two three with the typos and all it's like we know that it could have we could have added more and then also um seeing what isn't there is very different than just augmenting what is there um

read a question from the chat on this theme a little bit okay reconfigurability trainer wrote what are your expectations on how legacy enterprise architecture will have to change to adapt to ai how much it change do firms have control over versus locked in by service providers


SPEAKER_00:
yeah i think this is the is like a critical critical point of what i'm interested in with this presentation today is there's okay there's two layers to this the first is a ton of working decades long investment in infrastructure already exists so the idea that you're going to take

decades worth of PDFs and data and dump it all into a vector database and do your prompt engineering and fine tuning and use that instead of all these systems and investment that work, that's just insane to me.

I'm sorry.

Until we can show that Google can know that 27 degrees is below 32 degrees,

right like these are not trusted systems they're probabilistic systems and the the amount of cost is just you know outstanding uh astounding sorry numbers and so the idea and i know it's like people want to learn these skills you get a big k jump if you have like lane chain and vector db and you're you know

And everyone's starting from scratch, so you can become an expert quickly.

But there's already a ton of infrastructure that works.

And so rather than fire hosing everything through AI, how do you control what is AI and what is not?

How does even the business

team decide what specific data is going to be good and useful and have an ROI to use that, right?

It just feels like a

It feels like businesses feel so much pressure to adopt this, that the C-level has so much pressure to show to shareholders, here's how we are going to have competitive edge and we're going to be a leader and adopt AI.

They haven't first really thought through some of these questions.

And so I think that leveraging existing infrastructure is

The number one thing companies can do is just like avoid AI when possible.

Right.

But there are things that AI can do that deterministic systems can't, which leads me to the other side of the coin here, which is I've been in organizations that have massive means like a million dollars a year in like these data catalog and data management systems.

And like you talk to the business leaders or the BI leaders and

and the data teams, and they're not even understanding each other, right?

The business people say, oh, do this, it seems so basic.

And then the data teams, they do all this stuff and it's not matching expectations.

And the data teams are like, we're not mind readers for what the business wants, right?

And so the problem there is that the existing infrastructure doesn't work.

That's where I think AI can shine.

That's where I think AI actually has an incredible advantage around when you have all this inefficiency in getting people to collaborate on IT infrastructure, which is deterministic.

That's where I think an LLM or something can unify people on language and approaches and intent, right?

So I think that's a really important question that you just highlighted.


SPEAKER_01:
I think these are some key issues that are to live with, not to defeat.

If we equate determinism with safety and probabilistic with danger, then

we're not even going to get out of bed in the morning.

Like in cognitive security, we have one joke that the safest airport is a parking lot.

You're not going to have any issues with the airplanes there, but the surfaces and the processes that are being actually done, they can't be deterministically known ahead of time.

Little motifs can.

And so obviously again, low hanging fruit,

But then understanding everything is a threshold of tolerance.

There's always a probabilistic element to safety in the real soft and fuzzy world.

And so understanding how all of these...

difference um on one hand like constraints and strictures and formal language like what is reasonable and all of this what is risky and then on the other hand that the the experiences in the actual like

what cognitive science is pointing at yes people talk about affordances and umvelts and all these different aspects that help us study across different systems but like just taking into people's day-to-days experiences and what is um how do we even accurately evaluate whether something is too risky or not or what these trade-offs are and um it's hard to have situational awareness it

I'm sure even without knowing at the enterprise scale, because it is like mapping intents, preferences, what capacities and all these different features.

Again, the kinds of things that in cognitive science and active inference we work on with the models, those are features in the world too.

And then how do we even know like where, when, how, what if, and


SPEAKER_00:
it seems like now is the time to be exploring in those directions yeah yeah and and we're almost at a at a time so i just want to throw out there i want i'm going to comment on that in a second but just before i forget um thank you all for anyone who's who's uh shared your time with me today um i definitely am looking to connect with people in the active inference community whether

It's just to say, hello, if I want to meet people, I want to learn, or if you're interested in, we're looking for a case study partner to basically try out the trusted protocol to this talk has now led to some conversations with investors.

So if you want to connect, please connect because I'm really excited about this, these problems.

And I would like to kind of leave on a, on a note,

around systems and what you alluded to, like this real-world emergent kind of situation we have in enterprises.

There was a Santa Fe Institute talk.

I don't know why I like ants.

I like ants.

So they were talking about the efficiencies of colonies and how well they can adapt and learn and solve problems.

And the one thing that struck me, the scientist's entire research came down to this one line.

And she said, the density of feedback loops in a system directly correlate to how quickly it can adapt and learn.

And so the reason that this protocol is called the trust loop protocol is because it's really a UX kind of interface via just an API to get that feedback across various systems between probabilistic and deterministic and blend them together.

And yeah, I don't know.

Is there anything else, Daniel, or how do you feel about


SPEAKER_01:
could make many comments on on ants i'm i'm um what comes to mind is um over the decades of ants research um people are trying to understand like how do on every continent except of course ironically antarctica how do these collective behavioral strategies work in all these different niches with different regularities so those are kind of like different business operating environments or like

market conditions or something like that and so and there can be multiple species different strategies in the same niche so people have tried to apply all kinds of different approaches and famously eo wilson

promoted the caste ergonomic hypothesis.

The idea that something like an Adam Smith 1776 type story, like free market in the colony, colonies that are better from this economic perspective, are...

maximizing the amount of income food you know and outcome reproductively per unit of biomass so that's one view um and that relates like optimal foraging a lot of other concepts and then as a contrasting voice to that sort of economic optimization researchers like my phd advisor deborah gordon highlighted

some things like you mentioned, such as the rate and the frequency of interactions, the types of interactions and learning and developmental trajectories, those capacities and those patterns

can dominate such that whatever the economics are they sort of are however they end up but they're being driven by these micro level interactions and so it's like i when thinking about businesses or other epistemic environments um just how two people can look at the same colony you know same institute same company whatever it happens to be and both

its viability economically can be seen from that like macroeconomic perspective otherwise it's not going to persist but also the the micro interactions are the i mean if if um if there's a great foraging plan but the pheromone evaporates too rapidly then it's not gonna work and so it's like

where are we at with digital cognitive or knowledge work or information spaces which is largely what we're talking about including how those are propagating into physical spaces logistics and supply and all these different areas physical spaces that have the information space

influencing them and then what are going to be the sort of macro structural properties of the system that no nestmate could could grapple with

And then how is that going to relate to the first person considerations like accessibility, felt sense of trust, cognitive overhead, and kind of like if those can connect, then there will be at least local harmony and success.

But there's way more ways for those not to connect, way more unsuccessful collective behavioral strategies than successful.

And so in the digital space where there's almost total degrees of freedom of design,

what does that interface look like?

And you're approaching it.

And also, what is that?

It's like interfaces all the way down, markup blankets all the way down.

What is that interface between the business intelligence and the API call?

Like all these different connectors.

But if even one critical one

is not how it needs to be or not as functional as it could be that's what Improvement should seek to Target and what proactive design should seek to prevent hopefully we'll see this is also new but I'm hoping if we can get a good case study


SPEAKER_00:
then we should be able to quantify some of these benchmarks around connectivity through interfaces, through different systems, across tasks, various models, between deterministic and probabilistic rules.

That's my next step.

I hope we can come up with some empirical evidence.


SPEAKER_01:
What zone would the case study be?

And then how can people contact you?


SPEAKER_00:
Oh, so...

My email address is Ron at Ron edelman.com.

Um, uh, and the, the zone, I don't know, like talking to the head of a hospital for one possible case study, um, a couple of incubators, uh, food industry.

I don't know.

Like there's all kinds of conversations until someone actually signs an agreement.

Right.

Who knows, nothing may come out of it, but I do think we have a compelling approach.

I think this is a really hard, expensive problem.

And if we can find someone who's willing to give us a chance just to like demonstrate as, as, you know, I guess, I understand it's an N of one, right.

But if we can demonstrate, um,

that we can show the, I wanna use your language, right?

The act itself of making these connections across various interfaces, just doing that can add value, right?

The digital equivalent of pheromones, right?

But that's, that's effectively what we're calling these cognitive auditable cognitive trails.

It's just meant to say like, we're leaving these little pheromone drops, creating a data set that then can be handed over to a data science team to analyze and hopefully come up with insights.

Um, yeah.


SPEAKER_01:
epic okay good luck ron thank you to all the live chatters thank you for presenting and looking forward to learning more thank you so much everyone bye
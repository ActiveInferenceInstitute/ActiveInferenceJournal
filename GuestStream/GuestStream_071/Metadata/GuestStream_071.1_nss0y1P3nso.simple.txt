SPEAKER_02:
Hello and welcome.

This is Active Inference Guest Stream 71.1 on February 22nd, 2024, 2-2-2-2-2-2.

And we're here with Ryan Smith, Rowan Hodson, and Mariska Mehta.

There will be a quick overview and then discussion on their recent work, The Empirical Status of Predictive Coding in Active Inference.

So thank you all, and let's hear about it.


SPEAKER_00:
Okay, so you just want me to jump in and go?

Yeah.

Okay, cool.

Well, thanks a ton, Daniel, for inviting us on.

It's fun to get to present some of this stuff and hopefully get it out to the broader community a little bit.

So I'm going to quickly just kind of walk through sections of the paper to orient people to generally what the point is and what we're trying to do.

And then at that point, I think the goal is just to kind of launch into discussion and see if we can kind of extract out some of the more interesting points that might be most relevant or interesting to the active insurance community.

So just to start out with here, so the paper, as Daniel said, is called The Empirical Status of Predictive Coding and Active Inference.

The main point of this is just that, as a lot of people in this community know, it's at its kind of beginning and for a long time, like the active inference is primarily, so the literature you'll see has been very focused on theoretical sort of conceptual work.

and simulation-based work, right?

So you'll see a lot of things out there that's kind of, you know, an active inference model of X, where X is just some interesting psychological phenomena or condition.

And usually that involves showing some, you know, fun, interesting simulations that are kind of potential computational explanations for, you know, whatever the specific phenomenon of interest is.

And that work is great.

And I think there's been a lot of developments there.

But, you know, at a certain point, you know, we can come up with as many theories as we want.

But without actually being able to test them scientifically, it's really hard to be able to say with any confidence that

These are kind of accurate stories of what the brain's doing and what are the kind of, yeah, again, like empirically supported theory, you know, which of all these different sort of models and simulations people are proposing sort of actually correspond to what the brain's doing and whether they can actually explain human behavior.

So what we were interested in doing is actually kind of taking a step back and looking at what the empirical studies that have been done using these sorts of modeling approaches, what they actually say, and how supportive the evidence actually is for, you know, the hypothesis that the brain is doing things like predictive coding and active inference.

So that's why it's called the empirical status, right?

As we're trying to say, okay, what is the current evidence and where is evidence missing, right?

So what should future work and future studies focus on to try to actually like fill in and answer questions and provide additional support or not for these theories as hypotheses for what the brain is doing.

Um, so to kind of walk you through sections here, so we focused, you know, so people use this umbrella term of like predictive processing, but that's really a fairly vague overarching kind of umbrella term.

And so we picked the primary, actually well-defined algorithms that are most prominent under the predictive processing umbrella, which is predictive coding and active inference.

These are well-defined mathematical algorithms that can be sufficiently precise to test empirically.

So the first section is more or less just an introduction that says,

more or less the sort of thing I just said about predictive coding and active inference being the kind of most prominent, precisely formulated algorithms that the brain might be doing and the importance of testing these things empirically, et cetera, et cetera.

And so I should say that most of the credit for this paper should really go to the first author, which is Rowan Hodson here, who I'm hoping will certainly say much more after I give this kind of brief overview.

But so what Rowan did is he started out

in each of these sections by defining the algorithm and laying out the associated mathematics in a way that was really very clear, I thought.

Of course, I'm bragging a little bit about a paper that I'm on, so take that with a grain of salt.

But I thought he did a very good job of laying it out, being explicit, showing the mathematics, but describing it in a way that was

what I think should be very accessible to people that don't have a ton of mathematical background.

And the idea was that we would just, you know, introduce this so that what we were talking about was clear, but the focus really isn't on, you know, just the mathematics.

We're just introducing a mathematics so it can be kind of more understandable what, you know, what the studies that we review are actually testing.

And so we just kind of walk through a lot of that.

And I'll come back to this figure in a second.

but the underlying mathematics of, for instance, here is a definition of the negative free energy when applied within predictive coding and just kind of how you get to some of the predictions and prediction errors and things like that that we actually use.

And this figure is a figure that took a long time to make and to try to be clear on, but we thought it ended up being pretty colorful and engaging.

It looks complicated, but we tried pretty hard to kind of walk people through step by step what this is hypothesizing.

But this is just a representation of one way that cortical columns in the brain might implement hierarchical predictive coding.

And this is a particular form of a particular predictive coding algorithm that allows first a certain type of temporal depth to it, where you have

essentially a higher level hidden cause that's being represented that actually predicts dynamics in the representations at the level below, as opposed to just predicting and trying to minimize prediction error with respect to some sort of static equivalent temporal scale representation at the level below.

So we try to kind of lay that out so that people have an understanding of what, again, what the kind of general basis and what the hypothesis is.

It's kind of important here because predictive coding at an algorithmic level is a different thing to test than trying to test a theory about the specific neural mechanisms

that implement predictive coding.

So I think it's a really important point.

This is that you can test a theory empirically by looking at the brain to try to test a hypothesis about a specific way the brain could be set up to do predictive coding.

But the brain could be set up in more than one way that would implement predictive coding.

So to be specific about testing something about the brain process, you need to specify

what algorithm you want to see, whether there's evidence that the brain is doing that.

More generally, though, you could also just test for evidence of predictive coding at the algorithmic level.

And you could do that behaviorally just by looking at what people detect perceptually and how that evolves over time, whether that is just consistent with the predictive coding algorithm.

So just the mathematics, not some more specific hypothesis about the way the brain is doing it through different patterns of connectivity.

Um, so then, uh, as I mentioned, we kind of go through, um, now just, uh, we lay out what they propose neural implementation is, or as I said, one commonly proposed implementation.

There's actually a couple that we talk about.

Um, and then here in this section, we have the empirical studies of predictive coding, and we review more or less what the evidence is.

for these different, for different kind of aspects or testing different predictions that predictive coding might make about what you would see either behaviorally or in the brain.

And I won't kind of go into it in detail here.

We can obviously talk about it when we get more into the discussion, but, you know, take-home is,

Really, there is a lot of indirect evidence, but there's a lot of studies that still kind of remain to be done to test more specific hypotheses about the neural implementation.

whether or not, for example, there actually are separate neurons in the brain that are responding just to prediction errors and other ones that are representing just predictions and things like that.

There really is more kind of explicit model fitting that needs to be done to test out predictions associated with quantitative simulations and what those predict based on the predictive coding algorithm.

One place where probably

The most kind of related evidence is not with predictive coding proper, but with hierarchical Gaussian filter, which is a different model than predictive coding.

But it has related.

It predicts dynamics that are related to predictive coding.

So for instance, it does have prediction errors in it, and it does have a precision weighting on those predictions.

And the updates can be modeled in relation to prediction errors.

One difference though is instead of being hierarchical in the sense of each level in a hierarchy representing different causes in the hierarchical Gaussian filter, higher levels represent predictions about the stability of the contingencies at the level below.

So basically, the highest level is representing something like how quickly it expects the predictive relationships between hidden states and outcomes, how quickly it expects those will change over time.

And so it can do this kind of dynamic precision weighting, depending on essentially how much you should trust the predictions you have in a given moment based on how you think those predictive relationships will change over time.

So it's not the same thing as predictive coding, but it's related.

And people have done neuroimaging studies, for example,

and looked at the explicit relationship between the prediction aerodynamics that this model, the predictions it makes about those prediction and prediction aerodynamics, and whether or not you can find neural responses that look like they match those predicted simulated time courses.

And there haven't been a few studies that do support that specific brain regions are encoding prediction errors and predictions and things like that.

and different precisions.

So it's not the same thing as testing predictive coding directly, but it is testing and finding evidence for a neural basis of encoding of precision-weighted prediction errors, which again, overlaps in interesting ways as kind of again, indirect, but important evidence.

So then after we go through that, then we switch to talking about active inference.

And again, kind of similar structure.

First, Rowan introduced the mathematics associated with the Active Inference Framework.

And for people that are watching the Active Inference Institute program regularly should be aware that Active Inference nowadays has evolved into

kind of also being a broader umbrella term for multiple different specific mathematical formalisms.

And so to be clear, this one that we go through here is just the kind of standard initial partially observable Markov decision process framework.

So this is a specific model of decision-making based on a particular explore exploit trade-off in a model-based way.

where instead of just using reward as a cost function or an objective function, it's based on the expected free energy, which is just this combination of expected reward and expected information gain.

So it's talking about a specific algorithm and the original algorithm under active inference.

But now there's lots of different variants of it.

So we're just going through the basic one.

So again, we talk about how it's related to predictive coding and what's different about it from predictive coding.

And then in this case, what we do is there is actually quite a bit more in active inference, at least this version of it.

There are a number of empirical studies that have directly fit active inference models to decision making behavior.

Several of these come out of our lab, actually, but there are some other ones that haven't.

And so we reviewed each of these in detail, kind of laid out how the models were set up to model specific decision-making or perceptual decision-making tasks.

You know, so for instance, here we kind of introduced the model for a specific task we used to model interoception, so cardiac interoception.

And then, yeah, here you can just see this is just kind of a standard, you know, version of a depiction of the active inference model.

And then, let's see, you know, again,

These are sort of standard things you see in lots of active inference papers, right?

So just describing and laying out the mathematics of the general update equations and the overall algorithm, and then a very kind of coarse-grained, generic sort of representation of one way that the brain might implement those algorithms, kind of inspired by cortical column structure.

But so anyway, you can see this is a model that we used to, an active inference model we set up to model a three-armed bandit task that we used in substance use disorders to show that.

And in this one, what we were able to show, for example, was that substance users showed slower learning rates from negative outcomes than healthy individuals.

So you could see it's much more based on trying to answer psychiatric questions about clinical groups, because our lab is focused on computational psychiatry.

in particular.

So we walked through that.

We walked through a task that we modeled using an approach avoidance conflict paradigm.

And that model also was able to show differences in decision uncertainty and what we called emotion conflict.

It's a type of preference precision effectively that also differed in affective disorders and substance use disorders from healthy controls in an interesting way.

There was also a couple others.

One really cool study that was not by us, but by Gidgson was the first author.

And they took, for example, a set of publicly available data sets for what's called a two-step task, which is just a common task used in reinforcement learning models.

And in that task, there's a standard reinforcement learning model that's used that's essentially kind of a mixture-based model of what a model-free reinforcement learning model versus a model-based reinforcement learning model would predict.

And what that task was designed to do initially was to use this kind of mixture reinforcement learning model to test individual differences in how kind of model-based versus model-free people are.

We can talk about that a little more if people listening aren't totally familiar with the model-based versus model-free distinction.

But what they did, which was really cool, was they took that data and fit an active inference model to it and then compared the ability of active inference to explain behavior compared to the standard reinforcement learning model I just mentioned.

And what they were able to show is that the active inference model

in two of the four datasets was actually a little bit better at explaining the behavioral patterns than the reinforcement learning model was.

In the other two datasets, the models were about equivalent.

So that was probably the most direct evidence that has been shown.

And this was one of the major points I think that we wanted to make is that even though it's the case that us and other people have started to do actual empirical studies trying to fit active inference models to data,

The main purpose so far and what people have done really hasn't been to provide unique evidence for active inference.

It's just been using active inference as a way to try to identify individual differences or group differences in a psychiatric research context.

So one thing that is still really needed

aside from this one Gitchin study I mentioned, is more studies where people actually take behavioral data or collect behavioral data on decision making tasks and fit a bunch of fit different active inference models to it and also fit a bunch of sort of competing models like reinforcement learning models to it.

to really show that the behavioral patterns that active inference can do a better job of predicting that behavior than competing models can.

because that's really the only way.

It's not enough basically to show that active inference models fit well.

You need to show that active inference models fit better than competing models, because that's how you'd really show that active inference is more likely what the brain is doing when people are making decisions.

And so, you know, the overarching conclusion is that, you know, the evidence is promising, right?

I mean, there's nothing that suggests that the brain isn't doing active inference, and certainly it's consistent with people using active inference, but a lot more still needs to be done to show that active inference is a better explanation for what people do than simpler competing models.

So that's, you know, just as a kind of broad, brief overview of the general structure of the paper and the message we were trying to get across.

So, you know, we can certainly talk about it more just kind of interactively now.

There's lots that I didn't talk about, but hopefully that starts as just kind of an initial launching point for discussion.

So I guess I'll just stop there.


SPEAKER_02:
Thank you.

Awesome.

Perhaps the other authors could give their first takes, introduce themselves.


SPEAKER_03:
Sure, I'll start.

Yeah, I'm Rowan.

I'm a PhD student.

Ryan's my supervisor.

I'm at the Royal Institute for Brain Research.

I did my master's at the University of Cape Town under Jonathan Schock, Mark Soames, and Ryan as well.

um i think yeah just to talk a bit more generally about this paper this this paper started off as um a book chapter actually which was only going to be focused on active inference and sort of especially uh particularly parallel states of actual difference and in the process of writing this um i found myself uh

what needing almost to write about predictive coding.

I think whenever we can, we should take the opportunity to really present this in a very methodical and complete way.

And I think sometimes, I remember when I was first learning about active inference, it's very hard to dive into the middle.

And I think that's how it sort of became, it morphed from just being focused on active inference to being focused on active inference and predictive coding,

predictive coding acts as a sort of foundation for a large aspect of active inference.

And I think that's how that sort of happened.

And then of course, it morphed a little bit from just talking about predictive studies to also talking about the sort of

background mathematics and the theoretical foundations and so i i think this is the sort of field where um it's a noble cause to try whenever we can get the opportunity to try present a quite a thorough review of all aspects of this because it's a difficult field to learn uh it can be very confusing so yeah i really hope that this review i'm on top of uh presenting the empirical side of things also acts as uh

a reasonably intuitive way to look at the foundational theory of predictive coding actually


SPEAKER_00:
Yeah, and one thing I will say just to kind of add to that is that for people that have more of a kind of broad conceptual or philosophical interest in active inference as opposed to more of the kind of detailed mathematical modeling sort of understanding, often what I've seen in the past is that predictive coding and active inference get a little bit kind of conflated, right?

So people think that active inference is somehow just predictive coding plus

motor control or plus decision making or something.

And so making it clear that's actually really not the case, I think, is important.

That predictive coding is actually an entirely different algorithm.

It has a different generative model associated with it.

It uses continuous state spaces, whereas active inference uses discrete state spaces and discrete time.

To really lay out, I think Rowan did very well with Mariska's help also, was to kind of

state clearly what the connection is between predictive coding and active inference, but that they are not the same thing and they're not even directly connected to each other.

So to make that a little bit more clear and more intuitive maybe.

So that was just another thing I wanted to say that I thought that they did pretty well and is another kind of maybe useful take home point from the paper.


SPEAKER_04:
Hi, I'm Mariska.

Um, so I'm also like Rowan, a second year PhD student at the Laureate Institute.

And Ryan is my supervisor.

My background's more, um, neuroscience and psychology.

So this paper was like a really cool and challenging first step in my PhD, where I come from a more empirical background.

So kind of working on this paper was, uh, kind of going, not taking baby step, but deep dive into the active inference and the predictive coding literature.

And again,

I'm the people that Ryan was talking about who had more interest on the empirical side and kind of going through this journey like really helped me like walk step by step into like different aspects of the just like predictive coding or active inference literature that you should take into consideration and think about really like the algorithmic side the neural side of things and kind of like connecting those three levels how we

how different people kind of just think about from one perspective, but what really is needed is connecting all those three things.

And I felt like I was playing a supportive role in this paper, but this really was a great learning journey for me.


SPEAKER_02:
Okay.

Why not to Mr. Empirical Dean?


SPEAKER_01:
So

my interest in this was um anytime i see a title where there's a minimum of two uh ideas being brought together as the unit of analysis i'm i'm always curious is it okay if i just because i have one question that's sort of an unpacking question around predictive coding and one question related to active inference at the conclusion would it be okay if i read a little section of the paper

just start just just start just there's a little microphone thing can you just repeat again okay so i'm i'm on page eight of the paper and the section that i just like to get a little bit of unpacking around is

Thus fitting predictive coding models to responses on perception tasks and testing with quantitative predictions from simulations remains an important direction for work going forward.

This is crucial because simulated dynamics and predictive coding models can make predictions that are not always straightforward a priori and that depend

like this depend on the specific hypotheses built into a formal model.

Ryan kind of touched on that in his introduction.

For example, the specific mathematical form assumed for the mapping between levels in a hierarchy, the direction in which neural activity is assumed to represent a particular posterior estimate.

Often, this is the thing I was hoping somebody could unpack for me, often sequential dependencies

between task trials and patterns and dynamics can be missed in summary statistics that average over trials.

So models are necessary to predict and test for the presence of those precise dynamics.

I was hoping maybe somebody could put some color on that process and what it means.


SPEAKER_00:
um sure i can take a stab at that and um i mean you know rowan or mariska can as well but i'll um i i'm pretty sure that was something that i wrote so maybe that's for uh for me to be the one to at least take a start on it um so yeah i mean that in in so in the field of computational psychiatry more broadly right so not just active inference but um you know the goal is really just to

take a set of task behavior.

So we can ask people to do, say, some kind of sequential decision-making task that involves usually explore-exploit dynamics of some form.

So say one common one would be I give people a three-armed bandit task.

So I give people, they play some little game, and there's three different options, and they don't know what the reward probabilities are.

So, and then they can just trial and error in the beginning, they can say, choose option three and they can see whether they win or lose.

And if they win, you know, let's say they probably stick with option three again.

But if they lose, maybe they try option two and then say, if they lose again, they could try option three again, or they could try option one and so on and so forth, right?

And so predictive and active inference model in this case, I'm just using this as a kind of a simpler starting example, an active inference model,

under different parameter settings in that model, right?

Under different learning rates or under different directed exploration drives or however you want to parameterize it would make different predictions about what that sequence of choices would be.

And there's a sequential dependence because the choices on each trial are not independent, right?

So what a person chooses on trial two

depends very much on the outcome they saw at trial one.

And what they see on option three, what they choose on option three will depend on both what they saw after choice one and choice two.

So if I do this standard, right, empirical thing that you would, like analytic thing that you would do,

for a task like that that's not model-based is you might just have some summary statistics, right?

You might do something like a common thing to do would just be to count the number of times they switched what option they chose after they lost.

or how many times they stuck with the same choice after they lost.

So we call those win-stays or win-switches.

Or same thing, if you lose, did you stick with the same one or did you switch?

So it's win-stay, win-shift, lose-stay, lose-shift.

You could just kind of count the proportions of those.

So you could see something that maybe how often people lose switch, for example, might tell you something kind of like their learning rate for losses.

They might learn more quickly, update their beliefs more after a loss.

if they're the kind of person that switches away.

They'd be more quick to assume, okay, if this option led to a loss this time, that's probably not a good option anymore, so I'm going to switch to something else.

Whereas if a person has a slow learning rate for losses, then maybe they'll stick to the same one a couple of times before they decide, okay, this is definitely a bad option.

They have to see a few losses in a row.

but the point is is that if all you do is kind of average over trials and get some kind of summary statistic about how many times people you know win shifted or lose state etc um what you're not going to get is anything about the pattern in the dynamics

Right.

So if they like, for instance, if they like lose shifted in the early first few choices.

Right.

Like that tells you something pretty different than if they lose stay or lose shift on some of those later choices.

Right.

The early choices might be driven much more by like exploratory drives by information seeking, whereas on later trials that much might be much better explained by differences in learning rates.

So fitting models to behavior, which again just means you have the actual behavior and then you see what the model predicts under a bunch of different parameter values.

and you try to find the parameter values for a person that best reproduces their behavior.

The point is that we've done that kind of thing several times now for active inference, which you can do just by taking patterns of decisions that people make on games.

But you can do that same thing for predictive coding.

You can give people perceptual decision-making tasks where people say, okay, this is what I perceive this time.

Okay, now this is what I perceive this time, et cetera.

And those are also going to have sequential dependencies because people are going to build up prior expectations about what they're going to see next.

And so predictive coding models, again, under certain parameter values might say, OK, well, if they saw the same thing the last five times, they're going to probably a lot more biased.

They're probably going to have a much more precise prior that they're going to see that again.

So the probability they're going to say they saw that again, even if you showed something different, would be higher.

And so the point is that it's not always the case that a predictive coding model is going to predict some trajectory of choices that's going to be really obvious without actually fitting the model.

If, for instance, the person has no tendency whatsoever to be more likely to say they saw one thing just because they saw it a bunch more times in the past, that wouldn't really be very consistent with the idea that people are using predictive coding.

It gets even a little trickier than that because predictive coding is based on continuous state space.

So it's not really even something like a person would be just choosing, I saw this, I saw that.

It'd be much more something like them kind of continuously turning a dial or something as they see something get brighter or dimmer or like motion in a direction or another direction.

And because the prediction error equations are set up in this kind of continuous way,

they also have these kind of like oscillatory dynamics to them right so it's like not like you get error and then it drops as the thing resolves but it kind of oscillates up and down a little bit right and so that would also predict pretty kind of sometimes funny like not definitely not something you could just predict a priori like exactly how someone's gonna like turn a dial um so that's that's the kind of thing i mean okay uh in my in very simple terms daniel and i've had some conversations around if you're in a and if you're in a


SPEAKER_01:
a situation where your focus or your concentration is on the next move or if you can look and see the entire space and it's taken all moves perspective so my asking you the question was to get out of the sort of model piece of it and that so what what do people actually do and i think your the question around the oscillate their dance around the oscillation sort of speaks to that it's it's difficult when you're focusing on

that next part of the sequence to be able to take in the entirety of the and vice versa if you're focusing on the averages maybe you're not able to pick up on the nuances of the next step can i ask one more question sure or would the other authors like to continue to fill in on that first answer


SPEAKER_03:
I think Ryan explained that very well.

I think this is a general... I'll talk about it in a way that is referring to predictive coding.

But something I'm very interested in active inference is along the lines of differentiating, for example, between a model that uses reinforcement learning and a model that uses active inference.

And yeah, using summary statistics, sometimes it can be hard to actually see

differential behavior.

This is what we care about when we are looking at comparing different models.

Do they actually predict differential behavior?

And that differential behavior can be difficult to capture in summary statistics sometimes.

So in general, I think in the world of model fitting, this is just a common theme.

And as Ryan said, while we're doing this active reference, there are limited applications of that methodology in predictive coding.


SPEAKER_01:
OK, Mariska, so I'll ask this.

The other thing, this was in the conclusion of the paper.

And again, I'll just read it.

In contrast to predictive coding research, in contrast, predictive coding research can be traced back nearly four decades and make specific predictions that can be investigated across a variety of fields.

So there's a deeper well of priors there.

I assume that's what that means.

It will be an important direction for future research to further develop the neural process theory underlying active inference and allow for precise implementation level

as opposed to simply algorithmic level predictions about brain and behavior.

I completely agree.

Until that time, confidence in active inference as a neural model of decision making should remain tentative.

But this is the part that I thought was really interesting.

He wrote, another important limitation associated with the current active inference scheme is that of scalability.

which constrains the phenomena that can be examined in empirical studies.

That is, while these models work well in the context of simple tasks, become less tractable if applied to many real world problems, thigh dimension sets of states observations and policies.

My question was, is the idea of strategy, because active inference is known kind of as a basis for strategizing behavior,

Is the question around scalability one that ties into the idea that even when humans are trying to strategize, how far out they can generalize their strategy is a difficult measure to get precise, given that most contexts are dynamic and changing?


SPEAKER_00:
So there's several kind of related things here.

So the tractability issue kind of comes in two different flavors.

So one flavor is tractability with respect to our ability to use these models to even simulate behavior in contexts where the kind of space of options and how far in the future people are planning, when that gets big.

The other question is more about psychological plausibility, which is that even if I can get an active inference model to simulate 10 steps ahead and where there ends up being 30 or 40 different combinations of 10 moves or something like that, it's not really very plausible that humans are really doing that, that the brain is really doing that.

And standard computer, it's going to take

I don't know like Rowan you know Rowan has these uh fun planning models that he has this paper that um you know we uh I think are pretty close to submitting there's a pre-print of an earlier version that's out um where you know the thing does like plan right a bunch of different possible paths that it can take to you know find you know different sorts of rewards um and uh

It takes hours to run simulations on a computer to do something that, however humans are doing it, they can do it just in a minute or something.

So it's just not very plausible that the way that Active Inference is solving problems like that is in any kind of exact form the same way that humans are doing it.

So there's kind of scalability with respect to what humans can do because humans can't do this explicitly in a fully model based way.

It doesn't seem like after a certain sort of level of complexity to the planning problem that needs to be solved.

And then the other part is distractibility with actually doing the modeling itself because of just like how

just interactively long it could take to actually run these simulations.

And so both of those things kind of come into play.

And it's not super clear exactly how to address this.

I mean, there's certainly ideas out there, but most of them involve taking additional kind of machine learning tricks, like adding deep neural networks to active inference models, for example, or doing various other sorts of

you know, little kind of heuristic shortcutty things that make stuff tractable, right?

Like not actually searching all the way down every possible branch of a decision tree, but using some heuristic to kind of say two steps in or something.

Now, this branch seems bad.

I'm just going to cut this off and not consider it anymore.

You know, things, things like that or like sampling based approaches for exploring just kind of little bits of the decision tree at a time.

You know, it doesn't it's no longer

pure active inference anymore, right?

I mean, it's a kind of combination of active inference and a bunch of other machine learning things.

But to a certain extent, like that's probably just like necessary.

And I mean, bottom line is the brain probably has to be doing something other than fully model-based active inference to be tractable.

And so figuring out exactly what parts of what the brain is doing might be pure kind of active inference and what parts are these other things to make stuff tractable is I think part of the question.


SPEAKER_03:
Yeah, look, the scalability issue with active inference is not an active inference issue.

It's just a Bayesian learning and decision-making issue.

There's nothing inherent to active inference that slows things down.

Any technique that can be used in any other sort of Bayesian decision-making method can be used in active inference.

So yeah, this is the grand question of how the brain is so efficiently able to do this, where there's

Yeah, like Ryan said, I mean, you run these very basic tasks.

And because of this exploding state space of Bayesian decision making, right, where just this exploding state space of probabilities, you get a massively expanding search tree, ultimately.

And yeah, nothing to do with active inference.

Of course, active inference is a Bayesian framework, and so it has to use this.

But that's just a, I mean, if we can solve this, then that's a, yeah, it would solve a lot of things.

If we could solve how we can perform Bayesian inference, and especially Bayesian inference in the service of decision making, how we can do that very efficiently, that would solve a lot of things.

But as of yet, yeah, if we want to try speed active inference up, we have to speed the general field of Bayesian decision making up.


SPEAKER_01:
Okay, I was just gonna say, is part of it, perhaps from a strategy standpoint, not trying to generate the world's biggest plan, but actually going back to that rules business and saying, can we start there instead of a plan that because of its just its size makes things intractable?

Can we could we swap something else in for

something we know won't work, which is world's biggest plan.


SPEAKER_00:
I mean, I mean, look, there's lots of different, you know, things that, you know, might you might consider like like one one thing, you know, for these like exploding decision tree sorts of problems is, you know, finding some way to chunk things together.

Right.

To make like the

You know, so for instance, instead of, you know, say I'm going to go, I decide I'm going to go walk to the store or something, right?

Like in a certain sense, I've got a ton of different options about, you know, where to put my feet at each step and, you know, like what door to go out of and things like that.

But it's not really clear that when I'm planning like to go to the store that I'm really explicitly considering all of those details, right?

Like I can chunk it into, you know, I'm going to,

walk out to the street, you know, I'm gonna walk

you know, I'm going to walk a mile to the store and then I'm going to walk into the store, right?

In which case, if I've chunked that to just kind of three steps, right, then my decision tree is already much more tractable.

And then you just need to tell some story about, you know, how, you know, when we get to certain points in that really abstract chunked plan, when we get into those kind of chunk states, how we make these kind of more local decisions about what to do when we're at smaller scales, like when we're in those states.

You know, there's lots of kind of hierarchical chunking-ish sorts of things that, you know, you might think that the brain could be doing to make these things tractable, but, and there's lots of different kinds of options on the table and things that people might try, but all things have certainly, all these things have not been kind of, you know, tested against behavior yet to see, you know, which ones are the most plausible, but there's certainly different possibilities for what might be going on.

Um, you know, another example, you know, this is something that we test empirically in my lab right now is, um, the way people might do something called like aversive pruning.

Um, and so, so what you're kind of doing there is you're just, and this is kind of similar to what I said before is, you know, when you start kind of planning down some tree, if there's some early outcome where you're imagining that it's going to lead to some really negative outcome.

right like on like if like on step one or step two in a possible plan i think oh there's going to be some big negative thing then i will just no longer simulate down the rest of that branch so it's a way of kind of reducing the number of branches in a tray i have to search and um

and so that's it's often called again aversive aversive decision tree pruning and that um you know obviously you need to do something like that right to keep it tractable but at the same time it can cause problems right because sometimes the best plan might go through something negative in the short term but lead to the best thing in the long term right i mean so that's the kind of thing we we test in the lab is whether or not different psychiatric conditions involve kind of doing pruning too much right or too little um and how that could lead to so optimal behavior in the long term um

I think the other aspect of the previous question you asked also had to do with, you know, you read part of the thing talking about the neural basis of active inference and how that should kind of remain tentative.

And just to kind of clarify that a little bit, I mean, the main point is just that, again, this kind of involves being clear about the separation between algorithm and implementation, right?

The algorithm is basically just the mathematics that we kind of lay out.

But then the implementation question is, what are the different possible ways you can kind of set up the brain to carry out those equations?

And it's a separate question how the brain is doing that, even if you think that there's good reason to believe that the brain is doing something that's well characterized by the mathematics.

Um, and, um, unlike predictive coding, which has been around a long time and, you know, there's been much more opportunity to test, um, at least, um, qualitative predictions, right?

You know, things like whether or not there's evidence for like omission responses in the brain, right?

Like when there's the lack of a stimulus, when it was expected leads to a neural response, um, which is, you know, one of the stronger pieces of evidence that the brain is doing something predictive, like predictive coding.

Um, you know, for, for active inference, uh, there really has been like one imaging study and it was done like in 2015 and it was like an older version of active inference.

It wasn't the current full based on the current formalism.

um you know so there's this just hasn't been done right and the and even the um you know the little kind of column structure things like what's in the um like what's in the review in our in our in this paper um is super just kind of like promissory heuristic sort of things it's like here's a bunch of here's a bunch of little balls that will pretend are like neurons and here's how you could connect them together roughly you know to um do

some sort of, you know, some message passing algorithm that, you know, you're thinking is, you know, you're imagining or hypothesizing is the one that the brain's doing for this kind of approximate inference process.

And, you know, even when it comes to

the current mathematical proposal for how the brain might be doing something like this, there's different hypotheses, right?

So like initially most people, when they were doing simulations with active inference models were assuming something called variational message passing, which is just a particular way of kind of repeatedly doing local proximate Bayesian inference on different kinds of nodes in a graph.

And they,

you just kind of over and over again, the thing kind of converges to a good guess about what the posterior should be overstates at each time point, right?

But

But then after that, a little bit more recently, Thomas Parr and Carl and maybe other people were on the paper.

I can't remember.

They proposed a kind of updated version of that called marginal message passing, which is a little better.

It does a little slightly more.

It can get a better approximate posteriors on average.

um but then there are others right there's belief propagation is another sort of message passing algorithm that's you know been considered so there's a bunch of these right and each of those even if they're doing active inference right like they will also predict different neural dynamics um so so those are also even just active inference under what message passing algorithm um separates into a bunch of different competing hypotheses about what you would measure in the brain


SPEAKER_02:
Awesome.

I'll read a question from live chat.

Andrew P. writes, curious for Dr. Smith, where might he place his previous work like simulating the computational mechanisms of cognitive and behavioral psychotherapeutic interventions insights from active inference in relation here?

Would this kind of work be more on the side of theoretical exploration

like the cognitive-affective-behavioral interactions construct, or is there a direction for empirical testing?


SPEAKER_00:
I mean, as is absolutely, it was a theoretical simulation work sort of paper, right?

I mean, we weren't fitting that model to any sort of empirical data.

It does, I mean, that paper in particular does lend itself to at least certain, that model is a little too general.

Like the patterns of behavior that it predicts are really not specific enough to like test in a, you know, some sort of task.

You could make it, you could modify it in a way that might make it specific to some sort of task that would involve, you know, specific sorts of explore-exploit decision-making choices under kind of expected negative outcomes, things like that.

But it does make a kind of...

sort of qualitative prediction that could be tested.

So one parameter that you might fit based on that model that I think would be super interesting if you could kind of turn it into a task is this parameter that reflects the degree to which your cognitive beliefs influence your automatic affective responses or expected affective responses.

I mean, it would be really interesting to try to use that to figure out whether or not there are, we can measure individual differences in this kind of what often gets called like cognitive penetrability, right?

So whether or not,

i you know because i might explicitly believe i'm safe right in a certain situation but i might see something that still makes me feel really this automatic kind of fear response in my body or something even though i explicitly believe it's safe right my affective responses don't always have to match my explicit cognitive thoughts that's often a thing you see right in people with with um with affective disorders so this kind of individual difference that you could estimate about

the degree to which these two, essentially these two different hidden state factors in the model, the degree to which those things interact effectively.

But the other prediction that makes, again, it's a little more qualitative, but certainly something that could be tested or made more precise is that in that model, one of the interesting things that came out of the simulations is that

it's probably actually not a good idea to make people think that they're explicitly believe that they're in a safe context before you do something like exposure therapy.

Because if you do that, then basically what you're learning, the beliefs that you're updating in your likelihood about what actions are going to lead to what outcomes, those will be updated under the belief of safe context.

So that means all you have to do is switch back to believing you're in the dangerous context, and then you're just right back to the problematic avoidance behavior again.

And there's a bunch of other, there's recent empirical work that's very consistent with this idea actually about explaining using this kind of latent causal inference framework, not an active inference in particular, but just latent causal inference more generally, that to really get exposure therapy to work long term,

you need to kind of prevent that people are inferring that there's a new latent cause in operation when people are doing exposure because otherwise they're just learning safety under a context where they can really easily get their maladaptive avoidance to come back just spontaneously just by kind of

being uncertain about the context they're in later.

They're not actually unlearning the problematic belief.

They're just inferring there's a new cause where under that new cause it's safe.

So it was much better, the prediction of that paper,

would be that could be tested, right?

And again, like I said, there is now some evidence consistent with this, is that it's kind of better to do things to keep people uncertain or keep people believing that they're still in the same context so that when they go through this exposure therapy and get unexpected outcomes, they're actually kind of overwriting their previous beliefs as opposed to just inferring they're in a new context.


SPEAKER_02:
Awesome.

All right, I'll ask a kind of general question that I'd love to hear everyone's perspective on.

We have active inference or predictive coding or some other type of formal framework that does interface with empirical data.

like we're discussing.

So on one hand we have the empirical system or the measurements coming from it, existing data sets, data sets we create and so on.

And then on the other side of this kind of statistical interface is like numbers, free energy principle, category theory, stuff that is really not having its validity or truth or essence described by any particular system.

So how do you just approach this usefully as a graduate student researcher or as like a broader clinical research program and like think about what system are you choosing?

What statistical interface?

and how detailed do you go there and how do you think about like what's on the other side of the statistical interface because a lot of times the discussion around like the validity of active inference and free energy principle is a super philosophical question as if a philosophical discussion would resolve uncertainty about the empirical status of active inference

which is actually the direction that you all took it from the interface back to the system, rather than interface as outcome from something theoretical to debate.


SPEAKER_00:
I feel like I have been kind of hogging the show here a little, so I'll let other people answer that first, at least.


SPEAKER_03:
um okay sorry that was quite a long question so i'm sorry if i missed some of it um and just correct me if i'm not answering right uh i think i will talk on um this idea between yeah on the one side uh sort of mathematical formulations representing something like uh brain processes and then you have sort of mathematical formulations brain process and behavior and um

these are sort of seen as unified.

I think as Ryan said about talking about neuronal implementation, there's many ways in which we could set up like a neuronal implementation to achieve something like predictive coding or active inference.

The same goes for

algorithm where there's many different forms of algorithms could achieve, or statistical windows, as you said, if you want to go more broad, could achieve the same neuronal implementation and then ultimately behavior.

In terms of, and please stop me if I'm not answering correctly.

But yeah, in terms of as a graduate student choosing what to focus on,

I think it's sort of an evolving process where you constantly just try to read and come up with new ideas about how one can think about how these can connect and how one can represent the other.

And for me, it's been particularly interesting just actually understanding what active inference is in its mathematical essence.

We've got this first principle account of it, developed from the concepts of homeostasis,

And yeah, it can sometimes feel a bit strange when you look at quite simple mathematical implementations in an algorithm and see that, well, this isn't so different from something like Bayesian reinforcement learning with specific directed exploration.

So there's a sense of that sort of first principle account that comes all the way from Markov blankets

Where is that captured in these algorithms?

And that's something I'm still trying to figure out, to be honest.

I think in general, it's a problem.

And I think that's why

When Ryan mentioned sort of looking at more complex tasks, I think more complex tasks would allow us to explore more complex representations of active inference that can perhaps capture some of the foundations that Friston laid out.

So does that answer the question, sort of?


SPEAKER_00:
Yeah, that's awesome.

Yeah, I mean, the other, I mean, the other things I would say, I mean, I think I understood at least part of the question a little bit differently, was that, you know, there's sort of what, by that I don't mean that you misunderstood, Ron, you just answered a different part.


SPEAKER_03:
I did, that's really good.


SPEAKER_00:
Thanks for helping with my answer.

Part of what I take your question to be was something about to what degree answers about the, quote unquote, validity of active inference or the free energy principle can be established through kind of like philosophical argument versus what can be or what needs to be answered through kind of direct empirical testing.

Here, I think it depends a little bit.

I think what philosophy is really good at is actually more finding ways that something couldn't be true.

I have a philosophy background as well, and I think philosophy is really good at conceptual analysis, so trying to find where some particular conceptual framework

you know, where there's tensions, right, or where there's things that don't sort of at first need sort of appearance, seem like they're contradictory, have some sort of internal tension.

But when you really drill down, you find certain contradictions, right, that, you know, rule out something or make it really implausible, either through sort of direct logical

argumentation or through kind of like just proposing sort of thought experiments that really kind of pump intuitions in a way that make a difference.

So I think you can rule out with philosophy, philosophical methods, things that are conceptually problematic, right?

So you can get down to kind of the set of frameworks or the set of, yeah, the set of sort of interlinked concepts that make up a theory.

You can find a set of them that are at least internally consistent

and have some sort of general sort of sufficient coherence to them.

But once you've weeded things down to the kind of subset of possible theories that are internally consistent, right, and don't have any philosophical problems associated with them, then I think things really kind of need to get empirical, you know,

to the extent that different kind of competing conceptual sort of theoretical frameworks, to the extent that they make different predictions, and to the extent that the goal of this whole project is to try to come up with an accurate theory of what brains are doing.

If that is the goal, then the only way ultimately to validate that is by actually looking at brains and seeing what they're actually doing.

um and um and so and so i think i think that's kind of where rubber meets the road a bit right is this is that um um yeah you need to know what's plausible right and that's where i think philosophy can help a lot but after you know what's plausible then testing what's kind of actual right is kind of the next necessary step and um

And this kind of comes back to what Rowan was saying, was that testing the unique predictions of, say, some given active inference model versus some competing model from another framework like reinforcement learning,

can be tricky.

Usually you have to do this in an incremental way.

And you do that by finding by identifying some prediction that is different between those two models and then designing a task that would really put people in a decision in a position where they have to make choices that are consistent with either the prediction of one or the other.

And it really requires some creativity.

Right.

And coming up with

the right task with the right decision-making demands and constraints that will do that.

Because there's lots of tasks where, say, the simplest, most straightforward active inference model will really make almost exactly the same predictions as what a simpler reinforcement learning model would make.

So in those cases, you really can't differentiate with you because they pretty much predict the same thing.

And as Rowan mentioned, even though it's true that active inference is kind of motivated from these sort of first principles, which is super nice, the thing that you ultimately end up converging on in terms of the decision-making algorithms in active inference start to look pretty similar.

to the sorts of things that people in machine learning and reinforcement learning have ended up kind of coming to anyway.

They just haven't come to it from a first principles perspective.

They've come to it because they just kind of like tinkering around and figure out what works.

And so that's also kind of an interesting question is, you know, where can we find differences between active inference and the other stuff that's out there that has nothing, that has no connection to active inference, but it's kind of converged on a similar type of solution.

And then you get to a point where, okay, well, if we've gotten to the case where we have two algorithms that have different origins, but are basically doing the same thing, right?

Then there is no competition anymore.

We've just gotten to the same solution two different ways.

So that's, I don't know, I guess it goes a little bit beyond your question, but I think hopefully is all sufficiently relevant.


SPEAKER_02:
That's awesome.

Maybe, Mariska, and then like kind of just in closing here, how did people pick up and run with the work and just how do people continue with it?

But first, how do you approach this at a bigger level?


SPEAKER_04:
Yeah.

um so i come from like a different side to this i come from more empirical background like what ryan was saying so to me it's more about taking all of this information kind of accepting the broad literature that that's out there not just in active inference or predictive coding but also more from the psychology neuroscience side of things and kind of seeing where these things fit in an empirical setting

where do these algorithmic ways can be tested in a plausible manner?

For example, like how Ryan was talking about the last fMRI study was done in 2015 that actually tested

active inference models that to me is like the first things where we would want to test what is actually happening in the brain whether it's through fmri eeg kind of testing at different levels through different computational models what the brain is doing to me that's the most interesting next step like taking the computational work out there and trying to relate it back to brain and behavior


SPEAKER_02:
cool rowan and then ryan or anyone else can just kind of give any last thoughts or what they're gonna take going forward go for it go for it ron if there's anything you want to say


SPEAKER_03:
Sorry, I was muted.

Yeah, I think active inference is in an interesting spot.

And I think the world of AI is in a very interesting spot.

I know by AI I mean artificial intelligence, not active inference.

If we want to look at active inference as sort of an element of artificial intelligence, it represents a side of things, which is sort of

neuroscience-inspired artificial intelligence.

And as we all know, there's a bit of a war going on between that and the large language model kind of things.

And I think active inference is, yeah, I think while it's tempting to sort of go ahead with

amassing more data and training larger models, I think that it's really important not to forget about this aspect of trying to not just come up with more architecture and more data, but to come up with more interesting frameworks.

I also think that unlike traditional artificial intelligence or machine learning, it's important to remember that

There's this tension between, as we mentioned before, scalability and just efficiency and biological plausibility.

We don't always want a solution, a model to just be better and quicker.

That isn't necessarily what we're going for.

We know that in many cases, artificial intelligence can do things better and quicker than the human brain.

But we shouldn't necessarily only care about that.

Sometimes, yeah, it's important to get a solution where it doesn't act more effectively, but acts more like data, empirical data that we have gotten from actual human behavior.

And even if it isn't the most efficient sort of architectural algorithmic structure, it matches the structure that the brain might implement.

So I think it's important to remember these factors.

In this world where it's fast developing into big data and massive models, sometimes I find myself drawn to just always trying to make active inference faster and always trying to make it more efficient.

And I sometimes have to check myself with that and remember what I'm actually interested in.


SPEAKER_00:
Yeah, and I guess I guess as a concluding thing on my end, I mean, you know, Daniel, you asked specifically kind of about future directions, you know, and the kind of thing that we're doing in the lab right now is very kind of inspired by what we said needs to be done in the in the in the paper.

Right.

I mean, it's always a little bit of an advertisement for in a certain sense for, you know, here's what we've done and here's what needs to be done.

And, you know, we're trying to kind of follow that ourselves.

But

you know, and again, you know, very much kind of in the context of trying to also use these frameworks to, you know, be of some kind of practical benefit to the world, right?

So, I mean, you know, we kind of in tandem, right, use this as an opportunity to one, you know, test theories about

you know, how brains and minds work, but also to take it as a simultaneous opportunity to look for where these sorts of mechanisms may be, you know, going wrong a bit and people with different sorts of disorders and whether or not that can, you know, give clinicians, right?

Like ideas for novel treatment targets, right?

Like to target mechanism X versus Y, you know, as kind of shown by what the differences are in models when we fit them to behavior and say like healthy versus clinical populations.

And so, you know, the sorts of things that, you know, we're doing now on the corner of more basic science side is, you know, I have another grad student, Ko-Ping Chau, who's not on the call here, but, you know, she's done some very extensive work now fitting

something like 50 or so different models to some decision-making behavior that we have in both a Taiwanese sample where she's from and an American sample from around here.

um and you know trying to find right like which model fits best and so these are a range of simpler kind of reinforcement learning models and a range of different active inference models so just parameterized in different ways right so ones that assume sort of like you know static versus dynamic decision noise or static versus dynamic learning or i mean again there's tons of little variants on these things

And so she's found in both samples that the model that best fits the data and wins in model comparison is a specific active inference model that has six parameters, I think, that involves dynamic learning or dynamic decision noise that includes particular sort of forgetting rate function that's not included in kind of the simplest version of active inference.

And so showing much more kind of conclusively in two different data sets, that active inference at least wins in model comparison against some of these other kind of competing models.

That being said, I think it's important that even though it wins in model comparison, when you actually look at the predictive accuracies, they're very similar.

We're talking about active inference being able to explain like,

81.5% of the data versus the best reinforcement learning model being able to pick 80.6% of the behavior.

These differences are minor.

It comes down to being better at predicting just a couple of choices.

But sometimes if those choices follow from different sorts of expected dynamics along the way, then that can still be meaningful.

You know, so that's the kind of thing, you know, that we've been doing on the more kind of basic science end that follows up on, you know, what we were saying needs to be done, you know, on the...

sort of clinical end, still not super clinical.

We have a study that's ongoing right now that has both an online and an in-person component where we came up with five different tasks where different models would predict different patterns of behavior in distinct ways in each of those tasks.

A couple of them were designed specifically for active inference.

Others were designed more in relation to the HGF, the hierarchical Gaussian filter.

And so we're in the process of collecting a lot of data in those where, again, we'll be able to fit different models.

And that one, we're also focusing a lot on whether or not we can find models and parameter values in those models that predict individual differences in subjective well-being.

as well as negative affective stuff.

So we're trying to go for not just predicting like symptoms and negative emotional stuff, but also positive emotional stuff and what makes a difference to decision patterns that are say more consistent with greater satisfaction with life and subjective wellbeing and things like that.

So, we're doing that.

We're also doing some computational sort of Bayesian modeling of interoception data.

So how well people can like detect differences in their

heartbeats or differences in their kind of like respiratory difficulty, things like that.

Um, and how they're always anxiety disorders.

So, so we're, um, it's very much kind of a combination of this basic science stuff and the, and trying to move the usefulness of this thing clinically at the same time.

So it can be kind of practically helpful to people.


SPEAKER_02:
That's awesome, Dean, and then anyone else and then that will be awesome.

But where do you go from here, Dean?


SPEAKER_01:
Have a shower?

I don't know.

I'm not.

I don't have anything really planned.

But I appreciate the I appreciate the explanations.

And like I said,

when you start from a place where you take two things and hold them up together at the same time.

That's the part that I appreciate here.

And I think it, again, reading through all of this, I didn't understand all of the math,

But it also, I think, based on what you were able to show, I wasn't sort of questioning what process you went through to draw your conclusions and state what some of the limitations are until those tests have been carried out.

So, yeah.


SPEAKER_02:
yeah thank you for the very clear very relevant paper so good luck and you're always welcome back to share more we can follow up and um it'll be an ongoing rolling literature meta analysis across different systems there will be thousands of fractal sub reviews empirical status in the amygdala on this kind of thing so

It's great to hear how you did it this time.


SPEAKER_00:
Thanks for having us.

Yeah, thanks so much.

Bye.


SPEAKER_04:
Bye-bye.


SPEAKER_01:
Bye.
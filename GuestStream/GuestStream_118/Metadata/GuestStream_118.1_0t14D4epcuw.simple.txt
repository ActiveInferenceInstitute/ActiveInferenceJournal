SPEAKER_00:
hello welcome everyone it is august 20th at the stroke of zero utc on the 21st in 2025 and it's active inference guest stream 118.1 we're here with tadihiro taniguchi and we'll see a presentation on co-creative learning via metropolis hastings interaction between humans and ai

And then I will read any questions in the live chat and ask some things about this fascinating research.

So thank you for joining again, Tadahiro, and to you for the presentation.


SPEAKER_01:
Thank you very much for inviting me to this great guest stream again.

I'm Tadahiro Taniguchi from Kyoto University and today I'll be talking about co-creative learning by a method of processing interaction between humans and AI and this is based on our preprint which is recently available on arXiv.

So this is the co-creative learning.

It's a new theory of human-in-the-loop machine learning method.

So I'll also discuss a collective predictive coding hypothesis, which I talked in the previous guest stream.

Maybe you can check it later.

But as a background of this work.

OK, let me start.

So here is an overview of our main contributions of this paper.

The first, we formally propose co-creative learning.

This is a novel paradigm where humans and AI collaboratively achieve symbol emergence.

In other words, the formation of the external representation.

by mutually integrating their partial perceptual information to build shared representations.

This fundamentally shifts the perspective on human-AI alignment or teaching process, moving beyond traditional unilateral approach.

Second, we introduce an experimental framework based on the Metropolis-Hastings naming game.

Through this framework, we propose the comparing empirical evidence

can effectively engage in this creative learning process.

So the visual of this slide illustrates the idea of mutual interaction between the human and the AI centering around the shared world.

So let's consider why co-creative learning is so crucial currently at this phase.

So the first one is limitations of unilateral alignment or teaching.

Traditionally, unilateral alignment or teaching, typically supervised learning based on ground truth labels provided by human annotators, have significant limitations.

For instance, AI systems already excel beyond average human performance in many areas, possessing distinct and often superior domain knowledge.

Furthermore, the human and AI inherently have heterogeneous knowledge.

Our ways of understanding and processing information are fundamentally different.

And also, when embodied AI agents start exploring the world autonomously, it becomes questionable whether we should enforce our existing human knowledge and values as the sole ground truth.

In some sense, we should respect AI's perception and our knowledge to bring our knowledge better status.

Another thing is the bidirectional influence.

Actually, the large language models, for example, actively impact the human language use and decision-making, as shown by recent studies.

This mutual influence clearly highlights the limitation of the perspective of purely one-sided alignment teaching strategy.

Therefore, looking beyond the current generative AI era, it is important that we establish a new systems theory, principle, and learning method to achieve a true human-AI symbiosis, considering bidirectional alignment or teachings.

That can be called co-creative learning process.

So to position co-creative learning more intuitively from the machine learning perspective, let's briefly and roughly review existing traditional unilateral machine learning frameworks.

So in the most common, in supervised,

or reinforcement learning.

The human's perspective, P, this human's perspective can be written like that.

The Xn human is the human's observation.

And Sn is the kind of cross-level or description of the human's perceptional information.

In our society, we form the relationship between the labels and the partial information in some sense.

This effectively serves as the ground truth or the target or the source of reward.

for the AI to learn from.

The AI's understanding is therefore fundamentally bounded by the human's perspective.

In contrast, in unsupervised or self-supervised learning, the AI agent learns in isolation, inferring knowledge thoroughly from its own observations.

In this case, its understanding is bounded only by the limits of its own perceptual data.

A key takeaway from both these traditional approaches is that only partial information, either human or AI, is used for the learning process.

So this highlights a critical limitation.

The NISA approach fully leverages the combined potentially richer information available when humans and AI interact.

So this brings us to our proposal, the new bilateral frameworks, co-creative learning.

Here, the goal is not for one agent to teach the other, but for both to engage in bidirectional message passing through language games and communication to integrate their partial observations.

So, instead of treating one agent's perspective as the sole ground truth,

The running target for the diode that becomes the true posterior distribution are conditioned on the joint observation of both agents.

Here, X and AI and X, the human.

This is crucial distinction.

and partial information, this approach allows emergent shared understanding to surpass the knowledge and perception that either the humans or the AI could have formed alone.

The diagram on this slide illustrates this mutual interaction and the formation of a joint, more complete understanding.

But how can we do this?

That is the question.

And before diving into today's main topic, let me briefly introduce or review the collective predictive coding hypothesis, which I proposed last year and gave a talk in this guest stream last time.

So this hypothesis extends the idea of well-known predictive coding to social and linguistic phenomena.

So it states that symbol systems, including language, are formed through collective predictive coding by humans.

In other words, a human society, which can be regarded as a symbol emergence system, the systems performing symbol emergence technically, can be understood as a social representation learning system.

As a result, the information of the world may be encoded in the distributional semantics of our language.

The hypothesis also suggests that this is why large language models appear to understand the world.

they inherit this structure through vast amount of distributed human-generated text.

By the way, the diagram illustrates this complex process where the emergent, where the individual inference and action loop contribute to an emergent, the symbol system.

that in turn constrains the organizers' collective behavior.

So let me illustrate.

So maybe the explanation was too quick.

So let me illustrate the CPC hypothesis, collective predictive coding process, the bridging the idea of emergent communication and predictive coding, or free energy principle.

First, let's start from the predictive coding, or world model learning.

So there is a person.

He or she internally has internal representation over world models.

And we are living with action perception loop.

So this is, very roughly speaking, the inference process.

But we humans, we are so many.

And through communication, we form language.

So following that predictive coding hypothesis, through language game, we can form language-like external representations through this total learning process.

I think through this process, we model and adapt to the world collectively.

But here, let me take another view.

Until now, we believe, we assume that each person is an independent and autonomous agent.

But let's change and take another viewpoint.

First, language is here.

Language is a kind of autonomous system.

And they have sensors or also actuators.

That's us.

The language utilizes us to model the world through interaction.

From this viewpoint, we are subordinate to the language system.

This kind of view is often mentioned in sociology or other social science area or cultural psychology.

So I think this kind of dual viewpoint is very interesting to extend our view of cognition to the social viewpoint.

and actually this dual viewpoint actually gave us opportunity gives us opportunity to formalize this process from the viewpoint of free energy principle so the based on the generative model representing a cpc i mean this kind of the the language model let's let's let's consider a language is a kind of brain having these uh sensors sensor motor systems

So, following the graphical model, the probabilistic generative model, describing such kind of process, we can define the free energy in the same way as the normal, usual free energy principle.

Interestingly, we can naturally derive this kind of formulation.

And interestingly, the final form can be divided into two parts.

So the first one is an ordinary variation of free energy term.

And this is sum of the variation of free energy of a number of agents.

In addition to that, an interesting term pops up.

We call this a collective regularization term.

This term tries to align agents' external representations, W, and with each agent's internal representation, Z.

With this term, the problem of minimization of individual free energy becomes the minimization of free energy of a single total system.

I think this book can extend the free energy principle from the individual to the collective level, with a clear relationship to the emergence of symbol systems or language.

As the simplest instance of the theory of collective predictive coding, let me introduce the Metropolis-Hastings naming game, which we previously proposed.

It is a type of language game built on solid mathematical foundations.

This will serve as the basis of co-creative learning.

I will introduce data.

How does it work?

Here is the eyeline.

First, in the perception phase, the speaker and the listener observe the same object.

and infer their internal representations.

We assume joint attention here, which is crucial for human language acquisition.

Next, in communication phase, the speaker probabilistically utters a name for the object.

The listener then determines if it accepts the naming or not.

with a certain probability.

The probability depends on its current belief state.

Specifically, this probability shown in the equation on the slide reflects how well the listener's belief matches the name proposed by the speaker.

So if the listener agent does not find the incoming sign believable, it can reject the proposed name.

This ability to reject is crucial.

The third, the learning phase.

After the communication, the listener updates its internal parameter

are related to representation, learning, and naming.

And finally, in the turn-taking step, the speaker and the listener alternate their role, and the game continues.

Overall, this is a very, very simple game.

But importantly, this game has a solid mathematical basis.

So, let's dive into the data.

If the acceptance decision is made according to this probability, we can mathematically prove that the naming game is equivalent to the method-processing algorithm for Bayesian inference of the shared representation.

the shown in this figure on the left so let's begin with this graphical model so here OA and OB or we can rename this XA or XB represents a visual observation of each agent they are

the agents with the visual input here, but any kind of input is okay, basically.

ZA and ZB are the internal representations, latent variables.

We also assume the existence of a variable W, which serves as a prior for these two observations, two internal variables.

So if we treat W as a latent variable,

This graphical model becomes very similar to the multi-modal or multi-view variational autoencoder or latent variable models.

And if the two agents' brains were directly connected,

and could use both observations simultaneously, yes, this variable can be inferred in the natural way and would encode the combined information from both agents' input.

However, in reality, they are independent cognitive systems.

So let's introduce a trick.

Algorithmically, we can decompose the model into two parts.

So the left-hand side is agent A, and the right-hand side is agent B. So let's consider the kind of alternative.

the inference, the method for this w. Now think about the sampling process.

Agent A proposes a candidate of latent variable w by sampling from its posterior distribution.

and the proposed value is passed to agent B the agent B can decide whether to accept the sample from a sample based on that probability so this is this process

is the and this process can be designed by using the metropolis hastings algorithm which is which is a kind of marco chain monte carlo mcmc so actually the under certain conditions this guarantees that the shared signs

are formed and that categories emerges among the agents to approximate the posterior distribution based on the agents' observations.

So importantly, what I explained here is completely equivalent to this naming game.

So it proves that this naming game itself is Bayesian inference, approximate Bayesian inference procedure.

So this is a very important point.

This is a very important point.

So in the previous paper, we tested this idea on some data set.

And we showed that the two agents playing Metropolis Hastings' naming game can effectively integrate their perceptual information and form and share signs.

By signs, I mean the categories.

across the agent.

And actually, such kind of process even outperforms the learning result achieved by a single agent.

So without any ground truth, this kind of method-based hasting interaction can improve the representation learning process, in some sense, the knowledge, the formation process.

OK, so this is the background.

And actually, after this computation study,

As you saw in my talk, I always assume that this is something happening in our society, humans.

So, to show that, we need to check, we need to validate that if this kind of things is happening in human interaction.

So we designed an experiment to test whether a human participant, when playing a game similar to the MH, Metropolis-Hasings naming game, would follow the acceptance probability calculated by the algorithm.

The results, as shown in the graphs on the right, are positive.

We found that human acceptance behavior indeed mostly aligned with the theoretically derived MH acceptance probability.

This is a crucial finding because it suggests that humans, to a certain extent, follow similar patterns to the Metropolis Hastings naming game, providing empirical support for the idea that human behavior on simple emergence might follow a CPC mechanism.

This paves the way for our recent work, integrating AI and humans into this framework.

so this is the beginning of actually today's main topic so a simple way to introduce co-creative learning is to view it through the lens of a metropolitan style interaction between human and ai like this

So OK, let's jump into this section.

So given this insight, we then move to the core of our current work, the realization of collective predictive coding within a human and AI collaborative system.

As depicted in this graphical model, we envision

a system where a human and AI agent, each with partial observations, engage in bilateral message passing through language game or communication.

This interaction facilitates the formation and influence of a shared representation

And represented by the QSM, this is the kind of sign and shared latent variable.

And the XAI and XHUMAN, this means the AI's observations and the human's observations.

This is what we define as co-creative learning.

The key idea here is that instead of one agent simply teaching the other, the both mutually integrate their partial perceptual information.

The display process enables the enabled

by the mechanism like the MH energy, Metropolis-Hassings naming game, allows them to jointly construct a richer shared external representation that neither could achieve alone.

This is a direct instantiation of the CPC hypothesis in a human AI triad.

So more formally, to be precise, we formally define co-creative learning as an interactive, decentralized Bayesian inference process, as shown in definition one in our preprint.

Through message-based update, like the Metropolis-Hessing naming game, jointly construct local belief distributions

The crucial aspect is that this process ensures the expected collective free energy, FT, decreases over interaction steps.

This means that the Markov chain formed by their interactions converges to the true posterior distribution conditioned on a joint observation of both AI and humans.

So what's remarkable about this mechanism is that it allows for improvement of Monte Carlo estimates of the evidence P .

Without requiring either agent to disclose their private observation or internal gradient.

This decentralized nature is a key advantage in enabling robust and private knowledge co-construction.

With this formal definition in mind, our primary objective for this experiment was to empirically test this concept.

attention naming game.

This is a kind of experiment with participants asking to play the MH naming game like naming game.

So this game was played between human participant and the computer agent.

The central question we aim to answer was, can the human and the computer effectively integrate their respective partial observations through this naming game, thereby engage in the decentralized Bayesian inference, or as we term it, co-creative learning?

To ensure this, each agent was given only a partial

a missing subset of the full observational information.

As you can see in this diagram, the human might have certain dimension of features

and while the agent has others, and some are missing for each.

So this setup makes accurate categorization impossible based solely on individual information, thus necessitating information integration through communication.

We wanted to see if they could collaboratively fill in this missing piece and reach a shared understanding.

So, let's detail our material methods.

The stimuli consisted on image,

images depicting a circle with a radial notch.

So these were generated based on five-dimensional vectors sampled from one of three ground truth Gaussian distributions.

and using the CIE lab color space, which is perceptually uniform.

So the parameters were chosen so that categories would overlap significantly in the partial observation space, like this.

Crucially, as shown in figure this part,

Humans and computer agents receive different partial observations.

Human participants view a grayscale rendering based on the original figure.

And the agent only can see the color.

color information.

This partial observability makes accurate categorization impossible based solely on individual information, so making communication essential.

So the dyad's task was to collaborate the categorized stimuli into up to three categories.

Yes, the ABC.

By playing the naming game, figure B illustrates experimental procedure flow involving initial categorization, initial categorization, and the naming game, turns and role switching.

So this is actually a very difficult task for human participants, as you can imagine.

OK, this is the process.

The slides show a screenshot of the experimental interface used in our experiment.

First, participants were asked to categorize the given images by dragging them into labeled boxes, as seen on the left.

During the naming game, one

one image is presented on the right, and the naming agent proposes a name or sign for the image by clicking a button.

The listener then decides whether to accept the proposed name by clicking an accept or reject button.

Sorry, the instruction is in Japanese.

After this decision, the listener is prompted to revise their own categorization if needed.

This intuitive interface supports the partial observation and sign exchange between the human and computer agent, facilitating the interactive process.

OK, so the experiment was conducted online by a web browser.

We recruited 90 participants through CloudWorks, a crowdsourcing service.

And after filtering for engagement and adherence to instruction, 69 participants were included in our final analysis.

The experiment typically lasted about 90 to 120 minutes with participants receiving compensation for their time.

OK, so for Compison, participants were divided into three distinct groups.

So each interacting with computer agent, employing a different acceptance strategy for human proposal.

Group 1, the MH group, the Metropolitan Hastings, engaged with an AI that accepted the proposals based on the Metropolitan Hastings acceptance probability.

serving as our primary co-creative learning condition.

Group 2, the always-accept group, interacted with an AI that always accepts proposals, acting as a proxy for traditional supervised learning.

The human's suggestion is always true.

Finally, Group 3, the Always Reject Group, communicated with an AI that they always reject human proposals, mirroring an unsupervised learning scenario.

So the AI just learns the form categories from their observation.

These conditions allow us to systematically evaluate the effectiveness of the MHNG in facilitating co-creative learning.

So let's look at the result for clustering accuracy measured by adjusted random index, ARI.

Table 1 summarizes the initial and final mean ARI score for both human and computer agents across all groups.

A key finding is that participants in MH group achieve the highest categorization accuracy and sign agreement, particularly on the AI side.

When compared to both the always accept and always reject group, specifically the computer agent in MH condition

achieved a significantly higher final mean ARI.

This clearly indicates that the image-based interaction most effectively facilitated shared understanding between human and AI, engaging a more collaborative and accurate modeling of the world.

While interacting generally helped the human improve, the image mechanism was uniquely effective for the AI agent to leverage the interaction for superior performance.

To visualize this trend, this figure, figure 3, plus the mean ARI over the 200 communication steps for both human and computer agent in the condition.

You can clearly observe that both the MH and AA always acceptance conditions show a general trend to increase ARI for both humans and computers, indicating successful information integration and learning from interaction.

And in stark contrast, the AR condition, always reject condition, whereas the AI always reject proposal, show little to no improvement from the initial categorization accuracy, especially for the computer agent.

This graph vividly demonstrates the superior performance and learning trajectory achieved under the Metropolis-Hastings direction, particularly for the AI agent.

Beyond just categorization accuracy, we also evaluated the sign-posterior distribution convergence.

As highlighted in Table 1, the MH group achieves the highest convergence between the agent's final sign distribution and the target posterior distribution derived from the combined model.

This indicates that the MH-based interaction most effectively guided the dyad towards the emergence of a shared informative sign system that truly reflects the integrated information.

While human participants in the MH condition also show a high agreement,

the difference compared to the always accept condition was not statistically significant.

However, the AI's performance in the MH group was significantly superior, demonstrating its ability to converge the interaction to align its understanding with humans, leading to a richer shared representation as indicated by the convergence toward the S-given X-AI and X-human.

To summarize our discussion, our experiments clearly show that the human-AI pair can successfully engage in co-creative learning via the joint attention naming game, so under partial observation.

The Metropolis-Hassings acceptance rule significantly improved categorization accuracy and agreement that compare to both always accept and always reject control conditions.

This is evident in the ARI graph and the sign agreement score we just discussed.

Furthermore, a crucial finding was that human acceptance behavior aligned closely with the theoretically derived Metropolis-Hastings probability, as shown in Figure 4.

This reinforces the idea that the mechanism enhances perceptual integration and effectively facilitates decentralized Bayesian inference within human-AI diet.

These findings collectively suggest

a promising direction of designing truly symbiotic AI systems that run with humans by fostering emergent interaction-driven alignment, moving beyond the limitation of the traditional unilateral approach to AI development.

While our findings are highly promising, we acknowledged several limitations in current studies.

We utilized actually simple, artificially generated visual stimuli and categorical signs.

So this controlled setup was necessary for initial validation, I believe, but it naturally meets the generalizability of our findings to complex real-world tasks involving richer semantics and natural language, such as those handled by large luggage models.

Additionally, our joint attention naming game, Paradigm, assumes the perfect joint attention between agents.

In real-world interactions, attentional alignment can fail, which is an important aspect for future research.

Looking ahead, we identified several key future challenges.

These include scaling to richer modality and semantics, robustness to imperfect attention, designing real-time co-adaptive systems, and understanding emergent dynamics in large-scale systems, and also ethical.

And interpretability concern is also important, I think.

And actually, to build a truly symbiotic human-AI society,

so how to how to integrate this kind of idea to the practical applications that is also important topic okay and that's all from my side i thank you very much for your attention and i look forward to further discussion okay thank you very much thank you for the talk awesome all right


SPEAKER_00:
I will ask a few questions that I wrote and I will copy them into the chat as well.

And for anyone in the live chat, please feel free to write questions and I'll copy them in.

Alright, so first comment and question.

The task focused on categorical discrete as well as quantitative aspects of situational semantics played via a language game related to sharing information about a shape in a shared world.

What kinds of AI human tasks do you think this kind of algorithm could be used for?

For example, do you see it in the domain of the text chat or a robotic setting?

Or just in other words, which application directions do you find promising or most able to demonstrate the differences and strengths of the framework compared with the unilateral supervised or unsupervised learning systems?


SPEAKER_01:
Thank you very much for great questions.

There are actually that I should ask, that I should add in our explanation.

So actually, maybe this experiment itself looks very, how to say, simple and categorical.

categorical sign formation and this but what i want to show in this talk and in this paper is the kind of the very the basic basic validation of this approach co-creative learning

so and this kind of this graphical model and this setting is not limited to the categorical setting and also that you know the small number i i mean the simple visual image

And actually, so as I mentioned, what I did is swapping the one agent in this kind of the language game.

the swapping by a human participant.

So actually, this kind of approach, collective predictive coding-based approach, is now extending to many domains.

So let me share the appendix slide.

So actually, this approach was extended to the multiple agent.

and also the multi-agent repostment learning.

And also, importantly, this is also another preprint we recently shared.

So this approach can be extended to the more complex tasks.

So this is, for example, this paper is dealing with the captioning game, not the naming game, captioning game.

So between the two vision language model agents, one agent receives a natural image and describes it in natural language.

So another agent receives the text.

Text is just a vector, a sequence of categorical variables.

And that can be treated in our framework in a similar way.

So actually, we showed that this kind of process improved, successfully integrated the two agents, the knowledge of the vision-language models.

So I think we are now, many research is now ongoing.

So I think that we can, I believe we can extend this idea to the large language model setting.

and also that we can uh say that now as you know currently in robotics field visual language action model vision language action model is gathering attention so maybe we can uh consider the how can we extend this idea to such kind of robotics scenario as well awesome um


SPEAKER_00:
Okay, next question, and then I'll read from the chat.

Okay.

How do you see this in terms of converging to a shared stable truth versus situations of novelty like co-creation of art or research, where there's not necessarily a pre-existing ground truth, even of a latent kind, to converge to?

I'm thinking about situations where the agent models have overlapping information,

which is redundant though robust or non-overlapping information which is useful though fragile and then also information that maybe neither agent has so how do you adapt the situation to to tune whether you're looking to kind of find an existing compromise

versus move into a third space that might be different than what either agent has or knows.


SPEAKER_01:
Thank you very much for your great question.

And actually, that's very, very, very, very interesting topic and interesting discussion point.

So actually, mathematically speaking, if they don't have the shared observation,

the contain does not contains shared the information so it's very difficult to uh i i i expect that the it's very difficult to form the shared representations and reach agreement in some sense

But I don't have the empirical evidence about that with some experience.

But I think that this is now a very important step to conduct constructive studies to analyze what kind of phenomena happens

with this kind of basic mechanism of symbol emergence and formation of shared representation.

And also now, maybe this is somehow a bit outside of today's topic, but I think that this kind of scenario can be found in the democratic discussion in our society as well.

So I think that one of my students is working on the topic about the relationship between collective predictive coding and democracy and digital democracy.

so the if the two agents don't have the shared experience it's very difficult to have to reach or to reach agreement and also the you know the if they have a very different belief system

it will become very difficult to agree.

This kind of mechanism is somehow existing behind the curtain of the current social division.

I think that currently I'm very interested in the connection between this kind of computational research and the simple emergence of representation learning stuff and social science topics as well.

Thank you very much.


SPEAKER_00:
Yes, that makes a lot of sense.

I guess just speculatively, if two agents are in a shared environment,

with a simple optima to be reached or at least a gradient to be followed, at least there's a norm to which the gradient could be followed or local or global optima could be reached.

But if you have agents that are in two different environments,

you kind of lose even the possibility of a guarantee because if it's like if two people are in two different rooms with two different temperatures there's no guarantee they would converge to the same air conditioning but if they were in the same room with a shared air conditioning at least they could


SPEAKER_01:
yeah okay yeah go ahead i think it's very interesting to think over the kind of how can we how uh should we in how can we improve the design of communication and how can we how uh should we uh consider the the environment the difference of environment uh to achieve the mutual and us to you know uh to


SPEAKER_00:
i'll say to understand the mutual understanding between two or more agents something like that i'm sorry yeah i'll read a question from the live chat if anyone else has a live chat question they should write it so Frederick Quintanar wrote

in your model how do you approach the difference between the way silicon agents and humans model the concept level of words and context ai is opaque how can this be improved for interpretability okay


SPEAKER_01:
Thank you very much.

That's also a very important point.

In our framework, the most important part of our framework is that we totally deal with a system, a kind of decentralized system.

So in one of the most important part of this mechanism, each agent need not look into the internal phenomena of the counterpart agents.

so the even in the computer agent settings so what the agent do is just a kind of the utter the giving sign to speak something and listen and and determine the if it accepts or not

by itself.

So there is no need to look into the other agents' variables.

Actually, in the similar studies in emergent communication, many studies have said, ask

agent to pass the kind of the gradient of the parameter through agents you know so across agents it's impossible for human but uh but uh in this setting uh we don't need to do that so actually that in this mechanism we just consider the counterpart agent as a black box

So what we did in this framework is, in concrete planning, is just swapping the black box by human agent and just expect the human behavior follows the MH naming game-based mechanism, algorithm.

So actually, to validate that, we conducted this experiment previously.

And we somehow found that in this kind of setting, humans actually follow the MH algorithm to some extent.

And also, in this experiment again, we checked the human behavior.

is similar to the image algorithm and as a result that we can expect actually expect that the total the correct free energy can be decreased through this interaction that is a scenario and regarding the explainability there is a very hard say the difficult question and we need to consider

In the series of studies of explainable AI, the definition of explainability

is how say very somehow uncertain you know the pleasure and actually the uh explain how to say uh even the in the current large language model

the explainability, I mean, what's happening inside of the neural network is a very challenging problem.

And I would say the human, I would say,

I think that the similar problem can be found in this scenario as well.

But anyway, so when we introduce this kind of idea to our society, we assume that AI is

not just a follower of students, of human.

So the human decision can be affected by AI.

So maybe, of course, we need to reconsider the issues and explainability based on such kind of assumptions.

Sorry, my answer is not perfect, but that's comment from my side.


SPEAKER_00:
Great.

OK, and one more.

question I wrote in the comment in the chat.

So you mentioned different ways of communication amongst AI, human AI and human-human dyads and groups.

And I think the relevance of understanding and modeling and creating high integrity settings for these communications is clear.

So what might these communication strategies look like?

For example, let's assume that we can do real-time language translation between mother tongues like Japanese or English.

From there, what kinds of games might be possible?

For example, would that high bandwidth communication still look like, how are you doing?

What are you up to?

Or could there be some structured communication games like captioning or joint classification which could engage shared attention in ways that were previously impossible and discover new global free energy optima?


SPEAKER_01:
Thank you very much for your questions.

and try to grasp the point we can do real-time language translation this is a mother tongue like from there what kind of game if everyone could speak and hear in the natural language that was most comfortable to them


SPEAKER_00:
what would be the semantics of the language games that would support high bandwidth communication?


SPEAKER_01:
you mean the... so what does the real-time language translation


SPEAKER_00:
means here so everyone can this this means that everyone can understand so what you can speak you can speak in japanese and hear what i'm saying in japanese and i'll hear what you say in english and i could speak in english so we're using some ai system to auto translate what we hear and say

in close to real time so that so we can have a natural conversation with our mother tongues so then i'm just kind of curious and and wondering whether then we would have

a sort of normal like on the street conversation or whether your work is providing some situations or structured games like kind of like language chess

where we could play a game together like captioning or classification that could be used to reduce the free energy on a joint distribution in a way that would be like faster or different than just asking about your day or your friends or your family


SPEAKER_01:
Thank you for your question.

I'm not sure if I got that point clearly.

But basically, the image naming game, the naming game or captioning game, this is kind of a very, very small subset.

or what the what's happening in our total language game the inner society so the the actually basically the our assumption is that our assumption or our idea is that uh

the language the the formation of language or symbol emergence itself is a part of the adaptation to the environment and uh you know that's totally somehow same as uh collective uh predictive coding

and active influence.

I don't have a clear answer to this question.

know the we share some task we share some task or some environment if we are human that's task so maybe gradually the our way of communication uh gradually change and optimize

to the shared task or shared environment.

That's my idea.

And I don't know the connection with the real-time translation discussion point.

Is it okay?


SPEAKER_00:
I don't know.

I think it's a mystery for us and others to explore in the future.

Like if we were fluent,

in hearing and speaking to everybody what would we talk about like if we knew every language perfectly and we could communicate to to everyone and everyone could be included in the conversation would that conversation look like just meeting somebody at the bus stop or i think your work provides some structured settings where the information content of communication

could be quantitatively analyzed and different contributions and proposals could be understood in terms of how they contribute to co-alignment.

Okay.

Yes.

Last question.

So what are your next research directions or questions and what milestones or developments from your group and colleagues or in the broader AI space are you looking out for?


SPEAKER_01:
OK, thank you very much.

In the relation with the development of AI system, so the one thing is that how can we integrate this idea into LLM, the agent systems, and VLM, and the vision language action model.

And actually, basically, the CPC, the original idea

is CPC and the simple emergent systems.

So I would like to jump out from this kind of agent viewpoint.

But I think that this is crucial to consider how can we form the language system.

or shared the brief system together with other agent i think it's crucial for human intelligence i think and cognition i think and uh the and from this viewpoint we can i think we can integrate the human and ai and robot to this system

so and that can bring us to the next step of ai human and ai symbiotic society so that is important point and the first one step is to the make the this kind of system work the with ai and robot i mean that you know i showed the vlm

the image captioning game system so actually that actually improve the performance of vlm and uh also that i can i think that llm that can can be uh say the you know the kind of the decentralized or much agent lm system can be uh reformulated from this viewpoint

And also, we can put the human agent to such kind of ecosystems.

So of course, the human LLM interaction can take time.

And through that process, LLM can learn from the human participants adaptively.

But at the same time, inevitably, human agents can be affected by LLM's output.

But in many discussions, this kind of phenomena is regarded as just a bias.

Somehow, the bias tends to be considered a negative thing.

But such kind of bias

it can be regarded as a kind of knowledge from AI agent as well.

So I think that one of the most important next step is to formalize this kind of interaction from the viewpoint of the collective free energy minimization and language, image-like language gain.

and produce qualitatively different human AI system.

And that, hopefully, guarantees the co-creative process.

That is the most important step forward.

Yes.

Thank you very much.


SPEAKER_00:
Thank you.

A great way to end it.

I really appreciated the presentation.

I think that the comparison of supervised or unsupervised learning makes it clear that both of the unilateral frameworks have fundamental limitations and that a co-creative path is a great joint distributional future.

Thank you very much.

Thank you.

Okay.

Till next time.

See you.
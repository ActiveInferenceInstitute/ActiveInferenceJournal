1
00:00:06,600 --> 00:00:09,420
olá e bem-vindo, é 18 de setembro de

2
00:00:09,420 --> 00:00:14,040
2023 e está ativo como convidado 57.1

3
00:00:14,040 --> 00:00:16,740
com Andy Keller, estaremos

4
00:00:16,740 --> 00:00:19,619
falando sobre estrutura neural natural

5
00:00:19,619 --> 00:00:22,020
para inteligência artificial,

6
00:00:22,020 --> 00:00:24,300
haverá uma apresentação seguida de uma

7
00:00:24,300 --> 00:00:25,800
discussão, então se você estiver assistindo ao vivo,

8
00:00:25,800 --> 00:00:27,900
sinta-se à vontade para  escreva perguntas

9
00:00:27,900 --> 00:00:30,660
no chat ao vivo, caso contrário, obrigado Andy

10
00:00:30,660 --> 00:00:32,279
por isso, estou realmente ansioso por isso

11
00:00:32,279 --> 00:00:35,899
e por você pela apresentação,

12
00:00:36,360 --> 00:00:38,940
sim, muito obrigado, uh, obrigado por

13
00:00:38,940 --> 00:00:41,280
me receber, estou super animado para poder

14
00:00:41,280 --> 00:00:42,840
apresentar essas coisas com o

15
00:00:42,840 --> 00:00:45,239
grupo de referência ativo.  sou um fã e estou muito

16
00:00:45,239 --> 00:00:49,200
interessado, então espero que sim,

17
00:00:49,200 --> 00:00:50,399
tenha uma boa discussão e veja o que

18
00:00:50,399 --> 00:00:51,960
vocês pensam sobre isso,

19
00:00:51,960 --> 00:00:54,840
então meu nome é Andy, estou terminando

20
00:00:54,840 --> 00:00:57,180
meu doutorado supervisionado por Maxwelling na

21
00:00:57,180 --> 00:00:59,100
Universidade de Amsterdã,

22
00:00:59,100 --> 00:01:01,500
estou  começando um pós-doutorado em Harvard depois

23
00:01:01,500 --> 00:01:05,099
disso, então vou começar, só estou falando

24
00:01:05,099 --> 00:01:07,619
sobre o objetivo do meu trabalho em geral é

25
00:01:07,619 --> 00:01:09,540
tentar aproximar a inteligência artificial moderna

26
00:01:09,540 --> 00:01:12,540
de uma generalização mais semelhante à humana

27
00:01:12,540 --> 00:01:15,240
e então o que queremos dizer com

28
00:01:15,240 --> 00:01:17,159
isso é talvez algum  uma espécie de generalização de estrutura,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
ou talvez mais familiar ao

31
00:01:20,700 --> 00:01:22,080
comitê de bebês ativos, como um

32
00:01:22,080 --> 00:01:24,299
modelo mundial estruturado que acreditamos que os humanos

33
00:01:24,299 --> 00:01:26,340
possuem e a maneira que propomos fazer

34
00:01:26,340 --> 00:01:28,799
isso é integrando a estrutura neural natural

35
00:01:28,799 --> 00:01:32,400
na inteligência artificial,

36
00:01:32,400 --> 00:01:34,979
então primeiro vamos definir o que queremos dizer  por

37
00:01:34,979 --> 00:01:36,720
generalização de estrutura,

38
00:01:36,720 --> 00:01:38,400
então acho que é bastante incontroverso

39
00:01:38,400 --> 00:01:40,380
dizer que o aprendizado de máquina moderno

40
00:01:40,380 --> 00:01:42,900
generaliza além de seu conjunto de treinamento

41
00:01:42,900 --> 00:01:45,000
no sentido tradicional, por exemplo,

42
00:01:45,000 --> 00:01:46,799
mesmo as primeiras redes neurais artificiais

43
00:01:46,799 --> 00:01:49,020
perceptrons multicamadas poderiam

44
00:01:49,020 --> 00:01:51,659
ser treinados em conjuntos de dados de imagens como

45
00:01:51,659 --> 00:01:55,140
este e alcançar alta  precisão, então, quando

46
00:01:55,140 --> 00:01:56,880
eles são apresentados a um

47
00:01:56,880 --> 00:01:58,320
conjunto de imagens de teste que eles nunca viram

48
00:01:58,320 --> 00:02:00,240
antes, eles ainda podem classificá-los com

49
00:02:00,240 --> 00:02:02,460
relativa facilidade com o mesmo nível de

50
00:02:02,460 --> 00:02:04,500
precisão e isso é o que normalmente

51
00:02:04,500 --> 00:02:07,320
chamamos de generalização, embora mesmo no

52
00:02:07,320 --> 00:02:08,758
início fosse  notei que esses

53
00:02:08,758 --> 00:02:10,800
sistemas realmente lutam com pequenos

54
00:02:10,800 --> 00:02:12,720
deslocamentos ou deformações aplicadas às

55
00:02:12,720 --> 00:02:16,340
imagens, por exemplo, se

56
00:02:18,920 --> 00:02:23,099
sim, você pensa por que isso é surpreendente e

57
00:02:23,099 --> 00:02:24,720
eu argumento que é realmente devido à nossa

58
00:02:24,720 --> 00:02:26,640
capacidade inata de realizar esse tipo de

59
00:02:26,640 --> 00:02:28,680
generalização de estrutura que este

60
00:02:28,680 --> 00:02:31,920
exemplo é uma falha de então  uh, por

61
00:02:31,920 --> 00:02:33,239
exemplo, essa mudança é quase

62
00:02:33,239 --> 00:02:35,340
imperceptível para nós e nós a tratamos

63
00:02:35,340 --> 00:02:37,560
automaticamente, enquanto no sistema é

64
00:02:37,560 --> 00:02:39,959
claramente um grande problema, então em palavras

65
00:02:39,959 --> 00:02:41,940
podemos dizer que a generalização da estrutura

66
00:02:41,940 --> 00:02:44,819
é uma generalização para algumas

67
00:02:44,819 --> 00:02:47,040
transformações de simetria da entrada ou, neste

68
00:02:47,040 --> 00:02:48,959
caso, a Simetria  a transformação é uma

69
00:02:48,959 --> 00:02:50,580
pequena mudança que deixa a classe de dígitos

70
00:02:50,580 --> 00:02:52,019
inalterada,

71
00:02:52,019 --> 00:02:54,599
então a questão óbvia é o que

72
00:02:54,599 --> 00:02:56,340
exatamente queremos dizer com essa

73
00:02:56,340 --> 00:02:58,860
estrutura natural e por que achamos que isso

74
00:02:58,860 --> 00:03:01,620
nos ajudaria com essas configurações,

75
00:03:01,620 --> 00:03:03,780
então primeiro vamos falar sobre o que queremos dizer

76
00:03:03,780 --> 00:03:05,819
com neural natural  estrutura,

77
00:03:05,819 --> 00:03:08,700
uma maneira de falar sobre estrutura ou

78
00:03:08,700 --> 00:03:11,640
qualquer tipo de viés em um sistema é um

79
00:03:11,640 --> 00:03:14,040
viés indutivo e, portanto, um viés indutivo

80
00:03:14,040 --> 00:03:16,080
pode ser definido vagamente como uma

81
00:03:16,080 --> 00:03:17,940
restrição apropriada de um conjunto de

82
00:03:17,940 --> 00:03:19,440
hipóteses realizáveis ​​quando você está fazendo a

83
00:03:19,440 --> 00:03:22,440
seleção de modelos de forma mais coloquial, nós  pode chamar

84
00:03:22,440 --> 00:03:24,480
isso de algo como antes de ver qualquer

85
00:03:24,480 --> 00:03:27,060
dadoé uma restrição do que e como

86
00:03:27,060 --> 00:03:29,580
você pode aprender de forma muito ampla isso pode

87
00:03:29,580 --> 00:03:32,519
incluir qualquer coisa desde classe de modelo até

88
00:03:32,519 --> 00:03:34,739
procedimentos de otimização ou até mesmo

89
00:03:34,739 --> 00:03:37,379
hiperparâmetros e em certo sentido eles realmente

90
00:03:37,379 --> 00:03:39,739
definem o

91
00:03:39,739 --> 00:03:43,140
que é possível aprender  e define

92
00:03:43,140 --> 00:03:44,819
generalização no sentido de que

93
00:03:44,819 --> 00:03:47,220
você realmente não pode generalizar Além de um

94
00:03:47,220 --> 00:03:48,420
conjunto de treinamento sem ter alguma

95
00:03:48,420 --> 00:03:50,220
solução indutiva, isso é explicado

96
00:03:50,220 --> 00:03:52,560
mais detalhadamente neste artigo por David

97
00:03:52,560 --> 00:03:55,500
Wolford, então o que queremos dizer com

98
00:03:55,500 --> 00:03:58,620
preconceitos indutivos naturais são preconceitos que

99
00:03:58,620 --> 00:04:00,000
resultam de restrições e

100
00:04:00,000 --> 00:04:02,040
limitações  que são enfrentados pelos

101
00:04:02,040 --> 00:04:04,500
sistemas naturais, uh, pela natureza de ter que

102
00:04:04,500 --> 00:04:06,900
viver no mundo real, por exemplo, o

103
00:04:06,900 --> 00:04:08,459
cérebro tem muitas restrições de eficiência

104
00:04:08,459 --> 00:04:10,439
e restrições físicas pela natureza de

105
00:04:10,439 --> 00:04:13,140
sua construção, uh e seguindo essa

106
00:04:13,140 --> 00:04:14,580
lógica, então essas restrições estão realmente

107
00:04:14,580 --> 00:04:16,620
desempenhando algum papel em nossa generalização

108
00:04:16,620 --> 00:04:19,798
habilidades que atualmente excedem a

109
00:04:19,798 --> 00:04:21,478
inteligência artificial moderna, como veremos a

110
00:04:21,478 --> 00:04:24,720
seguir. Nesta palestra, focarei

111
00:04:24,720 --> 00:04:27,540
especificamente em dois tipos de estrutura

112
00:04:27,540 --> 00:04:29,880
que meu trabalho estudou:

113
00:04:29,880 --> 00:04:31,699
organização topográfica e

114
00:04:31,699 --> 00:04:34,320
dinâmica espaço-temporal e antes de

115
00:04:34,320 --> 00:04:36,120
iniciar meu trabalho,  darei um breve exemplo

116
00:04:36,120 --> 00:04:38,759
de por que acredito que a estrutura natural

117
00:04:38,759 --> 00:04:41,400
pode ser útil para alcançar a

118
00:04:41,400 --> 00:04:42,780
generalização da estrutura da qual eu estava falando

119
00:04:42,780 --> 00:04:44,280
antes,

120
00:04:44,280 --> 00:04:47,479
então o primeiro exemplo vem da arquitetura

121
00:04:47,479 --> 00:04:49,199
frontal neocognitiva de uh hukushima

122
00:04:49,199 --> 00:04:51,479
da década de 1980,

123
00:04:51,479 --> 00:04:53,520
que na verdade foi construída para

124
00:04:53,520 --> 00:04:55,440
abordar diretamente  o problema da

125
00:04:55,440 --> 00:04:57,300
robustez a essas pequenas mudanças e

126
00:04:57,300 --> 00:04:59,759
deformações, então na papelada ele

127
00:04:59,759 --> 00:05:02,040
escreve sobre a inspiração das

128
00:05:02,040 --> 00:05:04,380
medidas de hierarquia e agrupamento do aluno e da doninha,

129
00:05:04,380 --> 00:05:06,780
a fim de alcançar robustez

130
00:05:06,780 --> 00:05:08,699
a essas distorções e, portanto, se você olhar

131
00:05:08,699 --> 00:05:11,160
para a figura, se ele escrever U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 e representam células simples e

133
00:05:14,100 --> 00:05:16,919
complexas e, portanto, esta é uma

134
00:05:16,919 --> 00:05:18,840
abordagem bastante radical na época, mas

135
00:05:18,840 --> 00:05:20,699
realmente serviu para melhorar a robustez e as

136
00:05:20,699 --> 00:05:22,199
mudanças que estamos atormentando essas primeiras

137
00:05:22,199 --> 00:05:24,419
redes neurais artificiais e, com o tempo,

138
00:05:24,419 --> 00:05:25,860
essas ideias foram simplificadas e

139
00:05:25,860 --> 00:05:28,919
abstraídas e  obviamente produziu as

140
00:05:28,919 --> 00:05:30,539
redes neurais convolucionais, então sabemos

141
00:05:30,539 --> 00:05:32,759
hoje o que levou ao sucesso

142
00:05:32,759 --> 00:05:35,580
da revolução do aprendizado profundo, então este

143
00:05:35,580 --> 00:05:36,960
é realmente um exemplo de um

144
00:05:36,960 --> 00:05:39,479
viés indutivo natural que alcançou a

145
00:05:39,479 --> 00:05:42,120
generalização da estrutura, então para nossa pesquisa é

146
00:05:42,120 --> 00:05:43,919
realmente de maior interesse tentar

147
00:05:43,919 --> 00:05:45,900
entender o que faz isso  modelos funcionam

148
00:05:45,900 --> 00:05:47,280
tão bem

149
00:05:47,280 --> 00:05:49,320
e veja se este princípio pode

150
00:05:49,320 --> 00:05:51,360
ser potencialmente generalizado para cobrir

151
00:05:51,360 --> 00:05:53,940
transformações e simetrias mais abstratas,

152
00:05:53,940 --> 00:05:57,000


153
00:05:57,000 --> 00:06:00,360
então o que faz uma convolução atingir essa

154
00:06:00,360 --> 00:06:02,060
generalização de estrutura

155
00:06:02,060 --> 00:06:04,440
intuitivamente, você pode ver que isso é feito

156
00:06:04,440 --> 00:06:06,720
aplicando o mesmo filtro em ou ou

157
00:06:06,720 --> 00:06:08,699
extrator de recursos em  vários

158
00:06:08,699 --> 00:06:10,680
locais espaciais, então aqui vemos um único

159
00:06:10,680 --> 00:06:12,660
filtro convolucional sendo aplicado em

160
00:06:12,660 --> 00:06:14,820
todos os locais de uma imagem, isso significa

161
00:06:14,820 --> 00:06:16,259
que não importa onde sua entrada esteja,

162
00:06:16,259 --> 00:06:18,000
seja no meio

163
00:06:18,000 --> 00:06:20,400
da imagem ou à direita, você terá

164
00:06:20,400 --> 00:06:22,080
exatamente os mesmos recursos  com uma

165
00:06:22,080 --> 00:06:23,580
exceção, eles serão deslocados de forma equivalente,

166
00:06:23,580 --> 00:06:24,660


167
00:06:24,660 --> 00:06:26,819
então matematicamente esse tipo de mapeamento

168
00:06:26,819 --> 00:06:29,160
é chamado de homomorfismo, ele preserva

169
00:06:29,160 --> 00:06:30,840
a estrutura algébrica do

170
00:06:30,840 --> 00:06:33,180
espaço de entrada e do espaço de saída, neste caso

171
00:06:33,180 --> 00:06:35,580
é em relação à tradução e em

172
00:06:35,580 --> 00:06:37,440
um nível simples, algo como

173
00:06:37,440 --> 00:06:38,880
o que será importante lembrar

174
00:06:38,880 --> 00:06:40,259
no restante desta palestra é que podemos

175
00:06:40,259 --> 00:06:42,300
verificar os homomorfismos do nosso

176
00:06:42,300 --> 00:06:45,000
extrator de recursos se pudermos ver que há

177
00:06:45,000 --> 00:06:46,740
essa comutação comunitária com o

178
00:06:46,740 --> 00:06:49,560
diagrama comutativo de Transformações e,

179
00:06:49,560 --> 00:06:51,720
portanto, podemos escrever isso também algebricamente,

180
00:06:51,720 --> 00:06:53,759
mostrando que o extrator de recursos  f

181
00:06:53,759 --> 00:06:55,080
comuta com o

182
00:06:55,080 --> 00:06:57,000
operador de transformação t

183
00:06:57,000 --> 00:06:58,919
e basicamente o que queremos é que

184
00:06:58,919 --> 00:07:00,600
não haja diferença entre primeiro

185
00:07:00,600 --> 00:07:02,639
extrair os recursos e depois

186
00:07:02,639 --> 00:07:04,740
realizar a transformação ou

187
00:07:04,740 --> 00:07:06,240
realizar a transformação e depois

188
00:07:06,240 --> 00:07:08,639
extrair os recursos, então o desafio

189
00:07:08,639 --> 00:07:10,199
é até o momento,

190
00:07:10,199 --> 00:07:11,880
não sabemos realmente  como construir

191
00:07:11,880 --> 00:07:13,620
homomorfismos em relação a

192
00:07:13,620 --> 00:07:15,240
transformações mais complexas que vemos

193
00:07:15,240 --> 00:07:18,060
no mundo real, por exemplo, nosso cérebro é

194
00:07:18,060 --> 00:07:20,099
capaz de lidar com mudanças na iluminação e na

195
00:07:20,099 --> 00:07:22,259
estação naturalmente,

196
00:07:22,259 --> 00:07:24,599
então aqui vemos a iluminação no rosto de uma pessoa

197
00:07:24,599 --> 00:07:26,160
ou a mudança das estações, podemos

198
00:07:26,160 --> 00:07:27,840
dizer que é  a mesma face ou o mesmo

199
00:07:27,840 --> 00:07:29,880
caminho mas não sabemos como construir

200
00:07:29,880 --> 00:07:31,319
modelos que respeitem essas

201
00:07:31,319 --> 00:07:33,180
Transformações e por isso nos torna difícil

202
00:07:33,180 --> 00:07:35,520
construir sistemas que as lidem de forma

203
00:07:35,520 --> 00:07:37,620
robusta e previsível

204
00:07:37,620 --> 00:07:40,259
para dar um exemplo ainda mais abstrato do

205
00:07:40,259 --> 00:07:41,759
que eu  quero dizer com isso e as potenciais

206
00:07:41,759 --> 00:07:43,620
repercussões negativas de modelos que

207
00:07:43,620 --> 00:07:45,440
não lidam com simetria As transformações

208
00:07:45,440 --> 00:07:48,060
consideram programas modernos de geração de texto em imagem

209
00:07:48,060 --> 00:07:50,520
então neste exemplo pedi à

210
00:07:50,520 --> 00:07:53,940
Dolly para gerar uma imagem de um

211
00:07:53,940 --> 00:07:55,620
ursinho de pelúcia na lua e ela faz isso

212
00:07:55,620 --> 00:07:57,180
incrivelmente bem, certo  provavelmente melhor do

213
00:07:57,180 --> 00:08:00,960
que eu, tem textura

214
00:08:00,960 --> 00:08:03,360
incrivelmente detalhada, no entanto, se eu pedir para você

215
00:08:03,360 --> 00:08:05,340
fazer algo que vejo ser

216
00:08:05,340 --> 00:08:08,039
conceitualmente mais simples, como desenhar um

217
00:08:08,039 --> 00:08:10,560
cubo azul em cima de um cubo vermelho, ele não consegue fazer

218
00:08:10,560 --> 00:08:13,380
isso e para mim isso não parece intuitivo,

219
00:08:13,380 --> 00:08:15,300
pois  a segunda tarefa parece

220
00:08:15,300 --> 00:08:18,180
significativamente mais fácil mas o que estou a

221
00:08:18,180 --> 00:08:19,860
argumentar é que a razão pela qual isto é

222
00:08:19,860 --> 00:08:21,599
surpreendente é precisamente a mesma razão

223
00:08:21,599 --> 00:08:23,580
pela qual o exemplo da tradução da amnistia foi

224
00:08:23,580 --> 00:08:25,560
surpreendente existe esta

225
00:08:25,560 --> 00:08:28,020
transformação de simetria A acontecer Aqui nomeadamente a

226
00:08:28,020 --> 00:08:29,400
transformação entre estes

227
00:08:29,400 --> 00:08:31,740
objectos complexos de um ursinho de peluche  e a lua e

228
00:08:31,740 --> 00:08:34,320
esses objetos simples de Cubos que

229
00:08:34,320 --> 00:08:36,360
intuitivamente esperamos que a rede seja

230
00:08:36,360 --> 00:08:38,820
capaz de manipular e respeitar e vemos

231
00:08:38,820 --> 00:08:40,919
que não é assim, assim como o

232
00:08:40,919 --> 00:08:43,380
trabalho de Fukushima mostrou que essas

233
00:08:43,380 --> 00:08:46,200
estruturas naturais de hierarquia e

234
00:08:46,200 --> 00:08:47,700
agrupamento de nosso sistema visual são

235
00:08:47,700 --> 00:08:49,680
eficaz para fazer generalizações para

236
00:08:49,680 --> 00:08:52,380
pequenas transformações Eu argumento que uma

237
00:08:52,380 --> 00:08:54,060
estrutura de nível potencialmente superior pode

238
00:08:54,060 --> 00:08:55,740
ser necessária para corrigir esses

239
00:08:55,740 --> 00:08:58,200
problemas abstratos de generalização

240
00:08:58,200 --> 00:09:01,380
e, portanto, a questão que estou

241
00:09:01,380 --> 00:09:04,380
estudando e perguntando é o que

242
00:09:04,380 --> 00:09:06,060
poderia ser essa estrutura e como a

243
00:09:06,060 --> 00:09:08,640
implementamos  isso em uma

244
00:09:08,640 --> 00:09:10,080
arquitetura de rede neural artificial que pode realmente

245
00:09:10,080 --> 00:09:14,120
ser usada para realizar cálculos,

246
00:09:14,880 --> 00:09:17,700
então, para começar a responder, vou pular

247
00:09:17,700 --> 00:09:19,680
para minha primeira linha de trabalho sobre

248
00:09:19,680 --> 00:09:22,380
organização topográfica,

249
00:09:22,380 --> 00:09:25,260
para que a organização topográfica seja

250
00:09:25,260 --> 00:09:27,060
amplamente observada em todo o cérebro, desde o

251
00:09:27,060 --> 00:09:29,760
córtex visual primário Áreas de nível safira  e

252
00:09:29,760 --> 00:09:31,500
pode ser descrito de maneira muito vaga como esta

253
00:09:31,500 --> 00:09:33,540
propriedade de que neurônios que estão próximos

254
00:09:33,540 --> 00:09:35,760
uns dos outros tendem a responder a

255
00:09:35,760 --> 00:09:38,220
coisas semelhantes, por exemplo, à esquerda, mostramos

256
00:09:38,220 --> 00:09:39,720
a preferência codificada por cores de cada

257
00:09:39,720 --> 00:09:42,959
neurônio no córtex digital primário como

258
00:09:42,959 --> 00:09:45,360
uma resposta a linhas orientadas  e vemos

259
00:09:45,360 --> 00:09:46,740
esse conjunto de seletividades que variam suavemente,

260
00:09:46,740 --> 00:09:48,779
outro tipo de

261
00:09:48,779 --> 00:09:50,580
organização é conhecido como organização do tópico da retina,

262
00:09:50,580 --> 00:09:52,560
onde neurônios próximos no

263
00:09:52,560 --> 00:09:54,600
córtex visual tendem a responder a

264
00:09:54,600 --> 00:09:56,399
campos receptivos próximos,

265
00:09:56,399 --> 00:09:58,560
no entanto, essa organização não está limitada

266
00:09:58,560 --> 00:10:01,080
a esses recursos de baixo nível, estende-se a alguns

267
00:10:01,080 --> 00:10:02,519
mais complexos  características como aquelas

268
00:10:02,519 --> 00:10:05,459
presentes em rostos, objetos ou lugares

269
00:10:05,459 --> 00:10:07,920
e isso se relaciona com as chamadas

270
00:10:07,920 --> 00:10:10,080
áreas funcionalmente específicas do cérebro,

271
00:10:10,080 --> 00:10:12,779
como a área facial fusiforme FFA e

272
00:10:12,779 --> 00:10:15,420
a área facial paraquipocampal PPA,

273
00:10:15,420 --> 00:10:19,200
portanto, neste trabalho, a ideia principal novamente é

274
00:10:19,200 --> 00:10:21,300
que talvez isso

275
00:10:21,300 --> 00:10:23,580
organização topográfica em certo sentido que está

276
00:10:23,580 --> 00:10:25,080
intimamente relacionada com a

277
00:10:25,080 --> 00:10:27,980
operação de convolução e a arquitetura de Fukushima,

278
00:10:27,980 --> 00:10:30,660
talvez possamos generalizar os benefícios

279
00:10:30,660 --> 00:10:33,420
disso para transformações mais abstratas, em

280
00:10:33,420 --> 00:10:34,920
outras palavras, aprender como construir

281
00:10:34,920 --> 00:10:36,839
homomorfismos mais complexos que não podemos fazer,

282
00:10:36,839 --> 00:10:38,940
você sabe que não podemos  fazer analiticamente agora,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
apenas para mostrar que não somos

285
00:10:42,480 --> 00:10:44,760
completamente loucos com essa ideia,

286
00:10:44,760 --> 00:10:46,320
há alguns trabalhos anteriores neste domínio

287
00:10:46,320 --> 00:10:49,880
de pessoas como, uh, Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden no início dos anos 90 e 2000

289
00:10:54,060 --> 00:10:55,800
e eles estudaram como a

290
00:10:55,800 --> 00:10:57,720
organização topográfica pode ser  útil para aprender

291
00:10:57,720 --> 00:11:01,320
variações principalmente em modelos lineares, então

292
00:11:01,320 --> 00:11:03,060
a questão para nós quando entramos no

293
00:11:03,060 --> 00:11:04,680
espaço é qual é o

294
00:11:04,680 --> 00:11:07,140
mecanismo abstrato mais escalável que pode ser aproveitado

295
00:11:07,140 --> 00:11:08,880
a partir dessas abordagens que podemos

296
00:11:08,880 --> 00:11:10,800
integrar em arquiteturas modernas de redes neurais profundas

297
00:11:10,800 --> 00:11:12,959
e, finalmente,

298
00:11:12,959 --> 00:11:15,000
estabelecemos um  abordagem de modelagem generativa

299
00:11:15,000 --> 00:11:16,260
que eu acho que pode ser

300
00:11:16,260 --> 00:11:17,519
interessante para as pessoas nesta

301
00:11:17,519 --> 00:11:18,779
comunidade,

302
00:11:18,779 --> 00:11:21,779
que nos permite relacioná-la

303
00:11:21,779 --> 00:11:23,579
mais estreitamente com a

304
00:11:23,579 --> 00:11:26,040
análise de componentes topográficos independentes, com a ideia básica

305
00:11:26,040 --> 00:11:28,320
de que podemos aprender uma

306
00:11:28,320 --> 00:11:30,660
característica topográfica do espaço, impondo uma

307
00:11:30,660 --> 00:11:32,940
distribuição topográfica anterior sobre  nossas variáveis ​​latentes,

308
00:11:32,940 --> 00:11:34,440


309
00:11:34,440 --> 00:11:37,079
então, apenas para dar um breve histórico,

310
00:11:37,079 --> 00:11:39,120
presumo que a maioria das pessoas já esteja familiarizada

311
00:11:39,120 --> 00:11:40,440
com isso,

312
00:11:40,440 --> 00:11:42,540
mas o tipo de suposição geral é

313
00:11:42,540 --> 00:11:44,339
que o cérebro é um modelo generativo e

314
00:11:44,339 --> 00:11:45,720
essa ideia, em certo sentido, pode ser

315
00:11:45,720 --> 00:11:48,060
atribuída a helmholts do

316
00:11:48,060 --> 00:11:50,459
século 19, uh  onde ele disse que o que

317
00:11:50,459 --> 00:11:52,140
vemos é a solução para um

318
00:11:52,140 --> 00:11:54,420
problema computacional, nossos cérebros calculam as

319
00:11:54,420 --> 00:11:56,519
causas mais prováveis ​​​​da absorção de fótons

320
00:11:56,519 --> 00:11:59,519
em nossos olhos e isso é um exemplo,

321
00:11:59,519 --> 00:12:01,920
se eu mostrar esta imagem, você imediatamente

322
00:12:01,920 --> 00:12:03,720
a reconhecerá como um globo com alguma

323
00:12:03,720 --> 00:12:05,760
curvatura, no entanto  poderia

324
00:12:05,760 --> 00:12:07,620
igualmente ser um disco com uma

325
00:12:07,620 --> 00:12:09,600
perspectiva distorcida, então é assim que

326
00:12:09,600 --> 00:12:12,660
obtemos ilusões de ótica ou nossas imagens, assim

327
00:12:12,660 --> 00:12:14,880
como esta, seu cérebro infere que

328
00:12:14,880 --> 00:12:17,100
há um cubo aqui por causa da

329
00:12:17,100 --> 00:12:18,720
estrutura, mas na verdade é apenas um

330
00:12:18,720 --> 00:12:19,860
pedaço plano de papel

331
00:12:19,860 --> 00:12:22,740
então você pode pensar neste

332
00:12:22,740 --> 00:12:24,480
aspecto do modelo generativo que é como um

333
00:12:24,480 --> 00:12:26,160
programa gráfico inverso

334
00:12:26,160 --> 00:12:28,140
no programa as propriedades abstratas

335
00:12:28,140 --> 00:12:30,660
da esfera são conhecidas a posição o

336
00:12:30,660 --> 00:12:32,820
tamanho da iluminação e são usadas para

337
00:12:32,820 --> 00:12:34,560
projetar a esfera para criar a

338
00:12:34,560 --> 00:12:37,440
imagem 2D que é renderizada  então, na verdade, o que

339
00:12:37,440 --> 00:12:40,019
Humboldt e outros estão dizendo é que,

340
00:12:40,019 --> 00:12:41,940
como modelo generativo, o cérebro está

341
00:12:41,940 --> 00:12:43,680
na verdade tentando inverter esse

342
00:12:43,680 --> 00:12:45,959
processo generativo e fazendo inferências

343
00:12:45,959 --> 00:12:48,300
e inferindo as causas subjacentes de nossas

344
00:12:48,300 --> 00:12:49,680
sensações,

345
00:12:49,680 --> 00:12:51,600
então a razão pela qual estou elaborando

346
00:12:51,600 --> 00:12:53,100
esse ponto é que há  falamos muito de

347
00:12:53,100 --> 00:12:55,440
modelos generativos hoje,

348
00:12:55,440 --> 00:12:57,180
e não estou necessariamente falando apenas

349
00:12:57,180 --> 00:12:59,639
sobre gerar imagens ou imagens bonitas,

350
00:12:59,639 --> 00:13:01,260


351
00:13:01,260 --> 00:13:03,120
eu realmente quero me referir a

352
00:13:03,120 --> 00:13:07,920
uma estrutura para aprendizado não supervisionado,

353
00:13:07,920 --> 00:13:10,019
então, para entrar um pouco mais em

354
00:13:10,019 --> 00:13:11,700
detalhes, o que quero dizer com um

355
00:13:11,700 --> 00:13:14,279
modelos topográficos anteriores, portanto generativos,

356
00:13:14,279 --> 00:13:16,019
são normalmente descritos como uma

357
00:13:16,019 --> 00:13:18,720
distribuição conjunta sobre as observações X e

358
00:13:18,720 --> 00:13:21,720
variáveis ​​​​latentes que chamaremos de Z uh

359
00:13:21,720 --> 00:13:23,940
e isso normalmente é fatorado ou uma

360
00:13:23,940 --> 00:13:25,620
maneira de fazer isso é fatorada em

361
00:13:25,620 --> 00:13:28,440
termos de um P anterior de Z e isso é verdadeiro

362
00:13:28,440 --> 00:13:30,420
modelo generativo modelo generativo condicional

363
00:13:30,420 --> 00:13:33,420
P de x dado Z e então uma maneira de

364
00:13:33,420 --> 00:13:34,980
pensarmos sobre isso é que o

365
00:13:34,980 --> 00:13:37,019
anterior pode ser visto como codificando

366
00:13:37,019 --> 00:13:38,880
penalidades relativas para cada tipo de código que é

367
00:13:38,880 --> 00:13:41,279
produzido quando invertemos nosso

368
00:13:41,279 --> 00:13:42,899
modelo generativo, isso é chamado de Computação do

369
00:13:42,899 --> 00:13:45,920
posterior P de Z dado X

370
00:13:45,920 --> 00:13:49,139
e, portanto, para desenvolver um espaço topográfico latente,

371
00:13:49,139 --> 00:13:50,639
queremos introduzir algum tipo de

372
00:13:50,639 --> 00:13:53,279
anterior topográfico que foi ou

373
00:13:53,279 --> 00:13:55,620
que este trabalho topográfico do ICA mostrou

374
00:13:55,620 --> 00:13:57,720
ser equivalente a algo como uma

375
00:13:57,720 --> 00:13:59,700
penalidade de dispersão de grupo

376
00:13:59,700 --> 00:14:01,500
para que as pessoas possam estar familiarizadas com o típico

377
00:14:01,500 --> 00:14:03,180
penalidades de dispersão da

378
00:14:03,180 --> 00:14:04,560
análise de Aprendizagem Independente, você deseja que suas

379
00:14:04,560 --> 00:14:06,540
ativações sejam esparsas, o que significa que muitas

380
00:14:06,540 --> 00:14:09,420
delas são zero, uau, e isso poderia ser

381
00:14:09,420 --> 00:14:10,680
algo assim, você tem um monte de

382
00:14:10,680 --> 00:14:12,300
quadrados azuis que estão ativos, mas a maioria

383
00:14:12,300 --> 00:14:14,459
deles não está ativa, mas especificamente

384
00:14:14,459 --> 00:14:16,740
com o grupo  Penalidade do time do colégio, queremos que

385
00:14:16,740 --> 00:14:18,839
esses anteriores atribuam menor probabilidade

386
00:14:18,839 --> 00:14:21,600
a essas ativações esparsas distribuídas

387
00:14:21,600 --> 00:14:24,720
e maior probabilidade a essas

388
00:14:24,720 --> 00:14:26,940
representações agrupadas e densamente compactadas. Você

389
00:14:26,940 --> 00:14:28,860
também pode pensar nisso como uma penalidade mais alta

390
00:14:28,860 --> 00:14:30,720
quando as coisas estão espalhadas, uma

391
00:14:30,720 --> 00:14:33,540
penalidade menor quando as coisas estão mais próximas,

392
00:14:33,540 --> 00:14:36,380
então novamente uh  isso pode ser escrito

393
00:14:36,380 --> 00:14:39,060
abstratamente assim, mas eu quero fazer uma

394
00:14:39,060 --> 00:14:41,160
teoria de que esses neurônios, cada um

395
00:14:41,160 --> 00:14:42,779
desses quadrados aqui, representa uma espécie de

396
00:14:42,779 --> 00:14:44,220
neurônio em nosso modelo e eles estão

397
00:14:44,220 --> 00:14:46,560
organizados nesta grade 2D, então quando

398
00:14:46,560 --> 00:14:48,120
falamos sobre agrupamento, realmente queremos dizer

399
00:14:48,120 --> 00:14:50,820
agrupando nessa topologia 2D,

400
00:14:50,820 --> 00:14:53,100
então uma coisa que é realmente interessante

401
00:14:53,100 --> 00:14:55,560
e importante é que esses

402
00:14:55,560 --> 00:14:57,779
anteriores não apenas nos dão

403
00:14:57,779 --> 00:15:00,540
organização topográfica, mas também foram observados

404
00:15:00,540 --> 00:15:02,459
por pessoas como ou estudados por pessoas como

405
00:15:02,459 --> 00:15:05,760
Erosi Marcelli e Bruno também para

406
00:15:05,760 --> 00:15:07,740
realmente se encaixarem  as estatísticas de

407
00:15:07,740 --> 00:15:08,959
dados naturais são melhores,

408
00:15:08,959 --> 00:15:11,180
especificamente imagens naturais,

409
00:15:11,180 --> 00:15:14,100
eles mostraram que usando esse tipo de

410
00:15:14,100 --> 00:15:16,139
anterior você realmente obtém um conjunto mais esparso de

411
00:15:16,139 --> 00:15:18,839
ativações, o que significa que o anterior se ajusta

412
00:15:18,839 --> 00:15:20,459
um pouco melhor ao verdadeiro processo generativo

413
00:15:20,459 --> 00:15:22,620
e, como sabemos, o cérebro tem

414
00:15:22,620 --> 00:15:24,779
um alto grau de dispersão e

415
00:15:24,779 --> 00:15:26,339
acredita-se que isso seja muito relevante para a

416
00:15:26,339 --> 00:15:28,620
eficiência,

417
00:15:28,620 --> 00:15:30,839
então, para entrar um pouco mais nos

418
00:15:30,839 --> 00:15:32,760
detalhes para implementar esse tipo de

419
00:15:32,760 --> 00:15:35,160
grupo esparso, antes de usarmos um

420
00:15:35,160 --> 00:15:37,320
modelo generativo hierárquico e isso é

421
00:15:37,320 --> 00:15:39,060
basicamente introduzido por alguns dos

422
00:15:39,060 --> 00:15:41,339
trabalho topográfico do ICA,

423
00:15:41,339 --> 00:15:43,320
a ideia é que você tenha uma

424
00:15:43,320 --> 00:15:45,000
variável latente de nível superior U que

425
00:15:45,000 --> 00:15:47,820
regula simultaneamente a variância de

426
00:15:47,820 --> 00:15:50,279
múltiplas variáveis ​​​​de nível inferior T e

427
00:15:50,279 --> 00:15:52,440
é assim que obtemos a dispersão do grupo,

428
00:15:52,440 --> 00:15:55,440
então para obter a organização topográfica você

429
00:15:55,440 --> 00:15:56,760
pode ter múltiplas dessas

430
00:15:56,760 --> 00:15:59,339
variáveis ​​latentes usadas ligeiramente  sobrepondo-se aos

431
00:15:59,339 --> 00:16:02,519
seus campos de influência para que

432
00:16:02,519 --> 00:16:04,260
possamos chamá-los de suas vizinhanças

433
00:16:04,260 --> 00:16:05,699
e isso lhe dará essa

434
00:16:05,699 --> 00:16:07,440
estrutura de correlação suave que você procura,

435
00:16:07,440 --> 00:16:09,899
então tenha a intuição para isso, você

436
00:16:09,899 --> 00:16:12,060
vê que esta variável T

437
00:16:12,060 --> 00:16:14,279
aqui embaixo não está ficando  qualquer entrada

438
00:16:14,279 --> 00:16:17,160
deste U no topo, mas está compartilhando

439
00:16:17,160 --> 00:16:19,139
uma variável u com este T no meio,

440
00:16:19,139 --> 00:16:21,300
então é como se eles estivessem compartilhando variantes,

441
00:16:21,300 --> 00:16:22,980
eles estão compartilhando alguns componentes com

442
00:16:22,980 --> 00:16:24,959
seus vizinhos, mas não todos os componentes

443
00:16:24,959 --> 00:16:26,579
e isso é realmente devido a essa

444
00:16:26,579 --> 00:16:28,079
conectividade local de  essas variáveis ​​de nível superior,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
para simplificar sobre como usamos um

447
00:16:33,180 --> 00:16:34,980
modelo generativo, vamos voltar a uma

448
00:16:34,980 --> 00:16:37,440
única variável U e o desafio

449
00:16:37,440 --> 00:16:38,940
neste tipo de arquitetura que

450
00:16:38,940 --> 00:16:42,360
dificultou por muitos anos é como

451
00:16:42,360 --> 00:16:44,579
você inferir o posterior aproximado

452
00:16:44,579 --> 00:16:47,579
sobre essas variáveis ​​​​intermediárias

453
00:16:47,579 --> 00:16:50,100
nesta arquitetura hierárquica e isso

454
00:16:50,100 --> 00:16:52,560
não é muito simples, então

455
00:16:52,560 --> 00:16:54,420
trabalhos anteriores usaram heurísticas desenvolvidas para

456
00:16:54,420 --> 00:16:56,699
modelos lineares e em nosso trabalho descobrimos

457
00:16:56,699 --> 00:16:58,680
que isso realmente não se estendia às

458
00:16:58,680 --> 00:17:01,199
arquiteturas de redes neurais modernas, então

459
00:17:01,199 --> 00:17:02,880
nosso insight é aproveitar um

460
00:17:02,880 --> 00:17:04,760
fatoração, uma

461
00:17:04,760 --> 00:17:07,640
re-parametrização específica desta distribuição

462
00:17:07,640 --> 00:17:10,380
e, portanto, essa parametrização

463
00:17:10,380 --> 00:17:12,419
específica é alcançada definindo a

464
00:17:12,419 --> 00:17:14,579
prioridade, o que é conhecido como

465
00:17:14,579 --> 00:17:16,319
mistura de escala gaussiana, o que significa que nossa

466
00:17:16,319 --> 00:17:19,140
distribuição condicional de T dado U é

467
00:17:19,140 --> 00:17:21,179
na verdade uma distribuição normal onde

468
00:17:21,179 --> 00:17:24,299
a variância é definida por esta variável

469
00:17:24,299 --> 00:17:27,720
U e para certas escolhas de U esta

470
00:17:27,720 --> 00:17:29,340
distribuição é de fato esparsa e

471
00:17:29,340 --> 00:17:31,980
abrange uma gama de distribuições,

472
00:17:31,980 --> 00:17:33,780
como laplossianas, um naipe e

473
00:17:33,780 --> 00:17:36,299
distribuições T, uma maneira de defini-la é

474
00:17:36,299 --> 00:17:38,940
uma mistura de escala gaussiana que emite uma

475
00:17:38,940 --> 00:17:40,919
reformulação particular, re-parametrização

476
00:17:40,919 --> 00:17:42,900
em termos de variáveis ​​​​aleatórias gaussianas independentes

477
00:17:42,900 --> 00:17:45,720
Z e U tão especificamente,

478
00:17:45,720 --> 00:17:48,840
vemos que esta variável T que era

479
00:17:48,840 --> 00:17:50,760
originalmente bastante complexa é na verdade

480
00:17:50,760 --> 00:17:52,799
apenas um produto de um monte de

481
00:17:52,799 --> 00:17:54,840
variáveis ​​​​aleatórias gaussianas que agora sabem como

482
00:17:54,840 --> 00:17:57,660
trabalhar com muito mais eficiência em

483
00:17:57,660 --> 00:18:00,120
modelos generativos, especificamente o que

484
00:18:00,120 --> 00:18:02,039
estamos  o que vamos fazer é que possamos

485
00:18:02,039 --> 00:18:04,020
realmente obter posteriores aproximados para

486
00:18:04,020 --> 00:18:06,720
U e Z separadamente e então fazer uma

487
00:18:06,720 --> 00:18:08,640
combinação determinística deles

488
00:18:08,640 --> 00:18:10,020
para calcular nossa

489
00:18:10,020 --> 00:18:13,140
variável topográfica T e isso é muito mais fácil de fazer

490
00:18:13,140 --> 00:18:15,500
sem entrar em muitos detalhes

491
00:18:15,500 --> 00:18:17,700
o método que  que decidimos usar é o que é

492
00:18:17,700 --> 00:18:18,600
conhecido como

493
00:18:18,600 --> 00:18:20,640
autoencoder variacional, que aproveita técnicas

494
00:18:20,640 --> 00:18:23,220
de inferência variacional para derivar um

495
00:18:23,220 --> 00:18:24,780
limite inferior na probabilidade, permitindo-

496
00:18:24,780 --> 00:18:26,940
nos parametrizar esses

497
00:18:26,940 --> 00:18:29,400
posteriores aproximados com poderosas redes neurais profundas não lineares

498
00:18:29,400 --> 00:18:30,960
e otimizá-los com

499
00:18:30,960 --> 00:18:33,240
gradiente descendente.  estar

500
00:18:33,240 --> 00:18:34,440
familiarizado com a comunidade de inferência ativa,

501
00:18:34,440 --> 00:18:36,900
mas na verdade o que fizemos foi,

502
00:18:36,900 --> 00:18:38,760
em vez de ter um único codificador no

503
00:18:38,760 --> 00:18:41,640
decodificador como um baes típico, agora temos

504
00:18:41,640 --> 00:18:43,799
dois codificadores, um para você e outro para Z

505
00:18:43,799 --> 00:18:46,200
separadamente e então os combinamos

506
00:18:46,200 --> 00:18:48,419
desta maneira determinística para  construa

507
00:18:48,419 --> 00:18:51,660
nossa variável T topográfica se você perceber

508
00:18:51,660 --> 00:18:53,100
que esta é na verdade a

509
00:18:53,100 --> 00:18:54,900
construção de uma distribuição T de um aluno

510
00:18:54,900 --> 00:18:57,620
a partir de gaussianas

511
00:18:57,620 --> 00:19:00,480
e então podemos conectar isso, fazemos isso

512
00:19:00,480 --> 00:19:03,600
antes de decodificar e, uh, e então maximizar

513
00:19:03,600 --> 00:19:05,820
a probabilidade dos dados completamente, então

514
00:19:05,820 --> 00:19:07,260
este é o cotovelo  as evidências do

515
00:19:07,260 --> 00:19:09,360
limite inferior abundam na probabilidade dos

516
00:19:09,360 --> 00:19:12,600
dados e são, na verdade, muito semelhantes à

517
00:19:12,600 --> 00:19:15,720
energia livre variacional que é usada na

518
00:19:15,720 --> 00:19:18,299
comunidade de entrada ativa,

519
00:19:18,299 --> 00:19:20,520
portanto, com esses detalhes fora do

520
00:19:20,520 --> 00:19:22,500
caminho, o que é realmente interessante é o que

521
00:19:22,500 --> 00:19:23,940
acontece quando treinamos esse

522
00:19:23,940 --> 00:19:26,580
modelo generativo que  tem uma penalidade de esparsidade de grupo relativamente simples

523
00:19:26,580 --> 00:19:29,460
em seu espaço latente e

524
00:19:29,460 --> 00:19:30,720
queremos ver o que ele está

525
00:19:30,720 --> 00:19:32,700
aprendendo em termos de organização de

526
00:19:32,700 --> 00:19:34,980
Futuros e primeiro começamos com o

527
00:19:34,980 --> 00:19:36,480
conjunto de dados mais simples possível, temos um

528
00:19:36,480 --> 00:19:38,580
fundo preto com quadrados brancos em

529
00:19:38,580 --> 00:19:41,280
locais XY aleatórios  e se treinarmos nosso

530
00:19:41,280 --> 00:19:42,780
autoencoder com esta penalidade de dispersão de grupo

531
00:19:42,780 --> 00:19:44,760
nele e então olharmos para os

532
00:19:44,760 --> 00:19:47,640
vetores de peso do nosso decodificador que

533
00:19:47,640 --> 00:19:49,020
estamos plotando em azul aqui novamente

534
00:19:49,020 --> 00:19:52,520
organizados nesta grade 2D, vemos que

535
00:19:52,520 --> 00:19:54,780
de fato eles aprendem a ser organizados

536
00:19:54,780 --> 00:19:57,539
de acordo com o espaço  localização, então isso

537
00:19:57,539 --> 00:19:59,580
pode ser visto como semelhante aos

538
00:19:59,580 --> 00:20:01,799
campos receptivos convolucionais ou o campo receptivo

539
00:20:01,799 --> 00:20:04,679
de cada neurônio é realmente dado pelo

540
00:20:04,679 --> 00:20:09,059
tipo de entradas em sua localização

541
00:20:09,059 --> 00:20:10,860
e isso faz sentido intuitivamente da

542
00:20:10,860 --> 00:20:13,140
perspectiva de dispersão do grupo, uma vez que

543
00:20:13,140 --> 00:20:15,480
para qualquer região ser destacada como

544
00:20:15,480 --> 00:20:17,700
em  amarelo aqui, os filtros em um determinado

545
00:20:17,700 --> 00:20:19,260
grupo são muito mais correlacionados,

546
00:20:19,260 --> 00:20:20,880
eles têm esses campos receptivos sobrepostos

547
00:20:20,880 --> 00:20:23,460
do que outros locais aleatórios, então,

548
00:20:23,460 --> 00:20:25,020
essencialmente, vemos que nosso modelo está

549
00:20:25,020 --> 00:20:27,840
aprendendo a agrupar atividades de atividade,

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
uh, em uma espécie de folha vertical simulada

552
00:20:32,059 --> 00:20:34,260
de acordo com as correlações em  o

553
00:20:34,260 --> 00:20:36,840
conjunto de dados, então, em vez de na convolução,

554
00:20:36,840 --> 00:20:38,580
onde você está realmente amarrando o peso

555
00:20:38,580 --> 00:20:40,860
e especificando manualmente, quero

556
00:20:40,860 --> 00:20:42,539
copiar esse peso em todos os lugares, talvez você possa

557
00:20:42,539 --> 00:20:44,220
pensar nisso como um

558
00:20:44,220 --> 00:20:45,500
tempo de espera aproximado

559
00:20:45,500 --> 00:20:48,120
e realmente estamos aprendendo isso com a

560
00:20:48,120 --> 00:20:49,620
correlação  estrutura do próprio conjunto de dados

561
00:20:49,620 --> 00:20:51,660
e apenas para dar um pouco

562
00:20:51,660 --> 00:20:54,120
mais de inspiração biológica para

563
00:20:54,120 --> 00:20:56,280
isso e sabemos que a retinotopia está

564
00:20:56,280 --> 00:20:58,020
presente no cérebro este é um exemplo

565
00:20:58,020 --> 00:21:02,460
de retinotopia no córtex visual e

566
00:21:02,460 --> 00:21:04,500
você pode ver se mostrar ao macaco uma

567
00:21:04,500 --> 00:21:06,780
uma imagem como esta é projetada

568
00:21:06,780 --> 00:21:08,520
nesta

569
00:21:08,520 --> 00:21:11,700
topologia, preservando o espaço, na verdade,

570
00:21:11,700 --> 00:21:13,740
na superfície do córtex,

571
00:21:13,740 --> 00:21:16,080
então a ideia é que a

572
00:21:16,080 --> 00:21:18,419
organização topográfica e até mesmo aprender a

573
00:21:18,419 --> 00:21:21,299
organização topográfica está preservando as

574
00:21:21,299 --> 00:21:26,160
correlações de entrada de nossos conjuntos de dados, uh e,

575
00:21:26,160 --> 00:21:28,679
potencialmente, uh, isso pode ser benéfico

576
00:21:28,679 --> 00:21:30,840
para  generalizando essas ideias um

577
00:21:30,840 --> 00:21:32,340
pouco mais, como eu disse no

578
00:21:32,340 --> 00:21:34,679
início, seria ainda melhor se

579
00:21:34,679 --> 00:21:37,200
pudéssemos aprender algo mais do que

580
00:21:37,200 --> 00:21:39,320
apenas convolução, talvez equivariâncias mais complicadas,

581
00:21:39,320 --> 00:21:43,679
então como fazemos isso? Uma

582
00:21:43,679 --> 00:21:45,720
coisa que está clara na

583
00:21:45,720 --> 00:21:48,299
inteligência natural é que nós  não existem

584
00:21:48,299 --> 00:21:51,120
neste mundo de quadros IID, certo, existimos

585
00:21:51,120 --> 00:21:53,520
em um mundo com sequências contínuas de

586
00:21:53,520 --> 00:21:55,620
Transformações, então talvez possamos estender

587
00:21:55,620 --> 00:21:58,440
nosso modelo para este cenário para aprender a

588
00:21:58,440 --> 00:22:01,080
observar Transformações, esta é uma ideia

589
00:22:01,080 --> 00:22:03,299
de coerência temporal,

590
00:22:03,299 --> 00:22:05,280
então o que aconteceria se simplesmente

591
00:22:05,280 --> 00:22:08,280
estendemos nossa estrutura anterior ao longo do

592
00:22:08,280 --> 00:22:10,620
tempo Dimensão certa, então em vez de apenas

593
00:22:10,620 --> 00:22:13,080
agrupar dizendo que queremos que nossos neurônios

594
00:22:13,080 --> 00:22:15,059
sejam esparsos em termos de

595
00:22:15,059 --> 00:22:17,400
extensão espacial no córtex, na verdade queremos que

596
00:22:17,400 --> 00:22:18,960
eles sejam esparsos em grupo ao longo do tempo, o que

597
00:22:18,960 --> 00:22:20,640
significa que se um conjunto de neurônios estiver

598
00:22:20,640 --> 00:22:22,559
ativo  agora queremos que o mesmo conjunto de

599
00:22:22,559 --> 00:22:24,360
neurônios esteja ativo no futuro também,

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
se olharmos se

602
00:22:27,840 --> 00:22:30,600
pensarmos intuitivamente sobre isso, veremos que isso é na

603
00:22:30,600 --> 00:22:33,059
verdade mais encorajador invariância e

604
00:22:33,059 --> 00:22:35,039
equivariância uma maneira de entender isso é que estamos

605
00:22:35,039 --> 00:22:37,140
dizendo que  quero que os mesmos neurônios

606
00:22:37,140 --> 00:22:39,179
estejam ativos constantemente, mas a

607
00:22:39,179 --> 00:22:41,280
transformação de entrada está mudando exatamente quando os

608
00:22:41,280 --> 00:22:44,220
pés desta pequena raposa estão se movendo, então se

609
00:22:44,220 --> 00:22:45,960
os mesmos neurônios estão codificando a mesma

610
00:22:45,960 --> 00:22:47,880
coisa repetidamente, mas os pés

611
00:22:47,880 --> 00:22:49,320
estão se movendo, esses neurônios vão

612
00:22:49,320 --> 00:22:51,360
aprender  ser invariante ao movimento

613
00:22:51,360 --> 00:22:53,880
daquela perna deste cachorro, por exemplo,

614
00:22:53,880 --> 00:22:57,539
então, em vez disso, é que opa,

615
00:22:57,539 --> 00:23:01,200
eu segui o caminho errado aqui, uh, então,

616
00:23:01,200 --> 00:23:04,860
em vez disso, uh, nosso insight foi que esse

617
00:23:04,860 --> 00:23:06,659
grupo começa a ser poderia, em vez disso, ser

618
00:23:06,659 --> 00:23:09,059
deslocado em relação ao tempo, então isso

619
00:23:09,059 --> 00:23:10,980
significaria  que

620
00:23:10,980 --> 00:23:13,080
conjuntos de ativações deslocados sequencialmente seriam encorajados

621
00:23:13,080 --> 00:23:15,179
a serem ativados juntos e então nosso

622
00:23:15,179 --> 00:23:16,440
espaço latente seria realmente estruturado em

623
00:23:16,440 --> 00:23:18,000
relação às transformações observadas,

624
00:23:18,000 --> 00:23:19,980
então você pode ver aqui que, em vez de o

625
00:23:19,980 --> 00:23:21,480
mesmo conjunto de neurônios estar ativo em todas as

626
00:23:21,480 --> 00:23:23,340
etapas de tempo, é realmente um conjunto sequencial de ativações.

627
00:23:23,340 --> 00:23:24,900
conjunto permutado de neurônios que estamos

628
00:23:24,900 --> 00:23:27,780
agrupando desta forma esparsa

629
00:23:27,780 --> 00:23:29,940
e então isso nos permite modelar

630
00:23:29,940 --> 00:23:33,419
diferentes observações ao longo do tempo, mas

631
00:23:33,419 --> 00:23:34,860
eles ainda estão conectados em termos de

632
00:23:34,860 --> 00:23:36,960
aprender uma transformação e preservar

633
00:23:36,960 --> 00:23:38,340
essa estrutura de correlação da

634
00:23:38,340 --> 00:23:40,020
empatia,

635
00:23:40,020 --> 00:23:41,940
então se nós  junte isso em nossa

636
00:23:41,940 --> 00:23:44,400
arquitetura topográfica Bae você pode obter

637
00:23:44,400 --> 00:23:46,020
algo parecido com isto você vê

638
00:23:46,020 --> 00:23:48,120
que temos uma sequência de entrada estamos

639
00:23:48,120 --> 00:23:51,240
novamente codificando uma variável z e então

640
00:23:51,240 --> 00:23:53,520
múltiplas variáveis ​​U no denominador

641
00:23:53,520 --> 00:23:55,740
aqui e então cada uma dessas

642
00:23:55,740 --> 00:23:58,620
variáveis ​​U é deslocada  uh, mais ou menos como

643
00:23:58,620 --> 00:24:00,480
estávamos mostrando antes, para alcançar

644
00:24:00,480 --> 00:24:02,820
essa estrutura de equivariância de deslocamento

645
00:24:02,820 --> 00:24:04,740
que estamos procurando quando combinamos

646
00:24:04,740 --> 00:24:07,080
isso nesta distribuição do produto T do aluno,

647
00:24:07,080 --> 00:24:09,240
obtemos uma única

648
00:24:09,240 --> 00:24:10,740
variável latente, esta é agora nossa

649
00:24:10,740 --> 00:24:13,860
variável topográfica T e agora que nós  temos essa

650
00:24:13,860 --> 00:24:16,140
estrutura conhecida em nosso espaço latente, você

651
00:24:16,140 --> 00:24:17,460
pode pensar nisso como um modelo de mundo estruturado,

652
00:24:17,460 --> 00:24:19,919
sabemos como transformar esse

653
00:24:19,919 --> 00:24:21,659
espaço latente, neste caso, é

654
00:24:21,659 --> 00:24:23,580
permutando essas ativações em torno desses

655
00:24:23,580 --> 00:24:25,860
círculos, fazendo como um papel cíclico, uma

656
00:24:25,860 --> 00:24:28,380
mudança cíclica, sabemos que isso é

657
00:24:28,380 --> 00:24:30,120
corresponderá às nossas transformações de entrada aprendidas

658
00:24:30,120 --> 00:24:32,640
e podemos verificar isso

659
00:24:32,640 --> 00:24:34,620
dizendo ok, e se eu continuar esta

660
00:24:34,620 --> 00:24:36,480
transformação de entrada, a verdadeira

661
00:24:36,480 --> 00:24:38,100
transformação no conjunto de dados que é

662
00:24:38,100 --> 00:24:40,559
uma rotação e então comparo isso com a

663
00:24:40,559 --> 00:24:42,659
forma como desempenhei meu papel no final  Espaço

664
00:24:42,659 --> 00:24:44,700
movendo minhas ativações em meu

665
00:24:44,700 --> 00:24:47,280
cérebro e então decodificamos e vemos que

666
00:24:47,280 --> 00:24:49,919
obtemos exatamente a mesma coisa e isso

667
00:24:49,919 --> 00:24:52,140
está demonstrando essa

668
00:24:52,140 --> 00:24:53,580
propriedade de comutalidade da qual eu estava falando antes

669
00:24:53,580 --> 00:24:56,820
para verificar o homomorfismo

670
00:24:56,820 --> 00:24:58,799
e medir isso com um pouco mais

671
00:24:58,799 --> 00:25:02,460
de qualidade  quantitativamente, uh, podemos medir

672
00:25:02,460 --> 00:25:04,440
o que é chamado de perda de equivariância, então

673
00:25:04,440 --> 00:25:07,080
esta é realmente a quantificação

674
00:25:07,080 --> 00:25:09,360
dessa diferença entre a

675
00:25:09,360 --> 00:25:12,120
ativação de nossa cápsula enrolada ou o rolamento em nossas

676
00:25:12,120 --> 00:25:15,059
cabeças versus observar o desenrolar do movimento

677
00:25:15,059 --> 00:25:16,559
e para frente, eles estão observando a

678
00:25:16,559 --> 00:25:19,440
transformação se desenrolar diante de nós, então

679
00:25:19,440 --> 00:25:21,600
vemos o topográfico  Bae atinge um

680
00:25:21,600 --> 00:25:24,000


681
00:25:24,000 --> 00:25:26,700
erro de equivariância significativamente menor, esta bolha é o que eu estava

682
00:25:26,700 --> 00:25:27,960
falando antes, onde está aprendendo

683
00:25:27,960 --> 00:25:29,820
invariância, então não tem a

684
00:25:29,820 --> 00:25:32,340
operação de mudança e a vae tradicional meio

685
00:25:32,340 --> 00:25:35,640
que não tem noção de organização ou

686
00:25:35,640 --> 00:25:37,380
componente temporal, então o desempenho é muito

687
00:25:37,380 --> 00:25:40,320
ruim além disso  para isso, vemos que

688
00:25:40,320 --> 00:25:41,700
o modelo é um modelo generativo melhor

689
00:25:41,700 --> 00:25:45,059
de sequências, ele apenas fica menor, uh,

690
00:25:45,059 --> 00:25:48,179
menor, como log de probabilidade negativa uh no

691
00:25:48,179 --> 00:25:50,100
conjunto de dados, então é mais capaz de

692
00:25:50,100 --> 00:25:51,720
modelar esse conjunto de dados porque tem uma

693
00:25:51,720 --> 00:25:52,919
noção da estrutura do

694
00:25:52,919 --> 00:25:55,460
transformações,

695
00:25:55,980 --> 00:25:58,140
uh, podemos testar isso em vários

696
00:25:58,140 --> 00:25:59,760
tipos de transformação diferentes e na

697
00:25:59,760 --> 00:26:00,840
linha superior estamos mostrando a verdadeira

698
00:26:00,840 --> 00:26:02,880
transformação, retiramos essas

699
00:26:02,880 --> 00:26:05,039
imagens acinzentadas e, em seguida, na

700
00:26:05,039 --> 00:26:07,080
linha inferior codificamos e então apenas

701
00:26:07,080 --> 00:26:08,700
rolamos nossas ativações e continuamos

702
00:26:08,700 --> 00:26:12,140
decodificação para ver o que o modelo

703
00:26:12,140 --> 00:26:15,000
aprendeu como a

704
00:26:15,000 --> 00:26:17,039
transformação atual que está sendo observada e

705
00:26:17,039 --> 00:26:19,340
vemos que ele pode basicamente

706
00:26:19,340 --> 00:26:21,360
reconstruir perfeitamente esses elementos da

707
00:26:21,360 --> 00:26:23,640
sequência que nunca foi visto antes,

708
00:26:23,640 --> 00:26:25,260
além disso, com imagens que são do

709
00:26:25,260 --> 00:26:26,580
conjunto de teste que nunca foi visto  antes,

710
00:26:26,580 --> 00:26:28,380
simplesmente porque sabe qual

711
00:26:28,380 --> 00:26:29,760
é a transformação que está

712
00:26:29,760 --> 00:26:31,500
codificando atualmente, ele pode generalizar isso para novos

713
00:26:31,500 --> 00:26:33,919
exemplos,

714
00:26:34,020 --> 00:26:36,360
então a conclusão desta parte é realmente uma

715
00:26:36,360 --> 00:26:38,039
organização topográfica, mostramos que

716
00:26:38,039 --> 00:26:40,080
uma estrutura de entrada preservada e agora

717
00:26:40,080 --> 00:26:41,940
estamos mostrando que pode potencialmente melhorar a

718
00:26:41,940 --> 00:26:44,279
eficiência e a generalização  como esperamos,

719
00:26:44,279 --> 00:26:46,200


720
00:26:46,200 --> 00:26:48,600
finalmente algo que nos surpreendeu

721
00:26:48,600 --> 00:26:49,980
e que pensei ser potencialmente o mais

722
00:26:49,980 --> 00:26:52,500
interessante é que essas

723
00:26:52,500 --> 00:26:53,700
transformações que são aprendidas pelo nosso

724
00:26:53,700 --> 00:26:54,960
modelo na verdade generalizam as

725
00:26:54,960 --> 00:26:57,059
combinações de transformações que

726
00:26:57,059 --> 00:26:59,580
não estamos vendo durante o treinamento, por

727
00:26:59,580 --> 00:27:02,100
exemplo, apesar de apenas treinar  em

728
00:27:02,100 --> 00:27:04,200
transformações de cor e rotação e

729
00:27:04,200 --> 00:27:06,419
isolamento se o modelo for apresentado com

730
00:27:06,419 --> 00:27:08,340
uma transformação de rotação de cores combinada

731
00:27:08,340 --> 00:27:11,100
no tempo de teste, vemos que ele é capaz

732
00:27:11,100 --> 00:27:13,140
de modelar completamente e completar essas

733
00:27:13,140 --> 00:27:14,700
transformações perfeitamente através da

734
00:27:14,700 --> 00:27:17,159
função de cápsula, o que implica que ele aprendeu

735
00:27:17,159 --> 00:27:19,620
a fatorar representar esses

736
00:27:19,620 --> 00:27:20,880
diferentes  Transformações e pode

737
00:27:20,880 --> 00:27:24,600
combiná-las de forma flexível no momento da inferência,

738
00:27:24,600 --> 00:27:28,140
então, novamente, talvez também não obtenhamos apenas

739
00:27:28,140 --> 00:27:29,820
eficiência oficial na generalização,

740
00:27:29,820 --> 00:27:34,100
também obtenhamos alguma composicionalidade básica,

741
00:27:34,260 --> 00:27:36,059
então vamos falar sobre as limitações e o

742
00:27:36,059 --> 00:27:38,460
que poderíamos fazer a seguir.

743
00:27:38,460 --> 00:27:40,620


744
00:27:40,620 --> 00:27:44,159
transformação predefinida que estamos impondo no

745
00:27:44,159 --> 00:27:46,500
espaço e no tempo, embora tenhamos

746
00:27:46,500 --> 00:27:49,080
nos libertado das transformações de grupo e,

747
00:27:49,080 --> 00:27:52,440
especificamente, como a tradução ou

748
00:27:52,440 --> 00:27:53,940
rotação, como é feito atualmente no

749
00:27:53,940 --> 00:27:55,559
mundo do aprendizado de máquina,

750
00:27:55,559 --> 00:27:59,240
ainda temos esse

751
00:27:59,240 --> 00:28:01,980
papel latente codificado em nosso  cabeça para tudo o que

752
00:28:01,980 --> 00:28:03,900
vemos e para tornar isso um pouco

753
00:28:03,900 --> 00:28:05,700
mais flexível, então esperamos que possamos modelar

754
00:28:05,700 --> 00:28:08,880
uma maior diversidade de Transformações,

755
00:28:08,880 --> 00:28:10,980
achamos que talvez possamos nos

756
00:28:10,980 --> 00:28:13,860
inspirar em dinâmicas espaciais e temporais mais estruturadas

757
00:28:13,860 --> 00:28:15,600
que são observadas

758
00:28:15,600 --> 00:28:18,120
no cérebro e isso nos leva  para

759
00:28:18,120 --> 00:28:20,400
a segunda parte desta palestra que é, uh,

760
00:28:20,400 --> 00:28:22,140
dinâmica espacial-temporal que

761
00:28:22,140 --> 00:28:23,039
vamos tentar integrar em

762
00:28:23,039 --> 00:28:25,200
redes neurais artificiais, um exemplo

763
00:28:25,200 --> 00:28:27,059
disso são ondas viajantes como mostrei

764
00:28:27,059 --> 00:28:28,020
aqui,

765
00:28:28,020 --> 00:28:30,600
então o que queremos dizer com isso, uh, bem,

766
00:28:30,600 --> 00:28:32,279
aqui está um muito  artigo recente onde eles

767
00:28:32,279 --> 00:28:36,059
usaram um fmri de nove Tesla operando com

768
00:28:36,059 --> 00:28:38,700
resolução de 36 milissegundos para obter imagens de uma única

769
00:28:38,700 --> 00:28:40,980
fatia de um cérebro de rato sob anestesia

770
00:28:40,980 --> 00:28:43,320
e o que vemos é essa

771
00:28:43,320 --> 00:28:45,720
atividade temporal espacial muito claramente estruturada e

772
00:28:45,720 --> 00:28:48,299
correlações e esses autores do

773
00:28:48,299 --> 00:28:50,520
artigo passam a analisar isso  atividade em

774
00:28:50,520 --> 00:28:52,919
termos dos modos principais representados

775
00:28:52,919 --> 00:28:55,799
à direita, então nossa hipótese é que

776
00:28:55,799 --> 00:28:57,179
talvez algum tipo de

777
00:28:57,179 --> 00:28:59,039
estrutura de correlação como essa possa ser benéfica

778
00:28:59,039 --> 00:29:01,260
para estruturar as representações de

779
00:29:01,260 --> 00:29:03,240
nosso modelo em relação às

780
00:29:03,240 --> 00:29:05,100
transformações observadas, mas de uma forma muito mais

781
00:29:05,100 --> 00:29:07,440
flexível do que simplesmente  apenas uma

782
00:29:07,440 --> 00:29:10,700
mudança cíclica como estávamos fazendo antes,

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
e deixe-me dizer que isso não é

785
00:29:15,900 --> 00:29:19,320
observado apenas em ratos de um ssi, uh,

786
00:29:19,320 --> 00:29:20,940
você pode ver essas ondas viajantes

787
00:29:20,940 --> 00:29:24,179
acontecendo no córtex Mt de

788
00:29:24,179 --> 00:29:27,600
primatas com comportamento acordado, por exemplo,

789
00:29:27,600 --> 00:29:29,580
à esquerda  aqui eles mostram ondas viajantes

790
00:29:29,580 --> 00:29:31,740
que realmente

791
00:29:31,740 --> 00:29:35,520
mudam Qual a probabilidade de um primata ver um

792
00:29:35,520 --> 00:29:38,279
estímulo de baixo contraste com base na fase

793
00:29:38,279 --> 00:29:40,980
da onda, além disso, eles mostram que

794
00:29:40,980 --> 00:29:43,500
um estímulo de alto contraste à

795
00:29:43,500 --> 00:29:45,779
direita pode induzir uma atividade de onda viajante

796
00:29:45,779 --> 00:29:47,520
que se propaga para fora  mesmo

797
00:29:47,520 --> 00:29:50,039
no córtex visual primário, então eles são

798
00:29:50,039 --> 00:29:52,140
realmente onipresentes em todo o cérebro

799
00:29:52,140 --> 00:29:54,000
em vários níveis e seria

800
00:29:54,000 --> 00:29:55,440
interessante estudar quais são suas

801
00:29:55,440 --> 00:29:58,140
implicações para, uh,

802
00:29:58,140 --> 00:29:59,700
aprendizagem de representação de estrutura em nosso caso ou

803
00:29:59,700 --> 00:30:01,799
ou geralmente

804
00:30:01,799 --> 00:30:04,140
há trabalhos anteriores que estudaram

805
00:30:04,140 --> 00:30:06,720
esses tipos de uh Dinâmica  e eles

806
00:30:06,720 --> 00:30:08,700
constroem modelos, então no topo essas são as

807
00:30:08,700 --> 00:30:10,380
equações que descrevem uma

808
00:30:10,380 --> 00:30:12,600
rede neural com picos que eles mostram se você

809
00:30:12,600 --> 00:30:15,720
implementar atrasos de tempo, na verdade

810
00:30:15,720 --> 00:30:18,240
atrasos de tempo axonais entre neurônios, você obtém

811
00:30:18,240 --> 00:30:20,820
essas dinâmicas de estrutura de

812
00:30:20,820 --> 00:30:22,440
ondas viajantes, desde que o tamanho da sua rede seja

813
00:30:22,440 --> 00:30:24,059
grande o suficiente

814
00:30:24,059 --> 00:30:26,520
hum, no entanto, como muitas pessoas provavelmente sabem,

815
00:30:26,520 --> 00:30:28,620
é relativamente desafiador treinar

816
00:30:28,620 --> 00:30:31,320
redes neurais com picos do mesmo tamanho

817
00:30:31,320 --> 00:30:34,820
e desempenho que redes neurais profundas da

818
00:30:34,820 --> 00:30:37,679
mesma forma na parte inferior, outro sistema

819
00:30:37,679 --> 00:30:39,539
que é significativamente mais simples, mas

820
00:30:39,539 --> 00:30:42,840
talvez muito simples, uh é uma rede de

821
00:30:42,840 --> 00:30:45,120
osciladores acoplados, estes são conhecidos por

822
00:30:45,120 --> 00:30:48,779
exibem sincronia e dinâmica espacial-temporal

823
00:30:48,779 --> 00:30:52,200
e padrões complexos, mas, uh,

824
00:30:52,200 --> 00:30:53,520
isso é chamado de sistema de redução de fase

825
00:30:53,520 --> 00:30:55,500
e não captura

826
00:30:55,500 --> 00:30:57,059
toda a complexidade em que estamos interessados,

827
00:30:57,059 --> 00:30:58,140
então estamos olhando para algo que está

828
00:30:58,140 --> 00:31:00,779
potencialmente entre esses dois

829
00:31:00,779 --> 00:31:03,600
e o que nós  O que foi decidido é que este trabalho

830
00:31:03,600 --> 00:31:06,600
neste trabalho é parametrizar uma

831
00:31:06,600 --> 00:31:08,520
rede de alguns osciladores

832
00:31:08,520 --> 00:31:10,620
um pouco mais flexível do que um

833
00:31:10,620 --> 00:31:12,360
modelo paramoto, então isso é realmente

834
00:31:12,360 --> 00:31:14,580
construído nesta

835
00:31:14,580 --> 00:31:16,380
rede neural recorrente destilatória de Constantine

836
00:31:16,380 --> 00:31:18,720
rush e Nisha

837
00:31:18,720 --> 00:31:20,760
um, onde eles basicamente pegaram a

838
00:31:20,760 --> 00:31:22,200
equação que  descreve um

839
00:31:22,200 --> 00:31:23,820
oscilador harmônico simples é uma

840
00:31:23,820 --> 00:31:26,159
equação diferencial de segunda ordem a aceleração

841
00:31:26,159 --> 00:31:29,940
de uma bola em uma mola é proporcional ao

842
00:31:29,940 --> 00:31:32,480
seu deslocamento

843
00:31:32,480 --> 00:31:35,220
uh você pode adicionar termos adicionais como

844
00:31:35,220 --> 00:31:37,260
amortecimento para que as oscilações

845
00:31:37,260 --> 00:31:39,360
morram lentamente com o tempo

846
00:31:39,360 --> 00:31:41,580
você pode acionar este oscilador com um

847
00:31:41,580 --> 00:31:43,380
externo  entrada para neutralizar

848
00:31:43,380 --> 00:31:45,179
esse amortecimento ou para dar um pouco mais de

849
00:31:45,179 --> 00:31:47,279
complexidade à dinâmica

850
00:31:47,279 --> 00:31:49,260
e, além disso, se você tiver muitos

851
00:31:49,260 --> 00:31:50,940
desses osciladores, você pode acoplá-los

852
00:31:50,940 --> 00:31:53,000
com essas matrizes de acoplamento.

853
00:31:53,000 --> 00:31:55,320
Como demonstramos nesta

854
00:31:55,320 --> 00:31:56,640
imagem aqui, você pode realmente  pense

855
00:31:56,640 --> 00:31:58,140
nesta rede como um monte de bolhas em

856
00:31:58,140 --> 00:31:59,940
Springs e elas talvez estejam conectadas

857
00:31:59,940 --> 00:32:01,740
umas às outras também por Springs ou elásticos,

858
00:32:01,740 --> 00:32:03,600
seja qual for o casal,

859
00:32:03,600 --> 00:32:05,279
rede neural recorrente destilatória do russo

860
00:32:05,279 --> 00:32:08,100
Mishra uh com esses vários termos e

861
00:32:08,100 --> 00:32:09,899
isso se mostrou muito poderoso

862
00:32:09,899 --> 00:32:12,480
para modelar sequências longas, eles também

863
00:32:12,480 --> 00:32:13,740
mencionaram que foram inspirados pelo

864
00:32:13,740 --> 00:32:15,360
cérebro construindo isso e há muitas

865
00:32:15,360 --> 00:32:17,700
análises boas naquele artigo. Por

866
00:32:17,700 --> 00:32:19,020
exemplo, eles mostram que essas são

867
00:32:19,020 --> 00:32:21,440
propriedades realmente benéficas em relação aos

868
00:32:21,440 --> 00:32:23,460
problemas de gradiente de desaparecimento que

869
00:32:23,460 --> 00:32:25,440
normalmente acontecem em redes neurais recorrentes,

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
hum  mas se quisermos olhar para a

872
00:32:29,159 --> 00:32:30,960
dinâmica espacial-temporal e este tipo de

873
00:32:30,960 --> 00:32:32,820
modelo, uh, é um pouco desafiador

874
00:32:32,820 --> 00:32:34,919
porque essas matrizes de acoplamento aqui, os

875
00:32:34,919 --> 00:32:36,320
W's

876
00:32:36,320 --> 00:32:39,600
que conectam cada neural ou cada

877
00:32:39,600 --> 00:32:41,240
oscilador estão posicionados um ao outro,

878
00:32:41,240 --> 00:32:43,620
essas são matrizes densamente conectadas

879
00:32:43,620 --> 00:32:45,120
como eu tentei  represente à esquerda

880
00:32:45,120 --> 00:32:46,020
aqui,

881
00:32:46,020 --> 00:32:48,299
então se você tentar visualizar a dinâmica

882
00:32:48,299 --> 00:32:50,580
desta rede você não vê nenhuma

883
00:32:50,580 --> 00:32:51,899
organização espacial, não há

884
00:32:51,899 --> 00:32:55,380
herança, é um pedido de desculpas ao

885
00:32:55,380 --> 00:32:57,000
espaço latente deste modelo,

886
00:32:57,000 --> 00:32:58,799
então você pode pensar nisso como em nosso

887
00:32:58,799 --> 00:33:01,020
exemplo anterior, um neurônio  está conectado

888
00:33:01,020 --> 00:33:03,240
a um conjunto potencialmente arbitrário de outros

889
00:33:03,240 --> 00:33:04,919
neurônios, esses neurônios estão conectados a

890
00:33:04,919 --> 00:33:06,600
outro conjunto arbitrário de neurônios e

891
00:33:06,600 --> 00:33:08,520
você obterá dinâmica oscilatória

892
00:33:08,520 --> 00:33:10,860
certamente, mas tipos de flutuações que

893
00:33:10,860 --> 00:33:13,260
não fazem muito sentido estruturado,

894
00:33:13,260 --> 00:33:15,360
então em nosso trabalho pensamos  ok, como

895
00:33:15,360 --> 00:33:18,299
podemos converter isso mais para os tipos de

896
00:33:18,299 --> 00:33:19,860
dinâmica que estamos interessados ​​nesta

897
00:33:19,860 --> 00:33:22,140
propagação estruturada de atividade,

898
00:33:22,140 --> 00:33:23,940
e uma maneira clara de fazer isso é ter

899
00:33:23,940 --> 00:33:26,539
uma matriz de conectividade mais estruturada,

900
00:33:26,539 --> 00:33:29,880
que descobrimos ser facilmente implementada

901
00:33:29,880 --> 00:33:31,559
e eficiente  implementado através de uma

902
00:33:31,559 --> 00:33:33,000
operação de convolução que você pode

903
00:33:33,000 --> 00:33:34,620
pensar como uma camada local, uma

904
00:33:34,620 --> 00:33:36,299
camada conectada localmente, então, em vez de ter

905
00:33:36,299 --> 00:33:37,860
todos os neurônios conectados, todos os

906
00:33:37,860 --> 00:33:39,480
neurônios dos neurônios estão apenas conectados aos seus

907
00:33:39,480 --> 00:33:41,580
vizinhos próximos. Após o treinamento,

908
00:33:41,580 --> 00:33:42,840
você acabará obtendo algo que

909
00:33:42,840 --> 00:33:44,880
parece um espaço suave  Dinâmica temporal,

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
então, para ser um pouco mais claro no

912
00:33:48,419 --> 00:33:50,519
treinamento deste modelo, pegamos esta

913
00:33:50,519 --> 00:33:52,200
equação diferencial de segunda ordem separada que

914
00:33:52,200 --> 00:33:54,299
estávamos descrevendo antes de discretizá-

915
00:33:54,299 --> 00:33:56,340
la em duas equações de primeira ordem, você

916
00:33:56,340 --> 00:33:57,960
pode pensar nisso como se fosse

917
00:33:57,960 --> 00:34:01,200
integrar numericamente a ode, agora temos um

918
00:34:01,200 --> 00:34:03,120
velocidade e então atualizamos

919
00:34:03,120 --> 00:34:06,000
uh e podemos treinar este modelo como

920
00:34:06,000 --> 00:34:07,620
algo como um codificador automático ou um

921
00:34:07,620 --> 00:34:09,839
modelo auto regressivo, então se pegarmos uma

922
00:34:09,839 --> 00:34:11,460
entrada, nós a codificamos para nosso espaço latente,

923
00:34:11,460 --> 00:34:14,339
na verdade, a entrada é Dr é este termo f de x

924
00:34:14,339 --> 00:34:16,500
que atua  como o termo de acionamento, então

925
00:34:16,500 --> 00:34:18,599
é como acionar esses osciladores de

926
00:34:18,599 --> 00:34:20,879
baixo para cima, uh, e então eles têm sua

927
00:34:20,879 --> 00:34:23,099
própria dinâmica, que é definida pelos

928
00:34:23,099 --> 00:34:25,800
termos de acoplamento, esses acoplamentos locais e,

929
00:34:25,800 --> 00:34:27,540
a cada passo de tempo, pegamos esse

930
00:34:27,540 --> 00:34:29,460
estado latente, esse estado de onda e

931
00:34:29,460 --> 00:34:31,560
decodificamos para tentar  e reconstruir a entrada

932
00:34:31,560 --> 00:34:33,540
e estar no passo de tempo atual ou em um

933
00:34:33,540 --> 00:34:35,699
passo de tempo futuro,

934
00:34:35,699 --> 00:34:37,980
podemos fazer algumas análises desses

935
00:34:37,980 --> 00:34:42,300
modelos durante o treinamento para ver o que

936
00:34:42,300 --> 00:34:43,619
acontece antes do treinamento e depois do

937
00:34:43,619 --> 00:34:45,780
treinamento podemos calcular a fase e

938
00:34:45,780 --> 00:34:47,399
a velocidade da Dinâmica no

939
00:34:47,399 --> 00:34:49,379
espaço latente basicamente vemos no

940
00:34:49,379 --> 00:34:51,480
início da negociação que não há ondas em

941
00:34:51,480 --> 00:34:53,699
nosso modelo, mas após o treinamento após 50

942
00:34:53,699 --> 00:34:55,500
épocas, vemos que há uma

943
00:34:55,500 --> 00:34:57,119
atividade estruturada suave se propagando

944
00:34:57,119 --> 00:35:00,420
para baixo, uh, a serviço dessa

945
00:35:00,420 --> 00:35:01,800
tarefa de modelagem de sequência que estamos fazendo como

946
00:35:01,800 --> 00:35:04,380
girar objetos,

947
00:35:04,380 --> 00:35:05,940
hum, então  qual é o benefício disso,

948
00:35:05,940 --> 00:35:07,680
quero dizer, todo o motivo pelo qual motivei isso

949
00:35:07,680 --> 00:35:10,380
foi para dizer que queríamos ter uma

950
00:35:10,380 --> 00:35:11,880
estrutura aprendida com mais flexibilidade, estamos

951
00:35:11,880 --> 00:35:13,020
realmente fazendo isso ou estamos apenas

952
00:35:13,020 --> 00:35:15,060
recebendo ondas bonitas,

953
00:35:15,060 --> 00:35:16,859
então o que mostramos em nosso artigo é

954
00:35:16,859 --> 00:35:19,320
que realmente somos  aprendendo algum tipo de

955
00:35:19,320 --> 00:35:20,940
estrutura útil e a maneira como mostramos

956
00:35:20,940 --> 00:35:22,440
isso novamente com algo como este

957
00:35:22,440 --> 00:35:24,960
diagrama comutativo se você pegar uma entrada

958
00:35:24,960 --> 00:35:27,000
e codificá-la e obter um

959
00:35:27,000 --> 00:35:29,280
estado de onda e então propagar ondas

960
00:35:29,280 --> 00:35:31,619
artificialmente nesse estado de onda e então

961
00:35:31,619 --> 00:35:33,480
decodificar você pode  observe que

962
00:35:33,480 --> 00:35:35,220
na verdade é exatamente o mesmo que se você tivesse

963
00:35:35,220 --> 00:35:37,140
mostrado um monte de

964
00:35:37,140 --> 00:35:39,180
imagens diferentes de transformações diferentes, então

965
00:35:39,180 --> 00:35:41,640
muitos dígitos diferentes,

966
00:35:41,640 --> 00:35:43,920
características diferentes e vemos que obtemos

967
00:35:43,920 --> 00:35:46,140
diferentes tipos de atividade de onda em cada

968
00:35:46,140 --> 00:35:47,880
caso, a fim de modelar isso  transformação diferente

969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
se treinarmos em conjuntos de dados diferentes

971
00:35:51,119 --> 00:35:53,400
também, da mesma forma, veremos dinâmicas mais complexas,

972
00:35:53,400 --> 00:35:55,200
neste caso, talvez nem mesmo

973
00:35:55,200 --> 00:35:57,839
ondas viajantes ou ondas estacionárias, que

974
00:35:57,839 --> 00:36:00,359
podem ser consideradas como ondas viajantes em

975
00:36:00,359 --> 00:36:02,339
direções opostas, então vemos se estamos

976
00:36:02,339 --> 00:36:04,079
modelando esses orbitais  Dinâmica, obtemos

977
00:36:04,079 --> 00:36:06,000
esse tipo de bolhas de atividade em movimento suave

978
00:36:06,000 --> 00:36:07,619
em nosso espaço latente, se estivermos

979
00:36:07,619 --> 00:36:09,839
modelando um pêndulo, da mesma forma, obteremos um

980
00:36:09,839 --> 00:36:13,820
tipo de atividade oscilatória complexa,

981
00:36:14,099 --> 00:36:17,339
portanto, sua estrutura de entrada é preservada, mas

982
00:36:17,339 --> 00:36:19,560
além disso, mais flexibilidade do que

983
00:36:19,560 --> 00:36:21,599
tínhamos antes, o que é nosso objetivo final

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
então, finalmente, quero falar um pouco sobre

986
00:36:26,099 --> 00:36:28,320
como acho que o resultado desta pesquisa

987
00:36:28,320 --> 00:36:30,420
pode não apenas melhorar a

988
00:36:30,420 --> 00:36:32,099
inteligência artificial, mas também como nos ajuda a

989
00:36:32,099 --> 00:36:34,440
entender por que nossas medições do

990
00:36:34,440 --> 00:36:36,240
cérebro têm a aparência que têm, para dar um

991
00:36:36,240 --> 00:36:38,579
breve exemplo do que eu  quero dizer com isso, uh,

992
00:36:38,579 --> 00:36:41,040
eu falei um pouco antes sobre vistos

993
00:36:41,040 --> 00:36:43,740
e lugares, então neste trabalho fantástico

994
00:36:43,740 --> 00:36:46,859
com Ching higao estudamos se nosso

995
00:36:46,859 --> 00:36:48,900
anterior topográfico simples, como discutimos,

996
00:36:48,900 --> 00:36:50,579
pode ser capaz de reproduzir esses mesmos

997
00:36:50,579 --> 00:36:53,339
efeitos, então especificamente colocamos o valor

998
00:36:53,339 --> 00:36:56,099
deste D de Cohen  métrica de seletividade para

999
00:36:56,099 --> 00:36:58,200
cada um de nossos neurônios em relação a um

1000
00:36:58,200 --> 00:37:00,000
conjunto de dados diferente de imagens potencialmente

1001
00:37:00,000 --> 00:37:02,460
contendo apenas rostos ou apenas objetos ou

1002
00:37:02,460 --> 00:37:03,359
corpos

1003
00:37:03,359 --> 00:37:05,880
e assim medimos para cada neurônio se é

1004
00:37:05,880 --> 00:37:07,920
mais provável que ele responda a rostos ou o

1005
00:37:07,920 --> 00:37:10,380
russo emerge no cérebro, mas eu faço

1006
00:37:10,380 --> 00:37:12,839
acho que isso nos diz que a

1007
00:37:12,839 --> 00:37:15,300
organização relativa da seletividade pode ser pelo menos

1008
00:37:15,300 --> 00:37:17,400
parcialmente atribuível às

1009
00:37:17,400 --> 00:37:19,800
estatísticas de correlação nos dados que precisam ser repassados

1010
00:37:19,800 --> 00:37:21,359
após passarem por um

1011
00:37:21,359 --> 00:37:23,640
extrator futuro altamente não linear, como uma

1012
00:37:23,640 --> 00:37:25,440
rede neural profunda,

1013
00:37:25,440 --> 00:37:27,480
de maneira semelhante, algo que é

1014
00:37:27,480 --> 00:37:29,040
interessante, há um conhecido que é

1015
00:37:29,040 --> 00:37:30,900
chamado de fluxo visual tripartido ou não,

1016
00:37:30,900 --> 00:37:36,720
então uh imagens de uh ou objetos são

1017
00:37:36,720 --> 00:37:38,400
a seletividade em relação aos objetos

1018
00:37:38,400 --> 00:37:40,680
é organizada por propriedades mais abstratas,

1019
00:37:40,680 --> 00:37:43,200
como animação, essa coisa está viva ou

1020
00:37:43,200 --> 00:37:46,200
inanimada uh versus também o

1021
00:37:46,200 --> 00:37:48,480
tamanho do objeto do mundo real, como o que  é o tamanho de um

1022
00:37:48,480 --> 00:37:50,700
bule versus um carro

1023
00:37:50,700 --> 00:37:53,520
e o que vemos é que nos

1024
00:37:53,520 --> 00:37:56,160
humanos esta seletividade é organizada

1025
00:37:56,160 --> 00:37:57,540
nesta estrutura tripartida você

1026
00:37:57,540 --> 00:37:59,760
normalmente tem pequenos objetos que estão

1027
00:37:59,760 --> 00:38:01,920
entre objetos animados e inanimados

1028
00:38:01,920 --> 00:38:04,200
em termos de sua seletividade e vemos

1029
00:38:04,200 --> 00:38:06,060
a mesma coisa acontece aqui, então

1030
00:38:06,060 --> 00:38:07,440
eles estão medindo a seletividade

1031
00:38:07,440 --> 00:38:08,880
do mesmo conjunto de neurônios, mas com relação

1032
00:38:08,880 --> 00:38:10,859
a essas diferenças de estímulos você vê

1033
00:38:10,859 --> 00:38:12,440
que o pequeno cluster está entre o

1034
00:38:12,440 --> 00:38:14,820
cluster animado e o inanimado e novamente

1035
00:38:14,820 --> 00:38:16,079
isso acontece para várias

1036
00:38:16,079 --> 00:38:18,900
inicializações diferentes, então isso  é algo que

1037
00:38:18,900 --> 00:38:20,880
espero que possamos explorar um pouco mais para

1038
00:38:20,880 --> 00:38:22,980
esta comunidade. Acho interessante

1039
00:38:22,980 --> 00:38:24,119
porque é

1040
00:38:24,119 --> 00:38:26,220
realmente uma maneira de mostrar que

1041
00:38:26,220 --> 00:38:28,200
construímos um modelo mundial estruturado e,

1042
00:38:28,200 --> 00:38:30,119
potencialmente, esse modelo mundial é

1043
00:38:30,119 --> 00:38:31,980
benéfico para

1044
00:38:31,980 --> 00:38:34,740
representar melhor os dados do mundo real de uma

1045
00:38:34,740 --> 00:38:37,619
forma estruturada e  você obtém menos

1046
00:38:37,619 --> 00:38:39,119
energia livre nesse sentido,

1047
00:38:39,119 --> 00:38:40,619
então

1048
00:38:40,619 --> 00:38:42,300
acho que ao desenvolver esses modelos

1049
00:38:42,300 --> 00:38:44,400
como mostramos aqui, podemos obter

1050
00:38:44,400 --> 00:38:46,500
insights sobre novos mecanismos de como

1051
00:38:46,500 --> 00:38:48,900
essa estrutura emerge, incluindo

1052
00:38:48,900 --> 00:38:50,460
organização topográfica que nunca

1053
00:38:50,460 --> 00:38:52,920
pensamos antes, então Modelo de máquina que eu estava

1054
00:38:52,920 --> 00:38:55,520
olhando  a seletividade de orientação

1055
00:38:55,520 --> 00:38:58,260
dos neurônios que eu não

1056
00:38:58,260 --> 00:39:01,020
esperava que algo

1057
00:39:01,020 --> 00:39:03,420
acontecesse, mas você está vendo

1058
00:39:03,420 --> 00:39:05,339
essas ondas se propagando sobre esta

1059
00:39:05,339 --> 00:39:08,099
superfície vertical simulada e pensei

1060
00:39:08,099 --> 00:39:09,960
ok, talvez eu esteja mostrando imagens giradas,

1061
00:39:09,960 --> 00:39:11,820
talvez isso tenha algum efeito  a

1062
00:39:11,820 --> 00:39:13,740
seletividade de orientação

1063
00:39:13,740 --> 00:39:15,599
e, na verdade, se você entrar e

1064
00:39:15,599 --> 00:39:17,460
medir a seletividade de cada neurônio

1065
00:39:17,460 --> 00:39:18,660
em relação a essas

1066
00:39:18,660 --> 00:39:22,079
linhas orientadas de forma diferente, o que você vê é que é

1067
00:39:22,079 --> 00:39:24,300
surpreendentemente uma reminiscência das

1068
00:39:24,300 --> 00:39:25,859
colunas do tipo Oriente que são vistas no

1069
00:39:25,859 --> 00:39:27,599
córtex visual primário, isso é algo que remonta

1070
00:39:27,599 --> 00:39:29,520
para Hugo e Weasel e isso é algo

1071
00:39:29,520 --> 00:39:30,900
que simplesmente saiu desse modelo

1072
00:39:30,900 --> 00:39:33,060
e o fato de que ele tem a

1073
00:39:33,060 --> 00:39:34,440
estrutura espaço-temporal em relação às

1074
00:39:34,440 --> 00:39:37,619
Transformações, então, claro, esta é

1075
00:39:37,619 --> 00:39:39,599
uma analogia muito grosseira, mas acho que este

1076
00:39:39,599 --> 00:39:40,740
é um exemplo de como  construir esses

1077
00:39:40,740 --> 00:39:42,839
tipos de modelos pode nos ajudar a pensar sobre

1078
00:39:42,839 --> 00:39:45,240
como o cérebro constrói a

1079
00:39:45,240 --> 00:39:46,980
estrutura representacional e o branco, a maneira como ela é

1080
00:39:46,980 --> 00:39:48,660
organizada de uma forma que talvez não tenhamos

1081
00:39:48,660 --> 00:39:51,300
pensado antes,

1082
00:39:51,300 --> 00:39:53,460
hum, acho que não sou o único que está

1083
00:39:53,460 --> 00:39:55,859
fazendo esse tipo de  trabalho e então eu

1084
00:39:55,859 --> 00:39:57,240
quero falar um pouco sobre algumas

1085
00:39:57,240 --> 00:39:59,579
outras pessoas que estão fazendo isso uh então

1086
00:39:59,579 --> 00:40:00,780
eu tenho falado sobre essa

1087
00:40:00,780 --> 00:40:02,760
estrutura equivalente

1088
00:40:02,760 --> 00:40:04,920
hum pessoas como James Whittington e

1089
00:40:04,920 --> 00:40:08,880
Tim Barons e surrogengoolie uh

1090
00:40:08,880 --> 00:40:10,680
mostraram recentemente que ao introduzir

1091
00:40:10,680 --> 00:40:14,940
algébrica  restrições em uh em um

1092
00:40:14,940 --> 00:40:17,040
processo de aprendizagem, neste caso, foi

1093
00:40:17,040 --> 00:40:20,820
como o movimento de uh e e agentes em

1094
00:40:20,820 --> 00:40:23,220
um ambiente, dizendo que você precisa

1095
00:40:23,220 --> 00:40:24,780
preservar esse tipo de

1096
00:40:24,780 --> 00:40:27,540
estrutura algébrica se eu me mover em um círculo Oeste

1097
00:40:27,540 --> 00:40:29,280
Nordeste Sul, acabo voltando ao

1098
00:40:29,280 --> 00:40:31,440
mesmo  ponto Novamente, ao introduzir esses

1099
00:40:31,440 --> 00:40:32,820
tipos de restrições,

1100
00:40:32,820 --> 00:40:35,040
você obtém o surgimento de representações semelhantes a células de grade,

1101
00:40:35,040 --> 00:40:36,900


1102
00:40:36,900 --> 00:40:39,359
então eu estaria interessado em ver como essa

1103
00:40:39,359 --> 00:40:41,880
ideia de estrutura representacional pode

1104
00:40:41,880 --> 00:40:43,980
nos ajudar a explicar talvez mais do que nossas

1105
00:40:43,980 --> 00:40:45,480
descobertas científicas que estamos descobrindo também,

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
hum e  e como isso se relaciona com os

1108
00:40:48,359 --> 00:40:51,540
modelos generativos como um

1109
00:40:51,540 --> 00:40:52,800
todo, e finalmente acho que há

1110
00:40:52,800 --> 00:40:54,599
algo a ser dito sobre a

1111
00:40:54,599 --> 00:40:56,099
possibilidade cognitiva desses modelos também,

1112
00:40:56,099 --> 00:40:57,420
talvez não vamos testá-

1113
00:40:57,420 --> 00:40:59,579
los apenas da perspectiva da neurociência,

1114
00:40:59,579 --> 00:41:01,020
mas também da perspectiva da ciência micrognitiva

1115
00:41:01,020 --> 00:41:03,839
por exemplo, há essas

1116
00:41:03,839 --> 00:41:06,000
matrizes progressivas Ravens à esquerda,

1117
00:41:06,000 --> 00:41:08,640
onde você tem que dizer qual

1118
00:41:08,640 --> 00:41:11,099
dessas imagens tem maior probabilidade de se encaixar

1119
00:41:11,099 --> 00:41:12,599
neste padrão

1120
00:41:12,599 --> 00:41:14,760
ou, por exemplo, qual é a probabilidade de

1121
00:41:14,760 --> 00:41:16,740
esta Torre Jenga cair quando você

1122
00:41:16,740 --> 00:41:19,740
encosta um puxão  um bloco específico ou

1123
00:41:19,740 --> 00:41:22,740
com uma determinada estrutura e acho que essas

1124
00:41:22,740 --> 00:41:24,599
coisas são

1125
00:41:24,599 --> 00:41:26,640
esses tipos de testes que estão realmente testando

1126
00:41:26,640 --> 00:41:28,619
se nossos modelos de mundo que estamos construindo

1127
00:41:28,619 --> 00:41:31,560
são semelhantes aos tipos de modelos que

1128
00:41:31,560 --> 00:41:33,660
temos inatamente em nosso próprio senso comum

1129
00:41:33,660 --> 00:41:36,060
como humanos ou  como seres que vivem em um

1130
00:41:36,060 --> 00:41:38,400
mundo natural e eu fiz alguns

1131
00:41:38,400 --> 00:41:40,440
trabalhos preliminares nessa direção,

1132
00:41:40,440 --> 00:41:43,079
acho muito, uh, preliminar e não tão

1133
00:41:43,079 --> 00:41:45,480
complicado, mas uma espécie de tentativa

1134
00:41:45,480 --> 00:41:47,820
de modelar ilusões visuais, então se você pegar

1135
00:41:47,820 --> 00:41:50,520
um conjunto de dados realmente simples de uma barra móvel

1136
00:41:50,520 --> 00:41:52,980
estímulos ou uma barra ou quadro estático e você

1137
00:41:52,980 --> 00:41:54,960
o move um pouco, você pode ver que

1138
00:41:54,960 --> 00:41:57,060
o modelo irá realmente inferir aquele

1139
00:41:57,060 --> 00:41:58,800
quadro faltante e então também

1140
00:41:58,800 --> 00:42:01,079
inferir movimento contínuo, então é como

1141
00:42:01,079 --> 00:42:03,300
ultrapassar a trajetória do que os

1142
00:42:03,300 --> 00:42:05,820
estímulos reais estão fornecendo antes de

1143
00:42:05,820 --> 00:42:08,760
corrigir novamente  então eu acho que modelar

1144
00:42:08,760 --> 00:42:10,320
ilusões é certamente uma

1145
00:42:10,320 --> 00:42:12,660
maneira interessante de estudar se nossos modelos de mundo são

1146
00:42:12,660 --> 00:42:14,760
semelhantes aos tipos de modelos que nós

1147
00:42:14,760 --> 00:42:16,619
mesmos temos,

1148
00:42:16,619 --> 00:42:19,619
então, em conclusão, uh sim, acho que

1149
00:42:19,619 --> 00:42:21,900
poderíamos mostrar que

1150
00:42:21,900 --> 00:42:23,220
eles aprenderam efetivamente

1151
00:42:23,220 --> 00:42:24,839
representações estruturadas ou modelos de mundo estruturados.  a

1152
00:42:24,839 --> 00:42:26,700
estrutura aprendida é

1153
00:42:26,700 --> 00:42:29,160
flexível e adaptável a

1154
00:42:29,160 --> 00:42:30,780
transformações arbitrárias, ao contrário dos

1155
00:42:30,780 --> 00:42:33,720
equivariantes tradicionais e provedores topográficos,

1156
00:42:33,720 --> 00:42:35,579
podem ser induzidas estatisticamente como fizemos

1157
00:42:35,579 --> 00:42:37,619
na vae topográfica ou através da

1158
00:42:37,619 --> 00:42:39,480
dinâmica como estávamos mostrando nesses

1159
00:42:39,480 --> 00:42:42,000
modelos do tipo máquina de ondas neurais,

1160
00:42:42,000 --> 00:42:44,460
então, para concluir, deixarei vocês com isso

1161
00:42:44,460 --> 00:42:46,980
citação que encontrei no artigo de Fukushima

1162
00:42:46,980 --> 00:42:50,280
de 1980, pensei que estava muito à frente

1163
00:42:50,280 --> 00:42:52,079
de seu tempo, onde ele diz que se pudéssemos

1164
00:42:52,079 --> 00:42:53,520
fazer um modelo de rede neural que tivesse

1165
00:42:53,520 --> 00:42:55,020
a mesma capacidade de

1166
00:42:55,020 --> 00:42:57,060
reconhecimento de padrões que um ser humano, isso

1167
00:42:57,060 --> 00:42:58,800
nos daria uma pista poderosa em comparação com

1168
00:42:58,800 --> 00:43:00,000
entender o mecanismo neural

1169
00:43:00,000 --> 00:43:03,240
no cérebro, então acho que alguns

1170
00:43:03,240 --> 00:43:06,119
dos objetivos que buscamos aqui,

1171
00:43:06,119 --> 00:43:08,220
então acho que é meu orientador Max, meus

1172
00:43:08,220 --> 00:43:11,280
co-autores Patrick UA Emil jinghian e

1173
00:43:11,280 --> 00:43:17,359
Yorn e estou interessado na discussão, obrigado, tudo bem,

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
tudo bem  obrigado, ótimo,

1176
00:43:27,480 --> 00:43:31,079
apresentação muito interessante,

1177
00:43:31,079 --> 00:43:33,480
muitos lugares para começar, talvez apenas o que o

1178
00:43:33,480 --> 00:43:36,000
trouxe a este trabalho,

1179
00:43:36,000 --> 00:43:38,520
um pouco de contexto sobre como você entrou

1180
00:43:38,520 --> 00:43:43,819
neste trabalho para sua direção de doutorado,

1181
00:43:43,920 --> 00:43:45,119
sim,

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
quero dizer, estamos estudando, não o meu, o

1184
00:43:49,200 --> 00:43:51,000
grupo que  Estou na universidade e tenho

1185
00:43:51,000 --> 00:43:52,700
estudado representações estruturadas

1186
00:43:52,700 --> 00:43:56,640
do ponto de vista matemático por um

1187
00:43:56,640 --> 00:43:58,319
tempo sobre onde algumas pessoas têm

1188
00:43:58,319 --> 00:44:00,240
modelos, como o

1189
00:44:00,240 --> 00:44:01,740
codificador automático variacional,

1190
00:44:01,740 --> 00:44:04,680
e

1191
00:44:04,680 --> 00:44:06,960
acho que algo que sempre foi,

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
é um modelo  que respeita rotações,

1194
00:44:11,220 --> 00:44:13,560
rotações 2D perfeitamente bem, mas se quisermos

1195
00:44:13,560 --> 00:44:15,960
fazer rotações 3D, não podemos fazer isso

1196
00:44:15,960 --> 00:44:17,819
porque isso não é um grupo em termos de

1197
00:44:17,819 --> 00:44:19,740
projeção em um plano 2D, você está

1198
00:44:19,740 --> 00:44:21,180
perdendo informações quando essa coisa

1199
00:44:21,180 --> 00:44:23,460
gira, por exemplo,

1200
00:44:23,460 --> 00:44:24,240
um

1201
00:44:24,240 --> 00:44:26,280
ou  qualquer tipo de

1202
00:44:26,280 --> 00:44:27,960
transformação natural como eu estava tentando

1203
00:44:27,960 --> 00:44:29,339
apontar no início, acho que estava

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
tentando pensar sobre como o cérebro

1206
00:44:31,740 --> 00:44:34,020
modela as transformações naturais é

1207
00:44:34,020 --> 00:44:35,400
algo que é nessas estruturas atuais

1208
00:44:35,400 --> 00:44:37,200


1209
00:44:37,200 --> 00:44:41,099
onde você vê a ação desempenhando um papel

1210
00:44:41,099 --> 00:44:44,579
em termos de autoencoder variacional

1211
00:44:44,579 --> 00:44:48,420
modelos que incluem não apenas

1212
00:44:48,420 --> 00:44:50,520
padrões externos, mas também as

1213
00:44:50,520 --> 00:44:52,380
consequências da ação ou

1214
00:44:52,380 --> 00:44:55,800
estrutura estrutural do modelo mundial com ação

1215
00:44:55,800 --> 00:44:58,619
certa, sim, não, essa é uma boa pergunta e

1216
00:44:58,619 --> 00:45:01,319
acho que inferências acionadas

1217
00:45:01,319 --> 00:45:03,839
são efetivamente a resposta. Acho que

1218
00:45:03,839 --> 00:45:05,940
é uma boa resposta para isso, hum,

1219
00:45:05,940 --> 00:45:09,000
eu sei que existe  são

1220
00:45:09,000 --> 00:45:11,099
estruturas de aprendizado por reforço que

1221
00:45:11,099 --> 00:45:12,660
usam

1222
00:45:12,660 --> 00:45:15,060
modelos mundiais treinados externamente,

1223
00:45:15,060 --> 00:45:17,280
então você treina um vae ou algo assim e então

1224
00:45:17,280 --> 00:45:19,800
usa essa representação em seu

1225
00:45:19,800 --> 00:45:23,040
sistema de aprendizado por reforço, mas

1226
00:45:23,040 --> 00:45:24,720
acho que ter um

1227
00:45:24,720 --> 00:45:26,520
tipo completo de sistema que é um único

1228
00:45:26,520 --> 00:45:30,780
objetivo com uh ação  como parte da

1229
00:45:30,780 --> 00:45:33,660
probabilidade dos dados e,

1230
00:45:33,660 --> 00:45:35,280
sim, acho que isso é muito mais elegante

1231
00:45:35,280 --> 00:45:38,940
e, portanto, sou um grande defensor disso,

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
não cheguei ao ponto de estudar como

1234
00:45:43,140 --> 00:45:45,480
esses modelos mundiais estruturados em uma vae

1235
00:45:45,480 --> 00:45:47,520
ou não '  Não funcionou nisso, mas

1236
00:45:47,520 --> 00:45:48,780
acho que certamente seria muito

1237
00:45:48,780 --> 00:45:50,819
interessante ver se ter um

1238
00:45:50,819 --> 00:45:52,339
modelo mundial mais estruturado,

1239
00:45:52,339 --> 00:45:54,839
uh, em um codificador automático variacional,

1240
00:45:54,839 --> 00:45:56,880
seria benéfico em um

1241
00:45:56,880 --> 00:45:58,319
ambiente ativo também. Acho que seria

1242
00:45:58,319 --> 00:46:00,119
incrível, quero dizer, eu  acho que

1243
00:46:00,119 --> 00:46:03,599
alguns desses exemplos, como uh, mostrando

1244
00:46:03,599 --> 00:46:05,579
antes, como o surgimento de células de grade e

1245
00:46:05,579 --> 00:46:07,500
coisas assim, talvez apontem para

1246
00:46:07,500 --> 00:46:08,880
essa direção,

1247
00:46:08,880 --> 00:46:10,560
ok, talvez o cérebro esteja fazendo algo, é

1248
00:46:10,560 --> 00:46:12,540
realmente óbvio que tem muita

1249
00:46:12,540 --> 00:46:13,680
estrutura,

1250
00:46:13,680 --> 00:46:15,359
isso claramente tem que ser útil para

1251
00:46:15,359 --> 00:46:19,140
executar ações em alguns  ah,

1252
00:46:19,140 --> 00:46:21,720
sim, senti um

1253
00:46:21,720 --> 00:46:24,480
paralelo muito bom que você trouxe com

1254
00:46:24,480 --> 00:46:28,040
a palestra foi que as unidades conectadas localmente

1255
00:46:28,040 --> 00:46:30,960
permitiram que seus modelos

1256
00:46:30,960 --> 00:46:33,780
incorporassem estruturalmente a

1257
00:46:33,780 --> 00:46:35,640
restrição e o padrão convolucional e isso levou a

1258
00:46:35,640 --> 00:46:37,500
esses padrões emergentes e então,

1259
00:46:37,500 --> 00:46:41,339
analogamente, houve o uh Doral em

1260
00:46:41,339 --> 00:46:45,680
tudo onde  eles tinham a restrição de exploração de caminho

1261
00:46:45,680 --> 00:46:48,359
certa e então é

1262
00:46:48,359 --> 00:46:50,280
interessante,

1263
00:46:50,280 --> 00:46:53,760
você sabe, pensar sobre essas heurísticas de ação ou

1264
00:46:53,760 --> 00:46:56,819
política ou esparsidades como uma

1265
00:46:56,819 --> 00:46:59,579
exploração motora conjunta, eventualmente,

1266
00:46:59,579 --> 00:47:02,339
torna-se entendido que existem duas

1267
00:47:02,339 --> 00:47:04,980
maneiras mutuamente opostas de mover uma articulação

1268
00:47:04,980 --> 00:47:07,079
e então a composicionalidade  através das

1269
00:47:07,079 --> 00:47:09,119
articulações pode ser aprendido nesses

1270
00:47:09,119 --> 00:47:10,680
níveis mais altos, uma vez que está bloqueado em

1271
00:47:10,680 --> 00:47:14,480
níveis mais baixos, então é uma maneira muito atraente

1272
00:47:14,480 --> 00:47:17,599
e, uh uh,

1273
00:47:17,599 --> 00:47:20,460
relevante para o nicho de generalizar,

1274
00:47:20,460 --> 00:47:23,819
porque é baseado nas

1275
00:47:23,819 --> 00:47:25,740
restrições reais do mundo, mas

1276
00:47:25,740 --> 00:47:27,720
especialmente por meio da ação,

1277
00:47:27,720 --> 00:47:29,460
potencialmente incorporando algo que é

1278
00:47:29,460 --> 00:47:31,380
muito simples,

1279
00:47:31,380 --> 00:47:33,599
certo, sim, não, acho que isso é definitivamente

1280
00:47:33,599 --> 00:47:36,599
verdade, esse é um ponto muito bom se, uh, se

1281
00:47:36,599 --> 00:47:38,339
você tiver restrições provenientes de suas

1282
00:47:38,339 --> 00:47:40,500
próprias ações, isso

1283
00:47:40,500 --> 00:47:42,839
seria extremamente benéfico para ajudar

1284
00:47:42,839 --> 00:47:44,819
a estruturar

1285
00:47:44,819 --> 00:47:47,460
seu espaço latente e acho que sim, acho que

1286
00:47:47,460 --> 00:47:48,480
uma coisa  Eu queria mencionar que

1287
00:47:48,480 --> 00:47:49,980
há

1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
algo que me fez pensar, como o

1290
00:47:52,740 --> 00:47:55,500
trabalho de Stefano Fousey sobre o tipo de

1291
00:47:55,500 --> 00:47:58,859
geometria representacional, uh,

1292
00:47:58,859 --> 00:48:01,920
determina como nós,

1293
00:48:01,920 --> 00:48:04,440
quão generalizável é uma determinada compreensão

1294
00:48:04,440 --> 00:48:08,099
de um sistema, uh, e eu acho que se você puder

1295
00:48:08,099 --> 00:48:11,880
entender como esses conjuntos de

1296
00:48:11,880 --> 00:48:14,520
atividades são  separável ou altamente

1297
00:48:14,520 --> 00:48:16,079
paralelo separável com um

1298
00:48:16,079 --> 00:48:18,839
classificador linear essencialmente, então você será

1299
00:48:18,839 --> 00:48:20,700
capaz de fazer generalização e

1300
00:48:20,700 --> 00:48:23,099
acho que ao impor esses tipos de preconceitos

1301
00:48:23,099 --> 00:48:25,040
ou potencialmente através de

1302
00:48:25,040 --> 00:48:27,000
restrições que são impostas por uma ação

1303
00:48:27,000 --> 00:48:28,740
algo como isto

1304
00:48:28,740 --> 00:48:32,040
você está cedendo ou meio que induzindo um

1305
00:48:32,040 --> 00:48:33,660
melhor geometria representacional e

1306
00:48:33,660 --> 00:48:35,220
isso tem todos os tipos de benefícios para composicionalidade,

1307
00:48:35,220 --> 00:48:36,660


1308
00:48:36,660 --> 00:48:39,359
sim, nossa generalização, então

1309
00:48:39,359 --> 00:48:41,760
é um ótimo ponto, legal, sim,

1310
00:48:41,760 --> 00:48:43,440
área muito interessante, certo, vou ler

1311
00:48:43,440 --> 00:48:45,960
algumas perguntas do chat ao vivo,

1312
00:48:45,960 --> 00:48:48,420
amor evoluir, escreveu

1313
00:48:48,420 --> 00:48:52,260
quaisquer limitações práticas ou observadas na

1314
00:48:52,260 --> 00:48:55,579
modelagem de ilusões,

1315
00:48:58,800 --> 00:49:00,420
aprendendo  comunidades é que elas não têm

1316
00:49:00,420 --> 00:49:03,060
fobia, você não tem um centro de

1317
00:49:03,060 --> 00:49:05,940
olhar, então você também não tem

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
um tempo, quero dizer, a maioria das

1320
00:49:11,460 --> 00:49:13,260
redes neurais convolucionais, estou usando esse tipo de

1321
00:49:13,260 --> 00:49:15,599
rede neural recorrente, mas o tempo

1322
00:49:15,599 --> 00:49:18,420
não é tão claramente definido  nesses modelos,

1323
00:49:18,420 --> 00:49:20,220
pois está em um cenário de tempo contínuo

1324
00:49:20,220 --> 00:49:23,400
para um humano passando por um teste de ilusão,

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
hum,

1327
00:49:25,319 --> 00:49:27,480
e eu acho que a combinação desses dois é

1328
00:49:27,480 --> 00:49:30,359
o fato de que, como um humano ou a maioria das

1329
00:49:30,359 --> 00:49:33,300
coisas, uh,

1330
00:49:33,300 --> 00:49:35,940
seu olhar, suas mudanças de localização e

1331
00:49:35,940 --> 00:49:38,220
seus ganhos dependem de  como se você

1332
00:49:38,220 --> 00:49:40,140
olhasse para uma área específica com muitos

1333
00:49:40,140 --> 00:49:42,780
testes cognitivos, então acho que

1334
00:49:42,780 --> 00:49:46,560
seria muito útil se tivéssemos modelos que

1335
00:49:46,560 --> 00:49:48,540
sim, quero dizer, aprenda que você pode pensar

1336
00:49:48,540 --> 00:49:50,760
nisso como um tipo de ação, como aprender para

1337
00:49:50,760 --> 00:49:52,980
onde mover o olhar, um dos  o

1338
00:49:52,980 --> 00:49:54,420
mais simples possível que ajudaria muito

1339
00:49:54,420 --> 00:49:56,220
para ser capaz de modelar ilusões e

1340
00:49:56,220 --> 00:49:58,859
só quero dizer que para mim é como se eu tivesse lido um

1341
00:49:58,859 --> 00:50:00,720
artigo sobre alguns experimentos de ciências cognitivas

1342
00:50:00,720 --> 00:50:02,940
ou sobre alguma ilusão e

1343
00:50:02,940 --> 00:50:05,160
é como se eu pensasse, ok, posso colocar esse

1344
00:50:05,160 --> 00:50:07,560
conjunto de dados em meu  modele e teste-o e na

1345
00:50:07,560 --> 00:50:08,579
maioria das vezes a resposta é não,

1346
00:50:08,579 --> 00:50:10,619
porque eu não tenho um modelo que

1347
00:50:10,619 --> 00:50:12,900
olhe ao redor ou tenha um campo

1348
00:50:12,900 --> 00:50:14,660
de visão restrito, algo assim,

1349
00:50:14,660 --> 00:50:16,619
então sim, acho que essa é uma das

1350
00:50:16,619 --> 00:50:19,680
limitações, outra é,

1351
00:50:19,680 --> 00:50:20,579


1352
00:50:20,579 --> 00:50:22,920
sim, faça o  experimento muito mais

1353
00:50:22,920 --> 00:50:24,900
complicado, então essa é uma das

1354
00:50:24,900 --> 00:50:27,359
limitações práticas,

1355
00:50:27,359 --> 00:50:30,240
uau, ótima resposta, me faz pensar em um

1356
00:50:30,240 --> 00:50:33,920
papel com letras girando em uma mesa,

1357
00:50:33,920 --> 00:50:36,780
essa é a rotação dos dígitos, ótimos pontos

1358
00:50:36,780 --> 00:50:38,460
sobre a foveação e a dinâmica

1359
00:50:38,460 --> 00:50:40,079
da ilusão, acho que você realmente

1360
00:50:40,079 --> 00:50:42,599
mencionou uma ilusão que  no entanto, você

1361
00:50:42,599 --> 00:50:43,980
mencionou no contexto de generalização

1362
00:50:43,980 --> 00:50:46,859
que está girando na

1363
00:50:46,859 --> 00:50:49,500
tela bidimensional não generaliza para três

1364
00:50:49,500 --> 00:50:52,920
dimensões e esse colapso

1365
00:50:52,920 --> 00:50:55,559
ou redução dimensional é a base das

1366
00:50:55,559 --> 00:50:58,619
Ilusões de projeção do cubo e Ilusões de rotação de cubo e figura

1367
00:50:58,619 --> 00:51:01,880
está na sua tela

1368
00:51:01,880 --> 00:51:05,280
e há  uma silhueta ou há alguns

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
estímulos ambíguos de que um generativo está próximo da

1371
00:51:09,839 --> 00:51:12,359
criticidade ou de uma bifurcação em

1372
00:51:12,359 --> 00:51:13,680
modelos degenerativos para que possa

1373
00:51:13,680 --> 00:51:17,160
representá-lo de uma forma ou de outra

1374
00:51:17,160 --> 00:51:19,920
e, portanto, muitas das ilusões de mudança

1375
00:51:19,920 --> 00:51:22,020
são baseadas apenas na planicidade das

1376
00:51:22,020 --> 00:51:23,819
imagens

1377
00:51:23,819 --> 00:51:26,280
e nas limitações e generalização

1378
00:51:26,280 --> 00:51:28,740
que são revelados por isso,

1379
00:51:28,740 --> 00:51:32,460
sim, sim, acho que há até Oh, sim,

1380
00:51:32,460 --> 00:51:34,859
em algum lugar, desculpe, há algum trabalho ou

1381
00:51:34,859 --> 00:51:35,880
eles

1382
00:51:35,880 --> 00:51:37,619
podem argumentar que as pessoas têm uma

1383
00:51:37,619 --> 00:51:39,480
imagem tridimensional em suas cabeças,

1384
00:51:39,480 --> 00:51:42,000
como se até Nancy Ken fosse uma ou suas

1385
00:51:42,000 --> 00:51:45,119
laterais recentemente, mas e mostrando sim, eu

1386
00:51:45,119 --> 00:51:48,119
não  não sei se nossos modelos têm que

1387
00:51:48,119 --> 00:51:50,460
não é muito grande

1388
00:51:50,460 --> 00:51:53,700
de qualquer maneira, sim, isso é muito interessante,

1389
00:51:53,700 --> 00:51:56,160
tudo bem, do upcycle Club no

1390
00:51:56,160 --> 00:51:58,200
bate-papo, eles escreveram parabéns

1391
00:51:58,200 --> 00:52:00,000
se você for capaz de aprender de forma quase tão

1392
00:52:00,000 --> 00:52:02,160
eficaz se você imaginar que deseja apenas que

1393
00:52:02,160 --> 00:52:03,780
um único neurônio seja  ativo para cada

1394
00:52:03,780 --> 00:52:06,540
exemplo, uh, seu modelo

1395
00:52:06,540 --> 00:52:08,579
tentará memorizar o design do conjunto de dados

1396
00:52:08,579 --> 00:52:10,980
ou algo assim,

1397
00:52:10,980 --> 00:52:12,180
e você não terá

1398
00:52:12,180 --> 00:52:14,940
capacidade suficiente, então sim, acho que ajustar esse

1399
00:52:14,940 --> 00:52:18,359
nível de dispersão é certamente

1400
00:52:18,359 --> 00:52:22,200
um fator importante e

1401
00:52:22,200 --> 00:52:25,020
sim  quando você olha para a probabilidade, se

1402
00:52:25,020 --> 00:52:26,220
você está falando, se você está dobrando a

1403
00:52:26,220 --> 00:52:28,579
estrutura, normalmente isso é balanceado

1404
00:52:28,579 --> 00:52:32,040
automaticamente com a probabilidade em si.

1405
00:52:32,040 --> 00:52:33,000


1406
00:52:33,000 --> 00:52:34,380
Se você não estiver gerando modelagem,

1407
00:52:34,380 --> 00:52:35,760
você apenas terá uma penalidade de dispersão que você

1408
00:52:35,760 --> 00:52:38,460
vai querer ajustar  esse parâmetro

1409
00:52:38,460 --> 00:52:40,980
está bem, sim, é apenas para esclarecer o

1410
00:52:40,980 --> 00:52:43,380
comportamento descontrolado em armina, onde a

1411
00:52:43,380 --> 00:52:45,599
rede se torna instável ou caótica devido

1412
00:52:45,599 --> 00:52:47,040
a vários fatores, como

1413
00:52:47,040 --> 00:52:50,400
ruído de loops de feedback ou entradas adversárias,

1414
00:52:50,400 --> 00:52:52,380


1415
00:52:52,380 --> 00:52:54,180
sim, acho que não olhei para isso

1416
00:52:54,180 --> 00:52:55,859
como uma configuração recorrente onde  você

1417
00:52:55,859 --> 00:52:58,500
obteria ciclos de feedback,

1418
00:52:58,500 --> 00:52:59,460


1419
00:52:59,460 --> 00:53:01,800
mas eu poderia, sim, eu poderia ver

1420
00:53:01,800 --> 00:53:04,319
exemplos adversários sendo potencialmente

1421
00:53:04,319 --> 00:53:07,800
afetados pelo seu nível de dispersão.

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
O ponto interessante é o que você

1424
00:53:10,859 --> 00:53:12,660
seria mais suscetível ou menos suscetível

1425
00:53:12,660 --> 00:53:16,040
a compartilhar exemplos que não conheço

1426
00:53:16,040 --> 00:53:19,440
bem, esparsificação

1427
00:53:19,440 --> 00:53:21,720
projetando de um  modelo de dimensão superior totalmente conectado

1428
00:53:21,720 --> 00:53:23,579
apenas em

1429
00:53:23,579 --> 00:53:25,619
progressivamente menor, é

1430
00:53:25,619 --> 00:53:27,540
muito bem compreendido em geral

1431
00:53:27,540 --> 00:53:29,760
quais são as vantagens e desvantagens, são cálculos mais fáceis,

1432
00:53:29,760 --> 00:53:34,079
um modelo menor, mais esparso,

1433
00:53:34,079 --> 00:53:36,420
o gráfico básico será mais claro

1434
00:53:36,420 --> 00:53:39,119
para representar e também terá

1435
00:53:39,119 --> 00:53:41,339
todas as outras vantagens  offs com falsos

1436
00:53:41,339 --> 00:53:43,680
positivos e negativos de generalização,

1437
00:53:43,680 --> 00:53:45,720
mas é por isso que é um processo de ajuste iterativo,

1438
00:53:45,720 --> 00:53:47,579
então eu

1439
00:53:47,579 --> 00:53:49,760
acho que como o equilíbrio da sua

1440
00:53:49,760 --> 00:53:52,800
abordagem de esparsificação

1441
00:53:52,800 --> 00:53:55,700


1442
00:53:56,700 --> 00:53:59,520
não usa AIC ou Bic ou alguma outra

1443
00:53:59,520 --> 00:54:01,619
abordagem de ajuste de modelo para determinar a

1444
00:54:01,619 --> 00:54:03,660
esparsificação relevante

1445
00:54:03,660 --> 00:54:07,079
para uma determinada entrada,

1446
00:54:07,079 --> 00:54:09,780
como você  determine como, como na

1447
00:54:09,780 --> 00:54:11,940
regressão laço, como você sabe

1448
00:54:11,940 --> 00:54:14,339
quanto, como você limita quantos, quão

1449
00:54:14,339 --> 00:54:17,220
esparso você quer que seja

1450
00:54:17,220 --> 00:54:19,559
certo, sim, acho que há muita

1451
00:54:19,559 --> 00:54:22,440
literatura boa sobre isso e mesmo assim algumas

1452
00:54:22,440 --> 00:54:25,319
pessoas gostam deles em Harvard e  algumas

1453
00:54:25,319 --> 00:54:29,520
pessoas estão trabalhando agora, uh, fizeram

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
esse tipo de

1456
00:54:34,380 --> 00:54:36,780
rede de esparsificação iterativa desenrolada,

1457
00:54:36,780 --> 00:54:37,800
onde é como uma rede neural recorrente

1458
00:54:37,800 --> 00:54:40,380
e esparsifica iterativamente e

1459
00:54:40,380 --> 00:54:41,940
você pode mostrar que isso produz algo

1460
00:54:41,940 --> 00:54:45,780
como perda vermelha ou uh grupo como grupo

1461
00:54:45,780 --> 00:54:47,520
ativo grupo Ativações esportivas como

1462
00:54:47,520 --> 00:54:48,960
nós  estou usando aqui,

1463
00:54:48,960 --> 00:54:52,859
hum, nesta configuração, uh, é realmente apenas

1464
00:54:52,859 --> 00:54:55,859
por ter

1465
00:54:55,859 --> 00:54:59,280
essa construção dessa variável T

1466
00:54:59,280 --> 00:55:04,079
onde temos Z no topo e uh

1467
00:55:04,079 --> 00:55:07,859
e então é de alguma forma controlado por

1468
00:55:07,859 --> 00:55:09,119
essas

1469
00:55:09,119 --> 00:55:11,579
a soma das variáveis ​​​​U na parte inferior, então

1470
00:55:11,579 --> 00:55:13,200
se W  talvez eu não tenha sido muito claro sobre

1471
00:55:13,200 --> 00:55:16,500
isso é uma matriz que está conectando,

1472
00:55:16,500 --> 00:55:18,359
é isso que define o grupo, então estou

1473
00:55:18,359 --> 00:55:20,400
definindo o time do colégio do grupo, uh, que

1474
00:55:20,400 --> 00:55:22,380
conecta todos esses U's juntos e então

1475
00:55:22,380 --> 00:55:23,940
a ideia é,

1476
00:55:23,940 --> 00:55:27,540
uh, como aqui, se todos de um dos outros

1477
00:55:27,540 --> 00:55:31,740
exemplos se todos os seus usos uh não estiverem

1478
00:55:31,740 --> 00:55:35,520
ativos para um determinado t

1479
00:55:35,520 --> 00:55:38,280
ou se todos os varios estiverem ativos para um determinado

1480
00:55:38,280 --> 00:55:41,040
t uh essa variável t será muito

1481
00:55:41,040 --> 00:55:42,780
pequena, certo porque seu denominador

1482
00:55:42,780 --> 00:55:44,339
será muito grande e isso induz

1483
00:55:44,339 --> 00:55:47,160
esparsidade, então é  uh, é uma

1484
00:55:47,160 --> 00:55:49,260
satisfação de restrição se você tiver um

1485
00:55:49,260 --> 00:55:51,839
conjunto de U's que são todos pequenos, uh, então

1486
00:55:51,839 --> 00:55:54,480
essa restrição é satisfeita e

1487
00:55:54,480 --> 00:55:57,180
agora Z pode se expressar

1488
00:55:57,180 --> 00:56:00,240
e é isso que então,

1489
00:56:00,240 --> 00:56:02,880
uh, sim, alcança tanto quanto a

1490
00:56:02,880 --> 00:56:06,180
ativação  então isso é induzido por esses

1491
00:56:06,180 --> 00:56:07,020
dois

1492
00:56:07,020 --> 00:56:09,300
termos de divergência de couve aqui, eles estão

1493
00:56:09,300 --> 00:56:12,960
dizendo a que distância está cada unhc

1494
00:56:12,960 --> 00:56:15,180
de uma gaussiana e, em seguida, por meio dessa

1495
00:56:15,180 --> 00:56:16,980
construção da variável T do aluno,

1496
00:56:16,980 --> 00:56:20,880
estamos efetivamente construindo uma

1497
00:56:20,880 --> 00:56:23,040
distribuição anterior esparsa apenas a partir dessas

1498
00:56:23,040 --> 00:56:24,839
gaussianas, mas em  termos do ato, o

1499
00:56:24,839 --> 00:56:27,599
objetivo real, uh, os termos e o

1500
00:56:27,599 --> 00:56:28,920
objetivo que estamos otimizando são apenas

1501
00:56:28,920 --> 00:56:31,619
esses dois termos KL que estão empurrando-o

1502
00:56:31,619 --> 00:56:34,020
para a dispersão até certo ponto e isso

1503
00:56:34,020 --> 00:56:36,359
é equilibrado automaticamente com o

1504
00:56:36,359 --> 00:56:39,000
termo de probabilidade aqui através do decodificador,

1505
00:56:39,000 --> 00:56:41,220
então não '  não temos termos que estamos ajustando,

1506
00:56:41,220 --> 00:56:42,839
mas estamos aprendendo os parâmetros

1507
00:56:42,839 --> 00:56:44,280
desses diferentes codificadores e depois

1508
00:56:44,280 --> 00:56:48,200
analisando as falhas e emergências, tudo

1509
00:56:48,540 --> 00:56:49,920


1510
00:56:49,920 --> 00:56:52,859
bem, outra pergunta de Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas, que escreveu

1512
00:56:55,500 --> 00:56:59,160
falando sobre olhar e ilusão, os

1513
00:56:59,160 --> 00:57:01,920
estudos sobre constâncias em bebês podem ser

1514
00:57:01,920 --> 00:57:04,260
separados  em ilusão de nível inferior Rel

1515
00:57:04,260 --> 00:57:06,140
talvez constância conceitual de nível superior

1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
uh você pode ler

1518
00:57:15,720 --> 00:57:18,300
o tipo atual de arquitetura

1519
00:57:18,300 --> 00:57:23,520
os estudos sobre constâncias em bebês e

1520
00:57:23,520 --> 00:57:26,640
constâncias cognitivas podem ser separados

1521
00:57:26,640 --> 00:57:31,619
sim, provavelmente não sou, não sou

1522
00:57:31,619 --> 00:57:33,900
um especialista ou até mesmo muito familiarizado

1523
00:57:33,900 --> 00:57:35,460
com

1524
00:57:35,460 --> 00:57:37,859
estudos de permanência de objetos e bebês

1525
00:57:37,859 --> 00:57:40,260
e coisas de constância, mas acho que

1526
00:57:40,260 --> 00:57:42,300
seria incrivelmente interessante estudar

1527
00:57:42,300 --> 00:57:44,339
em arquiteturas de redes neurais e

1528
00:57:44,339 --> 00:57:46,740
essa foi uma espécie de ideia com

1529
00:57:46,740 --> 00:57:48,780
essa uh ilusão que eu estava tentando

1530
00:57:48,780 --> 00:57:51,780
modelar aqui com esta linha  Não

1531
00:57:51,780 --> 00:57:53,099
sei se fui muito claro sobre isso, mas

1532
00:57:53,099 --> 00:57:55,380
a linha superior é a entrada e estamos

1533
00:57:55,380 --> 00:57:57,359
efetivamente bloqueando a entrada para

1534
00:57:57,359 --> 00:58:00,119
um único quadro e eu queria ver se

1535
00:58:00,119 --> 00:58:03,240
a rede codifica se a

1536
00:58:03,240 --> 00:58:05,400
coisa ainda está lá  quando esse quadro

1537
00:58:05,400 --> 00:58:07,680
desaparecer, ainda posso decodificar a presença do

1538
00:58:07,680 --> 00:58:10,020
objeto a partir da atividade neural, uh,

1539
00:58:10,020 --> 00:58:11,819
e então o que ele também está inferindo sobre

1540
00:58:11,819 --> 00:58:13,920
o movimento devido ao fato de que ele

1541
00:58:13,920 --> 00:58:15,780
viu as barras em um local ligeiramente diferente

1542
00:58:15,780 --> 00:58:18,480
do que antes, quando o depois

1543
00:58:18,480 --> 00:58:20,520
do  o quadro desapareceu,

1544
00:58:20,520 --> 00:58:22,559
então

1545
00:58:22,559 --> 00:58:25,200
sim, acho que definitivamente tem vários

1546
00:58:25,200 --> 00:58:27,240
níveis,

1547
00:58:27,240 --> 00:58:29,160
onde alguns provavelmente seriam de

1548
00:58:29,160 --> 00:58:33,180
nível muito mais baixo e, uh,

1549
00:58:33,180 --> 00:58:35,880
talvez a permanência do objeto a longo prazo, eu

1550
00:58:35,880 --> 00:58:37,380
acho que seria um

1551
00:58:37,380 --> 00:58:39,059
nível significativamente mais alto,

1552
00:58:39,059 --> 00:58:39,900


1553
00:58:39,900 --> 00:58:41,760
isso só me faz pensar naqueles

1554
00:58:41,760 --> 00:58:44,640
experimentos com gatos  antigamente,

1555
00:58:44,640 --> 00:58:47,280
era como se eles os criassem na

1556
00:58:47,280 --> 00:58:49,020
escuridão, exceto por uma hora por dia, eles os

1557
00:58:49,020 --> 00:58:51,000
colocavam no mundo vertical ou no

1558
00:58:51,000 --> 00:58:53,160
mundo horizontal ou eles só viam linhas horizontais

1559
00:58:53,160 --> 00:58:57,299
ou linhas verticais, uh e você pode ver

1560
00:58:57,299 --> 00:58:59,880
a organização de seu córtex mudar

1561
00:58:59,880 --> 00:59:02,819
como eles  têm menos receptividade são

1562
00:59:02,819 --> 00:59:04,200
linhas horizontais se eles nunca viram

1563
00:59:04,200 --> 00:59:06,420
linhas horizontais antes e então você

1564
00:59:06,420 --> 00:59:07,980
pega um bastão e balança na frente do

1565
00:59:07,980 --> 00:59:09,599
rosto deles e se o bastão estiver na

1566
00:59:09,599 --> 00:59:11,220
horizontal eles simplesmente não fazem nada,

1567
00:59:11,220 --> 00:59:12,900
é vertical, eles estão batendo nele

1568
00:59:12,900 --> 00:59:14,460
eles estão tentando acertar, é como se eles

1569
00:59:14,460 --> 00:59:15,900
literalmente não precisassem barrar na

1570
00:59:15,900 --> 00:59:18,420
frente de seus rostos, então acho que, nesse

1571
00:59:18,420 --> 00:59:20,700
caso, isso é evidência de uma

1572
00:59:20,700 --> 00:59:24,260
deficiência de baixo nível e visão

1573
00:59:24,260 --> 00:59:26,940
contribuindo para algum tipo de ilusão,

1574
00:59:26,940 --> 00:59:28,980
então eu  acho que sim, certamente poderia

1575
00:59:28,980 --> 00:59:30,660
haver algum aspecto disso em bebês também,

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
um ponto muito curioso que você mencionou

1578
00:59:36,420 --> 00:59:40,339
foi a variedade animada e inanimada,

1579
00:59:40,339 --> 00:59:43,619
com pequenas coisas sendo

1580
00:59:43,619 --> 00:59:45,480
intermediárias,

1581
00:59:45,480 --> 00:59:49,140
certo, o que isso representa

1582
00:59:49,140 --> 00:59:52,319
ou ou é porque são manejáveis

1583
00:59:52,319 --> 00:59:55,619
ou isso  pode ser um inseto ou pode ser

1584
00:59:55,619 --> 00:59:57,839
algo que pode se afastar apenas com o

1585
00:59:57,839 --> 01:00:01,380
vento ou o que isso quer dizer, certo,

1586
01:00:01,380 --> 01:00:04,380
sim,

1587
01:00:04,380 --> 01:00:08,280
então este é o trabalho de Talia Conkle. Acho que

1588
01:00:08,280 --> 01:00:11,280
foi quem descobriu essa

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
organização e eles tentaram

1591
01:00:14,880 --> 01:00:16,440
descobrir isso.  Posso estar entendendo

1592
01:00:16,440 --> 01:00:19,500
errado, então recomendo que as pessoas leiam o

1593
01:00:19,500 --> 01:00:21,359
trabalho dela sobre isso, se chamam de

1594
01:00:21,359 --> 01:00:23,880
organização tripartida, mas se bem me

1595
01:00:23,880 --> 01:00:25,319
lembro,

1596
01:00:25,319 --> 01:00:27,900
eles fizeram muito trabalho de acompanhamento sobre por que

1597
01:00:27,900 --> 01:00:30,780
existe essa organização e

1598
01:00:30,780 --> 01:00:33,180
algumas evidências de

1599
01:00:33,180 --> 01:00:35,700
curvatura desses objetos e mais ou menos

1600
01:00:35,700 --> 01:00:37,440
como a distância que você vê os objetos

1601
01:00:37,440 --> 01:00:40,260
ou como

1602
01:00:40,260 --> 01:00:43,319
um objeto animado ou talvez mais curvo

1603
01:00:43,319 --> 01:00:45,599
ou existe, independentemente de qual seja a

1604
01:00:45,599 --> 01:00:46,859
resposta real, havia muitas

1605
01:00:46,859 --> 01:00:48,720
hipóteses diferentes decorrentes

1606
01:00:48,720 --> 01:00:51,720
de propriedades semelhantes desses objetos

1607
01:00:51,720 --> 01:00:54,000
talvez propriedades de nível médio ou baixo

1608
01:00:54,000 --> 01:00:56,280
mais do que propriedades de nível superior.

1609
01:00:56,280 --> 01:00:57,599
Ainda não sei se foi exatamente

1610
01:00:57,599 --> 01:00:59,339
resolvido se é como se a

1611
01:00:59,339 --> 01:01:01,500
interação como você disse com os

1612
01:01:01,500 --> 01:01:04,920
objetos causa a separação ou

1613
01:01:04,920 --> 01:01:06,119
hum

1614
01:01:06,119 --> 01:01:09,540
ou sim, as formas gerais desses

1615
01:01:09,540 --> 01:01:12,299
objetos eu  apostaria que, como acontece com a maioria das coisas,

1616
01:01:12,299 --> 01:01:13,980
é como uma combinação de todos os itens

1617
01:01:13,980 --> 01:01:16,980
acima, mas acho que o interessante

1618
01:01:16,980 --> 01:01:18,480
desse ponto de vista de modelagem

1619
01:01:18,480 --> 01:01:19,859
é que,

1620
01:01:19,859 --> 01:01:21,480
hum,

1621
01:01:21,480 --> 01:01:24,059
isso só é treinado em

1622
01:01:24,059 --> 01:01:26,819
estatísticas de correlação dos próprios conjuntos de dados de imagem,

1623
01:01:26,819 --> 01:01:28,799
então não tem interação.

1624
01:01:28,799 --> 01:01:32,760
não tem noção de animação, uh, quero dizer, isso

1625
01:01:32,760 --> 01:01:34,140
é apenas treinar um modelo no

1626
01:01:34,140 --> 01:01:37,859
imagenet, apenas imagens de cães, gatos, barcos, o

1627
01:01:37,859 --> 01:01:40,020
que quer que seja, e ainda assim ele atinge esse

1628
01:01:40,020 --> 01:01:41,640
tipo de organização, então há algum

1629
01:01:41,640 --> 01:01:42,540
tipo de

1630
01:01:42,540 --> 01:01:44,940
características semânticas,

1631
01:01:44,940 --> 01:01:46,740
certo, temos uma imagem, temos uma  rede

1632
01:01:46,740 --> 01:01:48,359
que pode classificar

1633
01:01:48,359 --> 01:01:51,000
barcos versus cães versus 20 outras raças

1634
01:01:51,000 --> 01:01:53,640
de cães, mas

1635
01:01:53,640 --> 01:01:55,920
também pode ter alguma correspondência

1636
01:01:55,920 --> 01:01:57,900
com estatísticas de acabamento de nível inferior,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
então sim, não sei, acho que

1639
01:02:03,960 --> 01:02:07,500
sim, uma analogia provocativa foi a

1640
01:02:07,500 --> 01:02:10,380
mudança de tradução

1641
01:02:10,380 --> 01:02:12,900
no mnist na caligrafia

1642
01:02:12,900 --> 01:02:14,819
configuração de reconhecimento

1643
01:02:14,819 --> 01:02:18,000
quais são as mudanças translacionais

1644
01:02:18,000 --> 01:02:20,160
que

1645
01:02:20,160 --> 01:02:22,740
existem hoje o que é o exemplo de três pixels

1646
01:02:22,740 --> 01:02:24,780
é que algum ataque de engenharia imediata

1647
01:02:24,780 --> 01:02:27,540
em um filme ou algo assim ou

1648
01:02:27,540 --> 01:02:29,099
algo certo um caractere especial

1649
01:02:29,099 --> 01:02:32,640
sendo inserido ou ou alguma

1650
01:02:32,640 --> 01:02:35,160
sobreposição em uma imagem que não podemos nem mesmo

1651
01:02:35,160 --> 01:02:37,020
detectar isso,

1652
01:02:37,020 --> 01:02:39,359
então o que você acha que são esses desafios

1653
01:02:39,359 --> 01:02:42,839
e quais são as maneiras pelas quais podemos perseguir

1654
01:02:42,839 --> 01:02:44,940
isso,

1655
01:02:44,940 --> 01:02:47,520
sim, absolutamente, quero dizer, acho que é mais ou menos como

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
eu estava pensando sobre isso, é como

1658
01:02:50,099 --> 01:02:52,680
essas transformações de simetria,

1659
01:02:52,680 --> 01:02:53,760


1660
01:02:53,760 --> 01:02:55,799
se você está pensando em modelos de linguagem,

1661
01:02:55,799 --> 01:02:57,420
você  posso imaginar uma

1662
01:02:57,420 --> 01:02:58,500
transformação de simetria que é como

1663
01:02:58,500 --> 01:03:00,240
substituir uma palavra por um sinônimo ou

1664
01:03:00,240 --> 01:03:03,780
algo assim. Você tem a frase para nós

1665
01:03:03,780 --> 01:03:06,000
significa exatamente a mesma coisa, mas agora,

1666
01:03:06,000 --> 01:03:07,380
de repente, o modelo vai responder de maneira

1667
01:03:07,380 --> 01:03:09,299
muito diferente,

1668
01:03:09,299 --> 01:03:11,240


1669
01:03:11,240 --> 01:03:15,359
como na tradução entre idiomas, isso

1670
01:03:15,359 --> 01:03:16,799
pode ser visto como um  tipo de transformação,

1671
01:03:16,799 --> 01:03:19,440
preserva o

1672
01:03:19,440 --> 01:03:21,960
significado subjacente da entrada

1673
01:03:21,960 --> 01:03:24,900
para nós, mas para o modelo parece

1674
01:03:24,900 --> 01:03:26,220
completamente diferente e gostaríamos

1675
01:03:26,220 --> 01:03:28,380
de ter modelos que se comportassem de maneira

1676
01:03:28,380 --> 01:03:29,940
previsível em relação a esses

1677
01:03:29,940 --> 01:03:32,160
tipos de transformações, porque

1678
01:03:32,160 --> 01:03:35,040
acho que os humanos se comportam de maneira muito previsível,

1679
01:03:35,040 --> 01:03:37,319
uma vez que estes  Transformações e quando

1680
01:03:37,319 --> 01:03:39,920
estamos lidando com sistemas de IA, esperamos que

1681
01:03:39,920 --> 01:03:43,200
eles também se comportem dessa maneira e acho que isso é

1682
01:03:43,200 --> 01:03:45,480
parte do que causa muitos

1683
01:03:45,480 --> 01:03:47,339
desafios na interação com esses

1684
01:03:47,339 --> 01:03:49,460
sistemas e eu meio que tentei fazer uma

1685
01:03:49,460 --> 01:03:52,500
demonstração atrevida disso

1686
01:03:52,500 --> 01:03:54,960
com isso  urso e quadrados e coisas

1687
01:03:54,960 --> 01:03:58,440
como hum, esperamos que

1688
01:03:58,440 --> 01:04:00,480
ele seja capaz de fazer algo simples

1689
01:04:00,480 --> 01:04:02,220
como isso porque achamos que a maioria dos humanos

1690
01:04:02,220 --> 01:04:04,020
poderia, mas isso não acontece e se você

1691
01:04:04,020 --> 01:04:05,460
imaginar que este é um cenário crítico

1692
01:04:05,460 --> 01:04:07,740
onde você espera isso e isso é um grande

1693
01:04:07,740 --> 01:04:08,700
problema,

1694
01:04:08,700 --> 01:04:11,099
hum  como lidamos com isso, sim,

1695
01:04:11,099 --> 01:04:12,720
acho que é isso que estou

1696
01:04:12,720 --> 01:04:15,859
procurando, acho que

1697
01:04:16,319 --> 01:04:18,420
minha

1698
01:04:18,420 --> 01:04:22,280
direção que estou tomando é parecer

1699
01:04:22,280 --> 01:04:26,940
mais simples e como

1700
01:04:26,940 --> 01:04:29,460
blocos de construção de baixo para cima de

1701
01:04:29,460 --> 01:04:31,380
arquiteturas de redes neurais ou

1702
01:04:31,380 --> 01:04:33,180
algoritmos que

1703
01:04:33,180 --> 01:04:35,760
produzem esses emergentes

1704
01:04:35,760 --> 01:04:37,680
propriedades estruturais e acho que é uma

1705
01:04:37,680 --> 01:04:39,839
maneira muito mais generalizável, em vez de

1706
01:04:39,839 --> 01:04:41,579
construir algo em cima do que

1707
01:04:41,579 --> 01:04:43,140
já temos.

1708
01:04:43,140 --> 01:04:43,920


1709
01:04:43,920 --> 01:04:45,839
Acho que é algo que será escalonado

1710
01:04:45,839 --> 01:04:47,819
muito melhor e também corresponderá mais ao que

1711
01:04:47,819 --> 01:04:50,299
o cérebro faz,

1712
01:04:50,760 --> 01:04:52,740
muito legal, um tipo de

1713
01:04:52,740 --> 01:04:54,740
questão de implementação, quais são as

1714
01:04:54,740 --> 01:04:57,000
requisitos computacionais de apenas executar isso ou

1715
01:04:57,000 --> 01:04:59,400
como é o dia-a-dia de ser um

1716
01:04:59,400 --> 01:05:01,920
estudante ou pesquisador executando variantes

1717
01:05:01,920 --> 01:05:04,380
destes, como eles usam terabytes de

1718
01:05:04,380 --> 01:05:07,020
dados e você está usando computação grande

1719
01:05:07,020 --> 01:05:08,940
ou isso é algo que as pessoas podem executar

1720
01:05:08,940 --> 01:05:11,880
por conta própria  laptops,

1721
01:05:11,880 --> 01:05:13,980
acho que quase tudo que apresentei

1722
01:05:13,980 --> 01:05:17,099
hoje pode ser executado localmente, então

1723
01:05:17,099 --> 01:05:20,040
esse material é super simples, você pode executar, quero

1724
01:05:20,040 --> 01:05:20,760
dizer, você vai

1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
pensar que pode executá-lo em seu laptop se

1727
01:05:24,299 --> 01:05:25,980
quiser treinar e experimentar

1728
01:05:25,980 --> 01:05:27,420
coisas diferentes.  ser

1729
01:05:27,420 --> 01:05:30,119
bem lento, então eu recomendaria alguma

1730
01:05:30,119 --> 01:05:33,359
GPU comercial como uma, eu executo praticamente

1731
01:05:33,359 --> 01:05:35,640
tudo, como Nvidia 1080,

1732
01:05:35,640 --> 01:05:38,819
bem antiga, bem barata, mas eles têm 12

1733
01:05:38,819 --> 01:05:41,099
GB de RAM ou algo assim e é

1734
01:05:41,099 --> 01:05:43,140
mais do que suficiente para esses modelos, quatro

1735
01:05:43,140 --> 01:05:46,440
gigabytes de RAM  Eu acho que uma coisa que

1736
01:05:46,440 --> 01:05:48,839
algumas pessoas acham estranha é que eu faço a maioria

1737
01:05:48,839 --> 01:05:51,480
dos meus experimentos em coisas como mnist, então

1738
01:05:51,480 --> 01:05:54,780
são imagens de 32 por 32 pixels porque posso

1739
01:05:54,780 --> 01:05:57,299
treiná-lo pequeno e vocalmente,

1740
01:05:57,299 --> 01:06:00,000
se você quiser, meus experimentos

1741
01:06:00,000 --> 01:06:02,460
ou um interminável, se você  Se você quiser fazer coisas

1742
01:06:02,460 --> 01:06:03,540
assim, essas são muito mais

1743
01:06:03,540 --> 01:06:05,640
complicadas, este Hamiltonian Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite, aqui você está entrando em

1745
01:06:08,160 --> 01:06:09,780
modelos maiores que rodam em vários

1746
01:06:09,780 --> 01:06:12,240
GPUs e então aqui está usando um cluster para

1747
01:06:12,240 --> 01:06:14,220
executar esses tipos de modelos,

1748
01:06:14,220 --> 01:06:16,140
mas eu diria que a maioria  a única

1749
01:06:16,140 --> 01:06:18,920
máquina com GPU é mais que suficiente

1750
01:06:18,920 --> 01:06:21,680
ou até mesmo como em um notebook colaborativo

1751
01:06:21,680 --> 01:06:24,539
algo assim se você quiser treinar

1752
01:06:24,539 --> 01:06:26,520
algo no imagenet fica mais

1753
01:06:26,520 --> 01:06:29,940
complicado e você precisa de pelo

1754
01:06:29,940 --> 01:06:33,660
menos uma GPU idealmente mais, mas sim, eu

1755
01:06:33,660 --> 01:06:35,160
não faço um  um monte de coisas em grande escala

1756
01:06:35,160 --> 01:06:37,200
ainda, acho que é certamente interessante

1757
01:06:37,200 --> 01:06:39,960
e definitivamente há muito mais que você

1758
01:06:39,960 --> 01:06:42,539
pode fazer lá, mas para alguns desses tipos

1759
01:06:42,539 --> 01:06:45,480
de questões mais simples ou mais fundamentais,

1760
01:06:45,480 --> 01:06:47,760
não sei como você quer chamá-la,

1761
01:06:47,760 --> 01:06:52,500
uma máquina menor é  bom e rápido tão legal

1762
01:06:52,500 --> 01:06:54,660
útil

1763
01:06:54,660 --> 01:06:58,140
tudo bem Vou ler um comentário de Dave

1764
01:06:58,140 --> 01:07:00,780
relembrando o comentário de Bert DeVries durante

1765
01:07:00,780 --> 01:07:02,880
o Simpósio de inferência ativa aplicada

1766
01:07:02,880 --> 01:07:05,520
sobre a conveniência de gastar menos

1767
01:07:05,520 --> 01:07:08,099
esforço ou ATP em situações de forrageamento ou controle

1768
01:07:08,099 --> 01:07:09,780
onde não precisamos de muita

1769
01:07:09,780 --> 01:07:11,579
precisão Eu não  não sei se você ouviu

1770
01:07:11,579 --> 01:07:13,859
isso, mas o Professor DeVries mencionou

1771
01:07:13,859 --> 01:07:17,520
sobre modelos de precisão variável e como

1772
01:07:17,520 --> 01:07:19,440
eles poderiam ser usados ​​para

1773
01:07:19,440 --> 01:07:21,059
permitir diferentes recursos de

1774
01:07:21,059 --> 01:07:23,039
generalização e

1775
01:07:23,039 --> 01:07:25,020
treinamento de curso estrutural real, bem como

1776
01:07:25,020 --> 01:07:27,059
requisitos computacionais reduzidos,

1777
01:07:27,059 --> 01:07:29,880
ele tem alguma sugestão sobre como

1778
01:07:29,880 --> 01:07:31,980
introduzir isso  distinção em

1779
01:07:31,980 --> 01:07:33,900
inferência ativa Teoria que tipos de

1780
01:07:33,900 --> 01:07:37,760
experimentos poderiam revelar isso,

1781
01:07:38,400 --> 01:07:40,680
uau, sim, isso é algo que não tenho,

1782
01:07:40,680 --> 01:07:42,660
não acho que tenha muita inteligência

1783
01:07:42,660 --> 01:07:46,619
para dizer sobre isso, é completamente honesto, é uma

1784
01:07:46,619 --> 01:07:48,740


1785
01:07:51,359 --> 01:07:53,460
pergunta super interessante porque acho que

1786
01:07:53,460 --> 01:07:56,220
a intuição faz uma  faz muito

1787
01:07:56,220 --> 01:07:58,260
sentido para mim que

1788
01:07:58,260 --> 01:08:00,000
você está falando

1789
01:08:00,000 --> 01:08:01,980
se eu entendi corretamente as taxas variáveis

1790
01:08:01,980 --> 01:08:05,160
de precisão quando você está codificando em ou

1791
01:08:05,160 --> 01:08:07,079
em seu modelo em geral fazendo

1792
01:08:07,079 --> 01:08:07,740
computação

1793
01:08:07,740 --> 01:08:09,420
[Música]

1794
01:08:09,420 --> 01:08:11,539
hum

1795
01:08:13,200 --> 01:08:17,040
que de alguma forma tem um impacto em seu

1796
01:08:17,040 --> 01:08:19,080
desempenho futuro como  uma relação com algum

1797
01:08:19,080 --> 01:08:22,259
armazenamento de energia, eu acho que sim, e se você

1798
01:08:22,259 --> 01:08:23,580
quisesse construir isso em um

1799
01:08:23,580 --> 01:08:26,279
sistema de esforço ativo, você precisaria ter

1800
01:08:26,279 --> 01:08:28,679
realmente um sistema incorporado onde o

1801
01:08:28,679 --> 01:08:31,500
agente tivesse alguma noção de energia, como um

1802
01:08:31,500 --> 01:08:34,439
armazenamento de energia interno e

1803
01:08:34,439 --> 01:08:36,238
sim, algo que está tentando

1804
01:08:36,238 --> 01:08:38,520
para conservar enquanto está executando suas

1805
01:08:38,520 --> 01:08:40,500
ações

1806
01:08:40,500 --> 01:08:42,359
e ficando sem energia precisaria de

1807
01:08:42,359 --> 01:08:44,759
algo ruim para os agentes e

1808
01:08:44,759 --> 01:08:47,399
então talvez você pudesse observar uma espécie de

1809
01:08:47,399 --> 01:08:48,679
emergência

1810
01:08:48,679 --> 01:08:52,040
uh redução e

1811
01:08:52,040 --> 01:08:55,198
precisão de codificação ou algo assim

1812
01:08:55,198 --> 01:08:57,479
como o agente está tentando aprender

1813
01:08:57,479 --> 01:09:00,060
a  agir de forma mais eficaz, você pode ter que

1814
01:09:00,060 --> 01:09:02,520
dar a ele a capacidade de controlar sua

1815
01:09:02,520 --> 01:09:04,020
precisão,

1816
01:09:04,020 --> 01:09:07,080
sim, como eu disse com base na minha experiência, mas

1817
01:09:07,080 --> 01:09:08,819
está tudo

1818
01:09:08,819 --> 01:09:11,460
bem neste slide aqui, primeira

1819
01:09:11,460 --> 01:09:14,219
imagem muito legal, é como um

1820
01:09:14,219 --> 01:09:18,359
Jackson Pollock digital,

1821
01:09:18,359 --> 01:09:22,920
se fosse  um tamanho de dados de entrada mais simples

1822
01:09:22,920 --> 01:09:26,520
ou apenas uma complexidade reduzida de

1823
01:09:26,520 --> 01:09:27,719
padrões ou se fosse uma

1824
01:09:27,719 --> 01:09:30,000
complexidade aumentada, como essa imagem ficaria

1825
01:09:30,000 --> 01:09:31,920
diferente,

1826
01:09:31,920 --> 01:09:34,620
sim, então fiz alguns experimentos tentando

1827
01:09:34,620 --> 01:09:36,738
alterar essas

1828
01:09:36,738 --> 01:09:40,439
colunas de orientação e

1829
01:09:40,439 --> 01:09:41,100
hum,

1830
01:09:41,100 --> 01:09:43,140
você pode, sim, basicamente, alterando os

1831
01:09:43,140 --> 01:09:44,520
parâmetros do modelo, você pode  faça com que

1832
01:09:44,520 --> 01:09:47,100
essas colunas sejam maiores, você pode fazer com que

1833
01:09:47,100 --> 01:09:49,380
elas não tenham uma estrutura muito semelhante

1834
01:09:49,380 --> 01:09:51,660
ao que vemos nos humanos onde

1835
01:09:51,660 --> 01:09:53,040
temos, você pode fazer com que elas tenham mais

1836
01:09:53,040 --> 01:09:55,199
faixas de atividade,

1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
e também, como você disse, depende

1839
01:09:58,199 --> 01:10:00,540
do conjunto de dados  que você está usando se eu usar

1840
01:10:00,540 --> 01:10:03,179
classificações senoidais realmente simples

1841
01:10:03,179 --> 01:10:05,580
como entrada, obtenho algo assim, obtenho

1842
01:10:05,580 --> 01:10:07,920
algo que é um pouco mais, uh,

1843
01:10:07,920 --> 01:10:11,640
rotacional, curvo, maior entropia,

1844
01:10:11,640 --> 01:10:12,800


1845
01:10:12,800 --> 01:10:15,660
então acho que essas são todas

1846
01:10:15,660 --> 01:10:18,000
coisas interessantes se você quiser estudar o

1847
01:10:18,000 --> 01:10:19,320
surgimento de  este tipo de organização

1848
01:10:19,320 --> 01:10:22,199
em um sistema natural, uh, se você tem um

1849
01:10:22,199 --> 01:10:24,120
modelo que agora produz uma

1850
01:10:24,120 --> 01:10:25,860
organização diferente para

1851
01:10:25,860 --> 01:10:28,620
configurações diferentes, tudo bem, então quais

1852
01:10:28,620 --> 01:10:31,679
configurações melhor correspondem aos nossos dados observados, então

1853
01:10:31,679 --> 01:10:32,340


1854
01:10:32,340 --> 01:10:34,679
sim, posso,

1855
01:10:34,679 --> 01:10:36,540
posso enviar aqueles em torno dele, se você estiver

1856
01:10:36,540 --> 01:10:38,520
interessado, mas

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
sim, acho que um também outro, desculpe,

1859
01:10:44,219 --> 01:10:45,659
outro ponto interessante é

1860
01:10:45,659 --> 01:10:46,920
que,

1861
01:10:46,920 --> 01:10:51,480
uh, diferentes animais e tipos de uh

1862
01:10:51,480 --> 01:10:53,219
seletividade de orientação e diferentes

1863
01:10:53,219 --> 01:10:54,659
números de cata-ventos, alguns animais não

1864
01:10:54,659 --> 01:10:57,420
têm isso, acho que talvez ratos, se eu estiver

1865
01:10:57,420 --> 01:11:00,120
correto, tem esse tipo de

1866
01:11:00,120 --> 01:11:01,800
seletividade de sal e pimenta, então é

1867
01:11:01,800 --> 01:11:03,480
basicamente aleatório, você não tem nenhum tipo

1868
01:11:03,480 --> 01:11:04,679
de sensibilidade de orientação topográfica,

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
então há evidências de que

1871
01:11:09,300 --> 01:11:10,920
sim, sistemas diferentes fazem isso de

1872
01:11:10,920 --> 01:11:13,020
maneira diferente e é interessante

1873
01:11:13,020 --> 01:11:14,760
descobrir por que

1874
01:11:14,760 --> 01:11:17,760
sim, isso  é muito legal, primeiro me lembra

1875
01:11:17,760 --> 01:11:21,300
a

1876
01:11:21,300 --> 01:11:22,980
base e o tempo de difusão da reação,

1877
01:11:22,980 --> 01:11:25,739
então é realmente

1878
01:11:25,739 --> 01:11:30,000
possível que uma região não tenha

1879
01:11:30,000 --> 01:11:32,840
atividade de uma determinada

1880
01:11:32,840 --> 01:11:35,640
granularidade, como se estivesse sendo

1881
01:11:35,640 --> 01:11:39,360
observada na escala de tempo espacial e temporal do fmri,

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
se os bolsões de atividade, mas  se os

1884
01:11:44,699 --> 01:11:46,380
bolsões de atividade forem

1885
01:11:46,380 --> 01:11:48,480
mais lentos, mais rápidos

1886
01:11:48,480 --> 01:11:52,260
do que essa medição, não será

1887
01:11:52,260 --> 01:11:54,060
diferente do ruído, tudo terá sido calculado em

1888
01:11:54,060 --> 01:11:55,739
média,

1889
01:11:55,739 --> 01:11:58,620
então pode haver alguns

1890
01:11:58,620 --> 01:12:01,560
conjuntos de dados interessantes, como conjuntos de dados que

1891
01:12:01,560 --> 01:12:03,360
realmente têm muita

1892
01:12:03,360 --> 01:12:06,179
riqueza, mas por um lado  por uma razão ou

1893
01:12:06,179 --> 01:12:08,520
outra, a média foi calculada

1894
01:12:08,520 --> 01:12:11,100
porque não estava conectado a você

1895
01:12:11,100 --> 01:12:12,420
ou algo assim, você realmente precisa

1896
01:12:12,420 --> 01:12:14,520
ir com um único nível de teste, você precisa

1897
01:12:14,520 --> 01:12:16,140
ter uma resolução espacial alta o suficiente

1898
01:12:16,140 --> 01:12:18,719
para que você saiba que satisfaz

1899
01:12:18,719 --> 01:12:23,100
microfrequências uh  e isso

1900
01:12:23,100 --> 01:12:24,659
é algo que as pessoas não faziam há

1901
01:12:24,659 --> 01:12:25,620
muito tempo, especialmente se você estiver fazendo

1902
01:12:25,620 --> 01:12:27,780
gravações de eleitorado único, você não

1903
01:12:27,780 --> 01:12:28,920
verá uma onda viajante,

1904
01:12:28,920 --> 01:12:30,900
verá oscilações,

1905
01:12:30,900 --> 01:12:32,219
então você precisa de um multi-elétrico

1906
01:12:32,219 --> 01:12:34,199
matrizes e basicamente eles estão dizendo ok,

1907
01:12:34,199 --> 01:12:36,000
sim, agora que temos a tecnologia para

1908
01:12:36,000 --> 01:12:37,520
fazer isso,

1909
01:12:37,520 --> 01:12:40,080
persiste muita coisa que não vimos antes e,

1910
01:12:40,080 --> 01:12:42,960
potencialmente, isso é uma explicação para

1911
01:12:42,960 --> 01:12:44,400
muito do ruído que víamos

1912
01:12:44,400 --> 01:12:46,260
antes, talvez seja realmente apenas  ondas viajantes,

1913
01:12:46,260 --> 01:12:47,219


1914
01:12:47,219 --> 01:12:47,760


1915
01:12:47,760 --> 01:12:51,480
então sim, acho que há muito a ser feito

1916
01:12:51,480 --> 01:12:53,880
no futuro com maiores

1917
01:12:53,880 --> 01:12:56,520
habilidades de gravação,

1918
01:12:56,520 --> 01:12:58,739
isso é muito legal,

1919
01:12:58,739 --> 01:13:02,940
bem, quaisquer pensamentos ou perguntas finais ou

1920
01:13:02,940 --> 01:13:06,239
para onde você vai levar esse trabalho,

1921
01:13:06,239 --> 01:13:08,520
sim, não, obrigado por me receber,

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
espero, na infraestrutura ativa

1924
01:13:11,640 --> 01:13:14,520
isso é que eu adoraria, acho

1925
01:13:14,520 --> 01:13:16,560
que seria super divertido, então sim, não tenho

1926
01:13:16,560 --> 01:13:18,980
certeza se estou olhando Talvez música,

1927
01:13:18,980 --> 01:13:21,659
uh, agora, hum,

1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760
olhando para

1930
01:13:26,760 --> 01:13:30,420
outras direções malucas, não

1931
01:13:30,420 --> 01:13:33,020
quero parecer muito maluco, uh,

1932
01:13:33,020 --> 01:13:36,900
mas  Vou analisar, sim, um monte de coisas, então

1933
01:13:36,900 --> 01:13:38,580
uma coisa que está surgindo é algo que

1934
01:13:38,580 --> 01:13:40,320
enviamos ao neurops é estudar a

1935
01:13:40,320 --> 01:13:43,140
memória com ondas viajantes,

1936
01:13:43,140 --> 01:13:45,060
então esse artigo acabou de ser

1937
01:13:45,060 --> 01:13:46,860
arquivado hoje, uh,

1938
01:13:46,860 --> 01:13:48,840
como as ondas são realmente boas na codificação de

1939
01:13:48,840 --> 01:13:50,580
memórias de longo prazo  o que eu acho

1940
01:13:50,580 --> 01:13:52,100
super interessante,

1941
01:13:52,100 --> 01:13:54,120
então eu poderia ir um pouco nessa

1942
01:13:54,120 --> 01:13:55,800
direção,

1943
01:13:55,800 --> 01:13:58,920
parece bom e sim, seria muito

1944
01:13:58,920 --> 01:14:01,400
emocionante ver a ação entrar em ação

1945
01:14:01,400 --> 01:14:04,560
quando os neurônios permanecessem

1946
01:14:04,560 --> 01:14:07,620
ativos mesmo quando as patas do cachorro se

1947
01:14:07,620 --> 01:14:09,060
moviam, há

1948
01:14:09,060 --> 01:14:11,480
muitas coisas assim  sequências de ação

1949
01:14:11,480 --> 01:14:14,280
como jogar uma bola de beisebol e então

1950
01:14:14,280 --> 01:14:15,900
acontece e é como se houvesse algo

1951
01:14:15,900 --> 01:14:18,440
nessa ação que continua

1952
01:14:18,440 --> 01:14:21,179
influenciando e tendo uma

1953
01:14:21,179 --> 01:14:23,699
representação temporal profunda de ações alternativas

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
e então o autoencoder variacional

1956
01:14:29,640 --> 01:14:33,179
já é basicamente o certo, qualquer coisa

1957
01:14:33,179 --> 01:14:35,219
assim, então

1958
01:14:35,219 --> 01:14:37,739
realmente aprecio isso  tudo bem, obrigado

1959
01:14:37,739 --> 01:14:39,480
até a próxima,

1960
01:14:39,480 --> 01:14:43,339
muito obrigado, tchau


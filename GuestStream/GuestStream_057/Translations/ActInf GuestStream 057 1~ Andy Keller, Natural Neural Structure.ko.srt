1
00:00:06,600 --> 00:00:09,420
안녕하세요, 환영합니다. 2023년 9월 18일입니다.

2
00:00:09,420 --> 00:00:14,040


3
00:00:14,040 --> 00:00:16,740
Andy Keller와 함께하는 활성 게스트 스트림 57.1입니다. 인공 지능의

4
00:00:16,740 --> 00:00:19,619
자연 신경 구조에 대해 이야기할 예정입니다. 프레젠테이션과

5
00:00:19,619 --> 00:00:22,020


6
00:00:22,020 --> 00:00:24,300
토론이 있을 예정

7
00:00:24,300 --> 00:00:25,800
이므로 실시간으로 시청하고 계시다면

8
00:00:25,800 --> 00:00:27,900
편하게 시청해 주세요.  라이브 채팅에 질문을 쓰세요.

9
00:00:27,900 --> 00:00:30,660
그렇지 않으면 정말 기대하고 있는 Andy에게 감사

10
00:00:30,660 --> 00:00:32,279


11
00:00:32,279 --> 00:00:35,899
하고 프레젠테이션을

12
00:00:36,360 --> 00:00:38,940
해주셔서 감사합니다. 응 정말 감사합니다.

13
00:00:38,940 --> 00:00:41,280
초대해 주셔서 감사합니다.

14
00:00:41,280 --> 00:00:42,840
활성 참조 그룹에서 이 내용을 발표할 수 있어서 정말 기쁩니다.

15
00:00:42,840 --> 00:00:45,239
나는 팬이고 매우

16
00:00:45,239 --> 00:00:49,200
관심이 많으므로

17
00:00:49,200 --> 00:00:50,399
잘 토론하고

18
00:00:50,399 --> 00:00:51,960
여러분이 그것에 대해 어떻게 생각하는지 알아보시기 바랍니다. 음,

19
00:00:51,960 --> 00:00:54,840
제 이름은 Andy입니다. 저는 암스테르담 대학

20
00:00:54,840 --> 00:00:57,180
에서 Maxwelling의 지도 하에 박사 과정을 마무리하고 있습니다. 음 저는

21
00:00:57,180 --> 00:00:59,100


22
00:00:59,100 --> 00:01:01,500


23
00:01:01,500 --> 00:01:05,099
그 후에 하버드 박사후 과정을 시작하겠습니다. 그럼 시작하겠습니다.

24
00:01:05,099 --> 00:01:07,619
제 작업의 일반적인 목표는

25
00:01:07,619 --> 00:01:09,540
현대 인공

26
00:01:09,540 --> 00:01:12,540
지능을 인간과 유사한 일반화에 더 가깝게 만드는 것입니다.

27
00:01:12,540 --> 00:01:15,240
따라서

28
00:01:15,240 --> 00:01:17,159
이것이 의미하는 바는 아마도 일부일 것입니다.  일종의 구조

29
00:01:17,159 --> 00:01:18,960
일반화,

30
00:01:18,960 --> 00:01:20,700
음 또는 어쩌면 우리가 인간이 가지고 있다고 믿는 구조화된 세계 모델과 같은 활동적인 유아 위원회에 더 친숙할 수도

31
00:01:20,700 --> 00:01:22,080


32
00:01:22,080 --> 00:01:24,299


33
00:01:24,299 --> 00:01:26,340
있고 이를 수행하기 위해 우리가 제안하는 방식은

34
00:01:26,340 --> 00:01:28,799
자연 신경

35
00:01:28,799 --> 00:01:32,400
구조를 인공 지능에 통합하는 것이므로

36
00:01:32,400 --> 00:01:34,979
먼저 우리가 의미하는 바를 정의합시다.

37
00:01:34,979 --> 00:01:36,720
구조 일반화를 통해

38
00:01:36,720 --> 00:01:38,400


39
00:01:38,400 --> 00:01:40,380
현대 기계 학습이 전통적인 의미의

40
00:01:40,380 --> 00:01:42,900
훈련 세트를 넘어 일반화한다고 말하는 것은 논란의 여지가 없다고 생각합니다.

41
00:01:42,900 --> 00:01:45,000
예를 들어

42
00:01:45,000 --> 00:01:46,799
최초의 인공 신경망

43
00:01:46,799 --> 00:01:49,020
다층 퍼셉트론조차도 이와

44
00:01:49,020 --> 00:01:51,659
같은 이미지 데이터 세트에 대해 훈련을 받고

45
00:01:51,659 --> 00:01:55,140
높은 수준을 달성할 수 있습니다.  정확도는 이전에

46
00:01:55,140 --> 00:01:56,880


47
00:01:56,880 --> 00:01:58,320
본 적이 없는 테스트 이미지 세트를 제시했을 때에도

48
00:01:58,320 --> 00:02:00,240


49
00:02:00,240 --> 00:02:02,460
동일한 수준의 정확도로 상대적으로 쉽게 분류할 수 있으며

50
00:02:02,460 --> 00:02:04,500
이를 일반적으로

51
00:02:04,500 --> 00:02:07,320
일반화라고 부르지만 상당히

52
00:02:07,320 --> 00:02:08,758
초기에는  예를 들어 이러한

53
00:02:08,758 --> 00:02:10,800
시스템이

54
00:02:10,800 --> 00:02:12,720
이미지에 적용되는 작은 변화 또는 변형으로 인해 어려움을 겪고 있다는 사실을 알아차렸습니다. 그렇다면

55
00:02:12,720 --> 00:02:16,340


56
00:02:18,920 --> 00:02:23,099
이것이 왜 놀라운 일인지 생각하고

57
00:02:23,099 --> 00:02:24,720
실제로

58
00:02:24,720 --> 00:02:26,640
이러한 유형의

59
00:02:26,640 --> 00:02:28,680
구조 일반화를 수행하는 우리의 타고난 능력 때문에 이

60
00:02:28,680 --> 00:02:31,920
예가 실패했다고 주장합니다.  어

61
00:02:31,920 --> 00:02:33,239
예를 들어 이 변화는

62
00:02:33,239 --> 00:02:35,340
우리가 거의 감지할 수 없으며

63
00:02:35,340 --> 00:02:37,560
자동으로 처리하는 반면 시스템에서는

64
00:02:37,560 --> 00:02:39,959
매우 분명히 중요한 문제이므로

65
00:02:39,959 --> 00:02:41,940
구조 일반화는 입력의

66
00:02:41,940 --> 00:02:44,819
일부 대칭 변환에 대한 일반화

67
00:02:44,819 --> 00:02:47,040
또는 이

68
00:02:47,040 --> 00:02:48,959
경우 대칭이라고 말할 수 있습니다.  변환은

69
00:02:48,959 --> 00:02:50,580
숫자 클래스를 변경하지 않고 유지하는 작은 이동

70
00:02:50,580 --> 00:02:52,019


71
00:02:52,019 --> 00:02:54,599
이므로 분명한 질문은

72
00:02:54,599 --> 00:02:56,340
이 자연 구조가 정확히 무엇을 의미하는지,

73
00:02:56,340 --> 00:02:58,860
이것이

74
00:02:58,860 --> 00:03:01,620
이러한 설정에 도움이 될 것이라고 생각하는 이유는 무엇입니까?

75
00:03:01,620 --> 00:03:03,780
먼저 자연 신경이 무엇을 의미하는지 이야기해 보겠습니다.

76
00:03:03,780 --> 00:03:05,819
구조

77
00:03:05,819 --> 00:03:08,700
음 시스템의 구조나 모든 유형의 편향에 대해 이야기하는 한 가지 방법은

78
00:03:08,700 --> 00:03:11,640


79
00:03:11,640 --> 00:03:14,040
귀납적 편향이므로 귀납적 편향은

80
00:03:14,040 --> 00:03:16,080


81
00:03:16,080 --> 00:03:17,940


82
00:03:17,940 --> 00:03:19,440
모델

83
00:03:19,440 --> 00:03:22,440
선택을 보다 구어적으로 수행할 때 실현 가능한 가설 세트에 대한 적절한 제한으로 느슨하게 정의할 수 있습니다.  데이터를

84
00:03:22,440 --> 00:03:24,480
보기 전에 이를 다음과 같이 부를 수 있습니다.

85
00:03:24,480 --> 00:03:27,060


86
00:03:27,060 --> 00:03:29,580
이는 무엇을 어떻게 배울 수 있는지에 대한 제한이므로 매우 광범위하게 여기에는

87
00:03:29,580 --> 00:03:32,519
모델 클래스부터

88
00:03:32,519 --> 00:03:34,739
최적화 절차 또는 심지어 하이퍼 매개

89
00:03:34,739 --> 00:03:37,379
변수까지 모든 것이 포함될 수 있으며 어떤 의미에서는 실제로

90
00:03:37,379 --> 00:03:39,739


91
00:03:39,739 --> 00:03:43,140
학습 가능한 것이 무엇인지 정의합니다.  그리고 이는 귀납적 솔루션 없이 훈련 세트를

92
00:03:43,140 --> 00:03:44,819


93
00:03:44,819 --> 00:03:47,220
넘어서 일반화할 수 없다는 점에서 일반화를 정의합니다.

94
00:03:47,220 --> 00:03:48,420


95
00:03:48,420 --> 00:03:50,220
이는

96
00:03:50,220 --> 00:03:52,560
David Wolford가 이 문서에서 더 자세히 설명하므로

97
00:03:52,560 --> 00:03:55,500
자연적 귀납적 편향이 의미하는 것은

98
00:03:55,500 --> 00:03:58,620


99
00:03:58,620 --> 00:04:00,000
제한과 제한에서 비롯되는 편향입니다.

100
00:04:00,000 --> 00:04:02,040
자연

101
00:04:02,040 --> 00:04:04,500
시스템이 직면하는 것 uh

102
00:04:04,500 --> 00:04:06,900
현실 세계에서 살아야 하는 특성상 예를 들어

103
00:04:06,900 --> 00:04:08,459
뇌는 구성 특성상 많은 효율성 제약

104
00:04:08,459 --> 00:04:10,439
과 물리적 제약을 가지고 있습니다

105
00:04:10,439 --> 00:04:13,140
uh 그리고 이

106
00:04:13,140 --> 00:04:14,580
논리에 따르면 이러한 제약은 실제로

107
00:04:14,580 --> 00:04:16,620
우리의 일반화에서 어느 정도 역할을 합니다.

108
00:04:16,620 --> 00:04:19,798
현재 현대 인공 지능을 능가하는 능력은

109
00:04:19,798 --> 00:04:21,478


110
00:04:21,478 --> 00:04:24,720
다음에 설명하겠습니다. 따라서 이 강연에서는 제가 연구한

111
00:04:24,720 --> 00:04:27,540
두 가지 유형의 구조,

112
00:04:27,540 --> 00:04:29,880
즉

113
00:04:29,880 --> 00:04:31,699
지형적 조직과

114
00:04:31,699 --> 00:04:34,320
시공간 역학에 구체적으로 초점을 맞출 것입니다. 작업을 시작하기 전에

115
00:04:34,320 --> 00:04:36,120


116
00:04:36,120 --> 00:04:38,759
내가 이전에 말했던

117
00:04:38,759 --> 00:04:41,400
구조 일반화를 달성하는 데 자연 구조가 유용할 수 있다고 믿는 이유에 대한 간단한 예를 들어 보겠습니다.

118
00:04:41,400 --> 00:04:42,780


119
00:04:42,780 --> 00:04:44,280


120
00:04:44,280 --> 00:04:47,479
첫 번째 예는 실제로 직접적으로 해결하기 위해 지어진 1980년대

121
00:04:47,479 --> 00:04:49,199
후쿠시마의 신인지 전면

122
00:04:49,199 --> 00:04:51,479
건축에서 나옵니다.

123
00:04:51,479 --> 00:04:53,520


124
00:04:53,520 --> 00:04:55,440


125
00:04:55,440 --> 00:04:57,300
이러한 작은 변화와 변형에 대한 견고성의 문제

126
00:04:57,300 --> 00:04:59,759
이므로 서류 작업에서 그는

127
00:04:59,759 --> 00:05:02,040


128
00:05:02,040 --> 00:05:04,380


129
00:05:04,380 --> 00:05:06,780


130
00:05:06,780 --> 00:05:08,699
이러한 왜곡에 대한 견고성을 달성하기 위해 동공과 족제비의 계층 및 풀링 측정에서 영감을 얻었으며

131
00:05:08,699 --> 00:05:11,160
그림을 보면 U sub S1 U라고 씁니다.

132
00:05:11,160 --> 00:05:14,100
sub C1과 이것들은 단순하고

133
00:05:14,100 --> 00:05:16,919
복잡한 세포를 의미합니다. 따라서 이것은

134
00:05:16,919 --> 00:05:18,840
당시에는 상당히 급진적인 접근 방식이었지만

135
00:05:18,840 --> 00:05:20,699
실제로는 초기 인공 신경망을 괴롭히는 견고성과 변화를 개선하는 데 도움이 되었으며

136
00:05:20,699 --> 00:05:22,199


137
00:05:22,199 --> 00:05:24,419
시간이 지남에 따라

138
00:05:24,419 --> 00:05:25,860
이러한 아이디어는 단순화되고

139
00:05:25,860 --> 00:05:28,919
추상화되었으며  분명히

140
00:05:28,919 --> 00:05:30,539
컨볼루셔널 신경망을 산출했기 때문에

141
00:05:30,539 --> 00:05:32,759
오늘날 우리는 그것이 궁극적으로

142
00:05:32,759 --> 00:05:35,580
딥 러닝 혁명의 성공을 이끈 것으로 알고 있습니다. 이것은 실제로

143
00:05:35,580 --> 00:05:36,960


144
00:05:36,960 --> 00:05:39,479
구조 일반화를 달성한 자연스러운 귀납적 편향의 예입니다.

145
00:05:39,479 --> 00:05:42,120


146
00:05:42,120 --> 00:05:43,919


147
00:05:43,919 --> 00:05:45,900
모델은 매우 잘 작동합니다.

148
00:05:45,900 --> 00:05:47,280


149
00:05:47,280 --> 00:05:49,320
그리고 이 원리가

150
00:05:49,320 --> 00:05:51,360
더

151
00:05:51,360 --> 00:05:53,940
추상적이고 더 추상적인 변환

152
00:05:53,940 --> 00:05:57,000
및 대칭을 포괄하기 위해 잠재적으로 일반화될 수 있는지 확인합니다.

153
00:05:57,000 --> 00:06:00,360
따라서 컨볼루션이 이

154
00:06:00,360 --> 00:06:02,060
구조 일반화를

155
00:06:02,060 --> 00:06:04,440
직관적으로 달성하도록 만드는 것은

156
00:06:04,440 --> 00:06:06,720
동일한 필터를 적용하거나

157
00:06:06,720 --> 00:06:08,699
기능 추출기에서 수행된다는 것을 알 수 있습니다.  다양한 공간

158
00:06:08,699 --> 00:06:10,680
위치에서 단일

159
00:06:10,680 --> 00:06:12,660
컨볼루셔널 필터가

160
00:06:12,660 --> 00:06:14,820
이미지의 모든 위치에 적용되는 것을 볼 수 있습니다. 이는

161
00:06:14,820 --> 00:06:16,259
입력이

162
00:06:16,259 --> 00:06:18,000


163
00:06:18,000 --> 00:06:20,400
이미지의 중앙에 있든 오른쪽에 있든 상관없이

164
00:06:20,400 --> 00:06:22,080
정확히 동일한 기능을 갖게 된다는 것을 의미합니다.  한 가지

165
00:06:22,080 --> 00:06:23,580
예외를 제외하면 그들은 동등하게

166
00:06:23,580 --> 00:06:24,660
이동될 것이므로

167
00:06:24,660 --> 00:06:26,819
수학적으로 이러한 유형의 매핑을 동형(

168
00:06:26,819 --> 00:06:29,160
homomorphism)이라고 하며

169
00:06:29,160 --> 00:06:30,840


170
00:06:30,840 --> 00:06:33,180
이 경우 입력 공간과 출력 공간의 대수적 구조를 유지합니다. 이 경우

171
00:06:33,180 --> 00:06:35,580
번역과 관련되며

172
00:06:35,580 --> 00:06:37,440
단순한 간단한 수준에서는 다음과 같습니다.

173
00:06:37,440 --> 00:06:38,880


174
00:06:38,880 --> 00:06:40,259
이 강연의 나머지 부분에서 기억해야 할 중요한 점은 변환 교환 다이어그램을 사용하여 이러한 커뮤니티 정류가 있음을 알 수 있으면

175
00:06:40,259 --> 00:06:42,300
특징 추출기의 동형을 확인할 수 있다는 것입니다.

176
00:06:42,300 --> 00:06:45,000


177
00:06:45,000 --> 00:06:46,740


178
00:06:46,740 --> 00:06:49,560


179
00:06:49,560 --> 00:06:51,720
따라서 특징 추출기가 다음을 보여줌으로써 이를 대수적으로 작성할 수도 있습니다.

180
00:06:51,720 --> 00:06:53,759
f는

181
00:06:53,759 --> 00:06:55,080
변환 연산자 t를 사용하여 통근하며

182
00:06:55,080 --> 00:06:57,000


183
00:06:57,000 --> 00:06:58,919
기본적으로 우리가 원하는 것은

184
00:06:58,919 --> 00:07:00,600
먼저

185
00:07:00,600 --> 00:07:02,639
특성을 추출한 다음

186
00:07:02,639 --> 00:07:04,740
변환을 수행하거나

187
00:07:04,740 --> 00:07:06,240
변환을 수행한 다음

188
00:07:06,240 --> 00:07:08,639
특성을 추출하는 것 사이에 차이가 없다는 것입니다. 따라서 현재의 문제는

189
00:07:08,639 --> 00:07:10,199


190
00:07:10,199 --> 00:07:11,880
우리가 실제로 알지 못한다는 것입니다.  현실 세계에서 볼 수 있는

191
00:07:11,880 --> 00:07:13,620
더 복잡한 변환과 관련하여 동형구조를 구성하는 방법

192
00:07:13,620 --> 00:07:15,240


193
00:07:15,240 --> 00:07:18,060
예를 들어 우리의 뇌는

194
00:07:18,060 --> 00:07:20,099
조명과 계절의 변화를 자연스럽게 처리할 수 있습니다.

195
00:07:20,099 --> 00:07:22,259


196
00:07:22,259 --> 00:07:24,599
음 여기서는 사람의 얼굴에 조명이 비치

197
00:07:24,599 --> 00:07:26,160
거나 계절의 변화를

198
00:07:26,160 --> 00:07:27,840
알 수 있습니다.  같은 얼굴이나 같은

199
00:07:27,840 --> 00:07:29,880
도로이지만 이러한 변환을 존중하는 모델을 구축하는 방법을 모르기

200
00:07:29,880 --> 00:07:31,319


201
00:07:31,319 --> 00:07:33,180
때문에

202
00:07:33,180 --> 00:07:35,520
이를

203
00:07:35,520 --> 00:07:37,620
강력하고 예측 가능한 방식으로 처리하여 내가 제시한 것의

204
00:07:37,620 --> 00:07:40,259
훨씬 더 추상적인 예를 제공하는 시스템을 구축하기가 어렵습니다.

205
00:07:40,259 --> 00:07:41,759
이는

206
00:07:41,759 --> 00:07:43,620


207
00:07:43,620 --> 00:07:45,440
대칭을 처리하지 않는 모델의 잠재적인 부정적인 영향을 의미합니다. 변환은

208
00:07:45,440 --> 00:07:48,060
최신 텍스트를 이미지 생성 프로그램으로 고려하므로

209
00:07:48,060 --> 00:07:50,520
이 예에서는

210
00:07:50,520 --> 00:07:53,940
Dolly에게

211
00:07:53,940 --> 00:07:55,620
달에 있는 테디베어 이미지를 생성하도록 요청했는데 이 작업이

212
00:07:55,620 --> 00:07:57,180
엄청나게 잘 수행되었습니다.  아마도

213
00:07:57,180 --> 00:08:00,960
내가 할 수 있는 것보다 더 나을 것입니다. 어 믿을 수 없을

214
00:08:00,960 --> 00:08:03,360
정도로 상세한 질감이 있습니다. 그러나 빨간색 큐브 위에

215
00:08:03,360 --> 00:08:05,340


216
00:08:05,340 --> 00:08:08,039
파란색 큐브를 그리는 것과 같이 개념적으로 더 간단한 작업을 수행하도록 요청하면

217
00:08:08,039 --> 00:08:10,560


218
00:08:10,560 --> 00:08:13,380
이 작업이 실패하고 나에게 이것은 직관적이지 않은 것 같습니다.

219
00:08:13,380 --> 00:08:15,300
두 번째 작업은

220
00:08:15,300 --> 00:08:18,180
훨씬 쉬워 보이지만 제가

221
00:08:18,180 --> 00:08:19,860
주장하는 것은 이것이 놀라운 이유가

222
00:08:19,860 --> 00:08:21,599


223
00:08:21,599 --> 00:08:23,580
Amnest 번역 예가

224
00:08:23,580 --> 00:08:25,560
놀라운 것과 정확히 같은 이유라는 것입니다. 여기에서 대칭

225
00:08:25,560 --> 00:08:28,020
변형이 발생합니다. 즉,

226
00:08:28,020 --> 00:08:29,400


227
00:08:29,400 --> 00:08:31,740
테디 베어의 이러한 복잡한 개체 사이의 변형입니다.  그리고 달과

228
00:08:31,740 --> 00:08:34,320
우리가

229
00:08:34,320 --> 00:08:36,360
직관적으로 네트워크가

230
00:08:36,360 --> 00:08:38,820
처리하고 존중할 수 있을 것으로 기대하는 이러한 단순한 큐브 개체는

231
00:08:38,820 --> 00:08:40,919


232
00:08:40,919 --> 00:08:43,380
후쿠시마의 작업이 이러한

233
00:08:43,380 --> 00:08:46,200
자연적인 계층 구조와

234
00:08:46,200 --> 00:08:47,700
시각 시스템의 풀링을 보여준 것과는 다르다는 것을 알 수 있습니다.

235
00:08:47,700 --> 00:08:49,680


236
00:08:49,680 --> 00:08:52,380
작은 변환을 일반화하는 데 효과적입니다. 저는 이러한

237
00:08:52,380 --> 00:08:54,060
추상적인 일반화 문제를 해결하려면 잠재적으로 더 높은 수준의 구조가 필요할 수 있다고 주장합니다.

238
00:08:54,060 --> 00:08:55,740


239
00:08:55,740 --> 00:08:58,200


240
00:08:58,200 --> 00:09:01,380
따라서 제가

241
00:09:01,380 --> 00:09:04,380
연구하고 묻는 질문은

242
00:09:04,380 --> 00:09:06,060
이 구조가 무엇이고 어떻게

243
00:09:06,060 --> 00:09:08,640
구현하는지입니다.  이것은

244
00:09:08,640 --> 00:09:10,080
실제로

245
00:09:10,080 --> 00:09:14,120
계산을 수행하는 데 사용될 수 있는 인공 신경망 아키텍처이므로

246
00:09:14,880 --> 00:09:17,700


247
00:09:17,700 --> 00:09:19,680


248
00:09:19,680 --> 00:09:22,380
지형 구성에 대한 첫 번째 작업으로 뛰어들어

249
00:09:22,380 --> 00:09:25,260
지형 구성이

250
00:09:25,260 --> 00:09:27,060
일차 시각 피질 사파이어 수준 영역에서 뇌 전체에 걸쳐 광범위하게 관찰되도록 대답하기 시작합니다.

251
00:09:27,060 --> 00:09:29,760


252
00:09:29,760 --> 00:09:31,500


253
00:09:31,500 --> 00:09:33,540


254
00:09:33,540 --> 00:09:35,760
서로 가까이 있는 뉴런이 유사한

255
00:09:35,760 --> 00:09:38,220
것에 반응하는 경향이 있다는 것은 매우 느슨하게 설명할 수 있습니다. 예를 들어 왼쪽에는 지향성 선에 대한 반응으로 일차 디지털 피질에 있는

256
00:09:38,220 --> 00:09:39,720
각 뉴런의 색상으로 구분된 선호도가 나와 있습니다.

257
00:09:39,720 --> 00:09:42,959


258
00:09:42,959 --> 00:09:45,360
그리고 우리는

259
00:09:45,360 --> 00:09:46,740
부드럽게 변화하는

260
00:09:46,740 --> 00:09:48,779
선택성 세트를 볼 수 있습니다. 또 다른 유형의

261
00:09:48,779 --> 00:09:50,580
조직은

262
00:09:50,580 --> 00:09:52,560


263
00:09:52,560 --> 00:09:54,600
시각 피질의 근처 뉴런이 근처의 수용 필드에 반응하는 경향이 있는 망막 주제 조직으로 알려져 있습니다.

264
00:09:54,600 --> 00:09:56,399


265
00:09:56,399 --> 00:09:58,560
그러나 이 조직은

266
00:09:58,560 --> 00:10:01,080
이러한 하위 수준 기능에 국한되지 않고 좀 더 복잡한 기능을 확장합니다.

267
00:10:01,080 --> 00:10:02,519


268
00:10:02,519 --> 00:10:05,459
얼굴, 사물 또는 장소에 존재하는 것과 같은 특징은

269
00:10:05,459 --> 00:10:07,920


270
00:10:07,920 --> 00:10:10,080


271
00:10:10,080 --> 00:10:12,779
방추형 얼굴 영역 FFA 및

272
00:10:12,779 --> 00:10:15,420
측두엽 얼굴 영역 PPA와 같은 소위 기능적으로 특정 뇌 영역과 관련이 있으므로

273
00:10:15,420 --> 00:10:19,200
이 연구에서 다시 주요 아이디어는

274
00:10:19,200 --> 00:10:21,300
아마도 이것이라는 것입니다.

275
00:10:21,300 --> 00:10:23,580
어떤 의미에서

276
00:10:23,580 --> 00:10:25,080
컨볼루션

277
00:10:25,080 --> 00:10:27,980
작업 및 후쿠시마의 아키텍처와 밀접하게 관련된 지형학적 조직을 통해

278
00:10:27,980 --> 00:10:30,660


279
00:10:30,660 --> 00:10:33,420
이것의 이점을 보다 추상적인 변환으로 일반화할 수 있습니다.

280
00:10:33,420 --> 00:10:34,920
즉, 우리가 할 수 없는 더 복잡한 동형을 구축하는 방법을 배울 수 있습니다.

281
00:10:34,920 --> 00:10:36,839


282
00:10:36,839 --> 00:10:38,940
지금 당장 분석적으로 해보세요.

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
우리가

285
00:10:42,480 --> 00:10:44,760
이 아이디어에 완전히 미친 것은 아니라는 것을 보여주기 위해 어 어

286
00:10:44,760 --> 00:10:46,320


287
00:10:46,320 --> 00:10:49,880


288
00:10:49,880 --> 00:10:54,060
90년대 초반과 2000년대 초반에 Conan Galaxy Barden과 같은 사람들이 이 영역에 대한 이전 작업을 한 적이 있는데

289
00:10:54,060 --> 00:10:55,800
그들은 지형학적 조직이 어떻게 될 수 있는지 연구했습니다.

290
00:10:55,800 --> 00:10:57,720


291
00:10:57,720 --> 00:11:01,320
주로 선형 모델에서 분산을 학습하는 데 유용하므로

292
00:11:01,320 --> 00:11:03,060
공간에 들어갈 때 우리가 묻는 질문은

293
00:11:03,060 --> 00:11:04,680


294
00:11:04,680 --> 00:11:07,140


295
00:11:07,140 --> 00:11:08,880


296
00:11:08,880 --> 00:11:10,800
현대 심층 신경망 아키텍처에 통합할 수 있는 이러한 접근 방식에서 활용할 수 있는 가장 확장 가능한 추상 메커니즘이 무엇인지이며 궁극적으로 다음과 같이

297
00:11:10,800 --> 00:11:12,959


298
00:11:12,959 --> 00:11:15,000
결정했습니다.

299
00:11:15,000 --> 00:11:16,260
제 생각에

300
00:11:16,260 --> 00:11:17,519
이 커뮤니티의 사람들에게 흥미로울 수 있는 생성 모델링 접근 방식을 사용하면

301
00:11:17,519 --> 00:11:18,779


302
00:11:18,779 --> 00:11:21,779


303
00:11:21,779 --> 00:11:23,579
지형 사전 분포를 부과하여 공간에 대한 지형 특징을 학습할 수 있다는 기본 아이디어를 사용하여 지형 독립 구성 요소 분석과 더 밀접하게 연관시킬 수 있습니다.

304
00:11:23,579 --> 00:11:26,040


305
00:11:26,040 --> 00:11:28,320


306
00:11:28,320 --> 00:11:30,660


307
00:11:30,660 --> 00:11:32,940
우리의 잠재

308
00:11:32,940 --> 00:11:34,440
변수에 대해

309
00:11:34,440 --> 00:11:37,079
간단히 설명하자면

310
00:11:37,079 --> 00:11:39,120
대부분의 사람들이 이미 이것에 익숙할 것이라고 가정합니다. 음

311
00:11:39,120 --> 00:11:40,440


312
00:11:40,440 --> 00:11:42,540
하지만 일반적인 가정의 종류는

313
00:11:42,540 --> 00:11:44,339
뇌가 생성 모델이고

314
00:11:44,339 --> 00:11:45,720
어떤 의미에서 이 아이디어는

315
00:11:45,720 --> 00:11:48,060
19세기의 헬름홀트에 기인할 수 있다는 것입니다

316
00:11:48,060 --> 00:11:50,459
uh  그는 우리가

317
00:11:50,459 --> 00:11:52,140
보는 것이

318
00:11:52,140 --> 00:11:54,420
우리의 두뇌가 눈 내의

319
00:11:54,420 --> 00:11:56,519
광자 흡수로 인한 가장 가능성 있는 원인을 계산하는 계산 문제에 대한 해결책이라고 말했습니다.

320
00:11:56,519 --> 00:11:59,519
따라서

321
00:11:59,519 --> 00:12:01,920
이 이미지를 보여주면 즉시

322
00:12:01,920 --> 00:12:03,720


323
00:12:03,720 --> 00:12:05,760
곡률이 있는 지구본으로 인식할 수 있습니다.

324
00:12:05,760 --> 00:12:07,620
마찬가지로 왜곡된 관점을 가진 디스크일 수도 있습니다.

325
00:12:07,620 --> 00:12:09,600
이것이 우리가

326
00:12:09,600 --> 00:12:12,660
착시 또는 이미지를 얻는 방법입니다.

327
00:12:12,660 --> 00:12:14,880
이와 같이 구조

328
00:12:14,880 --> 00:12:17,100
때문에 두뇌는 여기에 큐브가 있다고 추론

329
00:12:17,100 --> 00:12:18,720
하지만 실제로는 평평한

330
00:12:18,720 --> 00:12:19,860
종이 조각일 뿐입니다.

331
00:12:19,860 --> 00:12:22,740


332
00:12:22,740 --> 00:12:24,480


333
00:12:24,480 --> 00:12:26,160


334
00:12:26,160 --> 00:12:28,140
프로그램의 역그래픽 프로그램과 같은 생성 모델 측면을 생각할 수 있습니다. 구의 추상 속성은

335
00:12:28,140 --> 00:12:30,660
위치,

336
00:12:30,660 --> 00:12:32,820
조명 크기로 알려져 있으며, 이러한 속성은

337
00:12:32,820 --> 00:12:34,560
구를 투영하여

338
00:12:34,560 --> 00:12:37,440
렌더링되는 2D 이미지를 생성하는 데 사용됩니다.  따라서 사실상

339
00:12:37,440 --> 00:12:40,019
Humboldt와 다른 사람들이 말하는 것은

340
00:12:40,019 --> 00:12:41,940
생성 모델로서 뇌가

341
00:12:41,940 --> 00:12:43,680
실제로 이

342
00:12:43,680 --> 00:12:45,959
생성 과정을 반전시키고 추론을 수행하며

343
00:12:45,959 --> 00:12:48,300
우리 감각의 근본적인 원인을 추론하려고 한다는 것입니다. 따라서

344
00:12:48,300 --> 00:12:49,680


345
00:12:49,680 --> 00:12:51,600
제가 이 점에 대해 장황하게 설명하는 이유는 다음과 같습니다.

346
00:12:51,600 --> 00:12:53,100


347
00:12:53,100 --> 00:12:55,440
오늘은 생성 모델에 관해 이야기가 많이 나옵니다.

348
00:12:55,440 --> 00:12:57,180
음, 꼭

349
00:12:57,180 --> 00:12:59,639
이미지나 예쁜 그림을 생성하는 것에 대해서만 이야기하는 것은 아닙니다.

350
00:12:59,639 --> 00:13:01,260


351
00:13:01,260 --> 00:13:03,120
음 저는

352
00:13:03,120 --> 00:13:07,920
비지도 학습을 위한 프레임워크를 의미하고 싶습니다.

353
00:13:07,920 --> 00:13:10,019
그러면 좀 더 세부적으로 알아보겠습니다.

354
00:13:10,019 --> 00:13:11,700


355
00:13:11,700 --> 00:13:14,279
지형적 사전 생성 모델은

356
00:13:14,279 --> 00:13:16,019
일반적으로

357
00:13:16,019 --> 00:13:18,720
관찰 X와

358
00:13:18,720 --> 00:13:21,720
Zuh라고 부르는 잠재 변수에 대한 공동 분포로 설명되며

359
00:13:21,720 --> 00:13:23,940
이는 일반적으로 인수분해되거나

360
00:13:23,940 --> 00:13:25,620
이것이 수행되는 한 가지 방법은

361
00:13:25,620 --> 00:13:28,440
Z의 사전 P로 인수분해되며 이는 사실입니다.

362
00:13:28,440 --> 00:13:30,420
생성 모델 조건부 생성

363
00:13:30,420 --> 00:13:33,420
모델 P(x 주어진 Z)에 대해 생각할 수 있는 한 가지 방법은

364
00:13:33,420 --> 00:13:34,980


365
00:13:34,980 --> 00:13:37,019
사전이 생성 모델을 반전할 때 생성되는

366
00:13:37,019 --> 00:13:38,880
각 코드 유형에 대한 상대적 페널티를 인코딩하는 것으로 볼 수 있다는 것입니다.

367
00:13:38,880 --> 00:13:41,279


368
00:13:41,279 --> 00:13:42,899
이를 컴퓨팅이라고 합니다.

369
00:13:42,899 --> 00:13:45,920
X가 주어진 Z의 후방 P

370
00:13:45,920 --> 00:13:49,139
및 지형적 잠재 공간을 개발하기 위해

371
00:13:49,139 --> 00:13:50,639
우리는 일종의

372
00:13:50,639 --> 00:13:53,279
지형학적 사전을 도입하고 싶거나

373
00:13:53,279 --> 00:13:55,620
이 지형학적 ICA 작업이 보여준 것은

374
00:13:55,620 --> 00:13:57,720
그룹

375
00:13:57,720 --> 00:13:59,700
희소성 페널티

376
00:13:59,700 --> 00:14:01,500
와 동일하므로 사람들은 전형적인

377
00:14:01,500 --> 00:14:03,180
독립 학습 분석의 희소성 페널티

378
00:14:03,180 --> 00:14:04,560


379
00:14:04,560 --> 00:14:06,540
활성화가 희박해지기를 원합니다. 즉, 활성화 중 많은 부분이

380
00:14:06,540 --> 00:14:09,420
제로 와우이므로 이와 같이 보일 수 있습니다.

381
00:14:09,420 --> 00:14:10,680


382
00:14:10,680 --> 00:14:12,300
활성화된 파란색 사각형이 많이 있지만 대부분

383
00:14:12,300 --> 00:14:14,459
은 활성화되지 않지만 특히

384
00:14:14,459 --> 00:14:16,740
그룹의  Varsity 페널티 우리는

385
00:14:16,740 --> 00:14:18,839
이러한 사전 변수가

386
00:14:18,839 --> 00:14:21,600
이러한 분산된 희소 활성화에 더 낮은 확률을 할당

387
00:14:21,600 --> 00:14:24,720
하고 이러한 그룹화된

388
00:14:24,720 --> 00:14:26,940
조밀하게 포장된 표현에 더

389
00:14:26,940 --> 00:14:28,860
높은 확률을 할당하기를 원합니다.

390
00:14:28,860 --> 00:14:30,720
사물이 서로 더 가까울 때 더 낮은 페널티로 분산될 때 더 높은 페널티라고 생각할 수도 있습니다.

391
00:14:30,720 --> 00:14:33,540


392
00:14:33,540 --> 00:14:36,380
이것은

393
00:14:36,380 --> 00:14:39,060
이렇게 추상적으로 작성할 수 있지만

394
00:14:39,060 --> 00:14:41,160


395
00:14:41,160 --> 00:14:42,779
여기 있는 이 뉴런 각각은

396
00:14:42,779 --> 00:14:44,220
우리 모델의 뉴런을 나타내고

397
00:14:44,220 --> 00:14:46,560
이 2D 그리드로 구성되어 있으므로

398
00:14:46,560 --> 00:14:48,120
그룹화에 관해 이야기할 때 실제로 의미하는 것은 다음과 같습니다.

399
00:14:48,120 --> 00:14:50,820
2D 토폴로지로 그룹화하므로

400
00:14:50,820 --> 00:14:53,100
정말 흥미롭고 중요한 점 중 하나는

401
00:14:53,100 --> 00:14:55,560
이러한

402
00:14:55,560 --> 00:14:57,779
사전이 지형학적

403
00:14:57,779 --> 00:15:00,540
구성을 제공할 뿐만 아니라 erosi Marcelli 및 Bruno와

404
00:15:00,540 --> 00:15:02,459
같은 사람들이 주목하거나 연구한 결과

405
00:15:02,459 --> 00:15:05,760
도

406
00:15:05,760 --> 00:15:07,740
실제로 적합하다는 것입니다.  자연 데이터의 통계,

407
00:15:07,740 --> 00:15:08,959


408
00:15:08,959 --> 00:15:11,180
특히 자연 이미지에 대한 통계는

409
00:15:11,180 --> 00:15:14,100
이러한 유형의

410
00:15:14,100 --> 00:15:16,139
사전을 사용하면 실제로 더 희박한 활성화 세트를 얻게 된다는 것을 보여주었습니다. 이는

411
00:15:16,139 --> 00:15:18,839
사전이

412
00:15:18,839 --> 00:15:20,459
실제 생성 프로세스에 조금

413
00:15:20,459 --> 00:15:22,620
더 적합하다는 것을 의미하며 우리가 알고 있듯이 뇌는

414
00:15:22,620 --> 00:15:24,779
높은 수준의 희소성 및 이는 효율성과

415
00:15:24,779 --> 00:15:26,339
매우 관련이 있다고 생각되므로

416
00:15:26,339 --> 00:15:28,620


417
00:15:28,620 --> 00:15:30,839


418
00:15:30,839 --> 00:15:32,760


419
00:15:32,760 --> 00:15:35,160
계층적 생성 모델을 사용하기 전에 이러한 유형의 그룹 희소성을 구현하기 위한 세부 사항에 대해 좀 더 자세히 알아보기 위해

420
00:15:35,160 --> 00:15:37,320
이는

421
00:15:37,320 --> 00:15:39,060
기본적으로 일부에서 소개되었습니다.

422
00:15:39,060 --> 00:15:41,339
지형 ICA 작업

423
00:15:41,339 --> 00:15:43,320
음 아이디어는 여러 하위 수준 변수 T의 분산을 동시에 조절하는 더 높은

424
00:15:43,320 --> 00:15:45,000
수준의 잠재 변수 U가 있다는 것입니다.

425
00:15:45,000 --> 00:15:47,820


426
00:15:47,820 --> 00:15:50,279


427
00:15:50,279 --> 00:15:52,440
이것이 그룹 희소성을 얻은

428
00:15:52,440 --> 00:15:55,440
다음 지형적 구성을 얻기 위해

429
00:15:55,440 --> 00:15:56,760
이러한 잠재

430
00:15:56,760 --> 00:15:59,339
변수를 약간 사용할 수 있는 방법입니다.

431
00:15:59,339 --> 00:16:02,519
영향을 미치는 분야와 겹치므로

432
00:16:02,519 --> 00:16:04,260
이웃이라고 부를 수

433
00:16:04,260 --> 00:16:05,699
있으며 이는 당신이 추구하는

434
00:16:05,699 --> 00:16:07,440
행동이 매끄러운 상관 구조를 제공하므로 이에

435
00:16:07,440 --> 00:16:09,899
대한 직관을 얻으십시오.

436
00:16:09,899 --> 00:16:12,060


437
00:16:12,060 --> 00:16:14,279
여기서 아래에 있는 이 변수 ​​T는 얻지 못한다는 것을 알 수 있습니다.

438
00:16:14,279 --> 00:16:17,160
상단에 있는 이 U의 모든 입력이지만

439
00:16:17,160 --> 00:16:19,139
중간에 있는 이 T와 u 변수를 공유하므로

440
00:16:19,139 --> 00:16:21,300
변종을 공유하는 것과 같습니다. 이웃

441
00:16:21,300 --> 00:16:22,980
과 일부 구성 요소를 공유

442
00:16:22,980 --> 00:16:24,959
하지만 모든 구성 요소는 공유하지 않습니다.

443
00:16:24,959 --> 00:16:26,579
이는 실제로 로컬

444
00:16:26,579 --> 00:16:28,079
연결 때문입니다.  생성 모델을

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
사용하는 방법을 간단하게 유지하기 위해 이러한 상위 수준 변수를

447
00:16:33,180 --> 00:16:34,980


448
00:16:34,980 --> 00:16:37,440
단일 U 변수로 돌아가서 수년 동안 어렵게

449
00:16:37,440 --> 00:16:38,940
만들었던 이러한 유형의 아키텍처에서 과제는 대략적인

450
00:16:38,940 --> 00:16:42,360


451
00:16:42,360 --> 00:16:44,579
사후 변수를 어떻게 추론하는가입니다.

452
00:16:44,579 --> 00:16:47,579


453
00:16:47,579 --> 00:16:50,100
이 계층 구조의 이러한 중간 변수에 대한 작업은

454
00:16:50,100 --> 00:16:52,560
매우 간단하지 않으므로 이전

455
00:16:52,560 --> 00:16:54,420
Works에서는 선형 모델용으로 개발된 경험적 방법을 사용했으며

456
00:16:54,420 --> 00:16:56,699
우리 작업에서는

457
00:16:56,699 --> 00:16:58,680
이것이 실제로 현대 신경망 아키텍처로 확장되지 않았으므로

458
00:16:58,680 --> 00:17:01,199


459
00:17:01,199 --> 00:17:02,880
통찰력을 활용하는 것입니다.

460
00:17:02,880 --> 00:17:04,760
인수분해는

461
00:17:04,760 --> 00:17:07,640
이 분포의 특정 재매개변수화

462
00:17:07,640 --> 00:17:10,380
이므로 이 매개변수화는 구체적으로

463
00:17:10,380 --> 00:17:12,419


464
00:17:12,419 --> 00:17:14,579
가우스

465
00:17:14,579 --> 00:17:16,319
스케일 혼합으로 알려진 우선순위를 정의함으로써 달성됩니다. 즉,

466
00:17:16,319 --> 00:17:19,140
U에 대한 T의 조건부 분포는

467
00:17:19,140 --> 00:17:21,179
실제로

468
00:17:21,179 --> 00:17:24,299
분산이 이 변수에 의해 정의되는 정규 분포라는 의미입니다.

469
00:17:24,299 --> 00:17:27,720
U 및 U의 특정 선택에 대해 이

470
00:17:27,720 --> 00:17:29,340
분포는 실제로 희소하며

471
00:17:29,340 --> 00:17:31,980


472
00:17:31,980 --> 00:17:33,780
라프로시안 슈트 및 T

473
00:17:33,780 --> 00:17:36,299
분포와 같은 다양한 분포를 포함합니다. 이를 정의하는 한 가지 방법은

474
00:17:36,299 --> 00:17:38,940
가우스 스케일 혼합이 독립적인 가우스 확률 변수 측면에서

475
00:17:38,940 --> 00:17:40,919
특정 리프레임 재매개변수화를 방출하는 것입니다.

476
00:17:40,919 --> 00:17:42,900


477
00:17:42,900 --> 00:17:45,720
Z와 U를 구체적으로 살펴보면

478
00:17:45,720 --> 00:17:48,840


479
00:17:48,840 --> 00:17:50,760
원래 상당히 복잡했던 이 T 변수는 실제로 생성 모델에서

480
00:17:50,760 --> 00:17:52,799


481
00:17:52,799 --> 00:17:54,840


482
00:17:54,840 --> 00:17:57,660
훨씬 더 효율적으로 작업하는 방법을 알고 있는 가우스 확률 변수 무리의 산물이라는 것을 알 수 있습니다.  우리가

483
00:17:57,660 --> 00:18:00,120


484
00:18:00,120 --> 00:18:02,039
할 일은

485
00:18:02,039 --> 00:18:04,020
실제로 U와 Z에 대한 대략적인 사후값을

486
00:18:04,020 --> 00:18:06,720
개별적으로 얻은 다음 지형 변수 T를 계산하기 위해

487
00:18:06,720 --> 00:18:08,640
이들의 결정론적 조합을 수행하는 것입니다.

488
00:18:08,640 --> 00:18:10,020


489
00:18:10,020 --> 00:18:13,140
이는

490
00:18:13,140 --> 00:18:15,500
너무 많은 세부 사항을 다루지 않고도 그렇게 하는 것이 훨씬 쉽습니다.

491
00:18:15,500 --> 00:18:17,700
우리는

492
00:18:17,700 --> 00:18:18,600
변형 추론의

493
00:18:18,600 --> 00:18:20,640
기술을 활용하여 가능성에 대한 하한을 도출하는 변형 오토인코더를 사용하기로 결정했습니다. 이를 통해

494
00:18:20,640 --> 00:18:23,220


495
00:18:23,220 --> 00:18:24,780


496
00:18:24,780 --> 00:18:26,940


497
00:18:26,940 --> 00:18:29,400
강력한 비선형 심층

498
00:18:29,400 --> 00:18:30,960
신경망을 사용하여 이러한 근사 사후를 매개변수화하고 경사하강법으로 최적화할 수 있습니다.

499
00:18:30,960 --> 00:18:33,240


500
00:18:33,240 --> 00:18:34,440
활성 추론 커뮤니티에 익숙

501
00:18:34,440 --> 00:18:36,900
하지만 실제로 우리가 한 일은

502
00:18:36,900 --> 00:18:38,760


503
00:18:38,760 --> 00:18:41,640
일반적인 기본으로 디코더에 단일 인코더를 두는 대신 이제

504
00:18:41,640 --> 00:18:43,799
두 개의 인코더를 사용자용과 Z용으로

505
00:18:43,799 --> 00:18:46,200
별도로 만든 다음 결정론적인 방식으로 결합하는 것입니다.

506
00:18:46,200 --> 00:18:48,419


507
00:18:48,419 --> 00:18:51,660


508
00:18:51,660 --> 00:18:53,100
이것이 실제로 가우스로부터

509
00:18:53,100 --> 00:18:54,900
학생의 T 분포를 구성한 것이라는 것을 알면 지형학적 T 변수를 구성

510
00:18:54,900 --> 00:18:57,620


511
00:18:57,620 --> 00:19:00,480
하고 이를 연결할 수 있습니다.

512
00:19:00,480 --> 00:19:03,600
디코딩하기 전에 이 작업을 수행하고 어 그런 다음

513
00:19:03,600 --> 00:19:05,820
데이터의 가능성을 모두 최대화하므로

514
00:19:05,820 --> 00:19:07,260
이것이 팔꿈치입니다.

515
00:19:07,260 --> 00:19:09,360
데이터의 가능성에 대한 증거 하한은 풍부하며

516
00:19:09,360 --> 00:19:12,600
실제로는 활성 입구 커뮤니티

517
00:19:12,600 --> 00:19:15,720
에서 사용되는 변형 자유 에너지와 매우 유사합니다.

518
00:19:15,720 --> 00:19:18,299


519
00:19:18,299 --> 00:19:20,520
따라서 이러한 세부 사항을 제외하면

520
00:19:20,520 --> 00:19:22,500
정말 흥미로운 것은

521
00:19:22,500 --> 00:19:23,940
우리가 이 생성 모델을 훈련할 때 어떤 일이 발생하는지입니다.

522
00:19:23,940 --> 00:19:26,580


523
00:19:26,580 --> 00:19:29,460
잠재 공간에 상대적으로 간단한 그룹 희소성 페널티가 있으며

524
00:19:29,460 --> 00:19:30,720
우리는 Futures

525
00:19:30,720 --> 00:19:32,700
의 구성 측면에서 무엇을 학습하는지 살펴보고 싶습니다.

526
00:19:32,700 --> 00:19:34,980
먼저

527
00:19:34,980 --> 00:19:36,480
가장 간단한 가능한 데이터 세트로 시작합니다. 검정색

528
00:19:36,480 --> 00:19:38,580
배경에

529
00:19:38,580 --> 00:19:41,280
임의의 XY 위치에 흰색 사각형이 있습니다.  그리고 우리가

530
00:19:41,280 --> 00:19:42,780
이 그룹 희소성 페널티를 사용하여 오토인코더를 훈련한

531
00:19:42,780 --> 00:19:44,760
다음 여기 파란색으로 표시한

532
00:19:44,760 --> 00:19:47,640
디코더의 가중치 벡터를 보면

533
00:19:47,640 --> 00:19:49,020


534
00:19:49,020 --> 00:19:52,520
이 2D 그리드에 다시 정리되어 있으며

535
00:19:52,520 --> 00:19:54,780
실제로 공간에 따라 구성되는 방법을 학습한다는 것을 알 수 있습니다.

536
00:19:54,780 --> 00:19:57,539
위치는

537
00:19:57,539 --> 00:19:59,580
컨벌루션 수용 필드와 유사한 것으로 볼 수 있거나

538
00:19:59,580 --> 00:20:01,799


539
00:20:01,799 --> 00:20:04,679
각 뉴런의 수용 필드는 실제로

540
00:20:04,679 --> 00:20:09,059
해당 위치의 입력 종류에 의해 제공되며

541
00:20:09,059 --> 00:20:10,860


542
00:20:10,860 --> 00:20:13,140


543
00:20:13,140 --> 00:20:15,480
특정 영역이 다음과 같이 강조 표시되기 때문에 이는 그룹의 희소성 관점에서 직관적으로 의미가 있습니다.

544
00:20:15,480 --> 00:20:17,700
여기서 노란색은 주어진 그룹의 필터가

545
00:20:17,700 --> 00:20:19,260
훨씬 더 높은 상관 관계를 갖고 있으며

546
00:20:19,260 --> 00:20:20,880


547
00:20:20,880 --> 00:20:23,460
다른 무작위 위치보다 이러한 겹치는 수용 필드를 가지고 있으므로

548
00:20:23,460 --> 00:20:25,020
기본적으로 모델이

549
00:20:25,020 --> 00:20:27,840
활동 활동을 함께 클러스터링하는 방법을 학습하고 있음을 알 수 있습니다.

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
어, 다음의 상관 관계에 따라 일종의 시뮬레이션된 수직 시트입니다.

552
00:20:32,059 --> 00:20:34,260


553
00:20:34,260 --> 00:20:36,840
데이터 세트를 사용하면

554
00:20:36,840 --> 00:20:38,580
실제로 가중치 묶기를 수행

555
00:20:38,580 --> 00:20:40,860
하고 수동으로 지정하는 컨볼루션 대신에

556
00:20:40,860 --> 00:20:42,539
이 가중치를 모든 곳에 복사하려고 합니다.

557
00:20:42,539 --> 00:20:44,220
이를 대략적인 대기 시간처럼 생각할 수

558
00:20:44,220 --> 00:20:45,500


559
00:20:45,500 --> 00:20:48,120
있으며 실제로 우리는 상관 관계에서 이를 학습하고 있습니다.

560
00:20:48,120 --> 00:20:49,620
데이터 세트 자체의 구조

561
00:20:49,620 --> 00:20:51,660
와 이에 대한

562
00:20:51,660 --> 00:20:54,120
생물학적 영감을 조금 더 제공하기 위해

563
00:20:54,120 --> 00:20:56,280
우리는 망막증이

564
00:20:56,280 --> 00:20:58,020
뇌에 존재한다는 것을 알고 있습니다. 이것은

565
00:20:58,020 --> 00:21:02,460
시각 피질에 있는 망막증의 예입니다.

566
00:21:02,460 --> 00:21:04,500
원숭이를 보여주면 알 수 있습니다.

567
00:21:04,500 --> 00:21:06,780
이와 같은 이미지는

568
00:21:06,780 --> 00:21:08,520


569
00:21:08,520 --> 00:21:11,700
실제로 피질 표면의 공간을 보존하는 이 토폴로지로 투영됩니다.

570
00:21:11,700 --> 00:21:13,740


571
00:21:13,740 --> 00:21:16,080
따라서 지형

572
00:21:16,080 --> 00:21:18,419
구성과 학습 지형

573
00:21:18,419 --> 00:21:21,299
구성이

574
00:21:21,299 --> 00:21:26,160
데이터 세트의 입력 상관 관계를 보존한다는 아이디어가 있습니다. 어 그리고

575
00:21:26,160 --> 00:21:28,679
잠재적으로 어 이것이 유익할 수 있습니다

576
00:21:28,679 --> 00:21:30,840
이러한 아이디어를 조금 더 일반화하면

577
00:21:30,840 --> 00:21:32,340
제가 처음에 말했듯

578
00:21:32,340 --> 00:21:34,679
이 어

579
00:21:34,679 --> 00:21:37,200
우리가

580
00:21:37,200 --> 00:21:39,320
단순한 컨볼루션보다 더 복잡한 등분산을 배울 수 있다면 더 좋을 것입니다.

581
00:21:39,320 --> 00:21:43,679


582
00:21:43,679 --> 00:21:45,720
자연 지능에서 분명한 한 가지는

583
00:21:45,720 --> 00:21:48,299
우리가 어떻게 할 수 있다는 것입니다.

584
00:21:48,299 --> 00:21:51,120
이 IID 프레임 세계에는 존재하지 않습니다. 바로 우리는

585
00:21:51,120 --> 00:21:53,520
연속적인 변환 시퀀스가 ​​있는 세계에 존재하므로

586
00:21:53,520 --> 00:21:55,620


587
00:21:55,620 --> 00:21:58,440
모델을 이 설정으로 확장하여

588
00:21:58,440 --> 00:22:01,080
변환 관찰을 배울 수 있습니다. 이것은

589
00:22:01,080 --> 00:22:03,299
시간적 일관성에 대한 아이디어이므로

590
00:22:03,299 --> 00:22:05,280
간단히 말하면 어떻게 될까요?

591
00:22:05,280 --> 00:22:08,280
이전 프레임워크를 시간 차원에 걸쳐 확장했습니다.

592
00:22:08,280 --> 00:22:10,620
따라서 단순히

593
00:22:10,620 --> 00:22:13,080
그룹화하는 대신 피질의 공간적 범위 측면에서 뉴런이 그룹 희박해지기를 원한다고 말하는 대신

594
00:22:13,080 --> 00:22:15,059


595
00:22:15,059 --> 00:22:17,400
실제로

596
00:22:17,400 --> 00:22:18,960
시간이 지남에 따라 그룹 희박해지기를 원합니다. 즉,

597
00:22:18,960 --> 00:22:20,640
한 세트의 뉴런이

598
00:22:20,640 --> 00:22:22,559
활성화되면 의미합니다.  이제 우리는 동일한

599
00:22:22,559 --> 00:22:24,360
뉴런 세트가 미래에도 활성화되기를 원합니다.

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
만약 우리가

602
00:22:27,840 --> 00:22:30,600
이것에 대해 직관적으로 생각한다면 이것이

603
00:22:30,600 --> 00:22:33,059
실제로 더 고무적인 불변성과

604
00:22:33,059 --> 00:22:35,039
등분성이라는 것을 알 수 있습니다. 이것을 이해하는 방법은

605
00:22:35,039 --> 00:22:37,140
우리가 말하는 것입니다.  동일한 뉴런이

606
00:22:37,140 --> 00:22:39,179
지속적으로 활성화되기를 원하지만 입력

607
00:22:39,179 --> 00:22:41,280
변환이 바로 변경됩니다.

608
00:22:41,280 --> 00:22:44,220
이 작은 여우의 발이 움직이고 있으므로

609
00:22:44,220 --> 00:22:45,960
동일한 뉴런이

610
00:22:45,960 --> 00:22:47,880
계속해서 같은 것을 코딩하지만 발이

611
00:22:47,880 --> 00:22:49,320
움직이면 해당 뉴런은

612
00:22:49,320 --> 00:22:51,360
학습하게 될 것입니다.

613
00:22:51,360 --> 00:22:53,880
예를 들어 이 개 다리의 움직임에 불변한다는 것은

614
00:22:53,880 --> 00:22:57,539
대신에 죄송합니다

615
00:22:57,539 --> 00:23:01,200
제가 여기서 잘못된 길로 갔다는 것입니다 어

616
00:23:01,200 --> 00:23:04,860
그래서 대신 어 우리의 통찰은 이

617
00:23:04,860 --> 00:23:06,659
그룹이 대신

618
00:23:06,659 --> 00:23:09,059
시간에 따라 이동될 수 있다는 것이었습니다.

619
00:23:09,059 --> 00:23:10,980
순차적으로 이동된

620
00:23:10,980 --> 00:23:13,080
활성화 세트는

621
00:23:13,080 --> 00:23:15,179
함께 활성화되도록 권장되며 그러면 우리의 잠재

622
00:23:15,179 --> 00:23:16,440
공간은 실제로

623
00:23:16,440 --> 00:23:18,000
관찰된 변환과 관련하여 구조화될 것입니다.

624
00:23:18,000 --> 00:23:19,980
따라서 여기서는

625
00:23:19,980 --> 00:23:21,480
동일한 뉴런 세트가 모든 시간 단계에서 활성화되는 것이 아니라

626
00:23:21,480 --> 00:23:23,340
실제로 순차적이라는 것을 알 수 있습니다.

627
00:23:23,340 --> 00:23:24,900


628
00:23:24,900 --> 00:23:27,780
이렇게 희박한 방식으로 함께 그룹화한 순열된 뉴런 세트 어

629
00:23:27,780 --> 00:23:29,940
그리고 이를 통해

630
00:23:29,940 --> 00:23:33,419
시간이 지남에 따라 서로 다른 관찰을 모델링할 수 있지만

631
00:23:33,419 --> 00:23:34,860


632
00:23:34,860 --> 00:23:36,960
변환을 학습하고 공감

633
00:23:36,960 --> 00:23:38,340
의 상관 구조를 보존한다는 측면에서 여전히 연결되어 있습니다.

634
00:23:38,340 --> 00:23:40,020


635
00:23:40,020 --> 00:23:41,940
이것을

636
00:23:41,940 --> 00:23:44,400
지형학적 Bae 아키텍처에 합치면

637
00:23:44,400 --> 00:23:46,020
다음과 같은 결과를 얻을 수 있습니다. 입력 시퀀스가 ​​있다는 것을 알 수 있습니다.

638
00:23:46,020 --> 00:23:48,120


639
00:23:48,120 --> 00:23:51,240
다시 z 변수를 인코딩한 다음

640
00:23:51,240 --> 00:23:53,520
분모에 여러 U 변수를 인코딩

641
00:23:53,520 --> 00:23:55,740
하고 U

642
00:23:55,740 --> 00:23:58,620
변수 각각이 이동됩니다.  어 일종의 이 스튜던트 T 곱 분포에서 이들을 결합할 때 우리가 찾고 있는 이 이동 등분산 구조를

643
00:23:58,620 --> 00:24:00,480
달성하기 위해 이전에 보여준 것과 같습니다.

644
00:24:00,480 --> 00:24:02,820


645
00:24:02,820 --> 00:24:04,740


646
00:24:04,740 --> 00:24:07,080


647
00:24:07,080 --> 00:24:09,240
우리는 단일 잠재

648
00:24:09,240 --> 00:24:10,740
변수를 얻습니다. 이것은 이제 지형

649
00:24:10,740 --> 00:24:13,860
변수 T이고 이제 우리는

650
00:24:13,860 --> 00:24:16,140
잠재 공간에 이 알려진 구조가 있습니다.

651
00:24:16,140 --> 00:24:17,460
구조화된 세계 모델처럼 생각할 수 있습니다.

652
00:24:17,460 --> 00:24:19,919
우리는 이 잠재 공간을 변환하는 방법을 알고 있습니다.

653
00:24:19,919 --> 00:24:21,659
이 경우

654
00:24:21,659 --> 00:24:23,580


655
00:24:23,580 --> 00:24:25,860
순환 역할처럼 순환 이동을 수행하는 이러한 원 주위의 활성화를 순열하는 것입니다.

656
00:24:25,860 --> 00:24:28,380


657
00:24:28,380 --> 00:24:30,120
학습된 입력 변환에 대응할 것이며

658
00:24:30,120 --> 00:24:32,640


659
00:24:32,640 --> 00:24:34,620
이

660
00:24:34,620 --> 00:24:36,480
입력 변환을 계속하면 회전인

661
00:24:36,480 --> 00:24:38,100
데이터 세트의 실제 변환이 되는 것을 확인

662
00:24:38,100 --> 00:24:40,559
하고 이를

663
00:24:40,559 --> 00:24:42,659
제가 늦은 시간에 수행한 역할과 비교하여 확인할 수 있습니다.

664
00:24:42,659 --> 00:24:44,700
내 뇌에서 활성화를 움직여서 공간을 확보한

665
00:24:44,700 --> 00:24:47,280
다음 디코딩하면

666
00:24:47,280 --> 00:24:49,919
정확히 동일한 결과를 얻는다는 것을 알 수 있습니다. 따라서 이것은

667
00:24:49,919 --> 00:24:52,140


668
00:24:52,140 --> 00:24:53,580
제가 이전에 동형성을

669
00:24:53,580 --> 00:24:56,820
확인하기 위해 이야기했던 교환성 속성을 보여주고

670
00:24:56,820 --> 00:24:58,799
이를 좀 더 품질을 측정하기 위한 것입니다.

671
00:24:58,799 --> 00:25:02,460
정량적으로 어 우리는

672
00:25:02,460 --> 00:25:04,440
등분산 손실이라는 것을 측정할 수 있습니다. 따라서

673
00:25:04,440 --> 00:25:07,080
이것은 실제로

674
00:25:07,080 --> 00:25:09,360
롤링된

675
00:25:09,360 --> 00:25:12,120
캡슐 활성화 또는 머리에서 롤링하는 것과 롤링이 펼쳐지고

676
00:25:12,120 --> 00:25:15,059


677
00:25:15,059 --> 00:25:16,559
앞으로 나아가는 것을 보는 것 사이의 차이를 정량화한 것입니다. 그들은

678
00:25:16,559 --> 00:25:19,440
우리 앞에 변형이 펼쳐지는 것을 보고 있으므로

679
00:25:19,440 --> 00:25:21,600
지형을 볼 수 있습니다.  Bae는

680
00:25:21,600 --> 00:25:24,000
훨씬 더 낮은 등분산

681
00:25:24,000 --> 00:25:26,700
오류를 달성합니다. 이 버블 vae는 제가

682
00:25:26,700 --> 00:25:27,960
이전에 불변성을 학습하는 위치에 대해 이야기한 것이므로

683
00:25:27,960 --> 00:25:29,820
교대

684
00:25:29,820 --> 00:25:32,340
연산이 없으며 전통적인 vae

685
00:25:32,340 --> 00:25:35,640
종류에는 조직 또는

686
00:25:35,640 --> 00:25:37,380
시간적 구성 요소에 대한 개념이 없으므로 성능이 매우

687
00:25:37,380 --> 00:25:40,320
낮습니다.  이를 통해 우리는

688
00:25:40,320 --> 00:25:41,700
모델이

689
00:25:41,700 --> 00:25:45,059
시퀀스의 더 나은 생성 모델이라는 것을 알 수 있습니다. 데이터 세트

690
00:25:45,059 --> 00:25:48,179
에서 음의 로그 우도처럼 더 낮아집니다.

691
00:25:48,179 --> 00:25:50,100
따라서

692
00:25:50,100 --> 00:25:51,720
이 데이터 세트는 다음의 구조에 대한 개념을 갖고 있기 때문에 더 잘 모델링할 수 있습니다.

693
00:25:51,720 --> 00:25:52,919


694
00:25:52,919 --> 00:25:55,460
변환

695
00:25:55,980 --> 00:25:58,140
어 우리는 이것을 다양한 변환 유형에서 테스트할 수 있습니다.

696
00:25:58,140 --> 00:25:59,760


697
00:25:59,760 --> 00:26:00,840
위쪽 행에는 실제 변환이 표시됩니다.

698
00:26:00,840 --> 00:26:02,880


699
00:26:02,880 --> 00:26:05,039
회색으로 표시된 이미지를 꺼낸 다음 아래쪽

700
00:26:05,039 --> 00:26:07,080
행에서 인코딩하고

701
00:26:07,080 --> 00:26:08,700
활성화를 굴려서 유지합니다.

702
00:26:08,700 --> 00:26:12,140
모델이

703
00:26:12,140 --> 00:26:15,000
관찰 중인 현재 변환으로 학습한 내용을 확인하기 위해 디코딩하고

704
00:26:15,000 --> 00:26:17,039


705
00:26:17,039 --> 00:26:19,340


706
00:26:19,340 --> 00:26:21,360


707
00:26:21,360 --> 00:26:23,640
이전에 본 적이 없는 시퀀스의 이러한 요소를

708
00:26:23,640 --> 00:26:25,260
추가로

709
00:26:25,260 --> 00:26:26,580
본 적이 없는 테스트 세트의 이미지를 사용하여 기본적으로 완벽하게 재구성할 수 있음을 확인합니다.  이전에는

710
00:26:26,580 --> 00:26:28,380


711
00:26:28,380 --> 00:26:29,760
변환이 무엇인지 알고 있기 때문에 현재

712
00:26:29,760 --> 00:26:31,500
인코딩 중이므로 이를 새로운 예로 일반화할 수 있으므로

713
00:26:31,500 --> 00:26:33,919


714
00:26:34,020 --> 00:26:36,360
이 부분에서 얻은 교훈은 실제로

715
00:26:36,360 --> 00:26:38,039
지형학적 조직입니다. 우리는

716
00:26:38,039 --> 00:26:40,080
보존된 입력 구조를 보여 주었고 이제는

717
00:26:40,080 --> 00:26:41,940
잠재적으로 효율성과 일반화를 향상시킬 수 있음을 보여주고 있습니다.

718
00:26:41,940 --> 00:26:44,279
우리가

719
00:26:44,279 --> 00:26:46,200
바라던 대로 어,

720
00:26:46,200 --> 00:26:48,600
마침내 우리를 놀라게 했고

721
00:26:48,600 --> 00:26:49,980
잠재적으로 가장 흥미로웠던 점은

722
00:26:49,980 --> 00:26:52,500


723
00:26:52,500 --> 00:26:53,700
우리 모델에 의해 학습된 이러한 변환이 훈련 중에 볼 수 없는 변환의 조합을

724
00:26:53,700 --> 00:26:54,960
실제로 일반화한다는 것입니다.

725
00:26:54,960 --> 00:26:57,059


726
00:26:57,059 --> 00:26:59,580


727
00:26:59,580 --> 00:27:02,100
예를 들어 훈련만 했음에도 불구하고 말이죠.  색상

728
00:27:02,100 --> 00:27:04,200
및 회전 변환 및

729
00:27:04,200 --> 00:27:06,419
격리 테스트 시 모델에

730
00:27:06,419 --> 00:27:08,340
결합된 색상 회전 변환이 제공되는 경우

731
00:27:08,340 --> 00:27:11,100
어 우리는 캡슐 역할을 통해 이러한 변환을 완벽하게 모델링하고 완료할 수 있다는 것을 알 수 있습니다.

732
00:27:11,100 --> 00:27:13,140


733
00:27:13,140 --> 00:27:14,700


734
00:27:14,700 --> 00:27:17,159


735
00:27:17,159 --> 00:27:19,620


736
00:27:19,620 --> 00:27:20,880
변환은

737
00:27:20,880 --> 00:27:24,600
추론 시 유연하게 결합할 수 있으므로

738
00:27:24,600 --> 00:27:28,140


739
00:27:28,140 --> 00:27:29,820
일반화에서 공식적으로 효율성을 얻을 수 있을 뿐만 아니라

740
00:27:29,820 --> 00:27:34,100
기본적인 구성성도 얻을 수 있으므로

741
00:27:34,260 --> 00:27:36,059
제한 사항과

742
00:27:36,059 --> 00:27:38,460
다음에 수행할 수 있는 작업에 대해 이야기하겠습니다. 주요

743
00:27:38,460 --> 00:27:40,620
제한 사항은 다음과 같습니다.

744
00:27:40,620 --> 00:27:44,159
우리가

745
00:27:44,159 --> 00:27:46,500
공간과 시간 모두에서 부과하는 미리 정의된 변환입니다. 그래서 우리는

746
00:27:46,500 --> 00:27:49,080
그룹 변환에서 자유로워졌지만

747
00:27:49,080 --> 00:27:52,440
특히 기계 학습 세계

748
00:27:52,440 --> 00:27:53,940
에서 현재 수행되는 번역이나 회전과 같은

749
00:27:53,940 --> 00:27:55,559


750
00:27:55,559 --> 00:27:59,240
음 우리는 여전히 uh 우리의 작업에서 하드 코딩된

751
00:27:59,240 --> 00:28:01,980
잠재 역할을 가지고 있습니다.  우리가 보는 모든 것을 향하고

752
00:28:01,980 --> 00:28:03,900
이것을 좀

753
00:28:03,900 --> 00:28:05,700
더 유연하게 만들어 더 다양한 변형을 모델링할 수 있기를 바랍니다.

754
00:28:05,700 --> 00:28:08,880


755
00:28:08,880 --> 00:28:10,980
음 우리는 아마도 뇌에서 관찰되는

756
00:28:10,980 --> 00:28:13,860
보다 구조화된 공간적 시간 역학에서 영감을 얻을 수 있을 것이라고 생각합니다.

757
00:28:13,860 --> 00:28:15,600


758
00:28:15,600 --> 00:28:18,120


759
00:28:18,120 --> 00:28:20,400
이 강연의 두 번째 부분인 어

760
00:28:20,400 --> 00:28:22,140
우리가 인공 신경망에 통합하려고 하는 공간적 시간적 역학의

761
00:28:22,140 --> 00:28:23,039


762
00:28:23,039 --> 00:28:25,200
한 예는

763
00:28:25,200 --> 00:28:27,059
제가 여기서 보여드린 진행파입니다.

764
00:28:27,059 --> 00:28:28,020


765
00:28:28,020 --> 00:28:30,600
그럼 그게 무슨 뜻인가요? 어 음

766
00:28:30,600 --> 00:28:32,279
여기서는 매우  최근 논문에서는

767
00:28:32,279 --> 00:28:36,059
36밀리초 해상도로 작동하는 9개의 Tesla fmri를 사용하여

768
00:28:36,059 --> 00:28:38,700


769
00:28:38,700 --> 00:28:40,980
마취 상태에서 쥐 뇌의 단일 조각을 이미지화했으며

770
00:28:40,980 --> 00:28:43,320
우리가 보는 것은 매우 명확하게

771
00:28:43,320 --> 00:28:45,720
구조화된 공간적 시간적 활동과

772
00:28:45,720 --> 00:28:48,299
상관 관계이며 이 논문의 저자는

773
00:28:48,299 --> 00:28:50,520
계속해서 이를 분석합니다.  오른쪽에

774
00:28:50,520 --> 00:28:52,919
묘사된 바와 같이 주요 모드 측면에서 활동하므로

775
00:28:52,919 --> 00:28:55,799
우리의 가설은

776
00:28:55,799 --> 00:28:57,179
아마도

777
00:28:57,179 --> 00:28:59,039
이와 같은 일종의 상관 구조가 관찰된 변환과 관련하여 모델의 표현을 구조화하는 데 도움이 될 수

778
00:28:59,039 --> 00:29:01,260


779
00:29:01,260 --> 00:29:03,240


780
00:29:03,240 --> 00:29:05,100
있지만

781
00:29:05,100 --> 00:29:07,440
단순한 것보다 훨씬 더 유연한 방식일 수 있다는 것입니다.

782
00:29:07,440 --> 00:29:10,700
우리가 이전에 했던 것과 같은 순환적 변화입니다. 음

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
그리고 이것은

785
00:29:15,900 --> 00:29:19,320
단지 ssi의 쥐에게서만 관찰되는 것이 아니라는 점을 말씀드리고 싶습니다. 어 이건

786
00:29:19,320 --> 00:29:20,940
당신은 이러한 진행파가

787
00:29:20,940 --> 00:29:24,179
깨어 행동하는 영장류의 Mt 피질에서 일어나는 것을 볼 수 있습니다

788
00:29:24,179 --> 00:29:27,600
어 그래서 예를 들어

789
00:29:27,600 --> 00:29:29,580
왼쪽에 있습니다  여기 그들은

790
00:29:29,580 --> 00:29:31,740
실제로 어

791
00:29:31,740 --> 00:29:35,520
변화하는 진행파를 보여줍니다 영장류가 파동의

792
00:29:35,520 --> 00:29:38,279
위상을 기반으로 낮은 대비 자극을 볼 가능성이 얼마나 되는지

793
00:29:38,279 --> 00:29:40,980
또한 그들은 오른쪽

794
00:29:40,980 --> 00:29:43,500
에 있는 높은 대비 자극과 같은 것이

795
00:29:43,500 --> 00:29:45,779
어 바깥쪽으로 전파되는 진행파 활동을 유도할 수 있음을 보여줍니다

796
00:29:45,779 --> 00:29:47,520


797
00:29:47,520 --> 00:29:50,039
일차 시각 피질에서도 이러한 현상은

798
00:29:50,039 --> 00:29:52,140


799
00:29:52,140 --> 00:29:54,000
여러 수준에서 뇌 전반에 걸쳐 실제로 어디에나 존재하며, 우리의 경우

800
00:29:54,000 --> 00:29:55,440


801
00:29:55,440 --> 00:29:58,140
구조 표현 학습에 대한 의미가 무엇인지 연구하는 것은 흥미로울 것입니다.

802
00:29:58,140 --> 00:29:59,700


803
00:29:59,700 --> 00:30:01,799
또는 일반적으로

804
00:30:01,799 --> 00:30:04,140


805
00:30:04,140 --> 00:30:06,720
이러한 유형의 역학을 연구한 이전 작업이 있습니다.  그리고 그들은

806
00:30:06,720 --> 00:30:08,700
모델을 구축하므로 맨 위에 이것들은

807
00:30:08,700 --> 00:30:10,380
스파이크 신경망을 설명하는 방정식입니다.

808
00:30:10,380 --> 00:30:12,600


809
00:30:12,600 --> 00:30:15,720
시간 지연을 구현하면 실제로

810
00:30:15,720 --> 00:30:18,240
뉴런 사이의 축삭 시간 지연을 구현하면 네트워크 크기가 충분히 큰 한

811
00:30:18,240 --> 00:30:20,820
진행파의 구조 역학을 얻을 수 있습니다.

812
00:30:20,820 --> 00:30:22,440


813
00:30:22,440 --> 00:30:24,059


814
00:30:24,059 --> 00:30:26,520
음 그러나 많은 사람들이 알고 있듯이

815
00:30:26,520 --> 00:30:28,620


816
00:30:28,620 --> 00:30:31,320


817
00:30:31,320 --> 00:30:34,820
심층 신경망과 동일한 크기와 성능의 스파이킹 신경망을 훈련하는 것이 상대적으로 어렵다는 것을 유사하게

818
00:30:34,820 --> 00:30:37,679
바닥에 있는 또 다른 시스템은

819
00:30:37,679 --> 00:30:39,539
훨씬 더 간단하지만

820
00:30:39,539 --> 00:30:42,840
어쩌면 너무 단순할 수도 있습니다. 어는

821
00:30:42,840 --> 00:30:45,120
이것이 알려진 결합 발진기의 네트워크입니다.

822
00:30:45,120 --> 00:30:48,779
동시성 및 공간적 시간적 역동

823
00:30:48,779 --> 00:30:52,200
성과 복잡한 패턴을 보여주지만 어

824
00:30:52,200 --> 00:30:53,520
이것은 위상 감소 시스템이라고 불리며

825
00:30:53,520 --> 00:30:55,500


826
00:30:55,500 --> 00:30:57,059
우리가 관심 있는 전체 복잡성을 제대로 포착하지 못하므로

827
00:30:57,059 --> 00:30:58,140


828
00:30:58,140 --> 00:31:00,779
잠재적으로 이 둘과 우리가 원하는 것 사이에 있는 것을 살펴보고 있습니다.

829
00:31:00,779 --> 00:31:03,600
이 작업에서 이 작업은

830
00:31:03,600 --> 00:31:06,600


831
00:31:06,600 --> 00:31:08,520


832
00:31:08,520 --> 00:31:10,620
파라모토 모델보다 약간 더 유연하게 몇 개의 발진기 네트워크를 매개변수화하는 것으로 결정되었습니다.

833
00:31:10,620 --> 00:31:12,360
따라서 이것은 실제로 콘스탄틴 러시와 니샤 음의

834
00:31:12,360 --> 00:31:14,580
두 증류 순환 신경망을 기반으로 구축되었으며

835
00:31:14,580 --> 00:31:16,380


836
00:31:16,380 --> 00:31:18,720


837
00:31:18,720 --> 00:31:20,760
음 그들은 기본적으로 방정식을 취했습니다.

838
00:31:20,760 --> 00:31:22,200
간단한

839
00:31:22,200 --> 00:31:23,820
조화 진동자를 설명합니다. 이것은 2차 미분

840
00:31:23,820 --> 00:31:26,159
방정식입니다.

841
00:31:26,159 --> 00:31:29,940
스프링 위의 공의 가속도는 변위에 비례합니다.

842
00:31:29,940 --> 00:31:32,480


843
00:31:32,480 --> 00:31:35,220
어 진동이 시간이 지남에 따라 천천히 사라지도록 댐핑과 같은 추가 항을 추가할 수 있습니다.

844
00:31:35,220 --> 00:31:37,260


845
00:31:37,260 --> 00:31:39,360


846
00:31:39,360 --> 00:31:41,580
이 진동자를 외부 장치로 구동할 수 있습니다

847
00:31:41,580 --> 00:31:43,380


848
00:31:43,380 --> 00:31:45,179
이러한 감쇠에 대응하거나 동역학에 약간 더 복잡도를 부여하기 위한 입력을 제공

849
00:31:45,179 --> 00:31:47,279


850
00:31:47,279 --> 00:31:49,260
하고 더 나아가

851
00:31:49,260 --> 00:31:50,940
이러한 오실레이터가 많으면

852
00:31:50,940 --> 00:31:53,000
이러한 결합 행렬과 함께 결합할 수 있습니다. W

853
00:31:53,000 --> 00:31:55,320
uh 여기 이 그림에서 보여주듯이

854
00:31:55,320 --> 00:31:56,640
실제로 할 수 있는 것은

855
00:31:56,640 --> 00:31:58,140
이 네트워크를 Springs의 거품 다발로 생각

856
00:31:58,140 --> 00:31:59,940
하고

857
00:31:59,940 --> 00:32:01,740
아마도 Springs 또는 탄성 밴드에 의해 서로 연결되어 있을 수 있습니다.

858
00:32:01,740 --> 00:32:03,600


859
00:32:03,600 --> 00:32:05,279
러시아 Mishra의 몇 가지 증류 순환 신경망

860
00:32:05,279 --> 00:32:08,100
어 이러한 다양한 용어를 사용하면

861
00:32:08,100 --> 00:32:09,899
이는 매우 강력한 것으로 나타났습니다.

862
00:32:09,899 --> 00:32:12,480
긴 시퀀스를 모델링하기 위해 그들은 또한

863
00:32:12,480 --> 00:32:13,740


864
00:32:13,740 --> 00:32:15,360
이것을 구축하는 두뇌에서 영감을 받았다고 언급했으며

865
00:32:15,360 --> 00:32:17,700
그 논문에는 많은 좋은 분석이 있습니다.

866
00:32:17,700 --> 00:32:19,020
예를 들어 그들은 이것이

867
00:32:19,020 --> 00:32:21,440


868
00:32:21,440 --> 00:32:23,460


869
00:32:23,460 --> 00:32:25,440
순환 신경망에서 일반적으로 발생하는 Vanishing Gradient 문제와 관련하여 정말 유익한 속성임을 보여줍니다.

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
음  하지만 공간적

872
00:32:29,159 --> 00:32:30,960
시간 역학과 이러한 유형의

873
00:32:30,960 --> 00:32:32,820
모델을 살펴보고 싶다면 어 약간 어렵습니다.

874
00:32:32,820 --> 00:32:34,919
여기서 이러한 결합 행렬은

875
00:32:34,919 --> 00:32:36,320


876
00:32:36,320 --> 00:32:39,600
각 신경 또는 각

877
00:32:39,600 --> 00:32:41,240
발진기를 연결하는 W가 서로 배치되어 있기 때문입니다.

878
00:32:41,240 --> 00:32:43,620
이것은

879
00:32:43,620 --> 00:32:45,120
제가 시도한 것처럼 조밀하게 연결된 행렬입니다.  여기 왼쪽에 묘사하면

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299


882
00:32:48,299 --> 00:32:50,580
이 네트워크의 역학을 시각화하려고 하면 어떤 공간적 조직도 볼 수 없습니다.

883
00:32:50,580 --> 00:32:51,899


884
00:32:51,899 --> 00:32:55,380
상속이 없습니다. 이 모델의 잠재 공간에 대한 사과입니다.

885
00:32:55,380 --> 00:32:57,000


886
00:32:57,000 --> 00:32:58,799
음, 이전 예에서 뉴런과 같이 생각할 수 있습니다.  잠재적으로

887
00:32:58,799 --> 00:33:01,020


888
00:33:01,020 --> 00:33:03,240
임의적인 다른 뉴런 세트에 연결되어 있습니다.

889
00:33:03,240 --> 00:33:04,919
이 뉴런은

890
00:33:04,919 --> 00:33:06,600
다른 임의의 뉴런 세트에 연결되어 있으며

891
00:33:06,600 --> 00:33:08,520


892
00:33:08,520 --> 00:33:10,860
확실히 진동 역학을 얻을 수 있지만

893
00:33:10,860 --> 00:33:13,260
구조화된 의미가 많지 않은 일종의 변동이 있으므로

894
00:33:13,260 --> 00:33:15,360
우리 작업에서 우리는 생각했습니다.  좋아요, 어떻게

895
00:33:15,360 --> 00:33:18,299
이것을 우리가 관심 있는 역학 유형으로 더 변환할 수 있을까요

896
00:33:18,299 --> 00:33:19,860


897
00:33:19,860 --> 00:33:22,140
?

898
00:33:22,140 --> 00:33:23,940
그리고 이를 위한 한 가지 명확한 방법은

899
00:33:23,940 --> 00:33:26,539
보다 구조화된 연결성을 갖는 것입니다.

900
00:33:26,539 --> 00:33:29,880
우리가 발견한 매트릭스 W ​​어는 쉽게 구현되고 효율적입니다.

901
00:33:29,880 --> 00:33:31,559


902
00:33:31,559 --> 00:33:33,000


903
00:33:33,000 --> 00:33:34,620
로컬로 연결된 레이어처럼 생각할 수 있는 컨볼루션 작업을 통해 구현되므로

904
00:33:34,620 --> 00:33:36,299


905
00:33:36,299 --> 00:33:37,860
모든 뉴런을 연결하는 대신 모든 뉴런

906
00:33:37,860 --> 00:33:39,480
뉴런이 인근 이웃에 연결되므로

907
00:33:39,480 --> 00:33:41,580
훈련 후에는

908
00:33:41,580 --> 00:33:42,840


909
00:33:42,840 --> 00:33:44,880
부드러운 공간처럼 보이는 결과를 얻게 됩니다.  시간적

910
00:33:44,880 --> 00:33:46,620
동역학을

911
00:33:46,620 --> 00:33:48,419
통해 이 모델을 좀 더 명확하게

912
00:33:48,419 --> 00:33:50,519
학습하기 위해 우리는 이를 두 개의

913
00:33:50,519 --> 00:33:52,200
1차 방정식으로 분리하기

914
00:33:52,200 --> 00:33:54,299
전에 설명했던 별도의 2차 미분

915
00:33:54,299 --> 00:33:56,340
방정식을 취합니다.

916
00:33:56,340 --> 00:33:57,960
이를 수치적으로 통합하는 것처럼 생각할 수 있습니다.

917
00:33:57,960 --> 00:34:01,200


918
00:34:01,200 --> 00:34:03,120
속도를 업데이트한 다음

919
00:34:03,120 --> 00:34:06,000
어 그리고 이 모델을

920
00:34:06,000 --> 00:34:07,620
자동 인코더나

921
00:34:07,620 --> 00:34:09,839
자동 회귀 모델과 같은 것으로 훈련할 수 있으므로

922
00:34:09,839 --> 00:34:11,460
입력을 받으면 이를 잠재 공간으로 인코딩합니다.

923
00:34:11,460 --> 00:34:14,339
실제로 입력은 Dr입니다. 이는 f of x

924
00:34:14,339 --> 00:34:16,500
항으로 작동합니다.  구동 항으로 그것은

925
00:34:16,500 --> 00:34:18,599


926
00:34:18,599 --> 00:34:20,879
바닥에서 이러한 발진기를 구동하는 것과 같습니다. 어 그리고 그들은 결합 항

927
00:34:20,879 --> 00:34:23,099
으로 정의되는 자체 역학을 가지고 있습니다.

928
00:34:23,099 --> 00:34:25,800
이러한 로컬 결합은

929
00:34:25,800 --> 00:34:27,540
각 시간 단계에서 이

930
00:34:27,540 --> 00:34:29,460
잠재 상태를 취하여 이 파동 상태를

931
00:34:29,460 --> 00:34:31,560
디코딩하여 시도합니다.  입력을 재구성하고

932
00:34:31,560 --> 00:34:33,540
현재 시간 단계 또는

933
00:34:33,540 --> 00:34:35,699
미래 시간 단계에 있게 되면

934
00:34:35,699 --> 00:34:37,980


935
00:34:37,980 --> 00:34:42,300
훈련 중에 이러한 모델에 대한 일부 분석을 수행하여

936
00:34:42,300 --> 00:34:43,619
훈련 전에 무슨 일이 일어나는지 확인할 수 있고

937
00:34:43,619 --> 00:34:45,780
훈련 후에는 역학의 위상과 속도를 계산할 수 있습니다.

938
00:34:45,780 --> 00:34:47,399


939
00:34:47,399 --> 00:34:49,379
잠재 공간은 기본적으로

940
00:34:49,379 --> 00:34:51,480
거래 시작 시 모델에 파동이 없다는 것을 알 수

941
00:34:51,480 --> 00:34:53,699
있지만 50

942
00:34:53,699 --> 00:34:55,500
에포크 이후 훈련 후에는 물체 회전과 같이 수행하는 이 시퀀스 모델링 작업을 위해 아래쪽으로 전파되는 부드러운 구조적 활동이 있음을 알 수 있습니다.

943
00:34:55,500 --> 00:34:57,119


944
00:34:57,119 --> 00:35:00,420


945
00:35:00,420 --> 00:35:01,800


946
00:35:01,800 --> 00:35:04,380


947
00:35:04,380 --> 00:35:05,940
이것의 이점은 무엇입니까?

948
00:35:05,940 --> 00:35:07,680
제 말은 제가 이렇게 동기를 부여한 전체 이유는

949
00:35:07,680 --> 00:35:10,380
우리가 더

950
00:35:10,380 --> 00:35:11,880
유연하게 학습된 구조를 갖고 싶다고 말하는 것이었습니다. 우리가

951
00:35:11,880 --> 00:35:13,020
실제로 그렇게 하고 있습니까 아니면 단지

952
00:35:13,020 --> 00:35:15,060
예쁜 파도를 받고 있는 것입니까?

953
00:35:15,060 --> 00:35:16,859
음 그래서 우리가 논문에서 보여준 것은

954
00:35:16,859 --> 00:35:19,320
우리가 실제로는  일종의

955
00:35:19,320 --> 00:35:20,940
유용한 구조를 배우고 우리가 보여준 방법은

956
00:35:20,940 --> 00:35:22,440


957
00:35:22,440 --> 00:35:24,960
입력을 받아 인코딩하고

958
00:35:24,960 --> 00:35:27,000
파동

959
00:35:27,000 --> 00:35:29,280
상태를 얻은 다음

960
00:35:29,280 --> 00:35:31,619
해당 파동 상태에서 인위적으로 파동을 전파한 다음

961
00:35:31,619 --> 00:35:33,480
디코딩할 수 있는 교환 다이어그램과 같은 것입니다.  서로 다른 변환의

962
00:35:33,480 --> 00:35:35,220


963
00:35:35,220 --> 00:35:37,140
서로 다른 이미지를 여러 개 표시하여 실제로는 완전히 동일하다는 점을 관찰하세요.

964
00:35:37,140 --> 00:35:39,180


965
00:35:39,180 --> 00:35:41,640
매우 다양한 어 숫자의 서로 다른

966
00:35:41,640 --> 00:35:43,920
특징이 있으며 이를 모델링하기 위해

967
00:35:43,920 --> 00:35:46,140
각 경우에 서로 다른 유형의 파동 활동을 얻는다는 것을 알 수 있습니다.

968
00:35:46,140 --> 00:35:47,880


969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
다른 데이터 세트에 대해 훈련하면 다른 변환이 가능하며

971
00:35:51,119 --> 00:35:53,400


972
00:35:53,400 --> 00:35:55,200
이 경우 더 복잡한 역학을 유사하게 볼 수 있습니다.

973
00:35:55,200 --> 00:35:57,839
진행파나 정재파도 반대 방향으로

974
00:35:57,839 --> 00:36:00,359
진행하는 파동으로 간주될 수 있으므로

975
00:36:00,359 --> 00:36:02,339


976
00:36:02,339 --> 00:36:04,079
이러한 궤도를 모델링하는지 확인합니다.  역학 우리는 진자를 모델링하는 경우 잠재 공간에서

977
00:36:04,079 --> 00:36:06,000
이러한 종류의 원활하게 움직이는 활동 덩어리를 얻습니다.

978
00:36:06,000 --> 00:36:07,619


979
00:36:07,619 --> 00:36:09,839
마찬가지로

980
00:36:09,839 --> 00:36:13,820
일종의 복잡한 진동 활동을 얻으

981
00:36:14,099 --> 00:36:17,339
므로 입력 구조가 보존되지만

982
00:36:17,339 --> 00:36:19,560
추가로 이전보다 더 많은 유연성이

983
00:36:19,560 --> 00:36:21,599
우리의 궁극적인

984
00:36:21,599 --> 00:36:23,400
목표입니다.

985
00:36:23,400 --> 00:36:26,099
그래서 마지막으로 저는

986
00:36:26,099 --> 00:36:28,320
이 연구의 결과가

987
00:36:28,320 --> 00:36:30,420
인공

988
00:36:30,420 --> 00:36:32,099
지능을 향상시킬 수 있을 뿐만 아니라

989
00:36:32,099 --> 00:36:34,440


990
00:36:34,440 --> 00:36:36,240
뇌 측정 결과가 왜 그렇게 보이는지 이해하는 데 어떻게 도움이 될 수 있는지에 대해 조금 이야기하고 싶습니다.

991
00:36:36,240 --> 00:36:38,579
즉, 어

992
00:36:38,579 --> 00:36:41,040
제가 전에 비자와 장소에 관해 조금 이야기한 적이 있어서

993
00:36:41,040 --> 00:36:43,740


994
00:36:43,740 --> 00:36:46,859
Ching higao와의 이 환상적인 작업에서 우리가

995
00:36:46,859 --> 00:36:48,900
논의했던 단순한 지형 사전이

996
00:36:48,900 --> 00:36:50,579
이러한 동일한 효과를 재현할 수 있는지 연구했기 때문에

997
00:36:50,579 --> 00:36:53,339
구체적으로 우리는

998
00:36:53,339 --> 00:36:56,099
이 Cohen의 D의 가치를 두었습니다.  잠재적으로

999
00:36:56,099 --> 00:36:58,200


1000
00:36:58,200 --> 00:37:00,000


1001
00:37:00,000 --> 00:37:02,460
얼굴만 포함하거나 물체 또는 신체만 포함하는 다양한 이미지 데이터 세트와 관련하여

1002
00:37:02,460 --> 00:37:03,359


1003
00:37:03,359 --> 00:37:05,880
각 뉴런에 대한 선택성 측정 기준을 사용하므로 모든 뉴런에 대해 측정할

1004
00:37:05,880 --> 00:37:07,920
가능성이 얼굴에 반응할 가능성이 더 높은지 또는

1005
00:37:07,920 --> 00:37:10,380
러시아인이 뇌에 나타나는지 측정합니다.

1006
00:37:10,380 --> 00:37:12,839


1007
00:37:12,839 --> 00:37:15,300
선택성의 상대적 조직은 적어도

1008
00:37:15,300 --> 00:37:17,400
부분적으로

1009
00:37:17,400 --> 00:37:19,800
데이터의 상관 통계에 기인할 수 있으므로 심층 신경망과

1010
00:37:19,800 --> 00:37:21,359


1011
00:37:21,359 --> 00:37:23,640
같은 고도로 비선형적인 미래 추출기를 통과한 후 다시 경로를 지정해야 하므로

1012
00:37:23,640 --> 00:37:25,440


1013
00:37:25,440 --> 00:37:27,480
비슷한 맥락에서

1014
00:37:27,480 --> 00:37:29,040
흥미롭게도

1015
00:37:29,040 --> 00:37:30,900
삼자 또는 시각적

1016
00:37:30,900 --> 00:37:36,720
스트림이 아닌 것이 알려져 있습니다. 어 또는 개체의 이미지는

1017
00:37:36,720 --> 00:37:38,400
개체에 대한 선택성이며

1018
00:37:38,400 --> 00:37:40,680


1019
00:37:40,680 --> 00:37:43,200
애니메이션과 같은 보다 추상적인 속성으로 구성됩니다. 이것이 살아있는 것인지

1020
00:37:43,200 --> 00:37:46,200
무생물인지 어 대 실제

1021
00:37:46,200 --> 00:37:48,480
개체 크기는 무엇입니까?  는

1022
00:37:48,480 --> 00:37:50,700
찻주전자 대 자동차의 크기입니다. 음,

1023
00:37:50,700 --> 00:37:53,520
우리가 볼 수 있는 것은

1024
00:37:53,520 --> 00:37:56,160
인간의 선택성은 이

1025
00:37:56,160 --> 00:37:57,540
삼자 구조로 구성되어 있다는 것입니다.

1026
00:37:57,540 --> 00:37:59,760
일반적으로 선택성 측면에서

1027
00:37:59,760 --> 00:38:01,920
생물체와 무생물체 사이에 작은 물체가 있다는 것을

1028
00:38:01,920 --> 00:38:04,200
알 수 있습니다.

1029
00:38:04,200 --> 00:38:06,060
여기서도 같은 일이 발생하므로

1030
00:38:06,060 --> 00:38:07,440
이는

1031
00:38:07,440 --> 00:38:08,880
동일한 뉴런 세트의 선택성을 측정하지만

1032
00:38:08,880 --> 00:38:10,859
이러한 자극의 차이와 관련하여

1033
00:38:10,859 --> 00:38:12,440
작은 클러스터가

1034
00:38:12,440 --> 00:38:14,820
애니메이션 클러스터와 무생물 클러스터 사이에 있다는 것을 알 수 있으며

1035
00:38:14,820 --> 00:38:16,079
이는 다시 여러 다른 초기화에 대해 발생합니다.

1036
00:38:16,079 --> 00:38:18,900
이것은

1037
00:38:18,900 --> 00:38:20,880


1038
00:38:20,880 --> 00:38:22,980


1039
00:38:22,980 --> 00:38:24,119


1040
00:38:24,119 --> 00:38:26,220
우리가

1041
00:38:26,220 --> 00:38:28,200
구조화된 세계 모델을 구축했다는 것을 실제로 보여주는 방법이고

1042
00:38:28,200 --> 00:38:30,119
잠재적으로 이 세계 모델이 구조화된 방식으로

1043
00:38:30,119 --> 00:38:31,980


1044
00:38:31,980 --> 00:38:34,740
실제 데이터를 더 잘 표현하는 데 도움이 되기 때문에 흥미롭다고 생각합니다.

1045
00:38:34,740 --> 00:38:37,619


1046
00:38:37,619 --> 00:38:39,119
그런 의미에서 더 낮은 자유 에너지를 얻게 되므로

1047
00:38:39,119 --> 00:38:40,619


1048
00:38:40,619 --> 00:38:42,300
음

1049
00:38:42,300 --> 00:38:44,400
여기서 보여드린 것처럼 이러한 모델을 개발하면 이 구조가

1050
00:38:44,400 --> 00:38:46,500
어떻게 나타나는지에 대한 새로운 메커니즘에 대한 통찰력을 얻을 수 있을 것 같습니다.

1051
00:38:46,500 --> 00:38:48,900


1052
00:38:48,900 --> 00:38:50,460


1053
00:38:50,460 --> 00:38:52,920


1054
00:38:52,920 --> 00:38:55,520


1055
00:38:55,520 --> 00:38:58,260
뉴런의 방향 선택성 어 제가

1056
00:38:58,260 --> 00:39:01,020
특별히 무슨 일이

1057
00:39:01,020 --> 00:39:03,420
일어날 것이라고 기대하지는 않았지만 어 당신은

1058
00:39:03,420 --> 00:39:05,339
이런 파동이 이

1059
00:39:05,339 --> 00:39:08,099
시뮬레이션된 수직 표면 위로 전파되는 것을 보고 있고 저는 괜찮다고 생각했습니다.

1060
00:39:08,099 --> 00:39:09,960
아마도 회전된 이미지를 보여주고 있는 것 같습니다.

1061
00:39:09,960 --> 00:39:11,820
어쩌면 이것이 어떤 영향을 미칠 수도 있습니다.

1062
00:39:11,820 --> 00:39:13,740
방향 선택성

1063
00:39:13,740 --> 00:39:15,599
그리고 실제로 들어가서 서로 다른 방향의 선에 대해

1064
00:39:15,599 --> 00:39:17,460
각 뉴런의 선택성을 측정하면

1065
00:39:17,460 --> 00:39:18,660


1066
00:39:18,660 --> 00:39:22,079


1067
00:39:22,079 --> 00:39:24,300
놀랍게도

1068
00:39:24,300 --> 00:39:25,859
일차 시각 피질에서 볼 수 있는 동양 유형의 기둥을 연상시킵니다.

1069
00:39:25,859 --> 00:39:27,599
이것은 되돌아가는 것입니다.

1070
00:39:27,599 --> 00:39:29,520
Hugo와 Weasel에게 이것은

1071
00:39:29,520 --> 00:39:30,900
일종의 이 모델에서 나온 것이며

1072
00:39:30,900 --> 00:39:33,060


1073
00:39:33,060 --> 00:39:34,440


1074
00:39:34,440 --> 00:39:37,619
변환과 관련하여 공간적 시간적 구조를 가지고 있다는 사실입니다. 물론 이것은

1075
00:39:37,619 --> 00:39:39,599
매우 조잡한 비유이지만 이것이

1076
00:39:39,599 --> 00:39:40,740
어떻게  이러한

1077
00:39:40,740 --> 00:39:42,839
유형의 모델을 구축하면

1078
00:39:42,839 --> 00:39:45,240
뇌가 표상

1079
00:39:45,240 --> 00:39:46,980
구조를 구축하는 방식과 백인이 이전에 생각

1080
00:39:46,980 --> 00:39:48,660
하지 못했던 방식으로 조직되는 방식에 대해 생각하는 데 도움이 될 수 있습니다.

1081
00:39:48,660 --> 00:39:51,300


1082
00:39:51,300 --> 00:39:53,460
음 이런 유형의 작업을 수행하는 사람은 저 뿐만이 아닌 것 같습니다.  그래서

1083
00:39:53,460 --> 00:39:55,859
저는

1084
00:39:55,859 --> 00:39:57,240


1085
00:39:57,240 --> 00:39:59,579
이 일을 하고 있는 다른 사람들에 대해 조금 이야기하고 싶습니다. 어 그래서

1086
00:39:59,579 --> 00:40:00,780
저는 이와 같은 등가 구조에 대해 이야기해 왔습니다.

1087
00:40:00,780 --> 00:40:02,760


1088
00:40:02,760 --> 00:40:04,920
음 James Whittington과

1089
00:40:04,920 --> 00:40:08,880
Tim Barons 및 surrogengoolie와 같은 사람들은

1090
00:40:08,880 --> 00:40:10,680
최근에 대수학을 도입함으로써

1091
00:40:10,680 --> 00:40:14,940


1092
00:40:14,940 --> 00:40:17,040
이 경우 학습 과정에 대한 제약은 내가 서쪽 북동쪽 남쪽 서클로 이동하면 일종의 대수적 구조를 보존해야 한다고 말하는 환경

1093
00:40:17,040 --> 00:40:20,820
에서 어 및 에이전트의 동작과 같았습니다.

1094
00:40:20,820 --> 00:40:23,220


1095
00:40:23,220 --> 00:40:24,780


1096
00:40:24,780 --> 00:40:27,540


1097
00:40:27,540 --> 00:40:29,280
나는 결국 같은 상태로 돌아옵니다.

1098
00:40:29,280 --> 00:40:31,440
point 다시 이러한

1099
00:40:31,440 --> 00:40:32,820
유형의 제약 조건을 도입하면

1100
00:40:32,820 --> 00:40:35,040


1101
00:40:35,040 --> 00:40:36,900
표현과 같은 그리드 셀이 출현하게 됩니다.

1102
00:40:36,900 --> 00:40:39,359
음 그래서 저는 이

1103
00:40:39,359 --> 00:40:41,880
표현 구조에 대한 아이디어가 우리가 찾고 있는 과학적 발견

1104
00:40:41,880 --> 00:40:43,980
보다 더 많은 것을 설명하는 데 어떻게 도움이 될 수 있는지 알고 싶습니다.

1105
00:40:43,980 --> 00:40:45,480


1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
음 그리고  그리고 이것이

1108
00:40:48,359 --> 00:40:51,540
생성 모델

1109
00:40:51,540 --> 00:40:52,800
전체와 어떻게 관련되는지 그리고 마지막으로 이러한 모델의

1110
00:40:52,800 --> 00:40:54,599
인지 가능성에 대해 언급할 내용이 있다고 생각합니다.

1111
00:40:54,599 --> 00:40:56,099


1112
00:40:56,099 --> 00:40:57,420
아마도 우리는

1113
00:40:57,420 --> 00:40:59,579
어 신경과학 관점

1114
00:40:59,579 --> 00:41:01,020
뿐만 아니라 미시 과학

1115
00:41:01,020 --> 00:41:03,839
관점에서도 테스트할 것입니다.  예를 들어

1116
00:41:03,839 --> 00:41:06,000
왼쪽에는 어 Ravens 프로그레시브 행렬이 있는데

1117
00:41:06,000 --> 00:41:08,640


1118
00:41:08,640 --> 00:41:11,099
이 이미지 중 어느 것이 이 패턴에 더 잘 맞을지 말해야 합니다

1119
00:41:11,099 --> 00:41:12,599


1120
00:41:12,599 --> 00:41:14,760
음 또는 예를 들어

1121
00:41:14,760 --> 00:41:16,740
이 Jenga 타워가 당김쇠를 당길 때 넘어질 가능성은 얼마나 됩니까?

1122
00:41:16,740 --> 00:41:19,740
특정 블록이나

1123
00:41:19,740 --> 00:41:22,740
주어진 구조를 가지고

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
이러한 유형의 테스트는 우리가

1126
00:41:26,640 --> 00:41:28,619
구축하고 있는 세계 모델이 인간으로서 선천적으로 가지고 있는

1127
00:41:28,619 --> 00:41:31,560
모델 유형과 유사한지 테스트하는 것입니다.

1128
00:41:31,560 --> 00:41:33,660


1129
00:41:33,660 --> 00:41:36,060


1130
00:41:36,060 --> 00:41:38,400
자연 세계에 살고 있는 존재로서 저는

1131
00:41:38,400 --> 00:41:40,440
이 방향으로 몇 가지 예비 작업을 수행했습니다. 저는

1132
00:41:40,440 --> 00:41:43,079
매우 어 예비적이며 그다지

1133
00:41:43,079 --> 00:41:45,480
복잡하지는 않지만 음 일종의

1134
00:41:45,480 --> 00:41:47,820
시각적 환상을 모델링하려는 시도이므로

1135
00:41:47,820 --> 00:41:50,520
움직이는 막대의 매우 간단한 데이터 세트를 취한다면

1136
00:41:50,520 --> 00:41:52,980
자극이나 정적 막대 또는 프레임을

1137
00:41:52,980 --> 00:41:54,960
조금 움직이면

1138
00:41:54,960 --> 00:41:57,060
모델이 실제로

1139
00:41:57,060 --> 00:41:58,800
누락된 프레임을 추론한 다음 실제로

1140
00:41:58,800 --> 00:42:01,079
연속 동작도 추론하므로 다시 수정하기

1141
00:42:01,079 --> 00:42:03,300


1142
00:42:03,300 --> 00:42:05,820
전에 실제 자극이 제공하는 궤적을 오버슈팅하는 것과 같습니다.

1143
00:42:05,820 --> 00:42:08,760
그래서

1144
00:42:08,760 --> 00:42:10,320
환상 모델링은 확실히

1145
00:42:10,320 --> 00:42:12,660
우리의 세계 모델이

1146
00:42:12,660 --> 00:42:14,760
우리가 가지고 있는 모델 유형과 유사한지 연구하는 흥미로운 방법이라고 생각합니다.

1147
00:42:14,760 --> 00:42:16,619


1148
00:42:16,619 --> 00:42:19,619
결론적으로 어 그래,

1149
00:42:19,619 --> 00:42:21,900
지형 사전 분석을 통해

1150
00:42:21,900 --> 00:42:23,220
그들이 구조화된

1151
00:42:23,220 --> 00:42:24,839
표현이나 구조화된 세계

1152
00:42:24,839 --> 00:42:26,700
모델을 효과적으로 학습했음을 보여줄 수 있을 것 같습니다.  학습된 구조는

1153
00:42:26,700 --> 00:42:29,160
유연하고 임의에 적응 가능합니다.

1154
00:42:29,160 --> 00:42:30,780
전통적인

1155
00:42:30,780 --> 00:42:33,720
등변량 및 지형 제공자와는 달리 변환은

1156
00:42:33,720 --> 00:42:35,579


1157
00:42:35,579 --> 00:42:37,619
지형 vae에서 했던 것처럼 통계적으로 유도될 수 있거나

1158
00:42:37,619 --> 00:42:39,480
이러한 신경파 기계 유형 모델에서 보여준 것처럼 역학을 통해 유도될 수 있으므로

1159
00:42:39,480 --> 00:42:42,000


1160
00:42:42,000 --> 00:42:44,460
결론을 내리겠습니다.

1161
00:42:44,460 --> 00:42:46,980


1162
00:42:46,980 --> 00:42:50,280
1980년 후쿠시마의 논문에서 제가 찾은 인용문은 인간과 동일한 패턴 인식 능력을

1163
00:42:50,280 --> 00:42:52,079


1164
00:42:52,079 --> 00:42:53,520
가진 신경망 모델을 만들 수 있다면

1165
00:42:53,520 --> 00:42:55,020


1166
00:42:55,020 --> 00:42:57,060


1167
00:42:57,060 --> 00:42:58,800
우리에게 강력한 단서를 제공할 것이라고 그가 말한 시대보다 훨씬 앞서 있다고 생각했습니다.

1168
00:42:58,800 --> 00:43:00,000


1169
00:43:00,000 --> 00:43:03,240
뇌의 신경 메커니즘을 이해하는 것입니다.

1170
00:43:03,240 --> 00:43:06,119
우리가 여기서 이루고자 하는 목표 중 일부는

1171
00:43:06,119 --> 00:43:08,220
내 조언자인 Max, 공동

1172
00:43:08,220 --> 00:43:11,280
저자인 Patrick UA Emil jinghian

1173
00:43:11,280 --> 00:43:17,359
및 Yorn이고 토론에 관심이 있습니다. 감사합니다 알겠습니다 알겠습니다

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
고마워요 아주 어

1176
00:43:27,480 --> 00:43:31,079
흥미로운 프리젠테이션

1177
00:43:31,079 --> 00:43:33,480
시작할 곳이 많네요 아마도 어

1178
00:43:33,480 --> 00:43:36,000
어떻게 이 작업을 하게 되었는지

1179
00:43:36,000 --> 00:43:38,520


1180
00:43:38,520 --> 00:43:43,819
박사 과정을 위해 어떻게 이 작업에 참여하게 되었는지에 대한 약간의 맥락 예

1181
00:43:43,920 --> 00:43:45,119


1182
00:43:45,119 --> 00:43:46,020
음

1183
00:43:46,020 --> 00:43:49,200
내 말은 우리가 내 그룹이 아닌 연구를 해왔다는 뜻입니다.

1184
00:43:49,200 --> 00:43:51,000
저는 대학에 다니면서

1185
00:43:51,000 --> 00:43:52,700


1186
00:43:52,700 --> 00:43:56,640
한동안 수학적 관점에서 구조화된 표현을 연구해 왔습니다.

1187
00:43:56,640 --> 00:43:58,319
사람들 중 일부는

1188
00:43:58,319 --> 00:44:00,240
변형 자동 인코더와 같은 모델을 가지고 있고,

1189
00:44:00,240 --> 00:44:01,740


1190
00:44:01,740 --> 00:44:04,680
음 그리고

1191
00:44:04,680 --> 00:44:06,960
항상 있었던 것이

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
모델인 것 같아요  이는 회전 2D

1194
00:44:11,220 --> 00:44:13,560
회전을 완벽하게 존중하지만

1195
00:44:13,560 --> 00:44:15,960
3D 회전을 원하면 그렇게 할 수 없습니다.

1196
00:44:15,960 --> 00:44:17,819
왜냐하면 그것은 2D 계획에 대한 투영 측면에서 그룹이 아니기 때문입니다.

1197
00:44:17,819 --> 00:44:19,740


1198
00:44:19,740 --> 00:44:21,180


1199
00:44:21,180 --> 00:44:23,460
예를 들어 이것이 회전할 때 정보를 잃게 됩니다.

1200
00:44:23,460 --> 00:44:24,240
음

1201
00:44:24,240 --> 00:44:26,280
또는

1202
00:44:26,280 --> 00:44:27,960
제가 처음에 지적하려고 했던 것과 같은 모든 종류의 자연 변환은

1203
00:44:27,960 --> 00:44:29,339


1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
뇌가 자연 변환을 어떻게 모델링하는지 생각하려고 했던 것 같습니다.

1206
00:44:31,740 --> 00:44:34,020


1207
00:44:34,020 --> 00:44:35,400
이 현재

1208
00:44:35,400 --> 00:44:37,200
프레임워크

1209
00:44:37,200 --> 00:44:41,099
에서는

1210
00:44:41,099 --> 00:44:44,579
변형 오토인코더 측면에서 동작이 역할을 하는 것을 볼 수 있습니다.

1211
00:44:44,579 --> 00:44:48,420


1212
00:44:48,420 --> 00:44:50,520
외부 패턴뿐만 아니라

1213
00:44:50,520 --> 00:44:52,380
행동의 결과 또는

1214
00:44:52,380 --> 00:44:55,800
행동이 포함된 세계 모델 구조 구조를 포함하는 모델 맞죠

1215
00:44:55,800 --> 00:44:58,619
아니오 좋은 질문이고

1216
00:44:58,619 --> 00:45:01,319
행동 추론이

1217
00:45:01,319 --> 00:45:03,839
사실상 답인 것 같아요

1218
00:45:03,839 --> 00:45:05,940
그에 대한 좋은 답인 것 같아요

1219
00:45:05,940 --> 00:45:09,000
음 저도 알아요

1220
00:45:09,000 --> 00:45:11,099


1221
00:45:11,099 --> 00:45:12,660


1222
00:45:12,660 --> 00:45:15,060
외부에서 훈련된 월드 모델을 사용하는 강화 학습 프레임워크이므로

1223
00:45:15,060 --> 00:45:17,280
VAE 등을 훈련한 다음

1224
00:45:17,280 --> 00:45:19,800


1225
00:45:19,800 --> 00:45:23,040
강화 학습 시스템에서 해당 표현을 사용하지만 제

1226
00:45:23,040 --> 00:45:24,720
생각에는 어 액션이 포함된

1227
00:45:24,720 --> 00:45:26,520
단일 목표인 완전한 종류의 시스템이 있다고 생각합니다.

1228
00:45:26,520 --> 00:45:30,780


1229
00:45:30,780 --> 00:45:33,660
데이터의 가능성의 일부로 그리고 어

1230
00:45:33,660 --> 00:45:35,280
그래 나는 그것이 훨씬 더 우아하다고 생각하고

1231
00:45:35,280 --> 00:45:38,940
그래서 나는 그것을 크게 지지

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
합니다. 음 나는 지금까지

1234
00:45:43,140 --> 00:45:45,480
이러한 세계 모델이 vae에서 어떻게 구조화되었는지 연구하지 않았습니다.

1235
00:45:45,480 --> 00:45:47,520
전혀 작업을 하지 않았지만

1236
00:45:47,520 --> 00:45:48,780


1237
00:45:48,780 --> 00:45:50,819


1238
00:45:50,819 --> 00:45:52,339


1239
00:45:52,339 --> 00:45:54,839
변형 자동 인코더에 좀 더 구조화된 세계 모델을 갖는 것이

1240
00:45:54,839 --> 00:45:56,880
활성 설정에서도 도움이 될지 확인하는 것은 확실히 매우 흥미로울 것이라고 생각합니다. 제

1241
00:45:56,880 --> 00:45:58,319


1242
00:45:58,319 --> 00:46:00,119
생각엔 그게 굉장할 것 같아요.  어,

1243
00:46:00,119 --> 00:46:03,599


1244
00:46:03,599 --> 00:46:05,579
이전에 보여줬던 그리드 셀의 출현과

1245
00:46:05,579 --> 00:46:07,500
같은 것들이 그 방향을 가리킬 수도 있고

1246
00:46:07,500 --> 00:46:08,880


1247
00:46:08,880 --> 00:46:10,560
어쩌면 뇌가 뭔가를 하고 있을 수도 있겠네요.

1248
00:46:10,560 --> 00:46:12,540
정말 분명히 많은 구조를 가지고 있습니다.

1249
00:46:12,540 --> 00:46:13,680


1250
00:46:13,680 --> 00:46:15,359
음, 이것은 분명히

1251
00:46:15,359 --> 00:46:19,140
일부 작업을 수행하는 데 유용할 것 같습니다.

1252
00:46:19,140 --> 00:46:21,720
아, 네,

1253
00:46:21,720 --> 00:46:24,480
당신이 강연에서 가져온 정말 좋은 유사점은

1254
00:46:24,480 --> 00:46:28,040
로컬로 연결된 장치가

1255
00:46:28,040 --> 00:46:30,960
당신의 모델이

1256
00:46:30,960 --> 00:46:33,780
컨볼루션

1257
00:46:33,780 --> 00:46:35,640
제약과 패턴을 구조적으로 구현할 수 있게 했고 이것이

1258
00:46:35,640 --> 00:46:37,500
이러한 발생 패턴으로 이어졌고 유사

1259
00:46:37,500 --> 00:46:41,339
하게 어 Doral이 있었다는 것이었습니다.

1260
00:46:41,339 --> 00:46:45,680
그들은 경로 탐색

1261
00:46:45,680 --> 00:46:48,359
제약 조건을 올바르게 갖고 있었기 때문에

1262
00:46:48,359 --> 00:46:50,280


1263
00:46:50,280 --> 00:46:53,760
음 당신이 알고 있는 것은 흥미롭습니다. 이러한 행동이나

1264
00:46:53,760 --> 00:46:56,819
정책 경험적 방법 또는

1265
00:46:56,819 --> 00:46:59,579
공동 모터 탐색과 같은 희소성에 대해 생각해 보면 결국

1266
00:46:59,579 --> 00:47:02,339


1267
00:47:02,339 --> 00:47:04,980
관절을 움직이는 두 가지 상호 반대되는 방법

1268
00:47:04,980 --> 00:47:07,079
과 구성성이 있다는 것을 이해하게 됩니다.

1269
00:47:07,079 --> 00:47:09,119
관절 전반에 대한 학습은

1270
00:47:09,119 --> 00:47:10,680
낮은 수준에 고정되면 더

1271
00:47:10,680 --> 00:47:14,480
높은 수준에서 학습할 수 있으므로 매우 매력적이고

1272
00:47:14,480 --> 00:47:17,599
어 어 틈새 관련

1273
00:47:17,599 --> 00:47:20,460
일반화 방법입니다.

1274
00:47:20,460 --> 00:47:23,819
둘 다

1275
00:47:23,819 --> 00:47:25,740
세계의 실제 제약 조건을 기반으로 하지만

1276
00:47:25,740 --> 00:47:27,720
특히 행동을 통해

1277
00:47:27,720 --> 00:47:29,460
잠재적으로 다음과 같은 것을 포함할 수 있기 때문입니다.

1278
00:47:29,460 --> 00:47:31,380
아주 간단해요 그렇죠 네

1279
00:47:31,380 --> 00:47:33,599
아니요 제 생각에는 그게 확실히

1280
00:47:33,599 --> 00:47:36,599
사실인 것 같아요. 어 만약

1281
00:47:36,599 --> 00:47:38,339
당신의 행동 자체에서 오는 제약이 있다면

1282
00:47:38,339 --> 00:47:40,500
그것은

1283
00:47:40,500 --> 00:47:42,839


1284
00:47:42,839 --> 00:47:44,819
당신

1285
00:47:44,819 --> 00:47:47,460
의 잠재 공간을 구조화하는 데 큰 도움이 될 것이고 제 생각엔 예

1286
00:47:47,460 --> 00:47:48,480
한 가지인 것 같아요  저는

1287
00:47:48,480 --> 00:47:49,980


1288
00:47:49,980 --> 00:47:50,700
음

1289
00:47:50,700 --> 00:47:52,740
뭔가가 표현 기하학에 대한 Stefano fousey의 작업과 같은 생각을 하게 만들었다고 언급하고 싶었습니다.

1290
00:47:52,740 --> 00:47:55,500


1291
00:47:55,500 --> 00:47:58,859
어

1292
00:47:58,859 --> 00:48:01,920


1293
00:48:01,920 --> 00:48:04,440


1294
00:48:04,440 --> 00:48:08,099
시스템에 대한 주어진 이해가 얼마나 일반화 가능한지를 결정합니다 어 그리고 제 생각에는 여러분이

1295
00:48:08,099 --> 00:48:11,880
이러한 어 세트의

1296
00:48:11,880 --> 00:48:14,520
활동을 이해할 수 있다면  선형 분류기로 분리 가능하거나 고도로 병렬로

1297
00:48:14,520 --> 00:48:16,079
분리 가능

1298
00:48:16,079 --> 00:48:18,839
하면 본질적으로

1299
00:48:18,839 --> 00:48:20,700
일반화를 수행할 수 있으며

1300
00:48:20,700 --> 00:48:23,099
이러한 유형의 편향을 부과하거나

1301
00:48:23,099 --> 00:48:25,040
잠재적으로 이와 같은

1302
00:48:25,040 --> 00:48:27,000
행동에 의해 부과되는 제약 조건을 통해

1303
00:48:27,000 --> 00:48:28,740


1304
00:48:28,740 --> 00:48:32,040
양보하거나 유도할 수 있다고 생각합니다.

1305
00:48:32,040 --> 00:48:33,660
더 나은 표현 기하학과

1306
00:48:33,660 --> 00:48:35,220
이것은 구성성과 같은 모든 종류의 이점을 가지고 있습니다.

1307
00:48:35,220 --> 00:48:36,660


1308
00:48:36,660 --> 00:48:39,359
예, 우리의 일반화는

1309
00:48:39,359 --> 00:48:41,760
훌륭합니다. 포인트 멋지네요. 예, 매우

1310
00:48:41,760 --> 00:48:43,440
흥미로운 부분입니다. 좋아요

1311
00:48:43,440 --> 00:48:45,960
라이브 채팅에서 몇 가지 질문을 읽을 것입니다.

1312
00:48:45,960 --> 00:48:48,420
사랑 진화는 모델링 환상 학습에

1313
00:48:48,420 --> 00:48:52,260
대한 실제적이거나 관찰된 제한 사항을 작성했습니다.

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
커뮤니티는

1316
00:49:00,420 --> 00:49:03,060
공포증이 없습니다. 시선의 중심도 없고 음

1317
00:49:03,060 --> 00:49:05,940


1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
시간도 없습니다. 제 말은 대부분의 컨볼루셔널 신경망을 의미합니다.

1320
00:49:11,460 --> 00:49:13,260
이런 종류의

1321
00:49:13,260 --> 00:49:15,599
순환 신경망을 사용하고 있지만 시간이

1322
00:49:15,599 --> 00:49:18,420
명확하게 정의되지 않았습니다.  이러한 모델에서는

1323
00:49:18,420 --> 00:49:20,220


1324
00:49:20,220 --> 00:49:23,400
환상 재판을 겪고 있는 인간에 대한 연속적인 시간 설정에 있기 때문에

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
음

1327
00:49:25,319 --> 00:49:27,480
그리고 제 생각에는 이 두 가지의 조합은

1328
00:49:27,480 --> 00:49:30,359
인간으로서 또는 대부분의

1329
00:49:30,359 --> 00:49:33,300
사물로서 어

1330
00:49:33,300 --> 00:49:35,940
당신의 시선 이동 위치와

1331
00:49:35,940 --> 00:49:38,220
당신의 이득이 다음에 달려 있다는 사실입니다.

1332
00:49:38,220 --> 00:49:40,140
특정 영역을 바라보는 것처럼 많은

1333
00:49:40,140 --> 00:49:42,780
인지 테스트가 있기 때문에

1334
00:49:42,780 --> 00:49:46,560
모델이 있다면 정말 도움이 될 것 같아요.

1335
00:49:46,560 --> 00:49:48,540
예, 제 말은

1336
00:49:48,540 --> 00:49:50,760
이것을 시선을 어디로 움직여야 하는지 배우는 것과 같은 일종의 행동으로 생각할 수 있다는 것입니다.

1337
00:49:50,760 --> 00:49:52,980


1338
00:49:52,980 --> 00:49:54,420


1339
00:49:54,420 --> 00:49:56,220
환상을 모델링하는 데 많은 도움이 될 수 있는 가장 간단한 방법은 인지

1340
00:49:56,220 --> 00:49:58,859


1341
00:49:58,859 --> 00:50:00,720
과학

1342
00:50:00,720 --> 00:50:02,940
실험이나 환상에 관한 논문을 읽고

1343
00:50:02,940 --> 00:50:05,160
이 데이터 세트를 내 컴퓨터에 넣어도 괜찮을까 하는 생각과 같습니다.

1344
00:50:05,160 --> 00:50:07,560
모델을 만들어 테스트해 보면

1345
00:50:07,560 --> 00:50:08,579
대부분 대답은 '아니요'입니다.

1346
00:50:08,579 --> 00:50:10,619


1347
00:50:10,619 --> 00:50:12,900
주변을 둘러볼 수 있는 모델이 없거나 그런 식으로 제한된 시야를 가진 모델이 없기 때문입니다.

1348
00:50:12,900 --> 00:50:14,660


1349
00:50:14,660 --> 00:50:16,619
그래서 그래 내 생각엔 그게 한계 중 하나인 것 같아

1350
00:50:16,619 --> 00:50:19,680
또 하나는

1351
00:50:19,680 --> 00:50:20,579
음

1352
00:50:20,579 --> 00:50:22,920
그래 만들어  실험은 훨씬 더

1353
00:50:22,920 --> 00:50:24,900
복잡해서 실용적인 한계 중 하나입니다.

1354
00:50:24,900 --> 00:50:27,359


1355
00:50:27,359 --> 00:50:30,240
와우 훌륭한 답변을 통해

1356
00:50:30,240 --> 00:50:33,920
테이블 ​​위에서 회전하는 문자가 있는 종이를 생각하게 됩니다.

1357
00:50:33,920 --> 00:50:36,780
이는 숫자 회전입니다.

1358
00:50:36,780 --> 00:50:38,460
포비에이션과 환상의 역동성에 대한 좋은 점

1359
00:50:38,460 --> 00:50:40,079
제 생각에 실제로

1360
00:50:40,079 --> 00:50:42,599
환상에 대해 언급하신 것 같습니다.  그러나

1361
00:50:42,599 --> 00:50:43,980
일반화 맥락에서 언급한

1362
00:50:43,980 --> 00:50:46,859
2차원 화면에서 회전하는 것은

1363
00:50:46,859 --> 00:50:49,500
3차원으로 일반화되지 않으며

1364
00:50:49,500 --> 00:50:52,920
차원 붕괴

1365
00:50:52,920 --> 00:50:55,559
또는 축소는 큐브 투영의 기초입니다.

1366
00:50:55,559 --> 00:50:58,619
환상과 큐브 및 그림

1367
00:50:58,619 --> 00:51:01,880
회전 환상이 화면에 있고

1368
00:51:01,880 --> 00:51:05,280
거기에  실루엣 또는

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
생성이

1371
00:51:09,839 --> 00:51:12,359
임계에 가깝거나 퇴행 모델의 분기점에 가까워서

1372
00:51:12,359 --> 00:51:13,680


1373
00:51:13,680 --> 00:51:17,160
어떤 방식으로든 표현할 수 있다는 모호한 자극이 있으므로

1374
00:51:17,160 --> 00:51:19,920
전환 환상의 대부분은

1375
00:51:19,920 --> 00:51:22,020


1376
00:51:22,020 --> 00:51:23,819
이미지의 평면성

1377
00:51:23,819 --> 00:51:26,280
과 한계 및 일반화에 기반합니다.

1378
00:51:26,280 --> 00:51:28,740


1379
00:51:28,740 --> 00:51:32,460
아

1380
00:51:32,460 --> 00:51:34,859
그래 어딘가에 미안한데 일이 좀 있거나

1381
00:51:34,859 --> 00:51:35,880


1382
00:51:35,880 --> 00:51:37,619


1383
00:51:37,619 --> 00:51:39,480


1384
00:51:39,480 --> 00:51:42,000
낸시 켄도

1385
00:51:42,000 --> 00:51:45,119
최근에 옆모습이었던 것처럼 사람들의 머릿속에 3차원 이미지가 있다고 주장할 수 있지만 보여주는 그래 난 그렇지

1386
00:51:45,119 --> 00:51:48,119
않아  어차피 우리 모델이 그렇게

1387
00:51:48,119 --> 00:51:50,460
크지 않다는 걸 모르잖아 그래

1388
00:51:50,460 --> 00:51:53,700
그거 꽤 흥미롭네

1389
00:51:53,700 --> 00:51:56,160
음 알았어 음 알겠어? Upcycle Club

1390
00:51:56,160 --> 00:51:58,200
채팅에서 그들은 단 하나의 뉴런만 원한다고 상상하면

1391
00:51:58,200 --> 00:52:00,000
거의 효과적으로 학습할 수 있다면 칭찬을 썼어

1392
00:52:00,000 --> 00:52:02,160


1393
00:52:02,160 --> 00:52:03,780
모든 예에 대해 활성입니다.

1394
00:52:03,780 --> 00:52:06,540
어 귀하의 모델은

1395
00:52:06,540 --> 00:52:08,579
데이터 세트 디자인

1396
00:52:08,579 --> 00:52:10,980
이나 이와 유사한 것을 기억하려고 할 것입니다.

1397
00:52:10,980 --> 00:52:12,180
음 그리고 용량이 충분하지 않을 것이므로

1398
00:52:12,180 --> 00:52:14,940
그

1399
00:52:14,940 --> 00:52:18,359
희소성 수준을 조정하는 것이 확실히 어

1400
00:52:18,359 --> 00:52:22,200
중요한 요소라고 생각합니다.

1401
00:52:22,200 --> 00:52:25,020
가능성을 살펴볼 때

1402
00:52:25,020 --> 00:52:26,220


1403
00:52:26,220 --> 00:52:28,579
프레임워크를 두 배로 늘리는 경우 일반적으로

1404
00:52:28,579 --> 00:52:32,040
가능성 자체와 자동으로 균형이 맞춰집니다. 음

1405
00:52:32,040 --> 00:52:33,000


1406
00:52:33,000 --> 00:52:34,380
모델링 생성을 수행하지 않는 경우 조정하고

1407
00:52:34,380 --> 00:52:35,760


1408
00:52:35,760 --> 00:52:38,460
싶은 희소성 페널티만 있을 뿐입니다.  그 매개변수

1409
00:52:38,460 --> 00:52:40,980
알겠습니다.

1410
00:52:40,980 --> 00:52:43,380


1411
00:52:43,380 --> 00:52:45,599


1412
00:52:45,599 --> 00:52:47,040
피드백

1413
00:52:47,040 --> 00:52:50,400
루프 노이즈나 적대적 입력과 같은 다양한 요인으로 인해 네트워크가 불안정하거나 혼란스러워지는 armina의 런어웨이 동작을 명확히 하기 위한 것입니다. 음 네,

1414
00:52:50,400 --> 00:52:52,380


1415
00:52:52,380 --> 00:52:54,180


1416
00:52:54,180 --> 00:52:55,859
반복되는 설정처럼 이것을 살펴보지 않은 것 같습니다.

1417
00:52:55,859 --> 00:52:58,500
피드백 루프를 얻을 수도

1418
00:52:58,500 --> 00:52:59,460


1419
00:52:59,460 --> 00:53:01,800
있지만 예, 적의

1420
00:53:01,800 --> 00:53:04,319
예가

1421
00:53:04,319 --> 00:53:07,800
희소성 수준에 의해 잠재적으로 영향을 받는 것을 볼 수 있습니다. 음

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
흥미로운 점은 예를 공유하는 데

1424
00:53:10,859 --> 00:53:12,660
더 민감하거나 덜 민감하다는 것입니다.

1425
00:53:12,660 --> 00:53:16,040


1426
00:53:16,040 --> 00:53:19,440
잘 모르겠습니다.

1427
00:53:19,440 --> 00:53:21,720
완전히 연결된

1428
00:53:21,720 --> 00:53:23,579
고차원 모델이

1429
00:53:23,579 --> 00:53:25,619
점진적으로 작아집니다.

1430
00:53:25,619 --> 00:53:27,540
일반적으로

1431
00:53:27,540 --> 00:53:29,760
장단점이 무엇인지 잘 이해됩니다.

1432
00:53:29,760 --> 00:53:34,079
계산이 더 쉬워집니다. 모델이 더 작을수록

1433
00:53:34,079 --> 00:53:36,420
기본 그래프가 더 명확하게

1434
00:53:36,420 --> 00:53:39,119
표현되고 다른 모든 장단점도 갖게 됩니다.

1435
00:53:39,119 --> 00:53:41,339


1436
00:53:41,339 --> 00:53:43,680
일반화의 거짓 긍정 및 부정을 사용

1437
00:53:43,680 --> 00:53:45,720
하지만 그것이 반복적 적합

1438
00:53:45,720 --> 00:53:47,579
프로세스인 이유입니다. 따라서

1439
00:53:47,579 --> 00:53:49,760


1440
00:53:49,760 --> 00:53:52,800
희소화 접근 방식

1441
00:53:52,800 --> 00:53:55,700
균형이

1442
00:53:56,700 --> 00:53:59,520
AIC 또는 Bic 또는 기타

1443
00:53:59,520 --> 00:54:01,619
모델 피팅 접근 방식을 사용하지 않고 주어진 입력에 대한

1444
00:54:01,619 --> 00:54:03,660
관련 희소화를 결정하는 방법을 추측합니다.

1445
00:54:03,660 --> 00:54:07,079


1446
00:54:07,079 --> 00:54:09,780


1447
00:54:09,780 --> 00:54:11,940
올가미 회귀에서와 같은지 결정합니다. 얼마나

1448
00:54:11,940 --> 00:54:14,339
얼마나 희박한지 얼마나 임계값을 정하는지 어떻게 알 수 있나요? 예

1449
00:54:14,339 --> 00:54:17,220


1450
00:54:17,220 --> 00:54:19,559
제 생각에는

1451
00:54:19,559 --> 00:54:22,440
이것에 대한 좋은 문헌이 많이 있고 심지어

1452
00:54:22,440 --> 00:54:25,319
하버드에서 좋아하는 사람들도 있고  어떤

1453
00:54:25,319 --> 00:54:29,520
사람들은 지금 함께 일하고 있습니다.

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
이런 종류의 전개된

1456
00:54:34,380 --> 00:54:36,780
음 반복 희소화 네트워크를 수행했는데,

1457
00:54:36,780 --> 00:54:37,800
이는 순환 신경망과 같고

1458
00:54:37,800 --> 00:54:40,380
반복적으로 희소화되며

1459
00:54:40,380 --> 00:54:41,940
이것이

1460
00:54:41,940 --> 00:54:45,780
빨간색 손실 또는 어 그룹

1461
00:54:45,780 --> 00:54:47,520
활성 그룹과 같은 그룹 스포츠 활성화와 같은 결과를 낳는다는 것을 보여줄 수 있습니다.

1462
00:54:47,520 --> 00:54:48,960


1463
00:54:48,960 --> 00:54:52,859
여기서 음 이 설정을 사용하고 있습니다 어 정말

1464
00:54:52,859 --> 00:54:55,859
이건

1465
00:54:55,859 --> 00:54:59,280
음 이 T 변수의 구성을 가짐으로써

1466
00:54:59,280 --> 00:55:04,079
상단에 Z가 있고 어

1467
00:55:04,079 --> 00:55:07,859
그런 다음

1468
00:55:07,859 --> 00:55:09,119


1469
00:55:09,119 --> 00:55:11,579
하단에 있는 U 변수의 합에 의해 어떤 효과로 게이트됩니다.

1470
00:55:11,579 --> 00:55:13,200
그래서 W가  아마도 저는 이것이 그룹을 정의하는 연결 매트릭스라는 것에 대해 매우 명확하지 않았을 것입니다.

1471
00:55:13,200 --> 00:55:16,500


1472
00:55:16,500 --> 00:55:18,359
그래서 저는

1473
00:55:18,359 --> 00:55:20,400


1474
00:55:20,400 --> 00:55:22,380
이 모든 U를 함께 연결하는 그룹의 Varsity를 정의하고 있습니다. 그래서

1475
00:55:22,380 --> 00:55:23,940
아이디어는

1476
00:55:23,940 --> 00:55:27,540
uh 다른 하나가 모두 있다면 여기와 같습니다.

1477
00:55:27,540 --> 00:55:31,740
예를 들어 모든 사용이

1478
00:55:31,740 --> 00:55:35,520
주어진 t에 대해 활성화되지 않거나

1479
00:55:35,520 --> 00:55:38,280
모든 변수가 주어진 t에 대해 활성화된 경우

1480
00:55:38,280 --> 00:55:41,040
uh 그 t 변수는 매우

1481
00:55:41,040 --> 00:55:42,780
작을 것입니다. 왜냐하면 분모가

1482
00:55:42,780 --> 00:55:44,339
매우 커지고

1483
00:55:44,339 --> 00:55:47,160
희박성을 유발하기 때문입니다.  어 그것은 제약 조건

1484
00:55:47,160 --> 00:55:49,260
만족입니다. 만약 여러분이

1485
00:55:49,260 --> 00:55:51,839
모두 작은 U 세트를 가지고 있다면 어 그러면

1486
00:55:51,839 --> 00:55:54,480
그 제약 조건이 충족되고

1487
00:55:54,480 --> 00:55:57,180
이제 Z는 일종의 Express 자체가 허용되고

1488
00:55:57,180 --> 00:56:00,240
그게 바로

1489
00:56:00,240 --> 00:56:02,880


1490
00:56:02,880 --> 00:56:06,180
활성화까지 어 일종의 달성이 되는 것입니다.  그래서 이것은

1491
00:56:06,180 --> 00:56:07,020
두 개의

1492
00:56:07,020 --> 00:56:09,300
어 케일 발산 용어에 의해 유도됩니다. 이것은

1493
00:56:09,300 --> 00:56:12,960
각각의 unhc가 가우스로부터 얼마나 멀리 떨어져 있는지를 말하고

1494
00:56:12,960 --> 00:56:15,180
있으며, 이

1495
00:56:15,180 --> 00:56:16,980
스튜던트 T 변수의 구성을 통해

1496
00:56:16,980 --> 00:56:20,880
우리는 이 가우스로부터 희소 사전 분포를 효과적으로 구성하고 있습니다.

1497
00:56:20,880 --> 00:56:23,040


1498
00:56:23,040 --> 00:56:24,839
행위의 용어

1499
00:56:24,839 --> 00:56:27,599
실제 목적 어 용어와

1500
00:56:27,599 --> 00:56:28,920
우리가 최적화하는 목적은 어느 정도 희소성을 향해

1501
00:56:28,920 --> 00:56:31,619
나아가는 이 두 KL 용어일 뿐이며

1502
00:56:31,619 --> 00:56:34,020
이것은

1503
00:56:34,020 --> 00:56:36,359


1504
00:56:36,359 --> 00:56:39,000
디코더를 통해 여기 우도 용어와 자동으로 균형을 이루

1505
00:56:39,000 --> 00:56:41,220
므로 우리는 그렇게 하지 않습니다.  우리가 조정하고 있는 용어는 없지만

1506
00:56:41,220 --> 00:56:42,839


1507
00:56:42,839 --> 00:56:44,280
이러한 다양한 인코더의 매개 변수를 학습한 다음

1508
00:56:44,280 --> 00:56:48,200
실패와 응급 상황을 분석하고 있습니다.

1509
00:56:48,540 --> 00:56:49,920
아

1510
00:56:49,920 --> 00:56:52,859
그렇군요 시선과 환상에 관해 쓴 Dave Douglas의 또 다른 질문입니다.

1511
00:56:52,859 --> 00:56:55,500


1512
00:56:55,500 --> 00:56:59,160


1513
00:56:59,160 --> 00:57:01,920
유아의 상황에 대한 연구를

1514
00:57:01,920 --> 00:57:04,260
분리할 수 있습니까?  더 낮은 수준의 환상으로 Rel

1515
00:57:04,260 --> 00:57:06,140
아마도 더 높은 수준의

1516
00:57:06,140 --> 00:57:09,980
개념적 불변성

1517
00:57:13,619 --> 00:57:15,720
어 현재 종류의 아키텍처를 읽을 수 있습니까?

1518
00:57:15,720 --> 00:57:18,300


1519
00:57:18,300 --> 00:57:23,520
유아의 상황에 대한 연구가

1520
00:57:23,520 --> 00:57:26,640
음 인지적 상황으로 분리될 수 있습니까

1521
00:57:26,640 --> 00:57:31,619
? 예 아마도 저는 아닐 것입니다. 저는 어

1522
00:57:31,619 --> 00:57:33,900
전문가가 아니거나 실제로는 매우 익숙하지도 않습니다

1523
00:57:33,900 --> 00:57:35,460


1524
00:57:35,460 --> 00:57:37,859
객체 영속성 연구, 유아

1525
00:57:37,859 --> 00:57:40,260
및 불변성 연구와 같은 것들이 있지만 신경망 아키텍처를

1526
00:57:40,260 --> 00:57:42,300
연구하는 것은 엄청나게 흥미로울 것이라고 생각합니다.

1527
00:57:42,300 --> 00:57:44,339
그리고

1528
00:57:44,339 --> 00:57:46,740
그것은

1529
00:57:46,740 --> 00:57:48,780
제가

1530
00:57:48,780 --> 00:57:51,780
여기 이 라인으로 모델링하려고 했던 어 환상에 대한 아이디어 중 일부였습니다.

1531
00:57:51,780 --> 00:57:53,099
제가 이것에 대해 매우 명확한지는 모르겠지만 맨

1532
00:57:53,099 --> 00:57:55,380
윗줄은 입력이고 우리는

1533
00:57:55,380 --> 00:57:57,359
사실상 단일 프레임에 대한 입력을 차단하는 것과 같으며

1534
00:57:57,359 --> 00:58:00,119


1535
00:58:00,119 --> 00:58:03,240
네트워크가 해당

1536
00:58:03,240 --> 00:58:05,400
항목이 여전히 존재한다는 것을 인코딩하는지 확인하고 싶었습니다.  그 프레임이

1537
00:58:05,400 --> 00:58:07,680
없어진 후에도 신경 활동에서 물체의 존재를 여전히 해독할 수 있습니까?

1538
00:58:07,680 --> 00:58:10,020


1539
00:58:10,020 --> 00:58:11,819
그리고 이전과 약간 다른 위치에서 막대를 보았다는 사실 때문에 모션에 대해 추론하는 것은 무엇입니까?

1540
00:58:11,819 --> 00:58:13,920


1541
00:58:13,920 --> 00:58:15,780


1542
00:58:15,780 --> 00:58:18,480


1543
00:58:18,480 --> 00:58:20,520
프레임이 없어졌네요 음

1544
00:58:20,520 --> 00:58:22,559
그래서

1545
00:58:22,559 --> 00:58:25,200
네 확실히 여러 레벨인 것 같아요

1546
00:58:25,200 --> 00:58:27,240


1547
00:58:27,240 --> 00:58:29,160
음 일부는 훨씬

1548
00:58:29,160 --> 00:58:33,180
낮은 레벨일 수도 있고 어

1549
00:58:33,180 --> 00:58:35,880
아마도 장기적인 객체 영속성은

1550
00:58:35,880 --> 00:58:37,380
훨씬 더 높은 레벨일 것 같아요

1551
00:58:37,380 --> 00:58:39,059


1552
00:58:39,059 --> 00:58:39,900
음

1553
00:58:39,900 --> 00:58:41,760


1554
00:58:41,760 --> 00:58:44,640
고양이를 이용한 실험이 생각나네요  옛날에는

1555
00:58:44,640 --> 00:58:47,280


1556
00:58:47,280 --> 00:58:49,020
하루에 한 시간만 제외하고는 어둠 속에서 키우는 것 같았어요

1557
00:58:49,020 --> 00:58:51,000
수직 세계나 수평 세계에 넣거나

1558
00:58:51,000 --> 00:58:53,160
수평선

1559
00:58:53,160 --> 00:58:57,299
이나 수직선만 보거나 어 그리고

1560
00:58:57,299 --> 00:58:59,880
피질의 조직이

1561
00:58:59,880 --> 00:59:02,819
그들처럼 변하는 것을 볼 수 있습니다  수용력이 낮습니다. 이전에 수평선을

1562
00:59:02,819 --> 00:59:04,200
본 적이 없다면 수평선입니다.

1563
00:59:04,200 --> 00:59:06,420
그런 다음

1564
00:59:06,420 --> 00:59:07,980
막대기를 가져와 얼굴 앞에서 흔들고

1565
00:59:07,980 --> 00:59:09,599
막대기가

1566
00:59:09,599 --> 00:59:11,220
수평이면 아무 것도 하지 않고

1567
00:59:11,220 --> 00:59:12,900
수직입니다.

1568
00:59:12,900 --> 00:59:14,460
그들은 그것을 치려고 노력하고 있는데

1569
00:59:14,460 --> 00:59:15,900
말 그대로 얼굴 앞에서 바를 할 필요가 없는 것과 같기

1570
00:59:15,900 --> 00:59:18,420
때문에 그런 경우에는

1571
00:59:18,420 --> 00:59:20,700


1572
00:59:20,700 --> 00:59:24,260
낮은 수준의 결핍과 시력이

1573
00:59:24,260 --> 00:59:26,940
일종의 환상에 기여한다는 증거라고 생각합니다.

1574
00:59:26,940 --> 00:59:28,980
그래서 나는  예,

1575
00:59:28,980 --> 00:59:30,660
유아에게도 확실히 그런 측면이 있을 수 있다고 생각합니다. 그리고

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
당신이 제기한 매우 흥미로운 점 중 하나는

1578
00:59:36,420 --> 00:59:40,339
생명체와 무생물의

1579
00:59:40,339 --> 00:59:43,619
다양체이며, 작은 것들이

1580
00:59:43,619 --> 00:59:45,480
중간이라는 것입니다.

1581
00:59:45,480 --> 00:59:49,140
그게 무엇을 나타내는지,

1582
00:59:49,140 --> 00:59:52,319
아니면 다룰 수 있기 때문인가요?

1583
00:59:52,319 --> 00:59:55,619
곤충일 수도 있고, 바람에

1584
00:59:55,619 --> 00:59:57,839
의해서만 멀어질 수도 있는 것일 수도 있고, 그게 뭐라고

1585
00:59:57,839 --> 01:00:01,380
말하겠습니까?

1586
01:00:01,380 --> 01:00:04,380


1587
01:00:04,380 --> 01:00:08,280
그래 어 그럼 이건 Talia conkle 같은 사람의 작품입니다. 제

1588
01:00:08,280 --> 01:00:11,280
생각엔 이 조직을 발견한 사람이 그 사람

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
이고 그들은

1591
01:00:14,880 --> 01:00:16,440
그것을 알아내려고 노력한 것 같아요.  나는

1592
01:00:16,440 --> 01:00:19,500
이것을 잘못 이해하고 있지 않을 수도 있으므로 사람들이

1593
01:00:19,500 --> 01:00:21,359
그것을 삼자 조직이라고 부르면 그것에 대한 그녀의 작업을 읽어 보라고 권장합니다.

1594
01:00:21,359 --> 01:00:23,880
그러나 내가

1595
01:00:23,880 --> 01:00:25,319
올바르게 기억한다면

1596
01:00:25,319 --> 01:00:27,900
그들은 이 조직이 왜 존재하는지에 대한 많은 후속 작업을 수행했으며 이에

1597
01:00:27,900 --> 01:00:30,780


1598
01:00:30,780 --> 01:00:33,180
대한 몇 가지 증거가 있습니다.

1599
01:00:33,180 --> 01:00:35,700
이러한 물체의 곡률 및 물체에서

1600
01:00:35,700 --> 01:00:37,440
물체를 보는 거리와 같은 것 또는

1601
01:00:37,440 --> 01:00:40,260


1602
01:00:40,260 --> 01:00:43,319
음 물체에 애니메이션을 적용하거나 더 굴곡이 있거나

1603
01:00:43,319 --> 01:00:45,599


1604
01:00:45,599 --> 01:00:46,859
실제 대답이 무엇인지에 관계없이 존재합니다.

1605
01:00:46,859 --> 01:00:48,720


1606
01:00:48,720 --> 01:00:51,720
이러한 물체의 유사한 속성에서 비롯된 많은 다른 가설이 있었습니다.

1607
01:00:51,720 --> 01:00:54,000
어쩌면 중간 수준이나 낮은 수준의 속성이

1608
01:00:54,000 --> 01:00:56,280
더 높은 수준의 속성보다 더 많을 수도 있지만,

1609
01:00:56,280 --> 01:00:57,599


1610
01:00:57,599 --> 01:00:59,339


1611
01:00:59,339 --> 01:01:01,500
당신이 말한 것과 같은 개체와의 상호 작용이

1612
01:01:01,500 --> 01:01:04,920
분리를 일으키는 것인지, 아니면

1613
01:01:04,920 --> 01:01:06,119
음,

1614
01:01:06,119 --> 01:01:09,540
예 이러한 개체의 일반적인 모양이 정확히 해결되었는지는 아직 알 수 없습니다.

1615
01:01:09,540 --> 01:01:12,299
대부분의 경우 위

1616
01:01:12,299 --> 01:01:13,980
의 모든 항목을 조합한 것과 같다고 생각합니다.

1617
01:01:13,980 --> 01:01:16,980
하지만

1618
01:01:16,980 --> 01:01:18,480
이 모델링 관점에서 흥미로운 점은

1619
01:01:18,480 --> 01:01:19,859


1620
01:01:19,859 --> 01:01:21,480
음

1621
01:01:21,480 --> 01:01:24,059
이것이

1622
01:01:24,059 --> 01:01:26,819
이미지 데이터 세트

1623
01:01:26,819 --> 01:01:28,799
자체의 상관 통계에 대해서만 훈련되었기 때문에 상호 작용이 없다는 것입니다.

1624
01:01:28,799 --> 01:01:32,760
애니미시에 대한 개념이 없습니다 어 내 말은 이것은 단지

1625
01:01:32,760 --> 01:01:34,140


1626
01:01:34,140 --> 01:01:37,859
이미지넷에서 모델을 훈련하는 것일 뿐이라는 것입니다. 단지 개, 고양이, 보트의 이미지 등

1627
01:01:37,859 --> 01:01:40,020
무엇이든 간에 여전히 이러한

1628
01:01:40,020 --> 01:01:41,640
유형의 조직을 달성하므로 일종의

1629
01:01:41,640 --> 01:01:42,540


1630
01:01:42,540 --> 01:01:44,940
의미론적 특성이 있을 수 있습니다.

1631
01:01:44,940 --> 01:01:46,740


1632
01:01:46,740 --> 01:01:48,359


1633
01:01:48,359 --> 01:01:51,000
보트 대 개 대 20가지 다른 품종

1634
01:01:51,000 --> 01:01:53,640
의 개를 분류할 수 있는 네트워크이지만

1635
01:01:53,640 --> 01:01:55,920


1636
01:01:55,920 --> 01:01:57,900
낮은 수준의 마무리 통계

1637
01:01:57,900 --> 01:01:59,400
와도 어느 정도 일치할 수 있다면

1638
01:01:59,400 --> 01:02:02,780
그래 나도 모르겠어 그래

1639
01:02:03,960 --> 01:02:07,500
도발적인 비유는

1640
01:02:07,500 --> 01:02:10,380


1641
01:02:10,380 --> 01:02:12,900
필기에서 mnist의 번역 변화였다

1642
01:02:12,900 --> 01:02:14,819
인식 설정

1643
01:02:14,819 --> 01:02:18,000


1644
01:02:18,000 --> 01:02:20,160


1645
01:02:20,160 --> 01:02:22,740
오늘날 존재하는 변환 변화는 무엇입니까? 세 픽셀의

1646
01:02:22,740 --> 01:02:24,780
예는

1647
01:02:24,780 --> 01:02:27,540
llm이나 무언가에 대한 즉각적인 엔지니어링 공격입니다.

1648
01:02:27,540 --> 01:02:29,099
특수 문자가

1649
01:02:29,099 --> 01:02:32,640
삽입되거나 음 어

1650
01:02:32,640 --> 01:02:35,160
우리가 할 수 없는 이미지에 오버레이가 있는 것입니다.

1651
01:02:35,160 --> 01:02:37,020


1652
01:02:37,020 --> 01:02:39,359
그렇다면 그 도전 과제는 무엇

1653
01:02:39,359 --> 01:02:42,839
이며 우리가 그것을 추구할 수 있는 방법은 무엇이라고 생각하시나요?

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
예, 절대적으로 제 말은

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
제가 생각했던 방식이

1658
01:02:50,099 --> 01:02:52,680
대칭 변환과 같다고 생각합니다.

1659
01:02:52,680 --> 01:02:53,760
음

1660
01:02:53,760 --> 01:02:55,799
언어 모델에 대해 생각하고 있다면

1661
01:02:55,799 --> 01:02:57,420


1662
01:02:57,420 --> 01:02:58,500


1663
01:02:58,500 --> 01:03:00,240
단어를 동의어나 다른 것으로 바꾸는 것과 같은 대칭 변환을 상상할 수 있습니다.

1664
01:03:00,240 --> 01:03:03,780
어 당신이 우리에게 제시한 문장은

1665
01:03:03,780 --> 01:03:06,000
정확히 같은 것을 의미하지만 이제

1666
01:03:06,000 --> 01:03:07,380
갑자기 모델이 매우 다르게 반응할 것입니다.

1667
01:03:07,380 --> 01:03:09,299


1668
01:03:09,299 --> 01:03:11,240
음

1669
01:03:11,240 --> 01:03:15,359
언어 간의 번역처럼 이것은 다음

1670
01:03:15,359 --> 01:03:16,799
과 같이 볼 수 있습니다.  변환 유형은

1671
01:03:16,799 --> 01:03:19,440


1672
01:03:19,440 --> 01:03:21,960


1673
01:03:21,960 --> 01:03:24,900
우리에게 입력된 기본 의미를 유지하지만 모델에서는

1674
01:03:24,900 --> 01:03:26,220
완전히 다르게 보입니다. 우리는

1675
01:03:26,220 --> 01:03:28,380


1676
01:03:28,380 --> 01:03:29,940
이러한 유형의 변환과 관련하여 예측 가능한 방식으로 동작하는 모델을 갖고 싶습니다.

1677
01:03:29,940 --> 01:03:32,160


1678
01:03:32,160 --> 01:03:35,040
왜냐하면 인간은 매우 예측 가능하게 행동하기 때문입니다.

1679
01:03:35,040 --> 01:03:37,319
변형과

1680
01:03:37,319 --> 01:03:39,920
우리가 AI 시스템을 다룰 때 우리는 AI 시스템

1681
01:03:39,920 --> 01:03:43,200
도 그런 식으로 작동할 것으로 기대합니다. 이것이

1682
01:03:43,200 --> 01:03:45,480


1683
01:03:45,480 --> 01:03:47,339
이러한 시스템과 상호 작용하는 데 많은 문제를 일으키는 원인 중 하나라고 생각합니다.

1684
01:03:47,339 --> 01:03:49,460


1685
01:03:49,460 --> 01:03:52,500


1686
01:03:52,500 --> 01:03:54,960
곰과 사각형

1687
01:03:54,960 --> 01:03:58,440
같은 것들 음 우리는

1688
01:03:58,440 --> 01:04:00,480


1689
01:04:00,480 --> 01:04:02,220
이것이 이와 같은 간단한 일을 할 수 있을 것이라고 기대합니다. 왜냐하면 우리는 대부분의 인간이

1690
01:04:02,220 --> 01:04:04,020
할 수 있다고 생각하지만 아직은 그렇지 않기 때문입니다. 그리고

1691
01:04:04,020 --> 01:04:05,460
이것이

1692
01:04:05,460 --> 01:04:07,740
여러분이 이것을 기대하는 중요한 시나리오이고 그것은 큰

1693
01:04:07,740 --> 01:04:08,700
문제입니다

1694
01:04:08,700 --> 01:04:11,099
음  그건 어떻게 처리해야 할까요? 그게

1695
01:04:11,099 --> 01:04:12,720
제가

1696
01:04:12,720 --> 01:04:15,859
찾고 있는 것 같아요. 제가

1697
01:04:16,319 --> 01:04:18,420


1698
01:04:18,420 --> 01:04:22,280
취하는 방향은

1699
01:04:22,280 --> 01:04:26,940
좀 더 단순해 보이고,

1700
01:04:26,940 --> 01:04:29,460


1701
01:04:29,460 --> 01:04:31,380


1702
01:04:31,380 --> 01:04:33,180


1703
01:04:33,180 --> 01:04:35,760
이러한 새로운 결과를 낳는 신경망 아키텍처나 알고리즘의 상향식 빌딩 블록과 비슷해 보입니다.  구조적

1704
01:04:35,760 --> 01:04:37,680
특성과 저는 그것이

1705
01:04:37,680 --> 01:04:39,839


1706
01:04:39,839 --> 01:04:41,579
우리가 이미 가지고 있는 것 위에 무언가를 구축하는 것보다 훨씬 더 일반화 가능한 방법이라고 생각합니다.

1707
01:04:41,579 --> 01:04:43,140


1708
01:04:43,140 --> 01:04:43,920
음

1709
01:04:43,920 --> 01:04:45,839
제 생각에는 그것이 훨씬 더 확장성이

1710
01:04:45,839 --> 01:04:47,819
뛰어나고 또한 더 잘 일치할 것입니다.

1711
01:04:47,819 --> 01:04:50,299
뇌가 하는 일이

1712
01:04:50,760 --> 01:04:52,740
매우 멋지네요. 한 종류의 구현

1713
01:04:52,740 --> 01:04:54,740
질문이 무엇입니까?

1714
01:04:54,740 --> 01:04:57,000
이것을 실행하는 데 필요한 계산 요구 사항 또는

1715
01:04:57,000 --> 01:04:59,400


1716
01:04:59,400 --> 01:05:01,920


1717
01:05:01,920 --> 01:05:04,380
테라바이트급

1718
01:05:04,380 --> 01:05:07,020
데이터를 사용하고 대규모 계산을 사용하는 등 이러한 변형을 실행하는 학생 또는 연구원의 일상 생활은 무엇입니까?

1719
01:05:07,020 --> 01:05:08,940
아니면 사람들이 스스로 실행할 수 있는 것입니까?

1720
01:05:08,940 --> 01:05:11,880
노트북

1721
01:05:11,880 --> 01:05:13,980


1722
01:05:13,980 --> 01:05:17,099
오늘 제가 발표한 거의 모든 것이 로컬에서 실행될 수 있다고 생각합니다. 그래서

1723
01:05:17,099 --> 01:05:20,040
이 내용은 매우 간단하므로 실행할 수 있습니다. 즉,

1724
01:05:20,040 --> 01:05:20,760


1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299


1727
01:05:24,299 --> 01:05:25,980
기차를 좋아하고

1728
01:05:25,980 --> 01:05:27,420
앞으로 있을 다양한 일을 실험하고 싶다면 노트북에서 실행할 수 있다고 생각할 것입니다.

1729
01:05:27,420 --> 01:05:30,119
꽤 느리기 때문에

1730
01:05:30,119 --> 01:05:33,359


1731
01:05:33,359 --> 01:05:35,640
Nvidia 1080과 같은 거의 모든 것을 실행하는 꽤

1732
01:05:35,640 --> 01:05:38,819
오래되고 매우 저렴하지만

1733
01:05:38,819 --> 01:05:41,099
12GB RAM 등이 있고

1734
01:05:41,099 --> 01:05:43,140
이 모델에는 4GB RAM으로 충분합니다.

1735
01:05:43,140 --> 01:05:46,440


1736
01:05:46,440 --> 01:05:48,839
일부 사람들이 이상하다고 생각하는 것 중 하나는 제가 대부분

1737
01:05:48,839 --> 01:05:51,480
의 실험을 mnist와 같은 것에 대해 수행하므로

1738
01:05:51,480 --> 01:05:54,780
32 x 32 픽셀 이미지입니다. 왜냐하면 작게 음성으로 훈련할 수 있기 때문입니다. 음

1739
01:05:54,780 --> 01:05:57,299


1740
01:05:57,299 --> 01:06:00,000
원하신다면 제 실험은

1741
01:06:00,000 --> 01:06:02,460
끝이 없습니다.  이와 같은 작업을 하고 싶습니다.

1742
01:06:02,460 --> 01:06:03,540


1743
01:06:03,540 --> 01:06:05,640
이 해밀턴식 Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite는 훨씬 더 복잡합니다. 여기에서는

1745
01:06:08,160 --> 01:06:09,780
여러 GPU에서 실행되는 더 큰 모델을 사용하고 있으므로

1746
01:06:09,780 --> 01:06:12,240
여기서는 클러스터를 사용하여

1747
01:06:12,240 --> 01:06:14,220
이러한 유형의 모델을 실행합니다.

1748
01:06:14,220 --> 01:06:16,140
음. 하지만 대부분의 경우

1749
01:06:16,140 --> 01:06:18,920
GPU가 있는 단일 머신이면 충분하거나

1750
01:06:18,920 --> 01:06:21,680
심지어 공동 작업 노트북과 비슷합니다.

1751
01:06:21,680 --> 01:06:24,539


1752
01:06:24,539 --> 01:06:26,520
이미지넷에서 무언가를 훈련하려면 더 복잡해지고

1753
01:06:26,520 --> 01:06:29,940


1754
01:06:29,940 --> 01:06:33,660
이상적으로는 적어도 하나의 GPU가 더 필요하지만 예 저는

1755
01:06:33,660 --> 01:06:35,160
하지 않습니다.  엄청나게 많은 대규모 작업이

1756
01:06:35,160 --> 01:06:37,200
있지만 확실히 흥미롭고

1757
01:06:37,200 --> 01:06:39,960


1758
01:06:39,960 --> 01:06:42,539
거기에서 할 수 있는 일이 훨씬 더 많다고 생각합니다. 하지만 이런 종류

1759
01:06:42,539 --> 01:06:45,480
의 간단하거나 더 근본적인 질문에 대해서는

1760
01:06:45,480 --> 01:06:47,760
뭐라고 부르고 싶은지 모르겠습니다. 음 더

1761
01:06:47,760 --> 01:06:52,500
작은 기계는  훌륭하고 빠르네요 너무

1762
01:06:52,500 --> 01:06:54,660
멋지네요 유용해요

1763
01:06:54,660 --> 01:06:58,140
알겠습니다

1764
01:06:58,140 --> 01:07:00,780


1765
01:07:00,780 --> 01:07:02,880
적용된 능동 추론 심포지엄에서 정밀도가 많이 필요하지 않은

1766
01:07:02,880 --> 01:07:05,520


1767
01:07:05,520 --> 01:07:08,099
채집 또는 제어 상황에 더 적은 노력이나 ATP를 소비하는 것이 바람직하다는 것에 대한 Bert DeVries의 의견을 회상하는 Dave의 의견을 읽을 것입니다.

1768
01:07:08,099 --> 01:07:09,780


1769
01:07:09,780 --> 01:07:11,579


1770
01:07:11,579 --> 01:07:13,859
이 말을 듣는지는 모르겠지만 DeVries 교수는

1771
01:07:13,859 --> 01:07:17,520
가변 정밀도 모델과 이를

1772
01:07:17,520 --> 01:07:19,440
사용하여

1773
01:07:19,440 --> 01:07:21,059


1774
01:07:21,059 --> 01:07:23,039
일반화 및 실제 구조

1775
01:07:23,039 --> 01:07:25,020
과정 교육의 다양한 기능을 활성화하는 방법과

1776
01:07:25,020 --> 01:07:27,059
계산 요구 사항 감소 등을

1777
01:07:27,059 --> 01:07:29,880


1778
01:07:29,880 --> 01:07:31,980
소개하는 방법에 대해 제안했습니다.  능동 추론 이론으로의 구별

1779
01:07:31,980 --> 01:07:33,900
어떤 종류의

1780
01:07:33,900 --> 01:07:37,760
실험이 이것을 알아낼 수 있을까요? 오 와, 그래 그건

1781
01:07:38,400 --> 01:07:40,680
아닌 것 같아요.

1782
01:07:40,680 --> 01:07:42,660


1783
01:07:42,660 --> 01:07:46,619
그것에 대해 말할 만큼 지능이 부족한 것 같아요. 완전히 솔직해요.

1784
01:07:46,619 --> 01:07:48,740
음

1785
01:07:51,359 --> 01:07:53,460
정말 흥미로운 질문이군요.

1786
01:07:53,460 --> 01:07:56,220
직관이 다음을 만든다고 생각하기 때문이죠.

1787
01:07:56,220 --> 01:07:58,260
어

1788
01:07:58,260 --> 01:08:00,000
당신이 일반적으로 계산을 수행하는 모델에서 인코딩할 때 가변 정밀도 비율을 올바르게 이해하는지에 대해 말씀하시는 것 같습니다.

1789
01:08:00,000 --> 01:08:01,980


1790
01:08:01,980 --> 01:08:05,160


1791
01:08:05,160 --> 01:08:07,079


1792
01:08:07,079 --> 01:08:07,740


1793
01:08:07,740 --> 01:08:09,420
[음악]

1794
01:08:09,420 --> 01:08:11,539
음

1795
01:08:13,200 --> 01:08:17,040
그것은 어떻게든 미래 성능에 영향을 미칩니다.

1796
01:08:17,040 --> 01:08:19,080


1797
01:08:19,080 --> 01:08:22,259
내 생각에 일부 에너지 저장소와의 관계는 그렇습니다.

1798
01:08:22,259 --> 01:08:23,580
이것을 능동적인 노력 시스템으로 구축하려면

1799
01:08:23,580 --> 01:08:26,279


1800
01:08:26,279 --> 01:08:28,679


1801
01:08:28,679 --> 01:08:31,500
에이전트가 내부 에너지 저장소와 같은 에너지 개념을 갖고 있는 구체화된 시스템이 필요하며,

1802
01:08:31,500 --> 01:08:34,439


1803
01:08:34,439 --> 01:08:36,238
그렇습니다.

1804
01:08:36,238 --> 01:08:38,520


1805
01:08:38,520 --> 01:08:40,500
작업을 수행하는 동안 절약하려면 어

1806
01:08:40,500 --> 01:08:42,359
그리고 에너지가

1807
01:08:42,359 --> 01:08:44,759
부족하려면 에이전트에 나쁜 것이 필요할 것입니다.

1808
01:08:44,759 --> 01:08:47,399
그런 다음 일종의 출현을 관찰할 수 있습니다

1809
01:08:47,399 --> 01:08:48,679


1810
01:08:48,679 --> 01:08:52,040
어 감소 및

1811
01:08:52,040 --> 01:08:55,198
인코딩 정밀도 또는 에이전트가

1812
01:08:55,198 --> 01:08:57,479
배우려고 하는 것과 같은 것

1813
01:08:57,479 --> 01:09:00,060
좀 더 효율적으로 행동하려면 정밀도를

1814
01:09:00,060 --> 01:09:02,520
제어할 수 있는 능력을 주어야 할 수도 있습니다. 예,

1815
01:09:02,520 --> 01:09:04,020


1816
01:09:04,020 --> 01:09:07,080
제 전문 지식을 바탕으로 말씀드린 것처럼

1817
01:09:07,080 --> 01:09:08,819
생각이 좀 그렇습니다.

1818
01:09:08,819 --> 01:09:11,460
여기 이 슬라이드의 첫 번째 매우

1819
01:09:11,460 --> 01:09:14,219
멋진 이미지입니다.

1820
01:09:14,219 --> 01:09:18,359
디지털 잭슨 폴락과 비슷합니다.

1821
01:09:18,359 --> 01:09:22,920
음 그렇다면  입력 데이터

1822
01:09:22,920 --> 01:09:26,520
크기가 더 단순해지거나 패턴의 복잡성이 줄어들거나

1823
01:09:26,520 --> 01:09:27,719


1824
01:09:27,719 --> 01:09:30,000
복잡성이 증가하면 이 이미지가 어떻게 다르게 보일까요?

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
예 그래서 방향 열을 변경하려고 몇 가지 실험을 했는데

1827
01:09:34,620 --> 01:09:36,738


1828
01:09:36,738 --> 01:09:40,439


1829
01:09:40,439 --> 01:09:41,100


1830
01:09:41,100 --> 01:09:43,140
음 기본적으로

1831
01:09:43,140 --> 01:09:44,520
모델의 매개변수를 변경할 수 있습니다.

1832
01:09:44,520 --> 01:09:47,100
이 열을 더 크게 만들면 우리가 있는 인간에게서 볼 수 있는 것과

1833
01:09:47,100 --> 01:09:49,380
매우 유사한 구조를 가지지 않게 할 수 있습니다.

1834
01:09:49,380 --> 01:09:51,660


1835
01:09:51,660 --> 01:09:53,040
더 많은 활동 밴드를 갖도록 할 수 있습니다.

1836
01:09:53,040 --> 01:09:55,199


1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
그리고 말씀하신 것처럼

1839
01:09:58,199 --> 01:10:00,540
데이터 세트에 따라 다릅니다.  입력으로

1840
01:10:00,540 --> 01:10:03,179
정말 간단한 정현파 그레이딩을 사용하면 사용하고 있는 것입니다.

1841
01:10:03,179 --> 01:10:05,580
이와 같은 것을 얻습니다.

1842
01:10:05,580 --> 01:10:07,920
조금 더 많은 것을 얻습니다. 어

1843
01:10:07,920 --> 01:10:11,640
회전 곡선이 있고 엔트로피가 더 높습니다.

1844
01:10:11,640 --> 01:10:12,800
음

1845
01:10:12,800 --> 01:10:15,660
그래서 제 생각에는 이것들이 모두 흥미로운 것 같습니다.

1846
01:10:15,660 --> 01:10:18,000


1847
01:10:18,000 --> 01:10:19,320


1848
01:10:19,320 --> 01:10:22,199
자연계에 이런 유형의 조직이 있습니다.

1849
01:10:22,199 --> 01:10:24,120
이제 서로 다른 설정에 대해 서로 다른 조직을 생성하는 모델이 있다면

1850
01:10:24,120 --> 01:10:25,860


1851
01:10:25,860 --> 01:10:28,620
알겠습니다. 그러면

1852
01:10:28,620 --> 01:10:31,679
관찰된 데이터와 가장 잘 일치하는 설정이 무엇인지

1853
01:10:31,679 --> 01:10:32,340
음 그럼 그렇군요.

1854
01:10:32,340 --> 01:10:34,679


1855
01:10:34,679 --> 01:10:36,540
그렇다면 주변에 보내드릴 수 있습니다.

1856
01:10:36,540 --> 01:10:38,520
관심이 있지만 음

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
네 하나도 다른 것 같아요. 미안해요

1859
01:10:44,219 --> 01:10:45,659
하나도 다른 흥미로운 점은

1860
01:10:45,659 --> 01:10:46,920


1861
01:10:46,920 --> 01:10:51,480
어 다른 동물과 어

1862
01:10:51,480 --> 01:10:53,219
방향 선택성 유형, 그리고 다른

1863
01:10:53,219 --> 01:10:54,659
수의 바람개비 일부 동물은

1864
01:10:54,659 --> 01:10:57,420
전혀 가지고 있지 않다는 것입니다. 제가 생각한다면 아마도 생쥐일 것 같습니다.

1865
01:10:57,420 --> 01:11:00,120
맞습니다. 이런 종류의 어 그들은

1866
01:11:00,120 --> 01:11:01,800
소금과 후추 선택성이라고 부르기 때문에

1867
01:11:01,800 --> 01:11:03,480
기본적으로 무작위입니다.

1868
01:11:03,480 --> 01:11:04,679
지형적 방향 민감도와 같은 종류는 없습니다.

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
음 그래서 네,

1871
01:11:09,300 --> 01:11:10,920
다른 시스템이 이것을

1872
01:11:10,920 --> 01:11:13,020
다르게 한다는 증거가 있고, 그

1873
01:11:13,020 --> 01:11:14,760
이유를 알아내는 것은 흥미롭습니다.

1874
01:11:14,760 --> 01:11:17,760
네 이것이  매우 멋지네요.

1875
01:11:17,760 --> 01:11:21,300
먼저 반응 확산

1876
01:11:21,300 --> 01:11:22,980
기반과 시간이 생각나네요.

1877
01:11:22,980 --> 01:11:25,739
따라서 활성 영역이

1878
01:11:25,739 --> 01:11:30,000


1879
01:11:30,000 --> 01:11:32,840


1880
01:11:32,840 --> 01:11:35,640


1881
01:11:35,640 --> 01:11:39,360
fmri 공간 및 시간적 시간

1882
01:11:39,360 --> 01:11:40,520
척도에서 보이는 것처럼 특정 세분성에서 특정 영역에 활성이 없을 수도 있지만 실제로는 가능합니다.

1883
01:11:40,520 --> 01:11:44,699


1884
01:11:44,699 --> 01:11:46,380
활동 포켓이

1885
01:11:46,380 --> 01:11:48,480


1886
01:11:48,480 --> 01:11:52,260
해당 측정보다 느리고

1887
01:11:52,260 --> 01:11:54,060
소음과 다르지 않을 경우 모두

1888
01:11:54,060 --> 01:11:55,739
평균이 계산되므로

1889
01:11:55,739 --> 01:11:58,620


1890
01:11:58,620 --> 01:12:01,560


1891
01:12:01,560 --> 01:12:03,360
실제로 풍부함이 많은 데이터 세트와 같이 흥미로운 것이 있을 수 있습니다.  이런

1892
01:12:03,360 --> 01:12:06,179


1893
01:12:06,179 --> 01:12:08,520
저런 이유로 평균이 계산된 이유는

1894
01:12:08,520 --> 01:12:11,100
연결되지 않았기 때문이거나

1895
01:12:11,100 --> 01:12:12,420
이와 유사한 것입니다.

1896
01:12:12,420 --> 01:12:14,520
단일 시험 레벨을 사용해야 합니다.

1897
01:12:14,520 --> 01:12:16,140


1898
01:12:16,140 --> 01:12:18,719


1899
01:12:18,719 --> 01:12:23,100
마이크로 주파수를 충족한다는 것을 알 수 있을 정도로 공간 해상도가 충분히 높아야 합니다. uh  그리고 이것은

1900
01:12:23,100 --> 01:12:24,659
사람들이 오랫동안 하지 않았던 일입니다.

1901
01:12:24,659 --> 01:12:25,620
특히

1902
01:12:25,620 --> 01:12:27,780
단일 유권자 녹음을 하는 경우

1903
01:12:27,780 --> 01:12:28,920
진행파를 볼 수 없으며

1904
01:12:28,920 --> 01:12:30,900
진동을 볼 수 있으므로 음

1905
01:12:30,900 --> 01:12:32,219
다중 전기 같은 것이 필요합니다.

1906
01:12:32,219 --> 01:12:34,199
배열을 사용하고 기본적으로 그들은 괜찮다고 말하고 있습니다.

1907
01:12:34,199 --> 01:12:36,000
이제 우리는

1908
01:12:36,000 --> 01:12:37,520


1909
01:12:37,520 --> 01:12:40,080
이전에 볼 수 없었던 많은 지속성을 수행할 수 있는 기술을 보유하고 있으며

1910
01:12:40,080 --> 01:12:42,960
잠재적으로 이것은

1911
01:12:42,960 --> 01:12:44,400
우리가 이전에 보았던 많은 소음에 대한 설명일 수 있습니다.

1912
01:12:44,400 --> 01:12:46,260
어쩌면 실제로는 단지  이동하는

1913
01:12:46,260 --> 01:12:47,219
파도

1914
01:12:47,219 --> 01:12:47,760
음

1915
01:12:47,760 --> 01:12:51,480
그렇군요 앞으로는 녹음 능력이 향상되어 할 일이 많이 있을 것 같아요

1916
01:12:51,480 --> 01:12:53,880


1917
01:12:53,880 --> 01:12:56,520


1918
01:12:56,520 --> 01:12:58,739
정말 멋지군요

1919
01:12:58,739 --> 01:13:02,940
마지막 생각이나 질문이 있거나

1920
01:13:02,940 --> 01:13:06,239
이 작업을 어디로 할 건가요 아니면 저를 불러주셔서

1921
01:13:06,239 --> 01:13:08,520
감사합니다

1922
01:13:08,520 --> 01:13:10,140
음 음

1923
01:13:10,140 --> 01:13:11,640
활성 인프라에 있기를 바라겠습니다

1924
01:13:11,640 --> 01:13:14,520
그게 바로 그거야 난

1925
01:13:14,520 --> 01:13:16,560
정말 재미있을 것 같아 그래서 그래

1926
01:13:16,560 --> 01:13:18,980
잘 모르겠어 내가 지금 보고 있는 어쩌면 음악

1927
01:13:18,980 --> 01:13:21,659
uh 지금

1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760
음 음

1930
01:13:26,760 --> 01:13:30,420
다른 미친 방향을 보고 있어

1931
01:13:30,420 --> 01:13:33,020
너무 미친 소리를 하고 싶지는 않아 uh

1932
01:13:33,020 --> 01:13:36,900
하지만  내려갈게 네 많은 것들이 그래서

1933
01:13:36,900 --> 01:13:38,580


1934
01:13:38,580 --> 01:13:40,320
우리가 뉴로옵스에 제출한 것 중 하나는

1935
01:13:40,320 --> 01:13:43,140
진행하는 파동으로 기억을 연구하는 것입니다

1936
01:13:43,140 --> 01:13:45,060
음 그래서 오늘 막 아카이브에 종이가 나왔어요

1937
01:13:45,060 --> 01:13:46,860
어

1938
01:13:46,860 --> 01:13:48,840
파동이 장기 기억을 인코딩하는 데 정말 좋은가

1939
01:13:48,840 --> 01:13:50,580
제 생각에는

1940
01:13:50,580 --> 01:13:52,100
매우 흥미롭기

1941
01:13:52,100 --> 01:13:54,120
때문에 그 방향으로 조금 가보면 좋을 것

1942
01:13:54,120 --> 01:13:55,800


1943
01:13:55,800 --> 01:13:58,920
같습니다. 그렇습니다. 강아지의 발이 움직일

1944
01:13:58,920 --> 01:14:01,400


1945
01:14:01,400 --> 01:14:04,560
때에도 활성 상태를 유지하는 뉴런이 있을 때 행동이 일어나는 것을 보는 것은

1946
01:14:04,560 --> 01:14:07,620


1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
매우 흥미로울 것입니다.

1949
01:14:11,480 --> 01:14:14,280
야구공을 던지는 것과 같은 액션 시퀀스가

1950
01:14:14,280 --> 01:14:15,900


1951
01:14:15,900 --> 01:14:18,440
계속해서

1952
01:14:18,440 --> 01:14:21,179
영향을 미치고

1953
01:14:21,179 --> 01:14:23,699
대체 액션에 대한 깊은 시간적 표현을 갖는 것과 같은 액션에 대해 뭔가가 있는 것 같고

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
변형 오토인코더는

1956
01:14:29,640 --> 01:14:33,179
이미 기본적으로 그런 것에는 적합하므로

1957
01:14:33,179 --> 01:14:35,219


1958
01:14:35,219 --> 01:14:37,739
정말 감사합니다.  알았어

1959
01:14:37,739 --> 01:14:39,480
다음 번까지 고마워요

1960
01:14:39,480 --> 01:14:43,339
정말 고마워요 안녕


1
00:00:06,600 --> 00:00:09,420
bonjour et bienvenue, nous sommes le 18 septembre

2
00:00:09,420 --> 00:00:14,040
2023 et c'est le flux invité actif 57.1

3
00:00:14,040 --> 00:00:16,740
avec Andy Keller, nous allons

4
00:00:16,740 --> 00:00:19,619
parler de la structure neuronale naturelle

5
00:00:19,619 --> 00:00:22,020
pour l'intelligence artificielle, il y

6
00:00:22,020 --> 00:00:24,300
aura une présentation suivie d'une

7
00:00:24,300 --> 00:00:25,800
discussion, donc si vous regardez en direct,

8
00:00:25,800 --> 00:00:27,900
n'hésitez pas à  écrivez des questions dans

9
00:00:27,900 --> 00:00:30,660
le chat en direct sinon merci Andy

10
00:00:30,660 --> 00:00:32,279
pour cela, j'ai vraiment hâte

11
00:00:32,279 --> 00:00:35,899
et à vous pour la présentation

12
00:00:36,360 --> 00:00:38,940
ouais merci beaucoup euh merci de m'avoir invité,

13
00:00:38,940 --> 00:00:41,280
je suis super excité de pouvoir

14
00:00:41,280 --> 00:00:42,840
présenter ce truc avec le

15
00:00:42,840 --> 00:00:45,239
groupe de référence actif je  Je suis un fan et très

16
00:00:45,239 --> 00:00:49,200
intéressé, alors j'espère, euh ouais,

17
00:00:49,200 --> 00:00:50,399
avoir une bonne discussion et voir ce que

18
00:00:50,399 --> 00:00:51,960
vous en pensez,

19
00:00:51,960 --> 00:00:54,840
donc je m'appelle Andy, je termine

20
00:00:54,840 --> 00:00:57,180
mon doctorat supervisé par Maxwelling à l'

21
00:00:57,180 --> 00:00:59,100
Université d'Amsterdam,

22
00:00:59,100 --> 00:01:01,500
euh, je suis  je commence un postdoc à Harvard après

23
00:01:01,500 --> 00:01:05,099
cela, donc je vais commencer. Je parle juste

24
00:01:05,099 --> 00:01:07,619
du but de mon travail en général, c'est d'

25
00:01:07,619 --> 00:01:09,540
essayer de rapprocher l'intelligence artificielle moderne

26
00:01:09,540 --> 00:01:12,540
d'une généralisation plus humaine

27
00:01:12,540 --> 00:01:15,240
et donc ce que nous entendons par

28
00:01:15,240 --> 00:01:17,159
là est peut-être un peu  une sorte de généralisation de la structure,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
euh ou peut-être plus familière au

31
00:01:20,700 --> 00:01:22,080
comité des nourrissons actifs, comme un

32
00:01:22,080 --> 00:01:24,299
modèle mondial structuré que nous pensons que les humains

33
00:01:24,299 --> 00:01:26,340
ont et la façon dont nous proposons de le faire

34
00:01:26,340 --> 00:01:28,799
est d'intégrer la structure neuronale naturelle

35
00:01:28,799 --> 00:01:32,400
dans l'intelligence artificielle,

36
00:01:32,400 --> 00:01:34,979
alors définissons d'abord ce que nous voulons dire  par

37
00:01:34,979 --> 00:01:36,720
généralisation de la structure,

38
00:01:36,720 --> 00:01:38,400
donc je pense qu'il est assez peu controversé

39
00:01:38,400 --> 00:01:40,380
de dire que l'apprentissage automatique moderne

40
00:01:40,380 --> 00:01:42,900
se généralise au-delà de son ensemble de formation au

41
00:01:42,900 --> 00:01:45,000
sens traditionnel, donc par exemple,

42
00:01:45,000 --> 00:01:46,799
même les premiers réseaux de neurones artificiels, les

43
00:01:46,799 --> 00:01:49,020
perceptrons multicouches pourraient

44
00:01:49,020 --> 00:01:51,659
être formés sur des ensembles de données d'images comme

45
00:01:51,659 --> 00:01:55,140
celui-ci et atteindre un niveau élevé.  Ensuite, lorsqu'on

46
00:01:55,140 --> 00:01:56,880
leur présente un

47
00:01:56,880 --> 00:01:58,320
ensemble d'images de test qu'ils n'ont jamais vues

48
00:01:58,320 --> 00:02:00,240
auparavant, ils peuvent toujours les classer

49
00:02:00,240 --> 00:02:02,460
relativement facilement avec le même niveau de

50
00:02:02,460 --> 00:02:04,500
précision et c'est ce que nous

51
00:02:04,500 --> 00:02:07,320
appelons généralement la généralisation, même si c'était même assez

52
00:02:07,320 --> 00:02:08,758
tôt  remarqué que ces

53
00:02:08,758 --> 00:02:10,800
systèmes ont vraiment du mal avec les petits

54
00:02:10,800 --> 00:02:12,720
décalages ou déformations s'appliquent aux

55
00:02:12,720 --> 00:02:16,340
images par exemple si c'est le cas,

56
00:02:18,920 --> 00:02:23,099
vous pensez pourquoi est-ce surprenant et

57
00:02:23,099 --> 00:02:24,720
je soutiens que c'est vraiment dû à notre

58
00:02:24,720 --> 00:02:26,640
capacité innée à effectuer ce type de

59
00:02:26,640 --> 00:02:28,680
généralisation de structure que cet

60
00:02:28,680 --> 00:02:31,920
exemple est un échec de cette façon  euh par

61
00:02:31,920 --> 00:02:33,239
exemple ce changement nous est presque

62
00:02:33,239 --> 00:02:35,340
imperceptible et nous le gérons

63
00:02:35,340 --> 00:02:37,560
automatiquement alors que dans le système c'est

64
00:02:37,560 --> 00:02:39,959
très clairement un problème majeur donc en mots

65
00:02:39,959 --> 00:02:41,940
on peut dire que la généralisation de la structure

66
00:02:41,940 --> 00:02:44,819
est une généralisation à certaines

67
00:02:44,819 --> 00:02:47,040
transformations de symétrie de l'entrée ou dans ce

68
00:02:47,040 --> 00:02:48,959
cas la symétrie  La transformation est un

69
00:02:48,959 --> 00:02:50,580
petit changement qui laisse la classe de chiffres

70
00:02:50,580 --> 00:02:52,019
inchangée,

71
00:02:52,019 --> 00:02:54,599
donc la question évidente est alors de

72
00:02:54,599 --> 00:02:56,340
savoir ce que nous entendons précisément par cette

73
00:02:56,340 --> 00:02:58,860
structure naturelle et pourquoi pensons-nous que cela

74
00:02:58,860 --> 00:03:01,620
nous aiderait avec ces paramètres.

75
00:03:01,620 --> 00:03:03,780
Parlons donc d'abord de ce que nous entendons

76
00:03:03,780 --> 00:03:05,819
par neurone naturel.  structure

77
00:03:05,819 --> 00:03:08,700
euh, une façon de parler de structure ou de

78
00:03:08,700 --> 00:03:11,640
tout type de biais dans un système est un

79
00:03:11,640 --> 00:03:14,040
biais inductif et donc un biais inductif

80
00:03:14,040 --> 00:03:16,080
peut être défini en gros comme une

81
00:03:16,080 --> 00:03:17,940
restriction appropriée d'un ensemble d'

82
00:03:17,940 --> 00:03:19,440
hypothèses réalisables lorsque vous effectuez une

83
00:03:19,440 --> 00:03:22,440
sélection de modèle plus familièrement, nous  peut appeler

84
00:03:22,440 --> 00:03:24,480
cela quelque chose comme avant de voir des

85
00:03:24,480 --> 00:03:27,060
données, c'est une restriction de ce que

86
00:03:27,060 --> 00:03:29,580
vous pouvez apprendre et comment, de manière très large, cela peut

87
00:03:29,580 --> 00:03:32,519
inclure n'importe quoi, de la classe de modèle aux

88
00:03:32,519 --> 00:03:34,739
procédures d'optimisation ou même aux

89
00:03:34,739 --> 00:03:37,379
hyperparamètres et, dans un certain sens, ils

90
00:03:37,379 --> 00:03:39,739
définissent vraiment ce

91
00:03:39,739 --> 00:03:43,140
qu'il est possible d'apprendre  et cela définit

92
00:03:43,140 --> 00:03:44,819
la généralisation en ce sens que

93
00:03:44,819 --> 00:03:47,220
vous ne pouvez pas généraliser au-delà d'un

94
00:03:47,220 --> 00:03:48,420
ensemble de formation sans avoir une

95
00:03:48,420 --> 00:03:50,220
solution d'induction, cela est expliqué

96
00:03:50,220 --> 00:03:52,560
plus en détail dans cet article de David

97
00:03:52,560 --> 00:03:55,500
Wolford, donc ce que nous entendons par

98
00:03:55,500 --> 00:03:58,620
biais inductifs naturels, ce sont alors les biais qui

99
00:03:58,620 --> 00:04:00,000
découlent des restrictions et

100
00:04:00,000 --> 00:04:02,040
des limitations.  auxquels sont confrontés les

101
00:04:02,040 --> 00:04:04,500
systèmes naturels euh par la nature de devoir

102
00:04:04,500 --> 00:04:06,900
vivre dans le monde réel par exemple le

103
00:04:06,900 --> 00:04:08,459
cerveau a de nombreuses contraintes d'efficacité

104
00:04:08,459 --> 00:04:10,439
et contraintes physiques de par la nature de

105
00:04:10,439 --> 00:04:13,140
sa construction euh et suivant cette

106
00:04:13,140 --> 00:04:14,580
logique alors ces contraintes jouent vraiment

107
00:04:14,580 --> 00:04:16,620
un certain rôle dans notre généralisation

108
00:04:16,620 --> 00:04:19,798
des capacités qui dépassent actuellement l'

109
00:04:19,798 --> 00:04:21,478
intelligence artificielle moderne, comme nous le verrons

110
00:04:21,478 --> 00:04:24,720
ensuite, donc dans cet exposé, je me concentrerai

111
00:04:24,720 --> 00:04:27,540
spécifiquement sur deux types de structure

112
00:04:27,540 --> 00:04:29,880
que mon travail a étudié, à savoir

113
00:04:29,880 --> 00:04:31,699
l'organisation topographique et la

114
00:04:31,699 --> 00:04:34,320
dynamique spatio-temporelle et avant de me lancer

115
00:04:34,320 --> 00:04:36,120
dans mon travail, je '  Je vais donner un bref exemple expliquant

116
00:04:36,120 --> 00:04:38,759
pourquoi je pense que la structure naturelle

117
00:04:38,759 --> 00:04:41,400
peut être utile pour parvenir à la

118
00:04:41,400 --> 00:04:42,780
généralisation de la structure dont je parlais

119
00:04:42,780 --> 00:04:44,280
auparavant.

120
00:04:44,280 --> 00:04:47,479
Le premier exemple vient donc de l'architecture du

121
00:04:47,479 --> 00:04:49,199
front néocognitif de Hukushima

122
00:04:49,199 --> 00:04:51,479
des années 1980,

123
00:04:51,479 --> 00:04:53,520
qui a en fait été construite pour répondre

124
00:04:53,520 --> 00:04:55,440
directement à la question.  le problème de la

125
00:04:55,440 --> 00:04:57,300
robustesse à ces petits décalages et

126
00:04:57,300 --> 00:04:59,759
déformations, donc dans la paperasse, il

127
00:04:59,759 --> 00:05:02,040
écrit sur l'inspiration des

128
00:05:02,040 --> 00:05:04,380
mesures de hiérarchie et de mise en commun de l'élève et

129
00:05:04,380 --> 00:05:06,780
de la belette afin d'obtenir une robustesse

130
00:05:06,780 --> 00:05:08,699
à ces distorsions et donc si vous regardez

131
00:05:08,699 --> 00:05:11,160
la figure s'il écrit U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 et ceux-ci représentent des cellules simples et

133
00:05:14,100 --> 00:05:16,919
complexes et c'est donc une

134
00:05:16,919 --> 00:05:18,840
approche assez radicale à l'époque mais cela a

135
00:05:18,840 --> 00:05:20,699
vraiment servi à améliorer la robustesse et

136
00:05:20,699 --> 00:05:22,199
les changements dont nous tourmentons ces premiers

137
00:05:22,199 --> 00:05:24,419
réseaux de neurones artificiels et au fil du temps,

138
00:05:24,419 --> 00:05:25,860
ces idées ont été simplifiées et

139
00:05:25,860 --> 00:05:28,919
abstraites et  a évidemment donné naissance aux

140
00:05:28,919 --> 00:05:30,539
réseaux de neurones convolutifs, donc nous savons

141
00:05:30,539 --> 00:05:32,759
aujourd'hui ce qui a finalement conduit au succès

142
00:05:32,759 --> 00:05:35,580
de la révolution de l'apprentissage profond. C'est donc

143
00:05:35,580 --> 00:05:36,960
vraiment un exemple d'un

144
00:05:36,960 --> 00:05:39,479
biais inductif naturel qui a permis

145
00:05:39,479 --> 00:05:42,120
la généralisation de la structure, donc pour notre recherche, il est

146
00:05:42,120 --> 00:05:43,919
vraiment de la plus haute importance d'essayer de

147
00:05:43,919 --> 00:05:45,900
comprendre ce qui fait ces réseaux.  les modèles fonctionnent

148
00:05:45,900 --> 00:05:47,280
si bien

149
00:05:47,280 --> 00:05:49,320
euh et voyez si ce principe peut

150
00:05:49,320 --> 00:05:51,360
potentiellement être généralisé pour couvrir

151
00:05:51,360 --> 00:05:53,940
des transformations et des symétries plus abstraites et plus abstraites,

152
00:05:53,940 --> 00:05:57,000


153
00:05:57,000 --> 00:06:00,360
donc ce qui fait qu'une convolution atteint cette

154
00:06:00,360 --> 00:06:02,060
généralisation de structure

155
00:06:02,060 --> 00:06:04,440
intuitivement, vous pouvez voir que cela se fait en

156
00:06:04,440 --> 00:06:06,720
appliquant le même filtre à ou ou un

157
00:06:06,720 --> 00:06:08,699
extracteur de fonctionnalités à  divers

158
00:06:08,699 --> 00:06:10,680
emplacements spatiaux, donc ici nous voyons un seul

159
00:06:10,680 --> 00:06:12,660
filtre convolutif appliqué à

160
00:06:12,660 --> 00:06:14,820
tous les emplacements d'une image, cela signifie

161
00:06:14,820 --> 00:06:16,259
que peu importe où se trouve votre entrée, que ce soit

162
00:06:16,259 --> 00:06:18,000
au milieu de

163
00:06:18,000 --> 00:06:20,400
l'image ou à droite, vous aurez

164
00:06:20,400 --> 00:06:22,080
exactement les mêmes caractéristiques  à une

165
00:06:22,080 --> 00:06:23,580
exception près, ils seront décalés de manière équivalente,

166
00:06:23,580 --> 00:06:24,660


167
00:06:24,660 --> 00:06:26,819
donc mathématiquement ce type de mappage

168
00:06:26,819 --> 00:06:29,160
est appelé un homomorphisme, il préserve

169
00:06:29,160 --> 00:06:30,840
la structure algébrique de l'espace d'entrée

170
00:06:30,840 --> 00:06:33,180
et de l'espace de sortie dans ce cas,

171
00:06:33,180 --> 00:06:35,580
c'est par rapport à la traduction et à

172
00:06:35,580 --> 00:06:37,440
un niveau simple, quelque chose comme

173
00:06:37,440 --> 00:06:38,880
Ce qu'il sera important de retenir pour

174
00:06:38,880 --> 00:06:40,259
la suite de cet exposé, c'est que nous pouvons

175
00:06:40,259 --> 00:06:42,300
vérifier les homomorphismes de notre

176
00:06:42,300 --> 00:06:45,000
extracteur de caractéristiques si nous pouvons voir qu'il y a

177
00:06:45,000 --> 00:06:46,740
cette commutation de communauté avec le

178
00:06:46,740 --> 00:06:49,560
diagramme commutatif des Transformations et

179
00:06:49,560 --> 00:06:51,720
donc nous pouvons également écrire cela algébriquement

180
00:06:51,720 --> 00:06:53,759
en montrant que l'extracteur de caractéristiques  f fait

181
00:06:53,759 --> 00:06:55,080
la navette avec l'

182
00:06:55,080 --> 00:06:57,000
opérateur de transformation t

183
00:06:57,000 --> 00:06:58,919
et fondamentalement, ce que nous voulons, c'est qu'il

184
00:06:58,919 --> 00:07:00,600
n'y ait aucune différence entre d'abord

185
00:07:00,600 --> 00:07:02,639
extraire les fonctionnalités puis

186
00:07:02,639 --> 00:07:04,740
effectuer la transformation ou

187
00:07:04,740 --> 00:07:06,240
effectuer la transformation puis

188
00:07:06,240 --> 00:07:08,639
extraire les fonctionnalités, donc le défi

189
00:07:08,639 --> 00:07:10,199
est à ce jour que

190
00:07:10,199 --> 00:07:11,880
nous ne savons pas vraiment  comment construire

191
00:07:11,880 --> 00:07:13,620
des homomorphismes par rapport à

192
00:07:13,620 --> 00:07:15,240
des transformations plus complexes que nous voyons dans

193
00:07:15,240 --> 00:07:18,060
le monde réel, par exemple notre cerveau est

194
00:07:18,060 --> 00:07:20,099
capable de gérer naturellement les changements d'éclairage et

195
00:07:20,099 --> 00:07:22,259
de saison,

196
00:07:22,259 --> 00:07:24,599
donc ici nous voyons l'éclairage sur le visage d'une personne

197
00:07:24,599 --> 00:07:26,160
ou le changement de saisons, nous pouvons

198
00:07:26,160 --> 00:07:27,840
dire que c'est  le même visage ou la même

199
00:07:27,840 --> 00:07:29,880
route mais nous ne savons pas comment construire

200
00:07:29,880 --> 00:07:31,319
des modèles qui respectent ces

201
00:07:31,319 --> 00:07:33,180
Transformations et cela nous rend donc difficile

202
00:07:33,180 --> 00:07:35,520
de construire des systèmes qui les gèrent de

203
00:07:35,520 --> 00:07:37,620
manière robuste et prévisible

204
00:07:37,620 --> 00:07:40,259
pour donner un exemple encore plus abstrait de

205
00:07:40,259 --> 00:07:41,759
ce que je pense.  Je veux dire par là et les

206
00:07:41,759 --> 00:07:43,620
répercussions négatives potentielles des modèles qui

207
00:07:43,620 --> 00:07:45,440
ne gèrent pas la symétrie. Les transformations

208
00:07:45,440 --> 00:07:48,060
considèrent les programmes modernes de génération de texte en image.

209
00:07:48,060 --> 00:07:50,520
Dans cet exemple, j'ai demandé à

210
00:07:50,520 --> 00:07:53,940
Dolly de générer l'image d'un ours en

211
00:07:53,940 --> 00:07:55,620
peluche sur la lune et il le fait

212
00:07:55,620 --> 00:07:57,180
incroyablement bien.  probablement mieux

213
00:07:57,180 --> 00:08:00,960
que moi, il a une fourrure texturée,

214
00:08:00,960 --> 00:08:03,360
incroyablement détaillée, mais si je vous demande

215
00:08:03,360 --> 00:08:05,340
de faire quelque chose que je vois

216
00:08:05,340 --> 00:08:08,039
conceptuellement plus simple, comme dessiner un

217
00:08:08,039 --> 00:08:10,560
cube bleu au-dessus d'un cube rouge, il ne parvient pas à

218
00:08:10,560 --> 00:08:13,380
le faire et pour moi, cela ne semble pas intuitif

219
00:08:13,380 --> 00:08:15,300
puisque  la deuxième tâche semble

220
00:08:15,300 --> 00:08:18,180
beaucoup plus facile, mais ce que je

221
00:08:18,180 --> 00:08:19,860
dis, c'est que la raison pour laquelle cela est

222
00:08:19,860 --> 00:08:21,599
surprenant est précisément la même raison

223
00:08:21,599 --> 00:08:23,580
pour laquelle l'exemple de traduction amnest était

224
00:08:23,580 --> 00:08:25,560
surprenant : il y a cette

225
00:08:25,560 --> 00:08:28,020
transformation de symétrie qui se produit ici, à savoir la

226
00:08:28,020 --> 00:08:29,400
transformation entre ces

227
00:08:29,400 --> 00:08:31,740
objets complexes d'un ours en peluche.  et la lune et

228
00:08:31,740 --> 00:08:34,320
ces simples objets de Cubes que nous

229
00:08:34,320 --> 00:08:36,360
attendons intuitivement du réseau qu'il soit

230
00:08:36,360 --> 00:08:38,820
capable de manipuler et de respecter et nous voyons

231
00:08:38,820 --> 00:08:40,919
que ce n'est pas si simple que le

232
00:08:40,919 --> 00:08:43,380
travail de Fukushima a montré que ces

233
00:08:43,380 --> 00:08:46,200
structures naturelles de hiérarchie et de

234
00:08:46,200 --> 00:08:47,700
mutualisation de notre système visuel sont

235
00:08:47,700 --> 00:08:49,680
efficace pour faire des généralisations à de

236
00:08:49,680 --> 00:08:52,380
petites transformations. Je soutiens qu'une

237
00:08:52,380 --> 00:08:54,060
structure de niveau potentiellement supérieur peut

238
00:08:54,060 --> 00:08:55,740
être nécessaire pour résoudre ces

239
00:08:55,740 --> 00:08:58,200
problèmes de généralisation abstraits

240
00:08:58,200 --> 00:09:01,380
et donc la question que

241
00:09:01,380 --> 00:09:04,380
j'étudie et que je pose est de savoir quelle

242
00:09:04,380 --> 00:09:06,060
pourrait être cette structure et comment pouvons-nous la

243
00:09:06,060 --> 00:09:08,640
mettre en œuvre  ceci dans une

244
00:09:08,640 --> 00:09:10,080
architecture de réseau neuronal artificiel qui peut réellement

245
00:09:10,080 --> 00:09:14,120
être utilisée pour effectuer des calculs,

246
00:09:14,880 --> 00:09:17,700
donc pour commencer à répondre, je vais passer

247
00:09:17,700 --> 00:09:19,680
à ma première ligne de travail sur l'

248
00:09:19,680 --> 00:09:22,380
organisation topographique

249
00:09:22,380 --> 00:09:25,260
afin que l'organisation topographique soit

250
00:09:25,260 --> 00:09:27,060
largement observée dans tout le cerveau à partir des

251
00:09:27,060 --> 00:09:29,760
zones de niveau saphir du cortex visuel primaire  et

252
00:09:29,760 --> 00:09:31,500
on peut très vaguement décrire cette

253
00:09:31,500 --> 00:09:33,540
propriété selon laquelle les neurones proches les

254
00:09:33,540 --> 00:09:35,760
uns des autres ont tendance à répondre à des

255
00:09:35,760 --> 00:09:38,220
choses similaires. Par exemple, à gauche, nous montrons

256
00:09:38,220 --> 00:09:39,720
la préférence codée par couleur de chaque

257
00:09:39,720 --> 00:09:42,959
neurone du cortex numérique primaire en

258
00:09:42,959 --> 00:09:45,360
réponse à des lignes orientées.  et nous voyons

259
00:09:45,360 --> 00:09:46,740
cet ensemble de sélectivités qui varie en douceur.

260
00:09:46,740 --> 00:09:48,779
Un autre type d'

261
00:09:48,779 --> 00:09:50,580
organisation est connu sous le nom d'organisation thématique de la rétine,

262
00:09:50,580 --> 00:09:52,560
dans laquelle les neurones proches du

263
00:09:52,560 --> 00:09:54,600
cortex visuel ont tendance à répondre aux

264
00:09:54,600 --> 00:09:56,399
champs récepteurs proches.

265
00:09:56,399 --> 00:09:58,560
Toutefois, cette organisation ne se limite pas

266
00:09:58,560 --> 00:10:01,080
à ces fonctionnalités de bas niveau et s'étend à d'autres

267
00:10:01,080 --> 00:10:02,519
plus complexes.  des caractéristiques telles que celles

268
00:10:02,519 --> 00:10:05,459
présentes dans les visages, les objets ou les lieux

269
00:10:05,459 --> 00:10:07,920
et cela concerne les

270
00:10:07,920 --> 00:10:10,080
zones dites fonctionnellement spécifiques du cerveau

271
00:10:10,080 --> 00:10:12,779
telles que la zone fusiforme du visage FFA et

272
00:10:12,779 --> 00:10:15,420
la zone parakhippocampique du visage PPA

273
00:10:15,420 --> 00:10:19,200
donc dans ce travail, l'idée principale est encore une fois que

274
00:10:19,200 --> 00:10:21,300
peut-être ceci

275
00:10:21,300 --> 00:10:23,580
organisation topographique dans un certain sens qui est

276
00:10:23,580 --> 00:10:25,080
intimement liée à l'

277
00:10:25,080 --> 00:10:27,980
opération de convolution et à l'architecture de Fukushima,

278
00:10:27,980 --> 00:10:30,660
nous pouvons peut-être en généraliser les avantages

279
00:10:30,660 --> 00:10:33,420
à des transformations plus abstraites, en

280
00:10:33,420 --> 00:10:34,920
d'autres termes, apprendre à construire des

281
00:10:34,920 --> 00:10:36,839
homomorphismes plus complexes que nous ne pouvons pas faire,

282
00:10:36,839 --> 00:10:38,940
vous savez, nous ne pouvons pas  faites analytiquement maintenant,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
juste pour montrer que nous ne sommes pas

285
00:10:42,480 --> 00:10:44,760
complètement fous avec cette idée, euh, il

286
00:10:44,760 --> 00:10:46,320
y a des travaux antérieurs dans ce domaine

287
00:10:46,320 --> 00:10:49,880
de la part de personnes comme euh Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden au début des années 90 et 2000

289
00:10:54,060 --> 00:10:55,800
et ils ont étudié comment l'

290
00:10:55,800 --> 00:10:57,720
organisation topographique peut être  utile pour apprendre les

291
00:10:57,720 --> 00:11:01,320
variances principalement dans les modèles linéaires, donc

292
00:11:01,320 --> 00:11:03,060
la question qui se pose à nous lorsque nous sommes entrés dans l'

293
00:11:03,060 --> 00:11:04,680
espace est de savoir quel est le

294
00:11:04,680 --> 00:11:07,140
mécanisme abstrait le plus évolutif qui peut être exploité

295
00:11:07,140 --> 00:11:08,880
à partir de ces approches que nous pouvons

296
00:11:08,880 --> 00:11:10,800
intégrer dans les architectures modernes de réseaux neuronaux profonds

297
00:11:10,800 --> 00:11:12,959
et finalement nous avons

298
00:11:12,959 --> 00:11:15,000
opté pour un  approche de modélisation générative

299
00:11:15,000 --> 00:11:16,260
qui, je pense, pourrait être

300
00:11:16,260 --> 00:11:17,519
intéressante pour les membres de cette

301
00:11:17,519 --> 00:11:18,779
communauté,

302
00:11:18,779 --> 00:11:21,779
euh, qui nous permet ensuite de la relier

303
00:11:21,779 --> 00:11:23,579
plus étroitement à l'

304
00:11:23,579 --> 00:11:26,040
analyse des composants topographiques indépendants, l'idée de base

305
00:11:26,040 --> 00:11:28,320
étant que nous pouvons apprendre une

306
00:11:28,320 --> 00:11:30,660
caractéristique topographique dans l'espace en imposant une

307
00:11:30,660 --> 00:11:32,940
distribution topographique préalable sur  nos variables latentes

308
00:11:32,940 --> 00:11:34,440


309
00:11:34,440 --> 00:11:37,079
donc juste pour donner un bref aperçu, je

310
00:11:37,079 --> 00:11:39,120
suppose que la plupart des gens sont déjà familiers

311
00:11:39,120 --> 00:11:40,440
avec cela,

312
00:11:40,440 --> 00:11:42,540
euh, mais le genre d'hypothèse générale est

313
00:11:42,540 --> 00:11:44,339
que le cerveau est un modèle génératif et

314
00:11:44,339 --> 00:11:45,720
cette idée dans un certain sens peut être

315
00:11:45,720 --> 00:11:48,060
attribuée aux casques du 19ème

316
00:11:48,060 --> 00:11:50,459
siècle euh  où il a dit que ce que nous

317
00:11:50,459 --> 00:11:52,140
voyons est la solution à un

318
00:11:52,140 --> 00:11:54,420
problème informatique, notre cerveau calcule les causes les plus

319
00:11:54,420 --> 00:11:56,519
probables des absorptions de photons

320
00:11:56,519 --> 00:11:59,519
dans nos yeux et c'est donc un exemple

321
00:11:59,519 --> 00:12:01,920
si je vous montre cette image, vous

322
00:12:01,920 --> 00:12:03,720
la reconnaissez immédiatement comme un globe avec une certaine

323
00:12:03,720 --> 00:12:05,760
courbure, mais elle  pourrait tout aussi

324
00:12:05,760 --> 00:12:07,620
bien être un disque avec une

325
00:12:07,620 --> 00:12:09,600
perspective déformée, c'est ainsi que nous

326
00:12:09,600 --> 00:12:12,660
obtenons des illusions d'optique ou nos images,

327
00:12:12,660 --> 00:12:14,880
comme celle-ci, votre cerveau en déduit qu'il

328
00:12:14,880 --> 00:12:17,100
y a un cube ici à cause de la

329
00:12:17,100 --> 00:12:18,720
structure, mais en réalité, ce n'est qu'un

330
00:12:18,720 --> 00:12:19,860
morceau de papier plat

331
00:12:19,860 --> 00:12:22,740
vous pouvez donc penser à cet

332
00:12:22,740 --> 00:12:24,480
aspect du modèle génératif qui ressemble un peu à un

333
00:12:24,480 --> 00:12:26,160
programme graphique inverse

334
00:12:26,160 --> 00:12:28,140
dans le programme. Les propriétés abstraites

335
00:12:28,140 --> 00:12:30,660
de la sphère sont connues, la position, la

336
00:12:30,660 --> 00:12:32,820
taille de l'éclairage et celles-ci sont utilisées pour

337
00:12:32,820 --> 00:12:34,560
projeter la sphère afin de créer l'

338
00:12:34,560 --> 00:12:37,440
image 2D qui est rendue.  donc en fait, ce que

339
00:12:37,440 --> 00:12:40,019
disent Humboldt et d'autres, c'est qu'en

340
00:12:40,019 --> 00:12:41,940
tant que modèle génératif, le cerveau

341
00:12:41,940 --> 00:12:43,680
essaie en fait d'inverser ce

342
00:12:43,680 --> 00:12:45,959
processus génératif et de faire des inférences

343
00:12:45,959 --> 00:12:48,300
et de déduire les causes sous-jacentes de nos

344
00:12:48,300 --> 00:12:49,680
sensations.

345
00:12:49,680 --> 00:12:51,600
La raison pour laquelle j'insiste en quelque sorte sur

346
00:12:51,600 --> 00:12:53,100
ce point est qu'il y a

347
00:12:53,100 --> 00:12:55,440
on parle beaucoup de modèles génératifs aujourd'hui,

348
00:12:55,440 --> 00:12:57,180
euh et je ne parle pas nécessairement seulement

349
00:12:57,180 --> 00:12:59,639
de générer des images ou de jolies

350
00:12:59,639 --> 00:13:01,260
images,

351
00:13:01,260 --> 00:13:03,120
euh, je veux vraiment parler d'

352
00:13:03,120 --> 00:13:07,920
un cadre pour l'apprentissage non supervisé,

353
00:13:07,920 --> 00:13:10,019
alors pour entrer un peu plus dans

354
00:13:10,019 --> 00:13:11,700
les détails, qu'est-ce que j'entends par un

355
00:13:11,700 --> 00:13:14,279
a priori topographique, donc les modèles génératifs

356
00:13:14,279 --> 00:13:16,019
sont généralement décrits comme une

357
00:13:16,019 --> 00:13:18,720
distribution conjointe sur les observations X et

358
00:13:18,720 --> 00:13:21,720
les variables latentes que nous appellerons Z euh

359
00:13:21,720 --> 00:13:23,940
et cela est généralement factorisé ou une

360
00:13:23,940 --> 00:13:25,620
façon de procéder est de factoriser en

361
00:13:25,620 --> 00:13:28,440
termes d'un P a priori de Z et cela est vrai

362
00:13:28,440 --> 00:13:30,420
modèle génératif modèle génératif conditionnel

363
00:13:30,420 --> 00:13:33,420
P de x étant donné Z et donc une façon d'

364
00:13:33,420 --> 00:13:34,980
y penser est que l'

365
00:13:34,980 --> 00:13:37,019
a priori peut être vu comme codant

366
00:13:37,019 --> 00:13:38,880
des pénalités relatives pour chaque type de code qui est

367
00:13:38,880 --> 00:13:41,279
produit lorsque nous inversons notre

368
00:13:41,279 --> 00:13:42,899
modèle génératif, c'est ce qu'on appelle le calcul du

369
00:13:42,899 --> 00:13:45,920
P postérieur de Z étant donné X

370
00:13:45,920 --> 00:13:49,139
et ainsi pour développer un espace topographique latent,

371
00:13:49,139 --> 00:13:50,639
nous voulons introduire une sorte d'

372
00:13:50,639 --> 00:13:53,279
a priori topographique qui a été ou

373
00:13:53,279 --> 00:13:55,620
dont ce travail topographique de l'ICA a montré

374
00:13:55,620 --> 00:13:57,720
est équivalent à quelque chose comme une pénalité de rareté de groupe

375
00:13:57,720 --> 00:13:59,700


376
00:13:59,700 --> 00:14:01,500
afin que les gens puissent être familiers avec typique

377
00:14:01,500 --> 00:14:03,180
pénalités de rareté issues de

378
00:14:03,180 --> 00:14:04,560
l'analyse de l'apprentissage indépendant, vous voulez que vos

379
00:14:04,560 --> 00:14:06,540
activations soient clairsemées, ce qui signifie que beaucoup d'entre

380
00:14:06,540 --> 00:14:09,420
elles sont nulles et cela pourrait donc ressembler à

381
00:14:09,420 --> 00:14:10,680
ceci, vous avez un tas de

382
00:14:10,680 --> 00:14:12,300
carrés bleus qui sont actifs mais la plupart d'

383
00:14:12,300 --> 00:14:14,459
entre eux ne sont pas actifs mais spécifiquement

384
00:14:14,459 --> 00:14:16,740
avec le groupe  Pénalité universitaire, nous voulons que

385
00:14:16,740 --> 00:14:18,839
ces priorités attribuent une probabilité plus faible

386
00:14:18,839 --> 00:14:21,600
à ces activations dispersées distribuées

387
00:14:21,600 --> 00:14:24,720
et une probabilité plus élevée à ces

388
00:14:24,720 --> 00:14:26,940
représentations groupées densément emballées. Vous pouvez

389
00:14:26,940 --> 00:14:28,860
également considérer cela comme une pénalité plus élevée

390
00:14:28,860 --> 00:14:30,720
lorsque les choses sont réparties, une pénalité plus faible

391
00:14:30,720 --> 00:14:33,540
lorsque les choses sont plus rapprochées,

392
00:14:33,540 --> 00:14:36,380
donc encore une fois euh  cela peut être écrit

393
00:14:36,380 --> 00:14:39,060
de manière abstraite comme ceci mais je veux faire une

394
00:14:39,060 --> 00:14:41,160
théorie selon laquelle ces neurones, chacun de

395
00:14:41,160 --> 00:14:42,779
ces carrés ici représente une sorte de

396
00:14:42,779 --> 00:14:44,220
neurone dans notre modèle et ils sont

397
00:14:44,220 --> 00:14:46,560
organisés dans cette grille 2D donc quand nous

398
00:14:46,560 --> 00:14:48,120
parlons de regroupement, nous voulons vraiment dire

399
00:14:48,120 --> 00:14:50,820
regroupés dans cette topologie 2D,

400
00:14:50,820 --> 00:14:53,100
donc une chose qui est vraiment intéressante

401
00:14:53,100 --> 00:14:55,560
et assez importante est que ces

402
00:14:55,560 --> 00:14:57,779
priorités ne nous donnent pas seulement une

403
00:14:57,779 --> 00:15:00,540
organisation topographique, mais elles ont également été notées

404
00:15:00,540 --> 00:15:02,459
ou étudiées par des personnes comme

405
00:15:02,459 --> 00:15:05,760
erosi Marcelli et Bruno pour

406
00:15:05,760 --> 00:15:07,740
s'adapter également  les statistiques des

407
00:15:07,740 --> 00:15:08,959
données naturelles sont meilleures,

408
00:15:08,959 --> 00:15:11,180
en particulier les images naturelles,

409
00:15:11,180 --> 00:15:14,100
ils ont montré qu'en utilisant ce type d'a

410
00:15:14,100 --> 00:15:16,139
priori, vous obtenez en fait un ensemble d'activations plus clairsemé, ce qui

411
00:15:16,139 --> 00:15:18,839
signifie que l'a priori correspond

412
00:15:18,839 --> 00:15:20,459
un peu mieux au véritable processus génératif

413
00:15:20,459 --> 00:15:22,620
et, comme nous le savons, le cerveau a

414
00:15:22,620 --> 00:15:24,779
un degré élevé de parcimonie et cela est

415
00:15:24,779 --> 00:15:26,339
considéré comme très pertinent pour

416
00:15:26,339 --> 00:15:28,620
l'efficacité,

417
00:15:28,620 --> 00:15:30,839
donc pour entrer un peu plus dans les

418
00:15:30,839 --> 00:15:32,760
détails pour implémenter ce type de

419
00:15:32,760 --> 00:15:35,160
groupe clairsemé avant d'utiliser un

420
00:15:35,160 --> 00:15:37,320
modèle génératif hiérarchique et ceci est

421
00:15:37,320 --> 00:15:39,060
essentiellement introduit par certains des

422
00:15:39,060 --> 00:15:41,339
travail topographique de l'ICA,

423
00:15:41,339 --> 00:15:43,320
l'idée est que vous avez une

424
00:15:43,320 --> 00:15:45,000
variable latente de niveau supérieur U qui

425
00:15:45,000 --> 00:15:47,820
régule simultanément la variance de

426
00:15:47,820 --> 00:15:50,279
plusieurs variables de niveau inférieur T et

427
00:15:50,279 --> 00:15:52,440
c'est ainsi que nous obtenons la rareté du groupe,

428
00:15:52,440 --> 00:15:55,440
puis pour obtenir une organisation topographique, vous

429
00:15:55,440 --> 00:15:56,760
pouvez utiliser légèrement plusieurs de ces

430
00:15:56,760 --> 00:15:59,339
variables latentes  se chevauchant avec

431
00:15:59,339 --> 00:16:02,519
leurs champs d'influence afin que

432
00:16:02,519 --> 00:16:04,260
nous puissions les appeler leurs quartiers

433
00:16:04,260 --> 00:16:05,699
et cela vous donnera cette

434
00:16:05,699 --> 00:16:07,440
structure de corrélation fluide de votre acte que vous

435
00:16:07,440 --> 00:16:09,899
recherchez, alors obtenez l'intuition pour cela, vous

436
00:16:09,899 --> 00:16:12,060
voyez que cette variable T en

437
00:16:12,060 --> 00:16:14,279
bas ici n'obtient pas  n'importe quelle entrée

438
00:16:14,279 --> 00:16:17,160
de ce U en haut mais il partage

439
00:16:17,160 --> 00:16:19,139
une variable u avec ce T au milieu

440
00:16:19,139 --> 00:16:21,300
donc c'est comme s'ils partageaient des variantes,

441
00:16:21,300 --> 00:16:22,980
ils partageaient certains composants avec

442
00:16:22,980 --> 00:16:24,959
leurs voisins mais pas tous les composants

443
00:16:24,959 --> 00:16:26,579
et c'est vraiment dû à cette

444
00:16:26,579 --> 00:16:28,079
connectivité locale de  ces variables de niveau supérieur,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
pour rester simple sur la façon dont nous utilisons un

447
00:16:33,180 --> 00:16:34,980
modèle génératif, revenons à une

448
00:16:34,980 --> 00:16:37,440
seule variable U et le défi dans

449
00:16:37,440 --> 00:16:38,940
ce type d'architecture qui a rendu

450
00:16:38,940 --> 00:16:42,360
la tâche difficile pendant de nombreuses années est de savoir comment

451
00:16:42,360 --> 00:16:44,579
déduire le postérieur approximatif

452
00:16:44,579 --> 00:16:47,579
sur ces variables intermédiaires dans

453
00:16:47,579 --> 00:16:50,100
cette architecture hiérarchique et ce

454
00:16:50,100 --> 00:16:52,560
n'est pas très simple, donc

455
00:16:52,560 --> 00:16:54,420
des travaux antérieurs ont utilisé des heuristiques développées pour

456
00:16:54,420 --> 00:16:56,699
les modèles linéaires et dans notre travail, nous avons constaté

457
00:16:56,699 --> 00:16:58,680
que cela ne s'étendait pas vraiment aux

458
00:16:58,680 --> 00:17:01,199
architectures de réseaux neuronaux modernes, donc

459
00:17:01,199 --> 00:17:02,880
notre idée est vraiment de tirer parti d'un

460
00:17:02,880 --> 00:17:04,760
factorisation une

461
00:17:04,760 --> 00:17:07,640
reparamétrisation spécifique de cette distribution

462
00:17:07,640 --> 00:17:10,380
et donc ce paramétrage

463
00:17:10,380 --> 00:17:12,419
spécifiquement est obtenu en définissant la

464
00:17:12,419 --> 00:17:14,579
priorité ce qu'on appelle un

465
00:17:14,579 --> 00:17:16,319
mélange d'échelles gaussiennes, ce qui signifie que notre

466
00:17:16,319 --> 00:17:19,140
distribution conditionnelle de T étant donné U est

467
00:17:19,140 --> 00:17:21,179
en fait une distribution normale où

468
00:17:21,179 --> 00:17:24,299
la variance est définie par cette variable

469
00:17:24,299 --> 00:17:27,720
U et pour certains choix de U cette

470
00:17:27,720 --> 00:17:29,340
distribution est en effet clairsemée et

471
00:17:29,340 --> 00:17:31,980
englobe une gamme de distributions

472
00:17:31,980 --> 00:17:33,780
telles que les distributions laplossiennes a suit et T

473
00:17:33,780 --> 00:17:36,299
une façon de la définir est

474
00:17:36,299 --> 00:17:38,940
un mélange d'échelle gaussienne émet un

475
00:17:38,940 --> 00:17:40,919
recadrage particulier

476
00:17:40,919 --> 00:17:42,900
en termes de variables aléatoires gaussiennes indépendantes

477
00:17:42,900 --> 00:17:45,720
Z et U donc spécifiquement,

478
00:17:45,720 --> 00:17:48,840
nous voyons que cette variable T qui était à

479
00:17:48,840 --> 00:17:50,760
l'origine assez complexe n'est en fait

480
00:17:50,760 --> 00:17:52,799
qu'un produit d'un tas de

481
00:17:52,799 --> 00:17:54,840
variables aléatoires gaussiennes qui savent maintenant comment

482
00:17:54,840 --> 00:17:57,660
travailler avec beaucoup plus efficacement euh dans les

483
00:17:57,660 --> 00:18:00,120
modèles génératifs spécifiquement ce que

484
00:18:00,120 --> 00:18:02,039
nous sommes  ce que nous allons faire, c'est que nous puissions

485
00:18:02,039 --> 00:18:04,020
réellement obtenir des postérieurs approximatifs pour

486
00:18:04,020 --> 00:18:06,720
U et Z séparément, puis en faire une

487
00:18:06,720 --> 00:18:08,640
combinaison déterministe

488
00:18:08,640 --> 00:18:10,020
afin de calculer notre

489
00:18:10,020 --> 00:18:13,140
variable topographique T et c'est beaucoup plus facile à faire

490
00:18:13,140 --> 00:18:15,500
sans entrer trop dans les détails

491
00:18:15,500 --> 00:18:17,700
de la méthode qui  nous avons décidé d'utiliser ce que l'on

492
00:18:17,700 --> 00:18:18,600
appelle un

493
00:18:18,600 --> 00:18:20,640
auto-encodeur variationnel qui exploite les techniques de

494
00:18:20,640 --> 00:18:23,220
l'inférence variationnelle pour dériver une

495
00:18:23,220 --> 00:18:24,780
limite inférieure de la probabilité

496
00:18:24,780 --> 00:18:26,940
nous permettant de paramétrer ces

497
00:18:26,940 --> 00:18:29,400
postérieurs approximatifs avec de puissants réseaux neuronaux profonds non linéaires

498
00:18:29,400 --> 00:18:30,960
et de les optimiser avec une

499
00:18:30,960 --> 00:18:33,240
descente de gradient.  être

500
00:18:33,240 --> 00:18:34,440
familier à la communauté d'inférence active,

501
00:18:34,440 --> 00:18:36,900
mais en réalité, ce que nous avons fait, c'est

502
00:18:36,900 --> 00:18:38,760
qu'au lieu d'avoir un seul encodeur dans

503
00:18:38,760 --> 00:18:41,640
le décodeur comme un baes typique, nous avons maintenant

504
00:18:41,640 --> 00:18:43,799
deux encodeurs, un pour vous et un pour Z

505
00:18:43,799 --> 00:18:46,200
séparément, puis nous les combinons de

506
00:18:46,200 --> 00:18:48,419
cette manière déterministe pour  construisons

507
00:18:48,419 --> 00:18:51,660
notre variable topographique T si vous voyez

508
00:18:51,660 --> 00:18:53,100
qu'il s'agit en fait de la

509
00:18:53,100 --> 00:18:54,900
construction de la distribution T d'un étudiant

510
00:18:54,900 --> 00:18:57,620
à partir de gaussiennes

511
00:18:57,620 --> 00:19:00,480
et ensuite nous pouvons brancher cela, nous faisons cela

512
00:19:00,480 --> 00:19:03,600
avant le décodage et euh et ensuite maximiser

513
00:19:03,600 --> 00:19:05,820
la probabilité des données, donc

514
00:19:05,820 --> 00:19:07,260
c'est le coude  les preuves de la

515
00:19:07,260 --> 00:19:09,360
limite inférieure abondent sur la probabilité des

516
00:19:09,360 --> 00:19:12,600
données et sont en fait très similaires à l'

517
00:19:12,600 --> 00:19:15,720
énergie libre variationnelle qui est utilisée dans la

518
00:19:15,720 --> 00:19:18,299
communauté d'entrée active,

519
00:19:18,299 --> 00:19:20,520
donc avec ces détails à l'écart,

520
00:19:20,520 --> 00:19:22,500
ce qui est vraiment intéressant, c'est ce qui

521
00:19:22,500 --> 00:19:23,940
se passe lorsque nous formons ce

522
00:19:23,940 --> 00:19:26,580
modèle génératif qui  a une pénalité de parcimonie de groupe relativement simple

523
00:19:26,580 --> 00:19:29,460
dans son espace latent et

524
00:19:29,460 --> 00:19:30,720
nous voulons examiner ce qu'il

525
00:19:30,720 --> 00:19:32,700
apprend en termes d'organisation des

526
00:19:32,700 --> 00:19:34,980
Futures et nous commençons d'abord par l'ensemble de

527
00:19:34,980 --> 00:19:36,480
données le plus simple possible, nous avons un

528
00:19:36,480 --> 00:19:38,580
fond noir avec des carrés blancs à des

529
00:19:38,580 --> 00:19:41,280
emplacements XY aléatoires  et si nous entraînons notre

530
00:19:41,280 --> 00:19:42,780
auto-encodeur avec cette pénalité de parcimonie de groupe

531
00:19:42,780 --> 00:19:44,760
et que nous regardons ensuite les

532
00:19:44,760 --> 00:19:47,640
vecteurs de poids de notre décodeur que

533
00:19:47,640 --> 00:19:49,020
nous traçons ici encore en bleu

534
00:19:49,020 --> 00:19:52,520
organisés sur cette grille 2D, nous voyons qu'en

535
00:19:52,520 --> 00:19:54,780
effet ils apprennent à s'organiser

536
00:19:54,780 --> 00:19:57,539
en fonction de l'espace.  emplacement, donc cela

537
00:19:57,539 --> 00:19:59,580
peut être considéré comme similaire aux

538
00:19:59,580 --> 00:20:01,799
champs récepteurs convolutifs ou le champ récepteur

539
00:20:01,799 --> 00:20:04,679
de chaque neurone est vraiment donné par

540
00:20:04,679 --> 00:20:09,059
le type d'entrées à son emplacement

541
00:20:09,059 --> 00:20:10,860
et cela a du sens intuitivement du

542
00:20:10,860 --> 00:20:13,140
point de vue de la parcimonie du groupe puisque

543
00:20:13,140 --> 00:20:15,480
pour une région donnée, mettre en évidence comme

544
00:20:15,480 --> 00:20:17,700
dans  jaune ici, les filtres d'un

545
00:20:17,700 --> 00:20:19,260
groupe donné sont beaucoup plus fortement corrélés,

546
00:20:19,260 --> 00:20:20,880
ils ont ces

547
00:20:20,880 --> 00:20:23,460
champs récepteurs qui se chevauchent que d'autres emplacements aléatoires, donc

548
00:20:23,460 --> 00:20:25,020
essentiellement nous voyons que notre modèle

549
00:20:25,020 --> 00:20:27,840
apprend à regrouper les activités d'activité

550
00:20:27,840 --> 00:20:28,980
ensemble,

551
00:20:28,980 --> 00:20:32,059
euh dans une sorte de feuille verticale simulée

552
00:20:32,059 --> 00:20:34,260
en fonction des corrélations dans  les

553
00:20:34,260 --> 00:20:36,840
données sont définies de manière à ce qu'au lieu d'une convolution

554
00:20:36,840 --> 00:20:38,580
où vous effectuez réellement une liaison de poids

555
00:20:38,580 --> 00:20:40,860
et que vous spécifiez manuellement Je veux

556
00:20:40,860 --> 00:20:42,539
copier ce poids partout, vous pouvez

557
00:20:42,539 --> 00:20:44,220
peut-être considérer cela comme un

558
00:20:44,220 --> 00:20:45,500
temps d'attente approximatif

559
00:20:45,500 --> 00:20:48,120
et nous apprenons vraiment cela de la

560
00:20:48,120 --> 00:20:49,620
corrélation  structure de l'ensemble de données

561
00:20:49,620 --> 00:20:51,660
lui-même et juste pour donner un peu

562
00:20:51,660 --> 00:20:54,120
plus d'inspiration biologique à

563
00:20:54,120 --> 00:20:56,280
cela et nous savons que la rétinotopie est

564
00:20:56,280 --> 00:20:58,020
présente dans le cerveau, c'est un exemple

565
00:20:58,020 --> 00:21:02,460
de rétinotopie dans le cortex visuel et

566
00:21:02,460 --> 00:21:04,500
vous pouvez voir si vous montrez au macaque un

567
00:21:04,500 --> 00:21:06,780
image comme celle-ci, elle est projetée dans

568
00:21:06,780 --> 00:21:08,520
cette

569
00:21:08,520 --> 00:21:11,700
topologie en préservant l'espace réellement à

570
00:21:11,700 --> 00:21:13,740
la surface du cortex,

571
00:21:13,740 --> 00:21:16,080
donc l'idée est que l'

572
00:21:16,080 --> 00:21:18,419
organisation topographique et même l'apprentissage de l'

573
00:21:18,419 --> 00:21:21,299
organisation topographique préservent les

574
00:21:21,299 --> 00:21:26,160
corrélations d'entrée de nos ensembles de données euh et et

575
00:21:26,160 --> 00:21:28,679
potentiellement euh cela peut être bénéfique

576
00:21:28,679 --> 00:21:30,840
pour  généraliser ces idées un peu plus

577
00:21:30,840 --> 00:21:32,340
loin, donc comme je l'ai dit au

578
00:21:32,340 --> 00:21:34,679
début, ce serait encore mieux si

579
00:21:34,679 --> 00:21:37,200
nous pouvions simplement apprendre quelque chose de plus que de la

580
00:21:37,200 --> 00:21:39,320
simple convolution, peut-être des équivariances plus compliquées,

581
00:21:39,320 --> 00:21:43,679
alors comment pouvons-nous faire cette

582
00:21:43,679 --> 00:21:45,720
chose qui est claire dans l'

583
00:21:45,720 --> 00:21:48,299
intelligence naturelle, c'est que nous  n'existons pas dans

584
00:21:48,299 --> 00:21:51,120
ce monde de cadres IID, nous existons

585
00:21:51,120 --> 00:21:53,520
dans un monde avec des séquences continues de

586
00:21:53,520 --> 00:21:55,620
transformations, alors peut-être pouvons-nous étendre

587
00:21:55,620 --> 00:21:58,440
notre modèle à ce paramètre pour apprendre

588
00:21:58,440 --> 00:22:01,080
à observer les transformations, c'est une idée

589
00:22:01,080 --> 00:22:03,299
de cohérence temporelle,

590
00:22:03,299 --> 00:22:05,280
alors que se passerait-il si nous simplement

591
00:22:05,280 --> 00:22:08,280
étendu notre cadre précédent au fil du

592
00:22:08,280 --> 00:22:10,620
temps Dimension correctement, donc au lieu de simplement

593
00:22:10,620 --> 00:22:13,080
regrouper en disant que nous voulons que nos neurones

594
00:22:13,080 --> 00:22:15,059
soient des groupes clairsemés en termes d'

595
00:22:15,059 --> 00:22:17,400
étendue spatiale sur le cortex, nous voulons en fait qu'ils

596
00:22:17,400 --> 00:22:18,960
soient des groupes clairsemés au fil du temps, ce qui

597
00:22:18,960 --> 00:22:20,640
signifie que si un ensemble de neurones est

598
00:22:20,640 --> 00:22:22,559
actif  maintenant, nous voulons que ce même ensemble de

599
00:22:22,559 --> 00:22:24,360
neurones soit également actif dans le futur.

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
Si nous regardons si nous

602
00:22:27,840 --> 00:22:30,600
y réfléchissons intuitivement, nous voyons qu'il s'agit

603
00:22:30,600 --> 00:22:33,059
en fait d'une invariance et d'une équivariance plus encourageantes.

604
00:22:33,059 --> 00:22:35,039
Une façon de comprendre cela est que

605
00:22:35,039 --> 00:22:37,140
nous disons que nous  Je veux que les mêmes neurones

606
00:22:37,140 --> 00:22:39,179
soient actifs en permanence mais la transformation d'entrée

607
00:22:39,179 --> 00:22:41,280
change à droite, les

608
00:22:41,280 --> 00:22:44,220
pieds de ce petit renard bougent donc si

609
00:22:44,220 --> 00:22:45,960
les mêmes neurones codent pour la même

610
00:22:45,960 --> 00:22:47,880
chose encore et encore mais que les pieds

611
00:22:47,880 --> 00:22:49,320
bougent, ces neurones vont

612
00:22:49,320 --> 00:22:51,360
apprendre  être invariant au mouvement de

613
00:22:51,360 --> 00:22:53,880
cette patte de ce chien par exemple

614
00:22:53,880 --> 00:22:57,539
donc à la place c'est que oups

615
00:22:57,539 --> 00:23:01,200
j'ai fait fausse route ici euh

616
00:23:01,200 --> 00:23:04,860
donc à la place euh notre idée était que ce

617
00:23:04,860 --> 00:23:06,659
groupe commençait à être pourrait plutôt être

618
00:23:06,659 --> 00:23:09,059
décalé par rapport au temps donc cela

619
00:23:09,059 --> 00:23:10,980
signifierait  que des

620
00:23:10,980 --> 00:23:13,080
ensembles d'activations séquentiellement décalés seraient encouragés

621
00:23:13,080 --> 00:23:15,179
à s'activer ensemble et que notre

622
00:23:15,179 --> 00:23:16,440
espace latent serait alors vraiment structuré par

623
00:23:16,440 --> 00:23:18,000
rapport aux transformations observées,

624
00:23:18,000 --> 00:23:19,980
vous pouvez donc voir ici qu'au lieu que le

625
00:23:19,980 --> 00:23:21,480
même ensemble de neurones soit actif à tout

626
00:23:21,480 --> 00:23:23,340
moment, il s'agit en réalité d'un processus séquentiel.  ensemble

627
00:23:23,340 --> 00:23:24,900
permuté de neurones que nous

628
00:23:24,900 --> 00:23:27,780
regroupons de cette manière clairsemée euh

629
00:23:27,780 --> 00:23:29,940
et puis cela nous permet de modéliser

630
00:23:29,940 --> 00:23:33,419
différentes observations au fil du temps mais

631
00:23:33,419 --> 00:23:34,860
ils sont toujours connectés en termes d'

632
00:23:34,860 --> 00:23:36,960
apprentissage d'une transformation et de préservation

633
00:23:36,960 --> 00:23:38,340
de cette structure de corrélation de l'

634
00:23:38,340 --> 00:23:40,020
empathie

635
00:23:40,020 --> 00:23:41,940
donc si nous  mettez cela ensemble dans notre

636
00:23:41,940 --> 00:23:44,400
architecture topographique Bae, vous pouvez obtenir

637
00:23:44,400 --> 00:23:46,020
quelque chose qui ressemble à ceci, vous voyez

638
00:23:46,020 --> 00:23:48,120
que nous avons une séquence d'entrée, nous

639
00:23:48,120 --> 00:23:51,240
codons à nouveau une variable z, puis

640
00:23:51,240 --> 00:23:53,520
plusieurs variables U dans le dénominateur

641
00:23:53,520 --> 00:23:55,740
ici, puis chacune de ces

642
00:23:55,740 --> 00:23:58,620
variables U est décalée  euh, un peu comme nous le

643
00:23:58,620 --> 00:24:00,480
montrions auparavant, afin d'obtenir

644
00:24:00,480 --> 00:24:02,820
cette structure d'équivariance de déplacement

645
00:24:02,820 --> 00:24:04,740
que nous recherchons lorsque nous

646
00:24:04,740 --> 00:24:07,080
les combinons dans cette distribution de produits étudiant T,

647
00:24:07,080 --> 00:24:09,240
nous obtenons une seule

648
00:24:09,240 --> 00:24:10,740
variable latente, c'est maintenant notre

649
00:24:10,740 --> 00:24:13,860
variable topographique T et maintenant que nous  avons cette

650
00:24:13,860 --> 00:24:16,140
structure connue dans notre espace latent vous

651
00:24:16,140 --> 00:24:17,460
pouvez la penser comme un modèle du Monde structuré

652
00:24:17,460 --> 00:24:19,919
nous savons comment transformer cet

653
00:24:19,919 --> 00:24:21,659
espace latent dans ce cas c'est en

654
00:24:21,659 --> 00:24:23,580
permutant ces activations autour de ces

655
00:24:23,580 --> 00:24:25,860
cercles faisant comme un rôle cyclique un

656
00:24:25,860 --> 00:24:28,380
changement cyclique nous savons que c'est  va

657
00:24:28,380 --> 00:24:30,120
correspondre à nos transformations d'entrée apprises

658
00:24:30,120 --> 00:24:32,640
et nous pouvons vérifier cela

659
00:24:32,640 --> 00:24:34,620
en disant ok et si je continue cette

660
00:24:34,620 --> 00:24:36,480
transformation d'entrée la vraie

661
00:24:36,480 --> 00:24:38,100
transformation dans l'ensemble de données qui est

662
00:24:38,100 --> 00:24:40,559
une rotation et ensuite je compare cela avec

663
00:24:40,559 --> 00:24:42,659
la façon dont j'ai joué mon rôle à mon retour  L'espace

664
00:24:42,659 --> 00:24:44,700
en déplaçant mes activations dans mon

665
00:24:44,700 --> 00:24:47,280
cerveau puis nous décodons et nous voyons que

666
00:24:47,280 --> 00:24:49,919
nous obtenons exactement la même chose et donc cela

667
00:24:49,919 --> 00:24:52,140
démontre cette

668
00:24:52,140 --> 00:24:53,580
propriété de commutabilité dont je parlais auparavant

669
00:24:53,580 --> 00:24:56,820
pour vérifier l'homomorphisme

670
00:24:56,820 --> 00:24:58,799
et donc pour mesurer cela un peu plus de

671
00:24:58,799 --> 00:25:02,460
qualité  quantitativement, euh, nous pouvons mesurer

672
00:25:02,460 --> 00:25:04,440
ce qu'on appelle une perte d'équivariance, donc

673
00:25:04,440 --> 00:25:07,080
c'est vraiment la quantification de

674
00:25:07,080 --> 00:25:09,360
cette différence entre

675
00:25:09,360 --> 00:25:12,120
l'activation de notre capsule roulée ou le roulement dans notre

676
00:25:12,120 --> 00:25:15,059
tête et le fait de regarder le roulement se dérouler

677
00:25:15,059 --> 00:25:16,559
et en avant, ils regardent la

678
00:25:16,559 --> 00:25:19,440
transformation se dérouler devant nous donc nous

679
00:25:19,440 --> 00:25:21,600
voyons la topographie  Bae atteint une

680
00:25:21,600 --> 00:25:24,000


681
00:25:24,000 --> 00:25:26,700
erreur d'équivariance significativement plus faible. Cette bulle vae est ce dont je

682
00:25:26,700 --> 00:25:27,960
parlais auparavant où elle apprend

683
00:25:27,960 --> 00:25:29,820
l'invariance, donc elle n'a pas d'

684
00:25:29,820 --> 00:25:32,340
opération de décalage et la vae traditionnelle n'a en quelque sorte

685
00:25:32,340 --> 00:25:35,640
aucune notion d'organisation ou de

686
00:25:35,640 --> 00:25:37,380
composante temporelle, donc des performances très

687
00:25:37,380 --> 00:25:40,320
médiocres en plus  à cela, nous voyons que

688
00:25:40,320 --> 00:25:41,700
le modèle est un meilleur modèle génératif

689
00:25:41,700 --> 00:25:45,059
de séquences, il obtient juste une

690
00:25:45,059 --> 00:25:48,179
probabilité plus faible, comme un log négatif, sur

691
00:25:48,179 --> 00:25:50,100
l'ensemble de données, il est donc mieux capable de

692
00:25:50,100 --> 00:25:51,720
modéliser cet ensemble de données car il a une

693
00:25:51,720 --> 00:25:52,919
notion de la structure du

694
00:25:52,919 --> 00:25:55,460
transformations

695
00:25:55,980 --> 00:25:58,140
euh, nous pouvons tester cela sur plusieurs

696
00:25:58,140 --> 00:25:59,760
types de transformation différents et sur la

697
00:25:59,760 --> 00:26:00,840
rangée du haut, nous montrons la vraie

698
00:26:00,840 --> 00:26:02,880
transformation, nous avons extrait ces

699
00:26:02,880 --> 00:26:05,039
images grisées, puis sur la

700
00:26:05,039 --> 00:26:07,080
rangée du bas, nous encodons, puis nous

701
00:26:07,080 --> 00:26:08,700
déroulons simplement nos activations et nous gardons

702
00:26:08,700 --> 00:26:12,140
décodage pour voir ce que le modèle

703
00:26:12,140 --> 00:26:15,000
a appris comme

704
00:26:15,000 --> 00:26:17,039
transformation actuelle observée et

705
00:26:17,039 --> 00:26:19,340
nous voyons qu'il peut fondamentalement

706
00:26:19,340 --> 00:26:21,360
reconstruire parfaitement ces éléments de la

707
00:26:21,360 --> 00:26:23,640
séquence qu'il n'a jamais vus auparavant,

708
00:26:23,640 --> 00:26:25,260
en plus avec des images provenant de

709
00:26:25,260 --> 00:26:26,580
l'ensemble de test qu'il n'a jamais vu  avant,

710
00:26:26,580 --> 00:26:28,380
simplement parce qu'il sait quelle est la

711
00:26:28,380 --> 00:26:29,760
transformation qu'il est en train de

712
00:26:29,760 --> 00:26:31,500
coder, il peut généraliser cela à de nouveaux

713
00:26:31,500 --> 00:26:33,919
exemples,

714
00:26:34,020 --> 00:26:36,360
donc ce que nous retenons de cette partie est vraiment une

715
00:26:36,360 --> 00:26:38,039
organisation topographique. Nous avons montré qu'une

716
00:26:38,039 --> 00:26:40,080
structure d'entrée préservée et maintenant

717
00:26:40,080 --> 00:26:41,940
nous montrons qu'elle peut potentiellement améliorer l'

718
00:26:41,940 --> 00:26:44,279
efficacité et la généralisation.  comme on l'

719
00:26:44,279 --> 00:26:46,200
espérait euh

720
00:26:46,200 --> 00:26:48,600
finalement quelque chose qui nous a surpris

721
00:26:48,600 --> 00:26:49,980
et que j'ai pensé être potentiellement le plus

722
00:26:49,980 --> 00:26:52,500
intéressant c'est que ces

723
00:26:52,500 --> 00:26:53,700
Transformations qui sont apprises par notre

724
00:26:53,700 --> 00:26:54,960
modèle généralisent en fait les

725
00:26:54,960 --> 00:26:57,059
combinaisons de Transformations que l'

726
00:26:57,059 --> 00:26:59,580
on ne voit pas pendant l'entraînement donc par

727
00:26:59,580 --> 00:27:02,100
exemple malgré seulement l'entraînement  sur les

728
00:27:02,100 --> 00:27:04,200
transformations de couleur et de rotation et

729
00:27:04,200 --> 00:27:06,419
l'isolement si le modèle est présenté avec

730
00:27:06,419 --> 00:27:08,340
une transformation de rotation de couleur combinée

731
00:27:08,340 --> 00:27:11,100
au moment du test euh nous voyons qu'il est capable

732
00:27:11,100 --> 00:27:13,140
de modéliser complètement et de compléter

733
00:27:13,140 --> 00:27:14,700
parfaitement ces transformations à travers le

734
00:27:14,700 --> 00:27:17,159
rôle de capsule, ce qui implique qu'il a appris

735
00:27:17,159 --> 00:27:19,620
à factoriser et à représenter ces

736
00:27:19,620 --> 00:27:20,880
différentes  Les transformations et cela peut

737
00:27:20,880 --> 00:27:24,600
les combiner de manière flexible au moment de l'inférence,

738
00:27:24,600 --> 00:27:28,140
donc encore une fois, peut-être que nous n'obtenons pas seulement

739
00:27:28,140 --> 00:27:29,820
officiellement l'efficacité dans la généralisation,

740
00:27:29,820 --> 00:27:34,100
nous obtenons également une compositionnalité de base,

741
00:27:34,260 --> 00:27:36,059
alors parlons des limitations et de

742
00:27:36,059 --> 00:27:38,460
ce que nous pourrions faire ensuite, euh, la principale

743
00:27:38,460 --> 00:27:40,620
limitation est qu'il y a un

744
00:27:40,620 --> 00:27:44,159
transformation prédéfinie que nous imposons euh à la

745
00:27:44,159 --> 00:27:46,500
fois dans l'espace et dans le temps, donc même si nous nous sommes libérés des

746
00:27:46,500 --> 00:27:49,080
transformations de groupe et

747
00:27:49,080 --> 00:27:52,440
spécifiquement comme la traduction ou

748
00:27:52,440 --> 00:27:53,940
la rotation comme cela se fait actuellement dans le

749
00:27:53,940 --> 00:27:55,559
monde de l'apprentissage automatique,

750
00:27:55,559 --> 00:27:59,240
nous avons toujours ce

751
00:27:59,240 --> 00:28:01,980
rôle latent codé en dur dans notre  se dirige vers tout ce que

752
00:28:01,980 --> 00:28:03,900
nous voyons et pour rendre cela un peu

753
00:28:03,900 --> 00:28:05,700
plus flexible afin que nous puissions, espérons-le, modéliser

754
00:28:05,700 --> 00:28:08,880
une plus grande diversité de transformations.

755
00:28:08,880 --> 00:28:10,980
Euh, nous pensons que nous pouvons peut-être nous

756
00:28:10,980 --> 00:28:13,860
inspirer de dynamiques spatio-temporelles plus structurées

757
00:28:13,860 --> 00:28:15,600
qui sont observées dans

758
00:28:15,600 --> 00:28:18,120
le cerveau et donc cela nous amène  à

759
00:28:18,120 --> 00:28:20,400
la deuxième partie de cet exposé qui concerne la

760
00:28:20,400 --> 00:28:22,140
dynamique spatio-temporelle que nous

761
00:28:22,140 --> 00:28:23,039
allons essayer d'intégrer dans

762
00:28:23,039 --> 00:28:25,200
des réseaux de neurones artificiels, un exemple

763
00:28:25,200 --> 00:28:27,059
en est les ondes progressives comme je l'ai montré

764
00:28:27,059 --> 00:28:28,020
ici,

765
00:28:28,020 --> 00:28:30,600
alors qu'entendons-nous par là, euh,

766
00:28:30,600 --> 00:28:32,279
voici un très  article récent dans lequel ils ont

767
00:28:32,279 --> 00:28:36,059
utilisé un fmri de neuf Tesla fonctionnant à une

768
00:28:36,059 --> 00:28:38,700
résolution de 36 millisecondes pour imager une seule

769
00:28:38,700 --> 00:28:40,980
tranche de cerveau de rat sous anesthésie

770
00:28:40,980 --> 00:28:43,320
et ce que nous voyons, c'est cette

771
00:28:43,320 --> 00:28:45,720
activité spatio-temporelle très clairement structurée et ces

772
00:28:45,720 --> 00:28:48,299
corrélations et ces auteurs de l'

773
00:28:48,299 --> 00:28:50,520
article analysent ensuite cela  activité en

774
00:28:50,520 --> 00:28:52,919
termes des modes principaux représentés

775
00:28:52,919 --> 00:28:55,799
à droite, notre hypothèse est donc qu'une

776
00:28:55,799 --> 00:28:57,179
sorte de

777
00:28:57,179 --> 00:28:59,039
structure de corrélation comme celle-ci pourrait être bénéfique

778
00:28:59,039 --> 00:29:01,260
pour structurer les représentations de

779
00:29:01,260 --> 00:29:03,240
notre modèle par rapport aux

780
00:29:03,240 --> 00:29:05,100
transformations observées, mais d'une manière beaucoup plus

781
00:29:05,100 --> 00:29:07,440
flexible que simplement  juste un

782
00:29:07,440 --> 00:29:10,700
changement cyclique comme nous le faisions avant

783
00:29:11,279 --> 00:29:12,419
euh

784
00:29:12,419 --> 00:29:15,900
et laissez-moi vous dire que cela n'est pas seulement

785
00:29:15,900 --> 00:29:19,320
observé chez les rats d'un ssi euh,

786
00:29:19,320 --> 00:29:20,940
vous pouvez voir ces ondes progressives

787
00:29:20,940 --> 00:29:24,179
se produire dans le cortex Mt des

788
00:29:24,179 --> 00:29:27,600
primates éveillés, euh donc par exemple sur

789
00:29:27,600 --> 00:29:29,580
la gauche  ici, ils montrent des ondes progressives

790
00:29:29,580 --> 00:29:31,740
qui

791
00:29:31,740 --> 00:29:35,520
changent réellement. Quelle est la probabilité qu'un primate voie un

792
00:29:35,520 --> 00:29:38,279
stimuli à faible contraste en fonction de la phase

793
00:29:38,279 --> 00:29:40,980
de l'onde. En outre, ils montrent que,

794
00:29:40,980 --> 00:29:43,500
comme un stimulus à contraste élevé sur la

795
00:29:43,500 --> 00:29:45,779
droite, euh, peut induire une activité d'onde progressive

796
00:29:45,779 --> 00:29:47,520
qui se propage vers l'extérieur.  même

797
00:29:47,520 --> 00:29:50,039
dans le cortex visuel primaire, ils sont donc

798
00:29:50,039 --> 00:29:52,140
vraiment omniprésents dans tout le cerveau

799
00:29:52,140 --> 00:29:54,000
à plusieurs niveaux et il serait

800
00:29:54,000 --> 00:29:55,440
intéressant d'étudier quelles sont leurs

801
00:29:55,440 --> 00:29:58,140
implications pour

802
00:29:58,140 --> 00:29:59,700
l'apprentissage de la représentation structurelle dans notre cas ou

803
00:29:59,700 --> 00:30:01,799
ou, de manière générale,

804
00:30:01,799 --> 00:30:04,140
il existe des travaux antérieurs qui ont étudié

805
00:30:04,140 --> 00:30:06,720
ces types de dynamiques.  et ils

806
00:30:06,720 --> 00:30:08,700
construisent des modèles, donc en haut, ce sont les

807
00:30:08,700 --> 00:30:10,380
équations qui décrivent un

808
00:30:10,380 --> 00:30:12,600
réseau neuronal à pointes qu'ils montrent si vous

809
00:30:12,600 --> 00:30:15,720
implémentez des délais, en fait des

810
00:30:15,720 --> 00:30:18,240
délais axonaux entre les neurones, vous obtenez

811
00:30:18,240 --> 00:30:20,820
cette dynamique de structure d'

812
00:30:20,820 --> 00:30:22,440
ondes progressives tant que la taille de votre réseau est

813
00:30:22,440 --> 00:30:24,059
suffisamment grande.

814
00:30:24,059 --> 00:30:26,520
euh, cependant, comme beaucoup de gens le savent probablement,

815
00:30:26,520 --> 00:30:28,620
il est relativement difficile de

816
00:30:28,620 --> 00:30:31,320
former des réseaux de neurones de même taille

817
00:30:31,320 --> 00:30:34,820
et de même performance que les réseaux de neurones profonds, de la

818
00:30:34,820 --> 00:30:37,679
même manière en bas, un autre système

819
00:30:37,679 --> 00:30:39,539
qui est nettement plus simple mais

820
00:30:39,539 --> 00:30:42,840
peut-être trop simple, euh est un réseau d'

821
00:30:42,840 --> 00:30:45,120
oscillateurs couplés pour lesquels ils sont connus.

822
00:30:45,120 --> 00:30:48,779
présentent une synchronisation et une dynamique spatio-temporelle

823
00:30:48,779 --> 00:30:52,200
et des modèles complexes, mais euh,

824
00:30:52,200 --> 00:30:53,520
cela s'appelle comme un système de réduction de phase

825
00:30:53,520 --> 00:30:55,500
et ne capture pas vraiment

826
00:30:55,500 --> 00:30:57,059
toute la complexité qui nous intéresse,

827
00:30:57,059 --> 00:30:58,140
nous examinons donc quelque chose qui se situe

828
00:30:58,140 --> 00:31:00,779
potentiellement entre ces deux

829
00:31:00,779 --> 00:31:03,600
et ce que nous  Ce travail est réglé sur

830
00:31:03,600 --> 00:31:06,600
ce travail, euh, pour paramétrer un

831
00:31:06,600 --> 00:31:08,520
réseau de couple d'oscillateurs de manière

832
00:31:08,520 --> 00:31:10,620
légèrement plus flexible qu'un

833
00:31:10,620 --> 00:31:12,360
modèle paramoto, donc cela est vraiment

834
00:31:12,360 --> 00:31:14,580
construit sur ce couple de

835
00:31:14,580 --> 00:31:16,380
réseaux neuronaux récurrents distillatoires de Constantine

836
00:31:16,380 --> 00:31:18,720
Rush et Nisha

837
00:31:18,720 --> 00:31:20,760
euh où ils ont essentiellement pris l'

838
00:31:20,760 --> 00:31:22,200
équation qui  décrit un

839
00:31:22,200 --> 00:31:23,820
oscillateur harmonique simple c'est une

840
00:31:23,820 --> 00:31:26,159
équation différentielle du second ordre l'accélération

841
00:31:26,159 --> 00:31:29,940
d'une bille sur un ressort est proportionnelle à

842
00:31:29,940 --> 00:31:32,480
son déplacement

843
00:31:32,480 --> 00:31:35,220
euh vous pouvez ajouter des termes supplémentaires tels que

844
00:31:35,220 --> 00:31:37,260
l'amortissement pour que les oscillations

845
00:31:37,260 --> 00:31:39,360
s'éteignent lentement avec le temps

846
00:31:39,360 --> 00:31:41,580
vous pouvez piloter cet oscillateur avec un oscillateur

847
00:31:41,580 --> 00:31:43,380
externe  entrée pour contrecarrer en quelque sorte

848
00:31:43,380 --> 00:31:45,179
cet amortissement ou pour donner un peu plus de

849
00:31:45,179 --> 00:31:47,279
complexité à la dynamique

850
00:31:47,279 --> 00:31:49,260
et de plus, si vous avez plusieurs de

851
00:31:49,260 --> 00:31:50,940
ces oscillateurs, vous pouvez les coupler

852
00:31:50,940 --> 00:31:53,000
avec ces matrices de couplage W

853
00:31:53,000 --> 00:31:55,320
euh comme nous le démontrons en quelque sorte dans cette

854
00:31:55,320 --> 00:31:56,640
image ici afin que vous puissiez vraiment  pensez à

855
00:31:56,640 --> 00:31:58,140
ce réseau comme à un tas de bulles sur

856
00:31:58,140 --> 00:31:59,940
des ressorts et elles sont peut-être

857
00:31:59,940 --> 00:32:01,740
également connectées les unes aux autres par des ressorts ou

858
00:32:01,740 --> 00:32:03,600
des bandes élastiques, quel que soit le couple de

859
00:32:03,600 --> 00:32:05,279
réseaux neuronaux récurrents distillatoires du

860
00:32:05,279 --> 00:32:08,100
Mishra russe euh avec ces différents termes et

861
00:32:08,100 --> 00:32:09,899
cela s'est avéré très puissant

862
00:32:09,899 --> 00:32:12,480
pour la modélisation de longues séquences, ils ont également

863
00:32:12,480 --> 00:32:13,740
mentionné qu'ils étaient inspirés par la

864
00:32:13,740 --> 00:32:15,360
construction du cerveau et il y a beaucoup de

865
00:32:15,360 --> 00:32:17,700
bonnes analyses dans cet article, par

866
00:32:17,700 --> 00:32:19,020
exemple, ils montrent que ce sont

867
00:32:19,020 --> 00:32:21,440
des propriétés vraiment bénéfiques en ce qui concerne

868
00:32:21,440 --> 00:32:23,460
les problèmes de gradient de disparition qui

869
00:32:23,460 --> 00:32:25,440
se produisent généralement dans les réseaux neuronaux récurrents

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
euh  mais si nous voulons regarder la

872
00:32:29,159 --> 00:32:30,960
dynamique spatio-temporelle et ce type de

873
00:32:30,960 --> 00:32:32,820
modèle, euh, c'est un peu difficile

874
00:32:32,820 --> 00:32:34,919
parce que ces matrices de couplage ici, les

875
00:32:34,919 --> 00:32:36,320
W

876
00:32:36,320 --> 00:32:39,600
qui connectent chaque neuronal ou chaque

877
00:32:39,600 --> 00:32:41,240
oscillateur sont positionnés les uns aux autres,

878
00:32:41,240 --> 00:32:43,620
ce sont des matrices densément connectées

879
00:32:43,620 --> 00:32:45,120
comme j'ai essayé de le faire.  représenté à gauche

880
00:32:45,120 --> 00:32:46,020
ici,

881
00:32:46,020 --> 00:32:48,299
donc si vous essayez de visualiser la dynamique

882
00:32:48,299 --> 00:32:50,580
de ce réseau, vous ne voyez aucune

883
00:32:50,580 --> 00:32:51,899
organisation spatiale, il n'y a pas

884
00:32:51,899 --> 00:32:55,380
d'héritage, ce sont des excuses pour l'

885
00:32:55,380 --> 00:32:57,000
espace latent de ce modèle,

886
00:32:57,000 --> 00:32:58,799
donc vous pouvez penser à cela comme dans notre

887
00:32:58,799 --> 00:33:01,020
exemple précédent, un neurone  est connecté

888
00:33:01,020 --> 00:33:03,240
à un ensemble potentiellement arbitraire d'autres

889
00:33:03,240 --> 00:33:04,919
neurones, ces neurones sont connectés à un

890
00:33:04,919 --> 00:33:06,600
autre ensemble arbitraire de neurones et

891
00:33:06,600 --> 00:33:08,520
vous obtiendrez certainement une dynamique oscillatoire,

892
00:33:08,520 --> 00:33:10,860
mais des types de fluctuations qui

893
00:33:10,860 --> 00:33:13,260
n'ont pas beaucoup de sens structuré, donc

894
00:33:13,260 --> 00:33:15,360
dans notre travail, nous avons alors pensé  ok, comment pouvons-

895
00:33:15,360 --> 00:33:18,299
nous convertir cela davantage aux types de

896
00:33:18,299 --> 00:33:19,860
dynamiques qui nous intéressent dans cette

897
00:33:19,860 --> 00:33:22,140
propagation structurée de l'activité euh

898
00:33:22,140 --> 00:33:23,940
et un moyen clair de le faire est d'avoir

899
00:33:23,940 --> 00:33:26,539
une matrice de connectivité plus structurée W

900
00:33:26,539 --> 00:33:29,880
euh qui, selon nous, est facile à mettre en œuvre

901
00:33:29,880 --> 00:33:31,559
et efficace  implémenté via une

902
00:33:31,559 --> 00:33:33,000
opération de convolution que vous pouvez

903
00:33:33,000 --> 00:33:34,620
considérer comme une

904
00:33:34,620 --> 00:33:36,299
couche locale connectée localement. Ainsi, au lieu d'avoir

905
00:33:36,299 --> 00:33:37,860
chaque neurone connecté, tous

906
00:33:37,860 --> 00:33:39,480
les neurones sont simplement connectés à leurs

907
00:33:39,480 --> 00:33:41,580
voisins proches. Après l'entraînement,

908
00:33:41,580 --> 00:33:42,840
vous finirez par obtenir quelque chose qui

909
00:33:42,840 --> 00:33:44,880
ressemble à un espace lisse.  dynamique temporelle,

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
donc pour être un peu plus clair pour

912
00:33:48,419 --> 00:33:50,519
entraîner ce modèle, nous prenons cette

913
00:33:50,519 --> 00:33:52,200
équation différentielle distincte du second ordre que

914
00:33:52,200 --> 00:33:54,299
nous décrivions avant de la discrétiser

915
00:33:54,299 --> 00:33:56,340
en deux équations du premier ordre, vous

916
00:33:56,340 --> 00:33:57,960
pouvez considérer cela comme une

917
00:33:57,960 --> 00:34:01,200
intégration numérique de l'ode que nous avons maintenant un

918
00:34:01,200 --> 00:34:03,120
vitesse et ensuite nous mettons à jour

919
00:34:03,120 --> 00:34:06,000
euh et et nous pouvons entraîner ce modèle comme

920
00:34:06,000 --> 00:34:07,620
quelque chose comme un encodeur automatique ou un

921
00:34:07,620 --> 00:34:09,839
modèle auto-régressif donc si nous prenons une

922
00:34:09,839 --> 00:34:11,460
entrée, nous la codons dans notre espace latent,

923
00:34:11,460 --> 00:34:14,339
vraiment l'entrée est Dr est-ce ce f de x

924
00:34:14,339 --> 00:34:16,500
terme qui agit  comme terme moteur donc

925
00:34:16,500 --> 00:34:18,599
c'est comme piloter ces oscillateurs depuis

926
00:34:18,599 --> 00:34:20,879
le bas euh et ensuite ils ont leur

927
00:34:20,879 --> 00:34:23,099
propre dynamique qui est définie par les

928
00:34:23,099 --> 00:34:25,800
termes de couplage ces couplages locaux et

929
00:34:25,800 --> 00:34:27,540
puis à chaque pas de temps on prend cet

930
00:34:27,540 --> 00:34:29,460
état latent cet état d'onde et on

931
00:34:29,460 --> 00:34:31,560
décode pour essayer  et reconstruire l'entrée

932
00:34:31,560 --> 00:34:33,540
et être au pas de temps actuel ou à un

933
00:34:33,540 --> 00:34:35,699
pas de temps futur,

934
00:34:35,699 --> 00:34:37,980
nous pouvons faire une analyse de ces

935
00:34:37,980 --> 00:34:42,300
modèles, euh pendant la formation pour voir ce qui

936
00:34:42,300 --> 00:34:43,619
se passe avant la formation et après

937
00:34:43,619 --> 00:34:45,780
la formation, nous pouvons calculer la phase et

938
00:34:45,780 --> 00:34:47,399
la vitesse de la dynamique dans le

939
00:34:47,399 --> 00:34:49,379
espace latent, en gros, nous voyons au

940
00:34:49,379 --> 00:34:51,480
début du trading qu'il n'y a pas de vagues dans

941
00:34:51,480 --> 00:34:53,699
notre modèle, mais après un entraînement après 50

942
00:34:53,699 --> 00:34:55,500
époques, nous voyons qu'il y a une

943
00:34:55,500 --> 00:34:57,119
activité structurée et fluide qui se propage

944
00:34:57,119 --> 00:35:00,420
vers le bas, euh, au service de cette

945
00:35:00,420 --> 00:35:01,800
tâche de modélisation de séquence que nous faisons comme

946
00:35:01,800 --> 00:35:04,380
des objets en rotation,

947
00:35:04,380 --> 00:35:05,940
donc  quel est l'avantage de cela,

948
00:35:05,940 --> 00:35:07,680
je veux dire, la seule raison pour laquelle j'ai motivé cela

949
00:35:07,680 --> 00:35:10,380
était de dire que nous voulions avoir

950
00:35:10,380 --> 00:35:11,880
une structure d'apprentissage plus flexible, est-ce que nous le

951
00:35:11,880 --> 00:35:13,020
faisons réellement ou est-ce que nous

952
00:35:13,020 --> 00:35:15,060
recevons simplement de jolies vagues,

953
00:35:15,060 --> 00:35:16,859
donc ce que nous avons montré dans notre article, c'est

954
00:35:16,859 --> 00:35:19,320
que nous sommes vraiment  apprendre une sorte de

955
00:35:19,320 --> 00:35:20,940
structure utile et la façon dont nous l'avons montré

956
00:35:20,940 --> 00:35:22,440
est encore une fois avec quelque chose comme ce

957
00:35:22,440 --> 00:35:24,960
diagramme commutatif si vous prenez une entrée

958
00:35:24,960 --> 00:35:27,000
et que vous l'encodez et vous obtenez un

959
00:35:27,000 --> 00:35:29,280
état d'onde, puis vous propagez artificiellement les ondes

960
00:35:29,280 --> 00:35:31,619
dans cet état d'onde et ensuite

961
00:35:31,619 --> 00:35:33,480
décodez, vous pouvez  observez que c'est

962
00:35:33,480 --> 00:35:35,220
en fait exactement la même chose que si vous aviez

963
00:35:35,220 --> 00:35:37,140
simplement montré un tas d'

964
00:35:37,140 --> 00:35:39,180
images différentes de différentes transformations, donc

965
00:35:39,180 --> 00:35:41,640
beaucoup de chiffres différents, des

966
00:35:41,640 --> 00:35:43,920
caractéristiques différentes et nous voyons que nous obtenons

967
00:35:43,920 --> 00:35:46,140
différents types d'activité de vague dans chaque

968
00:35:46,140 --> 00:35:47,880
cas afin de modéliser cela  transformation différente

969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
si nous l'entraînons sur différents ensembles de données,

971
00:35:51,119 --> 00:35:53,400
nous voyons également une dynamique plus complexe

972
00:35:53,400 --> 00:35:55,200
dans ce cas, peut-être même pas des

973
00:35:55,200 --> 00:35:57,839
ondes progressives ou des ondes stationnaires qui

974
00:35:57,839 --> 00:36:00,359
peuvent être considérées comme des ondes progressives dans des

975
00:36:00,359 --> 00:36:02,339
directions opposées, nous voyons donc si nous

976
00:36:02,339 --> 00:36:04,079
modélisons ces orbitales  Dynamique, nous obtenons

977
00:36:04,079 --> 00:36:06,000
ce genre de blobs d'activité se déplaçant en douceur

978
00:36:06,000 --> 00:36:07,619
dans notre espace latent. Si nous

979
00:36:07,619 --> 00:36:09,839
modélisons un pendule, nous obtenons de la même manière une

980
00:36:09,839 --> 00:36:13,820
sorte d'activité oscillatoire complexe,

981
00:36:14,099 --> 00:36:17,339
de sorte que la structure d'entrée est préservée, mais

982
00:36:17,339 --> 00:36:19,560
en plus, plus de flexibilité qu'auparavant,

983
00:36:19,560 --> 00:36:21,599
ce qui est en quelque sorte notre objectif ultime.

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
donc enfin, je veux parler un peu de la

986
00:36:26,099 --> 00:36:28,320
façon dont je pense que les résultats de cette recherche

987
00:36:28,320 --> 00:36:30,420
peuvent non seulement améliorer l'

988
00:36:30,420 --> 00:36:32,099
intelligence artificielle, mais aussi comment elle nous aide à

989
00:36:32,099 --> 00:36:34,440
comprendre pourquoi nos mesures du

990
00:36:34,440 --> 00:36:36,240
cerveau ressemblent à ce qu'elles sont, pour donner un

991
00:36:36,240 --> 00:36:38,579
bref exemple de ce que je veux dire.  veux dire par là, euh,

992
00:36:38,579 --> 00:36:41,040
j'ai déjà parlé un peu des visas

993
00:36:41,040 --> 00:36:43,740
et des lieux, donc dans ce travail fantastique

994
00:36:43,740 --> 00:36:46,859
avec Ching higao, nous avons étudié si notre

995
00:36:46,859 --> 00:36:48,900
simple a priori topographique comme nous en avons discuté

996
00:36:48,900 --> 00:36:50,579
pouvait être capable de reproduire ces mêmes

997
00:36:50,579 --> 00:36:53,339
effets, donc spécifiquement nous mettons la valeur

998
00:36:53,339 --> 00:36:56,099
de ce D de Cohen  métrique de sélectivité pour

999
00:36:56,099 --> 00:36:58,200
chacun de nos neurones par rapport à un

1000
00:36:58,200 --> 00:37:00,000
ensemble de données différent d'images

1001
00:37:00,000 --> 00:37:02,460
contenant potentiellement uniquement des visages ou uniquement des objets ou

1002
00:37:02,460 --> 00:37:03,359
des corps.

1003
00:37:03,359 --> 00:37:05,880
Nous mesurons donc pour chaque neurone s'il est

1004
00:37:05,880 --> 00:37:07,920
plus susceptible de répondre aux visages ou si le

1005
00:37:07,920 --> 00:37:10,380
russe émerge dans le cerveau, mais je le fais.

1006
00:37:10,380 --> 00:37:12,839
Je pense que cela nous indique que l'

1007
00:37:12,839 --> 00:37:15,300
organisation relative de la sélectivité peut au moins

1008
00:37:15,300 --> 00:37:17,400
être partiellement attribuable aux

1009
00:37:17,400 --> 00:37:19,800
statistiques de corrélation dans les données qui doivent être retracées

1010
00:37:19,800 --> 00:37:21,359
après avoir été passées par un

1011
00:37:21,359 --> 00:37:23,640
extracteur futur hautement non linéaire tel qu'un

1012
00:37:23,640 --> 00:37:25,440
réseau neuronal profond,

1013
00:37:25,440 --> 00:37:27,480
donc dans la même veine, quelque chose qui est

1014
00:37:27,480 --> 00:37:29,040
intéressant, il y a ce qu'on

1015
00:37:29,040 --> 00:37:30,900
appelle le flux visuel tripartite ou non,

1016
00:37:30,900 --> 00:37:36,720
donc euh les images de euh ou d'objets sont

1017
00:37:36,720 --> 00:37:38,400
la sélectivité par rapport aux objets

1018
00:37:38,400 --> 00:37:40,680
est organisée par des propriétés plus abstraites

1019
00:37:40,680 --> 00:37:43,200
telles que l'animation, cette chose est-elle vivante ou

1020
00:37:43,200 --> 00:37:46,200
inanimée euh par rapport à

1021
00:37:46,200 --> 00:37:48,480
la taille des objets du monde réel comme quoi  est la taille d'une

1022
00:37:48,480 --> 00:37:50,700
théière par rapport à une voiture

1023
00:37:50,700 --> 00:37:53,520
euh et ce que nous voyons c'est que chez les

1024
00:37:53,520 --> 00:37:56,160
humains, la sélectivité est organisée

1025
00:37:56,160 --> 00:37:57,540
dans cette structure tripartite, vous

1026
00:37:57,540 --> 00:37:59,760
avez généralement de petits objets qui se situent

1027
00:37:59,760 --> 00:38:01,920
entre les objets animés et inanimés

1028
00:38:01,920 --> 00:38:04,200
en termes de leur sélectivité et nous voyons

1029
00:38:04,200 --> 00:38:06,060
la même chose se produit ici, donc

1030
00:38:06,060 --> 00:38:07,440
ceux-ci mesurent la sélectivité du

1031
00:38:07,440 --> 00:38:08,880
même ensemble de neurones, mais en ce qui concerne

1032
00:38:08,880 --> 00:38:10,859
ces différences de stimuli, vous voyez

1033
00:38:10,859 --> 00:38:12,440
que le petit cluster est entre un

1034
00:38:12,440 --> 00:38:14,820
cluster animé et un cluster inanimé et encore une fois,

1035
00:38:14,820 --> 00:38:16,079
cela se produit pour plusieurs

1036
00:38:16,079 --> 00:38:18,900
initialisations différentes, donc ceci  C'est quelque chose que j'espère que

1037
00:38:18,900 --> 00:38:20,880
nous pourrons explorer un peu plus loin pour

1038
00:38:20,880 --> 00:38:22,980
cette communauté. Je pense que c'est intéressant

1039
00:38:22,980 --> 00:38:24,119
car c'est

1040
00:38:24,119 --> 00:38:26,220
vraiment une façon de montrer que nous avons

1041
00:38:26,220 --> 00:38:28,200
construit un modèle mondial structuré et que

1042
00:38:28,200 --> 00:38:30,119
potentiellement ce modèle mondial est

1043
00:38:30,119 --> 00:38:31,980
bénéfique pour

1044
00:38:31,980 --> 00:38:34,740
mieux représenter les données du monde réel de

1045
00:38:34,740 --> 00:38:37,619
manière structurée et  vous obtenez une énergie libre inférieure

1046
00:38:37,619 --> 00:38:39,119
dans ce sens,

1047
00:38:39,119 --> 00:38:40,619
donc

1048
00:38:40,619 --> 00:38:42,300
euh, je pense qu'en développant ces modèles

1049
00:38:42,300 --> 00:38:44,400
comme nous l'avons montré ici, nous pourrions avoir un

1050
00:38:44,400 --> 00:38:46,500
aperçu de nouveaux mécanismes sur la façon dont

1051
00:38:46,500 --> 00:38:48,900
cette structure émerge, y compris une

1052
00:38:48,900 --> 00:38:50,460
organisation topographique à laquelle nous n'avions jamais

1053
00:38:50,460 --> 00:38:52,920
pensé auparavant, donc le modèle de machine que je

1054
00:38:52,920 --> 00:38:55,520
regardais  la sélectivité d'orientation

1055
00:38:55,520 --> 00:38:58,260
des neurones à laquelle je ne

1056
00:38:58,260 --> 00:39:01,020
m'attendais pas particulièrement,

1057
00:39:01,020 --> 00:39:03,420
mais euh, vous regardez une sorte de

1058
00:39:03,420 --> 00:39:05,339
propagation de ces ondes sur cette

1059
00:39:05,339 --> 00:39:08,099
surface verticale simulée et j'ai pensé que

1060
00:39:08,099 --> 00:39:09,960
d'accord, peut-être que je montre des images pivotées,

1061
00:39:09,960 --> 00:39:11,820
peut-être que cela a un effet sur  la

1062
00:39:11,820 --> 00:39:13,740
sélectivité de l'orientation

1063
00:39:13,740 --> 00:39:15,599
et en fait, si vous entrez et

1064
00:39:15,599 --> 00:39:17,460
mesurez la sélectivité de chaque neurone

1065
00:39:17,460 --> 00:39:18,660
par rapport à ces

1066
00:39:18,660 --> 00:39:22,079
lignes orientées différemment, ce que vous voyez, c'est que cela

1067
00:39:22,079 --> 00:39:24,300
rappelle étonnamment les

1068
00:39:24,300 --> 00:39:25,859
colonnes de type Orient que l'on voit dans le

1069
00:39:25,859 --> 00:39:27,599
cortex visuel primaire, c'est quelque chose qui remonte

1070
00:39:27,599 --> 00:39:29,520
à Hugo et à la belette et c'est quelque chose

1071
00:39:29,520 --> 00:39:30,900
qui vient en quelque sorte de ce modèle

1072
00:39:30,900 --> 00:39:33,060
et du fait qu'il a la

1073
00:39:33,060 --> 00:39:34,440
structure spatio-temporelle par rapport aux

1074
00:39:34,440 --> 00:39:37,619
transformations, donc bien sûr, c'est

1075
00:39:37,619 --> 00:39:39,599
une analogie vraiment grossière mais je pense que c'est

1076
00:39:39,599 --> 00:39:40,740
un exemple de la façon dont  construire ce

1077
00:39:40,740 --> 00:39:42,839
type de modèles peut nous aider à réfléchir à la

1078
00:39:42,839 --> 00:39:45,240
façon dont le cerveau construit la

1079
00:39:45,240 --> 00:39:46,980
structure de représentation et au blanc la façon dont elle est

1080
00:39:46,980 --> 00:39:48,660
organisée d'une manière à laquelle nous n'avions peut-être pas

1081
00:39:48,660 --> 00:39:51,300
pensé auparavant,

1082
00:39:51,300 --> 00:39:53,460
euh, je pense que je ne suis pas le seul à

1083
00:39:53,460 --> 00:39:55,859
faire ce type de  travail et donc je

1084
00:39:55,859 --> 00:39:57,240
veux parler un peu d'

1085
00:39:57,240 --> 00:39:59,579
autres personnes qui font ça euh donc

1086
00:39:59,579 --> 00:40:00,780
j'ai parlé de cette

1087
00:40:00,780 --> 00:40:02,760
structure équivalente

1088
00:40:02,760 --> 00:40:04,920
euh des gens comme James Whittington et

1089
00:40:04,920 --> 00:40:08,880
Tim Barons et surrogengoolie euh ont

1090
00:40:08,880 --> 00:40:10,680
montré récemment qu'en introduisant

1091
00:40:10,680 --> 00:40:14,940
l'algébrique  contraintes dans euh dans un

1092
00:40:14,940 --> 00:40:17,040
processus d'apprentissage dans ce cas, c'était

1093
00:40:17,040 --> 00:40:20,820
comme le mouvement d'euh et et d'agents dans

1094
00:40:20,820 --> 00:40:23,220
un environnement en disant que vous devez

1095
00:40:23,220 --> 00:40:24,780
préserver une sorte de

1096
00:40:24,780 --> 00:40:27,540
structure algébrique si je me déplace dans un cercle Ouest Nord

1097
00:40:27,540 --> 00:40:29,280
-Est Sud, je me retrouve au

1098
00:40:29,280 --> 00:40:31,440
même  point Encore une fois, en introduisant ces

1099
00:40:31,440 --> 00:40:32,820
types de contraintes,

1100
00:40:32,820 --> 00:40:35,040
vous obtenez l'émergence de représentations de type cellule de grille,

1101
00:40:35,040 --> 00:40:36,900


1102
00:40:36,900 --> 00:40:39,359
donc je serais intéressé de voir comment cette

1103
00:40:39,359 --> 00:40:41,880
idée de structure de représentation peut

1104
00:40:41,880 --> 00:40:43,980
nous aider à expliquer peut-être plus que nos

1105
00:40:43,980 --> 00:40:45,480
découvertes scientifiques que nous trouvons

1106
00:40:45,480 --> 00:40:46,619
également,

1107
00:40:46,619 --> 00:40:48,359
euh et  et comment cela se rapporte aux

1108
00:40:48,359 --> 00:40:51,540
modèles génératifs dans leur ensemble, euh

1109
00:40:51,540 --> 00:40:52,800
et enfin, je pense qu'il y a

1110
00:40:52,800 --> 00:40:54,599
quelque chose à dire sur la

1111
00:40:54,599 --> 00:40:56,099
possibilité cognitive de ces modèles aussi,

1112
00:40:56,099 --> 00:40:57,420
peut-être que nous n'allons pas seulement

1113
00:40:57,420 --> 00:40:59,579
les tester du point de vue des neurosciences,

1114
00:40:59,579 --> 00:41:01,020
mais aussi du

1115
00:41:01,020 --> 00:41:03,839
point de vue de la science micrognitive  par exemple, il y a ces

1116
00:41:03,839 --> 00:41:06,000
matrices Ravens Progressive sur la gauche

1117
00:41:06,000 --> 00:41:08,640
où vous devez dire laquelle de

1118
00:41:08,640 --> 00:41:11,099
ces images est la plus susceptible de correspondre à

1119
00:41:11,099 --> 00:41:12,599
ce modèle

1120
00:41:12,599 --> 00:41:14,760
euh ou par exemple Quelle est la probabilité que

1121
00:41:14,760 --> 00:41:16,740
cette tour Jenga tombe lorsque vous vous

1122
00:41:16,740 --> 00:41:19,740
arrêtez  un bloc spécifique ou ou

1123
00:41:19,740 --> 00:41:22,740
avec une structure donnée et je pense que ce

1124
00:41:22,740 --> 00:41:24,599
sont

1125
00:41:24,599 --> 00:41:26,640
ces types de tests qui testent vraiment

1126
00:41:26,640 --> 00:41:28,619
si nos modèles du monde que nous construisons

1127
00:41:28,619 --> 00:41:31,560
sont similaires aux types de modèles que

1128
00:41:31,560 --> 00:41:33,660
nous avons de manière innée notre propre bon sens en

1129
00:41:33,660 --> 00:41:36,060
tant qu'humains ou  en tant qu'êtres vivant dans un

1130
00:41:36,060 --> 00:41:38,400
monde naturel et j'ai fait des

1131
00:41:38,400 --> 00:41:40,440
travaux préliminaires dans cette direction, je

1132
00:41:40,440 --> 00:41:43,079
pense que c'est très préliminaire et pas aussi

1133
00:41:43,079 --> 00:41:45,480
compliqué, mais euh, j'essaie en quelque sorte

1134
00:41:45,480 --> 00:41:47,820
de modéliser des illusions visuelles, donc si vous prenez

1135
00:41:47,820 --> 00:41:50,520
un ensemble de données très simple d'une barre mobile

1136
00:41:50,520 --> 00:41:52,980
stimuli ou une barre ou un cadre statique et que vous

1137
00:41:52,980 --> 00:41:54,960
le déplacez un peu, vous pouvez voir que

1138
00:41:54,960 --> 00:41:57,060
le modèle déduira en fait ce

1139
00:41:57,060 --> 00:41:58,800
cadre manquant, puis en

1140
00:41:58,800 --> 00:42:01,079
déduira également un mouvement continu, donc c'est comme

1141
00:42:01,079 --> 00:42:03,300
dépasser la trajectoire de ce que les

1142
00:42:03,300 --> 00:42:05,820
stimuli réels lui fournissent avant de

1143
00:42:05,820 --> 00:42:08,760
corriger à nouveau  donc je pense que la modélisation des

1144
00:42:08,760 --> 00:42:10,320
illusions est certainement une

1145
00:42:10,320 --> 00:42:12,660
façon intéressante d'étudier si nos modèles du monde sont

1146
00:42:12,660 --> 00:42:14,760
similaires aux types de modèles que nous

1147
00:42:14,760 --> 00:42:16,619
avons nous-mêmes,

1148
00:42:16,619 --> 00:42:19,619
donc en conclusion, euh ouais, je pense que

1149
00:42:19,619 --> 00:42:21,900
nous pourrions montrer qu'ils ont

1150
00:42:21,900 --> 00:42:23,220
effectivement appris

1151
00:42:23,220 --> 00:42:24,839
des représentations structurées ou des modèles du monde structurés.  la

1152
00:42:24,839 --> 00:42:26,700
structure apprise est

1153
00:42:26,700 --> 00:42:29,160
flexible et adaptable à

1154
00:42:29,160 --> 00:42:30,780
des transformations arbitraires, contrairement aux

1155
00:42:30,780 --> 00:42:33,720
équivariantes traditionnelles et aux fournisseurs topographiques,

1156
00:42:33,720 --> 00:42:35,579
peuvent être induites statistiquement comme nous l'avons fait

1157
00:42:35,579 --> 00:42:37,619
dans la vae topographique ou via

1158
00:42:37,619 --> 00:42:39,480
la dynamique comme nous le montrions dans ces

1159
00:42:39,480 --> 00:42:42,000
modèles de type machine à ondes neuronales,

1160
00:42:42,000 --> 00:42:44,460
donc pour conclure, je vous laisse avec ceci

1161
00:42:44,460 --> 00:42:46,980
Citation que j'ai trouvée dans un article de Fukushima

1162
00:42:46,980 --> 00:42:50,280
datant de 1980, je pensais qu'elle était assez en avance

1163
00:42:50,280 --> 00:42:52,079
sur son temps, où il dit que si nous pouvions

1164
00:42:52,079 --> 00:42:53,520
créer un modèle de réseau neuronal doté de

1165
00:42:53,520 --> 00:42:55,020
la même capacité de

1166
00:42:55,020 --> 00:42:57,060
reconnaissance de formes qu'un être humain, cela

1167
00:42:57,060 --> 00:42:58,800
nous donnerait un indice puissant par rapport à

1168
00:42:58,800 --> 00:43:00,000
comprendre le mécanisme neuronal dans

1169
00:43:00,000 --> 00:43:03,240
le cerveau, donc c'est en quelque sorte, je pense, certains

1170
00:43:03,240 --> 00:43:06,119
des objectifs que nous visons ici,

1171
00:43:06,119 --> 00:43:08,220
donc je pense que c'est mon conseiller Max, mes

1172
00:43:08,220 --> 00:43:11,280
co-auteurs Patrick UA Emil Jinghian et

1173
00:43:11,280 --> 00:43:17,359
Yorn et intéressés par la discussion, merci d'accord, d'accord

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
merci super, très euh

1176
00:43:27,480 --> 00:43:31,079
présentation intéressante

1177
00:43:31,079 --> 00:43:33,480
beaucoup d'endroits pour commencer peut-être juste euh ce qui

1178
00:43:33,480 --> 00:43:36,000
vous a amené à ce travail

1179
00:43:36,000 --> 00:43:38,520
un peu de contexte sur la façon dont vous êtes entré dans

1180
00:43:38,520 --> 00:43:43,819
ce travail pour la direction de votre doctorat

1181
00:43:43,920 --> 00:43:45,119
ouais,

1182
00:43:45,119 --> 00:43:46,020
euh,

1183
00:43:46,020 --> 00:43:49,200
je veux dire, nous n'avons pas étudié mon

1184
00:43:49,200 --> 00:43:51,000
groupe qui  Je suis à l'université et

1185
00:43:51,000 --> 00:43:52,700
j'étudie les représentations structurées

1186
00:43:52,700 --> 00:43:56,640
d'un point de vue mathématique depuis un certain

1187
00:43:56,640 --> 00:43:58,319
temps, où certaines personnes ont

1188
00:43:58,319 --> 00:44:00,240
des modèles, comme l'

1189
00:44:00,240 --> 00:44:01,740
encodeur automatique variationnel,

1190
00:44:01,740 --> 00:44:04,680
euh et

1191
00:44:04,680 --> 00:44:06,960
je suppose que quelque chose qui a toujours

1192
00:44:06,960 --> 00:44:08,460
été,

1193
00:44:08,460 --> 00:44:11,220
c'est un modèle.  qui respecte parfaitement les rotations les rotations 2D

1194
00:44:11,220 --> 00:44:13,560
mais si on veut

1195
00:44:13,560 --> 00:44:15,960
faire des rotations 3D on ne peut pas faire ça

1196
00:44:15,960 --> 00:44:17,819
car ce n'est pas un groupe en terme de

1197
00:44:17,819 --> 00:44:19,740
projection sur un plan 2D il y a tu

1198
00:44:19,740 --> 00:44:21,180
perds des informations quand cette chose

1199
00:44:21,180 --> 00:44:23,460
tourne autour par exemple

1200
00:44:23,460 --> 00:44:24,240
euh

1201
00:44:24,240 --> 00:44:26,280
ou  n'importe quelle sorte de

1202
00:44:26,280 --> 00:44:27,960
transformations naturelles comme j'essayais de le

1203
00:44:27,960 --> 00:44:29,339
souligner au début, je pense qu'il

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
s'agissait de réfléchir à la façon dont le cerveau

1206
00:44:31,740 --> 00:44:34,020
modélise les transformations naturelles, c'est

1207
00:44:34,020 --> 00:44:35,400
quelque chose que ces cadres actuels

1208
00:44:35,400 --> 00:44:37,200


1209
00:44:37,200 --> 00:44:41,099
où voyez-vous l'action jouer un rôle

1210
00:44:41,099 --> 00:44:44,579
en termes d'auto-encodeur variationnel

1211
00:44:44,579 --> 00:44:48,420
des modèles qui incluent non seulement

1212
00:44:48,420 --> 00:44:50,520
des modèles externes mais aussi les

1213
00:44:50,520 --> 00:44:52,380
conséquences de l'action ou une

1214
00:44:52,380 --> 00:44:55,800
structure structurelle de modèle mondial avec action,

1215
00:44:55,800 --> 00:44:58,619
oui, non, c'est une bonne question et

1216
00:44:58,619 --> 00:45:01,319
je pense que les inférences agis, euh,

1217
00:45:01,319 --> 00:45:03,839
sont effectivement la réponse, je pense que

1218
00:45:03,839 --> 00:45:05,940
c'est une bonne réponse à cela,

1219
00:45:05,940 --> 00:45:09,000
euh, je sais là  sont

1220
00:45:09,000 --> 00:45:11,099
des cadres d'apprentissage par renforcement qui

1221
00:45:11,099 --> 00:45:12,660
utilisent

1222
00:45:12,660 --> 00:45:15,060
des types de modèles mondiaux formés en externe,

1223
00:45:15,060 --> 00:45:17,280
donc vous formez un vae ou quelque chose du genre, puis

1224
00:45:17,280 --> 00:45:19,800
vous utilisez cette représentation dans votre

1225
00:45:19,800 --> 00:45:23,040
système d'apprentissage par renforcement, mais je

1226
00:45:23,040 --> 00:45:24,720
pense avoir une

1227
00:45:24,720 --> 00:45:26,520
sorte de système complet qui est un

1228
00:45:26,520 --> 00:45:30,780
objectif unique avec euh action  dans le cadre de la

1229
00:45:30,780 --> 00:45:33,660
probabilité des données et euh

1230
00:45:33,660 --> 00:45:35,280
ouais, je pense que c'est beaucoup plus élégant

1231
00:45:35,280 --> 00:45:38,940
et donc je suis un grand partisan de cela, euh,

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
je ne suis pas allé jusqu'à étudier comment

1234
00:45:43,140 --> 00:45:45,480
ces modèles mondiaux structurés dans un vae

1235
00:45:45,480 --> 00:45:47,520
ou je l'ai fait  Je n'ai pas du tout travaillé là-dessus, mais je

1236
00:45:47,520 --> 00:45:48,780
pense qu'il serait certainement très

1237
00:45:48,780 --> 00:45:50,819
intéressant de voir si avoir un

1238
00:45:50,819 --> 00:45:52,339
modèle mondial plus structuré,

1239
00:45:52,339 --> 00:45:54,839
euh, dans un encodeur automatique variationnel

1240
00:45:54,839 --> 00:45:56,880
serait bénéfique dans la mesure où dans un

1241
00:45:56,880 --> 00:45:58,319
cadre actif également, je pense que ce serait

1242
00:45:58,319 --> 00:46:00,119
génial, je veux dire, je  pense que

1243
00:46:00,119 --> 00:46:03,599
certains de ces exemples, comme euh, montrés

1244
00:46:03,599 --> 00:46:05,579
auparavant, comme l'émergence de cellules de grille et

1245
00:46:05,579 --> 00:46:07,500
des choses comme celle-ci, pointent peut-être dans

1246
00:46:07,500 --> 00:46:08,880
cette direction,

1247
00:46:08,880 --> 00:46:10,560
ok, peut-être que le cerveau fait quelque chose,

1248
00:46:10,560 --> 00:46:12,540
il a évidemment beaucoup de

1249
00:46:12,540 --> 00:46:13,680
structure,

1250
00:46:13,680 --> 00:46:15,359
euh, cela doit clairement être utile pour

1251
00:46:15,359 --> 00:46:19,140
effectuer des actions dans certains  et bien,

1252
00:46:19,140 --> 00:46:21,720
oh ouais, j'ai ressenti un

1253
00:46:21,720 --> 00:46:24,480
parallèle très intéressant que vous avez évoqué avec

1254
00:46:24,480 --> 00:46:28,040
la discussion, à savoir que les unités connectées localement

1255
00:46:28,040 --> 00:46:30,960
permettaient à vos modèles d'

1256
00:46:30,960 --> 00:46:33,780
incarner structurellement la

1257
00:46:33,780 --> 00:46:35,640
contrainte et le modèle convolutifs et cela a conduit à

1258
00:46:35,640 --> 00:46:37,500
ces modèles émergents et puis,

1259
00:46:37,500 --> 00:46:41,339
de manière analogue, il y avait le euh Doral du

1260
00:46:41,339 --> 00:46:45,680
tout où  ils avaient la bonne contrainte d'exploration du chemin

1261
00:46:45,680 --> 00:46:48,359
et donc il est

1262
00:46:48,359 --> 00:46:50,280
intéressant de

1263
00:46:50,280 --> 00:46:53,760
penser à ces heuristiques d'action ou de

1264
00:46:53,760 --> 00:46:56,819
politique ou à ces heuristiques comme une

1265
00:46:56,819 --> 00:46:59,579
exploration motrice conjointe, finalement on

1266
00:46:59,579 --> 00:47:02,339
comprend qu'il y a comme deux

1267
00:47:02,339 --> 00:47:04,980
façons mutuellement opposées de déplacer une articulation

1268
00:47:04,980 --> 00:47:07,079
et puis la compositionnalité  à travers les

1269
00:47:07,079 --> 00:47:09,119
articulations peut être appris à ces

1270
00:47:09,119 --> 00:47:10,680
niveaux supérieurs une fois qu'il est verrouillé à

1271
00:47:10,680 --> 00:47:14,480
des niveaux inférieurs, c'est donc une manière très attrayante

1272
00:47:14,480 --> 00:47:17,599
et

1273
00:47:17,599 --> 00:47:20,460
pertinente de généraliser,

1274
00:47:20,460 --> 00:47:23,819
car elle est à la fois basée sur les

1275
00:47:23,819 --> 00:47:25,740
contraintes réelles du monde, mais

1276
00:47:25,740 --> 00:47:27,720
surtout à travers une action

1277
00:47:27,720 --> 00:47:29,460
intégrant potentiellement quelque chose qui est

1278
00:47:29,460 --> 00:47:31,380
assez simple, oui,

1279
00:47:31,380 --> 00:47:33,599
non, je pense que c'est tout à fait

1280
00:47:33,599 --> 00:47:36,599
vrai, c'est un très bon point si, euh, si

1281
00:47:36,599 --> 00:47:38,339
vous avez des contraintes provenant de vos

1282
00:47:38,339 --> 00:47:40,500
actions elles-mêmes, cela

1283
00:47:40,500 --> 00:47:42,839
serait extrêmement bénéfique pour vous aider

1284
00:47:42,839 --> 00:47:44,819
à structurer

1285
00:47:44,819 --> 00:47:47,460
votre espace latent et je pense que oui, je

1286
00:47:47,460 --> 00:47:48,480
suppose une chose  Je voulais mentionner qu'il y

1287
00:47:48,480 --> 00:47:49,980
a

1288
00:47:49,980 --> 00:47:50,700
eu

1289
00:47:50,700 --> 00:47:52,740
quelque chose qui m'a fait penser au

1290
00:47:52,740 --> 00:47:55,500
travail de Stefano Fousey sur un type de

1291
00:47:55,500 --> 00:47:58,859
géométrie représentationnelle, euh, qui

1292
00:47:58,859 --> 00:48:01,920
détermine comment nous

1293
00:48:01,920 --> 00:48:04,440
pouvons généraliser une compréhension donnée

1294
00:48:04,440 --> 00:48:08,099
d'un système, euh et je pense que si vous pouvez

1295
00:48:08,099 --> 00:48:11,880
comprendre comment ces ensembles d'

1296
00:48:11,880 --> 00:48:14,520
activités sont  séparable ou hautement

1297
00:48:14,520 --> 00:48:16,079
parallèle séparable avec un

1298
00:48:16,079 --> 00:48:18,839
classificateur linéaire, vous pourrez essentiellement

1299
00:48:18,839 --> 00:48:20,700
faire de la généralisation et je

1300
00:48:20,700 --> 00:48:23,099
pense qu'en imposant ces types de biais

1301
00:48:23,099 --> 00:48:25,040
ou potentiellement via

1302
00:48:25,040 --> 00:48:27,000
des contraintes imposées par une action,

1303
00:48:27,000 --> 00:48:28,740
quelque chose comme ça,

1304
00:48:28,740 --> 00:48:32,040
vous cédez ou induisez en quelque sorte un

1305
00:48:32,040 --> 00:48:33,660
une meilleure géométrie de représentation et

1306
00:48:33,660 --> 00:48:35,220
cela a toutes sortes d'avantages comme la

1307
00:48:35,220 --> 00:48:36,660
compositionnalité

1308
00:48:36,660 --> 00:48:39,359
ouais notre généralisation donc

1309
00:48:39,359 --> 00:48:41,760
c'est un super point cool ouais un

1310
00:48:41,760 --> 00:48:43,440
domaine très intéressant d'accord, je vais lire

1311
00:48:43,440 --> 00:48:45,960
quelques questions du chat en direct,

1312
00:48:45,960 --> 00:48:48,420
j'adore évoluer, j'ai écrit

1313
00:48:48,420 --> 00:48:52,260
toutes les limitations pratiques ou observées sur

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
l'apprentissage des illusions de modélisation  les communautés sont qu'elles n'ont pas de

1316
00:49:00,420 --> 00:49:03,060
phobie, vous n'avez pas de centre de

1317
00:49:03,060 --> 00:49:05,940
regard, alors vous n'avez pas non plus

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
de temps, je veux dire la plupart des

1320
00:49:11,460 --> 00:49:13,260
réseaux de neurones convolutifs, j'utilise ce genre de

1321
00:49:13,260 --> 00:49:15,599
réseaux de neurones récurrents mais le temps

1322
00:49:15,599 --> 00:49:18,420
n'est pas aussi clairement défini  dans ces modèles

1323
00:49:18,420 --> 00:49:20,220
comme c'est le cas dans un temps continu

1324
00:49:20,220 --> 00:49:23,400
pour un humain subissant un essai d'illusion

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
euh

1327
00:49:25,319 --> 00:49:27,480
et je pense que la combinaison de ces deux-là

1328
00:49:27,480 --> 00:49:30,359
du fait qu'en tant qu'humain ou la plupart

1329
00:49:30,359 --> 00:49:33,300
des choses, euh,

1330
00:49:33,300 --> 00:49:35,940
votre regard, vos déplacements et

1331
00:49:35,940 --> 00:49:38,220
vos gains dépendent de  comme si vous

1332
00:49:38,220 --> 00:49:40,140
examiniez un domaine particulier, de nombreux

1333
00:49:40,140 --> 00:49:42,780
tests cognitifs et donc je pense que ce

1334
00:49:42,780 --> 00:49:46,560
serait vraiment utile si nous avions des modèles qui,

1335
00:49:46,560 --> 00:49:48,540
oui, je veux dire, apprennent que vous pouvez considérer

1336
00:49:48,540 --> 00:49:50,760
cela comme un type d'action comme apprendre

1337
00:49:50,760 --> 00:49:52,980
où déplacer votre regard l'un des  le

1338
00:49:52,980 --> 00:49:54,420
plus simple possible, cela aiderait beaucoup

1339
00:49:54,420 --> 00:49:56,220
à être capable de modéliser des illusions et

1340
00:49:56,220 --> 00:49:58,859
je veux dire pour moi, c'est comme si je lisais un

1341
00:49:58,859 --> 00:50:00,720
article sur des expériences de sciences cognitives

1342
00:50:00,720 --> 00:50:02,940
ou sur une illusion et

1343
00:50:02,940 --> 00:50:05,160
c'est comme si je pensais à ok, puis-je mettre cet

1344
00:50:05,160 --> 00:50:07,560
ensemble de données dans mon  modélisez-le et testez-le et

1345
00:50:07,560 --> 00:50:08,579
la plupart du temps, la réponse est non

1346
00:50:08,579 --> 00:50:10,619
parce que je n'ai pas de modèle qui

1347
00:50:10,619 --> 00:50:12,900
regarde autour de moi ou qui a un champ

1348
00:50:12,900 --> 00:50:14,660
de vision restreint, quelque chose comme ça,

1349
00:50:14,660 --> 00:50:16,619
alors oui, je pense que c'est l'une des

1350
00:50:16,619 --> 00:50:19,680
limitations, une autre est

1351
00:50:19,680 --> 00:50:20,579
euh

1352
00:50:20,579 --> 00:50:22,920
ouais, faites le  expérience beaucoup plus

1353
00:50:22,920 --> 00:50:24,900
compliquée donc c'est l'une des

1354
00:50:24,900 --> 00:50:27,359
limitations pratiques

1355
00:50:27,359 --> 00:50:30,240
wow excellente réponse me fait penser à un

1356
00:50:30,240 --> 00:50:33,920
papier avec des lettres tournant sur une table

1357
00:50:33,920 --> 00:50:36,780
qui est la rotation des chiffres de bons points

1358
00:50:36,780 --> 00:50:38,460
sur la fovéation et la dynamique de

1359
00:50:38,460 --> 00:50:40,079
l'illusion Je pense que vous avez en fait

1360
00:50:40,079 --> 00:50:42,599
mentionné une illusion qui  Cependant, vous avez

1361
00:50:42,599 --> 00:50:43,980
mentionné dans le contexte de généralisation

1362
00:50:43,980 --> 00:50:46,859
que la rotation sur l'

1363
00:50:46,859 --> 00:50:49,500
écran bidimensionnel ne se généralise pas à trois

1364
00:50:49,500 --> 00:50:52,920
dimensions et que l'effondrement

1365
00:50:52,920 --> 00:50:55,559
ou la réduction dimensionnelle est à la base de la

1366
00:50:55,559 --> 00:50:58,619
projection du cube Illusions et de la rotation du cube et de la figure

1367
00:50:58,619 --> 00:51:01,880
Illusions c'est sur votre écran

1368
00:51:01,880 --> 00:51:05,280
et il y a  une silhouette ou il y a des

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
stimuli ambigus qu'un générateur est proche de la

1371
00:51:09,839 --> 00:51:12,359
criticité ou une bifurcation dans

1372
00:51:12,359 --> 00:51:13,680
des modèles dégénératifs donc cela pourrait

1373
00:51:13,680 --> 00:51:17,160
le représenter d'une manière ou d'une autre

1374
00:51:17,160 --> 00:51:19,920
et donc une grande partie des illusions de commutation

1375
00:51:19,920 --> 00:51:22,020
sont simplement basées sur la planéité des

1376
00:51:22,020 --> 00:51:23,819
images

1377
00:51:23,819 --> 00:51:26,280
et les limitations et la généralisation

1378
00:51:26,280 --> 00:51:28,740
qui sont révélés par ce

1379
00:51:28,740 --> 00:51:32,460
droit ouais ouais je pense qu'il y a même Oh

1380
00:51:32,460 --> 00:51:34,859
oui quelque part désolé il y a du travail ou

1381
00:51:34,859 --> 00:51:35,880
ils

1382
00:51:35,880 --> 00:51:37,619
peuvent dire que les gens ont une

1383
00:51:37,619 --> 00:51:39,480
image en trois dimensions dans leur tête

1384
00:51:39,480 --> 00:51:42,000
comme si même Nancy Ken était une ou ses

1385
00:51:42,000 --> 00:51:45,119
latéraux récemment mais et montrant ouais je

1386
00:51:45,119 --> 00:51:48,119
ne le fais pas  Je ne sais pas si nos modèles ont que

1387
00:51:48,119 --> 00:51:50,460
ce n'est pas très gros

1388
00:51:50,460 --> 00:51:53,700
de toute façon ouais c'est assez intéressant euh, d'

1389
00:51:53,700 --> 00:51:56,160
accord du upcycle Club dans le

1390
00:51:56,160 --> 00:51:58,200
chat, ils ont écrit des félicitations

1391
00:51:58,200 --> 00:52:00,000
si vous êtes capable d'apprendre presque aussi

1392
00:52:00,000 --> 00:52:02,160
efficacement si vous imaginez que vous ne voulez

1393
00:52:02,160 --> 00:52:03,780
qu'un seul neurone soit  actif pour chaque

1394
00:52:03,780 --> 00:52:06,540
exemple, euh, votre modèle va

1395
00:52:06,540 --> 00:52:08,579
essayer de mémoriser la conception de l'ensemble de données

1396
00:52:08,579 --> 00:52:10,980
ou quelque chose comme ça euh

1397
00:52:10,980 --> 00:52:12,180
et vous n'aurez pas assez de

1398
00:52:12,180 --> 00:52:14,940
capacité donc oui, je pense que régler ce

1399
00:52:14,940 --> 00:52:18,359
niveau de parcimonie est certainement

1400
00:52:18,359 --> 00:52:22,200
un facteur important et

1401
00:52:22,200 --> 00:52:25,020
ouais  lorsque vous regardez la probabilité, si

1402
00:52:25,020 --> 00:52:26,220
vous parlez, si vous doublez le

1403
00:52:26,220 --> 00:52:28,579
cadre, cela est généralement équilibré

1404
00:52:28,579 --> 00:52:32,040
automatiquement avec la probabilité elle-même,

1405
00:52:32,040 --> 00:52:33,000
euh,

1406
00:52:33,000 --> 00:52:34,380
si vous ne générez pas de modélisation,

1407
00:52:34,380 --> 00:52:35,760
vous avez juste une pénalité de parcimonie que vous

1408
00:52:35,760 --> 00:52:38,460
voudrez accorder  ce paramètre,

1409
00:52:38,460 --> 00:52:40,980
ok ouais, c'est juste pour clarifier le

1410
00:52:40,980 --> 00:52:43,380
comportement d'emballement dans Armina où le

1411
00:52:43,380 --> 00:52:45,599
réseau devient instable ou chaotique en raison de

1412
00:52:45,599 --> 00:52:47,040
divers facteurs tels que le

1413
00:52:47,040 --> 00:52:50,400
bruit des boucles de rétroaction ou les entrées contradictoires,

1414
00:52:50,400 --> 00:52:52,380
euh

1415
00:52:52,380 --> 00:52:54,180
ouais, je suppose que je n'ai pas regardé cela

1416
00:52:54,180 --> 00:52:55,859
comme un paramètre récurrent où  vous

1417
00:52:55,859 --> 00:52:58,500
auriez des boucles de rétroaction,

1418
00:52:58,500 --> 00:52:59,460
euh,

1419
00:52:59,460 --> 00:53:01,800
mais je pourrais ouais, je pourrais voir

1420
00:53:01,800 --> 00:53:04,319
des exemples contradictoires potentiellement

1421
00:53:04,319 --> 00:53:07,800
affectés par votre niveau de parcimonie,

1422
00:53:07,800 --> 00:53:09,119
euh,

1423
00:53:09,119 --> 00:53:10,859
le point intéressant est qu'est-ce que vous

1424
00:53:10,859 --> 00:53:12,660
seriez plus susceptible ou moins susceptible

1425
00:53:12,660 --> 00:53:16,040
de partager des exemples que je ne connais pas

1426
00:53:16,040 --> 00:53:19,440
bien

1427
00:53:19,440 --> 00:53:21,720
projetant de sparsification à partir d'un  modèle de dimension supérieure entièrement connecté

1428
00:53:21,720 --> 00:53:23,579
en

1429
00:53:23,579 --> 00:53:25,619
progressivement plus petit,

1430
00:53:25,619 --> 00:53:27,540
on comprend assez bien en général

1431
00:53:27,540 --> 00:53:29,760
quels sont les compromis, les calculs sont plus faciles, un

1432
00:53:29,760 --> 00:53:34,079
modèle plus petit, plus clairsemé,

1433
00:53:34,079 --> 00:53:36,420
le graphique de base sera plus clair

1434
00:53:36,420 --> 00:53:39,119
à représenter et il aura également

1435
00:53:39,119 --> 00:53:41,339
tous les autres compromis.  des faux

1436
00:53:41,339 --> 00:53:43,680
positifs et des négatifs de généralisation,

1437
00:53:43,680 --> 00:53:45,720
mais c'est pourquoi il s'agit d'un processus d'ajustement itératif,

1438
00:53:45,720 --> 00:53:47,579
donc

1439
00:53:47,579 --> 00:53:49,760
je suppose que comment votre

1440
00:53:49,760 --> 00:53:52,800
approche de sparsification

1441
00:53:52,800 --> 00:53:55,700


1442
00:53:56,700 --> 00:53:59,520
n'utilise-t-elle pas AIC ou Bic ou une autre

1443
00:53:59,520 --> 00:54:01,619
approche d'ajustement de modèle pour déterminer la

1444
00:54:01,619 --> 00:54:03,660
sparsification pertinente

1445
00:54:03,660 --> 00:54:07,079
pour une entrée donnée,

1446
00:54:07,079 --> 00:54:09,780
comment faites-vous  déterminez à quel point, comme dans la

1447
00:54:09,780 --> 00:54:11,940
régression au lasso, comme comment savez-vous à quel

1448
00:54:11,940 --> 00:54:14,339
point comment définissez-vous le seuil à quel

1449
00:54:14,339 --> 00:54:17,220
point vous voulez que ce soit clairsemé

1450
00:54:17,220 --> 00:54:19,559
ouais, je pense qu'il y a beaucoup de bonne

1451
00:54:19,559 --> 00:54:22,440
littérature à ce sujet et même ainsi, certaines

1452
00:54:22,440 --> 00:54:25,319
personnes les aiment à Harvard et  certaines

1453
00:54:25,319 --> 00:54:29,520
personnes travaillent avec maintenant, euh, ont fait

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
ce genre de

1456
00:54:34,380 --> 00:54:36,780
réseaux de sparsification itératifs déroulés

1457
00:54:36,780 --> 00:54:37,800
où c'est comme un réseau neuronal récurrent

1458
00:54:37,800 --> 00:54:40,380
et separsifie de manière itérative et

1459
00:54:40,380 --> 00:54:41,940
vous pouvez montrer que cela donne quelque chose

1460
00:54:41,940 --> 00:54:45,780
comme une perte rouge ou euh un groupe comme un groupe

1461
00:54:45,780 --> 00:54:47,520
actif, des activations sportives comme

1462
00:54:47,520 --> 00:54:48,960
nous  j'utilise ici

1463
00:54:48,960 --> 00:54:52,859
euh dans ce paramètre euh c'est vraiment juste

1464
00:54:52,859 --> 00:54:55,859
en ayant cette

1465
00:54:55,859 --> 00:54:59,280
euh cette construction de cette variable T

1466
00:54:59,280 --> 00:55:04,079
où nous avons Z en haut et euh

1467
00:55:04,079 --> 00:55:07,859
et puis c'est dans un certain effet contrôlé par

1468
00:55:07,859 --> 00:55:09,119
celles-ci

1469
00:55:09,119 --> 00:55:11,579
la somme des variables U en bas donc

1470
00:55:11,579 --> 00:55:13,200
si W  peut-être que je n'étais pas très clair à

1471
00:55:13,200 --> 00:55:16,500
ce sujet, c'est une matrice qui connecte,

1472
00:55:16,500 --> 00:55:18,359
c'est ce qui définit le groupe donc je

1473
00:55:18,359 --> 00:55:20,400
définis l'université du groupe euh qui

1474
00:55:20,400 --> 00:55:22,380
relie tous ces U ensemble et donc

1475
00:55:22,380 --> 00:55:23,940
l'idée est

1476
00:55:23,940 --> 00:55:27,540
euh comme ici si tous les uns des autres

1477
00:55:27,540 --> 00:55:31,740
exemples si toutes vos utilisations ne sont pas

1478
00:55:31,740 --> 00:55:35,520
actives pour un t donné

1479
00:55:35,520 --> 00:55:38,280
ou si tous les varios sont actifs pour un

1480
00:55:38,280 --> 00:55:41,040
t donné, cette variable t va être très

1481
00:55:41,040 --> 00:55:42,780
petite, c'est vrai parce que votre dénominateur

1482
00:55:42,780 --> 00:55:44,339
va être très grand et cela induit

1483
00:55:44,339 --> 00:55:47,160
une parcimonie donc c'est  euh c'est une

1484
00:55:47,160 --> 00:55:49,260
satisfaction de contrainte si vous avez un si vous avez un

1485
00:55:49,260 --> 00:55:51,839
ensemble de U qui sont tous petits euh alors que

1486
00:55:51,839 --> 00:55:54,480
cette contrainte est satisfaite et

1487
00:55:54,480 --> 00:55:57,180
maintenant Z est autorisé à s'exprimer en quelque sorte

1488
00:55:57,180 --> 00:56:00,240
et c'est ce que alors

1489
00:56:00,240 --> 00:56:02,880
euh en quelque sorte on réalise en ce qui concerne

1490
00:56:02,880 --> 00:56:06,180
l'activation  donc cela est induit par ces

1491
00:56:06,180 --> 00:56:07,020
deux

1492
00:56:07,020 --> 00:56:09,300
termes de divergence euh kale, ils

1493
00:56:09,300 --> 00:56:12,960
disent ici à quelle distance se trouve chaque unhc

1494
00:56:12,960 --> 00:56:15,180
d'une gaussienne, puis grâce à cette

1495
00:56:15,180 --> 00:56:16,980
construction de la variable étudiante T,

1496
00:56:16,980 --> 00:56:20,880
nous construisons effectivement une

1497
00:56:20,880 --> 00:56:23,040
distribution a priori clairsemée uniquement à partir de ces

1498
00:56:23,040 --> 00:56:24,839
gaussiennes, mais en  termes de l'acte, l'

1499
00:56:24,839 --> 00:56:27,599
objectif réel euh les termes et l'

1500
00:56:27,599 --> 00:56:28,920
objectif que nous optimisons ne sont que

1501
00:56:28,920 --> 00:56:31,619
ces deux termes KL qui le poussent

1502
00:56:31,619 --> 00:56:34,020
vers la rareté dans une certaine mesure et cela

1503
00:56:34,020 --> 00:56:36,359
est équilibré automatiquement avec le

1504
00:56:36,359 --> 00:56:39,000
terme de probabilité ici via le décodeur

1505
00:56:39,000 --> 00:56:41,220
donc nous ne le faisons pas.  Nous n'avons pas de termes que nous ajustons

1506
00:56:41,220 --> 00:56:42,839
mais nous apprenons les paramètres de

1507
00:56:42,839 --> 00:56:44,280
ces différents encodeurs, puis

1508
00:56:44,280 --> 00:56:48,200
analysons les échecs et les urgences,

1509
00:56:48,540 --> 00:56:49,920
oh

1510
00:56:49,920 --> 00:56:52,859
très bien, une autre question de Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas qui a écrit en

1512
00:56:55,500 --> 00:56:59,160
parlant du regard et de l'illusion, les

1513
00:56:59,160 --> 00:57:01,920
études sur les constances chez les nourrissons peuvent-elles être

1514
00:57:01,920 --> 00:57:04,260
séparées  dans une illusion de niveau inférieur Rel peut-être une

1515
00:57:04,260 --> 00:57:06,140


1516
00:57:06,140 --> 00:57:09,980
constance conceptuelle de niveau supérieur

1517
00:57:13,619 --> 00:57:15,720
euh pouvez-vous lire

1518
00:57:15,720 --> 00:57:18,300
le type d'architecture actuel,

1519
00:57:18,300 --> 00:57:23,520
les études sur les constances chez les nourrissons euh les

1520
00:57:23,520 --> 00:57:26,640
constances cognitives pourraient-elles être séparées

1521
00:57:26,640 --> 00:57:31,619
ouais probablement je ne le suis pas, je ne suis pas euh

1522
00:57:31,619 --> 00:57:33,900
un expert ou en fait même très familier

1523
00:57:33,900 --> 00:57:35,460
avec

1524
00:57:35,460 --> 00:57:37,859
des études de permanence d'objets similaires, des nourrissons

1525
00:57:37,859 --> 00:57:40,260
et des trucs de constance, mais je pense que ce

1526
00:57:40,260 --> 00:57:42,300
serait incroyablement intéressant d'étudier

1527
00:57:42,300 --> 00:57:44,339
dans les architectures de réseaux neuronaux et

1528
00:57:44,339 --> 00:57:46,740
c'était en quelque sorte une partie de l'idée avec

1529
00:57:46,740 --> 00:57:48,780
cette euh illusion que j'essayais de

1530
00:57:48,780 --> 00:57:51,780
modéliser ici avec cette ligne  Je ne

1531
00:57:51,780 --> 00:57:53,099
sais pas si j'ai été très clair à ce sujet, mais

1532
00:57:53,099 --> 00:57:55,380
la rangée du haut est l'entrée et nous

1533
00:57:55,380 --> 00:57:57,359
bloquons effectivement l'entrée pour

1534
00:57:57,359 --> 00:58:00,119
une seule image et je voulais voir si

1535
00:58:00,119 --> 00:58:03,240
le réseau code en quelque sorte pour que la

1536
00:58:03,240 --> 00:58:05,400
chose soit toujours là.  quand cette image est

1537
00:58:05,400 --> 00:58:07,680
partie, puis-je encore décoder la présence de

1538
00:58:07,680 --> 00:58:10,020
l'objet à partir de l'activité neuronale, euh,

1539
00:58:10,020 --> 00:58:11,819
et qu'est-ce que cela déduit également

1540
00:58:11,819 --> 00:58:13,920
du mouvement en raison du fait qu'il a

1541
00:58:13,920 --> 00:58:15,780
vu les barres à un endroit légèrement différent

1542
00:58:15,780 --> 00:58:18,480
d'avant quand après

1543
00:58:18,480 --> 00:58:20,520
l'image  le cadre a disparu

1544
00:58:20,520 --> 00:58:22,559
euh donc

1545
00:58:22,559 --> 00:58:25,200
oui, je pense qu'il y a définitivement plusieurs

1546
00:58:25,200 --> 00:58:27,240
niveaux,

1547
00:58:27,240 --> 00:58:29,160
euh où certains seraient probablement de

1548
00:58:29,160 --> 00:58:33,180
niveau beaucoup plus bas et euh

1549
00:58:33,180 --> 00:58:35,880
peut-être que la permanence des objets à long terme, je

1550
00:58:35,880 --> 00:58:37,380
suppose que ce serait un

1551
00:58:37,380 --> 00:58:39,059
niveau nettement plus élevé

1552
00:58:39,059 --> 00:58:39,900
euh,

1553
00:58:39,900 --> 00:58:41,760
ça me fait juste penser à ces

1554
00:58:41,760 --> 00:58:44,640
expériences avec des chats  à l'époque

1555
00:58:44,640 --> 00:58:47,280
où c'était comme s'ils les élevaient dans

1556
00:58:47,280 --> 00:58:49,020
l'obscurité sauf qu'une heure par jour, ils

1557
00:58:49,020 --> 00:58:51,000
les mettaient dans le monde vertical ou le

1558
00:58:51,000 --> 00:58:53,160
monde horizontal ou ils ne voyaient que des lignes horizontales

1559
00:58:53,160 --> 00:58:57,299
ou des lignes verticales euh et vous pouvez voir

1560
00:58:57,299 --> 00:58:59,880
l'organisation de leur cortex changer

1561
00:58:59,880 --> 00:59:02,819
comme ils  ont moins de réceptivité, ce sont des

1562
00:59:02,819 --> 00:59:04,200
lignes horizontales s'ils n'ont jamais vu de

1563
00:59:04,200 --> 00:59:06,420
lignes horizontales auparavant, puis vous

1564
00:59:06,420 --> 00:59:07,980
prenez un bâton et vous l'agitez devant

1565
00:59:07,980 --> 00:59:09,599
leur visage et si le bâton est

1566
00:59:09,599 --> 00:59:11,220
horizontal, ils ne font rien,

1567
00:59:11,220 --> 00:59:12,900
c'est vertical, ils le frappent

1568
00:59:12,900 --> 00:59:14,460
ils essaient de le frapper, c'est comme s'ils

1569
00:59:14,460 --> 00:59:15,900
n'étaient littéralement pas obligés de barrer

1570
00:59:15,900 --> 00:59:18,420
devant leur visage, donc je pense que dans ce

1571
00:59:18,420 --> 00:59:20,700
cas, c'est la preuve d'un

1572
00:59:20,700 --> 00:59:24,260
faible niveau de déficience et d'une vision

1573
00:59:24,260 --> 00:59:26,940
contribuant à une sorte d'illusion,

1574
00:59:26,940 --> 00:59:28,980
donc je  Je pense que oui, il pourrait certainement y

1575
00:59:28,980 --> 00:59:30,660
avoir un certain aspect de cela chez les nourrissons.

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
Un point très curieux que vous avez soulevé

1578
00:59:36,420 --> 00:59:40,339
était la variété animée et inanimée, les

1579
00:59:40,339 --> 00:59:43,619
petites choses étant

1580
00:59:43,619 --> 00:59:45,480
intermédiaires,

1581
00:59:45,480 --> 00:59:49,140
qu'est-ce que cela représente

1582
00:59:49,140 --> 00:59:52,319
ou ou est-ce parce qu'elles sont manipulables

1583
00:59:52,319 --> 00:59:55,619
ou cela  ça pourrait être un insecte ou ça pourrait être

1584
00:59:55,619 --> 00:59:57,839
quelque chose qui pourrait s'éloigner juste avec le

1585
00:59:57,839 --> 01:00:01,380
vent ou qu'est-ce que ça veut dire,

1586
01:00:01,380 --> 01:00:04,380
ouais,

1587
01:00:04,380 --> 01:00:08,280
donc c'est un travail comme Talia Conkle, je

1588
01:00:08,280 --> 01:00:11,280
pense que c'est celui qui a découvert cette

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
organisation et ils ont essayé de

1591
01:00:14,880 --> 01:00:16,440
le comprendre  Je ne pense pas, je me trompe peut-être,

1592
01:00:16,440 --> 01:00:19,500
donc je recommande aux gens de lire

1593
01:00:19,500 --> 01:00:21,359
son travail à ce sujet s'ils l'appellent une

1594
01:00:21,359 --> 01:00:23,880
organisation tripartite, mais si je me

1595
01:00:23,880 --> 01:00:25,319
souviens bien,

1596
01:00:25,319 --> 01:00:27,900
ils ont fait beaucoup de travail de suivi sur la raison pour laquelle

1597
01:00:27,900 --> 01:00:30,780
il existe cette organisation et

1598
01:00:30,780 --> 01:00:33,180
des preuves de

1599
01:00:33,180 --> 01:00:35,700
courbure de ces objets et un peu

1600
01:00:35,700 --> 01:00:37,440
comme la distance à laquelle vous voyez les objets

1601
01:00:37,440 --> 01:00:40,260
ou comme

1602
01:00:40,260 --> 01:00:43,319
euh des objets animés ou peut-être plus courbés

1603
01:00:43,319 --> 01:00:45,599
ou il y a là quelle que soit la

1604
01:00:45,599 --> 01:00:46,859
réponse réelle, il y avait beaucoup d'

1605
01:00:46,859 --> 01:00:48,720
hypothèses différentes qui découlaient

1606
01:00:48,720 --> 01:00:51,720
des propriétés similaires de ces objets

1607
01:00:51,720 --> 01:00:54,000
peut-être que les propriétés de niveau intermédiaire ou bas le sont

1608
01:00:54,000 --> 01:00:56,280
plus que les propriétés de niveau supérieur. Je

1609
01:00:56,280 --> 01:00:57,599
ne sais toujours pas si cela a été exactement

1610
01:00:57,599 --> 01:00:59,339
résolu, si c'est comme si l'

1611
01:00:59,339 --> 01:01:01,500
interaction comme vous l'avez dit avec les

1612
01:01:01,500 --> 01:01:04,920
objets provoquait la séparation ou

1613
01:01:04,920 --> 01:01:06,119
euh

1614
01:01:06,119 --> 01:01:09,540
ou ouais, les formes générales de ces

1615
01:01:09,540 --> 01:01:12,299
objets.  je parierais que comme pour la plupart des choses,

1616
01:01:12,299 --> 01:01:13,980
c'est comme une combinaison de tout ce qui

1617
01:01:13,980 --> 01:01:16,980
précède, euh, mais je pense que la

1618
01:01:16,980 --> 01:01:18,480
chose intéressante de ce point de vue de la modélisation

1619
01:01:18,480 --> 01:01:19,859
est que,

1620
01:01:19,859 --> 01:01:21,480
euh,

1621
01:01:21,480 --> 01:01:24,059
cela n'est formé que sur les

1622
01:01:24,059 --> 01:01:26,819
statistiques de corrélation des ensembles de données d'image

1623
01:01:26,819 --> 01:01:28,799
eux-mêmes, donc cela n'a aucune interaction avec cela

1624
01:01:28,799 --> 01:01:32,760
n'a aucune notion d'animation euh je veux dire, c'est

1625
01:01:32,760 --> 01:01:34,140
vraiment juste la formation d'un modèle sur

1626
01:01:34,140 --> 01:01:37,859
imagenet juste des images de chiens chats bateaux

1627
01:01:37,859 --> 01:01:40,020
peu importe et pourtant il réalise toujours ce

1628
01:01:40,020 --> 01:01:41,640
type d'organisation donc il y a une

1629
01:01:41,640 --> 01:01:42,540
sorte de

1630
01:01:42,540 --> 01:01:44,940
caractéristiques sémantiques, n'est-ce

1631
01:01:44,940 --> 01:01:46,740
pas, nous avons une image, nous avons un  réseau

1632
01:01:46,740 --> 01:01:48,359
qui peut classer

1633
01:01:48,359 --> 01:01:51,000
les bateaux par rapport aux chiens par rapport à 20 autres races

1634
01:01:51,000 --> 01:01:53,640
de chiens, mais s'il

1635
01:01:53,640 --> 01:01:55,920
peut également avoir une certaine correspondance

1636
01:01:55,920 --> 01:01:57,900
avec des statistiques de finition de niveau inférieur,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
alors oui, je ne sais pas, je suppose que

1639
01:02:03,960 --> 01:02:07,500
oui, une analogie provocatrice était le

1640
01:02:07,500 --> 01:02:10,380
changement de traduction

1641
01:02:10,380 --> 01:02:12,900
dans le mnist dans l'écriture manuscrite

1642
01:02:12,900 --> 01:02:14,819
réglage de la reconnaissance

1643
01:02:14,819 --> 01:02:18,000
quels sont les changements de traduction

1644
01:02:18,000 --> 01:02:20,160
qui

1645
01:02:20,160 --> 01:02:22,740
existent aujourd'hui quel est l'exemple de trois pixels,

1646
01:02:22,740 --> 01:02:24,780
c'est qu'une

1647
01:02:24,780 --> 01:02:27,540
attaque rapide sur un llm ou quelque chose ou

1648
01:02:27,540 --> 01:02:29,099
quelque chose de vrai, un caractère spécial

1649
01:02:29,099 --> 01:02:32,640
inséré ou ou euh euh une

1650
01:02:32,640 --> 01:02:35,160
superposition sur une image que nous ne pouvons même pas

1651
01:02:35,160 --> 01:02:37,020
détectez cela,

1652
01:02:37,020 --> 01:02:39,359
alors à votre avis, quels sont ces défis

1653
01:02:39,359 --> 01:02:42,839
et quelles sont les façons dont nous pouvons les poursuivre

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
ouais absolument, je veux dire, je pense que c'est un peu

1656
01:02:47,520 --> 01:02:48,420
comme ça que

1657
01:02:48,420 --> 01:02:50,099
j'y pensais, c'est comme

1658
01:02:50,099 --> 01:02:52,680
ces transformations de symétrie,

1659
01:02:52,680 --> 01:02:53,760
euh

1660
01:02:53,760 --> 01:02:55,799
si vous pensez aux modèles de langage,

1661
01:02:55,799 --> 01:02:57,420
vous  Je peux imaginer une

1662
01:02:57,420 --> 01:02:58,500
transformation de symétrie qui revient à

1663
01:02:58,500 --> 01:03:00,240
remplacer un mot par un synonyme ou

1664
01:03:00,240 --> 01:03:03,780
quelque chose comme ça, euh, vous avez pour nous, la phrase

1665
01:03:03,780 --> 01:03:06,000
signifie exactement la même chose mais maintenant,

1666
01:03:06,000 --> 01:03:07,380
tout à coup, le modèle va réagir

1667
01:03:07,380 --> 01:03:09,299
très différemment,

1668
01:03:09,299 --> 01:03:11,240
euh

1669
01:03:11,240 --> 01:03:15,359
comme la traduction entre langues, cela

1670
01:03:15,359 --> 01:03:16,799
peut être vu comme un  type de transformation,

1671
01:03:16,799 --> 01:03:19,440
il préserve le sens sous-jacent

1672
01:03:19,440 --> 01:03:21,960
de l'entrée

1673
01:03:21,960 --> 01:03:24,900
pour nous, mais pour le modèle, cela semble

1674
01:03:24,900 --> 01:03:26,220
complètement différent et nous aimerions

1675
01:03:26,220 --> 01:03:28,380
avoir des modèles qui se comportent de

1676
01:03:28,380 --> 01:03:29,940
manière prévisible par rapport à ces

1677
01:03:29,940 --> 01:03:32,160
types de transformations car

1678
01:03:32,160 --> 01:03:35,040
je pense que les humains se comportent de manière très prévisible

1679
01:03:35,040 --> 01:03:37,319
puisque ces  Les transformations et lorsque

1680
01:03:37,319 --> 01:03:39,920
nous avons affaire à des systèmes d'IA, nous nous attendons

1681
01:03:39,920 --> 01:03:43,200
à ce qu'ils se comportent également de cette façon et je pense

1682
01:03:43,200 --> 01:03:45,480
que cela fait partie de ce qui cause de nombreux

1683
01:03:45,480 --> 01:03:47,339
défis lors de l'interaction avec ces

1684
01:03:47,339 --> 01:03:49,460
systèmes et j'ai en quelque sorte essayé d'en faire une

1685
01:03:49,460 --> 01:03:52,500
démonstration grossière et effrontée

1686
01:03:52,500 --> 01:03:54,960
avec ceci.  ours et carrés et des trucs

1687
01:03:54,960 --> 01:03:58,440
comme euh, nous nous attendons à ce qu'il soit

1688
01:03:58,440 --> 01:04:00,480
capable de faire quelque chose de simple

1689
01:04:00,480 --> 01:04:02,220
comme ça parce que nous pensons que la plupart des humains le

1690
01:04:02,220 --> 01:04:04,020
pourraient et pourtant ce n'est pas le cas et si vous

1691
01:04:04,020 --> 01:04:05,460
imaginez que c'est un scénario critique

1692
01:04:05,460 --> 01:04:07,740
où vous vous attendez à cela et c'est un gros

1693
01:04:07,740 --> 01:04:08,700
problème

1694
01:04:08,700 --> 01:04:11,099
euh  comment pouvons-nous gérer cela, c'est oui, je

1695
01:04:11,099 --> 01:04:12,720
pense que c'est un peu ce que je

1696
01:04:12,720 --> 01:04:15,859
recherche, je pense que la

1697
01:04:16,319 --> 01:04:18,420


1698
01:04:18,420 --> 01:04:22,280
direction que je prends est

1699
01:04:22,280 --> 01:04:26,940
plus simple et ressemble à des

1700
01:04:26,940 --> 01:04:29,460
blocs de construction ascendants d'

1701
01:04:29,460 --> 01:04:31,380
architectures de réseaux neuronaux ou

1702
01:04:31,380 --> 01:04:33,180
d'algorithmes qui

1703
01:04:33,180 --> 01:04:35,760
produisent en quelque sorte ces émergents

1704
01:04:35,760 --> 01:04:37,680
propriétés structurelles et je pense que c'est une

1705
01:04:37,680 --> 01:04:39,839
manière beaucoup plus généralisable plutôt que de

1706
01:04:39,839 --> 01:04:41,579
construire quelque chose au-dessus de ce que nous

1707
01:04:41,579 --> 01:04:43,140
avons déjà,

1708
01:04:43,140 --> 01:04:43,920
euh,

1709
01:04:43,920 --> 01:04:45,839
je pense que c'est quelque chose qui évoluera

1710
01:04:45,839 --> 01:04:47,819
beaucoup mieux et qui correspondra également davantage à ce que fait

1711
01:04:47,819 --> 01:04:50,299
le cerveau,

1712
01:04:50,760 --> 01:04:52,740
très cool, un type de question de mise en œuvre,

1713
01:04:52,740 --> 01:04:54,740
quels sont les

1714
01:04:54,740 --> 01:04:57,000
les exigences de calcul pour simplement exécuter ceci ou à quoi

1715
01:04:57,000 --> 01:04:59,400
ressemble le quotidien d'un

1716
01:04:59,400 --> 01:05:01,920
étudiant ou d'un chercheur qui exécute des variantes

1717
01:05:01,920 --> 01:05:04,380
de ceux-ci, comme utilisent-ils des téraoctets de

1718
01:05:04,380 --> 01:05:07,020
données et que vous utilisez des calculs volumineux

1719
01:05:07,020 --> 01:05:08,940
ou est-ce quelque chose que les gens peuvent exécuter

1720
01:05:08,940 --> 01:05:11,880
par eux-mêmes  ordinateurs portables,

1721
01:05:11,880 --> 01:05:13,980
je pense que presque tout ce que j'ai présenté

1722
01:05:13,980 --> 01:05:17,099
aujourd'hui peut être exécuté localement, donc comme si

1723
01:05:17,099 --> 01:05:20,040
ce truc est super simple, vous pouvez l'exécuter, je veux

1724
01:05:20,040 --> 01:05:20,760
dire, vous allez

1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
penser que vous pouvez l'exécuter sur votre ordinateur portable si

1727
01:05:24,299 --> 01:05:25,980
vous voulez aimer vous entraîner et expérimenter

1728
01:05:25,980 --> 01:05:27,420
différentes choses ça va  être

1729
01:05:27,420 --> 01:05:30,119
assez lent, donc je recommanderais un

1730
01:05:30,119 --> 01:05:33,359
GPU commercial comme un sur lequel j'exécute à peu près

1731
01:05:33,359 --> 01:05:35,640
tout comme des Nvidia 1080

1732
01:05:35,640 --> 01:05:38,819
assez vieux, assez bon marché mais ils ont 12

1733
01:05:38,819 --> 01:05:41,099
Go de RAM ou autre et c'est

1734
01:05:41,099 --> 01:05:43,140
plus que suffisant pour ces modèles quatre

1735
01:05:43,140 --> 01:05:46,440
gigaoctets de RAM  Je pense qu'une chose que

1736
01:05:46,440 --> 01:05:48,839
certaines personnes trouvent bizarre, c'est que je fais la plupart

1737
01:05:48,839 --> 01:05:51,480
de mes expériences sur des trucs comme mnist, donc

1738
01:05:51,480 --> 01:05:54,780
ce sont des images de 32 x 32 pixels parce que je peux

1739
01:05:54,780 --> 01:05:57,299
l'entraîner petit et vocalement,

1740
01:05:57,299 --> 01:06:00,000
euh si vous voulez le faire, mes expériences

1741
01:06:00,000 --> 01:06:02,460
ou une infinité si vous  je veux faire des choses

1742
01:06:02,460 --> 01:06:03,540
comme celle-ci, c'est beaucoup plus

1743
01:06:03,540 --> 01:06:05,640
compliqué, cette suite dynamique hamiltonienne,

1744
01:06:05,640 --> 01:06:08,160
ici, vous entrez dans

1745
01:06:08,160 --> 01:06:09,780
des modèles plus gros qui fonctionnent sur plusieurs

1746
01:06:09,780 --> 01:06:12,240
GPU et voici donc l'utilisation d'un cluster pour

1747
01:06:12,240 --> 01:06:14,220
exécuter ces types de modèles,

1748
01:06:14,220 --> 01:06:16,140
euh, mais je dirais que la plupart des  la seule

1749
01:06:16,140 --> 01:06:18,920
machine avec le GPU est plus que suffisante

1750
01:06:18,920 --> 01:06:21,680
ou même comme dans un notebook collaboratif

1751
01:06:21,680 --> 01:06:24,539
quelque chose comme ça si vous voulez entraîner

1752
01:06:24,539 --> 01:06:26,520
quelque chose sur imagenet cela devient plus

1753
01:06:26,520 --> 01:06:29,940
compliqué et vous avez besoin

1754
01:06:29,940 --> 01:06:33,660
d'au moins un GPU idéalement de plus mais ouais je

1755
01:06:33,660 --> 01:06:35,160
ne fais pas de  beaucoup de choses à grande échelle,

1756
01:06:35,160 --> 01:06:37,200
mais je pense que c'est certainement intéressant

1757
01:06:37,200 --> 01:06:39,960
et que vous pouvez certainement faire beaucoup plus,

1758
01:06:39,960 --> 01:06:42,539
mais pour certaines de ces

1759
01:06:42,539 --> 01:06:45,480
questions plus simples ou plus fondamentales,

1760
01:06:45,480 --> 01:06:47,760
je ne sais pas comment vous voulez l'appeler, euh,

1761
01:06:47,760 --> 01:06:52,500
une machine plus petite est  sympa et rapide donc

1762
01:06:52,500 --> 01:06:54,660
cool utile

1763
01:06:54,660 --> 01:06:58,140
d'accord Je lirai un commentaire de Dave

1764
01:06:58,140 --> 01:07:00,780
rappelant le commentaire de Bert DeVries lors

1765
01:07:00,780 --> 01:07:02,880
du symposium d'inférence active appliquée

1766
01:07:02,880 --> 01:07:05,520
sur l'opportunité de consacrer moins d'

1767
01:07:05,520 --> 01:07:08,099
efforts ou d'ATP à la recherche de nourriture ou

1768
01:07:08,099 --> 01:07:09,780
aux situations de contrôle où nous n'avons pas besoin de beaucoup de

1769
01:07:09,780 --> 01:07:11,579
précision.  Je ne sais pas si vous écoutez

1770
01:07:11,579 --> 01:07:13,859
ceci, mais le professeur DeVries a mentionné les

1771
01:07:13,859 --> 01:07:17,520
modèles de précision variable et la manière dont

1772
01:07:17,520 --> 01:07:19,440
ils pourraient être utilisés pour

1773
01:07:19,440 --> 01:07:21,059
activer différentes fonctionnalités de

1774
01:07:21,059 --> 01:07:23,039
généralisation et de formation structurelle réelle

1775
01:07:23,039 --> 01:07:25,020
ainsi que des

1776
01:07:25,020 --> 01:07:27,059
exigences de calcul réduites. A-t-

1777
01:07:27,059 --> 01:07:29,880
il des suggestions sur la façon d'

1778
01:07:29,880 --> 01:07:31,980
introduire cela ?  distinction en

1779
01:07:31,980 --> 01:07:33,900
théorie de l'inférence active, quels types d'

1780
01:07:33,900 --> 01:07:37,760
expériences pourraient faire ressortir cela

1781
01:07:38,400 --> 01:07:40,680
oh wow ouais c'est quelque chose que je

1782
01:07:40,680 --> 01:07:42,660
ne pense pas avoir trop d'intelligence

1783
01:07:42,660 --> 01:07:46,619
à dire à ce sujet, c'est tout à fait honnête,

1784
01:07:46,619 --> 01:07:48,740
euh,

1785
01:07:51,359 --> 01:07:53,460
c'est une question super intéressante parce que

1786
01:07:53,460 --> 01:07:56,220
je pense que l'intuition fait un  cela a beaucoup de

1787
01:07:56,220 --> 01:07:58,260
sens pour moi que vous

1788
01:07:58,260 --> 01:08:00,000
parlez

1789
01:08:00,000 --> 01:08:01,980
si je comprends bien les taux

1790
01:08:01,980 --> 01:08:05,160
de précision variables lorsque vous encodez dans ou

1791
01:08:05,160 --> 01:08:07,079
ou dans votre modèle en général en faisant du

1792
01:08:07,079 --> 01:08:07,740
calcul

1793
01:08:07,740 --> 01:08:09,420
[Musique]

1794
01:08:09,420 --> 01:08:11,539
euh,

1795
01:08:13,200 --> 01:08:17,040
cela a d'une manière ou d'une autre un impact sur vos

1796
01:08:17,040 --> 01:08:19,080
performances futures en tant que  une relation avec une

1797
01:08:19,080 --> 01:08:22,259
réserve d'énergie, je pense que oui et si vous

1798
01:08:22,259 --> 01:08:23,580
vouliez intégrer cela dans un

1799
01:08:23,580 --> 01:08:26,279
système d'effort actif, vous auriez

1800
01:08:26,279 --> 01:08:28,679
vraiment besoin d'un système incarné dans lequel l'

1801
01:08:28,679 --> 01:08:31,500
agent a une certaine notion d'énergie comme une

1802
01:08:31,500 --> 01:08:34,439
réserve d'énergie interne et

1803
01:08:34,439 --> 01:08:36,238
oui, quelque chose qui essaie

1804
01:08:36,238 --> 01:08:38,520
pour conserver pendant qu'il effectue ses

1805
01:08:38,520 --> 01:08:40,500
actions, euh

1806
01:08:40,500 --> 01:08:42,359
et qu'il manque d'énergie, il faudrait

1807
01:08:42,359 --> 01:08:44,759
quelque chose de mauvais pour les agents et

1808
01:08:44,759 --> 01:08:47,399
alors peut-être pourriez-vous observer une sorte d'

1809
01:08:47,399 --> 01:08:48,679
émergence,

1810
01:08:48,679 --> 01:08:52,040
euh, de réduction et

1811
01:08:52,040 --> 01:08:55,198
de précision d'encodage ou quelque chose comme

1812
01:08:55,198 --> 01:08:57,479
ça alors que l'agent essaie d'apprendre

1813
01:08:57,479 --> 01:09:00,060
à  agissez plus efficacement, vous devrez peut-être

1814
01:09:00,060 --> 01:09:02,520
lui donner la capacité de contrôler sa

1815
01:09:02,520 --> 01:09:04,020
précision

1816
01:09:04,020 --> 01:09:07,080
ouais, comme je le dis en dehors de mon expertise, mais

1817
01:09:07,080 --> 01:09:08,819
c'est un peu une pensée, d'

1818
01:09:08,819 --> 01:09:11,460
accord sur cette diapositive ici, première

1819
01:09:11,460 --> 01:09:14,219
image très cool, c'est un peu comme un

1820
01:09:14,219 --> 01:09:18,359
Jackson Pollock numérique,

1821
01:09:18,359 --> 01:09:22,920
euh si c'était le cas  une taille de données d'entrée plus simple

1822
01:09:22,920 --> 01:09:26,520
ou simplement une complexité réduite des

1823
01:09:26,520 --> 01:09:27,719
modèles ou s'il

1824
01:09:27,719 --> 01:09:30,000
s'agissait d'une complexité accrue, en quoi cette image serait-elle

1825
01:09:30,000 --> 01:09:31,920
différente

1826
01:09:31,920 --> 01:09:34,620
ouais alors j'ai fait quelques expériences en essayant de

1827
01:09:34,620 --> 01:09:36,738
changer ces

1828
01:09:36,738 --> 01:09:40,439
colonnes d'orientation et

1829
01:09:40,439 --> 01:09:41,100
euh

1830
01:09:41,100 --> 01:09:43,140
vous pouvez ouais, en gros, changer les

1831
01:09:43,140 --> 01:09:44,520
paramètres du modèle, vous pouvez  faites en sorte que

1832
01:09:44,520 --> 01:09:47,100
ces colonnes soient plus grandes, vous pouvez

1833
01:09:47,100 --> 01:09:49,380
les faire en sorte qu'elles n'aient pas une structure très similaire

1834
01:09:49,380 --> 01:09:51,660
à celle que nous voyons chez les humains

1835
01:09:51,660 --> 01:09:53,040
chez nous, vous pouvez leur faire avoir plus de

1836
01:09:53,040 --> 01:09:55,199
bandes d'activité

1837
01:09:55,199 --> 01:09:56,160
euh

1838
01:09:56,160 --> 01:09:58,199
et comme vous l'avez dit, cela dépend de

1839
01:09:58,199 --> 01:10:00,540
l'ensemble de données  que vous utilisez si j'utilise des

1840
01:10:00,540 --> 01:10:03,179
classements sinusoïdaux très simples

1841
01:10:03,179 --> 01:10:05,580
comme entrée, j'obtiens quelque chose comme ça, j'obtiens

1842
01:10:05,580 --> 01:10:07,920
quelque chose qui est un peu plus euh,

1843
01:10:07,920 --> 01:10:11,640
courbé en rotation, avec une entropie plus élevée,

1844
01:10:11,640 --> 01:10:12,800


1845
01:10:12,800 --> 01:10:15,660
donc je pense que ce sont toutes

1846
01:10:15,660 --> 01:10:18,000
des choses intéressantes si vous voulez étudier l'

1847
01:10:18,000 --> 01:10:19,320
émergence de  ce type d'organisation

1848
01:10:19,320 --> 01:10:22,199
dans un système naturel euh si vous avez un

1849
01:10:22,199 --> 01:10:24,120
modèle qui produit maintenant une

1850
01:10:24,120 --> 01:10:25,860
organisation différente pour différents

1851
01:10:25,860 --> 01:10:28,620
paramètres, c'est à voir, alors quels

1852
01:10:28,620 --> 01:10:31,679
paramètres correspondent le mieux à nos données observées

1853
01:10:31,679 --> 01:10:32,340
euh,

1854
01:10:32,340 --> 01:10:34,679
alors oui, je peux,

1855
01:10:34,679 --> 01:10:36,540
je peux les envoyer autour de vous si vous êtes

1856
01:10:36,540 --> 01:10:38,520
intéressé mais

1857
01:10:38,520 --> 01:10:40,580
euh

1858
01:10:41,580 --> 01:10:44,219
ouais, je pense qu'un autre aussi désolé,

1859
01:10:44,219 --> 01:10:45,659
un autre point intéressant, c'est

1860
01:10:45,659 --> 01:10:46,920
que les

1861
01:10:46,920 --> 01:10:51,480
différents animaux et types de

1862
01:10:51,480 --> 01:10:53,219
sélectivité d'orientation et différents

1863
01:10:53,219 --> 01:10:54,659
nombres de moulinets, certains animaux ne

1864
01:10:54,659 --> 01:10:57,420
l'ont pas du tout, je pense que peut-être des souris si je suis  c'est

1865
01:10:57,420 --> 01:11:00,120
vrai, j'ai ce genre de euh qu'ils appellent une

1866
01:11:00,120 --> 01:11:01,800
sélectivité sel et poivre, donc c'est

1867
01:11:01,800 --> 01:11:03,480
fondamentalement aléatoire, vous n'avez aucune

1868
01:11:03,480 --> 01:11:04,679
sorte de sensibilité à l'orientation topographique,

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
donc il y a des preuves que

1871
01:11:09,300 --> 01:11:10,920
oui, différents systèmes font cela

1872
01:11:10,920 --> 01:11:13,020
différemment et c'est intéressant de

1873
01:11:13,020 --> 01:11:14,760
comprendre pourquoi

1874
01:11:14,760 --> 01:11:17,760
oui ça  c'est très cool, cela me rappelle d'

1875
01:11:17,760 --> 01:11:21,300
abord la

1876
01:11:21,300 --> 01:11:22,980
base et le temps de diffusion de la réaction,

1877
01:11:22,980 --> 01:11:25,739
donc il est en fait

1878
01:11:25,739 --> 01:11:30,000
possible qu'une région n'ait aucune

1879
01:11:30,000 --> 01:11:32,840
activité à partir d'une

1880
01:11:32,840 --> 01:11:35,640
granularité donnée, comme si elle était

1881
01:11:35,640 --> 01:11:39,360
examinée à l'échelle spatiale et temporelle de l'IRM

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
si les poches d'activité mais  si les

1884
01:11:44,699 --> 01:11:46,380
poches d'activité sont

1885
01:11:46,380 --> 01:11:48,480
plus lentes et plus rapides

1886
01:11:48,480 --> 01:11:52,260
que cette mesure ne sera pas

1887
01:11:52,260 --> 01:11:54,060
différente du bruit, tout aura été

1888
01:11:54,060 --> 01:11:55,739
moyenné,

1889
01:11:55,739 --> 01:11:58,620
donc il pourrait y avoir des

1890
01:11:58,620 --> 01:12:01,560
ensembles de données intéressants comme des ensembles de données qui

1891
01:12:01,560 --> 01:12:03,360
ont en fait beaucoup de

1892
01:12:03,360 --> 01:12:06,179
richesse, mais alors pour un  pour une raison ou

1893
01:12:06,179 --> 01:12:08,520
une autre, la moyenne a simplement été calculée

1894
01:12:08,520 --> 01:12:11,100
parce qu'elle n'était pas connectée à vous

1895
01:12:11,100 --> 01:12:12,420
ou quelque chose comme ça, vous devez vraiment

1896
01:12:12,420 --> 01:12:14,520
utiliser un seul niveau d'essai, vous devez

1897
01:12:14,520 --> 01:12:16,140
avoir une résolution spatiale suffisamment élevée

1898
01:12:16,140 --> 01:12:18,719
pour que vous sachiez qu'elle satisfait aux

1899
01:12:18,719 --> 01:12:23,100
microfréquences, euh  et c'est juste

1900
01:12:23,100 --> 01:12:24,659
quelque chose que les gens n'ont pas fait depuis

1901
01:12:24,659 --> 01:12:25,620
longtemps, surtout si vous faites

1902
01:12:25,620 --> 01:12:27,780
des enregistrements d'un seul électorat, vous n'allez pas

1903
01:12:27,780 --> 01:12:28,920
voir d'onde progressive, vous

1904
01:12:28,920 --> 01:12:30,900
allez voir des oscillations,

1905
01:12:30,900 --> 01:12:32,219
euh, donc vous avez besoin d'un multi-électrique

1906
01:12:32,219 --> 01:12:34,199
et fondamentalement, ils disent d'accord,

1907
01:12:34,199 --> 01:12:36,000
ouais, maintenant que nous avons la technologie pour le faire,

1908
01:12:36,000 --> 01:12:37,520
autant de choses

1909
01:12:37,520 --> 01:12:40,080
persistent que nous n'avions pas vues auparavant et

1910
01:12:40,080 --> 01:12:42,960
potentiellement c'est une explication pour une

1911
01:12:42,960 --> 01:12:44,400
grande partie du bruit que nous voyions

1912
01:12:44,400 --> 01:12:46,260
avant, peut-être que c'est vraiment juste  vagues voyageuses

1913
01:12:46,260 --> 01:12:47,219


1914
01:12:47,219 --> 01:12:47,760
euh

1915
01:12:47,760 --> 01:12:51,480
alors oui, je pense qu'il y a beaucoup à faire

1916
01:12:51,480 --> 01:12:53,880
dans le futur avec des

1917
01:12:53,880 --> 01:12:56,520
capacités accrues d'enregistrement,

1918
01:12:56,520 --> 01:12:58,739
c'est très cool,

1919
01:12:58,739 --> 01:13:02,940
toutes les dernières réflexions ou questions ou

1920
01:13:02,940 --> 01:13:06,239
où vas-tu emmener ce travail

1921
01:13:06,239 --> 01:13:08,520
ouais non merci de m'avoir, euh,

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
j'espère dans l'infrastructure active  c'est ça que

1924
01:13:11,640 --> 01:13:14,520
j'adorerais, je pense que

1925
01:13:14,520 --> 01:13:16,560
ce serait super amusant, alors ouais, je ne suis pas

1926
01:13:16,560 --> 01:13:18,980
vraiment sûr de regarder Peut-être de la musique,

1927
01:13:18,980 --> 01:13:21,659
euh en ce moment, euh, je

1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760
regarde euh,

1930
01:13:26,760 --> 01:13:30,420
d'autres autres directions folles, je ne

1931
01:13:30,420 --> 01:13:33,020
veux pas avoir l'air trop fou, euh

1932
01:13:33,020 --> 01:13:36,900
mais  Je vais aborder ouais beaucoup de choses, donc

1933
01:13:36,900 --> 01:13:38,580
une chose qui s'en vient, quelque chose que

1934
01:13:38,580 --> 01:13:40,320
nous avons soumis aux neurops, c'est l'étude de la

1935
01:13:40,320 --> 01:13:43,140
mémoire avec des ondes progressives,

1936
01:13:43,140 --> 01:13:45,060
euh, donc cet article vient de sortir dans les

1937
01:13:45,060 --> 01:13:46,860
archives aujourd'hui, euh

1938
01:13:46,860 --> 01:13:48,840
comment les ondes sont vraiment bonnes pour encoder les

1939
01:13:48,840 --> 01:13:50,580
souvenirs à long terme.  ce qui, je pense, est

1940
01:13:50,580 --> 01:13:52,100
super intéressant,

1941
01:13:52,100 --> 01:13:54,120
donc je pourrais aller un peu dans cette

1942
01:13:54,120 --> 01:13:55,800
direction, ça

1943
01:13:55,800 --> 01:13:58,920
sonne bien et oui, ce serait très

1944
01:13:58,920 --> 01:14:01,400
excitant de voir l'action entrer en jeu

1945
01:14:01,400 --> 01:14:04,560
quand les neurones restaient

1946
01:14:04,560 --> 01:14:07,620
actifs même lorsque les pieds du chien bougeaient,

1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
il y a beaucoup de choses comme  des séquences d'action

1949
01:14:11,480 --> 01:14:14,280
comme lancer une balle de baseball, puis ça

1950
01:14:14,280 --> 01:14:15,900
s'en va et c'est comme s'il y avait quelque chose

1951
01:14:15,900 --> 01:14:18,440
dans cette action qui continue d'

1952
01:14:18,440 --> 01:14:21,179
influencer et donc d'avoir une

1953
01:14:21,179 --> 01:14:23,699
représentation temporelle profonde d'actions alternatives

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
et puis l'auto-encodeur variationnel est

1956
01:14:29,640 --> 01:14:33,179
déjà fondamentalement le bon, quelque chose

1957
01:14:33,179 --> 01:14:35,219
comme ça, donc je

1958
01:14:35,219 --> 01:14:37,739
l'apprécie vraiment  très bien merci

1959
01:14:37,739 --> 01:14:39,480
à la prochaine fois

1960
01:14:39,480 --> 01:14:43,339
merci beaucoup au revoir


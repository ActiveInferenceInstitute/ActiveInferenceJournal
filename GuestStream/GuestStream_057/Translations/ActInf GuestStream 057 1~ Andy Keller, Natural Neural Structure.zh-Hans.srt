1
00:00:06,600 --> 00:00:09,420
大家好，欢迎来到 2023 年 9 月 18 日，

2
00:00:09,420 --> 00:00:14,040
这是

3
00:00:14,040 --> 00:00:16,740
与安迪·凯勒 (Andy Keller) 的活跃嘉宾直播 57.1，我们将

4
00:00:16,740 --> 00:00:19,619
讨论人工智能的自然神经结构，

5
00:00:19,619 --> 00:00:22,020
届时将

6
00:00:22,020 --> 00:00:24,300
有一个演示，然后进行

7
00:00:24,300 --> 00:00:25,800
讨论，因此如果您正在观看直播，

8
00:00:25,800 --> 00:00:27,900
请随时观看 在实时聊天中写下问题，

9
00:00:27,900 --> 00:00:30,660
否则谢谢安迪，

10
00:00:30,660 --> 00:00:32,279
真的很期待它，也谢谢

11
00:00:32,279 --> 00:00:35,899
你的演示，是的，

12
00:00:36,360 --> 00:00:38,940
非常感谢，谢谢你邀请

13
00:00:38,940 --> 00:00:41,280
我，我非常兴奋能够

14
00:00:41,280 --> 00:00:42,840
与活跃的参考小组一起展示这些东西，

15
00:00:42,840 --> 00:00:45,239
我 我是一个粉丝并且非常

16
00:00:45,239 --> 00:00:49,200
感兴趣所以希望呃是的能够进行

17
00:00:49,200 --> 00:00:50,399
一次很好的讨论并看看

18
00:00:50,399 --> 00:00:51,960
你们对此有何看法

19
00:00:51,960 --> 00:00:54,840
嗯所以我的名字是安迪我正在完成我

20
00:00:54,840 --> 00:00:57,180
在阿姆斯特丹大学麦克斯韦林指导下的博士学位

21
00:00:57,180 --> 00:00:59,100


22
00:00:59,100 --> 00:01:01,500
嗯我 在此之后开始在哈佛做博士后，

23
00:01:01,500 --> 00:01:05,099
所以我要开始我只是在谈论

24
00:01:05,099 --> 00:01:07,619
我工作的总体目标是

25
00:01:07,619 --> 00:01:09,540
尝试使现代

26
00:01:09,540 --> 00:01:12,540
人工智能更接近于更接近人类的

27
00:01:12,540 --> 00:01:15,240
概括，所以我们的意思

28
00:01:15,240 --> 00:01:17,159
可能是一些 某种结构

29
00:01:17,159 --> 00:01:18,960
概括，

30
00:01:18,960 --> 00:01:20,700
或者可能是活跃婴儿委员会更熟悉的，

31
00:01:20,700 --> 00:01:22,080
比如

32
00:01:22,080 --> 00:01:24,299
我们相信人类拥有的结构化世界模型，

33
00:01:24,299 --> 00:01:26,340
我们建议实现

34
00:01:26,340 --> 00:01:28,799
这一目标的方式是将自然神经

35
00:01:28,799 --> 00:01:32,400
结构集成到人工智能中，

36
00:01:32,400 --> 00:01:34,979
所以首先让我们定义我们的意思 通过

37
00:01:34,979 --> 00:01:36,720
结构泛化，

38
00:01:36,720 --> 00:01:38,400
所以我认为

39
00:01:38,400 --> 00:01:40,380
说现代机器学习

40
00:01:40,380 --> 00:01:42,900
泛化超出了传统意义上的训练集，这是相当没有争议的，

41
00:01:42,900 --> 00:01:45,000
例如，

42
00:01:45,000 --> 00:01:46,799
即使是最早的人工神经

43
00:01:46,799 --> 00:01:49,020
网络多层感知器也可以

44
00:01:49,020 --> 00:01:51,659
在这样的图像数据集上进行训练，

45
00:01:51,659 --> 00:01:55,140
并实现高 然后，当

46
00:01:55,140 --> 00:01:56,880
他们看到一

47
00:01:56,880 --> 00:01:58,320
组他们以前从未见过的测试图像时，

48
00:01:58,320 --> 00:02:00,240
他们仍然可以

49
00:02:00,240 --> 00:02:02,460
相对容易地以相同的

50
00:02:02,460 --> 00:02:04,500
准确度对它们进行分类，这就是我们通常

51
00:02:04,500 --> 00:02:07,320
所说的泛化，但即使在

52
00:02:07,320 --> 00:02:08,758
很早的时候，它也是如此 注意到这些

53
00:02:08,758 --> 00:02:10,800
系统确实在处理

54
00:02:10,800 --> 00:02:12,720
应用于图像的小位移或变形方面遇到困难，

55
00:02:12,720 --> 00:02:16,340
例如，如果

56
00:02:18,920 --> 00:02:23,099
你认为这为什么令人惊讶，

57
00:02:23,099 --> 00:02:24,720
我认为这实际上是由于我们

58
00:02:24,720 --> 00:02:26,640
执行这种类型的

59
00:02:26,640 --> 00:02:28,680
结构概括的天生能力，所以这个

60
00:02:28,680 --> 00:02:31,920
例子是失败的 呃，

61
00:02:31,920 --> 00:02:33,239
例如，这种转变对

62
00:02:33,239 --> 00:02:35,340
我们来说几乎察觉不到，我们会自动处理它，

63
00:02:35,340 --> 00:02:37,560
而在系统中，这

64
00:02:37,560 --> 00:02:39,959
显然是一个主要问题，所以用语言来说，

65
00:02:39,959 --> 00:02:41,940
我们可以说结构泛化

66
00:02:41,940 --> 00:02:44,819
是对

67
00:02:44,819 --> 00:02:47,040
输入的某些对称变换的泛化，或者在这种

68
00:02:47,040 --> 00:02:48,959
情况下是对称性 转换是一个

69
00:02:48,959 --> 00:02:50,580
小转变，使数字类别保持

70
00:02:50,580 --> 00:02:52,019
不变，

71
00:02:52,019 --> 00:02:54,599
所以明显的问题是

72
00:02:54,599 --> 00:02:56,340
我们所说的这种自然

73
00:02:56,340 --> 00:02:58,860
结构到底是什么意思，为什么我们认为这

74
00:02:58,860 --> 00:03:01,620
会帮助我们进行这些设置，

75
00:03:01,620 --> 00:03:03,780
所以首先让我们谈谈

76
00:03:03,780 --> 00:03:05,819
自然神经的含义 结构

77
00:03:05,819 --> 00:03:08,700
um 谈论

78
00:03:08,700 --> 00:03:11,640
系统中的结构或任何类型的偏差的一种方式是

79
00:03:11,640 --> 00:03:14,040
归纳偏差，因此归纳偏差

80
00:03:14,040 --> 00:03:16,080
可以宽松地定义为

81
00:03:16,080 --> 00:03:17,940


82
00:03:17,940 --> 00:03:19,440
当您

83
00:03:19,440 --> 00:03:22,440
更通俗地进行模型选择时，对一组可实现假设的适当限制。 可以

84
00:03:22,440 --> 00:03:24,480
像在看到任何数据之前那样称呼它，

85
00:03:24,480 --> 00:03:27,060
它限制了

86
00:03:27,060 --> 00:03:29,580
您可以学习什么以及如何学习，所以非常广泛，这可以

87
00:03:29,580 --> 00:03:32,519
包括从模型类到

88
00:03:32,519 --> 00:03:34,739
优化过程甚至超

89
00:03:34,739 --> 00:03:37,379
参数的任何内容，从某种意义上说，它们真正

90
00:03:37,379 --> 00:03:39,739
定义了

91
00:03:39,739 --> 00:03:43,140
可以学习的内容 它定义了

92
00:03:43,140 --> 00:03:44,819
泛化，因为如果

93
00:03:44,819 --> 00:03:47,220


94
00:03:47,220 --> 00:03:48,420
没有一些

95
00:03:48,420 --> 00:03:50,220
归纳解决方案，你实际上无法泛化到训练集之外，

96
00:03:50,220 --> 00:03:52,560
David Wolford 在本文中对此进行了更彻底的解释，

97
00:03:52,560 --> 00:03:55,500
因此我们所说的自然

98
00:03:55,500 --> 00:03:58,620
归纳偏差是指

99
00:03:58,620 --> 00:04:00,000
源于限制和

100
00:04:00,000 --> 00:04:02,040
局限性的偏差 自然系统面临着必须

101
00:04:02,040 --> 00:04:04,500


102
00:04:04,500 --> 00:04:06,900
生活在现实世界中的本质，例如

103
00:04:06,900 --> 00:04:08,459
大脑由于其构造的本质而有许多效率约束

104
00:04:08,459 --> 00:04:10,439
和物理约束，

105
00:04:10,439 --> 00:04:13,140
并且遵循这种

106
00:04:13,140 --> 00:04:14,580
逻辑，那么这些约束

107
00:04:14,580 --> 00:04:16,620
在我们的概括中确实发挥了一些作用

108
00:04:16,620 --> 00:04:19,798
目前超越现代

109
00:04:19,798 --> 00:04:21,478
人工智能的能力，我们将在

110
00:04:21,478 --> 00:04:24,720
接下来的讨论中，因此在本次演讲中，我将

111
00:04:24,720 --> 00:04:27,540
特别关注

112
00:04:27,540 --> 00:04:29,880
我的工作所研究的两种类型的结构，即

113
00:04:29,880 --> 00:04:31,699
地形组织和

114
00:04:31,699 --> 00:04:34,320
时空动力学，在我

115
00:04:34,320 --> 00:04:36,120
开始我的工作之前，我' 我将举一个简短的例子来说明

116
00:04:36,120 --> 00:04:38,759
为什么我相信自然结构

117
00:04:38,759 --> 00:04:41,400
可能有助于实现

118
00:04:41,400 --> 00:04:42,780
我之前讨论的结构泛化，

119
00:04:42,780 --> 00:04:44,280


120
00:04:44,280 --> 00:04:47,479
所以第一个例子来自 20 世纪 80 年代的呃

121
00:04:47,479 --> 00:04:49,199
hukushima 的新认知前端

122
00:04:49,199 --> 00:04:51,479
架构，

123
00:04:51,479 --> 00:04:53,520
它实际上是为了

124
00:04:53,520 --> 00:04:55,440
直接解决问题而构建的

125
00:04:55,440 --> 00:04:57,300
对这些小变化和变形的鲁棒性问题，

126
00:04:57,300 --> 00:04:59,759
所以在文书工作中，他

127
00:04:59,759 --> 00:05:02,040
写下了来自瞳孔和

128
00:05:02,040 --> 00:05:04,380
黄鼠狼对层次结构和

129
00:05:04,380 --> 00:05:06,780
池化的测量的灵感，以便实现

130
00:05:06,780 --> 00:05:08,699
对这些扭曲的鲁棒性，所以如果你看

131
00:05:08,699 --> 00:05:11,160
一下图中，如果他写下 U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 这些代表简单和

133
00:05:14,100 --> 00:05:16,919
复杂的细胞，所以这在当时是一种相当

134
00:05:16,919 --> 00:05:18,840
激进的方法，但它

135
00:05:18,840 --> 00:05:20,699
确实有助于提高

136
00:05:20,699 --> 00:05:22,199
我们困扰这些早期

137
00:05:22,199 --> 00:05:24,419
人工神经网络的鲁棒性和转变，随着时间的推移，

138
00:05:24,419 --> 00:05:25,860
这些想法被简化和

139
00:05:25,860 --> 00:05:28,919
抽象， 显然产生了

140
00:05:28,919 --> 00:05:30,539
卷积神经网络，所以我们

141
00:05:30,539 --> 00:05:32,759
今天知道它最终推动了

142
00:05:32,759 --> 00:05:35,580
深度学习革命的成功，所以这

143
00:05:35,580 --> 00:05:36,960
实际上是实现结构泛化的自然归纳偏差的一个例子，

144
00:05:36,960 --> 00:05:39,479


145
00:05:39,479 --> 00:05:42,120
所以对于我们的研究来说，

146
00:05:42,120 --> 00:05:43,919
尝试

147
00:05:43,919 --> 00:05:45,900
理解是什么使这些变得非常有趣 模型工作得

148
00:05:45,900 --> 00:05:47,280
很好，

149
00:05:47,280 --> 00:05:49,320
看看这个原理是否可以

150
00:05:49,320 --> 00:05:51,360
推广到

151
00:05:51,360 --> 00:05:53,940
更抽象的变换

152
00:05:53,940 --> 00:05:57,000
和对称性，

153
00:05:57,000 --> 00:06:00,360
那么是什么让卷积直观地实现了这种

154
00:06:00,360 --> 00:06:02,060
结构泛化，

155
00:06:02,060 --> 00:06:04,440
你可以看到这是通过

156
00:06:04,440 --> 00:06:06,720
在 或 处应用相同的过滤器或

157
00:06:06,720 --> 00:06:08,699
特征提取器来完成的 不同的空间

158
00:06:08,699 --> 00:06:10,680
位置，所以在这里我们看到单个

159
00:06:10,680 --> 00:06:12,660
卷积滤波器应用于

160
00:06:12,660 --> 00:06:14,820
图像的所有位置，这意味着

161
00:06:14,820 --> 00:06:16,259
无论您的输入

162
00:06:16,259 --> 00:06:18,000
位于图像的中间

163
00:06:18,000 --> 00:06:20,400
还是右侧，您都将具有

164
00:06:20,400 --> 00:06:22,080
完全相同的特征 除了一个

165
00:06:22,080 --> 00:06:23,580
例外，它们将被等效地

166
00:06:23,580 --> 00:06:24,660
移动，

167
00:06:24,660 --> 00:06:26,819
因此从数学上讲，这种类型的映射

168
00:06:26,819 --> 00:06:29,160
称为同态，它保留了

169
00:06:29,160 --> 00:06:30,840
输入

170
00:06:30,840 --> 00:06:33,180
空间和输出空间的代数结构，在这种情况下，

171
00:06:33,180 --> 00:06:35,580
它是相对于平移的，并且在

172
00:06:35,580 --> 00:06:37,440
一个简单的简单级别上类似于 在

173
00:06:37,440 --> 00:06:38,880


174
00:06:38,880 --> 00:06:40,259
本次演讲的其余部分中，要记住的重要一点是，

175
00:06:40,259 --> 00:06:42,300


176
00:06:42,300 --> 00:06:45,000
如果我们可以看到

177
00:06:45,000 --> 00:06:46,740
与

178
00:06:46,740 --> 00:06:49,560
变换交换图存在社区交换，我们就可以验证特征提取器的同态，

179
00:06:49,560 --> 00:06:51,720
因此我们也可以

180
00:06:51,720 --> 00:06:53,759
通过显示特征提取器以代数方式编写它 f

181
00:06:53,759 --> 00:06:55,080
与变换

182
00:06:55,080 --> 00:06:57,000
运算符 t 进行交换，

183
00:06:57,000 --> 00:06:58,919
基本上我们想要的是

184
00:06:58,919 --> 00:07:00,600
首先

185
00:07:00,600 --> 00:07:02,639
提取特征然后

186
00:07:02,639 --> 00:07:04,740
执行变换或

187
00:07:04,740 --> 00:07:06,240
执行变换然后

188
00:07:06,240 --> 00:07:08,639
提取特征之间没有区别，所以

189
00:07:08,639 --> 00:07:10,199
迄今为止的挑战是

190
00:07:10,199 --> 00:07:11,880
我们真的不知道 如何针对

191
00:07:11,880 --> 00:07:13,620


192
00:07:13,620 --> 00:07:15,240
我们在现实世界中看到的更复杂的变换构建同态，

193
00:07:15,240 --> 00:07:18,060
例如我们的大脑

194
00:07:18,060 --> 00:07:20,099
能够自然地处理光照和季节的变化，

195
00:07:20,099 --> 00:07:22,259


196
00:07:22,259 --> 00:07:24,599
所以在这里我们看到一个人脸上的光照

197
00:07:24,599 --> 00:07:26,160
或季节的变化，我们可以

198
00:07:26,160 --> 00:07:27,840
看出它是 同一张脸或同一

199
00:07:27,840 --> 00:07:29,880
条路，但我们不知道如何构建

200
00:07:29,880 --> 00:07:31,319
尊重这些

201
00:07:31,319 --> 00:07:33,180
转换的模型，因此这使我们很难

202
00:07:33,180 --> 00:07:35,520
构建以

203
00:07:35,520 --> 00:07:37,620
稳健且可预测的方式处理它们的系统，

204
00:07:37,620 --> 00:07:40,259
以给出一个更抽象的例子来说明

205
00:07:40,259 --> 00:07:41,759
我的想法 这意味着不处理对称性的

206
00:07:41,759 --> 00:07:43,620
模型的潜在负面影响

207
00:07:43,620 --> 00:07:45,440
转换

208
00:07:45,440 --> 00:07:48,060
考虑了现代文本到图像生成

209
00:07:48,060 --> 00:07:50,520
程序，所以在这个例子中，我要求多莉生成

210
00:07:50,520 --> 00:07:53,940


211
00:07:53,940 --> 00:07:55,620
月球上泰迪熊的图像，它做得

212
00:07:55,620 --> 00:07:57,180
非常好 可能比

213
00:07:57,180 --> 00:08:00,960
我更好，它的纹理毛皮

214
00:08:00,960 --> 00:08:03,360
非常详细，但是如果我要求你

215
00:08:03,360 --> 00:08:05,340
做一些我认为

216
00:08:05,340 --> 00:08:08,039
概念上更简单的事情，例如在

217
00:08:08,039 --> 00:08:10,560
红色立方体顶部绘制蓝色立方体，它无法做到

218
00:08:10,560 --> 00:08:13,380
这一点，对我来说，这似乎不直观，

219
00:08:13,380 --> 00:08:15,300
因为 第二个任务似乎

220
00:08:15,300 --> 00:08:18,180
容易得多，但我要

221
00:08:18,180 --> 00:08:19,860
争论的是，这令人惊讶的原因与

222
00:08:19,860 --> 00:08:21,599


223
00:08:21,599 --> 00:08:23,580
特赦翻译示例令人

224
00:08:23,580 --> 00:08:25,560
惊讶的原因完全相同，这里发生了这种对称

225
00:08:25,560 --> 00:08:28,020
变换，即

226
00:08:28,020 --> 00:08:29,400


227
00:08:29,400 --> 00:08:31,740
泰迪熊的这些复杂对象之间的变换 月亮和

228
00:08:31,740 --> 00:08:34,320
这些简单的立方体物体，我们

229
00:08:34,320 --> 00:08:36,360
直观地期望网络

230
00:08:36,360 --> 00:08:38,820
能够处理和尊重它们，我们发现

231
00:08:38,820 --> 00:08:40,919
它并不像

232
00:08:40,919 --> 00:08:43,380
福岛的工作那样表明，这些

233
00:08:43,380 --> 00:08:46,200
自然的层次结构和

234
00:08:46,200 --> 00:08:47,700
视觉系统的池化是

235
00:08:47,700 --> 00:08:49,680
对于小变换的泛化是有效的

236
00:08:49,680 --> 00:08:52,380
我认为

237
00:08:52,380 --> 00:08:54,060
潜在的更高层次的结构可能是

238
00:08:54,060 --> 00:08:55,740
解决这些抽象泛化问题所必需的

239
00:08:55,740 --> 00:08:58,200


240
00:08:58,200 --> 00:09:01,380
，所以我正在

241
00:09:01,380 --> 00:09:04,380
研究和问的问题是

242
00:09:04,380 --> 00:09:06,060
这个结构可能是什么以及我们如何

243
00:09:06,060 --> 00:09:08,640
实现 这是在人工神经

244
00:09:08,640 --> 00:09:10,080
网络架构中，实际上可以

245
00:09:10,080 --> 00:09:14,120
用于执行计算，

246
00:09:14,880 --> 00:09:17,700
所以开始回答我将

247
00:09:17,700 --> 00:09:19,680
跳入地形组织的第一行工作，

248
00:09:19,680 --> 00:09:22,380


249
00:09:22,380 --> 00:09:25,260
以便

250
00:09:25,260 --> 00:09:27,060
从初级

251
00:09:27,060 --> 00:09:29,760
视觉皮层蓝宝石水平区域在整个大脑中广泛观察地形组织

252
00:09:29,760 --> 00:09:31,500
可以非常宽松地描述这一

253
00:09:31,500 --> 00:09:33,540
属性，即彼此靠近的神经元

254
00:09:33,540 --> 00:09:35,760
倾向于对相似的事物做出反应，

255
00:09:35,760 --> 00:09:38,220
例如在左侧，我们显示了

256
00:09:38,220 --> 00:09:39,720


257
00:09:39,720 --> 00:09:42,959
初级数字皮层中每个神经元的颜色编码偏好，作为

258
00:09:42,959 --> 00:09:45,360
对定向线的反应 我们看到

259
00:09:45,360 --> 00:09:46,740
这种平滑变化的

260
00:09:46,740 --> 00:09:48,779
选择性集另一种类型的

261
00:09:48,779 --> 00:09:50,580
组织称为视网膜主题

262
00:09:50,580 --> 00:09:52,560
组织，其中视觉皮层中的附近神经元

263
00:09:52,560 --> 00:09:54,600
倾向于对附近的感受野做出反应，

264
00:09:54,600 --> 00:09:56,399


265
00:09:56,399 --> 00:09:58,560
但是这种组织并不局限于

266
00:09:58,560 --> 00:10:01,080
这些低级特征，还扩展了一些

267
00:10:01,080 --> 00:10:02,519
更复杂的特征 例如

268
00:10:02,519 --> 00:10:05,459
面部、物体或地方中存在的特征，

269
00:10:05,459 --> 00:10:07,920
这与所谓的

270
00:10:07,920 --> 00:10:10,080
大脑功能特定区域有关，

271
00:10:10,080 --> 00:10:12,779
例如梭状面部区域 FFA 和

272
00:10:12,779 --> 00:10:15,420
海马旁面部区域 PPA，

273
00:10:15,420 --> 00:10:19,200
所以在这项工作中，主要思想再次是，

274
00:10:19,200 --> 00:10:21,300
也许这

275
00:10:21,300 --> 00:10:23,580
从某种意义上说，拓扑组织与卷积

276
00:10:23,580 --> 00:10:25,080


277
00:10:25,080 --> 00:10:27,980
运算和福岛的体系结构密切相关，

278
00:10:27,980 --> 00:10:30,660
我们也许可以将其好处推广

279
00:10:30,660 --> 00:10:33,420
到更抽象的变换，

280
00:10:33,420 --> 00:10:34,920
换句话说，学习如何构建更

281
00:10:34,920 --> 00:10:36,839
复杂的同态，这是我们做不到的，

282
00:10:36,839 --> 00:10:38,940
你知道我们做不到 现在进行分析，

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
只是为了表明我们

285
00:10:42,480 --> 00:10:44,760
对这个想法并不是完全疯狂，呃，柯南·银河·巴登 (

286
00:10:44,760 --> 00:10:46,320


287
00:10:46,320 --> 00:10:49,880
Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden) 等人在 90 年代和 2000 年代早期在这个领域进行了一些工作，

289
00:10:54,060 --> 00:10:55,800
他们研究了地形

290
00:10:55,800 --> 00:10:57,720
组织可能是如何进行的 对于

291
00:10:57,720 --> 00:11:01,320
线性模型中的方差学习很有用，因此

292
00:11:01,320 --> 00:11:03,060
当我们进入这个领域时，我们面临的问题

293
00:11:03,060 --> 00:11:04,680
是，可以从这些方法中利用的最具可扩展性的

294
00:11:04,680 --> 00:11:07,140
抽象机制是什么，

295
00:11:07,140 --> 00:11:08,880
我们可以将其

296
00:11:08,880 --> 00:11:10,800
集成到现代深度神经

297
00:11:10,800 --> 00:11:12,959
网络架构中，最终我们决定采用

298
00:11:12,959 --> 00:11:15,000


299
00:11:15,000 --> 00:11:16,260
我认为

300
00:11:16,260 --> 00:11:17,519
这个社区的人们可能会感兴趣的生成建模方法，

301
00:11:17,519 --> 00:11:18,779


302
00:11:18,779 --> 00:11:21,779
它使我们能够将其

303
00:11:21,779 --> 00:11:23,579
与地形独立分量分析更紧密地联系起来，其

304
00:11:23,579 --> 00:11:26,040
基本思想

305
00:11:26,040 --> 00:11:28,320
是我们可以

306
00:11:28,320 --> 00:11:30,660
通过在地形上施加地形先验分布来学习地形特征空间

307
00:11:30,660 --> 00:11:32,940
我们的潜在

308
00:11:32,940 --> 00:11:34,440
变量，

309
00:11:34,440 --> 00:11:37,079
所以只是为了给出一个简短的背景，我

310
00:11:37,079 --> 00:11:39,120
假设大多数人已经熟悉

311
00:11:39,120 --> 00:11:40,440
这个

312
00:11:40,440 --> 00:11:42,540
嗯，但一般的假设

313
00:11:42,540 --> 00:11:44,339
是大脑是一个生成模型，

314
00:11:44,339 --> 00:11:45,720
这个想法在某种意义上可以

315
00:11:45,720 --> 00:11:48,060
归因于 19

316
00:11:48,060 --> 00:11:50,459
世纪的亥姆霍兹呃 他说，我们

317
00:11:50,459 --> 00:11:52,140
看到的是计算问题的解决方案，

318
00:11:52,140 --> 00:11:54,420
我们的大脑计算出我们眼睛内

319
00:11:54,420 --> 00:11:56,519
光子吸收最可能的原因，

320
00:11:56,519 --> 00:11:59,519
所以这是一个例子，

321
00:11:59,519 --> 00:12:01,920
如果我向你展示这张图像，你会立即

322
00:12:01,920 --> 00:12:03,720
认出它是一个具有一定

323
00:12:03,720 --> 00:12:05,760
曲率的球体，但是它 同样可以

324
00:12:05,760 --> 00:12:07,620
是一张透视扭曲的圆盘，

325
00:12:07,620 --> 00:12:09,600
这就是我们

326
00:12:09,600 --> 00:12:12,660
获得视错觉或图像的方式，

327
00:12:12,660 --> 00:12:14,880
就像这样，你的大脑

328
00:12:14,880 --> 00:12:17,100
根据结构推断这里有一个立方体，

329
00:12:17,100 --> 00:12:18,720
但实际上它只是一张平坦的

330
00:12:18,720 --> 00:12:19,860
纸

331
00:12:19,860 --> 00:12:22,740
所以你可以认为这个生成

332
00:12:22,740 --> 00:12:24,480
模型方面有点像

333
00:12:24,480 --> 00:12:26,160


334
00:12:26,160 --> 00:12:28,140
程序中的逆图形程序，

335
00:12:28,140 --> 00:12:30,660
球体的抽象属性是已知的，位置，

336
00:12:30,660 --> 00:12:32,820
大小，照明，这些用于

337
00:12:32,820 --> 00:12:34,560
投影球体以创建

338
00:12:34,560 --> 00:12:37,440
渲染的2D图像 所以实际上

339
00:12:37,440 --> 00:12:40,019
洪堡和其他人所说的是，

340
00:12:40,019 --> 00:12:41,940
作为一个生成模型，大脑

341
00:12:41,940 --> 00:12:43,680
实际上试图反转这个

342
00:12:43,680 --> 00:12:45,959
生成过程并进行推理

343
00:12:45,959 --> 00:12:48,300
并推断出我们感觉的根本原因，

344
00:12:48,300 --> 00:12:49,680


345
00:12:49,680 --> 00:12:51,600
所以我详细阐述

346
00:12:51,600 --> 00:12:53,100
这一点的原因是

347
00:12:53,100 --> 00:12:55,440
今天有很多讨论生成模型，

348
00:12:55,440 --> 00:12:57,180
我不一定只是在谈论

349
00:12:57,180 --> 00:12:59,639
生成图像或漂亮的

350
00:12:59,639 --> 00:13:01,260
图片，

351
00:13:01,260 --> 00:13:03,120
我真的想指的是

352
00:13:03,120 --> 00:13:07,920
无监督学习的框架，

353
00:13:07,920 --> 00:13:10,019
所以接下来要更详细地了解

354
00:13:10,019 --> 00:13:11,700
我所说的生成模型是什么意思

355
00:13:11,700 --> 00:13:14,279
地形先验生成模型

356
00:13:14,279 --> 00:13:16,019
通常被描述

357
00:13:16,019 --> 00:13:18,720
为观测值 X 和

358
00:13:18,720 --> 00:13:21,720
潜在变量（我们称之为 Z ）的联合分布，

359
00:13:21,720 --> 00:13:23,940
这通常是因式分解的，或者实现这一点的一种

360
00:13:23,940 --> 00:13:25,620
方法是

361
00:13:25,620 --> 00:13:28,440
根据 Z 的先验 P 进行因式分解，这是真实的

362
00:13:28,440 --> 00:13:30,420
生成模型

363
00:13:30,420 --> 00:13:33,420
给定 Z 的 x 的条件生成模型 P，因此

364
00:13:33,420 --> 00:13:34,980
我们可以考虑这一点的一种方式是，

365
00:13:34,980 --> 00:13:37,019
先验可以被视为

366
00:13:37,019 --> 00:13:38,880
对

367
00:13:38,880 --> 00:13:41,279
当我们反转生成模型时生成的每种类型的代码进行相对惩罚，

368
00:13:41,279 --> 00:13:42,899
这称为计算

369
00:13:42,899 --> 00:13:45,920
给定 X 的 Z 的后验 P，

370
00:13:45,920 --> 00:13:49,139
因此为了开发一个地形潜在

371
00:13:49,139 --> 00:13:50,639
空间，我们想要引入某种

372
00:13:50,639 --> 00:13:53,279
地形先验，该先验已经或

373
00:13:53,279 --> 00:13:55,620
该地形 ICA 工作表明

374
00:13:55,620 --> 00:13:57,720
相当于群体

375
00:13:57,720 --> 00:13:59,700
稀疏惩罚之类的东西，

376
00:13:59,700 --> 00:14:01,500
因此人们可能熟悉典型的

377
00:14:01,500 --> 00:14:03,180
独立学习分析的稀疏性惩罚

378
00:14:03,180 --> 00:14:04,560
你希望你的

379
00:14:04,560 --> 00:14:06,540
激活是稀疏的，这意味着它们中的许多

380
00:14:06,540 --> 00:14:09,420
都是零哇，所以看起来

381
00:14:09,420 --> 00:14:10,680
像这样，你有一堆

382
00:14:10,680 --> 00:14:12,300
活跃的蓝色方块，但其中大多数

383
00:14:12,300 --> 00:14:14,459
不活跃，但特别是

384
00:14:14,459 --> 00:14:16,740
与组的 大学惩罚，我们希望

385
00:14:16,740 --> 00:14:18,839
这些先验

386
00:14:18,839 --> 00:14:21,600
为这些分布式稀疏激活分配较低

387
00:14:21,600 --> 00:14:24,720
的概率，为这些分组的

388
00:14:24,720 --> 00:14:26,940
密集表示分配较高的概率，您也可以将其视为

389
00:14:26,940 --> 00:14:28,860


390
00:14:28,860 --> 00:14:30,720
当事物分散时更高的惩罚，

391
00:14:30,720 --> 00:14:33,540
当事物更接近时更低的惩罚，

392
00:14:33,540 --> 00:14:36,380
所以再次呃 这可以

393
00:14:36,380 --> 00:14:39,060
抽象地写成这样，但我想提出这样的

394
00:14:39,060 --> 00:14:41,160
理论：这些神经元中的每一个

395
00:14:41,160 --> 00:14:42,779
方块都代表

396
00:14:42,779 --> 00:14:44,220
我们模型中的一种神经元，它们

397
00:14:44,220 --> 00:14:46,560
被组织在这个 2D 网格中，所以当我们

398
00:14:46,560 --> 00:14:48,120
谈论分组时，我们真正的意思是

399
00:14:48,120 --> 00:14:50,820
在二维拓扑中进行分组，

400
00:14:50,820 --> 00:14:53,100
所以真正有趣

401
00:14:53,100 --> 00:14:55,560
且重要的一件事是，这些

402
00:14:55,560 --> 00:14:57,779
先验不仅为我们提供了拓扑

403
00:14:57,779 --> 00:15:00,540
组织，而且还被

404
00:15:00,540 --> 00:15:02,459


405
00:15:02,459 --> 00:15:05,760
erosi Marcelli 和 Bruno 等人注意到或研究过，以

406
00:15:05,760 --> 00:15:07,740
实际适合 自然数据的统计

407
00:15:07,740 --> 00:15:08,959
更好，

408
00:15:08,959 --> 00:15:11,180
特别是自然图像，

409
00:15:11,180 --> 00:15:14,100
他们已经表明，使用这种类型的

410
00:15:14,100 --> 00:15:16,139
先验，您实际上会得到一组更稀疏的

411
00:15:16,139 --> 00:15:18,839
激活，这意味着先验更适合

412
00:15:18,839 --> 00:15:20,459
真实的生成过程，

413
00:15:20,459 --> 00:15:22,620
并且正如我们所知，大脑已经

414
00:15:22,620 --> 00:15:24,779
高度稀疏性，这被

415
00:15:24,779 --> 00:15:26,339
认为与效率非常相关，

416
00:15:26,339 --> 00:15:28,620


417
00:15:28,620 --> 00:15:30,839
因此为了更详细地

418
00:15:30,839 --> 00:15:32,760
实现这种类型的

419
00:15:32,760 --> 00:15:35,160
组稀疏，我们使用分层

420
00:15:35,160 --> 00:15:37,320
生成模型，这

421
00:15:37,320 --> 00:15:39,060
基本上是由一些

422
00:15:39,060 --> 00:15:41,339
地形 ICA 的工作

423
00:15:41,339 --> 00:15:43,320
原理是，你有一个更高

424
00:15:43,320 --> 00:15:45,000
级别的潜在变量 U，它

425
00:15:45,000 --> 00:15:47,820
同时调节

426
00:15:47,820 --> 00:15:50,279
多个较低级别变量 T 的方差，

427
00:15:50,279 --> 00:15:52,440
这就是我们获得组稀疏性的方法，

428
00:15:52,440 --> 00:15:55,440
然后为了获得地形组织，你

429
00:15:55,440 --> 00:15:56,760
可以

430
00:15:56,760 --> 00:15:59,339
稍微使用多个这些潜在变量 与

431
00:15:59,339 --> 00:16:02,519
他们的影响力领域重叠，因此

432
00:16:02,519 --> 00:16:04,260
我们可以称之为他们的邻居，

433
00:16:04,260 --> 00:16:05,699
这将为您提供平滑的

434
00:16:05,699 --> 00:16:07,440
相关结构，您所追求的行为

435
00:16:07,440 --> 00:16:09,899
因此获得直觉，您会

436
00:16:09,899 --> 00:16:12,060
看到底部的这个变量 T 并

437
00:16:12,060 --> 00:16:14,279
没有得到

438
00:16:14,279 --> 00:16:17,160
来自顶部这个 U 的任何输入，但它

439
00:16:17,160 --> 00:16:19,139
与中间的这个 T 共享一个 u 变量，

440
00:16:19,139 --> 00:16:21,300
所以就像它们正在共享变体一样，

441
00:16:21,300 --> 00:16:22,980
它们与邻居共享一些组件，

442
00:16:22,980 --> 00:16:24,959
但不是所有组件

443
00:16:24,959 --> 00:16:26,579
，这实际上是由于这种本地

444
00:16:26,579 --> 00:16:28,079
连接 这些更高级别的

445
00:16:28,079 --> 00:16:30,779
变量，

446
00:16:30,779 --> 00:16:33,180
为了让我们如何使用

447
00:16:33,180 --> 00:16:34,980
生成模型变得简单，让我们回到

448
00:16:34,980 --> 00:16:37,440
单个 U 变量，

449
00:16:37,440 --> 00:16:38,940
这种类型的架构

450
00:16:38,940 --> 00:16:42,360
多年来一直面临的挑战是

451
00:16:42,360 --> 00:16:44,579
如何推断近似后验 在

452
00:16:44,579 --> 00:16:47,579


453
00:16:47,579 --> 00:16:50,100
这个分层架构中的这些中间变量上，这

454
00:16:50,100 --> 00:16:52,560
并不是超级简单，所以之前的

455
00:16:52,560 --> 00:16:54,420
作品使用了为线性模型开发的启发式方法，

456
00:16:54,420 --> 00:16:56,699
在我们的工作中，我们发现

457
00:16:56,699 --> 00:16:58,680
这确实没有扩展到现代

458
00:16:58,680 --> 00:17:01,199
神经网络架构，所以

459
00:17:01,199 --> 00:17:02,880
我们的见解实际上是利用

460
00:17:02,880 --> 00:17:04,760
因式分解是

461
00:17:04,760 --> 00:17:07,640
该分布的特定重新参数化，

462
00:17:07,640 --> 00:17:10,380
因此该参数化

463
00:17:10,380 --> 00:17:12,419
具体是通过定义

464
00:17:12,419 --> 00:17:14,579
优先级来实现的，即所谓的高斯

465
00:17:14,579 --> 00:17:16,319
比例混合，这意味着

466
00:17:16,319 --> 00:17:19,140
给定 U 的 T 的条件分布

467
00:17:19,140 --> 00:17:21,179
实际上是正态分布，其中

468
00:17:21,179 --> 00:17:24,299
方差由该变量定义

469
00:17:24,299 --> 00:17:27,720
U 和对于 U 的某些选择，这个

470
00:17:27,720 --> 00:17:29,340
分布确实是稀疏的，并且

471
00:17:29,340 --> 00:17:31,980
包含一系列分布，

472
00:17:31,980 --> 00:17:33,780
例如 laplossians 和 T

473
00:17:33,780 --> 00:17:36,299
分布，定义它的一种方法是

474
00:17:36,299 --> 00:17:38,940
高斯尺度混合，

475
00:17:38,940 --> 00:17:40,919


476
00:17:40,919 --> 00:17:42,900
根据独立的高斯随机

477
00:17:42,900 --> 00:17:45,720
变量发出特定的重新构建重新参数化 Z 和 U 如此具体，然后

478
00:17:45,720 --> 00:17:48,840
我们看到这个最初相当复杂的 T 变量

479
00:17:48,840 --> 00:17:50,760
实际上

480
00:17:50,760 --> 00:17:52,799
只是一堆高斯

481
00:17:52,799 --> 00:17:54,840
随机变量的乘积，这些变量现在知道如何

482
00:17:54,840 --> 00:17:57,660
在生成模型中更有效地工作，

483
00:17:57,660 --> 00:18:00,120
特别是

484
00:18:00,120 --> 00:18:02,039
我们正在做什么 要做的是，我们实际上可以

485
00:18:02,039 --> 00:18:04,020


486
00:18:04,020 --> 00:18:06,720
分别获得 U 和 Z 的近似后验，然后对

487
00:18:06,720 --> 00:18:08,640
它们进行确定性组合，以

488
00:18:08,640 --> 00:18:10,020
计算我们的地形

489
00:18:10,020 --> 00:18:13,140
变量 T，这更容易做到，

490
00:18:13,140 --> 00:18:15,500
而无需考虑太多细节

491
00:18:15,500 --> 00:18:17,700
我们决定使用所谓的

492
00:18:17,700 --> 00:18:18,600
变分自动

493
00:18:18,600 --> 00:18:20,640
编码器，它利用

494
00:18:20,640 --> 00:18:23,220
变分推理技术来导出

495
00:18:23,220 --> 00:18:24,780
可能性的下界，使

496
00:18:24,780 --> 00:18:26,940
我们能够使用

497
00:18:26,940 --> 00:18:29,400
强大的非线性深度

498
00:18:29,400 --> 00:18:30,960
神经网络参数化这些近似后验，并通过

499
00:18:30,960 --> 00:18:33,240
梯度下降对其进行优化，这将是

500
00:18:33,240 --> 00:18:34,440
主动推理

501
00:18:34,440 --> 00:18:36,900
社区很熟悉，但实际上我们所做的

502
00:18:36,900 --> 00:18:38,760
不是在解码器中使用单个编码器

503
00:18:38,760 --> 00:18:41,640
作为典型的基础，我们现在有

504
00:18:41,640 --> 00:18:43,799
两个编码器，一个用于您，一个用于 Z，

505
00:18:43,799 --> 00:18:46,200
然后我们以

506
00:18:46,200 --> 00:18:48,419
这种确定性方式将它们组合起来 构造

507
00:18:48,419 --> 00:18:51,660
我们的地形 T 变量，如果您看到

508
00:18:51,660 --> 00:18:53,100
这实际上是

509
00:18:53,100 --> 00:18:54,900


510
00:18:54,900 --> 00:18:57,620
从高斯构造学生的 T 分布，

511
00:18:57,620 --> 00:19:00,480
然后我们可以插入它，我们

512
00:19:00,480 --> 00:19:03,600
在解码之前执行此操作，然后最大化

513
00:19:03,600 --> 00:19:05,820
数据的可能性，所以

514
00:19:05,820 --> 00:19:07,260
这就是肘部

515
00:19:07,260 --> 00:19:09,360


516
00:19:09,360 --> 00:19:12,600
数据可能性的证据下限比比皆是，实际上与活跃入口社区

517
00:19:12,600 --> 00:19:15,720
中使用的变分自由能非常相似，

518
00:19:15,720 --> 00:19:18,299


519
00:19:18,299 --> 00:19:20,520
所以在排除这些细节后，

520
00:19:20,520 --> 00:19:22,500
真正有趣的是当

521
00:19:22,500 --> 00:19:23,940
我们训练这个生成模型时会发生什么

522
00:19:23,940 --> 00:19:26,580


523
00:19:26,580 --> 00:19:29,460
在其潜在空间中具有相对简单的组稀疏性惩罚，

524
00:19:29,460 --> 00:19:30,720
我们想要看看它

525
00:19:30,720 --> 00:19:32,700
在期货组织方面正在学习什么，

526
00:19:32,700 --> 00:19:34,980
首先我们从

527
00:19:34,980 --> 00:19:36,480
最简单的数据集开始，我们在随机 XY 位置有一个

528
00:19:36,480 --> 00:19:38,580
黑色背景和白色方块

529
00:19:38,580 --> 00:19:41,280
如果我们

530
00:19:41,280 --> 00:19:42,780
用这个组稀疏性惩罚来训练我们的自动编码器，

531
00:19:42,780 --> 00:19:44,760
然后我们看看

532
00:19:44,760 --> 00:19:47,640
我们的解码器的权重向量，

533
00:19:47,640 --> 00:19:49,020
我们在这里用蓝色绘制它，再次

534
00:19:49,020 --> 00:19:52,520
组织在这个二维网格上，我们看到

535
00:19:52,520 --> 00:19:54,780
它们确实学会了

536
00:19:54,780 --> 00:19:57,539
根据空间进行组织 位置，因此这

537
00:19:57,539 --> 00:19:59,580
可以被视为类似于卷积

538
00:19:59,580 --> 00:20:01,799
感受野，或者

539
00:20:01,799 --> 00:20:04,679
每个神经元的感受野实际上是由其位置处的输入类型给出的，

540
00:20:04,679 --> 00:20:09,059


541
00:20:09,059 --> 00:20:10,860
从

542
00:20:10,860 --> 00:20:13,140
组的稀疏性角度来看，这直观地有意义，因为

543
00:20:13,140 --> 00:20:15,480
对于任何给定的区域要突出显示，如

544
00:20:15,480 --> 00:20:17,700
黄色这里给定组中的过滤器的

545
00:20:17,700 --> 00:20:19,260
相关性要高得多，

546
00:20:19,260 --> 00:20:20,880
它们比其他随机位置具有这些重叠的感受

547
00:20:20,880 --> 00:20:23,460
野，所以

548
00:20:23,460 --> 00:20:25,020
本质上我们看到我们的模型正在

549
00:20:25,020 --> 00:20:27,840
学习根据以下相关性将活动活动聚类

550
00:20:27,840 --> 00:20:28,980
在一起，

551
00:20:28,980 --> 00:20:32,059
呃，以模拟垂直表的形式

552
00:20:32,059 --> 00:20:34,260


553
00:20:34,260 --> 00:20:36,840
数据集，而不是在卷积中，

554
00:20:36,840 --> 00:20:38,580
你实际上在进行权重绑定，

555
00:20:38,580 --> 00:20:40,860
并且手动指定我想

556
00:20:40,860 --> 00:20:42,539
在任何地方复制这个权重，你

557
00:20:42,539 --> 00:20:44,220
可能会认为这就像近似的

558
00:20:44,220 --> 00:20:45,500
等待时间

559
00:20:45,500 --> 00:20:48,120
，实际上我们正在从

560
00:20:48,120 --> 00:20:49,620
相关性中学习这一点 数据集本身的结构，

561
00:20:49,620 --> 00:20:51,660
只是为了为此提供

562
00:20:51,660 --> 00:20:54,120
更多的生物学灵感，

563
00:20:54,120 --> 00:20:56,280
我们知道

564
00:20:56,280 --> 00:20:58,020
大脑中存在视网膜异位，这是

565
00:20:58,020 --> 00:21:02,460
视觉皮层中视网膜异位的一个例子，

566
00:21:02,460 --> 00:21:04,500
你可以看看是否向猕猴展示

567
00:21:04,500 --> 00:21:06,780
像这样的图像，它被投影到

568
00:21:06,780 --> 00:21:08,520
这个呃拓扑中，

569
00:21:08,520 --> 00:21:11,700
实际上保留了

570
00:21:11,700 --> 00:21:13,740
皮层表面的空间，

571
00:21:13,740 --> 00:21:16,080
所以这个想法是，地形

572
00:21:16,080 --> 00:21:18,419
组织，甚至学习地形

573
00:21:18,419 --> 00:21:21,299
组织正在保留

574
00:21:21,299 --> 00:21:26,160
我们数据集的输入相关性，并且

575
00:21:26,160 --> 00:21:28,679
可能呃，这可能有益于

576
00:21:28,679 --> 00:21:30,840


577
00:21:30,840 --> 00:21:32,340
进一步概括这些想法，就像我一

578
00:21:32,340 --> 00:21:34,679
开始说的那样，如果

579
00:21:34,679 --> 00:21:37,200
我们能够学习不仅仅是

580
00:21:37,200 --> 00:21:39,320
卷积的东西，也许更复杂的

581
00:21:39,320 --> 00:21:43,679
等变性，那就更好了，所以我们如何做到这一点，

582
00:21:43,679 --> 00:21:45,720
在自然智能中很清楚的一件事

583
00:21:45,720 --> 00:21:48,299
是我们 不存在于

584
00:21:48,299 --> 00:21:51,120
IID 框架的世界中，我们存在

585
00:21:51,120 --> 00:21:53,520
于一个具有连续变换序列的世界中，

586
00:21:53,520 --> 00:21:55,620
所以也许我们可以将

587
00:21:55,620 --> 00:21:58,440
我们的模型扩展到此设置以学习

588
00:21:58,440 --> 00:22:01,080
观察变换，这是

589
00:22:01,080 --> 00:22:03,299
时间相干性的想法，

590
00:22:03,299 --> 00:22:05,280
所以如果我们只是简单地会发生什么

591
00:22:05,280 --> 00:22:08,280
随着时间的推移，维度正确地扩展了我们之前的框架，

592
00:22:08,280 --> 00:22:10,620
因此我们不只是

593
00:22:10,620 --> 00:22:13,080
分组说我们希望我们的神经元

594
00:22:13,080 --> 00:22:15,059


595
00:22:15,059 --> 00:22:17,400
在皮层的空间范围上保持稀疏，我们实际上希望

596
00:22:17,400 --> 00:22:18,960
它们随着时间的推移保持稀疏，

597
00:22:18,960 --> 00:22:20,640
这意味着如果一组神经元处于

598
00:22:20,640 --> 00:22:22,559
活动状态 现在我们希望同一组

599
00:22:22,559 --> 00:22:24,360
神经元在未来也能保持活跃，

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
如果我们直观地

602
00:22:27,840 --> 00:22:30,600
思考这一点，我们会发现这

603
00:22:30,600 --> 00:22:33,059
实际上是更令人鼓舞的不变性和

604
00:22:33,059 --> 00:22:35,039
等变性，一种理解这一点的方法是

605
00:22:35,039 --> 00:22:37,140
我们说我们 希望相同的神经元不断处于

606
00:22:37,140 --> 00:22:39,179
活动状态，但输入

607
00:22:39,179 --> 00:22:41,280
转换正在改变，

608
00:22:41,280 --> 00:22:44,220
这只小狐狸的脚在移动，因此，如果

609
00:22:44,220 --> 00:22:45,960
相同的神经元

610
00:22:45,960 --> 00:22:47,880
一遍又一遍地编码相同的东西，但脚

611
00:22:47,880 --> 00:22:49,320
在移动，那么这些神经元就会

612
00:22:49,320 --> 00:22:51,360
学习 例如，对于这只狗的那条腿的运动是不变的，

613
00:22:51,360 --> 00:22:53,880


614
00:22:53,880 --> 00:22:57,539
所以相反，哎呀

615
00:22:57,539 --> 00:23:01,200
我在这里走错了路呃

616
00:23:01,200 --> 00:23:04,860
所以相反呃我们的见解是这个

617
00:23:04,860 --> 00:23:06,659
组开始可以相

618
00:23:06,659 --> 00:23:09,059
对于时间移动所以这

619
00:23:09,059 --> 00:23:10,980
意味着 将鼓励按顺序移动的

620
00:23:10,980 --> 00:23:13,080
激活集

621
00:23:13,080 --> 00:23:15,179
一起激活，然后我们的潜在

622
00:23:15,179 --> 00:23:16,440
空间将真正

623
00:23:16,440 --> 00:23:18,000
根据观察到的转换来构建，

624
00:23:18,000 --> 00:23:19,980
因此您可以在这里看到，而不是

625
00:23:19,980 --> 00:23:21,480
同一组神经元在所有

626
00:23:21,480 --> 00:23:23,340
时间步骤都处于活动状态，它实际上是一个连续的神经元

627
00:23:23,340 --> 00:23:24,900
我们以

628
00:23:24,900 --> 00:23:27,780
这种稀疏的方式组合在一起的一组神经元的排列组合，

629
00:23:27,780 --> 00:23:29,940
然后这使我们能够

630
00:23:29,940 --> 00:23:33,419
随着时间的推移对不同的观察结果进行建模，但

631
00:23:33,419 --> 00:23:34,860
它们在

632
00:23:34,860 --> 00:23:36,960
学习转换和保留同理心

633
00:23:36,960 --> 00:23:38,340
的相关结构方面仍然是相互联系的，

634
00:23:38,340 --> 00:23:40,020


635
00:23:40,020 --> 00:23:41,940
所以如果我们 将其放入我们的

636
00:23:41,940 --> 00:23:44,400
地形 Bae 架构中，您可以得到

637
00:23:44,400 --> 00:23:46,020
如下所示的内容您看到

638
00:23:46,020 --> 00:23:48,120
我们有一个输入序列，我们

639
00:23:48,120 --> 00:23:51,240
再次编码 z 变量，然后

640
00:23:51,240 --> 00:23:53,520


641
00:23:53,520 --> 00:23:55,740
这里分母中的多个 U 变量，然后这些 U 变量中的每一个都

642
00:23:55,740 --> 00:23:58,620
被移位 呃有点像我们

643
00:23:58,620 --> 00:24:00,480
之前展示的为了实现

644
00:24:00,480 --> 00:24:02,820
这个我们正在寻找的这种平移等方差结构，

645
00:24:02,820 --> 00:24:04,740
当我们将

646
00:24:04,740 --> 00:24:07,080
这些组合在这个学生 T 产品

647
00:24:07,080 --> 00:24:09,240
分布中时，我们得到一个单一的潜在

648
00:24:09,240 --> 00:24:10,740
变量，这现在是我们的地形

649
00:24:10,740 --> 00:24:13,860
变量 T，现在我们

650
00:24:13,860 --> 00:24:16,140
在我们的潜在空间中有这个已知的结构，你

651
00:24:16,140 --> 00:24:17,460
可以把它想象成一个结构化的世界

652
00:24:17,460 --> 00:24:19,919
模型，我们知道如何转换这个

653
00:24:19,919 --> 00:24:21,659
潜在空间，在这种情况下，它是通过围绕

654
00:24:21,659 --> 00:24:23,580
这些圆圈排列这些激活，

655
00:24:23,580 --> 00:24:25,860
就像循环角色，

656
00:24:25,860 --> 00:24:28,380
循环移位我们知道这是 将

657
00:24:28,380 --> 00:24:30,120
对应于我们学习到的输入

658
00:24:30,120 --> 00:24:32,640
转换，我们可以

659
00:24:32,640 --> 00:24:34,620
通过说好吧，如果我继续这个

660
00:24:34,620 --> 00:24:36,480
输入转换，

661
00:24:36,480 --> 00:24:38,100
数据集中的真正转换（即

662
00:24:38,100 --> 00:24:40,559
旋转）会怎样，然后我将其与

663
00:24:40,559 --> 00:24:42,659
我最近完成的角色进行比较

664
00:24:42,659 --> 00:24:44,700
通过在我的大脑中移动我的激活空间，

665
00:24:44,700 --> 00:24:47,280
然后我们解码，我们看到

666
00:24:47,280 --> 00:24:49,919
我们得到了完全相同的东西，所以这

667
00:24:49,919 --> 00:24:52,140
证明了

668
00:24:52,140 --> 00:24:53,580
我之前讨论过的用于验证同态的交换属性

669
00:24:53,580 --> 00:24:56,820


670
00:24:56,820 --> 00:24:58,799
，因此可以更好地衡量它的

671
00:24:58,799 --> 00:25:02,460
质量 呃，我们可以定量地测量

672
00:25:02,460 --> 00:25:04,440
所谓的等方差损失，所以

673
00:25:04,440 --> 00:25:07,080
这实际上是

674
00:25:07,080 --> 00:25:09,360
我们滚动的

675
00:25:09,360 --> 00:25:12,120
胶囊激活或在我们的

676
00:25:12,120 --> 00:25:15,059
头部滚动与观察滚动展开

677
00:25:15,059 --> 00:25:16,559
和向前滚动之间的差异的量化，他们正在观察

678
00:25:16,559 --> 00:25:19,440
我们面前的变换展开，所以我们

679
00:25:19,440 --> 00:25:21,600
看到了地形 Bae 实现了显着

680
00:25:21,600 --> 00:25:24,000
较低的呃等方差

681
00:25:24,000 --> 00:25:26,700
误差，这个气泡 vae 就是我

682
00:25:26,700 --> 00:25:27,960
之前所说的，它正在学习

683
00:25:27,960 --> 00:25:29,820
不变性，因此它没有移位

684
00:25:29,820 --> 00:25:32,340
操作，而传统的 vae

685
00:25:32,340 --> 00:25:35,640
没有组织或

686
00:25:35,640 --> 00:25:37,380
时间组件的概念，因此性能非常

687
00:25:37,380 --> 00:25:40,320
差 对此，我们看到

688
00:25:40,320 --> 00:25:41,700
该模型是一个更好的序列生成模型，

689
00:25:41,700 --> 00:25:45,059
它只是

690
00:25:45,059 --> 00:25:48,179
在数据集上获得了较低的负对数似然性，

691
00:25:48,179 --> 00:25:50,100
因此能够更好地

692
00:25:50,100 --> 00:25:51,720
对该数据集进行建模，因为它具有序列

693
00:25:51,720 --> 00:25:52,919
结构的概念

694
00:25:52,919 --> 00:25:55,460
变换

695
00:25:55,980 --> 00:25:58,140
呃，我们可以在多种

696
00:25:58,140 --> 00:25:59,760
不同的变换类型上进行测试，在

697
00:25:59,760 --> 00:26:00,840
顶行我们显示真正的

698
00:26:00,840 --> 00:26:02,880
变换，我们拉出这些

699
00:26:02,880 --> 00:26:05,039
灰色图像，然后在底行

700
00:26:05,039 --> 00:26:07,080
我们编码，然后我们只是

701
00:26:07,080 --> 00:26:08,700
滚动我们的激活，我们保持

702
00:26:08,700 --> 00:26:12,140
解码以查看模型

703
00:26:12,140 --> 00:26:15,000
学到的内容作为

704
00:26:15,000 --> 00:26:17,039
正在观察的当前变换，

705
00:26:17,039 --> 00:26:19,340
我们看到它基本上可以完美地

706
00:26:19,340 --> 00:26:21,360
重建

707
00:26:21,360 --> 00:26:23,640
以前从未见过的序列中的这些元素，

708
00:26:23,640 --> 00:26:25,260
此外还有来自

709
00:26:25,260 --> 00:26:26,580
测试集的从未见过的图像 之前，

710
00:26:26,580 --> 00:26:28,380
仅仅因为它知道

711
00:26:28,380 --> 00:26:29,760
当前正在编码的转换是什么，

712
00:26:29,760 --> 00:26:31,500
它可以将其推广到新的

713
00:26:31,500 --> 00:26:33,919
示例，

714
00:26:34,020 --> 00:26:36,360
因此这部分的要点实际上是

715
00:26:36,360 --> 00:26:38,039
拓扑组织，我们展示了

716
00:26:38,039 --> 00:26:40,080
保留的输入结构，现在

717
00:26:40,080 --> 00:26:41,940
我们展示了它可以潜在地提高

718
00:26:41,940 --> 00:26:44,279
效率和泛化 正如我们

719
00:26:44,279 --> 00:26:46,200
所希望的，呃，

720
00:26:46,200 --> 00:26:48,600
最后让我们感到惊讶，

721
00:26:48,600 --> 00:26:49,980
我认为可能最

722
00:26:49,980 --> 00:26:52,500
有趣的是，

723
00:26:52,500 --> 00:26:53,700
我们的模型学习到的这些变换

724
00:26:53,700 --> 00:26:54,960
实际上概括了

725
00:26:54,960 --> 00:26:57,059


726
00:26:57,059 --> 00:26:59,580
我们在训练期间没有看到的变换的组合，

727
00:26:59,580 --> 00:27:02,100
例如，尽管只进行了训练 关于颜色

728
00:27:02,100 --> 00:27:04,200
和旋转变换和

729
00:27:04,200 --> 00:27:06,419
隔离如果模型在测试时呈现

730
00:27:06,419 --> 00:27:08,340
组合的颜色旋转变换，

731
00:27:08,340 --> 00:27:11,100
我们看到它能够

732
00:27:11,100 --> 00:27:13,140
完全建模并

733
00:27:13,140 --> 00:27:14,700
通过胶囊角色完美地完成这些变换，这

734
00:27:14,700 --> 00:27:17,159
意味着它学会了将表示分解

735
00:27:17,159 --> 00:27:19,620
为这些

736
00:27:19,620 --> 00:27:20,880
不同的 转换并且它可以

737
00:27:20,880 --> 00:27:24,600
在推理时灵活地组合它们，

738
00:27:24,600 --> 00:27:28,140
所以也许我们不仅仅获得

739
00:27:28,140 --> 00:27:29,820
泛化的正式效率，

740
00:27:29,820 --> 00:27:34,100
我们还获得一些基本的组合性，

741
00:27:34,260 --> 00:27:36,059
所以让我们谈谈限制以及

742
00:27:36,059 --> 00:27:38,460
我们下一步可以做什么呃主要

743
00:27:38,460 --> 00:27:40,620
限制是有一个

744
00:27:40,620 --> 00:27:44,159
我们在

745
00:27:44,159 --> 00:27:46,500
空间和时间上强加的预定义变换，因此，尽管我们将

746
00:27:46,500 --> 00:27:49,080
自己从组变换中解放出来，

747
00:27:49,080 --> 00:27:52,440
特别是平移或

748
00:27:52,440 --> 00:27:53,940
旋转，因为它目前在

749
00:27:53,940 --> 00:27:55,559
机器学习世界中完成，

750
00:27:55,559 --> 00:27:59,240
但我们仍然

751
00:27:59,240 --> 00:28:01,980
在我们的模型中具有这种硬编码的潜在角色。

752
00:28:01,980 --> 00:28:03,900
我们所看到的一切的头，并使其

753
00:28:03,900 --> 00:28:05,700
更加灵活，所以希望我们能够模拟

754
00:28:05,700 --> 00:28:08,880
更多样化的转换，

755
00:28:08,880 --> 00:28:10,980
嗯，我们认为也许我们可以从大脑中观察到的

756
00:28:10,980 --> 00:28:13,860
更结构化的时空动态中获得灵感，

757
00:28:13,860 --> 00:28:15,600


758
00:28:15,600 --> 00:28:18,120
这需要我们

759
00:28:18,120 --> 00:28:20,400
这次演讲的第二部分是呃

760
00:28:20,400 --> 00:28:22,140
时空动力学，我们

761
00:28:22,140 --> 00:28:23,039
将尝试将其整合到

762
00:28:23,039 --> 00:28:25,200
人工神经网络中，其中一个例子

763
00:28:25,200 --> 00:28:27,059
就是行波，就像我

764
00:28:27,059 --> 00:28:28,020
在这里展示的那样，

765
00:28:28,020 --> 00:28:30,600
那么我们的意思是什么，呃，这

766
00:28:30,600 --> 00:28:32,279
是一个非常好的例子。 最近的论文中，他们

767
00:28:32,279 --> 00:28:36,059
使用以 36 毫秒分辨率运行的 9 个特斯拉 fmri 对

768
00:28:36,059 --> 00:28:38,700


769
00:28:38,700 --> 00:28:40,980
麻醉下的大鼠大脑的单个切片进行成像

770
00:28:40,980 --> 00:28:43,320
，我们看到的是这种结构非常清晰的

771
00:28:43,320 --> 00:28:45,720
时空活动和

772
00:28:45,720 --> 00:28:48,299
相关性，论文的作者

773
00:28:48,299 --> 00:28:50,520
继续分析了这一点 右侧

774
00:28:50,520 --> 00:28:52,919
描述的主要模式的活动，

775
00:28:52,919 --> 00:28:55,799
因此我们的假设是，

776
00:28:55,799 --> 00:28:57,179
也许

777
00:28:57,179 --> 00:28:59,039
像这样的某种相关结构可能有利于

778
00:28:59,039 --> 00:29:01,260
构建

779
00:29:01,260 --> 00:29:03,240
我们的模型相对于观察到的

780
00:29:03,240 --> 00:29:05,100
变换的表示，但以

781
00:29:05,100 --> 00:29:07,440
比简单地更灵活的方式 只是一个循环

782
00:29:07,440 --> 00:29:10,700
移位，就像我们之前所做的那样，

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
嗯，让我说，这不仅仅是

785
00:29:15,900 --> 00:29:19,320
在 ssi 的老鼠中观察到的，

786
00:29:19,320 --> 00:29:20,940
你可以看到这些行波

787
00:29:20,940 --> 00:29:24,179
发生在清醒行为的灵长类动物的 Mt 皮层中，

788
00:29:24,179 --> 00:29:27,600
所以例如在

789
00:29:27,600 --> 00:29:29,580
左边 在这里，他们显示了

790
00:29:29,580 --> 00:29:31,740
实际上呃

791
00:29:31,740 --> 00:29:35,520
变化的行波。灵长类动物

792
00:29:35,520 --> 00:29:38,279
根据波的相位看到低对比度刺激的可能性有多大。

793
00:29:38,279 --> 00:29:40,980
此外，他们还表明，

794
00:29:40,980 --> 00:29:43,500
像右侧的高对比度刺激

795
00:29:43,500 --> 00:29:45,779
可以诱发

796
00:29:45,779 --> 00:29:47,520
向外传播的行波活动 即使

797
00:29:47,520 --> 00:29:50,039
在初级视觉皮层中，这些

798
00:29:50,039 --> 00:29:52,140
在整个大脑的

799
00:29:52,140 --> 00:29:54,000
多个层面上确实无处不在，

800
00:29:54,000 --> 00:29:55,440
研究它们对

801
00:29:55,440 --> 00:29:58,140


802
00:29:58,140 --> 00:29:59,700
我们案例中的呃结构表征学习的影响将是很有趣的，

803
00:29:59,700 --> 00:30:01,799
或者通常

804
00:30:01,799 --> 00:30:04,140
有先前的工作研究了

805
00:30:04,140 --> 00:30:06,720
这些类型的呃动力学 他们

806
00:30:06,720 --> 00:30:08,700
建立了模型，所以在顶部，这些是

807
00:30:08,700 --> 00:30:10,380
描述尖峰

808
00:30:10,380 --> 00:30:12,600
神经网络的方程，他们展示了如果你

809
00:30:12,600 --> 00:30:15,720
实现时间延迟，实际上是

810
00:30:15,720 --> 00:30:18,240
神经元之间的轴突时间延迟，只要你的网络规模足够大，你就可以获得

811
00:30:18,240 --> 00:30:20,820
行波的这些结构动力学

812
00:30:20,820 --> 00:30:22,440


813
00:30:22,440 --> 00:30:24,059


814
00:30:24,059 --> 00:30:26,520
嗯，然而，正如许多人可能知道的那样，

815
00:30:26,520 --> 00:30:28,620


816
00:30:28,620 --> 00:30:31,320


817
00:30:31,320 --> 00:30:34,820


818
00:30:34,820 --> 00:30:37,679
在另一个系统上训练与深度神经网络相同大小和性能的尖峰神经网络相对具有挑战性，该系统

819
00:30:37,679 --> 00:30:39,539
要简单得多，但

820
00:30:39,539 --> 00:30:42,840
可能太简单了，呃是一个

821
00:30:42,840 --> 00:30:45,120
耦合振荡器网络，这些网络众所周知

822
00:30:45,120 --> 00:30:48,779
表现出同步性、时空

823
00:30:48,779 --> 00:30:52,200
动力学和复杂的模式，但是呃，

824
00:30:52,200 --> 00:30:53,520
这被称为相位缩减

825
00:30:53,520 --> 00:30:55,500
系统，并不能完全捕获

826
00:30:55,500 --> 00:30:57,059
我们感兴趣的全部复杂性，

827
00:30:57,059 --> 00:30:58,140
因此我们正在寻找

828
00:30:58,140 --> 00:31:00,779
可能介于这两者

829
00:31:00,779 --> 00:31:03,600
和我们之间的东西 确定的是这项工作中的这项工作

830
00:31:03,600 --> 00:31:06,600
是呃参数化一对

831
00:31:06,600 --> 00:31:08,520
振荡器的网络，

832
00:31:08,520 --> 00:31:10,620
比 paramoto 模型稍微灵活一些，

833
00:31:10,620 --> 00:31:12,360
所以这实际上是

834
00:31:12,360 --> 00:31:14,580
建立在 Constantine rush 和 Nisha 的这对蒸馏

835
00:31:14,580 --> 00:31:16,380
循环神经网络的基础上的，

836
00:31:16,380 --> 00:31:18,720


837
00:31:18,720 --> 00:31:20,760
他们基本上采用了

838
00:31:20,760 --> 00:31:22,200
以下方程： 描述了一个简单的

839
00:31:22,200 --> 00:31:23,820
谐振子，它是一个二阶

840
00:31:23,820 --> 00:31:26,159
微分方程，

841
00:31:26,159 --> 00:31:29,940
弹簧上的球的加速度与其位移成正比，

842
00:31:29,940 --> 00:31:32,480


843
00:31:32,480 --> 00:31:35,220
你可以添加附加项，例如

844
00:31:35,220 --> 00:31:37,260
阻尼，以便振荡

845
00:31:37,260 --> 00:31:39,360
随着时间的推移慢慢消失，

846
00:31:39,360 --> 00:31:41,580
你可以用外部驱动器驱动这个振荡器

847
00:31:41,580 --> 00:31:43,380
输入来抵消

848
00:31:43,380 --> 00:31:45,179
这种阻尼或

849
00:31:45,179 --> 00:31:47,279
给动力学带来更多的复杂性，

850
00:31:47,279 --> 00:31:49,260
此外，如果您有许多

851
00:31:49,260 --> 00:31:50,940
这样的振荡器，您可以将它们

852
00:31:50,940 --> 00:31:53,000
与这些耦合矩阵 W

853
00:31:53,000 --> 00:31:55,320
uh 耦合在一起，正如我们在这张

854
00:31:55,320 --> 00:31:56,640
图片中演示的那样，这样您就可以真正

855
00:31:56,640 --> 00:31:58,140
将此网络视为弹簧上的一堆气泡

856
00:31:58,140 --> 00:31:59,940
，它们可能

857
00:31:59,940 --> 00:32:01,740
也通过弹簧或松紧

858
00:32:01,740 --> 00:32:03,600
带相互连接，无论

859
00:32:03,600 --> 00:32:05,279
俄罗斯米什拉的耦合蒸馏循环神经网络

860
00:32:05,279 --> 00:32:08,100
呃与这些不同的术语，

861
00:32:08,100 --> 00:32:09,899
这已被证明是非常强大的

862
00:32:09,899 --> 00:32:12,480
对于长序列的建模，他们还

863
00:32:12,480 --> 00:32:13,740
提到他们受到了

864
00:32:13,740 --> 00:32:15,360
大脑构建的启发，并且

865
00:32:15,360 --> 00:32:17,700
在那篇论文中有很多很好的分析，

866
00:32:17,700 --> 00:32:19,020
例如，他们表明这对于

867
00:32:19,020 --> 00:32:21,440


868
00:32:21,440 --> 00:32:23,460


869
00:32:23,460 --> 00:32:25,440
通常发生在循环神经

870
00:32:25,440 --> 00:32:26,820
网络中的消失梯度问题来说是非常有益的特性。

871
00:32:26,820 --> 00:32:29,159
但如果我们想研究

872
00:32:29,159 --> 00:32:30,960
时空动力学和这种类型的

873
00:32:30,960 --> 00:32:32,820
模型，呃，这有点具有挑战性，

874
00:32:32,820 --> 00:32:34,919
因为这里的这些耦合矩阵，

875
00:32:34,919 --> 00:32:36,320


876
00:32:36,320 --> 00:32:39,600
连接每个神经元或每个

877
00:32:39,600 --> 00:32:41,240
振荡器的 W 彼此定位，

878
00:32:41,240 --> 00:32:43,620
这些是密集连接的矩阵，

879
00:32:43,620 --> 00:32:45,120
就像我尝试过的那样 在左边描绘，

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
所以如果你尝试可视化

882
00:32:48,299 --> 00:32:50,580
这个网络的动态，你看不到任何

883
00:32:50,580 --> 00:32:51,899
空间组织，没有

884
00:32:51,899 --> 00:32:55,380
继承，这是对

885
00:32:55,380 --> 00:32:57,000
这个模型的潜在空间的道歉，

886
00:32:57,000 --> 00:32:58,799
嗯，所以你可以像我们

887
00:32:58,799 --> 00:33:01,020
前面的例子中的神经元一样思考这一点 连接

888
00:33:01,020 --> 00:33:03,240
到一组可能任意的其他

889
00:33:03,240 --> 00:33:04,919
神经元这些神经元连接到

890
00:33:04,919 --> 00:33:06,600
另一组任意的神经元，

891
00:33:06,600 --> 00:33:08,520
你肯定会得到振荡动力学，

892
00:33:08,520 --> 00:33:10,860
但这种波动

893
00:33:10,860 --> 00:33:13,260
不会产生很多结构化意义，所以

894
00:33:13,260 --> 00:33:15,360
在我们的工作中，我们认为 好吧，我们如何才能

895
00:33:15,360 --> 00:33:18,299
将其更多地转换为

896
00:33:18,299 --> 00:33:19,860
我们对

897
00:33:19,860 --> 00:33:22,140
活动的结构化传播感兴趣的动态类型 uh，

898
00:33:22,140 --> 00:33:23,940
一种明确的方法是拥有

899
00:33:23,940 --> 00:33:26,539
一个更加结构化的连接矩阵 W

900
00:33:26,539 --> 00:33:29,880
uh，我们发现它很容易实现

901
00:33:29,880 --> 00:33:31,559
且高效 通过

902
00:33:31,559 --> 00:33:33,000
卷积运算实现，您可以将其

903
00:33:33,000 --> 00:33:34,620
视为局部局部

904
00:33:34,620 --> 00:33:36,299
连接层，因此

905
00:33:36,299 --> 00:33:37,860
每个神经元不是连接到每个神经

906
00:33:37,860 --> 00:33:39,480
元，而是仅连接到其

907
00:33:39,480 --> 00:33:41,580
附近的邻居，训练后

908
00:33:41,580 --> 00:33:42,840
您最终会得到

909
00:33:42,840 --> 00:33:44,880
看起来像平滑空间的东西

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
为了更清楚地

912
00:33:48,419 --> 00:33:50,519
训练这个模型，我们采用这个单独的

913
00:33:50,519 --> 00:33:52,200
二阶微分方程，在将

914
00:33:52,200 --> 00:33:54,299


915
00:33:54,299 --> 00:33:56,340
其离散化为两个一阶方程之前我们已经描述过，您

916
00:33:56,340 --> 00:33:57,960
可以将其视为对

917
00:33:57,960 --> 00:34:01,200
我们现在有的颂歌进行数值积分

918
00:34:01,200 --> 00:34:03,120
速度，然后我们更新

919
00:34:03,120 --> 00:34:06,000
呃，我们可以将该模型训练为

920
00:34:06,000 --> 00:34:07,620
自动编码器或

921
00:34:07,620 --> 00:34:09,839
自动回归模型之类的东西，因此如果我们接受输入，

922
00:34:09,839 --> 00:34:11,460
我们将其编码到我们的潜在空间，

923
00:34:11,460 --> 00:34:14,339
实际上输入是 Dr 是 x 项的 f ，

924
00:34:14,339 --> 00:34:16,500
其作用 作为驱动项，所以

925
00:34:16,500 --> 00:34:18,599
这就像从底部驱动这些振荡器，

926
00:34:18,599 --> 00:34:20,879
然后它们有

927
00:34:20,879 --> 00:34:23,099
自己的动力学，这些动力学是由

928
00:34:23,099 --> 00:34:25,800
这些局部耦合的耦合项定义的，然后

929
00:34:25,800 --> 00:34:27,540
在每个时间步，我们采用这个

930
00:34:27,540 --> 00:34:29,460
潜在状态，这个波状态，然后我们

931
00:34:29,460 --> 00:34:31,560
解码以尝试 并重建输入，

932
00:34:31,560 --> 00:34:33,540
并在当前时间步长或

933
00:34:33,540 --> 00:34:35,699
未来时间步长，

934
00:34:35,699 --> 00:34:37,980
我们可以在训练期间对这些模型进行一些分析，

935
00:34:37,980 --> 00:34:42,300
看看

936
00:34:42,300 --> 00:34:43,619
训练前和训练后发生了什么，

937
00:34:43,619 --> 00:34:45,780
我们可以计算

938
00:34:45,780 --> 00:34:47,399
动态的相位和速度

939
00:34:47,399 --> 00:34:49,379
潜在空间基本上我们在

940
00:34:49,379 --> 00:34:51,480
交易开始时看到模型中没有波浪，

941
00:34:51,480 --> 00:34:53,699
但是经过 50 个时期的训练后，

942
00:34:53,699 --> 00:34:55,500
我们看到有一个平滑的

943
00:34:55,500 --> 00:34:57,119
结构化活动

944
00:34:57,119 --> 00:35:00,420
向下传播，呃，为

945
00:35:00,420 --> 00:35:01,800
我们正在做的序列建模任务服务，就像

946
00:35:01,800 --> 00:35:04,380
旋转对象一样，

947
00:35:04,380 --> 00:35:05,940
所以 这样做的好处是什么

948
00:35:05,940 --> 00:35:07,680
我的意思是我发起这个的全部原因

949
00:35:07,680 --> 00:35:10,380
是说我们想要有更

950
00:35:10,380 --> 00:35:11,880
灵活的学习结构我们

951
00:35:11,880 --> 00:35:13,020
真的在这样做还是我们只是

952
00:35:13,020 --> 00:35:15,060
得到了漂亮的波浪

953
00:35:15,060 --> 00:35:16,859
嗯所以我们在论文中展示的是

954
00:35:16,859 --> 00:35:19,320
我们真的是 学习某种

955
00:35:19,320 --> 00:35:20,940
有用的结构以及我们展示的方式，就像

956
00:35:20,940 --> 00:35:22,440
这个

957
00:35:22,440 --> 00:35:24,960
交换图一样，如果你接受一个输入

958
00:35:24,960 --> 00:35:27,000
并对它进行编码，你会得到一个波

959
00:35:27,000 --> 00:35:29,280
状态，然后你

960
00:35:29,280 --> 00:35:31,619
在该波状态中人工传播波，然后

961
00:35:31,619 --> 00:35:33,480
解码你可以 观察一下，它

962
00:35:33,480 --> 00:35:35,220
实际上与您

963
00:35:35,220 --> 00:35:37,140
刚刚通过显示一堆

964
00:35:37,140 --> 00:35:39,180
不同变换的不同图像相同，因此

965
00:35:39,180 --> 00:35:41,640
有很多不同的数字不同的

966
00:35:41,640 --> 00:35:43,920
特征，我们看到

967
00:35:43,920 --> 00:35:46,140
在每种情况下我们都会得到不同类型的波浪活动，

968
00:35:46,140 --> 00:35:47,880
以便对其进行建模

969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
如果我们在不同的数据集上训练它，

971
00:35:51,119 --> 00:35:53,400
我们也会看到不同的变换，在这种情况下，我们同样会看到更复杂的

972
00:35:53,400 --> 00:35:55,200
动力学，甚至可能不是

973
00:35:55,200 --> 00:35:57,839
行波或驻波，它们

974
00:35:57,839 --> 00:36:00,359
可以被认为是

975
00:36:00,359 --> 00:36:02,339
相反方向的行波，所以我们看看我们是否正在对

976
00:36:02,339 --> 00:36:04,079
这些轨道进行建模 动力学，我们在潜在空间中得到

977
00:36:04,079 --> 00:36:06,000
这种平滑移动的

978
00:36:06,000 --> 00:36:07,619
活动斑点，如果我们对

979
00:36:07,619 --> 00:36:09,839
一个钟摆进行建模，我们同样会得到

980
00:36:09,839 --> 00:36:13,820
一种复杂的振荡活动，

981
00:36:14,099 --> 00:36:17,339
因此它保留了输入结构，但

982
00:36:17,339 --> 00:36:19,560
比我们以前有更多的灵活性，

983
00:36:19,560 --> 00:36:21,599
这是我们的最终

984
00:36:21,599 --> 00:36:23,400
目标

985
00:36:23,400 --> 00:36:26,099
所以最后我想谈谈

986
00:36:26,099 --> 00:36:28,320
我如何认为这项研究的结果

987
00:36:28,320 --> 00:36:30,420
不仅可以改善

988
00:36:30,420 --> 00:36:32,099
人工智能，而且还可以帮助我们

989
00:36:32,099 --> 00:36:34,440
理解为什么我们对大脑的测量

990
00:36:34,440 --> 00:36:36,240
看起来像他们这样做的方式，以给出一个

991
00:36:36,240 --> 00:36:38,579
简短的例子来说明我的想法 呃，

992
00:36:38,579 --> 00:36:41,040
我之前谈过一些关于签证

993
00:36:41,040 --> 00:36:43,740
和地点的问题，所以在与 Ching higao 的这项出色的工作中，

994
00:36:43,740 --> 00:36:46,859
我们研究了我们

995
00:36:46,859 --> 00:36:48,900
讨论的简单地形先验

996
00:36:48,900 --> 00:36:50,579
是否能够重现这些相同的

997
00:36:50,579 --> 00:36:53,339
效果，所以具体来说，我们把

998
00:36:53,339 --> 00:36:56,099
这个 Cohen's D 的值

999
00:36:56,099 --> 00:36:58,200
我们每个神经元相对于

1000
00:36:58,200 --> 00:37:00,000
可能

1001
00:37:00,000 --> 00:37:02,460
仅包含面部或仅包含物体或

1002
00:37:02,460 --> 00:37:03,359
身体的不同图像数据集的选择性度量，

1003
00:37:03,359 --> 00:37:05,880
因此我们测量每个神经元是否

1004
00:37:05,880 --> 00:37:07,920
更有可能对面部或

1005
00:37:07,920 --> 00:37:10,380
大脑中出现的俄语做出反应，但我确实这样做

1006
00:37:10,380 --> 00:37:12,839
认为它告诉我们，

1007
00:37:12,839 --> 00:37:15,300
选择性的相对组织可能至少

1008
00:37:15,300 --> 00:37:17,400
部分归因于

1009
00:37:17,400 --> 00:37:19,800
数据中的相关统计数据

1010
00:37:19,800 --> 00:37:21,359
在通过高度

1011
00:37:21,359 --> 00:37:23,640
非线性的未来提取器（例如

1012
00:37:23,640 --> 00:37:25,440
深度神经网络）后必须重新路径，

1013
00:37:25,440 --> 00:37:27,480
因此以类似的方式，

1014
00:37:27,480 --> 00:37:29,040
有趣的是，有一个已知的

1015
00:37:29,040 --> 00:37:30,900
所谓的三方或非视觉

1016
00:37:30,900 --> 00:37:36,720
流，所以呃呃或物体的图像是

1017
00:37:36,720 --> 00:37:38,400
相对于物体的选择性是

1018
00:37:38,400 --> 00:37:40,680
由更抽象的属性组织的，

1019
00:37:40,680 --> 00:37:43,200
例如生命力是这个东西是活的还是无生命的

1020
00:37:43,200 --> 00:37:46,200
呃与现实世界的

1021
00:37:46,200 --> 00:37:48,480
物体大小相比，比如什么 是

1022
00:37:48,480 --> 00:37:50,700
茶壶与汽车的大小，

1023
00:37:50,700 --> 00:37:53,520
嗯，我们看到的是，在

1024
00:37:53,520 --> 00:37:56,160
人类中，选择性是

1025
00:37:56,160 --> 00:37:57,540
在这种三方结构中组织的，

1026
00:37:57,540 --> 00:37:59,760
通常有一些小物体的选择性

1027
00:37:59,760 --> 00:38:01,920
介于有生命和无生命的物体之间，

1028
00:38:01,920 --> 00:38:04,200
我们看到

1029
00:38:04,200 --> 00:38:06,060
这里发生了同样的事情，所以

1030
00:38:06,060 --> 00:38:07,440
这些正在测量

1031
00:38:07,440 --> 00:38:08,880
同一组神经元的选择性，但是相

1032
00:38:08,880 --> 00:38:10,859
对于刺激的这些差异，您会看到

1033
00:38:10,859 --> 00:38:12,440
小簇位于

1034
00:38:12,440 --> 00:38:14,820
有生命的簇和无生命的簇之间，并且

1035
00:38:14,820 --> 00:38:16,079
这种情况再次发生在多个不同的

1036
00:38:16,079 --> 00:38:18,900
初始化中，所以这 我

1037
00:38:18,900 --> 00:38:20,880
希望我们可以为

1038
00:38:20,880 --> 00:38:22,980
这个社区进一步探索，我认为这很有趣，

1039
00:38:22,980 --> 00:38:24,119
因为它

1040
00:38:24,119 --> 00:38:26,220
确实是一种表明我们

1041
00:38:26,220 --> 00:38:28,200
构建了结构化世界模型的方式，并且

1042
00:38:28,200 --> 00:38:30,119
这个世界模型可能有

1043
00:38:30,119 --> 00:38:31,980
利于以

1044
00:38:31,980 --> 00:38:34,740


1045
00:38:34,740 --> 00:38:37,619
结构化方式更好地表示现实世界数据，并且

1046
00:38:37,619 --> 00:38:39,119
从这个意义上说，你得到的自由能较低，

1047
00:38:39,119 --> 00:38:40,619
所以

1048
00:38:40,619 --> 00:38:42,300
嗯，我认为通过开发这些模型，

1049
00:38:42,300 --> 00:38:44,400
就像我们在这里展示的那样，我们可能会深入

1050
00:38:44,400 --> 00:38:46,500
了解

1051
00:38:46,500 --> 00:38:48,900
这种结构如何出现的新机制，包括

1052
00:38:48,900 --> 00:38:50,460
我们以前从未想到过的地形组织，

1053
00:38:50,460 --> 00:38:52,920
所以我正在研究机器模型

1054
00:38:52,920 --> 00:38:55,520


1055
00:38:55,520 --> 00:38:58,260
神经元的方向选择性，我并没有

1056
00:38:58,260 --> 00:39:01,020
特别期待会

1057
00:39:01,020 --> 00:39:03,420
发生什么，但是呃，你看到的是

1058
00:39:03,420 --> 00:39:05,339
这些波在这个

1059
00:39:05,339 --> 00:39:08,099
模拟的垂直表面上传播，我想

1060
00:39:08,099 --> 00:39:09,960
好吧，也许我正在显示旋转的图像，

1061
00:39:09,960 --> 00:39:11,820
也许这会对

1062
00:39:11,820 --> 00:39:13,740
方向选择性

1063
00:39:13,740 --> 00:39:15,599
，实际上，如果你进去

1064
00:39:15,599 --> 00:39:17,460
测量每个神经元

1065
00:39:17,460 --> 00:39:18,660
对这些不同

1066
00:39:18,660 --> 00:39:22,079
方向线的选择性，你会看到它

1067
00:39:22,079 --> 00:39:24,300
令人惊讶地让人想起

1068
00:39:24,300 --> 00:39:25,859
在初级视觉皮层中看到的东方类型的柱子，

1069
00:39:25,859 --> 00:39:27,599
这是可以追溯到的东西

1070
00:39:27,599 --> 00:39:29,520
雨果和黄鼠狼，这是

1071
00:39:29,520 --> 00:39:30,900
从这个模型中得出的东西

1072
00:39:30,900 --> 00:39:33,060
，事实上它具有

1073
00:39:33,060 --> 00:39:34,440
与变换相关的时空结构，

1074
00:39:34,440 --> 00:39:37,619
所以呃，当然这是

1075
00:39:37,619 --> 00:39:39,599
一个非常粗略的类比，但我认为这

1076
00:39:39,599 --> 00:39:40,740
是一个例子 建立这些

1077
00:39:40,740 --> 00:39:42,839
类型的模型可以帮助我们思考

1078
00:39:42,839 --> 00:39:45,240
大脑如何构建表征

1079
00:39:45,240 --> 00:39:46,980
结构以及它的

1080
00:39:46,980 --> 00:39:48,660
组织方式，这可能是我们

1081
00:39:48,660 --> 00:39:51,300
之前没有考虑过的，

1082
00:39:51,300 --> 00:39:53,460
嗯，我想我不是唯一一个

1083
00:39:53,460 --> 00:39:55,859
做这种类型的人 工作，所以我

1084
00:39:55,859 --> 00:39:57,240
想谈谈其他

1085
00:39:57,240 --> 00:39:59,579
一些正在做这件事的人，嗯，所以

1086
00:39:59,579 --> 00:40:00,780
我一直在谈论类似的

1087
00:40:00,780 --> 00:40:02,760
等效结构，

1088
00:40:02,760 --> 00:40:04,920
嗯，诸如 James Whittington 和

1089
00:40:04,920 --> 00:40:08,880
Tim Barons 以及 surrogengoolie 等人，

1090
00:40:08,880 --> 00:40:10,680
最近通过引入代数证明了这一点

1091
00:40:10,680 --> 00:40:14,940


1092
00:40:14,940 --> 00:40:17,040
在这种情况下，对学习过程的限制

1093
00:40:17,040 --> 00:40:20,820
就像呃和环境中代理的运动一样，

1094
00:40:20,820 --> 00:40:23,220


1095
00:40:23,220 --> 00:40:24,780


1096
00:40:24,780 --> 00:40:27,540
如果我在一个环西

1097
00:40:27,540 --> 00:40:29,280
东北南移动，你需要保留这种代数结构，我最终会回到

1098
00:40:29,280 --> 00:40:31,440
同样的位置 再次，通过引入这些

1099
00:40:31,440 --> 00:40:32,820
类型的约束，

1100
00:40:32,820 --> 00:40:35,040
你会出现像网格单元一样的

1101
00:40:35,040 --> 00:40:36,900
表示，

1102
00:40:36,900 --> 00:40:39,359
嗯，所以我有兴趣看看这种

1103
00:40:39,359 --> 00:40:41,880
表示结构的想法如何

1104
00:40:41,880 --> 00:40:43,980
帮助我们解释可能比我们

1105
00:40:43,980 --> 00:40:45,480
发现的科学发现更多的东西，

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
嗯， 以及这与

1108
00:40:48,359 --> 00:40:51,540
整个生成模型有何关系

1109
00:40:51,540 --> 00:40:52,800
，最后我认为

1110
00:40:52,800 --> 00:40:54,599
关于

1111
00:40:54,599 --> 00:40:56,099
这些模型的认知可能性也有一些话要说，

1112
00:40:56,099 --> 00:40:57,420
也许我们不仅要从

1113
00:40:57,420 --> 00:40:59,579
呃神经科学的角度来测试它们，

1114
00:40:59,579 --> 00:41:01,020
而且还要从微观认知科学的

1115
00:41:01,020 --> 00:41:03,839
角度来测试它们 例如，左边有这些

1116
00:41:03,839 --> 00:41:06,000
乌鸦渐进矩阵，

1117
00:41:06,000 --> 00:41:08,640
您必须说出

1118
00:41:08,640 --> 00:41:11,099
这些图像中哪一张更有可能符合

1119
00:41:11,099 --> 00:41:12,599
此模式，

1120
00:41:12,599 --> 00:41:14,760
或者例如，

1121
00:41:14,760 --> 00:41:16,740
当您拉过拉力时，这个 Jenga 塔倒塌的可能性有多大

1122
00:41:16,740 --> 00:41:19,740
一个特定的块或或

1123
00:41:19,740 --> 00:41:22,740
具有给定的结构，我认为这些

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
类型的测试实际上是在测试

1126
00:41:26,640 --> 00:41:28,619
我们正在构建的世界模型是否

1127
00:41:28,619 --> 00:41:31,560
与

1128
00:41:31,560 --> 00:41:33,660
我们天生具有

1129
00:41:33,660 --> 00:41:36,060
人类或人类常识的模型类型相似。 作为生活在

1130
00:41:36,060 --> 00:41:38,400
自然世界中的生物，我已经

1131
00:41:38,400 --> 00:41:40,440
在这个方向上做了一些初步工作，我

1132
00:41:40,440 --> 00:41:43,079
认为非常初步，并不是那么

1133
00:41:43,079 --> 00:41:45,480
复杂，但是嗯，有点试图

1134
00:41:45,480 --> 00:41:47,820
模拟视觉错觉，所以如果你采用

1135
00:41:47,820 --> 00:41:50,520
一个非常简单的移动条数据集

1136
00:41:50,520 --> 00:41:52,980
刺激或静态条或框架，你

1137
00:41:52,980 --> 00:41:54,960
稍微移动它，你可以看到

1138
00:41:54,960 --> 00:41:57,060
模型实际上会推断出

1139
00:41:57,060 --> 00:41:58,800
丢失的帧，然后实际上还会

1140
00:41:58,800 --> 00:42:01,079
推断出持续的运动，所以这就像

1141
00:42:01,079 --> 00:42:03,300


1142
00:42:03,300 --> 00:42:05,820
在再次校正之前超出了实际刺激提供的轨迹

1143
00:42:05,820 --> 00:42:08,760
所以我认为对幻象进行建模

1144
00:42:08,760 --> 00:42:10,320
肯定是一种有趣的

1145
00:42:10,320 --> 00:42:12,660
方法来研究我们的世界模型是否与

1146
00:42:12,660 --> 00:42:14,760
我们自己拥有的模型类型相似

1147
00:42:14,760 --> 00:42:16,619


1148
00:42:16,619 --> 00:42:19,619
所以结论是呃是的我认为

1149
00:42:19,619 --> 00:42:21,900
地形先验我们可以表明

1150
00:42:21,900 --> 00:42:23,220
他们有效地学习了结构化

1151
00:42:23,220 --> 00:42:24,839
表示或结构化世界

1152
00:42:24,839 --> 00:42:26,700
模型 学习的结构是

1153
00:42:26,700 --> 00:42:29,160
灵活的，并且能够适应任意的

1154
00:42:29,160 --> 00:42:30,780
变换，与传统的等

1155
00:42:30,780 --> 00:42:33,720
变体不同，并且地形提供者

1156
00:42:33,720 --> 00:42:35,579
可以像我们

1157
00:42:35,579 --> 00:42:37,619
在地形 vae 中所做的那样通过统计方式进行诱导，或者通过

1158
00:42:37,619 --> 00:42:39,480
动力学（就像我们在这些

1159
00:42:39,480 --> 00:42:42,000
神经波机类型模型中展示的那样）

1160
00:42:42,000 --> 00:42:44,460
所以总而言之，我将留给您这个 引用

1161
00:42:44,460 --> 00:42:46,980
我在福岛 1980 年的论文中找到的一段话，

1162
00:42:46,980 --> 00:42:50,280
我认为这远远超前

1163
00:42:50,280 --> 00:42:52,079
于那个时代，他说如果我们能够

1164
00:42:52,079 --> 00:42:53,520
制作一个具有与人类

1165
00:42:53,520 --> 00:42:55,020
相同的模式识别能力的神经网络模型，

1166
00:42:55,020 --> 00:42:57,060


1167
00:42:57,060 --> 00:42:58,800
那么与

1168
00:42:58,800 --> 00:43:00,000
了解大脑中的神经机制，

1169
00:43:00,000 --> 00:43:03,240
这就是我认为

1170
00:43:03,240 --> 00:43:06,119
我们在这里要实现的一些目标，

1171
00:43:06,119 --> 00:43:08,220
所以我认为这是我的顾问 Max，我的

1172
00:43:08,220 --> 00:43:11,280
合著者 Patrick UA Emil jinghian

1173
00:43:11,280 --> 00:43:17,359
和 Yorn 对讨论感兴趣，谢谢，好吧，

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
好吧 谢谢你，非常非常

1176
00:43:27,480 --> 00:43:31,079
有趣的演讲，

1177
00:43:31,079 --> 00:43:33,480
有很多地方可以开始，也许只是呃，是什么

1178
00:43:33,480 --> 00:43:36,000
让你从事这项工作，

1179
00:43:36,000 --> 00:43:38,520
介绍一下你是如何进入

1180
00:43:38,520 --> 00:43:43,819
这项工作的，以获得你的博士学位方向，

1181
00:43:43,920 --> 00:43:45,119
是的，

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
我的意思是，我们一直在研究，不是我的

1184
00:43:49,200 --> 00:43:51,000
小组 我在大学里

1185
00:43:51,000 --> 00:43:52,700


1186
00:43:52,700 --> 00:43:56,640
从数学的角度研究结构化表示一段

1187
00:43:56,640 --> 00:43:58,319
时间了，有些人有

1188
00:43:58,319 --> 00:44:00,240
模型，比如变分

1189
00:44:00,240 --> 00:44:01,740
自动编码器，

1190
00:44:01,740 --> 00:44:04,680


1191
00:44:04,680 --> 00:44:06,960
我猜一直以来

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
都是模型的东西 很好地尊重旋转 2D

1194
00:44:11,220 --> 00:44:13,560
旋转，但如果我们

1195
00:44:13,560 --> 00:44:15,960
想做 3D 旋转，我们不能这样做，

1196
00:44:15,960 --> 00:44:17,819
因为就投影到 2D 平面而言，这不是一个组，

1197
00:44:17,819 --> 00:44:19,740


1198
00:44:19,740 --> 00:44:21,180
当这个东西

1199
00:44:21,180 --> 00:44:23,460
旋转时，例如

1200
00:44:23,460 --> 00:44:24,240
嗯

1201
00:44:24,240 --> 00:44:26,280
或

1202
00:44:26,280 --> 00:44:27,960
就像我一

1203
00:44:27,960 --> 00:44:29,339
开始就试图指出的任何类型的自然变换，我认为它

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
试图思考大脑如何模拟

1206
00:44:31,740 --> 00:44:34,020
自然变换，

1207
00:44:34,020 --> 00:44:35,400
这些当前的

1208
00:44:35,400 --> 00:44:37,200
框架

1209
00:44:37,200 --> 00:44:41,099
在哪里你看到动作

1210
00:44:41,099 --> 00:44:44,579
在变分自动编码器方面发挥作用

1211
00:44:44,579 --> 00:44:48,420
模型不仅包括

1212
00:44:48,420 --> 00:44:50,520
外部模式，还包括

1213
00:44:50,520 --> 00:44:52,380
行动的后果或行动的世界模型

1214
00:44:52,380 --> 00:44:55,800
结构结构，是的，

1215
00:44:55,800 --> 00:44:58,619
不，这是一个很好的问题，

1216
00:44:58,619 --> 00:45:01,319
我认为行动推论呃

1217
00:45:01,319 --> 00:45:03,839
实际上是答案，我认为

1218
00:45:03,839 --> 00:45:05,940
这是一个很好的答案，

1219
00:45:05,940 --> 00:45:09,000
嗯我知道那里

1220
00:45:09,000 --> 00:45:11,099
强化学习框架

1221
00:45:11,099 --> 00:45:12,660
确实使用

1222
00:45:12,660 --> 00:45:15,060
某种外部训练的世界模型，

1223
00:45:15,060 --> 00:45:17,280
因此您可以训练 vae 或其他东西，然后

1224
00:45:17,280 --> 00:45:19,800
在强化学习系统中使用该表示形式，

1225
00:45:19,800 --> 00:45:23,040
但我

1226
00:45:23,040 --> 00:45:24,720
认为拥有一个完整

1227
00:45:24,720 --> 00:45:26,520
的系统，该系统是具有

1228
00:45:26,520 --> 00:45:30,780
呃行动的单一目标 作为

1229
00:45:30,780 --> 00:45:33,660
数据可能性的一部分，嗯，

1230
00:45:33,660 --> 00:45:35,280
是的，我认为这更优雅

1231
00:45:35,280 --> 00:45:38,940
，所以我是这一点的大力支持者，嗯，

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
我还没有研究过如何

1234
00:45:43,140 --> 00:45:45,480
在 vae 中构建这些结构化的世界模型，

1235
00:45:45,480 --> 00:45:47,520
或者我还没有' 我根本没有在这方面进行过工作，但我

1236
00:45:47,520 --> 00:45:48,780
认为

1237
00:45:48,780 --> 00:45:50,819
看看在变分自动编码器中拥有一个更

1238
00:45:50,819 --> 00:45:52,339
结构化的世界模型是否

1239
00:45:52,339 --> 00:45:54,839
会

1240
00:45:54,839 --> 00:45:56,880
有好处，这肯定会非常有趣，因为在活跃的

1241
00:45:56,880 --> 00:45:58,319
环境中我认为这会

1242
00:45:58,319 --> 00:46:00,119
很棒我的意思是我 想想

1243
00:46:00,119 --> 00:46:03,599
这些例子中的一些例子，比如

1244
00:46:03,599 --> 00:46:05,579
之前展示的网格细胞的出现，

1245
00:46:05,579 --> 00:46:07,500
像这样的东西可能指向

1246
00:46:07,500 --> 00:46:08,880
那个方向，

1247
00:46:08,880 --> 00:46:10,560
好吧，也许大脑正在做一些事情，

1248
00:46:10,560 --> 00:46:12,540
它确实显然有很多

1249
00:46:12,540 --> 00:46:13,680
结构，

1250
00:46:13,680 --> 00:46:15,359
嗯，这显然对于

1251
00:46:15,359 --> 00:46:19,140
在某些方面执行动作很有用

1252
00:46:19,140 --> 00:46:21,720
哦，是的，我觉得

1253
00:46:21,720 --> 00:46:24,480
你在演讲中引入的一个非常好的相似之处

1254
00:46:24,480 --> 00:46:28,040
是局部连接的单元

1255
00:46:28,040 --> 00:46:30,960
使你的模型能够在结构上

1256
00:46:30,960 --> 00:46:33,780
体现卷积

1257
00:46:33,780 --> 00:46:35,640
约束和模式，并导致

1258
00:46:35,640 --> 00:46:37,500
这些出现的模式，然后

1259
00:46:37,500 --> 00:46:41,339
类似地有呃多拉尔在

1260
00:46:41,339 --> 00:46:45,680
哪里 他们的路径探索

1261
00:46:45,680 --> 00:46:48,359
约束是正确的，所以

1262
00:46:48,359 --> 00:46:50,280
有趣的是，嗯，

1263
00:46:50,280 --> 00:46:53,760
你知道，考虑这些行动或

1264
00:46:53,760 --> 00:46:56,819
政策启发式或稀疏性，比如

1265
00:46:56,819 --> 00:46:59,579
联合运动探索，最终人们

1266
00:46:59,579 --> 00:47:02,339
会明白，有两种

1267
00:47:02,339 --> 00:47:04,980
相互对立的方式来移动关节

1268
00:47:04,980 --> 00:47:07,079
，然后是组合性

1269
00:47:07,079 --> 00:47:09,119


1270
00:47:09,119 --> 00:47:10,680
一旦锁定在较低的水平，就可以在这些较高的

1271
00:47:10,680 --> 00:47:14,480
水平上学习跨关节的知识，因此这是一种非常有吸引力

1272
00:47:14,480 --> 00:47:17,599
且与

1273
00:47:17,599 --> 00:47:20,460
利基相关的概括方式，

1274
00:47:20,460 --> 00:47:23,819
因为它既基于

1275
00:47:23,819 --> 00:47:25,740
世界的实际约束，又

1276
00:47:25,740 --> 00:47:27,720
特别是通过

1277
00:47:27,720 --> 00:47:29,460
可能嵌入某些东西的行动

1278
00:47:29,460 --> 00:47:31,380
很简单，

1279
00:47:31,380 --> 00:47:33,599
是的，不，我认为这绝对是

1280
00:47:33,599 --> 00:47:36,599
真的，这是一个非常好的观点，如果呃，如果

1281
00:47:36,599 --> 00:47:38,339
你确实受到来自你的

1282
00:47:38,339 --> 00:47:40,500
行为本身的限制，那么这对于

1283
00:47:40,500 --> 00:47:42,839
帮助

1284
00:47:42,839 --> 00:47:44,819
构建

1285
00:47:44,819 --> 00:47:47,460
你的潜在空间将非常有益，我想是的，我

1286
00:47:47,460 --> 00:47:48,480
猜一件事 我想说的

1287
00:47:48,480 --> 00:47:49,980
是，有一些

1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
东西让我想起 Stefano

1290
00:47:52,740 --> 00:47:55,500
Fousey 在表征几何方面的工作，

1291
00:47:55,500 --> 00:47:58,859


1292
00:47:58,859 --> 00:48:01,920
决定了我们

1293
00:48:01,920 --> 00:48:04,440
如何概括对系统的给定理解的通用性，

1294
00:48:04,440 --> 00:48:08,099
我想如果你能

1295
00:48:08,099 --> 00:48:11,880
理解这些活动集

1296
00:48:11,880 --> 00:48:14,520


1297
00:48:14,520 --> 00:48:16,079


1298
00:48:16,079 --> 00:48:18,839
本质上，与线性分类器可分离或高度并行可分离，那么你将

1299
00:48:18,839 --> 00:48:20,700
能够进行泛化，我

1300
00:48:20,700 --> 00:48:23,099
认为通过施加这些类型的偏差

1301
00:48:23,099 --> 00:48:25,040
或潜在地通过由

1302
00:48:25,040 --> 00:48:27,000


1303
00:48:27,000 --> 00:48:28,740
类似这样的动作施加的约束，

1304
00:48:28,740 --> 00:48:32,040
你将产生或诱导一种

1305
00:48:32,040 --> 00:48:33,660
更好的代表性几何学，

1306
00:48:33,660 --> 00:48:35,220
这对于组合性有各种各样的好处是的，

1307
00:48:35,220 --> 00:48:36,660


1308
00:48:36,660 --> 00:48:39,359
我们的概括所以

1309
00:48:39,359 --> 00:48:41,760
这是一个很棒的点很酷是的非常

1310
00:48:41,760 --> 00:48:43,440
有趣的领域好吧我会阅读

1311
00:48:43,440 --> 00:48:45,960
实时聊天中的一些问题

1312
00:48:45,960 --> 00:48:48,420
爱进化写了

1313
00:48:48,420 --> 00:48:52,260
关于建模幻觉学习的任何实际或观察到的限制

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
社区是他们不

1316
00:49:00,420 --> 00:49:03,060
恐惧你没有

1317
00:49:03,060 --> 00:49:05,940
凝视中心那么你也没有

1318
00:49:05,940 --> 00:49:07,940
嗯

1319
00:49:08,339 --> 00:49:11,460
像时间我的意思是大多数卷积

1320
00:49:11,460 --> 00:49:13,260
神经网络我正在使用这种

1321
00:49:13,260 --> 00:49:15,599
循环神经网络但时间

1322
00:49:15,599 --> 00:49:18,420
没有明确定义 在这些模型中，

1323
00:49:18,420 --> 00:49:20,220
因为它是在一个连续的时间环境中，

1324
00:49:20,220 --> 00:49:23,400
对于一个正在经历幻觉试验的人类来说，

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319


1327
00:49:25,319 --> 00:49:27,480
嗯，我认为这两个

1328
00:49:27,480 --> 00:49:30,359
事实的结合，作为一个人或大多数

1329
00:49:30,359 --> 00:49:33,300
事物，

1330
00:49:33,300 --> 00:49:35,940
你的目光的移动位置和

1331
00:49:35,940 --> 00:49:38,220
你的收益取决于 就像你

1332
00:49:38,220 --> 00:49:40,140
在某个特定领域进行大量

1333
00:49:40,140 --> 00:49:42,780
认知测试一样，所以我认为

1334
00:49:42,780 --> 00:49:46,560
如果我们有模型，那将会非常有帮助，是的，

1335
00:49:46,560 --> 00:49:48,540
我的意思是了解到你可以将

1336
00:49:48,540 --> 00:49:50,760
其视为一种行动，例如学习

1337
00:49:50,760 --> 00:49:52,980
将目光移向何处 最

1338
00:49:52,980 --> 00:49:54,420
简单的可能

1339
00:49:54,420 --> 00:49:56,220
对建立幻觉模型有很大帮助，

1340
00:49:56,220 --> 00:49:58,859
我的意思是对我来说，就像我读了一篇

1341
00:49:58,859 --> 00:50:00,720
关于一些认知科学

1342
00:50:00,720 --> 00:50:02,940
实验或关于某些幻觉的论文，

1343
00:50:02,940 --> 00:50:05,160
我想可以把这个数据

1344
00:50:05,160 --> 00:50:07,560
集放入我的数据集吗？ 建模并测试它，

1345
00:50:07,560 --> 00:50:08,579
大多数时候答案是否定的，

1346
00:50:08,579 --> 00:50:10,619
因为我没有一个可以

1347
00:50:10,619 --> 00:50:12,900
环顾四周或视野受限的模型，

1348
00:50:12,900 --> 00:50:14,660


1349
00:50:14,660 --> 00:50:16,619
所以是的，我认为这是

1350
00:50:16,619 --> 00:50:19,680
限制之一，另一个是

1351
00:50:19,680 --> 00:50:20,579
嗯

1352
00:50:20,579 --> 00:50:22,920
是的，让 实验要

1353
00:50:22,920 --> 00:50:24,900
复杂得多，所以这是

1354
00:50:24,900 --> 00:50:27,359
实际的限制之一

1355
00:50:27,359 --> 00:50:30,240
哇，很好的答案让我想起一篇

1356
00:50:30,240 --> 00:50:33,920
论文，上面有字母在桌子上旋转，

1357
00:50:33,920 --> 00:50:36,780
这是数字旋转

1358
00:50:36,780 --> 00:50:38,460
关于注视点和幻觉动力学的要点

1359
00:50:38,460 --> 00:50:40,079
我认为你实际上确实

1360
00:50:40,079 --> 00:50:42,599
提到了一种幻觉 然而，您

1361
00:50:42,599 --> 00:50:43,980
在二维屏幕上旋转的泛化上下文中提到的，并

1362
00:50:43,980 --> 00:50:46,859


1363
00:50:46,859 --> 00:50:49,500
不能泛化到

1364
00:50:49,500 --> 00:50:52,920
三维，并且维度崩溃

1365
00:50:52,920 --> 00:50:55,559
或缩小是立方体

1366
00:50:55,559 --> 00:50:58,619
投影幻觉和立方体和图形

1367
00:50:58,619 --> 00:51:01,880
旋转幻觉的基础，它在您的屏幕上，

1368
00:51:01,880 --> 00:51:05,280
并且有 一个剪影或者有一些

1369
00:51:05,280 --> 00:51:07,260
模糊的

1370
00:51:07,260 --> 00:51:09,839
刺激，生成它接近

1371
00:51:09,839 --> 00:51:12,359
临界点或

1372
00:51:12,359 --> 00:51:13,680
退化模型的分叉，因此它可以以

1373
00:51:13,680 --> 00:51:17,160
一种或另一种方式表示它，

1374
00:51:17,160 --> 00:51:19,920
所以很多切换幻觉

1375
00:51:19,920 --> 00:51:22,020
只是基于图像的平坦度

1376
00:51:22,020 --> 00:51:23,819


1377
00:51:23,819 --> 00:51:26,280
以及局限性和概括 是

1378
00:51:26,280 --> 00:51:28,740


1379
00:51:28,740 --> 00:51:32,460
的，是的，我想甚至还有哦，是的，

1380
00:51:32,460 --> 00:51:34,859
抱歉，有一些工作，或者

1381
00:51:34,859 --> 00:51:35,880
他们

1382
00:51:35,880 --> 00:51:37,619
可以说人们

1383
00:51:37,619 --> 00:51:39,480
脑子里有一个三维图像，

1384
00:51:39,480 --> 00:51:42,000
就像南希·肯最近是一个或她的

1385
00:51:42,000 --> 00:51:45,119
侧向，但显示是的，我

1386
00:51:45,119 --> 00:51:48,119
不' 不知道我们的模型是否有，

1387
00:51:48,119 --> 00:51:50,460
它不是超级大，

1388
00:51:50,460 --> 00:51:53,700
是的，这很有趣，嗯，好吧，

1389
00:51:53,700 --> 00:51:56,160
来自升级改造俱乐部，

1390
00:51:56,160 --> 00:51:58,200
他们在聊天中写道，

1391
00:51:58,200 --> 00:52:00,000


1392
00:52:00,000 --> 00:52:02,160
如果你想象你只想要

1393
00:52:02,160 --> 00:52:03,780
一个神经元，你能够几乎同样有效地学习，那就太好了 对每个

1394
00:52:03,780 --> 00:52:06,540
示例都有效，呃，你的模型将

1395
00:52:06,540 --> 00:52:08,579
尝试记住数据集设计

1396
00:52:08,579 --> 00:52:10,980
或类似的东西，

1397
00:52:10,980 --> 00:52:12,180
嗯，你不会有足够的

1398
00:52:12,180 --> 00:52:14,940
容量，所以是的，我认为调整

1399
00:52:14,940 --> 00:52:18,359
稀疏程度肯定是

1400
00:52:18,359 --> 00:52:22,200
一个重要因素，

1401
00:52:22,200 --> 00:52:25,020
是的 当你查看可能性时，如果

1402
00:52:25,020 --> 00:52:26,220
你正在谈论如果你正在加倍

1403
00:52:26,220 --> 00:52:28,579
框架，通常这会

1404
00:52:28,579 --> 00:52:32,040
自动与可能性本身平衡，

1405
00:52:32,040 --> 00:52:33,000
嗯，

1406
00:52:33,000 --> 00:52:34,380
如果你不进行生成建模，

1407
00:52:34,380 --> 00:52:35,760
你只会有一个稀疏性惩罚，你会

1408
00:52:35,760 --> 00:52:38,460
想要调整 这个参数

1409
00:52:38,460 --> 00:52:40,980
好吧，是的，这只是为了澄清

1410
00:52:40,980 --> 00:52:43,380
Armina 中的失控行为，其中

1411
00:52:43,380 --> 00:52:45,599
网络

1412
00:52:45,599 --> 00:52:47,040
由于各种因素（例如反馈

1413
00:52:47,040 --> 00:52:50,400
循环噪声或对抗性输入）而变得不稳定或混乱，

1414
00:52:50,400 --> 00:52:52,380


1415
00:52:52,380 --> 00:52:54,180
嗯，是的，我想我还没有

1416
00:52:54,180 --> 00:52:55,859
在循环设置中看过这个，其中 你

1417
00:52:55,859 --> 00:52:58,500
会得到反馈循环，

1418
00:52:58,500 --> 00:52:59,460
嗯，

1419
00:52:59,460 --> 00:53:01,800
但我可以，是的，我可以看到对抗性的

1420
00:53:01,800 --> 00:53:04,319
例子可能会受到

1421
00:53:04,319 --> 00:53:07,800
你的稀疏程度的影响，

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
有趣的一点是，你

1424
00:53:10,859 --> 00:53:12,660
会更容易或不太容易

1425
00:53:12,660 --> 00:53:16,040
分享我不太了解的

1426
00:53:16,040 --> 00:53:19,440
稀疏化

1427
00:53:19,440 --> 00:53:21,720
投影的例子 完全连接的高

1428
00:53:21,720 --> 00:53:23,579
维模型

1429
00:53:23,579 --> 00:53:25,619
逐渐变小，

1430
00:53:25,619 --> 00:53:27,540
总体上很好理解，

1431
00:53:27,540 --> 00:53:29,760
权衡是什么，计算更容易，

1432
00:53:29,760 --> 00:53:34,079
模型更小，稀疏，

1433
00:53:34,079 --> 00:53:36,420
基本图将更清晰地

1434
00:53:36,420 --> 00:53:39,119
表示，然后它还将具有

1435
00:53:39,119 --> 00:53:41,339
所有其他权衡 -

1436
00:53:41,339 --> 00:53:43,680
泛化的误报和否定，

1437
00:53:43,680 --> 00:53:45,720
但这就是为什么它是一个迭代拟合

1438
00:53:45,720 --> 00:53:47,579
过程，所以

1439
00:53:47,579 --> 00:53:49,760
我猜你的

1440
00:53:49,760 --> 00:53:52,800
稀疏化方法如何

1441
00:53:52,800 --> 00:53:55,700
平衡

1442
00:53:56,700 --> 00:53:59,520
不使用 AIC 或 Bic 或其他

1443
00:53:59,520 --> 00:54:01,619
模型拟合方法来确定给

1444
00:54:01,619 --> 00:54:03,660


1445
00:54:03,660 --> 00:54:07,079
定输入的相关稀疏化，

1446
00:54:07,079 --> 00:54:09,780
你如何 确定就像

1447
00:54:09,780 --> 00:54:11,940
套索回归一样，你怎么知道

1448
00:54:11,940 --> 00:54:14,339
多少，你如何阈值

1449
00:54:14,339 --> 00:54:17,220
多少，你希望它是正确的稀疏是的，

1450
00:54:17,220 --> 00:54:19,559
我认为有很多关于

1451
00:54:19,559 --> 00:54:22,440
这方面的好文献，即使如此，

1452
00:54:22,440 --> 00:54:25,319
哈佛大学和一些人喜欢他们

1453
00:54:25,319 --> 00:54:29,520
现在有些人正在使用呃已经完成了

1454
00:54:29,520 --> 00:54:31,559
这种

1455
00:54:31,559 --> 00:54:34,380
展开的

1456
00:54:34,380 --> 00:54:36,780
迭代稀疏网络，

1457
00:54:36,780 --> 00:54:37,800
它就像一个循环神经

1458
00:54:37,800 --> 00:54:40,380
网络并迭代稀疏，

1459
00:54:40,380 --> 00:54:41,940
你可以证明这会产生类似

1460
00:54:41,940 --> 00:54:45,780
红色丢失或呃团体这样的团体

1461
00:54:45,780 --> 00:54:47,520
活跃团体运动激活的东西 在

1462
00:54:47,520 --> 00:54:48,960
这里使用

1463
00:54:48,960 --> 00:54:52,859
um 在这个设置中，实际上只是

1464
00:54:52,859 --> 00:54:55,859
通过这个

1465
00:54:55,859 --> 00:54:59,280
um 这个 T 变量的构造，

1466
00:54:59,280 --> 00:55:04,079
其中我们在顶部有 Z，

1467
00:55:04,079 --> 00:55:07,859
然后它在某种程度上由

1468
00:55:07,859 --> 00:55:09,119


1469
00:55:09,119 --> 00:55:11,579
底部的 U 变量的总和控制，所以

1470
00:55:11,579 --> 00:55:13,200
如果 W 也许我不太清楚

1471
00:55:13,200 --> 00:55:16,500
这是一个连接的矩阵，

1472
00:55:16,500 --> 00:55:18,359
这就是定义该组的原因，所以我

1473
00:55:18,359 --> 00:55:20,400
定义了该组的 Varsity 呃，它将

1474
00:55:20,400 --> 00:55:22,380
所有这些 U 连接在一起，所以

1475
00:55:22,380 --> 00:55:23,940
这个想法

1476
00:55:23,940 --> 00:55:27,540
就像这里，如果所有其他一个

1477
00:55:27,540 --> 00:55:31,740
示例，如果您的所有使用 uh

1478
00:55:31,740 --> 00:55:35,520
对于给定 t 都不活跃，

1479
00:55:35,520 --> 00:55:38,280
或者如果所有变量对于给定

1480
00:55:38,280 --> 00:55:41,040
t uh 都活跃，那么 t 变量将非常

1481
00:55:41,040 --> 00:55:42,780
小，因为您的分母

1482
00:55:42,780 --> 00:55:44,339
将非常大并且会导致

1483
00:55:44,339 --> 00:55:47,160
稀疏性，所以它是 呃，这是一个约束

1484
00:55:47,160 --> 00:55:49,260
满足，如果你有一个如果你有一

1485
00:55:49,260 --> 00:55:51,839
组都很小的U，那么

1486
00:55:51,839 --> 00:55:54,480
这个约束就得到满足，

1487
00:55:54,480 --> 00:55:57,180
现在Z可以表达

1488
00:55:57,180 --> 00:56:00,240
自己，这就是

1489
00:56:00,240 --> 00:56:02,880


1490
00:56:02,880 --> 00:56:06,180
激活所达到的效果 所以这是由这

1491
00:56:06,180 --> 00:56:07,020
两个

1492
00:56:07,020 --> 00:56:09,300
呃羽衣甘蓝散度项引起的，这里的

1493
00:56:09,300 --> 00:56:12,960
意思是每个 unhc 距高斯有多远

1494
00:56:12,960 --> 00:56:15,180
，然后通过

1495
00:56:15,180 --> 00:56:16,980
学生 T 变量的构造，

1496
00:56:16,980 --> 00:56:20,880
我们有效地从这些高斯构造一个稀疏先验

1497
00:56:20,880 --> 00:56:23,040
分布，

1498
00:56:23,040 --> 00:56:24,839
但在

1499
00:56:24,839 --> 00:56:27,599
实际目标呃，

1500
00:56:27,599 --> 00:56:28,920
我们正在优化的项和目标只是

1501
00:56:28,920 --> 00:56:31,619
这两个 KL 项，它们

1502
00:56:31,619 --> 00:56:34,020
在某种程度上将其推向稀疏性，并且这

1503
00:56:34,020 --> 00:56:36,359


1504
00:56:36,359 --> 00:56:39,000
通过解码器与这里的似然项自动平衡，

1505
00:56:39,000 --> 00:56:41,220
所以我们不' 没有我们正在调整的术语，

1506
00:56:41,220 --> 00:56:42,839
但我们正在学习

1507
00:56:42,839 --> 00:56:44,280
这些不同编码器的参数，然后

1508
00:56:44,280 --> 00:56:48,200
分析失败和紧急情况

1509
00:56:48,540 --> 00:56:49,920
哦

1510
00:56:49,920 --> 00:56:52,859
好吧，戴夫·道格拉斯（Dave Douglas）写的另一个问题

1511
00:56:52,859 --> 00:56:55,500


1512
00:56:55,500 --> 00:56:59,160
谈到凝视和幻觉，

1513
00:56:59,160 --> 00:57:01,920
对婴儿康斯坦斯的研究可以

1514
00:57:01,920 --> 00:57:04,260
分开吗 进入较低层次的幻觉 相对

1515
00:57:04,260 --> 00:57:06,140
也许较高层次的

1516
00:57:06,140 --> 00:57:09,980
概念恒常性

1517
00:57:13,619 --> 00:57:15,720
呃，你能读一下

1518
00:57:15,720 --> 00:57:18,300
当前的架构吗？关于

1519
00:57:18,300 --> 00:57:23,520
婴儿中的恒常性的研究是否可以

1520
00:57:23,520 --> 00:57:26,640
分开，嗯，认知恒常性是的，

1521
00:57:26,640 --> 00:57:31,619
可能我不是，我不是呃

1522
00:57:31,619 --> 00:57:33,900
专家，或者实际上什至非常熟悉

1523
00:57:33,900 --> 00:57:35,460
类似的

1524
00:57:35,460 --> 00:57:37,859
对象永久性研究、婴儿

1525
00:57:37,859 --> 00:57:40,260
和恒常性的东西，所以但我认为

1526
00:57:40,260 --> 00:57:42,300


1527
00:57:42,300 --> 00:57:44,339
在神经网络架构中研究会非常有趣，

1528
00:57:44,339 --> 00:57:46,740
这就是

1529
00:57:46,740 --> 00:57:48,780
我试图用

1530
00:57:48,780 --> 00:57:51,780
这条线在这里建模的这种呃幻觉的一些想法 我不

1531
00:57:51,780 --> 00:57:53,099
知道我是否对此非常清楚，但

1532
00:57:53,099 --> 00:57:55,380
顶行是输入，我们

1533
00:57:55,380 --> 00:57:57,359
实际上就像阻止

1534
00:57:57,359 --> 00:58:00,119
单个帧的输入，我想看看

1535
00:58:00,119 --> 00:58:03,240
网络是否对那个

1536
00:58:03,240 --> 00:58:05,400
东西仍然存在进行了编码 当该帧

1537
00:58:05,400 --> 00:58:07,680
消失时，我仍然可以

1538
00:58:07,680 --> 00:58:10,020
从神经活动中解码该对象的存在，

1539
00:58:10,020 --> 00:58:11,819
然后它还推断出运动是什么，

1540
00:58:11,819 --> 00:58:13,920
因为它

1541
00:58:13,920 --> 00:58:15,780


1542
00:58:15,780 --> 00:58:18,480
在与之前略有不同的位置看到了条形，当之后

1543
00:58:18,480 --> 00:58:20,520
框架已经消失了，

1544
00:58:20,520 --> 00:58:22,559


1545
00:58:22,559 --> 00:58:25,200
嗯，所以，是的，我认为它肯定是多个

1546
00:58:25,200 --> 00:58:27,240
级别的，

1547
00:58:27,240 --> 00:58:29,160
其中一些可能会低得多，呃，

1548
00:58:29,160 --> 00:58:33,180


1549
00:58:33,180 --> 00:58:35,880
也许长期的对象永久性，我

1550
00:58:35,880 --> 00:58:37,380
猜会明显

1551
00:58:37,380 --> 00:58:39,059
更高的水平，

1552
00:58:39,059 --> 00:58:39,900


1553
00:58:39,900 --> 00:58:41,760
这只是让我想起那些

1554
00:58:41,760 --> 00:58:44,640
用猫做的实验 回到过去，

1555
00:58:44,640 --> 00:58:47,280
他们就像在黑暗中养育它们一样，

1556
00:58:47,280 --> 00:58:49,020
除了每天一个小时，他们

1557
00:58:49,020 --> 00:58:51,000
把它们放在垂直世界或水平

1558
00:58:51,000 --> 00:58:53,160
世界中，或者它们只看到水平线

1559
00:58:53,160 --> 00:58:57,299
或垂直线，呃，你可以看到

1560
00:58:57,299 --> 00:58:59,880
它们的皮质组织发生了变化，

1561
00:58:59,880 --> 00:59:02,819
就像它们一样

1562
00:59:02,819 --> 00:59:04,200
如果他们以前从未见过水平线，那么他们的接受能力较差，那么这是水平线，然后

1563
00:59:04,200 --> 00:59:06,420
你

1564
00:59:06,420 --> 00:59:07,980
拿起一根棍子，在

1565
00:59:07,980 --> 00:59:09,599
他们面前挥舞它，如果棍子是

1566
00:59:09,599 --> 00:59:11,220
水平的，他们只是什么也不做，如果是

1567
00:59:11,220 --> 00:59:12,900
垂直的，他们会用棍子猛击它

1568
00:59:12,900 --> 00:59:14,460
他们试图击中它，就好像他们

1569
00:59:14,460 --> 00:59:15,900
实际上不必在

1570
00:59:15,900 --> 00:59:18,420
自己面前打杠一样，所以我认为在这种

1571
00:59:18,420 --> 00:59:20,700
情况下，这是

1572
00:59:20,700 --> 00:59:24,260
低水平缺陷和视力

1573
00:59:24,260 --> 00:59:26,940
导致某种幻觉的证据，

1574
00:59:26,940 --> 00:59:28,980
所以我我 我想是的，

1575
00:59:28,980 --> 00:59:30,660
在婴儿身上肯定存在某些方面，以及

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
你提出的一个非常好奇的观点

1578
00:59:36,420 --> 00:59:40,339
是有生命和无生命的

1579
00:59:40,339 --> 00:59:43,619
流形，小东西处于

1580
00:59:43,619 --> 00:59:45,480
中间，

1581
00:59:45,480 --> 00:59:49,140
这代表什么，

1582
00:59:49,140 --> 00:59:52,319
或者是因为它们是可以处理的

1583
00:59:52,319 --> 00:59:55,619
还是它 可能是一只昆虫，或者可能是一些

1584
00:59:55,619 --> 00:59:57,839
可能会随着风而消失的东西，或者

1585
00:59:57,839 --> 01:00:01,380
这意味着什么，是的，

1586
01:00:01,380 --> 01:00:04,380
呃，

1587
01:00:04,380 --> 01:00:08,280
所以这是像 Talia conkle 这样的人的工作，我

1588
01:00:08,280 --> 01:00:11,280
认为是发现这个

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
组织的人，他们试图

1591
01:00:14,880 --> 01:00:16,440
弄清楚它 我不知道，我可能会

1592
01:00:16,440 --> 01:00:19,500
弄错，所以我建议人们阅读

1593
01:00:19,500 --> 01:00:21,359
她的著作，如果他们称之为

1594
01:00:21,359 --> 01:00:23,880
三方组织，但如果我没

1595
01:00:23,880 --> 01:00:25,319
记错的话，

1596
01:00:25,319 --> 01:00:27,900
他们做了很多后续工作，解释为什么

1597
01:00:27,900 --> 01:00:30,780
会有这个组织以及

1598
01:00:30,780 --> 01:00:33,180
一些证据

1599
01:00:33,180 --> 01:00:35,700
这些物体的曲率，有点

1600
01:00:35,700 --> 01:00:37,440
像你看到物体的距离，

1601
01:00:37,440 --> 01:00:40,260
或者像

1602
01:00:40,260 --> 01:00:43,319
有生命的物体，或者可能更弯曲，

1603
01:00:43,319 --> 01:00:45,599
或者不管

1604
01:00:45,599 --> 01:00:46,859
实际答案是什么，都有很多

1605
01:00:46,859 --> 01:00:48,720
不同的假设，

1606
01:00:48,720 --> 01:00:51,720
这些假设源于这些物体的相似属性

1607
01:00:51,720 --> 01:00:54,000
也许中级或低级属性

1608
01:00:54,000 --> 01:00:56,280
比高级属性更多，我

1609
01:00:56,280 --> 01:00:57,599
仍然不知道它是否已经完全

1610
01:00:57,599 --> 01:00:59,339
解决，是否

1611
01:00:59,339 --> 01:01:01,500
像您所说的与

1612
01:01:01,500 --> 01:01:04,920
对象的交互导致分离或

1613
01:01:04,920 --> 01:01:06,119
嗯

1614
01:01:06,119 --> 01:01:09,540
或者是的这些对象的一般形状

1615
01:01:09,540 --> 01:01:12,299
我 与大多数事情一样，

1616
01:01:12,299 --> 01:01:13,980
它就像上述所有内容的某种组合，

1617
01:01:13,980 --> 01:01:16,980
但是我认为

1618
01:01:16,980 --> 01:01:18,480
从这个建模的角度来看，有趣的事情

1619
01:01:18,480 --> 01:01:19,859
是，

1620
01:01:19,859 --> 01:01:21,480
嗯，

1621
01:01:21,480 --> 01:01:24,059
这只是

1622
01:01:24,059 --> 01:01:26,819
根据图像数据集本身的相关统计数据进行训练，

1623
01:01:26,819 --> 01:01:28,799
所以这没有交互作用

1624
01:01:28,799 --> 01:01:32,760
没有生命的概念，呃，我的意思是，这

1625
01:01:32,760 --> 01:01:34,140
实际上只是在 imagenet 上训练一个模型，

1626
01:01:34,140 --> 01:01:37,859
只是狗、猫、船的图像，

1627
01:01:37,859 --> 01:01:40,020
无论如何，但它仍然实现了这种

1628
01:01:40,020 --> 01:01:41,640
类型的组织，所以有某种

1629
01:01:41,640 --> 01:01:42,540


1630
01:01:42,540 --> 01:01:44,940
可能是语义特征，

1631
01:01:44,940 --> 01:01:46,740
我们有图像，我们有一个

1632
01:01:46,740 --> 01:01:48,359
可以对

1633
01:01:48,359 --> 01:01:51,000
船、狗和 20 个其他品种

1634
01:01:51,000 --> 01:01:53,640
的狗进行分类的网络，但如果

1635
01:01:53,640 --> 01:01:55,920
它也可能

1636
01:01:55,920 --> 01:01:57,900
与较低级别的完成统计数据有一些对应，

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
那么是的，我不知道，我猜是

1639
01:02:03,960 --> 01:02:07,500
的，挑衅性的类比是

1640
01:02:07,500 --> 01:02:10,380


1641
01:02:10,380 --> 01:02:12,900
笔迹中 mnist 的平移转变

1642
01:02:12,900 --> 01:02:14,819
识别设置

1643
01:02:14,819 --> 01:02:18,000


1644
01:02:18,000 --> 01:02:20,160


1645
01:02:20,160 --> 01:02:22,740
当今存在的平移位移是什么 三像素

1646
01:02:22,740 --> 01:02:24,780
示例是对

1647
01:02:24,780 --> 01:02:27,540
llm 或某物或

1648
01:02:27,540 --> 01:02:29,099
某物的某些即时工程攻击 插入的特殊字符

1649
01:02:29,099 --> 01:02:32,640
或或嗯嗯一些

1650
01:02:32,640 --> 01:02:35,160
我们甚至无法覆盖的图像

1651
01:02:35,160 --> 01:02:37,020
检测到这一点，那么

1652
01:02:37,020 --> 01:02:39,359
你认为这些挑战

1653
01:02:39,359 --> 01:02:42,839
是什么，我们可以采取哪些方式来实现这一目标，

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
是的，绝对是，我的意思是，我认为这就是

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
我思考的方式，就像

1658
01:02:50,099 --> 01:02:52,680
这些对称变换，

1659
01:02:52,680 --> 01:02:53,760
嗯，

1660
01:02:53,760 --> 01:02:55,799
如果你正在考虑语言模型，那么

1661
01:02:55,799 --> 01:02:57,420
你 可以想象一个对称

1662
01:02:57,420 --> 01:02:58,500
变换，就像用

1663
01:02:58,500 --> 01:03:00,240
同义词或

1664
01:03:00,240 --> 01:03:03,780
其他东西替换一个单词一样，你的句子对我们来说

1665
01:03:03,780 --> 01:03:06,000
意味着完全相同的事情，但现在

1666
01:03:06,000 --> 01:03:07,380
突然模型的反应会

1667
01:03:07,380 --> 01:03:09,299
非常不同，

1668
01:03:09,299 --> 01:03:11,240


1669
01:03:11,240 --> 01:03:15,359
就像语言之间的翻译一样，这

1670
01:03:15,359 --> 01:03:16,799
可以看作是

1671
01:03:16,799 --> 01:03:19,440
它保留了我们

1672
01:03:19,440 --> 01:03:21,960
输入的基本含义，

1673
01:03:21,960 --> 01:03:24,900
但对于模型来说它看起来

1674
01:03:24,900 --> 01:03:26,220
完全不同，我们希望

1675
01:03:26,220 --> 01:03:28,380
模型能够以

1676
01:03:28,380 --> 01:03:29,940
可预测的方式针对这些

1677
01:03:29,940 --> 01:03:32,160
类型的转换进行行为，因为

1678
01:03:32,160 --> 01:03:35,040
我认为人类的行为非常可预测，

1679
01:03:35,040 --> 01:03:37,319
因为这些 当

1680
01:03:37,319 --> 01:03:39,920
我们处理人工智能系统时，我们期望

1681
01:03:39,920 --> 01:03:43,200
它们也会这样做，我认为

1682
01:03:43,200 --> 01:03:45,480
这是导致

1683
01:03:45,480 --> 01:03:47,339
与这些系统交互的许多挑战的部分原因

1684
01:03:47,339 --> 01:03:49,460
，我试图用

1685
01:03:49,460 --> 01:03:52,500
这个来做一个粗略的厚颜无耻的演示。

1686
01:03:52,500 --> 01:03:54,960
熊和方块之类的东西，

1687
01:03:54,960 --> 01:03:58,440
嗯，我们希望

1688
01:03:58,440 --> 01:04:00,480
它能够做这样简单的事情，

1689
01:04:00,480 --> 01:04:02,220
因为我们认为大多数人都

1690
01:04:02,220 --> 01:04:04,020
可以，但它不能，如果你

1691
01:04:04,020 --> 01:04:05,460
想象这是一个关键场景，

1692
01:04:05,460 --> 01:04:07,740
你期望这样，那就是一个大

1693
01:04:07,740 --> 01:04:08,700
问题

1694
01:04:08,700 --> 01:04:11,099
嗯 我们如何处理这个问题，是的，我

1695
01:04:11,099 --> 01:04:12,720
认为这就是我正在

1696
01:04:12,720 --> 01:04:15,859
寻找的东西，我认为我

1697
01:04:16,319 --> 01:04:18,420


1698
01:04:18,420 --> 01:04:22,280
正在采取的方向看起来

1699
01:04:22,280 --> 01:04:26,940
更简单，就像

1700
01:04:26,940 --> 01:04:29,460


1701
01:04:29,460 --> 01:04:31,380
神经网络架构或

1702
01:04:31,380 --> 01:04:33,180
算法的自下而上的构建块，这些构建块会

1703
01:04:33,180 --> 01:04:35,760
产生这些新兴的 结构

1704
01:04:35,760 --> 01:04:37,680
属性，我认为这是一种

1705
01:04:37,680 --> 01:04:39,839
更通用的方式，而不是在

1706
01:04:39,839 --> 01:04:41,579
我们已经拥有的东西之上构建一些东西，

1707
01:04:41,579 --> 01:04:43,140


1708
01:04:43,140 --> 01:04:43,920
嗯，

1709
01:04:43,920 --> 01:04:45,839
我认为这种东西可以

1710
01:04:45,839 --> 01:04:47,819
更好地扩展，并且也更匹配

1711
01:04:47,819 --> 01:04:50,299
大脑所做的事情，

1712
01:04:50,760 --> 01:04:52,740
非常酷，一种实现

1713
01:04:52,740 --> 01:04:54,740
问题是什么

1714
01:04:54,740 --> 01:04:57,000
运行这个程序的计算要求，或者

1715
01:04:57,000 --> 01:04:59,400
作为

1716
01:04:59,400 --> 01:05:01,920
学生或研究人员运行这些变体的日常情况，

1717
01:05:01,920 --> 01:05:04,380
比如他们是否使用 TB 级的

1718
01:05:04,380 --> 01:05:07,020
数据，而你正在使用大型计算，

1719
01:05:07,020 --> 01:05:08,940
或者这是人们可以自己运行的东西

1720
01:05:08,940 --> 01:05:11,880
笔记本电脑

1721
01:05:11,880 --> 01:05:13,980
我认为我今天介绍的几乎所有内容都

1722
01:05:13,980 --> 01:05:17,099
可以在本地运行，所以就像

1723
01:05:17,099 --> 01:05:20,040
这个东西非常简单，你可以运行我的

1724
01:05:20,040 --> 01:05:20,760
意思是，

1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
如果你想训练和尝试不同的东西，你会认为你可以在你的笔记本电脑上运行它

1727
01:05:24,299 --> 01:05:25,980


1728
01:05:25,980 --> 01:05:27,420
速度

1729
01:05:27,420 --> 01:05:30,119
很慢，所以我推荐一些

1730
01:05:30,119 --> 01:05:33,359
商业 GPU，比如我可以

1731
01:05:33,359 --> 01:05:35,640
在 Nvidia 1080 上运行几乎所有的东西，

1732
01:05:35,640 --> 01:05:38,819
相当旧，很便宜，但它们有 12

1733
01:05:38,819 --> 01:05:41,099
GB 的 RAM 或其他什么，

1734
01:05:41,099 --> 01:05:43,140
对于这些模型来说，4

1735
01:05:43,140 --> 01:05:46,440
GB 的 RAM 已经足够了 我认为

1736
01:05:46,440 --> 01:05:48,839
有些人认为奇怪的一件事是，

1737
01:05:48,839 --> 01:05:51,480
我的大部分实验都是在 mnist 之类的东西上进行的，所以

1738
01:05:51,480 --> 01:05:54,780
它是 32 x 32 像素图像，因为我可以用

1739
01:05:54,780 --> 01:05:57,299
小而有声的方式训练它，

1740
01:05:57,299 --> 01:06:00,000
嗯，如果你想这样做的话，我的实验

1741
01:06:00,000 --> 01:06:02,460
或者是无休止的，如果你 想要做

1742
01:06:02,460 --> 01:06:03,540
这样的事情，这些要

1743
01:06:03,540 --> 01:06:05,640
复杂得多，这个哈密尔顿动态

1744
01:06:05,640 --> 01:06:08,160
套件在这里你要进入

1745
01:06:08,160 --> 01:06:09,780
在多个 GPU 上运行的更大的模型

1746
01:06:09,780 --> 01:06:12,240
，所以这里使用一个集群来

1747
01:06:12,240 --> 01:06:14,220
运行这些类型的模型，

1748
01:06:14,220 --> 01:06:16,140
但我想说的是大多数

1749
01:06:16,140 --> 01:06:18,920
带有 GPU 的单台机器就足够了，

1750
01:06:18,920 --> 01:06:21,680
甚至就像在协作笔记本中一样，

1751
01:06:21,680 --> 01:06:24,539
如果你想

1752
01:06:24,539 --> 01:06:26,520
在 imagenet 上训练一些东西，它会变得更加

1753
01:06:26,520 --> 01:06:29,940
复杂，你

1754
01:06:29,940 --> 01:06:33,660
至少需要一个 GPU，理想情况下更多，但是是的，我

1755
01:06:33,660 --> 01:06:35,160
不这样做 一大堆大型的东西，

1756
01:06:35,160 --> 01:06:37,200
但我认为这确实很有趣，

1757
01:06:37,200 --> 01:06:39,960
而且肯定还有更多你

1758
01:06:39,960 --> 01:06:42,539
可以做的事情，但是对于其中一些

1759
01:06:42,539 --> 01:06:45,480
更简单或更基本的问题，

1760
01:06:45,480 --> 01:06:47,760
我不知道你想称呼它什么，嗯，

1761
01:06:47,760 --> 01:06:52,500
一台较小的机器是 又好又快，太

1762
01:06:52,500 --> 01:06:54,660
酷了，有用，

1763
01:06:54,660 --> 01:06:58,140
好吧，我会读戴夫的评论，

1764
01:06:58,140 --> 01:07:00,780
回忆起伯特·德弗里斯在

1765
01:07:00,780 --> 01:07:02,880
应用主动推理研讨会上的评论，

1766
01:07:02,880 --> 01:07:05,520
关于在我们不需要太多精确度的

1767
01:07:05,520 --> 01:07:08,099
觅食或控制情况上花费更少的努力或 ATP 的可取性

1768
01:07:08,099 --> 01:07:09,780


1769
01:07:09,780 --> 01:07:11,579
我不知道 不知道你是否听过

1770
01:07:11,579 --> 01:07:13,859
这个，但 DeVries 教授提到

1771
01:07:13,859 --> 01:07:17,520
了可变精度模型，以及如何

1772
01:07:17,520 --> 01:07:19,440
使用它们来

1773
01:07:19,440 --> 01:07:21,059
实现

1774
01:07:21,059 --> 01:07:23,039
泛化和实际结构

1775
01:07:23,039 --> 01:07:25,020
课程训练的不同功能，以及减少

1776
01:07:25,020 --> 01:07:27,059
计算要求，

1777
01:07:27,059 --> 01:07:29,880
他对如何引入这个模型有什么建议吗？

1778
01:07:29,880 --> 01:07:31,980
与主动

1779
01:07:31,980 --> 01:07:33,900
推理理论的区别 什么样的

1780
01:07:33,900 --> 01:07:37,760
实验可以解决这个问题

1781
01:07:38,400 --> 01:07:40,680
哦，哇，是的，我不知道，我

1782
01:07:40,680 --> 01:07:42,660
不认为我有太多智慧

1783
01:07:42,660 --> 01:07:46,619
可以说，这是完全诚实的，

1784
01:07:46,619 --> 01:07:48,740
嗯，

1785
01:07:51,359 --> 01:07:53,460
这是一个非常有趣的问题，因为

1786
01:07:53,460 --> 01:07:56,220
我认为直觉使 对

1787
01:07:56,220 --> 01:07:58,260
我来说很有意义，呃，

1788
01:07:58,260 --> 01:08:00,000
你说的是，

1789
01:08:00,000 --> 01:08:01,980


1790
01:08:01,980 --> 01:08:05,160
当你在模型中进行编码时，如果我正确理解可变精度率，通常

1791
01:08:05,160 --> 01:08:07,079
会进行

1792
01:08:07,079 --> 01:08:07,740
计算

1793
01:08:07,740 --> 01:08:09,420
[音乐]，

1794
01:08:09,420 --> 01:08:11,539
嗯，

1795
01:08:13,200 --> 01:08:17,040
这会以某种方式对你

1796
01:08:17,040 --> 01:08:19,080
未来的表现产生影响，因为 与某些

1797
01:08:19,080 --> 01:08:22,259
能量存储的关系，我认为是的，如果你

1798
01:08:22,259 --> 01:08:23,580
想将其构建到一个积极

1799
01:08:23,580 --> 01:08:26,279
努力的系统中，你需要有

1800
01:08:26,279 --> 01:08:28,679
一个真正的具体系统，其中

1801
01:08:28,679 --> 01:08:31,500
代理有一些能量的概念，比如

1802
01:08:31,500 --> 01:08:34,439
内部能量存储，

1803
01:08:34,439 --> 01:08:36,238
是的，一些正在尝试的东西

1804
01:08:36,238 --> 01:08:38,520
为了在执行其动作时保持能量，

1805
01:08:38,520 --> 01:08:40,500
呃，

1806
01:08:40,500 --> 01:08:42,359
耗尽能量，

1807
01:08:42,359 --> 01:08:44,759
需要一些对代理不利的东西，

1808
01:08:44,759 --> 01:08:47,399
然后也许你可以观察到一种

1809
01:08:47,399 --> 01:08:48,679
出现，

1810
01:08:48,679 --> 01:08:52,040
呃，减少和

1811
01:08:52,040 --> 01:08:55,198
编码精度或类似的东西，

1812
01:08:55,198 --> 01:08:57,479
因为代理正在尝试

1813
01:08:57,479 --> 01:09:00,060
学习 更有效地行动，你可能必须

1814
01:09:00,060 --> 01:09:02,520
赋予它控制精度的能力，

1815
01:09:02,520 --> 01:09:04,020


1816
01:09:04,020 --> 01:09:07,080
是的，就像我根据我的专业知识所说的那样，但

1817
01:09:07,080 --> 01:09:08,819


1818
01:09:08,819 --> 01:09:11,460
这张幻灯片上的想法还可以，就在这里，第一个非常

1819
01:09:11,460 --> 01:09:14,219
酷的图像，它有点像

1820
01:09:14,219 --> 01:09:18,359
数字杰克逊·波洛克（Jackson Pollock），

1821
01:09:18,359 --> 01:09:22,920
嗯，如果它是的话 更简单的输入数据

1822
01:09:22,920 --> 01:09:26,520
大小或只是降低模式的复杂性，

1823
01:09:26,520 --> 01:09:27,719
或者如果

1824
01:09:27,719 --> 01:09:30,000
复杂性增加，该图像看起来会有何

1825
01:09:30,000 --> 01:09:31,920
不同是的，

1826
01:09:31,920 --> 01:09:34,620
所以我做了一些实验试图

1827
01:09:34,620 --> 01:09:36,738
改变这些

1828
01:09:36,738 --> 01:09:40,439
方向列，嗯，

1829
01:09:40,439 --> 01:09:41,100


1830
01:09:41,100 --> 01:09:43,140
你可以是的，基本上改变

1831
01:09:43,140 --> 01:09:44,520
模型的参数，你可以 让

1832
01:09:44,520 --> 01:09:47,100
这些列变得更大，你可以让

1833
01:09:47,100 --> 01:09:49,380
它们的结构

1834
01:09:49,380 --> 01:09:51,660
与我们在人类中

1835
01:09:51,660 --> 01:09:53,040
看到的结构非常相似，你可以让它们有更多的

1836
01:09:53,040 --> 01:09:55,199
活动带，

1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
嗯，也像你说的那样，这取决于

1839
01:09:58,199 --> 01:10:00,540
数据集 如果我使用

1840
01:10:00,540 --> 01:10:03,179
非常简单的正弦曲线

1841
01:10:03,179 --> 01:10:05,580
作为输入，你正在使用的东西我得到这样的东西我得到的

1842
01:10:05,580 --> 01:10:07,920
东西有点多一点呃

1843
01:10:07,920 --> 01:10:11,640
旋转曲线更高的熵

1844
01:10:11,640 --> 01:10:12,800
嗯

1845
01:10:12,800 --> 01:10:15,660
所以我认为

1846
01:10:15,660 --> 01:10:18,000
如果你想研究这些的

1847
01:10:18,000 --> 01:10:19,320
出现，这些都是有趣的事情 自然系统中的这种类型的组织

1848
01:10:19,320 --> 01:10:22,199
呃，如果你有一个

1849
01:10:22,199 --> 01:10:24,120
模型，现在可以针对不同的设置产生不同的

1850
01:10:24,120 --> 01:10:25,860
组织，

1851
01:10:25,860 --> 01:10:28,620
那么看看什么

1852
01:10:28,620 --> 01:10:31,679
设置最符合我们观察到的数据，

1853
01:10:31,679 --> 01:10:32,340
嗯，

1854
01:10:32,340 --> 01:10:34,679
所以，是的，我可以，

1855
01:10:34,679 --> 01:10:36,540
如果你是的话，我可以发送它周围的那些

1856
01:10:36,540 --> 01:10:38,520
感兴趣，但是

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
嗯，是的，我想还有另一个抱歉，

1859
01:10:44,219 --> 01:10:45,659
另一个有趣的点是

1860
01:10:45,659 --> 01:10:46,920
，

1861
01:10:46,920 --> 01:10:51,480
呃不同的动物和呃

1862
01:10:51,480 --> 01:10:53,219
方向选择性的类型以及不同

1863
01:10:53,219 --> 01:10:54,659
数量的风车，有些动物

1864
01:10:54,659 --> 01:10:57,420
根本没有它，我想如果我是老鼠的话

1865
01:10:57,420 --> 01:11:00,120
正确，有这种呃，他们称之为

1866
01:11:00,120 --> 01:11:01,800
盐和胡椒选择性，所以它

1867
01:11:01,800 --> 01:11:03,480
基本上是随机的，你没有任何

1868
01:11:03,480 --> 01:11:04,679
类似地形方向

1869
01:11:04,679 --> 01:11:06,239
敏感性的东西，

1870
01:11:06,239 --> 01:11:09,300
嗯，所以有证据表明，

1871
01:11:09,300 --> 01:11:10,920
是的，不同的系统以

1872
01:11:10,920 --> 01:11:13,020
不同的方式做这件事，弄清楚

1873
01:11:13,020 --> 01:11:14,760
为什么是

1874
01:11:14,760 --> 01:11:17,760
的，这很有趣 非常酷，它首先让我想起

1875
01:11:17,760 --> 01:11:21,300
反应扩散

1876
01:11:21,300 --> 01:11:22,980
基础和时间，

1877
01:11:22,980 --> 01:11:25,739
所以实际上，

1878
01:11:25,739 --> 01:11:30,000
一个区域可能

1879
01:11:30,000 --> 01:11:32,840
在给定的粒度上没有活动，

1880
01:11:32,840 --> 01:11:35,640
就像在

1881
01:11:35,640 --> 01:11:39,360
fmri 空间和时间时间

1882
01:11:39,360 --> 01:11:40,520
尺度上观察它一样，

1883
01:11:40,520 --> 01:11:44,699
如果活动的区域但是 如果

1884
01:11:44,699 --> 01:11:46,380
活动的范围

1885
01:11:46,380 --> 01:11:48,480


1886
01:11:48,480 --> 01:11:52,260
比测量的速度慢，那么与

1887
01:11:52,260 --> 01:11:54,060
噪音没有什么不同，那么所有的数据都会被

1888
01:11:54,060 --> 01:11:55,739
平均，

1889
01:11:55,739 --> 01:11:58,620
所以可能会有一些

1890
01:11:58,620 --> 01:12:01,560
有趣的数据集，比如

1891
01:12:01,560 --> 01:12:03,360
实际上有很多

1892
01:12:03,360 --> 01:12:06,179
丰富性的数据集，但对于一个 出于某种原因或

1893
01:12:06,179 --> 01:12:08,520
其他原因，它只是被平均掉了，

1894
01:12:08,520 --> 01:12:11,100
因为它没有连接到你

1895
01:12:11,100 --> 01:12:12,420
或类似的东西，你真的

1896
01:12:12,420 --> 01:12:14,520
需要使用单个试验级别，你

1897
01:12:14,520 --> 01:12:16,140
需要有足够高的空间分辨率，

1898
01:12:16,140 --> 01:12:18,719
这样你就知道它满足

1899
01:12:18,719 --> 01:12:23,100
微频率呃 这只是

1900
01:12:23,100 --> 01:12:24,659
人们很长一段时间没有做的事情，

1901
01:12:24,659 --> 01:12:25,620
特别是如果你正在做

1902
01:12:25,620 --> 01:12:27,780
单个选民的录音，你不会

1903
01:12:27,780 --> 01:12:28,920
看到行波，你

1904
01:12:28,920 --> 01:12:30,900
会看到振荡，

1905
01:12:30,900 --> 01:12:32,219
所以你需要多电

1906
01:12:32,219 --> 01:12:34,199
数组，基本上他们说好吧，

1907
01:12:34,199 --> 01:12:36,000
是的，现在我们有技术可以

1908
01:12:36,000 --> 01:12:37,520
做到这一点，

1909
01:12:37,520 --> 01:12:40,080
我们以前没有看到过，

1910
01:12:40,080 --> 01:12:42,960
这可能是对

1911
01:12:42,960 --> 01:12:44,400
我们之前看到的很多噪音的解释，

1912
01:12:44,400 --> 01:12:46,260
也许它真的只是 行

1913
01:12:46,260 --> 01:12:47,219
波

1914
01:12:47,219 --> 01:12:47,760
嗯，

1915
01:12:47,760 --> 01:12:51,480
所以是的，我认为未来还有很多工作要做，

1916
01:12:51,480 --> 01:12:53,880
随着

1917
01:12:53,880 --> 01:12:56,520
录音能力的增强，

1918
01:12:56,520 --> 01:12:58,739
这非常酷，

1919
01:12:58,739 --> 01:13:02,940
任何最终的想法或问题，或者

1920
01:13:02,940 --> 01:13:06,239
你要在哪里进行这项工作是的，

1921
01:13:06,239 --> 01:13:08,520
不，谢谢你让我

1922
01:13:08,520 --> 01:13:10,140
嗯

1923
01:13:10,140 --> 01:13:11,640
希望在活跃的基础设施中

1924
01:13:11,640 --> 01:13:14,520
这就是我很乐意的，我认为

1925
01:13:14,520 --> 01:13:16,560
那会非常有趣，所以是的，我

1926
01:13:16,560 --> 01:13:18,980
不太确定我正在看也许音乐

1927
01:13:18,980 --> 01:13:21,659
呃现在

1928
01:13:21,659 --> 01:13:22,440
嗯

1929
01:13:22,440 --> 01:13:26,760
看呃

1930
01:13:26,760 --> 01:13:30,420
其他疯狂的方向我

1931
01:13:30,420 --> 01:13:33,020
不想听起来太疯狂呃

1932
01:13:33,020 --> 01:13:36,900
但是 我会写下来，是的，很多事情，所以

1933
01:13:36,900 --> 01:13:38,580


1934
01:13:38,580 --> 01:13:40,320
我们提交给神经学家的一件事是

1935
01:13:40,320 --> 01:13:43,140
用行波研究记忆，

1936
01:13:43,140 --> 01:13:45,060
嗯，所以这篇论文

1937
01:13:45,060 --> 01:13:46,860
今天刚刚存档，嗯，

1938
01:13:46,860 --> 01:13:48,840
波如何真正擅长编码

1939
01:13:48,840 --> 01:13:50,580
长期记忆 我认为这

1940
01:13:50,580 --> 01:13:52,100
非常有趣，

1941
01:13:52,100 --> 01:13:54,120
所以我可能会朝那个方向走一点，

1942
01:13:54,120 --> 01:13:55,800


1943
01:13:55,800 --> 01:13:58,920
听起来不错，是的，

1944
01:13:58,920 --> 01:14:01,400


1945
01:14:01,400 --> 01:14:04,560
当神经元保持

1946
01:14:04,560 --> 01:14:07,620
活跃时，即使狗的脚在

1947
01:14:07,620 --> 01:14:09,060
移动，也能看到行动发挥作用，

1948
01:14:09,060 --> 01:14:11,480
这会非常令人兴奋，有很多类似的东西 动作序列，

1949
01:14:11,480 --> 01:14:14,280
例如扔棒球，然后它就

1950
01:14:14,280 --> 01:14:15,900
消失了，就像

1951
01:14:15,900 --> 01:14:18,440
该动作中的某些东西正在继续

1952
01:14:18,440 --> 01:14:21,179
影响一样，因此具有

1953
01:14:21,179 --> 01:14:23,699
替代动作的深层时间表示

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
，然后变分自动编码器基本上已经是

1956
01:14:29,640 --> 01:14:33,179
正确的事情了，

1957
01:14:33,179 --> 01:14:35,219
所以

1958
01:14:35,219 --> 01:14:37,739
真的很欣赏它 好的，谢谢，

1959
01:14:37,739 --> 01:14:39,480
下次再见，

1960
01:14:39,480 --> 01:14:43,339
非常感谢，再见


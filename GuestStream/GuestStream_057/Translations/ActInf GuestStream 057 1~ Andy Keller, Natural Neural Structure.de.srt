1
00:00:06,600 --> 00:00:09,420
Hallo und herzlich willkommen, es ist der 18. September

2
00:00:09,420 --> 00:00:14,040
2023 und der aktive Gaststream 57.1

3
00:00:14,040 --> 00:00:16,740
mit Andy Keller. Wir werden

4
00:00:16,740 --> 00:00:19,619
über natürliche neuronale Strukturen

5
00:00:19,619 --> 00:00:22,020
für künstliche Intelligenz sprechen. Es wird

6
00:00:22,020 --> 00:00:24,300
eine Präsentation mit anschließender

7
00:00:24,300 --> 00:00:25,800
Diskussion geben. Wenn Sie also live zuschauen, können Sie dies

8
00:00:25,800 --> 00:00:27,900
gerne tun  Schreiben Sie Fragen im

9
00:00:27,900 --> 00:00:30,660
Live-Chat. Ansonsten danke Andy, ich

10
00:00:30,660 --> 00:00:32,279
freue mich wirklich darauf

11
00:00:32,279 --> 00:00:35,899
und Ihnen für die Präsentation

12
00:00:36,360 --> 00:00:38,940


13
00:00:38,940 --> 00:00:41,280


14
00:00:41,280 --> 00:00:42,840


15
00:00:42,840 --> 00:00:45,239
Ich bin ein Fan und sehr

16
00:00:45,239 --> 00:00:49,200
interessiert, also hoffentlich, äh ja,

17
00:00:49,200 --> 00:00:50,399
eine gute Diskussion zu führen und zu sehen, was

18
00:00:50,399 --> 00:00:51,960
ihr darüber denkt,

19
00:00:51,960 --> 00:00:54,840
ähm, mein Name ist Andy, ich beende

20
00:00:54,840 --> 00:00:57,180
meine Doktorarbeit unter der Betreuung von Maxwelling an der

21
00:00:57,180 --> 00:00:59,100
Universität Amsterdam

22
00:00:59,100 --> 00:01:01,500
ähm, das bin ich  Danach fange ich als Postdoc an der Harvard University an.

23
00:01:01,500 --> 00:01:05,099
Ich rede nur

24
00:01:05,099 --> 00:01:07,619
davon, dass das Ziel meiner Arbeit im Allgemeinen darin besteht, die

25
00:01:07,619 --> 00:01:09,540
moderne künstliche

26
00:01:09,540 --> 00:01:12,540
Intelligenz näher an eine menschenähnlichere

27
00:01:12,540 --> 00:01:15,240
Verallgemeinerung heranzuführen, und was wir damit meinen,

28
00:01:15,240 --> 00:01:17,159
ist vielleicht einiges  Eine Art Strukturverallgemeinerung,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
ähm, oder vielleicht eher vertraut für das

31
00:01:20,700 --> 00:01:22,080
Komitee für aktive Säuglinge, wie ein strukturiertes

32
00:01:22,080 --> 00:01:24,299
Weltmodell, von dem wir glauben, dass es Menschen

33
00:01:24,299 --> 00:01:26,340
haben, und die Art und Weise, wie wir dies erreichen möchten,

34
00:01:26,340 --> 00:01:28,799
ist die Integration natürlicher neuronaler

35
00:01:28,799 --> 00:01:32,400
Strukturen in künstliche Intelligenz.

36
00:01:32,400 --> 00:01:34,979
Lassen Sie uns also zunächst definieren, was wir meinen  Durch

37
00:01:34,979 --> 00:01:36,720
Strukturverallgemeinerung halte

38
00:01:36,720 --> 00:01:38,400
ich es daher für ziemlich unumstritten

39
00:01:38,400 --> 00:01:40,380
zu sagen, dass modernes maschinelles Lernen

40
00:01:40,380 --> 00:01:42,900
über seinen Trainingssatz hinaus im

41
00:01:42,900 --> 00:01:45,000
traditionellen Sinne verallgemeinert, sodass beispielsweise

42
00:01:45,000 --> 00:01:46,799
selbst die frühesten

43
00:01:46,799 --> 00:01:49,020
mehrschichtigen Perzeptrone künstlicher neuronaler Netze

44
00:01:49,020 --> 00:01:51,659
auf Datensätzen von Bildern wie

45
00:01:51,659 --> 00:01:55,140
diesem trainiert werden könnten und hohe Ergebnisse erzielen  Wenn

46
00:01:55,140 --> 00:01:56,880
ihnen dann ein

47
00:01:56,880 --> 00:01:58,320
Testsatz von Bildern vorgelegt wird, die sie noch nie

48
00:01:58,320 --> 00:02:00,240
zuvor gesehen haben, können sie sie immer noch

49
00:02:00,240 --> 00:02:02,460
relativ leicht mit dem gleichen Grad an

50
00:02:02,460 --> 00:02:04,500
Genauigkeit klassifizieren, und das ist es, was wir normalerweise als

51
00:02:04,500 --> 00:02:07,320
Generalisierung bezeichnen, auch wenn das schon relativ

52
00:02:07,320 --> 00:02:08,758
früh der Fall war  Ich habe festgestellt, dass diese

53
00:02:08,758 --> 00:02:10,800
Systeme wirklich mit kleinen

54
00:02:10,800 --> 00:02:12,720
Verschiebungen oder Verformungen zu kämpfen haben, die sich beispielsweise auf die

55
00:02:12,720 --> 00:02:16,340
Bilder beziehen. Wenn

56
00:02:18,920 --> 00:02:23,099
ja, denken Sie, warum ist das überraschend?

57
00:02:23,099 --> 00:02:24,720
Ich behaupte, dass dieses Beispiel aufgrund unserer angeborenen

58
00:02:24,720 --> 00:02:26,640
Fähigkeit, diese Art der Strukturverallgemeinerung durchzuführen,

59
00:02:26,640 --> 00:02:28,680


60
00:02:28,680 --> 00:02:31,920
ein Misserfolg ist  Ähm, zum

61
00:02:31,920 --> 00:02:33,239
Beispiel ist diese Verschiebung

62
00:02:33,239 --> 00:02:35,340
für uns fast nicht wahrnehmbar und wir handhaben sie

63
00:02:35,340 --> 00:02:37,560
automatisch, während sie im System

64
00:02:37,560 --> 00:02:39,959
ganz klar ein großes Problem darstellt. In Worten

65
00:02:39,959 --> 00:02:41,940
können wir also sagen, dass die Strukturgeneralisierung

66
00:02:41,940 --> 00:02:44,819
eine Verallgemeinerung auf einige

67
00:02:44,819 --> 00:02:47,040
Symmetrietransformationen der Eingabe oder in diesem

68
00:02:47,040 --> 00:02:48,959
Fall der Symmetrie ist  Die Transformation ist eine

69
00:02:48,959 --> 00:02:50,580
kleine Verschiebung, die die Ziffernklasse unverändert lässt.

70
00:02:50,580 --> 00:02:52,019


71
00:02:52,019 --> 00:02:54,599
Die offensichtliche Frage ist also, was

72
00:02:54,599 --> 00:02:56,340
genau wir mit dieser natürlichen

73
00:02:56,340 --> 00:02:58,860
Struktur meinen und warum wir glauben, dass

74
00:02:58,860 --> 00:03:01,620
uns dies bei diesen Einstellungen helfen würde.

75
00:03:01,620 --> 00:03:03,780
Lassen Sie uns also zunächst darüber sprechen, was wir

76
00:03:03,780 --> 00:03:05,819
unter natürlicher neuronaler Struktur verstehen  Struktur

77
00:03:05,819 --> 00:03:08,700
Um eine Möglichkeit, über Struktur oder

78
00:03:08,700 --> 00:03:11,640
jede Art von Voreingenommenheit in einem System zu sprechen, ist eine

79
00:03:11,640 --> 00:03:14,040
induktive Voreingenommenheit, und daher kann eine induktive Voreingenommenheit

80
00:03:14,040 --> 00:03:16,080
lose als eine entsprechende

81
00:03:16,080 --> 00:03:17,940
Einschränkung einer Reihe realisierbarer

82
00:03:17,940 --> 00:03:19,440
Hypothesen definiert werden, wenn Sie

83
00:03:19,440 --> 00:03:22,440
umgangssprachlich eine Modellauswahl durchführen  Man kann

84
00:03:22,440 --> 00:03:24,480
das so nennen: Bevor man

85
00:03:24,480 --> 00:03:27,060
Daten sieht, ist es eine Einschränkung dessen, was und wie

86
00:03:27,060 --> 00:03:29,580
man lernen kann. Im weitesten Sinne kann dies

87
00:03:29,580 --> 00:03:32,519
alles umfassen, von Modellklassen über

88
00:03:32,519 --> 00:03:34,739
Optimierungsverfahren bis hin zu

89
00:03:34,739 --> 00:03:37,379
Hyperparametern, und in gewissem Sinne definieren sie wirklich,

90
00:03:37,379 --> 00:03:39,739
was

91
00:03:39,739 --> 00:03:43,140
gelernt werden kann  und es definiert die

92
00:03:43,140 --> 00:03:44,819
Verallgemeinerung dahingehend, dass

93
00:03:44,819 --> 00:03:47,220
man tatsächlich nicht über einen Trainingssatz hinaus verallgemeinern kann,

94
00:03:47,220 --> 00:03:48,420
ohne eine

95
00:03:48,420 --> 00:03:50,220
induktive Lösung zu haben. Dies wird

96
00:03:50,220 --> 00:03:52,560
in diesem Artikel von David

97
00:03:52,560 --> 00:03:55,500
Wolford ausführlicher erklärt. Was wir also unter natürlichen

98
00:03:55,500 --> 00:03:58,620
induktiven Verzerrungen verstehen, sind Verzerrungen, die

99
00:03:58,620 --> 00:04:00,000
aus den Einschränkungen und

100
00:04:00,000 --> 00:04:02,040
Begrenzungen resultieren  mit denen natürliche

101
00:04:02,040 --> 00:04:04,500
Systeme konfrontiert sind, äh, durch die Natur,

102
00:04:04,500 --> 00:04:06,900
in der realen Welt leben zu müssen, zum Beispiel

103
00:04:06,900 --> 00:04:08,459
hat das Gehirn aufgrund seiner Konstruktion viele Effizienzbeschränkungen

104
00:04:08,459 --> 00:04:10,439
und physikalische Beschränkungen,

105
00:04:10,439 --> 00:04:13,140
äh und dieser

106
00:04:13,140 --> 00:04:14,580
Logik folgend, dann spielen diese Beschränkungen wirklich

107
00:04:14,580 --> 00:04:16,620
eine Rolle in unserer Verallgemeinerung

108
00:04:16,620 --> 00:04:19,798
Fähigkeiten, die derzeit die moderne

109
00:04:19,798 --> 00:04:21,478
künstliche Intelligenz übertreffen, worauf wir als

110
00:04:21,478 --> 00:04:24,720
Nächstes eingehen werden. Daher werde ich mich in diesem Vortrag

111
00:04:24,720 --> 00:04:27,540
speziell auf zwei Arten von Strukturen konzentrieren,

112
00:04:27,540 --> 00:04:29,880
die ich in meiner Arbeit untersucht habe:

113
00:04:29,880 --> 00:04:31,699
topografische Organisation und

114
00:04:31,699 --> 00:04:34,320
raumzeitliche Dynamik. Bevor ich

115
00:04:34,320 --> 00:04:36,120
auf meine Arbeit eingehe, werde ich  Ich werde ein kurzes Beispiel

116
00:04:36,120 --> 00:04:38,759
dafür geben, warum ich glaube, dass natürliche Strukturen

117
00:04:38,759 --> 00:04:41,400
nützlich sein können, um die Strukturverallgemeinerung zu erreichen, über

118
00:04:41,400 --> 00:04:42,780
die ich zuvor gesprochen habe.

119
00:04:42,780 --> 00:04:44,280


120
00:04:44,280 --> 00:04:47,479
Das erste Beispiel stammt aus

121
00:04:47,479 --> 00:04:49,199
Hukushimas neokognitiver

122
00:04:49,199 --> 00:04:51,479
Fassadenarchitektur aus den 1980er Jahren, die

123
00:04:51,479 --> 00:04:53,520
eigentlich für eine

124
00:04:53,520 --> 00:04:55,440
direkte Adressierung gebaut wurde  Das Problem der

125
00:04:55,440 --> 00:04:57,300
Robustheit gegenüber diesen kleinen Verschiebungen und

126
00:04:57,300 --> 00:04:59,759
Verformungen, also schreibt er in den Unterlagen

127
00:04:59,759 --> 00:05:02,040
über die Inspiration durch Schüler- und

128
00:05:02,040 --> 00:05:04,380
Weasel-Messungen von Hierarchie und

129
00:05:04,380 --> 00:05:06,780
Pooling, um Robustheit

130
00:05:06,780 --> 00:05:08,699
gegenüber diesen Verzerrungen zu erreichen, und wenn Sie sich also

131
00:05:08,699 --> 00:05:11,160
die Abbildung ansehen, schreibt er U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 und diese stehen für einfache und

133
00:05:14,100 --> 00:05:16,919
komplexe Zellen und daher war dies damals ein ziemlich

134
00:05:16,919 --> 00:05:18,840
radikaler Ansatz, aber er

135
00:05:18,840 --> 00:05:20,699
diente wirklich dazu, die Robustheit und die

136
00:05:20,699 --> 00:05:22,199
Verschiebungen zu verbessern, mit denen wir diese frühen

137
00:05:22,199 --> 00:05:24,419
künstlichen neuronalen Netze zu kämpfen hatten, und im Laufe der Zeit

138
00:05:24,419 --> 00:05:25,860
wurden diese Ideen vereinfacht und

139
00:05:25,860 --> 00:05:28,919
abstrahiert und

140
00:05:28,919 --> 00:05:30,539
Wie wir heute wissen, haben sie offensichtlich die Faltungs-Neuronalen Netze hervorgebracht,

141
00:05:30,539 --> 00:05:32,759
die letztendlich den Erfolg

142
00:05:32,759 --> 00:05:35,580
der Deep-Learning-Revolution vorangetrieben haben. Dies

143
00:05:35,580 --> 00:05:36,960
ist also wirklich ein Beispiel für eine natürliche

144
00:05:36,960 --> 00:05:39,479
induktive Verzerrung, die eine Strukturverallgemeinerung erreichte.

145
00:05:39,479 --> 00:05:42,120
Daher ist es für unsere Forschung

146
00:05:42,120 --> 00:05:43,919
wirklich von größtem Interesse zu versuchen, zu

147
00:05:43,919 --> 00:05:45,900
verstehen, was diese ausmacht  Modelle funktionieren

148
00:05:45,900 --> 00:05:47,280
so gut,

149
00:05:47,280 --> 00:05:49,320
ähm, und prüfen Sie, ob dieses Prinzip

150
00:05:49,320 --> 00:05:51,360
möglicherweise verallgemeinert werden kann, um

151
00:05:51,360 --> 00:05:53,940
abstraktere, abstraktere Transformationen

152
00:05:53,940 --> 00:05:57,000
und Symmetrien abzudecken.

153
00:05:57,000 --> 00:06:00,360
Was macht also eine Faltung dazu, diese

154
00:06:00,360 --> 00:06:02,060
Strukturverallgemeinerung

155
00:06:02,060 --> 00:06:04,440
intuitiv zu erreichen? Sie können sehen, dass dies durch

156
00:06:04,440 --> 00:06:06,720
Anwenden desselben Filters bei oder oder

157
00:06:06,720 --> 00:06:08,699
Merkmalsextraktor bei erfolgt

158
00:06:08,699 --> 00:06:10,680
Hier sehen wir also, wie ein einziger

159
00:06:10,680 --> 00:06:12,660
Faltungsfilter an

160
00:06:12,660 --> 00:06:14,820
allen Stellen eines Bildes angewendet wird. Das bedeutet,

161
00:06:14,820 --> 00:06:16,259
dass Sie unabhängig davon, wo sich Ihre Eingabe befindet, ob sie sich

162
00:06:16,259 --> 00:06:18,000
in der Mitte

163
00:06:18,000 --> 00:06:20,400
des Bildes oder auf der rechten Seite befindet,

164
00:06:20,400 --> 00:06:22,080
genau die gleichen Merkmale haben  Mit einer

165
00:06:22,080 --> 00:06:23,580
Ausnahme werden sie äquivalent verschoben,

166
00:06:23,580 --> 00:06:24,660


167
00:06:24,660 --> 00:06:26,819
sodass diese Art der Zuordnung mathematisch

168
00:06:26,819 --> 00:06:29,160
als Homomorphismus bezeichnet wird. Sie behält

169
00:06:29,160 --> 00:06:30,840
die algebraische Struktur des

170
00:06:30,840 --> 00:06:33,180
Eingaberaums und des Ausgaberaums bei, in diesem Fall

171
00:06:33,180 --> 00:06:35,580
in Bezug auf die Übersetzung und auf

172
00:06:35,580 --> 00:06:37,440
einer einfachen Ebene so etwas wie

173
00:06:37,440 --> 00:06:38,880
Für

174
00:06:38,880 --> 00:06:40,259
den Rest dieses Vortrags wird es wichtig sein, sich daran zu erinnern, dass wir

175
00:06:40,259 --> 00:06:42,300
Homomorphismen unseres Merkmalsextraktors überprüfen können,

176
00:06:42,300 --> 00:06:45,000
wenn wir sehen können, dass es

177
00:06:45,000 --> 00:06:46,740
diese Community-Kommutierung mit dem

178
00:06:46,740 --> 00:06:49,560
kommutativen Diagramm Transformationen gibt, und

179
00:06:49,560 --> 00:06:51,720
dass wir dies auch algebraisch schreiben können,

180
00:06:51,720 --> 00:06:53,759
indem wir zeigen, dass der Merkmalsextraktor vorhanden ist  f

181
00:06:53,759 --> 00:06:55,080
pendelt mit dem

182
00:06:55,080 --> 00:06:57,000
Transformationsoperator t

183
00:06:57,000 --> 00:06:58,919
und im Grunde wollen wir, dass es

184
00:06:58,919 --> 00:07:00,600
keinen Unterschied zwischen dem ersten

185
00:07:00,600 --> 00:07:02,639
Extrahieren der Features und dem anschließenden

186
00:07:02,639 --> 00:07:04,740
Durchführen der Transformation oder dem

187
00:07:04,740 --> 00:07:06,240
Durchführen der Transformation und dem anschließenden

188
00:07:06,240 --> 00:07:08,639
Extrahieren der Features gibt. Die Herausforderung

189
00:07:08,639 --> 00:07:10,199
besteht also bisher darin, dass

190
00:07:10,199 --> 00:07:11,880
wir es nicht wirklich wissen  Wie man

191
00:07:11,880 --> 00:07:13,620
Homomorphismen in Bezug auf

192
00:07:13,620 --> 00:07:15,240
komplexere Transformationen konstruiert, die wir in

193
00:07:15,240 --> 00:07:18,060
der realen Welt sehen, zum Beispiel ist unser Gehirn

194
00:07:18,060 --> 00:07:20,099
in der Lage, auf natürliche Weise mit Veränderungen in der Beleuchtung und der Jahreszeit umzugehen. Ähm,

195
00:07:20,099 --> 00:07:22,259


196
00:07:22,259 --> 00:07:24,599
hier sehen wir also die Beleuchtung im Gesicht einer Person

197
00:07:24,599 --> 00:07:26,160
oder den Wechsel der Jahreszeiten, den wir

198
00:07:26,160 --> 00:07:27,840
erkennen können  das gleiche Gesicht oder die gleiche

199
00:07:27,840 --> 00:07:29,880
Straße, aber wir wissen nicht, wie wir

200
00:07:29,880 --> 00:07:31,319
Modelle erstellen können, die diese

201
00:07:31,319 --> 00:07:33,180
Transformationen berücksichtigen, und daher ist es für uns schwierig,

202
00:07:33,180 --> 00:07:35,520
Systeme zu erstellen, die sie auf

203
00:07:35,520 --> 00:07:37,620
robuste und vorhersehbare Weise handhaben,

204
00:07:37,620 --> 00:07:40,259
um ein noch abstrakteres Beispiel dafür zu geben,

205
00:07:40,259 --> 00:07:41,759
was ich  Damit meine ich die möglichen

206
00:07:41,759 --> 00:07:43,620
negativen Auswirkungen von Modellen, die

207
00:07:43,620 --> 00:07:45,440
keine Symmetrie berücksichtigen. Transformationen

208
00:07:45,440 --> 00:07:48,060
berücksichtigen moderne Programme zur Text-zu-Bild-Generierung.

209
00:07:48,060 --> 00:07:50,520
In diesem Beispiel habe ich

210
00:07:50,520 --> 00:07:53,940
Dolly gebeten, ein Bild eines

211
00:07:53,940 --> 00:07:55,620
Teddybären auf dem Mond zu erstellen, und das gelingt ihr

212
00:07:55,620 --> 00:07:57,180
unglaublich gut  Wahrscheinlich besser

213
00:07:57,180 --> 00:08:00,960
als ich es könnte, es hat eine Felltextur, äh,

214
00:08:00,960 --> 00:08:03,360
unglaublich detailliert, aber wenn ich Sie bitte,

215
00:08:03,360 --> 00:08:05,340
etwas zu tun, was meiner Meinung nach

216
00:08:05,340 --> 00:08:08,039
konzeptionell einfacher ist, wie zum Beispiel einen blauen

217
00:08:08,039 --> 00:08:10,560
Würfel auf einen roten Würfel zu zeichnen, gelingt

218
00:08:10,560 --> 00:08:13,380
dies nicht und für mich erscheint das

219
00:08:13,380 --> 00:08:15,300
seitdem unintuitiv  Die zweite Aufgabe scheint

220
00:08:15,300 --> 00:08:18,180
wesentlich einfacher zu sein, aber ich

221
00:08:18,180 --> 00:08:19,860
behaupte, dass der Grund dafür, warum dies

222
00:08:19,860 --> 00:08:21,599
überraschend ist, genau der gleiche ist,

223
00:08:21,599 --> 00:08:23,580
warum das Amnest-Übersetzungsbeispiel überraschend war.

224
00:08:23,580 --> 00:08:25,560


225
00:08:25,560 --> 00:08:28,020
Hier findet diese Symmetrietransformation statt, nämlich die

226
00:08:28,020 --> 00:08:29,400
Transformation zwischen diesen komplexen

227
00:08:29,400 --> 00:08:31,740
Objekten eines Teddybären  und der Mond und

228
00:08:31,740 --> 00:08:34,320
diese einfachen Würfelobjekte, von denen wir

229
00:08:34,320 --> 00:08:36,360
intuitiv erwarten, dass das Netzwerk in der

230
00:08:36,360 --> 00:08:38,820
Lage ist, damit umzugehen und sie zu respektieren, und wir sehen,

231
00:08:38,820 --> 00:08:40,919
dass es nicht so einfach ist, wie

232
00:08:40,919 --> 00:08:43,380
Fukushimas Arbeit gezeigt hat, dass diese

233
00:08:43,380 --> 00:08:46,200
natürlichen Strukturen der Hierarchie und

234
00:08:46,200 --> 00:08:47,700
Bündelung unseres visuellen Systems sind

235
00:08:47,700 --> 00:08:49,680
effektiv für die Durchführung von Verallgemeinerungen auf

236
00:08:49,680 --> 00:08:52,380
kleine Transformationen. Ich behaupte, dass

237
00:08:52,380 --> 00:08:54,060
möglicherweise eine Struktur auf höherer Ebene

238
00:08:54,060 --> 00:08:55,740
erforderlich sein kann, um diese abstrakten Verallgemeinerungsprobleme zu lösen.

239
00:08:55,740 --> 00:08:58,200


240
00:08:58,200 --> 00:09:01,380
Daher ist die Frage, die ich

241
00:09:01,380 --> 00:09:04,380
studiere und stelle, die, was

242
00:09:04,380 --> 00:09:06,060
diese Struktur sein könnte und wie wir sie

243
00:09:06,060 --> 00:09:08,640
umsetzen  Dies geschieht in einer künstlichen neuronalen

244
00:09:08,640 --> 00:09:10,080
Netzwerkarchitektur, die tatsächlich

245
00:09:10,080 --> 00:09:14,120
für die Durchführung von Berechnungen verwendet werden kann.

246
00:09:14,880 --> 00:09:17,700
Um mit der Beantwortung dieser Frage zu beginnen, beginne ich

247
00:09:17,700 --> 00:09:19,680
mit meiner ersten Arbeit zur

248
00:09:19,680 --> 00:09:22,380
topografischen Organisation,

249
00:09:22,380 --> 00:09:25,260
sodass die topografische Organisation

250
00:09:25,260 --> 00:09:27,060
im gesamten Gehirn von

251
00:09:27,060 --> 00:09:29,760
Bereichen auf Saphirebene des primären visuellen Kortex aus beobachtet werden kann  und

252
00:09:29,760 --> 00:09:31,500
man kann es sehr grob als diese

253
00:09:31,500 --> 00:09:33,540
Eigenschaft beschreiben, dass Neuronen, die nahe beieinander liegen, dazu

254
00:09:33,540 --> 00:09:35,760
neigen, auf ähnliche Dinge zu reagieren.

255
00:09:35,760 --> 00:09:38,220
Auf der linken Seite zeigen wir beispielsweise

256
00:09:38,220 --> 00:09:39,720
die farbcodierte Präferenz jedes

257
00:09:39,720 --> 00:09:42,959
Neurons im primären digitalen Kortex als

258
00:09:42,959 --> 00:09:45,360
Reaktion auf ausgerichtete Linien  und wir sehen

259
00:09:45,360 --> 00:09:46,740
diesen reibungslos variierenden Satz von

260
00:09:46,740 --> 00:09:48,779
Selektivitäten. Eine andere Art der

261
00:09:48,779 --> 00:09:50,580
Organisation ist als Retina-Topic-

262
00:09:50,580 --> 00:09:52,560
Organisation bekannt, bei der benachbarte Neuronen im

263
00:09:52,560 --> 00:09:54,600
visuellen Kortex dazu neigen, auf benachbarte rezeptive Felder zu reagieren.

264
00:09:54,600 --> 00:09:56,399


265
00:09:56,399 --> 00:09:58,560
Diese Organisation ist jedoch nicht

266
00:09:58,560 --> 00:10:01,080
auf diese Merkmale auf niedriger Ebene beschränkt, sondern um einiges

267
00:10:01,080 --> 00:10:02,519
komplexer  Merkmale wie diejenigen, die

268
00:10:02,519 --> 00:10:05,459
in Gesichtern, Objekten oder Orten vorhanden sind,

269
00:10:05,459 --> 00:10:07,920
und dies bezieht sich auf die sogenannten

270
00:10:07,920 --> 00:10:10,080
funktionsspezifischen Bereiche des Gehirns

271
00:10:10,080 --> 00:10:12,779
wie den fusiformen Gesichtsbereich FFA und

272
00:10:12,779 --> 00:10:15,420
den parakhippocampalen Gesichtsbereich PPA.

273
00:10:15,420 --> 00:10:19,200
Daher ist auch in dieser Arbeit die Hauptidee, dass

274
00:10:19,200 --> 00:10:21,300
dies möglicherweise der Fall ist  Wenn wir die topografische

275
00:10:21,300 --> 00:10:23,580
Organisation in gewissem Sinne nutzen, die

276
00:10:23,580 --> 00:10:25,080
eng mit der

277
00:10:25,080 --> 00:10:27,980
Faltungsoperation und der Architektur von Fukushima zusammenhängt,

278
00:10:27,980 --> 00:10:30,660
können wir die Vorteile davon vielleicht

279
00:10:30,660 --> 00:10:33,420
auf abstraktere Transformationen verallgemeinern, mit

280
00:10:33,420 --> 00:10:34,920
anderen Worten, lernen, wie wir komplexere Homomorphismen erstellen können,

281
00:10:34,920 --> 00:10:36,839
die wir nicht können.

282
00:10:36,839 --> 00:10:38,940
Sie wissen, dass wir das nicht können  Machen Sie es jetzt analytisch,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
nur um zu zeigen, dass wir

285
00:10:42,480 --> 00:10:44,760
mit dieser Idee nicht völlig verrückt sind. Es

286
00:10:44,760 --> 00:10:46,320
gibt einige frühere Arbeiten in diesem Bereich

287
00:10:46,320 --> 00:10:49,880
von Leuten wie Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden aus den frühen 90er und 2000er Jahren,

289
00:10:54,060 --> 00:10:55,800
und sie haben untersucht, wie die topografische

290
00:10:55,800 --> 00:10:57,720
Organisation aussehen könnte  nützlich für das Lernen

291
00:10:57,720 --> 00:11:01,320
von Varianzen, meist in linearen Modellen.

292
00:11:01,320 --> 00:11:03,060
Als wir den Raum betraten, stellte sich für uns daher die Frage, welcher

293
00:11:03,060 --> 00:11:04,680


294
00:11:04,680 --> 00:11:07,140
abstrakte Mechanismus am besten skalierbar ist, der

295
00:11:07,140 --> 00:11:08,880
von diesen Ansätzen genutzt werden kann, den wir

296
00:11:08,880 --> 00:11:10,800
in moderne Architekturen tiefer neuronaler

297
00:11:10,800 --> 00:11:12,959
Netzwerke integrieren können, und letztendlich haben wir uns für

298
00:11:12,959 --> 00:11:15,000
einen entschieden  Ein generativer

299
00:11:15,000 --> 00:11:16,260
Modellierungsansatz, der meiner Meinung nach

300
00:11:16,260 --> 00:11:17,519
für die Menschen in dieser Community interessant sein könnte,

301
00:11:17,519 --> 00:11:18,779


302
00:11:18,779 --> 00:11:21,779
der es uns dann ermöglicht, ihn enger

303
00:11:21,779 --> 00:11:23,579
mit der topografischen unabhängigen

304
00:11:23,579 --> 00:11:26,040
Komponentenanalyse in Verbindung zu bringen, mit der Grundidee,

305
00:11:26,040 --> 00:11:28,320
dass wir ein topografisches

306
00:11:28,320 --> 00:11:30,660
Merkmal im Raum lernen können, indem wir ihm eine topografische

307
00:11:30,660 --> 00:11:32,940
vorherige Verteilung auferlegen

308
00:11:32,940 --> 00:11:34,440


309
00:11:34,440 --> 00:11:37,079
Um nur einen kurzen Hintergrund zu geben, gehe ich

310
00:11:37,079 --> 00:11:39,120
davon aus, dass die meisten Menschen bereits mit unseren latenten Variablen vertraut sind,

311
00:11:39,120 --> 00:11:40,440


312
00:11:40,440 --> 00:11:42,540
aber die allgemeine Annahme ist,

313
00:11:42,540 --> 00:11:44,339
dass das Gehirn ein generatives Modell ist und

314
00:11:44,339 --> 00:11:45,720
diese Idee in gewisser Weise

315
00:11:45,720 --> 00:11:48,060
Helmholtz aus dem 19.

316
00:11:48,060 --> 00:11:50,459
Jahrhundert zugeschrieben werden kann  Dort sagte er, dass das, was wir

317
00:11:50,459 --> 00:11:52,140
sehen, die Lösung eines Rechenproblems ist.

318
00:11:52,140 --> 00:11:54,420
Unser Gehirn errechnet die

319
00:11:54,420 --> 00:11:56,519
wahrscheinlichsten Ursachen aus der Photonenabsorption

320
00:11:56,519 --> 00:11:59,519
in unseren Augen. Das ist ein Beispiel.

321
00:11:59,519 --> 00:12:01,920
Wenn ich Ihnen dieses Bild zeige,

322
00:12:01,920 --> 00:12:03,720
erkennen Sie es sofort als einen Globus mit einer gewissen

323
00:12:03,720 --> 00:12:05,760
Krümmung, wie auch immer er aussieht  Genauso gut könnte es sich um

324
00:12:05,760 --> 00:12:07,620
eine Scheibe mit einer verzerrten

325
00:12:07,620 --> 00:12:09,600
Perspektive darauf handeln. Auf diese Weise

326
00:12:09,600 --> 00:12:12,660
erhalten wir optische Täuschungen oder unsere Bilder,

327
00:12:12,660 --> 00:12:14,880
sodass Ihr Gehirn aufgrund der Struktur darauf schließen lässt, dass es sich

328
00:12:14,880 --> 00:12:17,100
hier um einen Würfel handelt,

329
00:12:17,100 --> 00:12:18,720
aber in Wirklichkeit ist es nur ein flaches

330
00:12:18,720 --> 00:12:19,860
Stück Papier

331
00:12:19,860 --> 00:12:22,740
Sie können sich diesen generativen

332
00:12:22,740 --> 00:12:24,480
Modellaspekt also wie ein

333
00:12:24,480 --> 00:12:26,160
inverses Grafikprogramm vorstellen.

334
00:12:26,160 --> 00:12:28,140
Im Programm sind die abstrakten Eigenschaften

335
00:12:28,140 --> 00:12:30,660
der Kugel bekannt, die Position, die

336
00:12:30,660 --> 00:12:32,820
Größe und die Beleuchtung. Diese werden zum

337
00:12:32,820 --> 00:12:34,560
Projizieren der Kugel verwendet, um das 2D-

338
00:12:34,560 --> 00:12:37,440
Bild zu erstellen, das gerendert wird  Was

339
00:12:37,440 --> 00:12:40,019
Humboldt und andere also sagen, ist im Grunde, dass

340
00:12:40,019 --> 00:12:41,940
das Gehirn als generatives Modell

341
00:12:41,940 --> 00:12:43,680
tatsächlich versucht, diesen

342
00:12:43,680 --> 00:12:45,959
generativen Prozess umzukehren und Schlussfolgerungen zu ziehen

343
00:12:45,959 --> 00:12:48,300
und auf die zugrunde liegenden Ursachen unserer Empfindungen zu schließen

344
00:12:48,300 --> 00:12:49,680


345
00:12:49,680 --> 00:12:51,600


346
00:12:51,600 --> 00:12:53,100


347
00:12:53,100 --> 00:12:55,440
Heutzutage wird viel von generativen Modellen gesprochen.

348
00:12:55,440 --> 00:12:57,180
Ähm, und ich spreche nicht unbedingt nur

349
00:12:57,180 --> 00:12:59,639
von der Generierung von Bildern oder hübschen

350
00:12:59,639 --> 00:13:01,260
Bildern.

351
00:13:01,260 --> 00:13:03,120
Ähm, ich möchte wirklich

352
00:13:03,120 --> 00:13:07,920
einen Rahmen für unbeaufsichtigtes Lernen meinen

353
00:13:07,920 --> 00:13:10,019


354
00:13:10,019 --> 00:13:11,700


355
00:13:11,700 --> 00:13:14,279
topografische priorisierte, also generative Modelle

356
00:13:14,279 --> 00:13:16,019
werden typischerweise als eine gemeinsame

357
00:13:16,019 --> 00:13:18,720
Verteilung über Beobachtungen

358
00:13:18,720 --> 00:13:21,720


359
00:13:21,720 --> 00:13:23,940


360
00:13:23,940 --> 00:13:25,620


361
00:13:25,620 --> 00:13:28,440


362
00:13:28,440 --> 00:13:30,420
Generatives Modell Bedingtes

363
00:13:30,420 --> 00:13:33,420


364
00:13:33,420 --> 00:13:34,980


365
00:13:34,980 --> 00:13:37,019


366
00:13:37,019 --> 00:13:38,880


367
00:13:38,880 --> 00:13:41,279
generatives

368
00:13:41,279 --> 00:13:42,899
Modell P von

369
00:13:42,899 --> 00:13:45,920
posterior P von Z bei gegebenem

370
00:13:45,920 --> 00:13:49,139


371
00:13:49,139 --> 00:13:50,639


372
00:13:50,639 --> 00:13:53,279


373
00:13:53,279 --> 00:13:55,620


374
00:13:55,620 --> 00:13:57,720


375
00:13:57,720 --> 00:13:59,700


376
00:13:59,700 --> 00:14:01,500


377
00:14:01,500 --> 00:14:03,180
Sparsity-Strafen aus der

378
00:14:03,180 --> 00:14:04,560
Analyse des unabhängigen Lernens: Sie möchten, dass Ihre

379
00:14:04,560 --> 00:14:06,540
Aktivierungen spärlich sind, was bedeutet, dass viele von

380
00:14:06,540 --> 00:14:09,420
ihnen null sind. Das könnte also

381
00:14:09,420 --> 00:14:10,680
etwa so aussehen: Sie haben eine Reihe

382
00:14:10,680 --> 00:14:12,300
blauer Quadrate, die aktiv sind, aber die meisten

383
00:14:12,300 --> 00:14:14,459
davon sind nicht aktiv, sondern speziell

384
00:14:14,459 --> 00:14:16,740
mit denen der Gruppe  Varsity-Strafe: Wir möchten, dass

385
00:14:16,740 --> 00:14:18,839
diese Priors

386
00:14:18,839 --> 00:14:21,600
diesen verteilten, spärlichen Aktivierungen eine geringere Wahrscheinlichkeit

387
00:14:21,600 --> 00:14:24,720
und diesen gruppierten,

388
00:14:24,720 --> 00:14:26,940
dicht gepackten Darstellungen eine höhere Wahrscheinlichkeit zuweisen. Sie können sich

389
00:14:26,940 --> 00:14:28,860
das auch wie eine höhere Strafe vorstellen,

390
00:14:28,860 --> 00:14:30,720
wenn die Dinge verteilt sind, und eine niedrigere

391
00:14:30,720 --> 00:14:33,540
Strafe, wenn die Dinge näher beieinander liegen,

392
00:14:33,540 --> 00:14:36,380
also noch einmal, ähm  Dies kann

393
00:14:36,380 --> 00:14:39,060
abstrakt so geschrieben werden, aber ich möchte die

394
00:14:39,060 --> 00:14:41,160
Theorie aufstellen, dass diese Neuronen, jedes

395
00:14:41,160 --> 00:14:42,779
dieser Quadrate hier, eine Art

396
00:14:42,779 --> 00:14:44,220
Neuron in unserem Modell darstellt und

397
00:14:44,220 --> 00:14:46,560
in diesem 2D-Raster organisiert sind. Wenn wir also

398
00:14:46,560 --> 00:14:48,120
über Gruppierung sprechen, meinen wir wirklich

399
00:14:48,120 --> 00:14:50,820
Gruppierung in dieser 2D-Topologie.

400
00:14:50,820 --> 00:14:53,100
Eine Sache, die wirklich interessant

401
00:14:53,100 --> 00:14:55,560
und irgendwie wichtig ist, ist, dass diese

402
00:14:55,560 --> 00:14:57,779
Prioritäten uns nicht nur eine topografische

403
00:14:57,779 --> 00:15:00,540
Organisation geben, sondern dass sie auch

404
00:15:00,540 --> 00:15:02,459
von Leuten wie

405
00:15:02,459 --> 00:15:05,760
Erosi Marcelli und Bruno bemerkt oder untersucht wurden, um

406
00:15:05,760 --> 00:15:07,740
tatsächlich zu passen  Die Statistiken natürlicher

407
00:15:07,740 --> 00:15:08,959
Daten,

408
00:15:08,959 --> 00:15:11,180
insbesondere natürlicher Bilder,

409
00:15:11,180 --> 00:15:14,100
haben gezeigt, dass man bei Verwendung dieser Art von

410
00:15:14,100 --> 00:15:16,139
Prior tatsächlich eine spärlichere Menge an

411
00:15:16,139 --> 00:15:18,839
Aktivierungen erhält, was bedeutet, dass der Prior

412
00:15:18,839 --> 00:15:20,459
etwas besser zum wahren generativen Prozess passt,

413
00:15:20,459 --> 00:15:22,620
und wie wir wissen, hat das Gehirn dies getan

414
00:15:22,620 --> 00:15:24,779
ein hohes Maß an Sparse, und es wird

415
00:15:24,779 --> 00:15:26,339
davon ausgegangen, dass dies für die Effizienz sehr relevant ist.

416
00:15:26,339 --> 00:15:28,620


417
00:15:28,620 --> 00:15:30,839
Um also ein wenig mehr auf die

418
00:15:30,839 --> 00:15:32,760
Details zur Implementierung dieser Art von

419
00:15:32,760 --> 00:15:35,160
Gruppensparse einzugehen, verwenden wir ein hierarchisches

420
00:15:35,160 --> 00:15:37,320
generatives Modell, und dies wird

421
00:15:37,320 --> 00:15:39,060
im Grunde genommen von einigen eingeführt

422
00:15:39,060 --> 00:15:41,339
Topografische ICA-Arbeit

423
00:15:41,339 --> 00:15:43,320
um die Idee ist, dass Sie eine

424
00:15:43,320 --> 00:15:45,000
latente Variable U auf höherer Ebene haben, die

425
00:15:45,000 --> 00:15:47,820
gleichzeitig die Varianz

426
00:15:47,820 --> 00:15:50,279
mehrerer Variablen T auf niedrigerer Ebene reguliert. Auf

427
00:15:50,279 --> 00:15:52,440
diese Weise erhalten wir Gruppensparsity.

428
00:15:52,440 --> 00:15:55,440
Um eine topografische Organisation zu erhalten,

429
00:15:55,440 --> 00:15:56,760
können Sie mehrere dieser latenten

430
00:15:56,760 --> 00:15:59,339
Variablen leicht verwenden  Sie überlappen sich mit

431
00:15:59,339 --> 00:16:02,519
ihren Einflussfeldern, so dass

432
00:16:02,519 --> 00:16:04,260
wir sie ihre Nachbarschaften nennen können,

433
00:16:04,260 --> 00:16:05,699
und das wird Ihnen diese glatte Korrelationsstruktur geben, die Sie suchen. Wenn

434
00:16:05,699 --> 00:16:07,440


435
00:16:07,440 --> 00:16:09,899
Sie also die Intuition dafür bekommen,

436
00:16:09,899 --> 00:16:12,060
sehen Sie, dass diese Variable T

437
00:16:12,060 --> 00:16:14,279
hier unten nicht hinkommt  jede Eingabe

438
00:16:14,279 --> 00:16:17,160
von diesem U oben, aber es teilt

439
00:16:17,160 --> 00:16:19,139
eine u-Variable mit diesem T in der Mitte,

440
00:16:19,139 --> 00:16:21,300
also ist es, als würden sie Varianten teilen,

441
00:16:21,300 --> 00:16:22,980
sie teilen einige Komponenten mit

442
00:16:22,980 --> 00:16:24,959
ihren Nachbarn, aber nicht alle Komponenten,

443
00:16:24,959 --> 00:16:26,579
und das liegt wirklich an dieser lokalen

444
00:16:26,579 --> 00:16:28,079
Konnektivität von

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
Um es einfach zu halten, wie wir ein

447
00:16:33,180 --> 00:16:34,980
generatives Modell verwenden, kehren wir zu einer

448
00:16:34,980 --> 00:16:37,440
einzelnen U-Variablen zurück. Die Herausforderung bei

449
00:16:37,440 --> 00:16:38,940
dieser Art von Architektur, die es

450
00:16:38,940 --> 00:16:42,360
viele Jahre lang schwierig gemacht hat, besteht darin, wie

451
00:16:42,360 --> 00:16:44,579
man den ungefähren Posteriorwert ableitet

452
00:16:44,579 --> 00:16:47,579
über diese Zwischenvariablen in

453
00:16:47,579 --> 00:16:50,100
dieser hierarchischen Architektur und das

454
00:16:50,100 --> 00:16:52,560
ist nicht ganz einfach, daher

455
00:16:52,560 --> 00:16:54,420
haben frühere Arbeiten Heuristiken verwendet, die für

456
00:16:54,420 --> 00:16:56,699
lineare Modelle entwickelt wurden, und in unserer Arbeit haben wir festgestellt,

457
00:16:56,699 --> 00:16:58,680
dass dies wirklich nicht auf moderne

458
00:16:58,680 --> 00:17:01,199
neuronale Netzwerkarchitekturen übertragbar ist, also besteht

459
00:17:01,199 --> 00:17:02,880
unsere Erkenntnis wirklich darin, a zu nutzen

460
00:17:02,880 --> 00:17:04,760
Faktorisierung ist eine spezifische

461
00:17:04,760 --> 00:17:07,640
Neuparametrisierung dieser Verteilung.

462
00:17:07,640 --> 00:17:10,380
Daher wird diese Parametrisierung

463
00:17:10,380 --> 00:17:12,419
insbesondere durch die Definition der

464
00:17:12,419 --> 00:17:14,579
Priorität einer sogenannten Gaußschen

465
00:17:14,579 --> 00:17:16,319
Skalenmischung erreicht, was bedeutet, dass unsere

466
00:17:16,319 --> 00:17:19,140
bedingte Verteilung von T bei gegebenem U

467
00:17:19,140 --> 00:17:21,179
tatsächlich eine Normalverteilung ist, bei der

468
00:17:21,179 --> 00:17:24,299
die Varianz durch diese Variable definiert wird

469
00:17:24,299 --> 00:17:27,720
U und für bestimmte Auswahlmöglichkeiten von U

470
00:17:27,720 --> 00:17:29,340
ist diese Verteilung in der Tat spärlich und

471
00:17:29,340 --> 00:17:31,980
umfasst eine Reihe von Verteilungen

472
00:17:31,980 --> 00:17:33,780
wie Laplossian-Verteilungen und T-

473
00:17:33,780 --> 00:17:36,299
Verteilungen. Eine Art der Definition ist

474
00:17:36,299 --> 00:17:38,940
eine Gaußsche Skalenmischung, die eine

475
00:17:38,940 --> 00:17:40,919
bestimmte Reframe-Neuparametrisierung

476
00:17:40,919 --> 00:17:42,900
in Bezug auf unabhängige Gaußsche

477
00:17:42,900 --> 00:17:45,720
Zufallsvariablen ausgibt  Z und U, also speziell dann

478
00:17:45,720 --> 00:17:48,840
sehen wir, dass diese T-Variable, die

479
00:17:48,840 --> 00:17:50,760
ursprünglich ziemlich komplex war, eigentlich

480
00:17:50,760 --> 00:17:52,799
nur ein Produkt einer Reihe von Gaußschen

481
00:17:52,799 --> 00:17:54,840
Zufallsvariablen ist, die jetzt

482
00:17:54,840 --> 00:17:57,660
viel effizienter in

483
00:17:57,660 --> 00:18:00,120
generativen Modellen arbeiten können, insbesondere in dem, was

484
00:18:00,120 --> 00:18:02,039
wir sind  Was wir tun werden, besteht darin, dass wir

485
00:18:02,039 --> 00:18:04,020
tatsächlich getrennt ungefähre Posteriorwerte für

486
00:18:04,020 --> 00:18:06,720
U und Z erhalten und dann eine

487
00:18:06,720 --> 00:18:08,640
deterministische Kombination davon durchführen können,

488
00:18:08,640 --> 00:18:10,020
um unsere topografische

489
00:18:10,020 --> 00:18:13,140
Variable T zu berechnen, und das ist viel einfacher,

490
00:18:13,140 --> 00:18:15,500
ohne zu viele Details

491
00:18:15,500 --> 00:18:17,700
der Methode einzugehen  Wir haben uns entschieden,

492
00:18:17,700 --> 00:18:18,600
einen sogenannten Variations-

493
00:18:18,600 --> 00:18:20,640
Autoencoder zu verwenden, der Techniken der

494
00:18:20,640 --> 00:18:23,220
Variationsinferenz nutzt, um eine

495
00:18:23,220 --> 00:18:24,780
Untergrenze für die Wahrscheinlichkeit abzuleiten, die es

496
00:18:24,780 --> 00:18:26,940
uns ermöglicht, diese ungefähren

497
00:18:26,940 --> 00:18:29,400
Posterioren mit leistungsstarken nichtlinearen tiefen

498
00:18:29,400 --> 00:18:30,960
neuronalen Netzen zu parametrisieren und sie mit einem

499
00:18:30,960 --> 00:18:33,240
Gradientenabstieg zu optimieren  Dies wird

500
00:18:33,240 --> 00:18:34,440
der aktiven Inferenz-

501
00:18:34,440 --> 00:18:36,900
Community bekannt sein, aber was wir wirklich getan haben, ist, dass

502
00:18:36,900 --> 00:18:38,760


503
00:18:38,760 --> 00:18:41,640
wir statt eines einzigen Encoders im Decoder, wie es bei Baes üblich ist, jetzt

504
00:18:41,640 --> 00:18:43,799
zwei Encoder haben, einen für Sie und einen für Z

505
00:18:43,799 --> 00:18:46,200
separat, und dann kombinieren wir sie auf

506
00:18:46,200 --> 00:18:48,419
diese deterministische Weise  Konstruieren Sie

507
00:18:48,419 --> 00:18:51,660
unsere topografische T-Variable, wenn Sie sehen,

508
00:18:51,660 --> 00:18:53,100
dass dies tatsächlich die

509
00:18:53,100 --> 00:18:54,900
Konstruktion der T-

510
00:18:54,900 --> 00:18:57,620
Verteilung eines Schülers aus Gaußschen Funktionen ist,

511
00:18:57,620 --> 00:19:00,480
und dann können wir dies einstecken. Wir tun dies

512
00:19:00,480 --> 00:19:03,600
vor der Dekodierung und äh und maximieren dann

513
00:19:03,600 --> 00:19:05,820
die Wahrscheinlichkeit der Daten insgesamt, sodass

514
00:19:05,820 --> 00:19:07,260
dies der Ellbogen ist  Die untere

515
00:19:07,260 --> 00:19:09,360
Beweisgrenze hängt von der Wahrscheinlichkeit der

516
00:19:09,360 --> 00:19:12,600
Daten ab und ist tatsächlich der

517
00:19:12,600 --> 00:19:15,720
freien Variationsenergie sehr ähnlich, die in der aktiven Eingangsgemeinschaft verwendet wird. Wenn wir also

518
00:19:15,720 --> 00:19:18,299


519
00:19:18,299 --> 00:19:20,520
diese Details aus dem

520
00:19:20,520 --> 00:19:22,500
Weg räumen, ist das wirklich Interessante, was

521
00:19:22,500 --> 00:19:23,940
passiert, wenn wir dieses generative

522
00:19:23,940 --> 00:19:26,580
Modell trainieren  hat einen relativ einfachen

523
00:19:26,580 --> 00:19:29,460
Gruppensparsity-Strafe in seinem latenten Raum und

524
00:19:29,460 --> 00:19:30,720
wir wollen uns ansehen, was er

525
00:19:30,720 --> 00:19:32,700
im Hinblick auf seine Organisation von

526
00:19:32,700 --> 00:19:34,980
Futures lernt. Zuerst beginnen wir mit dem

527
00:19:34,980 --> 00:19:36,480
einfachsten möglichen Datensatz, wir haben einen

528
00:19:36,480 --> 00:19:38,580
schwarzen Hintergrund mit weißen Quadraten an

529
00:19:38,580 --> 00:19:41,280
zufälligen XY-Positionen  Und wenn wir unseren

530
00:19:41,280 --> 00:19:42,780
Autoencoder mit dieser Gruppensparsity-

531
00:19:42,780 --> 00:19:44,760
Strafe trainieren und uns dann die

532
00:19:44,760 --> 00:19:47,640
Gewichtsvektoren unseres Decoders ansehen, die

533
00:19:47,640 --> 00:19:49,020
wir hier in Blau darstellen, wiederum

534
00:19:49,020 --> 00:19:52,520
organisiert in diesem 2D-Raster, sehen wir, dass

535
00:19:52,520 --> 00:19:54,780
sie tatsächlich lernen, räumlich organisiert zu werden

536
00:19:54,780 --> 00:19:57,539
Standort, so dass dies

537
00:19:57,539 --> 00:19:59,580
als ähnlich zu konvolutionellen

538
00:19:59,580 --> 00:20:01,799
rezeptiven Feldern angesehen werden kann, oder das rezeptive Feld

539
00:20:01,799 --> 00:20:04,679
jedes Neurons wird tatsächlich durch

540
00:20:04,679 --> 00:20:09,059
die Art der Eingaben an seinem Standort bestimmt,

541
00:20:09,059 --> 00:20:10,860
und dies macht intuitiv aus

542
00:20:10,860 --> 00:20:13,140
der Sparsity-Perspektive der Gruppe Sinn, da

543
00:20:13,140 --> 00:20:15,480
für jede bestimmte Region eine Hervorhebung wie

544
00:20:15,480 --> 00:20:17,700
in erfolgt  Gelb, hier sind die Filter in einer bestimmten

545
00:20:17,700 --> 00:20:19,260
Gruppe viel stärker korreliert,

546
00:20:19,260 --> 00:20:20,880
sie haben diese überlappenden

547
00:20:20,880 --> 00:20:23,460
Empfangsfelder als andere zufällige Orte, sodass

548
00:20:23,460 --> 00:20:25,020
wir im Wesentlichen sehen, dass unser Modell

549
00:20:25,020 --> 00:20:27,840
lernt, Aktivitätsaktivitäten in

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
einer Art simuliertem vertikalen Blatt

552
00:20:32,059 --> 00:20:34,260
gemäß den Korrelationen in zu gruppieren  Der

553
00:20:34,260 --> 00:20:36,840
Datensatz ist also nicht in der Faltung,

554
00:20:36,840 --> 00:20:38,580
wo Sie tatsächlich eine Gewichtungsbindung durchführen

555
00:20:38,580 --> 00:20:40,860
und manuell festlegen, dass ich

556
00:20:40,860 --> 00:20:42,539
dieses Gewicht überall hin kopieren möchte. Sie können sich

557
00:20:42,539 --> 00:20:44,220
das vielleicht als eine ungefähre Wartezeit vorstellen,

558
00:20:44,220 --> 00:20:45,500


559
00:20:45,500 --> 00:20:48,120
und das lernen wir tatsächlich aus der

560
00:20:48,120 --> 00:20:49,620
Korrelation  Struktur des Datensatzes

561
00:20:49,620 --> 00:20:51,660
selbst und nur um etwas

562
00:20:51,660 --> 00:20:54,120
mehr biologische Inspiration dafür zu geben

563
00:20:54,120 --> 00:20:56,280
und wir wissen, dass Retinotopie

564
00:20:56,280 --> 00:20:58,020
im Gehirn vorhanden ist. Dies ist ein Beispiel

565
00:20:58,020 --> 00:21:02,460
für Retinotopie im visuellen Kortex und

566
00:21:02,460 --> 00:21:04,500
Sie können sehen, ob Sie den Makaken zeigen  Ein

567
00:21:04,500 --> 00:21:06,780
Bild wie dieses wird in

568
00:21:06,780 --> 00:21:08,520
diese äh-

569
00:21:08,520 --> 00:21:11,700
Topologie projiziert, wodurch Platz tatsächlich auf

570
00:21:11,700 --> 00:21:13,740
der Oberfläche des Kortex erhalten bleibt.

571
00:21:13,740 --> 00:21:16,080
Die Idee ist also, dass die topografische

572
00:21:16,080 --> 00:21:18,419
Organisation und sogar die topografische

573
00:21:18,419 --> 00:21:21,299
Lernorganisation die

574
00:21:21,299 --> 00:21:26,160
Eingabekorrelationen unserer Datensätze beibehält, und

575
00:21:26,160 --> 00:21:28,679
möglicherweise kann dies von Vorteil sein  Ich

576
00:21:28,679 --> 00:21:30,840
verallgemeinere diese Ideen etwas

577
00:21:30,840 --> 00:21:32,340
weiter, also wie ich am

578
00:21:32,340 --> 00:21:34,679
Anfang sagte, es wäre sogar noch besser, wenn

579
00:21:34,679 --> 00:21:37,200
wir einfach etwas mehr als

580
00:21:37,200 --> 00:21:39,320
nur Faltung lernen könnten, vielleicht kompliziertere

581
00:21:39,320 --> 00:21:43,679
Äquivarianzen. Wie machen wir das? Eine

582
00:21:43,679 --> 00:21:45,720
Sache, die in der natürlichen Intelligenz klar ist,

583
00:21:45,720 --> 00:21:48,299
ist, dass wir  Wir existieren in

584
00:21:48,299 --> 00:21:51,120
dieser Welt der IID-Frames nicht, aber wir existieren

585
00:21:51,120 --> 00:21:53,520
in einer Welt mit kontinuierlichen Sequenzen von

586
00:21:53,520 --> 00:21:55,620
Transformationen. Vielleicht können wir

587
00:21:55,620 --> 00:21:58,440
unser Modell also auf diese Umgebung erweitern, um zu lernen,

588
00:21:58,440 --> 00:22:01,080
Transformationen zu beobachten. Dies ist eine Idee

589
00:22:01,080 --> 00:22:03,299
der zeitlichen Kohärenz.

590
00:22:03,299 --> 00:22:05,280
Was würde also passieren, wenn wir einfach  Wir haben

591
00:22:05,280 --> 00:22:08,280
unser bisheriges Framework im Laufe der

592
00:22:08,280 --> 00:22:10,620
Zeit erweitert. Anstatt also nur zu

593
00:22:10,620 --> 00:22:13,080
gruppieren, sagen wir, dass unsere Neuronen

594
00:22:13,080 --> 00:22:15,059
im Hinblick auf die räumliche

595
00:22:15,059 --> 00:22:17,400
Ausdehnung auf dem Kortex gruppenmäßig dünn besetzt sein sollen, wollen wir tatsächlich, dass

596
00:22:17,400 --> 00:22:18,960
sie im Laufe der Zeit gruppensparsam sind,

597
00:22:18,960 --> 00:22:20,640
d. h., wenn ein Satz von Neuronen

598
00:22:20,640 --> 00:22:22,559
aktiv ist  Jetzt möchten wir, dass derselbe Satz von

599
00:22:22,559 --> 00:22:24,360
Neuronen auch in Zukunft aktiv ist.

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
Wenn wir uns das ansehen, wenn wir intuitiv

602
00:22:27,840 --> 00:22:30,600
darüber nachdenken, sehen wir, dass dies

603
00:22:30,600 --> 00:22:33,059
tatsächlich ermutigender ist. Invarianz und

604
00:22:33,059 --> 00:22:35,039
Äquivarianz – eine Möglichkeit, dies zu verstehen, ist, dass

605
00:22:35,039 --> 00:22:37,140
wir sagen:  Ich möchte, dass dieselben Neuronen

606
00:22:37,140 --> 00:22:39,179
ständig aktiv sind, aber die

607
00:22:39,179 --> 00:22:41,280
Eingabetransformation ändert sich genau dann, wenn sich die

608
00:22:41,280 --> 00:22:44,220
Füße dieses kleinen Fuchses bewegen. Wenn also

609
00:22:44,220 --> 00:22:45,960
dieselben Neuronen immer wieder für dasselbe codieren,

610
00:22:45,960 --> 00:22:47,880
die Füße sich aber

611
00:22:47,880 --> 00:22:49,320
bewegen, werden diese Neuronen

612
00:22:49,320 --> 00:22:51,360
lernen  zum Beispiel invariant gegenüber der Bewegung

613
00:22:51,360 --> 00:22:53,880
dieses Beins dieses Hundes zu sein,

614
00:22:53,880 --> 00:22:57,539
also ist das stattdessen so, ups,

615
00:22:57,539 --> 00:23:01,200
ich bin hier den falschen Weg gegangen, äh, also

616
00:23:01,200 --> 00:23:04,860
stattdessen, äh, unsere Einsicht war, dass diese

617
00:23:04,860 --> 00:23:06,659
Gruppe anfängt, sich stattdessen

618
00:23:06,659 --> 00:23:09,059
in Bezug auf die Zeit zu verschieben, also

619
00:23:09,059 --> 00:23:10,980
würde dies bedeuten  dass sequentiell verschobene

620
00:23:10,980 --> 00:23:13,080
Sätze von Aktivierungen dazu angeregt würden,

621
00:23:13,080 --> 00:23:15,179
gemeinsam zu aktivieren, und dann wäre unser latenter

622
00:23:15,179 --> 00:23:16,440
Raum tatsächlich in

623
00:23:16,440 --> 00:23:18,000
Bezug auf die beobachteten Transformationen strukturiert,

624
00:23:18,000 --> 00:23:19,980
so dass Sie hier sehen können, dass es sich nicht um

625
00:23:19,980 --> 00:23:21,480
denselben Satz von Neuronen handelt, der zu allen Zeitschritten aktiv ist, sondern

626
00:23:21,480 --> 00:23:23,340
tatsächlich um einen sequentiellen

627
00:23:23,340 --> 00:23:24,900
permutierter Satz von Neuronen, die wir

628
00:23:24,900 --> 00:23:27,780
auf diese spärliche Art und Weise gruppieren, äh,

629
00:23:27,780 --> 00:23:29,940
und dann können wir

630
00:23:29,940 --> 00:23:33,419
verschiedene Beobachtungen im Laufe der Zeit modellieren, aber

631
00:23:33,419 --> 00:23:34,860
sie sind immer noch im Hinblick auf das

632
00:23:34,860 --> 00:23:36,960
Erlernen einer Transformation und die Bewahrung

633
00:23:36,960 --> 00:23:38,340
dieser Korrelationsstruktur der

634
00:23:38,340 --> 00:23:40,020
Empathie verbunden,

635
00:23:40,020 --> 00:23:41,940
also wenn wir  Wenn Sie dies in unserer

636
00:23:41,940 --> 00:23:44,400
topografischen Bae-Architektur zusammenfügen, können Sie

637
00:23:44,400 --> 00:23:46,020
etwas erhalten, das so aussieht. Sie sehen,

638
00:23:46,020 --> 00:23:48,120
dass wir eine Eingabesequenz haben, wir

639
00:23:48,120 --> 00:23:51,240
kodieren wieder eine Z-Variable und dann

640
00:23:51,240 --> 00:23:53,520


641
00:23:53,520 --> 00:23:55,740
hier mehrere U-Variablen im Nenner und dann wird jede dieser U-

642
00:23:55,740 --> 00:23:58,620
Variablen verschoben  Ähm, so wie wir es

643
00:23:58,620 --> 00:24:00,480
zuvor gezeigt haben, um

644
00:24:00,480 --> 00:24:02,820
diese Verschiebungsäquivarianzstruktur zu erreichen, nach

645
00:24:02,820 --> 00:24:04,740
der wir suchen. Wenn wir

646
00:24:04,740 --> 00:24:07,080
diese in dieser Student-T-

647
00:24:07,080 --> 00:24:09,240
Produktverteilung kombinieren, erhalten wir eine einzelne latente

648
00:24:09,240 --> 00:24:10,740
Variable. Dies ist jetzt unsere topografische

649
00:24:10,740 --> 00:24:13,860
Variable T, und jetzt, da wir  Wenn wir diese

650
00:24:13,860 --> 00:24:16,140
bekannte Struktur in unserem latenten Raum haben,

651
00:24:16,140 --> 00:24:17,460
können Sie es sich wie ein strukturiertes Weltmodell vorstellen.

652
00:24:17,460 --> 00:24:19,919
Wir wissen, wie wir diesen

653
00:24:19,919 --> 00:24:21,659
latenten Raum transformieren können. In diesem Fall geschieht dies durch

654
00:24:21,659 --> 00:24:23,580
Permutation dieser Aktivierungen um diese

655
00:24:23,580 --> 00:24:25,860
Kreise herum, was wie eine zyklische Rolle und eine

656
00:24:25,860 --> 00:24:28,380
zyklische Verschiebung wirkt, von der wir wissen, dass dies der Fall ist  Dies wird

657
00:24:28,380 --> 00:24:30,120
unseren erlernten Eingabetransformationen entsprechen,

658
00:24:30,120 --> 00:24:32,640
und wir können das überprüfen,

659
00:24:32,640 --> 00:24:34,620
indem wir sagen: Okay, was wäre, wenn ich diese

660
00:24:34,620 --> 00:24:36,480
Eingabetransformation fortsetze, die wahre

661
00:24:36,480 --> 00:24:38,100
Transformation im Datensatz, die

662
00:24:38,100 --> 00:24:40,559
eine Rotation ist, und dann vergleiche ich das damit,

663
00:24:40,559 --> 00:24:42,659
wie ich meine Rolle in meinem letzten Job gemacht habe  Raum,

664
00:24:42,659 --> 00:24:44,700
indem ich meine Aktivierungen in meinem Gehirn hin und her bewege,

665
00:24:44,700 --> 00:24:47,280
und dann dekodieren wir und wir sehen, dass

666
00:24:47,280 --> 00:24:49,919
wir genau das Gleiche erhalten. Dies

667
00:24:49,919 --> 00:24:52,140
demonstriert also diese

668
00:24:52,140 --> 00:24:53,580
Kommunalitätseigenschaft, über die ich zuvor gesprochen habe,

669
00:24:53,580 --> 00:24:56,820
um Homomorphismus zu verifizieren

670
00:24:56,820 --> 00:24:58,799
und so die Qualität etwas besser zu messen

671
00:24:58,799 --> 00:25:02,460
Quantitativ, ähm, wir können

672
00:25:02,460 --> 00:25:04,440
einen sogenannten Äquivarianzverlust messen,

673
00:25:04,440 --> 00:25:07,080
das ist also wirklich die Quantifizierung

674
00:25:07,080 --> 00:25:09,360
dieses Unterschieds zwischen der Aktivierung unserer gerollten

675
00:25:09,360 --> 00:25:12,120
Kapsel oder dem Rollen in unserem

676
00:25:12,120 --> 00:25:15,059
Kopf und der Beobachtung, wie sich das Rollen entfaltet,

677
00:25:15,059 --> 00:25:16,559
und vorwärts. Sie beobachten, wie sich die

678
00:25:16,559 --> 00:25:19,440
Transformation vor uns entfaltet, sodass wir

679
00:25:19,440 --> 00:25:21,600
die Topographie sehen  Bae erreicht einen

680
00:25:21,600 --> 00:25:24,000
deutlich geringeren

681
00:25:24,000 --> 00:25:26,700
Äquivarianzfehler. Diese Blasen-Vae ist das, worüber ich

682
00:25:26,700 --> 00:25:27,960
zuvor gesprochen habe, da sie

683
00:25:27,960 --> 00:25:29,820
Invarianz lernt, sodass sie nicht über die Verschiebungsoperation verfügt

684
00:25:29,820 --> 00:25:32,340
und die traditionelle Vae-Art

685
00:25:32,340 --> 00:25:35,640
keine Vorstellung von Organisation oder

686
00:25:35,640 --> 00:25:37,380
zeitlicher Komponente hat, sodass die Leistung

687
00:25:37,380 --> 00:25:40,320
zusätzlich sehr schlecht ist  Daraus sehen wir, dass

688
00:25:40,320 --> 00:25:41,700
das Modell ein besseres generatives Modell

689
00:25:41,700 --> 00:25:45,059
von Sequenzen ist, es erhält nur eine

690
00:25:45,059 --> 00:25:48,179
geringere negative logarithmische Wahrscheinlichkeit für

691
00:25:48,179 --> 00:25:50,100
den Datensatz, sodass es

692
00:25:50,100 --> 00:25:51,720
diesen Datensatz besser modellieren kann, da es eine

693
00:25:51,720 --> 00:25:52,919
Vorstellung von der Struktur der Sequenzen hat

694
00:25:52,919 --> 00:25:55,460
Transformationen,

695
00:25:55,980 --> 00:25:58,140
ähm, wir können das an mehreren

696
00:25:58,140 --> 00:25:59,760
verschiedenen Transformationstypen testen und in der

697
00:25:59,760 --> 00:26:00,840
oberen Reihe zeigen wir die wahre

698
00:26:00,840 --> 00:26:02,880
Transformation. Wir haben diese

699
00:26:02,880 --> 00:26:05,039
ausgegrauten Bilder herausgezogen und dann in der unteren

700
00:26:05,039 --> 00:26:07,080
Reihe codieren wir und dann rollen wir einfach

701
00:26:07,080 --> 00:26:08,700
unsere Aktivierungen herum und behalten sie bei  Durch die

702
00:26:08,700 --> 00:26:12,140
Dekodierung sehen wir, was das Modell

703
00:26:12,140 --> 00:26:15,000
als aktuelle

704
00:26:15,000 --> 00:26:17,039
beobachtete Transformation gelernt hat, und

705
00:26:17,039 --> 00:26:19,340
wir sehen, dass es

706
00:26:19,340 --> 00:26:21,360
diese Elemente der

707
00:26:21,360 --> 00:26:23,640
Sequenz, die es noch nie zuvor gesehen hat, grundsätzlich perfekt rekonstruieren kann,

708
00:26:23,640 --> 00:26:25,260
zusätzlich zu Bildern, die aus

709
00:26:25,260 --> 00:26:26,580
dem Testsatz stammen und noch nie gesehen wurden

710
00:26:26,580 --> 00:26:28,380
Da es vorher einfach weiß, was die

711
00:26:28,380 --> 00:26:29,760
Transformation ist, die es gerade

712
00:26:29,760 --> 00:26:31,500
codiert, kann es diese auf neue Beispiele verallgemeinern.

713
00:26:31,500 --> 00:26:33,919


714
00:26:34,020 --> 00:26:36,360
Die Erkenntnis aus diesem Teil ist also wirklich die

715
00:26:36,360 --> 00:26:38,039
topografische Organisation. Wir haben gezeigt, dass

716
00:26:38,039 --> 00:26:40,080
eine beibehaltene Eingabestruktur und jetzt

717
00:26:40,080 --> 00:26:41,940
zeigen wir, dass sie potenziell die

718
00:26:41,940 --> 00:26:44,279
Effizienz und Verallgemeinerung verbessern kann  Wie wir

719
00:26:44,279 --> 00:26:46,200
hoffen würden, hat

720
00:26:46,200 --> 00:26:48,600
uns endlich etwas überrascht,

721
00:26:48,600 --> 00:26:49,980
und ich dachte, es wäre möglicherweise das

722
00:26:49,980 --> 00:26:52,500
Interessanteste, dass diese

723
00:26:52,500 --> 00:26:53,700
Transformationen, die von unserem

724
00:26:53,700 --> 00:26:54,960
Modell gelernt werden, tatsächlich die

725
00:26:54,960 --> 00:26:57,059
Kombinationen von Transformationen verallgemeinern, die

726
00:26:57,059 --> 00:26:59,580
wir während des Trainings nicht sehen, also zum

727
00:26:59,580 --> 00:27:02,100
Beispiel trotz nur Training  zu Farb-

728
00:27:02,100 --> 00:27:04,200
und Rotationstransformationen und

729
00:27:04,200 --> 00:27:06,419
Isolation, wenn dem Modell zum Testzeitpunkt

730
00:27:06,419 --> 00:27:08,340
eine kombinierte Farbrotationstransformation präsentiert wird,

731
00:27:08,340 --> 00:27:11,100
sehen wir, dass es

732
00:27:11,100 --> 00:27:13,140
diese

733
00:27:13,140 --> 00:27:14,700
Transformationen durch die

734
00:27:14,700 --> 00:27:17,159
Kapselrolle vollständig modellieren und perfekt abschließen kann, was bedeutet, dass es gelernt hat, die

735
00:27:17,159 --> 00:27:19,620
Darstellung für diese verschiedenen zu faktorisieren

736
00:27:19,620 --> 00:27:20,880
Transformationen und es kann

737
00:27:20,880 --> 00:27:24,600
sie zur Inferenzzeit flexibel kombinieren.

738
00:27:24,600 --> 00:27:28,140
Vielleicht erhalten wir also nicht nur

739
00:27:28,140 --> 00:27:29,820
offiziell Effizienz bei der Verallgemeinerung, sondern auch

740
00:27:29,820 --> 00:27:34,100
eine gewisse grundlegende Kompositionalität.

741
00:27:34,260 --> 00:27:36,059
Sprechen wir also über die Einschränkungen und darüber,

742
00:27:36,059 --> 00:27:38,460
was wir als Nächstes tun könnten. Die

743
00:27:38,460 --> 00:27:40,620
Haupteinschränkung besteht darin, dass es eine gibt  Vordefinierte

744
00:27:40,620 --> 00:27:44,159
Transformation, die wir

745
00:27:44,159 --> 00:27:46,500
sowohl räumlich als auch zeitlich auferlegen. Obwohl wir

746
00:27:46,500 --> 00:27:49,080
uns von Gruppentransformationen befreit haben und

747
00:27:49,080 --> 00:27:52,440
insbesondere die Übersetzung oder

748
00:27:52,440 --> 00:27:53,940
Rotation mögen, wie sie derzeit in der

749
00:27:53,940 --> 00:27:55,559
Welt des maschinellen Lernens durchgeführt werden,

750
00:27:55,559 --> 00:27:59,240
haben wir immer noch diese fest codierte

751
00:27:59,240 --> 00:28:01,980
latente Rolle in unserem  Köpfe für alles, was

752
00:28:01,980 --> 00:28:03,900
wir sehen, und um dies ein wenig

753
00:28:03,900 --> 00:28:05,700
flexibler zu machen, damit wir hoffentlich

754
00:28:05,700 --> 00:28:08,880
eine größere Vielfalt an Transformationen modellieren können. Wir

755
00:28:08,880 --> 00:28:10,980
denken, wir können uns vielleicht

756
00:28:10,980 --> 00:28:13,860
von strukturierteren räumlichen und

757
00:28:13,860 --> 00:28:15,600
zeitlichen Dynamiken inspirieren lassen, die im Gehirn beobachtet werden

758
00:28:15,600 --> 00:28:18,120
und die uns begleiten  Zum

759
00:28:18,120 --> 00:28:20,400
zweiten Teil dieses Vortrags, bei dem es um die

760
00:28:20,400 --> 00:28:22,140
räumliche und zeitliche Dynamik geht, die wir

761
00:28:22,140 --> 00:28:23,039
in

762
00:28:23,039 --> 00:28:25,200
künstliche neuronale Netze zu integrieren versuchen. Ein Beispiel

763
00:28:25,200 --> 00:28:27,059
dafür sind Wanderwellen, wie ich sie hier gezeigt habe.

764
00:28:27,059 --> 00:28:28,020


765
00:28:28,020 --> 00:28:30,600
Was meinen wir also damit?

766
00:28:30,600 --> 00:28:32,279
Kürzlich erschienene Arbeit, in der sie

767
00:28:32,279 --> 00:28:36,059
ein 9-Tesla-FMRT mit einer

768
00:28:36,059 --> 00:28:38,700
Auflösung von 36 Millisekunden verwendeten, um eine einzelne

769
00:28:38,700 --> 00:28:40,980
Scheibe eines Rattengehirns unter Narkose abzubilden,

770
00:28:40,980 --> 00:28:43,320
und was wir sehen, ist diese sehr klar

771
00:28:43,320 --> 00:28:45,720
strukturierte räumliche und zeitliche Aktivität und

772
00:28:45,720 --> 00:28:48,299
Korrelationen, und diese Autoren der

773
00:28:48,299 --> 00:28:50,520
Arbeit analysieren dies anschließend  Aktivität in

774
00:28:50,520 --> 00:28:52,919
Bezug auf die Hauptmodi, wie

775
00:28:52,919 --> 00:28:55,799
rechts dargestellt, daher ist unsere Hypothese, dass

776
00:28:55,799 --> 00:28:57,179
möglicherweise eine Art

777
00:28:57,179 --> 00:28:59,039
Korrelationsstruktur wie diese

778
00:28:59,039 --> 00:29:01,260
für die Strukturierung der Darstellungen

779
00:29:01,260 --> 00:29:03,240
unseres Modells in Bezug auf beobachtete

780
00:29:03,240 --> 00:29:05,100
Transformationen von Vorteil sein kann, jedoch auf viel

781
00:29:05,100 --> 00:29:07,440
flexiblere Weise als einfach  Nur eine zyklische

782
00:29:07,440 --> 00:29:10,700
Verschiebung, wie wir sie vorher gemacht haben, ähm,

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
und lassen Sie mich sagen, dass dies nicht nur

785
00:29:15,900 --> 00:29:19,320
bei SSI-Ratten beobachtet wird. Äh, Sie

786
00:29:19,320 --> 00:29:20,940
können sehen, wie diese Wanderwellen

787
00:29:20,940 --> 00:29:24,179
im Mt.-Kortex

788
00:29:24,179 --> 00:29:27,600
wacher Primaten auftreten, ähm, also zum Beispiel auf

789
00:29:27,600 --> 00:29:29,580
der linken Seite  Hier zeigen sie Wanderwellen,

790
00:29:29,580 --> 00:29:31,740
die sich tatsächlich

791
00:29:31,740 --> 00:29:35,520
ändern. Wie wahrscheinlich es ist, dass ein Primat einen

792
00:29:35,520 --> 00:29:38,279
Reiz mit geringem Kontrast sieht, basierend auf der Phase

793
00:29:38,279 --> 00:29:40,980
der Welle. Außerdem zeigen sie, dass

794
00:29:40,980 --> 00:29:43,500
ein Reiz mit hohem Kontrast auf der

795
00:29:43,500 --> 00:29:45,779
rechten Seite eine Wanderwellenaktivität induzieren kann,

796
00:29:45,779 --> 00:29:47,520
die sich nach außen ausbreitet  Sogar

797
00:29:47,520 --> 00:29:50,039
im primären visuellen Kortex sind diese

798
00:29:50,039 --> 00:29:52,140
im gesamten Gehirn

799
00:29:52,140 --> 00:29:54,000
auf mehreren Ebenen wirklich allgegenwärtig und es wäre

800
00:29:54,000 --> 00:29:55,440
interessant zu untersuchen, welche Auswirkungen sie

801
00:29:55,440 --> 00:29:58,140


802
00:29:58,140 --> 00:29:59,700
in unserem Fall auf das Lernen von äh-Struktur-Repräsentationen haben, oder es

803
00:29:59,700 --> 00:30:01,799


804
00:30:01,799 --> 00:30:04,140
gibt allgemein frühere Arbeiten, die

805
00:30:04,140 --> 00:30:06,720
diese Arten von äh-Dynamik untersucht haben  Und sie

806
00:30:06,720 --> 00:30:08,700
erstellen Modelle, so dass oben die

807
00:30:08,700 --> 00:30:10,380
Gleichungen sind, die ein Spike-

808
00:30:10,380 --> 00:30:12,600
Neuronales Netzwerk beschreiben. Sie zeigen, dass man, wenn man

809
00:30:12,600 --> 00:30:15,720
Zeitverzögerungen, eigentlich axonale

810
00:30:15,720 --> 00:30:18,240
Zeitverzögerungen zwischen Neuronen, implementiert,

811
00:30:18,240 --> 00:30:20,820
diese Strukturdynamik von

812
00:30:20,820 --> 00:30:22,440
Wanderwellen erhält, solange die Netzwerkgröße

813
00:30:22,440 --> 00:30:24,059
groß genug ist

814
00:30:24,059 --> 00:30:26,520
Ähm, aber wie viele Leute wahrscheinlich wissen,

815
00:30:26,520 --> 00:30:28,620
ist es relativ schwierig,

816
00:30:28,620 --> 00:30:31,320
Spike-Neuronale Netze mit der gleichen Größe

817
00:30:31,320 --> 00:30:34,820
und Leistung wie tiefe Neuronale Netze zu trainieren,

818
00:30:34,820 --> 00:30:37,679
ähnlich wie auf der Unterseite ein anderes System,

819
00:30:37,679 --> 00:30:39,539
das deutlich einfacher, aber

820
00:30:39,539 --> 00:30:42,840
vielleicht zu einfach ist. Es handelt sich um ein Netzwerk

821
00:30:42,840 --> 00:30:45,120
gekoppelter Oszillatoren, von denen diese bekannt sind

822
00:30:45,120 --> 00:30:48,779
weisen Synchronität und räumliche zeitliche

823
00:30:48,779 --> 00:30:52,200
Dynamik und komplexe Muster auf, aber

824
00:30:52,200 --> 00:30:53,520
das wird als Phasenreduzierungssystem bezeichnet

825
00:30:53,520 --> 00:30:55,500
und erfasst nicht ganz die

826
00:30:55,500 --> 00:30:57,059
volle Komplexität, an der wir interessiert sind,

827
00:30:57,059 --> 00:30:58,140
also schauen wir uns etwas an, das

828
00:30:58,140 --> 00:31:00,779
möglicherweise zwischen diesen beiden

829
00:31:00,779 --> 00:31:03,600
und dem liegt, was wir tun  In dieser Arbeit geht es

830
00:31:03,600 --> 00:31:06,600
darum, ein

831
00:31:06,600 --> 00:31:08,520
Netzwerk aus mehreren Oszillatoren

832
00:31:08,520 --> 00:31:10,620
etwas flexibler zu parametrisieren als ein Paramoto-

833
00:31:10,620 --> 00:31:12,360
Modell, sodass dies wirklich

834
00:31:12,360 --> 00:31:14,580
auf diesem destillatorischen

835
00:31:14,580 --> 00:31:16,380
wiederkehrenden neuronalen Netzwerk von Constantine

836
00:31:16,380 --> 00:31:18,720
Rush und Nisha

837
00:31:18,720 --> 00:31:20,760
ähm basiert, bei dem sie im Grunde die

838
00:31:20,760 --> 00:31:22,200
Gleichung übernommen haben  beschreibt einen einfachen

839
00:31:22,200 --> 00:31:23,820
harmonischen Oszillator. Es handelt sich um eine Differentialgleichung zweiter Ordnung.

840
00:31:23,820 --> 00:31:26,159
Die Beschleunigung

841
00:31:26,159 --> 00:31:29,940
einer Kugel auf einer Feder ist proportional zu

842
00:31:29,940 --> 00:31:32,480
ihrer Verschiebung.

843
00:31:32,480 --> 00:31:35,220
Sie können zusätzliche Begriffe wie

844
00:31:35,220 --> 00:31:37,260
Dämpfung hinzufügen, sodass die Schwingungen

845
00:31:37,260 --> 00:31:39,360
mit der Zeit langsam abklingen.

846
00:31:39,360 --> 00:31:41,580
Sie können diesen Oszillator mit einem

847
00:31:41,580 --> 00:31:43,380
externen Gerät ansteuern  Eingabe, um

848
00:31:43,380 --> 00:31:45,179
dieser Dämpfung entgegenzuwirken oder der Dynamik etwas mehr Komplexität zu verleihen,

849
00:31:45,179 --> 00:31:47,279


850
00:31:47,279 --> 00:31:49,260
und wenn Sie dann viele

851
00:31:49,260 --> 00:31:50,940
dieser Oszillatoren haben, können Sie sie außerdem

852
00:31:50,940 --> 00:31:53,000
mit diesen Kopplungsmatrizen koppeln,

853
00:31:53,000 --> 00:31:55,320
wie wir es in diesem

854
00:31:55,320 --> 00:31:56,640
Bild hier demonstrieren, damit Sie das wirklich können  Stellen Sie sich

855
00:31:56,640 --> 00:31:58,140
dieses Netzwerk als eine Ansammlung von Blasen auf

856
00:31:58,140 --> 00:31:59,940
Federn vor, die möglicherweise

857
00:31:59,940 --> 00:32:01,740
auch durch Federn oder elastische

858
00:32:01,740 --> 00:32:03,600
Bänder miteinander verbunden sind, was auch immer das destillatorische

859
00:32:03,600 --> 00:32:05,279
wiederkehrende neuronale Netzwerk der russischen

860
00:32:05,279 --> 00:32:08,100
Mishra mit diesen verschiedenen Begriffen ist, und

861
00:32:08,100 --> 00:32:09,899
es hat sich als sehr mächtig erwiesen

862
00:32:09,899 --> 00:32:12,480
Bei der Modellierung langer Sequenzen erwähnten sie auch, dass

863
00:32:12,480 --> 00:32:13,740
sie sich vom

864
00:32:13,740 --> 00:32:15,360
Gehirn inspirieren ließen, das dies baute, und es gibt viele

865
00:32:15,360 --> 00:32:17,700
gute Analysen in diesem Artikel.

866
00:32:17,700 --> 00:32:19,020
Sie zeigen zum Beispiel, dass dies wirklich

867
00:32:19,020 --> 00:32:21,440
vorteilhafte Eigenschaften im Hinblick auf

868
00:32:21,440 --> 00:32:23,460
Probleme mit dem verschwindenden Gradienten sind, die

869
00:32:23,460 --> 00:32:25,440
typischerweise in wiederkehrenden neuronalen Netzen auftreten,

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
ähm  Aber wenn wir uns die räumliche

872
00:32:29,159 --> 00:32:30,960
zeitliche Dynamik und diese Art von

873
00:32:30,960 --> 00:32:32,820
Modell ansehen wollen, dann ist das eine kleine Herausforderung,

874
00:32:32,820 --> 00:32:34,919
denn diese Kopplungsmatrizen hier, die

875
00:32:34,919 --> 00:32:36,320
Ws,

876
00:32:36,320 --> 00:32:39,600
die jeden Neuronen oder jeden

877
00:32:39,600 --> 00:32:41,240
Oszillator miteinander verbinden,

878
00:32:41,240 --> 00:32:43,620
sind dicht verbundene Matrizen,

879
00:32:43,620 --> 00:32:45,120
wie ich es versucht habe  Stellen Sie es hier links dar.

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
Wenn Sie also versuchen, die Dynamik

882
00:32:48,299 --> 00:32:50,580
dieses Netzwerks zu visualisieren, sehen Sie keine

883
00:32:50,580 --> 00:32:51,899
räumliche Organisation, es gibt keine

884
00:32:51,899 --> 00:32:55,380
Vererbung, es ist eine Entschuldigung für den latenten

885
00:32:55,380 --> 00:32:57,000
Raum dieses Modells,

886
00:32:57,000 --> 00:32:58,799
also können Sie sich das wie in unserem

887
00:32:58,799 --> 00:33:01,020
vorherigen Beispiel ein Neuron vorstellen  ist

888
00:33:01,020 --> 00:33:03,240
mit einem potenziell willkürlichen Satz anderer

889
00:33:03,240 --> 00:33:04,919
Neuronen verbunden. Diese Neuronen sind mit einem

890
00:33:04,919 --> 00:33:06,600
anderen willkürlichen Satz von Neuronen verbunden, und

891
00:33:06,600 --> 00:33:08,520
Sie werden sicherlich nur oszillierende Dynamiken erhalten,

892
00:33:08,520 --> 00:33:10,860
aber Schwankungen, die

893
00:33:10,860 --> 00:33:13,260
keinen großen strukturierten Sinn ergeben,

894
00:33:13,260 --> 00:33:15,360
also dachten wir in unserer Arbeit  Okay, wie können

895
00:33:15,360 --> 00:33:18,299
wir das besser auf die Arten von

896
00:33:18,299 --> 00:33:19,860
Dynamiken übertragen, die uns an dieser

897
00:33:19,860 --> 00:33:22,140
strukturierten Ausbreitung von Aktivität interessieren,

898
00:33:22,140 --> 00:33:23,940
und ein klarer Weg, dies zu tun, ist

899
00:33:23,940 --> 00:33:26,539
eine strukturiertere Konnektivitätsmatrix,

900
00:33:26,539 --> 00:33:29,880
die unserer Meinung nach einfach

901
00:33:29,880 --> 00:33:31,559
und effizient zu implementieren ist  Wird durch eine

902
00:33:31,559 --> 00:33:33,000
Faltungsoperation implementiert, die Sie sich

903
00:33:33,000 --> 00:33:34,620
wie eine lokale, lokal

904
00:33:34,620 --> 00:33:36,299
verbundene Schicht vorstellen können. Anstatt also

905
00:33:36,299 --> 00:33:37,860
jedes Neuron zu verbinden,

906
00:33:37,860 --> 00:33:39,480
sind alle Neuronen einfach mit ihren

907
00:33:39,480 --> 00:33:41,580
benachbarten Nachbarn verbunden. Nach dem Training

908
00:33:41,580 --> 00:33:42,840
erhalten Sie am Ende etwas, das

909
00:33:42,840 --> 00:33:44,880
wie eine glatte räumliche Ebene aussieht  Zeitdynamik

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
Um das

912
00:33:48,419 --> 00:33:50,519
Training dieses Modells etwas klarer zu machen, nehmen wir diese separate

913
00:33:50,519 --> 00:33:52,200
Differentialgleichung zweiter Ordnung, die

914
00:33:52,200 --> 00:33:54,299
wir beschrieben haben, bevor Sie sie

915
00:33:54,299 --> 00:33:56,340
in zwei Gleichungen erster Ordnung diskretisieren. Sie

916
00:33:56,340 --> 00:33:57,960
können sich das als eine numerische

917
00:33:57,960 --> 00:34:01,200
Integration der Ode vorstellen, die wir jetzt haben

918
00:34:01,200 --> 00:34:03,120
Geschwindigkeit und dann aktualisieren wir

919
00:34:03,120 --> 00:34:06,000
ähm und und wir können dieses Modell als so

920
00:34:06,000 --> 00:34:07,620
etwas wie einen Auto-Encoder oder ein

921
00:34:07,620 --> 00:34:09,839
auto-regressives Modell trainieren. Wenn wir also eine

922
00:34:09,839 --> 00:34:11,460
Eingabe nehmen, kodieren wir sie in unseren latenten Raum.

923
00:34:11,460 --> 00:34:14,339
Die Eingabe ist eigentlich Dr. ist dieser f-von-x-

924
00:34:14,339 --> 00:34:16,500
Term, der wirkt  als treibender Begriff, also

925
00:34:16,500 --> 00:34:18,599
ist es so, als würde man diese Oszillatoren von

926
00:34:18,599 --> 00:34:20,879
unten antreiben, äh, und dann haben sie ihre

927
00:34:20,879 --> 00:34:23,099
eigene Dynamik, die durch die

928
00:34:23,099 --> 00:34:25,800
Kopplungsterme dieser lokalen Kopplungen definiert wird, und

929
00:34:25,800 --> 00:34:27,540
dann nehmen wir bei jedem Zeitschritt diesen

930
00:34:27,540 --> 00:34:29,460
latenten Zustand, diesen Wellenzustand, und

931
00:34:29,460 --> 00:34:31,560
dekodieren ihn, um ihn zu versuchen  und rekonstruieren Sie die Eingabe

932
00:34:31,560 --> 00:34:33,540
und befinden Sie sich im aktuellen Zeitschritt oder einem

933
00:34:33,540 --> 00:34:35,699
zukünftigen Zeitschritt.

934
00:34:35,699 --> 00:34:37,980
Wir können einige Analysen dieser

935
00:34:37,980 --> 00:34:42,300
Modelle während des Trainings durchführen, um zu sehen, was

936
00:34:42,300 --> 00:34:43,619
vor dem Training passiert, und nach dem

937
00:34:43,619 --> 00:34:45,780
Training können wir die Phase und

938
00:34:45,780 --> 00:34:47,399
die Geschwindigkeit der Dynamik in berechnen  Im

939
00:34:47,399 --> 00:34:49,379
latenten Raum sehen wir im Grunde zu

940
00:34:49,379 --> 00:34:51,480
Beginn des Handels, dass es in unserem Modell keine Wellen gibt,

941
00:34:51,480 --> 00:34:53,699
aber nach dem Training nach 50

942
00:34:53,699 --> 00:34:55,500
Epochen sehen wir, dass es eine glatte,

943
00:34:55,500 --> 00:34:57,119
strukturierte Aktivität gibt, die sich

944
00:34:57,119 --> 00:35:00,420
nach unten ausbreitet, ähm, im Dienste dieser

945
00:35:00,420 --> 00:35:01,800
Sequenzmodellierungsaufgabe, die wir wie

946
00:35:01,800 --> 00:35:04,380
rotierende Objekte ausführen,

947
00:35:04,380 --> 00:35:05,940
ähm  Was ist der Nutzen davon?

948
00:35:05,940 --> 00:35:07,680
Ich meine, der ganze Grund, warum ich dies motiviert habe,

949
00:35:07,680 --> 00:35:10,380
war, zu sagen, dass wir eine flexiblere erlernte Struktur haben wollten.

950
00:35:10,380 --> 00:35:11,880


951
00:35:11,880 --> 00:35:13,020
Tun wir das tatsächlich, oder bekommen wir nur

952
00:35:13,020 --> 00:35:15,060
ziemliche Wellen,

953
00:35:15,060 --> 00:35:16,859
ähm, was wir in unserer Arbeit gezeigt haben, ist,

954
00:35:16,859 --> 00:35:19,320
dass wir es wirklich sind  Das Erlernen einer

955
00:35:19,320 --> 00:35:20,940
nützlichen Struktur und die Art und Weise, wie wir das gezeigt haben,

956
00:35:20,940 --> 00:35:22,440
ist wiederum mit so etwas wie diesem

957
00:35:22,440 --> 00:35:24,960
kommutativen Diagramm, wenn man eine Eingabe nimmt

958
00:35:24,960 --> 00:35:27,000
und sie kodiert und einen Wellenzustand erhält

959
00:35:27,000 --> 00:35:29,280
und dann Wellen

960
00:35:29,280 --> 00:35:31,619
künstlich in diesem Wellenzustand ausbreitet und

961
00:35:31,619 --> 00:35:33,480
sie dann dekodiert  Beachten Sie, dass es

962
00:35:33,480 --> 00:35:35,220
tatsächlich genau das Gleiche ist, als ob Sie

963
00:35:35,220 --> 00:35:37,140
nur eine Menge verschiedener

964
00:35:37,140 --> 00:35:39,180
Bilder unterschiedlicher Transformationen zeigen würden, also

965
00:35:39,180 --> 00:35:41,640
viele verschiedene Ziffern, unterschiedliche

966
00:35:41,640 --> 00:35:43,920
Merkmale, und wir sehen, dass wir

967
00:35:43,920 --> 00:35:46,140
in jedem Fall unterschiedliche Arten von Wellenaktivität erhalten,

968
00:35:46,140 --> 00:35:47,880
um das zu modellieren  unterschiedliche

969
00:35:47,880 --> 00:35:49,140
Transformation,

970
00:35:49,140 --> 00:35:51,119
wenn wir sie auch auf verschiedenen Datensätzen trainieren,

971
00:35:51,119 --> 00:35:53,400
sehen wir in ähnlicher Weise komplexere

972
00:35:53,400 --> 00:35:55,200
Dynamiken, in diesem Fall vielleicht nicht einmal

973
00:35:55,200 --> 00:35:57,839
Wanderwellen oder stehende Wellen, die man sich

974
00:35:57,839 --> 00:36:00,359
als Wanderwellen in

975
00:36:00,359 --> 00:36:02,339
entgegengesetzte Richtungen vorstellen kann, also sehen wir, ob wir

976
00:36:02,339 --> 00:36:04,079
diese Orbitale modellieren  Dynamik: Wir erhalten

977
00:36:04,079 --> 00:36:06,000
diese Art von sich sanft bewegenden

978
00:36:06,000 --> 00:36:07,619
Aktivitätsflecken in unserem latenten Raum. Wenn wir

979
00:36:07,619 --> 00:36:09,839
ein Pendel modellieren, erhalten wir in ähnlicher Weise eine

980
00:36:09,839 --> 00:36:13,820
Art komplexe oszillierende Aktivität,

981
00:36:14,099 --> 00:36:17,339
so dass die Eingabestruktur erhalten bleibt, aber

982
00:36:17,339 --> 00:36:19,560
zusätzlich mehr Flexibilität als

983
00:36:19,560 --> 00:36:21,599
zuvor, was sozusagen unser ultimatives Ziel ist

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
Abschließend möchte ich noch ein wenig darüber sprechen,

986
00:36:26,099 --> 00:36:28,320
wie ich denke, dass das Ergebnis dieser Forschung

987
00:36:28,320 --> 00:36:30,420
nicht nur die künstliche Intelligenz verbessern kann,

988
00:36:30,420 --> 00:36:32,099
sondern auch, wie sie uns hilft zu

989
00:36:32,099 --> 00:36:34,440
verstehen, warum unsere Messungen des

990
00:36:34,440 --> 00:36:36,240
Gehirns so aussehen, wie sie aussehen. Um ein

991
00:36:36,240 --> 00:36:38,579
kurzes Beispiel dafür zu geben, was ich getan habe  Damit meine

992
00:36:38,579 --> 00:36:41,040
ich, dass ich zuvor ein wenig über Visa

993
00:36:41,040 --> 00:36:43,740
und Orte gesprochen habe. In dieser fantastischen Arbeit

994
00:36:43,740 --> 00:36:46,859
mit Ching Higao haben wir untersucht, ob unser

995
00:36:46,859 --> 00:36:48,900
einfacher topografischer Prior, wie wir ihn besprochen haben, in der

996
00:36:48,900 --> 00:36:50,579
Lage sein könnte, dieselben Effekte zu reproduzieren.

997
00:36:50,579 --> 00:36:53,339
Daher legen wir insbesondere den Wert

998
00:36:53,339 --> 00:36:56,099
dieses Cohens D  Selektivitätsmetrik für

999
00:36:56,099 --> 00:36:58,200
jedes unserer Neuronen in Bezug auf einen

1000
00:36:58,200 --> 00:37:00,000
anderen Datensatz von Bildern, der möglicherweise

1001
00:37:00,000 --> 00:37:02,460
nur Gesichter oder nur Objekte oder Körper enthält,

1002
00:37:02,460 --> 00:37:03,359


1003
00:37:03,359 --> 00:37:05,880
und daher messen wir für jedes Neuron, ob es

1004
00:37:05,880 --> 00:37:07,920
eher auf Gesichter reagiert oder ob das

1005
00:37:07,920 --> 00:37:10,380
Russische im Gehirn auftaucht, aber ich tue es  Ich

1006
00:37:10,380 --> 00:37:12,839
denke, dass es uns sagt, dass die relative

1007
00:37:12,839 --> 00:37:15,300
Organisation der Selektivität zumindest

1008
00:37:15,300 --> 00:37:17,400
teilweise auf

1009
00:37:17,400 --> 00:37:19,800
Korrelationsstatistiken in den Daten zurückzuführen sein kann,

1010
00:37:19,800 --> 00:37:21,359
nachdem sie durch einen hochgradig

1011
00:37:21,359 --> 00:37:23,640
nichtlinearen Zukunftsextraktor wie ein

1012
00:37:23,640 --> 00:37:25,440
tiefes neuronales Netzwerk geleitet wurden,

1013
00:37:25,440 --> 00:37:27,480
also in ähnlicher Weise etwas, das ist

1014
00:37:27,480 --> 00:37:29,040
Interessanterweise gibt es einen bekannten

1015
00:37:29,040 --> 00:37:30,900
sogenannten dreiteiligen oder nicht-visuellen

1016
00:37:30,900 --> 00:37:36,720
Strom, also äh, Bilder von äh oder Objekten.

1017
00:37:36,720 --> 00:37:38,400
Die Selektivität in Bezug auf Objekte

1018
00:37:38,400 --> 00:37:40,680
wird durch abstraktere Eigenschaften organisiert,

1019
00:37:40,680 --> 00:37:43,200
wie etwa Belebtheit, ist dieses Ding lebendig oder

1020
00:37:43,200 --> 00:37:46,200
unbelebt, äh, im Vergleich zur realen

1021
00:37:46,200 --> 00:37:48,480
Objektgröße wie was  ist etwa so groß wie eine

1022
00:37:48,480 --> 00:37:50,700
Teekanne im Vergleich zu einem Auto,

1023
00:37:50,700 --> 00:37:53,520
und wir sehen, dass beim

1024
00:37:53,520 --> 00:37:56,160
Menschen die Selektivität

1025
00:37:56,160 --> 00:37:57,540
in dieser dreiteiligen Struktur organisiert ist.

1026
00:37:57,540 --> 00:37:59,760
Typischerweise gibt es kleine Objekte, die hinsichtlich ihrer Selektivität

1027
00:37:59,760 --> 00:38:01,920
zwischen belebten und unbelebten Objekten liegen,

1028
00:38:01,920 --> 00:38:04,200
und wir sehen

1029
00:38:04,200 --> 00:38:06,060
Das Gleiche passiert hier, also

1030
00:38:06,060 --> 00:38:07,440
messen sie die Selektivität

1031
00:38:07,440 --> 00:38:08,880
desselben Satzes von Neuronen, aber in Bezug

1032
00:38:08,880 --> 00:38:10,859
auf diese Reizunterschiede sehen Sie,

1033
00:38:10,859 --> 00:38:12,440
dass der kleine Cluster zwischen einem

1034
00:38:12,440 --> 00:38:14,820
belebten und einem unbelebten Cluster liegt, und

1035
00:38:14,820 --> 00:38:16,079
dies geschieht wiederum bei mehreren unterschiedlichen

1036
00:38:16,079 --> 00:38:18,900
Initialisierungen, also hier  Ich

1037
00:38:18,900 --> 00:38:20,880
hoffe, wir können dies für diese Community etwas weiter erforschen.

1038
00:38:20,880 --> 00:38:22,980
Ich finde es interessant,

1039
00:38:22,980 --> 00:38:24,119
weil es

1040
00:38:24,119 --> 00:38:26,220
wirklich eine Möglichkeit ist zu zeigen, dass wir

1041
00:38:26,220 --> 00:38:28,200
ein strukturiertes Weltmodell erstellt haben und dass

1042
00:38:28,200 --> 00:38:30,119
dieses Weltmodell möglicherweise

1043
00:38:30,119 --> 00:38:31,980
nützlich ist, um

1044
00:38:31,980 --> 00:38:34,740
Daten aus der realen Welt besser auf

1045
00:38:34,740 --> 00:38:37,619
strukturierte Weise darzustellen  In diesem Sinne erhält man eine geringere freie

1046
00:38:37,619 --> 00:38:39,119
Energie,

1047
00:38:39,119 --> 00:38:40,619
also

1048
00:38:40,619 --> 00:38:42,300
denke ich, dass wir durch die Entwicklung dieser Modelle, wie

1049
00:38:42,300 --> 00:38:44,400
wir sie hier gezeigt haben,

1050
00:38:44,400 --> 00:38:46,500
Einblicke in neue Mechanismen erhalten können, wie

1051
00:38:46,500 --> 00:38:48,900
diese Struktur entsteht, einschließlich der

1052
00:38:48,900 --> 00:38:50,460
topografischen Organisation, an die wir noch nie

1053
00:38:50,460 --> 00:38:52,920
zuvor gedacht haben, also Maschinenmodell, das ich mir

1054
00:38:52,920 --> 00:38:55,520
angesehen habe  Die Orientierungsselektivität

1055
00:38:55,520 --> 00:38:58,260
von Neuronen, von der ich

1056
00:38:58,260 --> 00:39:01,020
eigentlich nicht erwartet hatte, dass etwas

1057
00:39:01,020 --> 00:39:03,420
passiert, aber Sie sehen, wie sich

1058
00:39:03,420 --> 00:39:05,339
diese Wellen über diese

1059
00:39:05,339 --> 00:39:08,099
simulierte vertikale Oberfläche ausbreiten, und ich dachte,

1060
00:39:08,099 --> 00:39:09,960
okay, vielleicht zeige ich gedrehte Bilder,

1061
00:39:09,960 --> 00:39:11,820
vielleicht hat das einen Einfluss darauf  die

1062
00:39:11,820 --> 00:39:13,740
Orientierungsselektivität,

1063
00:39:13,740 --> 00:39:15,599
und tatsächlich, wenn man hineingeht und

1064
00:39:15,599 --> 00:39:17,460
die Selektivität jedes Neurons

1065
00:39:17,460 --> 00:39:18,660
in Bezug auf diese unterschiedlich

1066
00:39:18,660 --> 00:39:22,079
ausgerichteten Linien misst, sieht man, dass es

1067
00:39:22,079 --> 00:39:24,300
überraschenderweise an die Orient-

1068
00:39:24,300 --> 00:39:25,859
Typ-Säulen erinnert, die im primären

1069
00:39:25,859 --> 00:39:27,599
visuellen Kortex zu sehen sind, das ist etwas, das zurückgeht

1070
00:39:27,599 --> 00:39:29,520
zu Hugo und Weasel, und das ist etwas,

1071
00:39:29,520 --> 00:39:30,900
das irgendwie aus diesem Modell

1072
00:39:30,900 --> 00:39:33,060
und der Tatsache hervorgegangen ist, dass es die räumliche und

1073
00:39:33,060 --> 00:39:34,440
zeitliche Struktur in Bezug auf

1074
00:39:34,440 --> 00:39:37,619
Transformationen hat, also ist das natürlich

1075
00:39:37,619 --> 00:39:39,599
eine wirklich grobe Analogie, aber ich denke, das

1076
00:39:39,599 --> 00:39:40,740
ist ein Beispiel dafür  Der Aufbau dieser

1077
00:39:40,740 --> 00:39:42,839
Art von Modellen kann uns dabei helfen, darüber nachzudenken,

1078
00:39:42,839 --> 00:39:45,240
wie das Gehirn

1079
00:39:45,240 --> 00:39:46,980
Repräsentationsstrukturen und die Art und Weise, wie sie

1080
00:39:46,980 --> 00:39:48,660
organisiert sind, auf eine Weise aufbaut, über die wir vielleicht noch nie nachgedacht haben.

1081
00:39:48,660 --> 00:39:51,300


1082
00:39:51,300 --> 00:39:53,460
Ähm, ich glaube, ich bin nicht der Einzige, der

1083
00:39:53,460 --> 00:39:55,859
so etwas macht  Arbeit und deshalb

1084
00:39:55,859 --> 00:39:57,240
möchte ich ein wenig über einige

1085
00:39:57,240 --> 00:39:59,579
andere Leute sprechen, die das tun.

1086
00:39:59,579 --> 00:40:00,780
Ich habe also über diese

1087
00:40:00,780 --> 00:40:02,760
äquivalente Struktur gesprochen.

1088
00:40:02,760 --> 00:40:04,920
Leute wie James Whittington und

1089
00:40:04,920 --> 00:40:08,880
Tim Barons und Surrogengoolie haben

1090
00:40:08,880 --> 00:40:10,680
das kürzlich durch die Einführung der

1091
00:40:10,680 --> 00:40:14,940
Algebra gezeigt  Einschränkungen in einen

1092
00:40:14,940 --> 00:40:17,040
Lernprozess einbinden, in diesem Fall war es

1093
00:40:17,040 --> 00:40:20,820
wie die Bewegung von äh und und Agenten in

1094
00:40:20,820 --> 00:40:23,220
einer Umgebung, indem man sagt, man muss eine

1095
00:40:23,220 --> 00:40:24,780
Art dieser algebraischen

1096
00:40:24,780 --> 00:40:27,540
Struktur beibehalten, wenn ich mich in einem Kreis West-

1097
00:40:27,540 --> 00:40:29,280
Nordost-Süd bewege, lande ich wieder am

1098
00:40:29,280 --> 00:40:31,440
selben Ort  Punkt Noch einmal: Durch die Einführung dieser

1099
00:40:31,440 --> 00:40:32,820
Arten von Einschränkungen

1100
00:40:32,820 --> 00:40:35,040
entstehen gitterzellenartige

1101
00:40:35,040 --> 00:40:36,900
Darstellungen,

1102
00:40:36,900 --> 00:40:39,359
ähm. Ich würde also gerne sehen, wie uns diese

1103
00:40:39,359 --> 00:40:41,880
Idee der Darstellungsstruktur dabei

1104
00:40:41,880 --> 00:40:43,980
helfen kann, vielleicht mehr zu erklären als unsere

1105
00:40:43,980 --> 00:40:45,480
wissenschaftlichen Erkenntnisse, die wir ebenfalls finden, ähm

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
und  und wie das mit

1108
00:40:48,359 --> 00:40:51,540
generativen Modellen als

1109
00:40:51,540 --> 00:40:52,800
Ganzes zusammenhängt, ähm, und schließlich denke ich, dass es auch

1110
00:40:52,800 --> 00:40:54,599
etwas über die kognitiven

1111
00:40:54,599 --> 00:40:56,099
Möglichkeiten dieser Modelle zu sagen gibt.

1112
00:40:56,099 --> 00:40:57,420
Vielleicht werden wir sie nicht nur

1113
00:40:57,420 --> 00:40:59,579
aus der Perspektive der Neurowissenschaften testen,

1114
00:40:59,579 --> 00:41:01,020
sondern auch aus der

1115
00:41:01,020 --> 00:41:03,839
Perspektive der Mikrokognitionswissenschaft  Zum Beispiel gibt es diese

1116
00:41:03,839 --> 00:41:06,000
progressiven Ravens-Matrizen auf der linken Seite,

1117
00:41:06,000 --> 00:41:08,640
in denen Sie sagen müssen, welches

1118
00:41:08,640 --> 00:41:11,099
dieser Bilder eher in

1119
00:41:11,099 --> 00:41:12,599
dieses Muster passt,

1120
00:41:12,599 --> 00:41:14,760
oder zum Beispiel: Wie wahrscheinlich ist es, dass

1121
00:41:14,760 --> 00:41:16,740
dieser Jenga-Turm umfällt, wenn Sie

1122
00:41:16,740 --> 00:41:19,740
über einen Zug fahren  ein bestimmter Block oder oder

1123
00:41:19,740 --> 00:41:22,740
mit einer bestimmten Struktur und ich denke,

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
diese Art von Tests testen wirklich,

1126
00:41:26,640 --> 00:41:28,619
ob unsere Weltmodelle, die wir bauen, den

1127
00:41:28,619 --> 00:41:31,560
Modelltypen ähneln, die

1128
00:41:31,560 --> 00:41:33,660
wir als Menschen von Natur aus mit unserem eigenen gesunden Menschenverstand haben

1129
00:41:33,660 --> 00:41:36,060
oder  Als Wesen, die in einer

1130
00:41:36,060 --> 00:41:38,400
natürlichen Welt leben, und ich habe einige

1131
00:41:38,400 --> 00:41:40,440
Vorarbeiten in dieser Richtung durchgeführt,

1132
00:41:40,440 --> 00:41:43,079
denke ich, sehr vorläufig und nicht annähernd so

1133
00:41:43,079 --> 00:41:45,480
kompliziert, aber ähm, ich versuche,

1134
00:41:45,480 --> 00:41:47,820
visuelle Illusionen zu modellieren, wenn Sie also

1135
00:41:47,820 --> 00:41:50,520
einen wirklich einfachen Datensatz eines sich bewegenden Balkens nehmen

1136
00:41:50,520 --> 00:41:52,980
Reize oder einen statischen Balken oder Rahmen und wenn Sie

1137
00:41:52,980 --> 00:41:54,960
ihn ein wenig bewegen, können Sie sehen, dass

1138
00:41:54,960 --> 00:41:57,060
das Modell tatsächlich auf den

1139
00:41:57,060 --> 00:41:58,800
fehlenden Rahmen und dann tatsächlich auch auf eine

1140
00:41:58,800 --> 00:42:01,079
fortgesetzte Bewegung schließt, sodass es so ist, als würde man über

1141
00:42:01,079 --> 00:42:03,300
die Flugbahn dessen hinausschießen, was die

1142
00:42:03,300 --> 00:42:05,820
tatsächlichen Reize liefern, bevor es

1143
00:42:05,820 --> 00:42:08,760
erneut korrigiert  Daher denke ich, dass die Modellierung von

1144
00:42:08,760 --> 00:42:10,320
Illusionen sicherlich eine interessante

1145
00:42:10,320 --> 00:42:12,660
Möglichkeit ist, zu untersuchen, ob unsere Weltmodelle

1146
00:42:12,660 --> 00:42:14,760
den Arten von Modellen ähneln, die wir

1147
00:42:14,760 --> 00:42:16,619
selbst haben.

1148
00:42:16,619 --> 00:42:19,619
Abschließend denke ich, dass

1149
00:42:19,619 --> 00:42:21,900
wir mit topografischen Prioritäten zeigen können, dass

1150
00:42:21,900 --> 00:42:23,220
sie strukturierte

1151
00:42:23,220 --> 00:42:24,839
Darstellungen oder strukturierte

1152
00:42:24,839 --> 00:42:26,700
Weltmodelle effektiv gelernt haben  Die erlernte Struktur ist

1153
00:42:26,700 --> 00:42:29,160
flexibel und an beliebige

1154
00:42:29,160 --> 00:42:30,780
Transformationen anpassbar. Im Gegensatz zu herkömmlichen

1155
00:42:30,780 --> 00:42:33,720
Äquivarianten und topografischen Anbietern

1156
00:42:33,720 --> 00:42:35,579
können sie statistisch induziert werden, wie wir es

1157
00:42:35,579 --> 00:42:37,619
in der topografischen Region getan haben, oder durch

1158
00:42:37,619 --> 00:42:39,480
Dynamik, wie wir es in diesen

1159
00:42:39,480 --> 00:42:42,000
Modellen vom Typ „Neuronale Wellenmaschine“ gezeigt haben.

1160
00:42:42,000 --> 00:42:44,460
Abschließend überlasse ich Ihnen dies

1161
00:42:44,460 --> 00:42:46,980
Ein Zitat, das ich in Fukushimas Artikel

1162
00:42:46,980 --> 00:42:50,280
aus dem Jahr 1980 gefunden habe, war meiner Meinung

1163
00:42:50,280 --> 00:42:52,079
nach seiner Zeit ziemlich weit voraus. Er sagt, wenn wir

1164
00:42:52,079 --> 00:42:53,520
ein neuronales Netzwerkmodell erstellen könnten, das

1165
00:42:53,520 --> 00:42:55,020
die gleiche Fähigkeit zur

1166
00:42:55,020 --> 00:42:57,060
Mustererkennung wie ein Mensch hätte, würde es

1167
00:42:57,060 --> 00:42:58,800
uns im Vergleich dazu einen aussagekräftigen Hinweis geben  Das

1168
00:42:58,800 --> 00:43:00,000
Verstehen des neuronalen Mechanismus

1169
00:43:00,000 --> 00:43:03,240
im Gehirn ist meiner Meinung nach einige der

1170
00:43:03,240 --> 00:43:06,119
Ziele, die wir hier anstreben.

1171
00:43:06,119 --> 00:43:08,220
Ich denke, es liegt an meinem Berater Max, meinen

1172
00:43:08,220 --> 00:43:11,280
Co-Autoren Patrick UA Emil Jinghian und

1173
00:43:11,280 --> 00:43:17,359
Yorn und ist an einer Diskussion interessiert. Vielen Dank,

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
alles klar  Vielen Dank, tolle, sehr

1176
00:43:27,480 --> 00:43:31,079
interessante Präsentation,

1177
00:43:31,079 --> 00:43:33,480
viele Ansatzpunkte, vielleicht nur äh, was

1178
00:43:33,480 --> 00:43:36,000
Sie zu dieser Arbeit gebracht hat,

1179
00:43:36,000 --> 00:43:38,520
ein kleiner Kontext darüber, wie Sie zu

1180
00:43:38,520 --> 00:43:43,819
dieser Arbeit für Ihre Doktorarbeit gekommen sind. Ja,

1181
00:43:43,920 --> 00:43:45,119


1182
00:43:45,119 --> 00:43:46,020
ähm,

1183
00:43:46,020 --> 00:43:49,200
ich meine, wir haben das studiert, nicht meine

1184
00:43:49,200 --> 00:43:51,000
Gruppe  Ich bin an der Universität und

1185
00:43:51,000 --> 00:43:52,700
beschäftige mich schon

1186
00:43:52,700 --> 00:43:56,640
seit einiger Zeit mit strukturierten Darstellungen aus mathematischer Sicht,

1187
00:43:56,640 --> 00:43:58,319
wobei einige Leute

1188
00:43:58,319 --> 00:44:00,240
Modelle haben, für die sie gedacht sind, wie zum Beispiel den Variations-

1189
00:44:00,240 --> 00:44:01,740
Auto-Encoder,

1190
00:44:01,740 --> 00:44:04,680
und

1191
00:44:04,680 --> 00:44:06,960
ich schätze, was das schon immer war,

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
ist ein Modell  Das respektiert Rotationen und 2D-

1194
00:44:11,220 --> 00:44:13,560
Rotationen perfekt, aber wenn wir

1195
00:44:13,560 --> 00:44:15,960
3D-Rotationen machen wollen, können wir das nicht machen,

1196
00:44:15,960 --> 00:44:17,819
weil das keine Gruppe im Sinne einer

1197
00:44:17,819 --> 00:44:19,740
Projektion auf einen 2D-Plan ist. Es gehen

1198
00:44:19,740 --> 00:44:21,180
Informationen verloren, wenn sich dieses Ding

1199
00:44:21,180 --> 00:44:23,460
zum Beispiel um

1200
00:44:23,460 --> 00:44:24,240
ähm

1201
00:44:24,240 --> 00:44:26,280
oder dreht  einfach jede Art von natürlichen

1202
00:44:26,280 --> 00:44:27,960
Transformationen, auf die ich

1203
00:44:27,960 --> 00:44:29,339
am Anfang hinweisen wollte. Ich glaube, ich

1204
00:44:29,339 --> 00:44:30,180
habe

1205
00:44:30,180 --> 00:44:31,740
versucht, darüber nachzudenken, wie das Gehirn

1206
00:44:31,740 --> 00:44:34,020
natürliche Transformationen modelliert. In

1207
00:44:34,020 --> 00:44:35,400
diesen aktuellen

1208
00:44:35,400 --> 00:44:37,200
Frameworks

1209
00:44:37,200 --> 00:44:41,099
spielt Aktion

1210
00:44:41,099 --> 00:44:44,579
in Bezug auf den Variations-Autoencoder eine Rolle

1211
00:44:44,579 --> 00:44:48,420
Modelle, die nicht nur

1212
00:44:48,420 --> 00:44:50,520
äußere Muster, sondern auch die

1213
00:44:50,520 --> 00:44:52,380
Konsequenzen von Handlungen oder die

1214
00:44:52,380 --> 00:44:55,800
strukturelle Struktur des Weltmodells mit Handlungen umfassen, richtig ja,

1215
00:44:55,800 --> 00:44:58,619
nein, das ist eine gute Frage und

1216
00:44:58,619 --> 00:45:01,319
ich denke, gehandelte Schlussfolgerungen

1217
00:45:01,319 --> 00:45:03,839
sind im Grunde die Antwort. Ich denke,

1218
00:45:03,839 --> 00:45:05,940
es ist eine gute Antwort darauf, ähm,

1219
00:45:05,940 --> 00:45:09,000
ich weiß da  Es handelt sich um

1220
00:45:09,000 --> 00:45:11,099
Frameworks für verstärkendes Lernen, die

1221
00:45:11,099 --> 00:45:12,660


1222
00:45:12,660 --> 00:45:15,060
extern trainierte Weltmodelle verwenden,

1223
00:45:15,060 --> 00:45:17,280
sodass Sie ein Vae oder so etwas trainieren und

1224
00:45:17,280 --> 00:45:19,800
diese Darstellung dann in Ihrem

1225
00:45:19,800 --> 00:45:23,040
System für verstärkendes Lernen verwenden

1226
00:45:23,040 --> 00:45:24,720


1227
00:45:24,720 --> 00:45:26,520


1228
00:45:26,520 --> 00:45:30,780
als Teil der

1229
00:45:30,780 --> 00:45:33,660
Wahrscheinlichkeit der Daten und äh

1230
00:45:33,660 --> 00:45:35,280
ja, ich denke, das ist viel eleganter

1231
00:45:35,280 --> 00:45:38,940
und deshalb bin ich ein großer Befürworter davon, ähm,

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
ich bin noch nicht so weit gekommen, zu untersuchen, wie

1234
00:45:43,140 --> 00:45:45,480
diese strukturierten Weltmodelle in einem vae

1235
00:45:45,480 --> 00:45:47,520
oder ich  Daran hat es überhaupt nicht funktioniert, aber ich

1236
00:45:47,520 --> 00:45:48,780
denke, es wäre auf jeden Fall sehr

1237
00:45:48,780 --> 00:45:50,819
interessant zu sehen, ob ein

1238
00:45:50,819 --> 00:45:52,339
strukturierteres Weltmodell

1239
00:45:52,339 --> 00:45:54,839
in einem Variations-Auto-Encoder

1240
00:45:54,839 --> 00:45:56,880


1241
00:45:56,880 --> 00:45:58,319
auch in einer aktiven Umgebung von Vorteil wäre. Ich denke, das wäre

1242
00:45:58,319 --> 00:46:00,119
großartig, meine ich  Ich denke, dass

1243
00:46:00,119 --> 00:46:03,599
einige dieser Beispiele, wie z. B. das

1244
00:46:03,599 --> 00:46:05,579
zuvor gezeigte, wie die Entstehung von Gitterzellen und

1245
00:46:05,579 --> 00:46:07,500
solche Dinge, vielleicht in

1246
00:46:07,500 --> 00:46:08,880
diese Richtung deuten.

1247
00:46:08,880 --> 00:46:10,560
Okay, vielleicht macht das Gehirn etwas,

1248
00:46:10,560 --> 00:46:12,540
es hat wirklich offensichtlich viel

1249
00:46:12,540 --> 00:46:13,680
Struktur.

1250
00:46:13,680 --> 00:46:15,359
Das muss eindeutig für die

1251
00:46:15,359 --> 00:46:19,140
Ausführung von Aktionen in manchen Fällen nützlich sein

1252
00:46:19,140 --> 00:46:21,720
Oh ja, ich hatte das Gefühl, eine wirklich

1253
00:46:21,720 --> 00:46:24,480
schöne Parallele, die Sie mit

1254
00:46:24,480 --> 00:46:28,040
dem Vortrag angeführt haben, war, dass die lokal verbundenen Einheiten es

1255
00:46:28,040 --> 00:46:30,960
Ihren Modellen ermöglichten,

1256
00:46:30,960 --> 00:46:33,780
die

1257
00:46:33,780 --> 00:46:35,640
Faltungsbeschränkung und das Faltungsmuster strukturell zu verkörpern, und das führte zu

1258
00:46:35,640 --> 00:46:37,500
diesen entstehenden Mustern, und

1259
00:46:37,500 --> 00:46:41,339
analog dazu gab es überhaupt das, ähm, Doral

1260
00:46:41,339 --> 00:46:45,680
Sie hatten die

1261
00:46:45,680 --> 00:46:48,359
richtige Pfaderkundungsbeschränkung und dann ist es

1262
00:46:48,359 --> 00:46:50,280
interessant,

1263
00:46:50,280 --> 00:46:53,760
über diese Aktions- oder

1264
00:46:53,760 --> 00:46:56,819
Richtlinienheuristiken oder Sparsitäten wie eine

1265
00:46:56,819 --> 00:46:59,579
gemeinsame motorische Erkundung nachzudenken. Irgendwann

1266
00:46:59,579 --> 00:47:02,339
wird klar, dass es zwei

1267
00:47:02,339 --> 00:47:04,980
einander entgegengesetzte Möglichkeiten gibt, ein Gelenk zu bewegen,

1268
00:47:04,980 --> 00:47:07,079
und dann die Kompositionalität

1269
00:47:07,079 --> 00:47:09,119
Gelenkübergreifend kann auf diesen

1270
00:47:09,119 --> 00:47:10,680
höheren Ebenen erlernt werden, sobald es auf

1271
00:47:10,680 --> 00:47:14,480
niedrigeren Ebenen verankert ist. Daher ist es eine sehr ansprechende

1272
00:47:14,480 --> 00:47:17,599
und, äh,

1273
00:47:17,599 --> 00:47:20,460
nischenrelevante Art, zu verallgemeinern,

1274
00:47:20,460 --> 00:47:23,819
da es einerseits auf den tatsächlichen

1275
00:47:23,819 --> 00:47:25,740
Zwängen der Welt basiert, andererseits aber vor

1276
00:47:25,740 --> 00:47:27,720
allem durch Maßnahmen, die

1277
00:47:27,720 --> 00:47:29,460
möglicherweise etwas einbetten, das ist

1278
00:47:29,460 --> 00:47:31,380
Ganz einfach, richtig ja,

1279
00:47:31,380 --> 00:47:33,599
nein, ich denke, das stimmt auf jeden Fall. Das

1280
00:47:33,599 --> 00:47:36,599
ist ein wirklich guter Punkt, wenn

1281
00:47:36,599 --> 00:47:38,339
Sie, ähm, Einschränkungen haben, die sich aus Ihren

1282
00:47:38,339 --> 00:47:40,500
Handlungen selbst ergeben, dann

1283
00:47:40,500 --> 00:47:42,839
wäre das äußerst hilfreich, um Ihnen dabei zu helfen,

1284
00:47:42,839 --> 00:47:44,819


1285
00:47:44,819 --> 00:47:47,460
Ihren latenten Raum zu strukturieren, und ich denke, ja, ich

1286
00:47:47,460 --> 00:47:48,480
denke, eines ist so  Ich wollte erwähnen, dass es

1287
00:47:48,480 --> 00:47:49,980


1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
etwas gibt, das mich an Stefano

1290
00:47:52,740 --> 00:47:55,500
Fouseys Arbeit über die

1291
00:47:55,500 --> 00:47:58,859
Darstellungsgeometrie denken lässt. Sie

1292
00:47:58,859 --> 00:48:01,920
bestimmt, wie wir ein

1293
00:48:01,920 --> 00:48:04,440
bestimmtes Verständnis

1294
00:48:04,440 --> 00:48:08,099
eines Systems verallgemeinern können, und ich denke, wenn man

1295
00:48:08,099 --> 00:48:11,880
verstehen kann, wie diese Gruppen von

1296
00:48:11,880 --> 00:48:14,520
Aktivitäten sind  trennbar oder

1297
00:48:14,520 --> 00:48:16,079
hochparallel trennbar mit einem linearen

1298
00:48:16,079 --> 00:48:18,839
Klassifikator im Wesentlichen, dann werden Sie in der

1299
00:48:18,839 --> 00:48:20,700
Lage sein, eine Verallgemeinerung durchzuführen, und ich

1300
00:48:20,700 --> 00:48:23,099
denke, indem Sie diese Art von Verzerrungen auferlegen

1301
00:48:23,099 --> 00:48:25,040
oder möglicherweise durch

1302
00:48:25,040 --> 00:48:27,000
Einschränkungen, die durch eine Aktion wie diese auferlegt werden, geben

1303
00:48:27,000 --> 00:48:28,740


1304
00:48:28,740 --> 00:48:32,040
Sie nach oder induzieren eine Art

1305
00:48:32,040 --> 00:48:33,660
Bessere Darstellungsgeometrie und

1306
00:48:33,660 --> 00:48:35,220
das hat alle möglichen Vorteile für die

1307
00:48:35,220 --> 00:48:36,660
Kompositionalität.

1308
00:48:36,660 --> 00:48:39,359
Ja, unsere Verallgemeinerung,

1309
00:48:39,359 --> 00:48:41,760
es ist also ein toller Punkt. Cool, ja, ein sehr

1310
00:48:41,760 --> 00:48:43,440
interessanter Bereich, alles klar. Ich werde

1311
00:48:43,440 --> 00:48:45,960
einige Fragen aus dem Live-Chat lesen.

1312
00:48:45,960 --> 00:48:48,420
Love Evolve hat

1313
00:48:48,420 --> 00:48:52,260
alle praktischen oder beobachteten Einschränkungen beim

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
Lernen von Modellierungsillusionen geschrieben  Gemeinschaften sind nicht

1316
00:49:00,420 --> 00:49:03,060
phobisch, man hat kein

1317
00:49:03,060 --> 00:49:05,940
Blickzentrum, dann hat man auch keine ähm

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
wie eine Zeit. Ich meine die meisten Faltungs-

1320
00:49:11,460 --> 00:49:13,260
Neuronalen Netze. Ich verwende diese Art von

1321
00:49:13,260 --> 00:49:15,599
wiederkehrenden Neuronalen Netzen, aber die Zeit ist

1322
00:49:15,599 --> 00:49:18,420
nicht so klar definiert  In diesen Modellen handelt

1323
00:49:18,420 --> 00:49:20,220
es sich um eine kontinuierliche Zeitumgebung

1324
00:49:20,220 --> 00:49:23,400
für einen Menschen, der sich einer Illusionsprüfung unterzieht,

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319


1327
00:49:25,319 --> 00:49:27,480
und ich denke, die Kombination dieser beiden hängt

1328
00:49:27,480 --> 00:49:30,359
von der Tatsache ab, dass als Mensch oder die meisten

1329
00:49:30,359 --> 00:49:33,300
Dinge, äh,

1330
00:49:33,300 --> 00:49:35,940
Ihr Blick, Ihre wechselnden Standorte und

1331
00:49:35,940 --> 00:49:38,220
Ihre Gewinne davon abhängen

1332
00:49:38,220 --> 00:49:40,140


1333
00:49:40,140 --> 00:49:42,780
Ich denke, es

1334
00:49:42,780 --> 00:49:46,560
wäre wirklich hilfreich, wenn wir Modelle hätten, die

1335
00:49:46,560 --> 00:49:48,540
lernen, dass man sich

1336
00:49:48,540 --> 00:49:50,760
das als eine Art von Aktion vorstellen kann, etwa lernen,

1337
00:49:50,760 --> 00:49:52,980
wohin man seinen Blick bewegt  Das

1338
00:49:52,980 --> 00:49:54,420
Einfachste, was möglich wäre, würde sehr helfen,

1339
00:49:54,420 --> 00:49:56,220
Illusionen modellieren zu können, und

1340
00:49:56,220 --> 00:49:58,859
ich meine, für mich ist es so, als würde ich einen

1341
00:49:58,859 --> 00:50:00,720
Aufsatz über kognitionswissenschaftliche

1342
00:50:00,720 --> 00:50:02,940
Experimente oder über eine Illusion lesen und

1343
00:50:02,940 --> 00:50:05,160
mir überlegen, ob ich diesen

1344
00:50:05,160 --> 00:50:07,560
Datensatz in meinen Datensatz einfügen kann  Modellieren und testen, und

1345
00:50:07,560 --> 00:50:08,579
meistens lautet die Antwort „Nein“,

1346
00:50:08,579 --> 00:50:10,619
weil ich kein Modell habe, das sich

1347
00:50:10,619 --> 00:50:12,900
umschaut oder ein eingeschränktes Sichtfeld hat, also ja,

1348
00:50:12,900 --> 00:50:14,660


1349
00:50:14,660 --> 00:50:16,619
ich denke, das ist eine der

1350
00:50:16,619 --> 00:50:19,680
Einschränkungen, eine andere ist ähm

1351
00:50:19,680 --> 00:50:20,579


1352
00:50:20,579 --> 00:50:22,920
ja, machen Sie das  Das Experiment ist viel

1353
00:50:22,920 --> 00:50:24,900
komplizierter, das ist also eine der

1354
00:50:24,900 --> 00:50:27,359
praktischen Einschränkungen.

1355
00:50:27,359 --> 00:50:30,240
Wow, tolle Antwort, lässt mich an ein

1356
00:50:30,240 --> 00:50:33,920
Papier mit Buchstaben denken, die auf einem Tisch rotieren. Das ist

1357
00:50:33,920 --> 00:50:36,780
die Ziffernrotation. Tolle Punkte

1358
00:50:36,780 --> 00:50:38,460
zur Foveation und zur Dynamik

1359
00:50:38,460 --> 00:50:40,079
der Illusion. Ich glaube, Sie haben tatsächlich

1360
00:50:40,079 --> 00:50:42,599
eine Illusion erwähnt, die  Haben Sie jedoch

1361
00:50:42,599 --> 00:50:43,980
im Generalisierungskontext erwähnt,

1362
00:50:43,980 --> 00:50:46,859
dass die Drehung auf dem zweidimensionalen

1363
00:50:46,859 --> 00:50:49,500
Bildschirm nicht auf drei

1364
00:50:49,500 --> 00:50:52,920
Dimensionen verallgemeinert werden kann und dass der dimensionale Kollaps

1365
00:50:52,920 --> 00:50:55,559
oder die Reduktion die Grundlage der

1366
00:50:55,559 --> 00:50:58,619
Illusionen der Würfelprojektion und der Illusionen der Würfel- und

1367
00:50:58,619 --> 00:51:01,880
Figurenrotation ist? Es ist auf Ihrem Bildschirm

1368
00:51:01,880 --> 00:51:05,280
und es gibt sie  eine Silhouette oder es gibt einige

1369
00:51:05,280 --> 00:51:07,260
mehrdeutige

1370
00:51:07,260 --> 00:51:09,839
Reize, dass ein generatives Modell nahezu

1371
00:51:09,839 --> 00:51:12,359
kritisch ist oder eine Gabelung in

1372
00:51:12,359 --> 00:51:13,680
degenerativen Modellen vorliegt, sodass es auf die

1373
00:51:13,680 --> 00:51:17,160
eine oder andere Weise dargestellt werden kann,

1374
00:51:17,160 --> 00:51:19,920
und so basieren viele der wechselnden Illusionen

1375
00:51:19,920 --> 00:51:22,020
einfach auf der Flachheit der

1376
00:51:22,020 --> 00:51:23,819
Bilder

1377
00:51:23,819 --> 00:51:26,280
und den Einschränkungen und der Verallgemeinerung

1378
00:51:26,280 --> 00:51:28,740
Das wird dadurch offenbart,

1379
00:51:28,740 --> 00:51:32,460
richtig ja ja, ich glaube, es gibt sogar

1380
00:51:32,460 --> 00:51:34,859
irgendwo Oh ja. Tut mir leid, da gibt es etwas Arbeit, oder

1381
00:51:34,859 --> 00:51:35,880
sie

1382
00:51:35,880 --> 00:51:37,619
können argumentieren, dass die Leute ein

1383
00:51:37,619 --> 00:51:39,480
dreidimensionales Bild in ihren Köpfen haben,

1384
00:51:39,480 --> 00:51:42,000
so wie sogar Nancy Ken kürzlich eine oder ihre

1385
00:51:42,000 --> 00:51:45,119
Lateralsas war, aber und ja, das zeige ich

1386
00:51:45,119 --> 00:51:48,119
nicht.  Ich weiß nicht, ob unsere Modelle sowieso sagen, dass

1387
00:51:48,119 --> 00:51:50,460
es nicht besonders groß ist.

1388
00:51:50,460 --> 00:51:53,700
Ja, das ist ziemlich interessant. Ähm,

1389
00:51:53,700 --> 00:51:56,160
alles klar, vom Upcycle Club im

1390
00:51:56,160 --> 00:51:58,200
Chat haben sie ein großes Lob geschrieben,

1391
00:51:58,200 --> 00:52:00,000
wenn Sie in der Lage sind, annähernd so effektiv zu lernen,

1392
00:52:00,000 --> 00:52:02,160
wenn Sie sich vorstellen, dass Sie nur

1393
00:52:02,160 --> 00:52:03,780
ein einziges Neuron haben möchten  aktiv für jedes

1394
00:52:03,780 --> 00:52:06,540
Beispiel. Ähm, Ihr Modell wird

1395
00:52:06,540 --> 00:52:08,579
versuchen, sich das Design des Datensatzes

1396
00:52:08,579 --> 00:52:10,980
oder etwas in der

1397
00:52:10,980 --> 00:52:12,180
Art zu merken, und Sie werden nicht über genügend

1398
00:52:12,180 --> 00:52:14,940
Kapazität verfügen, also ja, ich denke, die Optimierung dieses

1399
00:52:14,940 --> 00:52:18,359
Sparsitätsgrads ist sicherlich

1400
00:52:18,359 --> 00:52:22,200
ein wichtiger Faktor, und

1401
00:52:22,200 --> 00:52:25,020
ja  Wenn Sie sich die Wahrscheinlichkeit ansehen, wenn

1402
00:52:25,020 --> 00:52:26,220
Sie sprechen, wenn Sie das

1403
00:52:26,220 --> 00:52:28,579
Framework verdoppeln, wird dies normalerweise

1404
00:52:28,579 --> 00:52:32,040
automatisch mit der Wahrscheinlichkeit selbst ausgeglichen.

1405
00:52:32,040 --> 00:52:33,000
Ähm,

1406
00:52:33,000 --> 00:52:34,380
wenn Sie keine Generierungsmodellierung durchführen,

1407
00:52:34,380 --> 00:52:35,760
haben Sie nur eine Sparsity-Strafe, die Sie

1408
00:52:35,760 --> 00:52:38,460
einschalten möchten  Dieser Parameter ist in

1409
00:52:38,460 --> 00:52:40,980
Ordnung, ja, er dient nur dazu, das

1410
00:52:40,980 --> 00:52:43,380
außer Kontrolle geratene Verhalten in Armina zu verdeutlichen, bei dem das

1411
00:52:43,380 --> 00:52:45,599
Netzwerk

1412
00:52:45,599 --> 00:52:47,040
aufgrund verschiedener Faktoren wie

1413
00:52:47,040 --> 00:52:50,400
Rückkopplungsschleifenrauschen oder gegnerischer Eingaben instabil oder chaotisch wird

1414
00:52:50,400 --> 00:52:52,380


1415
00:52:52,380 --> 00:52:54,180


1416
00:52:54,180 --> 00:52:55,859
Sie

1417
00:52:55,859 --> 00:52:58,500
würden Feedback-Schleifen bekommen,

1418
00:52:58,500 --> 00:52:59,460
ähm,

1419
00:52:59,460 --> 00:53:01,800
aber ich könnte ja, ich könnte sehen, dass kontroverse

1420
00:53:01,800 --> 00:53:04,319
Beispiele möglicherweise

1421
00:53:04,319 --> 00:53:07,800
von Ihrem Grad an Sparsifizierung beeinflusst werden. Ähm,

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
der interessante Punkt ist, was Sie

1424
00:53:10,859 --> 00:53:12,660
anfälliger oder weniger anfällig wären,

1425
00:53:12,660 --> 00:53:16,040
Beispiele zu teilen, die ich nicht gut kenne, die

1426
00:53:16,040 --> 00:53:19,440


1427
00:53:19,440 --> 00:53:21,720
von einer Sparsifizierung projizieren  Wenn ein vollständig verbundenes

1428
00:53:21,720 --> 00:53:23,579
höherdimensionales Modell nur in

1429
00:53:23,579 --> 00:53:25,619
immer kleinere umgewandelt wird, ist

1430
00:53:25,619 --> 00:53:27,540
im Allgemeinen ziemlich gut bekannt,

1431
00:53:27,540 --> 00:53:29,760
was die Kompromisse sind. Es sind einfachere

1432
00:53:29,760 --> 00:53:34,079
Berechnungen. Ein kleineres Modell mit geringerer Dichte.

1433
00:53:34,079 --> 00:53:36,420
Der grundlegende Graph wird klarer

1434
00:53:36,420 --> 00:53:39,119
darzustellen sein, und dann wird es auch

1435
00:53:39,119 --> 00:53:41,339
alle anderen Kompromisse haben.  Offs mit falsch

1436
00:53:41,339 --> 00:53:43,680
positiven und negativen Ergebnissen der Verallgemeinerung,

1437
00:53:43,680 --> 00:53:45,720
aber deshalb handelt es sich um einen iterativen Anpassungsprozess.

1438
00:53:45,720 --> 00:53:47,579


1439
00:53:47,579 --> 00:53:49,760
Ich schätze also, wie funktioniert Ihr

1440
00:53:49,760 --> 00:53:52,800
Sparsifizierungsansatz? Die

1441
00:53:52,800 --> 00:53:55,700
Balance

1442
00:53:56,700 --> 00:53:59,520
verwendet nicht AIC oder Bic oder einen anderen

1443
00:53:59,520 --> 00:54:01,619
Modellanpassungsansatz, um die

1444
00:54:01,619 --> 00:54:03,660
relevante Sparsifizierung

1445
00:54:03,660 --> 00:54:07,079
für eine bestimmte Eingabe zu bestimmen.

1446
00:54:07,079 --> 00:54:09,780
Wie geht das?  Bestimmen Sie, wie ähnlich, wie bei der

1447
00:54:09,780 --> 00:54:11,940
Lasso-Regression, wie zum Beispiel „Woher weißt

1448
00:54:11,940 --> 00:54:14,339
du, wie viel, wie hoch ist der

1449
00:54:14,339 --> 00:54:17,220
Schwellenwert, wie viele, wie

1450
00:54:17,220 --> 00:54:19,559


1451
00:54:19,559 --> 00:54:22,440


1452
00:54:22,440 --> 00:54:25,319
spärlich soll es sein?“  Einige

1453
00:54:25,319 --> 00:54:29,520
Leute arbeiten jetzt damit, äh, haben

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
diese Art von entfalteten,

1456
00:54:34,380 --> 00:54:36,780
iterativen Sparsifizierungsnetzwerken gemacht,

1457
00:54:36,780 --> 00:54:37,800
bei denen es wie ein wiederkehrendes neuronales

1458
00:54:37,800 --> 00:54:40,380
Netzwerk ist und iterativ sparsifiziert, und

1459
00:54:40,380 --> 00:54:41,940
man kann zeigen, dass dies so etwas

1460
00:54:41,940 --> 00:54:45,780
wie Red Lose oder, äh, Gruppen-ähnliche Gruppen-

1461
00:54:45,780 --> 00:54:47,520
Aktiv-Gruppen-Sportaktivierungen wie

1462
00:54:47,520 --> 00:54:48,960
wir ergibt  Ich verwende hier

1463
00:54:48,960 --> 00:54:52,859
ähm in dieser Einstellung, ähm, es liegt wirklich nur

1464
00:54:52,859 --> 00:54:55,859
daran, diese

1465
00:54:55,859 --> 00:54:59,280
ähm diese Konstruktion dieser T-Variablen zu haben,

1466
00:54:59,280 --> 00:55:04,079
wo wir Z oben haben und ähm,

1467
00:55:04,079 --> 00:55:07,859
und dann wird es in gewisser Weise durch

1468
00:55:07,859 --> 00:55:09,119
diese

1469
00:55:09,119 --> 00:55:11,579
die Summe der U-Variablen unten bestimmt, also

1470
00:55:11,579 --> 00:55:13,200
wenn W  Vielleicht war mir nicht ganz klar, dass

1471
00:55:13,200 --> 00:55:16,500
dies eine verbindende Matrix ist, die

1472
00:55:16,500 --> 00:55:18,359
die Gruppe definiert, also

1473
00:55:18,359 --> 00:55:20,400
definiere ich die Varsity der Gruppe, die

1474
00:55:20,400 --> 00:55:22,380
alle diese U's miteinander verbindet, und so

1475
00:55:22,380 --> 00:55:23,940
ist die Idee,

1476
00:55:23,940 --> 00:55:27,540
äh, wie hier, wenn alle eines der anderen

1477
00:55:27,540 --> 00:55:31,740
Beispiele: Wenn alle Ihre Verwendungsvariablen

1478
00:55:31,740 --> 00:55:35,520
für ein bestimmtes t nicht aktiv sind

1479
00:55:35,520 --> 00:55:38,280
oder wenn alle Varios für ein bestimmtes t aktiv sind,

1480
00:55:38,280 --> 00:55:41,040
wird diese t-Variable sehr

1481
00:55:41,040 --> 00:55:42,780
klein sein, weil Ihr Nenner

1482
00:55:42,780 --> 00:55:44,339
sehr groß sein wird und das zu

1483
00:55:44,339 --> 00:55:47,160
Sparsität führt, also ist es so  Äh, es ist eine

1484
00:55:47,160 --> 00:55:49,260
Einschränkungsbefriedigung, wenn Sie eine

1485
00:55:49,260 --> 00:55:51,839
Menge von U haben, die alle klein sind, ähm, dann

1486
00:55:51,839 --> 00:55:54,480
ist diese Einschränkung erfüllt und

1487
00:55:54,480 --> 00:55:57,180
jetzt darf Z sich irgendwie ausdrücken,

1488
00:55:57,180 --> 00:56:00,240
und das ist es dann, äh, ja,

1489
00:56:00,240 --> 00:56:02,880
irgendwie, was die

1490
00:56:02,880 --> 00:56:06,180
Aktivierung betrifft  Dies wird also durch diese

1491
00:56:06,180 --> 00:56:07,020
beiden

1492
00:56:07,020 --> 00:56:09,300
Divergenzbegriffe induziert, die hier

1493
00:56:09,300 --> 00:56:12,960
sagen, wie weit jeder unhc

1494
00:56:12,960 --> 00:56:15,180
von einem Gaußschen Wert entfernt ist, und dann konstruieren wir durch diese

1495
00:56:15,180 --> 00:56:16,980
Konstruktion der Student-T-Variablen

1496
00:56:16,980 --> 00:56:20,880
effektiv eine spärliche

1497
00:56:20,880 --> 00:56:23,040
Prior-Verteilung nur aus diesen

1498
00:56:23,040 --> 00:56:24,839
Gaußschen Werten, aber in  Begriffe des Gesetzes, das

1499
00:56:24,839 --> 00:56:27,599
eigentliche Ziel, ähm, die Begriffe und das

1500
00:56:27,599 --> 00:56:28,920
Ziel, die wir optimieren, sind nur

1501
00:56:28,920 --> 00:56:31,619
diese beiden KL-Begriffe, die es bis

1502
00:56:31,619 --> 00:56:34,020
zu einem gewissen Grad in Richtung Sparsität treiben, und dies

1503
00:56:34,020 --> 00:56:36,359
wird

1504
00:56:36,359 --> 00:56:39,000
hier durch den Decoder automatisch mit dem Wahrscheinlichkeitsbegriff ausgeglichen,

1505
00:56:39,000 --> 00:56:41,220
sodass wir es nicht tun  Es gibt keine Begriffe, die wir optimieren,

1506
00:56:41,220 --> 00:56:42,839
aber wir lernen die Parameter

1507
00:56:42,839 --> 00:56:44,280
dieser verschiedenen Encoder kennen und

1508
00:56:44,280 --> 00:56:48,200
analysieren dann die Fehlschläge und Notfälle.

1509
00:56:48,540 --> 00:56:49,920
Na

1510
00:56:49,920 --> 00:56:52,859
gut, noch eine Frage von Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas, der geschrieben hat, dass er

1512
00:56:55,500 --> 00:56:59,160
über Blick und Illusion spricht: Können die

1513
00:56:59,160 --> 00:57:01,920
Studien zu Konstanzen bei Säuglingen

1514
00:57:01,920 --> 00:57:04,260
getrennt werden?  in Illusion auf niedrigerer Ebene Rel

1515
00:57:04,260 --> 00:57:06,140
vielleicht höhere Ebene

1516
00:57:06,140 --> 00:57:09,980
konzeptionelle Konstanz

1517
00:57:13,619 --> 00:57:15,720
ähm, können Sie

1518
00:57:15,720 --> 00:57:18,300
die aktuelle Art der Architektur lesen, könnten

1519
00:57:18,300 --> 00:57:23,520
die Studien zu Konstanzen bei Säuglingen

1520
00:57:23,520 --> 00:57:26,640
ähm kognitive Konstanzen getrennt werden?

1521
00:57:26,640 --> 00:57:31,619
Ja, wahrscheinlich bin ich nicht. Ich bin kein

1522
00:57:31,619 --> 00:57:33,900
Experte oder eigentlich sogar sehr vertraut

1523
00:57:33,900 --> 00:57:35,460
mit ähnlichen

1524
00:57:35,460 --> 00:57:37,859
Objektpermanenzstudien und Säuglings-

1525
00:57:37,859 --> 00:57:40,260
und Konstanzsachen, aber ich denke, es

1526
00:57:40,260 --> 00:57:42,300
wäre unglaublich interessant, die

1527
00:57:42,300 --> 00:57:44,339
Architekturen neuronaler Netzwerke zu untersuchen, und

1528
00:57:44,339 --> 00:57:46,740
das war irgendwie die Idee mit

1529
00:57:46,740 --> 00:57:48,780
dieser Illusion, die ich

1530
00:57:48,780 --> 00:57:51,780
hier unten mit dieser Zeile zu modellieren versuchte  Ich

1531
00:57:51,780 --> 00:57:53,099
weiß nicht, ob ich mir das ganz klar ausgedrückt habe, aber

1532
00:57:53,099 --> 00:57:55,380
die oberste Zeile ist die Eingabe, und wir

1533
00:57:55,380 --> 00:57:57,359
blockieren praktisch die Eingabe für

1534
00:57:57,359 --> 00:58:00,119
einen einzelnen Frame, und ich wollte sehen, ob

1535
00:58:00,119 --> 00:58:03,240
das Netzwerk irgendwie kodiert, dass das

1536
00:58:03,240 --> 00:58:05,400
Ding noch da ist  Wenn dieses Bild

1537
00:58:05,400 --> 00:58:07,680
weg ist, kann ich dann immer noch das Vorhandensein

1538
00:58:07,680 --> 00:58:10,020
des Objekts aus der neuronalen Aktivität entschlüsseln?

1539
00:58:10,020 --> 00:58:11,819
Und was schließt es dann auch über

1540
00:58:11,819 --> 00:58:13,920
die Bewegung, weil es

1541
00:58:13,920 --> 00:58:15,780
die Balken an einer etwas anderen Stelle gesehen hat

1542
00:58:15,780 --> 00:58:18,480
als vorher, als nach

1543
00:58:18,480 --> 00:58:20,520
dem?  Der Rahmen ist weg, ähm, also

1544
00:58:20,520 --> 00:58:22,559


1545
00:58:22,559 --> 00:58:25,200
ja, ich denke, es sind auf jeden Fall mehrere

1546
00:58:25,200 --> 00:58:27,240
Ebenen, ähm,

1547
00:58:27,240 --> 00:58:29,160
wobei einige wahrscheinlich auf einem viel

1548
00:58:29,160 --> 00:58:33,180
niedrigeren Niveau liegen würden und

1549
00:58:33,180 --> 00:58:35,880
vielleicht wäre die langfristige Objektbeständigkeit,

1550
00:58:35,880 --> 00:58:37,380
würde ich vermuten, deutlich

1551
00:58:37,380 --> 00:58:39,059
höher,

1552
00:58:39,059 --> 00:58:39,900
ähm,

1553
00:58:39,900 --> 00:58:41,760
das erinnert mich nur an diese

1554
00:58:41,760 --> 00:58:44,640
Experimente mit Katzen  Damals,

1555
00:58:44,640 --> 00:58:47,280
als es so war, als würden sie sie in der

1556
00:58:47,280 --> 00:58:49,020
Dunkelheit aufziehen, außer für eine Stunde am Tag,

1557
00:58:49,020 --> 00:58:51,000
steckten sie sie in die vertikale oder horizontale

1558
00:58:51,000 --> 00:58:53,160
Welt oder sie sahen nur horizontale

1559
00:58:53,160 --> 00:58:57,299
oder vertikale Linien, ähm, und man kann sehen, wie sich

1560
00:58:57,299 --> 00:58:59,880
die Organisation ihres Kortex

1561
00:58:59,880 --> 00:59:02,819
so verändert wie sie  haben weniger Aufnahmefähigkeit, es sind

1562
00:59:02,819 --> 00:59:04,200
horizontale Linien, wenn sie noch nie

1563
00:59:04,200 --> 00:59:06,420
zuvor horizontale Linien gesehen haben, und dann

1564
00:59:06,420 --> 00:59:07,980
nehmen Sie einen Stock und schwenken ihn vor

1565
00:59:07,980 --> 00:59:09,599
ihrem Gesicht und wenn der Stock

1566
00:59:09,599 --> 00:59:11,220
horizontal ist, tun sie einfach nichts,

1567
00:59:11,220 --> 00:59:12,900
er ist vertikal, sie schlagen darauf

1568
00:59:12,900 --> 00:59:14,460
Sie versuchen, es zu treffen, es ist, als

1569
00:59:14,460 --> 00:59:15,900
müssten sie buchstäblich nicht

1570
00:59:15,900 --> 00:59:18,420
vor ihrem Gesicht stehen, also denke ich, dass dies in diesem

1571
00:59:18,420 --> 00:59:20,700
Fall ein Beweis für einen

1572
00:59:20,700 --> 00:59:24,260
Mangel auf niedrigem Niveau und eine Sehkraft ist, die

1573
00:59:24,260 --> 00:59:26,940
zu einer Art Illusion beiträgt,

1574
00:59:26,940 --> 00:59:28,980
also ich ich  Ich denke, ja, es könnte sicherlich

1575
00:59:28,980 --> 00:59:30,660


1576
00:59:30,660 --> 00:59:32,839
auch bei Säuglingen einen Aspekt davon geben.

1577
00:59:33,540 --> 00:59:36,420
Ein sehr merkwürdiger Punkt, den Sie angesprochen haben,

1578
00:59:36,420 --> 00:59:40,339
war die belebte und unbelebte

1579
00:59:40,339 --> 00:59:43,619
Mannigfaltigkeit, wobei kleine Dinge dazwischen liegen. Richtig,

1580
00:59:43,619 --> 00:59:45,480


1581
00:59:45,480 --> 00:59:49,140
was stellt das dar

1582
00:59:49,140 --> 00:59:52,319
oder oder ist es, weil sie handhabbar sind

1583
00:59:52,319 --> 00:59:55,619
oder so?  Könnte ein Insekt sein oder

1584
00:59:55,619 --> 00:59:57,839
etwas, das sich nur mit dem

1585
00:59:57,839 --> 01:00:01,380
Wind fortbewegt, oder was soll das heißen? Ja,

1586
01:00:01,380 --> 01:00:04,380


1587
01:00:04,380 --> 01:00:08,280
das ist also eine Arbeit von Talia Conkle. Ich

1588
01:00:08,280 --> 01:00:11,280
glaube, sie war diejenige, die diese

1589
01:00:11,280 --> 01:00:12,240
ähm-

1590
01:00:12,240 --> 01:00:14,880
Organisation entdeckt hat, und sie haben versucht,

1591
01:00:14,880 --> 01:00:16,440
es herauszufinden  Ich glaube nicht, dass ich

1592
01:00:16,440 --> 01:00:19,500
das vielleicht falsch verstehe, also empfehle ich den Leuten,

1593
01:00:19,500 --> 01:00:21,359
ihre Arbeit dazu zu lesen, wenn sie es als

1594
01:00:21,359 --> 01:00:23,880
dreigliedrige Organisation bezeichnen, aber wenn ich mich

1595
01:00:23,880 --> 01:00:25,319
recht erinnere,

1596
01:00:25,319 --> 01:00:27,900
haben sie viel Nacharbeit darüber geleistet, warum

1597
01:00:27,900 --> 01:00:30,780
es diese Organisation gibt, und

1598
01:00:30,780 --> 01:00:33,180
einige Beweise dafür

1599
01:00:33,180 --> 01:00:35,700
Krümmung dieser Objekte und so etwas

1600
01:00:35,700 --> 01:00:37,440
wie die Entfernung, aus der man Objekte sieht,

1601
01:00:37,440 --> 01:00:40,260
oder wie

1602
01:00:40,260 --> 01:00:43,319
ähm animierte Objekte oder vielleicht kurviger

1603
01:00:43,319 --> 01:00:45,599
oder da, unabhängig davon, wie die

1604
01:00:45,599 --> 01:00:46,859
tatsächliche Antwort lautet, gab es viele

1605
01:00:46,859 --> 01:00:48,720
verschiedene Hypothesen, die

1606
01:00:48,720 --> 01:00:51,720
auf ähnlichen Eigenschaften dieser Objekte beruhten

1607
01:00:51,720 --> 01:00:54,000
Vielleicht eher bei Eigenschaften auf mittlerer oder niedriger Ebene

1608
01:00:54,000 --> 01:00:56,280
als bei Eigenschaften auf höherer Ebene. Ich

1609
01:00:56,280 --> 01:00:57,599
weiß immer noch nicht, ob genau

1610
01:00:57,599 --> 01:00:59,339
geklärt ist, ob es wie die

1611
01:00:59,339 --> 01:01:01,500
von Ihnen gesagte Interaktion mit den

1612
01:01:01,500 --> 01:01:04,920
Objekten ist, die die Trennung verursacht, oder

1613
01:01:04,920 --> 01:01:06,119
ähm

1614
01:01:06,119 --> 01:01:09,540
oder ja, die allgemeinen Formen dieser

1615
01:01:09,540 --> 01:01:12,299
Objekte  Ich wette, wie bei den meisten Dingen handelt

1616
01:01:12,299 --> 01:01:13,980
es sich um eine Kombination aus all dem

1617
01:01:13,980 --> 01:01:16,980
oben genannten, aber ich denke, das

1618
01:01:16,980 --> 01:01:18,480
Interessante an diesem Modellierungsstandpunkt

1619
01:01:18,480 --> 01:01:19,859
ist, dass

1620
01:01:19,859 --> 01:01:21,480


1621
01:01:21,480 --> 01:01:24,059
dies nur auf

1622
01:01:24,059 --> 01:01:26,819
Korrelationsstatistiken aus den Bilddatensätzen

1623
01:01:26,819 --> 01:01:28,799
selbst trainiert wird, sodass es hier keine Interaktion gibt

1624
01:01:28,799 --> 01:01:32,760
hat keine Ahnung von Animiertheit, ähm, ich meine, das

1625
01:01:32,760 --> 01:01:34,140
ist wirklich nur das Trainieren eines Modells auf

1626
01:01:34,140 --> 01:01:37,859
Imagenet, nur Bilder von Hunden, Katzen, Booten,

1627
01:01:37,859 --> 01:01:40,020
was auch immer, und dennoch erreicht es diese

1628
01:01:40,020 --> 01:01:41,640
Art von Organisation, also könnte es irgendeine

1629
01:01:41,640 --> 01:01:42,540
Art von

1630
01:01:42,540 --> 01:01:44,940
semantischen Merkmalen sein,

1631
01:01:44,940 --> 01:01:46,740
richtig, wir haben ein Bild, wir haben ein  Netzwerk, das

1632
01:01:46,740 --> 01:01:48,359


1633
01:01:48,359 --> 01:01:51,000
Boote vs. Hunde vs. 20 andere

1634
01:01:51,000 --> 01:01:53,640
Hunderassen klassifizieren kann, aber ob

1635
01:01:53,640 --> 01:01:55,920
es auch eine Übereinstimmung

1636
01:01:55,920 --> 01:01:57,900
mit Endstatistiken auf niedrigerem Niveau geben könnte, also ja,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
ich weiß es nicht, ich schätze,

1639
01:02:03,960 --> 01:02:07,500
ja, die provokante Analogie war die

1640
01:02:07,500 --> 01:02:10,380
Übersetzungsverschiebung

1641
01:02:10,380 --> 01:02:12,900
im Mnist in der Handschrift

1642
01:02:12,900 --> 01:02:14,819
Erkennungseinstellung,

1643
01:02:14,819 --> 01:02:18,000
was sind die Übersetzungsverschiebungen,

1644
01:02:18,000 --> 01:02:20,160
die

1645
01:02:20,160 --> 01:02:22,740
es heute gibt? Was ist das Drei-Pixel-

1646
01:02:22,740 --> 01:02:24,780
Beispiel? Es handelt sich um einen prompten, konstruierten

1647
01:02:24,780 --> 01:02:27,540
Angriff auf einen Film oder so oder so oder

1648
01:02:27,540 --> 01:02:29,099
so, ein Sonderzeichen, das

1649
01:02:29,099 --> 01:02:32,640
eingefügt wird, oder oder, ähm, eine

1650
01:02:32,640 --> 01:02:35,160
Überlagerung auf einem Bild, die wir nicht einmal können

1651
01:02:35,160 --> 01:02:37,020
Erkennen Sie das.

1652
01:02:37,020 --> 01:02:39,359
Was sind Ihrer Meinung nach diese Herausforderungen

1653
01:02:39,359 --> 01:02:42,839
und wie können wir dem nachgehen?

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
Ja, ich meine, ich denke, es ist so, wie

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
ich darüber nachgedacht habe, es ist wie

1658
01:02:50,099 --> 01:02:52,680
diese Symmetrietransformationen,

1659
01:02:52,680 --> 01:02:53,760
ähm,

1660
01:02:53,760 --> 01:02:55,799
wenn Sie über Sprachmodelle nachdenken  Ich

1661
01:02:55,799 --> 01:02:57,420
kann mir eine Symmetrietransformation vorstellen,

1662
01:02:57,420 --> 01:02:58,500
die so ist, als würde man

1663
01:02:58,500 --> 01:03:00,240
ein Wort durch ein Synonym oder

1664
01:03:00,240 --> 01:03:03,780
etwas ersetzen. Äh, Sie haben den Satz für uns

1665
01:03:03,780 --> 01:03:06,000
bedeutet genau dasselbe, aber jetzt

1666
01:03:06,000 --> 01:03:07,380
wird das Modell plötzlich

1667
01:03:07,380 --> 01:03:09,299
ganz anders reagieren,

1668
01:03:09,299 --> 01:03:11,240
ähm

1669
01:03:11,240 --> 01:03:15,359
wie eine Übersetzung zwischen Sprachen, dies

1670
01:03:15,359 --> 01:03:16,799
kann als angesehen werden  Bei dieser Art von Transformation

1671
01:03:16,799 --> 01:03:19,440
bleibt die zugrunde liegende

1672
01:03:19,440 --> 01:03:21,960
Bedeutung der Eingabe

1673
01:03:21,960 --> 01:03:24,900
für uns erhalten, aber für das Modell sieht es

1674
01:03:24,900 --> 01:03:26,220
völlig anders aus und wir

1675
01:03:26,220 --> 01:03:28,380
hätten gerne Modelle, die sich

1676
01:03:28,380 --> 01:03:29,940
in Bezug auf diese

1677
01:03:29,940 --> 01:03:32,160
Arten von Transformationen auf vorhersehbare Weise verhalten, weil

1678
01:03:32,160 --> 01:03:35,040
ich denke, dass sich Menschen seitdem sehr vorhersehbar verhalten

1679
01:03:35,040 --> 01:03:37,319
Transformationen und wenn

1680
01:03:37,319 --> 01:03:39,920
wir es mit KI-Systemen zu tun haben, erwarten wir, dass

1681
01:03:39,920 --> 01:03:43,200
sie sich auch so verhalten, und ich denke,

1682
01:03:43,200 --> 01:03:45,480
das ist einer der Gründe, warum die

1683
01:03:45,480 --> 01:03:47,339
Interaktion mit diesen

1684
01:03:47,339 --> 01:03:49,460
Systemen viele Herausforderungen mit sich bringt, und ich habe versucht,

1685
01:03:49,460 --> 01:03:52,500
das hiermit grob und frech zu demonstrieren

1686
01:03:52,500 --> 01:03:54,960
Bär und Quadrate und so etwas

1687
01:03:54,960 --> 01:03:58,440
wie ähm, wir gehen davon aus, dass

1688
01:03:58,440 --> 01:04:00,480
es in der Lage ist, so etwas Einfaches zu tun,

1689
01:04:00,480 --> 01:04:02,220
weil wir glauben, dass die meisten Menschen es

1690
01:04:02,220 --> 01:04:04,020
könnten, und doch tut es das nicht, und wenn Sie sich

1691
01:04:04,020 --> 01:04:05,460
vorstellen, dass dies ein kritisches Szenario ist,

1692
01:04:05,460 --> 01:04:07,740
in dem Sie dies und das erwarten, ist das ein großes

1693
01:04:07,740 --> 01:04:08,700
Problem, ähm

1694
01:04:08,700 --> 01:04:11,099
Wie gehen wir damit um? Ja, ich

1695
01:04:11,099 --> 01:04:12,720
denke, das ist in etwa das, wonach ich suche.

1696
01:04:12,720 --> 01:04:15,859
Ich denke,

1697
01:04:16,319 --> 01:04:18,420
meine

1698
01:04:18,420 --> 01:04:22,280
Richtung, die ich einschlage, sieht

1699
01:04:22,280 --> 01:04:26,940
einfacher aus und ähnelt Bottom-up-

1700
01:04:26,940 --> 01:04:29,460
Bausteinen

1701
01:04:29,460 --> 01:04:31,380
neuronaler Netzwerkarchitekturen oder

1702
01:04:31,380 --> 01:04:33,180
Algorithmen, die

1703
01:04:33,180 --> 01:04:35,760
diese entstehenden Elemente hervorbringen

1704
01:04:35,760 --> 01:04:37,680
Struktureigenschaften und ich denke, das ist ein viel

1705
01:04:37,680 --> 01:04:39,839
verallgemeinerbarer Weg, anstatt

1706
01:04:39,839 --> 01:04:41,579
etwas auf dem aufzubauen, was wir

1707
01:04:41,579 --> 01:04:43,140
bereits haben.

1708
01:04:43,140 --> 01:04:43,920
Ähm,

1709
01:04:43,920 --> 01:04:45,839
ich denke, das ist etwas, das sich

1710
01:04:45,839 --> 01:04:47,819
viel besser skalieren lässt und auch besser mit dem übereinstimmt, was

1711
01:04:47,819 --> 01:04:50,299
das Gehirn tut.

1712
01:04:50,760 --> 01:04:52,740
Sehr cool, eine Art

1713
01:04:52,740 --> 01:04:54,740
Implementierungsfrage, was sind das?

1714
01:04:54,740 --> 01:04:57,000
Rechenanforderungen, um dies einfach auszuführen, oder wie ist

1715
01:04:57,000 --> 01:04:59,400
der Alltag, wenn man als

1716
01:04:59,400 --> 01:05:01,920
Student oder Forscher Varianten davon ausführt,

1717
01:05:01,920 --> 01:05:04,380
z. B.: Verbrauchen sie Terabytes an

1718
01:05:04,380 --> 01:05:07,020
Daten und Sie verwenden umfangreiche Berechnungen,

1719
01:05:07,020 --> 01:05:08,940
oder ist dies etwas, das die Leute alleine ausführen können?

1720
01:05:08,940 --> 01:05:11,880
Laptops

1721
01:05:11,880 --> 01:05:13,980
Ich denke, dass fast alles, was ich heute vorgestellt habe,

1722
01:05:13,980 --> 01:05:17,099
lokal ausgeführt werden kann.

1723
01:05:17,099 --> 01:05:20,040
Das ist also supereinfach und Sie können es ausführen. Ich

1724
01:05:20,040 --> 01:05:20,760
meine,

1725
01:05:20,760 --> 01:05:22,319
Sie werden

1726
01:05:22,319 --> 01:05:24,299
denken, Sie können es auf Ihrem Laptop ausführen, wenn

1727
01:05:24,299 --> 01:05:25,980
Sie gerne trainieren und

1728
01:05:25,980 --> 01:05:27,420
mit verschiedenen Dingen experimentieren möchten, die es macht

1729
01:05:27,420 --> 01:05:30,119
ziemlich langsam zu sein, daher würde ich eine

1730
01:05:30,119 --> 01:05:33,359
kommerzielle GPU empfehlen, wie z. B. eine

1731
01:05:33,359 --> 01:05:35,640
Nvidia 1080, auf der so ziemlich alles läuft,

1732
01:05:35,640 --> 01:05:38,819
ziemlich alt, ziemlich günstig, aber sie haben 12

1733
01:05:38,819 --> 01:05:41,099
GB RAM oder was auch immer, und

1734
01:05:41,099 --> 01:05:43,140
vier Gigabyte RAM sind für diese Modelle mehr als genug

1735
01:05:43,140 --> 01:05:46,440
Ich denke, eine Sache, die

1736
01:05:46,440 --> 01:05:48,839
einige Leute seltsam finden, ist, dass ich die meisten

1737
01:05:48,839 --> 01:05:51,480
meiner Experimente mit Dingen wie Mnist mache, also

1738
01:05:51,480 --> 01:05:54,780
sind es Bilder mit 32 x 32 Pixeln, weil ich

1739
01:05:54,780 --> 01:05:57,299
sie klein und stimmlich trainieren kann,

1740
01:05:57,299 --> 01:06:00,000
ähm, wenn Sie das wollen, meine Experimente

1741
01:06:00,000 --> 01:06:02,460
oder eine Endlosversion, wenn Sie möchten  Ich möchte Dinge

1742
01:06:02,460 --> 01:06:03,540
wie diese tun, diese sind viel

1743
01:06:03,540 --> 01:06:05,640
komplizierter, diese hamiltonische Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite. Hier geht es um größere

1745
01:06:08,160 --> 01:06:09,780
Modelle, die auf mehreren

1746
01:06:09,780 --> 01:06:12,240
GPUs laufen, und hier wird ein Cluster verwendet, um

1747
01:06:12,240 --> 01:06:14,220
diese Art von Modellen auszuführen,

1748
01:06:14,220 --> 01:06:16,140
ähm, aber ich würde sagen, die meisten davon  Die einzelne

1749
01:06:16,140 --> 01:06:18,920
Maschine mit der GPU ist mehr als genug

1750
01:06:18,920 --> 01:06:21,680
oder auch einfach so, wie in einem Collab-Notebook,

1751
01:06:21,680 --> 01:06:24,539
wenn man

1752
01:06:24,539 --> 01:06:26,520
etwas auf Imagenet trainieren möchte, wird es

1753
01:06:26,520 --> 01:06:29,940
komplizierter und man braucht

1754
01:06:29,940 --> 01:06:33,660
mindestens eine GPU, idealerweise mehr, aber ja, das

1755
01:06:33,660 --> 01:06:35,160
mache ich nicht  Eine ganze Menge Dinge im großen Maßstab,

1756
01:06:35,160 --> 01:06:37,200
aber ich denke, dass es auf jeden Fall interessant ist,

1757
01:06:37,200 --> 01:06:39,960
und es gibt definitiv noch viel mehr, was man

1758
01:06:39,960 --> 01:06:42,539
dort machen kann, aber für einige dieser

1759
01:06:42,539 --> 01:06:45,480
einfacheren oder grundlegenderen Fragen

1760
01:06:45,480 --> 01:06:47,760
weiß ich nicht, wie man es nennen soll, ähm,

1761
01:06:47,760 --> 01:06:52,500
eine kleinere Maschine ist  Schön und schnell, also

1762
01:06:52,500 --> 01:06:54,660
cool, nützlich,

1763
01:06:54,660 --> 01:06:58,140
alles klar. Ich lese einen Kommentar von Dave, der

1764
01:06:58,140 --> 01:07:00,780
sich an den Kommentar von Bert DeVries während

1765
01:07:00,780 --> 01:07:02,880
des Applied Active Inference Symposiums erinnert, in dem es darum ging, dass es

1766
01:07:02,880 --> 01:07:05,520
wünschenswert sei, weniger

1767
01:07:05,520 --> 01:07:08,099
Aufwand oder ATP für Futtersuche oder

1768
01:07:08,099 --> 01:07:09,780
Kontrollsituationen aufzuwenden, in denen wir nicht viel

1769
01:07:09,780 --> 01:07:11,579
Präzision benötigen  Ich weiß nicht, ob Sie sich

1770
01:07:11,579 --> 01:07:13,859
das anhören, aber Professor DeVries erwähnte

1771
01:07:13,859 --> 01:07:17,520
variable Präzisionsmodelle und wie

1772
01:07:17,520 --> 01:07:19,440
sie verwendet werden könnten, um

1773
01:07:19,440 --> 01:07:21,059
verschiedene Funktionen der

1774
01:07:21,059 --> 01:07:23,039
Generalisierung und tatsächliche strukturelle

1775
01:07:23,039 --> 01:07:25,020
Kursausbildung sowie reduzierte

1776
01:07:25,020 --> 01:07:27,059
Rechenanforderungen zu ermöglichen.

1777
01:07:27,059 --> 01:07:29,880
Er hat Vorschläge, wie

1778
01:07:29,880 --> 01:07:31,980
dies eingeführt werden könnte  Unterscheidung in aktive

1779
01:07:31,980 --> 01:07:33,900
Inferenztheorie, welche Arten von

1780
01:07:33,900 --> 01:07:37,760
Experimenten könnten Winkle das herausbringen

1781
01:07:38,400 --> 01:07:40,680
oh wow ja, das ist etwas, was ich nicht weiß. Ich

1782
01:07:40,680 --> 01:07:42,660
glaube nicht, dass ich allzu viel Intelligentes

1783
01:07:42,660 --> 01:07:46,619
dazu sagen kann, das ist ganz ehrlich,

1784
01:07:46,619 --> 01:07:48,740
ähm,

1785
01:07:51,359 --> 01:07:53,460
es ist eine super interessante Frage, weil

1786
01:07:53,460 --> 01:07:56,220
ich denke, dass die Intuition einen ausmacht  Mir macht es sehr viel

1787
01:07:56,220 --> 01:07:58,260
Sinn, dass

1788
01:07:58,260 --> 01:08:00,000
Sie davon sprechen,

1789
01:08:00,000 --> 01:08:01,980
wenn ich die variablen Präzisionsraten richtig verstehe,

1790
01:08:01,980 --> 01:08:05,160
wenn Sie

1791
01:08:05,160 --> 01:08:07,079
in Ihrem Modell kodieren oder im Allgemeinen Berechnungen durchführen

1792
01:08:07,079 --> 01:08:07,740


1793
01:08:07,740 --> 01:08:09,420
[Musik]

1794
01:08:09,420 --> 01:08:11,539
ähm,

1795
01:08:13,200 --> 01:08:17,040
das hat irgendwie einen Einfluss auf Ihre

1796
01:08:17,040 --> 01:08:19,080
zukünftige Leistung als  eine Beziehung zu einem

1797
01:08:19,080 --> 01:08:22,259
Energiespeicher, denke ich, ja, und wenn Sie

1798
01:08:22,259 --> 01:08:23,580
dies in ein aktives Anstrengungssystem einbauen wollten,

1799
01:08:23,580 --> 01:08:26,279
müssten Sie

1800
01:08:26,279 --> 01:08:28,679
wirklich ein verkörpertes System haben, in dem der

1801
01:08:28,679 --> 01:08:31,500
Agent eine Vorstellung von Energie hat, wie einen

1802
01:08:31,500 --> 01:08:34,439
internen Energiespeicher, und

1803
01:08:34,439 --> 01:08:36,238
ja, etwas, das versucht

1804
01:08:36,238 --> 01:08:38,520
Um zu sparen, während es seine Aktionen ausführt, und wenn ihm die

1805
01:08:38,520 --> 01:08:40,500


1806
01:08:40,500 --> 01:08:42,359
Energie ausgeht, müsste

1807
01:08:42,359 --> 01:08:44,759
etwas Schlimmes für die Agenten nötig sein, und

1808
01:08:44,759 --> 01:08:47,399
dann könnte man vielleicht eine Art Entstehung beobachten,

1809
01:08:47,399 --> 01:08:48,679


1810
01:08:48,679 --> 01:08:52,040
äh, Reduzierung und

1811
01:08:52,040 --> 01:08:55,198
Codierung von Präzision oder so etwas in der Art,

1812
01:08:55,198 --> 01:08:57,479
wie es der Agent zu lernen versucht  Um

1813
01:08:57,479 --> 01:09:00,060
effektiver zu agieren, muss man

1814
01:09:00,060 --> 01:09:02,520
ihm vielleicht die Möglichkeit geben, seine Präzision zu kontrollieren.

1815
01:09:02,520 --> 01:09:04,020


1816
01:09:04,020 --> 01:09:07,080
Ja, wie ich aufgrund meines Fachwissens sage, aber

1817
01:09:07,080 --> 01:09:08,819


1818
01:09:08,819 --> 01:09:11,460
auf dieser Folie hier sind die Gedanken irgendwie in Ordnung. Das erste sehr

1819
01:09:11,460 --> 01:09:14,219
coole Bild, es ist ein bisschen wie ein

1820
01:09:14,219 --> 01:09:18,359
digitaler Jackson Pollock,

1821
01:09:18,359 --> 01:09:22,920
ähm, wenn es so wäre  eine einfachere

1822
01:09:22,920 --> 01:09:26,520
Eingabedatengröße oder nur eine reduzierte Komplexität der

1823
01:09:26,520 --> 01:09:27,719
Muster oder wenn es eine erhöhte

1824
01:09:27,719 --> 01:09:30,000
Komplexität wäre, wie würde dieses Bild anders aussehen?

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
Ja, also habe ich einige Experimente durchgeführt und versucht,

1827
01:09:34,620 --> 01:09:36,738
diese

1828
01:09:36,738 --> 01:09:40,439
Orientierungsspalten zu ändern, und

1829
01:09:40,439 --> 01:09:41,100
ähm,

1830
01:09:41,100 --> 01:09:43,140
Sie können ja grundsätzlich die

1831
01:09:43,140 --> 01:09:44,520
Parameter des Modells ändern, die Sie können  Wenn Sie

1832
01:09:44,520 --> 01:09:47,100
diese Spalten vergrößern, können Sie dafür sorgen, dass

1833
01:09:47,100 --> 01:09:49,380
sie keine sehr ähnliche Struktur haben

1834
01:09:49,380 --> 01:09:51,660
wie die, die wir bei den Menschen sehen, bei denen wir

1835
01:09:51,660 --> 01:09:53,040
Sie haben. Sie können dafür sorgen, dass sie mehr Aktivitätsbänder aufweisen,

1836
01:09:53,040 --> 01:09:55,199


1837
01:09:55,199 --> 01:09:56,160
ähm,

1838
01:09:56,160 --> 01:09:58,199
und es hängt auch, wie Sie sagten,

1839
01:09:58,199 --> 01:10:00,540
vom Datensatz ab  die Sie verwenden, wenn ich

1840
01:10:00,540 --> 01:10:03,179
wirklich einfache sinusförmige Abstufungen

1841
01:10:03,179 --> 01:10:05,580
als Eingabe verwende, bekomme ich so etwas wie dieses, ich bekomme

1842
01:10:05,580 --> 01:10:07,920
etwas, das ein bisschen mehr ist, äh,

1843
01:10:07,920 --> 01:10:11,640
Rotationskurve, höhere Entropie,

1844
01:10:11,640 --> 01:10:12,800
ähm,

1845
01:10:12,800 --> 01:10:15,660
also denke ich, dass das alles interessante

1846
01:10:15,660 --> 01:10:18,000
Dinge sind, wenn Sie die

1847
01:10:18,000 --> 01:10:19,320
Entstehung von untersuchen möchten  Diese Art von Organisation

1848
01:10:19,320 --> 01:10:22,199
in einem natürlichen System. Ähm, wenn Sie ein

1849
01:10:22,199 --> 01:10:24,120
Modell haben, das jetzt unterschiedliche

1850
01:10:24,120 --> 01:10:25,860
Organisationen für verschiedene

1851
01:10:25,860 --> 01:10:28,620
Einstellungen liefert, ist das mal in Ordnung. Welche

1852
01:10:28,620 --> 01:10:31,679
Einstellungen passen dann am besten zu unseren beobachteten Daten?

1853
01:10:31,679 --> 01:10:32,340


1854
01:10:32,340 --> 01:10:34,679
Äh, also ja,

1855
01:10:34,679 --> 01:10:36,540
ich kann die um ihn herum schicken, wenn Sie es sind

1856
01:10:36,540 --> 01:10:38,520
Interessiert, aber ähm

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
ja, ich denke, es gibt auch noch einen anderen, sorry,

1859
01:10:44,219 --> 01:10:45,659
einen weiteren interessanten Punkt:

1860
01:10:45,659 --> 01:10:46,920
Die

1861
01:10:46,920 --> 01:10:51,480
äh verschiedenen Tiere und Arten der äh

1862
01:10:51,480 --> 01:10:53,219
Orientierungsselektivität und die unterschiedliche

1863
01:10:53,219 --> 01:10:54,659
Anzahl von Windrädern, manche Tiere

1864
01:10:54,659 --> 01:10:57,420
haben es überhaupt nicht. Ich denke, vielleicht Mäuse, wenn ich es bin

1865
01:10:57,420 --> 01:11:00,120
Richtig, es gibt diese Art von, äh, sie nennen es

1866
01:11:00,120 --> 01:11:01,800
Salz- und Pfeffer-Selektivität, also ist es

1867
01:11:01,800 --> 01:11:03,480
im Grunde genommen zufällig, man hat nicht so etwas

1868
01:11:03,480 --> 01:11:04,679
wie eine topografische Orientierungsempfindlichkeit, ähm, es

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
gibt also Beweise dafür, dass

1871
01:11:09,300 --> 01:11:10,920
verschiedene Systeme dies

1872
01:11:10,920 --> 01:11:13,020
unterschiedlich machen, und es ist interessant

1873
01:11:13,020 --> 01:11:14,760
herauszufinden, warum

1874
01:11:14,760 --> 01:11:17,760
das so ist  Es ist sehr cool, es erinnert mich

1875
01:11:17,760 --> 01:11:21,300
zunächst an die

1876
01:11:21,300 --> 01:11:22,980
Reaktionsdiffusionsbasis und -zeit,

1877
01:11:22,980 --> 01:11:25,739
daher ist es tatsächlich

1878
01:11:25,739 --> 01:11:30,000
um möglich, dass eine Region keine

1879
01:11:30,000 --> 01:11:32,840
Aktivität von einer bestimmten

1880
01:11:32,840 --> 01:11:35,640
Granularität aufweist, wie wenn sie

1881
01:11:35,640 --> 01:11:39,360
auf der räumlichen und zeitlichen Zeitskala von fmri betrachtet würde,

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
wenn die Taschen der Aktivität aber vorhanden wären  Wenn die

1884
01:11:44,699 --> 01:11:46,380
Aktivitätsbereiche

1885
01:11:46,380 --> 01:11:48,480
langsamer und schneller sind, unterscheidet sich die

1886
01:11:48,480 --> 01:11:52,260
Messung nicht

1887
01:11:52,260 --> 01:11:54,060
vom Rauschen, es wurde alles

1888
01:11:54,060 --> 01:11:55,739
gemittelt,

1889
01:11:55,739 --> 01:11:58,620
sodass es einige, ja,

1890
01:11:58,620 --> 01:12:01,560
interessante Datensätze geben könnte, die

1891
01:12:01,560 --> 01:12:03,360
tatsächlich sehr

1892
01:12:03,360 --> 01:12:06,179
umfangreich sind, aber zum einen  Aus dem einen oder

1893
01:12:06,179 --> 01:12:08,520
anderen Grund wurde einfach der Mittelwert ermittelt,

1894
01:12:08,520 --> 01:12:11,100
weil keine Verbindung zu Ihnen bestand

1895
01:12:11,100 --> 01:12:12,420
oder so etwas in der Art. Sie müssen wirklich

1896
01:12:12,420 --> 01:12:14,520
mit einem einzigen Testlevel weitermachen, Sie müssen eine

1897
01:12:14,520 --> 01:12:16,140
ausreichend hohe räumliche Auflösung haben,

1898
01:12:16,140 --> 01:12:18,719
sodass Sie wissen, dass sie

1899
01:12:18,719 --> 01:12:23,100
Mikrofrequenzen erfüllt, äh  Und das

1900
01:12:23,100 --> 01:12:24,659
ist einfach etwas, was die Leute schon lange nicht mehr gemacht haben,

1901
01:12:24,659 --> 01:12:25,620
besonders wenn man

1902
01:12:25,620 --> 01:12:27,780
Einzelwähleraufnahmen macht, sieht man keine

1903
01:12:27,780 --> 01:12:28,920
Wanderwelle, sondern

1904
01:12:28,920 --> 01:12:30,900
Schwingungen,

1905
01:12:30,900 --> 01:12:32,219
also braucht man Multi-Elektrik

1906
01:12:32,219 --> 01:12:34,199
Arrays und im Grunde sagen sie, okay,

1907
01:12:34,199 --> 01:12:36,000
ja, jetzt, wo wir die Technologie haben,

1908
01:12:36,000 --> 01:12:37,520
dies zu tun, bleibt so viel

1909
01:12:37,520 --> 01:12:40,080
bestehen, was wir vorher nicht gesehen haben, und

1910
01:12:40,080 --> 01:12:42,960
möglicherweise ist dies eine Erklärung für einen

1911
01:12:42,960 --> 01:12:44,400
Großteil des Rauschens, das wir vorher gesehen haben,

1912
01:12:44,400 --> 01:12:46,260
vielleicht ist es wirklich so  Wanderwellen

1913
01:12:46,260 --> 01:12:47,219


1914
01:12:47,219 --> 01:12:47,760
ähm,

1915
01:12:47,760 --> 01:12:51,480
also ja, ich denke, es gibt in Zukunft viel zu tun

1916
01:12:51,480 --> 01:12:53,880
mit verbesserten

1917
01:12:53,880 --> 01:12:56,520
Möglichkeiten für die Aufnahme,

1918
01:12:56,520 --> 01:12:58,739
das ist sehr cool, also

1919
01:12:58,739 --> 01:13:02,940
irgendwelche letzten Gedanken oder Fragen oder

1920
01:13:02,940 --> 01:13:06,239
wo willst du diese Arbeit hinbringen?

1921
01:13:06,239 --> 01:13:08,520
ja nein, danke, dass du mich

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
hoffentlich ähm in der aktiven Infrastruktur hast

1924
01:13:11,640 --> 01:13:14,520
Das würde ich gerne tun. Ich denke,

1925
01:13:14,520 --> 01:13:16,560
das würde super Spaß machen. Also ja, ich bin mir nicht

1926
01:13:16,560 --> 01:13:18,980
wirklich sicher, ob ich mir vielleicht gerade Musik anschaue, ähm, ähm,

1927
01:13:18,980 --> 01:13:21,659


1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760
schaue ich mir

1930
01:13:26,760 --> 01:13:30,420
andere verrückte Richtungen an. Ich möchte nicht

1931
01:13:30,420 --> 01:13:33,020
zu verrückt klingen, ähm,

1932
01:13:33,020 --> 01:13:36,900
aber  Ich werde ja eine Menge Dinge durchgehen, also

1933
01:13:36,900 --> 01:13:38,580
eine Sache, die

1934
01:13:38,580 --> 01:13:40,320
wir den Neurops vorgelegt haben, ist die Untersuchung des

1935
01:13:40,320 --> 01:13:43,140
Gedächtnisses mit wandernden Wellen,

1936
01:13:43,140 --> 01:13:45,060
ähm, also ist dieser Artikel erst

1937
01:13:45,060 --> 01:13:46,860
heute im Archiv erschienen, ähm,

1938
01:13:46,860 --> 01:13:48,840
wie Wellen wirklich gut darin sind,

1939
01:13:48,840 --> 01:13:50,580
Langzeiterinnerungen zu kodieren  Das finde ich

1940
01:13:50,580 --> 01:13:52,100
superinteressant,

1941
01:13:52,100 --> 01:13:54,120
deshalb könnte ich ein bisschen in diese Richtung gehen. Das

1942
01:13:54,120 --> 01:13:55,800


1943
01:13:55,800 --> 01:13:58,920
klingt gut und es wäre sehr

1944
01:13:58,920 --> 01:14:01,400
spannend zu sehen, wie etwas ins Spiel kommt,

1945
01:14:01,400 --> 01:14:04,560
wenn die Neuronen

1946
01:14:04,560 --> 01:14:07,620
aktiv bleiben, selbst wenn sich die Füße des Hundes bewegen.

1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
Da gibt es viel Ähnliches  Action-Sequenzen

1949
01:14:11,480 --> 01:14:14,280
wie das Werfen eines Baseballs und dann

1950
01:14:14,280 --> 01:14:15,900
geht es los und es ist, als gäbe es etwas

1951
01:14:15,900 --> 01:14:18,440
an dieser Aktion, das weiterhin

1952
01:14:18,440 --> 01:14:21,179
Einfluss hat, und so eine tiefe

1953
01:14:21,179 --> 01:14:23,699
zeitliche Darstellung alternativer

1954
01:14:23,699 --> 01:14:26,159
Aktionen zu haben,

1955
01:14:26,159 --> 01:14:29,640
und dann ist der Variations-Autoencoder im

1956
01:14:29,640 --> 01:14:33,179
Grunde schon das Richtige für so etwas,

1957
01:14:33,179 --> 01:14:35,219
also

1958
01:14:35,219 --> 01:14:37,739
schätze ich es wirklich  Alles klar, danke

1959
01:14:37,739 --> 01:14:39,480
bis zum nächsten Mal.

1960
01:14:39,480 --> 01:14:43,339
Vielen Dank, tschüss


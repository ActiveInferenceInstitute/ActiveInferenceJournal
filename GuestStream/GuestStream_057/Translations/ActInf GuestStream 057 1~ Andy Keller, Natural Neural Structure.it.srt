1
00:00:06,600 --> 00:00:09,420
ciao e benvenuto, è il 18 settembre

2
00:00:09,420 --> 00:00:14,040
2023 ed è attivo il guest stream 57.1

3
00:00:14,040 --> 00:00:16,740
con Andy Keller,

4
00:00:16,740 --> 00:00:19,619
parleremo della struttura neurale naturale

5
00:00:19,619 --> 00:00:22,020
per l'intelligenza artificiale, ci

6
00:00:22,020 --> 00:00:24,300
sarà una presentazione seguita da una

7
00:00:24,300 --> 00:00:25,800
discussione, quindi se stai guardando dal vivo,

8
00:00:25,800 --> 00:00:27,900
sentiti libero di farlo  scrivi domande

9
00:00:27,900 --> 00:00:30,660
nella chat dal vivo altrimenti grazie Andy

10
00:00:30,660 --> 00:00:32,279
per questo, non vedo l'ora

11
00:00:32,279 --> 00:00:35,899
e a te per la presentazione

12
00:00:36,360 --> 00:00:38,940
sì, grazie mille uh grazie per avermi ospitato,

13
00:00:38,940 --> 00:00:41,280
sono super entusiasta di poter

14
00:00:41,280 --> 00:00:42,840
presentare queste cose con il

15
00:00:42,840 --> 00:00:45,239
gruppo di riferimento attivo I  sono un fan e sono molto

16
00:00:45,239 --> 00:00:49,200
interessato quindi spero che si possa

17
00:00:49,200 --> 00:00:50,399
avere una bella discussione e vedere cosa ne

18
00:00:50,399 --> 00:00:51,960
pensate ragazzi,

19
00:00:51,960 --> 00:00:54,840
quindi mi chiamo Andy, sto finendo il

20
00:00:54,840 --> 00:00:57,180
mio dottorato di ricerca supervisionato da Maxwelling presso l'

21
00:00:57,180 --> 00:00:59,100
Università di Amsterdam,

22
00:00:59,100 --> 00:01:01,500
um sono  iniziare un post-doc ad Harvard dopo

23
00:01:01,500 --> 00:01:05,099
questo quindi inizierò. Sto solo parlando

24
00:01:05,099 --> 00:01:07,619
dell'obiettivo del mio lavoro in generale è

25
00:01:07,619 --> 00:01:09,540
cercare di avvicinare l'intelligenza artificiale moderna

26
00:01:09,540 --> 00:01:12,540
a una generalizzazione più simile a quella umana

27
00:01:12,540 --> 00:01:15,240
e quindi ciò che intendiamo con

28
00:01:15,240 --> 00:01:17,159
questo è forse qualche  una sorta di generalizzazione della struttura

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
ehm o forse più familiare al

31
00:01:20,700 --> 00:01:22,080
comitato dei neonati attivi come un

32
00:01:22,080 --> 00:01:24,299
modello di mondo strutturato che crediamo abbiano gli esseri umani

33
00:01:24,299 --> 00:01:26,340
e il modo in cui proponiamo di farlo

34
00:01:26,340 --> 00:01:28,799
è integrando la struttura neurale naturale

35
00:01:28,799 --> 00:01:32,400
nell'intelligenza artificiale

36
00:01:32,400 --> 00:01:34,979
quindi prima definiamo cosa intendiamo  per

37
00:01:34,979 --> 00:01:36,720
generalizzazione della struttura,

38
00:01:36,720 --> 00:01:38,400
quindi penso che sia abbastanza incontrovertibile

39
00:01:38,400 --> 00:01:40,380
affermare che l'apprendimento automatico moderno

40
00:01:40,380 --> 00:01:42,900
generalizza oltre il suo set di addestramento in

41
00:01:42,900 --> 00:01:45,000
senso tradizionale, quindi ad esempio

42
00:01:45,000 --> 00:01:46,799
anche le prime reti neurali artificiali, i

43
00:01:46,799 --> 00:01:49,020
percettroni multistrato potrebbero

44
00:01:49,020 --> 00:01:51,659
essere addestrati su set di dati di immagini come

45
00:01:51,659 --> 00:01:55,140
questo e raggiungere livelli elevati  accuratezza quindi, quando

46
00:01:55,140 --> 00:01:56,880
viene presentato loro un

47
00:01:56,880 --> 00:01:58,320
set di immagini di prova che non hanno mai visto

48
00:01:58,320 --> 00:02:00,240
prima, possono comunque classificarle

49
00:02:00,240 --> 00:02:02,460
relativamente facilmente con lo stesso livello di

50
00:02:02,460 --> 00:02:04,500
precisione e questo è ciò che in genere

51
00:02:04,500 --> 00:02:07,320
chiamiamo generalizzazione, tuttavia anche abbastanza

52
00:02:07,320 --> 00:02:08,758
presto lo era  notato che questi

53
00:02:08,758 --> 00:02:10,800
sistemi lottano davvero con piccoli

54
00:02:10,800 --> 00:02:12,720
spostamenti o deformazioni applicate alle

55
00:02:12,720 --> 00:02:16,340
immagini, ad esempio, se

56
00:02:18,920 --> 00:02:23,099
sì, pensi perché è sorprendente e

57
00:02:23,099 --> 00:02:24,720
io sostengo che è proprio dovuto alla nostra innata

58
00:02:24,720 --> 00:02:26,640
capacità di eseguire questo tipo di

59
00:02:26,640 --> 00:02:28,680
generalizzazione della struttura che questo

60
00:02:28,680 --> 00:02:31,920
esempio è un fallimento di così  uh per

61
00:02:31,920 --> 00:02:33,239
esempio questo spostamento è quasi

62
00:02:33,239 --> 00:02:35,340
impercettibile per noi e lo gestiamo

63
00:02:35,340 --> 00:02:37,560
automaticamente mentre nel sistema è chiaramente

64
00:02:37,560 --> 00:02:39,959
un grosso problema quindi a parole

65
00:02:39,959 --> 00:02:41,940
possiamo dire che la generalizzazione della struttura

66
00:02:41,940 --> 00:02:44,819
è una generalizzazione ad alcune

67
00:02:44,819 --> 00:02:47,040
trasformazioni di simmetria dell'input o in questo

68
00:02:47,040 --> 00:02:48,959
caso la simmetria  la trasformazione è un

69
00:02:48,959 --> 00:02:50,580
piccolo spostamento che lascia invariata la classe delle cifre,

70
00:02:50,580 --> 00:02:52,019


71
00:02:52,019 --> 00:02:54,599
quindi la domanda ovvia è cosa

72
00:02:54,599 --> 00:02:56,340
intendiamo esattamente con questa

73
00:02:56,340 --> 00:02:58,860
struttura naturale e perché pensiamo che ciò

74
00:02:58,860 --> 00:03:01,620
ci aiuterebbe con queste impostazioni,

75
00:03:01,620 --> 00:03:03,780
quindi prima parliamo di cosa intendiamo

76
00:03:03,780 --> 00:03:05,819
per neurale naturale  struttura

77
00:03:05,819 --> 00:03:08,700
ehm, un modo per parlare di struttura o di

78
00:03:08,700 --> 00:03:11,640
qualsiasi tipo di pregiudizio in un sistema è un

79
00:03:11,640 --> 00:03:14,040
pregiudizio induttivo e quindi un pregiudizio induttivo

80
00:03:14,040 --> 00:03:16,080
può essere vagamente definito come una

81
00:03:16,080 --> 00:03:17,940
restrizione approssimativa di un insieme di

82
00:03:17,940 --> 00:03:19,440
ipotesi realizzabili quando si effettua

83
00:03:19,440 --> 00:03:22,440
la selezione del modello in modo più colloquiale noi  puoi chiamarlo in

84
00:03:22,440 --> 00:03:24,480
questo modo prima di vedere qualsiasi

85
00:03:24,480 --> 00:03:27,060
dato è una restrizione di cosa e come

86
00:03:27,060 --> 00:03:29,580
puoi imparare in modo molto ampio questo può

87
00:03:29,580 --> 00:03:32,519
includere qualsiasi cosa, dalla classe del modello alle

88
00:03:32,519 --> 00:03:34,739
procedure di ottimizzazione o persino agli

89
00:03:34,739 --> 00:03:37,379
iperparametri e in un certo senso definiscono davvero

90
00:03:37,379 --> 00:03:39,739


91
00:03:39,739 --> 00:03:43,140
cosa è possibile imparare  e definisce la

92
00:03:43,140 --> 00:03:44,819
generalizzazione nel senso che

93
00:03:44,819 --> 00:03:47,220
in realtà non è possibile generalizzare oltre un

94
00:03:47,220 --> 00:03:48,420
set di addestramento senza avere una

95
00:03:48,420 --> 00:03:50,220
soluzione induttiva questo è spiegato

96
00:03:50,220 --> 00:03:52,560
più approfonditamente in questo articolo da David

97
00:03:52,560 --> 00:03:55,500
Wolford quindi ciò che intendiamo per

98
00:03:55,500 --> 00:03:58,620
pregiudizi induttivi naturali sono i pregiudizi che

99
00:03:58,620 --> 00:04:00,000
derivano dalle restrizioni e

100
00:04:00,000 --> 00:04:02,040
limitazioni  che devono affrontare i

101
00:04:02,040 --> 00:04:04,500
sistemi naturali uh per la natura di dover

102
00:04:04,500 --> 00:04:06,900
vivere nel mondo reale ad esempio il

103
00:04:06,900 --> 00:04:08,459
cervello ha molti vincoli di efficienza

104
00:04:08,459 --> 00:04:10,439
e vincoli fisici per la natura della

105
00:04:10,439 --> 00:04:13,140
sua costruzione uh e seguendo questa

106
00:04:13,140 --> 00:04:14,580
logica questi vincoli stanno davvero

107
00:04:14,580 --> 00:04:16,620
giocando un ruolo nella nostra generalizzazione

108
00:04:16,620 --> 00:04:19,798
capacità che attualmente superano la moderna

109
00:04:19,798 --> 00:04:21,478
intelligenza artificiale di cui parleremo

110
00:04:21,478 --> 00:04:24,720
in seguito, quindi in questo discorso mi concentrerò

111
00:04:24,720 --> 00:04:27,540
specificamente su due tipi di strutture

112
00:04:27,540 --> 00:04:29,880
che il mio lavoro ha studiato, ovvero

113
00:04:29,880 --> 00:04:31,699
l'organizzazione topografica e le

114
00:04:31,699 --> 00:04:34,320
dinamiche spaziotemporali e prima di entrare

115
00:04:34,320 --> 00:04:36,120
nel mio lavoro, "  Fornirò un breve esempio del motivo per

116
00:04:36,120 --> 00:04:38,759
cui credo che la struttura naturale

117
00:04:38,759 --> 00:04:41,400
possa essere utile per ottenere la

118
00:04:41,400 --> 00:04:42,780
generalizzazione della struttura di cui parlavo

119
00:04:42,780 --> 00:04:44,280
prima,

120
00:04:44,280 --> 00:04:47,479
quindi il primo esempio viene dall'architettura

121
00:04:47,479 --> 00:04:49,199
neocognitiva della facciata di Hukushima

122
00:04:49,199 --> 00:04:51,479
degli anni '80

123
00:04:51,479 --> 00:04:53,520
che in realtà era stata costruita per

124
00:04:53,520 --> 00:04:55,440
affrontare direttamente  il problema della

125
00:04:55,440 --> 00:04:57,300
robustezza a questi piccoli spostamenti e

126
00:04:57,300 --> 00:04:59,759
deformazioni quindi nei documenti

127
00:04:59,759 --> 00:05:02,040
scrive di ispirazione dalle

128
00:05:02,040 --> 00:05:04,380
misurazioni della gerarchia e del pooling di pupilla e donnola

129
00:05:04,380 --> 00:05:06,780
al fine di ottenere robustezza

130
00:05:06,780 --> 00:05:08,699
a queste distorsioni e quindi se guardi

131
00:05:08,699 --> 00:05:11,160
la figura se scrive U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 e questi stanno per cellule semplici e

133
00:05:14,100 --> 00:05:16,919
complesse e quindi questo è un

134
00:05:16,919 --> 00:05:18,840
approccio abbastanza radicale all'epoca ma è

135
00:05:18,840 --> 00:05:20,699
servito davvero a migliorare la robustezza e i

136
00:05:20,699 --> 00:05:22,199
cambiamenti che stiamo affliggendo queste prime

137
00:05:22,199 --> 00:05:24,419
reti neurali artificiali e nel tempo

138
00:05:24,419 --> 00:05:25,860
queste idee sono state semplificate e

139
00:05:25,860 --> 00:05:28,919
astratte e  ovviamente ha prodotto le

140
00:05:28,919 --> 00:05:30,539
reti neurali convoluzionali, quindi

141
00:05:30,539 --> 00:05:32,759
oggi sappiamo che alla fine hanno guidato il successo della

142
00:05:32,759 --> 00:05:35,580
rivoluzione del deep learning, quindi questo

143
00:05:35,580 --> 00:05:36,960
è davvero un esempio di un

144
00:05:36,960 --> 00:05:39,479
pregiudizio induttivo naturale che ha raggiunto la

145
00:05:39,479 --> 00:05:42,120
generalizzazione della struttura, quindi per la nostra ricerca è

146
00:05:42,120 --> 00:05:43,919
davvero di massimo interesse cercare di

147
00:05:43,919 --> 00:05:45,900
capire cosa rende queste reti neurali convoluzionali  i modelli funzionano

148
00:05:45,900 --> 00:05:47,280
così bene

149
00:05:47,280 --> 00:05:49,320
ehm e vedi se questo principio può

150
00:05:49,320 --> 00:05:51,360
essere potenzialmente generalizzato per coprire

151
00:05:51,360 --> 00:05:53,940
trasformazioni e simmetrie più astratte più astratte,

152
00:05:53,940 --> 00:05:57,000


153
00:05:57,000 --> 00:06:00,360
quindi ciò che fa sì che una convoluzione raggiunga questa

154
00:06:00,360 --> 00:06:02,060
generalizzazione della struttura

155
00:06:02,060 --> 00:06:04,440
in modo intuitivo puoi vedere che ciò viene fatto

156
00:06:04,440 --> 00:06:06,720
applicando lo stesso filtro in o o

157
00:06:06,720 --> 00:06:08,699
estrattore di funzionalità in  varie

158
00:06:08,699 --> 00:06:10,680
posizioni spaziali quindi qui vediamo un singolo

159
00:06:10,680 --> 00:06:12,660
filtro convoluzionale applicato in

160
00:06:12,660 --> 00:06:14,820
tutte le posizioni di un'immagine ciò significa

161
00:06:14,820 --> 00:06:16,259
che non importa dove sia il tuo input, se

162
00:06:16,259 --> 00:06:18,000
è al centro

163
00:06:18,000 --> 00:06:20,400
dell'immagine o a destra, avrai

164
00:06:20,400 --> 00:06:22,080
esattamente le stesse caratteristiche  con

165
00:06:22,080 --> 00:06:23,580
un'eccezione saranno spostati in modo equivalente,

166
00:06:23,580 --> 00:06:24,660


167
00:06:24,660 --> 00:06:26,819
quindi matematicamente questo tipo di mappatura

168
00:06:26,819 --> 00:06:29,160
è chiamato omomorfismo e preserva

169
00:06:29,160 --> 00:06:30,840
la struttura algebrica dello

170
00:06:30,840 --> 00:06:33,180
spazio di input e dello spazio di output in questo caso

171
00:06:33,180 --> 00:06:35,580
è rispetto alla traduzione e ad un

172
00:06:35,580 --> 00:06:37,440
livello semplice qualcosa del genere

173
00:06:37,440 --> 00:06:38,880
quello che sarà importante ricordare per

174
00:06:38,880 --> 00:06:40,259
il resto di questo discorso è che possiamo

175
00:06:40,259 --> 00:06:42,300
verificare gli omomorfismi del nostro

176
00:06:42,300 --> 00:06:45,000
estrattore di caratteristiche se possiamo vedere che c'è

177
00:06:45,000 --> 00:06:46,740
questa commutazione di comunità con il

178
00:06:46,740 --> 00:06:49,560
diagramma commutativo Trasformazioni e

179
00:06:49,560 --> 00:06:51,720
quindi possiamo scriverlo anche algebricamente

180
00:06:51,720 --> 00:06:53,759
mostrando che l'estrattore di caratteristiche  f

181
00:06:53,759 --> 00:06:55,080
commuta con l'

182
00:06:55,080 --> 00:06:57,000
operatore di trasformazione t

183
00:06:57,000 --> 00:06:58,919
e fondamentalmente quello che vogliamo è che

184
00:06:58,919 --> 00:07:00,600
non ci sia differenza tra prima

185
00:07:00,600 --> 00:07:02,639
estrarre le caratteristiche e poi

186
00:07:02,639 --> 00:07:04,740
eseguire la trasformazione o

187
00:07:04,740 --> 00:07:06,240
eseguire la trasformazione e poi

188
00:07:06,240 --> 00:07:08,639
estrarre le caratteristiche quindi la sfida

189
00:07:08,639 --> 00:07:10,199
è che ad oggi

190
00:07:10,199 --> 00:07:11,880
non lo sappiamo veramente  come costruire

191
00:07:11,880 --> 00:07:13,620
omomorfismi rispetto a

192
00:07:13,620 --> 00:07:15,240
trasformazioni più complesse che vediamo

193
00:07:15,240 --> 00:07:18,060
nel mondo reale, ad esempio il nostro cervello è

194
00:07:18,060 --> 00:07:20,099
in grado di gestire i cambiamenti nell'illuminazione e nelle

195
00:07:20,099 --> 00:07:22,259
stagioni in modo naturale,

196
00:07:22,259 --> 00:07:24,599
quindi qui vediamo l'illuminazione sul viso di una persona

197
00:07:24,599 --> 00:07:26,160
o il cambio delle stagioni che possiamo

198
00:07:26,160 --> 00:07:27,840
dire che è  stesso volto o la stessa

199
00:07:27,840 --> 00:07:29,880
strada ma non sappiamo costruire

200
00:07:29,880 --> 00:07:31,319
modelli che rispettino queste

201
00:07:31,319 --> 00:07:33,180
Trasformazioni e quindi ci rende difficile

202
00:07:33,180 --> 00:07:35,520
costruire sistemi che le gestiscano in

203
00:07:35,520 --> 00:07:37,620
modo robusto e prevedibile

204
00:07:37,620 --> 00:07:40,259
per dare un esempio ancora più astratto di

205
00:07:40,259 --> 00:07:41,759
ciò che io  intendo con questo e le potenziali

206
00:07:41,759 --> 00:07:43,620
ripercussioni negative dei modelli che

207
00:07:43,620 --> 00:07:45,440
non gestiscono la simmetria Le trasformazioni

208
00:07:45,440 --> 00:07:48,060
considerano i moderni programmi di generazione di testo in immagini

209
00:07:48,060 --> 00:07:50,520
quindi in questo esempio ho chiesto a

210
00:07:50,520 --> 00:07:53,940
Dolly di generare l'immagine di un

211
00:07:53,940 --> 00:07:55,620
orsacchiotto sulla luna e lo fa

212
00:07:55,620 --> 00:07:57,180
incredibilmente bene  probabilmente meglio

213
00:07:57,180 --> 00:08:00,960
di quanto avrei potuto, ha una trama

214
00:08:00,960 --> 00:08:03,360
incredibilmente dettagliata, tuttavia se ti chiedo

215
00:08:03,360 --> 00:08:05,340
di fare qualcosa che vedo è

216
00:08:05,340 --> 00:08:08,039
concettualmente più semplice come disegnare un

217
00:08:08,039 --> 00:08:10,560
cubo blu sopra un cubo rosso, non riuscirà a farlo

218
00:08:10,560 --> 00:08:13,380
e per me non sembra intuitivo

219
00:08:13,380 --> 00:08:15,300
dal momento che  il secondo compito sembra

220
00:08:15,300 --> 00:08:18,180
molto più semplice ma quello che sto

221
00:08:18,180 --> 00:08:19,860
sostenendo è che il motivo per cui questo è

222
00:08:19,860 --> 00:08:21,599
sorprendenteèesattamente lo stesso motivo

223
00:08:21,599 --> 00:08:23,580
per cui l'esempio di traduzione di amnest era

224
00:08:23,580 --> 00:08:25,560
sorprendente c'è questa

225
00:08:25,560 --> 00:08:28,020
trasformazione di simmetria in corso qui vale a dire la

226
00:08:28,020 --> 00:08:29,400
trasformazione tra questi

227
00:08:29,400 --> 00:08:31,740
oggetti complessi di un orsacchiotto  e la luna e

228
00:08:31,740 --> 00:08:34,320
questi semplici oggetti di cubi che

229
00:08:34,320 --> 00:08:36,360
intuitivamente ci aspettiamo che la rete sia

230
00:08:36,360 --> 00:08:38,820
in grado di gestire e rispettare e vediamo

231
00:08:38,820 --> 00:08:40,919
che non è così, proprio come il

232
00:08:40,919 --> 00:08:43,380
lavoro di Fukushima ha mostrato che queste

233
00:08:43,380 --> 00:08:46,200
strutture naturali di gerarchia e

234
00:08:46,200 --> 00:08:47,700
condivisione del nostro sistema visivo sono

235
00:08:47,700 --> 00:08:49,680
efficace per fare generalizzazioni a

236
00:08:49,680 --> 00:08:52,380
piccole trasformazioni Sostengo che una

237
00:08:52,380 --> 00:08:54,060
struttura di livello potenzialmente superiore potrebbe

238
00:08:54,060 --> 00:08:55,740
essere necessaria per risolvere questi

239
00:08:55,740 --> 00:08:58,200
problemi di generalizzazione astratta

240
00:08:58,200 --> 00:09:01,380
e quindi la domanda che sto

241
00:09:01,380 --> 00:09:04,380
studiando e che mi sto chiedendo è quale

242
00:09:04,380 --> 00:09:06,060
potrebbe essere questa struttura e come

243
00:09:06,060 --> 00:09:08,640
implementarla  questo in un'architettura di rete neurale artificiale

244
00:09:08,640 --> 00:09:10,080
che può effettivamente

245
00:09:10,080 --> 00:09:14,120
essere utilizzata per eseguire calcoli,

246
00:09:14,880 --> 00:09:17,700
quindi per iniziare a rispondere passerò

247
00:09:17,700 --> 00:09:19,680
alla mia prima linea di lavoro

248
00:09:19,680 --> 00:09:22,380
sull'organizzazione topografica

249
00:09:22,380 --> 00:09:25,260
in modo che l'organizzazione topografica venga osservata

250
00:09:25,260 --> 00:09:27,060
ampiamente in tutto il cervello dalle

251
00:09:27,060 --> 00:09:29,760
aree a livello di zaffiro della corteccia visiva primaria  e

252
00:09:29,760 --> 00:09:31,500
può essere descritta molto vagamente come questa

253
00:09:31,500 --> 00:09:33,540
proprietà per cui i neuroni vicini tra

254
00:09:33,540 --> 00:09:35,760
loro tendono a rispondere a

255
00:09:35,760 --> 00:09:38,220
cose simili, ad esempio a sinistra mostriamo

256
00:09:38,220 --> 00:09:39,720
la preferenza codificata a colori di ciascun

257
00:09:39,720 --> 00:09:42,959
neurone nella corteccia digitale primaria come

258
00:09:42,959 --> 00:09:45,360
risposta alle linee orientate  e vediamo

259
00:09:45,360 --> 00:09:46,740
questo insieme di selettività che varia dolcemente,

260
00:09:46,740 --> 00:09:48,779
un altro tipo di

261
00:09:48,779 --> 00:09:50,580
organizzazione è noto come organizzazione tematica della retina

262
00:09:50,580 --> 00:09:52,560
in cui i neuroni vicini nella

263
00:09:52,560 --> 00:09:54,600
corteccia visiva tendono a rispondere ai

264
00:09:54,600 --> 00:09:56,399
campi recettivi vicini,

265
00:09:56,399 --> 00:09:58,560
tuttavia questa organizzazione non è limitata

266
00:09:58,560 --> 00:10:01,080
a queste caratteristiche di basso livello ne estende alcune

267
00:10:01,080 --> 00:10:02,519
più complesse  caratteristiche come quelle

268
00:10:02,519 --> 00:10:05,459
presenti nei volti, negli oggetti o nei luoghi

269
00:10:05,459 --> 00:10:07,920
e questo si riferisce alle cosiddette

270
00:10:07,920 --> 00:10:10,080
aree funzionalmente specifiche del cervello

271
00:10:10,080 --> 00:10:12,779
come l'area fusiforme del viso FFA e

272
00:10:12,779 --> 00:10:15,420
l'area parachippocampale del viso PPA,

273
00:10:15,420 --> 00:10:19,200
quindi in questo lavoro l'idea principale è ancora una volta

274
00:10:19,200 --> 00:10:21,300
che forse questo

275
00:10:21,300 --> 00:10:23,580
organizzazione topografica in un certo senso che è

276
00:10:23,580 --> 00:10:25,080
intimamente correlata

277
00:10:25,080 --> 00:10:27,980
all'operazione di convoluzione e all'architettura di Fukushima

278
00:10:27,980 --> 00:10:30,660
possiamo forse generalizzare i benefici di

279
00:10:30,660 --> 00:10:33,420
questa a trasformazioni più astratte in

280
00:10:33,420 --> 00:10:34,920
altre parole imparare come costruire

281
00:10:34,920 --> 00:10:36,839
omomorfismi più complessi che non possiamo fare

282
00:10:36,839 --> 00:10:38,940
sai che non possiamo  fallo analiticamente adesso,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
solo per dimostrare che non siamo

285
00:10:42,480 --> 00:10:44,760
completamente pazzi con questa idea uh

286
00:10:44,760 --> 00:10:46,320
c'è del lavoro precedente in questo campo

287
00:10:46,320 --> 00:10:49,880
di persone come uh Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden nei primi anni '90 e 2000

289
00:10:54,060 --> 00:10:55,800
e hanno studiato come

290
00:10:55,800 --> 00:10:57,720
potrebbe essere l'organizzazione topografica  utile per apprendere

291
00:10:57,720 --> 00:11:01,320
in varianze principalmente in modelli lineari, quindi

292
00:11:01,320 --> 00:11:03,060
la domanda per noi quando siamo entrati nello

293
00:11:03,060 --> 00:11:04,680
spazio è qual è il

294
00:11:04,680 --> 00:11:07,140
meccanismo astratto più scalabile che può essere sfruttato

295
00:11:07,140 --> 00:11:08,880
da questi approcci che possiamo

296
00:11:08,880 --> 00:11:10,800
integrare nelle moderne

297
00:11:10,800 --> 00:11:12,959
architetture di rete neurale profonda e alla fine abbiamo

298
00:11:12,959 --> 00:11:15,000
optato per un  approccio di modellazione generativa

299
00:11:15,000 --> 00:11:16,260
che penso potrebbe essere

300
00:11:16,260 --> 00:11:17,519
interessante per le persone di questa

301
00:11:17,519 --> 00:11:18,779
comunità

302
00:11:18,779 --> 00:11:21,779
e che ci consente quindi di correlarlo

303
00:11:21,779 --> 00:11:23,579
più da vicino all'analisi dei componenti topografici indipendenti

304
00:11:23,579 --> 00:11:26,040
con l'idea di base

305
00:11:26,040 --> 00:11:28,320
che possiamo apprendere una

306
00:11:28,320 --> 00:11:30,660
caratteristica topografica dello Spazio imponendo una

307
00:11:30,660 --> 00:11:32,940
distribuzione topografica a priori su  le nostre variabili latenti

308
00:11:32,940 --> 00:11:34,440


309
00:11:34,440 --> 00:11:37,079
quindi, solo per dare un breve background,

310
00:11:37,079 --> 00:11:39,120
presumo che la maggior parte delle persone abbia già familiarità

311
00:11:39,120 --> 00:11:40,440
con questo,

312
00:11:40,440 --> 00:11:42,540
ma il tipo di presupposto generale è

313
00:11:42,540 --> 00:11:44,339
che il cervello sia un modello generativo e

314
00:11:44,339 --> 00:11:45,720
questa idea in un certo senso può essere

315
00:11:45,720 --> 00:11:48,060
attribuita a Helmholts del 19°

316
00:11:48,060 --> 00:11:50,459
secolo uh  dove diceva che ciò che

317
00:11:50,459 --> 00:11:52,140
vediamo è la soluzione ad un

318
00:11:52,140 --> 00:11:54,420
problema computazionale il nostro cervello calcola le

319
00:11:54,420 --> 00:11:56,519
cause più probabili dell'assorbimento di fotoni

320
00:11:56,519 --> 00:11:59,519
nei nostri occhi e quindi questo è un esempio

321
00:11:59,519 --> 00:12:01,920
se ti mostro questa immagine

322
00:12:01,920 --> 00:12:03,720
la riconosci immediatamente come un globo con una certa

323
00:12:03,720 --> 00:12:05,760
curvatura comunque sia  potrebbe anche

324
00:12:05,760 --> 00:12:07,620
essere un disco con una

325
00:12:07,620 --> 00:12:09,600
prospettiva distorta quindi è così che

326
00:12:09,600 --> 00:12:12,660
otteniamo le illusioni ottiche o le nostre immagini così

327
00:12:12,660 --> 00:12:14,880
come questa il tuo cervello deduce che

328
00:12:14,880 --> 00:12:17,100
qui c'è un cubo a causa della

329
00:12:17,100 --> 00:12:18,720
struttura ma in realtà è solo un

330
00:12:18,720 --> 00:12:19,860
pezzo di carta piatto

331
00:12:19,860 --> 00:12:22,740
quindi puoi pensare a questo

332
00:12:22,740 --> 00:12:24,480
aspetto del modello generativo che è una specie di

333
00:12:24,480 --> 00:12:26,160
programma di grafica inversa

334
00:12:26,160 --> 00:12:28,140
nel programma le proprietà astratte

335
00:12:28,140 --> 00:12:30,660
della sfera sono note la posizione la

336
00:12:30,660 --> 00:12:32,820
dimensione l'illuminazione e queste vengono utilizzate per

337
00:12:32,820 --> 00:12:34,560
proiettare la sfera per creare l'

338
00:12:34,560 --> 00:12:37,440
immagine 2D renderizzata  quindi in effetti ciò che

339
00:12:37,440 --> 00:12:40,019
Humboldt e altri dicono è che

340
00:12:40,019 --> 00:12:41,940
come modello generativo il cervello sta

341
00:12:41,940 --> 00:12:43,680
effettivamente cercando di invertire questo

342
00:12:43,680 --> 00:12:45,959
processo generativo e di fare inferenze

343
00:12:45,959 --> 00:12:48,300
e dedurre le cause sottostanti delle nostre

344
00:12:48,300 --> 00:12:49,680
sensazioni,

345
00:12:49,680 --> 00:12:51,600
quindi il motivo per cui mi sto soffermando su

346
00:12:51,600 --> 00:12:53,100
questo punto è che c'è

347
00:12:53,100 --> 00:12:55,440
si parla molto di modelli generativi oggi

348
00:12:55,440 --> 00:12:57,180
e non sto necessariamente parlando solo

349
00:12:57,180 --> 00:12:59,639
di generare immagini o belle

350
00:12:59,639 --> 00:13:01,260
immagini.

351
00:13:01,260 --> 00:13:03,120
Voglio davvero intendere

352
00:13:03,120 --> 00:13:07,920
una struttura per l'apprendimento non supervisionato,

353
00:13:07,920 --> 00:13:10,019
quindi per entrare un po' più nei

354
00:13:10,019 --> 00:13:11,700
dettagli cosa intendo con a

355
00:13:11,700 --> 00:13:14,279
topografici a priori, quindi i modelli generativi

356
00:13:14,279 --> 00:13:16,019
sono tipicamente descritti come una

357
00:13:16,019 --> 00:13:18,720
distribuzione congiunta sulle osservazioni X e sulle

358
00:13:18,720 --> 00:13:21,720
variabili latenti che chiameremo Z uh

359
00:13:21,720 --> 00:13:23,940
e questo è tipicamente fattorizzato o un

360
00:13:23,940 --> 00:13:25,620
modo in cui ciò viene fatto è fattorizzato in

361
00:13:25,620 --> 00:13:28,440
termini di una P a priori di Z e questo è vero

362
00:13:28,440 --> 00:13:30,420
modello generativo modello generativo condizionale

363
00:13:30,420 --> 00:13:33,420
P di x dato Z e quindi un modo in cui

364
00:13:33,420 --> 00:13:34,980
possiamo pensare a questo è che si

365
00:13:34,980 --> 00:13:37,019
può vedere che il precedente codifica le

366
00:13:37,019 --> 00:13:38,880
penalità relative per ogni tipo di codice che viene

367
00:13:38,880 --> 00:13:41,279
prodotto quando invertiamo il nostro

368
00:13:41,279 --> 00:13:42,899
modello generativo questo è chiamato Computing the

369
00:13:42,899 --> 00:13:45,920
posteriore di Z dato X

370
00:13:45,920 --> 00:13:49,139
e quindi per sviluppare uno spazio latente topografico

371
00:13:49,139 --> 00:13:50,639
vogliamo introdurre una sorta di

372
00:13:50,639 --> 00:13:53,279
precedente topografico che è stato o

373
00:13:53,279 --> 00:13:55,620
che questo lavoro topografico dell'ICA ha mostrato

374
00:13:55,620 --> 00:13:57,720
equivale a qualcosa come una

375
00:13:57,720 --> 00:13:59,700
penalità di scarsità di gruppo

376
00:13:59,700 --> 00:14:01,500
in modo che le persone possano avere familiarità con i tipici

377
00:14:01,500 --> 00:14:03,180
penalità di scarsità

378
00:14:03,180 --> 00:14:04,560
dall'analisi dell'apprendimento indipendente vuoi che le tue

379
00:14:04,560 --> 00:14:06,540
attivazioni siano sparse, il che significa che molte di

380
00:14:06,540 --> 00:14:09,420
esse sono pari a zero wow e quindi potrebbe

381
00:14:09,420 --> 00:14:10,680
assomigliare a questo hai un gruppo di

382
00:14:10,680 --> 00:14:12,300
quadrati blu che sono attivi ma la maggior parte di

383
00:14:12,300 --> 00:14:14,459
essi non è attiva ma in particolare

384
00:14:14,459 --> 00:14:16,740
con quelli del gruppo  Penalità universitaria vogliamo che

385
00:14:16,740 --> 00:14:18,839
questi priori assegnino una probabilità inferiore

386
00:14:18,839 --> 00:14:21,600
a queste attivazioni sparse distribuite

387
00:14:21,600 --> 00:14:24,720
e una probabilità maggiore a queste

388
00:14:24,720 --> 00:14:26,940
rappresentazioni raggruppate e densamente imballate, puoi

389
00:14:26,940 --> 00:14:28,860
anche pensare a questo come una penalità più elevata

390
00:14:28,860 --> 00:14:30,720
quando le cose sono distribuite una

391
00:14:30,720 --> 00:14:33,540
penalità inferiore quando le cose sono più vicine tra loro,

392
00:14:33,540 --> 00:14:36,380
quindi di nuovo uh  questo può essere scritto in

393
00:14:36,380 --> 00:14:39,060
modo astratto in questo modo, ma voglio fare una

394
00:14:39,060 --> 00:14:41,160
teoria secondo cui questi neuroni, ognuno di

395
00:14:41,160 --> 00:14:42,779
questi quadrati qui rappresenta una specie di

396
00:14:42,779 --> 00:14:44,220
neurone nel nostro modello e sono

397
00:14:44,220 --> 00:14:46,560
organizzati in questa griglia 2D, quindi quando

398
00:14:46,560 --> 00:14:48,120
parliamo di raggruppamento intendiamo davvero

399
00:14:48,120 --> 00:14:50,820
raggruppandosi in quella topologia 2D

400
00:14:50,820 --> 00:14:53,100
quindi una cosa davvero interessante

401
00:14:53,100 --> 00:14:55,560
e importante è che questi

402
00:14:55,560 --> 00:14:57,779
priori non ci danno solo

403
00:14:57,779 --> 00:15:00,540
un'organizzazione topografica ma sono stati anche notati

404
00:15:00,540 --> 00:15:02,459
o studiati da persone come

405
00:15:02,459 --> 00:15:05,760
Erosi Marcelli e Bruno anche per

406
00:15:05,760 --> 00:15:07,740
adattarsi effettivamente  le statistiche dei

407
00:15:07,740 --> 00:15:08,959
dati naturali sono migliori,

408
00:15:08,959 --> 00:15:11,180
in particolare delle immagini naturali,

409
00:15:11,180 --> 00:15:14,100
hanno dimostrato che utilizzando questo tipo di a

410
00:15:14,100 --> 00:15:16,139
priori si ottiene effettivamente un insieme più sparso di

411
00:15:16,139 --> 00:15:18,839
attivazioni, il che significa che il precedente si adatta

412
00:15:18,839 --> 00:15:20,459
un po' meglio al vero processo generativo

413
00:15:20,459 --> 00:15:22,620
e, come sappiamo, il cervello ha

414
00:15:22,620 --> 00:15:24,779
un alto grado di scarsità e si

415
00:15:24,779 --> 00:15:26,339
ritiene che questo sia molto rilevante per

416
00:15:26,339 --> 00:15:28,620
l'efficienza,

417
00:15:28,620 --> 00:15:30,839
quindi per entrare un po' più nei

418
00:15:30,839 --> 00:15:32,760
dettagli per implementare questo tipo di

419
00:15:32,760 --> 00:15:35,160
gruppo sparso prima utilizziamo un

420
00:15:35,160 --> 00:15:37,320
modello generativo gerarchico e questo è

421
00:15:37,320 --> 00:15:39,060
fondamentalmente introdotto da alcuni dei

422
00:15:39,060 --> 00:15:41,339
lavoro ICA topografico,

423
00:15:41,339 --> 00:15:43,320
l'idea è che tu abbia una

424
00:15:43,320 --> 00:15:45,000
variabile latente di livello superiore U che

425
00:15:45,000 --> 00:15:47,820
regola simultaneamente la varianza di

426
00:15:47,820 --> 00:15:50,279
più variabili di livello inferiore T ed

427
00:15:50,279 --> 00:15:52,440
è così che otteniamo la scarsità di gruppo,

428
00:15:52,440 --> 00:15:55,440
quindi per ottenere l'organizzazione topografica

429
00:15:55,440 --> 00:15:56,760
puoi avere più di queste

430
00:15:56,760 --> 00:15:59,339
variabili latenti usate leggermente  si sovrappongono ai

431
00:15:59,339 --> 00:16:02,519
loro campi di influenza in modo che siano i loro

432
00:16:02,519 --> 00:16:04,260
quartieri e possiamo chiamarli

433
00:16:04,260 --> 00:16:05,699
e questo ti darà questa

434
00:16:05,699 --> 00:16:07,440
struttura di correlazione fluida per il tuo atto che stai

435
00:16:07,440 --> 00:16:09,899
cercando, quindi prendi l'intuizione per questo,

436
00:16:09,899 --> 00:16:12,060
vedi che questa variabile T in basso

437
00:16:12,060 --> 00:16:14,279
qui in basso non sta ottenendo  qualsiasi input

438
00:16:14,279 --> 00:16:17,160
da questa U in alto ma condivide

439
00:16:17,160 --> 00:16:19,139
una variabile u con questa T al centro

440
00:16:19,139 --> 00:16:21,300
quindi è come se condividessero delle varianti,

441
00:16:21,300 --> 00:16:22,980
condividono alcuni componenti con

442
00:16:22,980 --> 00:16:24,959
i loro vicini ma non tutti i componenti

443
00:16:24,959 --> 00:16:26,579
e questo è dovuto proprio a questa

444
00:16:26,579 --> 00:16:28,079
connettività locale di  queste variabili di livello superiore,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
quindi per semplificare il modo in cui utilizziamo un

447
00:16:33,180 --> 00:16:34,980
modello generativo, torniamo a una

448
00:16:34,980 --> 00:16:37,440
singola variabile U e la sfida in

449
00:16:37,440 --> 00:16:38,940
questo tipo di architettura che ha reso

450
00:16:38,940 --> 00:16:42,360
difficile per molti anni è come si

451
00:16:42,360 --> 00:16:44,579
deduce il posteriore approssimativo

452
00:16:44,579 --> 00:16:47,579
su queste variabili intermedie in

453
00:16:47,579 --> 00:16:50,100
questa architettura gerarchica e questo

454
00:16:50,100 --> 00:16:52,560
non è molto semplice, quindi i

455
00:16:52,560 --> 00:16:54,420
lavori precedenti hanno utilizzato euristiche sviluppate per

456
00:16:54,420 --> 00:16:56,699
modelli lineari e nel nostro lavoro abbiamo scoperto

457
00:16:56,699 --> 00:16:58,680
che questo in realtà non si estende alle

458
00:16:58,680 --> 00:17:01,199
architetture di reti neurali moderne, quindi in realtà

459
00:17:01,199 --> 00:17:02,880
la nostra intuizione è quella di sfruttare a

460
00:17:02,880 --> 00:17:04,760
fattorizzazione una

461
00:17:04,760 --> 00:17:07,640
riparametrizzazione specifica di questa distribuzione

462
00:17:07,640 --> 00:17:10,380
e quindi questa parametrizzazione

463
00:17:10,380 --> 00:17:12,419
si ottiene specificatamente definendo la

464
00:17:12,419 --> 00:17:14,579
priorità cosiddetta

465
00:17:14,579 --> 00:17:16,319
miscela di scala gaussiana, il che significa che la nostra

466
00:17:16,319 --> 00:17:19,140
distribuzione condizionale di T dato U è

467
00:17:19,140 --> 00:17:21,179
in realtà una distribuzione normale in cui

468
00:17:21,179 --> 00:17:24,299
la varianza è definita da questa variabile

469
00:17:24,299 --> 00:17:27,720
U e per alcune scelte di U questa

470
00:17:27,720 --> 00:17:29,340
distribuzione è effettivamente sparsa e

471
00:17:29,340 --> 00:17:31,980
comprende una gamma di distribuzioni

472
00:17:31,980 --> 00:17:33,780
come laplossiane, un seme e

473
00:17:33,780 --> 00:17:36,299
distribuzioni T un modo per definirla è

474
00:17:36,299 --> 00:17:38,940
una miscela su scala gaussiana emette una

475
00:17:38,940 --> 00:17:40,919
particolare ri-parametrizzazione del reframe

476
00:17:40,919 --> 00:17:42,900
in termini di variabili casuali gaussiane indipendenti

477
00:17:42,900 --> 00:17:45,720
Z e U quindi nello specifico

478
00:17:45,720 --> 00:17:48,840
vediamo che questa variabile T che

479
00:17:48,840 --> 00:17:50,760
originariamente era abbastanza complessa è in realtà

480
00:17:50,760 --> 00:17:52,799
solo il prodotto di un gruppo di

481
00:17:52,799 --> 00:17:54,840
variabili casuali gaussiane che ora sanno come

482
00:17:54,840 --> 00:17:57,660
lavorare in modo molto più efficiente uh nei

483
00:17:57,660 --> 00:18:00,120
modelli generativi, in particolare quello che

484
00:18:00,120 --> 00:18:02,039
siamo  faremo è in modo che possiamo

485
00:18:02,039 --> 00:18:04,020
effettivamente ottenere posteriori approssimativi per

486
00:18:04,020 --> 00:18:06,720
U e Z separatamente e quindi fare una

487
00:18:06,720 --> 00:18:08,640
combinazione deterministica di essi

488
00:18:08,640 --> 00:18:10,020
per calcolare la nostra

489
00:18:10,020 --> 00:18:13,140
variabile topografica T e questo è molto più semplice da fare

490
00:18:13,140 --> 00:18:15,500
senza entrare in troppi dettagli

491
00:18:15,500 --> 00:18:17,700
il metodo che  che abbiamo deciso di utilizzare è quello che è

492
00:18:17,700 --> 00:18:18,600
noto come

493
00:18:18,600 --> 00:18:20,640
autocodificatore variazionale che sfrutta le tecniche

494
00:18:20,640 --> 00:18:23,220
dell'inferenza variazionale per ricavare un

495
00:18:23,220 --> 00:18:24,780
limite inferiore sulla probabilità

496
00:18:24,780 --> 00:18:26,940
permettendoci di parametrizzare questi

497
00:18:26,940 --> 00:18:29,400
posteriori approssimativi con potenti reti neurali profonde non lineari

498
00:18:29,400 --> 00:18:30,960
e ottimizzarle con la

499
00:18:30,960 --> 00:18:33,240
discesa del gradiente che questo farà  avere

500
00:18:33,240 --> 00:18:34,440
familiarità con la community di inferenza attiva,

501
00:18:34,440 --> 00:18:36,900
ma in realtà quello che abbiamo fatto è

502
00:18:36,900 --> 00:18:38,760
invece di avere un singolo codificatore nel

503
00:18:38,760 --> 00:18:41,640
decodificatore come un tipico baes, ora abbiamo

504
00:18:41,640 --> 00:18:43,799
due codificatori uno per te e uno per Z

505
00:18:43,799 --> 00:18:46,200
separatamente e quindi li combiniamo in

506
00:18:46,200 --> 00:18:48,419
questo modo deterministico per  costruisci la

507
00:18:48,419 --> 00:18:51,660
nostra variabile topografica T se vedi

508
00:18:51,660 --> 00:18:53,100
che questa è in realtà la

509
00:18:53,100 --> 00:18:54,900
costruzione della distribuzione T di uno studente

510
00:18:54,900 --> 00:18:57,620
da gaussiani

511
00:18:57,620 --> 00:19:00,480
e poi possiamo collegarlo lo facciamo

512
00:19:00,480 --> 00:19:03,600
prima di decodificarlo e uh e poi massimizziamo

513
00:19:03,600 --> 00:19:05,820
del tutto la probabilità dei dati quindi

514
00:19:05,820 --> 00:19:07,260
questo è il gomito  le prove del

515
00:19:07,260 --> 00:19:09,360
limite inferiore abbondano sulla verosimiglianza dei

516
00:19:09,360 --> 00:19:12,600
dati ed è in realtà molto simile

517
00:19:12,600 --> 00:19:15,720
all'energia libera variazionale utilizzata nella

518
00:19:15,720 --> 00:19:18,299
comunità di ingresso attivo,

519
00:19:18,299 --> 00:19:20,520
quindi eliminati questi dettagli

520
00:19:20,520 --> 00:19:22,500
ciò che è veramente interessante è ciò che

521
00:19:22,500 --> 00:19:23,940
accade quando addestriamo questo

522
00:19:23,940 --> 00:19:26,580
modello generativo che  ha una penalità di scarsità di gruppo relativamente semplice

523
00:19:26,580 --> 00:19:29,460
nel suo spazio latente e

524
00:19:29,460 --> 00:19:30,720
vogliamo vedere cosa sta

525
00:19:30,720 --> 00:19:32,700
imparando in termini di organizzazione dei

526
00:19:32,700 --> 00:19:34,980
Futures e prima iniziamo con il

527
00:19:34,980 --> 00:19:36,480
set di dati più semplice possibile, abbiamo uno

528
00:19:36,480 --> 00:19:38,580
sfondo nero con quadrati bianchi in

529
00:19:38,580 --> 00:19:41,280
posizioni XY casuali  e se addestriamo il nostro

530
00:19:41,280 --> 00:19:42,780
autocodificatore con questa penalità di scarsità di gruppo

531
00:19:42,780 --> 00:19:44,760
e poi guardiamo i

532
00:19:44,760 --> 00:19:47,640
vettori di peso del nostro decodificatore che

533
00:19:47,640 --> 00:19:49,020
stiamo tracciando in blu anche qui

534
00:19:49,020 --> 00:19:52,520
organizzati su questa griglia 2D vediamo che

535
00:19:52,520 --> 00:19:54,780
in effetti imparano ad essere organizzati

536
00:19:54,780 --> 00:19:57,539
secondo spaziali  posizione, quindi questo

537
00:19:57,539 --> 00:19:59,580
può essere visto come simile ai

538
00:19:59,580 --> 00:20:01,799
campi recettivi convoluzionali o il campo recettivo

539
00:20:01,799 --> 00:20:04,679
di ciascun neurone è in realtà dato dal

540
00:20:04,679 --> 00:20:09,059
tipo di input nella sua posizione

541
00:20:09,059 --> 00:20:10,860
e questo ha senso intuitivamente dal

542
00:20:10,860 --> 00:20:13,140
punto di vista della scarsità del gruppo poiché

543
00:20:13,140 --> 00:20:15,480
per ogni data regione da evidenziare come

544
00:20:15,480 --> 00:20:17,700
in  giallo qui i filtri in un dato

545
00:20:17,700 --> 00:20:19,260
gruppo sono molto più altamente correlati

546
00:20:19,260 --> 00:20:20,880
hanno questi campi ricettivi sovrapposti

547
00:20:20,880 --> 00:20:23,460
rispetto ad altre posizioni casuali quindi

548
00:20:23,460 --> 00:20:25,020
essenzialmente vediamo che il nostro modello sta

549
00:20:25,020 --> 00:20:27,840
imparando a raggruppare insieme le attività di attività

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
uh in una sorta di foglio verticale simulato

552
00:20:32,059 --> 00:20:34,260
in base alle correlazioni in  il

553
00:20:34,260 --> 00:20:36,840
set di dati quindi invece che in convoluzione

554
00:20:36,840 --> 00:20:38,580
in cui stai effettivamente legando il peso

555
00:20:38,580 --> 00:20:40,860
e stai specificando manualmente Voglio

556
00:20:40,860 --> 00:20:42,539
copiare questo peso ovunque potresti

557
00:20:42,539 --> 00:20:44,220
forse pensarlo come un

558
00:20:44,220 --> 00:20:45,500
tempo di attesa approssimativo

559
00:20:45,500 --> 00:20:48,120
e in realtà lo stiamo imparando dalla

560
00:20:48,120 --> 00:20:49,620
correlazione  struttura del set di dati

561
00:20:49,620 --> 00:20:51,660
stesso e solo per dare un po'

562
00:20:51,660 --> 00:20:54,120
più di ispirazione biologica per

563
00:20:54,120 --> 00:20:56,280
questo e sappiamo che la retinotopia è

564
00:20:56,280 --> 00:20:58,020
presente nel cervello questo è un esempio

565
00:20:58,020 --> 00:21:02,460
di retinotopia nella corteccia visiva e

566
00:21:02,460 --> 00:21:04,500
puoi vedere se mostri al macaco un

567
00:21:04,500 --> 00:21:06,780
un'immagine come questa viene proiettata in

568
00:21:06,780 --> 00:21:08,520
questa uh

569
00:21:08,520 --> 00:21:11,700
topologia preservando lo spazio effettivamente

570
00:21:11,700 --> 00:21:13,740
sulla superficie della corteccia

571
00:21:13,740 --> 00:21:16,080
quindi l'idea è che l'

572
00:21:16,080 --> 00:21:18,419
organizzazione topografica e persino l'apprendimento

573
00:21:18,419 --> 00:21:21,299
dell'organizzazione topografica preservano le

574
00:21:21,299 --> 00:21:26,160
correlazioni di input dei nostri set di dati uh e e

575
00:21:26,160 --> 00:21:28,679
potenzialmente uh questo potrebbe essere utile

576
00:21:28,679 --> 00:21:30,840
per  generalizzando ulteriormente queste idee

577
00:21:30,840 --> 00:21:32,340
quindi, come ho detto

578
00:21:32,340 --> 00:21:34,679
all'inizio, sarebbe ancora meglio se

579
00:21:34,679 --> 00:21:37,200
potessimo imparare qualcosa di più della

580
00:21:37,200 --> 00:21:39,320
semplice convoluzione, forse equivarianze più complicate,

581
00:21:39,320 --> 00:21:43,679
quindi come facciamo a fare in modo che una

582
00:21:43,679 --> 00:21:45,720
cosa chiara nell'intelligenza naturale

583
00:21:45,720 --> 00:21:48,299
sia che noi  non esistiamo in

584
00:21:48,299 --> 00:21:51,120
questo mondo di frame IID, giusto, esistiamo

585
00:21:51,120 --> 00:21:53,520
in un mondo con sequenze continue di

586
00:21:53,520 --> 00:21:55,620
trasformazioni, quindi forse possiamo estendere il

587
00:21:55,620 --> 00:21:58,440
nostro modello a questo ambiente per imparare a

588
00:21:58,440 --> 00:22:01,080
osservare le trasformazioni, questa è un'idea

589
00:22:01,080 --> 00:22:03,299
di coerenza temporale,

590
00:22:03,299 --> 00:22:05,280
quindi cosa accadrebbe se semplicemente  abbiamo

591
00:22:05,280 --> 00:22:08,280
esteso la nostra struttura precedente nel

592
00:22:08,280 --> 00:22:10,620
tempo Dimension right quindi invece di limitarci a

593
00:22:10,620 --> 00:22:13,080
raggruppare dicendo che vogliamo che i nostri neuroni

594
00:22:13,080 --> 00:22:15,059
siano gruppi sparsi in termini di

595
00:22:15,059 --> 00:22:17,400
estensione spaziale sulla corteccia, in realtà

596
00:22:17,400 --> 00:22:18,960
vogliamo che siano gruppi sparsi nel tempo, il

597
00:22:18,960 --> 00:22:20,640
che significa che se un insieme di neuroni è

598
00:22:20,640 --> 00:22:22,559
attivo  ora vogliamo che lo stesso insieme di

599
00:22:22,559 --> 00:22:24,360
neuroni sia attivo anche nel futuro

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
se osserviamo se

602
00:22:27,840 --> 00:22:30,600
pensiamo in modo intuitivo a questo vediamo che questo è in

603
00:22:30,600 --> 00:22:33,059
realtà un'invarianza e un'equivarianza più incoraggianti

604
00:22:33,059 --> 00:22:35,039
un modo per capirlo è che stiamo

605
00:22:35,039 --> 00:22:37,140
dicendo che  voglio che gli stessi neuroni

606
00:22:37,140 --> 00:22:39,179
siano attivi costantemente ma la

607
00:22:39,179 --> 00:22:41,280
trasformazione dell'input cambia proprio mentre i

608
00:22:41,280 --> 00:22:44,220
piedi di questa piccola volpe si muovono, quindi se

609
00:22:44,220 --> 00:22:45,960
gli stessi neuroni codificano per la stessa

610
00:22:45,960 --> 00:22:47,880
cosa ancora e ancora ma i piedi

611
00:22:47,880 --> 00:22:49,320
si muovono quei neuroni

612
00:22:49,320 --> 00:22:51,360
impareranno  essere invariante rispetto al movimento di

613
00:22:51,360 --> 00:22:53,880
quella gamba di questo cane, ad esempio,

614
00:22:53,880 --> 00:22:57,539
quindi invece è che oops

615
00:22:57,539 --> 00:23:01,200
ho sbagliato strada qui uh quindi

616
00:23:01,200 --> 00:23:04,860
invece uh la nostra intuizione era che questo

617
00:23:04,860 --> 00:23:06,659
gruppo inizia ad essere potrebbe invece essere

618
00:23:06,659 --> 00:23:09,059
spostato rispetto al tempo quindi questo

619
00:23:09,059 --> 00:23:10,980
significherebbe  che

620
00:23:10,980 --> 00:23:13,080
serie di attivazioni spostate in sequenza verrebbero incoraggiate

621
00:23:13,080 --> 00:23:15,179
ad attivarsi insieme e quindi il nostro

622
00:23:15,179 --> 00:23:16,440
spazio latente sarebbe davvero strutturato

623
00:23:16,440 --> 00:23:18,000
rispetto alle trasformazioni osservate,

624
00:23:18,000 --> 00:23:19,980
quindi puoi vedere qui che invece di

625
00:23:19,980 --> 00:23:21,480
essere attivo sempre lo stesso insieme di neuroni,

626
00:23:21,480 --> 00:23:23,340
si tratta in realtà di una sequenza sequenziale  insieme

627
00:23:23,340 --> 00:23:24,900
permutato di neuroni che stiamo

628
00:23:24,900 --> 00:23:27,780
raggruppando insieme in questo modo sparso uh e

629
00:23:27,780 --> 00:23:29,940
e poi questo ci permette di modellare

630
00:23:29,940 --> 00:23:33,419
diverse osservazioni nel tempo ma

631
00:23:33,419 --> 00:23:34,860
sono comunque collegati in termini di

632
00:23:34,860 --> 00:23:36,960
apprendimento di una trasformazione e di preservazione di

633
00:23:36,960 --> 00:23:38,340
questa struttura di correlazione dell'empatia

634
00:23:38,340 --> 00:23:40,020


635
00:23:40,020 --> 00:23:41,940
quindi se noi  metti tutto insieme nella nostra

636
00:23:41,940 --> 00:23:44,400
architettura topografica Bae puoi ottenere

637
00:23:44,400 --> 00:23:46,020
qualcosa simile a questo vedi

638
00:23:46,020 --> 00:23:48,120
che abbiamo una sequenza di input stiamo

639
00:23:48,120 --> 00:23:51,240
di nuovo codificando una variabile z e poi

640
00:23:51,240 --> 00:23:53,520
più variabili U al denominatore

641
00:23:53,520 --> 00:23:55,740
qui e poi ognuna di queste

642
00:23:55,740 --> 00:23:58,620
variabili U viene spostata  uh un po' come

643
00:23:58,620 --> 00:24:00,480
stavamo mostrando prima per ottenere

644
00:24:00,480 --> 00:24:02,820
questa struttura di equivarianza di spostamento

645
00:24:02,820 --> 00:24:04,740
che stiamo cercando quando li combiniamo

646
00:24:04,740 --> 00:24:07,080
in questa distribuzione del prodotto T di Student

647
00:24:07,080 --> 00:24:09,240
otteniamo una singola

648
00:24:09,240 --> 00:24:10,740
variabile latente questa ora è la nostra

649
00:24:10,740 --> 00:24:13,860
variabile topografica T e ora che noi  abbiamo questa

650
00:24:13,860 --> 00:24:16,140
struttura conosciuta nel nostro spazio latente

651
00:24:16,140 --> 00:24:17,460
puoi pensarla come un modello mondiale strutturato

652
00:24:17,460 --> 00:24:19,919
sappiamo come trasformare questo

653
00:24:19,919 --> 00:24:21,659
spazio latente in questo caso è

654
00:24:21,659 --> 00:24:23,580
permutando queste attivazioni attorno a questi

655
00:24:23,580 --> 00:24:25,860
cerchi facendo come un ruolo ciclico uno

656
00:24:25,860 --> 00:24:28,380
spostamento ciclico sappiamo che questo è

657
00:24:28,380 --> 00:24:30,120
corrisponderà alle nostre trasformazioni di input apprese

658
00:24:30,120 --> 00:24:32,640
e possiamo verificarlo

659
00:24:32,640 --> 00:24:34,620
dicendo okay, cosa succede se continuo questa

660
00:24:34,620 --> 00:24:36,480
trasformazione di input la vera

661
00:24:36,480 --> 00:24:38,100
trasformazione nel set di dati che è

662
00:24:38,100 --> 00:24:40,559
una rotazione e poi la confronto con il

663
00:24:40,559 --> 00:24:42,659
modo in cui ho svolto il mio ruolo negli ultimi tempi  Spazio

664
00:24:42,659 --> 00:24:44,700
spostando le mie attivazioni nel mio

665
00:24:44,700 --> 00:24:47,280
cervello e poi decodifichiamo e vediamo che

666
00:24:47,280 --> 00:24:49,919
otteniamo esattamente la stessa cosa e quindi questo

667
00:24:49,919 --> 00:24:52,140
sta dimostrando questa

668
00:24:52,140 --> 00:24:53,580
proprietà di comunità di cui parlavo prima

669
00:24:53,580 --> 00:24:56,820
per verificare l'omomorfismo

670
00:24:56,820 --> 00:24:58,799
e quindi per misurarlo un po' più

671
00:24:58,799 --> 00:25:02,460
di qualità  quantitativamente possiamo misurare

672
00:25:02,460 --> 00:25:04,440
quella che viene chiamata perdita di equivarianza, quindi

673
00:25:04,440 --> 00:25:07,080
questa è davvero la quantificazione di

674
00:25:07,080 --> 00:25:09,360
questa differenza tra

675
00:25:09,360 --> 00:25:12,120
l'attivazione della capsula arrotolata o il rotolamento nella nostra

676
00:25:12,120 --> 00:25:15,059
testa rispetto al guardare il rotolamento svolgersi

677
00:25:15,059 --> 00:25:16,559
e in avanti stanno guardando la

678
00:25:16,559 --> 00:25:19,440
trasformazione svolgersi davanti a noi quindi

679
00:25:19,440 --> 00:25:21,600
vediamo la topografica  Bae ottiene un

680
00:25:21,600 --> 00:25:24,000


681
00:25:24,000 --> 00:25:26,700
errore di equivarianza significativamente più basso, questa bolla vae è ciò di cui stavo

682
00:25:26,700 --> 00:25:27,960
parlando prima dove sta imparando

683
00:25:27,960 --> 00:25:29,820
l'invarianza, quindi non ha l'

684
00:25:29,820 --> 00:25:32,340
operazione di spostamento e il tipo tradizionale

685
00:25:32,340 --> 00:25:35,640
di vae non ha nozione di organizzazione o

686
00:25:35,640 --> 00:25:37,380
componente temporale, quindi prestazioni molto

687
00:25:37,380 --> 00:25:40,320
scarse in aggiunta  a questo vediamo che

688
00:25:40,320 --> 00:25:41,700
il modello è un modello generativo migliore

689
00:25:41,700 --> 00:25:45,059
di sequenze ottiene solo una probabilità inferiore uh

690
00:25:45,059 --> 00:25:48,179
inferiore come log negativo uh sul

691
00:25:48,179 --> 00:25:50,100
set di dati quindi è in grado di

692
00:25:50,100 --> 00:25:51,720
modellare meglio questo set di dati perché ha una

693
00:25:51,720 --> 00:25:52,919
nozione della struttura del

694
00:25:52,919 --> 00:25:55,460
trasformazioni

695
00:25:55,980 --> 00:25:58,140
uh possiamo testarlo su più

696
00:25:58,140 --> 00:25:59,760
tipi di trasformazione diversi e nella

697
00:25:59,760 --> 00:26:00,840
riga superiore stiamo mostrando la vera

698
00:26:00,840 --> 00:26:02,880
trasformazione abbiamo estratto queste

699
00:26:02,880 --> 00:26:05,039
immagini in grigio e poi nella

700
00:26:05,039 --> 00:26:07,080
riga inferiore codifichiamo e poi semplicemente facciamo

701
00:26:07,080 --> 00:26:08,700
scorrere le nostre attivazioni e manteniamo

702
00:26:08,700 --> 00:26:12,140
decodifica per vedere cosa

703
00:26:12,140 --> 00:26:15,000
ha imparato il modello come

704
00:26:15,000 --> 00:26:17,039
trasformazione attuale osservata e

705
00:26:17,039 --> 00:26:19,340
vediamo che può ricostruire sostanzialmente perfettamente

706
00:26:19,340 --> 00:26:21,360
questi elementi della

707
00:26:21,360 --> 00:26:23,640
sequenza che non ha mai visto prima,

708
00:26:23,640 --> 00:26:25,260
inoltre con immagini provenienti

709
00:26:25,260 --> 00:26:26,580
dal set di test che non ha mai visto  prima

710
00:26:26,580 --> 00:26:28,380
semplicemente perché sa qual è la

711
00:26:28,380 --> 00:26:29,760
trasformazione che sta attualmente

712
00:26:29,760 --> 00:26:31,500
codificando può generalizzarla a nuovi

713
00:26:31,500 --> 00:26:33,919
esempi

714
00:26:34,020 --> 00:26:36,360
quindi il punto da questa parte è davvero l'

715
00:26:36,360 --> 00:26:38,039
organizzazione topografica abbiamo dimostrato che

716
00:26:38,039 --> 00:26:40,080
una struttura di input preservata e ora

717
00:26:40,080 --> 00:26:41,940
stiamo dimostrando che può potenzialmente migliorare l'

718
00:26:41,940 --> 00:26:44,279
efficienza e la generalizzazione  come speriamo,

719
00:26:44,279 --> 00:26:46,200


720
00:26:46,200 --> 00:26:48,600
finalmente qualcosa che ci ha sorpreso

721
00:26:48,600 --> 00:26:49,980
e che ho pensato fosse potenzialmente più

722
00:26:49,980 --> 00:26:52,500
interessante è che queste

723
00:26:52,500 --> 00:26:53,700
trasformazioni apprese dal nostro

724
00:26:53,700 --> 00:26:54,960
modello in realtà generalizzano le

725
00:26:54,960 --> 00:26:57,059
combinazioni di trasformazioni che

726
00:26:57,059 --> 00:26:59,580
non vediamo durante l'allenamento, quindi ad

727
00:26:59,580 --> 00:27:02,100
esempio nonostante il solo allenamento  sulle

728
00:27:02,100 --> 00:27:04,200
trasformazioni e sull'isolamento del colore e della rotazione

729
00:27:04,200 --> 00:27:06,419
se al modello viene presentata

730
00:27:06,419 --> 00:27:08,340
una trasformazione combinata della rotazione del colore

731
00:27:08,340 --> 00:27:11,100
al momento del test uh vediamo che è in grado

732
00:27:11,100 --> 00:27:13,140
di modellare completamente e completare

733
00:27:13,140 --> 00:27:14,700
perfettamente queste trasformazioni attraverso il

734
00:27:14,700 --> 00:27:17,159
ruolo della capsula, il che implica che ha imparato

735
00:27:17,159 --> 00:27:19,620
a fattorizzare per rappresentare queste

736
00:27:19,620 --> 00:27:20,880
diverse trasformazioni  Trasformazioni e può

737
00:27:20,880 --> 00:27:24,600
combinarle in modo flessibile al momento dell'inferenza,

738
00:27:24,600 --> 00:27:28,140
quindi forse non otteniamo solo

739
00:27:28,140 --> 00:27:29,820
ufficialmente l'efficienza nella generalizzazione, ma

740
00:27:29,820 --> 00:27:34,100
otteniamo anche una certa composizionalità di base,

741
00:27:34,260 --> 00:27:36,059
quindi parliamo delle limitazioni e di

742
00:27:36,059 --> 00:27:38,460
cosa potremmo fare dopo, uh la

743
00:27:38,460 --> 00:27:40,620
limitazione principale è che c'è un

744
00:27:40,620 --> 00:27:44,159
trasformazione predefinita che stiamo imponendo sia nello

745
00:27:44,159 --> 00:27:46,500
spazio che nel tempo, quindi anche se ci siamo liberati

746
00:27:46,500 --> 00:27:49,080
dalle trasformazioni di gruppo e

747
00:27:49,080 --> 00:27:52,440
in particolare apprezziamo la traduzione o

748
00:27:52,440 --> 00:27:53,940
la rotazione come viene attualmente fatto nel

749
00:27:53,940 --> 00:27:55,559
mondo dell'apprendimento automatico,

750
00:27:55,559 --> 00:27:59,240
abbiamo ancora questo

751
00:27:59,240 --> 00:28:01,980
ruolo latente codificato nel nostro  si dirige verso tutto ciò che

752
00:28:01,980 --> 00:28:03,900
vediamo e per renderlo un po'

753
00:28:03,900 --> 00:28:05,700
più flessibile, quindi speriamo di poter modellare

754
00:28:05,700 --> 00:28:08,880
una maggiore diversità di trasformazioni.

755
00:28:08,880 --> 00:28:10,980
um pensiamo che forse possiamo trarre

756
00:28:10,980 --> 00:28:13,860
ispirazione da dinamiche spazio-temporali più strutturate

757
00:28:13,860 --> 00:28:15,600
che vengono osservate

758
00:28:15,600 --> 00:28:18,120
nel cervello e quindi questo ci porta  alla

759
00:28:18,120 --> 00:28:20,400
seconda parte di questo discorso che riguarda le

760
00:28:20,400 --> 00:28:22,140
dinamiche spazio-temporali che

761
00:28:22,140 --> 00:28:23,039
cercheremo di integrare nelle

762
00:28:23,039 --> 00:28:25,200
reti neurali artificiali, un esempio

763
00:28:25,200 --> 00:28:27,059
di ciò sono le onde viaggianti come ho mostrato

764
00:28:27,059 --> 00:28:28,020
qui,

765
00:28:28,020 --> 00:28:30,600
quindi cosa intendiamo con questo, uh beh,

766
00:28:30,600 --> 00:28:32,279
ecco una cosa molto  recente articolo in cui hanno

767
00:28:32,279 --> 00:28:36,059
utilizzato una fmri da nove Tesla operante con una

768
00:28:36,059 --> 00:28:38,700
risoluzione di 36 millisecondi per visualizzare una singola

769
00:28:38,700 --> 00:28:40,980
fetta di cervello di ratto sotto anestesia

770
00:28:40,980 --> 00:28:43,320
e ciò che vediamo è questa

771
00:28:43,320 --> 00:28:45,720
attività spaziale temporale e

772
00:28:45,720 --> 00:28:48,299
correlazioni molto chiaramente strutturate e questi autori dell'articolo

773
00:28:48,299 --> 00:28:50,520
continuano ad analizzare questo  attività in

774
00:28:50,520 --> 00:28:52,919
termini delle modalità principali illustrate

775
00:28:52,919 --> 00:28:55,799
a destra, quindi la nostra ipotesi è che

776
00:28:55,799 --> 00:28:57,179
forse una sorta di

777
00:28:57,179 --> 00:28:59,039
struttura di correlazione come questa potrebbe essere utile

778
00:28:59,039 --> 00:29:01,260
per strutturare le rappresentazioni del

779
00:29:01,260 --> 00:29:03,240
nostro modello rispetto alle

780
00:29:03,240 --> 00:29:05,100
trasformazioni osservate ma in un modo molto più

781
00:29:05,100 --> 00:29:07,440
flessibile rispetto al semplice fatto  solo uno

782
00:29:07,440 --> 00:29:10,700
spostamento ciclico come stavamo facendo prima

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
e lasciami dire che questo non si

785
00:29:15,900 --> 00:29:19,320
osserva solo nei ratti di un ssi uh questo puoi

786
00:29:19,320 --> 00:29:20,940
puoi vedere queste onde viaggianti

787
00:29:20,940 --> 00:29:24,179
verificarsi nella corteccia del Monte dei

788
00:29:24,179 --> 00:29:27,600
primati che si comportano svegli uh quindi per esempio

789
00:29:27,600 --> 00:29:29,580
a sinistra  qui mostrano onde viaggianti

790
00:29:29,580 --> 00:29:31,740
che in realtà

791
00:29:31,740 --> 00:29:35,520
cambiano la probabilità che un primate veda

792
00:29:35,520 --> 00:29:38,279
stimoli a basso contrasto in base alla fase

793
00:29:38,279 --> 00:29:40,980
dell'onda inoltre mostrano che

794
00:29:40,980 --> 00:29:43,500
uno stimolo simile ad alto contrasto sulla

795
00:29:43,500 --> 00:29:45,779
destra può indurre un'attività di onde viaggianti

796
00:29:45,779 --> 00:29:47,520
che si propagano verso l'esterno  anche

797
00:29:47,520 --> 00:29:50,039
nella corteccia visiva primaria, quindi questi sono

798
00:29:50,039 --> 00:29:52,140
davvero onnipresenti in tutto il cervello

799
00:29:52,140 --> 00:29:54,000
a più livelli e sarebbe

800
00:29:54,000 --> 00:29:55,440
interessante studiare quali sono le loro

801
00:29:55,440 --> 00:29:58,140
implicazioni per l'

802
00:29:58,140 --> 00:29:59,700
apprendimento della rappresentazione della struttura nel nostro caso o

803
00:29:59,700 --> 00:30:01,799
o in generale

804
00:30:01,799 --> 00:30:04,140
esiste un lavoro precedente che ha studiato

805
00:30:04,140 --> 00:30:06,720
questi tipi di dinamiche uh  e

806
00:30:06,720 --> 00:30:08,700
costruiscono modelli quindi in alto queste sono le

807
00:30:08,700 --> 00:30:10,380
equazioni che descrivono una

808
00:30:10,380 --> 00:30:12,600
rete neurale ad impulsi che mostrano se si

809
00:30:12,600 --> 00:30:15,720
implementano ritardi temporali in realtà

810
00:30:15,720 --> 00:30:18,240
ritardi temporali assonali tra i neuroni si ottengono

811
00:30:18,240 --> 00:30:20,820
queste dinamiche strutturali delle

812
00:30:20,820 --> 00:30:22,440
onde viaggianti purché la dimensione della rete sia

813
00:30:22,440 --> 00:30:24,059
abbastanza grande

814
00:30:24,059 --> 00:30:26,520
ehm, tuttavia, come molte persone probabilmente sanno,

815
00:30:26,520 --> 00:30:28,620
è relativamente impegnativo addestrare

816
00:30:28,620 --> 00:30:31,320
reti neurali a spillo delle stesse dimensioni

817
00:30:31,320 --> 00:30:34,820
e prestazioni delle reti neurali profonde, allo

818
00:30:34,820 --> 00:30:37,679
stesso modo in basso un altro sistema

819
00:30:37,679 --> 00:30:39,539
che è significativamente più semplice ma

820
00:30:39,539 --> 00:30:42,840
forse troppo semplice uh è una rete di

821
00:30:42,840 --> 00:30:45,120
oscillatori accoppiati che sono noti a

822
00:30:45,120 --> 00:30:48,779
mostrano sincronia e dinamiche spazio-temporali

823
00:30:48,779 --> 00:30:52,200
e schemi complessi ma

824
00:30:52,200 --> 00:30:53,520
questo è chiamato come un sistema di riduzione di fase

825
00:30:53,520 --> 00:30:55,500
e non cattura del tutto la

826
00:30:55,500 --> 00:30:57,059
complessità a cui siamo interessati,

827
00:30:57,059 --> 00:30:58,140
quindi stiamo osservando qualcosa che è

828
00:30:58,140 --> 00:31:00,779
potenzialmente tra questi due

829
00:31:00,779 --> 00:31:03,600
e ciò che noi  deciso se questo lavoro in

830
00:31:03,600 --> 00:31:06,600
questo lavoro consiste nel parametrizzare una

831
00:31:06,600 --> 00:31:08,520
rete di coppia di oscillatori in modo

832
00:31:08,520 --> 00:31:10,620
leggermente più flessibile rispetto a un

833
00:31:10,620 --> 00:31:12,360
modello paramoto, quindi questo è davvero

834
00:31:12,360 --> 00:31:14,580
costruito su questa coppia di

835
00:31:14,580 --> 00:31:16,380
reti neurali ricorrenti distillatorie di Constantine

836
00:31:16,380 --> 00:31:18,720
rush e Nisha

837
00:31:18,720 --> 00:31:20,760
um dove fondamentalmente hanno preso l'

838
00:31:20,760 --> 00:31:22,200
equazione che  descrive un semplice

839
00:31:22,200 --> 00:31:23,820
oscillatore armonico è

840
00:31:23,820 --> 00:31:26,159
un'equazione differenziale del secondo ordine l'accelerazione

841
00:31:26,159 --> 00:31:29,940
di una sfera su una molla è proporzionale al

842
00:31:29,940 --> 00:31:32,480
suo spostamento

843
00:31:32,480 --> 00:31:35,220
uh puoi aggiungere termini aggiuntivi come

844
00:31:35,220 --> 00:31:37,260
smorzamento in modo che le oscillazioni si

845
00:31:37,260 --> 00:31:39,360
estinguano lentamente nel tempo

846
00:31:39,360 --> 00:31:41,580
puoi pilotare questo oscillatore con un oscillatore

847
00:31:41,580 --> 00:31:43,380
esterno  input per contrastare

848
00:31:43,380 --> 00:31:45,179
questo smorzamento o per dare un po' più

849
00:31:45,179 --> 00:31:47,279
complessità alla dinamica

850
00:31:47,279 --> 00:31:49,260
e inoltre se hai molti di

851
00:31:49,260 --> 00:31:50,940
questi oscillatori puoi accoppiarli

852
00:31:50,940 --> 00:31:53,000
insieme con queste matrici di accoppiamento W

853
00:31:53,000 --> 00:31:55,320
uh come dimostriamo in questa

854
00:31:55,320 --> 00:31:56,640
immagine qui così puoi davvero  pensa a

855
00:31:56,640 --> 00:31:58,140
questa rete come un mucchio di bolle sulle

856
00:31:58,140 --> 00:31:59,940
molle e forse sono collegate tra

857
00:31:59,940 --> 00:32:01,740
loro anche da molle o elastici

858
00:32:01,740 --> 00:32:03,600
qualunque sia la coppia di

859
00:32:03,600 --> 00:32:05,279
reti neurali ricorrenti distillatorie della

860
00:32:05,279 --> 00:32:08,100
Mishra russa uh con questi vari termini e

861
00:32:08,100 --> 00:32:09,899
questo ha dimostrato di essere molto potente

862
00:32:09,899 --> 00:32:12,480
per modellare sequenze lunghe hanno anche

863
00:32:12,480 --> 00:32:13,740
detto che si sono ispirati al

864
00:32:13,740 --> 00:32:15,360
cervello per costruirlo e ci sono molte

865
00:32:15,360 --> 00:32:17,700
buone analisi in quell'articolo uh per

866
00:32:17,700 --> 00:32:19,020
esempio mostrano che si tratta di

867
00:32:19,020 --> 00:32:21,440
proprietà davvero benefiche rispetto ai

868
00:32:21,440 --> 00:32:23,460
problemi del gradiente di fuga che

869
00:32:23,460 --> 00:32:25,440
tipicamente si verificano nelle reti neurali ricorrenti

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
um  ma se vogliamo guardare alla

872
00:32:29,159 --> 00:32:30,960
dinamica spazio-temporale e a questo tipo di

873
00:32:30,960 --> 00:32:32,820
modello, uh è un po' impegnativo

874
00:32:32,820 --> 00:32:34,919
perché queste matrici di accoppiamento qui, le

875
00:32:34,919 --> 00:32:36,320
W

876
00:32:36,320 --> 00:32:39,600
che connettono ogni neurale o ogni

877
00:32:39,600 --> 00:32:41,240
oscillatore sono posizionati l'uno con l'altro,

878
00:32:41,240 --> 00:32:43,620
queste sono matrici densamente connesse

879
00:32:43,620 --> 00:32:45,120
come ho provato a fare  raffigurato qui a sinistra

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
quindi se provi a visualizzare le dinamiche

882
00:32:48,299 --> 00:32:50,580
di questa rete non vedi alcuna

883
00:32:50,580 --> 00:32:51,899
organizzazione spaziale non c'è

884
00:32:51,899 --> 00:32:55,380
ereditarietà sono scuse per lo

885
00:32:55,380 --> 00:32:57,000
spazio latente di questo modello

886
00:32:57,000 --> 00:32:58,799
um quindi puoi pensare a questo come nel nostro

887
00:32:58,799 --> 00:33:01,020
esempio precedente un neurone  è collegato

888
00:33:01,020 --> 00:33:03,240
a un insieme potenzialmente arbitrario di altri

889
00:33:03,240 --> 00:33:04,919
neuroni quei neuroni sono collegati a

890
00:33:04,919 --> 00:33:06,600
un altro insieme arbitrario di neuroni e

891
00:33:06,600 --> 00:33:08,520
otterrai sicuramente solo dinamiche oscillatorie

892
00:33:08,520 --> 00:33:10,860
ma tipi di fluttuazioni che

893
00:33:10,860 --> 00:33:13,260
non hanno molto senso strutturato quindi

894
00:33:13,260 --> 00:33:15,360
nel nostro lavoro abbiamo pensato  okay, come possiamo

895
00:33:15,360 --> 00:33:18,299
convertirlo maggiormente nei tipi di

896
00:33:18,299 --> 00:33:19,860
dinamiche che ci interessano in questa

897
00:33:19,860 --> 00:33:22,140
propagazione strutturata dell'attività uh

898
00:33:22,140 --> 00:33:23,940
e un modo chiaro per farlo è avere

899
00:33:23,940 --> 00:33:26,539
una matrice di connettività più strutturata W

900
00:33:26,539 --> 00:33:29,880
uh che abbiamo scoperto essere facilmente implementabile

901
00:33:29,880 --> 00:33:31,559
ed efficiente  implementato attraverso

902
00:33:31,559 --> 00:33:33,000
un'operazione di convoluzione che puoi

903
00:33:33,000 --> 00:33:34,620
pensare come uno

904
00:33:34,620 --> 00:33:36,299
strato locale connesso localmente quindi invece di avere

905
00:33:36,299 --> 00:33:37,860
ogni neurone connesso ogni neurone i

906
00:33:37,860 --> 00:33:39,480
neuroni sono semplicemente collegati ai loro

907
00:33:39,480 --> 00:33:41,580
vicini vicini dopo l'allenamento

908
00:33:41,580 --> 00:33:42,840
finirai per ottenere qualcosa che

909
00:33:42,840 --> 00:33:44,880
assomiglia ad uno spazio spaziale liscio  dinamica temporale

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
quindi per essere un po' più chiari

912
00:33:48,419 --> 00:33:50,519
nell'addestrare questo modello prendiamo questa

913
00:33:50,519 --> 00:33:52,200
equazione differenziale separata del secondo ordine che

914
00:33:52,200 --> 00:33:54,299
stavamo descrivendo prima di discretizzarla

915
00:33:54,299 --> 00:33:56,340
in due equazioni del primo ordine,

916
00:33:56,340 --> 00:33:57,960
puoi pensarla come se

917
00:33:57,960 --> 00:34:01,200
integrassi numericamente l'ode che ora abbiamo a

918
00:34:01,200 --> 00:34:03,120
velocità e poi aggiorniamo

919
00:34:03,120 --> 00:34:06,000
uh e possiamo addestrare questo modello come

920
00:34:06,000 --> 00:34:07,620
qualcosa come un codificatore automatico o un

921
00:34:07,620 --> 00:34:09,839
modello auto regressivo quindi se prendiamo un

922
00:34:09,839 --> 00:34:11,460
input lo codifichiamo nel nostro spazio latente

923
00:34:11,460 --> 00:34:14,339
in realtà l'input è Dr è questo termine f di x

924
00:34:14,339 --> 00:34:16,500
che agisce  come termine guida quindi

925
00:34:16,500 --> 00:34:18,599
è come guidare questi oscillatori

926
00:34:18,599 --> 00:34:20,879
dal basso uh e poi hanno le

927
00:34:20,879 --> 00:34:23,099
loro dinamiche che sono definite dai

928
00:34:23,099 --> 00:34:25,800
termini di accoppiamento questi accoppiamenti locali e

929
00:34:25,800 --> 00:34:27,540
poi ad ogni passo temporale prendiamo questo

930
00:34:27,540 --> 00:34:29,460
stato latente questo stato d'onda e lo

931
00:34:29,460 --> 00:34:31,560
decodifichiamo per provare  e ricostruire l'input

932
00:34:31,560 --> 00:34:33,540
e trovarsi nella fase temporale corrente o in una

933
00:34:33,540 --> 00:34:35,699
fase temporale futura

934
00:34:35,699 --> 00:34:37,980
possiamo fare qualche analisi di questi

935
00:34:37,980 --> 00:34:42,300
modelli uh durante l'allenamento per vedere cosa

936
00:34:42,300 --> 00:34:43,619
succede prima dell'allenamento e dopo

937
00:34:43,619 --> 00:34:45,780
l'allenamento possiamo calcolare la fase e

938
00:34:45,780 --> 00:34:47,399
la velocità della dinamica nel

939
00:34:47,399 --> 00:34:49,379
spazio latente in pratica vediamo che

940
00:34:49,379 --> 00:34:51,480
all'inizio dello scambio non ci sono onde nel

941
00:34:51,480 --> 00:34:53,699
nostro modello ma dopo l'addestramento dopo 50

942
00:34:53,699 --> 00:34:55,500
epoche vediamo che c'è

943
00:34:55,500 --> 00:34:57,119
un'attività strutturata uniforme che si propaga

944
00:34:57,119 --> 00:35:00,420
verso il basso uh al servizio di questa

945
00:35:00,420 --> 00:35:01,800
attività di modellazione di sequenze che stiamo facendo come

946
00:35:01,800 --> 00:35:04,380
oggetti rotanti,

947
00:35:04,380 --> 00:35:05,940
um quindi  qual è il vantaggio di questo,

948
00:35:05,940 --> 00:35:07,680
intendo dire che l'intera ragione per cui ho motivato questo

949
00:35:07,680 --> 00:35:10,380
è stato dire che volevamo avere una

950
00:35:10,380 --> 00:35:11,880
struttura appresa più flessibile, lo stiamo

951
00:35:11,880 --> 00:35:13,020
effettivamente facendo o stiamo semplicemente

952
00:35:13,020 --> 00:35:15,060
ottenendo belle onde,

953
00:35:15,060 --> 00:35:16,859
quindi quello che abbiamo mostrato nel nostro articolo è

954
00:35:16,859 --> 00:35:19,320
che lo siamo davvero  imparando una sorta di

955
00:35:19,320 --> 00:35:20,940
struttura utile e il modo in cui l'abbiamo mostrato

956
00:35:20,940 --> 00:35:22,440
è di nuovo con qualcosa come questo

957
00:35:22,440 --> 00:35:24,960
diagramma commutativo se prendi un input

958
00:35:24,960 --> 00:35:27,000
e lo codifichi e ottieni uno

959
00:35:27,000 --> 00:35:29,280
stato d'onda e poi propaghi le onde

960
00:35:29,280 --> 00:35:31,619
artificialmente in quello stato d'onda e poi

961
00:35:31,619 --> 00:35:33,480
decodifichi puoi  osserva che

962
00:35:33,480 --> 00:35:35,220
in realtà è esattamente lo stesso che se avessi

963
00:35:35,220 --> 00:35:37,140
semplicemente mostrato un mucchio di

964
00:35:37,140 --> 00:35:39,180
immagini diverse di trasformazioni diverse, quindi

965
00:35:39,180 --> 00:35:41,640
molte caratteristiche diverse di cifre diverse

966
00:35:41,640 --> 00:35:43,920
e vediamo che otteniamo

967
00:35:43,920 --> 00:35:46,140
diversi tipi di attività delle onde in ciascun

968
00:35:46,140 --> 00:35:47,880
caso per modellarlo  trasformazione diversa

969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
se lo alleniamo su set di dati diversi

971
00:35:51,119 --> 00:35:53,400
vediamo allo stesso modo dinamiche più complesse

972
00:35:53,400 --> 00:35:55,200
in questo caso forse nemmeno

973
00:35:55,200 --> 00:35:57,839
onde viaggianti o onde stazionarie che

974
00:35:57,839 --> 00:36:00,359
possono essere pensate come onde viaggianti in

975
00:36:00,359 --> 00:36:02,339
direzioni opposte quindi vediamo se stiamo

976
00:36:02,339 --> 00:36:04,079
modellando questi orbitali  Dinamiche otteniamo

977
00:36:04,079 --> 00:36:06,000
questo tipo di blob di attività in movimento fluido

978
00:36:06,000 --> 00:36:07,619
nel nostro spazio latente se stiamo

979
00:36:07,619 --> 00:36:09,839
modellando un pendolo otteniamo allo stesso modo una

980
00:36:09,839 --> 00:36:13,820
sorta di attività oscillatoria complessa

981
00:36:14,099 --> 00:36:17,339
in modo che venga preservata la struttura di input ma

982
00:36:17,339 --> 00:36:19,560
inoltre maggiore flessibilità rispetto a

983
00:36:19,560 --> 00:36:21,599
prima, che è una specie di nostro obiettivo finale

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
quindi, infine, voglio parlare un po' di

986
00:36:26,099 --> 00:36:28,320
come penso che il risultato di questa ricerca

987
00:36:28,320 --> 00:36:30,420
potrebbe non solo migliorare l'

988
00:36:30,420 --> 00:36:32,099
intelligenza artificiale, ma anche come aiutarci a

989
00:36:32,099 --> 00:36:34,440
capire perché le nostre misurazioni del

990
00:36:34,440 --> 00:36:36,240
cervello appaiono in questo modo, per fornire un

991
00:36:36,240 --> 00:36:38,579
breve esempio di ciò che penso  intendo con questo uh

992
00:36:38,579 --> 00:36:41,040
ho parlato un po' prima di visti

993
00:36:41,040 --> 00:36:43,740
e luoghi quindi in questo fantastico lavoro

994
00:36:43,740 --> 00:36:46,859
con Ching Higao abbiamo studiato se il nostro

995
00:36:46,859 --> 00:36:48,900
semplice precedente topografico come abbiamo discusso

996
00:36:48,900 --> 00:36:50,579
possa essere in grado di riprodurre questi stessi

997
00:36:50,579 --> 00:36:53,339
effetti quindi nello specifico abbiamo messo il valore

998
00:36:53,339 --> 00:36:56,099
di questa D di Cohen  metrica di selettività per

999
00:36:56,099 --> 00:36:58,200
ciascuno dei nostri neuroni rispetto a un

1000
00:36:58,200 --> 00:37:00,000
diverso set di dati di immagini potenzialmente

1001
00:37:00,000 --> 00:37:02,460
contenenti solo volti o solo oggetti o

1002
00:37:02,460 --> 00:37:03,359
corpi

1003
00:37:03,359 --> 00:37:05,880
e quindi misuriamo per ogni neurone se è

1004
00:37:05,880 --> 00:37:07,920
più probabile che risponda ai volti o che il

1005
00:37:07,920 --> 00:37:10,380
russo emerga nel cervello ma lo faccio

1006
00:37:10,380 --> 00:37:12,839
penso che ci dica che l'

1007
00:37:12,839 --> 00:37:15,300
organizzazione relativa della selettività potrebbe

1008
00:37:15,300 --> 00:37:17,400
essere almeno parzialmente attribuibile alle

1009
00:37:17,400 --> 00:37:19,800
statistiche di correlazione nei dati che devono ripercorrere

1010
00:37:19,800 --> 00:37:21,359
dopo essere stati passati attraverso un

1011
00:37:21,359 --> 00:37:23,640
futuro estrattore altamente non lineare come una

1012
00:37:23,640 --> 00:37:25,440
rete neurale profonda,

1013
00:37:25,440 --> 00:37:27,480
quindi in modo simile qualcosa che è

1014
00:37:27,480 --> 00:37:29,040
interessante, è noto ciò che

1015
00:37:29,040 --> 00:37:30,900
viene chiamato flusso visivo tripartito o meno,

1016
00:37:30,900 --> 00:37:36,720
quindi uh le immagini di uh o oggetti sono

1017
00:37:36,720 --> 00:37:38,400
la selettività rispetto agli oggetti

1018
00:37:38,400 --> 00:37:40,680
è organizzata da proprietà più astratte

1019
00:37:40,680 --> 00:37:43,200
come l'animazione è questa cosa viva o

1020
00:37:43,200 --> 00:37:46,200
inanimata uh rispetto anche alle

1021
00:37:46,200 --> 00:37:48,480
dimensioni degli oggetti del mondo reale come quello  è la dimensione di una

1022
00:37:48,480 --> 00:37:50,700
teiera rispetto a un automobile

1023
00:37:50,700 --> 00:37:53,520
e ciò che vediamo è che negli

1024
00:37:53,520 --> 00:37:56,160
esseri umani la selettività è organizzata

1025
00:37:56,160 --> 00:37:57,540
in questa struttura tripartita

1026
00:37:57,540 --> 00:37:59,760
in genere si hanno piccoli oggetti che si trovano

1027
00:37:59,760 --> 00:38:01,920
tra oggetti animati e inanimati

1028
00:38:01,920 --> 00:38:04,200
in termini di selettività e vediamo

1029
00:38:04,200 --> 00:38:06,060
qui accade la stessa cosa, quindi

1030
00:38:06,060 --> 00:38:07,440
misurano la selettività dello

1031
00:38:07,440 --> 00:38:08,880
stesso insieme di neuroni ma rispetto

1032
00:38:08,880 --> 00:38:10,859
a queste differenze di stimoli vedi

1033
00:38:10,859 --> 00:38:12,440
che il piccolo cluster si trova tra il

1034
00:38:12,440 --> 00:38:14,820
cluster animato e quello inanimato e di nuovo

1035
00:38:14,820 --> 00:38:16,079
questo accade per più

1036
00:38:16,079 --> 00:38:18,900
inizializzazioni diverse, quindi questo  è qualcosa che

1037
00:38:18,900 --> 00:38:20,880
spero potremo esplorare ulteriormente per

1038
00:38:20,880 --> 00:38:22,980
questa community. Penso che sia interessante

1039
00:38:22,980 --> 00:38:24,119
perché è

1040
00:38:24,119 --> 00:38:26,220
davvero un modo per dimostrare che abbiamo

1041
00:38:26,220 --> 00:38:28,200
costruito un modello mondiale strutturato e

1042
00:38:28,200 --> 00:38:30,119
potenzialmente questo modello mondiale è

1043
00:38:30,119 --> 00:38:31,980
utile per

1044
00:38:31,980 --> 00:38:34,740
rappresentare meglio i dati del mondo reale in

1045
00:38:34,740 --> 00:38:37,619
modo strutturato e  si ottiene una minore

1046
00:38:37,619 --> 00:38:39,119
energia libera in questo senso,

1047
00:38:39,119 --> 00:38:40,619
quindi

1048
00:38:40,619 --> 00:38:42,300
penso che sviluppando questi modelli

1049
00:38:42,300 --> 00:38:44,400
come quelli che abbiamo mostrato qui potremmo ottenere

1050
00:38:44,400 --> 00:38:46,500
informazioni su nuovi meccanismi su come

1051
00:38:46,500 --> 00:38:48,900
emerge questa struttura, inclusa

1052
00:38:48,900 --> 00:38:50,460
l'organizzazione topografica a cui non avevamo mai

1053
00:38:50,460 --> 00:38:52,920
pensato prima, quindi modello di macchina che stavo

1054
00:38:52,920 --> 00:38:55,520
guardando  la selettività dell'orientamento

1055
00:38:55,520 --> 00:38:58,260
uh dei neuroni che non mi

1056
00:38:58,260 --> 00:39:01,020
aspettavo particolarmente che

1057
00:39:01,020 --> 00:39:03,420
succedesse qualcosa ma uh stai guardando

1058
00:39:03,420 --> 00:39:05,339
queste onde che si propagano su questa

1059
00:39:05,339 --> 00:39:08,099
superficie verticale simulata e ho pensato

1060
00:39:08,099 --> 00:39:09,960
ok forse sto mostrando immagini ruotate

1061
00:39:09,960 --> 00:39:11,820
forse questo ha qualche effetto su  la

1062
00:39:11,820 --> 00:39:13,740
selettività dell'orientamento

1063
00:39:13,740 --> 00:39:15,599
e in realtà se entri e

1064
00:39:15,599 --> 00:39:17,460
misuri la selettività di ciascun neurone

1065
00:39:17,460 --> 00:39:18,660
rispetto a queste

1066
00:39:18,660 --> 00:39:22,079
linee diversamente orientate quello che vedi è che

1067
00:39:22,079 --> 00:39:24,300
ricorda sorprendentemente il

1068
00:39:24,300 --> 00:39:25,859
tipo di colonne orientali che si vedono nella

1069
00:39:25,859 --> 00:39:27,599
corteccia visiva primaria, questa è roba che risale al passato

1070
00:39:27,599 --> 00:39:29,520
a Hugo e Weasel e questo è qualcosa

1071
00:39:29,520 --> 00:39:30,900
che è venuto fuori da questo modello

1072
00:39:30,900 --> 00:39:33,060
e dal fatto che ha la

1073
00:39:33,060 --> 00:39:34,440
struttura spazio-temporale rispetto a

1074
00:39:34,440 --> 00:39:37,619
Trasformazioni quindi ovviamente questa è

1075
00:39:37,619 --> 00:39:39,599
un'analogia davvero grossolana ma penso che questo

1076
00:39:39,599 --> 00:39:40,740
sia un esempio di come  costruire questo

1077
00:39:40,740 --> 00:39:42,839
tipo di modelli può aiutarci a pensare a

1078
00:39:42,839 --> 00:39:45,240
come il cervello costruisce la

1079
00:39:45,240 --> 00:39:46,980
struttura rappresentazionale e al modo in cui il bianco è

1080
00:39:46,980 --> 00:39:48,660
organizzato in un modo a cui forse non abbiamo mai

1081
00:39:48,660 --> 00:39:51,300
pensato prima,

1082
00:39:51,300 --> 00:39:53,460
ehm, penso di non essere l'unico a

1083
00:39:53,460 --> 00:39:55,859
fare questo tipo di  lavoro e quindi

1084
00:39:55,859 --> 00:39:57,240
voglio parlare un po' di

1085
00:39:57,240 --> 00:39:59,579
altre persone che lo fanno uh quindi ho

1086
00:39:59,579 --> 00:40:00,780
parlato di questa

1087
00:40:00,780 --> 00:40:02,760
struttura equivalente

1088
00:40:02,760 --> 00:40:04,920
um persone come James Whittington e

1089
00:40:04,920 --> 00:40:08,880
Tim Barons e surrogengoolie uh hanno

1090
00:40:08,880 --> 00:40:10,680
dimostrato recentemente che introducendo

1091
00:40:10,680 --> 00:40:14,940
l'algebrica  vincoli in uh in un

1092
00:40:14,940 --> 00:40:17,040
processo di apprendimento in questo caso era

1093
00:40:17,040 --> 00:40:20,820
come il movimento di uh e e agenti in

1094
00:40:20,820 --> 00:40:23,220
un ambiente dicendo che devi

1095
00:40:23,220 --> 00:40:24,780
preservare una specie di questa

1096
00:40:24,780 --> 00:40:27,540
struttura algebrica se mi muovo in un cerchio ovest

1097
00:40:27,540 --> 00:40:29,280
nord-est sud mi ritrovo di nuovo allo

1098
00:40:29,280 --> 00:40:31,440
stesso  punto Ancora una volta introducendo questi

1099
00:40:31,440 --> 00:40:32,820
tipi di vincoli

1100
00:40:32,820 --> 00:40:35,040
si ottiene l'emergere di rappresentazioni simili a celle della griglia,

1101
00:40:35,040 --> 00:40:36,900


1102
00:40:36,900 --> 00:40:39,359
um quindi sarei interessato a vedere come questa

1103
00:40:39,359 --> 00:40:41,880
idea di struttura rappresentazionale può

1104
00:40:41,880 --> 00:40:43,980
aiutarci a spiegare forse più delle nostre

1105
00:40:43,980 --> 00:40:45,480
scoperte scientifiche che stiamo trovando anche

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
um e  e come questo si collega ai

1108
00:40:48,359 --> 00:40:51,540
modelli generativi nel loro complesso

1109
00:40:51,540 --> 00:40:52,800
um e poi finalmente penso che ci sia

1110
00:40:52,800 --> 00:40:54,599
qualcosa da dire anche sulla

1111
00:40:54,599 --> 00:40:56,099
possibilità cognitiva di questi modelli,

1112
00:40:56,099 --> 00:40:57,420
forse non li testeremo solo

1113
00:40:57,420 --> 00:40:59,579
dal punto di vista delle neuroscienze

1114
00:40:59,579 --> 00:41:01,020
ma anche dal

1115
00:41:01,020 --> 00:41:03,839
punto di vista della scienza micrognitiva  per esempio ci sono queste uh

1116
00:41:03,839 --> 00:41:06,000
matrici progressive di Ravens sulla sinistra

1117
00:41:06,000 --> 00:41:08,640
dove devi dire quale di

1118
00:41:08,640 --> 00:41:11,099
queste immagini ha maggiori probabilità di adattarsi a

1119
00:41:11,099 --> 00:41:12,599
questo schema

1120
00:41:12,599 --> 00:41:14,760
um o per esempio Quanto è probabile che

1121
00:41:14,760 --> 00:41:16,740
questa torre di Jenga cada quando ti

1122
00:41:16,740 --> 00:41:19,740
fermi  un blocco specifico o/o

1123
00:41:19,740 --> 00:41:22,740
con una determinata struttura e penso che queste

1124
00:41:22,740 --> 00:41:24,599
cose siano

1125
00:41:24,599 --> 00:41:26,640
questi tipi di test che stanno davvero testando

1126
00:41:26,640 --> 00:41:28,619
se i nostri modelli del mondo che stiamo costruendo

1127
00:41:28,619 --> 00:41:31,560
sono simili ai tipi di modelli che

1128
00:41:31,560 --> 00:41:33,660
abbiamo innatamente con il nostro buon senso

1129
00:41:33,660 --> 00:41:36,060
come esseri umani o  come esseri che vivono in un

1130
00:41:36,060 --> 00:41:38,400
mondo naturale e ho svolto del

1131
00:41:38,400 --> 00:41:40,440
lavoro preliminare in questa direzione,

1132
00:41:40,440 --> 00:41:43,079
penso molto preliminare e non così

1133
00:41:43,079 --> 00:41:45,480
complicato, ma una specie di tentativo

1134
00:41:45,480 --> 00:41:47,820
di modellare le illusioni visive, quindi se prendi

1135
00:41:47,820 --> 00:41:50,520
un set di dati davvero semplice di una barra in movimento

1136
00:41:50,520 --> 00:41:52,980
stimoli o una barra o un fotogramma statico e

1137
00:41:52,980 --> 00:41:54,960
lo sposti un po' puoi vedere che

1138
00:41:54,960 --> 00:41:57,060
il modello dedurrà effettivamente quel

1139
00:41:57,060 --> 00:41:58,800
fotogramma mancante e quindi

1140
00:41:58,800 --> 00:42:01,079
dedurrà anche il movimento continuo, quindi è come

1141
00:42:01,079 --> 00:42:03,300
superare la traiettoria di ciò che gli

1142
00:42:03,300 --> 00:42:05,820
stimoli effettivi gli stanno fornendo prima di

1143
00:42:05,820 --> 00:42:08,760
correggerlo nuovamente  quindi penso che modellare le

1144
00:42:08,760 --> 00:42:10,320
illusioni sia sicuramente un

1145
00:42:10,320 --> 00:42:12,660
modo interessante per studiare se i nostri modelli del mondo sono

1146
00:42:12,660 --> 00:42:14,760
simili ai tipi di modelli che

1147
00:42:14,760 --> 00:42:16,619
abbiamo noi stessi

1148
00:42:16,619 --> 00:42:19,619
quindi in conclusione uh sì, penso che

1149
00:42:19,619 --> 00:42:21,900
a priori topografici potremmo dimostrare che

1150
00:42:21,900 --> 00:42:23,220
hanno effettivamente imparato

1151
00:42:23,220 --> 00:42:24,839
rappresentazioni strutturate o modelli del mondo strutturati

1152
00:42:24,839 --> 00:42:26,700
questo  la struttura appresa è

1153
00:42:26,700 --> 00:42:29,160
flessibile e adattabile a

1154
00:42:29,160 --> 00:42:30,780
trasformazioni arbitrarie, a differenza degli

1155
00:42:30,780 --> 00:42:33,720
equivarianti tradizionali e dei fornitori topografici,

1156
00:42:33,720 --> 00:42:35,579
può essere indotta statisticamente come abbiamo fatto

1157
00:42:35,579 --> 00:42:37,619
nella vae topografica o attraverso

1158
00:42:37,619 --> 00:42:39,480
la dinamica come stavamo mostrando in questi

1159
00:42:39,480 --> 00:42:42,000
modelli di tipo macchina a onde neurali,

1160
00:42:42,000 --> 00:42:44,460
quindi per concludere vi lascio con questo

1161
00:42:44,460 --> 00:42:46,980
citazione che ho trovato nell'articolo di Fukushima

1162
00:42:46,980 --> 00:42:50,280
del 1980 che pensavo fosse molto in anticipo

1163
00:42:50,280 --> 00:42:52,079
sui tempi in cui dice che se potessimo

1164
00:42:52,079 --> 00:42:53,520
creare un modello di rete neurale che abbia

1165
00:42:53,520 --> 00:42:55,020
la stessa capacità di

1166
00:42:55,020 --> 00:42:57,060
riconoscimento di schemi di un essere umano

1167
00:42:57,060 --> 00:42:58,800
ci darebbe un indizio potente rispetto a

1168
00:42:58,800 --> 00:43:00,000
comprendere il meccanismo neurale nel

1169
00:43:00,000 --> 00:43:03,240
cervello quindi è un po' penso che alcuni

1170
00:43:03,240 --> 00:43:06,119
degli obiettivi che stiamo perseguendo qui

1171
00:43:06,119 --> 00:43:08,220
quindi penso che sia il mio consulente Max i miei

1172
00:43:08,220 --> 00:43:11,280
coautori Patrick UA Emil jinghian e

1173
00:43:11,280 --> 00:43:17,359
Yorn e sono interessato alla discussione grazie va bene va

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
bene  grazie fantastico

1176
00:43:27,480 --> 00:43:31,079
presentazione molto interessante

1177
00:43:31,079 --> 00:43:33,480
molti punti da cui iniziare forse solo uh

1178
00:43:33,480 --> 00:43:36,000
cosa ti ha portato a questo lavoro

1179
00:43:36,000 --> 00:43:38,520
un piccolo contesto su come sei arrivato a

1180
00:43:38,520 --> 00:43:43,819
questo lavoro per la tua direzione di dottorato

1181
00:43:43,920 --> 00:43:45,119
sì

1182
00:43:45,119 --> 00:43:46,020
um

1183
00:43:46,020 --> 00:43:49,200
voglio dire che abbiamo studiato non il mio

1184
00:43:49,200 --> 00:43:51,000
gruppo che  Sono all'università e sto

1185
00:43:51,000 --> 00:43:52,700
studiando

1186
00:43:52,700 --> 00:43:56,640
da un po' le rappresentazioni strutturate dal punto di vista matematico,

1187
00:43:56,640 --> 00:43:58,319
dove alcune persone hanno dei

1188
00:43:58,319 --> 00:44:00,240
modelli, uh servono come il

1189
00:44:00,240 --> 00:44:01,740
codificatore automatico variazionale,

1190
00:44:01,740 --> 00:44:04,680
um e

1191
00:44:04,680 --> 00:44:06,960
immagino che qualcosa che è sempre

1192
00:44:06,960 --> 00:44:08,460
stato

1193
00:44:08,460 --> 00:44:11,220
è un modello  che rispetta

1194
00:44:11,220 --> 00:44:13,560
perfettamente le rotazioni 2D, ma se vogliamo

1195
00:44:13,560 --> 00:44:15,960
fare rotazioni 3D non possiamo farlo

1196
00:44:15,960 --> 00:44:17,819
perché non è un gruppo in termini di

1197
00:44:17,819 --> 00:44:19,740
proiezione su un piano 2D, stai

1198
00:44:19,740 --> 00:44:21,180
perdendo informazioni quando questa cosa

1199
00:44:21,180 --> 00:44:23,460
ruota, ad esempio

1200
00:44:23,460 --> 00:44:24,240
um

1201
00:44:24,240 --> 00:44:26,280
o  qualsiasi tipo di

1202
00:44:26,280 --> 00:44:27,960
Trasformazione naturale come stavo cercando di

1203
00:44:27,960 --> 00:44:29,339
sottolineare all'inizio Penso che si

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
trattasse di pensare a come il cervello

1206
00:44:31,740 --> 00:44:34,020
modella le Trasformazioni naturali è

1207
00:44:34,020 --> 00:44:35,400
qualcosa che in questi attuali

1208
00:44:35,400 --> 00:44:37,200
Framework

1209
00:44:37,200 --> 00:44:41,099
dove vedi l'azione giocare un ruolo

1210
00:44:41,099 --> 00:44:44,579
in termini di autocodificatore variazionale

1211
00:44:44,579 --> 00:44:48,420
modelli che includono non solo

1212
00:44:48,420 --> 00:44:50,520
modelli esterni ma anche le

1213
00:44:50,520 --> 00:44:52,380
conseguenze dell'azione o la

1214
00:44:52,380 --> 00:44:55,800
struttura strutturale del modello mondiale con l'azione

1215
00:44:55,800 --> 00:44:58,619
giusto sì no, è una buona domanda e

1216
00:44:58,619 --> 00:45:01,319
penso che le inferenze agite uh

1217
00:45:01,319 --> 00:45:03,839
sia effettivamente la risposta Penso

1218
00:45:03,839 --> 00:45:05,940
che sia una buona risposta a questa

1219
00:45:05,940 --> 00:45:09,000
um lo so lì  sono

1220
00:45:09,000 --> 00:45:11,099
quadri di apprendimento per rinforzo che

1221
00:45:11,099 --> 00:45:12,660
utilizzano

1222
00:45:12,660 --> 00:45:15,060
tipi di modelli mondiali formati esternamente

1223
00:45:15,060 --> 00:45:17,280
in modo da addestrare un vae o qualcosa del genere e quindi

1224
00:45:17,280 --> 00:45:19,800
utilizzare quella rappresentazione nel proprio

1225
00:45:19,800 --> 00:45:23,040
sistema di apprendimento per rinforzo, ma

1226
00:45:23,040 --> 00:45:24,720
penso che avere un

1227
00:45:24,720 --> 00:45:26,520
sistema completo che sia un unico

1228
00:45:26,520 --> 00:45:30,780
obiettivo con uh azione  come parte della

1229
00:45:30,780 --> 00:45:33,660
verosimiglianza dei dati e uh

1230
00:45:33,660 --> 00:45:35,280
sì, penso che sia molto più elegante

1231
00:45:35,280 --> 00:45:38,940
e quindi ne sono un grande sostenitore.

1232
00:45:38,940 --> 00:45:39,960
um,

1233
00:45:39,960 --> 00:45:43,140
non sono arrivato così lontano da studiare come

1234
00:45:43,140 --> 00:45:45,480
questi modelli mondiali strutturati in un vae

1235
00:45:45,480 --> 00:45:47,520
o non l'ho fatto'  Non ci ho lavorato affatto, ma

1236
00:45:47,520 --> 00:45:48,780
penso che sarebbe sicuramente molto

1237
00:45:48,780 --> 00:45:50,819
interessante vedere se avere un

1238
00:45:50,819 --> 00:45:52,339
modello mondiale più strutturato

1239
00:45:52,339 --> 00:45:54,839
uh in un codificatore automatico variazionale

1240
00:45:54,839 --> 00:45:56,880
sarebbe vantaggioso in questo anche in un

1241
00:45:56,880 --> 00:45:58,319
ambiente attivo, penso che sarebbe

1242
00:45:58,319 --> 00:46:00,119
fantastico, voglio dire, io  pensa ad

1243
00:46:00,119 --> 00:46:03,599
alcuni di questi esempi come uh mostrati

1244
00:46:03,599 --> 00:46:05,579
prima come l'emergere di cellule della griglia e

1245
00:46:05,579 --> 00:46:07,500
cose come questa forse puntano verso

1246
00:46:07,500 --> 00:46:08,880
quella direzione

1247
00:46:08,880 --> 00:46:10,560
okay forse il cervello sta facendo qualcosa è

1248
00:46:10,560 --> 00:46:12,540
davvero ovviamente ha molta

1249
00:46:12,540 --> 00:46:13,680
struttura

1250
00:46:13,680 --> 00:46:15,359
um questo chiaramente deve essere utile per

1251
00:46:15,359 --> 00:46:19,140
eseguire azioni in alcuni

1252
00:46:19,140 --> 00:46:21,720
oh sì, ho sentito che un

1253
00:46:21,720 --> 00:46:24,480
parallelo davvero carino che hai portato con

1254
00:46:24,480 --> 00:46:28,040
il discorso è che le unità connesse localmente

1255
00:46:28,040 --> 00:46:30,960
hanno permesso ai tuoi modelli di

1256
00:46:30,960 --> 00:46:33,780
incorporare strutturalmente il

1257
00:46:33,780 --> 00:46:35,640
vincolo e il modello convoluzionale e che hanno portato a

1258
00:46:35,640 --> 00:46:37,500
questi modelli emergenti e poi

1259
00:46:37,500 --> 00:46:41,339
analogamente c'era il uh Doral

1260
00:46:41,339 --> 00:46:45,680
dove  avevano il vincolo di esplorazione del percorso

1261
00:46:45,680 --> 00:46:48,359
giusto e quindi è

1262
00:46:48,359 --> 00:46:50,280
interessante

1263
00:46:50,280 --> 00:46:53,760
pensare a queste azioni o

1264
00:46:53,760 --> 00:46:56,819
euristiche politiche o scarsità come

1265
00:46:56,819 --> 00:46:59,579
un'esplorazione motoria congiunta alla fine si

1266
00:46:59,579 --> 00:47:02,339
capisce che ci sono due

1267
00:47:02,339 --> 00:47:04,980
modi reciprocamente opposti per muovere un'articolazione

1268
00:47:04,980 --> 00:47:07,079
e quindi la composizionalità  attraverso le

1269
00:47:07,079 --> 00:47:09,119
articolazioni può essere appreso a questi

1270
00:47:09,119 --> 00:47:10,680
livelli più alti una volta bloccato ai

1271
00:47:10,680 --> 00:47:14,480
livelli più bassi quindi è un modo molto attraente

1272
00:47:14,480 --> 00:47:17,599
e

1273
00:47:17,599 --> 00:47:20,460
rilevante per la nicchia per generalizzare

1274
00:47:20,460 --> 00:47:23,819
perché è basato sia sui

1275
00:47:23,819 --> 00:47:25,740
vincoli effettivi del mondo ma

1276
00:47:25,740 --> 00:47:27,720
soprattutto attraverso l'azione che

1277
00:47:27,720 --> 00:47:29,460
potenzialmente incorpora qualcosa che è

1278
00:47:29,460 --> 00:47:31,380
abbastanza semplice, vero, sì,

1279
00:47:31,380 --> 00:47:33,599
no, penso che sia assolutamente

1280
00:47:33,599 --> 00:47:36,599
vero, questo è davvero un buon punto se uh se

1281
00:47:36,599 --> 00:47:38,339
hai dei vincoli derivanti dalle tue

1282
00:47:38,339 --> 00:47:40,500
stesse azioni, allora

1283
00:47:40,500 --> 00:47:42,839
sarebbe estremamente utile per aiutarti

1284
00:47:42,839 --> 00:47:44,819
a strutturare il

1285
00:47:44,819 --> 00:47:47,460
tuo spazio latente e penso che sì,

1286
00:47:47,460 --> 00:47:48,480
immagino una cosa  Volevo menzionare che

1287
00:47:48,480 --> 00:47:49,980
c'è

1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
qualcosa che mi ha fatto pensare, ad

1290
00:47:52,740 --> 00:47:55,500
esempio, al lavoro di Stefano Fousey sulla

1291
00:47:55,500 --> 00:47:58,859
geometria rappresentativa che

1292
00:47:58,859 --> 00:48:01,920
determina il modo in cui noi

1293
00:48:01,920 --> 00:48:04,440
quanto generalizzabile è una data comprensione

1294
00:48:04,440 --> 00:48:08,099
di un sistema e penso che se riesci a

1295
00:48:08,099 --> 00:48:11,880
capire come sono questi insiemi di

1296
00:48:11,880 --> 00:48:14,520
attività  separabile o

1297
00:48:14,520 --> 00:48:16,079
separabile altamente parallelo con un

1298
00:48:16,079 --> 00:48:18,839
classificatore lineare essenzialmente allora sarai in

1299
00:48:18,839 --> 00:48:20,700
grado di fare generalizzazioni e

1300
00:48:20,700 --> 00:48:23,099
penso che imponendo questi tipi di pregiudizi

1301
00:48:23,099 --> 00:48:25,040
o potenzialmente attraverso

1302
00:48:25,040 --> 00:48:27,000
vincoli imposti da un'azione

1303
00:48:27,000 --> 00:48:28,740
come questa

1304
00:48:28,740 --> 00:48:32,040
stai cedendo o in qualche modo inducendo un

1305
00:48:32,040 --> 00:48:33,660
migliore geometria rappresentativa e

1306
00:48:33,660 --> 00:48:35,220
questo ha tutti i tipi di vantaggi per la composizionalità simile,

1307
00:48:35,220 --> 00:48:36,660


1308
00:48:36,660 --> 00:48:39,359
sì, la nostra generalizzazione, quindi

1309
00:48:39,359 --> 00:48:41,760
è un ottimo punto interessante, sì,

1310
00:48:41,760 --> 00:48:43,440
un'area molto interessante, va bene, leggerò

1311
00:48:43,440 --> 00:48:45,960
alcune domande dalla chat dal vivo,

1312
00:48:45,960 --> 00:48:48,420
amore evolve, ha scritto

1313
00:48:48,420 --> 00:48:52,260
eventuali limitazioni pratiche o osservate

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
sull'apprendimento delle illusioni di modellazione  le comunità sono non sono

1316
00:49:00,420 --> 00:49:03,060
fobiate non hai un centro di

1317
00:49:03,060 --> 00:49:05,940
sguardo quindi non hai nemmeno

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
un tempo, intendo la maggior parte delle

1320
00:49:11,460 --> 00:49:13,260
reti neurali convoluzionali Sto usando questo tipo di

1321
00:49:13,260 --> 00:49:15,599
reti neurali ricorrenti ma il tempo

1322
00:49:15,599 --> 00:49:18,420
non è così chiaramente definito  in questi modelli

1323
00:49:18,420 --> 00:49:20,220
così come è in un ambiente temporale continuo

1324
00:49:20,220 --> 00:49:23,400
per un essere umano sottoposto a una prova di illusione

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
ehm

1327
00:49:25,319 --> 00:49:27,480
e penso che la combinazione di questi due

1328
00:49:27,480 --> 00:49:30,359
del fatto che come essere umano o la maggior parte delle

1329
00:49:30,359 --> 00:49:33,300
cose uh il

1330
00:49:33,300 --> 00:49:35,940
tuo sguardo, le tue posizioni mutevoli e i

1331
00:49:35,940 --> 00:49:38,220
tuoi guadagni dipendano da  come se

1332
00:49:38,220 --> 00:49:40,140
guardassi a un'area particolare molti

1333
00:49:40,140 --> 00:49:42,780
test cognitivi e quindi penso che

1334
00:49:42,780 --> 00:49:46,560
sarebbe davvero utile se avessimo modelli che

1335
00:49:46,560 --> 00:49:48,540
sì, intendo imparare a pensare a

1336
00:49:48,540 --> 00:49:50,760
questo come a un tipo di azione come imparare

1337
00:49:50,760 --> 00:49:52,980
dove muovere lo sguardo, uno dei  il

1338
00:49:52,980 --> 00:49:54,420
più semplice possibile che aiuterebbe molto

1339
00:49:54,420 --> 00:49:56,220
per poter modellare le illusioni e

1340
00:49:56,220 --> 00:49:58,859
intendo solo che per me è come leggere un

1341
00:49:58,859 --> 00:50:00,720
articolo di alcuni esperimenti di scienze cognitive

1342
00:50:00,720 --> 00:50:02,940
o su qualche illusione ed

1343
00:50:02,940 --> 00:50:05,160
è come se pensassi a okay, posso inserire questo

1344
00:50:05,160 --> 00:50:07,560
set di dati nel mio  modellalo e provalo e la

1345
00:50:07,560 --> 00:50:08,579
maggior parte delle volte la risposta è no

1346
00:50:08,579 --> 00:50:10,619
perché non ho un modello che

1347
00:50:10,619 --> 00:50:12,900
si guardi intorno o abbia un campo

1348
00:50:12,900 --> 00:50:14,660
visivo ristretto qualcosa del genere quindi sì,

1349
00:50:14,660 --> 00:50:16,619
penso che sia una delle

1350
00:50:16,619 --> 00:50:19,680
limitazioni, un'altra è

1351
00:50:19,680 --> 00:50:20,579
um

1352
00:50:20,579 --> 00:50:22,920
sì, fai il  esperimento molto più

1353
00:50:22,920 --> 00:50:24,900
complicato, quindi questa è una delle

1354
00:50:24,900 --> 00:50:27,359
limitazioni pratiche

1355
00:50:27,359 --> 00:50:30,240
wow, la risposta fantastica mi fa pensare a un

1356
00:50:30,240 --> 00:50:33,920
foglio con lettere che ruotano su un tavolo

1357
00:50:33,920 --> 00:50:36,780
che è la rotazione delle cifre grandi punti

1358
00:50:36,780 --> 00:50:38,460
sulla foveazione e sulla dinamica

1359
00:50:38,460 --> 00:50:40,079
dell'illusione Penso che tu abbia effettivamente

1360
00:50:40,079 --> 00:50:42,599
menzionato un'illusione che  hai comunque

1361
00:50:42,599 --> 00:50:43,980
menzionato nel contesto della generalizzazione

1362
00:50:43,980 --> 00:50:46,859
che la rotazione sullo

1363
00:50:46,859 --> 00:50:49,500
schermo bidimensionale non si generalizza a tre

1364
00:50:49,500 --> 00:50:52,920
dimensioni e che il collasso

1365
00:50:52,920 --> 00:50:55,559
o la riduzione dimensionale è la base della

1366
00:50:55,559 --> 00:50:58,619
proiezione del cubo Illusioni e rotazione del cubo e della figura

1367
00:50:58,619 --> 00:51:01,880
Illusioni è sul tuo schermo

1368
00:51:01,880 --> 00:51:05,280
e c'è  una silhouette o ci sono degli

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
stimoli ambigui che indicano che un generativo è vicino alla

1371
00:51:09,839 --> 00:51:12,359
criticità o una biforcazione nei

1372
00:51:12,359 --> 00:51:13,680
modelli degenerativi, quindi potrebbe

1373
00:51:13,680 --> 00:51:17,160
rappresentarlo in un modo o nell'altro

1374
00:51:17,160 --> 00:51:19,920
e quindi molte delle illusioni di commutazione

1375
00:51:19,920 --> 00:51:22,020
sono basate solo sulla piattezza delle

1376
00:51:22,020 --> 00:51:23,819
immagini

1377
00:51:23,819 --> 00:51:26,280
, sulle limitazioni e sulla generalizzazione

1378
00:51:26,280 --> 00:51:28,740
che vengono rivelati da quel

1379
00:51:28,740 --> 00:51:32,460
diritto sì sì penso che ci sia anche Oh

1380
00:51:32,460 --> 00:51:34,859
Sì da qualche parte scusa c'è del lavoro o

1381
00:51:34,859 --> 00:51:35,880


1382
00:51:35,880 --> 00:51:37,619
possono sostenere che le persone hanno

1383
00:51:37,619 --> 00:51:39,480
un'immagine tridimensionale nelle loro teste

1384
00:51:39,480 --> 00:51:42,000
come anche Nancy Ken era una o i suoi

1385
00:51:42,000 --> 00:51:45,119
laterali di recente ma e lo dimostra sì, non lo so

1386
00:51:45,119 --> 00:51:48,119
Non so che i nostri modelli abbiano che

1387
00:51:48,119 --> 00:51:50,460
non è molto grande

1388
00:51:50,460 --> 00:51:53,700
comunque sì, è piuttosto interessante, va

1389
00:51:53,700 --> 00:51:56,160
bene da upcycle Club nella

1390
00:51:56,160 --> 00:51:58,200
chat hanno scritto complimenti

1391
00:51:58,200 --> 00:52:00,000
se sei in grado di imparare in modo quasi altrettanto

1392
00:52:00,000 --> 00:52:02,160
efficace se immagini di volere

1393
00:52:02,160 --> 00:52:03,780
che sia solo un singolo neurone  attivo per ogni

1394
00:52:03,780 --> 00:52:06,540
esempio uh il tuo modello

1395
00:52:06,540 --> 00:52:08,579
proverà a memorizzare la progettazione del set di dati

1396
00:52:08,579 --> 00:52:10,980
o qualcosa del genere

1397
00:52:10,980 --> 00:52:12,180
um e non avrai abbastanza

1398
00:52:12,180 --> 00:52:14,940
capacità quindi sì, penso che l'ottimizzazione di quel

1399
00:52:14,940 --> 00:52:18,359
livello di scarsità sia certamente uh

1400
00:52:18,359 --> 00:52:22,200
un fattore importante e

1401
00:52:22,200 --> 00:52:25,020
sì  quando guardi la probabilità se stai

1402
00:52:25,020 --> 00:52:26,220
parlando se stai raddoppiando il

1403
00:52:26,220 --> 00:52:28,579
framework in genere questo viene bilanciato

1404
00:52:28,579 --> 00:52:32,040
automaticamente con la probabilità stessa

1405
00:52:32,040 --> 00:52:33,000
um

1406
00:52:33,000 --> 00:52:34,380
se non stai generando modellizzazione

1407
00:52:34,380 --> 00:52:35,760
hai solo una penalità di scarsità che

1408
00:52:35,760 --> 00:52:38,460
vorrai sintonizzare  quel parametro

1409
00:52:38,460 --> 00:52:40,980
okay sì, è solo per chiarire il

1410
00:52:40,980 --> 00:52:43,380
comportamento fuori controllo in Armina in cui la

1411
00:52:43,380 --> 00:52:45,599
rete diventa instabile o caotica a causa

1412
00:52:45,599 --> 00:52:47,040
di vari fattori come il

1413
00:52:47,040 --> 00:52:50,400
rumore dei circuiti di feedback o gli input contraddittori

1414
00:52:50,400 --> 00:52:52,380
ehm

1415
00:52:52,380 --> 00:52:54,180
sì, immagino di non averlo considerato

1416
00:52:54,180 --> 00:52:55,859
in un ambiente ricorrente in cui

1417
00:52:55,859 --> 00:52:58,500
otterresti cicli di feedback

1418
00:52:58,500 --> 00:52:59,460
um

1419
00:52:59,460 --> 00:53:01,800
ma potrei sì, potrei vedere

1420
00:53:01,800 --> 00:53:04,319
esempi di avversari potenzialmente

1421
00:53:04,319 --> 00:53:07,800
influenzati dal tuo livello di sparsità

1422
00:53:07,800 --> 00:53:09,119
um

1423
00:53:09,119 --> 00:53:10,859
il punto interessante è cosa

1424
00:53:10,859 --> 00:53:12,660
saresti più suscettibile o meno suscettibile

1425
00:53:12,660 --> 00:53:16,040
a condividere esempi che non conosco

1426
00:53:16,040 --> 00:53:19,440
bene la

1427
00:53:19,440 --> 00:53:21,720
proiezione della sparsificazione da a  modello dimensionale superiore completamente connesso

1428
00:53:21,720 --> 00:53:23,579
fino a diventare

1429
00:53:23,579 --> 00:53:25,619
progressivamente più piccolo

1430
00:53:25,619 --> 00:53:27,540
si capiscono abbastanza bene in generale

1431
00:53:27,540 --> 00:53:29,760
quali sono i compromessi sono calcoli più semplici

1432
00:53:29,760 --> 00:53:34,079
un modello più piccolo più sparso

1433
00:53:34,079 --> 00:53:36,420
il grafico di base sarà più chiaro

1434
00:53:36,420 --> 00:53:39,119
da rappresentare e quindi avrà anche

1435
00:53:39,119 --> 00:53:41,339
tutti gli altri scambi  con falsi

1436
00:53:41,339 --> 00:53:43,680
positivi e negativi della generalizzazione,

1437
00:53:43,680 --> 00:53:45,720
ma è per questo che è un processo di adattamento iterativo,

1438
00:53:45,720 --> 00:53:47,579
quindi

1439
00:53:47,579 --> 00:53:49,760
immagino come fa il tuo

1440
00:53:49,760 --> 00:53:52,800


1441
00:53:52,800 --> 00:53:55,700
equilibrio di approccio alla sparsificazione a

1442
00:53:56,700 --> 00:53:59,520
non utilizzare AIC o Bic o qualche altro

1443
00:53:59,520 --> 00:54:01,619
approccio di adattamento del modello per determinare la

1444
00:54:01,619 --> 00:54:03,660
sparsificazione rilevante

1445
00:54:03,660 --> 00:54:07,079
per un dato input

1446
00:54:07,079 --> 00:54:09,780
come fai  determina come come nella

1447
00:54:09,780 --> 00:54:11,940
regressione con il lazo come come fai a sapere

1448
00:54:11,940 --> 00:54:14,339
quanto come fai a superare la soglia quanti quanto

1449
00:54:14,339 --> 00:54:17,220
vuoi che sia scarso

1450
00:54:17,220 --> 00:54:19,559
sì, penso che ci sia molta buona

1451
00:54:19,559 --> 00:54:22,440
letteratura a riguardo e anche così ad alcune

1452
00:54:22,440 --> 00:54:25,319
persone piacciono ad Harvard e  alcune

1453
00:54:25,319 --> 00:54:29,520
persone stanno lavorando con ora e hanno fatto

1454
00:54:29,520 --> 00:54:31,559
queste

1455
00:54:31,559 --> 00:54:34,380


1456
00:54:34,380 --> 00:54:36,780
reti di sparsificazione iterative srotolate

1457
00:54:36,780 --> 00:54:37,800
in cui è come una rete neurale ricorrente

1458
00:54:37,800 --> 00:54:40,380
e si sparsifica iterativamente e

1459
00:54:40,380 --> 00:54:41,940
puoi dimostrare che questo produce qualcosa

1460
00:54:41,940 --> 00:54:45,780
come la perdita rossa o uh gruppo come

1461
00:54:45,780 --> 00:54:47,520
gruppo gruppo attivo Attivazioni sportive come

1462
00:54:47,520 --> 00:54:48,960
noi  stai usando qui

1463
00:54:48,960 --> 00:54:52,859
um in questa impostazione uh è in realtà solo

1464
00:54:52,859 --> 00:54:55,859
avendo questa

1465
00:54:55,859 --> 00:54:59,280
um questa costruzione di questa variabile T

1466
00:54:59,280 --> 00:55:04,079
dove abbiamo Z in alto e uh

1467
00:55:04,079 --> 00:55:07,859
e poi è in qualche modo delimitato da

1468
00:55:07,859 --> 00:55:09,119
queste

1469
00:55:09,119 --> 00:55:11,579
la somma delle variabili U in basso quindi

1470
00:55:11,579 --> 00:55:13,200
se W  forse non sono stato molto chiaro a riguardo si tratta di

1471
00:55:13,200 --> 00:55:16,500
una matrice che si connette ed è

1472
00:55:16,500 --> 00:55:18,359
ciò che definisce il gruppo, quindi sto

1473
00:55:18,359 --> 00:55:20,400
definendo la Varsity del gruppo che

1474
00:55:20,400 --> 00:55:22,380
collega tutte queste U insieme e quindi

1475
00:55:22,380 --> 00:55:23,940
l'idea è

1476
00:55:23,940 --> 00:55:27,540
come se tutte queste U fossero tutte insieme

1477
00:55:27,540 --> 00:55:31,740
esempi se tutti i tuoi usi uh non sono

1478
00:55:31,740 --> 00:55:35,520
attivi per un dato t

1479
00:55:35,520 --> 00:55:38,280
o se tutti i vario sono attivi per un dato

1480
00:55:38,280 --> 00:55:41,040
t uh quella variabile t sarà molto

1481
00:55:41,040 --> 00:55:42,780
piccola, giusto perché il tuo denominatore

1482
00:55:42,780 --> 00:55:44,339
sarà molto grande e questo induce

1483
00:55:44,339 --> 00:55:47,160
scarsità quindi è  uh è una

1484
00:55:47,160 --> 00:55:49,260
soddisfazione del vincolo se hai una

1485
00:55:49,260 --> 00:55:51,839
serie di U che sono tutte piccole uh allora

1486
00:55:51,839 --> 00:55:54,480
quel vincolo è soddisfatto e

1487
00:55:54,480 --> 00:55:57,180
ora Z può in un certo senso esprimersi da

1488
00:55:57,180 --> 00:56:00,240
solo ed è quello che poi

1489
00:56:00,240 --> 00:56:02,880
uh in un certo senso sì ottiene fino

1490
00:56:02,880 --> 00:56:06,180
all'attivazione  quindi questo è indotto da questi

1491
00:56:06,180 --> 00:56:07,020
due

1492
00:56:07,020 --> 00:56:09,300
termini di divergenza di cavolo qui questi

1493
00:56:09,300 --> 00:56:12,960
dicono quanto dista ciascun unhc

1494
00:56:12,960 --> 00:56:15,180
da una gaussiana e quindi attraverso questa

1495
00:56:15,180 --> 00:56:16,980
costruzione della variabile T di studente

1496
00:56:16,980 --> 00:56:20,880
stiamo effettivamente costruendo una

1497
00:56:20,880 --> 00:56:23,040
distribuzione a priori sparsa solo da queste

1498
00:56:23,040 --> 00:56:24,839
gaussiane ma in  i termini dell'atto l'

1499
00:56:24,839 --> 00:56:27,599
obiettivo reale uh i termini e l'

1500
00:56:27,599 --> 00:56:28,920
obiettivo che stiamo ottimizzando sono proprio

1501
00:56:28,920 --> 00:56:31,619
questi due termini KL che lo stanno spingendo

1502
00:56:31,619 --> 00:56:34,020
verso la scarsità in una certa misura e questo

1503
00:56:34,020 --> 00:56:36,359
viene bilanciato automaticamente con il

1504
00:56:36,359 --> 00:56:39,000
termine di probabilità qui attraverso il decodificatore,

1505
00:56:39,000 --> 00:56:41,220
quindi non lo facciamo  non abbiamo termini che stiamo sintonizzando

1506
00:56:41,220 --> 00:56:42,839
ma stiamo imparando i parametri di

1507
00:56:42,839 --> 00:56:44,280
questi diversi codificatori e poi

1508
00:56:44,280 --> 00:56:48,200
analizzando i fallimenti e le emergenze

1509
00:56:48,540 --> 00:56:49,920
oh va

1510
00:56:49,920 --> 00:56:52,859
bene un'altra domanda da Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas che ha scritto

1512
00:56:55,500 --> 00:56:59,160
parlando di sguardo e illusione è possibile separare gli

1513
00:56:59,160 --> 00:57:01,920
studi sulle costanti nei neonati

1514
00:57:01,920 --> 00:57:04,260
nell'illusione di livello inferiore Rel

1515
00:57:04,260 --> 00:57:06,140
forse costanza concettuale di livello superiore

1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
uh puoi leggere

1518
00:57:15,720 --> 00:57:18,300
l'attuale tipo di architettura potrebbero

1519
00:57:18,300 --> 00:57:23,520
gli studi sulle costanti nei neonati

1520
00:57:23,520 --> 00:57:26,640
um costanti cognitive essere separati

1521
00:57:26,640 --> 00:57:31,619
sì probabilmente non lo sono non sono uh

1522
00:57:31,619 --> 00:57:33,900
un esperto o addirittura molto familiare

1523
00:57:33,900 --> 00:57:35,460
con

1524
00:57:35,460 --> 00:57:37,859
studi simili sulla permanenza degli oggetti, neonati

1525
00:57:37,859 --> 00:57:40,260
e cose sulla costanza, quindi penso che

1526
00:57:40,260 --> 00:57:42,300
sarebbe incredibilmente interessante

1527
00:57:42,300 --> 00:57:44,339
studiare le architetture delle reti neurali e

1528
00:57:44,339 --> 00:57:46,740
quella era una specie di idea con

1529
00:57:46,740 --> 00:57:48,780
questa illusione che stavo cercando di

1530
00:57:48,780 --> 00:57:51,780
modellare qui con questa linea  Non so

1531
00:57:51,780 --> 00:57:53,099
se sono stato molto chiaro a riguardo, ma

1532
00:57:53,099 --> 00:57:55,380
la riga superiore è l'input e

1533
00:57:55,380 --> 00:57:57,359
in effetti stiamo bloccando l'input per

1534
00:57:57,359 --> 00:58:00,119
un singolo fotogramma e volevo vedere se

1535
00:58:00,119 --> 00:58:03,240
la rete codifica che quella

1536
00:58:03,240 --> 00:58:05,400
cosa è ancora lì  quando quel fotogramma è

1537
00:58:05,400 --> 00:58:07,680
sparito, posso ancora decodificare la presenza

1538
00:58:07,680 --> 00:58:10,020
dell'oggetto dall'attività neurale uh

1539
00:58:10,020 --> 00:58:11,819
e poi cosa si sta deducendo sul

1540
00:58:11,819 --> 00:58:13,920
movimento a causa del fatto che ha

1541
00:58:13,920 --> 00:58:15,780
visto le barre in una posizione leggermente diversa

1542
00:58:15,780 --> 00:58:18,480
rispetto a prima quando dopo

1543
00:58:18,480 --> 00:58:20,520
il  il frame è sparito,

1544
00:58:20,520 --> 00:58:22,559
quindi

1545
00:58:22,559 --> 00:58:25,200
sì, penso che ci siano sicuramente più

1546
00:58:25,200 --> 00:58:27,240
livelli,

1547
00:58:27,240 --> 00:58:29,160
dove alcuni probabilmente sarebbero di

1548
00:58:29,160 --> 00:58:33,180
livello molto più basso e uh

1549
00:58:33,180 --> 00:58:35,880
forse la permanenza dell'oggetto a lungo termine

1550
00:58:35,880 --> 00:58:37,380
immagino sarebbe di

1551
00:58:37,380 --> 00:58:39,059
livello significativamente più alto,

1552
00:58:39,059 --> 00:58:39,900
um mi

1553
00:58:39,900 --> 00:58:41,760
fa pensare a quegli

1554
00:58:41,760 --> 00:58:44,640
esperimenti con i gatti  ai tempi

1555
00:58:44,640 --> 00:58:47,280
in cui era come se li allevassero

1556
00:58:47,280 --> 00:58:49,020
nell'oscurità tranne che per un'ora al giorno

1557
00:58:49,020 --> 00:58:51,000
li mettevano nel mondo verticale o nel

1558
00:58:51,000 --> 00:58:53,160
mondo orizzontale oppure vedevano solo linee orizzontali

1559
00:58:53,160 --> 00:58:57,299
o verticali uh e puoi vedere

1560
00:58:57,299 --> 00:58:59,880
l'organizzazione della loro corteccia cambiare

1561
00:58:59,880 --> 00:59:02,819
come loro  hanno meno ricettività sono

1562
00:59:02,819 --> 00:59:04,200
linee orizzontali se non hanno mai visto

1563
00:59:04,200 --> 00:59:06,420
linee orizzontali prima e poi

1564
00:59:06,420 --> 00:59:07,980
prendi un bastoncino e lo agiti davanti alla

1565
00:59:07,980 --> 00:59:09,599
loro faccia e se il bastoncino è

1566
00:59:09,599 --> 00:59:11,220
orizzontale semplicemente non fanno nulla

1567
00:59:11,220 --> 00:59:12,900
è verticale lo colpiscono

1568
00:59:12,900 --> 00:59:14,460
stanno cercando di colpirlo, è come se

1569
00:59:14,460 --> 00:59:15,900
non dovessero letteralmente barare

1570
00:59:15,900 --> 00:59:18,420
davanti alla loro faccia, quindi penso che in quel

1571
00:59:18,420 --> 00:59:20,700
caso questa sia la prova di una

1572
00:59:20,700 --> 00:59:24,260
carenza di basso livello e della vista che

1573
00:59:24,260 --> 00:59:26,940
contribuisce a una sorta di illusione,

1574
00:59:26,940 --> 00:59:28,980
quindi io io  penso che sì, potrebbe certamente

1575
00:59:28,980 --> 00:59:30,660
esserci qualche aspetto di questo anche nei neonati,

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
un punto molto curioso che hai sollevato

1578
00:59:36,420 --> 00:59:40,339
è stata la varietà animata e inanimata

1579
00:59:40,339 --> 00:59:43,619
con le piccole cose che sono

1580
00:59:43,619 --> 00:59:45,480
intermedie,

1581
00:59:45,480 --> 00:59:49,140
proprio cosa rappresenta

1582
00:59:49,140 --> 00:59:52,319
o è perché sono gestibili

1583
00:59:52,319 --> 00:59:55,619
o così?  potrebbe essere un insetto o potrebbe essere

1584
00:59:55,619 --> 00:59:57,839
qualcosa che potrebbe allontanarsi solo con il

1585
00:59:57,839 --> 01:00:01,380
vento o cosa dice

1586
01:00:01,380 --> 01:00:04,380
giusto sì,

1587
01:00:04,380 --> 01:00:08,280
quindi questo è il lavoro di Talia Conkle,

1588
01:00:08,280 --> 01:00:11,280
penso che sia stata quella a scoprire questa

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
organizzazione e hanno cercato di

1591
01:00:14,880 --> 01:00:16,440
capirlo  Non so, potrei

1592
01:00:16,440 --> 01:00:19,500
sbagliarmi, quindi consiglio alle persone di leggere il

1593
01:00:19,500 --> 01:00:21,359
suo lavoro sull'argomento se la chiamano

1594
01:00:21,359 --> 01:00:23,880
organizzazione tripartita, ma se

1595
01:00:23,880 --> 01:00:25,319
ricordo bene hanno

1596
01:00:25,319 --> 01:00:27,900
fatto molto lavoro di follow-up sul motivo per cui

1597
01:00:27,900 --> 01:00:30,780
esiste questa organizzazione e

1598
01:00:30,780 --> 01:00:33,180
alcune prove di

1599
01:00:33,180 --> 01:00:35,700
curvatura di questi oggetti e un po'

1600
01:00:35,700 --> 01:00:37,440
come la distanza da cui vedi gli oggetti

1601
01:00:37,440 --> 01:00:40,260
o come

1602
01:00:40,260 --> 01:00:43,319
oggetti animati o forse più sinuosi

1603
01:00:43,319 --> 01:00:45,599
o c'è lì indipendentemente da quale sia la

1604
01:00:45,599 --> 01:00:46,859
risposta effettiva c'erano molte

1605
01:00:46,859 --> 01:00:48,720
ipotesi diverse che derivavano

1606
01:00:48,720 --> 01:00:51,720
da proprietà simili di questi oggetti

1607
01:00:51,720 --> 01:00:54,000
forse proprietà di livello medio o basso

1608
01:00:54,000 --> 01:00:56,280
più che proprietà di livello superiore

1609
01:00:56,280 --> 01:00:57,599
Non so ancora se è stato

1610
01:00:57,599 --> 01:00:59,339
risolto esattamente se è come se l'

1611
01:00:59,339 --> 01:01:01,500
interazione come hai detto con gli

1612
01:01:01,500 --> 01:01:04,920
oggetti causi la separazione o

1613
01:01:04,920 --> 01:01:06,119
ehm

1614
01:01:06,119 --> 01:01:09,540
o sì, le forme generali di questi

1615
01:01:09,540 --> 01:01:12,299
oggetti io  scommetto che, come per la maggior parte delle cose,

1616
01:01:12,299 --> 01:01:13,980
è come una combinazione di tutto quanto

1617
01:01:13,980 --> 01:01:16,980
sopra, uh ma penso che la

1618
01:01:16,980 --> 01:01:18,480
cosa interessante da questo punto di vista della modellazione

1619
01:01:18,480 --> 01:01:19,859
sia che,

1620
01:01:19,859 --> 01:01:21,480
ehm,

1621
01:01:21,480 --> 01:01:24,059
questo è addestrato solo sulle

1622
01:01:24,059 --> 01:01:26,819
statistiche di correlazione dai set di dati dell'immagine

1623
01:01:26,819 --> 01:01:28,799
stessa, quindi non ha alcuna interazione con questo

1624
01:01:28,799 --> 01:01:32,760
non ha nozione di animazione, voglio dire, in

1625
01:01:32,760 --> 01:01:34,140
realtà si tratta solo di addestrare un modello su

1626
01:01:34,140 --> 01:01:37,859
Imagenet, solo immagini di cani, gatti, barche,

1627
01:01:37,859 --> 01:01:40,020
qualunque cosa e tuttavia raggiunge comunque questo

1628
01:01:40,020 --> 01:01:41,640
tipo di organizzazione, quindi ce n'è una

1629
01:01:41,640 --> 01:01:42,540
sorta di

1630
01:01:42,540 --> 01:01:44,940
caratteristiche semantiche,

1631
01:01:44,940 --> 01:01:46,740
giusto, abbiamo un'immagine, abbiamo un  rete

1632
01:01:46,740 --> 01:01:48,359
che può classificare

1633
01:01:48,359 --> 01:01:51,000
barche rispetto a cani rispetto ad altre 20 razze

1634
01:01:51,000 --> 01:01:53,640
di cani ma se

1635
01:01:53,640 --> 01:01:55,920
potrebbe anche avere qualche corrispondenza

1636
01:01:55,920 --> 01:01:57,900
con le statistiche sui risultati di livello inferiore quindi

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
sì, non lo so, immagino sì,

1639
01:02:03,960 --> 01:02:07,500
un'analogia provocatoria è stata lo

1640
01:02:07,500 --> 01:02:10,380
spostamento traslazionale

1641
01:02:10,380 --> 01:02:12,900
del mnist nella scrittura a mano

1642
01:02:12,900 --> 01:02:14,819
impostazione di riconoscimento

1643
01:02:14,819 --> 01:02:18,000
quali sono gli spostamenti traslazionali

1644
01:02:18,000 --> 01:02:20,160
che

1645
01:02:20,160 --> 01:02:22,740
esistono oggi qual è l'esempio di tre pixel

1646
01:02:22,740 --> 01:02:24,780
è che un attacco tempestivo progettato

1647
01:02:24,780 --> 01:02:27,540
su un llm o qualcosa del genere o

1648
01:02:27,540 --> 01:02:29,099
qualcosa di corretto viene inserito un carattere speciale

1649
01:02:29,099 --> 01:02:32,640
o o um uh qualche

1650
01:02:32,640 --> 01:02:35,160
sovrapposizione su un'immagine che non possiamo nemmeno

1651
01:02:35,160 --> 01:02:37,020
rilevalo,

1652
01:02:37,020 --> 01:02:39,359
quindi cosa pensi che siano queste sfide

1653
01:02:39,359 --> 01:02:42,839
e quali sono i modi in cui possiamo perseguirle sì,

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
assolutamente, voglio dire, penso che sia un po' il

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
modo in cui stavo pensando, è come

1658
01:02:50,099 --> 01:02:52,680
queste trasformazioni di simmetria

1659
01:02:52,680 --> 01:02:53,760
um

1660
01:02:53,760 --> 01:02:55,799
se stai pensando a modelli linguistici

1661
01:02:55,799 --> 01:02:57,420
tu  posso immaginare una

1662
01:02:57,420 --> 01:02:58,500
trasformazione di simmetria che è proprio come

1663
01:02:58,500 --> 01:03:00,240
sostituire una parola con un sinonimo o

1664
01:03:00,240 --> 01:03:03,780
qualcosa del genere uh hai capito che la frase per noi

1665
01:03:03,780 --> 01:03:06,000
significa esattamente la stessa cosa ma ora

1666
01:03:06,000 --> 01:03:07,380
all'improvviso il modello risponderà in modo

1667
01:03:07,380 --> 01:03:09,299
molto diverso

1668
01:03:09,299 --> 01:03:11,240
um

1669
01:03:11,240 --> 01:03:15,359
come la traduzione tra lingue questo

1670
01:03:15,359 --> 01:03:16,799
può essere visto come un  tipo di trasformazione

1671
01:03:16,799 --> 01:03:19,440
preserva per noi il

1672
01:03:19,440 --> 01:03:21,960
significato sottostante dell'input

1673
01:03:21,960 --> 01:03:24,900
ma al modello sembra

1674
01:03:24,900 --> 01:03:26,220
completamente diverso e vorremmo

1675
01:03:26,220 --> 01:03:28,380
avere modelli che si comportino in

1676
01:03:28,380 --> 01:03:29,940
modo prevedibile rispetto a questi

1677
01:03:29,940 --> 01:03:32,160
tipi di trasformazioni perché

1678
01:03:32,160 --> 01:03:35,040
penso che gli esseri umani si comportino in modo molto prevedibile

1679
01:03:35,040 --> 01:03:37,319
poiché questi  Trasformazioni e quando

1680
01:03:37,319 --> 01:03:39,920
abbiamo a che fare con i sistemi di intelligenza artificiale ci aspettiamo che

1681
01:03:39,920 --> 01:03:43,200
anche loro si comportino in questo modo e penso

1682
01:03:43,200 --> 01:03:45,480
che sia parte di ciò che causa molte

1683
01:03:45,480 --> 01:03:47,339
sfide nell'interazione con questi

1684
01:03:47,339 --> 01:03:49,460
sistemi e ho provato a fare una

1685
01:03:49,460 --> 01:03:52,500
dimostrazione approssimativa e sfacciata di ciò

1686
01:03:52,500 --> 01:03:54,960
con questo  orsi, quadrati e cose

1687
01:03:54,960 --> 01:03:58,440
come um ci aspettiamo che sia in

1688
01:03:58,440 --> 01:04:00,480
grado di fare qualcosa di semplice

1689
01:04:00,480 --> 01:04:02,220
come questo perché pensiamo che la maggior parte degli esseri umani

1690
01:04:02,220 --> 01:04:04,020
potrebbe farlo e tuttavia non è così e se

1691
01:04:04,020 --> 01:04:05,460
immagini che questo sia uno scenario critico

1692
01:04:05,460 --> 01:04:07,740
in cui ti aspetti questo e questo è un grosso

1693
01:04:07,740 --> 01:04:08,700
problema

1694
01:04:08,700 --> 01:04:11,099
um  come gestiamo la cosa, sì,

1695
01:04:11,099 --> 01:04:12,720
penso che sia più o meno quello che sto

1696
01:04:12,720 --> 01:04:15,859
cercando, penso che la

1697
01:04:16,319 --> 01:04:18,420


1698
01:04:18,420 --> 01:04:22,280
direzione che sto prendendo sia

1699
01:04:22,280 --> 01:04:26,940
più semplice e come elementi costitutivi dal basso verso l'alto

1700
01:04:26,940 --> 01:04:29,460
di

1701
01:04:29,460 --> 01:04:31,380
architetture o algoritmi di rete neurale

1702
01:04:31,380 --> 01:04:33,180
che

1703
01:04:33,180 --> 01:04:35,760
producono questi elementi emergenti

1704
01:04:35,760 --> 01:04:37,680
proprietà strutturali e penso che sia un

1705
01:04:37,680 --> 01:04:39,839
modo molto più generalizzabile piuttosto che

1706
01:04:39,839 --> 01:04:41,579
costruire qualcosa sopra ciò che

1707
01:04:41,579 --> 01:04:43,140
già abbiamo

1708
01:04:43,140 --> 01:04:43,920
um

1709
01:04:43,920 --> 01:04:45,839
penso che sia qualcosa che si ridimensionerà

1710
01:04:45,839 --> 01:04:47,819
molto meglio e corrisponderà anche di più a ciò che

1711
01:04:47,819 --> 01:04:50,299
fa il cervello,

1712
01:04:50,760 --> 01:04:52,740
molto interessante un tipo di

1713
01:04:52,740 --> 01:04:54,740
domanda implementativa quali sono i

1714
01:04:54,740 --> 01:04:57,000
requisiti computazionali per eseguire semplicemente questo o

1715
01:04:57,000 --> 01:04:59,400
com'è la quotidianità di essere uno

1716
01:04:59,400 --> 01:05:01,920
studente o un ricercatore che esegue varianti

1717
01:05:01,920 --> 01:05:04,380
di questi come usano terabyte di

1718
01:05:04,380 --> 01:05:07,020
dati e stai utilizzando calcoli di grandi dimensioni

1719
01:05:07,020 --> 01:05:08,940
o è qualcosa che le persone possono eseguire

1720
01:05:08,940 --> 01:05:11,880
da sole  laptop

1721
01:05:11,880 --> 01:05:13,980
Penso che quasi tutto ciò che ho presentato

1722
01:05:13,980 --> 01:05:17,099
oggi possa essere eseguito localmente, quindi

1723
01:05:17,099 --> 01:05:20,040
questa roba è semplicissima e puoi eseguirla, voglio dire, penserai di

1724
01:05:20,040 --> 01:05:20,760


1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
poterla eseguire sul tuo laptop se

1727
01:05:24,299 --> 01:05:25,980
vuoi allenarti e sperimentare le

1728
01:05:25,980 --> 01:05:27,420
diverse cose che sta succedendo  essere

1729
01:05:27,420 --> 01:05:30,119
piuttosto lento quindi consiglierei qualche

1730
01:05:30,119 --> 01:05:33,359
GPU commerciale come una su cui utilizzo praticamente

1731
01:05:33,359 --> 01:05:35,640
tutto come Nvidia 1080

1732
01:05:35,640 --> 01:05:38,819
piuttosto vecchi piuttosto economici ma hanno 12

1733
01:05:38,819 --> 01:05:41,099
giga di RAM o altro ed è

1734
01:05:41,099 --> 01:05:43,140
più che sufficiente per questi modelli quattro

1735
01:05:43,140 --> 01:05:46,440
gigabyte di RAM  Penso che una cosa che

1736
01:05:46,440 --> 01:05:48,839
alcune persone pensano sia strana è che faccio la maggior parte

1737
01:05:48,839 --> 01:05:51,480
dei miei esperimenti su cose come mnist quindi

1738
01:05:51,480 --> 01:05:54,780
sono immagini da 32 x 32 pixel perché posso

1739
01:05:54,780 --> 01:05:57,299
addestrarle in piccolo e vocalmente

1740
01:05:57,299 --> 01:06:00,000
um se vuoi farlo i miei esperimenti

1741
01:06:00,000 --> 01:06:02,460
o un infinito se vuoi  voglio fare cose

1742
01:06:02,460 --> 01:06:03,540
del genere, queste sono molto più

1743
01:06:03,540 --> 01:06:05,640
complicate questa hamiltoniana Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite qui stai entrando in modelli più grandi

1745
01:06:08,160 --> 01:06:09,780
che funzionano su più

1746
01:06:09,780 --> 01:06:12,240
GPU e quindi qui sta usando un cluster per

1747
01:06:12,240 --> 01:06:14,220
eseguire questi tipi di modelli

1748
01:06:14,220 --> 01:06:16,140
um ma direi che la maggior parte di  la singola

1749
01:06:16,140 --> 01:06:18,920
macchina con la GPU è più che sufficiente

1750
01:06:18,920 --> 01:06:21,680
o anche solo come in un notebook di collaborazione

1751
01:06:21,680 --> 01:06:24,539
qualcosa del genere se vuoi addestrare

1752
01:06:24,539 --> 01:06:26,520
qualcosa su imagenet diventa più

1753
01:06:26,520 --> 01:06:29,940
complicato e idealmente hai bisogno di

1754
01:06:29,940 --> 01:06:33,660
almeno una GPU in più ma sì,

1755
01:06:33,660 --> 01:06:35,160
non lo faccio  un sacco di cose su larga scala,

1756
01:06:35,160 --> 01:06:37,200
ma penso che sia sicuramente interessante

1757
01:06:37,200 --> 01:06:39,960
e c'è sicuramente molto altro che

1758
01:06:39,960 --> 01:06:42,539
puoi fare lì, ma per alcuni di questi tipi

1759
01:06:42,539 --> 01:06:45,480
di domande più semplici o fondamentali

1760
01:06:45,480 --> 01:06:47,760
non so come chiamarla, ehm,

1761
01:06:47,760 --> 01:06:52,500
una macchina più piccola è  bello e veloce così bello

1762
01:06:52,500 --> 01:06:54,660
utile va

1763
01:06:54,660 --> 01:06:58,140
bene leggerò un commento di Dave che

1764
01:06:58,140 --> 01:07:00,780
ricorda il commento di Bert DeVries durante

1765
01:07:00,780 --> 01:07:02,880
il simposio sull'inferenza attiva applicata

1766
01:07:02,880 --> 01:07:05,520
sull'opportunità di spendere meno

1767
01:07:05,520 --> 01:07:08,099
sforzo o ATP nel foraggiamento o nelle

1768
01:07:08,099 --> 01:07:09,780
situazioni di controllo in cui non abbiamo bisogno di molta

1769
01:07:09,780 --> 01:07:11,579
precisione Non lo faccio  non so se

1770
01:07:11,579 --> 01:07:13,859
lo ascolti, ma il professor DeVries ha menzionato i

1771
01:07:13,859 --> 01:07:17,520
modelli di precisione variabile e come

1772
01:07:17,520 --> 01:07:19,440
potrebbero essere utilizzati per

1773
01:07:19,440 --> 01:07:21,059
abilitare diverse funzionalità di

1774
01:07:21,059 --> 01:07:23,039
generalizzazione e

1775
01:07:23,039 --> 01:07:25,020
formazione del corso strutturale effettivo, nonché

1776
01:07:25,020 --> 01:07:27,059
requisiti computazionali ridotti,

1777
01:07:27,059 --> 01:07:29,880
ha qualche suggerimento su come

1778
01:07:29,880 --> 01:07:31,980
introdurlo?  distinzione in

1779
01:07:31,980 --> 01:07:33,900
inferenza attiva Teoria che tipo di

1780
01:07:33,900 --> 01:07:37,760
esperimenti potrebbero risolverlo

1781
01:07:38,400 --> 01:07:40,680
oh wow sì, è qualcosa che

1782
01:07:40,680 --> 01:07:42,660
non penso di avere molto intelligente

1783
01:07:42,660 --> 01:07:46,619
da dire al riguardo è del tutto onesto

1784
01:07:46,619 --> 01:07:48,740
um

1785
01:07:51,359 --> 01:07:53,460
è una domanda super interessante perché

1786
01:07:53,460 --> 01:07:56,220
penso che l'intuizione faccia un  ha molto

1787
01:07:56,220 --> 01:07:58,260
senso per me quello di cui

1788
01:07:58,260 --> 01:08:00,000
stai parlando

1789
01:08:00,000 --> 01:08:01,980
se capisco correttamente i tassi

1790
01:08:01,980 --> 01:08:05,160
di precisione variabili quando stai codificando nel

1791
01:08:05,160 --> 01:08:07,079
o nel tuo modello in generale facendo

1792
01:08:07,079 --> 01:08:07,740
calcoli

1793
01:08:07,740 --> 01:08:09,420
[Musica]

1794
01:08:09,420 --> 01:08:11,539
um

1795
01:08:13,200 --> 01:08:17,040
che in qualche modo ha un impatto sulle tue

1796
01:08:17,040 --> 01:08:19,080
prestazioni future come  una relazione con qualche

1797
01:08:19,080 --> 01:08:22,259
riserva di energia penso di sì e se

1798
01:08:22,259 --> 01:08:23,580
volessi trasformarlo in un

1799
01:08:23,580 --> 01:08:26,279
sistema di sforzo attivo dovresti avere

1800
01:08:26,279 --> 01:08:28,679
davvero un sistema incarnato in cui l'

1801
01:08:28,679 --> 01:08:31,500
agente ha una certa nozione di energia come una

1802
01:08:31,500 --> 01:08:34,439
riserva di energia interna e

1803
01:08:34,439 --> 01:08:36,238
sì qualcosa qualcosa che sta provando

1804
01:08:36,238 --> 01:08:38,520
per conservare mentre esegue le sue

1805
01:08:38,520 --> 01:08:40,500
azioni uh

1806
01:08:40,500 --> 01:08:42,359
e rimanere senza energia avrebbe

1807
01:08:42,359 --> 01:08:44,759
bisogno di qualcosa di negativo per gli agenti e

1808
01:08:44,759 --> 01:08:47,399
quindi forse potresti osservare una sorta di

1809
01:08:47,399 --> 01:08:48,679
emergenza

1810
01:08:48,679 --> 01:08:52,040
uh riduzione e

1811
01:08:52,040 --> 01:08:55,198
precisione di codifica o qualcosa del

1812
01:08:55,198 --> 01:08:57,479
genere mentre l'agente sta cercando di imparare

1813
01:08:57,479 --> 01:09:00,060
a  agire in modo più efficace potresti dover

1814
01:09:00,060 --> 01:09:02,520
dargli la capacità di controllare la sua

1815
01:09:02,520 --> 01:09:04,020
precisione

1816
01:09:04,020 --> 01:09:07,080
sì, come dico per la mia esperienza, ma

1817
01:09:07,080 --> 01:09:08,819
è un po' un pensiero

1818
01:09:08,819 --> 01:09:11,460
ok su questa diapositiva proprio qui prima

1819
01:09:11,460 --> 01:09:14,219
immagine molto bella è un po' come un

1820
01:09:14,219 --> 01:09:18,359
Jackson Pollock digitale,

1821
01:09:18,359 --> 01:09:22,920
ehm, se fosse  una dimensione dei dati di input più semplice

1822
01:09:22,920 --> 01:09:26,520
o semplicemente una complessità ridotta dei

1823
01:09:26,520 --> 01:09:27,719
modelli o se fosse una maggiore

1824
01:09:27,719 --> 01:09:30,000
complessità come apparirebbe diversa questa immagine

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
sì, quindi ho fatto alcuni esperimenti cercando di

1827
01:09:34,620 --> 01:09:36,738
cambiare queste

1828
01:09:36,738 --> 01:09:40,439
colonne di orientamento e

1829
01:09:40,439 --> 01:09:41,100
um

1830
01:09:41,100 --> 01:09:43,140
puoi sì, sostanzialmente cambiando i

1831
01:09:43,140 --> 01:09:44,520
parametri del modello puoi  fai in modo che

1832
01:09:44,520 --> 01:09:47,100
queste colonne siano più grandi puoi fare in modo che

1833
01:09:47,100 --> 01:09:49,380
non abbiano una struttura molto simile

1834
01:09:49,380 --> 01:09:51,660
a quella che vediamo negli esseri umani in cui

1835
01:09:51,660 --> 01:09:53,040
ti abbiamo puoi fare in modo che abbiano più

1836
01:09:53,040 --> 01:09:55,199
bande di attività

1837
01:09:55,199 --> 01:09:56,160
um

1838
01:09:56,160 --> 01:09:58,199
e anche come hai detto tu dipende

1839
01:09:58,199 --> 01:10:00,540
dal set di dati  che stai usando se utilizzo

1840
01:10:00,540 --> 01:10:03,179
come input delle graduazioni sinusoidali molto semplici

1841
01:10:03,179 --> 01:10:05,580
ottengo qualcosa del genere ottengo

1842
01:10:05,580 --> 01:10:07,920
qualcosa che è un po' più uh

1843
01:10:07,920 --> 01:10:11,640
curva di rotazione maggiore entropia

1844
01:10:11,640 --> 01:10:12,800
um

1845
01:10:12,800 --> 01:10:15,660
quindi penso che queste siano tutte

1846
01:10:15,660 --> 01:10:18,000
cose interessanti se vuoi studiare l'

1847
01:10:18,000 --> 01:10:19,320
emergere di  questo tipo di organizzazione

1848
01:10:19,320 --> 01:10:22,199
in un sistema naturale uh se hai un

1849
01:10:22,199 --> 01:10:24,120
modello che ora produce

1850
01:10:24,120 --> 01:10:25,860
un'organizzazione diversa per

1851
01:10:25,860 --> 01:10:28,620
impostazioni diverse è un vedere okay, allora quali

1852
01:10:28,620 --> 01:10:31,679
impostazioni corrispondono meglio ai nostri dati osservati

1853
01:10:31,679 --> 01:10:32,340
um

1854
01:10:32,340 --> 01:10:34,679
quindi sì,

1855
01:10:34,679 --> 01:10:36,540
posso inviarli se lo sei

1856
01:10:36,540 --> 01:10:38,520
interessato ma

1857
01:10:38,520 --> 01:10:40,580
ehm

1858
01:10:41,580 --> 01:10:44,219
sì, penso che anche un altro mi dispiace,

1859
01:10:44,219 --> 01:10:45,659
un altro punto interessante è

1860
01:10:45,659 --> 01:10:46,920
che i

1861
01:10:46,920 --> 01:10:51,480
diversi animali e i tipi di

1862
01:10:51,480 --> 01:10:53,219
selettività di orientamento e il diverso

1863
01:10:53,219 --> 01:10:54,659
numero di girandole alcuni animali non ce l'

1864
01:10:54,659 --> 01:10:57,420
hanno affatto, penso che forse i topi se sono

1865
01:10:57,420 --> 01:11:00,120
corretto, hai questo tipo di uh chiamano

1866
01:11:00,120 --> 01:11:01,800
selettività sale e pepe quindi è

1867
01:11:01,800 --> 01:11:03,480
fondamentalmente casuale non hai alcun tipo

1868
01:11:03,480 --> 01:11:04,679
di sensibilità all'orientamento topografico

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
um quindi ci sono prove che

1871
01:11:09,300 --> 01:11:10,920
sì diversi sistemi lo fanno in

1872
01:11:10,920 --> 01:11:13,020
modo diverso ed è interessante

1873
01:11:13,020 --> 01:11:14,760
capire perché

1874
01:11:14,760 --> 01:11:17,760
sì questo  è molto interessante mi ricorda

1875
01:11:17,760 --> 01:11:21,300
innanzitutto la

1876
01:11:21,300 --> 01:11:22,980
base e il tempo di diffusione della reazione,

1877
01:11:22,980 --> 01:11:25,739
quindi in realtà è

1878
01:11:25,739 --> 01:11:30,000
possibile che una regione non abbia

1879
01:11:30,000 --> 01:11:32,840
attività da una data

1880
01:11:32,840 --> 01:11:35,640
granularità come se fosse osservata

1881
01:11:35,640 --> 01:11:39,360
sulla scala temporale spaziale e temporale fmri

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
se le sacche di attività ma  se le

1884
01:11:44,699 --> 01:11:46,380
sacche di attività sono

1885
01:11:46,380 --> 01:11:48,480
più lente e più veloci

1886
01:11:48,480 --> 01:11:52,260
di quella misurazione non sarà

1887
01:11:52,260 --> 01:11:54,060
diversa dal rumore, sarà stata

1888
01:11:54,060 --> 01:11:55,739
calcolata la media,

1889
01:11:55,739 --> 01:11:58,620
quindi potrebbero esserci alcuni sì

1890
01:11:58,620 --> 01:12:01,560
interessanti come set di dati che in

1891
01:12:01,560 --> 01:12:03,360
realtà hanno molta

1892
01:12:03,360 --> 01:12:06,179
ricchezza ma poi per uno  per un motivo o per

1893
01:12:06,179 --> 01:12:08,520
un altro è stata semplicemente calcolata la media

1894
01:12:08,520 --> 01:12:11,100
perché non era connesso a te

1895
01:12:11,100 --> 01:12:12,420
o qualcosa del genere devi davvero

1896
01:12:12,420 --> 01:12:14,520
seguire un singolo livello di prova devi

1897
01:12:14,520 --> 01:12:16,140
avere una risoluzione spaziale sufficientemente alta

1898
01:12:16,140 --> 01:12:18,719
da sapere che soddisfa le

1899
01:12:18,719 --> 01:12:23,100
microfrequenze uh  e questo

1900
01:12:23,100 --> 01:12:24,659
è qualcosa che le persone non hanno fatto per

1901
01:12:24,659 --> 01:12:25,620
molto tempo, specialmente se stai facendo

1902
01:12:25,620 --> 01:12:27,780
registrazioni di elettori singoli non

1903
01:12:27,780 --> 01:12:28,920
vedrai un'onda viaggiante

1904
01:12:28,920 --> 01:12:30,900
vedrai oscillazioni,

1905
01:12:30,900 --> 01:12:32,219
quindi hai bisogno di un multi-elettrico

1906
01:12:32,219 --> 01:12:34,199
array e in pratica dicono okay

1907
01:12:34,199 --> 01:12:36,000
sì, ora che abbiamo la tecnologia per

1908
01:12:36,000 --> 01:12:37,520
farlo,

1909
01:12:37,520 --> 01:12:40,080
persistono molte cose che non vedevamo prima e

1910
01:12:40,080 --> 01:12:42,960
potenzialmente questa è una spiegazione per

1911
01:12:42,960 --> 01:12:44,400
gran parte del rumore che vedevamo

1912
01:12:44,400 --> 01:12:46,260
prima, forse è davvero solo  onde viaggianti

1913
01:12:46,260 --> 01:12:47,219


1914
01:12:47,219 --> 01:12:47,760
ehm

1915
01:12:47,760 --> 01:12:51,480
quindi sì, penso che ci sia molto da fare

1916
01:12:51,480 --> 01:12:53,880
in futuro con maggiori

1917
01:12:53,880 --> 01:12:56,520
capacità di registrazione, è

1918
01:12:56,520 --> 01:12:58,739
molto bello,

1919
01:12:58,739 --> 01:13:02,940
beh, qualche pensiero finale o domanda o

1920
01:13:02,940 --> 01:13:06,239
dove porterai questo lavoro

1921
01:13:06,239 --> 01:13:08,520
sì no, grazie per avermi,

1922
01:13:08,520 --> 01:13:10,140
ehm,

1923
01:13:10,140 --> 01:13:11,640
spero nell'infrastruttura attiva

1924
01:13:11,640 --> 01:13:14,520
ecco, mi piacerebbe, penso

1925
01:13:14,520 --> 01:13:16,560
che sarebbe molto divertente, quindi sì, non sono

1926
01:13:16,560 --> 01:13:18,980
proprio sicuro che sto guardando Forse la musica,

1927
01:13:18,980 --> 01:13:21,659
uh in questo momento,

1928
01:13:21,659 --> 01:13:22,440
um, sto

1929
01:13:22,440 --> 01:13:26,760
guardando

1930
01:13:26,760 --> 01:13:30,420
altre direzioni folli, non

1931
01:13:30,420 --> 01:13:33,020
voglio sembrare troppo pazzo, uh

1932
01:13:33,020 --> 01:13:36,900
ma  Andrò giù, sì, un sacco di cose, quindi

1933
01:13:36,900 --> 01:13:38,580
una cosa che sta emergendo, qualcosa che

1934
01:13:38,580 --> 01:13:40,320
abbiamo sottoposto ai neuropi, è studiare la

1935
01:13:40,320 --> 01:13:43,140
memoria con onde viaggianti,

1936
01:13:43,140 --> 01:13:45,060
um, quindi quel documento è appena uscito in

1937
01:13:45,060 --> 01:13:46,860
archivio oggi, uh,

1938
01:13:46,860 --> 01:13:48,840
come le onde sono davvero brave a codificare

1939
01:13:48,840 --> 01:13:50,580
ricordi a lungo termine  che penso sia

1940
01:13:50,580 --> 01:13:52,100
molto interessante,

1941
01:13:52,100 --> 01:13:54,120
quindi potrei andare un po' in quella

1942
01:13:54,120 --> 01:13:55,800
direzione,

1943
01:13:55,800 --> 01:13:58,920
suona bene e sì, sarebbe molto

1944
01:13:58,920 --> 01:14:01,400
emozionante vedere l'azione entrare in gioco

1945
01:14:01,400 --> 01:14:04,560
quando c'erano i neuroni che rimanevano

1946
01:14:04,560 --> 01:14:07,620
attivi anche mentre i piedi del cane si

1947
01:14:07,620 --> 01:14:09,060
muovevano,

1948
01:14:09,060 --> 01:14:11,480
ci sono molte cose simili  sequenze d'azione

1949
01:14:11,480 --> 01:14:14,280
come lanciare una palla da baseball e poi

1950
01:14:14,280 --> 01:14:15,900
parte ed è come se ci fosse qualcosa

1951
01:14:15,900 --> 01:14:18,440
in quell'azione che continua a

1952
01:14:18,440 --> 01:14:21,179
influenzare e quindi avere una

1953
01:14:21,179 --> 01:14:23,699
rappresentazione temporale profonda di azioni alternative

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
e quindi l'autocodificatore variazionale è

1956
01:14:29,640 --> 01:14:33,179
già fondamentalmente la cosa giusta,

1957
01:14:33,179 --> 01:14:35,219
quindi

1958
01:14:35,219 --> 01:14:37,739
apprezzalo davvero  va bene grazie

1959
01:14:37,739 --> 01:14:39,480
alla prossima volta

1960
01:14:39,480 --> 01:14:43,339
grazie mille ciao


1
00:00:06,600 --> 00:00:09,420
здравствуйте и добро пожаловать! Сегодня 18 сентября

2
00:00:09,420 --> 00:00:14,040
2023 года, активен гостевой поток 57.1

3
00:00:14,040 --> 00:00:16,740
с Энди Келлером. Мы собираемся

4
00:00:16,740 --> 00:00:19,619
поговорить о естественной нейронной структуре

5
00:00:19,619 --> 00:00:22,020
для искусственного интеллекта.

6
00:00:22,020 --> 00:00:24,300
Будет презентация, за которой последует

7
00:00:24,300 --> 00:00:25,800
обсуждение, поэтому, если вы смотрите прямую трансляцию,

8
00:00:25,800 --> 00:00:27,900
пожалуйста, не стесняйтесь  пишите вопросы в

9
00:00:27,900 --> 00:00:30,660
чате, в противном случае спасибо Энди

10
00:00:30,660 --> 00:00:32,279
за это, очень жду этого

11
00:00:32,279 --> 00:00:35,899
и вас за презентацию,

12
00:00:36,360 --> 00:00:38,940
да, большое спасибо, спасибо, что пригласили

13
00:00:38,940 --> 00:00:41,280
меня, я очень рад возможности

14
00:00:41,280 --> 00:00:42,840
представить этот материал активной

15
00:00:42,840 --> 00:00:45,239
референтной группе, я  Я фанат и очень

16
00:00:45,239 --> 00:00:49,200
заинтересован, так что, надеюсь, да, у нас будет

17
00:00:49,200 --> 00:00:50,399
хорошая дискуссия и посмотрим, что вы,

18
00:00:50,399 --> 00:00:51,960
ребята, думаете об этом,

19
00:00:51,960 --> 00:00:54,840
так что меня зовут Энди, я заканчиваю

20
00:00:54,840 --> 00:00:57,180
докторскую диссертацию под руководством Максвеллинга в

21
00:00:57,180 --> 00:00:59,100
Университете Амстердама,

22
00:00:59,100 --> 00:01:01,500
хм, я  после этого я начинаю постдок в Гарварде,

23
00:01:01,500 --> 00:01:05,099
так что я начну. Я просто говорю

24
00:01:05,099 --> 00:01:07,619
о цели моей работы в целом —

25
00:01:07,619 --> 00:01:09,540
попытаться приблизить современный искусственный

26
00:01:09,540 --> 00:01:12,540
интеллект к более человеческому

27
00:01:12,540 --> 00:01:15,240
обобщению, и поэтому то, что мы под

28
00:01:15,240 --> 00:01:17,159
этим подразумеваем, возможно, является некоторым  своего рода обобщение структуры,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
хм, или, может быть, более знакомое

31
00:01:20,700 --> 00:01:22,080
комитету активных младенцев, например, структурированная

32
00:01:22,080 --> 00:01:24,299
модель мира, которая, по нашему мнению,

33
00:01:24,299 --> 00:01:26,340
есть у людей, и способ, которым мы предлагаем

34
00:01:26,340 --> 00:01:28,799
это сделать, заключается в интеграции естественной нейронной

35
00:01:28,799 --> 00:01:32,400
структуры в искусственный интеллект,

36
00:01:32,400 --> 00:01:34,979
поэтому сначала давайте определим, что мы имеем в виду.  путем

37
00:01:34,979 --> 00:01:36,720
обобщения структуры,

38
00:01:36,720 --> 00:01:38,400
поэтому я думаю, что совершенно бесспорно

39
00:01:38,400 --> 00:01:40,380
сказать, что современное машинное обучение

40
00:01:40,380 --> 00:01:42,900
обобщает помимо своего обучающего набора в

41
00:01:42,900 --> 00:01:45,000
традиционном смысле, поэтому, например,

42
00:01:45,000 --> 00:01:46,799
даже самые ранние

43
00:01:46,799 --> 00:01:49,020
многослойные перцептроны искусственных нейронных сетей можно было

44
00:01:49,020 --> 00:01:51,659
обучать на наборах данных таких изображений

45
00:01:51,659 --> 00:01:55,140
и достигать высоких результатов.  точности, то, когда

46
00:01:55,140 --> 00:01:56,880
им предоставляется тестовый

47
00:01:56,880 --> 00:01:58,320
набор изображений, которые они никогда

48
00:01:58,320 --> 00:02:00,240
раньше не видели, они все равно могут

49
00:02:00,240 --> 00:02:02,460
относительно легко классифицировать их с тем же уровнем

50
00:02:02,460 --> 00:02:04,500
точности, и это то, что мы обычно

51
00:02:04,500 --> 00:02:07,320
называем обобщением, однако даже на довольно

52
00:02:07,320 --> 00:02:08,758
раннем этапе это было  заметил, что эти

53
00:02:08,758 --> 00:02:10,800
системы действительно борются с небольшими

54
00:02:10,800 --> 00:02:12,720
сдвигами или деформациями, применимыми,

55
00:02:12,720 --> 00:02:16,340
например, к изображениям, если

56
00:02:18,920 --> 00:02:23,099
да, то вы думаете, почему это удивительно, и

57
00:02:23,099 --> 00:02:24,720
я утверждаю, что на самом деле этот пример является неудачным из-за нашей врожденной

58
00:02:24,720 --> 00:02:26,640
способности выполнять такого рода

59
00:02:26,640 --> 00:02:28,680
структурное обобщение.

60
00:02:28,680 --> 00:02:31,920
ну,

61
00:02:31,920 --> 00:02:33,239
например, этот сдвиг почти

62
00:02:33,239 --> 00:02:35,340
незаметен для нас, и мы обрабатываем его

63
00:02:35,340 --> 00:02:37,560
автоматически, тогда как в системе это

64
00:02:37,560 --> 00:02:39,959
явно серьезная проблема, поэтому на словах

65
00:02:39,959 --> 00:02:41,940
мы можем сказать, что обобщение структуры -

66
00:02:41,940 --> 00:02:44,819
это обобщение некоторых

67
00:02:44,819 --> 00:02:47,040
преобразований симметрии входных данных или, в данном

68
00:02:47,040 --> 00:02:48,959
случае, симметрии.  Трансформация — это

69
00:02:48,959 --> 00:02:50,580
небольшой сдвиг, который оставляет класс цифр

70
00:02:50,580 --> 00:02:52,019
неизменным,

71
00:02:52,019 --> 00:02:54,599
поэтому очевидный вопрос заключается в том, что

72
00:02:54,599 --> 00:02:56,340
именно мы подразумеваем под этой естественной

73
00:02:56,340 --> 00:02:58,860
структурой и почему мы думаем, что это

74
00:02:58,860 --> 00:03:01,620
поможет нам с этими настройками,

75
00:03:01,620 --> 00:03:03,780
поэтому сначала давайте поговорим о том, что мы подразумеваем

76
00:03:03,780 --> 00:03:05,819
под естественной нейронной структурой.  структура,

77
00:03:05,819 --> 00:03:08,700
гм, один из способов говорить о структуре или

78
00:03:08,700 --> 00:03:11,640
любом типе смещения в системе — это

79
00:03:11,640 --> 00:03:14,040
индуктивное смещение, и поэтому индуктивное смещение

80
00:03:14,040 --> 00:03:16,080
можно в общих чертах определить как априорное

81
00:03:16,080 --> 00:03:17,940
ограничение набора реализуемых

82
00:03:17,940 --> 00:03:19,440
гипотез, когда вы делаете

83
00:03:19,440 --> 00:03:22,440
выбор модели.  можно назвать

84
00:03:22,440 --> 00:03:24,480
это как-то так: прежде чем увидеть какие-либо

85
00:03:24,480 --> 00:03:27,060
данные, это ограничение того, что и как

86
00:03:27,060 --> 00:03:29,580
вы можете изучить, поэтому в очень широком смысле это может

87
00:03:29,580 --> 00:03:32,519
включать в себя что угодно, от класса модели до

88
00:03:32,519 --> 00:03:34,739
процедур оптимизации или даже

89
00:03:34,739 --> 00:03:37,379
гиперпараметров, и в некотором смысле они действительно

90
00:03:37,379 --> 00:03:39,739
определяют, что

91
00:03:39,739 --> 00:03:43,140
можно изучить.  и он определяет

92
00:03:43,140 --> 00:03:44,819
обобщение в том смысле, что

93
00:03:44,819 --> 00:03:47,220
вы на самом деле не можете делать обобщения за пределами

94
00:03:47,220 --> 00:03:48,420
обучающего набора, не имея какого-либо

95
00:03:48,420 --> 00:03:50,220
индуктивного решения. Это

96
00:03:50,220 --> 00:03:52,560
более подробно объяснено в этой статье Дэвидом

97
00:03:52,560 --> 00:03:55,500
Вулфордом, поэтому под естественными

98
00:03:55,500 --> 00:03:58,620
индуктивными предубеждениями мы подразумеваем предубеждения, которые

99
00:03:58,620 --> 00:04:00,000
проистекают из ограничений и

100
00:04:00,000 --> 00:04:02,040
ограничений.  с которыми сталкиваются естественные

101
00:04:02,040 --> 00:04:04,500
системы, э-э, по природе необходимости

102
00:04:04,500 --> 00:04:06,900
жить в реальном мире, например,

103
00:04:06,900 --> 00:04:08,459
мозг имеет множество ограничений эффективности

104
00:04:08,459 --> 00:04:10,439
и физических ограничений по природе

105
00:04:10,439 --> 00:04:13,140
своей конструкции, э-э, и, следуя этой

106
00:04:13,140 --> 00:04:14,580
логике, тогда эти ограничения действительно

107
00:04:14,580 --> 00:04:16,620
играют некоторую роль в нашем обобщении

108
00:04:16,620 --> 00:04:19,798
способности, которые в настоящее время превосходят современный

109
00:04:19,798 --> 00:04:21,478
искусственный интеллект, о чем мы поговорим

110
00:04:21,478 --> 00:04:24,720
дальше, поэтому в этом разговоре я сосредоточусь

111
00:04:24,720 --> 00:04:27,540
конкретно на двух типах структур,

112
00:04:27,540 --> 00:04:29,880
которые изучала моя работа:

113
00:04:29,880 --> 00:04:31,699
топографическая организация и

114
00:04:31,699 --> 00:04:34,320
пространственно-временная динамика, и прежде чем я перейду

115
00:04:34,320 --> 00:04:36,120
к своей работе, я  Я приведу короткий пример того,

116
00:04:36,120 --> 00:04:38,759
почему я считаю, что естественная структура

117
00:04:38,759 --> 00:04:41,400
может быть полезна для достижения структурного

118
00:04:41,400 --> 00:04:42,780
обобщения, о котором я говорил

119
00:04:42,780 --> 00:04:44,280
ранее,

120
00:04:44,280 --> 00:04:47,479
поэтому первый пример взят из

121
00:04:47,479 --> 00:04:49,199
неокогнитивной фасадной

122
00:04:49,199 --> 00:04:51,479
архитектуры Хукусимы 1980-х годов,

123
00:04:51,479 --> 00:04:53,520
которая на самом деле была построена для

124
00:04:53,520 --> 00:04:55,440
непосредственного решения проблем.  проблема

125
00:04:55,440 --> 00:04:57,300
устойчивости к этим небольшим сдвигам и

126
00:04:57,300 --> 00:04:59,759
деформациям, поэтому в документах он

127
00:04:59,759 --> 00:05:02,040
пишет о вдохновении от

128
00:05:02,040 --> 00:05:04,380
измерений иерархии и объединения учеников и

129
00:05:04,380 --> 00:05:06,780
ласки, чтобы добиться устойчивости

130
00:05:06,780 --> 00:05:08,699
к этим искажениям, и поэтому, если вы посмотрите

131
00:05:08,699 --> 00:05:11,160
на рисунок, если он напишет U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1, и они обозначают простые и

133
00:05:14,100 --> 00:05:16,919
сложные клетки, и поэтому это довольно

134
00:05:16,919 --> 00:05:18,840
радикальный подход в то время, но он

135
00:05:18,840 --> 00:05:20,699
действительно способствовал повышению надежности и

136
00:05:20,699 --> 00:05:22,199
сдвигов, которые мы преследуем в этих ранних

137
00:05:22,199 --> 00:05:24,419
искусственных нейронных сетях, и со временем

138
00:05:24,419 --> 00:05:25,860
эти идеи были упрощены и

139
00:05:25,860 --> 00:05:28,919
абстрагированы и  очевидно, привели к появлению

140
00:05:28,919 --> 00:05:30,539
сверточных нейронных сетей, как мы знаем

141
00:05:30,539 --> 00:05:32,759
сегодня, что в конечном итоге привело к успеху

142
00:05:32,759 --> 00:05:35,580
революции глубокого обучения, так что это

143
00:05:35,580 --> 00:05:36,960
действительно пример естественного

144
00:05:36,960 --> 00:05:39,479
индуктивного смещения, которое привело к

145
00:05:39,479 --> 00:05:42,120
обобщению структуры, поэтому для нашего исследования

146
00:05:42,120 --> 00:05:43,919
действительно крайне интересно попытаться

147
00:05:43,919 --> 00:05:45,900
понять, что делает эти  модели работают

148
00:05:45,900 --> 00:05:47,280
так хорошо,

149
00:05:47,280 --> 00:05:49,320
и посмотрите, можно ли

150
00:05:49,320 --> 00:05:51,360
потенциально обобщить этот принцип, чтобы охватить более

151
00:05:51,360 --> 00:05:53,940
абстрактные, более абстрактные преобразования

152
00:05:53,940 --> 00:05:57,000
и симметрии,

153
00:05:57,000 --> 00:06:00,360
так что заставляет свертку достигать этого

154
00:06:00,360 --> 00:06:02,060
обобщения структуры

155
00:06:02,060 --> 00:06:04,440
интуитивно, вы можете видеть, что это делается путем

156
00:06:04,440 --> 00:06:06,720
применения того же фильтра в или или

157
00:06:06,720 --> 00:06:08,699
экстрактора функций в  различных пространственных

158
00:06:08,699 --> 00:06:10,680
местоположениях, поэтому здесь мы видим, что один

159
00:06:10,680 --> 00:06:12,660
сверточный фильтр применяется во

160
00:06:12,660 --> 00:06:14,820
всех местах изображения. Это означает,

161
00:06:14,820 --> 00:06:16,259
что независимо от того, где находятся ваши входные данные,

162
00:06:16,259 --> 00:06:18,000
будь то в середине

163
00:06:18,000 --> 00:06:20,400
изображения или справа, у вас будут одни и те

164
00:06:20,400 --> 00:06:22,080
же функции.  за одним

165
00:06:22,080 --> 00:06:23,580
исключением, они будут эквивалентно

166
00:06:23,580 --> 00:06:24,660
сдвинуты,

167
00:06:24,660 --> 00:06:26,819
поэтому математически этот тип отображения

168
00:06:26,819 --> 00:06:29,160
называется гомоморфизмом, он сохраняет

169
00:06:29,160 --> 00:06:30,840
алгебраическую структуру входного

170
00:06:30,840 --> 00:06:33,180
пространства и выходного пространства, в данном случае

171
00:06:33,180 --> 00:06:35,580
это относительно перевода и на

172
00:06:35,580 --> 00:06:37,440
простом простом уровне что-то вроде

173
00:06:37,440 --> 00:06:38,880
В

174
00:06:38,880 --> 00:06:40,259
оставшейся части этого выступления будет важно помнить, что мы можем

175
00:06:40,259 --> 00:06:42,300
проверить гомоморфизмы нашего

176
00:06:42,300 --> 00:06:45,000
экстрактора признаков, если увидим, что существует

177
00:06:45,000 --> 00:06:46,740
коммутация сообщества с

178
00:06:46,740 --> 00:06:49,560
коммутативной диаграммой преобразований, и

179
00:06:49,560 --> 00:06:51,720
поэтому мы можем записать это также алгебраически,

180
00:06:51,720 --> 00:06:53,759
показав, что экстрактор признаков  f

181
00:06:53,759 --> 00:06:55,080
коммутирует с

182
00:06:55,080 --> 00:06:57,000
оператором преобразования t,

183
00:06:57,000 --> 00:06:58,919
и, по сути, мы хотим, чтобы

184
00:06:58,919 --> 00:07:00,600
не было разницы между первым

185
00:07:00,600 --> 00:07:02,639
извлечением признаков и последующим

186
00:07:02,639 --> 00:07:04,740
выполнением преобразования или

187
00:07:04,740 --> 00:07:06,240
выполнением преобразования и последующим

188
00:07:06,240 --> 00:07:08,639
извлечением признаков, поэтому

189
00:07:08,639 --> 00:07:10,199
на сегодняшний день проблема состоит в том, что

190
00:07:10,199 --> 00:07:11,880
мы на самом деле не знаем  как создавать

191
00:07:11,880 --> 00:07:13,620
гомоморфизмы относительно более

192
00:07:13,620 --> 00:07:15,240
сложных преобразований, которые мы видим в

193
00:07:15,240 --> 00:07:18,060
реальном мире, например, наш мозг

194
00:07:18,060 --> 00:07:20,099
способен обрабатывать изменения в освещении и

195
00:07:20,099 --> 00:07:22,259
времени года, естественно,

196
00:07:22,259 --> 00:07:24,599
хм, поэтому здесь мы видим освещение на

197
00:07:24,599 --> 00:07:26,160
лице человека или смену времен года, мы можем

198
00:07:26,160 --> 00:07:27,840
сказать, что это  одно и то же лицо или та же

199
00:07:27,840 --> 00:07:29,880
дорога, но мы не знаем, как строить

200
00:07:29,880 --> 00:07:31,319
модели, которые учитывают эти

201
00:07:31,319 --> 00:07:33,180
Преобразования, и поэтому нам трудно

202
00:07:33,180 --> 00:07:35,520
создавать системы, которые обрабатывают их

203
00:07:35,520 --> 00:07:37,620
надежным и предсказуемым образом,

204
00:07:37,620 --> 00:07:40,259
чтобы дать еще более абстрактный пример того,

205
00:07:40,259 --> 00:07:41,759
что я  Имею в виду это и потенциальные

206
00:07:41,759 --> 00:07:43,620
негативные последствия моделей, которые

207
00:07:43,620 --> 00:07:45,440
не обрабатывают симметрию. Преобразования

208
00:07:45,440 --> 00:07:48,060
учитывают современные программы создания текста в изображения,

209
00:07:48,060 --> 00:07:50,520
поэтому в этом примере я попросил

210
00:07:50,520 --> 00:07:53,940
Долли сгенерировать изображение плюшевого

211
00:07:53,940 --> 00:07:55,620
мишки на Луне, и она делает это

212
00:07:55,620 --> 00:07:57,180
невероятно хорошо, верно.  вероятно, лучше,

213
00:07:57,180 --> 00:08:00,960
чем я мог бы, у него текстура меха

214
00:08:00,960 --> 00:08:03,360
невероятно детализированная, однако, если я попрошу вас

215
00:08:03,360 --> 00:08:05,340
сделать что-то, что, как мне кажется,

216
00:08:05,340 --> 00:08:08,039
концептуально проще, например, нарисовать синий

217
00:08:08,039 --> 00:08:10,560
куб поверх красного куба, это не удастся,

218
00:08:10,560 --> 00:08:13,380
и мне это кажется неинтуитивным,

219
00:08:13,380 --> 00:08:15,300
поскольку  вторая задача кажется

220
00:08:15,300 --> 00:08:18,180
значительно проще, но я

221
00:08:18,180 --> 00:08:19,860
утверждаю, что причина, по которой это

222
00:08:19,860 --> 00:08:21,599
удивительно, - это точно та же самая причина, по

223
00:08:21,599 --> 00:08:23,580
которой пример перевода амнистии был

224
00:08:23,580 --> 00:08:25,560
неожиданным:

225
00:08:25,560 --> 00:08:28,020
здесь происходит это преобразование симметрии, а именно

226
00:08:28,020 --> 00:08:29,400
преобразование между этими сложными

227
00:08:29,400 --> 00:08:31,740
объектами плюшевого мишки  и луна, и

228
00:08:31,740 --> 00:08:34,320
эти простые объекты в виде кубов, с которыми мы

229
00:08:34,320 --> 00:08:36,360
интуитивно ожидаем, что сеть

230
00:08:36,360 --> 00:08:38,820
сможет обращаться и уважать их, и мы видим,

231
00:08:38,820 --> 00:08:40,919
что это не совсем так, как

232
00:08:40,919 --> 00:08:43,380
показала работа Фукусимы, что эта

233
00:08:43,380 --> 00:08:46,200
естественная структура иерархии и

234
00:08:46,200 --> 00:08:47,700
объединения нашей зрительной системы

235
00:08:47,700 --> 00:08:49,680
эффективен для обобщений

236
00:08:49,680 --> 00:08:52,380
небольших преобразований. Я утверждаю, что

237
00:08:52,380 --> 00:08:54,060
потенциально структура более высокого уровня может

238
00:08:54,060 --> 00:08:55,740
быть необходима для решения этих абстрактных

239
00:08:55,740 --> 00:08:58,200
проблем обобщения,

240
00:08:58,200 --> 00:09:01,380
и поэтому вопрос, который я

241
00:09:01,380 --> 00:09:04,380
изучаю и задаю, заключается в том, какой

242
00:09:04,380 --> 00:09:06,060
может быть эта структура и как мы ее

243
00:09:06,060 --> 00:09:08,640
реализуем.  это в архитектуре искусственной нейронной

244
00:09:08,640 --> 00:09:10,080
сети, которую на самом деле можно

245
00:09:10,080 --> 00:09:14,120
использовать для выполнения вычислений,

246
00:09:14,880 --> 00:09:17,700
поэтому, чтобы начать отвечать, я перейду

247
00:09:17,700 --> 00:09:19,680
к своей первой линии работы по

248
00:09:19,680 --> 00:09:22,380
топографической организации,

249
00:09:22,380 --> 00:09:25,260
чтобы топографическая организация

250
00:09:25,260 --> 00:09:27,060
широко наблюдалась по всему мозгу из

251
00:09:27,060 --> 00:09:29,760
областей первичной зрительной коры на уровне сапфира.  и

252
00:09:29,760 --> 00:09:31,500
это можно очень условно описать как это

253
00:09:31,500 --> 00:09:33,540
свойство, что нейроны, которые расположены близко

254
00:09:33,540 --> 00:09:35,760
друг к другу, склонны реагировать на схожие

255
00:09:35,760 --> 00:09:38,220
вещи, например, слева мы показываем

256
00:09:38,220 --> 00:09:39,720
цветовую кодировку предпочтений каждого

257
00:09:39,720 --> 00:09:42,959
нейрона в первичной цифровой коре как

258
00:09:42,959 --> 00:09:45,360
реакцию на ориентированные линии.  и мы видим

259
00:09:45,360 --> 00:09:46,740
этот плавно меняющийся набор

260
00:09:46,740 --> 00:09:48,779
избирательностей. Другой тип

261
00:09:48,779 --> 00:09:50,580
организации известен как тематическая

262
00:09:50,580 --> 00:09:52,560
организация сетчатки, где близлежащие нейроны

263
00:09:52,560 --> 00:09:54,600
зрительной коры имеют тенденцию реагировать на близлежащие

264
00:09:54,600 --> 00:09:56,399
рецептивные поля,

265
00:09:56,399 --> 00:09:58,560
однако эта организация не ограничивается

266
00:09:58,560 --> 00:10:01,080
этими функциями низкого уровня, она простирается и на

267
00:10:01,080 --> 00:10:02,519
более сложные структуры.  такие особенности, как те, которые

268
00:10:02,519 --> 00:10:05,459
присутствуют на лицах, объектах или местах,

269
00:10:05,459 --> 00:10:07,920
и это относится к так называемым

270
00:10:07,920 --> 00:10:10,080
функционально специфическим областям мозга,

271
00:10:10,080 --> 00:10:12,779
таким как веретенообразная область лица FFA и

272
00:10:12,779 --> 00:10:15,420
парахиппокампальная область лица PPA,

273
00:10:15,420 --> 00:10:19,200
поэтому в этой работе основная идея снова заключается в том,

274
00:10:19,200 --> 00:10:21,300
что, возможно, это  топографическая

275
00:10:21,300 --> 00:10:23,580
организация в некотором смысле

276
00:10:23,580 --> 00:10:25,080
тесно связана с

277
00:10:25,080 --> 00:10:27,980
операцией свертки и архитектурой Фукусимы,

278
00:10:27,980 --> 00:10:30,660
мы, возможно, сможем обобщить преимущества

279
00:10:30,660 --> 00:10:33,420
этого на более абстрактные преобразования,

280
00:10:33,420 --> 00:10:34,920
другими словами, научиться строить более

281
00:10:34,920 --> 00:10:36,839
сложные гомоморфизмы, которые мы не можем,

282
00:10:36,839 --> 00:10:38,940
знаете ли, мы не можем  займитесь аналитическим анализом прямо

283
00:10:38,940 --> 00:10:40,740
сейчас,

284
00:10:40,740 --> 00:10:42,480
просто чтобы показать, что мы не

285
00:10:42,480 --> 00:10:44,760
совсем безумны в этой идее, ну,

286
00:10:44,760 --> 00:10:46,320
в этой области уже есть некоторые предыдущие работы таких

287
00:10:46,320 --> 00:10:49,880
людей, как Конан Гэлакси

288
00:10:49,880 --> 00:10:54,060
Барден, в начале 90-х и 2000-х,

289
00:10:54,060 --> 00:10:55,800
и они изучали, как

290
00:10:55,800 --> 00:10:57,720
может быть топографическая организация  полезно для изучения

291
00:10:57,720 --> 00:11:01,320
отклонений, в основном в линейных моделях, поэтому,

292
00:11:01,320 --> 00:11:03,060
когда мы вошли в эту область, перед нами стоял вопрос:

293
00:11:03,060 --> 00:11:04,680
какой наиболее масштабируемый

294
00:11:04,680 --> 00:11:07,140
абстрактный механизм можно использовать

295
00:11:07,140 --> 00:11:08,880
из этих подходов, который мы можем

296
00:11:08,880 --> 00:11:10,800
интегрировать в современные архитектуры глубоких нейронных

297
00:11:10,800 --> 00:11:12,959
сетей, и в конечном итоге мы

298
00:11:12,959 --> 00:11:15,000
остановились на  Подход генеративного моделирования,

299
00:11:15,000 --> 00:11:16,260
который, я думаю, может быть

300
00:11:16,260 --> 00:11:17,519
интересен людям в этом

301
00:11:17,519 --> 00:11:18,779
сообществе,

302
00:11:18,779 --> 00:11:21,779
который затем позволяет нам

303
00:11:21,779 --> 00:11:23,579
более тесно связать его с анализом топографических независимых

304
00:11:23,579 --> 00:11:26,040
компонентов, основная идея которого

305
00:11:26,040 --> 00:11:28,320
состоит в том, что мы можем изучить топографический

306
00:11:28,320 --> 00:11:30,660
объект пространства, наложив топографическое

307
00:11:30,660 --> 00:11:32,940
априорное распределение на  наши скрытые

308
00:11:32,940 --> 00:11:34,440
переменные,

309
00:11:34,440 --> 00:11:37,079
так что просто чтобы дать краткое представление, я

310
00:11:37,079 --> 00:11:39,120
предполагаю, что большинство людей уже знакомы

311
00:11:39,120 --> 00:11:40,440
с этим,

312
00:11:40,440 --> 00:11:42,540
но общее предположение заключается в том,

313
00:11:42,540 --> 00:11:44,339
что мозг представляет собой генеративную модель, и

314
00:11:44,339 --> 00:11:45,720
эту идею в некотором смысле можно

315
00:11:45,720 --> 00:11:48,060
отнести к Гельмгольтам из 19-го

316
00:11:48,060 --> 00:11:50,459
века, э-э  где он сказал, что то, что мы

317
00:11:50,459 --> 00:11:52,140
видим, является решением вычислительной

318
00:11:52,140 --> 00:11:54,420
проблемы, наш мозг вычисляет наиболее

319
00:11:54,420 --> 00:11:56,519
вероятные причины поглощения фотонов

320
00:11:56,519 --> 00:11:59,519
в наших глазах, и это пример,

321
00:11:59,519 --> 00:12:01,920
если я покажу вам это изображение, вы сразу

322
00:12:01,920 --> 00:12:03,720
узнаете в нем шар с некоторой

323
00:12:03,720 --> 00:12:05,760
кривизной, однако он  с таким же успехом это может

324
00:12:05,760 --> 00:12:07,620
быть диск с искаженной

325
00:12:07,620 --> 00:12:09,600
перспективой, поэтому именно так мы

326
00:12:09,600 --> 00:12:12,660
получаем оптические иллюзии или наши изображения, так

327
00:12:12,660 --> 00:12:14,880
что ваш мозг делает вывод, что

328
00:12:14,880 --> 00:12:17,100
здесь есть куб из-за

329
00:12:17,100 --> 00:12:18,720
структуры, но на самом деле это просто плоский

330
00:12:18,720 --> 00:12:19,860
лист бумаги.

331
00:12:19,860 --> 00:12:22,740
Итак, вы можете думать об этом

332
00:12:22,740 --> 00:12:24,480
аспекте генеративной модели, который похож на

333
00:12:24,480 --> 00:12:26,160
обратную графическую программу,

334
00:12:26,160 --> 00:12:28,140
в программе известны абстрактные свойства

335
00:12:28,140 --> 00:12:30,660
сферы, положение,

336
00:12:30,660 --> 00:12:32,820
размер, освещение, и они используются для

337
00:12:32,820 --> 00:12:34,560
проецирования сферы для создания 2D-

338
00:12:34,560 --> 00:12:37,440
изображения, которое визуализируется.  Таким образом, по сути,

339
00:12:37,440 --> 00:12:40,019
Гумбольдт и другие говорят, что в

340
00:12:40,019 --> 00:12:41,940
качестве генеративной модели мозг

341
00:12:41,940 --> 00:12:43,680
на самом деле пытается инвертировать этот

342
00:12:43,680 --> 00:12:45,959
генеративный процесс, делать выводы

343
00:12:45,959 --> 00:12:48,300
и выводить основные причины наших

344
00:12:48,300 --> 00:12:49,680
ощущений,

345
00:12:49,680 --> 00:12:51,600
поэтому причина, по которой я как бы углубляюсь в

346
00:12:51,600 --> 00:12:53,100
этот вопрос, заключается в том, что существует

347
00:12:53,100 --> 00:12:55,440
сегодня много разговоров о генеративных моделях,

348
00:12:55,440 --> 00:12:57,180
и я не обязательно говорю только

349
00:12:57,180 --> 00:12:59,639
о создании изображений или красивых

350
00:12:59,639 --> 00:13:01,260
картинок,

351
00:13:01,260 --> 00:13:03,120
я действительно хочу иметь в виду

352
00:13:03,120 --> 00:13:07,920
структуру для обучения без учителя,

353
00:13:07,920 --> 00:13:10,019
чтобы затем немного больше вдаваться в

354
00:13:10,019 --> 00:13:11,700
детали, что я имею в виду под

355
00:13:11,700 --> 00:13:14,279
топографический априор, поэтому генеративные модели

356
00:13:14,279 --> 00:13:16,019
обычно описываются как совместное

357
00:13:16,019 --> 00:13:18,720
распределение по наблюдениям X и

358
00:13:18,720 --> 00:13:21,720
скрытым переменным, которые мы будем называть Z,

359
00:13:21,720 --> 00:13:23,940
и это обычно факторизуется, или один из

360
00:13:23,940 --> 00:13:25,620
способов, которым это делается, факторизуется с точки зрения

361
00:13:25,620 --> 00:13:28,440
априорного P от Z, и это верно

362
00:13:28,440 --> 00:13:30,420
генеративная модель условная генеративная

363
00:13:30,420 --> 00:13:33,420
модель P от x с учетом Z, и поэтому

364
00:13:33,420 --> 00:13:34,980
мы можем думать об этом одним способом: можно

365
00:13:34,980 --> 00:13:37,019
увидеть, что априор кодирует относительные

366
00:13:37,019 --> 00:13:38,880
штрафы для каждого типа кода, который

367
00:13:38,880 --> 00:13:41,279
создается, когда мы инвертируем нашу генеративную

368
00:13:41,279 --> 00:13:42,899
модель. Это называется вычислением

369
00:13:42,899 --> 00:13:45,920
апостериорное P от Z при заданном X,

370
00:13:45,920 --> 00:13:49,139
и поэтому для разработки топографического скрытого

371
00:13:49,139 --> 00:13:50,639
пространства мы хотим ввести своего рода

372
00:13:50,639 --> 00:13:53,279
топографическое априорное состояние, которое было или

373
00:13:53,279 --> 00:13:55,620
которое, как показала эта топографическая работа ICA,

374
00:13:55,620 --> 00:13:57,720
эквивалентно чему-то вроде

375
00:13:57,720 --> 00:13:59,700
штрафа за разреженность группы,

376
00:13:59,700 --> 00:14:01,500
чтобы люди могли быть знакомы с типичным

377
00:14:01,500 --> 00:14:03,180
штрафы за разреженность из

378
00:14:03,180 --> 00:14:04,560
анализа независимого обучения. Вы хотите, чтобы ваши

379
00:14:04,560 --> 00:14:06,540
активации были редкими, что означает, что многие из

380
00:14:06,540 --> 00:14:09,420
них нулевые, и это может выглядеть

381
00:14:09,420 --> 00:14:10,680
примерно так: у вас есть куча

382
00:14:10,680 --> 00:14:12,300
синих квадратов, которые активны, но большинство из

383
00:14:12,300 --> 00:14:14,459
них не активны, но особенно

384
00:14:14,459 --> 00:14:16,740
с группой  Университетский штраф, мы хотим, чтобы

385
00:14:16,740 --> 00:14:18,839
эти априоры назначали более низкую вероятность

386
00:14:18,839 --> 00:14:21,600
этим распределенным редким активациям

387
00:14:21,600 --> 00:14:24,720
и более высокую вероятность этим сгруппированным

388
00:14:24,720 --> 00:14:26,940
плотно упакованным представлениям, вы

389
00:14:26,940 --> 00:14:28,860
также можете думать об этом как о более высоком штрафе,

390
00:14:28,860 --> 00:14:30,720
когда вещи разбросаны, более низком

391
00:14:30,720 --> 00:14:33,540
штрафе, когда вещи расположены ближе друг к другу,

392
00:14:33,540 --> 00:14:36,380
так что еще раз  это можно записать

393
00:14:36,380 --> 00:14:39,060
абстрактно вот так, но я хочу выдвинуть

394
00:14:39,060 --> 00:14:41,160
теорию, согласно которой эти нейроны, каждый из

395
00:14:41,160 --> 00:14:42,779
этих квадратов представляет собой своего рода

396
00:14:42,779 --> 00:14:44,220
нейрон в нашей модели, и они

397
00:14:44,220 --> 00:14:46,560
организованы в этой двумерной сетке, поэтому, когда мы

398
00:14:46,560 --> 00:14:48,120
говорим о группировке, мы на самом деле имеем в виду

399
00:14:48,120 --> 00:14:50,820
группировка в этой 2D-топологии,

400
00:14:50,820 --> 00:14:53,100
поэтому одна вещь, которая действительно интересна

401
00:14:53,100 --> 00:14:55,560
и отчасти важна, заключается в том, что эти

402
00:14:55,560 --> 00:14:57,779
априорные положения не только дают нам топографическую

403
00:14:57,779 --> 00:15:00,540
организацию, но также были отмечены

404
00:15:00,540 --> 00:15:02,459
или изучены такими людьми, как

405
00:15:02,459 --> 00:15:05,760
Эрози Марчелли и Бруно, также

406
00:15:05,760 --> 00:15:07,740
действительно подходят  статистика природных

407
00:15:07,740 --> 00:15:08,959
данных лучше,

408
00:15:08,959 --> 00:15:11,180
в частности естественных изображений,

409
00:15:11,180 --> 00:15:14,100
они показали, что, используя этот тип

410
00:15:14,100 --> 00:15:16,139
априора, вы на самом деле получаете более редкий набор

411
00:15:16,139 --> 00:15:18,839
активаций, а это означает, что априор

412
00:15:18,839 --> 00:15:20,459
немного лучше соответствует истинному генеративному процессу,

413
00:15:20,459 --> 00:15:22,620
и, как мы знаем, мозг

414
00:15:22,620 --> 00:15:24,779
высокая степень разреженности, и

415
00:15:24,779 --> 00:15:26,339
считается, что это очень важно для

416
00:15:26,339 --> 00:15:28,620
эффективности,

417
00:15:28,620 --> 00:15:30,839
поэтому чтобы получить немного больше

418
00:15:30,839 --> 00:15:32,760
подробностей о реализации этого типа

419
00:15:32,760 --> 00:15:35,160
разреженной группы, прежде чем мы будем использовать иерархическую

420
00:15:35,160 --> 00:15:37,320
генеративную модель, и это

421
00:15:37,320 --> 00:15:39,060
в основном введено некоторыми из

422
00:15:39,060 --> 00:15:41,339
топографическая работа ICA,

423
00:15:41,339 --> 00:15:43,320
идея состоит в том, что у вас есть

424
00:15:43,320 --> 00:15:45,000
скрытая переменная более высокого уровня U, которая

425
00:15:45,000 --> 00:15:47,820
одновременно регулирует дисперсию

426
00:15:47,820 --> 00:15:50,279
нескольких переменных более низкого уровня T, и

427
00:15:50,279 --> 00:15:52,440
именно так мы получаем разреженность групп, а

428
00:15:52,440 --> 00:15:55,440
затем, чтобы получить топографическую организацию, вы

429
00:15:55,440 --> 00:15:56,760
можете слегка использовать несколько этих скрытых

430
00:15:56,760 --> 00:15:59,339
переменных.  перекрываются с

431
00:15:59,339 --> 00:16:02,519
их полями влияния, поэтому

432
00:16:02,519 --> 00:16:04,260
мы можем называть их их окрестностями,

433
00:16:04,260 --> 00:16:05,699
и это даст вам плавную

434
00:16:05,699 --> 00:16:07,440
корреляционную структуру, к которой вы стремитесь,

435
00:16:07,440 --> 00:16:09,899
так что интуитивно поймите это, вы

436
00:16:09,899 --> 00:16:12,060
видите, что эта переменная T внизу

437
00:16:12,060 --> 00:16:14,279
внизу не получает  любой ввод

438
00:16:14,279 --> 00:16:17,160
от этого U вверху, но он использует общую

439
00:16:17,160 --> 00:16:19,139
переменную u с этим T в середине,

440
00:16:19,139 --> 00:16:21,300
так что они как будто делятся вариантами,

441
00:16:21,300 --> 00:16:22,980
они делятся некоторыми компонентами со

442
00:16:22,980 --> 00:16:24,959
своими соседями, но не всеми компонентами,

443
00:16:24,959 --> 00:16:26,579
и это на самом деле из-за этой локальной

444
00:16:26,579 --> 00:16:28,079
связности  эти переменные более высокого уровня,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
чтобы было проще понять, как мы используем

447
00:16:33,180 --> 00:16:34,980
генеративную модель, давайте вернемся к

448
00:16:34,980 --> 00:16:37,440
одной переменной U, и проблема в

449
00:16:37,440 --> 00:16:38,940
этом типе архитектуры, которая

450
00:16:38,940 --> 00:16:42,360
затрудняла ее на протяжении многих лет, заключается в том, как

451
00:16:42,360 --> 00:16:44,579
вы можете сделать приблизительный апостериорный вывод

452
00:16:44,579 --> 00:16:47,579
над этими промежуточными переменными в

453
00:16:47,579 --> 00:16:50,100
этой иерархической архитектуре, и это

454
00:16:50,100 --> 00:16:52,560
не очень просто, поэтому в предыдущих

455
00:16:52,560 --> 00:16:54,420
работах использовались эвристики, разработанные для

456
00:16:54,420 --> 00:16:56,699
линейных моделей, и в нашей работе мы обнаружили,

457
00:16:56,699 --> 00:16:58,680
что это на самом деле не распространяется на современные

458
00:16:58,680 --> 00:17:01,199
архитектуры нейронных сетей, поэтому на самом деле

459
00:17:01,199 --> 00:17:02,880
наша идея состоит в том, чтобы использовать

460
00:17:02,880 --> 00:17:04,760
факторизация - конкретная

461
00:17:04,760 --> 00:17:07,640
повторная параметризация этого распределения,

462
00:17:07,640 --> 00:17:10,380
и поэтому эта параметризация,

463
00:17:10,380 --> 00:17:12,419
в частности, достигается путем определения

464
00:17:12,419 --> 00:17:14,579
приоритета, известного как

465
00:17:14,579 --> 00:17:16,319
смесь гауссовского масштаба, что означает, что наше

466
00:17:16,319 --> 00:17:19,140
условное распределение T с учетом U

467
00:17:19,140 --> 00:17:21,179
на самом деле является нормальным распределением, где

468
00:17:21,179 --> 00:17:24,299
дисперсия определяется этой переменной

469
00:17:24,299 --> 00:17:27,720
U и для некоторых вариантов выбора U это

470
00:17:27,720 --> 00:17:29,340
распределение действительно разрежено и

471
00:17:29,340 --> 00:17:31,980
охватывает ряд распределений,

472
00:17:31,980 --> 00:17:33,780
таких как лаплоссианы костюма и Т-

473
00:17:33,780 --> 00:17:36,299
распределения. Один из способов его определения - это

474
00:17:36,299 --> 00:17:38,940
смесь гауссовских масштабов, вызывающая

475
00:17:38,940 --> 00:17:40,919
конкретную перепараметризацию повторного кадра

476
00:17:40,919 --> 00:17:42,900
в терминах независимых гауссовских случайных

477
00:17:42,900 --> 00:17:45,720
величин.  Z и U, поэтому

478
00:17:45,720 --> 00:17:48,840
мы видим, что эта переменная T, которая

479
00:17:48,840 --> 00:17:50,760
изначально была довольно сложной, на самом деле является

480
00:17:50,760 --> 00:17:52,799
просто продуктом группы гауссовских

481
00:17:52,799 --> 00:17:54,840
случайных величин, с которыми теперь известно, как

482
00:17:54,840 --> 00:17:57,660
работать гораздо эффективнее, э-э, в

483
00:17:57,660 --> 00:18:00,120
генеративных моделях, особенно в том, что

484
00:18:00,120 --> 00:18:02,039
мы  Мы собираемся сделать так, чтобы мы могли

485
00:18:02,039 --> 00:18:04,020
получить приблизительные апостериорные данные для

486
00:18:04,020 --> 00:18:06,720
U и Z по отдельности, а затем выполнить

487
00:18:06,720 --> 00:18:08,640
их детерминированную комбинацию,

488
00:18:08,640 --> 00:18:10,020
чтобы вычислить нашу топографическую

489
00:18:10,020 --> 00:18:13,140
переменную T, и это гораздо проще сделать,

490
00:18:13,140 --> 00:18:15,500
не вдаваясь в слишком много деталей,

491
00:18:15,500 --> 00:18:17,700
метод, который  мы решили использовать так

492
00:18:17,700 --> 00:18:18,600
называемый вариационный

493
00:18:18,600 --> 00:18:20,640
автоэнкодер, который использует методы

494
00:18:20,640 --> 00:18:23,220
вариационного вывода для получения

495
00:18:23,220 --> 00:18:24,780
нижней границы вероятности, что позволяет

496
00:18:24,780 --> 00:18:26,940
нам параметризовать эти приблизительные

497
00:18:26,940 --> 00:18:29,400
апостериорные данные с помощью мощных нелинейных глубоких

498
00:18:29,400 --> 00:18:30,960
нейронных сетей и оптимизировать их с помощью

499
00:18:30,960 --> 00:18:33,240
градиентного спуска.  быть

500
00:18:33,240 --> 00:18:34,440
знакомы с сообществом активного вывода,

501
00:18:34,440 --> 00:18:36,900
но на самом деле мы сделали

502
00:18:36,900 --> 00:18:38,760
вместо того, чтобы использовать один кодировщик в

503
00:18:38,760 --> 00:18:41,640
декодере, как это обычно бывает с baes, у нас теперь есть

504
00:18:41,640 --> 00:18:43,799
два кодировщика, один для вас, и один для Z

505
00:18:43,799 --> 00:18:46,200
отдельно, а затем мы объединяем их

506
00:18:46,200 --> 00:18:48,419
таким детерминированным способом, чтобы  создайте

507
00:18:48,419 --> 00:18:51,660
нашу топографическую переменную T, если вы видите,

508
00:18:51,660 --> 00:18:53,100
что это на самом деле

509
00:18:53,100 --> 00:18:54,900
построение распределения T Стьюдента

510
00:18:54,900 --> 00:18:57,620
из гауссиан,

511
00:18:57,620 --> 00:19:00,480
а затем мы можем подключить это, мы делаем это

512
00:19:00,480 --> 00:19:03,600
перед декодированием и, а затем максимизируем

513
00:19:03,600 --> 00:19:05,820
вероятность данных в целом, так что

514
00:19:05,820 --> 00:19:07,260
это локоть  нижняя

515
00:19:07,260 --> 00:19:09,360
граница доказательств изобилует правдоподобием

516
00:19:09,360 --> 00:19:12,600
данных и на самом деле очень похожа на

517
00:19:12,600 --> 00:19:15,720
вариационную свободную энергию, которая используется в

518
00:19:15,720 --> 00:19:18,299
сообществе активного входа,

519
00:19:18,299 --> 00:19:20,520
поэтому, если оставить в стороне эти детали,

520
00:19:20,520 --> 00:19:22,500
действительно интересно то, что

521
00:19:22,500 --> 00:19:23,940
происходит, когда мы тренируем эту генеративную

522
00:19:23,940 --> 00:19:26,580
модель, которая  имеет относительно простой

523
00:19:26,580 --> 00:19:29,460
штраф за групповую разреженность в скрытом пространстве, и

524
00:19:29,460 --> 00:19:30,720
мы хотим посмотреть, что он

525
00:19:30,720 --> 00:19:32,700
изучает с точки зрения организации

526
00:19:32,700 --> 00:19:34,980
Futures, и сначала мы начинаем с

527
00:19:34,980 --> 00:19:36,480
простейшего возможного набора данных, у нас есть

528
00:19:36,480 --> 00:19:38,580
черный фон с белыми квадратами в

529
00:19:38,580 --> 00:19:41,280
случайных местоположениях XY  и если мы обучим наш

530
00:19:41,280 --> 00:19:42,780
автоэнкодер этому штрафу за разреженность группы,

531
00:19:42,780 --> 00:19:44,760
а затем посмотрим на

532
00:19:44,760 --> 00:19:47,640
весовые векторы нашего декодера, которые

533
00:19:47,640 --> 00:19:49,020
мы отображаем синим цветом,

534
00:19:49,020 --> 00:19:52,520
организованные в этой двумерной сетке, мы увидим, что

535
00:19:52,520 --> 00:19:54,780
они действительно учатся организовываться

536
00:19:54,780 --> 00:19:57,539
в соответствии с пространственными  местоположение, так что это

537
00:19:57,539 --> 00:19:59,580
можно рассматривать как похожее на сверточные

538
00:19:59,580 --> 00:20:01,799
рецептивные поля, или рецептивное поле

539
00:20:01,799 --> 00:20:04,679
каждого нейрона на самом деле определяется

540
00:20:04,679 --> 00:20:09,059
типом входных данных в его местоположении,

541
00:20:09,059 --> 00:20:10,860
и это имеет смысл интуитивно с

542
00:20:10,860 --> 00:20:13,140
точки зрения разреженности группы, поскольку

543
00:20:13,140 --> 00:20:15,480
для любой заданной области необходимо выделить, как

544
00:20:15,480 --> 00:20:17,700
в  желтый здесь фильтры в данной

545
00:20:17,700 --> 00:20:19,260
группе гораздо более сильно коррелированы,

546
00:20:19,260 --> 00:20:20,880
у них есть эти перекрывающиеся рецептивные

547
00:20:20,880 --> 00:20:23,460
поля, чем в других случайных местах, поэтому,

548
00:20:23,460 --> 00:20:25,020
по сути, мы видим, что наша модель

549
00:20:25,020 --> 00:20:27,840
учится группировать действия активности

550
00:20:27,840 --> 00:20:28,980
вместе, э-э,

551
00:20:28,980 --> 00:20:32,059
в виде смоделированного вертикального листа

552
00:20:32,059 --> 00:20:34,260
в соответствии с корреляциями в

553
00:20:34,260 --> 00:20:36,840
набор данных, поэтому вместо свертки,

554
00:20:36,840 --> 00:20:38,580
когда вы на самом деле выполняете привязку веса

555
00:20:38,580 --> 00:20:40,860
и вручную указываете, что я хочу

556
00:20:40,860 --> 00:20:42,539
скопировать этот вес везде, вы

557
00:20:42,539 --> 00:20:44,220
можете думать об этом как о примерном

558
00:20:44,220 --> 00:20:45,500
времени ожидания,

559
00:20:45,500 --> 00:20:48,120
и на самом деле мы узнаем это из

560
00:20:48,120 --> 00:20:49,620
корреляции  Структура самого набора данных

561
00:20:49,620 --> 00:20:51,660
и просто чтобы дать немного

562
00:20:51,660 --> 00:20:54,120
больше биологического вдохновения для

563
00:20:54,120 --> 00:20:56,280
этого, и мы знаем, что ретинотопия

564
00:20:56,280 --> 00:20:58,020
присутствует в мозгу. Это пример

565
00:20:58,020 --> 00:21:02,460
ретинотопии в зрительной коре, и

566
00:21:02,460 --> 00:21:04,500
вы можете увидеть, если покажете макаке

567
00:21:04,500 --> 00:21:06,780
такое изображение проецируется в

568
00:21:06,780 --> 00:21:08,520
эту

569
00:21:08,520 --> 00:21:11,700
топологию, сохраняя пространство фактически на

570
00:21:11,700 --> 00:21:13,740
поверхности коры,

571
00:21:13,740 --> 00:21:16,080
так что идея в том, что топографическая

572
00:21:16,080 --> 00:21:18,419
организация и даже изучение топографической

573
00:21:18,419 --> 00:21:21,299
организации сохраняют входные

574
00:21:21,299 --> 00:21:26,160
корреляции наших наборов данных, и

575
00:21:26,160 --> 00:21:28,679
потенциально, это может быть полезно

576
00:21:28,679 --> 00:21:30,840
для  обобщая эти идеи немного

577
00:21:30,840 --> 00:21:32,340
дальше, так что, как я сказал в

578
00:21:32,340 --> 00:21:34,679
начале, было бы даже лучше, если бы

579
00:21:34,679 --> 00:21:37,200
мы могли просто изучить что-то большее, чем

580
00:21:37,200 --> 00:21:39,320
просто свертка, может быть, более сложные

581
00:21:39,320 --> 00:21:43,679
эквивалентности, так как нам сделать одну

582
00:21:43,679 --> 00:21:45,720
вещь, которая ясна в естественном

583
00:21:45,720 --> 00:21:48,299
интеллекте, это то, что мы  не существуют в

584
00:21:48,299 --> 00:21:51,120
этом мире фреймов IID, верно, мы существуем

585
00:21:51,120 --> 00:21:53,520
в мире с непрерывными последовательностями

586
00:21:53,520 --> 00:21:55,620
Преобразований, так что, возможно, мы сможем расширить

587
00:21:55,620 --> 00:21:58,440
нашу модель до этого параметра, чтобы научиться

588
00:21:58,440 --> 00:22:01,080
наблюдать за Трансформациями, это идея

589
00:22:01,080 --> 00:22:03,299
временной согласованности,

590
00:22:03,299 --> 00:22:05,280
так что же произойдет, если мы просто просто

591
00:22:05,280 --> 00:22:08,280
расширили нашу предыдущую структуру во

592
00:22:08,280 --> 00:22:10,620
времени. Правильное измерение, поэтому вместо того, чтобы просто

593
00:22:10,620 --> 00:22:13,080
группировать, говоря, что мы хотим, чтобы наши нейроны

594
00:22:13,080 --> 00:22:15,059
были разреженными группами с точки зрения пространственной

595
00:22:15,059 --> 00:22:17,400
протяженности в коре, мы на самом деле хотим, чтобы

596
00:22:17,400 --> 00:22:18,960
они были разреженными группами с течением времени,

597
00:22:18,960 --> 00:22:20,640
что означает, что если один набор нейронов

598
00:22:20,640 --> 00:22:22,559
активен  теперь мы хотим, чтобы тот же самый набор

599
00:22:22,559 --> 00:22:24,360
нейронов был активен и в будущем, и

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
если мы посмотрим, если мы интуитивно

602
00:22:27,840 --> 00:22:30,600
думаем об этом, мы увидим, что на

603
00:22:30,600 --> 00:22:33,059
самом деле это более обнадеживает инвариантность и

604
00:22:33,059 --> 00:22:35,039
эквивариантность.

605
00:22:35,039 --> 00:22:37,140
хочу, чтобы одни и те же нейроны

606
00:22:37,140 --> 00:22:39,179
были активны постоянно, но входное

607
00:22:39,179 --> 00:22:41,280
преобразование меняется прямо в тот момент, когда

608
00:22:41,280 --> 00:22:44,220
ступни этой маленькой лисы движутся, поэтому, если

609
00:22:44,220 --> 00:22:45,960
одни и те же нейроны

610
00:22:45,960 --> 00:22:47,880
снова и снова кодируют одно и то же, но ступни

611
00:22:47,880 --> 00:22:49,320
движутся, эти нейроны будут

612
00:22:49,320 --> 00:22:51,360
учиться  быть инвариантным к движению

613
00:22:51,360 --> 00:22:53,880
этой ноги этой собаки, например,

614
00:22:53,880 --> 00:22:57,539
так что вместо этого, упс,

615
00:22:57,539 --> 00:23:01,200
я пошел неправильным путем, а,

616
00:23:01,200 --> 00:23:04,860
так что вместо этого, наше понимание заключалось в том, что эта

617
00:23:04,860 --> 00:23:06,659
группа начинает быть, вместо этого может быть

618
00:23:06,659 --> 00:23:09,059
сдвинута относительно времени, так что это

619
00:23:09,059 --> 00:23:10,980
будет означать  что последовательно смещаемые

620
00:23:10,980 --> 00:23:13,080
наборы активаций будут стимулироваться

621
00:23:13,080 --> 00:23:15,179
к совместной активации, и тогда наше скрытое

622
00:23:15,179 --> 00:23:16,440
пространство действительно будет структурировано

623
00:23:16,440 --> 00:23:18,000
относительно наблюдаемых Трансформаций,

624
00:23:18,000 --> 00:23:19,980
так что вы можете видеть здесь, что вместо того, чтобы один и

625
00:23:19,980 --> 00:23:21,480
тот же набор нейронов был активен на всех

626
00:23:21,480 --> 00:23:23,340
временных шагах, на самом деле это последовательная активация.

627
00:23:23,340 --> 00:23:24,900
перестановочный набор нейронов, которые мы

628
00:23:24,900 --> 00:23:27,780
группируем таким редким образом,

629
00:23:27,780 --> 00:23:29,940
а затем это позволяет нам моделировать

630
00:23:29,940 --> 00:23:33,419
различные наблюдения с течением времени, но

631
00:23:33,419 --> 00:23:34,860
они все еще связаны с точки зрения

632
00:23:34,860 --> 00:23:36,960
обучения трансформации и сохранения

633
00:23:36,960 --> 00:23:38,340
этой корреляционной структуры

634
00:23:38,340 --> 00:23:40,020
эмпатии,

635
00:23:40,020 --> 00:23:41,940
поэтому, если мы  объедините это в нашу

636
00:23:41,940 --> 00:23:44,400
топографическую архитектуру Bae, вы можете получить

637
00:23:44,400 --> 00:23:46,020
что-то похожее на это. Вы видите,

638
00:23:46,020 --> 00:23:48,120
что у нас есть входная последовательность, мы

639
00:23:48,120 --> 00:23:51,240
снова кодируем переменную z, а затем

640
00:23:51,240 --> 00:23:53,520
несколько переменных U в знаменателе

641
00:23:53,520 --> 00:23:55,740
здесь, а затем каждая из этих

642
00:23:55,740 --> 00:23:58,620
переменных U сдвигается  примерно так же, как мы

643
00:23:58,620 --> 00:24:00,480
показывали раньше, чтобы достичь

644
00:24:00,480 --> 00:24:02,820
этой структуры эквивалентности сдвигов,

645
00:24:02,820 --> 00:24:04,740
которую мы ищем, когда мы объединяем

646
00:24:04,740 --> 00:24:07,080
их в этом распределении продуктов T учащихся,

647
00:24:07,080 --> 00:24:09,240
мы получаем одну скрытую

648
00:24:09,240 --> 00:24:10,740
переменную, теперь это наша топографическая

649
00:24:10,740 --> 00:24:13,860
переменная T, и теперь, когда мы  Имея эту

650
00:24:13,860 --> 00:24:16,140
известную структуру в нашем скрытом пространстве, вы

651
00:24:16,140 --> 00:24:17,460
можете думать о ней как о структурированной

652
00:24:17,460 --> 00:24:19,919
модели Мира, мы знаем, как преобразовать это

653
00:24:19,919 --> 00:24:21,659
скрытое пространство, в данном случае это путем

654
00:24:21,659 --> 00:24:23,580
перестановки этих активаций вокруг этих

655
00:24:23,580 --> 00:24:25,860
кругов, выполняющих циклическую роль,

656
00:24:25,860 --> 00:24:28,380
циклический сдвиг, мы знаем, что это  будет

657
00:24:28,380 --> 00:24:30,120
соответствовать нашим изученным входным

658
00:24:30,120 --> 00:24:32,640
преобразованиям, и мы можем убедиться в этом,

659
00:24:32,640 --> 00:24:34,620
сказав: «Хорошо, что, если я продолжу это

660
00:24:34,620 --> 00:24:36,480
входное преобразование, истинное

661
00:24:36,480 --> 00:24:38,100
преобразование в наборе данных, которое представляет собой

662
00:24:38,100 --> 00:24:40,559
вращение, а затем я сравню это с

663
00:24:40,559 --> 00:24:42,659
тем, как я выполнил свою роль в своем последнем  Пространство

664
00:24:42,659 --> 00:24:44,700
путем перемещения моих активаций в моем

665
00:24:44,700 --> 00:24:47,280
мозгу, а затем мы декодируем и видим, что

666
00:24:47,280 --> 00:24:49,919
получаем одно и то же, и это

667
00:24:49,919 --> 00:24:52,140
демонстрирует то

668
00:24:52,140 --> 00:24:53,580
свойство общности, о котором я говорил раньше,

669
00:24:53,580 --> 00:24:56,820
для проверки гомоморфизма

670
00:24:56,820 --> 00:24:58,799
и, таким образом, для измерения этого немного более

671
00:24:58,799 --> 00:25:02,460
качественно.  количественно, ну, мы можем измерить

672
00:25:02,460 --> 00:25:04,440
то, что называется потерей эквивариантности, так что

673
00:25:04,440 --> 00:25:07,080
на самом деле это количественная оценка

674
00:25:07,080 --> 00:25:09,360
этой разницы между активацией нашей свернутой

675
00:25:09,360 --> 00:25:12,120
капсулы или ее вращением в нашей

676
00:25:12,120 --> 00:25:15,059
голове и наблюдением за тем, как разворачивается перекатывание,

677
00:25:15,059 --> 00:25:16,559
и вперед, они наблюдают, как

678
00:25:16,559 --> 00:25:19,440
трансформация разворачивается перед нами, поэтому мы

679
00:25:19,440 --> 00:25:21,600
видим топографическую  Bae достигает

680
00:25:21,600 --> 00:25:24,000
значительно меньшей

681
00:25:24,000 --> 00:25:26,700
ошибки эквивалентности, эта пузырьковая ваэ - это то, о чем я

682
00:25:26,700 --> 00:25:27,960
говорил раньше, где она изучает

683
00:25:27,960 --> 00:25:29,820
инвариантность, поэтому у нее нет

684
00:25:29,820 --> 00:25:32,340
операции сдвига, а традиционная ваэ

685
00:25:32,340 --> 00:25:35,640
не имеет понятия об организации или

686
00:25:35,640 --> 00:25:37,380
временной составляющей, поэтому производительность

687
00:25:37,380 --> 00:25:40,320
вдобавок очень низкая.  к этому мы видим, что

688
00:25:40,320 --> 00:25:41,700
модель является лучшей генеративной моделью

689
00:25:41,700 --> 00:25:45,059
последовательностей, она просто получает более низкую, ну,

690
00:25:45,059 --> 00:25:48,179
меньшую, например, отрицательную логарифмическую вероятность, э-э, для

691
00:25:48,179 --> 00:25:50,100
набора данных, поэтому она лучше способна

692
00:25:50,100 --> 00:25:51,720
моделировать этот набор данных, потому что у нее есть

693
00:25:51,720 --> 00:25:52,919
представление о структуре

694
00:25:52,919 --> 00:25:55,460
преобразования,

695
00:25:55,980 --> 00:25:58,140
мы можем протестировать это на нескольких

696
00:25:58,140 --> 00:25:59,760
различных типах преобразований, и в

697
00:25:59,760 --> 00:26:00,840
верхнем ряду мы показываем истинное

698
00:26:00,840 --> 00:26:02,880
преобразование, мы вытащили эти

699
00:26:02,880 --> 00:26:05,039
серые изображения, а затем в нижнем

700
00:26:05,039 --> 00:26:07,080
ряду мы кодируем, а затем мы просто как бы

701
00:26:07,080 --> 00:26:08,700
прокручиваем наши активации и сохраняем

702
00:26:08,700 --> 00:26:12,140
декодирование, чтобы увидеть, что модель

703
00:26:12,140 --> 00:26:15,000
усвоила как текущее

704
00:26:15,000 --> 00:26:17,039
наблюдаемое преобразование, и

705
00:26:17,039 --> 00:26:19,340
мы видим, что она может в основном идеально

706
00:26:19,340 --> 00:26:21,360
реконструировать эти элементы

707
00:26:21,360 --> 00:26:23,640
последовательности, которые она никогда раньше не видела,

708
00:26:23,640 --> 00:26:25,260
дополнительно с изображениями из

709
00:26:25,260 --> 00:26:26,580
тестового набора, которые она никогда не видела  раньше

710
00:26:26,580 --> 00:26:28,380
просто потому, что он знает, что представляет собой

711
00:26:28,380 --> 00:26:29,760
преобразование, которое он в данный момент

712
00:26:29,760 --> 00:26:31,500
кодирует, он может обобщить это на новые

713
00:26:31,500 --> 00:26:33,919
примеры,

714
00:26:34,020 --> 00:26:36,360
поэтому вывод из этой части на самом деле является

715
00:26:36,360 --> 00:26:38,039
топографической организацией, мы показали, что

716
00:26:38,039 --> 00:26:40,080
сохраненная структура ввода, и теперь

717
00:26:40,080 --> 00:26:41,940
мы показываем, что она потенциально может повысить

718
00:26:41,940 --> 00:26:44,279
эффективность и обобщение  как мы

719
00:26:44,279 --> 00:26:46,200
надеялись,

720
00:26:46,200 --> 00:26:48,600
наконец, что-то, что нас удивило,

721
00:26:48,600 --> 00:26:49,980
и я подумал, что это потенциально самое

722
00:26:49,980 --> 00:26:52,500
интересное, это то, что эти

723
00:26:52,500 --> 00:26:53,700
Преобразования, которые изучаются нашей

724
00:26:53,700 --> 00:26:54,960
моделью, на самом деле обобщают

725
00:26:54,960 --> 00:26:57,059
комбинации Преобразований, которые

726
00:26:57,059 --> 00:26:59,580
мы не видим во время обучения,

727
00:26:59,580 --> 00:27:02,100
например, несмотря на только обучение  о цвете

728
00:27:02,100 --> 00:27:04,200
и вращении Преобразования и

729
00:27:04,200 --> 00:27:06,419
изоляция, если модель представлена ​​с

730
00:27:06,419 --> 00:27:08,340
комбинированным преобразованием вращения цвета

731
00:27:08,340 --> 00:27:11,100
во время тестирования. Мы видим, что она способна

732
00:27:11,100 --> 00:27:13,140
полностью моделировать и идеально выполнять эти

733
00:27:13,140 --> 00:27:14,700
преобразования с помощью

734
00:27:14,700 --> 00:27:17,159
роли капсулы, подразумевая, что она научилась

735
00:27:17,159 --> 00:27:19,620
факторизовать представление этих

736
00:27:19,620 --> 00:27:20,880
различных  Преобразования, и он может

737
00:27:20,880 --> 00:27:24,600
гибко комбинировать их во время вывода,

738
00:27:24,600 --> 00:27:28,140
так что, опять же, возможно, мы также не просто получаем

739
00:27:28,140 --> 00:27:29,820
официальную эффективность при обобщении,

740
00:27:29,820 --> 00:27:34,100
мы также получаем некоторую базовую композиционность,

741
00:27:34,260 --> 00:27:36,059
поэтому давайте поговорим об ограничениях и о том,

742
00:27:36,059 --> 00:27:38,460
что мы можем делать дальше. Главное

743
00:27:38,460 --> 00:27:40,620
ограничение в том, что есть  предопределенное

744
00:27:40,620 --> 00:27:44,159
преобразование, которое мы навязываем

745
00:27:44,159 --> 00:27:46,500
как в пространстве, так и во времени, поэтому, хотя мы освободились

746
00:27:46,500 --> 00:27:49,080
от групповых преобразований и, в

747
00:27:49,080 --> 00:27:52,440
частности, таких как перемещение или

748
00:27:52,440 --> 00:27:53,940
вращение, как это сейчас делается в

749
00:27:53,940 --> 00:27:55,559
мире машинного обучения, у

750
00:27:55,559 --> 00:27:59,240
нас все еще есть эта жестко запрограммированная

751
00:27:59,240 --> 00:28:01,980
скрытая роль в нашем  стремится ко всему, что

752
00:28:01,980 --> 00:28:03,900
мы видим, и сделать это немного

753
00:28:03,900 --> 00:28:05,700
более гибким, поэтому, надеюсь, мы сможем смоделировать

754
00:28:05,700 --> 00:28:08,880
большее разнообразие Трансформаций,

755
00:28:08,880 --> 00:28:10,980
хм, мы думаем, что, возможно, мы сможем черпать

756
00:28:10,980 --> 00:28:13,860
вдохновение из более структурированной пространственно-

757
00:28:13,860 --> 00:28:15,600
временной динамики, которая наблюдается в

758
00:28:15,600 --> 00:28:18,120
мозгу, и это требует от нас  ко

759
00:28:18,120 --> 00:28:20,400
второй части этого выступления, которая посвящена

760
00:28:20,400 --> 00:28:22,140
пространственно-временной динамике, которую мы

761
00:28:22,140 --> 00:28:23,039
собираемся попытаться интегрировать в

762
00:28:23,039 --> 00:28:25,200
искусственные нейронные сети, одним из примеров

763
00:28:25,200 --> 00:28:27,059
этого являются бегущие волны, как я показал

764
00:28:27,059 --> 00:28:28,020
здесь,

765
00:28:28,020 --> 00:28:30,600
так что мы под этим подразумеваем, ну,

766
00:28:30,600 --> 00:28:32,279
вот очень  недавняя статья, в которой они

767
00:28:32,279 --> 00:28:36,059
использовали девять Тесла-ФМРТ, работающий с

768
00:28:36,059 --> 00:28:38,700
разрешением 36 миллисекунд, для изображения одного

769
00:28:38,700 --> 00:28:40,980
среза мозга крысы под анестезией,

770
00:28:40,980 --> 00:28:43,320
и мы видим эту очень четко

771
00:28:43,320 --> 00:28:45,720
структурированную пространственно-временную активность и

772
00:28:45,720 --> 00:28:48,299
корреляции, и авторы

773
00:28:48,299 --> 00:28:50,520
статьи продолжают анализировать это.  активность с точки

774
00:28:50,520 --> 00:28:52,919
зрения основных модусов, как показано

775
00:28:52,919 --> 00:28:55,799
справа, поэтому наша гипотеза состоит в том, что,

776
00:28:55,799 --> 00:28:57,179
возможно, некоторая корреляционная

777
00:28:57,179 --> 00:28:59,039
структура, подобная этой, может быть полезной

778
00:28:59,039 --> 00:29:01,260
для структурирования представлений

779
00:29:01,260 --> 00:29:03,240
нашей модели относительно наблюдаемых

780
00:29:03,240 --> 00:29:05,100
Преобразований, но гораздо более

781
00:29:05,100 --> 00:29:07,440
гибким способом, чем просто  просто циклический

782
00:29:07,440 --> 00:29:10,700
сдвиг, как мы делали раньше, хм,

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
и позвольте мне сказать, что это наблюдается не только

785
00:29:15,900 --> 00:29:19,320
у крыс SSI, хм,

786
00:29:19,320 --> 00:29:20,940
вы можете увидеть, как эти бегущие волны

787
00:29:20,940 --> 00:29:24,179
происходят в коре головного мозга

788
00:29:24,179 --> 00:29:27,600
бодрствующих приматов, хм, например,

789
00:29:27,600 --> 00:29:29,580
слева  здесь они показывают бегущие волны,

790
00:29:29,580 --> 00:29:31,740
которые на самом деле

791
00:29:31,740 --> 00:29:35,520
меняются. Насколько вероятно, что примат увидит

792
00:29:35,520 --> 00:29:38,279
низкоконтрастные стимулы в зависимости от фазы

793
00:29:38,279 --> 00:29:40,980
волны, кроме того, они показывают, что

794
00:29:40,980 --> 00:29:43,500
подобный высококонтрастный стимул

795
00:29:43,500 --> 00:29:45,779
справа может вызвать активность бегущей волны,

796
00:29:45,779 --> 00:29:47,520
которая распространяется наружу  даже

797
00:29:47,520 --> 00:29:50,039
в первичной зрительной коре, так что они

798
00:29:50,039 --> 00:29:52,140
действительно повсеместно распространены по всему мозгу

799
00:29:52,140 --> 00:29:54,000
на нескольких уровнях, и было бы

800
00:29:54,000 --> 00:29:55,440
интересно изучить, каковы их

801
00:29:55,440 --> 00:29:58,140
последствия для

802
00:29:58,140 --> 00:29:59,700
обучения представлению структур в нашем случае или,

803
00:29:59,700 --> 00:30:01,799
или вообще,

804
00:30:01,799 --> 00:30:04,140
есть предыдущая работа, в которой изучались

805
00:30:04,140 --> 00:30:06,720
эти типы э-э-динамики.  и они

806
00:30:06,720 --> 00:30:08,700
строят модели, так что сверху это

807
00:30:08,700 --> 00:30:10,380
уравнения, которые описывают импульсную

808
00:30:10,380 --> 00:30:12,600
нейронную сеть, которую они показывают, если вы

809
00:30:12,600 --> 00:30:15,720
реализуете временные задержки, на самом деле аксональные

810
00:30:15,720 --> 00:30:18,240
временные задержки между нейронами, вы действительно получаете

811
00:30:18,240 --> 00:30:20,820
эту структурную динамику бегущих

812
00:30:20,820 --> 00:30:22,440
волн, пока размер вашей сети

813
00:30:22,440 --> 00:30:24,059
достаточно велик.

814
00:30:24,059 --> 00:30:26,520
хм, однако, как многие люди, вероятно, знают, что

815
00:30:26,520 --> 00:30:28,620
относительно сложно обучать

816
00:30:28,620 --> 00:30:31,320
импульсные нейронные сети того же размера

817
00:30:31,320 --> 00:30:34,820
и производительности, что и глубокие нейронные сети,

818
00:30:34,820 --> 00:30:37,679
аналогично внизу другая система,

819
00:30:37,679 --> 00:30:39,539
которая значительно проще, но,

820
00:30:39,539 --> 00:30:42,840
возможно, слишком проста, э-э, это сеть

821
00:30:42,840 --> 00:30:45,120
связанных осцилляторов, которые, как известно,

822
00:30:45,120 --> 00:30:48,779
демонстрируют синхронность, пространственно-временную

823
00:30:48,779 --> 00:30:52,200
динамику и сложные закономерности, но

824
00:30:52,200 --> 00:30:53,520
это называется системой уменьшения фазы

825
00:30:53,520 --> 00:30:55,500
и не совсем отражает

826
00:30:55,500 --> 00:30:57,059
всю сложность, которая нас интересует,

827
00:30:57,059 --> 00:30:58,140
поэтому мы смотрим на что-то

828
00:30:58,140 --> 00:31:00,779
потенциально среднее между этими двумя

829
00:31:00,779 --> 00:31:03,600
и тем, что мы  решено, что эта работа в

830
00:31:03,600 --> 00:31:06,600
этой работе заключается в параметризации

831
00:31:06,600 --> 00:31:08,520
сети из пары осцилляторов

832
00:31:08,520 --> 00:31:10,620
немного более гибко, чем

833
00:31:10,620 --> 00:31:12,360
модель парамото, так что на самом деле она

834
00:31:12,360 --> 00:31:14,580
построена на этой паре дистилляционных

835
00:31:14,580 --> 00:31:16,380
рекуррентных нейронных сетей Константина

836
00:31:16,380 --> 00:31:18,720
Раша и Ниши,

837
00:31:18,720 --> 00:31:20,760
где они в основном взяли

838
00:31:20,760 --> 00:31:22,200
уравнение, которое  описывает простой

839
00:31:22,200 --> 00:31:23,820
гармонический осциллятор, это

840
00:31:23,820 --> 00:31:26,159
дифференциальное уравнение второго порядка, ускорение

841
00:31:26,159 --> 00:31:29,940
шарика на пружине пропорционально

842
00:31:29,940 --> 00:31:32,480
его смещению,

843
00:31:32,480 --> 00:31:35,220
вы можете добавить дополнительные условия, такие как

844
00:31:35,220 --> 00:31:37,260
демпфирование, чтобы колебания медленно

845
00:31:37,260 --> 00:31:39,360
затухали со временем,

846
00:31:39,360 --> 00:31:41,580
вы можете управлять этим осциллятором с помощью

847
00:31:41,580 --> 00:31:43,380
внешнего источника.  ввод, чтобы как бы противодействовать

848
00:31:43,380 --> 00:31:45,179
этому затуханию или немного

849
00:31:45,179 --> 00:31:47,279
усложнить динамику,

850
00:31:47,279 --> 00:31:49,260
а затем, более того, если у вас много

851
00:31:49,260 --> 00:31:50,940
таких осцилляторов, вы можете соединить их

852
00:31:50,940 --> 00:31:53,000
вместе с этими матрицами связи.

853
00:31:53,000 --> 00:31:55,320
Ну, как мы демонстрируем здесь что-то вроде этого

854
00:31:55,320 --> 00:31:56,640
изображения, так что вы действительно можете  думайте об

855
00:31:56,640 --> 00:31:58,140
этой сети как о куче пузырьков на

856
00:31:58,140 --> 00:31:59,940
пружинах, и они, возможно,

857
00:31:59,940 --> 00:32:01,740
также связаны друг с другом с помощью пружин или эластичных

858
00:32:01,740 --> 00:32:03,600
лент, что бы то ни было, парочка дистилляционных рекуррентных

859
00:32:03,600 --> 00:32:05,279
нейронных сетей русского

860
00:32:05,279 --> 00:32:08,100
Мишры, э-э, с этими различными терминами, и

861
00:32:08,100 --> 00:32:09,899
было показано, что это очень мощно

862
00:32:09,899 --> 00:32:12,480
для моделирования длинных последовательностей они также

863
00:32:12,480 --> 00:32:13,740
упомянули, что их вдохновил

864
00:32:13,740 --> 00:32:15,360
мозг, строящий это, и

865
00:32:15,360 --> 00:32:17,700
в этой статье есть много хорошего анализа, например,

866
00:32:17,700 --> 00:32:19,020
они показывают, что это действительно

867
00:32:19,020 --> 00:32:21,440
полезные свойства в отношении

868
00:32:21,440 --> 00:32:23,460
проблем с исчезающим градиентом, которые

869
00:32:23,460 --> 00:32:25,440
обычно возникают в рекуррентных нейронных

870
00:32:25,440 --> 00:32:26,820
сетях.

871
00:32:26,820 --> 00:32:29,159
но если мы хотим взглянуть на пространственно-

872
00:32:29,159 --> 00:32:30,960
временную динамику и этот тип

873
00:32:30,960 --> 00:32:32,820
модели, это немного сложно,

874
00:32:32,820 --> 00:32:34,919
потому что эти матрицы связи здесь,

875
00:32:34,919 --> 00:32:36,320
W,

876
00:32:36,320 --> 00:32:39,600
которые соединяют каждый нейрон или каждый

877
00:32:39,600 --> 00:32:41,240
осциллятор, расположены друг с другом,

878
00:32:41,240 --> 00:32:43,620
это плотно связанные матрицы,

879
00:32:43,620 --> 00:32:45,120
как я пытался  изобразите здесь слева,

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
поэтому, если вы попытаетесь визуализировать динамику

882
00:32:48,299 --> 00:32:50,580
этой сети, вы не увидите никакой

883
00:32:50,580 --> 00:32:51,899
пространственной организации, нет никакого

884
00:32:51,899 --> 00:32:55,380
наследования, это извинение скрытому

885
00:32:55,380 --> 00:32:57,000
пространству этой модели,

886
00:32:57,000 --> 00:32:58,799
так что вы можете думать об этом как в нашем

887
00:32:58,799 --> 00:33:01,020
предыдущем примере о нейроне  связаны

888
00:33:01,020 --> 00:33:03,240
с потенциально произвольным набором других

889
00:33:03,240 --> 00:33:04,919
нейронов, эти нейроны связаны с

890
00:33:04,919 --> 00:33:06,600
другим произвольным набором нейронов, и

891
00:33:06,600 --> 00:33:08,520
вы, конечно, получите просто колебательную динамику,

892
00:33:08,520 --> 00:33:10,860
но это своего рода колебания, которые

893
00:33:10,860 --> 00:33:13,260
не имеют большого структурированного смысла, поэтому

894
00:33:13,260 --> 00:33:15,360
в нашей работе мы подумали, что  хорошо, как

895
00:33:15,360 --> 00:33:18,299
мы можем преобразовать это больше в те типы

896
00:33:18,299 --> 00:33:19,860
динамики, которые нас интересуют в этом

897
00:33:19,860 --> 00:33:22,140
структурированном распространении активности,

898
00:33:22,140 --> 00:33:23,940
и один ясный способ сделать это - иметь

899
00:33:23,940 --> 00:33:26,539
более структурированную матрицу связности,

900
00:33:26,539 --> 00:33:29,880
которая, как мы обнаружили, легко реализуется

901
00:33:29,880 --> 00:33:31,559
и эффективно  реализовано с помощью

902
00:33:31,559 --> 00:33:33,000
операции свертки, которую вы можете

903
00:33:33,000 --> 00:33:34,620
рассматривать как локальный локально

904
00:33:34,620 --> 00:33:36,299
связанный слой, поэтому вместо того, чтобы

905
00:33:36,299 --> 00:33:37,860
каждый нейрон был связан с каждым нейроном,

906
00:33:37,860 --> 00:33:39,480
нейроны просто соединяются со своими

907
00:33:39,480 --> 00:33:41,580
ближайшими соседями, после обучения

908
00:33:41,580 --> 00:33:42,840
вы в конечном итоге получите что-то,

909
00:33:42,840 --> 00:33:44,880
похожее на гладкое пространственное изображение.  Временная

910
00:33:44,880 --> 00:33:46,620
динамика,

911
00:33:46,620 --> 00:33:48,419
поэтому, чтобы было немного более понятно для

912
00:33:48,419 --> 00:33:50,519
обучения этой модели, мы берем это отдельное

913
00:33:50,519 --> 00:33:52,200
дифференциальное уравнение второго порядка, которое

914
00:33:52,200 --> 00:33:54,299
мы описывали, прежде чем вы дискретизируете

915
00:33:54,299 --> 00:33:56,340
его на два уравнения первого порядка, вы

916
00:33:56,340 --> 00:33:57,960
можете думать об этом как о численном

917
00:33:57,960 --> 00:34:01,200
интегрировании оды, которую мы теперь имеем

918
00:34:01,200 --> 00:34:03,120
скорость, а затем мы обновляем,

919
00:34:03,120 --> 00:34:06,000
ну, и мы можем обучить эту модель как

920
00:34:06,000 --> 00:34:07,620
что-то вроде автокодировщика или

921
00:34:07,620 --> 00:34:09,839
авторегрессионной модели, поэтому, если мы принимаем

922
00:34:09,839 --> 00:34:11,460
входные данные, мы кодируем их в наше скрытое пространство,

923
00:34:11,460 --> 00:34:14,339
на самом деле входными данными являются Dr, это термин f из x,

924
00:34:14,339 --> 00:34:16,500
который действует  в качестве движущего термина,

925
00:34:16,500 --> 00:34:18,599
это похоже на управление этими осцилляторами

926
00:34:18,599 --> 00:34:20,879
снизу, а затем у них есть своя

927
00:34:20,879 --> 00:34:23,099
собственная динамика, которая определяется

928
00:34:23,099 --> 00:34:25,800
условиями связи, этими локальными связями, а

929
00:34:25,800 --> 00:34:27,540
затем на каждом временном шаге мы принимаем это

930
00:34:27,540 --> 00:34:29,460
скрытое состояние, это волновое состояние, и мы

931
00:34:29,460 --> 00:34:31,560
декодируем, чтобы попытаться  и реконструировать входные данные

932
00:34:31,560 --> 00:34:33,540
и находиться на текущем временном шаге или на

933
00:34:33,540 --> 00:34:35,699
будущем временном шаге,

934
00:34:35,699 --> 00:34:37,980
мы можем провести некоторый анализ этих

935
00:34:37,980 --> 00:34:42,300
моделей во время обучения, чтобы увидеть, что

936
00:34:42,300 --> 00:34:43,619
происходит до обучения, а после

937
00:34:43,619 --> 00:34:45,780
обучения мы можем вычислить фазу и

938
00:34:45,780 --> 00:34:47,399
скорость динамики в

939
00:34:47,399 --> 00:34:49,379
скрытое пространство, по сути, мы видим в

940
00:34:49,379 --> 00:34:51,480
начале торговли, что в

941
00:34:51,480 --> 00:34:53,699
нашей модели нет волн, но после обучения после 50

942
00:34:53,699 --> 00:34:55,500
эпох мы видим, что есть плавная

943
00:34:55,500 --> 00:34:57,119
структурированная активность, распространяющаяся

944
00:34:57,119 --> 00:35:00,420
вниз, э-э, служащая для этой

945
00:35:00,420 --> 00:35:01,800
задачи моделирования последовательности, которую мы выполняем, как

946
00:35:01,800 --> 00:35:04,380
вращающиеся объекты,

947
00:35:04,380 --> 00:35:05,940
гм, так что  Какова в этом польза?

948
00:35:05,940 --> 00:35:07,680
Я имею в виду, что вся причина, по которой я мотивировал это,

949
00:35:07,680 --> 00:35:10,380
заключалась в том, чтобы сказать, что мы хотим иметь более

950
00:35:10,380 --> 00:35:11,880
гибко обучаемую структуру, мы

951
00:35:11,880 --> 00:35:13,020
на самом деле это делаем или у нас просто

952
00:35:13,020 --> 00:35:15,060
появляются красивые волны,

953
00:35:15,060 --> 00:35:16,859
хм, и в нашей статье мы показали,

954
00:35:16,859 --> 00:35:19,320
что мы действительно  изучение какой-то

955
00:35:19,320 --> 00:35:20,940
полезной структуры, и способ, который мы показали,

956
00:35:20,940 --> 00:35:22,440
снова заключается в чем-то вроде этой

957
00:35:22,440 --> 00:35:24,960
коммутативной диаграммы, если вы берете входные данные,

958
00:35:24,960 --> 00:35:27,000
кодируете их и получаете волновое

959
00:35:27,000 --> 00:35:29,280
состояние, а затем искусственно распространяете волны

960
00:35:29,280 --> 00:35:31,619
в этом волновом состоянии, а затем

961
00:35:31,619 --> 00:35:33,480
декодируете, вы можете  Обратите внимание, что на

962
00:35:33,480 --> 00:35:35,220
самом деле это точно так же, как если бы вы

963
00:35:35,220 --> 00:35:37,140
просто показали кучу разных

964
00:35:37,140 --> 00:35:39,180
изображений разных Преобразований, так что

965
00:35:39,180 --> 00:35:41,640
много разных, ну, цифр, разных

966
00:35:41,640 --> 00:35:43,920
функций, и мы видим, что

967
00:35:43,920 --> 00:35:46,140
в каждом случае мы получаем разные типы волновой активности,

968
00:35:46,140 --> 00:35:47,880
чтобы смоделировать это.  другое

969
00:35:47,880 --> 00:35:49,140
преобразование,

970
00:35:49,140 --> 00:35:51,119
если мы обучаем его на разных наборах данных,

971
00:35:51,119 --> 00:35:53,400
мы также видим более сложную

972
00:35:53,400 --> 00:35:55,200
динамику в этом случае, возможно, даже не

973
00:35:55,200 --> 00:35:57,839
бегущие волны или стоячие волны, которые

974
00:35:57,839 --> 00:36:00,359
можно рассматривать как бегущие волны в

975
00:36:00,359 --> 00:36:02,339
противоположных направлениях, поэтому мы посмотрим, моделируем ли мы

976
00:36:02,339 --> 00:36:04,079
эти орбитальные  Динамика: мы получаем

977
00:36:04,079 --> 00:36:06,000
такие плавно движущиеся сгустки

978
00:36:06,000 --> 00:36:07,619
активности в нашем скрытом пространстве. Если мы

979
00:36:07,619 --> 00:36:09,839
моделируем маятник, мы аналогичным

980
00:36:09,839 --> 00:36:13,820
образом получаем сложную колебательную активность,

981
00:36:14,099 --> 00:36:17,339
поэтому структура входных данных сохраняется, но

982
00:36:17,339 --> 00:36:19,560
дополнительно появляется больше гибкости, чем

983
00:36:19,560 --> 00:36:21,599
было раньше, что является своего рода нашей конечной

984
00:36:21,599 --> 00:36:23,400
целью.

985
00:36:23,400 --> 00:36:26,099
Итак, наконец, я хочу немного поговорить о том,

986
00:36:26,099 --> 00:36:28,320
как, по моему мнению, результаты этого исследования

987
00:36:28,320 --> 00:36:30,420
могут не только улучшить искусственный

988
00:36:30,420 --> 00:36:32,099
интеллект, но и как оно поможет нам

989
00:36:32,099 --> 00:36:34,440
понять, почему наши измерения

990
00:36:34,440 --> 00:36:36,240
мозга выглядят именно так, чтобы дать

991
00:36:36,240 --> 00:36:38,579
краткий пример того, что я думаю.  Я имею в виду это,

992
00:36:38,579 --> 00:36:41,040
я уже немного говорил о визах

993
00:36:41,040 --> 00:36:43,740
и местах, поэтому в этой фантастической работе

994
00:36:43,740 --> 00:36:46,859
с Чинг Хигао мы изучали, может ли наш

995
00:36:46,859 --> 00:36:48,900
простой топографический априор, о котором мы говорили,

996
00:36:48,900 --> 00:36:50,579
воспроизвести те же самые

997
00:36:50,579 --> 00:36:53,339
эффекты, поэтому специально мы указали значение

998
00:36:53,339 --> 00:36:56,099
этого Коэна D  метрика избирательности для

999
00:36:56,099 --> 00:36:58,200
каждого из наших нейронов по отношению к

1000
00:36:58,200 --> 00:37:00,000
другому набору данных изображений, потенциально

1001
00:37:00,000 --> 00:37:02,460
содержащих только лица или только объекты или

1002
00:37:02,460 --> 00:37:03,359
тела,

1003
00:37:03,359 --> 00:37:05,880
и поэтому мы измеряем для каждого нейрона, будет ли он

1004
00:37:05,880 --> 00:37:07,920
с большей вероятностью реагировать на лица, или

1005
00:37:07,920 --> 00:37:10,380
в мозгу появляется русский язык, но я делаю

1006
00:37:10,380 --> 00:37:12,839
думаю, это говорит нам о том, что относительная

1007
00:37:12,839 --> 00:37:15,300
организация избирательности может, по крайней мере,

1008
00:37:15,300 --> 00:37:17,400
частично быть связана со

1009
00:37:17,400 --> 00:37:19,800
статистикой корреляции в данных, которые необходимо перенаправить

1010
00:37:19,800 --> 00:37:21,359
после прохождения через сильно

1011
00:37:21,359 --> 00:37:23,640
нелинейный экстрактор будущего, такой как

1012
00:37:23,640 --> 00:37:25,440
глубокая нейронная сеть,

1013
00:37:25,440 --> 00:37:27,480
поэтому в том же духе что-то, что

1014
00:37:27,480 --> 00:37:29,040
Интересно, есть известный так

1015
00:37:29,040 --> 00:37:30,900
называемый трехсторонний или нет визуальный

1016
00:37:30,900 --> 00:37:36,720
поток, так что изображения э-э или объектов - это

1017
00:37:36,720 --> 00:37:38,400
избирательность по отношению к объектам,

1018
00:37:38,400 --> 00:37:40,680
организованная более абстрактными свойствами,

1019
00:37:40,680 --> 00:37:43,200
такими как анимация, является ли эта вещь живой или

1020
00:37:43,200 --> 00:37:46,200
неодушевленной, а также

1021
00:37:46,200 --> 00:37:48,480
размером объекта реального мира, например, что  это размер

1022
00:37:48,480 --> 00:37:50,700
чайника по сравнению с автомобилем,

1023
00:37:50,700 --> 00:37:53,520
и мы видим, что у

1024
00:37:53,520 --> 00:37:56,160
людей эта избирательность организована

1025
00:37:56,160 --> 00:37:57,540
в эту трехчастную структуру.

1026
00:37:57,540 --> 00:37:59,760
Обычно у вас есть небольшие объекты, которые находятся

1027
00:37:59,760 --> 00:38:01,920
между одушевленными и неодушевленными объектами

1028
00:38:01,920 --> 00:38:04,200
с точки зрения их избирательности, и мы видим

1029
00:38:04,200 --> 00:38:06,060
здесь происходит то же самое, так что

1030
00:38:06,060 --> 00:38:07,440
они измеряют избирательность одного и

1031
00:38:07,440 --> 00:38:08,880
того же набора нейронов, но в отношении

1032
00:38:08,880 --> 00:38:10,859
этих различий стимулов вы видите,

1033
00:38:10,859 --> 00:38:12,440
что небольшой кластер находится между

1034
00:38:12,440 --> 00:38:14,820
одушевленным и неодушевленным кластером, и снова

1035
00:38:14,820 --> 00:38:16,079
это происходит для нескольких разных

1036
00:38:16,079 --> 00:38:18,900
инициализаций, так что это  Я

1037
00:38:18,900 --> 00:38:20,880
надеюсь, что мы сможем изучить

1038
00:38:20,880 --> 00:38:22,980
это сообщество немного дальше. Я думаю, это интересно,

1039
00:38:22,980 --> 00:38:24,119
потому что это

1040
00:38:24,119 --> 00:38:26,220
действительно способ показать, что мы

1041
00:38:26,220 --> 00:38:28,200
создали структурированную модель мира, и

1042
00:38:28,200 --> 00:38:30,119
потенциально эта модель мира

1043
00:38:30,119 --> 00:38:31,980
полезна для

1044
00:38:31,980 --> 00:38:34,740
лучшего представления данных реального мира в

1045
00:38:34,740 --> 00:38:37,619
структурированном виде и  в этом смысле вы получаете меньшую свободную

1046
00:38:37,619 --> 00:38:39,119
энергию,

1047
00:38:39,119 --> 00:38:40,619
так что,

1048
00:38:40,619 --> 00:38:42,300
я думаю, развивая эти модели,

1049
00:38:42,300 --> 00:38:44,400
как мы показали здесь, мы можем получить

1050
00:38:44,400 --> 00:38:46,500
представление о новых механизмах

1051
00:38:46,500 --> 00:38:48,900
возникновения этой структуры, включая

1052
00:38:48,900 --> 00:38:50,460
топографическую организацию, о которой мы никогда

1053
00:38:50,460 --> 00:38:52,920
раньше не думали, поэтому модель машины, на которую я

1054
00:38:52,920 --> 00:38:55,520
смотрел  избирательность ориентации

1055
00:38:55,520 --> 00:38:58,260
нейронов, я не

1056
00:38:58,260 --> 00:39:01,020
особо ожидал, что что-то

1057
00:39:01,020 --> 00:39:03,420
произойдет, но вы смотрите, как

1058
00:39:03,420 --> 00:39:05,339
эти волны распространяются по этой

1059
00:39:05,339 --> 00:39:08,099
моделируемой вертикальной поверхности, и я подумал,

1060
00:39:08,099 --> 00:39:09,960
хорошо, может быть, я показываю повернутые изображения,

1061
00:39:09,960 --> 00:39:11,820
может быть, это имеет какой-то эффект

1062
00:39:11,820 --> 00:39:13,740
избирательность ориентации,

1063
00:39:13,740 --> 00:39:15,599
и на самом деле, если вы войдете и

1064
00:39:15,599 --> 00:39:17,460
измерите избирательность каждого нейрона

1065
00:39:17,460 --> 00:39:18,660
по отношению к этим по-разному

1066
00:39:18,660 --> 00:39:22,079
ориентированным линиям, вы увидите, что это

1067
00:39:22,079 --> 00:39:24,300
удивительно напоминает

1068
00:39:24,300 --> 00:39:25,859
столбцы типа Orient, которые наблюдаются в первичной

1069
00:39:25,859 --> 00:39:27,599
зрительной коре, это вещи, уходящие корнями в прошлое.

1070
00:39:27,599 --> 00:39:29,520
Хьюго и ласке, и это что-то,

1071
00:39:29,520 --> 00:39:30,900
что как бы вышло из этой модели

1072
00:39:30,900 --> 00:39:33,060
и того факта, что она имеет пространственно-

1073
00:39:33,060 --> 00:39:34,440
временную структуру по отношению к

1074
00:39:34,440 --> 00:39:37,619
Трансформациям, так что, конечно, это

1075
00:39:37,619 --> 00:39:39,599
действительно грубая аналогия, но я думаю, что это

1076
00:39:39,599 --> 00:39:40,740
пример того, как  построение таких

1077
00:39:40,740 --> 00:39:42,839
моделей может помочь нам задуматься о том,

1078
00:39:42,839 --> 00:39:45,240
как мозг строит репрезентативную

1079
00:39:45,240 --> 00:39:46,980
структуру и о том, как белый цвет

1080
00:39:46,980 --> 00:39:48,660
организован таким образом, о котором, возможно, мы

1081
00:39:48,660 --> 00:39:51,300
раньше не задумывались,

1082
00:39:51,300 --> 00:39:53,460
хм, я думаю, я не единственный, кто

1083
00:39:53,460 --> 00:39:55,859
занимается этим типом  и поэтому я

1084
00:39:55,859 --> 00:39:57,240
хочу немного поговорить о некоторых

1085
00:39:57,240 --> 00:39:59,579
других людях, которые делают это,

1086
00:39:59,579 --> 00:40:00,780
я говорил об этой

1087
00:40:00,780 --> 00:40:02,760
эквивалентной структуре, такие

1088
00:40:02,760 --> 00:40:04,920
люди, как Джеймс Уиттингтон,

1089
00:40:04,920 --> 00:40:08,880
Тим Бэронс и суррогенгули,

1090
00:40:08,880 --> 00:40:10,680
недавно показали, что, введя

1091
00:40:10,680 --> 00:40:14,940
алгебраические  ограничения в

1092
00:40:14,940 --> 00:40:17,040
процесс обучения, в этом случае это было

1093
00:40:17,040 --> 00:40:20,820
похоже на движение агентов в

1094
00:40:20,820 --> 00:40:23,220
окружающей среде, говоря, что вам нужно

1095
00:40:23,220 --> 00:40:24,780
сохранить вид этой алгебраической

1096
00:40:24,780 --> 00:40:27,540
структуры, если я двигаюсь по кругу на запад,

1097
00:40:27,540 --> 00:40:29,280
северо-восток, юг, я в конечном итоге снова оказываюсь в

1098
00:40:29,280 --> 00:40:31,440
том же месте.  Опять же, вводя эти

1099
00:40:31,440 --> 00:40:32,820
типы ограничений,

1100
00:40:32,820 --> 00:40:35,040
вы получаете появление ячеек сетки, подобных

1101
00:40:35,040 --> 00:40:36,900
представлениям,

1102
00:40:36,900 --> 00:40:39,359
так что мне было бы интересно посмотреть, как эта

1103
00:40:39,359 --> 00:40:41,880
идея репрезентативной структуры может

1104
00:40:41,880 --> 00:40:43,980
помочь нам объяснить, возможно, больше, чем наши

1105
00:40:43,980 --> 00:40:45,480
научные открытия, которые мы также обнаруживаем,

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
хм и  и как это связано с

1108
00:40:48,359 --> 00:40:51,540
генеративными моделями в

1109
00:40:51,540 --> 00:40:52,800
целом, и, наконец, я думаю, что есть

1110
00:40:52,800 --> 00:40:54,599
что сказать и о когнитивных

1111
00:40:54,599 --> 00:40:56,099
возможностях этих моделей,

1112
00:40:56,099 --> 00:40:57,420
возможно, мы собираемся тестировать их не только

1113
00:40:57,420 --> 00:40:59,579
с точки зрения нейробиологии,

1114
00:40:59,579 --> 00:41:01,020
но и с

1115
00:41:01,020 --> 00:41:03,839
точки зрения микрогнитивной науки.  например,

1116
00:41:03,839 --> 00:41:06,000
слева есть прогрессивные матрицы Воронов,

1117
00:41:06,000 --> 00:41:08,640
где вам нужно сказать, какое из

1118
00:41:08,640 --> 00:41:11,099
этих изображений с большей вероятностью впишется в

1119
00:41:11,099 --> 00:41:12,599
этот шаблон,

1120
00:41:12,599 --> 00:41:14,760
хм или, например, Насколько вероятно, что

1121
00:41:14,760 --> 00:41:16,740
эта Башня Дженга упадет, когда вы

1122
00:41:16,740 --> 00:41:19,740
остановитесь на тяге  определенный блок или или

1123
00:41:19,740 --> 00:41:22,740
с заданной структурой, и я думаю, что

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
эти типы тестов на самом деле проверяют,

1126
00:41:26,640 --> 00:41:28,619


1127
00:41:28,619 --> 00:41:31,560
похожи ли наши модели мира, которые мы строим, на типы моделей, которые

1128
00:41:31,560 --> 00:41:33,660
у нас врожденно есть собственный здравый смысл,

1129
00:41:33,660 --> 00:41:36,060
как у людей или  как существа, живущие в

1130
00:41:36,060 --> 00:41:38,400
естественном мире, и я проделал некоторую

1131
00:41:38,400 --> 00:41:40,440
предварительную работу в этом направлении, я

1132
00:41:40,440 --> 00:41:43,079
думаю, очень, ну, предварительную и не настолько

1133
00:41:43,079 --> 00:41:45,480
сложную, но, хм, своего рода попытку

1134
00:41:45,480 --> 00:41:47,820
смоделировать визуальные иллюзии, поэтому, если вы возьмете

1135
00:41:47,820 --> 00:41:50,520
действительно простой набор данных о движущейся полосе

1136
00:41:50,520 --> 00:41:52,980
стимулы или статическую полосу или кадр, и вы

1137
00:41:52,980 --> 00:41:54,960
немного перемещаете его, вы можете видеть, что

1138
00:41:54,960 --> 00:41:57,060
модель фактически делает вывод об этом

1139
00:41:57,060 --> 00:41:58,800
недостающем кадре, а затем фактически также

1140
00:41:58,800 --> 00:42:01,079
делает вывод о продолжении движения, так что это все равно, что

1141
00:42:01,079 --> 00:42:03,300
промахнуться по траектории того, что

1142
00:42:03,300 --> 00:42:05,820
фактические стимулы обеспечивают ей, прежде чем

1143
00:42:05,820 --> 00:42:08,760
снова исправить  поэтому я думаю, что моделирование

1144
00:42:08,760 --> 00:42:10,320
иллюзий, безусловно, является интересным

1145
00:42:10,320 --> 00:42:12,660
способом изучения, похожи ли наши модели мира

1146
00:42:12,660 --> 00:42:14,760
на те типы моделей, которые

1147
00:42:14,760 --> 00:42:16,619
есть у нас самих,

1148
00:42:16,619 --> 00:42:19,619
так что в заключение, хм, да, я думаю, что

1149
00:42:19,619 --> 00:42:21,900
топографические априоры мы могли бы показать, что

1150
00:42:21,900 --> 00:42:23,220
они эффективно изучили структурированные

1151
00:42:23,220 --> 00:42:24,839
представления или структурированные

1152
00:42:24,839 --> 00:42:26,700
модели мира.  изученная структура является

1153
00:42:26,700 --> 00:42:29,160
гибкой и адаптируемой к произвольным

1154
00:42:29,160 --> 00:42:30,780
преобразованиям. Преобразования, в отличие от традиционных

1155
00:42:30,780 --> 00:42:33,720
эквивариантов и топографических поставщиков,

1156
00:42:33,720 --> 00:42:35,579
могут быть вызваны статистически, как мы это делали

1157
00:42:35,579 --> 00:42:37,619
в топографической модели, или с помощью

1158
00:42:37,619 --> 00:42:39,480
динамики, как мы показывали в этих

1159
00:42:39,480 --> 00:42:42,000
моделях типа нейронных волновых машин,

1160
00:42:42,000 --> 00:42:44,460
поэтому в заключение я оставлю вас с этим

1161
00:42:44,460 --> 00:42:46,980
Цитата, которую я нашел в статье Фукусимы

1162
00:42:46,980 --> 00:42:50,280
от 1980 года, которая, как мне казалось, значительно опередила

1163
00:42:50,280 --> 00:42:52,079
свое время, где он говорит, что если бы мы могли

1164
00:42:52,079 --> 00:42:53,520
создать модель нейронной сети, которая имела бы

1165
00:42:53,520 --> 00:42:55,020
такие же способности к

1166
00:42:55,020 --> 00:42:57,060
распознаванию образов, как и человек, это

1167
00:42:57,060 --> 00:42:58,800
дало бы нам мощный ключ к пониманию по сравнению с

1168
00:42:58,800 --> 00:43:00,000
понимание нейронного механизма в

1169
00:43:00,000 --> 00:43:03,240
мозге, так что я думаю, что это некоторые

1170
00:43:03,240 --> 00:43:06,119
из целей, к которым мы здесь стремимся,

1171
00:43:06,119 --> 00:43:08,220
поэтому я думаю, что это мой советник Макс, мои

1172
00:43:08,220 --> 00:43:11,280
соавторы Патрик У.А., Эмиль Цзингиан и

1173
00:43:11,280 --> 00:43:17,359
Йорн и заинтересованы в обсуждении, спасибо, хорошо,

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
хорошо  спасибо, отлично, очень, очень

1176
00:43:27,480 --> 00:43:31,079
интересная презентация,

1177
00:43:31,079 --> 00:43:33,480
много с чего начать, может быть, просто,

1178
00:43:33,480 --> 00:43:36,000
что привело вас к этой работе,

1179
00:43:36,000 --> 00:43:38,520
небольшой контекст того, как вы пришли к

1180
00:43:38,520 --> 00:43:43,819
этой работе над своим докторским направлением,

1181
00:43:43,920 --> 00:43:45,119
да,

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
я имею в виду, что мы изучали не мою

1184
00:43:49,200 --> 00:43:51,000
группу, которая  Я учусь в университете, некоторое время

1185
00:43:51,000 --> 00:43:52,700
изучал структурированные представления

1186
00:43:52,700 --> 00:43:56,640
с математической точки зрения,

1187
00:43:56,640 --> 00:43:58,319
где у некоторых людей есть

1188
00:43:58,319 --> 00:44:00,240
модели, например, для вариационного

1189
00:44:00,240 --> 00:44:01,740
автоматического кодировщика,

1190
00:44:01,740 --> 00:44:04,680
хм, и

1191
00:44:04,680 --> 00:44:06,960
я думаю, что-то, что всегда было,

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
это модель  это прекрасно учитывает вращение 2D-

1194
00:44:11,220 --> 00:44:13,560
вращение, но если мы хотим

1195
00:44:13,560 --> 00:44:15,960
выполнить 3D-вращение, мы не можем этого сделать,

1196
00:44:15,960 --> 00:44:17,819
потому что это не группа с точки зрения

1197
00:44:17,819 --> 00:44:19,740
проекции на 2D-план, вы

1198
00:44:19,740 --> 00:44:21,180
теряете информацию, когда эта штука

1199
00:44:21,180 --> 00:44:23,460
вращается, например,

1200
00:44:23,460 --> 00:44:24,240
гм

1201
00:44:24,240 --> 00:44:26,280
или  просто любые естественные

1202
00:44:26,280 --> 00:44:27,960
трансформации, на которые я пытался

1203
00:44:27,960 --> 00:44:29,339
указать в начале. Я думаю, он

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
пытался подумать о том, как мозг

1206
00:44:31,740 --> 00:44:34,020
моделирует естественные трансформации.

1207
00:44:34,020 --> 00:44:35,400


1208
00:44:35,400 --> 00:44:37,200


1209
00:44:37,200 --> 00:44:41,099


1210
00:44:41,099 --> 00:44:44,579


1211
00:44:44,579 --> 00:44:48,420
модели, которые включают не только

1212
00:44:48,420 --> 00:44:50,520
внешние закономерности, но и

1213
00:44:50,520 --> 00:44:52,380
последствия действий, или

1214
00:44:52,380 --> 00:44:55,800
структурную структуру модели мира с действием,

1215
00:44:55,800 --> 00:44:58,619
верно, да, нет, это хороший вопрос, и

1216
00:44:58,619 --> 00:45:01,319
я думаю, что действовавшие выводы, э-э, по

1217
00:45:01,319 --> 00:45:03,839
сути, являются ответом, я думаю, что

1218
00:45:03,839 --> 00:45:05,940
это хороший ответ на этот вопрос,

1219
00:45:05,940 --> 00:45:09,000
я знаю там  - это

1220
00:45:09,000 --> 00:45:11,099
структуры обучения с подкреплением, которые

1221
00:45:11,099 --> 00:45:12,660
используют

1222
00:45:12,660 --> 00:45:15,060
своего рода модели мира, обученные извне,

1223
00:45:15,060 --> 00:45:17,280
поэтому вы тренируете ваэ или что-то в этом роде, а затем

1224
00:45:17,280 --> 00:45:19,800
используете это представление в своей

1225
00:45:19,800 --> 00:45:23,040
системе обучения с подкреплением, но я

1226
00:45:23,040 --> 00:45:24,720
думаю, что у вас есть полностью своего

1227
00:45:24,720 --> 00:45:26,520
рода система, которая представляет собой единую

1228
00:45:26,520 --> 00:45:30,780
цель с э-э-э-действием  как часть

1229
00:45:30,780 --> 00:45:33,660
вероятности данных, и

1230
00:45:33,660 --> 00:45:35,280
да, я думаю, что это гораздо более элегантно,

1231
00:45:35,280 --> 00:45:38,940
и поэтому я большой сторонник этого,

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
я не зашел так далеко, чтобы изучить, как

1234
00:45:43,140 --> 00:45:45,480
эти структурированные модели мира в ваэ,

1235
00:45:45,480 --> 00:45:47,520
или у меня нет  Я вообще над этим не работал, но

1236
00:45:47,520 --> 00:45:48,780
думаю, что, безусловно, было бы очень

1237
00:45:48,780 --> 00:45:50,819
интересно посмотреть, будет ли полезно иметь более

1238
00:45:50,819 --> 00:45:52,339
структурированную модель мира,

1239
00:45:52,339 --> 00:45:54,839
э-э, в вариационном автоматическом кодировщике,

1240
00:45:54,839 --> 00:45:56,880


1241
00:45:56,880 --> 00:45:58,319
и в активной настройке, я думаю, это было бы

1242
00:45:58,319 --> 00:46:00,119
здорово, я имею в виду, я  подумайте,

1243
00:46:00,119 --> 00:46:03,599
некоторые из этих примеров, например, показанные

1244
00:46:03,599 --> 00:46:05,579
ранее, такие как появление ячеек сетки и

1245
00:46:05,579 --> 00:46:07,500
подобные вещи, возможно, указывают в

1246
00:46:07,500 --> 00:46:08,880
этом направлении,

1247
00:46:08,880 --> 00:46:10,560
окей, может быть, мозг что-то делает,

1248
00:46:10,560 --> 00:46:12,540
у него действительно много

1249
00:46:12,540 --> 00:46:13,680
структур,

1250
00:46:13,680 --> 00:46:15,359
хм, это явно должно быть полезно для

1251
00:46:15,359 --> 00:46:19,140
выполнения действий в некоторых  кстати, о

1252
00:46:19,140 --> 00:46:21,720
да, я почувствовал действительно

1253
00:46:21,720 --> 00:46:24,480
хорошую параллель, которую вы привели в

1254
00:46:24,480 --> 00:46:28,040
своем разговоре: локально связанные единицы

1255
00:46:28,040 --> 00:46:30,960
позволили вашим моделям структурно

1256
00:46:30,960 --> 00:46:33,780
воплощать сверточные

1257
00:46:33,780 --> 00:46:35,640
ограничения и шаблоны, и это привело к

1258
00:46:35,640 --> 00:46:37,500
возникновению этих шаблонов, а затем,

1259
00:46:37,500 --> 00:46:41,339
аналогично, вообще был э-э-э, Дорал,

1260
00:46:41,339 --> 00:46:45,680
где  у них было

1261
00:46:45,680 --> 00:46:48,359
правильное ограничение исследования пути, и тогда

1262
00:46:48,359 --> 00:46:50,280
интересно, ну,

1263
00:46:50,280 --> 00:46:53,760
вы знаете, подумать об этих действиях или

1264
00:46:53,760 --> 00:46:56,819
политических эвристиках или разреженности, таких как

1265
00:46:56,819 --> 00:46:59,579
совместное двигательное исследование, в конечном итоге

1266
00:46:59,579 --> 00:47:02,339
становится понятно, что есть как бы два

1267
00:47:02,339 --> 00:47:04,980
взаимно противоположных способа перемещения сустава,

1268
00:47:04,980 --> 00:47:07,079
а затем композиционность  все

1269
00:47:07,079 --> 00:47:09,119
суставы могут быть изучены на этих

1270
00:47:09,119 --> 00:47:10,680
более высоких уровнях, как только они зафиксированы на

1271
00:47:10,680 --> 00:47:14,480
более низких уровнях, так что это очень привлекательный

1272
00:47:14,480 --> 00:47:17,599
и, ух,

1273
00:47:17,599 --> 00:47:20,460
нишевый способ обобщения,

1274
00:47:20,460 --> 00:47:23,819
потому что он оба основан на реальных

1275
00:47:23,819 --> 00:47:25,740
ограничениях мира, но затем,

1276
00:47:25,740 --> 00:47:27,720
особенно через действие,

1277
00:47:27,720 --> 00:47:29,460
потенциально встраивающее что-то, что

1278
00:47:29,460 --> 00:47:31,380
довольно просто, да,

1279
00:47:31,380 --> 00:47:33,599
нет, я думаю, что это определенно

1280
00:47:33,599 --> 00:47:36,599
правда, это действительно хороший момент, если, э-э, если

1281
00:47:36,599 --> 00:47:38,339
у вас действительно есть ограничения, возникающие из

1282
00:47:38,339 --> 00:47:40,500
-за самих ваших действий, то это

1283
00:47:40,500 --> 00:47:42,839
было бы чрезвычайно полезно для помощи

1284
00:47:42,839 --> 00:47:44,819
в структурировании

1285
00:47:44,819 --> 00:47:47,460
вашего скрытого пространства, и я думаю, да, я

1286
00:47:47,460 --> 00:47:48,480
думаю, одна вещь  Я хотел упомянуть, что

1287
00:47:48,480 --> 00:47:49,980
есть

1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
кое-что, что заставило меня задуматься, например,

1290
00:47:52,740 --> 00:47:55,500
работа Стефано Фузи о своего рода

1291
00:47:55,500 --> 00:47:58,859
репрезентативной геометрии, э-э,

1292
00:47:58,859 --> 00:48:01,920
определяет, как мы,

1293
00:48:01,920 --> 00:48:04,440
насколько обобщаемым является данное понимание

1294
00:48:04,440 --> 00:48:08,099
системы, э-э, и я думаю, если вы можете

1295
00:48:08,099 --> 00:48:11,880
понять, как эти, э-э, наборы

1296
00:48:11,880 --> 00:48:14,520
действий  сепарабельный или

1297
00:48:14,520 --> 00:48:16,079
высокопараллельный сепарабельный с помощью линейного

1298
00:48:16,079 --> 00:48:18,839
классификатора, по сути, тогда вы

1299
00:48:18,839 --> 00:48:20,700
сможете делать обобщение, и я

1300
00:48:20,700 --> 00:48:23,099
думаю, что, налагая эти типы предубеждений

1301
00:48:23,099 --> 00:48:25,040
или, возможно, через

1302
00:48:25,040 --> 00:48:27,000
ограничения, налагаемые действием,

1303
00:48:27,000 --> 00:48:28,740
что-то вроде этого,

1304
00:48:28,740 --> 00:48:32,040
вы даете или своего рода вызываете

1305
00:48:32,040 --> 00:48:33,660
лучшая репрезентативная геометрия, и

1306
00:48:33,660 --> 00:48:35,220
это имеет множество преимуществ, таких как

1307
00:48:35,220 --> 00:48:36,660
композиционность,

1308
00:48:36,660 --> 00:48:39,359
да, наше обобщение, так что

1309
00:48:39,359 --> 00:48:41,760
это здорово. Круто, да, очень

1310
00:48:41,760 --> 00:48:43,440
интересная область, хорошо, я прочитаю

1311
00:48:43,440 --> 00:48:45,960
несколько вопросов из живого чата,

1312
00:48:45,960 --> 00:48:48,420
люблю развиваться, написал

1313
00:48:48,420 --> 00:48:52,260
любые практические или наблюдаемые ограничения в

1314
00:48:52,260 --> 00:48:55,579
моделировании иллюзий,

1315
00:48:58,800 --> 00:49:00,420
обучение  сообщества, у них нет

1316
00:49:00,420 --> 00:49:03,060
фобий, у вас нет центра

1317
00:49:03,060 --> 00:49:05,940
взгляда, тогда у вас также нет

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
времени, я имею в виду большинство сверточных

1320
00:49:11,460 --> 00:49:13,260
нейронных сетей. Я использую такие

1321
00:49:13,260 --> 00:49:15,599
рекуррентные нейронные сети, но время

1322
00:49:15,599 --> 00:49:18,420
не так четко определено  в этих моделях,

1323
00:49:18,420 --> 00:49:20,220
как и в условиях непрерывного времени

1324
00:49:20,220 --> 00:49:23,400
для человека, проходящего испытание иллюзией,

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
хм,

1327
00:49:25,319 --> 00:49:27,480
и я думаю, что комбинация этих двух

1328
00:49:27,480 --> 00:49:30,359
фактов заключается в том, что как человек или большинство

1329
00:49:30,359 --> 00:49:33,300
вещей, хм,

1330
00:49:33,300 --> 00:49:35,940
ваш взгляд, ваше перемещение местоположения и

1331
00:49:35,940 --> 00:49:38,220
ваши достижения зависят от  например, вы

1332
00:49:38,220 --> 00:49:40,140
изучаете определенную область, много

1333
00:49:40,140 --> 00:49:42,780
когнитивных тестов, и поэтому я думаю, было бы

1334
00:49:42,780 --> 00:49:46,560
очень полезно, если бы у нас были модели, которые,

1335
00:49:46,560 --> 00:49:48,540
да, я имею в виду обучение, вы можете думать об

1336
00:49:48,540 --> 00:49:50,760
этом как о типе действия, например, о том, как научиться,

1337
00:49:50,760 --> 00:49:52,980
куда направлять взгляд, один из  самый

1338
00:49:52,980 --> 00:49:54,420
простой из возможных, который очень помог бы в

1339
00:49:54,420 --> 00:49:56,220
моделировании иллюзий, и

1340
00:49:56,220 --> 00:49:58,859
я имею в виду, что для меня это как если бы я прочитал

1341
00:49:58,859 --> 00:50:00,720
статью о каких-то когнитивных

1342
00:50:00,720 --> 00:50:02,940
экспериментах или о какой-то иллюзии, и

1343
00:50:02,940 --> 00:50:05,160
я подумал: «Хорошо, могу ли я поместить этот

1344
00:50:05,160 --> 00:50:07,560
набор данных в свой  смоделируйте и протестируйте ее, и

1345
00:50:07,560 --> 00:50:08,579
в большинстве случаев ответ — нет,

1346
00:50:08,579 --> 00:50:10,619
потому что у меня нет модели, которая

1347
00:50:10,619 --> 00:50:12,900
смотрит вокруг или имеет ограниченное поле

1348
00:50:12,900 --> 00:50:14,660
зрения, что-то в этом роде,

1349
00:50:14,660 --> 00:50:16,619
так что да, я думаю, что это одно из

1350
00:50:16,619 --> 00:50:19,680
ограничений, другое —

1351
00:50:19,680 --> 00:50:20,579
хм

1352
00:50:20,579 --> 00:50:22,920
да, сделайте  эксперимент намного

1353
00:50:22,920 --> 00:50:24,900
сложнее, так что это одно из

1354
00:50:24,900 --> 00:50:27,359
практических ограничений,

1355
00:50:27,359 --> 00:50:30,240
вау, отличный ответ заставляет меня думать о

1356
00:50:30,240 --> 00:50:33,920
бумаге с буквами, вращающимися на столе,

1357
00:50:33,920 --> 00:50:36,780
это вращение цифр, отличные моменты

1358
00:50:36,780 --> 00:50:38,460
о фовеации и динамике

1359
00:50:38,460 --> 00:50:40,079
иллюзии. Я думаю, вы действительно

1360
00:50:40,079 --> 00:50:42,599
упомянули иллюзию, которая  Однако вы

1361
00:50:42,599 --> 00:50:43,980
упомянули в контексте обобщения,

1362
00:50:43,980 --> 00:50:46,859
что вращение на двухмерном

1363
00:50:46,859 --> 00:50:49,500
экране не обобщается на три

1364
00:50:49,500 --> 00:50:52,920
измерения, и этот размерный коллапс

1365
00:50:52,920 --> 00:50:55,559
или уменьшение является основой иллюзий

1366
00:50:55,559 --> 00:50:58,619
проекции куба и

1367
00:50:58,619 --> 00:51:01,880
иллюзий вращения куба и фигуры, это на вашем экране,

1368
00:51:01,880 --> 00:51:05,280
и там  силуэт или какие-то

1369
00:51:05,280 --> 00:51:07,260
неоднозначные

1370
00:51:07,260 --> 00:51:09,839
стимулы, которые генеративны, это близкая к

1371
00:51:09,839 --> 00:51:12,359
критичности или развилка в

1372
00:51:12,359 --> 00:51:13,680
дегенеративных моделях, поэтому они могут

1373
00:51:13,680 --> 00:51:17,160
представлять это так или иначе,

1374
00:51:17,160 --> 00:51:19,920
и поэтому многие иллюзии переключения

1375
00:51:19,920 --> 00:51:22,020
просто основаны на плоскостности

1376
00:51:22,020 --> 00:51:23,819
изображений

1377
00:51:23,819 --> 00:51:26,280
, ограничениях и обобщениях.

1378
00:51:26,280 --> 00:51:28,740
это раскрывается,

1379
00:51:28,740 --> 00:51:32,460
да, да, я думаю, что где-то даже есть «О да»,

1380
00:51:32,460 --> 00:51:34,859
извините, есть кое-какая работа, или

1381
00:51:34,859 --> 00:51:35,880
они

1382
00:51:35,880 --> 00:51:37,619
могут утверждать, что у людей

1383
00:51:37,619 --> 00:51:39,480
в голове есть трехмерный образ,

1384
00:51:39,480 --> 00:51:42,000
как будто даже Нэнси Кен недавно была или ее

1385
00:51:42,000 --> 00:51:45,119
побочными действиями, но и показывает, что да, я

1386
00:51:45,119 --> 00:51:48,119
не  не знаю, есть ли в наших моделях то, что

1387
00:51:48,119 --> 00:51:50,460
он в любом случае не очень большой,

1388
00:51:50,460 --> 00:51:53,700
да, это довольно интересно, хм, ладно,

1389
00:51:53,700 --> 00:51:56,160
из Upcycle Club в

1390
00:51:56,160 --> 00:51:58,200
чате они написали спасибо,

1391
00:51:58,200 --> 00:52:00,000
если вы можете учиться почти так же

1392
00:52:00,000 --> 00:52:02,160
эффективно, если представить, что вам нужен только

1393
00:52:02,160 --> 00:52:03,780
один нейрон  активен для каждого

1394
00:52:03,780 --> 00:52:06,540
примера, ваша модель будет

1395
00:52:06,540 --> 00:52:08,579
пытаться запомнить структуру набора данных

1396
00:52:08,579 --> 00:52:10,980
или что-то в этом роде,

1397
00:52:10,980 --> 00:52:12,180
и у вас не будет достаточной

1398
00:52:12,180 --> 00:52:14,940
емкости, так что да, я думаю, что настройка этого

1399
00:52:14,940 --> 00:52:18,359
уровня разреженности, безусловно, является

1400
00:52:18,359 --> 00:52:22,200
важным фактором, и

1401
00:52:22,200 --> 00:52:25,020
да  когда вы смотрите на вероятность, если

1402
00:52:25,020 --> 00:52:26,220
вы говорите, если вы удваиваете

1403
00:52:26,220 --> 00:52:28,579
структуру, обычно это

1404
00:52:28,579 --> 00:52:32,040
автоматически балансируется с самой вероятностью,

1405
00:52:32,040 --> 00:52:33,000
гм,

1406
00:52:33,000 --> 00:52:34,380
если вы не выполняете моделирование генерации,

1407
00:52:34,380 --> 00:52:35,760
у вас просто есть штраф за разреженность, который вы

1408
00:52:35,760 --> 00:52:38,460
захотите настроить  этот параметр,

1409
00:52:38,460 --> 00:52:40,980
окей, да, это просто для того, чтобы прояснить

1410
00:52:40,980 --> 00:52:43,380
неконтролируемое поведение в Armina, когда

1411
00:52:43,380 --> 00:52:45,599
сеть становится нестабильной или хаотичной из-

1412
00:52:45,599 --> 00:52:47,040
за различных факторов, таких как

1413
00:52:47,040 --> 00:52:50,400
шум контуров обратной связи или состязательные входные данные,

1414
00:52:50,400 --> 00:52:52,380
хм,

1415
00:52:52,380 --> 00:52:54,180
да, я думаю, я не рассматривал это

1416
00:52:54,180 --> 00:52:55,859
как повторяющуюся настройку, где  вы бы

1417
00:52:55,859 --> 00:52:58,500
получили петли обратной связи,

1418
00:52:58,500 --> 00:52:59,460


1419
00:52:59,460 --> 00:53:01,800
но я мог бы, да, я мог бы увидеть состязательные

1420
00:53:01,800 --> 00:53:04,319
примеры, на которые потенциально

1421
00:53:04,319 --> 00:53:07,800
влияет ваш уровень разреженности,

1422
00:53:07,800 --> 00:53:09,119
гм,

1423
00:53:09,119 --> 00:53:10,859
интересный момент в том, что вы

1424
00:53:10,859 --> 00:53:12,660
были бы более восприимчивы или менее восприимчивы,

1425
00:53:12,660 --> 00:53:16,040
чтобы поделиться примерами, которые я не знаю,

1426
00:53:16,040 --> 00:53:19,440


1427
00:53:19,440 --> 00:53:21,720
проецирование разреженности из  полностью связанная модель более высокого

1428
00:53:21,720 --> 00:53:23,579
измерения, постепенно превращающаяся в

1429
00:53:23,579 --> 00:53:25,619
постепенно уменьшающуюся,

1430
00:53:25,619 --> 00:53:27,540
в целом довольно хорошо понятно,

1431
00:53:27,540 --> 00:53:29,760
каковы компромиссы: проще

1432
00:53:29,760 --> 00:53:34,079
вычисления, меньшая модель более разрежена,

1433
00:53:34,079 --> 00:53:36,420
базовый график будет яснее

1434
00:53:36,420 --> 00:53:39,119
представлять, и тогда у него также будут

1435
00:53:39,119 --> 00:53:41,339
все остальные компромиссы.  выключены с ложными

1436
00:53:41,339 --> 00:53:43,680
положительными и отрицательными результатами обобщения,

1437
00:53:43,680 --> 00:53:45,720
но именно поэтому это итеративный

1438
00:53:45,720 --> 00:53:47,579
процесс подгонки, поэтому

1439
00:53:47,579 --> 00:53:49,760
я думаю, как ваш

1440
00:53:49,760 --> 00:53:52,800


1441
00:53:52,800 --> 00:53:55,700
баланс подхода к разрежению

1442
00:53:56,700 --> 00:53:59,520
не использует AIC, Bic или какой-либо другой

1443
00:53:59,520 --> 00:54:01,619
подход к подгонке модели для определения

1444
00:54:01,619 --> 00:54:03,660
соответствующего разрежения

1445
00:54:03,660 --> 00:54:07,079
для заданных входных данных,

1446
00:54:07,079 --> 00:54:09,780
как вы это делаете  определить, как это похоже на

1447
00:54:09,780 --> 00:54:11,940
лассо-регрессию, например, откуда вы знаете,

1448
00:54:11,940 --> 00:54:14,339
сколько, как вы устанавливаете порог,

1449
00:54:14,339 --> 00:54:17,220
сколько, насколько разрежено вы хотите, чтобы это было

1450
00:54:17,220 --> 00:54:19,559
правильно да, я думаю, что есть много хорошей

1451
00:54:19,559 --> 00:54:22,440
литературы по этому вопросу, и даже несмотря на это некоторым

1452
00:54:22,440 --> 00:54:25,319
людям они нравятся в Гарварде и  некоторые

1453
00:54:25,319 --> 00:54:29,520
люди сейчас работают с

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
такими развернутыми итеративными

1456
00:54:34,380 --> 00:54:36,780
сетями разрежения,

1457
00:54:36,780 --> 00:54:37,800
где это похоже на рекуррентную нейронную

1458
00:54:37,800 --> 00:54:40,380
сеть и итеративно разрежается, и

1459
00:54:40,380 --> 00:54:41,940
вы можете показать, что это дает что-то

1460
00:54:41,940 --> 00:54:45,780
вроде красного проигрыша или группы, например, группы,

1461
00:54:45,780 --> 00:54:47,520
активной группы, спортивных активаций, таких как

1462
00:54:47,520 --> 00:54:48,960
мы  Я использую здесь,

1463
00:54:48,960 --> 00:54:52,859
хм, в этой настройке, хм, на самом деле это просто

1464
00:54:52,859 --> 00:54:55,859


1465
00:54:55,859 --> 00:54:59,280
такая конструкция этой переменной T,

1466
00:54:59,280 --> 00:55:04,079
где у нас есть Z сверху и хм,

1467
00:55:04,079 --> 00:55:07,859
а затем это в некотором смысле ограничивается

1468
00:55:07,859 --> 00:55:09,119
этими

1469
00:55:09,119 --> 00:55:11,579
суммами переменных U внизу, поэтому,

1470
00:55:11,579 --> 00:55:13,200
если W  возможно, я не очень ясно выразился,

1471
00:55:13,200 --> 00:55:16,500
что это соединяющая матрица, которая

1472
00:55:16,500 --> 00:55:18,359
определяет группу, поэтому я

1473
00:55:18,359 --> 00:55:20,400
определяю университетскую группу группы, которая

1474
00:55:20,400 --> 00:55:22,380
соединяет все эти U вместе, и поэтому

1475
00:55:22,380 --> 00:55:23,940
идея такая,

1476
00:55:23,940 --> 00:55:27,540
как здесь, если все одно из другого

1477
00:55:27,540 --> 00:55:31,740
примеры, если все ваши использования uh не

1478
00:55:31,740 --> 00:55:35,520
активны для данного t

1479
00:55:35,520 --> 00:55:38,280
или если все варианты активны для данного

1480
00:55:38,280 --> 00:55:41,040
t uh, эта переменная t будет очень

1481
00:55:41,040 --> 00:55:42,780
маленькой, потому что ваш знаменатель

1482
00:55:42,780 --> 00:55:44,339
будет очень большим, и это вызывает

1483
00:55:44,339 --> 00:55:47,160
разреженность, поэтому это  ну, это

1484
00:55:47,160 --> 00:55:49,260
удовлетворение ограничений, если у вас есть, если у вас есть

1485
00:55:49,260 --> 00:55:51,839
набор U, которые все маленькие, тогда

1486
00:55:51,839 --> 00:55:54,480
это ограничение удовлетворено, и

1487
00:55:54,480 --> 00:55:57,180
теперь Z разрешено как бы выражать

1488
00:55:57,180 --> 00:56:00,240
себя, и это то, чего тогда,

1489
00:56:00,240 --> 00:56:02,880
э-э, вроде да, достигает в плане

1490
00:56:02,880 --> 00:56:06,180
активации  Итак, это вызвано этими

1491
00:56:06,180 --> 00:56:07,020
двумя

1492
00:56:07,020 --> 00:56:09,300
терминами дивергенции, здесь они

1493
00:56:09,300 --> 00:56:12,960
говорят, например, насколько далеко каждый unhc

1494
00:56:12,960 --> 00:56:15,180
от гауссианы, а затем с помощью этой

1495
00:56:15,180 --> 00:56:16,980
конструкции переменной T студента

1496
00:56:16,980 --> 00:56:20,880
мы эффективно строим разреженное

1497
00:56:20,880 --> 00:56:23,040
априорное распределение только из этих

1498
00:56:23,040 --> 00:56:24,839
гауссиан, но в  с точки зрения действия,

1499
00:56:24,839 --> 00:56:27,599
фактическая цель, э-э, условия и

1500
00:56:27,599 --> 00:56:28,920
цель, которые мы оптимизируем, - это всего лишь

1501
00:56:28,920 --> 00:56:31,619
эти два термина KL, которые

1502
00:56:31,619 --> 00:56:34,020
в некоторой степени подталкивают его к разреженности, и это

1503
00:56:34,020 --> 00:56:36,359
автоматически уравновешивается

1504
00:56:36,359 --> 00:56:39,000
термином правдоподобия здесь через декодер,

1505
00:56:39,000 --> 00:56:41,220
поэтому мы не  у нас есть термины, которые мы настраиваем,

1506
00:56:41,220 --> 00:56:42,839
но мы изучаем параметры

1507
00:56:42,839 --> 00:56:44,280
этих разных кодировщиков, а затем

1508
00:56:44,280 --> 00:56:48,200
анализируем сбои и чрезвычайные ситуации, о,

1509
00:56:48,540 --> 00:56:49,920


1510
00:56:49,920 --> 00:56:52,859
ладно, еще один вопрос от Дэйва

1511
00:56:52,859 --> 00:56:55,500
Дугласа, который написал,

1512
00:56:55,500 --> 00:56:59,160
говоря о взгляде и иллюзии, можно ли

1513
00:56:59,160 --> 00:57:01,920


1514
00:57:01,920 --> 00:57:04,260
разделить исследования постоянства у младенцев  в иллюзию более низкого уровня Rel,

1515
00:57:04,260 --> 00:57:06,140
возможно, концептуальное постоянство более высокого уровня,

1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
ну, вы можете прочитать

1518
00:57:15,720 --> 00:57:18,300
текущий вид архитектуры, могут ли

1519
00:57:18,300 --> 00:57:23,520
исследования констант у младенцев,

1520
00:57:23,520 --> 00:57:26,640
хм, когнитивные константы быть разделены,

1521
00:57:26,640 --> 00:57:31,619
да, вероятно, я не я не

1522
00:57:31,619 --> 00:57:33,900
эксперт или на самом деле даже не очень знакомый

1523
00:57:33,900 --> 00:57:35,460
с подобными

1524
00:57:35,460 --> 00:57:37,859
исследованиями постоянства объектов, детьми

1525
00:57:37,859 --> 00:57:40,260
и постоянством, но я думаю, что это

1526
00:57:40,260 --> 00:57:42,300
было бы невероятно интересно изучить

1527
00:57:42,300 --> 00:57:44,339
в архитектуре нейронных сетей, и

1528
00:57:44,339 --> 00:57:46,740
это была своего рода идея с

1529
00:57:46,740 --> 00:57:48,780
этой иллюзией, которую я пытался

1530
00:57:48,780 --> 00:57:51,780
смоделировать здесь с помощью этой строки  Я не

1531
00:57:51,780 --> 00:57:53,099
знаю, ясно ли я выразил это, но

1532
00:57:53,099 --> 00:57:55,380
верхняя строка — это входные данные, и мы

1533
00:57:55,380 --> 00:57:57,359
фактически блокируем входные данные для

1534
00:57:57,359 --> 00:58:00,119
одного кадра, и я хотел посмотреть,

1535
00:58:00,119 --> 00:58:03,240
кодирует ли сеть то, что эта

1536
00:58:03,240 --> 00:58:05,400
штука все еще там.  когда этот кадр

1537
00:58:05,400 --> 00:58:07,680
исчезнет, ​​могу ли я все еще расшифровать присутствие

1538
00:58:07,680 --> 00:58:10,020
объекта по нейронной активности,

1539
00:58:10,020 --> 00:58:11,819
а затем что он делает вывод о

1540
00:58:11,819 --> 00:58:13,920
движении из-за того, что он

1541
00:58:13,920 --> 00:58:15,780
видел полосы в немного другом

1542
00:58:15,780 --> 00:58:18,480
месте, чем раньше, когда после

1543
00:58:18,480 --> 00:58:20,520
фрейм исчез,

1544
00:58:20,520 --> 00:58:22,559
так что

1545
00:58:22,559 --> 00:58:25,200
да, я думаю, что это определенно несколько

1546
00:58:25,200 --> 00:58:27,240
уровней,

1547
00:58:27,240 --> 00:58:29,160
где некоторые из них, вероятно, будут намного

1548
00:58:29,160 --> 00:58:33,180
более низкими, и,

1549
00:58:33,180 --> 00:58:35,880
возможно, долгосрочное постоянство объекта, я

1550
00:58:35,880 --> 00:58:37,380
думаю, будет значительно

1551
00:58:37,380 --> 00:58:39,059
более высоким уровнем,

1552
00:58:39,059 --> 00:58:39,900
хм,

1553
00:58:39,900 --> 00:58:41,760
это просто заставляет меня думать об этих

1554
00:58:41,760 --> 00:58:44,640
экспериментах с кошками  в те времена,

1555
00:58:44,640 --> 00:58:47,280
когда они как будто поднимали их в

1556
00:58:47,280 --> 00:58:49,020
темноте, за исключением часа в день, они

1557
00:58:49,020 --> 00:58:51,000
помещали их в вертикальный мир или горизонтальный

1558
00:58:51,000 --> 00:58:53,160
мир, или они видели только горизонтальные линии

1559
00:58:53,160 --> 00:58:57,299
или вертикальные линии, и вы можете видеть, как

1560
00:58:57,299 --> 00:58:59,880
меняется организация их коры,

1561
00:58:59,880 --> 00:59:02,819
как они  у них меньше восприимчивости, это

1562
00:59:02,819 --> 00:59:04,200
горизонтальные линии, если они никогда

1563
00:59:04,200 --> 00:59:06,420
раньше не видели горизонтальных линий, а затем вы

1564
00:59:06,420 --> 00:59:07,980
берете палку и размахиваете ею перед

1565
00:59:07,980 --> 00:59:09,599
их лицом, и если палка

1566
00:59:09,599 --> 00:59:11,220
горизонтальна, они просто ничего не делают,

1567
00:59:11,220 --> 00:59:12,900
это вертикально, они хлопают по ней

1568
00:59:12,900 --> 00:59:14,460
они пытаются попасть в цель, как будто им

1569
00:59:14,460 --> 00:59:15,900
просто буквально не нужно бить

1570
00:59:15,900 --> 00:59:18,420
перед лицом, так что я думаю, что в этом

1571
00:59:18,420 --> 00:59:20,700
случае это свидетельство

1572
00:59:20,700 --> 00:59:24,260
низкого уровня дефицита и зрения,

1573
00:59:24,260 --> 00:59:26,940
способствующего своего рода иллюзии,

1574
00:59:26,940 --> 00:59:28,980
так что я...  думаю, да, конечно, может

1575
00:59:28,980 --> 00:59:30,660
быть какой-то аспект этого и у младенцев,

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
один очень любопытный момент, который вы подняли, -

1578
00:59:36,420 --> 00:59:40,339
это многообразие одушевленных и неодушевленных

1579
00:59:40,339 --> 00:59:43,619
вещей, где мелкие предметы являются

1580
00:59:43,619 --> 00:59:45,480
промежуточными,

1581
00:59:45,480 --> 00:59:49,140
верно, что это представляет,

1582
00:59:49,140 --> 00:59:52,319
или или это потому, что с ними можно обращаться,

1583
00:59:52,319 --> 00:59:55,619
или это  может быть насекомое или это может быть

1584
00:59:55,619 --> 00:59:57,839
что-то, что может улететь просто с

1585
00:59:57,839 --> 01:00:01,380
ветром, или что это говорит, да, ну,

1586
01:00:01,380 --> 01:00:04,380


1587
01:00:04,380 --> 01:00:08,280
так это работа Талии Конкл, я

1588
01:00:08,280 --> 01:00:11,280
думаю, это был тот, кто обнаружил эту

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
организацию, и они пытались

1591
01:00:14,880 --> 01:00:16,440
это выяснить  Нет, возможно, я

1592
01:00:16,440 --> 01:00:19,500
ошибаюсь, поэтому я рекомендую людям прочитать

1593
01:00:19,500 --> 01:00:21,359
ее работу по этому вопросу, если они называют это

1594
01:00:21,359 --> 01:00:23,880
трехсторонней организацией, но если я

1595
01:00:23,880 --> 01:00:25,319
правильно помню,

1596
01:00:25,319 --> 01:00:27,900
они проделали большую дополнительную работу над тем, почему существует

1597
01:00:27,900 --> 01:00:30,780
эта организация, и

1598
01:00:30,780 --> 01:00:33,180
некоторые доказательства существования этой организации.

1599
01:00:33,180 --> 01:00:35,700
кривизна этих объектов и что-то вроде

1600
01:00:35,700 --> 01:00:37,440
расстояния, на котором вы видите объекты,

1601
01:00:37,440 --> 01:00:40,260
или похожие на

1602
01:00:40,260 --> 01:00:43,319
одушевленные объекты, или, может быть, более изогнутые,

1603
01:00:43,319 --> 01:00:45,599
или что-то еще, независимо от того, каков

1604
01:00:45,599 --> 01:00:46,859
фактический ответ, было много

1605
01:00:46,859 --> 01:00:48,720
разных гипотез, которые вытекали

1606
01:00:48,720 --> 01:00:51,720
из одинаковых свойств этих объектов

1607
01:00:51,720 --> 01:00:54,000
возможно, свойства среднего или низкого уровня

1608
01:00:54,000 --> 01:00:56,280
в большей степени, чем свойства более высокого уровня, я

1609
01:00:56,280 --> 01:00:57,599
до сих пор не знаю, точно ли

1610
01:00:57,599 --> 01:00:59,339
решено, похоже ли это на

1611
01:00:59,339 --> 01:01:01,500
взаимодействие, как вы сказали, с

1612
01:01:01,500 --> 01:01:04,920
объектами, вызывающее разделение или,

1613
01:01:04,920 --> 01:01:06,119
хм,

1614
01:01:06,119 --> 01:01:09,540
или да, общие формы этих

1615
01:01:09,540 --> 01:01:12,299
объектов, которые я  готов поспорить, что, как и в большинстве случаев,

1616
01:01:12,299 --> 01:01:13,980
это похоже на некую комбинацию всего

1617
01:01:13,980 --> 01:01:16,980
вышеперечисленного, но я думаю, что интересная

1618
01:01:16,980 --> 01:01:18,480
вещь с этой точки зрения моделирования

1619
01:01:18,480 --> 01:01:19,859
заключается в том, что

1620
01:01:19,859 --> 01:01:21,480


1621
01:01:21,480 --> 01:01:24,059
это обучается только на

1622
01:01:24,059 --> 01:01:26,819
статистике корреляции из самих наборов данных изображений,

1623
01:01:26,819 --> 01:01:28,799
поэтому это не имеет никакого взаимодействия.

1624
01:01:28,799 --> 01:01:32,760
не имеет понятия об анимации, я имею в виду, что на самом

1625
01:01:32,760 --> 01:01:34,140
деле это просто тренировка модели в сети

1626
01:01:34,140 --> 01:01:37,859
изображений, просто изображения собак, кошек, лодок,

1627
01:01:37,859 --> 01:01:40,020
что угодно, и все же он все равно достигает такого

1628
01:01:40,020 --> 01:01:41,640
типа организации, так что

1629
01:01:41,640 --> 01:01:42,540


1630
01:01:42,540 --> 01:01:44,940
это могут быть какие-то семантические характеристики,

1631
01:01:44,940 --> 01:01:46,740
верно, у нас есть изображение, у нас есть  сеть,

1632
01:01:46,740 --> 01:01:48,359
которая может классифицировать

1633
01:01:48,359 --> 01:01:51,000
лодки по сравнению с собаками по сравнению с 20 другими породами

1634
01:01:51,000 --> 01:01:53,640
собак, но если бы

1635
01:01:53,640 --> 01:01:55,920
она также могла иметь какое-то соответствие

1636
01:01:55,920 --> 01:01:57,900
со статистикой финиша более низкого уровня,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
так что да, я не знаю, я думаю,

1639
01:02:03,960 --> 01:02:07,500
да, провокационной аналогией был

1640
01:02:07,500 --> 01:02:10,380
сдвиг

1641
01:02:10,380 --> 01:02:12,900
в переводе в почерке

1642
01:02:12,900 --> 01:02:14,819
настройка распознавания,

1643
01:02:14,819 --> 01:02:18,000
какие трансляционные сдвиги

1644
01:02:18,000 --> 01:02:20,160


1645
01:02:20,160 --> 01:02:22,740
существуют сегодня, что такое трехпиксельный

1646
01:02:22,740 --> 01:02:24,780
пример: какая-то подсказка, спланированная

1647
01:02:24,780 --> 01:02:27,540
атака на фильм или что-то в этом

1648
01:02:27,540 --> 01:02:29,099
роде, правильно вставляется специальный символ

1649
01:02:29,099 --> 01:02:32,640
или или, хм, какое-то

1650
01:02:32,640 --> 01:02:35,160
наложение на изображение, которое мы даже не можем

1651
01:02:35,160 --> 01:02:37,020
обнаружить это, и

1652
01:02:37,020 --> 01:02:39,359
как вы думаете, в чем заключаются эти проблемы

1653
01:02:39,359 --> 01:02:42,839
и как мы можем их решить,

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
да, абсолютно я имею в виду, я думаю, что это вроде

1656
01:02:47,520 --> 01:02:48,420
того, как

1657
01:02:48,420 --> 01:02:50,099
я думал об этом, это похоже на

1658
01:02:50,099 --> 01:02:52,680
эти преобразования симметрии,

1659
01:02:52,680 --> 01:02:53,760
хм,

1660
01:02:53,760 --> 01:02:55,799
если вы думаете о языковых моделях,

1661
01:02:55,799 --> 01:02:57,420
вы  можно представить

1662
01:02:57,420 --> 01:02:58,500
преобразование симметрии, которое похоже на

1663
01:02:58,500 --> 01:03:00,240
замену слова синонимом или

1664
01:03:00,240 --> 01:03:03,780
чем-то еще, у вас есть предложение для нас

1665
01:03:03,780 --> 01:03:06,000
означает одно и то же, но теперь

1666
01:03:06,000 --> 01:03:07,380
внезапно модель будет реагировать

1667
01:03:07,380 --> 01:03:09,299
совсем по-другому,

1668
01:03:09,299 --> 01:03:11,240
хм,

1669
01:03:11,240 --> 01:03:15,359
как перевод между языками, это

1670
01:03:15,359 --> 01:03:16,799
можно рассматривать как  типа преобразования,

1671
01:03:16,799 --> 01:03:19,440
он сохраняет для нас основной

1672
01:03:19,440 --> 01:03:21,960
смысл входных данных,

1673
01:03:21,960 --> 01:03:24,900
но для модели он выглядит

1674
01:03:24,900 --> 01:03:26,220
совершенно по-другому, и мы хотели бы

1675
01:03:26,220 --> 01:03:28,380
иметь модели, которые ведут себя

1676
01:03:28,380 --> 01:03:29,940
предсказуемым образом по отношению к этим

1677
01:03:29,940 --> 01:03:32,160
типам преобразований, потому что

1678
01:03:32,160 --> 01:03:35,040
я думаю, что люди ведут себя очень предсказуемо,

1679
01:03:35,040 --> 01:03:37,319
поскольку эти типы преобразований  Трансформации, и когда

1680
01:03:37,319 --> 01:03:39,920
мы имеем дело с системами искусственного интеллекта, мы ожидаем, что

1681
01:03:39,920 --> 01:03:43,200
они также будут вести себя таким же образом, и я думаю,

1682
01:03:43,200 --> 01:03:45,480
что это часть того, что вызывает множество

1683
01:03:45,480 --> 01:03:47,339
проблем при взаимодействии с этими

1684
01:03:47,339 --> 01:03:49,460
системами, и я как бы попытался

1685
01:03:49,460 --> 01:03:52,500
продемонстрировать это

1686
01:03:52,500 --> 01:03:54,960
с помощью этого  медведь, квадраты и тому

1687
01:03:54,960 --> 01:03:58,440
подобное, мы ожидаем, что

1688
01:03:58,440 --> 01:04:00,480
он сможет сделать что-то простое,

1689
01:04:00,480 --> 01:04:02,220
вроде этого, потому что мы думаем, что большинство людей

1690
01:04:02,220 --> 01:04:04,020
могут, но это не так, и если вы

1691
01:04:04,020 --> 01:04:05,460
представляете, что это критический сценарий,

1692
01:04:05,460 --> 01:04:07,740
где вы ожидаете этого, и это большая

1693
01:04:07,740 --> 01:04:08,700
проблема,

1694
01:04:08,700 --> 01:04:11,099
хм  как мы с этим справимся это да, я

1695
01:04:11,099 --> 01:04:12,720
думаю, что это то, что я

1696
01:04:12,720 --> 01:04:15,859
ищу, я думаю, что

1697
01:04:16,319 --> 01:04:18,420
мое

1698
01:04:18,420 --> 01:04:22,280
направление, в котором я придерживаюсь, выглядит

1699
01:04:22,280 --> 01:04:26,940
более простым и похоже на восходящие

1700
01:04:26,940 --> 01:04:29,460
строительные блоки

1701
01:04:29,460 --> 01:04:31,380
архитектур нейронных сетей или

1702
01:04:31,380 --> 01:04:33,180
алгоритмов, которые

1703
01:04:33,180 --> 01:04:35,760
как бы дают эти возникающие  структурные

1704
01:04:35,760 --> 01:04:37,680
свойства, и я думаю, что это гораздо

1705
01:04:37,680 --> 01:04:39,839
более обобщаемый способ, вместо того, чтобы

1706
01:04:39,839 --> 01:04:41,579
строить что-то поверх того, что у нас

1707
01:04:41,579 --> 01:04:43,140
уже есть.

1708
01:04:43,140 --> 01:04:43,920


1709
01:04:43,920 --> 01:04:45,839
Я думаю, это то, что будет

1710
01:04:45,839 --> 01:04:47,819
гораздо лучше масштабироваться, а также больше соответствовать тому, что

1711
01:04:47,819 --> 01:04:50,299
делает мозг,

1712
01:04:50,760 --> 01:04:52,740
очень круто, один из видов

1713
01:04:52,740 --> 01:04:54,740
вопросов реализации, каковы  вычислительные

1714
01:04:54,740 --> 01:04:57,000
требования для простого выполнения этого или

1715
01:04:57,000 --> 01:04:59,400
что такое повседневная работа

1716
01:04:59,400 --> 01:05:01,920
студента или исследователя, выполняющего

1717
01:05:01,920 --> 01:05:04,380
такие варианты, например, используют ли они терабайты

1718
01:05:04,380 --> 01:05:07,020
данных, а вы используете большие вычисления,

1719
01:05:07,020 --> 01:05:08,940
или это то, что люди могут запускать

1720
01:05:08,940 --> 01:05:11,880
самостоятельно  ноутбуки

1721
01:05:11,880 --> 01:05:13,980
Я думаю, что почти все, что я представил

1722
01:05:13,980 --> 01:05:17,099
сегодня, можно запускать локально, так что, как будто это

1723
01:05:17,099 --> 01:05:20,040
очень просто, вы можете запустить, я

1724
01:05:20,040 --> 01:05:20,760
имею в виду, вы

1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
думаете, что можете запустить это на своем ноутбуке, если

1727
01:05:24,299 --> 01:05:25,980
вам нравится тренироваться и экспериментировать

1728
01:05:25,980 --> 01:05:27,420
с разными вещами, которые он делает  быть

1729
01:05:27,420 --> 01:05:30,119
довольно медленным, поэтому я бы порекомендовал какой-нибудь

1730
01:05:30,119 --> 01:05:33,359
коммерческий графический процессор, например, я запускаю почти

1731
01:05:33,359 --> 01:05:35,640
все, например, Nvidia 1080,

1732
01:05:35,640 --> 01:05:38,819
довольно старые, довольно дешевые, но у них 12

1733
01:05:38,819 --> 01:05:41,099
гигабайт ОЗУ или что-то в этом роде, и

1734
01:05:41,099 --> 01:05:43,140
для этих моделей более чем достаточно четырех

1735
01:05:43,140 --> 01:05:46,440
гигабайт ОЗУ  Я думаю, что

1736
01:05:46,440 --> 01:05:48,839
некоторые люди считают странной одну вещь: я провожу большую

1737
01:05:48,839 --> 01:05:51,480
часть своих экспериментов с такими вещами, как mnist, поэтому

1738
01:05:51,480 --> 01:05:54,780
это изображения размером 32 на 32 пикселя, потому что я могу

1739
01:05:54,780 --> 01:05:57,299
тренировать их маленькими и вслух,

1740
01:05:57,299 --> 01:06:00,000
хм, если вы хотите, мои эксперименты

1741
01:06:00,000 --> 01:06:02,460
или бесконечные, если вы  хочу делать

1742
01:06:02,460 --> 01:06:03,540
такие вещи, они намного

1743
01:06:03,540 --> 01:06:05,640
сложнее, этот гамильтоновский динамический

1744
01:06:05,640 --> 01:06:08,160
пакет, здесь вы попадаете в более крупные

1745
01:06:08,160 --> 01:06:09,780
модели, которые работают на нескольких

1746
01:06:09,780 --> 01:06:12,240
графических процессорах, и поэтому здесь используется кластер для

1747
01:06:12,240 --> 01:06:14,220
запуска этих типов моделей,

1748
01:06:14,220 --> 01:06:16,140
гм, но я бы сказал, что большинство  одной

1749
01:06:16,140 --> 01:06:18,920
машины с графическим процессором более чем достаточно

1750
01:06:18,920 --> 01:06:21,680
или даже как в совместном ноутбуке,

1751
01:06:21,680 --> 01:06:24,539
что-то в этом роде, если вы хотите

1752
01:06:24,539 --> 01:06:26,520
что-то обучить в imagenet, это становится

1753
01:06:26,520 --> 01:06:29,940
сложнее, и в идеале вам нужен

1754
01:06:29,940 --> 01:06:33,660
хотя бы один графический процессор, но да, я

1755
01:06:33,660 --> 01:06:35,160
не делаю  много крупномасштабных вещей,

1756
01:06:35,160 --> 01:06:37,200
но я думаю, что это, безусловно, интересно,

1757
01:06:37,200 --> 01:06:39,960
и там определенно

1758
01:06:39,960 --> 01:06:42,539
можно сделать гораздо больше, но для некоторых из таких

1759
01:06:42,539 --> 01:06:45,480
более простых или более фундаментальных вопросов

1760
01:06:45,480 --> 01:06:47,760
я не знаю, как вы хотите это назвать, хм,

1761
01:06:47,760 --> 01:06:52,500
машина меньшего размера - это  красиво и быстро так

1762
01:06:52,500 --> 01:06:54,660
круто полезно

1763
01:06:54,660 --> 01:06:58,140
хорошо хорошо я прочитаю комментарий Дэйва,

1764
01:06:58,140 --> 01:07:00,780
вспоминающий комментарий Берта ДеВриса во время

1765
01:07:00,780 --> 01:07:02,880
симпозиума по прикладному активному выводу

1766
01:07:02,880 --> 01:07:05,520
о желательности тратить меньше

1767
01:07:05,520 --> 01:07:08,099
усилий или АТФ на поиск пищи или контроль над

1768
01:07:08,099 --> 01:07:09,780
ситуациями, где нам не нужна большая

1769
01:07:09,780 --> 01:07:11,579
точность Я не знаю  не знаю, слушаете ли вы

1770
01:07:11,579 --> 01:07:13,859
это, но профессор ДеВрис упомянул

1771
01:07:13,859 --> 01:07:17,520
о моделях переменной точности и о том, как

1772
01:07:17,520 --> 01:07:19,440
их можно использовать для

1773
01:07:19,440 --> 01:07:21,059
реализации различных функций

1774
01:07:21,059 --> 01:07:23,039
обобщения и фактического структурного

1775
01:07:23,039 --> 01:07:25,020
обучения, а также для снижения

1776
01:07:25,020 --> 01:07:27,059
вычислительных требований,

1777
01:07:27,059 --> 01:07:29,880
есть ли у него какие-либо предложения о том, как

1778
01:07:29,880 --> 01:07:31,980
это реализовать?  различие в

1779
01:07:31,980 --> 01:07:33,900
теории активного вывода, какие

1780
01:07:33,900 --> 01:07:37,760
эксперименты могли бы это выяснить,

1781
01:07:38,400 --> 01:07:40,680
ох, да, это то, чего я не думаю, я

1782
01:07:40,680 --> 01:07:42,660
не думаю, что у меня слишком много ума,

1783
01:07:42,660 --> 01:07:46,619
чтобы сказать об этом, это совершенно честно,

1784
01:07:46,619 --> 01:07:48,740
хм,

1785
01:07:51,359 --> 01:07:53,460
это очень интересный вопрос, потому что

1786
01:07:53,460 --> 01:07:56,220
я думаю, что интуиция делает

1787
01:07:56,220 --> 01:07:58,260
для меня очень много смысла в

1788
01:07:58,260 --> 01:08:00,000
том, о чем вы говорите,

1789
01:08:00,000 --> 01:08:01,980
если я правильно понимаю переменную степень

1790
01:08:01,980 --> 01:08:05,160
точности, когда вы кодируете в своей модели

1791
01:08:05,160 --> 01:08:07,079
или в своей модели в целом, выполняя

1792
01:08:07,079 --> 01:08:07,740
вычисления

1793
01:08:07,740 --> 01:08:09,420
[Музыка]

1794
01:08:09,420 --> 01:08:11,539
гм,

1795
01:08:13,200 --> 01:08:17,040
это каким-то образом влияет на вашу

1796
01:08:17,040 --> 01:08:19,080
будущую производительность, поскольку  связь с каким-то

1797
01:08:19,080 --> 01:08:22,259
хранилищем энергии, я думаю, да, и если вы

1798
01:08:22,259 --> 01:08:23,580
хотите встроить это в

1799
01:08:23,580 --> 01:08:26,279
систему активных усилий, вам понадобится

1800
01:08:26,279 --> 01:08:28,679
действительно воплощенная система, в которой у

1801
01:08:28,679 --> 01:08:31,500
агента есть некоторое представление об энергии, например о

1802
01:08:31,500 --> 01:08:34,439
внутреннем хранилище энергии, и

1803
01:08:34,439 --> 01:08:36,238
да, что-то, что пытается

1804
01:08:36,238 --> 01:08:38,520
для сохранения, пока он выполняет свои

1805
01:08:38,520 --> 01:08:40,500
действия, э-э,

1806
01:08:40,500 --> 01:08:42,359
и у него заканчивается энергия, потребуется

1807
01:08:42,359 --> 01:08:44,759
что-то плохое для агентов, и

1808
01:08:44,759 --> 01:08:47,399
тогда, возможно, вы могли бы наблюдать своего рода

1809
01:08:47,399 --> 01:08:48,679
появление,

1810
01:08:48,679 --> 01:08:52,040
э-э, сокращение и

1811
01:08:52,040 --> 01:08:55,198
кодирование Точность или что-то в

1812
01:08:55,198 --> 01:08:57,479
этом роде, поскольку агент пытается научиться

1813
01:08:57,479 --> 01:09:00,060
действовать более эффективно, возможно, вам придется

1814
01:09:00,060 --> 01:09:02,520
дать ему возможность контролировать его

1815
01:09:02,520 --> 01:09:04,020
точность,

1816
01:09:04,020 --> 01:09:07,080
да, как я говорю, исходя из своего опыта, но

1817
01:09:07,080 --> 01:09:08,819


1818
01:09:08,819 --> 01:09:11,460
на этом слайде вроде мысли в порядке, прямо здесь, сначала очень

1819
01:09:11,460 --> 01:09:14,219
крутое изображение, это что-то вроде

1820
01:09:14,219 --> 01:09:18,359
цифрового Джексона Поллока,

1821
01:09:18,359 --> 01:09:22,920
хм, если бы оно было  более простой размер входных данных

1822
01:09:22,920 --> 01:09:26,520
или просто уменьшение сложности

1823
01:09:26,520 --> 01:09:27,719
шаблонов, или если бы это была повышенная

1824
01:09:27,719 --> 01:09:30,000
сложность, как бы это изображение выглядело

1825
01:09:30,000 --> 01:09:31,920
по-другому,

1826
01:09:31,920 --> 01:09:34,620
да, поэтому я провел несколько экспериментов, пытаясь

1827
01:09:34,620 --> 01:09:36,738
изменить эти

1828
01:09:36,738 --> 01:09:40,439
столбцы ориентации, и

1829
01:09:40,439 --> 01:09:41,100


1830
01:09:41,100 --> 01:09:43,140
вы можете, да, в основном изменяя

1831
01:09:43,140 --> 01:09:44,520
параметры модели, вы можете

1832
01:09:44,520 --> 01:09:47,100
увеличьте эти столбцы, вы можете сделать так, чтобы

1833
01:09:47,100 --> 01:09:49,380
они не имели очень похожую структуру

1834
01:09:49,380 --> 01:09:51,660
на ту, которую мы видим у людей, где вы

1835
01:09:51,660 --> 01:09:53,040
есть, вы можете сделать так, чтобы у них было больше

1836
01:09:53,040 --> 01:09:55,199
полос активности,

1837
01:09:55,199 --> 01:09:56,160
хм,

1838
01:09:56,160 --> 01:09:58,199
и это также, как вы сказали, зависит от

1839
01:09:58,199 --> 01:10:00,540
набора данных  что вы используете, если я использую в качестве

1840
01:10:00,540 --> 01:10:03,179


1841
01:10:03,179 --> 01:10:05,580
входных данных действительно простые синусоидальные градуировки. Я получаю что-то вроде этого. Я получаю

1842
01:10:05,580 --> 01:10:07,920
что-то немного более эээ,

1843
01:10:07,920 --> 01:10:11,640
вращательная кривая с более высокой энтропией,

1844
01:10:11,640 --> 01:10:12,800


1845
01:10:12,800 --> 01:10:15,660
так что я думаю, что это все интересные

1846
01:10:15,660 --> 01:10:18,000
вещи, если вы хотите изучить

1847
01:10:18,000 --> 01:10:19,320
возникновение  такой тип организации

1848
01:10:19,320 --> 01:10:22,199
в естественной системе, ну, если у вас есть

1849
01:10:22,199 --> 01:10:24,120
модель, которая теперь дает разную

1850
01:10:24,120 --> 01:10:25,860
организацию для разных

1851
01:10:25,860 --> 01:10:28,620
настроек, это понятно, тогда какие

1852
01:10:28,620 --> 01:10:31,679
настройки лучше всего соответствуют нашим наблюдаемым данным,

1853
01:10:31,679 --> 01:10:32,340
ну,

1854
01:10:32,340 --> 01:10:34,679
так что да,

1855
01:10:34,679 --> 01:10:36,540
я могу отправить тех, кто вокруг нее, если вы

1856
01:10:36,540 --> 01:10:38,520
интересно, но,

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
хм, да, я думаю, одно и другое, извините, еще один

1859
01:10:44,219 --> 01:10:45,659
интересный момент, есть то,

1860
01:10:45,659 --> 01:10:46,920
что

1861
01:10:46,920 --> 01:10:51,480
разные животные и типы, э-э,

1862
01:10:51,480 --> 01:10:53,219
селективность ориентации и разное

1863
01:10:53,219 --> 01:10:54,659
количество вертушек, у некоторых животных

1864
01:10:54,659 --> 01:10:57,420
его вообще нет, я думаю, может быть, у мышей, если я

1865
01:10:57,420 --> 01:11:00,120
правильно, есть такой тип, э-э, они называют

1866
01:11:00,120 --> 01:11:01,800
селективностью по соли и перцу, так что это

1867
01:11:01,800 --> 01:11:03,480
в основном случайно, у вас нет какой-

1868
01:11:03,480 --> 01:11:04,679
либо чувствительности к топографической ориентации,

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
хм, так что есть свидетельства того, что

1871
01:11:09,300 --> 01:11:10,920
да, разные системы делают это

1872
01:11:10,920 --> 01:11:13,020
по-разному, и интересно

1873
01:11:13,020 --> 01:11:14,760
выяснить, почему

1874
01:11:14,760 --> 01:11:17,760
да, это  это очень круто, это напоминает мне, во

1875
01:11:17,760 --> 01:11:21,300
-первых, о

1876
01:11:21,300 --> 01:11:22,980
базе и времени диффузии реакции,

1877
01:11:22,980 --> 01:11:25,739
так что на самом деле

1878
01:11:25,739 --> 01:11:30,000
вполне возможно, что в регионе может не быть

1879
01:11:30,000 --> 01:11:32,840
активности с заданной

1880
01:11:32,840 --> 01:11:35,640
степенью детализации, как если бы на него смотрели

1881
01:11:35,640 --> 01:11:39,360
в пространственно-временной

1882
01:11:39,360 --> 01:11:40,520
шкале,

1883
01:11:40,520 --> 01:11:44,699
если очаги активности, но  если

1884
01:11:44,699 --> 01:11:46,380
очаги активности

1885
01:11:46,380 --> 01:11:48,480
медленнее и быстрее,

1886
01:11:48,480 --> 01:11:52,260
чем это измерение не будет

1887
01:11:52,260 --> 01:11:54,060
отличаться от шума, все будет

1888
01:11:54,060 --> 01:11:55,739
усреднено,

1889
01:11:55,739 --> 01:11:58,620
так что тогда могут быть некоторые, да,

1890
01:11:58,620 --> 01:12:01,560
интересные наборы данных, которые

1891
01:12:01,560 --> 01:12:03,360
на самом деле имеют большое

1892
01:12:03,360 --> 01:12:06,179
богатство, но тогда для одного  по той или

1893
01:12:06,179 --> 01:12:08,520
иной причине оно просто было усреднено,

1894
01:12:08,520 --> 01:12:11,100
потому что оно не было связано с вами

1895
01:12:11,100 --> 01:12:12,420
или что-то в этом роде, вам действительно нужно

1896
01:12:12,420 --> 01:12:14,520
использовать один пробный уровень, вам нужно

1897
01:12:14,520 --> 01:12:16,140
иметь достаточно высокое пространственное разрешение,

1898
01:12:16,140 --> 01:12:18,719
чтобы вы знали, что оно удовлетворяет

1899
01:12:18,719 --> 01:12:23,100
микрочастотам, э-э  и это просто

1900
01:12:23,100 --> 01:12:24,659
то, чего люди не делали в течение

1901
01:12:24,659 --> 01:12:25,620
долгого времени, особенно если вы делаете

1902
01:12:25,620 --> 01:12:27,780
записи отдельных электоратов, вы не

1903
01:12:27,780 --> 01:12:28,920
увидите бегущую волну, вы

1904
01:12:28,920 --> 01:12:30,900
увидите колебания,

1905
01:12:30,900 --> 01:12:32,219
хм, так что вам нужно что-то вроде мультиэлектрического

1906
01:12:32,219 --> 01:12:34,199
массивы, и, по сути, они говорят: окей,

1907
01:12:34,199 --> 01:12:36,000
да, теперь, когда у нас есть технология, позволяющая

1908
01:12:36,000 --> 01:12:37,520
сделать это,

1909
01:12:37,520 --> 01:12:40,080
сохраняется столько всего, чего мы раньше не видели, и

1910
01:12:40,080 --> 01:12:42,960
потенциально это объяснение

1911
01:12:42,960 --> 01:12:44,400
большого количества шума, который мы видели

1912
01:12:44,400 --> 01:12:46,260
раньше, может быть, это действительно просто  путешествующие

1913
01:12:46,260 --> 01:12:47,219
волны,

1914
01:12:47,219 --> 01:12:47,760
хм,

1915
01:12:47,760 --> 01:12:51,480
так что да, я думаю, что в будущем многое предстоит сделать

1916
01:12:51,480 --> 01:12:53,880
с расширенными

1917
01:12:53,880 --> 01:12:56,520
возможностями для записи,

1918
01:12:56,520 --> 01:12:58,739
это очень круто,

1919
01:12:58,739 --> 01:13:02,940
ну, какие-нибудь заключительные мысли или вопросы, или

1920
01:13:02,940 --> 01:13:06,239
где вы собираетесь заняться этой работой,

1921
01:13:06,239 --> 01:13:08,520
да, нет, спасибо, что пригласили меня,

1922
01:13:08,520 --> 01:13:10,140
хм,

1923
01:13:10,140 --> 01:13:11,640
надеюсь, в активную инфраструктуру

1924
01:13:11,640 --> 01:13:14,520
вот и мне бы очень хотелось, я думаю,

1925
01:13:14,520 --> 01:13:16,560
это было бы супер весело, так что да, я не

1926
01:13:16,560 --> 01:13:18,980
совсем уверен, что смотрю на «Может быть, музыку,

1927
01:13:18,980 --> 01:13:21,659
прямо сейчас,

1928
01:13:21,659 --> 01:13:22,440
гм,

1929
01:13:22,440 --> 01:13:26,760
смотрю на

1930
01:13:26,760 --> 01:13:30,420
другие сумасшедшие направления, я не

1931
01:13:30,420 --> 01:13:33,020
хочу показаться слишком сумасшедшим,

1932
01:13:33,020 --> 01:13:36,900
но  Я расскажу о многих вещах, и

1933
01:13:36,900 --> 01:13:38,580
одна вещь, которую

1934
01:13:38,580 --> 01:13:40,320
мы отправили нейропсихам, - это изучение

1935
01:13:40,320 --> 01:13:43,140
памяти с помощью бегущих волн, хм,

1936
01:13:43,140 --> 01:13:45,060
так что статья только сегодня вышла в

1937
01:13:45,060 --> 01:13:46,860
архиве, хм,

1938
01:13:46,860 --> 01:13:48,840
как волны действительно хороши в кодировании

1939
01:13:48,840 --> 01:13:50,580
долговременных воспоминаний.  что, на мой взгляд,

1940
01:13:50,580 --> 01:13:52,100
очень интересно,

1941
01:13:52,100 --> 01:13:54,120
так что я мог бы пойти немного в этом

1942
01:13:54,120 --> 01:13:55,800
направлении,

1943
01:13:55,800 --> 01:13:58,920
звучит хорошо, и да, было бы очень

1944
01:13:58,920 --> 01:14:01,400
интересно увидеть, как действие вступает в игру,

1945
01:14:01,400 --> 01:14:04,560
когда нейроны остаются

1946
01:14:04,560 --> 01:14:07,620
активными, даже когда лапы собаки двигаются,

1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
есть много чего-то вроде  последовательности действий,

1949
01:14:11,480 --> 01:14:14,280
такие как бросок бейсбольного мяча, а затем все

1950
01:14:14,280 --> 01:14:15,900
идет, и как будто в

1951
01:14:15,900 --> 01:14:18,440
этом действии есть что-то, что продолжает

1952
01:14:18,440 --> 01:14:21,179
влиять и поэтому имеет глубокое

1953
01:14:21,179 --> 01:14:23,699
временное представление альтернативных

1954
01:14:23,699 --> 01:14:26,159
действий,

1955
01:14:26,159 --> 01:14:29,640
а затем вариационный автокодировщик

1956
01:14:29,640 --> 01:14:33,179
уже в принципе правильный, что-то подобное,

1957
01:14:33,179 --> 01:14:35,219
так что

1958
01:14:35,219 --> 01:14:37,739
очень ценю это  хорошо, спасибо,

1959
01:14:37,739 --> 01:14:39,480
до следующего раза,

1960
01:14:39,480 --> 01:14:43,339
большое спасибо, пока


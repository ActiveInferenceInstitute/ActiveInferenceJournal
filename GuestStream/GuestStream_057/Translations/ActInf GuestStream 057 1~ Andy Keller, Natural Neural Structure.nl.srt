1
00:00:06,600 --> 00:00:09,420
hallo en welkom het is 18 september

2
00:00:09,420 --> 00:00:14,040
2023 en het is actieve gaststream 57.1

3
00:00:14,040 --> 00:00:16,740
met Andy Keller we gaan

4
00:00:16,740 --> 00:00:19,619
het hebben over de natuurlijke neurale structuur

5
00:00:19,619 --> 00:00:22,020
voor kunstmatige intelligentie er zal

6
00:00:22,020 --> 00:00:24,300
een presentatie zijn gevolgd door een

7
00:00:24,300 --> 00:00:25,800
discussie dus als je live kijkt,

8
00:00:25,800 --> 00:00:27,900
voel je vrij om  schrijf vragen in

9
00:00:27,900 --> 00:00:30,660
de livechat, anders bedankt Andy

10
00:00:30,660 --> 00:00:32,279
hiervoor, ik kijk er echt naar uit

11
00:00:32,279 --> 00:00:35,899
en jou voor de presentatie,

12
00:00:36,360 --> 00:00:38,940
heel erg bedankt, bedankt dat je

13
00:00:38,940 --> 00:00:41,280
me hebt. Ik ben super enthousiast om

14
00:00:41,280 --> 00:00:42,840
dit spul te kunnen presenteren aan de actieve

15
00:00:42,840 --> 00:00:45,239
referentiegroep.  Ik ben een fan en zeer

16
00:00:45,239 --> 00:00:49,200
geïnteresseerd, dus hopelijk

17
00:00:49,200 --> 00:00:50,399
krijg je een goede discussie en zie je wat

18
00:00:50,399 --> 00:00:51,960
jullie ervan denken,

19
00:00:51,960 --> 00:00:54,840
dus mijn naam is Andy. Ik ben

20
00:00:54,840 --> 00:00:57,180
mijn PhD aan het afronden onder begeleiding van Maxwelling aan de

21
00:00:57,180 --> 00:00:59,100
Universiteit van Amsterdam, ik

22
00:00:59,100 --> 00:01:01,500
ben  Hierna start ik een postdoc aan Harvard,

23
00:01:01,500 --> 00:01:05,099
dus ik begin. Ik heb het alleen

24
00:01:05,099 --> 00:01:07,619
over het doel van mijn werk in het algemeen:

25
00:01:07,619 --> 00:01:09,540
proberen moderne kunstmatige

26
00:01:09,540 --> 00:01:12,540
intelligentie dichter bij een meer mensachtige

27
00:01:12,540 --> 00:01:15,240
generalisatie te brengen. Wat we hiermee bedoelen

28
00:01:15,240 --> 00:01:17,159
is misschien wat  een soort structuurgeneralisatie,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
of misschien meer bekend bij de actieve

31
00:01:20,700 --> 00:01:22,080
kindercommissie, zoals een gestructureerd

32
00:01:22,080 --> 00:01:24,299
wereldmodel waarvan we denken dat mensen dat

33
00:01:24,299 --> 00:01:26,340
hebben en de manier waarop we

34
00:01:26,340 --> 00:01:28,799
dit voorstellen is door de natuurlijke neurale

35
00:01:28,799 --> 00:01:32,400
structuur te integreren in kunstmatige intelligentie,

36
00:01:32,400 --> 00:01:34,979
dus laten we eerst definiëren wat we bedoelen  door

37
00:01:34,979 --> 00:01:36,720
structuurgeneralisatie,

38
00:01:36,720 --> 00:01:38,400
dus ik denk dat het tamelijk oncontroversieel is

39
00:01:38,400 --> 00:01:40,380
om te zeggen dat modern machinaal leren

40
00:01:40,380 --> 00:01:42,900
generaliseert buiten de training in

41
00:01:42,900 --> 00:01:45,000
de traditionele zin, dus

42
00:01:45,000 --> 00:01:46,799
zelfs de vroegste kunstmatige neurale

43
00:01:46,799 --> 00:01:49,020
netwerken, meerlaagse perceptrons, zouden kunnen

44
00:01:49,020 --> 00:01:51,659
worden getraind op datasets van dit soort afbeeldingen

45
00:01:51,659 --> 00:01:55,140
en een hoog niveau kunnen bereiken  Als

46
00:01:55,140 --> 00:01:56,880
ze dan een

47
00:01:56,880 --> 00:01:58,320
testset met afbeeldingen te zien krijgen die ze nog nooit

48
00:01:58,320 --> 00:02:00,240
eerder hebben gezien, kunnen ze deze nog steeds

49
00:02:00,240 --> 00:02:02,460
relatief eenvoudig classificeren met hetzelfde

50
00:02:02,460 --> 00:02:04,500
nauwkeurigheidsniveau. Dit is wat we doorgaans

51
00:02:04,500 --> 00:02:07,320
generalisatie noemen, maar zelfs al vrij

52
00:02:07,320 --> 00:02:08,758
vroeg was dat zo.  merkte op dat deze

53
00:02:08,758 --> 00:02:10,800
systemen echt worstelen met kleine

54
00:02:10,800 --> 00:02:12,720
verschuivingen of vervormingen, van toepassing op de

55
00:02:12,720 --> 00:02:16,340
afbeeldingen, als dat

56
00:02:18,920 --> 00:02:23,099
zo is, denk je, waarom is dit verrassend en

57
00:02:23,099 --> 00:02:24,720
ik beweer dat het echt te wijten is aan ons aangeboren

58
00:02:24,720 --> 00:02:26,640
vermogen om dit soort structuurgeneralisatie uit te voeren,

59
00:02:26,640 --> 00:02:28,680
dat dit

60
00:02:28,680 --> 00:02:31,920
voorbeeld een mislukking is van zo  uh,

61
00:02:31,920 --> 00:02:33,239
deze verschuiving is bijvoorbeeld bijna

62
00:02:33,239 --> 00:02:35,340
onmerkbaar voor ons en we behandelen het

63
00:02:35,340 --> 00:02:37,560
automatisch, terwijl het in het systeem

64
00:02:37,560 --> 00:02:39,959
heel duidelijk een groot probleem is, dus in woorden

65
00:02:39,959 --> 00:02:41,940
kunnen we zeggen dat structuurgeneralisatie

66
00:02:41,940 --> 00:02:44,819
een generalisatie is naar sommige

67
00:02:44,819 --> 00:02:47,040
symmetrietransformaties van de invoer of in dit

68
00:02:47,040 --> 00:02:48,959
geval de symmetrie  transformatie is een

69
00:02:48,959 --> 00:02:50,580
kleine verschuiving die de cijferklasse ongewijzigd laat,

70
00:02:50,580 --> 00:02:52,019


71
00:02:52,019 --> 00:02:54,599
dus de voor de hand liggende vraag is dan wat

72
00:02:54,599 --> 00:02:56,340
we precies bedoelen met deze natuurlijke

73
00:02:56,340 --> 00:02:58,860
structuur en waarom denken we dat dit

74
00:02:58,860 --> 00:03:01,620
ons zou helpen met deze instellingen,

75
00:03:01,620 --> 00:03:03,780
dus laten we eerst praten over wat we bedoelen

76
00:03:03,780 --> 00:03:05,819
met natuurlijke neurale structuur  structuur

77
00:03:05,819 --> 00:03:08,700
Een manier om over structuur of

78
00:03:08,700 --> 00:03:11,640
enige vorm van vooringenomenheid in een systeem te praten is een

79
00:03:11,640 --> 00:03:14,040
inductieve vooringenomenheid en dus kan een inductieve vooringenomenheid

80
00:03:14,040 --> 00:03:16,080
losjes worden gedefinieerd als een duidelijke

81
00:03:16,080 --> 00:03:17,940
beperking van een reeks realiseerbare

82
00:03:17,940 --> 00:03:19,440
hypothesen wanneer je bezig bent met

83
00:03:19,440 --> 00:03:22,440
modelselectie, meer in de volksmond  je kunt

84
00:03:22,440 --> 00:03:24,480
dit zoiets noemen als voordat je

85
00:03:24,480 --> 00:03:27,060
gegevens ziet, het is een beperking van wat en hoe

86
00:03:27,060 --> 00:03:29,580
je kunt leren, dus heel breed, dit kan

87
00:03:29,580 --> 00:03:32,519
alles omvatten, van modelklasse tot

88
00:03:32,519 --> 00:03:34,739
optimalisatieprocedures of zelfs

89
00:03:34,739 --> 00:03:37,379
hyperparameters en in zekere zin definiëren ze echt

90
00:03:37,379 --> 00:03:39,739
wat

91
00:03:39,739 --> 00:03:43,140
er mogelijk is om te leren  en het definieert

92
00:03:43,140 --> 00:03:44,819
generalisatie in die zin dat

93
00:03:44,819 --> 00:03:47,220
je eigenlijk niet buiten een trainingsset kunt generaliseren

94
00:03:47,220 --> 00:03:48,420
zonder een of andere

95
00:03:48,420 --> 00:03:50,220
inductieve oplossing te hebben. Dit wordt

96
00:03:50,220 --> 00:03:52,560
in dit artikel grondiger uitgelegd door David

97
00:03:52,560 --> 00:03:55,500
Wolford, dus wat we bedoelen met natuurlijke

98
00:03:55,500 --> 00:03:58,620
inductieve vooroordelen zijn dan vooroordelen die

99
00:03:58,620 --> 00:04:00,000
voortkomen uit de beperkingen en

100
00:04:00,000 --> 00:04:02,040
beperkingen  die worden geconfronteerd met natuurlijke

101
00:04:02,040 --> 00:04:04,500
systemen, door de aard van het moeten

102
00:04:04,500 --> 00:04:06,900
leven in de echte wereld, de

103
00:04:06,900 --> 00:04:08,459
hersenen hebben bijvoorbeeld veel efficiëntiebeperkingen

104
00:04:08,459 --> 00:04:10,439
en fysieke beperkingen door de aard van

105
00:04:10,439 --> 00:04:13,140
de constructie, en als we deze

106
00:04:13,140 --> 00:04:14,580
logica volgen, spelen deze beperkingen echt

107
00:04:14,580 --> 00:04:16,620
een rol in onze generalisatie

108
00:04:16,620 --> 00:04:19,798
capaciteiten die momenteel de moderne

109
00:04:19,798 --> 00:04:21,478
kunstmatige intelligentie overtreffen, zoals we

110
00:04:21,478 --> 00:04:24,720
hierna zullen bespreken, dus in deze lezing zal ik me

111
00:04:24,720 --> 00:04:27,540
specifiek concentreren op twee soorten structuren

112
00:04:27,540 --> 00:04:29,880
die ik in mijn werk heb bestudeerd, namelijk

113
00:04:29,880 --> 00:04:31,699
topografische organisatie en

114
00:04:31,699 --> 00:04:34,320
spatiotemporele dynamiek. Voordat ik inga

115
00:04:34,320 --> 00:04:36,120
op mijn werk,  Ik zal een kort voorbeeld geven

116
00:04:36,120 --> 00:04:38,759
waarom ik geloof dat natuurlijke structuur

117
00:04:38,759 --> 00:04:41,400
nuttig kan zijn om de structuurgeneralisatie te bereiken

118
00:04:41,400 --> 00:04:42,780
waar ik het eerder over had,

119
00:04:42,780 --> 00:04:44,280


120
00:04:44,280 --> 00:04:47,479
dus het eerste voorbeeld komt van uh

121
00:04:47,479 --> 00:04:49,199
hukushima's neocognitieve

122
00:04:49,199 --> 00:04:51,479
frontarchitectuur uit de jaren tachtig, die

123
00:04:51,479 --> 00:04:53,520
eigenlijk gebouwd was om

124
00:04:53,520 --> 00:04:55,440
direct aan te pakken  het probleem van de

125
00:04:55,440 --> 00:04:57,300
robuustheid van deze kleine verschuivingen en

126
00:04:57,300 --> 00:04:59,759
vervormingen, dus in het papierwerk

127
00:04:59,759 --> 00:05:02,040
schrijft hij over inspiratie uit de

128
00:05:02,040 --> 00:05:04,380
metingen van hiërarchie en

129
00:05:04,380 --> 00:05:06,780
pooling door leerlingen en wezels om robuustheid

130
00:05:06,780 --> 00:05:08,699
van deze vervormingen te bereiken en dus als je

131
00:05:08,699 --> 00:05:11,160
naar de figuur kijkt, als hij U sub S1 U schrijft

132
00:05:11,160 --> 00:05:14,100
sub C1 en deze staan ​​voor eenvoudige en

133
00:05:14,100 --> 00:05:16,919
complexe cellen en dus is dit destijds een vrij

134
00:05:16,919 --> 00:05:18,840
radicale aanpak, maar het heeft

135
00:05:18,840 --> 00:05:20,699
echt bijgedragen aan het verbeteren van de robuustheid en de

136
00:05:20,699 --> 00:05:22,199
verschuivingen waarmee we deze vroege

137
00:05:22,199 --> 00:05:24,419
kunstmatige neurale netwerken teisteren en in de loop van de tijd

138
00:05:24,419 --> 00:05:25,860
werden deze ideeën vereenvoudigd en

139
00:05:25,860 --> 00:05:28,919
geabstraheerd en  heeft duidelijk de

140
00:05:28,919 --> 00:05:30,539
convolutionele neurale netwerken opgeleverd, zoals we

141
00:05:30,539 --> 00:05:32,759
vandaag de dag weten, die uiteindelijk het succes

142
00:05:32,759 --> 00:05:35,580
van de deep learning-revolutie aandreven. Dit

143
00:05:35,580 --> 00:05:36,960
is dus echt een voorbeeld van een natuurlijke

144
00:05:36,960 --> 00:05:39,479
inductieve bias die structuurgeneralisatie tot stand heeft gebracht,

145
00:05:39,479 --> 00:05:42,120
dus voor ons onderzoek is het

146
00:05:42,120 --> 00:05:43,919
echt van het grootste belang om te proberen te

147
00:05:43,919 --> 00:05:45,900
begrijpen wat deze maakt  modellen werken

148
00:05:45,900 --> 00:05:47,280
zo goed,

149
00:05:47,280 --> 00:05:49,320
en kijk of dit principe

150
00:05:49,320 --> 00:05:51,360
potentieel kan worden gegeneraliseerd om meer

151
00:05:51,360 --> 00:05:53,940
abstracte, meer abstracte transformaties

152
00:05:53,940 --> 00:05:57,000
en symmetrieën te dekken,

153
00:05:57,000 --> 00:06:00,360
dus wat zorgt ervoor dat een convolutie deze

154
00:06:00,360 --> 00:06:02,060
structuur-generalisatie

155
00:06:02,060 --> 00:06:04,440
intuïtief bereikt, je kunt zien dat dit wordt gedaan door

156
00:06:04,440 --> 00:06:06,720
hetzelfde filter toe te passen op of of

157
00:06:06,720 --> 00:06:08,699
feature extractor op  verschillende ruimtelijke

158
00:06:08,699 --> 00:06:10,680
locaties, dus hier zien we dat een enkel

159
00:06:10,680 --> 00:06:12,660
convolutioneel filter wordt toegepast op

160
00:06:12,660 --> 00:06:14,820
alle locaties van een afbeelding. Dit betekent

161
00:06:14,820 --> 00:06:16,259
dat het niet uitmaakt waar uw invoer zich bevindt, of

162
00:06:16,259 --> 00:06:18,000
deze zich nu in het midden van

163
00:06:18,000 --> 00:06:20,400
de afbeelding bevindt of aan de rechterkant, u

164
00:06:20,400 --> 00:06:22,080
exact dezelfde kenmerken heeft  op één

165
00:06:22,080 --> 00:06:23,580
uitzondering na zullen ze gelijkwaardig worden

166
00:06:23,580 --> 00:06:24,660
verschoven,

167
00:06:24,660 --> 00:06:26,819
dus wiskundig gezien wordt dit type afbeelding

168
00:06:26,819 --> 00:06:29,160
een homomorfisme genoemd, het behoudt

169
00:06:29,160 --> 00:06:30,840
de algebraïsche structuur van de

170
00:06:30,840 --> 00:06:33,180
invoerruimte en de uitvoerruimte. In dit geval

171
00:06:33,180 --> 00:06:35,580
is het met betrekking tot vertaling en op

172
00:06:35,580 --> 00:06:37,440
een eenvoudig niveau zoiets als

173
00:06:37,440 --> 00:06:38,880
wat belangrijk zal zijn om te onthouden voor

174
00:06:38,880 --> 00:06:40,259
de rest van deze lezing is dat we

175
00:06:40,259 --> 00:06:42,300
homomorfismen van onze feature

176
00:06:42,300 --> 00:06:45,000
extractor kunnen verifiëren als we kunnen zien dat er sprake is van

177
00:06:45,000 --> 00:06:46,740
gemeenschapscommutatie met het

178
00:06:46,740 --> 00:06:49,560
Transformations commutatieve diagram en

179
00:06:49,560 --> 00:06:51,720
dus kunnen we dit ook algebraïsch schrijven

180
00:06:51,720 --> 00:06:53,759
door aan te tonen dat de feature extractor  f

181
00:06:53,759 --> 00:06:55,080
pendelt met de transformatie-

182
00:06:55,080 --> 00:06:57,000
operator t

183
00:06:57,000 --> 00:06:58,919
en wat we eigenlijk willen is dat er

184
00:06:58,919 --> 00:07:00,600
geen verschil is tussen eerst

185
00:07:00,600 --> 00:07:02,639
de kenmerken extraheren en vervolgens

186
00:07:02,639 --> 00:07:04,740
de transformatie uitvoeren of

187
00:07:04,740 --> 00:07:06,240
de transformatie uitvoeren en vervolgens

188
00:07:06,240 --> 00:07:08,639
de kenmerken extraheren, dus de uitdaging

189
00:07:08,639 --> 00:07:10,199
is tot nu toe dat

190
00:07:10,199 --> 00:07:11,880
we het niet echt weten  hoe homomorfismen te construeren

191
00:07:11,880 --> 00:07:13,620
met betrekking tot

192
00:07:13,620 --> 00:07:15,240
complexere transformaties die we in

193
00:07:15,240 --> 00:07:18,060
de echte wereld zien, onze hersenen zijn bijvoorbeeld

194
00:07:18,060 --> 00:07:20,099
in staat om veranderingen in verlichting en

195
00:07:20,099 --> 00:07:22,259
seizoenen op natuurlijke wijze aan te pakken,

196
00:07:22,259 --> 00:07:24,599
dus hier zien we verlichting op iemands

197
00:07:24,599 --> 00:07:26,160
gezicht of de wisseling van seizoenen waarvan we kunnen

198
00:07:26,160 --> 00:07:27,840
zien dat het zo is  hetzelfde gezicht of dezelfde

199
00:07:27,840 --> 00:07:29,880
weg, maar we weten niet hoe we modellen moeten bouwen

200
00:07:29,880 --> 00:07:31,319
die deze

201
00:07:31,319 --> 00:07:33,180
transformaties respecteren en dus maakt het ons moeilijk

202
00:07:33,180 --> 00:07:35,520
om systemen te bouwen die ze op een

203
00:07:35,520 --> 00:07:37,620
robuuste en voorspelbare manier verwerken

204
00:07:37,620 --> 00:07:40,259
om een ​​nog abstracter voorbeeld te geven van

205
00:07:40,259 --> 00:07:41,759
wat ik  bedoel ik hiermee en de mogelijke

206
00:07:41,759 --> 00:07:43,620
negatieve gevolgen van modellen die

207
00:07:43,620 --> 00:07:45,440
niet met symmetrie omgaan. Transformaties

208
00:07:45,440 --> 00:07:48,060
houden rekening met moderne programma's voor het genereren van tekst naar afbeeldingen,

209
00:07:48,060 --> 00:07:50,520
dus in dit voorbeeld vroeg ik

210
00:07:50,520 --> 00:07:53,940
Dolly om een ​​afbeelding van een

211
00:07:53,940 --> 00:07:55,620
teddybeer op de maan te genereren en dat doet het

212
00:07:55,620 --> 00:07:57,180
ongelooflijk goed.  waarschijnlijk beter

213
00:07:57,180 --> 00:08:00,960
dan ik zou kunnen, het heeft textuur, uh

214
00:08:00,960 --> 00:08:03,360
ongelooflijk gedetailleerd, maar als ik je vraag

215
00:08:03,360 --> 00:08:05,340
iets te doen waarvan ik zie dat het

216
00:08:05,340 --> 00:08:08,039
conceptueel eenvoudiger is, zoals het tekenen van een blauwe

217
00:08:08,039 --> 00:08:10,560
kubus bovenop een rode kubus, lukt

218
00:08:10,560 --> 00:08:13,380
dit niet en voor mij lijkt dit niet intuïtief,

219
00:08:13,380 --> 00:08:15,300
aangezien  de tweede taak lijkt

220
00:08:15,300 --> 00:08:18,180
aanzienlijk eenvoudiger, maar wat ik

221
00:08:18,180 --> 00:08:19,860
beweer is dat de reden dat dit

222
00:08:19,860 --> 00:08:21,599
verrassend is precies dezelfde reden is

223
00:08:21,599 --> 00:08:23,580
waarom het voorbeeld van de amnestvertaling verrassend was.

224
00:08:23,580 --> 00:08:25,560


225
00:08:25,560 --> 00:08:28,020
Hier vindt deze symmetrietransformatie plaats, namelijk de

226
00:08:28,020 --> 00:08:29,400
transformatie tussen deze complexe

227
00:08:29,400 --> 00:08:31,740
objecten van een teddybeer  en de maan en

228
00:08:31,740 --> 00:08:34,320
deze eenvoudige kubusobjecten waarvan we

229
00:08:34,320 --> 00:08:36,360
intuïtief verwachten dat het netwerk deze

230
00:08:36,360 --> 00:08:38,820
kan hanteren en respecteren, en we zien

231
00:08:38,820 --> 00:08:40,919
dat het niet zo precies is als hoe

232
00:08:40,919 --> 00:08:43,380
Fukushima's werk aantoonde dat deze

233
00:08:43,380 --> 00:08:46,200
natuurlijke structuur van hiërarchie en

234
00:08:46,200 --> 00:08:47,700
samenvoeging van ons visuele systeem

235
00:08:47,700 --> 00:08:49,680
effectief voor het maken van generalisaties naar

236
00:08:49,680 --> 00:08:52,380
kleine transformaties. Ik beweer dat een

237
00:08:52,380 --> 00:08:54,060
potentieel hogere structuur

238
00:08:54,060 --> 00:08:55,740
nodig kan zijn om deze abstracte generalisatieproblemen op te lossen.

239
00:08:55,740 --> 00:08:58,200


240
00:08:58,200 --> 00:09:01,380
De vraag die ik

241
00:09:01,380 --> 00:09:04,380
bestudeer en die ik stel is dus: wat

242
00:09:04,380 --> 00:09:06,060
zou deze structuur kunnen zijn en hoe kunnen we deze

243
00:09:06,060 --> 00:09:08,640
implementeren?  dit in een kunstmatige neurale

244
00:09:08,640 --> 00:09:10,080
netwerkarchitectuur die feitelijk kan worden

245
00:09:10,080 --> 00:09:14,120
gebruikt voor het uitvoeren van berekeningen,

246
00:09:14,880 --> 00:09:17,700
dus om te beginnen met antwoorden dat ik

247
00:09:17,700 --> 00:09:19,680
in mijn eerste werk over

248
00:09:19,680 --> 00:09:22,380
topografische organisatie zal springen,

249
00:09:22,380 --> 00:09:25,260
zodat topografische organisatie

250
00:09:25,260 --> 00:09:27,060
breed door de hersenen wordt waargenomen vanuit de primaire

251
00:09:27,060 --> 00:09:29,760
visuele cortex. Saffierniveaugebieden  en

252
00:09:29,760 --> 00:09:31,500
het kan heel losjes worden omschreven als deze

253
00:09:31,500 --> 00:09:33,540
eigenschap dat neuronen die dicht bij

254
00:09:33,540 --> 00:09:35,760
elkaar staan ​​de neiging hebben om op vergelijkbare

255
00:09:35,760 --> 00:09:38,220
dingen te reageren. Aan de linkerkant laten we bijvoorbeeld

256
00:09:38,220 --> 00:09:39,720
de kleurgecodeerde voorkeur zien van elk

257
00:09:39,720 --> 00:09:42,959
neuron in de primaire digitale cortex als

258
00:09:42,959 --> 00:09:45,360
reactie op georiënteerde lijnen  en we zien

259
00:09:45,360 --> 00:09:46,740
deze soepel variërende reeks

260
00:09:46,740 --> 00:09:48,779
selectiviteiten. Een ander type

261
00:09:48,779 --> 00:09:50,580
organisatie staat bekend als de retina-topische

262
00:09:50,580 --> 00:09:52,560
organisatie, waarbij nabijgelegen neuronen in de

263
00:09:52,560 --> 00:09:54,600
visuele cortex de neiging hebben te reageren op nabijgelegen

264
00:09:54,600 --> 00:09:56,399
receptieve velden,

265
00:09:56,399 --> 00:09:58,560
maar deze organisatie is niet beperkt

266
00:09:58,560 --> 00:10:01,080
tot deze kenmerken op laag niveau.

267
00:10:01,080 --> 00:10:02,519
kenmerken zoals die

268
00:10:02,519 --> 00:10:05,459
aanwezig zijn in gezichten of objecten of plaatsen

269
00:10:05,459 --> 00:10:07,920
en dit heeft betrekking op de zogenaamde

270
00:10:07,920 --> 00:10:10,080
functioneel specifieke gebieden van de hersenen

271
00:10:10,080 --> 00:10:12,779
zoals het fusiforme gezichtsgebied FFA en

272
00:10:12,779 --> 00:10:15,420
het parakhippocampale gezichtsgebied PPA,

273
00:10:15,420 --> 00:10:19,200
dus in dit werk is het hoofdidee opnieuw

274
00:10:19,200 --> 00:10:21,300
dat dit misschien  topografische

275
00:10:21,300 --> 00:10:23,580
organisatie in zekere zin die

276
00:10:23,580 --> 00:10:25,080
nauw verwant is aan de

277
00:10:25,080 --> 00:10:27,980
convolutieoperatie en de architectuur van Fukushima.

278
00:10:27,980 --> 00:10:30,660
We kunnen de voordelen hiervan misschien generaliseren

279
00:10:30,660 --> 00:10:33,420
naar meer abstracte transformaties, met

280
00:10:33,420 --> 00:10:34,920
andere woorden, leren hoe we

281
00:10:34,920 --> 00:10:36,839
complexere homomorfismen kunnen bouwen die we niet kunnen.

282
00:10:36,839 --> 00:10:38,940
Weet je, dat kunnen we niet.  doe nu analytisch,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
dus alleen maar om te laten zien dat we niet

285
00:10:42,480 --> 00:10:44,760
helemaal gek zijn met dit idee.

286
00:10:44,760 --> 00:10:46,320
Er is eerder werk op dit gebied

287
00:10:46,320 --> 00:10:49,880
van mensen zoals Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden in de vroege jaren 90 en 2000

289
00:10:54,060 --> 00:10:55,800
en zij bestudeerden hoe topografische

290
00:10:55,800 --> 00:10:57,720
organisatie zou kunnen zijn  nuttig voor het leren

291
00:10:57,720 --> 00:11:01,320
van varianties, meestal in lineaire modellen, dus

292
00:11:01,320 --> 00:11:03,060
de vraag voor ons toen we de

293
00:11:03,060 --> 00:11:04,680
ruimte betraden, was wat het meest schaalbare

294
00:11:04,680 --> 00:11:07,140
abstracte mechanisme is dat kan worden gebruikt

295
00:11:07,140 --> 00:11:08,880
vanuit deze benaderingen die we kunnen

296
00:11:08,880 --> 00:11:10,800
integreren in moderne diepe neurale

297
00:11:10,800 --> 00:11:12,959
netwerkarchitecturen en uiteindelijk

298
00:11:12,959 --> 00:11:15,000
kwamen we tot een  generatieve

299
00:11:15,000 --> 00:11:16,260
modelleringsbenadering waarvan ik denk dat deze interessant zou kunnen zijn

300
00:11:16,260 --> 00:11:17,519
voor de mensen in deze

301
00:11:17,519 --> 00:11:18,779
gemeenschap,

302
00:11:18,779 --> 00:11:21,779
waardoor we deze vervolgens

303
00:11:21,779 --> 00:11:23,579
nauwer kunnen relateren aan de analyse van topografische onafhankelijke

304
00:11:23,579 --> 00:11:26,040
componenten, met als basisidee

305
00:11:26,040 --> 00:11:28,320
dat we een topografisch

306
00:11:28,320 --> 00:11:30,660
kenmerk van de ruimte kunnen leren door een topografische

307
00:11:30,660 --> 00:11:32,940
voorafgaande verdeling over de ruimte op te leggen.  onze latente

308
00:11:32,940 --> 00:11:34,440
variabelen,

309
00:11:34,440 --> 00:11:37,079
dus om een ​​korte achtergrond te geven. Ik

310
00:11:37,079 --> 00:11:39,120
neem aan dat de meeste mensen hier al bekend mee zijn,

311
00:11:39,120 --> 00:11:40,440


312
00:11:40,440 --> 00:11:42,540
maar het soort algemene aanname is

313
00:11:42,540 --> 00:11:44,339
dat de hersenen een generatief model zijn en dat

314
00:11:44,339 --> 00:11:45,720
dit idee in zekere zin kan worden

315
00:11:45,720 --> 00:11:48,060
toegeschreven aan helmholten uit de 19e

316
00:11:48,060 --> 00:11:50,459
eeuw.  waar hij zei dat wat we

317
00:11:50,459 --> 00:11:52,140
zien de oplossing is voor een

318
00:11:52,140 --> 00:11:54,420
computerprobleem. Onze hersenen berekenen de meest

319
00:11:54,420 --> 00:11:56,519
waarschijnlijke oorzaken van fotonabsorpties

320
00:11:56,519 --> 00:11:59,519
in onze ogen en dat is dus een voorbeeld.

321
00:11:59,519 --> 00:12:01,920
Als ik je dit beeld laat zien,

322
00:12:01,920 --> 00:12:03,720
herken je het onmiddellijk als een bol met enige

323
00:12:03,720 --> 00:12:05,760
kromming, hoe dan ook  kan net zo

324
00:12:05,760 --> 00:12:07,620
goed een schijf zijn met een vervormd

325
00:12:07,620 --> 00:12:09,600
perspectief daarop, dus dit is hoe we

326
00:12:09,600 --> 00:12:12,660
optische illusies of onze beelden krijgen, dus

327
00:12:12,660 --> 00:12:14,880
zoals deze concluderen je hersenen dat

328
00:12:14,880 --> 00:12:17,100
er hier een kubus is vanwege de

329
00:12:17,100 --> 00:12:18,720
structuur, maar eigenlijk is het gewoon een plat

330
00:12:18,720 --> 00:12:19,860
stuk papier

331
00:12:19,860 --> 00:12:22,740
je kunt dus denken aan dit generatieve

332
00:12:22,740 --> 00:12:24,480
modelaspect dat lijkt op een

333
00:12:24,480 --> 00:12:26,160
omgekeerd grafisch programma

334
00:12:26,160 --> 00:12:28,140
in het programma. De abstracte eigenschappen

335
00:12:28,140 --> 00:12:30,660
van de bol zijn bekend, de positie, de

336
00:12:30,660 --> 00:12:32,820
grootte van de verlichting en deze worden gebruikt om

337
00:12:32,820 --> 00:12:34,560
de bol te projecteren om het 2D-

338
00:12:34,560 --> 00:12:37,440
beeld te creëren dat wordt weergegeven.  Dus wat

339
00:12:37,440 --> 00:12:40,019
Humboldt en anderen in feite zeggen is dat

340
00:12:40,019 --> 00:12:41,940
de hersenen als generatief model

341
00:12:41,940 --> 00:12:43,680
feitelijk proberen dit

342
00:12:43,680 --> 00:12:45,959
generatieve proces om te keren en gevolgtrekkingen te maken

343
00:12:45,959 --> 00:12:48,300
en de onderliggende oorzaken van onze sensaties af te leiden,

344
00:12:48,300 --> 00:12:49,680


345
00:12:49,680 --> 00:12:51,600
dus de reden dat ik

346
00:12:51,600 --> 00:12:53,100
dit punt enigszins uitwerk, is dat er  er wordt

347
00:12:53,100 --> 00:12:55,440
tegenwoordig veel over generatieve modellen gesproken,

348
00:12:55,440 --> 00:12:57,180
en ik heb het niet noodzakelijkerwijs alleen maar

349
00:12:57,180 --> 00:12:59,639
over het genereren van afbeeldingen of mooie

350
00:12:59,639 --> 00:13:01,260
plaatjes.

351
00:13:01,260 --> 00:13:03,120
Ik wil echt

352
00:13:03,120 --> 00:13:07,920
een raamwerk bedoelen voor leren zonder toezicht,

353
00:13:07,920 --> 00:13:10,019
dus om wat meer in

354
00:13:10,019 --> 00:13:11,700
de details te komen, wat bedoel ik met een

355
00:13:11,700 --> 00:13:14,279
topografische eerdere dus generatieve modellen

356
00:13:14,279 --> 00:13:16,019
worden doorgaans beschreven als een gezamenlijke

357
00:13:16,019 --> 00:13:18,720
verdeling over observaties X en

358
00:13:18,720 --> 00:13:21,720
latente variabelen die we Z uh zullen noemen

359
00:13:21,720 --> 00:13:23,940
en dit wordt doorgaans gefactoriseerd of een

360
00:13:23,940 --> 00:13:25,620
manier waarop dit wordt gedaan is gefactoriseerd in

361
00:13:25,620 --> 00:13:28,440
termen van een eerdere P van Z en dit is waar

362
00:13:28,440 --> 00:13:30,420
generatief model conditioneel generatief

363
00:13:30,420 --> 00:13:33,420
model P van x gegeven Z en een manier waarop

364
00:13:33,420 --> 00:13:34,980
we hierover kunnen nadenken is dat de

365
00:13:34,980 --> 00:13:37,019
prior relatieve

366
00:13:37,019 --> 00:13:38,880
straffen codeert voor elk type code dat wordt

367
00:13:38,880 --> 00:13:41,279
geproduceerd wanneer we ons generatieve

368
00:13:41,279 --> 00:13:42,899
model omkeren. Dit wordt het berekenen van de

369
00:13:42,899 --> 00:13:45,920
posterior P van Z gegeven X

370
00:13:45,920 --> 00:13:49,139
en om een ​​topografische latente

371
00:13:49,139 --> 00:13:50,639
ruimte te ontwikkelen willen we een soort

372
00:13:50,639 --> 00:13:53,279
topografische prior introduceren die is geweest of

373
00:13:53,279 --> 00:13:55,620
waarvan dit topografische ICA-werk heeft aangetoond dat deze

374
00:13:55,620 --> 00:13:57,720
equivalent is aan zoiets als een

375
00:13:57,720 --> 00:13:59,700
groepssparsity penalty,

376
00:13:59,700 --> 00:14:01,500
zodat mensen bekend kunnen zijn met typische

377
00:14:01,500 --> 00:14:03,180
spaarzaamheidsboetes van onafhankelijke

378
00:14:03,180 --> 00:14:04,560
leeranalyse u wilt dat uw

379
00:14:04,560 --> 00:14:06,540
activeringen schaars zijn, wat betekent dat veel

380
00:14:06,540 --> 00:14:09,420
ervan nul zijn, wauw en dat zou er

381
00:14:09,420 --> 00:14:10,680
ongeveer zo uit kunnen zien, u heeft een aantal

382
00:14:10,680 --> 00:14:12,300
blauwe vierkanten die actief zijn, maar de meeste

383
00:14:12,300 --> 00:14:14,459
zijn niet actief, maar specifiek

384
00:14:14,459 --> 00:14:16,740
met die van de groep  Varsity-straf: we willen dat

385
00:14:16,740 --> 00:14:18,839
deze priors een lagere waarschijnlijkheid toekennen

386
00:14:18,839 --> 00:14:21,600
aan deze gedistribueerde schaarse activeringen

387
00:14:21,600 --> 00:14:24,720
en een hogere waarschijnlijkheid aan deze gegroepeerde,

388
00:14:24,720 --> 00:14:26,940
dicht opeengepakte representaties. Je kunt

389
00:14:26,940 --> 00:14:28,860
dit ook zien als een hogere straf

390
00:14:28,860 --> 00:14:30,720
als dingen verspreid zijn, een lagere

391
00:14:30,720 --> 00:14:33,540
straf als dingen dichter bij elkaar zijn,

392
00:14:33,540 --> 00:14:36,380
dus nogmaals uh  dit kan

393
00:14:36,380 --> 00:14:39,060
abstract zo worden geschreven, maar ik wil

394
00:14:39,060 --> 00:14:41,160
theorie maken dat deze neuronen, elk van

395
00:14:41,160 --> 00:14:42,779
deze vierkanten hier een soort

396
00:14:42,779 --> 00:14:44,220
neuron vertegenwoordigt in ons model en dat ze zijn

397
00:14:44,220 --> 00:14:46,560
georganiseerd in dit 2D-raster, dus als we het

398
00:14:46,560 --> 00:14:48,120
over groeperen hebben, bedoelen we echt

399
00:14:48,120 --> 00:14:50,820
groeperen in die 2D-topologie,

400
00:14:50,820 --> 00:14:53,100
dus iets dat heel interessant

401
00:14:53,100 --> 00:14:55,560
en nogal belangrijk is, is dat deze

402
00:14:55,560 --> 00:14:57,779
priors ons niet alleen een topografische

403
00:14:57,779 --> 00:15:00,540
organisatie geven, maar dat ze ook zijn opgemerkt

404
00:15:00,540 --> 00:15:02,459
door mensen zoals of bestudeerd door mensen als

405
00:15:02,459 --> 00:15:05,760
erosi Marcelli en Bruno, zodat ze ook

406
00:15:05,760 --> 00:15:07,740
daadwerkelijk passen  de statistieken van natuurlijke

407
00:15:07,740 --> 00:15:08,959
gegevens, beter

408
00:15:08,959 --> 00:15:11,180
specifiek natuurlijke beelden,

409
00:15:11,180 --> 00:15:14,100
ze hebben aangetoond dat als je dit type a

410
00:15:14,100 --> 00:15:16,139
prior gebruikt, je feitelijk een schaarser aantal

411
00:15:16,139 --> 00:15:18,839
activeringen krijgt, wat betekent dat de prior

412
00:15:18,839 --> 00:15:20,459
een beetje beter aansluit bij het echte generatieve proces

413
00:15:20,459 --> 00:15:22,620
en zoals we weten, hebben de hersenen dat ook gedaan

414
00:15:22,620 --> 00:15:24,779
een hoge mate van schaarsheid en er wordt

415
00:15:24,779 --> 00:15:26,339
aangenomen dat dit zeer relevant is voor de

416
00:15:26,339 --> 00:15:28,620
efficiëntie,

417
00:15:28,620 --> 00:15:30,839
dus om wat meer in de

418
00:15:30,839 --> 00:15:32,760
details te komen om dit type

419
00:15:32,760 --> 00:15:35,160
groep te implementeren, gebruiken we een hiërarchisch

420
00:15:35,160 --> 00:15:37,320
generatief model en dit wordt

421
00:15:37,320 --> 00:15:39,060
feitelijk geïntroduceerd door enkele van de

422
00:15:39,060 --> 00:15:41,339
topografisch ICA-werk,

423
00:15:41,339 --> 00:15:43,320
het idee is dat je een

424
00:15:43,320 --> 00:15:45,000
latente variabele U op een hoger niveau hebt die

425
00:15:45,000 --> 00:15:47,820
tegelijkertijd de variantie van

426
00:15:47,820 --> 00:15:50,279
meerdere variabelen op een lager niveau T reguleert en

427
00:15:50,279 --> 00:15:52,440
dit is hoe we groepssparsiteit krijgen.

428
00:15:52,440 --> 00:15:55,440
Om een ​​topografische organisatie te krijgen,

429
00:15:55,440 --> 00:15:56,760
kun je meerdere van deze latente

430
00:15:56,760 --> 00:15:59,339
variabelen enigszins gebruiken  overlappend met

431
00:15:59,339 --> 00:16:02,519
hun invloedsvelden, zodat

432
00:16:02,519 --> 00:16:04,260
we ze hun buurten kunnen noemen

433
00:16:04,260 --> 00:16:05,699
en dit geeft je deze soepele

434
00:16:05,699 --> 00:16:07,440
correlatiestructuur waar je naar op zoek bent,

435
00:16:07,440 --> 00:16:09,899
dus zorg dat je de intuïtie hiervoor hebt. Je

436
00:16:09,899 --> 00:16:12,060
ziet dat deze variabele T helemaal

437
00:16:12,060 --> 00:16:14,279
onderaan hier niet klopt  elke invoer

438
00:16:14,279 --> 00:16:17,160
van deze U bovenaan, maar het deelt

439
00:16:17,160 --> 00:16:19,139
een u-variabele met deze T in het midden,

440
00:16:19,139 --> 00:16:21,300
dus het is alsof ze varianten delen,

441
00:16:21,300 --> 00:16:22,980
ze delen sommige componenten met

442
00:16:22,980 --> 00:16:24,959
hun buren, maar niet alle componenten

443
00:16:24,959 --> 00:16:26,579
en dat komt echt door deze lokale

444
00:16:26,579 --> 00:16:28,079
connectiviteit van  deze variabelen op een hoger niveau,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
dus om het simpel te houden over hoe we een

447
00:16:33,180 --> 00:16:34,980
generatief model gebruiken, laten we teruggaan naar een

448
00:16:34,980 --> 00:16:37,440
enkele U-variabele en de uitdaging in

449
00:16:37,440 --> 00:16:38,940
dit type architectuur dat

450
00:16:38,940 --> 00:16:42,360
het jarenlang moeilijk maakte, is hoe

451
00:16:42,360 --> 00:16:44,579
je de geschatte posterieure waarde afleidt

452
00:16:44,579 --> 00:16:47,579
over deze tussenvariabelen in

453
00:16:47,579 --> 00:16:50,100
deze hiërarchische architectuur en dit

454
00:16:50,100 --> 00:16:52,560
is niet super eenvoudig, dus in eerdere

455
00:16:52,560 --> 00:16:54,420
werken is gebruik gemaakt van heuristieken die zijn ontwikkeld voor

456
00:16:54,420 --> 00:16:56,699
lineaire modellen en in ons werk hebben we ontdekt

457
00:16:56,699 --> 00:16:58,680
dat dit zich niet echt uitstrekte tot moderne

458
00:16:58,680 --> 00:17:01,199
neurale netwerkarchitecturen, dus eigenlijk

459
00:17:01,199 --> 00:17:02,880
is ons inzicht het benutten van een

460
00:17:02,880 --> 00:17:04,760
factorisatie een specifieke

461
00:17:04,760 --> 00:17:07,640
herparametrisatie van deze verdeling

462
00:17:07,640 --> 00:17:10,380
en dus wordt deze parametrisering

463
00:17:10,380 --> 00:17:12,419
specifiek bereikt door de

464
00:17:12,419 --> 00:17:14,579
prioriteit te definiëren die bekend staat als een gaussiaanse

465
00:17:14,579 --> 00:17:16,319
schaalmengsel, wat betekent dat onze

466
00:17:16,319 --> 00:17:19,140
voorwaardelijke verdeling van T gegeven U

467
00:17:19,140 --> 00:17:21,179
feitelijk een normale verdeling is waarbij

468
00:17:21,179 --> 00:17:24,299
de variantie wordt gedefinieerd door deze variabele

469
00:17:24,299 --> 00:17:27,720
U en voor bepaalde keuzes van U

470
00:17:27,720 --> 00:17:29,340
is deze distributie inderdaad schaars en

471
00:17:29,340 --> 00:17:31,980
omvat deze een reeks distributies,

472
00:17:31,980 --> 00:17:33,780
zoals laplossianen, een reeks en T-

473
00:17:33,780 --> 00:17:36,299
distributies. Een manier om het te definiëren is

474
00:17:36,299 --> 00:17:38,940
een mengsel op gaussiaanse schaal dat een

475
00:17:38,940 --> 00:17:40,919
bepaalde reframe-herparameterisatie uitzendt

476
00:17:40,919 --> 00:17:42,900
in termen van onafhankelijke gaussiaanse willekeurige

477
00:17:42,900 --> 00:17:45,720
variabelen  Z en U, dus specifiek, dan

478
00:17:45,720 --> 00:17:48,840
zien we dat deze T-variabele, die

479
00:17:48,840 --> 00:17:50,760
oorspronkelijk tamelijk complex was, eigenlijk

480
00:17:50,760 --> 00:17:52,799
slechts een product is van een stel Gaussiaanse

481
00:17:52,799 --> 00:17:54,840
willekeurige variabelen die nu weten hoe ze

482
00:17:54,840 --> 00:17:57,660
veel efficiënter moeten werken, in

483
00:17:57,660 --> 00:18:00,120
generatieve modellen, specifiek wat

484
00:18:00,120 --> 00:18:02,039
we zijn  wat we gaan doen is dat we

485
00:18:02,039 --> 00:18:04,020
feitelijk de posteriors voor

486
00:18:04,020 --> 00:18:06,720
U en Z afzonderlijk kunnen berekenen en dan een

487
00:18:06,720 --> 00:18:08,640
deterministische combinatie ervan kunnen maken

488
00:18:08,640 --> 00:18:10,020
om onze topografische

489
00:18:10,020 --> 00:18:13,140
variabele T te berekenen. Dit is veel gemakkelijker om te doen

490
00:18:13,140 --> 00:18:15,500
zonder al te veel in details te treden over

491
00:18:15,500 --> 00:18:17,700
de methode die  we besloten te gebruiken is

492
00:18:17,700 --> 00:18:18,600
wat bekend staat als een variatie-

493
00:18:18,600 --> 00:18:20,640
autoencoder die technieken

494
00:18:20,640 --> 00:18:23,220
uit variatie-inferentie gebruikt om een

495
00:18:23,220 --> 00:18:24,780
ondergrens voor de waarschijnlijkheid af te leiden, waardoor

496
00:18:24,780 --> 00:18:26,940
we deze geschatte

497
00:18:26,940 --> 00:18:29,400
posteriors kunnen parametriseren met krachtige niet-lineaire diepe

498
00:18:29,400 --> 00:18:30,960
neurale netwerken en ze kunnen optimaliseren met

499
00:18:30,960 --> 00:18:33,240
gradiëntafdaling.  Het zal

500
00:18:33,240 --> 00:18:34,440
bekend zijn bij de actieve

501
00:18:34,440 --> 00:18:36,900
inferentiegemeenschap, maar wat we eigenlijk hebben gedaan is dat

502
00:18:36,900 --> 00:18:38,760


503
00:18:38,760 --> 00:18:41,640
we in plaats van een enkele encoder in de decoder te hebben, zoals een typische baes, nu

504
00:18:41,640 --> 00:18:43,799
twee encoders hebben, één voor jou en één voor Z

505
00:18:43,799 --> 00:18:46,200
afzonderlijk, en dan combineren we ze op

506
00:18:46,200 --> 00:18:48,419
deze deterministische manier om  construeer

507
00:18:48,419 --> 00:18:51,660
onze topografische T-variabele als je ziet

508
00:18:51,660 --> 00:18:53,100
dat dit eigenlijk de

509
00:18:53,100 --> 00:18:54,900
constructie is van de T-verdeling van een student

510
00:18:54,900 --> 00:18:57,620
uit gaussianen

511
00:18:57,620 --> 00:19:00,480
en dan kunnen we dit pluggen, we doen dit

512
00:19:00,480 --> 00:19:03,600
vóór het decoderen en uh en maximaliseren dan

513
00:19:03,600 --> 00:19:05,820
de waarschijnlijkheid van de gegevens helemaal, dus

514
00:19:05,820 --> 00:19:07,260
dit is de elleboog  de ondergrens van het bewijsmateriaal over

515
00:19:07,260 --> 00:19:09,360
de waarschijnlijkheid van de

516
00:19:09,360 --> 00:19:12,600
gegevens is overvloedig aanwezig en lijkt in feite erg op de

517
00:19:12,600 --> 00:19:15,720
variatievrije energie die wordt gebruikt in

518
00:19:15,720 --> 00:19:18,299
actieve entreegemeenschappen,

519
00:19:18,299 --> 00:19:20,520
dus met deze details uit de

520
00:19:20,520 --> 00:19:22,500
weg geruimd, wat echt interessant is, is wat er

521
00:19:22,500 --> 00:19:23,940
gebeurt als we dit generatieve

522
00:19:23,940 --> 00:19:26,580
model trainen dat  heeft een relatief eenvoudige

523
00:19:26,580 --> 00:19:29,460
groepssparsiteitstraf in zijn latente ruimte en

524
00:19:29,460 --> 00:19:30,720
we willen kijken naar wat het

525
00:19:30,720 --> 00:19:32,700
leert in termen van de organisatie van

526
00:19:32,700 --> 00:19:34,980
Futures en eerst beginnen we met de

527
00:19:34,980 --> 00:19:36,480
eenvoudigst mogelijke dataset. We hebben een

528
00:19:36,480 --> 00:19:38,580
zwarte achtergrond met witte vierkanten op

529
00:19:38,580 --> 00:19:41,280
willekeurige XY-locaties  en als we onze

530
00:19:41,280 --> 00:19:42,780
autoencoder trainen met deze groepssparsity-

531
00:19:42,780 --> 00:19:44,760
straf erop en dan kijken naar de

532
00:19:44,760 --> 00:19:47,640
gewichtsvectoren van onze decoder die

533
00:19:47,640 --> 00:19:49,020
we hier in blauw uitzetten, opnieuw

534
00:19:49,020 --> 00:19:52,520
georganiseerd op dit 2D-raster, zien we dat

535
00:19:52,520 --> 00:19:54,780
ze inderdaad leren georganiseerd te zijn

536
00:19:54,780 --> 00:19:57,539
volgens ruimtelijke  locatie, zodat dit

537
00:19:57,539 --> 00:19:59,580
kan worden gezien als vergelijkbaar met convolutionele

538
00:19:59,580 --> 00:20:01,799
receptieve velden, of het receptieve veld

539
00:20:01,799 --> 00:20:04,679
van elk neuron wordt in werkelijkheid bepaald door het

540
00:20:04,679 --> 00:20:09,059
soort input op zijn locatie

541
00:20:09,059 --> 00:20:10,860
en dit is intuïtief logisch vanuit

542
00:20:10,860 --> 00:20:13,140
het spaarzaamheidsperspectief van de groep, aangezien

543
00:20:13,140 --> 00:20:15,480
voor een bepaald gebied de nadruk moet worden gelegd zoals

544
00:20:15,480 --> 00:20:17,700
in  geel hier zijn de filters in een bepaalde

545
00:20:17,700 --> 00:20:19,260
groep veel sterker gecorreleerd,

546
00:20:19,260 --> 00:20:20,880
ze hebben deze overlappende receptieve

547
00:20:20,880 --> 00:20:23,460
velden dan andere willekeurige locaties, dus in

548
00:20:23,460 --> 00:20:25,020
wezen zien we dat ons model

549
00:20:25,020 --> 00:20:27,840
leert activiteitsactiviteiten samen te clusteren,

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
uh in een soort gesimuleerd verticaal blad

552
00:20:32,059 --> 00:20:34,260
volgens de correlaties in  de

553
00:20:34,260 --> 00:20:36,840
dataset dus in plaats van in convolutie,

554
00:20:36,840 --> 00:20:38,580
waarbij je feitelijk gewichtsvastlegging doet

555
00:20:38,580 --> 00:20:40,860
en je handmatig specificeert. Ik wil

556
00:20:40,860 --> 00:20:42,539
dit gewicht overal kopiëren. Je kunt

557
00:20:42,539 --> 00:20:44,220
dit misschien zien als een geschatte

558
00:20:44,220 --> 00:20:45,500
wachttijd

559
00:20:45,500 --> 00:20:48,120
en eigenlijk leren we dit uit de

560
00:20:48,120 --> 00:20:49,620
correlatie  structuur van de dataset

561
00:20:49,620 --> 00:20:51,660
zelf en om hier wat

562
00:20:51,660 --> 00:20:54,120
meer biologische inspiratie voor te geven

563
00:20:54,120 --> 00:20:56,280
en we weten dat retinotopie

564
00:20:56,280 --> 00:20:58,020
aanwezig is in de hersenen. Dit is een voorbeeld

565
00:20:58,020 --> 00:21:02,460
van retinotopie in de visuele cortex en

566
00:21:02,460 --> 00:21:04,500
je kunt zien of je de makaak laat zien  Dit

567
00:21:04,500 --> 00:21:06,780
beeld als dit wordt geprojecteerd in

568
00:21:06,780 --> 00:21:08,520
deze uh-

569
00:21:08,520 --> 00:21:11,700
topologie, waarbij de ruimte daadwerkelijk op

570
00:21:11,700 --> 00:21:13,740
het oppervlak van de cortex wordt behouden,

571
00:21:13,740 --> 00:21:16,080
dus het idee is dat topografische

572
00:21:16,080 --> 00:21:18,419
organisatie en zelfs topografische

573
00:21:18,419 --> 00:21:21,299
organisatie de

574
00:21:21,299 --> 00:21:26,160
invoercorrelaties van onze datasets behoudt, en

575
00:21:26,160 --> 00:21:28,679
mogelijk kan dit gunstig zijn

576
00:21:28,679 --> 00:21:30,840
voor  door deze ideeën nog een

577
00:21:30,840 --> 00:21:32,340
beetje verder te generaliseren, dus zoals ik in het

578
00:21:32,340 --> 00:21:34,679
begin al zei, het zou nog beter zijn als

579
00:21:34,679 --> 00:21:37,200
we gewoon iets meer konden leren dan

580
00:21:37,200 --> 00:21:39,320
alleen convolutie, misschien ingewikkelder

581
00:21:39,320 --> 00:21:43,679
equivalenties, dus hoe doen we dat ene

582
00:21:43,679 --> 00:21:45,720
ding dat duidelijk is in natuurlijke

583
00:21:45,720 --> 00:21:48,299
intelligentie is dat we  bestaan ​​niet in

584
00:21:48,299 --> 00:21:51,120
deze wereld van IID-frames, toch bestaan ​​we

585
00:21:51,120 --> 00:21:53,520
in een wereld met continue reeksen

586
00:21:53,520 --> 00:21:55,620
transformaties, dus misschien kunnen we

587
00:21:55,620 --> 00:21:58,440
ons model uitbreiden naar deze setting om

588
00:21:58,440 --> 00:22:01,080
transformaties te leren observeren. Dit is een idee

589
00:22:01,080 --> 00:22:03,299
van temporele coherentie,

590
00:22:03,299 --> 00:22:05,280
dus wat zou er gebeuren als we gewoonweg  heeft

591
00:22:05,280 --> 00:22:08,280
ons vorige raamwerk in de loop van de

592
00:22:08,280 --> 00:22:10,620
tijd uitgebreid. Dimensie goed, dus in plaats van alleen maar te

593
00:22:10,620 --> 00:22:13,080
groeperen en te zeggen dat we willen dat onze neuronen

594
00:22:13,080 --> 00:22:15,059
groepsgewijs schaars zijn in termen van ruimtelijke

595
00:22:15,059 --> 00:22:17,400
omvang op de cortex, willen we eigenlijk dat

596
00:22:17,400 --> 00:22:18,960
ze in de loop van de tijd groepsgewijs schaars zijn, wat

597
00:22:18,960 --> 00:22:20,640
betekent dat als één reeks neuronen

598
00:22:20,640 --> 00:22:22,559
actief is  nu willen we dat dezelfde reeks

599
00:22:22,559 --> 00:22:24,360
neuronen ook in de toekomst actief zal zijn.

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
Als we kijken of we

602
00:22:27,840 --> 00:22:30,600
hier intuïtief over nadenken, zien we dat dit

603
00:22:30,600 --> 00:22:33,059
eigenlijk meer bemoedigende invariantie en

604
00:22:33,059 --> 00:22:35,039
gelijkwaardigheid is. Een manier om dit te begrijpen is dat

605
00:22:35,039 --> 00:22:37,140
we zeggen dat we  Ik wil dat dezelfde neuronen

606
00:22:37,140 --> 00:22:39,179
constant actief zijn, maar de

607
00:22:39,179 --> 00:22:41,280
invoertransformatie verandert precies. De

608
00:22:41,280 --> 00:22:44,220
voeten van deze kleine vos bewegen, dus als

609
00:22:44,220 --> 00:22:45,960
dezelfde neuronen keer op keer voor hetzelfde coderen,

610
00:22:45,960 --> 00:22:47,880
maar de voeten

611
00:22:47,880 --> 00:22:49,320
bewegen, zullen die neuronen

612
00:22:49,320 --> 00:22:51,360
leren  om onveranderlijk te zijn voor de beweging van

613
00:22:51,360 --> 00:22:53,880
dat been van deze hond,

614
00:22:53,880 --> 00:22:57,539
dus in plaats daarvan is dat oeps,

615
00:22:57,539 --> 00:23:01,200
ik ging hier de verkeerde kant op,

616
00:23:01,200 --> 00:23:04,860
dus in plaats daarvan was ons inzicht dat deze

617
00:23:04,860 --> 00:23:06,659
groep begint te worden, in plaats daarvan kan worden

618
00:23:06,659 --> 00:23:09,059
verschoven met betrekking tot de tijd, dus dit

619
00:23:09,059 --> 00:23:10,980
zou betekenen  dat opeenvolgend verschoven

620
00:23:10,980 --> 00:23:13,080
sets van activeringen zouden worden aangemoedigd

621
00:23:13,080 --> 00:23:15,179
om samen te activeren en dan zou onze latente

622
00:23:15,179 --> 00:23:16,440
ruimte echt worden gestructureerd met

623
00:23:16,440 --> 00:23:18,000
betrekking tot de waargenomen transformaties,

624
00:23:18,000 --> 00:23:19,980
zodat je hier kunt zien dat in plaats van dat

625
00:23:19,980 --> 00:23:21,480
dezelfde set neuronen actief is bij alle

626
00:23:21,480 --> 00:23:23,340
tijdstappen, het in werkelijkheid een sequentiële reeks is

627
00:23:23,340 --> 00:23:24,900
gepermuteerde reeks neuronen die we

628
00:23:24,900 --> 00:23:27,780
op deze schaarse manier samen groeperen, uh

629
00:23:27,780 --> 00:23:29,940
en dit stelt ons in staat om

630
00:23:29,940 --> 00:23:33,419
verschillende observaties in de loop van de tijd te modelleren, maar

631
00:23:33,419 --> 00:23:34,860
ze zijn nog steeds verbonden in termen van

632
00:23:34,860 --> 00:23:36,960
het leren van een transformatie en het behouden van

633
00:23:36,960 --> 00:23:38,340
deze correlatiestructuur van de

634
00:23:38,340 --> 00:23:40,020
empathie,

635
00:23:40,020 --> 00:23:41,940
dus als we  voeg dit samen in onze

636
00:23:41,940 --> 00:23:44,400
topografische Bae-architectuur, je kunt

637
00:23:44,400 --> 00:23:46,020
iets krijgen dat er zo uitziet, je ziet

638
00:23:46,020 --> 00:23:48,120
dat we een invoerreeks hebben, we

639
00:23:48,120 --> 00:23:51,240
coderen opnieuw een z-variabele en dan

640
00:23:51,240 --> 00:23:53,520
meerdere U-variabelen in de noemer

641
00:23:53,520 --> 00:23:55,740
hier, en dan wordt elk van deze U-

642
00:23:55,740 --> 00:23:58,620
variabelen verschoven  uh, net zoals we

643
00:23:58,620 --> 00:24:00,480
eerder lieten zien om

644
00:24:00,480 --> 00:24:02,820
dit te bereiken, deze verschuivingsequivariantiestructuur

645
00:24:02,820 --> 00:24:04,740
waar we naar op zoek zijn. Als we

646
00:24:04,740 --> 00:24:07,080
deze combineren in deze studenten-T-

647
00:24:07,080 --> 00:24:09,240
productdistributie, krijgen we een enkele latente

648
00:24:09,240 --> 00:24:10,740
variabele, dit is nu onze topografische

649
00:24:10,740 --> 00:24:13,860
variabele T en nu we  als je deze

650
00:24:13,860 --> 00:24:16,140
bekende structuur in onze latente ruimte hebt,

651
00:24:16,140 --> 00:24:17,460
kun je het zien als een gestructureerd

652
00:24:17,460 --> 00:24:19,919
wereldmodel. We weten hoe we deze

653
00:24:19,919 --> 00:24:21,659
latente ruimte moeten transformeren. In dit geval is het door

654
00:24:21,659 --> 00:24:23,580
deze activeringen rond deze

655
00:24:23,580 --> 00:24:25,860
cirkels te permuteren. We doen het als een cyclische rol. Een

656
00:24:25,860 --> 00:24:28,380
cyclische verschuiving. We weten dat dit zo is.  gaan

657
00:24:28,380 --> 00:24:30,120
overeenkomen met onze geleerde

658
00:24:30,120 --> 00:24:32,640
invoertransformaties en we kunnen dat verifiëren

659
00:24:32,640 --> 00:24:34,620
door te zeggen: oké, wat als ik doorga met deze

660
00:24:34,620 --> 00:24:36,480
invoertransformatie, de echte

661
00:24:36,480 --> 00:24:38,100
transformatie in de dataset, wat

662
00:24:38,100 --> 00:24:40,559
een rotatie is, en dan vergelijk ik dat met

663
00:24:40,559 --> 00:24:42,659
hoe ik mijn rol heb vervuld in mijn late  Ruimte

664
00:24:42,659 --> 00:24:44,700
door mijn activaties in mijn

665
00:24:44,700 --> 00:24:47,280
hersenen te verplaatsen en dan decoderen we en we zien dat

666
00:24:47,280 --> 00:24:49,919
we precies hetzelfde krijgen en dus

667
00:24:49,919 --> 00:24:52,140
demonstreert dit deze

668
00:24:52,140 --> 00:24:53,580
commutiliteitseigenschap waar ik het eerder over had

669
00:24:53,580 --> 00:24:56,820
voor het verifiëren van homomorfisme

670
00:24:56,820 --> 00:24:58,799
en om dit een beetje meer

671
00:24:58,799 --> 00:25:02,460
kwaliteit te meten  kwantitatief kunnen we meten

672
00:25:02,460 --> 00:25:04,440
wat een equivalentieverlies wordt genoemd, dus

673
00:25:04,440 --> 00:25:07,080
dit is eigenlijk de kwantificering van

674
00:25:07,080 --> 00:25:09,360
dit verschil tussen onze opgerolde

675
00:25:09,360 --> 00:25:12,120
capsule-activering of het rollen in ons

676
00:25:12,120 --> 00:25:15,059
hoofd versus het kijken naar het rollen

677
00:25:15,059 --> 00:25:16,559
en vooruit, ze kijken naar de

678
00:25:16,559 --> 00:25:19,440
transformatie die zich voor ons ontvouwt, dus we

679
00:25:19,440 --> 00:25:21,600
zien de topografische  Bae behaalt een

680
00:25:21,600 --> 00:25:24,000
aanzienlijk lagere uh-

681
00:25:24,000 --> 00:25:26,700
equivalentiefout. Deze bubble vae is waar ik het

682
00:25:26,700 --> 00:25:27,960
eerder over had, waar het

683
00:25:27,960 --> 00:25:29,820
invariantie leert, dus het heeft niet de

684
00:25:29,820 --> 00:25:32,340
ploegendienst en de traditionele vae

685
00:25:32,340 --> 00:25:35,640
heeft geen idee van organisatie of

686
00:25:35,640 --> 00:25:37,380
temporele component, dus de prestaties zijn

687
00:25:37,380 --> 00:25:40,320
bovendien erg slecht  Hierbij zien we dat

688
00:25:40,320 --> 00:25:41,700
het model een beter generatief model

689
00:25:41,700 --> 00:25:45,059
van reeksen is, het wordt gewoon een lagere uh

690
00:25:45,059 --> 00:25:48,179
lager, zoals een negatieve log-waarschijnlijkheid op

691
00:25:48,179 --> 00:25:50,100
de dataset, dus het is beter in staat om

692
00:25:50,100 --> 00:25:51,720
deze dataset te modelleren omdat het een

693
00:25:51,720 --> 00:25:52,919
idee heeft van de structuur van de

694
00:25:52,919 --> 00:25:55,460
transformaties

695
00:25:55,980 --> 00:25:58,140
uh, we kunnen dit testen op meerdere

696
00:25:58,140 --> 00:25:59,760
verschillende transformatietypen en in de

697
00:25:59,760 --> 00:26:00,840
bovenste rij laten we de echte

698
00:26:00,840 --> 00:26:02,880
transformatie zien. We hebben deze

699
00:26:02,880 --> 00:26:05,039
grijs gemaakte afbeeldingen eruit gehaald en vervolgens op de onderste

700
00:26:05,039 --> 00:26:07,080
rij gecodeerd en dan

701
00:26:07,080 --> 00:26:08,700
rollen we onze activeringen gewoon rond en we houden

702
00:26:08,700 --> 00:26:12,140
decoderen om te zien wat het model

703
00:26:12,140 --> 00:26:15,000
heeft geleerd als de huidige

704
00:26:15,000 --> 00:26:17,039
transformatie die wordt waargenomen en

705
00:26:17,039 --> 00:26:19,340
we zien dat het in principe

706
00:26:19,340 --> 00:26:21,360
deze elementen van de

707
00:26:21,360 --> 00:26:23,640
reeks die het nog nooit eerder heeft gezien perfect kan reconstrueren,

708
00:26:23,640 --> 00:26:25,260
bovendien met afbeeldingen uit

709
00:26:25,260 --> 00:26:26,580
de testset die het nog nooit heeft gezien  voorheen

710
00:26:26,580 --> 00:26:28,380
simpelweg omdat het weet wat de

711
00:26:28,380 --> 00:26:29,760
transformatie is die het momenteel

712
00:26:29,760 --> 00:26:31,500
codeert, kan het dat generaliseren naar nieuwe

713
00:26:31,500 --> 00:26:33,919
voorbeelden,

714
00:26:34,020 --> 00:26:36,360
dus de conclusie van dit deel is echt een

715
00:26:36,360 --> 00:26:38,039
topografische organisatie. We hebben laten zien dat

716
00:26:38,039 --> 00:26:40,080
een bewaarde invoerstructuur en nu laten

717
00:26:40,080 --> 00:26:41,940
we zien dat dit de

718
00:26:41,940 --> 00:26:44,279
efficiëntie en generalisatie potentieel kan verbeteren  zoals we

719
00:26:44,279 --> 00:26:46,200
zouden hopen,

720
00:26:46,200 --> 00:26:48,600
dus eindelijk iets dat ons verraste

721
00:26:48,600 --> 00:26:49,980
en waarvan ik dacht dat het potentieel het meest

722
00:26:49,980 --> 00:26:52,500
interessant was, is dat deze

723
00:26:52,500 --> 00:26:53,700
transformaties die door ons model zijn geleerd,

724
00:26:53,700 --> 00:26:54,960
feitelijk de

725
00:26:54,960 --> 00:26:57,059
combinaties van transformaties generaliseren die

726
00:26:57,059 --> 00:26:59,580
we niet zien tijdens de training, dus

727
00:26:59,580 --> 00:27:02,100
bijvoorbeeld ondanks alleen training  over kleur

728
00:27:02,100 --> 00:27:04,200
en rotatie Transformaties en

729
00:27:04,200 --> 00:27:06,419
isolatie als het model tijdens de test

730
00:27:06,419 --> 00:27:08,340
een gecombineerde kleurrotatietransformatie krijgt gepresenteerd,

731
00:27:08,340 --> 00:27:11,100
zien we dat het in staat is

732
00:27:11,100 --> 00:27:13,140
om deze transformaties volledig te modelleren en te voltooien

733
00:27:13,140 --> 00:27:14,700
via de

734
00:27:14,700 --> 00:27:17,159
capsulerol, wat impliceert dat het heeft geleerd om de

735
00:27:17,159 --> 00:27:19,620
representatie van deze

736
00:27:19,620 --> 00:27:20,880
verschillende te ontbinden in factoren  Transformaties en het kan

737
00:27:20,880 --> 00:27:24,600
ze flexibel combineren tijdens de conclusie,

738
00:27:24,600 --> 00:27:28,140
dus nogmaals, misschien krijgen we ook niet alleen

739
00:27:28,140 --> 00:27:29,820
officieel efficiëntie in generalisatie,

740
00:27:29,820 --> 00:27:34,100
we krijgen ook wat basiscompositie,

741
00:27:34,260 --> 00:27:36,059
dus laten we het hebben over de beperkingen en

742
00:27:36,059 --> 00:27:38,460
wat we vervolgens zouden kunnen doen. De belangrijkste

743
00:27:38,460 --> 00:27:40,620
beperking is dat er een  voorgedefinieerde

744
00:27:40,620 --> 00:27:44,159
transformatie die we opleggen,

745
00:27:44,159 --> 00:27:46,500
zowel in ruimte als in tijd, dus hoewel we

746
00:27:46,500 --> 00:27:49,080
ons hebben bevrijd van groepstransformaties en

747
00:27:49,080 --> 00:27:52,440
specifiek van vertaling of

748
00:27:52,440 --> 00:27:53,940
rotatie zoals het momenteel wordt gedaan in de

749
00:27:53,940 --> 00:27:55,559
machine learning-wereld,

750
00:27:55,559 --> 00:27:59,240
hebben we nog steeds deze hardgecodeerde

751
00:27:59,240 --> 00:28:01,980
latente rol in onze  gaat voor alles wat

752
00:28:01,980 --> 00:28:03,900
we zien en om dit een beetje

753
00:28:03,900 --> 00:28:05,700
flexibeler te maken, zodat we hopelijk

754
00:28:05,700 --> 00:28:08,880
een grotere diversiteit aan transformaties kunnen modelleren. We

755
00:28:08,880 --> 00:28:10,980
denken dat we misschien inspiratie kunnen halen

756
00:28:10,980 --> 00:28:13,860
uit meer gestructureerde ruimtelijke en

757
00:28:13,860 --> 00:28:15,600
temporele dynamieken die in

758
00:28:15,600 --> 00:28:18,120
de hersenen worden waargenomen en dat kost ons  naar

759
00:28:18,120 --> 00:28:20,400
het tweede deel van deze lezing, de

760
00:28:20,400 --> 00:28:22,140
ruimtelijke en temporele dynamiek die we

761
00:28:22,140 --> 00:28:23,039
gaan proberen te integreren in

762
00:28:23,039 --> 00:28:25,200
kunstmatige neurale netwerken. Een voorbeeld

763
00:28:25,200 --> 00:28:27,059
daarvan zijn reizende golven zoals ik hier liet zien,

764
00:28:27,059 --> 00:28:28,020


765
00:28:28,020 --> 00:28:30,600
dus wat bedoelen we daarmee, nou,

766
00:28:30,600 --> 00:28:32,279
hier is een heel  recent artikel waarin ze

767
00:28:32,279 --> 00:28:36,059
een negen Tesla FMRI gebruikten die werkte met een

768
00:28:36,059 --> 00:28:38,700
resolutie van 36 milliseconden om een ​​enkel

769
00:28:38,700 --> 00:28:40,980
stukje van een rattenbrein onder narcose af te beelden

770
00:28:40,980 --> 00:28:43,320
en wat we zien is deze zeer duidelijk

771
00:28:43,320 --> 00:28:45,720
gestructureerde ruimtelijke temporele activiteit en

772
00:28:45,720 --> 00:28:48,299
correlaties en deze auteurs van het

773
00:28:48,299 --> 00:28:50,520
artikel gaan dit verder analyseren  activiteit in

774
00:28:50,520 --> 00:28:52,919
termen van de belangrijkste modi zoals

775
00:28:52,919 --> 00:28:55,799
rechts afgebeeld, dus onze hypothese is dat

776
00:28:55,799 --> 00:28:57,179
een soort

777
00:28:57,179 --> 00:28:59,039
correlatiestructuur als deze misschien nuttig kan zijn

778
00:28:59,039 --> 00:29:01,260
voor het structureren van de representaties van

779
00:29:01,260 --> 00:29:03,240
ons model met betrekking tot waargenomen

780
00:29:03,240 --> 00:29:05,100
transformaties, maar op een veel

781
00:29:05,100 --> 00:29:07,440
flexibelere manier dan simpelweg  gewoon een cyclische

782
00:29:07,440 --> 00:29:10,700
verschuiving zoals we eerder deden,

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
en laat me zeggen dat dit niet alleen wordt

785
00:29:15,900 --> 00:29:19,320
waargenomen bij ratten van een ssi. Uh dit

786
00:29:19,320 --> 00:29:20,940
kun je zien, je kunt deze reizende golven zien

787
00:29:20,940 --> 00:29:24,179
gebeuren in de bergcortex van wakker

788
00:29:24,179 --> 00:29:27,600
gedragende primaten, dus bijvoorbeeld aan

789
00:29:27,600 --> 00:29:29,580
de linkerkant  hier laten ze reizende golven zien

790
00:29:29,580 --> 00:29:31,740
die feitelijk

791
00:29:31,740 --> 00:29:35,520
veranderen. Hoe waarschijnlijk het is dat een primaat

792
00:29:35,520 --> 00:29:38,279
stimuli met een laag contrast waarneemt op basis van de fase

793
00:29:38,279 --> 00:29:40,980
van de golf. Bovendien laten ze zien dat

794
00:29:40,980 --> 00:29:43,500
een soortgelijke stimulus met hoog contrast aan de

795
00:29:43,500 --> 00:29:45,779
rechterkant een lopende golfactiviteit kan veroorzaken

796
00:29:45,779 --> 00:29:47,520
die zich naar buiten voortplant  zelfs

797
00:29:47,520 --> 00:29:50,039
in de primaire visuele cortex, dus deze zijn

798
00:29:50,039 --> 00:29:52,140
echt alomtegenwoordig in de hersenen

799
00:29:52,140 --> 00:29:54,000
op meerdere niveaus en het zou

800
00:29:54,000 --> 00:29:55,440
interessant zijn om te bestuderen wat hun

801
00:29:55,440 --> 00:29:58,140
implicaties zijn voor

802
00:29:58,140 --> 00:29:59,700
het leren van structuurrepresentaties in ons geval, of in

803
00:29:59,700 --> 00:30:01,799
het algemeen

804
00:30:01,799 --> 00:30:04,140
is er eerder werk dat

805
00:30:04,140 --> 00:30:06,720
dit soort uh-dynamieken heeft bestudeerd  en ze

806
00:30:06,720 --> 00:30:08,700
bouwen modellen, dus bovenaan zijn dit de

807
00:30:08,700 --> 00:30:10,380
vergelijkingen die een piekend

808
00:30:10,380 --> 00:30:12,600
neuraal netwerk beschrijven dat ze laten zien als je

809
00:30:12,600 --> 00:30:15,720
tijdsvertragingen implementeert, eigenlijk axonale

810
00:30:15,720 --> 00:30:18,240
tijdsvertragingen tussen neuronen, je krijgt

811
00:30:18,240 --> 00:30:20,820
deze structuurdynamiek van reizende

812
00:30:20,820 --> 00:30:22,440
golven zolang je netwerkgrootte

813
00:30:22,440 --> 00:30:24,059
groot genoeg is

814
00:30:24,059 --> 00:30:26,520
Eh, zoals veel mensen waarschijnlijk weten, is

815
00:30:26,520 --> 00:30:28,620
het relatief uitdagend om

816
00:30:28,620 --> 00:30:31,320
piekende neurale netwerken van dezelfde grootte

817
00:30:31,320 --> 00:30:34,820
en prestaties te trainen als diepe neurale netwerken, op

818
00:30:34,820 --> 00:30:37,679
dezelfde manier aan de onderkant een ander systeem

819
00:30:37,679 --> 00:30:39,539
dat aanzienlijk eenvoudiger is, maar

820
00:30:39,539 --> 00:30:42,840
misschien te simpel. Uh is een netwerk van

821
00:30:42,840 --> 00:30:45,120
gekoppelde oscillatoren waarvan bekend is dat ze

822
00:30:45,120 --> 00:30:48,779
vertonen synchronie en ruimtelijke temporele

823
00:30:48,779 --> 00:30:52,200
dynamiek en complexe patronen, maar

824
00:30:52,200 --> 00:30:53,520
dit wordt een fasereductiesysteem genoemd

825
00:30:53,520 --> 00:30:55,500
en geeft niet helemaal de

826
00:30:55,500 --> 00:30:57,059
volledige complexiteit weer waarin we geïnteresseerd zijn,

827
00:30:57,059 --> 00:30:58,140
dus we kijken naar iets dat

828
00:30:58,140 --> 00:31:00,779
mogelijk tussen deze twee in ligt

829
00:31:00,779 --> 00:31:03,600
en wat we  Het punt is dat dit werk in

830
00:31:03,600 --> 00:31:06,600
dit werk is om een

831
00:31:06,600 --> 00:31:08,520
netwerk van een paar oscillatoren

832
00:31:08,520 --> 00:31:10,620
iets flexibeler te parametriseren dan een paramoto-

833
00:31:10,620 --> 00:31:12,360
model, dus dit is echt

834
00:31:12,360 --> 00:31:14,580
gebouwd op dit paar destillatorische,

835
00:31:14,580 --> 00:31:16,380
terugkerende neurale netwerk van Constantine

836
00:31:16,380 --> 00:31:18,720
Rush en Nisha,

837
00:31:18,720 --> 00:31:20,760
waar ze in feite de

838
00:31:20,760 --> 00:31:22,200
vergelijking namen die  beschrijft een eenvoudige

839
00:31:22,200 --> 00:31:23,820
harmonische oscillator. Het is een differentiaalvergelijking van de tweede orde.

840
00:31:23,820 --> 00:31:26,159
De versnelling

841
00:31:26,159 --> 00:31:29,940
van een bal op een veer is evenredig met

842
00:31:29,940 --> 00:31:32,480
zijn verplaatsing.

843
00:31:32,480 --> 00:31:35,220
U kunt extra termen toevoegen, zoals

844
00:31:35,220 --> 00:31:37,260
demping, zodat de oscillaties

845
00:31:37,260 --> 00:31:39,360
in de loop van de tijd langzaam uitsterven.

846
00:31:39,360 --> 00:31:41,580
Je kunt deze oscillator aansturen met een

847
00:31:41,580 --> 00:31:43,380
externe  input om

848
00:31:43,380 --> 00:31:45,179
deze demping tegen te gaan of om iets meer

849
00:31:45,179 --> 00:31:47,279
complexiteit aan de dynamiek te geven

850
00:31:47,279 --> 00:31:49,260
en bovendien, als je veel van

851
00:31:49,260 --> 00:31:50,940
deze oscillatoren hebt, kun je ze aan

852
00:31:50,940 --> 00:31:53,000
elkaar koppelen met deze koppelingsmatrices. W

853
00:31:53,000 --> 00:31:55,320
uh, zoals we op deze

854
00:31:55,320 --> 00:31:56,640
afbeelding hier een beetje demonstreren, zodat je echt kunt  beschouw

855
00:31:56,640 --> 00:31:58,140
dit netwerk als een stel bubbels op

856
00:31:58,140 --> 00:31:59,940
Springs en ze zijn misschien

857
00:31:59,940 --> 00:32:01,740
ook met elkaar verbonden door veren of elastische

858
00:32:01,740 --> 00:32:03,600
banden, ongeacht het paar destillerende

859
00:32:03,600 --> 00:32:05,279
terugkerende neurale netwerk van de Russische

860
00:32:05,279 --> 00:32:08,100
Mishra uh met deze verschillende termen en

861
00:32:08,100 --> 00:32:09,899
dit is zeer krachtig gebleken

862
00:32:09,899 --> 00:32:12,480
voor het modelleren van lange reeksen zeiden ze ook dat

863
00:32:12,480 --> 00:32:13,740
ze geïnspireerd waren door de

864
00:32:13,740 --> 00:32:15,360
hersenen om dit te bouwen en er staat veel

865
00:32:15,360 --> 00:32:17,700
goede analyse in dat artikel.

866
00:32:17,700 --> 00:32:19,020
Ze laten bijvoorbeeld zien dat dit echt

867
00:32:19,020 --> 00:32:21,440
gunstige eigenschappen is met betrekking tot

868
00:32:21,440 --> 00:32:23,460
verdwijnende gradiëntproblemen die

869
00:32:23,460 --> 00:32:25,440
doorgaans voorkomen in terugkerende neurale

870
00:32:25,440 --> 00:32:26,820
netwerken.

871
00:32:26,820 --> 00:32:29,159
maar als we naar ruimtelijke

872
00:32:29,159 --> 00:32:30,960
temporele dynamiek en dit type

873
00:32:30,960 --> 00:32:32,820
model willen kijken, is het een beetje uitdagend

874
00:32:32,820 --> 00:32:34,919
omdat deze koppelingsmatrices hier de

875
00:32:34,919 --> 00:32:36,320
W's

876
00:32:36,320 --> 00:32:39,600
die elke neurale verbinding verbinden of elke

877
00:32:39,600 --> 00:32:41,240
oscillator ten opzichte van elkaar zijn gepositioneerd,

878
00:32:41,240 --> 00:32:43,620
dit zijn dicht verbonden matrices

879
00:32:43,620 --> 00:32:45,120
zoals ik heb geprobeerd  hier links afgebeeld,

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
dus als je de dynamiek

882
00:32:48,299 --> 00:32:50,580
van dit netwerk probeert te visualiseren, zie je geen enkele

883
00:32:50,580 --> 00:32:51,899
ruimtelijke organisatie. Er is geen

884
00:32:51,899 --> 00:32:55,380
erfenis. Het is een verontschuldiging voor de latente

885
00:32:55,380 --> 00:32:57,000
ruimte van dit model,

886
00:32:57,000 --> 00:32:58,799
dus je kunt dit bedenken zoals in ons

887
00:32:58,799 --> 00:33:01,020
vorige voorbeeld een neuron  is verbonden

888
00:33:01,020 --> 00:33:03,240
met een potentieel willekeurige reeks andere

889
00:33:03,240 --> 00:33:04,919
neuronen. Deze neuronen zijn verbonden met een

890
00:33:04,919 --> 00:33:06,600
andere willekeurige reeks neuronen en

891
00:33:06,600 --> 00:33:08,520
je krijgt zeker oscillerende dynamiek,

892
00:33:08,520 --> 00:33:10,860
maar een soort fluctuaties die

893
00:33:10,860 --> 00:33:13,260
niet veel gestructureerde betekenis hebben, dus

894
00:33:13,260 --> 00:33:15,360
in ons werk dachten we  Oké, hoe kunnen

895
00:33:15,360 --> 00:33:18,299
we dit meer omzetten naar de soorten

896
00:33:18,299 --> 00:33:19,860
dynamiek waarin we geïnteresseerd zijn in deze

897
00:33:19,860 --> 00:33:22,140
gestructureerde voortplanting van activiteit,

898
00:33:22,140 --> 00:33:23,940
en een duidelijke manier om dat te doen is door

899
00:33:23,940 --> 00:33:26,539
een meer gestructureerde connectiviteitsmatrix te hebben,

900
00:33:26,539 --> 00:33:29,880
waarvan we ontdekten dat deze eenvoudig

901
00:33:29,880 --> 00:33:31,559
en efficiënt kan worden geïmplementeerd  geïmplementeerd via een

902
00:33:31,559 --> 00:33:33,000
convolutieoperatie die je kunt

903
00:33:33,000 --> 00:33:34,620
zien als een lokale en lokaal

904
00:33:34,620 --> 00:33:36,299
verbonden laag, dus in plaats van dat

905
00:33:36,299 --> 00:33:37,860
elk neuron verbonden is,

906
00:33:37,860 --> 00:33:39,480
zijn alle neuronen alleen verbonden met hun

907
00:33:39,480 --> 00:33:41,580
nabijgelegen buren. Na de training

908
00:33:41,580 --> 00:33:42,840
krijg je uiteindelijk iets dat

909
00:33:42,840 --> 00:33:44,880
lijkt op een vloeiende ruimtelijke laag.  temporele

910
00:33:44,880 --> 00:33:46,620
dynamiek,

911
00:33:46,620 --> 00:33:48,419
dus om een ​​beetje duidelijker te zijn om

912
00:33:48,419 --> 00:33:50,519
dit model te trainen, nemen we deze afzonderlijke

913
00:33:50,519 --> 00:33:52,200
differentiaalvergelijking van de tweede orde die

914
00:33:52,200 --> 00:33:54,299
we aan het beschrijven waren voordat je

915
00:33:54,299 --> 00:33:56,340
deze discretiseert in twee vergelijkingen van de eerste orde. Je

916
00:33:56,340 --> 00:33:57,960
kunt dit zien als het numeriek

917
00:33:57,960 --> 00:34:01,200
integreren van de ode die we nu hebben

918
00:34:01,200 --> 00:34:03,120
snelheid en dan updaten we

919
00:34:03,120 --> 00:34:06,000
uh en we kunnen dit model trainen als

920
00:34:06,000 --> 00:34:07,620
zoiets als een automatische encoder of een

921
00:34:07,620 --> 00:34:09,839
automatisch regressief model, dus als we een

922
00:34:09,839 --> 00:34:11,460
invoer nemen, coderen we deze naar onze latente ruimte.

923
00:34:11,460 --> 00:34:14,339
De invoer is eigenlijk Dr. is deze f van x-

924
00:34:14,339 --> 00:34:16,500
term die werkt  als de drijvende term, dus

925
00:34:16,500 --> 00:34:18,599
het is alsof je deze oscillatoren van

926
00:34:18,599 --> 00:34:20,879
onderaf aanstuurt, uh en dan hebben ze hun

927
00:34:20,879 --> 00:34:23,099
eigen dynamiek die wordt gedefinieerd door de

928
00:34:23,099 --> 00:34:25,800
koppelingstermen, deze lokale koppelingen en

929
00:34:25,800 --> 00:34:27,540
dan nemen we bij elke tijdstap deze

930
00:34:27,540 --> 00:34:29,460
latente toestand, deze golftoestand en we

931
00:34:29,460 --> 00:34:31,560
decoderen om te proberen  en de invoer reconstrueren

932
00:34:31,560 --> 00:34:33,540
en bij de huidige tijdstap of een

933
00:34:33,540 --> 00:34:35,699
toekomstige tijdstap zijn.

934
00:34:35,699 --> 00:34:37,980
We kunnen deze

935
00:34:37,980 --> 00:34:42,300
modellen tijdens de training analyseren om te zien wat er

936
00:34:42,300 --> 00:34:43,619
vóór de training gebeurt. En na de

937
00:34:43,619 --> 00:34:45,780
training kunnen we de fase en

938
00:34:45,780 --> 00:34:47,399
de snelheid van de dynamiek in de

939
00:34:47,399 --> 00:34:49,379
latente ruimte zien we eigenlijk dat er aan het

940
00:34:49,379 --> 00:34:51,480
begin van de handel geen golven in

941
00:34:51,480 --> 00:34:53,699
ons model zitten, maar na training na 50

942
00:34:53,699 --> 00:34:55,500
tijdperken zien we dat er een soepele

943
00:34:55,500 --> 00:34:57,119
gestructureerde activiteit is die zich

944
00:34:57,119 --> 00:35:00,420
naar beneden voortplant, ten dienste van deze

945
00:35:00,420 --> 00:35:01,800
sequentiemodelleringstaak die we doen zoals

946
00:35:01,800 --> 00:35:04,380
roterende objecten,

947
00:35:04,380 --> 00:35:05,940
dus  wat is het voordeel hiervan?

948
00:35:05,940 --> 00:35:07,680
Ik bedoel, de hele reden dat ik dit motiveerde

949
00:35:07,680 --> 00:35:10,380
was om te zeggen dat we een

950
00:35:10,380 --> 00:35:11,880
flexibeler leerstructuur wilden hebben. Doen we

951
00:35:11,880 --> 00:35:13,020
dat echt of krijgen we gewoon

952
00:35:13,020 --> 00:35:15,060
mooie golven?

953
00:35:15,060 --> 00:35:16,859
Dus wat we in onze paper lieten zien, is

954
00:35:16,859 --> 00:35:19,320
dat we dat echt zijn  het leren van een soort

955
00:35:19,320 --> 00:35:20,940
nuttige structuur en de manier waarop we dat lieten zien

956
00:35:20,940 --> 00:35:22,440
is opnieuw met zoiets als dit

957
00:35:22,440 --> 00:35:24,960
commutatieve diagram: als je een invoer neemt

958
00:35:24,960 --> 00:35:27,000
en deze codeert en je krijgt een golftoestand

959
00:35:27,000 --> 00:35:29,280
en dan propageer je golven

960
00:35:29,280 --> 00:35:31,619
kunstmatig in die golftoestand en dan

961
00:35:31,619 --> 00:35:33,480
decodeer je dat  merk op dat het

962
00:35:33,480 --> 00:35:35,220
eigenlijk precies hetzelfde is als wanneer je

963
00:35:35,220 --> 00:35:37,140
gewoon een aantal verschillende

964
00:35:37,140 --> 00:35:39,180
afbeeldingen van verschillende transformaties laat zien, dus

965
00:35:39,180 --> 00:35:41,640
veel verschillende cijfers, verschillende

966
00:35:41,640 --> 00:35:43,920
kenmerken en we zien dat we

967
00:35:43,920 --> 00:35:46,140
in elk geval verschillende soorten golfactiviteit krijgen

968
00:35:46,140 --> 00:35:47,880
om dat te modelleren  verschillende

969
00:35:47,880 --> 00:35:49,140
transformatie

970
00:35:49,140 --> 00:35:51,119
als we het ook op verschillende datasets trainen,

971
00:35:51,119 --> 00:35:53,400
zien we op dezelfde manier complexere

972
00:35:53,400 --> 00:35:55,200
dynamiek, in dit geval misschien niet eens

973
00:35:55,200 --> 00:35:57,839
lopende golven of staande golven die

974
00:35:57,839 --> 00:36:00,359
kunnen worden gezien als reizende golven in

975
00:36:00,359 --> 00:36:02,339
tegengestelde richtingen, dus we kijken of we

976
00:36:02,339 --> 00:36:04,079
deze orbitaal modelleren  Dynamiek, we krijgen

977
00:36:04,079 --> 00:36:06,000
dit soort soepel bewegende klodders

978
00:36:06,000 --> 00:36:07,619
activiteit in onze latente ruimte. Als we

979
00:36:07,619 --> 00:36:09,839
een slinger modelleren, krijgen we op dezelfde manier een

980
00:36:09,839 --> 00:36:13,820
soort complexe oscillerende activiteit,

981
00:36:14,099 --> 00:36:17,339
dus de invoerstructuur blijft behouden, maar

982
00:36:17,339 --> 00:36:19,560
bovendien meer flexibiliteit dan

983
00:36:19,560 --> 00:36:21,599
voorheen, wat een beetje ons uiteindelijke doel is.

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
dus tot slot wil ik het even hebben over

986
00:36:26,099 --> 00:36:28,320
hoe ik denk dat de uitkomst van dit onderzoek

987
00:36:28,320 --> 00:36:30,420
niet alleen de kunstmatige intelligentie kan verbeteren,

988
00:36:30,420 --> 00:36:32,099
maar ook hoe het ons helpt

989
00:36:32,099 --> 00:36:34,440
begrijpen waarom onze metingen van de

990
00:36:34,440 --> 00:36:36,240
hersenen er zo uitzien zoals ze doen. Om een

991
00:36:36,240 --> 00:36:38,579
kort voorbeeld te geven van wat ik denk  bedoel hiermee, uh,

992
00:36:38,579 --> 00:36:41,040
ik heb eerder wat gesproken over visa

993
00:36:41,040 --> 00:36:43,740
en plaatsen, dus in dit fantastische werk

994
00:36:43,740 --> 00:36:46,859
met Ching higao hebben we onderzocht of onze

995
00:36:46,859 --> 00:36:48,900
eenvoudige topografische prior, zoals we bespraken,

996
00:36:48,900 --> 00:36:50,579
dezelfde effecten zou kunnen reproduceren,

997
00:36:50,579 --> 00:36:53,339
dus specifiek hebben we de waarde

998
00:36:53,339 --> 00:36:56,099
van deze Cohen's D  selectiviteitsmetriek voor

999
00:36:56,099 --> 00:36:58,200
elk van onze neuronen met betrekking tot een

1000
00:36:58,200 --> 00:37:00,000
andere dataset van afbeeldingen die mogelijk

1001
00:37:00,000 --> 00:37:02,460
alleen gezichten of alleen objecten of lichamen bevatten

1002
00:37:02,460 --> 00:37:03,359


1003
00:37:03,359 --> 00:37:05,880
en dus meten we voor elk neuron of het

1004
00:37:05,880 --> 00:37:07,920
waarschijnlijker zal reageren op gezichten of dat de

1005
00:37:07,920 --> 00:37:10,380
Rus in de hersenen verschijnt, maar dat doe ik wel

1006
00:37:10,380 --> 00:37:12,839
denk dat het ons vertelt dat de relatieve

1007
00:37:12,839 --> 00:37:15,300
organisatie van selectiviteit op zijn minst

1008
00:37:15,300 --> 00:37:17,400
gedeeltelijk kan worden toegeschreven aan

1009
00:37:17,400 --> 00:37:19,800
correlatiestatistieken in de gegevens die moeten worden opnieuw gepath

1010
00:37:19,800 --> 00:37:21,359
nadat ze door een zeer

1011
00:37:21,359 --> 00:37:23,640
niet-lineaire toekomstextractor zijn gegaan, zoals een

1012
00:37:23,640 --> 00:37:25,440
diep neuraal netwerk,

1013
00:37:25,440 --> 00:37:27,480
dus in dezelfde geest iets dat is

1014
00:37:27,480 --> 00:37:29,040
interessant, er is een bekende

1015
00:37:29,040 --> 00:37:30,900
zogenaamde tripartiete of niet de visuele

1016
00:37:30,900 --> 00:37:36,720
stroom, dus uh afbeeldingen van uh of objecten zijn

1017
00:37:36,720 --> 00:37:38,400
de selectiviteit met betrekking tot objecten

1018
00:37:38,400 --> 00:37:40,680
is georganiseerd door meer abstracte eigenschappen

1019
00:37:40,680 --> 00:37:43,200
zoals animiteit, is dit ding levend of

1020
00:37:43,200 --> 00:37:46,200
levenloos uh versus ook objectgrootte in de echte wereld,

1021
00:37:46,200 --> 00:37:48,480
zoals wat  is de grootte van een

1022
00:37:48,480 --> 00:37:50,700
theepot versus een auto,

1023
00:37:50,700 --> 00:37:53,520
en wat we zien is dat bij

1024
00:37:53,520 --> 00:37:56,160
mensen de selectiviteit is georganiseerd

1025
00:37:56,160 --> 00:37:57,540
in deze tripartiete structuur, je

1026
00:37:57,540 --> 00:37:59,760
hebt meestal kleine objecten die

1027
00:37:59,760 --> 00:38:01,920
tussen levende en levenloze objecten in zitten

1028
00:38:01,920 --> 00:38:04,200
in termen van hun selectiviteit en we zien

1029
00:38:04,200 --> 00:38:06,060
hetzelfde gebeurt hier, dus

1030
00:38:06,060 --> 00:38:07,440
deze meten de selectiviteit van

1031
00:38:07,440 --> 00:38:08,880
dezelfde reeks neuronen, maar met betrekking

1032
00:38:08,880 --> 00:38:10,859
tot deze verschillen in stimuli zie je

1033
00:38:10,859 --> 00:38:12,440
dat het kleine cluster zich tussen een

1034
00:38:12,440 --> 00:38:14,820
levend en levenloos cluster bevindt en opnieuw

1035
00:38:14,820 --> 00:38:16,079
gebeurt dit voor meerdere verschillende

1036
00:38:16,079 --> 00:38:18,900
initialisaties, dus dit  is iets waarvan ik

1037
00:38:18,900 --> 00:38:20,880
hoop dat we het voor deze gemeenschap wat verder kunnen onderzoeken.

1038
00:38:20,880 --> 00:38:22,980
Ik denk dat het interessant is

1039
00:38:22,980 --> 00:38:24,119
omdat het

1040
00:38:24,119 --> 00:38:26,220
echt een manier is om te laten zien dat we

1041
00:38:26,220 --> 00:38:28,200
een gestructureerd wereldmodel hebben gebouwd en

1042
00:38:28,200 --> 00:38:30,119
mogelijk is dit wereldmodel

1043
00:38:30,119 --> 00:38:31,980
nuttig voor het

1044
00:38:31,980 --> 00:38:34,740
beter weergeven van gegevens uit de echte wereld op een

1045
00:38:34,740 --> 00:38:37,619
gestructureerde manier en  je krijgt

1046
00:38:37,619 --> 00:38:39,119
in die zin een lagere vrije energie,

1047
00:38:39,119 --> 00:38:40,619
dus ik

1048
00:38:40,619 --> 00:38:42,300
denk dat we door deze modellen te ontwikkelen

1049
00:38:42,300 --> 00:38:44,400
zoals we hier hebben laten zien, inzichten kunnen krijgen

1050
00:38:44,400 --> 00:38:46,500
in nieuwe mechanismen voor hoe

1051
00:38:46,500 --> 00:38:48,900
deze structuur ontstaat, inclusief een

1052
00:38:48,900 --> 00:38:50,460
topografische organisatie waar we nog nooit

1053
00:38:50,460 --> 00:38:52,920
eerder aan hebben gedacht, dus het machinemodel waar ik naar

1054
00:38:52,920 --> 00:38:55,520
keek  de oriëntatieselectiviteit

1055
00:38:55,520 --> 00:38:58,260
van neuronen waarvan ik niet

1056
00:38:58,260 --> 00:39:01,020
echt verwachtte dat er iets zou

1057
00:39:01,020 --> 00:39:03,420
gebeuren, maar je kijkt naar een soort van

1058
00:39:03,420 --> 00:39:05,339
deze golven die zich voortplanten over dit

1059
00:39:05,339 --> 00:39:08,099
gesimuleerde verticale oppervlak en ik dacht:

1060
00:39:08,099 --> 00:39:09,960
oké, misschien laat ik geroteerde beelden zien,

1061
00:39:09,960 --> 00:39:11,820
misschien heeft dit enig effect op  de

1062
00:39:11,820 --> 00:39:13,740
oriëntatieselectiviteit

1063
00:39:13,740 --> 00:39:15,599
en eigenlijk, als je naar binnen gaat en

1064
00:39:15,599 --> 00:39:17,460
de selectiviteit van elk neuron meet

1065
00:39:17,460 --> 00:39:18,660
met betrekking tot deze verschillend

1066
00:39:18,660 --> 00:39:22,079
georiënteerde lijnen, wat je ziet is dat het

1067
00:39:22,079 --> 00:39:24,300
verrassend doet denken aan het Orient-

1068
00:39:24,300 --> 00:39:25,859
type kolommen die je ziet in de primaire

1069
00:39:25,859 --> 00:39:27,599
visuele cortex, dit is iets dat teruggaat

1070
00:39:27,599 --> 00:39:29,520
tegen Hugo en Wezel en dit is iets

1071
00:39:29,520 --> 00:39:30,900
dat zojuist uit dit model voortkwam

1072
00:39:30,900 --> 00:39:33,060
en het feit dat het de ruimtelijke,

1073
00:39:33,060 --> 00:39:34,440
temporele structuur heeft met betrekking tot

1074
00:39:34,440 --> 00:39:37,619
Transformaties, dus dit is natuurlijk

1075
00:39:37,619 --> 00:39:39,599
een heel grove analogie, maar ik denk dat dit

1076
00:39:39,599 --> 00:39:40,740
een voorbeeld is van hoe  Het bouwen van dit

1077
00:39:40,740 --> 00:39:42,839
soort modellen kan ons helpen nadenken over

1078
00:39:42,839 --> 00:39:45,240
hoe de hersenen

1079
00:39:45,240 --> 00:39:46,980
de representatiestructuur opbouwen en hoe het wit is

1080
00:39:46,980 --> 00:39:48,660
georganiseerd op een manier waar we misschien nog niet

1081
00:39:48,660 --> 00:39:51,300
eerder over hebben nagedacht.

1082
00:39:51,300 --> 00:39:53,460
Ik denk dat ik niet de enige ben die

1083
00:39:53,460 --> 00:39:55,859
dit soort modellen doet.  werk en dus

1084
00:39:55,859 --> 00:39:57,240
wil ik het even hebben over een aantal

1085
00:39:57,240 --> 00:39:59,579
andere mensen die dit doen, dus

1086
00:39:59,579 --> 00:40:00,780
ik heb het gehad over deze

1087
00:40:00,780 --> 00:40:02,760
gelijkwaardige structuur,

1088
00:40:02,760 --> 00:40:04,920
mensen zoals James Whittington en

1089
00:40:04,920 --> 00:40:08,880
Tim Barons en surrogengoolie uh hebben

1090
00:40:08,880 --> 00:40:10,680
onlangs aangetoond dat door de introductie van

1091
00:40:10,680 --> 00:40:14,940
algebraïsche  beperkingen in uh in een

1092
00:40:14,940 --> 00:40:17,040
leerproces, in dit geval was het

1093
00:40:17,040 --> 00:40:20,820
als de beweging van uh en agenten in

1094
00:40:20,820 --> 00:40:23,220
een omgeving door te zeggen dat je een

1095
00:40:23,220 --> 00:40:24,780
soort van deze algebraïsche

1096
00:40:24,780 --> 00:40:27,540
structuur moet behouden als ik in een cirkel west-

1097
00:40:27,540 --> 00:40:29,280
noordoost-zuid beweeg, eindig ik weer op

1098
00:40:29,280 --> 00:40:31,440
hetzelfde  punt Nogmaals, door dit

1099
00:40:31,440 --> 00:40:32,820
soort beperkingen te introduceren,

1100
00:40:32,820 --> 00:40:35,040
krijg je de opkomst van rastercelachtige

1101
00:40:35,040 --> 00:40:36,900
representaties,

1102
00:40:36,900 --> 00:40:39,359
dus ik zou graag willen zien hoe dit

1103
00:40:39,359 --> 00:40:41,880
idee van representatiestructuur

1104
00:40:41,880 --> 00:40:43,980
ons kan helpen misschien meer te verklaren dan onze

1105
00:40:43,980 --> 00:40:45,480
wetenschappelijke bevindingen die we ook vinden,

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
eh en  en hoe dit zich verhoudt tot

1108
00:40:48,359 --> 00:40:51,540
generatieve modellen als

1109
00:40:51,540 --> 00:40:52,800
geheel, en tot slot denk ik dat er ook

1110
00:40:52,800 --> 00:40:54,599
iets te zeggen valt over de cognitieve

1111
00:40:54,599 --> 00:40:56,099
mogelijkheden van deze modellen.

1112
00:40:56,099 --> 00:40:57,420
Misschien gaan we ze niet alleen testen

1113
00:40:57,420 --> 00:40:59,579
vanuit het neurowetenschappelijk perspectief,

1114
00:40:59,579 --> 00:41:01,020
maar ook vanuit het perspectief van de micrognitief wetenschap.

1115
00:41:01,020 --> 00:41:03,839
Er zijn bijvoorbeeld deze uh

1116
00:41:03,839 --> 00:41:06,000
Ravens Progressive-matrices aan de linkerkant,

1117
00:41:06,000 --> 00:41:08,640
waar je moet zeggen welke van

1118
00:41:08,640 --> 00:41:11,099
deze afbeeldingen waarschijnlijker in

1119
00:41:11,099 --> 00:41:12,599
dit patroon past,

1120
00:41:12,599 --> 00:41:14,760
eh of bijvoorbeeld. Hoe waarschijnlijk is het dat

1121
00:41:14,760 --> 00:41:16,740
deze Jenga-toren omvalt als je

1122
00:41:16,740 --> 00:41:19,740
aan de kant trekt?  een specifiek blok of of

1123
00:41:19,740 --> 00:41:22,740
met een bepaalde structuur en ik denk dat

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
dit soort tests echt testen

1126
00:41:26,640 --> 00:41:28,619
of onze wereldmodellen die we bouwen

1127
00:41:28,619 --> 00:41:31,560
vergelijkbaar zijn met het soort modellen waarvan

1128
00:41:31,560 --> 00:41:33,660
we van nature ons gezond verstand hebben

1129
00:41:33,660 --> 00:41:36,060
als mens of  als wezens die in een

1130
00:41:36,060 --> 00:41:38,400
natuurlijke wereld leven en ik heb wat

1131
00:41:38,400 --> 00:41:40,440
voorbereidend werk in deze richting gedaan, ik

1132
00:41:40,440 --> 00:41:43,079
denk dat het heel voorlopig is en lang niet zo

1133
00:41:43,079 --> 00:41:45,480
ingewikkeld, maar een soort poging

1134
00:41:45,480 --> 00:41:47,820
om visuele illusies te modelleren, dus als je

1135
00:41:47,820 --> 00:41:50,520
een heel eenvoudige dataset van een bewegende balk neemt

1136
00:41:50,520 --> 00:41:52,980
stimuli of een statische balk of frame en je

1137
00:41:52,980 --> 00:41:54,960
beweegt het een beetje, je kunt zien dat

1138
00:41:54,960 --> 00:41:57,060
het model daadwerkelijk dat

1139
00:41:57,060 --> 00:41:58,800
ontbrekende frame zal afleiden en dan feitelijk ook een

1140
00:41:58,800 --> 00:42:01,079
voortgezette beweging zal afleiden, dus het is alsof je

1141
00:42:01,079 --> 00:42:03,300
het traject overschrijdt van wat de

1142
00:42:03,300 --> 00:42:05,820
feitelijke stimuli het bieden voordat het

1143
00:42:05,820 --> 00:42:08,760
opnieuw corrigeert  dus ik denk dat het modelleren van

1144
00:42:08,760 --> 00:42:10,320
Illusies zeker een interessante

1145
00:42:10,320 --> 00:42:12,660
manier is om te bestuderen of onze wereldmodellen

1146
00:42:12,660 --> 00:42:14,760
vergelijkbaar zijn met de soorten modellen die we

1147
00:42:14,760 --> 00:42:16,619
zelf hebben,

1148
00:42:16,619 --> 00:42:19,619
dus tot slot, ja, ik denk dat

1149
00:42:19,619 --> 00:42:21,900
we topografische priors zouden kunnen laten zien dat

1150
00:42:21,900 --> 00:42:23,220
ze effectief gestructureerde

1151
00:42:23,220 --> 00:42:24,839
representaties of gestructureerde

1152
00:42:24,839 --> 00:42:26,700
wereldmodellen hebben geleerd  De aangeleerde structuur is

1153
00:42:26,700 --> 00:42:29,160
flexibel en aanpasbaar aan willekeurige

1154
00:42:29,160 --> 00:42:30,780
transformaties, in tegenstelling tot traditionele

1155
00:42:30,780 --> 00:42:33,720
equivalenten en topografische aanbieders,

1156
00:42:33,720 --> 00:42:35,579
kunnen statistisch worden geïnduceerd zoals we deden

1157
00:42:35,579 --> 00:42:37,619
in de topografische vae of via

1158
00:42:37,619 --> 00:42:39,480
Dynamics, zoals we lieten zien in deze

1159
00:42:39,480 --> 00:42:42,000
modellen van het neurale golfmachinetype,

1160
00:42:42,000 --> 00:42:44,460
dus om af te ronden laat ik dit achter

1161
00:42:44,460 --> 00:42:46,980
citaat dat ik vond in Fukushima's artikel

1162
00:42:46,980 --> 00:42:50,280
uit 1980, waarvan ik dacht dat het zijn tijd behoorlijk ver vooruit was,

1163
00:42:50,280 --> 00:42:52,079
waarin hij zegt dat als we

1164
00:42:52,079 --> 00:42:53,520
een neuraal netwerkmodel zouden kunnen maken dat

1165
00:42:53,520 --> 00:42:55,020
hetzelfde vermogen tot

1166
00:42:55,020 --> 00:42:57,060
patroonherkenning heeft als een mens, het

1167
00:42:57,060 --> 00:42:58,800
ons een krachtige aanwijzing zou geven vergeleken met

1168
00:42:58,800 --> 00:43:00,000
het neurale mechanisme in

1169
00:43:00,000 --> 00:43:03,240
de hersenen begrijpen, dus dat is een beetje een

1170
00:43:03,240 --> 00:43:06,119
van de doelen waar we hier voor gaan,

1171
00:43:06,119 --> 00:43:08,220
dus ik denk dat het mijn adviseur is Max, mijn

1172
00:43:08,220 --> 00:43:11,280
co-auteurs Patrick UA Emil jinghian en

1173
00:43:11,280 --> 00:43:17,359
Yorn en geïnteresseerd in discussie, bedankt oké

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
oké  bedankt geweldig, heel uh

1176
00:43:27,480 --> 00:43:31,079
interessante presentatie,

1177
00:43:31,079 --> 00:43:33,480
veel plaatsen om te beginnen, misschien gewoon

1178
00:43:33,480 --> 00:43:36,000
wat je naar dit werk bracht,

1179
00:43:36,000 --> 00:43:38,520
een beetje context over hoe je in

1180
00:43:38,520 --> 00:43:43,819
dit werk terechtkwam voor je PhD-richting,

1181
00:43:43,920 --> 00:43:45,119
ja,

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
ik bedoel, we hebben gestudeerd, niet mijn

1184
00:43:49,200 --> 00:43:51,000
groep die  Ik zit op de universiteit en

1185
00:43:51,000 --> 00:43:52,700
bestudeer al een tijdje gestructureerde representaties

1186
00:43:52,700 --> 00:43:56,640
vanuit wiskundig oogpunt,

1187
00:43:56,640 --> 00:43:58,319
waar sommige mensen

1188
00:43:58,319 --> 00:44:00,240
modellen hebben, zoals de variatieve

1189
00:44:00,240 --> 00:44:01,740
automatische encoder,

1190
00:44:01,740 --> 00:44:04,680
eh, en

1191
00:44:04,680 --> 00:44:06,960
ik denk dat wat altijd al

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
een model was geweest  dat respecteert rotaties 2D-

1194
00:44:11,220 --> 00:44:13,560
rotaties prima, maar als we

1195
00:44:13,560 --> 00:44:15,960
3D-rotaties willen doen, kunnen we dat niet doen,

1196
00:44:15,960 --> 00:44:17,819
want dat is geen groep in termen van een

1197
00:44:17,819 --> 00:44:19,740
projectie op een 2D-plan. Je

1198
00:44:19,740 --> 00:44:21,180
verliest informatie wanneer dit ding

1199
00:44:21,180 --> 00:44:23,460
bijvoorbeeld ronddraait,

1200
00:44:23,460 --> 00:44:24,240
eh

1201
00:44:24,240 --> 00:44:26,280
of  gewoon elke vorm van natuurlijke

1202
00:44:26,280 --> 00:44:27,960
transformaties zoals ik

1203
00:44:27,960 --> 00:44:29,339
in het begin probeerde aan te geven. Ik denk dat het

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
probeerde na te denken over hoe het brein

1206
00:44:31,740 --> 00:44:34,020
natuurlijke transformaties modelleert.

1207
00:44:34,020 --> 00:44:35,400
Dit is iets dat in deze huidige

1208
00:44:35,400 --> 00:44:37,200
raamwerken voorkomt,

1209
00:44:37,200 --> 00:44:41,099
waar zie je dat actie een rol speelt

1210
00:44:41,099 --> 00:44:44,579
in termen van variabele auto-encoder?

1211
00:44:44,579 --> 00:44:48,420
modellen die niet alleen

1212
00:44:48,420 --> 00:44:50,520
externe patronen omvatten, maar ook de

1213
00:44:50,520 --> 00:44:52,380
gevolgen van actie of de

1214
00:44:52,380 --> 00:44:55,800
structurele structuur van het wereldmodel met actie, ja

1215
00:44:55,800 --> 00:44:58,619
nee, dat is een goede vraag en

1216
00:44:58,619 --> 00:45:01,319
ik denk dat gevolgtrekkingen zijn uitgevoerd, uh

1217
00:45:01,319 --> 00:45:03,839
is in feite het antwoord. Ik denk dat

1218
00:45:03,839 --> 00:45:05,940
het daar een goed antwoord op is,

1219
00:45:05,940 --> 00:45:09,000
ik weet het daar  zijn

1220
00:45:09,000 --> 00:45:11,099
raamwerken voor versterkend leren die een

1221
00:45:11,099 --> 00:45:12,660


1222
00:45:12,660 --> 00:45:15,060
soort extern getrainde wereldmodellen gebruiken,

1223
00:45:15,060 --> 00:45:17,280
dus je traint een vae of zoiets en dan

1224
00:45:17,280 --> 00:45:19,800
gebruik je die representatie in je

1225
00:45:19,800 --> 00:45:23,040
versterkende leersysteem, maar ik

1226
00:45:23,040 --> 00:45:24,720
denk dat je een volledig

1227
00:45:24,720 --> 00:45:26,520
soort systeem hebt dat één enkel

1228
00:45:26,520 --> 00:45:30,780
doel heeft met actie  als onderdeel van de

1229
00:45:30,780 --> 00:45:33,660
waarschijnlijkheid van de gegevens en uh

1230
00:45:33,660 --> 00:45:35,280
ja, ik denk dat dat veel eleganter is

1231
00:45:35,280 --> 00:45:38,940
en dus ben ik daar een groot voorstander van.

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
Ik ben nog niet zo ver gekomen om te bestuderen hoe

1234
00:45:43,140 --> 00:45:45,480
deze gestructureerde wereldmodellen in een vae zijn

1235
00:45:45,480 --> 00:45:47,520
of ik heb '  Daar heb ik helemaal niet aan gewerkt, maar ik

1236
00:45:47,520 --> 00:45:48,780
denk dat het zeker heel

1237
00:45:48,780 --> 00:45:50,819
interessant zou zijn om te zien of het hebben van een meer

1238
00:45:50,819 --> 00:45:52,339
gestructureerd wereldmodel,

1239
00:45:52,339 --> 00:45:54,839
uh in een variabele automatische encoder,

1240
00:45:54,839 --> 00:45:56,880
daarin ook nuttig zou zijn in een actieve

1241
00:45:56,880 --> 00:45:58,319
omgeving. Ik denk dat dat geweldig zou zijn.

1242
00:45:58,319 --> 00:46:00,119
Ik bedoel, ik  denk dat

1243
00:46:00,119 --> 00:46:03,599
sommige van deze voorbeelden, zoals eerder laten zien,

1244
00:46:03,599 --> 00:46:05,579
zoals de opkomst van rastercellen en

1245
00:46:05,579 --> 00:46:07,500
dingen als deze, misschien in

1246
00:46:07,500 --> 00:46:08,880
die richting wijzen,

1247
00:46:08,880 --> 00:46:10,560
oké, misschien zijn de hersenen iets aan het doen,

1248
00:46:10,560 --> 00:46:12,540
het heeft echt duidelijk veel

1249
00:46:12,540 --> 00:46:13,680
structuur,

1250
00:46:13,680 --> 00:46:15,359
dit moet duidelijk nuttig zijn voor

1251
00:46:15,359 --> 00:46:19,140
het uitvoeren van acties in sommige gevallen

1252
00:46:19,140 --> 00:46:21,720
Oh ja, ik voelde een hele

1253
00:46:21,720 --> 00:46:24,480
mooie parallel die je naar voren bracht met

1254
00:46:24,480 --> 00:46:28,040
de lezing. De lokaal verbonden eenheden

1255
00:46:28,040 --> 00:46:30,960
stelden je modellen in staat om structureel

1256
00:46:30,960 --> 00:46:33,780
de convolutionele

1257
00:46:33,780 --> 00:46:35,640
beperking en het patroon te belichamen en dat leidde tot

1258
00:46:35,640 --> 00:46:37,500
deze ontstaande patronen en dan

1259
00:46:37,500 --> 00:46:41,339
was er analoog de uh Doral

1260
00:46:41,339 --> 00:46:45,680
waar  ze hadden de

1261
00:46:45,680 --> 00:46:48,359
beperking voor padverkenning goed en dus is het

1262
00:46:48,359 --> 00:46:50,280
interessant om, eh,

1263
00:46:50,280 --> 00:46:53,760
je weet wel, na te denken over deze actie- of

1264
00:46:53,760 --> 00:46:56,819
beleidsheuristieken of sparsities als een

1265
00:46:56,819 --> 00:46:59,579
gezamenlijke motorische verkenning. Uiteindelijk

1266
00:46:59,579 --> 00:47:02,339
wordt duidelijk dat er twee

1267
00:47:02,339 --> 00:47:04,980
onderling tegengestelde manieren zijn om een ​​gewricht te bewegen

1268
00:47:04,980 --> 00:47:07,079
en dan de compositoriteit  over

1269
00:47:07,079 --> 00:47:09,119
gewrichten kan op deze

1270
00:47:09,119 --> 00:47:10,680
hogere niveaus worden geleerd zodra het op

1271
00:47:10,680 --> 00:47:14,480
lagere niveaus is opgesloten, dus het is een zeer aantrekkelijke

1272
00:47:14,480 --> 00:47:17,599
en uh

1273
00:47:17,599 --> 00:47:20,460
uh niche-relevante manier om te generaliseren,

1274
00:47:20,460 --> 00:47:23,819
omdat het zowel gebaseerd is op de feitelijke

1275
00:47:23,819 --> 00:47:25,740
beperkingen van de wereld, maar dan

1276
00:47:25,740 --> 00:47:27,720
vooral door middel van actie die

1277
00:47:27,720 --> 00:47:29,460
mogelijk iets insluit dat is

1278
00:47:29,460 --> 00:47:31,380
heel simpel toch ja

1279
00:47:31,380 --> 00:47:33,599
nee ik denk dat dat zeker

1280
00:47:33,599 --> 00:47:36,599
waar is, dat is een heel goed punt, als

1281
00:47:36,599 --> 00:47:38,339
je beperkingen hebt die voortkomen uit je

1282
00:47:38,339 --> 00:47:40,500
acties zelf, dan

1283
00:47:40,500 --> 00:47:42,839
zou dat enorm nuttig zijn om

1284
00:47:42,839 --> 00:47:44,819


1285
00:47:44,819 --> 00:47:47,460
je latente ruimte te helpen structureren en ik denk ja, ik

1286
00:47:47,460 --> 00:47:48,480
denk één ding  Ik wilde vermelden dat er

1287
00:47:48,480 --> 00:47:49,980


1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
iets is waar ik aan moest denken, zoals het

1290
00:47:52,740 --> 00:47:55,500
werk van Stefano Fousey over een soort

1291
00:47:55,500 --> 00:47:58,859
representatieve geometrie, die

1292
00:47:58,859 --> 00:48:01,920
bepaalt hoe we

1293
00:48:01,920 --> 00:48:04,440
hoe generaliseerbaar een bepaald begrip

1294
00:48:04,440 --> 00:48:08,099
van een systeem is, en ik denk dat als je

1295
00:48:08,099 --> 00:48:11,880
deze reeks activiteiten kunt begrijpen,

1296
00:48:11,880 --> 00:48:14,520
scheidbaar of zeer

1297
00:48:14,520 --> 00:48:16,079
parallel scheidbaar met een lineaire

1298
00:48:16,079 --> 00:48:18,839
classificator, dan kun je in wezen

1299
00:48:18,839 --> 00:48:20,700
generaliseren en ik

1300
00:48:20,700 --> 00:48:23,099
denk dat je door dit soort vooroordelen op te leggen

1301
00:48:23,099 --> 00:48:25,040
of mogelijk door

1302
00:48:25,040 --> 00:48:27,000
beperkingen die worden opgelegd door

1303
00:48:27,000 --> 00:48:28,740
iets als deze,

1304
00:48:28,740 --> 00:48:32,040
een

1305
00:48:32,040 --> 00:48:33,660
betere representatieve geometrie en

1306
00:48:33,660 --> 00:48:35,220
dit heeft allerlei voordelen zoals

1307
00:48:35,220 --> 00:48:36,660
compositoriteit

1308
00:48:36,660 --> 00:48:39,359
ja onze generalisatie dus

1309
00:48:39,359 --> 00:48:41,760
het is een geweldig punt cool ja heel

1310
00:48:41,760 --> 00:48:43,440
interessant gebied oké ik zal

1311
00:48:43,440 --> 00:48:45,960
een paar vragen lezen van de livechat

1312
00:48:45,960 --> 00:48:48,420
liefde evolueren schreef

1313
00:48:48,420 --> 00:48:52,260
praktische of waargenomen beperkingen op het

1314
00:48:52,260 --> 00:48:55,579
modelleren van illusies

1315
00:48:58,800 --> 00:49:00,420
leren  gemeenschappen zijn niet

1316
00:49:00,420 --> 00:49:03,060
gefoobeerd, je hebt geen middelpunt van

1317
00:49:03,060 --> 00:49:05,940
blik, dan heb je ook geen

1318
00:49:05,940 --> 00:49:07,940
eh,

1319
00:49:08,339 --> 00:49:11,460
zoals een tijd. Ik bedoel de meeste convolutionele

1320
00:49:11,460 --> 00:49:13,260
neurale netwerken. Ik gebruik dit soort

1321
00:49:13,260 --> 00:49:15,599
terugkerende neurale netwerken, maar de tijd is

1322
00:49:15,599 --> 00:49:18,420
niet zo duidelijk gedefinieerd  in deze modellen

1323
00:49:18,420 --> 00:49:20,220
omdat het zich in een continue tijdssetting bevindt

1324
00:49:20,220 --> 00:49:23,400
voor een mens die een illusieproef ondergaat,

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
eh

1327
00:49:25,319 --> 00:49:27,480
en ik denk dat de combinatie van deze twee

1328
00:49:27,480 --> 00:49:30,359
van het feit dat je als mens of de meeste

1329
00:49:30,359 --> 00:49:33,300
dingen uh

1330
00:49:33,300 --> 00:49:35,940
kijkt, je wisselende locaties en

1331
00:49:35,940 --> 00:49:38,220
je winsten afhankelijk zijn van  alsof je

1332
00:49:38,220 --> 00:49:40,140
naar een bepaald gebied kijkt, veel

1333
00:49:40,140 --> 00:49:42,780
cognitieve tests, en dus denk ik dat het

1334
00:49:42,780 --> 00:49:46,560
erg nuttig zou zijn als we modellen hadden die

1335
00:49:46,560 --> 00:49:48,540
ja, ik bedoel, leer dat je dit kunt zien

1336
00:49:48,540 --> 00:49:50,760
als een soort actie, zoals leren

1337
00:49:50,760 --> 00:49:52,980
waar je je blik naartoe moet bewegen.  de

1338
00:49:52,980 --> 00:49:54,420
eenvoudigst mogelijke die veel zou helpen

1339
00:49:54,420 --> 00:49:56,220
bij het kunnen modelleren van illusies en

1340
00:49:56,220 --> 00:49:58,859
ik bedoel, voor mij is het alsof ik een

1341
00:49:58,859 --> 00:50:00,720
artikel lees over een aantal cognitieve wetenschappelijke

1342
00:50:00,720 --> 00:50:02,940
experimenten of over een of andere illusie en

1343
00:50:02,940 --> 00:50:05,160
het is alsof ik denk: oké, kan ik deze

1344
00:50:05,160 --> 00:50:07,560
dataset in mijn  modelleer en test het en

1345
00:50:07,560 --> 00:50:08,579
meestal is het antwoord nee,

1346
00:50:08,579 --> 00:50:10,619
want ik heb geen model dat

1347
00:50:10,619 --> 00:50:12,900
rondkijkt of een beperkt gezichtsveld heeft,

1348
00:50:12,900 --> 00:50:14,660
zoiets, dus

1349
00:50:14,660 --> 00:50:16,619
ja, ik denk dat dat een van de

1350
00:50:16,619 --> 00:50:19,680
beperkingen is, een andere is,

1351
00:50:19,680 --> 00:50:20,579


1352
00:50:20,579 --> 00:50:22,920
ja, maak de  experiment veel

1353
00:50:22,920 --> 00:50:24,900
ingewikkelder, dus dat is een van de

1354
00:50:24,900 --> 00:50:27,359
praktische beperkingen.

1355
00:50:27,359 --> 00:50:30,240
Wauw, een geweldig antwoord doet me denken aan een

1356
00:50:30,240 --> 00:50:33,920
papier met letters die op een tafel draaien,

1357
00:50:33,920 --> 00:50:36,780
dat is de cijferrotatie. Geweldige punten

1358
00:50:36,780 --> 00:50:38,460
over de foveatie en de dynamiek van

1359
00:50:38,460 --> 00:50:40,079
de illusie. Ik denk dat je inderdaad

1360
00:50:40,079 --> 00:50:42,599
een illusie noemde die  wordt u echter

1361
00:50:42,599 --> 00:50:43,980
genoemd in de generalisatiecontext

1362
00:50:43,980 --> 00:50:46,859
die roteert op het tweedimensionale

1363
00:50:46,859 --> 00:50:49,500
scherm, generaliseert niet naar drie

1364
00:50:49,500 --> 00:50:52,920
dimensies en die dimensionale ineenstorting

1365
00:50:52,920 --> 00:50:55,559
of reductie is de basis van de

1366
00:50:55,559 --> 00:50:58,619
kubusprojectie Illusies en Kubus- en

1367
00:50:58,619 --> 00:51:01,880
figuurrotatie Illusies, het staat op uw scherm

1368
00:51:01,880 --> 00:51:05,280
en er is  een silhouet of er zijn enkele

1369
00:51:05,280 --> 00:51:07,260
dubbelzinnige

1370
00:51:07,260 --> 00:51:09,839
stimuli dat een generatief het bijna

1371
00:51:09,839 --> 00:51:12,359
kritisch of een splitsing is in

1372
00:51:12,359 --> 00:51:13,680
degeneratieve modellen, zodat het

1373
00:51:13,680 --> 00:51:17,160
het op de een of andere manier zou kunnen weergeven

1374
00:51:17,160 --> 00:51:19,920
en dus zijn veel van de schakelende illusies

1375
00:51:19,920 --> 00:51:22,020
alleen maar gebaseerd op de vlakheid van

1376
00:51:22,020 --> 00:51:23,819
beelden

1377
00:51:23,819 --> 00:51:26,280
en de beperkingen en generalisatie

1378
00:51:26,280 --> 00:51:28,740
die door dat

1379
00:51:28,740 --> 00:51:32,460
recht worden onthuld, ja ja, ik denk dat er zelfs Oh

1380
00:51:32,460 --> 00:51:34,859
Ja ergens is, sorry, er is wat werk, of ze

1381
00:51:34,859 --> 00:51:35,880


1382
00:51:35,880 --> 00:51:37,619
kunnen beweren dat mensen een

1383
00:51:37,619 --> 00:51:39,480
driedimensionaal beeld in hun hoofd hebben,

1384
00:51:39,480 --> 00:51:42,000
alsof zelfs Nancy Ken onlangs een of haar

1385
00:51:42,000 --> 00:51:45,119
zijtakken was, maar en laten zien: ja, dat

1386
00:51:45,119 --> 00:51:48,119
doe ik niet  Ik weet het, onze modellen hebben dat

1387
00:51:48,119 --> 00:51:50,460
het toch niet super groot is,

1388
00:51:50,460 --> 00:51:53,700
ja dat is behoorlijk interessant, oké,

1389
00:51:53,700 --> 00:51:56,160
van Upcycle Club in de

1390
00:51:56,160 --> 00:51:58,200
chat, ze schreven een pluim

1391
00:51:58,200 --> 00:52:00,000
als je bijna net zo effectief kunt leren

1392
00:52:00,000 --> 00:52:02,160
als je je voorstelt dat je maar

1393
00:52:02,160 --> 00:52:03,780
één neuron wilt hebben  actief voor elk

1394
00:52:03,780 --> 00:52:06,540
voorbeeld, uh, uw model gaat

1395
00:52:06,540 --> 00:52:08,579
proberen het ontwerp van de dataset te onthouden

1396
00:52:08,579 --> 00:52:10,980
of iets dergelijks,

1397
00:52:10,980 --> 00:52:12,180
en u zult niet genoeg

1398
00:52:12,180 --> 00:52:14,940
capaciteit hebben, dus ja, ik denk dat het afstemmen van dat

1399
00:52:14,940 --> 00:52:18,359
niveau van spaarzaamheid zeker

1400
00:52:18,359 --> 00:52:22,200
een belangrijke factor is, en

1401
00:52:22,200 --> 00:52:25,020
ja  als je naar de waarschijnlijkheid kijkt, als

1402
00:52:25,020 --> 00:52:26,220
je praat, als je het

1403
00:52:26,220 --> 00:52:28,579
raamwerk verdubbelt, wordt dit meestal automatisch in evenwicht gebracht

1404
00:52:28,579 --> 00:52:32,040
met de waarschijnlijkheid zelf,

1405
00:52:32,040 --> 00:52:33,000


1406
00:52:33,000 --> 00:52:34,380
als je geen modellering genereert,

1407
00:52:34,380 --> 00:52:35,760
heb je gewoon een sparsity-straf, waar je op

1408
00:52:35,760 --> 00:52:38,460
wilt afstemmen  die parameter

1409
00:52:38,460 --> 00:52:40,980
oké ja, het is alleen maar om op

1410
00:52:40,980 --> 00:52:43,380
hol geslagen gedrag in Armina te verduidelijken, waar het

1411
00:52:43,380 --> 00:52:45,599
netwerk onstabiel of chaotisch wordt als gevolg

1412
00:52:45,599 --> 00:52:47,040
van verschillende factoren, zoals

1413
00:52:47,040 --> 00:52:50,400
ruis van feedbacklussen of vijandige input,

1414
00:52:50,400 --> 00:52:52,380
eh

1415
00:52:52,380 --> 00:52:54,180
ja, ik denk dat ik hier niet naar heb gekeken

1416
00:52:54,180 --> 00:52:55,859
in een terugkerende omgeving waarin  je

1417
00:52:55,859 --> 00:52:58,500
zou feedbackloops krijgen,

1418
00:52:58,500 --> 00:52:59,460


1419
00:52:59,460 --> 00:53:01,800
maar ik zou kunnen zien dat vijandige

1420
00:53:01,800 --> 00:53:04,319
voorbeelden mogelijk worden

1421
00:53:04,319 --> 00:53:07,800
beïnvloed door jouw niveau van spaarzaamheid.

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
Het interessante punt is waar je

1424
00:53:10,859 --> 00:53:12,660
gevoeliger of minder vatbaar voor zou zijn

1425
00:53:12,660 --> 00:53:16,040
om voorbeelden te delen. Ik weet het niet

1426
00:53:16,040 --> 00:53:19,440
goed, de sparsificatie

1427
00:53:19,440 --> 00:53:21,720
projecteert vanuit een  volledig verbonden hoger

1428
00:53:21,720 --> 00:53:23,579
dimensionaal model, gewoon in

1429
00:53:23,579 --> 00:53:25,619
steeds kleiner,

1430
00:53:25,619 --> 00:53:27,540
het is over het algemeen vrij goed begrepen

1431
00:53:27,540 --> 00:53:29,760
wat de afwegingen zijn, het zijn gemakkelijker

1432
00:53:29,760 --> 00:53:34,079
berekeningen, een kleiner model schaarser,

1433
00:53:34,079 --> 00:53:36,420
de basisgrafiek zal duidelijker

1434
00:53:36,420 --> 00:53:39,119
te vertegenwoordigen zijn en dan zal het ook

1435
00:53:39,119 --> 00:53:41,339
alle andere afwegingen hebben  af met valse

1436
00:53:41,339 --> 00:53:43,680
positieven en negatieven van generaliseren,

1437
00:53:43,680 --> 00:53:45,720
maar daarom is het een iteratief fit-

1438
00:53:45,720 --> 00:53:47,579
proces, dus

1439
00:53:47,579 --> 00:53:49,760
ik denk dat hoe werkt uw

1440
00:53:49,760 --> 00:53:52,800
sparsificatiebenadering, de

1441
00:53:52,800 --> 00:53:55,700
balans

1442
00:53:56,700 --> 00:53:59,520
gebruikt geen AIC of Bic of een andere

1443
00:53:59,520 --> 00:54:01,619
modelaanpassingsbenadering om de

1444
00:54:01,619 --> 00:54:03,660
relevante sparsificatie

1445
00:54:03,660 --> 00:54:07,079
voor een bepaalde input te bepalen,

1446
00:54:07,079 --> 00:54:09,780
hoe gaat u  bepaal hoe zoals bij

1447
00:54:09,780 --> 00:54:11,940
lasso-regressie, zoals hoe weet je

1448
00:54:11,940 --> 00:54:14,339
hoeveel, hoe bepaal je hoeveel

1449
00:54:14,339 --> 00:54:17,220
hoe schaars je wilt dat het klopt?

1450
00:54:17,220 --> 00:54:19,559
Ja, ik denk dat er veel goede

1451
00:54:19,559 --> 00:54:22,440
literatuur hierover is, en toch vinden sommige

1452
00:54:22,440 --> 00:54:25,319
mensen ze leuk op Harvard en  sommige

1453
00:54:25,319 --> 00:54:29,520
mensen werken nu met

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
dit soort uitgerolde

1456
00:54:34,380 --> 00:54:36,780
um iteratieve sparsificatienetwerken

1457
00:54:36,780 --> 00:54:37,800
waar het is als een terugkerend neuraal

1458
00:54:37,800 --> 00:54:40,380
netwerk en iteratief sparsificeert en

1459
00:54:40,380 --> 00:54:41,940
je kunt laten zien dat dit zoiets oplevert

1460
00:54:41,940 --> 00:54:45,780
als rood verlies of uh groep als groep

1461
00:54:45,780 --> 00:54:47,520
actieve groep Sportactivaties zoals

1462
00:54:47,520 --> 00:54:48,960
wij  we gebruiken hier

1463
00:54:48,960 --> 00:54:52,859
eh in deze setting, uh, het is eigenlijk gewoon

1464
00:54:52,859 --> 00:54:55,859
door

1465
00:54:55,859 --> 00:54:59,280
deze constructie van deze T-variabele te hebben

1466
00:54:59,280 --> 00:55:04,079
waar we Z bovenaan hebben en uh

1467
00:55:04,079 --> 00:55:07,859
en dan wordt het op de een of andere manier hierdoor

1468
00:55:07,859 --> 00:55:09,119


1469
00:55:09,119 --> 00:55:11,579
de som van U-variabelen onderaan, dus

1470
00:55:11,579 --> 00:55:13,200
als W  Misschien was ik er niet heel duidelijk over.

1471
00:55:13,200 --> 00:55:16,500
Dit is een matrix die verbindt,

1472
00:55:16,500 --> 00:55:18,359
dat is wat de groep definieert, dus ik

1473
00:55:18,359 --> 00:55:20,400
definieer de Varsity van de groep, uh, die

1474
00:55:20,400 --> 00:55:22,380
al deze U's met elkaar verbindt, en dus

1475
00:55:22,380 --> 00:55:23,940
is het idee,

1476
00:55:23,940 --> 00:55:27,540
uh, zoals hier als alle van de andere

1477
00:55:27,540 --> 00:55:31,740
voorbeelden als al uw gebruik uh niet

1478
00:55:31,740 --> 00:55:35,520
actief is voor een bepaalde t

1479
00:55:35,520 --> 00:55:38,280
of als alle vario's actief zijn voor een bepaalde

1480
00:55:38,280 --> 00:55:41,040
t uh, zal die t-variabele erg

1481
00:55:41,040 --> 00:55:42,780
klein zijn, toch omdat uw noemer

1482
00:55:42,780 --> 00:55:44,339
erg groot zal zijn en dat veroorzaakt

1483
00:55:44,339 --> 00:55:47,160
schaarsheid, dus het is  uh, het is een

1484
00:55:47,160 --> 00:55:49,260
beperkingstevredenheid als je een

1485
00:55:49,260 --> 00:55:51,839
reeks U's hebt die allemaal klein zijn, uh dan is aan

1486
00:55:51,839 --> 00:55:54,480
die beperking voldaan en

1487
00:55:54,480 --> 00:55:57,180
nu mag Z zichzelf een beetje uitdrukken

1488
00:55:57,180 --> 00:56:00,240
en dat is wat dan

1489
00:56:00,240 --> 00:56:02,880
uh soort van ja bereikt wat betreft

1490
00:56:02,880 --> 00:56:06,180
activering  dus dit wordt veroorzaakt door deze

1491
00:56:06,180 --> 00:56:07,020
twee

1492
00:56:07,020 --> 00:56:09,300
divergentietermen, hier zeggen deze,

1493
00:56:09,300 --> 00:56:12,960
zoals hoe ver elke unhc is

1494
00:56:12,960 --> 00:56:15,180
van een gaussiaanse en vervolgens door deze

1495
00:56:15,180 --> 00:56:16,980
constructie van de student T-variabele

1496
00:56:16,980 --> 00:56:20,880
construeren we effectief een schaarse

1497
00:56:20,880 --> 00:56:23,040
eerdere verdeling, alleen van deze

1498
00:56:23,040 --> 00:56:24,839
gaussianen, maar in  termen van de handeling het

1499
00:56:24,839 --> 00:56:27,599
feitelijke doel uh de termen en het

1500
00:56:27,599 --> 00:56:28,920
doel dat we optimaliseren zijn slechts

1501
00:56:28,920 --> 00:56:31,619
deze twee KL-termen die het

1502
00:56:31,619 --> 00:56:34,020
tot op zekere hoogte naar schaarsheid duwen en dit

1503
00:56:34,020 --> 00:56:36,359
wordt automatisch in evenwicht gebracht met de

1504
00:56:36,359 --> 00:56:39,000
waarschijnlijkheidsterm hier via de decoder,

1505
00:56:39,000 --> 00:56:41,220
dus dat doen we niet '  We hebben geen termen die we afstemmen,

1506
00:56:41,220 --> 00:56:42,839
maar we leren de parameters van

1507
00:56:42,839 --> 00:56:44,280
deze verschillende encoders en

1508
00:56:44,280 --> 00:56:48,200
analyseren vervolgens de defecten en noodsituaties.

1509
00:56:48,540 --> 00:56:49,920
Oh

1510
00:56:49,920 --> 00:56:52,859
oké, nog een vraag van Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas die

1512
00:56:55,500 --> 00:56:59,160
schreef over staren en illusie. Kunnen de

1513
00:56:59,160 --> 00:57:01,920
onderzoeken naar constances bij baby's worden

1514
00:57:01,920 --> 00:57:04,260
gescheiden?  naar illusie op een lager niveau Rel

1515
00:57:04,260 --> 00:57:06,140
misschien conceptuele constantheid op een hoger niveau,

1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
kunt u

1518
00:57:15,720 --> 00:57:18,300
het huidige soort architectuur lezen, kunnen

1519
00:57:18,300 --> 00:57:23,520
de onderzoeken naar constances bij zuigelingen,

1520
00:57:23,520 --> 00:57:26,640
eh cognitieve constances, worden gescheiden

1521
00:57:26,640 --> 00:57:31,619
ja waarschijnlijk ben ik dat niet. Ik ben geen

1522
00:57:31,619 --> 00:57:33,900
expert of eigenlijk zelfs heel bekend

1523
00:57:33,900 --> 00:57:35,460
met

1524
00:57:35,460 --> 00:57:37,859
objectduurzaamheidsstudies en baby's

1525
00:57:37,859 --> 00:57:40,260
en constantheidsdingen, dus maar ik denk dat dat

1526
00:57:40,260 --> 00:57:42,300
ongelooflijk interessant zou zijn om

1527
00:57:42,300 --> 00:57:44,339
in neurale netwerkarchitecturen te studeren en

1528
00:57:44,339 --> 00:57:46,740
dat was een beetje het idee met

1529
00:57:46,740 --> 00:57:48,780
deze uh-illusie die ik

1530
00:57:48,780 --> 00:57:51,780
hier met deze regel probeerde te modelleren  Ik

1531
00:57:51,780 --> 00:57:53,099
weet niet of ik hier heel duidelijk over was, maar

1532
00:57:53,099 --> 00:57:55,380
de bovenste rij is de invoer en we lijken

1533
00:57:55,380 --> 00:57:57,359
in feite op het blokkeren van de invoer voor

1534
00:57:57,359 --> 00:58:00,119
een enkel frame en ik wilde zien of

1535
00:58:00,119 --> 00:58:03,240
het netwerk codeert dat het

1536
00:58:03,240 --> 00:58:05,400
ding er nog steeds is  als dat frame

1537
00:58:05,400 --> 00:58:07,680
weg is, kan ik dan nog steeds de aanwezigheid van

1538
00:58:07,680 --> 00:58:10,020
het object decoderen uit de neurale activiteit,

1539
00:58:10,020 --> 00:58:11,819
en wat concludeert het dan ook over

1540
00:58:11,819 --> 00:58:13,920
de beweging vanwege het feit dat het

1541
00:58:13,920 --> 00:58:15,780
de balken op een iets andere

1542
00:58:15,780 --> 00:58:18,480
locatie zag dan voorheen, toen de na

1543
00:58:18,480 --> 00:58:20,520
de  frame is verdwenen,

1544
00:58:20,520 --> 00:58:22,559
dus

1545
00:58:22,559 --> 00:58:25,200
ja, ik denk dat het zeker meerdere

1546
00:58:25,200 --> 00:58:27,240
niveaus zijn,

1547
00:58:27,240 --> 00:58:29,160
waar sommige waarschijnlijk een veel

1548
00:58:29,160 --> 00:58:33,180
lager niveau zouden zijn en

1549
00:58:33,180 --> 00:58:35,880
misschien zou de duurzaamheid van objecten op de lange termijn een

1550
00:58:35,880 --> 00:58:37,380
aanzienlijk

1551
00:58:37,380 --> 00:58:39,059
hoger niveau zijn,

1552
00:58:39,059 --> 00:58:39,900


1553
00:58:39,900 --> 00:58:41,760
het doet me gewoon denken aan die

1554
00:58:41,760 --> 00:58:44,640
experimenten met katten  vroeger was

1555
00:58:44,640 --> 00:58:47,280
het alsof ze ze in het

1556
00:58:47,280 --> 00:58:49,020
donker opvoedden, behalve een uur per dag. Ze

1557
00:58:49,020 --> 00:58:51,000
plaatsten ze in de verticale wereld of de horizontale

1558
00:58:51,000 --> 00:58:53,160
wereld, of ze zagen alleen horizontale lijnen

1559
00:58:53,160 --> 00:58:57,299
of verticale lijnen, en je kunt zien dat

1560
00:58:57,299 --> 00:58:59,880
de organisatie van hun cortex verandert

1561
00:58:59,880 --> 00:59:02,819
zoals zij  hebben minder ontvankelijkheid, het zijn

1562
00:59:02,819 --> 00:59:04,200
horizontale lijnen als ze nog nooit

1563
00:59:04,200 --> 00:59:06,420
horizontale lijnen hebben gezien en dan

1564
00:59:06,420 --> 00:59:07,980
neem je een stok en zwaai je ermee voor

1565
00:59:07,980 --> 00:59:09,599
hun gezicht en als de stok

1566
00:59:09,599 --> 00:59:11,220
horizontaal is, doen ze gewoon niets,

1567
00:59:11,220 --> 00:59:12,900
het is verticaal, ze slaan erop

1568
00:59:12,900 --> 00:59:14,460
ze proberen het te raken, het is alsof ze

1569
00:59:14,460 --> 00:59:15,900
gewoon letterlijk niet

1570
00:59:15,900 --> 00:59:18,420
voor hun gezicht hoeven te baren, dus ik denk dat dit in dat

1571
00:59:18,420 --> 00:59:20,700
geval het bewijs is van een

1572
00:59:20,700 --> 00:59:24,260
tekort op laag niveau en een visie die

1573
00:59:24,260 --> 00:59:26,940
bijdraagt ​​aan een soort illusie,

1574
00:59:26,940 --> 00:59:28,980
dus ik ik  denk ja, daar zou zeker ook

1575
00:59:28,980 --> 00:59:30,660
een aspect van kunnen zijn bij baby's.

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
Een heel merkwaardig punt dat je naar voren bracht

1578
00:59:36,420 --> 00:59:40,339
was het levende en levenloze

1579
00:59:40,339 --> 00:59:43,619
spruitstuk, waarbij kleine dingen tussenliggend zijn, toch,

1580
00:59:43,619 --> 00:59:45,480


1581
00:59:45,480 --> 00:59:49,140
wat vertegenwoordigt dat

1582
00:59:49,140 --> 00:59:52,319
of of is het omdat ze hanteerbaar zijn

1583
00:59:52,319 --> 00:59:55,619
of zo  Het kan een insect zijn, of het kan

1584
00:59:55,619 --> 00:59:57,839
iets zijn dat alleen door de wind weg kan bewegen,

1585
00:59:57,839 --> 01:00:01,380
of wat zegt dat

1586
01:00:01,380 --> 01:00:04,380
juist? Ja,

1587
01:00:04,380 --> 01:00:08,280
dus dit is werk van Talia Conkle. Ik

1588
01:00:08,280 --> 01:00:11,280
denk dat hij degene was die deze

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
organisatie ontdekte en zij probeerden

1591
01:00:14,880 --> 01:00:16,440
het uit te zoeken.  Ik begrijp het niet

1592
01:00:16,440 --> 01:00:19,500
verkeerd, dus ik raad mensen aan

1593
01:00:19,500 --> 01:00:21,359
haar werk daarover te lezen als ze het een

1594
01:00:21,359 --> 01:00:23,880
tripartiete organisatie noemen, maar als ik het me

1595
01:00:23,880 --> 01:00:25,319
goed herinner, hebben

1596
01:00:25,319 --> 01:00:27,900
ze veel vervolgwerk gedaan over waarom er

1597
01:00:27,900 --> 01:00:30,780
deze organisatie is en

1598
01:00:30,780 --> 01:00:33,180
enig bewijs van

1599
01:00:33,180 --> 01:00:35,700
kromming van deze objecten en een beetje

1600
01:00:35,700 --> 01:00:37,440
zoals de afstand waarop je objecten ziet

1601
01:00:37,440 --> 01:00:40,260
of zoals

1602
01:00:40,260 --> 01:00:43,319
bewegende objecten of misschien meer bochtig

1603
01:00:43,319 --> 01:00:45,599
of er is er, ongeacht wat het

1604
01:00:45,599 --> 01:00:46,859
feitelijke antwoord is, er waren veel

1605
01:00:46,859 --> 01:00:48,720
verschillende hypothesen die voortkwamen

1606
01:00:48,720 --> 01:00:51,720
uit soortgelijke eigenschappen van deze objecten

1607
01:00:51,720 --> 01:00:54,000
misschien eigenschappen op het midden- of laag niveau

1608
01:00:54,000 --> 01:00:56,280
meer dan eigenschappen op een hoger niveau. Ik

1609
01:00:56,280 --> 01:00:57,599
weet nog steeds niet of het precies is

1610
01:00:57,599 --> 01:00:59,339
opgelost of het is alsof

1611
01:00:59,339 --> 01:01:01,500
interactie zoals je zei met de

1612
01:01:01,500 --> 01:01:04,920
objecten de scheiding veroorzaakt of

1613
01:01:04,920 --> 01:01:06,119
eh

1614
01:01:06,119 --> 01:01:09,540
of ja, de algemene vormen van deze

1615
01:01:09,540 --> 01:01:12,299
objecten.  Ik durf te wedden dat het, zoals bij de meeste dingen,

1616
01:01:12,299 --> 01:01:13,980
een combinatie is van al het

1617
01:01:13,980 --> 01:01:16,980
bovenstaande, maar ik denk dat het interessante

1618
01:01:16,980 --> 01:01:18,480
vanuit dit modelleringsperspectief

1619
01:01:18,480 --> 01:01:19,859
is dat

1620
01:01:19,859 --> 01:01:21,480


1621
01:01:21,480 --> 01:01:24,059
dit alleen wordt getraind op

1622
01:01:24,059 --> 01:01:26,819
correlatiestatistieken van de beeldgegevenssets

1623
01:01:26,819 --> 01:01:28,799
zelf, dus dit heeft geen interactie

1624
01:01:28,799 --> 01:01:32,760
heeft geen idee van animiteit, uh, ik bedoel, dit

1625
01:01:32,760 --> 01:01:34,140
is eigenlijk gewoon het trainen van een model op

1626
01:01:34,140 --> 01:01:37,859
Imagenet, alleen afbeeldingen van honden, katten, boten,

1627
01:01:37,859 --> 01:01:40,020
wat dan ook, en toch bereikt het nog steeds dit

1628
01:01:40,020 --> 01:01:41,640
soort organisatie, dus er is een

1629
01:01:41,640 --> 01:01:42,540
soort van.

1630
01:01:42,540 --> 01:01:44,940
Het kunnen semantische kenmerken zijn,

1631
01:01:44,940 --> 01:01:46,740
toch, we hebben een beeld, we hebben een  netwerk

1632
01:01:46,740 --> 01:01:48,359
dat

1633
01:01:48,359 --> 01:01:51,000
boten versus honden kan classificeren versus 20 andere

1634
01:01:51,000 --> 01:01:53,640
hondenrassen, maar als

1635
01:01:53,640 --> 01:01:55,920
het ook enige correspondentie zou kunnen hebben

1636
01:01:55,920 --> 01:01:57,900
met afwerkingsstatistieken op een lager niveau,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
dus ja, ik weet het niet, ik denk

1639
01:02:03,960 --> 01:02:07,500
dat de provocerende analogie de

1640
01:02:07,500 --> 01:02:10,380
translationele verschuiving

1641
01:02:10,380 --> 01:02:12,900
in de mnist in het handschrift was

1642
01:02:12,900 --> 01:02:14,819
herkenningsinstelling,

1643
01:02:14,819 --> 01:02:18,000
wat zijn de translationele verschuivingen

1644
01:02:18,000 --> 01:02:20,160
die

1645
01:02:20,160 --> 01:02:22,740
vandaag de dag bestaan? Wat is het voorbeeld van drie pixels?

1646
01:02:22,740 --> 01:02:24,780
Dat is een prompt ontworpen

1647
01:02:24,780 --> 01:02:27,540
aanval op een llm of zoiets of

1648
01:02:27,540 --> 01:02:29,099
zoiets, waarbij een speciaal teken

1649
01:02:29,099 --> 01:02:32,640
wordt ingevoegd of een eh, een

1650
01:02:32,640 --> 01:02:35,160
overlay op een afbeelding die we niet eens kunnen zien

1651
01:02:35,160 --> 01:02:37,020
ontdek dat,

1652
01:02:37,020 --> 01:02:39,359
dus wat denk je dat die uitdagingen

1653
01:02:39,359 --> 01:02:42,839
zijn en wat zijn manieren waarop we dat kunnen nastreven,

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
ja absoluut, ik bedoel, ik denk dat het een beetje

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
de manier is waarop ik erover nadacht, het is als

1658
01:02:50,099 --> 01:02:52,680
deze symmetrie. Transformaties,

1659
01:02:52,680 --> 01:02:53,760
eh,

1660
01:02:53,760 --> 01:02:55,799
als je aan taalmodellen denkt,

1661
01:02:55,799 --> 01:02:57,420
jij  Je kunt je een symmetrietransformatie voorstellen

1662
01:02:57,420 --> 01:02:58,500
die net zoiets is als het

1663
01:02:58,500 --> 01:03:00,240
vervangen van een woord door een synoniem of

1664
01:03:00,240 --> 01:03:03,780
zoiets. Uh, je hebt de zin voor ons,

1665
01:03:03,780 --> 01:03:06,000
betekent precies hetzelfde, maar nu

1666
01:03:06,000 --> 01:03:07,380
gaat het model plotseling

1667
01:03:07,380 --> 01:03:09,299
heel anders reageren,

1668
01:03:09,299 --> 01:03:11,240
eh,

1669
01:03:11,240 --> 01:03:15,359
zoals vertaling tussen talen, dit

1670
01:03:15,359 --> 01:03:16,799
kan worden gezien als een  Bij dit type transformatie

1671
01:03:16,799 --> 01:03:19,440
behoudt het de onderliggende

1672
01:03:19,440 --> 01:03:21,960
betekenis van de input

1673
01:03:21,960 --> 01:03:24,900
voor ons, maar voor het model ziet het er

1674
01:03:24,900 --> 01:03:26,220
totaal anders uit en we zouden graag

1675
01:03:26,220 --> 01:03:28,380
modellen willen hebben die zich op een

1676
01:03:28,380 --> 01:03:29,940
voorspelbare manier gedragen met betrekking tot dit

1677
01:03:29,940 --> 01:03:32,160
soort transformaties, omdat

1678
01:03:32,160 --> 01:03:35,040
ik denk dat mensen zich zeer voorspelbaar gedragen

1679
01:03:35,040 --> 01:03:37,319
sinds deze  Transformaties en als

1680
01:03:37,319 --> 01:03:39,920
we te maken hebben met AI-systemen verwachten we dat

1681
01:03:39,920 --> 01:03:43,200
ze zich ook zo gedragen en ik denk

1682
01:03:43,200 --> 01:03:45,480
dat dit een deel is van wat veel

1683
01:03:45,480 --> 01:03:47,339
uitdagingen veroorzaakt bij de interactie met deze

1684
01:03:47,339 --> 01:03:49,460
systemen en ik heb geprobeerd

1685
01:03:49,460 --> 01:03:52,500
daar een ruwe, brutale demonstratie van te geven

1686
01:03:52,500 --> 01:03:54,960
met deze  beer en vierkanten en

1687
01:03:54,960 --> 01:03:58,440
zo, we verwachten dat

1688
01:03:58,440 --> 01:04:00,480
het zoiets eenvoudigs

1689
01:04:00,480 --> 01:04:02,220
als dit kan doen, omdat we denken dat de meeste mensen dat

1690
01:04:02,220 --> 01:04:04,020
kunnen, maar toch niet en als je je

1691
01:04:04,020 --> 01:04:05,460
voorstelt dat dit een kritiek scenario is

1692
01:04:05,460 --> 01:04:07,740
waarin je dit verwacht en dat is een groot

1693
01:04:07,740 --> 01:04:08,700
probleem,

1694
01:04:08,700 --> 01:04:11,099
eh  hoe gaan we daarmee om, dat is ja, ik

1695
01:04:11,099 --> 01:04:12,720
denk dat dat is waar ik naar op

1696
01:04:12,720 --> 01:04:15,859
zoek ben. Ik denk dat

1697
01:04:16,319 --> 01:04:18,420
mijn

1698
01:04:18,420 --> 01:04:22,280
richting die ik neem, er

1699
01:04:22,280 --> 01:04:26,940
eenvoudiger uitziet en lijkt op bottom-up

1700
01:04:26,940 --> 01:04:29,460
bouwstenen van

1701
01:04:29,460 --> 01:04:31,380
neurale netwerkarchitecturen of

1702
01:04:31,380 --> 01:04:33,180
algoritmen die

1703
01:04:33,180 --> 01:04:35,760
deze opkomende soorten opleveren  structurele

1704
01:04:35,760 --> 01:04:37,680
eigenschappen en ik denk dat dat een

1705
01:04:37,680 --> 01:04:39,839
veel generaliseerbare manier is in plaats van

1706
01:04:39,839 --> 01:04:41,579
iets te bouwen bovenop wat we

1707
01:04:41,579 --> 01:04:43,140
al hebben.

1708
01:04:43,140 --> 01:04:43,920


1709
01:04:43,920 --> 01:04:45,839
Ik denk dat dat iets is dat

1710
01:04:45,839 --> 01:04:47,819
veel beter zal schalen en ook meer overeenkomt met wat

1711
01:04:47,819 --> 01:04:50,299
de hersenen doen.

1712
01:04:50,760 --> 01:04:52,740
Heel cool. Een soort

1713
01:04:52,740 --> 01:04:54,740
implementatievraag: wat zijn de  computationele

1714
01:04:54,740 --> 01:04:57,000
vereisten om dit gewoon uit te voeren of hoe is het van

1715
01:04:57,000 --> 01:04:59,400
dag tot dag om een

1716
01:04:59,400 --> 01:05:01,920
student of onderzoeker te zijn die varianten

1717
01:05:01,920 --> 01:05:04,380
hiervan uitvoert, zoals gebruiken ze terabytes aan

1718
01:05:04,380 --> 01:05:07,020
gegevens en gebruik je grote berekeningen

1719
01:05:07,020 --> 01:05:08,940
of is dit iets dat mensen zelf kunnen uitvoeren

1720
01:05:08,940 --> 01:05:11,880
laptops

1721
01:05:11,880 --> 01:05:13,980
Ik denk dat bijna alles wat ik

1722
01:05:13,980 --> 01:05:17,099
vandaag heb gepresenteerd lokaal kan worden uitgevoerd, dus alsof

1723
01:05:17,099 --> 01:05:20,040
dit spul supereenvoudig is, kun je het uitvoeren. Ik

1724
01:05:20,040 --> 01:05:20,760
bedoel, je gaat

1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
denken dat je het op je laptop kunt draaien als

1727
01:05:24,299 --> 01:05:25,980
je wilt trainen en experimenteren

1728
01:05:25,980 --> 01:05:27,420
met verschillende dingen die het gaat  om

1729
01:05:27,420 --> 01:05:30,119
behoorlijk traag te zijn, dus ik zou een

1730
01:05:30,119 --> 01:05:33,359
commerciële GPU aanbevelen, zoals een. Ik voer vrijwel

1731
01:05:33,359 --> 01:05:35,640
alles uit, zoals Nvidia 1080s,

1732
01:05:35,640 --> 01:05:38,819
behoorlijk oud, behoorlijk goedkoop, maar ze hebben 12

1733
01:05:38,819 --> 01:05:41,099
gigabytes RAM of wat dan ook en het is

1734
01:05:41,099 --> 01:05:43,140
meer dan genoeg voor deze modellen, vier

1735
01:05:43,140 --> 01:05:46,440
gigabytes RAM  Ik denk dat een ding dat

1736
01:05:46,440 --> 01:05:48,839
sommige mensen raar vinden, is dat ik de meeste

1737
01:05:48,839 --> 01:05:51,480
van mijn experimenten doe met dingen als mnist, dus

1738
01:05:51,480 --> 01:05:54,780
het zijn afbeeldingen van 32 bij 32 pixels, omdat ik

1739
01:05:54,780 --> 01:05:57,299
het klein en vocaal kan trainen,

1740
01:05:57,299 --> 01:06:00,000
eh als je dat wilt, mijn experimenten

1741
01:06:00,000 --> 01:06:02,460
of een eindeloos als je  wil

1742
01:06:02,460 --> 01:06:03,540
dit soort dingen doen, deze zijn veel

1743
01:06:03,540 --> 01:06:05,640
ingewikkelder, deze Hamiltoniaanse Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite, hier kom je terecht in grotere

1745
01:06:08,160 --> 01:06:09,780
modellen die over meerdere

1746
01:06:09,780 --> 01:06:12,240
GPU's draaien en dus gebruik je hier een cluster om

1747
01:06:12,240 --> 01:06:14,220
dit soort modellen uit te voeren,

1748
01:06:14,220 --> 01:06:16,140
maar ik zou zeggen het meeste  de enkele

1749
01:06:16,140 --> 01:06:18,920
machine met de GPU is meer dan genoeg

1750
01:06:18,920 --> 01:06:21,680
of zelfs net als in een collab-notebook

1751
01:06:21,680 --> 01:06:24,539
zoiets als je iets wilt trainen

1752
01:06:24,539 --> 01:06:26,520
op imagenet, het wordt

1753
01:06:26,520 --> 01:06:29,940
ingewikkelder en je hebt

1754
01:06:29,940 --> 01:06:33,660
minstens één GPU nodig, idealiter meer, maar ja, ik

1755
01:06:33,660 --> 01:06:35,160
doe geen  heel veel grootschalige dingen,

1756
01:06:35,160 --> 01:06:37,200
maar ik denk dat het zeker interessant is

1757
01:06:37,200 --> 01:06:39,960
en je kunt daar zeker nog veel meer

1758
01:06:39,960 --> 01:06:42,539
doen, maar voor sommige van dit soort

1759
01:06:42,539 --> 01:06:45,480
eenvoudigere of meer fundamentele vragen

1760
01:06:45,480 --> 01:06:47,760
weet ik niet hoe je het wilt noemen,

1761
01:06:47,760 --> 01:06:52,500
een kleinere machine is  lekker snel, zo

1762
01:06:52,500 --> 01:06:54,660
cool, handig,

1763
01:06:54,660 --> 01:06:58,140
oké. Ik zal een reactie van Dave lezen die

1764
01:06:58,140 --> 01:07:00,780
herinnert aan de opmerking van Bert DeVries tijdens

1765
01:07:00,780 --> 01:07:02,880
het toegepaste actieve inferentie-symposium

1766
01:07:02,880 --> 01:07:05,520
over de wenselijkheid om minder

1767
01:07:05,520 --> 01:07:08,099
moeite of ATP te besteden aan foerageer- of

1768
01:07:08,099 --> 01:07:09,780
controlesituaties waarin we niet veel nodig hebben.

1769
01:07:09,780 --> 01:07:11,579
Precisie, dat doe ik niet  Ik weet niet of je hiernaar luistert,

1770
01:07:11,579 --> 01:07:13,859
maar professor DeVries heeft het gehad

1771
01:07:13,859 --> 01:07:17,520
over variabele precisiemodellen en hoe

1772
01:07:17,520 --> 01:07:19,440
ze kunnen worden gebruikt om

1773
01:07:19,440 --> 01:07:21,059
verschillende kenmerken van

1774
01:07:21,059 --> 01:07:23,039
generalisatie en daadwerkelijke structurele

1775
01:07:23,039 --> 01:07:25,020
cursustraining mogelijk te maken, evenals om verminderde computervereisten te verminderen.

1776
01:07:25,020 --> 01:07:27,059


1777
01:07:27,059 --> 01:07:29,880
Heeft hij suggesties over hoe

1778
01:07:29,880 --> 01:07:31,980
dit te introduceren?  onderscheid in actieve

1779
01:07:31,980 --> 01:07:33,900
gevolgtrekking Theorie, wat voor soort

1780
01:07:33,900 --> 01:07:37,760
experimenten zouden dit kunnen ophelderen

1781
01:07:38,400 --> 01:07:40,680
oh wauw ja dat is iets wat ik niet denk, ik

1782
01:07:40,680 --> 01:07:42,660
denk niet dat ik te veel intelligent ben om

1783
01:07:42,660 --> 01:07:46,619
daarover te zeggen, dat is volkomen eerlijk,

1784
01:07:46,619 --> 01:07:48,740


1785
01:07:51,359 --> 01:07:53,460
het is een superinteressante vraag, want

1786
01:07:53,460 --> 01:07:56,220
ik denk dat de intuïtie een

1787
01:07:56,220 --> 01:07:58,260
Ik vind het heel logisch dat

1788
01:07:58,260 --> 01:08:00,000
je het hebt over

1789
01:08:00,000 --> 01:08:01,980
of ik de variabele nauwkeurigheidsniveaus correct begrijp

1790
01:08:01,980 --> 01:08:05,160
wanneer je codeert in of

1791
01:08:05,160 --> 01:08:07,079
of in je model in het algemeen bezig bent met

1792
01:08:07,079 --> 01:08:07,740
berekeningen

1793
01:08:07,740 --> 01:08:09,420
[Muziek]

1794
01:08:09,420 --> 01:08:11,539
eh,

1795
01:08:13,200 --> 01:08:17,040
dat heeft op de een of andere manier invloed op je

1796
01:08:17,040 --> 01:08:19,080
toekomstige prestaties als  een relatie met een

1797
01:08:19,080 --> 01:08:22,259
energieopslag denk ik, ja en als je

1798
01:08:22,259 --> 01:08:23,580
dit in een actief inspanningssysteem zou willen inbouwen,

1799
01:08:23,580 --> 01:08:26,279
zou je

1800
01:08:26,279 --> 01:08:28,679
echt een belichaamd systeem nodig hebben waarbij de

1801
01:08:28,679 --> 01:08:31,500
agent een idee van energie heeft, zoals een

1802
01:08:31,500 --> 01:08:34,439
interne energieopslag en

1803
01:08:34,439 --> 01:08:36,238
ja, iets dat probeert

1804
01:08:36,238 --> 01:08:38,520
om te behouden terwijl het zijn acties uitvoert

1805
01:08:38,520 --> 01:08:40,500


1806
01:08:40,500 --> 01:08:42,359
en de energie opraakt, zou er

1807
01:08:42,359 --> 01:08:44,759
iets slechts nodig zijn voor de agenten en

1808
01:08:44,759 --> 01:08:47,399
dan zou je misschien een soort opkomst kunnen waarnemen,

1809
01:08:47,399 --> 01:08:48,679


1810
01:08:48,679 --> 01:08:52,040
uh-reductie en

1811
01:08:52,040 --> 01:08:55,198
coderingsprecisie of iets

1812
01:08:55,198 --> 01:08:57,479
dergelijks, terwijl de agent probeert te leren

1813
01:08:57,479 --> 01:09:00,060
effectiever handelen, misschien moet je

1814
01:09:00,060 --> 01:09:02,520
het de mogelijkheid geven om de precisie ervan te beheersen,

1815
01:09:02,520 --> 01:09:04,020


1816
01:09:04,020 --> 01:09:07,080
ja zoals ik zeg vanuit mijn expertise, maar

1817
01:09:07,080 --> 01:09:08,819
het zijn een soort gedachten,

1818
01:09:08,819 --> 01:09:11,460
oké op deze dia hier, eerst een heel

1819
01:09:11,460 --> 01:09:14,219
cool beeld, het lijkt een beetje op een

1820
01:09:14,219 --> 01:09:18,359
digitale Jackson Pollock,

1821
01:09:18,359 --> 01:09:22,920
eh, als dat zo was  een eenvoudiger invoergegevensgrootte

1822
01:09:22,920 --> 01:09:26,520
of gewoon een verminderde complexiteit van

1823
01:09:26,520 --> 01:09:27,719
patronen of als het een grotere

1824
01:09:27,719 --> 01:09:30,000
complexiteit was, hoe zou deze afbeelding er dan anders uitzien?

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
Ja, dus ik heb een aantal experimenten gedaan om

1827
01:09:34,620 --> 01:09:36,738
deze

1828
01:09:36,738 --> 01:09:40,439
oriëntatiekolommen te veranderen en

1829
01:09:40,439 --> 01:09:41,100
eh,

1830
01:09:41,100 --> 01:09:43,140
je kunt ja, in principe de

1831
01:09:43,140 --> 01:09:44,520
parameters van het model veranderen, dat kun je  zorg ervoor dat

1832
01:09:44,520 --> 01:09:47,100
deze kolommen groter zijn, je kunt ervoor zorgen dat

1833
01:09:47,100 --> 01:09:49,380
ze niet een erg vergelijkbare structuur hebben

1834
01:09:49,380 --> 01:09:51,660
als wat we zien bij de mensen waar we

1835
01:09:51,660 --> 01:09:53,040
jou hebben, je kunt ervoor zorgen dat ze meer activiteitsbanden hebben,

1836
01:09:53,040 --> 01:09:55,199


1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
en het is ook zoals je zei, het hangt af van

1839
01:09:58,199 --> 01:10:00,540
de dataset  die je gebruikt als ik

1840
01:10:00,540 --> 01:10:03,179
heel eenvoudige sinusoïdale indelingen

1841
01:10:03,179 --> 01:10:05,580
als invoer gebruik, krijg ik zoiets als dit, ik krijg

1842
01:10:05,580 --> 01:10:07,920
iets dat een beetje meer is, uh

1843
01:10:07,920 --> 01:10:11,640
rotatie, bochtige, hogere entropie,

1844
01:10:11,640 --> 01:10:12,800


1845
01:10:12,800 --> 01:10:15,660
dus ik denk dat dit allemaal interessante

1846
01:10:15,660 --> 01:10:18,000
dingen zijn als je de

1847
01:10:18,000 --> 01:10:19,320
opkomst van wilt bestuderen  dit type organisatie

1848
01:10:19,320 --> 01:10:22,199
in een natuurlijk systeem, uh, als je een

1849
01:10:22,199 --> 01:10:24,120
model hebt dat nu verschillende

1850
01:10:24,120 --> 01:10:25,860
organisaties oplevert voor verschillende

1851
01:10:25,860 --> 01:10:28,620
instellingen, dat is een kijk, oké, welke

1852
01:10:28,620 --> 01:10:31,679
instellingen passen dan het beste bij onze waargenomen gegevens,

1853
01:10:31,679 --> 01:10:32,340


1854
01:10:32,340 --> 01:10:34,679
dus ja, dat kan ik kan ik

1855
01:10:34,679 --> 01:10:36,540
die eromheen sturen als je dat bent

1856
01:10:36,540 --> 01:10:38,520
geïnteresseerd, maar

1857
01:10:38,520 --> 01:10:40,580
eh

1858
01:10:41,580 --> 01:10:44,219
ja, ik denk dat er nog een ander is, sorry, nog

1859
01:10:44,219 --> 01:10:45,659
een ander interessant punt, er is

1860
01:10:45,659 --> 01:10:46,920
dat de

1861
01:10:46,920 --> 01:10:51,480
uh verschillende dieren en soorten uh

1862
01:10:51,480 --> 01:10:53,219
oriëntatieselectiviteit en verschillende

1863
01:10:53,219 --> 01:10:54,659
aantallen vuurraderen, sommige dieren

1864
01:10:54,659 --> 01:10:57,420
hebben het helemaal niet. Ik denk dat misschien muizen als ik dat ben

1865
01:10:57,420 --> 01:11:00,120
correct, ze noemen dit een

1866
01:11:00,120 --> 01:11:01,800
zout- en peperselectiviteit, dus het is

1867
01:11:01,800 --> 01:11:03,480
eigenlijk willekeurig, je hebt geen enkele vorm

1868
01:11:03,480 --> 01:11:04,679
van topografische oriëntatiegevoeligheid,

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
dus er is bewijs dat

1871
01:11:09,300 --> 01:11:10,920
verschillende systemen dit

1872
01:11:10,920 --> 01:11:13,020
anders doen en het is interessant om

1873
01:11:13,020 --> 01:11:14,760
erachter te komen waarom

1874
01:11:14,760 --> 01:11:17,760
ja dit  is erg cool, het doet me eerst denken aan

1875
01:11:17,760 --> 01:11:21,300
de

1876
01:11:21,300 --> 01:11:22,980
reactiediffusiebasis en -tijd,

1877
01:11:22,980 --> 01:11:25,739
dus het is eigenlijk

1878
01:11:25,739 --> 01:11:30,000
mogelijk dat een regio geen

1879
01:11:30,000 --> 01:11:32,840
activiteit heeft vanuit een bepaalde

1880
01:11:32,840 --> 01:11:35,640
granulariteit, alsof er naar wordt gekeken

1881
01:11:35,640 --> 01:11:39,360
op de ruimtelijke en temporele tijdschaal van de FMRI

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
als de gebieden met activiteit maar  als de

1884
01:11:44,699 --> 01:11:46,380
activiteitsgebieden

1885
01:11:46,380 --> 01:11:48,480
langzamer zijn, sneller

1886
01:11:48,480 --> 01:11:52,260
dan die meting, zal deze niet

1887
01:11:52,260 --> 01:11:54,060
anders zijn dan ruis, het zal allemaal zijn

1888
01:11:54,060 --> 01:11:55,739
uitgemiddeld,

1889
01:11:55,739 --> 01:11:58,620
dus dan kunnen er een aantal ja,

1890
01:11:58,620 --> 01:12:01,560
interessante zijn, zoals datasets die

1891
01:12:01,560 --> 01:12:03,360
eigenlijk veel

1892
01:12:03,360 --> 01:12:06,179
rijkdom hebben, maar dan voor één  om de een of

1893
01:12:06,179 --> 01:12:08,520
andere reden werd het gewoon gemiddeld

1894
01:12:08,520 --> 01:12:11,100
omdat het niet met jou verbonden was

1895
01:12:11,100 --> 01:12:12,420
of zoiets. Je moet echt voor

1896
01:12:12,420 --> 01:12:14,520
een enkel proefniveau gaan. Je moet een

1897
01:12:14,520 --> 01:12:16,140
voldoende hoge ruimtelijke resolutie hebben,

1898
01:12:16,140 --> 01:12:18,719
zodat je weet dat het voldoet aan

1899
01:12:18,719 --> 01:12:23,100
microfrequenties.  en dit

1900
01:12:23,100 --> 01:12:24,659
is gewoon iets dat mensen

1901
01:12:24,659 --> 01:12:25,620
lange tijd niet hebben gedaan, vooral als je

1902
01:12:25,620 --> 01:12:27,780
opnames van afzonderlijke kiezers maakt, zul je geen

1903
01:12:27,780 --> 01:12:28,920
reizende golf zien, je

1904
01:12:28,920 --> 01:12:30,900
zult oscillaties zien,

1905
01:12:30,900 --> 01:12:32,219
dus je hebt zoiets als multi-elektrisch nodig

1906
01:12:32,219 --> 01:12:34,199
arrays en eigenlijk zeggen ze oké,

1907
01:12:34,199 --> 01:12:36,000
ja nu we de technologie hebben om

1908
01:12:36,000 --> 01:12:37,520
dit te doen, blijft er zoveel

1909
01:12:37,520 --> 01:12:40,080
bestaan ​​dat we niet eerder zagen en

1910
01:12:40,080 --> 01:12:42,960
mogelijk is dit een verklaring voor

1911
01:12:42,960 --> 01:12:44,400
veel van de ruis die we

1912
01:12:44,400 --> 01:12:46,260
eerder zagen, misschien is het echt gewoon  reizende

1913
01:12:46,260 --> 01:12:47,219
golven,

1914
01:12:47,219 --> 01:12:47,760


1915
01:12:47,760 --> 01:12:51,480
dus ja, ik denk dat er

1916
01:12:51,480 --> 01:12:53,880
in de toekomst veel moet worden gedaan met verhoogde

1917
01:12:53,880 --> 01:12:56,520
mogelijkheden voor opnemen,

1918
01:12:56,520 --> 01:12:58,739
dat is erg cool,

1919
01:12:58,739 --> 01:13:02,940
nou, eventuele laatste gedachten of vragen of

1920
01:13:02,940 --> 01:13:06,239
waar ga je dit werk naartoe brengen?

1921
01:13:06,239 --> 01:13:08,520
Ja nee bedankt dat je me

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
hopelijk in de actieve infrastructuur hebt

1924
01:13:11,640 --> 01:13:14,520
dat is dat zou ik graag willen, ik denk dat dat

1925
01:13:14,520 --> 01:13:16,560
superleuk zou zijn, dus ja, ik ben er niet

1926
01:13:16,560 --> 01:13:18,980
echt zeker van of ik naar misschien muziek kijk,

1927
01:13:18,980 --> 01:13:21,659
uh nu, uh, ik

1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760
kijk naar uh,

1930
01:13:26,760 --> 01:13:30,420
andere andere gekke richtingen. Ik

1931
01:13:30,420 --> 01:13:33,020
wil niet te gek klinken, uh,

1932
01:13:33,020 --> 01:13:36,900
maar  Ik zal naar beneden gaan, ja, veel dingen, dus

1933
01:13:36,900 --> 01:13:38,580
een ding dat naar voren komt, iets dat

1934
01:13:38,580 --> 01:13:40,320
we aan neurops hebben voorgelegd, is het bestuderen van het

1935
01:13:40,320 --> 01:13:43,140
geheugen met reizende golven,

1936
01:13:43,140 --> 01:13:45,060
dus dat papier kwam

1937
01:13:45,060 --> 01:13:46,860
vandaag in het archief uit, uh,

1938
01:13:46,860 --> 01:13:48,840
hoe golven echt goed zijn in het coderen van

1939
01:13:48,840 --> 01:13:50,580
langetermijnherinneringen  wat ik

1940
01:13:50,580 --> 01:13:52,100
super interessant vind,

1941
01:13:52,100 --> 01:13:54,120
dus ik zou een kleintje in die richting kunnen gaan,

1942
01:13:54,120 --> 01:13:55,800


1943
01:13:55,800 --> 01:13:58,920
klinkt goed en ja, het zou heel

1944
01:13:58,920 --> 01:14:01,400
spannend zijn om te zien dat er actie in het spel komt

1945
01:14:01,400 --> 01:14:04,560
als de neuronen

1946
01:14:04,560 --> 01:14:07,620
actief blijven, zelfs als de voeten van de hond bewegen.

1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
Er is veel zoiets als  actiescènes

1949
01:14:11,480 --> 01:14:14,280
zoals het gooien van een honkbal en dan

1950
01:14:14,280 --> 01:14:15,900
gaat het en het is alsof er iets

1951
01:14:15,900 --> 01:14:18,440
aan die actie is dat van

1952
01:14:18,440 --> 01:14:21,179
invloed blijft en dus een diepe

1953
01:14:21,179 --> 01:14:23,699
temporele weergave heeft van alternatieve

1954
01:14:23,699 --> 01:14:26,159
acties

1955
01:14:26,159 --> 01:14:29,640
en dan is de variërende auto-encoder

1956
01:14:29,640 --> 01:14:33,179
eigenlijk al de juiste,

1957
01:14:33,179 --> 01:14:35,219
zoiets, dus

1958
01:14:35,219 --> 01:14:37,739
waardeer het echt  oke bedankt

1959
01:14:37,739 --> 01:14:39,480
tot de volgende keer

1960
01:14:39,480 --> 01:14:43,339
heel erg bedankt doei


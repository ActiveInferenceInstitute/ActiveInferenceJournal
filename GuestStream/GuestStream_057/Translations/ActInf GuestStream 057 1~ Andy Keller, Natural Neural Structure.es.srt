1
00:00:06,600 --> 00:00:09,420
hola y bienvenido, es el 18 de septiembre de

2
00:00:09,420 --> 00:00:14,040
2023 y es un invitado activo 57.1

3
00:00:14,040 --> 00:00:16,740
con Andy Keller,

4
00:00:16,740 --> 00:00:19,619
hablaremos sobre la estructura neuronal natural

5
00:00:19,619 --> 00:00:22,020
para la inteligencia artificial.

6
00:00:22,020 --> 00:00:24,300
Habrá una presentación seguida de una

7
00:00:24,300 --> 00:00:25,800
discusión, así que si estás viendo en vivo, no dudes

8
00:00:25,800 --> 00:00:27,900
en  escribe preguntas en

9
00:00:27,900 --> 00:00:30,660
el chat en vivo, de lo contrario, gracias Andy

10
00:00:30,660 --> 00:00:32,279
por esto, lo espero con ansias

11
00:00:32,279 --> 00:00:35,899
y a ti por la presentación,

12
00:00:36,360 --> 00:00:38,940
sí, muchas gracias, gracias por

13
00:00:38,940 --> 00:00:41,280
invitarme. Estoy muy emocionado de poder

14
00:00:41,280 --> 00:00:42,840
presentar esto con el

15
00:00:42,840 --> 00:00:45,239
grupo de referencia activo.  Soy un fan y estoy muy

16
00:00:45,239 --> 00:00:49,200
interesado, así que espero, sí, poder

17
00:00:49,200 --> 00:00:50,399
tener una buena discusión y ver qué

18
00:00:50,399 --> 00:00:51,960
piensan al respecto.

19
00:00:51,960 --> 00:00:54,840
Entonces mi nombre es Andy. Estoy terminando

20
00:00:54,840 --> 00:00:57,180
mi doctorado supervisado por Maxwelling en la

21
00:00:57,180 --> 00:00:59,100
Universidad de Amsterdam.

22
00:00:59,100 --> 00:01:01,500
Comencé un postdoctorado en Harvard después de

23
00:01:01,500 --> 00:01:05,099
esto, así que comenzaré. Solo estoy hablando

24
00:01:05,099 --> 00:01:07,619
de que el objetivo de mi trabajo en general es

25
00:01:07,619 --> 00:01:09,540
tratar de acercar la inteligencia artificial moderna

26
00:01:09,540 --> 00:01:12,540
a una generalización más parecida a la humana

27
00:01:12,540 --> 00:01:15,240
, por lo que lo que queremos decir con

28
00:01:15,240 --> 00:01:17,159
esto es tal vez algo de  una especie de generalización de la estructura,

29
00:01:17,159 --> 00:01:18,960


30
00:01:18,960 --> 00:01:20,700
um o tal vez más familiar para el

31
00:01:20,700 --> 00:01:22,080
comité de infantes activos, como un

32
00:01:22,080 --> 00:01:24,299
modelo mundial estructurado que creemos que tienen los humanos

33
00:01:24,299 --> 00:01:26,340
y la forma que proponemos hacer

34
00:01:26,340 --> 00:01:28,799
esto es integrando la estructura neuronal natural

35
00:01:28,799 --> 00:01:32,400
en la inteligencia artificial,

36
00:01:32,400 --> 00:01:34,979
así que primero definamos lo que queremos decir.  por

37
00:01:34,979 --> 00:01:36,720
generalización de estructuras,

38
00:01:36,720 --> 00:01:38,400
por lo que creo que es bastante indiscutible

39
00:01:38,400 --> 00:01:40,380
decir que el aprendizaje automático moderno

40
00:01:40,380 --> 00:01:42,900
se generaliza más allá de su conjunto de entrenamiento en

41
00:01:42,900 --> 00:01:45,000
el sentido tradicional, por lo que, por ejemplo,

42
00:01:45,000 --> 00:01:46,799
incluso las primeras redes neuronales artificiales,

43
00:01:46,799 --> 00:01:49,020
perceptrones multicapa, podrían

44
00:01:49,020 --> 00:01:51,659
entrenarse en conjuntos de datos de imágenes como

45
00:01:51,659 --> 00:01:55,140
esta y lograr altos niveles de  precisión, entonces, cuando se

46
00:01:55,140 --> 00:01:56,880
les presenta un

47
00:01:56,880 --> 00:01:58,320
conjunto de imágenes de prueba que nunca

48
00:01:58,320 --> 00:02:00,240
antes habían visto, aún pueden clasificarlas con

49
00:02:00,240 --> 00:02:02,460
relativa facilidad con el mismo nivel de

50
00:02:02,460 --> 00:02:04,500
precisión y esto es lo que normalmente

51
00:02:04,500 --> 00:02:07,320
llamamos generalización, sin embargo, incluso desde el

52
00:02:07,320 --> 00:02:08,758
principio lo fue  Me di cuenta de que estos

53
00:02:08,758 --> 00:02:10,800
sistemas realmente luchan con pequeños

54
00:02:10,800 --> 00:02:12,720
cambios o deformaciones. Se aplica a las

55
00:02:12,720 --> 00:02:16,340
imágenes, por ejemplo, si es

56
00:02:18,920 --> 00:02:23,099
así, ¿piensa por qué es esto sorprendente?

57
00:02:23,099 --> 00:02:24,720
Yo sostengo que en realidad se debe a nuestra

58
00:02:24,720 --> 00:02:26,640
capacidad innata para realizar este tipo de

59
00:02:26,640 --> 00:02:28,680
generalización de estructuras que este

60
00:02:28,680 --> 00:02:31,920
ejemplo es un fracaso.  uh, por

61
00:02:31,920 --> 00:02:33,239
ejemplo, este cambio es casi

62
00:02:33,239 --> 00:02:35,340
imperceptible para nosotros y lo manejamos

63
00:02:35,340 --> 00:02:37,560
automáticamente, mientras que en el sistema es

64
00:02:37,560 --> 00:02:39,959
claramente un problema importante, por lo que en palabras

65
00:02:39,959 --> 00:02:41,940
podemos decir que la generalización de la estructura

66
00:02:41,940 --> 00:02:44,819
es una generalización de algunas

67
00:02:44,819 --> 00:02:47,040
transformaciones de simetría de la entrada o, en este

68
00:02:47,040 --> 00:02:48,959
caso, la simetría.  La transformación es un

69
00:02:48,959 --> 00:02:50,580
pequeño cambio que deja la clase de dígitos

70
00:02:50,580 --> 00:02:52,019
sin cambios,

71
00:02:52,019 --> 00:02:54,599
por lo que la pregunta obvia es qué

72
00:02:54,599 --> 00:02:56,340
queremos decir exactamente con esta

73
00:02:56,340 --> 00:02:58,860
estructura natural y por qué creemos que esto

74
00:02:58,860 --> 00:03:01,620
nos ayudaría con estas configuraciones,

75
00:03:01,620 --> 00:03:03,780
así que primero hablemos de lo que queremos decir

76
00:03:03,780 --> 00:03:05,819
con neuronas naturales.  estructura

77
00:03:05,819 --> 00:03:08,700
Una forma de hablar sobre la estructura o

78
00:03:08,700 --> 00:03:11,640
cualquier tipo de sesgo en un sistema es un

79
00:03:11,640 --> 00:03:14,040
sesgo inductivo y, por lo tanto, un sesgo inductivo

80
00:03:14,040 --> 00:03:16,080
puede definirse libremente como una

81
00:03:16,080 --> 00:03:17,940
restricción apropiada de un conjunto de

82
00:03:17,940 --> 00:03:19,440
hipótesis realizables cuando se hace una

83
00:03:19,440 --> 00:03:22,440
selección de modelos de manera más coloquial.  Puede llamar a

84
00:03:22,440 --> 00:03:24,480
esto algo así como antes de ver cualquier

85
00:03:24,480 --> 00:03:27,060
dato, es una restricción de qué y cómo se

86
00:03:27,060 --> 00:03:29,580
puede aprender de manera muy amplia, esto puede

87
00:03:29,580 --> 00:03:32,519
incluir cualquier cosa, desde clases de modelo hasta

88
00:03:32,519 --> 00:03:34,739
procedimientos de optimización o incluso

89
00:03:34,739 --> 00:03:37,379
hiperparámetros y, en cierto sentido, realmente

90
00:03:37,379 --> 00:03:39,739
definen qué es lo

91
00:03:39,739 --> 00:03:43,140
que es posible aprender.  y define la

92
00:03:43,140 --> 00:03:44,819
generalización en el sentido de que

93
00:03:44,819 --> 00:03:47,220
en realidad no se puede generalizar más allá de un

94
00:03:47,220 --> 00:03:48,420
conjunto de entrenamiento sin tener alguna

95
00:03:48,420 --> 00:03:50,220
solución inductiva. Esto lo explica

96
00:03:50,220 --> 00:03:52,560
más detalladamente en este artículo David

97
00:03:52,560 --> 00:03:55,500
Wolford, por lo que lo que queremos decir con

98
00:03:55,500 --> 00:03:58,620
sesgos inductivos naturales son sesgos que

99
00:03:58,620 --> 00:04:00,000
surgen de las restricciones y

100
00:04:00,000 --> 00:04:02,040
limitaciones.  que enfrentan los

101
00:04:02,040 --> 00:04:04,500
sistemas naturales, por la naturaleza de tener que

102
00:04:04,500 --> 00:04:06,900
vivir en el mundo real, por ejemplo, el

103
00:04:06,900 --> 00:04:08,459
cerebro tiene muchas limitaciones de eficiencia

104
00:04:08,459 --> 00:04:10,439
y limitaciones físicas por la naturaleza de

105
00:04:10,439 --> 00:04:13,140
su construcción, y siguiendo esta

106
00:04:13,140 --> 00:04:14,580
lógica, entonces estas limitaciones realmente están

107
00:04:14,580 --> 00:04:16,620
jugando algún papel en nuestra generalización.

108
00:04:16,620 --> 00:04:19,798
habilidades que actualmente exceden la

109
00:04:19,798 --> 00:04:21,478
inteligencia artificial moderna, como veremos a

110
00:04:21,478 --> 00:04:24,720
continuación, por lo que en esta charla me centraré

111
00:04:24,720 --> 00:04:27,540
específicamente en dos tipos de estructura

112
00:04:27,540 --> 00:04:29,880
que mi trabajo ha estudiado: la

113
00:04:29,880 --> 00:04:31,699
organización topográfica y la

114
00:04:31,699 --> 00:04:34,320
dinámica espaciotemporal y antes de comenzar

115
00:04:34,320 --> 00:04:36,120
con mi trabajo.  Daré un breve ejemplo

116
00:04:36,120 --> 00:04:38,759
de por qué creo que la estructura natural

117
00:04:38,759 --> 00:04:41,400
puede ser útil para lograr la

118
00:04:41,400 --> 00:04:42,780
generalización de la estructura de la que estaba hablando

119
00:04:42,780 --> 00:04:44,280
antes,

120
00:04:44,280 --> 00:04:47,479
así que el primer ejemplo proviene de la arquitectura

121
00:04:47,479 --> 00:04:49,199
frontal neocognitiva de Hukushima

122
00:04:49,199 --> 00:04:51,479
de la década de 1980,

123
00:04:51,479 --> 00:04:53,520
que en realidad fue construida para

124
00:04:53,520 --> 00:04:55,440
abordar directamente  el problema de la

125
00:04:55,440 --> 00:04:57,300
solidez a estos pequeños cambios y

126
00:04:57,300 --> 00:04:59,759
deformaciones, por lo que en el papeleo

127
00:04:59,759 --> 00:05:02,040
escribe sobre la inspiración de las

128
00:05:02,040 --> 00:05:04,380
mediciones de jerarquía y

129
00:05:04,380 --> 00:05:06,780
agrupación de pupila y comadreja para lograr robustez

130
00:05:06,780 --> 00:05:08,699
ante estas distorsiones y, por lo tanto, si nos fijamos

131
00:05:08,699 --> 00:05:11,160
en la figura, escribe U sub S1 U.

132
00:05:11,160 --> 00:05:14,100
sub C1 y estos representan células simples y

133
00:05:14,100 --> 00:05:16,919
complejas, por lo que este es un

134
00:05:16,919 --> 00:05:18,840
enfoque bastante radical en ese momento, pero

135
00:05:18,840 --> 00:05:20,699
realmente sirvió para mejorar la solidez y

136
00:05:20,699 --> 00:05:22,199
los cambios que estamos afectando a estas primeras

137
00:05:22,199 --> 00:05:24,419
redes neuronales artificiales y, con el tiempo,

138
00:05:24,419 --> 00:05:25,860
estas ideas se simplificaron y

139
00:05:25,860 --> 00:05:28,919
abstrajeron y  Obviamente, produjeron las

140
00:05:28,919 --> 00:05:30,539
redes neuronales convolucionales, como sabemos

141
00:05:30,539 --> 00:05:32,759
hoy, que en última instancia impulsaron el éxito

142
00:05:32,759 --> 00:05:35,580
de la Revolución del aprendizaje profundo, por lo que este

143
00:05:35,580 --> 00:05:36,960
es realmente un ejemplo de un

144
00:05:36,960 --> 00:05:39,479
sesgo inductivo natural que logró la

145
00:05:39,479 --> 00:05:42,120
generalización de la estructura, por lo que para nuestra investigación es

146
00:05:42,120 --> 00:05:43,919
realmente de sumo interés intentar

147
00:05:43,919 --> 00:05:45,900
comprender qué hace que estas  Los modelos funcionan

148
00:05:45,900 --> 00:05:47,280
muy bien

149
00:05:47,280 --> 00:05:49,320
y veo si este principio puede

150
00:05:49,320 --> 00:05:51,360
generalizarse potencialmente para cubrir

151
00:05:51,360 --> 00:05:53,940
transformaciones y simetrías más abstractas.

152
00:05:53,940 --> 00:05:57,000


153
00:05:57,000 --> 00:06:00,360
Entonces, ¿qué hace que una convolución logre esta

154
00:06:00,360 --> 00:06:02,060
generalización de estructura

155
00:06:02,060 --> 00:06:04,440
de manera intuitiva? Puedes ver que esto se hace

156
00:06:04,440 --> 00:06:06,720
aplicando el mismo filtro en o o

157
00:06:06,720 --> 00:06:08,699
extractor de características en  varias

158
00:06:08,699 --> 00:06:10,680
ubicaciones espaciales, por lo que aquí vemos que se aplica un único

159
00:06:10,680 --> 00:06:12,660
filtro convolucional en

160
00:06:12,660 --> 00:06:14,820
todas las ubicaciones de una imagen. Esto significa

161
00:06:14,820 --> 00:06:16,259
que no importa dónde esté su entrada, ya sea

162
00:06:16,259 --> 00:06:18,000
en el medio de

163
00:06:18,000 --> 00:06:20,400
la imagen o a la derecha, tendrá

164
00:06:20,400 --> 00:06:22,080
exactamente las mismas características.  con una

165
00:06:22,080 --> 00:06:23,580
excepción, se desplazarán de manera equivalente, por lo que

166
00:06:23,580 --> 00:06:24,660


167
00:06:24,660 --> 00:06:26,819
matemáticamente este tipo de mapeo

168
00:06:26,819 --> 00:06:29,160
se llama homomorfismo, preserva

169
00:06:29,160 --> 00:06:30,840
la estructura algebraica del

170
00:06:30,840 --> 00:06:33,180
espacio de entrada y el espacio de salida, en este caso

171
00:06:33,180 --> 00:06:35,580
es con respecto a la traducción y, en

172
00:06:35,580 --> 00:06:37,440
un nivel simple, algo así como

173
00:06:37,440 --> 00:06:38,880
Lo que será importante recordar durante

174
00:06:38,880 --> 00:06:40,259
el resto de esta charla es que podemos

175
00:06:40,259 --> 00:06:42,300
verificar los homomorfismos de nuestro

176
00:06:42,300 --> 00:06:45,000
extractor de características si podemos ver que existe

177
00:06:45,000 --> 00:06:46,740
esta conmutación comunitaria con el

178
00:06:46,740 --> 00:06:49,560
diagrama conmutativo de Transformaciones y,

179
00:06:49,560 --> 00:06:51,720
por lo tanto, podemos escribir esto también algebraicamente

180
00:06:51,720 --> 00:06:53,759
mostrando que el extractor de características  f

181
00:06:53,759 --> 00:06:55,080
conmuta con el

182
00:06:55,080 --> 00:06:57,000
operador de transformación t

183
00:06:57,000 --> 00:06:58,919
y básicamente lo que queremos es que

184
00:06:58,919 --> 00:07:00,600
no haya diferencia entre

185
00:07:00,600 --> 00:07:02,639
extraer primero las características y luego

186
00:07:02,639 --> 00:07:04,740
realizar la transformación o

187
00:07:04,740 --> 00:07:06,240
realizar la transformación y luego

188
00:07:06,240 --> 00:07:08,639
extraer las características, por lo que el desafío

189
00:07:08,639 --> 00:07:10,199
hasta la fecha es que

190
00:07:10,199 --> 00:07:11,880
realmente no lo sabemos.  cómo construir

191
00:07:11,880 --> 00:07:13,620
homomorfismos con respecto a

192
00:07:13,620 --> 00:07:15,240
transformaciones más complejas que vemos en

193
00:07:15,240 --> 00:07:18,060
el mundo real, por ejemplo, nuestro cerebro es

194
00:07:18,060 --> 00:07:20,099
capaz de manejar cambios en la iluminación y las

195
00:07:20,099 --> 00:07:22,259
estaciones de forma natural,

196
00:07:22,259 --> 00:07:24,599
así que aquí vemos la iluminación en la cara de una persona

197
00:07:24,599 --> 00:07:26,160
o el cambio de estaciones, podemos

198
00:07:26,160 --> 00:07:27,840
decir que es.  la misma cara o el mismo

199
00:07:27,840 --> 00:07:29,880
camino pero no sabemos cómo construir

200
00:07:29,880 --> 00:07:31,319
modelos que respeten estas

201
00:07:31,319 --> 00:07:33,180
Transformaciones y por eso nos resulta difícil

202
00:07:33,180 --> 00:07:35,520
construir sistemas que las manejen de una

203
00:07:35,520 --> 00:07:37,620
manera robusta y predecible

204
00:07:37,620 --> 00:07:40,259
para dar un ejemplo aún más abstracto de

205
00:07:40,259 --> 00:07:41,759
lo que yo  Lo que quiero decir con esto y las posibles

206
00:07:41,759 --> 00:07:43,620
repercusiones negativas de los modelos que

207
00:07:43,620 --> 00:07:45,440
no manejan la simetría. Las transformaciones

208
00:07:45,440 --> 00:07:48,060
consideran programas modernos de generación de texto a imagen,

209
00:07:48,060 --> 00:07:50,520
por lo que en este ejemplo le pedí a

210
00:07:50,520 --> 00:07:53,940
Dolly que generara una imagen de un

211
00:07:53,940 --> 00:07:55,620
osito de peluche en la luna y lo hace

212
00:07:55,620 --> 00:07:57,180
increíblemente bien.  probablemente mejor

213
00:07:57,180 --> 00:08:00,960
que yo, tiene una textura de piel

214
00:08:00,960 --> 00:08:03,360
increíblemente detallada, sin embargo, si te pido que

215
00:08:03,360 --> 00:08:05,340
hagas algo que veo que es

216
00:08:05,340 --> 00:08:08,039
conceptualmente más simple, como dibujar un

217
00:08:08,039 --> 00:08:10,560
cubo azul encima de un cubo rojo, no

218
00:08:10,560 --> 00:08:13,380
lo hace y me parece poco intuitivo

219
00:08:13,380 --> 00:08:15,300
ya que  la segunda tarea parece

220
00:08:15,300 --> 00:08:18,180
significativamente más fácil, pero lo que estoy

221
00:08:18,180 --> 00:08:19,860
argumentando es que la razón por la que esto es

222
00:08:19,860 --> 00:08:21,599
sorprendente es precisamente la misma razón por

223
00:08:21,599 --> 00:08:23,580
la que el ejemplo de traducción de amnistía fue

224
00:08:23,580 --> 00:08:25,560
sorprendente: está

225
00:08:25,560 --> 00:08:28,020
ocurriendo esta transformación de simetría aquí, es decir, la

226
00:08:28,020 --> 00:08:29,400
transformación entre estos

227
00:08:29,400 --> 00:08:31,740
objetos complejos de un osito de peluche.  y la luna y

228
00:08:31,740 --> 00:08:34,320
estos simples objetos de Cubos que

229
00:08:34,320 --> 00:08:36,360
intuitivamente esperamos que la red

230
00:08:36,360 --> 00:08:38,820
pueda manejar y respetar y vemos

231
00:08:38,820 --> 00:08:40,919
que no es así, como el

232
00:08:40,919 --> 00:08:43,380
trabajo de Fukushima demostró que estas

233
00:08:43,380 --> 00:08:46,200
estructuras naturales de jerarquía y

234
00:08:46,200 --> 00:08:47,700
agrupación de nuestro sistema visual son

235
00:08:47,700 --> 00:08:49,680
eficaz para hacer generalizaciones a

236
00:08:49,680 --> 00:08:52,380
transformaciones pequeñas. Sostengo que una

237
00:08:52,380 --> 00:08:54,060
estructura de nivel potencialmente superior puede

238
00:08:54,060 --> 00:08:55,740
ser necesaria para solucionar estos

239
00:08:55,740 --> 00:08:58,200
problemas de generalización abstracta, por

240
00:08:58,200 --> 00:09:01,380
lo que la pregunta que estoy

241
00:09:01,380 --> 00:09:04,380
estudiando y que me pregunto es cuál

242
00:09:04,380 --> 00:09:06,060
podría ser esta estructura y cómo la

243
00:09:06,060 --> 00:09:08,640
implementamos.  esto en una arquitectura de red neuronal artificial

244
00:09:08,640 --> 00:09:10,080
que en realidad se puede

245
00:09:10,080 --> 00:09:14,120
usar para realizar cálculos,

246
00:09:14,880 --> 00:09:17,700
así que para comenzar a responder, saltaré

247
00:09:17,700 --> 00:09:19,680
a mi primera línea de trabajo sobre

248
00:09:19,680 --> 00:09:22,380
organización topográfica,

249
00:09:22,380 --> 00:09:25,260
de modo que la organización topográfica se observe

250
00:09:25,260 --> 00:09:27,060
ampliamente en todo el cerebro desde las

251
00:09:27,060 --> 00:09:29,760
áreas de nivel de zafiro de la corteza visual primaria.  y

252
00:09:29,760 --> 00:09:31,500
se puede describir muy vagamente como esta

253
00:09:31,500 --> 00:09:33,540
propiedad de que las neuronas que están cerca

254
00:09:33,540 --> 00:09:35,760
unas de otras tienden a responder a

255
00:09:35,760 --> 00:09:38,220
cosas similares, por ejemplo, a la izquierda mostramos

256
00:09:38,220 --> 00:09:39,720
la preferencia codificada por colores de cada

257
00:09:39,720 --> 00:09:42,959
neurona en la corteza digital primaria como

258
00:09:42,959 --> 00:09:45,360
respuesta a líneas orientadas.  y vemos

259
00:09:45,360 --> 00:09:46,740
este conjunto de selectividades que varían suavemente,

260
00:09:46,740 --> 00:09:48,779
otro tipo de

261
00:09:48,779 --> 00:09:50,580
organización se conoce como organización tópica de la retina,

262
00:09:50,580 --> 00:09:52,560
donde las neuronas cercanas en la

263
00:09:52,560 --> 00:09:54,600
corteza visual tienden a responder a

264
00:09:54,600 --> 00:09:56,399
campos receptivos cercanos;

265
00:09:56,399 --> 00:09:58,560
sin embargo, esta organización no se limita

266
00:09:58,560 --> 00:10:01,080
a estas características de bajo nivel, se extiende a algunas

267
00:10:01,080 --> 00:10:02,519
más complejas.  características como las

268
00:10:02,519 --> 00:10:05,459
presentes en caras u objetos o lugares

269
00:10:05,459 --> 00:10:07,920
y esto se relaciona con las llamadas

270
00:10:07,920 --> 00:10:10,080
áreas funcionalmente específicas del cerebro,

271
00:10:10,080 --> 00:10:12,779
como el área de la cara fusiforme FFA y

272
00:10:12,779 --> 00:10:15,420
el área de la cara paraquipocampal PPA,

273
00:10:15,420 --> 00:10:19,200
por lo que en este trabajo la idea principal nuevamente es

274
00:10:19,200 --> 00:10:21,300
que tal vez esto

275
00:10:21,300 --> 00:10:23,580
organización topográfica en algún sentido que está

276
00:10:23,580 --> 00:10:25,080
íntimamente relacionada con la

277
00:10:25,080 --> 00:10:27,980
operación de convolución y la arquitectura de Fukushima, tal

278
00:10:27,980 --> 00:10:30,660
vez podamos generalizar los beneficios de

279
00:10:30,660 --> 00:10:33,420
esto a transformaciones más abstractas, en

280
00:10:33,420 --> 00:10:34,920
otras palabras, aprender a construir

281
00:10:34,920 --> 00:10:36,839
homomorfismos más complejos que no podemos hacer, ¿

282
00:10:36,839 --> 00:10:38,940
sabes que no podemos?  hágalo analíticamente ahora mismo,

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
solo para demostrar que no estamos

285
00:10:42,480 --> 00:10:44,760
completamente locos con esta idea.

286
00:10:44,760 --> 00:10:46,320
Hay algunos trabajos previos en este dominio

287
00:10:46,320 --> 00:10:49,880
de personas como Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden a principios de los años 90 y 2000

289
00:10:54,060 --> 00:10:55,800
y estudiaron cómo

290
00:10:55,800 --> 00:10:57,720
puede ser la organización topográfica.  útil para aprender

291
00:10:57,720 --> 00:11:01,320
en varianzas principalmente en modelos lineales, por lo que

292
00:11:01,320 --> 00:11:03,060
la pregunta para nosotros cuando ingresamos al

293
00:11:03,060 --> 00:11:04,680
espacio es cuál es el

294
00:11:04,680 --> 00:11:07,140
mecanismo abstracto más escalable que se puede aprovechar

295
00:11:07,140 --> 00:11:08,880
de estos enfoques que podemos

296
00:11:08,880 --> 00:11:10,800
integrar en las arquitecturas modernas de redes neuronales profundas

297
00:11:10,800 --> 00:11:12,959
y, en última instancia, nos

298
00:11:12,959 --> 00:11:15,000
decidimos por un  enfoque de modelado generativo

299
00:11:15,000 --> 00:11:16,260
que creo que podría ser

300
00:11:16,260 --> 00:11:17,519
interesante para la gente de esta

301
00:11:17,519 --> 00:11:18,779
comunidad,

302
00:11:18,779 --> 00:11:21,779
que luego nos permite relacionarlo

303
00:11:21,779 --> 00:11:23,579
más estrechamente con el

304
00:11:23,579 --> 00:11:26,040
análisis de componentes topográficos independientes, con la idea básica

305
00:11:26,040 --> 00:11:28,320
de que podemos aprender una

306
00:11:28,320 --> 00:11:30,660
característica topográfica del espacio imponiendo una

307
00:11:30,660 --> 00:11:32,940
distribución topográfica previa sobre  nuestras variables latentes,

308
00:11:32,940 --> 00:11:34,440


309
00:11:34,440 --> 00:11:37,079
así que solo para dar un breve contexto,

310
00:11:37,079 --> 00:11:39,120
supongo que la mayoría de la gente ya está familiarizada

311
00:11:39,120 --> 00:11:40,440
con esto,

312
00:11:40,440 --> 00:11:42,540
pero el tipo de suposición general es

313
00:11:42,540 --> 00:11:44,339
que el cerebro es un modelo generativo y

314
00:11:44,339 --> 00:11:45,720
esta idea, en cierto sentido, se puede

315
00:11:45,720 --> 00:11:48,060
atribuir a los helmholts del

316
00:11:48,060 --> 00:11:50,459
siglo XIX.  donde dijo que lo que

317
00:11:50,459 --> 00:11:52,140
vemos es la solución a un

318
00:11:52,140 --> 00:11:54,420
problema computacional, nuestros cerebros calculan las

319
00:11:54,420 --> 00:11:56,519
causas más probables de las absorciones de fotones

320
00:11:56,519 --> 00:11:59,519
dentro de nuestros ojos y ese es un ejemplo.

321
00:11:59,519 --> 00:12:01,920
Si les muestro esta imagen, inmediatamente

322
00:12:01,920 --> 00:12:03,720
la reconocen como un globo con cierta

323
00:12:03,720 --> 00:12:05,760
curvatura, sin embargo  También podría

324
00:12:05,760 --> 00:12:07,620
ser un disco con una

325
00:12:07,620 --> 00:12:09,600
perspectiva distorsionada, así es como

326
00:12:09,600 --> 00:12:12,660
obtenemos ilusiones ópticas o nuestras imágenes.

327
00:12:12,660 --> 00:12:14,880
Así, tu cerebro infiere que

328
00:12:14,880 --> 00:12:17,100
hay un cubo aquí debido a la

329
00:12:17,100 --> 00:12:18,720
estructura, pero en realidad es solo una

330
00:12:18,720 --> 00:12:19,860
hoja de papel plana.

331
00:12:19,860 --> 00:12:22,740
Entonces puedes pensar en este

332
00:12:22,740 --> 00:12:24,480
aspecto del modelo generativo que es como un

333
00:12:24,480 --> 00:12:26,160
programa de gráficos inverso.

334
00:12:26,160 --> 00:12:28,140
En el programa se conocen las propiedades abstractas

335
00:12:28,140 --> 00:12:30,660
de la esfera, la posición, el

336
00:12:30,660 --> 00:12:32,820
tamaño de la iluminación y se usan para

337
00:12:32,820 --> 00:12:34,560
proyectar la esfera para crear la

338
00:12:34,560 --> 00:12:37,440
imagen 2D que se representa.  Entonces, en efecto, lo que

339
00:12:37,440 --> 00:12:40,019
dicen Humboldt y otros es que,

340
00:12:40,019 --> 00:12:41,940
como modelo generativo, el cerebro

341
00:12:41,940 --> 00:12:43,680
en realidad está tratando de invertir este

342
00:12:43,680 --> 00:12:45,959
proceso generativo y hacer inferencias

343
00:12:45,959 --> 00:12:48,300
e inferir las causas subyacentes de nuestras

344
00:12:48,300 --> 00:12:49,680
sensaciones,

345
00:12:49,680 --> 00:12:51,600
por lo que la razón por la que estoy insistiendo en

346
00:12:51,600 --> 00:12:53,100
este punto es que hay

347
00:12:53,100 --> 00:12:55,440
Hoy en día se habla mucho de modelos generativos,

348
00:12:55,440 --> 00:12:57,180
um, y no me refiero necesariamente solo

349
00:12:57,180 --> 00:12:59,639
a generar imágenes o fotografías bonitas,

350
00:12:59,639 --> 00:13:01,260


351
00:13:01,260 --> 00:13:03,120
um, realmente quiero referirme a

352
00:13:03,120 --> 00:13:07,920
un marco para el aprendizaje no supervisado,

353
00:13:07,920 --> 00:13:10,019
para luego entrar un poco más en

354
00:13:10,019 --> 00:13:11,700
detalles, ¿a qué me refiero con un

355
00:13:11,700 --> 00:13:14,279
topográfico anterior, por lo que los modelos generativos

356
00:13:14,279 --> 00:13:16,019
generalmente se describen como una

357
00:13:16,019 --> 00:13:18,720
distribución conjunta sobre observaciones X y

358
00:13:18,720 --> 00:13:21,720
variables latentes que llamaremos Z uh

359
00:13:21,720 --> 00:13:23,940
y esto generalmente se factoriza o una

360
00:13:23,940 --> 00:13:25,620
forma de hacerlo se factoriza en

361
00:13:25,620 --> 00:13:28,440
términos de una P previa de Z y esto es verdadero.

362
00:13:28,440 --> 00:13:30,420
modelo generativo modelo generativo condicional

363
00:13:30,420 --> 00:13:33,420
P de x dado Z, por lo que una forma en que

364
00:13:33,420 --> 00:13:34,980
podemos pensar en esto es que se

365
00:13:34,980 --> 00:13:37,019
puede ver que lo anterior codifica

366
00:13:37,019 --> 00:13:38,880
penalizaciones relativas para cada tipo de código que se

367
00:13:38,880 --> 00:13:41,279
produce cuando invertimos nuestro

368
00:13:41,279 --> 00:13:42,899
modelo generativo. Esto se llama Computar el

369
00:13:42,899 --> 00:13:45,920
posterior P de Z dado X

370
00:13:45,920 --> 00:13:49,139
y, por lo tanto, para desarrollar un espacio topográfico latente

371
00:13:49,139 --> 00:13:50,639
queremos introducir algún tipo de

372
00:13:50,639 --> 00:13:53,279
previo topográfico que ha sido o

373
00:13:53,279 --> 00:13:55,620
que este trabajo topográfico de ICA mostró que

374
00:13:55,620 --> 00:13:57,720
es equivalente a algo así como una

375
00:13:57,720 --> 00:13:59,700
penalización de dispersión de grupo

376
00:13:59,700 --> 00:14:01,500
para que la gente pueda estar familiarizada con la típica

377
00:14:01,500 --> 00:14:03,180
penalizaciones por escasez del

378
00:14:03,180 --> 00:14:04,560
análisis de Aprendizaje Independiente, desea que sus

379
00:14:04,560 --> 00:14:06,540
activaciones sean escasas, lo que significa que muchas de

380
00:14:06,540 --> 00:14:09,420
ellas son cero, por lo que podría verse

381
00:14:09,420 --> 00:14:10,680
así, tiene un montón de

382
00:14:10,680 --> 00:14:12,300
cuadrados azules que están activos, pero la mayoría de

383
00:14:12,300 --> 00:14:14,459
ellos no están activos, pero específicamente

384
00:14:14,459 --> 00:14:16,740
con el grupo.  Penalización universitaria queremos que

385
00:14:16,740 --> 00:14:18,839
estos priores asigne una menor probabilidad

386
00:14:18,839 --> 00:14:21,600
a estas activaciones dispersas distribuidas

387
00:14:21,600 --> 00:14:24,720
y una mayor probabilidad a estas

388
00:14:24,720 --> 00:14:26,940
representaciones agrupadas densamente empaquetadas.

389
00:14:26,940 --> 00:14:28,860
También puede pensar en esto como una penalización más alta

390
00:14:28,860 --> 00:14:30,720
cuando las cosas están distribuidas, una penalización más baja

391
00:14:30,720 --> 00:14:33,540
cuando las cosas están más juntas,

392
00:14:33,540 --> 00:14:36,380
así que de nuevo, uh.  Esto se puede escribir de

393
00:14:36,380 --> 00:14:39,060
manera abstracta así, pero quiero hacer una

394
00:14:39,060 --> 00:14:41,160
teoría de que estas neuronas, cada uno de

395
00:14:41,160 --> 00:14:42,779
estos cuadrados aquí, representan una especie de

396
00:14:42,779 --> 00:14:44,220
neurona en nuestro modelo y están

397
00:14:44,220 --> 00:14:46,560
organizadas en esta cuadrícula 2D, por lo que cuando

398
00:14:46,560 --> 00:14:48,120
hablamos de agrupación realmente queremos decir.

399
00:14:48,120 --> 00:14:50,820
agruparse en esa topología 2D,

400
00:14:50,820 --> 00:14:53,100
por lo que una cosa que es realmente interesante

401
00:14:53,100 --> 00:14:55,560
y algo importante es que estos

402
00:14:55,560 --> 00:14:57,779
antecedentes no solo nos brindan una

403
00:14:57,779 --> 00:15:00,540
organización topográfica, sino que también han sido observados

404
00:15:00,540 --> 00:15:02,459
o estudiados por personas como

405
00:15:02,459 --> 00:15:05,760
erosi Marcelli y Bruno para que

406
00:15:05,760 --> 00:15:07,740
realmente encajen.  Las estadísticas de los

407
00:15:07,740 --> 00:15:08,959
datos naturales son mejores,

408
00:15:08,959 --> 00:15:11,180
específicamente las imágenes naturales,

409
00:15:11,180 --> 00:15:14,100
han demostrado que al usar este tipo de a

410
00:15:14,100 --> 00:15:16,139
priori en realidad se obtiene un conjunto más escaso de

411
00:15:16,139 --> 00:15:18,839
activaciones, lo que significa que el a priori se ajusta

412
00:15:18,839 --> 00:15:20,459
un poco mejor al verdadero proceso generativo

413
00:15:20,459 --> 00:15:22,620
y, como sabemos, el cerebro tiene

414
00:15:22,620 --> 00:15:24,779
un alto grado de dispersión y se

415
00:15:24,779 --> 00:15:26,339
cree que esto es muy relevante para la

416
00:15:26,339 --> 00:15:28,620
eficiencia,

417
00:15:28,620 --> 00:15:30,839
por lo que para profundizar un poco más en los

418
00:15:30,839 --> 00:15:32,760
detalles para implementar este tipo de

419
00:15:32,760 --> 00:15:35,160
grupo disperso antes de usar un

420
00:15:35,160 --> 00:15:37,320
modelo generativo jerárquico y esto es

421
00:15:37,320 --> 00:15:39,060
básicamente introducido por algunos de los

422
00:15:39,060 --> 00:15:41,339
Trabajo topográfico de ICA.

423
00:15:41,339 --> 00:15:43,320
La idea es que tienes una

424
00:15:43,320 --> 00:15:45,000
variable latente de nivel superior U que

425
00:15:45,000 --> 00:15:47,820
regula simultáneamente la varianza de

426
00:15:47,820 --> 00:15:50,279
múltiples variables de nivel inferior T y

427
00:15:50,279 --> 00:15:52,440
así es como obtenemos la dispersión del grupo.

428
00:15:52,440 --> 00:15:55,440
Luego, para obtener la organización topográfica,

429
00:15:55,440 --> 00:15:56,760
puedes usar varias de estas

430
00:15:56,760 --> 00:15:59,339
variables latentes ligeramente.  superponiéndose con

431
00:15:59,339 --> 00:16:02,519
sus campos de influencia para que sus

432
00:16:02,519 --> 00:16:04,260
vecindarios podamos llamarlos

433
00:16:04,260 --> 00:16:05,699
y esto le dará esta

434
00:16:05,699 --> 00:16:07,440
estructura de correlación suave que es el acto que

435
00:16:07,440 --> 00:16:09,899
busca, así que tenga la intuición para esto,

436
00:16:09,899 --> 00:16:12,060
verá que esta variable T abajo en

437
00:16:12,060 --> 00:16:14,279
la parte inferior aquí no se está obteniendo.  cualquier entrada

438
00:16:14,279 --> 00:16:17,160
de esta U en la parte superior pero comparte

439
00:16:17,160 --> 00:16:19,139
una variable u con esta T en el medio,

440
00:16:19,139 --> 00:16:21,300
por lo que es como si estuvieran compartiendo variantes,

441
00:16:21,300 --> 00:16:22,980
están compartiendo algunos componentes con

442
00:16:22,980 --> 00:16:24,959
sus vecinos pero no todos los componentes

443
00:16:24,959 --> 00:16:26,579
y eso realmente se debe a esta

444
00:16:26,579 --> 00:16:28,079
conectividad local de  estas variables de nivel superior,

445
00:16:28,079 --> 00:16:30,779


446
00:16:30,779 --> 00:16:33,180
así que para simplificar cómo usamos un

447
00:16:33,180 --> 00:16:34,980
modelo generativo, volvamos a una

448
00:16:34,980 --> 00:16:37,440
sola variable U y el desafío en

449
00:16:37,440 --> 00:16:38,940
este tipo de arquitectura que

450
00:16:38,940 --> 00:16:42,360
lo hizo difícil durante muchos años es cómo

451
00:16:42,360 --> 00:16:44,579
se infiere el posterior aproximado.

452
00:16:44,579 --> 00:16:47,579
sobre estas variables intermedias en

453
00:16:47,579 --> 00:16:50,100
esta arquitectura jerárquica y esto

454
00:16:50,100 --> 00:16:52,560
no es muy sencillo, por lo que

455
00:16:52,560 --> 00:16:54,420
trabajos anteriores han utilizado heurísticas desarrolladas para

456
00:16:54,420 --> 00:16:56,699
modelos lineales y en nuestro trabajo descubrimos

457
00:16:56,699 --> 00:16:58,680
que esto realmente no se extendía a las

458
00:16:58,680 --> 00:17:01,199
arquitecturas de redes neuronales modernas, por lo que

459
00:17:01,199 --> 00:17:02,880
nuestra percepción es aprovechar una

460
00:17:02,880 --> 00:17:04,760
La factorización es una

461
00:17:04,760 --> 00:17:07,640
reparametrización específica de esta distribución,

462
00:17:07,640 --> 00:17:10,380
por lo que esta parametrización

463
00:17:10,380 --> 00:17:12,419
se logra específicamente definiendo la

464
00:17:12,419 --> 00:17:14,579
prioridad, lo que se conoce como una

465
00:17:14,579 --> 00:17:16,319
mezcla de escala gaussiana, lo que significa que nuestra

466
00:17:16,319 --> 00:17:19,140
distribución condicional de T dada U es

467
00:17:19,140 --> 00:17:21,179
en realidad una distribución normal donde

468
00:17:21,179 --> 00:17:24,299
la varianza está definida por esta variable.

469
00:17:24,299 --> 00:17:27,720
U y para ciertas opciones de U, esta

470
00:17:27,720 --> 00:17:29,340
distribución es de hecho escasa y

471
00:17:29,340 --> 00:17:31,980
abarca una gama de distribuciones

472
00:17:31,980 --> 00:17:33,780
como laplosianas, un traje y

473
00:17:33,780 --> 00:17:36,299
distribuciones T. Una forma de definirla es

474
00:17:36,299 --> 00:17:38,940
una mezcla de escala gaussiana que emite una

475
00:17:38,940 --> 00:17:40,919
reestructuración particular y una reparametrización

476
00:17:40,919 --> 00:17:42,900
en términos de variables aleatorias gaussianas independientes.

477
00:17:42,900 --> 00:17:45,720
Z y U, entonces

478
00:17:45,720 --> 00:17:48,840
vemos específicamente que esta variable T, que

479
00:17:48,840 --> 00:17:50,760
originalmente era bastante compleja, en realidad es

480
00:17:50,760 --> 00:17:52,799
solo un producto de un conjunto de

481
00:17:52,799 --> 00:17:54,840
variables aleatorias gaussianas con las que ahora sabemos cómo

482
00:17:54,840 --> 00:17:57,660
trabajar de manera mucho más eficiente, en

483
00:17:57,660 --> 00:18:00,120
modelos generativos, específicamente lo que

484
00:18:00,120 --> 00:18:02,039
estamos  Lo que vamos a hacer es poder

485
00:18:02,039 --> 00:18:04,020
obtener posteriores aproximados para

486
00:18:04,020 --> 00:18:06,720
U y Z por separado y luego hacer una

487
00:18:06,720 --> 00:18:08,640
combinación determinista de ellos

488
00:18:08,640 --> 00:18:10,020
para calcular nuestra

489
00:18:10,020 --> 00:18:13,140
variable topográfica T y esto es mucho más fácil de hacer

490
00:18:13,140 --> 00:18:15,500
sin entrar en demasiados detalles con

491
00:18:15,500 --> 00:18:17,700
el método que  Decidimos usar lo que se

492
00:18:17,700 --> 00:18:18,600
conoce como un

493
00:18:18,600 --> 00:18:20,640
codificador automático variacional que aprovecha técnicas

494
00:18:20,640 --> 00:18:23,220
de inferencia variacional para derivar un

495
00:18:23,220 --> 00:18:24,780
límite inferior de probabilidad, lo que

496
00:18:24,780 --> 00:18:26,940
nos permite parametrizar estos

497
00:18:26,940 --> 00:18:29,400
posteriores aproximados con potentes redes neuronales profundas no lineales

498
00:18:29,400 --> 00:18:30,960
y optimizarlas con un

499
00:18:30,960 --> 00:18:33,240
descenso de gradiente.  Estará

500
00:18:33,240 --> 00:18:34,440
familiarizado con la comunidad de inferencia activa,

501
00:18:34,440 --> 00:18:36,900
pero en realidad lo que hemos hecho es que

502
00:18:36,900 --> 00:18:38,760
en lugar de tener un único codificador en el

503
00:18:38,760 --> 00:18:41,640
decodificador como un baes típico, ahora tenemos

504
00:18:41,640 --> 00:18:43,799
dos codificadores, uno para usted y otro para Z

505
00:18:43,799 --> 00:18:46,200
por separado, y luego los combinamos de

506
00:18:46,200 --> 00:18:48,419
esta manera determinista para  construya

507
00:18:48,419 --> 00:18:51,660
nuestra variable T topográfica si ve

508
00:18:51,660 --> 00:18:53,100
que esto es en realidad la

509
00:18:53,100 --> 00:18:54,900
construcción de la distribución T de un estudiante a

510
00:18:54,900 --> 00:18:57,620
partir de gaussianos

511
00:18:57,620 --> 00:19:00,480
y luego podemos conectar esto, hacemos esto

512
00:19:00,480 --> 00:19:03,600
antes de decodificar y luego maximizamos

513
00:19:03,600 --> 00:19:05,820
la probabilidad de los datos por completo, así que

514
00:19:05,820 --> 00:19:07,260
este es el codo  La evidencia del

515
00:19:07,260 --> 00:19:09,360
límite inferior abunda en la probabilidad de los

516
00:19:09,360 --> 00:19:12,600
datos y en realidad es muy similar a la

517
00:19:12,600 --> 00:19:15,720
energía libre variacional que se usa en la

518
00:19:15,720 --> 00:19:18,299
comunidad de entrada activa,

519
00:19:18,299 --> 00:19:20,520
por lo que, quitando estos detalles,

520
00:19:20,520 --> 00:19:22,500
lo realmente interesante es lo que

521
00:19:22,500 --> 00:19:23,940
sucede cuando entrenamos este

522
00:19:23,940 --> 00:19:26,580
modelo generativo que  tiene una penalización de dispersión de grupo relativamente simple

523
00:19:26,580 --> 00:19:29,460
en su espacio latente y

524
00:19:29,460 --> 00:19:30,720
queremos ver qué está

525
00:19:30,720 --> 00:19:32,700
aprendiendo en términos de su organización de

526
00:19:32,700 --> 00:19:34,980
Futuros y primero comenzamos con el

527
00:19:34,980 --> 00:19:36,480
conjunto de datos más simple posible, tenemos un

528
00:19:36,480 --> 00:19:38,580
fondo negro con cuadrados blancos en

529
00:19:38,580 --> 00:19:41,280
ubicaciones XY aleatorias.  y si entrenamos nuestro

530
00:19:41,280 --> 00:19:42,780
codificador automático con esta penalización por escasez de grupo

531
00:19:42,780 --> 00:19:44,760
y luego miramos los

532
00:19:44,760 --> 00:19:47,640
vectores de peso de nuestro decodificador que

533
00:19:47,640 --> 00:19:49,020
estamos trazando en azul aquí nuevamente

534
00:19:49,020 --> 00:19:52,520
organizados en esta cuadrícula 2D, vemos que,

535
00:19:52,520 --> 00:19:54,780
de hecho, aprenden a organizarse

536
00:19:54,780 --> 00:19:57,539
de acuerdo con el espacio.  ubicación, por lo que esto

537
00:19:57,539 --> 00:19:59,580
puede verse como similar a los

538
00:19:59,580 --> 00:20:01,799
campos receptivos convolucionales o el campo receptivo

539
00:20:01,799 --> 00:20:04,679
de cada neurona en realidad está dado por

540
00:20:04,679 --> 00:20:09,059
el tipo de entradas en su ubicación

541
00:20:09,059 --> 00:20:10,860
y esto tiene sentido intuitivamente desde

542
00:20:10,860 --> 00:20:13,140
la perspectiva de escasez del grupo, ya que

543
00:20:13,140 --> 00:20:15,480
para cualquier región determinada resaltar como

544
00:20:15,480 --> 00:20:17,700
en  En amarillo, aquí los filtros en un

545
00:20:17,700 --> 00:20:19,260
grupo determinado están mucho más correlacionados,

546
00:20:19,260 --> 00:20:20,880
tienen estos campos receptivos superpuestos

547
00:20:20,880 --> 00:20:23,460
que otras ubicaciones aleatorias, por lo que

548
00:20:23,460 --> 00:20:25,020
esencialmente vemos que nuestro modelo está

549
00:20:25,020 --> 00:20:27,840
aprendiendo a agrupar actividades de actividad

550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059
en una especie de hoja vertical simulada

552
00:20:32,059 --> 00:20:34,260
de acuerdo con las correlaciones en  el

553
00:20:34,260 --> 00:20:36,840
conjunto de datos, de modo que en lugar de estar en una convolución

554
00:20:36,840 --> 00:20:38,580
donde en realidad estás vinculando el peso

555
00:20:38,580 --> 00:20:40,860
y especificas manualmente Quiero

556
00:20:40,860 --> 00:20:42,539
copiar este peso en todas partes, tal

557
00:20:42,539 --> 00:20:44,220
vez puedas pensar en esto como un

558
00:20:44,220 --> 00:20:45,500
tiempo de espera aproximado

559
00:20:45,500 --> 00:20:48,120
y realmente estamos aprendiendo esto de la

560
00:20:48,120 --> 00:20:49,620
correlación.  estructura del conjunto de datos

561
00:20:49,620 --> 00:20:51,660
en sí y solo para dar un poco

562
00:20:51,660 --> 00:20:54,120
más de inspiración biológica para

563
00:20:54,120 --> 00:20:56,280
esto y sabemos que la retinotopía está

564
00:20:56,280 --> 00:20:58,020
presente en el cerebro. Este es un ejemplo

565
00:20:58,020 --> 00:21:02,460
de retinotopía en la corteza visual y

566
00:21:02,460 --> 00:21:04,500
puedes ver si le muestras al macaco un

567
00:21:04,500 --> 00:21:06,780
Una imagen como esta se proyecta en

568
00:21:06,780 --> 00:21:08,520
esta

569
00:21:08,520 --> 00:21:11,700
topología preservando el espacio en realidad en

570
00:21:11,700 --> 00:21:13,740
la superficie de la corteza,

571
00:21:13,740 --> 00:21:16,080
por lo que la idea es que la

572
00:21:16,080 --> 00:21:18,419
organización topográfica e incluso aprender la

573
00:21:18,419 --> 00:21:21,299
organización topográfica preserva las

574
00:21:21,299 --> 00:21:26,160
correlaciones de entrada de nuestros conjuntos de datos y

575
00:21:26,160 --> 00:21:28,679
potencialmente esto puede ser beneficioso

576
00:21:28,679 --> 00:21:30,840
para  generalizando estas ideas un

577
00:21:30,840 --> 00:21:32,340
poco más, como dije al

578
00:21:32,340 --> 00:21:34,679
principio, sería incluso mejor si

579
00:21:34,679 --> 00:21:37,200
pudiéramos aprender algo más que

580
00:21:37,200 --> 00:21:39,320
solo convolución, tal vez equivarianzas más complicadas,

581
00:21:39,320 --> 00:21:43,679
entonces, ¿cómo hacemos eso? Una

582
00:21:43,679 --> 00:21:45,720
cosa que está clara en la

583
00:21:45,720 --> 00:21:48,299
inteligencia natural es que  No existimos en

584
00:21:48,299 --> 00:21:51,120
este mundo de marcos IID, cierto, existimos

585
00:21:51,120 --> 00:21:53,520
en un mundo con secuencias continuas de

586
00:21:53,520 --> 00:21:55,620
Transformaciones, así que tal vez podamos extender

587
00:21:55,620 --> 00:21:58,440
nuestro modelo a esta configuración para aprender a

588
00:21:58,440 --> 00:22:01,080
observar Transformaciones. Esta es una idea

589
00:22:01,080 --> 00:22:03,299
de coherencia temporal,

590
00:22:03,299 --> 00:22:05,280
entonces, ¿qué pasaría si simplemente

591
00:22:05,280 --> 00:22:08,280
Extendimos nuestro marco anterior a lo largo de la

592
00:22:08,280 --> 00:22:10,620
dimensión temporal, por lo que en lugar de simplemente

593
00:22:10,620 --> 00:22:13,080
agruparnos decimos que queremos que nuestras neuronas

594
00:22:13,080 --> 00:22:15,059
sean un grupo disperso en términos de

595
00:22:15,059 --> 00:22:17,400
extensión espacial en la corteza, en realidad queremos

596
00:22:17,400 --> 00:22:18,960
que sean un grupo disperso a lo largo del tiempo, lo que

597
00:22:18,960 --> 00:22:20,640
significa que si un conjunto de neuronas está

598
00:22:20,640 --> 00:22:22,559
activo.  ahora queremos que ese mismo conjunto de

599
00:22:22,559 --> 00:22:24,360
neuronas también esté activo en el futuro.

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840
Si miramos, si

602
00:22:27,840 --> 00:22:30,600
pensamos intuitivamente en esto, vemos que en realidad esto es

603
00:22:30,600 --> 00:22:33,059
más alentador. La invariancia y la

604
00:22:33,059 --> 00:22:35,039
equivarianza son una forma de entender esto.

605
00:22:35,039 --> 00:22:37,140
Estamos diciendo que  Quiero que las mismas neuronas

606
00:22:37,140 --> 00:22:39,179
estén activas constantemente, pero la

607
00:22:39,179 --> 00:22:41,280
transformación de entrada está cambiando justo cuando los

608
00:22:41,280 --> 00:22:44,220
pies de este pequeño zorro se mueven, así que si

609
00:22:44,220 --> 00:22:45,960
las mismas neuronas codifican lo mismo

610
00:22:45,960 --> 00:22:47,880
una y otra vez pero los pies

611
00:22:47,880 --> 00:22:49,320
se mueven, esas neuronas

612
00:22:49,320 --> 00:22:51,360
aprenderán.  ser invariante al movimiento de

613
00:22:51,360 --> 00:22:53,880
esa pata de este perro, por ejemplo,

614
00:22:53,880 --> 00:22:57,539
entonces, en lugar de eso, vaya,

615
00:22:57,539 --> 00:23:01,200
fui por el camino equivocado aquí,

616
00:23:01,200 --> 00:23:04,860
entonces, en lugar de eso, nuestra percepción fue que este

617
00:23:04,860 --> 00:23:06,659
grupo comienza a ser, en cambio, podría

618
00:23:06,659 --> 00:23:09,059
desplazarse con respecto al tiempo, por lo que esto

619
00:23:09,059 --> 00:23:10,980
significaría  que se

620
00:23:10,980 --> 00:23:13,080
alentaría a que conjuntos de activaciones desplazadas secuencialmente se

621
00:23:13,080 --> 00:23:15,179
activaran juntas y luego nuestro

622
00:23:15,179 --> 00:23:16,440
espacio latente realmente estaría estructurado con

623
00:23:16,440 --> 00:23:18,000
respecto a las Transformaciones observadas,

624
00:23:18,000 --> 00:23:19,980
por lo que puedes ver aquí que en lugar de que el

625
00:23:19,980 --> 00:23:21,480
mismo conjunto de neuronas esté activo en todos los

626
00:23:21,480 --> 00:23:23,340
pasos del tiempo, en realidad es una secuencia.

627
00:23:23,340 --> 00:23:24,900
conjunto permutado de neuronas que estamos

628
00:23:24,900 --> 00:23:27,780
agrupando de esta manera dispersa, uh,

629
00:23:27,780 --> 00:23:29,940
y luego esto nos permite modelar

630
00:23:29,940 --> 00:23:33,419
diferentes observaciones a lo largo del tiempo, pero

631
00:23:33,419 --> 00:23:34,860
aún están conectadas en términos de

632
00:23:34,860 --> 00:23:36,960
aprender una transformación y preservar

633
00:23:36,960 --> 00:23:38,340
esta estructura de correlación de la

634
00:23:38,340 --> 00:23:40,020
empatía,

635
00:23:40,020 --> 00:23:41,940
así que si  junte esto en nuestra

636
00:23:41,940 --> 00:23:44,400
arquitectura topográfica Bae, puede obtener

637
00:23:44,400 --> 00:23:46,020
algo que se parece a esto, verá

638
00:23:46,020 --> 00:23:48,120
que tenemos una secuencia de entrada,

639
00:23:48,120 --> 00:23:51,240
nuevamente estamos codificando una variable z y luego

640
00:23:51,240 --> 00:23:53,520
múltiples variables U en el denominador

641
00:23:53,520 --> 00:23:55,740
aquí y luego cada una de estas

642
00:23:55,740 --> 00:23:58,620
variables U se desplaza  uh, algo así como lo

643
00:23:58,620 --> 00:24:00,480
estábamos mostrando antes para lograr

644
00:24:00,480 --> 00:24:02,820
esta estructura de equivarianza de desplazamiento

645
00:24:02,820 --> 00:24:04,740
que estamos buscando cuando las combinamos

646
00:24:04,740 --> 00:24:07,080
en esta distribución de producto T de estudiante

647
00:24:07,080 --> 00:24:09,240
obtenemos una única

648
00:24:09,240 --> 00:24:10,740
variable latente, esta es ahora nuestra

649
00:24:10,740 --> 00:24:13,860
variable topográfica T y ahora que  Si tenemos esta

650
00:24:13,860 --> 00:24:16,140
estructura conocida en nuestro espacio latente,

651
00:24:16,140 --> 00:24:17,460
podemos pensar en ello como un modelo mundial estructurado.

652
00:24:17,460 --> 00:24:19,919
Sabemos cómo transformar este

653
00:24:19,919 --> 00:24:21,659
espacio latente. En este caso, es

654
00:24:21,659 --> 00:24:23,580
permutando estas activaciones alrededor de estos

655
00:24:23,580 --> 00:24:25,860
círculos haciendo como un papel cíclico, un

656
00:24:25,860 --> 00:24:28,380
cambio cíclico. Sabemos que esto es  va

657
00:24:28,380 --> 00:24:30,120
a corresponder a nuestras Transformaciones de entrada aprendidas

658
00:24:30,120 --> 00:24:32,640
y podemos verificar eso

659
00:24:32,640 --> 00:24:34,620
diciendo: "Está bien, ¿qué pasa si continúo con esta

660
00:24:34,620 --> 00:24:36,480
transformación de entrada, la verdadera

661
00:24:36,480 --> 00:24:38,100
transformación en el conjunto de datos, que es

662
00:24:38,100 --> 00:24:40,559
una rotación, y luego lo comparo con

663
00:24:40,559 --> 00:24:42,659
cómo he desempeñado mi papel en mi última etapa?".  Espacio

664
00:24:42,659 --> 00:24:44,700
moviendo mis activaciones en mi

665
00:24:44,700 --> 00:24:47,280
cerebro y luego decodificamos y vemos que

666
00:24:47,280 --> 00:24:49,919
obtenemos exactamente lo mismo y esto

667
00:24:49,919 --> 00:24:52,140
demuestra esta

668
00:24:52,140 --> 00:24:53,580
propiedad de conmutabilidad de la que hablaba antes

669
00:24:53,580 --> 00:24:56,820
para verificar el homomorfismo

670
00:24:56,820 --> 00:24:58,799
y así medir esto con un poco más de

671
00:24:58,799 --> 00:25:02,460
calidad.  Cuantitativamente, podemos medir

672
00:25:02,460 --> 00:25:04,440
lo que se llama una pérdida de equivarianza, por lo que

673
00:25:04,440 --> 00:25:07,080
esta es realmente la cuantificación de

674
00:25:07,080 --> 00:25:09,360
esta diferencia entre la

675
00:25:09,360 --> 00:25:12,120
activación de nuestra cápsula enrollada o rodar en nuestra

676
00:25:12,120 --> 00:25:15,059
cabeza versus ver cómo se desarrolla el movimiento

677
00:25:15,059 --> 00:25:16,559
y hacia adelante, están observando cómo se

678
00:25:16,559 --> 00:25:19,440
desarrolla la transformación ante nosotros, por lo que

679
00:25:19,440 --> 00:25:21,600
vemos la topografía.  Bae logra un

680
00:25:21,600 --> 00:25:24,000


681
00:25:24,000 --> 00:25:26,700
error de equivarianza significativamente menor. Esta burbuja vae es de lo que estaba

682
00:25:26,700 --> 00:25:27,960
hablando antes, donde está aprendiendo

683
00:25:27,960 --> 00:25:29,820
invariancia, por lo que no tiene la

684
00:25:29,820 --> 00:25:32,340
operación de cambio y el vae tradicional

685
00:25:32,340 --> 00:25:35,640
no tiene noción de organización o

686
00:25:35,640 --> 00:25:37,380
componente temporal, por lo que además el rendimiento es muy

687
00:25:37,380 --> 00:25:40,320
pobre.  a esto vemos que

688
00:25:40,320 --> 00:25:41,700
el modelo es un mejor modelo generativo

689
00:25:41,700 --> 00:25:45,059
de secuencias, simplemente obtiene una

690
00:25:45,059 --> 00:25:48,179
probabilidad logarítmica negativa más baja en

691
00:25:48,179 --> 00:25:50,100
el conjunto de datos, por lo que es más capaz de

692
00:25:50,100 --> 00:25:51,720
modelar este conjunto de datos porque tiene una

693
00:25:51,720 --> 00:25:52,919
noción de la estructura del

694
00:25:52,919 --> 00:25:55,460
transformaciones

695
00:25:55,980 --> 00:25:58,140
uh, podemos probar esto en múltiples

696
00:25:58,140 --> 00:25:59,760
tipos de transformación diferentes y en la

697
00:25:59,760 --> 00:26:00,840
fila superior estamos mostrando la verdadera

698
00:26:00,840 --> 00:26:02,880
transformación, sacamos estas

699
00:26:02,880 --> 00:26:05,039
imágenes atenuadas y luego en la

700
00:26:05,039 --> 00:26:07,080
fila inferior codificamos y luego simplemente hacemos

701
00:26:07,080 --> 00:26:08,700
rodar nuestras activaciones y mantenemos

702
00:26:08,700 --> 00:26:12,140
decodificando para ver qué ha aprendido el modelo

703
00:26:12,140 --> 00:26:15,000
como la

704
00:26:15,000 --> 00:26:17,039
transformación actual que se está observando y

705
00:26:17,039 --> 00:26:19,340
vemos que básicamente puede

706
00:26:19,340 --> 00:26:21,360
reconstruir perfectamente estos elementos de la

707
00:26:21,360 --> 00:26:23,640
secuencia que nunca antes se había visto,

708
00:26:23,640 --> 00:26:25,260
además con imágenes que son

709
00:26:25,260 --> 00:26:26,580
del conjunto de prueba que nunca se ha visto.  antes

710
00:26:26,580 --> 00:26:28,380
simplemente porque sabe cuál

711
00:26:28,380 --> 00:26:29,760
es la transformación que está

712
00:26:29,760 --> 00:26:31,500
codificando actualmente, puede generalizarla a nuevos

713
00:26:31,500 --> 00:26:33,919
ejemplos,

714
00:26:34,020 --> 00:26:36,360
por lo que la conclusión de esta parte es realmente la

715
00:26:36,360 --> 00:26:38,039
organización topográfica. Mostramos que

716
00:26:38,039 --> 00:26:40,080
una estructura de entrada preservada y ahora

717
00:26:40,080 --> 00:26:41,940
estamos mostrando que puede mejorar potencialmente la

718
00:26:41,940 --> 00:26:44,279
eficiencia y la generalización.  Como esperábamos,

719
00:26:44,279 --> 00:26:46,200


720
00:26:46,200 --> 00:26:48,600
finalmente algo que nos sorprendió

721
00:26:48,600 --> 00:26:49,980
y pensé que era potencialmente lo más

722
00:26:49,980 --> 00:26:52,500
interesante es que estas

723
00:26:52,500 --> 00:26:53,700
Transformaciones que aprende nuestro

724
00:26:53,700 --> 00:26:54,960
modelo en realidad generalizan las

725
00:26:54,960 --> 00:26:57,059
combinaciones de Transformaciones que

726
00:26:57,059 --> 00:26:59,580
no vemos durante el entrenamiento, por

727
00:26:59,580 --> 00:27:02,100
ejemplo, a pesar de solo entrenar.  sobre

728
00:27:02,100 --> 00:27:04,200
transformaciones de color y rotación y

729
00:27:04,200 --> 00:27:06,419
aislamiento, si al modelo se le presenta

730
00:27:06,419 --> 00:27:08,340
una transformación de rotación de color combinada

731
00:27:08,340 --> 00:27:11,100
en el momento de la prueba, vemos que es capaz

732
00:27:11,100 --> 00:27:13,140
de modelar completamente y completar estas

733
00:27:13,140 --> 00:27:14,700
transformaciones perfectamente a través del

734
00:27:14,700 --> 00:27:17,159
rol de cápsula, lo que implica que ha aprendido

735
00:27:17,159 --> 00:27:19,620
a factorizar y representar a estos

736
00:27:19,620 --> 00:27:20,880
diferentes.  Transformaciones y puede

737
00:27:20,880 --> 00:27:24,600
combinarlas de manera flexible en el momento de la inferencia,

738
00:27:24,600 --> 00:27:28,140
así que nuevamente tal vez no solo obtengamos

739
00:27:28,140 --> 00:27:29,820
oficialmente eficiencia en la generalización, sino que

740
00:27:29,820 --> 00:27:34,100
también obtengamos algo de composicionalidad básica,

741
00:27:34,260 --> 00:27:36,059
así que hablemos de las limitaciones y de

742
00:27:36,059 --> 00:27:38,460
lo que podríamos hacer a continuación. La

743
00:27:38,460 --> 00:27:40,620
limitación principal es que hay una

744
00:27:40,620 --> 00:27:44,159
transformación predefinida que estamos imponiendo

745
00:27:44,159 --> 00:27:46,500
tanto en el espacio como en el tiempo, así que, aunque

746
00:27:46,500 --> 00:27:49,080
nos liberamos de las transformaciones grupales y

747
00:27:49,080 --> 00:27:52,440
específicamente de la traducción o

748
00:27:52,440 --> 00:27:53,940
rotación como se hace actualmente en el

749
00:27:53,940 --> 00:27:55,559
mundo del aprendizaje automático,

750
00:27:55,559 --> 00:27:59,240
todavía tenemos este

751
00:27:59,240 --> 00:28:01,980
papel latente codificado en nuestro  Se dirige a todo lo que

752
00:28:01,980 --> 00:28:03,900
vemos y hacer esto un poco

753
00:28:03,900 --> 00:28:05,700
más flexible para que, con suerte, podamos modelar

754
00:28:05,700 --> 00:28:08,880
una mayor diversidad de Transformaciones.

755
00:28:08,880 --> 00:28:10,980
Creemos que tal vez podamos

756
00:28:10,980 --> 00:28:13,860
inspirarnos en dinámicas espaciales temporales más estructuradas

757
00:28:13,860 --> 00:28:15,600
que se observan en

758
00:28:15,600 --> 00:28:18,120
el cerebro y eso nos lleva.  a

759
00:28:18,120 --> 00:28:20,400
la segunda parte de esta charla, que es la

760
00:28:20,400 --> 00:28:22,140
dinámica espacio-temporal que vamos a

761
00:28:22,140 --> 00:28:23,039
intentar integrar en

762
00:28:23,039 --> 00:28:25,200
redes neuronales artificiales, un ejemplo

763
00:28:25,200 --> 00:28:27,059
de eso son las ondas viajeras como las que mostré

764
00:28:27,059 --> 00:28:28,020
aquí,

765
00:28:28,020 --> 00:28:30,600
entonces, ¿qué queremos decir con eso? Bueno,

766
00:28:30,600 --> 00:28:32,279
aquí hay una muy  artículo reciente en el que

767
00:28:32,279 --> 00:28:36,059
utilizaron una fmri de nueve Tesla operando a una

768
00:28:36,059 --> 00:28:38,700
resolución de 36 milisegundos para obtener imágenes de una sola

769
00:28:38,700 --> 00:28:40,980
porción de cerebro de rata bajo anestesia

770
00:28:40,980 --> 00:28:43,320
y lo que vemos es esta

771
00:28:43,320 --> 00:28:45,720
actividad temporal espacial muy claramente estructurada y

772
00:28:45,720 --> 00:28:48,299
correlaciones y estos autores del

773
00:28:48,299 --> 00:28:50,520
artículo continúan analizando esto.  actividad en

774
00:28:50,520 --> 00:28:52,919
términos de los modos principales como se muestra

775
00:28:52,919 --> 00:28:55,799
a la derecha, por lo que nuestra hipótesis es que

776
00:28:55,799 --> 00:28:57,179
tal vez algún tipo de

777
00:28:57,179 --> 00:28:59,039
estructura de correlación como esta pueda ser beneficiosa

778
00:28:59,039 --> 00:29:01,260
para estructurar las representaciones de

779
00:29:01,260 --> 00:29:03,240
nuestro modelo con respecto a las

780
00:29:03,240 --> 00:29:05,100
Transformaciones observadas, pero de una manera mucho más

781
00:29:05,100 --> 00:29:07,440
flexible que simplemente  solo un

782
00:29:07,440 --> 00:29:10,700
cambio cíclico como lo estábamos haciendo antes

783
00:29:11,279 --> 00:29:12,419
um

784
00:29:12,419 --> 00:29:15,900
y déjame decirte que esto no solo se

785
00:29:15,900 --> 00:29:19,320
observa en las ratas de un ssi, uh,

786
00:29:19,320 --> 00:29:20,940
puedes ver estas ondas viajeras que

787
00:29:20,940 --> 00:29:24,179
suceden en la corteza del Monte de

788
00:29:24,179 --> 00:29:27,600
primates que se comportan despiertos, uh, por ejemplo, a

789
00:29:27,600 --> 00:29:29,580
la izquierda.  Aquí muestran ondas viajeras

790
00:29:29,580 --> 00:29:31,740
que en realidad

791
00:29:31,740 --> 00:29:35,520
cambian. ¿Qué probabilidad hay de que un primate vea un

792
00:29:35,520 --> 00:29:38,279
estímulo de bajo contraste basado en la fase

793
00:29:38,279 --> 00:29:40,980
de la onda? Además, muestran que

794
00:29:40,980 --> 00:29:43,500
un estímulo de alto contraste a la

795
00:29:43,500 --> 00:29:45,779
derecha puede inducir una actividad de onda viajera

796
00:29:45,779 --> 00:29:47,520
que se propaga hacia afuera.  incluso

797
00:29:47,520 --> 00:29:50,039
en la corteza visual primaria, por lo que son

798
00:29:50,039 --> 00:29:52,140
realmente omnipresentes en todo el cerebro

799
00:29:52,140 --> 00:29:54,000
en múltiples niveles y sería

800
00:29:54,000 --> 00:29:55,440
interesante estudiar cuáles son sus

801
00:29:55,440 --> 00:29:58,140
implicaciones para el

802
00:29:58,140 --> 00:29:59,700
aprendizaje de la representación de la estructura en nuestro caso

803
00:29:59,700 --> 00:30:01,799
o, en general,

804
00:30:01,799 --> 00:30:04,140
hay trabajos previos que han estudiado

805
00:30:04,140 --> 00:30:06,720
estos tipos de dinámica.  y

806
00:30:06,720 --> 00:30:08,700
construyen modelos, por lo que en la parte superior estas son las

807
00:30:08,700 --> 00:30:10,380
ecuaciones que describen una

808
00:30:10,380 --> 00:30:12,600
red neuronal de picos que muestran que si

809
00:30:12,600 --> 00:30:15,720
implementas retrasos de tiempo, en realidad

810
00:30:15,720 --> 00:30:18,240
retrasos de tiempo axonales entre neuronas, obtienes

811
00:30:18,240 --> 00:30:20,820
esta dinámica estructural de

812
00:30:20,820 --> 00:30:22,440
ondas viajeras siempre que el tamaño de tu red sea

813
00:30:22,440 --> 00:30:24,059
lo suficientemente grande.

814
00:30:24,059 --> 00:30:26,520
um, sin embargo, como mucha gente probablemente sabe,

815
00:30:26,520 --> 00:30:28,620
es relativamente difícil entrenar

816
00:30:28,620 --> 00:30:31,320
redes neuronales con picos del mismo tamaño

817
00:30:31,320 --> 00:30:34,820
y rendimiento que las redes neuronales profundas. De

818
00:30:34,820 --> 00:30:37,679
manera similar, en la parte inferior hay otro sistema

819
00:30:37,679 --> 00:30:39,539
que es significativamente más simple pero

820
00:30:39,539 --> 00:30:42,840
quizás demasiado simple. Es una red de

821
00:30:42,840 --> 00:30:45,120
osciladores acoplados que se sabe que

822
00:30:45,120 --> 00:30:48,779
exhiben sincronía y dinámica espacial temporal

823
00:30:48,779 --> 00:30:52,200
y patrones complejos, pero

824
00:30:52,200 --> 00:30:53,520
esto se llama sistema de reducción de fase

825
00:30:53,520 --> 00:30:55,500
y no captura

826
00:30:55,500 --> 00:30:57,059
toda la complejidad que nos interesa,

827
00:30:57,059 --> 00:30:58,140
por lo que estamos viendo algo que está

828
00:30:58,140 --> 00:31:00,779
potencialmente entre estos dos

829
00:31:00,779 --> 00:31:03,600
y lo que  Lo decidido es que este trabajo en

830
00:31:03,600 --> 00:31:06,600
este trabajo es parametrizar una

831
00:31:06,600 --> 00:31:08,520
red de un par de osciladores

832
00:31:08,520 --> 00:31:10,620
un poco más flexible que un

833
00:31:10,620 --> 00:31:12,360
modelo de paramoto, por lo que esto realmente se

834
00:31:12,360 --> 00:31:14,580
basa en esta

835
00:31:14,580 --> 00:31:16,380
red neuronal recurrente destiladora de par de Constantine

836
00:31:16,380 --> 00:31:18,720
Rush y Nisha

837
00:31:18,720 --> 00:31:20,760
um, donde básicamente tomaron la

838
00:31:20,760 --> 00:31:22,200
ecuación que  Describe un

839
00:31:22,200 --> 00:31:23,820
oscilador armónico simple. Es una

840
00:31:23,820 --> 00:31:26,159
ecuación diferencial de segundo orden. La aceleración

841
00:31:26,159 --> 00:31:29,940
de una bola sobre un resorte es proporcional a

842
00:31:29,940 --> 00:31:32,480
su desplazamiento.

843
00:31:32,480 --> 00:31:35,220
Puedes agregar términos adicionales como

844
00:31:35,220 --> 00:31:37,260
amortiguación para que las oscilaciones

845
00:31:37,260 --> 00:31:39,360
desaparezcan lentamente con el tiempo.

846
00:31:39,360 --> 00:31:41,580
Puedes controlar este oscilador con un

847
00:31:41,580 --> 00:31:43,380
externo.  entrada para contrarrestar

848
00:31:43,380 --> 00:31:45,179
esta amortiguación o para darle un poco más de

849
00:31:45,179 --> 00:31:47,279
complejidad a la dinámica

850
00:31:47,279 --> 00:31:49,260
y luego, además, si tiene muchos de

851
00:31:49,260 --> 00:31:50,940
estos osciladores, puede acoplarlos

852
00:31:50,940 --> 00:31:53,000
con estas matrices de acoplamiento. W

853
00:31:53,000 --> 00:31:55,320
uh, como lo demostramos en esta

854
00:31:55,320 --> 00:31:56,640
imagen aquí, para que realmente pueda  Piense en

855
00:31:56,640 --> 00:31:58,140
esta red como un montón de burbujas en

856
00:31:58,140 --> 00:31:59,940
Springs y tal vez estén conectadas entre

857
00:31:59,940 --> 00:32:01,740
sí también mediante Springs o

858
00:32:01,740 --> 00:32:03,600
bandas elásticas, cualquiera que sea la

859
00:32:03,600 --> 00:32:05,279
red neuronal recurrente destiladora del ruso

860
00:32:05,279 --> 00:32:08,100
Mishra, uh, con estos diversos términos, y se

861
00:32:08,100 --> 00:32:09,899
ha demostrado que esto es muy poderoso.

862
00:32:09,899 --> 00:32:12,480
para modelar secuencias largas, también

863
00:32:12,480 --> 00:32:13,740
mencionaron que se inspiraron en el

864
00:32:13,740 --> 00:32:15,360
cerebro que construyó esto y hay muchos

865
00:32:15,360 --> 00:32:17,700
buenos análisis en ese artículo. Por

866
00:32:17,700 --> 00:32:19,020
ejemplo, muestran que estas son

867
00:32:19,020 --> 00:32:21,440
propiedades realmente beneficiosas con respecto a los

868
00:32:21,440 --> 00:32:23,460
problemas de gradiente de desaparición que

869
00:32:23,460 --> 00:32:25,440
generalmente ocurren en las redes neuronales recurrentes.

870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
pero si queremos observar la

872
00:32:29,159 --> 00:32:30,960
dinámica espacial temporal y este tipo de

873
00:32:30,960 --> 00:32:32,820
modelo, es un poco desafiante

874
00:32:32,820 --> 00:32:34,919
porque estas matrices de acoplamiento aquí, las

875
00:32:34,919 --> 00:32:36,320
W

876
00:32:36,320 --> 00:32:39,600
que conectan cada neural o cada

877
00:32:39,600 --> 00:32:41,240
oscilador, están posicionadas entre sí,

878
00:32:41,240 --> 00:32:43,620
estas son matrices densamente conectadas,

879
00:32:43,620 --> 00:32:45,120
como he intentado.  Se muestra aquí a la izquierda,

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
así que si intentas visualizar la dinámica

882
00:32:48,299 --> 00:32:50,580
de esta red, no verás ninguna

883
00:32:50,580 --> 00:32:51,899
organización espacial, no hay

884
00:32:51,899 --> 00:32:55,380
herencia, es una disculpa por el

885
00:32:55,380 --> 00:32:57,000
espacio latente de este modelo,

886
00:32:57,000 --> 00:32:58,799
así que puedes pensar en esto como en nuestro

887
00:32:58,799 --> 00:33:01,020
ejemplo anterior, una neurona.  está conectada

888
00:33:01,020 --> 00:33:03,240
a un conjunto potencialmente arbitrario de otras

889
00:33:03,240 --> 00:33:04,919
neuronas, esas neuronas están conectadas a

890
00:33:04,919 --> 00:33:06,600
otro conjunto arbitrario de neuronas y

891
00:33:06,600 --> 00:33:08,520
obtendrás dinámicas oscilatorias,

892
00:33:08,520 --> 00:33:10,860
pero tipo de fluctuaciones que

893
00:33:10,860 --> 00:33:13,260
no tienen mucho sentido estructurado, así que

894
00:33:13,260 --> 00:33:15,360
en nuestro trabajo pensamos  Bien, ¿cómo

895
00:33:15,360 --> 00:33:18,299
podemos convertir esto más a los tipos de

896
00:33:18,299 --> 00:33:19,860
dinámicas que nos interesan en esta

897
00:33:19,860 --> 00:33:22,140
propagación estructurada de actividad?

898
00:33:22,140 --> 00:33:23,940
Y una manera clara de hacerlo es tener

899
00:33:23,940 --> 00:33:26,539
una Matriz de conectividad más estructurada

900
00:33:26,539 --> 00:33:29,880
que encontramos que se implementa fácilmente

901
00:33:29,880 --> 00:33:31,559
y de manera eficiente.  implementado a través de una

902
00:33:31,559 --> 00:33:33,000
operación de convolución que puede

903
00:33:33,000 --> 00:33:34,620
considerar como una

904
00:33:34,620 --> 00:33:36,299
capa local conectada localmente, por lo que en lugar de tener

905
00:33:36,299 --> 00:33:37,860
cada neurona conectada, cada

906
00:33:37,860 --> 00:33:39,480
neurona simplemente está conectada a sus

907
00:33:39,480 --> 00:33:41,580
vecinas cercanas, después del entrenamiento

908
00:33:41,580 --> 00:33:42,840
terminará obteniendo algo que

909
00:33:42,840 --> 00:33:44,880
parece un espacio suave.  Dinámica temporal,

910
00:33:44,880 --> 00:33:46,620


911
00:33:46,620 --> 00:33:48,419
así que para ser un poco más claro al

912
00:33:48,419 --> 00:33:50,519
entrenar este modelo, tomamos esta

913
00:33:50,519 --> 00:33:52,200
ecuación diferencial de segundo orden separada que

914
00:33:52,200 --> 00:33:54,299
estábamos describiendo antes de discretizarla

915
00:33:54,299 --> 00:33:56,340
en dos ecuaciones de primer orden.

916
00:33:56,340 --> 00:33:57,960
Puedes pensar en esto como

917
00:33:57,960 --> 00:34:01,200
integrar numéricamente la oda que ahora tenemos.

918
00:34:01,200 --> 00:34:03,120
velocidad y luego actualizamos

919
00:34:03,120 --> 00:34:06,000
uh y podemos entrenar este modelo como

920
00:34:06,000 --> 00:34:07,620
algo así como un codificador automático o un

921
00:34:07,620 --> 00:34:09,839
modelo regresivo automático, por lo que si tomamos una

922
00:34:09,839 --> 00:34:11,460
entrada la codificamos en nuestro espacio latente,

923
00:34:11,460 --> 00:34:14,339
realmente la entrada es Dr es este término f de x

924
00:34:14,339 --> 00:34:16,500
que actúa  como término impulsor,

925
00:34:16,500 --> 00:34:18,599
es como impulsar estos osciladores desde

926
00:34:18,599 --> 00:34:20,879
abajo y luego tienen su

927
00:34:20,879 --> 00:34:23,099
propia dinámica que se define mediante los

928
00:34:23,099 --> 00:34:25,800
términos de acoplamiento, estos acoplamientos locales y

929
00:34:25,800 --> 00:34:27,540
luego, en cada paso de tiempo, tomamos este

930
00:34:27,540 --> 00:34:29,460
estado latente, este estado de onda y lo

931
00:34:29,460 --> 00:34:31,560
decodificamos para intentarlo.  y reconstruir la entrada

932
00:34:31,560 --> 00:34:33,540
y estar en el paso de tiempo actual o en un

933
00:34:33,540 --> 00:34:35,699
paso de tiempo futuro,

934
00:34:35,699 --> 00:34:37,980
podemos hacer algún análisis de estos

935
00:34:37,980 --> 00:34:42,300
modelos durante el entrenamiento para ver qué

936
00:34:42,300 --> 00:34:43,619
sucede antes del entrenamiento y después del

937
00:34:43,619 --> 00:34:45,780
entrenamiento, podemos calcular la fase y

938
00:34:45,780 --> 00:34:47,399
la velocidad de la dinámica en el

939
00:34:47,399 --> 00:34:49,379
espacio latente básicamente vemos al

940
00:34:49,379 --> 00:34:51,480
comienzo del comercio que no hay ondas en

941
00:34:51,480 --> 00:34:53,699
nuestro modelo, pero después del entrenamiento después de 50

942
00:34:53,699 --> 00:34:55,500
épocas vemos que hay una

943
00:34:55,500 --> 00:34:57,119
actividad estructurada suave que se propaga

944
00:34:57,119 --> 00:35:00,420
hacia abajo, al servicio de esta

945
00:35:00,420 --> 00:35:01,800
tarea de modelado de secuencia que estamos haciendo como

946
00:35:01,800 --> 00:35:04,380
objetos giratorios,

947
00:35:04,380 --> 00:35:05,940
entonces.  ¿Cuál es el beneficio de esto?

948
00:35:05,940 --> 00:35:07,680
Quiero decir, la única razón por la que motivé esto

949
00:35:07,680 --> 00:35:10,380
fue para decir que queríamos tener una

950
00:35:10,380 --> 00:35:11,880
estructura aprendida de manera más flexible. ¿

951
00:35:11,880 --> 00:35:13,020
Realmente lo estamos haciendo o simplemente

952
00:35:13,020 --> 00:35:15,060
estamos obteniendo ondas bonitas?

953
00:35:15,060 --> 00:35:16,859
Entonces, lo que mostramos en nuestro artículo es

954
00:35:16,859 --> 00:35:19,320
que realmente lo estamos.  aprender algún tipo de

955
00:35:19,320 --> 00:35:20,940
estructura útil y la forma en que lo mostramos

956
00:35:20,940 --> 00:35:22,440
es nuevamente con algo como este

957
00:35:22,440 --> 00:35:24,960
diagrama conmutativo, si tomas una entrada

958
00:35:24,960 --> 00:35:27,000
y la codificas y obtienes un

959
00:35:27,000 --> 00:35:29,280
estado de onda y luego propagas ondas

960
00:35:29,280 --> 00:35:31,619
artificialmente en ese estado de onda y luego

961
00:35:31,619 --> 00:35:33,480
decodificas, puedes  observe que

962
00:35:33,480 --> 00:35:35,220
en realidad es exactamente lo mismo que si

963
00:35:35,220 --> 00:35:37,140
simplemente hubiera mostrado un montón de

964
00:35:37,140 --> 00:35:39,180
imágenes diferentes de diferentes transformaciones, por lo que hay

965
00:35:39,180 --> 00:35:41,640
muchos dígitos uh diferentes,

966
00:35:41,640 --> 00:35:43,920
características diferentes y vemos que obtenemos

967
00:35:43,920 --> 00:35:46,140
diferentes tipos de actividad de onda en cada

968
00:35:46,140 --> 00:35:47,880
caso para poder modelar eso.  transformación diferente

969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
si lo entrenamos en diferentes conjuntos de datos

971
00:35:51,119 --> 00:35:53,400
también veremos dinámicas más complejas,

972
00:35:53,400 --> 00:35:55,200
en este caso tal vez ni siquiera

973
00:35:55,200 --> 00:35:57,839
ondas viajeras u ondas estacionarias que

974
00:35:57,839 --> 00:36:00,359
pueden considerarse como ondas viajeras en

975
00:36:00,359 --> 00:36:02,339
direcciones opuestas, por lo que vemos si estamos

976
00:36:02,339 --> 00:36:04,079
modelando estos orbitales.  Dinámica, obtenemos

977
00:36:04,079 --> 00:36:06,000
este tipo de masas de actividad que se mueven suavemente

978
00:36:06,000 --> 00:36:07,619
en nuestro espacio latente. Si

979
00:36:07,619 --> 00:36:09,839
modelamos un péndulo, de manera similar obtenemos un

980
00:36:09,839 --> 00:36:13,820
tipo de actividad oscilatoria compleja,

981
00:36:14,099 --> 00:36:17,339
por lo que se conserva la estructura de entrada, pero

982
00:36:17,339 --> 00:36:19,560
además hay más flexibilidad que la que

983
00:36:19,560 --> 00:36:21,599
teníamos antes, que es nuestro objetivo final.

984
00:36:21,599 --> 00:36:23,400


985
00:36:23,400 --> 00:36:26,099
Así que finalmente quiero hablar un poco sobre

986
00:36:26,099 --> 00:36:28,320
cómo creo que el resultado de esta investigación

987
00:36:28,320 --> 00:36:30,420
puede no solo mejorar la

988
00:36:30,420 --> 00:36:32,099
inteligencia artificial sino también cómo nos ayuda a

989
00:36:32,099 --> 00:36:34,440
comprender por qué nuestras mediciones del

990
00:36:34,440 --> 00:36:36,240
cerebro se ven como lo son, para dar un

991
00:36:36,240 --> 00:36:38,579
breve ejemplo de lo que creo.  Con esto

992
00:36:38,579 --> 00:36:41,040
hablé un poco antes sobre visas

993
00:36:41,040 --> 00:36:43,740
y lugares, así que en este fantástico trabajo

994
00:36:43,740 --> 00:36:46,859
con Ching Higao estudiamos si nuestro

995
00:36:46,859 --> 00:36:48,900
previo topográfico simple como discutimos

996
00:36:48,900 --> 00:36:50,579
puede reproducir estos mismos

997
00:36:50,579 --> 00:36:53,339
efectos, así que específicamente ponemos el valor

998
00:36:53,339 --> 00:36:56,099
de esta D de Cohen.  métrica de selectividad para

999
00:36:56,099 --> 00:36:58,200
cada una de nuestras neuronas con respecto a un

1000
00:36:58,200 --> 00:37:00,000
conjunto de datos diferente de imágenes que potencialmente

1001
00:37:00,000 --> 00:37:02,460
contienen solo rostros o solo objetos o

1002
00:37:02,460 --> 00:37:03,359
cuerpos,

1003
00:37:03,359 --> 00:37:05,880
por lo que medimos para cada neurona si es

1004
00:37:05,880 --> 00:37:07,920
más probable que responda a rostros o si el

1005
00:37:07,920 --> 00:37:10,380
ruso emerge en el cerebro, pero yo sí.

1006
00:37:10,380 --> 00:37:12,839
Creo que nos dice que la

1007
00:37:12,839 --> 00:37:15,300
organización relativa de la selectividad puede ser al menos

1008
00:37:15,300 --> 00:37:17,400
parcialmente atribuible a las

1009
00:37:17,400 --> 00:37:19,800
estadísticas de correlación en los datos que tienen que rerutarse

1010
00:37:19,800 --> 00:37:21,359
después de pasar por un

1011
00:37:21,359 --> 00:37:23,640
extractor futuro altamente no lineal, como una

1012
00:37:23,640 --> 00:37:25,440
red neuronal profunda,

1013
00:37:25,440 --> 00:37:27,480
por lo que, en una línea similar, algo que es

1014
00:37:27,480 --> 00:37:29,040
interesante, se sabe lo que se

1015
00:37:29,040 --> 00:37:30,900
llama flujo visual tripartito o no,

1016
00:37:30,900 --> 00:37:36,720
por lo que las imágenes de uh u objetos son

1017
00:37:36,720 --> 00:37:38,400
la selectividad con respecto a los objetos

1018
00:37:38,400 --> 00:37:40,680
está organizada por propiedades más abstractas

1019
00:37:40,680 --> 00:37:43,200
como la animacidad, ¿es esta cosa viva o

1020
00:37:43,200 --> 00:37:46,200
inanimada? versus también

1021
00:37:46,200 --> 00:37:48,480
el tamaño de objeto del mundo real como qué  es el tamaño de una

1022
00:37:48,480 --> 00:37:50,700
tetera versus un car

1023
00:37:50,700 --> 00:37:53,520
um y lo que vemos es que en los

1024
00:37:53,520 --> 00:37:56,160
humanos esta selectividad está organizada

1025
00:37:56,160 --> 00:37:57,540
en esta estructura tripartita,

1026
00:37:57,540 --> 00:37:59,760
normalmente tienes objetos pequeños que se encuentran

1027
00:37:59,760 --> 00:38:01,920
entre objetos animados e inanimados

1028
00:38:01,920 --> 00:38:04,200
en términos de su selectividad y vemos

1029
00:38:04,200 --> 00:38:06,060
Lo mismo sucede aquí, por lo que

1030
00:38:06,060 --> 00:38:07,440
están midiendo la selectividad

1031
00:38:07,440 --> 00:38:08,880
del mismo conjunto de neuronas, pero con respecto

1032
00:38:08,880 --> 00:38:10,859
a estas diferencias de estímulos, se ve

1033
00:38:10,859 --> 00:38:12,440
que el pequeño grupo está entre el

1034
00:38:12,440 --> 00:38:14,820
grupo animado y el inanimado y nuevamente

1035
00:38:14,820 --> 00:38:16,079
esto sucede para múltiples

1036
00:38:16,079 --> 00:38:18,900
inicializaciones diferentes, por lo que esto  es algo que

1037
00:38:18,900 --> 00:38:20,880
espero que podamos explorar un poco más para

1038
00:38:20,880 --> 00:38:22,980
esta comunidad. Creo que es interesante

1039
00:38:22,980 --> 00:38:24,119
porque es

1040
00:38:24,119 --> 00:38:26,220
realmente una forma de mostrar que

1041
00:38:26,220 --> 00:38:28,200
construimos un modelo mundial estructurado y

1042
00:38:28,200 --> 00:38:30,119
potencialmente este modelo mundial es

1043
00:38:30,119 --> 00:38:31,980
beneficioso para

1044
00:38:31,980 --> 00:38:34,740
representar mejor los datos del mundo real de una

1045
00:38:34,740 --> 00:38:37,619
manera estructurada y  obtienes menor

1046
00:38:37,619 --> 00:38:39,119
energía libre en ese sentido,

1047
00:38:39,119 --> 00:38:40,619
así que

1048
00:38:40,619 --> 00:38:42,300
creo que al desarrollar estos modelos

1049
00:38:42,300 --> 00:38:44,400
como los que mostramos aquí podemos obtener

1050
00:38:44,400 --> 00:38:46,500
información sobre nuevos mecanismos sobre cómo

1051
00:38:46,500 --> 00:38:48,900
emerge esta estructura, incluida la

1052
00:38:48,900 --> 00:38:50,460
organización topográfica en la que nunca

1053
00:38:50,460 --> 00:38:52,920
antes habíamos pensado, así que el modelo de máquina que estaba

1054
00:38:52,920 --> 00:38:55,520
mirando  la selectividad de orientación

1055
00:38:55,520 --> 00:38:58,260
de las neuronas, lo cual no

1056
00:38:58,260 --> 00:39:01,020
esperaba particularmente que

1057
00:39:01,020 --> 00:39:03,420
sucediera algo, pero estás viendo cómo

1058
00:39:03,420 --> 00:39:05,339
estas ondas se propagan sobre esta

1059
00:39:05,339 --> 00:39:08,099
superficie vertical simulada y pensé, está

1060
00:39:08,099 --> 00:39:09,960
bien, tal vez estoy mostrando imágenes rotadas,

1061
00:39:09,960 --> 00:39:11,820
tal vez esto tenga algún efecto.  la

1062
00:39:11,820 --> 00:39:13,740
selectividad de orientación

1063
00:39:13,740 --> 00:39:15,599
y, de hecho, si entras y

1064
00:39:15,599 --> 00:39:17,460
mides la selectividad de cada neurona

1065
00:39:17,460 --> 00:39:18,660
con respecto a estas líneas orientadas de manera diferente,

1066
00:39:18,660 --> 00:39:22,079
lo que ves es que

1067
00:39:22,079 --> 00:39:24,300
recuerda sorprendentemente al

1068
00:39:24,300 --> 00:39:25,859
tipo de columnas de Oriente que se ven en la

1069
00:39:25,859 --> 00:39:27,599
corteza visual primaria, esto es algo que se remonta al pasado.

1070
00:39:27,599 --> 00:39:29,520
a Hugo y Weasel y esto es algo

1071
00:39:29,520 --> 00:39:30,900
que surgió de este modelo

1072
00:39:30,900 --> 00:39:33,060
y el hecho de que tiene la

1073
00:39:33,060 --> 00:39:34,440
estructura temporal espacial con respecto a

1074
00:39:34,440 --> 00:39:37,619
las Transformaciones, así que, por supuesto, esta es

1075
00:39:37,619 --> 00:39:39,599
una analogía muy burda, pero creo que este

1076
00:39:39,599 --> 00:39:40,740
es un ejemplo de cómo  Construir este

1077
00:39:40,740 --> 00:39:42,839
tipo de modelos puede ayudarnos a pensar en

1078
00:39:42,839 --> 00:39:45,240
cómo el cerebro construye la

1079
00:39:45,240 --> 00:39:46,980
estructura representacional y el blanco en la forma en que se

1080
00:39:46,980 --> 00:39:48,660
organiza de una manera en la que quizás no habíamos

1081
00:39:48,660 --> 00:39:51,300
pensado antes.

1082
00:39:51,300 --> 00:39:53,460
Creo que no soy el único que está

1083
00:39:53,460 --> 00:39:55,859
haciendo este tipo de  trabajo y por eso

1084
00:39:55,859 --> 00:39:57,240
quiero hablar un poco sobre

1085
00:39:57,240 --> 00:39:59,579
otras personas que están haciendo esto, así que he

1086
00:39:59,579 --> 00:40:00,780
estado hablando de esta

1087
00:40:00,780 --> 00:40:02,760
estructura equivalente,

1088
00:40:02,760 --> 00:40:04,920
personas como James Whittington y

1089
00:40:04,920 --> 00:40:08,880
Tim Barons y surrogengoolie han

1090
00:40:08,880 --> 00:40:10,680
demostrado recientemente que al introducir el sistema

1091
00:40:10,680 --> 00:40:14,940
algebraico  restricciones en uh en un

1092
00:40:14,940 --> 00:40:17,040
proceso de aprendizaje en este caso fue

1093
00:40:17,040 --> 00:40:20,820
como el movimiento de uh y y agentes en

1094
00:40:20,820 --> 00:40:23,220
un entorno al decir que necesitas

1095
00:40:23,220 --> 00:40:24,780
preservar una especie de

1096
00:40:24,780 --> 00:40:27,540
estructura algebraica si me muevo en un Círculo Oeste

1097
00:40:27,540 --> 00:40:29,280
Noreste Sur termino nuevamente en el

1098
00:40:29,280 --> 00:40:31,440
mismo  punto Una vez más, al introducir este

1099
00:40:31,440 --> 00:40:32,820
tipo de restricciones

1100
00:40:32,820 --> 00:40:35,040
se obtiene el surgimiento de representaciones similares a celdas de cuadrícula,

1101
00:40:35,040 --> 00:40:36,900


1102
00:40:36,900 --> 00:40:39,359
um, así que me interesaría ver cómo esta

1103
00:40:39,359 --> 00:40:41,880
idea de estructura representacional puede

1104
00:40:41,880 --> 00:40:43,980
ayudarnos a explicar tal vez más que nuestros

1105
00:40:43,980 --> 00:40:45,480
hallazgos científicos que estamos encontrando también,

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
um y  y cómo esto se relaciona con los

1108
00:40:48,359 --> 00:40:51,540
modelos generativos en su conjunto,

1109
00:40:51,540 --> 00:40:52,800
um y finalmente creo que también hay

1110
00:40:52,800 --> 00:40:54,599
algo que decir sobre la

1111
00:40:54,599 --> 00:40:56,099
posibilidad cognitiva de estos modelos, tal

1112
00:40:56,099 --> 00:40:57,420
vez no solo

1113
00:40:57,420 --> 00:40:59,579
los probaremos desde la perspectiva de la neurociencia

1114
00:40:59,579 --> 00:41:01,020
sino también desde la perspectiva de la ciencia micrognitiva.

1115
00:41:01,020 --> 00:41:03,839
por ejemplo, hay estas

1116
00:41:03,839 --> 00:41:06,000
matrices progresivas de Ravens a la izquierda

1117
00:41:06,000 --> 00:41:08,640
donde tienes que decir cuál de

1118
00:41:08,640 --> 00:41:11,099
estas imágenes es más probable que encaje en

1119
00:41:11,099 --> 00:41:12,599
este patrón

1120
00:41:12,599 --> 00:41:14,760
o, por ejemplo, ¿qué probabilidad hay de que

1121
00:41:14,760 --> 00:41:16,740
esta torre Jenga se caiga cuando te

1122
00:41:16,740 --> 00:41:19,740
detienes?  un bloque específico o

1123
00:41:19,740 --> 00:41:22,740
con una estructura determinada y creo que estas

1124
00:41:22,740 --> 00:41:24,599
cosas son

1125
00:41:24,599 --> 00:41:26,640
este tipo de pruebas que realmente prueban

1126
00:41:26,640 --> 00:41:28,619
si nuestros modelos mundiales que estamos construyendo

1127
00:41:28,619 --> 00:41:31,560
son similares a los tipos de modelos que

1128
00:41:31,560 --> 00:41:33,660
tenemos de forma innata nuestro propio sentido común

1129
00:41:33,660 --> 00:41:36,060
como humanos o  como seres que viven en un

1130
00:41:36,060 --> 00:41:38,400
mundo natural y he hecho un

1131
00:41:38,400 --> 00:41:40,440
trabajo preliminar en esta dirección.

1132
00:41:40,440 --> 00:41:43,079
Creo que es muy preliminar y no tan

1133
00:41:43,079 --> 00:41:45,480
complicado, pero intento

1134
00:41:45,480 --> 00:41:47,820
modelar ilusiones visuales, así que si tomas

1135
00:41:47,820 --> 00:41:50,520
un conjunto de datos realmente simple de una barra en movimiento.

1136
00:41:50,520 --> 00:41:52,980
estímulos o una barra o cuadro estático y

1137
00:41:52,980 --> 00:41:54,960
lo mueves un poco, puedes ver que

1138
00:41:54,960 --> 00:41:57,060
el modelo en realidad inferirá ese

1139
00:41:57,060 --> 00:41:58,800
cuadro faltante y luego también

1140
00:41:58,800 --> 00:42:01,079
inferirá el movimiento continuo, por lo que es como

1141
00:42:01,079 --> 00:42:03,300
sobrepasar la trayectoria de lo que los

1142
00:42:03,300 --> 00:42:05,820
estímulos reales le están proporcionando antes de

1143
00:42:05,820 --> 00:42:08,760
corregir nuevamente.  Así que creo que modelar

1144
00:42:08,760 --> 00:42:10,320
ilusiones es sin duda una

1145
00:42:10,320 --> 00:42:12,660
forma interesante de estudiar si nuestros modelos mundiales son

1146
00:42:12,660 --> 00:42:14,760
similares a los tipos de modelos que

1147
00:42:14,760 --> 00:42:16,619
tenemos nosotros mismos.

1148
00:42:16,619 --> 00:42:19,619
En conclusión, sí, creo que a través de

1149
00:42:19,619 --> 00:42:21,900
antecedentes topográficos podríamos demostrar que

1150
00:42:21,900 --> 00:42:23,220
efectivamente aprendieron

1151
00:42:23,220 --> 00:42:24,839
representaciones estructuradas o modelos mundiales estructurados.  La

1152
00:42:24,839 --> 00:42:26,700
estructura aprendida es

1153
00:42:26,700 --> 00:42:29,160
flexible y adaptable a

1154
00:42:29,160 --> 00:42:30,780
transformaciones arbitrarias, a diferencia de las

1155
00:42:30,780 --> 00:42:33,720
equivariantes tradicionales y los proveedores topográficos, se

1156
00:42:33,720 --> 00:42:35,579
puede inducir estadísticamente como lo hicimos

1157
00:42:35,579 --> 00:42:37,619
en el vae topográfico o mediante

1158
00:42:37,619 --> 00:42:39,480
dinámica como estábamos mostrando en estos

1159
00:42:39,480 --> 00:42:42,000
modelos tipo máquina de ondas neuronales,

1160
00:42:42,000 --> 00:42:44,460
así que para concluir los dejo con esto.

1161
00:42:44,460 --> 00:42:46,980
Una cita que encontré en el artículo de Fukushima

1162
00:42:46,980 --> 00:42:50,280
de 1980, que pensé que estaba bastante adelantada

1163
00:42:50,280 --> 00:42:52,079
a su tiempo, dice que si pudiéramos

1164
00:42:52,079 --> 00:42:53,520
crear un modelo de red neuronal que tuviera

1165
00:42:53,520 --> 00:42:55,020
la misma capacidad de

1166
00:42:55,020 --> 00:42:57,060
reconocimiento de patrones que un ser humano,

1167
00:42:57,060 --> 00:42:58,800
nos daría una pista poderosa en comparación con

1168
00:42:58,800 --> 00:43:00,000
comprender el mecanismo neuronal en

1169
00:43:00,000 --> 00:43:03,240
el cerebro, así que creo que algunos

1170
00:43:03,240 --> 00:43:06,119
de los objetivos que perseguimos aquí,

1171
00:43:06,119 --> 00:43:08,220
así que creo que es mi asesor Max, mis

1172
00:43:08,220 --> 00:43:11,280
coautores Patrick UA, Emil jinghian y

1173
00:43:11,280 --> 00:43:17,359
Yorn, y estoy interesado en la discusión, gracias, está bien, está

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
bien.  gracias excelente

1176
00:43:27,480 --> 00:43:31,079
presentación muy interesante

1177
00:43:31,079 --> 00:43:33,480
muchos lugares para comenzar tal vez solo lo

1178
00:43:33,480 --> 00:43:36,000
que te trajo a este trabajo

1179
00:43:36,000 --> 00:43:38,520
un poco de contexto sobre cómo llegaste a

1180
00:43:38,520 --> 00:43:43,819
este trabajo para tu doctorado Dirección

1181
00:43:43,920 --> 00:43:45,119
sí, quiero

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
decir, hemos estado estudiando, no es mi

1184
00:43:49,200 --> 00:43:51,000
grupo ese  Estoy en la universidad y he

1185
00:43:51,000 --> 00:43:52,700
estado estudiando representaciones estructuradas

1186
00:43:52,700 --> 00:43:56,640
desde el punto de vista matemático durante un

1187
00:43:56,640 --> 00:43:58,319
tiempo, donde algunas de las personas tienen

1188
00:43:58,319 --> 00:44:00,240
modelos como el

1189
00:44:00,240 --> 00:44:01,740
codificador automático variacional

1190
00:44:01,740 --> 00:44:04,680
y

1191
00:44:04,680 --> 00:44:06,960
supongo que algo que siempre ha

1192
00:44:06,960 --> 00:44:08,460
sido

1193
00:44:08,460 --> 00:44:11,220
es un modelo.  eso respeta perfectamente las rotaciones, las rotaciones 2D,

1194
00:44:11,220 --> 00:44:13,560
pero si queremos

1195
00:44:13,560 --> 00:44:15,960
hacer rotaciones 3D, no podemos hacerlo

1196
00:44:15,960 --> 00:44:17,819
porque no es un grupo en términos de una

1197
00:44:17,819 --> 00:44:19,740
proyección en un plano 2D, se está

1198
00:44:19,740 --> 00:44:21,180
perdiendo información cuando esta cosa

1199
00:44:21,180 --> 00:44:23,460
gira, por ejemplo,

1200
00:44:23,460 --> 00:44:24,240
um

1201
00:44:24,240 --> 00:44:26,280
o  cualquier tipo de

1202
00:44:26,280 --> 00:44:27,960
Transformaciones naturales como estaba tratando de

1203
00:44:27,960 --> 00:44:29,339
señalar al principio. Creo que

1204
00:44:29,339 --> 00:44:30,180
estaba

1205
00:44:30,180 --> 00:44:31,740
tratando de pensar en cómo el cerebro

1206
00:44:31,740 --> 00:44:34,020
modela las Transformaciones naturales, es

1207
00:44:34,020 --> 00:44:35,400
algo que en estos Marcos actuales, ¿dónde

1208
00:44:35,400 --> 00:44:37,200


1209
00:44:37,200 --> 00:44:41,099
ves que la acción juega un papel

1210
00:44:41,099 --> 00:44:44,579
en términos de autocodificador variacional?

1211
00:44:44,579 --> 00:44:48,420
modelos que incluyen no solo

1212
00:44:48,420 --> 00:44:50,520
patrones externos sino también las

1213
00:44:50,520 --> 00:44:52,380
consecuencias de la acción o la

1214
00:44:52,380 --> 00:44:55,800
estructura estructural del modelo mundial con acción, cierto, sí,

1215
00:44:55,800 --> 00:44:58,619
no, esa es una buena pregunta y

1216
00:44:58,619 --> 00:45:01,319
creo que las inferencias actuadas

1217
00:45:01,319 --> 00:45:03,839
son efectivamente la respuesta. Creo que

1218
00:45:03,839 --> 00:45:05,940
es una buena respuesta a eso.

1219
00:45:05,940 --> 00:45:09,000
Lo sé.  Hay

1220
00:45:09,000 --> 00:45:11,099
marcos de aprendizaje por refuerzo que

1221
00:45:11,099 --> 00:45:12,660
utilizan

1222
00:45:12,660 --> 00:45:15,060
modelos mundiales entrenados externamente,

1223
00:45:15,060 --> 00:45:17,280
por lo que entrenas un vae o algo así y luego

1224
00:45:17,280 --> 00:45:19,800
usas esa representación en tu

1225
00:45:19,800 --> 00:45:23,040
sistema de aprendizaje por refuerzo, pero

1226
00:45:23,040 --> 00:45:24,720
creo que tener un

1227
00:45:24,720 --> 00:45:26,520
sistema completo que sea un solo

1228
00:45:26,520 --> 00:45:30,780
objetivo con uh acción.  como parte de la

1229
00:45:30,780 --> 00:45:33,660
probabilidad de los datos y,

1230
00:45:33,660 --> 00:45:35,280
sí, creo que eso es mucho más elegante, por lo que

1231
00:45:35,280 --> 00:45:38,940
soy un gran defensor de eso.

1232
00:45:38,940 --> 00:45:39,960


1233
00:45:39,960 --> 00:45:43,140
No he llegado tan lejos como para estudiar cómo

1234
00:45:43,140 --> 00:45:45,480
estructuran estos modelos mundiales en un vae

1235
00:45:45,480 --> 00:45:47,520
o lo he hecho.  No funcionó en eso en absoluto, pero

1236
00:45:47,520 --> 00:45:48,780
creo que sin duda sería muy

1237
00:45:48,780 --> 00:45:50,819
interesante ver si tener un

1238
00:45:50,819 --> 00:45:52,339
modelo mundial más estructurado,

1239
00:45:52,339 --> 00:45:54,839
eh, en un codificador automático variacional, también

1240
00:45:54,839 --> 00:45:56,880
sería beneficioso en un

1241
00:45:56,880 --> 00:45:58,319
entorno activo. Creo que sería

1242
00:45:58,319 --> 00:46:00,119
increíble, quiero decir.  Piense en

1243
00:46:00,119 --> 00:46:03,599
algunos de estos ejemplos, como uh, que se muestran

1244
00:46:03,599 --> 00:46:05,579
antes, como el surgimiento de celdas de la cuadrícula y

1245
00:46:05,579 --> 00:46:07,500
cosas como esta, tal vez apunten en

1246
00:46:07,500 --> 00:46:08,880
esa dirección, está

1247
00:46:08,880 --> 00:46:10,560
bien, tal vez el cerebro esté haciendo algo, es

1248
00:46:10,560 --> 00:46:12,540
realmente obvio que tiene mucha

1249
00:46:12,540 --> 00:46:13,680
estructura,

1250
00:46:13,680 --> 00:46:15,359
um, esto claramente tiene que ser útil para

1251
00:46:15,359 --> 00:46:19,140
realizar acciones en alguna

1252
00:46:19,140 --> 00:46:21,720
Ah, sí, sentí un

1253
00:46:21,720 --> 00:46:24,480
paralelo realmente agradable que usted trajo con

1254
00:46:24,480 --> 00:46:28,040
la charla: las unidades conectadas localmente

1255
00:46:28,040 --> 00:46:30,960
permitieron que sus modelos

1256
00:46:30,960 --> 00:46:33,780
encarnaran estructuralmente la

1257
00:46:33,780 --> 00:46:35,640
restricción y el patrón convolucional y eso condujo a

1258
00:46:35,640 --> 00:46:37,500
estos patrones emergentes y luego, de

1259
00:46:37,500 --> 00:46:41,339
manera análoga, estaba el Doral en

1260
00:46:41,339 --> 00:46:45,680
absoluto, donde  tenían la restricción de exploración de ruta

1261
00:46:45,680 --> 00:46:48,359
correcta y entonces es

1262
00:46:48,359 --> 00:46:50,280
interesante

1263
00:46:50,280 --> 00:46:53,760
pensar en estas heurísticas de acción o

1264
00:46:53,760 --> 00:46:56,819
políticas o escasez como una

1265
00:46:56,819 --> 00:46:59,579
exploración motora conjunta, eventualmente se

1266
00:46:59,579 --> 00:47:02,339
entiende que hay dos

1267
00:47:02,339 --> 00:47:04,980
formas mutuamente opuestas de mover una articulación

1268
00:47:04,980 --> 00:47:07,079
y luego la composicionalidad.  a través de las

1269
00:47:07,079 --> 00:47:09,119
articulaciones se puede aprender en estos

1270
00:47:09,119 --> 00:47:10,680
niveles superiores una vez que está bloqueado en los

1271
00:47:10,680 --> 00:47:14,480
niveles inferiores, por lo que es una forma muy atractiva

1272
00:47:14,480 --> 00:47:17,599
y

1273
00:47:17,599 --> 00:47:20,460
relevante de generalizar

1274
00:47:20,460 --> 00:47:23,819
porque se basa en las

1275
00:47:23,819 --> 00:47:25,740
limitaciones reales del mundo, pero

1276
00:47:25,740 --> 00:47:27,720
especialmente a través de la acción que

1277
00:47:27,720 --> 00:47:29,460
potencialmente incorpora algo que es

1278
00:47:29,460 --> 00:47:31,380
bastante simple, cierto,

1279
00:47:31,380 --> 00:47:33,599
sí, no, creo que eso es definitivamente

1280
00:47:33,599 --> 00:47:36,599
cierto, es un muy buen punto si

1281
00:47:36,599 --> 00:47:38,339
tienes limitaciones provenientes de tus

1282
00:47:38,339 --> 00:47:40,500
propias acciones, entonces eso

1283
00:47:40,500 --> 00:47:42,839
sería enormemente beneficioso para ayudar

1284
00:47:42,839 --> 00:47:44,819
a estructurar

1285
00:47:44,819 --> 00:47:47,460
tu espacio latente y creo que sí,

1286
00:47:47,460 --> 00:47:48,480
supongo que una cosa.  Quería mencionar que

1287
00:47:48,480 --> 00:47:49,980
hay

1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
algo que me hizo pensar en el

1290
00:47:52,740 --> 00:47:55,500
trabajo de Stefano Fousey sobre el tipo de

1291
00:47:55,500 --> 00:47:58,859
geometría representacional que

1292
00:47:58,859 --> 00:48:01,920
determina cómo podemos

1293
00:48:01,920 --> 00:48:04,440
generalizar una comprensión dada

1294
00:48:04,440 --> 00:48:08,099
de un sistema y creo que si puedes

1295
00:48:08,099 --> 00:48:11,880
entender cómo son estos conjuntos de

1296
00:48:11,880 --> 00:48:14,520
actividades.  separable o altamente

1297
00:48:14,520 --> 00:48:16,079
paralelo separable con un

1298
00:48:16,079 --> 00:48:18,839
clasificador lineal esencialmente, entonces

1299
00:48:18,839 --> 00:48:20,700
podrás hacer una generalización y

1300
00:48:20,700 --> 00:48:23,099
creo que al imponer este tipo de sesgos

1301
00:48:23,099 --> 00:48:25,040
o potencialmente a través de

1302
00:48:25,040 --> 00:48:27,000
restricciones impuestas por una acción

1303
00:48:27,000 --> 00:48:28,740
como esta,

1304
00:48:28,740 --> 00:48:32,040
estás cediendo o de alguna manera induciendo un

1305
00:48:32,040 --> 00:48:33,660
mejor geometría representacional y

1306
00:48:33,660 --> 00:48:35,220
esto tiene todo tipo de beneficios para la

1307
00:48:35,220 --> 00:48:36,660
composicionalidad,

1308
00:48:36,660 --> 00:48:39,359
sí, nuestra generalización, por lo que

1309
00:48:39,359 --> 00:48:41,760
es un gran punto. Genial, sí, un

1310
00:48:41,760 --> 00:48:43,440
área muy interesante. Vale, leeré

1311
00:48:43,440 --> 00:48:45,960
algunas preguntas del chat en vivo.

1312
00:48:45,960 --> 00:48:48,420
Me encanta evolucionar. Escribí

1313
00:48:48,420 --> 00:48:52,260
cualquier limitación práctica u observada en el

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
aprendizaje del modelado de ilusiones.  Las comunidades no están

1316
00:49:00,420 --> 00:49:03,060
fobias, no tienes un centro de

1317
00:49:03,060 --> 00:49:05,940
mirada, entonces tampoco tienes

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
un tiempo, me refiero a la mayoría de las

1320
00:49:11,460 --> 00:49:13,260
redes neuronales convolucionales. Estoy usando este tipo de

1321
00:49:13,260 --> 00:49:15,599
redes neuronales recurrentes, pero el tiempo

1322
00:49:15,599 --> 00:49:18,420
no está tan claramente definido.  en estos modelos,

1323
00:49:18,420 --> 00:49:20,220
ya que es en un entorno de tiempo continuo

1324
00:49:20,220 --> 00:49:23,400
para un humano que se somete a una prueba de ilusión,

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319
um,

1327
00:49:25,319 --> 00:49:27,480
y creo que la combinación de estos dos

1328
00:49:27,480 --> 00:49:30,359
del hecho de que, como humano o la mayoría de las

1329
00:49:30,359 --> 00:49:33,300
cosas,

1330
00:49:33,300 --> 00:49:35,940
tu mirada, tus ubicaciones cambiantes y

1331
00:49:35,940 --> 00:49:38,220
tus ganancias dependen de  como si

1332
00:49:38,220 --> 00:49:40,140
miraras un área en particular, hay muchas

1333
00:49:40,140 --> 00:49:42,780
pruebas cognitivas, por lo que creo que

1334
00:49:42,780 --> 00:49:46,560
sería muy útil si tuviéramos modelos,

1335
00:49:46,560 --> 00:49:48,540
sí, quiero decir, aprender que puedes pensar en

1336
00:49:48,540 --> 00:49:50,760
esto como un tipo de acción, como aprender

1337
00:49:50,760 --> 00:49:52,980
dónde mover la mirada en una de  lo

1338
00:49:52,980 --> 00:49:54,420
más simple posible, eso ayudaría mucho

1339
00:49:54,420 --> 00:49:56,220
para poder modelar ilusiones y

1340
00:49:56,220 --> 00:49:58,859
solo quiero decir, para mí es como si leyera un

1341
00:49:58,859 --> 00:50:00,720
artículo sobre algunos experimentos de ciencia cognitiva

1342
00:50:00,720 --> 00:50:02,940
o sobre alguna ilusión y

1343
00:50:02,940 --> 00:50:05,160
es como si pensara: ¿puedo poner este

1344
00:50:05,160 --> 00:50:07,560
conjunto de datos en mi?  modelo y pruébalo y la

1345
00:50:07,560 --> 00:50:08,579
mayoría de las veces la respuesta es no

1346
00:50:08,579 --> 00:50:10,619
porque no tengo un modelo que

1347
00:50:10,619 --> 00:50:12,900
mire a su alrededor o tenga un campo

1348
00:50:12,900 --> 00:50:14,660
de visión restringido, algo así, así que sí,

1349
00:50:14,660 --> 00:50:16,619
creo que esa es una de las

1350
00:50:16,619 --> 00:50:19,680
limitaciones, otra es

1351
00:50:19,680 --> 00:50:20,579
um,

1352
00:50:20,579 --> 00:50:22,920
sí, haz el  experimento mucho más

1353
00:50:22,920 --> 00:50:24,900
complicado, así que esa es una de las

1354
00:50:24,900 --> 00:50:27,359
limitaciones prácticas.

1355
00:50:27,359 --> 00:50:30,240
Vaya, la gran respuesta me hace pensar en un

1356
00:50:30,240 --> 00:50:33,920
papel con letras girando sobre una mesa

1357
00:50:33,920 --> 00:50:36,780
que es la rotación de los dígitos. Grandes puntos

1358
00:50:36,780 --> 00:50:38,460
sobre la foveación y la dinámica de

1359
00:50:38,460 --> 00:50:40,079
la ilusión. Creo que en realidad

1360
00:50:40,079 --> 00:50:42,599
mencionaste una ilusión que  Sin embargo,

1361
00:50:42,599 --> 00:50:43,980
mencionaste en el contexto de generalización

1362
00:50:43,980 --> 00:50:46,859
que la rotación en la

1363
00:50:46,859 --> 00:50:49,500
pantalla bidimensional no se generaliza a tres

1364
00:50:49,500 --> 00:50:52,920
dimensiones y ese colapso

1365
00:50:52,920 --> 00:50:55,559
o reducción dimensional es la base de las

1366
00:50:55,559 --> 00:50:58,619
ilusiones de proyección del cubo y la rotación de cubos y figuras. Las ilusiones

1367
00:50:58,619 --> 00:51:01,880
están en tu pantalla

1368
00:51:01,880 --> 00:51:05,280
y hay  una silueta o hay algunos

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
estímulos ambiguos que un generativo está cerca de la

1371
00:51:09,839 --> 00:51:12,359
criticidad o una bifurcación en

1372
00:51:12,359 --> 00:51:13,680
modelos degenerativos por lo que podría

1373
00:51:13,680 --> 00:51:17,160
representarlo de una manera u otra

1374
00:51:17,160 --> 00:51:19,920
y por eso muchas de las ilusiones cambiantes

1375
00:51:19,920 --> 00:51:22,020
se basan simplemente en la planitud de las

1376
00:51:22,020 --> 00:51:23,819
imágenes

1377
00:51:23,819 --> 00:51:26,280
y las limitaciones y generalización.

1378
00:51:26,280 --> 00:51:28,740
que se revelan con ese

1379
00:51:28,740 --> 00:51:32,460
derecho, sí, sí, creo que incluso hay Oh

1380
00:51:32,460 --> 00:51:34,859
Sí en algún lugar, lo siento, hay algo de trabajo o

1381
00:51:34,859 --> 00:51:35,880


1382
00:51:35,880 --> 00:51:37,619
pueden argumentar que la gente tiene una

1383
00:51:37,619 --> 00:51:39,480
imagen tridimensional en sus cabezas,

1384
00:51:39,480 --> 00:51:42,000
como si incluso Nancy Ken fuera una o sus

1385
00:51:42,000 --> 00:51:45,119
laterales recientemente, pero y muestra que sí, no lo hago.

1386
00:51:45,119 --> 00:51:48,119
No sé, nuestros modelos tienen que

1387
00:51:48,119 --> 00:51:50,460
no es súper grande de

1388
00:51:50,460 --> 00:51:53,700


1389
00:51:53,700 --> 00:51:56,160
todos modos, sí, eso es bastante interesante. Está bien, desde Upcycle Club en el

1390
00:51:56,160 --> 00:51:58,200
chat escribieron felicitaciones

1391
00:51:58,200 --> 00:52:00,000
si eres capaz de aprender con la misma

1392
00:52:00,000 --> 00:52:02,160
eficacia si imaginas que solo quieres que

1393
00:52:02,160 --> 00:52:03,780
haya una neurona.  activo para cada

1394
00:52:03,780 --> 00:52:06,540
ejemplo, tu modelo

1395
00:52:06,540 --> 00:52:08,579
intentará memorizar el diseño del conjunto de datos

1396
00:52:08,579 --> 00:52:10,980
o algo como esto

1397
00:52:10,980 --> 00:52:12,180
y no tendrás suficiente

1398
00:52:12,180 --> 00:52:14,940
capacidad, así que sí, creo que ajustar ese

1399
00:52:14,940 --> 00:52:18,359
nivel de escasez es ciertamente

1400
00:52:18,359 --> 00:52:22,200
un factor importante y

1401
00:52:22,200 --> 00:52:25,020
sí.  cuando miras la probabilidad, si estás

1402
00:52:25,020 --> 00:52:26,220
hablando, si estás duplicando el

1403
00:52:26,220 --> 00:52:28,579
marco, generalmente esto se equilibra

1404
00:52:28,579 --> 00:52:32,040
automáticamente con la probabilidad misma.

1405
00:52:32,040 --> 00:52:33,000
um,

1406
00:52:33,000 --> 00:52:34,380
si no estás generando modelado,

1407
00:52:34,380 --> 00:52:35,760
solo tienes una penalización por escasez que

1408
00:52:35,760 --> 00:52:38,460
querrás sintonizar.  ese parámetro

1409
00:52:38,460 --> 00:52:40,980
está bien, sí, es solo para aclarar el

1410
00:52:40,980 --> 00:52:43,380
comportamiento descontrolado en armina donde la

1411
00:52:43,380 --> 00:52:45,599
red se vuelve inestable o caótica debido

1412
00:52:45,599 --> 00:52:47,040
a varios factores, como el

1413
00:52:47,040 --> 00:52:50,400
ruido de los bucles de retroalimentación o entradas adversas.

1414
00:52:50,400 --> 00:52:52,380
um,

1415
00:52:52,380 --> 00:52:54,180
sí, supongo que no he visto esto

1416
00:52:54,180 --> 00:52:55,859
en una configuración recurrente donde

1417
00:52:55,859 --> 00:52:58,500
obtendrías bucles de retroalimentación,

1418
00:52:58,500 --> 00:52:59,460
um,

1419
00:52:59,460 --> 00:53:01,800
pero podría, sí, podría ver

1420
00:53:01,800 --> 00:53:04,319
ejemplos contradictorios potencialmente

1421
00:53:04,319 --> 00:53:07,800
afectados por tu nivel de escasez.

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
El punto interesante es qué

1424
00:53:10,859 --> 00:53:12,660
serías más susceptible o menos susceptible

1425
00:53:12,660 --> 00:53:16,040
de compartir ejemplos. No lo sé

1426
00:53:16,040 --> 00:53:19,440
bien, la escasez

1427
00:53:19,440 --> 00:53:21,720
se proyecta desde un  modelo de dimensiones superiores completamente conectado

1428
00:53:21,720 --> 00:53:23,579
solo a

1429
00:53:23,579 --> 00:53:25,619
progresivamente más pequeño,

1430
00:53:25,619 --> 00:53:27,540
en general se entiende bastante bien

1431
00:53:27,540 --> 00:53:29,760
cuáles son las compensaciones, son cálculos más fáciles, un

1432
00:53:29,760 --> 00:53:34,079
modelo más pequeño, más escaso,

1433
00:53:34,079 --> 00:53:36,420
el gráfico básico será más claro

1434
00:53:36,420 --> 00:53:39,119
de representar y luego también tendrá

1435
00:53:39,119 --> 00:53:41,339
todas las demás compensaciones.  Hay errores con falsos

1436
00:53:41,339 --> 00:53:43,680
positivos y negativos de la generalización,

1437
00:53:43,680 --> 00:53:45,720
pero es por eso que es un proceso de ajuste iterativo,

1438
00:53:45,720 --> 00:53:47,579
así que

1439
00:53:47,579 --> 00:53:49,760
supongo que cómo funciona su

1440
00:53:49,760 --> 00:53:52,800
enfoque de dispersión. El

1441
00:53:52,800 --> 00:53:55,700
equilibrio

1442
00:53:56,700 --> 00:53:59,520
no utiliza AIC o Bic o algún otro

1443
00:53:59,520 --> 00:54:01,619
enfoque de ajuste de modelos para determinar la

1444
00:54:01,619 --> 00:54:03,660
dispersión relevante

1445
00:54:03,660 --> 00:54:07,079
para una entrada determinada. ¿

1446
00:54:07,079 --> 00:54:09,780
Cómo se hace?  determinar qué tan

1447
00:54:09,780 --> 00:54:11,940


1448
00:54:11,940 --> 00:54:14,339


1449
00:54:14,339 --> 00:54:17,220
escaso quieres que sea

1450
00:54:17,220 --> 00:54:19,559
correcto sí, creo que hay mucha buena

1451
00:54:19,559 --> 00:54:22,440
literatura sobre esto y aun así a algunas

1452
00:54:22,440 --> 00:54:25,319
personas les gustan en Harvard y  algunas

1453
00:54:25,319 --> 00:54:29,520
personas están trabajando ahora, han hecho

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
este tipo de

1456
00:54:34,380 --> 00:54:36,780
redes de dispersión iterativas desenrolladas

1457
00:54:36,780 --> 00:54:37,800
donde es como una red neuronal recurrente

1458
00:54:37,800 --> 00:54:40,380
y se dispersa iterativamente y

1459
00:54:40,380 --> 00:54:41,940
puedes demostrar que esto produce algo

1460
00:54:41,940 --> 00:54:45,780
como pérdida roja o grupo como grupo activo,

1461
00:54:45,780 --> 00:54:47,520
grupo activo, activaciones deportivas como

1462
00:54:47,520 --> 00:54:48,960
nosotros.  Estamos usando aquí

1463
00:54:48,960 --> 00:54:52,859
um en esta configuración uh en realidad es solo

1464
00:54:52,859 --> 00:54:55,859
al tener esta

1465
00:54:55,859 --> 00:54:59,280
um esta construcción de esta variable T

1466
00:54:59,280 --> 00:55:04,079
donde tenemos Z en la parte superior y uh

1467
00:55:04,079 --> 00:55:07,859
y luego, de algún modo, está controlado por

1468
00:55:07,859 --> 00:55:09,119
estas

1469
00:55:09,119 --> 00:55:11,579
la suma de las variables U en la parte inferior, así que

1470
00:55:11,579 --> 00:55:13,200
si W  tal vez no estaba muy claro acerca de

1471
00:55:13,200 --> 00:55:16,500
esto es una matriz que conecta

1472
00:55:16,500 --> 00:55:18,359
eso es lo que define al grupo así que estoy

1473
00:55:18,359 --> 00:55:20,400
definiendo el Varsity del grupo que

1474
00:55:20,400 --> 00:55:22,380
conecta todas estas U juntas y entonces

1475
00:55:22,380 --> 00:55:23,940
la idea es

1476
00:55:23,940 --> 00:55:27,540
como aquí si todos de uno de los otros

1477
00:55:27,540 --> 00:55:31,740
ejemplos si todos sus usos uh no están

1478
00:55:31,740 --> 00:55:35,520
activos para una t determinada

1479
00:55:35,520 --> 00:55:38,280
o si todos los varios están activos para una

1480
00:55:38,280 --> 00:55:41,040
t determinada uh, esa variable t va a ser muy

1481
00:55:41,040 --> 00:55:42,780
pequeña, porque su denominador

1482
00:55:42,780 --> 00:55:44,339
va a ser muy grande y eso induce

1483
00:55:44,339 --> 00:55:47,160
escasez, por lo que es  uh, es una

1484
00:55:47,160 --> 00:55:49,260
satisfacción de restricción si tienes un

1485
00:55:49,260 --> 00:55:51,839
conjunto de U que son todas pequeñas uh, entonces

1486
00:55:51,839 --> 00:55:54,480
esa restricción se satisface y

1487
00:55:54,480 --> 00:55:57,180
ahora Z puede expresarse

1488
00:55:57,180 --> 00:56:00,240
y eso es lo que entonces,

1489
00:56:00,240 --> 00:56:02,880
sí, se logra en cuanto a

1490
00:56:02,880 --> 00:56:06,180
Activación.  entonces esto es inducido por estos

1491
00:56:06,180 --> 00:56:07,020
dos

1492
00:56:07,020 --> 00:56:09,300
términos de divergencia de uh kale aquí

1493
00:56:09,300 --> 00:56:12,960
dicen qué tan lejos está cada unhc

1494
00:56:12,960 --> 00:56:15,180
de un gaussiano y luego, a través de esta

1495
00:56:15,180 --> 00:56:16,980
construcción de la variable T de estudiante,

1496
00:56:16,980 --> 00:56:20,880
estamos construyendo efectivamente una

1497
00:56:20,880 --> 00:56:23,040
distribución previa escasa solo a partir de estos

1498
00:56:23,040 --> 00:56:24,839
gaussianos, pero en  Los términos del acto, el

1499
00:56:24,839 --> 00:56:27,599
objetivo real, los términos y el

1500
00:56:27,599 --> 00:56:28,920
objetivo que estamos optimizando son solo

1501
00:56:28,920 --> 00:56:31,619
estos dos términos de KL que lo están empujando

1502
00:56:31,619 --> 00:56:34,020
hacia la escasez hasta cierto punto y esto

1503
00:56:34,020 --> 00:56:36,359
se equilibra automáticamente con el

1504
00:56:36,359 --> 00:56:39,000
término de probabilidad aquí a través del decodificador,

1505
00:56:39,000 --> 00:56:41,220
por lo que no lo hacemos.  No hay términos que estamos ajustando,

1506
00:56:41,220 --> 00:56:42,839
pero estamos aprendiendo los parámetros de

1507
00:56:42,839 --> 00:56:44,280
estos diferentes codificadores y luego

1508
00:56:44,280 --> 00:56:48,200
analizando los fallos y las emergencias. Ah, está

1509
00:56:48,540 --> 00:56:49,920


1510
00:56:49,920 --> 00:56:52,859
bien, otra pregunta de Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas, que escribió

1512
00:56:55,500 --> 00:56:59,160
hablando de la mirada y la ilusión, ¿se pueden separar los

1513
00:56:59,160 --> 00:57:01,920
estudios sobre las constancias en los bebés?

1514
00:57:01,920 --> 00:57:04,260
en ilusión de nivel inferior Rel

1515
00:57:04,260 --> 00:57:06,140
quizás constancia conceptual de nivel superior

1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
uh, ¿puedes leer

1518
00:57:15,720 --> 00:57:18,300
el tipo actual de arquitectura? ¿Podrían

1519
00:57:18,300 --> 00:57:23,520


1520
00:57:23,520 --> 00:57:26,640
separarse los estudios sobre las constancias en bebés? um, las constancias cognitivas,

1521
00:57:26,640 --> 00:57:31,619
sí, probablemente no lo soy, no soy

1522
00:57:31,619 --> 00:57:33,900
un experto o, de hecho, incluso muy familiar

1523
00:57:33,900 --> 00:57:35,460
con

1524
00:57:35,460 --> 00:57:37,859
estudios de permanencia de objetos similares e infantes

1525
00:57:37,859 --> 00:57:40,260
y cosas de constancia, pero creo que

1526
00:57:40,260 --> 00:57:42,300
sería increíblemente interesante estudiar

1527
00:57:42,300 --> 00:57:44,339
en arquitecturas de redes neuronales y

1528
00:57:44,339 --> 00:57:46,740
esa fue una especie de idea con

1529
00:57:46,740 --> 00:57:48,780
esta ilusión que estaba tratando de

1530
00:57:48,780 --> 00:57:51,780
modelar aquí con esta línea.  No

1531
00:57:51,780 --> 00:57:53,099
sé si fui muy claro al respecto, pero

1532
00:57:53,099 --> 00:57:55,380
la fila superior es la entrada y

1533
00:57:55,380 --> 00:57:57,359
efectivamente estamos bloqueando la entrada para

1534
00:57:57,359 --> 00:58:00,119
un solo cuadro y quería ver si

1535
00:58:00,119 --> 00:58:03,240
la red codifica de alguna manera que la

1536
00:58:03,240 --> 00:58:05,400
cosa todavía está allí.  cuando ese cuadro

1537
00:58:05,400 --> 00:58:07,680
desaparece, ¿puedo todavía decodificar la presencia

1538
00:58:07,680 --> 00:58:10,020
del objeto a partir de la actividad neuronal? Ah,

1539
00:58:10,020 --> 00:58:11,819
y entonces, ¿qué infiere también sobre

1540
00:58:11,819 --> 00:58:13,920
el movimiento debido al hecho de que

1541
00:58:13,920 --> 00:58:15,780
vio las barras en una ubicación ligeramente diferente

1542
00:58:15,780 --> 00:58:18,480
a la de antes, cuando después

1543
00:58:18,480 --> 00:58:20,520
del  El marco se ha ido,

1544
00:58:20,520 --> 00:58:22,559
así que

1545
00:58:22,559 --> 00:58:25,200
sí, creo que definitivamente son múltiples

1546
00:58:25,200 --> 00:58:27,240
niveles,

1547
00:58:27,240 --> 00:58:29,160
donde algunos probablemente tendrían un

1548
00:58:29,160 --> 00:58:33,180
nivel mucho más bajo y tal

1549
00:58:33,180 --> 00:58:35,880
vez la permanencia del objeto a largo plazo,

1550
00:58:35,880 --> 00:58:37,380
supongo, sería un

1551
00:58:37,380 --> 00:58:39,059
nivel significativamente más alto,

1552
00:58:39,059 --> 00:58:39,900


1553
00:58:39,900 --> 00:58:41,760
solo me hace pensar en esos

1554
00:58:41,760 --> 00:58:44,640
experimentos con gatos.  En el pasado,

1555
00:58:44,640 --> 00:58:47,280
era como si los criaran en la

1556
00:58:47,280 --> 00:58:49,020
oscuridad, excepto que durante una hora al día

1557
00:58:49,020 --> 00:58:51,000
los ponían en el mundo vertical o en el

1558
00:58:51,000 --> 00:58:53,160
mundo horizontal, o solo veían líneas horizontales

1559
00:58:53,160 --> 00:58:57,299
o líneas verticales, uh, y se puede ver que

1560
00:58:57,299 --> 00:58:59,880
la organización de su corteza cambia

1561
00:58:59,880 --> 00:59:02,819
como si  tienen menos receptividad son

1562
00:59:02,819 --> 00:59:04,200
líneas horizontales si nunca

1563
00:59:04,200 --> 00:59:06,420
antes han visto líneas horizontales y luego

1564
00:59:06,420 --> 00:59:07,980
tomas un palo y lo agitas frente a

1565
00:59:07,980 --> 00:59:09,599
su cara y si el palo es

1566
00:59:09,599 --> 00:59:11,220
horizontal simplemente no hacen nada,

1567
00:59:11,220 --> 00:59:12,900
es vertical, lo golpean

1568
00:59:12,900 --> 00:59:14,460
están tratando de golpearlo, es como si

1569
00:59:14,460 --> 00:59:15,900
literalmente no tuvieran que hacer una barra

1570
00:59:15,900 --> 00:59:18,420
frente a su cara, así que creo que en ese

1571
00:59:18,420 --> 00:59:20,700
caso esto es evidencia de una

1572
00:59:20,700 --> 00:59:24,260
deficiencia de bajo nivel y de que la visión

1573
00:59:24,260 --> 00:59:26,940
contribuye a algún tipo de ilusión,

1574
00:59:26,940 --> 00:59:28,980
así que yo.  Creo que sí, ciertamente podría

1575
00:59:28,980 --> 00:59:30,660
haber algún aspecto de eso en los bebés.

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
Un punto muy curioso que mencionaste

1578
00:59:36,420 --> 00:59:40,339
fue la variedad animada e inanimada

1579
00:59:40,339 --> 00:59:43,619
con las cosas pequeñas como

1580
00:59:43,619 --> 00:59:45,480
intermedias, ¿

1581
00:59:45,480 --> 00:59:49,140
qué representa eso?

1582
00:59:49,140 --> 00:59:52,319
o es porque son manejables

1583
00:59:52,319 --> 00:59:55,619
o  podría ser un insecto o podría ser

1584
00:59:55,619 --> 00:59:57,839
algo que podría alejarse solo con el

1585
00:59:57,839 --> 01:00:01,380
viento o qué dice eso, sí,

1586
01:00:01,380 --> 01:00:04,380


1587
01:00:04,380 --> 01:00:08,280
entonces este es un trabajo de Talia Conkle.

1588
01:00:08,280 --> 01:00:11,280
Creo que fue quien descubrió esta

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
organización y trataron de

1591
01:00:14,880 --> 01:00:16,440
resolverlo.  No, es posible que me esté

1592
01:00:16,440 --> 01:00:19,500
equivocando, así que recomiendo a la gente que lea

1593
01:00:19,500 --> 01:00:21,359
su trabajo sobre eso si lo llaman

1594
01:00:21,359 --> 01:00:23,880
organización tripartita, pero si no

1595
01:00:23,880 --> 01:00:25,319
recuerdo mal,

1596
01:00:25,319 --> 01:00:27,900
hicieron mucho trabajo de seguimiento sobre por qué

1597
01:00:27,900 --> 01:00:30,780
existe esta organización y

1598
01:00:30,780 --> 01:00:33,180
alguna evidencia de

1599
01:00:33,180 --> 01:00:35,700
curvatura de estos objetos y algo así

1600
01:00:35,700 --> 01:00:37,440
como la distancia desde la que ves los objetos

1601
01:00:37,440 --> 01:00:40,260
o como

1602
01:00:40,260 --> 01:00:43,319
objetos animados o tal vez más curvos

1603
01:00:43,319 --> 01:00:45,599
o hay independientemente de cuál sea la

1604
01:00:45,599 --> 01:00:46,859
respuesta real, hubo muchas

1605
01:00:46,859 --> 01:00:48,720
hipótesis diferentes que surgieron

1606
01:00:48,720 --> 01:00:51,720
de propiedades similares de estos objetos.

1607
01:00:51,720 --> 01:00:54,000
tal vez las propiedades de nivel medio o bajo

1608
01:00:54,000 --> 01:00:56,280
más que las propiedades de nivel superior.

1609
01:00:56,280 --> 01:00:57,599
Todavía no sé si se ha

1610
01:00:57,599 --> 01:00:59,339
resuelto exactamente si es como si la

1611
01:00:59,339 --> 01:01:01,500
interacción como dijiste con los

1612
01:01:01,500 --> 01:01:04,920
objetos causa la separación o

1613
01:01:04,920 --> 01:01:06,119
um

1614
01:01:06,119 --> 01:01:09,540
o sí, las formas generales de estos

1615
01:01:09,540 --> 01:01:12,299
objetos.  Apostaría que, como ocurre con la mayoría de las cosas,

1616
01:01:12,299 --> 01:01:13,980
es como una combinación de todo lo

1617
01:01:13,980 --> 01:01:16,980
anterior, pero creo que lo interesante

1618
01:01:16,980 --> 01:01:18,480
desde este punto de vista del modelado

1619
01:01:18,480 --> 01:01:19,859
es que

1620
01:01:19,859 --> 01:01:21,480


1621
01:01:21,480 --> 01:01:24,059
esto solo se entrena con

1622
01:01:24,059 --> 01:01:26,819
estadísticas de correlación de los conjuntos de datos de imágenes,

1623
01:01:26,819 --> 01:01:28,799
por lo que no tiene interacción.

1624
01:01:28,799 --> 01:01:32,760
no tiene noción de animacidad uh, quiero decir, esto

1625
01:01:32,760 --> 01:01:34,140
en realidad es solo entrenar un modelo en

1626
01:01:34,140 --> 01:01:37,859
imagenet, solo imágenes de perros, gatos, barcos,

1627
01:01:37,859 --> 01:01:40,020
lo que sea y aún así logra este

1628
01:01:40,020 --> 01:01:41,640
tipo de organización, por lo que hay algún

1629
01:01:41,640 --> 01:01:42,540
tipo de

1630
01:01:42,540 --> 01:01:44,940
características semánticas,

1631
01:01:44,940 --> 01:01:46,740
correcto, tenemos una imagen, tenemos una  red

1632
01:01:46,740 --> 01:01:48,359
que puede clasificar

1633
01:01:48,359 --> 01:01:51,000
barcos versus perros versus otras 20 razas

1634
01:01:51,000 --> 01:01:53,640
de perros, pero si

1635
01:01:53,640 --> 01:01:55,920
también podría tener alguna correspondencia

1636
01:01:55,920 --> 01:01:57,900
con estadísticas de finalización de nivel inferior,

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
así que sí, no lo sé, supongo que

1639
01:02:03,960 --> 01:02:07,500
sí, una analogía provocativa fue el

1640
01:02:07,500 --> 01:02:10,380
cambio de traducción

1641
01:02:10,380 --> 01:02:12,900
en el mnist en la escritura

1642
01:02:12,900 --> 01:02:14,819
configuración de reconocimiento

1643
01:02:14,819 --> 01:02:18,000
cuáles son los cambios de traducción

1644
01:02:18,000 --> 01:02:20,160
que

1645
01:02:20,160 --> 01:02:22,740
existen hoy en día cuál es el ejemplo de los tres píxeles

1646
01:02:22,740 --> 01:02:24,780
es que algún ataque rápido diseñado

1647
01:02:24,780 --> 01:02:27,540
en una película o

1648
01:02:27,540 --> 01:02:29,099
algo así se inserta un carácter especial

1649
01:02:29,099 --> 01:02:32,640
o una

1650
01:02:32,640 --> 01:02:35,160
superposición en una imagen que ni siquiera podemos

1651
01:02:35,160 --> 01:02:37,020
detectar eso,

1652
01:02:37,020 --> 01:02:39,359
entonces, ¿cuáles crees que son esos desafíos

1653
01:02:39,359 --> 01:02:42,839
y cuáles son las formas en que podemos perseguir

1654
01:02:42,839 --> 01:02:44,940
eso,

1655
01:02:44,940 --> 01:02:47,520
sí, absolutamente? Quiero decir, creo que es más o menos

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
la forma en que estaba pensando, es como

1658
01:02:50,099 --> 01:02:52,680
estas transformaciones de simetría,

1659
01:02:52,680 --> 01:02:53,760
um,

1660
01:02:53,760 --> 01:02:55,799
si estás pensando en modelos de lenguaje,  ¿

1661
01:02:55,799 --> 01:02:57,420
Puedo imaginar una

1662
01:02:57,420 --> 01:02:58,500
transformación de simetría que es como

1663
01:02:58,500 --> 01:03:00,240
reemplazar una palabra con un sinónimo o

1664
01:03:00,240 --> 01:03:03,780
algo así? Tienes la oración para nosotros que

1665
01:03:03,780 --> 01:03:06,000
significa exactamente lo mismo, pero ahora,

1666
01:03:06,000 --> 01:03:07,380
de repente, el modelo va a responder de manera

1667
01:03:07,380 --> 01:03:09,299
muy diferente,

1668
01:03:09,299 --> 01:03:11,240


1669
01:03:11,240 --> 01:03:15,359
como la traducción entre idiomas, esto

1670
01:03:15,359 --> 01:03:16,799
puede verse como una  tipo de transformación

1671
01:03:16,799 --> 01:03:19,440
preserva el

1672
01:03:19,440 --> 01:03:21,960
significado subyacente de la entrada

1673
01:03:21,960 --> 01:03:24,900
para nosotros, pero para el modelo se ve

1674
01:03:24,900 --> 01:03:26,220
completamente diferente y nos gustaría

1675
01:03:26,220 --> 01:03:28,380
tener modelos que se comporten de una

1676
01:03:28,380 --> 01:03:29,940
manera predecible con respecto a estos

1677
01:03:29,940 --> 01:03:32,160
tipos de Transformaciones porque

1678
01:03:32,160 --> 01:03:35,040
creo que los humanos se comportan de manera muy predecible

1679
01:03:35,040 --> 01:03:37,319
ya que estos  Transformaciones y cuando

1680
01:03:37,319 --> 01:03:39,920
tratamos con sistemas de IA esperamos

1681
01:03:39,920 --> 01:03:43,200
que también se comporten de esa manera y creo que eso es

1682
01:03:43,200 --> 01:03:45,480
parte de lo que causa muchos

1683
01:03:45,480 --> 01:03:47,339
desafíos al interactuar con estos

1684
01:03:47,339 --> 01:03:49,460
sistemas y en cierto modo intenté hacer una

1685
01:03:49,460 --> 01:03:52,500
demostración tosca y descarada de eso

1686
01:03:52,500 --> 01:03:54,960
con esto.  oso y cuadrados y cosas

1687
01:03:54,960 --> 01:03:58,440
como um, esperamos que sea

1688
01:03:58,440 --> 01:04:00,480
capaz de hacer algo simple

1689
01:04:00,480 --> 01:04:02,220
como esto porque creemos que la mayoría de los humanos

1690
01:04:02,220 --> 01:04:04,020
podrían pero no es así y si

1691
01:04:04,020 --> 01:04:05,460
imaginas que este es un escenario crítico

1692
01:04:05,460 --> 01:04:07,740
en el que esperas esto y ese es un gran

1693
01:04:07,740 --> 01:04:08,700
problema

1694
01:04:08,700 --> 01:04:11,099
um  ¿Cómo manejamos eso? Sí,

1695
01:04:11,099 --> 01:04:12,720
creo que eso es lo que estoy

1696
01:04:12,720 --> 01:04:15,859
buscando. Creo que

1697
01:04:16,319 --> 01:04:18,420
mi

1698
01:04:18,420 --> 01:04:22,280
dirección que estoy tomando es que parezca

1699
01:04:22,280 --> 01:04:26,940
más simple y como

1700
01:04:26,940 --> 01:04:29,460
bloques de construcción de abajo hacia arriba de

1701
01:04:29,460 --> 01:04:31,380
arquitecturas de redes neuronales o

1702
01:04:31,380 --> 01:04:33,180
algoritmos que de

1703
01:04:33,180 --> 01:04:35,760
alguna manera produzcan estos emergentes.

1704
01:04:35,760 --> 01:04:37,680
propiedades estructurales y creo que es una

1705
01:04:37,680 --> 01:04:39,839
forma mucho más generalizable en lugar de

1706
01:04:39,839 --> 01:04:41,579
construir algo sobre lo que

1707
01:04:41,579 --> 01:04:43,140
ya tenemos.

1708
01:04:43,140 --> 01:04:43,920


1709
01:04:43,920 --> 01:04:45,839
Creo que es algo que escalará

1710
01:04:45,839 --> 01:04:47,819
mucho mejor y también coincidirá más con lo que

1711
01:04:47,819 --> 01:04:50,299
hace el cerebro.

1712
01:04:50,760 --> 01:04:52,740
Un tipo de pregunta de implementación muy interesante, ¿

1713
01:04:52,740 --> 01:04:54,740
cuáles son las

1714
01:04:54,740 --> 01:04:57,000
requisitos computacionales de simplemente ejecutar esto o

1715
01:04:57,000 --> 01:04:59,400
cómo es el día a día de ser un

1716
01:04:59,400 --> 01:05:01,920
estudiante o investigador ejecutando variantes

1717
01:05:01,920 --> 01:05:04,380
de estos, por ejemplo, si usan terabytes de

1718
01:05:04,380 --> 01:05:07,020
datos y estás usando una gran computación

1719
01:05:07,020 --> 01:05:08,940
o es algo que las personas pueden ejecutar

1720
01:05:08,940 --> 01:05:11,880
por su cuenta.  laptops

1721
01:05:11,880 --> 01:05:13,980
Creo que casi todo lo que presenté

1722
01:05:13,980 --> 01:05:17,099
hoy se puede ejecutar localmente, así que

1723
01:05:17,099 --> 01:05:20,040
esto es súper simple, puedes ejecutarlo. Quiero decir, ¿crees que

1724
01:05:20,040 --> 01:05:20,760


1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
puedes ejecutarlo en tu computadora portátil si

1727
01:05:24,299 --> 01:05:25,980
quieres entrenar y experimentar

1728
01:05:25,980 --> 01:05:27,420
con diferentes cosas?  ser

1729
01:05:27,420 --> 01:05:30,119
bastante lento, por lo que recomendaría alguna

1730
01:05:30,119 --> 01:05:33,359
GPU comercial como una. Ejecuto prácticamente

1731
01:05:33,359 --> 01:05:35,640
todo, como Nvidia 1080,

1732
01:05:35,640 --> 01:05:38,819
bastante antiguo, bastante barato, pero tienen 12

1733
01:05:38,819 --> 01:05:41,099
gigas de RAM o lo que sea y es

1734
01:05:41,099 --> 01:05:43,140
más que suficiente para estos modelos, cuatro

1735
01:05:43,140 --> 01:05:46,440
gigabytes de RAM.  Creo que una cosa que

1736
01:05:46,440 --> 01:05:48,839
algunas personas piensan que es extraña es que hago la mayoría

1737
01:05:48,839 --> 01:05:51,480
de mis experimentos con cosas como mnist, por lo que

1738
01:05:51,480 --> 01:05:54,780
son imágenes de 32 por 32 píxeles porque puedo

1739
01:05:54,780 --> 01:05:57,299
entrenarlas de forma pequeña y vocal,

1740
01:05:57,299 --> 01:06:00,000
um si quieres, hazlo con mis experimentos

1741
01:06:00,000 --> 01:06:02,460
o sin fin si quieres.  Si quiero hacer cosas

1742
01:06:02,460 --> 01:06:03,540
como esta, son mucho más

1743
01:06:03,540 --> 01:06:05,640
complicadas, esta suite dinámica hamiltoniana,

1744
01:06:05,640 --> 01:06:08,160
aquí está ingresando a modelos más grandes

1745
01:06:08,160 --> 01:06:09,780
que se ejecutan en múltiples

1746
01:06:09,780 --> 01:06:12,240
GPU, por lo que aquí se usa un clúster para

1747
01:06:12,240 --> 01:06:14,220
ejecutar este tipo de modelos,

1748
01:06:14,220 --> 01:06:16,140
pero yo diría que la mayoría de  una sola

1749
01:06:16,140 --> 01:06:18,920
máquina con GPU es más que suficiente

1750
01:06:18,920 --> 01:06:21,680
o incluso como en un cuaderno de colaboración,

1751
01:06:21,680 --> 01:06:24,539
algo así. Si quieres entrenar

1752
01:06:24,539 --> 01:06:26,520
algo en imagenet, se vuelve más

1753
01:06:26,520 --> 01:06:29,940
complicado y necesitas

1754
01:06:29,940 --> 01:06:33,660
al menos una GPU, idealmente más, pero sí,

1755
01:06:33,660 --> 01:06:35,160
no hago una.  Hay un montón de cosas a gran escala,

1756
01:06:35,160 --> 01:06:37,200
pero creo que es ciertamente interesante

1757
01:06:37,200 --> 01:06:39,960
y definitivamente hay mucho más que

1758
01:06:39,960 --> 01:06:42,539
puedes hacer allí, pero para algunas de estas

1759
01:06:42,539 --> 01:06:45,480
preguntas más simples o más fundamentales,

1760
01:06:45,480 --> 01:06:47,760
no sé cómo quieres llamarlas.

1761
01:06:47,760 --> 01:06:52,500
Una máquina más pequeña es.  agradable y rápido, genial,

1762
01:06:52,500 --> 01:06:54,660
útil,

1763
01:06:54,660 --> 01:06:58,140
de acuerdo. Leeré un comentario de Dave

1764
01:06:58,140 --> 01:07:00,780
recordando el comentario de Bert DeVries durante

1765
01:07:00,780 --> 01:07:02,880
el Simposio de inferencia activa aplicada

1766
01:07:02,880 --> 01:07:05,520
sobre la conveniencia de gastar menos

1767
01:07:05,520 --> 01:07:08,099
esfuerzo o ATP en situaciones de búsqueda de alimento o control

1768
01:07:08,099 --> 01:07:09,780
en las que no necesitamos mucha

1769
01:07:09,780 --> 01:07:11,579
Precisión. No lo hago.  No sé si escuchas

1770
01:07:11,579 --> 01:07:13,859
esto, pero el profesor DeVries mencionó los

1771
01:07:13,859 --> 01:07:17,520
modelos de precisión variables y cómo

1772
01:07:17,520 --> 01:07:19,440
podrían usarse para

1773
01:07:19,440 --> 01:07:21,059
habilitar diferentes características de

1774
01:07:21,059 --> 01:07:23,039
generalización y entrenamiento estructural real del

1775
01:07:23,039 --> 01:07:25,020
curso, así como

1776
01:07:25,020 --> 01:07:27,059
requisitos computacionales reducidos. ¿

1777
01:07:27,059 --> 01:07:29,880
Tiene alguna sugerencia sobre cómo

1778
01:07:29,880 --> 01:07:31,980
introducir esto?  distinción en

1779
01:07:31,980 --> 01:07:33,900
inferencia activa Teoría de qué tipo de

1780
01:07:33,900 --> 01:07:37,760
experimentos podrían resolver esto

1781
01:07:38,400 --> 01:07:40,680
oh wow, sí, eso es algo que no,

1782
01:07:40,680 --> 01:07:42,660
no creo que tenga mucho que

1783
01:07:42,660 --> 01:07:46,619
decir sobre eso, es completamente honesto,

1784
01:07:46,619 --> 01:07:48,740


1785
01:07:51,359 --> 01:07:53,460
es una pregunta súper interesante porque creo que

1786
01:07:53,460 --> 01:07:56,220
la intuición hace un

1787
01:07:56,220 --> 01:07:58,260
Para mí tiene mucho sentido que

1788
01:07:58,260 --> 01:08:00,000
estés hablando

1789
01:08:00,000 --> 01:08:01,980
si entiendo correctamente las tasas

1790
01:08:01,980 --> 01:08:05,160
de precisión variables cuando estás codificando en

1791
01:08:05,160 --> 01:08:07,079
o en tu modelo en general haciendo

1792
01:08:07,079 --> 01:08:07,740
cálculos

1793
01:08:07,740 --> 01:08:09,420
[Música]

1794
01:08:09,420 --> 01:08:11,539
um,

1795
01:08:13,200 --> 01:08:17,040
eso de alguna manera tiene un impacto en tu

1796
01:08:17,040 --> 01:08:19,080
desempeño futuro como  una relación con algún

1797
01:08:19,080 --> 01:08:22,259
almacén de energía, creo que sí, y si

1798
01:08:22,259 --> 01:08:23,580
quisieras convertir esto en un

1799
01:08:23,580 --> 01:08:26,279
sistema de esfuerzo activo, necesitarías tener

1800
01:08:26,279 --> 01:08:28,679
realmente un sistema incorporado donde el

1801
01:08:28,679 --> 01:08:31,500
agente tenga alguna noción de energía como un

1802
01:08:31,500 --> 01:08:34,439
almacén de energía interno y

1803
01:08:34,439 --> 01:08:36,238
sí, algo que esté intentando.

1804
01:08:36,238 --> 01:08:38,520
para conservar mientras realiza sus

1805
01:08:38,520 --> 01:08:40,500
acciones

1806
01:08:40,500 --> 01:08:42,359
y quedarse sin energía

1807
01:08:42,359 --> 01:08:44,759
necesitaría algo malo para los agentes y

1808
01:08:44,759 --> 01:08:47,399
luego tal vez podrías observar una especie de

1809
01:08:47,399 --> 01:08:48,679
emergencia,

1810
01:08:48,679 --> 01:08:52,040
reducción y

1811
01:08:52,040 --> 01:08:55,198
codificación, precisión o algo

1812
01:08:55,198 --> 01:08:57,479
así, ya que el agente está tratando de aprender

1813
01:08:57,479 --> 01:09:00,060
a  actuar de manera más efectiva, es posible que tengas que

1814
01:09:00,060 --> 01:09:02,520
darle la capacidad de controlar su

1815
01:09:02,520 --> 01:09:04,020
precisión,

1816
01:09:04,020 --> 01:09:07,080
sí, como digo según mi experiencia, pero son

1817
01:09:07,080 --> 01:09:08,819
pensamientos, está

1818
01:09:08,819 --> 01:09:11,460
bien en esta diapositiva aquí, la primera

1819
01:09:11,460 --> 01:09:14,219
imagen muy interesante, es como un

1820
01:09:14,219 --> 01:09:18,359
Jackson Pollock digital,

1821
01:09:18,359 --> 01:09:22,920
um, si lo fuera.  un tamaño de datos de entrada más simple

1822
01:09:22,920 --> 01:09:26,520
o simplemente una complejidad reducida de los

1823
01:09:26,520 --> 01:09:27,719
patrones o si fuera una mayor

1824
01:09:27,719 --> 01:09:30,000
complejidad, ¿cómo se vería diferente esta imagen?

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
Sí, así que hice algunos experimentos tratando de

1827
01:09:34,620 --> 01:09:36,738
cambiar estas

1828
01:09:36,738 --> 01:09:40,439
columnas de orientación y,

1829
01:09:40,439 --> 01:09:41,100
um,

1830
01:09:41,100 --> 01:09:43,140
puedes, sí, básicamente, cambiando los

1831
01:09:43,140 --> 01:09:44,520
parámetros del modelo.  Haz que

1832
01:09:44,520 --> 01:09:47,100
estas columnas sean más grandes, puedes hacer que

1833
01:09:47,100 --> 01:09:49,380
no tengan una estructura muy similar

1834
01:09:49,380 --> 01:09:51,660
a la que vemos en los humanos donde te tenemos,

1835
01:09:51,660 --> 01:09:53,040
puedes hacer que tengan más

1836
01:09:53,040 --> 01:09:55,199
bandas de actividad,

1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
y también, como dijiste, depende

1839
01:09:58,199 --> 01:10:00,540
del conjunto de datos.  que estás usando, si uso

1840
01:10:00,540 --> 01:10:03,179
graduaciones sinusoidales realmente simples

1841
01:10:03,179 --> 01:10:05,580
como entrada, obtengo algo como esto, obtengo

1842
01:10:05,580 --> 01:10:07,920
algo que es un poco más

1843
01:10:07,920 --> 01:10:11,640
curvo rotacional, con mayor entropía,

1844
01:10:11,640 --> 01:10:12,800
um,

1845
01:10:12,800 --> 01:10:15,660
así que creo que todas estas son

1846
01:10:15,660 --> 01:10:18,000
cosas interesantes si quieres estudiar la

1847
01:10:18,000 --> 01:10:19,320
aparición de  este tipo de organización

1848
01:10:19,320 --> 01:10:22,199
en un sistema natural, uh, si tienes un

1849
01:10:22,199 --> 01:10:24,120
modelo que ahora produce una

1850
01:10:24,120 --> 01:10:25,860
organización diferente para diferentes

1851
01:10:25,860 --> 01:10:28,620
configuraciones, eso es ver, está bien, entonces qué

1852
01:10:28,620 --> 01:10:31,679
configuraciones coinciden mejor con nuestros datos observados,

1853
01:10:31,679 --> 01:10:32,340


1854
01:10:32,340 --> 01:10:34,679
así que sí,

1855
01:10:34,679 --> 01:10:36,540
puedo enviarlas si estás

1856
01:10:36,540 --> 01:10:38,520
interesado, pero

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
sí, creo que también hay otro, lo siento,

1859
01:10:44,219 --> 01:10:45,659
otro punto interesante es

1860
01:10:45,659 --> 01:10:46,920
que los

1861
01:10:46,920 --> 01:10:51,480
diferentes animales y los diferentes tipos de

1862
01:10:51,480 --> 01:10:53,219
selectividad de orientación y diferentes

1863
01:10:53,219 --> 01:10:54,659
números de molinetes, algunos animales no

1864
01:10:54,659 --> 01:10:57,420
lo tienen en absoluto. Creo que tal vez los ratones si estoy

1865
01:10:57,420 --> 01:11:00,120
correcto, tienen este tipo de

1866
01:11:00,120 --> 01:11:01,800
selectividad de sal y pimienta, por lo que es

1867
01:11:01,800 --> 01:11:03,480
básicamente aleatorio, no tienes ningún tipo

1868
01:11:03,480 --> 01:11:04,679
de sensibilidad de orientación topográfica,

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300
por lo que hay evidencia de que

1871
01:11:09,300 --> 01:11:10,920
sí, diferentes sistemas hacen esto

1872
01:11:10,920 --> 01:11:13,020
de manera diferente y es interesante

1873
01:11:13,020 --> 01:11:14,760
descubrir por qué

1874
01:11:14,760 --> 01:11:17,760
sí.  es muy bueno, me recuerda

1875
01:11:17,760 --> 01:11:21,300
primero la base de difusión de la reacción

1876
01:11:21,300 --> 01:11:22,980
y el tiempo,

1877
01:11:22,980 --> 01:11:25,739
por lo que en realidad es

1878
01:11:25,739 --> 01:11:30,000
muy posible que una región no tenga

1879
01:11:30,000 --> 01:11:32,840
actividad a partir de una

1880
01:11:32,840 --> 01:11:35,640
granularidad determinada, como si se estuviera

1881
01:11:35,640 --> 01:11:39,360
observando en una escala de tiempo espacial y temporal de fmri

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
si los focos de actividad pero  Si los

1884
01:11:44,699 --> 01:11:46,380
focos de actividad son más

1885
01:11:46,380 --> 01:11:48,480
lentos y más rápidos

1886
01:11:48,480 --> 01:11:52,260
que esa medición, no será

1887
01:11:52,260 --> 01:11:54,060
diferente del ruido, todo se habrá

1888
01:11:54,060 --> 01:11:55,739
promediado,

1889
01:11:55,739 --> 01:11:58,620
por lo que podría haber algunos, sí,

1890
01:11:58,620 --> 01:12:01,560
interesantes, como conjuntos de datos que

1891
01:12:01,560 --> 01:12:03,360
en realidad tienen mucha

1892
01:12:03,360 --> 01:12:06,179
riqueza, pero por un lado.  Por alguna razón u

1893
01:12:06,179 --> 01:12:08,520
otra, simplemente se promedió

1894
01:12:08,520 --> 01:12:11,100
porque no estaba conectado a usted

1895
01:12:11,100 --> 01:12:12,420
o algo así, realmente necesita

1896
01:12:12,420 --> 01:12:14,520
ir con un solo nivel de prueba, necesita

1897
01:12:14,520 --> 01:12:16,140
tener una resolución espacial lo suficientemente alta

1898
01:12:16,140 --> 01:12:18,719
como para que sepa que satisface las

1899
01:12:18,719 --> 01:12:23,100
microfrecuencias, uh.  y esto

1900
01:12:23,100 --> 01:12:24,659
es algo que la gente no hizo durante

1901
01:12:24,659 --> 01:12:25,620
mucho tiempo, especialmente si estás haciendo

1902
01:12:25,620 --> 01:12:27,780
grabaciones de un solo electorado, no verás

1903
01:12:27,780 --> 01:12:28,920
una onda viajera,

1904
01:12:28,920 --> 01:12:30,900
verás oscilaciones,

1905
01:12:30,900 --> 01:12:32,219
entonces necesitas algo como multieléctrico.

1906
01:12:32,219 --> 01:12:34,199
arreglos y básicamente dicen que está bien,

1907
01:12:34,199 --> 01:12:36,000
sí, ahora que tenemos la tecnología para

1908
01:12:36,000 --> 01:12:37,520
hacer esto,

1909
01:12:37,520 --> 01:12:40,080
persiste mucho que no vimos antes y

1910
01:12:40,080 --> 01:12:42,960
potencialmente esto es una explicación para

1911
01:12:42,960 --> 01:12:44,400
gran parte del ruido que estábamos viendo

1912
01:12:44,400 --> 01:12:46,260
antes, tal vez realmente sea solo  ondas viajeras

1913
01:12:46,260 --> 01:12:47,219


1914
01:12:47,219 --> 01:12:47,760
um,

1915
01:12:47,760 --> 01:12:51,480
así que sí, creo que hay mucho por hacer

1916
01:12:51,480 --> 01:12:53,880
en el futuro con mayores

1917
01:12:53,880 --> 01:12:56,520
capacidades de grabación,

1918
01:12:56,520 --> 01:12:58,739
eso es genial,

1919
01:12:58,739 --> 01:13:02,940
bueno, cualquier pensamiento o pregunta final o

1920
01:13:02,940 --> 01:13:06,239
hacia dónde llevarás este trabajo.

1921
01:13:06,239 --> 01:13:08,520
Sí, no, gracias por invitarme,

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
con suerte, en la infraestructura activa.

1924
01:13:11,640 --> 01:13:14,520
Eso es, me encantaría, creo

1925
01:13:14,520 --> 01:13:16,560
que sería súper divertido, así que sí, no estoy

1926
01:13:16,560 --> 01:13:18,980
muy seguro. Estoy mirando Quizás música,

1927
01:13:18,980 --> 01:13:21,659
uh, ahora mismo,

1928
01:13:21,659 --> 01:13:22,440
estoy

1929
01:13:22,440 --> 01:13:26,760
mirando

1930
01:13:26,760 --> 01:13:30,420
otras direcciones locas. No

1931
01:13:30,420 --> 01:13:33,020
quiero sonar demasiado loco, uh,

1932
01:13:33,020 --> 01:13:36,900
pero.  Bajaré, sí, muchas cosas, así que

1933
01:13:36,900 --> 01:13:38,580
una cosa que está surgiendo, algo que

1934
01:13:38,580 --> 01:13:40,320
enviamos a los neurops, es estudiar la

1935
01:13:40,320 --> 01:13:43,140
memoria con ondas viajeras,

1936
01:13:43,140 --> 01:13:45,060
entonces ese artículo acaba de aparecer en el

1937
01:13:45,060 --> 01:13:46,860
archivo hoy,

1938
01:13:46,860 --> 01:13:48,840
cómo las ondas son realmente buenas para codificar

1939
01:13:48,840 --> 01:13:50,580
recuerdos a largo plazo.  lo cual creo que es

1940
01:13:50,580 --> 01:13:52,100
súper interesante,

1941
01:13:52,100 --> 01:13:54,120
así que podría ir un poco en esa

1942
01:13:54,120 --> 01:13:55,800
dirección,

1943
01:13:55,800 --> 01:13:58,920
suena bien y sí, sería muy

1944
01:13:58,920 --> 01:14:01,400
emocionante ver la acción entrar en juego

1945
01:14:01,400 --> 01:14:04,560
cuando las neuronas permanecían

1946
01:14:04,560 --> 01:14:07,620
activas incluso cuando las patas del perro se

1947
01:14:07,620 --> 01:14:09,060
movían.

1948
01:14:09,060 --> 01:14:11,480
Hay muchas cosas similares.  secuencias de acción

1949
01:14:11,480 --> 01:14:14,280
como lanzar una pelota de béisbol y luego se

1950
01:14:14,280 --> 01:14:15,900
va y es como si hubiera algo

1951
01:14:15,900 --> 01:14:18,440
en esa acción que continúa

1952
01:14:18,440 --> 01:14:21,179
influyendo y, por lo tanto, tiene una

1953
01:14:21,179 --> 01:14:23,699
representación temporal profunda de acciones alternativas

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
y luego el codificador automático variacional

1956
01:14:29,640 --> 01:14:33,179
ya es básicamente lo correcto, algo así,

1957
01:14:33,179 --> 01:14:35,219
así que

1958
01:14:35,219 --> 01:14:37,739
realmente lo aprecio.  esta bien gracias

1959
01:14:37,739 --> 01:14:39,480
hasta la proxima

1960
01:14:39,480 --> 01:14:43,339
muchas gracias adios


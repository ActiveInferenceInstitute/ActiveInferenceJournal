1
00:00:06,600 --> 00:00:09,420
こんにちは、ようこそ、2023 年 9 月 18 日です。

2
00:00:09,420 --> 00:00:14,040


3
00:00:14,040 --> 00:00:16,740
アンディ ケラーとのアクティブなゲスト ストリーム 57.1 です。

4
00:00:16,740 --> 00:00:19,619


5
00:00:19,619 --> 00:00:22,020
人工知能の自然な神経構造について話します。

6
00:00:22,020 --> 00:00:24,300


7
00:00:24,300 --> 00:00:25,800
プレゼンテーションに続いてディスカッションが行われます。ライブで視聴している場合は、

8
00:00:25,800 --> 00:00:27,900
お気軽に ライブチャットに質問を書いてください。

9
00:00:27,900 --> 00:00:30,660
そうでない場合は、アンディに感謝します。

10
00:00:30,660 --> 00:00:32,279
本当に楽しみにしています。

11
00:00:32,279 --> 00:00:35,899
そしてプレゼンテーションをしてくれてありがとう。

12
00:00:36,360 --> 00:00:38,940
はい、本当にありがとう。

13
00:00:38,940 --> 00:00:41,280
私を迎えてくれてありがとう。

14
00:00:41,280 --> 00:00:42,840
アクティブなリファレンスグループでこの内容を発表できることにとても興奮しています。

15
00:00:42,840 --> 00:00:45,239
私はファンでとても

16
00:00:45,239 --> 00:00:49,200
興味があるので、うまくいけば、ええと、

17
00:00:49,200 --> 00:00:50,399
良い議論をして、

18
00:00:50,399 --> 00:00:51,960
皆さんがそれについてどう思うか見てみましょう、ええと、

19
00:00:51,960 --> 00:00:54,840
私の名前はアンディです、私はアムステルダム大学の

20
00:00:54,840 --> 00:00:57,180
マックスウェリングの監督の下で博士号を取得する予定です、ええと、

21
00:00:57,180 --> 00:00:59,100


22
00:00:59,100 --> 00:01:01,500
私はです この後ハーバード大学でポスドクを始めるので

23
00:01:01,500 --> 00:01:05,099
始めます

24
00:01:05,099 --> 00:01:07,619
私の仕事の一般的な目標について話しているのは、

25
00:01:07,619 --> 00:01:09,540
現代の人工

26
00:01:09,540 --> 00:01:12,540
知能をより人間らしい

27
00:01:12,540 --> 00:01:15,240
一般化に近づけることです。

28
00:01:15,240 --> 00:01:17,159
ある種の構造の

29
00:01:17,159 --> 00:01:18,960
一般化です。

30
00:01:18,960 --> 00:01:20,700
ええと、アクティブな

31
00:01:20,700 --> 00:01:22,080
幼児委員会にとっては、人間が持っていると信じている構造化された

32
00:01:22,080 --> 00:01:24,299
世界モデルのようなものです。これを

33
00:01:24,299 --> 00:01:26,340
行うために私たちが提案している方法は、

34
00:01:26,340 --> 00:01:28,799
自然な神経

35
00:01:28,799 --> 00:01:32,400
構造を人工知能に統合することです。

36
00:01:32,400 --> 00:01:34,979
まず、これが何を意味するのか定義しましょう したがって、

37
00:01:34,979 --> 00:01:36,720


38
00:01:36,720 --> 00:01:38,400


39
00:01:38,400 --> 00:01:40,380
現代の機械学習が従来の意味での

40
00:01:40,380 --> 00:01:42,900
トレーニング セットを超えて一般化していると言っても、かなり議論の余地はないと思います。

41
00:01:42,900 --> 00:01:45,000
たとえば、

42
00:01:45,000 --> 00:01:46,799
初期の人工ニューラル

43
00:01:46,799 --> 00:01:49,020
ネットワークの多層パーセプトロンでさえ、この

44
00:01:49,020 --> 00:01:51,659
ような画像のデータ セットでトレーニングでき、

45
00:01:51,659 --> 00:01:55,140
高いレベルのパフォーマンスを達成できます。 精度が向上し、これまで見たことのない画像の

46
00:01:55,140 --> 00:01:56,880
テスト セットが提示された場合でも、

47
00:01:56,880 --> 00:01:58,320


48
00:01:58,320 --> 00:02:00,240


49
00:02:00,240 --> 00:02:02,460
同じレベルの精度で比較的簡単に分類できます。

50
00:02:02,460 --> 00:02:04,500
これが一般

51
00:02:04,500 --> 00:02:07,320
化と呼ばれるものですが、かなり

52
00:02:07,320 --> 00:02:08,758
早い段階でさえそうでした。 これらの

53
00:02:08,758 --> 00:02:10,800
システムは、たとえば画像

54
00:02:10,800 --> 00:02:12,720
に適用される小さなシフトや変形に非常に苦労していることに気づきました。

55
00:02:12,720 --> 00:02:16,340
もしそうだとしたら、

56
00:02:18,920 --> 00:02:23,099
これがなぜ驚くべきことなのか考えてみ

57
00:02:23,099 --> 00:02:24,720
てください。この例が失敗しているのは、この種の構造一般化を実行する人間の生来の能力によるものだと私は主張します。

58
00:02:24,720 --> 00:02:26,640


59
00:02:26,640 --> 00:02:28,680


60
00:02:28,680 --> 00:02:31,920
ええと、

61
00:02:31,920 --> 00:02:33,239
たとえば、この変化は

62
00:02:33,239 --> 00:02:35,340
私たちにはほとんど知覚できず、自動的に処理されますが、

63
00:02:35,340 --> 00:02:37,560
システム内では

64
00:02:37,560 --> 00:02:39,959
明らかに大きな問題であるため、言葉で言えば、

65
00:02:39,959 --> 00:02:41,940
構造の一般化は、

66
00:02:41,940 --> 00:02:44,819


67
00:02:44,819 --> 00:02:47,040
入力のいくつかの対称変換、この場合は

68
00:02:47,040 --> 00:02:48,959
対称性の一般化であると言えます。 変換は

69
00:02:48,959 --> 00:02:50,580
数字クラスを変更しない小さなシフトです。

70
00:02:50,580 --> 00:02:52,019


71
00:02:52,019 --> 00:02:54,599
したがって、当然の疑問は、

72
00:02:54,599 --> 00:02:56,340
この自然な構造とは正確に何を意味するのか

73
00:02:56,340 --> 00:02:58,860
、そしてなぜこれが

74
00:02:58,860 --> 00:03:01,620
これらの設定に役立つと思うのかということです。

75
00:03:01,620 --> 00:03:03,780
まず、自然なニューラルとは何を意味するのかについて話しましょう。

76
00:03:03,780 --> 00:03:05,819
構造

77
00:03:05,819 --> 00:03:08,700
構造、または

78
00:03:08,700 --> 00:03:11,640
システム内のあらゆるタイプのバイアスについて話す 1 つの方法は、

79
00:03:11,640 --> 00:03:14,040
帰納的バイアスです。したがって、帰納的バイアスは、

80
00:03:14,040 --> 00:03:16,080


81
00:03:16,080 --> 00:03:17,940


82
00:03:17,940 --> 00:03:19,440


83
00:03:19,440 --> 00:03:22,440
より口語的にモデルの選択を行うときに、実現可能な一連の仮説の事前制限として大まかに定義できます。

84
00:03:22,440 --> 00:03:24,480
これは、データを見る前のようなものと呼ぶことができます。

85
00:03:24,480 --> 00:03:27,060


86
00:03:27,060 --> 00:03:29,580
これは、非常に広範囲に何をどのように学習できるかの制限です。これには、

87
00:03:29,580 --> 00:03:32,519
モデルクラスから

88
00:03:32,519 --> 00:03:34,739
最適化手順、さらにはハイパー

89
00:03:34,739 --> 00:03:37,379
パラメーターまで、あらゆるものが含まれます。ある意味では、それらは実際に

90
00:03:37,379 --> 00:03:39,739


91
00:03:39,739 --> 00:03:43,140
何が学習できるかを定義します そしてそれは、帰納的な解決策がなければトレーニングセットを

92
00:03:43,140 --> 00:03:44,819


93
00:03:44,819 --> 00:03:47,220
超えて一般化することは実際にはできないという点で一般化を定義します。

94
00:03:47,220 --> 00:03:48,420


95
00:03:48,420 --> 00:03:50,220
これについては、

96
00:03:50,220 --> 00:03:52,560
David Wolford によるこの論文でより詳しく説明されています。

97
00:03:52,560 --> 00:03:55,500
つまり、自然な

98
00:03:55,500 --> 00:03:58,620
帰納的バイアスとは、制約や制限から生じるバイアスのことです。

99
00:03:58,620 --> 00:04:00,000


100
00:04:00,000 --> 00:04:02,040
自然のシステムが直面しているのは、

101
00:04:02,040 --> 00:04:04,500


102
00:04:04,500 --> 00:04:06,900
現実世界で生きなければならないという性質上、たとえば

103
00:04:06,900 --> 00:04:08,459
脳にはその

104
00:04:08,459 --> 00:04:10,439
構造の性質上、効率に関する制約や物理的な制約がたくさんあり、

105
00:04:10,439 --> 00:04:13,140
この

106
00:04:13,140 --> 00:04:14,580
ロジックに従うと、これらの制約は

107
00:04:14,580 --> 00:04:16,620
私たちの一般化において実際に何らかの役割を果たしているのです。

108
00:04:16,620 --> 00:04:19,798
現在、現代の人工知能を超えている能力については、

109
00:04:19,798 --> 00:04:21,478


110
00:04:21,478 --> 00:04:24,720
次に説明しますので、この講演では、私の研究で研究してきた

111
00:04:24,720 --> 00:04:27,540
2 つのタイプの構造、

112
00:04:27,540 --> 00:04:29,880


113
00:04:29,880 --> 00:04:31,699
地形組織と

114
00:04:31,699 --> 00:04:34,320
時空間ダイナミクスに特に焦点を当てます。

115
00:04:34,320 --> 00:04:36,120
私の研究に入る前に、 前に話していた構造の一般化を達成するために

116
00:04:36,120 --> 00:04:38,759
自然構造が役立つと私が考える理由について短い例を示します。

117
00:04:38,759 --> 00:04:41,400


118
00:04:41,400 --> 00:04:42,780


119
00:04:42,780 --> 00:04:44,280


120
00:04:44,280 --> 00:04:47,479
最初の例は、1980 年代の

121
00:04:47,479 --> 00:04:49,199
福島のネオコグニティブ フロント

122
00:04:49,199 --> 00:04:51,479
アーキテクチャから来ています。

123
00:04:51,479 --> 00:04:53,520
これは実際に

124
00:04:53,520 --> 00:04:55,440
直接対処するために構築されました。

125
00:04:55,440 --> 00:04:57,300
これらの小さな変化や変形に対するロバスト性の問題である

126
00:04:57,300 --> 00:04:59,759
ため、彼は書類の中で、

127
00:04:59,759 --> 00:05:02,040


128
00:05:02,040 --> 00:05:04,380


129
00:05:04,380 --> 00:05:06,780


130
00:05:06,780 --> 00:05:08,699
これらの歪みに対するロバスト性を達成するための階層とプールの生徒とイタチの測定からのインスピレーションについて書いています。したがって、

131
00:05:08,699 --> 00:05:11,160
図を見ると、彼は U sub S1 U と書いています。

132
00:05:11,160 --> 00:05:14,100
sub C1 とこれらは単純な

133
00:05:14,100 --> 00:05:16,919
セルと複雑なセルを表すため、これは

134
00:05:16,919 --> 00:05:18,840
当時かなり過激なアプローチでしたが、

135
00:05:18,840 --> 00:05:20,699


136
00:05:20,699 --> 00:05:22,199
これらの初期の人工ニューラル ネットワークを悩ませていた堅牢性と変化を改善するのに非常に役立ち、

137
00:05:22,199 --> 00:05:24,419
時間が経つにつれて、

138
00:05:24,419 --> 00:05:25,860
これらのアイデアは単純化され、

139
00:05:25,860 --> 00:05:28,919
抽象化されました。 明らかに

140
00:05:28,919 --> 00:05:30,539
畳み込みニューラル ネットワークを生み出したので、

141
00:05:30,539 --> 00:05:32,759
最終的にディープ ラーニング革命の成功を導いたことが今日わかっています。

142
00:05:32,759 --> 00:05:35,580
これは実際、

143
00:05:35,580 --> 00:05:36,960


144
00:05:36,960 --> 00:05:39,479
構造の一般化を達成した自然な帰納的バイアスの一例です。

145
00:05:39,479 --> 00:05:42,120
したがって、私たちの研究にとって、

146
00:05:42,120 --> 00:05:43,919


147
00:05:43,919 --> 00:05:45,900
これらがどのように機能するのかを理解することは非常に興味深いことです。 モデルは非常にうまく機能します。

148
00:05:45,900 --> 00:05:47,280


149
00:05:47,280 --> 00:05:49,320
ええと、この原理が

150
00:05:49,320 --> 00:05:51,360


151
00:05:51,360 --> 00:05:53,940
より抽象的な変換

152
00:05:53,940 --> 00:05:57,000
と対称性をカバー

153
00:05:57,000 --> 00:06:00,360
するために潜在的に一般化できるかどうかを確認してください。畳み込みがこの

154
00:06:00,360 --> 00:06:02,060
構造の一般化を

155
00:06:02,060 --> 00:06:04,440
直感的に達成する理由は、

156
00:06:04,440 --> 00:06:06,720
同じフィルターを適用するか、または

157
00:06:06,720 --> 00:06:08,699
特徴抽出器で適用することによってこれが行われることがわかります。 さまざまな空間的

158
00:06:08,699 --> 00:06:10,680
位置があるため、ここでは単一の

159
00:06:10,680 --> 00:06:12,660
畳み込みフィルターが

160
00:06:12,660 --> 00:06:14,820
画像のすべての位置に適用されていることがわかります。これは、

161
00:06:14,820 --> 00:06:16,259
入力が画像の

162
00:06:16,259 --> 00:06:18,000
中央であろう

163
00:06:18,000 --> 00:06:20,400
と右側であろうと、どこに入力されても、

164
00:06:20,400 --> 00:06:22,080
まったく同じ特徴が得られることを意味します。  1 つの例外を除いて、

165
00:06:22,080 --> 00:06:23,580
それらは等価に

166
00:06:23,580 --> 00:06:24,660
シフトされる

167
00:06:24,660 --> 00:06:26,819
ため、数学的にはこのタイプの写像は準

168
00:06:26,819 --> 00:06:29,160
同型写像と呼ばれ、入力空間と出力空間の代数構造が保存されます。

169
00:06:29,160 --> 00:06:30,840


170
00:06:30,840 --> 00:06:33,180
この場合、

171
00:06:33,180 --> 00:06:35,580
それは変換に関するものであり、

172
00:06:35,580 --> 00:06:37,440
単純な単純なレベルでは次のようなものになります。

173
00:06:37,440 --> 00:06:38,880


174
00:06:38,880 --> 00:06:40,259
この話の残りの部分で覚えておくことが重要なのは、

175
00:06:40,259 --> 00:06:42,300


176
00:06:42,300 --> 00:06:45,000


177
00:06:45,000 --> 00:06:46,740


178
00:06:46,740 --> 00:06:49,560
変換交換図でこのコミュニティ交換があることが確認できれば、特徴抽出器の

179
00:06:49,560 --> 00:06:51,720
準同型性を検証できるということです。したがって、

180
00:06:51,720 --> 00:06:53,759
特徴抽出器が f は

181
00:06:53,759 --> 00:06:55,080
変換

182
00:06:55,080 --> 00:06:57,000
演算子 t と互換性があり、

183
00:06:57,000 --> 00:06:58,919
基本的に私たちが望んでいるのは、

184
00:06:58,919 --> 00:07:00,600
最初に

185
00:07:00,600 --> 00:07:02,639
特徴を抽出してから

186
00:07:02,639 --> 00:07:04,740
変換を実行するか、

187
00:07:04,740 --> 00:07:06,240
変換を実行してから

188
00:07:06,240 --> 00:07:08,639
特徴を抽出するかの違いがないことです。そのため、

189
00:07:08,639 --> 00:07:10,199
現時点での課題は、実際には

190
00:07:10,199 --> 00:07:11,880
分からないことです。 現実世界

191
00:07:11,880 --> 00:07:13,620


192
00:07:13,620 --> 00:07:15,240
で見られるより複雑な変換に関して準同型性を構築する方法

193
00:07:15,240 --> 00:07:18,060
たとえば、私たちの脳は

194
00:07:18,060 --> 00:07:20,099
照明や季節の変化を自然に処理することができます

195
00:07:20,099 --> 00:07:22,259


196
00:07:22,259 --> 00:07:24,599
ええと、ここでは人の

197
00:07:24,599 --> 00:07:26,160
顔の照明や季節の変化がわかります

198
00:07:26,160 --> 00:07:27,840
同じ顔や同じ

199
00:07:27,840 --> 00:07:29,880
道路でも、これらの変換を考慮したモデルを構築する方法がわからない

200
00:07:29,880 --> 00:07:31,319


201
00:07:31,319 --> 00:07:33,180
ため、より抽象的な例として、これらの変換を堅牢かつ予測可能な方法で処理するシステムを構築することが困難になります。

202
00:07:33,180 --> 00:07:35,520


203
00:07:35,520 --> 00:07:37,620


204
00:07:37,620 --> 00:07:40,259


205
00:07:40,259 --> 00:07:41,759
これは、対称性を処理しないモデルの潜在的な悪影響を意味します。変換では、

206
00:07:41,759 --> 00:07:43,620


207
00:07:43,620 --> 00:07:45,440


208
00:07:45,440 --> 00:07:48,060
現代のテキストから画像への生成プログラムを考慮している

209
00:07:48,060 --> 00:07:50,520
ため、この例では、

210
00:07:50,520 --> 00:07:53,940
Dolly に月面のテディベアの画像を生成するように依頼しました。

211
00:07:53,940 --> 00:07:55,620
これは信じられない

212
00:07:55,620 --> 00:07:57,180
ほどうまく機能します。 おそらく

213
00:07:57,180 --> 00:08:00,960
私ができるよりも優れていると思いますが、テクスチャファーは信じられ

214
00:08:00,960 --> 00:08:03,360
ないほど詳細です。しかし、私があなたに、

215
00:08:03,360 --> 00:08:05,340


216
00:08:05,340 --> 00:08:08,039


217
00:08:08,039 --> 00:08:10,560
赤い立方体の上に青い立方体を描くなど、概念的に単純であると思われる何かをするように依頼すると、これは失敗し、

218
00:08:10,560 --> 00:08:13,380
私にとってこれは直感的ではないように思えます。

219
00:08:13,380 --> 00:08:15,300
2 番目のタスクは

220
00:08:15,300 --> 00:08:18,180
かなり簡単に見えますが、私が

221
00:08:18,180 --> 00:08:19,860
主張しているのは、これが

222
00:08:19,860 --> 00:08:21,599
驚くべき理由は、恩赦の

223
00:08:21,599 --> 00:08:23,580
翻訳例が

224
00:08:23,580 --> 00:08:25,560
驚くべき理由とまったく同じであるということです。ここでこの対称性の

225
00:08:25,560 --> 00:08:28,020
変換が起こっています。つまり、

226
00:08:28,020 --> 00:08:29,400


227
00:08:29,400 --> 00:08:31,740
テディベアのこれらの複雑なオブジェクト間の変換です。 そして月や

228
00:08:31,740 --> 00:08:34,320
立方体のこれらの単純なオブジェクトは、

229
00:08:34,320 --> 00:08:36,360
ネットワークが

230
00:08:36,360 --> 00:08:38,820
処理し尊重できると直感的に期待しますが、実際には

231
00:08:38,820 --> 00:08:40,919
そうではないことがわかります。これは、

232
00:08:40,919 --> 00:08:43,380
福島の研究が、これらの

233
00:08:43,380 --> 00:08:46,200
自然な階層構造と

234
00:08:46,200 --> 00:08:47,700
私たちの視覚システムのプールがどのように行われているかを示したのとまったく同じです。

235
00:08:47,700 --> 00:08:49,680


236
00:08:49,680 --> 00:08:52,380
小さな変換を一般化するのに効果的です。私は、

237
00:08:52,380 --> 00:08:54,060


238
00:08:54,060 --> 00:08:55,740
これらの抽象的な一般化問題を解決するには、潜在的により高いレベルの構造が必要である可能性があると主張します。そのため、

239
00:08:55,740 --> 00:08:58,200


240
00:08:58,200 --> 00:09:01,380
私が

241
00:09:01,380 --> 00:09:04,380
研究しており、私が尋ねている問題は、

242
00:09:04,380 --> 00:09:06,060
この構造が何であり、どのように

243
00:09:06,060 --> 00:09:08,640
実装するかということです。 これは、

244
00:09:08,640 --> 00:09:10,080
実際に

245
00:09:10,080 --> 00:09:14,120
計算を実行するために使用できる人工ニューラル ネットワーク アーキテクチャです。そのため、

246
00:09:14,880 --> 00:09:17,700
答えを始めるために、地形的組織化

247
00:09:17,700 --> 00:09:19,680
に関する最初の作業に取り掛かります。

248
00:09:19,680 --> 00:09:22,380


249
00:09:22,380 --> 00:09:25,260
そうすることで、地形的組織化は、一

250
00:09:25,260 --> 00:09:27,060
次

251
00:09:27,060 --> 00:09:29,760
視覚野のサファイア レベルの領域から脳全体にわたって広く観察されます。 そして、

252
00:09:29,760 --> 00:09:31,500
非常に大まかに言うと、

253
00:09:31,500 --> 00:09:33,540


254
00:09:33,540 --> 00:09:35,760
互いに近いニューロンが同様のものに反応する傾向があるという特性です。

255
00:09:35,760 --> 00:09:38,220
たとえば、左側には、指向性線に対する反応

256
00:09:38,220 --> 00:09:39,720


257
00:09:39,720 --> 00:09:42,959
として、一次デジタル皮質の各ニューロンの色分けされた優先順位が示されています。

258
00:09:42,959 --> 00:09:45,360


259
00:09:45,360 --> 00:09:46,740
このスムーズに変化する一連の

260
00:09:46,740 --> 00:09:48,779
選択性がわかります 別のタイプの

261
00:09:48,779 --> 00:09:50,580
組織は網膜トピック組織として知られており、

262
00:09:50,580 --> 00:09:52,560


263
00:09:52,560 --> 00:09:54,600
視覚野の近くのニューロンが近くの受容野に反応する傾向があります

264
00:09:54,600 --> 00:09:56,399


265
00:09:56,399 --> 00:09:58,560
が、この組織は

266
00:09:58,560 --> 00:10:01,080
これらの低レベルの機能に限定されず、より複雑な機能を拡張します

267
00:10:01,080 --> 00:10:02,519


268
00:10:02,519 --> 00:10:05,459
顔、物体、場所に存在する特徴などの特徴であり

269
00:10:05,459 --> 00:10:07,920
、これは

270
00:10:07,920 --> 00:10:10,080


271
00:10:10,080 --> 00:10:12,779
紡錘状顔面領域 FFA や

272
00:10:12,779 --> 00:10:15,420
海馬傍顔面領域 PPA などの、いわゆる機能的に特定された脳の領域に関連するため、

273
00:10:15,420 --> 00:10:19,200
この研究でも主なアイデアは

274
00:10:19,200 --> 00:10:21,300
おそらくこれであるということです。

275
00:10:21,300 --> 00:10:23,580


276
00:10:23,580 --> 00:10:25,080
畳み込み

277
00:10:25,080 --> 00:10:27,980
演算と Fukushima のアーキテクチャに密接に関係しているある意味でのトポグラフィック構成です。

278
00:10:27,980 --> 00:10:30,660


279
00:10:30,660 --> 00:10:33,420
この利点をより抽象的な変換に一般化できるかもしれません。

280
00:10:33,420 --> 00:10:34,920
言い換えれば、

281
00:10:34,920 --> 00:10:36,839
私たちにはできない、より複雑な準同型射影を構築する方法を学ぶことができるかもしれません。

282
00:10:36,839 --> 00:10:38,940
今は分析を行っていますので、

283
00:10:38,940 --> 00:10:40,740


284
00:10:40,740 --> 00:10:42,480
私たちが

285
00:10:42,480 --> 00:10:44,760
この考えに完全に狂っているわけではないことを示すためです、えー、

286
00:10:44,760 --> 00:10:46,320


287
00:10:46,320 --> 00:10:49,880


288
00:10:49,880 --> 00:10:54,060
90 年代初頭と 2000 年代にコナン・ギャラクシー・バーデンのような人々によるこの領域に関する先行研究があり、

289
00:10:54,060 --> 00:10:55,800
彼らは地形学的組織がどのようになり得るかを研究しました これは

290
00:10:55,800 --> 00:10:57,720


291
00:10:57,720 --> 00:11:01,320
主に線形モデルでの分散学習に役立つため、

292
00:11:01,320 --> 00:11:03,060


293
00:11:03,060 --> 00:11:04,680


294
00:11:04,680 --> 00:11:07,140


295
00:11:07,140 --> 00:11:08,880
この分野に参入したときの私たちの疑問は、これらのアプローチから活用でき、

296
00:11:08,880 --> 00:11:10,800
最新のディープ ニューラル

297
00:11:10,800 --> 00:11:12,959
ネットワーク アーキテクチャに統合できる最もスケーラブルな抽象メカニズムは何かということであり、最終的には

298
00:11:12,959 --> 00:11:15,000
生成モデリング

299
00:11:15,000 --> 00:11:16,260
アプローチは、

300
00:11:16,260 --> 00:11:17,519
このコミュニティの人々にとって興味深いかもしれないと思います。

301
00:11:17,519 --> 00:11:18,779


302
00:11:18,779 --> 00:11:21,779
ええと、これにより、

303
00:11:21,779 --> 00:11:23,579
地形的事前分布を課すことによって地形的特徴空間を学習できるという

304
00:11:23,579 --> 00:11:26,040
基本的な考え方で、それを地形的独立成分分析とより密接に関連付けることができます。

305
00:11:26,040 --> 00:11:28,320


306
00:11:28,320 --> 00:11:30,660


307
00:11:30,660 --> 00:11:32,940
私たちの潜在

308
00:11:32,940 --> 00:11:34,440
変数

309
00:11:34,440 --> 00:11:37,079
ですので簡単な背景を説明しますと、

310
00:11:37,079 --> 00:11:39,120
ほとんどの人はすでにこれに精通していると思います

311
00:11:39,120 --> 00:11:40,440


312
00:11:40,440 --> 00:11:42,540
が、一般的な仮定のようなものは、

313
00:11:42,540 --> 00:11:44,339
脳は生成モデルであり、

314
00:11:44,339 --> 00:11:45,720
このアイデアはある意味で

315
00:11:45,720 --> 00:11:48,060
19世紀のヘルムホルトに起因する可能性があります

316
00:11:48,060 --> 00:11:50,459
そこで彼は、私たちが

317
00:11:50,459 --> 00:11:52,140
見ているものは計算上の問題の解決策であり、

318
00:11:52,140 --> 00:11:54,420
私たちの脳は目の中の

319
00:11:54,420 --> 00:11:56,519
光子の吸収から最も可能性の高い原因を計算する

320
00:11:56,519 --> 00:11:59,519
ので、これは一例であると述べました。

321
00:11:59,519 --> 00:12:01,920
この画像を見せれば、あなたはすぐに

322
00:12:01,920 --> 00:12:03,720
それが曲率のある球体であると認識すると思いますが、

323
00:12:03,720 --> 00:12:05,760
それは

324
00:12:05,760 --> 00:12:07,620
同様に、歪んだ視点を持つディスクである可能性もあります。

325
00:12:07,620 --> 00:12:09,600
これが私たちが目の

326
00:12:09,600 --> 00:12:12,660
錯覚やイメージを得る方法です。

327
00:12:12,660 --> 00:12:14,880
このように、脳はその構造

328
00:12:14,880 --> 00:12:17,100
からここに立方体があると推測します

329
00:12:17,100 --> 00:12:18,720
が、実際には単なる平らな紙です。

330
00:12:18,720 --> 00:12:19,860


331
00:12:19,860 --> 00:12:22,740
したがって、この生成

332
00:12:22,740 --> 00:12:24,480
モデルの側面は、プログラム内の

333
00:12:24,480 --> 00:12:26,160
逆グラフィックス プログラムのようなものであると考えることができます。

334
00:12:26,160 --> 00:12:28,140


335
00:12:28,140 --> 00:12:30,660
球の抽象的なプロパティは位置、

336
00:12:30,660 --> 00:12:32,820
サイズ、照明がわかっており、これらは

337
00:12:32,820 --> 00:12:34,560
球を投影して

338
00:12:34,560 --> 00:12:37,440
レンダリングされる 2D 画像を作成するために使用されます。 つまり、実質的に

339
00:12:37,440 --> 00:12:40,019
フンボルトや他の人たちが言っていることは、

340
00:12:40,019 --> 00:12:41,940
脳は生成モデルとして

341
00:12:41,940 --> 00:12:43,680
実際にこの

342
00:12:43,680 --> 00:12:45,959
生成プロセスを逆転させ、推論を行って

343
00:12:45,959 --> 00:12:48,300
、感覚の根本的な原因を推論しようとしているということなので、

344
00:12:48,300 --> 00:12:49,680


345
00:12:49,680 --> 00:12:51,600
私がこの点について少し詳しく説明しているのは、次のような理由

346
00:12:51,600 --> 00:12:53,100
があるからです。

347
00:12:53,100 --> 00:12:55,440
今日は生成モデルについてたくさん話しました

348
00:12:55,440 --> 00:12:57,180
えーと、私は必ずしも

349
00:12:57,180 --> 00:12:59,639
画像やきれいな写真の生成についてだけ話しているわけではありません えー、

350
00:12:59,639 --> 00:13:01,260


351
00:13:01,260 --> 00:13:03,120
本当に言いたいのは

352
00:13:03,120 --> 00:13:07,920
教師なし学習のフレームワークです

353
00:13:07,920 --> 00:13:10,019
それで、もう少し

354
00:13:10,019 --> 00:13:11,700
詳しく説明します とはどういう意味ですか

355
00:13:11,700 --> 00:13:14,279
地形的な事前生成モデルは

356
00:13:14,279 --> 00:13:16,019
通常、

357
00:13:16,019 --> 00:13:18,720
観測値 X と

358
00:13:18,720 --> 00:13:21,720
Z と呼ぶ潜在変数にわたる結合分布として記述されます。

359
00:13:21,720 --> 00:13:23,940
これは通常因数分解されるか、

360
00:13:23,940 --> 00:13:25,620
これが行われる 1 つの方法は

361
00:13:25,620 --> 00:13:28,440
Z の事前 P に関して因数分解され、これは真です

362
00:13:28,440 --> 00:13:30,420
生成モデル 条件付き生成

363
00:13:30,420 --> 00:13:33,420
モデル P of x 与えられた Z、したがって、これについて考えることができる 1 つの方法は、

364
00:13:33,420 --> 00:13:34,980


365
00:13:34,980 --> 00:13:37,019


366
00:13:37,019 --> 00:13:38,880


367
00:13:38,880 --> 00:13:41,279
生成モデルを反転するときに生成される各タイプのコードに対する相対的なペナルティをエンコードする事前分布が見られるということです。

368
00:13:41,279 --> 00:13:42,899
これは、「計算」と呼ばれます。

369
00:13:42,899 --> 00:13:45,920
X が与えられた Z の事後 P である

370
00:13:45,920 --> 00:13:49,139
ため、地形学的潜在

371
00:13:49,139 --> 00:13:50,639
空間を開発するには、グループ スパース性ペナルティのようなものと同等であることが示された、または

372
00:13:50,639 --> 00:13:53,279


373
00:13:53,279 --> 00:13:55,620
この地形学的 ICA 作業で示された、ある種の地形的事前分布を導入したいと考えています。

374
00:13:55,620 --> 00:13:57,720


375
00:13:57,720 --> 00:13:59,700


376
00:13:59,700 --> 00:14:01,500


377
00:14:01,500 --> 00:14:03,180
独立学習分析からのスパース ペナルティ

378
00:14:03,180 --> 00:14:04,560


379
00:14:04,560 --> 00:14:06,540
アクティベーションをスパースにしたい場合は、

380
00:14:06,540 --> 00:14:09,420
その多くがゼロであることを意味します。つまり、

381
00:14:09,420 --> 00:14:10,680
このようになります。

382
00:14:10,680 --> 00:14:12,300
アクティブな青い四角形が多数ありますが、

383
00:14:12,300 --> 00:14:14,459
そのほとんどはアクティブではありませんが、特に

384
00:14:14,459 --> 00:14:16,740
グループの場合はアクティブではありません。 バーシティペナルティでは、

385
00:14:16,740 --> 00:14:18,839
これらの事前分布

386
00:14:18,839 --> 00:14:21,600
に、これらの分散された疎なアクティベーションには

387
00:14:21,600 --> 00:14:24,720
低い確率を割り当て、これらのグループ化された

388
00:14:24,720 --> 00:14:26,940
密にパックされた表現には高い確率を割り当てるようにしたいと考えています。

389
00:14:26,940 --> 00:14:28,860
これは、物事が

390
00:14:28,860 --> 00:14:30,720
分散している場合にはより高いペナルティを、

391
00:14:30,720 --> 00:14:33,540
物事が互いに接近している場合にはより低いペナルティのように考えることもできます。

392
00:14:33,540 --> 00:14:36,380
これは次の

393
00:14:36,380 --> 00:14:39,060
ように抽象的に書くことができますが、ここでの

394
00:14:39,060 --> 00:14:41,160
これらのニューロンのそれぞれが

395
00:14:41,160 --> 00:14:42,779


396
00:14:42,779 --> 00:14:44,220
モデル内の一種のニューロンを表しており、

397
00:14:44,220 --> 00:14:46,560
この 2D グリッド内で組織されているという理論を立てたいと考えています。つまり、

398
00:14:46,560 --> 00:14:48,120
グループ化について話しているときは、実際には次のことを意味します。

399
00:14:48,120 --> 00:14:50,820
その 2D トポロジーでグループ化する

400
00:14:50,820 --> 00:14:53,100
ため、非常に興味深く

401
00:14:53,100 --> 00:14:55,560
、ある意味重要なことの 1 つは、これらの

402
00:14:55,560 --> 00:14:57,779
事前分布は地形的な

403
00:14:57,779 --> 00:15:00,540
組織化を提供するだけでなく、エロシ・マルチェリやブルーノのような

404
00:15:00,540 --> 00:15:02,459
人々によって注目されたり、エロシ・マルチェリやブルーノのような人々によって研究されたりしていること

405
00:15:02,459 --> 00:15:05,760
も

406
00:15:05,760 --> 00:15:07,740
実際に適合するということです。 自然

407
00:15:07,740 --> 00:15:08,959
データ、具体

408
00:15:08,959 --> 00:15:11,180
的には自然画像の統計では、

409
00:15:11,180 --> 00:15:14,100
このタイプの事前分布を使用すると、実際にはよりまばらな一連の活性化が得られることが示されています。これは、

410
00:15:14,100 --> 00:15:16,139


411
00:15:16,139 --> 00:15:18,839
事前分布が

412
00:15:18,839 --> 00:15:20,459
真の生成プロセスにもう少し

413
00:15:20,459 --> 00:15:22,620
よく適合することを意味しており、私たちが知っているように、脳は

414
00:15:22,620 --> 00:15:24,779
高度なスパース性であり、これは

415
00:15:24,779 --> 00:15:26,339


416
00:15:26,339 --> 00:15:28,620
効率に非常に関連していると考えられています。そのため、

417
00:15:28,620 --> 00:15:30,839


418
00:15:30,839 --> 00:15:32,760


419
00:15:32,760 --> 00:15:35,160
階層生成モデルを使用する前に、このタイプのグループ スパースを実装するための詳細をもう少し詳しく説明します。

420
00:15:35,160 --> 00:15:37,320
これは

421
00:15:37,320 --> 00:15:39,060
基本的に、一部の

422
00:15:39,060 --> 00:15:41,339
地形学的な ICA の作業です。

423
00:15:41,339 --> 00:15:43,320
アイデアは、複数の下位レベルの変数 T の分散を同時に制御するより高い

424
00:15:43,320 --> 00:15:45,000
レベルの潜在変数 U があり、

425
00:15:45,000 --> 00:15:47,820


426
00:15:47,820 --> 00:15:50,279


427
00:15:50,279 --> 00:15:52,440
これがグループの疎性を取得する方法です。

428
00:15:52,440 --> 00:15:55,440
その後、地形学的な組織化を得るために、

429
00:15:55,440 --> 00:15:56,760
これらの潜在変数を複数使用することができます。

430
00:15:56,760 --> 00:15:59,339


431
00:15:59,339 --> 00:16:02,519
彼らの影響力の領域と重なっているので、彼らの

432
00:16:02,519 --> 00:16:04,260
近所を私たちは彼らと呼ぶことができます。

433
00:16:04,260 --> 00:16:05,699
これにより、あなたが求めている行為にこの滑らかな

434
00:16:05,699 --> 00:16:07,440
相関構造が得られます。

435
00:16:07,440 --> 00:16:09,899
これを直感的に理解してください、

436
00:16:09,899 --> 00:16:12,060


437
00:16:12,060 --> 00:16:14,279
ここの一番下にあるこの変数Tが取得されていないことがわかります

438
00:16:14,279 --> 00:16:17,160
この U からの入力は上部にありますが、

439
00:16:17,160 --> 00:16:19,139
中央のこの T と u

440
00:16:19,139 --> 00:16:21,300
変数を共有しているため、バリアントを共有しているようです。

441
00:16:21,300 --> 00:16:22,980
一部のコンポーネントを

442
00:16:22,980 --> 00:16:24,959
隣接するコンポーネントと共有していますが、すべてのコンポーネントを共有しているわけではありません。

443
00:16:24,959 --> 00:16:26,579
これは実際には、このローカル

444
00:16:26,579 --> 00:16:28,079
接続によるものです。 これらのより高いレベルの

445
00:16:28,079 --> 00:16:30,779
変数を使用するので、

446
00:16:30,779 --> 00:16:33,180


447
00:16:33,180 --> 00:16:34,980
生成モデルの使用方法を簡単にするために、

448
00:16:34,980 --> 00:16:37,440
単一の U 変数に戻りましょう。長年にわたって困難にしてきた

449
00:16:37,440 --> 00:16:38,940
このタイプのアーキテクチャにおける課題は、

450
00:16:38,940 --> 00:16:42,360


451
00:16:42,360 --> 00:16:44,579
近似事後関数をどのように推論するかです。

452
00:16:44,579 --> 00:16:47,579


453
00:16:47,579 --> 00:16:50,100
この階層アーキテクチャのこれらの中間変数に関して、これは

454
00:16:50,100 --> 00:16:52,560
非常に単純ではないため、以前の

455
00:16:52,560 --> 00:16:54,420
研究では

456
00:16:54,420 --> 00:16:56,699
線形モデル用に開発されたヒューリスティックを使用していましたが、私たちの研究では、

457
00:16:56,699 --> 00:16:58,680
これが実際には現代のニューラル ネットワーク アーキテクチャに拡張されていないことがわかりました。

458
00:16:58,680 --> 00:17:01,199


459
00:17:01,199 --> 00:17:02,880


460
00:17:02,880 --> 00:17:04,760
因数分解は

461
00:17:04,760 --> 00:17:07,640
この分布の特定の再パラメータ化であり

462
00:17:07,640 --> 00:17:10,380
、このパラメータ化は

463
00:17:10,380 --> 00:17:12,419


464
00:17:12,419 --> 00:17:14,579
ガウススケール混合として知られる優先順位を定義することによって具体的に達成されます。これは、

465
00:17:14,579 --> 00:17:16,319


466
00:17:16,319 --> 00:17:19,140
U が与えられた T の条件付き分布が

467
00:17:19,140 --> 00:17:21,179
実際には

468
00:17:21,179 --> 00:17:24,299
分散がこの変数によって定義される正規分布であることを意味します。

469
00:17:24,299 --> 00:17:27,720
U および U の特定の選択では、この

470
00:17:27,720 --> 00:17:29,340
分布は確かに疎であり、

471
00:17:29,340 --> 00:17:31,980


472
00:17:31,980 --> 00:17:33,780
ラプロシアン分布やスート分布、T 分布などの分布の範囲を包含します。

473
00:17:33,780 --> 00:17:36,299


474
00:17:36,299 --> 00:17:38,940
ガウス スケール混合を定義する 1 つの方法は、

475
00:17:38,940 --> 00:17:40,919


476
00:17:40,919 --> 00:17:42,900
独立したガウス確率

477
00:17:42,900 --> 00:17:45,720
変数に関して特定の再フレーム再パラメータ化を発行します。 特に Z と U については、

478
00:17:45,720 --> 00:17:48,840


479
00:17:48,840 --> 00:17:50,760
もともとかなり複雑だったこの T 変数が、実際には

480
00:17:50,760 --> 00:17:52,799
ガウス

481
00:17:52,799 --> 00:17:54,840
確率変数の束にすぎないことがわかります。これらの変数は、

482
00:17:54,840 --> 00:17:57,660


483
00:17:57,660 --> 00:18:00,120
生成モデルにおいて、特に

484
00:18:00,120 --> 00:18:02,039
私たちのものをより効率的に扱う方法を知っています。 これから行うことは、

485
00:18:02,039 --> 00:18:04,020
実際に

486
00:18:04,020 --> 00:18:06,720
U と Z の近似事後分布を個別に取得し、地形変数 T を計算するため

487
00:18:06,720 --> 00:18:08,640
にそれらの決定論的な組み合わせを実行できるようにするためです。

488
00:18:08,640 --> 00:18:10,020


489
00:18:10,020 --> 00:18:13,140
これは、

490
00:18:13,140 --> 00:18:15,500
あまり詳細に説明することなく、

491
00:18:15,500 --> 00:18:17,700
これを行うのがはるかに簡単です。 私たちは、

492
00:18:17,700 --> 00:18:18,600
変分

493
00:18:18,600 --> 00:18:20,640
オートエンコーダとして知られるものを使用することにしました。これは、変分推論の技術を活用して

494
00:18:20,640 --> 00:18:23,220


495
00:18:23,220 --> 00:18:24,780
尤度の下限を導き出し、

496
00:18:24,780 --> 00:18:26,940
これらの近似事後値を

497
00:18:26,940 --> 00:18:29,400
強力な非線形ディープ

498
00:18:29,400 --> 00:18:30,960
ニューラル ネットワークでパラメータ化し、勾配降下法で最適化できるようにします。

499
00:18:30,960 --> 00:18:33,240


500
00:18:33,240 --> 00:18:34,440
アクティブな推論コミュニティにはよく知られています

501
00:18:34,440 --> 00:18:36,900
が、実際に私たちが行ったことは、

502
00:18:36,900 --> 00:18:38,760


503
00:18:38,760 --> 00:18:41,640
典型的なベースとしてデコーダに 1 つのエンコーダを持たせる代わりに、

504
00:18:41,640 --> 00:18:43,799
2 つのエンコーダを用意し、1 つはユーザー用、もう 1 つは Z 用に

505
00:18:43,799 --> 00:18:46,200
それぞれ用意し、これらを

506
00:18:46,200 --> 00:18:48,419
この決定論的な方法で組み合わせます。

507
00:18:48,419 --> 00:18:51,660


508
00:18:51,660 --> 00:18:53,100
これが実際にガウス分布から

509
00:18:53,100 --> 00:18:54,900
スチューデントの T 分布を構築したものであることがわかったら、地形 T 変数を構築します。

510
00:18:54,900 --> 00:18:57,620


511
00:18:57,620 --> 00:19:00,480
これをプラグインします。

512
00:19:00,480 --> 00:19:03,600
デコードする前にこれを行い、データ全体の尤度を最大化します。

513
00:19:03,600 --> 00:19:05,820


514
00:19:05,820 --> 00:19:07,260
これがエルボです。

515
00:19:07,260 --> 00:19:09,360


516
00:19:09,360 --> 00:19:12,600
データの尤度に関する証拠の下限は豊富で、実際にはアクティブな入口コミュニティで使用される変分自由エネルギーに非常に似ています。そのため、

517
00:19:12,600 --> 00:19:15,720


518
00:19:15,720 --> 00:19:18,299


519
00:19:18,299 --> 00:19:20,520
これらの詳細は邪魔にならないので、

520
00:19:20,520 --> 00:19:22,500
本当に興味深いのは、

521
00:19:22,500 --> 00:19:23,940
この生成モデルをトレーニングすると何が起こるかということです。

522
00:19:23,940 --> 00:19:26,580


523
00:19:26,580 --> 00:19:29,460
潜在空間には比較的単純なグループ スパーシティ ペナルティがあり、

524
00:19:29,460 --> 00:19:30,720


525
00:19:30,720 --> 00:19:32,700


526
00:19:32,700 --> 00:19:34,980
Future の構成に関して学習していることを確認したいと思います。まず、

527
00:19:34,980 --> 00:19:36,480
可能な限り単純なデータ セットから始めます。

528
00:19:36,480 --> 00:19:38,580
黒い背景に

529
00:19:38,580 --> 00:19:41,280
ランダムな XY 位置に白い四角があります。 そして、

530
00:19:41,280 --> 00:19:42,780
このグループ スパース性ペナルティを使用してオートエンコーダをトレーニングし、

531
00:19:42,780 --> 00:19:44,760


532
00:19:44,760 --> 00:19:47,640


533
00:19:47,640 --> 00:19:49,020
ここで青色でプロットしているデコーダの重みベクトルを

534
00:19:49,020 --> 00:19:52,520
この 2D グリッド上で再度整理すると、

535
00:19:52,520 --> 00:19:54,780
確かに空間に従って整理されることを学習していることがわかります。

536
00:19:54,780 --> 00:19:57,539
したがって、これは

537
00:19:57,539 --> 00:19:59,580
畳み込み受容野に似ていると見なすことができます。

538
00:19:59,580 --> 00:20:01,799
または、

539
00:20:01,799 --> 00:20:04,679
各ニューロンの受容野は実際には

540
00:20:04,679 --> 00:20:09,059
その位置での入力の種類によって与えられます。

541
00:20:09,059 --> 00:20:10,860
これは、グループのスパース性の観点から直感的に理にかなっています。これは、

542
00:20:10,860 --> 00:20:13,140


543
00:20:13,140 --> 00:20:15,480
任意の領域が次のように強調表示されるためです。

544
00:20:15,480 --> 00:20:17,700
黄色 ここでは、特定のグループのフィルターは、

545
00:20:17,700 --> 00:20:19,260


546
00:20:19,260 --> 00:20:20,880


547
00:20:20,880 --> 00:20:23,460
他のランダムな場所よりも重複する受容フィールドを持っているため、

548
00:20:23,460 --> 00:20:25,020


549
00:20:25,020 --> 00:20:27,840


550
00:20:27,840 --> 00:20:28,980


551
00:20:28,980 --> 00:20:32,059


552
00:20:32,059 --> 00:20:34,260
相関関係に従ってシミュレートされた垂直シートのようなもので、アクティビティをクラスター化することをモデルが学習していることがわかります。

553
00:20:34,260 --> 00:20:36,840
データセットなので、

554
00:20:36,840 --> 00:20:38,580
実際に重みを結び付けて

555
00:20:38,580 --> 00:20:40,860
手動で指定する畳み込みではなく、

556
00:20:40,860 --> 00:20:42,539
この重みをどこにでもコピーしたいと

557
00:20:42,539 --> 00:20:44,220
考えています。おそらくこれをおおよその待ち時間のようなものと考えることができ

558
00:20:44,220 --> 00:20:45,500


559
00:20:45,500 --> 00:20:48,120
、実際には相関関係からこれを学習しています

560
00:20:48,120 --> 00:20:49,620
データセット

561
00:20:49,620 --> 00:20:51,660
自体の構造と、これ

562
00:20:51,660 --> 00:20:54,120
についてもう少し生物学的なインスピレーションを与えます。

563
00:20:54,120 --> 00:20:56,280
網膜視症が

564
00:20:56,280 --> 00:20:58,020
脳に存在することはわかっています。これは

565
00:20:58,020 --> 00:21:02,460
視覚野における網膜症の一例であり、

566
00:21:02,460 --> 00:21:04,500
マカクに画像を見せればわかります。

567
00:21:04,500 --> 00:21:06,780
このような画像は、

568
00:21:06,780 --> 00:21:08,520


569
00:21:08,520 --> 00:21:11,700
実際に皮質の表面にあるトポロジーを保存する空間に投影されます。

570
00:21:11,700 --> 00:21:13,740


571
00:21:13,740 --> 00:21:16,080
つまり、トポグラフィーの

572
00:21:16,080 --> 00:21:18,419
組織化、さらにはトポグラフィーの組織化を学習することで、

573
00:21:18,419 --> 00:21:21,299


574
00:21:21,299 --> 00:21:26,160
データセットの入力相関が保存され、

575
00:21:26,160 --> 00:21:28,679
潜在的にこれは有益になる可能性があるということです。

576
00:21:28,679 --> 00:21:30,840
これらのアイデアをもう少し一般化して、

577
00:21:30,840 --> 00:21:32,340
最初に言ったように、

578
00:21:32,340 --> 00:21:34,679


579
00:21:34,679 --> 00:21:37,200


580
00:21:37,200 --> 00:21:39,320
畳み込み以上のもの、おそらくより複雑な

581
00:21:39,320 --> 00:21:43,679
等分散を学ぶことができればさらに良いでしょう。自然知能で明らかな 1 つのことは、どのように行うかということです。

582
00:21:43,679 --> 00:21:45,720


583
00:21:45,720 --> 00:21:48,299


584
00:21:48,299 --> 00:21:51,120
この IID フレームの世界には存在しません。変換

585
00:21:51,120 --> 00:21:53,520
の連続シーケンスがある世界に存在します。

586
00:21:53,520 --> 00:21:55,620
そのため、

587
00:21:55,620 --> 00:21:58,440
モデルをこの設定に拡張して、変換の観察を学習できるかもしれません。

588
00:21:58,440 --> 00:22:01,080
これは

589
00:22:01,080 --> 00:22:03,299
時間的コヒーレンスの考え方です。

590
00:22:03,299 --> 00:22:05,280
したがって、単純に実行するとどうなるでしょうか。

591
00:22:05,280 --> 00:22:08,280
以前のフレームワークを時間の経過とともに拡張しました。ディメンションが

592
00:22:08,280 --> 00:22:10,620
適切であるため、

593
00:22:10,620 --> 00:22:13,080
ニューロンを

594
00:22:13,080 --> 00:22:15,059


595
00:22:15,059 --> 00:22:17,400
皮質上の空間範囲の点でグループにまばらにしたいと単にグループ化するのではなく、実際に

596
00:22:17,400 --> 00:22:18,960
時間の経過とともにグループにまばらになるようにしたいのです。つまり、

597
00:22:18,960 --> 00:22:20,640
1 セットのニューロンがアクティブである場合を意味します。

598
00:22:20,640 --> 00:22:22,559
ここで、同じ

599
00:22:22,559 --> 00:22:24,360
ニューロンのセットが将来もアクティブになるようにしたいと考えています。

600
00:22:24,360 --> 00:22:25,440


601
00:22:25,440 --> 00:22:27,840


602
00:22:27,840 --> 00:22:30,600
これについて直感的に考えてみると、これは

603
00:22:30,600 --> 00:22:33,059
実際には不変性と等変性をより促進していることがわかります。

604
00:22:33,059 --> 00:22:35,039
これを理解する方法は、これを理解するための方法です。

605
00:22:35,039 --> 00:22:37,140
同じニューロンが

606
00:22:37,140 --> 00:22:39,179
常にアクティブであることを望んでいるが、入力

607
00:22:39,179 --> 00:22:41,280
変換は正しく変化しており、

608
00:22:41,280 --> 00:22:44,220
この小さなキツネの足は動いているので、

609
00:22:44,220 --> 00:22:45,960
同じニューロンが同じことを

610
00:22:45,960 --> 00:22:47,880
何度も何度もコーディングしていても足が

611
00:22:47,880 --> 00:22:49,320
動いている場合、それらのニューロンは

612
00:22:49,320 --> 00:22:51,360
学習することになる

613
00:22:51,360 --> 00:22:53,880
たとえば、この犬の脚の動きに対して不変であるということは、その

614
00:22:53,880 --> 00:22:57,539
代わりに、

615
00:22:57,539 --> 00:23:01,200
おっと、ここで間違った道を行ってしまったということで、その

616
00:23:01,200 --> 00:23:04,860
代わりに、ええと、私たちの洞察では、この

617
00:23:04,860 --> 00:23:06,659
グループは

618
00:23:06,659 --> 00:23:09,059
時間に関してシフトされ始めている可能性があり、これは次のことを

619
00:23:09,059 --> 00:23:10,980
意味します 連続してシフトされた

620
00:23:10,980 --> 00:23:13,080
活性化のセットが

621
00:23:13,080 --> 00:23:15,179
一緒に活性化するように促され、その後、

622
00:23:15,179 --> 00:23:16,440


623
00:23:16,440 --> 00:23:18,000
観察された変換に関して潜在空間が実際に構造化されるため、

624
00:23:18,000 --> 00:23:19,980


625
00:23:19,980 --> 00:23:21,480
すべてのタイム ステップで同じニューロンのセットがアクティブであるのではなく、

626
00:23:21,480 --> 00:23:23,340
実際には連続的に活性化されていることがわかります。

627
00:23:23,340 --> 00:23:24,900


628
00:23:24,900 --> 00:23:27,780
このようにまばらな方法でグループ化しているニューロンの並べ替えられたセットです。

629
00:23:27,780 --> 00:23:29,940
これにより、

630
00:23:29,940 --> 00:23:33,419
時間の経過とともに異なる観察をモデル化することができますが、

631
00:23:33,419 --> 00:23:34,860


632
00:23:34,860 --> 00:23:36,960
変換を学習し、共感の

633
00:23:36,960 --> 00:23:38,340
この相関構造を保存するという点では、それらは依然として接続されています。

634
00:23:38,340 --> 00:23:40,020


635
00:23:40,020 --> 00:23:41,940
これを

636
00:23:41,940 --> 00:23:44,400
トポグラフィック Bae アーキテクチャにまとめると、次

637
00:23:44,400 --> 00:23:46,020
のようなものが得られます。

638
00:23:46,020 --> 00:23:48,120
入力シーケンスがあることがわかります。z

639
00:23:48,120 --> 00:23:51,240
変数を再度エンコードし、次に

640
00:23:51,240 --> 00:23:53,520
分母の複数の U 変数をエンコードし

641
00:23:53,520 --> 00:23:55,740
、これらの U

642
00:23:55,740 --> 00:23:58,620
変数のそれぞれがシフトされます。 えー、これを

643
00:23:58,620 --> 00:24:00,480
達成するために前に示したようなものです。

644
00:24:00,480 --> 00:24:02,820
このシフト等分散構造を

645
00:24:02,820 --> 00:24:04,740


646
00:24:04,740 --> 00:24:07,080
この学生 T の

647
00:24:07,080 --> 00:24:09,240
積分布に組み合わせると、単一の潜在変数が得られます。

648
00:24:09,240 --> 00:24:10,740
これが地形

649
00:24:10,740 --> 00:24:13,860
変数 T になります。 この

650
00:24:13,860 --> 00:24:16,140
既知の構造が潜在空間にあります。これを

651
00:24:16,140 --> 00:24:17,460
構造化された世界モデルのように考えることができます。

652
00:24:17,460 --> 00:24:19,919


653
00:24:19,919 --> 00:24:21,659
この場合、この潜在空間を変換する方法はわかっています。この場合、

654
00:24:21,659 --> 00:24:23,580
これらの円の周りでこれらの活性化を並べ替えることで、

655
00:24:23,580 --> 00:24:25,860
循環的な役割のように

656
00:24:25,860 --> 00:24:28,380
循環シフトを行うことができます。 これは

657
00:24:28,380 --> 00:24:30,120
学習した入力変換に対応することになり、「

658
00:24:30,120 --> 00:24:32,640


659
00:24:32,640 --> 00:24:34,620
わかりました」と言うことで、この入力変換を続行すると、回転で

660
00:24:34,620 --> 00:24:36,480


661
00:24:36,480 --> 00:24:38,100
あるデータセットの真の変換が行われることを確認できます。

662
00:24:38,100 --> 00:24:40,559
その後、それを、

663
00:24:40,559 --> 00:24:42,659
私が後期に自分の役割をどのように実行したかと比較します。

664
00:24:42,659 --> 00:24:44,700
脳内で活性化を動かして空間を作り、

665
00:24:44,700 --> 00:24:47,280
それからデコードすると、

666
00:24:47,280 --> 00:24:49,919
まったく同じものが得られることがわかります。これは、準

667
00:24:49,919 --> 00:24:52,140


668
00:24:52,140 --> 00:24:53,580


669
00:24:53,580 --> 00:24:56,820
同型性を検証するために以前話した可換性の性質を実証しており

670
00:24:56,820 --> 00:24:58,799
、これをもう少し品質を測定するためのものです。

671
00:24:58,799 --> 00:25:02,460
定量的には、いわゆる等

672
00:25:02,460 --> 00:25:04,440
分散損失を測定できるので、

673
00:25:04,440 --> 00:25:07,080
これは実際に、

674
00:25:07,080 --> 00:25:09,360
転がる

675
00:25:09,360 --> 00:25:12,120
カプセルの活性化または頭の中で転がるのと、

676
00:25:12,120 --> 00:25:15,059
転がって展開するのを観察するのとの間の違いを定量化したものであり、

677
00:25:15,059 --> 00:25:16,559
彼らは

678
00:25:16,559 --> 00:25:19,440
目の前で展開する変換を観察しているので、

679
00:25:19,440 --> 00:25:21,600
地形が見えます Bae は、

680
00:25:21,600 --> 00:25:24,000
かなり低い等分散

681
00:25:24,000 --> 00:25:26,700
誤差を達成します。このバブル vae は、不変性を

682
00:25:26,700 --> 00:25:27,960
学習する前に話したものです。そのため、

683
00:25:27,960 --> 00:25:29,820
シフト

684
00:25:29,820 --> 00:25:32,340
演算がありません。また、従来の vae には、

685
00:25:32,340 --> 00:25:35,640
組織化や

686
00:25:35,640 --> 00:25:37,380
時間コンポーネントの概念がないため、パフォーマンスが非常に

687
00:25:37,380 --> 00:25:40,320
低下します。 これにより、この

688
00:25:40,320 --> 00:25:41,700
モデルは

689
00:25:41,700 --> 00:25:45,059
シーケンスのより優れた生成モデルであることがわかります。データ セットの

690
00:25:45,059 --> 00:25:48,179
負の対数尤度のように、より低い値が得られるだけです。

691
00:25:48,179 --> 00:25:50,100


692
00:25:50,100 --> 00:25:51,720
つまり、このデータ セットは、

693
00:25:51,720 --> 00:25:52,919


694
00:25:52,919 --> 00:25:55,460
変換

695
00:25:55,980 --> 00:25:58,140
ええと、これを複数の異なる変換タイプでテストできます。

696
00:25:58,140 --> 00:25:59,760


697
00:25:59,760 --> 00:26:00,840
上の行は、

698
00:26:00,840 --> 00:26:02,880


699
00:26:02,880 --> 00:26:05,039
グレーアウトされた画像を取り出して実際の変換を示しています。次に、下の

700
00:26:05,039 --> 00:26:07,080
行でエンコードし、

701
00:26:07,080 --> 00:26:08,700
アクティベーションをロールバックして、そのままにしておきます。

702
00:26:08,700 --> 00:26:12,140
デコードして、観察されて

703
00:26:12,140 --> 00:26:15,000
いる現在の変換としてモデルが何を学習したかを確認

704
00:26:15,000 --> 00:26:17,039


705
00:26:17,039 --> 00:26:19,340
します。また、

706
00:26:19,340 --> 00:26:21,360


707
00:26:21,360 --> 00:26:23,640
これまでに見たことのないテスト セット

708
00:26:23,640 --> 00:26:25,260
からの画像を使用して、これまでに

709
00:26:25,260 --> 00:26:26,580
見たことのないシーケンスのこれらの要素を基本的に完全に再構築できることがわかります。 以前は

710
00:26:26,580 --> 00:26:28,380
単に、現在エンコード中の変換が何であるかを知っているため、

711
00:26:28,380 --> 00:26:29,760


712
00:26:29,760 --> 00:26:31,500
それを新しい例に一般化できるため、

713
00:26:31,500 --> 00:26:33,919


714
00:26:34,020 --> 00:26:36,360
この部分から得られるポイントは実際に

715
00:26:36,360 --> 00:26:38,039
地形的な

716
00:26:38,039 --> 00:26:40,080
構成です。保存された入力構造を示しましたが、今回は

717
00:26:40,080 --> 00:26:41,940
それが潜在的に効率と一般化を向上させることができることを示しています。

718
00:26:41,940 --> 00:26:44,279


719
00:26:44,279 --> 00:26:46,200


720
00:26:46,200 --> 00:26:48,600
最後に、私たちを驚かせ、

721
00:26:48,600 --> 00:26:49,980
潜在的に最も

722
00:26:49,980 --> 00:26:52,500
興味深いと思ったのは、モデル

723
00:26:52,500 --> 00:26:53,700
によって学習されたこれらの変換が、

724
00:26:53,700 --> 00:26:54,960


725
00:26:54,960 --> 00:26:57,059


726
00:26:57,059 --> 00:26:59,580
トレーニング中には表示されない変換の組み合わせを実際に一般化しているということです。

727
00:26:59,580 --> 00:27:02,100
たとえば、トレーニングだけを行っているにもかかわらずです。 色

728
00:27:02,100 --> 00:27:04,200
と回転の変換と

729
00:27:04,200 --> 00:27:06,419
分離

730
00:27:06,419 --> 00:27:08,340


731
00:27:08,340 --> 00:27:11,100
テスト時にモデルに結合された色の回転変換が提示された場合、カプセルの役割を通じて

732
00:27:11,100 --> 00:27:13,140
これらの変換を完全にモデル化し、

733
00:27:13,140 --> 00:27:14,700
完全に完了できることがわかります。これは、

734
00:27:14,700 --> 00:27:17,159


735
00:27:17,159 --> 00:27:19,620
これらの

736
00:27:19,620 --> 00:27:20,880
異なるものを因数分解して表現することを学習していることを意味します。 変換と

737
00:27:20,880 --> 00:27:24,600
それを推論時に柔軟に組み合わせることができる

738
00:27:24,600 --> 00:27:28,140
ので、おそらく

739
00:27:28,140 --> 00:27:29,820
一般化で公式の効率が得られるだけでなく、

740
00:27:29,820 --> 00:27:34,100
基本的な構成性も得られるでしょう。そこで、

741
00:27:34,260 --> 00:27:36,059
制限と次に何ができるかについて話しましょう。

742
00:27:36,059 --> 00:27:38,460
主な

743
00:27:38,460 --> 00:27:40,620
制限は、 事前に定義された

744
00:27:40,620 --> 00:27:44,159
変換で、

745
00:27:44,159 --> 00:27:46,500
空間と時間の両方に課しているため、

746
00:27:46,500 --> 00:27:49,080
グループ変換、

747
00:27:49,080 --> 00:27:52,440
具体的には機械学習の世界

748
00:27:52,440 --> 00:27:53,940
で現在行われている変換や回転などから解放されましたが、

749
00:27:53,940 --> 00:27:55,559


750
00:27:55,559 --> 00:27:59,240
ハードコードされた潜在的な

751
00:27:59,240 --> 00:28:01,980
役割がまだ残っています。 私たちが目にするものすべてに目を向け、

752
00:28:01,980 --> 00:28:03,900
これをもう少し

753
00:28:03,900 --> 00:28:05,700
柔軟にすることで、

754
00:28:05,700 --> 00:28:08,880
より多様な変換をモデル化できればいいのですが、

755
00:28:08,880 --> 00:28:10,980


756
00:28:10,980 --> 00:28:13,860


757
00:28:13,860 --> 00:28:15,600


758
00:28:15,600 --> 00:28:18,120
おそらく脳内で観察される、より構造化された時空間ダイナミクスからインスピレーションを得られるのではないかと考えています。

759
00:28:18,120 --> 00:28:20,400
この講演の第 2 部は、

760
00:28:20,400 --> 00:28:22,140
時空間ダイナミクスです。これを

761
00:28:22,140 --> 00:28:23,039


762
00:28:23,039 --> 00:28:25,200
人工ニューラル ネットワークに統合しようとしています。その一例は、

763
00:28:25,200 --> 00:28:27,059


764
00:28:27,059 --> 00:28:28,020
ここで示したような進行波です。では、

765
00:28:28,020 --> 00:28:30,600
これは何を意味するのでしょうか。これは非常

766
00:28:30,600 --> 00:28:32,279
に重要です。 最近の論文では、

767
00:28:32,279 --> 00:28:36,059
36 ミリ秒の解像度で動作する 9 テスラ fmri を使用して、

768
00:28:36,059 --> 00:28:38,700


769
00:28:38,700 --> 00:28:40,980
麻酔下のラットの脳の単一スライスを画像化しました。

770
00:28:40,980 --> 00:28:43,320
そして、私たちが見ているのは、この非常に明確に

771
00:28:43,320 --> 00:28:45,720
構造化された空間時間活動と

772
00:28:45,720 --> 00:28:48,299
相関関係であり、論文の著者らは

773
00:28:48,299 --> 00:28:50,520
これを分析し続けています。 右側に

774
00:28:50,520 --> 00:28:52,919
示されているように主モードに関するアクティビティである

775
00:28:52,919 --> 00:28:55,799
ため、

776
00:28:55,799 --> 00:28:57,179
おそらくこのようなある種の相関

777
00:28:57,179 --> 00:28:59,039
構造は、

778
00:28:59,039 --> 00:29:01,260


779
00:29:01,260 --> 00:29:03,240
観察された変換に関してモデルの表現を構築するのに有益である可能性がありますが、

780
00:29:03,240 --> 00:29:05,100


781
00:29:05,100 --> 00:29:07,440
単純な方法よりもはるかに柔軟な方法であると私たちの仮説は考えられます。

782
00:29:07,440 --> 00:29:10,700
私たちが前にやっていたようなただの循環シフトです

783
00:29:11,279 --> 00:29:12,419


784
00:29:12,419 --> 00:29:15,900
えっと、これは

785
00:29:15,900 --> 00:29:19,320
SSI のネズミだけで観察されているわけではありません えー、

786
00:29:19,320 --> 00:29:20,940
この進行波が起きている霊長類

787
00:29:20,940 --> 00:29:24,179
の Mt 皮質で起こっているのがわかります えー、

788
00:29:24,179 --> 00:29:27,600
たとえば

789
00:29:27,600 --> 00:29:29,580
左側 ここでは、

790
00:29:29,580 --> 00:29:31,740
実際に

791
00:29:31,740 --> 00:29:35,520
変化する進行波を示しています。波の位相に基づいて、霊長類が低コントラストの刺激を見る可能性がどの程度あるのかを

792
00:29:35,520 --> 00:29:38,279


793
00:29:38,279 --> 00:29:40,980
示しています。さらに、

794
00:29:40,980 --> 00:29:43,500
右側の高コントラストの刺激のようなものは、外側に伝播する進行

795
00:29:43,500 --> 00:29:45,779
波の活動を誘発する可能性があることを示しています。

796
00:29:45,779 --> 00:29:47,520


797
00:29:47,520 --> 00:29:50,039
一次視覚野でさえ、これらは実際に

798
00:29:50,039 --> 00:29:52,140
脳全体の

799
00:29:52,140 --> 00:29:54,000
複数のレベルで遍在しているので、私たちの場合、構造表現学習に

800
00:29:54,000 --> 00:29:55,440
それらがどのような影響を与えるのかを研究することは興味深いでしょう、

801
00:29:55,440 --> 00:29:58,140


802
00:29:58,140 --> 00:29:59,700


803
00:29:59,700 --> 00:30:01,799
または一般に、

804
00:30:01,799 --> 00:30:04,140


805
00:30:04,140 --> 00:30:06,720
これらのタイプのええダイナミクスを研究した先行研究があります 彼らは

806
00:30:06,720 --> 00:30:08,700
モデルを構築するので、これらは

807
00:30:08,700 --> 00:30:10,380
スパイクニューラルネットワークを記述する方程式です。

808
00:30:10,380 --> 00:30:12,600


809
00:30:12,600 --> 00:30:15,720
時間遅延を実装すると、実際には

810
00:30:15,720 --> 00:30:18,240
ニューロン間の軸索時間遅延が示されます。ネットワークサイズが十分に大きい限り、

811
00:30:18,240 --> 00:30:20,820
進行波の構造ダイナミクスが得られます。

812
00:30:20,820 --> 00:30:22,440


813
00:30:22,440 --> 00:30:24,059


814
00:30:24,059 --> 00:30:26,520
えー、しかし、おそらく多くの人が知っていると思いますが、ディープ ニューラル ネットワークと

815
00:30:26,520 --> 00:30:28,620


816
00:30:28,620 --> 00:30:31,320
同じサイズとパフォーマンスのスパイキング ニューラル ネットワークを

817
00:30:31,320 --> 00:30:34,820


818
00:30:34,820 --> 00:30:37,679
同様に訓練するのは比較的困難です、底部の別のシステムは

819
00:30:37,679 --> 00:30:39,539
非常に単純ですが、

820
00:30:39,539 --> 00:30:42,840
おそらく単純すぎるかもしれません、えー、

821
00:30:42,840 --> 00:30:45,120
これらは結合された発振器のネットワークであり、これらは知られています

822
00:30:45,120 --> 00:30:48,779
同期性と時空間

823
00:30:48,779 --> 00:30:52,200
ダイナミクスと複雑なパターンを示しますが、

824
00:30:52,200 --> 00:30:53,520
これは位相低減システムのようなものと呼ばれるもので、

825
00:30:53,520 --> 00:30:55,500


826
00:30:55,500 --> 00:30:57,059
私たちが興味を持っている完全な複雑性を完全には捉えていないため、

827
00:30:57,059 --> 00:30:58,140


828
00:30:58,140 --> 00:31:00,779
これら 2 つと私たちが考えているものの間にある可能性のある

829
00:31:00,779 --> 00:31:03,600
ものに注目しています。 この作業で決定したのは、

830
00:31:03,600 --> 00:31:06,600


831
00:31:06,600 --> 00:31:08,520


832
00:31:08,520 --> 00:31:10,620
パラモト モデルよりもわずかに柔軟にいくつかのオシレーターのネットワークをパラメーター化することです。

833
00:31:10,620 --> 00:31:12,360
つまり、これは実際には、

834
00:31:12,360 --> 00:31:14,580


835
00:31:14,580 --> 00:31:16,380
コンスタンティン ラッシュとニシャのこのカップルの蒸留リカレント ニューラル ネットワークに基づいて構築されており、

836
00:31:16,380 --> 00:31:18,720


837
00:31:18,720 --> 00:31:20,760
基本的に次の方程式を採用しています。

838
00:31:20,760 --> 00:31:22,200
単純な

839
00:31:22,200 --> 00:31:23,820
調和振動子を説明します これは 2 次微分

840
00:31:23,820 --> 00:31:26,159
方程式です ばね

841
00:31:26,159 --> 00:31:29,940
上のボールの加速度はその変位に比例します

842
00:31:29,940 --> 00:31:32,480


843
00:31:32,480 --> 00:31:35,220
ああ、振動が時間の経過とともにゆっくりと消えるように、減衰などの追加項を追加できます

844
00:31:35,220 --> 00:31:37,260


845
00:31:37,260 --> 00:31:39,360


846
00:31:39,360 --> 00:31:41,580
この振動子を外部で駆動できます

847
00:31:41,580 --> 00:31:43,380


848
00:31:43,380 --> 00:31:45,179
このダンピングを打ち消すため、または

849
00:31:45,179 --> 00:31:47,279
ダイナミクスにもう少し複雑さを与えるための入力を追加します。

850
00:31:47,279 --> 00:31:49,260
さらに、これらのオシレーターを多数持っている場合は、

851
00:31:49,260 --> 00:31:50,940
これらのオシレーターを結合マトリックスで結合することができます。

852
00:31:50,940 --> 00:31:53,000


853
00:31:53,000 --> 00:31:55,320
この図で説明しているように、実際に

854
00:31:55,320 --> 00:31:56,640
次のことができます。

855
00:31:56,640 --> 00:31:58,140
このネットワークをスプリング上の泡の束と考えてください。それらはおそらく、

856
00:31:58,140 --> 00:31:59,940


857
00:31:59,940 --> 00:32:01,740


858
00:32:01,740 --> 00:32:03,600


859
00:32:03,600 --> 00:32:05,279
ロシアのミシュラのカップル蒸留リカレント ニューラル ネットワークが

860
00:32:05,279 --> 00:32:08,100
これらのさまざまな用語を使用して、スプリングや弾性バンドによって相互に接続されている可能性があります。

861
00:32:08,100 --> 00:32:09,899
これは非常に強力であることが示されています。

862
00:32:09,899 --> 00:32:12,480
長いシーケンスのモデリングに関して、彼らは

863
00:32:12,480 --> 00:32:13,740


864
00:32:13,740 --> 00:32:15,360
これを構築する脳からインスピレーションを得たとも述べていて、

865
00:32:15,360 --> 00:32:17,700
その論文には多くの優れた分析があり、

866
00:32:17,700 --> 00:32:19,020
たとえば、これがリカレント ニューラル ネットワークで通常発生する勾配消失問題に関して非常に有益な特性であることを示しています。

867
00:32:19,020 --> 00:32:21,440


868
00:32:21,440 --> 00:32:23,460


869
00:32:23,460 --> 00:32:25,440


870
00:32:25,440 --> 00:32:26,820


871
00:32:26,820 --> 00:32:29,159
しかし、時空

872
00:32:29,159 --> 00:32:30,960
間ダイナミクスとこのタイプの

873
00:32:30,960 --> 00:32:32,820
モデルを見たい場合は、少し難しいです。

874
00:32:32,820 --> 00:32:34,919
なぜなら、ここでのこれらの結合行列は、

875
00:32:34,919 --> 00:32:36,320


876
00:32:36,320 --> 00:32:39,600
各ニューラルまたは各

877
00:32:39,600 --> 00:32:41,240
振動子を接続する W が互いに配置されているためです。

878
00:32:41,240 --> 00:32:43,620
これらは、

879
00:32:43,620 --> 00:32:45,120
私が試みたように密に接続された行列であるためです。 ここの左側に描かれている

880
00:32:45,120 --> 00:32:46,020


881
00:32:46,020 --> 00:32:48,299
ので、このネットワークのダイナミクスを視覚化しようとすると、

882
00:32:48,299 --> 00:32:50,580


883
00:32:50,580 --> 00:32:51,899
空間構成がまったく表示されません。

884
00:32:51,899 --> 00:32:55,380
継承はありません。これは、

885
00:32:55,380 --> 00:32:57,000
このモデルの潜在空間に対するお詫びです。

886
00:32:57,000 --> 00:32:58,799
ええと、これを前の例のニューロンのように考えることができます

887
00:32:58,799 --> 00:33:01,020


888
00:33:01,020 --> 00:33:03,240
他のニューロンの任意のセットに接続されている可能性があります

889
00:33:03,240 --> 00:33:04,919
これらのニューロンは別の

890
00:33:04,919 --> 00:33:06,600
任意のニューロンのセットに接続されており、

891
00:33:06,600 --> 00:33:08,520


892
00:33:08,520 --> 00:33:10,860
確かに振動ダイナミクスが得られるだけですが、

893
00:33:10,860 --> 00:33:13,260
構造的にあまり意味をなさない一種の変動が得られるため、

894
00:33:13,260 --> 00:33:15,360
私たちの研究では次のように考えました。 さて、

895
00:33:15,360 --> 00:33:18,299
これを、この構造化されたアクティビティの伝播に関心のあるダイナミクスのタイプにさらに変換するにはどうすればよいでしょうか。そのための

896
00:33:18,299 --> 00:33:19,860


897
00:33:19,860 --> 00:33:22,140


898
00:33:22,140 --> 00:33:23,940
1 つの明確な方法は、

899
00:33:23,940 --> 00:33:26,539
より構造化された接続性行列 W を用意することです。

900
00:33:26,539 --> 00:33:29,880
これは簡単に実装でき

901
00:33:29,880 --> 00:33:31,559
、効率的であることがわかりました。 畳み込み演算を通じて実装されるため、

902
00:33:31,559 --> 00:33:33,000


903
00:33:33,000 --> 00:33:34,620
ローカルに

904
00:33:34,620 --> 00:33:36,299
接続されたレイヤーのように考えることができます。そのため、

905
00:33:36,299 --> 00:33:37,860
すべてのニューロンが接続されるのではなく、すべての

906
00:33:37,860 --> 00:33:39,480
ニューロンが近隣のニューロンに接続されるだけです。

907
00:33:39,480 --> 00:33:41,580
トレーニング後は、

908
00:33:41,580 --> 00:33:42,840


909
00:33:42,840 --> 00:33:44,880
滑らかな空間のように見えるものを取得することになります。 時間

910
00:33:44,880 --> 00:33:46,620
ダイナミクス

911
00:33:46,620 --> 00:33:48,419


912
00:33:48,419 --> 00:33:50,519
このモデルをトレーニングすることをもう少し明確にするために、説明していたこの個別の

913
00:33:50,519 --> 00:33:52,200
2 次微分方程式を

914
00:33:52,200 --> 00:33:54,299


915
00:33:54,299 --> 00:33:56,340
2 つの 1 次方程式に離散化する前に取得します。

916
00:33:56,340 --> 00:33:57,960
これは、今あるオードを数値的に積分するようなものと考えることができます。

917
00:33:57,960 --> 00:34:01,200


918
00:34:01,200 --> 00:34:03,120
速度を更新して、

919
00:34:03,120 --> 00:34:06,000
このモデルを

920
00:34:06,000 --> 00:34:07,620
自動エンコーダまたは

921
00:34:07,620 --> 00:34:09,839
自動回帰モデルのようなものとしてトレーニングできるので、

922
00:34:09,839 --> 00:34:11,460
入力を受け取ると、それを潜在空間にエンコードします。

923
00:34:11,460 --> 00:34:14,339
実際の入力は、Dr が作用する x 項の f です。

924
00:34:14,339 --> 00:34:16,500
駆動項として、

925
00:34:16,500 --> 00:34:18,599
これらの発振器を底から駆動するようなものです。

926
00:34:18,599 --> 00:34:20,879
そして、それらにはローカル結合である結合項

927
00:34:20,879 --> 00:34:23,099
によって定義される独自のダイナミクスがあり、

928
00:34:23,099 --> 00:34:25,800


929
00:34:25,800 --> 00:34:27,540
各タイムステップでこの

930
00:34:27,540 --> 00:34:29,460
潜在状態、この波の状態を取得し、デコードして

931
00:34:29,460 --> 00:34:31,560
試してみます。 入力を再構築し、

932
00:34:31,560 --> 00:34:33,540
現在のタイム ステップまたは

933
00:34:33,540 --> 00:34:35,699
将来のタイム ステップにある場合、

934
00:34:35,699 --> 00:34:37,980


935
00:34:37,980 --> 00:34:42,300
トレーニング中にこれらのモデルの分析を行うことができ、

936
00:34:42,300 --> 00:34:43,619
トレーニング前とトレーニング後に何が起こるかを確認

937
00:34:43,619 --> 00:34:45,780
できます。ダイナミクスの位相と速度を計算できます。

938
00:34:45,780 --> 00:34:47,399


939
00:34:47,399 --> 00:34:49,379
潜在空間 基本的に、

940
00:34:49,379 --> 00:34:51,480
取引の開始時にはモデルに波がないことがわかります

941
00:34:51,480 --> 00:34:53,699
が、50

942
00:34:53,699 --> 00:34:55,500
エポック後のトレーニング後は、オブジェクトを回転させる

943
00:34:55,500 --> 00:34:57,119


944
00:34:57,119 --> 00:35:00,420


945
00:35:00,420 --> 00:35:01,800
ように実行しているこのシーケンス モデリング タスクのために、スムーズに構造化されたアクティビティが下方に伝播していることがわかります。

946
00:35:01,800 --> 00:35:04,380


947
00:35:04,380 --> 00:35:05,940
これの利点は何ですか

948
00:35:05,940 --> 00:35:07,680
つまり、私がこれを動機付けた全体の理由は、

949
00:35:07,680 --> 00:35:10,380
より

950
00:35:10,380 --> 00:35:11,880
柔軟に学習された構造を持ちたいということでした。

951
00:35:11,880 --> 00:35:13,020
実際にそうしているのですか、それとも

952
00:35:13,020 --> 00:35:15,060
かなりの波が来ているだけですか、

953
00:35:15,060 --> 00:35:16,859
ええと、私たちが論文で示したのは、

954
00:35:16,859 --> 00:35:19,320
私たちが実際にそうしているということです ある種の

955
00:35:19,320 --> 00:35:20,940
有用な構造を学習し、それを示した方法は、

956
00:35:20,940 --> 00:35:22,440


957
00:35:22,440 --> 00:35:24,960
入力を受け取り

958
00:35:24,960 --> 00:35:27,000
、それをエンコードして波動状態を取得し、

959
00:35:27,000 --> 00:35:29,280


960
00:35:29,280 --> 00:35:31,619
その波動状態で人工的に波動を伝播させてデコードすると、この可換図のようなものを使用して、次のことが

961
00:35:31,619 --> 00:35:33,480
できます。

962
00:35:33,480 --> 00:35:35,220
実際には、

963
00:35:35,220 --> 00:35:37,140


964
00:35:37,140 --> 00:35:39,180
さまざまな変換のさまざまな画像を表示するだけで得られたものとまったく同じであることがわかります。したがって、

965
00:35:39,180 --> 00:35:41,640
多くのさまざまな桁の異なる

966
00:35:41,640 --> 00:35:43,920
特徴があり、それをモデル化するために、

967
00:35:43,920 --> 00:35:46,140
それぞれのケースでさまざまな種類の波動アクティビティが得られることがわかります。

968
00:35:46,140 --> 00:35:47,880


969
00:35:47,880 --> 00:35:49,140


970
00:35:49,140 --> 00:35:51,119
異なるデータセットでトレーニングすると、

971
00:35:51,119 --> 00:35:53,400
同様に、より複雑なダイナミクスが表示されます。

972
00:35:53,400 --> 00:35:55,200
この場合、反対方向の進行波と考えられる

973
00:35:55,200 --> 00:35:57,839
進行波や定在波さえない可能性がある

974
00:35:57,839 --> 00:36:00,359


975
00:36:00,359 --> 00:36:02,339
ため、

976
00:36:02,339 --> 00:36:04,079
これらの軌道をモデル化しているかどうかを確認します。 ダイナミクスでは、潜在空間で

977
00:36:04,079 --> 00:36:06,000
このようなスムーズに動く活動の塊が得られます。

978
00:36:06,000 --> 00:36:07,619


979
00:36:07,619 --> 00:36:09,839
振り子をモデル化している場合、同様に

980
00:36:09,839 --> 00:36:13,820
複雑な振動活動が得られるため、

981
00:36:14,099 --> 00:36:17,339
入力構造は維持されますが、

982
00:36:17,339 --> 00:36:19,560


983
00:36:19,560 --> 00:36:21,599
以前よりも柔軟性がさらに高まり、これが最終的な

984
00:36:21,599 --> 00:36:23,400
目標のようなものです。

985
00:36:23,400 --> 00:36:26,099
それでは最後に、

986
00:36:26,099 --> 00:36:28,320
この研究の結果が

987
00:36:28,320 --> 00:36:30,420
人工知能を向上させるだけでなく、脳の測定結果がなぜそのように見えるのかを理解するのに

988
00:36:30,420 --> 00:36:32,099
どのように役立つと私が考えるかについて少し話したいと思います。

989
00:36:32,099 --> 00:36:34,440


990
00:36:34,440 --> 00:36:36,240


991
00:36:36,240 --> 00:36:38,579
つまり、これは、

992
00:36:38,579 --> 00:36:41,040
以前、ビザと場所について少し話しましたが、

993
00:36:41,040 --> 00:36:43,740


994
00:36:43,740 --> 00:36:46,859
チン・ヒガオとのこの素晴らしい仕事で、

995
00:36:46,859 --> 00:36:48,900
私たちが議論した単純な地形図の事前設定が

996
00:36:48,900 --> 00:36:50,579
これらと同じ効果を再現できるかどうかを研究したため、

997
00:36:50,579 --> 00:36:53,339
特に

998
00:36:53,339 --> 00:36:56,099
このコーエンの D の値を入れました。 顔だけ、または物体や体だけを含む

999
00:36:56,099 --> 00:36:58,200


1000
00:36:58,200 --> 00:37:00,000
可能性のある画像の異なるデータセットに対する各ニューロンの選択性メトリックを測定する

1001
00:37:00,000 --> 00:37:02,460


1002
00:37:02,460 --> 00:37:03,359


1003
00:37:03,359 --> 00:37:05,880
ため、

1004
00:37:05,880 --> 00:37:07,920
顔に反応する可能性が高いか、それとも

1005
00:37:07,920 --> 00:37:10,380
脳にロシア人が現れるかをすべてのニューロンについて測定しますが、私はそうします

1006
00:37:10,380 --> 00:37:12,839
これは、

1007
00:37:12,839 --> 00:37:15,300
選択性の相対的な組織化が少なくとも

1008
00:37:15,300 --> 00:37:17,400
部分的にはデータ内の相関統計に起因している可能性があり、ディープ ニューラル ネットワークなどの

1009
00:37:17,400 --> 00:37:19,800


1010
00:37:19,800 --> 00:37:21,359
高度に

1011
00:37:21,359 --> 00:37:23,640
非線形な将来の抽出器を通過した後にパスを変更する必要があることを示していると考えてください。

1012
00:37:23,640 --> 00:37:25,440


1013
00:37:25,440 --> 00:37:27,480


1014
00:37:27,480 --> 00:37:29,040
興味深いのは、

1015
00:37:29,040 --> 00:37:30,900
三部構成と呼ばれるものが既知であるか、

1016
00:37:30,900 --> 00:37:36,720


1017
00:37:36,720 --> 00:37:38,400


1018
00:37:38,400 --> 00:37:40,680
ビジュアル ストリームではないので、えー、またはオブジェクトの画像です。オブジェクトに関する選択性です。

1019
00:37:40,680 --> 00:37:43,200
これは生きているのか

1020
00:37:43,200 --> 00:37:46,200
無生物なのかなど、より抽象的なプロパティによって整理されています。また、現実世界の

1021
00:37:46,200 --> 00:37:48,480
オブジェクトのサイズも何ですか これは

1022
00:37:48,480 --> 00:37:50,700
ティーポットと車のサイズです。

1023
00:37:50,700 --> 00:37:53,520


1024
00:37:53,520 --> 00:37:56,160
人間の場合、選択性は

1025
00:37:56,160 --> 00:37:57,540
この三部構成で組織されており、

1026
00:37:57,540 --> 00:37:59,760
通常、選択性の点で生物と無生物の中間に位置する小さな物体が存在することが

1027
00:37:59,760 --> 00:38:01,920


1028
00:38:01,920 --> 00:38:04,200
わかります。

1029
00:38:04,200 --> 00:38:06,060
ここでも同じことが起こっているので、

1030
00:38:06,060 --> 00:38:07,440
これらは同じセットのニューロンの選択性を測定しています

1031
00:38:07,440 --> 00:38:08,880
が、

1032
00:38:08,880 --> 00:38:10,859
これらの刺激の違いに関しては、

1033
00:38:10,859 --> 00:38:12,440
小さなクラスターが生物クラスターと無生物クラスターの間にあることがわかります。

1034
00:38:12,440 --> 00:38:14,820


1035
00:38:14,820 --> 00:38:16,079
これは複数の異なる初期化でも発生します。

1036
00:38:16,079 --> 00:38:18,900
これは、

1037
00:38:18,900 --> 00:38:20,880


1038
00:38:20,880 --> 00:38:22,980
このコミュニティでもう少し詳しく調査できることを願っています。これは、

1039
00:38:22,980 --> 00:38:24,119


1040
00:38:24,119 --> 00:38:26,220


1041
00:38:26,220 --> 00:38:28,200
構造化された世界モデルを構築したことを示す実際の方法であり、

1042
00:38:28,200 --> 00:38:30,119
潜在的にこの世界モデルは、構造化された方法で

1043
00:38:30,119 --> 00:38:31,980


1044
00:38:31,980 --> 00:38:34,740
現実世界のデータをより適切に表現するのに有益であるため、興味深いと思います。

1045
00:38:34,740 --> 00:38:37,619


1046
00:38:37,619 --> 00:38:39,119
そういう意味では自由エネルギーが低くなります

1047
00:38:39,119 --> 00:38:40,619
ので、

1048
00:38:40,619 --> 00:38:42,300


1049
00:38:42,300 --> 00:38:44,400
ここで示したようにこれらのモデルを開発することで、この構造がどのように出現するのかについての新しいメカニズムについての洞察が得られるかもしれません。

1050
00:38:44,400 --> 00:38:46,500


1051
00:38:46,500 --> 00:38:48,900


1052
00:38:48,900 --> 00:38:50,460


1053
00:38:50,460 --> 00:38:52,920
これまで考えもしなかった地形的組織も含めて、私が

1054
00:38:52,920 --> 00:38:55,520
見ていたマシンモデル ニューロンの向きの選択性、

1055
00:38:55,520 --> 00:38:58,260
私は

1056
00:38:58,260 --> 00:39:01,020
特に何かが起こることを期待していませんでしたが、

1057
00:39:01,020 --> 00:39:03,420


1058
00:39:03,420 --> 00:39:05,339
この

1059
00:39:05,339 --> 00:39:08,099
シミュレートされた垂直面上を伝播する波のようなものを見ていると、

1060
00:39:08,099 --> 00:39:09,960
おそらく回転した画像を表示しているので、これが何らかの影響を与えているのではないかと思いました

1061
00:39:09,960 --> 00:39:11,820


1062
00:39:11,820 --> 00:39:13,740
方向の選択性、

1063
00:39:13,740 --> 00:39:15,599
そして実際に、これらの異なる方向を向いた

1064
00:39:15,599 --> 00:39:17,460
線に関して各ニューロンの選択性を測定すると、

1065
00:39:17,460 --> 00:39:18,660


1066
00:39:18,660 --> 00:39:22,079


1067
00:39:22,079 --> 00:39:24,300
驚くほど一

1068
00:39:24,300 --> 00:39:25,859
次視覚野で見られる方向のタイプの列を思い出させることがわかります。

1069
00:39:25,859 --> 00:39:27,599
これは遡ることです。

1070
00:39:27,599 --> 00:39:29,520
ヒューゴとイタチにとって、これは

1071
00:39:29,520 --> 00:39:30,900
このモデルと、

1072
00:39:30,900 --> 00:39:33,060
それが変換に関する時空間構造を持っているという事実から出てきたものです。

1073
00:39:33,060 --> 00:39:34,440


1074
00:39:34,440 --> 00:39:37,619
もちろん、これは

1075
00:39:37,619 --> 00:39:39,599
非常に大まかな例えですが、これはどのような方法であるかを示す

1076
00:39:39,599 --> 00:39:40,740
一例だと思います このような

1077
00:39:40,740 --> 00:39:42,839
タイプのモデルを構築することは、

1078
00:39:42,839 --> 00:39:45,240
脳がどのように表現構造を構築し、

1079
00:39:45,240 --> 00:39:46,980


1080
00:39:46,980 --> 00:39:48,660
おそらくこれまで考えたこともなかった方法でそれが組織される方法について考えるのに役立ちます、

1081
00:39:48,660 --> 00:39:51,300


1082
00:39:51,300 --> 00:39:53,460
ええと、この種のことをしているのは私だけではないと思います

1083
00:39:53,460 --> 00:39:55,859
仕事なので、

1084
00:39:55,859 --> 00:39:57,240


1085
00:39:57,240 --> 00:39:59,579
これをやっている他の人たちについて少し話したいと思います。それで、

1086
00:39:59,579 --> 00:40:00,780
私はこの等価構造のようなものについて話してきました。

1087
00:40:00,780 --> 00:40:02,760


1088
00:40:02,760 --> 00:40:04,920
ジェームス・ウィッティントンや

1089
00:40:04,920 --> 00:40:08,880
ティム・バロンズ、サロゲンゴーリーなどの人々は、

1090
00:40:08,880 --> 00:40:10,680
最近、代数を導入することでそれを示しました。

1091
00:40:10,680 --> 00:40:14,940


1092
00:40:14,940 --> 00:40:17,040
この場合、学習プロセスに制約を加えると、

1093
00:40:17,040 --> 00:40:20,820


1094
00:40:20,820 --> 00:40:23,220


1095
00:40:23,220 --> 00:40:24,780


1096
00:40:24,780 --> 00:40:27,540
環状西北東南に移動すると、最終的には

1097
00:40:27,540 --> 00:40:29,280


1098
00:40:29,280 --> 00:40:31,440
同じ状態に戻るという、環境内の環境内のエージェントの動きに似ています。 ポイント 繰り返しになりますが、これらの種類の制約を導入すると、

1099
00:40:31,440 --> 00:40:32,820


1100
00:40:32,820 --> 00:40:35,040
グリッド セルのような表現が出現します。

1101
00:40:35,040 --> 00:40:36,900


1102
00:40:36,900 --> 00:40:39,359
それで、この

1103
00:40:39,359 --> 00:40:41,880
表現構造のアイデアが、

1104
00:40:41,880 --> 00:40:43,980
おそらく私たちが

1105
00:40:43,980 --> 00:40:45,480
見つけている科学的発見以上のものを説明するのにどのように役立つかに興味があります。

1106
00:40:45,480 --> 00:40:46,619


1107
00:40:46,619 --> 00:40:48,359
そして、これが

1108
00:40:48,359 --> 00:40:51,540
全体としての生成モデルにどのように関係するのか えーっと

1109
00:40:51,540 --> 00:40:52,800
、そして最後に、

1110
00:40:52,800 --> 00:40:54,599


1111
00:40:54,599 --> 00:40:56,099
これらのモデルの認知的可能性についても言うべきことがあると思います

1112
00:40:56,099 --> 00:40:57,420
おそらく、私たちは

1113
00:40:57,420 --> 00:40:59,579
それらを神経科学の観点からだけでなく、

1114
00:40:59,579 --> 00:41:01,020
微認識科学の

1115
00:41:01,020 --> 00:41:03,839
観点からもテストするつもりです たとえば、左側にこれらの

1116
00:41:03,839 --> 00:41:06,000
レイブンズ プログレッシブ マトリックスがあります。

1117
00:41:06,000 --> 00:41:08,640
ここでは、

1118
00:41:08,640 --> 00:41:11,099
これらの画像のどれがこのパターンに適合する可能性が高いかを言う必要があります。

1119
00:41:11,099 --> 00:41:12,599


1120
00:41:12,599 --> 00:41:14,760
または、たとえば、

1121
00:41:14,760 --> 00:41:16,740
このジェンガ タワーが

1122
00:41:16,740 --> 00:41:19,740
車を引っ張ったときに倒れる可能性はどのくらいですか 特定のブロックや

1123
00:41:19,740 --> 00:41:22,740
特定の構造を持つ

1124
00:41:22,740 --> 00:41:24,599


1125
00:41:24,599 --> 00:41:26,640
これらのタイプのテストは、

1126
00:41:26,640 --> 00:41:28,619
私たちが構築している世界モデルが、人間

1127
00:41:28,619 --> 00:41:31,560


1128
00:41:31,560 --> 00:41:33,660
として、または私たちが生来持っている常識を持っているタイプのモデルに似ているか

1129
00:41:33,660 --> 00:41:36,060
どうかを実際にテストしていると思います。 自然界に住む生き物として、

1130
00:41:36,060 --> 00:41:38,400


1131
00:41:38,400 --> 00:41:40,440
この方向でいくつかの予備作業をしました。

1132
00:41:40,440 --> 00:41:43,079
かなり予備的なもので、それほど複雑ではないと思いますが、

1133
00:41:43,079 --> 00:41:45,480


1134
00:41:45,480 --> 00:41:47,820
視覚的な錯覚をモデル化しようとしているようなもので、

1135
00:41:47,820 --> 00:41:50,520
移動バーの非常に単純なデータセットを取得するとします。

1136
00:41:50,520 --> 00:41:52,980
刺激や静的なバーやフレームを

1137
00:41:52,980 --> 00:41:54,960
少し動かすと、

1138
00:41:54,960 --> 00:41:57,060
モデルが実際にその

1139
00:41:57,060 --> 00:41:58,800
欠落したフレームを推論し、その後実際に

1140
00:41:58,800 --> 00:42:01,079
継続的な動きも推論することがわかります。そのため、

1141
00:42:01,079 --> 00:42:03,300


1142
00:42:03,300 --> 00:42:05,820
実際の刺激が提供する軌道をオーバーシュートしてから

1143
00:42:05,820 --> 00:42:08,760
再度修正するようなものです。 だから、イリュージョンのモデリングは、

1144
00:42:08,760 --> 00:42:10,320


1145
00:42:10,320 --> 00:42:12,660
私たちの世界モデルが

1146
00:42:12,660 --> 00:42:14,760
私たち自身が持っているタイプのモデルに似ているかどうかを研究するのに確かに興味深い方法だと思います。

1147
00:42:14,760 --> 00:42:16,619


1148
00:42:16,619 --> 00:42:19,619
結論として、ええと、

1149
00:42:19,619 --> 00:42:21,900
地形学的事前学習により、

1150
00:42:21,900 --> 00:42:23,220
構造化された

1151
00:42:23,220 --> 00:42:24,839
表現または構造化された世界

1152
00:42:24,839 --> 00:42:26,700
モデルを効果的に学習したことを示すことができると思います。 学習された構造は

1153
00:42:26,700 --> 00:42:29,160
柔軟性があり、任意の

1154
00:42:29,160 --> 00:42:30,780
変換に適応できます。 従来の等

1155
00:42:30,780 --> 00:42:33,720
変量とは異なり、地形プロバイダーは、地形学的

1156
00:42:33,720 --> 00:42:35,579


1157
00:42:35,579 --> 00:42:37,619
VAE で行ったように統計的に、または

1158
00:42:37,619 --> 00:42:39,480
これらの

1159
00:42:39,480 --> 00:42:42,000
ニューラル ウェーブ マシン タイプ モデルで示したようにダイナミクスを通じて誘導できます。

1160
00:42:42,000 --> 00:42:44,460
結論として、これで終わります。

1161
00:42:44,460 --> 00:42:46,980


1162
00:42:46,980 --> 00:42:50,280
1980 年の福島氏の論文で見つけた引用で、人間と同じパターン認識能力を

1163
00:42:50,280 --> 00:42:52,079


1164
00:42:52,079 --> 00:42:53,520
持つニューラル ネットワーク モデルを作ることができれば、

1165
00:42:53,520 --> 00:42:55,020


1166
00:42:55,020 --> 00:42:57,060
人間と

1167
00:42:57,060 --> 00:42:58,800
比較して強力な手がかりが得られるだろうと彼は言っていて、かなり時代を先取りしていると思いました。

1168
00:42:58,800 --> 00:43:00,000


1169
00:43:00,000 --> 00:43:03,240
脳の神経メカニズムを理解するのはそういうことだと思うので、

1170
00:43:03,240 --> 00:43:06,119
私たちがここで目指している目標の一部はそうだと思うので、

1171
00:43:06,119 --> 00:43:08,220
それが私のアドバイザーであるマックスだと思います、

1172
00:43:08,220 --> 00:43:11,280
共著者のパトリック・UA、エミール・ジンギャンと

1173
00:43:11,280 --> 00:43:17,359
ヨーン、議論に興味があります、ありがとう、わかりました、

1174
00:43:20,640 --> 00:43:23,420


1175
00:43:24,660 --> 00:43:27,480
わかりました ありがとうございます 素晴らしい、とても

1176
00:43:27,480 --> 00:43:31,079
興味深いプレゼンテーションです

1177
00:43:31,079 --> 00:43:33,480
始めるべき場所がたくさんあります たぶん、ちょうどええと、

1178
00:43:33,480 --> 00:43:36,000
あなたがこの仕事に至ったきっかけ 博士課程の方向性のためにこの仕事に取り組むようになった

1179
00:43:36,000 --> 00:43:38,520
経緯について少し説明してください

1180
00:43:38,520 --> 00:43:43,819


1181
00:43:43,920 --> 00:43:45,119
そうですね、

1182
00:43:45,119 --> 00:43:46,020


1183
00:43:46,020 --> 00:43:49,200
つまり、私たちが研究してきたのは私のグループではありません

1184
00:43:49,200 --> 00:43:51,000
私は大学にいて、

1185
00:43:51,000 --> 00:43:52,700


1186
00:43:52,700 --> 00:43:56,640
しばらくの間数学的な観点から構造化表現を研究していて、

1187
00:43:56,640 --> 00:43:58,319
そこで何人かの人々が

1188
00:43:58,319 --> 00:44:00,240
モデルを持っていますが、それは変分

1189
00:44:00,240 --> 00:44:01,740
自動エンコーダのようなもので、

1190
00:44:01,740 --> 00:44:04,680


1191
00:44:04,680 --> 00:44:06,960
昔からあったものは何だろうと思います、それは

1192
00:44:06,960 --> 00:44:08,460


1193
00:44:08,460 --> 00:44:11,220
モデルです これは回転 2D

1194
00:44:11,220 --> 00:44:13,560
回転を完全に尊重しますが、

1195
00:44:13,560 --> 00:44:15,960
3D 回転をしたい場合はそれができません。

1196
00:44:15,960 --> 00:44:17,819
これは

1197
00:44:17,819 --> 00:44:19,740
2D プランへの投影という観点から見るとグループではないためです。

1198
00:44:19,740 --> 00:44:21,180


1199
00:44:21,180 --> 00:44:23,460
たとえば、これが回転すると情報が失われます。

1200
00:44:23,460 --> 00:44:24,240


1201
00:44:24,240 --> 00:44:26,280


1202
00:44:26,280 --> 00:44:27,960
私が

1203
00:44:27,960 --> 00:44:29,339
最初に指摘しようとしていたような自然な変換の一種です。

1204
00:44:29,339 --> 00:44:30,180


1205
00:44:30,180 --> 00:44:31,740
脳がどのように

1206
00:44:31,740 --> 00:44:34,020
自然な変換をモデル化するかについて考えようとしていたと思います。

1207
00:44:34,020 --> 00:44:35,400
現在のフレーム

1208
00:44:35,400 --> 00:44:37,200
ワークでは、

1209
00:44:37,200 --> 00:44:41,099


1210
00:44:41,099 --> 00:44:44,579
変分オートエンコーダーの観点からアクションがどのような役割を果たしていると思いますか?

1211
00:44:44,579 --> 00:44:48,420


1212
00:44:48,420 --> 00:44:50,520
外部のパターンだけでなく、

1213
00:44:50,520 --> 00:44:52,380
アクションの結果や

1214
00:44:52,380 --> 00:44:55,800
アクションを伴う世界モデルの構造構造も含むモデル はい、

1215
00:44:55,800 --> 00:44:58,619
いいえ、それは良い質問で、行為された

1216
00:44:58,619 --> 00:45:01,319
推論が

1217
00:45:01,319 --> 00:45:03,839
事実上

1218
00:45:03,839 --> 00:45:05,940
答えだと思います、それに対する良い答えだと思います、ええと、

1219
00:45:05,940 --> 00:45:09,000
そこはわかっています

1220
00:45:09,000 --> 00:45:11,099
強化学習フレームワークは、

1221
00:45:11,099 --> 00:45:12,660


1222
00:45:12,660 --> 00:45:15,060
外部でトレーニングされた世界モデルのようなものを使用する

1223
00:45:15,060 --> 00:45:17,280
ため、VAE か何かをトレーニングして、

1224
00:45:17,280 --> 00:45:19,800
その表現を強化学習システムで使用します

1225
00:45:19,800 --> 00:45:23,040
が、

1226
00:45:23,040 --> 00:45:24,720


1227
00:45:24,720 --> 00:45:26,520


1228
00:45:26,520 --> 00:45:30,780
アクションを伴う単一の目的である完全な種類のシステムがあると思います

1229
00:45:30,780 --> 00:45:33,660
データの可能性の一部として、

1230
00:45:33,660 --> 00:45:35,280
そうそう、そのほうがはるかにエレガントだと思います。

1231
00:45:35,280 --> 00:45:38,940
だから私はそのことの大支持者です。

1232
00:45:38,940 --> 00:45:39,960
うーん、

1233
00:45:39,960 --> 00:45:43,140


1234
00:45:43,140 --> 00:45:45,480
これらの構造化された世界モデルが Vae で

1235
00:45:45,480 --> 00:45:47,520
どのように構築されているかについてはまだ研究していません。 それについてはまったく取り組んでいませんでしたが、

1236
00:45:47,520 --> 00:45:48,780


1237
00:45:48,780 --> 00:45:50,819


1238
00:45:50,819 --> 00:45:52,339


1239
00:45:52,339 --> 00:45:54,839
変分オートエンコーダーでより構造化されたワールドモデルを使用することが、

1240
00:45:54,839 --> 00:45:56,880
アクティブな

1241
00:45:56,880 --> 00:45:58,319
設定でも同様に有益であるかどうかを確認するのは確かに非常に興味深いと思います、それは素晴らしいことだと

1242
00:45:58,319 --> 00:46:00,119
思います

1243
00:46:00,119 --> 00:46:03,599
これらの例のいくつかは、

1244
00:46:03,599 --> 00:46:05,579
グリッドセルの出現などを前に示したものだと考えてください。おそらく

1245
00:46:05,579 --> 00:46:07,500


1246
00:46:07,500 --> 00:46:08,880
その方向を示しているかもしれません。まあ、おそらく

1247
00:46:08,880 --> 00:46:10,560
脳は何かを行っているのですが、

1248
00:46:10,560 --> 00:46:12,540


1249
00:46:12,540 --> 00:46:13,680


1250
00:46:13,680 --> 00:46:15,359
明らかに多くの構造を持っています。これは明らかに、

1251
00:46:15,359 --> 00:46:19,140
いくつかのアクションを実行するのに役立つはずです。 そうそう、

1252
00:46:19,140 --> 00:46:21,720


1253
00:46:21,720 --> 00:46:24,480
あなたが話に持ち込んだ非常に素晴らしい類似点を感じました。それは、

1254
00:46:24,480 --> 00:46:28,040
ローカルに接続されたユニットにより、

1255
00:46:28,040 --> 00:46:30,960
モデルが

1256
00:46:30,960 --> 00:46:33,780
畳み込み

1257
00:46:33,780 --> 00:46:35,640
制約とパターンを構造的に具現化できるようになり、それが

1258
00:46:35,640 --> 00:46:37,500
これらのパターンの発生につながりました。そして、

1259
00:46:37,500 --> 00:46:41,339
同様に、ああ、ドーラルが存在しました。

1260
00:46:41,339 --> 00:46:45,680
彼らは経路探索の

1261
00:46:45,680 --> 00:46:48,359
制約を正しく持っていたので、

1262
00:46:48,359 --> 00:46:50,280


1263
00:46:50,280 --> 00:46:53,760
これらのアクションや

1264
00:46:53,760 --> 00:46:56,819
ポリシーのヒューリスティックやスパース性を

1265
00:46:56,819 --> 00:46:59,579
関節の運動探索のように考えるのは興味深いことです。最終的には、

1266
00:46:59,579 --> 00:47:02,339


1267
00:47:02,339 --> 00:47:04,980
関節を動かすには 2 つの相互に反対の方法があり

1268
00:47:04,980 --> 00:47:07,079
、その構成性が理解されるようになります。

1269
00:47:07,079 --> 00:47:09,119
関節全体の学習は、

1270
00:47:09,119 --> 00:47:10,680
低いレベルで固定された後は、より高いレベルで学習できる

1271
00:47:10,680 --> 00:47:14,480
ため、世界の実際の制約に基づいているだけでなく、特に潜在的に何かを埋め込むアクションを通じて、一般化するための非常に魅力

1272
00:47:14,480 --> 00:47:17,599


1273
00:47:17,599 --> 00:47:20,460
的でニッチに関連した方法です。 非常に

1274
00:47:20,460 --> 00:47:23,819


1275
00:47:23,819 --> 00:47:25,740


1276
00:47:25,740 --> 00:47:27,720


1277
00:47:27,720 --> 00:47:29,460


1278
00:47:29,460 --> 00:47:31,380
単純です、

1279
00:47:31,380 --> 00:47:33,599
そうです、いいえ、それは間違いなく真実だと思います、それは

1280
00:47:33,599 --> 00:47:36,599
本当に良い点です、ええと、もし

1281
00:47:36,599 --> 00:47:38,339
あなたが自分の行動自体から来る制約を持っているなら、

1282
00:47:38,339 --> 00:47:40,500
それは

1283
00:47:40,500 --> 00:47:42,839


1284
00:47:42,839 --> 00:47:44,819
あなた

1285
00:47:44,819 --> 00:47:47,460
の潜在的な空間を構築するのを助けるのに非常に有益です、そして私はそう思う、私は

1286
00:47:47,460 --> 00:47:48,480
一つのことを思う 私が言いたかったのは、

1287
00:47:48,480 --> 00:47:49,980


1288
00:47:49,980 --> 00:47:50,700


1289
00:47:50,700 --> 00:47:52,740
ステファノ・

1290
00:47:52,740 --> 00:47:55,500
フージーの一種の表現幾何学に関する研究のようなことを思い出させてくれたものがあって、それが

1291
00:47:55,500 --> 00:47:58,859


1292
00:47:58,859 --> 00:48:01,920


1293
00:48:01,920 --> 00:48:04,440


1294
00:48:04,440 --> 00:48:08,099
システムの与えられた理解をどのように一般化できるかということをどのように決定するかということです。そして、これらの一連のアクティビティがどのようなものであるかを理解していただければと思います。

1295
00:48:08,099 --> 00:48:11,880


1296
00:48:11,880 --> 00:48:14,520


1297
00:48:14,520 --> 00:48:16,079
線形分類器で分離可能または高度に並列分離可能であれば、本質的に

1298
00:48:16,079 --> 00:48:18,839


1299
00:48:18,839 --> 00:48:20,700
一般化ができるようになり、

1300
00:48:20,700 --> 00:48:23,099
これらのタイプのバイアスを課したり、

1301
00:48:23,099 --> 00:48:25,040


1302
00:48:25,040 --> 00:48:27,000
アクションによって課される制約を通じて潜在的に

1303
00:48:27,000 --> 00:48:28,740
このような何かを生み出したり、

1304
00:48:28,740 --> 00:48:32,040
ある種の結果を引き起こしたりしていると思います

1305
00:48:32,040 --> 00:48:33,660
表現幾何学がより良くなり、

1306
00:48:33,660 --> 00:48:35,220
これは構成性などにあらゆる種類の利点をもたらします

1307
00:48:35,220 --> 00:48:36,660


1308
00:48:36,660 --> 00:48:39,359
はい、私たちの一般化ですので、

1309
00:48:39,359 --> 00:48:41,760
それは素晴らしいポイントです クールです はい、非常に

1310
00:48:41,760 --> 00:48:43,440
興味深い分野です わかりました

1311
00:48:43,440 --> 00:48:45,960
ライブチャットからのいくつかの質問を読みます

1312
00:48:45,960 --> 00:48:48,420
愛の進化は、モデリング錯視の学習

1313
00:48:48,420 --> 00:48:52,260
に関する実用的または観察された制限を書きました

1314
00:48:52,260 --> 00:48:55,579


1315
00:48:58,800 --> 00:49:00,420
コミュニティは

1316
00:49:00,420 --> 00:49:03,060
恐怖症ではありません あなたには視線の中心がありません

1317
00:49:03,060 --> 00:49:05,940
それからあなたには

1318
00:49:05,940 --> 00:49:07,940


1319
00:49:08,339 --> 00:49:11,460
時間のようなものもあります つまりほとんどの畳み込み

1320
00:49:11,460 --> 00:49:13,260
ニューラルネットワークのことです 私はこの種の

1321
00:49:13,260 --> 00:49:15,599
リカレントニューラルネットワークを使用していますが、時間は

1322
00:49:15,599 --> 00:49:18,420
それほど明確に定義されていません これらのモデルでは、

1323
00:49:18,420 --> 00:49:20,220


1324
00:49:20,220 --> 00:49:23,400
錯覚実験を受けている人間の連続時間設定と同じように、

1325
00:49:23,400 --> 00:49:24,720


1326
00:49:24,720 --> 00:49:25,319


1327
00:49:25,319 --> 00:49:27,480


1328
00:49:27,480 --> 00:49:30,359
人間またはほとんどの物事として、

1329
00:49:30,359 --> 00:49:33,300


1330
00:49:33,300 --> 00:49:35,940
あなたの視線の移動位置と

1331
00:49:35,940 --> 00:49:38,220
利益が依存しているという事実のこれら 2 つの組み合わせだと思います。 あなたが

1332
00:49:38,220 --> 00:49:40,140
特定の領域に注目しているように、多くの

1333
00:49:40,140 --> 00:49:42,780
認知テストを行っているので、これを、視線をどこに動かすかを学習するような一種のアクションとして考えることができることを学ぶための

1334
00:49:42,780 --> 00:49:46,560
モデルがあれば、非常に役立つと思います。

1335
00:49:46,560 --> 00:49:48,540


1336
00:49:48,540 --> 00:49:50,760


1337
00:49:50,760 --> 00:49:52,980


1338
00:49:52,980 --> 00:49:54,420
可能な限り単純なもので、錯覚をモデル化するのに非常に役立ちます。つまり、

1339
00:49:54,420 --> 00:49:56,220


1340
00:49:56,220 --> 00:49:58,859
私にとっては、

1341
00:49:58,859 --> 00:50:00,720
認知科学の

1342
00:50:00,720 --> 00:50:02,940
実験や錯覚についての論文を読んで、

1343
00:50:02,940 --> 00:50:05,160
この

1344
00:50:05,160 --> 00:50:07,560
データセットを自分のデータセットに入れてもいいのではないかと考えるようなものです。 モデルを作成してテストすると、

1345
00:50:07,560 --> 00:50:08,579
ほとんどの場合、答えはノーです。

1346
00:50:08,579 --> 00:50:10,619
なぜなら、

1347
00:50:10,619 --> 00:50:12,900
周りを見渡したり、視野が制限されたりするようなモデルを持っていないからです。

1348
00:50:12,900 --> 00:50:14,660


1349
00:50:14,660 --> 00:50:16,619
そう、それは制限の 1 つだと思います、

1350
00:50:16,619 --> 00:50:19,680
もう 1 つは、

1351
00:50:19,680 --> 00:50:20,579


1352
00:50:20,579 --> 00:50:22,920
そうですね、 実験ははるかに

1353
00:50:22,920 --> 00:50:24,900
複雑なので、それは実用的な制限の1つです

1354
00:50:24,900 --> 00:50:27,359


1355
00:50:27,359 --> 00:50:30,240
すごい素晴らしい答えは、数字の回転である

1356
00:50:30,240 --> 00:50:33,920
テーブルの上で回転する文字が書かれた紙を思い出させます 中心

1357
00:50:33,920 --> 00:50:36,780
窩形成

1358
00:50:36,780 --> 00:50:38,460
と錯視のダイナミクスに関する素晴らしい点

1359
00:50:38,460 --> 00:50:40,079
あなたは実際に錯視について言及したと思います

1360
00:50:40,079 --> 00:50:42,599
ただし、

1361
00:50:42,599 --> 00:50:43,980
一般化の文脈で述べた、

1362
00:50:43,980 --> 00:50:46,859
二次元の画面上で回転しているものは

1363
00:50:46,859 --> 00:50:49,500
三次元に一般化されず

1364
00:50:49,500 --> 00:50:52,920
、次元の崩壊

1365
00:50:52,920 --> 00:50:55,559
または縮小は立方体

1366
00:50:55,559 --> 00:50:58,619
投影の錯覚と立方体と図形の回転

1367
00:50:58,619 --> 00:51:01,880
錯視の基礎であると述べましたが、それはあなたの画面上にあり

1368
00:51:01,880 --> 00:51:05,280
、そこにあります シルエット、または

1369
00:51:05,280 --> 00:51:07,260


1370
00:51:07,260 --> 00:51:09,839
生成という曖昧な刺激があり、

1371
00:51:09,839 --> 00:51:12,359
臨界に近い、または

1372
00:51:12,359 --> 00:51:13,680
退化モデルの分岐があるため、何らかの

1373
00:51:13,680 --> 00:51:17,160
方法でそれを表現できるため、

1374
00:51:17,160 --> 00:51:19,920
多くの切り替え幻想は、

1375
00:51:19,920 --> 00:51:22,020


1376
00:51:22,020 --> 00:51:23,819
画像の平坦性

1377
00:51:23,819 --> 00:51:26,280
と制限と一般化に基づいています。

1378
00:51:26,280 --> 00:51:28,740
それはそれによって明らかになります、

1379
00:51:28,740 --> 00:51:32,460
そうです、ええ、私はどこかにオー・イエスさえあると思います、

1380
00:51:32,460 --> 00:51:34,859
申し訳ありませんが、いくつかの仕事があるか、

1381
00:51:34,859 --> 00:51:35,880


1382
00:51:35,880 --> 00:51:37,619


1383
00:51:37,619 --> 00:51:39,480


1384
00:51:39,480 --> 00:51:42,000
ナンシー・ケンでさえ

1385
00:51:42,000 --> 00:51:45,119
最近側壁であったように、人々は頭の中に三次元のイメージを持っていると主張することもできますが、そして、ええ、私はそうではあり

1386
00:51:45,119 --> 00:51:48,119
ません 私たちのモデルは

1387
00:51:48,119 --> 00:51:50,460
それが超巨大ではないことを知っていますが、とにかく

1388
00:51:50,460 --> 00:51:53,700
それはかなり興味深いです、ええと、

1389
00:51:53,700 --> 00:51:56,160
アップサイクルクラブからチャットで

1390
00:51:56,160 --> 00:51:58,200
彼らは、単一のニューロンだけを

1391
00:51:58,200 --> 00:52:00,000


1392
00:52:00,000 --> 00:52:02,160
望んでいると想像した場合とほぼ同じ効果的に学習できれば称賛の言葉を書きました

1393
00:52:02,160 --> 00:52:03,780
すべての例でアクティブです

1394
00:52:03,780 --> 00:52:06,540
えー、あなたのモデルは

1395
00:52:06,540 --> 00:52:08,579
データセットの設計

1396
00:52:08,579 --> 00:52:10,980
か何かを記憶しようとしています

1397
00:52:10,980 --> 00:52:12,180
えー、そして十分な

1398
00:52:12,180 --> 00:52:14,940
容量がないでしょう、それでそうですね、その

1399
00:52:14,940 --> 00:52:18,359
レベルのスパース性を調整することは確かに

1400
00:52:18,359 --> 00:52:22,200
重要な要素だと思います、

1401
00:52:22,200 --> 00:52:25,020
そしてええ フレームワークを 2 倍にしている場合、尤度を見ると、

1402
00:52:25,020 --> 00:52:26,220


1403
00:52:26,220 --> 00:52:28,579
通常、これは

1404
00:52:28,579 --> 00:52:32,040
尤度自体と自動的にバランスが取れます。

1405
00:52:32,040 --> 00:52:33,000
ええと、

1406
00:52:33,000 --> 00:52:34,380
生成モデリングを行っていない場合は、

1407
00:52:34,380 --> 00:52:35,760
スパース性ペナルティが発生するだけです。

1408
00:52:35,760 --> 00:52:38,460
調整する必要があります。 このパラメータは、

1409
00:52:38,460 --> 00:52:40,980


1410
00:52:40,980 --> 00:52:43,380


1411
00:52:43,380 --> 00:52:45,599


1412
00:52:45,599 --> 00:52:47,040
フィードバック

1413
00:52:47,040 --> 00:52:50,400
ループ ノイズや敵対的な入力などのさまざまな要因によりネットワークが不安定または混乱する Armina での暴走動作を明確にするためだけです。

1414
00:52:50,400 --> 00:52:52,380


1415
00:52:52,380 --> 00:52:54,180
ええと、これを繰り返し設定のように見ていなかったと思います。

1416
00:52:54,180 --> 00:52:55,859


1417
00:52:55,859 --> 00:52:58,500
フィードバック ループが発生するでしょう

1418
00:52:58,500 --> 00:52:59,460


1419
00:52:59,460 --> 00:53:01,800
が、そうですね、敵対的な

1420
00:53:01,800 --> 00:53:04,319
例があなた

1421
00:53:04,319 --> 00:53:07,800
のスパース度のレベルによって潜在的に影響を受けるのはわかりました。

1422
00:53:07,800 --> 00:53:09,119


1423
00:53:09,119 --> 00:53:10,859
興味深い点は、

1424
00:53:10,859 --> 00:53:12,660


1425
00:53:12,660 --> 00:53:16,040
例を共有する場合にどのような影響を受けやすいか、それとも影響を受けにくいかということです。よくわかりません。

1426
00:53:16,040 --> 00:53:19,440


1427
00:53:19,440 --> 00:53:21,720
完全に接続された高

1428
00:53:21,720 --> 00:53:23,579
次元モデルは徐々に

1429
00:53:23,579 --> 00:53:25,619
小さくなり、

1430
00:53:25,619 --> 00:53:27,540
一般にトレードオフが何であるかはよく理解されています。

1431
00:53:27,540 --> 00:53:29,760


1432
00:53:29,760 --> 00:53:34,079
計算が簡単になり、モデルが小さくなり、

1433
00:53:34,079 --> 00:53:36,420
基本的なグラフがより明確に

1434
00:53:36,420 --> 00:53:39,119
表現できるようになります。また、

1435
00:53:39,119 --> 00:53:41,339
他のすべてのトレードオフもあります。

1436
00:53:41,339 --> 00:53:43,680
一般化の偽陽性と陰性はオフになります

1437
00:53:43,680 --> 00:53:45,720
が、それが反復的な適合

1438
00:53:45,720 --> 00:53:47,579
プロセスである

1439
00:53:47,579 --> 00:53:49,760
理由です。そのため、

1440
00:53:49,760 --> 00:53:52,800
スパース化アプローチの

1441
00:53:52,800 --> 00:53:55,700
バランスはどのようになっているのでしょうか。

1442
00:53:56,700 --> 00:53:59,520


1443
00:53:59,520 --> 00:54:01,619


1444
00:54:01,619 --> 00:54:03,660


1445
00:54:03,660 --> 00:54:07,079
特定の入力に関連するスパース化を決定するために、AIC、Bic、またはその他のモデルフィッティングアプローチを使用しません。

1446
00:54:07,079 --> 00:54:09,780
なげな

1447
00:54:09,780 --> 00:54:11,940
わ回帰のように、どのように判断するか、

1448
00:54:11,940 --> 00:54:14,339
どのくらいの量をどのように知るか、どのようにしきい値を設定するか、どの

1449
00:54:14,339 --> 00:54:17,220
程度

1450
00:54:17,220 --> 00:54:19,559
のスパースを正しく設定するか、これについては優れた文献がたくさんあると思いますが、

1451
00:54:19,559 --> 00:54:22,440


1452
00:54:22,440 --> 00:54:25,319
ハーバード大学でもそれらを好む人がいます。 何

1453
00:54:25,319 --> 00:54:29,520
人かは今取り組んでいます、ああ、これをやりました、そのような

1454
00:54:29,520 --> 00:54:31,559


1455
00:54:31,559 --> 00:54:34,380
展開された

1456
00:54:34,380 --> 00:54:36,780
反復的分散化ネットワークのようなもの

1457
00:54:36,780 --> 00:54:37,800
で、リカレントニューラル

1458
00:54:37,800 --> 00:54:40,380
ネットワークのようなもので、反復的に分散化することで、

1459
00:54:40,380 --> 00:54:41,940


1460
00:54:41,940 --> 00:54:45,780
赤負けや、グループアクティブグループのような

1461
00:54:45,780 --> 00:54:47,520
スポーツアクティベーションのようなものが得られることを示すことができます

1462
00:54:47,520 --> 00:54:48,960
ここで使用しているのは、

1463
00:54:48,960 --> 00:54:52,859
えー、この設定で使用しているのは、

1464
00:54:52,859 --> 00:54:55,859
これを持っているだけです、えー、

1465
00:54:55,859 --> 00:54:59,280
この T 変数の構造

1466
00:54:59,280 --> 00:55:04,079
で、上部に Z があり、えー、

1467
00:55:04,079 --> 00:55:07,859


1468
00:55:07,859 --> 00:55:09,119


1469
00:55:09,119 --> 00:55:11,579
下部の U 変数の合計によって何らかの効果でゲートされます。

1470
00:55:11,579 --> 00:55:13,200
多分私は、

1471
00:55:13,200 --> 00:55:16,500
これが結合しているマトリックスであることをあまり明確にしていなかったのですが、それが

1472
00:55:16,500 --> 00:55:18,359
グループを定義しているものなので、グループ

1473
00:55:18,359 --> 00:55:20,400
の代表チームを定義しています。これは、

1474
00:55:20,400 --> 00:55:22,380
これらすべての U を一緒に接続するものです。つまり、

1475
00:55:22,380 --> 00:55:23,940
アイデアは、

1476
00:55:23,940 --> 00:55:27,540
ここにあるように、他の 1 つがすべてである場合です。

1477
00:55:27,540 --> 00:55:31,740
例: 使用するすべてが

1478
00:55:31,740 --> 00:55:35,520
指定された t に対してアクティブではない場合、

1479
00:55:35,520 --> 00:55:38,280
またはすべての varios が指定された t に対してアクティブである場合、

1480
00:55:38,280 --> 00:55:41,040


1481
00:55:41,040 --> 00:55:42,780
分母が

1482
00:55:42,780 --> 00:55:44,339
非常に大きくなり、

1483
00:55:44,339 --> 00:55:47,160
スパース性が誘発されるため、その t 変数は非常に小さくなります。 えー、それは制約を

1484
00:55:47,160 --> 00:55:49,260
満たすことです。

1485
00:55:49,260 --> 00:55:51,839
すべて小さい U のセットがある場合、

1486
00:55:51,839 --> 00:55:54,480
その制約は満たされ、

1487
00:55:54,480 --> 00:55:57,180
Z は一種の Express 自体が許可されます。

1488
00:55:57,180 --> 00:56:00,240
それが、

1489
00:56:00,240 --> 00:56:02,880
えー、アクティブ化に関しては、まあまあ達成されます。

1490
00:56:02,880 --> 00:56:06,180
これは、これらの

1491
00:56:06,180 --> 00:56:07,020
2 つの

1492
00:56:07,020 --> 00:56:09,300
ケールの発散項によって引き起こされます。ここでは、

1493
00:56:09,300 --> 00:56:12,960
各 unhc がガウス分布からどれだけ離れているかを示しています。

1494
00:56:12,960 --> 00:56:15,180
そして、この

1495
00:56:15,180 --> 00:56:16,980
スチューデント T 変数の構築を通じて、

1496
00:56:16,980 --> 00:56:20,880


1497
00:56:20,880 --> 00:56:23,040
これらの

1498
00:56:23,040 --> 00:56:24,839
ガウス分布だけから疎な事前分布を効果的に構築しています。 行為の

1499
00:56:24,839 --> 00:56:27,599
条件 実際の目的 えー、

1500
00:56:27,599 --> 00:56:28,920
最適化している条件と目的は、ある程度スパース性を高める

1501
00:56:28,920 --> 00:56:31,619
これら 2 つの KL 条件であり

1502
00:56:31,619 --> 00:56:34,020
、これは

1503
00:56:34,020 --> 00:56:36,359


1504
00:56:36,359 --> 00:56:39,000
デコーダを通じて尤度条件と自動的にバランスが取られるため、

1505
00:56:39,000 --> 00:56:41,220
調整中の項はありません

1506
00:56:41,220 --> 00:56:42,839
が、

1507
00:56:42,839 --> 00:56:44,280
これらのさまざまなエンコーダーのパラメーターを学習し、

1508
00:56:44,280 --> 00:56:48,200
失敗した緊急事態を分析しています。

1509
00:56:48,540 --> 00:56:49,920
ああ、わかり

1510
00:56:49,920 --> 00:56:52,859
ました、視線と錯覚について書いたデイブ・ダグラスからの別の質問、

1511
00:56:52,859 --> 00:56:55,500


1512
00:56:55,500 --> 00:56:59,160


1513
00:56:59,160 --> 00:57:01,920
乳児の定常状態に関する研究は

1514
00:57:01,920 --> 00:57:04,260
分離できますか おそらくより高いレベルの概念的な恒常性について、低レベルの幻想に関連して、

1515
00:57:04,260 --> 00:57:06,140


1516
00:57:06,140 --> 00:57:09,980


1517
00:57:13,619 --> 00:57:15,720
ええと、あなたは

1518
00:57:15,720 --> 00:57:18,300
現在の種類のアーキテクチャを読むことができますか、

1519
00:57:18,300 --> 00:57:23,520
幼児における

1520
00:57:23,520 --> 00:57:26,640
定常性に関する研究は、ええと認知的定常性が分離される可能性があります、

1521
00:57:26,640 --> 00:57:31,619
ええ、おそらく私はそうではありません、私は専門家ではなく

1522
00:57:31,619 --> 00:57:33,900
、実際には非常に精通しさえしていません

1523
00:57:33,900 --> 00:57:35,460


1524
00:57:35,460 --> 00:57:37,859
オブジェクトの永続性の研究や幼児

1525
00:57:37,859 --> 00:57:40,260
や恒常性の研究もそうですが、ニューラル ネットワーク アーキテクチャで

1526
00:57:40,260 --> 00:57:42,300
研究するのは非常に興味深いものになると思います。

1527
00:57:42,300 --> 00:57:44,339


1528
00:57:44,339 --> 00:57:46,740
それは、

1529
00:57:46,740 --> 00:57:48,780
私が

1530
00:57:48,780 --> 00:57:51,780
ここでこの行をモデル化しようとしていた、うーんと幻想を伴うアイデアの一部でした。

1531
00:57:51,780 --> 00:57:53,099
これについて非常に明確だったかどうかはわかりませんが、

1532
00:57:53,099 --> 00:57:55,380
上の行は入力であり、

1533
00:57:55,380 --> 00:57:57,359
事実上単一フレームの入力をブロックしているようなもので

1534
00:57:57,359 --> 00:58:00,119
、ネットワークがそのものがまだそこにあることをエンコードしているかどうかを確認したかったのです

1535
00:58:00,119 --> 00:58:03,240


1536
00:58:03,240 --> 00:58:05,400
そのフレームが

1537
00:58:05,400 --> 00:58:07,680
なくなっても、神経活動からオブジェクトの存在をデコードすることはできますか。

1538
00:58:07,680 --> 00:58:10,020


1539
00:58:10,020 --> 00:58:11,819
それから、その後、前とは

1540
00:58:11,819 --> 00:58:13,920


1541
00:58:13,920 --> 00:58:15,780
わずかに異なる位置にバーが表示されたという事実から、動きについて何を推測しているのでしょうか？

1542
00:58:15,780 --> 00:58:18,480


1543
00:58:18,480 --> 00:58:20,520
フレームが消えました ええと、

1544
00:58:20,520 --> 00:58:22,559


1545
00:58:22,559 --> 00:58:25,200
そうですね、これには間違いなく複数のレベルがあると思います、ええと、

1546
00:58:25,200 --> 00:58:27,240


1547
00:58:27,240 --> 00:58:29,160
いくつかはおそらくはるかに

1548
00:58:29,160 --> 00:58:33,180
低いレベルであり、

1549
00:58:33,180 --> 00:58:35,880
おそらく長期的なオブジェクトの永続性は

1550
00:58:35,880 --> 00:58:37,380
かなり高いレベルになると思います、

1551
00:58:37,380 --> 00:58:39,059


1552
00:58:39,059 --> 00:58:39,900
ええと、

1553
00:58:39,900 --> 00:58:41,760


1554
00:58:41,760 --> 00:58:44,640
猫を使った実験を思い出します 昔は、彼らは1日1

1555
00:58:44,640 --> 00:58:47,280


1556
00:58:47,280 --> 00:58:49,020
時間以外は暗闇の中で子供たちを育てていたようなもので、彼らは子供

1557
00:58:49,020 --> 00:58:51,000
たちを垂直の世界か水平の世界に置くか、

1558
00:58:51,000 --> 00:58:53,160
あるいは水平の線

1559
00:58:53,160 --> 00:58:57,299
か垂直の線しか見えなかったのですが、

1560
00:58:57,299 --> 00:58:59,880
皮質の組織が

1561
00:58:59,880 --> 00:59:02,819
彼らと同じように変化するのがわかります。 受容力が低い 彼らがこれまで

1562
00:59:02,819 --> 00:59:04,200
水平線を見たことがない場合、それは

1563
00:59:04,200 --> 00:59:06,420
水平線であり、

1564
00:59:06,420 --> 00:59:07,980
棒を手に取り、顔の前で振って、

1565
00:59:07,980 --> 00:59:09,599
棒が

1566
00:59:09,599 --> 00:59:11,220
水平であれば、彼らは何もしないだけで、

1567
00:59:11,220 --> 00:59:12,900
垂直である場合、彼らは棒をたたいているだけです

1568
00:59:12,900 --> 00:59:14,460
彼らはそれを打とうとしていて、

1569
00:59:14,460 --> 00:59:15,900
文字通り顔の前でバーをする必要がないようなものなので、

1570
00:59:15,900 --> 00:59:18,420
その

1571
00:59:18,420 --> 00:59:20,700
場合、これは

1572
00:59:20,700 --> 00:59:24,260
低レベルの欠陥と視覚が

1573
00:59:24,260 --> 00:59:26,940
何らかの錯覚に寄与している証拠だと思うので、

1574
00:59:26,940 --> 00:59:28,980
私は 確かに

1575
00:59:28,980 --> 00:59:30,660
乳児にもその側面があるかもしれないと思います、

1576
00:59:30,660 --> 00:59:32,839


1577
00:59:33,540 --> 00:59:36,420
あなたが提起した非常に興味深い点の1つは、

1578
00:59:36,420 --> 00:59:40,339


1579
00:59:40,339 --> 00:59:43,619
小さなものが中間である生物と無生物の多様体でした、

1580
00:59:43,619 --> 00:59:45,480


1581
00:59:45,480 --> 00:59:49,140
それは何を表しているのでしょうか、

1582
00:59:49,140 --> 00:59:52,319
それとも扱いやすいからですか、

1583
00:59:52,319 --> 00:59:55,619
それともそれは 昆虫かもしれないし、

1584
00:59:55,619 --> 00:59:57,839


1585
00:59:57,839 --> 01:00:01,380
風だけで去っていくかもしれない何かかもしれないし、それは正しいと思います

1586
01:00:01,380 --> 01:00:04,380
か？

1587
01:00:04,380 --> 01:00:08,280
ええと、これはタリア・コンクルのような人の作品です、私は

1588
01:00:08,280 --> 01:00:11,280
この組織を発見した人だと思います、

1589
01:00:11,280 --> 01:00:12,240


1590
01:00:12,240 --> 01:00:14,880
そして彼らは

1591
01:00:14,880 --> 01:00:16,440
それを解明しようとしました 私はそう思いません、私は

1592
01:00:16,440 --> 01:00:19,500
これを誤解しているかもしれないので、

1593
01:00:19,500 --> 01:00:21,359
それを三者構成組織と呼んでいる人にはそれについての彼女の著作を読むことをお勧めします

1594
01:00:21,359 --> 01:00:23,880
が、私の

1595
01:00:23,880 --> 01:00:25,319
記憶が正しければ、

1596
01:00:25,319 --> 01:00:27,900
彼らはなぜ

1597
01:00:27,900 --> 01:00:30,780
この組織が存在するのか、そしてその

1598
01:00:30,780 --> 01:00:33,180
証拠について多くの追跡調査を行ったはずです。

1599
01:00:33,180 --> 01:00:35,700
これらのオブジェクトの曲率や

1600
01:00:35,700 --> 01:00:37,440
オブジェクトが見える距離のようなもの、

1601
01:00:37,440 --> 01:00:40,260
あるいは

1602
01:00:40,260 --> 01:00:43,319
アニメーションオブジェクトのようなもの、あるいはもっと曲線的なもの、または

1603
01:00:43,319 --> 01:00:45,599


1604
01:00:45,599 --> 01:00:46,859
実際の答えが何であるかに関係なく、

1605
01:00:46,859 --> 01:00:48,720


1606
01:00:48,720 --> 01:00:51,720
これらのオブジェクトの同様の特性から派生したさまざまな仮説がたくさんありました

1607
01:00:51,720 --> 01:00:54,000
おそらく高レベルのプロパティよりも中レベルまたは

1608
01:00:54,000 --> 01:00:56,280
低レベルのプロパティのほうがそうかもしれませんが、

1609
01:00:56,280 --> 01:00:57,599


1610
01:00:57,599 --> 01:00:59,339


1611
01:00:59,339 --> 01:01:01,500
あなたが言ったようなオブジェクトとの相互作用のようなものが

1612
01:01:01,500 --> 01:01:04,920
分離を引き起こすのか、

1613
01:01:04,920 --> 01:01:06,119


1614
01:01:06,119 --> 01:01:09,540
それともこれらのオブジェクトの一般的な形状が正確に解決されているかどうかはまだわかりません。

1615
01:01:09,540 --> 01:01:12,299
ほとんどのものと同じように、上記

1616
01:01:12,299 --> 01:01:13,980
すべての組み合わせのようなものだと

1617
01:01:13,980 --> 01:01:16,980
思いますが、

1618
01:01:16,980 --> 01:01:18,480
このモデリングの観点から興味深いのは、

1619
01:01:18,480 --> 01:01:19,859


1620
01:01:19,859 --> 01:01:21,480


1621
01:01:21,480 --> 01:01:24,059
これは画像データセット自体からの相関統計に基づいてのみトレーニングされている

1622
01:01:24,059 --> 01:01:26,819


1623
01:01:26,819 --> 01:01:28,799
ため、相互作用がないことです。

1624
01:01:28,799 --> 01:01:32,760
アニマシーの概念はありません、ええと、これは

1625
01:01:32,760 --> 01:01:34,140
実際にはイメージネット上でモデルをトレーニングしているだけです、

1626
01:01:34,140 --> 01:01:37,859
犬、猫、ボートなどの画像だけですが、

1627
01:01:37,859 --> 01:01:40,020
それでもこのタイプの組織化が達成されている

1628
01:01:40,020 --> 01:01:41,640
ため、ある種の

1629
01:01:41,640 --> 01:01:42,540


1630
01:01:42,540 --> 01:01:44,940
意味的特徴がある可能性があります。

1631
01:01:44,940 --> 01:01:46,740
私たちはイメージを持っています。

1632
01:01:46,740 --> 01:01:48,359


1633
01:01:48,359 --> 01:01:51,000
ボート、犬、他の 20 品種の

1634
01:01:51,000 --> 01:01:53,640
犬を分類できるネットワークですが、

1635
01:01:53,640 --> 01:01:55,920


1636
01:01:55,920 --> 01:01:57,900
下位レベルのフィニッシュ統計にも対応する可能性がある場合は、

1637
01:01:57,900 --> 01:01:59,400


1638
01:01:59,400 --> 01:02:02,780
そうですね、わかりません、そうですね、

1639
01:02:03,960 --> 01:02:07,500
挑発的なアナロジーは、

1640
01:02:07,500 --> 01:02:10,380


1641
01:02:10,380 --> 01:02:12,900
手書きの Mnist の翻訳シフトだったと思います

1642
01:02:12,900 --> 01:02:14,819
認識設定

1643
01:02:14,819 --> 01:02:18,000


1644
01:02:18,000 --> 01:02:20,160


1645
01:02:20,160 --> 01:02:22,740
現在存在する並進シフトとは何ですか 3 ピクセルの

1646
01:02:22,740 --> 01:02:24,780
例は、LLM か何かに対する何らかの即時操作された攻撃です。

1647
01:02:24,780 --> 01:02:27,540


1648
01:02:27,540 --> 01:02:29,099
特殊文字が

1649
01:02:29,099 --> 01:02:32,640
挿入されているか、または

1650
01:02:32,640 --> 01:02:35,160
画像上のオーバーレイです。

1651
01:02:35,160 --> 01:02:37,020
それを検出してください、

1652
01:02:37,020 --> 01:02:39,359
それで、それらの課題は何だと思いますか、

1653
01:02:39,359 --> 01:02:42,839
そしてそれを追求できる方法は何ですか、

1654
01:02:42,839 --> 01:02:44,940


1655
01:02:44,940 --> 01:02:47,520
ええ、絶対に、つまり、

1656
01:02:47,520 --> 01:02:48,420


1657
01:02:48,420 --> 01:02:50,099
これは私が考えていた方法のようなものだと思います、

1658
01:02:50,099 --> 01:02:52,680
これらの対称変換のようなものです、

1659
01:02:52,680 --> 01:02:53,760
ええと、

1660
01:02:53,760 --> 01:02:55,799
言語モデルについて考えている場合は、

1661
01:02:55,799 --> 01:02:57,420


1662
01:02:57,420 --> 01:02:58,500


1663
01:02:58,500 --> 01:03:00,240
単語を同義語か

1664
01:03:00,240 --> 01:03:03,780
何かに置き換えるのと同じような対称性の変換を想像してみてください。えー、私たちに与えられた文は

1665
01:03:03,780 --> 01:03:06,000
まったく同じ意味ですが、

1666
01:03:06,000 --> 01:03:07,380
突然モデルが

1667
01:03:07,380 --> 01:03:09,299
まったく異なる反応を示すようになります。ええと、

1668
01:03:09,299 --> 01:03:11,240


1669
01:03:11,240 --> 01:03:15,359
言語間の翻訳のようなものです。

1670
01:03:15,359 --> 01:03:16,799
変換のタイプでは、

1671
01:03:16,799 --> 01:03:19,440


1672
01:03:19,440 --> 01:03:21,960


1673
01:03:21,960 --> 01:03:24,900
私たちにとっては入力の根本的な意味が保持されますが、モデルにとっては

1674
01:03:24,900 --> 01:03:26,220
完全に異なって見えます。

1675
01:03:26,220 --> 01:03:28,380


1676
01:03:28,380 --> 01:03:29,940
これらのタイプの変換に関して予測可能な方法で動作するモデルが欲しいと考えています。

1677
01:03:29,940 --> 01:03:32,160


1678
01:03:32,160 --> 01:03:35,040
なぜなら、人間はこれらの変換以来非常に予測どおりに行動すると思うからです。

1679
01:03:35,040 --> 01:03:37,319
変換、そして

1680
01:03:37,319 --> 01:03:39,920
AI システムを扱うとき、私たちは AI システムも

1681
01:03:39,920 --> 01:03:43,200
そのように動作することを期待します。それが、

1682
01:03:43,200 --> 01:03:45,480


1683
01:03:45,480 --> 01:03:47,339
これらのシステムと対話する際に多くの課題を引き起こす原因の一部であると思います。

1684
01:03:47,339 --> 01:03:49,460
そして、これを使って、

1685
01:03:49,460 --> 01:03:52,500
それを大まかに生意気なデモンストレーションをしてみました。

1686
01:03:52,500 --> 01:03:54,960
クマや四角

1687
01:03:54,960 --> 01:03:58,440
など、私たちは

1688
01:03:58,440 --> 01:04:00,480


1689
01:04:00,480 --> 01:04:02,220
このような単純なことができると期待しています。なぜなら、ほとんどの人間は

1690
01:04:02,220 --> 01:04:04,020
できると思うからですが、実際にはそうではありません。

1691
01:04:04,020 --> 01:04:05,460
これが重要なシナリオであると想像すると、

1692
01:04:05,460 --> 01:04:07,740
これが期待されますが、それは大きな

1693
01:04:07,740 --> 01:04:08,700
問題です

1694
01:04:08,700 --> 01:04:11,099
。 それをどう処理すればいいですか それはそうですね それが

1695
01:04:11,099 --> 01:04:12,720
私が

1696
01:04:12,720 --> 01:04:15,859
探しているもののようなものだと思います

1697
01:04:16,319 --> 01:04:18,420
私の

1698
01:04:18,420 --> 01:04:22,280
方向性は

1699
01:04:22,280 --> 01:04:26,940
もっとシンプルで、これらの創発的なものを生み出す

1700
01:04:26,940 --> 01:04:29,460


1701
01:04:29,460 --> 01:04:31,380
ニューラルネットワークアーキテクチャまたは

1702
01:04:31,380 --> 01:04:33,180
アルゴリズムのボトムアップビルディングブロックのように見えると思います

1703
01:04:33,180 --> 01:04:35,760
構造的

1704
01:04:35,760 --> 01:04:37,680
特性、そしてそれは

1705
01:04:37,680 --> 01:04:39,839


1706
01:04:39,839 --> 01:04:41,579
私たちがすでに持っているものの上に何かを構築するよりもはるかに一般化可能な方法だと思います、

1707
01:04:41,579 --> 01:04:43,140


1708
01:04:43,140 --> 01:04:43,920
うーん、それははるかに

1709
01:04:43,920 --> 01:04:45,839


1710
01:04:45,839 --> 01:04:47,819
うまくスケールできるものであり、脳の機能によりよく一致するものだと思います

1711
01:04:47,819 --> 01:04:50,299


1712
01:04:50,760 --> 01:04:52,740
非常にクールな一種の実装上の

1713
01:04:52,740 --> 01:04:54,740
質問は何ですか

1714
01:04:54,740 --> 01:04:57,000
これを実行するだけの計算要件、または

1715
01:04:57,000 --> 01:04:59,400


1716
01:04:59,400 --> 01:05:01,920


1717
01:05:01,920 --> 01:05:04,380
これらの亜種を実行する学生や研究者の日常はどうなるか、テラバイトの

1718
01:05:04,380 --> 01:05:07,020
データを使用し、大規模な計算を使用するのか、

1719
01:05:07,020 --> 01:05:08,940
それともこれは個人で実行できるものなのか

1720
01:05:08,940 --> 01:05:11,880
ラップトップ

1721
01:05:11,880 --> 01:05:13,980


1722
01:05:13,980 --> 01:05:17,099
今日私が紹介したものはほとんどすべてローカルで実行できると思います。つまり、

1723
01:05:17,099 --> 01:05:20,040
このようなものは非常に簡単に実行

1724
01:05:20,040 --> 01:05:20,760


1725
01:05:20,760 --> 01:05:22,319


1726
01:05:22,319 --> 01:05:24,299
できます。つまり、

1727
01:05:24,299 --> 01:05:25,980


1728
01:05:25,980 --> 01:05:27,420
さまざまなことを訓練したり実験したりしたい場合は、ラップトップで実行できると思います。

1729
01:05:27,420 --> 01:05:30,119
かなり遅いので、私は Nvidia 1080 の

1730
01:05:30,119 --> 01:05:33,359
ようなほとんどすべてのものを実行するような商用 GPU をお勧めします。

1731
01:05:33,359 --> 01:05:35,640


1732
01:05:35,640 --> 01:05:38,819
かなり古くて安いですが、12

1733
01:05:38,819 --> 01:05:41,099
ギガの RAM か何かを搭載しており、

1734
01:05:41,099 --> 01:05:43,140
これらのモデルには十分すぎるほどです 4

1735
01:05:43,140 --> 01:05:46,440
ギガバイトの RAM

1736
01:05:46,440 --> 01:05:48,839
一部の人々が奇妙だと思うことの1つは、私は

1737
01:05:48,839 --> 01:05:51,480
mnistのようなものでほとんどの実験を行っているため、

1738
01:05:51,480 --> 01:05:54,780


1739
01:05:54,780 --> 01:05:57,299
小さくて音声でトレーニングできるため、32 x 32ピクセルの画像になっているということです。そうしたい

1740
01:05:57,299 --> 01:06:00,000
場合は私の実験

1741
01:06:00,000 --> 01:06:02,460
、または無限に実行してください このようなことをやりたいのですが、

1742
01:06:02,460 --> 01:06:03,540
これらははるかに

1743
01:06:03,540 --> 01:06:05,640
複雑です、このハミルトニアン ダイナミック

1744
01:06:05,640 --> 01:06:08,160
スイートでは、

1745
01:06:08,160 --> 01:06:09,780
複数の GPU で実行されるより大きなモデルを扱うので、

1746
01:06:09,780 --> 01:06:12,240
ここではクラスターを使用して

1747
01:06:12,240 --> 01:06:14,220
これらのタイプのモデルを実行します

1748
01:06:14,220 --> 01:06:16,140
が、私はほとんどのことを言うと思います GPU を備えた 1 台の

1749
01:06:16,140 --> 01:06:18,920
マシンで十分すぎる、

1750
01:06:18,920 --> 01:06:21,680
あるいはコラボ ノートブックのような

1751
01:06:21,680 --> 01:06:24,539
ものでも、

1752
01:06:24,539 --> 01:06:26,520
imagenet で何かをトレーニングしたい場合はより

1753
01:06:26,520 --> 01:06:29,940
複雑になり、

1754
01:06:29,940 --> 01:06:33,660
理想的には GPU が少なくとも 1 つ必要になりますが、私はそう

1755
01:06:33,660 --> 01:06:35,160
しません。 大規模なものがたくさんあります

1756
01:06:35,160 --> 01:06:37,200
が、確かに興味深いと思います

1757
01:06:37,200 --> 01:06:39,960
し、そこでできることは間違いなくたくさんあります

1758
01:06:39,960 --> 01:06:42,539
が、この種のより

1759
01:06:42,539 --> 01:06:45,480
単純な、またはより基本的な質問のいくつかについては、

1760
01:06:45,480 --> 01:06:47,760
それを何と呼びたいのかわかりませんが、

1761
01:06:47,760 --> 01:06:52,500
小さなマシンとは何ですか？ 素晴らしくて速い とてもクール

1762
01:06:52,500 --> 01:06:54,660
役に立つ

1763
01:06:54,660 --> 01:06:58,140
大丈夫 応用能動推論シンポジウム

1764
01:06:58,140 --> 01:07:00,780
でのバート・デブリーズのコメントを思い出したデイブのコメントを読みます。

1765
01:07:00,780 --> 01:07:02,880


1766
01:07:02,880 --> 01:07:05,520


1767
01:07:05,520 --> 01:07:08,099


1768
01:07:08,099 --> 01:07:09,780


1769
01:07:09,780 --> 01:07:11,579
精度をあまり必要としない採餌や制御状況に費やす労力や ATP を減らすことが望ましいということについてです。 これを聞いたら分からないかもしれませんが、

1770
01:07:11,579 --> 01:07:13,859
デブリーズ教授は、

1771
01:07:13,859 --> 01:07:17,520
可変精度モデルについて、そしてそれを

1772
01:07:17,520 --> 01:07:19,440


1773
01:07:19,440 --> 01:07:21,059


1774
01:07:21,059 --> 01:07:23,039
一般化や実際の構造

1775
01:07:23,039 --> 01:07:25,020
コーストレーニングのさまざまな機能を有効にするためにどのように使用できるか、また

1776
01:07:25,020 --> 01:07:27,059
計算要件の削減などについて言及し

1777
01:07:27,059 --> 01:07:29,880
ました。これを導入する方法について何か提案はありますか?

1778
01:07:29,880 --> 01:07:31,980
能動的

1779
01:07:31,980 --> 01:07:33,900
推論と理論の区別 どのような種類の

1780
01:07:33,900 --> 01:07:37,760
実験がこれを解明できるでしょうか

1781
01:07:38,400 --> 01:07:40,680
ああ、そうそう、それは私にはありません それについて言えるほど

1782
01:07:40,680 --> 01:07:42,660
の知性は私にはないと思います

1783
01:07:42,660 --> 01:07:46,619


1784
01:07:46,619 --> 01:07:48,740


1785
01:07:51,359 --> 01:07:53,460
それは完全に正直です ええと、非常に興味深い質問です

1786
01:07:53,460 --> 01:07:56,220
直感が何かを生み出すと思うからです

1787
01:07:56,220 --> 01:07:58,260
そうですね、

1788
01:07:58,260 --> 01:08:00,000


1789
01:08:00,000 --> 01:08:01,980


1790
01:08:01,980 --> 01:08:05,160
あなたがモデル内でエンコードしているとき、

1791
01:08:05,160 --> 01:08:07,079
またはモデル内で一般的に計算を行っているときの精度の可変率を私が正しく理解しているかどうかについて話しているのは私にとっては非常に理にかなっています

1792
01:08:07,079 --> 01:08:07,740


1793
01:08:07,740 --> 01:08:09,420
[音楽]

1794
01:08:09,420 --> 01:08:11,539
ええと、

1795
01:08:13,200 --> 01:08:17,040
それは何らかの形であなたの将来のパフォーマンスに影響を与えます

1796
01:08:17,040 --> 01:08:19,080
何らかの

1797
01:08:19,080 --> 01:08:22,259
エネルギー貯蔵庫との関係だと思います、そして、

1798
01:08:22,259 --> 01:08:23,580
これを能動的努力システムに構築したい場合は、

1799
01:08:23,580 --> 01:08:26,279


1800
01:08:26,279 --> 01:08:28,679


1801
01:08:28,679 --> 01:08:31,500
エージェントが内部エネルギー貯蔵庫のようなエネルギーの概念を持ち、

1802
01:08:31,500 --> 01:08:34,439


1803
01:08:34,439 --> 01:08:36,238
そう、何かが試みている、実際に具体化されたシステムが必要になります。

1804
01:08:36,238 --> 01:08:38,520


1805
01:08:38,520 --> 01:08:40,500
アクションを実行している間に保存するには、えーっと、

1806
01:08:40,500 --> 01:08:42,359
エネルギーがなくなったら、エージェントにとって何か悪いことが必要になるでしょう。そして、

1807
01:08:42,359 --> 01:08:44,759


1808
01:08:44,759 --> 01:08:47,399
おそらく、エージェントが学習しようとしているときに、出現のようなものを観察できるかもしれません。

1809
01:08:47,399 --> 01:08:48,679


1810
01:08:48,679 --> 01:08:52,040


1811
01:08:52,040 --> 01:08:55,198


1812
01:08:55,198 --> 01:08:57,479


1813
01:08:57,479 --> 01:09:00,060
より効果的に行動するには、その精度を制御する機能を与える必要があるかもしれません

1814
01:09:00,060 --> 01:09:02,520


1815
01:09:02,520 --> 01:09:04,020


1816
01:09:04,020 --> 01:09:07,080
ええ、私の専門知識から言いますが、

1817
01:09:07,080 --> 01:09:08,819


1818
01:09:08,819 --> 01:09:11,460
このスライドでは大丈夫です最初の非常に

1819
01:09:11,460 --> 01:09:14,219
クールな画像、

1820
01:09:14,219 --> 01:09:18,359
デジタルのジャクソン・ポロックのようなものです、

1821
01:09:18,359 --> 01:09:22,920
ええと、もしそうなら 入力データ サイズが単純になる

1822
01:09:22,920 --> 01:09:26,520
か、パターンの複雑さが軽減されるだけです。あるいは、

1823
01:09:26,520 --> 01:09:27,719


1824
01:09:27,719 --> 01:09:30,000
複雑さが増加した場合、この画像はどのように変化するでしょうか。

1825
01:09:30,000 --> 01:09:31,920


1826
01:09:31,920 --> 01:09:34,620
はい、それで、これらの向きの列を変更しようとしていくつかの実験をしました。

1827
01:09:34,620 --> 01:09:36,738


1828
01:09:36,738 --> 01:09:40,439


1829
01:09:40,439 --> 01:09:41,100


1830
01:09:41,100 --> 01:09:43,140
ええと、基本的には

1831
01:09:43,140 --> 01:09:44,520
モデルのパラメータを変更することができます。

1832
01:09:44,520 --> 01:09:47,100
これらの列を大きくすると、私たちが人間で見ている構造と

1833
01:09:47,100 --> 01:09:49,380
あまり似ていない構造にすることができます。

1834
01:09:49,380 --> 01:09:51,660


1835
01:09:51,660 --> 01:09:53,040
より多くの活動帯域を持たせることができます。

1836
01:09:53,040 --> 01:09:55,199


1837
01:09:55,199 --> 01:09:56,160


1838
01:09:56,160 --> 01:09:58,199
それと、あなたが言ったように、それはデータセットに依存します 本当に

1839
01:09:58,199 --> 01:10:00,540


1840
01:10:00,540 --> 01:10:03,179
単純な正弦波グレーディングのようなものを

1841
01:10:03,179 --> 01:10:05,580
入力として使用すると、次のようなものが得られます。

1842
01:10:05,580 --> 01:10:07,920
もう少し

1843
01:10:07,920 --> 01:10:11,640
回転曲線の高いエントロピーが得られます。

1844
01:10:11,640 --> 01:10:12,800


1845
01:10:12,800 --> 01:10:15,660
したがって、これらはすべて、次の創発を

1846
01:10:15,660 --> 01:10:18,000
研究したい場合には興味深いものだと思います

1847
01:10:18,000 --> 01:10:19,320


1848
01:10:19,320 --> 01:10:22,199
自然システムにおけるこの種の組織は、ええと、さまざまな設定に対して

1849
01:10:22,199 --> 01:10:24,120
異なる組織を生成するモデルがある場合、それはわかります。

1850
01:10:24,120 --> 01:10:25,860


1851
01:10:25,860 --> 01:10:28,620
それでは、どの

1852
01:10:28,620 --> 01:10:31,679
設定が私たちの観測データに最もよく一致するか、ええと、

1853
01:10:31,679 --> 01:10:32,340


1854
01:10:32,340 --> 01:10:34,679
そうですね、あなたが

1855
01:10:34,679 --> 01:10:36,540
いる場合は、その周りの組織を送信できます

1856
01:10:36,540 --> 01:10:38,520
興味はあるけど、ええと、もう

1857
01:10:38,520 --> 01:10:40,580


1858
01:10:41,580 --> 01:10:44,219
一つ、申し訳ありませんが、もう一つ、もう

1859
01:10:44,219 --> 01:10:45,659
一つ興味深い点は、

1860
01:10:45,659 --> 01:10:46,920


1861
01:10:46,920 --> 01:10:51,480
ええと、さまざまな動物と、

1862
01:10:51,480 --> 01:10:53,219
方向選択性の種類と、

1863
01:10:53,219 --> 01:10:54,659
風車の数が異なるということです。いくつかの動物には、

1864
01:10:54,659 --> 01:10:57,420
それがまったくありません。私なら、おそらくマウスだと思います

1865
01:10:57,420 --> 01:11:00,120
正しいのですが、このようなものは

1866
01:11:00,120 --> 01:11:01,800
塩とコショウの選択性と呼ばれるものなので、

1867
01:11:01,800 --> 01:11:03,480
基本的にランダムです。

1868
01:11:03,480 --> 01:11:04,679
地形方向の感度のようなものはありません。

1869
01:11:04,679 --> 01:11:06,239


1870
01:11:06,239 --> 01:11:09,300


1871
01:11:09,300 --> 01:11:10,920
ええと、異なるシステムがこれを

1872
01:11:10,920 --> 01:11:13,020
異なる方法で行うという証拠があり、その理由を理解するのは興味深いです。

1873
01:11:13,020 --> 01:11:14,760


1874
01:11:14,760 --> 01:11:17,760
非常にクールです。

1875
01:11:17,760 --> 01:11:21,300
最初に反応拡散

1876
01:11:21,300 --> 01:11:22,980
ベースと時間を思い出させます。その

1877
01:11:22,980 --> 01:11:25,739
ため、実際には、

1878
01:11:25,739 --> 01:11:30,000


1879
01:11:30,000 --> 01:11:32,840


1880
01:11:32,840 --> 01:11:35,640


1881
01:11:35,640 --> 01:11:39,360
fmri の空間的および時間的時間スケールで調べている場合のように、特定の粒度からは領域に活動がない可能性があります。

1882
01:11:39,360 --> 01:11:40,520


1883
01:11:40,520 --> 01:11:44,699
活動のポケットがある場合ですが、

1884
01:11:44,699 --> 01:11:46,380
アクティビティのポケットがそれ

1885
01:11:46,380 --> 01:11:48,480


1886
01:11:48,480 --> 01:11:52,260
よりも遅い場合、その測定値は

1887
01:11:52,260 --> 01:11:54,060
ノイズと変わらないでしょう。すべて

1888
01:11:54,060 --> 01:11:55,739
平均化されているため、

1889
01:11:55,739 --> 01:11:58,620


1890
01:11:58,620 --> 01:12:01,560


1891
01:12:01,560 --> 01:12:03,360
実際には

1892
01:12:03,360 --> 01:12:06,179
豊富なデータセットのような興味深いものがあるかもしれませんが、その場合は 1 つだけです。 何らかの理由で、

1893
01:12:06,179 --> 01:12:08,520


1894
01:12:08,520 --> 01:12:11,100
それはあなたに接続されていなかったので平均化されただけです、

1895
01:12:11,100 --> 01:12:12,420
またはこのようなものです、本当に

1896
01:12:12,420 --> 01:12:14,520
単一のトライアルレベルを使用する必要があります、

1897
01:12:14,520 --> 01:12:16,140


1898
01:12:16,140 --> 01:12:18,719
それが

1899
01:12:18,719 --> 01:12:23,100
マイクロ周波数を満たすことがわかるように、十分に高い空間分解能が必要です そして、これは

1900
01:12:23,100 --> 01:12:24,659
人々が長い間やらなかったことです、

1901
01:12:24,659 --> 01:12:25,620
特に

1902
01:12:25,620 --> 01:12:27,780
単一の選挙区の記録を行っている場合、

1903
01:12:27,780 --> 01:12:28,920
進行波は見られず、

1904
01:12:28,920 --> 01:12:30,900
振動が見られるでしょう、

1905
01:12:30,900 --> 01:12:32,219
それでマルチエレクトリックのようなものが必要です

1906
01:12:32,219 --> 01:12:34,199
配列であり、基本的に彼らは大丈夫と言っているのですが、

1907
01:12:34,199 --> 01:12:36,000
今ではこれを行うためのテクノロジーがあり、

1908
01:12:36,000 --> 01:12:37,520


1909
01:12:37,520 --> 01:12:40,080
以前には見られなかった持続性があり、

1910
01:12:40,080 --> 01:12:42,960
これは潜在的に、以前に見られていた多くのノイズの説明になる可能性があります。

1911
01:12:42,960 --> 01:12:44,400


1912
01:12:44,400 --> 01:12:46,260
進行

1913
01:12:46,260 --> 01:12:47,219
波、

1914
01:12:47,219 --> 01:12:47,760


1915
01:12:47,760 --> 01:12:51,480
そうですね、録音能力の向上により、今後やるべきことがたくさんあると思います。

1916
01:12:51,480 --> 01:12:53,880


1917
01:12:53,880 --> 01:12:56,520


1918
01:12:56,520 --> 01:12:58,739
それは非常にクールです、

1919
01:12:58,739 --> 01:13:02,940
最終的な考えや質問、または

1920
01:13:02,940 --> 01:13:06,239
この作業をどこに持っていくつもりですか、

1921
01:13:06,239 --> 01:13:08,520
ええ、いいえ、私を

1922
01:13:08,520 --> 01:13:10,140


1923
01:13:10,140 --> 01:13:11,640
アクティブなインフラストラクチャに入れてくれてありがとう

1924
01:13:11,640 --> 01:13:14,520
それはそれです、ぜひそうしたいです、

1925
01:13:14,520 --> 01:13:16,560
それはとても楽しいことだと思うので、ええ、

1926
01:13:16,560 --> 01:13:18,980
私は多分音楽を見ているのか、ええと

1927
01:13:18,980 --> 01:13:21,659
今はええと、他の

1928
01:13:21,659 --> 01:13:22,440


1929
01:13:22,440 --> 01:13:26,760


1930
01:13:26,760 --> 01:13:30,420
他のクレイジーな方向を見ています、

1931
01:13:30,420 --> 01:13:33,020
あまりクレイジーに思われたくないのですが、ええと

1932
01:13:33,020 --> 01:13:36,900
そうですね、たくさんのことを説明します それで、

1933
01:13:36,900 --> 01:13:38,580


1934
01:13:38,580 --> 01:13:40,320
私たちがニューロプスに提出したもので、

1935
01:13:40,320 --> 01:13:43,140
進行波を使って記憶を研究することが考えられています

1936
01:13:43,140 --> 01:13:45,060
えーっと、その論文は今日アーカイブで公開されたばかりです えー、

1937
01:13:45,060 --> 01:13:46,860


1938
01:13:46,860 --> 01:13:48,840
波が長期記憶のエンコードにどのように優れているのか

1939
01:13:48,840 --> 01:13:50,580
これは

1940
01:13:50,580 --> 01:13:52,100
とても興味深いと思う

1941
01:13:52,100 --> 01:13:54,120
ので、その方向に少し進めてみるかもしれません。

1942
01:13:54,120 --> 01:13:55,800


1943
01:13:55,800 --> 01:13:58,920
それは良さそうですし、

1944
01:13:58,920 --> 01:14:01,400


1945
01:14:01,400 --> 01:14:04,560


1946
01:14:04,560 --> 01:14:07,620
犬の足が動いているときでもアクティブなままのニューロンが存在するときにアクションが始まるのを見るのは

1947
01:14:07,620 --> 01:14:09,060


1948
01:14:09,060 --> 01:14:11,480
非常にエキサイティングです。

1949
01:14:11,480 --> 01:14:14,280
野球を投げてそれが終わるようなアクションシーケンスは、

1950
01:14:14,280 --> 01:14:15,900


1951
01:14:15,900 --> 01:14:18,440
そのアクションに何か影響を及ぼし続けているようなもので、

1952
01:14:18,440 --> 01:14:21,179


1953
01:14:21,179 --> 01:14:23,699
代替アクションの深い時間表現のようなもので、

1954
01:14:23,699 --> 01:14:26,159


1955
01:14:26,159 --> 01:14:29,640
変分オートエンコーダーは

1956
01:14:29,640 --> 01:14:33,179
すでに基本的にそのようなものには適しているので、

1957
01:14:33,179 --> 01:14:35,219


1958
01:14:35,219 --> 01:14:37,739
本当に感謝しています わかりました、

1959
01:14:37,739 --> 01:14:39,480
次回までありがとう、

1960
01:14:39,480 --> 01:14:43,339
本当にありがとう、さようなら


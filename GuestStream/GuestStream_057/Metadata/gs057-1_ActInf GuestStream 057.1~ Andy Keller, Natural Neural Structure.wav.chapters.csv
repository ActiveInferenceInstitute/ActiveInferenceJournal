start	end	startTime	summary	headline	gist
4050	51460	00:04	We're going to be talking about natural neural structure for artificial intelligence. There will be a presentation followed by a discussion. If you're watching live, please feel free to write questions in the live chat.	Andy Keller will talk about natural neural structure for artificial intelligence	NATIVE neural structure for artificial intelligence talk
51990	262038	00:51	Andy's goal is to bring modern artificial intelligence closer to more human like generalization. The way that we propose to do this is by integrating natural neural structure into artificial intelligence.	Modern machine learning cannot generalize beyond its training sets in the traditional sense	Bringing natural neural structure into AI
262204	552370	04:22	In this talk, I'll be focusing specifically on two types of structure which my work has studied. These are topographic organization and spatiotemporal dynamics. I believe that natural structure may be useful to achieve the structure generalization that. drives the success of the Deep Learning Revolution.	This talk focuses on two types of structure which my work has studied	Natural structure and its implications for deep learning
554810	1156760	09:14	Topographic organization is observed widely throughout the brain from primary visual cortex to higher level areas. The idea is that we can generalize the benefits of this to more abstract transformations. We settled on a generative modeling approach which can be integrated into modern deep neural network architectures.	Topographic organization is observed widely throughout the brain from primary visual cortex	Topographic Priorism and the generative model
1158250	1696078	19:18	When we train this generative model which has relatively simple group sparsity penalty in its latent space. We see that our model is learning to cluster activities together in a simulated cortical sheet according to the correlations in the data set. Maybe we can extend our model to this setting to learn, observe transformations.	Generative model has relatively simple group sparsity penalty in its latent space	Training a neural network with group sparsity penalty
1696174	2182110	28:16	A recent paper shows structured spatiotemporal dynamics to integrate into artificial neural networks. One example of that is traveling waves. And it would be interesting to study what their implications are for structured representation learning.	To integrate into artificial neural networks. One example of that is traveling waves	Spiking neural networks and structured representation learning
2183490	2609510	36:23	The research may not only improve artificial intelligence, but also how it helps us understand why our measurements of the brain look the way they do. Building these types of models can help us think about how the brain builds representational structure.	This research may help us understand why our measurements of the brain look the way	Neuroscience 3, The Brain
2610970	2922650	43:30	Aims to include not just external patterns, but also the consequences of action or world model structure with action. This has all sorts of benefits for compositionality or generalization. Has not gotten so far as to study how these structured world models in a VAE or I haven't worked.	Active inference is a very appealing way to generalize	Interaction between structured world models and neural networks
2922720	3113000	48:42	Deep learning, community uses. They're not foveated. You don't have a center of gaze, and most convolutional neural networks. Time is not as clearly defined in these models as it is. Another one is makes the experiment much more complicated.	Are there practical or observed limitations on modeling illusions? Deep learning, community uses	Deep Learning: The Illusion Model
3113370	3409240	51:53	Tuning the level of sparsity is certainly an important factor. Would you be more susceptible or less susceptible to adversary examples? Some people have done unrolled iterative sparsification networks.	Of sparsity is certainly an important factor in generative modeling	Neural Network Sparsity in Armina
3409770	3571120	56:49	Can the studies on constancies in infants be separated into lower level illusion, relative, perhaps higher level conceptual constancy? Can you read the current kind of architecture?	Can the studies on constancies in infants be separated into illusion and conceptual constancy	Neural Network Constancies in Infants
3573570	3724574	59:33	One very curious point you brought up was the animate and inanimate manifold with small things being intermediate. What does that represent? Is it because they're handleable or it might be an insect. I would bet it's some combination of all of the above.	One very curious point you brought up was the animate and inanimate manifold	The organization of the animate and inanimate manifold
3724692	3891410	1:02:04	Provocative analogy was the translational shift in the MNIST in the handwriting recognition setting. We would like to have models which behave in a predictable way with respect to these types of transformations. My direction is to look more simple, kind of like bottom up building blocks of neural network architectures.	What are the translational shifts that exist today	Transliterational Shifts in AI
3891480	4014010	1:04:51	Almost everything I presented today can be run locally. You can run you can run it on your laptop. I run pretty much everything on like Nvidia 1080s. If you want to train something on ImageNet, it gets more complicated and you need at least one GPU. A smaller machine is nice and fast.	Almost everything I presented today can be run locally on a laptop	Minimum computational requirements for MNIST and ImageNet
4014510	4148390	1:06:54	 variable precision models could be used to enable different features of generalization and actual structural course training. If you wanted to build this into an active inference system, you would need to have really an embodied system. What kinds of experiments could winkle this out?	Dave: Variable precision models could enable reduced computational requirements in active inference	Active Inference and Variable-Prism
4149070	4377730	1:09:09	The different animals and types of orientation selectivity and different numbers of pinwheels. Some animals don't have it at all. Do this differently and it's interesting to figure out why. There's a lot to be done in the future with increased abilities for recording.	It depends on the data set that you're using	MRI pinwheels and their origin
4378660	4474030	1:12:58	One thing that's coming up is studying memory with traveling waves. Would be very exciting to see action come into play when there was the neurons that stayed active even as the dog's feet were moving. Where are you going to take this work?	Where are you going to take this work next	How did the brain store memories?
4475120	4481080	1:14:35	Really appreciate it. All right, thank you. Till next time. Bye.	Really appreciate it. All right, thank you. Till next time. Bye.	Thank you for listening

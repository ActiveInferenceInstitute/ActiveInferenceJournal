start	end	sentNum	speaker	confidence	text
6570	7702	2	B	1	Hello and welcome.
7836	11986	3	B	0.99899	It's September 18, 2023, and it's active.
12018	15826	4	B	0.9953	Guest stream 57.1 with Andy Keller.
15938	21250	5	B	0.99523	We're going to be talking about natural neural structure for artificial intelligence.
21410	24594	6	B	0.99995	There will be a presentation followed by a discussion.
24642	28618	7	B	1	So if you're watching live, please feel free to write questions in the live chat.
28754	31022	8	B	0.99863	Otherwise, thank you Andy, for this.
31076	34190	9	B	0.99991	Really looking forward to it. And to you for the presentation.
36370	37854	10	C	1	Yeah, thanks so much.
38052	39006	11	C	0.99971	Thanks for having me.
39028	43314	12	C	0.75594	I'm super excited to be able to present this stuff with Active Inference group.
43352	45794	13	C	0.80984	I'm a fan and very interested, so,
45832	46530	14	C	0.96438	hopefully,
48550	51460	15	C	0.98837	get to have a good discussion and see what you guys think about it.
51990	53470	16	C	0.98	So my name is Andy.
53550	61926	17	C	0.73056	I'm finishing up my PhD, supervised by Max Welling at the University of Amsterdam, starting a postdoc at Harvard after this.
62028	73302	18	C	0.91	So I'll start out just talking about the goal of my work in general is to try to bring modern artificial intelligence closer to more human like generalization.
73446	84606	19	C	1	And so what we mean by this is maybe some sort of structured generalization or maybe more familiar to the active inference community, like a structured world model which we believe that humans have.
84708	91470	20	C	0.59	And the way that we propose to do this is by integrating natural neural structure into artificial intelligence.
92210	96130	21	C	1	So first let's define what we mean by structured generalization.
96550	103842	22	C	1	So I think it's fairly uncontroversial to say that modern machine learning generalizes beyond its training sets in the traditional sense.
103976	114146	23	C	1	So for example, even the earliest artificial neural networks, multilayer perceptrons, could be trained on data sets of images like this and achieve high accuracy.
114338	122950	24	C	0.79176	Then, when they're presented with a held out test set of images that they've never seen before, they can still classify them relatively easily with the same level of accuracy.
123030	125526	25	C	1	And this is what we typically call generalization.
125718	133194	26	C	1	However, even fairly early on it was noticed that these systems really struggle with small shifts or deformations applied to the images.
133322	142346	27	C	0.9999	For example, [unintelligible] if so, you think, why is this surprising?
142538	150338	28	C	1	And I argue it's really due to our innate ability to perform this type of structured generalization that this example is a failure of.
150424	159126	29	C	1	So for example, this shift is nearly imperceptible to us and we handle it automatically, whereas in these systems it's very clearly a major problem.
159228	171610	30	C	1	So in words, we can say that structure generalization is a generalization to some symmetry transformations of the input, or in this case, the symmetry transformation is a small shift that leaves the digit class unchanged.
171950	178474	31	C	1	So the obvious question then is, what precisely do we mean by this natural structure and why do we think that
178512	180750	32	C	0.99984	this would help us with these settings?
181570	185150	33	C	1	So first, let's talk about what we mean by natural neural structure.
185730	192458	34	C	1	One way to talk about structure or any type of bias in a system is an inductive bias.
192554	198574	35	C	1	And so an inductive bias can loosely be defined as an apriori restriction of a set of realizable hypotheses.
198622	200290	36	C	0.83888	When you're doing model selection.
200870	207734	37	C	1	More colloquially, we can call this something like before seeing any data, it's a restriction of what and how you can learn.
207852	215430	38	C	1	So very broadly, this can include anything from model class to optimization procedures or even hyperparameters.
215930	221738	39	C	1	And in some sense they really define what is possible to learn.
221824	229158	40	C	0.99	And it defines generalization in that you actually can't generalize beyond a training set without having some inductive biases.
229254	230906	41	C	0.97	And this is explained more thoroughly in.
230928	233314	42	A	0.99763	This paper by David Wolford.
233462	245506	43	C	0.94	So what we mean by natural inductive biases then is biases that stem from the restrictions and limitations that are faced by natural systems, by the nature of having to live in the real world.
245608	251410	44	C	1	For example, the brain has many efficiency constraints and physical constraints by nature of its construction.
252070	260706	45	C	1	And following this logic, then these constraints are really playing some role in our generalization abilities which currently exceed modern artificial intelligence.
260738	262038	46	A	0.97174	As we'll go into next.
262204	269078	47	C	1	So in this talk, I'll be focusing specifically on two types of structure which my work has studied.
269254	273254	48	C	0.99995	These are topographic organization and spatiotemporal dynamics.
273382	282046	49	C	1	And before I go into my work, I'll give a short example for why I believe that natural structure may be useful to achieve the structure generalization that.
282068	283360	50	A	1	I was talking about before.
284210	298070	51	C	1	So the first example comes from Fukushima's neocognitron architecture from the 1980s, which was actually built to directly address the problem of robustness to these small shifts in deformation.
298250	307746	52	C	1	So in the paper he writes about inspiration from pupil and weasel's measurements of hierarchy and pooling in order to achieve robustness to these distortions.
307858	315190	53	C	1	And so if you look at the figure he writes u sub s one, U sub C one and these stand for simple and complex cells.
315610	323606	54	C	0.83	And so this is a fairly radical approach at the time, but it really served to improve robustness and shifts that were plaguing these early artificial neural networks.
323638	334250	55	C	1	And over time these ideas were simplified and abstracted and obviously yielded the convolutional neural networks that we know today, which ultimately drove the success of the Deep Learning Revolution.
334410	340106	56	C	1	So this is really an example of a natural inductive bias which achieved structured generalization.
340218	355190	57	C	1	So for our research, it's really of utmost interest to try and understand what makes these models work so well and see if this principle can potentially be generalized to cover more abstract transformations and symmetries.
357050	361510	58	C	1	So what makes a convolution achieve this structure generalization?
362010	369234	59	C	0.99991	Intuitively, you can see this is done by applying the same filter or feature extractor at various spatial locations.
369362	374086	60	C	1	So here we see a single convolutional filter being applied at all locations of an image.
374198	384378	61	C	1	This means that no matter where your input is, whether it's kind of in the middle of the image or on the right, you'll have the exact same features, with one exception, they'll be equivalently shifted.
384554	387994	62	C	1	So mathematically, this type of a mapping is called a homomorphism.
388122	392270	63	C	0.99119	It preserves the algebraic structure of the input space and the output space.
392420	394694	64	C	0.99994	In this case, it's with respect to translation.
394842	408882	65	C	1	And at a simple level, something like that will be important to remember for the rest of this talk is that we can verify homomorphisms of our feature extractor if we can see that there's this commutation with the transformation, this commutative diagram.
409026	416360	66	C	1	And so we can write this also algebraically by showing that the feature extractor f commutes with the transformation operator T.
416890	427494	67	C	1	And basically what we want is there to be no difference between first extracting the features and then performing the transformation, or performing the transformation and then extracting the features.
427622	436078	68	C	1	So the challenges to date is we don't really know how to construct homomorphisms with respect to more complex transformations that we see in the real world.
436244	441310	69	C	1	For example, our brain is able to handle changes in lighting and season naturally.
442450	445738	70	C	1	So here we see lighting on a person's face or the change of seasons.
445754	448434	71	C	0.99995	We can tell it's the same face or the same road.
448632	451822	72	C	0.52	But we don't know how to build models which respect these transformations.
451886	457080	73	C	0.72	And so it makes us hard to build systems which handle them in a robust and predictable way.
457450	468738	74	C	1	To give an even more abstract example of what I mean by this and the potential negative repercussions of models which don't handle symmetry transformations, consider modern text to image generation programs.
468834	474710	75	C	1	So in this example, I asked Dolly Two to generate image of a teddy bear on the Moon.
474790	476554	76	C	1	And it does this incredibly well, right?
476592	478218	77	C	0.85555	Probably better than I could.
478304	482198	78	C	0.99989	It has texture fur, incredibly detailed.
482294	490910	79	C	0.99994	However, if I ask it to do something which I see as conceptually simpler, such as draw a blue cube on top of a red cube, it fails to do this.
491060	496850	80	C	0.99	And to me this seems unintuitive since the second task seems significantly easier.
497270	504174	81	C	1	But what I'm arguing is that the reason that this is surprising is precisely the same reason that the MNS translation example was surprising.
504302	517762	82	C	0.99912	There is this symmetry transformation happening here, namely the transformation between these complex objects of a teddy bear and the Moon and these simple objects of cubes which we intuitively expect the network to be able to handle and respect.
517906	519474	83	C	1	And we see that it doesn't.
519602	537280	84	C	0.99	So just like how Fukushima's work showed that these natural structure of hierarchy and pooling of our visual system are effective for making generalizations to small transformations, I argue that potentially higher level structure may be necessary to fix these abstract generalization problems.
538130	552370	85	C	0.85	And so the question then that I'm studying and that I'm asking is what might this structure be and how do we implement this in an artificial neural network architecture that can actually be used for performing computation?
554810	560950	86	C	1	So, to begin to answer that, I'll jump into my first line of work on topographic organization.
562250	569218	87	C	1	So topographic organization is observed widely throughout the brain from primary visual cortex to higher level areas.
569314	576202	88	C	1	And it can very loosely be described as this property that neurons which are close to one another tend to respond to similar things.
576336	587626	89	C	0.99988	For example, on the left we show the color coded preference of each neuron in the Macaque primary visual cortex as a response to oriented lines and we see this smoothly varying set of selectivities.
587818	595810	90	C	0.99997	Another type of organization is known as retinotopic organization where nearby neurons in the visual cortex tend to respond to nearby receptive fields.
596310	599758	91	C	0.99988	However, this organization isn't limited to these low level features.
599854	605794	92	C	0.98043	It extends to more complex features such as those present in faces or objects or places.
605922	615030	93	C	0.99	And this relates to the so called functionally specific areas of the brain such as the fusiform face area FFA and the perihepocampal place area PPA.
615370	639440	94	C	0.94	So in this work, the main idea again is that perhaps this topographic organization in some sense which is intimately related to the convolution operation and Fukushima's architecture we can maybe generalize the benefits of this to more abstract transformations in other words, learn how to build more complex homomorphisms that we can't do analytically right now.
640770	660898	95	C	1	So just to show that we're not completely insane with this idea there is some prior work in this domain from people such as Honin Yanlaka Apohevarnan in the early ninety s and two thousand s and they studied how topographic organization may be useful for learning invariances mostly in linear models.
660994	671814	96	C	1	So the question for us when we entered this space is what is the most scalable abstract mechanism that can be leveraged from these approaches which we can integrate into modern deep neural network architectures?
671942	693730	97	C	1	And ultimately we settled on a generative modeling approach which I think might be interesting to the people in this community which then allows us to relate it more closely to topographic independent component analysis with the basic idea being that we can learn a topographic feature space by imposing a topographic prior distribution over our latent variables.
694390	712742	98	C	1	So just to give a brief background I assume most people are already familiar with this but the kind of general assumption is that the brain is a generative model and this idea in some sense can be attributed to Helmholtz from the 19th century where he said that what we see is the solution to a computational problem.
712876	717606	99	C	0.9999	Our brains compute the most likely causes from photon absorptions within our eyes.
717788	724390	100	C	1	And so as an example, if I show you this image you immediately recognize it as a globe with some curvature.
724550	728570	101	C	0.98835	However, it could just as equally be a disk with a distorted perspective on it.
728640	732262	102	C	0.85	So this is how we get optical illusions or our images.
732326	738062	103	C	1	So like this one, your brain infers that there is a cube here because of the structure but really it's just.
738116	739440	104	A	1	A flat piece of paper.
739810	745954	105	C	1	So you can think of this generative model aspect as kind of like an inverse graphics program.
746152	756002	106	C	0.99999	In the program, the abstract properties of the sphere are known the position, the size, the lighting and these are used to project the sphere to create the 2D image that is rendered.
756146	769130	107	C	1	So in effect, what Helmholtz and others are saying is that as a generative model, the brain is actually trying to invert this generative process and doing inference and infer the underlying causes of our sensations.
769550	780350	108	C	1	So the reason I'm kind of belaboring this point is that there's a lot of talk of generative models today, and I'm not necessarily just talking about generating images or pretty pictures.
780930	786080	109	C	0.73	I really want to mean a framework for unsupervised learning.
787890	792926	110	C	1	So then to get a little bit more into the details, what do I mean by a topographic prior?
793038	800740	111	C	1	So generative models are typically described as a joint distribution over observations, x and latent variables, which we'll call Z.
801670	807074	112	C	1	And this is typically factorized, or one way that this is done is factorized in terms of a prior p of Z.
807192	811990	113	C	1	And this true generative model conditional generative model p of X given Z.
812140	819142	114	C	0.99	And so one way that we can think about this is that the prior can be seen to encode relative penalties for each type of code that is produced.
819206	825180	115	C	0.99958	When we invert our generative model, this is called computing the posterior e of Z given X.
826270	838990	116	C	1	And so to develop a topographic latent space, we want to introduce some sort of a topographic prior, which this topographic ICA work showed is equivalent to something like a group sparsity penalty.
839490	843866	117	C	0.99	So people might be familiar with typical sparsity penalties from independent component analysis.
843898	847300	118	C	1	You want your activations to be sparse, meaning many of them are zero.
848390	849794	119	C	1	So that could look something like this.
849832	853342	120	C	1	You have a bunch of blue squares that are active, but most of them are not active.
853486	866482	121	C	0.82	But specifically, with a group sparsity penalty, we want these priors to assign lower probability to these distributed sparse activations and higher probability to these grouped, densely packed representations.
866546	872620	122	C	1	You can also think of this like a higher penalty when things are spread out, a lower penalty when things are closer together.
873470	877818	123	C	1	So again, this can be written abstractly like this.
877904	883598	124	C	1	But I want to make clear that each one of these squares here represents kind of a neuron in our model.
883684	885770	125	C	1	And they're organized in this 2D grid.
885850	890190	126	C	1	So when we're talking about grouping, we really mean grouping in that 2D topology.
891170	911030	127	C	1	So one thing that's really interesting and kind of important is that these priors don't just give us topographic organization, but they've also been noted by people or studied by people like Erosimicelli and Bruno Olshausen to actually fit the statistics of natural data better, specifically natural images.
911530	920842	128	C	0.93615	They've shown that using this type of a prior, you actually get a sparser set of activations, meaning that the prior fits the true generative process a little bit better.
920976	927130	129	C	0.98	And as we're aware, the brain has a high degree of sparsity and this is believed to be very relevant for efficiency.
928670	940560	130	C	0.99	So to get a little bit more into the details, to implement this type of a group sparse prior, we use a hierarchical generative model and this is basically introduced by some of the topographic ICA work.
941330	949714	131	C	1	The idea is that you have a higher level latent variable U which simultaneously regulates the variance of multiple lower level variables T.
949832	951890	132	C	1	And this is how we get group sparsity.
952470	960402	133	C	0.99998	Then to get topographic organization, you can have multiple of these latent variables U slightly overlapping with their fields of influence.
960466	967654	134	C	0.99	So their neighborhoods, we can call them, and this will give you this smooth correlation structure you're after.
967772	969398	135	C	1	So get the intuition for this.
969484	979274	136	C	0.93	You see that this variable T over down on the bottom here is not getting any input from this U on the top, but it is sharing a U variable with this T in the middle.
979392	984826	137	C	1	So it's like they're sharing variance, they're sharing some components with their neighbors, but not all components.
984858	989200	138	C	1	And that's really due to this local connectivity of these higher level variables U.
990770	996046	139	C	1	So to keep it simple about how we use this generative model, let's go back to a single U variable.
996158	1009110	140	C	1	And the challenge in this type of an architecture which made it difficult for many years is how do you infer the approximate posterior over these intermediate latent variables in this hierarchical architecture?
1009530	1011266	141	C	0.95175	This is not super straightforward.
1011378	1015298	142	C	1	So prior works have used heuristics developed for linear models.
1015394	1020198	143	C	1	And in our work we found that this really didn't extend to modern neural network architectures.
1020374	1027130	144	C	1	So really our insight is to leverage a factorization, a specific reprometerization of this distribution.
1027870	1044866	145	C	1	And so this reprimmatrization specifically is achieved by defining the prior to be what's known as a Gaussian scale mixture, meaning that our conditional distribution of T given U is actually a normal distribution where the variance is defined by this variable U.
1044968	1054606	146	C	0.99	And for certain choices of U, this distribution is indeed sparse and encompasses a range of distributions such as Laplacians and student T distributions.
1054718	1063974	147	C	1	One way of defining it is a Gaussian scale mixture admits a particular repromaturization in terms of independent gaussian random variables Z and U.
1064092	1078726	148	C	0.99	So specifically, then we see that this T variable, which was originally fairly complex, is actually just a product of a bunch of gaussian random variables which we now know how to work with much more efficiently in generative models.
1078838	1089786	149	C	1	And specifically what we're going to do is so that we can actually get approximate posteriors for U and Z separately and then do a deterministic combination of them in order to compute our topographic.
1089818	1090590	150	A	0.39438	Variable T.
1090660	1092480	151	A	1	And this is much easier to do.
1093090	1112066	152	C	1	So, without going into too many details, the method that we decide to use is what's known as a variational auto encoder, which leverages techniques from variational inference to derive a lower bound on the likelihood, allowing us to parameterize these approximate posteriors with powerful nonlinear deep neural networks and optimize them with gradient descent.
1112258	1114918	153	C	0.99315	This is going to be familiar to the active inference community.
1115084	1119078	154	C	1	But really what we've done is instead of having a single encoder and decoder.
1119094	1124582	155	C	0.99423	As is typical Bayes, we now have two encoders, one for U and one for Z separately.
1124726	1130510	156	C	0.99	And then we combine them in this deterministic manner to construct our topographic T variable.
1130850	1143958	157	C	0.99767	If you see that this is actually the construction of a student's T distribution from Gaussians and then we do this before decoding and then maximize the likelihood.
1143994	1145182	158	A	1	Of the data altogether.
1145326	1154994	159	C	1	So this is the elbow, the evidence lower bound abound on the likelihood of the data and is actually very similar to the variational free energy that is.
1155032	1156760	160	A	0.99999	Used in the active inference community.
1158250	1168774	161	C	1	So with these details out of the way, what's really interesting is what happens when we train this generative model which has relatively simple group sparsity penalty in its latent space.
1168972	1171626	162	C	1	And we want to look at kind of what it's learning in terms of.
1171648	1173462	163	A	0.99983	Its organization of features.
1173606	1175994	164	C	1	And first we start with the simplest possible data set.
1176032	1179930	165	C	1	We have a black background with white squares at random XY locations.
1180270	1196782	166	C	0.99995	If we train our auto encoder with this group sparsity penalty on it and then we look at the weight vectors of our decoder which we're plotting in blue here, again organized in this 2D grid, we see that indeed they learn to be organized according to spatial location.
1196926	1207990	167	C	1	So this can be seen as similar to convolutional receptive fields where the receptive field of each neuron is really given by the kind of inputs at its location.
1208970	1219126	168	C	1	And this makes sense intuitively from the group sparsity perspective since for any given region, which we highlight, like in yellow here, the filters in a given group are much more highly correlated.
1219158	1222918	169	C	0.99997	They have these overlapping receptive fields than other random locations.
1223014	1234910	170	C	1	So essentially we see that our model is learning to cluster activities together in sort of a simulated cortical sheet according to the correlations in the data set.
1235060	1245150	171	C	1	So instead of in convolution where you're actually doing weight tying and you're manually specifying I want to copy this weight everywhere, you can maybe think of this as like approximate weight tying.
1245910	1250098	172	C	1	And really we're learning this from the correlation structure of the data set itself.
1250264	1257006	173	C	0.99	And just to give a little bit more of a biological inspiration for this, we know that retinotopy is present in the brain.
1257038	1261890	174	C	0.99997	This is an example of retinotopy and the Macaque visual cortex.
1262050	1273130	175	C	0.98	And you can see if you show the Macaque an image like this, it gets projected into this topology preserving space actually on the surface of the cortex.
1273710	1283120	176	C	1	So the idea is that topographic organization and even learn topographic organization is preserving the input correlations of our data set.
1285570	1291210	177	C	0.99	And potentially this may be beneficial for generalizing these ideas a little bit further.
1291290	1300478	178	C	1	So like I said at the beginning, it would be even better if we could just learn something more than just convolution, maybe more complicated equivalences.
1300654	1303026	179	C	1	So how do we do that?
1303208	1310454	180	C	1	One thing that's clear in natural intelligence is that we don't exist in this world of IID frames, right?
1310492	1314226	181	C	0.99999	We exist in a world of continuous sequences of transformations.
1314338	1320022	182	C	1	So maybe we can extend our model to this setting to learn, observe transformations.
1320166	1322570	183	C	0.99898	This is the idea of temporal coherence.
1323230	1329594	184	C	1	So what would happen if we just simply extended our previous framework over the time dimension, right?
1329632	1338958	185	C	1	So instead of just grouping, saying we want our neurons to be group sparse in terms of spatial extent on the cortex, we actually want them to be group sparse over time.
1339044	1343506	186	C	0.99981	Meaning that if one set of neurons is active now, we want that same set of neurons to be active into.
1343528	1344820	187	A	0.71	The future as well.
1346470	1353614	188	C	0.99979	If we kind of intuitively think about this, we see that this is actually more encouraging invariance than equivariance.
1353662	1360918	189	C	0.85	A way to understand this is we're saying we want the same neurons to be active constantly, but the input transformation is changing, right?
1361004	1363510	190	C	1	The feet of this little fox are moving.
1363660	1371754	191	C	1	So if the same neurons are coding for the same thing over and over again, but the feet are moving, those neurons are going to learn to be invariant to the motion of that leg.
1371792	1373260	192	A	0.67	Of this dog, for example.
1373790	1375580	193	C	1	So instead is that.
1377390	1379100	194	A	1	I went the wrong way here.
1381090	1388526	195	C	1	So instead, our insight was that this group Sarcity could instead be shifted with respect to time.
1388628	1398142	196	C	0.99	So this would mean that sequentially shifted sets of activations would be encouraged to activate together, and then our latent space would really be structured with respect to the observed transformations.
1398286	1407000	197	C	1	So you can see here that rather than the same set of neurons being active at all time steps, it's really a sequentially permuted set of neurons that we're grouping together in this sparse way.
1408010	1419240	198	C	1	And then this allows us to model different observations over time, but they're still connected in terms of learning a transformation and preserving this correlation structure of the input data set.
1419930	1425402	199	C	0.99	So if we put this together into our topographic BAE architecture, you can get something that looks like this.
1425456	1427702	200	C	1	You see that we have an input sequence.
1427766	1434158	201	C	0.99857	We're again encoding a Z variable and then multiple U variables in the denominator here.
1434324	1439278	202	C	1	And then each one of these U variables is shifted kind of like we were showing before.
1439364	1442266	203	C	0.99999	In order to achieve this shift equivalent.
1442298	1443706	204	A	0.99966	Structure that we're looking for.
1443828	1449374	205	C	0.99999	When we combine these in this student T product distribution, we get a single latent variable.
1449422	1451938	206	C	0.95302	This is now our topographic latent variable T.
1452104	1457990	207	C	1	And now that we have this known structure in our latent space, you can think of it like a structured world model.
1458140	1460422	208	C	1	We know how to transform this latent space.
1460476	1466850	209	C	0.9614	In this case, it's by permuting these activations around these circles, doing like a cyclic roll, a cyclic shift.
1467010	1470998	210	C	0.99997	We know that this is going to correspond to our learned input transformations.
1471174	1478950	211	C	0.76	And we can verify that by saying, okay, what if I continue this input transformation, the true transformation in the data set, which is a rotation.
1479110	1485210	212	C	1	And then I compare that with how I've done my role in my latent space by moving my activations around in my brain.
1485370	1488830	213	C	1	And then we decode and we see that we get the exact same thing.
1488980	1495490	214	C	0.84	And so this is demonstrating this commutivity property that I was talking about before for verifying homomorphism.
1496790	1503962	215	C	1	And so to measure this a little bit more quantitatively, we can measure what's called an equivalence loss.
1504046	1518314	216	C	1	So this is really the quantification of this difference between our rolled capsule activation, our rolling in our head, versus watching the rolling unfold, watching the transformation unfold before us.
1518432	1524358	217	C	0.96	So we see that topographic VAE achieves significantly lower equivalent error.
1524454	1535914	218	C	0.99929	This bubble VAE is what I was talking about before, where it's learning invariance, so it doesn't have the shift operation and then the traditional VAE kind of has no notion of organization or temporal.
1535962	1537950	219	A	0.6554	Component, so it performs very poorly.
1538690	1542378	220	C	0.99983	In addition to this, we see that the model is a better generative model of sequences.
1542474	1548434	221	C	0.99944	It just gets a lower negative log likelihood on the data set.
1548472	1551806	222	C	1	So it's better able to model this data set because it has a notion.
1551838	1553650	223	A	1	Of the structure of the transformations.
1556310	1559234	224	C	0.99244	We can test this on multiple different transformation types.
1559282	1576518	225	C	0.76813	On the top row we're showing the true transformation, we hold out these grayed out images and then on the bottom row we encode and then we just kind of roll our activations around and we keep decoding to see what the model has learned as the current transformation that's being observed.
1576614	1582860	226	C	1	And we see that it can basically perfectly reconstruct these elements of the sequence that it's never seen before.
1583630	1590366	227	C	0.99973	Additionally, with images that are from the test set that it's never seen before, simply because it knows what the transformation is that it's currently encoding and it.
1590388	1592290	228	A	0.99999	Can generalize that to new examples.
1593990	1597342	229	C	1	So the takeaway from this part is really topographic organization.
1597406	1604900	230	C	0.9064	We showed that it preserved input structure and now we're showing it can potentially improve efficiency and generalization as we would hope.
1606230	1618650	231	C	1	So, finally, something that surprised us and I thought was potentially the most interesting is that these transformations that are learned by our model actually generalize the combinations of transformations that were not seen during training.
1618800	1643140	232	C	1	So, for example, despite only training on color and rotation transformations in isolation, if the model is presented with a combined color rotation transformation at test time, we see that it's able to completely model and complete these transformations perfectly through the capsule role, implying that it's learned to factorize represent to these different transformations and it can flexibly combine them at inference time.
1644630	1652290	233	C	1	So again, maybe we also don't just get efficiency and generalization, we also get some basic compositionality.
1654090	1657320	234	C	1	So let's talk about the limitations and what we could do next.
1657850	1665126	235	C	1	The main limitation is that there's a predefined transformation that we're imposing in both space and time.
1665228	1674940	236	C	1	So although we freed ourselves from group transformations and specifically like translation or rotation as is currently done in the machine learning world.
1675630	1682558	237	C	0.99999	We still have this hard coded latent role in our heads for everything we see.
1682644	1696078	238	C	0.99	And to make this a little bit more flexible, so hopefully we can model a greater diversity of transformations, we think maybe we can take inspiration from more structured spatiotemporal dynamics that are observed in the brain.
1696174	1702226	239	C	1	And so that takes us to the second part of this talk, which is spatiotemporal dynamics that we're going to try.
1702248	1704318	240	A	1	To integrate into artificial neural networks.
1704414	1706286	241	C	0.72	One example of that is traveling waves.
1706318	1707480	242	A	1	Like I show here.
1707930	1709640	243	C	1	So what do we mean by that?
1710250	1721046	244	C	0.94172	Here's a very recent paper where they used a nine tesla fMRI operating at 36 millisecond resolution to image a single slice of a rat brain under anesthesia.
1721158	1726422	245	C	0.64	And what we see is this very clearly structured spatiotemporal activity and correlations.
1726566	1733582	246	C	0.98	And these authors of the paper go on to analyze this activity in terms of the principal modes as depicted on the right.
1733716	1747694	247	C	1	So our hypothesis is that perhaps some sort of a correlation structure like this may be beneficial for structuring the representations of our model with respect to observed transformations, but in a much more flexible way than simply just a cyclic shift.
1747742	1748980	248	A	1	Like we were doing before.
1752790	1758390	249	C	1	And let me say that this is not just observed in anesthesized rats.
1759290	1765590	250	C	1	You can see these traveling waves happen in the mt cortex of awake behaving primates.
1766170	1779130	251	C	1	So for example, on the left here, they show traveling waves that actually change how likely a primate is to see a low contrast stimuli based on the phase of the wave.
1779550	1789178	252	C	0.90639	Furthermore, they show that a high contrast stimulus on the right can induce a traveling wave of activity that propagates outwards union primary visual cortex.
1789274	1793054	253	C	1	So these are really ubiquitous throughout the brain at multiple levels.
1793102	1798802	254	C	1	And it would be interesting to study what their implications are for structured representation learning.
1798856	1807186	255	C	0.99999	In our case, or generally, there is prior work which has studied these types of dynamics and they built models.
1807218	1823180	256	C	1	So on the top, these are the equations which describe a spiking neural network, which they show if you implement time delays, actually exxonal time delays between neurons, you do get these structured dynamics of traveling waves as long as your network size is large enough.
1824030	1834030	257	C	0.99999	However, as many people probably know, it's relatively challenging to train spiking neural networks of the same size and performance as deep neural networks.
1835010	1843774	258	C	0.94195	Similarly, on the bottom, another system which is significantly simpler, but perhaps too simple, is a network of coupled oscillators.
1843822	1850734	259	C	0.99999	These are known to exhibit synchrony and spatial temporal dynamics and complex patterns.
1850862	1856966	260	C	1	But this is called like a phase reduced system and doesn't quite capture the full complexity that we're interested in.
1856988	1860486	261	C	0.99	So we're looking at something that's potentially in between these two.
1860668	1870778	262	C	0.99	And what we settled on is this work, in this work is to parameterize a network of coupled oscillators slightly more flexibly than a Kuramoto model.
1870864	1871226	263	C	0.96	So this.
1871248	1882986	264	C	0.99916	Is really built on this coupled distillatory recurrent neural network of Constantine, Rouge and Nisha, where they basically took the equation which describes a simple harmonic oscillator.
1883018	1884826	265	C	0.95979	It's a second order differential equation.
1884938	1891070	266	C	0.53	The acceleration on a ball on a spring is proportional to its displacement.
1893170	1898420	267	C	1	You can add additional terms such as damping so that the oscillations slowly die out over time.
1899350	1906770	268	C	1	You can drive this oscillator with an external input to kind of counteract this damping or to give slightly more complexity to the dynamics.
1907190	1915702	269	C	1	And then furthermore, if you have many of these oscillators, you can couple them together with these coupling matrices W, as we demonstrate kind of in this picture here.
1915756	1922362	270	C	0.97	So you can really think of this network as a bunch of balls on springs and they're maybe connected to each other also by springs or elastic bands, whatever.
1922416	1927658	271	C	0.81	The couple, the Silatory recurrent neural network of Rush and Mishra with these various terms.
1927744	1931670	272	C	0.99	And this has been shown to be very powerful for modeling long sequences.
1931750	1934414	273	C	0.99985	They also mentioned they were inspired by the brain building this.
1934452	1936720	274	C	1	And there's a lot of good analysis in that paper.
1937090	1946130	275	C	0.99999	For example, they show that this has really beneficial properties with respect to vanishing gradient problems that typically happen in recurrent neural networks.
1946790	1964246	276	C	1	But if we want to look at spatiotemporal dynamics in this type of a model, it's slightly challenging because these coupling matrices here, the W's that connect each oscillator's position to one another, these are densely connected matrices like I've tried to.
1964268	1965718	277	A	0.81375	Depict on the left here.
1965884	1971270	278	C	1	So if you try to visualize the dynamics of this network, you don't see any spatial organization.
1971350	1975158	279	C	0.97301	There's no inherent topology to the latent.
1975174	1976300	280	A	0.99516	Space of this model.
1976670	1979530	281	C	1	So you can think of this like in our previous example.
1979680	1983594	282	C	1	A neuron is connected to a potentially arbitrary set of other neurons.
1983642	1986234	283	C	0.75006	Those neurons are connected to another arbitrary set of neurons.
1986282	1993694	284	C	0.93	And you'll just get oscillatory dynamics, certainly, but kind of fluctuations that don't make a lot of structured sense in our work.
1993732	2001598	285	C	0.99993	Then we thought, okay, how can we convert this more to the types of dynamics that we're interested in, this structured propagation of activity?
2001774	2015410	286	C	1	And one clear way to do that is to have a more structured connectivity matrix W, which we found is easily implemented and efficiently implemented through a convolution operation, which you can think of like a locally connected layer.
2015490	2020470	287	C	1	So instead of having every neuron connected to every neuron, neurons are just connected to their nearby neighbors.
2020630	2022474	288	C	0.99998	After training, you'll end up getting something.
2022512	2025690	289	A	0.99998	That looks like a smooth spatial temporal dynamics.
2026510	2035962	290	C	1	So to be a little bit more clear, to train this model, we take this second order differential equation that we were describing before you discretize it into two first order equations.
2036026	2039790	291	C	1	You can think of this as like numerically integrating the ode.
2040290	2048546	292	C	0.99997	We now have a velocity and then we update and we can train this model as something like an auto encoder or an autoregressive model.
2048648	2051682	293	C	1	So we take an input, we encode it to our latent space.
2051816	2056074	294	C	0.99999	Really, the input is this F of x term which acts as the driving term.
2056142	2065234	295	C	1	So it's like driving these oscillators from the bottom and then they have their own dynamics which are defined by the coupling terms, these local couplings.
2065362	2071590	296	C	1	And then at each time step we take this latent state, this wave state, and we decode to try and reconstruct the input.
2071670	2083102	297	C	0.9684	Maybe at the current time step or a future time step, we can do some analysis of these models during training to see what happens before training.
2083156	2088062	298	C	1	And after training, we can compute the phase and the velocity of the dynamics in the latent space.
2088196	2091906	299	C	0.99999	Basically, we see at the beginning of training there's no waves in our model.
2092008	2103090	300	C	1	But after training, after 50 epochs, we see that there's these smooth structured activity propagating downwards in service of this sequence modeling task that we're doing, like rotating objects.
2104070	2105794	301	C	1	So what's the benefit of this?
2105832	2111394	302	C	0.6	I mean, the whole reason I motivated this was to say we wanted to have more flexibly learned structure.
2111442	2112598	303	C	0.99999	Are we actually doing that or are.
2112604	2114150	304	A	0.99998	We just getting pretty waves?
2114970	2119986	305	C	1	So what we showed in our paper is that we really are learning some sort of useful structure.
2120018	2123446	306	C	1	And the way we showed that is, again, with something like this commutative diagram.
2123558	2138602	307	C	0.99999	If you take an input and you encode it and you get a wave state and then you propagate waves artificially in that wave state and then decode, you can observe that it's actually exactly the same as if you had just by showing a bunch of different images of different transformations.
2138666	2142250	308	C	0.96	So a lot of different digits, different features.
2142330	2148846	309	C	0.6	And we see that we get different types of wave activity in each case in order to model that different transformation.
2149038	2153806	310	C	0.99998	If we train it on different data sets as well, we similarly see more complex dynamics.
2153838	2160962	311	C	0.99996	In this case, maybe not even traveling waves or standing waves, which can be thought of as traveling waves in opposite directions.
2161026	2167050	312	C	1	So we see if we're modeling these orbital dynamics, we get these kind of smoothly moving blobs of activity in our latent space.
2167120	2182110	313	C	0.99999	If we're modeling a pendulum, we similarly get kind of complex oscillatory activity so it's preserved input structure, but additionally more flexibility than we had before, which is kind of our ultimate goal.
2183490	2195410	314	C	1	So finally, I want to talk a bit about how I think the outcome of this research may not only improve artificial intelligence, but also how it helps us understand why our measurements of the brain look the way they do.
2195480	2201998	315	C	0.98	So to give a brief example of what I mean by this, I talked a bit about before, about these and places.
2202174	2211154	316	C	0.99	So in this fantastic work with HIE Gao, we studied if our simple topographic prior, as we discussed, may be able to reproduce these same effects.
2211282	2223126	317	C	0.99	So specifically, we plot the value of this Cohen's D selectivity metric for each of our neurons with respect to a different data set of images potentially containing just faces or just objects or bodies.
2223318	2229382	318	C	1	And so we measure for every neuron, is it more likely to respond to faces or the ration emerges in the brain?
2229526	2242734	319	C	1	But I do think that it tells us that the relative organization of selectivity may at least be partially attributable to correlation statistics in the data after being passed through a highly nonlinear future extractor.
2242862	2244850	320	A	1	Such as a deep neural network.
2245350	2251262	321	C	1	So, in a similar vein, something that's interesting, there's a known what's called tripartite, or the visual stream.
2251326	2269900	322	C	1	So images of or objects are selectivity with respect to objects is organized by more abstract properties, such as animacy, is this thing alive or inanimate versus also real world object size, like what is the size of a teapot versus a car?
2270910	2277206	323	C	1	And what we see is that in humans, this selectivity is organized in this tripartite structure.
2277238	2283254	324	C	1	You typically have small objects that are in between animate and inanimate objects in terms of their selectivity.
2283382	2285694	325	C	0.98	And we see the same thing kind of happens here.
2285732	2290170	326	C	1	So these are measuring the selectivity of the same set of neurons, but with respect to these different sets of stimuli.
2290250	2294254	327	C	0.74269	We see that the small cluster is in between animate and inanimate cluster.
2294302	2297134	328	A	0.72	And again, this happens for multiple different initializations.
2297262	2301250	329	C	1	So this is something I hope we can explore a bit further for this community.
2301320	2307782	330	C	1	I think it's interesting because it's really a way of showing that we built a structured world model.
2307836	2314406	331	C	1	And potentially, this world model is beneficial for better representing real world data in.
2314428	2318700	332	A	1	A structured way, and you get lower free energy in that sense.
2319070	2331322	333	C	0.99	So by developing these models like we showed here, we may get insights into new mechanisms for how this structure emerges, including topographic organization that we never thought of before.
2331456	2347422	334	C	1	So, machine model, I was looking at the orientation selectivity of neurons, which I wasn't particularly expecting something to happen, but you're looking at kind of these waves propagate over this simulated vertical surface.
2347486	2350094	335	C	1	And I thought, okay, maybe I'm showing rotated images.
2350142	2353250	336	C	0.99997	Maybe this has some effect on the orientation selectivity.
2353670	2366514	337	C	1	And actually, if you go in and you measure the selectivity of each neuron with respect to these differently oriented lines, what you see is that it's surprisingly reminiscent of the orient paper columns that are seen in primary visual cortex.
2366562	2368866	338	C	0.99964	This is stuff going back to Hugle and Weasel.
2368898	2372806	339	C	1	And this is something that just kind of came out of this model and the fact that it has the spatial.
2372838	2375270	340	A	0.94165	Temporal structure with respect to transformation.
2375430	2387806	341	C	0.96	So, of course, this is a really coarse analogy, but I think this is an example of how building these types of models can help us think about how the brain builds representational structure and the way it's organized in a way.
2387828	2389520	342	A	0.99999	That maybe we haven't thought about before.
2391490	2394786	343	C	0.89	I think I'm not the only one who's doing this type of work.
2394888	2398580	344	C	0.74	And so I want to talk a little bit about some other people who are doing this.
2398950	2402210	345	C	0.98	So I've been talking about this equivalent structure.
2402630	2421318	346	C	0.99998	People such as James Whittington and Tim Barrons and Surge Anguli have shown recently that by introducing algebraic constraints into a learning process, in this case, it was like the motion of an agent in an environment.
2421414	2429454	347	C	0.99996	By saying, you need to preserve kind of this algebraic structure of if I move in a circle west, north, east, south, I end up back at the same point.
2429492	2435870	348	C	0.82646	Again, by introducing these types of constraints, you get the emergence of grid, cell like representations.
2436770	2450020	349	C	1	So I'd be interested to see how this idea of representational structure can help us explain maybe more than our scientific findings we're finding as well and how this relates to generative models as a whole.
2451190	2455986	350	C	0.99	And then finally, I think there's something to be said about cognitive plausibility of these models as well.
2456008	2461522	351	C	0.9999	Maybe we're not just going to be testing them from a neuroscience perspective, but also from a cognitive science perspective.
2461586	2471910	352	C	0.99973	For example, there's these Ravens Progressive Matrices on the left where you have to say which one of these images is more likely to fit in this pattern?
2472410	2481370	353	C	1	Or for example, how likely is it that this Jenga Tower falls over when you pull a specific block or with a given structure?
2481530	2497346	354	C	1	And I think these types of tests are really testing if our world models that we're building are similar to the types of models that we innately have our own common sense as humans or as beings living in a natural world.
2497528	2506894	355	C	1	And I've done some preliminary work in this direction, I think very preliminary and not nearly this complicated, but trying to model visual illusions.
2507022	2520374	356	C	1	So if you take a really simple data set of a moving bar stimuli or a static bar frame and you move it a little bit, you can see that the model will actually infer that missing frame and then actually also infer continued motion.
2520422	2526890	357	C	1	So it's like overshooting the trajectory of what the actual stimuli is providing it before correcting again.
2527040	2533338	358	C	0.54	So I think modeling illusions is certainly an interesting way to study if our world models are similar to the types.
2533354	2535630	359	A	1	Of models that we have ourselves.
2536530	2545338	360	C	1	So in conclusion, yeah, I think topographic priors we could show that they effectively learn structured representations or structured world models.
2545434	2551774	361	C	0.99997	This learned structure is flexible and adaptable to arbitrary transformations, unlike traditional equivariance.
2551902	2561510	362	C	0.98	And topographic riders can be induced statistically as we did in the topographic VAE or through dynamics like we were showing in these neural wave machine type models.
2561930	2580650	363	C	1	So to conclude, I'll leave you with this quote that I found in Fukushima's paper from 1980 I thought was pretty far ahead of its time, where he says, if we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to understanding the neural mechanism in the brain.
2581070	2583006	364	C	1	So that's kind of, I think, some.
2583028	2585200	365	A	1	Of the goals that we're going for here.
2585890	2594670	366	C	0.92	So I'll say thanks to my advisor Max, my co authors Patrick Yue, Emile Jinghe, and Yorn, and interested in discussion.
2594830	2595540	367	A	0.99974	Thanks.
2604570	2605830	368	B	0.99	All right, thank you.
2605980	2606486	369	B	0.99993	Great.
2606588	2609510	370	B	0.99999	Very interesting presentation.
2610970	2612470	371	B	0.93	A lot of places to start.
2612540	2622330	372	B	0.99999	Maybe just what brought you to this work, a little context on how you came into this work for your PhD direction.
2623950	2638970	373	C	1	Yeah, I mean, there's been studying the group that I'm in at the university has been studying structured representations from mathematical point of view for a while, and we're some of the people to models.
2639130	2641310	374	A	0.84	Or for the variational auto encoder.
2641990	2652594	375	C	0.53	And I guess what something that had always been the model that respects rotations, 2D rotations perfectly well.
2652712	2657686	376	C	1	But if we want to do 3D rotations, we can't do that because that's not a group in terms of a.
2657708	2659298	377	A	0.99987	Projection onto a 2D plane.
2659394	2661506	378	C	0.72409	You're losing information when this thing rotates.
2661538	2662680	379	A	1	Around, for example.
2664570	2666962	380	C	1	Or just any sort of natural transformations.
2667026	2675034	381	C	0.69	Like I was trying to point out at the beginning, I think it was trying to think about how the brain models natural transformations is something that these.
2675072	2694530	382	B	0.99997	Current frameworks where do you see action playing a role in terms of variational, auto encoder models that include not just external patterns, but also the consequences of action or world model structure with action.
2695750	2697074	383	C	0.99996	Right, yeah.
2697272	2698322	384	C	0.99949	No, it's a good question.
2698376	2705240	385	C	0.59	And I think active inference is effectively I think it's a good answer to that.
2706410	2715058	386	C	1	I know there are reinforcement learning frameworks that do use kind of externally trained world models.
2715154	2719466	387	C	1	So you train a VAE or something and then you use that representation in.
2719488	2721660	388	A	0.99987	Your reinforcement learning system.
2722190	2726362	389	C	1	But I think having a fully kind of a system that is a single.
2726416	2730666	390	A	0.99997	Objective with action as part of the.
2730688	2735470	391	C	0.83109	Likelihood of the data, I think that's much more elegant.
2735810	2738320	392	C	0.48079	I'm a big proponent of that.
2740050	2746114	393	C	1	I have not gotten so far as to study how these structured world models in a VAE or I haven't worked.
2746152	2746754	394	A	0.99999	On that at all.
2746792	2750466	395	A	1	But I think it would certainly be very interesting to see if having a.
2750488	2756306	396	C	1	More structured world model in a variational auto encoder would be beneficial in an.
2756328	2757366	397	A	0.99783	Active setting as well.
2757388	2758534	398	A	1	I think that would be awesome.
2758652	2764758	399	C	0.87	I mean, I think some of these examples, like, showing before, like, emergence of.
2764764	2766534	400	A	0.97587	Grid cells and things like this, maybe.
2766652	2770678	401	C	0.99	Point towards that direction of, hey, maybe the brain is doing something.
2770764	2773286	402	C	0.9915	It really obviously has a lot of structure.
2773478	2777660	403	A	0.99967	This clearly has to be useful for performing actions in some way.
2779170	2779678	404	A	0.99354	Cool.
2779764	2780062	405	C	1	Yeah.
2780116	2796798	406	B	0.99	I thought a really nice parallel that you brought in with the talk was the locally connected units enabled your models to structurally embody the convolutional constraint and pattern, and that led to these arising patterns.
2796894	2801730	407	B	1	And then analogously, there was the Doral et al.
2801880	2806722	408	B	0.99989	Where they had the path exploration constraint.
2806786	2807062	409	A	0.98969	Right.
2807116	2818694	410	B	0.55	And so then it's interesting to think about these action or policy heuristics or sparsities like a joint motor exploration.
2818742	2831066	411	B	0.88429	Eventually it becomes understood that there's like two mutually opposing ways to move a joint, and then the compositionality across joints can be learned at these higher levels once it's locked in at lower levels.
2831098	2850260	412	B	0.9	So it's a very appealing and niche relevant way to generalize because it's both based upon the actual constraints of the world, but then especially through action, potentially embedding something that's quite simple.
2851350	2851858	413	C	1	Right?
2851944	2853774	414	C	0.88	Yeah, no, I think that's definitely true.
2853832	2855080	415	C	0.98105	That's a really good point.
2856090	2858054	416	C	0.99995	If you do have constraints coming from.
2858092	2865750	417	A	0.99998	Your actions themselves, then that would be hugely beneficial for helping to structure your latent space.
2865820	2885680	418	C	0.95	And I think yeah, I guess one thing I wanted to mention there's something made me think of, like, Stefano Fousey's work on kind of the representational geometry determines how generalizable a given understanding of a system is.
2886210	2899998	419	C	1	And I think if you can understand if these sets of activities are separable or highly parallel separable with a linear classifier, essentially, then you're going to be able to do generalization.
2900094	2913314	420	C	0.86	And I think by imposing these types of biases or potentially through constraints that are imposed by action, something like this, you are yielding or kind of inducing a better representational geometry.
2913362	2917910	421	A	1	And this has all sorts of benefits for compositionality or generalization.
2919150	2920330	422	A	0.31454	It's a great point.
2920480	2920938	423	A	0.99984	Cool.
2921024	2921322	424	C	1	Yeah.
2921376	2922650	425	B	0.99994	Very interesting area.
2922720	2933790	426	B	0.96	All right, I'll read some questions from the live chat love Evolve wrote any practical or observed limitations on modeling illusions?
2938450	2939754	427	C	0.92301	Deep learning, community uses.
2939802	2940842	428	C	0.878	They're not foveated.
2940906	2952062	429	C	1	You don't have a center of gaze, and you also don't have most convolutional neural networks.
2952126	2958694	430	C	0.8288	I'm using these kind of recurrent neural networks, but time is not as clearly defined in these models as it is.
2958732	2963990	431	A	0.99999	In a continuous time setting for human undergoing an illusion trial.
2965210	2979898	432	C	1	And I think the combination of these two, of the fact that as a human, for most things, you're shifting locations and your gaze are dependent on you looking to a particular area, a lot.
2979904	2981498	433	A	1	Of cognitive science tests.
2981594	2992298	434	C	1	And so I think it would be really helpful if we had models that you can think of this as a type of action of learning where to move your gaze.
2992394	2995754	435	C	1	One of the simplest possible that would help a lot for being able to model illusions.
2995802	3007266	436	C	0.61	And just I mean, for me, it's like I read a paper of some cognitive science experiments or about some illusion, and I think of, okay, can I put this data set into my model and test it?
3007288	3009318	437	C	1	And most of the time the answer is no, because I don't have a.
3009324	3014360	438	A	1	Model that looks around or has a restricted field of view, something like that.
3015050	3016962	439	C	0.98	So, yeah, I think that's one of the limitations.
3017026	3023398	440	C	0.99976	Another one is makes the experiment much more complicated.
3023494	3025930	441	A	1	So that's one of the practical limitations.
3027310	3028058	442	C	0.83685	Wow.
3028224	3029066	443	B	0.99999	Great answer.
3029168	3033978	444	B	0.99999	Makes me think of a paper with letters rotating on a table.
3034154	3036042	445	B	0.99974	That's the digit rotation.
3036106	3039002	446	B	0.50643	Great points about the Foveation and the dynamics of the illusion.
3039066	3060130	447	B	1	I think you actually did mention an illusion, which is however, you mentioned in the generalization context, which is rotating on the two dimensional screen doesn't generalize to three dimensions, and that dimensional collapse or reduction is the basis of the cube projection illusions and cube and figure rotation illusions.
3060290	3073082	448	B	0.99012	It's on your screen and there's a silhouette or there's some ambiguous stimuli that it's near a criticality or a bifurcation in the generative model.
3073136	3076620	449	B	1	So it could represent it one way or another way.
3076990	3087920	450	B	1	And so a lot of the switching illusions are just based upon the flatness of images and the limitations in generalization that are revealed by.
3092370	3098286	451	C	0.57459	Some work sorry, there's some work where they can argue people have a kind of three dimensional.
3098318	3099390	452	A	0.99421	Image in their heads.
3099470	3105640	453	A	0.64	Like even Nancy Kenwich had her library on this recently and showing yeah, I don't know.
3106250	3108054	454	C	0.99998	Do our models have that?
3108252	3111158	455	A	0.97534	It's not super clear anyway.
3111324	3113000	456	B	1	Yeah, that's pretty interesting.
3113370	3114230	457	B	0.96	All right.
3114380	3117750	458	B	0.99977	From Upcycle club in the chat, they wrote Kudos.
3118250	3120638	459	A	0.88143	You're able to learn nearly as effectively.
3120754	3124300	460	C	0.99997	If you imagine you only want a single neuron to be active for every example.
3124670	3128394	461	C	0.71993	Your model is going to be trying to memorize the data set to some.
3128432	3132730	462	A	0.99997	Extent or something like this, and you're not going to have enough capacity.
3132810	3134974	463	C	0.98	So, yeah, I think tuning that level.
3135012	3140030	464	A	0.95	Of sparsity is certainly an important factor.
3140850	3151460	465	C	1	And when you look at the likelihood, if you're calling framework, typically this is balanced automatically with the likelihood itself.
3152870	3155506	466	C	0.8692	If you're not doing generative modeling, you just have a sparsity penalty.
3155538	3157350	467	A	0.9972	You're going to want to tune that parameter.
3158490	3159046	468	B	0.96	Okay.
3159148	3169450	469	B	0.97387	They added just to clarify runaway behavior in Armina, where the network becomes unstable or chaotic due to various factors such as feedback loops, noise, or adversarial inputs.
3172270	3186670	470	C	1	Yeah, I guess I haven't looked at this in a recurrent setting where you would get feedback loops, but I could see adversarial examples being potentially affected by your level of sparsity.
3188930	3194222	471	C	0.99	The interesting point is, would you be more susceptible or less susceptible to adversary examples?
3194366	3195380	472	A	1	I don't know.
3196390	3208790	473	B	0.97	Well, sparsification projecting from a fully connected, higher dimensional model just into progressively smaller it's pretty well understood in general what the trade offs are.
3208860	3213190	474	B	0.99447	It's easier computations, a smaller model, sparser.
3213930	3223846	475	B	1	The Bayes graph is going to be clearer to represent and then also it will have all of the other trade offs with false positive and negatives of generalizing.
3223958	3226090	476	B	1	But that's why it's an iterative fit process.
3226160	3233790	477	B	0.92	So I guess how does your sparsification approach balance?
3236610	3245650	478	B	0.995	Does it use AIC or BIC or some other model fitting approach to determine the relevant sparsification for a given input?
3246950	3256120	479	B	0.95678	How do you determine in Lasso regression, how do you threshold how many how sparse you want it to be?
3257210	3264834	480	C	0.98686	Right, yeah, I think there's a lot of good literature on this, and even so, some people like DembaBa at Harvard.
3264882	3280010	481	C	1	And some people I'm working with now have done these kind of unrolled iterative sparsification networks where it's like a recurrent neural network and iteratively sparsifies.
3280090	3281706	482	C	1	And you can show that this yields.
3281738	3283680	483	A	0.99997	Something like relus or.
3285970	3290430	484	C	0.99951	Group sparse activations like we're using here in this setting.
3291670	3311254	485	C	0.95714	It's really just by having this construction of this T variable where we have Z on top and then it's in some effect gated by the sum of U variables in the bottom.
3311292	3316482	486	C	1	So, w maybe I wasn't super clear about this is a matrix that is connecting.
3316546	3321754	487	C	0.99998	That's what defines the groups when I'm defining the group sparsity, that connects all of these U's together.
3321872	3325338	488	C	1	And so the idea is like here.
3325424	3327386	489	A	0.70218	If all of one of the other.
3327408	3341646	490	C	0.99958	Examples, if all of your U's are not active for a given T, or if all of your U's are active for a given T, that T variable is going to be very small, right?
3341668	3345022	491	C	0.99864	Because your denominator is going to be very big, and that induces sparsity.
3345166	3348206	492	C	1	So it's like constraint satisfaction.
3348398	3353818	493	C	0.73701	If you have a set of U's that are all small, then that that constraint is satisfied.
3353934	3363442	494	C	0.99	And now Z is allowed to kind of express itself and that's what then kind of achieves the sparse activation.
3363586	3368746	495	C	0.97	So this is induced by these two KL divergence terms here.
3368768	3373718	496	C	0.99942	These are saying like, how far is each U and each Z from a gaussian?
3373814	3383386	497	C	1	And then through this construction of the student T variable, we're effectively constructing a sparse prior distribution just from these Gaussians.
3383418	3389902	498	C	1	But in terms of the actual objective, the terms and the objective that we're optimizing are just these two KL terms.
3389956	3393514	499	A	0.99981	That are pushing it towards sparsity to some extent.
3393562	3396386	500	C	0.99	And this is balanced automatically with the.
3396408	3398814	501	A	0.95456	Likelihood term here through the decoder.
3398862	3402658	502	C	0.99	So we don't have terms that we're tuning, but we're learning the parameters of.
3402664	3406370	503	A	0.9999	These different encoders and then analyzing the failed urgencies.
3408490	3409240	504	B	0.97822	Cool.
3409770	3410438	505	A	0.81	All right.
3410524	3418454	506	B	0.99993	Another question from Dave Douglas, who wrote speaking of gaze and illusion.
3418582	3428170	507	B	0.99997	Can the studies on constancies in infants be separated into lower level illusion, relative, perhaps higher level conceptual constancy?
3434070	3437678	508	B	0.99078	Can you read the current kind of architecture?
3437854	3445910	509	B	0.99995	Might the studies on constancies in infants cognitive constancies, be separated?
3446650	3447880	510	C	1	Yeah, probably.
3448250	3457366	511	C	0.83418	I'm not an expert or actually even very familiar with object permanency studies and.
3457388	3460018	512	A	0.99913	Infants and constancy stuff, but I think.
3460044	3464054	513	C	0.99987	That would be incredibly interesting to study in neural network architectures.
3464102	3471114	514	C	1	And that was kind of some of the idea with this illusion that I was trying to model down here with this line.
3471232	3478186	515	C	0.97	I don't know if I was super clear about this, but the top row is the input, and we're effectively like blocking the input for a single frame.
3478298	3485598	516	C	1	And I wanted to see, does the network kind of encode that the thing is still there when that frame is gone?
3485684	3487458	517	C	1	Can I still decode the presence of.
3487464	3489250	518	A	1	The object from the neural activity?
3489750	3495318	519	C	1	And then what is it also inferring about the motion because of the fact that it saw the bars at a.
3495324	3499720	520	A	0.99919	Slightly different location than from before after the frame is gone.
3501130	3516186	521	C	0.91	So, yeah, I think there's definitely multiple levels to it where some would probably be much lower level and maybe long term object permanency, I would guess would.
3516208	3518140	522	A	1	Be significantly higher level.
3519870	3521530	523	C	0.99999	It just makes me think of those.
3521600	3524462	524	A	0.55045	Experiments with cats back in the day.
3524516	3528654	525	C	0.99999	Where it's like they raised them in darkness except for an hour a day.
3528692	3545490	526	C	0.99999	They put them in vertical world or horizontal world where they only saw horizontal lines or vertical lines, and you can see the organization of their cortex changes, like they have less receptivity to horizontal lines if they've never seen horizontal lines before.
3545640	3551142	527	C	1	And then you take a stick and you wave it in front of their face, and if the stick is horizontal, they do nothing.
3551196	3552758	528	C	0.99614	If it's vertical, they're swatting at it.
3552764	3554134	529	A	0.99845	They'Re trying to hit it, it's like.
3554172	3555638	530	C	1	They just literally don't hunzle bar in.
3555644	3556582	531	A	0.99985	Front of their face.
3556716	3566902	532	C	1	So I think in that case, then, this is evidence of a low level efficiency in vision contributing to some sort of an illusion.
3566966	3568714	533	C	1	So I think, yeah, there could certainly.
3568752	3571120	534	A	0.96752	Be some aspect to that in infants as well.
3573570	3586210	535	B	1	One very curious point you brought up was the animate and inanimate manifold with small things being intermediate, right?
3586360	3588130	536	B	0.99996	What does that represent?
3589670	3600360	537	B	0.85	Or is it because they're handleable or it might be an insect or it might be something that might move away just with wind or what does that say?
3601530	3615594	538	C	0.98343	Right, yeah, so this is work by Talia Konkel, I think was the one who discovered this organization and they tried to figure it out.
3615712	3620670	539	C	1	I might be getting this wrong, so I recommend people to read her work on that.
3620740	3622762	540	C	0.99997	They call it Tripartite organization.
3622906	3651270	541	C	1	But if I remember correctly, they did a lot of follow up work on why there's this organization and some evidence of curvature of these objects and kind of like the distance that you see objects from or like animate objects are maybe more curvy or regardless of what the actual answer is, there were a lot of different hypotheses that were stemming from properties of these objects.
3652010	3655666	542	C	0.99999	Maybe mid level or low level properties, more so than higher level properties.
3655778	3670102	543	C	1	I still don't know if it's exactly been solved of whether it's like, interaction, like you said, with the objects causes the separation or the general shapes of these objects.
3670246	3674510	544	C	1	I would bet, as with most things, it's like some combination of all of the above.
3675410	3686922	545	C	1	But I think the interesting thing from this modeling point of view is that this is only trained on correlation statistics from the image data sets itself.
3686996	3690930	546	C	0.9995	This has no interaction, this has no notion of animacy.
3691910	3700738	547	C	0.93	I mean, this is really just training a model on ImageNet, just images of dogs, cats, boats, whatever, and yet it still achieves this type of organization.
3700834	3705766	548	C	1	So there's some sort of it could be semantic characteristics, right?
3705948	3714714	549	C	0.99999	We have a network that can classify boats versus dogs versus 20 other breeds of dogs, but it might also have.
3714752	3718380	550	A	1	Some correspondence with lower level image statistics as well.
3719150	3719514	551	A	0.58	So?
3719552	3719814	552	C	0.86	Yeah.
3719872	3720286	553	A	0.89	I don't know.
3720308	3721600	554	A	1	I guess he's my answer.
3723970	3724574	555	C	0.54	Yeah.
3724692	3734290	556	B	0.82293	Provocative analogy was the translational shift in the MNIST in the handwriting recognition setting.
3734870	3741202	557	B	0.99999	What are the translational shifts that exist today?
3741336	3742882	558	B	0.99855	What's the three pixel example?
3742936	3746806	559	B	0.89777	Is that some prompt engineered attack on an Lom or something?
3746988	3747814	560	B	0.86	Or something.
3747932	3755990	561	B	0.79	A special character being inserted, or some overlay on an image that we can't even detect.
3756970	3763340	562	B	0.63	So what do you think those challenges are and what are ways that we can pursue that?
3764910	3765754	563	C	1	Yeah, absolutely.
3765872	3771950	564	C	0.69	I mean, I think kind of the way I was thinking about it is like these symmetry transformations.
3773730	3780720	565	C	0.99997	If you're thinking about language models, you can imagine a symmetry transformation is just like replacing a word with a synonym or something.
3781890	3786206	566	C	0.74	You have the sentence to us means the exact same thing, but now suddenly.
3786238	3788370	567	A	1	The model is going to respond very differently.
3791110	3795266	568	C	0.93	Or like translation between languages, this can.
3795288	3796754	569	A	0.99997	Be seen as a type of transformation.
3796802	3804342	570	C	0.99983	It preserves the underlying meaning of the input to us, but to the model.
3804396	3805510	571	A	0.97519	It looks completely different.
3805580	3816710	572	C	1	And we would like to have models which behave in a predictable way with respect to these types of transformations, because I think humans behave very predictably with the type of these transformations.
3816790	3822358	573	C	1	And when we're dealing with AI systems, we expect them to also behave that way.
3822464	3827514	574	C	0.57	And I think that's part of what causes a lot of challenges interacting with these systems.
3827562	3832894	575	C	1	And I kind of tried to do a rough cheeky demonstration of that with.
3832932	3835200	576	A	0.99998	This bear and squares and stuff.
3836310	3843294	577	C	0.99996	We expect it to be able to do something simple like this because we think most humans could, and yet it doesn't.
3843342	3844498	578	C	1	And if you imagine this is a.
3844504	3848200	579	A	0.99998	Critical scenario where you expect this, then that's a big problem.
3848810	3850360	580	C	1	How do we handle that?
3850810	3853382	581	C	1	I think that's kind of what I'm searching for.
3853436	3854200	582	A	0.99	I think.
3856570	3876218	583	C	1	My direction I'm taking it is look more simple, kind of like bottom up building blocks of neural network architectures or algorithms that kind of yield these emergent structural properties.
3876314	3878622	584	C	1	And I think that's a much more generalizable way.
3878676	3881166	585	C	1	Rather than building something on top of.
3881188	3884894	586	A	1	What we already have, I think that's.
3884942	3886466	587	C	0.99998	Something that would scale much better and.
3886488	3888580	588	A	0.99998	Also matches more what the brain does.
3890710	3891410	589	B	0.99999	Very cool.
3891480	3893218	590	B	1	One kind of implementational question.
3893304	3902262	591	B	1	What are the computational requirements of just running this, or what's the day to day like of being a student or researcher running variants of these?
3902316	3906962	592	B	0.92	Like, do they use terabytes of data and you're using large computation?
3907026	3910970	593	B	1	Or is this something that people can run on their own laptops?
3911870	3915750	594	C	1	I think almost everything I presented today can be run locally.
3915830	3918826	595	C	1	So this stuff is super simple.
3918928	3923406	596	C	0.98	You can run you can run it on.
3923428	3923914	597	C	1	Your laptop.
3923962	3927914	598	C	0.99984	If you want to train and experiment with different things, it's going to be pretty slow.
3927962	3931790	599	C	1	So I'd recommend some commercial GPU.
3932290	3942798	600	C	1	I run pretty much everything on like Nvidia 1080s, pretty old, pretty cheap, but they have twelve gigs of Ram or whatever, and it's kind of more than enough for these models.
3942894	3950802	601	C	0.92	Gigabytes of Ram I think one thing that some people think is weird is I do most of my experiments on stuff like MNIST.
3950946	3955334	602	C	1	So it's 32 x 32 pixel images because I can train it small.
3955372	3958886	603	C	0.85	And locally, if you want to do.
3958908	3960790	604	A	0.59883	Stuff, my experiments are on mnists.
3961310	3964070	605	C	0.99966	If you want to do stuff like this, these are much more complicated.
3964150	3970182	606	C	0.99937	This Hamiltonian Dynamic suite here, you're getting into bigger models that are running across multiple GPUs.
3970246	3973710	607	A	0.99	And so here I was using a cluster to run these types of models.
3974290	3979198	608	C	1	But I say most of the single machine with the GPU is more than enough.
3979364	3980986	609	C	0.78	Or even just like in a collab.
3981018	3983422	610	A	0.99644	Notebook, something like that.
3983556	3992370	611	C	0.99997	If you want to train something on ImageNet, it gets more complicated and you need at least one GPU, ideally more.
3992520	3995586	612	C	1	But yeah, I don't do a whole lot of big scale stuff yet.
3995688	4000374	613	C	0.78	I think it's certainly interesting and there's definitely a lot more you can do there.
4000412	4005718	614	C	1	But for some of these kind of simpler or more fundamental questions, I don't.
4005724	4007160	615	A	0.9997	Know what you want to call it.
4007630	4009900	616	C	0.93	A smaller machine is nice and fast.
4012430	4014010	617	B	0.99027	Cool, useful.
4014510	4015034	618	B	0.51	All right.
4015072	4030330	619	B	0.53482	I'll read a comment from Dave recalling Bert Devries'comment during the Applied Active Inference Symposium about the desirability of spending less effort or ATP on foraging or control situations where we don't need much precision.
4030410	4046390	620	B	1	I don't know if you listen to this, but Professor DeVries mentioned about variable precision models and how they could be used to enable different features of generalization and actual structural course training as well as reduced computational requirements.
4046970	4052882	621	B	0.99999	Does he have any suggestions on how to introduce this distinction into active inference theory?
4053026	4055960	622	B	1	What kinds of experiments could winkle this out?
4058330	4059030	623	C	1	Oh, wow.
4059100	4065420	624	C	1	Yeah, that's something I don't think I have too much intelligence to say about, to be completely honest.
4069150	4070010	625	A	0.87515	Hmm.
4071470	4099834	626	C	0.8684	It's super interesting question because I think the intuition makes a lot of sense to me that you're talking about, if I understand correctly, variable rates of vision when you're encoding or in your model in general, doing computation that somehow has an impact on your future performance as a relation to some energy store.
4099952	4110026	627	C	0.87	I think if you wanted to build this into an active inference system, you would need to have really an embodied system where the agent has some notion.
4110058	4113440	628	A	1	Of energy, like an internal energy store, and.
4115250	4122434	629	C	0.99999	Something that is trying to conserve while it's performing its actions and running out of energy would need to mean.
4122472	4124034	630	A	0.99991	Something bad for the agent.
4124232	4126978	631	C	1	And then maybe you could observe kind.
4126984	4138742	632	A	1	Of an emergent reduction in encoding precision or something like this as the agent is trying to learn to act more.
4138876	4140406	633	C	0.99948	Effectively, you might have to give it.
4140428	4143190	634	A	0.99954	An ability to control its precision.
4143930	4148390	635	A	0.86	Yeah, like I say, out of my thoughts.
4149070	4149482	636	B	0.6	Okay.
4149536	4152054	637	B	0.99995	On this slide right here, first, very cool image.
4152102	4157130	638	B	0.99863	It's kind of like a digital Jackson Pollock.
4159410	4170560	639	B	0.99988	If it were a simpler input data size or just reduced complexity of patterns, or if it were an increased complexity, how would this image look different?
4171830	4183922	640	C	0.58	Yeah, so I did some experiments trying to change these orientation columns and basically changing the parameters of the model.
4183976	4186418	641	C	1	You can get these columns to be bigger.
4186514	4194470	642	C	1	You can get them to not have very similar structure to what we see in the humans, where you can get them to have more bands of activity.
4196170	4199594	643	C	1	And it also, like you said, it depends on the data set that you're using.
4199712	4205130	644	C	0.54246	If I use really simple sinusoidal gradings as input, I get something like this.
4205200	4210830	645	C	1	I get something that's a little bit more rotational, curvy, higher entropy.
4212690	4220560	646	C	1	So I think these are all interesting things if you want to study the emergence of this type of organization in a natural system.
4221170	4230900	647	C	0.99996	If you have a model that now yields different organization for different settings that see, okay, then what settings best match our observed data?
4234490	4236886	648	C	0.63	I can send those around if you're interested.
4236988	4253988	649	C	0.94	But I think one other interesting point there is that the different animals and types of orientation selectivity and different numbers of pinwheels.
4254004	4255368	650	C	0.99999	Some animals don't have it at all.
4255454	4261256	651	C	1	I think maybe mice, if I'm correct, have this kind of they call it salt and pepper selectivity.
4261288	4262536	652	C	1	So it's basically random.
4262568	4265580	653	C	1	You don't have any sort of, like, topographic orientation selectivity.
4266480	4270296	654	C	1	So there is evidence that different systems.
4270328	4273810	655	A	0.99997	Do this differently, and it's interesting to figure out why.
4274740	4276512	656	B	1	Yeah, this very cool.
4276566	4282530	657	B	0.73352	It reminds me of, first, the reaction diffusion, space and time.
4282900	4300040	658	B	1	So it's actually possible that a region might have no activity from a given granularity, like if it was being looked at at fMRI spatial and temporal timescale.
4303180	4313064	659	B	1	But if the pockets of activity are slower, faster, then that measurement is going to not be different than noise.
4313112	4315230	660	B	0.9395	It'll all have been averaged out.
4315840	4330636	661	B	1	So then there might be some interesting data sets that do actually have a lot of richness, but then for one reason or another, it just was averaged out over because it wasn't being connected.
4330668	4331824	662	C	0.5	To you or something like this.
4331862	4333888	663	C	1	You really need to go at the single trial level.
4333974	4340340	664	C	0.92	You need to have high enough spatial resolution such that it satisfies Nyquist frequencies.
4342200	4347024	665	C	1	And this just is something that people didn't do for a long time, especially if you're doing single electrode recordings.
4347152	4348656	666	C	0.73736	You're not going to see a traveling wave.
4348688	4349940	667	A	0.90327	You're going to see oscillations.
4350680	4352804	668	C	0.97	You need, like, multi electrode arrays.
4352932	4355976	669	C	1	And basically they're saying, okay, yeah, now that we have the technology to do.
4355998	4359592	670	A	0.99998	This, it's much resists that we didn't see before.
4359646	4364492	671	C	0.8	And potentially, this is an explanation for a lot of the noise that we were seeing before.
4364546	4366860	672	C	0.99991	Maybe it really is just traveling waves.
4367840	4372252	673	C	0.98	So yeah, I think there's a lot to be done in the future with.
4372306	4375920	674	A	0.76063	Increased abilities for recording.
4376420	4377730	675	B	0.54379	That's very cool.
4378660	4385330	676	B	1	Well, any final thoughts or questions or where are you going to take this work?
4386260	4387072	677	C	1	Yeah, no.
4387126	4388290	678	C	0.99997	Thanks for having me.
4390020	4392240	679	C	0.99899	Hopefully in the active infrastructure direction.
4392740	4393856	680	C	1	I would love to.
4393958	4395300	681	C	0.76	I think it'll be super fun.
4395370	4396804	682	C	0.91	So yeah, I'm not really sure.
4396842	4409668	683	C	0.99996	I'm looking at maybe music right now, looking at other kind of crazy directions.
4409844	4414956	684	C	1	I don't want to sound too crazy, but I'll go down.
4415058	4416492	685	C	0.82	Yeah, a lot of things.
4416546	4422140	686	C	0.85	So one thing that's coming up, something we submitted to Neurops is studying memory with traveling waves.
4423200	4424876	687	A	1	So that paper just came out on.
4424898	4430016	688	C	0.99051	Archive today of how waves are really good at encoding long term memories, which.
4430038	4431330	689	A	1	I think is super interesting.
4432500	4434560	690	A	0.87	So I might go a little more in that direction.
4435800	4437380	691	B	0.99999	Sounds good.
4437450	4448150	692	B	1	And yes, would be very exciting to see action come into play when there was the neurons that stayed active even as the dog's feet were moving.
4449000	4464540	693	B	0.99987	There's a lot of action sequences, like throwing a baseball, and then it goes, and it's like there's something about that action that's continuing to influence it's to having, like, a deep temporal representation of alternative actions.
4466160	4474030	694	B	1	And then the variational auto encoder is already basically the right anything like that.
4475120	4476444	695	A	0.75204	Really appreciate it.
4476562	4477532	696	B	1	All right, thank you.
4477586	4478670	697	B	0.99277	Till next time.
4479520	4480620	698	A	0.99892	Thanks so much.
4480770	4481080	699	A	0.99081	Bye.

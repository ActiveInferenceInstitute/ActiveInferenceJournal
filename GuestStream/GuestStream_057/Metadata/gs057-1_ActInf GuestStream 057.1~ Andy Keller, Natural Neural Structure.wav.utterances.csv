start	end	speaker	confidence	text
4050	4600	A	0.74	You.
6570	34190	B	0.9602424637681158	Hello and welcome. It's September 18, 2023, and it's active. Guest stream 57.1 with Andy Keller. We're going to be talking about natural neural structure for artificial intelligence. There will be a presentation followed by a discussion. So if you're watching live, please feel free to write questions in the live chat. Otherwise, thank you Andy, for this. Really looking forward to it and to you for the presentation.
36370	45794	C	0.9409717241379311	Yeah, thanks so much. Thanks for having me. I'm super excited to be able to present this stuff with Active Inference group. I'm a fan and very interested, so.
45832	46530	A	0.96438	Hopefully.
48550	178474	C	0.9508955555555553	Get to have a good discussion and see what you guys think about it. So my name is Andy. I'm finishing up my PhD, supervised by Maxwelling at the University of Amsterdam, starting a postdoc at Harvard after this. So I'll start out just talking about the goal of my work in general is to try to bring modern artificial intelligence closer to more human like generalization. And so what we mean by this is maybe some sort of structured generalization or maybe more familiar to the active inference community, like a structured world model which we believe that humans have. And the way that we propose to do this is by integrating natural neural structure into artificial intelligence. So first let's define what we mean by structured generalization. So I think it's fairly uncontroversial to say that modern machine learning generalizes beyond its training sets in the traditional sense. So for example, even the earliest artificial neural networks, multilayer perceptrons, could be trained on data sets of images like this and achieve high accuracy. Then, when they're presented with a held out test set of images that they've never seen before, they can still classify them relatively easily with the same level of accuracy. And this is what we typically call generalization. However, even fairly early on it was noticed that these systems really struggle with small shifts or deformations applied to the images. For example, if so, you think, why is this surprising? And I argue it's really due to our innate ability to perform this type of structured generalization that this example is a failure of. So for example, this shift is nearly imperceptible to us and we handle it automatically, whereas in these systems it's very clearly a major problem. So in words, we can say that structure generalization is a generalization to some symmetry transformations of the input, or in this case, the symmetry transformation is a small shift that leaves the digit class unchanged. So the obvious question then is what precisely do we mean by this natural structure and why do we think that.
178512	180750	A	0.99264	This would help us with these settings?
181570	230906	C	0.9661043846153844	So first, let's talk about what we mean by natural neural structure. One way to talk about structure or any type of bias in a system is an inductive bias. And so an inductive bias can loosely be defined as an apriori restriction of a set of realizable hypotheses. When you're doing model selection. More colloquially, we can call this something like before seeing any data, it's a restriction of what and how you can learn. So very broadly, this can include anything from model class to optimization procedures or even hyperparameters. And in some sense they really define what is possible to learn. And it defines generalization in that you actually can't generalize beyond a training set without having some inductive biases. And this is explained more thoroughly in.
230928	233314	A	0.888908	This paper by David Wolford.
233462	260706	C	0.9635894520547943	So what we mean by natural inductive biases then is biases that stem from the restrictions and limitations that are faced by natural systems, by the nature of having to live in the real world. For example, the brain has many efficiency constraints and physical constraints by nature of its construction. And following this logic, then these constraints are really playing some role in our generalization abilities which currently exceed modern artificial intelligence.
260738	262038	A	0.90675	As we'll go into next.
262204	282046	C	0.9488935849056608	So in this talk, I'll be focusing specifically on two types of structure which my work has studied. These are topographic organization and spatiotemporal dynamics. And before I go into my work, I'll give a short example for why I believe that natural structure may be useful to achieve the structure generalization that.
282068	283360	A	0.999994	I was talking about before.
284210	738062	C	0.9471872072784836	So the first example comes from Fukushima's neocognitron architecture from the 1980s, which was actually built to directly address the problem of robustness to these small shifts in deformation. So in the paper he writes about inspiration from pupil and weasel's measurements of hierarchy and pooling in order to achieve robustness to these distortions. And so if you look at the figure he writes u sub s one, U sub C one and these stand for simple and complex cells. And so this is a fairly radical approach at the time, but it really served to improve robustness and shifts that were plaguing these early artificial neural networks. And over time these ideas were simplified and abstracted and obviously yielded the convolutional neural networks that we know today, which ultimately drove the success of the Deep Learning Revolution. So this is really an example of a natural inductive bias which achieved structured generalization. So for our research, it's really of utmost interest to try and understand what makes these models work so well and see if this principle can potentially be generalized to cover more abstract transformations and symmetries. So what makes a convolution achieve this structure generalization? Intuitively, you can see this is done by applying the same filter or feature extractor at various spatial locations. So here we see a single convolutional filter being applied at all locations of an image. This means that no matter where your input is, whether it's kind of in the middle of the image or on the right, you'll have the exact same features, with one exception, they'll be equivalently shifted. So mathematically, this type of a mapping is called a homomorphism. It preserves the algebraic structure of the input space and the output space. In this case, it's with respect to translation. And at a simple level, something like that will be important to remember for the rest of this talk is that we can verify homomorphisms of our feature extractor if we can see that there's this commutation with the transformation, this commutative diagram. And so we can write this also algebraically by showing that the feature extractor f commutes with the transformation operator T. And basically what we want is there to be no difference between first extracting the features and then performing the transformation, or performing the transformation and then extracting the features. So the challenges to date is we don't really know how to construct homomorphisms with respect to more complex transformations that we see in the real world. For example, our brain is able to handle changes in lighting and season naturally. So here we see lighting on a person's face or the change of seasons. We can tell it's the same face or the same road. But we don't know how to build models which respect these transformations. And so it makes us hard to build systems which handle them in a robust and predictable way. To give an even more abstract example of what I mean by this and the potential negative repercussions of models which don't handle symmetry transformations, consider modern text to image generation programs. So in this example, I asked Dolly Two to generate image of a teddy bear on the Moon. And it does this incredibly well, right? Probably better than I could. It has texture fur, incredibly detailed. However, if I ask it to do something which I see as conceptually simpler, such as draw a blue cube on top of a red cube, it fails to do this. And to me this seems unintuitive since the second task seems significantly easier. But what I'm arguing is that the reason that this is surprising is precisely the same reason that the MNS translation example was surprising. There is this symmetry transformation happening here, namely the transformation between these complex objects of a teddy bear and the Moon and these simple objects of cubes which we intuitively expect the network to be able to handle and respect. And we see that it doesn't. So just like how Fukushima's work showed that these natural structure of hierarchy and pooling of our visual system are effective for making generalizations to small transformations, I argue that potentially higher level structure may be necessary to fix these abstract generalization problems. And so the question then that I'm studying and that I'm asking is what might this structure be and how do we implement this in an artificial neural network architecture that can actually be used for performing computation? So, to begin to answer that, I'll jump into my first line of work on topographic organization. So topographic organization is observed widely throughout the brain from primary visual cortex to higher level areas. And it can very loosely be described as this property that neurons which are close to one another tend to respond to similar things. For example, on the left we show the color coded preference of each neuron in the Macaque primary visual cortex as a response to oriented lines and we see this smoothly varying set of selectivities. Another type of organization is known as retinotopic organization where nearby neurons in the visual cortex tend to respond to nearby receptive fields. However, this organization isn't limited to these low level features. It extends to more complex features such as those present in faces or objects or places. And this relates to the so called functionally specific areas of the brain such as the fusiform face area FFA and the perihepocampal place area PPA. So in this work, the main idea again is that perhaps this topographic organization in some sense which is intimately related to the convolution operation and Fukushima's architecture we can maybe generalize the benefits of this to more abstract transformations in other words, learn how to build more complex homomorphisms that we can't do analytically right now. So just to show that we're not completely insane with this idea there is some prior work in this domain from people such as Honin Yanlaka Apohevarnan in the early ninety s and two thousand s and they studied how topographic organization may be useful for learning invariances mostly in linear models. So the question for us when we entered this space is what is the most scalable abstract mechanism that can be leveraged from these approaches which we can integrate into modern deep neural network architectures? And ultimately we settled on a generative modeling approach which I think might be interesting to the people in this community which then allows us to relate it more closely to topographic independent component analysis with the basic idea being that we can learn a topographic feature space by imposing a topographic prior distribution over our latent variables. So just to give a brief background I assume most people are already familiar with this but the kind of general assumption is that the brain is a generative model and this idea in some sense can be attributed to Helmholtz from the 19th century where he said that what we see is the solution to a computational problem. Our brains compute the most likely causes from photon absorptions within our eyes. And so as an example, if I show you this image you immediately recognize it as a globe with some curvature. However, it could just as equally be a disk with a distorted perspective on it. So this is how we get optical illusions or our images. So like this one, your brain infers that there is a cube here because of the structure but really it's just.
738116	739440	A	0.9657560000000001	A flat piece of paper.
739810	1089786	C	0.9540609601634352	So you can think of this generative model aspect as kind of like an inverse graphics program. In the program, the abstract properties of the sphere are known the position, the size, the lighting and these are used to project the sphere to create the 2D image that is rendered. So in effect, what Helmholtz and others are saying is that as a generative model, the brain is actually trying to invert this generative process and doing inference and infer the underlying causes of our sensations. So the reason I'm kind of belaboring this point is that there's a lot of talk of generative models today, and I'm not necessarily just talking about generating images or pretty pictures. I really want to mean a framework for unsupervised learning. So then to get a little bit more into the details, what do I mean by a topographic prior? So generative models are typically described as a joint distribution over observations, x and latent variables, which we'll call Z. And this is typically factorized, or one way that this is done is factorized in terms of a prior p of Z. And this true generative model conditional generative model p of X given Z. And so one way that we can think about this is that the prior can be seen to encode relative penalties for each type of code that is produced. When we invert our generative model, this is called computing the posterior e of Z given X. And so to develop a topographic latent space, we want to introduce some sort of a topographic prior, which this topographic ICA work showed is equivalent to something like a group sparsity penalty. So people might be familiar with typical sparsity penalties from independent component analysis. You want your activations to be sparse, meaning many of them are zero. So that could look something like this. You have a bunch of blue squares that are active, but most of them are not active. But specifically, with a group sparsity penalty, we want these priors to assign lower probability to these distributed sparse activations and higher probability to these grouped, densely packed representations. You can also think of this like a higher penalty when things are spread out, a lower penalty when things are closer together. So again, this can be written abstractly like this. But I want to make clear that each one of these squares here represents kind of a neuron in our model. And they're organized in this 2D grid. So when we're talking about grouping, we really mean grouping in that 2D topology. So one thing that's really interesting and kind of important is that these priors don't just give us topographic organization, but they've also been noted by people or studied by people like Erosimicelli and Bruno Olshausen to actually fit the statistics of natural data better, specifically natural images. They've shown that using this type of a prior, you actually get a sparser set of activations, meaning that the prior fits the true generative process a little bit better. And as we're aware, the brain has a high degree of sparsity and this is believed to be very relevant for efficiency. So to get a little bit more into the details, to implement this type of a group sparse prior, we use a hierarchical generative model and this is basically introduced by some of the topographic ICA work. The idea is that you have a higher level latent variable U which simultaneously regulates the variance of multiple lower level variables T. And this is how we get group sparsity. Then to get topographic organization, you can have multiple of these latent variables U slightly overlapping with their fields of influence. So their neighborhoods, we can call them, and this will give you this smooth correlation structure you're after. So get the intuition for this. You see that this variable T over down on the bottom here is not getting any input from this U on the top, but it is sharing a U variable with this T in the middle. So it's like they're sharing variance, they're sharing some components with their neighbors, but not all components. And that's really due to this local connectivity of these higher level variables U. So to keep it simple about how we use this generative model, let's go back to a single U variable. And the challenge in this type of an architecture which made it difficult for many years is how do you infer the approximate posterior over these intermediate latent variables in this hierarchical architecture? This is not super straightforward. So prior works have used heuristics developed for linear models. And in our work we found that this really didn't extend to modern neural network architectures. So really our insight is to leverage a factorization, a specific reprometerization of this distribution. And so this reprimmatrization specifically is achieved by defining the prior to be what's known as a Gaussian scale mixture, meaning that our conditional distribution of T given U is actually a normal distribution where the variance is defined by this variable U. And for certain choices of U, this distribution is indeed sparse and encompasses a range of distributions such as Laplacians and student T distributions. One way of defining it is a Gaussian scale mixture admits a particular repromaturization in terms of independent gaussian random variables Z and U. So specifically, then we see that this T variable, which was originally fairly complex, is actually just a product of a bunch of gaussian random variables which we now know how to work with much more efficiently in generative models. And specifically what we're going to do is so that we can actually get approximate posteriors for U and Z separately and then do a deterministic combination of them in order to compute our topographic.
1089818	1092480	A	0.9326888888888889	Variable T. And this is much easier to do.
1093090	1143958	C	0.9542990714285712	So, without going into too many details, the method that we decide to use is what's known as a variational auto encoder, which leverages techniques from variational inference to derive a lower bound on the likelihood, allowing us to parameterize these approximate posteriors with powerful nonlinear deep neural networks and optimize them with gradient descent. This is going to be familiar to the active inference community. But really what we've done is instead of having a single encoder and decoder. As is typical Bayes, we now have two encoders, one for U and one for Z separately. And then we combine them in this deterministic manner to construct our topographic T variable. If you see that this is actually the construction of a student's T distribution from Gaussians and then we do this before decoding and then maximize the likelihood.
1143994	1145182	A	0.999015	Of the data altogether.
1145326	1154994	C	0.9753571428571427	So this is the elbow, the evidence lower bound abound on the likelihood of the data and is actually very similar to the variational free energy that is.
1155032	1156760	A	0.8939116666666665	Used in the active inference community.
1158250	1171626	C	0.9780615555555555	So with these details out of the way, what's really interesting is what happens when we train this generative model which has relatively simple group sparsity penalty in its latent space. And we want to look at kind of what it's learning in terms of.
1171648	1173462	A	0.996375	Its organization of features.
1173606	1343506	C	0.959675503211993	And first we start with the simplest possible data set. We have a black background with white squares at random XY locations. If we train our auto encoder with this group sparsity penalty on it and then we look at the weight vectors of our decoder which we're plotting in blue here, again organized in this 2D grid, we see that indeed they learn to be organized according to spatial location. So this can be seen as similar to convolutional receptive fields where the receptive field of each neuron is really given by the kind of inputs at its location. And this makes sense intuitively from the group sparsity perspective since for any given region, which we highlight, like in yellow here, the filters in a given group are much more highly correlated. They have these overlapping receptive fields than other random locations. So essentially we see that our model is learning to cluster activities together in sort of a simulated cortical sheet according to the correlations in the data set. So instead of in convolution where you're actually doing weight tying and you're manually specifying I want to copy this weight everywhere, you can maybe think of this as like approximate weight tying. And really we're learning this from the correlation structure of the data set itself. And just to give a little bit more of a biological inspiration for this, we know that retinotopy is present in the brain. This is an example of retinotopy and the Macaque visual cortex. And you can see if you show the Macaque an image like this, it gets projected into this topology preserving space actually on the surface of the cortex. So the idea is that topographic organization and even learn topographic organization is preserving the input correlations of our data set. And potentially this may be beneficial for generalizing these ideas a little bit further. So like I said at the beginning, it would be even better if we could just learn something more than just convolution, maybe more complicated equivalences. So how do we do that? One thing that's clear in natural intelligence is that we don't exist in this world of IID frames, right? We exist in a world of continuous sequences of transformations. So maybe we can extend our model to this setting to learn, observe transformations. This is the idea of temporal coherence. So what would happen if we just simply extended our previous framework over the time dimension, right? So instead of just grouping, saying we want our neurons to be group sparse in terms of spatial extent on the cortex, we actually want them to be group sparse over time. Meaning that if one set of neurons is active now, we want that same set of neurons to be active into.
1343528	1344820	A	0.9275	The future as well.
1346470	1371754	C	0.9523543023255813	If we kind of intuitively think about this, we see that this is actually more encouraging invariance than equivariance. A way to understand this is we're saying we want the same neurons to be active constantly, but the input transformation is changing, right? The feet of this little fox are moving. So if the same neurons are coding for the same thing over and over again, but the feet are moving, those neurons are going to learn to be invariant to the motion of that leg.
1371792	1373260	A	0.933956	Of this dog, for example.
1373790	1375580	C	0.9343874999999999	So instead is that.
1377390	1379100	A	0.9999416666666666	I went the wrong way here.
1381090	1442266	C	0.9458665555555551	So instead, our insight was that this group Sarcity could instead be shifted with respect to time. So this would mean that sequentially shifted sets of activations would be encouraged to activate together, and then our latent space would really be structured with respect to the observed transformations. So you can see here that rather than the same set of neurons being active at all time steps, it's really a sequentially permuted set of neurons that we're grouping together in this sparse way. And then this allows us to model different observations over time, but they're still connected in terms of learning a transformation and preserving this correlation structure of the input data set. So if we put this together into our topographic BAE architecture, you can get something that looks like this. You see that we have an input sequence. We're again encoding a Z variable and then multiple U variables in the denominator here. And then each one of these U variables is shifted kind of like we were showing before. In order to achieve this shift equivalent.
1442298	1443706	A	0.9999139999999999	Structure that we're looking for.
1443828	1535914	C	0.9399284999999996	When we combine these in this student T product distribution, we get a single latent variable. This is now our topographic latent variable T. And now that we have this known structure in our latent space, you can think of it like a structured world model. We know how to transform this latent space. In this case, it's by permuting these activations around these circles, doing like a cyclic roll, a cyclic shift. We know that this is going to correspond to our learned input transformations. And we can verify that by saying, okay, what if I continue this input transformation, the true transformation in the data set, which is a rotation. And then I compare that with how I've done my role in my latent space by moving my activations around in my brain. And then we decode and we see that we get the exact same thing. And so this is demonstrating this commutivity property that I was talking about before for verifying homomorphism. And so to measure this a little bit more quantitatively, we can measure what's called an equivalence loss. So this is really the quantification of this difference between our rolled capsule activation, our rolling in our head, versus watching the rolling unfold, watching the transformation unfold before us. So we see that topographic VAE achieves significantly lower equivalent error. This bubble VAE is what I was talking about before, where it's learning invariance, so it doesn't have the shift operation and then the traditional VAE kind of has no notion of organization or temporal.
1535962	1537950	A	0.87051	Component, so it performs very poorly.
1538690	1551806	C	0.9684997619047616	In addition to this, we see that the model is a better generative model of sequences. It just gets a lower negative log likelihood on the data set. So it's better able to model this data set because it has a notion.
1551838	1553650	A	0.9885333333333333	Of the structure of the transformations.
1556310	1590366	C	0.9594409259259259	We can test this on multiple different transformation types. On the top row we're showing the true transformation, we hold out these grayed out images and then on the bottom row we encode and then we just kind of roll our activations around and we keep decoding to see what the model has learned as the current transformation that's being observed. And we see that it can basically perfectly reconstruct these elements of the sequence that it's never seen before. Additionally, with images that are from the test set that it's never seen before, simply because it knows what the transformation is that it's currently encoding and it.
1590388	1592290	A	0.9237700000000001	Can generalize that to new examples.
1593990	1702226	C	0.9472285036496343	So the takeaway from this part is really topographic organization. We showed that it preserved input structure and now we're showing it can potentially improve efficiency and generalization as we would hope. So, finally, something that surprised us and I thought was potentially the most interesting is that these transformations that are learned by our model actually generalize the combinations of transformations that were not seen during training. So, for example, despite only training on color and rotation transformations in isolation, if the model is presented with a combined color rotation transformation at test time, we see that it's able to completely model and complete these transformations perfectly through the capsule role, implying that it's learned to factorize represent to these different transformations and it can flexibly combine them at inference time. So again, maybe we also don't just get efficiency and generalization, we also get some basic compositionality. So let's talk about the limitations and what we could do next. The main limitation is that there's a predefined transformation that we're imposing in both space and time. So although we freed ourselves from group transformations and specifically like translation or rotation as is currently done in the machine learning world. We still have this hard coded latent role in our heads for everything we see. And to make this a little bit more flexible, so hopefully we can model a greater diversity of transformations, we think maybe we can take inspiration from more structured spatiotemporal dynamics that are observed in the brain. And so that takes us to the second part of this talk, which is spatiotemporal dynamics that we're going to try.
1702248	1704318	A	0.9987016666666667	To integrate into artificial neural networks.
1704414	1706286	C	0.9264485714285715	One example of that is traveling waves.
1706318	1707480	A	0.99993	Like I show here.
1707930	1747694	C	0.9462007079646014	So what do we mean by that? Here's a very recent paper where they used a nine tesla fMRI operating at 36 millisecond resolution to image a single slice of a rat brain under anesthesia. And what we see is this very clearly structured spatiotemporal activity and correlations. And these authors of the paper go on to analyze this activity in terms of the principal modes as depicted on the right. So our hypothesis is that perhaps some sort of a correlation structure like this may be beneficial for structuring the representations of our model with respect to observed transformations, but in a much more flexible way than simply just a cyclic shift.
1747742	1748980	A	0.969972	Like we were doing before.
1752790	1964246	C	0.9442650905797108	And let me say that this is not just observed in anesthesized rats. You can see these traveling waves happen in the mt cortex of awake behaving primates. So for example, on the left here, they show traveling waves that actually change how likely a primate is to see a low contrast stimuli based on the phase of the wave. Furthermore, they show that a high contrast stimulus on the right can induce a traveling wave of activity that propagates outwards union primary visual cortex. So these are really ubiquitous throughout the brain at multiple levels. And it would be interesting to study what their implications are for structured representation learning. In our case, or generally, there is prior work which has studied these types of dynamics and they built models. So on the top, these are the equations which describe a spiking neural network, which they show if you implement time delays, actually exxonal time delays between neurons, you do get these structured dynamics of traveling waves as long as your network size is large enough. However, as many people probably know, it's relatively challenging to train spiking neural networks of the same size and performance as deep neural networks. Similarly, on the bottom, another system which is significantly simpler, but perhaps too simple, is a network of coupled oscillators. These are known to exhibit synchrony and spatial temporal dynamics and complex patterns. But this is called like a phase reduced system and doesn't quite capture the full complexity that we're interested in. So we're looking at something that's potentially in between these two. And what we settled on is this work, in this work is to parameterize a network of coupled oscillators slightly more flexibly than a Kuramoto model. So this. Is really built on this coupled distillatory recurrent neural network of Constantine, Rouge and Nisha, where they basically took the equation which describes a simple harmonic oscillator. It's a second order differential equation. The acceleration on a ball on a spring is proportional to its displacement. You can add additional terms such as damping so that the oscillations slowly die out over time. You can drive this oscillator with an external input to kind of counteract this damping or to give slightly more complexity to the dynamics. And then furthermore, if you have many of these oscillators, you can couple them together with these coupling matrices W, as we demonstrate kind of in this picture here. So you can really think of this network as a bunch of balls on springs and they're maybe connected to each other also by springs or elastic bands, whatever. The couple, the Silatory recurrent neural network of Rush and Mishra with these various terms. And this has been shown to be very powerful for modeling long sequences. They also mentioned they were inspired by the brain building this. And there's a lot of good analysis in that paper. For example, they show that this has really beneficial properties with respect to vanishing gradient problems that typically happen in recurrent neural networks. But if we want to look at spatiotemporal dynamics in this type of a model, it's slightly challenging because these coupling matrices here, the W's that connect each oscillator's position to one another, these are densely connected matrices like I've tried to.
1964268	1965718	A	0.962744	Depict on the left here.
1965884	1975158	C	0.9508541666666664	So if you try to visualize the dynamics of this network, you don't see any spatial organization. There's no inherent topology to the latent.
1975174	1976300	A	0.9987875	Space of this model.
1976670	2022474	C	0.9562742657342651	So you can think of this like in our previous example. A neuron is connected to a potentially arbitrary set of other neurons. Those neurons are connected to another arbitrary set of neurons. And you'll just get oscillatory dynamics, certainly, but kind of fluctuations that don't make a lot of structured sense in our work. Then we thought, okay, how can we convert this more to the types of dynamics that we're interested in, this structured propagation of activity? And one clear way to do that is to have a more structured connectivity matrix W, which we found is easily implemented and efficiently implemented through a convolution operation, which you can think of like a locally connected layer. So instead of having every neuron connected to every neuron, neurons are just connected to their nearby neighbors. After training, you'll end up getting something.
2022512	2025690	A	0.9336900000000001	That looks like a smooth spatial temporal dynamics.
2026510	2112598	C	0.9616941666666661	So to be a little bit more clear, to train this model, we take this second order differential equation that we were describing before you discretize it into two first order equations. You can think of this as like numerically integrating the ode. We now have a velocity and then we update and we can train this model as something like an auto encoder or an autoregressive model. So we take an input, we encode it to our latent space. Really, the input is this F of x term which acts as the driving term. So it's like driving these oscillators from the bottom and then they have their own dynamics which are defined by the coupling terms, these local couplings. And then at each time step we take this latent state, this wave state, and we decode to try and reconstruct the input. Maybe at the current time step or a future time step, we can do some analysis of these models during training to see what happens before training. And after training, we can compute the phase and the velocity of the dynamics in the latent space. Basically, we see at the beginning of training there's no waves in our model. But after training, after 50 epochs, we see that there's these smooth structured activity propagating downwards in service of this sequence modeling task that we're doing, like rotating objects. So what's the benefit of this? I mean, the whole reason I motivated this was to say we wanted to have more flexibly learned structure. Are we actually doing that or are.
2112604	2114150	A	0.999916	We just getting pretty waves?
2114970	2242734	C	0.9534261340206182	So what we showed in our paper is that we really are learning some sort of useful structure. And the way we showed that is, again, with something like this commutative diagram. If you take an input and you encode it and you get a wave state and then you propagate waves artificially in that wave state and then decode, you can observe that it's actually exactly the same as if you had just by showing a bunch of different images of different transformations. So a lot of different digits, different features. And we see that we get different types of wave activity in each case in order to model that different transformation. If we train it on different data sets as well, we similarly see more complex dynamics. In this case, maybe not even traveling waves or standing waves, which can be thought of as traveling waves in opposite directions. So we see if we're modeling these orbital dynamics, we get these kind of smoothly moving blobs of activity in our latent space. If we're modeling a pendulum, we similarly get kind of complex oscillatory activity so it's preserved input structure, but additionally more flexibility than we had before, which is kind of our ultimate goal. So finally, I want to talk a bit about how I think the outcome of this research may not only improve artificial intelligence, but also how it helps us understand why our measurements of the brain look the way they do. So to give a brief example of what I mean by this, I talked a bit about before, about these and places. So in this fantastic work with HIE Gao, we studied if our simple topographic prior, as we discussed, may be able to reproduce these same effects. So specifically, we plot the value of this Cohen's D selectivity metric for each of our neurons with respect to a different data set of images potentially containing just faces or just objects or bodies. And so we measure for every neuron, is it more likely to respond to faces or the ration emerges in the brain? But I do think that it tells us that the relative organization of selectivity may at least be partially attributable to correlation statistics in the data after being passed through a highly nonlinear future extractor.
2242862	2244850	A	0.9984716666666668	Such as a deep neural network.
2245350	2294254	C	0.9476946043165465	So, in a similar vein, something that's interesting, there's a known what's called tripartite, or the visual stream. So images of or objects are selectivity with respect to objects is organized by more abstract properties, such as animacy, is this thing alive or inanimate versus also real world object size, like what is the size of a teapot versus a car? And what we see is that in humans, this selectivity is organized in this tripartite structure. You typically have small objects that are in between animate and inanimate objects in terms of their selectivity. And we see the same thing kind of happens here. So these are measuring the selectivity of the same set of neurons, but with respect to these different sets of stimuli. We see that the small cluster is in between animate and inanimate cluster.
2294302	2297134	A	0.95565375	And again, this happens for multiple different initializations.
2297262	2314406	C	0.9470665957446811	So this is something I hope we can explore a bit further for this community. I think it's interesting because it's really a way of showing that we built a structured world model. And potentially, this world model is beneficial for better representing real world data in.
2314428	2318700	A	0.9986066666666669	A structured way, and you get lower free energy in that sense.
2319070	2372806	C	0.9456039869281042	So by developing these models like we showed here, we may get insights into new mechanisms for how this structure emerges, including topographic organization that we never thought of before. So, machine model, I was looking at the orientation selectivity of neurons, which I wasn't particularly expecting something to happen, but you're looking at kind of these waves propagate over this simulated vertical surface. And I thought, okay, maybe I'm showing rotated images. Maybe this has some effect on the orientation selectivity. And actually, if you go in and you measure the selectivity of each neuron with respect to these differently oriented lines, what you see is that it's surprisingly reminiscent of the orient paper columns that are seen in primary visual cortex. This is stuff going back to Hugle and Weasel. And this is something that just kind of came out of this model and the fact that it has the spatial.
2372838	2375270	A	0.9058816666666666	Temporal structure with respect to transformation.
2375430	2387806	C	0.9488157142857144	So, of course, this is a really coarse analogy, but I think this is an example of how building these types of models can help us think about how the brain builds representational structure and the way it's organized in a way.
2387828	2389520	A	0.9785057142857142	That maybe we haven't thought about before.
2391490	2533338	C	0.9518526010101016	I think I'm not the only one who's doing this type of work. And so I want to talk a little bit about some other people who are doing this. So I've been talking about this equivalent structure. People such as James Whittington and Tim Barrons and Surge Anguli have shown recently that by introducing algebraic constraints into a learning process, in this case, it was like the motion of an agent in an environment. By saying, you need to preserve kind of this algebraic structure of if I move in a circle west, north, east, south, I end up back at the same point. Again, by introducing these types of constraints, you get the emergence of grid, cell like representations. So I'd be interested to see how this idea of representational structure can help us explain maybe more than our scientific findings we're finding as well and how this relates to generative models as a whole. And then finally, I think there's something to be said about cognitive plausibility of these models as well. Maybe we're not just going to be testing them from a neuroscience perspective, but also from a cognitive science perspective. For example, there's these Ravens Progressive Matrices on the left where you have to say which one of these images is more likely to fit in this pattern? Or for example, how likely is it that this Jenga Tower falls over when you pull a specific block or with a given structure? And I think these types of tests are really testing if our world models that we're building are similar to the types of models that we innately have our own common sense as humans or as beings living in a natural world. And I've done some preliminary work in this direction, I think very preliminary and not nearly this complicated, but trying to model visual illusions. So if you take a really simple data set of a moving bar stimuli or a static bar frame and you move it a little bit, you can see that the model will actually infer that missing frame and then actually also infer continued motion. So it's like overshooting the trajectory of what the actual stimuli is providing it before correcting again. So I think modeling illusions is certainly an interesting way to study if our world models are similar to the types.
2533354	2535630	A	0.9046083333333333	Of models that we have ourselves.
2536530	2583006	C	0.9298551127819547	So in conclusion, yeah, I think topographic priors we could show that they effectively learn structured representations or structured world models. This learned structure is flexible and adaptable to arbitrary transformations, unlike traditional equivariance. And topographic riders can be induced statistically as we did in the topographic VAE or through dynamics like we were showing in these neural wave machine type models. So to conclude, I'll leave you with this quote that I found in Fukushima's paper from 1980 I thought was pretty far ahead of its time, where he says, if we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to understanding the neural mechanism in the brain. So that's kind of, I think, some.
2583028	2585200	A	0.9359237499999999	Of the goals that we're going for here.
2585890	2594670	C	0.8046371428571428	So I'll say thanks to my advisor Max, my co authors Patrick Yue, Emile Jinghe, and Yorn, and interested in discussion.
2594830	2595540	A	0.99974	Thanks.
2604570	2622330	B	0.9784105555555558	All right, thank you. Great. Very interesting presentation. A lot of places to start. Maybe just what brought you to this work, a little context on how you came into this work for your PhD direction.
2623950	2638970	C	0.9065142857142857	Yeah, I mean, there's been studying the group that I'm in at the university has been studying structured representations from mathematical point of view for a while, and we're some of the people to models.
2639130	2641310	A	0.7914983333333333	Or for the variational auto encoder.
2641990	2657686	C	0.9493887179487177	And I guess what something that had always been the model that respects rotations, 2D rotations perfectly well. But if we want to do 3D rotations, we can't do that because that's not a group in terms of a.
2657708	2659298	A	0.995832	Projection onto a 2D plane.
2659394	2661506	C	0.9229671428571429	You're losing information when this thing rotates.
2661538	2662680	A	0.9478433333333333	Around, for example.
2664570	2675034	C	0.979974857142857	Or just any sort of natural transformations. Like I was trying to point out at the beginning, I think it was trying to think about how the brain models natural transformations is something that these.
2675072	2694530	B	0.9432282857142857	Current frameworks where do you see action playing a role in terms of variational, auto encoder models that include not just external patterns, but also the consequences of action or world model structure with action.
2695750	2719466	C	0.9199509615384618	Right, yeah. No, it's a good question. And I think active inference is effectively I think it's a good answer to that. I know there are reinforcement learning frameworks that do use kind of externally trained world models. So you train a VAE or something and then you use that representation in.
2719488	2721660	A	0.9948250000000001	Your reinforcement learning system.
2722190	2726362	C	0.9501242857142858	But I think having a fully kind of a system that is a single.
2726416	2730666	A	0.9411728571428571	Objective with action as part of the.
2730688	2746114	C	0.9385410810810813	Likelihood of the data, I think that's much more elegant. I'm a big proponent of that. I have not gotten so far as to study how these structured world models in a VAE or I haven't worked.
2746152	2750466	A	0.993026111111111	On that at all. But I think it would certainly be very interesting to see if having a.
2750488	2756306	C	0.9724542857142857	More structured world model in a variational auto encoder would be beneficial in an.
2756328	2758534	A	0.956415	Active setting as well. I think that would be awesome.
2758652	2764758	C	0.9569735714285715	I mean, I think some of these examples, like, showing before, like, emergence of.
2764764	2766534	A	0.9906357142857143	Grid cells and things like this, maybe.
2766652	2773286	C	0.9310765	Point towards that direction of, hey, maybe the brain is doing something. It really obviously has a lot of structure.
2773478	2779678	A	0.9470938461538462	This clearly has to be useful for performing actions in some way. Cool.
2779764	2780062	C	1.0	Yeah.
2780116	2806722	B	0.919031153846154	I thought a really nice parallel that you brought in with the talk was the locally connected units enabled your models to structurally embody the convolutional constraint and pattern, and that led to these arising patterns. And then analogously, there was the Doral et al. Where they had the path exploration constraint.
2806786	2807062	A	0.98969	Right.
2807116	2850260	B	0.9401167045454543	And so then it's interesting to think about these action or policy heuristics or sparsities like a joint motor exploration. Eventually it becomes understood that there's like two mutually opposing ways to move a joint, and then the compositionality across joints can be learned at these higher levels once it's locked in at lower levels. So it's a very appealing and niche relevant way to generalize because it's both based upon the actual constraints of the world, but then especially through action, potentially embedding something that's quite simple.
2851350	2858054	C	0.9649519999999999	Right? Yeah, no, I think that's definitely true. That's a really good point. If you do have constraints coming from.
2858092	2865750	A	0.9377618750000002	Your actions themselves, then that would be hugely beneficial for helping to structure your latent space.
2865820	2913314	C	0.9388750980392152	And I think yeah, I guess one thing I wanted to mention there's something made me think of, like, Stefano Fousey's work on kind of the representational geometry determines how generalizable a given understanding of a system is. And I think if you can understand if these sets of activities are separable or highly parallel separable with a linear classifier, essentially, then you're going to be able to do generalization. And I think by imposing these types of biases or potentially through constraints that are imposed by action, something like this, you are yielding or kind of inducing a better representational geometry.
2913362	2920938	A	0.85020375	And this has all sorts of benefits for compositionality or generalization. It's a great point. Cool.
2921024	2921322	C	1.0	Yeah.
2921376	2933790	B	0.9054300000000001	Very interesting area. All right, I'll read some questions from the live chat love Evolve wrote any practical or observed limitations on modeling illusions?
2938450	2958694	C	0.9395679545454545	Deep learning, community uses. They're not foveated. You don't have a center of gaze, and you also don't have most convolutional neural networks. I'm using these kind of recurrent neural networks, but time is not as clearly defined in these models as it is.
2958732	2963990	A	0.9088845454545456	In a continuous time setting for human undergoing an illusion trial.
2965210	2979898	C	0.9278940000000002	And I think the combination of these two, of the fact that as a human, for most things, you're shifting locations and your gaze are dependent on you looking to a particular area, a lot.
2979904	2981498	A	0.7232475	Of cognitive science tests.
2981594	3009318	C	0.9729534343434337	And so I think it would be really helpful if we had models that you can think of this as a type of action of learning where to move your gaze. One of the simplest possible that would help a lot for being able to model illusions. And just I mean, for me, it's like I read a paper of some cognitive science experiments or about some illusion, and I think of, okay, can I put this data set into my model and test it? And most of the time the answer is no, because I don't have a.
3009324	3014360	A	0.9825157142857144	Model that looks around or has a restricted field of view, something like that.
3015050	3023398	C	0.9769938888888888	So, yeah, I think that's one of the limitations. Another one is makes the experiment much more complicated.
3023494	3025930	A	0.9674414285714285	So that's one of the practical limitations.
3027310	3028058	C	0.83685	Wow.
3028224	3087920	B	0.947905395683453	Great answer. Makes me think of a paper with letters rotating on a table. That's the digit rotation. Great points about the Foveation and the dynamics of the illusion. I think you actually did mention an illusion, which is however, you mentioned in the generalization context, which is rotating on the two dimensional screen doesn't generalize to three dimensions, and that dimensional collapse or reduction is the basis of the cube projection illusions and cube and figure rotation illusions. It's on your screen and there's a silhouette or there's some ambiguous stimuli that it's near a criticality or a bifurcation in the generative model. So it could represent it one way or another way. And so a lot of the switching illusions are just based upon the flatness of images and the limitations in generalization that are revealed by.
3092370	3098286	C	0.9232052941176471	Some work sorry, there's some work where they can argue people have a kind of three dimensional.
3098318	3105640	A	0.8612245	Image in their heads. Like even Nancy Kenwich had her library on this recently and showing yeah, I don't know.
3106250	3108054	C	0.9943359999999999	Do our models have that?
3108252	3111158	A	0.9601219999999999	It's not super clear anyway.
3111324	3117750	B	0.9842233333333336	Yeah, that's pretty interesting. All right. From Upcycle club in the chat, they wrote Kudos.
3118250	3120638	A	0.9151385714285715	You're able to learn nearly as effectively.
3120754	3128394	C	0.9452541379310343	If you imagine you only want a single neuron to be active for every example. Your model is going to be trying to memorize the data set to some.
3128432	3132730	A	0.999303076923077	Extent or something like this, and you're not going to have enough capacity.
3132810	3134974	C	0.8821628571428571	So, yeah, I think tuning that level.
3135012	3140030	A	0.9133657142857142	Of sparsity is certainly an important factor.
3140850	3155506	C	0.934101875	And when you look at the likelihood, if you're calling framework, typically this is balanced automatically with the likelihood itself. If you're not doing generative modeling, you just have a sparsity penalty.
3155538	3157350	A	0.9872025	You're going to want to tune that parameter.
3158490	3169450	B	0.933151379310345	Okay. They added just to clarify runaway behavior in Armina, where the network becomes unstable or chaotic due to various factors such as feedback loops, noise, or adversarial inputs.
3172270	3194222	C	0.9588668085106383	Yeah, I guess I haven't looked at this in a recurrent setting where you would get feedback loops, but I could see adversarial examples being potentially affected by your level of sparsity. The interesting point is, would you be more susceptible or less susceptible to adversary examples?
3194366	3195380	A	0.9929866666666666	I don't know.
3196390	3256120	B	0.9256563865546218	Well, sparsification projecting from a fully connected, higher dimensional model just into progressively smaller it's pretty well understood in general what the trade offs are. It's easier computations, a smaller model, sparser. The Bayes graph is going to be clearer to represent and then also it will have all of the other trade offs with false positive and negatives of generalizing. But that's why it's an iterative fit process. So I guess how does your sparsification approach balance? Does it use AIC or BIC or some other model fitting approach to determine the relevant sparsification for a given input? How do you determine in Lasso regression, how do you threshold how many how sparse you want it to be?
3257210	3281706	C	0.9436042592592595	Right, yeah, I think there's a lot of good literature on this, and even so, some people like DembaBa at Harvard. And some people I'm working with now have done these kind of unrolled iterative sparsification networks where it's like a recurrent neural network and iteratively sparsifies. And you can show that this yields.
3281738	3283680	A	0.90333	Something like relus or.
3285970	3325338	C	0.9567777108433733	Group sparse activations like we're using here in this setting. It's really just by having this construction of this T variable where we have Z on top and then it's in some effect gated by the sum of U variables in the bottom. So, w maybe I wasn't super clear about this is a matrix that is connecting. That's what defines the groups when I'm defining the group sparsity, that connects all of these U's together. And so the idea is like here.
3325424	3327386	A	0.8745900000000001	If all of one of the other.
3327408	3389902	C	0.9433346202531648	Examples, if all of your U's are not active for a given T, or if all of your U's are active for a given T, that T variable is going to be very small, right? Because your denominator is going to be very big, and that induces sparsity. So it's like constraint satisfaction. If you have a set of U's that are all small, then that that constraint is satisfied. And now Z is allowed to kind of express itself and that's what then kind of achieves the sparse activation. So this is induced by these two KL divergence terms here. These are saying like, how far is each U and each Z from a gaussian? And then through this construction of the student T variable, we're effectively constructing a sparse prior distribution just from these Gaussians. But in terms of the actual objective, the terms and the objective that we're optimizing are just these two KL terms.
3389956	3393514	A	0.9468244444444444	That are pushing it towards sparsity to some extent.
3393562	3396386	C	0.9494585714285714	And this is balanced automatically with the.
3396408	3398814	A	0.8602833333333333	Likelihood term here through the decoder.
3398862	3402658	C	0.9609321428571429	So we don't have terms that we're tuning, but we're learning the parameters of.
3402664	3406370	A	0.8481522222222222	These different encoders and then analyzing the failed urgencies.
3408490	3409240	B	0.97822	Cool.
3409770	3410438	A	0.89097	All right.
3410524	3445910	B	0.9430696000000001	Another question from Dave Douglas, who wrote speaking of gaze and illusion. Can the studies on constancies in infants be separated into lower level illusion, relative, perhaps higher level conceptual constancy? Can you read the current kind of architecture? Might the studies on constancies in infants cognitive constancies, be separated?
3446650	3457366	C	0.97442875	Yeah, probably. I'm not an expert or actually even very familiar with object permanency studies and.
3457388	3460018	A	0.9852685714285715	Infants and constancy stuff, but I think.
3460044	3487458	C	0.9540365934065931	That would be incredibly interesting to study in neural network architectures. And that was kind of some of the idea with this illusion that I was trying to model down here with this line. I don't know if I was super clear about this, but the top row is the input, and we're effectively like blocking the input for a single frame. And I wanted to see, does the network kind of encode that the thing is still there when that frame is gone? Can I still decode the presence of.
3487464	3489250	A	0.9996083333333332	The object from the neural activity?
3489750	3495318	C	0.9149447619047621	And then what is it also inferring about the motion because of the fact that it saw the bars at a.
3495324	3499720	A	0.9824554545454546	Slightly different location than from before after the frame is gone.
3501130	3516186	C	0.9459314285714286	So, yeah, I think there's definitely multiple levels to it where some would probably be much lower level and maybe long term object permanency, I would guess would.
3516208	3518140	A	0.9988175	Be significantly higher level.
3519870	3521530	C	0.9508471428571428	It just makes me think of those.
3521600	3524462	A	0.9198142857142857	Experiments with cats back in the day.
3524516	3552758	C	0.9339156818181816	Where it's like they raised them in darkness except for an hour a day. They put them in vertical world or horizontal world where they only saw horizontal lines or vertical lines, and you can see the organization of their cortex changes, like they have less receptivity to horizontal lines if they've never seen horizontal lines before. And then you take a stick and you wave it in front of their face, and if the stick is horizontal, they do nothing. If it's vertical, they're swatting at it.
3552764	3554134	A	0.8864642857142858	They'Re trying to hit it, it's like.
3554172	3555638	C	0.8380828571428571	They just literally don't hunzle bar in.
3555644	3556582	A	0.9999450000000001	Front of their face.
3556716	3568714	C	0.9244777419354839	So I think in that case, then, this is evidence of a low level efficiency in vision contributing to some sort of an illusion. So I think, yeah, there could certainly.
3568752	3571120	A	0.9683622222222223	Be some aspect to that in infants as well.
3573570	3600360	B	0.9568682692307691	One very curious point you brought up was the animate and inanimate manifold with small things being intermediate, right? What does that represent? Or is it because they're handleable or it might be an insect or it might be something that might move away just with wind or what does that say?
3601530	3714714	C	0.9545799256505569	Right, yeah, so this is work by Talia Konkel, I think was the one who discovered this organization and they tried to figure it out. I might be getting this wrong, so I recommend people to read her work on that. They call it Tripartite organization. But if I remember correctly, they did a lot of follow up work on why there's this organization and some evidence of curvature of these objects and kind of like the distance that you see objects from or like animate objects are maybe more curvy or regardless of what the actual answer is, there were a lot of different hypotheses that were stemming from properties of these objects. Maybe mid level or low level properties, more so than higher level properties. I still don't know if it's exactly been solved of whether it's like, interaction, like you said, with the objects causes the separation or the general shapes of these objects. I would bet, as with most things, it's like some combination of all of the above. But I think the interesting thing from this modeling point of view is that this is only trained on correlation statistics from the image data sets itself. This has no interaction, this has no notion of animacy. I mean, this is really just training a model on ImageNet, just images of dogs, cats, boats, whatever, and yet it still achieves this type of organization. So there's some sort of it could be semantic characteristics, right? We have a network that can classify boats versus dogs versus 20 other breeds of dogs, but it might also have.
3714752	3719514	A	0.874671	Some correspondence with lower level image statistics as well. So?
3719552	3719814	C	0.86	Yeah.
3719872	3721600	A	0.8942162499999999	I don't know. I guess he's my answer.
3723970	3724574	C	0.54	Yeah.
3724692	3763340	B	0.9435142465753423	Provocative analogy was the translational shift in the MNIST in the handwriting recognition setting. What are the translational shifts that exist today? What's the three pixel example? Is that some prompt engineered attack on an Lom or something? Or something. A special character being inserted, or some overlay on an image that we can't even detect. So what do you think those challenges are and what are ways that we can pursue that?
3764910	3786206	C	0.970857192982456	Yeah, absolutely. I mean, I think kind of the way I was thinking about it is like these symmetry transformations. If you're thinking about language models, you can imagine a symmetry transformation is just like replacing a word with a synonym or something. You have the sentence to us means the exact same thing, but now suddenly.
3786238	3788370	A	0.9995700000000001	The model is going to respond very differently.
3791110	3795266	C	0.8296442857142858	Or like translation between languages, this can.
3795288	3796754	A	0.9999714285714286	Be seen as a type of transformation.
3796802	3804342	C	0.9989328571428572	It preserves the underlying meaning of the input to us, but to the model.
3804396	3805510	A	0.993785	It looks completely different.
3805580	3832894	C	0.949444615384615	And we would like to have models which behave in a predictable way with respect to these types of transformations, because I think humans behave very predictably with the type of these transformations. And when we're dealing with AI systems, we expect them to also behave that way. And I think that's part of what causes a lot of challenges interacting with these systems. And I kind of tried to do a rough cheeky demonstration of that with.
3832932	3835200	A	0.8996116666666666	This bear and squares and stuff.
3836310	3844498	C	0.9999700000000001	We expect it to be able to do something simple like this because we think most humans could, and yet it doesn't. And if you imagine this is a.
3844504	3848200	A	0.9737654545454546	Critical scenario where you expect this, then that's a big problem.
3848810	3853382	C	0.9563242857142857	How do we handle that? I think that's kind of what I'm searching for.
3853436	3854200	A	0.99499	I think.
3856570	3881166	C	0.9212741304347823	My direction I'm taking it is look more simple, kind of like bottom up building blocks of neural network architectures or algorithms that kind of yield these emergent structural properties. And I think that's a much more generalizable way. Rather than building something on top of.
3881188	3884894	A	0.9991814285714286	What we already have, I think that's.
3884942	3886466	C	0.9919257142857143	Something that would scale much better and.
3886488	3888580	A	0.9348571428571428	Also matches more what the brain does.
3890710	3910970	B	0.9502759649122805	Very cool. One kind of implementational question. What are the computational requirements of just running this, or what's the day to day like of being a student or researcher running variants of these? Like, do they use terabytes of data and you're using large computation? Or is this something that people can run on their own laptops?
3911870	3958886	C	0.9579333599999998	I think almost everything I presented today can be run locally. So this stuff is super simple. You can run you can run it on. Your laptop. If you want to train and experiment with different things, it's going to be pretty slow. So I'd recommend some commercial GPU. I run pretty much everything on like Nvidia 1080s, pretty old, pretty cheap, but they have twelve gigs of Ram or whatever, and it's kind of more than enough for these models. Gigabytes of Ram I think one thing that some people think is weird is I do most of my experiments on stuff like MNIST. So it's 32 x 32 pixel images because I can train it small. And locally, if you want to do.
3958908	3960790	A	0.7280733333333332	Stuff, my experiments are on mnists.
3961310	3970182	C	0.9552003448275862	If you want to do stuff like this, these are much more complicated. This Hamiltonian Dynamic suite here, you're getting into bigger models that are running across multiple GPUs.
3970246	3973710	A	0.9014257142857144	And so here I was using a cluster to run these types of models.
3974290	3980986	C	0.9642081818181819	But I say most of the single machine with the GPU is more than enough. Or even just like in a collab.
3981018	3983422	A	0.999035	Notebook, something like that.
3983556	4005718	C	0.9739030158730159	If you want to train something on ImageNet, it gets more complicated and you need at least one GPU, ideally more. But yeah, I don't do a whole lot of big scale stuff yet. I think it's certainly interesting and there's definitely a lot more you can do there. But for some of these kind of simpler or more fundamental questions, I don't.
4005724	4007160	A	0.9938985714285715	Know what you want to call it.
4007630	4009900	C	0.9759414285714286	A smaller machine is nice and fast.
4012430	4055960	B	0.93256	Cool, useful. All right. I'll read a comment from Dave recalling Bert Devries'comment during the Applied Active Inference Symposium about the desirability of spending less effort or ATP on foraging or control situations where we don't need much precision. I don't know if you listen to this, but Professor DeVries mentioned about variable precision models and how they could be used to enable different features of generalization and actual structural course training as well as reduced computational requirements. Does he have any suggestions on how to introduce this distinction into active inference theory? What kinds of experiments could winkle this out?
4058330	4065420	C	0.9569325	Oh, wow. Yeah, that's something I don't think I have too much intelligence to say about, to be completely honest.
4069150	4070010	A	0.87515	Hmm.
4071470	4110026	C	0.9382136144578308	It's super interesting question because I think the intuition makes a lot of sense to me that you're talking about, if I understand correctly, variable rates of vision when you're encoding or in your model in general, doing computation that somehow has an impact on your future performance as a relation to some energy store. I think if you wanted to build this into an active inference system, you would need to have really an embodied system where the agent has some notion.
4110058	4113440	A	0.9705312500000001	Of energy, like an internal energy store, and.
4115250	4122434	C	0.9387890000000001	Something that is trying to conserve while it's performing its actions and running out of energy would need to mean.
4122472	4124034	A	0.925624	Something bad for the agent.
4124232	4126978	C	0.9458728571428571	And then maybe you could observe kind.
4126984	4138742	A	0.9531147619047619	Of an emergent reduction in encoding precision or something like this as the agent is trying to learn to act more.
4138876	4140406	C	0.9998814285714285	Effectively, you might have to give it.
4140428	4148390	A	0.8974571428571428	An ability to control its precision. Yeah, like I say, out of my thoughts.
4149070	4170560	B	0.9500400000000001	Okay. On this slide right here, first, very cool image. It's kind of like a digital Jackson Pollock. If it were a simpler input data size or just reduced complexity of patterns, or if it were an increased complexity, how would this image look different?
4171830	4270296	C	0.9528864864864861	Yeah, so I did some experiments trying to change these orientation columns and basically changing the parameters of the model. You can get these columns to be bigger. You can get them to not have very similar structure to what we see in the humans, where you can get them to have more bands of activity. And it also, like you said, it depends on the data set that you're using. If I use really simple sinusoidal gradings as input, I get something like this. I get something that's a little bit more rotational, curvy, higher entropy. So I think these are all interesting things if you want to study the emergence of this type of organization in a natural system. If you have a model that now yields different organization for different settings that see, okay, then what settings best match our observed data? I can send those around if you're interested. But I think one other interesting point there is that the different animals and types of orientation selectivity and different numbers of pinwheels. Some animals don't have it at all. I think maybe mice, if I'm correct, have this kind of they call it salt and pepper selectivity. So it's basically random. You don't have any sort of, like, topographic orientation selectivity. So there is evidence that different systems.
4270328	4273810	A	0.9610779999999999	Do this differently, and it's interesting to figure out why.
4274740	4330636	B	0.9572484615384611	Yeah, this very cool. It reminds me of, first, the reaction diffusion, space and time. So it's actually possible that a region might have no activity from a given granularity, like if it was being looked at at fMRI spatial and temporal timescale. But if the pockets of activity are slower, faster, then that measurement is going to not be different than noise. It'll all have been averaged out. So then there might be some interesting data sets that do actually have a lot of richness, but then for one reason or another, it just was averaged out over because it wasn't being connected.
4330668	4348656	C	0.9669625862068962	To you or something like this. You really need to go at the single trial level. You need to have high enough spatial resolution such that it satisfies Nyquist frequencies. And this just is something that people didn't do for a long time, especially if you're doing single electrode recordings. You're not going to see a traveling wave.
4348688	4349940	A	0.90251	You're going to see oscillations.
4350680	4355976	C	0.9295614999999999	You need, like, multi electrode arrays. And basically they're saying, okay, yeah, now that we have the technology to do.
4355998	4359592	A	0.9401744444444444	This, it's much resists that we didn't see before.
4359646	4372252	C	0.9758236842105261	And potentially, this is an explanation for a lot of the noise that we were seeing before. Maybe it really is just traveling waves. So yeah, I think there's a lot to be done in the future with.
4372306	4375920	A	0.75364	Increased abilities for recording.
4376420	4385330	B	0.9414233333333334	That's very cool. Well, any final thoughts or questions or where are you going to take this work?
4386260	4422140	C	0.9369353333333331	Yeah, no. Thanks for having me. Hopefully in the active infrastructure direction. I would love to. I think it'll be super fun. So yeah, I'm not really sure. I'm looking at maybe music right now, looking at other kind of crazy directions. I don't want to sound too crazy, but I'll go down. Yeah, a lot of things. So one thing that's coming up, something we submitted to Neurops is studying memory with traveling waves.
4423200	4424876	A	0.9975714285714287	So that paper just came out on.
4424898	4430016	C	0.996745	Archive today of how waves are really good at encoding long term memories, which.
4430038	4434560	A	0.9898086666666667	I think is super interesting. So I might go a little more in that direction.
4435800	4474030	B	0.9237886250000005	Sounds good. And yes, would be very exciting to see action come into play when there was the neurons that stayed active even as the dog's feet were moving. There's a lot of action sequences, like throwing a baseball, and then it goes, and it's like there's something about that action that's continuing to influence it's to having, like, a deep temporal representation of alternative actions. And then the variational auto encoder is already basically the right anything like that.
4475120	4476444	A	0.8477966666666666	Really appreciate it.
4476562	4478670	B	0.9931042857142858	All right, thank you. Till next time.
4479520	4481080	A	0.9973924999999999	Thanks so much. Bye.

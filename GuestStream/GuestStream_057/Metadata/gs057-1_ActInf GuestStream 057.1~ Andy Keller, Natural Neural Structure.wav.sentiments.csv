start	end	speaker	sentiment	confidence	text
4050	4600	A	0.546256959438324	You.
6570	7702	B	0.872096598148346	Hello and welcome.
7836	11986	B	0.7004976272583008	It's September 18, 2023, and it's active.
12018	15826	B	0.9153308272361755	Guest stream 57.1 with Andy Keller.
15938	21250	B	0.8554250597953796	We're going to be talking about natural neural structure for artificial intelligence.
21410	24594	B	0.9043112993240356	There will be a presentation followed by a discussion.
24642	28618	B	0.716742217540741	So if you're watching live, please feel free to write questions in the live chat.
28754	31022	B	0.9247612953186035	Otherwise, thank you Andy, for this.
31076	34190	B	0.9879838228225708	Really looking forward to it and to you for the presentation.
36370	37854	C	0.9301478266716003	Yeah, thanks so much.
38052	39006	C	0.9428794384002686	Thanks for having me.
39028	43314	C	0.9901887774467468	I'm super excited to be able to present this stuff with Active Inference group.
43352	45794	C	0.943861186504364	I'm a fan and very interested, so.
45832	46530	A	0.4921221137046814	Hopefully.
48550	51460	C	0.6993448734283447	Get to have a good discussion and see what you guys think about it.
51990	53470	C	0.7578305006027222	So my name is Andy.
53550	61926	C	0.8653042316436768	I'm finishing up my PhD, supervised by Maxwelling at the University of Amsterdam, starting a postdoc at Harvard after this.
62028	73302	C	0.630046010017395	So I'll start out just talking about the goal of my work in general is to try to bring modern artificial intelligence closer to more human like generalization.
73446	84606	C	0.851559579372406	And so what we mean by this is maybe some sort of structured generalization or maybe more familiar to the active inference community, like a structured world model which we believe that humans have.
84708	91470	C	0.7805507183074951	And the way that we propose to do this is by integrating natural neural structure into artificial intelligence.
92210	96130	C	0.8052069544792175	So first let's define what we mean by structured generalization.
96550	103842	C	0.6480921506881714	So I think it's fairly uncontroversial to say that modern machine learning generalizes beyond its training sets in the traditional sense.
103976	114146	C	0.5865371823310852	So for example, even the earliest artificial neural networks, multilayer perceptrons, could be trained on data sets of images like this and achieve high accuracy.
114338	122950	C	0.6825507879257202	Then, when they're presented with a held out test set of images that they've never seen before, they can still classify them relatively easily with the same level of accuracy.
123030	125526	C	0.5003347992897034	And this is what we typically call generalization.
125718	133194	C	0.6233516931533813	However, even fairly early on it was noticed that these systems really struggle with small shifts or deformations applied to the images.
133322	142346	C	0.8061771392822266	For example, if so, you think, why is this surprising?
142538	150338	C	0.4813235104084015	And I argue it's really due to our innate ability to perform this type of structured generalization that this example is a failure of.
150424	159126	C	0.7274492383003235	So for example, this shift is nearly imperceptible to us and we handle it automatically, whereas in these systems it's very clearly a major problem.
159228	171610	C	0.8587896823883057	So in words, we can say that structure generalization is a generalization to some symmetry transformations of the input, or in this case, the symmetry transformation is a small shift that leaves the digit class unchanged.
171950	178474	C	0.7657473683357239	So the obvious question then is what precisely do we mean by this natural structure and why do we think that.
178512	180750	A	0.7158110737800598	This would help us with these settings?
181570	185150	C	0.883898913860321	So first, let's talk about what we mean by natural neural structure.
185730	192458	C	0.532498300075531	One way to talk about structure or any type of bias in a system is an inductive bias.
192554	198574	C	0.7025464177131653	And so an inductive bias can loosely be defined as an apriori restriction of a set of realizable hypotheses.
198622	200290	C	0.803514838218689	When you're doing model selection.
200870	207734	C	0.7924716472625732	More colloquially, we can call this something like before seeing any data, it's a restriction of what and how you can learn.
207852	215430	C	0.6483773589134216	So very broadly, this can include anything from model class to optimization procedures or even hyperparameters.
215930	221738	C	0.7518671154975891	And in some sense they really define what is possible to learn.
221824	229158	C	0.5065203905105591	And it defines generalization in that you actually can't generalize beyond a training set without having some inductive biases.
229254	230906	C	0.5208023190498352	And this is explained more thoroughly in.
230928	233314	A	0.867293119430542	This paper by David Wolford.
233462	245506	C	0.5851529240608215	So what we mean by natural inductive biases then is biases that stem from the restrictions and limitations that are faced by natural systems, by the nature of having to live in the real world.
245608	251410	C	0.7681995034217834	For example, the brain has many efficiency constraints and physical constraints by nature of its construction.
252070	260706	C	0.5259830951690674	And following this logic, then these constraints are really playing some role in our generalization abilities which currently exceed modern artificial intelligence.
260738	262038	A	0.8299434781074524	As we'll go into next.
262204	269078	C	0.8360649347305298	So in this talk, I'll be focusing specifically on two types of structure which my work has studied.
269254	273254	C	0.8302584886550903	These are topographic organization and spatiotemporal dynamics.
273382	282046	C	0.6384774446487427	And before I go into my work, I'll give a short example for why I believe that natural structure may be useful to achieve the structure generalization that.
282068	283360	A	0.7747987508773804	I was talking about before.
284210	298070	C	0.8175297975540161	So the first example comes from Fukushima's neocognitron architecture from the 1980s, which was actually built to directly address the problem of robustness to these small shifts in deformation.
298250	307746	C	0.805021345615387	So in the paper he writes about inspiration from pupil and weasel's measurements of hierarchy and pooling in order to achieve robustness to these distortions.
307858	315190	C	0.8774802088737488	And so if you look at the figure he writes u sub s one, U sub C one and these stand for simple and complex cells.
315610	323606	C	0.7967203259468079	And so this is a fairly radical approach at the time, but it really served to improve robustness and shifts that were plaguing these early artificial neural networks.
323638	334250	C	0.7196691632270813	And over time these ideas were simplified and abstracted and obviously yielded the convolutional neural networks that we know today, which ultimately drove the success of the Deep Learning Revolution.
334410	340106	C	0.5262709856033325	So this is really an example of a natural inductive bias which achieved structured generalization.
340218	355190	C	0.7642636299133301	So for our research, it's really of utmost interest to try and understand what makes these models work so well and see if this principle can potentially be generalized to cover more abstract transformations and symmetries.
357050	361510	C	0.8595298528671265	So what makes a convolution achieve this structure generalization?
362010	369234	C	0.8855296969413757	Intuitively, you can see this is done by applying the same filter or feature extractor at various spatial locations.
369362	374086	C	0.885085940361023	So here we see a single convolutional filter being applied at all locations of an image.
374198	384378	C	0.7100421190261841	This means that no matter where your input is, whether it's kind of in the middle of the image or on the right, you'll have the exact same features, with one exception, they'll be equivalently shifted.
384554	387994	C	0.8457397222518921	So mathematically, this type of a mapping is called a homomorphism.
388122	392270	C	0.8338688611984253	It preserves the algebraic structure of the input space and the output space.
392420	394694	C	0.8542543053627014	In this case, it's with respect to translation.
394842	408882	C	0.7172093391418457	And at a simple level, something like that will be important to remember for the rest of this talk is that we can verify homomorphisms of our feature extractor if we can see that there's this commutation with the transformation, this commutative diagram.
409026	416360	C	0.8381681442260742	And so we can write this also algebraically by showing that the feature extractor f commutes with the transformation operator T.
416890	427494	C	0.7419102787971497	And basically what we want is there to be no difference between first extracting the features and then performing the transformation, or performing the transformation and then extracting the features.
427622	436078	C	0.5029743313789368	So the challenges to date is we don't really know how to construct homomorphisms with respect to more complex transformations that we see in the real world.
436244	441310	C	0.5006064176559448	For example, our brain is able to handle changes in lighting and season naturally.
442450	445738	C	0.8484795093536377	So here we see lighting on a person's face or the change of seasons.
445754	448434	C	0.7612072229385376	We can tell it's the same face or the same road.
448632	451822	C	0.5420553088188171	But we don't know how to build models which respect these transformations.
451886	457080	C	0.5712558627128601	And so it makes us hard to build systems which handle them in a robust and predictable way.
457450	468738	C	0.525844395160675	To give an even more abstract example of what I mean by this and the potential negative repercussions of models which don't handle symmetry transformations, consider modern text to image generation programs.
468834	474710	C	0.8767445683479309	So in this example, I asked Dolly Two to generate image of a teddy bear on the Moon.
474790	476554	C	0.687799870967865	And it does this incredibly well, right?
476592	478218	C	0.6291214823722839	Probably better than I could.
478304	482198	C	0.8340674042701721	It has texture fur, incredibly detailed.
482294	490910	C	0.5653015971183777	However, if I ask it to do something which I see as conceptually simpler, such as draw a blue cube on top of a red cube, it fails to do this.
491060	496850	C	0.4858905076980591	And to me this seems unintuitive since the second task seems significantly easier.
497270	504174	C	0.6883286833763123	But what I'm arguing is that the reason that this is surprising is precisely the same reason that the MNS translation example was surprising.
504302	517762	C	0.7688918113708496	There is this symmetry transformation happening here, namely the transformation between these complex objects of a teddy bear and the Moon and these simple objects of cubes which we intuitively expect the network to be able to handle and respect.
517906	519474	C	0.5249077677726746	And we see that it doesn't.
519602	537280	C	0.534235954284668	So just like how Fukushima's work showed that these natural structure of hierarchy and pooling of our visual system are effective for making generalizations to small transformations, I argue that potentially higher level structure may be necessary to fix these abstract generalization problems.
538130	552370	C	0.8871423602104187	And so the question then that I'm studying and that I'm asking is what might this structure be and how do we implement this in an artificial neural network architecture that can actually be used for performing computation?
554810	560950	C	0.8673838376998901	So, to begin to answer that, I'll jump into my first line of work on topographic organization.
562250	569218	C	0.8497487306594849	So topographic organization is observed widely throughout the brain from primary visual cortex to higher level areas.
569314	576202	C	0.8432509899139404	And it can very loosely be described as this property that neurons which are close to one another tend to respond to similar things.
576336	587626	C	0.8879334330558777	For example, on the left we show the color coded preference of each neuron in the Macaque primary visual cortex as a response to oriented lines and we see this smoothly varying set of selectivities.
587818	595810	C	0.8837495446205139	Another type of organization is known as retinotopic organization where nearby neurons in the visual cortex tend to respond to nearby receptive fields.
596310	599758	C	0.7823407649993896	However, this organization isn't limited to these low level features.
599854	605794	C	0.8165605068206787	It extends to more complex features such as those present in faces or objects or places.
605922	615030	C	0.8092451691627502	And this relates to the so called functionally specific areas of the brain such as the fusiform face area FFA and the perihepocampal place area PPA.
615370	639440	C	0.6067546606063843	So in this work, the main idea again is that perhaps this topographic organization in some sense which is intimately related to the convolution operation and Fukushima's architecture we can maybe generalize the benefits of this to more abstract transformations in other words, learn how to build more complex homomorphisms that we can't do analytically right now.
640770	660898	C	0.5752531886100769	So just to show that we're not completely insane with this idea there is some prior work in this domain from people such as Honin Yanlaka Apohevarnan in the early ninety s and two thousand s and they studied how topographic organization may be useful for learning invariances mostly in linear models.
660994	671814	C	0.7947210669517517	So the question for us when we entered this space is what is the most scalable abstract mechanism that can be leveraged from these approaches which we can integrate into modern deep neural network architectures?
671942	693730	C	0.8686532378196716	And ultimately we settled on a generative modeling approach which I think might be interesting to the people in this community which then allows us to relate it more closely to topographic independent component analysis with the basic idea being that we can learn a topographic feature space by imposing a topographic prior distribution over our latent variables.
694390	712742	C	0.7907608151435852	So just to give a brief background I assume most people are already familiar with this but the kind of general assumption is that the brain is a generative model and this idea in some sense can be attributed to Helmholtz from the 19th century where he said that what we see is the solution to a computational problem.
712876	717606	C	0.8084692358970642	Our brains compute the most likely causes from photon absorptions within our eyes.
717788	724390	C	0.8507332801818848	And so as an example, if I show you this image you immediately recognize it as a globe with some curvature.
724550	728570	C	0.6818236112594604	However, it could just as equally be a disk with a distorted perspective on it.
728640	732262	C	0.7290610074996948	So this is how we get optical illusions or our images.
732326	738062	C	0.704215943813324	So like this one, your brain infers that there is a cube here because of the structure but really it's just.
738116	739440	A	0.6952506303787231	A flat piece of paper.
739810	745954	C	0.8703497648239136	So you can think of this generative model aspect as kind of like an inverse graphics program.
746152	756002	C	0.9003299474716187	In the program, the abstract properties of the sphere are known the position, the size, the lighting and these are used to project the sphere to create the 2D image that is rendered.
756146	769130	C	0.5003652572631836	So in effect, what Helmholtz and others are saying is that as a generative model, the brain is actually trying to invert this generative process and doing inference and infer the underlying causes of our sensations.
769550	780350	C	0.7250838279724121	So the reason I'm kind of belaboring this point is that there's a lot of talk of generative models today, and I'm not necessarily just talking about generating images or pretty pictures.
780930	786080	C	0.5209145545959473	I really want to mean a framework for unsupervised learning.
787890	792926	C	0.8866599202156067	So then to get a little bit more into the details, what do I mean by a topographic prior?
793038	800740	C	0.8736491799354553	So generative models are typically described as a joint distribution over observations, x and latent variables, which we'll call Z.
801670	807074	C	0.9026089906692505	And this is typically factorized, or one way that this is done is factorized in terms of a prior p of Z.
807192	811990	C	0.8567922711372375	And this true generative model conditional generative model p of X given Z.
812140	819142	C	0.8143989443778992	And so one way that we can think about this is that the prior can be seen to encode relative penalties for each type of code that is produced.
819206	825180	C	0.8481128811836243	When we invert our generative model, this is called computing the posterior e of Z given X.
826270	838990	C	0.8039023280143738	And so to develop a topographic latent space, we want to introduce some sort of a topographic prior, which this topographic ICA work showed is equivalent to something like a group sparsity penalty.
839490	843866	C	0.7658748626708984	So people might be familiar with typical sparsity penalties from independent component analysis.
843898	847300	C	0.5829451680183411	You want your activations to be sparse, meaning many of them are zero.
848390	849794	C	0.7998585104942322	So that could look something like this.
849832	853342	C	0.5481281280517578	You have a bunch of blue squares that are active, but most of them are not active.
853486	866482	C	0.8231380581855774	But specifically, with a group sparsity penalty, we want these priors to assign lower probability to these distributed sparse activations and higher probability to these grouped, densely packed representations.
866546	872620	C	0.6557028293609619	You can also think of this like a higher penalty when things are spread out, a lower penalty when things are closer together.
873470	877818	C	0.8077266812324524	So again, this can be written abstractly like this.
877904	883598	C	0.8718085289001465	But I want to make clear that each one of these squares here represents kind of a neuron in our model.
883684	885770	C	0.8399640321731567	And they're organized in this 2D grid.
885850	890190	C	0.8335318565368652	So when we're talking about grouping, we really mean grouping in that 2D topology.
891170	911030	C	0.9193329811096191	So one thing that's really interesting and kind of important is that these priors don't just give us topographic organization, but they've also been noted by people or studied by people like Erosimicelli and Bruno Olshausen to actually fit the statistics of natural data better, specifically natural images.
911530	920842	C	0.7481567859649658	They've shown that using this type of a prior, you actually get a sparser set of activations, meaning that the prior fits the true generative process a little bit better.
920976	927130	C	0.5617506504058838	And as we're aware, the brain has a high degree of sparsity and this is believed to be very relevant for efficiency.
928670	940560	C	0.8927858471870422	So to get a little bit more into the details, to implement this type of a group sparse prior, we use a hierarchical generative model and this is basically introduced by some of the topographic ICA work.
941330	949714	C	0.8876668810844421	The idea is that you have a higher level latent variable U which simultaneously regulates the variance of multiple lower level variables T.
949832	951890	C	0.7998141050338745	And this is how we get group sparsity.
952470	960402	C	0.8869374394416809	Then to get topographic organization, you can have multiple of these latent variables U slightly overlapping with their fields of influence.
960466	967654	C	0.6822391748428345	So their neighborhoods, we can call them, and this will give you this smooth correlation structure you're after.
967772	969398	C	0.835143506526947	So get the intuition for this.
969484	979274	C	0.6695739030838013	You see that this variable T over down on the bottom here is not getting any input from this U on the top, but it is sharing a U variable with this T in the middle.
979392	984826	C	0.7520250678062439	So it's like they're sharing variance, they're sharing some components with their neighbors, but not all components.
984858	989200	C	0.8115407824516296	And that's really due to this local connectivity of these higher level variables U.
990770	996046	C	0.8300187587738037	So to keep it simple about how we use this generative model, let's go back to a single U variable.
996158	1009110	C	0.5846912264823914	And the challenge in this type of an architecture which made it difficult for many years is how do you infer the approximate posterior over these intermediate latent variables in this hierarchical architecture?
1009530	1011266	C	0.6783310174942017	This is not super straightforward.
1011378	1015298	C	0.9011620283126831	So prior works have used heuristics developed for linear models.
1015394	1020198	C	0.5462941527366638	And in our work we found that this really didn't extend to modern neural network architectures.
1020374	1027130	C	0.8173940777778625	So really our insight is to leverage a factorization, a specific reprometerization of this distribution.
1027870	1044866	C	0.8272337913513184	And so this reprimmatrization specifically is achieved by defining the prior to be what's known as a Gaussian scale mixture, meaning that our conditional distribution of T given U is actually a normal distribution where the variance is defined by this variable U.
1044968	1054606	C	0.8376825451850891	And for certain choices of U, this distribution is indeed sparse and encompasses a range of distributions such as Laplacians and student T distributions.
1054718	1063974	C	0.8922356963157654	One way of defining it is a Gaussian scale mixture admits a particular repromaturization in terms of independent gaussian random variables Z and U.
1064092	1078726	C	0.5003793239593506	So specifically, then we see that this T variable, which was originally fairly complex, is actually just a product of a bunch of gaussian random variables which we now know how to work with much more efficiently in generative models.
1078838	1089786	C	0.8622327446937561	And specifically what we're going to do is so that we can actually get approximate posteriors for U and Z separately and then do a deterministic combination of them in order to compute our topographic.
1089818	1090590	A	0.572387158870697	Variable T.
1090660	1092480	A	0.7309466600418091	And this is much easier to do.
1093090	1112066	C	0.6729699969291687	So, without going into too many details, the method that we decide to use is what's known as a variational auto encoder, which leverages techniques from variational inference to derive a lower bound on the likelihood, allowing us to parameterize these approximate posteriors with powerful nonlinear deep neural networks and optimize them with gradient descent.
1112258	1114918	C	0.5444182753562927	This is going to be familiar to the active inference community.
1115084	1119078	C	0.6394074559211731	But really what we've done is instead of having a single encoder and decoder.
1119094	1124582	C	0.8938781023025513	As is typical Bayes, we now have two encoders, one for U and one for Z separately.
1124726	1130510	C	0.859201967716217	And then we combine them in this deterministic manner to construct our topographic T variable.
1130850	1143958	C	0.8593550324440002	If you see that this is actually the construction of a student's T distribution from Gaussians and then we do this before decoding and then maximize the likelihood.
1143994	1145182	A	0.6926141977310181	Of the data altogether.
1145326	1154994	C	0.6825585961341858	So this is the elbow, the evidence lower bound abound on the likelihood of the data and is actually very similar to the variational free energy that is.
1155032	1156760	A	0.8738359212875366	Used in the active inference community.
1158250	1168774	C	0.8884862065315247	So with these details out of the way, what's really interesting is what happens when we train this generative model which has relatively simple group sparsity penalty in its latent space.
1168972	1171626	C	0.8854925036430359	And we want to look at kind of what it's learning in terms of.
1171648	1173462	A	0.7104933857917786	Its organization of features.
1173606	1175994	C	0.6664718389511108	And first we start with the simplest possible data set.
1176032	1179930	C	0.9257689118385315	We have a black background with white squares at random XY locations.
1180270	1196782	C	0.8834802508354187	If we train our auto encoder with this group sparsity penalty on it and then we look at the weight vectors of our decoder which we're plotting in blue here, again organized in this 2D grid, we see that indeed they learn to be organized according to spatial location.
1196926	1207990	C	0.8708804845809937	So this can be seen as similar to convolutional receptive fields where the receptive field of each neuron is really given by the kind of inputs at its location.
1208970	1219126	C	0.5157435536384583	And this makes sense intuitively from the group sparsity perspective since for any given region, which we highlight, like in yellow here, the filters in a given group are much more highly correlated.
1219158	1222918	C	0.835503101348877	They have these overlapping receptive fields than other random locations.
1223014	1234910	C	0.8515062928199768	So essentially we see that our model is learning to cluster activities together in sort of a simulated cortical sheet according to the correlations in the data set.
1235060	1245150	C	0.6970626711845398	So instead of in convolution where you're actually doing weight tying and you're manually specifying I want to copy this weight everywhere, you can maybe think of this as like approximate weight tying.
1245910	1250098	C	0.8340758681297302	And really we're learning this from the correlation structure of the data set itself.
1250264	1257006	C	0.8146430253982544	And just to give a little bit more of a biological inspiration for this, we know that retinotopy is present in the brain.
1257038	1261890	C	0.6677065491676331	This is an example of retinotopy and the Macaque visual cortex.
1262050	1273130	C	0.8502309322357178	And you can see if you show the Macaque an image like this, it gets projected into this topology preserving space actually on the surface of the cortex.
1273710	1283120	C	0.8250429630279541	So the idea is that topographic organization and even learn topographic organization is preserving the input correlations of our data set.
1285570	1291210	C	0.9258018136024475	And potentially this may be beneficial for generalizing these ideas a little bit further.
1291290	1300478	C	0.5922448635101318	So like I said at the beginning, it would be even better if we could just learn something more than just convolution, maybe more complicated equivalences.
1300654	1303026	C	0.7475632429122925	So how do we do that?
1303208	1310454	C	0.60675048828125	One thing that's clear in natural intelligence is that we don't exist in this world of IID frames, right?
1310492	1314226	C	0.809810996055603	We exist in a world of continuous sequences of transformations.
1314338	1320022	C	0.7075759172439575	So maybe we can extend our model to this setting to learn, observe transformations.
1320166	1322570	C	0.8403446078300476	This is the idea of temporal coherence.
1323230	1329594	C	0.894782543182373	So what would happen if we just simply extended our previous framework over the time dimension, right?
1329632	1338958	C	0.7541654706001282	So instead of just grouping, saying we want our neurons to be group sparse in terms of spatial extent on the cortex, we actually want them to be group sparse over time.
1339044	1343506	C	0.8546545505523682	Meaning that if one set of neurons is active now, we want that same set of neurons to be active into.
1343528	1344820	A	0.7080593109130859	The future as well.
1346470	1353614	C	0.5036500096321106	If we kind of intuitively think about this, we see that this is actually more encouraging invariance than equivariance.
1353662	1360918	C	0.8446555733680725	A way to understand this is we're saying we want the same neurons to be active constantly, but the input transformation is changing, right?
1361004	1363510	C	0.6053605079650879	The feet of this little fox are moving.
1363660	1371754	C	0.8056809306144714	So if the same neurons are coding for the same thing over and over again, but the feet are moving, those neurons are going to learn to be invariant to the motion of that leg.
1371792	1373260	A	0.7658249735832214	Of this dog, for example.
1373790	1375580	C	0.6283095479011536	So instead is that.
1377390	1379100	A	0.9233339428901672	I went the wrong way here.
1381090	1388526	C	0.8559929728507996	So instead, our insight was that this group Sarcity could instead be shifted with respect to time.
1388628	1398142	C	0.672611653804779	So this would mean that sequentially shifted sets of activations would be encouraged to activate together, and then our latent space would really be structured with respect to the observed transformations.
1398286	1407000	C	0.7724586129188538	So you can see here that rather than the same set of neurons being active at all time steps, it's really a sequentially permuted set of neurons that we're grouping together in this sparse way.
1408010	1419240	C	0.7017799615859985	And then this allows us to model different observations over time, but they're still connected in terms of learning a transformation and preserving this correlation structure of the input data set.
1419930	1425402	C	0.6325527429580688	So if we put this together into our topographic BAE architecture, you can get something that looks like this.
1425456	1427702	C	0.8257807493209839	You see that we have an input sequence.
1427766	1434158	C	0.8831092715263367	We're again encoding a Z variable and then multiple U variables in the denominator here.
1434324	1439278	C	0.8412752747535706	And then each one of these U variables is shifted kind of like we were showing before.
1439364	1442266	C	0.8659279942512512	In order to achieve this shift equivalent.
1442298	1443706	A	0.8118016719818115	Structure that we're looking for.
1443828	1449374	C	0.8748676180839539	When we combine these in this student T product distribution, we get a single latent variable.
1449422	1451938	C	0.8611484169960022	This is now our topographic latent variable T.
1452104	1457990	C	0.7761958837509155	And now that we have this known structure in our latent space, you can think of it like a structured world model.
1458140	1460422	C	0.604684054851532	We know how to transform this latent space.
1460476	1466850	C	0.9022484421730042	In this case, it's by permuting these activations around these circles, doing like a cyclic roll, a cyclic shift.
1467010	1470998	C	0.6814509034156799	We know that this is going to correspond to our learned input transformations.
1471174	1478950	C	0.8818599581718445	And we can verify that by saying, okay, what if I continue this input transformation, the true transformation in the data set, which is a rotation.
1479110	1485210	C	0.8605359792709351	And then I compare that with how I've done my role in my latent space by moving my activations around in my brain.
1485370	1488830	C	0.6210997700691223	And then we decode and we see that we get the exact same thing.
1488980	1495490	C	0.8770968914031982	And so this is demonstrating this commutivity property that I was talking about before for verifying homomorphism.
1496790	1503962	C	0.8658133745193481	And so to measure this a little bit more quantitatively, we can measure what's called an equivalence loss.
1504046	1518314	C	0.8444000482559204	So this is really the quantification of this difference between our rolled capsule activation, our rolling in our head, versus watching the rolling unfold, watching the transformation unfold before us.
1518432	1524358	C	0.5971808433532715	So we see that topographic VAE achieves significantly lower equivalent error.
1524454	1535914	C	0.6496194005012512	This bubble VAE is what I was talking about before, where it's learning invariance, so it doesn't have the shift operation and then the traditional VAE kind of has no notion of organization or temporal.
1535962	1537950	A	0.9516129493713379	Component, so it performs very poorly.
1538690	1542378	C	0.8503194451332092	In addition to this, we see that the model is a better generative model of sequences.
1542474	1548434	C	0.6045883893966675	It just gets a lower negative log likelihood on the data set.
1548472	1551806	C	0.49073097109794617	So it's better able to model this data set because it has a notion.
1551838	1553650	A	0.6909523606300354	Of the structure of the transformations.
1556310	1559234	C	0.8098916411399841	We can test this on multiple different transformation types.
1559282	1576518	C	0.9005638360977173	On the top row we're showing the true transformation, we hold out these grayed out images and then on the bottom row we encode and then we just kind of roll our activations around and we keep decoding to see what the model has learned as the current transformation that's being observed.
1576614	1582860	C	0.5256943702697754	And we see that it can basically perfectly reconstruct these elements of the sequence that it's never seen before.
1583630	1590366	C	0.8392508029937744	Additionally, with images that are from the test set that it's never seen before, simply because it knows what the transformation is that it's currently encoding and it.
1590388	1592290	A	0.8396614193916321	Can generalize that to new examples.
1593990	1597342	C	0.8263208866119385	So the takeaway from this part is really topographic organization.
1597406	1604900	C	0.8991817831993103	We showed that it preserved input structure and now we're showing it can potentially improve efficiency and generalization as we would hope.
1606230	1618650	C	0.9251514077186584	So, finally, something that surprised us and I thought was potentially the most interesting is that these transformations that are learned by our model actually generalize the combinations of transformations that were not seen during training.
1618800	1643140	C	0.7370442152023315	So, for example, despite only training on color and rotation transformations in isolation, if the model is presented with a combined color rotation transformation at test time, we see that it's able to completely model and complete these transformations perfectly through the capsule role, implying that it's learned to factorize represent to these different transformations and it can flexibly combine them at inference time.
1644630	1652290	C	0.8037919402122498	So again, maybe we also don't just get efficiency and generalization, we also get some basic compositionality.
1654090	1657320	C	0.8679343461990356	So let's talk about the limitations and what we could do next.
1657850	1665126	C	0.48994728922843933	The main limitation is that there's a predefined transformation that we're imposing in both space and time.
1665228	1674940	C	0.7369483113288879	So although we freed ourselves from group transformations and specifically like translation or rotation as is currently done in the machine learning world.
1675630	1682558	C	0.6682717204093933	We still have this hard coded latent role in our heads for everything we see.
1682644	1696078	C	0.7800712585449219	And to make this a little bit more flexible, so hopefully we can model a greater diversity of transformations, we think maybe we can take inspiration from more structured spatiotemporal dynamics that are observed in the brain.
1696174	1702226	C	0.8912228345870972	And so that takes us to the second part of this talk, which is spatiotemporal dynamics that we're going to try.
1702248	1704318	A	0.7890358567237854	To integrate into artificial neural networks.
1704414	1706286	C	0.7135409712791443	One example of that is traveling waves.
1706318	1707480	A	0.7609991431236267	Like I show here.
1707930	1709640	C	0.8194909691810608	So what do we mean by that?
1710250	1721046	C	0.9143484830856323	Here's a very recent paper where they used a nine tesla fMRI operating at 36 millisecond resolution to image a single slice of a rat brain under anesthesia.
1721158	1726422	C	0.8232183456420898	And what we see is this very clearly structured spatiotemporal activity and correlations.
1726566	1733582	C	0.9347615242004395	And these authors of the paper go on to analyze this activity in terms of the principal modes as depicted on the right.
1733716	1747694	C	0.7905959486961365	So our hypothesis is that perhaps some sort of a correlation structure like this may be beneficial for structuring the representations of our model with respect to observed transformations, but in a much more flexible way than simply just a cyclic shift.
1747742	1748980	A	0.8207166194915771	Like we were doing before.
1752790	1758390	C	0.7759753465652466	And let me say that this is not just observed in anesthesized rats.
1759290	1765590	C	0.8887221217155457	You can see these traveling waves happen in the mt cortex of awake behaving primates.
1766170	1779130	C	0.8959315419197083	So for example, on the left here, they show traveling waves that actually change how likely a primate is to see a low contrast stimuli based on the phase of the wave.
1779550	1789178	C	0.809416651725769	Furthermore, they show that a high contrast stimulus on the right can induce a traveling wave of activity that propagates outwards union primary visual cortex.
1789274	1793054	C	0.6162273287773132	So these are really ubiquitous throughout the brain at multiple levels.
1793102	1798802	C	0.5026707649230957	And it would be interesting to study what their implications are for structured representation learning.
1798856	1807186	C	0.8627577424049377	In our case, or generally, there is prior work which has studied these types of dynamics and they built models.
1807218	1823180	C	0.772742509841919	So on the top, these are the equations which describe a spiking neural network, which they show if you implement time delays, actually exxonal time delays between neurons, you do get these structured dynamics of traveling waves as long as your network size is large enough.
1824030	1834030	C	0.5589138865470886	However, as many people probably know, it's relatively challenging to train spiking neural networks of the same size and performance as deep neural networks.
1835010	1843774	C	0.5885211229324341	Similarly, on the bottom, another system which is significantly simpler, but perhaps too simple, is a network of coupled oscillators.
1843822	1850734	C	0.8279724717140198	These are known to exhibit synchrony and spatial temporal dynamics and complex patterns.
1850862	1856966	C	0.536528468132019	But this is called like a phase reduced system and doesn't quite capture the full complexity that we're interested in.
1856988	1860486	C	0.9181863069534302	So we're looking at something that's potentially in between these two.
1860668	1870778	C	0.7905542254447937	And what we settled on is this work, in this work is to parameterize a network of coupled oscillators slightly more flexibly than a Kuramoto model.
1870864	1871226	C	0.7397664189338684	So this.
1871248	1882986	C	0.8668820858001709	Is really built on this coupled distillatory recurrent neural network of Constantine, Rouge and Nisha, where they basically took the equation which describes a simple harmonic oscillator.
1883018	1884826	C	0.764559268951416	It's a second order differential equation.
1884938	1891070	C	0.8703977465629578	The acceleration on a ball on a spring is proportional to its displacement.
1893170	1898420	C	0.7792751789093018	You can add additional terms such as damping so that the oscillations slowly die out over time.
1899350	1906770	C	0.8904501795768738	You can drive this oscillator with an external input to kind of counteract this damping or to give slightly more complexity to the dynamics.
1907190	1915702	C	0.8810555934906006	And then furthermore, if you have many of these oscillators, you can couple them together with these coupling matrices W, as we demonstrate kind of in this picture here.
1915756	1922362	C	0.8769688606262207	So you can really think of this network as a bunch of balls on springs and they're maybe connected to each other also by springs or elastic bands, whatever.
1922416	1927658	C	0.9285439252853394	The couple, the Silatory recurrent neural network of Rush and Mishra with these various terms.
1927744	1931670	C	0.9251980781555176	And this has been shown to be very powerful for modeling long sequences.
1931750	1934414	C	0.8409464359283447	They also mentioned they were inspired by the brain building this.
1934452	1936720	C	0.9596461653709412	And there's a lot of good analysis in that paper.
1937090	1946130	C	0.9563108682632446	For example, they show that this has really beneficial properties with respect to vanishing gradient problems that typically happen in recurrent neural networks.
1946790	1964246	C	0.5799915194511414	But if we want to look at spatiotemporal dynamics in this type of a model, it's slightly challenging because these coupling matrices here, the W's that connect each oscillator's position to one another, these are densely connected matrices like I've tried to.
1964268	1965718	A	0.641609787940979	Depict on the left here.
1965884	1971270	C	0.535298764705658	So if you try to visualize the dynamics of this network, you don't see any spatial organization.
1971350	1975158	C	0.7552220225334167	There's no inherent topology to the latent.
1975174	1976300	A	0.7623288035392761	Space of this model.
1976670	1979530	C	0.8806806206703186	So you can think of this like in our previous example.
1979680	1983594	C	0.8647060990333557	A neuron is connected to a potentially arbitrary set of other neurons.
1983642	1986234	C	0.839948296546936	Those neurons are connected to another arbitrary set of neurons.
1986282	1993694	C	0.6356574892997742	And you'll just get oscillatory dynamics, certainly, but kind of fluctuations that don't make a lot of structured sense in our work.
1993732	2001598	C	0.8120317459106445	Then we thought, okay, how can we convert this more to the types of dynamics that we're interested in, this structured propagation of activity?
2001774	2015410	C	0.791591227054596	And one clear way to do that is to have a more structured connectivity matrix W, which we found is easily implemented and efficiently implemented through a convolution operation, which you can think of like a locally connected layer.
2015490	2020470	C	0.7174117565155029	So instead of having every neuron connected to every neuron, neurons are just connected to their nearby neighbors.
2020630	2022474	C	0.5620579719543457	After training, you'll end up getting something.
2022512	2025690	A	0.5664189457893372	That looks like a smooth spatial temporal dynamics.
2026510	2035962	C	0.8767385482788086	So to be a little bit more clear, to train this model, we take this second order differential equation that we were describing before you discretize it into two first order equations.
2036026	2039790	C	0.8754159212112427	You can think of this as like numerically integrating the ode.
2040290	2048546	C	0.8399246335029602	We now have a velocity and then we update and we can train this model as something like an auto encoder or an autoregressive model.
2048648	2051682	C	0.8482049703598022	So we take an input, we encode it to our latent space.
2051816	2056074	C	0.893131673336029	Really, the input is this F of x term which acts as the driving term.
2056142	2065234	C	0.8494638204574585	So it's like driving these oscillators from the bottom and then they have their own dynamics which are defined by the coupling terms, these local couplings.
2065362	2071590	C	0.8848677277565002	And then at each time step we take this latent state, this wave state, and we decode to try and reconstruct the input.
2071670	2083102	C	0.9153833985328674	Maybe at the current time step or a future time step, we can do some analysis of these models during training to see what happens before training.
2083156	2088062	C	0.8425168991088867	And after training, we can compute the phase and the velocity of the dynamics in the latent space.
2088196	2091906	C	0.795985996723175	Basically, we see at the beginning of training there's no waves in our model.
2092008	2103090	C	0.8084544539451599	But after training, after 50 epochs, we see that there's these smooth structured activity propagating downwards in service of this sequence modeling task that we're doing, like rotating objects.
2104070	2105794	C	0.700617790222168	So what's the benefit of this?
2105832	2111394	C	0.6711257696151733	I mean, the whole reason I motivated this was to say we wanted to have more flexibly learned structure.
2111442	2112598	C	0.7850608229637146	Are we actually doing that or are.
2112604	2114150	A	0.78584885597229	We just getting pretty waves?
2114970	2119986	C	0.8603010773658752	So what we showed in our paper is that we really are learning some sort of useful structure.
2120018	2123446	C	0.824265718460083	And the way we showed that is, again, with something like this commutative diagram.
2123558	2138602	C	0.728102445602417	If you take an input and you encode it and you get a wave state and then you propagate waves artificially in that wave state and then decode, you can observe that it's actually exactly the same as if you had just by showing a bunch of different images of different transformations.
2138666	2142250	C	0.7995661497116089	So a lot of different digits, different features.
2142330	2148846	C	0.8712393045425415	And we see that we get different types of wave activity in each case in order to model that different transformation.
2149038	2153806	C	0.6554953455924988	If we train it on different data sets as well, we similarly see more complex dynamics.
2153838	2160962	C	0.7693659067153931	In this case, maybe not even traveling waves or standing waves, which can be thought of as traveling waves in opposite directions.
2161026	2167050	C	0.8635204434394836	So we see if we're modeling these orbital dynamics, we get these kind of smoothly moving blobs of activity in our latent space.
2167120	2182110	C	0.5921952724456787	If we're modeling a pendulum, we similarly get kind of complex oscillatory activity so it's preserved input structure, but additionally more flexibility than we had before, which is kind of our ultimate goal.
2183490	2195410	C	0.7721486687660217	So finally, I want to talk a bit about how I think the outcome of this research may not only improve artificial intelligence, but also how it helps us understand why our measurements of the brain look the way they do.
2195480	2201998	C	0.883950412273407	So to give a brief example of what I mean by this, I talked a bit about before, about these and places.
2202174	2211154	C	0.8169714212417603	So in this fantastic work with HIE Gao, we studied if our simple topographic prior, as we discussed, may be able to reproduce these same effects.
2211282	2223126	C	0.919264018535614	So specifically, we plot the value of this Cohen's D selectivity metric for each of our neurons with respect to a different data set of images potentially containing just faces or just objects or bodies.
2223318	2229382	C	0.8571493625640869	And so we measure for every neuron, is it more likely to respond to faces or the ration emerges in the brain?
2229526	2242734	C	0.7952883839607239	But I do think that it tells us that the relative organization of selectivity may at least be partially attributable to correlation statistics in the data after being passed through a highly nonlinear future extractor.
2242862	2244850	A	0.7636184692382812	Such as a deep neural network.
2245350	2251262	C	0.818123996257782	So, in a similar vein, something that's interesting, there's a known what's called tripartite, or the visual stream.
2251326	2269900	C	0.8118723034858704	So images of or objects are selectivity with respect to objects is organized by more abstract properties, such as animacy, is this thing alive or inanimate versus also real world object size, like what is the size of a teapot versus a car?
2270910	2277206	C	0.858913242816925	And what we see is that in humans, this selectivity is organized in this tripartite structure.
2277238	2283254	C	0.8913965225219727	You typically have small objects that are in between animate and inanimate objects in terms of their selectivity.
2283382	2285694	C	0.6112162470817566	And we see the same thing kind of happens here.
2285732	2290170	C	0.8940210938453674	So these are measuring the selectivity of the same set of neurons, but with respect to these different sets of stimuli.
2290250	2294254	C	0.8593895435333252	We see that the small cluster is in between animate and inanimate cluster.
2294302	2297134	A	0.7710806727409363	And again, this happens for multiple different initializations.
2297262	2301250	C	0.8535422682762146	So this is something I hope we can explore a bit further for this community.
2301320	2307782	C	0.9176769256591797	I think it's interesting because it's really a way of showing that we built a structured world model.
2307836	2314406	C	0.8891757130622864	And potentially, this world model is beneficial for better representing real world data in.
2314428	2318700	A	0.7264795303344727	A structured way, and you get lower free energy in that sense.
2319070	2331322	C	0.5267701148986816	So by developing these models like we showed here, we may get insights into new mechanisms for how this structure emerges, including topographic organization that we never thought of before.
2331456	2347422	C	0.6691257953643799	So, machine model, I was looking at the orientation selectivity of neurons, which I wasn't particularly expecting something to happen, but you're looking at kind of these waves propagate over this simulated vertical surface.
2347486	2350094	C	0.7371627688407898	And I thought, okay, maybe I'm showing rotated images.
2350142	2353250	C	0.8607431054115295	Maybe this has some effect on the orientation selectivity.
2353670	2366514	C	0.5305253863334656	And actually, if you go in and you measure the selectivity of each neuron with respect to these differently oriented lines, what you see is that it's surprisingly reminiscent of the orient paper columns that are seen in primary visual cortex.
2366562	2368866	C	0.8186535835266113	This is stuff going back to Hugle and Weasel.
2368898	2372806	C	0.8512621521949768	And this is something that just kind of came out of this model and the fact that it has the spatial.
2372838	2375270	A	0.8614614009857178	Temporal structure with respect to transformation.
2375430	2387806	C	0.6505805850028992	So, of course, this is a really coarse analogy, but I think this is an example of how building these types of models can help us think about how the brain builds representational structure and the way it's organized in a way.
2387828	2389520	A	0.7376865148544312	That maybe we haven't thought about before.
2391490	2394786	C	0.7361958026885986	I think I'm not the only one who's doing this type of work.
2394888	2398580	C	0.8500853180885315	And so I want to talk a little bit about some other people who are doing this.
2398950	2402210	C	0.9047168493270874	So I've been talking about this equivalent structure.
2402630	2421318	C	0.7963210344314575	People such as James Whittington and Tim Barrons and Surge Anguli have shown recently that by introducing algebraic constraints into a learning process, in this case, it was like the motion of an agent in an environment.
2421414	2429454	C	0.8941271305084229	By saying, you need to preserve kind of this algebraic structure of if I move in a circle west, north, east, south, I end up back at the same point.
2429492	2435870	C	0.8671112060546875	Again, by introducing these types of constraints, you get the emergence of grid, cell like representations.
2436770	2450020	C	0.5188699960708618	So I'd be interested to see how this idea of representational structure can help us explain maybe more than our scientific findings we're finding as well and how this relates to generative models as a whole.
2451190	2455986	C	0.8220248818397522	And then finally, I think there's something to be said about cognitive plausibility of these models as well.
2456008	2461522	C	0.8424218893051147	Maybe we're not just going to be testing them from a neuroscience perspective, but also from a cognitive science perspective.
2461586	2471910	C	0.8662738800048828	For example, there's these Ravens Progressive Matrices on the left where you have to say which one of these images is more likely to fit in this pattern?
2472410	2481370	C	0.774331271648407	Or for example, how likely is it that this Jenga Tower falls over when you pull a specific block or with a given structure?
2481530	2497346	C	0.7933090925216675	And I think these types of tests are really testing if our world models that we're building are similar to the types of models that we innately have our own common sense as humans or as beings living in a natural world.
2497528	2506894	C	0.6858643293380737	And I've done some preliminary work in this direction, I think very preliminary and not nearly this complicated, but trying to model visual illusions.
2507022	2520374	C	0.8341032266616821	So if you take a really simple data set of a moving bar stimuli or a static bar frame and you move it a little bit, you can see that the model will actually infer that missing frame and then actually also infer continued motion.
2520422	2526890	C	0.5389068722724915	So it's like overshooting the trajectory of what the actual stimuli is providing it before correcting again.
2527040	2533338	C	0.8642769455909729	So I think modeling illusions is certainly an interesting way to study if our world models are similar to the types.
2533354	2535630	A	0.710262656211853	Of models that we have ourselves.
2536530	2545338	C	0.7361814975738525	So in conclusion, yeah, I think topographic priors we could show that they effectively learn structured representations or structured world models.
2545434	2551774	C	0.7178292870521545	This learned structure is flexible and adaptable to arbitrary transformations, unlike traditional equivariance.
2551902	2561510	C	0.903745174407959	And topographic riders can be induced statistically as we did in the topographic VAE or through dynamics like we were showing in these neural wave machine type models.
2561930	2580650	C	0.5860791206359863	So to conclude, I'll leave you with this quote that I found in Fukushima's paper from 1980 I thought was pretty far ahead of its time, where he says, if we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to understanding the neural mechanism in the brain.
2581070	2583006	C	0.8119107484817505	So that's kind of, I think, some.
2583028	2585200	A	0.6280426979064941	Of the goals that we're going for here.
2585890	2594670	C	0.9289609789848328	So I'll say thanks to my advisor Max, my co authors Patrick Yue, Emile Jinghe, and Yorn, and interested in discussion.
2594830	2595540	A	0.6283750534057617	Thanks.
2604570	2605830	B	0.8092430830001831	All right, thank you.
2605980	2606486	B	0.7671424746513367	Great.
2606588	2609510	B	0.9685792326927185	Very interesting presentation.
2610970	2612470	B	0.771442174911499	A lot of places to start.
2612540	2622330	B	0.9011372923851013	Maybe just what brought you to this work, a little context on how you came into this work for your PhD direction.
2623950	2638970	C	0.8316164016723633	Yeah, I mean, there's been studying the group that I'm in at the university has been studying structured representations from mathematical point of view for a while, and we're some of the people to models.
2639130	2641310	A	0.7776333689689636	Or for the variational auto encoder.
2641990	2652594	C	0.5715227723121643	And I guess what something that had always been the model that respects rotations, 2D rotations perfectly well.
2652712	2657686	C	0.5606828331947327	But if we want to do 3D rotations, we can't do that because that's not a group in terms of a.
2657708	2659298	A	0.8348838090896606	Projection onto a 2D plane.
2659394	2661506	C	0.8873756527900696	You're losing information when this thing rotates.
2661538	2662680	A	0.7868906259536743	Around, for example.
2664570	2666962	C	0.8388015031814575	Or just any sort of natural transformations.
2667026	2675034	C	0.8749329447746277	Like I was trying to point out at the beginning, I think it was trying to think about how the brain models natural transformations is something that these.
2675072	2694530	B	0.9058869481086731	Current frameworks where do you see action playing a role in terms of variational, auto encoder models that include not just external patterns, but also the consequences of action or world model structure with action.
2695750	2697074	C	0.6106663942337036	Right, yeah.
2697272	2698322	C	0.48203182220458984	No, it's a good question.
2698376	2705240	C	0.7581607103347778	And I think active inference is effectively I think it's a good answer to that.
2706410	2715058	C	0.8257313966751099	I know there are reinforcement learning frameworks that do use kind of externally trained world models.
2715154	2719466	C	0.8857990503311157	So you train a VAE or something and then you use that representation in.
2719488	2721660	A	0.8128433227539062	Your reinforcement learning system.
2722190	2726362	C	0.824946403503418	But I think having a fully kind of a system that is a single.
2726416	2730666	A	0.8208547830581665	Objective with action as part of the.
2730688	2735470	C	0.7556743025779724	Likelihood of the data, I think that's much more elegant.
2735810	2738320	C	0.7072434425354004	I'm a big proponent of that.
2740050	2746114	C	0.625787615776062	I have not gotten so far as to study how these structured world models in a VAE or I haven't worked.
2746152	2746754	A	0.6236767768859863	On that at all.
2746792	2750466	A	0.8366400003433228	But I think it would certainly be very interesting to see if having a.
2750488	2756306	C	0.6277014017105103	More structured world model in a variational auto encoder would be beneficial in an.
2756328	2757366	A	0.7811763882637024	Active setting as well.
2757388	2758534	A	0.9766846895217896	I think that would be awesome.
2758652	2764758	C	0.8696750998497009	I mean, I think some of these examples, like, showing before, like, emergence of.
2764764	2766534	A	0.846276044845581	Grid cells and things like this, maybe.
2766652	2770678	C	0.7704189419746399	Point towards that direction of, hey, maybe the brain is doing something.
2770764	2773286	C	0.7472796440124512	It really obviously has a lot of structure.
2773478	2777660	A	0.6603724360466003	This clearly has to be useful for performing actions in some way.
2779170	2779678	A	0.84200119972229	Cool.
2779764	2780062	C	0.5491447448730469	Yeah.
2780116	2796798	B	0.896237850189209	I thought a really nice parallel that you brought in with the talk was the locally connected units enabled your models to structurally embody the convolutional constraint and pattern, and that led to these arising patterns.
2796894	2801730	B	0.8625656366348267	And then analogously, there was the Doral et al.
2801880	2806722	B	0.7742852568626404	Where they had the path exploration constraint.
2806786	2807062	A	0.5664746165275574	Right.
2807116	2818694	B	0.5411571264266968	And so then it's interesting to think about these action or policy heuristics or sparsities like a joint motor exploration.
2818742	2831066	B	0.8119232058525085	Eventually it becomes understood that there's like two mutually opposing ways to move a joint, and then the compositionality across joints can be learned at these higher levels once it's locked in at lower levels.
2831098	2850260	B	0.906749427318573	So it's a very appealing and niche relevant way to generalize because it's both based upon the actual constraints of the world, but then especially through action, potentially embedding something that's quite simple.
2851350	2851858	C	0.7360090017318726	Right?
2851944	2853774	C	0.4988401532173157	Yeah, no, I think that's definitely true.
2853832	2855080	C	0.9683849215507507	That's a really good point.
2856090	2858054	C	0.7553325891494751	If you do have constraints coming from.
2858092	2865750	A	0.881079375743866	Your actions themselves, then that would be hugely beneficial for helping to structure your latent space.
2865820	2885680	C	0.8221573829650879	And I think yeah, I guess one thing I wanted to mention there's something made me think of, like, Stefano Fousey's work on kind of the representational geometry determines how generalizable a given understanding of a system is.
2886210	2899998	C	0.7011932730674744	And I think if you can understand if these sets of activities are separable or highly parallel separable with a linear classifier, essentially, then you're going to be able to do generalization.
2900094	2913314	C	0.7989550828933716	And I think by imposing these types of biases or potentially through constraints that are imposed by action, something like this, you are yielding or kind of inducing a better representational geometry.
2913362	2917910	A	0.8535662293434143	And this has all sorts of benefits for compositionality or generalization.
2919150	2920330	A	0.8860891461372375	It's a great point.
2920480	2920938	A	0.84200119972229	Cool.
2921024	2921322	C	0.5491447448730469	Yeah.
2921376	2922650	B	0.9300366640090942	Very interesting area.
2922720	2933790	B	0.8962700366973877	All right, I'll read some questions from the live chat love Evolve wrote any practical or observed limitations on modeling illusions?
2938450	2939754	C	0.5827231407165527	Deep learning, community uses.
2939802	2940842	C	0.8172686696052551	They're not foveated.
2940906	2952062	C	0.6074023842811584	You don't have a center of gaze, and you also don't have most convolutional neural networks.
2952126	2958694	C	0.6813496351242065	I'm using these kind of recurrent neural networks, but time is not as clearly defined in these models as it is.
2958732	2963990	A	0.8568922877311707	In a continuous time setting for human undergoing an illusion trial.
2965210	2979898	C	0.7342160940170288	And I think the combination of these two, of the fact that as a human, for most things, you're shifting locations and your gaze are dependent on you looking to a particular area, a lot.
2979904	2981498	A	0.7831547856330872	Of cognitive science tests.
2981594	2992298	C	0.5555119514465332	And so I think it would be really helpful if we had models that you can think of this as a type of action of learning where to move your gaze.
2992394	2995754	C	0.7908154129981995	One of the simplest possible that would help a lot for being able to model illusions.
2995802	3007266	C	0.6852900385856628	And just I mean, for me, it's like I read a paper of some cognitive science experiments or about some illusion, and I think of, okay, can I put this data set into my model and test it?
3007288	3009318	C	0.6955734491348267	And most of the time the answer is no, because I don't have a.
3009324	3014360	A	0.8641438484191895	Model that looks around or has a restricted field of view, something like that.
3015050	3016962	C	0.5024803876876831	So, yeah, I think that's one of the limitations.
3017026	3023398	C	0.7337918877601624	Another one is makes the experiment much more complicated.
3023494	3025930	A	0.4920366108417511	So that's one of the practical limitations.
3027310	3028058	C	0.7093686461448669	Wow.
3028224	3029066	B	0.7966645956039429	Great answer.
3029168	3033978	B	0.8168795704841614	Makes me think of a paper with letters rotating on a table.
3034154	3036042	B	0.8023399114608765	That's the digit rotation.
3036106	3039002	B	0.7620176672935486	Great points about the Foveation and the dynamics of the illusion.
3039066	3060130	B	0.7439225316047668	I think you actually did mention an illusion, which is however, you mentioned in the generalization context, which is rotating on the two dimensional screen doesn't generalize to three dimensions, and that dimensional collapse or reduction is the basis of the cube projection illusions and cube and figure rotation illusions.
3060290	3073082	B	0.8461059331893921	It's on your screen and there's a silhouette or there's some ambiguous stimuli that it's near a criticality or a bifurcation in the generative model.
3073136	3076620	B	0.808143138885498	So it could represent it one way or another way.
3076990	3087920	B	0.5172434449195862	And so a lot of the switching illusions are just based upon the flatness of images and the limitations in generalization that are revealed by.
3092370	3098286	C	0.5834736824035645	Some work sorry, there's some work where they can argue people have a kind of three dimensional.
3098318	3099390	A	0.6501815915107727	Image in their heads.
3099470	3105640	A	0.6525521874427795	Like even Nancy Kenwich had her library on this recently and showing yeah, I don't know.
3106250	3108054	C	0.8391115069389343	Do our models have that?
3108252	3111158	A	0.5739941596984863	It's not super clear anyway.
3111324	3113000	B	0.9679949283599854	Yeah, that's pretty interesting.
3113370	3114230	B	0.4896698594093323	All right.
3114380	3117750	B	0.8275649547576904	From Upcycle club in the chat, they wrote Kudos.
3118250	3120638	A	0.6706591248512268	You're able to learn nearly as effectively.
3120754	3124300	C	0.7144977450370789	If you imagine you only want a single neuron to be active for every example.
3124670	3128394	C	0.8459360599517822	Your model is going to be trying to memorize the data set to some.
3128432	3132730	A	0.6325626373291016	Extent or something like this, and you're not going to have enough capacity.
3132810	3134974	C	0.8378974795341492	So, yeah, I think tuning that level.
3135012	3140030	A	0.6331192851066589	Of sparsity is certainly an important factor.
3140850	3151460	C	0.8549426794052124	And when you look at the likelihood, if you're calling framework, typically this is balanced automatically with the likelihood itself.
3152870	3155506	C	0.8451902866363525	If you're not doing generative modeling, you just have a sparsity penalty.
3155538	3157350	A	0.7302054166793823	You're going to want to tune that parameter.
3158490	3159046	B	0.584351658821106	Okay.
3159148	3169450	B	0.5528507828712463	They added just to clarify runaway behavior in Armina, where the network becomes unstable or chaotic due to various factors such as feedback loops, noise, or adversarial inputs.
3172270	3186670	C	0.5455963611602783	Yeah, I guess I haven't looked at this in a recurrent setting where you would get feedback loops, but I could see adversarial examples being potentially affected by your level of sparsity.
3188930	3194222	C	0.8342927098274231	The interesting point is, would you be more susceptible or less susceptible to adversary examples?
3194366	3195380	A	0.5666733384132385	I don't know.
3196390	3208790	B	0.5716805458068848	Well, sparsification projecting from a fully connected, higher dimensional model just into progressively smaller it's pretty well understood in general what the trade offs are.
3208860	3213190	B	0.5417848825454712	It's easier computations, a smaller model, sparser.
3213930	3223846	B	0.6543182134628296	The Bayes graph is going to be clearer to represent and then also it will have all of the other trade offs with false positive and negatives of generalizing.
3223958	3226090	B	0.803358793258667	But that's why it's an iterative fit process.
3226160	3233790	B	0.8628928661346436	So I guess how does your sparsification approach balance?
3236610	3245650	B	0.9236465692520142	Does it use AIC or BIC or some other model fitting approach to determine the relevant sparsification for a given input?
3246950	3256120	B	0.8544607758522034	How do you determine in Lasso regression, how do you threshold how many how sparse you want it to be?
3257210	3264834	C	0.8318734765052795	Right, yeah, I think there's a lot of good literature on this, and even so, some people like DembaBa at Harvard.
3264882	3280010	C	0.8694091439247131	And some people I'm working with now have done these kind of unrolled iterative sparsification networks where it's like a recurrent neural network and iteratively sparsifies.
3280090	3281706	C	0.7072417140007019	And you can show that this yields.
3281738	3283680	A	0.8358684778213501	Something like relus or.
3285970	3290430	C	0.9093406796455383	Group sparse activations like we're using here in this setting.
3291670	3311254	C	0.887860119342804	It's really just by having this construction of this T variable where we have Z on top and then it's in some effect gated by the sum of U variables in the bottom.
3311292	3316482	C	0.6859867572784424	So, w maybe I wasn't super clear about this is a matrix that is connecting.
3316546	3321754	C	0.7704302072525024	That's what defines the groups when I'm defining the group sparsity, that connects all of these U's together.
3321872	3325338	C	0.8020398020744324	And so the idea is like here.
3325424	3327386	A	0.7066453099250793	If all of one of the other.
3327408	3341646	C	0.6746217012405396	Examples, if all of your U's are not active for a given T, or if all of your U's are active for a given T, that T variable is going to be very small, right?
3341668	3345022	C	0.5966644287109375	Because your denominator is going to be very big, and that induces sparsity.
3345166	3348206	C	0.7351964712142944	So it's like constraint satisfaction.
3348398	3353818	C	0.7192264199256897	If you have a set of U's that are all small, then that that constraint is satisfied.
3353934	3363442	C	0.7178469896316528	And now Z is allowed to kind of express itself and that's what then kind of achieves the sparse activation.
3363586	3368746	C	0.8560516238212585	So this is induced by these two KL divergence terms here.
3368768	3373718	C	0.8836049437522888	These are saying like, how far is each U and each Z from a gaussian?
3373814	3383386	C	0.873318076133728	And then through this construction of the student T variable, we're effectively constructing a sparse prior distribution just from these Gaussians.
3383418	3389902	C	0.7699559330940247	But in terms of the actual objective, the terms and the objective that we're optimizing are just these two KL terms.
3389956	3393514	A	0.9053791761398315	That are pushing it towards sparsity to some extent.
3393562	3396386	C	0.8623095750808716	And this is balanced automatically with the.
3396408	3398814	A	0.8750479817390442	Likelihood term here through the decoder.
3398862	3402658	C	0.8264384269714355	So we don't have terms that we're tuning, but we're learning the parameters of.
3402664	3406370	A	0.6394597887992859	These different encoders and then analyzing the failed urgencies.
3408490	3409240	B	0.84200119972229	Cool.
3409770	3410438	A	0.4896698594093323	All right.
3410524	3418454	B	0.8216983079910278	Another question from Dave Douglas, who wrote speaking of gaze and illusion.
3418582	3428170	B	0.7026174068450928	Can the studies on constancies in infants be separated into lower level illusion, relative, perhaps higher level conceptual constancy?
3434070	3437678	B	0.8067928552627563	Can you read the current kind of architecture?
3437854	3445910	B	0.7112674117088318	Might the studies on constancies in infants cognitive constancies, be separated?
3446650	3447880	C	0.6494656801223755	Yeah, probably.
3448250	3457366	C	0.5531295537948608	I'm not an expert or actually even very familiar with object permanency studies and.
3457388	3460018	A	0.8017420768737793	Infants and constancy stuff, but I think.
3460044	3464054	C	0.930158793926239	That would be incredibly interesting to study in neural network architectures.
3464102	3471114	C	0.87742018699646	And that was kind of some of the idea with this illusion that I was trying to model down here with this line.
3471232	3478186	C	0.539692759513855	I don't know if I was super clear about this, but the top row is the input, and we're effectively like blocking the input for a single frame.
3478298	3485598	C	0.8484161496162415	And I wanted to see, does the network kind of encode that the thing is still there when that frame is gone?
3485684	3487458	C	0.8254911303520203	Can I still decode the presence of.
3487464	3489250	A	0.8284248113632202	The object from the neural activity?
3489750	3495318	C	0.8617109656333923	And then what is it also inferring about the motion because of the fact that it saw the bars at a.
3495324	3499720	A	0.8787299990653992	Slightly different location than from before after the frame is gone.
3501130	3516186	C	0.7255920171737671	So, yeah, I think there's definitely multiple levels to it where some would probably be much lower level and maybe long term object permanency, I would guess would.
3516208	3518140	A	0.5002289414405823	Be significantly higher level.
3519870	3521530	C	0.7023738026618958	It just makes me think of those.
3521600	3524462	A	0.8893613815307617	Experiments with cats back in the day.
3524516	3528654	C	0.5867798328399658	Where it's like they raised them in darkness except for an hour a day.
3528692	3545490	C	0.5866059064865112	They put them in vertical world or horizontal world where they only saw horizontal lines or vertical lines, and you can see the organization of their cortex changes, like they have less receptivity to horizontal lines if they've never seen horizontal lines before.
3545640	3551142	C	0.5923776030540466	And then you take a stick and you wave it in front of their face, and if the stick is horizontal, they do nothing.
3551196	3552758	C	0.7377028465270996	If it's vertical, they're swatting at it.
3552764	3554134	A	0.7087746858596802	They'Re trying to hit it, it's like.
3554172	3555638	C	0.6772365570068359	They just literally don't hunzle bar in.
3555644	3556582	A	0.658764660358429	Front of their face.
3556716	3566902	C	0.7194011211395264	So I think in that case, then, this is evidence of a low level efficiency in vision contributing to some sort of an illusion.
3566966	3568714	C	0.5778592228889465	So I think, yeah, there could certainly.
3568752	3571120	A	0.9010153412818909	Be some aspect to that in infants as well.
3573570	3586210	B	0.6367886066436768	One very curious point you brought up was the animate and inanimate manifold with small things being intermediate, right?
3586360	3588130	B	0.8304682374000549	What does that represent?
3589670	3600360	B	0.74924236536026	Or is it because they're handleable or it might be an insect or it might be something that might move away just with wind or what does that say?
3601530	3615594	C	0.86184161901474	Right, yeah, so this is work by Talia Konkel, I think was the one who discovered this organization and they tried to figure it out.
3615712	3620670	C	0.5594549775123596	I might be getting this wrong, so I recommend people to read her work on that.
3620740	3622762	C	0.8697962164878845	They call it Tripartite organization.
3622906	3651270	C	0.8648926019668579	But if I remember correctly, they did a lot of follow up work on why there's this organization and some evidence of curvature of these objects and kind of like the distance that you see objects from or like animate objects are maybe more curvy or regardless of what the actual answer is, there were a lot of different hypotheses that were stemming from properties of these objects.
3652010	3655666	C	0.8093820810317993	Maybe mid level or low level properties, more so than higher level properties.
3655778	3670102	C	0.6903459429740906	I still don't know if it's exactly been solved of whether it's like, interaction, like you said, with the objects causes the separation or the general shapes of these objects.
3670246	3674510	C	0.6641353368759155	I would bet, as with most things, it's like some combination of all of the above.
3675410	3686922	C	0.749193549156189	But I think the interesting thing from this modeling point of view is that this is only trained on correlation statistics from the image data sets itself.
3686996	3690930	C	0.7153337001800537	This has no interaction, this has no notion of animacy.
3691910	3700738	C	0.5533459782600403	I mean, this is really just training a model on ImageNet, just images of dogs, cats, boats, whatever, and yet it still achieves this type of organization.
3700834	3705766	C	0.878641664981842	So there's some sort of it could be semantic characteristics, right?
3705948	3714714	C	0.8415321707725525	We have a network that can classify boats versus dogs versus 20 other breeds of dogs, but it might also have.
3714752	3718380	A	0.73906409740448	Some correspondence with lower level image statistics as well.
3719150	3719514	A	0.6590757966041565	So?
3719552	3719814	C	0.5491447448730469	Yeah.
3719872	3720286	A	0.5666733384132385	I don't know.
3720308	3721600	A	0.7322689294815063	I guess he's my answer.
3723970	3724574	C	0.5491447448730469	Yeah.
3724692	3734290	B	0.8767005801200867	Provocative analogy was the translational shift in the MNIST in the handwriting recognition setting.
3734870	3741202	B	0.8957099914550781	What are the translational shifts that exist today?
3741336	3742882	B	0.8730171322822571	What's the three pixel example?
3742936	3746806	B	0.604282557964325	Is that some prompt engineered attack on an Lom or something?
3746988	3747814	B	0.6280289888381958	Or something.
3747932	3755990	B	0.5049973130226135	A special character being inserted, or some overlay on an image that we can't even detect.
3756970	3763340	B	0.904385507106781	So what do you think those challenges are and what are ways that we can pursue that?
3764910	3765754	C	0.5014309883117676	Yeah, absolutely.
3765872	3771950	C	0.8487043976783752	I mean, I think kind of the way I was thinking about it is like these symmetry transformations.
3773730	3780720	C	0.689680278301239	If you're thinking about language models, you can imagine a symmetry transformation is just like replacing a word with a synonym or something.
3781890	3786206	C	0.7330219149589539	You have the sentence to us means the exact same thing, but now suddenly.
3786238	3788370	A	0.821053147315979	The model is going to respond very differently.
3791110	3795266	C	0.833530604839325	Or like translation between languages, this can.
3795288	3796754	A	0.8642420172691345	Be seen as a type of transformation.
3796802	3804342	C	0.8280826210975647	It preserves the underlying meaning of the input to us, but to the model.
3804396	3805510	A	0.6499979496002197	It looks completely different.
3805580	3816710	C	0.7169232964515686	And we would like to have models which behave in a predictable way with respect to these types of transformations, because I think humans behave very predictably with the type of these transformations.
3816790	3822358	C	0.7449832558631897	And when we're dealing with AI systems, we expect them to also behave that way.
3822464	3827514	C	0.8159551620483398	And I think that's part of what causes a lot of challenges interacting with these systems.
3827562	3832894	C	0.8534786105155945	And I kind of tried to do a rough cheeky demonstration of that with.
3832932	3835200	A	0.8138043880462646	This bear and squares and stuff.
3836310	3843294	C	0.5297544002532959	We expect it to be able to do something simple like this because we think most humans could, and yet it doesn't.
3843342	3844498	C	0.726678192615509	And if you imagine this is a.
3844504	3848200	A	0.8936979174613953	Critical scenario where you expect this, then that's a big problem.
3848810	3850360	C	0.6150665283203125	How do we handle that?
3850810	3853382	C	0.7842497229576111	I think that's kind of what I'm searching for.
3853436	3854200	A	0.6888490915298462	I think.
3856570	3876218	C	0.6508288979530334	My direction I'm taking it is look more simple, kind of like bottom up building blocks of neural network architectures or algorithms that kind of yield these emergent structural properties.
3876314	3878622	C	0.5995301604270935	And I think that's a much more generalizable way.
3878676	3881166	C	0.5738024115562439	Rather than building something on top of.
3881188	3884894	A	0.8087828755378723	What we already have, I think that's.
3884942	3886466	C	0.7358240485191345	Something that would scale much better and.
3886488	3888580	A	0.8237918615341187	Also matches more what the brain does.
3890710	3891410	B	0.9677404165267944	Very cool.
3891480	3893218	B	0.7473616003990173	One kind of implementational question.
3893304	3902262	B	0.8971177339553833	What are the computational requirements of just running this, or what's the day to day like of being a student or researcher running variants of these?
3902316	3906962	B	0.5768054723739624	Like, do they use terabytes of data and you're using large computation?
3907026	3910970	B	0.7698950171470642	Or is this something that people can run on their own laptops?
3911870	3915750	C	0.7494751214981079	I think almost everything I presented today can be run locally.
3915830	3918826	C	0.6527660489082336	So this stuff is super simple.
3918928	3923406	C	0.7103970646858215	You can run you can run it on.
3923428	3923914	C	0.6564805507659912	Your laptop.
3923962	3927914	C	0.5460677146911621	If you want to train and experiment with different things, it's going to be pretty slow.
3927962	3931790	C	0.5750076174736023	So I'd recommend some commercial GPU.
3932290	3942798	C	0.6420516967773438	I run pretty much everything on like Nvidia 1080s, pretty old, pretty cheap, but they have twelve gigs of Ram or whatever, and it's kind of more than enough for these models.
3942894	3950802	C	0.5911741852760315	Gigabytes of Ram I think one thing that some people think is weird is I do most of my experiments on stuff like MNIST.
3950946	3955334	C	0.7468218803405762	So it's 32 x 32 pixel images because I can train it small.
3955372	3958886	C	0.8131596446037292	And locally, if you want to do.
3958908	3960790	A	0.7511329650878906	Stuff, my experiments are on mnists.
3961310	3964070	C	0.5393112301826477	If you want to do stuff like this, these are much more complicated.
3964150	3970182	C	0.5683952569961548	This Hamiltonian Dynamic suite here, you're getting into bigger models that are running across multiple GPUs.
3970246	3973710	A	0.8392422199249268	And so here I was using a cluster to run these types of models.
3974290	3979198	C	0.4981614351272583	But I say most of the single machine with the GPU is more than enough.
3979364	3980986	C	0.8126396536827087	Or even just like in a collab.
3981018	3983422	A	0.8343350291252136	Notebook, something like that.
3983556	3992370	C	0.4830753803253174	If you want to train something on ImageNet, it gets more complicated and you need at least one GPU, ideally more.
3992520	3995586	C	0.5810533165931702	But yeah, I don't do a whole lot of big scale stuff yet.
3995688	4000374	C	0.9794629812240601	I think it's certainly interesting and there's definitely a lot more you can do there.
4000412	4005718	C	0.6894413232803345	But for some of these kind of simpler or more fundamental questions, I don't.
4005724	4007160	A	0.7646836638450623	Know what you want to call it.
4007630	4009900	C	0.8615474700927734	A smaller machine is nice and fast.
4012430	4014010	B	0.9162558317184448	Cool, useful.
4014510	4015034	B	0.4896698594093323	All right.
4015072	4030330	B	0.7858501672744751	I'll read a comment from Dave recalling Bert Devries'comment during the Applied Active Inference Symposium about the desirability of spending less effort or ATP on foraging or control situations where we don't need much precision.
4030410	4046390	B	0.6702318787574768	I don't know if you listen to this, but Professor DeVries mentioned about variable precision models and how they could be used to enable different features of generalization and actual structural course training as well as reduced computational requirements.
4046970	4052882	B	0.9256316423416138	Does he have any suggestions on how to introduce this distinction into active inference theory?
4053026	4055960	B	0.5803893208503723	What kinds of experiments could winkle this out?
4058330	4059030	C	0.758330225944519	Oh, wow.
4059100	4065420	C	0.6600692868232727	Yeah, that's something I don't think I have too much intelligence to say about, to be completely honest.
4069150	4070010	A	0.6434743404388428	Hmm.
4071470	4099834	C	0.8063569664955139	It's super interesting question because I think the intuition makes a lot of sense to me that you're talking about, if I understand correctly, variable rates of vision when you're encoding or in your model in general, doing computation that somehow has an impact on your future performance as a relation to some energy store.
4099952	4110026	C	0.849193274974823	I think if you wanted to build this into an active inference system, you would need to have really an embodied system where the agent has some notion.
4110058	4113440	A	0.7419657111167908	Of energy, like an internal energy store, and.
4115250	4122434	C	0.625842809677124	Something that is trying to conserve while it's performing its actions and running out of energy would need to mean.
4122472	4124034	A	0.8142904043197632	Something bad for the agent.
4124232	4126978	C	0.7994768619537354	And then maybe you could observe kind.
4126984	4138742	A	0.8387610912322998	Of an emergent reduction in encoding precision or something like this as the agent is trying to learn to act more.
4138876	4140406	C	0.7859224081039429	Effectively, you might have to give it.
4140428	4143190	A	0.8010855913162231	An ability to control its precision.
4143930	4148390	A	0.7276497483253479	Yeah, like I say, out of my thoughts.
4149070	4149482	B	0.584351658821106	Okay.
4149536	4152054	B	0.969348132610321	On this slide right here, first, very cool image.
4152102	4157130	B	0.8226051926612854	It's kind of like a digital Jackson Pollock.
4159410	4170560	B	0.8080090880393982	If it were a simpler input data size or just reduced complexity of patterns, or if it were an increased complexity, how would this image look different?
4171830	4183922	C	0.868165910243988	Yeah, so I did some experiments trying to change these orientation columns and basically changing the parameters of the model.
4183976	4186418	C	0.5150143504142761	You can get these columns to be bigger.
4186514	4194470	C	0.7684899568557739	You can get them to not have very similar structure to what we see in the humans, where you can get them to have more bands of activity.
4196170	4199594	C	0.8227950930595398	And it also, like you said, it depends on the data set that you're using.
4199712	4205130	C	0.7634784579277039	If I use really simple sinusoidal gradings as input, I get something like this.
4205200	4210830	C	0.6494531631469727	I get something that's a little bit more rotational, curvy, higher entropy.
4212690	4220560	C	0.8902518153190613	So I think these are all interesting things if you want to study the emergence of this type of organization in a natural system.
4221170	4230900	C	0.8600066304206848	If you have a model that now yields different organization for different settings that see, okay, then what settings best match our observed data?
4234490	4236886	C	0.6255104541778564	I can send those around if you're interested.
4236988	4253988	C	0.7412312626838684	But I think one other interesting point there is that the different animals and types of orientation selectivity and different numbers of pinwheels.
4254004	4255368	C	0.7309853434562683	Some animals don't have it at all.
4255454	4261256	C	0.8333818912506104	I think maybe mice, if I'm correct, have this kind of they call it salt and pepper selectivity.
4261288	4262536	C	0.669572114944458	So it's basically random.
4262568	4265580	C	0.6174358129501343	You don't have any sort of, like, topographic orientation selectivity.
4266480	4270296	C	0.8263729810714722	So there is evidence that different systems.
4270328	4273810	A	0.5794501900672913	Do this differently, and it's interesting to figure out why.
4274740	4276512	B	0.9803731441497803	Yeah, this very cool.
4276566	4282530	B	0.789314866065979	It reminds me of, first, the reaction diffusion, space and time.
4282900	4300040	B	0.7560076117515564	So it's actually possible that a region might have no activity from a given granularity, like if it was being looked at at fMRI spatial and temporal timescale.
4303180	4313064	B	0.5397325754165649	But if the pockets of activity are slower, faster, then that measurement is going to not be different than noise.
4313112	4315230	B	0.7978245615959167	It'll all have been averaged out.
4315840	4330636	B	0.5360414981842041	So then there might be some interesting data sets that do actually have a lot of richness, but then for one reason or another, it just was averaged out over because it wasn't being connected.
4330668	4331824	C	0.8561233878135681	To you or something like this.
4331862	4333888	C	0.8259483575820923	You really need to go at the single trial level.
4333974	4340340	C	0.8006513118743896	You need to have high enough spatial resolution such that it satisfies Nyquist frequencies.
4342200	4347024	C	0.750445544719696	And this just is something that people didn't do for a long time, especially if you're doing single electrode recordings.
4347152	4348656	C	0.5025560259819031	You're not going to see a traveling wave.
4348688	4349940	A	0.8194695115089417	You're going to see oscillations.
4350680	4352804	C	0.8448325991630554	You need, like, multi electrode arrays.
4352932	4355976	C	0.6579883098602295	And basically they're saying, okay, yeah, now that we have the technology to do.
4355998	4359592	A	0.4584265351295471	This, it's much resists that we didn't see before.
4359646	4364492	C	0.6259093284606934	And potentially, this is an explanation for a lot of the noise that we were seeing before.
4364546	4366860	C	0.7875558137893677	Maybe it really is just traveling waves.
4367840	4372252	C	0.6803538799285889	So yeah, I think there's a lot to be done in the future with.
4372306	4375920	A	0.7699864506721497	Increased abilities for recording.
4376420	4377730	B	0.9849647879600525	That's very cool.
4378660	4385330	B	0.9092254042625427	Well, any final thoughts or questions or where are you going to take this work?
4386260	4387072	C	0.4872484803199768	Yeah, no.
4387126	4388290	C	0.9428794384002686	Thanks for having me.
4390020	4392240	C	0.6507731676101685	Hopefully in the active infrastructure direction.
4392740	4393856	C	0.9199475646018982	I would love to.
4393958	4395300	C	0.9822161197662354	I think it'll be super fun.
4395370	4396804	C	0.6744767427444458	So yeah, I'm not really sure.
4396842	4409668	C	0.7519580721855164	I'm looking at maybe music right now, looking at other kind of crazy directions.
4409844	4414956	C	0.47869133949279785	I don't want to sound too crazy, but I'll go down.
4415058	4416492	C	0.6493716239929199	Yeah, a lot of things.
4416546	4422140	C	0.9144124984741211	So one thing that's coming up, something we submitted to Neurops is studying memory with traveling waves.
4423200	4424876	A	0.7861068248748779	So that paper just came out on.
4424898	4430016	C	0.6901193857192993	Archive today of how waves are really good at encoding long term memories, which.
4430038	4431330	A	0.9766942262649536	I think is super interesting.
4432500	4434560	A	0.7482343316078186	So I might go a little more in that direction.
4435800	4437380	B	0.9471787214279175	Sounds good.
4437450	4448150	B	0.8974699974060059	And yes, would be very exciting to see action come into play when there was the neurons that stayed active even as the dog's feet were moving.
4449000	4464540	B	0.8586569428443909	There's a lot of action sequences, like throwing a baseball, and then it goes, and it's like there's something about that action that's continuing to influence it's to having, like, a deep temporal representation of alternative actions.
4466160	4474030	B	0.7777445316314697	And then the variational auto encoder is already basically the right anything like that.
4475120	4476444	A	0.9529626369476318	Really appreciate it.
4476562	4477532	B	0.8092430830001831	All right, thank you.
4477586	4478670	B	0.6996942758560181	Till next time.
4479520	4480620	A	0.9646503329277039	Thanks so much.
4480770	4481080	A	0.5137447118759155	Bye.

1
00:00:06,600 --> 00:00:09,420
hello and welcome it's September 18th

2
00:00:09,420 --> 00:00:14,040
2023 and it's active guest stream 57.1

3
00:00:14,040 --> 00:00:16,740
with Andy Keller we're going to be

4
00:00:16,740 --> 00:00:19,619
talking about natural neural structure

5
00:00:19,619 --> 00:00:22,020
for artificial intelligence there will

6
00:00:22,020 --> 00:00:24,300
be a presentation followed by a

7
00:00:24,300 --> 00:00:25,800
discussion so if you're watching live

8
00:00:25,800 --> 00:00:27,900
please feel free to write questions in

9
00:00:27,900 --> 00:00:30,660
the live chat otherwise thank you Andy

10
00:00:30,660 --> 00:00:32,279
for this really looking forward to it

11
00:00:32,279 --> 00:00:35,899
and to you for the presentation

12
00:00:36,360 --> 00:00:38,940
yeah thanks so much uh thanks for having

13
00:00:38,940 --> 00:00:41,280
me I'm super excited to be able to

14
00:00:41,280 --> 00:00:42,840
present this stuff with the active

15
00:00:42,840 --> 00:00:45,239
reference group I'm a fan and very

16
00:00:45,239 --> 00:00:49,200
interested so hopefully uh yeah get to

17
00:00:49,200 --> 00:00:50,399
have a good discussion and see what you

18
00:00:50,399 --> 00:00:51,960
guys think about it

19
00:00:51,960 --> 00:00:54,840
um so my name is Andy I'm finishing up

20
00:00:54,840 --> 00:00:57,180
my PhD supervised by maxwelling at the

21
00:00:57,180 --> 00:00:59,100
University of Amsterdam

22
00:00:59,100 --> 00:01:01,500
um I'm starting a postdoc Harvard after

23
00:01:01,500 --> 00:01:05,099
this so I'll start out I'm just talking

24
00:01:05,099 --> 00:01:07,619
about the goal of my work in general is

25
00:01:07,619 --> 00:01:09,540
to try to bring modern artificial

26
00:01:09,540 --> 00:01:12,540
intelligence closer to more human-like

27
00:01:12,540 --> 00:01:15,240
generalization and so what we mean by

28
00:01:15,240 --> 00:01:17,159
this is maybe some sort of structure

29
00:01:17,159 --> 00:01:18,960
generalization

30
00:01:18,960 --> 00:01:20,700
um or maybe more familiar to the active

31
00:01:20,700 --> 00:01:22,080
infants committee like a structured

32
00:01:22,080 --> 00:01:24,299
World model which we believe that humans

33
00:01:24,299 --> 00:01:26,340
have and the way that we propose to do

34
00:01:26,340 --> 00:01:28,799
this is by integrating natural neural

35
00:01:28,799 --> 00:01:32,400
structure into artificial intelligence

36
00:01:32,400 --> 00:01:34,979
so first let's define what we mean by

37
00:01:34,979 --> 00:01:36,720
structure generalization

38
00:01:36,720 --> 00:01:38,400
so I think it's fairly uncontroversial

39
00:01:38,400 --> 00:01:40,380
to say that modern machine learning

40
00:01:40,380 --> 00:01:42,900
generalizes Beyond its training set in

41
00:01:42,900 --> 00:01:45,000
the traditional sense so for example

42
00:01:45,000 --> 00:01:46,799
even the earliest artificial neural

43
00:01:46,799 --> 00:01:49,020
networks multi-layer perceptrons could

44
00:01:49,020 --> 00:01:51,659
be trained on data sets of images like

45
00:01:51,659 --> 00:01:55,140
this and Achieve high accuracy then when

46
00:01:55,140 --> 00:01:56,880
they're presented with a held out test

47
00:01:56,880 --> 00:01:58,320
set of images that they've never seen

48
00:01:58,320 --> 00:02:00,240
before they can still classify them

49
00:02:00,240 --> 00:02:02,460
relatively easily with the same level of

50
00:02:02,460 --> 00:02:04,500
accuracy and this is what we typically

51
00:02:04,500 --> 00:02:07,320
call generalization however even fairly

52
00:02:07,320 --> 00:02:08,758
early on it was noticed that these

53
00:02:08,758 --> 00:02:10,800
systems really struggle with small

54
00:02:10,800 --> 00:02:12,720
shifts or deformations apply to the

55
00:02:12,720 --> 00:02:16,340
images for example if

56
00:02:18,920 --> 00:02:23,099
so you think why is this surprising and

57
00:02:23,099 --> 00:02:24,720
I argue it's really due to our innate

58
00:02:24,720 --> 00:02:26,640
ability to perform this type of

59
00:02:26,640 --> 00:02:28,680
structure generalization that this

60
00:02:28,680 --> 00:02:31,920
example is a failure of so uh for

61
00:02:31,920 --> 00:02:33,239
example this shift is nearly

62
00:02:33,239 --> 00:02:35,340
imperceptible to us and we handle it

63
00:02:35,340 --> 00:02:37,560
automatically whereas in the system it's

64
00:02:37,560 --> 00:02:39,959
very clearly a major problem so in words

65
00:02:39,959 --> 00:02:41,940
we can say that structure generalization

66
00:02:41,940 --> 00:02:44,819
is a generalization to some symmetry

67
00:02:44,819 --> 00:02:47,040
transformations of the input or in this

68
00:02:47,040 --> 00:02:48,959
case the Symmetry transformation is a

69
00:02:48,959 --> 00:02:50,580
small shift that leaves the digit class

70
00:02:50,580 --> 00:02:52,019
unchanged

71
00:02:52,019 --> 00:02:54,599
so the obvious question then is what

72
00:02:54,599 --> 00:02:56,340
precisely do we mean by this natural

73
00:02:56,340 --> 00:02:58,860
structure and why do we think that this

74
00:02:58,860 --> 00:03:01,620
would help us with these settings

75
00:03:01,620 --> 00:03:03,780
so first let's talk about what we mean

76
00:03:03,780 --> 00:03:05,819
by natural neural structure

77
00:03:05,819 --> 00:03:08,700
um one way to talk about structure or or

78
00:03:08,700 --> 00:03:11,640
any type of bias in a system is an

79
00:03:11,640 --> 00:03:14,040
inductive bias and so an inductive bias

80
00:03:14,040 --> 00:03:16,080
can Loosely be defined as an appriore

81
00:03:16,080 --> 00:03:17,940
restriction of a set of realizable

82
00:03:17,940 --> 00:03:19,440
hypotheses when you're doing model

83
00:03:19,440 --> 00:03:22,440
selection more colloquially we can call

84
00:03:22,440 --> 00:03:24,480
this something like before seeing any

85
00:03:24,480 --> 00:03:27,060
data it's a restriction of what and how

86
00:03:27,060 --> 00:03:29,580
you can learn so very broadly this can

87
00:03:29,580 --> 00:03:32,519
include anything from model class to

88
00:03:32,519 --> 00:03:34,739
optimization procedures or even hyper

89
00:03:34,739 --> 00:03:37,379
parameters and in some sense they really

90
00:03:37,379 --> 00:03:39,739
Define what is

91
00:03:39,739 --> 00:03:43,140
what is possible to learn and it defines

92
00:03:43,140 --> 00:03:44,819
generalization in that

93
00:03:44,819 --> 00:03:47,220
you actually can't generalize Beyond a

94
00:03:47,220 --> 00:03:48,420
training set without having some

95
00:03:48,420 --> 00:03:50,220
inductifying solution this is explained

96
00:03:50,220 --> 00:03:52,560
more thoroughly in this paper by David

97
00:03:52,560 --> 00:03:55,500
Wolford so what we mean by natural

98
00:03:55,500 --> 00:03:58,620
inductive biases then is biases that

99
00:03:58,620 --> 00:04:00,000
stem from the restrictions and

100
00:04:00,000 --> 00:04:02,040
limitations that are faced by natural

101
00:04:02,040 --> 00:04:04,500
systems uh by the nature of having to

102
00:04:04,500 --> 00:04:06,900
live in the real world for example the

103
00:04:06,900 --> 00:04:08,459
brain has many efficiency constraints

104
00:04:08,459 --> 00:04:10,439
and physical constraints by nature of

105
00:04:10,439 --> 00:04:13,140
its construction uh and following this

106
00:04:13,140 --> 00:04:14,580
logic then these constraints are really

107
00:04:14,580 --> 00:04:16,620
playing some role in our generalization

108
00:04:16,620 --> 00:04:19,798
abilities which currently exceed modern

109
00:04:19,798 --> 00:04:21,478
artificial intelligence as we'll go into

110
00:04:21,478 --> 00:04:24,720
next so in this talk I'll be focusing

111
00:04:24,720 --> 00:04:27,540
specifically on two types of structure

112
00:04:27,540 --> 00:04:29,880
which my work has studied these are

113
00:04:29,880 --> 00:04:31,699
topographic organization and

114
00:04:31,699 --> 00:04:34,320
spatiotemporal Dynamics and before I go

115
00:04:34,320 --> 00:04:36,120
into my work I'll give a short example

116
00:04:36,120 --> 00:04:38,759
for why I believe that natural structure

117
00:04:38,759 --> 00:04:41,400
may be useful to achieve the structure

118
00:04:41,400 --> 00:04:42,780
generalization that I was talking about

119
00:04:42,780 --> 00:04:44,280
before

120
00:04:44,280 --> 00:04:47,479
so the first example comes from uh

121
00:04:47,479 --> 00:04:49,199
hukushima's neocognitive front

122
00:04:49,199 --> 00:04:51,479
architecture from the 1980s which was

123
00:04:51,479 --> 00:04:53,520
actually which was actually built to

124
00:04:53,520 --> 00:04:55,440
directly address the problem of

125
00:04:55,440 --> 00:04:57,300
robustness to these small shifts and

126
00:04:57,300 --> 00:04:59,759
deformations so in the paperwork he

127
00:04:59,759 --> 00:05:02,040
writes about inspiration from pupil and

128
00:05:02,040 --> 00:05:04,380
weasel's measurements of hierarchy and

129
00:05:04,380 --> 00:05:06,780
pooling in order to achieve robustness

130
00:05:06,780 --> 00:05:08,699
to these distortions and so if you look

131
00:05:08,699 --> 00:05:11,160
at the figure if he writes U sub S1 U

132
00:05:11,160 --> 00:05:14,100
sub C1 and these stand for simple and

133
00:05:14,100 --> 00:05:16,919
complex cells and so this is a fairly

134
00:05:16,919 --> 00:05:18,840
radical Approach at the time but it

135
00:05:18,840 --> 00:05:20,699
really served to improve robustness and

136
00:05:20,699 --> 00:05:22,199
shifts that we're plaguing these early

137
00:05:22,199 --> 00:05:24,419
artificial neural networks and over time

138
00:05:24,419 --> 00:05:25,860
these ideas were simplified and

139
00:05:25,860 --> 00:05:28,919
abstracted and obviously yielded the

140
00:05:28,919 --> 00:05:30,539
convolutional neural networks so we know

141
00:05:30,539 --> 00:05:32,759
today which ultimately drove the success

142
00:05:32,759 --> 00:05:35,580
of the deep learning Revolution so this

143
00:05:35,580 --> 00:05:36,960
is really an example of a natural

144
00:05:36,960 --> 00:05:39,479
inductive bias which achieved structure

145
00:05:39,479 --> 00:05:42,120
generalization so for our research it's

146
00:05:42,120 --> 00:05:43,919
really utmost interest to try and

147
00:05:43,919 --> 00:05:45,900
understand what makes these models work

148
00:05:45,900 --> 00:05:47,280
so well

149
00:05:47,280 --> 00:05:49,320
um and see if this principle can

150
00:05:49,320 --> 00:05:51,360
potentially be generalized to cover more

151
00:05:51,360 --> 00:05:53,940
abstract more abstract Transformations

152
00:05:53,940 --> 00:05:57,000
and symmetries

153
00:05:57,000 --> 00:06:00,360
so what makes a convolution achieve this

154
00:06:00,360 --> 00:06:02,060
structure generalization

155
00:06:02,060 --> 00:06:04,440
intuitively you can see this is done by

156
00:06:04,440 --> 00:06:06,720
applying the same filter at or or

157
00:06:06,720 --> 00:06:08,699
feature extractor at various spatial

158
00:06:08,699 --> 00:06:10,680
locations so here we see a single

159
00:06:10,680 --> 00:06:12,660
convolutional filter being applied at

160
00:06:12,660 --> 00:06:14,820
all locations of an image this means

161
00:06:14,820 --> 00:06:16,259
that no matter where your input is

162
00:06:16,259 --> 00:06:18,000
whether it's kind of in the middle of

163
00:06:18,000 --> 00:06:20,400
the image or on the right you'll have

164
00:06:20,400 --> 00:06:22,080
the exact same features with one

165
00:06:22,080 --> 00:06:23,580
exception they'll be equivalently

166
00:06:23,580 --> 00:06:24,660
shifted

167
00:06:24,660 --> 00:06:26,819
so mathematically this type of a mapping

168
00:06:26,819 --> 00:06:29,160
is called a homomorphism it preserves

169
00:06:29,160 --> 00:06:30,840
the algebraic structure of the input

170
00:06:30,840 --> 00:06:33,180
space and the output space in this case

171
00:06:33,180 --> 00:06:35,580
it's with respect to translation and at

172
00:06:35,580 --> 00:06:37,440
a simple a simple level something like

173
00:06:37,440 --> 00:06:38,880
that will be important to remember for

174
00:06:38,880 --> 00:06:40,259
the rest of this talk is that we can

175
00:06:40,259 --> 00:06:42,300
verify homomorphisms of our feature

176
00:06:42,300 --> 00:06:45,000
extractor if we can see that there's

177
00:06:45,000 --> 00:06:46,740
this community commutation with the

178
00:06:46,740 --> 00:06:49,560
Transformations commutative diagram and

179
00:06:49,560 --> 00:06:51,720
so we can write this also algebraically

180
00:06:51,720 --> 00:06:53,759
by showing that the feature extractor f

181
00:06:53,759 --> 00:06:55,080
commutes with the transformation

182
00:06:55,080 --> 00:06:57,000
operator t

183
00:06:57,000 --> 00:06:58,919
and basically what we want is there to

184
00:06:58,919 --> 00:07:00,600
be no difference between first

185
00:07:00,600 --> 00:07:02,639
extracting the features and then

186
00:07:02,639 --> 00:07:04,740
performing the transformation or

187
00:07:04,740 --> 00:07:06,240
performing the transformation and then

188
00:07:06,240 --> 00:07:08,639
extracting the features so the challenge

189
00:07:08,639 --> 00:07:10,199
is to date is

190
00:07:10,199 --> 00:07:11,880
we don't really know how to construct

191
00:07:11,880 --> 00:07:13,620
homomorphisms with respect to more

192
00:07:13,620 --> 00:07:15,240
complex Transformations that we see in

193
00:07:15,240 --> 00:07:18,060
the real world for example our brain is

194
00:07:18,060 --> 00:07:20,099
able to handle changes in lighting and

195
00:07:20,099 --> 00:07:22,259
season naturally

196
00:07:22,259 --> 00:07:24,599
um so here we see Lighting on a person's

197
00:07:24,599 --> 00:07:26,160
face or the change of seasons we can

198
00:07:26,160 --> 00:07:27,840
tell it's the same face or the the same

199
00:07:27,840 --> 00:07:29,880
road but we don't know how to build

200
00:07:29,880 --> 00:07:31,319
models which respect these

201
00:07:31,319 --> 00:07:33,180
Transformations and so it makes us hard

202
00:07:33,180 --> 00:07:35,520
to build systems which handle them in a

203
00:07:35,520 --> 00:07:37,620
robust and predictable way

204
00:07:37,620 --> 00:07:40,259
to give an even more abstract example of

205
00:07:40,259 --> 00:07:41,759
what I mean by this and the potential

206
00:07:41,759 --> 00:07:43,620
negative repercussions of models which

207
00:07:43,620 --> 00:07:45,440
don't handle symmetry Transformations

208
00:07:45,440 --> 00:07:48,060
consider modern text to image generation

209
00:07:48,060 --> 00:07:50,520
programs so in this example I asked

210
00:07:50,520 --> 00:07:53,940
Dolly to to generate a image of a teddy

211
00:07:53,940 --> 00:07:55,620
bear on the moon and it does this

212
00:07:55,620 --> 00:07:57,180
incredibly well right probably better

213
00:07:57,180 --> 00:08:00,960
than I could it has texture fur uh

214
00:08:00,960 --> 00:08:03,360
incredibly detailed however if I ask you

215
00:08:03,360 --> 00:08:05,340
to do something which I see is

216
00:08:05,340 --> 00:08:08,039
conceptually simpler such as draw a blue

217
00:08:08,039 --> 00:08:10,560
cube on top of a Red Cube it fails to do

218
00:08:10,560 --> 00:08:13,380
this and to me this seems unintuitive

219
00:08:13,380 --> 00:08:15,300
since the second task seems

220
00:08:15,300 --> 00:08:18,180
significantly easier but what I'm

221
00:08:18,180 --> 00:08:19,860
arguing is that the reason that this is

222
00:08:19,860 --> 00:08:21,599
surprising is precisely the same reason

223
00:08:21,599 --> 00:08:23,580
that the amnest translation example was

224
00:08:23,580 --> 00:08:25,560
surprising there is this symmetry

225
00:08:25,560 --> 00:08:28,020
transformation Happening Here namely the

226
00:08:28,020 --> 00:08:29,400
transformation between these complex

227
00:08:29,400 --> 00:08:31,740
objects of a teddy bear and the moon and

228
00:08:31,740 --> 00:08:34,320
these simple objects of Cubes which we

229
00:08:34,320 --> 00:08:36,360
intuitively expect the network to be

230
00:08:36,360 --> 00:08:38,820
able to handle and respect and we see

231
00:08:38,820 --> 00:08:40,919
that it doesn't so just like how

232
00:08:40,919 --> 00:08:43,380
fukushima's work showed that these

233
00:08:43,380 --> 00:08:46,200
natural structure of hierarchy and

234
00:08:46,200 --> 00:08:47,700
pooling of our visual system are

235
00:08:47,700 --> 00:08:49,680
effective for making generalizations to

236
00:08:49,680 --> 00:08:52,380
small Transformations I argue that

237
00:08:52,380 --> 00:08:54,060
potentially higher level structure may

238
00:08:54,060 --> 00:08:55,740
be necessary to fix these abstract

239
00:08:55,740 --> 00:08:58,200
generalization problems

240
00:08:58,200 --> 00:09:01,380
and so the question then that I'm

241
00:09:01,380 --> 00:09:04,380
studying and that I'm asking is what

242
00:09:04,380 --> 00:09:06,060
might this structure be and how do we

243
00:09:06,060 --> 00:09:08,640
implement this in a artificial neural

244
00:09:08,640 --> 00:09:10,080
network architecture that can actually

245
00:09:10,080 --> 00:09:14,120
be used for performing computation

246
00:09:14,880 --> 00:09:17,700
so to begin to answer that I'll jump

247
00:09:17,700 --> 00:09:19,680
into my first line of work on

248
00:09:19,680 --> 00:09:22,380
topographic organization

249
00:09:22,380 --> 00:09:25,260
so topographic organization is observed

250
00:09:25,260 --> 00:09:27,060
widely throughout the brain from primary

251
00:09:27,060 --> 00:09:29,760
visual cortex Sapphire level areas and

252
00:09:29,760 --> 00:09:31,500
it can very Loosely be described as this

253
00:09:31,500 --> 00:09:33,540
property that neurons which are close to

254
00:09:33,540 --> 00:09:35,760
one another tend to respond to similar

255
00:09:35,760 --> 00:09:38,220
things for example on the left we show

256
00:09:38,220 --> 00:09:39,720
the color-coded preference of each

257
00:09:39,720 --> 00:09:42,959
neuron in the primary digital cortex as

258
00:09:42,959 --> 00:09:45,360
a response to oriented lines and we see

259
00:09:45,360 --> 00:09:46,740
this smoothly varying set of

260
00:09:46,740 --> 00:09:48,779
selectivities another type of

261
00:09:48,779 --> 00:09:50,580
organization is known as retina topic

262
00:09:50,580 --> 00:09:52,560
organization where nearby neurons in the

263
00:09:52,560 --> 00:09:54,600
visual cortex tend to respond to nearby

264
00:09:54,600 --> 00:09:56,399
receptive fields

265
00:09:56,399 --> 00:09:58,560
however this organization isn't limited

266
00:09:58,560 --> 00:10:01,080
to these low-level features extends some

267
00:10:01,080 --> 00:10:02,519
more complex features such as those

268
00:10:02,519 --> 00:10:05,459
present in faces or objects or places

269
00:10:05,459 --> 00:10:07,920
and this relates to the so-called

270
00:10:07,920 --> 00:10:10,080
functionally specific areas of the brain

271
00:10:10,080 --> 00:10:12,779
such as the fusiform face area FFA and

272
00:10:12,779 --> 00:10:15,420
the parakhippocampal face area PPA

273
00:10:15,420 --> 00:10:19,200
so in this work the main idea again is

274
00:10:19,200 --> 00:10:21,300
that perhaps this topographic

275
00:10:21,300 --> 00:10:23,580
organization in some sense which is

276
00:10:23,580 --> 00:10:25,080
intimately related to the convolution

277
00:10:25,080 --> 00:10:27,980
operation and fukushima's architecture

278
00:10:27,980 --> 00:10:30,660
we can maybe generalize the benefits of

279
00:10:30,660 --> 00:10:33,420
this to more abstract transformations in

280
00:10:33,420 --> 00:10:34,920
other words learn how to build more

281
00:10:34,920 --> 00:10:36,839
complex homomorphisms that we can't do

282
00:10:36,839 --> 00:10:38,940
you know we can't do analytically right

283
00:10:38,940 --> 00:10:40,740
now

284
00:10:40,740 --> 00:10:42,480
so just to show that we're not

285
00:10:42,480 --> 00:10:44,760
completely insane with this idea uh

286
00:10:44,760 --> 00:10:46,320
there is some prior work in this domain

287
00:10:46,320 --> 00:10:49,880
from people such as uh Conan Galaxy

288
00:10:49,880 --> 00:10:54,060
Barden in the early 90s and and 2000s

289
00:10:54,060 --> 00:10:55,800
and they studied how topographic

290
00:10:55,800 --> 00:10:57,720
organization may be useful for learning

291
00:10:57,720 --> 00:11:01,320
in variances mostly in linear models so

292
00:11:01,320 --> 00:11:03,060
the question for us when we entered the

293
00:11:03,060 --> 00:11:04,680
space is what is the most scalable

294
00:11:04,680 --> 00:11:07,140
abstract mechanism that can be leveraged

295
00:11:07,140 --> 00:11:08,880
from these approaches which we can

296
00:11:08,880 --> 00:11:10,800
integrate into modern deep neural

297
00:11:10,800 --> 00:11:12,959
network architectures and ultimately we

298
00:11:12,959 --> 00:11:15,000
settled on a generative modeling

299
00:11:15,000 --> 00:11:16,260
approach which I think might be

300
00:11:16,260 --> 00:11:17,519
interesting to the people in this

301
00:11:17,519 --> 00:11:18,779
community

302
00:11:18,779 --> 00:11:21,779
um which then allows us to relate it

303
00:11:21,779 --> 00:11:23,579
more closely to topographic independent

304
00:11:23,579 --> 00:11:26,040
component analysis with the basic idea

305
00:11:26,040 --> 00:11:28,320
being that we can learn a topographic

306
00:11:28,320 --> 00:11:30,660
feature Space by imposing a topographic

307
00:11:30,660 --> 00:11:32,940
prior distribution over our latent

308
00:11:32,940 --> 00:11:34,440
variables

309
00:11:34,440 --> 00:11:37,079
so just to give a brief background I

310
00:11:37,079 --> 00:11:39,120
assume most people are already familiar

311
00:11:39,120 --> 00:11:40,440
with this

312
00:11:40,440 --> 00:11:42,540
um but the kind of General assumption is

313
00:11:42,540 --> 00:11:44,339
that the brain is a generative model and

314
00:11:44,339 --> 00:11:45,720
this idea in some sense can be

315
00:11:45,720 --> 00:11:48,060
attributed to helmholts from the 19th

316
00:11:48,060 --> 00:11:50,459
century uh where he said that what we

317
00:11:50,459 --> 00:11:52,140
see is the solution to a computational

318
00:11:52,140 --> 00:11:54,420
problem our brains compute the most

319
00:11:54,420 --> 00:11:56,519
likely causes from Photon absorptions

320
00:11:56,519 --> 00:11:59,519
within our eyes and so that's an example

321
00:11:59,519 --> 00:12:01,920
if I show you this image you immediately

322
00:12:01,920 --> 00:12:03,720
recognize it as a globe with some

323
00:12:03,720 --> 00:12:05,760
curvature however it could just as

324
00:12:05,760 --> 00:12:07,620
equally be a disc with a distorted

325
00:12:07,620 --> 00:12:09,600
perspective on that so this is how we

326
00:12:09,600 --> 00:12:12,660
get optical illusions or our images so

327
00:12:12,660 --> 00:12:14,880
like this one your brain infers that

328
00:12:14,880 --> 00:12:17,100
there is a cube here because of the

329
00:12:17,100 --> 00:12:18,720
structure but really it's just a flat

330
00:12:18,720 --> 00:12:19,860
piece of paper

331
00:12:19,860 --> 00:12:22,740
so you can think of this generative

332
00:12:22,740 --> 00:12:24,480
model aspect that's kind of like an

333
00:12:24,480 --> 00:12:26,160
inverse Graphics program

334
00:12:26,160 --> 00:12:28,140
in the program the abstract properties

335
00:12:28,140 --> 00:12:30,660
of the sphere are known the position the

336
00:12:30,660 --> 00:12:32,820
size the lighting and these are used to

337
00:12:32,820 --> 00:12:34,560
project the sphere to create the 2D

338
00:12:34,560 --> 00:12:37,440
image that is rendered so in effect what

339
00:12:37,440 --> 00:12:40,019
Humboldt's and others are saying is that

340
00:12:40,019 --> 00:12:41,940
as a generative model the brain is

341
00:12:41,940 --> 00:12:43,680
actually trying to invert this

342
00:12:43,680 --> 00:12:45,959
generative process and doing inference

343
00:12:45,959 --> 00:12:48,300
and infer the underlying causes of our

344
00:12:48,300 --> 00:12:49,680
Sensations

345
00:12:49,680 --> 00:12:51,600
so the reason I'm kind of belaboring

346
00:12:51,600 --> 00:12:53,100
this point is that there's a lot of

347
00:12:53,100 --> 00:12:55,440
talking generative models today

348
00:12:55,440 --> 00:12:57,180
um and I'm not necessarily just talking

349
00:12:57,180 --> 00:12:59,639
about generating images or pretty

350
00:12:59,639 --> 00:13:01,260
pictures

351
00:13:01,260 --> 00:13:03,120
um I really want to mean

352
00:13:03,120 --> 00:13:07,920
a framework for unsupervised learning

353
00:13:07,920 --> 00:13:10,019
so then to get a little bit more into

354
00:13:10,019 --> 00:13:11,700
the details what do I mean by a

355
00:13:11,700 --> 00:13:14,279
topographic prior so generative models

356
00:13:14,279 --> 00:13:16,019
are typically described as a joint

357
00:13:16,019 --> 00:13:18,720
distribution over observations X and

358
00:13:18,720 --> 00:13:21,720
latent variables which we'll call Z uh

359
00:13:21,720 --> 00:13:23,940
and this is typically factorized or one

360
00:13:23,940 --> 00:13:25,620
way that this is done is factorized in

361
00:13:25,620 --> 00:13:28,440
terms of a prior P of Z and this true

362
00:13:28,440 --> 00:13:30,420
generative model conditional generative

363
00:13:30,420 --> 00:13:33,420
model P of x given Z and so one way that

364
00:13:33,420 --> 00:13:34,980
we can think about this is that the

365
00:13:34,980 --> 00:13:37,019
prior can be seen to encode relative

366
00:13:37,019 --> 00:13:38,880
penalties for each type of code that is

367
00:13:38,880 --> 00:13:41,279
produced when we invert our generative

368
00:13:41,279 --> 00:13:42,899
model this is called Computing the

369
00:13:42,899 --> 00:13:45,920
posterior P of Z given X

370
00:13:45,920 --> 00:13:49,139
and so to develop a topographic latent

371
00:13:49,139 --> 00:13:50,639
space we want to introduce some sort of

372
00:13:50,639 --> 00:13:53,279
a topographic prior which has been or

373
00:13:53,279 --> 00:13:55,620
which this topographic ICA work showed

374
00:13:55,620 --> 00:13:57,720
is equivalent to something like a group

375
00:13:57,720 --> 00:13:59,700
sparsity penalty

376
00:13:59,700 --> 00:14:01,500
so people might be familiar with typical

377
00:14:01,500 --> 00:14:03,180
sparsity penalties from Independent

378
00:14:03,180 --> 00:14:04,560
Learning analysis you want your

379
00:14:04,560 --> 00:14:06,540
activations to be sparse meaning many of

380
00:14:06,540 --> 00:14:09,420
them are zero wow and so that could look

381
00:14:09,420 --> 00:14:10,680
something like this you have a bunch of

382
00:14:10,680 --> 00:14:12,300
blue squares that are active but most of

383
00:14:12,300 --> 00:14:14,459
them are not active but specifically

384
00:14:14,459 --> 00:14:16,740
with the group's Varsity penalty we want

385
00:14:16,740 --> 00:14:18,839
these priors to assign lower probability

386
00:14:18,839 --> 00:14:21,600
to these distributed sparse activations

387
00:14:21,600 --> 00:14:24,720
and higher probability to these grouped

388
00:14:24,720 --> 00:14:26,940
densely packed representations you can

389
00:14:26,940 --> 00:14:28,860
also think of this like a higher penalty

390
00:14:28,860 --> 00:14:30,720
when things are spread out a lower

391
00:14:30,720 --> 00:14:33,540
penalty when things are closer together

392
00:14:33,540 --> 00:14:36,380
so again uh this can be written

393
00:14:36,380 --> 00:14:39,060
abstractly like this but I want to make

394
00:14:39,060 --> 00:14:41,160
theory that these neurons each one of

395
00:14:41,160 --> 00:14:42,779
these squares here represents kind of a

396
00:14:42,779 --> 00:14:44,220
neuron in our model and they're

397
00:14:44,220 --> 00:14:46,560
organized in this 2D grid so when we're

398
00:14:46,560 --> 00:14:48,120
talking about grouping we really mean

399
00:14:48,120 --> 00:14:50,820
grouping in that 2D topology

400
00:14:50,820 --> 00:14:53,100
so one thing that's really interesting

401
00:14:53,100 --> 00:14:55,560
and kind of important is that these

402
00:14:55,560 --> 00:14:57,779
priors don't just give us topographic

403
00:14:57,779 --> 00:15:00,540
organization but they've also been noted

404
00:15:00,540 --> 00:15:02,459
by people like or studied by people like

405
00:15:02,459 --> 00:15:05,760
erosi Marcelli and Bruno also to

406
00:15:05,760 --> 00:15:07,740
actually fit the statistics of natural

407
00:15:07,740 --> 00:15:08,959
data better

408
00:15:08,959 --> 00:15:11,180
specifically natural images

409
00:15:11,180 --> 00:15:14,100
they've shown that using this type of a

410
00:15:14,100 --> 00:15:16,139
prior you actually get a sparser set of

411
00:15:16,139 --> 00:15:18,839
activations meaning that the prior fits

412
00:15:18,839 --> 00:15:20,459
the true generative process a little bit

413
00:15:20,459 --> 00:15:22,620
better and as we're aware the brain has

414
00:15:22,620 --> 00:15:24,779
a high degree of sparsity and this is

415
00:15:24,779 --> 00:15:26,339
believed to be very relevant for

416
00:15:26,339 --> 00:15:28,620
efficiency

417
00:15:28,620 --> 00:15:30,839
so to get a little bit more into the

418
00:15:30,839 --> 00:15:32,760
details to implement this type of a

419
00:15:32,760 --> 00:15:35,160
group sparse prior we use a hierarchical

420
00:15:35,160 --> 00:15:37,320
generative model and this is uh

421
00:15:37,320 --> 00:15:39,060
basically introduced by some of the

422
00:15:39,060 --> 00:15:41,339
topographic ICA work

423
00:15:41,339 --> 00:15:43,320
um the idea is that you have a higher

424
00:15:43,320 --> 00:15:45,000
level latent variable U which

425
00:15:45,000 --> 00:15:47,820
simultaneously regulates the variance of

426
00:15:47,820 --> 00:15:50,279
multiple lower level variables T and

427
00:15:50,279 --> 00:15:52,440
this is how we get group sparsity

428
00:15:52,440 --> 00:15:55,440
then to get topographic organization you

429
00:15:55,440 --> 00:15:56,760
can have multiple of these latent

430
00:15:56,760 --> 00:15:59,339
variables used slightly overlapping with

431
00:15:59,339 --> 00:16:02,519
their fields of influence so their their

432
00:16:02,519 --> 00:16:04,260
neighborhoods we can call them

433
00:16:04,260 --> 00:16:05,699
and this will give you this smooth

434
00:16:05,699 --> 00:16:07,440
correlation structure your act you're

435
00:16:07,440 --> 00:16:09,899
after so get the intuition for this you

436
00:16:09,899 --> 00:16:12,060
see that this variable T over down on

437
00:16:12,060 --> 00:16:14,279
the bottom here is not getting any input

438
00:16:14,279 --> 00:16:17,160
from this U on the top but it is sharing

439
00:16:17,160 --> 00:16:19,139
a u variable with this T in the middle

440
00:16:19,139 --> 00:16:21,300
so it's like they're sharing variants

441
00:16:21,300 --> 00:16:22,980
they're sharing some components with

442
00:16:22,980 --> 00:16:24,959
their neighbors but not all components

443
00:16:24,959 --> 00:16:26,579
and that's really due to this local

444
00:16:26,579 --> 00:16:28,079
connectivity of these higher level

445
00:16:28,079 --> 00:16:30,779
variables you

446
00:16:30,779 --> 00:16:33,180
so to keep it simple about how we use a

447
00:16:33,180 --> 00:16:34,980
generative model let's go back to a

448
00:16:34,980 --> 00:16:37,440
single U variable and the challenge in

449
00:16:37,440 --> 00:16:38,940
this type of an architecture which made

450
00:16:38,940 --> 00:16:42,360
it difficult for for many years is how

451
00:16:42,360 --> 00:16:44,579
do you infer the approximate posterior

452
00:16:44,579 --> 00:16:47,579
over these intermediate variables in

453
00:16:47,579 --> 00:16:50,100
this hierarchical architecture and this

454
00:16:50,100 --> 00:16:52,560
is not super straightforward so prior

455
00:16:52,560 --> 00:16:54,420
Works have used heuristics developed for

456
00:16:54,420 --> 00:16:56,699
linear models and in our work we found

457
00:16:56,699 --> 00:16:58,680
that this really didn't extend to Modern

458
00:16:58,680 --> 00:17:01,199
neural network architectures so really

459
00:17:01,199 --> 00:17:02,880
our Insight is to leverage a

460
00:17:02,880 --> 00:17:04,760
factorization a specific

461
00:17:04,760 --> 00:17:07,640
re-parametrization of this distribution

462
00:17:07,640 --> 00:17:10,380
and so this parameterization

463
00:17:10,380 --> 00:17:12,419
specifically is achieved by defining the

464
00:17:12,419 --> 00:17:14,579
priority what's known as a gaussian

465
00:17:14,579 --> 00:17:16,319
scale mixture meaning that our

466
00:17:16,319 --> 00:17:19,140
conditional distribution of T given U is

467
00:17:19,140 --> 00:17:21,179
actually a normal distribution where the

468
00:17:21,179 --> 00:17:24,299
the variance is defined by this variable

469
00:17:24,299 --> 00:17:27,720
U and for certain choices of U this

470
00:17:27,720 --> 00:17:29,340
distribution is indeed sparse and

471
00:17:29,340 --> 00:17:31,980
encompasses a range of distributions

472
00:17:31,980 --> 00:17:33,780
such as laplossians a suit and T

473
00:17:33,780 --> 00:17:36,299
distributions one way of defining it is

474
00:17:36,299 --> 00:17:38,940
a gaussian scale mixture emits a

475
00:17:38,940 --> 00:17:40,919
particular reframe re-parameterization

476
00:17:40,919 --> 00:17:42,900
in terms of independent gaussian random

477
00:17:42,900 --> 00:17:45,720
variables Z and U so specifically then

478
00:17:45,720 --> 00:17:48,840
we we see that this T variable which was

479
00:17:48,840 --> 00:17:50,760
originally fairly complex is actually

480
00:17:50,760 --> 00:17:52,799
just a product of a bunch of gaussian

481
00:17:52,799 --> 00:17:54,840
random variables which now know how to

482
00:17:54,840 --> 00:17:57,660
work with much more efficiently uh in in

483
00:17:57,660 --> 00:18:00,120
generative models specifically what

484
00:18:00,120 --> 00:18:02,039
we're going to do is so that we can

485
00:18:02,039 --> 00:18:04,020
actually get approximate posteriors for

486
00:18:04,020 --> 00:18:06,720
U and Z separately and then do a

487
00:18:06,720 --> 00:18:08,640
deterministic combination of them in

488
00:18:08,640 --> 00:18:10,020
order to compute our topographic

489
00:18:10,020 --> 00:18:13,140
variable T and this is much easier to do

490
00:18:13,140 --> 00:18:15,500
so without going into too many details

491
00:18:15,500 --> 00:18:17,700
the method that we decided to use is

492
00:18:17,700 --> 00:18:18,600
what's known as a variational

493
00:18:18,600 --> 00:18:20,640
autoencoder which leverages techniques

494
00:18:20,640 --> 00:18:23,220
from variational inference to derive a

495
00:18:23,220 --> 00:18:24,780
lower bound on the likelihood allowing

496
00:18:24,780 --> 00:18:26,940
us to parametrize these approximate

497
00:18:26,940 --> 00:18:29,400
posteriors with powerful non-linear deep

498
00:18:29,400 --> 00:18:30,960
neural networks and optimize them with

499
00:18:30,960 --> 00:18:33,240
gradient descent this is going to be

500
00:18:33,240 --> 00:18:34,440
familiar to the active inference

501
00:18:34,440 --> 00:18:36,900
Community but really what we've done is

502
00:18:36,900 --> 00:18:38,760
instead of having a single encoder in

503
00:18:38,760 --> 00:18:41,640
decoder as a typical baes we now have

504
00:18:41,640 --> 00:18:43,799
two encoders one for you and one for Z

505
00:18:43,799 --> 00:18:46,200
separately and then we combine them in

506
00:18:46,200 --> 00:18:48,419
this deterministic manner to construct

507
00:18:48,419 --> 00:18:51,660
our topographic T variable if you see

508
00:18:51,660 --> 00:18:53,100
that this is a is actually the

509
00:18:53,100 --> 00:18:54,900
construction of a student's T

510
00:18:54,900 --> 00:18:57,620
distribution from gaussians

511
00:18:57,620 --> 00:19:00,480
and then we can plug this we do this

512
00:19:00,480 --> 00:19:03,600
before decoding and uh and then maximize

513
00:19:03,600 --> 00:19:05,820
the likelihood of the data altogether so

514
00:19:05,820 --> 00:19:07,260
this is the elbow the evidence lower

515
00:19:07,260 --> 00:19:09,360
bound abound on the likelihood of the

516
00:19:09,360 --> 00:19:12,600
data and is actually very similar to the

517
00:19:12,600 --> 00:19:15,720
variational free energy that is used in

518
00:19:15,720 --> 00:19:18,299
active entrance community

519
00:19:18,299 --> 00:19:20,520
so with with these details out of the

520
00:19:20,520 --> 00:19:22,500
way what's really interesting is what

521
00:19:22,500 --> 00:19:23,940
happens when we train this generative

522
00:19:23,940 --> 00:19:26,580
model which has relatively simple group

523
00:19:26,580 --> 00:19:29,460
sparsity penalty in its latent space and

524
00:19:29,460 --> 00:19:30,720
we want to look at kind of what it's

525
00:19:30,720 --> 00:19:32,700
learning in terms of its organization of

526
00:19:32,700 --> 00:19:34,980
Futures and first we start with the

527
00:19:34,980 --> 00:19:36,480
simplest possible data set we have a

528
00:19:36,480 --> 00:19:38,580
black background with white squares at

529
00:19:38,580 --> 00:19:41,280
random XY locations and if we train our

530
00:19:41,280 --> 00:19:42,780
autoencoder with this group sparsity

531
00:19:42,780 --> 00:19:44,760
penalty on it and then we look at the

532
00:19:44,760 --> 00:19:47,640
weight vectors of our decoder which

533
00:19:47,640 --> 00:19:49,020
we're plotting in blue here again

534
00:19:49,020 --> 00:19:52,520
organized on this 2D grid we see that

535
00:19:52,520 --> 00:19:54,780
indeed they learn to be organized

536
00:19:54,780 --> 00:19:57,539
according to spatial location so this

537
00:19:57,539 --> 00:19:59,580
can be seen as similar to convolutional

538
00:19:59,580 --> 00:20:01,799
receptive fields or the receptive field

539
00:20:01,799 --> 00:20:04,679
of each neuron is really given by the

540
00:20:04,679 --> 00:20:09,059
the kind of inputs at its location

541
00:20:09,059 --> 00:20:10,860
and this makes sense intuitively from

542
00:20:10,860 --> 00:20:13,140
the group's sparsity perspective since

543
00:20:13,140 --> 00:20:15,480
for any given region to highlight like

544
00:20:15,480 --> 00:20:17,700
in yellow here the filters in a given

545
00:20:17,700 --> 00:20:19,260
group are much more highly correlated

546
00:20:19,260 --> 00:20:20,880
they have these overlapping receptive

547
00:20:20,880 --> 00:20:23,460
Fields than other random locations so

548
00:20:23,460 --> 00:20:25,020
essentially we see that our model is

549
00:20:25,020 --> 00:20:27,840
learning to Cluster activity activities

550
00:20:27,840 --> 00:20:28,980
together

551
00:20:28,980 --> 00:20:32,059
uh in sort of a simulated vertical sheet

552
00:20:32,059 --> 00:20:34,260
according to the correlations in the

553
00:20:34,260 --> 00:20:36,840
data set so instead of in convolution

554
00:20:36,840 --> 00:20:38,580
where you're actually doing weight tying

555
00:20:38,580 --> 00:20:40,860
and you're manually specifying I want to

556
00:20:40,860 --> 00:20:42,539
copy this weight everywhere you can

557
00:20:42,539 --> 00:20:44,220
maybe think of this as like approximate

558
00:20:44,220 --> 00:20:45,500
wait time

559
00:20:45,500 --> 00:20:48,120
and really we're learning this from the

560
00:20:48,120 --> 00:20:49,620
correlation structure of the data set

561
00:20:49,620 --> 00:20:51,660
itself and just to give a little bit

562
00:20:51,660 --> 00:20:54,120
more of a biological inspiration for

563
00:20:54,120 --> 00:20:56,280
this and we know that retinotopia is

564
00:20:56,280 --> 00:20:58,020
present in the brain this is an example

565
00:20:58,020 --> 00:21:02,460
of retinotopia in the visual cortex and

566
00:21:02,460 --> 00:21:04,500
you can see if you show the macaque an

567
00:21:04,500 --> 00:21:06,780
image like this it gets projected into

568
00:21:06,780 --> 00:21:08,520
this uh

569
00:21:08,520 --> 00:21:11,700
topology preserving space actually on

570
00:21:11,700 --> 00:21:13,740
the surface of the cortex

571
00:21:13,740 --> 00:21:16,080
so the idea is that topographic

572
00:21:16,080 --> 00:21:18,419
organization and even learn topographic

573
00:21:18,419 --> 00:21:21,299
organization is preserving the input

574
00:21:21,299 --> 00:21:26,160
correlations of our data sets uh and and

575
00:21:26,160 --> 00:21:28,679
potentially uh this may be beneficial

576
00:21:28,679 --> 00:21:30,840
for generalizing these ideas a little

577
00:21:30,840 --> 00:21:32,340
bit further so like I said at the

578
00:21:32,340 --> 00:21:34,679
beginning uh it would be even better if

579
00:21:34,679 --> 00:21:37,200
we could just learn something more than

580
00:21:37,200 --> 00:21:39,320
just convolution maybe more complicated

581
00:21:39,320 --> 00:21:43,679
equivariances so how do we do that one

582
00:21:43,679 --> 00:21:45,720
thing that's clear in natural

583
00:21:45,720 --> 00:21:48,299
intelligence is that we don't exist in

584
00:21:48,299 --> 00:21:51,120
this world of IID frames right we exist

585
00:21:51,120 --> 00:21:53,520
in a world with continuous sequences of

586
00:21:53,520 --> 00:21:55,620
Transformations so maybe we can extend

587
00:21:55,620 --> 00:21:58,440
our model to this setting to learn

588
00:21:58,440 --> 00:22:01,080
observe Transformations this is an idea

589
00:22:01,080 --> 00:22:03,299
of temporal coherence

590
00:22:03,299 --> 00:22:05,280
so what would happen if we just simply

591
00:22:05,280 --> 00:22:08,280
extended our previous framework over the

592
00:22:08,280 --> 00:22:10,620
time Dimension right so instead of just

593
00:22:10,620 --> 00:22:13,080
grouping saying we want our neurons to

594
00:22:13,080 --> 00:22:15,059
be group sparse in terms of spatial

595
00:22:15,059 --> 00:22:17,400
extent on the cortex we actually want

596
00:22:17,400 --> 00:22:18,960
them to be group sparse over time

597
00:22:18,960 --> 00:22:20,640
meaning that if one set of neurons is

598
00:22:20,640 --> 00:22:22,559
active now we want that same set of

599
00:22:22,559 --> 00:22:24,360
neurons to be active into the future as

600
00:22:24,360 --> 00:22:25,440
well

601
00:22:25,440 --> 00:22:27,840
if we look at if we kind of intuitively

602
00:22:27,840 --> 00:22:30,600
think about this we see that this is

603
00:22:30,600 --> 00:22:33,059
actually more encouraging invariance and

604
00:22:33,059 --> 00:22:35,039
equivariance a way to understand this is

605
00:22:35,039 --> 00:22:37,140
we're saying we want the same neurons to

606
00:22:37,140 --> 00:22:39,179
be active constantly but the input

607
00:22:39,179 --> 00:22:41,280
transformation is changing right the the

608
00:22:41,280 --> 00:22:44,220
feet of this little fox are moving so if

609
00:22:44,220 --> 00:22:45,960
the same neurons are coding for the same

610
00:22:45,960 --> 00:22:47,880
thing over and over again but the feet

611
00:22:47,880 --> 00:22:49,320
are moving those neurons are going to

612
00:22:49,320 --> 00:22:51,360
learn to be invariant to the motion of

613
00:22:51,360 --> 00:22:53,880
that leg of this dog for example

614
00:22:53,880 --> 00:22:57,539
so instead is that oops

615
00:22:57,539 --> 00:23:01,200
I went the wrong way here uh

616
00:23:01,200 --> 00:23:04,860
so instead uh our Insight was that this

617
00:23:04,860 --> 00:23:06,659
group starts to be could instead be

618
00:23:06,659 --> 00:23:09,059
shifted with respect to time so this

619
00:23:09,059 --> 00:23:10,980
would mean that sequentially shifted

620
00:23:10,980 --> 00:23:13,080
sets of activations would be encouraged

621
00:23:13,080 --> 00:23:15,179
to activate together and then our latent

622
00:23:15,179 --> 00:23:16,440
space would really be structured with

623
00:23:16,440 --> 00:23:18,000
respect to the observed Transformations

624
00:23:18,000 --> 00:23:19,980
so you can see here that rather than the

625
00:23:19,980 --> 00:23:21,480
same set of neurons being active at all

626
00:23:21,480 --> 00:23:23,340
time steps it's really a sequentially

627
00:23:23,340 --> 00:23:24,900
permuted set of neurons that we're

628
00:23:24,900 --> 00:23:27,780
grouping together in this sparse way uh

629
00:23:27,780 --> 00:23:29,940
and and then this allows us to model

630
00:23:29,940 --> 00:23:33,419
different observations over time but

631
00:23:33,419 --> 00:23:34,860
they're still connected in terms of

632
00:23:34,860 --> 00:23:36,960
learning a transformation and preserving

633
00:23:36,960 --> 00:23:38,340
this correlation structure of the

634
00:23:38,340 --> 00:23:40,020
empathy

635
00:23:40,020 --> 00:23:41,940
so if we put this together into our

636
00:23:41,940 --> 00:23:44,400
topographic Bae architecture you can get

637
00:23:44,400 --> 00:23:46,020
something that looks like this you see

638
00:23:46,020 --> 00:23:48,120
that we have an input sequence we're

639
00:23:48,120 --> 00:23:51,240
again encoding a z variable and then

640
00:23:51,240 --> 00:23:53,520
multiple U variables in the denominator

641
00:23:53,520 --> 00:23:55,740
here and then each one of these U

642
00:23:55,740 --> 00:23:58,620
variables is shifted uh kind of like we

643
00:23:58,620 --> 00:24:00,480
were showing before in order to achieve

644
00:24:00,480 --> 00:24:02,820
this this shift equivariance structure

645
00:24:02,820 --> 00:24:04,740
that we're looking for when we combine

646
00:24:04,740 --> 00:24:07,080
these in this student T product

647
00:24:07,080 --> 00:24:09,240
distribution we get a single latent

648
00:24:09,240 --> 00:24:10,740
variable this is now our topographic

649
00:24:10,740 --> 00:24:13,860
variable T and now that we have this

650
00:24:13,860 --> 00:24:16,140
known structure in our latent space you

651
00:24:16,140 --> 00:24:17,460
can think of it like a structured World

652
00:24:17,460 --> 00:24:19,919
model we know how to transform this

653
00:24:19,919 --> 00:24:21,659
latent space in this case it's by

654
00:24:21,659 --> 00:24:23,580
permuting these activations around these

655
00:24:23,580 --> 00:24:25,860
circles doing like a cyclic role a

656
00:24:25,860 --> 00:24:28,380
cyclic shift we know that this is going

657
00:24:28,380 --> 00:24:30,120
to correspond to our learned input

658
00:24:30,120 --> 00:24:32,640
Transformations and we can verify that

659
00:24:32,640 --> 00:24:34,620
by saying okay what if I continue this

660
00:24:34,620 --> 00:24:36,480
input transformation the true

661
00:24:36,480 --> 00:24:38,100
transformation in the data set which is

662
00:24:38,100 --> 00:24:40,559
a rotation and then I compare that with

663
00:24:40,559 --> 00:24:42,659
how I've done my role in my late Space

664
00:24:42,659 --> 00:24:44,700
by moving my activations around in my

665
00:24:44,700 --> 00:24:47,280
brain and then we decode and we see that

666
00:24:47,280 --> 00:24:49,919
we get the exact same thing and so this

667
00:24:49,919 --> 00:24:52,140
is demonstrating this commutility

668
00:24:52,140 --> 00:24:53,580
property that I was talking about before

669
00:24:53,580 --> 00:24:56,820
for verifying homomorphism

670
00:24:56,820 --> 00:24:58,799
and so to measure this a little bit more

671
00:24:58,799 --> 00:25:02,460
quality quantitatively uh we can measure

672
00:25:02,460 --> 00:25:04,440
what's called an equivariance loss so

673
00:25:04,440 --> 00:25:07,080
this is really the quantification of

674
00:25:07,080 --> 00:25:09,360
this difference between our rolled

675
00:25:09,360 --> 00:25:12,120
capsule activation or rolling in our

676
00:25:12,120 --> 00:25:15,059
head versus watching the rolling unfold

677
00:25:15,059 --> 00:25:16,559
and forward they're watching the

678
00:25:16,559 --> 00:25:19,440
transformation unfold before us so we

679
00:25:19,440 --> 00:25:21,600
see the topographic Bae achieves

680
00:25:21,600 --> 00:25:24,000
significantly lower uh equivariance

681
00:25:24,000 --> 00:25:26,700
error this bubble vae is what I was

682
00:25:26,700 --> 00:25:27,960
talking about before where it's learning

683
00:25:27,960 --> 00:25:29,820
invariance so it doesn't have the shift

684
00:25:29,820 --> 00:25:32,340
operation and the traditional vae kind

685
00:25:32,340 --> 00:25:35,640
of has no notion of organization or

686
00:25:35,640 --> 00:25:37,380
temporal component so performance very

687
00:25:37,380 --> 00:25:40,320
poorly in addition to this we see that

688
00:25:40,320 --> 00:25:41,700
the model is a better generative model

689
00:25:41,700 --> 00:25:45,059
of sequences it just gets a lower uh

690
00:25:45,059 --> 00:25:48,179
lower like negative log likelihood uh on

691
00:25:48,179 --> 00:25:50,100
the data set so it's it's better able to

692
00:25:50,100 --> 00:25:51,720
model this data set because it has a

693
00:25:51,720 --> 00:25:52,919
notion of the structure of the

694
00:25:52,919 --> 00:25:55,460
transformations

695
00:25:55,980 --> 00:25:58,140
uh we can test this on multiple

696
00:25:58,140 --> 00:25:59,760
different transformation types and the

697
00:25:59,760 --> 00:26:00,840
top row we're showing the true

698
00:26:00,840 --> 00:26:02,880
transformation we pulled out these

699
00:26:02,880 --> 00:26:05,039
grayed out images and then on the bottom

700
00:26:05,039 --> 00:26:07,080
row we encode and then we just kind of

701
00:26:07,080 --> 00:26:08,700
roll our activations around and we keep

702
00:26:08,700 --> 00:26:12,140
decoding to see what the what the model

703
00:26:12,140 --> 00:26:15,000
uh has learned as the current

704
00:26:15,000 --> 00:26:17,039
transformation that's being observed and

705
00:26:17,039 --> 00:26:19,340
we see that it can basically perfectly

706
00:26:19,340 --> 00:26:21,360
reconstruct these elements of the

707
00:26:21,360 --> 00:26:23,640
sequence that it's never seen before

708
00:26:23,640 --> 00:26:25,260
additionally with images that are from

709
00:26:25,260 --> 00:26:26,580
the test set that it's never seen before

710
00:26:26,580 --> 00:26:28,380
simply because it knows what the

711
00:26:28,380 --> 00:26:29,760
transformation is that it's currently

712
00:26:29,760 --> 00:26:31,500
encoding it can generalize that to new

713
00:26:31,500 --> 00:26:33,919
examples

714
00:26:34,020 --> 00:26:36,360
so the takeaway from this part is really

715
00:26:36,360 --> 00:26:38,039
topographic organization we showed that

716
00:26:38,039 --> 00:26:40,080
a preserved input structure and now

717
00:26:40,080 --> 00:26:41,940
we're showing it can potentially improve

718
00:26:41,940 --> 00:26:44,279
efficiency and generalization as we

719
00:26:44,279 --> 00:26:46,200
would hope uh

720
00:26:46,200 --> 00:26:48,600
so finally something that surprised us

721
00:26:48,600 --> 00:26:49,980
and I thought was potentially the most

722
00:26:49,980 --> 00:26:52,500
interesting is that these these

723
00:26:52,500 --> 00:26:53,700
Transformations that are learned by our

724
00:26:53,700 --> 00:26:54,960
model actually generalize the

725
00:26:54,960 --> 00:26:57,059
combinations of Transformations that

726
00:26:57,059 --> 00:26:59,580
we're not seeing during training so for

727
00:26:59,580 --> 00:27:02,100
example despite only training on color

728
00:27:02,100 --> 00:27:04,200
and rotation Transformations and

729
00:27:04,200 --> 00:27:06,419
isolation if the model is presented with

730
00:27:06,419 --> 00:27:08,340
a combined color rotation transformation

731
00:27:08,340 --> 00:27:11,100
at test time uh we we see that it's able

732
00:27:11,100 --> 00:27:13,140
to completely model and complete these

733
00:27:13,140 --> 00:27:14,700
Transformations perfectly through the

734
00:27:14,700 --> 00:27:17,159
capsule role implying that it's learned

735
00:27:17,159 --> 00:27:19,620
to factorize represent to these

736
00:27:19,620 --> 00:27:20,880
different Transformations and it can

737
00:27:20,880 --> 00:27:24,600
flexibly combine them at inference time

738
00:27:24,600 --> 00:27:28,140
so again maybe we also don't just get

739
00:27:28,140 --> 00:27:29,820
officially efficiency in generalization

740
00:27:29,820 --> 00:27:34,100
we also get some basic compositionality

741
00:27:34,260 --> 00:27:36,059
so let's talk about the limitations and

742
00:27:36,059 --> 00:27:38,460
what we could do next uh the main

743
00:27:38,460 --> 00:27:40,620
limitation is that there's a predefined

744
00:27:40,620 --> 00:27:44,159
transformation that we're imposing uh in

745
00:27:44,159 --> 00:27:46,500
both space and time so although we freed

746
00:27:46,500 --> 00:27:49,080
ourselves from group Transformations and

747
00:27:49,080 --> 00:27:52,440
specifically like uh translation or

748
00:27:52,440 --> 00:27:53,940
rotation as it's currently done in the

749
00:27:53,940 --> 00:27:55,559
machine learning World

750
00:27:55,559 --> 00:27:59,240
um we still have uh this hard-coded

751
00:27:59,240 --> 00:28:01,980
latent role in our heads for everything

752
00:28:01,980 --> 00:28:03,900
we see and to make this a little bit

753
00:28:03,900 --> 00:28:05,700
more flexible so hopefully we can model

754
00:28:05,700 --> 00:28:08,880
a greater diversity of Transformations

755
00:28:08,880 --> 00:28:10,980
um we we think maybe we can take

756
00:28:10,980 --> 00:28:13,860
inspiration from more structured spatial

757
00:28:13,860 --> 00:28:15,600
temporal dynamics that are observed in

758
00:28:15,600 --> 00:28:18,120
the brain and so that takes us to the

759
00:28:18,120 --> 00:28:20,400
the second part of this talk which is uh

760
00:28:20,400 --> 00:28:22,140
spatial temporal dynamics that we're

761
00:28:22,140 --> 00:28:23,039
going to try to integrate into

762
00:28:23,039 --> 00:28:25,200
artificial neural networks one example

763
00:28:25,200 --> 00:28:27,059
of that is traveling waves like I showed

764
00:28:27,059 --> 00:28:28,020
here

765
00:28:28,020 --> 00:28:30,600
so what do we mean by that uh well

766
00:28:30,600 --> 00:28:32,279
here's a very recent paper where they

767
00:28:32,279 --> 00:28:36,059
used a nine Tesla fmri operating at 36

768
00:28:36,059 --> 00:28:38,700
millisecond resolution to image a single

769
00:28:38,700 --> 00:28:40,980
slice of a rat brain under anesthesia

770
00:28:40,980 --> 00:28:43,320
and what we see is this very clearly

771
00:28:43,320 --> 00:28:45,720
structured spatial temporal activity and

772
00:28:45,720 --> 00:28:48,299
correlations and these authors of the

773
00:28:48,299 --> 00:28:50,520
paper go on to analyze this activity in

774
00:28:50,520 --> 00:28:52,919
terms of the principal modes as depicted

775
00:28:52,919 --> 00:28:55,799
on the right so our hypothesis is that

776
00:28:55,799 --> 00:28:57,179
perhaps some sort of a correlation

777
00:28:57,179 --> 00:28:59,039
structure like this may be beneficial

778
00:28:59,039 --> 00:29:01,260
for structuring the representations of

779
00:29:01,260 --> 00:29:03,240
our model with respect to observed

780
00:29:03,240 --> 00:29:05,100
Transformations but in a much more

781
00:29:05,100 --> 00:29:07,440
flexible way than simply just a cyclic

782
00:29:07,440 --> 00:29:10,700
shift like we were doing before

783
00:29:11,279 --> 00:29:12,419
um

784
00:29:12,419 --> 00:29:15,900
and let me say that this is not just

785
00:29:15,900 --> 00:29:19,320
observed in an ssi's rats uh this you

786
00:29:19,320 --> 00:29:20,940
can you can see these traveling waves

787
00:29:20,940 --> 00:29:24,179
happen in the Mt cortex of awake

788
00:29:24,179 --> 00:29:27,600
behaving primates uh so for example on

789
00:29:27,600 --> 00:29:29,580
the left here they show traveling waves

790
00:29:29,580 --> 00:29:31,740
that actually uh

791
00:29:31,740 --> 00:29:35,520
change How likely a primate is to see a

792
00:29:35,520 --> 00:29:38,279
low contrast stimuli based on the phase

793
00:29:38,279 --> 00:29:40,980
of the wave furthermore they show that

794
00:29:40,980 --> 00:29:43,500
the like a high contrast stimulus on the

795
00:29:43,500 --> 00:29:45,779
right uh can induce a traveling wave

796
00:29:45,779 --> 00:29:47,520
activity that propagates outwards even

797
00:29:47,520 --> 00:29:50,039
in primary visual cortex so these are

798
00:29:50,039 --> 00:29:52,140
really ubiquitous throughout the brain

799
00:29:52,140 --> 00:29:54,000
at multiple levels and it would be

800
00:29:54,000 --> 00:29:55,440
interesting to study what their

801
00:29:55,440 --> 00:29:58,140
implications are for uh structure

802
00:29:58,140 --> 00:29:59,700
representation learning in our case or

803
00:29:59,700 --> 00:30:01,799
or generally

804
00:30:01,799 --> 00:30:04,140
there is prior work which has studied

805
00:30:04,140 --> 00:30:06,720
these types of uh Dynamics and they

806
00:30:06,720 --> 00:30:08,700
build models so on the top these are the

807
00:30:08,700 --> 00:30:10,380
equations which describe a spiking

808
00:30:10,380 --> 00:30:12,600
neural network which they show if you

809
00:30:12,600 --> 00:30:15,720
implement time delays actually axonal

810
00:30:15,720 --> 00:30:18,240
time delays between neurons you do get

811
00:30:18,240 --> 00:30:20,820
these structure dynamics of traveling

812
00:30:20,820 --> 00:30:22,440
waves as long as your network size is

813
00:30:22,440 --> 00:30:24,059
large enough

814
00:30:24,059 --> 00:30:26,520
um however as many people probably know

815
00:30:26,520 --> 00:30:28,620
it's relatively challenging to train

816
00:30:28,620 --> 00:30:31,320
spiking neural networks of the same size

817
00:30:31,320 --> 00:30:34,820
and performance as deep neural networks

818
00:30:34,820 --> 00:30:37,679
similarly on the bottom another system

819
00:30:37,679 --> 00:30:39,539
which is significantly simpler but

820
00:30:39,539 --> 00:30:42,840
perhaps too simple uh is a network of

821
00:30:42,840 --> 00:30:45,120
coupled oscillators these are known to

822
00:30:45,120 --> 00:30:48,779
exhibit synchrony and spatial temporal

823
00:30:48,779 --> 00:30:52,200
Dynamics and complex patterns but uh

824
00:30:52,200 --> 00:30:53,520
this is called like a phase reduce

825
00:30:53,520 --> 00:30:55,500
system and doesn't quite capture the

826
00:30:55,500 --> 00:30:57,059
full complexity that we're interested in

827
00:30:57,059 --> 00:30:58,140
so we're looking at something that's

828
00:30:58,140 --> 00:31:00,779
potentially in between these two

829
00:31:00,779 --> 00:31:03,600
and what we settled on is this work in

830
00:31:03,600 --> 00:31:06,600
this work is uh to parameterize a

831
00:31:06,600 --> 00:31:08,520
network of couple of oscillators

832
00:31:08,520 --> 00:31:10,620
slightly more flexibly than a paramoto

833
00:31:10,620 --> 00:31:12,360
model so this is really

834
00:31:12,360 --> 00:31:14,580
built on this couple distillatory

835
00:31:14,580 --> 00:31:16,380
recurrent neural network of Constantine

836
00:31:16,380 --> 00:31:18,720
rush and Nisha

837
00:31:18,720 --> 00:31:20,760
um where they basically took the

838
00:31:20,760 --> 00:31:22,200
equation which describes a simple

839
00:31:22,200 --> 00:31:23,820
harmonic oscillator it's a second order

840
00:31:23,820 --> 00:31:26,159
differential equation the acceleration

841
00:31:26,159 --> 00:31:29,940
on a ball on a spring is proportional to

842
00:31:29,940 --> 00:31:32,480
its displacement

843
00:31:32,480 --> 00:31:35,220
uh you can add additional terms such as

844
00:31:35,220 --> 00:31:37,260
damping so that the oscillations slowly

845
00:31:37,260 --> 00:31:39,360
die out over time

846
00:31:39,360 --> 00:31:41,580
you can drive this oscillator with an

847
00:31:41,580 --> 00:31:43,380
external input to kind of counteract

848
00:31:43,380 --> 00:31:45,179
this damping or to give slightly more

849
00:31:45,179 --> 00:31:47,279
complexity to the Dynamics

850
00:31:47,279 --> 00:31:49,260
and then furthermore if you have many of

851
00:31:49,260 --> 00:31:50,940
these oscillators you can couple them

852
00:31:50,940 --> 00:31:53,000
together with these coupling matrices W

853
00:31:53,000 --> 00:31:55,320
uh as we demonstrate kind of in this

854
00:31:55,320 --> 00:31:56,640
picture here so you can really think of

855
00:31:56,640 --> 00:31:58,140
this network as a bunch of bubbles on

856
00:31:58,140 --> 00:31:59,940
Springs and they're maybe connected to

857
00:31:59,940 --> 00:32:01,740
each other also by Springs or elastic

858
00:32:01,740 --> 00:32:03,600
bands whatever the couple distillatory

859
00:32:03,600 --> 00:32:05,279
recurrent neural network of Russian

860
00:32:05,279 --> 00:32:08,100
Mishra uh with these various terms and

861
00:32:08,100 --> 00:32:09,899
this has been shown to be very powerful

862
00:32:09,899 --> 00:32:12,480
for modeling long sequences they also

863
00:32:12,480 --> 00:32:13,740
mentioned they were inspired by the

864
00:32:13,740 --> 00:32:15,360
brain building this and there's a lot of

865
00:32:15,360 --> 00:32:17,700
good analysis in that paper uh for

866
00:32:17,700 --> 00:32:19,020
example they show that this is really

867
00:32:19,020 --> 00:32:21,440
beneficial properties with respect to

868
00:32:21,440 --> 00:32:23,460
Vanishing gradient problems that

869
00:32:23,460 --> 00:32:25,440
typically happen in recurrent neural

870
00:32:25,440 --> 00:32:26,820
networks

871
00:32:26,820 --> 00:32:29,159
um but if we want to look at spatial

872
00:32:29,159 --> 00:32:30,960
temporal Dynamics and this type of a

873
00:32:30,960 --> 00:32:32,820
model uh it's slightly challenging

874
00:32:32,820 --> 00:32:34,919
because these coupling matrices here the

875
00:32:34,919 --> 00:32:36,320
W's

876
00:32:36,320 --> 00:32:39,600
that connect each neural or each

877
00:32:39,600 --> 00:32:41,240
oscillator is positioned to one another

878
00:32:41,240 --> 00:32:43,620
these are densely connected matrices

879
00:32:43,620 --> 00:32:45,120
like I've tried to depict on the left

880
00:32:45,120 --> 00:32:46,020
here

881
00:32:46,020 --> 00:32:48,299
so if you try to visualize the Dynamics

882
00:32:48,299 --> 00:32:50,580
of this network you don't see any

883
00:32:50,580 --> 00:32:51,899
spatial organization there's no

884
00:32:51,899 --> 00:32:55,380
inheritance it's apology to the latent

885
00:32:55,380 --> 00:32:57,000
space of this model

886
00:32:57,000 --> 00:32:58,799
um so you can think of this like in our

887
00:32:58,799 --> 00:33:01,020
previous example a neuron is connected

888
00:33:01,020 --> 00:33:03,240
to a potentially arbitrary set of other

889
00:33:03,240 --> 00:33:04,919
neurons those neurons are connected to

890
00:33:04,919 --> 00:33:06,600
another arbitrary set of neurons and

891
00:33:06,600 --> 00:33:08,520
you'll just get oscillatory Dynamics

892
00:33:08,520 --> 00:33:10,860
certainly but kind of fluctuations that

893
00:33:10,860 --> 00:33:13,260
don't make a lot of structured sense so

894
00:33:13,260 --> 00:33:15,360
in our work then we thought okay how can

895
00:33:15,360 --> 00:33:18,299
we convert this more to the the types of

896
00:33:18,299 --> 00:33:19,860
dynamics that we're interested in this

897
00:33:19,860 --> 00:33:22,140
structured propagation of activity uh

898
00:33:22,140 --> 00:33:23,940
and one clear way to do that is to have

899
00:33:23,940 --> 00:33:26,539
a more structured connectivity Matrix W

900
00:33:26,539 --> 00:33:29,880
uh which we found is easily implemented

901
00:33:29,880 --> 00:33:31,559
and efficiently implemented through a

902
00:33:31,559 --> 00:33:33,000
convolution operation which you can

903
00:33:33,000 --> 00:33:34,620
think of like a local a locally

904
00:33:34,620 --> 00:33:36,299
connected layer so instead of having

905
00:33:36,299 --> 00:33:37,860
every neuron connected every neuron

906
00:33:37,860 --> 00:33:39,480
neurons are just connected to their

907
00:33:39,480 --> 00:33:41,580
nearby neighbors you after training

908
00:33:41,580 --> 00:33:42,840
you'll end up getting something that

909
00:33:42,840 --> 00:33:44,880
looks like a smooth spatial temporal

910
00:33:44,880 --> 00:33:46,620
Dynamics

911
00:33:46,620 --> 00:33:48,419
so to be a little bit more clear to

912
00:33:48,419 --> 00:33:50,519
train this model we take this separate

913
00:33:50,519 --> 00:33:52,200
second order differential equation that

914
00:33:52,200 --> 00:33:54,299
we were describing before you discretize

915
00:33:54,299 --> 00:33:56,340
it into two first order equations you

916
00:33:56,340 --> 00:33:57,960
can think of this as like numerically

917
00:33:57,960 --> 00:34:01,200
integrating the ode we now have a

918
00:34:01,200 --> 00:34:03,120
velocity and then we update

919
00:34:03,120 --> 00:34:06,000
uh and and we can train this model as

920
00:34:06,000 --> 00:34:07,620
something like an auto encoder or an

921
00:34:07,620 --> 00:34:09,839
auto regressive model so if we take an

922
00:34:09,839 --> 00:34:11,460
input we encode it to our latent space

923
00:34:11,460 --> 00:34:14,339
really the input is Dr is this f of x

924
00:34:14,339 --> 00:34:16,500
term which acts as the driving term so

925
00:34:16,500 --> 00:34:18,599
it's like driving these oscillators from

926
00:34:18,599 --> 00:34:20,879
the bottom uh and then they have their

927
00:34:20,879 --> 00:34:23,099
own Dynamics which are defined by the

928
00:34:23,099 --> 00:34:25,800
coupling terms these local couplings and

929
00:34:25,800 --> 00:34:27,540
then at each time step we take this

930
00:34:27,540 --> 00:34:29,460
latent State this wave state and we

931
00:34:29,460 --> 00:34:31,560
decode to try and reconstruct the input

932
00:34:31,560 --> 00:34:33,540
and be at the current time step or a

933
00:34:33,540 --> 00:34:35,699
future time step

934
00:34:35,699 --> 00:34:37,980
we can do some analysis of of these

935
00:34:37,980 --> 00:34:42,300
models uh during training to see what

936
00:34:42,300 --> 00:34:43,619
happens before training and after

937
00:34:43,619 --> 00:34:45,780
training we can compute the phase and

938
00:34:45,780 --> 00:34:47,399
the velocity of the Dynamics in the

939
00:34:47,399 --> 00:34:49,379
latent space basically we see at the

940
00:34:49,379 --> 00:34:51,480
beginning of trading there's no waves in

941
00:34:51,480 --> 00:34:53,699
our model but after training after 50

942
00:34:53,699 --> 00:34:55,500
epochs we see that there's a smooth

943
00:34:55,500 --> 00:34:57,119
structured activity propagating

944
00:34:57,119 --> 00:35:00,420
downwards uh in service of this sequence

945
00:35:00,420 --> 00:35:01,800
modeling task that we're doing like

946
00:35:01,800 --> 00:35:04,380
rotating objects

947
00:35:04,380 --> 00:35:05,940
um so what's what's the benefit of this

948
00:35:05,940 --> 00:35:07,680
I mean the whole reason I motivated this

949
00:35:07,680 --> 00:35:10,380
was to say we wanted to have more

950
00:35:10,380 --> 00:35:11,880
flexibly learned structure are we

951
00:35:11,880 --> 00:35:13,020
actually doing that or are we just

952
00:35:13,020 --> 00:35:15,060
getting pretty waves

953
00:35:15,060 --> 00:35:16,859
um so what we showed in our paper is

954
00:35:16,859 --> 00:35:19,320
that we really are learning some sort of

955
00:35:19,320 --> 00:35:20,940
useful structure and the way we showed

956
00:35:20,940 --> 00:35:22,440
that is again with something like this

957
00:35:22,440 --> 00:35:24,960
commutative diagram if you take an input

958
00:35:24,960 --> 00:35:27,000
and you encode it and you get a wave

959
00:35:27,000 --> 00:35:29,280
state and then you propagate waves

960
00:35:29,280 --> 00:35:31,619
artificially in that wave state and then

961
00:35:31,619 --> 00:35:33,480
decode you can observe that it's

962
00:35:33,480 --> 00:35:35,220
actually exactly the same as if you had

963
00:35:35,220 --> 00:35:37,140
just by showing a bunch of different

964
00:35:37,140 --> 00:35:39,180
images of different Transformations so a

965
00:35:39,180 --> 00:35:41,640
lot of different uh digits different

966
00:35:41,640 --> 00:35:43,920
features and we see that we get

967
00:35:43,920 --> 00:35:46,140
different types of wave activity in each

968
00:35:46,140 --> 00:35:47,880
case in order to model that different

969
00:35:47,880 --> 00:35:49,140
transformation

970
00:35:49,140 --> 00:35:51,119
if we train it on different data sets as

971
00:35:51,119 --> 00:35:53,400
well we similarly see more complex

972
00:35:53,400 --> 00:35:55,200
Dynamics in this case maybe not even

973
00:35:55,200 --> 00:35:57,839
traveling waves or standing waves which

974
00:35:57,839 --> 00:36:00,359
can be thought of as traveling waves in

975
00:36:00,359 --> 00:36:02,339
opposite directions so we see if we're

976
00:36:02,339 --> 00:36:04,079
modeling these orbital Dynamics we get

977
00:36:04,079 --> 00:36:06,000
these kind of smoothly moving Blobs of

978
00:36:06,000 --> 00:36:07,619
activity in our latent space if we're

979
00:36:07,619 --> 00:36:09,839
modeling a pendulum we similarly get

980
00:36:09,839 --> 00:36:13,820
kind of complex oscillatory activity

981
00:36:14,099 --> 00:36:17,339
so it's preserved input structure but

982
00:36:17,339 --> 00:36:19,560
additionally more flexibility than we

983
00:36:19,560 --> 00:36:21,599
had before which is kind of our ultimate

984
00:36:21,599 --> 00:36:23,400
goal

985
00:36:23,400 --> 00:36:26,099
so finally I want to talk a bit about

986
00:36:26,099 --> 00:36:28,320
how I think the outcome of This research

987
00:36:28,320 --> 00:36:30,420
may not only improve artificial

988
00:36:30,420 --> 00:36:32,099
intelligence but also how it helps us

989
00:36:32,099 --> 00:36:34,440
understand why our measurements of the

990
00:36:34,440 --> 00:36:36,240
brain look the way they do so to give a

991
00:36:36,240 --> 00:36:38,579
brief example of what I mean by this uh

992
00:36:38,579 --> 00:36:41,040
I talked a bit about before about visas

993
00:36:41,040 --> 00:36:43,740
and places so in this fantastic work

994
00:36:43,740 --> 00:36:46,859
with Ching higao we studied if our

995
00:36:46,859 --> 00:36:48,900
simple topographic prior as we discussed

996
00:36:48,900 --> 00:36:50,579
may be able to reproduce these same

997
00:36:50,579 --> 00:36:53,339
effects so specifically we put the value

998
00:36:53,339 --> 00:36:56,099
of this Cohen's D selectivity metric for

999
00:36:56,099 --> 00:36:58,200
each of our neurons with respect to a

1000
00:36:58,200 --> 00:37:00,000
different data set of images potentially

1001
00:37:00,000 --> 00:37:02,460
containing just faces or just objects or

1002
00:37:02,460 --> 00:37:03,359
bodies

1003
00:37:03,359 --> 00:37:05,880
and so we measure for every neuron is it

1004
00:37:05,880 --> 00:37:07,920
more likely to respond to faces or the

1005
00:37:07,920 --> 00:37:10,380
Russian emerges in the brain but I do

1006
00:37:10,380 --> 00:37:12,839
think that it tells us that the relative

1007
00:37:12,839 --> 00:37:15,300
organization of selectivity May at least

1008
00:37:15,300 --> 00:37:17,400
be partially attributable to correlation

1009
00:37:17,400 --> 00:37:19,800
statistics in the data have to repath

1010
00:37:19,800 --> 00:37:21,359
after being passed through a highly

1011
00:37:21,359 --> 00:37:23,640
non-linear future extractor such as a

1012
00:37:23,640 --> 00:37:25,440
deep neural network

1013
00:37:25,440 --> 00:37:27,480
so in a similar vein something that's

1014
00:37:27,480 --> 00:37:29,040
interesting there's a known what's

1015
00:37:29,040 --> 00:37:30,900
called tripartite or not the visual

1016
00:37:30,900 --> 00:37:36,720
stream so uh images of uh or objects are

1017
00:37:36,720 --> 00:37:38,400
the selectivity with respect to objects

1018
00:37:38,400 --> 00:37:40,680
is organized by more abstract properties

1019
00:37:40,680 --> 00:37:43,200
such as animacy is this thing alive or

1020
00:37:43,200 --> 00:37:46,200
inanimate uh versus also real world

1021
00:37:46,200 --> 00:37:48,480
object size like what is the size of a

1022
00:37:48,480 --> 00:37:50,700
teapot versus a car

1023
00:37:50,700 --> 00:37:53,520
um and what we see is that in the in

1024
00:37:53,520 --> 00:37:56,160
humans this the selectivity is organized

1025
00:37:56,160 --> 00:37:57,540
in this tripartite structure you

1026
00:37:57,540 --> 00:37:59,760
typically have small objects that are in

1027
00:37:59,760 --> 00:38:01,920
between animates and inanimate objects

1028
00:38:01,920 --> 00:38:04,200
in terms of their selectivity and we see

1029
00:38:04,200 --> 00:38:06,060
the same thing kind of happens here so

1030
00:38:06,060 --> 00:38:07,440
these are measuring the selectivity of

1031
00:38:07,440 --> 00:38:08,880
the same set of neurons but with respect

1032
00:38:08,880 --> 00:38:10,859
to these differences of stimuli you see

1033
00:38:10,859 --> 00:38:12,440
that the small cluster is in between

1034
00:38:12,440 --> 00:38:14,820
animate and inanimate cluster and again

1035
00:38:14,820 --> 00:38:16,079
this happens for multiple different

1036
00:38:16,079 --> 00:38:18,900
initializations so this is something I

1037
00:38:18,900 --> 00:38:20,880
hope we can explore a bit further for

1038
00:38:20,880 --> 00:38:22,980
this community I think it's interesting

1039
00:38:22,980 --> 00:38:24,119
because it's

1040
00:38:24,119 --> 00:38:26,220
it's really a way of showing that we

1041
00:38:26,220 --> 00:38:28,200
built a structured World model and

1042
00:38:28,200 --> 00:38:30,119
potentially this world model is

1043
00:38:30,119 --> 00:38:31,980
beneficial for

1044
00:38:31,980 --> 00:38:34,740
better representing real world data in a

1045
00:38:34,740 --> 00:38:37,619
structured way and you get lower free

1046
00:38:37,619 --> 00:38:39,119
energy in that sense

1047
00:38:39,119 --> 00:38:40,619
so

1048
00:38:40,619 --> 00:38:42,300
um I think by developing these models

1049
00:38:42,300 --> 00:38:44,400
like like we showed here we may get

1050
00:38:44,400 --> 00:38:46,500
insights into new mechanisms for how

1051
00:38:46,500 --> 00:38:48,900
this structure emerges uh including

1052
00:38:48,900 --> 00:38:50,460
topographic organization that we never

1053
00:38:50,460 --> 00:38:52,920
thought of before so Machine model I was

1054
00:38:52,920 --> 00:38:55,520
looking at the orientation selectivity

1055
00:38:55,520 --> 00:38:58,260
uh of neurons which I wasn't

1056
00:38:58,260 --> 00:39:01,020
particularly expecting something to

1057
00:39:01,020 --> 00:39:03,420
happen but uh you're looking at kind of

1058
00:39:03,420 --> 00:39:05,339
these waves propagate over this

1059
00:39:05,339 --> 00:39:08,099
simulated vertical surface and I thought

1060
00:39:08,099 --> 00:39:09,960
okay maybe I'm showing rotated images

1061
00:39:09,960 --> 00:39:11,820
maybe this has some effect on the

1062
00:39:11,820 --> 00:39:13,740
orientation selectivity

1063
00:39:13,740 --> 00:39:15,599
and actually if you go in and you

1064
00:39:15,599 --> 00:39:17,460
measure the selectivity of each neuron

1065
00:39:17,460 --> 00:39:18,660
with respect to these differently

1066
00:39:18,660 --> 00:39:22,079
oriented lines what you see is that it's

1067
00:39:22,079 --> 00:39:24,300
surprisingly reminiscent of the Orient

1068
00:39:24,300 --> 00:39:25,859
type of columns that are seen in primary

1069
00:39:25,859 --> 00:39:27,599
visual cortex this is stuff going back

1070
00:39:27,599 --> 00:39:29,520
to Hugo and weasel and this is something

1071
00:39:29,520 --> 00:39:30,900
that just kind of came out of this model

1072
00:39:30,900 --> 00:39:33,060
and the fact that it has the spatial

1073
00:39:33,060 --> 00:39:34,440
temporal structure with respect to

1074
00:39:34,440 --> 00:39:37,619
Transformations so uh of course this is

1075
00:39:37,619 --> 00:39:39,599
a really coarse analogy but I think this

1076
00:39:39,599 --> 00:39:40,740
is an example of how building these

1077
00:39:40,740 --> 00:39:42,839
types of models can help us think about

1078
00:39:42,839 --> 00:39:45,240
how the brain builds representational

1079
00:39:45,240 --> 00:39:46,980
structure and the white the way it's

1080
00:39:46,980 --> 00:39:48,660
organized in a way that maybe we haven't

1081
00:39:48,660 --> 00:39:51,300
thought about before

1082
00:39:51,300 --> 00:39:53,460
um I I think I'm not the only one who's

1083
00:39:53,460 --> 00:39:55,859
doing this type of work and and so I

1084
00:39:55,859 --> 00:39:57,240
want to talk a little bit about some

1085
00:39:57,240 --> 00:39:59,579
other people who are doing this uh so

1086
00:39:59,579 --> 00:40:00,780
I've been talking about like this

1087
00:40:00,780 --> 00:40:02,760
equivalent structure

1088
00:40:02,760 --> 00:40:04,920
um people such as James Whittington and

1089
00:40:04,920 --> 00:40:08,880
Tim Barons and surrogengoolie uh have

1090
00:40:08,880 --> 00:40:10,680
shown recently that by introducing

1091
00:40:10,680 --> 00:40:14,940
algebraic constraints into uh into a

1092
00:40:14,940 --> 00:40:17,040
learning process in this case it was

1093
00:40:17,040 --> 00:40:20,820
like the motion of uh and and agents in

1094
00:40:20,820 --> 00:40:23,220
an environment by saying you need to

1095
00:40:23,220 --> 00:40:24,780
preserve kind of this algebraic

1096
00:40:24,780 --> 00:40:27,540
structure if I move in a Circle West

1097
00:40:27,540 --> 00:40:29,280
Northeast South I end up back at the

1098
00:40:29,280 --> 00:40:31,440
same point Again by introducing these

1099
00:40:31,440 --> 00:40:32,820
types of constraints

1100
00:40:32,820 --> 00:40:35,040
you get the emergence of grid cell like

1101
00:40:35,040 --> 00:40:36,900
representations

1102
00:40:36,900 --> 00:40:39,359
um so I'd be interested to see how this

1103
00:40:39,359 --> 00:40:41,880
idea of representational structure can

1104
00:40:41,880 --> 00:40:43,980
help us explain maybe more than our

1105
00:40:43,980 --> 00:40:45,480
scientific findings we're finding as

1106
00:40:45,480 --> 00:40:46,619
well

1107
00:40:46,619 --> 00:40:48,359
um and and how this relates to

1108
00:40:48,359 --> 00:40:51,540
generative models as a whole

1109
00:40:51,540 --> 00:40:52,800
um and then finally I think there's

1110
00:40:52,800 --> 00:40:54,599
something to be said about cognitive

1111
00:40:54,599 --> 00:40:56,099
possibility of these models as well

1112
00:40:56,099 --> 00:40:57,420
maybe we're not just going to be testing

1113
00:40:57,420 --> 00:40:59,579
them from uh neuroscience perspective

1114
00:40:59,579 --> 00:41:01,020
but also for micrognitive science

1115
00:41:01,020 --> 00:41:03,839
perspective for example there's these uh

1116
00:41:03,839 --> 00:41:06,000
Ravens Progressive matrices on the left

1117
00:41:06,000 --> 00:41:08,640
where you have to like say which one of

1118
00:41:08,640 --> 00:41:11,099
these images is more likely to fit in

1119
00:41:11,099 --> 00:41:12,599
this pattern

1120
00:41:12,599 --> 00:41:14,760
um or for example How likely is it that

1121
00:41:14,760 --> 00:41:16,740
this Jenga Tower Falls over when you

1122
00:41:16,740 --> 00:41:19,740
pull over a pull a specific block or or

1123
00:41:19,740 --> 00:41:22,740
with a given structure and I think these

1124
00:41:22,740 --> 00:41:24,599
things are

1125
00:41:24,599 --> 00:41:26,640
these types of tests are really testing

1126
00:41:26,640 --> 00:41:28,619
if our world models that we're building

1127
00:41:28,619 --> 00:41:31,560
are similar to the types of models that

1128
00:41:31,560 --> 00:41:33,660
we innately have our own common sense as

1129
00:41:33,660 --> 00:41:36,060
as humans or as beings living in a

1130
00:41:36,060 --> 00:41:38,400
natural world and I've done some

1131
00:41:38,400 --> 00:41:40,440
preliminary work in this direction I

1132
00:41:40,440 --> 00:41:43,079
think very uh preliminary and not nearly

1133
00:41:43,079 --> 00:41:45,480
this complicated but um kind of trying

1134
00:41:45,480 --> 00:41:47,820
to model visual Illusions so if you take

1135
00:41:47,820 --> 00:41:50,520
a really simple data set of a moving bar

1136
00:41:50,520 --> 00:41:52,980
stimuli or a static bar or frame and you

1137
00:41:52,980 --> 00:41:54,960
move it a little bit you can see that

1138
00:41:54,960 --> 00:41:57,060
the model will actually infer that

1139
00:41:57,060 --> 00:41:58,800
missing frame and then actually also

1140
00:41:58,800 --> 00:42:01,079
infer continued motion so it's like

1141
00:42:01,079 --> 00:42:03,300
overshooting the trajectory of what the

1142
00:42:03,300 --> 00:42:05,820
actual stimuli is providing it before

1143
00:42:05,820 --> 00:42:08,760
correcting again so I think modeling

1144
00:42:08,760 --> 00:42:10,320
Illusions is certainly an interesting

1145
00:42:10,320 --> 00:42:12,660
way to study if our world models are

1146
00:42:12,660 --> 00:42:14,760
similar to the types of models that we

1147
00:42:14,760 --> 00:42:16,619
have ourselves

1148
00:42:16,619 --> 00:42:19,619
so in conclusion uh yeah I think

1149
00:42:19,619 --> 00:42:21,900
topographic priors we could show that

1150
00:42:21,900 --> 00:42:23,220
they effectively learned structured

1151
00:42:23,220 --> 00:42:24,839
representations or structured World

1152
00:42:24,839 --> 00:42:26,700
models this learned structure is

1153
00:42:26,700 --> 00:42:29,160
flexible and adaptable to arbitrary

1154
00:42:29,160 --> 00:42:30,780
Transformations unlike traditional

1155
00:42:30,780 --> 00:42:33,720
equivariants and topographic providers

1156
00:42:33,720 --> 00:42:35,579
can be induced statistically as we did

1157
00:42:35,579 --> 00:42:37,619
in the topographic vae or through

1158
00:42:37,619 --> 00:42:39,480
Dynamics like we were showing in these

1159
00:42:39,480 --> 00:42:42,000
neural wave machine type models

1160
00:42:42,000 --> 00:42:44,460
so to conclude I'll leave you with this

1161
00:42:44,460 --> 00:42:46,980
quote that I found in fukushima's paper

1162
00:42:46,980 --> 00:42:50,280
from 1980 I thought was pretty far ahead

1163
00:42:50,280 --> 00:42:52,079
of its time where he says if we could

1164
00:42:52,079 --> 00:42:53,520
make a neural network model which has

1165
00:42:53,520 --> 00:42:55,020
the same capability for pattern

1166
00:42:55,020 --> 00:42:57,060
recognition as a human being it would

1167
00:42:57,060 --> 00:42:58,800
give us a powerful clue compared to

1168
00:42:58,800 --> 00:43:00,000
understanding the neural mechanism in

1169
00:43:00,000 --> 00:43:03,240
the brain so that's kind of I think some

1170
00:43:03,240 --> 00:43:06,119
of the goals that we're going for here

1171
00:43:06,119 --> 00:43:08,220
so I think it's my advisor Max my

1172
00:43:08,220 --> 00:43:11,280
co-authors Patrick UA Emil jinghian and

1173
00:43:11,280 --> 00:43:17,359
Yorn and interested in discussion thanks

1174
00:43:20,640 --> 00:43:23,420
all right

1175
00:43:24,660 --> 00:43:27,480
all right thank you great very uh

1176
00:43:27,480 --> 00:43:31,079
interesting presentation

1177
00:43:31,079 --> 00:43:33,480
a lot of places to start maybe just uh

1178
00:43:33,480 --> 00:43:36,000
what brought you to this work

1179
00:43:36,000 --> 00:43:38,520
a little context on how you came into

1180
00:43:38,520 --> 00:43:43,819
this work for your PhD Direction

1181
00:43:43,920 --> 00:43:45,119
yeah

1182
00:43:45,119 --> 00:43:46,020
um

1183
00:43:46,020 --> 00:43:49,200
I mean we've been studying not my the

1184
00:43:49,200 --> 00:43:51,000
group that I'm in at the university has

1185
00:43:51,000 --> 00:43:52,700
been studying structured representations

1186
00:43:52,700 --> 00:43:56,640
from uh mathematical point of view for a

1187
00:43:56,640 --> 00:43:58,319
while on where some of the people have

1188
00:43:58,319 --> 00:44:00,240
models uh are for like the variational

1189
00:44:00,240 --> 00:44:01,740
auto encoder

1190
00:44:01,740 --> 00:44:04,680
um and

1191
00:44:04,680 --> 00:44:06,960
I guess what something that had always

1192
00:44:06,960 --> 00:44:08,460
been

1193
00:44:08,460 --> 00:44:11,220
it's a model that respects rotations 2D

1194
00:44:11,220 --> 00:44:13,560
rotations perfectly well but if we want

1195
00:44:13,560 --> 00:44:15,960
to do 3D rotations we can't do that

1196
00:44:15,960 --> 00:44:17,819
because that's not a group in terms of a

1197
00:44:17,819 --> 00:44:19,740
projection onto a 2d plan there's you're

1198
00:44:19,740 --> 00:44:21,180
losing information when this thing

1199
00:44:21,180 --> 00:44:23,460
rotates around for example

1200
00:44:23,460 --> 00:44:24,240
um

1201
00:44:24,240 --> 00:44:26,280
or just any sort of natural

1202
00:44:26,280 --> 00:44:27,960
Transformations like I was trying to

1203
00:44:27,960 --> 00:44:29,339
point out at the beginning I think it

1204
00:44:29,339 --> 00:44:30,180
was

1205
00:44:30,180 --> 00:44:31,740
trying to think about how the brain

1206
00:44:31,740 --> 00:44:34,020
models natural Transformations is

1207
00:44:34,020 --> 00:44:35,400
something that this these current

1208
00:44:35,400 --> 00:44:37,200
Frameworks

1209
00:44:37,200 --> 00:44:41,099
where do you see action playing a role

1210
00:44:41,099 --> 00:44:44,579
in terms of variational autoencoder

1211
00:44:44,579 --> 00:44:48,420
models that include not just

1212
00:44:48,420 --> 00:44:50,520
external patterns but also the

1213
00:44:50,520 --> 00:44:52,380
consequences of action or World model

1214
00:44:52,380 --> 00:44:55,800
structural structure with action

1215
00:44:55,800 --> 00:44:58,619
right yeah no that's a good question and

1216
00:44:58,619 --> 00:45:01,319
I think acted inferences uh

1217
00:45:01,319 --> 00:45:03,839
is effectively the the answer I think

1218
00:45:03,839 --> 00:45:05,940
it's a good answer to that

1219
00:45:05,940 --> 00:45:09,000
um I know there are

1220
00:45:09,000 --> 00:45:11,099
reinforcement learning Frameworks that

1221
00:45:11,099 --> 00:45:12,660
do use

1222
00:45:12,660 --> 00:45:15,060
kind of externally trained World models

1223
00:45:15,060 --> 00:45:17,280
so you train a vae or something and then

1224
00:45:17,280 --> 00:45:19,800
you use that representation in in your

1225
00:45:19,800 --> 00:45:23,040
reinforcement learning system but I

1226
00:45:23,040 --> 00:45:24,720
think having a fully

1227
00:45:24,720 --> 00:45:26,520
kind of a system that is a single

1228
00:45:26,520 --> 00:45:30,780
objective with uh action as part of the

1229
00:45:30,780 --> 00:45:33,660
likelihood of the data and uh

1230
00:45:33,660 --> 00:45:35,280
yeah I think that's much more elegant

1231
00:45:35,280 --> 00:45:38,940
and and so I'm a big proponent of that

1232
00:45:38,940 --> 00:45:39,960
um

1233
00:45:39,960 --> 00:45:43,140
I have not gotten so far as to study how

1234
00:45:43,140 --> 00:45:45,480
these structured World models in a vae

1235
00:45:45,480 --> 00:45:47,520
or I haven't worked on that at all but I

1236
00:45:47,520 --> 00:45:48,780
think it would certainly be very

1237
00:45:48,780 --> 00:45:50,819
interesting to see if having a more

1238
00:45:50,819 --> 00:45:52,339
structured World model

1239
00:45:52,339 --> 00:45:54,839
uh in a variational auto encoder would

1240
00:45:54,839 --> 00:45:56,880
be beneficial in that in an active

1241
00:45:56,880 --> 00:45:58,319
setting as well I think that would be

1242
00:45:58,319 --> 00:46:00,119
awesome I mean I think

1243
00:46:00,119 --> 00:46:03,599
some of these examples like uh showing

1244
00:46:03,599 --> 00:46:05,579
before like emergence of grid cells and

1245
00:46:05,579 --> 00:46:07,500
things like this maybe points towards

1246
00:46:07,500 --> 00:46:08,880
that direction

1247
00:46:08,880 --> 00:46:10,560
okay maybe the brain is doing something

1248
00:46:10,560 --> 00:46:12,540
it's really obviously has a lot of

1249
00:46:12,540 --> 00:46:13,680
structure

1250
00:46:13,680 --> 00:46:15,359
um this clearly has to be useful for

1251
00:46:15,359 --> 00:46:19,140
performing actions in some way

1252
00:46:19,140 --> 00:46:21,720
oh yeah I felt a really

1253
00:46:21,720 --> 00:46:24,480
nice parallel that you brought in with

1254
00:46:24,480 --> 00:46:28,040
the talk was the locally connected units

1255
00:46:28,040 --> 00:46:30,960
enabled your models to structurally

1256
00:46:30,960 --> 00:46:33,780
embody the convolutional

1257
00:46:33,780 --> 00:46:35,640
constraint and pattern and that led to

1258
00:46:35,640 --> 00:46:37,500
these arising patterns and then

1259
00:46:37,500 --> 00:46:41,339
analogously there was the uh Doral at

1260
00:46:41,339 --> 00:46:45,680
all where they had the path exploration

1261
00:46:45,680 --> 00:46:48,359
constraint right and so then it's

1262
00:46:48,359 --> 00:46:50,280
interesting to to

1263
00:46:50,280 --> 00:46:53,760
um you know think about these action or

1264
00:46:53,760 --> 00:46:56,819
policy heuristics or sparsities like a

1265
00:46:56,819 --> 00:46:59,579
joint motor exploration eventually it

1266
00:46:59,579 --> 00:47:02,339
becomes understood that there's like two

1267
00:47:02,339 --> 00:47:04,980
mutually opposing ways to move a joint

1268
00:47:04,980 --> 00:47:07,079
and then the compositionality across

1269
00:47:07,079 --> 00:47:09,119
joints can can be learned at these

1270
00:47:09,119 --> 00:47:10,680
higher levels once it's locked in at

1271
00:47:10,680 --> 00:47:14,480
lower levels so it's a very appealing

1272
00:47:14,480 --> 00:47:17,599
and uh

1273
00:47:17,599 --> 00:47:20,460
uh Niche relevant way to generalize

1274
00:47:20,460 --> 00:47:23,819
because it's both based upon the actual

1275
00:47:23,819 --> 00:47:25,740
constraints of the world but then

1276
00:47:25,740 --> 00:47:27,720
especially through action

1277
00:47:27,720 --> 00:47:29,460
potentially embedding something that's

1278
00:47:29,460 --> 00:47:31,380
quite simple

1279
00:47:31,380 --> 00:47:33,599
right yeah no I think that's definitely

1280
00:47:33,599 --> 00:47:36,599
true that's a really good point if uh if

1281
00:47:36,599 --> 00:47:38,339
you do have constraints coming from your

1282
00:47:38,339 --> 00:47:40,500
actions themselves then that's that

1283
00:47:40,500 --> 00:47:42,839
would be hugely beneficial for helping

1284
00:47:42,839 --> 00:47:44,819
to structure your

1285
00:47:44,819 --> 00:47:47,460
your latent space and I think yeah I

1286
00:47:47,460 --> 00:47:48,480
guess one thing I wanted to mention

1287
00:47:48,480 --> 00:47:49,980
there's

1288
00:47:49,980 --> 00:47:50,700
um

1289
00:47:50,700 --> 00:47:52,740
something made me think of like Stefano

1290
00:47:52,740 --> 00:47:55,500
fousey's work on kind of the

1291
00:47:55,500 --> 00:47:58,859
representational geometry uh

1292
00:47:58,859 --> 00:48:01,920
determines how we how

1293
00:48:01,920 --> 00:48:04,440
how generalizable a given understanding

1294
00:48:04,440 --> 00:48:08,099
of a system is uh and I think if you can

1295
00:48:08,099 --> 00:48:11,880
understand like these uh sets of

1296
00:48:11,880 --> 00:48:14,520
activities are separable or highly

1297
00:48:14,520 --> 00:48:16,079
parallel separable with a linear

1298
00:48:16,079 --> 00:48:18,839
classifier essentially then you're going

1299
00:48:18,839 --> 00:48:20,700
to be able to do generalization and I

1300
00:48:20,700 --> 00:48:23,099
think by imposing these types of biases

1301
00:48:23,099 --> 00:48:25,040
or potentially through

1302
00:48:25,040 --> 00:48:27,000
constraints that are imposed by action

1303
00:48:27,000 --> 00:48:28,740
something like this

1304
00:48:28,740 --> 00:48:32,040
you are yielding or kind of inducing a

1305
00:48:32,040 --> 00:48:33,660
better representational geometry and

1306
00:48:33,660 --> 00:48:35,220
this has all sorts of benefits for like

1307
00:48:35,220 --> 00:48:36,660
compositionality

1308
00:48:36,660 --> 00:48:39,359
yeah our generalization so

1309
00:48:39,359 --> 00:48:41,760
it's a great Point cool yeah very

1310
00:48:41,760 --> 00:48:43,440
interesting area all right I'll read

1311
00:48:43,440 --> 00:48:45,960
some questions from the live chat

1312
00:48:45,960 --> 00:48:48,420
love evolve wrote

1313
00:48:48,420 --> 00:48:52,260
any practical or observed limitations on

1314
00:48:52,260 --> 00:48:55,579
modeling illusions

1315
00:48:58,800 --> 00:49:00,420
learning communities is they're not

1316
00:49:00,420 --> 00:49:03,060
phobiated you don't have a a center of

1317
00:49:03,060 --> 00:49:05,940
gaze then you also don't have

1318
00:49:05,940 --> 00:49:07,940
um

1319
00:49:08,339 --> 00:49:11,460
like a time I mean most convolutional

1320
00:49:11,460 --> 00:49:13,260
neural networks I'm using these kind of

1321
00:49:13,260 --> 00:49:15,599
recurrent neural networks but time is

1322
00:49:15,599 --> 00:49:18,420
not as clearly defined in these models

1323
00:49:18,420 --> 00:49:20,220
as it is in a continuous time setting

1324
00:49:20,220 --> 00:49:23,400
for for a human undergoing an illusion

1325
00:49:23,400 --> 00:49:24,720
trial

1326
00:49:24,720 --> 00:49:25,319
um

1327
00:49:25,319 --> 00:49:27,480
and I think the combination of these two

1328
00:49:27,480 --> 00:49:30,359
of the fact that as a human or most

1329
00:49:30,359 --> 00:49:33,300
things uh

1330
00:49:33,300 --> 00:49:35,940
your gaze your shifting locations and

1331
00:49:35,940 --> 00:49:38,220
your gains are dependent on like you

1332
00:49:38,220 --> 00:49:40,140
looking to a particular area a lot of

1333
00:49:40,140 --> 00:49:42,780
cognitive tests and so I think it would

1334
00:49:42,780 --> 00:49:46,560
be really helpful if we had models that

1335
00:49:46,560 --> 00:49:48,540
yeah I mean learn that you can think of

1336
00:49:48,540 --> 00:49:50,760
this as a type of action like learning

1337
00:49:50,760 --> 00:49:52,980
where to move your gaze one of the

1338
00:49:52,980 --> 00:49:54,420
simplest possible that would help a lot

1339
00:49:54,420 --> 00:49:56,220
for being able to model Illusions and

1340
00:49:56,220 --> 00:49:58,859
just I mean for me it's like I read a

1341
00:49:58,859 --> 00:50:00,720
paper of some cognitive science

1342
00:50:00,720 --> 00:50:02,940
experiments or about some illusion and

1343
00:50:02,940 --> 00:50:05,160
it's like I think of okay can I put this

1344
00:50:05,160 --> 00:50:07,560
data set into my model and test it and

1345
00:50:07,560 --> 00:50:08,579
most of the time the answer is no

1346
00:50:08,579 --> 00:50:10,619
because I don't have a model that

1347
00:50:10,619 --> 00:50:12,900
looks around or has a restricted field

1348
00:50:12,900 --> 00:50:14,660
of view something like that

1349
00:50:14,660 --> 00:50:16,619
so yeah I think that's one of the

1350
00:50:16,619 --> 00:50:19,680
limitations another one is

1351
00:50:19,680 --> 00:50:20,579
um

1352
00:50:20,579 --> 00:50:22,920
yeah make the experiment much more

1353
00:50:22,920 --> 00:50:24,900
complicated so that's one of the

1354
00:50:24,900 --> 00:50:27,359
Practical limitations

1355
00:50:27,359 --> 00:50:30,240
wow great answer makes me think of a

1356
00:50:30,240 --> 00:50:33,920
paper with letters rotating on a table

1357
00:50:33,920 --> 00:50:36,780
that's the digit rotation great points

1358
00:50:36,780 --> 00:50:38,460
about the foveation and the Dynamics of

1359
00:50:38,460 --> 00:50:40,079
the illusion I think you actually did

1360
00:50:40,079 --> 00:50:42,599
mention an illusion which is however you

1361
00:50:42,599 --> 00:50:43,980
mentioned in the generalization context

1362
00:50:43,980 --> 00:50:46,859
which is rotating on the two-dimensional

1363
00:50:46,859 --> 00:50:49,500
screen doesn't generalize to three

1364
00:50:49,500 --> 00:50:52,920
dimensions and that dimensional collapse

1365
00:50:52,920 --> 00:50:55,559
or reduction is the basis of the cube

1366
00:50:55,559 --> 00:50:58,619
projection Illusions and Cube and figure

1367
00:50:58,619 --> 00:51:01,880
rotation Illusions it's on your screen

1368
00:51:01,880 --> 00:51:05,280
and there's a silhouette or there's some

1369
00:51:05,280 --> 00:51:07,260
ambiguous

1370
00:51:07,260 --> 00:51:09,839
stimuli that a generative it's near

1371
00:51:09,839 --> 00:51:12,359
criticality or a bifurcation in

1372
00:51:12,359 --> 00:51:13,680
degenerative models so it could

1373
00:51:13,680 --> 00:51:17,160
represent it one way or another way

1374
00:51:17,160 --> 00:51:19,920
and so a lot of the switching Illusions

1375
00:51:19,920 --> 00:51:22,020
are just based upon the flatness of

1376
00:51:22,020 --> 00:51:23,819
images

1377
00:51:23,819 --> 00:51:26,280
and the limitations and generalization

1378
00:51:26,280 --> 00:51:28,740
that are revealed by that

1379
00:51:28,740 --> 00:51:32,460
right yeah yeah I think there's even Oh

1380
00:51:32,460 --> 00:51:34,859
Yes somewhere sorry there's some work or

1381
00:51:34,859 --> 00:51:35,880
they

1382
00:51:35,880 --> 00:51:37,619
can argue people have a

1383
00:51:37,619 --> 00:51:39,480
three-dimensional image in their heads

1384
00:51:39,480 --> 00:51:42,000
like even Nancy Ken was a or her

1385
00:51:42,000 --> 00:51:45,119
laterals recently but and showing yeah I

1386
00:51:45,119 --> 00:51:48,119
don't know do do our models have that

1387
00:51:48,119 --> 00:51:50,460
it's not super big

1388
00:51:50,460 --> 00:51:53,700
anyway yeah that's pretty interesting

1389
00:51:53,700 --> 00:51:56,160
um all right from upcycle Club in the

1390
00:51:56,160 --> 00:51:58,200
chat they wrote kudos

1391
00:51:58,200 --> 00:52:00,000
if you're able to learn nearly as

1392
00:52:00,000 --> 00:52:02,160
effectively if you imagine you only want

1393
00:52:02,160 --> 00:52:03,780
a single neuron to be active for every

1394
00:52:03,780 --> 00:52:06,540
example uh your model is going to be

1395
00:52:06,540 --> 00:52:08,579
trying to memorize the data set design

1396
00:52:08,579 --> 00:52:10,980
or something like this

1397
00:52:10,980 --> 00:52:12,180
um and you're not going to have enough

1398
00:52:12,180 --> 00:52:14,940
capacity so yeah I think tuning that

1399
00:52:14,940 --> 00:52:18,359
level of sparsity is certainly uh

1400
00:52:18,359 --> 00:52:22,200
an important factor and

1401
00:52:22,200 --> 00:52:25,020
yeah when you look at the likelihood if

1402
00:52:25,020 --> 00:52:26,220
you're talking if you're doubling

1403
00:52:26,220 --> 00:52:28,579
framework typically this is balanced

1404
00:52:28,579 --> 00:52:32,040
automatically with the likelihood itself

1405
00:52:32,040 --> 00:52:33,000
um

1406
00:52:33,000 --> 00:52:34,380
if you're not doing generate modeling

1407
00:52:34,380 --> 00:52:35,760
you just have a sparsity penalty you're

1408
00:52:35,760 --> 00:52:38,460
going to want to tune in that parameter

1409
00:52:38,460 --> 00:52:40,980
okay yeah it's just just to clarify

1410
00:52:40,980 --> 00:52:43,380
runaway behavior in armina where the

1411
00:52:43,380 --> 00:52:45,599
network becomes unstable or chaotic due

1412
00:52:45,599 --> 00:52:47,040
to various factors such as feedback

1413
00:52:47,040 --> 00:52:50,400
loops noise or adversarial inputs

1414
00:52:50,400 --> 00:52:52,380
um

1415
00:52:52,380 --> 00:52:54,180
yeah I guess I haven't looked at this in

1416
00:52:54,180 --> 00:52:55,859
in like a recurrent setting where you

1417
00:52:55,859 --> 00:52:58,500
would get feedback loops

1418
00:52:58,500 --> 00:52:59,460
um

1419
00:52:59,460 --> 00:53:01,800
but I could yeah I could see adversarial

1420
00:53:01,800 --> 00:53:04,319
examples being potentially

1421
00:53:04,319 --> 00:53:07,800
affected by your level of sparsity

1422
00:53:07,800 --> 00:53:09,119
um

1423
00:53:09,119 --> 00:53:10,859
the interesting point is what would you

1424
00:53:10,859 --> 00:53:12,660
be more susceptible or less susceptible

1425
00:53:12,660 --> 00:53:16,040
to share examples I don't know

1426
00:53:16,040 --> 00:53:19,440
well sparsification

1427
00:53:19,440 --> 00:53:21,720
projecting from a fully connected higher

1428
00:53:21,720 --> 00:53:23,579
dimensional model just into

1429
00:53:23,579 --> 00:53:25,619
progressively smaller

1430
00:53:25,619 --> 00:53:27,540
it's pretty well understood in general

1431
00:53:27,540 --> 00:53:29,760
what the trade-offs are it's easier

1432
00:53:29,760 --> 00:53:34,079
computations a smaller model sparser

1433
00:53:34,079 --> 00:53:36,420
the basic graph is going to be clearer

1434
00:53:36,420 --> 00:53:39,119
to represent and then also it will have

1435
00:53:39,119 --> 00:53:41,339
all of the other trade-offs with false

1436
00:53:41,339 --> 00:53:43,680
positive and negatives of generalizing

1437
00:53:43,680 --> 00:53:45,720
but that's why it's an iterative fit

1438
00:53:45,720 --> 00:53:47,579
process so

1439
00:53:47,579 --> 00:53:49,760
I guess how does your

1440
00:53:49,760 --> 00:53:52,800
sparsification approach

1441
00:53:52,800 --> 00:53:55,700
balance

1442
00:53:56,700 --> 00:53:59,520
doesn't use AIC or Bic or some other

1443
00:53:59,520 --> 00:54:01,619
model fitting approach to determine the

1444
00:54:01,619 --> 00:54:03,660
relevant sparsification

1445
00:54:03,660 --> 00:54:07,079
for a given input

1446
00:54:07,079 --> 00:54:09,780
how do you determine how like like in

1447
00:54:09,780 --> 00:54:11,940
lasso regression like how do you know

1448
00:54:11,940 --> 00:54:14,339
how how much how do you threshold how

1449
00:54:14,339 --> 00:54:17,220
many how sparse you want it to be

1450
00:54:17,220 --> 00:54:19,559
right yeah I think there's a lot of good

1451
00:54:19,559 --> 00:54:22,440
literature on this and even so some

1452
00:54:22,440 --> 00:54:25,319
people like them at Harvard and some

1453
00:54:25,319 --> 00:54:29,520
people are working with now uh have done

1454
00:54:29,520 --> 00:54:31,559
these

1455
00:54:31,559 --> 00:54:34,380
that kind of unrolled

1456
00:54:34,380 --> 00:54:36,780
um iterative sparsification networks

1457
00:54:36,780 --> 00:54:37,800
where it's like a recurrent neural

1458
00:54:37,800 --> 00:54:40,380
network and iteratively sparsifies and

1459
00:54:40,380 --> 00:54:41,940
you can show that this yields something

1460
00:54:41,940 --> 00:54:45,780
like red lose or uh group like group

1461
00:54:45,780 --> 00:54:47,520
active group Sports activations like

1462
00:54:47,520 --> 00:54:48,960
we're using here

1463
00:54:48,960 --> 00:54:52,859
um in this setting uh it's really just

1464
00:54:52,859 --> 00:54:55,859
by having this

1465
00:54:55,859 --> 00:54:59,280
um this construction of this T variable

1466
00:54:59,280 --> 00:55:04,079
where we have Z on top and uh

1467
00:55:04,079 --> 00:55:07,859
and then it's in some effect gated by

1468
00:55:07,859 --> 00:55:09,119
these

1469
00:55:09,119 --> 00:55:11,579
the sum of U variables in the bottom so

1470
00:55:11,579 --> 00:55:13,200
if W maybe I wasn't super clear about

1471
00:55:13,200 --> 00:55:16,500
this is a matrix that is connecting

1472
00:55:16,500 --> 00:55:18,359
that's what defines the group so I'm

1473
00:55:18,359 --> 00:55:20,400
defining the group's Varsity uh that

1474
00:55:20,400 --> 00:55:22,380
connects all these U's together and so

1475
00:55:22,380 --> 00:55:23,940
the idea is

1476
00:55:23,940 --> 00:55:27,540
uh like here if all of one of the other

1477
00:55:27,540 --> 00:55:31,740
examples if all of your use uh are not

1478
00:55:31,740 --> 00:55:35,520
active for a given t

1479
00:55:35,520 --> 00:55:38,280
or if all varios are active for a given

1480
00:55:38,280 --> 00:55:41,040
t uh that t variable is going to be very

1481
00:55:41,040 --> 00:55:42,780
small right because your denominator is

1482
00:55:42,780 --> 00:55:44,339
going to be very big and that induces

1483
00:55:44,339 --> 00:55:47,160
sparsity so it's uh it's a constraint

1484
00:55:47,160 --> 00:55:49,260
satisfaction if you have a if you have a

1485
00:55:49,260 --> 00:55:51,839
set of U's that are all small uh then

1486
00:55:51,839 --> 00:55:54,480
that that constraint is satisfied and

1487
00:55:54,480 --> 00:55:57,180
now Z is allowed to kind of Express

1488
00:55:57,180 --> 00:56:00,240
itself and that's what then

1489
00:56:00,240 --> 00:56:02,880
uh kind of yeah achieves as far as

1490
00:56:02,880 --> 00:56:06,180
Activation so this is induced by these

1491
00:56:06,180 --> 00:56:07,020
two

1492
00:56:07,020 --> 00:56:09,300
uh kale Divergence terms here these are

1493
00:56:09,300 --> 00:56:12,960
saying like how far is the each unhc

1494
00:56:12,960 --> 00:56:15,180
from a gaussian and then through this

1495
00:56:15,180 --> 00:56:16,980
construction of the student T variable

1496
00:56:16,980 --> 00:56:20,880
we're effectively constructing a sparse

1497
00:56:20,880 --> 00:56:23,040
prior distribution just from these

1498
00:56:23,040 --> 00:56:24,839
gaussians but in terms of the act the

1499
00:56:24,839 --> 00:56:27,599
actual objective uh the terms and the

1500
00:56:27,599 --> 00:56:28,920
objective that we're optimizing are just

1501
00:56:28,920 --> 00:56:31,619
these two KL terms that are pushing it

1502
00:56:31,619 --> 00:56:34,020
towards sparsity to some extent and this

1503
00:56:34,020 --> 00:56:36,359
is balanced automatically with the

1504
00:56:36,359 --> 00:56:39,000
likelihood term here through the decoder

1505
00:56:39,000 --> 00:56:41,220
so we don't have terms that we're tuning

1506
00:56:41,220 --> 00:56:42,839
but we're learning the parameters of

1507
00:56:42,839 --> 00:56:44,280
these different encoders and then

1508
00:56:44,280 --> 00:56:48,200
analyzing the failed and emergencies

1509
00:56:48,540 --> 00:56:49,920
oh

1510
00:56:49,920 --> 00:56:52,859
all right another question from Dave

1511
00:56:52,859 --> 00:56:55,500
Douglas who wrote

1512
00:56:55,500 --> 00:56:59,160
speaking of gaze and illusion can the

1513
00:56:59,160 --> 00:57:01,920
studies on constances in infants be

1514
00:57:01,920 --> 00:57:04,260
separated into lower level illusion Rel

1515
00:57:04,260 --> 00:57:06,140
perhaps higher level

1516
00:57:06,140 --> 00:57:09,980
conceptual constancy

1517
00:57:13,619 --> 00:57:15,720
uh can you read

1518
00:57:15,720 --> 00:57:18,300
the current kind of architecture might

1519
00:57:18,300 --> 00:57:23,520
the studies on constances in in infants

1520
00:57:23,520 --> 00:57:26,640
um cognitive constances be separated

1521
00:57:26,640 --> 00:57:31,619
yeah probably I'm not I'm not uh

1522
00:57:31,619 --> 00:57:33,900
an expert or actually even very familiar

1523
00:57:33,900 --> 00:57:35,460
with like

1524
00:57:35,460 --> 00:57:37,859
object permanency studies and infants

1525
00:57:37,859 --> 00:57:40,260
and constancy stuff so but I think that

1526
00:57:40,260 --> 00:57:42,300
would be incredibly interesting to study

1527
00:57:42,300 --> 00:57:44,339
in in neural network architectures and

1528
00:57:44,339 --> 00:57:46,740
that was kind of some of the idea with

1529
00:57:46,740 --> 00:57:48,780
this uh illusion that I was trying to

1530
00:57:48,780 --> 00:57:51,780
model down here with this line I don't

1531
00:57:51,780 --> 00:57:53,099
know if I was super clear about this but

1532
00:57:53,099 --> 00:57:55,380
the top row is the input and we're

1533
00:57:55,380 --> 00:57:57,359
effectively like blocking the input for

1534
00:57:57,359 --> 00:58:00,119
a single frame and I wanted to see does

1535
00:58:00,119 --> 00:58:03,240
the network kind of encode that that the

1536
00:58:03,240 --> 00:58:05,400
thing is still there when that frame is

1537
00:58:05,400 --> 00:58:07,680
gone can I still decode the presence of

1538
00:58:07,680 --> 00:58:10,020
the object from the neural activity uh

1539
00:58:10,020 --> 00:58:11,819
and then what is it also inferring about

1540
00:58:11,819 --> 00:58:13,920
the motion because of the fact that it

1541
00:58:13,920 --> 00:58:15,780
saw the bars at a slightly different

1542
00:58:15,780 --> 00:58:18,480
location than from before when the after

1543
00:58:18,480 --> 00:58:20,520
the frame is gone

1544
00:58:20,520 --> 00:58:22,559
um so

1545
00:58:22,559 --> 00:58:25,200
yeah I think it's definitely multiple

1546
00:58:25,200 --> 00:58:27,240
levels to it

1547
00:58:27,240 --> 00:58:29,160
um where some would probably be much

1548
00:58:29,160 --> 00:58:33,180
lower level and uh

1549
00:58:33,180 --> 00:58:35,880
maybe long-term object permanency I

1550
00:58:35,880 --> 00:58:37,380
would guess would be significantly

1551
00:58:37,380 --> 00:58:39,059
higher level

1552
00:58:39,059 --> 00:58:39,900
um

1553
00:58:39,900 --> 00:58:41,760
it just makes me think of those

1554
00:58:41,760 --> 00:58:44,640
experiments with cats back in the day

1555
00:58:44,640 --> 00:58:47,280
where it's like they raise them in

1556
00:58:47,280 --> 00:58:49,020
darkness except for an hour a day they

1557
00:58:49,020 --> 00:58:51,000
put them in Vertical World or horizontal

1558
00:58:51,000 --> 00:58:53,160
world or they only saw horizontal lines

1559
00:58:53,160 --> 00:58:57,299
or vertical lines uh and you can see the

1560
00:58:57,299 --> 00:58:59,880
the organization of their cortex changes

1561
00:58:59,880 --> 00:59:02,819
like they have less receptivity it's a

1562
00:59:02,819 --> 00:59:04,200
horizontal lines if they've never seen

1563
00:59:04,200 --> 00:59:06,420
horizontal lines before and then you

1564
00:59:06,420 --> 00:59:07,980
take a stick and you wave it in front of

1565
00:59:07,980 --> 00:59:09,599
their face and if the stick is

1566
00:59:09,599 --> 00:59:11,220
horizontal they just they do nothing

1567
00:59:11,220 --> 00:59:12,900
it's vertical they're swatting at it

1568
00:59:12,900 --> 00:59:14,460
they're trying to hit it it's like they

1569
00:59:14,460 --> 00:59:15,900
just literally don't have to Bar in

1570
00:59:15,900 --> 00:59:18,420
front of their face so I think in that

1571
00:59:18,420 --> 00:59:20,700
case then this is evidence of a

1572
00:59:20,700 --> 00:59:24,260
low-level deficiency and vision

1573
00:59:24,260 --> 00:59:26,940
contributing to some sort of an illusion

1574
00:59:26,940 --> 00:59:28,980
so I I think yeah there could certainly

1575
00:59:28,980 --> 00:59:30,660
be some aspect of that in infants as

1576
00:59:30,660 --> 00:59:32,839
well

1577
00:59:33,540 --> 00:59:36,420
one very curious point you've brought up

1578
00:59:36,420 --> 00:59:40,339
was the animate and inanimate

1579
00:59:40,339 --> 00:59:43,619
manifold with small things being

1580
00:59:43,619 --> 00:59:45,480
intermediate

1581
00:59:45,480 --> 00:59:49,140
right what does that represent

1582
00:59:49,140 --> 00:59:52,319
or or is it because they're handleable

1583
00:59:52,319 --> 00:59:55,619
or it might be an insect or it might be

1584
00:59:55,619 --> 00:59:57,839
something that might move away just with

1585
00:59:57,839 --> 01:00:01,380
wind or what does that say

1586
01:00:01,380 --> 01:00:04,380
right yeah uh

1587
01:00:04,380 --> 01:00:08,280
so this is work by like Talia conkle I

1588
01:00:08,280 --> 01:00:11,280
think was the one who discovered this

1589
01:00:11,280 --> 01:00:12,240
um

1590
01:00:12,240 --> 01:00:14,880
organization and they they tried to

1591
01:00:14,880 --> 01:00:16,440
figure it out I don't I might be getting

1592
01:00:16,440 --> 01:00:19,500
this wrong so I recommend people to read

1593
01:00:19,500 --> 01:00:21,359
her work on that if they call it

1594
01:00:21,359 --> 01:00:23,880
tripartite organization but if I

1595
01:00:23,880 --> 01:00:25,319
remember correctly

1596
01:00:25,319 --> 01:00:27,900
they did a lot of follow-up work on why

1597
01:00:27,900 --> 01:00:30,780
it's there's this organization and

1598
01:00:30,780 --> 01:00:33,180
some evidence of

1599
01:00:33,180 --> 01:00:35,700
curvature of these objects and kind of

1600
01:00:35,700 --> 01:00:37,440
like the distance that you see objects

1601
01:00:37,440 --> 01:00:40,260
from or like

1602
01:00:40,260 --> 01:00:43,319
um animate objects or maybe more curvy

1603
01:00:43,319 --> 01:00:45,599
or there's there regardless of what the

1604
01:00:45,599 --> 01:00:46,859
actual answer is there were a lot of

1605
01:00:46,859 --> 01:00:48,720
different hypotheses that were stemming

1606
01:00:48,720 --> 01:00:51,720
from like properties of these objects

1607
01:00:51,720 --> 01:00:54,000
maybe mid-level or low level properties

1608
01:00:54,000 --> 01:00:56,280
more so than higher level properties I

1609
01:00:56,280 --> 01:00:57,599
still don't know if it's exactly been

1610
01:00:57,599 --> 01:00:59,339
solved of whether it's like

1611
01:00:59,339 --> 01:01:01,500
interaction like you said with the

1612
01:01:01,500 --> 01:01:04,920
objects causes the separation or

1613
01:01:04,920 --> 01:01:06,119
um

1614
01:01:06,119 --> 01:01:09,540
or yeah the general shapes of these

1615
01:01:09,540 --> 01:01:12,299
objects I would bet as with most things

1616
01:01:12,299 --> 01:01:13,980
it's like some combination of all of the

1617
01:01:13,980 --> 01:01:16,980
above uh but I think the interesting

1618
01:01:16,980 --> 01:01:18,480
thing from this modeling point of view

1619
01:01:18,480 --> 01:01:19,859
is that

1620
01:01:19,859 --> 01:01:21,480
um

1621
01:01:21,480 --> 01:01:24,059
this is only trained on correlation

1622
01:01:24,059 --> 01:01:26,819
statistics from the image data sets

1623
01:01:26,819 --> 01:01:28,799
itself so this has no interaction this

1624
01:01:28,799 --> 01:01:32,760
has no notion of animacy uh I mean this

1625
01:01:32,760 --> 01:01:34,140
is really just training a model on

1626
01:01:34,140 --> 01:01:37,859
imagenet just images of dogs cats boats

1627
01:01:37,859 --> 01:01:40,020
whatever and yet it still achieves this

1628
01:01:40,020 --> 01:01:41,640
type of organization so there's some

1629
01:01:41,640 --> 01:01:42,540
sort of

1630
01:01:42,540 --> 01:01:44,940
it could be semantic characteristics

1631
01:01:44,940 --> 01:01:46,740
right we have image we have a network

1632
01:01:46,740 --> 01:01:48,359
that can classify

1633
01:01:48,359 --> 01:01:51,000
boats versus dogs versus 20 other breeds

1634
01:01:51,000 --> 01:01:53,640
of dogs but if

1635
01:01:53,640 --> 01:01:55,920
it might also have some correspondence

1636
01:01:55,920 --> 01:01:57,900
with lower level finish statistics as

1637
01:01:57,900 --> 01:01:59,400
well

1638
01:01:59,400 --> 01:02:02,780
so yeah I don't know I guess

1639
01:02:03,960 --> 01:02:07,500
yeah provocative analogy was the

1640
01:02:07,500 --> 01:02:10,380
translational shift

1641
01:02:10,380 --> 01:02:12,900
in the mnist in the handwriting

1642
01:02:12,900 --> 01:02:14,819
recognition setting

1643
01:02:14,819 --> 01:02:18,000
what are the translational shifts

1644
01:02:18,000 --> 01:02:20,160
that

1645
01:02:20,160 --> 01:02:22,740
exists today what's the three pixel

1646
01:02:22,740 --> 01:02:24,780
example is that some prompt engineered

1647
01:02:24,780 --> 01:02:27,540
attack on an llm or something or

1648
01:02:27,540 --> 01:02:29,099
something right a special character

1649
01:02:29,099 --> 01:02:32,640
being inserted or or a um uh some

1650
01:02:32,640 --> 01:02:35,160
overlay on an image that we can't even

1651
01:02:35,160 --> 01:02:37,020
detect that

1652
01:02:37,020 --> 01:02:39,359
so what do you think those challenges

1653
01:02:39,359 --> 01:02:42,839
are and what are ways that we can pursue

1654
01:02:42,839 --> 01:02:44,940
that

1655
01:02:44,940 --> 01:02:47,520
yeah absolutely I mean I think it's kind

1656
01:02:47,520 --> 01:02:48,420
of

1657
01:02:48,420 --> 01:02:50,099
the way I was thinking about it is like

1658
01:02:50,099 --> 01:02:52,680
these symmetry Transformations

1659
01:02:52,680 --> 01:02:53,760
um

1660
01:02:53,760 --> 01:02:55,799
if you're thinking about language models

1661
01:02:55,799 --> 01:02:57,420
you can imagine a symmetry

1662
01:02:57,420 --> 01:02:58,500
transformation that's just like

1663
01:02:58,500 --> 01:03:00,240
replacing a word with a synonym or

1664
01:03:00,240 --> 01:03:03,780
something uh you have the sentence to us

1665
01:03:03,780 --> 01:03:06,000
means the exact same thing but now

1666
01:03:06,000 --> 01:03:07,380
suddenly the model is going to respond

1667
01:03:07,380 --> 01:03:09,299
very differently

1668
01:03:09,299 --> 01:03:11,240
um

1669
01:03:11,240 --> 01:03:15,359
like translation between languages this

1670
01:03:15,359 --> 01:03:16,799
can be seen as a type of transformation

1671
01:03:16,799 --> 01:03:19,440
it preserves the underlying

1672
01:03:19,440 --> 01:03:21,960
meaning of the input

1673
01:03:21,960 --> 01:03:24,900
to us but to the model it looks

1674
01:03:24,900 --> 01:03:26,220
completely different and we would like

1675
01:03:26,220 --> 01:03:28,380
to have models which behave in a

1676
01:03:28,380 --> 01:03:29,940
predictable way with respect to these

1677
01:03:29,940 --> 01:03:32,160
types of Transformations because

1678
01:03:32,160 --> 01:03:35,040
I think humans behave very predictably

1679
01:03:35,040 --> 01:03:37,319
since these Transformations and when

1680
01:03:37,319 --> 01:03:39,920
we're dealing with AI systems we expect

1681
01:03:39,920 --> 01:03:43,200
them to also behave that way and I think

1682
01:03:43,200 --> 01:03:45,480
that's part of what causes a lot of

1683
01:03:45,480 --> 01:03:47,339
challenges interacting with these

1684
01:03:47,339 --> 01:03:49,460
systems and I kind of tried to do a

1685
01:03:49,460 --> 01:03:52,500
rough cheeky demonstration of that with

1686
01:03:52,500 --> 01:03:54,960
with this bear and squares and stuff

1687
01:03:54,960 --> 01:03:58,440
like um we expect

1688
01:03:58,440 --> 01:04:00,480
it to be able to do something simple

1689
01:04:00,480 --> 01:04:02,220
like this because we think most humans

1690
01:04:02,220 --> 01:04:04,020
could and yet it doesn't and if you

1691
01:04:04,020 --> 01:04:05,460
imagine this is a critical scenario

1692
01:04:05,460 --> 01:04:07,740
where you expect this and that's a big

1693
01:04:07,740 --> 01:04:08,700
problem

1694
01:04:08,700 --> 01:04:11,099
um how do we handle that that's yeah I

1695
01:04:11,099 --> 01:04:12,720
think that's kind of the what I'm

1696
01:04:12,720 --> 01:04:15,859
searching for I think

1697
01:04:16,319 --> 01:04:18,420
my

1698
01:04:18,420 --> 01:04:22,280
Direction I'm taking it is look

1699
01:04:22,280 --> 01:04:26,940
more simple and like bottom up

1700
01:04:26,940 --> 01:04:29,460
building blocks of

1701
01:04:29,460 --> 01:04:31,380
neural network architectures or

1702
01:04:31,380 --> 01:04:33,180
algorithms that

1703
01:04:33,180 --> 01:04:35,760
kind of yield these emergent structural

1704
01:04:35,760 --> 01:04:37,680
properties and I think that's a much

1705
01:04:37,680 --> 01:04:39,839
more generalizable way rather than

1706
01:04:39,839 --> 01:04:41,579
building something on top of what we

1707
01:04:41,579 --> 01:04:43,140
already have

1708
01:04:43,140 --> 01:04:43,920
um

1709
01:04:43,920 --> 01:04:45,839
I think that's something that will scale

1710
01:04:45,839 --> 01:04:47,819
much better and also matches more what

1711
01:04:47,819 --> 01:04:50,299
the brain does

1712
01:04:50,760 --> 01:04:52,740
very cool one kind of implementational

1713
01:04:52,740 --> 01:04:54,740
question what are the computational

1714
01:04:54,740 --> 01:04:57,000
requirements of just running this or

1715
01:04:57,000 --> 01:04:59,400
what's the day-to-day like of being a

1716
01:04:59,400 --> 01:05:01,920
student or researcher running variants

1717
01:05:01,920 --> 01:05:04,380
of these like do they use terabytes of

1718
01:05:04,380 --> 01:05:07,020
data and you're using large computation

1719
01:05:07,020 --> 01:05:08,940
or is this something that people can run

1720
01:05:08,940 --> 01:05:11,880
on their own laptops

1721
01:05:11,880 --> 01:05:13,980
I think almost everything I presented

1722
01:05:13,980 --> 01:05:17,099
today can be run locally so like

1723
01:05:17,099 --> 01:05:20,040
this stuff is super simple you can run I

1724
01:05:20,040 --> 01:05:20,760
mean

1725
01:05:20,760 --> 01:05:22,319
you're gonna you

1726
01:05:22,319 --> 01:05:24,299
think you can run it on your laptop if

1727
01:05:24,299 --> 01:05:25,980
you want to like train and experiment

1728
01:05:25,980 --> 01:05:27,420
with different things it's going to be

1729
01:05:27,420 --> 01:05:30,119
pretty slow so I'd recommend some

1730
01:05:30,119 --> 01:05:33,359
commercial GPU like a I run pretty much

1731
01:05:33,359 --> 01:05:35,640
everything on like Nvidia 1080s

1732
01:05:35,640 --> 01:05:38,819
pretty old pretty cheap but they have 12

1733
01:05:38,819 --> 01:05:41,099
gigs of RAM or whatever and it's kind of

1734
01:05:41,099 --> 01:05:43,140
more than enough for these models four

1735
01:05:43,140 --> 01:05:46,440
gigabytes of RAM I think one thing that

1736
01:05:46,440 --> 01:05:48,839
some people think is weird is I do most

1737
01:05:48,839 --> 01:05:51,480
of my experiments on stuff like mnist so

1738
01:05:51,480 --> 01:05:54,780
it's 32 by 32 pixel images because I can

1739
01:05:54,780 --> 01:05:57,299
train it small and vocally

1740
01:05:57,299 --> 01:06:00,000
um if you want to do so my experiments

1741
01:06:00,000 --> 01:06:02,460
or an endless if you want to do stuff

1742
01:06:02,460 --> 01:06:03,540
like this these are much more

1743
01:06:03,540 --> 01:06:05,640
complicated this hamiltonian Dynamic

1744
01:06:05,640 --> 01:06:08,160
Suite here you're getting into bigger

1745
01:06:08,160 --> 01:06:09,780
models that are running across multiple

1746
01:06:09,780 --> 01:06:12,240
gpus and so here is using a cluster to

1747
01:06:12,240 --> 01:06:14,220
run these types of models

1748
01:06:14,220 --> 01:06:16,140
um but I'd say most of the single

1749
01:06:16,140 --> 01:06:18,920
machine with the GPU is more than enough

1750
01:06:18,920 --> 01:06:21,680
or even just like in a collab notebook

1751
01:06:21,680 --> 01:06:24,539
something like that if you want to train

1752
01:06:24,539 --> 01:06:26,520
something on imagenet it gets more

1753
01:06:26,520 --> 01:06:29,940
complicated and you need

1754
01:06:29,940 --> 01:06:33,660
at least one GPU ideally more but yeah I

1755
01:06:33,660 --> 01:06:35,160
don't do a whole lot of big scale stuff

1756
01:06:35,160 --> 01:06:37,200
yet I think it's certainly interesting

1757
01:06:37,200 --> 01:06:39,960
and there's definitely a lot more you

1758
01:06:39,960 --> 01:06:42,539
can do there but for some of these kind

1759
01:06:42,539 --> 01:06:45,480
of simpler or more fundamental questions

1760
01:06:45,480 --> 01:06:47,760
I don't know what you want to call it um

1761
01:06:47,760 --> 01:06:52,500
a smaller machine is nice and fast so

1762
01:06:52,500 --> 01:06:54,660
cool useful

1763
01:06:54,660 --> 01:06:58,140
all right I'll read a comment from Dave

1764
01:06:58,140 --> 01:07:00,780
recalling Bert DeVries comment during

1765
01:07:00,780 --> 01:07:02,880
the applied active inference Symposium

1766
01:07:02,880 --> 01:07:05,520
about the desirability of spending less

1767
01:07:05,520 --> 01:07:08,099
effort or ATP on foraging or control

1768
01:07:08,099 --> 01:07:09,780
situations where we don't need much

1769
01:07:09,780 --> 01:07:11,579
Precision I don't know if you listen to

1770
01:07:11,579 --> 01:07:13,859
this but Professor DeVries mentioned

1771
01:07:13,859 --> 01:07:17,520
about variable Precision models and how

1772
01:07:17,520 --> 01:07:19,440
they could be used to

1773
01:07:19,440 --> 01:07:21,059
enable different features of

1774
01:07:21,059 --> 01:07:23,039
generalization and actual structural

1775
01:07:23,039 --> 01:07:25,020
course training as well as like reduced

1776
01:07:25,020 --> 01:07:27,059
computational requirements

1777
01:07:27,059 --> 01:07:29,880
does he have any suggestions on how to

1778
01:07:29,880 --> 01:07:31,980
introduce this distinction into active

1779
01:07:31,980 --> 01:07:33,900
inference Theory what kinds of

1780
01:07:33,900 --> 01:07:37,760
experiments could Winkle this out

1781
01:07:38,400 --> 01:07:40,680
oh wow yeah that's something I don't I

1782
01:07:40,680 --> 01:07:42,660
don't think I have too much intelligent

1783
01:07:42,660 --> 01:07:46,619
to say about that's completely honest

1784
01:07:46,619 --> 01:07:48,740
um

1785
01:07:51,359 --> 01:07:53,460
it's super interesting question because

1786
01:07:53,460 --> 01:07:56,220
I think the intuition makes a lot of

1787
01:07:56,220 --> 01:07:58,260
sense to me that uh

1788
01:07:58,260 --> 01:08:00,000
you're talking about

1789
01:08:00,000 --> 01:08:01,980
if I understand correctly variable rates

1790
01:08:01,980 --> 01:08:05,160
of precision when you're encoding in or

1791
01:08:05,160 --> 01:08:07,079
or in your model in general doing

1792
01:08:07,079 --> 01:08:07,740
computation

1793
01:08:07,740 --> 01:08:09,420
[Music]

1794
01:08:09,420 --> 01:08:11,539
um

1795
01:08:13,200 --> 01:08:17,040
that somehow has an impact on your your

1796
01:08:17,040 --> 01:08:19,080
future performance as a relation to some

1797
01:08:19,080 --> 01:08:22,259
energy store I think yeah and if you

1798
01:08:22,259 --> 01:08:23,580
wanted to build this into an active

1799
01:08:23,580 --> 01:08:26,279
effort system you would need to have

1800
01:08:26,279 --> 01:08:28,679
really an embodied system where the

1801
01:08:28,679 --> 01:08:31,500
agent has some notion of energy like an

1802
01:08:31,500 --> 01:08:34,439
internal energy store and

1803
01:08:34,439 --> 01:08:36,238
yeah something something that is trying

1804
01:08:36,238 --> 01:08:38,520
to conserve while it's performing its

1805
01:08:38,520 --> 01:08:40,500
actions uh

1806
01:08:40,500 --> 01:08:42,359
and running out of energy would need to

1807
01:08:42,359 --> 01:08:44,759
need something bad for the agents and

1808
01:08:44,759 --> 01:08:47,399
then maybe you could observe kind of an

1809
01:08:47,399 --> 01:08:48,679
emergence

1810
01:08:48,679 --> 01:08:52,040
uh reduction and

1811
01:08:52,040 --> 01:08:55,198
encoding Precision or something like

1812
01:08:55,198 --> 01:08:57,479
this as as the agent is trying to learn

1813
01:08:57,479 --> 01:09:00,060
to act more effectively you might have

1814
01:09:00,060 --> 01:09:02,520
to give it an ability to control its

1815
01:09:02,520 --> 01:09:04,020
precision

1816
01:09:04,020 --> 01:09:07,080
yeah like I say out of my expertise but

1817
01:09:07,080 --> 01:09:08,819
it's kind of thoughts

1818
01:09:08,819 --> 01:09:11,460
okay on this slide right here first very

1819
01:09:11,460 --> 01:09:14,219
cool image it's kind of like a

1820
01:09:14,219 --> 01:09:18,359
digital Jackson Pollock

1821
01:09:18,359 --> 01:09:22,920
um if it were a simpler input data

1822
01:09:22,920 --> 01:09:26,520
size or just reduced complexity of

1823
01:09:26,520 --> 01:09:27,719
patterns or if it were an increased

1824
01:09:27,719 --> 01:09:30,000
complexity how would this image look

1825
01:09:30,000 --> 01:09:31,920
different

1826
01:09:31,920 --> 01:09:34,620
yeah so I did some experiments trying to

1827
01:09:34,620 --> 01:09:36,738
change these

1828
01:09:36,738 --> 01:09:40,439
orientation columns and

1829
01:09:40,439 --> 01:09:41,100
um

1830
01:09:41,100 --> 01:09:43,140
you can yeah basically changing the

1831
01:09:43,140 --> 01:09:44,520
parameters of the model you can get

1832
01:09:44,520 --> 01:09:47,100
these columns to be bigger you can get

1833
01:09:47,100 --> 01:09:49,380
them to not have very similar structure

1834
01:09:49,380 --> 01:09:51,660
to what we see in the humans where we

1835
01:09:51,660 --> 01:09:53,040
have you you can get them to have more

1836
01:09:53,040 --> 01:09:55,199
bands of activity

1837
01:09:55,199 --> 01:09:56,160
um

1838
01:09:56,160 --> 01:09:58,199
and it also like you said it depends on

1839
01:09:58,199 --> 01:10:00,540
the data set that you're using if I use

1840
01:10:00,540 --> 01:10:03,179
like really simple sinusoidal gradings

1841
01:10:03,179 --> 01:10:05,580
as input I get something like this I get

1842
01:10:05,580 --> 01:10:07,920
something that's a little bit more uh

1843
01:10:07,920 --> 01:10:11,640
rotational curvy higher entropy

1844
01:10:11,640 --> 01:10:12,800
um

1845
01:10:12,800 --> 01:10:15,660
so I think these are all interesting

1846
01:10:15,660 --> 01:10:18,000
things if you want to study the

1847
01:10:18,000 --> 01:10:19,320
emergence of this type of organization

1848
01:10:19,320 --> 01:10:22,199
in a natural system uh if you have a

1849
01:10:22,199 --> 01:10:24,120
model that now yields different

1850
01:10:24,120 --> 01:10:25,860
Organization for different

1851
01:10:25,860 --> 01:10:28,620
settings that's a see okay then what

1852
01:10:28,620 --> 01:10:31,679
settings best match our observed data

1853
01:10:31,679 --> 01:10:32,340
um

1854
01:10:32,340 --> 01:10:34,679
so so yeah I can

1855
01:10:34,679 --> 01:10:36,540
I can send those around it if you're

1856
01:10:36,540 --> 01:10:38,520
interested but

1857
01:10:38,520 --> 01:10:40,580
um

1858
01:10:41,580 --> 01:10:44,219
yeah I think one also other sorry one

1859
01:10:44,219 --> 01:10:45,659
one other interesting point there is

1860
01:10:45,659 --> 01:10:46,920
that the

1861
01:10:46,920 --> 01:10:51,480
uh different animals and types of uh

1862
01:10:51,480 --> 01:10:53,219
orientation selectivity and different

1863
01:10:53,219 --> 01:10:54,659
numbers of pinwheels some animals don't

1864
01:10:54,659 --> 01:10:57,420
have it at all I think maybe mice if I'm

1865
01:10:57,420 --> 01:11:00,120
correct have this kind of uh they call a

1866
01:11:00,120 --> 01:11:01,800
salt and pepper selectivity so it's

1867
01:11:01,800 --> 01:11:03,480
basically random you don't have any sort

1868
01:11:03,480 --> 01:11:04,679
of like topographic orientation

1869
01:11:04,679 --> 01:11:06,239
sensitivity

1870
01:11:06,239 --> 01:11:09,300
um so there is evidence that

1871
01:11:09,300 --> 01:11:10,920
yeah different systems do this

1872
01:11:10,920 --> 01:11:13,020
differently and it's interesting to

1873
01:11:13,020 --> 01:11:14,760
figure out why

1874
01:11:14,760 --> 01:11:17,760
yeah this is very cool it reminds me of

1875
01:11:17,760 --> 01:11:21,300
first the reaction diffusion

1876
01:11:21,300 --> 01:11:22,980
base and time

1877
01:11:22,980 --> 01:11:25,739
so it's actually

1878
01:11:25,739 --> 01:11:30,000
um possible that a region might have no

1879
01:11:30,000 --> 01:11:32,840
activity from a given

1880
01:11:32,840 --> 01:11:35,640
granularity like if it was being looked

1881
01:11:35,640 --> 01:11:39,360
at at fmri spatial and temporal time

1882
01:11:39,360 --> 01:11:40,520
scale

1883
01:11:40,520 --> 01:11:44,699
if the pockets of activity but if the

1884
01:11:44,699 --> 01:11:46,380
pockets of activity are

1885
01:11:46,380 --> 01:11:48,480
slower faster

1886
01:11:48,480 --> 01:11:52,260
than that measurement is going to not be

1887
01:11:52,260 --> 01:11:54,060
different than noise it'll all have been

1888
01:11:54,060 --> 01:11:55,739
averaged out

1889
01:11:55,739 --> 01:11:58,620
so then there might be some yeah

1890
01:11:58,620 --> 01:12:01,560
interesting like data sets that do

1891
01:12:01,560 --> 01:12:03,360
actually have a lot of

1892
01:12:03,360 --> 01:12:06,179
richness but then for one reason or

1893
01:12:06,179 --> 01:12:08,520
another it just was averaged out over

1894
01:12:08,520 --> 01:12:11,100
because it wasn't being connected to you

1895
01:12:11,100 --> 01:12:12,420
or something like this you really need

1896
01:12:12,420 --> 01:12:14,520
to go with a single trial level you need

1897
01:12:14,520 --> 01:12:16,140
to have high enough spatial resolution

1898
01:12:16,140 --> 01:12:18,719
such that it's you know it satisfies

1899
01:12:18,719 --> 01:12:23,100
micro frequencies uh and and this just

1900
01:12:23,100 --> 01:12:24,659
is something that people didn't do for a

1901
01:12:24,659 --> 01:12:25,620
long time especially if you're doing

1902
01:12:25,620 --> 01:12:27,780
single electorate recordings you're not

1903
01:12:27,780 --> 01:12:28,920
going to see a traveling wave you're

1904
01:12:28,920 --> 01:12:30,900
going to see oscillations

1905
01:12:30,900 --> 01:12:32,219
um so you need like multi-electric

1906
01:12:32,219 --> 01:12:34,199
arrays and basically they're saying okay

1907
01:12:34,199 --> 01:12:36,000
yeah now that we have the technology to

1908
01:12:36,000 --> 01:12:37,520
do this as much

1909
01:12:37,520 --> 01:12:40,080
persists that we didn't see before and

1910
01:12:40,080 --> 01:12:42,960
potentially this is an explanation for a

1911
01:12:42,960 --> 01:12:44,400
lot of the noise that we were seeing

1912
01:12:44,400 --> 01:12:46,260
before maybe it really is just traveling

1913
01:12:46,260 --> 01:12:47,219
waves

1914
01:12:47,219 --> 01:12:47,760
um

1915
01:12:47,760 --> 01:12:51,480
so yeah I think there's a lot to be done

1916
01:12:51,480 --> 01:12:53,880
in the future with increased

1917
01:12:53,880 --> 01:12:56,520
abilities for recording

1918
01:12:56,520 --> 01:12:58,739
that's very cool

1919
01:12:58,739 --> 01:13:02,940
well any final thoughts or questions or

1920
01:13:02,940 --> 01:13:06,239
where are you gonna take this work

1921
01:13:06,239 --> 01:13:08,520
yeah no thanks for having me

1922
01:13:08,520 --> 01:13:10,140
um

1923
01:13:10,140 --> 01:13:11,640
hopefully in the active infrastructure

1924
01:13:11,640 --> 01:13:14,520
that's that's I would love to I think

1925
01:13:14,520 --> 01:13:16,560
that would be super fun so yeah I'm not

1926
01:13:16,560 --> 01:13:18,980
really sure I'm looking at Maybe music

1927
01:13:18,980 --> 01:13:21,659
uh right now

1928
01:13:21,659 --> 01:13:22,440
um

1929
01:13:22,440 --> 01:13:26,760
looking at uh

1930
01:13:26,760 --> 01:13:30,420
other other crazy directions I don't

1931
01:13:30,420 --> 01:13:33,020
want to sound too crazy uh

1932
01:13:33,020 --> 01:13:36,900
but I'll go down yeah a lot of things so

1933
01:13:36,900 --> 01:13:38,580
one one thing that's coming up something

1934
01:13:38,580 --> 01:13:40,320
we submitted to neurops is studying

1935
01:13:40,320 --> 01:13:43,140
memory with traveling waves

1936
01:13:43,140 --> 01:13:45,060
um so that paper just came out on

1937
01:13:45,060 --> 01:13:46,860
archive today uh

1938
01:13:46,860 --> 01:13:48,840
how waves are really good at encoding

1939
01:13:48,840 --> 01:13:50,580
long-term memories which I think is

1940
01:13:50,580 --> 01:13:52,100
super interesting

1941
01:13:52,100 --> 01:13:54,120
so I might go a little one in that

1942
01:13:54,120 --> 01:13:55,800
direction

1943
01:13:55,800 --> 01:13:58,920
sounds good and yes would be very

1944
01:13:58,920 --> 01:14:01,400
exciting to see action come into play

1945
01:14:01,400 --> 01:14:04,560
when there was the neurons that stayed

1946
01:14:04,560 --> 01:14:07,620
active even as the dog's feet were

1947
01:14:07,620 --> 01:14:09,060
moving

1948
01:14:09,060 --> 01:14:11,480
there's a lot of like action sequences

1949
01:14:11,480 --> 01:14:14,280
like throwing a baseball and then it

1950
01:14:14,280 --> 01:14:15,900
goes and it's like there's something

1951
01:14:15,900 --> 01:14:18,440
about that action that's continuing to

1952
01:14:18,440 --> 01:14:21,179
influence and so having like a deep

1953
01:14:21,179 --> 01:14:23,699
temporal representation of alternative

1954
01:14:23,699 --> 01:14:26,159
actions

1955
01:14:26,159 --> 01:14:29,640
and then the variational autoencoder is

1956
01:14:29,640 --> 01:14:33,179
already basically the right anything

1957
01:14:33,179 --> 01:14:35,219
like that so

1958
01:14:35,219 --> 01:14:37,739
really appreciate it all right thank you

1959
01:14:37,739 --> 01:14:39,480
until next time

1960
01:14:39,480 --> 01:14:43,339
thanks so much bye


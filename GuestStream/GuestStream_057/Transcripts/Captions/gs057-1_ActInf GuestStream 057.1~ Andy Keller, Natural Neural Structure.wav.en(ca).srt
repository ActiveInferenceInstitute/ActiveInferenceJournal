1
00:00:04,050 --> 00:00:07,702
Hello and welcome.

2
00:00:07,836 --> 00:00:09,606
It's September 18, 2023,

3
00:00:09,628 --> 00:00:14,166
and it's active. Guest stream 57.1

4
00:00:14,268 --> 00:00:16,758
with Andy Keller. We're going to be

5
00:00:16,844 --> 00:00:19,570
talking about natural neural structure

6
00:00:19,650 --> 00:00:21,846
for artificial intelligence. There will

7
00:00:21,868 --> 00:00:24,134
be a presentation followed by a

8
00:00:24,172 --> 00:00:25,862
discussion. So if you're watching live,

9
00:00:25,916 --> 00:00:27,718
please feel free to write questions in

10
00:00:27,724 --> 00:00:30,046
the live chat. Otherwise, thank you

11
00:00:30,148 --> 00:00:31,774
Andy, for this. Really looking forward

12
00:00:31,812 --> 00:00:34,190
to it and to you for the presentation.

13
00:00:36,370 --> 00:00:38,814
Yeah, thanks so much. Thanks for having

14
00:00:38,852 --> 00:00:41,134
me. I'm super excited to be able to

15
00:00:41,172 --> 00:00:43,054
present this stuff with Active Inference

16
00:00:43,102 --> 00:00:45,458
group. I'm a fan and very interested,

17
00:00:45,544 --> 00:00:49,026
so. Hopefully. Get to

18
00:00:49,048 --> 00:00:50,258
have a good discussion and see what you

19
00:00:50,264 --> 00:00:53,470
guys think about it. So my name is Andy.

20
00:00:53,550 --> 00:00:56,226
I'm finishing up my PhD, supervised by

21
00:00:56,248 --> 00:00:57,398
Maxwelling at the University of

22
00:00:57,404 --> 00:01:01,122
Amsterdam, starting a postdoc at Harvard

23
00:01:01,186 --> 00:01:04,774
after this. So I'll start out just

24
00:01:04,812 --> 00:01:06,646
talking about the goal of my work in

25
00:01:06,668 --> 00:01:08,802
general is to try to bring modern

26
00:01:08,866 --> 00:01:11,734
artificial intelligence closer to more

27
00:01:11,772 --> 00:01:14,586
human like generalization. And so what

28
00:01:14,608 --> 00:01:16,554
we mean by this is maybe some sort of

29
00:01:16,592 --> 00:01:19,706
structured generalization or maybe more

30
00:01:19,728 --> 00:01:20,818
familiar to the active inference

31
00:01:20,854 --> 00:01:22,782
community, like a structured world model

32
00:01:22,916 --> 00:01:24,974
which we believe that humans have. And

33
00:01:25,012 --> 00:01:27,214
the way that we propose to do this is by

34
00:01:27,252 --> 00:01:29,402
integrating natural neural structure

35
00:01:29,546 --> 00:01:32,914
into artificial intelligence. So first

36
00:01:32,952 --> 00:01:35,246
let's define what we mean by structured

37
00:01:35,278 --> 00:01:37,486
generalization. So I think it's fairly

38
00:01:37,518 --> 00:01:39,486
uncontroversial to say that modern

39
00:01:39,518 --> 00:01:41,874
machine learning generalizes beyond its

40
00:01:41,912 --> 00:01:43,842
training sets in the traditional sense.

41
00:01:43,976 --> 00:01:45,794
So for example, even the earliest

42
00:01:45,842 --> 00:01:48,018
artificial neural networks, multilayer

43
00:01:48,034 --> 00:01:49,782
perceptrons, could be trained on data

44
00:01:49,836 --> 00:01:52,946
sets of images like this and achieve

45
00:01:52,978 --> 00:01:55,186
high accuracy. Then, when they're

46
00:01:55,218 --> 00:01:57,046
presented with a held out test set of

47
00:01:57,068 --> 00:01:58,610
images that they've never seen before,

48
00:01:58,700 --> 00:02:00,774
they can still classify them relatively

49
00:02:00,822 --> 00:02:02,950
easily with the same level of accuracy.

50
00:02:03,030 --> 00:02:04,554
And this is what we typically call

51
00:02:04,592 --> 00:02:07,206
generalization. However, even fairly

52
00:02:07,238 --> 00:02:08,646
early on it was noticed that these

53
00:02:08,688 --> 00:02:10,702
systems really struggle with small

54
00:02:10,756 --> 00:02:12,558
shifts or deformations applied to the

55
00:02:12,564 --> 00:02:19,966
images. For example, if so,

56
00:02:20,148 --> 00:02:23,018
you think, why is this surprising? And I

57
00:02:23,044 --> 00:02:24,638
argue it's really due to our innate

58
00:02:24,654 --> 00:02:26,674
ability to perform this type of

59
00:02:26,792 --> 00:02:28,514
structured generalization that this

60
00:02:28,552 --> 00:02:32,018
example is a failure of. So for example,

61
00:02:32,104 --> 00:02:34,338
this shift is nearly imperceptible to us

62
00:02:34,424 --> 00:02:36,434
and we handle it automatically, whereas

63
00:02:36,482 --> 00:02:38,406
in these systems it's very clearly a

64
00:02:38,428 --> 00:02:40,326
major problem. So in words, we can say

65
00:02:40,348 --> 00:02:42,486
that structure generalization is a

66
00:02:42,508 --> 00:02:44,674
generalization to some symmetry

67
00:02:44,722 --> 00:02:46,886
transformations of the input, or in this

68
00:02:46,908 --> 00:02:48,770
case, the symmetry transformation is a

69
00:02:48,780 --> 00:02:50,570
small shift that leaves the digit class

70
00:02:50,640 --> 00:02:53,514
unchanged. So the obvious question then

71
00:02:53,552 --> 00:02:55,914
is what precisely do we mean by this

72
00:02:55,952 --> 00:02:58,266
natural structure and why do we think

73
00:02:58,288 --> 00:02:59,854
that. This would help us with these

74
00:02:59,892 --> 00:03:02,990
settings? So first, let's talk about

75
00:03:03,060 --> 00:03:04,346
what we mean by natural neural

76
00:03:04,378 --> 00:03:07,134
structure. One way to talk about

77
00:03:07,252 --> 00:03:10,718
structure or any type of bias in a

78
00:03:10,724 --> 00:03:13,130
system is an inductive bias. And so an

79
00:03:13,140 --> 00:03:15,234
inductive bias can loosely be defined as

80
00:03:15,272 --> 00:03:17,266
an apriori restriction of a set of

81
00:03:17,288 --> 00:03:19,154
realizable hypotheses. When you're doing

82
00:03:19,192 --> 00:03:21,986
model selection. More colloquially, we

83
00:03:22,008 --> 00:03:23,790
can call this something like before

84
00:03:23,880 --> 00:03:26,118
seeing any data, it's a restriction of

85
00:03:26,204 --> 00:03:28,422
what and how you can learn. So very

86
00:03:28,476 --> 00:03:30,790
broadly, this can include anything from

87
00:03:30,940 --> 00:03:33,762
model class to optimization procedures

88
00:03:33,826 --> 00:03:36,666
or even hyperparameters. And in some

89
00:03:36,688 --> 00:03:40,586
sense they really define what

90
00:03:40,608 --> 00:03:42,966
is possible to learn. And it defines

91
00:03:42,998 --> 00:03:45,530
generalization in that you actually

92
00:03:45,600 --> 00:03:47,514
can't generalize beyond a training set

93
00:03:47,552 --> 00:03:49,158
without having some inductive biases.

94
00:03:49,254 --> 00:03:50,694
And this is explained more thoroughly

95
00:03:50,742 --> 00:03:54,606
in. This paper by David Wolford. So what

96
00:03:54,628 --> 00:03:56,894
we mean by natural inductive biases then

97
00:03:57,012 --> 00:03:59,086
is biases that stem from the

98
00:03:59,108 --> 00:04:00,814
restrictions and limitations that are

99
00:04:00,852 --> 00:04:03,798
faced by natural systems, by the nature

100
00:04:03,834 --> 00:04:05,506
of having to live in the real world.

101
00:04:05,608 --> 00:04:07,394
For example, the brain has many

102
00:04:07,432 --> 00:04:09,054
efficiency constraints and physical

103
00:04:09,102 --> 00:04:10,434
constraints by nature of its

104
00:04:10,472 --> 00:04:13,326
construction. And following this logic,

105
00:04:13,358 --> 00:04:14,486
then these constraints are really

106
00:04:14,508 --> 00:04:16,530
playing some role in our generalization

107
00:04:16,610 --> 00:04:19,746
abilities which currently exceed modern

108
00:04:19,778 --> 00:04:21,206
artificial intelligence. As we'll go

109
00:04:21,228 --> 00:04:24,134
into next. So in this talk, I'll be

110
00:04:24,172 --> 00:04:26,774
focusing specifically on two types of

111
00:04:26,892 --> 00:04:29,078
structure which my work has studied.

112
00:04:29,254 --> 00:04:31,514
These are topographic organization and

113
00:04:31,552 --> 00:04:34,186
spatiotemporal dynamics. And before I go

114
00:04:34,208 --> 00:04:35,978
into my work, I'll give a short example

115
00:04:36,064 --> 00:04:38,770
for why I believe that natural structure

116
00:04:38,870 --> 00:04:41,226
may be useful to achieve the structure

117
00:04:41,258 --> 00:04:42,734
generalization that. I was talking about

118
00:04:42,772 --> 00:04:46,560
before. So the first example comes from

119
00:04:47,170 --> 00:04:49,642
Fukushima's neocognitron architecture

120
00:04:49,706 --> 00:04:53,134
from the 1980s, which was actually built

121
00:04:53,172 --> 00:04:55,214
to directly address the problem of

122
00:04:55,252 --> 00:04:57,118
robustness to these small shifts in

123
00:04:57,124 --> 00:04:59,806
deformation. So in the paper he writes

124
00:04:59,838 --> 00:05:01,858
about inspiration from pupil and

125
00:05:01,864 --> 00:05:04,114
weasel's measurements of hierarchy and

126
00:05:04,152 --> 00:05:06,606
pooling in order to achieve robustness

127
00:05:06,638 --> 00:05:08,566
to these distortions. And so if you look

128
00:05:08,588 --> 00:05:10,966
at the figure he writes u sub s one, U

129
00:05:10,988 --> 00:05:13,894
sub C one and these stand for simple and

130
00:05:13,932 --> 00:05:16,658
complex cells. And so this is a fairly

131
00:05:16,674 --> 00:05:18,666
radical approach at the time, but it

132
00:05:18,688 --> 00:05:20,554
really served to improve robustness and

133
00:05:20,592 --> 00:05:22,186
shifts that were plaguing these early

134
00:05:22,288 --> 00:05:24,074
artificial neural networks. And over

135
00:05:24,112 --> 00:05:25,674
time these ideas were simplified and

136
00:05:25,712 --> 00:05:28,714
abstracted and obviously yielded the

137
00:05:28,752 --> 00:05:30,218
convolutional neural networks that we

138
00:05:30,224 --> 00:05:32,206
know today, which ultimately drove the

139
00:05:32,228 --> 00:05:34,250
success of the Deep Learning Revolution.

140
00:05:34,410 --> 00:05:36,558
So this is really an example of a

141
00:05:36,564 --> 00:05:38,794
natural inductive bias which achieved

142
00:05:38,842 --> 00:05:40,894
structured generalization. So for our

143
00:05:40,932 --> 00:05:43,282
research, it's really of utmost interest

144
00:05:43,336 --> 00:05:45,154
to try and understand what makes these

145
00:05:45,192 --> 00:05:48,354
models work so well and see if this

146
00:05:48,392 --> 00:05:50,222
principle can potentially be generalized

147
00:05:50,286 --> 00:05:53,930
to cover more abstract transformations

148
00:05:54,030 --> 00:05:55,190
and symmetries.

149
00:05:57,050 --> 00:06:00,134
So what makes a convolution achieve this

150
00:06:00,172 --> 00:06:02,962
structure generalization? Intuitively,

151
00:06:03,026 --> 00:06:04,886
you can see this is done by applying the

152
00:06:04,908 --> 00:06:07,814
same filter or feature extractor at

153
00:06:07,852 --> 00:06:10,006
various spatial locations. So here we

154
00:06:10,028 --> 00:06:11,994
see a single convolutional filter being

155
00:06:12,032 --> 00:06:14,086
applied at all locations of an image.

156
00:06:14,198 --> 00:06:15,546
This means that no matter where your

157
00:06:15,568 --> 00:06:17,466
input is, whether it's kind of in the

158
00:06:17,488 --> 00:06:19,674
middle of the image or on the right,

159
00:06:19,792 --> 00:06:21,514
you'll have the exact same features,

160
00:06:21,562 --> 00:06:22,926
with one exception, they'll be

161
00:06:22,948 --> 00:06:25,706
equivalently shifted. So mathematically,

162
00:06:25,738 --> 00:06:27,038
this type of a mapping is called a

163
00:06:27,044 --> 00:06:29,754
homomorphism. It preserves the algebraic

164
00:06:29,802 --> 00:06:31,326
structure of the input space and the

165
00:06:31,348 --> 00:06:33,406
output space. In this case, it's with

166
00:06:33,428 --> 00:06:36,402
respect to translation. And at a simple

167
00:06:36,456 --> 00:06:37,778
level, something like that will be

168
00:06:37,784 --> 00:06:39,106
important to remember for the rest of

169
00:06:39,128 --> 00:06:40,494
this talk is that we can verify

170
00:06:40,542 --> 00:06:42,930
homomorphisms of our feature extractor

171
00:06:43,510 --> 00:06:45,414
if we can see that there's this

172
00:06:45,532 --> 00:06:47,394
commutation with the transformation,

173
00:06:47,442 --> 00:06:50,166
this commutative diagram. And so we can

174
00:06:50,188 --> 00:06:52,338
write this also algebraically by showing

175
00:06:52,354 --> 00:06:54,114
that the feature extractor f commutes

176
00:06:54,162 --> 00:06:57,254
with the transformation operator T. And

177
00:06:57,292 --> 00:06:59,114
basically what we want is there to be no

178
00:06:59,152 --> 00:07:01,226
difference between first extracting the

179
00:07:01,248 --> 00:07:03,066
features and then performing the

180
00:07:03,088 --> 00:07:05,226
transformation, or performing the

181
00:07:05,248 --> 00:07:06,778
transformation and then extracting the

182
00:07:06,784 --> 00:07:09,994
features. So the challenges to date is

183
00:07:10,192 --> 00:07:11,674
we don't really know how to construct

184
00:07:11,722 --> 00:07:13,406
homomorphisms with respect to more

185
00:07:13,428 --> 00:07:15,118
complex transformations that we see in

186
00:07:15,124 --> 00:07:17,738
the real world. For example, our brain

187
00:07:17,754 --> 00:07:19,642
is able to handle changes in lighting

188
00:07:19,706 --> 00:07:23,326
and season naturally. So here

189
00:07:23,348 --> 00:07:24,974
we see lighting on a person's face or

190
00:07:25,012 --> 00:07:26,310
the change of seasons. We can tell it's

191
00:07:26,330 --> 00:07:29,138
the same face or the same road. But we

192
00:07:29,144 --> 00:07:30,546
don't know how to build models which

193
00:07:30,568 --> 00:07:32,386
respect these transformations. And so it

194
00:07:32,408 --> 00:07:34,162
makes us hard to build systems which

195
00:07:34,216 --> 00:07:36,434
handle them in a robust and predictable

196
00:07:36,482 --> 00:07:39,122
way. To give an even more abstract

197
00:07:39,186 --> 00:07:41,206
example of what I mean by this and the

198
00:07:41,228 --> 00:07:42,886
potential negative repercussions of

199
00:07:42,908 --> 00:07:44,434
models which don't handle symmetry

200
00:07:44,482 --> 00:07:47,206
transformations, consider modern text to

201
00:07:47,228 --> 00:07:49,526
image generation programs. So in this

202
00:07:49,548 --> 00:07:52,590
example, I asked Dolly Two to generate

203
00:07:52,770 --> 00:07:54,986
image of a teddy bear on the Moon. And

204
00:07:55,008 --> 00:07:56,554
it does this incredibly well, right?

205
00:07:56,592 --> 00:07:58,714
Probably better than I could. It has

206
00:07:58,752 --> 00:08:02,198
texture fur, incredibly detailed.

207
00:08:02,294 --> 00:08:04,062
However, if I ask it to do something

208
00:08:04,196 --> 00:08:06,442
which I see as conceptually simpler,

209
00:08:06,506 --> 00:08:08,974
such as draw a blue cube on top of a red

210
00:08:09,012 --> 00:08:11,774
cube, it fails to do this. And to me

211
00:08:11,812 --> 00:08:14,162
this seems unintuitive since the second

212
00:08:14,216 --> 00:08:16,850
task seems significantly easier.

213
00:08:17,270 --> 00:08:19,122
But what I'm arguing is that the reason

214
00:08:19,176 --> 00:08:21,026
that this is surprising is precisely the

215
00:08:21,048 --> 00:08:22,814
same reason that the MNS translation

216
00:08:22,862 --> 00:08:24,866
example was surprising. There is this

217
00:08:24,888 --> 00:08:27,014
symmetry transformation happening here,

218
00:08:27,132 --> 00:08:28,854
namely the transformation between these

219
00:08:28,892 --> 00:08:30,678
complex objects of a teddy bear and the

220
00:08:30,684 --> 00:08:33,506
Moon and these simple objects of cubes

221
00:08:33,618 --> 00:08:35,906
which we intuitively expect the network

222
00:08:35,938 --> 00:08:38,454
to be able to handle and respect. And we

223
00:08:38,492 --> 00:08:40,694
see that it doesn't. So just like how

224
00:08:40,732 --> 00:08:43,162
Fukushima's work showed that these

225
00:08:43,296 --> 00:08:46,026
natural structure of hierarchy and

226
00:08:46,048 --> 00:08:47,594
pooling of our visual system are

227
00:08:47,632 --> 00:08:49,646
effective for making generalizations to

228
00:08:49,748 --> 00:08:52,254
small transformations, I argue that

229
00:08:52,292 --> 00:08:53,886
potentially higher level structure may

230
00:08:53,908 --> 00:08:55,738
be necessary to fix these abstract

231
00:08:55,834 --> 00:08:59,454
generalization problems. And so the

232
00:08:59,492 --> 00:09:02,146
question then that I'm studying and that

233
00:09:02,168 --> 00:09:05,054
I'm asking is what might this structure

234
00:09:05,102 --> 00:09:07,522
be and how do we implement this in an

235
00:09:07,656 --> 00:09:09,486
artificial neural network architecture

236
00:09:09,518 --> 00:09:11,486
that can actually be used for performing

237
00:09:11,518 --> 00:09:12,370
computation?

238
00:09:14,810 --> 00:09:17,506
So, to begin to answer that, I'll jump

239
00:09:17,538 --> 00:09:19,494
into my first line of work on

240
00:09:19,532 --> 00:09:23,266
topographic organization. So topographic

241
00:09:23,298 --> 00:09:25,426
organization is observed widely

242
00:09:25,458 --> 00:09:27,266
throughout the brain from primary visual

243
00:09:27,298 --> 00:09:29,846
cortex to higher level areas. And it can

244
00:09:29,868 --> 00:09:31,306
very loosely be described as this

245
00:09:31,328 --> 00:09:33,418
property that neurons which are close to

246
00:09:33,424 --> 00:09:35,610
one another tend to respond to similar

247
00:09:35,680 --> 00:09:38,074
things. For example, on the left we show

248
00:09:38,112 --> 00:09:39,594
the color coded preference of each

249
00:09:39,632 --> 00:09:41,306
neuron in the Macaque primary visual

250
00:09:41,338 --> 00:09:44,570
cortex as a response to oriented lines

251
00:09:44,650 --> 00:09:46,606
and we see this smoothly varying set of

252
00:09:46,628 --> 00:09:48,606
selectivities. Another type of

253
00:09:48,628 --> 00:09:50,426
organization is known as retinotopic

254
00:09:50,458 --> 00:09:52,318
organization where nearby neurons in the

255
00:09:52,324 --> 00:09:54,414
visual cortex tend to respond to nearby

256
00:09:54,462 --> 00:09:57,074
receptive fields. However, this

257
00:09:57,112 --> 00:09:58,914
organization isn't limited to these low

258
00:09:58,952 --> 00:10:01,026
level features. It extends to more

259
00:10:01,048 --> 00:10:02,738
complex features such as those present

260
00:10:02,824 --> 00:10:06,454
in faces or objects or places. And this

261
00:10:06,492 --> 00:10:08,402
relates to the so called functionally

262
00:10:08,466 --> 00:10:10,486
specific areas of the brain such as the

263
00:10:10,508 --> 00:10:12,726
fusiform face area FFA and the

264
00:10:12,748 --> 00:10:15,030
perihepocampal place area PPA.

265
00:10:15,370 --> 00:10:19,066
So in this work, the main idea again is

266
00:10:19,088 --> 00:10:21,174
that perhaps this topographic

267
00:10:21,222 --> 00:10:23,434
organization in some sense which is

268
00:10:23,472 --> 00:10:25,094
intimately related to the convolution

269
00:10:25,142 --> 00:10:28,010
operation and Fukushima's architecture

270
00:10:28,430 --> 00:10:30,526
we can maybe generalize the benefits of

271
00:10:30,548 --> 00:10:33,166
this to more abstract transformations in

272
00:10:33,188 --> 00:10:34,766
other words, learn how to build more

273
00:10:34,788 --> 00:10:37,934
complex homomorphisms that we can't do

274
00:10:37,972 --> 00:10:41,406
analytically right now. So just

275
00:10:41,428 --> 00:10:43,054
to show that we're not completely insane

276
00:10:43,102 --> 00:10:45,554
with this idea there is some prior work

277
00:10:45,592 --> 00:10:48,398
in this domain from people such as Honin

278
00:10:48,494 --> 00:10:52,162
Yanlaka Apohevarnan in the early ninety

279
00:10:52,216 --> 00:10:54,846
s and two thousand s and they studied

280
00:10:54,878 --> 00:10:56,566
how topographic organization may be

281
00:10:56,588 --> 00:10:59,362
useful for learning invariances mostly

282
00:10:59,426 --> 00:11:02,134
in linear models. So the question for us

283
00:11:02,172 --> 00:11:03,766
when we entered this space is what is

284
00:11:03,788 --> 00:11:06,114
the most scalable abstract mechanism

285
00:11:06,162 --> 00:11:07,494
that can be leveraged from these

286
00:11:07,532 --> 00:11:09,434
approaches which we can integrate into

287
00:11:09,472 --> 00:11:10,886
modern deep neural network

288
00:11:10,918 --> 00:11:13,174
architectures? And ultimately we settled

289
00:11:13,222 --> 00:11:15,466
on a generative modeling approach which

290
00:11:15,488 --> 00:11:16,826
I think might be interesting to the

291
00:11:16,848 --> 00:11:19,862
people in this community which then

292
00:11:19,936 --> 00:11:22,206
allows us to relate it more closely to

293
00:11:22,228 --> 00:11:23,802
topographic independent component

294
00:11:23,866 --> 00:11:26,638
analysis with the basic idea being that

295
00:11:26,724 --> 00:11:28,830
we can learn a topographic feature space

296
00:11:28,900 --> 00:11:30,874
by imposing a topographic prior

297
00:11:30,922 --> 00:11:33,730
distribution over our latent variables.

298
00:11:34,390 --> 00:11:36,866
So just to give a brief background I

299
00:11:36,888 --> 00:11:38,926
assume most people are already familiar

300
00:11:38,958 --> 00:11:41,730
with this but the kind of general

301
00:11:41,800 --> 00:11:43,138
assumption is that the brain is a

302
00:11:43,144 --> 00:11:44,994
generative model and this idea in some

303
00:11:45,032 --> 00:11:47,042
sense can be attributed to Helmholtz

304
00:11:47,106 --> 00:11:49,846
from the 19th century where he said that

305
00:11:49,948 --> 00:11:51,558
what we see is the solution to a

306
00:11:51,564 --> 00:11:53,474
computational problem. Our brains

307
00:11:53,522 --> 00:11:55,446
compute the most likely causes from

308
00:11:55,468 --> 00:11:58,134
photon absorptions within our eyes. And

309
00:11:58,172 --> 00:12:00,394
so as an example, if I show you this

310
00:12:00,432 --> 00:12:02,698
image you immediately recognize it as a

311
00:12:02,704 --> 00:12:05,146
globe with some curvature. However, it

312
00:12:05,168 --> 00:12:06,986
could just as equally be a disk with a

313
00:12:07,008 --> 00:12:09,178
distorted perspective on it. So this is

314
00:12:09,184 --> 00:12:11,754
how we get optical illusions or our

315
00:12:11,792 --> 00:12:13,994
images. So like this one, your brain

316
00:12:14,042 --> 00:12:16,574
infers that there is a cube here because

317
00:12:16,612 --> 00:12:18,062
of the structure but really it's just.

318
00:12:18,116 --> 00:12:21,054
A flat piece of paper. So you can think

319
00:12:21,092 --> 00:12:23,838
of this generative model aspect as kind

320
00:12:23,844 --> 00:12:26,466
of like an inverse graphics program. In

321
00:12:26,488 --> 00:12:28,226
the program, the abstract properties of

322
00:12:28,248 --> 00:12:30,434
the sphere are known the position, the

323
00:12:30,472 --> 00:12:32,706
size, the lighting and these are used to

324
00:12:32,728 --> 00:12:34,514
project the sphere to create the 2D

325
00:12:34,552 --> 00:12:37,062
image that is rendered. So in effect,

326
00:12:37,116 --> 00:12:39,494
what Helmholtz and others are saying is

327
00:12:39,532 --> 00:12:41,814
that as a generative model, the brain is

328
00:12:41,852 --> 00:12:43,494
actually trying to invert this

329
00:12:43,532 --> 00:12:45,874
generative process and doing inference

330
00:12:45,922 --> 00:12:48,226
and infer the underlying causes of our

331
00:12:48,268 --> 00:12:50,858
sensations. So the reason I'm kind of

332
00:12:50,864 --> 00:12:52,698
belaboring this point is that there's a

333
00:12:52,704 --> 00:12:54,620
lot of talk of generative models today,

334
00:12:55,310 --> 00:12:57,162
and I'm not necessarily just talking

335
00:12:57,216 --> 00:12:59,534
about generating images or pretty

336
00:12:59,572 --> 00:13:03,966
pictures. I really want to mean a

337
00:13:03,988 --> 00:13:06,080
framework for unsupervised learning.

338
00:13:07,890 --> 00:13:10,014
So then to get a little bit more into

339
00:13:10,052 --> 00:13:11,634
the details, what do I mean by a

340
00:13:11,672 --> 00:13:14,206
topographic prior? So generative models

341
00:13:14,238 --> 00:13:15,886
are typically described as a joint

342
00:13:15,918 --> 00:13:18,594
distribution over observations, x and

343
00:13:18,632 --> 00:13:20,740
latent variables, which we'll call Z.

344
00:13:21,670 --> 00:13:23,746
And this is typically factorized, or one

345
00:13:23,768 --> 00:13:25,426
way that this is done is factorized in

346
00:13:25,448 --> 00:13:28,278
terms of a prior p of Z. And this true

347
00:13:28,364 --> 00:13:30,306
generative model conditional generative

348
00:13:30,338 --> 00:13:33,126
model p of X given Z. And so one way

349
00:13:33,148 --> 00:13:34,806
that we can think about this is that the

350
00:13:34,828 --> 00:13:36,962
prior can be seen to encode relative

351
00:13:37,026 --> 00:13:38,746
penalties for each type of code that is

352
00:13:38,768 --> 00:13:41,174
produced. When we invert our generative

353
00:13:41,222 --> 00:13:42,746
model, this is called computing the

354
00:13:42,768 --> 00:13:45,180
posterior e of Z given X.

355
00:13:46,270 --> 00:13:48,918
And so to develop a topographic latent

356
00:13:48,934 --> 00:13:50,526
space, we want to introduce some sort of

357
00:13:50,548 --> 00:13:53,854
a topographic prior, which this

358
00:13:53,892 --> 00:13:55,646
topographic ICA work showed is

359
00:13:55,668 --> 00:13:57,582
equivalent to something like a group

360
00:13:57,636 --> 00:14:00,526
sparsity penalty. So people might be

361
00:14:00,548 --> 00:14:02,314
familiar with typical sparsity penalties

362
00:14:02,362 --> 00:14:03,866
from independent component analysis.

363
00:14:03,898 --> 00:14:05,766
You want your activations to be sparse,

364
00:14:05,818 --> 00:14:08,866
meaning many of them are zero. So that

365
00:14:08,888 --> 00:14:10,098
could look something like this. You have

366
00:14:10,104 --> 00:14:11,646
a bunch of blue squares that are active,

367
00:14:11,678 --> 00:14:13,842
but most of them are not active. But

368
00:14:13,896 --> 00:14:15,394
specifically, with a group sparsity

369
00:14:15,442 --> 00:14:17,954
penalty, we want these priors to assign

370
00:14:18,002 --> 00:14:20,626
lower probability to these distributed

371
00:14:20,658 --> 00:14:22,422
sparse activations and higher

372
00:14:22,476 --> 00:14:25,426
probability to these grouped, densely

373
00:14:25,458 --> 00:14:27,014
packed representations. You can also

374
00:14:27,052 --> 00:14:29,114
think of this like a higher penalty when

375
00:14:29,152 --> 00:14:31,094
things are spread out, a lower penalty

376
00:14:31,142 --> 00:14:33,930
when things are closer together. So

377
00:14:34,000 --> 00:14:37,206
again, this can be written abstractly

378
00:14:37,238 --> 00:14:39,740
like this. But I want to make clear that

379
00:14:40,270 --> 00:14:41,754
each one of these squares here

380
00:14:41,792 --> 00:14:43,246
represents kind of a neuron in our

381
00:14:43,268 --> 00:14:45,294
model. And they're organized in this 2D

382
00:14:45,332 --> 00:14:47,022
grid. So when we're talking about

383
00:14:47,076 --> 00:14:48,686
grouping, we really mean grouping in

384
00:14:48,708 --> 00:14:52,234
that 2D topology. So one thing that's

385
00:14:52,282 --> 00:14:54,100
really interesting and kind of important

386
00:14:54,710 --> 00:14:57,026
is that these priors don't just give us

387
00:14:57,048 --> 00:14:58,766
topographic organization, but they've

388
00:14:58,798 --> 00:15:01,906
also been noted by people or studied by

389
00:15:01,928 --> 00:15:03,838
people like Erosimicelli and Bruno

390
00:15:03,854 --> 00:15:07,154
Olshausen to actually fit the statistics

391
00:15:07,202 --> 00:15:09,794
of natural data better, specifically

392
00:15:09,842 --> 00:15:13,254
natural images. They've shown that using

393
00:15:13,292 --> 00:15:15,014
this type of a prior, you actually get a

394
00:15:15,052 --> 00:15:17,366
sparser set of activations, meaning that

395
00:15:17,388 --> 00:15:19,494
the prior fits the true generative

396
00:15:19,542 --> 00:15:21,386
process a little bit better. And as

397
00:15:21,408 --> 00:15:22,886
we're aware, the brain has a high degree

398
00:15:22,918 --> 00:15:25,226
of sparsity and this is believed to be

399
00:15:25,248 --> 00:15:27,130
very relevant for efficiency.

400
00:15:28,670 --> 00:15:30,638
So to get a little bit more into the

401
00:15:30,644 --> 00:15:32,478
details, to implement this type of a

402
00:15:32,484 --> 00:15:34,366
group sparse prior, we use a

403
00:15:34,388 --> 00:15:36,126
hierarchical generative model and this

404
00:15:36,148 --> 00:15:38,878
is basically introduced by some of the

405
00:15:38,884 --> 00:15:41,950
topographic ICA work. The idea

406
00:15:42,020 --> 00:15:43,758
is that you have a higher level latent

407
00:15:43,774 --> 00:15:45,694
variable U which simultaneously

408
00:15:45,742 --> 00:15:48,322
regulates the variance of multiple lower

409
00:15:48,376 --> 00:15:50,578
level variables T. And this is how we

410
00:15:50,584 --> 00:15:53,282
get group sparsity. Then to get

411
00:15:53,336 --> 00:15:55,554
topographic organization, you can have

412
00:15:55,592 --> 00:15:57,374
multiple of these latent variables U

413
00:15:57,432 --> 00:15:59,618
slightly overlapping with their fields

414
00:15:59,634 --> 00:16:02,978
of influence. So their neighborhoods,

415
00:16:02,994 --> 00:16:05,046
we can call them, and this will give you

416
00:16:05,068 --> 00:16:07,186
this smooth correlation structure you're

417
00:16:07,218 --> 00:16:09,398
after. So get the intuition for this.

418
00:16:09,484 --> 00:16:11,706
You see that this variable T over down

419
00:16:11,728 --> 00:16:13,786
on the bottom here is not getting any

420
00:16:13,808 --> 00:16:16,442
input from this U on the top, but it is

421
00:16:16,496 --> 00:16:18,858
sharing a U variable with this T in the

422
00:16:18,864 --> 00:16:20,602
middle. So it's like they're sharing

423
00:16:20,666 --> 00:16:22,174
variance, they're sharing some

424
00:16:22,212 --> 00:16:24,014
components with their neighbors, but not

425
00:16:24,052 --> 00:16:25,646
all components. And that's really due to

426
00:16:25,668 --> 00:16:27,742
this local connectivity of these higher

427
00:16:27,796 --> 00:16:31,326
level variables U. So to

428
00:16:31,348 --> 00:16:33,006
keep it simple about how we use this

429
00:16:33,028 --> 00:16:34,866
generative model, let's go back to a

430
00:16:34,888 --> 00:16:37,218
single U variable. And the challenge in

431
00:16:37,224 --> 00:16:38,754
this type of an architecture which made

432
00:16:38,792 --> 00:16:42,114
it difficult for many years is how

433
00:16:42,152 --> 00:16:44,802
do you infer the approximate posterior

434
00:16:44,946 --> 00:16:47,186
over these intermediate latent variables

435
00:16:47,218 --> 00:16:49,894
in this hierarchical architecture? This

436
00:16:49,932 --> 00:16:52,434
is not super straightforward. So prior

437
00:16:52,482 --> 00:16:54,294
works have used heuristics developed for

438
00:16:54,332 --> 00:16:56,586
linear models. And in our work we found

439
00:16:56,608 --> 00:16:58,566
that this really didn't extend to modern

440
00:16:58,598 --> 00:17:01,082
neural network architectures. So really

441
00:17:01,136 --> 00:17:02,746
our insight is to leverage a

442
00:17:02,768 --> 00:17:04,362
factorization, a specific

443
00:17:04,496 --> 00:17:07,130
reprometerization of this distribution.

444
00:17:07,870 --> 00:17:10,214
And so this reprimmatrization

445
00:17:10,262 --> 00:17:12,238
specifically is achieved by defining the

446
00:17:12,244 --> 00:17:14,394
prior to be what's known as a Gaussian

447
00:17:14,442 --> 00:17:16,174
scale mixture, meaning that our

448
00:17:16,212 --> 00:17:19,006
conditional distribution of T given U is

449
00:17:19,028 --> 00:17:21,458
actually a normal distribution where the

450
00:17:21,464 --> 00:17:24,866
variance is defined by this variable U.

451
00:17:24,968 --> 00:17:27,586
And for certain choices of U, this

452
00:17:27,608 --> 00:17:29,234
distribution is indeed sparse and

453
00:17:29,272 --> 00:17:31,966
encompasses a range of distributions

454
00:17:31,998 --> 00:17:33,714
such as Laplacians and student T

455
00:17:33,752 --> 00:17:36,290
distributions. One way of defining it is

456
00:17:36,360 --> 00:17:38,706
a Gaussian scale mixture admits a

457
00:17:38,728 --> 00:17:41,414
particular repromaturization in terms of

458
00:17:41,452 --> 00:17:43,334
independent gaussian random variables Z

459
00:17:43,372 --> 00:17:46,582
and U. So specifically, then we see that

460
00:17:46,636 --> 00:17:49,046
this T variable, which was originally

461
00:17:49,078 --> 00:17:51,418
fairly complex, is actually just a

462
00:17:51,424 --> 00:17:52,966
product of a bunch of gaussian random

463
00:17:52,998 --> 00:17:54,874
variables which we now know how to work

464
00:17:54,912 --> 00:17:58,054
with much more efficiently in generative

465
00:17:58,102 --> 00:18:00,038
models. And specifically what we're

466
00:18:00,054 --> 00:18:02,062
going to do is so that we can actually

467
00:18:02,116 --> 00:18:04,926
get approximate posteriors for U and Z

468
00:18:05,028 --> 00:18:07,226
separately and then do a deterministic

469
00:18:07,258 --> 00:18:09,018
combination of them in order to compute

470
00:18:09,034 --> 00:18:11,166
our topographic. Variable T. And this is

471
00:18:11,188 --> 00:18:13,966
much easier to do. So, without going

472
00:18:13,988 --> 00:18:16,766
into too many details, the method that

473
00:18:16,788 --> 00:18:18,098
we decide to use is what's known as a

474
00:18:18,104 --> 00:18:19,634
variational auto encoder, which

475
00:18:19,672 --> 00:18:21,166
leverages techniques from variational

476
00:18:21,198 --> 00:18:23,778
inference to derive a lower bound on the

477
00:18:23,784 --> 00:18:26,094
likelihood, allowing us to parameterize

478
00:18:26,142 --> 00:18:27,894
these approximate posteriors with

479
00:18:27,932 --> 00:18:29,986
powerful nonlinear deep neural networks

480
00:18:30,018 --> 00:18:32,066
and optimize them with gradient descent.

481
00:18:32,258 --> 00:18:33,686
This is going to be familiar to the

482
00:18:33,708 --> 00:18:35,734
active inference community. But really

483
00:18:35,772 --> 00:18:37,686
what we've done is instead of having a

484
00:18:37,708 --> 00:18:39,434
single encoder and decoder. As is

485
00:18:39,472 --> 00:18:42,294
typical Bayes, we now have two encoders,

486
00:18:42,342 --> 00:18:44,986
one for U and one for Z separately. And

487
00:18:45,008 --> 00:18:46,154
then we combine them in this

488
00:18:46,192 --> 00:18:48,662
deterministic manner to construct our

489
00:18:48,736 --> 00:18:51,694
topographic T variable. If you see that

490
00:18:51,732 --> 00:18:54,046
this is actually the construction of a

491
00:18:54,068 --> 00:18:57,150
student's T distribution from Gaussians

492
00:18:57,970 --> 00:19:01,790
and then we do this before decoding

493
00:19:02,370 --> 00:19:04,178
and then maximize the likelihood. Of the

494
00:19:04,184 --> 00:19:06,446
data altogether. So this is the elbow,

495
00:19:06,478 --> 00:19:08,498
the evidence lower bound abound on the

496
00:19:08,504 --> 00:19:10,786
likelihood of the data and is actually

497
00:19:10,968 --> 00:19:13,314
very similar to the variational free

498
00:19:13,352 --> 00:19:15,826
energy that is. Used in the active

499
00:19:15,858 --> 00:19:19,446
inference community. So with

500
00:19:19,468 --> 00:19:21,138
these details out of the way, what's

501
00:19:21,154 --> 00:19:22,886
really interesting is what happens when

502
00:19:22,908 --> 00:19:25,030
we train this generative model which has

503
00:19:25,100 --> 00:19:27,474
relatively simple group sparsity penalty

504
00:19:27,522 --> 00:19:29,926
in its latent space. And we want to look

505
00:19:29,948 --> 00:19:31,434
at kind of what it's learning in terms

506
00:19:31,472 --> 00:19:34,010
of. Its organization of features. And

507
00:19:34,080 --> 00:19:35,206
first we start with the simplest

508
00:19:35,238 --> 00:19:36,554
possible data set. We have a black

509
00:19:36,592 --> 00:19:38,726
background with white squares at random

510
00:19:38,758 --> 00:19:41,366
XY locations. If we train our auto

511
00:19:41,398 --> 00:19:43,146
encoder with this group sparsity penalty

512
00:19:43,178 --> 00:19:44,906
on it and then we look at the weight

513
00:19:44,938 --> 00:19:47,658
vectors of our decoder which we're

514
00:19:47,674 --> 00:19:49,514
plotting in blue here, again organized

515
00:19:49,562 --> 00:19:53,386
in this 2D grid, we see that indeed

516
00:19:53,418 --> 00:19:55,490
they learn to be organized according to

517
00:19:55,560 --> 00:19:58,226
spatial location. So this can be seen as

518
00:19:58,248 --> 00:20:00,046
similar to convolutional receptive

519
00:20:00,078 --> 00:20:02,034
fields where the receptive field of each

520
00:20:02,072 --> 00:20:05,586
neuron is really given by the kind

521
00:20:05,608 --> 00:20:07,990
of inputs at its location.

522
00:20:08,970 --> 00:20:10,646
And this makes sense intuitively from

523
00:20:10,668 --> 00:20:13,366
the group sparsity perspective since for

524
00:20:13,388 --> 00:20:14,994
any given region, which we highlight,

525
00:20:15,042 --> 00:20:17,250
like in yellow here, the filters in a

526
00:20:17,260 --> 00:20:18,646
given group are much more highly

527
00:20:18,678 --> 00:20:20,214
correlated. They have these overlapping

528
00:20:20,262 --> 00:20:22,326
receptive fields than other random

529
00:20:22,358 --> 00:20:24,106
locations. So essentially we see that

530
00:20:24,128 --> 00:20:26,966
our model is learning to cluster

531
00:20:27,158 --> 00:20:30,298
activities together in sort of a

532
00:20:30,304 --> 00:20:33,086
simulated cortical sheet according to

533
00:20:33,108 --> 00:20:35,566
the correlations in the data set. So

534
00:20:35,668 --> 00:20:37,178
instead of in convolution where you're

535
00:20:37,194 --> 00:20:38,826
actually doing weight tying and you're

536
00:20:38,858 --> 00:20:41,166
manually specifying I want to copy this

537
00:20:41,188 --> 00:20:42,766
weight everywhere, you can maybe think

538
00:20:42,788 --> 00:20:44,394
of this as like approximate weight

539
00:20:44,442 --> 00:20:47,490
tying. And really we're learning this

540
00:20:47,560 --> 00:20:48,946
from the correlation structure of the

541
00:20:48,968 --> 00:20:51,218
data set itself. And just to give a

542
00:20:51,224 --> 00:20:52,942
little bit more of a biological

543
00:20:53,086 --> 00:20:54,946
inspiration for this, we know that

544
00:20:54,968 --> 00:20:57,006
retinotopy is present in the brain.

545
00:20:57,038 --> 00:20:59,606
This is an example of retinotopy and the

546
00:20:59,628 --> 00:21:02,806
Macaque visual cortex. And you can see

547
00:21:02,828 --> 00:21:04,854
if you show the Macaque an image like

548
00:21:04,892 --> 00:21:07,340
this, it gets projected into this

549
00:21:08,430 --> 00:21:11,546
topology preserving space actually on

550
00:21:11,568 --> 00:21:14,522
the surface of the cortex. So the idea

551
00:21:14,576 --> 00:21:16,954
is that topographic organization and

552
00:21:16,992 --> 00:21:19,550
even learn topographic organization is

553
00:21:19,620 --> 00:21:22,254
preserving the input correlations of our

554
00:21:22,292 --> 00:21:23,120
data set.

555
00:21:25,570 --> 00:21:28,602
And potentially this may be beneficial

556
00:21:28,666 --> 00:21:30,686
for generalizing these ideas a little

557
00:21:30,708 --> 00:21:32,146
bit further. So like I said at the

558
00:21:32,168 --> 00:21:34,706
beginning, it would be even better if we

559
00:21:34,728 --> 00:21:37,154
could just learn something more than

560
00:21:37,192 --> 00:21:39,214
just convolution, maybe more complicated

561
00:21:39,262 --> 00:21:43,026
equivalences. So how do we do that?

562
00:21:43,208 --> 00:21:45,706
One thing that's clear in natural

563
00:21:45,758 --> 00:21:48,118
intelligence is that we don't exist in

564
00:21:48,124 --> 00:21:50,646
this world of IID frames, right? We

565
00:21:50,668 --> 00:21:52,850
exist in a world of continuous sequences

566
00:21:53,010 --> 00:21:55,206
of transformations. So maybe we can

567
00:21:55,228 --> 00:21:57,914
extend our model to this setting to

568
00:21:57,952 --> 00:22:00,538
learn, observe transformations. This is

569
00:22:00,544 --> 00:22:02,570
the idea of temporal coherence.

570
00:22:03,230 --> 00:22:05,094
So what would happen if we just simply

571
00:22:05,142 --> 00:22:08,106
extended our previous framework over the

572
00:22:08,128 --> 00:22:10,266
time dimension, right? So instead of

573
00:22:10,288 --> 00:22:12,346
just grouping, saying we want our

574
00:22:12,368 --> 00:22:14,446
neurons to be group sparse in terms of

575
00:22:14,468 --> 00:22:16,926
spatial extent on the cortex, we

576
00:22:16,948 --> 00:22:18,346
actually want them to be group sparse

577
00:22:18,378 --> 00:22:19,966
over time. Meaning that if one set of

578
00:22:19,988 --> 00:22:22,138
neurons is active now, we want that same

579
00:22:22,164 --> 00:22:23,666
set of neurons to be active into. The

580
00:22:23,688 --> 00:22:27,154
future as well. If we kind of

581
00:22:27,192 --> 00:22:29,314
intuitively think about this, we see

582
00:22:29,352 --> 00:22:31,982
that this is actually more encouraging

583
00:22:32,046 --> 00:22:34,066
invariance than equivariance. A way to

584
00:22:34,088 --> 00:22:36,066
understand this is we're saying we want

585
00:22:36,088 --> 00:22:37,554
the same neurons to be active

586
00:22:37,602 --> 00:22:39,666
constantly, but the input transformation

587
00:22:39,698 --> 00:22:41,942
is changing, right? The feet of this

588
00:22:41,996 --> 00:22:44,454
little fox are moving. So if the same

589
00:22:44,492 --> 00:22:46,134
neurons are coding for the same thing

590
00:22:46,172 --> 00:22:47,926
over and over again, but the feet are

591
00:22:47,948 --> 00:22:49,354
moving, those neurons are going to learn

592
00:22:49,392 --> 00:22:51,482
to be invariant to the motion of that

593
00:22:51,536 --> 00:22:54,250
leg. Of this dog, for example. So

594
00:22:54,320 --> 00:22:58,058
instead is that. I went the

595
00:22:58,064 --> 00:22:59,100
wrong way here.

596
00:23:01,090 --> 00:23:04,654
So instead, our insight was that this

597
00:23:04,692 --> 00:23:07,386
group Sarcity could instead be shifted

598
00:23:07,418 --> 00:23:09,326
with respect to time. So this would mean

599
00:23:09,348 --> 00:23:11,326
that sequentially shifted sets of

600
00:23:11,348 --> 00:23:13,086
activations would be encouraged to

601
00:23:13,108 --> 00:23:15,018
activate together, and then our latent

602
00:23:15,034 --> 00:23:16,354
space would really be structured with

603
00:23:16,392 --> 00:23:18,142
respect to the observed transformations.

604
00:23:18,286 --> 00:23:19,826
So you can see here that rather than the

605
00:23:19,848 --> 00:23:21,346
same set of neurons being active at all

606
00:23:21,368 --> 00:23:23,214
time steps, it's really a sequentially

607
00:23:23,262 --> 00:23:24,810
permuted set of neurons that we're

608
00:23:24,830 --> 00:23:27,000
grouping together in this sparse way.

609
00:23:28,010 --> 00:23:30,070
And then this allows us to model

610
00:23:30,220 --> 00:23:33,206
different observations over time, but

611
00:23:33,228 --> 00:23:34,726
they're still connected in terms of

612
00:23:34,748 --> 00:23:36,786
learning a transformation and preserving

613
00:23:36,818 --> 00:23:38,338
this correlation structure of the input

614
00:23:38,354 --> 00:23:41,222
data set. So if we put this together

615
00:23:41,276 --> 00:23:43,638
into our topographic BAE architecture,

616
00:23:43,734 --> 00:23:45,114
you can get something that looks like

617
00:23:45,152 --> 00:23:47,126
this. You see that we have an input

618
00:23:47,158 --> 00:23:49,962
sequence. We're again encoding a Z

619
00:23:50,016 --> 00:23:52,554
variable and then multiple U variables

620
00:23:52,602 --> 00:23:55,054
in the denominator here. And then each

621
00:23:55,092 --> 00:23:58,078
one of these U variables is shifted kind

622
00:23:58,084 --> 00:23:59,694
of like we were showing before. In order

623
00:23:59,732 --> 00:24:02,266
to achieve this shift equivalent.

624
00:24:02,298 --> 00:24:04,066
Structure that we're looking for. When

625
00:24:04,088 --> 00:24:06,338
we combine these in this student T

626
00:24:06,504 --> 00:24:08,722
product distribution, we get a single

627
00:24:08,776 --> 00:24:09,986
latent variable. This is now our

628
00:24:10,008 --> 00:24:12,722
topographic latent variable T. And now

629
00:24:12,776 --> 00:24:15,186
that we have this known structure in our

630
00:24:15,208 --> 00:24:16,806
latent space, you can think of it like a

631
00:24:16,828 --> 00:24:19,014
structured world model. We know how to

632
00:24:19,052 --> 00:24:20,806
transform this latent space. In this

633
00:24:20,828 --> 00:24:22,246
case, it's by permuting these

634
00:24:22,268 --> 00:24:24,374
activations around these circles, doing

635
00:24:24,412 --> 00:24:27,334
like a cyclic roll, a cyclic shift. We

636
00:24:27,372 --> 00:24:29,058
know that this is going to correspond to

637
00:24:29,084 --> 00:24:31,514
our learned input transformations. And

638
00:24:31,552 --> 00:24:33,242
we can verify that by saying, okay,

639
00:24:33,296 --> 00:24:34,806
what if I continue this input

640
00:24:34,838 --> 00:24:36,854
transformation, the true transformation

641
00:24:36,902 --> 00:24:38,950
in the data set, which is a rotation.

642
00:24:39,110 --> 00:24:40,838
And then I compare that with how I've

643
00:24:40,854 --> 00:24:42,894
done my role in my latent space by

644
00:24:42,932 --> 00:24:44,526
moving my activations around in my

645
00:24:44,548 --> 00:24:47,006
brain. And then we decode and we see

646
00:24:47,028 --> 00:24:49,534
that we get the exact same thing. And so

647
00:24:49,572 --> 00:24:52,046
this is demonstrating this commutivity

648
00:24:52,078 --> 00:24:53,570
property that I was talking about before

649
00:24:53,640 --> 00:24:57,490
for verifying homomorphism. And so

650
00:24:57,560 --> 00:24:59,154
to measure this a little bit more

651
00:24:59,352 --> 00:25:02,558
quantitatively, we can measure what's

652
00:25:02,574 --> 00:25:04,886
called an equivalence loss. So this is

653
00:25:04,988 --> 00:25:07,094
really the quantification of this

654
00:25:07,132 --> 00:25:10,034
difference between our rolled capsule

655
00:25:10,082 --> 00:25:12,406
activation, our rolling in our head,

656
00:25:12,508 --> 00:25:15,270
versus watching the rolling unfold,

657
00:25:15,770 --> 00:25:17,414
watching the transformation unfold

658
00:25:17,462 --> 00:25:20,342
before us. So we see that topographic

659
00:25:20,406 --> 00:25:22,714
VAE achieves significantly lower

660
00:25:22,912 --> 00:25:26,042
equivalent error. This bubble VAE is

661
00:25:26,096 --> 00:25:27,386
what I was talking about before, where

662
00:25:27,408 --> 00:25:29,078
it's learning invariance, so it doesn't

663
00:25:29,094 --> 00:25:30,998
have the shift operation and then the

664
00:25:31,024 --> 00:25:34,318
traditional VAE kind of has no notion of

665
00:25:34,484 --> 00:25:36,446
organization or temporal. Component, so

666
00:25:36,468 --> 00:25:39,486
it performs very poorly. In addition to

667
00:25:39,508 --> 00:25:40,846
this, we see that the model is a better

668
00:25:40,868 --> 00:25:42,906
generative model of sequences. It just

669
00:25:42,948 --> 00:25:46,930
gets a lower negative log likelihood

670
00:25:47,510 --> 00:25:49,986
on the data set. So it's better able to

671
00:25:50,008 --> 00:25:51,538
model this data set because it has a

672
00:25:51,544 --> 00:25:52,706
notion. Of the structure of the

673
00:25:52,728 --> 00:25:53,650
transformations.

674
00:25:56,310 --> 00:25:58,214
We can test this on multiple different

675
00:25:58,252 --> 00:25:59,986
transformation types. On the top row

676
00:26:00,018 --> 00:26:01,506
we're showing the true transformation,

677
00:26:01,618 --> 00:26:04,086
we hold out these grayed out images and

678
00:26:04,108 --> 00:26:06,246
then on the bottom row we encode and

679
00:26:06,268 --> 00:26:07,354
then we just kind of roll our

680
00:26:07,392 --> 00:26:09,206
activations around and we keep decoding

681
00:26:09,238 --> 00:26:13,578
to see what the model has

682
00:26:13,664 --> 00:26:15,414
learned as the current transformation

683
00:26:15,462 --> 00:26:17,306
that's being observed. And we see that

684
00:26:17,328 --> 00:26:20,214
it can basically perfectly reconstruct

685
00:26:20,262 --> 00:26:21,798
these elements of the sequence that it's

686
00:26:21,814 --> 00:26:24,358
never seen before. Additionally, with

687
00:26:24,384 --> 00:26:25,758
images that are from the test set that

688
00:26:25,764 --> 00:26:27,374
it's never seen before, simply because

689
00:26:27,412 --> 00:26:29,166
it knows what the transformation is that

690
00:26:29,188 --> 00:26:30,478
it's currently encoding and it. Can

691
00:26:30,484 --> 00:26:32,290
generalize that to new examples.

692
00:26:33,990 --> 00:26:36,274
So the takeaway from this part is really

693
00:26:36,312 --> 00:26:37,938
topographic organization. We showed that

694
00:26:37,944 --> 00:26:39,874
it preserved input structure and now

695
00:26:39,912 --> 00:26:41,822
we're showing it can potentially improve

696
00:26:41,886 --> 00:26:44,098
efficiency and generalization as we

697
00:26:44,104 --> 00:26:47,762
would hope. So, finally, something that

698
00:26:47,816 --> 00:26:49,106
surprised us and I thought was

699
00:26:49,128 --> 00:26:51,654
potentially the most interesting is that

700
00:26:51,852 --> 00:26:53,298
these transformations that are learned

701
00:26:53,314 --> 00:26:54,854
by our model actually generalize the

702
00:26:54,892 --> 00:26:56,806
combinations of transformations that

703
00:26:56,828 --> 00:26:59,386
were not seen during training. So, for

704
00:26:59,408 --> 00:27:02,058
example, despite only training on color

705
00:27:02,144 --> 00:27:04,106
and rotation transformations in

706
00:27:04,128 --> 00:27:06,166
isolation, if the model is presented

707
00:27:06,198 --> 00:27:07,894
with a combined color rotation

708
00:27:07,942 --> 00:27:10,638
transformation at test time, we see that

709
00:27:10,644 --> 00:27:12,350
it's able to completely model and

710
00:27:12,420 --> 00:27:14,154
complete these transformations perfectly

711
00:27:14,202 --> 00:27:16,446
through the capsule role, implying that

712
00:27:16,468 --> 00:27:18,910
it's learned to factorize represent to

713
00:27:19,060 --> 00:27:20,606
these different transformations and it

714
00:27:20,628 --> 00:27:22,526
can flexibly combine them at inference

715
00:27:22,558 --> 00:27:26,050
time. So again,

716
00:27:26,120 --> 00:27:28,894
maybe we also don't just get efficiency

717
00:27:28,942 --> 00:27:30,594
and generalization, we also get some

718
00:27:30,632 --> 00:27:32,290
basic compositionality.

719
00:27:34,090 --> 00:27:36,022
So let's talk about the limitations and

720
00:27:36,076 --> 00:27:38,294
what we could do next. The main

721
00:27:38,332 --> 00:27:40,594
limitation is that there's a predefined

722
00:27:40,722 --> 00:27:43,974
transformation that we're imposing in

723
00:27:44,012 --> 00:27:46,006
both space and time. So although we

724
00:27:46,028 --> 00:27:47,882
freed ourselves from group

725
00:27:47,936 --> 00:27:50,620
transformations and specifically like

726
00:27:51,070 --> 00:27:53,354
translation or rotation as is currently

727
00:27:53,392 --> 00:27:55,994
done in the machine learning world. We

728
00:27:56,032 --> 00:27:59,270
still have this hard coded

729
00:27:59,430 --> 00:28:01,902
latent role in our heads for everything

730
00:28:01,956 --> 00:28:03,726
we see. And to make this a little bit

731
00:28:03,748 --> 00:28:05,582
more flexible, so hopefully we can model

732
00:28:05,636 --> 00:28:07,950
a greater diversity of transformations,

733
00:28:09,490 --> 00:28:11,482
we think maybe we can take inspiration

734
00:28:11,546 --> 00:28:14,138
from more structured spatiotemporal

735
00:28:14,154 --> 00:28:16,078
dynamics that are observed in the brain.

736
00:28:16,174 --> 00:28:18,706
And so that takes us to the second part

737
00:28:18,728 --> 00:28:21,086
of this talk, which is spatiotemporal

738
00:28:21,118 --> 00:28:22,338
dynamics that we're going to try. To

739
00:28:22,344 --> 00:28:23,646
integrate into artificial neural

740
00:28:23,678 --> 00:28:25,554
networks. One example of that is

741
00:28:25,592 --> 00:28:28,294
traveling waves. Like I show here. So

742
00:28:28,332 --> 00:28:30,966
what do we mean by that? Here's a very

743
00:28:30,988 --> 00:28:32,902
recent paper where they used a nine

744
00:28:32,956 --> 00:28:36,546
tesla fMRI operating at 36 millisecond

745
00:28:36,578 --> 00:28:39,066
resolution to image a single slice of a

746
00:28:39,088 --> 00:28:41,706
rat brain under anesthesia. And what we

747
00:28:41,728 --> 00:28:43,862
see is this very clearly structured

748
00:28:43,926 --> 00:28:45,594
spatiotemporal activity and

749
00:28:45,632 --> 00:28:48,106
correlations. And these authors of the

750
00:28:48,128 --> 00:28:50,378
paper go on to analyze this activity in

751
00:28:50,384 --> 00:28:52,826
terms of the principal modes as depicted

752
00:28:52,858 --> 00:28:55,630
on the right. So our hypothesis is that

753
00:28:55,700 --> 00:28:57,114
perhaps some sort of a correlation

754
00:28:57,162 --> 00:28:59,002
structure like this may be beneficial

755
00:28:59,066 --> 00:29:01,118
for structuring the representations of

756
00:29:01,124 --> 00:29:03,006
our model with respect to observed

757
00:29:03,038 --> 00:29:04,994
transformations, but in a much more

758
00:29:05,032 --> 00:29:07,374
flexible way than simply just a cyclic

759
00:29:07,422 --> 00:29:08,980
shift. Like we were doing before.

760
00:29:12,790 --> 00:29:15,798
And let me say that this is not just

761
00:29:15,884 --> 00:29:18,390
observed in anesthesized rats.

762
00:29:19,290 --> 00:29:21,190
You can see these traveling waves happen

763
00:29:21,260 --> 00:29:24,706
in the mt cortex of awake behaving

764
00:29:24,738 --> 00:29:27,802
primates. So for example, on the left

765
00:29:27,856 --> 00:29:29,914
here, they show traveling waves that

766
00:29:29,952 --> 00:29:33,594
actually change how likely a

767
00:29:33,632 --> 00:29:36,534
primate is to see a low contrast stimuli

768
00:29:36,582 --> 00:29:39,130
based on the phase of the wave.

769
00:29:39,550 --> 00:29:41,854
Furthermore, they show that a high

770
00:29:41,892 --> 00:29:44,542
contrast stimulus on the right can

771
00:29:44,596 --> 00:29:46,238
induce a traveling wave of activity that

772
00:29:46,244 --> 00:29:48,506
propagates outwards union primary visual

773
00:29:48,538 --> 00:29:51,274
cortex. So these are really ubiquitous

774
00:29:51,322 --> 00:29:53,054
throughout the brain at multiple levels.

775
00:29:53,102 --> 00:29:54,786
And it would be interesting to study

776
00:29:54,888 --> 00:29:56,900
what their implications are for

777
00:29:57,430 --> 00:29:59,026
structured representation learning. In

778
00:29:59,048 --> 00:30:02,654
our case, or generally, there is prior

779
00:30:02,702 --> 00:30:05,060
work which has studied these types of

780
00:30:05,510 --> 00:30:07,526
dynamics and they built models. So on

781
00:30:07,548 --> 00:30:09,094
the top, these are the equations which

782
00:30:09,132 --> 00:30:11,106
describe a spiking neural network,

783
00:30:11,218 --> 00:30:13,942
which they show if you implement time

784
00:30:13,996 --> 00:30:16,306
delays, actually exxonal time delays

785
00:30:16,338 --> 00:30:18,394
between neurons, you do get these

786
00:30:18,432 --> 00:30:21,014
structured dynamics of traveling waves

787
00:30:21,062 --> 00:30:22,554
as long as your network size is large

788
00:30:22,592 --> 00:30:25,706
enough. However, as many

789
00:30:25,728 --> 00:30:27,334
people probably know, it's relatively

790
00:30:27,382 --> 00:30:29,258
challenging to train spiking neural

791
00:30:29,274 --> 00:30:31,614
networks of the same size and

792
00:30:31,652 --> 00:30:34,030
performance as deep neural networks.

793
00:30:35,010 --> 00:30:37,694
Similarly, on the bottom, another system

794
00:30:37,812 --> 00:30:39,470
which is significantly simpler, but

795
00:30:39,540 --> 00:30:42,706
perhaps too simple, is a network of

796
00:30:42,728 --> 00:30:45,042
coupled oscillators. These are known to

797
00:30:45,176 --> 00:30:48,606
exhibit synchrony and spatial temporal

798
00:30:48,638 --> 00:30:52,306
dynamics and complex patterns. But this

799
00:30:52,328 --> 00:30:53,986
is called like a phase reduced system

800
00:30:54,088 --> 00:30:55,566
and doesn't quite capture the full

801
00:30:55,608 --> 00:30:57,078
complexity that we're interested in. So

802
00:30:57,084 --> 00:30:58,066
we're looking at something that's

803
00:30:58,098 --> 00:31:00,966
potentially in between these two. And

804
00:31:00,988 --> 00:31:03,574
what we settled on is this work, in this

805
00:31:03,612 --> 00:31:06,954
work is to parameterize a network of

806
00:31:06,992 --> 00:31:08,954
coupled oscillators slightly more

807
00:31:08,992 --> 00:31:11,226
flexibly than a Kuramoto model. So this.

808
00:31:11,248 --> 00:31:13,846
Is really built on this coupled

809
00:31:13,878 --> 00:31:15,706
distillatory recurrent neural network of

810
00:31:15,728 --> 00:31:18,090
Constantine, Rouge and Nisha,

811
00:31:18,750 --> 00:31:20,954
where they basically took the equation

812
00:31:21,002 --> 00:31:22,586
which describes a simple harmonic

813
00:31:22,618 --> 00:31:23,694
oscillator. It's a second order

814
00:31:23,732 --> 00:31:26,218
differential equation. The acceleration

815
00:31:26,314 --> 00:31:29,806
on a ball on a spring is proportional to

816
00:31:29,828 --> 00:31:33,658
its displacement. You can

817
00:31:33,684 --> 00:31:35,778
add additional terms such as damping so

818
00:31:35,784 --> 00:31:37,554
that the oscillations slowly die out

819
00:31:37,592 --> 00:31:41,054
over time. You can drive this oscillator

820
00:31:41,102 --> 00:31:42,706
with an external input to kind of

821
00:31:42,728 --> 00:31:44,594
counteract this damping or to give

822
00:31:44,632 --> 00:31:45,906
slightly more complexity to the

823
00:31:45,928 --> 00:31:48,438
dynamics. And then furthermore, if you

824
00:31:48,444 --> 00:31:50,246
have many of these oscillators, you can

825
00:31:50,268 --> 00:31:51,906
couple them together with these coupling

826
00:31:51,938 --> 00:31:54,966
matrices W, as we demonstrate kind of in

827
00:31:54,988 --> 00:31:56,246
this picture here. So you can really

828
00:31:56,268 --> 00:31:57,446
think of this network as a bunch of

829
00:31:57,468 --> 00:31:59,302
balls on springs and they're maybe

830
00:31:59,356 --> 00:32:01,046
connected to each other also by springs

831
00:32:01,078 --> 00:32:02,794
or elastic bands, whatever. The couple,

832
00:32:02,832 --> 00:32:04,650
the Silatory recurrent neural network of

833
00:32:04,720 --> 00:32:07,222
Rush and Mishra with these various

834
00:32:07,286 --> 00:32:09,146
terms. And this has been shown to be

835
00:32:09,168 --> 00:32:11,082
very powerful for modeling long

836
00:32:11,136 --> 00:32:12,846
sequences. They also mentioned they were

837
00:32:12,868 --> 00:32:14,414
inspired by the brain building this.

838
00:32:14,452 --> 00:32:15,966
And there's a lot of good analysis in

839
00:32:15,988 --> 00:32:18,318
that paper. For example, they show that

840
00:32:18,324 --> 00:32:19,802
this has really beneficial properties

841
00:32:19,866 --> 00:32:22,686
with respect to vanishing gradient

842
00:32:22,718 --> 00:32:24,514
problems that typically happen in

843
00:32:24,552 --> 00:32:27,666
recurrent neural networks. But if we

844
00:32:27,688 --> 00:32:29,966
want to look at spatiotemporal dynamics

845
00:32:29,998 --> 00:32:32,206
in this type of a model, it's slightly

846
00:32:32,238 --> 00:32:33,770
challenging because these coupling

847
00:32:33,790 --> 00:32:37,302
matrices here, the W's that

848
00:32:37,356 --> 00:32:40,406
connect each oscillator's position to

849
00:32:40,428 --> 00:32:42,946
one another, these are densely connected

850
00:32:42,978 --> 00:32:44,726
matrices like I've tried to. Depict on

851
00:32:44,748 --> 00:32:47,078
the left here. So if you try to

852
00:32:47,084 --> 00:32:49,238
visualize the dynamics of this network,

853
00:32:49,414 --> 00:32:51,270
you don't see any spatial organization.

854
00:32:51,350 --> 00:32:54,794
There's no inherent topology to the

855
00:32:54,832 --> 00:32:57,226
latent. Space of this model. So you can

856
00:32:57,248 --> 00:32:58,934
think of this like in our previous

857
00:32:58,982 --> 00:33:01,198
example. A neuron is connected to a

858
00:33:01,204 --> 00:33:03,086
potentially arbitrary set of other

859
00:33:03,108 --> 00:33:04,798
neurons. Those neurons are connected to

860
00:33:04,804 --> 00:33:06,446
another arbitrary set of neurons. And

861
00:33:06,468 --> 00:33:08,394
you'll just get oscillatory dynamics,

862
00:33:08,442 --> 00:33:10,830
certainly, but kind of fluctuations that

863
00:33:10,900 --> 00:33:13,294
don't make a lot of structured sense in

864
00:33:13,332 --> 00:33:15,234
our work. Then we thought, okay, how can

865
00:33:15,272 --> 00:33:18,146
we convert this more to the types of

866
00:33:18,168 --> 00:33:19,714
dynamics that we're interested in, this

867
00:33:19,752 --> 00:33:22,114
structured propagation of activity? And

868
00:33:22,152 --> 00:33:24,146
one clear way to do that is to have a

869
00:33:24,168 --> 00:33:26,440
more structured connectivity matrix W,

870
00:33:27,370 --> 00:33:30,086
which we found is easily implemented and

871
00:33:30,108 --> 00:33:31,398
efficiently implemented through a

872
00:33:31,404 --> 00:33:32,886
convolution operation, which you can

873
00:33:32,908 --> 00:33:35,410
think of like a locally connected layer.

874
00:33:35,490 --> 00:33:36,706
So instead of having every neuron

875
00:33:36,738 --> 00:33:38,518
connected to every neuron, neurons are

876
00:33:38,524 --> 00:33:39,654
just connected to their nearby

877
00:33:39,702 --> 00:33:42,026
neighbors. After training, you'll end up

878
00:33:42,048 --> 00:33:43,546
getting something. That looks like a

879
00:33:43,648 --> 00:33:46,986
smooth spatial temporal dynamics. So to

880
00:33:47,008 --> 00:33:48,474
be a little bit more clear, to train

881
00:33:48,512 --> 00:33:50,814
this model, we take this second order

882
00:33:50,852 --> 00:33:52,206
differential equation that we were

883
00:33:52,228 --> 00:33:54,622
describing before you discretize it into

884
00:33:54,676 --> 00:33:56,446
two first order equations. You can think

885
00:33:56,468 --> 00:33:58,618
of this as like numerically integrating

886
00:33:58,714 --> 00:34:01,966
the ode. We now have a velocity and then

887
00:34:01,988 --> 00:34:05,570
we update and we can train this model

888
00:34:05,640 --> 00:34:07,378
as something like an auto encoder or an

889
00:34:07,384 --> 00:34:09,666
autoregressive model. So we take an

890
00:34:09,688 --> 00:34:11,682
input, we encode it to our latent space.

891
00:34:11,816 --> 00:34:14,526
Really, the input is this F of x term

892
00:34:14,558 --> 00:34:16,578
which acts as the driving term. So it's

893
00:34:16,594 --> 00:34:18,694
like driving these oscillators from the

894
00:34:18,732 --> 00:34:20,934
bottom and then they have their own

895
00:34:20,972 --> 00:34:22,966
dynamics which are defined by the

896
00:34:22,988 --> 00:34:25,234
coupling terms, these local couplings.

897
00:34:25,362 --> 00:34:27,366
And then at each time step we take this

898
00:34:27,388 --> 00:34:29,306
latent state, this wave state, and we

899
00:34:29,328 --> 00:34:31,590
decode to try and reconstruct the input.

900
00:34:31,670 --> 00:34:33,258
Maybe at the current time step or a

901
00:34:33,264 --> 00:34:36,394
future time step, we can do some

902
00:34:36,432 --> 00:34:40,426
analysis of these models during training

903
00:34:40,608 --> 00:34:43,102
to see what happens before training.

904
00:34:43,156 --> 00:34:44,958
And after training, we can compute the

905
00:34:44,964 --> 00:34:46,954
phase and the velocity of the dynamics

906
00:34:47,002 --> 00:34:48,926
in the latent space. Basically, we see

907
00:34:48,948 --> 00:34:50,814
at the beginning of training there's no

908
00:34:50,852 --> 00:34:53,010
waves in our model. But after training,

909
00:34:53,080 --> 00:34:54,926
after 50 epochs, we see that there's

910
00:34:54,958 --> 00:34:56,382
these smooth structured activity

911
00:34:56,446 --> 00:34:59,874
propagating downwards in service of this

912
00:34:59,912 --> 00:35:01,474
sequence modeling task that we're doing,

913
00:35:01,512 --> 00:35:05,106
like rotating objects. So what's the

914
00:35:05,128 --> 00:35:06,306
benefit of this? I mean, the whole

915
00:35:06,328 --> 00:35:09,414
reason I motivated this was to say we

916
00:35:09,452 --> 00:35:11,026
wanted to have more flexibly learned

917
00:35:11,058 --> 00:35:12,486
structure. Are we actually doing that or

918
00:35:12,508 --> 00:35:15,334
are. We just getting pretty waves? So

919
00:35:15,372 --> 00:35:17,494
what we showed in our paper is that we

920
00:35:17,532 --> 00:35:19,554
really are learning some sort of useful

921
00:35:19,602 --> 00:35:20,954
structure. And the way we showed that

922
00:35:20,992 --> 00:35:22,266
is, again, with something like this

923
00:35:22,288 --> 00:35:24,458
commutative diagram. If you take an

924
00:35:24,464 --> 00:35:26,698
input and you encode it and you get a

925
00:35:26,704 --> 00:35:29,142
wave state and then you propagate waves

926
00:35:29,206 --> 00:35:31,434
artificially in that wave state and then

927
00:35:31,472 --> 00:35:33,386
decode, you can observe that it's

928
00:35:33,418 --> 00:35:35,086
actually exactly the same as if you had

929
00:35:35,108 --> 00:35:37,054
just by showing a bunch of different

930
00:35:37,092 --> 00:35:38,894
images of different transformations. So

931
00:35:38,932 --> 00:35:41,582
a lot of different digits, different

932
00:35:41,636 --> 00:35:43,786
features. And we see that we get

933
00:35:43,828 --> 00:35:45,954
different types of wave activity in each

934
00:35:45,992 --> 00:35:47,762
case in order to model that different

935
00:35:47,816 --> 00:35:49,986
transformation. If we train it on

936
00:35:50,008 --> 00:35:51,394
different data sets as well, we

937
00:35:51,432 --> 00:35:53,938
similarly see more complex dynamics. In

938
00:35:53,944 --> 00:35:55,806
this case, maybe not even traveling

939
00:35:55,838 --> 00:35:58,006
waves or standing waves, which can be

940
00:35:58,028 --> 00:36:00,166
thought of as traveling waves in

941
00:36:00,188 --> 00:36:02,226
opposite directions. So we see if we're

942
00:36:02,258 --> 00:36:04,006
modeling these orbital dynamics, we get

943
00:36:04,028 --> 00:36:05,846
these kind of smoothly moving blobs of

944
00:36:05,868 --> 00:36:07,466
activity in our latent space. If we're

945
00:36:07,488 --> 00:36:09,722
modeling a pendulum, we similarly get

946
00:36:09,776 --> 00:36:14,714
kind of complex oscillatory activity so

947
00:36:14,832 --> 00:36:17,194
it's preserved input structure, but

948
00:36:17,232 --> 00:36:19,386
additionally more flexibility than we

949
00:36:19,408 --> 00:36:21,006
had before, which is kind of our

950
00:36:21,028 --> 00:36:24,590
ultimate goal. So finally,

951
00:36:24,740 --> 00:36:26,926
I want to talk a bit about how I think

952
00:36:26,948 --> 00:36:29,166
the outcome of this research may not

953
00:36:29,188 --> 00:36:30,842
only improve artificial intelligence,

954
00:36:30,906 --> 00:36:33,374
but also how it helps us understand why

955
00:36:33,412 --> 00:36:34,890
our measurements of the brain look the

956
00:36:34,900 --> 00:36:36,690
way they do. So to give a brief example

957
00:36:36,760 --> 00:36:39,026
of what I mean by this, I talked a bit

958
00:36:39,048 --> 00:36:41,998
about before, about these and places.

959
00:36:42,174 --> 00:36:45,134
So in this fantastic work with HIE Gao,

960
00:36:45,182 --> 00:36:47,906
we studied if our simple topographic

961
00:36:47,938 --> 00:36:49,462
prior, as we discussed, may be able to

962
00:36:49,516 --> 00:36:51,670
reproduce these same effects. So

963
00:36:51,740 --> 00:36:53,654
specifically, we plot the value of this

964
00:36:53,692 --> 00:36:56,166
Cohen's D selectivity metric for each of

965
00:36:56,188 --> 00:36:58,182
our neurons with respect to a different

966
00:36:58,236 --> 00:36:59,894
data set of images potentially

967
00:36:59,942 --> 00:37:02,314
containing just faces or just objects or

968
00:37:02,352 --> 00:37:04,874
bodies. And so we measure for every

969
00:37:04,912 --> 00:37:06,826
neuron, is it more likely to respond to

970
00:37:06,848 --> 00:37:08,746
faces or the ration emerges in the

971
00:37:08,768 --> 00:37:11,678
brain? But I do think that it tells us

972
00:37:11,844 --> 00:37:13,646
that the relative organization of

973
00:37:13,668 --> 00:37:15,834
selectivity may at least be partially

974
00:37:15,882 --> 00:37:17,994
attributable to correlation statistics

975
00:37:18,042 --> 00:37:20,958
in the data after being passed through a

976
00:37:20,964 --> 00:37:23,106
highly nonlinear future extractor. Such

977
00:37:23,128 --> 00:37:26,098
as a deep neural network. So, in a

978
00:37:26,104 --> 00:37:27,406
similar vein, something that's

979
00:37:27,438 --> 00:37:28,878
interesting, there's a known what's

980
00:37:28,894 --> 00:37:31,262
called tripartite, or the visual stream.

981
00:37:31,326 --> 00:37:35,318
So images of or

982
00:37:35,484 --> 00:37:37,926
objects are selectivity with respect to

983
00:37:37,948 --> 00:37:40,194
objects is organized by more abstract

984
00:37:40,242 --> 00:37:42,006
properties, such as animacy, is this

985
00:37:42,028 --> 00:37:45,462
thing alive or inanimate versus also

986
00:37:45,596 --> 00:37:47,706
real world object size, like what is the

987
00:37:47,728 --> 00:37:49,900
size of a teapot versus a car?

988
00:37:50,910 --> 00:37:54,326
And what we see is that in humans,

989
00:37:54,518 --> 00:37:56,218
this selectivity is organized in this

990
00:37:56,224 --> 00:37:57,994
tripartite structure. You typically have

991
00:37:58,032 --> 00:38:00,026
small objects that are in between

992
00:38:00,128 --> 00:38:02,106
animate and inanimate objects in terms

993
00:38:02,128 --> 00:38:04,238
of their selectivity. And we see the

994
00:38:04,244 --> 00:38:05,886
same thing kind of happens here. So

995
00:38:05,908 --> 00:38:07,278
these are measuring the selectivity of

996
00:38:07,284 --> 00:38:08,446
the same set of neurons, but with

997
00:38:08,468 --> 00:38:09,598
respect to these different sets of

998
00:38:09,604 --> 00:38:11,706
stimuli. We see that the small cluster

999
00:38:11,738 --> 00:38:13,798
is in between animate and inanimate

1000
00:38:13,834 --> 00:38:15,426
cluster. And again, this happens for

1001
00:38:15,448 --> 00:38:17,842
multiple different initializations. So

1002
00:38:17,976 --> 00:38:19,406
this is something I hope we can explore

1003
00:38:19,438 --> 00:38:21,506
a bit further for this community. I

1004
00:38:21,528 --> 00:38:24,574
think it's interesting because it's

1005
00:38:24,622 --> 00:38:26,726
really a way of showing that we built a

1006
00:38:26,748 --> 00:38:28,354
structured world model. And potentially,

1007
00:38:28,402 --> 00:38:31,400
this world model is beneficial for

1008
00:38:31,930 --> 00:38:34,406
better representing real world data in.

1009
00:38:34,428 --> 00:38:37,434
A structured way, and you get lower free

1010
00:38:37,472 --> 00:38:41,066
energy in that sense. So by

1011
00:38:41,088 --> 00:38:43,526
developing these models like we showed

1012
00:38:43,558 --> 00:38:45,354
here, we may get insights into new

1013
00:38:45,392 --> 00:38:46,854
mechanisms for how this structure

1014
00:38:46,902 --> 00:38:49,398
emerges, including topographic

1015
00:38:49,414 --> 00:38:50,794
organization that we never thought of

1016
00:38:50,832 --> 00:38:53,182
before. So, machine model, I was looking

1017
00:38:53,236 --> 00:38:56,574
at the orientation selectivity of

1018
00:38:56,612 --> 00:38:59,194
neurons, which I wasn't particularly

1019
00:38:59,322 --> 00:39:01,966
expecting something to happen, but

1020
00:39:02,148 --> 00:39:03,994
you're looking at kind of these waves

1021
00:39:04,122 --> 00:39:06,886
propagate over this simulated vertical

1022
00:39:06,938 --> 00:39:08,958
surface. And I thought, okay, maybe I'm

1023
00:39:08,974 --> 00:39:10,626
showing rotated images. Maybe this has

1024
00:39:10,648 --> 00:39:12,222
some effect on the orientation

1025
00:39:12,286 --> 00:39:15,106
selectivity. And actually, if you go in

1026
00:39:15,128 --> 00:39:16,934
and you measure the selectivity of each

1027
00:39:16,972 --> 00:39:18,466
neuron with respect to these differently

1028
00:39:18,498 --> 00:39:21,270
oriented lines, what you see is that

1029
00:39:21,420 --> 00:39:23,702
it's surprisingly reminiscent of the

1030
00:39:23,756 --> 00:39:25,366
orient paper columns that are seen in

1031
00:39:25,388 --> 00:39:27,014
primary visual cortex. This is stuff

1032
00:39:27,052 --> 00:39:29,126
going back to Hugle and Weasel. And this

1033
00:39:29,148 --> 00:39:30,298
is something that just kind of came out

1034
00:39:30,304 --> 00:39:32,314
of this model and the fact that it has

1035
00:39:32,352 --> 00:39:33,866
the spatial. Temporal structure with

1036
00:39:33,888 --> 00:39:36,986
respect to transformation. So, of

1037
00:39:37,008 --> 00:39:38,614
course, this is a really coarse analogy,

1038
00:39:38,662 --> 00:39:40,186
but I think this is an example of how

1039
00:39:40,208 --> 00:39:41,486
building these types of models can help

1040
00:39:41,508 --> 00:39:44,426
us think about how the brain builds

1041
00:39:44,458 --> 00:39:46,606
representational structure and the way

1042
00:39:46,628 --> 00:39:48,286
it's organized in a way. That maybe we

1043
00:39:48,308 --> 00:39:49,520
haven't thought about before.

1044
00:39:51,490 --> 00:39:53,774
I think I'm not the only one who's doing

1045
00:39:53,812 --> 00:39:56,306
this type of work. And so I want to talk

1046
00:39:56,328 --> 00:39:57,586
a little bit about some other people who

1047
00:39:57,608 --> 00:40:00,034
are doing this. So I've been talking

1048
00:40:00,072 --> 00:40:03,042
about this equivalent structure. People

1049
00:40:03,096 --> 00:40:05,074
such as James Whittington and Tim

1050
00:40:05,112 --> 00:40:08,678
Barrons and Surge Anguli have

1051
00:40:08,764 --> 00:40:10,594
shown recently that by introducing

1052
00:40:10,642 --> 00:40:14,454
algebraic constraints into

1053
00:40:14,492 --> 00:40:16,886
a learning process, in this case, it was

1054
00:40:16,908 --> 00:40:20,554
like the motion of an agent in

1055
00:40:20,592 --> 00:40:22,986
an environment. By saying, you need to

1056
00:40:23,008 --> 00:40:24,694
preserve kind of this algebraic

1057
00:40:24,742 --> 00:40:27,334
structure of if I move in a circle west,

1058
00:40:27,382 --> 00:40:29,086
north, east, south, I end up back at the

1059
00:40:29,108 --> 00:40:31,246
same point. Again, by introducing these

1060
00:40:31,268 --> 00:40:33,486
types of constraints, you get the

1061
00:40:33,508 --> 00:40:34,894
emergence of grid, cell like

1062
00:40:34,932 --> 00:40:38,350
representations. So I'd be interested

1063
00:40:38,420 --> 00:40:40,970
to see how this idea of representational

1064
00:40:41,050 --> 00:40:43,486
structure can help us explain maybe more

1065
00:40:43,508 --> 00:40:44,926
than our scientific findings we're

1066
00:40:44,958 --> 00:40:48,306
finding as well and how this relates to

1067
00:40:48,328 --> 00:40:51,794
generative models as a whole. And then

1068
00:40:51,832 --> 00:40:53,186
finally, I think there's something to be

1069
00:40:53,208 --> 00:40:55,186
said about cognitive plausibility of

1070
00:40:55,208 --> 00:40:56,486
these models as well. Maybe we're not

1071
00:40:56,508 --> 00:40:58,246
just going to be testing them from a

1072
00:40:58,348 --> 00:41:00,134
neuroscience perspective, but also from

1073
00:41:00,172 --> 00:41:01,766
a cognitive science perspective. For

1074
00:41:01,788 --> 00:41:04,386
example, there's these Ravens

1075
00:41:04,418 --> 00:41:06,262
Progressive Matrices on the left where

1076
00:41:06,316 --> 00:41:08,694
you have to say which one of these

1077
00:41:08,732 --> 00:41:11,142
images is more likely to fit in this

1078
00:41:11,196 --> 00:41:14,246
pattern? Or for example, how likely is

1079
00:41:14,268 --> 00:41:16,406
it that this Jenga Tower falls over when

1080
00:41:16,428 --> 00:41:20,318
you pull a specific block or with a

1081
00:41:20,324 --> 00:41:24,894
given structure? And I think these

1082
00:41:24,932 --> 00:41:27,102
types of tests are really testing if our

1083
00:41:27,156 --> 00:41:29,150
world models that we're building are

1084
00:41:29,220 --> 00:41:31,486
similar to the types of models that we

1085
00:41:31,508 --> 00:41:33,618
innately have our own common sense as

1086
00:41:33,704 --> 00:41:36,654
humans or as beings living in a natural

1087
00:41:36,702 --> 00:41:38,814
world. And I've done some preliminary

1088
00:41:38,862 --> 00:41:41,426
work in this direction, I think very

1089
00:41:41,608 --> 00:41:43,426
preliminary and not nearly this

1090
00:41:43,528 --> 00:41:46,126
complicated, but trying to model visual

1091
00:41:46,158 --> 00:41:47,974
illusions. So if you take a really

1092
00:41:48,012 --> 00:41:51,074
simple data set of a moving bar stimuli

1093
00:41:51,122 --> 00:41:53,238
or a static bar frame and you move it a

1094
00:41:53,244 --> 00:41:55,142
little bit, you can see that the model

1095
00:41:55,196 --> 00:41:57,654
will actually infer that missing frame

1096
00:41:57,702 --> 00:42:00,006
and then actually also infer continued

1097
00:42:00,038 --> 00:42:02,026
motion. So it's like overshooting the

1098
00:42:02,048 --> 00:42:04,074
trajectory of what the actual stimuli is

1099
00:42:04,112 --> 00:42:06,890
providing it before correcting again.

1100
00:42:07,040 --> 00:42:09,386
So I think modeling illusions is

1101
00:42:09,408 --> 00:42:11,374
certainly an interesting way to study if

1102
00:42:11,412 --> 00:42:13,118
our world models are similar to the

1103
00:42:13,124 --> 00:42:15,630
types. Of models that we have ourselves.

1104
00:42:16,530 --> 00:42:19,454
So in conclusion, yeah, I think

1105
00:42:19,492 --> 00:42:21,646
topographic priors we could show that

1106
00:42:21,668 --> 00:42:23,034
they effectively learn structured

1107
00:42:23,082 --> 00:42:24,734
representations or structured world

1108
00:42:24,772 --> 00:42:26,594
models. This learned structure is

1109
00:42:26,632 --> 00:42:29,054
flexible and adaptable to arbitrary

1110
00:42:29,102 --> 00:42:30,654
transformations, unlike traditional

1111
00:42:30,702 --> 00:42:33,858
equivariance. And topographic riders can

1112
00:42:33,864 --> 00:42:35,618
be induced statistically as we did in

1113
00:42:35,624 --> 00:42:38,066
the topographic VAE or through dynamics

1114
00:42:38,098 --> 00:42:39,906
like we were showing in these neural

1115
00:42:39,938 --> 00:42:42,646
wave machine type models. So to

1116
00:42:42,668 --> 00:42:44,642
conclude, I'll leave you with this quote

1117
00:42:44,706 --> 00:42:47,174
that I found in Fukushima's paper from

1118
00:42:47,212 --> 00:42:50,198
1980 I thought was pretty far ahead of

1119
00:42:50,204 --> 00:42:51,898
its time, where he says, if we could

1120
00:42:51,904 --> 00:42:53,386
make a neural network model which has

1121
00:42:53,408 --> 00:42:54,854
the same capability for pattern

1122
00:42:54,902 --> 00:42:56,906
recognition as a human being, it would

1123
00:42:56,928 --> 00:42:58,918
give us a powerful clue to understanding

1124
00:42:58,934 --> 00:43:01,398
the neural mechanism in the brain. So

1125
00:43:01,424 --> 00:43:03,246
that's kind of, I think, some. Of the

1126
00:43:03,268 --> 00:43:06,426
goals that we're going for here. So I'll

1127
00:43:06,458 --> 00:43:08,254
say thanks to my advisor Max, my co

1128
00:43:08,292 --> 00:43:11,086
authors Patrick Yue, Emile Jinghe, and

1129
00:43:11,108 --> 00:43:14,670
Yorn, and interested in discussion.

1130
00:43:14,830 --> 00:43:15,540
Thanks.

1131
00:43:24,570 --> 00:43:27,190
All right, thank you. Great. Very

1132
00:43:27,340 --> 00:43:29,510
interesting presentation.

1133
00:43:30,970 --> 00:43:33,302
A lot of places to start. Maybe just

1134
00:43:33,436 --> 00:43:36,650
what brought you to this work, a little

1135
00:43:36,720 --> 00:43:39,754
context on how you came into this work

1136
00:43:39,792 --> 00:43:42,330
for your PhD direction.

1137
00:43:43,950 --> 00:43:47,930
Yeah, I mean, there's been studying

1138
00:43:48,830 --> 00:43:50,714
the group that I'm in at the university

1139
00:43:50,762 --> 00:43:51,866
has been studying structured

1140
00:43:51,898 --> 00:43:55,406
representations from mathematical point

1141
00:43:55,428 --> 00:43:57,358
of view for a while, and we're some of

1142
00:43:57,364 --> 00:43:59,838
the people to models. Or for the

1143
00:43:59,844 --> 00:44:05,106
variational auto encoder. And I

1144
00:44:05,128 --> 00:44:06,882
guess what something that had always

1145
00:44:06,936 --> 00:44:10,558
been the model that respects rotations,

1146
00:44:10,654 --> 00:44:13,266
2D rotations perfectly well. But if we

1147
00:44:13,288 --> 00:44:15,666
want to do 3D rotations, we can't do

1148
00:44:15,688 --> 00:44:17,366
that because that's not a group in terms

1149
00:44:17,388 --> 00:44:19,298
of a. Projection onto a 2D plane.

1150
00:44:19,394 --> 00:44:20,886
You're losing information when this

1151
00:44:20,908 --> 00:44:22,680
thing rotates. Around, for example.

1152
00:44:24,570 --> 00:44:26,354
Or just any sort of natural

1153
00:44:26,402 --> 00:44:27,798
transformations. Like I was trying to

1154
00:44:27,804 --> 00:44:29,226
point out at the beginning, I think it

1155
00:44:29,248 --> 00:44:31,574
was trying to think about how the brain

1156
00:44:31,622 --> 00:44:33,866
models natural transformations is

1157
00:44:33,968 --> 00:44:36,170
something that these. Current frameworks

1158
00:44:37,150 --> 00:44:40,190
where do you see action playing a role

1159
00:44:41,170 --> 00:44:44,490
in terms of variational, auto encoder

1160
00:44:44,570 --> 00:44:49,146
models that include not just external

1161
00:44:49,178 --> 00:44:51,226
patterns, but also the consequences of

1162
00:44:51,268 --> 00:44:53,714
action or world model structure with

1163
00:44:53,752 --> 00:44:56,258
action. Right,

1164
00:44:56,344 --> 00:44:58,626
yeah. No, it's a good question. And I

1165
00:44:58,648 --> 00:45:02,610
think active inference is effectively

1166
00:45:03,270 --> 00:45:05,240
I think it's a good answer to that.

1167
00:45:06,410 --> 00:45:10,022
I know there are reinforcement learning

1168
00:45:10,076 --> 00:45:13,094
frameworks that do use kind of

1169
00:45:13,132 --> 00:45:15,574
externally trained world models. So you

1170
00:45:15,612 --> 00:45:17,306
train a VAE or something and then you

1171
00:45:17,328 --> 00:45:19,626
use that representation in. Your

1172
00:45:19,648 --> 00:45:22,906
reinforcement learning system. But I

1173
00:45:22,928 --> 00:45:25,434
think having a fully kind of a system

1174
00:45:25,472 --> 00:45:29,622
that is a single. Objective with action

1175
00:45:29,766 --> 00:45:31,980
as part of the. Likelihood of the data,

1176
00:45:33,730 --> 00:45:36,878
I think that's much more elegant. I'm a

1177
00:45:36,884 --> 00:45:38,320
big proponent of that.

1178
00:45:40,050 --> 00:45:42,862
I have not gotten so far as to study how

1179
00:45:42,916 --> 00:45:45,294
these structured world models in a VAE

1180
00:45:45,342 --> 00:45:46,754
or I haven't worked. On that at all.

1181
00:45:46,792 --> 00:45:48,594
But I think it would certainly be very

1182
00:45:48,632 --> 00:45:50,626
interesting to see if having a. More

1183
00:45:50,648 --> 00:45:53,838
structured world model in a variational

1184
00:45:53,854 --> 00:45:56,306
auto encoder would be beneficial in an.

1185
00:45:56,328 --> 00:45:57,766
Active setting as well. I think that

1186
00:45:57,788 --> 00:46:00,406
would be awesome. I mean, I think some

1187
00:46:00,428 --> 00:46:03,990
of these examples, like, showing before,

1188
00:46:04,060 --> 00:46:05,446
like, emergence of. Grid cells and

1189
00:46:05,468 --> 00:46:07,302
things like this, maybe. Point towards

1190
00:46:07,356 --> 00:46:09,906
that direction of, hey, maybe the brain

1191
00:46:09,938 --> 00:46:11,610
is doing something. It really obviously

1192
00:46:11,680 --> 00:46:14,346
has a lot of structure. This clearly has

1193
00:46:14,368 --> 00:46:16,746
to be useful for performing actions in

1194
00:46:16,768 --> 00:46:20,062
some way. Cool. Yeah.

1195
00:46:20,116 --> 00:46:23,406
I thought a really nice parallel that

1196
00:46:23,428 --> 00:46:25,326
you brought in with the talk was the

1197
00:46:25,348 --> 00:46:28,874
locally connected units enabled

1198
00:46:28,922 --> 00:46:32,274
your models to structurally embody the

1199
00:46:32,312 --> 00:46:34,894
convolutional constraint and pattern,

1200
00:46:34,942 --> 00:46:36,798
and that led to these arising patterns.

1201
00:46:36,894 --> 00:46:39,220
And then analogously, there was the

1202
00:46:39,910 --> 00:46:43,554
Doral et al. Where they had

1203
00:46:43,592 --> 00:46:46,722
the path exploration constraint.

1204
00:46:46,786 --> 00:46:49,800
Right. And so then it's interesting to

1205
00:46:51,370 --> 00:46:54,280
think about these action or policy

1206
00:46:54,730 --> 00:46:57,398
heuristics or sparsities like a joint

1207
00:46:57,574 --> 00:46:59,846
motor exploration. Eventually it becomes

1208
00:46:59,878 --> 00:47:02,122
understood that there's like two

1209
00:47:02,176 --> 00:47:04,854
mutually opposing ways to move a joint,

1210
00:47:04,902 --> 00:47:06,842
and then the compositionality across

1211
00:47:06,896 --> 00:47:09,134
joints can be learned at these higher

1212
00:47:09,172 --> 00:47:10,702
levels once it's locked in at lower

1213
00:47:10,756 --> 00:47:15,280
levels. So it's a very appealing and

1214
00:47:18,130 --> 00:47:21,426
niche relevant way to generalize because

1215
00:47:21,448 --> 00:47:23,730
it's both based upon the actual

1216
00:47:23,800 --> 00:47:25,602
constraints of the world, but then

1217
00:47:25,656 --> 00:47:28,222
especially through action, potentially

1218
00:47:28,286 --> 00:47:30,260
embedding something that's quite simple.

1219
00:47:31,350 --> 00:47:33,086
Right? Yeah, no, I think that's

1220
00:47:33,118 --> 00:47:34,486
definitely true. That's a really good

1221
00:47:34,508 --> 00:47:37,814
point. If you do have constraints coming

1222
00:47:37,852 --> 00:47:40,326
from. Your actions themselves, then that

1223
00:47:40,348 --> 00:47:42,674
would be hugely beneficial for helping

1224
00:47:42,722 --> 00:47:46,226
to structure your latent space. And I

1225
00:47:46,268 --> 00:47:47,994
think yeah, I guess one thing I wanted

1226
00:47:48,032 --> 00:47:51,386
to mention there's something made me

1227
00:47:51,408 --> 00:47:54,698
think of, like, Stefano Fousey's work on

1228
00:47:54,864 --> 00:47:57,050
kind of the representational geometry

1229
00:47:58,850 --> 00:48:02,254
determines how

1230
00:48:02,292 --> 00:48:04,686
generalizable a given understanding of a

1231
00:48:04,708 --> 00:48:08,062
system is. And I think if you can

1232
00:48:08,116 --> 00:48:11,578
understand if these sets

1233
00:48:11,594 --> 00:48:14,366
of activities are separable or highly

1234
00:48:14,398 --> 00:48:16,206
parallel separable with a linear

1235
00:48:16,318 --> 00:48:18,558
classifier, essentially, then you're

1236
00:48:18,574 --> 00:48:19,998
going to be able to do generalization.

1237
00:48:20,094 --> 00:48:22,498
And I think by imposing these types of

1238
00:48:22,504 --> 00:48:24,280
biases or potentially through

1239
00:48:25,130 --> 00:48:26,914
constraints that are imposed by action,

1240
00:48:26,962 --> 00:48:30,786
something like this, you are yielding

1241
00:48:30,818 --> 00:48:32,054
or kind of inducing a better

1242
00:48:32,092 --> 00:48:33,894
representational geometry. And this has

1243
00:48:33,932 --> 00:48:35,062
all sorts of benefits for

1244
00:48:35,116 --> 00:48:37,910
compositionality or generalization.

1245
00:48:39,150 --> 00:48:41,690
It's a great point. Cool. Yeah. Very

1246
00:48:41,760 --> 00:48:43,386
interesting area. All right, I'll read

1247
00:48:43,408 --> 00:48:46,362
some questions from the live chat love

1248
00:48:46,416 --> 00:48:49,850
Evolve wrote any practical

1249
00:48:50,010 --> 00:48:52,842
or observed limitations on modeling

1250
00:48:52,906 --> 00:48:53,790
illusions?

1251
00:48:58,450 --> 00:49:00,058
Deep learning, community uses. They're

1252
00:49:00,074 --> 00:49:02,834
not foveated. You don't have a center of

1253
00:49:02,872 --> 00:49:10,514
gaze, and you also don't have most

1254
00:49:10,632 --> 00:49:12,594
convolutional neural networks. I'm using

1255
00:49:12,632 --> 00:49:14,386
these kind of recurrent neural networks,

1256
00:49:14,418 --> 00:49:17,766
but time is not as clearly defined in

1257
00:49:17,788 --> 00:49:19,394
these models as it is. In a continuous

1258
00:49:19,442 --> 00:49:22,822
time setting for human undergoing an

1259
00:49:22,876 --> 00:49:26,114
illusion trial. And I think the

1260
00:49:26,172 --> 00:49:27,962
combination of these two, of the fact

1261
00:49:28,016 --> 00:49:31,100
that as a human, for most things,

1262
00:49:34,030 --> 00:49:36,294
you're shifting locations and your gaze

1263
00:49:36,342 --> 00:49:38,778
are dependent on you looking to a

1264
00:49:38,784 --> 00:49:40,338
particular area, a lot. Of cognitive

1265
00:49:40,374 --> 00:49:42,606
science tests. And so I think it would

1266
00:49:42,628 --> 00:49:46,240
be really helpful if we had models that

1267
00:49:47,650 --> 00:49:49,086
you can think of this as a type of

1268
00:49:49,108 --> 00:49:51,726
action of learning where to move your

1269
00:49:51,748 --> 00:49:53,646
gaze. One of the simplest possible that

1270
00:49:53,668 --> 00:49:55,182
would help a lot for being able to model

1271
00:49:55,236 --> 00:49:57,746
illusions. And just I mean, for me,

1272
00:49:57,768 --> 00:50:00,034
it's like I read a paper of some

1273
00:50:00,072 --> 00:50:01,810
cognitive science experiments or about

1274
00:50:01,880 --> 00:50:04,322
some illusion, and I think of, okay,

1275
00:50:04,376 --> 00:50:06,402
can I put this data set into my model

1276
00:50:06,536 --> 00:50:07,958
and test it? And most of the time the

1277
00:50:07,964 --> 00:50:09,318
answer is no, because I don't have a.

1278
00:50:09,324 --> 00:50:12,198
Model that looks around or has a

1279
00:50:12,204 --> 00:50:13,734
restricted field of view, something like

1280
00:50:13,772 --> 00:50:16,278
that. So, yeah, I think that's one of

1281
00:50:16,284 --> 00:50:21,354
the limitations. Another one is makes

1282
00:50:21,392 --> 00:50:23,398
the experiment much more complicated.

1283
00:50:23,494 --> 00:50:25,062
So that's one of the practical

1284
00:50:25,126 --> 00:50:29,066
limitations. Wow. Great answer.

1285
00:50:29,168 --> 00:50:32,250
Makes me think of a paper with letters

1286
00:50:32,330 --> 00:50:35,546
rotating on a table. That's the digit

1287
00:50:35,578 --> 00:50:36,926
rotation. Great points about the

1288
00:50:36,948 --> 00:50:38,526
Foveation and the dynamics of the

1289
00:50:38,548 --> 00:50:40,014
illusion. I think you actually did

1290
00:50:40,052 --> 00:50:42,302
mention an illusion, which is however,

1291
00:50:42,356 --> 00:50:43,422
you mentioned in the generalization

1292
00:50:43,486 --> 00:50:46,226
context, which is rotating on the two

1293
00:50:46,248 --> 00:50:49,122
dimensional screen doesn't generalize to

1294
00:50:49,176 --> 00:50:52,254
three dimensions, and that dimensional

1295
00:50:52,302 --> 00:50:54,566
collapse or reduction is the basis of

1296
00:50:54,588 --> 00:50:57,986
the cube projection illusions and cube

1297
00:50:58,018 --> 00:51:01,014
and figure rotation illusions. It's on

1298
00:51:01,052 --> 00:51:04,694
your screen and there's a silhouette or

1299
00:51:04,732 --> 00:51:08,780
there's some ambiguous stimuli that

1300
00:51:09,150 --> 00:51:11,686
it's near a criticality or a bifurcation

1301
00:51:11,798 --> 00:51:13,546
in the generative model. So it could

1302
00:51:13,568 --> 00:51:16,620
represent it one way or another way.

1303
00:51:16,990 --> 00:51:19,834
And so a lot of the switching illusions

1304
00:51:19,882 --> 00:51:21,982
are just based upon the flatness of

1305
00:51:22,036 --> 00:51:25,242
images and the limitations

1306
00:51:25,306 --> 00:51:27,920
in generalization that are revealed by.

1307
00:51:32,370 --> 00:51:34,674
Some work sorry, there's some work where

1308
00:51:34,712 --> 00:51:37,586
they can argue people have a kind of

1309
00:51:37,608 --> 00:51:39,390
three dimensional. Image in their heads.

1310
00:51:39,470 --> 00:51:42,046
Like even Nancy Kenwich had her library

1311
00:51:42,078 --> 00:51:44,918
on this recently and showing yeah, I

1312
00:51:44,924 --> 00:51:48,054
don't know. Do our models have that?

1313
00:51:48,252 --> 00:51:51,158
It's not super clear anyway.

1314
00:51:51,324 --> 00:51:53,686
Yeah, that's pretty interesting. All

1315
00:51:53,708 --> 00:51:56,274
right. From Upcycle club in the chat,

1316
00:51:56,322 --> 00:51:59,302
they wrote Kudos. You're able to learn

1317
00:51:59,356 --> 00:52:01,494
nearly as effectively. If you imagine

1318
00:52:01,542 --> 00:52:02,986
you only want a single neuron to be

1319
00:52:03,008 --> 00:52:05,466
active for every example. Your model is

1320
00:52:05,488 --> 00:52:07,834
going to be trying to memorize the data

1321
00:52:07,872 --> 00:52:09,374
set to some. Extent or something like

1322
00:52:09,412 --> 00:52:11,806
this, and you're not going to have

1323
00:52:11,828 --> 00:52:14,174
enough capacity. So, yeah, I think

1324
00:52:14,212 --> 00:52:15,934
tuning that level. Of sparsity is

1325
00:52:15,972 --> 00:52:20,030
certainly an important factor.

1326
00:52:20,850 --> 00:52:24,110
And when you look at the likelihood,

1327
00:52:25,170 --> 00:52:27,294
if you're calling framework, typically

1328
00:52:27,342 --> 00:52:30,306
this is balanced automatically with the

1329
00:52:30,328 --> 00:52:33,666
likelihood itself. If you're not doing

1330
00:52:33,688 --> 00:52:34,738
generative modeling, you just have a

1331
00:52:34,744 --> 00:52:35,958
sparsity penalty. You're going to want

1332
00:52:35,964 --> 00:52:40,034
to tune that parameter. Okay. They added

1333
00:52:40,162 --> 00:52:42,006
just to clarify runaway behavior in

1334
00:52:42,028 --> 00:52:44,034
Armina, where the network becomes

1335
00:52:44,082 --> 00:52:45,826
unstable or chaotic due to various

1336
00:52:45,858 --> 00:52:47,654
factors such as feedback loops, noise,

1337
00:52:47,702 --> 00:52:49,450
or adversarial inputs.

1338
00:52:52,270 --> 00:52:53,930
Yeah, I guess I haven't looked at this

1339
00:52:54,000 --> 00:52:55,786
in a recurrent setting where you would

1340
00:52:55,808 --> 00:52:57,450
get feedback loops,

1341
00:52:59,410 --> 00:53:02,154
but I could see adversarial examples

1342
00:53:02,202 --> 00:53:05,614
being potentially affected by your level

1343
00:53:05,652 --> 00:53:06,670
of sparsity.

1344
00:53:08,930 --> 00:53:10,846
The interesting point is, would you be

1345
00:53:10,868 --> 00:53:12,926
more susceptible or less susceptible to

1346
00:53:12,948 --> 00:53:15,380
adversary examples? I don't know.

1347
00:53:16,390 --> 00:53:20,094
Well, sparsification projecting

1348
00:53:20,142 --> 00:53:21,522
from a fully connected, higher

1349
00:53:21,576 --> 00:53:23,442
dimensional model just into

1350
00:53:23,496 --> 00:53:26,534
progressively smaller it's pretty well

1351
00:53:26,572 --> 00:53:28,194
understood in general what the trade

1352
00:53:28,242 --> 00:53:30,886
offs are. It's easier computations, a

1353
00:53:30,908 --> 00:53:34,706
smaller model, sparser. The Bayes

1354
00:53:34,738 --> 00:53:36,374
graph is going to be clearer to

1355
00:53:36,412 --> 00:53:39,466
represent and then also it will have all

1356
00:53:39,488 --> 00:53:41,206
of the other trade offs with false

1357
00:53:41,238 --> 00:53:43,846
positive and negatives of generalizing.

1358
00:53:43,958 --> 00:53:45,690
But that's why it's an iterative fit

1359
00:53:45,760 --> 00:53:49,100
process. So I guess how does your

1360
00:53:49,890 --> 00:53:53,790
sparsification approach balance?

1361
00:53:56,610 --> 00:53:59,422
Does it use AIC or BIC or some other

1362
00:53:59,476 --> 00:54:01,426
model fitting approach to determine the

1363
00:54:01,448 --> 00:54:04,610
relevant sparsification for a given

1364
00:54:04,760 --> 00:54:08,370
input? How do you determine

1365
00:54:09,270 --> 00:54:10,930
in Lasso regression,

1366
00:54:13,030 --> 00:54:15,006
how do you threshold how many how sparse

1367
00:54:15,038 --> 00:54:18,294
you want it to be? Right, yeah,

1368
00:54:18,412 --> 00:54:20,018
I think there's a lot of good literature

1369
00:54:20,114 --> 00:54:23,046
on this, and even so, some people like

1370
00:54:23,148 --> 00:54:25,538
DembaBa at Harvard. And some people I'm

1371
00:54:25,554 --> 00:54:30,060
working with now have done these

1372
00:54:31,710 --> 00:54:34,790
kind of unrolled iterative

1373
00:54:34,870 --> 00:54:36,938
sparsification networks where it's like

1374
00:54:36,944 --> 00:54:38,346
a recurrent neural network and

1375
00:54:38,368 --> 00:54:40,814
iteratively sparsifies. And you can show

1376
00:54:40,852 --> 00:54:42,874
that this yields. Something like relus

1377
00:54:43,002 --> 00:54:46,666
or. Group sparse

1378
00:54:46,698 --> 00:54:49,406
activations like we're using here in

1379
00:54:49,428 --> 00:54:53,780
this setting. It's really just by having

1380
00:54:56,150 --> 00:54:58,770
this construction of this T variable

1381
00:54:59,350 --> 00:55:04,366
where we have Z on top and

1382
00:55:04,408 --> 00:55:07,800
then it's in some effect gated by

1383
00:55:09,050 --> 00:55:11,254
the sum of U variables in the bottom.

1384
00:55:11,292 --> 00:55:13,126
So, w maybe I wasn't super clear about

1385
00:55:13,148 --> 00:55:16,482
this is a matrix that is connecting.

1386
00:55:16,546 --> 00:55:18,198
That's what defines the groups when I'm

1387
00:55:18,214 --> 00:55:20,154
defining the group sparsity, that

1388
00:55:20,192 --> 00:55:22,106
connects all of these U's together. And

1389
00:55:22,128 --> 00:55:25,338
so the idea is like here.

1390
00:55:25,424 --> 00:55:28,166
If all of one of the other. Examples,

1391
00:55:28,278 --> 00:55:32,394
if all of your U's are not active

1392
00:55:32,522 --> 00:55:33,840
for a given T,

1393
00:55:35,410 --> 00:55:37,838
or if all of your U's are active for a

1394
00:55:37,844 --> 00:55:40,798
given T, that T variable is going to be

1395
00:55:40,804 --> 00:55:42,014
very small, right? Because your

1396
00:55:42,052 --> 00:55:43,394
denominator is going to be very big,

1397
00:55:43,432 --> 00:55:46,594
and that induces sparsity. So it's like

1398
00:55:46,632 --> 00:55:49,106
constraint satisfaction. If you have a

1399
00:55:49,128 --> 00:55:51,826
set of U's that are all small, then that

1400
00:55:51,848 --> 00:55:55,654
that constraint is satisfied. And now Z

1401
00:55:55,772 --> 00:55:57,846
is allowed to kind of express itself and

1402
00:55:57,868 --> 00:56:01,574
that's what then kind of

1403
00:56:01,692 --> 00:56:04,886
achieves the sparse activation. So this

1404
00:56:04,908 --> 00:56:08,206
is induced by these two KL divergence

1405
00:56:08,258 --> 00:56:10,042
terms here. These are saying like, how

1406
00:56:10,176 --> 00:56:13,098
far is each U and each Z from a

1407
00:56:13,104 --> 00:56:15,034
gaussian? And then through this

1408
00:56:15,072 --> 00:56:16,998
construction of the student T variable,

1409
00:56:17,094 --> 00:56:20,578
we're effectively constructing a sparse

1410
00:56:20,774 --> 00:56:22,846
prior distribution just from these

1411
00:56:22,868 --> 00:56:24,910
Gaussians. But in terms of the actual

1412
00:56:24,980 --> 00:56:27,706
objective, the terms and the objective

1413
00:56:27,738 --> 00:56:29,166
that we're optimizing are just these two

1414
00:56:29,188 --> 00:56:31,982
KL terms. That are pushing it towards

1415
00:56:32,116 --> 00:56:34,130
sparsity to some extent. And this is

1416
00:56:34,200 --> 00:56:36,386
balanced automatically with the.

1417
00:56:36,408 --> 00:56:38,258
Likelihood term here through the

1418
00:56:38,264 --> 00:56:40,098
decoder. So we don't have terms that

1419
00:56:40,104 --> 00:56:42,098
we're tuning, but we're learning the

1420
00:56:42,104 --> 00:56:43,646
parameters of. These different encoders

1421
00:56:43,678 --> 00:56:46,370
and then analyzing the failed urgencies.

1422
00:56:48,490 --> 00:56:52,246
Cool. All right. Another question from

1423
00:56:52,348 --> 00:56:56,178
Dave Douglas, who wrote speaking

1424
00:56:56,284 --> 00:56:59,850
of gaze and illusion. Can the studies on

1425
00:56:59,920 --> 00:57:02,682
constancies in infants be separated into

1426
00:57:02,736 --> 00:57:04,902
lower level illusion, relative, perhaps

1427
00:57:04,966 --> 00:57:08,170
higher level conceptual constancy?

1428
00:57:14,070 --> 00:57:16,626
Can you read the current kind of

1429
00:57:16,648 --> 00:57:19,458
architecture? Might the studies on

1430
00:57:19,544 --> 00:57:24,334
constancies in infants cognitive

1431
00:57:24,382 --> 00:57:27,158
constancies, be separated? Yeah,

1432
00:57:27,244 --> 00:57:31,894
probably. I'm not an

1433
00:57:31,932 --> 00:57:33,842
expert or actually even very familiar

1434
00:57:33,906 --> 00:57:37,366
with object permanency studies and.

1435
00:57:37,388 --> 00:57:39,878
Infants and constancy stuff, but I

1436
00:57:39,884 --> 00:57:41,174
think. That would be incredibly

1437
00:57:41,222 --> 00:57:43,414
interesting to study in neural network

1438
00:57:43,462 --> 00:57:45,626
architectures. And that was kind of some

1439
00:57:45,648 --> 00:57:48,218
of the idea with this illusion that I

1440
00:57:48,224 --> 00:57:50,634
was trying to model down here with this

1441
00:57:50,672 --> 00:57:52,442
line. I don't know if I was super clear

1442
00:57:52,496 --> 00:57:54,126
about this, but the top row is the

1443
00:57:54,148 --> 00:57:56,046
input, and we're effectively like

1444
00:57:56,068 --> 00:57:58,186
blocking the input for a single frame.

1445
00:57:58,298 --> 00:58:00,554
And I wanted to see, does the network

1446
00:58:00,602 --> 00:58:03,774
kind of encode that the thing is still

1447
00:58:03,812 --> 00:58:06,066
there when that frame is gone? Can I

1448
00:58:06,088 --> 00:58:07,842
still decode the presence of. The object

1449
00:58:07,896 --> 00:58:10,466
from the neural activity? And then what

1450
00:58:10,488 --> 00:58:12,478
is it also inferring about the motion

1451
00:58:12,574 --> 00:58:15,018
because of the fact that it saw the bars

1452
00:58:15,054 --> 00:58:16,534
at a. Slightly different location than

1453
00:58:16,572 --> 00:58:19,720
from before after the frame is gone.

1454
00:58:21,130 --> 00:58:24,120
So, yeah, I think there's definitely

1455
00:58:24,490 --> 00:58:27,882
multiple levels to it where some

1456
00:58:27,936 --> 00:58:30,940
would probably be much lower level and

1457
00:58:33,150 --> 00:58:35,626
maybe long term object permanency, I

1458
00:58:35,648 --> 00:58:37,174
would guess would. Be significantly

1459
00:58:37,222 --> 00:58:40,746
higher level. It just makes me

1460
00:58:40,768 --> 00:58:43,622
think of those. Experiments with cats

1461
00:58:43,686 --> 00:58:45,854
back in the day. Where it's like they

1462
00:58:45,892 --> 00:58:48,078
raised them in darkness except for an

1463
00:58:48,084 --> 00:58:49,674
hour a day. They put them in vertical

1464
00:58:49,722 --> 00:58:51,646
world or horizontal world where they

1465
00:58:51,668 --> 00:58:53,614
only saw horizontal lines or vertical

1466
00:58:53,662 --> 00:58:57,554
lines, and you can see the

1467
00:58:57,592 --> 00:58:59,714
organization of their cortex changes,

1468
00:58:59,832 --> 00:59:02,466
like they have less receptivity to

1469
00:59:02,488 --> 00:59:04,114
horizontal lines if they've never seen

1470
00:59:04,152 --> 00:59:06,274
horizontal lines before. And then you

1471
00:59:06,312 --> 00:59:07,858
take a stick and you wave it in front of

1472
00:59:07,864 --> 00:59:09,390
their face, and if the stick is

1473
00:59:09,400 --> 00:59:11,506
horizontal, they do nothing. If it's

1474
00:59:11,538 --> 00:59:12,758
vertical, they're swatting at it.

1475
00:59:12,764 --> 00:59:14,134
They'Re trying to hit it, it's like.

1476
00:59:14,172 --> 00:59:15,638
They just literally don't hunzle bar in.

1477
00:59:15,644 --> 00:59:18,246
Front of their face. So I think in that

1478
00:59:18,268 --> 00:59:20,694
case, then, this is evidence of a low

1479
00:59:20,732 --> 00:59:25,174
level efficiency in vision contributing

1480
00:59:25,222 --> 00:59:27,754
to some sort of an illusion. So I think,

1481
00:59:27,792 --> 00:59:29,146
yeah, there could certainly. Be some

1482
00:59:29,168 --> 00:59:31,120
aspect to that in infants as well.

1483
00:59:33,570 --> 00:59:36,286
One very curious point you brought up

1484
00:59:36,308 --> 00:59:41,530
was the animate and inanimate manifold

1485
00:59:41,690 --> 00:59:44,690
with small things being intermediate,

1486
00:59:45,510 --> 00:59:48,130
right? What does that represent?

1487
00:59:49,670 --> 00:59:52,882
Or is it because they're handleable or

1488
00:59:52,936 --> 00:59:55,574
it might be an insect or it might be

1489
00:59:55,612 --> 00:59:57,734
something that might move away just with

1490
00:59:57,772 --> 01:00:00,360
wind or what does that say?

1491
01:00:01,530 --> 01:00:04,886
Right, yeah, so this

1492
01:00:04,908 --> 01:00:07,822
is work by Talia Konkel,

1493
01:00:07,906 --> 01:00:10,060
I think was the one who discovered this

1494
01:00:12,270 --> 01:00:15,146
organization and they tried to figure it

1495
01:00:15,168 --> 01:00:17,482
out. I might be getting this wrong, so I

1496
01:00:17,536 --> 01:00:20,366
recommend people to read her work on

1497
01:00:20,388 --> 01:00:21,962
that. They call it Tripartite

1498
01:00:22,026 --> 01:00:24,030
organization. But if I remember

1499
01:00:24,100 --> 01:00:26,574
correctly, they did a lot of follow up

1500
01:00:26,612 --> 01:00:29,322
work on why there's this organization

1501
01:00:29,466 --> 01:00:33,790
and some evidence of curvature

1502
01:00:33,870 --> 01:00:35,778
of these objects and kind of like the

1503
01:00:35,784 --> 01:00:38,210
distance that you see objects from or

1504
01:00:38,280 --> 01:00:42,114
like animate objects are maybe

1505
01:00:42,312 --> 01:00:45,458
more curvy or regardless of what the

1506
01:00:45,464 --> 01:00:46,670
actual answer is, there were a lot of

1507
01:00:46,680 --> 01:00:48,594
different hypotheses that were stemming

1508
01:00:48,642 --> 01:00:51,270
from properties of these objects.

1509
01:00:52,010 --> 01:00:53,922
Maybe mid level or low level properties,

1510
01:00:53,986 --> 01:00:56,054
more so than higher level properties. I

1511
01:00:56,092 --> 01:00:57,494
still don't know if it's exactly been

1512
01:00:57,532 --> 01:00:59,146
solved of whether it's like,

1513
01:00:59,328 --> 01:01:01,306
interaction, like you said, with the

1514
01:01:01,328 --> 01:01:06,954
objects causes the separation or

1515
01:01:07,152 --> 01:01:10,506
the general shapes of these objects. I

1516
01:01:10,528 --> 01:01:12,426
would bet, as with most things, it's

1517
01:01:12,458 --> 01:01:13,806
like some combination of all of the

1518
01:01:13,828 --> 01:01:17,054
above. But I think the interesting thing

1519
01:01:17,092 --> 01:01:19,200
from this modeling point of view is that

1520
01:01:21,490 --> 01:01:23,962
this is only trained on correlation

1521
01:01:24,026 --> 01:01:26,586
statistics from the image data sets

1522
01:01:26,618 --> 01:01:28,674
itself. This has no interaction, this

1523
01:01:28,712 --> 01:01:30,930
has no notion of animacy.

1524
01:01:31,910 --> 01:01:33,586
I mean, this is really just training a

1525
01:01:33,608 --> 01:01:36,266
model on ImageNet, just images of dogs,

1526
01:01:36,318 --> 01:01:39,206
cats, boats, whatever, and yet it still

1527
01:01:39,228 --> 01:01:41,046
achieves this type of organization. So

1528
01:01:41,068 --> 01:01:43,174
there's some sort of it could be

1529
01:01:43,212 --> 01:01:46,358
semantic characteristics, right? We have

1530
01:01:46,364 --> 01:01:49,174
a network that can classify boats versus

1531
01:01:49,222 --> 01:01:51,690
dogs versus 20 other breeds of dogs,

1532
01:01:52,030 --> 01:01:55,050
but it might also have. Some

1533
01:01:55,120 --> 01:01:56,918
correspondence with lower level image

1534
01:01:56,934 --> 01:02:00,138
statistics as well. So? Yeah. I don't

1535
01:02:00,154 --> 01:02:01,600
know. I guess he's my answer.

1536
01:02:03,970 --> 01:02:07,422
Yeah. Provocative analogy was the

1537
01:02:07,556 --> 01:02:11,642
translational shift in the MNIST

1538
01:02:11,786 --> 01:02:14,290
in the handwriting recognition setting.

1539
01:02:14,870 --> 01:02:18,740
What are the translational shifts that

1540
01:02:20,150 --> 01:02:22,574
exist today? What's the three pixel

1541
01:02:22,622 --> 01:02:24,762
example? Is that some prompt engineered

1542
01:02:24,846 --> 01:02:27,334
attack on an Lom or something? Or

1543
01:02:27,372 --> 01:02:29,334
something. A special character being

1544
01:02:29,372 --> 01:02:33,266
inserted, or some overlay

1545
01:02:33,298 --> 01:02:35,990
on an image that we can't even detect.

1546
01:02:36,970 --> 01:02:39,222
So what do you think those challenges

1547
01:02:39,286 --> 01:02:42,694
are and what are ways that we can pursue

1548
01:02:42,742 --> 01:02:45,754
that? Yeah, absolutely.

1549
01:02:45,872 --> 01:02:49,066
I mean, I think kind of the way I was

1550
01:02:49,088 --> 01:02:50,954
thinking about it is like these symmetry

1551
01:02:51,002 --> 01:02:54,266
transformations. If you're

1552
01:02:54,298 --> 01:02:56,270
thinking about language models, you can

1553
01:02:56,340 --> 01:02:58,014
imagine a symmetry transformation is

1554
01:02:58,052 --> 01:02:59,358
just like replacing a word with a

1555
01:02:59,364 --> 01:03:02,786
synonym or something. You have the

1556
01:03:02,808 --> 01:03:04,722
sentence to us means the exact same

1557
01:03:04,776 --> 01:03:06,706
thing, but now suddenly. The model is

1558
01:03:06,728 --> 01:03:08,370
going to respond very differently.

1559
01:03:11,110 --> 01:03:13,970
Or like translation between languages,

1560
01:03:14,710 --> 01:03:16,246
this can. Be seen as a type of

1561
01:03:16,268 --> 01:03:17,686
transformation. It preserves the

1562
01:03:17,708 --> 01:03:21,746
underlying meaning of the input

1563
01:03:21,938 --> 01:03:24,774
to us, but to the model. It looks

1564
01:03:24,812 --> 01:03:26,166
completely different. And we would like

1565
01:03:26,188 --> 01:03:28,198
to have models which behave in a

1566
01:03:28,204 --> 01:03:29,706
predictable way with respect to these

1567
01:03:29,728 --> 01:03:32,474
types of transformations, because I

1568
01:03:32,512 --> 01:03:34,854
think humans behave very predictably

1569
01:03:34,902 --> 01:03:36,710
with the type of these transformations.

1570
01:03:36,790 --> 01:03:39,110
And when we're dealing with AI systems,

1571
01:03:39,270 --> 01:03:42,358
we expect them to also behave that way.

1572
01:03:42,464 --> 01:03:44,318
And I think that's part of what causes a

1573
01:03:44,324 --> 01:03:47,054
lot of challenges interacting with these

1574
01:03:47,092 --> 01:03:49,822
systems. And I kind of tried to do a

1575
01:03:49,876 --> 01:03:52,894
rough cheeky demonstration of that with.

1576
01:03:52,932 --> 01:03:55,200
This bear and squares and stuff.

1577
01:03:56,310 --> 01:03:59,970
We expect it to be able to do something

1578
01:04:00,040 --> 01:04:01,634
simple like this because we think most

1579
01:04:01,672 --> 01:04:03,618
humans could, and yet it doesn't. And if

1580
01:04:03,624 --> 01:04:05,294
you imagine this is a. Critical scenario

1581
01:04:05,342 --> 01:04:07,574
where you expect this, then that's a big

1582
01:04:07,612 --> 01:04:11,206
problem. How do we handle that? I think

1583
01:04:11,228 --> 01:04:13,382
that's kind of what I'm searching for.

1584
01:04:13,436 --> 01:04:18,934
I think. My direction

1585
01:04:18,982 --> 01:04:24,460
I'm taking it is look more simple,

1586
01:04:24,990 --> 01:04:28,102
kind of like bottom up building blocks

1587
01:04:28,166 --> 01:04:31,134
of neural network architectures or

1588
01:04:31,172 --> 01:04:34,094
algorithms that kind of yield these

1589
01:04:34,132 --> 01:04:36,686
emergent structural properties. And I

1590
01:04:36,708 --> 01:04:38,346
think that's a much more generalizable

1591
01:04:38,378 --> 01:04:40,766
way. Rather than building something on

1592
01:04:40,788 --> 01:04:42,400
top of. What we already have,

1593
01:04:44,070 --> 01:04:45,506
I think that's. Something that would

1594
01:04:45,528 --> 01:04:47,474
scale much better and. Also matches more

1595
01:04:47,512 --> 01:04:48,580
what the brain does.

1596
01:04:50,710 --> 01:04:52,750
Very cool. One kind of implementational

1597
01:04:52,830 --> 01:04:54,766
question. What are the computational

1598
01:04:54,878 --> 01:04:56,910
requirements of just running this, or

1599
01:04:56,920 --> 01:04:59,398
what's the day to day like of being a

1600
01:04:59,564 --> 01:05:01,826
student or researcher running variants

1601
01:05:01,858 --> 01:05:04,326
of these? Like, do they use terabytes of

1602
01:05:04,348 --> 01:05:06,962
data and you're using large computation?

1603
01:05:07,026 --> 01:05:08,874
Or is this something that people can run

1604
01:05:08,912 --> 01:05:12,810
on their own laptops? I think almost

1605
01:05:12,880 --> 01:05:15,018
everything I presented today can be run

1606
01:05:15,104 --> 01:05:18,826
locally. So this stuff is super simple.

1607
01:05:18,928 --> 01:05:22,766
You can run you

1608
01:05:22,788 --> 01:05:24,446
can run it on. Your laptop. If you want

1609
01:05:24,468 --> 01:05:26,494
to train and experiment with different

1610
01:05:26,532 --> 01:05:27,914
things, it's going to be pretty slow.

1611
01:05:27,962 --> 01:05:31,790
So I'd recommend some commercial GPU.

1612
01:05:32,290 --> 01:05:33,806
I run pretty much everything on like

1613
01:05:33,828 --> 01:05:37,214
Nvidia 1080s, pretty old, pretty cheap,

1614
01:05:37,262 --> 01:05:39,794
but they have twelve gigs of Ram or

1615
01:05:39,832 --> 01:05:41,554
whatever, and it's kind of more than

1616
01:05:41,592 --> 01:05:43,586
enough for these models. Gigabytes of

1617
01:05:43,608 --> 01:05:46,834
Ram I think one thing that some people

1618
01:05:46,872 --> 01:05:48,966
think is weird is I do most of my

1619
01:05:48,988 --> 01:05:51,794
experiments on stuff like MNIST. So it's

1620
01:05:51,922 --> 01:05:54,822
32 x 32 pixel images because I can train

1621
01:05:54,876 --> 01:05:58,406
it small. And locally, if you

1622
01:05:58,428 --> 01:06:00,006
want to do. Stuff, my experiments are on

1623
01:06:00,028 --> 01:06:02,474
mnists. If you want to do stuff like

1624
01:06:02,512 --> 01:06:04,070
this, these are much more complicated.

1625
01:06:04,150 --> 01:06:06,762
This Hamiltonian Dynamic suite here,

1626
01:06:06,896 --> 01:06:08,506
you're getting into bigger models that

1627
01:06:08,528 --> 01:06:10,634
are running across multiple GPUs. And so

1628
01:06:10,672 --> 01:06:12,494
here I was using a cluster to run these

1629
01:06:12,532 --> 01:06:15,694
types of models. But I say most of the

1630
01:06:15,732 --> 01:06:18,574
single machine with the GPU is more than

1631
01:06:18,612 --> 01:06:20,986
enough. Or even just like in a collab.

1632
01:06:21,018 --> 01:06:23,930
Notebook, something like that. If you

1633
01:06:23,940 --> 01:06:25,986
want to train something on ImageNet, it

1634
01:06:26,008 --> 01:06:30,226
gets more complicated and you need at

1635
01:06:30,248 --> 01:06:33,154
least one GPU, ideally more. But yeah,

1636
01:06:33,192 --> 01:06:34,766
I don't do a whole lot of big scale

1637
01:06:34,798 --> 01:06:36,610
stuff yet. I think it's certainly

1638
01:06:36,680 --> 01:06:39,302
interesting and there's definitely a lot

1639
01:06:39,356 --> 01:06:41,446
more you can do there. But for some of

1640
01:06:41,468 --> 01:06:44,646
these kind of simpler or more

1641
01:06:44,668 --> 01:06:45,846
fundamental questions, I don't. Know

1642
01:06:45,868 --> 01:06:48,246
what you want to call it. A smaller

1643
01:06:48,278 --> 01:06:49,900
machine is nice and fast.

1644
01:06:52,430 --> 01:06:55,514
Cool, useful. All right. I'll read

1645
01:06:55,552 --> 01:06:59,174
a comment from Dave recalling Bert

1646
01:06:59,222 --> 01:07:01,414
Devries'comment during the Applied

1647
01:07:01,462 --> 01:07:03,646
Active Inference Symposium about the

1648
01:07:03,668 --> 01:07:06,174
desirability of spending less effort or

1649
01:07:06,212 --> 01:07:08,714
ATP on foraging or control situations

1650
01:07:08,762 --> 01:07:10,558
where we don't need much precision. I

1651
01:07:10,564 --> 01:07:12,186
don't know if you listen to this, but

1652
01:07:12,308 --> 01:07:14,450
Professor DeVries mentioned about

1653
01:07:14,520 --> 01:07:17,506
variable precision models and how they

1654
01:07:17,528 --> 01:07:20,402
could be used to enable different

1655
01:07:20,456 --> 01:07:22,450
features of generalization and actual

1656
01:07:22,520 --> 01:07:24,274
structural course training as well as

1657
01:07:24,392 --> 01:07:26,390
reduced computational requirements.

1658
01:07:26,970 --> 01:07:29,686
Does he have any suggestions on how to

1659
01:07:29,708 --> 01:07:31,826
introduce this distinction into active

1660
01:07:31,858 --> 01:07:33,734
inference theory? What kinds of

1661
01:07:33,772 --> 01:07:35,960
experiments could winkle this out?

1662
01:07:38,330 --> 01:07:40,934
Oh, wow. Yeah, that's something I don't

1663
01:07:41,062 --> 01:07:42,618
think I have too much intelligence to

1664
01:07:42,624 --> 01:07:45,420
say about, to be completely honest.

1665
01:07:49,150 --> 01:07:53,050
Hmm. It's super interesting question

1666
01:07:53,120 --> 01:07:55,898
because I think the intuition makes a

1667
01:07:55,904 --> 01:07:58,842
lot of sense to me that you're talking

1668
01:07:58,896 --> 01:08:01,174
about, if I understand correctly,

1669
01:08:01,222 --> 01:08:03,726
variable rates of vision when you're

1670
01:08:03,758 --> 01:08:06,626
encoding or in your model in general,

1671
01:08:06,728 --> 01:08:13,622
doing computation that

1672
01:08:13,676 --> 01:08:17,030
somehow has an impact on your future

1673
01:08:17,100 --> 01:08:19,290
performance as a relation to some energy

1674
01:08:19,360 --> 01:08:22,586
store. I think if you wanted to build

1675
01:08:22,608 --> 01:08:23,962
this into an active inference system,

1676
01:08:24,016 --> 01:08:26,874
you would need to have really an

1677
01:08:26,912 --> 01:08:29,694
embodied system where the agent has some

1678
01:08:29,732 --> 01:08:31,786
notion. Of energy, like an internal

1679
01:08:31,818 --> 01:08:33,440
energy store, and.

1680
01:08:35,250 --> 01:08:37,018
Something that is trying to conserve

1681
01:08:37,114 --> 01:08:40,830
while it's performing its actions and

1682
01:08:40,900 --> 01:08:42,258
running out of energy would need to

1683
01:08:42,264 --> 01:08:44,546
mean. Something bad for the agent. And

1684
01:08:44,568 --> 01:08:47,266
then maybe you could observe kind. Of an

1685
01:08:47,288 --> 01:08:50,900
emergent reduction in

1686
01:08:51,990 --> 01:08:54,994
encoding precision or something like

1687
01:08:55,032 --> 01:08:57,750
this as the agent is trying to learn to

1688
01:08:57,820 --> 01:09:00,086
act more. Effectively, you might have to

1689
01:09:00,108 --> 01:09:02,294
give it. An ability to control its

1690
01:09:02,332 --> 01:09:05,686
precision. Yeah, like I say, out of

1691
01:09:05,708 --> 01:09:08,390
my thoughts.

1692
01:09:09,070 --> 01:09:11,034
Okay. On this slide right here, first,

1693
01:09:11,072 --> 01:09:13,420
very cool image. It's kind of like a

1694
01:09:14,190 --> 01:09:17,130
digital Jackson Pollock.

1695
01:09:19,410 --> 01:09:23,614
If it were a simpler input data size

1696
01:09:23,732 --> 01:09:26,666
or just reduced complexity of patterns,

1697
01:09:26,698 --> 01:09:28,122
or if it were an increased complexity,

1698
01:09:28,186 --> 01:09:30,560
how would this image look different?

1699
01:09:31,830 --> 01:09:34,114
Yeah, so I did some experiments trying

1700
01:09:34,152 --> 01:09:37,630
to change these orientation

1701
01:09:37,710 --> 01:09:42,258
columns and basically

1702
01:09:42,344 --> 01:09:43,922
changing the parameters of the model.

1703
01:09:43,976 --> 01:09:46,418
You can get these columns to be bigger.

1704
01:09:46,514 --> 01:09:48,454
You can get them to not have very

1705
01:09:48,492 --> 01:09:50,806
similar structure to what we see in the

1706
01:09:50,828 --> 01:09:52,726
humans, where you can get them to have

1707
01:09:52,748 --> 01:09:56,694
more bands of activity. And it

1708
01:09:56,732 --> 01:09:58,138
also, like you said, it depends on the

1709
01:09:58,144 --> 01:10:00,586
data set that you're using. If I use

1710
01:10:00,688 --> 01:10:03,354
really simple sinusoidal gradings as

1711
01:10:03,392 --> 01:10:05,466
input, I get something like this. I get

1712
01:10:05,488 --> 01:10:07,020
something that's a little bit more

1713
01:10:07,950 --> 01:10:10,830
rotational, curvy, higher entropy.

1714
01:10:12,690 --> 01:10:15,406
So I think these are all interesting

1715
01:10:15,508 --> 01:10:17,726
things if you want to study the

1716
01:10:17,748 --> 01:10:19,162
emergence of this type of organization

1717
01:10:19,226 --> 01:10:22,290
in a natural system. If you have a model

1718
01:10:22,360 --> 01:10:24,590
that now yields different organization

1719
01:10:24,670 --> 01:10:27,474
for different settings that see,

1720
01:10:27,512 --> 01:10:29,762
okay, then what settings best match our

1721
01:10:29,816 --> 01:10:30,900
observed data?

1722
01:10:34,490 --> 01:10:36,466
I can send those around if you're

1723
01:10:36,498 --> 01:10:42,148
interested. But I

1724
01:10:42,154 --> 01:10:45,444
think one other interesting point there

1725
01:10:45,482 --> 01:10:49,548
is that the different animals and types

1726
01:10:49,584 --> 01:10:52,936
of orientation selectivity and

1727
01:10:52,958 --> 01:10:54,184
different numbers of pinwheels. Some

1728
01:10:54,222 --> 01:10:55,864
animals don't have it at all. I think

1729
01:10:55,902 --> 01:10:58,456
maybe mice, if I'm correct, have this

1730
01:10:58,478 --> 01:11:00,664
kind of they call it salt and pepper

1731
01:11:00,712 --> 01:11:02,536
selectivity. So it's basically random.

1732
01:11:02,568 --> 01:11:03,516
You don't have any sort of, like,

1733
01:11:03,538 --> 01:11:06,940
topographic orientation selectivity. So

1734
01:11:07,010 --> 01:11:09,884
there is evidence that different

1735
01:11:09,922 --> 01:11:12,236
systems. Do this differently, and it's

1736
01:11:12,258 --> 01:11:15,440
interesting to figure out why. Yeah,

1737
01:11:15,590 --> 01:11:17,696
this very cool. It reminds me of,

1738
01:11:17,798 --> 01:11:20,240
first, the reaction diffusion,

1739
01:11:21,380 --> 01:11:24,610
space and time. So it's actually

1740
01:11:26,200 --> 01:11:29,892
possible that a region might have no

1741
01:11:29,946 --> 01:11:33,936
activity from a given granularity,

1742
01:11:34,048 --> 01:11:37,072
like if it was being looked at at fMRI

1743
01:11:37,216 --> 01:11:40,040
spatial and temporal timescale.

1744
01:11:43,180 --> 01:11:46,184
But if the pockets of activity are

1745
01:11:46,382 --> 01:11:50,288
slower, faster, then that measurement

1746
01:11:50,404 --> 01:11:53,064
is going to not be different than noise.

1747
01:11:53,112 --> 01:11:55,230
It'll all have been averaged out.

1748
01:11:55,840 --> 01:11:59,310
So then there might be some interesting

1749
01:12:00,400 --> 01:12:02,990
data sets that do actually have a lot of

1750
01:12:03,360 --> 01:12:06,076
richness, but then for one reason or

1751
01:12:06,098 --> 01:12:08,560
another, it just was averaged out over

1752
01:12:08,630 --> 01:12:10,816
because it wasn't being connected. To

1753
01:12:10,838 --> 01:12:12,176
you or something like this. You really

1754
01:12:12,198 --> 01:12:13,888
need to go at the single trial level.

1755
01:12:13,974 --> 01:12:15,516
You need to have high enough spatial

1756
01:12:15,548 --> 01:12:18,672
resolution such that it satisfies

1757
01:12:18,736 --> 01:12:20,340
Nyquist frequencies.

1758
01:12:22,200 --> 01:12:23,764
And this just is something that people

1759
01:12:23,802 --> 01:12:25,236
didn't do for a long time, especially if

1760
01:12:25,258 --> 01:12:26,256
you're doing single electrode

1761
01:12:26,288 --> 01:12:28,036
recordings. You're not going to see a

1762
01:12:28,058 --> 01:12:29,076
traveling wave. You're going to see

1763
01:12:29,098 --> 01:12:31,668
oscillations. You need, like, multi

1764
01:12:31,684 --> 01:12:33,716
electrode arrays. And basically they're

1765
01:12:33,748 --> 01:12:35,256
saying, okay, yeah, now that we have the

1766
01:12:35,278 --> 01:12:37,130
technology to do. This, it's much

1767
01:12:37,500 --> 01:12:39,912
resists that we didn't see before. And

1768
01:12:39,966 --> 01:12:42,170
potentially, this is an explanation for

1769
01:12:42,480 --> 01:12:44,204
a lot of the noise that we were seeing

1770
01:12:44,242 --> 01:12:45,612
before. Maybe it really is just

1771
01:12:45,666 --> 01:12:49,244
traveling waves. So yeah,

1772
01:12:49,442 --> 01:12:51,628
I think there's a lot to be done in the

1773
01:12:51,634 --> 01:12:54,992
future with. Increased abilities for

1774
01:12:55,046 --> 01:12:57,730
recording. That's very cool.

1775
01:12:58,660 --> 01:13:02,624
Well, any final thoughts or questions or

1776
01:13:02,822 --> 01:13:05,330
where are you going to take this work?

1777
01:13:06,260 --> 01:13:08,290
Yeah, no. Thanks for having me.

1778
01:13:10,020 --> 01:13:11,468
Hopefully in the active infrastructure

1779
01:13:11,484 --> 01:13:14,304
direction. I would love to. I think

1780
01:13:14,342 --> 01:13:16,388
it'll be super fun. So yeah, I'm not

1781
01:13:16,394 --> 01:13:18,730
really sure. I'm looking at maybe music

1782
01:13:20,060 --> 01:13:23,690
right now, looking at

1783
01:13:26,780 --> 01:13:29,668
other kind of crazy directions.

1784
01:13:29,844 --> 01:13:31,550
I don't want to sound too crazy,

1785
01:13:33,280 --> 01:13:36,492
but I'll go down. Yeah, a lot of things.

1786
01:13:36,546 --> 01:13:38,284
So one thing that's coming up,

1787
01:13:38,322 --> 01:13:39,804
something we submitted to Neurops is

1788
01:13:39,842 --> 01:13:42,140
studying memory with traveling waves.

1789
01:13:43,200 --> 01:13:45,304
So that paper just came out on. Archive

1790
01:13:45,352 --> 01:13:48,236
today of how waves are really good at

1791
01:13:48,258 --> 01:13:50,128
encoding long term memories, which. I

1792
01:13:50,134 --> 01:13:53,136
think is super interesting. So I might

1793
01:13:53,158 --> 01:13:54,560
go a little more in that direction.

1794
01:13:55,800 --> 01:13:58,820
Sounds good. And yes, would be very

1795
01:13:58,890 --> 01:14:01,524
exciting to see action come into play

1796
01:14:01,722 --> 01:14:04,384
when there was the neurons that stayed

1797
01:14:04,432 --> 01:14:07,556
active even as the dog's feet were

1798
01:14:07,578 --> 01:14:10,432
moving. There's a lot of action

1799
01:14:10,496 --> 01:14:13,584
sequences, like throwing a baseball,

1800
01:14:13,632 --> 01:14:15,536
and then it goes, and it's like there's

1801
01:14:15,568 --> 01:14:17,024
something about that action that's

1802
01:14:17,072 --> 01:14:20,476
continuing to influence it's to having,

1803
01:14:20,498 --> 01:14:23,020
like, a deep temporal representation of

1804
01:14:23,090 --> 01:14:24,540
alternative actions.

1805
01:14:26,160 --> 01:14:29,452
And then the variational auto encoder is

1806
01:14:29,506 --> 01:14:33,100
already basically the right anything

1807
01:14:33,170 --> 01:14:36,444
like that. Really appreciate it.

1808
01:14:36,562 --> 01:14:38,670
All right, thank you. Till next time.

1809
01:14:39,520 --> 01:14:40,620
Thanks so much.



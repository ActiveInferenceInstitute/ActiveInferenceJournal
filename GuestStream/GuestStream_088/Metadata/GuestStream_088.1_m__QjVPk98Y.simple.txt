SPEAKER_00:
hello and welcome everyone this is Active Inference guest stream 88.1 on September 17th 2024

We're here with Hunar Abdulrahman and we'll be discussing what is the limit of our brains.

There's a fun backstory here maybe we'll go into.

There will be a presentation followed by a discussion.

So I'm very much looking forward to it and to reading people's comments and questions too.

So Hunar, thank you again for joining and to you for any backstory and introduction and for the presentation.

So thanks again.


SPEAKER_01:
Thank you, Daniel, for inviting me to this nice stream.

Actually, it's the first time, as I told you before, we start to do a live stream using this software.

So I hope the audience will forgive us if there is any upcoming technical problems.

Hopefully there will be not.

So thanks a lot for inviting me again.

My name is Huner.

I have a PhD in neuroscience.

I spent many, many years trying to learn how our brain works using functional MRI EEG.

So I did my PhD at, I get my PhD from the Cambridge University.

And we will be talking a bit about what I did in my PhD and how I try to link the concepts that I learn about brain with the outside world.

with other concepts like the free energy.

So I'm actually a newcomer to the free energy principle.

I hope that people with technical background forgive us if I have any technical errors, but hopefully maybe I'll try to learn and share my insights about what I have discovered from my searching in the past few years.

So can you see the slides if I start?

Great.

So the topic is actually about what's the limit of our brain and I will try to talk about two concepts.

I will talk about some few concepts and later on we will link them all together.

So first I will try to talk about the brain as a predictive machine, usually the predictive coding account, then I will try to ease into the free energy principles.

And then later on, I will try to talk about what makes prediction difficult.

I will talk about extrinsic factors.

So even if we have the best learning machine, the best machine learning algorithm, or even if our brain is perfect in learning, there are still a lot of

difficulties basically because of computational irreducibility principle which i will be talking more about it in the upcoming slides so i will try to i i would love to start with repetition suppression it's a well-known concept in neuroscience so uh if if somebody wants to go into the details what's repetition suppression i will recommend to you uh this article by my phd supervisor rick henson

uh but it's a bit difficult to wrap your head around the results so i'll try to make it easy basically what repetition suppression means the activity that we record from our brains whether using eeg fmri or

single-cell recordings.

For example, if you see a sequence of pictures of elephants, and then later on you will see something surprising or novel, like the kangaroo image here, the activity of our brain increases.

However, when you see repetitions of the same image again, like the kangaroo here, the activity decreases.

That's what we call a repetition separation.

If you repeat something, the activity of your brain, your brain will not be responding in the same way it responded the first time.

then you will see something new like this beer here.

Because it's novel, the activity of your brain increases again.

So this repetition suppression is not only for repetition.

Sometimes we call it expectation suppression.

So even if something is not repeated but not expected, the activity of your brain increases.

But if you expect something, so usually repetition and expectation are

synonymous if something repeats a lot it will be expected if something is novel it's unexpected so these results you will be finding in whether repetition studies or expectation studies they will tell you if you see something unexpected or novel the recordings of your brain increases if something becomes

expected or repeated the recordings degrees so that's the repetition suppression now uh during my phd we we saw a very interesting paper titled less is more expectations sharpens representations in the primary cortex so so normally a neuroscientist will be interesting why this happens why repetition suppression is there so this paper

proposed a new idea, which actually a bit surprising for us.

So the paper tested these are grating orientations.

The subject will see different orientations.

Sometimes it's expected, sometimes it's not expected.

So the results they found is that the activity of the brain decreases when something is expected or

so this was expectation study not repetition study but the same usually the results will be similar so the activity decreases as you see here uh i i you don't see my mouse can you see my my cursor oh okay so the activity decreases however for classification uh

They tested classification accuracy, so they have like two orientations.

If you repeat something, if you use machine learning to classify between orientations, in the repetition, the classification accuracy is more.

So let me see.

uh okay so during uh this is the classification the the bottom panel is about the classification accuracy so as you see in the repetition of the classification accuracy increases and the very explanation was that because sharpening happens so i will give you an ex an illustration so this is not the exact experiment but this illustration so for example here you see the number five

If you repeat that, basically, it will become sharper.

I mean, the neurons, that's the concept, that are not directly representing number five will be suppressed.

So when you repeat something or when something becomes expected, the non-interested activity will become suppressed.

So that's their explanation.

Basically, it's sharpening.

The image will become sharper.

I think this is intuitive explanation.

However,

we uh so during my phd we did have a lot of data set about repetition so two different data sets we have one data set was

pictures of faces versus scrambled faces were shown to the subjects.

So you see here two faces, similar, repeated.

And then you see here two scrambled faces repeated.

We also have the data for gratings, orientations.

It's similar.

We have either repetitions or novel.

So what we found in our results is same.

The repetition, the activity decreases, we saw.

But for the phase dataset, we saw that even the classification accuracy also decreases.

So it means like when you repeat something, the brain activity decreases.

But for the gratings, we saw that the classification increases.

So the same result as the previous group, which I talked about, they found.

They explained that as sharpening.

So how can we explain this result?

Because for the grading data set, we see the classification accuracy increases, so we can say, oh, it's sharpening.

But now for the faces, the classification accuracy decreases.

So what is that?

So we thought it would be very, very difficult without simulations just to explain the data with our minds.

Intuition doesn't work, to be quite honest, without a simulation.

the reason because functional mri is uh even though it's high resolution it's still very very low each pixel if each voxel here represents the activity of millions of neurons averaged together so so basically when you are trying saying that uh sharpening it means that then interest neurons will become less active we are talking about the average activity of many many many neurons and it will be very very difficult to intuition about that without simulation

So this representation will tell you, like, for example, here we are talking about different representations of the data.

It will be very, very difficult to conceptualize what happens really in the underlying mechanism.

So what we proposed was we thought like maybe classification accuracy also decreases for local scaling or local.

So what people think, you see here in the first panel.

So I'm just wondering, can you see the mouse when I am moving it?


SPEAKER_00:
Yeah, it's small, but we can see it.

I'll also go with my mouse too.


SPEAKER_01:
Because I have two screens, to be quite honest.

I mean, which screen you see the mouse moving so that I... Yeah, we see the mouse on P1 uniform.

Okay, okay, so this screen, okay, I just, okay, great.

So five here, if you repeat the five, people usually think that suppression will happen uniformly.

So you can see the five suppressed with all the videos.

So sharpening will tell you, no, the suppression will not happen uniformly.

It only happens with the non-interested neurons or neurons that doesn't represent

five but we say that even if neurons for example the neurons that represent five even we if we selectively suppress those neurons like in the bottom panel we may still find increase in classification accuracy depending on how many voxels or we suppress so basically this was our conceptual idea before simulations

So how do we simulate data?

I mean, how do we go from the neural activity to the functional MRI?

Because the simulation is kind of difficult because we are trying to express neural activity in the functional MRI.

So one idea is, for example, this is from neural activity studies from a single neuron.

Let's say this neuron is very active.

uh for uh orientation if if the orientation is 90 degree so you see a lot of spikes here and and the if you if you represent that with a tuning curve you will see that it will have maximum response when the orientation is at 19. but but the response of the neuron will be less if you go because we will say this neuron is very selective to the orientation 90.

So the way we simulated that, we simulated that using orientation curves.

So let's say this orientation curve is selective to the horizontal orientation, but it's less selective to the other orientations.

What sharpening means, it means it becomes sharper.

When we repeat that, the neuron will be more selective to the horizontal selector.

So this is less sharpening, this is more sharpening.

So that's how we are trying to simulate that.

Now, on the other hand, scaling or dampening, it means like the activity will be decreased.

The activity of the neuron will be increased for the horizontal stimuli.

Now we have a lot of neurons, not one neuron, so this was only a representation for one neuron, but we have a lot of neurons.

Some neurons are responding to the orientation 19, some neurons are responding to the horizontal orientation, and some in between, and there is overlapping.

And when we want to simulate a voxel, we usually sum this line, the activities on this line.

So I don't know if it is clear or not the procedure that we are trying to do.

So I hope you interrupt me if something is unclear.

So when we sum the activities, we will have a representation for the activity of a voxel.

Now, in this case,

We talked about sharpening and scaling, but we may have a global effect.

The global effect means everything will be suppressed, everything.

So this is the global effect.

The local effect, it means no, selectively a part of neurons will be suppressed.

So this is similar to the sharpening.

we call it remote effect it means that the neuron that responds to the horizontal stimuli it won't be suppressed but the neurons that respond to other orientations they will be suppressed according to their distance so this is a parameter that we can fine-tune

And local separation, it means the opposite.

The neuron that is selective to the horizontal stimuli, it will be suppressed.

So you see, there are many ways that we can simulate the repetition separation.

so we identified six ways to to represent repetition suppression either we have global so the red one represents the after repetition what happens to the neural tuning curves after repetition so either the global everything is suppressed or everything is sharpened become narrower

Or we have local, the stimulus of interest, the effect will be more concentrated on the stimulus of interest.

Or we have remote, the effect will be more concentrated on the neurons that are far away from the stimulus of interest.

So for the completion, we actually examined six other models.

We call them shifting models, but they are not important because we don't think our brain will behave like that, but just for completeness.

So for those who want to do a mathematical modeling, it's just like for here, G is the tuning curve, which is represented by x and mu, which is the mean of the tuning curve, and sigma, which is how much narrow is the tuning curve.

And then we multiply it by a factor, which is C. That's the suppressing factor.

So in local scaling, we suppress the tuning curve.

In sharpening models, we suppress the sigma or how much narrow it is.

And then in shifting models, we shift the tuning curve.

So for more technical details, you can actually go to our paper and see more details about how did we simulate that.

As I said, we have two datasets, the phase dataset and the grading dataset.

So what we want, we want to see if we can use our simulations to match our findings.

And actually using functional MRI, we actually identified some other features.

We didn't only use repetition, suppression and classification.

We identified six more data features and we want to see if we can actually use the simulations to explain that.

And the results which we found was very, very interesting.

so these green uh circles it means that that specific model so remember we we examined 12 models in total 12 different models so if it is green it means this model can explain this functional mri feature

If it's red, it means it can't explain.

So for global scaling, you can see it can explain a lot of features, but except BC, which I didn't have time to.

These are functional MRI features.

You can see this is the data and we are trying to fit it.

So the interesting thing is.

you only see local model local scaling can actually explain all the features so you don't see any red circles here for both deficits so it can qualitatively explain all the features that we saw in our data

all the other models they they can individually they can explain individual features for example here sharpening model can explain repetition separation can explain within correlations can explain between correlation but it can't explain for example the features of sharpening in our phase data set

So the short story long, as the long story short story, we find that local scaling best represent the findings of our data.

So basically the local scaling, it means that the best model is the model that actually predict when we repeat the neuron that are selective to that feature, it will be suppressed the most, not global and not sharpening,

the the suppression of the neuron that so basically what that means if you see a horizontal orientation the neurons that are specific to this uh horizontal orientation they will be suppressed more the more selectivity to that they will be more suppressed

that there are two reasons we like the results like that.

So basically later on the paper, which we actually, it was just later, they did similar simulation to us, and they actually found similar conclusions that this model, this new one is, this model is best explains our dataset.

So the reason we like local tailing or dampening, it's because it's very parsimonious.

It can be explained easily without instigating complicated explanations.

So basically, it just means that synaptic fatigue.

So when you stimulate a neuron, when a neuron is stimulated, it will get tired.

then it will be more suppressed when you repeat something so this is a parsimonious explanation and the second reason why we like this result because now i'm trying to connect it with predictive coding literature and with the free energy principle first with the predictive coding literature

we think this result is more in line with web because you see in in predictive coding literature you usually see this image a lot you will say oh you have a stimuli and then you have a prediction the lower upper level of your brain try to predict and there will be an error and you have a prediction error so the question is not what this prediction error represents we think that these results will tell us that the prediction error is the unexplained data so it's

nowhere says that the prediction error is the noise it's usually the non-explained stimuli so the part that you can't explain and that we think is actually more in line with with the suppression with the with this model i will actually come back to this uh later uh but but uh first i want to talk about so we established that our brain is a prediction machine and

And the results in our functional MRI actually supports that our brain seemingly is not interested in expected stimuli because it has predicted that.

Our brain is trying to predict the unexpected.

That's why more activity is being exploited for novel and unexplained stimuli.

So what's the enemy of prediction?

So if our brain is a prediction machine, what are the obstacles in front of our ability or our brain to predict things?

So I will tell you a story that I faced.

I'm usually interested to train MRI.

At some point of my life, I was trying to train a neural network to predict Mandelbrot set.

And the reason why I thought Mandelbrot set is more interesting than language predictions predicting the next token, because from Mandelbrot, you actually know the ground truth.

The ground truth is, actually, if you copy this JavaScript line into JavaScript,

it will produce a Mandelbrot.

It's very, very easy.

It's a few lines of code, just one equation.

It will produce.

And then when you change these numbers, you can zoom in to this Mandelbrot set.

So for Mandelbrot set, I know actually what's the ground truth.

It's not like the language.

For language, I don't know what's the best ground truth.

Is my sentence is the best?

Is your sentence?

Which phrasing is the ground truth?

So it's very difficult to identify the ground truth in language models.

But for Mandelbrot, we know what's the ground truth.

And that's why I was interested to train neural networks to predict Mandelbrot's set.

So the surprising thing for me, I thought this will be easy because Mandelbrot has a lot of regularities, a lot of places where it repeats.

So I thought this will be an easy task.

i was shocked i mean even when i used a neural network with very very big size to predict different parts so you can train a neural network to do to predict that specific part that's easy but when you want to generalize it will be very difficult the reason is because i was underestimating motherboard there are it's true that there are a lot of regularities but the more you zoom in there are

there will be always new features it's mind-blowing how this simple line of recursive they will always present you with something new that your model or the neural network no matter how good how good is trained it can't predict any anything so

So the puzzle for me, actually, we know that recurrent neural networks are Turing-complete.

So it is possible that you can train a neural network to simulate that code.

But in that case, the neural network will not help you in degradation.

It will just simulate the same code for you.

the the main hope that we have with neural network is that we predict ahead of time we predict two steps ahead of time or three steps ahead of time not not just recursively uh predict one step because

that's what uh which i came later on on computational it's the first of that uh so so before coming to computational irreducibility so this is my first time when i saw there is a challenge in prediction it's it's it's it's very difficult uh to predict these intricate features never ending emergent patterns

So one of the reasons, this is phrased by Stephen Wolfram, is the principle of computational equivalence.

So basically, if you manage to train a recurrent neural network to simulate the same rule that actually generates Mandelbrot, it means that at the underlying level, they are doing the same thing.

They are doing the same computation.

so you basically according to this principle you cannot simulate a rule with another rule that are more efficient that basically if you if you simulate them at their most basic level so you can simulate rule a with rule b and then rule b can actually predict two steps ahead of rule b that's not possible

So this may be difficult to understand, but the way Stephen Wolfram did it, he explained that in his TED talk.

So he actually, so basically in his TED talk, he was like, I'm paraphrasing, he was like saying, okay, you have this simple rule.

So this is just like a Mandelbrot rule.

You just give this rule to computer and then it will produce for you a pattern.

These are step-by-step patterns.

And then it will tell you if you could, is it possible to predict ahead of time just from knowing the rule?

I mean, you know the rule, just like we know the theory of everything.

Can we actually predict ahead of time without simulating the intermediate steps?

And I actually don't know if it's true or not, but I believe that because Stephen Wolfram has spent like his decades of his life trying to do this.

And he says that this is not possible.

You have to compute all the steps.

You have to compute all the steps.

And he even put a price for that.

The reason why I believe computational irreducibility principles should be, it must be true, because in essence, it's similar to two other principles, Godel's incompleteness theorem and the halting problem.

In spirit, if you think more about it, you will see that these are all talking about probably the same phenomena, but in different.

So what I learned is that complexity is easy to generate, actually.

It was a very shocking thing for me.

But I have been simulating these simple rules for a while, and they always surprise me what they can produce.

Very simple rules, you fine tune them, and you can generate.

But the problem is it's hard to understand.

And this is because of that principle which was formulated by Stephen Wolfram, which says computational irreducibility principles.

So basically, let's say we have two systems, system one and system two.

And then system one tries to predict system two.

So if system one can predict system two with less computational power, then we can say that system two is reducible.

And this system one is analogous to our brain and system two is analogous to the environment and the world which we are trying to learn about.

So what Stephen Wolff found that he examined so many different rules and the reason why he examined actually cellular automata because of their simplicity.

So what he found that is each rule are producing unique patterns.

So actually, by looking at these patterns, you can see what computational equivalence means.

It just means that you can stimulate, for example, this pattern using another rule.

Now, there are some rules that are faster than itself.

That's what I mean.

Now, there are rules that are truly complete, like the rule 110.

But even if you manage to initialize the rule number 10 to produce another rule, it will not produce it faster.

I mean, you can't use it two steps ahead of time.

So some of these rules will be trapped into repetition and some of these rules will never be able to predict what comes next.

And I thought there is another phenomena like that.

It's the irrational numbers versus rational numbers.

They are actually similar to that because we have irrational numbers.

And their definition is, if you go to the definition, they will never trap into repetition.

So we have pi, which is a very simple rule.

It's just the circumference divided by radius.

It's a very, very simple rule.

If you simulate that rule, it will always give you a new pattern, a new number.

the same for the square root of two three so this is amazing i mean i i don't know why i didn't thought about that before stephen wolfram talking about uh cellular automata thing because i thought like these are actually have the same amazement so if you have a generative

And actually, we usually talk about universality, Turing complete universality.

So they will tell you if you have an infinite tape, and if you have a reader, a writer, and specific rules, you can compute everything.

And they will call that Turing complete universality.

now according to steven more from he actually revised the what what does it mean by computation or universality so so he usually uh explains that in the term of cellular automata but i will try to explain that in in the term of rational and irrational numbers so it's actually a bit mind-blowing but we we usually don't think about it like that so let's say pi that's the definition a definition means this rule will never trap interest unlike the rational rules so the rational uh numbers

if you divide them at some point they will repeat they will repeat they will never give new patches but but irrational numbers so that means

if you if you compute pi for infinity it is possible that at some point you will find the same pattern that the square represents the square root of two so so if you if you compute pi for infinity it's possible that you can find a space of one one one one one but but it will never trap into that option at some point it will it will change so so that means in every irrational number you have

The other irrational numbers embedded in it, there is no way to say this is not true, because if you say this is not true, it means that these patterns will be trapped at some point into a cyclic.

repetition but if they don't trap they will always give you novelty and that means so someone can tell you pi is too incomplete why because he says like if you if you if you keep computing pi for infinity at some point you will find that it computes my genetics the same sequence that if you represent it as a number and i i mean you can say this is not true

Can you?

I mean, do you have a comment, Daniel?


SPEAKER_00:
Oh, it's a great connection.

It just makes me think like within Pi is every possible operating system, every possible note file you could write to yourself.

So it's like it contains all of these things.

And also it's a great connection with the computational classes from Wolfram.


SPEAKER_01:
exactly exactly because going off from usually represents a rule number 30 it says that oh this rule number 10 you can see all patterns are there so if you go for infinity i think it's a computational universality is there it's just you just can't formalize and i think it's the same to be quite honest i mean if you go if you say it's it doesn't repeat it means that it uh even though the rule is very simple five is the simplest possible rule it's just a certain price divided by

radius it's as simple as possible and then you have unimaginable sequence patterns so okay this is interesting but this is also problematic because it tells you a prediction with 100 accuracy is impossible remember we talked about the super intelligence we say that oh agi is is actually near but many people will say oh if agi is near then we will have

intelligence asi artificial super intelligence will be just in the corner about the corner i don't think so because of computational disability it means that agi will also face the same problems that the problem is you have to compute to beat this system you have to you have to have a universe-sized computer that simulates our universe faster and i don't think agi can have that

because it lives in the same environment like us, so that it reaches the same ceiling that we actually reached, which means that at the age of unknowledge, at the age of uncertainty, the AGI will also suffer from uncertainty, just like us.

So, okay, so I talked about that, that even if we know the rule of everything, so I was, when I was younger, I was always following the physics news because I was very, very happy to see if a physicist will find the theory of everything because I thought, oh, it's even...

If Stephen Hawking can find the theory of everything, that will be the happiest day because we can explain.

But now I see this is not possible.

I mean, even if you know that, even if you put theoretically, I thought like this is the difficult part, but this is not even the difficult part.

Even if we say this is easy, we have all the atoms, the state, their velocity, their position inside our computer, or even if we have a very powerful computer, that computer.

that compute and predict the next state it will just produce the next update which is the next picosecond or whatever is the smallest type of the plank level so that means it means to to simulate to predict the future you have to simulate the future you have to simulate the present faster than the universe and that's impossible unless we are outside the universe and we have we can somehow simulate everything faster than our universe

so okay so how do we how do we do that i mean in the beginning i told i told you that our brain is a prediction machine you know we can predict things so how prediction is possible in this system the only way i can think that prediction is possible is because two things memorization and abstraction the reason why i didn't put memorization before abstraction because memorization is not very without abstraction memorization is not useful

Because memorization, it means if something repeats, then if you memorize it later on, you just catch that, and if something catches that, it will be computationally very easy to read it.

But that will be possible if we live in a computationally reducible universe, if something repeats.

Memorization will be great.

But if we live in a computationally irreducible where there is always emergent pattern, memorization doesn't help.

The reason is because you should have infinite sized memory to store all the emergent patterns.

You have to have abstraction with memorization.

Just like, so remember that previously before AI, people stored pixels inside the computer just using memorization.

But what neural networks does is they memorize, but not just using that.

It will abstract the information before memorization.

So you just don't memorize the data like itself.

You have to find a way to find the useful.

So that's the formal definition.

I actually gave that to Jack Chifty.

And that system, which I like it actually more than the Wikipedia definition, is the process of reducing complexity by focusing on main ideas and acknowledge specific details.

So that's the only way that we can beat the computational irreducible.

It's just we pay attention to what matters for our survival.

So you can see I am trying to actually reconnect back to the active inference

So there are many ways to abstract.

There are many different algorithms.

But I think the easiest one is that's what they usually introduce you in category theory.

They just say, oh, we just gave the same name to everything.

So let's say we have Bob, Ali, Joe.

And if you even want to do statistics or if you want to find any relationship, we just need to group things according to their similarity.

so similarity is actually a very complex topic it's not just easy to say according to similarity but but they will be right now so coarse graining is actually simpler because coarse graining you group things according to proximity proximity is easier to define that similarity you just say if things they have a euclidean distance them a specific threshold you just group them so coarse graining actually used everywhere i i'm surprised because grain is not

not so much well known, but it's used everywhere.

Another thing which may be more familiar to your group is Markov Blankets.

I think Markov Blankets is also a more sophisticated way of abstraction.

The reason why Markov, for example, here you see these nodes, the parent nodes, the children nodes and the partner nodes, and the internal nodes.

So when the Markov Blanket becomes stable, you can just treat this as one entity.

So just like,

another way of abstraction so we say anything inside this membrane is just one entity anything inside this house the house is one entity with everything that happens inside I don't care about only when somebody comes outside the house I should care about but when they are inside about I should just look at the house so my point is that the stable membranes can abstract things for us

Linear regression is another way to abstract information, but linear regression is just a more sophisticated way of coarse-graining because we are using a weighted.

In coarse-graining, we just average the nearby information, but here we are doing weighted summation.

So we have neural networks as well.

Neural networks are the most sophisticated way of abstraction.

so far uh so so coarse graining let me just give you a bit about coarse grain the importance of course gaining so i mean it's self-explanatory you just average the nearby pixels or the nearby uh so so what i think about uh talk about uh why coarse grain is important there is a simulation called part life simulation i have a video about that a little bit but let me just i don't know why this particle simulation is not

more uh familiar among the scientists uh it's it's actually a more interesting extension of the game of life i don't know who invented that but there are old videos uh seven years ago doing these particle simulations i didn't have the code back then when i wanted to re-stimulate that so i improvised the code and i was shocked the reason why i i just wanted to simulate a primordial soup

where you have some particles attracting each other.

So you can see these few lines of code.

These few lines of code just represent particles attracting to each other according to Newton's law, something similar to Newton's law.

So the inner loop here just says if the distance is this, you have an attraction force or a repulsion force.

parameterized by G. So the G parameter actually determines the attraction of repulsion forces.

So just less than a page of code.

And when you have this rule, you can simulate this rule.

For example, you can specify how green particles attracted to each other and how red particles attracted to each other.

So just a few lines of code, and you will have so many interesting patterns.

so many interesting patterns so so we we have made a c plus plus software about that it's available on our github uh so it's it's just amazing how complexity is easy to generate so you just have these parameters are just attraction and reposition between different color particles so so you just define how much red particles attracted to read particles

and then you let the simulation run and you will as you fine tune this so maybe maybe if you are interested i can give you a live demo is it possible is it okay oh yeah okay so so so so if you go to the github uh there is actually a link a javascript uh let me let me reshare the screen

Okay, so you can see the screen.

Okay, so if you go to the GitHub, Hunerp4321 Particle Life, and then we have a downloadable version, so you can download it.

And we have an online version, a 2D online version.

So let me just open the 2D online version.

I hope you can still see it.

do you see it or should i yep and i'm also pasting it in the chat so people can go to it okay so the interesting thing is so you see i i will just so here are the parameters so i'm trying to change the parameters but even if you push the random button so here is a random button

So this random button will just randomly revisit the attraction and repulsion force.

So even if you push the random button, you can still see interesting patterns emerging on the screen.

So let me zoom in.

So you can see I'm just now pushing the random button.

And you see cellular things like very, very interesting things coming to

So we will come soon to Markov blankets.

You'll see that Markov blanket is very natural.

It's something, when you have local rules, that's what Carl Froston usually says, when you have local rules, there will be always a Markov blanket.

So because we have local rules, so the attraction and repulsion rules, they have a threshold.

So you usually see this particle can attract this particle, but there is a circle.

It can't attract very far particles.

the same power as the nearby particles, just like in real life, just like the gravity in real life.

So you can let these systems evolve.

They will produce, but what interests me is even if you push the random buttons, the random, these random rules, push it and you will have interesting patterns.

wow right there just like these chasing chasing around so now you see a bit a bit stability now you can see a stability but this stability will not go forever because of computational industry sometimes something comes to disturb the system so now they have a market blanket they live happy they they have their internal system but at some point you see this okay now the system is disturbed

So they lived for a period of time.

They have nice mark of blanket, but then something outside happened.

We don't know.


SPEAKER_00:
That's the halting problem is could you look at this at a given time, all the rules, every single particle state, all the rules, could you say what the final endpoint will be or what will happen in a certain number of steps?

And you might have a heuristic, but you can't know without actually going through the procedure.


SPEAKER_01:
without actually going to know it would be in simulating the system i mean if why if i want to predict the future i have i should have a supercomputer that is faster than this i should put the initial settings and the supercomputer can simulate the system faster than me but but if i am living inside this system i don't have this leisure to to simulate the system faster than itself so how do


SPEAKER_00:
oh one other one other interesting piece there which you brought up a few times with like having a more powerful computer somehow outside the system so as you start to look deeper and deeper into futures it's almost like the procedure has run time n like it's just whatever complexity class it is it's just one or x but if you start looking further and further ahead

there are more and more adjacent possible so that you actually it's like trying to run up a wall that's getting exponentially steeper because the the faster supercomputer even if it's it's a million times faster but then if it starts looking several time steps ahead each of those time steps have a bigger and bigger adjacent possible so eventually that one million fold increase gets eaten up pretty fast yes yes


SPEAKER_01:
Yes, so that's why I think this part, I actually can't trust the newspaper that it has a primordial suit, but they usually I think it uses MATLAB.

And then so I mean, people nowadays usually use more sophisticated tools.

that have nicer visualizations.

And you actually couldn't find these interesting patterns if you didn't have a real system.

So when I was coding this system, it's impossible to find interesting patterns without having a GUI system to explore easily in real time.

uh and that's what made us discover so many many interesting patterns so now there are other systems that are similar to it actually i think there is a system called lenya which are smoother i think it's also very interesting but the reason why i like that it's very naturally connected to mark of blankets and and active energy so so i will actually now go to to what we can actually elicit from these systems and how can how can we

do abstractions and how can we reconnect what we have learned to our findings that I explained in the beginning of the video.

So let me reshare my screen again.

So I'm sharing the slides again.

Okay, so you can see the slides again.


SPEAKER_00:
Yep.


SPEAKER_01:
OK.

So you saw, like these parameters, I found these interesting patterns.

But somebody can't reproduce it if I download the software, the C++ one, not the online one, because the C++ one is more powerful.

You can simulate more particles without the, I mean, the computer can't handle it.

So you can find amazing, amazing patterns.

And this is just four particles, four color particles.

I mean, you can go five color particles, six color particles.

okay so why so my point is how how how coarse graining can help us uh in in this case so let's say uh i mean this bug shape to be quite honest i actually produced this bug shape from

So this is also not the profile.

The profile is just for the illustration.

But this bug, which you can see, this is from the particle life simulations.

This is, believe it or not, just from these rules.

And it accidentally happened to me.

So green attracts to green with this power.

green attracts red or repuffs red.

I forget whether the negative is attraction or the positive.

But these simple rules of attractions, it can produce you this flying shape, a bug-like flying shape.

So this is some screenshots.

how it flies around.

So my point is that let's say this frog is inside this system and it's a predator.

It wants to predict because without prediction, you can't catch this bug.

You can't catch this bug.

But the frog is inside the system.

According to computational pre-reducibility, it can't stimulate the system faster than itself.

So it can't know.

The only way it can do is does some way of abstraction.

And the easiest abstraction, as we say, is coarse graining.

So if we do coarse graining,

So here I did course going, this is not a simulation, it's just illustrations because, but I think it's trivial to understand.

So now the number of particles becomes 70 particles.

So if the brain of the frog, it just samples the, it just didn't sample all the 600 particles.

It just samples based on locality.

I mean, local things, sample them and then,

In the future, I mean, repetitively, local things will be aggregated together.

You will have a coarse-grained image.

And this coarse-grained image is very, very easy to simulate.

For the frog, now it deals with 70 particles instead of 600 particles.

and that's why maybe even if the frog is inside the system it can actually predict the simulation of course it won't be exactly the same simulation because we are simulating a coarse-grained world

But it will be close enough for the frog to survive.

So if you do the real simulation, the bug maybe go this way and then go this way.

But if you do the coarse-grained simulation inside the brain of the frog, it may predict a different path.

But if the path is close enough for the frog to survive, then that's it.

what we come here into a very interesting conclusion.

Living things like a probe will value accuracy and will value efficiency.

It values both accuracy and efficiency.

So you see, we are closely intuitively coming to the free energy principle, which tells you you have to minimize surprise and complexity.

That's the equation of free energy.

So it's the same.

You can say, so let me go.

actually i i explained that in the video that coarse graining is a very underrated concept it's like it's used in physics in chemistry do you know there is in physics there are so many theories under the name of effective theories they use coarse graining to explain so many things and usually you do not see physicists telling people oh we predicted that but we used coarse graining

i i saw that this is like hiding a different fact because we usually talk about uncertainty principle and things like that okay but you use you use coarse graining coarse grain is part of uncertainty so so in this simulation now

It's true that we can predict the bug, but now we are less certain about the individual particles because we did coarse graining.

But that was okay for us because the aim of the probe is to predict the whole system, not the individual particles.

So I'm actually surprised why physicists usually don't explicitly say, hey, we used coarse graining.

So they use everything to be quite honest.

So many theories of physics depend on, they just use all, even if only when you go to the details, you'll find them, they have coarse grained the system before doing the simulations, whether it's a fluid system, a particle system, anything.

Well, I think everything is coarse grained.

I don't think we can measure anything precisely.

So coarse graining is there, whether we like it or not.

Okay, Markov blankets.

I think Markov blankets is well explained.

Carl Frost has many, many videos about Markov blankets.

So Markov blankets is a different way of abstracting information.

So the nodes that are surrounding, let's say we want to predict X. So if we want to predict X exactly,

we have to know that we have to know the position and the velocity of all the particles in the system but according to markov blanket you don't have to if you know the state of the markov blanket you only need to simulate the state but the problem is how do we know the state of the markov blanket

we know we need to know the state of their market blanket but how do we know the state of so so that will defeat the purpose and it means that we have to simulate the entire system but as i said luckily these markov blankets are sparsely connected i mean you you sometimes see uh the world the reason to be quite honest i'm still not sure why why our world is sparsely connected the only reason that comes to my mind is because we have different force ranges

we have nuclear force which is very very short range we have uh electromagnetic force which is wider range we have gravity force and then because there is a speed limit of light if you combine the speed of limit of light and the the range of forces you can have various ranges of forces just like just like in our simulation so just like in our simulation so we can also say that the particle simulation is also sparsely connected because we have simulated different ranges of forces

That's why close by particles, so they will arrange sooner to produce lumps.

So that's why I think Markov blankets are possible.

Stable Markov blankets are possible because of sparsely connected system.

Why sparsely connected system?

Because of different range forces.

That's my explanation for that.

And this makes prediction easier.

Now, the reason why I think this makes prediction easier

So I will give you this example, last example of particle simulation, because to be quite honest, the reason why I like particle simulation is it gives you intuition about a lot of things.

So one of the intuitions, I usually follow up Stephen Wolfram's video and usually say, oh, I understand entropy now.

I think entropy is the feature of observers because we observers are computationally limited.

That's why entropy is there.

I was usually thinking what the hell he's talking about.

it's a bit so i finally have aha moment when i was trying to understand how entropy works when watching these particle systems so so let's say this was the initial uh state of the before we start the simulation this is all the particles if if somebody sees this system it says this system have high entropy

it's intuitive i mean even if we don't specifically define what's entropy here it's it this system is all sparse around different colors everywhere so if i tell you can you predict what will happen to this particle here at the middle which is circled by the where does this particle go

I mean, you don't know.

I'm sure you don't know.

The reason you don't know, even you have the rule, is just your brain cannot simulate all the particles inside your brain, the forces,

you can't you can't figure out what's the direction of this particle will it go this way this way but if your brain is unlimited power i mean if you can compute all these particles right away there is no uncertainty you can tell me i know this particle will go this direction but the reason you can't tell me because you can't compute all these particles now as the system evolves like you you let the system evolve

now we have different entities with different mark of blankets they are stable so you have chunkers so now your brain can meaningfully course grain so so you can easily course when you can say oh we have four systems and these four systems they attract everything so now if i tell you can you tell me where this dot goes

according to the attraction rules.

I think now you can say, you can say, oh, this thought is very close.

I think it will go down.

So the entropy in your brain is increased.

So the entropy, I mean, the way that the uncertainty is decreased, sorry, because now you can predict where this thought will go.

And the reason you go, because now your brain can compute what happens because your brain chunked, coarse grained all these particles together as one entity.

that's because of the Markov blanket or because of these stable blankets which encapsulate these entities.

You treat them as one entity and the system is now become more predictable.

So now you can tell these particles will go downward.

And you can say this will go from high energy to low energy.

So basically, you can use these particle systems.

I actually didn't do that.

It's in my plan to do actual simulations, not just intuition about these systems.

So this is Gibbs pre-energy, in which Carl Preston actually derives from the concept of pre-energy in the living beings.

It just relates entropy with enthalpy.

So we know that entropy of systems increase, but local entropy, because of these local forces, local entropy decreases.

But the entropy, overall entropy decreases because particles will go down and then the energy as a light will go into the universe.

Basically, these simulations are so simple to capture all the details.

But all the details, the global entropy either stays same or increase.

But the local entropy decrease.

And that's what self-organizing systems like us, living beings, will try to do.

They will go to that.

So what Carl Friston does is generalize this concept to living beings.

And to be quite honest, I don't know the details that he do that.

But he usually use these sequence, Langevin equations with full body attractors.

Fokker-Planck equation, then he comes to the Markov blanket.

Once it reaches the Markov blanket, it can connect to the living world, Bayesian world.

And then we have, so these particle world can be connected with living world.

That's what Carl Friston usually does.

And the interesting thing, it's the same objective.

So the high-level equation in which that Carl Friston uses, it just, you have living systems want to reduce surprise and want to reduce complexity.

And if you remember, we told you because of computational, we want to be efficient and we want to increase the accuracy.

So we reach the same intuition without driving it.

Just because we have computation, we have to be efficient.

There is no way around that.

So this part and that's what makes because

Long time ago, I didn't understand what's exactly the difference between free energy principle and the predictive coding.

So the main difference is that the free energy principle also added efficiency.

It says that we try to predict, but we also try to be as efficient as possible, as simple as comes principle way, or the least action way.

So it tries to reduce the complexity.

And I think the reason why we want to reduce the complexity is because we have competition in relation to principle.

so now we come to active in france because according to active in france there is a trick we can do we can not only predict the system by learning about it we can actually be participate in the system so so you either learn from the world uh which i call the bottom up system or teach the world is the one that means you you are trying to change the world so just like if you go back to this to this particle it's just like

You see, so this particle either attracts the entire system to itself, or either the particle goes to the bigger system.

So we individuals also do that.

Either we are trying to change the community, or either we adapt to the community.

And we usually have to do the later.

The majority of us will be just slaves.

We will just say, we can't change the world usually.

But I think the analogy is very easy to see.

so so there are a lot of questions uh so i i usually usually this question comes up if we try to predict the systems why we just don't go to the dark room uh i still i i also see the answer to this from the computational irreducibility principle because your computation doesn't let you there will be always emergent factors so locally you can do that as an individual but collectively we can we can do that because the system will always change

This is analogous.

Another example which I like, I don't see it's used.

Why we don't use fascism?

Because fascism actually makes the system predictable.

Everything behaves according to some order.

I mean, we can use a less inflammatory word than fascism, actually.

Actually, people do that.

I mean, you can see religion as a way to make everything predictable.

If you follow a community, a strict community, they will tell you, you should behave that.

You should do that.

The whole day will come that.

So everything will become predictable.

And that's why these systems sometimes are appealing for people, because it encapsulates the idea.

But the problem with that, again, computational doesn't let you live that forever.

You will be trapped with a local maxima.

And you can do that.

You can live in this predictable system for years, for thousands of years.

The outside world will always change.

And at some point, these changes will either affect you collectively or individually.

That's why we have to change.

That's why we have curiosity.

We want to learn.

So according to the simulations that have been done in free energy principle, even these particle systems, they are actually modeling the outside.

The reason why they did that, so if you try to follow the activity of these particles, you will see they are correlating with the outside.

if a change happens in outside you will see that a change happens in the inside so as if the inside of these entities they are trying to model and the reason why they are trying to if you see that the mark of blanket just actually just looks like a neural network so it has sensory nodes it has inter internal nodes it has active nodes so it's just like a neural network it's just like how it's trained so either use evolutionary technique to train the

some blankets survive some will not survive so we are using evolutionary technique to train it or you can use back propagation or you can use the technique that happens inside our brains although there are a lot of unknowns how the learning happens in our brain but but the thing is same so so nested levels of markov blanket you can see it as a nested level of neural networks it's same it's same level of abstraction

uh okay so so that's what makes us that's what i'm saying we can beat the computational irreducibility because so all the brains

are local.

They can't compute the entire system.

But collectively, the lower brains, they abstract the information.

They don't send the entire information to the upper, the higher-level brain.

They abstract it away.

So the higher-level brain, even though it's computationally very limited, it has the same, it can actually simulate the system because it's abstracted.

It's coarse-grained.

So it's usually a balance between top-down and bottom-up.

So basically what happens is that the inner levels will be more predictable, but the outer edge will always be less predictable, or the higher levels will always be.

And I'm usually saying cautiousness leads to the higher level.

at the age of uncertainty so we have subconscious everything is predictable like for example if a fire here my hand will go away that the gland gangrenous in our spinal cord will decide even before it goes to our brain the decision so predictable things are happens in your subconscious the unpredictable things happens in our conscious uh

Okay, I think I have tried to explain all the important things that I want to do, but just before finishing, I want to just reconnect this concept back.

So far, we are talking about how do we do abstractions.

We generally, the best abstraction, as I said, we have neural networks, and how neural networks work using backpropagation, and how backpropagation works, we really don't understand.

Even people who produce chat GPT, they say, we can code the system for you, but we don't know how back propagation will decide how abstractions are formed.

so i i mean a few years ago i came across an algorithm which is which actually uh drives multi-linear regression i i thought this is beautiful because it it matches with our predictions without with our simulations very very well the simulations that i did in the beginning so i don't know have you heard about lattice filters lattice filters


SPEAKER_00:
Yeah, go for it, though.

Continue.


SPEAKER_01:
Yeah, so lattice filters are actually, they used all the way in Kalman filters and signal processing filters.

And I just don't know why they are less famous in the machine learning community.

They are more, if you go to the signal processing community, you will see a lot of talk about lattice filters and Kalman filter and things like that.

For me, lattice filters are actually very, very similar to how the brain is depicted.

Because the predictive coding system will tell you we have predictions and we have prediction errors.

And they match very, very neatly together.

And these lattice filters are very, very easily derived.

So if you have two groups, for example, let's say we want to see which group are more reliable to have a high blood sugar.

We have obese group and less obesity.

and usually in scientific tests we have more variations and these variations will affect our results so the biggest experimental design is to remove these variations that's what we say control design but sometimes you can't do that because there will be always variations so many multifactorials that you're even not aware of it

So what you can do that, you can do that post hoc.

So let's say we want to predict these Xs using X3.

And what's not predicted is we call them prediction error.

So the prediction error will go to the upper labels.

And then we use the residual errors to predict the other remaining errors.

So you do that until you end up with the last prediction error, which represents this feature, in this case, obesity.

This is where you can actually find the relation with obesity and why the factor that you want to test.

And if you do that, especially for a sequential data, you actually end up deriving that lattice filter again, which I talked about here.

so you can you you will drive lattice filter and this lattice filter is actually very very similar to how a prediction error works and this this way it's similar to what our findings in in our paper when we say the expected stimuli will be suppressed so these lattice filters will also tell you the same thing it will tell you

In multivariate linear regression, you are trying to predict the stimuli.

The prediction error, which is not predicted, it will go to the upper vehicle, and then the outer context will try to predict that and that.

you will go to increase the context to predict the remaining errors so i thought this is a really an interesting connection but these lattice filters are not used in machine learning the reason is because mostly they are linear systems there are ways that you can make it non-linear but they are because they learn very fast they are very very sensitive to noise so if you have a small noise

they will just learn the noise too and that's why that's why i think people in machine learning system they actually go to uh use optimization methods using back propagation because in back profession you have a learning rate which you can reduce it that means even if a noise comes it will not affect the entire system so that's why my only explanation why people don't use this method but i like this method because it connects our brain the predicted coding error

and it is very similar to the findings that we have in our paper and and i think it connects nicely somehow to to the free energy principle so uh thank you a lot for for for uh listening and i'm here if you have any questions awesome


SPEAKER_00:
okay well thanks for the awesome and wide-ranging presentation while i crop and get everything back if anyone wants to write a question in the chat go for it and just to kind of begin the discussion i mean could you share a little bit how how did you get to studying the suppression


SPEAKER_01:
uh repetition suppression in your phd then what what journey has brought you to making the self-organizing particles and all the cool youtube videos oh thank you uh well repetition suppression uh it's uh my phd supervisor is actually uh a very uh big name in repetition suppression so he he have a lot of reviewed uh review papers on repetition suppression

And I found this is interesting.

My beginning PhD, I was interested in episodic memory, because my master's degree was about episodic memory.

But later on, I thought there is a very nice connection between repetition suppression and predictive coding literature.

Back then, I wasn't aware too much with three energy principle, but more aware with predictive coding literature.

so i thought because there is this connection and and and it was always surprising to me uh the phenomena of repetition suppression it just means that when something becomes very expected when when you finally predict something your brain will go either so usually i thought in the beginning it's counterintuitive because usually you think oh if something becomes very predictable you should your brain should be more active

But that's interestingly not true.

Your brain is usually what I think is more active at the age of uncertainty.

Because let me be honest, if you see noise, it's completely unpredictable.

But at some point, your brain will go idle.

interested so we uh our paper was not about that but i am sure if there are papers uh it's a u-shaped curve so if something is very predictable your brain will be less interested less active and if something is completely unpredictable again we we are not interested in it so i think the

For some reason, I think it may be connected to consciousness.

I am not sure about, my studies was all about consciousness, but I think repetition suppression is probably the closest phenomena that we can objectively examine and in some ways connect because subjectively, when something becomes predictable, you also feel bored.

It's boring.

and we feel sleepy.

So the objective findings that we see, the activity decreases, also correlates with the subjective findings that we actually feel.

And because these are too connected, I think it's probably the closest objective phenomena that we can test, and somehow we can correlate to consciousness.


SPEAKER_00:
Yeah, I mean, the example of just a random pattern

that could be intelligently coarse grained and you go okay frame after frame the coarse graining of this whole picture is just gray so then the coarse graining is unsurprising so then even if the pixel level details are still surprising you've reduced or bounded surprise at the slower or the deeper layer so again that becomes not interesting

oh wow that's actually a very so so i actually didn't thought about it like that but the core screening idea if you combine it it will just reconnect back to that more extra that's very nice actually i i really liked how you connected the core screening with a the markov blanket a lot of work has connected neural networks and the loss function that's used in training the neural network with the variational free energy and then in the neuroimaging setting

coarse graining like you brought up it's a really simple heuristic because it's just spatial proximity you don't need to cluster things by their functional similarity you just use the anatomy and then it's interesting to think about the markov blankets as coarse graining in a causal space like if you had um 10 10 different uh sub procedures

but then you say well one procedure one two and three are like one unit and then that becomes like its own thing but procedure one might be anatomically close to seven but then one two and three are causal and so then it's like building these parallel maps

your phd it was more grounded in the spatial and the temporal and then the markov blanket abstracts that into a causal but then abstracting away from space and time is where we get patterns and that is how abstraction helps us predict the future


SPEAKER_01:
exactly oh god this is interesting i think i think i think course graining is just like a door for understanding so many things and i usually i i sometimes interested in physics because even in my high school i was interested in physics i don't have a formal education in physics but i am very very interested and i am very very interested in the phenomena uncertainty principle and

um bills phenomena and i usually i'm afraid to ask this because they usually get angry is there a relation between course graining and uncertainty so they usually tell you no and uncertainty in physics is fundamental

I don't know whether they may be, there may be physicists outside, outlier physicists like Stephen Wolfram or some other physicists, they have more deterministic view of what uncertainty means or what randomness means at the lower level.

to my experience usually when i go discussing reddit they will get angry they will say no you don't understand anything about the underlying atoms they will just go random it's just a a true random generator


SPEAKER_00:
Well, I don't know who will get mad about what, but in a way, the movement from quantum scale physics to classical physics is like coarse graining.

It's like saying, well, if you go to one atom, it might be difficult to know what vector it's going to move in next, even if you knew the temperature.

So maybe that's near the limits of a true ontological uncertainty.

But then if you have a ball moving through space,

You can get a really good prediction on a slower, larger thing, and then that's classical mechanics, where you don't have the same uncertainty, you can just assume this basically infinite precision.

But then quantum is where you can't assume infinite precision, and people debate whether that's ontologically how it is at a certain scale, or whether that's an artifact of our interfacing with small things,

But regardless, things that are about as big as we are, it's kind of like, these are like pebbles that are about as big as we can pick up.

Those are the things that we can think about.

That's what makes sense to us.

And then those are the classics.

And then things that are more challenging to abstract or predict, they're nonsensical or they're outside of our scope of understanding.


UNKNOWN:
Yes.


SPEAKER_00:
But from earlier, how did you get from the repetition suppression to working on what you're doing now and making cool videos?


SPEAKER_01:
uh actually uh particle uh making videos uh i i as i told you before i actually learn more when i make a video about the topic so i call that active learning there are so many topics that in my life i want to try learning it's just like because i see many intelligent people they say oh this this topic is very important this topic is

so right now in in current situation i'm trying to learn mona and i i i look at many videos i don't know what the hell they are talking about but because so many intelligent people they say oh mona is a very interesting way of abstraction in programming just like i have to learn that and at some point i think when i fail understanding and

i say oh if i have if i if i try to make a video on it i will finally understand it so usually making videos on something it actually pushes you to understand it i don't know why this is true but i find it very interesting because our brain course brings a lot of information and self-attention it always tells you this is not interesting this is not interesting it has a lot of a big selectivity so this part of selectivity tells you which direction you go

So when you make a video on something, it will actually now say, okay, now I know this is important.

So it gives you, it makes your attention on the topic.

Without that, it will always something tells you to procrastinate and

not not pay attention to it so the reason why i make videos is just interest to be quite honest i make videos when there is some idea and i like to represent i like to learn about it and i like to represent that in a video uh so one of the these ideas which i like local maxima idea

i i find it fascinating and actually it links back to computational irreducibility in some way because local maxima tells me that because we are limited because our brain is limitless we we every now and then we will be trapped on a local maximum and that will be very difficult when we travel in a local magazine that's it we we have to become conservatives there is no way around either you risk something some people risk their lives go outside otherwise

the direction will be lost.

when you when you are trapped on a lot of them so that idea was very interesting and particle simulations i was trying to explain to some friends how because usually when you discuss evolution theory they will tell you how pure cells arise and i thought i almost thought the first cells are actually easy to explain i mean people usually think that the first cells are the complicated cells like now we see no the first cells are probably just a membrane and a nucleus and i thought that this is very

easy to simulate with some particles attracting each other and i was looking at youtube videos to see and then i came across this particle life system uh i don't remember the name of the guy but he was explaining uh these cellular shapes

with attraction and repulsion forces and i thought wow this is very interesting actually i've tried to implement that by myself and i improvised that and i was surprised i was trying to do that and then a bug on my screen appeared a flying bug

So that was very, very surprising.

I was shocked.

I mean, how can I, I'm just randomly examining some simple rules and I see something like a flag.

And then I actually made a video and that video actually have a lot of views and the code have a lot of participants.

They made the code,

way more sophisticated so now it has a lot of gooey parts and thanks to the participants uh and i think i think it's uh the most uh uh yeah i mean many many people actually become more popular uh the particle life uh after that video and the reason why i think it's because

The code was the simplest possible code.

I thought, because it is educational, I have to make it the simplest possible.

I actually reduced all the parts that I thought it may not be interesting.

And because the code is very simple and the patterns is very complex, I thought that's what attracted.

And I am hoping more people actually come to the part of my life.

Because I know Game of Life is very interesting.

It's touring complete.

But particle life is less known about, but they are more biologically looking.


SPEAKER_00:
Yeah, a lot of insights there about the formation of, well, first the joy and the fun of open source and of people's participation in science around the world.

And then in the first cells forming, there's like hydrophobic and hydrophilic.


SPEAKER_01:
Exactly, that was my question about how huge cells are.


SPEAKER_00:
So how the material gets there is, you know, these are all inquiry to explore.

And then also just about the videos.

So one thing about video is it's an audio and a visual component.

Now, some videos you can watch with no audio and vice versa.

But when audio and visual are happening together, it's kind of like two touches from across the Markov blanket.

Like if someone's doing hand gestures while they're talking,

then they're both being emitted from the same cognitive state.

Whereas if it scrambled what hand gestures were happening with the audio, it would remove that meaning.

But synchronously, they're experienced together.

So then they're seen as meaningful.

So then like I was...

paying attention to the presentation.

And it was like slide after slide, I would be initially surprised.

And then through the suppression of surprise, I keep seeing in these two images here, it's like sometimes you connect with a layer above or below,

which is top down from the top's perspective or bottom up from the bottom's perspective, or you go sideways.

And so that could be in a grid world, lateral, spatial, that could be through time, like on a timeline.

And it's like, those are the local moves that can be made.

You can move up or down a layer of abstraction and generalization, and then you can move around in space and time and cause.

It's like those are our, that's like the push-up and the pull-up of the local interactions.

These are the moves, the dance moves.

And then the question is just like, what does the party look like?

And that's the particle simulation.

What really, what starts to happen when all of these moves, what if some of the particles can do abstraction?


SPEAKER_01:
Yes.


SPEAKER_00:
Yeah.


SPEAKER_01:
Can I show you a demo?

Oh, yeah.

Go for it.

Okay, so this latest filter, I do have a video, another video on it, but I have also a demo, so I can show you a Python code.

So let me open up my... So do you code in Python?

I saw you code in Python.

What IDE do you use?


SPEAKER_00:
I use Cursor for the AI augmentation or just VS Code.


SPEAKER_01:
Oh, Visual Studio Code.

I use what's called Spider.


SPEAKER_00:
Oh, yeah.

For scientific Python.

Yes.


SPEAKER_01:
It's very similar to Matlab because in my PhD, I left Matlab.

I hate it as a language, but it's very nice IDE.

It has a very nice IDE.


SPEAKER_00:
Hate it as a language, love it as a friend.


SPEAKER_01:
Exactly.

So when I transitioned to Python code, I saw Spider as a home because it looked like... It looked like... The MATLAB.


SPEAKER_00:
Yeah.

Also like RStudio.


SPEAKER_01:
Oh, yeah.

Yeah.

Okay.

So you see the IDE now, huh?


SPEAKER_00:
Yep.

Go for it.

Great.


SPEAKER_01:
So let me...

Okay, so I'm pasting a previous... Let me paste a code from... Okay, so can you see the code?

OK, so I like this algorithm.

People usually call it lattice filters.

As I said, I derived this algorithm just from the prediction coding multilinear regression system.

And I have a video about that.

It's titled Regression and Brain.

so it's very very simple code but it somewhat explains repetition suppression so if you if you run the code okay so now this this will act this is activity this is the error and this is by the way do you see the plots or you just see the spider ide no plots just the ide

OK, so I have to share the screen.

The screen is better.

So let me share the screen.

I think you should see.


SPEAKER_00:
Oh, yeah.

All right.

Looks good.


SPEAKER_01:
OK, great.

So I run the code and.

this is the pattern so so let's let's simulate a sinusoidal pattern in the beginning so so let's say we are just simulating a sinusoidal okay

So this is the truth, the green, and the orange is the prediction.

You see in the beginning, it doesn't predict.

The orange line doesn't overlie the blue line, the sinusoidal activity.

So the sinusoidal activity is our data, the truth, and the prediction is the algorithm which I showed you.

It's just a few lines of code here.

So I will just explain it right now.

But the interesting connection I will try to make to repetition suppression.

So you see after a few iterations, it can perfectly predict the sinusoidal activity.

And you see the error plot actually reducing.

In the beginning, it's high, but then it's reducing to zero.

and this actually tells you how much activity do we have in different layers so you can see in the beginning in the lower layers we have higher activity let me zoom in so we have higher activity in the lower layers and then as we go to the upper layers we have less activity that's why because because the system can predict what happens this now i will introduce a surprise to this system let me uh

in this code now i am for every for every two steps i am just changing y to one it means just sinusoidal wave but i'm introducing one to it so just like now we have a surprise

Okay, now you see it's more difficult.

So in the beginning, it can't predict the entire system, but then later on, it learns to predict the sequence.

So you see, it's the same sinusoidal, but I have introduced, number one, I have introduced something surprising.

but then later on it predicts and then the error goes down and then again we have the activities but now the activities can go to higher layers before it's suppressed to zero okay now i will just introduce one surprising event

So just minus 1 at point 17.

So at point 17, I will just introduce a surprise to the sequence and see how the algorithm behaves.

OK.

So you see it predicts the system.

But here, when I introduced a surprise, it can't predict it.

It can't fit the data.

and if you see the activity you see when the at the level at this level when i introduce start plus surprise you can see the prediction error goes all the way to all the upper layers so just like repetition separation when you see something surprising the activity increases so just like that in the beginning so this algorithm is actually a very nice view to tell us how our brain may be working


SPEAKER_00:
And here we even see that almost diagonal propagation up and to the right as it sort of normalizes and calms down at higher levels.


SPEAKER_01:
Exactly.

So maybe that's a good thing and a bad thing because the algorithm learns this surprise.

That's why even in the upcoming steps, it needs some time to come back.

So maybe we can put a filter there.

We can say, oh, if the prediction error is very high, don't learn the pattern.

So maybe we can put a threshold.

And maybe that will actually tells me that's why we have belief system in us.

That's why our brain is actually fixed on some ideas.

Sometimes if something very surprising and it doesn't conform with your previous beliefs, because the learning algorithm inside our brain is very fast,

if we can learn everything that even it doesn't conform to our previous beliefs it means it can mess up uh sometimes they it's similar to catastrophic forgetting it's just like the activity the new activity may superimpose the previous activities in your brain i can mess up everything and that's why maybe some people are actually hard to change their minds even after

when they say new data, when they say new things, people usually, if it doesn't conform to their beliefs, they will just deny it.

Because sometimes you need to fix, you need for this surprise to become a pattern so that your brain surrenders, says, okay, I want to learn that.

That's what I think.

I mean, I think this algorithm can tell us more about our brain that we even...

uh so so it's very very easy it's just it's just like the prediction uh your system so so you have a sequence of data y is and then you just have just just focused on these two lines because everything happens here so you have so you just have a forward prediction and backward prediction basically w and this is the weight multiplied by the activity current activity it will try to predict

the future activity, FF.

And the subtraction, the difference between these two is the prediction error.

So that prediction error will go to the upper level.

So we have a nice illustration on that.

So the quote just illustrates this.

so y5 here the sequence tries to predict y4 and then the prediction error goes up and then y4 tries to predict y5 the prediction the the forward prediction and the backward prediction so the prediction error we have two prediction error the forward prediction error and the backward prediction error so each level tries to predict the previous prediction error and each prediction error will go up hierarchy

And hopefully at the higher hierarchy, you will have less prediction error.

It means like your brain can predict everything.


SPEAKER_00:
I'll just add a note on prediction error.

The prediction error is denominated in the units of the measurement.

Like if you expect the room to be 30 degrees and it's 31, then the error is one.

And then the move in active inference is to translate that error signal into information units, bits, surprise.

And then surprise, if you can compute it exactly for a simple system, go for it.

But then in cases where it's intractable or you want to do an optimization on bounding surprise, that's variational free energy.

And that's been used for decades, variational autoencoder, all these other kinds of variational methods in physics.

so those are all kind of aligned like if you bow if the arrow were zero the surprise would be minimal the free energy would be minimal if there's a large error the surprise is higher the variational free energy is higher so like they're slightly different formulations but they're all monotonically related


SPEAKER_01:
They are related.

I think they are using other optimization algorithms to learn this.

So there may be better optimization.

That's what I say.

You have to learn gradually.

The problem with this system, it learns.

It tries to exactly learn.

And that's why when you show something surprises,


SPEAKER_00:
Well, that kind of even comes back to the earlier numbers that you showed with the decimal points.

It's like, are you trying to predict to one significant figure that's coarse graining to just one digit, but then if you give a computer program

a float precision number it might have a ton of decimal points and then it will think that those are just as important to compute as the earlier decimal points and then it gets catastrophic because it's doing really well on like the the 50th decimal point but it lost the plot on the main thrust


SPEAKER_01:
Sometimes precision error.

By the way, floating point numbers are also a form of coarse graining because we can't actually simulate real numbers.

Real numbers have infinite precision.

So the moment we say we use floating point, it means we have coarse grained the real number.

there is no way without coarse graining even in in the computers even if you see if you use double precision it's it's just you have at some point you just cut the the number yeah um cool so what are your next research or or directions that you're going to be exploring

Okay, there are two directions.

One is I actually want to explore more free energy, what happens in these particle systems, but not just intuitively, because to be quite honest, the things that I showed you, some stuff that I thought it is trivial, we don't need to even stimulate it.

But I have to simulate that because sometimes simulations give you a surprise, just like the first example, which I showed you.

I mean, you think this happens because of this, but sometimes this is not true.

The next thing, I am actually working on a different algorithm.

I call it PAN, Predictive Hebian Unified Neurons.

uh i'm so far i'm not successful at this algorithm we can see this algorithm is a variant of this algorithm which i just showed you uh but but i'm trying to make this algorithm more local i mean learn locally uh the problem is because you so so what's the problem with the current neural network approach the neural network works very very good we saw magic with gpt but the problem is

these systems they are very very slow training they are sample inefficient they you have to give them so many samples so that they can live the reason why because they you have to tweak the learning rate to to lower evils because if you tweak the learning rate higher if the system learns fast

it will have a catastrophic forgetting.

The moment it sees something new, novel, because it has high learning rate, it tries to incorporate that new thing into all the systems, all the neural network nodes.

And because these systems are distributed, it means it tries to fine-tune every node, every weight.

and that could be catastrophic especially if this outlier is just a noise it's just a noise you need to ignore but if you have a high learning rate the algorithm doesn't can't learn that is why

you have to make the learning rate small but if you make the learning rate small it means you have to iteratively learn so many samples and variations and that's why only big companies nowadays can train neural networks because it has it needs so many iterations and even these big companies they can't make a real-time system so even if they make a robot

That robot can't learn real time.

It's just like the company can tell you, oh, this robot can only learn, do things that we have trained in 2020.

We have to take it back and update and retrain again on the data from 2023 or something like that.

So it's not a real time learning system.

So even now, it's not a real time learning system.

if OpenAI needed to retrain everything again.

So basically, it means these algorithms are not real-time.

But this algorithm is real-time, the one I showed you.

It means once you see the data in just a few samples, it learns.

But again, the problem with this algorithm is this catastrophic disturbance.

I mean, this instability, once it sees something surprising, because it tries to learn it, it just mess up.

And that's what I try to figure out, something that learns fast, but it doesn't disturb the entire system.

So I'm actually trying to do that, but so far, not very successful.

But every now and then, I have ideas to implement and then try again and again.

I think now, I mean, a few months ago, I have a new idea, but I still don't have time to implement it.

So maybe you can hear more about me in the future, about the predicted hidden unified neurons, which I shortly called one.

which is not fun.

It doesn't work.

It's not fun yet.


SPEAKER_00:
Planning for fun is still fun.

Okay.

Do you have any last comments?


SPEAKER_01:
No, thank you very much for actually inviting me to this.

And I hope you continue.

I actually find Active Inference

uh channel very interesting and i find the topic is very interesting actually the entire topic that you specialized yourself to just this one topic and it's a very very interesting i i thought it's a very good candidate for the theory of everything i mean people usually think that the theory of everything should come from physics but i think it will come from neuroscience that's fine

I mean, it doesn't make sense to have a theory of everything without consciousness.

That's why it has to come from neuroscience.

I mean, physics can help in neuroscience, but the theory of everything

It comes from neuroscience.

There is no way around it.


SPEAKER_00:
Or it comes from a new combinatoric fusion.

I mean, one of the key moves in the free energy principle is to talk about the cognitive particle.

So then if it's a Bayesian mechanics, if it's a statistical physics for cognitive particles, then it's not surprising that it has a lot to do with particle physics.


UNKNOWN:
Yes.


SPEAKER_01:
Yeah, maybe.

That's the best thing to actually do.

Actually, I have maybe one extra animation slide I can show you before.

Sounds good.

Yes.

About this combinatory things.

I usually show that to friends and they usually like it.

And I explained that in some of my videos.

So, I mean, if we see this graph as the entire human knowledge, and then this child, it likes to learn, it's just born, it doesn't know anything.

So the problem is our age is very limited.

So by the time the child, the curious child that tries to collect information, it will get old and it will, at some point, it will die before learning about everything.

And that's the reality.

Okay, now what I think actually better is specialization.

It means that child shouldn't try to learn everything.

In the beginning, it tries to learn all the things that are surrounding and then should specialize.

If you specialize, at some point you will reach the edge of knowledge.

And once you reach the edge of knowledge, this is where you can find a new information, the extrapolation.

That's what I call extrapolation.

Okay, but the problem with specialization is we will be trapped at a local maxima.

So it means if this guy is a doctor, he tries to treat cancer, he's specialized in cancer, but maybe the true treatment of cancer comes from an engineering trick.

Someone can find a new radiation that doesn't affect normal cells, only affect cancer cells.

So I mean, if you trap yourself just reading medicine, you can never find the solution.

So this solution, how it comes?

So let's say, so that's my point.

You can never make distant connections, long term connections, because you specialize, you trap yourself.

Now, if other people who are also specialized in different topics,

If they collaborate with each other, that's why I'm coming back to you.

So if physics and collaborate with chemistry, with chemists, and they collaborate with neuroscientists, and then this collaboration, we can overcome the local maximum traps.


SPEAKER_00:
Totally agree.

And there they are working on the interdisciplinary team, sending each other papers, sending each other jokes and memes that reflect their worldviews to give a kind of qualitative sense of their understanding.

And also I think in the coarse graining, I think about nestmate ants returning with food.

And so they're not saying what temperature or anything.

It's just interacting with nestmate.

Yes, there was food.

Someone else came that way with food.

And so that's like a binary coarse graining that tells you something about that direction.

And so it's like every node is a local sense maker receiving and sending and that's active inference.

yes yes that's a nice way to conclude okay thank you good luck with your with your work and with your education so it's been awesome thank you very much daniel see you soon bye see you bye